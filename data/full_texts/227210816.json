{"id": 227210816, "updated": "2023-10-06 08:19:31.224", "metadata": {"title": "Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers", "authors": "[{\"first\":\"Kaidi\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Huan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Shiqi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yihan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Suman\",\"last\":\"Jana\",\"middle\":[]},{\"first\":\"Xue\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Cho-Jui\",\"last\":\"Hsieh\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 11, "day": 27}, "abstract": "Formal verification of neural networks (NNs) is a challenging and important problem. Existing efficient complete solvers typically require the branch-and-bound (BaB) process, which splits the problem domain into sub-domains and solves each sub-domain using faster but weaker incomplete verifiers, such as Linear Programming (LP) on linearly relaxed sub-domains. In this paper, we propose to use the backward mode linear relaxation based perturbation analysis (LiRPA) to replace LP during the BaB process, which can be efficiently implemented on the typical machine learning accelerators such as GPUs and TPUs. However, unlike LP, LiRPA when applied naively can produce much weaker bounds and even cannot check certain conflicts of sub-domains during splitting, making the entire procedure incomplete after BaB. To address these challenges, we apply a fast gradient based bound tightening procedure combined with batch splits and the design of minimal usage of LP bound procedure, enabling us to effectively use LiRPA on the accelerator hardware for the challenging complete NN verification problem and significantly outperform LP-based approaches. On a single GPU, we demonstrate an order of magnitude speedup compared to existing LP-based approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2011.13824", "mag": "3133084582", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/XuZ0WJLH21", "doi": null}}, "content": {"source": {"pdf_hash": "01fe33d7147cfb08d4542402089535ed911b4024", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2011.13824v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cb0179b41b536a5b45d7f01225cc233681839eec", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/01fe33d7147cfb08d4542402089535ed911b4024.txt", "contents": "\nFast and Complete: Enabling Complete Neural Network Veri- fication with Rapid and Massively Parallel Incomplete Verifiers\n\n\nKaidi Xu xu.kaid@northeastern.edu \nNortheastern University\n\n\nHuan Zhang \nUCLA\n\n\nShiqi Wang tcwangshiqi@cs.columbia.edu \nColumbia University\n\n\nYihan Wang wangyihan617@gmail.com \nUCLA\n\n\nSuman Jana \nColumbia University\n\n\nXue Lin xue.lin@northeastern.edu \nNortheastern University\n\n\nCho-Jui Hsieh chohsieh@cs.ucla.edu \nUCLA\n\n\nFast and Complete: Enabling Complete Neural Network Veri- fication with Rapid and Massively Parallel Incomplete Verifiers\n\nFormal verification of neural networks (NNs) is a challenging and important problem. Existing efficient complete solvers typically require the branch-and-bound (BaB) process, which splits the problem domain into sub-domains and solves each sub-domain using faster but weaker incomplete verifiers, such as Linear Programming (LP) on linearly relaxed sub-domains. In this paper, we propose to use the backward mode linear relaxation based perturbation analysis (LiRPA) to replace LP during the BaB process, which can be efficiently implemented on the typical machine learning accelerators such as GPUs and TPUs. However, unlike LP, LiRPA when applied naively can produce much weaker bounds and even cannot check certain conflicts of sub-domains during splitting, making the entire procedure incomplete after BaB. To address these challenges, we apply a fast gradient based bound tightening procedure combined with batch splits and the design of minimal usage of LP bound procedure, enabling us to effectively use LiRPA on the accelerator hardware for the challenging complete NN verification problem and significantly outperform LP-based approaches. On a single GPU, we demonstrate an order of magnitude speedup compared to existing LP-based approaches.Recently, a Branch and Bound (BaB) style framework(Bunel et al., 2018;2020b)  has been adopted for efficient complete verification. BaB solves the optimization problem min x\u2208C f (x) to a global * Equal Contribution.\n\nINTRODUCTION\n\nAlthough neural networks (NNs) have achieved great success on various complicated tasks, they remain susceptible to adversarial examples (Szegedy et al., 2013): imperceptible perturbations of test samples might unexpectedly change the NN predictions. Therefore, it is crucial to conduct formal verification for NNs such that they can be adopted in safety or security-critical settings.\n\nFormally, the neural network verification problem can be cast into the following decision problem:\n\nGiven a neural network f (\u00b7), an input domain C, and a property P. \u2200x \u2208 C, does f (x) satisfy P?\n\nThe property P is typically a set of desirable outputs of the NN conditioned on the inputs. Typically, consider a binary classifier f (x) and a positive example x 0 (f (x 0 ) \u2265 0), we can set P to be nonnegative numbers R + and x is bounded within an l \u221e norm ball C = {x| x \u2212 x 0 \u221e \u2264 }. The success of verification guarantees that the label of x 0 cannot flip for any perturbed inputs within C.\n\nIn this paper we study the complete verification setting, where given sufficient time, the verifier should give a definite \"yes/no\" answer for a property under verification. In the above setting, it must solve the non-convex optimization problem min x\u2208C f (x) to a global minimum. Complete NN verification is generally a challenging NP-Hard problem (Katz et al., 2017) which usually requires expensive formal verification methods such as SMT (Katz et al., 2017) or MILP solvers (Tjeng et al., 2019b). On the other hand, incomplete solvers such as convex relaxations of NNs (Salman et al., 2019) can only provide a sound analysis, i.e., they can only approximate the lower bound of min x\u2208C f (x) as f and verify the property when f \u2265 0. No conclusion can be drawn when f < 0. minimum by branching into multiple sub-domains recursively and bounding the solution for each sub-domain using incomplete verifiers. BaB typically uses a Linear Programming (LP) bounding procedure as an incomplete verifier to provide feasibility checking and relatively tight bounds for each sub-domain. However, the relatively high solving cost of LPs and incapability of parallelization (especially on massively parallel hardware accelerators like GPUs or TPUs) greatly limit the performance and scalability of the existing complete BaB based verifiers.\n\nIn this paper, we aim to use fast and typically weak incomplete verifiers for complete verification. Specifically, we focus on a class of incomplete verifiers using efficient bound propagation operations, referred to as linear relaxation based perturbation analysis (LiRPA) algorithms (Xu et al., 2020). Representative algorithms in this class include convex outer adversarial polytope , CROWN  and DeepPoly (Singh et al., 2019b). LiRPA algorithms exhibit high parallelism as the bound propagation process is similar to forward or backward propagation of NNs, which can fully exploit machine learning accelerators (e.g., GPUs and TPUs).\n\nAlthough LiRPA bounds are very efficient for incomplete verification, especially in training certified adversarial defenses Wang et al., 2018a;, they are generally considered too loose to be useful compared to LPs in the complete verification settings with BaB. As we will demonstrate later, using LiRPA bounds naively in BaB cannot even guarantee the completeness when splitting ReLU nodes, and thus we need additional measures to make them useful for complete verification. In fact, LiRPA methods have been used to get upper and lower bounds for each ReLU neuron in constructing tighter LPs (Bunel et al., 2018;Lu & Kumar, 2020). It was also used in (Wang et al., 2018c) for verifying small-scale problems with relatively low dimensional input domains using input splits, but splitting the input space can be quite ineffective (Bunel et al., 2018) and is unable to scale to high dimensional input case like CIFAR-10. Except one concurrent work (Bunel et al., 2020a), most complete verifiers are based on relatively expensive solvers like LP and cannot fully take benefit from massively parallel hardware (e.g., GPUs) to obtain tight bounds for accelerating large-scale complete verification problems. Our main contributions are:\n\n\u2022 We show that LiRPA bounds, when improved with fast gradient optimizers, can potentially outperform bounds obtained by LP verifiers. This is because LiRPA allows joint optimization of both intermediate layer bounds of ReLU neurons (which determine the tightness of relaxation) and output bounds, while LP can only optimize output bounds with fixed relaxations on ReLU neurons. \u2022 We show that BaB purely using LiRPA bounds is insufficient for complete verification due to the lack of feasibility checking for ReLU node splits. To address this issue, we design our algorithm to only invoke LP when absolutely necessary and exploits hardware parallelism when possible. \u2022 To fully exploit the hardware parallelism on the machine learning accelerators, we use a batch splitting approach that splits on multiple neurons simultaneously, further improving our efficiency.\n\n\u2022 On a few standard and representative benchmarks, our proposed NN verification framework can outperform previous baselines significantly, with a speedup of around 30X compared to basic BaB+LP baselines, and up to 5X compared to recent state-of-the-art complete verifiers.\n\n\nBACKGROUND\n\n\nFormal definition of Neural network (NN) verification\n\nNotations of NN. For illustration, we define an L-layer feedforward NN f :\nR |x| \u2192 R with L weights W (i) (i \u2208 {1, \u00b7 \u00b7 \u00b7 , L}) recursively as h (i) (x) = W (i) g (i\u22121) (x), hidden layer g (i) (x) = ReLU(h (i) (x)), input layer g (0) (x) = x, and final output f (x) = h (L) (x)\n. For simplicity we ignore biases. We sometimes omit x and use h (i) j to represent the pre-activation of the j-th ReLU neuron in i-th layer for x \u2208 C, and we use g (i) j to represent the post-activation value. We focus on verifying ReLU based NNs, but our method is generalizable to other activation functions supported by LiRPA.\n\nNN Verification Problem. Given an input x, its bounded input domain C, and a feedforward NN f (\u00b7), the aim of formal verification is to prove or disprove certain properties P of NN outputs. Since most properties studied in previous works can be expressed as a Boolean expression over a linear equation of network output, where the linear property can be merged into the last layer weights of a NN, the ultimate goal of complete verification reduces to prove or disprove:\n\u2200x \u2208 C, f (x) \u2265 0(1)\nOne way to prove Eq. 1 is to solve min x\u2208C f (x). Due to the non-convexity of NNs, finding the exact minimum of f (x) over the input domain C is challenging as the optimization process is generally NP-complete (Katz et al., 2017). However, in practice, a sound approximation of the lower bound for f (x), denoted as f , can be more easily obtained and is sufficient to verify the property. Thus, a good verification strategy to obtain a tight approximation f can save several magnitudes of time cost. Note that the approximation must be sound, i.e., \u2200x \u2208 C, f \u2264 f (x) such that proving f \u2265 0 is sufficient to prove the desired property f (x) \u2265 0 given x \u2208 C.\n\n\nThe Branch and Bound (BaB) framework for Neural Network Verification\n\nBranch and Bound (BaB), an effective strategy in solving traditional combinatorial optimization problems, has been customized and widely adopted for NN verification (Bunel et al., 2018;2020b). Specifically, BaB based verification framework is a recursive process, consisting of two main steps: branching and bounding. For branching, BaB based methods will divide the bounded input domain C into sub-domains {C i |C = i C i }, each defined as a new independent verification problem. For instance, it can split a ReLU unit g\n(k) j = ReLU(h (k)\nj ) to be negative and positive cases as\nC 0 = C \u2229 h (k) j \u2265 0 and C 1 = C \u2229 h (k) j\n< 0 for a ReLU-based network; for each sub-domain C i , BaB based methods perform bounding to obtain a relaxed but sound lower bound f Ci . A tightened global lower bound over C can then be obtained by taking the minimum values of the sub-domain lower bounds from all the sub-domains: f = min i f Ci . Branching and bounding will be performed recursively to tighten the approximated global lower bound over C until either (1) the global lower bound f becomes larger than 0 and prove the property or (2) a violation (e.g., adversarial example) is located in a sub-domain to disprove the property. Essentially, we build a search tree where each leaf is a sub-domain, and the property P can be proven only when it is valid on all leaves.\n\n\nSoundness of BaB\n\nWe say the verification process is sound if we can always trust the \"yes\" (P is verified) answer given by the verifier. It is straightforward to see that the whole BaB based verification process is sound as long as the bounding method used for each sub-domain C i is sound.\n\n\nCompleteness of BaB\n\nThe completeness of the BaB-based NN verification process, which was usually assumed true in some previous works (Bunel et al., 2020b;, in fact, is not always true even if all possible sub-domains are considered with a sound bounding method. Additional requirements for the bounding method are required -we point out that a key factor for completeness involves feasibility checking in the bounding method which we will discuss in Section 3.2.\n\nBranching in BaB Since branching step determines the shape of the search tree, the main challenge is to efficiently choose a good leaf to split, which can significantly reduce the total number of branches and running time. In this work we focus on branching on activation (ReLU) nodes. BaBSR (Bunel et al., 2018) includes a simple branching heuristic which assigns each ReLU node a score to estimate the improvement for tightening f by splitting it, and splits the node with the highest score.\n\nBounding with Linear Programming (LP) A typical bounding method used in BaB based verification is the Linear Programming bounding procedure (sometimes simply referred to as \"LP\" or \"LP verifier\" in our paper). Specifically, we transform the original verification problem into a linear programming problem by relaxing every activation unit as a convex (linear) domain (Ehlers, 2017) and then get the lower bound f Ci with a linear solver given domain C i . For instance, as shown in Figure 1a, g\n(i) j = ReLU(h (i) j )\ncan be linearly relaxed with the following 3 constraints: (1) g (i)\nj \u2265 h (i) j ; (2)g (i) j \u2265 0; (3) g (i) j \u2264 u (i) j u (i) j \u2212l (i) j (h (i) j \u2212 l (i) j ).\nNote that the lower bound l  \n\n\nLinear Relaxation based Perturbation Analysis (LiRPA)\n\nBound propagation in LiRPA Linear relaxation based perturbation analysis (LiRPA) 1 , which will be used as our bound procedure in BaB, aims to find linear upper and lower bounds of NN output Figure 1: Relaxations of a ReLU: (a) \"triangle\" relaxation in LP; (b)(c) No relaxation when u (i)\nl (i) j u (i) j x (i) j h (i) j l (i) j u (i) j h (i) j l (i) j u (i) j h (i) j l (i) j u (i) j h (i) j l (i) j u (i) j h (i) j (a) (b) (c) (d)j \u2264 0 (always inactive) or l (i) j \u2265 0 (always active); (d) linear relaxation in LiRPA when l (i) j < 0, u (i) j > 0 (unstable). w.r.t input x \u2208 C: Ax + b \u2264 f (x) \u2264 Ax + b, x \u2208 C(2)\nA lower bound f can then be simply obtained by taking the lower bound of the linear equation Ax + b w.r.t input x \u2208 C, which can be obtained via H\u00f6lder's inequation when C is a p norm ball.\n\nTo get the coefficients A, A, b, b, LiRPA propagates bounds of f (x) as a linear function to the output of each layer, in a backward manner. At the output layer h (L) (x) we simply have:\nIh (L) (x) \u2264 f (x) \u2264 Ih (L) (x), x \u2208 C(3)\nThen, the next step is to backward propagate the identity linear relationship through a linear layer\nh (L) (x) = W (L) g (L\u22121) (x) to get the linear bounds of f (x) w.r.t g (L\u22121) : W (L) g (L\u22121) (x) \u2264 f (x) \u2264 W (L) g (L\u22121) (x), x \u2208 C(4)\nTo get the linear relationship of h (L\u22121) w.r.t f (x), we need to backward propagate through ReLU layer g (L\u22121) (x) = ReLU(h (L\u22121) (x)). Since it is nonlinear, we perform linear relaxations. For illustration, considering the j-th ReLU neuron at i-th layer, g\n(i) j (x) = ReLU(h (i)\nj (x)), we can linearly upper and lower bound it by a\n(i) j h (i) j (x) + b (i) j \u2264 g (i) j (x) \u2264 a (i) j h (i) j (x) + b (i) j , where a (i) j , a (i) j , b (i) j , b (i) j are: \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 a (i) j = a (i) j = 0, b (i) j = b (i) j = 0 u (i) j \u2264 0 (always inactive for x \u2208 C) a (i) j = a (i) j = 1, b (i) j = b (i) j = 0 l (i) j \u2265 0 (always active for x \u2208 C) a (i) j = \u03b1 (i) j , a (i) j = u (i) j u (i) j \u2212l (i) j , b (i) j = 0, b (i) j = \u2212 u (i) j l (i) j u (i) j \u2212l (i) j l (i) j < 0, u (i) j > 0 (unstable for x \u2208 C) (5) Here l (i) j \u2264 h (i) j (x) \u2264 u (i) j are intermediate pre-activation bounds for x \u2208 C, and \u03b1 (i)\nj is an arbitrary value between 0 and 1. The pre-activation bounds l\n(i) j and u (i) j can be computed by treating h (i) j (x)\nas the output neuron with LiRPA. Figure 1(b,c,d) illustrate the relaxation for each state of ReLU neuron. With these linear relaxations, we can get the linear equation of h (L\u22121) w.r.t output f (x):\nW (L) D (L\u22121) \u03b1 h (L\u22121) (x) + b (L) \u2264 f (x) \u2264 W (L) D (L\u22121) \u03b1 h (L\u22121) (x) + b (L) , x \u2208 C D (L) \u03b1,(j,j) = a (L) j , W (L) j \u2265 0 a (L) j , W (L) j < 0 , b (L) = b (L) W (L) , where b (L) j = b (L) j , W (L) j \u2265 0 b (L) j , W (L) j < 0(6)\nThe diagonal matrices D (L\u22121) Importantly, D (L\u22121) \u03b1 has free variables \u03b1 (i) j \u2208 [0, 1] which do not affect correctness of the bounds. We can continue backward propagating these bounds layer by layer (e.g.,\ng (L\u22122) (x), h (L\u22122) (x), etc) until reaching g (0) (x) = x, getting the eventual linear equations of f (x) in terms of input x: L(x, \u03b1) \u2264 f (x) \u2264 U(x, \u03b1), \u2200x \u2208 C, where L(x, \u03b1) = W (L) D (L\u22121) \u03b1 \u00b7 \u00b7 \u00b7 D (1) \u03b1 W (1) x + b, U(x, \u03b1) = W (L) D (L\u22121) \u03b1 \u00b7 \u00b7 \u00b7 D (1) \u03b1 W (1) x + b(7)\nHere \u03b1 denotes \u03b1 (i) j for all unstable ReLU neurons in NN. The obtained bounds (L(x, \u03b1), U(x, \u03b1)) of f (x) are linear functions in terms of x. Beyond the simple feedforward NN presented here, LiRPA can support more complicated NN architectures like DenseNet and Transformers by computing L and U automatically and efficiently on general computational graphs (Xu et al., 2020). (2) \u210e 2\n\n(2) \u210e 3\n\n(2) \u210e 1\n\n(1) \u210e 2\n\n(1) \u210e 3\n\n(1) \u210e 1\n\n(2) \u210e 2\n\n(2) \u210e 3\n\n(2)\n\nSplit \u210e 2\n\n(2)\n\nPick \u210e 1\n\n(2)\n\nSplit \u210e 1\n\n(2)\n\nPick \u210e 2\n\n(2)\n= \u22120.15 = 0.17 \u2713 = 0.25 \u2713 = 0.31 \u2713 ReLU Lower bound by Optimized LiRPA\nExample: Optimized LiRPA for \u210e 1\n\n(2) tigher 1 (2) and 1 (2) make tighter relaxation Example: BaB to find tighter lower bound 1 (2) 1\n\n(2) 1\n\n(2) 1\n\n(2) Soundness of LiRPA The above backward bound propagation process guarantees that L(x, \u03b1) and U(x, \u03b1) soundly bound f (x) for all x \u2208 C. Detailed proofs can be found in Singh et al., 2019b) for feedforward NNs and (Xu et al., 2020) for general networks.\n\n\nPROPOSED ALGORITHM\n\nOverview In this section, we will first introduce our proposed efficient optimization of LiRPA bounds on GPUs that can allow us to achieve tight approximation on par with LP or even tighter for some cases but in a much faster manner. In Fig. 2, we provide a two-layer NN example to illustrate how our optimized LiRPA can improve the performance of BaB verification. In Section 3.2, we then show that feasibility checking is important to guarantee the completeness of BaB, and BaB using LiRPA without feasibility checking will end up to be incomplete. To ensure completeness, we design our algorithm with minimal usage of LP for checking feasibility of splits. Finally, we propose a batch split design by solving a batch of sub-domains in a massively parallel manner on GPUs to fully leverage the benefits of cheap and parallelizable LiRPA. We further improve BaBSR in a parallel fashion for branching and we summarize the detailed proposed algorithm in Section 3.4.\n\n\nOptimized LiRPA for Complete Verification\n\nConcrete outer bounds with optimizable parameters We propose to use LiRPA as the bounding step in BaB. A pair of sound and concrete lower bound and upper bound (f , f ) to f (x) can be obtained according to Eq. 7 given fixed \u03b1 = \u03b1 0 :\nf (\u03b10) = min x\u2208C L(x, \u03b10), f (\u03b10) = max x\u2208C U(x, \u03b10)(8)\nBecause L, U are linear functions w.r.t x when \u03b1 0 is fixed, it is easy to solve Eq. 8 using H\u00f6lder's inequality when C is a p norm ball (Xu et al., 2020). In incomplete verification settings, \u03b1 can be set via certain heuristics . Salman et al. (2019) showed that, the variable \u03b1 is equivalent to dual variables in the LP relaxed verification problem . Thus, an optimal selection of \u03b1 given the same pre-activation bounds l  ), and solve the LP to obtain bounds at output layer. The main reason for using LP is that it typically produces much tighter bounds than LiRPA when \u03b1 is not optimized. To replace LP with fast and accelerator-friendly LiRPA algorithms in the complete verification setting, we use optimized LiRPA to produce tighter bounds:\nf = min \u03b1 min x\u2208C L(x, \u03b1), f = max \u03b1 max x\u2208C U(x, \u03b1), \u03b1 (i) j \u2208 [0, 1](9)\nThe inner minimization or maximization has closed form solutions (Xu et al., 2020) based on H\u00f6lder's inequality, so we only need to optimize on \u03b1. Since we use a differentiable framework (Xu et al., 2020) to compute the LiRPA bound functions L and U, the gradients \u2202L \u2202\u03b1 and \u2202U \u2202\u03b1 can be obtained easily. Optimization over \u03b1 can be done via projected gradient descent (each coordinate of \u03b1 is constrained in [0, 1]). Since the gradient computation and optimization are done on GPUs, the bounding process is still very fast and can be one or two magnitudes faster than solving an LP.\n\nOptimized LiRPA bounds can be tighter than LP Solving Eq. 9 using gradient descent with a small number of iterations cannot guarantee to converge to the global optima, so it seems the bounds must be looser than LP. Counter-intuitively, by optimizing \u03b1, we can potentially obtain tighter bounds than LP. When a LP relaxation is constructed, preactivation bounds l\n(i) j , u (i)\nj must be fixed to construct the \"triangle\" relaxation for the j-th ReLU neuron in layer i. During the LP optimization process, only the output bounds are optimized; l j are obtained from unoptimized LiRPA bounds or even weaker methods such as interval arithmetic, so the corresponding LP is similar to the red line in Figure 3; instead, our optimized LiRPA bounds can produce quite competitive bounds, and also exploit parallel acceleration from machine learning accelerators. The tightness and efficiency of our optimized LiRPA procedure lead to huge improvements in verification time compared to existing baselines.\n\n\nReLU Split Constraints\n\nIn the BaB process, when a ReLU neuron h \n\nj and optimal \u03b1. After splits, LiRPA cannot check certain constraints where LP is capable to, as we will discuss in the next section.\n\n\nCompleteness with Minimal Usage of LP Bounding Procedure\n\nEven though our optimized LiRPA can bring us huge speed improvement over LP for BaB based verification, we observe that it may end up to be incomplete due to the lack of feasibility checking: it cannot detect some conflicting settings of ReLU splits. We state such an observation in Theorem 3.1:\n\nTheorem 3.1 (Incompleteness without feasibility checking) When using LiRPA variants described in Section 2.3 as the bounding procedure, BaB based verification is incomplete.\n\nWe prove the theorem by giving a counter-example in Appendix A.1 where all ReLU neurons are split and thus LiRPA runs on a linear network for each sub-domain. As a result, LiRPA can still be indecisive for the verification problem. The main reason is that LiRPA variants will lose the feasibility information encoded by the sub-domain constraints. For illustration, consider a sub-Algorithm 1 Parallel BaB with optimized LiRPA bounding (we highlight the differences between our algorithm and regular BaB (Bunel et al., 2018) in blue. Comments are in brown.) 1: Inputs: f , C, n (batch size), \u03b7 (threshold to switch to LP) 2:\n(f , f ) \u2190 optimized LiRPA(f, [C]) 3: P \u2190 (f , f , C)\nP is the set of all unverified sub-domains 4: while f < 0 and f \u2265 0 do 5:\n\n(C1, . . . , Cn) \u2190 batch pick out(P, n) Pick sub-domains to split and removed them from P 6:\n\nC l 1 , C u 1 , . . . , C l n , C u n \u2190 batch split(C1, . . . , Cn) Each Ci splits into two sub-domains C l i and C u\ni 7: f C l 1 , f C l 1 , f C u 1 , f C u 1 , . . . , f C l n , f C l n , f C u n , f C u n \u2190 optimized LiRPA(f, C l 1 , C u 1 , .\n. . , C l n , C u n ) Compute lower and upper bounds using LiRPA for each sub-domain on GPUs in a batch 8:\nP \u2190 P Domain Filter [f C l 1 , f C l 1 , C l 1 ], [f C u 1 , f C u 1 , C u 1 ], . . . , [f C l n , f C 1 n , C l n ], [f C u n , f C u n , C u n ]\nFilter out verified sub-domains, insert the left domains back to P 9:\n\nf \u2190 min{f\nC i | (f C i\n, Ci) \u2208 P}, i = 1, . . . , n To ease notation, Ci here indicates both C u i and C l i 10:\nf \u2190 min{f C i | (f C i , Ci) \u2208 P}, i = 1, . . . , n 11:\nif length(P) > \u03b7 then Fall back to LP for completeness\n12: f C l 1 , f C l 1 , f C u 1 , f C u 1 , . . . , f C l n , f C l n , f C u n , f C u n , \u2190 compute bound LP(f, C l 1 , C u 1 , . . . , C l n , C u n ) 13: P \u2190 P Domain Filter [f C l 1 , f C l 1 , C l 1 ], [f C u 1 , f C u 1 , C u 1 ], . . . , [f C l n , f C 1 n , C l n ], [f C u n , f C u n , C u n ] 14: Outputs: f , f domain C i = C \u2229 (h (i1) j1 < 0) \u2229 (h (i2) j2 \u2265 0), LiRPA will force g (i1) j1 (x) = 0 (inactive ReLU, a zero function) and g (i2) j2 (x) = h (i2)\nj2 (x) (active ReLU, an identity function) respectively and propagate these bounds to get the approximated lower bound f Ci . However, the split feasibility constraint\n(h (i1) j1 < 0) \u2229 (h (i2)\nj2 \u2265 0) is ignored, so two conflict splits may be conducted (e.g., when h (i1)\nj1 < 0, h (i2)\nj2 cannot be \u2265 0). On the contrary, LP can fully preserve such feasibility information due to the linear solver involved and detect the infeasible sub-domains. Then, in Theorem 3.2 we show that the minimal usage of feasibility checking with LP can guarantee the completeness of BaB with LiRPA.\n\nTheorem 3.2 (Minimal feasibility checking for completeness) When using LiRPA variants described in Section 2.3 as the bounding procedure, BaB based verification is complete if all infeasible leaf sub-domains (i.e., sub-domains cannot be further split) are detected by linear programming.\n\nWe prove the theorem in Appendix A.2, where we show that by checking the feasibility of splits with LP, we can eliminate the cases where incompatible splits are chosen in the LiRPA BaB process. Since LP is slow while LiRPA is highly efficient, we propose to only use LP when the LiRPA based bounding process is stuck, either (1) when partitioning and bounding new sub-domains with LiRPA cannot further improve the bounds, or (2) when all unstable neurons have been split. In this way, the infeasible sub-domains can be eventually detected by occasional usage of LP while the advantage of massive parallel LiRPA on GPUs is fully enjoyed. We will describe our full algorithm in Sec. 3.4.\n\n\nBatch Splits\n\nIn state-of-the-art BaB methods (Bunel et al., 2020b;Lu & Kumar, 2020), only one sub-domain can be selected to split during each branching step. Benefiting from our design of using cheap and GPU-friendly LiRPA bounds, we can select a batch of sub-domains to split and propagate LiRPA bounds for them in a batch. Such a batching splitting design can greatly improve hardware efficiency on GPUs. Given a batch size n that allows us to fully use the memory of one GPU device, we can obtain n bounds simultaneously. This can help us grow the search tree on a single leaf by a depth of log 2 n, or split n/2 leaf nodes at the same time, accelerating by up to n times.\n\n\nOur Complete Verification Framework\n\nOur LiRPA based complete verification framework is presented in Alg. 1. The algorithm takes a target NN function f and a domain C as inputs. We run optimized LiRPA to get initial bounds (f , f ) for x \u2208 C (Line 2). Then we utilize the power of GPUs to split in parallel and maintain a global set P storing all the sub-domains which cannot be verified with optimized LiRPA (Line 5-10). Specifically, batch pick out improves BaBSR (Bunel et al., 2018) in a parallel manner to select n sub-domains in P and determine the corresponding ReLU neuron to split for each of them. If the length of P is less than n, then we reduce n to the length of P. batch split splits each selected C i to two sub-domains C l i and C u i by forcing the selected unstable ReLU neuron to be positive and negative, respectively. optimize LiRPA runs optimized LiRPA in parallel as a batch and returns the lower and upper bounds for n selected sub-domains simultaneously. Domain Filter filters out verified sub-domains (proved with f Ci \u2265 0) and we insert the remaining ones to P. The loop breaks if the property is proved (f \u2265 0) or a counter-example is found in any sub-domain (f < 0).\n\nTo avoid excessive splits, we set the maximum length of the sub-domains to \u03b7 (Line 12). Once the length of P reaches this threshold, compute bound LP will be called. It solves these \u03b7 sub-domains by LP (one by one in a loop, or in parallel if using multiple CPUs is allowed) with optimized LiRPA computed intermediate layer bounds. If a sub-domain C i \u2208 P (which previously cannot be verified by LiRPA) is proved or detected to be infeasible by LP, as an effective heuristic, we will backtrack and prioritize to check its parent node with LP. If the parent sub-domain is also proved or infeasible, we can prune all its child nodes to greatly reduce the size of the search tree.\n\nCompleteness of our framework Our algorithm is complete, because we follow Theorem 3.2 and check feasibility of all split sub-domains that have deep BaB search tree depth (length of P reaches threshold \u03b7), forming a superset of the worst case where all ReLU neurons are split.\n\n\nEXPERIMENTS\n\nIn this section, we compare our verifier against the state-of-the-art ones to illustrate the effectiveness of our proposed framework. Overall, our verifier is about 4X, 7X and 20X faster than the best LP-based verifier (Lu & Kumar, 2020) on the Base, Wide and Deep models, respectively.\n\nSetup We follow the most challenging experimental setup used in the state-of-the-art verifiers GNN-ONLINE (Lu & Kumar, 2020) and BABSR (Bunel et al., 2020b). Specifically, we evaluate on CIFAR10 dataset on three NNs: Base, Wide and Deep. The dataset is categorized into three difficulty levels: Easy, Medium, and Hard, which is generated according to the performance of BaBSR. The verification task is defined as given a l \u221e norm perturbation less than , the classifier will not predict a specific (predefined) wrong label for each image x (see Appendix B). We set batch size n = 64 and threshold \u03b7 = 12000. More details on experimental setup are provided in Appendix B. Our code is available at https://github.com/kaidixu/LiRPA Verify.\n\nComparisons against state-of-the-art verifiers We include five different methods for comparison: (1) BaBSR (Bunel et al., 2020b), a BaB and LP based verifier using a simple ReLU split heuristic;\n\n(2) MIPPLANET (Ehlers, 2017), a customized MIP solver for NN verification where unstable ReLU neurons are randomly selected for split; (3) GNN (Lu & Kumar, 2020) and (4) GNN-ONLINE (Lu & Kumar, 2020) are BaB and LP based verifiers using a learned graph neural network (GNN) to guide the ReLU split. (5) PROXIMAL BABSR (Bunel et al., 2020a) is a very recently proposed verification framework based on Lagrangian decomposition which also supports GPU acceleration without solving LPs. 2 All methods use 1 CPU with 1 GPU. The timeout threshold is 3,600 seconds. For the Base model, Table 1 shows that we are around 10X faster than baseline BaBSR and around 4X faster than GNN split baselines. The accumulative solved properties with increasing runtime are shown in Figure 4. In all our experiments, we use the basic heuristic in BaBSR for branching and do not use GNNs, so our speedup comes purely from the faster LiRPA based bounding procedure. We are also competitive against Lagrangian decomposition on GPUs, especially on hard properties.   Performance on Larger Models In Table 2, we show that our verifier is more scalable on larger (wider or deeper) NNs compared to other state-of-the-art verifiers. Our method enjoys efficient GPU acceleration particularly on deep models and can achieve 30X speedup compared to BABSR, and we are also significantly faster than Lagrangian decomposition based GPU verifier (Proximal BaBSR). When compared to the state-of-the-art LP based BaB, GNN-ONLINE, our method can save 20X running time. In Appendix C, we analyze the effectiveness of optimized LiRPA and batch splits separately, and find that optimized LiRPA is crucial for NN verification. Performance comparisons of our proposed framework on CPU cores without GPU acceleration are included in Appendix D.\n\n\nRELATED WORK\n\nComplete verifiers Early complete verifiers rely on satisfiability modulo theory (SMT) (Katz et al., 2017;Huang et al., 2017;Ehlers, 2017) and mixed integer linear programming (MILP) (Tjeng et al., 2019a;Dutta et al., 2018), and they typically do not scale well. Higher order logic provers such as proof assistant (Bentkamp et al., 2018) can also be potentially used for NN verification, but their scalability to the NN setting has not been demonstrated. Recently, Bunel et al. (2018) unified many approaches used in various complete verifiers into a BaB framework. An LP based bounding procedure is used in most of the existing BaB framework (Bunel et al., 2018;Wang et al., 2018c;Royo et al., 2019;Lu & Kumar, 2020). For branching, two categories of branching strategies were proposed: (1) input node branching (Wang et al., 2018c;Bunel et al., 2020b;Royo et al., 2019;Anderson et al., 2019) where input features are divided into sub-domains, and (2) activation node (especially, ReLU) branching (Katz et al., 2017;Bunel et al., 2018;Wang et al., 2018b;Ehlers, 2017;Lu & Kumar, 2020) where hidden layer activations are split into sub-domains. Bunel et al. (2018) found that input node branching cost is exponential to input dimension. Thus, many state-of-the-art verifiers use activation node branching instead, focusing on heuristics to select good nodes to split. BaBSR (Bunel et al., 2018) prioritizes ReLUs for splitting based on their pre-activation bounds; Lu & Kumar (2020) used a graph neural network (GNN) to learn good splitting heuristics. Our work focuses on improving bounding and can use better branching heuristics to achieve further speedup.\n\nTwo mostly relevant concurrent works using GPUs for accelerating NN verification are: (1) GPUPoly (M\u00fcller et al., 2020), an extension of DeepPoly on CUDA, is still an incomplete verifier. Also, it is implemented in CUDA C++, requiring manual effort for customization and gradient computation, so it is not easy to get the gradients for optimizing bounds as we have done in Section 3.1.\n\n(2) Lagrangian Decomposition (Bunel et al., 2020a) is a GPU-accelerated BaB based complete verifier that iteratively tightens the bounds based on a Lagrangian decomposition optimization formulation and does not reply on LP. However, it solves a much more complicated optimization problem than LiRPA, and typically requires hundreds of iterations to converge for a single sub-domain.\n\nIncomplete verifiers Many incomplete verification methods rely on convex relaxations of NN, replacing nonlinear activations like ReLUs with linear constraints Wang et al., 2018b;Singh et al., 2018a;b;a) or semidefinite constraints (Raghunathan et al., 2018;Dathathri et al., 2020). Tightening the relaxation for incomplete verification was discussed in (Dvijotham et al., 2018;Singh et al., 2019a;Lyu et al., 2019;Tjandraatmadja et al., 2020). Typically, tight relaxations require more computation and memory in general. We refer the readers to (Salman et al., 2019) for a comprehensive survey. Recently, Xu et al. (2020) categorized the family of linear relaxation based incomplete verifiers into LiRPA framework, allowing efficient implementation on machine learning accelerators. Our work uses LiRPA as the bounding procedure for complete verification and exploits its computational efficiency to accelerate complete verification, and our main contribution is to show that fast but weak incomplete verifiers can be used as the main driver for complete verification when strategically applied.\n\n\nCONCLUSION\n\nWe use a LiRPA based incomplete NN verifier to accelerate the bounding procedure in branch and bound (BaB) for complete NN verification on massively parallel accelerators. We use a fast gradient based procedure to tighten LiRPA bounds. We study the completeness of BaB with LiRPA, and show up to 5X speedup compared to state-of-the-art verifiers across multiple models and properties.\n\n\nA PROOFS\n\nA.1 Proof of Theorem 3.1\n\nWe prove Theorem 3.1 by providing a simple counterexample, and we illustrate the necessity of feasibility checking for the completeness of BaB based verification. Consider an NN with only two ReLU units g\n(2) 1 = ReLU(h(2)\n1 ) and g\n\n(2)\n2 = ReLU(h(2)\n2 ) where they share the same one dimension input h\n(2) 1 = h (2) 2 = x. The final output function of NN is defined as f = g (2) 1 \u2212 g(2)\n2 . As a verification problem, we want to verify the property f \u2265 0 where x = [\u22121, 1]. Since hidden nodes h 1 and h 2 are exactly the same, the ground-truth output range is f * (x) \u2208 [0, 0]. A complete BaB based verifier is expected to obtain that optimal bound and prove the property after splitting h 1 and h 2 together while BaB with only LiPRA cannot guarantee that completeness. Specifically, BaB with only LiRPA will split the original domain x \u2208 [\u22121, 1] into four sub-domains and approximate the bound with LiRPA respectively:\n(1) (feasible) sub-domain x \u2208 [\u22121, 1], h(2)1 \u2265 0, h (2) 2 \u2265 0 with output f = [x, x] \u2212 [x, x] \u2208 [0, 0] (2) (feasible) sub-domain x \u2208 [\u22121, 1], h (2) 1 < 0, h (2) 2 < 0 with output f = [0, 0] \u2212 [0, 0] \u2208 [0, 0] (3) (infeasible) sub-domain x \u2208 [\u22121, 1], h (2) 1 < 0, h (2) 2 \u2265 0 with output f = [0, 0] \u2212 [x, x] \u2208 [\u22121, 1] (4) (infeasible) sub-domain x \u2208 [\u22121, 1], h (2) 1 \u2265 0, h (2) 2 < 0 with output f = [x, x] \u2212 [0, 0] \u2208 [\u22121, 1]\nOnly the first two split sub-domains are feasible and therefore the ground-truth lower bound 0 can be obtained by taking the minimum of the estimated bounds from sub-domains (1) and (2). However, pure LiRPA is not able to tell the infeasibility of sub-domains (3) and (4) and thus BaB with pure LiRPA will report the minimum \u22121 got from all these four sub-domains as the global lower bound for the original input domain, ending up not being able to verify the property, i.e., incomplete.\n\n\nA.2 Proof of Theorem 3.2\n\nWe prove Theorem 3.2 by considering the worst case where all unstable ReLU neurons are split.\n\nGiven a neural network function f with input domain C, assume there are N unstable ReLU neurons {g i = ReLU(h i )|i = 1, \u00b7 \u00b7 \u00b7 , N } in total. In the worst case, we have 2 N leaf sub-domains S = {C i |i = 1, \u00b7 \u00b7 \u00b7 , 2 N }, where each C i corresponds to one assignment of unstable ReLU neuron splits. For example, we can have\nC 1 = C \u2229 (h 1 \u2265 0) \u2229 (h 2 \u2265 0) \u2229 \u00b7 \u00b7 \u00b7 \u2229 (h N \u2265 0) C 2 = C \u2229 (h 1 < 0) \u2229 (h 2 \u2265 0) \u2229 \u00b7 \u00b7 \u00b7 \u2229 (h N \u2265 0) C 3 = C \u2229 (h 1 \u2265 0) \u2229 (h 2 < 0) \u2229 \u00b7 \u00b7 \u00b7 \u2229 (h N \u2265 0) C 4 = C \u2229 (h 1 < 0) \u2229 (h 2 < 0) \u2229 \u00b7 \u00b7 \u00b7 \u2229 (h N \u2265 0) \u00b7 \u00b7 \u00b7\nNote that by definition the original input domain C = \u222a C \u2208S C ; in other words, all the 2 N split sub-domains combined will be the same as the original input domain.\n\nNot all of the sub-domains are actually feasible, due to the consistency requirements between neurons. For example, in our proof in Section A.1, h\n\n1 and h\n\n2 cannot be both \u2265 0 or both < 0. We can divide the sub-domains S into two mutually exclusive sub-sets, S feas for all the feasible sub-domains, and S infeas for all the infeasible sub-domains. We have C = \u222a C \u2208S feas C since these infeasible sub-domains are empty sets.\n\nWe first show that linear programming (LP) can be used to effectively detect these infeasible subdomains. For some C \u2208 S infeas , because all the ReLU neurons are fixed to be positive or negative, no relaxation is needed and the network is essentially linear; thus, the input value of every hidden neuron h i can be written as a linear equation w.r.t. input x. We add all the Boolean predicates on h i to a LP problem as linear constraints w.r.t x. If this LP is feasible, then we can find some input x 0 that assigns compatible values to all h i ; otherwise, the LP is infeasible.\n\nDue to the lack of feasibility checking in LiRPA, the computed global lower (or upper) bounds from LiRPA is f LiRPA = min C \u2208S f C = min min C \u2208S feas f C , min C \u2208S infeas f C . With feasibility checking from LP, we can remove all infeasible sub-domains from this min such that they do not contribute to the global lower bound: f = min C \u2208S feas f C .\n\nTo prove the whole BaB verification is complete, it is sufficient to prove this lower bound f is the exact minimum of f bounded in C. Since any sub-domain C \u2208 S feas is a leaf sub-domain with no unstable ReLU neurons, the neural network bounded within C is a linear function. LiRPA can give an exact minimum of f within sub-domain C . Since C = \u222a C \u2208S feas C (in other words, S feas covers all the feasible sub-domains within C), the minimal value for all of them f = min C \u2208S feas f C forms the exact minimum of f within the input domain C. Thus, BaB with LiRPA based bounding procedure is complete when feasibility checking is applied.\n\n\nB EXPERIMENTAL SETUP\n\nWe use the same set of models and benchmark examples used in the state-of-the-art verifiers GNN-ONLINE (Lu & Kumar, 2020) and BABSR (Bunel et al., 2020b). Specifically, we evaluate on the most challenging CIFAR-10 dataset with the same standard robustly trained convolutional neural networks: Base, Wide, and Deep. These model structures are also used in (Lu & Kumar, 2020;Bunel et al., 2020a). The Base model contains 2 convolution layers with 8 and 16 filters as well as two linear layers with 100 and 10 hidden units, respectively. In total, the Base model has 3,172 ReLU activation units. The Wide model contains 2 convolution layers with 16 and 32 filters and two linear layers with 100 and 10 hidden units, respectively, which contains 6,244 ReLU activation units in total. The Deep model contains 4 convolution layers and all of them have 8 filters and two linear layers with 100 and 10 hidden units, respectively, with 3,756 ReLU activation units in total.\n\nGiven an correctly classified image x with label y c , and another wrong label y c = y c (pre-defined in this benchmark) and , the verifier needs to prove:\n(e (c) \u2212 e (c ) ) T f (x ) > 0 s.t \u2200x x \u2212 x \u221e \u2264 (10) where f (\u00b7)\nis the logit-layer output of a multi-class classifier, e (c) and e (c ) are one-hot encoding vectors for labels y c and y c . We want to verify that for a given , the trained classifier will not predict wrong label y c for image x. All properties including x, , and c are provided by (Lu & Kumar, 2020). Specifically, they categorize verification properties solved by BABSR within 800s as easy, between 800s and 2400s as medium and more than 2400s as hard.\n\nOur experiments are conducted on one Intel E5 CPU and one Nvidia GTX 1080 Ti GPU. The parallel batch size n is set to 64 and the \u03b7 is set to 12,000 due to GPU memory constraint. To make a fair comparison, we use one CPU core for all methods. Also, we use one GPU for GNN, GNN-ONLINE, PROXIMAL-BABSR and our method. When optimizing the LiRPA bounds, we apply 200 steps gradient decent for obtaining the initial f (Line 2 in Algorithm 1). After that, we use 10 steps gradient decent (Line 7) and early stop once f > 0 or f has no improvement.\n\n\nC ABLATION STUDY\n\nOur efficient framework leverages two powerful components: (1) optimized LiRPA bounds and (2) batch splits on GPUs. In this section, we conduct breakdown experiments to show how each individual technique can help with complete verification. As we can see in Table 3, using batched split with unoptimized LiRPA is not very successful and cannot beat BABSR. We observe that, without optimized LiRPA, the bounds are very loose and cannot quickly improve the global lower bound. In contrast, using optimized LiRPA bounds without batch splits (splitting a single node at a time and running a batch size of 1 on GPU) can still significantly speed up complete verification, around 3 \u223c 10X compared to BABSR. Finally, combining batch splits and optimized LiRPA allows us to gain up to 31X speedup compared to BABSR.\n\n\nD COMPLETE VERIFICATION WITH LiRPA ON CPU VS GPU\n\nFor a fair comparison, we only use one CPU core and one GPU (the same as GNN and GNN-ONLINE) in our experimental results in Section 4. In this section, we investigate the performance of our algorithm for the cases where one or multiple CPU cores are available without GPU acceleration. Note that existing baselines such as BABSR and MIPPLANET can only effectively utilize one CPU core subject to the Gurobi solver. GNN and GNN-ONLINE can utilize one GPU to run the GNN during branching while the rest of the verification processes all perform on one CPU core. In contrast, our method is much more flexible, and we are not limited by the number of CPU cores or GPUs. When running on multi-core CPUs, LiRPA can be automatically accelerated by the underlying linear algebra library (e.g., Intel MKL or OpenBLAS) since the main computation of LiRPA is just matrix multiplications.\n\nIn Figure 5, we show the performance of our algorithm on a single CPU core and multiple CPU cores (in blue), and compare it to our main results with one CPU core plus one GPU (in red). As we can see, the running time decreases when the number of CPU cores increases, but the speedup is not linear due to the limitation of the underlying linear algebra library and hardware. There is a big gap between the running time on 8 CPU cores and the time on one CPU core + one GPU, and the performance gap is more obvious on Wide and Deep models. Thus, the speedup of LiRPA computation on GPUs is significant. However, surprisingly, even when using only one CPU core, we are still significantly faster than baseline BABSR and also get very competitive performance when compared to GNN-ONLINE which needs one GPU additionally. This shows the efficiency of LiRPA based verification algorithms.  Figure 5: Running time of our method on the Base, Wide, and Deep networks when using 1, 2, 4 and 8 CPU cores without a GPU (blue), and our method using 1 CPU core + 1 GPU (red) and a strong baseline method, GNN-ONLINE (green). We report the baseline BaBSR verification time in captions because they are out of range on the figures.\n\n\nrequired in the LP construction given C i . They are typically computed by the existing cheap bounding methods like LiRPA variants with low cost. The tighter the intermediate bounds (l are, the tighter f approximated by LP is.\n\n\nreflects the linear relaxations and also considers the signs in W (L) to maintain the lower and upper bound. The definitions for j-th diagonal element D (L) \u03b1,(j,j) and bias b (L) are similar, with the conditions for checking the signs of W\n\nFigure 2 :\n2Illustration of our optimized LiRPA bounds and the BaB process. Given a two-layer neural network, we aim to verify output f (x) \u2265 0. Optimized LiRPA chooses optimized slopes for ReLU lower bounds, allowing tightening the intermediate layer bounds l improve f and verify all sub-domains (f \u2265 0 for all cases).\n\n\nin fact, lead to the the same optimal solution for f and f as in LP. Previous complete verifiers typically use LiRPA variants to obtain intermediate layer bounds l construct a LP problem (e.g., Bunel et al. (2018); Lu & Kumar (2020) used convex outer adversarial polytope\n\nFigure 3 :\n3unchanged. However, in the LiRPA formulation, L(x, \u03b1) and U(x, \u03b1) are complex functions of \u03b1: since preprevious layers. Thus, the gradients \u2202L \u2202\u03b1 and \u2202U \u2202\u03b1 can tighten output layer bounds f and f indirectly by tightening intermediate layer bounds l optimization variables makes the problem non-linear. This is the key to our success of applying LiRPA based bounds for the complete verification setting, where tighter bounds are essential. Optimized LiRPA bound (from 0 to 200 iterations) vs LP bounds.InFigure 3, we illustrate our optimized LiRPA bounds and the LP solution. Initially, we use LiRPA with \u03b1 set via a fast heuristic to compute intermediate layer bounds l use them to build a relaxed LP problem. The solution to this initial LP problem is the red line. This LP solution is much tighter than the LiRPA solution with a heuristically set \u03b1, shown as the left-most point of the blue line. Then, we optimize \u03b1 with gradient decent, and LiRPA quickly outperforms this initial LP solution. The optimized \u03b1 also produces tighter intermediate layer bounds, l is the light blue line. This new LP only produces a slightly tighter bound than LiRPA with optimized \u03b1. In most previous works on complete verification, l\n\n\n0 in the bound propagation process. This allows LiRPA to tighten bounds because a split ReLU neuron essentially becomes linear, reducing relaxation errors. However, when splits are added, LiRPA and LP are not equivalent even under fixed l (i) j , u\n\nFigure 4 :\n4Cactus plots for our method and other baselines in Easy, Medium and Hard properties for the base model. We plot the percentage of solved properties with growing running time.\n\nTable 1 :\n1Performance of various methods on the Base (smallest) model. We compare each method's avg.solving time, the avg. number of branches required, and the percentage of timed out properties.Easy \nMedium \nHard \n\nMethod \ntime(s) branches %timeout time(s) branches %timeout time(s) branches %timeout \n\nBABSR \n522.48 \n585.27 \n0.0 \n1335.40 1471.55 \n0.0 \n3017.11 2350.80 \n40.2 \nMIPPLANET \n1462.24 \n-\n16.5 \n1912.25 \n-\n43.5 \n2172.23 \n-\n46.2 \nGNN \n312.93 \n301.72 \n0.0 \n625.12 \n547.40 \n0.9 \n1017.35 1059.42 \n18.3 \nGNN-ONLINE \n207.43 \n269.81 \n0.0 \n613.15 \n542.51 \n0.4 \n844.35 \n959.40 \n15.8 \nPROXIMAL BABSR 2 \n73.61 \n1377.67 \n0.4 \n129.54 3269.535 \n0.3 \n1126.86 12036.00 \n21.9 \nOURS \n56.49 \n6493.87 \n0.0 \n124.38 \n7570.41 \n0.8 \n267.35 18241.39 \n11.7 \n\n\nTable 2 :\n2Performance comparison on larger models (Wide and Deep) in similar metrics as inTable 1.Wide \nDeep \nMethod \ntime \nbranches %timeout \ntime \nbranches %timeout \nBABSR \n2305.65 \n646.22 \n50.3 \n2455.17 \n299.04 \n54.0 \nMIPPLANET \n5712.40 \n-\n74.3 \n5122.54 \n-\n60.8 \nGNN \n1491.52 \n352.41 \n20.1 \n1587.63 \n169.82 \n20.8 \nGNN-ONLINE \n1343.03 \n338.85 \n18.9 \n1569.71 \n166.82 \n18.4 \nPROXIMAL BABSR 2 891.16 \n3343.60 \n18.0 \n384.98 \n1655.52 \n6.8 \nOURS \n181.30 15795.45 \n12.7 \n78.17 \n592.20 \n6.0 \n\n\n\nTable 3 :\n3Ablation study for different components of our algorithm. The speedup rate is computed based on running time of BABSR baseline: speedup = Time of BaBSR/Time of our method. Method time(s) speedup time(s) speedup time(s) speedup time(s) speedup time(s) speedupEasy \nMedium \nHard \nWide \nDeep \n\nBABSR baseline \n522.48 \n-\n1335.40 \n-\n3017.11 \n-\n2305.65 \n-\n2455.17 \n-\nBatch Splits (unoptimized LiRPA) 587.10 \n0.89 \n1470.02 \n0.91 \n3242.57 \n0.93 \n2497.30 \n0.92 \n2835.50 \n0.87 \nOptimized LiRPA (no batch splits) 191.08 \n2.73 \n477.53 \n2.80 \n684.22 \n4.41 \n487.56 \n4.73 \n207.33 \n11.84 \nOptimized LiRPA & Batch Splits \n56.49 \n9.25 \n124.38 \n10.74 \n267.35 \n11.26 \n181.30 \n12.71 \n78.17 \n31.41 \n\n\n\n\ntime(s)n CPU core(s) One CPU core + one GPU GNN Online1 2 \n4 \n8 \n\nnumber of CPU core \n\n60 \n80 \n100 \n120 \n140 \n160 \n180 \n200 \n220 \n\ntime(s) \n\nn CPU core(s) \nOne CPU core + one GPU \nGNN Online \n\n1 2 \n4 \n8 \n\nnumber of CPU core \n\n200 \n\n400 \n\n600 \n\n800 \n\n1000 \n\n1200 \n\n1400 \n\ntime(s) \n\nn CPU core(s) \nOne CPU core + one GPU \nGNN Online \n\n1 2 \n4 \n8 \n\nnumber of CPU core \n\n200 \n400 \n600 \n800 \n1000 \n1200 \n1400 \n1600 \n\nBase Model (easy instances) \nWide Model \nDeep Model \n(BaBSR: 522.48s) \n(BaBSR: 2305.65s) \n(BaBSR: 2455.17s) \n\n\nWe only use the backward mode LiRPA bounds (e.g., CROWN and DeepPoly) in this paper.\nThe code link provided in(Bunel et al., 2020a) does not contain complete verification, so we directly report the best results in their paper in our table. Since we use a similar GPU model as theirs, the time is comparable.\n\nOptimization and abstraction: A synergistic approach for analyzing neural network robustness. Greg Anderson, Shankara Pailoor, Isil Dillig, Swarat Chaudhuri, Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation. the 40th ACM SIGPLAN Conference on Programming Language Design and ImplementationGreg Anderson, Shankara Pailoor, Isil Dillig, and Swarat Chaudhuri. Optimization and abstraction: A synergistic approach for analyzing neural network robustness. In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation, pp. 731- 744, 2019.\n\nSuperposition for lambda-free higher-order logic. Alexander Bentkamp, Jasmin Christian Blanchette, Simon Cruanes, Uwe Waldmann, International Joint Conference on Automated Reasoning. SpringerAlexander Bentkamp, Jasmin Christian Blanchette, Simon Cruanes, and Uwe Waldmann. Super- position for lambda-free higher-order logic. In International Joint Conference on Automated Reasoning, pp. 28-46. Springer, 2018.\n\nLagrangian decomposition for neural network verification. Rudy Bunel, Alessandro De Palma, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli, H S Philip, M. Pawan Torr, Kumar, Conference on Uncertainty in Artificial Intelligence (UAI). 2020Rudy Bunel, Alessandro De Palma, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli, Philip H. S. Torr, and M. Pawan Kumar. Lagrangian decomposition for neural network verifica- tion. Conference on Uncertainty in Artificial Intelligence (UAI) 2020, 2020a.\n\nBranch and bound for piecewise linear neural network verification. Rudy Bunel, Jingyue Lu, Ilker Turkaslan, P Kohli, P Torr, Mudigonda, Journal of Machine Learning Research. 212020Rudy Bunel, Jingyue Lu, Ilker Turkaslan, P Kohli, P Torr, and P Mudigonda. Branch and bound for piecewise linear neural network verification. Journal of Machine Learning Research, 21(2020), 2020b.\n\nA unified view of piecewise linear neural network verification. Ilker Rudy R Bunel, Philip Turkaslan, Pushmeet Torr, Pawan K Kohli, Mudigonda, Advances in Neural Information Processing Systems. Rudy R Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, and Pawan K Mudigonda. A unified view of piecewise linear neural network verification. In Advances in Neural Information Process- ing Systems, pp. 4790-4799, 2018.\n\nEnabling certification of verification-agnostic networks via memory-efficient semidefinite programming. Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato, Rudy R Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy S Liang, Advances in Neural Information Processing Systems. 33Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Ue- sato, Rudy R Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy S Liang, et al. Enabling certification of verification-agnostic networks via memory-efficient semidefinite pro- gramming. Advances in Neural Information Processing Systems, 33, 2020.\n\nOutput range analysis for deep feedforward neural networks. Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, Ashish Tiwari, NASA Formal Methods Symposium. SpringerSouradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. Output range analy- sis for deep feedforward neural networks. In NASA Formal Methods Symposium, pp. 121-138. Springer, 2018.\n\nTraining verified learners with learned verifiers. Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, O&apos; Brendan, Jonathan Donoghue, Pushmeet Uesato, Kohli, arXiv:1805.10265arXiv preprintKrishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan O'Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned ver- ifiers. arXiv preprint arXiv:1805.10265, 2018.\n\nSoham De, and Pushmeet Kohli. Efficient neural network verification with exactness characterization. Robert Krishnamurthy Dj Dvijotham, Sven Stanforth, Chongli Gowal, Qin, Uncertainty in Artificial Intelligence. PMLRKrishnamurthy Dj Dvijotham, Robert Stanforth, Sven Gowal, Chongli Qin, Soham De, and Push- meet Kohli. Efficient neural network verification with exactness characterization. In Uncertainty in Artificial Intelligence, pp. 497-507. PMLR, 2020.\n\nFormal verification of piece-wise linear feed-forward neural networks. Ruediger Ehlers, International Symposium on Automated Technology for Verification and Analysis. SpringerRuediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In Interna- tional Symposium on Automated Technology for Verification and Analysis, pp. 269-286. Springer, 2017.\n\nSwarat Chaudhuri, and Martin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, 2018 IEEE Symposium on Security and Privacy (SP). IEEETimon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Mar- tin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpreta- tion. In 2018 IEEE Symposium on Security and Privacy (SP), pp. 3-18. IEEE, 2018.\n\nSafety verification of deep neural networks. Xiaowei Huang, Marta Kwiatkowska, Sen Wang, Min Wu, International Conference on Computer Aided Verification. SpringerXiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks. In International Conference on Computer Aided Verification, pp. 3-29. Springer, 2017.\n\nReluplex: An efficient smt solver for verifying deep neural networks. Guy Katz, Clark Barrett, L David, Kyle Dill, Julian, Kochenderfer, International Conference on Computer Aided Verification. SpringerGuy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification, pp. 97-117. Springer, 2017.\n\nJingyue Lu, M Pawan Kumar, Neural network branching for neural network verification. International Conference on Learning Representation (ICLR. 2020Jingyue Lu and M Pawan Kumar. Neural network branching for neural network verification. Inter- national Conference on Learning Representation (ICLR), 2020.\n\nZhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, Luca Daniel, arXiv:1912.00574Fastened crown: Tightened neural network robustness certificates. arXiv preprintZhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, and Luca Daniel. Fastened crown: Tightened neural network robustness certificates. arXiv preprint arXiv:1912.00574, 2019.\n\nDifferentiable abstract interpretation for provably robust neural networks. Matthew Mirman, Timon Gehr, Martin Vechev, International Conference on Machine Learning. Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov- ably robust neural networks. In International Conference on Machine Learning, pp. 3575-3583, 2018.\n\nChristoph M\u00fcller, Gagandeep Singh, Markus P\u00fcschel, Martin Vechev, arXiv:2007.10868Neural network robustness verification on gpus. arXiv preprintChristoph M\u00fcller, Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev. Neural network robust- ness verification on gpus. arXiv preprint arXiv:2007.10868, 2020.\n\nSemidefinite relaxations for certifying robustness to adversarial examples. Aditi Raghunathan, Jacob Steinhardt, Percy S Liang, Advances in Neural Information Processing Systems. Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying robustness to adversarial examples. In Advances in Neural Information Processing Systems, pp. 10877-10887, 2018.\n\nFast neural network verification via shadow prices. Roberto Vicenc Rubies Royo, Calandra, M Dusan, Claire Stipanovic, Tomlin, arXiv:1902.07247arXiv preprintVicenc Rubies Royo, Roberto Calandra, Dusan M Stipanovic, and Claire Tomlin. Fast neural net- work verification via shadow prices. arXiv preprint arXiv:1902.07247, 2019.\n\nA convex relaxation barrier to tight robustness verification of neural networks. Greg Hadi Salman, Huan Yang, Cho-Jui Zhang, Pengchuan Hsieh, Zhang, Advances in Neural Information Processing Systems. 32Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier to tight robustness verification of neural networks. In Advances in Neural Information Processing Systems 32, pp. 9832-9842, 2019.\n\nFast and effective robustness certification. Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, Martin Vechev, Advances in Neural Information Processing Systems. Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, and Martin Vechev. Fast and effective robustness certification. In Advances in Neural Information Processing Systems, pp. 10825-10836, 2018a.\n\nBoosting robustness certification of neural networks. Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, Martin Vechev, International Conference on Learning Representations. Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev. Boosting robustness certifica- tion of neural networks. In International Conference on Learning Representations, 2018b.\n\nBeyond the single neuron convex barrier for neural network certification. Gagandeep Singh, Rupanshu Ganvir, Markus P\u00fcschel, Martin Vechev, Advances in Neural Information Processing Systems. Gagandeep Singh, Rupanshu Ganvir, Markus P\u00fcschel, and Martin Vechev. Beyond the single neuron convex barrier for neural network certification. In Advances in Neural Information Processing Systems, pp. 15072-15083, 2019a.\n\nAn abstract domain for certifying neural networks. Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, Martin Vechev, Proceedings of the ACM on Programming Languages. 341Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev. An abstract domain for cer- tifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):41, 2019b.\n\nIntriguing properties of neural networks. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus, In ICLR. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2013.\n\nChristian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, Juan Pablo Vielma, arXiv:2006.14076The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification. arXiv preprintChristian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, and Juan Pablo Vielma. The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification. arXiv preprint arXiv:2006.14076, 2020.\n\nEvaluating robustness of neural networks with mixed integer programming. ICLR. Vincent Tjeng, Kai Xiao, Russ Tedrake, Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer programming. ICLR, 2019a.\n\nEvaluating robustness of neural networks with mixed integer programming. Vincent Tjeng, Kai Y Xiao, Russ Tedrake, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAVincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer programming. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019b. URL https://openreview.net/ forum?id=HyGIdiRqtm.\n\nShiqi Wang, Yizheng Chen, Ahmed Abdou, Suman Jana Mixtrain, arXiv:1811.02625Scalable training of formally robust neural networks. arXiv preprintShiqi Wang, Yizheng Chen, Ahmed Abdou, and Suman Jana. Mixtrain: Scalable training of formally robust neural networks. arXiv preprint arXiv:1811.02625, 2018a.\n\nEfficient formal safety analysis of neural networks. Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana, Advances in Neural Information Processing Systems. Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety analysis of neural networks. In Advances in Neural Information Processing Systems, pp. 6367- 6377, 2018b.\n\nFormal security analysis of neural networks using symbolic intervals. Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana, 27th {USENIX} Security Symposium ({USENIX} Security 18). Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security analysis of neural networks using symbolic intervals. In 27th {USENIX} Security Symposium ({USENIX} Security 18), pp. 1599-1614, 2018c.\n\nTowards fast computation of certified robustness for relu networks. Huan Tsui-Wei Weng, Hongge Zhang, Zhao Chen, Cho-Jui Song, Luca Hsieh, Duane Daniel, Inderjit Boning, Dhillon, International Conference on Machine Learning. Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Bon- ing, and Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In International Conference on Machine Learning, pp. 5273-5282, 2018.\n\nProvable defenses against adversarial examples via the convex outer adversarial polytope. Eric Wong, Zico Kolter, International Conference on Machine Learning. Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pp. 5283-5292, 2018.\n\nScaling provable adversarial defenses. Eric Wong, Frank Schmidt, Jan Hendrik Metzen, J Zico Kolter, NIPS. Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial defenses. In NIPS, 2018.\n\nAutomatic perturbation analysis for scalable certified robustness and beyond. Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, Cho-Jui Hsieh, Advances in Neural Information Processing Systems. 33Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified robustness and beyond. Advances in Neural Information Processing Systems, 33, 2020.\n\nEfficient neural network robustness certification with general activation functions. Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel, Advances in neural information processing systems. Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural net- work robustness certification with general activation functions. In Advances in neural information processing systems, pp. 4939-4948, 2018.\n\nTowards stable and efficient training of verifiably robust neural networks. Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, Cho-Jui Hsieh, International Conference on Learning Representations. Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks. In International Conference on Learning Representations, 2020.\n", "annotations": {"author": "[{\"end\":185,\"start\":125},{\"end\":204,\"start\":186},{\"end\":266,\"start\":205},{\"end\":308,\"start\":267},{\"end\":342,\"start\":309},{\"end\":402,\"start\":343},{\"end\":445,\"start\":403}]", "publisher": null, "author_last_name": "[{\"end\":133,\"start\":131},{\"end\":196,\"start\":191},{\"end\":215,\"start\":211},{\"end\":277,\"start\":273},{\"end\":319,\"start\":315},{\"end\":350,\"start\":347},{\"end\":416,\"start\":411}]", "author_first_name": "[{\"end\":130,\"start\":125},{\"end\":190,\"start\":186},{\"end\":210,\"start\":205},{\"end\":272,\"start\":267},{\"end\":314,\"start\":309},{\"end\":346,\"start\":343},{\"end\":410,\"start\":403}]", "author_affiliation": "[{\"end\":184,\"start\":160},{\"end\":203,\"start\":198},{\"end\":265,\"start\":245},{\"end\":307,\"start\":302},{\"end\":341,\"start\":321},{\"end\":401,\"start\":377},{\"end\":444,\"start\":439}]", "title": "[{\"end\":122,\"start\":1},{\"end\":567,\"start\":446}]", "venue": null, "abstract": "[{\"end\":2035,\"start\":569}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2210,\"start\":2188},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3401,\"start\":3382},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3494,\"start\":3475},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3532,\"start\":3511},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3627,\"start\":3606},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4667,\"start\":4650},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4794,\"start\":4773},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5146,\"start\":5127},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5616,\"start\":5596},{\"end\":5633,\"start\":5616},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5675,\"start\":5655},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5852,\"start\":5832},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5970,\"start\":5949},{\"end\":7889,\"start\":7886},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8774,\"start\":8755},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9461,\"start\":9441},{\"end\":9467,\"start\":9461},{\"end\":11089,\"start\":11068},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11711,\"start\":11691},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12275,\"start\":12261},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16263,\"start\":16246},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16814,\"start\":16794},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16856,\"start\":16839},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18357,\"start\":18340},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19107,\"start\":19090},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19229,\"start\":19212},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21864,\"start\":21844},{\"end\":25083,\"start\":25062},{\"end\":25100,\"start\":25083},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26181,\"start\":26161},{\"end\":28101,\"start\":28083},{\"end\":28276,\"start\":28258},{\"end\":28308,\"start\":28287},{\"end\":29018,\"start\":28997},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29114,\"start\":29100},{\"end\":29247,\"start\":29229},{\"end\":29285,\"start\":29267},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29424,\"start\":29404},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31008,\"start\":30989},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31027,\"start\":31008},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31040,\"start\":31027},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31106,\"start\":31085},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31125,\"start\":31106},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31239,\"start\":31216},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31386,\"start\":31367},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31565,\"start\":31545},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31584,\"start\":31565},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31602,\"start\":31584},{\"end\":31619,\"start\":31602},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31735,\"start\":31715},{\"end\":31755,\"start\":31735},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31773,\"start\":31755},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31795,\"start\":31773},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31919,\"start\":31900},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31938,\"start\":31919},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31957,\"start\":31938},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31970,\"start\":31957},{\"end\":31987,\"start\":31970},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32066,\"start\":32047},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32296,\"start\":32276},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32682,\"start\":32661},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32999,\"start\":32979},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33512,\"start\":33493},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33532,\"start\":33512},{\"end\":33534,\"start\":33532},{\"end\":33536,\"start\":33534},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33591,\"start\":33565},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33614,\"start\":33591},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33711,\"start\":33687},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33731,\"start\":33711},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33748,\"start\":33731},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33776,\"start\":33748},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33955,\"start\":33939},{\"end\":39682,\"start\":39664},{\"end\":39714,\"start\":39693},{\"end\":39934,\"start\":39916},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":39953,\"start\":39934},{\"end\":41050,\"start\":41032},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":50041,\"start\":50020}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":44949,\"start\":44721},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45192,\"start\":44950},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45514,\"start\":45193},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45788,\"start\":45515},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47020,\"start\":45789},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47271,\"start\":47021},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47459,\"start\":47272},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":48204,\"start\":47460},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48694,\"start\":48205},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49385,\"start\":48695},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":49909,\"start\":49386}]", "paragraph": "[{\"end\":2436,\"start\":2051},{\"end\":2536,\"start\":2438},{\"end\":2634,\"start\":2538},{\"end\":3031,\"start\":2636},{\"end\":4363,\"start\":3033},{\"end\":5001,\"start\":4365},{\"end\":6233,\"start\":5003},{\"end\":7099,\"start\":6235},{\"end\":7373,\"start\":7101},{\"end\":7518,\"start\":7444},{\"end\":8051,\"start\":7721},{\"end\":8523,\"start\":8053},{\"end\":9203,\"start\":8545},{\"end\":9798,\"start\":9276},{\"end\":9858,\"start\":9818},{\"end\":10637,\"start\":9903},{\"end\":10931,\"start\":10658},{\"end\":11397,\"start\":10955},{\"end\":11892,\"start\":11399},{\"end\":12388,\"start\":11894},{\"end\":12479,\"start\":12412},{\"end\":12600,\"start\":12571},{\"end\":12946,\"start\":12658},{\"end\":13461,\"start\":13272},{\"end\":13649,\"start\":13463},{\"end\":13792,\"start\":13692},{\"end\":14187,\"start\":13929},{\"end\":14264,\"start\":14211},{\"end\":14906,\"start\":14838},{\"end\":15163,\"start\":14965},{\"end\":15608,\"start\":15401},{\"end\":16272,\"start\":15887},{\"end\":16281,\"start\":16274},{\"end\":16290,\"start\":16283},{\"end\":16299,\"start\":16292},{\"end\":16308,\"start\":16301},{\"end\":16317,\"start\":16310},{\"end\":16326,\"start\":16319},{\"end\":16335,\"start\":16328},{\"end\":16340,\"start\":16337},{\"end\":16351,\"start\":16342},{\"end\":16356,\"start\":16353},{\"end\":16366,\"start\":16358},{\"end\":16371,\"start\":16368},{\"end\":16382,\"start\":16373},{\"end\":16387,\"start\":16384},{\"end\":16397,\"start\":16389},{\"end\":16402,\"start\":16399},{\"end\":16506,\"start\":16474},{\"end\":16607,\"start\":16508},{\"end\":16614,\"start\":16609},{\"end\":16621,\"start\":16616},{\"end\":16878,\"start\":16623},{\"end\":17866,\"start\":16901},{\"end\":18146,\"start\":17912},{\"end\":18950,\"start\":18203},{\"end\":19607,\"start\":19025},{\"end\":19971,\"start\":19609},{\"end\":20604,\"start\":19986},{\"end\":20672,\"start\":20631},{\"end\":20807,\"start\":20674},{\"end\":21163,\"start\":20868},{\"end\":21338,\"start\":21165},{\"end\":21964,\"start\":21340},{\"end\":22092,\"start\":22019},{\"end\":22186,\"start\":22094},{\"end\":22305,\"start\":22188},{\"end\":22542,\"start\":22436},{\"end\":22759,\"start\":22690},{\"end\":22770,\"start\":22761},{\"end\":22873,\"start\":22784},{\"end\":22984,\"start\":22930},{\"end\":23623,\"start\":23456},{\"end\":23728,\"start\":23650},{\"end\":24037,\"start\":23744},{\"end\":24326,\"start\":24039},{\"end\":25013,\"start\":24328},{\"end\":25692,\"start\":25030},{\"end\":26891,\"start\":25732},{\"end\":27570,\"start\":26893},{\"end\":27848,\"start\":27572},{\"end\":28150,\"start\":27864},{\"end\":28888,\"start\":28152},{\"end\":29084,\"start\":28890},{\"end\":30885,\"start\":29086},{\"end\":32561,\"start\":30902},{\"end\":32948,\"start\":32563},{\"end\":33332,\"start\":32950},{\"end\":34429,\"start\":33334},{\"end\":34828,\"start\":34444},{\"end\":34865,\"start\":34841},{\"end\":35071,\"start\":34867},{\"end\":35099,\"start\":35090},{\"end\":35104,\"start\":35101},{\"end\":35170,\"start\":35119},{\"end\":35790,\"start\":35257},{\"end\":36702,\"start\":36215},{\"end\":36824,\"start\":36731},{\"end\":37150,\"start\":36826},{\"end\":37531,\"start\":37365},{\"end\":37679,\"start\":37533},{\"end\":37688,\"start\":37681},{\"end\":37960,\"start\":37690},{\"end\":38543,\"start\":37962},{\"end\":38897,\"start\":38545},{\"end\":39536,\"start\":38899},{\"end\":40525,\"start\":39561},{\"end\":40682,\"start\":40527},{\"end\":41204,\"start\":40748},{\"end\":41746,\"start\":41206},{\"end\":42574,\"start\":41767},{\"end\":43503,\"start\":42627},{\"end\":44720,\"start\":43505}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7720,\"start\":7519},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8544,\"start\":8524},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9817,\"start\":9799},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9902,\"start\":9859},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12411,\"start\":12389},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12570,\"start\":12480},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13090,\"start\":12947},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13271,\"start\":13090},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13691,\"start\":13650},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13928,\"start\":13793},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14210,\"start\":14188},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14837,\"start\":14265},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14964,\"start\":14907},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15400,\"start\":15164},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15886,\"start\":15609},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16473,\"start\":16403},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18202,\"start\":18147},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19024,\"start\":18951},{\"attributes\":{\"id\":\"formula_18\"},\"end\":19985,\"start\":19972},{\"attributes\":{\"id\":\"formula_20\"},\"end\":22018,\"start\":21965},{\"attributes\":{\"id\":\"formula_21\"},\"end\":22435,\"start\":22306},{\"attributes\":{\"id\":\"formula_22\"},\"end\":22689,\"start\":22543},{\"attributes\":{\"id\":\"formula_23\"},\"end\":22783,\"start\":22771},{\"attributes\":{\"id\":\"formula_24\"},\"end\":22929,\"start\":22874},{\"attributes\":{\"id\":\"formula_25\"},\"end\":23455,\"start\":22985},{\"attributes\":{\"id\":\"formula_26\"},\"end\":23649,\"start\":23624},{\"attributes\":{\"id\":\"formula_27\"},\"end\":23743,\"start\":23729},{\"attributes\":{\"id\":\"formula_28\"},\"end\":35089,\"start\":35072},{\"attributes\":{\"id\":\"formula_29\"},\"end\":35118,\"start\":35105},{\"attributes\":{\"id\":\"formula_30\"},\"end\":35256,\"start\":35171},{\"attributes\":{\"id\":\"formula_31\"},\"end\":35834,\"start\":35791},{\"attributes\":{\"id\":\"formula_32\"},\"end\":36214,\"start\":35834},{\"attributes\":{\"id\":\"formula_33\"},\"end\":37364,\"start\":37151},{\"attributes\":{\"id\":\"formula_36\"},\"end\":40747,\"start\":40683}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":29672,\"start\":29665},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30167,\"start\":30160},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":42032,\"start\":42025}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2049,\"start\":2037},{\"attributes\":{\"n\":\"2\"},\"end\":7386,\"start\":7376},{\"attributes\":{\"n\":\"2.1\"},\"end\":7442,\"start\":7389},{\"attributes\":{\"n\":\"2.2\"},\"end\":9274,\"start\":9206},{\"end\":10656,\"start\":10640},{\"end\":10953,\"start\":10934},{\"attributes\":{\"n\":\"2.3\"},\"end\":12656,\"start\":12603},{\"attributes\":{\"n\":\"3\"},\"end\":16899,\"start\":16881},{\"attributes\":{\"n\":\"3.1\"},\"end\":17910,\"start\":17869},{\"end\":20629,\"start\":20607},{\"attributes\":{\"n\":\"3.2\"},\"end\":20866,\"start\":20810},{\"attributes\":{\"n\":\"3.3\"},\"end\":25028,\"start\":25016},{\"attributes\":{\"n\":\"3.4\"},\"end\":25730,\"start\":25695},{\"attributes\":{\"n\":\"4\"},\"end\":27862,\"start\":27851},{\"attributes\":{\"n\":\"5\"},\"end\":30900,\"start\":30888},{\"attributes\":{\"n\":\"6\"},\"end\":34442,\"start\":34432},{\"end\":34839,\"start\":34831},{\"end\":36729,\"start\":36705},{\"end\":39559,\"start\":39539},{\"end\":41765,\"start\":41749},{\"end\":42625,\"start\":42577},{\"end\":45204,\"start\":45194},{\"end\":45800,\"start\":45790},{\"end\":47283,\"start\":47273},{\"end\":47470,\"start\":47461},{\"end\":48215,\"start\":48206},{\"end\":48705,\"start\":48696}]", "table": "[{\"end\":48204,\"start\":47657},{\"end\":48694,\"start\":48305},{\"end\":49385,\"start\":48965},{\"end\":49909,\"start\":49442}]", "figure_caption": "[{\"end\":44949,\"start\":44723},{\"end\":45192,\"start\":44952},{\"end\":45514,\"start\":45206},{\"end\":45788,\"start\":45517},{\"end\":47020,\"start\":45802},{\"end\":47271,\"start\":47023},{\"end\":47459,\"start\":47285},{\"end\":47657,\"start\":47472},{\"end\":48305,\"start\":48217},{\"end\":48965,\"start\":48707},{\"end\":49442,\"start\":49388}]", "figure_ref": "[{\"end\":12385,\"start\":12376},{\"end\":12857,\"start\":12849},{\"end\":15006,\"start\":14998},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17144,\"start\":17138},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20313,\"start\":20305},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":29856,\"start\":29848},{\"end\":43516,\"start\":43508},{\"end\":44397,\"start\":44389}]", "bib_author_first_name": "[{\"end\":50317,\"start\":50313},{\"end\":50336,\"start\":50328},{\"end\":50350,\"start\":50346},{\"end\":50365,\"start\":50359},{\"end\":50899,\"start\":50890},{\"end\":50916,\"start\":50910},{\"end\":50926,\"start\":50917},{\"end\":50944,\"start\":50939},{\"end\":50957,\"start\":50954},{\"end\":51313,\"start\":51309},{\"end\":51331,\"start\":51321},{\"end\":51334,\"start\":51332},{\"end\":51347,\"start\":51342},{\"end\":51372,\"start\":51359},{\"end\":51392,\"start\":51384},{\"end\":51401,\"start\":51400},{\"end\":51403,\"start\":51402},{\"end\":51420,\"start\":51412},{\"end\":51832,\"start\":51828},{\"end\":51847,\"start\":51840},{\"end\":51857,\"start\":51852},{\"end\":51870,\"start\":51869},{\"end\":51879,\"start\":51878},{\"end\":52208,\"start\":52203},{\"end\":52229,\"start\":52223},{\"end\":52249,\"start\":52241},{\"end\":52263,\"start\":52256},{\"end\":52669,\"start\":52662},{\"end\":52694,\"start\":52681},{\"end\":52712,\"start\":52706},{\"end\":52727,\"start\":52722},{\"end\":52749,\"start\":52741},{\"end\":52762,\"start\":52758},{\"end\":52764,\"start\":52763},{\"end\":52778,\"start\":52772},{\"end\":52793,\"start\":52788},{\"end\":52809,\"start\":52806},{\"end\":52827,\"start\":52822},{\"end\":52829,\"start\":52828},{\"end\":53311,\"start\":53302},{\"end\":53325,\"start\":53319},{\"end\":53337,\"start\":53331},{\"end\":53362,\"start\":53356},{\"end\":53673,\"start\":53660},{\"end\":53689,\"start\":53685},{\"end\":53703,\"start\":53697},{\"end\":53720,\"start\":53715},{\"end\":53742,\"start\":53735},{\"end\":53760,\"start\":53752},{\"end\":53779,\"start\":53771},{\"end\":54157,\"start\":54151},{\"end\":54190,\"start\":54186},{\"end\":54209,\"start\":54202},{\"end\":54588,\"start\":54580},{\"end\":55018,\"start\":55013},{\"end\":55032,\"start\":55025},{\"end\":55045,\"start\":55041},{\"end\":55068,\"start\":55063},{\"end\":55456,\"start\":55449},{\"end\":55469,\"start\":55464},{\"end\":55486,\"start\":55483},{\"end\":55496,\"start\":55493},{\"end\":55827,\"start\":55824},{\"end\":55839,\"start\":55834},{\"end\":55850,\"start\":55849},{\"end\":55862,\"start\":55858},{\"end\":56200,\"start\":56193},{\"end\":56212,\"start\":56205},{\"end\":56506,\"start\":56498},{\"end\":56521,\"start\":56512},{\"end\":56533,\"start\":56526},{\"end\":56544,\"start\":56540},{\"end\":56556,\"start\":56551},{\"end\":56566,\"start\":56562},{\"end\":56941,\"start\":56934},{\"end\":56955,\"start\":56950},{\"end\":56968,\"start\":56962},{\"end\":57228,\"start\":57219},{\"end\":57246,\"start\":57237},{\"end\":57260,\"start\":57254},{\"end\":57276,\"start\":57270},{\"end\":57604,\"start\":57599},{\"end\":57623,\"start\":57618},{\"end\":57643,\"start\":57636},{\"end\":57971,\"start\":57964},{\"end\":58003,\"start\":58002},{\"end\":58017,\"start\":58011},{\"end\":58324,\"start\":58320},{\"end\":58342,\"start\":58338},{\"end\":58356,\"start\":58349},{\"end\":58373,\"start\":58364},{\"end\":58727,\"start\":58718},{\"end\":58740,\"start\":58735},{\"end\":58754,\"start\":58747},{\"end\":58769,\"start\":58763},{\"end\":58785,\"start\":58779},{\"end\":59112,\"start\":59103},{\"end\":59125,\"start\":59120},{\"end\":59138,\"start\":59132},{\"end\":59154,\"start\":59148},{\"end\":59485,\"start\":59476},{\"end\":59501,\"start\":59493},{\"end\":59516,\"start\":59510},{\"end\":59532,\"start\":59526},{\"end\":59874,\"start\":59865},{\"end\":59887,\"start\":59882},{\"end\":59900,\"start\":59894},{\"end\":59916,\"start\":59910},{\"end\":60214,\"start\":60205},{\"end\":60232,\"start\":60224},{\"end\":60246,\"start\":60242},{\"end\":60262,\"start\":60258},{\"end\":60277,\"start\":60270},{\"end\":60288,\"start\":60285},{\"end\":60304,\"start\":60301},{\"end\":60501,\"start\":60492},{\"end\":60522,\"start\":60518},{\"end\":60537,\"start\":60533},{\"end\":60552,\"start\":60548},{\"end\":60563,\"start\":60557},{\"end\":60575,\"start\":60571},{\"end\":60581,\"start\":60576},{\"end\":61070,\"start\":61063},{\"end\":61081,\"start\":61078},{\"end\":61092,\"start\":61088},{\"end\":61312,\"start\":61305},{\"end\":61323,\"start\":61320},{\"end\":61325,\"start\":61324},{\"end\":61336,\"start\":61332},{\"end\":61741,\"start\":61736},{\"end\":61755,\"start\":61748},{\"end\":61767,\"start\":61762},{\"end\":61780,\"start\":61775},{\"end\":61785,\"start\":61781},{\"end\":62098,\"start\":62093},{\"end\":62110,\"start\":62105},{\"end\":62122,\"start\":62116},{\"end\":62142,\"start\":62135},{\"end\":62154,\"start\":62149},{\"end\":62490,\"start\":62485},{\"end\":62502,\"start\":62497},{\"end\":62514,\"start\":62508},{\"end\":62534,\"start\":62527},{\"end\":62546,\"start\":62541},{\"end\":62907,\"start\":62903},{\"end\":62929,\"start\":62923},{\"end\":62941,\"start\":62937},{\"end\":62955,\"start\":62948},{\"end\":62966,\"start\":62962},{\"end\":62979,\"start\":62974},{\"end\":62996,\"start\":62988},{\"end\":63410,\"start\":63406},{\"end\":63421,\"start\":63417},{\"end\":63707,\"start\":63703},{\"end\":63719,\"start\":63714},{\"end\":63732,\"start\":63729},{\"end\":63740,\"start\":63733},{\"end\":63755,\"start\":63749},{\"end\":63973,\"start\":63968},{\"end\":63986,\"start\":63978},{\"end\":63996,\"start\":63992},{\"end\":64009,\"start\":64004},{\"end\":64023,\"start\":64016},{\"end\":64037,\"start\":64031},{\"end\":64051,\"start\":64045},{\"end\":64066,\"start\":64063},{\"end\":64079,\"start\":64072},{\"end\":64492,\"start\":64488},{\"end\":64508,\"start\":64500},{\"end\":64521,\"start\":64515},{\"end\":64535,\"start\":64528},{\"end\":64547,\"start\":64543},{\"end\":64922,\"start\":64918},{\"end\":64936,\"start\":64930},{\"end\":64950,\"start\":64943},{\"end\":64959,\"start\":64957},{\"end\":64969,\"start\":64964},{\"end\":64985,\"start\":64978}]", "bib_author_last_name": "[{\"end\":50326,\"start\":50318},{\"end\":50344,\"start\":50337},{\"end\":50357,\"start\":50351},{\"end\":50375,\"start\":50366},{\"end\":50908,\"start\":50900},{\"end\":50937,\"start\":50927},{\"end\":50952,\"start\":50945},{\"end\":50966,\"start\":50958},{\"end\":51319,\"start\":51314},{\"end\":51340,\"start\":51335},{\"end\":51357,\"start\":51348},{\"end\":51382,\"start\":51373},{\"end\":51398,\"start\":51393},{\"end\":51410,\"start\":51404},{\"end\":51425,\"start\":51421},{\"end\":51432,\"start\":51427},{\"end\":51838,\"start\":51833},{\"end\":51850,\"start\":51848},{\"end\":51867,\"start\":51858},{\"end\":51876,\"start\":51871},{\"end\":51884,\"start\":51880},{\"end\":51895,\"start\":51886},{\"end\":52221,\"start\":52209},{\"end\":52239,\"start\":52230},{\"end\":52254,\"start\":52250},{\"end\":52269,\"start\":52264},{\"end\":52280,\"start\":52271},{\"end\":52679,\"start\":52670},{\"end\":52704,\"start\":52695},{\"end\":52720,\"start\":52713},{\"end\":52739,\"start\":52728},{\"end\":52756,\"start\":52750},{\"end\":52770,\"start\":52765},{\"end\":52786,\"start\":52779},{\"end\":52804,\"start\":52794},{\"end\":52820,\"start\":52810},{\"end\":52835,\"start\":52830},{\"end\":53317,\"start\":53312},{\"end\":53329,\"start\":53326},{\"end\":53354,\"start\":53338},{\"end\":53369,\"start\":53363},{\"end\":53683,\"start\":53674},{\"end\":53695,\"start\":53690},{\"end\":53713,\"start\":53704},{\"end\":53733,\"start\":53721},{\"end\":53750,\"start\":53743},{\"end\":53769,\"start\":53761},{\"end\":53786,\"start\":53780},{\"end\":53793,\"start\":53788},{\"end\":54184,\"start\":54158},{\"end\":54200,\"start\":54191},{\"end\":54215,\"start\":54210},{\"end\":54220,\"start\":54217},{\"end\":54595,\"start\":54589},{\"end\":55023,\"start\":55019},{\"end\":55039,\"start\":55033},{\"end\":55061,\"start\":55046},{\"end\":55076,\"start\":55069},{\"end\":55462,\"start\":55457},{\"end\":55481,\"start\":55470},{\"end\":55491,\"start\":55487},{\"end\":55499,\"start\":55497},{\"end\":55832,\"start\":55828},{\"end\":55847,\"start\":55840},{\"end\":55856,\"start\":55851},{\"end\":55867,\"start\":55863},{\"end\":55875,\"start\":55869},{\"end\":55889,\"start\":55877},{\"end\":56203,\"start\":56201},{\"end\":56218,\"start\":56213},{\"end\":56510,\"start\":56507},{\"end\":56524,\"start\":56522},{\"end\":56538,\"start\":56534},{\"end\":56549,\"start\":56545},{\"end\":56560,\"start\":56557},{\"end\":56573,\"start\":56567},{\"end\":56948,\"start\":56942},{\"end\":56960,\"start\":56956},{\"end\":56975,\"start\":56969},{\"end\":57235,\"start\":57229},{\"end\":57252,\"start\":57247},{\"end\":57268,\"start\":57261},{\"end\":57283,\"start\":57277},{\"end\":57616,\"start\":57605},{\"end\":57634,\"start\":57624},{\"end\":57649,\"start\":57644},{\"end\":57990,\"start\":57972},{\"end\":58000,\"start\":57992},{\"end\":58009,\"start\":58004},{\"end\":58028,\"start\":58018},{\"end\":58036,\"start\":58030},{\"end\":58336,\"start\":58325},{\"end\":58347,\"start\":58343},{\"end\":58362,\"start\":58357},{\"end\":58379,\"start\":58374},{\"end\":58386,\"start\":58381},{\"end\":58733,\"start\":58728},{\"end\":58745,\"start\":58741},{\"end\":58761,\"start\":58755},{\"end\":58777,\"start\":58770},{\"end\":58792,\"start\":58786},{\"end\":59118,\"start\":59113},{\"end\":59130,\"start\":59126},{\"end\":59146,\"start\":59139},{\"end\":59161,\"start\":59155},{\"end\":59491,\"start\":59486},{\"end\":59508,\"start\":59502},{\"end\":59524,\"start\":59517},{\"end\":59539,\"start\":59533},{\"end\":59880,\"start\":59875},{\"end\":59892,\"start\":59888},{\"end\":59908,\"start\":59901},{\"end\":59923,\"start\":59917},{\"end\":60222,\"start\":60215},{\"end\":60240,\"start\":60233},{\"end\":60256,\"start\":60247},{\"end\":60268,\"start\":60263},{\"end\":60283,\"start\":60278},{\"end\":60299,\"start\":60289},{\"end\":60311,\"start\":60305},{\"end\":60516,\"start\":60502},{\"end\":60531,\"start\":60523},{\"end\":60546,\"start\":60538},{\"end\":60555,\"start\":60553},{\"end\":60569,\"start\":60564},{\"end\":60588,\"start\":60582},{\"end\":61076,\"start\":61071},{\"end\":61086,\"start\":61082},{\"end\":61100,\"start\":61093},{\"end\":61318,\"start\":61313},{\"end\":61330,\"start\":61326},{\"end\":61344,\"start\":61337},{\"end\":61746,\"start\":61742},{\"end\":61760,\"start\":61756},{\"end\":61773,\"start\":61768},{\"end\":61794,\"start\":61786},{\"end\":62103,\"start\":62099},{\"end\":62114,\"start\":62111},{\"end\":62133,\"start\":62123},{\"end\":62147,\"start\":62143},{\"end\":62159,\"start\":62155},{\"end\":62495,\"start\":62491},{\"end\":62506,\"start\":62503},{\"end\":62525,\"start\":62515},{\"end\":62539,\"start\":62535},{\"end\":62551,\"start\":62547},{\"end\":62921,\"start\":62908},{\"end\":62935,\"start\":62930},{\"end\":62946,\"start\":62942},{\"end\":62960,\"start\":62956},{\"end\":62972,\"start\":62967},{\"end\":62986,\"start\":62980},{\"end\":63003,\"start\":62997},{\"end\":63012,\"start\":63005},{\"end\":63415,\"start\":63411},{\"end\":63428,\"start\":63422},{\"end\":63712,\"start\":63708},{\"end\":63727,\"start\":63720},{\"end\":63747,\"start\":63741},{\"end\":63762,\"start\":63756},{\"end\":63976,\"start\":63974},{\"end\":63990,\"start\":63987},{\"end\":64002,\"start\":63997},{\"end\":64014,\"start\":64010},{\"end\":64029,\"start\":64024},{\"end\":64043,\"start\":64038},{\"end\":64061,\"start\":64052},{\"end\":64070,\"start\":64067},{\"end\":64085,\"start\":64080},{\"end\":64498,\"start\":64493},{\"end\":64513,\"start\":64509},{\"end\":64526,\"start\":64522},{\"end\":64541,\"start\":64536},{\"end\":64554,\"start\":64548},{\"end\":64928,\"start\":64923},{\"end\":64941,\"start\":64937},{\"end\":64955,\"start\":64951},{\"end\":64962,\"start\":64960},{\"end\":64976,\"start\":64970},{\"end\":64991,\"start\":64986}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":127982021},\"end\":50838,\"start\":50219},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3355733},\"end\":51249,\"start\":50840},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":211259417},\"end\":51759,\"start\":51251},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":202577669},\"end\":52137,\"start\":51761},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":41612217},\"end\":52556,\"start\":52139},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":225039902},\"end\":53240,\"start\":52558},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3332132},\"end\":53607,\"start\":53242},{\"attributes\":{\"doi\":\"arXiv:1805.10265\",\"id\":\"b7\"},\"end\":54048,\"start\":53609},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":197661813},\"end\":54507,\"start\":54050},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1931807},\"end\":54884,\"start\":54509},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206579396},\"end\":55402,\"start\":54886},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11626373},\"end\":55752,\"start\":55404},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":516928},\"end\":56191,\"start\":55754},{\"attributes\":{\"id\":\"b13\"},\"end\":56496,\"start\":56193},{\"attributes\":{\"doi\":\"arXiv:1912.00574\",\"id\":\"b14\"},\"end\":56856,\"start\":56498},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":51872670},\"end\":57217,\"start\":56858},{\"attributes\":{\"doi\":\"arXiv:2007.10868\",\"id\":\"b16\"},\"end\":57521,\"start\":57219},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53215541},\"end\":57910,\"start\":57523},{\"attributes\":{\"doi\":\"arXiv:1902.07247\",\"id\":\"b18\"},\"end\":58237,\"start\":57912},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":67855530},\"end\":58671,\"start\":58239},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":53960414},\"end\":59047,\"start\":58673},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":196059499},\"end\":59400,\"start\":59049},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":207796611},\"end\":59812,\"start\":59402},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":57757287},\"end\":60161,\"start\":59814},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":604334},\"end\":60490,\"start\":60163},{\"attributes\":{\"doi\":\"arXiv:2006.14076\",\"id\":\"b25\"},\"end\":60982,\"start\":60492},{\"attributes\":{\"id\":\"b26\"},\"end\":61230,\"start\":60984},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":47016770},\"end\":61734,\"start\":61232},{\"attributes\":{\"doi\":\"arXiv:1811.02625\",\"id\":\"b28\"},\"end\":62038,\"start\":61736},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":52347370},\"end\":62413,\"start\":62040},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13745458},\"end\":62833,\"start\":62415},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13750928},\"end\":63314,\"start\":62835},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3659467},\"end\":63662,\"start\":63316},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":44124705},\"end\":63888,\"start\":63664},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":225086082},\"end\":64401,\"start\":63890},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":53297058},\"end\":64840,\"start\":64403},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":189856873},\"end\":65264,\"start\":64842}]", "bib_title": "[{\"end\":50311,\"start\":50219},{\"end\":50888,\"start\":50840},{\"end\":51307,\"start\":51251},{\"end\":51826,\"start\":51761},{\"end\":52201,\"start\":52139},{\"end\":52660,\"start\":52558},{\"end\":53300,\"start\":53242},{\"end\":54149,\"start\":54050},{\"end\":54578,\"start\":54509},{\"end\":55011,\"start\":54886},{\"end\":55447,\"start\":55404},{\"end\":55822,\"start\":55754},{\"end\":56932,\"start\":56858},{\"end\":57597,\"start\":57523},{\"end\":58318,\"start\":58239},{\"end\":58716,\"start\":58673},{\"end\":59101,\"start\":59049},{\"end\":59474,\"start\":59402},{\"end\":59863,\"start\":59814},{\"end\":60203,\"start\":60163},{\"end\":61303,\"start\":61232},{\"end\":62091,\"start\":62040},{\"end\":62483,\"start\":62415},{\"end\":62901,\"start\":62835},{\"end\":63404,\"start\":63316},{\"end\":63701,\"start\":63664},{\"end\":63966,\"start\":63890},{\"end\":64486,\"start\":64403},{\"end\":64916,\"start\":64842}]", "bib_author": "[{\"end\":50328,\"start\":50313},{\"end\":50346,\"start\":50328},{\"end\":50359,\"start\":50346},{\"end\":50377,\"start\":50359},{\"end\":50910,\"start\":50890},{\"end\":50939,\"start\":50910},{\"end\":50954,\"start\":50939},{\"end\":50968,\"start\":50954},{\"end\":51321,\"start\":51309},{\"end\":51342,\"start\":51321},{\"end\":51359,\"start\":51342},{\"end\":51384,\"start\":51359},{\"end\":51400,\"start\":51384},{\"end\":51412,\"start\":51400},{\"end\":51427,\"start\":51412},{\"end\":51434,\"start\":51427},{\"end\":51840,\"start\":51828},{\"end\":51852,\"start\":51840},{\"end\":51869,\"start\":51852},{\"end\":51878,\"start\":51869},{\"end\":51886,\"start\":51878},{\"end\":51897,\"start\":51886},{\"end\":52223,\"start\":52203},{\"end\":52241,\"start\":52223},{\"end\":52256,\"start\":52241},{\"end\":52271,\"start\":52256},{\"end\":52282,\"start\":52271},{\"end\":52681,\"start\":52662},{\"end\":52706,\"start\":52681},{\"end\":52722,\"start\":52706},{\"end\":52741,\"start\":52722},{\"end\":52758,\"start\":52741},{\"end\":52772,\"start\":52758},{\"end\":52788,\"start\":52772},{\"end\":52806,\"start\":52788},{\"end\":52822,\"start\":52806},{\"end\":52837,\"start\":52822},{\"end\":53319,\"start\":53302},{\"end\":53331,\"start\":53319},{\"end\":53356,\"start\":53331},{\"end\":53371,\"start\":53356},{\"end\":53685,\"start\":53660},{\"end\":53697,\"start\":53685},{\"end\":53715,\"start\":53697},{\"end\":53735,\"start\":53715},{\"end\":53752,\"start\":53735},{\"end\":53771,\"start\":53752},{\"end\":53788,\"start\":53771},{\"end\":53795,\"start\":53788},{\"end\":54186,\"start\":54151},{\"end\":54202,\"start\":54186},{\"end\":54217,\"start\":54202},{\"end\":54222,\"start\":54217},{\"end\":54597,\"start\":54580},{\"end\":55025,\"start\":55013},{\"end\":55041,\"start\":55025},{\"end\":55063,\"start\":55041},{\"end\":55078,\"start\":55063},{\"end\":55464,\"start\":55449},{\"end\":55483,\"start\":55464},{\"end\":55493,\"start\":55483},{\"end\":55501,\"start\":55493},{\"end\":55834,\"start\":55824},{\"end\":55849,\"start\":55834},{\"end\":55858,\"start\":55849},{\"end\":55869,\"start\":55858},{\"end\":55877,\"start\":55869},{\"end\":55891,\"start\":55877},{\"end\":56205,\"start\":56193},{\"end\":56220,\"start\":56205},{\"end\":56512,\"start\":56498},{\"end\":56526,\"start\":56512},{\"end\":56540,\"start\":56526},{\"end\":56551,\"start\":56540},{\"end\":56562,\"start\":56551},{\"end\":56575,\"start\":56562},{\"end\":56950,\"start\":56934},{\"end\":56962,\"start\":56950},{\"end\":56977,\"start\":56962},{\"end\":57237,\"start\":57219},{\"end\":57254,\"start\":57237},{\"end\":57270,\"start\":57254},{\"end\":57285,\"start\":57270},{\"end\":57618,\"start\":57599},{\"end\":57636,\"start\":57618},{\"end\":57651,\"start\":57636},{\"end\":57992,\"start\":57964},{\"end\":58002,\"start\":57992},{\"end\":58011,\"start\":58002},{\"end\":58030,\"start\":58011},{\"end\":58038,\"start\":58030},{\"end\":58338,\"start\":58320},{\"end\":58349,\"start\":58338},{\"end\":58364,\"start\":58349},{\"end\":58381,\"start\":58364},{\"end\":58388,\"start\":58381},{\"end\":58735,\"start\":58718},{\"end\":58747,\"start\":58735},{\"end\":58763,\"start\":58747},{\"end\":58779,\"start\":58763},{\"end\":58794,\"start\":58779},{\"end\":59120,\"start\":59103},{\"end\":59132,\"start\":59120},{\"end\":59148,\"start\":59132},{\"end\":59163,\"start\":59148},{\"end\":59493,\"start\":59476},{\"end\":59510,\"start\":59493},{\"end\":59526,\"start\":59510},{\"end\":59541,\"start\":59526},{\"end\":59882,\"start\":59865},{\"end\":59894,\"start\":59882},{\"end\":59910,\"start\":59894},{\"end\":59925,\"start\":59910},{\"end\":60224,\"start\":60205},{\"end\":60242,\"start\":60224},{\"end\":60258,\"start\":60242},{\"end\":60270,\"start\":60258},{\"end\":60285,\"start\":60270},{\"end\":60301,\"start\":60285},{\"end\":60313,\"start\":60301},{\"end\":60518,\"start\":60492},{\"end\":60533,\"start\":60518},{\"end\":60548,\"start\":60533},{\"end\":60557,\"start\":60548},{\"end\":60571,\"start\":60557},{\"end\":60590,\"start\":60571},{\"end\":61078,\"start\":61063},{\"end\":61088,\"start\":61078},{\"end\":61102,\"start\":61088},{\"end\":61320,\"start\":61305},{\"end\":61332,\"start\":61320},{\"end\":61346,\"start\":61332},{\"end\":61748,\"start\":61736},{\"end\":61762,\"start\":61748},{\"end\":61775,\"start\":61762},{\"end\":61796,\"start\":61775},{\"end\":62105,\"start\":62093},{\"end\":62116,\"start\":62105},{\"end\":62135,\"start\":62116},{\"end\":62149,\"start\":62135},{\"end\":62161,\"start\":62149},{\"end\":62497,\"start\":62485},{\"end\":62508,\"start\":62497},{\"end\":62527,\"start\":62508},{\"end\":62541,\"start\":62527},{\"end\":62553,\"start\":62541},{\"end\":62923,\"start\":62903},{\"end\":62937,\"start\":62923},{\"end\":62948,\"start\":62937},{\"end\":62962,\"start\":62948},{\"end\":62974,\"start\":62962},{\"end\":62988,\"start\":62974},{\"end\":63005,\"start\":62988},{\"end\":63014,\"start\":63005},{\"end\":63417,\"start\":63406},{\"end\":63430,\"start\":63417},{\"end\":63714,\"start\":63703},{\"end\":63729,\"start\":63714},{\"end\":63749,\"start\":63729},{\"end\":63764,\"start\":63749},{\"end\":63978,\"start\":63968},{\"end\":63992,\"start\":63978},{\"end\":64004,\"start\":63992},{\"end\":64016,\"start\":64004},{\"end\":64031,\"start\":64016},{\"end\":64045,\"start\":64031},{\"end\":64063,\"start\":64045},{\"end\":64072,\"start\":64063},{\"end\":64087,\"start\":64072},{\"end\":64500,\"start\":64488},{\"end\":64515,\"start\":64500},{\"end\":64528,\"start\":64515},{\"end\":64543,\"start\":64528},{\"end\":64556,\"start\":64543},{\"end\":64930,\"start\":64918},{\"end\":64943,\"start\":64930},{\"end\":64957,\"start\":64943},{\"end\":64964,\"start\":64957},{\"end\":64978,\"start\":64964},{\"end\":64993,\"start\":64978}]", "bib_venue": "[{\"end\":50473,\"start\":50377},{\"end\":51021,\"start\":50968},{\"end\":51492,\"start\":51434},{\"end\":51933,\"start\":51897},{\"end\":52331,\"start\":52282},{\"end\":52886,\"start\":52837},{\"end\":53400,\"start\":53371},{\"end\":53658,\"start\":53609},{\"end\":54260,\"start\":54222},{\"end\":54674,\"start\":54597},{\"end\":55126,\"start\":55078},{\"end\":55556,\"start\":55501},{\"end\":55946,\"start\":55891},{\"end\":56335,\"start\":56220},{\"end\":56655,\"start\":56591},{\"end\":57021,\"start\":56977},{\"end\":57347,\"start\":57301},{\"end\":57700,\"start\":57651},{\"end\":57962,\"start\":57912},{\"end\":58437,\"start\":58388},{\"end\":58843,\"start\":58794},{\"end\":59215,\"start\":59163},{\"end\":59590,\"start\":59541},{\"end\":59972,\"start\":59925},{\"end\":60320,\"start\":60313},{\"end\":60715,\"start\":60606},{\"end\":61061,\"start\":60984},{\"end\":61413,\"start\":61346},{\"end\":61864,\"start\":61812},{\"end\":62210,\"start\":62161},{\"end\":62608,\"start\":62553},{\"end\":63058,\"start\":63014},{\"end\":63474,\"start\":63430},{\"end\":63768,\"start\":63764},{\"end\":64136,\"start\":64087},{\"end\":64605,\"start\":64556},{\"end\":65045,\"start\":64993},{\"end\":50556,\"start\":50475},{\"end\":61435,\"start\":61415}]"}}}, "year": 2023, "month": 12, "day": 17}