{"id": 196192573, "updated": "2023-07-18 23:01:10.506", "metadata": {"title": "Towards Fine-grained Text Sentiment Transfer", "authors": "[{\"first\":\"Fuli\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Peng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Pengcheng\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Yutong\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Baobao\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Zhifang\",\"last\":\"Sui\",\"middle\":[]},{\"first\":\"Xu\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "In this paper, we focus on the task of fine-grained text sentiment transfer (FGST). This task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. Different from the conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the FTST task requires more nuanced and fine-grained control of sentiment. To remedy this, we propose a novel Seq2SentiSeq model. Specifically, the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output. Moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. In this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2949980515", "acl": "P19-1194", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/LuoLYZTCSS19", "doi": "10.18653/v1/p19-1194"}}, "content": {"source": {"pdf_hash": "608a2a208e3ced44da309a8930ef713e4a06a2f2", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/P19-1194.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/P19-1194.pdf", "status": "HYBRID"}}, "grobid": {"id": "917eb0866168c120bc0fd4abccce45471446efe7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/608a2a208e3ced44da309a8930ef713e4a06a2f2.txt", "contents": "\nTowards Fine-grained Text Sentiment Transfer\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsJuly 28 -August 2, 2019. 2019. 2013\n\nFuli Luo luofuli@pku.edu.cn \nKey Lab of Computational Linguistics\nPeking University\n\n\nPeng Li \nPattern Recognition Center\nTencent Inc\nWeChat AIChina\n\nPengcheng Yang yangpc@pku.edu.cn \nKey Lab of Computational Linguistics\nPeking University\n\n\nJie Zhou \nPattern Recognition Center\nTencent Inc\nWeChat AIChina\n\nYutong Tan \nComputer Science and Technology\nBeijing Normal University\n\n\nBaobao Chang \nKey Lab of Computational Linguistics\nPeking University\n\n\nPeng Cheng Laboratory\nChina\n\nZhifang Sui \nKey Lab of Computational Linguistics\nPeking University\n\n\nPeng Cheng Laboratory\nChina\n\nXu Sun xusun@pku.edu.cn \nKey Lab of Computational Linguistics\nPeking University\n\n\nTowards Fine-grained Text Sentiment Transfer\n\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics\nthe 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJuly 28 -August 2, 2019. 2019. 2013\nIn this paper, we focus on the task of finegrained text sentiment transfer (FGST). This task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. Different from conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the FTST task requires more nuanced and fine-grained control of sentiment. To remedy this, we propose a novel Seq2SentiSeq model.Specifically, the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output. Moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. In this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation. Our code and data, including outputs of all baselines and our model are available at https://github.com/luofuli/ Fine-grained-Sentiment-Transfer. 1\n\nIntroduction\n\nText sentiment transfer aims to rephrase the input to satisfy a given sentiment label (value) while preserving its original semantic content. It facilitates various NLP applications, such as automatically converting the attitude of review and fighting against offensive language in social media (dos Santos et al., 2018).\n\nPrevious work (Shen et al., 2017;Luo et al., 2019) on text sentiment transfer mainly focuses on the coarse-grained level: the reversal of 1 Joint work between WeChat AI and Peking University.\n\n\nInput Sentence\n\nTasty food and wonderful service.\n\n\nTarget Sentiment\n\nOutput Sentence\n\n\n0.1\n\nHorrible food and terrible service!\n\n\n0.3\n\nPlain food, slow service.\n\n\n0.5\n\nFood and service need improvement.\n\n\n0.7\n\nGood food and service.\n\n\n0.9\n\nAmazing food and perfect service!! Tar Figure 1: An example of the input and output of the fine-grained text sentiment transfer task. The output reviews describe the same content (e.g. food/service) as the input while expressing different sentiment intensity. positive and negative sentiment polarity. They are confined to scenarios where there are two discrete sentiment labels. To achieve more nuanced and precise sentiment control of text generation, we turn to fine-grained text sentiment transfer (FTST) which revises a sequence to satisfy a given sentiment intensity 2 , while keeping the semantic content unchanged. Taking Figure 1 as an example, given the same input and five sentiment intensity values ranging from 0 (most negative) to 1 (most positive), the system generates five different outputs that satisfy the corresponding sentiment intensity in a relative order.\n\nThere are two main challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018;Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018;. However, these methods are not feasible for fine-grained text sentiment transfer since the target sentiment intensity value is a real value, other than discrete labels. Second, parallel data 3 is unavailable in practice. In other words, we can only access the corpora which are labeled with fine-grained sentiment ratings or intensity values. Therefore, in the FTST task, we can not train a generative model via ground truth outputs.\n\nTo tackle the two challenges mentioned above, we propose two corresponding solutions. First, in order to control the sentiment intensity of the generated sentence, we propose a novel sentiment intensity controlled sequence-to-sequence (Seq2Seq) model Seq2SentiSeq. It incorporates the sentiment intensity value into the conventional Seq2Seq model via a Gaussian kernel layer. By this means, the model can encourage the generation of words whose sentiment intensity closer to the given intensity value during decoding. Second, due to the lack of parallel data, we can not directly train the proposed model via MLE (maximum likelihood estimation). Therefore, we propose a cycle reinforcement learning algorithm to guide the model training without any parallel data. The designed reward can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Evaluation of the FTST task is also challenging and complex. In order to build a reliable automatic evaluation, we collect human references for FTST task on the Yelp review dataset 4 via crowdsourcing and design a series of automatic metrics.\n\nThe main contributions of this work are summarized as follows:\n\n\u2022 We propose a sentiment intensity controlled generative model Seq2SentiSeq, in which a sentiment intensity value is introduced via a Gaussian kernel layer to achieve fine-grained sentiment control of the generated sentence.\n\n\u2022 In order to adapt to non-parallel data, we design a cycle reinforcement learning algorithm CycleRL to guide the model training in an unsupervised way.\n\n\u2022 Experiments show that the proposed approach can largely outperform state-of-theart systems in both automatic evaluation and human evaluation.\n\n2 Proposed Model\n\n\nTask Definition\n\nGiven an input sequence x and a target sentiment intensity value v y , the FTST task aims to generate a sequence y which not only expresses the target sentiment intensity v y , but also preserve the original semantic content of the input x. Without loss of generality, we limit the sentiment intensity value v y ranging from 0 (most negative) to 1 (most positive).\n\n\nSeq2SentiSeq: Sentiment Intensity Controlled Seq2Seq Model\n\nFigure 2 presents a sketch of the proposed Seq2SentiSeq model. The model is based on the encoder-decoder framework, which takes a source text x as the input and outputs a target sentence y with the given sentiment intensity v y . In order to control the sentiment intensity of y, we introduce a Gaussian kernel layer into the decoder.\n\n\nEncoder\n\nWe use a bidirectional RNN as the encoder to capture source content information. Each word in the source sequence x = (x 1 , \u00b7 \u00b7 \u00b7 , x m ) is firstly represented by its semantic representation mapped by semantic embedding E c . The RNN reads the semantic representations from both directions and computes the forward hidden states\n{ \u2212 \u2192 h i } m i=1 and backward hidden states { \u2190 \u2212 h i } m i=1\nfor each word. We obtain the final hidden representation of the ith word by concatenating the hidden states from\nboth directions h i = [ \u2212 \u2192 h i ; \u2190 \u2212 h i ].\n\nDecoder\n\nGiven the hidden representations {h i } m i=1 of the input sequence x and the target sentiment intensity value v y , the decoder aims to generate a sequence y which not only describes the same content as the input sequence x, but also expresses a close sentiment intensity to v y .\n\nIn order to achieve the aim of controlling sentiment during decoding, we firstly embedded each word with an additional sentiment representation, besides the original semantic representation. The semantic representation characterizes the semantic content of the word, while the sentiment representation characterizes its sentiment intensity. Formally, the hidden state s t of the decoder at timestep t is computed as follows:\ns t = f s t\u22121 , [E c (y t\u22121 ); E s (y t\u22121 )] , c t (1) \u210e ! \u210e \" \u210e # \u2026 Attention Layer Encoder \" ! \" \" \" $ Decoder \" $ # % # & Target Sentiment Intensity Value ' ! = 0.9 Gaussian Kernel Layer <eos> Semantic Embeddings Sentiment Embeddings \" , $ $ $ $ % & (( ) ) + ( $ The food is okay % , (( ) ) \" - 0.9\nThe food is where E s (y t\u22121 ) refers to the sentiment representation of the word y t\u22121 mapped by the sentiment embedding matrix E s , E c (y t\u22121 ) is the semantic representation, and the context vector c t is computed by an attention mechanism in the same way as Luong et al. (2015). Considering two goals of the FTST task: sentiment transformation and content preservation, we model the final generation probability into a mixture of semantic probability and sentiment probability, where the former evaluates content preservation and the latter measures sentiment transformation. Similar to the traditional Seq2Seq model (Bahdanau et al., 2014), the semantic probability distribution over the whole vocabulary is computed as follows:\ngood \" , extremely The food is not % & ( , ( $ + $ % ( $ + $ & (( $ ) sum Encoder Decoder - . ( ' ! Thumbnail Viewp c t = softmax(W c s t )(2)\nwhere W c is a trainable weight matrix. The sentiment probability measures how close the sentiment intensity of the generated sequence to the target v y . Normally, each word has a specific sentiment intensity. For example, the word \"okay\" has a positive intensity around 0.6, \"good\" is around 0.7, and \"great\" is around 0.8. However, when involving to the previous generated words, the sentiment intensity of current generated word may be totally different. For example, the phrase \"not good\" has a negative intensity around 0.3, while \"extremely good\" is around 0.9. That is to say, the sentiment intensity of each word at time-step t should be decided by both the sentiment representation E s and the current decoder state s t . Therefore, we define a sentiment intensity prediction function g(E s , s t ) as follows:\ng(E s , s t ) = sigmoid(E s W s s t )(3)\nwhere W s is a trainable parameter, and sigmoid is used to scale the predicted intensity value to [0, 1]. Intuitively, in order to achieve fine-grained control of sentiment, words whose sentiment intensities are closer to the target sentiment intensity value v y should be assigned a higher probability. Take Figure 2 as an example, at the 5-th time-step, word \"good\" should be assigned a higher probability than word \"bad\", thus the predicted intensity value g(\"good\", s 4 ) is closer to the target sentiment intensity than g(\"bad\", s 4 ). To favor words whose sentiment intensity is near v y , we introduce a Gaussian kernel layer which places a Gaussian distribution centered around v y , inspired by Luong et al. (2015) and Zhang et al. (2018a). Specifically, the sentiment probability is formulated as: Figure 3: Cycle reinforcement learning. Note that the upper encoder-decoder model and the lower encoderdecoder are just one Seq2SentiSeq model.\no s t = 1 \u221a 2\u03c0\u03c3 exp \u2212 g(E s , s t ) \u2212 v y Encoder Decoder ! \" # Encoder Decoder $ % & ' ( ) Sentiment Scorer $ * & )\n\nTraining: Cycle Reinforcement Learning\n\nA serious challenge of the FTST task is the lack of parallel data. Since the ground truth output y is unobserved, we can not directly use the maximum likelihood estimation (MLE) for training. To remedy this, we design a cycle reinforcement learning (CycleRL) algorithm. An overview of the training process is summarized in Algorithm 1. Two rewards are designed to encourage changing sentiment but preserving content, without the need of parallel data. The definitions of the two rewards and the corresponding gradients for Seq2SentiSeq model S are introduced as follows.\n\n\nReward Design\n\nWe design the respective rewards for two goals (sentiment transformation and content preservation) of the FTST task. Then, an overall reward r is calculated to balance these two goals and guide the model training.\n\nReward for sentiment transformation. A pretrained sentiment scorer is used to evaluate how well the sampled sentence\u0177 matches the target sentiment intensity value v y . Specifically, the reward for sentiment transformation is formulated as:\nr s = 1/(|v y \u2212 \u03d5(\u0177)| + 1)(7)\nwhere \u03d5 refers to the pre-trained sentiment scorer which is implemented as LSTM-based linear regression model. Reward for content preservation. Intuitively, if the model performs well in content preservation, it is easy to back-reconstruct the source input x. Therefore, we design the reward for content preservation to be the probability of the model reconstructing x based on the generated text\u0177 and the source sentiment intensity value v x .\nr c = p(x|\u0177, v x ; \u03b8)(8)\nwhere \u03b8 is the parameter of Seq2SentiSeq model.\n\n\nAlgorithm 1\n\nThe cycle reinforcement learning algorithm for training Seq2SentiSeq.\n\nInput: A corpora D = {(xi,i )} where each sequence xi is labeled with a fine-grained sentiment label vi 1: Initial the pseudo-parallel data V0 = {(xi,\u0177i)} 2: Pre-train Seq2SentiSeq model S \u03b8 using V0 3: for each iteration t = 1, 2, ..., T do 4:\n\nSample a sentence x from D 5:\n\nfor k = 1, 2, ..., K do 6:\n\nSample a intensity value v\n(k) y from interval [0, 1] 7:\nGenerate a target sequence:\n\u0177 (k) = S(x, v (k) y ; \u03b8) 8:\nCompute sentiment reward r (k) s based on Eq. 7 9:\n\nCompute content reward r (k) c based on Eq. 8 10:\n\nCompute total reward r (k) based on Eq. 9 11: end for 12:\n\nUpdate \u03b8 using reward {r (k) } K k=1 based on Eq. 11 13:\n\nUpdate \u03b8 using cycle reconstruction loss in Eq. 12 14: end for Overall reward. To encourage the model to improve both sentiment transformation and content preservation, the final reward r guiding the model training is designed to be the harmonic mean of the above two rewards:\nr = 1 + \u03b2 2 r c \u00b7 r s (\u03b2 2 \u00b7 r c ) + r s (9)\nwhere \u03b2 is a harmonic weight that controls the trade-off between two rewards.\n\n\nOptimization\n\nThe goal of RL training is to minimize the negative expected reward,\nL(\u03b8) = \u2212 k r (k) p \u03b8 (\u0177 (k) |x)(10)\nwhere\u0177 (k) is the k-th sampled sequence according to probability distribution p in Eq. 6, r (k) is the reward of\u0177 (k) , and \u03b8 is the parameter of the proposed model in Figure 2. By means of policy gradient method (Williams, 1992), for each training example, the expected gradient of Eq. 10 can be approximated as:\n\u2207 \u03b8 L(\u03b8) \u2212 1 K K k=1 r (k) \u2212 b \u2207 \u03b8 log p \u03b8 (\u0177 (k) )(11)\nwhere K is the sample size and b is the greedy search decoding baseline that aims to reduce the variance of gradient estimate which is implemented in the same way as Paulus et al. (2017).\n\nNevertheless, RL training strives to optimize a specific metric which may not guarantee the fluency of the generated text (Paulus et al., 2017), and usually faces the unstable training problems (Li et al., 2017). The most direct way is to expose the sentences which are from the training corpus to the decoder and trained via MLE (also called teacher-forcing). In order to expose the decoder to the original sentence from the training corpus, we borrow ideas from back-translation (Lample et al., 2018a,b). Specifically, the model first generates a sequence\u0177 based on the input text x and the target sentiment intensity value v y , and then reconstructs the source input x based on\u0177 and the source sentiment intensity value v x . Therefore, the gradient of the cycle reconstruction loss is defined as:\n\u2207 \u03b8 J (\u03b8) = \u2207 \u03b8 log p x|S(x, v y ; \u03b8), v x ; \u03b8(12)\nwhere S refers to the Seq2SeniSeq model.\n\nFinally, we alternately update the model parameters \u03b8 based on Eq. 11 and Eq. 12.\n\n\nExperimental Setup\n\nIn this section, we introduce the dataset, experiment settings, baselines, and evaluation metrics.\n\n\nDataset\n\nWe conduct experiments on the Yelp dataset 5 , which consists of a large number of product reviews. Each review is assigned a sentiment rating ranging from 1 to 5. Since the label inconsistency between human is more serious in fine-grained ratings, we average the ratings for the sentences which have a Jaccard Similarity more than 0.9. Then, averaged ratings are normalized between 0 and 1 as the sentiment intensity. Other data preprocessing is the same as Shen et al. (2017). Finally, we obtain a total of 640K sentences. We randomly hold 630K for training, 10K for validation, and 500 for testing. Even though the sentiment intensity distribution of training dataset is not uniform, the proposed framework consists of a uniform data augmentation which generates sentences whose intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1).\n\n\nExperiment Settings\n\nWe tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight \u03b2 in Eq. 9 is 1 and \u03b3 in Eq. 6 is 0.5. The standard deviation \u03c3 is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10 \u22123 initial learning rate for pre-training and 10 \u22125 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting.\n\n\nBaselines\n\nWe compare our proposed method with the following two series of state-of-the-art systems.\n\nFine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pseudo-parallel corpus to train a model which is a combination of a revised-VAE and a coupling component modeling pseudo-parallel data with three extra losses L extra . What's more, we also consider SC-Seq2Seq (Zhang et al., 2018a) which is a specificity controlled Seq2Seq model proposed in dialogue generation. In order to adapt to this unsupervised task, the proposed CycleRL training algorithm is used to train the SC-Seq2Seq model.\n\nCoarse-grained systems aim to reverse the sentiment polarity (positive/negative) of the input, which can be regarded as a special case where the sentiment intensity is set below average (negative) or above average (positive). We compare our proposed method with the following state-of-the-art systems: CrossAlign (Shen et al., 2017), MultiDecoder (Fu et al., 2018), DeleteRetrieve  and Unpaired (Xu et al., 2018).\n\n\nEvaluation Metrics\n\nWe adopt both automatic and human evaluation.\n\n\nAutomatic Evaluation\n\nAutomatic evaluation of FTST is an open and challenging issue, thereby we adopt a combination of multiple evaluation methods.\n\nContent: To evaluate the content preservation performance, we hired crowd-workers on Crowd-Flower 6 to write human references. 7 For each review in the test dataset, crowd-workers are required to write five references with sentiment intensity value from V = [0.1, 0.3, 0.5, 0.7, 0.9]. Therefore, the BLEU (Papineni et al., 2002) score between the human reference and the corresponding generated text of the same sentiment intensity can evaluate the content preservation performance. Fluency: To measure the fluency, we calculate the perplexity (PPL) of each generated sequence via a pre-trained bi-directional LSTM language model (Mousa and Schuller, 2017).\n\nSentiment: In order to measure how close the sentiment intensity of outputs to the target intensity values, we define three metrics. Given an input sentence x and a list of target intensity values V = [v 1 , v 2 , ..., v N ], the corresponding outputs of the model are [\u0177 1 ,\u0177 2 , ...,\u0177 N ]. We then use a pre-trained sentiment regression scorer to predict the sentiment intensity values of outputs asV = [v 1 ,v 2 , ...,v N ]. Following Liao et al. (2018), we use the mean absolute error (MAE = 1 N N i=1 |v i \u2212v i |) between V andV to measure the absolute gap.\n\nMoreover, for fine-grained text sentiment transfer task, we expect that given a higher sentiment intensity value, the model will generate a more positive sentence. That is to say, the relative intensity ranking of all generated sentences of the same input is also important. Inspired by the Mean Reciprocal Rank metric which is widely used in the Information Retrieval area, we design a Mean Relative Reciprocal Rank (MRRR) metric to measure the relative ranking\nMRRR = 1 N N i=1 1 |rank(vi) \u2212 rank(vi)| + 1(13)\nIn addition, we also compare our model with the coarse-grained sentiment transfer systems. In order to make the results comparable, we define the generated test samples of all baselines for reproducibility. sentiment intensity larger/smaller than 0.5 as positive/negative results. Then we use a pre-trained binary TextCNN classifier (Kim, 2014) to compute the classification accuracy.\n\n\nHuman Evaluation\n\nWe also perform human evaluation to assess the quality of generated sentences more accurately. Each item contains the source input, the sampled target sentiment intensity value, and the output of different systems. Then 500 items are distributed to 3 evaluators, who are required to score the generated sentences from 1 to 5 based on the input and target sentiment intensity value in terms of three criteria: content, sentiment, fluency. Content evaluates the content preservation degree. Sentiment refers to how much the output matches the target sentiment intensity. Fluency is designed to measure whether the generated texts are fluent. For each metric, the average Pearson correlation coefficient of the scores given by three evaluators is greater than 0.71, which ensures the interevaluator agreement.\n\n\nResults and Discussion\n\n\nEvaluation Results\n\nThe automatic evaluation and human evaluation results are shown in Table 1. It shows that our approach achieves the best performance in all metrics. More specifically, we have the following observations: (1) The proposed model Seq2SentiSeq obtains 8.6/3.1/0.98 points absolute improvement over the best results on BLEU-1/BLEU-2/Content score. It demonstrates the effectiveness of our approach in improving the content preservation of the input sentences.\n\n(2) Our model can more precisely control the sentiment intensity from human scores on sentiment, and it can also obtain both best results in sentiment mean absolute error (MAE) and relative sentiment rank (MRRR).\n\n\nModel\n\nAutomatic   However, SC-Seq2Seq gets the second best MAE score while Revised-VAE + L extra gets the second best MRRR score. We can infer that the two models excel at different aspects. And MRRR provides a different perspective on the sentiment results.\nEvaluation Human Evaluation\n(3) The proposed model can generate more fluent sentences than all baselines. The main reason for these three phenomenons is that we design two rewards that can directly ensure the content preservation and sentiment transformation in the cycle reinforcement training process. In addition, the cycle reconstruction loss can effectively guarantee the fluency of generated sentences, which has been further verified in the ablation study. What's more, we also simplify our task to the setting of coarse-grained (positive/negative) sentiment transfer task. Table 3 shows the binary sentiment accuracy of the representative systems. We can find that the proposed model achieve the best results over the fine-grained systems, and it is comparable to the best coarse-grained system.\n\n\nAblation Study\n\nIn this section, we further discuss the impacts of the components of the proposed model. We retrain our model by ablating multiple components of our model: without pre-training, without cycle reconstruction (Eq. 12), without reinforcement learning ( Eq. 11). Table 2 shows the corresponding automatic and human evaluations. The perfor-Input the beer isn't bad, but the food was less than desirable.\n\n\nOutput Seq2SentiSeq\n\nV=0.1 the beer is terrible, and the food was the worst. V=0.3 the beer wasn't bad, and the food wasn't great too. V=0.5 the food is ok, but not worth the drive to the strip. V=0.7 the beer is good, and the food is great. V=0.9\n\nthe wine is great, and the food is extremely fantastic.\n\n\nOutput\n\nRevised-VAE + L extra V=0.1 n't no about about no when about that was when about V=0.3 the beer sucks , but the food is not typical time. V=0.5 the beer is cheap, but the food was salty and decor. V=0.7\n\ni just because decent management salty were impersonal. V=0.9\n\nn't that about was that when was about as when was mance declines most when without pre-training. This reveals that reinforcement learning is heavily dependent on pre-training as a warm start because it is hard for RL architecture to train from scratch. Moreover, no pre-training will lead the model to generate frequent words and short sentence which gets low PPL score. What's more, the performance of ablated version without cycle reconstruction also drops significantly, since cycle reconstruction plays an important role of teacherforcing in our paper. Finally, even though the proposed Seq2SentiSeq without reinforcement learning can beat the best baseline in terms of human average score, reinforcement learning still helps to boost the performance of the proposed model by a large margin. Table 4 shows the example outputs on the YELP datasets with five sentiment intensity values. This case demonstrates that our model can both preserve the content (\"beer\", \"food\") and change the sentiment to the desired intensity. More importantly, our model can capture the subtle sentiment difference of the words or phrases, e.g., \"the worst\" \u2192 \"bad\" \u2192 \"ok\" \u2192 \"good\" \u2192 \"extremely fantastic\". However, the Revised-VAE + L extra system does not show this sentiment trend and may collapse when intensity value V is very small (0.1) or very big (0.9). And our model sometimes may also suffer from semantic drift, e.g., \"beer\" is revised to \"wine\".\n\n\nCase Study\n\n\nAnalysis on Sentiment Representation\n\nWe also conduct analysis to understand the sentiment representations of words introduced in our model. We use the 1000 most frequent words from the training dataset. Then, we use a human annotated sentiment lexicon (Hutto and Gilbert, 2014) to classify them into three categories: positive, neutral and negative. After that, we get 112 positive words, 841 neutral words and 47 negative words. Finally, we apply t-SNE (Rauber et al., 2016) to visualize both semantic and sentiment embeddings of the proposed model ( Figure  2) when finished training. As shown in Figure 4, we can see that the distributions of the two embeddings are significantly different. In the semantic embedding space, most of the positive words and negative words lie closely. On the contrary, in the sentiment embedding space, positive words are far from negative words. In conclusion, neighbors on semantic embedding space are semantically related, while neighbors on sentiment embedding space express a similar sentiment intensity.\n\n\nRelated Work\n\nRecently, there is a growing literature on the task of unsupervised sentiment transfer. This task aims to reverse the sentiment polarity of a sentence but keep its content unchanged without parallel data (Fu et al., 2018;Tsvetkov et al., 2018;Xu et al., 2018;Lample et al., 2019). However, there are few researches focus on the fine-grained control of sentiment. Liao et al. (2018) exploits pseudo-parallel data via heuristic rules, thus turns this task to a supervised setting. They then propose a model based on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from them, we dynamically update the pseudo-parallel data via on-the-fly back-translation (Lample et al., 2018b) during training (Eq. 12).\n\nThere are some other tasks of NLP also show interest in controlling the fine-grained attribute of text generation. For example, Zhang et al. (2018a) and Ke et al. (2018) propose to control the specificity and diversity in dialogue generation. We borrow ideas from these works but the motivation and proposed models of our work are a far cry from them. The main differences are:\n\n(1) Since sentiment is dependent on local context while specificity is independent of local context, there is a series of design in our model to take the local context (or previous generated words) s t into consideration (e.g., Eq. 1, Eq. 3). (2) Due to the lack of parallel data, we propose a cycle reinforcement learning algorithm to train the proposed model (Section 2.3).\n\n\nConclusion\n\nIn this paper, we focus on solving the finegrained text sentiment transfer task, which is a natural extension of the binary sentiment transfer task but with more challenges. We propose a Seq2SentiSeq model to achieve the aim of controlling the fine-grained sentiment intensity of the generated sentence. In order to train the proposed model without any parallel data, we design a cycle reinforcement learning algorithm. We apply the proposed approach to the Yelp review dataset, obtaining state-of-the-art results in both automatic evaluation and human evaluation.\n\nFigure 2 :\n2The proposed sequence to sentiment controlled sequence (Seq2SentiSeq) model.\n\nFigure 4\n4: t-SNE visualization of the semantic embeddings (upper) and sentiment embeddings (lower) in the Seq2SentiSeq model.\n\n\n1\u2191 BLEU-2\u2191 MAE\u2193 MRRR\u2191 PPL\u2193 Content\u2191 Sentiment\u2191 Fluency\u2191 Avg\u2191 Avg shows the average human scores. \u2191 denotes larger is better, and vice versa. Bold denotes the best results.Model \nAutomatic Evaluation \nHuman Evaluation \nBLEU-Revised-VAE \n22.6 \n7.2 \n0.24 \n0.62 \n102.2 \n2.64 \n2.52 \n2.13 \n2.43 \nRevised-VAE + Lextra \n20.7 \n5.7 \n0.18 \n0.67 \n102.6 \n2.54 \n3.84 \n2.14 \n2.84 \nSC-Seq2Seq \n23.9 \n3.8 \n0.25 \n0.69 \n41.2 \n2.37 \n3.85 \n3.41 \n3.21 \n\nSeq2SentiSeq \n32.5 \n10.3 \n0.13 \n0.78 \n35.1 \n3.62 \n4.09 \n4.17 \n3.96 \n\nHuman Reference \n100.0 \n100.0 \n0.07 \n0.83 \n31.2 \n4.51 \n4.36 \n4.75 \n4.54 \n\nTable 1: Automatic evaluation and human evaluation in three aspects: Content (BLUE-1, BLUE-2), Sentiment \n(MAE, MRRR) and Fluency (PPL). \n\n\nBLEU-1\u2191 BLEU-2\u2191 MAE\u2193 MRRR\u2191 PPL\u2193 Content\u2191 Sentiment\u2191 Fluency\u2191 Avg\u2191Full Model \n32.5 \n10.3 \n0.13 \n0.78 \n35.1 \n3.62 \n4.09 \n4.17 \n3.96 \n\nw/o Pre-training \n14.3 \n0.7 \n0.32 \n0.48 \n7.2 \n1.01 \n1.30 \n3.86 \n2.06 \nw/o Cycle reconstruction \n16.5 \n2.3 \n0.31 \n0.41 \n70.1 \n1.92 \n1.48 \n3.16 \n2.19 \nw/o Reinforcement learning \n25.7 \n4.1 \n0.22 \n0.63 \n46.0 \n2.69 \n3.74 \n3.80 \n3.41 \n\n\n\nTable 2 :\n2Automatic evaluation and human evaluation of ablation study.Model \nneg-to-pos pos-to-neg \n\nMultidecoder \n54.3 \n50.2 \nCrossAlign \n73.3 \n71.7 \nUnpaired \n78.9 \n73.0 \nDeleteRetrieve \n89.6 \n83.1 \n\nRevised-VAE \n64.3 \n62.0 \nRevised-VAE + Lextra \n89.3 \n77.9 \nSC-Seq2Seq \n67.2 \n59.6 \nSeq2SentiSeq \n89.4 \n83.5 \n\n\n\nTable 3 :\n3Binary sentiment classification accuracy of \nthe coarse-grained (upper) and fine-grained (lower) text \nsentiment transfer systems. Bold denotes the best re-\nsults of each task. \n\n\n\nTable 4 :\n4Example outputs with five sentiment intensity \nvalues V ranging from 0 to 1. \n\n\nThe sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis(Zhang et al., 2017;Mohammad et al., 2018).\nParallel data in this paper denotes the corpus where each pair of sentences describes the same content while expressing the different sentiment intensity. 4 https://www.yelp.com/dataset\n2\u03c3 2(4)p s t = softmax(o s t )(5)where \u03c3 is the standard deviation.To balance both sentiment transformation and content preservation, the final probability distribution p t over the entire vocabulary is defined as a mixture of two probability distributions:p t = \u03b3p s t + (1 \u2212 \u03b3)p c t(6)where \u03b3 is the hyper-parameter that controls the trade-off between two generation probabilities.\nhttps://www.yelp.com/dataset\nhttps://www.crowdflower.com/ 7 We will release the collected human references and the\nAcknowledgmentsThis paper is supported by NSFC project 61751201, 61772040 and 61876004. The contact authors are Baobao Chang and Zhifang Sui.\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsICLRDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2014. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Represen- tations, ICLR 2014.\n\nStyle transfer in text: Exploration and evaluation. Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, Rui Yan, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI-18. the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI-18Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui Yan. 2018. Style transfer in text: Exploration and evaluation. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelli- gence, AAAI-18, pages 663-670.\n\nVADER: A parsimonious rule-based model for sentiment analysis of social media text. J Clayton, Eric Hutto, Gilbert, Proceedings of the Eighth International Conference on Weblogs and Social Media. the Eighth International Conference on Weblogs and Social MediaICWSMClayton J. Hutto and Eric Gilbert. 2014. VADER: A parsimonious rule-based model for sentiment anal- ysis of social media text. In Proceedings of the Eighth International Conference on Weblogs and Social Media, ICWSM 2014.\n\nGenerating informative responses with controlled sentence function. Pei Ke, Jian Guan, Minlie Huang, Xiaoyan Zhu, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsPei Ke, Jian Guan, Minlie Huang, and Xiaoyan Zhu. 2018. Generating informative responses with con- trolled sentence function. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics, ACL 2018.\n\nConvolutional neural networks for sentence classification. Yoon Kim, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingYoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan- guage Processing, EMNLP 2014, pages 1746-1751.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, Proceedings of International Conference on Learning Representations. International Conference on Learning RepresentationsDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In Proceedings of International Conference on Learning Represen- tations, ICLR 2014.\n\nUnsupervised machine translation using monolingual corpora only. Guillaume Lample, Alexis Conneau, Ludovic Denoyer, Marc&apos;aurelio Ranzato, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsGuillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato. 2018a. Unsupervised machine translation using monolingual corpora only. In Proceedings of the International Conference on Learning Representations, ICLR 2018.\n\nPhrase-based & neural unsupervised machine translation. Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc&apos;aurelio Ranzato, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingGuillaume Lample, Myle Ott, Alexis Conneau, Lu- dovic Denoyer, and Marc'Aurelio Ranzato. 2018b. Phrase-based & neural unsupervised machine trans- lation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process- ing, EMNLP 2018, pages 5039-5049.\n\nMultiple-attribute text rewriting. Guillaume Lample, Sandeep Subramanian, Eric Smith, Ludovic Denoyer, Marc&apos;aurelio Ranzato, Y-Lan Boureau, International Conference on Learning Representations. ICLR 2019Guillaume Lample, Sandeep Subramanian, Eric Smith, Ludovic Denoyer, Marc'Aurelio Ranzato, and Y- Lan Boureau. 2019. Multiple-attribute text rewrit- ing. In International Conference on Learning Rep- resentations, ICLR 2019.\n\nAdversarial learning for neural dialogue generation. Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, Dan Jurafsky, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingJiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, and Dan Jurafsky. 2017. Adversarial learning for neural dialogue generation. In Proceed- ings of the 2017 Conference on Empirical Meth- ods in Natural Language Processing, EMNLP 2017, pages 2157-2169.\n\nDelete, retrieve, generate: a simple approach to sentiment and style transfer. Juncen Li, Robin Jia, He He, Percy Liang, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJuncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to sen- timent and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, pages 1865-1874.\n\nQuaSE: Sequence editing under quantifiable guidance. Yi Liao, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, Tong Zhang, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingYi Liao, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, and Tong Zhang. 2018. QuaSE: Sequence editing under quantifiable guidance. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018, pages 3855- 3864.\n\nA dual reinforcement learning framework for unsupervised text style transfer. Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Zhifang Sui, Xu Sun, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial Intelligence2019Fuli Luo, Peng Li, Jie Zhou, Pengcheng Yang, Baobao Chang, Zhifang Sui, and Xu Sun. 2019. A dual rein- forcement learning framework for unsupervised text style transfer. In Proceedings of the 28th Interna- tional Joint Conference on Artificial Intelligence, IJ- CAI 2019.\n\nEffective approaches to attention-based neural machine translation. Thang Luong, Hieu Pham, Christopher D Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingThang Luong, Hieu Pham, and Christopher D. Man- ning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, pages 1412- 1421.\n\nSaif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, Svetlana Kiritchenko, Proceedings of The 12th International Workshop on Semantic Evaluation. The 12th International Workshop on Semantic EvaluationSemEval-2018 task 1: Affect in tweetsSaif Mohammad, Felipe Bravo-Marquez, Moham- mad Salameh, and Svetlana Kiritchenko. 2018. SemEval-2018 task 1: Affect in tweets. In Proceed- ings of The 12th International Workshop on Seman- tic Evaluation, SemEval@NAACL-HLT, pages 1-17.\n\nContextual bidirectional long short-term memory recurrent neural network language models: A generative approach to sentiment analysis. Amr Eldesoky Mousa, Bjorn W Schuller, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsAmr Eldesoky Mousa and Bjorn W Schuller. 2017. Contextual bidirectional long short-term memory re- current neural network language models: A genera- tive approach to sentiment analysis. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, pages 1023- 1032.\n\nBLEU: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL 2002, pages 311- 318.\n\nA deep reinforced model for abstractive summarization. Romain Paulus, Caiming Xiong, Richard Socher, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsRomain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization. In Proceedings of the Interna- tional Conference on Learning Representations, ICLR 2017.\n\nVisualizing time-dependent data using dynamic t-SNE. Paulo E Rauber, Alexandre X Falc\u00e3o, Alexandru C Telea, Eurographics Conference on Visualization. Paulo E. Rauber, Alexandre X. Falc\u00e3o, and Alexan- dru C. Telea. 2016. Visualizing time-dependent data using dynamic t-SNE. In Eurographics Conference on Visualization, EuroVis 2016, pages 73-77.\n\nFighting offensive language on social media with unsupervised text style transfer. C\u00edcero Nogueira Dos Santos, Igor Melnyk, Inkit Padhi, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018. the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018C\u00edcero Nogueira dos Santos, Igor Melnyk, and Inkit Padhi. 2018. Fighting offensive language on so- cial media with unsupervised text style transfer. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics, ACL 2018, pages 189-194.\n\nStyle transfer from non-parallel text by cross-alignment. Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi S Jaakkola, Advances in Neural Information Processing Systems. Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. 2017. Style transfer from non-parallel text by cross-alignment. In Advances in Neural Informa- tion Processing Systems, NIPS 2017, pages 6833- 6844.\n\nDropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Journal of Machine Learning Research. Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. 2014. Dropout: a simple way to prevent neural networks from overfitting. In Journal of Machine Learning Research, pages 1929-1958.\n\nStyle transfer through back-translation. Yulia Tsvetkov, Alan W Black, Ruslan Salakhutdinov, Shrimai Prabhumoye, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018. the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018Yulia Tsvetkov, Alan W. Black, Ruslan Salakhutdi- nov, and Shrimai Prabhumoye. 2018. Style trans- fer through back-translation. In Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics, ACL 2018, pages 866-876.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. Ronald J Williams, Machine Learning. Ronald J. Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. In Machine Learning, pages 229- 256.\n\nUnpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach. Jingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xuancheng Ren, Houfeng Wang, Wenjie Li, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsJingjing Xu, Xu Sun, Qi Zeng, Xiaodong Zhang, Xu- ancheng Ren, Houfeng Wang, and Wenjie Li. 2018. Unpaired sentiment-to-sentiment translation: A cy- cled reinforcement learning approach. In Proceed- ings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, pages 979-988.\n\nLearning to control the specificity in neural response generation. Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, Jun Xu, Xueqi Cheng, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018. the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, Jun Xu, and Xueqi Cheng. 2018a. Learning to con- trol the specificity in neural response generation. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics, ACL 2018, pages 1108-1117.\n\nYNU-HPCC at EmoInt-2017: Using a CNN-LSTM model for sentiment intensity prediction. You Zhang, Hang Yuan, Jin Wang, Xuejie Zhang, Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media AnalysisYou Zhang, Hang Yuan, Jin Wang, and Xuejie Zhang. 2017. YNU-HPCC at EmoInt-2017: Using a CNN- LSTM model for sentiment intensity prediction. In Proceedings of the 8th Workshop on Computa- tional Approaches to Subjectivity, Sentiment and So- cial Media Analysis, WASSA@EMNLP 2017, pages 200-204.\n\nStyle transfer as unsupervised machine translation. Zhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang, Peng Chen, Mu Li, Ming Zhou, Enhong Chen, arXivpreprintarXiv:1808.07894Zhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang, Peng Chen, Mu Li, Ming Zhou, and Enhong Chen. 2018b. Style transfer as unsupervised machine translation. In arXiv preprint arXiv:1808.07894.\n", "annotations": {"author": "[{\"end\":260,\"start\":175},{\"end\":324,\"start\":261},{\"end\":415,\"start\":325},{\"end\":480,\"start\":416},{\"end\":552,\"start\":481},{\"end\":652,\"start\":553},{\"end\":751,\"start\":653},{\"end\":833,\"start\":752},{\"end\":260,\"start\":175},{\"end\":324,\"start\":261},{\"end\":415,\"start\":325},{\"end\":480,\"start\":416},{\"end\":552,\"start\":481},{\"end\":652,\"start\":553},{\"end\":751,\"start\":653},{\"end\":833,\"start\":752}]", "publisher": "[{\"end\":87,\"start\":46},{\"end\":1096,\"start\":1055},{\"end\":87,\"start\":46},{\"end\":1096,\"start\":1055}]", "author_last_name": "[{\"end\":183,\"start\":180},{\"end\":268,\"start\":266},{\"end\":339,\"start\":335},{\"end\":424,\"start\":420},{\"end\":491,\"start\":488},{\"end\":565,\"start\":560},{\"end\":664,\"start\":661},{\"end\":758,\"start\":755},{\"end\":183,\"start\":180},{\"end\":268,\"start\":266},{\"end\":339,\"start\":335},{\"end\":424,\"start\":420},{\"end\":491,\"start\":488},{\"end\":565,\"start\":560},{\"end\":664,\"start\":661},{\"end\":758,\"start\":755}]", "author_first_name": "[{\"end\":179,\"start\":175},{\"end\":265,\"start\":261},{\"end\":334,\"start\":325},{\"end\":419,\"start\":416},{\"end\":487,\"start\":481},{\"end\":559,\"start\":553},{\"end\":660,\"start\":653},{\"end\":754,\"start\":752},{\"end\":179,\"start\":175},{\"end\":265,\"start\":261},{\"end\":334,\"start\":325},{\"end\":419,\"start\":416},{\"end\":487,\"start\":481},{\"end\":559,\"start\":553},{\"end\":660,\"start\":653},{\"end\":754,\"start\":752}]", "author_affiliation": "[{\"end\":259,\"start\":204},{\"end\":323,\"start\":270},{\"end\":414,\"start\":359},{\"end\":479,\"start\":426},{\"end\":551,\"start\":493},{\"end\":622,\"start\":567},{\"end\":651,\"start\":624},{\"end\":721,\"start\":666},{\"end\":750,\"start\":723},{\"end\":832,\"start\":777},{\"end\":259,\"start\":204},{\"end\":323,\"start\":270},{\"end\":414,\"start\":359},{\"end\":479,\"start\":426},{\"end\":551,\"start\":493},{\"end\":622,\"start\":567},{\"end\":651,\"start\":624},{\"end\":721,\"start\":666},{\"end\":750,\"start\":723},{\"end\":832,\"start\":777}]", "title": "[{\"end\":45,\"start\":1},{\"end\":878,\"start\":834},{\"end\":45,\"start\":1},{\"end\":878,\"start\":834}]", "venue": "[{\"end\":967,\"start\":880},{\"end\":967,\"start\":880}]", "abstract": "[{\"end\":2364,\"start\":1132},{\"end\":2364,\"start\":1132}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2700,\"start\":2675},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2736,\"start\":2717},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2753,\"start\":2736},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4300,\"start\":4283},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4320,\"start\":4300},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4392,\"start\":4375},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9223,\"start\":9204},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9586,\"start\":9563},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11404,\"start\":11385},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11429,\"start\":11409},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14858,\"start\":14842},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15185,\"start\":15165},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15331,\"start\":15310},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15399,\"start\":15382},{\"end\":15693,\"start\":15669},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16774,\"start\":16756},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17680,\"start\":17662},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18022,\"start\":17997},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18268,\"start\":18250},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18510,\"start\":18489},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19049,\"start\":19030},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19081,\"start\":19064},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19129,\"start\":19112},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19678,\"start\":19655},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20006,\"start\":19980},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20465,\"start\":20447},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21429,\"start\":21418},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27013,\"start\":26992},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27819,\"start\":27802},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27841,\"start\":27819},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27857,\"start\":27841},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27877,\"start\":27857},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27979,\"start\":27961},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28526,\"start\":28504},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28702,\"start\":28682},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28723,\"start\":28707},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31937,\"start\":31917},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31959,\"start\":31937},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2700,\"start\":2675},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2736,\"start\":2717},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2753,\"start\":2736},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4300,\"start\":4283},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4320,\"start\":4300},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4392,\"start\":4375},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9223,\"start\":9204},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9586,\"start\":9563},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11404,\"start\":11385},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11429,\"start\":11409},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14858,\"start\":14842},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15185,\"start\":15165},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15331,\"start\":15310},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15399,\"start\":15382},{\"end\":15693,\"start\":15669},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16774,\"start\":16756},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17680,\"start\":17662},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18022,\"start\":17997},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18268,\"start\":18250},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18510,\"start\":18489},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19049,\"start\":19030},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19081,\"start\":19064},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19129,\"start\":19112},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19678,\"start\":19655},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20006,\"start\":19980},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20465,\"start\":20447},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21429,\"start\":21418},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27013,\"start\":26992},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27819,\"start\":27802},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27841,\"start\":27819},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27857,\"start\":27841},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27877,\"start\":27857},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27979,\"start\":27961},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28526,\"start\":28504},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28702,\"start\":28682},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28723,\"start\":28707},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31937,\"start\":31917},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31959,\"start\":31937}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":29977,\"start\":29888},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30105,\"start\":29978},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30820,\"start\":30106},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31186,\"start\":30821},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31501,\"start\":31187},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31693,\"start\":31502},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31785,\"start\":31694},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29977,\"start\":29888},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30105,\"start\":29978},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30820,\"start\":30106},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31186,\"start\":30821},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31501,\"start\":31187},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31693,\"start\":31502},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31785,\"start\":31694}]", "paragraph": "[{\"end\":2701,\"start\":2380},{\"end\":2894,\"start\":2703},{\"end\":2946,\"start\":2913},{\"end\":2982,\"start\":2967},{\"end\":3025,\"start\":2990},{\"end\":3058,\"start\":3033},{\"end\":3100,\"start\":3066},{\"end\":3130,\"start\":3108},{\"end\":4017,\"start\":3138},{\"end\":4827,\"start\":4019},{\"end\":5968,\"start\":4829},{\"end\":6032,\"start\":5970},{\"end\":6258,\"start\":6034},{\"end\":6412,\"start\":6260},{\"end\":6557,\"start\":6414},{\"end\":6575,\"start\":6559},{\"end\":6959,\"start\":6595},{\"end\":7356,\"start\":7022},{\"end\":7698,\"start\":7368},{\"end\":7874,\"start\":7762},{\"end\":8211,\"start\":7930},{\"end\":8637,\"start\":8213},{\"end\":9675,\"start\":8940},{\"end\":10639,\"start\":9819},{\"end\":11632,\"start\":10681},{\"end\":12361,\"start\":11791},{\"end\":12592,\"start\":12379},{\"end\":12834,\"start\":12594},{\"end\":13309,\"start\":12865},{\"end\":13382,\"start\":13335},{\"end\":13467,\"start\":13398},{\"end\":13713,\"start\":13469},{\"end\":13744,\"start\":13715},{\"end\":13772,\"start\":13746},{\"end\":13800,\"start\":13774},{\"end\":13858,\"start\":13831},{\"end\":13938,\"start\":13888},{\"end\":13989,\"start\":13940},{\"end\":14048,\"start\":13991},{\"end\":14106,\"start\":14050},{\"end\":14384,\"start\":14108},{\"end\":14507,\"start\":14430},{\"end\":14592,\"start\":14524},{\"end\":14942,\"start\":14629},{\"end\":15186,\"start\":14999},{\"end\":15989,\"start\":15188},{\"end\":16081,\"start\":16041},{\"end\":16164,\"start\":16083},{\"end\":16285,\"start\":16187},{\"end\":17185,\"start\":16297},{\"end\":18052,\"start\":17209},{\"end\":18155,\"start\":18066},{\"end\":18715,\"start\":18157},{\"end\":19130,\"start\":18717},{\"end\":19198,\"start\":19153},{\"end\":19348,\"start\":19223},{\"end\":20007,\"start\":19350},{\"end\":20571,\"start\":20009},{\"end\":21035,\"start\":20573},{\"end\":21469,\"start\":21085},{\"end\":22296,\"start\":21490},{\"end\":22798,\"start\":22344},{\"end\":23012,\"start\":22800},{\"end\":23274,\"start\":23022},{\"end\":24078,\"start\":23303},{\"end\":24495,\"start\":24097},{\"end\":24745,\"start\":24519},{\"end\":24802,\"start\":24747},{\"end\":25015,\"start\":24813},{\"end\":25078,\"start\":25017},{\"end\":26521,\"start\":25080},{\"end\":27581,\"start\":26575},{\"end\":28552,\"start\":27598},{\"end\":28931,\"start\":28554},{\"end\":29308,\"start\":28933},{\"end\":29887,\"start\":29323},{\"end\":2701,\"start\":2380},{\"end\":2894,\"start\":2703},{\"end\":2946,\"start\":2913},{\"end\":2982,\"start\":2967},{\"end\":3025,\"start\":2990},{\"end\":3058,\"start\":3033},{\"end\":3100,\"start\":3066},{\"end\":3130,\"start\":3108},{\"end\":4017,\"start\":3138},{\"end\":4827,\"start\":4019},{\"end\":5968,\"start\":4829},{\"end\":6032,\"start\":5970},{\"end\":6258,\"start\":6034},{\"end\":6412,\"start\":6260},{\"end\":6557,\"start\":6414},{\"end\":6575,\"start\":6559},{\"end\":6959,\"start\":6595},{\"end\":7356,\"start\":7022},{\"end\":7698,\"start\":7368},{\"end\":7874,\"start\":7762},{\"end\":8211,\"start\":7930},{\"end\":8637,\"start\":8213},{\"end\":9675,\"start\":8940},{\"end\":10639,\"start\":9819},{\"end\":11632,\"start\":10681},{\"end\":12361,\"start\":11791},{\"end\":12592,\"start\":12379},{\"end\":12834,\"start\":12594},{\"end\":13309,\"start\":12865},{\"end\":13382,\"start\":13335},{\"end\":13467,\"start\":13398},{\"end\":13713,\"start\":13469},{\"end\":13744,\"start\":13715},{\"end\":13772,\"start\":13746},{\"end\":13800,\"start\":13774},{\"end\":13858,\"start\":13831},{\"end\":13938,\"start\":13888},{\"end\":13989,\"start\":13940},{\"end\":14048,\"start\":13991},{\"end\":14106,\"start\":14050},{\"end\":14384,\"start\":14108},{\"end\":14507,\"start\":14430},{\"end\":14592,\"start\":14524},{\"end\":14942,\"start\":14629},{\"end\":15186,\"start\":14999},{\"end\":15989,\"start\":15188},{\"end\":16081,\"start\":16041},{\"end\":16164,\"start\":16083},{\"end\":16285,\"start\":16187},{\"end\":17185,\"start\":16297},{\"end\":18052,\"start\":17209},{\"end\":18155,\"start\":18066},{\"end\":18715,\"start\":18157},{\"end\":19130,\"start\":18717},{\"end\":19198,\"start\":19153},{\"end\":19348,\"start\":19223},{\"end\":20007,\"start\":19350},{\"end\":20571,\"start\":20009},{\"end\":21035,\"start\":20573},{\"end\":21469,\"start\":21085},{\"end\":22296,\"start\":21490},{\"end\":22798,\"start\":22344},{\"end\":23012,\"start\":22800},{\"end\":23274,\"start\":23022},{\"end\":24078,\"start\":23303},{\"end\":24495,\"start\":24097},{\"end\":24745,\"start\":24519},{\"end\":24802,\"start\":24747},{\"end\":25015,\"start\":24813},{\"end\":25078,\"start\":25017},{\"end\":26521,\"start\":25080},{\"end\":27581,\"start\":26575},{\"end\":28552,\"start\":27598},{\"end\":28931,\"start\":28554},{\"end\":29308,\"start\":28933},{\"end\":29887,\"start\":29323}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7761,\"start\":7699},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7919,\"start\":7875},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8939,\"start\":8638},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9790,\"start\":9676},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9818,\"start\":9790},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10680,\"start\":10640},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11749,\"start\":11633},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12864,\"start\":12835},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13334,\"start\":13310},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13830,\"start\":13801},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13887,\"start\":13859},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14429,\"start\":14385},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14628,\"start\":14593},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14998,\"start\":14943},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16040,\"start\":15990},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21084,\"start\":21036},{\"attributes\":{\"id\":\"formula_0\"},\"end\":7761,\"start\":7699},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7919,\"start\":7875},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8939,\"start\":8638},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9790,\"start\":9676},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9818,\"start\":9790},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10680,\"start\":10640},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11749,\"start\":11633},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12864,\"start\":12835},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13334,\"start\":13310},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13830,\"start\":13801},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13887,\"start\":13859},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14429,\"start\":14385},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14628,\"start\":14593},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14998,\"start\":14943},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16040,\"start\":15990},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21084,\"start\":21036}]", "table_ref": "[{\"end\":3176,\"start\":3173},{\"end\":22418,\"start\":22411},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23863,\"start\":23856},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24363,\"start\":24356},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25884,\"start\":25877},{\"end\":3176,\"start\":3173},{\"end\":22418,\"start\":22411},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23863,\"start\":23856},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24363,\"start\":24356},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25884,\"start\":25877}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2378,\"start\":2366},{\"end\":2911,\"start\":2897},{\"end\":2965,\"start\":2949},{\"end\":2988,\"start\":2985},{\"end\":3031,\"start\":3028},{\"end\":3064,\"start\":3061},{\"end\":3106,\"start\":3103},{\"end\":3136,\"start\":3133},{\"attributes\":{\"n\":\"2.1\"},\"end\":6593,\"start\":6578},{\"attributes\":{\"n\":\"2.2\"},\"end\":7020,\"start\":6962},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":7366,\"start\":7359},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":7928,\"start\":7921},{\"attributes\":{\"n\":\"2.3\"},\"end\":11789,\"start\":11751},{\"attributes\":{\"n\":\"2.3.1\"},\"end\":12377,\"start\":12364},{\"end\":13396,\"start\":13385},{\"attributes\":{\"n\":\"2.3.2\"},\"end\":14522,\"start\":14510},{\"attributes\":{\"n\":\"3\"},\"end\":16185,\"start\":16167},{\"attributes\":{\"n\":\"3.1\"},\"end\":16295,\"start\":16288},{\"attributes\":{\"n\":\"3.2\"},\"end\":17207,\"start\":17188},{\"attributes\":{\"n\":\"3.3\"},\"end\":18064,\"start\":18055},{\"attributes\":{\"n\":\"3.4\"},\"end\":19151,\"start\":19133},{\"attributes\":{\"n\":\"3.4.1\"},\"end\":19221,\"start\":19201},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":21488,\"start\":21472},{\"attributes\":{\"n\":\"4\"},\"end\":22321,\"start\":22299},{\"attributes\":{\"n\":\"4.1\"},\"end\":22342,\"start\":22324},{\"end\":23020,\"start\":23015},{\"attributes\":{\"n\":\"4.2\"},\"end\":24095,\"start\":24081},{\"end\":24517,\"start\":24498},{\"end\":24811,\"start\":24805},{\"attributes\":{\"n\":\"4.3\"},\"end\":26534,\"start\":26524},{\"attributes\":{\"n\":\"4.4\"},\"end\":26573,\"start\":26537},{\"attributes\":{\"n\":\"5\"},\"end\":27596,\"start\":27584},{\"attributes\":{\"n\":\"6\"},\"end\":29321,\"start\":29311},{\"end\":29899,\"start\":29889},{\"end\":29987,\"start\":29979},{\"end\":31197,\"start\":31188},{\"end\":31512,\"start\":31503},{\"end\":31704,\"start\":31695},{\"attributes\":{\"n\":\"1\"},\"end\":2378,\"start\":2366},{\"end\":2911,\"start\":2897},{\"end\":2965,\"start\":2949},{\"end\":2988,\"start\":2985},{\"end\":3031,\"start\":3028},{\"end\":3064,\"start\":3061},{\"end\":3106,\"start\":3103},{\"end\":3136,\"start\":3133},{\"attributes\":{\"n\":\"2.1\"},\"end\":6593,\"start\":6578},{\"attributes\":{\"n\":\"2.2\"},\"end\":7020,\"start\":6962},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":7366,\"start\":7359},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":7928,\"start\":7921},{\"attributes\":{\"n\":\"2.3\"},\"end\":11789,\"start\":11751},{\"attributes\":{\"n\":\"2.3.1\"},\"end\":12377,\"start\":12364},{\"end\":13396,\"start\":13385},{\"attributes\":{\"n\":\"2.3.2\"},\"end\":14522,\"start\":14510},{\"attributes\":{\"n\":\"3\"},\"end\":16185,\"start\":16167},{\"attributes\":{\"n\":\"3.1\"},\"end\":16295,\"start\":16288},{\"attributes\":{\"n\":\"3.2\"},\"end\":17207,\"start\":17188},{\"attributes\":{\"n\":\"3.3\"},\"end\":18064,\"start\":18055},{\"attributes\":{\"n\":\"3.4\"},\"end\":19151,\"start\":19133},{\"attributes\":{\"n\":\"3.4.1\"},\"end\":19221,\"start\":19201},{\"attributes\":{\"n\":\"3.4.2\"},\"end\":21488,\"start\":21472},{\"attributes\":{\"n\":\"4\"},\"end\":22321,\"start\":22299},{\"attributes\":{\"n\":\"4.1\"},\"end\":22342,\"start\":22324},{\"end\":23020,\"start\":23015},{\"attributes\":{\"n\":\"4.2\"},\"end\":24095,\"start\":24081},{\"end\":24517,\"start\":24498},{\"end\":24811,\"start\":24805},{\"attributes\":{\"n\":\"4.3\"},\"end\":26534,\"start\":26524},{\"attributes\":{\"n\":\"4.4\"},\"end\":26573,\"start\":26537},{\"attributes\":{\"n\":\"5\"},\"end\":27596,\"start\":27584},{\"attributes\":{\"n\":\"6\"},\"end\":29321,\"start\":29311},{\"end\":29899,\"start\":29889},{\"end\":29987,\"start\":29979},{\"end\":31197,\"start\":31188},{\"end\":31512,\"start\":31503},{\"end\":31704,\"start\":31695}]", "table": "[{\"end\":30820,\"start\":30279},{\"end\":31186,\"start\":30888},{\"end\":31501,\"start\":31259},{\"end\":31693,\"start\":31514},{\"end\":31785,\"start\":31706},{\"end\":30820,\"start\":30279},{\"end\":31186,\"start\":30888},{\"end\":31501,\"start\":31259},{\"end\":31693,\"start\":31514},{\"end\":31785,\"start\":31706}]", "figure_caption": "[{\"end\":29977,\"start\":29901},{\"end\":30105,\"start\":29989},{\"end\":30279,\"start\":30108},{\"end\":30888,\"start\":30823},{\"end\":31259,\"start\":31199},{\"end\":29977,\"start\":29901},{\"end\":30105,\"start\":29989},{\"end\":30279,\"start\":30108},{\"end\":30888,\"start\":30823},{\"end\":31259,\"start\":31199}]", "figure_ref": "[{\"end\":3185,\"start\":3177},{\"end\":3776,\"start\":3768},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10998,\"start\":10990},{\"end\":11497,\"start\":11489},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14805,\"start\":14797},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27099,\"start\":27090},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27145,\"start\":27137},{\"end\":3185,\"start\":3177},{\"end\":3776,\"start\":3768},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10998,\"start\":10990},{\"end\":11497,\"start\":11489},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14805,\"start\":14797},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27099,\"start\":27090},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27145,\"start\":27137}]", "bib_author_first_name": "[{\"end\":32866,\"start\":32859},{\"end\":32886,\"start\":32877},{\"end\":32898,\"start\":32892},{\"end\":33320,\"start\":33313},{\"end\":33331,\"start\":33325},{\"end\":33343,\"start\":33337},{\"end\":33357,\"start\":33350},{\"end\":33367,\"start\":33364},{\"end\":33842,\"start\":33841},{\"end\":33856,\"start\":33852},{\"end\":34315,\"start\":34312},{\"end\":34324,\"start\":34320},{\"end\":34337,\"start\":34331},{\"end\":34352,\"start\":34345},{\"end\":34813,\"start\":34809},{\"end\":35221,\"start\":35220},{\"end\":35237,\"start\":35232},{\"end\":35613,\"start\":35604},{\"end\":35628,\"start\":35622},{\"end\":35645,\"start\":35638},{\"end\":35672,\"start\":35655},{\"end\":36113,\"start\":36104},{\"end\":36126,\"start\":36122},{\"end\":36138,\"start\":36132},{\"end\":36155,\"start\":36148},{\"end\":36182,\"start\":36165},{\"end\":36672,\"start\":36663},{\"end\":36688,\"start\":36681},{\"end\":36706,\"start\":36702},{\"end\":36721,\"start\":36714},{\"end\":36748,\"start\":36731},{\"end\":36763,\"start\":36758},{\"end\":37118,\"start\":37113},{\"end\":37127,\"start\":37123},{\"end\":37143,\"start\":37136},{\"end\":37158,\"start\":37149},{\"end\":37169,\"start\":37165},{\"end\":37181,\"start\":37178},{\"end\":37703,\"start\":37697},{\"end\":37713,\"start\":37708},{\"end\":37721,\"start\":37719},{\"end\":37731,\"start\":37726},{\"end\":38379,\"start\":38377},{\"end\":38392,\"start\":38386},{\"end\":38403,\"start\":38399},{\"end\":38415,\"start\":38408},{\"end\":38424,\"start\":38421},{\"end\":38434,\"start\":38430},{\"end\":38933,\"start\":38929},{\"end\":38943,\"start\":38939},{\"end\":38951,\"start\":38948},{\"end\":38967,\"start\":38958},{\"end\":38980,\"start\":38974},{\"end\":38995,\"start\":38988},{\"end\":39003,\"start\":39001},{\"end\":39508,\"start\":39503},{\"end\":39520,\"start\":39516},{\"end\":39538,\"start\":39527},{\"end\":39540,\"start\":39539},{\"end\":39963,\"start\":39959},{\"end\":39980,\"start\":39974},{\"end\":40004,\"start\":39996},{\"end\":40022,\"start\":40014},{\"end\":40574,\"start\":40571},{\"end\":40596,\"start\":40591},{\"end\":40598,\"start\":40597},{\"end\":41148,\"start\":41141},{\"end\":41164,\"start\":41159},{\"end\":41177,\"start\":41173},{\"end\":41192,\"start\":41184},{\"end\":41671,\"start\":41665},{\"end\":41687,\"start\":41680},{\"end\":41702,\"start\":41695},{\"end\":42099,\"start\":42094},{\"end\":42101,\"start\":42100},{\"end\":42119,\"start\":42110},{\"end\":42121,\"start\":42120},{\"end\":42139,\"start\":42130},{\"end\":42141,\"start\":42140},{\"end\":42476,\"start\":42470},{\"end\":42502,\"start\":42498},{\"end\":42516,\"start\":42511},{\"end\":43040,\"start\":43032},{\"end\":43050,\"start\":43047},{\"end\":43062,\"start\":43056},{\"end\":43078,\"start\":43073},{\"end\":43080,\"start\":43079},{\"end\":43429,\"start\":43423},{\"end\":43450,\"start\":43442},{\"end\":43452,\"start\":43451},{\"end\":43465,\"start\":43461},{\"end\":43482,\"start\":43478},{\"end\":43500,\"start\":43494},{\"end\":43832,\"start\":43827},{\"end\":43847,\"start\":43843},{\"end\":43849,\"start\":43848},{\"end\":43863,\"start\":43857},{\"end\":43886,\"start\":43879},{\"end\":44424,\"start\":44418},{\"end\":44426,\"start\":44425},{\"end\":44708,\"start\":44700},{\"end\":44715,\"start\":44713},{\"end\":44723,\"start\":44721},{\"end\":44738,\"start\":44730},{\"end\":44755,\"start\":44746},{\"end\":44768,\"start\":44761},{\"end\":44781,\"start\":44775},{\"end\":45327,\"start\":45321},{\"end\":45342,\"start\":45335},{\"end\":45354,\"start\":45348},{\"end\":45366,\"start\":45360},{\"end\":45375,\"start\":45372},{\"end\":45385,\"start\":45380},{\"end\":45935,\"start\":45932},{\"end\":45947,\"start\":45943},{\"end\":45957,\"start\":45954},{\"end\":45970,\"start\":45964},{\"end\":46543,\"start\":46537},{\"end\":46555,\"start\":46551},{\"end\":46567,\"start\":46561},{\"end\":46581,\"start\":46573},{\"end\":46592,\"start\":46588},{\"end\":46601,\"start\":46599},{\"end\":46610,\"start\":46606},{\"end\":46623,\"start\":46617},{\"end\":32866,\"start\":32859},{\"end\":32886,\"start\":32877},{\"end\":32898,\"start\":32892},{\"end\":33320,\"start\":33313},{\"end\":33331,\"start\":33325},{\"end\":33343,\"start\":33337},{\"end\":33357,\"start\":33350},{\"end\":33367,\"start\":33364},{\"end\":33842,\"start\":33841},{\"end\":33856,\"start\":33852},{\"end\":34315,\"start\":34312},{\"end\":34324,\"start\":34320},{\"end\":34337,\"start\":34331},{\"end\":34352,\"start\":34345},{\"end\":34813,\"start\":34809},{\"end\":35221,\"start\":35220},{\"end\":35237,\"start\":35232},{\"end\":35613,\"start\":35604},{\"end\":35628,\"start\":35622},{\"end\":35645,\"start\":35638},{\"end\":35672,\"start\":35655},{\"end\":36113,\"start\":36104},{\"end\":36126,\"start\":36122},{\"end\":36138,\"start\":36132},{\"end\":36155,\"start\":36148},{\"end\":36182,\"start\":36165},{\"end\":36672,\"start\":36663},{\"end\":36688,\"start\":36681},{\"end\":36706,\"start\":36702},{\"end\":36721,\"start\":36714},{\"end\":36748,\"start\":36731},{\"end\":36763,\"start\":36758},{\"end\":37118,\"start\":37113},{\"end\":37127,\"start\":37123},{\"end\":37143,\"start\":37136},{\"end\":37158,\"start\":37149},{\"end\":37169,\"start\":37165},{\"end\":37181,\"start\":37178},{\"end\":37703,\"start\":37697},{\"end\":37713,\"start\":37708},{\"end\":37721,\"start\":37719},{\"end\":37731,\"start\":37726},{\"end\":38379,\"start\":38377},{\"end\":38392,\"start\":38386},{\"end\":38403,\"start\":38399},{\"end\":38415,\"start\":38408},{\"end\":38424,\"start\":38421},{\"end\":38434,\"start\":38430},{\"end\":38933,\"start\":38929},{\"end\":38943,\"start\":38939},{\"end\":38951,\"start\":38948},{\"end\":38967,\"start\":38958},{\"end\":38980,\"start\":38974},{\"end\":38995,\"start\":38988},{\"end\":39003,\"start\":39001},{\"end\":39508,\"start\":39503},{\"end\":39520,\"start\":39516},{\"end\":39538,\"start\":39527},{\"end\":39540,\"start\":39539},{\"end\":39963,\"start\":39959},{\"end\":39980,\"start\":39974},{\"end\":40004,\"start\":39996},{\"end\":40022,\"start\":40014},{\"end\":40574,\"start\":40571},{\"end\":40596,\"start\":40591},{\"end\":40598,\"start\":40597},{\"end\":41148,\"start\":41141},{\"end\":41164,\"start\":41159},{\"end\":41177,\"start\":41173},{\"end\":41192,\"start\":41184},{\"end\":41671,\"start\":41665},{\"end\":41687,\"start\":41680},{\"end\":41702,\"start\":41695},{\"end\":42099,\"start\":42094},{\"end\":42101,\"start\":42100},{\"end\":42119,\"start\":42110},{\"end\":42121,\"start\":42120},{\"end\":42139,\"start\":42130},{\"end\":42141,\"start\":42140},{\"end\":42476,\"start\":42470},{\"end\":42502,\"start\":42498},{\"end\":42516,\"start\":42511},{\"end\":43040,\"start\":43032},{\"end\":43050,\"start\":43047},{\"end\":43062,\"start\":43056},{\"end\":43078,\"start\":43073},{\"end\":43080,\"start\":43079},{\"end\":43429,\"start\":43423},{\"end\":43450,\"start\":43442},{\"end\":43452,\"start\":43451},{\"end\":43465,\"start\":43461},{\"end\":43482,\"start\":43478},{\"end\":43500,\"start\":43494},{\"end\":43832,\"start\":43827},{\"end\":43847,\"start\":43843},{\"end\":43849,\"start\":43848},{\"end\":43863,\"start\":43857},{\"end\":43886,\"start\":43879},{\"end\":44424,\"start\":44418},{\"end\":44426,\"start\":44425},{\"end\":44708,\"start\":44700},{\"end\":44715,\"start\":44713},{\"end\":44723,\"start\":44721},{\"end\":44738,\"start\":44730},{\"end\":44755,\"start\":44746},{\"end\":44768,\"start\":44761},{\"end\":44781,\"start\":44775},{\"end\":45327,\"start\":45321},{\"end\":45342,\"start\":45335},{\"end\":45354,\"start\":45348},{\"end\":45366,\"start\":45360},{\"end\":45375,\"start\":45372},{\"end\":45385,\"start\":45380},{\"end\":45935,\"start\":45932},{\"end\":45947,\"start\":45943},{\"end\":45957,\"start\":45954},{\"end\":45970,\"start\":45964},{\"end\":46543,\"start\":46537},{\"end\":46555,\"start\":46551},{\"end\":46567,\"start\":46561},{\"end\":46581,\"start\":46573},{\"end\":46592,\"start\":46588},{\"end\":46601,\"start\":46599},{\"end\":46610,\"start\":46606},{\"end\":46623,\"start\":46617}]", "bib_author_last_name": "[{\"end\":32875,\"start\":32867},{\"end\":32890,\"start\":32887},{\"end\":32905,\"start\":32899},{\"end\":33323,\"start\":33321},{\"end\":33335,\"start\":33332},{\"end\":33348,\"start\":33344},{\"end\":33362,\"start\":33358},{\"end\":33371,\"start\":33368},{\"end\":33850,\"start\":33843},{\"end\":33862,\"start\":33857},{\"end\":33871,\"start\":33864},{\"end\":34318,\"start\":34316},{\"end\":34329,\"start\":34325},{\"end\":34343,\"start\":34338},{\"end\":34356,\"start\":34353},{\"end\":34817,\"start\":34814},{\"end\":35230,\"start\":35222},{\"end\":35244,\"start\":35238},{\"end\":35248,\"start\":35246},{\"end\":35620,\"start\":35614},{\"end\":35636,\"start\":35629},{\"end\":35653,\"start\":35646},{\"end\":35680,\"start\":35673},{\"end\":36120,\"start\":36114},{\"end\":36130,\"start\":36127},{\"end\":36146,\"start\":36139},{\"end\":36163,\"start\":36156},{\"end\":36190,\"start\":36183},{\"end\":36679,\"start\":36673},{\"end\":36700,\"start\":36689},{\"end\":36712,\"start\":36707},{\"end\":36729,\"start\":36722},{\"end\":36756,\"start\":36749},{\"end\":36771,\"start\":36764},{\"end\":37121,\"start\":37119},{\"end\":37134,\"start\":37128},{\"end\":37147,\"start\":37144},{\"end\":37163,\"start\":37159},{\"end\":37176,\"start\":37170},{\"end\":37190,\"start\":37182},{\"end\":37706,\"start\":37704},{\"end\":37717,\"start\":37714},{\"end\":37724,\"start\":37722},{\"end\":37737,\"start\":37732},{\"end\":38384,\"start\":38380},{\"end\":38397,\"start\":38393},{\"end\":38406,\"start\":38404},{\"end\":38419,\"start\":38416},{\"end\":38428,\"start\":38425},{\"end\":38440,\"start\":38435},{\"end\":38937,\"start\":38934},{\"end\":38946,\"start\":38944},{\"end\":38956,\"start\":38952},{\"end\":38972,\"start\":38968},{\"end\":38986,\"start\":38981},{\"end\":38999,\"start\":38996},{\"end\":39007,\"start\":39004},{\"end\":39514,\"start\":39509},{\"end\":39525,\"start\":39521},{\"end\":39548,\"start\":39541},{\"end\":39972,\"start\":39964},{\"end\":39994,\"start\":39981},{\"end\":40012,\"start\":40005},{\"end\":40034,\"start\":40023},{\"end\":40589,\"start\":40575},{\"end\":40607,\"start\":40599},{\"end\":41157,\"start\":41149},{\"end\":41171,\"start\":41165},{\"end\":41182,\"start\":41178},{\"end\":41196,\"start\":41193},{\"end\":41678,\"start\":41672},{\"end\":41693,\"start\":41688},{\"end\":41709,\"start\":41703},{\"end\":42108,\"start\":42102},{\"end\":42128,\"start\":42122},{\"end\":42147,\"start\":42142},{\"end\":42496,\"start\":42477},{\"end\":42509,\"start\":42503},{\"end\":42522,\"start\":42517},{\"end\":43045,\"start\":43041},{\"end\":43054,\"start\":43051},{\"end\":43071,\"start\":43063},{\"end\":43089,\"start\":43081},{\"end\":43440,\"start\":43430},{\"end\":43459,\"start\":43453},{\"end\":43476,\"start\":43466},{\"end\":43492,\"start\":43483},{\"end\":43514,\"start\":43501},{\"end\":43841,\"start\":43833},{\"end\":43855,\"start\":43850},{\"end\":43877,\"start\":43864},{\"end\":43897,\"start\":43887},{\"end\":44435,\"start\":44427},{\"end\":44711,\"start\":44709},{\"end\":44719,\"start\":44716},{\"end\":44728,\"start\":44724},{\"end\":44744,\"start\":44739},{\"end\":44759,\"start\":44756},{\"end\":44773,\"start\":44769},{\"end\":44784,\"start\":44782},{\"end\":45333,\"start\":45328},{\"end\":45346,\"start\":45343},{\"end\":45358,\"start\":45355},{\"end\":45370,\"start\":45367},{\"end\":45378,\"start\":45376},{\"end\":45391,\"start\":45386},{\"end\":45941,\"start\":45936},{\"end\":45952,\"start\":45948},{\"end\":45962,\"start\":45958},{\"end\":45976,\"start\":45971},{\"end\":46549,\"start\":46544},{\"end\":46559,\"start\":46556},{\"end\":46571,\"start\":46568},{\"end\":46586,\"start\":46582},{\"end\":46597,\"start\":46593},{\"end\":46604,\"start\":46602},{\"end\":46615,\"start\":46611},{\"end\":46628,\"start\":46624},{\"end\":32875,\"start\":32867},{\"end\":32890,\"start\":32887},{\"end\":32905,\"start\":32899},{\"end\":33323,\"start\":33321},{\"end\":33335,\"start\":33332},{\"end\":33348,\"start\":33344},{\"end\":33362,\"start\":33358},{\"end\":33371,\"start\":33368},{\"end\":33850,\"start\":33843},{\"end\":33862,\"start\":33857},{\"end\":33871,\"start\":33864},{\"end\":34318,\"start\":34316},{\"end\":34329,\"start\":34325},{\"end\":34343,\"start\":34338},{\"end\":34356,\"start\":34353},{\"end\":34817,\"start\":34814},{\"end\":35230,\"start\":35222},{\"end\":35244,\"start\":35238},{\"end\":35248,\"start\":35246},{\"end\":35620,\"start\":35614},{\"end\":35636,\"start\":35629},{\"end\":35653,\"start\":35646},{\"end\":35680,\"start\":35673},{\"end\":36120,\"start\":36114},{\"end\":36130,\"start\":36127},{\"end\":36146,\"start\":36139},{\"end\":36163,\"start\":36156},{\"end\":36190,\"start\":36183},{\"end\":36679,\"start\":36673},{\"end\":36700,\"start\":36689},{\"end\":36712,\"start\":36707},{\"end\":36729,\"start\":36722},{\"end\":36756,\"start\":36749},{\"end\":36771,\"start\":36764},{\"end\":37121,\"start\":37119},{\"end\":37134,\"start\":37128},{\"end\":37147,\"start\":37144},{\"end\":37163,\"start\":37159},{\"end\":37176,\"start\":37170},{\"end\":37190,\"start\":37182},{\"end\":37706,\"start\":37704},{\"end\":37717,\"start\":37714},{\"end\":37724,\"start\":37722},{\"end\":37737,\"start\":37732},{\"end\":38384,\"start\":38380},{\"end\":38397,\"start\":38393},{\"end\":38406,\"start\":38404},{\"end\":38419,\"start\":38416},{\"end\":38428,\"start\":38425},{\"end\":38440,\"start\":38435},{\"end\":38937,\"start\":38934},{\"end\":38946,\"start\":38944},{\"end\":38956,\"start\":38952},{\"end\":38972,\"start\":38968},{\"end\":38986,\"start\":38981},{\"end\":38999,\"start\":38996},{\"end\":39007,\"start\":39004},{\"end\":39514,\"start\":39509},{\"end\":39525,\"start\":39521},{\"end\":39548,\"start\":39541},{\"end\":39972,\"start\":39964},{\"end\":39994,\"start\":39981},{\"end\":40012,\"start\":40005},{\"end\":40034,\"start\":40023},{\"end\":40589,\"start\":40575},{\"end\":40607,\"start\":40599},{\"end\":41157,\"start\":41149},{\"end\":41171,\"start\":41165},{\"end\":41182,\"start\":41178},{\"end\":41196,\"start\":41193},{\"end\":41678,\"start\":41672},{\"end\":41693,\"start\":41688},{\"end\":41709,\"start\":41703},{\"end\":42108,\"start\":42102},{\"end\":42128,\"start\":42122},{\"end\":42147,\"start\":42142},{\"end\":42496,\"start\":42477},{\"end\":42509,\"start\":42503},{\"end\":42522,\"start\":42517},{\"end\":43045,\"start\":43041},{\"end\":43054,\"start\":43051},{\"end\":43071,\"start\":43063},{\"end\":43089,\"start\":43081},{\"end\":43440,\"start\":43430},{\"end\":43459,\"start\":43453},{\"end\":43476,\"start\":43466},{\"end\":43492,\"start\":43483},{\"end\":43514,\"start\":43501},{\"end\":43841,\"start\":43833},{\"end\":43855,\"start\":43850},{\"end\":43877,\"start\":43864},{\"end\":43897,\"start\":43887},{\"end\":44435,\"start\":44427},{\"end\":44711,\"start\":44709},{\"end\":44719,\"start\":44716},{\"end\":44728,\"start\":44724},{\"end\":44744,\"start\":44739},{\"end\":44759,\"start\":44756},{\"end\":44773,\"start\":44769},{\"end\":44784,\"start\":44782},{\"end\":45333,\"start\":45328},{\"end\":45346,\"start\":45343},{\"end\":45358,\"start\":45355},{\"end\":45370,\"start\":45367},{\"end\":45378,\"start\":45376},{\"end\":45391,\"start\":45386},{\"end\":45941,\"start\":45936},{\"end\":45952,\"start\":45948},{\"end\":45962,\"start\":45958},{\"end\":45976,\"start\":45971},{\"end\":46549,\"start\":46544},{\"end\":46559,\"start\":46556},{\"end\":46571,\"start\":46568},{\"end\":46586,\"start\":46582},{\"end\":46597,\"start\":46593},{\"end\":46604,\"start\":46602},{\"end\":46615,\"start\":46611},{\"end\":46628,\"start\":46624}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11212020},\"end\":33259,\"start\":32788},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6484065},\"end\":33755,\"start\":33261},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":12233345},\"end\":34242,\"start\":33757},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":51788338},\"end\":34748,\"start\":34244},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9672033},\"end\":35174,\"start\":34750},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6628106},\"end\":35537,\"start\":35176},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3518190},\"end\":36046,\"start\":35539},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5033497},\"end\":36626,\"start\":36048},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":53334018},\"end\":37058,\"start\":36628},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":98180},\"end\":37616,\"start\":37060},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4937880},\"end\":38322,\"start\":37618},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":96445784},\"end\":38849,\"start\":38324},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":165163728},\"end\":39433,\"start\":38851},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1998416},\"end\":39957,\"start\":39435},{\"attributes\":{\"id\":\"b14\"},\"end\":40434,\"start\":39959},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6854952},\"end\":41075,\"start\":40436},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11080756},\"end\":41608,\"start\":41077},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":21850704},\"end\":42039,\"start\":41610},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6858787},\"end\":42385,\"start\":42041},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":29165442},\"end\":42972,\"start\":42387},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7296803},\"end\":43354,\"start\":42974},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6844431},\"end\":43784,\"start\":43356},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13959787},\"end\":44326,\"start\":43786},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2332513},\"end\":44611,\"start\":44328},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":46889991},\"end\":45252,\"start\":44613},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":51869205},\"end\":45846,\"start\":45254},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":42979957},\"end\":46483,\"start\":45848},{\"attributes\":{\"doi\":\"arXivpreprintarXiv:1808.07894\",\"id\":\"b27\"},\"end\":46850,\"start\":46485},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11212020},\"end\":33259,\"start\":32788},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6484065},\"end\":33755,\"start\":33261},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":12233345},\"end\":34242,\"start\":33757},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":51788338},\"end\":34748,\"start\":34244},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9672033},\"end\":35174,\"start\":34750},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6628106},\"end\":35537,\"start\":35176},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3518190},\"end\":36046,\"start\":35539},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5033497},\"end\":36626,\"start\":36048},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":53334018},\"end\":37058,\"start\":36628},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":98180},\"end\":37616,\"start\":37060},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4937880},\"end\":38322,\"start\":37618},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":96445784},\"end\":38849,\"start\":38324},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":165163728},\"end\":39433,\"start\":38851},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1998416},\"end\":39957,\"start\":39435},{\"attributes\":{\"id\":\"b14\"},\"end\":40434,\"start\":39959},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6854952},\"end\":41075,\"start\":40436},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11080756},\"end\":41608,\"start\":41077},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":21850704},\"end\":42039,\"start\":41610},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6858787},\"end\":42385,\"start\":42041},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":29165442},\"end\":42972,\"start\":42387},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7296803},\"end\":43354,\"start\":42974},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6844431},\"end\":43784,\"start\":43356},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13959787},\"end\":44326,\"start\":43786},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2332513},\"end\":44611,\"start\":44328},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":46889991},\"end\":45252,\"start\":44613},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":51869205},\"end\":45846,\"start\":45254},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":42979957},\"end\":46483,\"start\":45848},{\"attributes\":{\"doi\":\"arXivpreprintarXiv:1808.07894\",\"id\":\"b27\"},\"end\":46850,\"start\":46485}]", "bib_title": "[{\"end\":32857,\"start\":32788},{\"end\":33311,\"start\":33261},{\"end\":33839,\"start\":33757},{\"end\":34310,\"start\":34244},{\"end\":34807,\"start\":34750},{\"end\":35218,\"start\":35176},{\"end\":35602,\"start\":35539},{\"end\":36102,\"start\":36048},{\"end\":36661,\"start\":36628},{\"end\":37111,\"start\":37060},{\"end\":37695,\"start\":37618},{\"end\":38375,\"start\":38324},{\"end\":38927,\"start\":38851},{\"end\":39501,\"start\":39435},{\"end\":40569,\"start\":40436},{\"end\":41139,\"start\":41077},{\"end\":41663,\"start\":41610},{\"end\":42092,\"start\":42041},{\"end\":42468,\"start\":42387},{\"end\":43030,\"start\":42974},{\"end\":43421,\"start\":43356},{\"end\":43825,\"start\":43786},{\"end\":44416,\"start\":44328},{\"end\":44698,\"start\":44613},{\"end\":45319,\"start\":45254},{\"end\":45930,\"start\":45848},{\"end\":32857,\"start\":32788},{\"end\":33311,\"start\":33261},{\"end\":33839,\"start\":33757},{\"end\":34310,\"start\":34244},{\"end\":34807,\"start\":34750},{\"end\":35218,\"start\":35176},{\"end\":35602,\"start\":35539},{\"end\":36102,\"start\":36048},{\"end\":36661,\"start\":36628},{\"end\":37111,\"start\":37060},{\"end\":37695,\"start\":37618},{\"end\":38375,\"start\":38324},{\"end\":38927,\"start\":38851},{\"end\":39501,\"start\":39435},{\"end\":40569,\"start\":40436},{\"end\":41139,\"start\":41077},{\"end\":41663,\"start\":41610},{\"end\":42092,\"start\":42041},{\"end\":42468,\"start\":42387},{\"end\":43030,\"start\":42974},{\"end\":43421,\"start\":43356},{\"end\":43825,\"start\":43786},{\"end\":44416,\"start\":44328},{\"end\":44698,\"start\":44613},{\"end\":45319,\"start\":45254},{\"end\":45930,\"start\":45848}]", "bib_author": "[{\"end\":32877,\"start\":32859},{\"end\":32892,\"start\":32877},{\"end\":32907,\"start\":32892},{\"end\":33325,\"start\":33313},{\"end\":33337,\"start\":33325},{\"end\":33350,\"start\":33337},{\"end\":33364,\"start\":33350},{\"end\":33373,\"start\":33364},{\"end\":33852,\"start\":33841},{\"end\":33864,\"start\":33852},{\"end\":33873,\"start\":33864},{\"end\":34320,\"start\":34312},{\"end\":34331,\"start\":34320},{\"end\":34345,\"start\":34331},{\"end\":34358,\"start\":34345},{\"end\":34819,\"start\":34809},{\"end\":35232,\"start\":35220},{\"end\":35246,\"start\":35232},{\"end\":35250,\"start\":35246},{\"end\":35622,\"start\":35604},{\"end\":35638,\"start\":35622},{\"end\":35655,\"start\":35638},{\"end\":35682,\"start\":35655},{\"end\":36122,\"start\":36104},{\"end\":36132,\"start\":36122},{\"end\":36148,\"start\":36132},{\"end\":36165,\"start\":36148},{\"end\":36192,\"start\":36165},{\"end\":36681,\"start\":36663},{\"end\":36702,\"start\":36681},{\"end\":36714,\"start\":36702},{\"end\":36731,\"start\":36714},{\"end\":36758,\"start\":36731},{\"end\":36773,\"start\":36758},{\"end\":37123,\"start\":37113},{\"end\":37136,\"start\":37123},{\"end\":37149,\"start\":37136},{\"end\":37165,\"start\":37149},{\"end\":37178,\"start\":37165},{\"end\":37192,\"start\":37178},{\"end\":37708,\"start\":37697},{\"end\":37719,\"start\":37708},{\"end\":37726,\"start\":37719},{\"end\":37739,\"start\":37726},{\"end\":38386,\"start\":38377},{\"end\":38399,\"start\":38386},{\"end\":38408,\"start\":38399},{\"end\":38421,\"start\":38408},{\"end\":38430,\"start\":38421},{\"end\":38442,\"start\":38430},{\"end\":38939,\"start\":38929},{\"end\":38948,\"start\":38939},{\"end\":38958,\"start\":38948},{\"end\":38974,\"start\":38958},{\"end\":38988,\"start\":38974},{\"end\":39001,\"start\":38988},{\"end\":39009,\"start\":39001},{\"end\":39516,\"start\":39503},{\"end\":39527,\"start\":39516},{\"end\":39550,\"start\":39527},{\"end\":39974,\"start\":39959},{\"end\":39996,\"start\":39974},{\"end\":40014,\"start\":39996},{\"end\":40036,\"start\":40014},{\"end\":40591,\"start\":40571},{\"end\":40609,\"start\":40591},{\"end\":41159,\"start\":41141},{\"end\":41173,\"start\":41159},{\"end\":41184,\"start\":41173},{\"end\":41198,\"start\":41184},{\"end\":41680,\"start\":41665},{\"end\":41695,\"start\":41680},{\"end\":41711,\"start\":41695},{\"end\":42110,\"start\":42094},{\"end\":42130,\"start\":42110},{\"end\":42149,\"start\":42130},{\"end\":42498,\"start\":42470},{\"end\":42511,\"start\":42498},{\"end\":42524,\"start\":42511},{\"end\":43047,\"start\":43032},{\"end\":43056,\"start\":43047},{\"end\":43073,\"start\":43056},{\"end\":43091,\"start\":43073},{\"end\":43442,\"start\":43423},{\"end\":43461,\"start\":43442},{\"end\":43478,\"start\":43461},{\"end\":43494,\"start\":43478},{\"end\":43516,\"start\":43494},{\"end\":43843,\"start\":43827},{\"end\":43857,\"start\":43843},{\"end\":43879,\"start\":43857},{\"end\":43899,\"start\":43879},{\"end\":44437,\"start\":44418},{\"end\":44713,\"start\":44700},{\"end\":44721,\"start\":44713},{\"end\":44730,\"start\":44721},{\"end\":44746,\"start\":44730},{\"end\":44761,\"start\":44746},{\"end\":44775,\"start\":44761},{\"end\":44786,\"start\":44775},{\"end\":45335,\"start\":45321},{\"end\":45348,\"start\":45335},{\"end\":45360,\"start\":45348},{\"end\":45372,\"start\":45360},{\"end\":45380,\"start\":45372},{\"end\":45393,\"start\":45380},{\"end\":45943,\"start\":45932},{\"end\":45954,\"start\":45943},{\"end\":45964,\"start\":45954},{\"end\":45978,\"start\":45964},{\"end\":46551,\"start\":46537},{\"end\":46561,\"start\":46551},{\"end\":46573,\"start\":46561},{\"end\":46588,\"start\":46573},{\"end\":46599,\"start\":46588},{\"end\":46606,\"start\":46599},{\"end\":46617,\"start\":46606},{\"end\":46630,\"start\":46617},{\"end\":32877,\"start\":32859},{\"end\":32892,\"start\":32877},{\"end\":32907,\"start\":32892},{\"end\":33325,\"start\":33313},{\"end\":33337,\"start\":33325},{\"end\":33350,\"start\":33337},{\"end\":33364,\"start\":33350},{\"end\":33373,\"start\":33364},{\"end\":33852,\"start\":33841},{\"end\":33864,\"start\":33852},{\"end\":33873,\"start\":33864},{\"end\":34320,\"start\":34312},{\"end\":34331,\"start\":34320},{\"end\":34345,\"start\":34331},{\"end\":34358,\"start\":34345},{\"end\":34819,\"start\":34809},{\"end\":35232,\"start\":35220},{\"end\":35246,\"start\":35232},{\"end\":35250,\"start\":35246},{\"end\":35622,\"start\":35604},{\"end\":35638,\"start\":35622},{\"end\":35655,\"start\":35638},{\"end\":35682,\"start\":35655},{\"end\":36122,\"start\":36104},{\"end\":36132,\"start\":36122},{\"end\":36148,\"start\":36132},{\"end\":36165,\"start\":36148},{\"end\":36192,\"start\":36165},{\"end\":36681,\"start\":36663},{\"end\":36702,\"start\":36681},{\"end\":36714,\"start\":36702},{\"end\":36731,\"start\":36714},{\"end\":36758,\"start\":36731},{\"end\":36773,\"start\":36758},{\"end\":37123,\"start\":37113},{\"end\":37136,\"start\":37123},{\"end\":37149,\"start\":37136},{\"end\":37165,\"start\":37149},{\"end\":37178,\"start\":37165},{\"end\":37192,\"start\":37178},{\"end\":37708,\"start\":37697},{\"end\":37719,\"start\":37708},{\"end\":37726,\"start\":37719},{\"end\":37739,\"start\":37726},{\"end\":38386,\"start\":38377},{\"end\":38399,\"start\":38386},{\"end\":38408,\"start\":38399},{\"end\":38421,\"start\":38408},{\"end\":38430,\"start\":38421},{\"end\":38442,\"start\":38430},{\"end\":38939,\"start\":38929},{\"end\":38948,\"start\":38939},{\"end\":38958,\"start\":38948},{\"end\":38974,\"start\":38958},{\"end\":38988,\"start\":38974},{\"end\":39001,\"start\":38988},{\"end\":39009,\"start\":39001},{\"end\":39516,\"start\":39503},{\"end\":39527,\"start\":39516},{\"end\":39550,\"start\":39527},{\"end\":39974,\"start\":39959},{\"end\":39996,\"start\":39974},{\"end\":40014,\"start\":39996},{\"end\":40036,\"start\":40014},{\"end\":40591,\"start\":40571},{\"end\":40609,\"start\":40591},{\"end\":41159,\"start\":41141},{\"end\":41173,\"start\":41159},{\"end\":41184,\"start\":41173},{\"end\":41198,\"start\":41184},{\"end\":41680,\"start\":41665},{\"end\":41695,\"start\":41680},{\"end\":41711,\"start\":41695},{\"end\":42110,\"start\":42094},{\"end\":42130,\"start\":42110},{\"end\":42149,\"start\":42130},{\"end\":42498,\"start\":42470},{\"end\":42511,\"start\":42498},{\"end\":42524,\"start\":42511},{\"end\":43047,\"start\":43032},{\"end\":43056,\"start\":43047},{\"end\":43073,\"start\":43056},{\"end\":43091,\"start\":43073},{\"end\":43442,\"start\":43423},{\"end\":43461,\"start\":43442},{\"end\":43478,\"start\":43461},{\"end\":43494,\"start\":43478},{\"end\":43516,\"start\":43494},{\"end\":43843,\"start\":43827},{\"end\":43857,\"start\":43843},{\"end\":43879,\"start\":43857},{\"end\":43899,\"start\":43879},{\"end\":44437,\"start\":44418},{\"end\":44713,\"start\":44700},{\"end\":44721,\"start\":44713},{\"end\":44730,\"start\":44721},{\"end\":44746,\"start\":44730},{\"end\":44761,\"start\":44746},{\"end\":44775,\"start\":44761},{\"end\":44786,\"start\":44775},{\"end\":45335,\"start\":45321},{\"end\":45348,\"start\":45335},{\"end\":45360,\"start\":45348},{\"end\":45372,\"start\":45360},{\"end\":45380,\"start\":45372},{\"end\":45393,\"start\":45380},{\"end\":45943,\"start\":45932},{\"end\":45954,\"start\":45943},{\"end\":45964,\"start\":45954},{\"end\":45978,\"start\":45964},{\"end\":46551,\"start\":46537},{\"end\":46561,\"start\":46551},{\"end\":46573,\"start\":46561},{\"end\":46588,\"start\":46573},{\"end\":46599,\"start\":46588},{\"end\":46606,\"start\":46599},{\"end\":46617,\"start\":46606},{\"end\":46630,\"start\":46617}]", "bib_venue": "[{\"end\":33036,\"start\":32980},{\"end\":33528,\"start\":33459},{\"end\":34016,\"start\":33953},{\"end\":34519,\"start\":34447},{\"end\":34978,\"start\":34907},{\"end\":35371,\"start\":35319},{\"end\":35811,\"start\":35755},{\"end\":36351,\"start\":36280},{\"end\":37351,\"start\":37280},{\"end\":38010,\"start\":37883},{\"end\":38601,\"start\":38530},{\"end\":39158,\"start\":39092},{\"end\":39709,\"start\":39638},{\"end\":40161,\"start\":40107},{\"end\":40770,\"start\":40698},{\"end\":41359,\"start\":41287},{\"end\":41840,\"start\":41784},{\"end\":42705,\"start\":42623},{\"end\":44080,\"start\":43998},{\"end\":44947,\"start\":44875},{\"end\":45574,\"start\":45492},{\"end\":46189,\"start\":46092},{\"end\":33036,\"start\":32980},{\"end\":33528,\"start\":33459},{\"end\":34016,\"start\":33953},{\"end\":34519,\"start\":34447},{\"end\":34978,\"start\":34907},{\"end\":35371,\"start\":35319},{\"end\":35811,\"start\":35755},{\"end\":36351,\"start\":36280},{\"end\":37351,\"start\":37280},{\"end\":38010,\"start\":37883},{\"end\":38601,\"start\":38530},{\"end\":39158,\"start\":39092},{\"end\":39709,\"start\":39638},{\"end\":40161,\"start\":40107},{\"end\":40770,\"start\":40698},{\"end\":41359,\"start\":41287},{\"end\":41840,\"start\":41784},{\"end\":42705,\"start\":42623},{\"end\":44080,\"start\":43998},{\"end\":44947,\"start\":44875},{\"end\":45574,\"start\":45492},{\"end\":46189,\"start\":46092},{\"end\":32978,\"start\":32907},{\"end\":33457,\"start\":33373},{\"end\":33951,\"start\":33873},{\"end\":34445,\"start\":34358},{\"end\":34905,\"start\":34819},{\"end\":35317,\"start\":35250},{\"end\":35753,\"start\":35682},{\"end\":36278,\"start\":36192},{\"end\":36825,\"start\":36773},{\"end\":37278,\"start\":37192},{\"end\":37881,\"start\":37739},{\"end\":38528,\"start\":38442},{\"end\":39090,\"start\":39009},{\"end\":39636,\"start\":39550},{\"end\":40105,\"start\":40036},{\"end\":40696,\"start\":40609},{\"end\":41285,\"start\":41198},{\"end\":41782,\"start\":41711},{\"end\":42189,\"start\":42149},{\"end\":42621,\"start\":42524},{\"end\":43140,\"start\":43091},{\"end\":43552,\"start\":43516},{\"end\":43996,\"start\":43899},{\"end\":44453,\"start\":44437},{\"end\":44873,\"start\":44786},{\"end\":45490,\"start\":45393},{\"end\":46090,\"start\":45978},{\"end\":46535,\"start\":46485},{\"end\":32978,\"start\":32907},{\"end\":33457,\"start\":33373},{\"end\":33951,\"start\":33873},{\"end\":34445,\"start\":34358},{\"end\":34905,\"start\":34819},{\"end\":35317,\"start\":35250},{\"end\":35753,\"start\":35682},{\"end\":36278,\"start\":36192},{\"end\":36825,\"start\":36773},{\"end\":37278,\"start\":37192},{\"end\":37881,\"start\":37739},{\"end\":38528,\"start\":38442},{\"end\":39090,\"start\":39009},{\"end\":39636,\"start\":39550},{\"end\":40105,\"start\":40036},{\"end\":40696,\"start\":40609},{\"end\":41285,\"start\":41198},{\"end\":41782,\"start\":41711},{\"end\":42189,\"start\":42149},{\"end\":42621,\"start\":42524},{\"end\":43140,\"start\":43091},{\"end\":43552,\"start\":43516},{\"end\":43996,\"start\":43899},{\"end\":44453,\"start\":44437},{\"end\":44873,\"start\":44786},{\"end\":45490,\"start\":45393},{\"end\":46090,\"start\":45978},{\"end\":46535,\"start\":46485}]"}}}, "year": 2023, "month": 12, "day": 17}