{"id": 6656096, "updated": "2023-09-28 20:12:13.496", "metadata": {"title": "FeUdal Networks for Hierarchical Reinforcement Learning", "authors": "[{\"first\":\"Alexander\",\"last\":\"Vezhnevets\",\"middle\":[\"Sasha\"]},{\"first\":\"Simon\",\"last\":\"Osindero\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Schaul\",\"middle\":[]},{\"first\":\"Nicolas\",\"last\":\"Heess\",\"middle\":[]},{\"first\":\"Max\",\"last\":\"Jaderberg\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Silver\",\"middle\":[]},{\"first\":\"Koray\",\"last\":\"Kavukcuoglu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 3, "day": 3}, "abstract": "We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -- allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1703.01161", "mag": "2949267040", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/VezhnevetsOSHJS17", "doi": null}}, "content": {"source": {"pdf_hash": "049c6e5736313374c6e594c34b9be89a3a09dced", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1703.01161v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "742e0c6286f8d75765b07a0107bf2939c454f99d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/049c6e5736313374c6e594c34b9be89a3a09dced.txt", "contents": "\nFeUdal Networks for Hierarchical Reinforcement Learning Koray Kavukcuoglu\n\n\nAlexander Sasha Vezhnevets \nVEZHNICK@GOOGLE.COM\nOSINDERO@GOOGLE.COM\nSCHAUL@GOOGLE.COM\nHEESS@GOOGLE.COM\nJADERBERG@GOOGLE.COM\nDAVIDSILVER@GOOGLE.COM\n\n\nSimon Osindero \nVEZHNICK@GOOGLE.COM\nOSINDERO@GOOGLE.COM\nSCHAUL@GOOGLE.COM\nHEESS@GOOGLE.COM\nJADERBERG@GOOGLE.COM\nDAVIDSILVER@GOOGLE.COM\n\n\nTom Schaul \nVEZHNICK@GOOGLE.COM\nOSINDERO@GOOGLE.COM\nSCHAUL@GOOGLE.COM\nHEESS@GOOGLE.COM\nJADERBERG@GOOGLE.COM\nDAVIDSILVER@GOOGLE.COM\n\n\nNicolas Heess \nVEZHNICK@GOOGLE.COM\nOSINDERO@GOOGLE.COM\nSCHAUL@GOOGLE.COM\nHEESS@GOOGLE.COM\nJADERBERG@GOOGLE.COM\nDAVIDSILVER@GOOGLE.COM\n\n\nMax Jaderberg \nVEZHNICK@GOOGLE.COM\nOSINDERO@GOOGLE.COM\nSCHAUL@GOOGLE.COM\nHEESS@GOOGLE.COM\nJADERBERG@GOOGLE.COM\nDAVIDSILVER@GOOGLE.COM\n\n\nDavid Silver \nVEZHNICK@GOOGLE.COM\nOSINDERO@GOOGLE.COM\nSCHAUL@GOOGLE.COM\nHEESS@GOOGLE.COM\nJADERBERG@GOOGLE.COM\nDAVIDSILVER@GOOGLE.COM\n\n\nKorayk@google Com Deepmind \nVEZHNICK@GOOGLE.COM\nOSINDERO@GOOGLE.COM\nSCHAUL@GOOGLE.COM\nHEESS@GOOGLE.COM\nJADERBERG@GOOGLE.COM\nDAVIDSILVER@GOOGLE.COM\n\n\nFeUdal Networks for Hierarchical Reinforcement Learning Koray Kavukcuoglu\n\nWe introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels -allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits -in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D Deep-Mind Lab environment.\n\nIntroduction\n\nDeep reinforcement learning has recently enjoyed successes in many domains (Mnih et al., 2015;Schulman et al., 2015;Levine et al., 2015;Mnih et al., 2016;Lillicrap et al., 2015). Nevertheless, long-term credit assignment re-mains a major challenge for these methods, especially in environments with sparse reward signals, such as the infamous Montezuma's Revenge ATARI game. It is symptomatic that the standard approach on the ATARI benchmark suite (Bellemare et al., 2012) is to use an actionrepeat heuristic, where each action translates into several (usually 4) consecutive actions in the environment. Yet another dimension of complexity is seen in non-Markovian environments that require memory -these are particularly challenging, since the agent has to learn which parts of experience to store for later, using only a sparse reward signal.\n\nThe framework we propose takes inspiration from feudal reinforcement learning (FRL) introduced by Dayan & Hinton (1993), where levels of hierarchy within an agent communicate via explicit goals. Some key insights from FRL are that goals can be generated in a top-down fashion, and that goal setting can be decoupled from goal achievement; a level in the hierarchy communicates to the level below it what must be achieved, but does not specify how to do so. Making higher levels reason at a lower temporal resolution naturally structures the agents behaviour into temporally extended sub-policies.\n\nThe architecture explored in this work is a fullydifferentiable neural network with two levels of hierarchy (though there are obvious generalisations to deeper hierarchies). The top level, the Manager, sets goals at a lower temporal resolution in a latent state-space that is itself learnt by the Manager. The lower level, the Worker, operates at a higher temporal resolution and produces primitive actions, conditioned on the goals it receives from the Manager. The Worker is motivated to follow the goals by an intrinsic reward. However, significantly, no gradients are propagated between Worker and Manager; the Manager receives its learning signal from the environment alone. In other words, the Manager learns to select latent goals that maximise extrinsic reward. arXiv:1703.01161v2 [cs.AI] 6 Mar 2017\n\nThe key contributions of our proposal are: (1) A consistent, end-to-end differentiable model that embodies and generalizes the principles of FRL. (2) A novel, approximate transition policy gradient update for training the Manager, which exploits the semantic meaning of the goals it produces. (3) The use of goals that are directional rather than absolute in nature. (4) A novel RNN design for the Manager -a dilated LSTM -which extends the longevity of the recurrent state memories and allows gradients to flow through large hops in time, enabling effective back-propagation through hundreds of steps.\n\nOur ablative analysis (Section 5.4) confirms that transitional policy gradient and directional goals are crucial for best performance. Our experiments on a selection of ATARI games (including the infamous Montezuma's revenge) and on several memory tasks in the 3D DeepMind Lab environment (Beattie et al., 2016) show that FuN significantly improves long-term credit assignment and memorisation.\n\n\nRelated Work\n\nBuilding hierarchical agents is a long standing topic in reinforcement learning (Sutton et al., 1999;Precup, 2000;Dayan & Hinton, 1993;Dietterich, 2000;Boutilier et al., 1997;Dayan, 1993;Kaelbling, 2014;Parr & Russell, 1998;Precup et al., 1997;Schmidhuber, 1991;Sutton, 1995;Wiering & Schmidhuber, 1997;Vezhnevets et al., 2016;Bacon et al., 2017). The options framework (Sutton et al., 1999;Precup, 2000) is a popular formulation for considering the problem with a two level hierarchy. The bottom level -an option -is a sub-policy with a termination condition, which takes in environment observations and outputs actions until the termination condition is met. An agent picks an option using its policy-over-options (the top level) and subsequently follows it until termination, at which point the policy-over-options is queried again and the process continues. Options are typically learned using sub-goals and 'pseudo-rewards' that are provided explicitly (Sutton et al., 1999;Dietterich, 2000;Dayan & Hinton, 1993). For a simple, tabular case (Wiering & Schmidhuber, 1997;Schaul et al., 2015), each state can be used as a sub-goal. Given the options, a policy-over-options can be learned using standard techniques by treating options as actions. Recently (Tessler et al., 2016;Kulkarni et al., 2016) have demonstrated that combining deep learning with predefined sub-goals delivers promising results in challenging environments like Minecraft and Atari, however sub-goal discovery was not addressed.\n\nA recent work of (Bacon et al., 2017) shows the possibility of learning options jointly with a policy-over-options in an end-to-end fashion by extending the policy gradient theorem to options. When options are learnt end-to-  end, they tend to degenerate to one of two trivial solutions: (i) only one active option that solves the whole task; (ii) a policy-over-options that changes options at every step, micro-managing the behaviour. Consequently, regularisers (Bacon et al., 2017;Vezhnevets et al., 2016) are usually introduced to steer the solution towards multiple options of extended length. This is believed to provide an inductive bias towards re-usable temporal abstractions and to help generalisation.\n\nA key difference between our approach and the options framework is that in our proposal the top level produces a meaningful and explicit goal for the bottom level to achieve. Sub-goals emerge as directions in the latent statespace and are naturally diverse. We also achieve significantly better scores on ATARI than Option-Critic (section 5).\n\nThere has also been a significant progress in nonhierarchical deep RL methods by using auxiliary losses and rewards. (Bellemare et al., 2016a) have significantly advanced the state-of-the-art on Montezuma's Revenge by using pseudo-count based auxiliary rewards for exploration, which stimulate agents to explore new parts of the state space. The recently proposed UNREAL agent (Jaderberg et al., 2016) also demonstrates a strong improvement by using unsupervised auxiliary tasks to help refine its internal representations. We note that these benefits are orthogonal to those provided by FuN, and that both approaches could be combined with FuN for even greater effect.\n\n\nThe model\n\nWhat is FuN? FuN is a modular neural-network consisting of two modules -the Worker and the Manager. The Manager internally computes a latent state representation s t and outputs a goal vector g t . The Worker produces actions conditioned on external observation, its own state, and the Managers goal. The Manager and the Worker share a perceptual module which takes an observation from the environment x t and computes a shared intermediate representation z t . The Manager's goals g t are trained using an approximate transition policy gradient. This is a particularly efficient form of policy gradient training that exploits the knowledge that the Worker's behaviour will ultimately align with the goal directions it has been set. The Worker is then trained via intrinsic reward to produce actions that cause these goal directions to be achieved. Figure 1a illustrates the overall design and the following equations describe the forward dynamics of our network:\nz t = f percept (x t )\n(1)\ns t = f Mspace (z t ) (2) h M t ,\u011d t = f Mrnn (s t , h M t\u22121 ); g t =\u011d t /||\u011d t ||;\n(3)\nw t = \u03c6( t i=t\u2212c g i ) (4) h W , U t = f Wrnn (z t , h W t\u22121 ) (5) \u03c0 t = SoftMax(U t w t )(6)\nwhere both the Manager and the Worker are recurrent. Here h M and h W correspond to the internal states of the Manager and the Worker respectively. A linear transform \u03c6 maps a goal g t into an embedding vector w t \u2208 R k , which is then combined via product with matrix U t (Workers output) to produce policy \u03c0 -vector of probabilities over primitive actions. The next section provides the details on goal embedding and the following sections 3.2,3.3 describes how FuN is trained.\n\n\nGoal embedding\n\nThe goal g modulates the policy via a multiplicative interaction in a low dimensional goal-embedding space R k , k << d. The Worker first produces an embedding vector for every action, represented by rows of matrix U \u2208 R |a|\u00d7k (eq. 5). To incorporate goals from the Manager, the last c goals are first pooled by summation and then embedded into a vector w \u2208 R k using a linear projection \u03c6 (eq. 4). The projection \u03c6 is linear, with no biases, and is learnt with gradients coming from the Worker's actions. The embedding matrix U is then combined with the goal embedding w via a matrix-vector product (eq. 6). Since \u03c6 has no biases it can never produce a constant non-zero vector -which is the only way the setup could ignore the Manager's input. This makes sure that the goal output by the Manager always influences the final policy. Notice how, due to pooling of goals over several time-steps, the conditioning from the Manager varies smoothly.\n\n\nLearning\n\nWe consider a standard reinforcement learning setup. At each step t, the agent receives an observation x t from the environment and selects an action a t from a finite set of possible actions. The environment responds with a new observation x t+1 and a scalar reward r t . The process continues until the terminal state is reached, after which it restarts. The goal of the agent is to maximise the discounted return R t = \u221e k=0 \u03b3 k r t+k+1 with \u03b3 \u2208 [0, 1]. The agent's behaviour is defined by its action-selection policy \u03c0. FuN produces a distribution over possible actions (a stochastic policy) as defined in eq. 6.\n\nThe conventional wisdom would be to train the whole architecture monolithically through gradient descent on either the policy directly or via TD-learning. Notice, that since FuN is fully differentiable we could train it end-toend using a policy gradient algorithm operating on the actions taken by the Worker. The outputs g of the Manager would be trained by gradients coming from the Worker. This, however would deprive Manager's goals g of any semantic meaning, making them just internal latent variables of the model. We propose instead to independently train Manager to predict advantageous directions (transitions) in state space and to intrinsically reward the Worker to follow these directions. If the Worker can fulfil the goal of moving in these directions (as it is rewarded for doing), then we ought to end up taking advantageous trajectories through state-space. We formalise this in the following update rule for the Manager:\n\u2207g t = A M t \u2207 \u03b8 d cos (s t+c \u2212 s t , g t (\u03b8)),(7)\nwhere\nA M t = R t \u2212 V M t (x t , \u03b8)\nis the Manager's advantage function, computed using a value function estimate V M t (x t , \u03b8) from the internal critic; d cos (\u03b1, \u03b2) = \u03b1 T \u03b2/(|\u03b1||\u03b2|) is the cosine similarity between two vectors. Note: the dependence of s on \u03b8 is ignored when computing \u2207 \u03b8 d cos -this avoids trivial solutions. Notice that now g t acquires a semantic meaning as an advantageous direction in the latent state space at a horizon c, which defines the temporal resolution of the Manager.\n\nThe intrinsic reward that encourages the Worker to follow the goals is defined as:\nr I t = 1/c c i=1 d cos (s t \u2212 s t\u2212i , g t\u2212i )(8)\nWe use directions because it is more feasible for the Worker to be able to reliably cause directional shifts in the latent state than it is to assume that the Worker can take us to (potentially) arbitrary new absolute locations. It also gives a degree of invariance to the goals and allows for structural generalisation -the same directional sub-goal g can invoke a sub-policy that is valid and useful in a large part of the latent state space; e.g. evade an enemy, swim up for air, etc. We compare absolute against directional goals empirically in section 5.4.\n\nThe original feudal reinforcement learning formulation of Dayan & Hinton (1993) advocated completely concealing the reward from the environment from lower levels of hierarchy. In practice we take a softer approach by adding an intrinsic reward for following the goals, but retaining the environment reward as well. The Worker is then trained to maximise a weighted sum R t + \u03b1R I t , where \u03b1 is a hyperparameter that regulates the influence of the intrinsic reward. The Workers policy \u03c0 can be trained to maximise intrinsic reward by using any off-the shelf deep reinforcement learning algorithm. Here we use an advantage actor critic :\n\u2207\u03c0 t = A D t \u2207 \u03b8 log \u03c0(a t |x t ; \u03b8)(9)\nThe Advantage function\nA D t = (R t + \u03b1R I t \u2212 V D t (x t ; \u03b8)\n) is calculated using an internal critic, which estimates the value functions for both rewards.\n\nNote that the Worker and Manager can potentially have different discount factors \u03b3 for computing the return. This allows, for instance, the Worker to be more greedy and focus on immediate rewards while the Manager can consider a long-term perspective.\n\n\nTransition Policy Gradients\n\nWe now motivate our proposed update rule for the Manager as a novel form of policy gradient with respect to a model of the Worker's behaviour. Consider a high-level policy o t = \u00b5(s t , \u03b8) that selects among sub-policies (possibly from a continuous set), where we assume for now that these sub-policies are fixed duration behaviours (lasting for c steps). Corresponding to each sub-policy is a transition distribution, p(s t+c |s t , o t ), that describes the distribution of states that we end up at the end of the sub-policy, given the start state and the sub-policy enacted. The high-level policy can be composed with the transition distribution to give a 'transition policy' \u03c0 T P (s t+c |s t ) = p(s t+c |s t , \u00b5(s t , \u03b8)) describing the distribution over end states given start states. It is valid to refer to this as a policy because the original MDP is isomorphic to a new MDP with policy \u03c0 T P and transition function s t+c = \u03c0 T P (s t ) (i.e. the state always transitions to the end state picked by the transition policy). As a result, we can apply the policy gradient theorem to the transition policy \u03c0 T P , so as to find the performance gradient with respect to the policy parameters, \n\u2207 \u03b8 \u03c0 T P t = E [(R t \u2212 V (s t ))\u2207 \u03b8 log p(s t+c |s t , \u00b5(s t , \u03b8))](10)\n\nArchitecture details\n\nThis section provides the particular details of the model as described in section 3. The perceptual module f percept is a convolutional network (CNN) followed by a fully connected layer. The CNN has a first layer with 16 8x8 filters of stride 4, followed by a layer with with 32 4x4 filters of stride 2. The fully connected layer has 256 hidden units. Each convolutional and fully-connected layer is followed by a rectifier non-linearity 1 . The state space which the Manager implicitly models in formulating its goals is computed via f Mspace , which is another fully connected layer followed by a rectifier non-linearity. The dimensionality of the embedding vectors, w, is set as k = 16. To encourage exploration in transition policy, at every step with a small probability we emit a random goal sampled from a uni-variate Gaussian.\n\nThe Worker's recurrent network f Wrnn is a standard LSTM (Hochreiter & Schmidhuber, 1997). For the Manager's recurrent network, f Mrnn , we propose a novel design -the dilated LSTM, which is introduced in the next section. Both f Mrnn and f Wrnn have 256 hidden units. The plot corresponds to the number of past states for which a frame maximizes the goal -i.e. the taller the bar, the more frequently that state was a maximizer of the expression for some previous state. Notice that FuN has learnt a semantically meaningful sub-goals -the tall bars in the plot (i.e. consistent goals) correspond to interpretably useful waypoints in Montezuma.\n\n\nDilated LSTM\n\nWe propose a novel RNN architecture for the Manager, which operates at lower temporal resolution than the data stream. We define a dilated LSTM analogously to dilated convolutional networks (Yu & Koltun, 2016). For a dilation radius r let the full state of the network be h = {\u0125 i } r i=1 , i.e. it is composed of r separate groups of sub-states or 'cores'. At time t the network is governed by the following equations:\u0125 t%r t , g t = LSTM(s t ,\u0125 t%r t\u22121 ; \u03b8 LSTM ), where % denotes the modulo operation and allows us to indicate which group of cores is currently being updated. We make the parameters of the LSTM network \u03b8 LSTM explicit to stress that the same set of parameters governs the update for each of the r groups within the dLSTM.\n\nAt each time step only the corresponding part of the state is updated and the output is pooled across the previous c outputs. This allows the r groups of cores inside the dLSTM to preserve the memories for long periods, yet the dLSTM as a whole is still able to process and learn from every input experience, and is also able to update its output at every step. This idea is similar to clockwork RNNs (Koutn\u00edk et al., 2014), however there the top level \"ticks\" at a fixed, slow pace, whereas the dLSTM observes all the available training data instead. In the experiments we set r = 10, and this was also used as the predictions horizon, c.\n\n\nExperiments\n\nThe goal of our experiments is to demonstrate that FuN learns non-trivial, helpful, and interpretable sub-policies and sub-goals, and also to validate components of the architecture. We start by describing technical details of the experimental setup and then present results on Montezuma's revenge -an infamously hard ATARI game -in section 5.1. Section 5.2 presents results on more ATARI games and extensively compares FuN to LSTM baseline with different discount factors and BPTT lengths. In section 5.3 we present results on a set of visual memorisation tasks in 3D environment. Section 5.4 presents an ablation study of FuN, validating our design choices.\n\nBaseline. Our main baseline is a recurrent LSTM network on top of a representation learned by a CNN. The LSTM (Hochreiter & Schmidhuber, 1997) architecture is a widely used recurrent network and it was demonstrated to perform very well on a suite of reinforcement learning problems . LSTM uses 316 hidden units 2 and its inputs are the feature representation of an observation and the previous action of the agent. Action probabilities and the value function estimate are regressed from its hidden state. All the methods the same CNN architecture, input pre-processing, and an action repeat of 4.\n\nOptimisation. We use the A3C method  for all reinforcement learning experiments. It was shown to achieve state-of-the-art results on several challenging benchmarks . We cut the trajectory and run backpropagation through time (BPTT) (Mozer, 1989) after K forward passes of a network or if a terminal signal is received. For FuN K = 400, for LSTM, unless otherwise stated, K = 40. We discuss different choice of K for LSTM in section 5.2. The optimization process runs 32 asynchronous threads using shared RMSProp. There are 3 hyper-parameters in FuN and 2 in the LSTM baselines. For each method,\n\n\nFull FuN\n\nExample frame sub-policy 1 sub-policy 2 sub-policy 3 sub-policy 4 LSTM Figure 3. Visualisation of sub-policies learnt on sea quest game. We sample a random goal and feed it as a constant conditioning for the Worker and record its behaviour. We filter out only the image of the ship and average the frames, acquiring the heat-map of agents spatial location. From left to right: i) an example frame of the game ii) policy learnt by LSTM baseline iii) full policy learnt by FuN followed by set of different sub-policies. Notice how sub-policies are concentrated around different areas of the playable space. Sub-policy 3 is used to swim up for oxygen.  we ran 100 experiments, each using randomly sampled hyper-parameters. Learning rate and entropy penalty were sampled from a LogUniform(10 \u22124 , 10 \u22123 ) interval for LSTM. For FuN the learning rate was sampled from LogUniform(10 \u22124.5 , 10 \u22123.5 ), to account for higher gradients due to longer BPTT unrolls. The learning rate was linearly annealed from a sampled value to half the initial rate for all agents. To explore intrinsic motivation in FuN, we sample its weight \u03b1 \u223c Uniform(0, 1). We define a training epoch as one million observations. When reporting learning curves, we plot the average episode score of the top 5 agents (according to the final score) against the training epochs. For all ATARI experiments we clip the reward to [\u22121, +1] interval\n\n\nMontezuma's revenge\n\nMontezuma's revenge is one of the hardest games available through the ALE (Bellemare et al., 2012). The game is infamous for challenging agents with lethal traps and sparse rewards. We had to broaden and intensify our hyper-parameter search for the LSTM baseline to see any progress at all for that model. We have experimented with many different hyper-parameter configurations for LSTM baseline, for instance expanding learning rate search to LogUniform(10 \u22123 , 10 \u22122 ), and we report on the configuration that worked best. We use a small discount 0.99 for LSTM; for FuN we use 0.99 in Worker and 0.999 in Manager. Figure 2b analyses the sub-goals learnt by FuN in the first room. They turn out to be meaningful milestones, which bridge the agents progress to its first extrinsic reward -picking up the key. Interestingly, two of the learnt sub-goals correspond to roughly the same locations as the ones hand-crafted in (Kulkarni et al., 2016) (ladder and key), but here they are learnt by the agent itself. Figure 2a plots the learning curves. Notice how FuN starts learning much earlier and achieves much higher scores. It takes > 300 epochs for LSTM to reach the score 400, which corresponds to solving the first room (take the key, open a door); it stagnates at that score until about 900 epochs, when it starts exploring further. FuN solves the first room in less than 200 epochs and immediately moves on to explore further, eventually visiting several other rooms and scoring up to 2600 points.\n\n\nATARI\n\nExperiments in this section validate that the capabilities of FuN go beyond what standard tools for long-term credit assignment -discount factors and BPTT unroll lengthcan provide for a baseline LSTM agent. We use two discounts 0.99 and 0.95 for both FuN and LSTM agents.\n\n(For the experiments on FuN only the discount for the Manager changes, while the Worker's discount is fixed at 0.95.) For the LSTM we explore BPTT of 40 and 100, while for FuN we use a BPTT unroll of 400. For LSTM with BPTT 100 we search for learning rate in the interval LogUniform(10 \u22124.5 , 10 \u22123.5 ), as for FuN. We use a diverse set of ATARI games, some of which involve longterm credit assignment and some which are more reactive. Figure 4 plots the learning curves. A few categories emerge. On Ms. Pacman, Amidar, and Gravitar FuN with a low Manager discount of 0.99 strongly outperforms all other methods. All of these games are known to require long-term reasoning to play well. Enduro stands out as all the LSTM agents completely fail at it. In this game the agent controls a racing car and scores points for overtaking other racers; this requires accelerating and steering for significant amount of time before the first reward is experienced. Frostbite is a hard game (Vezhnevets et al., 2016;Lake et al., 2016) that requires both long-term credit assignment and good exploration. The best-performing frostbite agent is FuN with 0.95 Manager discount, which outperforms the rest by a factor of 7. On Hero and Space Invaders all agents perform equally well. On Seaquest and Breakout, the baseline LSTM with a more aggressive discount of 0.95 is the best. This suggests that in these games long-term credit assignment is not important and the agent is better off optimising more immediate rewards in a greedy fashion. Alien is the only game where using different discounts doesn't meaningfully influence the agents performance; here we see the baseline LSTM outperforms our FuN model, although both still achieve a satisfactory scores. We provide qualitative analysis of subpolicies learnt on Seaquest in supplementary material.\n\nNote how using an unroll for BPTT=100 in the baseline LSTM significantly hurts its performance (hence we do not explore longer unrolls), while FuN performs very well with BPTT of 400 thanks to its ability to leverage the dLSTM. Being able to train a recurrent network over very long sequences could be an enabling tool for many memory related task, as we demonstrate in section 5.3.\n\n\nQualitative analysis on Seaquest\n\nTo qualitatively inspect sub-policies learnt by the Worker we use the following procedure: first, we record goals emitted by Manager during the play; we then sample one of them and provide it as a constant input to the Worker for the duration of an episode and record its behaviour. This allows us to qualitatively inspect what kind of sub-policies emerge. Figure 3 plots sub-policies learnt on the seaquest game. Notice how different options correspond to rough spatial positions or manoeuvres for the agent's submarine -for instance subpolicy 3 corresponds to swimming up for air.\n\nOption-critic architecture (Bacon et al., 2017) is, to the best of our knowledge, the only other end-to-end trainable system with sub-policies. The experimental results for Option-Critic on 4 ATARI (Bacon et al., 2017) games show scores similar those from a flat DQN (Mnih et al., 2015) baseline agent. Notice that our baseline  is much stronger than DQN. We also ran FuN on the same games as Option-Critic (Asterix, Ms. Pacman, Seaquest and Zaxxon) and after 200 epochs it achieves a similar score on Seaquest, doubles it on Ms. Pacman, more than triples it on Zaxxon and gets more than 20x improvement on Asterix. Figure 7 presents our results on Asterix and Zaxxon. We took the approximate performance of Option-Critic from the original paper -8000 for Asterix and 6000 for Zaxxon. Plots in the original paper also suggest that score stagnates around these levels, notice that our score keeps going up.\n\n\nMemory in Labyrinth\n\nDeepMind Lab (Beattie et al., 2016) is a first-person 3D game platform extended from OpenArena. It's a visually complex 3D environment with agent actions corresponding to movement and orientation. We use 4 different levels that test long-term credit assignment and visual memory:\n\nWater maze is a reproduction of the Morris water maze experiment (Morris, 1981) from the behavioural science literature. An agent is dropped into a circular pool of water with a concealed platform at unknown random location. The agent can move around and upon stepping on the platform it receives a reward and the trial restarts. The platform remains in the same location for the rest of the episode, while agent starts each trial at a random location. The walls of the pool are decorated with visual cues to assist localisation.\n\nT-maze is another classic animal cognition test. The agent spawns in a small T-shaped maze. Two objects with randomly chosen shape and colour are spawned at the left and right \"baiting\" locations. One of them is assigned a reward of +1 and the other a reward of -1. When the agent Score for Option-Critic is taken from the original paper collects one of the objects, it receives the reward and is respawned at the beginning of the T-maze. The objects are also re-instantiated in the same locations and with the same rewards on the re-spawn event. The agent should remember which object gives the positive reward across re-spawns and collect it as many times as possible within the fixed time given for the episode. T-maze+ is a modification of T-maze, where at each trial the length of corridors can vary, adding additional dimension of complexity.\n\nNon-match is a visual memorisation task. Each trial begins in small room with an out of reach object being dis-played in one of two display pods. There is a pad in the middle, which upon touching, the agent is rewarded with 1 point, and is teleported to a second room which has two objects in it, one of which matches the object in the previous room. Collecting the matching object gives a reward of -10 points, collecting the non matching object gives a reward of 10 points. Once either is collected, the agent is teleported back to the first room, with the same object being shown.\n\nFor all agents we include reward as a part of the observation. Figure 5a illustrates T-maze and non-match environments and figure 6 plots the learning curves. FuN consitently outperforms the LSTM baseline -it learns faster and also reaches a higher final reward. We analyse the FuN agent's behaviour in more detail in Figure 5b. It demonstrates that FuN learns meaningful sub-policies, which are then efficiently integrated with memory to produce rewarding behaviour. Interestingly, the LSTM agent doesn't appear to use its memory for water maze task at all, always circling the maze at the roughly the same radius.\n\n\nAblative analysis\n\nThis section empirically validates the main innovations of this paper: transition policy gradient for training the Manager; relative rather than absolute goals; lower temporal resolution for Manager; intrinsic motivation for the Worker. Transition policy gradient First we consider a 'non-Feudal' FuN -it has exactly the same network architecture as FuN, but the Managers output g is trained with gradients coming directly from the Worker and no intrinsic reward is used, much like in Option-Critic architecture (Bacon et al., 2017). Second, g is learnt using a standard policy gradient approach with the Manager emitting the mean of a Gaussian distribution from which goals are sampled (as if the Manager were solving a continuous control problem (Schulman et al., 2016;Mnih et al., 2016;Lillicrap et al., 2015)). Third, we explore a variant of FuN in which g specifies absolute, rather than relative/directional, goals (and the Worker's intrinsic reward is adjusted accordingly) but otherwise everything is the same. The experiments ( Figure 8) reveal that, although alternatives do work to some degree their performance is significantly inferior. We also evaluate a purely feudal version of FuN -in which the Worker is trained from the intrinsic reward alone. This ablation performs better than other, but still inferior to the full FuN approach. It shows that allowing the Worker to experience the external reward is beneficial.\n\nTemporal resolution ablations. An important feature of FuN is the ability of the Manager to operate at a low temporal resolution. This is achieved through dilation in the LSTM and through the prediction horizon c. To investigate their influence we use two baselines: i) the Manager uses a vanilla LSTM with no dilation; ii) FuN with Manager's prediction horizon c = 1. Figure 10 presents the results. The non-dilated LSTM fails catastrophically, most likely overwhelmed by the recurrent gradient. Reducing the horizon c to 1 did hurt the performance, although interestingly less so than other ablations. It seems that even at high temporal resolution Manager captures certain properties of the underlying MDP and communicate them down to Worker in a helpful way. This confirms that by learning in two separate formulations FuN is able to capture richer structural properties of the environment and thus train faster.\n\nIntrinsic motivation weight. This section look at the impact of the weight \u03b1, which regulates the relative weight of intrinsic reward (if \u03b1 = 0 then intrinsic reward is not used). We train agents with learning rate and entropy penalty fixed to 10 \u22123.5 and only vary \u03b1 between [0, 1]. Figure 11 shows scatter plots of agents final score vs \u03b1 hyper-parameter. Notice a clear correlation between the score and high value of \u03b1 on gravitar and amidar; however on other games the optimal value of \u03b1 can be less than to 1.\n\nDilate LSTM agent baseline One of innovations this paper presents is dLSTM design for a Recurrent network.\n\nIn principle, it could alone be used in an agent on top of a CNN, without the rest of FuN structures. We evaluate such an agent as an additional baseline. We use the same hyper-parameters as for FuN -BPTT=400, discount = 0.99, learning rate sampled in the interval LogUniform(10 \u22124.5 , 10 \u22123.5 ), entropy penalty LogUniform(10 \u22124 , 10 \u22123 ). Figure 12  dLSTM is in the ability to operate at lower temporal resolution, which is useful in the Manager, but not so much on it's own.\n\n\nATARI action repeat transfer\n\nOne of the advantages of FuN is the clear separation of duties between Manager and Worker. The Manager learns a transition policy, while the Worker learns to operate primitive actions to enact these transitions. This transition policy is invariant to the underlying embodiment of the agent -the way its primitive actions translate into state space transitions. Potentially, the transition policy can be transferred between agents with different embodiment -e.g. robot models with physical design or different operational frequency. We provide evidence towards that possibility by transferring policies across agents with different action repeat on ATARI. 3\n\nTo perform transfer, we initialise the FuN system with parameters extracted from an agent trained with action repeat of 4 and then make the following adjustments: (i) we accordingly adjust the discounts for all rewards; (ii) we increase the dilation of the dLSTM by a factor of 4; (iii) we increase the Manager's goal horizon c by a factor of 4. (These modifications adapt all the \"hard-wired\" 3 Action repeat is a heuristic used in all successful agents (Mnih et al., 2015;Bellemare et al., 2016b;Vezhnevets et al., 2016). It enables better exploration, eases credit assignment, and saves computation by repeating any action chosen by the agent several (= 4) times. but explicitly temporally sensitive aspects of the agent.) We then train this agent without action repeat. As a baseline we use an LSTM agent transferred in a similar way (with adjusted discounts) as well as FuN and LSTM agents trained without action repeat from scratch. Figure 9 shows the corresponding learning curves. The transferred FuN agent (green curve) significantly outperforms every other method. Furthermore it shows positive transfer on each environment, whereas LSTM only shows positive transfer on Ms. Pacman.\n\n\nDiscussion and future work\n\nHow to create agents that can learn to decompose their behaviour into meaningful primitives and then reuse them to more efficiently acquire new behaviours is a long standing research question. The solution to this question may be an important stepping stone towards agents with general intelligence and competence. This paper introduced FeUdal Networks, a novel architecture that formulates sub-goals as directions in latent state space, which, if followed, translate into a meaningful behavioural primitives. FuN clearly separates the module that discovers and sets sub-goals from the module that generates the behaviour through primitive actions. This creates a natural hierarchy that is stable and allows both modules to learn in complementary ways. Our experiments clearly demonstrate that this makes long-term credit assignment and memorisation more tractable. This also opens many avenues for further research, for instance: deeper hierarchies can be constructed by setting goals at multiple time scales, scaling agents to truly large environments with sparse rewards and partial observability. The modular structure of FuN is also lends itself to transfer and multitask learning -learnt behavioural primitives can be re-used to acquire new complex skills, or alternatively the transitional policies of the Manager can be transferred to agents with different embodiment.\n\nFigure 1 .\n1The schematic illustration of FuN (section 3)\n\nFigure 2 .\n2a) Learning curve on Montezuma's Revenge b) This is a visualisation of sub-goals learnt by FuN in the first room. For each time step we compute the latent state st and the corresponding goal gt. We then find a future state for which cos(st \u2212 st, gt) is maximized.\n\nFigure 4 .\n4ATARI training curves. Epochs corresponds to a million training steps of an agent. The value is the average per episode score of top 5 agents, according to the final score. We used two different discount factors 0.95 and 0.99.\n\nFigure 5 .Figure 6 .Figure 7 .\n567a) Schematic illustration of t-maze and non-match domains b) FuN in water maze. First two plots from the left, are a visualisation of FuN trajectories during one episode. The first trajectory (green) performs a search for the target in different locations, while subsequent ones (other colours) perform searches along a circle of a fixed radius matched to that of the target, always finding the target. The rightmost plot visualises different learnt sub-policies, produced by sampling a random g and fixing it for 200 steps. Each colour corresponds to a different g, the black circle represents the starting location. Training curves for memory tasks on Labyrinth. Comparison to Option-Critic on Zaxxon and Asterix.\n\nFigure 8 .Figure 9 .\n89Ablative Action repeat transfer\n\nFigure 10 .Figure 11 .\n1011plots the learning curves for FuN, LSTM and dLSTM agents. dLSTM generally underperforms both LSTM and FuN. The power of Learning curves for ablations of FuN that investigate influence of dLSTM in the Manager and Managers prediction horizon c. No dilation -FuN trained with a regular LSTM in the Manager; Manager horizon =1 -FuN trained with c = 1. Scatter plot of agents reward after 200 epochs vs intrinsic reward weight \u03b1.\n\nFigure 12 .\n12Learning curves for dLSTM based agent with LSTM and FuN for comparison.\n\n\nIn general, the Worker may follow a complex trajectory. A naive application of policy gradients requires the agent to learn from samples of these trajectories. But if we know where these trajectories are likely to end up, by modelling the transitions, then we can skip directly over the Worker's behaviour and instead follow the policy gradient of the predicted transition. FuN assumes a particular form for the transition model: that the direction in state-space, s t+c \u2212s t , follows a von Mises-Fisher distribution. Specifically, if the mean direction of the von Mises-Fisher distribution is given by g(o t ) (which for compactness we write as g t ) we would have p(s t+c |s t , o t ) \u221d e dcos(st+c\u2212st,gt) . If this functional form were indeed correct, then we see that our proposed update heuristic for the Manager, eqn.7, is in fact the proper form for the transition policy gradient arrived at in eqn.10.Note that the Worker's intrinsic reward (eqn. 8) is based on the log-likelihood of state trajectory. Through that the FuN architecture actively encourages the functional form of the transition model to hold true. Because the Worker is learning to achieve the Manager's direction, its transitions should, over time, closely follow a distribution around this direction, and hence our approximation for transition policy gradients should hold reasonably well.\nThis is substantially the same CNN as in, the only difference is that in the pre-processing stage we retain all colour channels.\nThis choice means that FuN and the LSTM baseline to have roughly the same number of total parameters.\nAcknowledgementsWe thank Alex Graves, Daan Wierstra, Olivier Pietquin, Oriol Vinyals, Joseph Modayil and Vlad Mnih for many helpful discussions, suggestions and comments on the paper.\nThe option-critic architecture. Pierre-Luc Bacon, Doina Precup, Jean Harb, AAAI. Bacon, Pierre-Luc, Precup, Doina, and Harb, Jean. The option-critic architecture. In AAAI, 2017.\n\n. Charles Beattie, Joel Z Leibo, Teplyashin, Denis, Ward, Tom, Wainwright, Marcus, K\u00fcttler, Heinrich, Lefrancq, Andrew, Green, Simon, Vald\u00e9s, V\u00edctor, Sadik, Amir, Schrittwieser, Anderson Julian, Keith, York, Sarah, Cant, Max, Cain, Adam, Adrian Bolton, Gaffney, Stephen, King, Helen, Hassabis, Demis, Shane Legg, Stig Petersen, Deepmind, Lab, arXiv:1612.03801arXiv preprintBeattie, Charles, Leibo, Joel Z., Teplyashin, Denis, Ward, Tom, Wainwright, Marcus, K\u00fcttler, Heinrich, Lefrancq, Andrew, Green, Simon, Vald\u00e9s, V\u00edctor, Sadik, Amir, Schrittwieser, Julian, Anderson, Keith, York, Sarah, Cant, Max, Cain, Adam, Bolton, Adrian, Gaffney, Stephen, King, Helen, Hassabis, Demis, Legg, Shane, and Petersen, Stig. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.\n\nUnifying count-based exploration and intrinsic motivation. Marc Bellemare, Srinivasan, Sriram, Ostrovski, Georg, Schaul, Tom, David Saxton, Remi Munos, NIPS. Bellemare, Marc, Srinivasan, Sriram, Ostrovski, Georg, Schaul, Tom, Saxton, David, and Munos, Remi. Uni- fying count-based exploration and intrinsic motivation. In NIPS, 2016a.\n\nThe arcade learning environment: An evaluation platform for general agents. Marc G Bellemare, Naddaf, Yavar, Joel Veness, Michael Bowling, Journal of Artificial Intelligence Research. Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and Bowling, Michael. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 2012.\n\nIncreasing the action gap: New operators for reinforcement learning. Marc G Bellemare, Ostrovski, Georg, Guez, Arthur, Philip S Thomas, R\u00e9mi Munos, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceBellemare, Marc G., Ostrovski, Georg, Guez, Arthur, Thomas, Philip S., and Munos, R\u00e9mi. Increasing the ac- tion gap: New operators for reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intel- ligence, 2016b.\n\nPrioritized goal decomposition of markov decision processes: Toward a synthesis of classical and decision theoretic planning. Craig Boutilier, Ronen I Brafman, Christopher Geib, IJCAI. Boutilier, Craig, Brafman, Ronen I, and Geib, Christopher. Prioritized goal decomposition of markov decision pro- cesses: Toward a synthesis of classical and decision the- oretic planning. In IJCAI, 1997.\n\nImproving generalization for temporal difference learning: The successor representation. Peter Dayan, Neural Computation. Dayan, Peter. Improving generalization for temporal dif- ference learning: The successor representation. Neural Computation, 1993.\n\nFeudal reinforcement learning. Peter Dayan, Geoffrey E Hinton, NIPS. Morgan Kaufmann PublishersDayan, Peter and Hinton, Geoffrey E. Feudal reinforce- ment learning. In NIPS. Morgan Kaufmann Publishers, 1993.\n\nHierarchical reinforcement learning with the maxq value function decomposition. Thomas G Dietterich, J. Artif. Intell. Res. Dietterich, Thomas G. Hierarchical reinforcement learning with the maxq value function decomposition. J. Artif. Intell. Res.(JAIR), 2000.\n\nLong shortterm memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. Hochreiter, Sepp and Schmidhuber, J\u00fcrgen. Long short- term memory. Neural computation, 1997.\n\nReinforcement learning with unsupervised auxiliary tasks. Max Jaderberg, Mnih, Volodymyr, Wojciech Czarnecki, Marian, Schaul, Tom, Joel Z Leibo, David Silver, Koray Kavukcuoglu, arXiv:1611.05397arXiv preprintJaderberg, Max, Mnih, Volodymyr, Czarnecki, Woj- ciech Marian, Schaul, Tom, Leibo, Joel Z, Silver, David, and Kavukcuoglu, Koray. Reinforcement learn- ing with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.\n\nHierarchical learning in stochastic domains: Preliminary results. Leslie Kaelbling, Pack, ICML. Kaelbling, Leslie Pack. Hierarchical learning in stochastic domains: Preliminary results. In ICML, 2014.\n\nA clockwork rnn. Jan Koutn\u00edk, Greff, Klaus, Faustino Gomez, J\u00fcrgen Schmidhuber, ICML. Koutn\u00edk, Jan, Greff, Klaus, Gomez, Faustino, and Schmid- huber, J\u00fcrgen. A clockwork rnn. In ICML, 2014.\n\nHierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Tejas D Kulkarni, Narasimhan, R Karthik, Ardavan Saeedi, Joshua B Tenenbaum, arXiv:1604.06057arXiv preprintKulkarni, Tejas D., Narasimhan, Karthik R., Saeedi, Arda- van, and Tenenbaum, Joshua B. Hierarchical deep rein- forcement learning: Integrating temporal abstraction and intrinsic motivation. arXiv preprint arXiv:1604.06057, 2016.\n\nBuilding machines that learn and think like people. Brenden M Lake, Ullman, D Tomer, Tenenbaum, B Joshua, Gershman Samuel, J , arXiv:1604.00289arXiv preprintLake, Brenden M, Ullman, Tomer D, Tenenbaum, Joshua B, and Gershman, Samuel J. Building ma- chines that learn and think like people. arXiv preprint arXiv:1604.00289, 2016.\n\nEnd-to-end training of deep visuomotor policies. Levine, Sergey, Chelsea Finn, Trevor Darrell, Pieter Abbeel, arXiv:1504.00702arXiv preprintLevine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel, Pieter. End-to-end training of deep visuomotor policies. arXiv preprint arXiv:1504.00702, 2015.\n\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Heess, Nicolas, Erez, Tom, Tassa, David Silver, Daan Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintLillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander, Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David, and Wierstra, Daan. Continuous control with deep re- inforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\nHuman-level control through deep reinforcement learning. Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Riedmiller, Martin, Andreas K Fidjeland, Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Shane Legg, Demis Hassabis, Nature. 5187540Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A., Veness, Joel, Bellemare, Marc G., Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K., Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis, Demis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 02 2015.\n\nMnih, Volodymyr, Adria Badia, Puigdomenech, Mirza, Mehdi, Alex Graves, Timothy P Lillicrap, Harley, Tim, David Silver, Koray Kavukcuoglu, Asynchronous methods for deep reinforcement learning. ICML. Mnih, Volodymyr, Badia, Adria Puigdomenech, Mirza, Mehdi, Graves, Alex, Lillicrap, Timothy P, Harley, Tim, Silver, David, and Kavukcuoglu, Koray. Asynchronous methods for deep reinforcement learning. ICML, 2016.\n\nSpatial localization does not require the presence of local cues. Richard Morris, Gm, Learning and motivation. 122Morris, Richard GM. Spatial localization does not require the presence of local cues. Learning and motivation, 12 (2):239-260, 1981.\n\nA focused back-propagation algorithm for temporal pattern recognition. Complex systems. Michael C Mozer, Mozer, Michael C. A focused back-propagation algo- rithm for temporal pattern recognition. Complex sys- tems, 1989.\n\nReinforcement learning with hierarchies of machines. Ronald Parr, Russell , Stuart , NIPS. Parr, Ronald and Russell, Stuart. Reinforcement learning with hierarchies of machines. NIPS, 1998.\n\nTemporal abstraction in reinforcement learning. Doina Precup, University of MassachusettsPhD thesisPrecup, Doina. Temporal abstraction in reinforcement learning. PhD thesis, University of Massachusetts, 2000.\n\nPlanning with closed-loop macro actions. Precup, Doina, Sutton, S Richard, Singh, P Satinder, Technical reportPrecup, Doina, Sutton, Richard S, and Singh, Satinder P. Planning with closed-loop macro actions. Technical re- port, 1997.\n\nTheoretical results on reinforcement learning with temporally abstract options. Precup, Doina, Sutton, S Richard, Singh, Satinder, European Conference on Machine Learning (ECML). SpringerPrecup, Doina, Sutton, Richard S, and Singh, Satinder. Theoretical results on reinforcement learning with tem- porally abstract options. In European Conference on Ma- chine Learning (ECML). Springer, 1998.\n\nUniversal value function approximators. Tom Schaul, Horgan, Dan, Karol Gregor, David Silver, Schaul, Tom, Horgan, Dan, Gregor, Karol, and Silver, David. Universal value function approximators. ICML, 2015.\n\nNeural sequence chunkers. J\u00fcrgen Schmidhuber, Technical reportSchmidhuber, J\u00fcrgen. Neural sequence chunkers. Techni- cal report, 1991.\n\nTrust region policy optimization. John Schulman, Levine, Sergey, Moritz, Jordan Philipp, I Michael, Pieter Abbeel, ICML. Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan, Michael I, and Abbeel, Pieter. Trust region policy opti- mization. In ICML, 2015.\n\nHigh-dimensional continuous control using generalized advantage estimation. ICLR. John Schulman, Moritz, Philipp, Levine, Sergey, Michael Jordan, Pieter Abbeel, Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan, Michael, and Abbeel, Pieter. High-dimensional con- tinuous control using generalized advantage estimation. ICLR, 2016.\n\nTd models: Modeling the world at a mixture of time scales. Richard S Sutton, ICML. Sutton, Richard S. Td models: Modeling the world at a mixture of time scales. In ICML, 1995.\n\nBetween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence. Richard S Sutton, Doina Precup, Singh, Satinder, Sutton, Richard S, Precup, Doina, and Singh, Satinder. Be- tween mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelli- gence, 1999.\n\nA deep hierarchical approach to lifelong learning in minecraft. Chen Tessler, Givony, Shahar, Zahavy, Tom, Mankowitz, J Daniel, Shie Mannor, arXiv:1604.07255arXiv preprintTessler, Chen, Givony, Shahar, Zahavy, Tom, Mankowitz, Daniel J, and Mannor, Shie. A deep hierarchical ap- proach to lifelong learning in minecraft. arXiv preprint arXiv:1604.07255, 2016.\n\nStrategic attentive writer for learning macro-actions. Alexander Vezhnevets, Mnih, Volodymyr, Osindero, Simon, Alex Graves, Vinyals, Oriol, John Agapiou, NIPS. Vezhnevets, Alexander, Mnih, Volodymyr, Osindero, Si- mon, Graves, Alex, Vinyals, Oriol, Agapiou, John, and kavukcuoglu, koray. Strategic attentive writer for learn- ing macro-actions. In NIPS, 2016.\n\nHq-learning. Marco Wiering, J\u00fcrgen Schmidhuber, Adaptive Behavior. Wiering, Marco and Schmidhuber, J\u00fcrgen. Hq-learning. Adaptive Behavior, 1997.\n\nMulti-scale context aggregation by dilated convolutions. ICLR. Fisher Yu, Vladlen Koltun, Yu, Fisher and Koltun, Vladlen. Multi-scale context aggre- gation by dilated convolutions. ICLR, 2016.\n", "annotations": {"author": "[{\"end\":225,\"start\":77},{\"end\":362,\"start\":226},{\"end\":495,\"start\":363},{\"end\":631,\"start\":496},{\"end\":767,\"start\":632},{\"end\":902,\"start\":768},{\"end\":1051,\"start\":903}]", "publisher": null, "author_last_name": "[{\"end\":103,\"start\":93},{\"end\":240,\"start\":232},{\"end\":373,\"start\":367},{\"end\":509,\"start\":504},{\"end\":645,\"start\":636},{\"end\":780,\"start\":774},{\"end\":929,\"start\":921}]", "author_first_name": "[{\"end\":86,\"start\":77},{\"end\":92,\"start\":87},{\"end\":231,\"start\":226},{\"end\":366,\"start\":363},{\"end\":503,\"start\":496},{\"end\":635,\"start\":632},{\"end\":773,\"start\":768},{\"end\":916,\"start\":903},{\"end\":920,\"start\":917}]", "author_affiliation": "[{\"end\":224,\"start\":105},{\"end\":361,\"start\":242},{\"end\":494,\"start\":375},{\"end\":630,\"start\":511},{\"end\":766,\"start\":647},{\"end\":901,\"start\":782},{\"end\":1050,\"start\":931}]", "title": "[{\"end\":74,\"start\":1},{\"end\":1125,\"start\":1052}]", "venue": null, "abstract": "[{\"end\":2226,\"start\":1127}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2336,\"start\":2317},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2358,\"start\":2336},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2378,\"start\":2358},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2396,\"start\":2378},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2419,\"start\":2396},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2715,\"start\":2691},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3208,\"start\":3187},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5411,\"start\":5389},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5612,\"start\":5591},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5625,\"start\":5612},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5646,\"start\":5625},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5663,\"start\":5646},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5686,\"start\":5663},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5698,\"start\":5686},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5714,\"start\":5698},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5735,\"start\":5714},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5755,\"start\":5735},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5773,\"start\":5755},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5786,\"start\":5773},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5814,\"start\":5786},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5838,\"start\":5814},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5857,\"start\":5838},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5902,\"start\":5881},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5915,\"start\":5902},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6490,\"start\":6469},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6507,\"start\":6490},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6528,\"start\":6507},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6586,\"start\":6557},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6606,\"start\":6586},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6791,\"start\":6769},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6813,\"start\":6791},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7052,\"start\":7032},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7498,\"start\":7478},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7522,\"start\":7498},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8214,\"start\":8189},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8473,\"start\":8449},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14272,\"start\":14251},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17534,\"start\":17502},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18315,\"start\":18296},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19272,\"start\":19250},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20307,\"start\":20275},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21008,\"start\":20995},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22896,\"start\":22872},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23741,\"start\":23719},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25586,\"start\":25561},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25603,\"start\":25586},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27471,\"start\":27451},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27642,\"start\":27622},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27710,\"start\":27691},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28387,\"start\":28366},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28713,\"start\":28699},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31769,\"start\":31749},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32008,\"start\":31985},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32026,\"start\":32008},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32049,\"start\":32026},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35856,\"start\":35837},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35880,\"start\":35856},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":35904,\"start\":35880}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38039,\"start\":37981},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38316,\"start\":38040},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38556,\"start\":38317},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39307,\"start\":38557},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39363,\"start\":39308},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39816,\"start\":39364},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39903,\"start\":39817},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41272,\"start\":39904}]", "paragraph": "[{\"end\":3087,\"start\":2242},{\"end\":3685,\"start\":3089},{\"end\":4494,\"start\":3687},{\"end\":5098,\"start\":4496},{\"end\":5494,\"start\":5100},{\"end\":7013,\"start\":5511},{\"end\":7726,\"start\":7015},{\"end\":8070,\"start\":7728},{\"end\":8741,\"start\":8072},{\"end\":9718,\"start\":8755},{\"end\":9745,\"start\":9742},{\"end\":9833,\"start\":9830},{\"end\":10407,\"start\":9928},{\"end\":11371,\"start\":10426},{\"end\":12000,\"start\":11384},{\"end\":12940,\"start\":12002},{\"end\":12997,\"start\":12992},{\"end\":13495,\"start\":13028},{\"end\":13579,\"start\":13497},{\"end\":14191,\"start\":13630},{\"end\":14829,\"start\":14193},{\"end\":14892,\"start\":14870},{\"end\":15028,\"start\":14933},{\"end\":15281,\"start\":15030},{\"end\":16512,\"start\":15313},{\"end\":17443,\"start\":16609},{\"end\":18089,\"start\":17445},{\"end\":18847,\"start\":18106},{\"end\":19488,\"start\":18849},{\"end\":20163,\"start\":19504},{\"end\":20761,\"start\":20165},{\"end\":21357,\"start\":20763},{\"end\":22774,\"start\":21370},{\"end\":24299,\"start\":22798},{\"end\":24580,\"start\":24309},{\"end\":26419,\"start\":24582},{\"end\":26803,\"start\":26421},{\"end\":27422,\"start\":26840},{\"end\":28329,\"start\":27424},{\"end\":28632,\"start\":28353},{\"end\":29163,\"start\":28634},{\"end\":30013,\"start\":29165},{\"end\":30598,\"start\":30015},{\"end\":31215,\"start\":30600},{\"end\":32669,\"start\":31237},{\"end\":33587,\"start\":32671},{\"end\":34104,\"start\":33589},{\"end\":34212,\"start\":34106},{\"end\":34691,\"start\":34214},{\"end\":35380,\"start\":34724},{\"end\":36573,\"start\":35382},{\"end\":37980,\"start\":36604}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9741,\"start\":9719},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9829,\"start\":9746},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9927,\"start\":9834},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12991,\"start\":12941},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13027,\"start\":12998},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13629,\"start\":13580},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14869,\"start\":14830},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14932,\"start\":14893},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16585,\"start\":16513}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2240,\"start\":2228},{\"attributes\":{\"n\":\"2.\"},\"end\":5509,\"start\":5497},{\"attributes\":{\"n\":\"3.\"},\"end\":8753,\"start\":8744},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10424,\"start\":10410},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11382,\"start\":11374},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15311,\"start\":15284},{\"attributes\":{\"n\":\"4.\"},\"end\":16607,\"start\":16587},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18104,\"start\":18092},{\"attributes\":{\"n\":\"5.\"},\"end\":19502,\"start\":19491},{\"end\":21368,\"start\":21360},{\"attributes\":{\"n\":\"5.1.\"},\"end\":22796,\"start\":22777},{\"attributes\":{\"n\":\"5.2.\"},\"end\":24307,\"start\":24302},{\"end\":26838,\"start\":26806},{\"attributes\":{\"n\":\"5.3.\"},\"end\":28351,\"start\":28332},{\"attributes\":{\"n\":\"5.4.\"},\"end\":31235,\"start\":31218},{\"attributes\":{\"n\":\"5.5.\"},\"end\":34722,\"start\":34694},{\"attributes\":{\"n\":\"6.\"},\"end\":36602,\"start\":36576},{\"end\":37992,\"start\":37982},{\"end\":38051,\"start\":38041},{\"end\":38328,\"start\":38318},{\"end\":38588,\"start\":38558},{\"end\":39329,\"start\":39309},{\"end\":39387,\"start\":39365},{\"end\":39829,\"start\":39818}]", "table": null, "figure_caption": "[{\"end\":38039,\"start\":37994},{\"end\":38316,\"start\":38053},{\"end\":38556,\"start\":38330},{\"end\":39307,\"start\":38592},{\"end\":39363,\"start\":39332},{\"end\":39816,\"start\":39392},{\"end\":39903,\"start\":39832},{\"end\":41272,\"start\":39906}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9613,\"start\":9604},{\"end\":21449,\"start\":21441},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23423,\"start\":23414},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23816,\"start\":23807},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25026,\"start\":25018},{\"end\":27205,\"start\":27197},{\"end\":28048,\"start\":28040},{\"end\":30672,\"start\":30663},{\"end\":30731,\"start\":30723},{\"end\":30927,\"start\":30918},{\"end\":32282,\"start\":32274},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33049,\"start\":33040},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33882,\"start\":33873},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34564,\"start\":34555},{\"end\":36329,\"start\":36321}]", "bib_author_first_name": "[{\"end\":41730,\"start\":41720},{\"end\":41743,\"start\":41738},{\"end\":41756,\"start\":41752},{\"end\":41876,\"start\":41869},{\"end\":41890,\"start\":41886},{\"end\":41892,\"start\":41891},{\"end\":42053,\"start\":42045},{\"end\":42111,\"start\":42105},{\"end\":42173,\"start\":42168},{\"end\":42184,\"start\":42180},{\"end\":42694,\"start\":42690},{\"end\":42762,\"start\":42757},{\"end\":42775,\"start\":42771},{\"end\":43047,\"start\":43043},{\"end\":43049,\"start\":43048},{\"end\":43080,\"start\":43076},{\"end\":43096,\"start\":43089},{\"end\":43422,\"start\":43418},{\"end\":43424,\"start\":43423},{\"end\":43474,\"start\":43468},{\"end\":43476,\"start\":43475},{\"end\":43489,\"start\":43485},{\"end\":43972,\"start\":43967},{\"end\":43989,\"start\":43984},{\"end\":43991,\"start\":43990},{\"end\":44012,\"start\":44001},{\"end\":44326,\"start\":44321},{\"end\":44522,\"start\":44517},{\"end\":44538,\"start\":44530},{\"end\":44540,\"start\":44539},{\"end\":44781,\"start\":44775},{\"end\":44783,\"start\":44782},{\"end\":44985,\"start\":44981},{\"end\":45004,\"start\":44998},{\"end\":45193,\"start\":45190},{\"end\":45230,\"start\":45222},{\"end\":45267,\"start\":45263},{\"end\":45269,\"start\":45268},{\"end\":45282,\"start\":45277},{\"end\":45296,\"start\":45291},{\"end\":45642,\"start\":45636},{\"end\":45792,\"start\":45789},{\"end\":45824,\"start\":45816},{\"end\":45838,\"start\":45832},{\"end\":46069,\"start\":46064},{\"end\":46071,\"start\":46070},{\"end\":46095,\"start\":46094},{\"end\":46112,\"start\":46105},{\"end\":46127,\"start\":46121},{\"end\":46129,\"start\":46128},{\"end\":46461,\"start\":46454},{\"end\":46463,\"start\":46462},{\"end\":46479,\"start\":46478},{\"end\":46499,\"start\":46498},{\"end\":46516,\"start\":46508},{\"end\":46526,\"start\":46525},{\"end\":46804,\"start\":46797},{\"end\":46817,\"start\":46811},{\"end\":46833,\"start\":46827},{\"end\":47036,\"start\":47029},{\"end\":47038,\"start\":47037},{\"end\":47058,\"start\":47050},{\"end\":47060,\"start\":47059},{\"end\":47076,\"start\":47067},{\"end\":47125,\"start\":47120},{\"end\":47138,\"start\":47134},{\"end\":47578,\"start\":47572},{\"end\":47580,\"start\":47579},{\"end\":47591,\"start\":47587},{\"end\":47604,\"start\":47600},{\"end\":47606,\"start\":47605},{\"end\":47622,\"start\":47618},{\"end\":47658,\"start\":47651},{\"end\":47660,\"start\":47659},{\"end\":47811,\"start\":47806},{\"end\":47823,\"start\":47818},{\"end\":48293,\"start\":48288},{\"end\":48333,\"start\":48329},{\"end\":48349,\"start\":48342},{\"end\":48351,\"start\":48350},{\"end\":48381,\"start\":48376},{\"end\":48395,\"start\":48390},{\"end\":48755,\"start\":48748},{\"end\":49025,\"start\":49018},{\"end\":49027,\"start\":49026},{\"end\":49211,\"start\":49205},{\"end\":49225,\"start\":49218},{\"end\":49234,\"start\":49228},{\"end\":49396,\"start\":49391},{\"end\":49618,\"start\":49617},{\"end\":49636,\"start\":49635},{\"end\":49892,\"start\":49891},{\"end\":50225,\"start\":50222},{\"end\":50252,\"start\":50247},{\"end\":50266,\"start\":50261},{\"end\":50420,\"start\":50414},{\"end\":50562,\"start\":50558},{\"end\":50603,\"start\":50597},{\"end\":50614,\"start\":50613},{\"end\":50630,\"start\":50624},{\"end\":50871,\"start\":50867},{\"end\":50922,\"start\":50915},{\"end\":50937,\"start\":50931},{\"end\":51189,\"start\":51182},{\"end\":51191,\"start\":51190},{\"end\":51424,\"start\":51417},{\"end\":51426,\"start\":51425},{\"end\":51440,\"start\":51435},{\"end\":51717,\"start\":51713},{\"end\":51768,\"start\":51767},{\"end\":51781,\"start\":51777},{\"end\":52073,\"start\":52064},{\"end\":52124,\"start\":52120},{\"end\":52153,\"start\":52149},{\"end\":52388,\"start\":52383},{\"end\":52404,\"start\":52398},{\"end\":52585,\"start\":52579},{\"end\":52597,\"start\":52590}]", "bib_author_last_name": "[{\"end\":41736,\"start\":41731},{\"end\":41750,\"start\":41744},{\"end\":41761,\"start\":41757},{\"end\":41884,\"start\":41877},{\"end\":41898,\"start\":41893},{\"end\":41910,\"start\":41900},{\"end\":41917,\"start\":41912},{\"end\":41923,\"start\":41919},{\"end\":41928,\"start\":41925},{\"end\":41940,\"start\":41930},{\"end\":41948,\"start\":41942},{\"end\":41957,\"start\":41950},{\"end\":41967,\"start\":41959},{\"end\":41977,\"start\":41969},{\"end\":41985,\"start\":41979},{\"end\":41992,\"start\":41987},{\"end\":41999,\"start\":41994},{\"end\":42007,\"start\":42001},{\"end\":42015,\"start\":42009},{\"end\":42022,\"start\":42017},{\"end\":42028,\"start\":42024},{\"end\":42043,\"start\":42030},{\"end\":42060,\"start\":42054},{\"end\":42067,\"start\":42062},{\"end\":42073,\"start\":42069},{\"end\":42080,\"start\":42075},{\"end\":42086,\"start\":42082},{\"end\":42091,\"start\":42088},{\"end\":42097,\"start\":42093},{\"end\":42103,\"start\":42099},{\"end\":42118,\"start\":42112},{\"end\":42127,\"start\":42120},{\"end\":42136,\"start\":42129},{\"end\":42142,\"start\":42138},{\"end\":42149,\"start\":42144},{\"end\":42159,\"start\":42151},{\"end\":42166,\"start\":42161},{\"end\":42178,\"start\":42174},{\"end\":42193,\"start\":42185},{\"end\":42203,\"start\":42195},{\"end\":42208,\"start\":42205},{\"end\":42704,\"start\":42695},{\"end\":42716,\"start\":42706},{\"end\":42724,\"start\":42718},{\"end\":42735,\"start\":42726},{\"end\":42742,\"start\":42737},{\"end\":42750,\"start\":42744},{\"end\":42755,\"start\":42752},{\"end\":42769,\"start\":42763},{\"end\":42781,\"start\":42776},{\"end\":43059,\"start\":43050},{\"end\":43067,\"start\":43061},{\"end\":43074,\"start\":43069},{\"end\":43087,\"start\":43081},{\"end\":43104,\"start\":43097},{\"end\":43434,\"start\":43425},{\"end\":43445,\"start\":43436},{\"end\":43452,\"start\":43447},{\"end\":43458,\"start\":43454},{\"end\":43466,\"start\":43460},{\"end\":43483,\"start\":43477},{\"end\":43495,\"start\":43490},{\"end\":43982,\"start\":43973},{\"end\":43999,\"start\":43992},{\"end\":44017,\"start\":44013},{\"end\":44332,\"start\":44327},{\"end\":44528,\"start\":44523},{\"end\":44547,\"start\":44541},{\"end\":44794,\"start\":44784},{\"end\":44996,\"start\":44986},{\"end\":45016,\"start\":45005},{\"end\":45203,\"start\":45194},{\"end\":45209,\"start\":45205},{\"end\":45220,\"start\":45211},{\"end\":45240,\"start\":45231},{\"end\":45248,\"start\":45242},{\"end\":45256,\"start\":45250},{\"end\":45261,\"start\":45258},{\"end\":45275,\"start\":45270},{\"end\":45289,\"start\":45283},{\"end\":45308,\"start\":45297},{\"end\":45652,\"start\":45643},{\"end\":45658,\"start\":45654},{\"end\":45800,\"start\":45793},{\"end\":45807,\"start\":45802},{\"end\":45814,\"start\":45809},{\"end\":45830,\"start\":45825},{\"end\":45850,\"start\":45839},{\"end\":46080,\"start\":46072},{\"end\":46092,\"start\":46082},{\"end\":46103,\"start\":46096},{\"end\":46119,\"start\":46113},{\"end\":46139,\"start\":46130},{\"end\":46468,\"start\":46464},{\"end\":46476,\"start\":46470},{\"end\":46485,\"start\":46480},{\"end\":46496,\"start\":46487},{\"end\":46506,\"start\":46500},{\"end\":46523,\"start\":46517},{\"end\":46787,\"start\":46781},{\"end\":46795,\"start\":46789},{\"end\":46809,\"start\":46805},{\"end\":46825,\"start\":46818},{\"end\":46840,\"start\":46834},{\"end\":47048,\"start\":47039},{\"end\":47065,\"start\":47061},{\"end\":47084,\"start\":47077},{\"end\":47091,\"start\":47086},{\"end\":47100,\"start\":47093},{\"end\":47106,\"start\":47102},{\"end\":47111,\"start\":47108},{\"end\":47118,\"start\":47113},{\"end\":47132,\"start\":47126},{\"end\":47147,\"start\":47139},{\"end\":47524,\"start\":47520},{\"end\":47535,\"start\":47526},{\"end\":47548,\"start\":47537},{\"end\":47555,\"start\":47550},{\"end\":47563,\"start\":47557},{\"end\":47570,\"start\":47565},{\"end\":47585,\"start\":47581},{\"end\":47598,\"start\":47592},{\"end\":47616,\"start\":47607},{\"end\":47629,\"start\":47623},{\"end\":47641,\"start\":47631},{\"end\":47649,\"start\":47643},{\"end\":47670,\"start\":47661},{\"end\":47681,\"start\":47672},{\"end\":47688,\"start\":47683},{\"end\":47698,\"start\":47690},{\"end\":47704,\"start\":47700},{\"end\":47713,\"start\":47706},{\"end\":47722,\"start\":47715},{\"end\":47729,\"start\":47724},{\"end\":47735,\"start\":47731},{\"end\":47747,\"start\":47737},{\"end\":47756,\"start\":47749},{\"end\":47762,\"start\":47758},{\"end\":47769,\"start\":47764},{\"end\":47778,\"start\":47771},{\"end\":47788,\"start\":47780},{\"end\":47798,\"start\":47790},{\"end\":47804,\"start\":47800},{\"end\":47816,\"start\":47812},{\"end\":47832,\"start\":47824},{\"end\":48275,\"start\":48271},{\"end\":48286,\"start\":48277},{\"end\":48299,\"start\":48294},{\"end\":48313,\"start\":48301},{\"end\":48320,\"start\":48315},{\"end\":48327,\"start\":48322},{\"end\":48340,\"start\":48334},{\"end\":48361,\"start\":48352},{\"end\":48369,\"start\":48363},{\"end\":48374,\"start\":48371},{\"end\":48388,\"start\":48382},{\"end\":48407,\"start\":48396},{\"end\":48762,\"start\":48756},{\"end\":48766,\"start\":48764},{\"end\":49033,\"start\":49028},{\"end\":49216,\"start\":49212},{\"end\":49403,\"start\":49397},{\"end\":49600,\"start\":49594},{\"end\":49607,\"start\":49602},{\"end\":49615,\"start\":49609},{\"end\":49626,\"start\":49619},{\"end\":49633,\"start\":49628},{\"end\":49645,\"start\":49637},{\"end\":49874,\"start\":49868},{\"end\":49881,\"start\":49876},{\"end\":49889,\"start\":49883},{\"end\":49900,\"start\":49893},{\"end\":49907,\"start\":49902},{\"end\":49917,\"start\":49909},{\"end\":50232,\"start\":50226},{\"end\":50240,\"start\":50234},{\"end\":50245,\"start\":50242},{\"end\":50259,\"start\":50253},{\"end\":50273,\"start\":50267},{\"end\":50432,\"start\":50421},{\"end\":50571,\"start\":50563},{\"end\":50579,\"start\":50573},{\"end\":50587,\"start\":50581},{\"end\":50595,\"start\":50589},{\"end\":50611,\"start\":50604},{\"end\":50622,\"start\":50615},{\"end\":50637,\"start\":50631},{\"end\":50880,\"start\":50872},{\"end\":50888,\"start\":50882},{\"end\":50897,\"start\":50890},{\"end\":50905,\"start\":50899},{\"end\":50913,\"start\":50907},{\"end\":50929,\"start\":50923},{\"end\":50944,\"start\":50938},{\"end\":51198,\"start\":51192},{\"end\":51433,\"start\":51427},{\"end\":51447,\"start\":51441},{\"end\":51454,\"start\":51449},{\"end\":51464,\"start\":51456},{\"end\":51725,\"start\":51718},{\"end\":51733,\"start\":51727},{\"end\":51741,\"start\":51735},{\"end\":51749,\"start\":51743},{\"end\":51754,\"start\":51751},{\"end\":51765,\"start\":51756},{\"end\":51775,\"start\":51769},{\"end\":51788,\"start\":51782},{\"end\":52084,\"start\":52074},{\"end\":52090,\"start\":52086},{\"end\":52101,\"start\":52092},{\"end\":52111,\"start\":52103},{\"end\":52118,\"start\":52113},{\"end\":52131,\"start\":52125},{\"end\":52140,\"start\":52133},{\"end\":52147,\"start\":52142},{\"end\":52161,\"start\":52154},{\"end\":52396,\"start\":52389},{\"end\":52416,\"start\":52405},{\"end\":52588,\"start\":52586},{\"end\":52604,\"start\":52598}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6627476},\"end\":41865,\"start\":41688},{\"attributes\":{\"doi\":\"arXiv:1612.03801\",\"id\":\"b1\"},\"end\":42629,\"start\":41867},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8310565},\"end\":42965,\"start\":42631},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1552061},\"end\":43347,\"start\":42967},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1907310},\"end\":43839,\"start\":43349},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2248528},\"end\":44230,\"start\":43841},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":12559116},\"end\":44484,\"start\":44232},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2801572},\"end\":44693,\"start\":44486},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":57341},\"end\":44956,\"start\":44695},{\"attributes\":{\"id\":\"b9\"},\"end\":45130,\"start\":44958},{\"attributes\":{\"doi\":\"arXiv:1611.05397\",\"id\":\"b10\"},\"end\":45568,\"start\":45132},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15784464},\"end\":45770,\"start\":45570},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14936429},\"end\":45961,\"start\":45772},{\"attributes\":{\"doi\":\"arXiv:1604.06057\",\"id\":\"b13\"},\"end\":46400,\"start\":45963},{\"attributes\":{\"doi\":\"arXiv:1604.00289\",\"id\":\"b14\"},\"end\":46730,\"start\":46402},{\"attributes\":{\"doi\":\"arXiv:1504.00702\",\"id\":\"b15\"},\"end\":47027,\"start\":46732},{\"attributes\":{\"doi\":\"arXiv:1509.02971\",\"id\":\"b16\"},\"end\":47461,\"start\":47029},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":205242740},\"end\":48269,\"start\":47463},{\"attributes\":{\"id\":\"b18\"},\"end\":48680,\"start\":48271},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":145235077},\"end\":48928,\"start\":48682},{\"attributes\":{\"id\":\"b20\"},\"end\":49150,\"start\":48930},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6760236},\"end\":49341,\"start\":49152},{\"attributes\":{\"id\":\"b22\"},\"end\":49551,\"start\":49343},{\"attributes\":{\"id\":\"b23\"},\"end\":49786,\"start\":49553},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":886415},\"end\":50180,\"start\":49788},{\"attributes\":{\"id\":\"b25\"},\"end\":50386,\"start\":50182},{\"attributes\":{\"id\":\"b26\"},\"end\":50522,\"start\":50388},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":16046818},\"end\":50783,\"start\":50524},{\"attributes\":{\"id\":\"b28\"},\"end\":51121,\"start\":50785},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":11070703},\"end\":51298,\"start\":51123},{\"attributes\":{\"id\":\"b30\"},\"end\":51647,\"start\":51300},{\"attributes\":{\"doi\":\"arXiv:1604.07255\",\"id\":\"b31\"},\"end\":52007,\"start\":51649},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":341469},\"end\":52368,\"start\":52009},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":209028833},\"end\":52514,\"start\":52370},{\"attributes\":{\"id\":\"b34\"},\"end\":52708,\"start\":52516}]", "bib_title": "[{\"end\":41718,\"start\":41688},{\"end\":42688,\"start\":42631},{\"end\":43041,\"start\":42967},{\"end\":43416,\"start\":43349},{\"end\":43965,\"start\":43841},{\"end\":44319,\"start\":44232},{\"end\":44515,\"start\":44486},{\"end\":44773,\"start\":44695},{\"end\":44979,\"start\":44958},{\"end\":45634,\"start\":45570},{\"end\":45787,\"start\":45772},{\"end\":47518,\"start\":47463},{\"end\":48746,\"start\":48682},{\"end\":49203,\"start\":49152},{\"end\":49866,\"start\":49788},{\"end\":50556,\"start\":50524},{\"end\":51180,\"start\":51123},{\"end\":52062,\"start\":52009},{\"end\":52381,\"start\":52370}]", "bib_author": "[{\"end\":41738,\"start\":41720},{\"end\":41752,\"start\":41738},{\"end\":41763,\"start\":41752},{\"end\":41886,\"start\":41869},{\"end\":41900,\"start\":41886},{\"end\":41912,\"start\":41900},{\"end\":41919,\"start\":41912},{\"end\":41925,\"start\":41919},{\"end\":41930,\"start\":41925},{\"end\":41942,\"start\":41930},{\"end\":41950,\"start\":41942},{\"end\":41959,\"start\":41950},{\"end\":41969,\"start\":41959},{\"end\":41979,\"start\":41969},{\"end\":41987,\"start\":41979},{\"end\":41994,\"start\":41987},{\"end\":42001,\"start\":41994},{\"end\":42009,\"start\":42001},{\"end\":42017,\"start\":42009},{\"end\":42024,\"start\":42017},{\"end\":42030,\"start\":42024},{\"end\":42045,\"start\":42030},{\"end\":42062,\"start\":42045},{\"end\":42069,\"start\":42062},{\"end\":42075,\"start\":42069},{\"end\":42082,\"start\":42075},{\"end\":42088,\"start\":42082},{\"end\":42093,\"start\":42088},{\"end\":42099,\"start\":42093},{\"end\":42105,\"start\":42099},{\"end\":42120,\"start\":42105},{\"end\":42129,\"start\":42120},{\"end\":42138,\"start\":42129},{\"end\":42144,\"start\":42138},{\"end\":42151,\"start\":42144},{\"end\":42161,\"start\":42151},{\"end\":42168,\"start\":42161},{\"end\":42180,\"start\":42168},{\"end\":42195,\"start\":42180},{\"end\":42205,\"start\":42195},{\"end\":42210,\"start\":42205},{\"end\":42706,\"start\":42690},{\"end\":42718,\"start\":42706},{\"end\":42726,\"start\":42718},{\"end\":42737,\"start\":42726},{\"end\":42744,\"start\":42737},{\"end\":42752,\"start\":42744},{\"end\":42757,\"start\":42752},{\"end\":42771,\"start\":42757},{\"end\":42783,\"start\":42771},{\"end\":43061,\"start\":43043},{\"end\":43069,\"start\":43061},{\"end\":43076,\"start\":43069},{\"end\":43089,\"start\":43076},{\"end\":43106,\"start\":43089},{\"end\":43436,\"start\":43418},{\"end\":43447,\"start\":43436},{\"end\":43454,\"start\":43447},{\"end\":43460,\"start\":43454},{\"end\":43468,\"start\":43460},{\"end\":43485,\"start\":43468},{\"end\":43497,\"start\":43485},{\"end\":43984,\"start\":43967},{\"end\":44001,\"start\":43984},{\"end\":44019,\"start\":44001},{\"end\":44334,\"start\":44321},{\"end\":44530,\"start\":44517},{\"end\":44549,\"start\":44530},{\"end\":44796,\"start\":44775},{\"end\":44998,\"start\":44981},{\"end\":45018,\"start\":44998},{\"end\":45205,\"start\":45190},{\"end\":45211,\"start\":45205},{\"end\":45222,\"start\":45211},{\"end\":45242,\"start\":45222},{\"end\":45250,\"start\":45242},{\"end\":45258,\"start\":45250},{\"end\":45263,\"start\":45258},{\"end\":45277,\"start\":45263},{\"end\":45291,\"start\":45277},{\"end\":45310,\"start\":45291},{\"end\":45654,\"start\":45636},{\"end\":45660,\"start\":45654},{\"end\":45802,\"start\":45789},{\"end\":45809,\"start\":45802},{\"end\":45816,\"start\":45809},{\"end\":45832,\"start\":45816},{\"end\":45852,\"start\":45832},{\"end\":46082,\"start\":46064},{\"end\":46094,\"start\":46082},{\"end\":46105,\"start\":46094},{\"end\":46121,\"start\":46105},{\"end\":46141,\"start\":46121},{\"end\":46470,\"start\":46454},{\"end\":46478,\"start\":46470},{\"end\":46487,\"start\":46478},{\"end\":46498,\"start\":46487},{\"end\":46508,\"start\":46498},{\"end\":46525,\"start\":46508},{\"end\":46529,\"start\":46525},{\"end\":46789,\"start\":46781},{\"end\":46797,\"start\":46789},{\"end\":46811,\"start\":46797},{\"end\":46827,\"start\":46811},{\"end\":46842,\"start\":46827},{\"end\":47050,\"start\":47029},{\"end\":47067,\"start\":47050},{\"end\":47086,\"start\":47067},{\"end\":47093,\"start\":47086},{\"end\":47102,\"start\":47093},{\"end\":47108,\"start\":47102},{\"end\":47113,\"start\":47108},{\"end\":47120,\"start\":47113},{\"end\":47134,\"start\":47120},{\"end\":47149,\"start\":47134},{\"end\":47526,\"start\":47520},{\"end\":47537,\"start\":47526},{\"end\":47550,\"start\":47537},{\"end\":47557,\"start\":47550},{\"end\":47565,\"start\":47557},{\"end\":47572,\"start\":47565},{\"end\":47587,\"start\":47572},{\"end\":47600,\"start\":47587},{\"end\":47618,\"start\":47600},{\"end\":47631,\"start\":47618},{\"end\":47643,\"start\":47631},{\"end\":47651,\"start\":47643},{\"end\":47672,\"start\":47651},{\"end\":47683,\"start\":47672},{\"end\":47690,\"start\":47683},{\"end\":47700,\"start\":47690},{\"end\":47706,\"start\":47700},{\"end\":47715,\"start\":47706},{\"end\":47724,\"start\":47715},{\"end\":47731,\"start\":47724},{\"end\":47737,\"start\":47731},{\"end\":47749,\"start\":47737},{\"end\":47758,\"start\":47749},{\"end\":47764,\"start\":47758},{\"end\":47771,\"start\":47764},{\"end\":47780,\"start\":47771},{\"end\":47790,\"start\":47780},{\"end\":47800,\"start\":47790},{\"end\":47806,\"start\":47800},{\"end\":47818,\"start\":47806},{\"end\":47834,\"start\":47818},{\"end\":48277,\"start\":48271},{\"end\":48288,\"start\":48277},{\"end\":48301,\"start\":48288},{\"end\":48315,\"start\":48301},{\"end\":48322,\"start\":48315},{\"end\":48329,\"start\":48322},{\"end\":48342,\"start\":48329},{\"end\":48363,\"start\":48342},{\"end\":48371,\"start\":48363},{\"end\":48376,\"start\":48371},{\"end\":48390,\"start\":48376},{\"end\":48409,\"start\":48390},{\"end\":48764,\"start\":48748},{\"end\":48768,\"start\":48764},{\"end\":49035,\"start\":49018},{\"end\":49218,\"start\":49205},{\"end\":49228,\"start\":49218},{\"end\":49237,\"start\":49228},{\"end\":49405,\"start\":49391},{\"end\":49602,\"start\":49594},{\"end\":49609,\"start\":49602},{\"end\":49617,\"start\":49609},{\"end\":49628,\"start\":49617},{\"end\":49635,\"start\":49628},{\"end\":49647,\"start\":49635},{\"end\":49876,\"start\":49868},{\"end\":49883,\"start\":49876},{\"end\":49891,\"start\":49883},{\"end\":49902,\"start\":49891},{\"end\":49909,\"start\":49902},{\"end\":49919,\"start\":49909},{\"end\":50234,\"start\":50222},{\"end\":50242,\"start\":50234},{\"end\":50247,\"start\":50242},{\"end\":50261,\"start\":50247},{\"end\":50275,\"start\":50261},{\"end\":50434,\"start\":50414},{\"end\":50573,\"start\":50558},{\"end\":50581,\"start\":50573},{\"end\":50589,\"start\":50581},{\"end\":50597,\"start\":50589},{\"end\":50613,\"start\":50597},{\"end\":50624,\"start\":50613},{\"end\":50639,\"start\":50624},{\"end\":50882,\"start\":50867},{\"end\":50890,\"start\":50882},{\"end\":50899,\"start\":50890},{\"end\":50907,\"start\":50899},{\"end\":50915,\"start\":50907},{\"end\":50931,\"start\":50915},{\"end\":50946,\"start\":50931},{\"end\":51200,\"start\":51182},{\"end\":51435,\"start\":51417},{\"end\":51449,\"start\":51435},{\"end\":51456,\"start\":51449},{\"end\":51466,\"start\":51456},{\"end\":51727,\"start\":51713},{\"end\":51735,\"start\":51727},{\"end\":51743,\"start\":51735},{\"end\":51751,\"start\":51743},{\"end\":51756,\"start\":51751},{\"end\":51767,\"start\":51756},{\"end\":51777,\"start\":51767},{\"end\":51790,\"start\":51777},{\"end\":52086,\"start\":52064},{\"end\":52092,\"start\":52086},{\"end\":52103,\"start\":52092},{\"end\":52113,\"start\":52103},{\"end\":52120,\"start\":52113},{\"end\":52133,\"start\":52120},{\"end\":52142,\"start\":52133},{\"end\":52149,\"start\":52142},{\"end\":52163,\"start\":52149},{\"end\":52398,\"start\":52383},{\"end\":52418,\"start\":52398},{\"end\":52590,\"start\":52579},{\"end\":52606,\"start\":52590}]", "bib_venue": "[{\"end\":43606,\"start\":43560},{\"end\":41767,\"start\":41763},{\"end\":42787,\"start\":42783},{\"end\":43149,\"start\":43106},{\"end\":43558,\"start\":43497},{\"end\":44024,\"start\":44019},{\"end\":44352,\"start\":44334},{\"end\":44553,\"start\":44549},{\"end\":44817,\"start\":44796},{\"end\":45036,\"start\":45018},{\"end\":45188,\"start\":45132},{\"end\":45664,\"start\":45660},{\"end\":45856,\"start\":45852},{\"end\":46062,\"start\":45963},{\"end\":46452,\"start\":46402},{\"end\":46779,\"start\":46732},{\"end\":47216,\"start\":47165},{\"end\":47840,\"start\":47834},{\"end\":48467,\"start\":48409},{\"end\":48791,\"start\":48768},{\"end\":49016,\"start\":48930},{\"end\":49241,\"start\":49237},{\"end\":49389,\"start\":49343},{\"end\":49592,\"start\":49553},{\"end\":49965,\"start\":49919},{\"end\":50220,\"start\":50182},{\"end\":50412,\"start\":50388},{\"end\":50643,\"start\":50639},{\"end\":50865,\"start\":50785},{\"end\":51204,\"start\":51200},{\"end\":51415,\"start\":51300},{\"end\":51711,\"start\":51649},{\"end\":52167,\"start\":52163},{\"end\":52435,\"start\":52418},{\"end\":52577,\"start\":52516}]"}}}, "year": 2023, "month": 12, "day": 17}