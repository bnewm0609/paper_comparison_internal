{"id": 258291564, "updated": "2023-10-05 01:39:18.147", "metadata": {"title": "Reconciling High Accuracy, Cost-Efficiency, and Low Latency of Inference Serving Systems", "authors": "[{\"first\":\"Mehran\",\"last\":\"Salmani\",\"middle\":[]},{\"first\":\"Saeid\",\"last\":\"Ghafouri\",\"middle\":[]},{\"first\":\"Alireza\",\"last\":\"Sanaee\",\"middle\":[]},{\"first\":\"Kamran\",\"last\":\"Razavi\",\"middle\":[]},{\"first\":\"Max\",\"last\":\"Muhlhauser\",\"middle\":[]},{\"first\":\"Joseph\",\"last\":\"Doyle\",\"middle\":[]},{\"first\":\"Pooyan\",\"last\":\"Jamshidi\",\"middle\":[]},{\"first\":\"Mohsen\",\"last\":\"Science\",\"middle\":[\"Sharifi\",\"Iran\",\"University\",\"of\"]},{\"first\":\"\",\"last\":\"Technology\",\"middle\":[]},{\"first\":\"Queen\",\"last\":\"London\",\"middle\":[\"Mary\",\"University\",\"of\"]},{\"first\":\"Technical\",\"last\":\"Darmstadt\",\"middle\":[\"University\",\"of\"]},{\"first\":\"University\",\"last\":\"Carolina\",\"middle\":[\"of\",\"South\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "The use of machine learning (ML) inference for various applications is growing drastically. ML inference services engage with users directly, requiring fast and accurate responses. Moreover, these services face dynamic workloads of requests, imposing changes in their computing resources. Failing to right-size computing resources results in either latency service level objectives (SLOs) violations or wasted computing resources. Adapting to dynamic workloads considering all the pillars of accuracy, latency, and resource cost is challenging. In response to these challenges, we propose InfAdapter, which proactively selects a set of ML model variants with their resource allocations to meet latency SLO while maximizing an objective function composed of accuracy and cost. InfAdapter decreases SLO violation and costs up to 65% and 33%, respectively, compared to a popular industry autoscaler (Kubernetes Vertical Pod Autoscaler).", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2304.10892", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eurosys/SalmaniGSRMDJS23", "doi": "10.1145/3578356.3592578"}}, "content": {"source": {"pdf_hash": "19fa9e33a2a2b5e46c97c3f21771a3885b897771", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.10892v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f5a7e8303583ced106685dcbd25713c52d06d630", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/19fa9e33a2a2b5e46c97c3f21771a3885b897771.txt", "contents": "\nReconciling High Accuracy, Cost-Efficiency, and Low Latency of Inference Serving Systems\n\n\nMehran Salmani \nIran University of Science and Technology *\nMary University of London \u00a7\nTechnical University of Darmstadt \u2020\nUniversity of South Carolina \u2021\nQueen\n\nSaeid Ghafouri \nIran University of Science and Technology *\nMary University of London \u00a7\nTechnical University of Darmstadt \u2020\nUniversity of South Carolina \u2021\nQueen\n\nAlireza Sanaee \nIran University of Science and Technology *\nMary University of London \u00a7\nTechnical University of Darmstadt \u2020\nUniversity of South Carolina \u2021\nQueen\n\nKamran Razavi \nIran University of Science and Technology *\nMary University of London \u00a7\nTechnical University of Darmstadt \u2020\nUniversity of South Carolina \u2021\nQueen\n\nMax M\u00fchlh\u00e4user \nIran University of Science and Technology *\nMary University of London \u00a7\nTechnical University of Darmstadt \u2020\nUniversity of South Carolina \u2021\nQueen\n\nJoseph Doyle \nIran University of Science and Technology *\nMary University of London \u00a7\nTechnical University of Darmstadt \u2020\nUniversity of South Carolina \u2021\nQueen\n\nPooyan Jamshidi \nIran University of Science and Technology *\nMary University of London \u00a7\nTechnical University of Darmstadt \u2020\nUniversity of South Carolina \u2021\nQueen\n\nMohsen Sharifi \nIran University of Science and Technology *\nMary University of London \u00a7\nTechnical University of Darmstadt \u2020\nUniversity of South Carolina \u2021\nQueen\n\nReconciling High Accuracy, Cost-Efficiency, and Low Latency of Inference Serving Systems\n10.1145/3578356.3592578CCS CONCEPTS \u2022 Computer systems organization \u2192 Cloud comput- ingReconfigurable computingKEYWORDS Inference Serving Systems, Autoscaling, Machine Learning\nThe use of machine learning (ML) inference for various applications is growing drastically. ML inference services engage with users directly, requiring fast and accurate responses. Moreover, these services face dynamic workloads of requests, imposing changes in their computing resources. Failing to right-size computing resources results in either latency service level objectives (SLOs) violations or wasted computing resources. Adapting to dynamic workloads considering all the pillars of accuracy, latency, and resource cost is challenging. In response to these challenges, we propose InfAdapter, which proactively selects a set of ML model variants with their resource allocations to meet latency SLO while maximizing an objective function composed of accuracy and cost. InfAdapter decreases SLO violation and costs up to 65% and 33%, respectively, compared to a popular industry autoscaler (Kubernetes Vertical Pod Autoscaler).\n Table 1\n: InfAdapter is superior compared to the stateof-the-art solutions. ( * ) Cocktail uses model ensembling leading to cost inefficiencies in particular scenarios (see Section 6).\n\n\nFeature\n\nMS [38] INFaaS [30] Cocktail [20] VPA [9] InfAdapter\n\nCost Optimization \u2715 \u2713 \u2713 * \u2713 \u2713 Accuracy Maximization \u2713 \u2715 \u2713 \u2715 \u2713 Predictive Decision-Making \u2713 \u2715 \u2713 \u2713 \u2713 Container as a Service (CaaS) \u2715 \u2715 \u2715 \u2713 \u2713 Latency SLO-aware \u2713 \u2713 \u2713 \u2715 \u2713 machine translation, chatbots, medical, and recommender systems, are running in data centers [13,28,32,34], comprising more than 90% of computing resources allocated to ML [10,13,25]. ML inference services are user-facing, which mandates high responsiveness [18,37]. Moreover, high accuracy is crucial for these services [20,26]. Consequently, inference systems must deliver highly accurate predictions with fewer computing resources (cost-efficient) while meeting latency constraints under workload variations [18,19,29,37]. The dynamic nature of inference serving workloads requires different resource allocations for ML services [18,37]. Failing to right-size the services results in over or underresource provisioning. Under-provisioning leads to service level objective (SLO) violations (e.g., 99 \u210e percentile of latency distribution, P99-latency) [18,36]. Conversely, overprovisioning wastes computing resources [30,36]. To address these problems caused by dynamic workloads, Autoscaling [2, 9,17,18,30,36] resizes the resources of the service, and Model-switching [26,38] switches between ML model variants that differ in their inference latency and accuracy (higher accuracy, higher latency); the former tries to be cost-efficient, and the latter tries to be more accurate, while both guarantee latency SLOs.\n\nAuto-scaling and model-switching as the state-of-theart adaptation mechanisms fail to consider the accuracy and cost-efficiency simultaneously. Auto-scaling may sacrifice accuracy if it works with a low-accuracy model variant or incur high resource costs if used for a high-accuracy model variant. Conversely, model-switching can be a subject of under-provisioning in cases where even the least accurate model variant cannot respond to the workload; it also fails to be cost-efficient when the capacity of the most accurate model variant is more than the workload on the service.\n\nThe ability to jointly resize and switch ML model variants provides new opportunities. For instance, our experiments demonstrate that a Resnet50 model variant on 8 CPU cores allocation can sustain almost the same load that a Resnet152 variant does with 20 CPU cores; moreover, a Resnet18 with 8 CPU cores can process the same load as a Resnet50 with 20 cores, while meeting P99-latency (750ms). Using a set of model variants instead of a single variant provides more granular accuracy/cost trade-offs.\n\nInference systems should be adaptive in response to dynamic workloads and be able to consider all the contrasting objectives, including responsiveness (latency), accuracy, and cost-efficiency (allocated CPU cores) when dedicating resources to services and models. Moreover, reconciling these three measures is challenging as achieving one causes a violation or sacrifice of the other, and finding a trade-off among these three is daunting.\n\nIn response to these challenges, we design InfAdapter and empirically show that it can address the limitations of existing solutions. It predicts the service workload to mitigate provisioning overhead and, by using the predicted load, selects a set of ML model variants and their sizes (CPU cores) as the service backends to meet latency SLO and to maximize an objective function composed of average accuracy, resource cost, and loading cost (Section 3). To process the incoming requests, we implemented a dispatcher that load balances user requests to the backend variants according to their capacity(Section 4). Our experiments demonstrate that InfAdapter reduces average accuracy loss for latency SLO of 750 ms at 99 \u210e percentile up to 4% given the same load compared to existing solutions (Section 5).\n\nWe have prototyped InfAdapter 1 in a Kubernetes cluster and used TensorFlow Serving [27] model server to serve our ML models. We experimentally evaluate InfAdapter using a real workload trace, Twitter-trace [12] (Section 5) and compare it against existing solutions (e.g., vertical Pod autoscaler (VPA) [9], and Model-Switching [38]). Our experiments illustrate that InfAdapter reduces SLO violations by up to 65% compared to existing solutions. We further opensourced our implementation for community engagement and 1 https://github.com/reconfigurable-ml-pipeline/InfAdapter reproducibility of our experiments. Table 1 summarizes differences between InfAdapter and other existing approaches.\n\n\nMOTIVATION\n\nDue to interaction with online users, inference services are latency-sensitive [18,36], and since they contain heavy computations, they are resource-intensive [10]. Accuracy is also a pillar dimension of these services [26]. Faced with dynamic workload [18,37], it is essential to consider the ternary tradeoff space between latency, accuracy, and the resource cost dynamically to address latency requirements while gaining higher accuracy cost-efficiently.\n\nThe importance of having accurate predictions while being cost-efficient, on one hand, hinders us from selecting a computationally light model variant with a low accuracy; on the other hand, selecting the most accurate model requires a very high resource cost to fulfill latency SLOs, which may even be unavailable. We conducted experiments with Resnet model variants under different CPU core assignments and captured their sustained throughput (number of requests they could handle given 750ms P99-latency SLO). Figure 1 shows our experiment's result for Resnet18, Resnet50, and Resnet152 variants used for image classification under 8, 14, and 20 CPU core assignments. In the figure, a Resent18 with 8 CPU cores, and a Resnet50 with 20 CPU cores, can almost sustain the same throughput under the latency SLO; a similar argument is applicable to Resnet50 with 8 cores and Resnet152 with 20 cores. Due to the fact that latencyaccuracy trade-off space changes based on the workload, it is a non-trivial task to pick the right model variant with the right resource allocation.\n\nObservation 1. ML model variants provide the opportunity to reduce resource costs while adapting to the dynamic workload.\n\nUsing the traces collected from the previous experiment, we used two approaches to select backend model variant(s) to sustain a 75RPS load under a 750ms P99-latency SLO, using different CPU budgets (8,14, and 20 CPU cores). Approach 1 is to opportunistically take advantage of selecting both a set of model variants and also their sizes (InfAdapter). Approach 2 is to select only one model variant and its size (MS). For example, under a 14 CPU core budget, InfAdapter selects Resnet50, Resnet101, and Resnet152 with 2, 6, and 6 CPU cores, respectively. However, the most accurate model that MS can pick under the 14-core budget for 75RPS is Resnet50. We compared the two approaches' accuracy loss (i.e., the accuracy we obtained subtracted from the accuracy of our most accurate variant, Resnet152). In Figure 2, we observe that InfAdapter is able to gain higher average accuracy (lower accuracy loss) for the requests by having more options to select from, i.e., selecting a set of models rather than a single model.\n\nObservation 2. Using a set of model variants simultaneously can provide better average accuracy compared to having one active variant.\n\nGiven dynamic workloads, we propose an adaptive mechanism for ML inference services to achieve latency SLOaware, highly accurate, and cost-efficient inference systems. InfAdapter selects a subset of model variants to meet latency SLOs and maximizes an objective function of accuracy and cost. InfAdapter reconciles three important yet contradictory objectives (accuracy, cost, and latency).\n\n\nPROBLEM FORMULATION\n\nWe formally describe the accuracy-cost problem using ML model variants while guaranteeing the latency SLO.\n\nWe denote as the set of model variants for a specific task, with the latency SLO, , the given accuracy of model \u2208 , , and the model readiness time (loading the model into memory and the model initialization),\n\n. We profile our set of model variants under different CPU assignments to capture the number of requests they can process concerning latency SLO . Furthermore, by using the profiled data, we train a linear regression model to estimate the processing latency and throughput of model variant \u2208 under any CPU cores \u2264 , ( ), \u210e ( ), where is the total CPU budget and the total resource cost as = \u2208 . To maintain system stability during a dynamic workload, the aggregated throughput of all available models for a given task must stay above an expected (predicted) request rate . Mathematically, this can be expressed as \u2208 , \u2264 \u210e ( ) \u2265 . Moreover, we define the weighted average accuracy, based on the quota of the workload on variant , , as = \u2208 \u00b7 . Furthermore, we define the model loading cost as = max{ ( ) * , \u2208 } where the transition cost, ( ), is equal to 1 if the model variant needs to be loaded, and 0 otherwise. Table 2 summarizes the notations we use in the paper.\n\nWe define a multi-objective optimization problem to decide which subset of model variants to use such that under a given workload, the end-to-end latency is guaranteed. The goal is to maximize the weighted average accuracy and to minimize the total resource and loading costs. The problem can be formulated with the following integer linear programming (ILP):\nmax \u00b7 \u2212 ( \u00b7 + \u00b7 ) subject to \u2264 \u2211\ufe01 \u2208 \u210e ( ), \u2264 \u210e ( ) ( ) \u2264 , \u2200 \u2208 , \u2264 , \u2208 W, \u2200 \u2208 .\n(1)  Figure 3: InfAdapter structure; variants can be scheduled (by the Kubernetes scheduler) in any of the nodes.\n\nIn the objective function, we introduce , , to normalize the resource and loading costs and give importance to the objectives based on user preference. The first two constraints ensure the system's stability, e.g., there are enough resources to support the incoming workload. The third constraint satisfies the latency SLO, while the last two constraints bound the CPU core per model to be non-negative and within the available resources in the system. We use the Gurobi optimizer [21] to solve the ILP in the above equation.\n\n\nSYSTEM DESIGN\n\nAn overview of the InfAdapter architecture is demonstrated in Figure 3. The system consists of three major components (monitoring, adapter, and dispatcher). Monitoring keeps monitoring statistics about the distribution of request arrivals. Adapter is responsible for first predicting the next timeinterval workload based on the workload history gathered from the monitoring component and then finding a set of model variants, their CPU cores, and their workload quota by solving the ILP in Equation 1. Dispatcher controls distributing the requests to the set of multi variants based on the models' workload quota provided by the Adapter component.\n\nMonitoring. The monitoring demon is in charge of fetching the arrival rate from the dispatcher. We get the number of requests per second and pass it to the forecaster to predict the arrival workload for the next time interval.\n\nAdapter. The Adapter consists of two sub-components, a time-series forecaster and a solver. Time-series forecaster predicts future workload based on historical incoming workload patterns of requests on the system. The solver aims to solve the ILP in Equation 1 (every 30 seconds) to achieve the highest possible accuracy while respecting the latency SLO and available resources using the predicted workload and the current state of CPU allocation. Finally, the Adapter passes the set of models and their CPU cores to the cluster for enforcing the system configuration and the model's quota variables to the dispatcher for load balancing the incoming workload.\n\nDispatcher. The Dispatcher component load balances the incoming workload among the models in the cluster based on the weighted round-robin algorithm using the received models' quota variable, , from the solver in the adapter component.\n\n\nEXPERIMENTAL EVALUATION\n\nWe have prototyped InfAdapter in a Kubernetes cluster of two homogeneous physical machines from the Chameleon Cloud [24] equipped with 48 CPU cores of type Intel(R) Xeon(R) Gold 6126 CPU @ 2.60GHz and 192 GiB of RAM. TensorFlow Serving is used to serve the model variants in separate Docker containers. Batching and parallelism parameters. Batching and parallelism parameters are practical configuration knobs of ML inference services. Batching refers to aggregating multiple requests into one request, which is widely adapted for GPU inference systems [16,23,33,35]. However, as shown in Figure 4, inference on CPU does not substantially benefit from batching in increasing the throughput, but increasing batch size leads to higher latency. Intra-op parallelism defines the parallelism degree within an operation (such as matrix multiplication), and inter-op parallelism defines the parallelism across independent operations of inference requests [3-5].\n\nWe measured the effect of batching and CPU intra/inter operation parallelism on Resnet50 with 8 CPU cores regarding throughput and latency. Experimental results are shown in Figure 4, which captures throughput-average latency relation under different batching and parallelism configurations. We choose (starred plot) to disable batching (set to 1), set inter-op parallelism to the number of CPU cores, and disable intra-op parallelism (set to 1) in InfAdapter across all the experiments to get the best throughput with a latency within the 750ms SLO. Further, we observed the same trend for all other model variants and CPU allocations.\n\nProfiling methodology InfAdapter optimization formulation (Equation 1) needs to know the throughput \u210e ( ) of each variant under different CPU core allocation . We use a linear regression model for each ML model variant and train these regression models using the profiling data of only 5 CPU allocations (1, 2, 4, 8, and 16 cores) out of all possible allocations to avoid extra profiling costs. The regression models are then used to predict the throughput of the variants under any CPU allocation. Figure 6 shows the prediction accuracy of regression models for Resnet18 and Resnet50. As observed in the experiments the predicted RPS can be accurately predicted using the regression model. For instance, the R-squared ( 2 ) for the regression models of Resnet18 and Resnet50 are 0.996 and 0.994 respectively. Load forecaster We used LSTM [22] for time-series forecasting. Our LSTM model takes as input the load per second of the past 10 minutes collected from the monitoring component, and predicts the maximum workload for the next minute; we used the first two weeks of Twitter-trace dataset [12] to train the LSTM model. The LSTM neural network comprises a 25-unit LSTM layer followed by a one-unit dense layer for the output layer. We used Adam optimizer with MSE loss function for training the network. Figure 5, the top plot, shows the prediction accuracy of the LSTM on a sample from the Twitter trace. InfAdapter handles bursty and non-bursty workloads. First, we experiment with bursty workloads to understand the performance of InfAdapter. We compared InfAdapter against an extended version of Kubernetes built-in Vertical Pod Autoscaler (VPA) [9] and an enhanced version of Model-Switching [38] (MS+). As the performance of the built-in VPA was very poor in the empirical evaluations, we made the following changes to it for a fair comparison against our approach. Initially, at each recommendation timestep, the built-in VPA removes the old container and then creates a new container with predicted resource allocations; this results in a service downtime during the recreation episode; to prevent this, we first create the container with the VPA recommended resources, and after it is up and running, remove the previous version. Secondly, we dropped the consideration of resource lower bound in VPA to scale up faster in response to the dynamic workload. For more information on the VPA algorithm, refer to [31]. Also, in MS+, since Model-Switching performs on a fixed resource budget, we add predictive allocation. At each time step, a model variant and its resource allocation are selected based on the same objective function we use for InfAdapter in Equation 1.\n\nWe evaluated the results on a 20-minute sample of Twittertrace ( Figure 5 top) that contains a steady load (0-600s), a load spike (600s-800s), a gradual decrease in the load (800s-100s) and a sample of going back to the initial load (1000s-1200s). Almost all the compared methods can stay under the 750 ms SLO under a steady load. Once there is a load spike at 600s, almost all the compared methods suffer from SLO violations with a non-negligible margin (E.g., we observed a 10-minute violation for Resnet152). However, InfAdapter and MS+ temporarily trade-off a little accuracy in favor of being responsive to the load spike with a short SLO violation time. Between MS+ and InfAdapter, InfAdapter can achieve the same SLO attainment with less accuracy loss during the load burst.\n\nInfAdapter aims to provide a tradeoff between accuracy and cost objectives. Under = 0.05, we observed that In-fAdapter could balance the cost and accuracy objectives and comply with latency SLO. The same trend can be identified from the cumulative result of the entire experiment in Figure 7. The InfAdapter can always balance the cost and accuracy objectives better than MS+. Also, VPA variants mostly took an extreme in maximizing only one objective; e.g., VPA-18 is the most cost-effective, but it comes at the expense of being very inefficient in accuracy.\n\nSimilarly, we used a non-bursty workload ( Figure 8). We observed that InfAdapter has less accuracy loss than all other methods (except VPA+ with Resnet152, which has zero accuracy loss at the expense of high cost and SLO violations). Although in most cases InfAdapter has better SLO compliance, the difference between MS+ and InfAdapter in terms of cost and accuracy is small. We found that the difference was higher for a synthesized workload. In future work, we aim to evaluate InfAdapter the cost and accuracy trade-off with respect to SLO guarantees with different workloads.\n\nRefer to the appendix for experiments with = 0.0125 and = 0.2. We observed that lower values of InfAdapter prioritize accuracy over resource cost, and more significant values of do the opposite.\n\n\nRELATED WORK\n\nConfiguration of machine learning inference systems has gained considerable attraction in recent years. Clipper [16] is one of the early inference serving systems that introduced a general-purpose inference server with functionalities like caching, batching, and adaptive model selection.\n\nMArk [36] employs request batching, predictive scaling, and serverless functions and proposes autoscaling policies that also take the hardware heterogeneity and service type diversity (FaaS, CaaS, IaaS) of inference serving data-centers into consideration.\n\nINFaaS [30] provides an abstraction layer that decouples the model serving task from the used model for serving. Per each inference request, it searches through all the available sets of models for that specific inference task. Based on the request requirement, it finds the suitable model variant and dynamically offloads and unloads models as the user requirements change. Model switching [38] is the first work that proposes switching between lightweight and heavier models as a workload adaptation mechanism. To be responsive to workload surges, it switches to a smaller but less accurate model. Unlike In-fAdapter, their model is not cost-aware and can only work under a fixed resource budget.\n\nCocktail [20] is the most similar work to InfAdapter. It proposes an approach based on ensemble learning to reduce the cost while meeting the previous works' latency and accuracy efficiency. Cocktail uses ensembling as its accuracy maximization technique, which is costly as all the requests should be sent to all the ML models. Most of the time, many model sets should be used to get to the accuracy of the largest model. Cocktail uses transient virtual machines to improve cost efficiency. Nevertheless, using unstable transient instances can cause interruptions in the inference service. Deploying In-fAdapter on CaaS platforms like Google Autopilot does not suffer from similar problems. Due to fundamental structural differences and different problem formulations, we could not compare InfAdapter with Cocktail.\n\n\nFUTURE WORKS\n\nHardware Heterogeneity. While in this work, we focused on homogeneous CPU inferencing, the performance of In-fAdapter under general purposed (GPUs) and ASIC ML hardware can be evaluated. With packing requests into batches, GPUs can process higher workloads without a considerable increase in latency. Scalability with ML. Our proposed solution to the ILP problem, works by brute-forcing through all possible configurations and picking the one that maximizes the objective function. Such an approach could suffer from scalability in case of growth in configuration space (more model variants e.g. scalability under one-for-all-networks [14] \n\n\nCONCLUSION\n\nIn this work, we presented InfAdapter for ML inference services. It selects a set of ML model variants and their resource allocations to achieve a trade-off between accuracy and cost while preserving latency SLO guarantee. Experiments on real-world traces showed that InfAdapter adapts better to dynamic workloads compared to the existing solutions by utilizing scaling and ML model variants selection.\n\n\nAPPENDIX\n\nintroduced in Equation 1 is the parameter responsible for increasing the effect of cost optimization in the optimization formulation. One of the main goals of InfAdapter is to provide a tunable framework for achieving a trade-off between accuracy and cost. The relation between the values of and is central to the behavior of InfAdapter. Larger / ratios prioritize cost efficiency over accuracy optimization, and smaller values will result in the opposite. Figure 9 shows the case that InfAdapter prioritize cost over cost optimization with the value = 0.2 while in Figure 10 a smaller value of = 0.0125 results in achieving higher accuracy at the expense of sacrificing more cost. \n\nFigure 1 :\n1Throughput of three Resnet variants under 8, 14, and 20 CPU cores. We ensured the latency of all configurations is lower than 750 ms at the P99-latency at the saturation load.\n\nFigure 2 :\n2Comparison of InfAdapter (the ability to use a set of alternative models in the back-end) and Model-Switching+ on accuracy loss (accuracy of the most accurate model, Resnet152, subtracted by the accuracy of each bar). Each bar sustains the SLO of 750ms at P99latency for a 75RPS load.\n\nFigure 4 :\n4Throughput-Average latency for batch sizes of 1 (batching disabled) and 8 with different parallelism configurations on Resnet50 with 8 CPU cores allocations. The starred configuration is the chosen configuration through our experiments.\n\nFigure 5 :\n5Comparison of InfAdapter with VPA used along with Resnet18, Resnet50 and Resnet152 on accuracy loss, cost and P99-latency, during the experiment, = 0.05.\n\nFigure 6 :\n6Profiled values and predicted values of throughput of Resnet18 and Resnet50 models under different CPU allocations.\n\nFigure 7 :\n7Comparison of InfAdapter with VPA used along with Resnet18, Resnet50 and Resnet152 on accuracy loss, cost and 99th percentile latency, for the whole experiment, under different values.\n\nFigure 8 :\n8Comparison of InfAdapter with VPA used along with Resnet18, Resnet50 and Resnet152 on accuracy loss, cost and P99-latency, during the experiment with a non-bursty workload, = 0.05.\n\nFigure 9 :Figure 10 :\n910Comparison of InfAdapter with VPA used along with Resnet18, Resnet50 and Resnet152 on accuracy loss, cost and P99-latency, during the experiment with a non-bursty workload, Comparison of InfAdapter with VPA used along with Resnet18, Resnet50 and Resnet152 on accuracy loss, cost and P99-latency, during the experiment with a non-bursty workload, = 0.0125.\n\nTable 2 :\n2NotationsSymbol \nDescription \n\nSet of all model variants for a given task \nLatency SLO \nAn ML model variant from set \nAccuracy of variant \nReadiness time of variant \nTransition cost of variant \nNumber of CPU cores for variant \n( ) \nProcessing latency of variant with \nCPU cores \n\u210e ( ) \nThroughput of variant with \nCPU cores \nAverage Accuracy \nResource cost \nLoading cost \nCPU budget \nWorkload on the system \nWorkload quota on variant \n\n\n\n\nwith 10 15 possible variants and bigger resource budgets in our case in a larger experimental setting). Utilizing ML-based solutions can decrease the amount of sampling in the search space, resulting in faster decision-making. Multi Model Serving. In the case of using accelerators like GPUs, it is hard to share them among several containers, as there is no built-in mechanism for GPU sharing in container orchestration platforms like Kubernetes[1]. The multi-model deployment pattern, adapted in most production ML model servers[6-8, 15, 27], can mitigate the issues. Considering these emerging ML serving paradigms for improving adaptation mechanisms is a potential future work.\nACKNOWLEDGEMENTS\nGPU virtualization in K8S: challenges and state of the art. GPU virtualization in K8S: challenges and state of the art. https://www.arrikto.com/blog/ gpu-virtualization-in-k8s-challenges-and-state-of-the-art/. (Nov 2022).\n\nTriton inference server. ] 2023. Triton inference server. https://github.com/ triton-inference-server/server. (2023).\n\nVertical Pod autoscaling. 2023. Vertical Pod autoscaling. https://github.com/kubernetes/ autoscaler/tree/master/vertical-pod-autoscaler. (2023).\n\nArnaud Van Looveren, and Clive Cox. 2022. Desiderata for next generation of ML model serving. Andrei Sherif Akoush, Paleyes, arXiv:2210.14665arXiv preprintSherif Akoush, Andrei Paleyes, Arnaud Van Looveren, and Clive Cox. 2022. Desiderata for next generation of ML model serving. arXiv preprint arXiv:2210.14665 (2022).\n\nAI and compute. Dario Amodei, Danny Hernandez, Dario Amodei and Danny Hernandez. 2019. AI and compute. https: //openai.com/blog/ai-and-compute/. (Nov 2019).\n\nArchiveteam-twitter-stream-2021-08. archiveteam. 2021archiveteam. 2021. Archiveteam-twitter-stream-2021-08. https:// archive.org/details/archiveteam-twitter-stream-2021-08. (2021).\n\nAmazon EC2 ML inference. Jeff Bar, Jeff Bar. 2019. Amazon EC2 ML inference. https://tinyurl.com/ 5n8yb5ub. (Dec 2019).\n\nOnce-for-all: train one network and specialize it for efficient deployment. Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, Song Han, arXiv:1908.09791arXiv preprintHan Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. 2019. Once-for-all: train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791 (2019).\n\nPyTorch Serve Contributors. 2020. Torch serve. PyTorch Serve Contributors. 2020. Torch serve. https://pytorch.org/ serve/. (2020).\n\nClipper: a low-latency online prediction serving system. Daniel Crankshaw, Xin Wang, Guilio Zhou, J Michael, Joseph E Franklin, Ion Gonzalez, Stoica, 14th {USENIX} Symposium on Networked Systems Design and Implementation. Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, and Ion Stoica. 2017. Clipper: a low-latency online prediction serving system. In 14th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 17). 613-627.\n\nAutoscale: dynamic, robust capacity management for multi-tier data centers. Anshul Gandhi, Mor Harchol-Balter, Ram Raghunathan, Michael A Kozuch, ACM Transactions on Computer Systems (TOCS). 30Anshul Gandhi, Mor Harchol-Balter, Ram Raghunathan, and Michael A Kozuch. 2012. Autoscale: dynamic, robust capacity management for multi-tier data centers. ACM Transactions on Computer Systems (TOCS) 30, 4 (2012), 1-26.\n\nSwayam: distributed autoscaling to meet SLAs of machine learning inference services with resource efficiency. Arpan Gujarati, Sameh Elnikety, Yuxiong He, S Kathryn, Mckinley, B Bj\u00f6rn, Brandenburg, Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference. the 18th ACM/IFIP/USENIX Middleware ConferenceArpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S McKinley, and Bj\u00f6rn B Brandenburg. 2017. Swayam: distributed autoscaling to meet SLAs of machine learning inference services with resource efficiency. In Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference. 109-120.\n\nServing DNNs like clockwork: performance predictability from the bottom up. Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann, Ymir Vigfusson, Jonathan Mace, arXiv:2006.02464arXiv preprintArpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kauf- mann, Ymir Vigfusson, and Jonathan Mace. 2020. Serving DNNs like clockwork: performance predictability from the bottom up. arXiv preprint arXiv:2006.02464 (2020).\n\nCocktail: a multidimensional optimization for model serving in cloud. Cyan Jashwant Raj Gunasekaran, Prashanth Subhra Mishra, Bikash Thinakaran, Sharma, Chita R Mahmut Taylan Kandemir, Das, USENIX NSDI. Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thi- nakaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R Das. 2022. Cocktail: a multidimensional optimization for model serving in cloud. In USENIX NSDI. 1041-1057.\n\nGurobi optimizer reference manual. Llc Gurobi Optimization, Gurobi Optimization, LLC. 2023. Gurobi optimizer reference manual. (2023). https://www.gurobi.com\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 9Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735-1780.\n\nScrooge: a cost-effective deep learning inference system. Yitao Hu, Rajrup Ghosh, Ramesh Govindan, Proceedings of the ACM Symposium on Cloud Computing. the ACM Symposium on Cloud ComputingYitao Hu, Rajrup Ghosh, and Ramesh Govindan. 2021. Scrooge: a cost-effective deep learning inference system. In Proceedings of the ACM Symposium on Cloud Computing. 624-638.\n\nKate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi S Gunawi, Cody Hammock, Joe Mambretti, Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC '20). USENIX Association. the 2020 USENIX Annual Technical Conference (USENIX ATC '20). USENIX AssociationAlexander Barnes, Fran\u00e7ois Halbach, Alex Rochaand Joe Stubbs. 2020. Lessons learned from the Chameleon testbedKate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi S. Gunawi, Cody Hammock, Joe Mambretti, Alexander Barnes, Fran\u00e7ois Halbach, Alex Rocha, and Joe Stubbs. 2020. Lessons learned from the Chameleon testbed. In Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC '20). USENIX Association.\n\nAWS to offer Nvidia's T4 GPUs for AI inferencing. George Leopold, George Leopold. 2019. AWS to offer Nvidia's T4 GPUs for AI inferencing. https://www.hpcwire.com/2019/03/19/ aws-upgrades-its-gpu-backed-ai-inference-platform/. (Mar 2019).\n\nJellyfish: timely inference serving for dynamic edge networks. Vinod Nigade, Pablo Bauszat, Henri Bal, Lin Wang, 2022 IEEE Real-Time Systems Symposium (RTSS). IEEE. Vinod Nigade, Pablo Bauszat, Henri Bal, and Lin Wang. 2022. Jellyfish: timely inference serving for dynamic edge networks. In 2022 IEEE Real-Time Systems Symposium (RTSS). IEEE, 277-290.\n\nTensorflow-Serving: flexible, high-performance ML serving. Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, Jordan Soyke, arXiv:1712.06139arXiv preprintChristopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke. 2017. Tensorflow-Serving: flexible, high-performance ML serving. arXiv preprint arXiv:1712.06139 (2017).\n\nDeep learning inference in Facebook data centers: characterization, performance optimizations and hardware implications. Jongsoo Park, Maxim Naumov, Protonu Basu, Summer Deng, Aravind Kalaiah, Daya Khudia, James Law, Parth Malani, Andrey Malevich, Satish Nadathur, arXiv:1811.09886arXiv preprintJongsoo Park, Maxim Naumov, Protonu Basu, Summer Deng, Aravind Kalaiah, Daya Khudia, James Law, Parth Malani, Andrey Malevich, Satish Nadathur, et al. 2018. Deep learning inference in Facebook data centers: characterization, performance optimizations and hardware implications. arXiv preprint arXiv:1811.09886 (2018).\n\nFA2: fast, accurate autoscaling for serving deep learning inference with SLA guarantees. Kamran Razavi, Manisha Luthra, Boris Koldehofe, Max M\u00fchlh\u00e4user, Lin Wang, 10.1109/RTAS54340.2022.000202022 IEEE 28th Real-Time and Embedded Technology and Applications Symposium (RTAS). Kamran Razavi, Manisha Luthra, Boris Koldehofe, Max M\u00fchlh\u00e4user, and Lin Wang. 2022. FA2: fast, accurate autoscaling for serving deep learning inference with SLA guarantees. In 2022 IEEE 28th Real-Time and Embedded Technology and Applications Symposium (RTAS). 146- 159. https://doi.org/10.1109/RTAS54340.2022.00020\n\n2021. {INFaaS}: automated model-less inference serving. Francisco Romero, Qian Li, J Neeraja, Christos Yadwadkar, Kozyrakis, 2021 USENIX Annual Technical Conference (USENIX ATC 21. Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos Kozyrakis. 2021. {INFaaS}: automated model-less inference serving. In 2021 USENIX Annual Technical Conference (USENIX ATC 21). 397-411.\n\nAutopilot: workload autoscaling at Google. Krzysztof Rzadca, Pawel Findeisen, Jacek Swiderski, Przemyslaw Zych, Przemyslaw Broniek, Jarek Kusmierek, Pawel Nowak, Beata Strack, Piotr Witusowski, Steven Hand, Proceedings of the Fifteenth European Conference on Computer Systems. the Fifteenth European Conference on Computer SystemsKrzysztof Rzadca, Pawel Findeisen, Jacek Swiderski, Przemyslaw Zych, Przemyslaw Broniek, Jarek Kusmierek, Pawel Nowak, Beata Strack, Piotr Witusowski, Steven Hand, et al. 2020. Autopilot: workload au- toscaling at Google. In Proceedings of the Fifteenth European Conference on Computer Systems. 1-16.\n\nDeep learning in image classification using residual network (ResNet) variants for detection of colorectal cancer. Devvi Sarwinda, Radifa Hilya Paradisa, Alhadi Bustamam, Pinkie Anggia, Procedia Computer Science. 179Devvi Sarwinda, Radifa Hilya Paradisa, Alhadi Bustamam, and Pinkie Anggia. 2021. Deep learning in image classification using residual network (ResNet) variants for detection of colorectal cancer. Procedia Computer Science 179 (2021), 423-431.\n\nNexus: a GPU cluster engine for accelerating DNN-based video analysis. Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, Ravi Sundaram, Proceedings of the 27th ACM Symposium on Operating Systems Principles. the 27th ACM Symposium on Operating Systems PrinciplesHaichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019. Nexus: a GPU cluster engine for accelerating DNN-based video analy- sis. In Proceedings of the 27th ACM Symposium on Operating Systems Principles. 322-337.\n\nSemantic lattice processing in contextual automatic speech recognition for Google assistant. Leonid Velikovich, Ian Williams, Justin Scheiner, S Petar, Pedro J Aleksic, Michael Moreno, Riley, Leonid Velikovich, Ian Williams, Justin Scheiner, Petar S Aleksic, Pe- dro J Moreno, and Michael Riley. 2018. Semantic lattice processing in contextual automatic speech recognition for Google assistant. In Interspeech. 2222-2226.\n\nMorphling: fast, near-optimal auto-configuration for cloud-native model serving. Luping Wang, Lingyun Yang, Yinghao Yu, Wei Wang, Bo Li, Xianchao Sun, Jian He, Liping Zhang, Proceedings of the ACM Symposium on Cloud Computing. the ACM Symposium on Cloud ComputingLuping Wang, Lingyun Yang, Yinghao Yu, Wei Wang, Bo Li, Xianchao Sun, Jian He, and Liping Zhang. 2021. Morphling: fast, near-optimal auto-configuration for cloud-native model serving. In Proceedings of the ACM Symposium on Cloud Computing. 639-653.\n\nMArk: exploiting cloud services for cost-effective, SLO-aware machine learning inference serving. Chengliang Zhang, Minchen Yu, Wei Wang, Feng Yan, 2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19. Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. 2019. MArk: exploiting cloud services for cost-effective, SLO-aware machine learn- ing inference serving. In 2019 {USENIX} Annual Technical Conference ({USENIX} {ATC} 19). 1049-1062.\n\nLive video analytics at scale with approximation and {delay-tolerance}. Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Philipose, Paramvir Bahl, Michael J Freedman, 14th USENIX Symposium on Networked Systems Design and Implementation. Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Phili- pose, Paramvir Bahl, and Michael J Freedman. 2017. Live video analyt- ics at scale with approximation and {delay-tolerance}. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). 377-392.\n\nModel-switching: dealing with fluctuating workloads in machine-learning-as-a-service systems. Jeff Zhang, Sameh Elnikety, Shuayb Zarar, Atul Gupta, Siddharth Garg, 12th {USENIX} Workshop on Hot Topics in Cloud Computing. Jeff Zhang, Sameh Elnikety, Shuayb Zarar, Atul Gupta, and Siddharth Garg. 2020. Model-switching: dealing with fluctuating workloads in machine-learning-as-a-service systems. In 12th {USENIX} Workshop on Hot Topics in Cloud Computing (HotCloud 20).\n", "annotations": {"author": "[{\"end\":253,\"start\":92},{\"end\":415,\"start\":254},{\"end\":577,\"start\":416},{\"end\":738,\"start\":578},{\"end\":900,\"start\":739},{\"end\":1060,\"start\":901},{\"end\":1223,\"start\":1061},{\"end\":1385,\"start\":1224},{\"end\":253,\"start\":92},{\"end\":415,\"start\":254},{\"end\":577,\"start\":416},{\"end\":738,\"start\":578},{\"end\":900,\"start\":739},{\"end\":1060,\"start\":901},{\"end\":1223,\"start\":1061},{\"end\":1385,\"start\":1224}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":99},{\"end\":268,\"start\":260},{\"end\":430,\"start\":424},{\"end\":591,\"start\":585},{\"end\":753,\"start\":743},{\"end\":913,\"start\":908},{\"end\":1076,\"start\":1068},{\"end\":1238,\"start\":1231},{\"end\":106,\"start\":99},{\"end\":268,\"start\":260},{\"end\":430,\"start\":424},{\"end\":591,\"start\":585},{\"end\":753,\"start\":743},{\"end\":913,\"start\":908},{\"end\":1076,\"start\":1068},{\"end\":1238,\"start\":1231}]", "author_first_name": "[{\"end\":98,\"start\":92},{\"end\":259,\"start\":254},{\"end\":423,\"start\":416},{\"end\":584,\"start\":578},{\"end\":742,\"start\":739},{\"end\":907,\"start\":901},{\"end\":1067,\"start\":1061},{\"end\":1230,\"start\":1224},{\"end\":98,\"start\":92},{\"end\":259,\"start\":254},{\"end\":423,\"start\":416},{\"end\":584,\"start\":578},{\"end\":742,\"start\":739},{\"end\":907,\"start\":901},{\"end\":1067,\"start\":1061},{\"end\":1230,\"start\":1224}]", "author_affiliation": "[{\"end\":252,\"start\":108},{\"end\":414,\"start\":270},{\"end\":576,\"start\":432},{\"end\":737,\"start\":593},{\"end\":899,\"start\":755},{\"end\":1059,\"start\":915},{\"end\":1222,\"start\":1078},{\"end\":1384,\"start\":1240},{\"end\":252,\"start\":108},{\"end\":414,\"start\":270},{\"end\":576,\"start\":432},{\"end\":737,\"start\":593},{\"end\":899,\"start\":755},{\"end\":1059,\"start\":915},{\"end\":1222,\"start\":1078},{\"end\":1384,\"start\":1240}]", "title": "[{\"end\":89,\"start\":1},{\"end\":1474,\"start\":1386},{\"end\":89,\"start\":1},{\"end\":1474,\"start\":1386}]", "venue": null, "abstract": "[{\"end\":2585,\"start\":1652},{\"end\":2585,\"start\":1652}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2790,\"start\":2786},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2802,\"start\":2798},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2816,\"start\":2812},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2824,\"start\":2821},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3101,\"start\":3097},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3104,\"start\":3101},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3107,\"start\":3104},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3110,\"start\":3107},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3180,\"start\":3176},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3183,\"start\":3180},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3186,\"start\":3183},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3266,\"start\":3262},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3269,\"start\":3266},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3329,\"start\":3325},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3332,\"start\":3329},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3519,\"start\":3515},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3522,\"start\":3519},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3525,\"start\":3522},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3528,\"start\":3525},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3640,\"start\":3636},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3643,\"start\":3640},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3861,\"start\":3857},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3864,\"start\":3861},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3926,\"start\":3922},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3929,\"start\":3926},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4004,\"start\":4002},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4007,\"start\":4004},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4010,\"start\":4007},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4013,\"start\":4010},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4016,\"start\":4013},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4079,\"start\":4075},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4082,\"start\":4079},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6742,\"start\":6738},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6865,\"start\":6861},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6960,\"start\":6957},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6986,\"start\":6982},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7444,\"start\":7440},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7447,\"start\":7444},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7524,\"start\":7520},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7584,\"start\":7580},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7618,\"start\":7614},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7621,\"start\":7618},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9220,\"start\":9217},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9222,\"start\":9220},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12916,\"start\":12912},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14895,\"start\":14891},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15332,\"start\":15328},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15335,\"start\":15332},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15338,\"start\":15335},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15341,\"start\":15338},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17212,\"start\":17208},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17468,\"start\":17464},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18027,\"start\":18024},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18075,\"start\":18071},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18795,\"start\":18791},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21305,\"start\":21301},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21488,\"start\":21484},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21748,\"start\":21744},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22132,\"start\":22128},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22450,\"start\":22446},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23909,\"start\":23905},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2790,\"start\":2786},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2802,\"start\":2798},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2816,\"start\":2812},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2824,\"start\":2821},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3101,\"start\":3097},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3104,\"start\":3101},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3107,\"start\":3104},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3110,\"start\":3107},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3180,\"start\":3176},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3183,\"start\":3180},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3186,\"start\":3183},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3266,\"start\":3262},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3269,\"start\":3266},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3329,\"start\":3325},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3332,\"start\":3329},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3519,\"start\":3515},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3522,\"start\":3519},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3525,\"start\":3522},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3528,\"start\":3525},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3640,\"start\":3636},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3643,\"start\":3640},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3861,\"start\":3857},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3864,\"start\":3861},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3926,\"start\":3922},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3929,\"start\":3926},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4004,\"start\":4002},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4007,\"start\":4004},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4010,\"start\":4007},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4013,\"start\":4010},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4016,\"start\":4013},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4079,\"start\":4075},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4082,\"start\":4079},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6742,\"start\":6738},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6865,\"start\":6861},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6960,\"start\":6957},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6986,\"start\":6982},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7444,\"start\":7440},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7447,\"start\":7444},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7524,\"start\":7520},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7584,\"start\":7580},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7618,\"start\":7614},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7621,\"start\":7618},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9220,\"start\":9217},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9222,\"start\":9220},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12916,\"start\":12912},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14895,\"start\":14891},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15332,\"start\":15328},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15335,\"start\":15332},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15338,\"start\":15335},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15341,\"start\":15338},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17212,\"start\":17208},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17468,\"start\":17464},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18027,\"start\":18024},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18075,\"start\":18071},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18795,\"start\":18791},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21305,\"start\":21301},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21488,\"start\":21484},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21748,\"start\":21744},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22132,\"start\":22128},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22450,\"start\":22446},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23909,\"start\":23905}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25211,\"start\":25023},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25509,\"start\":25212},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25759,\"start\":25510},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25926,\"start\":25760},{\"attributes\":{\"id\":\"fig_4\"},\"end\":26055,\"start\":25927},{\"attributes\":{\"id\":\"fig_5\"},\"end\":26253,\"start\":26056},{\"attributes\":{\"id\":\"fig_6\"},\"end\":26447,\"start\":26254},{\"attributes\":{\"id\":\"fig_7\"},\"end\":26829,\"start\":26448},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27278,\"start\":26830},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27962,\"start\":27279},{\"attributes\":{\"id\":\"fig_0\"},\"end\":25211,\"start\":25023},{\"attributes\":{\"id\":\"fig_1\"},\"end\":25509,\"start\":25212},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25759,\"start\":25510},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25926,\"start\":25760},{\"attributes\":{\"id\":\"fig_4\"},\"end\":26055,\"start\":25927},{\"attributes\":{\"id\":\"fig_5\"},\"end\":26253,\"start\":26056},{\"attributes\":{\"id\":\"fig_6\"},\"end\":26447,\"start\":26254},{\"attributes\":{\"id\":\"fig_7\"},\"end\":26829,\"start\":26448},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27278,\"start\":26830},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27962,\"start\":27279}]", "paragraph": "[{\"end\":2771,\"start\":2595},{\"end\":2835,\"start\":2783},{\"end\":4320,\"start\":2837},{\"end\":4901,\"start\":4322},{\"end\":5404,\"start\":4903},{\"end\":5845,\"start\":5406},{\"end\":6652,\"start\":5847},{\"end\":7346,\"start\":6654},{\"end\":7818,\"start\":7361},{\"end\":8894,\"start\":7820},{\"end\":9017,\"start\":8896},{\"end\":10037,\"start\":9019},{\"end\":10173,\"start\":10039},{\"end\":10565,\"start\":10175},{\"end\":10695,\"start\":10589},{\"end\":10905,\"start\":10697},{\"end\":11874,\"start\":10907},{\"end\":12235,\"start\":11876},{\"end\":12429,\"start\":12316},{\"end\":12956,\"start\":12431},{\"end\":13621,\"start\":12974},{\"end\":13849,\"start\":13623},{\"end\":14510,\"start\":13851},{\"end\":14747,\"start\":14512},{\"end\":15729,\"start\":14775},{\"end\":16367,\"start\":15731},{\"end\":19049,\"start\":16369},{\"end\":19832,\"start\":19051},{\"end\":20394,\"start\":19834},{\"end\":20976,\"start\":20396},{\"end\":21172,\"start\":20978},{\"end\":21477,\"start\":21189},{\"end\":21735,\"start\":21479},{\"end\":22435,\"start\":21737},{\"end\":23253,\"start\":22437},{\"end\":23910,\"start\":23270},{\"end\":24327,\"start\":23925},{\"end\":25022,\"start\":24340},{\"end\":2771,\"start\":2595},{\"end\":2835,\"start\":2783},{\"end\":4320,\"start\":2837},{\"end\":4901,\"start\":4322},{\"end\":5404,\"start\":4903},{\"end\":5845,\"start\":5406},{\"end\":6652,\"start\":5847},{\"end\":7346,\"start\":6654},{\"end\":7818,\"start\":7361},{\"end\":8894,\"start\":7820},{\"end\":9017,\"start\":8896},{\"end\":10037,\"start\":9019},{\"end\":10173,\"start\":10039},{\"end\":10565,\"start\":10175},{\"end\":10695,\"start\":10589},{\"end\":10905,\"start\":10697},{\"end\":11874,\"start\":10907},{\"end\":12235,\"start\":11876},{\"end\":12429,\"start\":12316},{\"end\":12956,\"start\":12431},{\"end\":13621,\"start\":12974},{\"end\":13849,\"start\":13623},{\"end\":14510,\"start\":13851},{\"end\":14747,\"start\":14512},{\"end\":15729,\"start\":14775},{\"end\":16367,\"start\":15731},{\"end\":19049,\"start\":16369},{\"end\":19832,\"start\":19051},{\"end\":20394,\"start\":19834},{\"end\":20976,\"start\":20396},{\"end\":21172,\"start\":20978},{\"end\":21477,\"start\":21189},{\"end\":21735,\"start\":21479},{\"end\":22435,\"start\":21737},{\"end\":23253,\"start\":22437},{\"end\":23910,\"start\":23270},{\"end\":24327,\"start\":23925},{\"end\":25022,\"start\":24340}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12315,\"start\":12236},{\"attributes\":{\"id\":\"formula_0\"},\"end\":12315,\"start\":12236}]", "table_ref": "[{\"end\":2594,\"start\":2587},{\"end\":7273,\"start\":7266},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11828,\"start\":11821},{\"end\":2594,\"start\":2587},{\"end\":7273,\"start\":7266},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11828,\"start\":11821}]", "section_header": "[{\"end\":2781,\"start\":2774},{\"attributes\":{\"n\":\"2\"},\"end\":7359,\"start\":7349},{\"attributes\":{\"n\":\"3\"},\"end\":10587,\"start\":10568},{\"attributes\":{\"n\":\"4\"},\"end\":12972,\"start\":12959},{\"attributes\":{\"n\":\"5\"},\"end\":14773,\"start\":14750},{\"attributes\":{\"n\":\"6\"},\"end\":21187,\"start\":21175},{\"attributes\":{\"n\":\"7\"},\"end\":23268,\"start\":23256},{\"attributes\":{\"n\":\"8\"},\"end\":23923,\"start\":23913},{\"attributes\":{\"n\":\"9\"},\"end\":24338,\"start\":24330},{\"end\":25034,\"start\":25024},{\"end\":25223,\"start\":25213},{\"end\":25521,\"start\":25511},{\"end\":25771,\"start\":25761},{\"end\":25938,\"start\":25928},{\"end\":26067,\"start\":26057},{\"end\":26265,\"start\":26255},{\"end\":26470,\"start\":26449},{\"end\":26840,\"start\":26831},{\"end\":2781,\"start\":2774},{\"attributes\":{\"n\":\"2\"},\"end\":7359,\"start\":7349},{\"attributes\":{\"n\":\"3\"},\"end\":10587,\"start\":10568},{\"attributes\":{\"n\":\"4\"},\"end\":12972,\"start\":12959},{\"attributes\":{\"n\":\"5\"},\"end\":14773,\"start\":14750},{\"attributes\":{\"n\":\"6\"},\"end\":21187,\"start\":21175},{\"attributes\":{\"n\":\"7\"},\"end\":23268,\"start\":23256},{\"attributes\":{\"n\":\"8\"},\"end\":23923,\"start\":23913},{\"attributes\":{\"n\":\"9\"},\"end\":24338,\"start\":24330},{\"end\":25034,\"start\":25024},{\"end\":25223,\"start\":25213},{\"end\":25521,\"start\":25511},{\"end\":25771,\"start\":25761},{\"end\":25938,\"start\":25928},{\"end\":26067,\"start\":26057},{\"end\":26265,\"start\":26255},{\"end\":26470,\"start\":26449},{\"end\":26840,\"start\":26831}]", "table": "[{\"end\":27278,\"start\":26851},{\"end\":27278,\"start\":26851}]", "figure_caption": "[{\"end\":25211,\"start\":25036},{\"end\":25509,\"start\":25225},{\"end\":25759,\"start\":25523},{\"end\":25926,\"start\":25773},{\"end\":26055,\"start\":25940},{\"end\":26253,\"start\":26069},{\"end\":26447,\"start\":26267},{\"end\":26829,\"start\":26474},{\"end\":26851,\"start\":26842},{\"end\":27962,\"start\":27281},{\"end\":25211,\"start\":25036},{\"end\":25509,\"start\":25225},{\"end\":25759,\"start\":25523},{\"end\":25926,\"start\":25773},{\"end\":26055,\"start\":25940},{\"end\":26253,\"start\":26069},{\"end\":26447,\"start\":26267},{\"end\":26829,\"start\":26474},{\"end\":26851,\"start\":26842},{\"end\":27962,\"start\":27281}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8341,\"start\":8333},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9831,\"start\":9823},{\"end\":12329,\"start\":12321},{\"end\":13044,\"start\":13036},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15372,\"start\":15364},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15913,\"start\":15905},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16876,\"start\":16868},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17686,\"start\":17678},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19124,\"start\":19116},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20125,\"start\":20117},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20447,\"start\":20439},{\"end\":24805,\"start\":24797},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24915,\"start\":24906},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8341,\"start\":8333},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9831,\"start\":9823},{\"end\":12329,\"start\":12321},{\"end\":13044,\"start\":13036},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15372,\"start\":15364},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15913,\"start\":15905},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16876,\"start\":16868},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17686,\"start\":17678},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19124,\"start\":19116},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20125,\"start\":20117},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20447,\"start\":20439},{\"end\":24805,\"start\":24797},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24915,\"start\":24906}]", "bib_author_first_name": "[{\"end\":28568,\"start\":28562},{\"end\":28810,\"start\":28805},{\"end\":28824,\"start\":28819},{\"end\":29158,\"start\":29154},{\"end\":29328,\"start\":29325},{\"end\":29340,\"start\":29334},{\"end\":29353,\"start\":29346},{\"end\":29366,\"start\":29360},{\"end\":29378,\"start\":29374},{\"end\":29795,\"start\":29789},{\"end\":29810,\"start\":29807},{\"end\":29823,\"start\":29817},{\"end\":29831,\"start\":29830},{\"end\":29847,\"start\":29841},{\"end\":29849,\"start\":29848},{\"end\":29863,\"start\":29860},{\"end\":30292,\"start\":30286},{\"end\":30304,\"start\":30301},{\"end\":30324,\"start\":30321},{\"end\":30347,\"start\":30338},{\"end\":30739,\"start\":30734},{\"end\":30755,\"start\":30750},{\"end\":30773,\"start\":30766},{\"end\":30779,\"start\":30778},{\"end\":30800,\"start\":30799},{\"end\":31292,\"start\":31287},{\"end\":31307,\"start\":31303},{\"end\":31321,\"start\":31316},{\"end\":31334,\"start\":31331},{\"end\":31347,\"start\":31340},{\"end\":31362,\"start\":31358},{\"end\":31382,\"start\":31374},{\"end\":31724,\"start\":31720},{\"end\":31760,\"start\":31751},{\"end\":31782,\"start\":31776},{\"end\":31810,\"start\":31803},{\"end\":32121,\"start\":32118},{\"end\":32270,\"start\":32266},{\"end\":32289,\"start\":32283},{\"end\":32501,\"start\":32496},{\"end\":32512,\"start\":32506},{\"end\":32526,\"start\":32520},{\"end\":32805,\"start\":32801},{\"end\":32819,\"start\":32814},{\"end\":32834,\"start\":32830},{\"end\":32847,\"start\":32841},{\"end\":32860,\"start\":32856},{\"end\":32870,\"start\":32867},{\"end\":32886,\"start\":32882},{\"end\":32899,\"start\":32894},{\"end\":32917,\"start\":32910},{\"end\":32919,\"start\":32918},{\"end\":32932,\"start\":32928},{\"end\":32945,\"start\":32942},{\"end\":33672,\"start\":33666},{\"end\":33923,\"start\":33918},{\"end\":33937,\"start\":33932},{\"end\":33952,\"start\":33947},{\"end\":33961,\"start\":33958},{\"end\":34278,\"start\":34267},{\"end\":34291,\"start\":34287},{\"end\":34305,\"start\":34300},{\"end\":34323,\"start\":34315},{\"end\":34335,\"start\":34333},{\"end\":34348,\"start\":34341},{\"end\":34357,\"start\":34353},{\"end\":34378,\"start\":34371},{\"end\":34393,\"start\":34387},{\"end\":34803,\"start\":34796},{\"end\":34815,\"start\":34810},{\"end\":34831,\"start\":34824},{\"end\":34844,\"start\":34838},{\"end\":34858,\"start\":34851},{\"end\":34872,\"start\":34868},{\"end\":34886,\"start\":34881},{\"end\":34897,\"start\":34892},{\"end\":34912,\"start\":34906},{\"end\":34929,\"start\":34923},{\"end\":35384,\"start\":35378},{\"end\":35400,\"start\":35393},{\"end\":35414,\"start\":35409},{\"end\":35429,\"start\":35426},{\"end\":35445,\"start\":35442},{\"end\":35945,\"start\":35936},{\"end\":35958,\"start\":35954},{\"end\":35964,\"start\":35963},{\"end\":35982,\"start\":35974},{\"end\":36311,\"start\":36302},{\"end\":36325,\"start\":36320},{\"end\":36342,\"start\":36337},{\"end\":36364,\"start\":36354},{\"end\":36381,\"start\":36371},{\"end\":36396,\"start\":36391},{\"end\":36413,\"start\":36408},{\"end\":36426,\"start\":36421},{\"end\":36440,\"start\":36435},{\"end\":36459,\"start\":36453},{\"end\":37011,\"start\":37006},{\"end\":37028,\"start\":37022},{\"end\":37051,\"start\":37045},{\"end\":37068,\"start\":37062},{\"end\":37429,\"start\":37422},{\"end\":37441,\"start\":37436},{\"end\":37454,\"start\":37448},{\"end\":37467,\"start\":37460},{\"end\":37480,\"start\":37474},{\"end\":37494,\"start\":37487},{\"end\":37512,\"start\":37506},{\"end\":37532,\"start\":37528},{\"end\":38055,\"start\":38049},{\"end\":38071,\"start\":38068},{\"end\":38088,\"start\":38082},{\"end\":38100,\"start\":38099},{\"end\":38113,\"start\":38108},{\"end\":38115,\"start\":38114},{\"end\":38132,\"start\":38125},{\"end\":38466,\"start\":38460},{\"end\":38480,\"start\":38473},{\"end\":38494,\"start\":38487},{\"end\":38502,\"start\":38499},{\"end\":38511,\"start\":38509},{\"end\":38524,\"start\":38516},{\"end\":38534,\"start\":38530},{\"end\":38545,\"start\":38539},{\"end\":39000,\"start\":38990},{\"end\":39015,\"start\":39008},{\"end\":39023,\"start\":39020},{\"end\":39034,\"start\":39030},{\"end\":39417,\"start\":39412},{\"end\":39431,\"start\":39425},{\"end\":39455,\"start\":39450},{\"end\":39470,\"start\":39463},{\"end\":39490,\"start\":39482},{\"end\":39506,\"start\":39497},{\"end\":39969,\"start\":39965},{\"end\":39982,\"start\":39977},{\"end\":39999,\"start\":39993},{\"end\":40011,\"start\":40007},{\"end\":40028,\"start\":40019},{\"end\":28568,\"start\":28562},{\"end\":28810,\"start\":28805},{\"end\":28824,\"start\":28819},{\"end\":29158,\"start\":29154},{\"end\":29328,\"start\":29325},{\"end\":29340,\"start\":29334},{\"end\":29353,\"start\":29346},{\"end\":29366,\"start\":29360},{\"end\":29378,\"start\":29374},{\"end\":29795,\"start\":29789},{\"end\":29810,\"start\":29807},{\"end\":29823,\"start\":29817},{\"end\":29831,\"start\":29830},{\"end\":29847,\"start\":29841},{\"end\":29849,\"start\":29848},{\"end\":29863,\"start\":29860},{\"end\":30292,\"start\":30286},{\"end\":30304,\"start\":30301},{\"end\":30324,\"start\":30321},{\"end\":30347,\"start\":30338},{\"end\":30739,\"start\":30734},{\"end\":30755,\"start\":30750},{\"end\":30773,\"start\":30766},{\"end\":30779,\"start\":30778},{\"end\":30800,\"start\":30799},{\"end\":31292,\"start\":31287},{\"end\":31307,\"start\":31303},{\"end\":31321,\"start\":31316},{\"end\":31334,\"start\":31331},{\"end\":31347,\"start\":31340},{\"end\":31362,\"start\":31358},{\"end\":31382,\"start\":31374},{\"end\":31724,\"start\":31720},{\"end\":31760,\"start\":31751},{\"end\":31782,\"start\":31776},{\"end\":31810,\"start\":31803},{\"end\":32121,\"start\":32118},{\"end\":32270,\"start\":32266},{\"end\":32289,\"start\":32283},{\"end\":32501,\"start\":32496},{\"end\":32512,\"start\":32506},{\"end\":32526,\"start\":32520},{\"end\":32805,\"start\":32801},{\"end\":32819,\"start\":32814},{\"end\":32834,\"start\":32830},{\"end\":32847,\"start\":32841},{\"end\":32860,\"start\":32856},{\"end\":32870,\"start\":32867},{\"end\":32886,\"start\":32882},{\"end\":32899,\"start\":32894},{\"end\":32917,\"start\":32910},{\"end\":32919,\"start\":32918},{\"end\":32932,\"start\":32928},{\"end\":32945,\"start\":32942},{\"end\":33672,\"start\":33666},{\"end\":33923,\"start\":33918},{\"end\":33937,\"start\":33932},{\"end\":33952,\"start\":33947},{\"end\":33961,\"start\":33958},{\"end\":34278,\"start\":34267},{\"end\":34291,\"start\":34287},{\"end\":34305,\"start\":34300},{\"end\":34323,\"start\":34315},{\"end\":34335,\"start\":34333},{\"end\":34348,\"start\":34341},{\"end\":34357,\"start\":34353},{\"end\":34378,\"start\":34371},{\"end\":34393,\"start\":34387},{\"end\":34803,\"start\":34796},{\"end\":34815,\"start\":34810},{\"end\":34831,\"start\":34824},{\"end\":34844,\"start\":34838},{\"end\":34858,\"start\":34851},{\"end\":34872,\"start\":34868},{\"end\":34886,\"start\":34881},{\"end\":34897,\"start\":34892},{\"end\":34912,\"start\":34906},{\"end\":34929,\"start\":34923},{\"end\":35384,\"start\":35378},{\"end\":35400,\"start\":35393},{\"end\":35414,\"start\":35409},{\"end\":35429,\"start\":35426},{\"end\":35445,\"start\":35442},{\"end\":35945,\"start\":35936},{\"end\":35958,\"start\":35954},{\"end\":35964,\"start\":35963},{\"end\":35982,\"start\":35974},{\"end\":36311,\"start\":36302},{\"end\":36325,\"start\":36320},{\"end\":36342,\"start\":36337},{\"end\":36364,\"start\":36354},{\"end\":36381,\"start\":36371},{\"end\":36396,\"start\":36391},{\"end\":36413,\"start\":36408},{\"end\":36426,\"start\":36421},{\"end\":36440,\"start\":36435},{\"end\":36459,\"start\":36453},{\"end\":37011,\"start\":37006},{\"end\":37028,\"start\":37022},{\"end\":37051,\"start\":37045},{\"end\":37068,\"start\":37062},{\"end\":37429,\"start\":37422},{\"end\":37441,\"start\":37436},{\"end\":37454,\"start\":37448},{\"end\":37467,\"start\":37460},{\"end\":37480,\"start\":37474},{\"end\":37494,\"start\":37487},{\"end\":37512,\"start\":37506},{\"end\":37532,\"start\":37528},{\"end\":38055,\"start\":38049},{\"end\":38071,\"start\":38068},{\"end\":38088,\"start\":38082},{\"end\":38100,\"start\":38099},{\"end\":38113,\"start\":38108},{\"end\":38115,\"start\":38114},{\"end\":38132,\"start\":38125},{\"end\":38466,\"start\":38460},{\"end\":38480,\"start\":38473},{\"end\":38494,\"start\":38487},{\"end\":38502,\"start\":38499},{\"end\":38511,\"start\":38509},{\"end\":38524,\"start\":38516},{\"end\":38534,\"start\":38530},{\"end\":38545,\"start\":38539},{\"end\":39000,\"start\":38990},{\"end\":39015,\"start\":39008},{\"end\":39023,\"start\":39020},{\"end\":39034,\"start\":39030},{\"end\":39417,\"start\":39412},{\"end\":39431,\"start\":39425},{\"end\":39455,\"start\":39450},{\"end\":39470,\"start\":39463},{\"end\":39490,\"start\":39482},{\"end\":39506,\"start\":39497},{\"end\":39969,\"start\":39965},{\"end\":39982,\"start\":39977},{\"end\":39999,\"start\":39993},{\"end\":40011,\"start\":40007},{\"end\":40028,\"start\":40019}]", "bib_author_last_name": "[{\"end\":28582,\"start\":28569},{\"end\":28591,\"start\":28584},{\"end\":28817,\"start\":28811},{\"end\":28834,\"start\":28825},{\"end\":29162,\"start\":29159},{\"end\":29332,\"start\":29329},{\"end\":29344,\"start\":29341},{\"end\":29358,\"start\":29354},{\"end\":29372,\"start\":29367},{\"end\":29382,\"start\":29379},{\"end\":29805,\"start\":29796},{\"end\":29815,\"start\":29811},{\"end\":29828,\"start\":29824},{\"end\":29839,\"start\":29832},{\"end\":29858,\"start\":29850},{\"end\":29872,\"start\":29864},{\"end\":29880,\"start\":29874},{\"end\":30299,\"start\":30293},{\"end\":30319,\"start\":30305},{\"end\":30336,\"start\":30325},{\"end\":30354,\"start\":30348},{\"end\":30748,\"start\":30740},{\"end\":30764,\"start\":30756},{\"end\":30776,\"start\":30774},{\"end\":30787,\"start\":30780},{\"end\":30797,\"start\":30789},{\"end\":30806,\"start\":30801},{\"end\":30819,\"start\":30808},{\"end\":31301,\"start\":31293},{\"end\":31314,\"start\":31308},{\"end\":31329,\"start\":31322},{\"end\":31338,\"start\":31335},{\"end\":31356,\"start\":31348},{\"end\":31372,\"start\":31363},{\"end\":31387,\"start\":31383},{\"end\":31749,\"start\":31725},{\"end\":31774,\"start\":31761},{\"end\":31793,\"start\":31783},{\"end\":31801,\"start\":31795},{\"end\":31833,\"start\":31811},{\"end\":31838,\"start\":31835},{\"end\":32141,\"start\":32122},{\"end\":32281,\"start\":32271},{\"end\":32301,\"start\":32290},{\"end\":32504,\"start\":32502},{\"end\":32518,\"start\":32513},{\"end\":32535,\"start\":32527},{\"end\":32812,\"start\":32806},{\"end\":32828,\"start\":32820},{\"end\":32839,\"start\":32835},{\"end\":32854,\"start\":32848},{\"end\":32865,\"start\":32861},{\"end\":32880,\"start\":32871},{\"end\":32892,\"start\":32887},{\"end\":32908,\"start\":32900},{\"end\":32926,\"start\":32920},{\"end\":32940,\"start\":32933},{\"end\":32955,\"start\":32946},{\"end\":33680,\"start\":33673},{\"end\":33930,\"start\":33924},{\"end\":33945,\"start\":33938},{\"end\":33956,\"start\":33953},{\"end\":33966,\"start\":33962},{\"end\":34285,\"start\":34279},{\"end\":34298,\"start\":34292},{\"end\":34313,\"start\":34306},{\"end\":34331,\"start\":34324},{\"end\":34339,\"start\":34336},{\"end\":34351,\"start\":34349},{\"end\":34369,\"start\":34358},{\"end\":34385,\"start\":34379},{\"end\":34399,\"start\":34394},{\"end\":34808,\"start\":34804},{\"end\":34822,\"start\":34816},{\"end\":34836,\"start\":34832},{\"end\":34849,\"start\":34845},{\"end\":34866,\"start\":34859},{\"end\":34879,\"start\":34873},{\"end\":34890,\"start\":34887},{\"end\":34904,\"start\":34898},{\"end\":34921,\"start\":34913},{\"end\":34938,\"start\":34930},{\"end\":35391,\"start\":35385},{\"end\":35407,\"start\":35401},{\"end\":35424,\"start\":35415},{\"end\":35440,\"start\":35430},{\"end\":35450,\"start\":35446},{\"end\":35952,\"start\":35946},{\"end\":35961,\"start\":35959},{\"end\":35972,\"start\":35965},{\"end\":35992,\"start\":35983},{\"end\":36003,\"start\":35994},{\"end\":36318,\"start\":36312},{\"end\":36335,\"start\":36326},{\"end\":36352,\"start\":36343},{\"end\":36369,\"start\":36365},{\"end\":36389,\"start\":36382},{\"end\":36406,\"start\":36397},{\"end\":36419,\"start\":36414},{\"end\":36433,\"start\":36427},{\"end\":36451,\"start\":36441},{\"end\":36464,\"start\":36460},{\"end\":37020,\"start\":37012},{\"end\":37043,\"start\":37029},{\"end\":37060,\"start\":37052},{\"end\":37075,\"start\":37069},{\"end\":37434,\"start\":37430},{\"end\":37446,\"start\":37442},{\"end\":37458,\"start\":37455},{\"end\":37472,\"start\":37468},{\"end\":37485,\"start\":37481},{\"end\":37504,\"start\":37495},{\"end\":37526,\"start\":37513},{\"end\":37541,\"start\":37533},{\"end\":38066,\"start\":38056},{\"end\":38080,\"start\":38072},{\"end\":38097,\"start\":38089},{\"end\":38106,\"start\":38101},{\"end\":38123,\"start\":38116},{\"end\":38139,\"start\":38133},{\"end\":38146,\"start\":38141},{\"end\":38471,\"start\":38467},{\"end\":38485,\"start\":38481},{\"end\":38497,\"start\":38495},{\"end\":38507,\"start\":38503},{\"end\":38514,\"start\":38512},{\"end\":38528,\"start\":38525},{\"end\":38537,\"start\":38535},{\"end\":38551,\"start\":38546},{\"end\":39006,\"start\":39001},{\"end\":39018,\"start\":39016},{\"end\":39028,\"start\":39024},{\"end\":39038,\"start\":39035},{\"end\":39423,\"start\":39418},{\"end\":39448,\"start\":39432},{\"end\":39461,\"start\":39456},{\"end\":39480,\"start\":39471},{\"end\":39495,\"start\":39491},{\"end\":39515,\"start\":39507},{\"end\":39975,\"start\":39970},{\"end\":39991,\"start\":39983},{\"end\":40005,\"start\":40000},{\"end\":40017,\"start\":40012},{\"end\":40033,\"start\":40029},{\"end\":28582,\"start\":28569},{\"end\":28591,\"start\":28584},{\"end\":28817,\"start\":28811},{\"end\":28834,\"start\":28825},{\"end\":29162,\"start\":29159},{\"end\":29332,\"start\":29329},{\"end\":29344,\"start\":29341},{\"end\":29358,\"start\":29354},{\"end\":29372,\"start\":29367},{\"end\":29382,\"start\":29379},{\"end\":29805,\"start\":29796},{\"end\":29815,\"start\":29811},{\"end\":29828,\"start\":29824},{\"end\":29839,\"start\":29832},{\"end\":29858,\"start\":29850},{\"end\":29872,\"start\":29864},{\"end\":29880,\"start\":29874},{\"end\":30299,\"start\":30293},{\"end\":30319,\"start\":30305},{\"end\":30336,\"start\":30325},{\"end\":30354,\"start\":30348},{\"end\":30748,\"start\":30740},{\"end\":30764,\"start\":30756},{\"end\":30776,\"start\":30774},{\"end\":30787,\"start\":30780},{\"end\":30797,\"start\":30789},{\"end\":30806,\"start\":30801},{\"end\":30819,\"start\":30808},{\"end\":31301,\"start\":31293},{\"end\":31314,\"start\":31308},{\"end\":31329,\"start\":31322},{\"end\":31338,\"start\":31335},{\"end\":31356,\"start\":31348},{\"end\":31372,\"start\":31363},{\"end\":31387,\"start\":31383},{\"end\":31749,\"start\":31725},{\"end\":31774,\"start\":31761},{\"end\":31793,\"start\":31783},{\"end\":31801,\"start\":31795},{\"end\":31833,\"start\":31811},{\"end\":31838,\"start\":31835},{\"end\":32141,\"start\":32122},{\"end\":32281,\"start\":32271},{\"end\":32301,\"start\":32290},{\"end\":32504,\"start\":32502},{\"end\":32518,\"start\":32513},{\"end\":32535,\"start\":32527},{\"end\":32812,\"start\":32806},{\"end\":32828,\"start\":32820},{\"end\":32839,\"start\":32835},{\"end\":32854,\"start\":32848},{\"end\":32865,\"start\":32861},{\"end\":32880,\"start\":32871},{\"end\":32892,\"start\":32887},{\"end\":32908,\"start\":32900},{\"end\":32926,\"start\":32920},{\"end\":32940,\"start\":32933},{\"end\":32955,\"start\":32946},{\"end\":33680,\"start\":33673},{\"end\":33930,\"start\":33924},{\"end\":33945,\"start\":33938},{\"end\":33956,\"start\":33953},{\"end\":33966,\"start\":33962},{\"end\":34285,\"start\":34279},{\"end\":34298,\"start\":34292},{\"end\":34313,\"start\":34306},{\"end\":34331,\"start\":34324},{\"end\":34339,\"start\":34336},{\"end\":34351,\"start\":34349},{\"end\":34369,\"start\":34358},{\"end\":34385,\"start\":34379},{\"end\":34399,\"start\":34394},{\"end\":34808,\"start\":34804},{\"end\":34822,\"start\":34816},{\"end\":34836,\"start\":34832},{\"end\":34849,\"start\":34845},{\"end\":34866,\"start\":34859},{\"end\":34879,\"start\":34873},{\"end\":34890,\"start\":34887},{\"end\":34904,\"start\":34898},{\"end\":34921,\"start\":34913},{\"end\":34938,\"start\":34930},{\"end\":35391,\"start\":35385},{\"end\":35407,\"start\":35401},{\"end\":35424,\"start\":35415},{\"end\":35440,\"start\":35430},{\"end\":35450,\"start\":35446},{\"end\":35952,\"start\":35946},{\"end\":35961,\"start\":35959},{\"end\":35972,\"start\":35965},{\"end\":35992,\"start\":35983},{\"end\":36003,\"start\":35994},{\"end\":36318,\"start\":36312},{\"end\":36335,\"start\":36326},{\"end\":36352,\"start\":36343},{\"end\":36369,\"start\":36365},{\"end\":36389,\"start\":36382},{\"end\":36406,\"start\":36397},{\"end\":36419,\"start\":36414},{\"end\":36433,\"start\":36427},{\"end\":36451,\"start\":36441},{\"end\":36464,\"start\":36460},{\"end\":37020,\"start\":37012},{\"end\":37043,\"start\":37029},{\"end\":37060,\"start\":37052},{\"end\":37075,\"start\":37069},{\"end\":37434,\"start\":37430},{\"end\":37446,\"start\":37442},{\"end\":37458,\"start\":37455},{\"end\":37472,\"start\":37468},{\"end\":37485,\"start\":37481},{\"end\":37504,\"start\":37495},{\"end\":37526,\"start\":37513},{\"end\":37541,\"start\":37533},{\"end\":38066,\"start\":38056},{\"end\":38080,\"start\":38072},{\"end\":38097,\"start\":38089},{\"end\":38106,\"start\":38101},{\"end\":38123,\"start\":38116},{\"end\":38139,\"start\":38133},{\"end\":38146,\"start\":38141},{\"end\":38471,\"start\":38467},{\"end\":38485,\"start\":38481},{\"end\":38497,\"start\":38495},{\"end\":38507,\"start\":38503},{\"end\":38514,\"start\":38512},{\"end\":38528,\"start\":38525},{\"end\":38537,\"start\":38535},{\"end\":38551,\"start\":38546},{\"end\":39006,\"start\":39001},{\"end\":39018,\"start\":39016},{\"end\":39028,\"start\":39024},{\"end\":39038,\"start\":39035},{\"end\":39423,\"start\":39418},{\"end\":39448,\"start\":39432},{\"end\":39461,\"start\":39456},{\"end\":39480,\"start\":39471},{\"end\":39495,\"start\":39491},{\"end\":39515,\"start\":39507},{\"end\":39975,\"start\":39970},{\"end\":39991,\"start\":39983},{\"end\":40005,\"start\":40000},{\"end\":40017,\"start\":40012},{\"end\":40033,\"start\":40029}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":28201,\"start\":27980},{\"attributes\":{\"id\":\"b1\"},\"end\":28320,\"start\":28203},{\"attributes\":{\"id\":\"b2\"},\"end\":28466,\"start\":28322},{\"attributes\":{\"doi\":\"arXiv:2210.14665\",\"id\":\"b3\"},\"end\":28787,\"start\":28468},{\"attributes\":{\"id\":\"b4\"},\"end\":28945,\"start\":28789},{\"attributes\":{\"doi\":\"archiveteam. 2021\",\"id\":\"b5\"},\"end\":29127,\"start\":28947},{\"attributes\":{\"id\":\"b6\"},\"end\":29247,\"start\":29129},{\"attributes\":{\"doi\":\"arXiv:1908.09791\",\"id\":\"b7\"},\"end\":29598,\"start\":29249},{\"attributes\":{\"id\":\"b8\"},\"end\":29730,\"start\":29600},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1701442},\"end\":30208,\"start\":29732},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3036526},\"end\":30622,\"start\":30210},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8367854},\"end\":31209,\"start\":30624},{\"attributes\":{\"doi\":\"arXiv:2006.02464\",\"id\":\"b12\"},\"end\":31648,\"start\":31211},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":249059077},\"end\":32081,\"start\":31650},{\"attributes\":{\"id\":\"b14\"},\"end\":32240,\"start\":32083},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1915014},\"end\":32436,\"start\":32242},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":239890182},\"end\":32799,\"start\":32438},{\"attributes\":{\"id\":\"b17\"},\"end\":33614,\"start\":32801},{\"attributes\":{\"id\":\"b18\"},\"end\":33853,\"start\":33616},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":255170481},\"end\":34206,\"start\":33855},{\"attributes\":{\"doi\":\"arXiv:1712.06139\",\"id\":\"b20\"},\"end\":34673,\"start\":34208},{\"attributes\":{\"doi\":\"arXiv:1811.09886\",\"id\":\"b21\"},\"end\":35287,\"start\":34675},{\"attributes\":{\"doi\":\"10.1109/RTAS54340.2022.00020\",\"id\":\"b22\",\"matched_paper_id\":250118660},\"end\":35878,\"start\":35289},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":236922757},\"end\":36257,\"start\":35880},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":218489692},\"end\":36889,\"start\":36259},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":234292627},\"end\":37349,\"start\":36891},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":204812163},\"end\":37954,\"start\":37351},{\"attributes\":{\"id\":\"b27\"},\"end\":38377,\"start\":37956},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":239890145},\"end\":38890,\"start\":38379},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":196810567},\"end\":39338,\"start\":38892},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11894138},\"end\":39869,\"start\":39340},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":220835829},\"end\":40339,\"start\":39871},{\"attributes\":{\"id\":\"b0\"},\"end\":28201,\"start\":27980},{\"attributes\":{\"id\":\"b1\"},\"end\":28320,\"start\":28203},{\"attributes\":{\"id\":\"b2\"},\"end\":28466,\"start\":28322},{\"attributes\":{\"doi\":\"arXiv:2210.14665\",\"id\":\"b3\"},\"end\":28787,\"start\":28468},{\"attributes\":{\"id\":\"b4\"},\"end\":28945,\"start\":28789},{\"attributes\":{\"doi\":\"archiveteam. 2021\",\"id\":\"b5\"},\"end\":29127,\"start\":28947},{\"attributes\":{\"id\":\"b6\"},\"end\":29247,\"start\":29129},{\"attributes\":{\"doi\":\"arXiv:1908.09791\",\"id\":\"b7\"},\"end\":29598,\"start\":29249},{\"attributes\":{\"id\":\"b8\"},\"end\":29730,\"start\":29600},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1701442},\"end\":30208,\"start\":29732},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3036526},\"end\":30622,\"start\":30210},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8367854},\"end\":31209,\"start\":30624},{\"attributes\":{\"doi\":\"arXiv:2006.02464\",\"id\":\"b12\"},\"end\":31648,\"start\":31211},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":249059077},\"end\":32081,\"start\":31650},{\"attributes\":{\"id\":\"b14\"},\"end\":32240,\"start\":32083},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1915014},\"end\":32436,\"start\":32242},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":239890182},\"end\":32799,\"start\":32438},{\"attributes\":{\"id\":\"b17\"},\"end\":33614,\"start\":32801},{\"attributes\":{\"id\":\"b18\"},\"end\":33853,\"start\":33616},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":255170481},\"end\":34206,\"start\":33855},{\"attributes\":{\"doi\":\"arXiv:1712.06139\",\"id\":\"b20\"},\"end\":34673,\"start\":34208},{\"attributes\":{\"doi\":\"arXiv:1811.09886\",\"id\":\"b21\"},\"end\":35287,\"start\":34675},{\"attributes\":{\"doi\":\"10.1109/RTAS54340.2022.00020\",\"id\":\"b22\",\"matched_paper_id\":250118660},\"end\":35878,\"start\":35289},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":236922757},\"end\":36257,\"start\":35880},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":218489692},\"end\":36889,\"start\":36259},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":234292627},\"end\":37349,\"start\":36891},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":204812163},\"end\":37954,\"start\":37351},{\"attributes\":{\"id\":\"b27\"},\"end\":38377,\"start\":37956},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":239890145},\"end\":38890,\"start\":38379},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":196810567},\"end\":39338,\"start\":38892},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11894138},\"end\":39869,\"start\":39340},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":220835829},\"end\":40339,\"start\":39871}]", "bib_title": "[{\"end\":29787,\"start\":29732},{\"end\":30284,\"start\":30210},{\"end\":30732,\"start\":30624},{\"end\":31718,\"start\":31650},{\"end\":32264,\"start\":32242},{\"end\":32494,\"start\":32438},{\"end\":33916,\"start\":33855},{\"end\":35376,\"start\":35289},{\"end\":35934,\"start\":35880},{\"end\":36300,\"start\":36259},{\"end\":37004,\"start\":36891},{\"end\":37420,\"start\":37351},{\"end\":38458,\"start\":38379},{\"end\":38988,\"start\":38892},{\"end\":39410,\"start\":39340},{\"end\":39963,\"start\":39871},{\"end\":29787,\"start\":29732},{\"end\":30284,\"start\":30210},{\"end\":30732,\"start\":30624},{\"end\":31718,\"start\":31650},{\"end\":32264,\"start\":32242},{\"end\":32494,\"start\":32438},{\"end\":33916,\"start\":33855},{\"end\":35376,\"start\":35289},{\"end\":35934,\"start\":35880},{\"end\":36300,\"start\":36259},{\"end\":37004,\"start\":36891},{\"end\":37420,\"start\":37351},{\"end\":38458,\"start\":38379},{\"end\":38988,\"start\":38892},{\"end\":39410,\"start\":39340},{\"end\":39963,\"start\":39871}]", "bib_author": "[{\"end\":28584,\"start\":28562},{\"end\":28593,\"start\":28584},{\"end\":28819,\"start\":28805},{\"end\":28836,\"start\":28819},{\"end\":29164,\"start\":29154},{\"end\":29334,\"start\":29325},{\"end\":29346,\"start\":29334},{\"end\":29360,\"start\":29346},{\"end\":29374,\"start\":29360},{\"end\":29384,\"start\":29374},{\"end\":29807,\"start\":29789},{\"end\":29817,\"start\":29807},{\"end\":29830,\"start\":29817},{\"end\":29841,\"start\":29830},{\"end\":29860,\"start\":29841},{\"end\":29874,\"start\":29860},{\"end\":29882,\"start\":29874},{\"end\":30301,\"start\":30286},{\"end\":30321,\"start\":30301},{\"end\":30338,\"start\":30321},{\"end\":30356,\"start\":30338},{\"end\":30750,\"start\":30734},{\"end\":30766,\"start\":30750},{\"end\":30778,\"start\":30766},{\"end\":30789,\"start\":30778},{\"end\":30799,\"start\":30789},{\"end\":30808,\"start\":30799},{\"end\":30821,\"start\":30808},{\"end\":31303,\"start\":31287},{\"end\":31316,\"start\":31303},{\"end\":31331,\"start\":31316},{\"end\":31340,\"start\":31331},{\"end\":31358,\"start\":31340},{\"end\":31374,\"start\":31358},{\"end\":31389,\"start\":31374},{\"end\":31751,\"start\":31720},{\"end\":31776,\"start\":31751},{\"end\":31795,\"start\":31776},{\"end\":31803,\"start\":31795},{\"end\":31835,\"start\":31803},{\"end\":31840,\"start\":31835},{\"end\":32143,\"start\":32118},{\"end\":32283,\"start\":32266},{\"end\":32303,\"start\":32283},{\"end\":32506,\"start\":32496},{\"end\":32520,\"start\":32506},{\"end\":32537,\"start\":32520},{\"end\":32814,\"start\":32801},{\"end\":32830,\"start\":32814},{\"end\":32841,\"start\":32830},{\"end\":32856,\"start\":32841},{\"end\":32867,\"start\":32856},{\"end\":32882,\"start\":32867},{\"end\":32894,\"start\":32882},{\"end\":32910,\"start\":32894},{\"end\":32928,\"start\":32910},{\"end\":32942,\"start\":32928},{\"end\":32957,\"start\":32942},{\"end\":33682,\"start\":33666},{\"end\":33932,\"start\":33918},{\"end\":33947,\"start\":33932},{\"end\":33958,\"start\":33947},{\"end\":33968,\"start\":33958},{\"end\":34287,\"start\":34267},{\"end\":34300,\"start\":34287},{\"end\":34315,\"start\":34300},{\"end\":34333,\"start\":34315},{\"end\":34341,\"start\":34333},{\"end\":34353,\"start\":34341},{\"end\":34371,\"start\":34353},{\"end\":34387,\"start\":34371},{\"end\":34401,\"start\":34387},{\"end\":34810,\"start\":34796},{\"end\":34824,\"start\":34810},{\"end\":34838,\"start\":34824},{\"end\":34851,\"start\":34838},{\"end\":34868,\"start\":34851},{\"end\":34881,\"start\":34868},{\"end\":34892,\"start\":34881},{\"end\":34906,\"start\":34892},{\"end\":34923,\"start\":34906},{\"end\":34940,\"start\":34923},{\"end\":35393,\"start\":35378},{\"end\":35409,\"start\":35393},{\"end\":35426,\"start\":35409},{\"end\":35442,\"start\":35426},{\"end\":35452,\"start\":35442},{\"end\":35954,\"start\":35936},{\"end\":35963,\"start\":35954},{\"end\":35974,\"start\":35963},{\"end\":35994,\"start\":35974},{\"end\":36005,\"start\":35994},{\"end\":36320,\"start\":36302},{\"end\":36337,\"start\":36320},{\"end\":36354,\"start\":36337},{\"end\":36371,\"start\":36354},{\"end\":36391,\"start\":36371},{\"end\":36408,\"start\":36391},{\"end\":36421,\"start\":36408},{\"end\":36435,\"start\":36421},{\"end\":36453,\"start\":36435},{\"end\":36466,\"start\":36453},{\"end\":37022,\"start\":37006},{\"end\":37045,\"start\":37022},{\"end\":37062,\"start\":37045},{\"end\":37077,\"start\":37062},{\"end\":37436,\"start\":37422},{\"end\":37448,\"start\":37436},{\"end\":37460,\"start\":37448},{\"end\":37474,\"start\":37460},{\"end\":37487,\"start\":37474},{\"end\":37506,\"start\":37487},{\"end\":37528,\"start\":37506},{\"end\":37543,\"start\":37528},{\"end\":38068,\"start\":38049},{\"end\":38082,\"start\":38068},{\"end\":38099,\"start\":38082},{\"end\":38108,\"start\":38099},{\"end\":38125,\"start\":38108},{\"end\":38141,\"start\":38125},{\"end\":38148,\"start\":38141},{\"end\":38473,\"start\":38460},{\"end\":38487,\"start\":38473},{\"end\":38499,\"start\":38487},{\"end\":38509,\"start\":38499},{\"end\":38516,\"start\":38509},{\"end\":38530,\"start\":38516},{\"end\":38539,\"start\":38530},{\"end\":38553,\"start\":38539},{\"end\":39008,\"start\":38990},{\"end\":39020,\"start\":39008},{\"end\":39030,\"start\":39020},{\"end\":39040,\"start\":39030},{\"end\":39425,\"start\":39412},{\"end\":39450,\"start\":39425},{\"end\":39463,\"start\":39450},{\"end\":39482,\"start\":39463},{\"end\":39497,\"start\":39482},{\"end\":39517,\"start\":39497},{\"end\":39977,\"start\":39965},{\"end\":39993,\"start\":39977},{\"end\":40007,\"start\":39993},{\"end\":40019,\"start\":40007},{\"end\":40035,\"start\":40019},{\"end\":28584,\"start\":28562},{\"end\":28593,\"start\":28584},{\"end\":28819,\"start\":28805},{\"end\":28836,\"start\":28819},{\"end\":29164,\"start\":29154},{\"end\":29334,\"start\":29325},{\"end\":29346,\"start\":29334},{\"end\":29360,\"start\":29346},{\"end\":29374,\"start\":29360},{\"end\":29384,\"start\":29374},{\"end\":29807,\"start\":29789},{\"end\":29817,\"start\":29807},{\"end\":29830,\"start\":29817},{\"end\":29841,\"start\":29830},{\"end\":29860,\"start\":29841},{\"end\":29874,\"start\":29860},{\"end\":29882,\"start\":29874},{\"end\":30301,\"start\":30286},{\"end\":30321,\"start\":30301},{\"end\":30338,\"start\":30321},{\"end\":30356,\"start\":30338},{\"end\":30750,\"start\":30734},{\"end\":30766,\"start\":30750},{\"end\":30778,\"start\":30766},{\"end\":30789,\"start\":30778},{\"end\":30799,\"start\":30789},{\"end\":30808,\"start\":30799},{\"end\":30821,\"start\":30808},{\"end\":31303,\"start\":31287},{\"end\":31316,\"start\":31303},{\"end\":31331,\"start\":31316},{\"end\":31340,\"start\":31331},{\"end\":31358,\"start\":31340},{\"end\":31374,\"start\":31358},{\"end\":31389,\"start\":31374},{\"end\":31751,\"start\":31720},{\"end\":31776,\"start\":31751},{\"end\":31795,\"start\":31776},{\"end\":31803,\"start\":31795},{\"end\":31835,\"start\":31803},{\"end\":31840,\"start\":31835},{\"end\":32143,\"start\":32118},{\"end\":32283,\"start\":32266},{\"end\":32303,\"start\":32283},{\"end\":32506,\"start\":32496},{\"end\":32520,\"start\":32506},{\"end\":32537,\"start\":32520},{\"end\":32814,\"start\":32801},{\"end\":32830,\"start\":32814},{\"end\":32841,\"start\":32830},{\"end\":32856,\"start\":32841},{\"end\":32867,\"start\":32856},{\"end\":32882,\"start\":32867},{\"end\":32894,\"start\":32882},{\"end\":32910,\"start\":32894},{\"end\":32928,\"start\":32910},{\"end\":32942,\"start\":32928},{\"end\":32957,\"start\":32942},{\"end\":33682,\"start\":33666},{\"end\":33932,\"start\":33918},{\"end\":33947,\"start\":33932},{\"end\":33958,\"start\":33947},{\"end\":33968,\"start\":33958},{\"end\":34287,\"start\":34267},{\"end\":34300,\"start\":34287},{\"end\":34315,\"start\":34300},{\"end\":34333,\"start\":34315},{\"end\":34341,\"start\":34333},{\"end\":34353,\"start\":34341},{\"end\":34371,\"start\":34353},{\"end\":34387,\"start\":34371},{\"end\":34401,\"start\":34387},{\"end\":34810,\"start\":34796},{\"end\":34824,\"start\":34810},{\"end\":34838,\"start\":34824},{\"end\":34851,\"start\":34838},{\"end\":34868,\"start\":34851},{\"end\":34881,\"start\":34868},{\"end\":34892,\"start\":34881},{\"end\":34906,\"start\":34892},{\"end\":34923,\"start\":34906},{\"end\":34940,\"start\":34923},{\"end\":35393,\"start\":35378},{\"end\":35409,\"start\":35393},{\"end\":35426,\"start\":35409},{\"end\":35442,\"start\":35426},{\"end\":35452,\"start\":35442},{\"end\":35954,\"start\":35936},{\"end\":35963,\"start\":35954},{\"end\":35974,\"start\":35963},{\"end\":35994,\"start\":35974},{\"end\":36005,\"start\":35994},{\"end\":36320,\"start\":36302},{\"end\":36337,\"start\":36320},{\"end\":36354,\"start\":36337},{\"end\":36371,\"start\":36354},{\"end\":36391,\"start\":36371},{\"end\":36408,\"start\":36391},{\"end\":36421,\"start\":36408},{\"end\":36435,\"start\":36421},{\"end\":36453,\"start\":36435},{\"end\":36466,\"start\":36453},{\"end\":37022,\"start\":37006},{\"end\":37045,\"start\":37022},{\"end\":37062,\"start\":37045},{\"end\":37077,\"start\":37062},{\"end\":37436,\"start\":37422},{\"end\":37448,\"start\":37436},{\"end\":37460,\"start\":37448},{\"end\":37474,\"start\":37460},{\"end\":37487,\"start\":37474},{\"end\":37506,\"start\":37487},{\"end\":37528,\"start\":37506},{\"end\":37543,\"start\":37528},{\"end\":38068,\"start\":38049},{\"end\":38082,\"start\":38068},{\"end\":38099,\"start\":38082},{\"end\":38108,\"start\":38099},{\"end\":38125,\"start\":38108},{\"end\":38141,\"start\":38125},{\"end\":38148,\"start\":38141},{\"end\":38473,\"start\":38460},{\"end\":38487,\"start\":38473},{\"end\":38499,\"start\":38487},{\"end\":38509,\"start\":38499},{\"end\":38516,\"start\":38509},{\"end\":38530,\"start\":38516},{\"end\":38539,\"start\":38530},{\"end\":38553,\"start\":38539},{\"end\":39008,\"start\":38990},{\"end\":39020,\"start\":39008},{\"end\":39030,\"start\":39020},{\"end\":39040,\"start\":39030},{\"end\":39425,\"start\":39412},{\"end\":39450,\"start\":39425},{\"end\":39463,\"start\":39450},{\"end\":39482,\"start\":39463},{\"end\":39497,\"start\":39482},{\"end\":39517,\"start\":39497},{\"end\":39977,\"start\":39965},{\"end\":39993,\"start\":39977},{\"end\":40007,\"start\":39993},{\"end\":40019,\"start\":40007},{\"end\":40035,\"start\":40019}]", "bib_venue": "[{\"end\":30930,\"start\":30884},{\"end\":32626,\"start\":32590},{\"end\":33180,\"start\":33054},{\"end\":36589,\"start\":36536},{\"end\":37668,\"start\":37614},{\"end\":38642,\"start\":38606},{\"end\":30930,\"start\":30884},{\"end\":32626,\"start\":32590},{\"end\":33180,\"start\":33054},{\"end\":36589,\"start\":36536},{\"end\":37668,\"start\":37614},{\"end\":38642,\"start\":38606},{\"end\":28038,\"start\":27980},{\"end\":28226,\"start\":28203},{\"end\":28346,\"start\":28322},{\"end\":28560,\"start\":28468},{\"end\":28803,\"start\":28789},{\"end\":28981,\"start\":28947},{\"end\":29152,\"start\":29129},{\"end\":29323,\"start\":29249},{\"end\":29645,\"start\":29600},{\"end\":29952,\"start\":29882},{\"end\":30399,\"start\":30356},{\"end\":30882,\"start\":30821},{\"end\":31285,\"start\":31211},{\"end\":31851,\"start\":31840},{\"end\":32116,\"start\":32083},{\"end\":32321,\"start\":32303},{\"end\":32588,\"start\":32537},{\"end\":33052,\"start\":32957},{\"end\":33664,\"start\":33616},{\"end\":34018,\"start\":33968},{\"end\":34265,\"start\":34208},{\"end\":34794,\"start\":34675},{\"end\":35562,\"start\":35480},{\"end\":36059,\"start\":36005},{\"end\":36534,\"start\":36466},{\"end\":37102,\"start\":37077},{\"end\":37612,\"start\":37543},{\"end\":38047,\"start\":37956},{\"end\":38604,\"start\":38553},{\"end\":39100,\"start\":39040},{\"end\":39585,\"start\":39517},{\"end\":40090,\"start\":40035},{\"end\":28038,\"start\":27980},{\"end\":28226,\"start\":28203},{\"end\":28346,\"start\":28322},{\"end\":28560,\"start\":28468},{\"end\":28803,\"start\":28789},{\"end\":28981,\"start\":28947},{\"end\":29152,\"start\":29129},{\"end\":29323,\"start\":29249},{\"end\":29645,\"start\":29600},{\"end\":29952,\"start\":29882},{\"end\":30399,\"start\":30356},{\"end\":30882,\"start\":30821},{\"end\":31285,\"start\":31211},{\"end\":31851,\"start\":31840},{\"end\":32116,\"start\":32083},{\"end\":32321,\"start\":32303},{\"end\":32588,\"start\":32537},{\"end\":33052,\"start\":32957},{\"end\":33664,\"start\":33616},{\"end\":34018,\"start\":33968},{\"end\":34265,\"start\":34208},{\"end\":34794,\"start\":34675},{\"end\":35562,\"start\":35480},{\"end\":36059,\"start\":36005},{\"end\":36534,\"start\":36466},{\"end\":37102,\"start\":37077},{\"end\":37612,\"start\":37543},{\"end\":38047,\"start\":37956},{\"end\":38604,\"start\":38553},{\"end\":39100,\"start\":39040},{\"end\":39585,\"start\":39517},{\"end\":40090,\"start\":40035}]"}}}, "year": 2023, "month": 12, "day": 17}