{"id": 252735071, "updated": "2023-10-05 09:54:10.281", "metadata": {"title": "DigiFace-1M: 1 Million Digital Face Images for Face Recognition", "authors": "[{\"first\":\"Gwangbin\",\"last\":\"Bae\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Gorce\",\"middle\":[\"de\",\"La\"]},{\"first\":\"Tadas\",\"last\":\"Baltrusaitis\",\"middle\":[]},{\"first\":\"Charlie\",\"last\":\"Hewitt\",\"middle\":[]},{\"first\":\"Dong\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Julien\",\"last\":\"Valentin\",\"middle\":[]},{\"first\":\"Roberto\",\"last\":\"Cipolla\",\"middle\":[]},{\"first\":\"Jingjing\",\"last\":\"Shen\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "State-of-the-art face recognition models show impressive accuracy, achieving over 99.8% on Labeled Faces in the Wild (LFW) dataset. Such models are trained on large-scale datasets that contain millions of real human face images collected from the internet. Web-crawled face images are severely biased (in terms of race, lighting, make-up, etc) and often contain label noise. More importantly, the face images are collected without explicit consent, raising ethical concerns. To avoid such problems, we introduce a large-scale synthetic dataset for face recognition, obtained by rendering digital faces using a computer graphics pipeline. We first demonstrate that aggressive data augmentation can significantly reduce the synthetic-to-real domain gap. Having full control over the rendering pipeline, we also study how each attribute (e.g., variation in facial pose, accessories and textures) affects the accuracy. Compared to SynFace, a recent method trained on GAN-generated synthetic faces, we reduce the error rate on LFW by 52.5% (accuracy from 91.93% to 96.17%). By fine-tuning the network on a smaller number of real face images that could reasonably be obtained with consent, we achieve accuracy that is comparable to the methods trained on millions of real face images.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2210.02579", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wacv/BaeGBHCVCS23", "doi": "10.1109/wacv56688.2023.00352"}}, "content": {"source": {"pdf_hash": "b5fd223fae12e64d9295217359441ab8185d4ac0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.02579v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b8d51a451c61660dcf72237052fff8f744a5ae4d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b5fd223fae12e64d9295217359441ab8185d4ac0.txt", "contents": "\nDigiFace-1M: 1 Million Digital Face Images for Face Recognition\n\n\nGwangbin Bae \nUniversity of Cambridge\nUniversity of Cambridge\n\n\nMartin De \nUniversity of Cambridge\nUniversity of Cambridge\n\n\nLa Gorce Microsoft \nUniversity of Cambridge\nUniversity of Cambridge\n\n\nTadas Baltru\u0161aitis Microsoft \nUniversity of Cambridge\nUniversity of Cambridge\n\n\nCharlie Hewitt Microsoft \nUniversity of Cambridge\nUniversity of Cambridge\n\n\nDong Chen Microsoft \nUniversity of Cambridge\nUniversity of Cambridge\n\n\nJulien Valentin Microsoft \nUniversity of Cambridge\nUniversity of Cambridge\n\n\nRoberto Cipolla \nUniversity of Cambridge\nUniversity of Cambridge\n\n\nJingjing Shen jinshen@microsoft.com \nUniversity of Cambridge\nUniversity of Cambridge\n\n\nMicrosoft \nUniversity of Cambridge\nUniversity of Cambridge\n\n\nDigiFace-1M: 1 Million Digital Face Images for Face Recognition\n\nState-of-the-art face recognition models show impressive accuracy, achieving over 99.8% on Labeled Faces in the Wild (LFW) dataset. Such models are trained on large-scale datasets that contain millions of real human face images collected from the internet. Web-crawled face images are severely biased (in terms of race, lighting, make-up, etc) and often contain label noise. More importantly, the face images are collected without explicit consent, raising ethical concerns. To avoid such problems, we introduce a largescale synthetic dataset for face recognition, obtained by rendering digital faces using a computer graphics pipeline 1 . We first demonstrate that aggressive data augmentation can significantly reduce the synthetic-to-real domain gap. Having full control over the rendering pipeline, we also study how each attribute (e.g., variation in facial pose, accessories and textures) affects the accuracy. Compared to Syn-Face, a recent method trained on GAN-generated synthetic faces, we reduce the error rate on LFW by 52.5% (accuracy from 91.93% to 96.17%). By fine-tuning the network on a smaller number of real face images that could reasonably be obtained with consent, we achieve accuracy that is comparable to the methods trained on millions of real face images.\n\nIntroduction\n\nLearning-based face recognition models [29,23,33,35,8,15,24,18] use Deep Neural Networks (DNNs) to encode the given face image into an embedding vector of fixed di-mension (e.g., 512). These embeddings can then be used for various tasks, such as face identification (who is this person) and verification (are they the same person). To learn diverse, discriminative embeddings, the training dataset should contain a large number of unique identities. To learn robust embeddings, i.e., which are not sensitive to the changes in pose, expression, accessories, camera and lighting, the dataset should also contain a sufficient number of images per identity with these variations.\n\nPublicly available face recognition datasets satisfy both. MS1MV2 [8] contains 5.8M images of 85K identities (approx. 68 images per ID). Recently released Web-Face260M [43] contains 260M images of 4M identities (approx. 65 images per ID). While such datasets have driven recent advances in face recognition models, there are several problems associated with them.\n\n(1) Ethical issues. Large-scale face recognition datasets are often criticized for ethical issues including privacy violation and the lack of informed consent. For example, datasets like [39,12,8,43] are obtained by crawling web images of celebrities without consent. To increase the number of identities, some datasets exploited the term \"celebrities\" to include anyone with online presence. Datasets like [17,26] collected face images of the general public (including children) from Flickr [3]. Projects like MegaPixels [4] are exposing the ethical problems of such web-crawled face recognition datasets. Following severe criticism, public access to several datasets has been removed [2].\n\n(2) Label noise. Web images collected by searching the names of celebrities often contain label errors. For example, the Labeled Faces in the Wild (LFW) dataset [14] contains several known errors including: (1) mislabeled images; (2) distinct persons with the same name labeled as the same per- (3) Data bias. Face recognition models are generally trained and tested on celebrity faces, many of which are taken with strong lighting and make-up. Celebrity faces also have imbalanced racial distribution (e.g., 84.5% of the faces in CASIA-WebFace [39] are Caucasian faces [34]), leading to poor recognition accuracy for the under-represented racial groups [34].\n\nIn order to circumvent all these issues that affect the existing real face datasets, we introduce a new large-scale face recognition dataset consisting only of photo-realistic digital face images rendered using a computer graphics pipeline and make this dataset available to the community. Specifically, we build upon the face generation pipeline introduced by Wood et al. [36], tailoring the amount of variability for each attribute (e.g., pose and accessories) for our recognition task, and generate 1.22M images with 110K unique identities. Each identity is generated by randomizing the facial geometry and texture as well as the hair style. The generated face is then rendered with different poses, expressions, hair color, hair thickness and density, accessories (including clothes, make-ups, glasses, and head/face wear), cameras and environments, to encourage the network to learn a robust embedding. Figure 1 shows examples of synthetic face images in this new dataset. We generated 1.22M images, but in practice the number of identities and images you can generate with synthetics pipeline is only limited by the cost of generating and storing these images.\n\nDigital synthetic faces can solve the aforementioned problems associated with the real face datasets. Firstly, the generated faces are free of label noise. Secondly, the bias in lighting, make-up and skin color can be reduced as we have full control over those attributes. Most importantly, the face generation pipeline does not rely on any privacy-sensitive data obtained without consent. This is a critical difference from the GAN-generated synthetic faces; face GANs rely (either directly or indirectly) on large-scale real face datasets to train some components of their pipeline, leaving unresolved ethical problems. For ex-ample, a recent method called SynFace [28] was trained on synthetic faces generated using DiscoFaceGAN [9]. While the generated face images are free of label noise, millions of real face images were used for training DiscoFaceGAN. The GANs may also inherit any bias that exists in the real face images used to train them. For our dataset, only 511 face scans, obtained with consent, were used to build a parametric model of face geometry and texture library [36]. From this limited source data, we can generate infinite number of identities, making our approach easily scalable.\n\nOur contributions can be summarized as below:\n\n\u2022 We release a new large-scale synthetic dataset for face recognition that is free from privacy violations and lack of consent. To the best of our knowledge, our dataset, containing 1.22M images of 110K identities, is the largest public synthetic dataset for face recognition.\n\n\u2022 Compared to SynFace [28], which is trained on GANgenerated faces, we reduce the error rate on LFW by 52.5% (accuracy from 91.93% to 96.17%). For five popular benchmarks [14,30,41,25,42], the average error rate is reduced by 46.0% (accuracy from 74.75% to 86.37%).\n\n\u2022 We demonstrate how the proposed synthetic dataset can be used in conjunction with a small number of real face images to substantially improve the accuracy. This simulates a scenario where a small number of curated (i.e., no label noise and reduced bias) real face images are collected with consent. By fine-tuning our network with only 120K real face images (i.e., 2% of the commonly-used MS1MV2 dataset [8]), we achieve 99.33% accuracy on LFW and 93.61% on average across the five benchmarks, which is comparable to the methods trained on millions of real face images.\n\n\u2022 Having full control over the rendering pipeline, we perform extensive experiments to study how each attribute (e.g., variation in facial pose, accessories and textures) affects the face recognition accuracy.\n\n\nRelated Work\n\nFace recognition datasets with real face images. Major tech companies can utilize private data to train their face recognition models. Google used 100M-200M images of 8M identities to train FaceNet [29], and Facebook used 500M images of 10M identities [31]. It is challenging to construct datasets of comparable size using face images that are publicly available. Public datasets generally rely on celebrity images [14,39,12,43] or web images that are posted with Creative Commons license [17,26]. As discussed in section 1, such datasets have ethical issues and suffer from label noise and data bias. Synthetic faces generated using deep generative models.\n\nDeep generative models such as GANs [11] can produce photo-realistic images and have been used to generate synthetic data to train face recognition [32,28]. While traditional generators (e.g., [16]) generate a face image from a single latent vector that changes both the identity and its appearance, DiscoFaceGAN [9] learned disentangled latent representations for identity, pose, expression and illumination. SynFace [28] used DiscoFaceGAN to generate a synthetic dataset for face recognition, consisting of 10K identities and 500K images. SynFace achieved 91.93% accuracy on LFW dataset [14], and by mixing the synthetic dataset with 2K real identities (20 images each), the accuracy was pushed up to 97.23%. However, their performance is poor for large-pose-variation datasets (e.g., 75.03% on CFP-FP [30] and 70.43% on CPLFW [41]). This is mainly because it is challenging to train a 2D GAN to produce images that preserve 3D geometric consistency [10]. Synthetic faces generated using 3D parametric models. Classical 3D parametric face models such as morphable models [5] explicitly model the identity independently from other parameters which makes them well suited for generating face recognition datasets. However, previous results obtained with this kind of synthetic images have shown limited performance [20,19] unless combined with a large number of real images. This can be due to the lack of realism and variability in the models that have been used to generate the faces. Wood et al. [36] introduced a pipeline for generating and rendering diverse and photo-realistic 3D face models. A generative face model, learned from the 3D scans of 511 individuals, is used to generate a random 3D face. The face is then combined with artist-created assets (e.g., texture, hair, accessories) and is rendered under a random environment (simulated with HDRIs -high dynamic range images). The rendered synthetic face images (and the corresponding autogenerated ground truth annotations) were used to learn various face analysis tasks such as face parsing [36], landmark localization [36,37] and face reconstruction [37], demonstrating state-of-the-art performance. In this paper, we aim to demonstrate that such photo-realistic rendered synthetic faces can be used to tackle face recognition.\n\nAccessory #1 Accessory #2 Accessory #3 Accessory #4 Figure 2. Each row shows the same identity rendered with different accessory setups. Accessories include clothes, glasses, makeup (e.g., eyeshadow and eyeliner), face-wear and head-wear. The color, density and thickness of facial and head hair are also randomized. The hair style is modified only when the sampled accessory conflicts with the original hair style.\n\n\nRandomize\n\n\nColor Density Thickness\n\nRandomize Color Density Thickness + Style Figure 3. Randomizing the hair style makes the problem unnecessarily difficult (see the bottom row), as most people maintain similar hair styles. Therefore, we only randomize the color, density and thickness of the hair as shown in the top row (the hair is also randomly flipped horizontally).\n\n\nDigital Faces for Face Recognition\n\nThis section explains how the proposed dataset is generated. We first explain how digital faces are controlled, rendered and aligned to create the dataset (subsection 3.1). After providing the dataset statistics (subsection 3.2), we introduce the data augmentation details which help in minimizing the synthetic-to-real domain-gap (subsection 3.3).\n\n\nFace Rendering\n\nWe build upon the face generation and rendering pipeline introduced by Wood et al. [36]. In this section, we explain the modifications we made to the original pipeline to create a large-scale dataset for face recognition.\n\nWe define identity as a unique combination of facial geometry, texture (albedo and displacement), eye color and hair style. For each identity, we render a number of im- ages where all other parameters are varied to encourage the network to learn robust embeddings. While hair style can change for an individual, most people maintain similar hair style (for both facial and head hair) which makes hair style an important cue for the person's identity. Consequently, for the same identity, we randomize only the color, density and thickness of the hair (see Figure 3 for examples), and the hair style is only changed when the added head-wear is not compatible with the original hair style to avoid intersection (e.g., third image of top row in Figure 2). For sampling facial geometry, texture and eye color we follow [36].\n\nFor a given identity, we sample different accessories including clothing, make-up, glasses, face-wear (e.g., face masks) and head-wear (e.g., hats). After selecting the clothing randomly from the digital wardrobe, other accessories are added with probability p = {0.15, 0.15, 0.01, 0.15} respectively. We also add hands and secondary faces with a small probability (p = 0.01) to simulate the case when (1) the face is occluded by hands and when (2) there are multiple faces in the image. Figure 2 shows examples of the sampled identities rendered with different sets of accessories.\n\nFor each accessory setup, we vary the pose, expression, camera and environment (lighting and background) to render multiple images. The camera is rotated around the face, both horizontally and vertically. Horizontal angle is sampled from a truncated zero-mean normal distribution with support \u03b8 hori \u2208 [\u221290 \u2022 , 90 \u2022 ]. The variance is set such that the probability density p(\u03b8 hori = 90 \u2022 ) equals\nto 10 \u22123 \u00d7 p(\u03b8 hori = 0 \u2022 ). Vertical angle is sampled from a similar truncated normal distribution with support \u03b8 vert \u2208 [\u221230 \u2022 , 30 \u2022 ] and p(\u03b8 vert = 30 \u2022 ) = 10 \u22123 \u00d7 p(\u03b8 vert = 0 \u2022 ).\nThis allows us to render a wide range of poses while making sure that frontal views are rendered more often. Lastly, the face is randomly translated within the viewing frustum to add additional perspective distortion. For pose, expression, and environment sampling, we follow [36]. Figure 4 shows the impact of varying the pose, expression, environment and camera for the same identity and accessory setup. Face alignment. The input to the face embedding network should be an aligned crop around the face. Instead of de-\n\n\nRendered Image\n\nLandmarks Aligned Image Rendered Image Landmarks Aligned Image Figure 5. For synthetic faces, it is trivial to extract the locations of ground-truth facial landmarks (e.g., eyes, nose-tip and mouth corners) and align the crop around the face. This enables robust face alignment, even when some of the landmarks are not visible.\n\ntecting facial landmarks using pre-trained DNNs (such as MTCNN [40] and RetinaFace [7]), we align the faces using the ground truth landmarks (see Figure 5), which enable robust alignment even when some landmarks are not visible.\n\nLimitations. The face generation pipeline [36] we build upon has a number of limitations resulting in domain-gap to real face images. Particularly relevant to face recognition is that we cannot generate the same person at different ages. While we simulate aging to some extent by randomizing the color, density and thickness of the hair (as hair typically becomes grayer, sparser and thinner during aging), more work should be done to faithfully simulate aging. Lack of coverage (e.g., no jewelry and tattoos) may also mean that the distribution of the synthetic data does not match reality.\n\n\nDataset Statistics\n\nThe proposed dataset consists of two parts. The first part contains 720K images with 10K identities. For each identity, 4 different sets of accessories are sampled and 18 images are rendered for each set (i.e., 72 images-peridentity). Since many views of the same face are available, the network can learn embedding that is robust to the changes in accessories, camera, pose, expression, and environment. The second part contains 500K images with 100K identities. For each identity, only one set of accessories is sampled and only 5 images are rendered. This part was added to substantially increase the total number of identities with small rendering cost. Ensuring sufficient number of identities is important since the network should learn to distinguish between similar-looking faces of different identities. We show in the experiments that mixing the two parts leads to better accuracy than using one of them (Table 3).\n\n\nData Augmentation\n\nThe quality of in-the-wild face images can vary significantly. Certain parts of the face may be occluded, and the images are subject to distortion and noise that are specific to each camera. As our synthetic faces are rendered with controlled quality using a perfect pinhole camera, aggressive data augmentation is needed to reduce the synthetic-to-real domain-gap. We first apply random horizontal flipping and cropping, following [18]. Then, we apply two sets of augmentations -appearance and warping. Figure 6 shows train-  Figure 6. Synthetic face images at different stages of data augmentation. Aggressive augmentation helps to simulate effects such as motion blur and distortion common in real-world images and thus improve the robustness of DNNs trained on synthetic images.\n\ning images with these augmentations. Note that we apply the data augmentation on-the-fly during training, i.e., each epoch sees different random augmentations. For each type of augmentation, we indicate its probability p to be applied on a sample image. Appearance augmentation. We apply random Gaussian blur (p = 0.05) and Gaussian noise (p = 0.035). By applying the Gaussian blur along a random direction using an anisotropic covariance, we also simulate motion blur (p = 0.05). Brightness, contrast, hue and saturation are randomized with p = {0.15, 0.3, 0.1, 0.1}. Images are converted into grayscale with p = 0.01. Lastly, the image quality is randomized by downsampling-and-upsampling (p = 0.01) and JPEG compression (p = 0.05). Warping augmentation. Warping is performed by randomly shifting the four corners of the image. Firstly, the aspect ratio is randomized with p = 0.1. Then, all images undergo random scaling, rotation and shift. Lastly, the four corners are shifted differently for additional distortion.\n\n\nExperimental Setup\n\nImplementation details. Synthetic faces are rendered using Cycles renderer [1], with 256 samples per pixel. The rendering of the full dataset took approximately 10 days, using 300 NVIDIA M60 GPUs. The images are rendered at 256\u00d7256 resolution, and the aligned crop around the face is resized into 112\u00d7112. We use ResNet-50 [13] backbone for the experiments in subsection 5.1, 5.2 and 5.3. For comparison against the state-of-the-art methods in subsection 5.4, we use their encoder architecture to ensure fair comparison. For all experiments, the networks are implemented with Py-Torch [27] and are trained for 40 epochs using SGD. The batch size is set to 256 and the networks are trained on four NVIDIA P100 GPUs. We follow the learning rate scheduling of [28], and use the training loss from [18]. Note that all networks are trained from scratch (not pre-trained on, e.g., ImageNet [6]), to make sure that no real images are used. Evaluation protocol. Following state-of-the-art methods [15,21,24,22,18], we report the face verification accuracy on five benchmark datasets -LFW [14], CFP-FP [30], CPLFW [41], AgeDB [25] and CALFW [42]. LFW contains 6,000 pairs of in-the-wild face images. CFP-FP and CPLFW have larger pose variation (CFP-FP specifically compares frontal views to profile views). AgeDB and CALFW have larger age variation.\n\n\nExperiments\n\nWe run a series of experiments to demonstrate the usefulness of the proposed dataset. Subsection 5.1 compares different data augmentations. In subsection 5.2, we train the network on various different subsets of the full dataset to understand how each attribute sampling in rendering affects the accuracy. In subsection 5.3, we show that our synthetic faces can be used in conjunction with a small number of real faces to substantially improve the accuracy. Lastly, we provide comparison against the state-of-the-art methods in subsection 5.4.\n\n\nData Augmentation\n\nIn subsection 3.3, we introduced appearance and warping augmentations. As shown in Table 1, both lead to significant improvement across all datasets. We also compare against the augmentation used by AdaFace [18], which includes horizontal flipping, cropping and mild color augmentation. For our synthetic face images which are free of imperfection, more aggressive data augmentation is needed to reduce the domain-gap. Notice that the warping augmentation improves the performance especially for the largepose-variation datasets (CFP-FP and CPLFW).\n\n\nDataset Composition\n\nHaving full control over the rendering pipeline, we can create a dataset with desired statistics to study how each attribute affects the face recognition accuracy. The results are provided in Table 2. Accessory sampling. For 10K synthetic identities, we sampled 4 accessory setups and rendered 18 images for each setup (i.e., 720K images in total). These 18 images have variations in pose, expression, camera, and environment (see Figure 4). From this, we can create a subset of 180K images by selecting 18 images per ID with fixed accessory. Similarly, we can select 18 images randomly so that images with different accessories are used during training. When randomizing the accessories, we also randomized the color, thickness and density of the hair to simulate aging (Figure 3). As a result, the accuracy is improved especially for the large-age-variation datasets (AgeDB and CALFW). For CFP-FP and CPLFW, which has smaller age gap (i.e.,  Table 2, increasing the variation in horizontal and vertical angles improved the accuracy especially for the large-posevariation datasets (CFP-FP and CPLFW). For AgeDB and CALFW, which consists mainly of frontal faces, the accuracy was similar. Texture sampling. While we can create infinite number of unique facial geometries, the texture is sampled from a li-brary built from 208 scans of real human faces (obtained with consent). Since we generated 110K identities in total, many of them share the same texture. To see how the number of textures affects the accuracy, we created a dataset of 1200 identities with N textures, by generating 1200/N identities for each texture. As shown in Row 6-9 of Table 2, increasing the number of textures did not lead to a meaningful improvement in the accuracy. This is contrary to the intuition that small number of textures and lack of texture generative model are limitations of synthetic data for face recognition. We believe that the appearance variability is a combination of geometry, texture, hair, accessories, environment and image quality. In Figure 7, we show that (1) the texture library already covers diverse skin color and age, (2) an arbitrary number of unique identities can be generated with the same texture, and (3) skin appearance is greatly affected by the environment. Also, the image quality for face recognition task is in general limited due to low resolution and data augmentation. Thus, the contribution of texture variation is likely less important than that of geometry and environment. Balance between # IDs and # images/ID. Ensuring large number of IDs is important for learning diverse discriminative embedding. On the other hand, large number of images per ID (referred to as images/ID) is needed for learning robust embedding (that is not affected by the changes in pose, accessories, expressions, camera and environment). Mixing the two datasets with different number of images/ID can be considered as an efficient way of getting the best of both. This also simulates the long-tailed distribution of real face datasets (i.e., most identities have small number of images).\n\nThe result in Table 3 shows that mixing the two datasets leads to better accuracy than using one of them.   Table 3. Number of IDs and number of images/ID should both be high to learn diverse and robust embedding. Mixing two datasets with large/small number of images/ID can be an efficient way of satisfying both. The overall accuracy becomes higher than relying on one of the two datasets.\n\n\nMixing with Real Faces\n\nThe main problems associated with large-scale real face datasets are ethical issues, label noise and data bias. In this study, we assume a scenario where a small number of real face images are collected with consent. For small number of images, it would also be possible to remove (or reduce) the label noise and data bias.\n\nFor synthetic data, we used 10K identities with 72 images per identity. For real face images, we varied the number of identities from 200 to 2000, with 20 images sampled for each identity (the identities and images were sampled randomly from CASIA-WebFace [39]).\n\nWe first tried training only on the synthetic data. Secondly, we tried training only on the real data. Then, we explored two different strategies for using both real and synthetic images: (1) dataset mixing and (2) pre-training on synthetic data and fine-tuning on the real data. For finetuning, we reduced the learning rate by 1/10 for the prediction head, and 1/100 for the encoder to avoid catastrophic forgetting. The results are provided in Figure 8.\n\nWhen the network is trained only on a small number of real face images, the accuracy is worse than the network trained only on our synthetic dataset. Both dataset mixing and pre-training can lead to significantly higher accuracy, especially for the large-pose-variation datasets (CFP-FP and CPLFW). Compared to dataset mixing, pre-training on synthetics followed by fine-tuning on real images led to better accuracy. This can be due to the imbalance between the number of images (we use 720K synthetic images, and a lot fewer real images).\n\n\nComparison to the State-of-the-Art\n\nComparison to SynFace. SynFace [28] is the current state-of-the-art for face recognition model trained on synthetic faces. They used DiscoFaceGAN [9] to generate 500K synthetic faces (10K identities & 50 images/ID). To ensure a fair comparison, we trained the same encoder (LResNet50E-IR) with same number of images. We also trained using our full dataset (1.22M images). The results are provided in Row 1-3 of Table 4. In the second scenario, we additionally used 40K real face images from CASIA-WebFace [39]. While SynFace mixed their synthetic dataset with the real faces, we instead adopted the two-stage method of pre-training and fine-tuning as discussed in subsection 5.3. The results are provided in Row 4-6 of Table 4. For both scenarios, we significantly outperform SynFace across all datasets. This suggests that our rendered synthetic faces are better than GAN-generated faces for learning face recognition. While GANs like [9] can generate realistic face images, the data they generate is not ideal for face recognition, due to following reasons: (1) Identity change. While [9] is encouraged to preserve the identity when changing other latent variables, there is no guarantee that the identity will be preserved during data generation. (2) Geometric inconsistency. As pointed out by [10], the images generated  Table 5. Comparison to the state-of-the-art methods trained on real face images (MS1MV2 [8]). We use the same backbone (ResNet100) for fair comparison. By only using 120K real face images (2% of MS1MV2 [8]), we achieve accuracy that is comparable to the methods trained on millions of real face images. Since we do not model aging explicitly, our accuracy is worse for large-age-variation datasets (AgeDB and CALFW). Avg \u2020 shows average of LFW, CFP-FP and CPLFW, and on these, we outperform [35] and are similar to [23].\n\nby [9] for same identity and different poses lack 3D consistency.\n\n(3) Lack of accessory change.\n\n[9] cannot randomize accessories. (4) Unresolved ethical concerns. Training the GAN model itself requires large-scale real face dataset. For example, 70K images are used to train [9]. To learn to preserve identity, they also used a perceptual loss based on [38], which is trained on 3M real face images. In Row 2 and 3 of Table 4, we increase our synthetics dataset size from 500K to 1.22M, and achieve better accuracy. This indicates that the accuracy may not have converged yet and could be improved further by generating more synthetic data. Comparison to methods trained on real faces. Lastly, we compare the accuracy against the methods that are trained on real face images. In Table 5, we provide the accuracy of six methods that use ResNet100 as the embedding network and MS1MV2 [8] as the training data. We trained the same architecture on our synthetic dataset (Row 1). We also tried fine-tuning the network on a small number of real face images (Row 2). When trained only with the proposed synthetic dataset, the network can achieve 96.17% on LFW. For LFW, CFP-FP and CPLFW (excluding the highage-variation datasets), the average accuracy is 89.40%. By fine-tuning the network on just 120K images (2.0% of MS1MV2), the accuracy becomes comparable to the methods trained on MS1MV2 (e.g., average accuracy on LFW, CFP-FP and CPLFW becomes higher than that of SV-AM-Softmax [35]).\n\nThe performance of our method on AgeDB [25] and CALFW [42] has a significantly larger gap than for the other datasets evaluated. This is expected given the lack of aging simulation in our synthetic data. We suspect that other causes of domain-gap, as described at the end of subsection 3.1, are the primary reason for the remaining performance gap for other evaluation datasets. Reducing this domain-gap remains an area of ongoing work for our synthetic data and is likely to result in improved performance for all downstream tasks, including face recognition. We leave this as future work.\n\n\nConclusion\n\nIn this paper, we introduced a new large-scale synthetic dataset for face recognition by rendering digital faces using a graphics pipeline. We ran extensive experiments to study how data augmentation and various other attributes affect the accuracy. We demonstrated that our synthetic faces are significantly better than the GAN-generated faces for learning face recognition. With a small number of real face images, we achieve accuracy that is comparable to the methods trained on millions of web-crawled face images. We hope this dataset would be a meaningful step towards developing socially responsible face recognition models that do not depend on privacy-sensitive data obtained without consent.\n\nFigure 1 .\n1Examples of synthetic face images in our dataset. Our dataset captures a wide variety of facial geometry, pose, textures, expressions, accessories and environments. son; and (3) the same person that goes by different names labeled as different persons.\n\nFigure 4 .\n4Examples of images rendered for the same identity and accessory setup. The same face can look very different depending on the pose, expression, environment (lighting and background) and camera, encouraging the network to learn robust embedding.\n\nFigure 8 .\n8Comparison between training with our synthetic data only (black dashed line), with small amount of real data only (red line), with the mixture of the two (blue line), and pre-training on synthetics and fine-tuning with the real data (black line). The number of real identities varies from 200 to 2000, and 20 images are sampled for each identity. When only a small number of real face images are available (e.g., due to ethical issues), the proposed synthetic dataset can substantially improve the accuracy.\n\n\nRaw Image+ Flip & CropRaw Image \n\nRaw Image \n+ Flip & Crop \n\n+ Appearance \n\nRaw Image \n+ Flip & Crop \n+ Appearance \n\n+ Warping \n\n\n\n\nTable 1. The proposed aggressive data augmentation significantly improves the accuracy across all datasets.Experiment \n\nMethod \nLFW CFP-FP CPLFW AgeDB CALFW Avg \n\nData augmentation \n\nNo augmentation \n88.07 70.99 \n66.73 \n60.92 \n69.23 71.19 \nAugmentation from AdaFace [18] 90.12 76.41 \n71.33 \n67.17 \n74.13 75.83 \nOurs (appearance) \n94.32 80.00 \n74.83 \n75.82 \n76.92 80.38 \nOurs (appearance + warping) \n94.55 84.86 \n77.08 \n76.97 \n77.20 82.13 \n\nExperiment \nMethod \nLFW CFP-FP CPLFW AgeDB CALFW Avg \n\nAccessory sampling \nFix accessory \n93.50 82.16 \n75.75 \n73.05 \n73.83 79.66 \nRandomize accessory \n94.23 82.04 \n75.18 \n76.43 \n77.22 81.02 \n\nPose sampling \n\nMinimize horizontal angle 93.42 67.19 \n66.48 \n76.78 \n77.22 76.22 \nMinimize vertical angle \n93.67 81.13 \n74.57 \n76.57 \n76.68 80.52 \nRandom pose \n94.23 82.04 \n75.18 \n76.43 \n77.22 81.02 \n\nTexture sampling \n\n50 \n89.63 75.04 \n69.72 \n69.47 \n70.10 74.79 \n\n(# textures to select from) \n\n100 \n90.83 74.84 \n70.30 \n70.62 \n70.57 75.43 \n150 \n90.03 73.01 \n69.63 \n71.48 \n70.27 74.89 \n200 \n89.82 73.37 \n69.37 \n71.45 \n70.50 74.90 \n\nTable 2. Dataset composition experiments to study how the sampling of each attribute affects the accuracy. \n\nFigure 7. Left: 40 textures selected randomly from the texture li-\nbrary. The library covers diverse skin color and age. Right top \nrow: various identities (facial geometry and hair style) sampled \nwith the same texture. Right bottom row: same identity with the \nsame texture under different environments (taken from [36]). With \nlarge variations in geometry, hair style and environments, rich ap-\npearance variation could be achieved with limited textures. \n\npositive pairs capture the identity at similar age), fixing the \naccessory and hair leads to slightly better accuracy. \nPose sampling. Similar to the accessory sampling, we \ncan select 18 images for each of the 10K identities by se-\nlecting the ones with the smallest horizontal/vertical an-\ngles. Then, we can compare them against the 18 images \nselected randomly. For the randomly selected images, the \nstandard deviation in horizontal and vertical angles were \n(\u03c3 hori , \u03c3 vert ) = (24.13 \u2022 , 9.20 \u2022 ). For the images with the \nsmallest horizontal/vertical angles, they were (4.71 \u2022 , 8.06 \u2022 ) \nand (22.02 \u2022 , 1.72 \u2022 ) respectively. As shown in Row 3-5 in \n\n\nMethod #\nMethodSynthetic images # Real images LFW CFP-FP CPLFW AgeDB CALFW Avg Avg \u2020 (# IDs \u00d7 # imgs/ID) (# IDs \u00d7 # imgs/ID)Table 4. Comparison to SynFace using the same encoder architecture (LResNet50E-IR [28]). For both scenarios -training only on synthetic faces & using a small number of real faces -we significantly outperform SynFace across all datasets. Avg \u2020 shows average of LFW, CFP-FP and CPLFW, excluding the large-age-variation datasets. Method # Synthetic images # Real images LFW CFP-FP CPLFW AgeDB CALFW Avg Avg \u2020SynFace [28] \n\n500K (10K\u00d750) \n\n0 \n91.93 75.03 \n70.43 \n61.63 \n74.73 74.75 79.13 \nOurs \n\n500K (10K\u00d750) \n\n0 \n95.40 87.40 \n78.87 \n76.97 \n78.62 83.45 87.22 \nOurs \n\n1.22M (10K\u00d772+100K\u00d75) \n\n0 \n95.82 88.77 \n81.62 \n79.72 \n80.70 85.32 88.74 \n\nSynFace [28] \n\n500K (10K\u00d750) \n40K (2K\u00d720) \n\n97.23 87.68 \n80.32 \n81.42 \n85.08 86.35 88.41 \nOurs \n\n500K (10K\u00d750) \n40K (2K\u00d720) \n\n99.05 94.01 \n87.27 \n89.77 \n90.08 92.04 93.44 \nOurs \n\n1.22M (10K\u00d772+100K\u00d75) \n40K (2K\u00d720) \n\n99.17 94.63 \n88.10 \n90.50 \n90.97 92.67 93.97 \n\nOurs (SX best) \n1.22M \n0 \n96.17 89.81 \n82.23 \n81.10 \n82.55 86.37 89.40 \nOurs (SX+Real best) \n1.22M \n120K \n99.33 95.93 \n89.47 \n91.55 \n91.78 93.61 94.91 \nSV-AM-Softmax [35] \n\n0 \n5.8M \n\n99.50 95.10 \n89.48 \n95.68 \n94.38 94.83 94.69 \nSphereFace [23] \n99.67 96.84 \n91.27 \n97.05 \n95.58 96.08 95.93 \nCosFace [33] \n99.78 98.26 \n92.18 \n98.17 \n96.18 96.91 96.74 \nArcFace [8] \n99.81 98.40 \n92.72 \n98.05 \n95.96 96.99 96.98 \nMagFace [24] \n99.83 98.46 \n92.87 \n98.17 \n96.15 97.10 97.05 \nAdaFace [18] \n99.82 98.49 \n93.53 \n98.05 \n96.08 97.19 97.28 \n\n\nDigiFace-1M dataset can be downloaded from https://github. com/microsoft/DigiFace1M\n\nBlender foundation. cycles renderer. Blender foundation. cycles renderer. https://www. cycles-renderer.org/. Accessed: 2022-10-03.\n\nThe ethical questions that haunt facial-recognition research. d41586-020-03187-3. Accessed: 2022-10-03The ethical questions that haunt facial-recognition re- search. https://www.nature.com/articles/ d41586-020-03187-3. Accessed: 2022-10-03.\n\n. Megapixels, Megapixels. https://ahprojects.com/ megapixels-glassroom/. Accessed: 2022-10-03.\n\nA morphable model for the synthesis of 3d faces. Volker Blanz, Thomas Vetter, Proc. SIGGRAPH. SIGGRAPHVolker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In Proc. SIGGRAPH, 1999.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.\n\nRetinaface: Single-shot multi-level face localisation in the wild. Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, Stefanos Zafeiriou, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2020Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-level face localisation in the wild. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n\nArcface: Additive angular margin loss for deep face recognition. Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nDisentangled and controllable face image generation via 3d imitative-contrastive learning. Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, Xin Tong, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2020Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong. Disentangled and controllable face image genera- tion via 3d imitative-contrastive learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n\nGram: Generative radiance manifolds for 3d-aware image generation. Yu Deng, Jiaolong Yang, Jianfeng Xiang, Xin Tong, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2022Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong. Gram: Generative radiance manifolds for 3d-aware image generation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems (NeurIPS). Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NeurIPS), 2014.\n\nMs-celeb-1m: A dataset and benchmark for large-scale face recognition. Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao, Proc. European Conference on Computer Vision (ECCV). European Conference on Computer Vision (ECCV)Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In Proc. European Conference on Computer Vision (ECCV), 2016.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nLabeled faces in the wild: A database for studying face recognition in unconstrained environments. B Gary, Marwan Huang, Tamara Mattar, Eric Berg, Learned-Miller, Workshop on faces in 'Real-Life' Images: detection, alignment, and recognition. Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. In Workshop on faces in 'Real-Life' Images: detection, alignment, and recognition, 2008.\n\nCurricularface: adaptive curriculum learning loss for deep face recognition. Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, Feiyue Huang, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2020Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: adaptive curriculum learning loss for deep face recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n\nProgressive growing of gans for improved quality, stability, and variation. Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, International Conference on Learning Representations (ICLR). Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehti- nen. Progressive growing of gans for improved quality, sta- bility, and variation. In International Conference on Learning Representations (ICLR), 2018.\n\nThe megaface benchmark: 1 million faces for recognition at scale. Ira Kemelmacher-Shlizerman, M Steven, Daniel Seitz, Evan Miller, Brossard, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The megaface benchmark: 1 mil- lion faces for recognition at scale. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nAdaface: Quality adaptive margin for face recognition. Minchul Kim, Xiaoming Jain, Liu, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2022Minchul Kim, Anil K Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n\nCan synthetic faces undo the damage of dataset bias to face recognition and facial landmark detection?. Adam Kortylewski, Bernhard Egger, Andreas Morel-Forster, Andreas Schneider, Thomas Gerig, Clemens Blumer, Corius Reyneke, Thomas Vetter, arXiv:1811.08565arXiv preprintAdam Kortylewski, Bernhard Egger, Andreas Morel-Forster, Andreas Schneider, Thomas Gerig, Clemens Blumer, Corius Reyneke, and Thomas Vetter. Can synthetic faces undo the damage of dataset bias to face recognition and facial land- mark detection? arXiv preprint arXiv:1811.08565, 2018.\n\nAnalyzing and reducing the damage of dataset bias to face recognition with synthetic data. Adam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig, Andreas Morel-Forster, Thomas Vetter, Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops. IEEE Conference on Computer Vision and Pattern Recognition WorkshopsAdam Kortylewski, Bernhard Egger, Andreas Schneider, Thomas Gerig, Andreas Morel-Forster, and Thomas Vetter. Analyzing and reducing the damage of dataset bias to face recognition with synthetic data. In Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2019.\n\nDynamic class queue for large scale face recognition in the wild. Bi Li, Teng Xi, Gang Zhang, Haocheng Feng, Junyu Han, Jingtuo Liu, Errui Ding, Wenyu Liu, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2021Bi Li, Teng Xi, Gang Zhang, Haocheng Feng, Junyu Han, Jingtuo Liu, Errui Ding, and Wenyu Liu. Dynamic class queue for large scale face recognition in the wild. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nSpherical confidence learning for face recognition. Shen Li, Jianqing Xu, Xiaqing Xu, Pengcheng Shen, Shaoxin Li, Bryan Hooi, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2021Shen Li, Jianqing Xu, Xiaqing Xu, Pengcheng Shen, Shaoxin Li, and Bryan Hooi. Spherical confidence learning for face recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nSphereface: Deep hypersphere embedding for face recognition. Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, Le Song, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR. IEEE Conference on Computer Vision and Pattern Recognition (CVPRWeiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nMagface: A universal representation for face recognition and quality assessment. Qiang Meng, Shichao Zhao, Zhida Huang, Feng Zhou, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2021Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou. Magface: A universal representation for face recognition and quality assessment. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nAgedb: the first manually collected, in-the-wild age database. Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, Stefanos Zafeiriou, Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops. IEEE Conference on Computer Vision and Pattern Recognition WorkshopsStylianos Moschoglou, Athanasios Papaioannou, Chris- tos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017.\n\nLevel playing field for million scale face recognition. Aaron Nech, Ira Kemelmacher-Shlizerman, Proc. IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionAaron Nech and Ira Kemelmacher-Shlizerman. Level play- ing field for million scale face recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Advances in Neural Information Processing Systems (NeurIPS). Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zem- ing Lin, Natalia Gimelshein, Luca Antiga, et al. Py- torch: An imperative style, high-performance deep learn- ing library. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nSynface: Face recognition with synthetic data. Haibo Qiu, Baosheng Yu, Dihong Gong, Zhifeng Li, Wei Liu, Dacheng Tao, Proc. IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)2021Haibo Qiu, Baosheng Yu, Dihong Gong, Zhifeng Li, Wei Liu, and Dacheng Tao. Synface: Face recognition with synthetic data. In Proc. IEEE International Conference on Computer Vision (ICCV), 2021.\n\nFacenet: A unified embedding for face recognition and clustering. Florian Schroff, Dmitry Kalenichenko, James Philbin, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clus- tering. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\nFrontal to profile face verification in the wild. Soumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, M Vishal, Rama Patel, David W Chellappa, Jacobs, Proc. IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE Winter Conference on Applications of Computer Vision (WACV)Soumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, Vishal M Patel, Rama Chellappa, and David W Jacobs. Frontal to profile face verification in the wild. In Proc. IEEE Winter Conference on Applications of Computer Vision (WACV), 2016.\n\nWeb-scale training for face identification. Yaniv Taigman, Ming Yang, Marc&apos;aurelio Ranzato, Lior Wolf, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, and Lior Wolf. Web-scale training for face identification. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\nGenerating photo-realistic training data to improve face recognition accuracy. Li Daniel S\u00e1ez Trigueros, Margaret Meng, Hartnett, Neural Networks. 134Daniel S\u00e1ez Trigueros, Li Meng, and Margaret Hartnett. Generating photo-realistic training data to improve face recognition accuracy. Neural Networks, 134:86-94, 2021.\n\nCosface: Large margin cosine loss for deep face recognition. Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, Wei Liu, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cos- face: Large margin cosine loss for deep face recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nRacial faces in the wild: Reducing racial bias by information maximization adaptation network. Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, Yaohai Huang, Proc. IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial faces in the wild: Reducing racial bias by information maximization adaptation network. In Proc. IEEE International Conference on Computer Vision (ICCV), 2019.\n\nSupport vector guided softmax loss for face recognition. Xiaobo Wang, Shuo Wang, Shifeng Zhang, Tianyu Fu, Hailin Shi, Tao Mei, arXiv:1812.11317arXiv preprintXiaobo Wang, Shuo Wang, Shifeng Zhang, Tianyu Fu, Hailin Shi, and Tao Mei. Support vector guided softmax loss for face recognition. arXiv preprint arXiv:1812.11317, 2018.\n\nFake it till you make it: face analysis in the wild using synthetic data alone. Erroll Wood, Tadas Baltru\u0161aitis, Charlie Hewitt, Sebastian Dziadzio, J Thomas, Jamie Cashman, Shotton, Proc. IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)2021Erroll Wood, Tadas Baltru\u0161aitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie Shotton. Fake it till you make it: face analysis in the wild using synthetic data alone. In Proc. IEEE International Conference on Computer Vision (ICCV), 2021.\n\nIvan Stojiljkovic, et al. 3d face reconstruction with dense landmarks. Erroll Wood, Tadas Baltru\u0161aitis, Charlie Hewitt, Matthew Johnson, Jingjing Shen, Nikola Milosavljevic, Daniel Wilde, Stephan Garbin, Toby Sharp, Proc. European Conference on Computer Vision (ECCV). European Conference on Computer Vision (ECCV)2022Erroll Wood, Tadas Baltru\u0161aitis, Charlie Hewitt, Matthew Johnson, Jingjing Shen, Nikola Milosavljevic, Daniel Wilde, Stephan Garbin, Toby Sharp, Ivan Stojiljkovic, et al. 3d face reconstruction with dense landmarks. In Proc. European Conference on Computer Vision (ECCV), 2022.\n\nNeural aggregation network for video face recognition. Jiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong Li, Gang Hua, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR. IEEE Conference on Computer Vision and Pattern Recognition (CVPRJiaolong Yang, Peiran Ren, Dongqing Zhang, Dong Chen, Fang Wen, Hongdong Li, and Gang Hua. Neural aggre- gation network for video face recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\n\nDong Yi, Zhen Lei, Shengcai Liao, Stan Z Li, arXiv:1411.7923Learning face representation from scratch. arXiv preprintDong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv preprint arXiv:1411.7923, 2014.\n\nJoint face detection and alignment using multitask cascaded convolutional networks. Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, Yu Qiao, IEEE signal processing letters. 2310Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. IEEE signal processing letters, 23(10):1499-1503, 2016.\n\nCross-pose lfw: A database for studying cross-pose face recognition in unconstrained environments. Tianyue Zheng, Weihong Deng, Tech. Rep. 57Beijing University of Posts and TelecommunicationsTianyue Zheng and Weihong Deng. Cross-pose lfw: A database for studying cross-pose face recognition in un- constrained environments. Beijing University of Posts and Telecommunications, Tech. Rep, 5:7, 2018.\n\nCross-age lfw: A database for studying cross-age face recognition in unconstrained environments. Tianyue Zheng, Weihong Deng, Jiani Hu, arXiv:1708.08197arXiv preprintTianyue Zheng, Weihong Deng, and Jiani Hu. Cross-age lfw: A database for studying cross-age face recognition in un- constrained environments. arXiv preprint arXiv:1708.08197, 2017.\n\nWebface260m: A benchmark unveiling the power of million-scale deep face recognition. Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Dalong Du, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2021Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Da- long Du, et al. Webface260m: A benchmark unveiling the power of million-scale deep face recognition. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n", "annotations": {"author": "[{\"end\":130,\"start\":67},{\"end\":191,\"start\":131},{\"end\":261,\"start\":192},{\"end\":341,\"start\":262},{\"end\":417,\"start\":342},{\"end\":488,\"start\":418},{\"end\":565,\"start\":489},{\"end\":632,\"start\":566},{\"end\":719,\"start\":633},{\"end\":780,\"start\":720}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":76},{\"end\":140,\"start\":138},{\"end\":210,\"start\":201},{\"end\":290,\"start\":281},{\"end\":366,\"start\":357},{\"end\":437,\"start\":428},{\"end\":514,\"start\":505},{\"end\":581,\"start\":574},{\"end\":646,\"start\":642}]", "author_first_name": "[{\"end\":75,\"start\":67},{\"end\":137,\"start\":131},{\"end\":194,\"start\":192},{\"end\":200,\"start\":195},{\"end\":267,\"start\":262},{\"end\":280,\"start\":268},{\"end\":349,\"start\":342},{\"end\":356,\"start\":350},{\"end\":422,\"start\":418},{\"end\":427,\"start\":423},{\"end\":495,\"start\":489},{\"end\":504,\"start\":496},{\"end\":573,\"start\":566},{\"end\":641,\"start\":633},{\"end\":729,\"start\":720}]", "author_affiliation": "[{\"end\":129,\"start\":81},{\"end\":190,\"start\":142},{\"end\":260,\"start\":212},{\"end\":340,\"start\":292},{\"end\":416,\"start\":368},{\"end\":487,\"start\":439},{\"end\":564,\"start\":516},{\"end\":631,\"start\":583},{\"end\":718,\"start\":670},{\"end\":779,\"start\":731}]", "title": "[{\"end\":64,\"start\":1},{\"end\":844,\"start\":781}]", "venue": null, "abstract": "[{\"end\":2127,\"start\":846}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2186,\"start\":2182},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2189,\"start\":2186},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2192,\"start\":2189},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2195,\"start\":2192},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2197,\"start\":2195},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2200,\"start\":2197},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2203,\"start\":2200},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2206,\"start\":2203},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2889,\"start\":2886},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2992,\"start\":2988},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3376,\"start\":3372},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3379,\"start\":3376},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3381,\"start\":3379},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3384,\"start\":3381},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3596,\"start\":3592},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3599,\"start\":3596},{\"end\":3680,\"start\":3677},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3710,\"start\":3707},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3874,\"start\":3871},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4042,\"start\":4038},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4110,\"start\":4107},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4426,\"start\":4422},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4451,\"start\":4447},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4535,\"start\":4531},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4915,\"start\":4911},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6377,\"start\":6373},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6441,\"start\":6438},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6797,\"start\":6793},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7266,\"start\":7262},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7415,\"start\":7411},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7418,\"start\":7415},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7421,\"start\":7418},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7424,\"start\":7421},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7427,\"start\":7424},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7916,\"start\":7913},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8508,\"start\":8504},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8562,\"start\":8558},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8725,\"start\":8721},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8728,\"start\":8725},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8731,\"start\":8728},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8734,\"start\":8731},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8799,\"start\":8795},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8802,\"start\":8799},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9005,\"start\":9001},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9117,\"start\":9113},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9120,\"start\":9117},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9162,\"start\":9158},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9281,\"start\":9278},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9387,\"start\":9383},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9558,\"start\":9554},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9773,\"start\":9769},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9798,\"start\":9794},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9921,\"start\":9917},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10041,\"start\":10038},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10284,\"start\":10280},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10287,\"start\":10284},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10468,\"start\":10464},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11025,\"start\":11021},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11053,\"start\":11049},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11056,\"start\":11053},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11085,\"start\":11081},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12543,\"start\":12539},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13498,\"start\":13494},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13949,\"start\":13946},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14951,\"start\":14947},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15606,\"start\":15602},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15625,\"start\":15622},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15815,\"start\":15811},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17765,\"start\":17761},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19234,\"start\":19231},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19483,\"start\":19479},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19745,\"start\":19741},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19917,\"start\":19913},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19954,\"start\":19950},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20043,\"start\":20040},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20149,\"start\":20145},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20152,\"start\":20149},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20155,\"start\":20152},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20158,\"start\":20155},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20161,\"start\":20158},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20240,\"start\":20236},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20253,\"start\":20249},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20265,\"start\":20261},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20277,\"start\":20273},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20292,\"start\":20288},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21288,\"start\":21284},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25746,\"start\":25742},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26820,\"start\":26816},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26934,\"start\":26931},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27294,\"start\":27290},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27724,\"start\":27721},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27875,\"start\":27872},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28086,\"start\":28082},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28201,\"start\":28198},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28315,\"start\":28312},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28605,\"start\":28601},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28629,\"start\":28625},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28638,\"start\":28635},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28912,\"start\":28909},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28991,\"start\":28987},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29519,\"start\":29516},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30115,\"start\":30111},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30162,\"start\":30158},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30177,\"start\":30173}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31691,\"start\":31426},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31949,\"start\":31692},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32470,\"start\":31950},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32602,\"start\":32471},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34897,\"start\":32603},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36456,\"start\":34898}]", "paragraph": "[{\"end\":2818,\"start\":2143},{\"end\":3183,\"start\":2820},{\"end\":3875,\"start\":3185},{\"end\":4536,\"start\":3877},{\"end\":5704,\"start\":4538},{\"end\":6913,\"start\":5706},{\"end\":6960,\"start\":6915},{\"end\":7238,\"start\":6962},{\"end\":7505,\"start\":7240},{\"end\":8078,\"start\":7507},{\"end\":8289,\"start\":8080},{\"end\":8963,\"start\":8306},{\"end\":11258,\"start\":8965},{\"end\":11675,\"start\":11260},{\"end\":12050,\"start\":11715},{\"end\":12437,\"start\":12089},{\"end\":12677,\"start\":12456},{\"end\":13499,\"start\":12679},{\"end\":14083,\"start\":13501},{\"end\":14482,\"start\":14085},{\"end\":15191,\"start\":14671},{\"end\":15537,\"start\":15210},{\"end\":15767,\"start\":15539},{\"end\":16360,\"start\":15769},{\"end\":17307,\"start\":16383},{\"end\":18111,\"start\":17329},{\"end\":19133,\"start\":18113},{\"end\":20496,\"start\":19156},{\"end\":21055,\"start\":20512},{\"end\":21625,\"start\":21077},{\"end\":24741,\"start\":21649},{\"end\":25134,\"start\":24743},{\"end\":25484,\"start\":25161},{\"end\":25748,\"start\":25486},{\"end\":26205,\"start\":25750},{\"end\":26746,\"start\":26207},{\"end\":28630,\"start\":26785},{\"end\":28697,\"start\":28632},{\"end\":28728,\"start\":28699},{\"end\":30117,\"start\":28730},{\"end\":30709,\"start\":30119},{\"end\":31425,\"start\":30724}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14670,\"start\":14483}]", "table_ref": "[{\"end\":17306,\"start\":17297},{\"end\":21167,\"start\":21160},{\"end\":21848,\"start\":21841},{\"end\":22600,\"start\":22593},{\"end\":23301,\"start\":23294},{\"end\":24764,\"start\":24757},{\"end\":24858,\"start\":24851},{\"end\":27203,\"start\":27196},{\"end\":27511,\"start\":27504},{\"end\":28117,\"start\":28110},{\"end\":29059,\"start\":29052},{\"end\":29420,\"start\":29413}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2141,\"start\":2129},{\"attributes\":{\"n\":\"2.\"},\"end\":8304,\"start\":8292},{\"end\":11687,\"start\":11678},{\"end\":11713,\"start\":11690},{\"attributes\":{\"n\":\"3.\"},\"end\":12087,\"start\":12053},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12454,\"start\":12440},{\"end\":15208,\"start\":15194},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16381,\"start\":16363},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17327,\"start\":17310},{\"attributes\":{\"n\":\"4.\"},\"end\":19154,\"start\":19136},{\"attributes\":{\"n\":\"5.\"},\"end\":20510,\"start\":20499},{\"attributes\":{\"n\":\"5.1.\"},\"end\":21075,\"start\":21058},{\"attributes\":{\"n\":\"5.2.\"},\"end\":21647,\"start\":21628},{\"attributes\":{\"n\":\"5.3.\"},\"end\":25159,\"start\":25137},{\"attributes\":{\"n\":\"5.4.\"},\"end\":26783,\"start\":26749},{\"attributes\":{\"n\":\"6.\"},\"end\":30722,\"start\":30712},{\"end\":31437,\"start\":31427},{\"end\":31703,\"start\":31693},{\"end\":31961,\"start\":31951},{\"end\":34907,\"start\":34899}]", "table": "[{\"end\":32602,\"start\":32495},{\"end\":34897,\"start\":32712},{\"end\":36456,\"start\":35428}]", "figure_caption": "[{\"end\":31691,\"start\":31439},{\"end\":31949,\"start\":31705},{\"end\":32470,\"start\":31963},{\"end\":32495,\"start\":32473},{\"end\":32712,\"start\":32605},{\"end\":35428,\"start\":34914}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5454,\"start\":5446},{\"end\":11320,\"start\":11312},{\"end\":11765,\"start\":11757},{\"end\":13243,\"start\":13235},{\"end\":13429,\"start\":13421},{\"end\":13997,\"start\":13989},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14961,\"start\":14953},{\"end\":15281,\"start\":15273},{\"end\":15693,\"start\":15685},{\"end\":17841,\"start\":17833},{\"end\":17864,\"start\":17856},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22088,\"start\":22080},{\"end\":22430,\"start\":22420},{\"end\":23695,\"start\":23687},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26204,\"start\":26196}]", "bib_author_first_name": "[{\"end\":37067,\"start\":37061},{\"end\":37081,\"start\":37075},{\"end\":37277,\"start\":37274},{\"end\":37287,\"start\":37284},{\"end\":37301,\"start\":37294},{\"end\":37316,\"start\":37310},{\"end\":37324,\"start\":37321},{\"end\":37331,\"start\":37329},{\"end\":37761,\"start\":37753},{\"end\":37771,\"start\":37768},{\"end\":37786,\"start\":37777},{\"end\":37802,\"start\":37797},{\"end\":37819,\"start\":37811},{\"end\":38278,\"start\":38270},{\"end\":38288,\"start\":38285},{\"end\":38301,\"start\":38294},{\"end\":38315,\"start\":38307},{\"end\":38767,\"start\":38765},{\"end\":38782,\"start\":38774},{\"end\":38793,\"start\":38789},{\"end\":38804,\"start\":38800},{\"end\":38813,\"start\":38810},{\"end\":39266,\"start\":39264},{\"end\":39281,\"start\":39273},{\"end\":39296,\"start\":39288},{\"end\":39307,\"start\":39304},{\"end\":39692,\"start\":39689},{\"end\":39709,\"start\":39705},{\"end\":39730,\"start\":39725},{\"end\":39742,\"start\":39738},{\"end\":39752,\"start\":39747},{\"end\":39774,\"start\":39767},{\"end\":39787,\"start\":39782},{\"end\":39805,\"start\":39799},{\"end\":40182,\"start\":40175},{\"end\":40191,\"start\":40188},{\"end\":40205,\"start\":40199},{\"end\":40218,\"start\":40210},{\"end\":40231,\"start\":40223},{\"end\":40588,\"start\":40581},{\"end\":40600,\"start\":40593},{\"end\":40616,\"start\":40608},{\"end\":40626,\"start\":40622},{\"end\":41054,\"start\":41053},{\"end\":41067,\"start\":41061},{\"end\":41081,\"start\":41075},{\"end\":41094,\"start\":41090},{\"end\":41534,\"start\":41530},{\"end\":41547,\"start\":41542},{\"end\":41558,\"start\":41554},{\"end\":41572,\"start\":41564},{\"end\":41587,\"start\":41578},{\"end\":41601,\"start\":41594},{\"end\":41611,\"start\":41606},{\"end\":41622,\"start\":41616},{\"end\":42116,\"start\":42112},{\"end\":42129,\"start\":42125},{\"end\":42142,\"start\":42136},{\"end\":42156,\"start\":42150},{\"end\":42507,\"start\":42504},{\"end\":42533,\"start\":42532},{\"end\":42548,\"start\":42542},{\"end\":42560,\"start\":42556},{\"end\":43008,\"start\":43001},{\"end\":43022,\"start\":43014},{\"end\":43466,\"start\":43462},{\"end\":43488,\"start\":43480},{\"end\":43503,\"start\":43496},{\"end\":43526,\"start\":43519},{\"end\":43544,\"start\":43538},{\"end\":43559,\"start\":43552},{\"end\":43574,\"start\":43568},{\"end\":43590,\"start\":43584},{\"end\":44010,\"start\":44006},{\"end\":44032,\"start\":44024},{\"end\":44047,\"start\":44040},{\"end\":44065,\"start\":44059},{\"end\":44080,\"start\":44073},{\"end\":44102,\"start\":44096},{\"end\":44609,\"start\":44607},{\"end\":44618,\"start\":44614},{\"end\":44627,\"start\":44623},{\"end\":44643,\"start\":44635},{\"end\":44655,\"start\":44650},{\"end\":44668,\"start\":44661},{\"end\":44679,\"start\":44674},{\"end\":44691,\"start\":44686},{\"end\":45138,\"start\":45134},{\"end\":45151,\"start\":45143},{\"end\":45163,\"start\":45156},{\"end\":45177,\"start\":45168},{\"end\":45191,\"start\":45184},{\"end\":45201,\"start\":45196},{\"end\":45631,\"start\":45624},{\"end\":45644,\"start\":45637},{\"end\":45657,\"start\":45650},{\"end\":45666,\"start\":45662},{\"end\":45678,\"start\":45671},{\"end\":45686,\"start\":45684},{\"end\":46132,\"start\":46127},{\"end\":46146,\"start\":46139},{\"end\":46158,\"start\":46153},{\"end\":46170,\"start\":46166},{\"end\":46609,\"start\":46600},{\"end\":46632,\"start\":46622},{\"end\":46654,\"start\":46646},{\"end\":46672,\"start\":46664},{\"end\":46684,\"start\":46679},{\"end\":46701,\"start\":46693},{\"end\":47186,\"start\":47181},{\"end\":47196,\"start\":47193},{\"end\":47603,\"start\":47599},{\"end\":47615,\"start\":47612},{\"end\":47632,\"start\":47623},{\"end\":47644,\"start\":47640},{\"end\":47657,\"start\":47652},{\"end\":47675,\"start\":47668},{\"end\":47690,\"start\":47684},{\"end\":47706,\"start\":47700},{\"end\":47719,\"start\":47712},{\"end\":47736,\"start\":47732},{\"end\":48158,\"start\":48153},{\"end\":48172,\"start\":48164},{\"end\":48183,\"start\":48177},{\"end\":48197,\"start\":48190},{\"end\":48205,\"start\":48202},{\"end\":48218,\"start\":48211},{\"end\":48614,\"start\":48607},{\"end\":48630,\"start\":48624},{\"end\":48650,\"start\":48645},{\"end\":49065,\"start\":49056},{\"end\":49085,\"start\":49076},{\"end\":49098,\"start\":49092},{\"end\":49110,\"start\":49109},{\"end\":49123,\"start\":49119},{\"end\":49136,\"start\":49131},{\"end\":49138,\"start\":49137},{\"end\":49580,\"start\":49575},{\"end\":49594,\"start\":49590},{\"end\":49618,\"start\":49601},{\"end\":49632,\"start\":49628},{\"end\":50048,\"start\":50046},{\"end\":50080,\"start\":50072},{\"end\":50350,\"start\":50347},{\"end\":50363,\"start\":50357},{\"end\":50375,\"start\":50370},{\"end\":50386,\"start\":50382},{\"end\":50397,\"start\":50391},{\"end\":50412,\"start\":50404},{\"end\":50426,\"start\":50419},{\"end\":50434,\"start\":50431},{\"end\":50919,\"start\":50916},{\"end\":50933,\"start\":50926},{\"end\":50945,\"start\":50940},{\"end\":50958,\"start\":50950},{\"end\":50970,\"start\":50964},{\"end\":51393,\"start\":51387},{\"end\":51404,\"start\":51400},{\"end\":51418,\"start\":51411},{\"end\":51432,\"start\":51426},{\"end\":51443,\"start\":51437},{\"end\":51452,\"start\":51449},{\"end\":51746,\"start\":51740},{\"end\":51758,\"start\":51753},{\"end\":51780,\"start\":51773},{\"end\":51798,\"start\":51789},{\"end\":51810,\"start\":51809},{\"end\":51824,\"start\":51819},{\"end\":52301,\"start\":52295},{\"end\":52313,\"start\":52308},{\"end\":52335,\"start\":52328},{\"end\":52351,\"start\":52344},{\"end\":52369,\"start\":52361},{\"end\":52382,\"start\":52376},{\"end\":52404,\"start\":52398},{\"end\":52419,\"start\":52412},{\"end\":52432,\"start\":52428},{\"end\":52884,\"start\":52876},{\"end\":52897,\"start\":52891},{\"end\":52911,\"start\":52903},{\"end\":52923,\"start\":52919},{\"end\":52934,\"start\":52930},{\"end\":52948,\"start\":52940},{\"end\":52957,\"start\":52953},{\"end\":53334,\"start\":53330},{\"end\":53343,\"start\":53339},{\"end\":53357,\"start\":53349},{\"end\":53368,\"start\":53364},{\"end\":53370,\"start\":53369},{\"end\":53669,\"start\":53662},{\"end\":53685,\"start\":53677},{\"end\":53700,\"start\":53693},{\"end\":53707,\"start\":53705},{\"end\":54053,\"start\":54046},{\"end\":54068,\"start\":54061},{\"end\":54450,\"start\":54443},{\"end\":54465,\"start\":54458},{\"end\":54477,\"start\":54472},{\"end\":54784,\"start\":54779},{\"end\":54794,\"start\":54790},{\"end\":54810,\"start\":54802},{\"end\":54820,\"start\":54817},{\"end\":54831,\"start\":54825},{\"end\":54844,\"start\":54839},{\"end\":54858,\"start\":54851},{\"end\":54868,\"start\":54864},{\"end\":54880,\"start\":54875},{\"end\":54891,\"start\":54885}]", "bib_author_last_name": "[{\"end\":36928,\"start\":36918},{\"end\":37073,\"start\":37068},{\"end\":37088,\"start\":37082},{\"end\":37282,\"start\":37278},{\"end\":37292,\"start\":37288},{\"end\":37308,\"start\":37302},{\"end\":37319,\"start\":37317},{\"end\":37327,\"start\":37325},{\"end\":37339,\"start\":37332},{\"end\":37766,\"start\":37762},{\"end\":37775,\"start\":37772},{\"end\":37795,\"start\":37787},{\"end\":37809,\"start\":37803},{\"end\":37829,\"start\":37820},{\"end\":38283,\"start\":38279},{\"end\":38292,\"start\":38289},{\"end\":38305,\"start\":38302},{\"end\":38325,\"start\":38316},{\"end\":38772,\"start\":38768},{\"end\":38787,\"start\":38783},{\"end\":38798,\"start\":38794},{\"end\":38808,\"start\":38805},{\"end\":38818,\"start\":38814},{\"end\":39271,\"start\":39267},{\"end\":39286,\"start\":39282},{\"end\":39302,\"start\":39297},{\"end\":39312,\"start\":39308},{\"end\":39703,\"start\":39693},{\"end\":39723,\"start\":39710},{\"end\":39736,\"start\":39731},{\"end\":39745,\"start\":39743},{\"end\":39765,\"start\":39753},{\"end\":39780,\"start\":39775},{\"end\":39797,\"start\":39788},{\"end\":39812,\"start\":39806},{\"end\":40186,\"start\":40183},{\"end\":40197,\"start\":40192},{\"end\":40208,\"start\":40206},{\"end\":40221,\"start\":40219},{\"end\":40235,\"start\":40232},{\"end\":40591,\"start\":40589},{\"end\":40606,\"start\":40601},{\"end\":40620,\"start\":40617},{\"end\":40630,\"start\":40627},{\"end\":41059,\"start\":41055},{\"end\":41073,\"start\":41068},{\"end\":41088,\"start\":41082},{\"end\":41099,\"start\":41095},{\"end\":41115,\"start\":41101},{\"end\":41540,\"start\":41535},{\"end\":41552,\"start\":41548},{\"end\":41562,\"start\":41559},{\"end\":41576,\"start\":41573},{\"end\":41592,\"start\":41588},{\"end\":41604,\"start\":41602},{\"end\":41614,\"start\":41612},{\"end\":41628,\"start\":41623},{\"end\":42123,\"start\":42117},{\"end\":42134,\"start\":42130},{\"end\":42148,\"start\":42143},{\"end\":42165,\"start\":42157},{\"end\":42530,\"start\":42508},{\"end\":42540,\"start\":42534},{\"end\":42554,\"start\":42549},{\"end\":42567,\"start\":42561},{\"end\":42577,\"start\":42569},{\"end\":43012,\"start\":43009},{\"end\":43027,\"start\":43023},{\"end\":43032,\"start\":43029},{\"end\":43478,\"start\":43467},{\"end\":43494,\"start\":43489},{\"end\":43517,\"start\":43504},{\"end\":43536,\"start\":43527},{\"end\":43550,\"start\":43545},{\"end\":43566,\"start\":43560},{\"end\":43582,\"start\":43575},{\"end\":43597,\"start\":43591},{\"end\":44022,\"start\":44011},{\"end\":44038,\"start\":44033},{\"end\":44057,\"start\":44048},{\"end\":44071,\"start\":44066},{\"end\":44094,\"start\":44081},{\"end\":44109,\"start\":44103},{\"end\":44612,\"start\":44610},{\"end\":44621,\"start\":44619},{\"end\":44633,\"start\":44628},{\"end\":44648,\"start\":44644},{\"end\":44659,\"start\":44656},{\"end\":44672,\"start\":44669},{\"end\":44684,\"start\":44680},{\"end\":44695,\"start\":44692},{\"end\":45141,\"start\":45139},{\"end\":45154,\"start\":45152},{\"end\":45166,\"start\":45164},{\"end\":45182,\"start\":45178},{\"end\":45194,\"start\":45192},{\"end\":45206,\"start\":45202},{\"end\":45635,\"start\":45632},{\"end\":45648,\"start\":45645},{\"end\":45660,\"start\":45658},{\"end\":45669,\"start\":45667},{\"end\":45682,\"start\":45679},{\"end\":45691,\"start\":45687},{\"end\":46137,\"start\":46133},{\"end\":46151,\"start\":46147},{\"end\":46164,\"start\":46159},{\"end\":46175,\"start\":46171},{\"end\":46620,\"start\":46610},{\"end\":46644,\"start\":46633},{\"end\":46662,\"start\":46655},{\"end\":46677,\"start\":46673},{\"end\":46691,\"start\":46685},{\"end\":46711,\"start\":46702},{\"end\":47191,\"start\":47187},{\"end\":47219,\"start\":47197},{\"end\":47610,\"start\":47604},{\"end\":47621,\"start\":47616},{\"end\":47638,\"start\":47633},{\"end\":47650,\"start\":47645},{\"end\":47666,\"start\":47658},{\"end\":47682,\"start\":47676},{\"end\":47698,\"start\":47691},{\"end\":47710,\"start\":47707},{\"end\":47730,\"start\":47720},{\"end\":47743,\"start\":47737},{\"end\":48162,\"start\":48159},{\"end\":48175,\"start\":48173},{\"end\":48188,\"start\":48184},{\"end\":48200,\"start\":48198},{\"end\":48209,\"start\":48206},{\"end\":48222,\"start\":48219},{\"end\":48622,\"start\":48615},{\"end\":48643,\"start\":48631},{\"end\":48658,\"start\":48651},{\"end\":49074,\"start\":49066},{\"end\":49090,\"start\":49086},{\"end\":49107,\"start\":49099},{\"end\":49117,\"start\":49111},{\"end\":49129,\"start\":49124},{\"end\":49148,\"start\":49139},{\"end\":49156,\"start\":49150},{\"end\":49588,\"start\":49581},{\"end\":49599,\"start\":49595},{\"end\":49626,\"start\":49619},{\"end\":49637,\"start\":49633},{\"end\":50070,\"start\":50049},{\"end\":50085,\"start\":50081},{\"end\":50095,\"start\":50087},{\"end\":50355,\"start\":50351},{\"end\":50368,\"start\":50364},{\"end\":50380,\"start\":50376},{\"end\":50389,\"start\":50387},{\"end\":50402,\"start\":50398},{\"end\":50417,\"start\":50413},{\"end\":50429,\"start\":50427},{\"end\":50438,\"start\":50435},{\"end\":50924,\"start\":50920},{\"end\":50938,\"start\":50934},{\"end\":50948,\"start\":50946},{\"end\":50962,\"start\":50959},{\"end\":50976,\"start\":50971},{\"end\":51398,\"start\":51394},{\"end\":51409,\"start\":51405},{\"end\":51424,\"start\":51419},{\"end\":51435,\"start\":51433},{\"end\":51447,\"start\":51444},{\"end\":51456,\"start\":51453},{\"end\":51751,\"start\":51747},{\"end\":51771,\"start\":51759},{\"end\":51787,\"start\":51781},{\"end\":51807,\"start\":51799},{\"end\":51817,\"start\":51811},{\"end\":51832,\"start\":51825},{\"end\":51841,\"start\":51834},{\"end\":52306,\"start\":52302},{\"end\":52326,\"start\":52314},{\"end\":52342,\"start\":52336},{\"end\":52359,\"start\":52352},{\"end\":52374,\"start\":52370},{\"end\":52396,\"start\":52383},{\"end\":52410,\"start\":52405},{\"end\":52426,\"start\":52420},{\"end\":52438,\"start\":52433},{\"end\":52889,\"start\":52885},{\"end\":52901,\"start\":52898},{\"end\":52917,\"start\":52912},{\"end\":52928,\"start\":52924},{\"end\":52938,\"start\":52935},{\"end\":52951,\"start\":52949},{\"end\":52961,\"start\":52958},{\"end\":53337,\"start\":53335},{\"end\":53347,\"start\":53344},{\"end\":53362,\"start\":53358},{\"end\":53373,\"start\":53371},{\"end\":53675,\"start\":53670},{\"end\":53691,\"start\":53686},{\"end\":53703,\"start\":53701},{\"end\":53712,\"start\":53708},{\"end\":54059,\"start\":54054},{\"end\":54073,\"start\":54069},{\"end\":54456,\"start\":54451},{\"end\":54470,\"start\":54466},{\"end\":54480,\"start\":54478},{\"end\":54788,\"start\":54785},{\"end\":54800,\"start\":54795},{\"end\":54815,\"start\":54811},{\"end\":54823,\"start\":54821},{\"end\":54837,\"start\":54832},{\"end\":54849,\"start\":54845},{\"end\":54862,\"start\":54859},{\"end\":54873,\"start\":54869},{\"end\":54883,\"start\":54881},{\"end\":54894,\"start\":54892}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":36672,\"start\":36542},{\"attributes\":{\"doi\":\"d41586-020-03187-3. Accessed: 2022-10-03\",\"id\":\"b1\"},\"end\":36914,\"start\":36674},{\"attributes\":{\"id\":\"b2\"},\"end\":37010,\"start\":36916},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":203705211},\"end\":37219,\"start\":37012},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":57246310},\"end\":37684,\"start\":37221},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":219964874},\"end\":38203,\"start\":37686},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8923541},\"end\":38672,\"start\":38205},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":216144533},\"end\":39195,\"start\":38674},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":245218753},\"end\":39658,\"start\":39197},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1033682},\"end\":40102,\"start\":39660},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2908606},\"end\":40533,\"start\":40104},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":40952,\"start\":40535},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":88166},\"end\":41451,\"start\":40954},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":209050760},\"end\":42034,\"start\":41453},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3568073},\"end\":42436,\"start\":42036},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7811489},\"end\":42944,\"start\":42438},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":247940176},\"end\":43356,\"start\":42946},{\"attributes\":{\"doi\":\"arXiv:1811.08565\",\"id\":\"b17\"},\"end\":43913,\"start\":43358},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":198183828},\"end\":44539,\"start\":43915},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235166551},\"end\":45080,\"start\":44541},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":235692714},\"end\":45561,\"start\":45082},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206596594},\"end\":46044,\"start\":45563},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":232185285},\"end\":46535,\"start\":46046},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1755257},\"end\":47123,\"start\":46537},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":12883586},\"end\":47527,\"start\":47125},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":202786778},\"end\":48104,\"start\":47529},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":237194623},\"end\":48539,\"start\":48106},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206592766},\"end\":49004,\"start\":48541},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6544744},\"end\":49529,\"start\":49006},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":9380519},\"end\":49965,\"start\":49531},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":53189344},\"end\":50284,\"start\":49967},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":68589},\"end\":50819,\"start\":50286},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":198968250},\"end\":51328,\"start\":50821},{\"attributes\":{\"doi\":\"arXiv:1812.11317\",\"id\":\"b33\"},\"end\":51658,\"start\":51330},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":238226716},\"end\":52222,\"start\":51660},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":247996742},\"end\":52819,\"start\":52224},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206596237},\"end\":53328,\"start\":52821},{\"attributes\":{\"doi\":\"arXiv:1411.7923\",\"id\":\"b37\"},\"end\":53576,\"start\":53330},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":10585115},\"end\":53945,\"start\":53578},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":202721170},\"end\":54344,\"start\":53947},{\"attributes\":{\"doi\":\"arXiv:1708.08197\",\"id\":\"b40\"},\"end\":54692,\"start\":54346},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":232148002},\"end\":55330,\"start\":54694}]", "bib_title": "[{\"end\":37059,\"start\":37012},{\"end\":37272,\"start\":37221},{\"end\":37751,\"start\":37686},{\"end\":38268,\"start\":38205},{\"end\":38763,\"start\":38674},{\"end\":39262,\"start\":39197},{\"end\":39687,\"start\":39660},{\"end\":40173,\"start\":40104},{\"end\":40579,\"start\":40535},{\"end\":41051,\"start\":40954},{\"end\":41528,\"start\":41453},{\"end\":42110,\"start\":42036},{\"end\":42502,\"start\":42438},{\"end\":42999,\"start\":42946},{\"end\":44004,\"start\":43915},{\"end\":44605,\"start\":44541},{\"end\":45132,\"start\":45082},{\"end\":45622,\"start\":45563},{\"end\":46125,\"start\":46046},{\"end\":46598,\"start\":46537},{\"end\":47179,\"start\":47125},{\"end\":47597,\"start\":47529},{\"end\":48151,\"start\":48106},{\"end\":48605,\"start\":48541},{\"end\":49054,\"start\":49006},{\"end\":49573,\"start\":49531},{\"end\":50044,\"start\":49967},{\"end\":50345,\"start\":50286},{\"end\":50914,\"start\":50821},{\"end\":51738,\"start\":51660},{\"end\":52293,\"start\":52224},{\"end\":52874,\"start\":52821},{\"end\":53660,\"start\":53578},{\"end\":54044,\"start\":53947},{\"end\":54777,\"start\":54694}]", "bib_author": "[{\"end\":36930,\"start\":36918},{\"end\":37075,\"start\":37061},{\"end\":37090,\"start\":37075},{\"end\":37284,\"start\":37274},{\"end\":37294,\"start\":37284},{\"end\":37310,\"start\":37294},{\"end\":37321,\"start\":37310},{\"end\":37329,\"start\":37321},{\"end\":37341,\"start\":37329},{\"end\":37768,\"start\":37753},{\"end\":37777,\"start\":37768},{\"end\":37797,\"start\":37777},{\"end\":37811,\"start\":37797},{\"end\":37831,\"start\":37811},{\"end\":38285,\"start\":38270},{\"end\":38294,\"start\":38285},{\"end\":38307,\"start\":38294},{\"end\":38327,\"start\":38307},{\"end\":38774,\"start\":38765},{\"end\":38789,\"start\":38774},{\"end\":38800,\"start\":38789},{\"end\":38810,\"start\":38800},{\"end\":38820,\"start\":38810},{\"end\":39273,\"start\":39264},{\"end\":39288,\"start\":39273},{\"end\":39304,\"start\":39288},{\"end\":39314,\"start\":39304},{\"end\":39705,\"start\":39689},{\"end\":39725,\"start\":39705},{\"end\":39738,\"start\":39725},{\"end\":39747,\"start\":39738},{\"end\":39767,\"start\":39747},{\"end\":39782,\"start\":39767},{\"end\":39799,\"start\":39782},{\"end\":39814,\"start\":39799},{\"end\":40188,\"start\":40175},{\"end\":40199,\"start\":40188},{\"end\":40210,\"start\":40199},{\"end\":40223,\"start\":40210},{\"end\":40237,\"start\":40223},{\"end\":40593,\"start\":40581},{\"end\":40608,\"start\":40593},{\"end\":40622,\"start\":40608},{\"end\":40632,\"start\":40622},{\"end\":41061,\"start\":41053},{\"end\":41075,\"start\":41061},{\"end\":41090,\"start\":41075},{\"end\":41101,\"start\":41090},{\"end\":41117,\"start\":41101},{\"end\":41542,\"start\":41530},{\"end\":41554,\"start\":41542},{\"end\":41564,\"start\":41554},{\"end\":41578,\"start\":41564},{\"end\":41594,\"start\":41578},{\"end\":41606,\"start\":41594},{\"end\":41616,\"start\":41606},{\"end\":41630,\"start\":41616},{\"end\":42125,\"start\":42112},{\"end\":42136,\"start\":42125},{\"end\":42150,\"start\":42136},{\"end\":42167,\"start\":42150},{\"end\":42532,\"start\":42504},{\"end\":42542,\"start\":42532},{\"end\":42556,\"start\":42542},{\"end\":42569,\"start\":42556},{\"end\":42579,\"start\":42569},{\"end\":43014,\"start\":43001},{\"end\":43029,\"start\":43014},{\"end\":43034,\"start\":43029},{\"end\":43480,\"start\":43462},{\"end\":43496,\"start\":43480},{\"end\":43519,\"start\":43496},{\"end\":43538,\"start\":43519},{\"end\":43552,\"start\":43538},{\"end\":43568,\"start\":43552},{\"end\":43584,\"start\":43568},{\"end\":43599,\"start\":43584},{\"end\":44024,\"start\":44006},{\"end\":44040,\"start\":44024},{\"end\":44059,\"start\":44040},{\"end\":44073,\"start\":44059},{\"end\":44096,\"start\":44073},{\"end\":44111,\"start\":44096},{\"end\":44614,\"start\":44607},{\"end\":44623,\"start\":44614},{\"end\":44635,\"start\":44623},{\"end\":44650,\"start\":44635},{\"end\":44661,\"start\":44650},{\"end\":44674,\"start\":44661},{\"end\":44686,\"start\":44674},{\"end\":44697,\"start\":44686},{\"end\":45143,\"start\":45134},{\"end\":45156,\"start\":45143},{\"end\":45168,\"start\":45156},{\"end\":45184,\"start\":45168},{\"end\":45196,\"start\":45184},{\"end\":45208,\"start\":45196},{\"end\":45637,\"start\":45624},{\"end\":45650,\"start\":45637},{\"end\":45662,\"start\":45650},{\"end\":45671,\"start\":45662},{\"end\":45684,\"start\":45671},{\"end\":45693,\"start\":45684},{\"end\":46139,\"start\":46127},{\"end\":46153,\"start\":46139},{\"end\":46166,\"start\":46153},{\"end\":46177,\"start\":46166},{\"end\":46622,\"start\":46600},{\"end\":46646,\"start\":46622},{\"end\":46664,\"start\":46646},{\"end\":46679,\"start\":46664},{\"end\":46693,\"start\":46679},{\"end\":46713,\"start\":46693},{\"end\":47193,\"start\":47181},{\"end\":47221,\"start\":47193},{\"end\":47612,\"start\":47599},{\"end\":47623,\"start\":47612},{\"end\":47640,\"start\":47623},{\"end\":47652,\"start\":47640},{\"end\":47668,\"start\":47652},{\"end\":47684,\"start\":47668},{\"end\":47700,\"start\":47684},{\"end\":47712,\"start\":47700},{\"end\":47732,\"start\":47712},{\"end\":47745,\"start\":47732},{\"end\":48164,\"start\":48153},{\"end\":48177,\"start\":48164},{\"end\":48190,\"start\":48177},{\"end\":48202,\"start\":48190},{\"end\":48211,\"start\":48202},{\"end\":48224,\"start\":48211},{\"end\":48624,\"start\":48607},{\"end\":48645,\"start\":48624},{\"end\":48660,\"start\":48645},{\"end\":49076,\"start\":49056},{\"end\":49092,\"start\":49076},{\"end\":49109,\"start\":49092},{\"end\":49119,\"start\":49109},{\"end\":49131,\"start\":49119},{\"end\":49150,\"start\":49131},{\"end\":49158,\"start\":49150},{\"end\":49590,\"start\":49575},{\"end\":49601,\"start\":49590},{\"end\":49628,\"start\":49601},{\"end\":49639,\"start\":49628},{\"end\":50072,\"start\":50046},{\"end\":50087,\"start\":50072},{\"end\":50097,\"start\":50087},{\"end\":50357,\"start\":50347},{\"end\":50370,\"start\":50357},{\"end\":50382,\"start\":50370},{\"end\":50391,\"start\":50382},{\"end\":50404,\"start\":50391},{\"end\":50419,\"start\":50404},{\"end\":50431,\"start\":50419},{\"end\":50440,\"start\":50431},{\"end\":50926,\"start\":50916},{\"end\":50940,\"start\":50926},{\"end\":50950,\"start\":50940},{\"end\":50964,\"start\":50950},{\"end\":50978,\"start\":50964},{\"end\":51400,\"start\":51387},{\"end\":51411,\"start\":51400},{\"end\":51426,\"start\":51411},{\"end\":51437,\"start\":51426},{\"end\":51449,\"start\":51437},{\"end\":51458,\"start\":51449},{\"end\":51753,\"start\":51740},{\"end\":51773,\"start\":51753},{\"end\":51789,\"start\":51773},{\"end\":51809,\"start\":51789},{\"end\":51819,\"start\":51809},{\"end\":51834,\"start\":51819},{\"end\":51843,\"start\":51834},{\"end\":52308,\"start\":52295},{\"end\":52328,\"start\":52308},{\"end\":52344,\"start\":52328},{\"end\":52361,\"start\":52344},{\"end\":52376,\"start\":52361},{\"end\":52398,\"start\":52376},{\"end\":52412,\"start\":52398},{\"end\":52428,\"start\":52412},{\"end\":52440,\"start\":52428},{\"end\":52891,\"start\":52876},{\"end\":52903,\"start\":52891},{\"end\":52919,\"start\":52903},{\"end\":52930,\"start\":52919},{\"end\":52940,\"start\":52930},{\"end\":52953,\"start\":52940},{\"end\":52963,\"start\":52953},{\"end\":53339,\"start\":53330},{\"end\":53349,\"start\":53339},{\"end\":53364,\"start\":53349},{\"end\":53375,\"start\":53364},{\"end\":53677,\"start\":53662},{\"end\":53693,\"start\":53677},{\"end\":53705,\"start\":53693},{\"end\":53714,\"start\":53705},{\"end\":54061,\"start\":54046},{\"end\":54075,\"start\":54061},{\"end\":54458,\"start\":54443},{\"end\":54472,\"start\":54458},{\"end\":54482,\"start\":54472},{\"end\":54790,\"start\":54779},{\"end\":54802,\"start\":54790},{\"end\":54817,\"start\":54802},{\"end\":54825,\"start\":54817},{\"end\":54839,\"start\":54825},{\"end\":54851,\"start\":54839},{\"end\":54864,\"start\":54851},{\"end\":54875,\"start\":54864},{\"end\":54885,\"start\":54875},{\"end\":54896,\"start\":54885}]", "bib_venue": "[{\"end\":36577,\"start\":36542},{\"end\":36734,\"start\":36674},{\"end\":37104,\"start\":37090},{\"end\":37412,\"start\":37341},{\"end\":37902,\"start\":37831},{\"end\":38398,\"start\":38327},{\"end\":38891,\"start\":38820},{\"end\":39385,\"start\":39314},{\"end\":39873,\"start\":39814},{\"end\":40288,\"start\":40237},{\"end\":40703,\"start\":40632},{\"end\":41195,\"start\":41117},{\"end\":41701,\"start\":41630},{\"end\":42226,\"start\":42167},{\"end\":42650,\"start\":42579},{\"end\":43105,\"start\":43034},{\"end\":43460,\"start\":43358},{\"end\":44185,\"start\":44111},{\"end\":44768,\"start\":44697},{\"end\":45279,\"start\":45208},{\"end\":45763,\"start\":45693},{\"end\":46248,\"start\":46177},{\"end\":46787,\"start\":46713},{\"end\":47285,\"start\":47221},{\"end\":47804,\"start\":47745},{\"end\":48285,\"start\":48224},{\"end\":48731,\"start\":48660},{\"end\":49228,\"start\":49158},{\"end\":49710,\"start\":49639},{\"end\":50112,\"start\":50097},{\"end\":50511,\"start\":50440},{\"end\":51039,\"start\":50978},{\"end\":51385,\"start\":51330},{\"end\":51904,\"start\":51843},{\"end\":52491,\"start\":52440},{\"end\":53033,\"start\":52963},{\"end\":53431,\"start\":53390},{\"end\":53744,\"start\":53714},{\"end\":54084,\"start\":54075},{\"end\":54441,\"start\":54346},{\"end\":54967,\"start\":54896},{\"end\":37114,\"start\":37106},{\"end\":37479,\"start\":37414},{\"end\":37969,\"start\":37904},{\"end\":38465,\"start\":38400},{\"end\":38958,\"start\":38893},{\"end\":39452,\"start\":39387},{\"end\":40335,\"start\":40290},{\"end\":40770,\"start\":40705},{\"end\":41768,\"start\":41703},{\"end\":42717,\"start\":42652},{\"end\":43172,\"start\":43107},{\"end\":44255,\"start\":44187},{\"end\":44835,\"start\":44770},{\"end\":45346,\"start\":45281},{\"end\":45829,\"start\":45765},{\"end\":46315,\"start\":46250},{\"end\":46857,\"start\":46789},{\"end\":47345,\"start\":47287},{\"end\":48342,\"start\":48287},{\"end\":48798,\"start\":48733},{\"end\":49294,\"start\":49230},{\"end\":49777,\"start\":49712},{\"end\":50578,\"start\":50513},{\"end\":51096,\"start\":51041},{\"end\":51961,\"start\":51906},{\"end\":52538,\"start\":52493},{\"end\":53099,\"start\":53035},{\"end\":55034,\"start\":54969}]"}}}, "year": 2023, "month": 12, "day": 17}