{"id": 44100802, "updated": "2023-09-30 21:24:48.827", "metadata": {"title": "MolGAN: An implicit generative model for small molecular graphs", "authors": "[{\"first\":\"Nicola\",\"last\":\"Cao\",\"middle\":[\"De\"]},{\"first\":\"Thomas\",\"last\":\"Kipf\",\"middle\":[]}]", "venue": "ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative Models", "journal": "ArXiv", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse. Code at https://github.com/nicola-decao/MolGAN", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1805.11973", "mag": "2806351858", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1805-11973", "doi": null}}, "content": {"source": {"pdf_hash": "860323829de36c9b2b9f99e8269ad26ae0c93d3d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/1805.11973v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7d00ea3ec678deea808d400c08124f4406c98766", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/860323829de36c9b2b9f99e8269ad26ae0c93d3d.txt", "contents": "\nMolGAN: An implicit generative model for small molecular graphs\n\n\nNicola De Cao \nThomas Kipf \nMolGAN: An implicit generative model for small molecular graphs\n\nDeep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihoodfree generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihoodbased method that directly generates graphs, albeit being susceptible to mode collapse.\n\nIntroduction\n\nFinding new chemical compounds with desired properties is a challenging task with important applications such as de novo drug design (Schneider & Fechner, 2005). The space of synthesizable molecules is vast and search in this space proves to be very difficult, mostly owing to its discrete nature.\n\nRecent progress in the development of deep generative models has spawned a range of promising proposals to address this issue. Most works in this area (G\u00f3mez-Bombarelli z ~ p(z) 0/1 0/1 x ~ p data (x) Figure 1. Schema of MolGAN. A vector z is sampled from a prior and passed to the generator which outputs the graph representation of a molecule. The discriminator classifies whether the molecular graph comes from the generator or the dataset. The reward network tries to estimate the reward for the chemical properties of a particular molecule provided by an external software. Kusner et al., 2017;Guimaraes et al., 2017;Dai et al., 2018) make use of a so-called SMILES representation (Weininger, 1988) of molecules: a string-based representation derived from molecular graphs. Recurrent neural networks (RNNs) are ideal candidates for these representations and consequently, most recent works follow the recipe of applying RNN-based generative models on this type of encoding. String-based representations of molecules, however, have certain disadvantages: RNNs have to spend capacity on learning both the syntactic rules and the order ambiguity of the representation. Besides, this is approach not applicable to generic (non-molecular) graphs. SMILES strings are generated from a graph-based representation of molecules, thereby working in the original graph space has the benefit of removing additional overhead. With recent progress in the area of deep learning on graphs (Bronstein et al., 2017;Hamilton et al., 2017), training deep generative models directly on graph representations becomes a feasible alternative that has been explored in a range of recent works (Kipf & Welling, 2016;Johnson, 2017;Grover et al., 2019;Li et al., 2018b;Simonovsky & Komodakis, 2018;You et al., 2018).\n\nLikelihood-based methods for molecular graph generation arXiv:1805.11973v2 [stat.ML] 27 Sep 2022 (Li et al., 2018b;Simonovsky & Komodakis, 2018) however, either require providing a fixed (or randomly chosen) ordered representation of the graph or an expensive graph matching procedure to evaluate the likelihood of a generated molecule, as the evaluation of all possible node orderings is prohibitive already for graphs of small size.\n\nIn this work, we sidestep this issue by utilizing implicit, likelihood-free methods, in particular, a generative adversarial network (GAN) (Goodfellow et al., 2014) that we adapt to work directly on graph representations. We further utilize a reinforcement learning (RL) objective similar to ORGAN (Guimaraes et al., 2017) to encourage the generation of molecules with particular properties.\n\nOur molecular GAN (MolGAN) model (outlined in Figure  1) is the first to address the generation of graph-structured data in the context of molecular synthesis using GANs (Goodfellow et al., 2014). The generative model of Mol-GAN predicts discrete graph structure at once (i.e., nonsequentially) for computational efficiency, although sequential variants are possible in general. MolGAN further utilizes a permutation-invariant discriminator and reward network (for RL-based optimization towards desired chemical properties) based on graph convolution layers (Bruna et al., 2014;Duvenaud et al., 2015;Schlichtkrull et al., 2017) that both operate directly on graphstructured representations.\n\n\nBackground\n\n\nMolecules as graphs\n\nMost previous deep generative models for molecular data (G\u00f3mez-Bombarelli et al., 2016;Kusner et al., 2017;Guimaraes et al., 2017;Dai et al., 2018) resort to generating SMILES representations of molecules. The SMILES syntax, however, is not robust to small changes or mistakes, which can result in the generation of invalid or drastically different structures. Grammar VAEs (Kusner et al., 2017) alleviate this problem by constraining the generative process to follow a particular grammar.\n\nOperating directly in the space of graphs has recently been shown to be a viable alternative for generative modeling of molecular data (Li et al., 2018b;Simonovsky & Komodakis, 2018) with the added benefit that all generated outputs are valid graphs (but not necessarily valid molecules).\n\nWe consider that each molecule can be represented by an undirected graph G with a set of edges E and nodes V. Each atom corresponds to a node v i \u2208 V that is associated with a T -dimensional one-hot vector x i , indicating the type of the atom. We further represent each atomic bond as an edge (v i , v j ) \u2208 E associated with a bond type y \u2208 {1, ..., Y }. For a molecular graph with N nodes, we can summarize this representation in a node feature matrix X = [x 1 , ..., x N ] T \u2208 R N \u00d7T and an adjacency tensor A \u2208 R N \u00d7N \u00d7Y where A ij \u2208 R Y is a one-hot vector indicating the type of the edge between i and j.\n\n\nImplicit vs. likelihood-based methods\n\nLikelihood-based methods such as the variational autoencoder (VAE) (Kingma & Welling, 2014;Rezende et al., 2014) typically allow for easier and more stable optimization than implicit generative models such as a GAN (Goodfellow et al., 2014). When generating graph-structured data, however, we wish to be invariant to reordering of nodes in the (ordered) matrix representation of the graph, which requires us to either perform a prohibitively expensive graph matching procedure (Simonovsky & Komodakis, 2018) or to evaluate the likelihood for all possible node permutations explicitly.\n\nBy resorting to implicit generative models, in particular to the GAN framework, we circumvent the need for an explicit likelihood. While the discriminator of the GAN can be made invariant to node ordering by utilizing graph convolutions (Bruna et al., 2014;Duvenaud et al., 2015;) and a node aggregation operator (Li et al., 2016), the generator still has to decide on a specific node ordering when generating a graph. Since we do not provide a likelihood, however, the generator is free to choose any suitable ordering for the task at hand. We provide a brief introduction to GANs in the following.\n\nGenerative adversarial networks GANs (Goodfellow et al., 2014) are implicit generative models in the sense that they allow for inference of model parameters without requiring one to specify a likelihood.\n\nA GAN consist of two main components: a generative model G \u03b8 , that learns a map from a prior to the data distribution to sample new data-points, and a discriminative model D \u03c6 , that learns to classify whether samples came from the data distribution rather than from G \u03b8 . Those two models are implemented as neural networks and trained simultaneously with stochastic gradient descent (SGD). G \u03b8 and D \u03c6 have different objectives, and they can be seen as two players in a minimax game\nmin \u03b8 max \u03c6 E x\u223cp data (x) [log D \u03c6 (x)]+ E z\u223cpz(z) [log(1 \u2212 D \u03c6 (G \u03b8 (z))] ,(1)\nwhere G \u03b8 tries to generate samples to fool the discriminator and D \u03c6 tries to differentiate samples correctly. To prevent undesired behaviour such as mode collapse (Salimans et al., 2016) and to stabilize learning, we use minibatch discrimination (Salimans et al., 2016) and improved WGAN (Gulrajani et al., 2017), an alternative and more stable GAN model that minimizes a better suited divergence.\n\nImproved WGAN WGANs (Arjovsky et al., 2017) minimize an approximation of the Earth Mover (EM) distance (also know as Wasserstein-1 distance) defined between two probability distributions. Formally, the Wasserstein distance between p and q, using the Kantorovich-Rubinstein duality is\nD W [p||q] = 1 K sup f L <K E x\u223cp(x) f (x) \u2212E x\u223cq(x) f (x) ,\n(2) where in the case of WGAN, p is the empirical distribution and q is the generator distribution. Note that the supremum is over all the K-Lipschitz functions for some K > 0. Gulrajani et al. (2017) introduce a gradient penalty as an alternative soft constraint on the 1-Lipschitz continuity as an improvement upon the gradient clipping scheme from the original WGAN. The loss with respect to the generator remains the same as in WGAN, but the loss function with respect to the discriminator is modified to be\nL(x (i) , G \u03b8 (z (i) ); \u03c6) = \u2212D \u03c6 (x (i) ) + D \u03c6 (G \u03b8 (z (i) )) original WGAN loss + \u03b1 \u2207x(i)D \u03c6 (x (i) ) \u2212 1 2 gradient penalty ,(3)\nwhere \u03b1 is a hyperparameter (we use \u03b1 = 10 as in the original paper),x (i) is a sampled linear combination between\nx (i) \u223c p data (x) and G \u03b8 (z (i) ) with z (i) \u223c p z (z), thusx (i) = x (i) + (1 \u2212 ) G \u03b8 (z (i) ) with \u223c U(0, 1).\n\nDeterministic policy gradients\n\nA GAN generator learns a transformation from a prior distribution to the data distribution. Thus, generated samples resemble data samples. However, in de novo drug design methods, we are not only interested in generating chemically valid compounds, but we want them to have some useful property (e.g., to be easily synthesizable). Therefore, we also optimize the generation process towards some non-differentiable metrics using reinforcement learning.\n\nIn reinforcement learning, a stochastic policy is represented by \u03c0 \u03b8 (s) = p \u03b8 (a|s) which is a parametric probability distribution in \u03b8 that selects a categorical action a conditioned on an state s. Conversely, a deterministic policy is represented by \u00b5 \u03b8 (s) = a which deterministically outputs an action.\n\nIn initial experiments, we explored using REINFORCE (Williams, 1992) in combination with a stochastic policy that models graph generation as a set of categorical choices (actions). However, we found that it converged poorly due to the high dimensional action space when generating graphs at once. We instead base our method on a deterministic policy gradient algorithm which is known to perform well in high-dimensional action spaces (Silver et al., 2014). In particular, we employ a simplified version of deep deterministic policy gradient (DDPG) introduced by Lillicrap et al. (2016), an off-policy actor-critic algorithm that uses deterministic policy gradients to maximize an approximation of the expected future reward.\n\nIn our case, the policy is the GAN generator G \u03b8 which takes a sample z for the prior as input, instead of an environmental state s, and it outputs a molecular graph as an action (a = G). Moreover, we do not model episodes, so there is no need to assess the quality of a state-action combination since it does only depend on the graph G. Therefore, we introduce a learnable and differentiable approximation of the reward functionR \u03c8 (G) that predicts the immediate reward, and we train it via a mean squared error objective based on the real reward provided by an external system (e.g., the synthesizability score of a molecule). Then, we train the generator maximizing the predicted reward viaR \u03c8 (G) which, being differentiable, provides a gradient to the policy towards the desired metric.\n\n\nModel\n\nThe MolGAN architecture ( Figure 2) consists of three main components: a generator G \u03b8 , a discriminator D \u03c6 and a reward networkR \u03c8 .\n\nThe generator takes a sample from a prior distribution and generates an annotated graph G representing a molecule. Nodes and edges of G are associated with annotations denoting atom type and bond type respectively. The discriminator takes both samples from the dataset and the generator and learns to distinguish them. Both G \u03b8 and D \u03c6 are trained using improved WGAN such that the generator learns to match the empirical distribution and eventually outputs valid molecules.\n\nThe reward network is used to approximate the reward function of a sample and optimize molecule generation towards non-differentiable metrics using reinforcement learning. Dataset and generated samples are inputs ofR \u03c8 , but, differently from the discriminator, it assigns scores to them (e.g., how likely the generated molecule is to be soluble in water). The reward network learns to assign a reward to each molecule to match a score provided by an external software 1 . Notice that, when MolGAN outputs a non-valid molecule, it is not possible to assign a reward since the graph is not even a compound. Thus, for invalid molecular graphs, we assign zero rewards.\n\nThe discriminator is trained using the WGAN objective while the generator uses a linear combination of the WGAN A < l a t e x i t s h a 1 _ b a s e 6 4 = \" E M P y u 5 A S l E p I 1 q v r J e u 1 m c k h U A U = \" > A A A B 8 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U S E u q y 4 c V n B P r A N Z T K d t E M n k z B z I 5 T Q v 3 D j Q h G 3 / o 0 7 / 8 Z J m 4 W 2 H h g 4 n H M v c + 4 J E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 0 2 I 8 o j o M w u 5 k N K l W 3 5 s 5 B V o l X k C o U a A 4 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z u Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k X O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L Y l e M s n r 5 L 2 Z c 1 z a 9 7 9 V b V R\n\n\nL + o o w S m c w Q V 4 U I c G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j K 4 5 x c 4 J / I H z + Q O m V 5 D a < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" E M P y u 5 A S l E p I 1 q v r J e u 1 m c k h U A U = \" > A A A B 8 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U S E u q y 4 c V n B P r A N Z T K d t E M n k z B z I 5 T Q v 3 D j Q h G 3 / o 0 7 / 8 Z J m 4 W 2 H h g 4 n H\n\nM v c + 4 J E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 0 2 I 8 o j o M w u 5 k N K l W 3 5 s 5 B V o l X k C o U a A 4 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z u Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k X O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L Y l e M s n r 5 L 2 Z c 1 z a 9 7 9 V b V R\n\n\nL + o o w S m c w Q V 4 U I c G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j K 4 5 x c 4 J / I H z + Q O m V 5 D a < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" E M P y u 5 A S l E p I 1 q v r J e u 1 m c k h U A U = \" > A A A B 8 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U S E u q y 4 c V n B P r A N Z T K d t E M n k z B z I 5 T Q v 3 D j Q h G 3 / o 0 7 / 8 Z J m 4 W 2 H h g 4 n H\n\nM v c + 4 J E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 0 2 I 8 o j o M w u 5 k N K l W 3 5 s 5 B V o l X k C o U a A 4 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z u Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k X O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L Y l e M s n r 5 L 2 Z c 1 z a 9 7 9 V b V R\n\n\nL + o o w S m c w Q V 4 U I c G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j K 4 5 x c 4 J / I H z + Q O m V 5 D a < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" E M P y u 5 A S l E p I 1 q v r J e u 1 m c k h U A U = \" > A A A B 8 X i c b V D L S s N A F L 3 x W e u r 6 t L N Y B F c l U S E u q y 4 c V n B P r A N Z T K d t E M n k z B z I 5 T Q v 3 D j Q h G 3 / o 0 7 / 8 Z J m 4 W 2 H h g 4 n H\n\nM v c + 4 J E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 0 2 I 8 o j o M w u 5 k N K l W 3 5 s 5 B V o l X k C o U a A 4 q X / 1 h z N K I K 2 S S G t P z 3 A T 9 j G o U T P J Z u Z 8 a n l A 2 o S P e s 1 T R i B s / m y e e k X O r D E k Y a / s U k r n 6 e y O j k T H T K L C T e U K z 7 O X i f 1 4 v x f D a z 4 R K U u S K L T 4 K U 0 k w J v n 5 Z C g 0 Z y i n l l C m h c 1 K 2 J h q y t C W V L Y l e M s n r 5 L 2 Z c 1 z a 9 7 9\nV b V R L + o o w S m c w Q V 4 U I c G 3 E E T W s B A w T O 8 w p t j n B f n 3 f l Y j K 4 5 x c 4 J / I H z + Q O m V 5 D a < / l a t e x i t > X < l a t e x i t s h a 1 _ b a s e 6 4 = \" k 8 f M T Y M p b c A k 1 m 6 r T Y M e g J s d M O M = \" > A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 1 G X B j c s K 9 o F t K J P p p B 0 6 m Y S Z G 6 G E / o U b F 4 q 4 9 W / c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e L a i F g 9 4 C z h f k T H S o S C U b T S 4 y C i O A n C r D c f V m t u 3 V 2 A r B O v I D U o 0 B p W v w a j m K U R V 8 g k N a b v u Q n 6 G d U o m O T z y i A 1 P K F s S s e 8 b 6 m i E T d + t k g 8 J x d W G Z E w 1 v Y p J A v 1 9 0 Z G I 2 N m U W A n 8 4 R m 1 c v F / 7 x + i u G N n w m V p M g V W 3 4 U p p J g T P L z y U h o z l D O L K F M C 5 u V s A n V l K E t q W J L 8 F Z P X i e d q 7 r n 1 r 3 7 6 1 q z U d R R h j M 4 h 0 v w o A F N u I M W t I G B g m d 4 h T f H O C / O u / O x H C 0 5 x c 4 p / I H z + Q P J S p D x < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" k 8 f M T Y M p b c A k 1 m 6 r T Y M e g J s d M O M = \" > A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 1 G X B j c s K 9 o F t K J P p p B 0 6 m Y S Z G 6 G E / o U b F 4 q 4 9 W / c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e L a i F g 9 4 C z h f k T H S o S C U b T S 4 y C i O A n C r D c f V m t u 3 V 2 A r B O v I D U o 0 B p W v w a j m K U R V 8 g k N a b v u Q n 6 G d U o m O T z y i A 1 P K F s S s e 8 b 6 m i E T d + t k g 8 J x d W G Z E w 1 v Y p J A v 1 9 0 Z G I 2 N m U W A n 8 4 R m 1 c v F / 7 x + i u G N n w m V p M g V W 3 4 U p p J g T P L z y U h o z l D O L K F M C 5 u V s A n V l K E t q W J L 8 F Z P X i e d q 7 r n 1 r 3 7 6 1 q z U d R R h j M 4 h 0 v w o A F N u I M W t I G B g m d 4 h T f H O C / O u / O x H C 0 5 x c 4 p / I H z + Q P J S p D x < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" k 8 f M T Y M p b c A k 1 m 6 r T Y M e g J s d M O M = \" > A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 1 G X B j c s K 9 o F t K J P p p B 0 6 m Y S Z G 6 G E / o U b F 4 q 4 9 W / c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e L a i F g 9 4 C z h f k T H S o S C U b T S 4 y C i O A n C r D c f V m t u 3 V 2 A r B O v I D U o 0 B p W v w a j m K U R V 8 g k N a b v u Q n 6 G d U o m O T z y i A 1 P K F s S s e 8 b 6 m i E T d + t k g 8 J x d W G Z E w 1 v Y p J A v 1 9 0 Z G I 2 N m U W A n 8 4 R m 1 c v F / 7 x + i u G N n w m V p M g V W 3 4 U p p J g T P L z y U h o z l D O L K F M C 5 u V s A n V l K E t q W J L 8 F Z P X i e d q 7 r n 1 r 3 7 6 1 q z U d R R h j M 4 h 0 v w o A F N u I M W t I G B g m d 4 h T f H O C / O u / O x H C 0 5 x c 4 p / I H z + Q P J S p D x < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" k 8 f M T Y M p b c A k 1 m 6 r T Y M e g J s d M O M = \" > A A A B 8 X i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 1 G X B j c s K 9 o F t K J P p p B 0 6 m Y S Z G 6 G E / o U b F 4 q 4 9 W / c + T d O 2 i y 0 9 c D A 4 Z x 7 m X N P k E h h 0 H W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 7 p m D j V j L d Z L G P d C 6 j h U i j e R o G S 9 x L N a R R I 3 g 2 m t 7 n f f e L a i F g 9 4 C z h f k T H S o S C U b T S 4 y C i O A n C r D c f V m t u 3 V 2 A r B O v I D U o 0 B p W v w a j m K U R V 8 g k N a b v u Q n 6 G d U o m O T z y i A 1 P K F s S s e 8 b 6 m i E T d + t k g 8 J x d W G Z E w 1 v Y p J A v 1 9 0 Z G I 2 N m U W A n 8 4 R m 1 c v F / 7 x + i u G N n w m V p M g V W 3 4 U p p J g T P L z y U h o z l D O L K F M C 5 u V s A n V l K E t q W J L 8 F Z P X i e d q 7 r n 1 r 3 7 6 1 q z U d R R h j M 4 h 0 v w o A F N u I M W t I G B g m d 4 h T f H O C / O u / O x H C 0 5 x c 4 p / I H z + Q P J S p D x < / l a t e x i t >X < l a t e x i t s h a 1 _ b a s e 6 4 = \" h 5 f k k v O P N q e 9 N I 7 w 0 S L n 2 N 2 F V m c = \" > A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 K o k I d V l w 4 7 K C f U A T y m Q y a Y d O J m F m I p a Q X 3 H j Q h G 3 / o g 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J U s 6 U d p x v a 2 N z a 3 t n t 7 Z X 3 z 8 4 P D q 2 T x p 9 l W S S 0 B 5 J e C K H A V a U M 0 F 7 m m l O h 6 m k O A 4 4 H Q S z 2 9 I f P F K p W C I e 9 D y l f o w n g k W M Y G 2 k s d 3 w Y q y n Q Z R 7 m v G Q 5 s O i G N t N p + U s g N a J W 5 E m V O i O 7 S 8 v T E g W U 6 E J x 0 q N X C f V f o 6 l Z o T T o u 5 l i q a Y z P C E j g w V O K b K z x f Z C 3 R h l B B F i T R P a L R Q f 2 / k O F Z q H g d m s k y q V r 1 S / M 8 b Z T q 6 8 X M m 0 k x T Q Z a H o o w j n a C y C B Q y S Y n m c 0 M w k c x k R W S K J S b a 1 F U 3 J b i r X 1 4 n / a u W 6 7 T c + + t m p 1 3 V U Y M z O I d L c K E N H b i D L v S A w B M 8 w y u 8 W Y X 1 Y r 1 b H 8 v R D a v a O Y U / s D 5 / A L y e l N g = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" h 5 f k k v O P N q e 9 N I 7 w 0 S L n 2 N 2 F V m c = \" > A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 K o k I d V l w 4 7 K C f U A T y m Q y a Y d O J m F m I p a Q X 3 H j Q h G 3 / o g 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J U s 6 U d p x v a 2 N z a 3 t n t 7 Z X 3 z 8 4 P D q 2 T x p 9 l W S S 0 B 5 J e C K H A V a U M 0 F 7 m m l O h 6 m k O A 4 4 H Q S z 2 9 I f P F K p W C I e 9 D y l f o w n g k W M Y G 2 k s d 3 w Y q y n Q Z R 7 m v G Q 5 s O i G N t N p + U s g N a J W 5 E m V O i O 7 S 8 v T E g W U 6 E J x 0 q N X C f V f o 6 l Z o T T o u 5 l i q a Y z P C E j g w V O K b K z x f Z C 3 R h l B B F i T R P a L R Q f 2 / k O F Z q H g d m s k y q V r 1 S / M 8 b Z T q 6 8 X M m 0 k x T Q Z a H o o w j n a C y C B Q y S Y n m c 0 M w k c x k R W S K J S b a 1 F U 3 J b i r X 1 4 n / a u W 6 7 T c + + t m p 1 3 V U Y M z O I d L c K E N H b i D L v S A w B M 8 w y u 8 W Y X 1 Y r 1 b H 8 v R D a v a\nO Y U / s D 5 / A L y e l N g = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" h 5 f k k v O P N q e 9 N I 7 w 0 S L n 2 N 2 F V m c = \" > A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 K o k I d V l w 4 7 K C f U A T y m Q y a Y d O J m F m I p a Q X 3 H j Q h G 3 / o g 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J U s 6 U d p x v a 2 N z a 3 t n t 7 Z X 3 z 8 4 P D q 2 T x p 9 l W S S 0 B 5 J e C K H A V a U M 0 F 7 m m l O h 6 m k O A 4 4 H Q S z 2 9 I f P F K p W C I e 9 D y l f o w n g k W M Y G 2 k s d 3 w Y q y n Q Z R 7 m\nv G Q 5 s O i G N t N p + U s g N a J W 5 E m V O i O 7 S 8 v T E g W U 6 E J x 0 q N X C f V f o 6 l Z o T T o u 5 l i q a Y z P C E j g w V O K b K z x f Z C 3 R h l B B F i T R P a L R Q f 2 / k O F Z q H g d m s k y q V r 1 S / M 8 b Z T q 6 8 X M m 0 k x T Q Z a H o o w j n a C y C B Q y S Y n m c 0 M w k c x k R W S K J S b a 1 F U 3 J b i r X 1 4 n / a u W 6 7 T c + + t m p 1 3 V U Y M z O I d L c K E N H b i D L v S A w B M 8 w y u 8 W Y X 1 Y r 1 b H 8 v R D a v a\nO Y U / s D 5 / A L y e l N g = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" h 5 f k k v O P N q e 9 N I 7 w 0 S L n 2 N 2 F V m c = \" > A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 K o k I d V l w 4 7 K C f U A T y m Q y a Y d O J m F m I p a Q X 3 H j Q h G 3 / o g 7 / 8 Z J m 4 W 2 H h g 4 n H M v 9 8 w J U s 6 U d p x v a 2 N z a 3 t n t 7 Z X 3 z 8 4 P D q 2 T x p 9 l W S S 0 B 5 J e C K H A V a U M 0 F 7 m m l O h 6 m k O A 4 4 H Q S z 2 9 I f P F K p W C I e 9 D y l f o w n g k W M Y G 2 k s d 3 w Y q y n Q Z R 7 m\nv G Q 5 s O i G N t N p + U s g N a J W 5 E m V O i O 7 S 8 v T E g W U 6 E J x 0 q N X C f V f o 6 l Z o T T o u 5 l i q a Y z P C E j g w V O K b K z x f Z C 3 R h l B B F i T R P a L R Q f 2 / k O F Z q H g d m s k y q V r 1 S / M 8 b Z T q 6 8 X M m 0 k x T Q Z a H o o w j n a C y C B Q y S Y n m c 0 M w k c x k R W S K J S b a 1 F U 3 J b i r X 1 4 n / a u W 6 7 T c + + t m p 1 3 V U Y M z O I d L c K E N H b i D L v S A w B M 8 w y u 8 W Y X 1 Y r 1 b H 8 v R D a v a\nO Y U / s D 5 / A L y e l N g = < / l a t e x i t > A < l a t e x i t s h a 1 _ b a s e 6 4 = \" I V J A E z j P j i X P v p 4 O o 4 Q N T U c / K d s = \" > A A A B + 3 i c b V D L S s N A F L 2 p r 1 p f t S 7 d D B b B V U l E q M u K G 5 c V 7 A O a U C a T S T t 0 M g k z E 7 G E / I o b F 4 q 4 9 U f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W e O n 3 C m t G 1 / W 5 W N z a 3 t n e p u b W / / 4 P C o f t z o q z i V h P Z I z G M 5 9 L G i n A n a 0 0 x z O k w k x Z H P 6 c C f 3 R b + 4 J F K x W L x o O c J 9 S I 8 E S x k B G s j j e s N N 8 J 6 6 o e Z q x k P a H a T 5 + N 6 0 2 7 Z C 6 B 1 4 p S k C S W 6 4 / q X G 8 Q k j a j Q h G O l R o 6 d a C / D U j P C a V 5 z U 0 U T T G Z 4 Q k e G C h x R 5 W W L 7 D k 6 N 0 q A w l i a J z R a q L 8 3 M h w p N Y 9 8 M 1 k k V a t e I f 7 n j V I d X n s Z E 0 m q q S D L Q 2 H K k Y 5 R U Q Q K m K R E 8 7 k h m E h m s i I y x R I T b e q q m R K c 1 S + v k / 5 l y 7 F b z v 1 V s 9 M u 6 6 j C K Z z B B T j Q h g 7 c Q R d 6 Q O A J n u E V 3 q z c e r H e r Y / l a M U q d 0 7 g D 6 z P H 5 m U l M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" I V J A E z j P j i X P v p 4 O o 4 Q N T U c / K d s = \" > A A A B + 3 i c b V D L S s N A F L 2 p r 1 p f t S 7 d D B b B V U l E q M u K G 5 c V 7 A O a U C a T S T t 0 M g k z E 7 G E / I o b F 4 q 4 9 U f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W e O n 3 C m t G 1 / W 5 W N z a 3 t n e p u b W / / 4 P C o f t z o q z i V h P Z I z G M 5 9 L G i n A n a 0 0 x z O k w k x Z H P 6 c C f 3 R b + 4 J F K x W L x o O c J 9 S I 8 E S x k B G s j j e s N N 8 J 6 6 o e Z q x k P a H a T 5 + N 6 0 2 7 Z C 6 B 1 4 p S k C S W 6 4 / q X G 8 Q k j a j Q h G O l R o 6 d a C / D U j P C a V 5 z U 0 U T T G Z 4 Q k e G C h x R 5 W W L 7 D k 6 N 0 q A w l i a J z R a q L 8 3 M h w p N Y 9 8 M 1 k k V a t e I f 7 n j V I d X n s Z E 0 m q q S D L Q 2 H K k Y 5 R U Q Q K m K R E 8 7 k h m E h m s i I y x R I T b e q q m R K c 1 S + v k / 5 l y 7 F b z v 1 V s 9 M u 6 6 j C K Z z B B T j Q h g 7 c Q R d 6 Q O A J n u E V 3 q z c e r H e r Y / l a M U q d 0 7 g D 6 z P H 5 m U l M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" I V J A E z j P j i X P\n\n\nv p 4 O o 4 Q N T U c / K d s = \" > A A A B + 3 i c b V D L S s N A F L 2 p r 1 p f t S 7 d D B b B V U l E q M u K G 5 c V 7 A O a U C a T S T t 0 M g k z E 7 G E / I o b F 4 q 4 9 U f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W e O n 3 C m t G 1 / W 5 W N z a 3 t n e p u b W / / 4 P C o f t z o q z i V h P Z I z G M 5 9 L G i n A n a 0 0 x z O k w k x Z H P 6 c C f 3 R b + 4 J F K x W L x o O c J 9 S I 8 E S x k B G s j j e s N N 8 J 6 6 o e Z q x k P a H a T 5 + N 6 0 2 7 Z C 6 B 1 4 p S k C S W 6 4 / q X G 8 Q k j a j Q h G O l R o 6 d a C / D U j P C a V 5 z U 0 U T T G Z 4 Q k e G C h x R 5 W W L 7 D k 6 N 0 q A w l i a J z R a q L 8 3 M h w p N Y 9 8 M 1 k k V a t e I f 7 n j V I d X n s Z E 0 m q q S D L Q 2 H K k Y 5 R U Q Q K m K R E 8 7 k h m E h m s i I y x R I T b e q q m R K c 1 S + v k / 5 l y 7 F b z v 1 V s 9 M u 6 6 j C K Z z B B T j Q h g 7 c Q R d 6 Q O A J n u E V 3 q z c e r H e r Y / l a M U q d 0 7 g D 6 z P H 5 m U l M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" I V J A E z j P j i X P v p 4 O o 4 Q N T U c / K d s = \" > A A A B + 3 i c b V D L S s N A F L 2 p r 1 p f t S 7 d D B b B V U l E q M u K G 5 c V 7 A O a U C a T S T t 0 M g k z E 7 G E / I o b F 4 q 4 9 U f c + T d O 2 i y 0 9 c D A 4 Z x 7 u W e O n 3 C m t G 1 / W 5 W N z a 3 t n e p u b W / / 4 P C o f t z o q z i V h P Z I z G M 5 9 L G i n A n a 0 0 x z O k w k x Z H P 6 c C f 3 R b + 4 J F K x W L x o O c J 9 S I 8 E S x k B G s j j e s N N 8 J 6\n\n6 o e Z q x k P a H a T 5 + N 6 0 2 7 Z C 6 B 1 4 p S k C S W 6 4 / q X G 8 Q k j a j Q h G O l R o 6 d a C / D U j P C a V 5 z U 0 U T T G Z 4 Q k e G C h x R 5 W W L 7 D k 6 N 0 q A w l i a J z R a q L 8 3 M h w p N Y 9 8 M 1 k k V a t e I f 7 n j V I d X n s Z E 0 m q q S D L Q 2 H K k Y 5 R U Q Q K m K R E 8 7 k h m E h m s i I y x R I T b e q q m R K c 1 S + v k / 5 l y 7 F b z v 1 V s 9 M u 6 6 j C K Z z B B T j Q h g 7 c Q R d 6 Q O A J n u E V 3 q z c e r H e r Y / l a M U q d 0 7 g D 6 z P H 5 m U l M E = < / l a t e x i t > Figure 2. Outline of MolGAN. From left: the generator takes a sample from a prior distribution and generates a dense adjacency tensor A and an annotation matrix X. Subsequently, sparse and discrete\u00c3 andX are obtained from A and X respectively via categorical sampling. The combination of\u00c3 andX represents an annotated molecular graph which corresponds to a specific chemical compound. Finally, the graph is processed by both the discriminator and reward networks that are invariant to node order permutations and based on Relational-GCN (Schlichtkrull et al., 2017) layers. loss and the RL loss:\nL(\u03b8) = \u03bb \u00b7 L W GAN (\u03b8) + (1 \u2212 \u03bb) \u00b7 L RL (\u03b8) ,(4)\nwhere \u03bb \u2208 [0, 1] is a hyperparameter that regulates the tradeoff between the two components.\n\n\nGenerator\n\nG \u03c6 (z) takes D-dimensional vectors z \u2208 R D sampled from a standard normal distribution z \u223c N (0, I) and outputs graphs. While recent works have shown that it is feasible to generate graphs of small size by using an RNN-based generative model (Johnson, 2017;You et al., 2018;Li et al., 2018a;b) we, for simplicity, utilize a generative model that predicts the entire graph at once using a simple multi-layer perceptron (MLP), as similarly done in Simonovsky & Komodakis (2018). While this limits our study to graphs of a pre-chosen maximum size, we find that it is significantly faster and easier to optimize.\n\nWe restrict the domain to graphs of a limited number of nodes and, for each z, G \u03b8 outputs two continuous and dense objects: X \u2208 R N \u00d7T that defines atom types and A N \u00d7N \u00d7Y that defines bonds types (see Section 2.1). Both X and A have a probabilistic interpretation since each node and edge type is represented with probabilities of categorical distributions over types. To generate a molecule we obtain discrete, sparse objectsX and\u00c3 via categorical sampling from X and A, respectively. We overload notation and also represent samples from the dataset with binaryX and\u00c3.\n\nAs this discretization process is non-differentiable, we explore three model variations to allow for gradient-based training: We can i) use the continuous objects X and A directly during the forward pass (i.e.,X = X and A = A), ii) add Gumbel noise to X and A before passing them to D \u03c6 andR \u03c8 in order to make the generation stochastic while still forwarding continuous objects (i.e.,X ij = X ij + Gumbel(\u00b5 = 0, \u03b2 = 1) and A = A ijy + Gumbel(\u00b5 = 0, \u03b2 = 1)), or iii) use a straightthrough gradient based on categorical reparameterization with the Gumbel-Softmax (Jang et al., 2017;Maddison et al., 2017), that is we use a sample form a categorical distribution during the forward pass (i.e.,X i = Cat(X i ) and\u00c3 ij = Cat(A ij )) and the continuous relaxed values (i.e., the original X and A) in the backward pass.\n\n\nDiscriminator and reward network\n\nBoth the discriminator D \u03c6 and the reward networkR \u03c8 receive a graph as input, and they output a scalar value each. We choose the same architecture for both networks but do not share parameters between them. A series of graph convolution layers convolve node signalsX using the graph adjacency tensor\u00c3. We base our model on Relational-GCN (Schlichtkrull et al., 2017), a convolutional network for graphs with support for multiple edge types. At every layer, feature representations of nodes are convolved/propagated according to:\nh ( +1) i = f ( ) s (h ( ) i , x i ) + N j=1 Y y=1\u00c3 ijy |N i | f ( ) y (h ( ) j , x j ) , h ( +1) i = tanh(h ( +1) i ) ,(5)\nwhere h ( ) i is the signal of the node i at layer and f ( ) s is a linear transformation function that acts as a self-connection between layers. We further utilize an edge type-specific affine function f ( ) y for each layer. N i denotes the set of neighbors for node i. The normalization factor 1/|N i | ensures that activations are on a similar scale irrespective of the number of neighbors.\n\nAfter several layers of propagation via graph convolutions, following Li et al. (2016) we aggregate node embeddings into a graph level representation vector as\nh G = v\u2208V \u03c3(i(h (L) v , x v )) tanh(j(h (L) v , x v )) , h G = tanh h G ,(6)\nwhere \u03c3(x) = 1/(1+exp(\u2212x)) is the logistic sigmoid function, i and j are MLPs with a linear output layer and denotes element-wise multiplication. Then, h G is a vector representation of the graph G and it is further processed by an MLP to produce a graph level scalar output \u2208 (\u2212\u221e, +\u221e) for the discriminator and \u2208 (0, 1) for the reward network.\n\n\nRelated work\n\nObjective-Reinforced Generative Adversarial Networks (ORGAN) by Guimaraes et al. (2017) is the closest related work to ours. Their model relies on SeqGAN (Yu et al., 2017) to adversarially learn to output sequences while optimizing towards chemical metrics with REINFORCE (Williams, 1992). The main differences from our approach is that they model sequences of SMILES as molecular representations instead of graphs, and their RL component uses REINFORCE while we use DDPG. Segler et al. (2018) also employs RL for drug discovery by searching retrosynthetic routes using Monte Carlo Tree Search (MCTS) in combination with an expansion policy network.\n\nSeveral other works have explored training generative models on SMILES representations of molecules: CharacterVAE (G\u00f3mez-Bombarelli et al., 2016) is the first such model that is based on a VAE with recurrent encoder and decoder networks. GrammarVAE (Kusner et al., 2017) and SDVAE (Dai et al., 2018) constrain the decoding process to follow particular syntactic and semantic rules.\n\nA related line of research considers training deep generative models to output graph-structured data directly. Several works explored auto-encoder architectures utilizing graph convolutions for link prediction within graphs (Kipf & Welling, 2016;Grover et al., 2019;Davidson et al., 2018). Johnson (2017) For link prediction within graphs, a range of adversarial methods have been introduced in the literature (Minervini et al., 2017;Wang et al., 2018;Bojchevski et al., 2018). This class of models, however, is not suitable to generate molec-ular graphs from scratch, which makes direct comparison infeasible.\n\n\nExperiments\n\nWe compare MolGAN against recent neural network-based drug generation models in a range of experiments on established benchmarks using the QM9 (Ramakrishnan et al., 2014) chemical database. We first focus on studying the effect of the \u03bb parameter to find the best trade-off between the GAN and RL objective (see Section 5.1). We then compare MolGAN with ORGAN (Guimaraes et al., 2017) since it is the most related work to ours: ORGAN is a sequential generative model operating on SMILES representations, optimizing towards several chemical properties with an RL objective (see Section 5.2). We also compare our model against variational autoencoding methods (Section 5.3) such as CharacterVAE (G\u00f3mez-Bombarelli et al., 2016), GrammarVAE (Kusner et al., 2017), as well as a recent graph-based generative model: GraphVAE (Simonovsky & Komodakis, 2018).\n\nDataset In all experiments, we used QM9 (Ramakrishnan et al., 2014) a subset of the massive 166.4 billion molecules GDB-17 chemical database (Ruddigkeit et al., 2012). QM9 contains 133,885 organic compounds up to 9 heavy atoms: carbon (C), oxygen (O), nitrogen (N) and fluorine (F).\n\nGenerator architecture The generator architecture is fixed for all experiments. We use N = 9 as the maximum number of nodes, T = 5 as the number of atom types (C, O, N, F, and one padding symbol), and Y = 4 as the number of bond types (single, double, triple and no bond). These dimensionalities are enough to cover all molecules in QM9. The generator takes a 32-dimensional vector sampled from a standard normal distribution z \u223c N (0, I) and process it with a 3-layer MLP of [128,256,512] hidden units respectively, with tanh as activation functions. Eventually, the last layer is linearly projected to match X and A dimensions and normalized in their last dimension with a softmax operation (softmax(\nx) i = exp(x i )/ D i=1 exp(x i )).\nDiscriminator and reward network architecture Both networks use a RelationalGCN encoder (see Eq. 5) with two layers and [64, 32] hidden units, respectively, to process the input graphs. Subsequently, we compute a 128-dimensional graph-level representation (see Eq. 6) further processed by a 2-layer MLP of dimensions [128, 1] and with tanh as hidden layer activation function. In the reward network, we further use a sigmoid activation function on the output.\n\n\nEvaluation measures\n\nWe measure the following statistics as defined in Samanta et al. (2018): validity, novelty, and uniqueness. Validity is defined as the ratio between the number of valid and all generated molecules. Novelty measures the ratio between the set of valid samples that are not in the dataset and the total number of valid samples. Finally, uniqueness is defined as the ratio between the number of unique samples and valid samples and it measures the degree of variety during sampling.\n\nTraining In all experiments, we use a batch size of 32 and train using the Adam (Kingma & Ba, 2015) optimizer with a learning rate of 10 \u22123 . For each setting, we employ a grid search over dropout rates \u2208 {0.0, 0.1, 0.25} (Srivastava et al., 2014) as well as over discretization variations (as described in Section 3.1). We always report the results of the best model depending on what we are optimizing for (e.g., when optimizing solubility we report the model with the highest solubility score -when no metric is optimized we report the model with the highest sum of individual scores).\n\nAlthough the use of WGAN should prevent, to some extent, undesired behaviors like mode collapse (Salimans et al., 2016), we notice that our models suffer from that problem. We leave addressing this issue for future work. As a simple countermeasure, we employ early stopping, evaluating every 10 epochs, to avoid completely collapsed modes. In particular, we use the unique score to measure the degree of collapse of our models since it intrinsically indicates how much variety there is in the generation process. We set an arbitrary threshold of 2% under which we consider a model to be collapsed and stop training.\n\nDuring early stages of our work, we noticed that the reward network needs several epochs of pretraining before being used to propagate the gradient to the generator, otherwise the generator easily diverges. We think this happens because at the beginning of the training,R \u03c8 does not predict the reward accurately and then it does not optimize the generator well. Therefore, in each experiment, we train the generator for the first half of the epochs without the RL component, but using the WGAN objective only. We train the reward network during these epochs, but no RL loss is used to train the generator. For the second half of the epochs we use the combined loss in Equation 4.\n\n\nEffect of \u03bb\n\nAs in Guimaraes et al. (2017), the \u03bb hyperparameter controls the trade-off between maximizing the desired objective and regulating the generator to match the data distribution. We study the effects of \u03bb \u2208 {0.0, 0.01, 0.05, 0.5, 1.0} on the solubility metric (see Section 5.2 for more details). We train for 300 epochs (150 of which for pretraining) on the 5k subset of QM9 used in Guimaraes et al. (2017). We use the best \u03bb parameter -determined via the model with the maximum sum of valid, unique, novel, and solubility scores -on all other experiments (Section 5.2 and 5.3) without doing any further search.\n\n\nResults\n\nWe report results in Table 1. We observe a clear trend towards higher validity scores for lower values of \u03bb. This is likely due to the implicit optimization of valid molecules since invalid ones receive zero reward during training. Therefore, if the RL loss component is strong, the generator is optimized to generate mostly valid molecular graphs. Conversely, it appears that \u03bb does not mainly affect the unique and novel scores. Notice that these scores are not optimized, neither directly nor indirectly, and therefore they are a result of model architecture, hyperparameters, and training procedure. Indeed, the unique score is always close to 2% (which is our threshold) indicating that models appear to collapse (even in the RL only case) if we do not apply early stopping.\n\nWe also run \u03bb = 0 without starting from a pretrained model. We observe that it succeeds in optimizing toward the desired metrics, but it collapses outputting very few samples (i.e., low unique score). This behavior may indicate that pretraining is fundamental for matching the data distribution before using RL since the GAN acts regularizing towards diversity.\n\nSince \u03bb controls the trade-off between the WGAN and RL losses, it is not surprising that \u03bb = 0 (i.e., only RL in the second half of training) results in the highest valid and solubility scores compared to other values. The \u03bb value with the highest sum of scores is \u03bb = 0. We use this value for subsequent experiments. \n\n\nObjectives optimization\n\nSimilarly to the previous experiment, we train our model for 300 epochs on the 5k QM9 subset while optimizing the same objectives as Guimaraes et al. (2017) to compare against their work. Moreover, we also report results on the full dataset trained for 30 epochs (note that the full dataset is 20 times larger than the subset). All scores are normalized to lie within [0, 1]. We assign a score of zero to invalid compounds (i.e., implicitly we are also optimizing a validity score). We choose to optimize the following objectives which represent qualities typically desired for drug discovery:\n\nDruglikeness: how likely a compound is to be a drug. The Quantitative Estimate of Druglikeness (QED) score quantifies compound quality with a weighted geometric mean of desirability scores capturing the underlying data distribution of several drug properties (Bickerton et al., 2012).\n\nSolubility: the degree to which a molecule is hydrophilic. The log octanol-water partition coefficient (logP), is defined as the logarithm of the ratio of the concentrations between two solvents of a solute (Comer & Tam, 2001).\n\nSynthetizability: this measure quantifies how easy a molecule is to synthesize. The Synthetic Accessibility score (Ertl & Schuffenhauer, 2009) is a method to estimate the ease of synthesis in a probabilistic way.\n\nWe also measure, without optimizing for it, a diversity score which indicates how likely a molecule is to be diverse with respect to the dataset. This measure compares sub-structures between samples and a random subset from the dataset indicating how many repetitions there are.\n\nFor evaluation, we report average scores from 6400 sampled compounds as in (Guimaraes et al., 2017). Additionally, we re-run experiments from (Guimaraes et al., 2017) to compute unique scores and execution time since it is not reported. Differently from ORGAN, to optimize for all objectives, we do not alternate between optimizing them individually during training which in our case is not possible since the reward network is specific to a single type of reward. Thus, we instead optimize a joint reward which we define as the product (to lie within \u2208 [0, 1]) of all objectives.\n\nResults Results are reported in Table 2. Qualitative samples are provided in the Appendix (Figure 3). We observe that MolGAN models always converge to very high validity outputs > 97% at the end of the training. This is coherent as observed in the previous experiment, since also here there is an implicit optimization of validity. Moreover, in all single metrics settings, our models beat ORGAN models in terms of valid scores as well as all the three objective scores we optimize for.\n\nWe argue that this should be mainly due to two factors: i) intuitively, it should be easier to optimize a molecular graph predicted as a single sample than to optimize an RNN model that outputs a sequence of characters, and ii) using the deterministic policy gradient instead of REINFORCE effectively provides a better gradient and it improves the sampling procedure towards metrics while penalizing invalid graphs.\n\nTraining on the full QM9 dataset for 10 times fewer epochs further improves results in almost all scores. During training, our algorithm observes more different samples, and therefore it can learn well with much fewer iterations. Moreover, it can observe molecules with more diverse structures and properties.\n\nAs previously observed in Section 5.1, also in this experiment the unique score is always close to 2% confirming our hypothesis that our models are susceptible to mode collapse. This is not the case for the ORGAN baseline. During sampling, ORGAN generates sequences of maximum 51 characters which allows it to generate larger molecules whereas our model is (by choice) constrained to generate up to 9 atoms. This explain the difference in unique score since the chance of generating different molecules in a smaller space is much lower. Notice that in ORGAN, the RL component relies on REINFORCE, and the unique score is optimized penalizing non-unique outputs which we do not.\n\nIn terms of training time, our model outperforms ORGAN by a large margin when training on the 5k dataset (at least \u223c5 times faster in each setting), as we do not rely on sequential generation or discrimination. Both ORGAN and MolGAN have a comparable number of parameters, with the latter being approximately 20% larger.\n\n\nVAE Baselines\n\nIn this experiment, we compare MolGAN against recent likelihood-based methods that utilize VAEs. We report a comparison with CharacterVAE (G\u00f3mez-Bombarelli et al., 2016), GrammarVAE (Kusner et al., 2017), and GraphVAE (Simonovsky & Komodakis, 2018). Here we train using the complete QM9 dataset. Naturally, we compare only with metrics that measure the quality of the generative process since the likelihood is not computed directly in MolGAN. Moreover, we do not optimize any particular chemical property except validity (i.e., we do not optimize any metric described above, but we optimize towards chemically valid compounds). The final evaluation scores are an average from 10 4 random samples. The number of samples differs from the previous experiment to be in line with the setting in Simonovsky & Komodakis (2018).\n\nResults Results are reported in Table 3. Training on the full QM9 dataset (without optimizing any metric except validity) results in a model with a higher unique score compared to the ones in Section 5.2.\n\nThough the unique score of MolGAN is slightly higher compared to GrammarVAE, the other baselines are superior in terms of this score. Even though here we do not consider our model to be collapsed, such a low score confirms our hypothesis that our model is prone to mode collapse. On the other hand, we observe significantly higher validity scores compared to the VAE-based baselines.To verify that sampled unique molecules are (mostly) novel and not simply memorized from the dataset, we additionally measure how many of the unique molecules are also novel for our model. This score is 97% indicating that almost all unique molecules are indeed novel and MolGAN does not suffer from such problems.\n\nDifferently from our approach, VAEs optimize the evidence lower bound (ELBO) and there is no explicit nor implicit optimization of output validity. Moreover, since a part of the ELBO maximizes reconstruction of the observations, the novelty in the sampling process is not expected to be high since it is not optimized. However, in all reported methods novelty is > 60% and, in the case of CharacterVAE, 90%.\n\nThough CharacterVAE can achieve a high novelty score, it underperforms in terms of validity. MolGAN, on the other hand, achieves both high validity and novelty scores.\n\n\nConclusions\n\nIn this work, we have introduced MolGAN: an implicit generative model for molecular graphs of small size. Through joint training with a GAN and an RL objective, our model is capable of generating molecular graphs with both higher validity and novelty than previous comparable VAE-based generative models, while not requiring a permutation-dependent likelihood function. Compared to a recent SMILES-based sequential GAN model for molecular generation, MolGAN can achieve higher chemical property scores (such as solubility) while allowing for at least \u223c5x faster training time.\n\nA central limitation of our current formulation of MolGANs is their susceptibility to mode collapse: both the GAN and the RL objective do not encourage generation of diverse and non-unique outputs whereby the model tends to be pulled towards a solution that only involves little sample variability. This ultimately results in the generation of only a handful of different molecules if training is not stopped early.\n\nWe think that this issue can be addressed in future work, for example via careful design of reward functions or some form of pretraining. The MolGAN framework taken together with established benchmark datasets for chemical synthesis offer a new test bed for improvements on GAN stability with respect to the issue of mode collapse. We believe that insights gained from such evaluations will be valuable to the community even outside of the scope of generating molecular graphs. Lastly, it will be promising to explore alternative generative architectures within the Mol-GAN framework, such as recurrent graph-based generative models (Johnson, 2017;Li et al., 2018b;You et al., 2018), as our current one-shot prediction of the adjacency tensor is most likely feasible only for graphs of small size.   . Samples from the QM9 dataset (left) and MolGAN trained to optimize druglikeliness (QED) on the 5k QM9 subset (right). We also report their relative QED scores.\n\n\n;Li et al. (2018b);You et al. (2018);Li et al. (2018a) on the other hand developed likelihood-based methods to directly output graphs of arbitrary size in a sequential manner. Several related works have explored extending VAEs to generate graphs directly, examples include the GraphVAE(Simonovsky & Komodakis, 2018), Junction Tree VAE(Jin et al., 2018) and the NeVAE(Samanta et al., 2018) model.\n\nFigure 3\n3Figure 3. Samples from the QM9 dataset (left) and MolGAN trained to optimize druglikeliness (QED) on the 5k QM9 subset (right). We also report their relative QED scores.\n\n\nTable 3. Comparison with different algorithms on QM9. Values are reported in percentages. Baseline results are taken from Simonovsky & Komodakis (2018).Algorithm \nValid Unique Novel \n\nCharacterVAE \n10.3 \n67.5 \n90.0 \nGrammarVAE \n60.2 \n9.3 \n80.9 \nGraphVAE \n55.7 \n76.0 \n61.6 \nGraphVAE/imp \n56.2 \n42.0 \n75.8 \nGraphVAE NoGM 81.0 \n24.1 \n61.0 \n\nMolGAN \n98.1 \n10.4 \n94.2 \n\n\nInformatics Institute, University of Amsterdam, Amsterdam, The Netherlands. Correspondence to: Nicola De Cao <nicola.decao@gmail.com>.\nWe used the RDKit Open-Source Cheminformatics Software: http://www.rdkit.org.\nAcknowledgementsThe authors would like to thank Luca Falorsi, Tim R. Davidson, Herke van Hoof and Max Welling for helpful discussions and feedback. T.K. is supported by SAP SE Berlin.\nWasserstein generative adversarial networks. Mart\u00edn Arjovsky, Soumith Chintala, L\u00e9on Bottou, PMLRProceedings of the 34th International Conference on Machine Learning. Precup, Doina and Teh, Yee Whyethe 34th International Conference on Machine LearningSydney, NSW, Australia70Arjovsky, Mart\u00edn, Chintala, Soumith, and Bottou, L\u00e9on. Wasserstein generative adversarial networks. In Pre- cup, Doina and Teh, Yee Whye (eds.), Proceed- ings of the 34th International Conference on Ma- chine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 214-223. PMLR, 2017. URL http://proceedings.mlr.press/ v70/arjovsky17a.html.\n\nQuantifying the chemical beauty of drugs. G Bickerton, Richard, Paolini, V Gaia, Besnard, J\u00e9r\u00e9my, Sorel Muresan, Andrew L Hopkins, Nature chemistry. 4290Bickerton, G Richard, Paolini, Gaia V, Besnard, J\u00e9r\u00e9my, Muresan, Sorel, and Hopkins, Andrew L. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90, 2012.\n\nNetgan: Generating graphs via random walks. Aleksandar Bojchevski, Shchur, Oleksandr, Daniel Z\u00fcgner, Stephan G\u00fcnnemann, PMLRProceedings of the 35th International Conference on Machine Learning. Dy, Jennifer G. and Krause, Andreasthe 35th International Conference on Machine LearningStockholmsm\u00e4ssan, Stockholm, Sweden80Bojchevski, Aleksandar, Shchur, Oleksandr, Z\u00fcgner, Daniel, and G\u00fcnnemann, Stephan. Netgan: Generating graphs via random walks. In Dy, Jennifer G. and Krause, Andreas (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Pro- ceedings of Machine Learning Research, pp. 609-618. PMLR, 2018. URL http://proceedings.mlr. press/v80/bojchevski18a.html.\n\nGeometric deep learning: going beyond euclidean data. Michael M Bronstein, Joan Bruna, Lecun, Yann, Arthur Szlam, Pierre Vandergheynst, IEEE Signal Processing Magazine34Bronstein, Michael M, Bruna, Joan, LeCun, Yann, Szlam, Arthur, and Vandergheynst, Pierre. Geometric deep learn- ing: going beyond euclidean data. IEEE Signal Process- ing Magazine, 34(4):18-42, 2017.\n\nSpectral networks and locally connected networks on graphs. Joan Bruna, Zaremba, Wojciech, Arthur Szlam, Yann Le-Cun, 2nd International Conference on Learning Representations. Bengio, Yoshua and LeCun, YannBanff, AB, CanadaConference Track ProceedingsBruna, Joan, Zaremba, Wojciech, Szlam, Arthur, and Le- Cun, Yann. Spectral networks and locally connected networks on graphs. In Bengio, Yoshua and LeCun, Yann (eds.), 2nd International Conference on Learning Rep- resentations, ICLR 2014, Banff, AB, Canada, April 14- 16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6203.\n\nLipophilicity profiles: theory and measurement. John Comer, Kin Tam, Wiley-VCHZ\u00fcrich, SwitzerlandComer, John and Tam, Kin. Lipophilicity profiles: the- ory and measurement. Wiley-VCH: Z\u00fcrich, Switzerland, 2001.\n\nSyntax-directed variational autoencoder for molecule generation. Hanjun Dai, Tian, Yingtao, Dai, Bo, Steven Skiena, Le Song, International Conference on Machine Learning. Dai, Hanjun, Tian, Yingtao, Dai, Bo, Skiena, Steven, and Song, Le. Syntax-directed variational autoencoder for molecule generation. In International Conference on Machine Learning, 2018.\n\nHyperspherical variational auto-encoders. Tim R Davidson, Falorsi, Luca, Nicola Cao, De, Thomas Kipf, Tomczak, M Jakub, Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018. Globerson, Amir and Silva, Ricardothe Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018Monterey, California, USAAUAI PressDavidson, Tim R., Falorsi, Luca, Cao, Nicola De, Kipf, Thomas, and Tomczak, Jakub M. Hyperspherical vari- ational auto-encoders. In Globerson, Amir and Silva, Ricardo (eds.), Proceedings of the Thirty-Fourth Confer- ence on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018, pp. 856- 865. AUAI Press, 2018. URL http://auai.org/ uai2018/proceedings/papers/309.pdf.\n\nConvolutional networks on graphs for learning molecular fingerprints. David Duvenaud, Maclaurin, Dougal, Jorge Aguilera-Iparraguirre, G\u00f3mez-Bombarelli, Rafael, Timothy Hirzel, Al\u00e1n Aspuru-Guzik, Adams , Ryan P , Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Cortes, Corinna, Lawrence, Neil D., Lee, Daniel D., Sugiyama, Masashi, and Garnett, RomanMontreal, Quebec, CanadaDuvenaud, David, Maclaurin, Dougal, Aguilera- Iparraguirre, Jorge, G\u00f3mez-Bombarelli, Rafael, Hirzel, Timothy, Aspuru-Guzik, Al\u00e1n, and Adams, Ryan P. Convolutional networks on graphs for learning molecular fingerprints. In Cortes, Corinna, Lawrence, Neil D., Lee, Daniel D., Sugiyama, Masashi, and Garnett, Roman (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 2224- 2232, 2015. URL https://proceedings.\n\nEstimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Peter Ertl, Ansgar Schuffenhauer, Journal of cheminformatics. 11Ertl, Peter and Schuffenhauer, Ansgar. Estimation of syn- thetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Jour- nal of cheminformatics, 1(1):8, 2009.\n\nAutomatic chemical design using a data-driven continuous representation of molecules. G\u00f3mez-Bombarelli, Rafael, Jennifer N Wei, Duvenaud, David, Jos\u00e9 Hern\u00e1ndez-Lobato, Miguel, S\u00e1nchez-Lengeling, Benjam\u00edn, Sheberla, Dennis, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Adams, P Ryan, Aspuru-Guzik , Al\u00e1n , ACS Central Science. G\u00f3mez-Bombarelli, Rafael, Wei, Jennifer N, Duve- naud, David, Hern\u00e1ndez-Lobato, Jos\u00e9 Miguel, S\u00e1nchez- Lengeling, Benjam\u00edn, Sheberla, Dennis, Aguilera- Iparraguirre, Jorge, Hirzel, Timothy D, Adams, Ryan P, and Aspuru-Guzik, Al\u00e1n. Automatic chemical design us- ing a data-driven continuous representation of molecules. ACS Central Science, 2016.\n\nGenerative adversarial nets. Ian J Goodfellow, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Warde - Bing, Farley, David, Ozair, Sherjil, Aaron C Courville, Bengio, Yoshua ; Zoubin, Welling, Max, Corinna Cortes, Neil D Lawrence, Weinberger , Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Kilian Q.Montreal, Quebec, CanadaGhahramaniGoodfellow, Ian J., Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron C., and Bengio, Yoshua. Generative adver- sarial nets. In Ghahramani, Zoubin, Welling, Max, Cortes, Corinna, Lawrence, Neil D., and Weinberger, Kilian Q. (eds.), Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 2672- 2680, 2014. URL https://proceedings.\n\nGraphite: Iterative generative modeling of graphs. Aditya Grover, Aaron Zweig, Stefano Ermon, PMLRProceedings of the 36th International Conference on Machine Learning, ICML 2019. Chaudhuri, Kamalika and Salakhutdinov, Ruslanthe 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97Grover, Aditya, Zweig, Aaron, and Ermon, Stefano. Graphite: Iterative generative modeling of graphs. In Chaudhuri, Kamalika and Salakhutdinov, Ruslan (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2434-2444. PMLR, 2019. URL http://proceedings.mlr.press/ v97/grover19a.html.\n\nObjective-reinforced generative adversarial networks (organ) for sequence generation models. Gabriel Guimaraes, Lima, Sanchez-Lengeling, Benjamin, Pedro Luis Farias, Aspuru-Guzik Cunha, Al\u00e1n , abs/1705.10843ArXiv preprintGuimaraes, Gabriel Lima, Sanchez-Lengeling, Benjamin, Farias, Pedro Luis Cunha, and Aspuru-Guzik, Al\u00e1n. Objective-reinforced generative adversarial networks (or- gan) for sequence generation models. ArXiv preprint, abs/1705.10843, 2017. URL https://arxiv.org/ abs/1705.10843.\n\nImproved training of wasserstein gans. Gulrajani, Ishaan, Ahmed, Faruk, Arjovsky, Mart\u00edn, Vincent Dumoulin, Aaron C Courville, Guyon. Gulrajani, Ishaan, Ahmed, Faruk, Arjovsky, Mart\u00edn, Dumoulin, Vincent, and Courville, Aaron C. Im- proved training of wasserstein gans. In Guyon,\n\nVon Isabelle, Luxburg, Ulrike, Bengio, Samy, Wallach, M Hanna, Fergus, Rob, S V N Vishwanathan, Roman Garnett, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USAIsabelle, von Luxburg, Ulrike, Bengio, Samy, Wallach, Hanna M., Fergus, Rob, Vishwanathan, S. V. N., and Garnett, Roman (eds.), Advances in Neural Infor- mation Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5767- 5777, 2017. URL https://proceedings.\n\nWilliam L Hamilton, Rex Ying, Jure Leskovec, abs/1709.05584Representation learning on graphs: Methods and applications. ArXiv preprint. Hamilton, William L, Ying, Rex, and Leskovec, Jure. Rep- resentation learning on graphs: Methods and applica- tions. ArXiv preprint, abs/1709.05584, 2017. URL https://arxiv.org/abs/1709.05584.\n\nCategorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netJang, Eric, Gu, Shixiang, and Poole, Ben. Categorical reparameterization with gumbel-softmax. In 5th Inter- national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=rkE3y85ee.\n\nJunction tree variational autoencoder for molecular graph generation. Wengong Jin, Regina Barzilay, Tommi S Jaakkola, PMLRProceedings of the 35th International Conference on Machine Learning. Dy, Jennifer G. and Krause, Andreasthe 35th International Conference on Machine LearningStockholmsm\u00e4ssan, Stockholm, Sweden80Jin, Wengong, Barzilay, Regina, and Jaakkola, Tommi S. Junction tree variational autoencoder for molecular graph generation. In Dy, Jennifer G. and Krause, Andreas (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Pro- ceedings of Machine Learning Research, pp. 2328-2337. PMLR, 2018. URL http://proceedings.mlr. press/v80/jin18a.html.\n\nLearning graphical state transitions. Daniel D Johnson, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netJohnson, Daniel D. Learning graphical state transitions. In 5th International Conference on Learning Repre- sentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum? id=HJ0NvFzxl.\n\nA method for stochastic optimization. Diederik P Kingma, Jimmy Ba, Adam, 3rd International Conference on Learning Representations. Bengio, Yoshua and LeCun, YannSan Diego, CA, USAConference Track ProceedingsKingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. In Bengio, Yoshua and LeCun, Yann (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\n\nAuto-encoding variational bayes. Diederik P Kingma, Max Welling, 2nd International Conference on Learning Representations. Bengio, Yoshua and LeCun, YannBanff, AB, CanadaConference Track ProceedingsKingma, Diederik P. and Welling, Max. Auto-encoding variational bayes. In Bengio, Yoshua and LeCun, Yann (eds.), 2nd International Conference on Learning Rep- resentations, ICLR 2014, Banff, AB, Canada, April 14- 16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6114.\n\nVariational graph autoencoders. Thomas N Kipf, Max Welling, NIPS Bayesian Deep Learning Workshop. Kipf, Thomas N and Welling, Max. Variational graph auto- encoders. In NIPS Bayesian Deep Learning Workshop, 2016.\n\nSemi-supervised classification with graph convolutional networks. Thomas N Kipf, Max Welling, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netKipf, Thomas N. and Welling, Max. Semi-supervised classi- fication with graph convolutional networks. In 5th Inter- national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=SJU4ayYgl.\n\nGrammar variational autoencoder. Matt J Kusner, Brooks Paige, Jos\u00e9 Hern\u00e1ndez-Lobato, Miguel, PMLRProceedings of the 34th International Conference on Machine Learning. Precup, Doina and Teh, Yee Whyethe 34th International Conference on Machine LearningSydney, NSW, Australia70Kusner, Matt J., Paige, Brooks, and Hern\u00e1ndez-Lobato, Jos\u00e9 Miguel. Grammar variational autoencoder. In Precup, Doina and Teh, Yee Whye (eds.), Proceed- ings of the 34th International Conference on Ma- chine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 1945-1954. PMLR, 2017. URL http://proceedings.mlr.press/ v70/kusner17a.html.\n\nMultiobjective de novo drug design with conditional graph generative model. Yibo Li, Liangren Zhang, Zhenming Liu, abs/1801.07299ArXiv preprintLi, Yibo, Zhang, Liangren, and Liu, Zhenming. Multi- objective de novo drug design with conditional graph generative model. ArXiv preprint, abs/1801.07299, 2018a. URL https://arxiv.org/abs/1801.07299.\n\nGated graph sequence neural networks. Yujia Li, Tarlow, Daniel, Marc Brockschmidt, Richard S Zemel, 4th International Conference on Learning Representations, ICLR 2016. Bengio, Yoshua and LeCun, YannSan Juan, Puerto RicoConference Track ProceedingsLi, Yujia, Tarlow, Daniel, Brockschmidt, Marc, and Zemel, Richard S. Gated graph sequence neural networks. In Bengio, Yoshua and LeCun, Yann (eds.), 4th Interna- tional Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/ abs/1511.05493.\n\nLearning deep generative models of graphs. Yujia Li, Vinyals, Oriol, Chris Dyer, Razvan Pascanu, Peter Battaglia, abs/1803.03324ArXiv preprintLi, Yujia, Vinyals, Oriol, Dyer, Chris, Pascanu, Razvan, and Battaglia, Peter. Learning deep generative models of graphs. ArXiv preprint, abs/1803.03324, 2018b. URL https://arxiv.org/abs/1803.03324.\n\nContinuous control with deep reinforcement learning. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Heess, Nicolas, Erez, Tom, Tassa, David Silver, Daan Wierstra, 4th International Conference on Learning Representations. Bengio, Yoshua and LeCun, YannSan Juan, Puerto RicoConference Track ProceedingsLillicrap, Timothy P., Hunt, Jonathan J., Pritzel, Alexander, Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David, and Wierstra, Daan. Continuous control with deep re- inforcement learning. In Bengio, Yoshua and LeCun, Yann (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1509.02971.\n\nThe concrete distribution: A continuous relaxation of discrete random variables. Chris J Maddison, Andriy Mnih, Yee Teh, Whye, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netMaddison, Chris J., Mnih, Andriy, and Teh, Yee Whye. The concrete distribution: A continuous relaxation of discrete random variables. In 5th International Confer- ence on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview. net/forum?id=S1jE5L5gl.\n\nAdversarial sets for regularising neural link predictors. Pasquale Minervini, Demeester, Thomas, Tim Rockt\u00e4schel, Sebastian Riedel, Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence. Elidan, Gal, Kersting, Kristian, and Ihler, Alexander T.the Thirty-Third Conference on Uncertainty in Artificial IntelligenceSydney, AustraliaAUAI PressMinervini, Pasquale, Demeester, Thomas, Rockt\u00e4schel, Tim, and Riedel, Sebastian. Adversarial sets for regularising neural link predictors. In Elidan, Gal, Kersting, Kristian, and Ihler, Alexander T. (eds.), Proceedings of the Thirty- Third Conference on Uncertainty in Artificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017. AUAI Press, 2017. URL http://auai.org/uai2017/ proceedings/papers/306.pdf.\n\nQuantum chemistry structures and properties of 134 kilo molecules. Raghunathan Ramakrishnan, Dral, O Pavlo, Matthias Rupp, Von Lilienfeld, Anatole , Scientific data. 1140022Ramakrishnan, Raghunathan, Dral, Pavlo O, Rupp, Matthias, and Von Lilienfeld, O Anatole. Quantum chem- istry structures and properties of 134 kilo molecules. Sci- entific data, 1:140022, 2014.\n\nStochastic backpropagation and approximate inference in deep generative models. Danilo Rezende, Jimenez, Shakir Mohamed, Daan Wierstra, Proceedings of the 31th International Conference on Machine Learning. the 31th International Conference on Machine LearningBeijing, China32JMLR Workshop and Conference ProceedingsRezende, Danilo Jimenez, Mohamed, Shakir, and Wier- stra, Daan. Stochastic backpropagation and approx- imate inference in deep generative models. In Pro- ceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21- 26 June 2014, volume 32 of JMLR Workshop and Conference Proceedings, pp. 1278-1286. JMLR.org, 2014. URL http://proceedings.mlr.press/ v32/rezende14.html.\n\nEnumeration of 166 billion organic small molecules in the chemical universe database gdb-17. Lars Ruddigkeit, Van Deursen, Ruud, Blum, C Lorenz, Reymond , Jean-Louis , Journal of chemical information and modeling. 5211Ruddigkeit, Lars, Van Deursen, Ruud, Blum, Lorenz C, and Reymond, Jean-Louis. Enumeration of 166 billion or- ganic small molecules in the chemical universe database gdb-17. Journal of chemical information and modeling, 52(11):2864-2875, 2012.\n\nImproved techniques for training gans. Tim Salimans, Ian J Goodfellow, Zaremba, Wojciech, Cheung, Vicki, Alec Radford, Xi Chen, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems. Lee, Daniel D., Sugiyama, Masashi, von Luxburg, Ulrike, Guyon, Isabelle, and Garnett, RomanBarcelona, SpainSalimans, Tim, Goodfellow, Ian J., Zaremba, Wojciech, Cheung, Vicki, Radford, Alec, and Chen, Xi. Improved techniques for training gans. In Lee, Daniel D., Sugiyama, Masashi, von Luxburg, Ulrike, Guyon, Isabelle, and Garnett, Roman (eds.), Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 2226- 2234, 2016. URL https://proceedings.\n\nDesigning random graph models using variational autoencoders with applications to chemical design. ArXiv preprint. Samanta, Bidisha, De, Abir, Niloy Ganguly, Manuel Gomez-Rodriguez, abs/1802.05283Samanta, Bidisha, De, Abir, Ganguly, Niloy, and Gomez- Rodriguez, Manuel. Designing random graph models using variational autoencoders with applications to chem- ical design. ArXiv preprint, abs/1802.05283, 2018. URL https://arxiv.org/abs/1802.05283.\n\nModeling relational data with graph convolutional networks. ArXiv preprint. Michael Schlichtkrull, Thomas N Kipf, Bloem, Peter, Rianne Berg, Van Den, Ivan Titov, Max Welling, abs/1703.06103Schlichtkrull, Michael, Kipf, Thomas N, Bloem, Peter, Berg, Rianne van den, Titov, Ivan, and Welling, Max. Modeling relational data with graph convolutional net- works. ArXiv preprint, abs/1703.06103, 2017. URL https://arxiv.org/abs/1703.06103.\n\nComputer-based de novo design of drug-like molecules. Gisbert Schneider, Uli Fechner, Nature Reviews Drug Discovery. 48649Schneider, Gisbert and Fechner, Uli. Computer-based de novo design of drug-like molecules. Nature Reviews Drug Discovery, 4(8):649, 2005.\n\nPlanning chemical syntheses with deep neural networks and symbolic ai. Marwin Hs Segler, Mike Preuss, Waller, P Mark, Nature. 5557698604Segler, Marwin HS, Preuss, Mike, and Waller, Mark P. Plan- ning chemical syntheses with deep neural networks and symbolic ai. Nature, 555(7698):604, 2018.\n\nDeterministic policy gradient algorithms. David Silver, Lever, Guy, Heess, Nicolas, Degris, Thomas, Daan Wierstra, Riedmiller, A Martin, Proceedings of the 31th International Conference on Machine Learning. the 31th International Conference on Machine LearningBeijing, China32JMLR Workshop and Conference ProceedingsSilver, David, Lever, Guy, Heess, Nicolas, Degris, Thomas, Wierstra, Daan, and Riedmiller, Martin A. Deterministic policy gradient algorithms. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, volume 32 of JMLR Workshop and Conference Proceedings, pp. 387- 395. JMLR.org, 2014. URL http://proceedings. mlr.press/v32/silver14.html.\n\nGraphvae: Towards generation of small graphs using variational autoencoders. ArXiv preprint. Martin Simonovsky, Nikos Komodakis, abs/1802.03480Simonovsky, Martin and Komodakis, Nikos. Graphvae: Towards generation of small graphs using variational au- toencoders. ArXiv preprint, abs/1802.03480, 2018. URL https://arxiv.org/abs/1802.03480.\n\nDropout: A simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, The Journal of Machine Learning Research. 151Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A simple way to prevent neural networks from overfit- ting. The Journal of Machine Learning Research, 15(1): 1929-1958, 2014.\n\nGraphgan: Graph representation learning with generative adversarial nets. Wang, Hongwei, Wang, Jia, Wang, Jialin, Zhao, Miao, Zhang, Weinan, Zhang, Fuzheng, Xing Xie, Minyi Guo, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). McIlraith, Sheila A. and Weinberger, Kilian Q.the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI PressWang, Hongwei, Wang, Jia, Wang, Jialin, Zhao, Miao, Zhang, Weinan, Zhang, Fuzheng, Xie, Xing, and Guo, Minyi. Graphgan: Graph representation learning with generative adversarial nets. In McIlraith, Sheila A. and Weinberger, Kilian Q. (eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 2508-2515. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/ AAAI/AAAI18/paper/view/16611.\n\nSmiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of chemical information and computer sciences. 281Weininger, David. Smiles, a chemical language and in- formation system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31-36, 1988.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. Ronald J Williams, Reinforcement Learning. SpringerWilliams, Ronald J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Reinforcement Learning, pp. 5-32. Springer, 1992.\n\nGraphrnn: A deep generative model for graphs. Jiaxuan You, Rex Ying, Ren, Xiang, Hamilton, L William, Jure Leskovec, International Conference on Machine Learning. You, Jiaxuan, Ying, Rex, Ren, Xiang, Hamilton, William L, and Leskovec, Jure. Graphrnn: A deep generative model for graphs. In International Conference on Machine Learning, 2018.\n\nSeqgan: Sequence generative adversarial nets with policy gradient. Lantao Yu, Zhang, Weinan, Jun Wang, Yong Yu, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. Singh, Satinder P. and Markovitch, Shaulthe Thirty-First AAAI Conference on Artificial IntelligenceSan Francisco, California, USAAAAI PressYu, Lantao, Zhang, Weinan, Wang, Jun, and Yu, Yong. Se- qgan: Sequence generative adversarial nets with policy gradient. In Singh, Satinder P. and Markovitch, Shaul (eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Fran- cisco, California, USA, pp. 2852-2858. AAAI Press, 2017. URL http://aaai.org/ocs/index.php/ AAAI/AAAI17/paper/view/14344.\n", "annotations": {"author": "[{\"end\":81,\"start\":67},{\"end\":94,\"start\":82}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":74},{\"end\":93,\"start\":89}]", "author_first_name": "[{\"end\":73,\"start\":67},{\"end\":88,\"start\":82}]", "author_affiliation": null, "title": "[{\"end\":64,\"start\":1},{\"end\":158,\"start\":95}]", "venue": null, "abstract": "[{\"end\":1277,\"start\":160}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1453,\"start\":1426},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2191,\"start\":2171},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2214,\"start\":2191},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2230,\"start\":2214},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2295,\"start\":2278},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3093,\"start\":3069},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3115,\"start\":3093},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3286,\"start\":3264},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3300,\"start\":3286},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3320,\"start\":3300},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3337,\"start\":3320},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3366,\"start\":3337},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3383,\"start\":3366},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3501,\"start\":3483},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3530,\"start\":3501},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3986,\"start\":3961},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4144,\"start\":4120},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4410,\"start\":4385},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4793,\"start\":4773},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4815,\"start\":4793},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4842,\"start\":4815},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5029,\"start\":4998},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5049,\"start\":5029},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5072,\"start\":5049},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5089,\"start\":5072},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5337,\"start\":5316},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5586,\"start\":5568},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5615,\"start\":5586},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6467,\"start\":6443},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6488,\"start\":6467},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6616,\"start\":6591},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6883,\"start\":6853},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7219,\"start\":7199},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7241,\"start\":7219},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7292,\"start\":7275},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7625,\"start\":7600},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8523,\"start\":8500},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8606,\"start\":8583},{\"end\":8649,\"start\":8625},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8779,\"start\":8756},{\"end\":9281,\"start\":9258},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10818,\"start\":10802},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11205,\"start\":11184},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11335,\"start\":11312},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":30733,\"start\":30705},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31177,\"start\":31162},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":31194,\"start\":31177},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31211,\"start\":31194},{\"end\":31213,\"start\":31211},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31395,\"start\":31366},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32685,\"start\":32666},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32707,\"start\":32685},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33321,\"start\":33293},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34090,\"start\":34074},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34689,\"start\":34666},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":34773,\"start\":34756},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":34890,\"start\":34874},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":35095,\"start\":35075},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":35398,\"start\":35367},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35523,\"start\":35502},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":35552,\"start\":35534},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":35882,\"start\":35860},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35902,\"start\":35882},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35924,\"start\":35902},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36070,\"start\":36046},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36088,\"start\":36070},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":36112,\"start\":36088},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":36432,\"start\":36405},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36646,\"start\":36622},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":36986,\"start\":36955},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37020,\"start\":36999},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":37111,\"start\":37081},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37181,\"start\":37154},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":37280,\"start\":37255},{\"end\":37879,\"start\":37874},{\"end\":37883,\"start\":37879},{\"end\":37887,\"start\":37883},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":38691,\"start\":38670},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":39199,\"start\":39180},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":39347,\"start\":39322},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":39809,\"start\":39786},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":41032,\"start\":41009},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":41407,\"start\":41384},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":43270,\"start\":43247},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":43992,\"start\":43968},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":44221,\"start\":44202},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":44365,\"start\":44338},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":44817,\"start\":44793},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":44884,\"start\":44860},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47736,\"start\":47715},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":47781,\"start\":47751},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":48353,\"start\":48324},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":51496,\"start\":51481},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":51513,\"start\":51496},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":51530,\"start\":51513},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":51830,\"start\":51813},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":51848,\"start\":51831},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":51866,\"start\":51849},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":52127,\"start\":52097},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":52164,\"start\":52146},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":52200,\"start\":52178}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":52207,\"start\":51810},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52388,\"start\":52208},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":52756,\"start\":52389}]", "paragraph": "[{\"end\":1590,\"start\":1293},{\"end\":3384,\"start\":1592},{\"end\":3820,\"start\":3386},{\"end\":4213,\"start\":3822},{\"end\":4905,\"start\":4215},{\"end\":5431,\"start\":4942},{\"end\":5721,\"start\":5433},{\"end\":6334,\"start\":5723},{\"end\":6960,\"start\":6376},{\"end\":7561,\"start\":6962},{\"end\":7766,\"start\":7563},{\"end\":8253,\"start\":7768},{\"end\":8734,\"start\":8335},{\"end\":9019,\"start\":8736},{\"end\":9592,\"start\":9081},{\"end\":9840,\"start\":9726},{\"end\":10439,\"start\":9988},{\"end\":10748,\"start\":10441},{\"end\":11474,\"start\":10750},{\"end\":12268,\"start\":11476},{\"end\":12412,\"start\":12278},{\"end\":12888,\"start\":12414},{\"end\":13555,\"start\":12890},{\"end\":14548,\"start\":13557},{\"end\":15569,\"start\":14974},{\"end\":16590,\"start\":15995},{\"end\":17603,\"start\":17016},{\"end\":24422,\"start\":23859},{\"end\":25464,\"start\":24901},{\"end\":28146,\"start\":25943},{\"end\":30763,\"start\":29628},{\"end\":30905,\"start\":30813},{\"end\":31528,\"start\":30919},{\"end\":32102,\"start\":31530},{\"end\":32917,\"start\":32104},{\"end\":33483,\"start\":32954},{\"end\":34002,\"start\":33608},{\"end\":34163,\"start\":34004},{\"end\":34585,\"start\":34241},{\"end\":35251,\"start\":34602},{\"end\":35634,\"start\":35253},{\"end\":36246,\"start\":35636},{\"end\":37112,\"start\":36262},{\"end\":37396,\"start\":37114},{\"end\":38100,\"start\":37398},{\"end\":38596,\"start\":38137},{\"end\":39098,\"start\":38620},{\"end\":39688,\"start\":39100},{\"end\":40305,\"start\":39690},{\"end\":40987,\"start\":40307},{\"end\":41612,\"start\":41003},{\"end\":42403,\"start\":41624},{\"end\":42766,\"start\":42405},{\"end\":43086,\"start\":42768},{\"end\":43707,\"start\":43114},{\"end\":43993,\"start\":43709},{\"end\":44222,\"start\":43995},{\"end\":44436,\"start\":44224},{\"end\":44716,\"start\":44438},{\"end\":45298,\"start\":44718},{\"end\":45786,\"start\":45300},{\"end\":46203,\"start\":45788},{\"end\":46514,\"start\":46205},{\"end\":47193,\"start\":46516},{\"end\":47515,\"start\":47195},{\"end\":48354,\"start\":47533},{\"end\":48560,\"start\":48356},{\"end\":49259,\"start\":48562},{\"end\":49668,\"start\":49261},{\"end\":49837,\"start\":49670},{\"end\":50429,\"start\":49853},{\"end\":50846,\"start\":50431},{\"end\":51809,\"start\":50848}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8334,\"start\":8254},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9080,\"start\":9020},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9725,\"start\":9593},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9954,\"start\":9841},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23858,\"start\":17604},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24900,\"start\":24423},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25942,\"start\":25465},{\"attributes\":{\"id\":\"formula_7\"},\"end\":30812,\"start\":30764},{\"attributes\":{\"id\":\"formula_8\"},\"end\":33607,\"start\":33484},{\"attributes\":{\"id\":\"formula_9\"},\"end\":34240,\"start\":34164},{\"attributes\":{\"id\":\"formula_10\"},\"end\":38136,\"start\":38101}]", "table_ref": "[{\"end\":41652,\"start\":41645},{\"end\":45339,\"start\":45332},{\"end\":48395,\"start\":48388}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1291,\"start\":1279},{\"attributes\":{\"n\":\"2.\"},\"end\":4918,\"start\":4908},{\"attributes\":{\"n\":\"2.1.\"},\"end\":4940,\"start\":4921},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6374,\"start\":6337},{\"attributes\":{\"n\":\"2.3.\"},\"end\":9986,\"start\":9956},{\"attributes\":{\"n\":\"3.\"},\"end\":12276,\"start\":12271},{\"end\":14972,\"start\":14551},{\"end\":15993,\"start\":15572},{\"end\":17014,\"start\":16593},{\"end\":29626,\"start\":28149},{\"attributes\":{\"n\":\"3.1.\"},\"end\":30917,\"start\":30908},{\"attributes\":{\"n\":\"3.2.\"},\"end\":32952,\"start\":32920},{\"attributes\":{\"n\":\"4.\"},\"end\":34600,\"start\":34588},{\"attributes\":{\"n\":\"5.\"},\"end\":36260,\"start\":36249},{\"end\":38618,\"start\":38599},{\"attributes\":{\"n\":\"5.1.\"},\"end\":41001,\"start\":40990},{\"end\":41622,\"start\":41615},{\"attributes\":{\"n\":\"5.2.\"},\"end\":43112,\"start\":43089},{\"attributes\":{\"n\":\"5.3.\"},\"end\":47531,\"start\":47518},{\"attributes\":{\"n\":\"6.\"},\"end\":49851,\"start\":49840},{\"end\":52217,\"start\":52209}]", "table": "[{\"end\":52756,\"start\":52543}]", "figure_caption": "[{\"end\":52207,\"start\":51812},{\"end\":52388,\"start\":52219},{\"end\":52543,\"start\":52391}]", "figure_ref": "[{\"end\":1801,\"start\":1793},{\"end\":4270,\"start\":4261},{\"end\":12312,\"start\":12304},{\"end\":30176,\"start\":30168},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":45399,\"start\":45390}]", "bib_author_first_name": "[{\"end\":53205,\"start\":53199},{\"end\":53223,\"start\":53216},{\"end\":53238,\"start\":53234},{\"end\":53881,\"start\":53880},{\"end\":53912,\"start\":53911},{\"end\":53941,\"start\":53936},{\"end\":53957,\"start\":53951},{\"end\":53959,\"start\":53958},{\"end\":54216,\"start\":54206},{\"end\":54254,\"start\":54248},{\"end\":54270,\"start\":54263},{\"end\":54993,\"start\":54986},{\"end\":54995,\"start\":54994},{\"end\":55011,\"start\":55007},{\"end\":55038,\"start\":55032},{\"end\":55052,\"start\":55046},{\"end\":55366,\"start\":55362},{\"end\":55399,\"start\":55393},{\"end\":55411,\"start\":55407},{\"end\":55956,\"start\":55952},{\"end\":55967,\"start\":55964},{\"end\":56187,\"start\":56181},{\"end\":56223,\"start\":56217},{\"end\":56234,\"start\":56232},{\"end\":56520,\"start\":56517},{\"end\":56522,\"start\":56521},{\"end\":56554,\"start\":56548},{\"end\":56570,\"start\":56564},{\"end\":56587,\"start\":56586},{\"end\":57326,\"start\":57321},{\"end\":57361,\"start\":57356},{\"end\":57418,\"start\":57411},{\"end\":57431,\"start\":57427},{\"end\":57451,\"start\":57446},{\"end\":57458,\"start\":57454},{\"end\":57460,\"start\":57459},{\"end\":58354,\"start\":58349},{\"end\":58367,\"start\":58361},{\"end\":58744,\"start\":58736},{\"end\":58746,\"start\":58745},{\"end\":58773,\"start\":58769},{\"end\":58852,\"start\":58847},{\"end\":58883,\"start\":58876},{\"end\":58885,\"start\":58884},{\"end\":58902,\"start\":58901},{\"end\":58921,\"start\":58909},{\"end\":58928,\"start\":58924},{\"end\":59330,\"start\":59327},{\"end\":59332,\"start\":59331},{\"end\":59389,\"start\":59384},{\"end\":59391,\"start\":59390},{\"end\":59434,\"start\":59429},{\"end\":59436,\"start\":59435},{\"end\":59494,\"start\":59487},{\"end\":59507,\"start\":59503},{\"end\":59509,\"start\":59508},{\"end\":59530,\"start\":59520},{\"end\":60241,\"start\":60235},{\"end\":60255,\"start\":60250},{\"end\":60270,\"start\":60263},{\"end\":61022,\"start\":61015},{\"end\":61074,\"start\":61069},{\"end\":61079,\"start\":61075},{\"end\":61100,\"start\":61088},{\"end\":61112,\"start\":61108},{\"end\":61517,\"start\":61510},{\"end\":61533,\"start\":61528},{\"end\":61535,\"start\":61534},{\"end\":61703,\"start\":61700},{\"end\":61755,\"start\":61754},{\"end\":61777,\"start\":61776},{\"end\":61781,\"start\":61778},{\"end\":61801,\"start\":61796},{\"end\":62289,\"start\":62282},{\"end\":62291,\"start\":62290},{\"end\":62305,\"start\":62302},{\"end\":62316,\"start\":62312},{\"end\":62668,\"start\":62664},{\"end\":62683,\"start\":62675},{\"end\":62691,\"start\":62688},{\"end\":63196,\"start\":63189},{\"end\":63208,\"start\":63202},{\"end\":63224,\"start\":63219},{\"end\":63226,\"start\":63225},{\"end\":63924,\"start\":63918},{\"end\":63926,\"start\":63925},{\"end\":64365,\"start\":64357},{\"end\":64367,\"start\":64366},{\"end\":64381,\"start\":64376},{\"end\":64865,\"start\":64857},{\"end\":64867,\"start\":64866},{\"end\":64879,\"start\":64876},{\"end\":65356,\"start\":65350},{\"end\":65358,\"start\":65357},{\"end\":65368,\"start\":65365},{\"end\":65603,\"start\":65597},{\"end\":65605,\"start\":65604},{\"end\":65615,\"start\":65612},{\"end\":66090,\"start\":66086},{\"end\":66092,\"start\":66091},{\"end\":66107,\"start\":66101},{\"end\":66119,\"start\":66115},{\"end\":66815,\"start\":66811},{\"end\":66828,\"start\":66820},{\"end\":66844,\"start\":66836},{\"end\":67123,\"start\":67118},{\"end\":67148,\"start\":67144},{\"end\":67170,\"start\":67163},{\"end\":67172,\"start\":67171},{\"end\":67709,\"start\":67704},{\"end\":67735,\"start\":67730},{\"end\":67748,\"start\":67742},{\"end\":67763,\"start\":67758},{\"end\":68063,\"start\":68056},{\"end\":68065,\"start\":68064},{\"end\":68085,\"start\":68077},{\"end\":68087,\"start\":68086},{\"end\":68103,\"start\":68094},{\"end\":68152,\"start\":68147},{\"end\":68165,\"start\":68161},{\"end\":68815,\"start\":68810},{\"end\":68817,\"start\":68816},{\"end\":68834,\"start\":68828},{\"end\":68844,\"start\":68841},{\"end\":69382,\"start\":69374},{\"end\":69416,\"start\":69413},{\"end\":69439,\"start\":69430},{\"end\":70183,\"start\":70172},{\"end\":70205,\"start\":70204},{\"end\":70221,\"start\":70213},{\"end\":70231,\"start\":70228},{\"end\":70251,\"start\":70244},{\"end\":70558,\"start\":70552},{\"end\":70583,\"start\":70577},{\"end\":70597,\"start\":70593},{\"end\":71291,\"start\":71287},{\"end\":71307,\"start\":71304},{\"end\":71330,\"start\":71329},{\"end\":71346,\"start\":71339},{\"end\":71359,\"start\":71349},{\"end\":71698,\"start\":71695},{\"end\":71712,\"start\":71709},{\"end\":71714,\"start\":71713},{\"end\":71765,\"start\":71761},{\"end\":71777,\"start\":71775},{\"end\":72599,\"start\":72594},{\"end\":72615,\"start\":72609},{\"end\":72982,\"start\":72975},{\"end\":73004,\"start\":72998},{\"end\":73006,\"start\":73005},{\"end\":73033,\"start\":73027},{\"end\":73053,\"start\":73049},{\"end\":73064,\"start\":73061},{\"end\":73395,\"start\":73388},{\"end\":73410,\"start\":73407},{\"end\":73672,\"start\":73666},{\"end\":73675,\"start\":73673},{\"end\":73688,\"start\":73684},{\"end\":73706,\"start\":73705},{\"end\":73934,\"start\":73929},{\"end\":73991,\"start\":73987},{\"end\":74015,\"start\":74014},{\"end\":74701,\"start\":74695},{\"end\":74719,\"start\":74714},{\"end\":75015,\"start\":75009},{\"end\":75036,\"start\":75028},{\"end\":75049,\"start\":75045},{\"end\":75066,\"start\":75062},{\"end\":75084,\"start\":75078},{\"end\":75542,\"start\":75538},{\"end\":75553,\"start\":75548},{\"end\":76872,\"start\":76867},{\"end\":77237,\"start\":77231},{\"end\":77239,\"start\":77238},{\"end\":77500,\"start\":77493},{\"end\":77509,\"start\":77506},{\"end\":77539,\"start\":77538},{\"end\":77553,\"start\":77549},{\"end\":77863,\"start\":77857},{\"end\":77886,\"start\":77883},{\"end\":77897,\"start\":77893}]", "bib_author_last_name": "[{\"end\":53214,\"start\":53206},{\"end\":53232,\"start\":53224},{\"end\":53245,\"start\":53239},{\"end\":53891,\"start\":53882},{\"end\":53900,\"start\":53893},{\"end\":53909,\"start\":53902},{\"end\":53917,\"start\":53913},{\"end\":53926,\"start\":53919},{\"end\":53934,\"start\":53928},{\"end\":53949,\"start\":53942},{\"end\":53967,\"start\":53960},{\"end\":54227,\"start\":54217},{\"end\":54235,\"start\":54229},{\"end\":54246,\"start\":54237},{\"end\":54261,\"start\":54255},{\"end\":54280,\"start\":54271},{\"end\":55005,\"start\":54996},{\"end\":55017,\"start\":55012},{\"end\":55024,\"start\":55019},{\"end\":55030,\"start\":55026},{\"end\":55044,\"start\":55039},{\"end\":55066,\"start\":55053},{\"end\":55372,\"start\":55367},{\"end\":55381,\"start\":55374},{\"end\":55391,\"start\":55383},{\"end\":55405,\"start\":55400},{\"end\":55418,\"start\":55412},{\"end\":55962,\"start\":55957},{\"end\":55971,\"start\":55968},{\"end\":56191,\"start\":56188},{\"end\":56197,\"start\":56193},{\"end\":56206,\"start\":56199},{\"end\":56211,\"start\":56208},{\"end\":56215,\"start\":56213},{\"end\":56230,\"start\":56224},{\"end\":56239,\"start\":56235},{\"end\":56531,\"start\":56523},{\"end\":56540,\"start\":56533},{\"end\":56546,\"start\":56542},{\"end\":56558,\"start\":56555},{\"end\":56562,\"start\":56560},{\"end\":56575,\"start\":56571},{\"end\":56584,\"start\":56577},{\"end\":56593,\"start\":56588},{\"end\":57335,\"start\":57327},{\"end\":57346,\"start\":57337},{\"end\":57354,\"start\":57348},{\"end\":57383,\"start\":57362},{\"end\":57401,\"start\":57385},{\"end\":57409,\"start\":57403},{\"end\":57425,\"start\":57419},{\"end\":57444,\"start\":57432},{\"end\":58359,\"start\":58355},{\"end\":58381,\"start\":58368},{\"end\":58726,\"start\":58710},{\"end\":58734,\"start\":58728},{\"end\":58750,\"start\":58747},{\"end\":58760,\"start\":58752},{\"end\":58767,\"start\":58762},{\"end\":58790,\"start\":58774},{\"end\":58798,\"start\":58792},{\"end\":58817,\"start\":58800},{\"end\":58827,\"start\":58819},{\"end\":58837,\"start\":58829},{\"end\":58845,\"start\":58839},{\"end\":58874,\"start\":58853},{\"end\":58892,\"start\":58886},{\"end\":58899,\"start\":58894},{\"end\":58907,\"start\":58903},{\"end\":59343,\"start\":59333},{\"end\":59358,\"start\":59345},{\"end\":59364,\"start\":59360},{\"end\":59371,\"start\":59366},{\"end\":59378,\"start\":59373},{\"end\":59382,\"start\":59380},{\"end\":59396,\"start\":59392},{\"end\":59404,\"start\":59398},{\"end\":59411,\"start\":59406},{\"end\":59418,\"start\":59413},{\"end\":59427,\"start\":59420},{\"end\":59446,\"start\":59437},{\"end\":59454,\"start\":59448},{\"end\":59471,\"start\":59456},{\"end\":59480,\"start\":59473},{\"end\":59485,\"start\":59482},{\"end\":59501,\"start\":59495},{\"end\":59518,\"start\":59510},{\"end\":60248,\"start\":60242},{\"end\":60261,\"start\":60256},{\"end\":60276,\"start\":60271},{\"end\":61032,\"start\":61023},{\"end\":61038,\"start\":61034},{\"end\":61057,\"start\":61040},{\"end\":61067,\"start\":61059},{\"end\":61086,\"start\":61080},{\"end\":61106,\"start\":61101},{\"end\":61468,\"start\":61459},{\"end\":61476,\"start\":61470},{\"end\":61483,\"start\":61478},{\"end\":61490,\"start\":61485},{\"end\":61500,\"start\":61492},{\"end\":61508,\"start\":61502},{\"end\":61526,\"start\":61518},{\"end\":61545,\"start\":61536},{\"end\":61712,\"start\":61704},{\"end\":61721,\"start\":61714},{\"end\":61729,\"start\":61723},{\"end\":61737,\"start\":61731},{\"end\":61743,\"start\":61739},{\"end\":61752,\"start\":61745},{\"end\":61761,\"start\":61756},{\"end\":61769,\"start\":61763},{\"end\":61774,\"start\":61771},{\"end\":61794,\"start\":61782},{\"end\":61809,\"start\":61802},{\"end\":62300,\"start\":62292},{\"end\":62310,\"start\":62306},{\"end\":62325,\"start\":62317},{\"end\":62673,\"start\":62669},{\"end\":62686,\"start\":62684},{\"end\":62697,\"start\":62692},{\"end\":63200,\"start\":63197},{\"end\":63217,\"start\":63209},{\"end\":63235,\"start\":63227},{\"end\":63934,\"start\":63927},{\"end\":64374,\"start\":64368},{\"end\":64384,\"start\":64382},{\"end\":64390,\"start\":64386},{\"end\":64874,\"start\":64868},{\"end\":64887,\"start\":64880},{\"end\":65363,\"start\":65359},{\"end\":65376,\"start\":65369},{\"end\":65610,\"start\":65606},{\"end\":65623,\"start\":65616},{\"end\":66099,\"start\":66093},{\"end\":66113,\"start\":66108},{\"end\":66136,\"start\":66120},{\"end\":66144,\"start\":66138},{\"end\":66818,\"start\":66816},{\"end\":66834,\"start\":66829},{\"end\":66848,\"start\":66845},{\"end\":67126,\"start\":67124},{\"end\":67134,\"start\":67128},{\"end\":67142,\"start\":67136},{\"end\":67161,\"start\":67149},{\"end\":67178,\"start\":67173},{\"end\":67712,\"start\":67710},{\"end\":67721,\"start\":67714},{\"end\":67728,\"start\":67723},{\"end\":67740,\"start\":67736},{\"end\":67756,\"start\":67749},{\"end\":67773,\"start\":67764},{\"end\":68075,\"start\":68066},{\"end\":68092,\"start\":68088},{\"end\":68111,\"start\":68104},{\"end\":68118,\"start\":68113},{\"end\":68127,\"start\":68120},{\"end\":68133,\"start\":68129},{\"end\":68138,\"start\":68135},{\"end\":68145,\"start\":68140},{\"end\":68159,\"start\":68153},{\"end\":68174,\"start\":68166},{\"end\":68826,\"start\":68818},{\"end\":68839,\"start\":68835},{\"end\":68848,\"start\":68845},{\"end\":68854,\"start\":68850},{\"end\":69392,\"start\":69383},{\"end\":69403,\"start\":69394},{\"end\":69411,\"start\":69405},{\"end\":69428,\"start\":69417},{\"end\":69446,\"start\":69440},{\"end\":70196,\"start\":70184},{\"end\":70202,\"start\":70198},{\"end\":70211,\"start\":70206},{\"end\":70226,\"start\":70222},{\"end\":70242,\"start\":70232},{\"end\":70566,\"start\":70559},{\"end\":70575,\"start\":70568},{\"end\":70591,\"start\":70584},{\"end\":70606,\"start\":70598},{\"end\":71302,\"start\":71292},{\"end\":71315,\"start\":71308},{\"end\":71321,\"start\":71317},{\"end\":71327,\"start\":71323},{\"end\":71337,\"start\":71331},{\"end\":71707,\"start\":71699},{\"end\":71725,\"start\":71715},{\"end\":71734,\"start\":71727},{\"end\":71744,\"start\":71736},{\"end\":71752,\"start\":71746},{\"end\":71759,\"start\":71754},{\"end\":71773,\"start\":71766},{\"end\":71782,\"start\":71778},{\"end\":72573,\"start\":72566},{\"end\":72582,\"start\":72575},{\"end\":72586,\"start\":72584},{\"end\":72592,\"start\":72588},{\"end\":72607,\"start\":72600},{\"end\":72631,\"start\":72616},{\"end\":72996,\"start\":72983},{\"end\":73011,\"start\":73007},{\"end\":73018,\"start\":73013},{\"end\":73025,\"start\":73020},{\"end\":73038,\"start\":73034},{\"end\":73047,\"start\":73040},{\"end\":73059,\"start\":73054},{\"end\":73072,\"start\":73065},{\"end\":73405,\"start\":73396},{\"end\":73418,\"start\":73411},{\"end\":73682,\"start\":73676},{\"end\":73695,\"start\":73689},{\"end\":73703,\"start\":73697},{\"end\":73711,\"start\":73707},{\"end\":73941,\"start\":73935},{\"end\":73948,\"start\":73943},{\"end\":73953,\"start\":73950},{\"end\":73960,\"start\":73955},{\"end\":73969,\"start\":73962},{\"end\":73977,\"start\":73971},{\"end\":73985,\"start\":73979},{\"end\":74000,\"start\":73992},{\"end\":74012,\"start\":74002},{\"end\":74022,\"start\":74016},{\"end\":74712,\"start\":74702},{\"end\":74729,\"start\":74720},{\"end\":75026,\"start\":75016},{\"end\":75043,\"start\":75037},{\"end\":75060,\"start\":75050},{\"end\":75076,\"start\":75067},{\"end\":75098,\"start\":75085},{\"end\":75459,\"start\":75455},{\"end\":75468,\"start\":75461},{\"end\":75474,\"start\":75470},{\"end\":75479,\"start\":75476},{\"end\":75485,\"start\":75481},{\"end\":75493,\"start\":75487},{\"end\":75499,\"start\":75495},{\"end\":75505,\"start\":75501},{\"end\":75512,\"start\":75507},{\"end\":75520,\"start\":75514},{\"end\":75527,\"start\":75522},{\"end\":75536,\"start\":75529},{\"end\":75546,\"start\":75543},{\"end\":75557,\"start\":75554},{\"end\":76882,\"start\":76873},{\"end\":77248,\"start\":77240},{\"end\":77504,\"start\":77501},{\"end\":77514,\"start\":77510},{\"end\":77519,\"start\":77516},{\"end\":77526,\"start\":77521},{\"end\":77536,\"start\":77528},{\"end\":77547,\"start\":77540},{\"end\":77562,\"start\":77554},{\"end\":77866,\"start\":77864},{\"end\":77873,\"start\":77868},{\"end\":77881,\"start\":77875},{\"end\":77891,\"start\":77887},{\"end\":77900,\"start\":77898}]", "bib_entry": "[{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b0\",\"matched_paper_id\":2057420},\"end\":53836,\"start\":53154},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":205289650},\"end\":54160,\"start\":53838},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b2\",\"matched_paper_id\":3671269},\"end\":54930,\"start\":54162},{\"attributes\":{\"id\":\"b3\"},\"end\":55300,\"start\":54932},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":17682909},\"end\":55902,\"start\":55302},{\"attributes\":{\"id\":\"b5\"},\"end\":56114,\"start\":55904},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":31301740},\"end\":56473,\"start\":56116},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4566639},\"end\":57249,\"start\":56475},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1690180},\"end\":58222,\"start\":57251},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7423230},\"end\":58622,\"start\":58224},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3347345},\"end\":59296,\"start\":58624},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1033682},\"end\":60182,\"start\":59298},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b12\",\"matched_paper_id\":4398957},\"end\":60920,\"start\":60184},{\"attributes\":{\"doi\":\"abs/1705.10843\",\"id\":\"b13\"},\"end\":61418,\"start\":60922},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":10894094},\"end\":61698,\"start\":61420},{\"attributes\":{\"id\":\"b15\"},\"end\":62280,\"start\":61700},{\"attributes\":{\"doi\":\"abs/1709.05584\",\"id\":\"b16\"},\"end\":62610,\"start\":62282},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2428314},\"end\":63117,\"start\":62612},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b18\",\"matched_paper_id\":3364940},\"end\":63878,\"start\":63119},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":29504454},\"end\":64317,\"start\":63880},{\"attributes\":{\"id\":\"b20\"},\"end\":64822,\"start\":64319},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":216078090},\"end\":65316,\"start\":64824},{\"attributes\":{\"id\":\"b22\"},\"end\":65529,\"start\":65318},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3144218},\"end\":66051,\"start\":65531},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b24\",\"matched_paper_id\":7648414},\"end\":66733,\"start\":66053},{\"attributes\":{\"doi\":\"abs/1801.07299\",\"id\":\"b25\"},\"end\":67078,\"start\":66735},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":8393918},\"end\":67659,\"start\":67080},{\"attributes\":{\"doi\":\"abs/1803.03324\",\"id\":\"b27\"},\"end\":68001,\"start\":67661},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":16326763},\"end\":68727,\"start\":68003},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14307651},\"end\":69314,\"start\":68729},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3194306},\"end\":70103,\"start\":69316},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15367821},\"end\":70470,\"start\":70105},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":16895865},\"end\":71192,\"start\":70472},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12358271},\"end\":71654,\"start\":71194},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1687220},\"end\":72449,\"start\":71656},{\"attributes\":{\"doi\":\"abs/1802.05283\",\"id\":\"b35\"},\"end\":72897,\"start\":72451},{\"attributes\":{\"doi\":\"abs/1703.06103\",\"id\":\"b36\"},\"end\":73332,\"start\":72899},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2549851},\"end\":73593,\"start\":73334},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":205264340},\"end\":73885,\"start\":73595},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":13928442},\"end\":74600,\"start\":73887},{\"attributes\":{\"doi\":\"abs/1802.03480\",\"id\":\"b40\"},\"end\":74940,\"start\":74602},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":6844431},\"end\":75379,\"start\":74942},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":19140125},\"end\":76762,\"start\":75381},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":5445756},\"end\":77138,\"start\":76764},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":2332513},\"end\":77445,\"start\":77140},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":125920802},\"end\":77788,\"start\":77447},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":3439214},\"end\":78517,\"start\":77790}]", "bib_title": "[{\"end\":53197,\"start\":53154},{\"end\":53878,\"start\":53838},{\"end\":54204,\"start\":54162},{\"end\":55360,\"start\":55302},{\"end\":56179,\"start\":56116},{\"end\":56515,\"start\":56475},{\"end\":57319,\"start\":57251},{\"end\":58347,\"start\":58224},{\"end\":58708,\"start\":58624},{\"end\":59325,\"start\":59298},{\"end\":60233,\"start\":60184},{\"end\":61457,\"start\":61420},{\"end\":62662,\"start\":62612},{\"end\":63187,\"start\":63119},{\"end\":63916,\"start\":63880},{\"end\":64355,\"start\":64319},{\"end\":64855,\"start\":64824},{\"end\":65348,\"start\":65318},{\"end\":65595,\"start\":65531},{\"end\":66084,\"start\":66053},{\"end\":67116,\"start\":67080},{\"end\":68054,\"start\":68003},{\"end\":68808,\"start\":68729},{\"end\":69372,\"start\":69316},{\"end\":70170,\"start\":70105},{\"end\":70550,\"start\":70472},{\"end\":71285,\"start\":71194},{\"end\":71693,\"start\":71656},{\"end\":73386,\"start\":73334},{\"end\":73664,\"start\":73595},{\"end\":73927,\"start\":73887},{\"end\":75007,\"start\":74942},{\"end\":75453,\"start\":75381},{\"end\":76865,\"start\":76764},{\"end\":77229,\"start\":77140},{\"end\":77491,\"start\":77447},{\"end\":77855,\"start\":77790}]", "bib_author": "[{\"end\":53216,\"start\":53199},{\"end\":53234,\"start\":53216},{\"end\":53247,\"start\":53234},{\"end\":53893,\"start\":53880},{\"end\":53902,\"start\":53893},{\"end\":53911,\"start\":53902},{\"end\":53919,\"start\":53911},{\"end\":53928,\"start\":53919},{\"end\":53936,\"start\":53928},{\"end\":53951,\"start\":53936},{\"end\":53969,\"start\":53951},{\"end\":54229,\"start\":54206},{\"end\":54237,\"start\":54229},{\"end\":54248,\"start\":54237},{\"end\":54263,\"start\":54248},{\"end\":54282,\"start\":54263},{\"end\":55007,\"start\":54986},{\"end\":55019,\"start\":55007},{\"end\":55026,\"start\":55019},{\"end\":55032,\"start\":55026},{\"end\":55046,\"start\":55032},{\"end\":55068,\"start\":55046},{\"end\":55374,\"start\":55362},{\"end\":55383,\"start\":55374},{\"end\":55393,\"start\":55383},{\"end\":55407,\"start\":55393},{\"end\":55420,\"start\":55407},{\"end\":55964,\"start\":55952},{\"end\":55973,\"start\":55964},{\"end\":56193,\"start\":56181},{\"end\":56199,\"start\":56193},{\"end\":56208,\"start\":56199},{\"end\":56213,\"start\":56208},{\"end\":56217,\"start\":56213},{\"end\":56232,\"start\":56217},{\"end\":56241,\"start\":56232},{\"end\":56533,\"start\":56517},{\"end\":56542,\"start\":56533},{\"end\":56548,\"start\":56542},{\"end\":56560,\"start\":56548},{\"end\":56564,\"start\":56560},{\"end\":56577,\"start\":56564},{\"end\":56586,\"start\":56577},{\"end\":56595,\"start\":56586},{\"end\":57337,\"start\":57321},{\"end\":57348,\"start\":57337},{\"end\":57356,\"start\":57348},{\"end\":57385,\"start\":57356},{\"end\":57403,\"start\":57385},{\"end\":57411,\"start\":57403},{\"end\":57427,\"start\":57411},{\"end\":57446,\"start\":57427},{\"end\":57454,\"start\":57446},{\"end\":57463,\"start\":57454},{\"end\":58361,\"start\":58349},{\"end\":58383,\"start\":58361},{\"end\":58728,\"start\":58710},{\"end\":58736,\"start\":58728},{\"end\":58752,\"start\":58736},{\"end\":58762,\"start\":58752},{\"end\":58769,\"start\":58762},{\"end\":58792,\"start\":58769},{\"end\":58800,\"start\":58792},{\"end\":58819,\"start\":58800},{\"end\":58829,\"start\":58819},{\"end\":58839,\"start\":58829},{\"end\":58847,\"start\":58839},{\"end\":58876,\"start\":58847},{\"end\":58894,\"start\":58876},{\"end\":58901,\"start\":58894},{\"end\":58909,\"start\":58901},{\"end\":58924,\"start\":58909},{\"end\":58931,\"start\":58924},{\"end\":59345,\"start\":59327},{\"end\":59360,\"start\":59345},{\"end\":59366,\"start\":59360},{\"end\":59373,\"start\":59366},{\"end\":59380,\"start\":59373},{\"end\":59384,\"start\":59380},{\"end\":59398,\"start\":59384},{\"end\":59406,\"start\":59398},{\"end\":59413,\"start\":59406},{\"end\":59420,\"start\":59413},{\"end\":59429,\"start\":59420},{\"end\":59448,\"start\":59429},{\"end\":59456,\"start\":59448},{\"end\":59473,\"start\":59456},{\"end\":59482,\"start\":59473},{\"end\":59487,\"start\":59482},{\"end\":59503,\"start\":59487},{\"end\":59520,\"start\":59503},{\"end\":59533,\"start\":59520},{\"end\":60250,\"start\":60235},{\"end\":60263,\"start\":60250},{\"end\":60278,\"start\":60263},{\"end\":61034,\"start\":61015},{\"end\":61040,\"start\":61034},{\"end\":61059,\"start\":61040},{\"end\":61069,\"start\":61059},{\"end\":61088,\"start\":61069},{\"end\":61108,\"start\":61088},{\"end\":61115,\"start\":61108},{\"end\":61470,\"start\":61459},{\"end\":61478,\"start\":61470},{\"end\":61485,\"start\":61478},{\"end\":61492,\"start\":61485},{\"end\":61502,\"start\":61492},{\"end\":61510,\"start\":61502},{\"end\":61528,\"start\":61510},{\"end\":61547,\"start\":61528},{\"end\":61714,\"start\":61700},{\"end\":61723,\"start\":61714},{\"end\":61731,\"start\":61723},{\"end\":61739,\"start\":61731},{\"end\":61745,\"start\":61739},{\"end\":61754,\"start\":61745},{\"end\":61763,\"start\":61754},{\"end\":61771,\"start\":61763},{\"end\":61776,\"start\":61771},{\"end\":61796,\"start\":61776},{\"end\":61811,\"start\":61796},{\"end\":62302,\"start\":62282},{\"end\":62312,\"start\":62302},{\"end\":62327,\"start\":62312},{\"end\":62675,\"start\":62664},{\"end\":62688,\"start\":62675},{\"end\":62699,\"start\":62688},{\"end\":63202,\"start\":63189},{\"end\":63219,\"start\":63202},{\"end\":63237,\"start\":63219},{\"end\":63936,\"start\":63918},{\"end\":64376,\"start\":64357},{\"end\":64386,\"start\":64376},{\"end\":64392,\"start\":64386},{\"end\":64876,\"start\":64857},{\"end\":64889,\"start\":64876},{\"end\":65365,\"start\":65350},{\"end\":65378,\"start\":65365},{\"end\":65612,\"start\":65597},{\"end\":65625,\"start\":65612},{\"end\":66101,\"start\":66086},{\"end\":66115,\"start\":66101},{\"end\":66138,\"start\":66115},{\"end\":66146,\"start\":66138},{\"end\":66820,\"start\":66811},{\"end\":66836,\"start\":66820},{\"end\":66850,\"start\":66836},{\"end\":67128,\"start\":67118},{\"end\":67136,\"start\":67128},{\"end\":67144,\"start\":67136},{\"end\":67163,\"start\":67144},{\"end\":67180,\"start\":67163},{\"end\":67714,\"start\":67704},{\"end\":67723,\"start\":67714},{\"end\":67730,\"start\":67723},{\"end\":67742,\"start\":67730},{\"end\":67758,\"start\":67742},{\"end\":67775,\"start\":67758},{\"end\":68077,\"start\":68056},{\"end\":68094,\"start\":68077},{\"end\":68113,\"start\":68094},{\"end\":68120,\"start\":68113},{\"end\":68129,\"start\":68120},{\"end\":68135,\"start\":68129},{\"end\":68140,\"start\":68135},{\"end\":68147,\"start\":68140},{\"end\":68161,\"start\":68147},{\"end\":68176,\"start\":68161},{\"end\":68828,\"start\":68810},{\"end\":68841,\"start\":68828},{\"end\":68850,\"start\":68841},{\"end\":68856,\"start\":68850},{\"end\":69394,\"start\":69374},{\"end\":69405,\"start\":69394},{\"end\":69413,\"start\":69405},{\"end\":69430,\"start\":69413},{\"end\":69448,\"start\":69430},{\"end\":70198,\"start\":70172},{\"end\":70204,\"start\":70198},{\"end\":70213,\"start\":70204},{\"end\":70228,\"start\":70213},{\"end\":70244,\"start\":70228},{\"end\":70254,\"start\":70244},{\"end\":70568,\"start\":70552},{\"end\":70577,\"start\":70568},{\"end\":70593,\"start\":70577},{\"end\":70608,\"start\":70593},{\"end\":71304,\"start\":71287},{\"end\":71317,\"start\":71304},{\"end\":71323,\"start\":71317},{\"end\":71329,\"start\":71323},{\"end\":71339,\"start\":71329},{\"end\":71349,\"start\":71339},{\"end\":71362,\"start\":71349},{\"end\":71709,\"start\":71695},{\"end\":71727,\"start\":71709},{\"end\":71736,\"start\":71727},{\"end\":71746,\"start\":71736},{\"end\":71754,\"start\":71746},{\"end\":71761,\"start\":71754},{\"end\":71775,\"start\":71761},{\"end\":71784,\"start\":71775},{\"end\":72575,\"start\":72566},{\"end\":72584,\"start\":72575},{\"end\":72588,\"start\":72584},{\"end\":72594,\"start\":72588},{\"end\":72609,\"start\":72594},{\"end\":72633,\"start\":72609},{\"end\":72998,\"start\":72975},{\"end\":73013,\"start\":72998},{\"end\":73020,\"start\":73013},{\"end\":73027,\"start\":73020},{\"end\":73040,\"start\":73027},{\"end\":73049,\"start\":73040},{\"end\":73061,\"start\":73049},{\"end\":73074,\"start\":73061},{\"end\":73407,\"start\":73388},{\"end\":73420,\"start\":73407},{\"end\":73684,\"start\":73666},{\"end\":73697,\"start\":73684},{\"end\":73705,\"start\":73697},{\"end\":73713,\"start\":73705},{\"end\":73943,\"start\":73929},{\"end\":73950,\"start\":73943},{\"end\":73955,\"start\":73950},{\"end\":73962,\"start\":73955},{\"end\":73971,\"start\":73962},{\"end\":73979,\"start\":73971},{\"end\":73987,\"start\":73979},{\"end\":74002,\"start\":73987},{\"end\":74014,\"start\":74002},{\"end\":74024,\"start\":74014},{\"end\":74714,\"start\":74695},{\"end\":74731,\"start\":74714},{\"end\":75028,\"start\":75009},{\"end\":75045,\"start\":75028},{\"end\":75062,\"start\":75045},{\"end\":75078,\"start\":75062},{\"end\":75100,\"start\":75078},{\"end\":75461,\"start\":75455},{\"end\":75470,\"start\":75461},{\"end\":75476,\"start\":75470},{\"end\":75481,\"start\":75476},{\"end\":75487,\"start\":75481},{\"end\":75495,\"start\":75487},{\"end\":75501,\"start\":75495},{\"end\":75507,\"start\":75501},{\"end\":75514,\"start\":75507},{\"end\":75522,\"start\":75514},{\"end\":75529,\"start\":75522},{\"end\":75538,\"start\":75529},{\"end\":75548,\"start\":75538},{\"end\":75559,\"start\":75548},{\"end\":76884,\"start\":76867},{\"end\":77250,\"start\":77231},{\"end\":77506,\"start\":77493},{\"end\":77516,\"start\":77506},{\"end\":77521,\"start\":77516},{\"end\":77528,\"start\":77521},{\"end\":77538,\"start\":77528},{\"end\":77549,\"start\":77538},{\"end\":77564,\"start\":77549},{\"end\":77868,\"start\":77857},{\"end\":77875,\"start\":77868},{\"end\":77883,\"start\":77875},{\"end\":77893,\"start\":77883},{\"end\":77902,\"start\":77893}]", "bib_venue": "[{\"end\":53319,\"start\":53251},{\"end\":53985,\"start\":53969},{\"end\":54354,\"start\":54286},{\"end\":54984,\"start\":54932},{\"end\":55476,\"start\":55420},{\"end\":55950,\"start\":55904},{\"end\":56285,\"start\":56241},{\"end\":56690,\"start\":56595},{\"end\":57575,\"start\":57463},{\"end\":58409,\"start\":58383},{\"end\":58950,\"start\":58931},{\"end\":59645,\"start\":59533},{\"end\":60361,\"start\":60282},{\"end\":61013,\"start\":60922},{\"end\":61552,\"start\":61547},{\"end\":61923,\"start\":61811},{\"end\":62416,\"start\":62341},{\"end\":62755,\"start\":62699},{\"end\":63309,\"start\":63241},{\"end\":63992,\"start\":63936},{\"end\":64448,\"start\":64392},{\"end\":64945,\"start\":64889},{\"end\":65414,\"start\":65378},{\"end\":65681,\"start\":65625},{\"end\":66218,\"start\":66150},{\"end\":66809,\"start\":66735},{\"end\":67247,\"start\":67180},{\"end\":67702,\"start\":67661},{\"end\":68232,\"start\":68176},{\"end\":68912,\"start\":68856},{\"end\":69532,\"start\":69448},{\"end\":70269,\"start\":70254},{\"end\":70676,\"start\":70608},{\"end\":71406,\"start\":71362},{\"end\":71896,\"start\":71784},{\"end\":72564,\"start\":72451},{\"end\":72973,\"start\":72899},{\"end\":73449,\"start\":73420},{\"end\":73719,\"start\":73713},{\"end\":74092,\"start\":74024},{\"end\":74693,\"start\":74602},{\"end\":75140,\"start\":75100},{\"end\":75805,\"start\":75559},{\"end\":76937,\"start\":76884},{\"end\":77272,\"start\":77250},{\"end\":77608,\"start\":77564},{\"end\":77976,\"start\":77902},{\"end\":53427,\"start\":53352},{\"end\":54479,\"start\":54391},{\"end\":55525,\"start\":55508},{\"end\":56831,\"start\":56726},{\"end\":57690,\"start\":57666},{\"end\":59680,\"start\":59656},{\"end\":60499,\"start\":60408},{\"end\":61944,\"start\":61925},{\"end\":62771,\"start\":62757},{\"end\":63434,\"start\":63346},{\"end\":64008,\"start\":63994},{\"end\":64498,\"start\":64480},{\"end\":64994,\"start\":64977},{\"end\":65697,\"start\":65683},{\"end\":66326,\"start\":66251},{\"end\":67300,\"start\":67279},{\"end\":68285,\"start\":68264},{\"end\":68928,\"start\":68914},{\"end\":69676,\"start\":69590},{\"end\":70745,\"start\":70678},{\"end\":72005,\"start\":71989},{\"end\":74161,\"start\":74094},{\"end\":76111,\"start\":75853},{\"end\":78107,\"start\":78018}]"}}}, "year": 2023, "month": 12, "day": 17}