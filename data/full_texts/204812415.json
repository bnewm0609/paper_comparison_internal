{"id": 204812415, "updated": "2023-08-31 19:32:20.861", "metadata": {"title": "Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl", "authors": "[{\"first\":\"Juan\",\"last\":\"Caicedo\",\"middle\":[\"C.\"]},{\"first\":\"Allen\",\"last\":\"Goodman\",\"middle\":[]},{\"first\":\"Kyle\",\"last\":\"Karhohs\",\"middle\":[\"W.\"]},{\"first\":\"Beth\",\"last\":\"Cimini\",\"middle\":[\"A.\"]},{\"first\":\"Jeanelle\",\"last\":\"Ackerman\",\"middle\":[]},{\"first\":\"Marzieh\",\"last\":\"Haghighi\",\"middle\":[]},{\"first\":\"CherKeng\",\"last\":\"Heng\",\"middle\":[]},{\"first\":\"Tim\",\"last\":\"Becker\",\"middle\":[]},{\"first\":\"Minh\",\"last\":\"Doan\",\"middle\":[]},{\"first\":\"Claire\",\"last\":\"McQuin\",\"middle\":[]},{\"first\":\"Mohammad\",\"last\":\"Rohban\",\"middle\":[]},{\"first\":\"Shantanu\",\"last\":\"Singh\",\"middle\":[]},{\"first\":\"Anne\",\"last\":\"Carpenter\",\"middle\":[\"E.\"]}]", "venue": "Nature Methods", "journal": "Nature Methods", "publication_date": {"year": 2019, "month": 10, "day": 21}, "abstract": "Segmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2980998394", "acl": null, "pubmed": "31636459", "pubmedcentral": "6919559", "dblp": null, "doi": "10.1038/s41592-019-0612-7"}}, "content": {"source": {"pdf_hash": "b79ca6fd3df135a9bcf778844be625b764fbcfb3", "pdf_src": "Springer", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.nature.com/articles/s41592-019-0612-7.pdf", "status": "HYBRID"}}, "grobid": {"id": "aec7b5ddc30b3090da37bb1fca29ed8501e598fd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b79ca6fd3df135a9bcf778844be625b764fbcfb3.txt", "contents": "\nNucleus segmentation across imaging experiments: the 2018 Data Science Bowl\n\n\nUnaffiliated : Cherkeng \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nHeng \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nJuan C Caicedo \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nAllen Goodman \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nKyle W Karhohs \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nBeth A Cimini \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nJeanelle Ackerman \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nMarzieh Haghighi \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nCherkeng Heng \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nTim Becker \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nMinh Doan \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nClaire Mcquin \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nMohammad Rohban \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nShantanu Singh \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nAnne E Carpenter \nBroad Institute of MIT and Harvard\nCambridgeMAUSA\n\nNucleus segmentation across imaging experiments: the 2018 Data Science Bowl\n10.1038/s41592-019-0612-7FOCUS | AnAlysis Corrected: Publisher Correction FOCUS | AnAlysis\nSegmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools.\n\nM icroscopy is a central technology of biomedical research, mentioned in nearly one million PubMed-indexed scientific papers to date (PubMed search 'microscopy OR microscope OR microscopic' , accessed 7 October 2018). Increasingly, the images produced are analyzed quantitatively [1][2][3] . Various microscopy techniques allow capturing structural and functional properties of biological model systems, including cultured cells, tissues and organoids. As microscopy makes progress to capture such systems in greater detail and throughput and as the development of novel assays reveals more complex properties of living organisms, the need for robust and easy to use microscopy image analysis methods becomes critical to answer a wider variety of biological questions.\n\nMany image analysis workflows involve the identification (segmentation) of cell nuclei as a first step to extract meaningful biological signals. Research studies may involve counting cells, tracking moving populations, localizing proteins and classifying phenotypes or profiling treatments; in all of these and more, the nucleus is a reliable compartment of reference for identifying single cells in microscopy images.\n\nHowever, selecting strategies to segment nuclei is not an easy task for nonexpert users in regular biology labs. Most existing user-friendly bioimage analysis tools 4-6 identify nuclei using classical segmentation algorithms such as thresholding 7 , watershed 8 or active contours 9 . These need to be configured for each study to account for different microscopy modalities, scales and experimental conditions, often requiring great expertise to select the algorithm that suits the problem and to adjust its parameters. For advanced users, the choice can also be daunting, considering that hundreds of papers are published every year presenting new methods for cell and nucleus segmentation. And even under controlled experimental conditions, no single parameter choice can segment all images correctly, because classical algorithms can fail to adapt to the heterogeneity of biological samples or can be sensitive to technical artifacts [10][11][12] . Altogether, this situation slows down the pace of research and hinders biological laboratories from adopting imaging technologies owing to the time and expertise required.\n\nHere, we explore the idea of creating a segmentation model that can identify the nucleus of cells automatically in a diverse set of stained two-dimensional (2D) light microscopy images without human interaction. Such a model could power future robotic microscopes to facilitate a wide range of biological applications, by finding and counting nuclei in images in real time across cell types, staining types, magnification and in spite of experimental variations. A single trained model effective across instruments, stains and cell types would improve the experience of biologists and speed their research. Classical algorithms for identifying nuclei in microscopy images follow very similar computational strategies, with varying parameters or configurations (Methods, Supplementary Note 3). Our goal was to investigate whether any modern solutions, such as large capacity deep-learning models, could provide a single unifying solution without requiring manual configuration.\n\nBiological image segmentation on the basis of machine learning already exists in user-friendly software, such as Ilastik 13 and ImageJ 14 , and recent studies confirm the usefulness of this approach 15 . Deep learning has shown great potential to solve difficult problems in cellular image analysis 16 , and neural network models for image segmentation also exist [17][18][19][20] . However, existing solutions require users to create models that are customized for each experiment, taking time to prepare annotations, train models and/or configure algorithms. We instead aimed to create a generic, reusable model that is trained once and can be shared and run on a variety of fluorescence microscopy experiments without additional user intervention. We envisioned software tools for nucleus segmentation that can be used with the same ease and robustness as face detectors in natural images; they just work, without users having to train models or to configure settings and under varying lighting and scenery conditions. This paper reports the results of the 2018 Data Science Bowl, which challenged participants to segment nuclei in a variety of 2D light microscopy images without the need for any manual interaction or adjustment. The competition provided participants with a training set of images comprising problems (images containing nuclei) along with the corresponding solutions (segmentation masks for the nuclei) and test sets of images for which they had to generate the segmentations using a two-stage evaluation protocol. Importantly, the holdout set was comprised of 15 diverse image sets from biological experiments that were not present in the training set, to realistically evaluate how well the algorithms perform across different experimental conditions. This is the first time that nucleus segmentation methods have been challenged to generalize by operating blindly on unseen biological experiments without user interaction or additional annotation/training.\n\n\nAnAlysis | FOCUS\n\n\nNaTure MeThoDS\n\n\nResults\n\nThe 2018 Data Science Bowl. For the competition, we created a dataset with 37,333 manually annotated nuclei in 841 2D images from more than 30 experiments across different samples, cell lines, microscopy instruments, imaging conditions, operators, research facilities and staining protocols. The annotations were manually made by a team of expert biologists that followed a collaborative workflow (Methods), and we call these 'target masks' instead of ground truth, given that each annotation was created by a single expert and reviewed by the rest. Researchers around the world freely contributed the images and agreed to a Creative Commons 0 license (public domain); our team's annotations are similarly freely available. This dataset is publicly accessible in the Broad Bioimage Benchmark Collection with accession number BBBC038.\n\nThe challenge was run for a total of 3 months in which participants had access to the training set (with target masks) and the first-stage test sets (with target masks withheld). The evaluation of competitors' predictions on the withheld masks of the first-stage test set powered the leaderboard, and in the final week of the competition, a second-stage test set (with target masks withheld) was released to determine the challenge winners. This second-stage evaluation aimed to assess the robustness of models to segment new images from new experiments and to also evaluate the ability of the models to run completely autonomous segmentation without user interaction. To deter manual intervention on the images, the second-stage holdout set had 3,200 images with approximately 100,000 single nuclei that had to be segmented in <7 d, only a small fraction of which had accompanying manually defined target masks and were actually used for scoring (Methods). Participants uploaded their segmentation masks to the Kaggle server (https://www.kaggle. com/c/data-science-bowl-2018), which validated against the real masks hidden from the public, using a quantitative score to rank participants (Methods).\n\nA total of 17,929 competitors signed up for the competition in 3,891 teams during the first stage and 739 teams made successful entries during the second stage to compete for US$170,000 in cash and prizes. Overall, participants submitted a total of 68,017 submissions throughout the duration of the competition. This contest fostered the development of new methods with contributions of data scientists around the world that usually do not work on microscopy images, bringing state-of-the-art innovations. The top three participants, among many other competitors in the challenge, made their solutions open source, which will facilitate their adoption and extension by the wider scientific community.\n\nTop solutions improve usability and accuracy of nucleus segmentation. In the second-stage evaluation, competitors were not permitted to use any nonautomated, image-specific configuration. As a result, the winning models yielded a major improvement in usability compared to current practices for microscopy image segmentation, which need either algorithm selection and tuning (for classical methods) or manual annotations (for machine-learning methods) for different image sets.\n\nWhen compared to a reference segmentation obtained with classical image processing techniques adapted for the holdout image sets using minimal user intervention (Methods), we found that 85 candidate algorithms from the challenge yielded higher accuracy (Fig. 1a). In particular, the top three solutions outperformed these minimally tuned classical algorithms by a large margin, producing better segmentations over all coverage thresholds (Fig. 1b). We evaluated the accuracy of the segmentations using metrics common in computer vision research for object segmentation (Methods).\n\nImportantly, the segmentations obtained with classical image processing algorithms, unlike the methods in the challenge, required manual configuration; there exists no classical algorithm that could claim to produce reasonable results on 15 diverse image sets with no user intervention. In contrast, a novice with no prior exposure to bioimage analysis performed substantially worse than the expert and the top-scoring deep-learning models (Fig. 1c). The novice and expert invested 5 h and 3 h of work to achieve their corresponding segmentation results. Embedded in a user-friendly interface, as has already been prototyped in the NucleAIzer system 21 , the top models would require no configuration time.\n\nThe classical methods were tested by first organizing images into five groups using visual inspection, then analysis pipelines were created in the open source software, CellProfiler 5 (Methods). These pipelines are representative of widely adopted techniques surveyed recently in the literature of microscopy image segmentation 12,22,23 , though they are likely suboptimal solutions because the techniques were not fully optimized for each of the 15 image sets independently to prevent overfitting and in keeping with the no-or low-configuration mission of the data challenge.\n\nIn addition to reference segmentations using classical techniques, we also evaluated the performance of the top-scoring models to deep-learning models trained separately for each type of images. We chose U-Net 17 , a popular deep-learning-based method to solve microscopy image segmentation problems, including nucleus segmentation 20 . The learning capacity of a single regular U-Net was not sufficient to capture the experimental variation in the challenge (such solutions entered the competition); therefore, we trained five models to reduce variance, as in the case of classical algorithms, and applied image pre and post-processing routines specific to each group (Supplementary Note 4). The results show that even spending ~20 h of hands-on time (development and training time not included in our estimates), these models did not reach competitive performance compared to the top solution (Fig. 1c). Several factors contributed to this result: limited learning capacity of the evaluated U-Net relative to the top models, reduced number of training examples in the five groups after splitting and experimental variability of the test sets.\n\nFinally, we asked an additional annotator to create target masks for a subset of images in the test set and we observed interobserver variability (Supplementary Fig. 3) a well-known problem 24 . Interestingly, the top performing model agreed on boundary annotations more often with each annotator than they agreed with each other (Supplementary Fig. 3). This suggests that the model fitted smooth boundaries that were close to the edge of nuclei, whereas manual annotations may be biased by subjective noise ( Supplementary Fig. 5). The performance of the top model was also more similar to humans than to classical algorithms in terms of segmentation accuracy ( Supplementary Fig. 3). Although we did not investigate this result extensively, it suggests that the top models may reach human-annotator-like performance with similar error rates.\n\nBest-performing solutions segment a diversity of microscopy images. The most challenging aspect of the competition was that the holdout set included microscopy images from 15 different biological experiments, including various 2D light microscopy types, acquisition equipment and biological conditions. This is in contrast to previous research studies that optimize nucleus segmentation methods individually for each image set or type 12,18,22,[25][26][27][28] . From a visual standpoint, we identified five groups of images comprising nuclei of very different appearances, including two major types of light microscopy ( Top participants stood out by making models that generalized well across diverse image types and experimental variation (Fig. 2a), and despite a heavily unbalanced dataset (Fig. 2b). Dataset biases can mislead the performance of machine-learning models 29,30 ; lessrepresented image types were indeed challenging to segment for the average participant (Fig. 2a). With the largest group of images, containing 80% of the training examples, top participants reached a maximum accuracy of 0.90 in the test set, and with the smallest group, containing 0.6% of the examples, they reached a maximum accuracy of 0.55. In all cases, their performance surpassed the reference CellProfiler segmentations, as well as the average participant, by a large margin.\n\nBest-performing solutions reduce segmentation errors. The solutions of the top three teams were significantly better than the reference segmentations according to the competition score and other metrics that we used to analyze the results (Figs. 1-3 and Supplementary Figs. 3 and 4). The competition score was an aggregated metric that considered multiple factors of segmentation quality, including precision, recall and object coverage (Methods) and could be deconstructed in multiple ways to understand performance and error modes.\n\nFirst, we assessed performance at a single-object coverage threshold to interpret the differences in accuracy between the models. When a threshold equal to 0.5 was chosen (common in previous works 12,31 ), the top performing model got an F1 accuracy of 0.889, compared to 0.819 for the CellProfiler reference (Supplementary Table 3). Note that these results were the average across all images from 15 different experiments in the second-stage evaluation, 12 cell lines and five image types. When we considered fluorescent images with only small nuclei, the F1 accuracy of the top performing model was 0.932, whereas CellProfiler obtained an F1 score of 0.844 (Supplementary Table 2). Using a threshold of 0.7 (Fig. 2a) challenges methods by requiring a larger minimum object coverage. We observed that the top three models all surpassed the CellProfiler reference for three image types (small fluorescent, purple and pink and purple tissues). For the other two image types (big fluorescent and grayscale tissue), all but one model performed worse than   In general, the top three models reached similar aggregated performance, but exhibited different behavior and error modes. The accuracy results can be disaggregated by image type across multiple object coverage thresholds (Fig. 2c), allowing us to identify the strengths and weaknesses of each strategy. For instance, the second place solution was the best for large fluorescent nuclei (blue line) but poorer for grayscale tissue (green line).\n\nWe conducted other performance analyses on the top three models relative to the CellProfiler reference (Supplementary Note 6) and observed reduced error rates from the models, which missed fewer objects, successfully separated merged nuclei (Supplementary Fig. 6) and improved precision and recall ( Supplementary Fig. 7). All these observations support the idea that the top three solutions can make configuration-free segmentation of 2D stained nuclei a reality.\n\nTop algorithms were on the basis of deep convolutional neural networks. The majority of participants used deep convolutional neural networks (CNNs), a popular technique to solve computer vision tasks 32 , as well as various microscopy image and pathology problems 16,26 . A wide variety of CNN architectures can be used for image segmentation and participants designed creative solutions to improve segmentation accuracy. Interestingly, the top three participants used very different solutions: an ensemble of U-Nets, a fully convolutional feature pyramid network (FPN) and a Mask-RCNN (region-based CNN) model. Their performance is summarized in Table 1 and the main characteristics of each model are described below and in the Methods. Figure 3 presents example segmentations obtained by these models together with a reference segmentation obtained by CellProfiler.\n\nBest-performing solution. A. Buslaev, V. Durnov and S. Seferbekov formed the [ods.ai] topcoders team, and introduced a highly optimized, multinetwork (ensemble) model with sophisticated data augmentation and data post-processing. Coordinating all these elements in a successful solution was a major achievement because models with larger learning capacity may overfit and fail to perform well with new images. Instead, this solution generalized well to the holdout of 15 image sets. In terms of computational requirements, this was the most demanding solution, as a single image needs to be processed by 32 different neural networks using graphics processing units (GPUs). In addition, the post-processing steps need to check and combine the predicted objects from all 32 outputs. Altogether, this system was the most accurate, albeit at a high computational cost and complexity. More details in the Methods section.\n\nSecond-best-performing solution. M. Jiang (team name: Jacobkie) presented a solution with a good balance of accuracy and speed; only a single neural network was used to process new images. Her solution introduced several innovations that can be adopted in other models, such as a loss function that penalizes errors taking into account object size (small objects have as equal weight as large objects), the use of distance maps instead of binary masks as a target for learning and pretraining with natural-image, object detection datasets. More details in the Methods section.   \n\n\nAnAlysis | FOCUS\n\n\nNaTure MeThoDS\n\n\nFOCUS | AnAlysis\n\n\nNaTure MeThoDS\n\nThird-best-performing solution. A. Lopez-Urrutia (team name: Deep Retina) presented a solution on the basis of a single neural network that processed regions with candidate objects instead of using a fully convolutional approach. The base model is known as Mask-RCNN 33 , which is a popular architecture for object detection and instance segmentation in natural images. The simplicity of the solution was attractive, as various implementations of this solution existed and could be adapted to this problem by retraining the output layers with the right data. In addition, the Mask-RCNN model was actively investigated in the computer vision community, making innovations readily available to the nucleus segmentation problem. More details in the Methods section.\n\nOther solutions, participants and strategies. Apart from the top three methods, the fourth place solution 34,35 by the team 'Nuclear Vision' fell 0.04 points behind third place and combined classical watershed transform with modern deep learning 36 . A stage-one U-Net (direction net) was used to predict the direction vector of a pixel inside nuclei and to the nearest nucleus boundary. Another stage-two U-Net (water transform net) estimated the watershed levels and output the masks, eroded masks and mask centers. Such methods may be useful for automatic or interactive segmentation, whereby the traditional watershed transform energy landscape is replaced by the output of learned deep networks. \n\n\nFig. 3 | Example segmentation maps for various images obtained by the top three participants and the CellProfiler reference. The segmentation maps\n\nshow pixel-wise alignments between target segmentation masks and predicted segmentations. If the masks align correctly, pixels in the boundaries are colored white. If the target mask or part of it is missing, pixels in the boundaries are colored blue. If the predicted segmentation is introduced in a region without real object, the boundary pixels are red. rows show information about each method and columns show performance metrics. Core model, type of machine-learning algorithm used to solve the task, with the number indicating how many neural networks were used in the solution. The names of neural networks are explained in the main text. Competition score, metric used during the competition to rank participants in the scoreboard (https:// www.kaggle.com/c/data-science-bowl-2018#evaluation). The rest of the performance metrics were computed offline after the competition ended for analysis purposes only. Average F1 is the accuracy metric closely related to the official score, which treats the segmentation problem as a binary decision problem (correctly segmented or not) for each object. The average F1 score was computed at different IoU thresholds between target masks and estimated segmentations and then averaged across all thresholds. By setting a single IoU threshold, we could count how many objects were correctly segmented (true positives or recall at 0.7 IoU), how many were missed (false negatives or Missed at 0.7 IoU) and how many false objects were introduced (false positives or extra at 0.7 IoU). a Note that the CellProfiler reference segmentations were generated with a different experimental protocol involving manual adjustment of pipelines for five image types in the test set. More details are provided in the Methods section.\n\n\nFOCUS | AnAlysis\n\nAnAlysis | FOCUS\n\n\nNaTure MeThoDS\n\nThe team 'Creepy ReLU' 37 generated synthetic images using CycleGAN 38 . They showed that color stains could be transferred from one image to another. However they did not have time to train on synthetic images; this approach might have improved the top solutions in the competition.\n\nOn a sociological note, the competition brought experts from different domains together to share software and cloud resources and to develop ideas. In particular, team 'minerva.ml' 39 open sourced their code and development process at the start of the competition and provided a cloud-based platform that Kagglers could use. The author of the open source Matterport Mask-RCNN implementation (https://github.com/matterport/Mask_RCNN) also participated in the competition 40 and provided a complete software pipeline tool for training, visualization and submission. We noted that the third and fifth-place teams in the competition based their solutions on this Matterport implementation. This shows that the quality of open source implementation was high, included suitable options and parameters and could be used off the shelf.\n\n\nDiscussion\n\nThe 2018 Data Science Bowl presented the challenge of automatically finding nuclei in a large variety of unseen microscopy images, with no configuration step. This was the first documented attempt to produce a model that could segment the stained nuclei of cells in 15 biological experiments, across experimental conditions, acquisition equipment and source laboratory. The main goal of the challenge was to investigate generic segmentation strategies that could be automatically applied to many imaging experiments with no further user intervention. This approach may reduce the time to quantify images, empowering future generations of biologists to adopt and run more quantitative imaging experiments for research and clinical practice.\n\nTraining automated nucleus segmentation tools using modern machine-learning approaches requires collecting annotated examples. The 2018 Data Science Bowl created a resource of diverse images contributed by numerous biological laboratories and manually annotated by a team of expert biologists at the Broad Institute. All those data are now publicly available with public domain licenses to facilitate future scientific research as well as industrial development. We hope others in the wider bioimaging community will contribute more images and annotations to grow this resource with additional experimental variations, including unstained brightfield and electron microscopy, as well as many other common image modalities that were not included in our study.\n\nThe challenge attracted participation from different teams in the data science community, who made all types of contributions, learned together and collaborated to understand the problem better and make progress toward the proposed goal. Solutions presented by several participants achieved the goal of a single model able to segment various microscopy images with no intervention. The experimental results indicate that nucleus segmentation could be fully automated, requiring no manual settings or image processing expertise from users, while still providing improved accuracy versus the evaluated tools. Higher accuracy may be possible through a larger, more diverse training set and by incorporating the latest advances in machine learning and computer vision research.\n\nThe top participants presented solutions on the basis of fully convolutional networks (U-Nets and FPNs) or Mask-RCNN. These two approaches were widespread during the competition; what distinguishes the winners was a combination of pre-processing and post-processing techniques, as well as the application of best practices during training (mostly data balancing and data augmentation). A common theme among the top competitors was the use of data augmentation during training and testing, including color shifts to make networks color invariant, and scaling methods to address object size challenges. Interestingly, all top three solutions used a ranking strategy to select the best segmentation masks from several candidates predicted by the base models. While this is common practice for RCNN-like models (third-best solution), the top two models also created their own strategies to achieve a similar effect with fully convolutional networks.\n\nThe results present a successful proof of concept that deep learning is indeed capable of delivering accurate results without user interaction. However, even though the top models are publicly available, they still require computational expertise to be applied to images. A user-friendly tool is needed to bridge the gap between these cutting-edge solutions and everyday biomedical practice, similarly to what the NucleiAIzer system proposes 21 . We also found that data availability is a limitation to reach top performance for various image types, thus, additional efforts are needed to collect and annotate more data to expand the applicability of future systems. The generalization ability of models may also be evaluated in other datasets not used during the Data Science Bowl challenge, such as the Cell Tracking Challenge and others. Other aspects of usability remain to be addressed. For instance, if there are mistakes in the segmentation, how can these models efficiently and easily take feedback from humans to correct segmentation errors? The results of the 2018 Data Science Bowl are a first step toward creating a generic system for segmenting the nucleus of cells in every microscopy image. Future work could expand the dataset to cover missing major microscopy imaging types, such as unstained brightfield images and three-dimensional images. Following the strategy laid out here, models could also be constructed to segment cell structures in addition to the nucleus, such as cell borders and organelles.\n\n\nonline content\n\nAny methods, additional references, Nature Research reporting summaries, source data, statements of code and data availability and associated accession codes are available at https://doi.org/10.1038/ s41592-019-0612-7.\n\n\nAnAlysis\n\n\nNaTure MeThoDS\n\n\nMethods\n\nDataset. The image sets were donated by multiple laboratories studying different aspects of cell biology. The names and credits are listed in Supplementary File 1. A total of 841 images were collected, representing a wide variety of nuclei observed under different experimental conditions and imaged with various staining protocols. Our goal was to collect as many independent biological experiments as possible to create a resource that contains enough technical and biological variability to train generic nucleus segmentation models.\n\nIn total, the dataset contained images from more than 30 different biological experiments, which were split into 16 experiments for training (670 images) and first-stage evaluation (65 images) and exactly 15 experiments for the second-stage evaluation (106 images). The number of experiments represented in the training and first-stage evaluation is approximate because these include images from public or anonymous sources without metadata to confirm the exact number. Holding 15 experiments for the second-stage evaluation allowed us to simulate the realistic evaluation case of bringing newly acquired images for segmenting their nuclei. See Supplementary Note 1 for more statistics and details about the dataset.\n\nAnnotation strategy. Overall, the image set was annotated with 29,464 individual nuclei in the training set, 4,152 in the first-stage test set and 3,717 in the secondstage test set, for a total of 37,333. The annotations were created by expert biologists who manually delineated each object in the images using one of two tools: (1) an assisted annotation tool that precomputed superpixel segmentations to facilitate the selection of regions in the foreground or background; and (2) the GIMP image editing software to create annotation masks by coloring individual pixels outlining each nucleus.\n\nThe assisted annotation tool made an initial over-segmentation of the image using the simple linear iterative clustering superpixels algorithm 41 . The annotators could then color each superpixel with one of four colors to indicate what regions correspond to objects and what others to background. Objects were required to have different colors if they were touching each other. Superpixels are very helpful to reduce the amount of annotation time, but also may contain systematic noise because their boundaries are not necessarily perfectly aligned with the real object boundary. This strategy was used for training images only to facilitate large scale annotation; for test images, we used the per-pixel annotation strategy using GIMP to score participants with respect to masks drawn 100% manually. Human annotations were not post-processed to avoid introducing unintended artifacts. See Supplementary Note 5 for more details.\n\nImage modalities. In this dataset, we included 2D light microscopy images of stained nuclei. The majority of the images in this dataset came from fluorescent images with cells of different sizes and various types, primarily stained with DAPI or Hoechst. The dataset also included tissue samples stained with hematoxylin and eosin, displaying structures from a diversity of organs and animal models. The image collection was organized to include different technical settings and a variety of biologically different experiments. We excluded phase-contrast, differential interference contrast and other image modalities because during the data collection period we did not find image sets or donating laboratories that could make these images available in the public domain.\n\nEvaluation. Performance metrics. The evaluation strategy was on the basis of identifying object-level errors. This was accomplished by matching target object masks with predicted objects submitted by participants and then computing true positives and false positives. In order to match target masks and predicted objects, the intersection-over-union (IoU) score was computed for all pairs of objects using IoU \u00bc jA\\Bj jA \u222a Bj I , where A and B are two objects, and the operator | | measures area. A minimum IoU threshold t was selected to identify correctly segmented objects and any other predicted segmentation mask below the threshold was considered an error. With all true positives (TP), false positives (FP), true negatives (TN) and false negatives (FN), we created a confusion matrix and computed precision (P), recall (R) and F1 scores, using a fixed IoU threshold t as follows:\nP t \u00f0 \u00de \u00bc TP t \u00f0 \u00de TP t \u00f0 \u00de\u00feFP t \u00f0 \u00de R t \u00f0 \u00de \u00bc TP t \u00f0 \u00de TP t \u00f0 \u00de\u00feFN t \u00f0 \u00de F1 t \u00f0 \u00de \u00bc 2TP t \u00f0 \u00de 2TP t \u00f0 \u00de\u00feFP t \u00f0 \u00de\u00feFN t \u00f0 \u00de\nWe used increasing IoU thresholds to estimate shape-matching accuracy. When a segmentation covered the target mask perfectly, the IoU score was 1 and the object was correctly detected no matter which threshold was used. In practice, segmentations can only approximate the real shape of the object, so at certain coverage threshold the object was missed. This test estimated how well segmentations matched the shape of manually defined target masks. Then, the official competition score S, was defined in terms of type I and II errors, using multiple IoU thresholds as follows:\nS \u00bc 1 T j j X t2T TP t \u00f0 \u00de TP t \u00f0 \u00de \u00fe FP t \u00f0 \u00de \u00fe FN t \u00f0 \u00de ;\nwhere T \u00bc 0:10; 0:15; :::; 0:95 f g For some of the results reported in this paper, we also computed F1, precision and recall in an aggregated manner, similar to the official score. The competition evaluation score is described in detail in Supplementary Note 2.\n\nTwo-stage evaluation protocol. The data challenge was on the basis of a two-stage evaluation protocol with one training set and two holdout sets. The training and first-stage holdout sets were available to competitors during a period of 2.5 months for calibrating the algorithms. The first-stage holdout target masks were not directly accessible to the competitors but instead used to test their submitted segmented images against, yielding a numerical score. Participants were allowed to make a maximum of five submissions per day to obtain feedback about their performance, and they could select two submissions for evaluation and ranking. Depending on the scores obtained from these submissions, they decided to tune their methods and submit again later during the competition.\n\nThe second-stage holdout set was released during the final period of the competition, giving participants only 1 week to process 3,200 images. These images had an estimated 100,000 single nuclei, only 106 (3%) images had manually defined target masks useful for scoring, and the competitors did not know which images were going to be evaluated. This approach was enforced to prevent competitors from using extremely slow solutions, hand-outlining results or choosing among a large number of algorithms or settings by visually verifying the results. The same submission rules applied during the second-stage evaluation, allowing participants to submit at most five times per day and select only two submissions for final scoring.\n\nThe segmented images produced by the top solutions were manually screened and presented naturally occurring errors produced by automated solutions; none of them had signs of hacking or cheating. This experimental procedure was as rigorous as those used in other initiatives organized by scientists for other scientists. Kaggle has a long history of experience working with scientists to define these experimental settings and has a good pool of best practices for data science that were enforced to assure the validity of the results and identify hacking.\n\nGrouping of images. The guiding principles for creating the five groups of images in the dataset was visual similarity on the basis of image colors and object sizes. The goal was to facilitate the application of classical image segmentation algorithms, which most heavily relied on (1) the nuclei and background colors (white versus black, purple versus white, purple versus pink, back versus gray); and (2) the approximate size of nuclei in the image. The number of groups constructed was kept as low as possible while maintaining the ability to create a robust analysis workflow for each group.\n\nThis organization in five groups may reflect staining protocols and microscopy techniques used in the experiments. However, that information was not explicitly used for determining the assignment of images to groups, it was all on the basis of visual inspection. In fact, the analyst that created the groups and designed classical segmentation workflows had no knowledge of experimental details of the test sets. This was intentional, with the purpose of making generic segmentation solutions with minimal assumptions, similarly to the conditions presented to participants of the competition.\n\nThe five groups of images were also used to conduct data analysis of segmentation performance of competitors. Most of the results presented in this paper have been organized around these five generic groups of images. However, this information was not provided to participants of the competition, for them, the entire dataset contained varied example microscopy images with a single object of interest, the nucleus.\n\nReference segmentations. Our goal was to evaluate the contribution of nucleus segmentation methods proposed by participants of the challenge, considering that these methods worked across a variety of experiments with no user intervention. The most appropriate baseline was an existing strategy that could be applied to any image (within the constraints laid out, 2D images stained for nuclei) and produce accurate results with no human interaction. Given that there was no such method in existence, we approximated it by taking an approach that used as little human time as possible. As an approximation for quantitative reference, we do not call the approach baseline segmentations, but rather reference segmentations.\n\nWe chose CellProfiler v.3.1.5 as the tool to create reference segmentations given its flexibility to configure robust pipelines on the basis of well-established algorithms, while investing a minimum amount of time. CellProfiler is a powerful open source tool for microscopy image analysis that includes a variety of fundamental image processing algorithms in a modular way. The algorithms can be organized in a computational graph (pipeline) that takes images as inputs and produces various types of outputs. Importantly, a pipeline is defined using a user-friendly interface and can run complex operations without the need to write a single line of code. These properties made CellProfiler a good reference for models in the competition, because the goal of the challenge was to investigate generic nucleus segmentation methods with minimal user interaction.\n\nFive custom pipelines were designed, one for each of the image types in the test set, with the goal of evaluating the classical algorithms implemented in CellProfiler. A single classical segmentation pipeline was unlikely to work well in the variety of images represented in the test set, thus, we adapted the best practices reported in\n\n\nAnAlysis | FOCUS\n\n\nAnAlysis | FOCUS\n\n\nNaTure MeThoDS\n\nAnAlysis | FOCUS\n\n\nFig. 2a): fluorescence microscopy of mainly cultured cells and brightfield microscopy of stained tissue samples. Tissue samples are typically a more challenging image processing task owing to the irregular appearances of nuclei and their crowded layout. Small fluorescent nuclei images are very common in biomedical research and the most common in both training and test sets. The entire dataset included 31 different experiments (16 for training and first-stage evaluation, 15 for second-stage evaluation), representing 22 cell types, 15 image resolutions and five groups of images, resulting in 841 images and 37,333 manually annotated nuclei (Methods). We omitted dramatically different modalities such as unstained brightfield microscopy and electron microscopy.\n\nFig. 1 |\n1Accuracy and usability of segmentation strategies in the second-stage holdout sets. a, The histogram counts participant teams (n = 739) according to the official competition score of their best submission. The top five competitors are labeled in the distribution, as is the reference segmentation obtained by an expert analyst using CellProfiler. b, Accuracy of the top three solutions measured as the F1 score at multiple IoU thresholds. The scale of the x axis of the histogram in panel a (competition score) is correlated with the area under the curve of the F1 score versus IoU thresholds. The top three models had a similar performance with slight differences at the tails of the curves. c, Breakdown of accuracy in the second-stage evaluation set for the top performing model and three reference solutions. The distribution of F1-scores at a single IoU threshold (IoU = 0.7) shows points (n = 106) that each represented the segmentation accuracy of one image in the set of 106 annotated images of the second-stage evaluation (Methods). The color of single-image points corresponds to the group of images defined for reference evaluations (Methods andFig. 2). The average of the distribution is marked with a larger point labeled with the corresponding average accuracy value. d, estimated time required to configure the segmentation tools evaluated in c (Supplementary Note 4).\n\n\nreference, primarily owing to the limited number of examples in the training set(Fig. 2).\n\nFig. 2 |\n2Performance of submitted solutions across varying, imbalanced image types. a, example images of the five visually grouped image types (Methods) are shown across the bottom and the chart shows the spread of F1 scores (Methods) across all second-stage submissions. F1 scores were measured at a threshold of 0.7 IoU (Methods). Box plot: center line, median; box limits, upper and lower quartiles; whiskers, 1.5\u00d7 interquartile range; points, outliers; colored points, top three participants. b, The distribution of the various image types is shown, color-coded as in a. The top competitors segmented all image types with high accuracy despite the imbalance of examples in the training set. c, Detail of accuracy results by image types and object coverage (IoU) thresholds. The x axis displays IoU thresholds and the y axis represents accuracy measured with F1 scores. For each participant, the plot displays five curves showing the trend of segmentation accuracy at different object coverage thresholds.\n\n\nDistribution of scores in second-stage evaluation40 \n\na \nb \n\nc \nd \n\n30 \n\n20 \n\n10 \n\n0 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \n0.6 \n\nCompetition score \n\nAccurancy and usability \n\nTop-performing \nmodel \nNo configuration time needed \n\nExpert image analyst: 3 h \n\nNovice image analyst: 5 h \n\nData scientist: 20 h \n\nCellProfiler \nexpert \n\nCellProfiler \nnovice \n\nU-net models \nData scientist \n\n0 \n5 \n10 \n15 \n20 \n\nConfiguration time (h) \n\n5: Inom mirzaev \n\nAccuracy of top three models \n1.0 \n\n0.8 \n\n0.6 \n\n0.4 \n\n0.2 \n\n0 \n\nIoU threshold \n\n0.5 \n0.6 \n\n1st place \n2nd place \nAccuracy: F1 score \n\nAccuracy: F1 score at 0.7 loU \n\n0 \n0.2 \n0.4 \n0.6 \n0.8 \n1.0 \n\n0.816 \n\n0.743 \n\n0.616 \n\n0.620 \n\n3rd place \nReference \n\n0.7 \n0.8 \n0.9 \n\n4: Nuclear vision \n\n3: Deep retina \n\n2: Jacobkie \n1: [ods.ai] topcoders \n\nCellprofiler reference* \n\nNumber of participant teams \n\n\n\nTable 1 |\n1Comparison of performance of the top three methodologiesTeam \nCore model \nCompetition score \nAverage F1 \nRecall at \n0.7 Iou (%) \n\nMissed at \n0.7 Iou (%) \n\nExtra at \n0.7 Iou (%) \n\n[ods.ai] topcoders \n32\u00d7 U-Net/FPN \n0.6316 \n0.7120 \n77.62 \n22.38 \n14.55 \n\nJacobkie \n1\u00d7 FC-FPN \n0.6147 \n0.6987 \n69.14 \n30.86 \n15.04 \n\nDeep retina \n1\u00d7 Mask-rCNN \n0.6141 \n0.7008 \n68.07 \n31.93 \n10.90 \n\nCellProfiler a \n-\n0.5281 \n0.6280 \n59.35 \n40.65 \n39.55 \n\n\nNATuRE METHoDS | VOL 16 | DeCeMBer 2019 | 1247-1253 | www.nature.com/naturemethods\nNaTure MeThoDSNATuRE METHoDS | VOL 16 | DeCeMBer 2019 | 1247-1253 | www.nature.com/naturemethods\nNaTure MeThoDSNATuRE METHoDS | www.nature.com/naturemethods\nAcknowledgementsWe are extremely grateful to the biologists who donated images for the challenge and, further, agreed to deem them public domain to facilitate further research without constraint. The contributors are publicly credited here: https://www.kaggle.com/c/datascience-bowl-2018/discussion/54759. The authors thank the team at Kaggle and Booz Allen Hamilton, who facilitated the operations, funding, administration and marketing of the data challenge; in particular, we thank W. Cukierski, M. Demkin, J. Oder Moynihan, J. Sullivan, E. Sager, R. Hensberger and P. Sedivec. We also thank the companies providing sponsorship and support for the challenge, including NVIDIA and PerkinElmer. We thank A. de Souza, now at Eli Lilly and Company, for spearheading the Data Science Bowl. Finally, we thank the members and friends of the Carpenter laboratory for assisting with image annotations, including S. Amaral, P. Faliano, I. Schmidt, V. Chernyshev and G. Way. The Broad Institute team's research effort on this post-competition analysis was supported by the US National Institutes of Health grant (no. R35 GM122547 to A.E.C.). The experiments were run on GPUs donated by the NVIDIA Corporation through their GPU Grant Program.AnAlysis | FOCUSAdditional informationSupplementary information is available for this paper at https://doi.org/10.1038/ s41592-019-0612-7.Correspondence and requests for materials should be addressed to A.E.C.Peer review information Rita Strack was the primary editor on this article and managed its editorial process and peer review in collaboration with the rest of the editorial team.Reprints and permissions information is available at www.nature.com/reprints.Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http:// creativecommons.org/licenses/by/4.0/. \u00a9 The Author(s) 2019FOCUS | AnAlysisAnAlysis NaTure MeThoDS the literature for each group of images. This approach had an advantage over the deep-learning models tested during the competition because the image sets were manually organized and processed with special routines according to their type. Deep-learning models were not expected to have any manual intervention from users. The pipelines were designed with three major sequential steps: (1) preprocessing to transform the image into a grayscale matrix, where nuclei were observed as relatively smooth white shapes on a black background;(2) segmentation of the grayscale image using thresholding, distance transforms and watershed on the basis of approximate expected nuclear size; and (3) segmentation revision using seeded watershed on the basis of the previous segmentation and additional nuclear size priors. Steps 2 and 3 were on the basis of the Identify Primary Objects and Identify Secondary Objects modules of CellProfiler, which are documented in the cell segmentation literature5,8,[42][43][44][45][46]. Pipelines are available online at https://github.com/carpenterlab/2019_caicedo_submitted/tree/master/pipelines, with annotations in each module to describe that module's function in the overall pipeline. Supplementary Figs. 1 and 2 illustrate the corresponding computational graphs for each of the five pipelines designed.Importantly, we did not optimize the algorithm parameters for each experiment, but rather tuned the methods to be as generic and automatic as possible for each group. In that sense, the solution may be suboptimal, but is representative of the daily use of image analysis. Perhaps some errors can be fixed by tweaking parameters for individual images or experiments, but we are not interested in these types of solutions, given that our goal is to minimize manual overhead work.Top three solutions.Best-performing solution. The system was on the basis of an ensemble strategy with eight fully convolutional neural network architectures; for each, four replicate models were trained resulting in a total of 32 trained segmentation networks in the final solution. All eight base architectures followed the encoder-decoder principle to process an input image and generate the segmentation map in the output. Six of these base architectures used U-Netlike decoders17The team reported that properly modeling the target masks for training U-Net or FPN models was critical to achieve the best performance. In their final solution, they incorporated an approach on the basis of nuclei masks separated by artificially generated boundaries. Then, the task of a segmentation network was to classify pixels into three types: background pixels, interior of cells and boundary pixels. The best performance was obtained when the boundary pixels were marked between only touching cells. Previous works have also considered modeling the target masks in a similar way18,20,52, which is equivalent to a semantic segmentation approach to separate instances.The combination of outputs from the 32 networks was performed in three steps: first, aggregation of predicted masks using the mean of all, second using a ranking model to filter out noisy predictions and third applying a watershed algorithm to refine boundaries. The ranking model of the second step used classical morphological features extracted from each candidate nucleus. These features were used to train a regression model (gradient-boosted trees) that learned to predict IoU scores from ground-truth examples. During test, each candidate object was post-processed in this way to estimate how well it aligned with a potentially real object. This strategy allowed scoring many segmentation masks and ranking them from the most to the least promising one, which was useful to identify and remove false predictions.The team focused on preventing overfitting with two strategies: (1) using neural networks pretrained on the popular ImageNet database 53 as feature encoders for all eight architectures; and (2) using heavy data augmentation to harness the training examples as efficiently as possible. A total of 24 augmentation routines, including channel shuffling, color inversion and object copying, were used for training all models. Additional microscopy images from publicly available databases were also employed by this team to expand the pool of training examples, including Wikimedia images, which were manually annotated by them. The open source code is available at https://github.com/selimsef/dsb2018_topcodersSecond-best-performing solution. The system was a single neural network model on the basis of the FPN architecture47. The solution introduced two customized output layers, each producing multichannel-relative position masks with estimated distances of each nucleus to their boundaries in four directions (vertical, horizontal, 45 degrees and 135 degrees). Relative position masks are analogous to the 'deltas' or distances of pixels with respect to anchor reference points in region proposal networks54. Importantly, the coordinate maps were computed densely for every pixel in the interior of nuclei, whereas pixels in the background were set to zero. Also, relative position masks were post-processed and transformed into boundaries, refined with watershed and ranked by consistency between local and global scores to select the final set of nonoverlapping masks.The backbone FPN in this solution was pretrained on the ImageNet 53 and COCO 55 datasets using the Matterport implementation of the Mask-RCNN 33 framework. The two output layers were trained using a multitask framework.A new loss function was introduced to penalize instance errors by the size of objects, balancing the contribution of errors by small objects with respect to large objects. Various data augmentation techniques were applied during training and testing and no external data were used. Test-time data augmentation consisted of making predictions on transformed versions of the test image (such as scaling or rotation) and then integrating those predictions in a single output. The open source code is available at https://github.com/jacobkie/2018DSB.Third-best-performing solution.This solution was a Mask-RCNN model, pretrained with the COCO dataset 55 for detecting and segmenting objects in natural images. The solution included data augmentations that were meaningful in the biological context, including simulated magnifications of microscopes by scaling images up and down artificially. Aspect ratio modifications, flips and rotations were also used. The training dataset was balanced with respect to types of image, although no special analysis was used to determine the image type; only image size was considered to oversample underrepresented images with random augmentations. Additional data augmentations were also applied during training, and the model was not retrained for the stage-two evaluation with additional data or more iterations. This leaves room to investigate the role of more data when using this model.To generate segmentations for new images, the participant introduced 15 test-time data augmentations, which looked at the test image under different transformations and aggregated the predictions in a single output. These transformations included rotations in different angles, image scaling and color shifts. This was one of the differences of the participant's approach with respect to others that also used Mask-RCNN without the same success. The participant also reported that the simpler post-processing techniques, such as morphological dilation, may reach similar performance and that the parameter configuration and data augmentation during training seemed to be more important according to his experiments. The open source code is available at https://github.com/Lopezurrutia/ DSB_2018.Reporting Summary. Further information on research design is available in the Nature Research Reporting Summary linked to this article.Code availabilityThe methods of the top three participants are publicly available with usage instructions and parameters described by their creators. The authors of this manuscript did not implement and do not maintain these repositories. All the credits and copyright belong to the top three participants who created these models for the 2018 Data Science Bowl challenge.We analyzed data from the participants, downloaded from the Kaggle website using administrative permissions provided by them. Custom code was developed to investigate the patterns and trends in the submitted entries. All the data used to complete the analysis are not made publicly available owing to Kaggle's privacy policies. Instead, aggregated results and code are available at https://github.com/ carpenterlab/2019_caicedo_dsb.Data availabilityThe dataset is publicly accessible in the Broad Bioimage Benchmark Collection with accession number BBBC038.FOCUS | AnAlysisAnAlysis | FOCUS NaTure MeThoDS\nMachine learning in cell biology-teaching computers to recognize phenotypes. C Sommer, D W Gerlich, J. Cell Sci. 126Sommer, C. & Gerlich, D. W. Machine learning in cell biology-teaching computers to recognize phenotypes. J. Cell Sci. 126, 5529-5539 (2013).\n\nMicroscopy-based high-content screening. M Boutros, F Heigwer, C Laufer, Cell. 163Boutros, M., Heigwer, F. & Laufer, C. Microscopy-based high-content screening. Cell 163, 1314-1325 (2015).\n\nHigh-content screening for quantitative cell biology. M Mattiazzi Usaj, Trends Cell Biol. 26Mattiazzi Usaj, M. et al. High-content screening for quantitative cell biology. Trends Cell Biol. 26, 598-611 (2016).\n\nFiji: an open-source platform for biological image analysis. J Schindelin, Nat. Methods. 9Schindelin, J. et al. Fiji: an open-source platform for biological image analysis. Nat. Methods 9, 676-682 (2012).\n\nCellProfiler 3.0: next-generation image processing for biology. C Mcquin, PLoS Biol. 162005970McQuin, C. et al. CellProfiler 3.0: next-generation image processing for biology. PLoS Biol. 16, e2005970 (2018).\n\nReview of free software tools for image analysis of fluorescence cell micrographs. V Wiesmann, J. Microsc. 257Wiesmann, V. et al. Review of free software tools for image analysis of fluorescence cell micrographs. J. Microsc. 257, 39-53 (2015).\n\nA threshold selection method from Gray-level histograms. N Otsu, IEEE Trans. Syst. Man. Cybern. 9Otsu, N. A threshold selection method from Gray-level histograms. IEEE Trans. Syst. Man. Cybern. 9, 62-66 (1979).\n\nApplying watershed algorithms to the segmentation of clustered nuclei. N Malpica, Cytom. A. 28Malpica, N. et al. Applying watershed algorithms to the segmentation of clustered nuclei. Cytom. A 28, 289-297 (1998).\n\nActive contours without edges. T F Chan, L A Vese, IEEE Trans. Image Process. 10Chan, T. F. & Vese, L. A. Active contours without edges. IEEE Trans. Image Process. 10, 266-277 (2001).\n\nComparison of segmentation algorithms for fluorescence microscopy images of cells. A A Dima, Cytom. A. 79Dima, A. A. et al. Comparison of segmentation algorithms for fluorescence microscopy images of cells. Cytom. A 79, 545-559 (2011).\n\nCell segmentation: 50 years down the road. E Meijering, IEEE Signal Process. Mag. 29Meijering, E. Cell segmentation: 50 years down the road. IEEE Signal Process. Mag. 29, 140-145 (2012).\n\nAn objective comparison of cell-tracking algorithms. V Ulman, Nat. Methods. 14Ulman, V. et al. An objective comparison of cell-tracking algorithms. Nat. Methods 14, 1141-1152 (2017).\n\nIlastik: Interactive learning and segmentation toolkit. in Biomedical Imaging: From Nano to Macro. C Sommer, C Straehle, U Kothe, F A Hamprecht, IEEE International Symposium. 230233Sommer, C., Straehle, C., Kothe, U. & Hamprecht, F. A. Ilastik: Interactive learning and segmentation toolkit. in Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium 230-233 (2011).\n\nU-Net: deep learning for cell counting, detection, and morphometry. T Falk, Nat. Methods. 16Falk, T. et al. U-Net: deep learning for cell counting, detection, and morphometry. Nat. Methods 16, 67-70 (2019).\n\nMachine-learning applications in cell image analysis. A Kan, Immunol. Cell Biol. 95Kan, A. Machine-learning applications in cell image analysis. Immunol. Cell Biol. 95, 525-530 (2017).\n\nDeep learning for cellular image analysis. E Moen, 10.1038/s41592-019-0403-1Nat. Methods. Moen, E. et al. Deep learning for cellular image analysis. Nat. Methods https://doi.org/10.1038/s41592-019-0403-1 (2019).\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Med. Image Comput. Comput. Assist. Interv. 9351Ronneberger, O., Fischer, P. & Brox, T. U-net: Convolutional networks for biomedical image segmentation. Med. Image Comput. Comput. Assist. Interv. 9351, 234-241 (2015).\n\nDeep learning automates the quantitative analysis of individual cells in live-cell imaging experiments. D A Van Valen, PLoS Comput. Biol. 121005177Van Valen, D. A. et al. Deep learning automates the quantitative analysis of individual cells in live-cell imaging experiments. PLoS Comput. Biol. 12, e1005177 (2016).\n\nAutomated training of deep convolutional neural networks for cell segmentation. S K Sadanandan, P Ranefall, S Le Guyader, C W\u00e4hlby, Sci. Rep. 77860Sadanandan, S. K., Ranefall, P., Le Guyader, S. & W\u00e4hlby, C. Automated training of deep convolutional neural networks for cell segmentation. Sci. Rep. 7, 7860 (2017).\n\nEvaluation of deep learning strategies for nucleus segmentation in fluorescence images. J C Caicedo, Cytometry A. 95Caicedo, J. C. et al. Evaluation of deep learning strategies for nucleus segmentation in fluorescence images. Cytometry A 95, 952-965 (2019).\n\nA deep learning framework for nucleus segmentation using image style transfer. R Hollandi, 10.1101/580605Preprint at bioRxivHollandi, R. et al. A deep learning framework for nucleus segmentation using image style transfer. Preprint at bioRxiv https://doi.org/10.1101/580605 (2019).\n\nRobust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review. F Xing, L Yang, IEEE Rev. Biomed. Eng. 9Xing, F. & Yang, L. Robust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review. IEEE Rev. Biomed. Eng. 9, 234-263 (2016).\n\nStacked sparse autoencoder (SSAE) for nuclei detection on breast cancer histopathology images. J Xu, IEEE Trans. Med. Imaging. 35Xu, J. et al. Stacked sparse autoencoder (SSAE) for nuclei detection on breast cancer histopathology images. IEEE Trans. Med. Imaging 35, 119-130 (2016).\n\n. A Jungo, Medical Image Computing and Computer Assisted Intervention -MICCAI. Frangi, A. et al.SpringerJungo, A. et al. in Medical Image Computing and Computer Assisted Intervention -MICCAI 2018 (eds Frangi, A. et al.) 682-690 (Springer, 2018).\n\nDetection and segmentation of cell nuclei in virtual microscopy images: a minimum-model approach. S Wienert, Sci. Rep. 2503Wienert, S. et al. Detection and segmentation of cell nuclei in virtual microscopy images: a minimum-model approach. Sci. Rep. 2, 503 (2012).\n\nAutomatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks. A Cruz-Roa, In Medical Imaging. 9041904103International Society for Optics and PhotonicsDigital PathologyCruz-Roa, A. et al. Automatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks. In Medical Imaging 2014: Digital Pathology 9041, 904103 (International Society for Optics and Photonics, 2014).\n\nObject-oriented segmentation of cell nuclei in fluorescence microscopy images. C F Koyuncu, R Cetin-Atalay, C Gunduz-Demir, Cytometry A. 93Koyuncu, C. F., Cetin-Atalay, R. & Gunduz-Demir, C. Object-oriented segmentation of cell nuclei in fluorescence microscopy images. Cytometry A 93, 1019-1028 (2018).\n\nComparison of different classifiers with active learning to support quality control in nucleus segmentation in pathology images. S Wen, AMIA Jt Summits Transl Sci. Proc. 2017Wen, S. et al. Comparison of different classifiers with active learning to support quality control in nucleus segmentation in pathology images. AMIA Jt Summits Transl Sci. Proc. 2017, 227-236 (2018).\n\nT Bolukbasi, K.-W Chang, J Y Zou, V Saligrama, A Kalai, Advances in Neural Information Processing Systems. Lee, D. D. et al.Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V. & Kalai, A. T. in Advances in Neural Information Processing Systems (eds Lee, D. D. et al.) 4349-4357 (papers.nips.cc, 2016).\n\nGender shades: intersectional accuracy disparities in commercial gender classification. J Buolamwini, T Gebru, Proc. 1st Conference on Fairness, Accountability and Transparency. Friedler, S. A. & Wilson, C.1st Conference on Fairness, Accountability and TransparencyPMLR81Buolamwini, J. & Gebru, T. Gender shades: intersectional accuracy disparities in commercial gender classification. In Proc. 1st Conference on Fairness, Accountability and Transparency Vol. 81 (eds. Friedler, S. A. & Wilson, C.) 77-91 (PMLR, 2018).\n\nThe Pascal Visual Object Classes (VOC) Challenge. M Everingham, L Van Gool, C K I Williams, J Winn, A Zisserman, Int. J. Comput. Vis. 88Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J. & Zisserman, A. The Pascal Visual Object Classes (VOC) Challenge. Int. J. Comput. Vis. 88, 303-338 (2010).\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 521LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436-444 (2015).\n\nK He, G Gkioxari, P Doll\u00e1r, R Girshick, R-Cnn Mask, Proc. 2017 IEEE International Conference on Computer Vision. 2017 IEEE International Conference on Computer VisionHe, K., Gkioxari, G., Doll\u00e1r, P. & Girshick, R. Mask R-CNN. In Proc. 2017 IEEE International Conference on Computer Vision 2980-2988 (ICCV, 2017).\n\n. D Poplavskiy, Data Science Bowl-Discussion. 55118Poplavskiy, D. 2018 Data Science Bowl-Discussion 55118 https://www.kaggle. com/c/data-science-bowl-2018/discussion/55118 (2018).\n\nApplying deep watershed transform to Kaggle data Science Bowl 2018 (dockerized solution). \u0410 \u0412\u0435\u0439\u0441\u043e\u0432, Spark in Me\u0412\u0435\u0439\u0441\u043e\u0432, \u0410. Applying deep watershed transform to Kaggle data Science Bowl 2018 (dockerized solution). Spark in Me http://spark-in.me/post/playing-with- dwt-and-ds-bowl-2018 (2018).\n\nDeep watershed transform for instance segmentation. M Bai, R Urtasun, Proc. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)IEEEBai, M. & Urtasun, R. Deep watershed transform for instance segmentation. In Proc. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2858-2866 (IEEE, 2017).\n\n. A Torrubia, Data Science Bowl-Discussion. 54816Torrubia, A. 2018 Data Science Bowl-Discussion 54816 https://www.kaggle. com/c/data-science-bowl-2018/discussion/54816 (2018).\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, IEEE International Conference on Computer Vision (ICCV). IEEEZhu, J.-Y., Park, T., Isola, P. & Efros, A. A. Unpaired image-to-image translation using cycle-consistent adversarial networks. in IEEE International Conference on Computer Vision (ICCV) 2223-2232 (IEEE, 2017).\n\n. Kamil, Data Science Bowl-Discussion. 47590Kamil. 2018 Data Science Bowl-Discussion 47590 https://www.kaggle.com/c/ data-science-bowl-2018/discussion/47590 (2018).\n\nWaleed , Data Science Bowl-Discussion54089. Waleed. 2018 Data Science Bowl-Discussion54089 https://www.kaggle.com/c/ data-science-bowl-2018/discussion/54089 (2018).\n\nSLIC superpixels compared to state-of-the-art superpixel methods. R Achanta, IEEE Trans. Pattern Anal. Mach. Intel. 34Achanta, R. et al. SLIC superpixels compared to state-of-the-art superpixel methods. IEEE Trans. Pattern Anal. Mach. Intel. 34, 2274-2282 (2012).\n\nMorphological segmentation. F Meyer, S Beucher, J. Vis. Commun. Image Represent. 1Meyer, F. & Beucher, S. Morphological segmentation. J. Vis. Commun. Image Represent. 1, 21-46 (1990).\n\nSegmentation of confocal microscope images of cell nuclei in thick tissue sections. C Ortiz De Sol\u00f3rzano, J. Microsc. 193Ortiz de Sol\u00f3rzano, C. et al. Segmentation of confocal microscope images of cell nuclei in thick tissue sections. J. Microsc. 193, 212-226 (1999).\n\nAlgorithms for Applied Digital Image Cytometry PhD thesis. C W\u00e4hlby, Acta Universitatis UpsaliensisW\u00e4hlby, C. Algorithms for Applied Digital Image Cytometry PhD thesis, Acta Universitatis Upsaliensis (2003).\n\nCombining intensity, edge and shape information for 2D and 3D segmentation of cell nuclei in tissue sections. C W\u00e4hlby, I.-M Sintorn, F Erlandsson, G Borgefors, E Bengtsson, J. Microsc. 215W\u00e4hlby, C., Sintorn, I.-M., Erlandsson, F., Borgefors, G. & Bengtsson, E. Combining intensity, edge and shape information for 2D and 3D segmentation of cell nuclei in tissue sections. J. Microsc. 215, 67-76 (2004).\n\nCellProfiler: image analysis software for identifying and quantifying cell phenotypes. A E Carpenter, Genome Biol. 7100Carpenter, A. E. et al. CellProfiler: image analysis software for identifying and quantifying cell phenotypes. Genome Biol. 7, R100 (2006).\n\nFeature pyramid networks for object detection. T.-Y Lin, Proc. IEEE conference on computer vision and pattern recognition (CVPR). IEEE conference on computer vision and pattern recognition (CVPR)IEEELin, T.-Y. et al. Feature pyramid networks for object detection. Proc. IEEE conference on computer vision and pattern recognition (CVPR) 2117-2125 (IEEE, 2017).\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEHe, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 770-778 (IEEE, 2016)\n\nIdentity Mappings in Deep Residual Networks. K He, X Zhang, S Ren, J Sun, Computer Vision-ECCV 2016. Leibe, B. et al.SpringerHe, K., Zhang, X., Ren, S. & Sun, J. Identity Mappings in Deep Residual Networks. In Computer Vision-ECCV 2016 (eds Leibe, B. et al.) 630-645 (Springer, 2016).\n\nDual path networks. Y Chen, Adv. Neural Inf. Proc. Syst. 30Chen, Y. et al. Dual path networks. Adv. Neural Inf. Proc. Syst. 30, 4467-4475 (2017).\n\nInception-v4, inceptionresnet and the impact of residual connections on learning. C Szegedy, S Ioffe, V Vanhoucke, A A Alemi, Proc. 31st AAAI Conference. 31st AAAI ConferenceSzegedy, C., Ioffe, S., Vanhoucke, V. & Alemi, A. A. Inception-v4, inception- resnet and the impact of residual connections on learning. In Proc. 31st AAAI Conference (2017).\n\nA deep learning algorithm for one-step contour aware nuclei segmentation of histopathological images. Y Cui, G Zhang, Z Liu, Z Xiong, J Hu, Med. Bio. Eng. Comp. 57Cui, Y., Zhang, G., Liu, Z., Xiong, Z. & Hu, J. A deep learning algorithm for one-step contour aware nuclei segmentation of histopathological images. Med. Bio. Eng. Comp. 57, 2027-2043 (2019).\n\nImageNet large scale visual recognition challenge. O Russakovsky, Int. J. Comput. Vis. 115Russakovsky, O. et al. ImageNet large scale visual recognition challenge. Int. J. Comput. Vis. 115, 211-252 (2015).\n\nS Ren, K He, R Girshick, J Sun, Advances in Neural Information Processing Systems. Cortes, C. et al.Curran Associates28Ren, S., He, K., Girshick, R. & Sun, J. in Advances in Neural Information Processing Systems 28 (eds. Cortes, C. et al.) 91-99 (Curran Associates, 2015).\n\nMicrosoft COCO: common objects in context. T.-Y Lin, Computer Vision-ECCV 2014. Fleet, D. et al.SpringerLin, T.-Y. et al. Microsoft COCO: common objects in context. In Computer Vision-ECCV 2014 (eds Fleet, D. et al.) 740-755 (Springer, 2014).\n", "annotations": {"author": "[{\"end\":154,\"start\":79},{\"end\":211,\"start\":155},{\"end\":278,\"start\":212},{\"end\":344,\"start\":279},{\"end\":411,\"start\":345},{\"end\":477,\"start\":412},{\"end\":547,\"start\":478},{\"end\":616,\"start\":548},{\"end\":682,\"start\":617},{\"end\":745,\"start\":683},{\"end\":807,\"start\":746},{\"end\":873,\"start\":808},{\"end\":941,\"start\":874},{\"end\":1008,\"start\":942},{\"end\":1077,\"start\":1009},{\"end\":154,\"start\":79},{\"end\":211,\"start\":155},{\"end\":278,\"start\":212},{\"end\":344,\"start\":279},{\"end\":411,\"start\":345},{\"end\":477,\"start\":412},{\"end\":547,\"start\":478},{\"end\":616,\"start\":548},{\"end\":682,\"start\":617},{\"end\":745,\"start\":683},{\"end\":807,\"start\":746},{\"end\":873,\"start\":808},{\"end\":941,\"start\":874},{\"end\":1008,\"start\":942},{\"end\":1077,\"start\":1009}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":94},{\"end\":226,\"start\":219},{\"end\":292,\"start\":285},{\"end\":359,\"start\":352},{\"end\":425,\"start\":419},{\"end\":495,\"start\":487},{\"end\":564,\"start\":556},{\"end\":630,\"start\":626},{\"end\":693,\"start\":687},{\"end\":755,\"start\":751},{\"end\":821,\"start\":815},{\"end\":889,\"start\":883},{\"end\":956,\"start\":951},{\"end\":1025,\"start\":1016},{\"end\":102,\"start\":94},{\"end\":226,\"start\":219},{\"end\":292,\"start\":285},{\"end\":359,\"start\":352},{\"end\":425,\"start\":419},{\"end\":495,\"start\":487},{\"end\":564,\"start\":556},{\"end\":630,\"start\":626},{\"end\":693,\"start\":687},{\"end\":755,\"start\":751},{\"end\":821,\"start\":815},{\"end\":889,\"start\":883},{\"end\":956,\"start\":951},{\"end\":1025,\"start\":1016}]", "author_first_name": "[{\"end\":91,\"start\":79},{\"end\":93,\"start\":92},{\"end\":159,\"start\":155},{\"end\":216,\"start\":212},{\"end\":218,\"start\":217},{\"end\":284,\"start\":279},{\"end\":349,\"start\":345},{\"end\":351,\"start\":350},{\"end\":416,\"start\":412},{\"end\":418,\"start\":417},{\"end\":486,\"start\":478},{\"end\":555,\"start\":548},{\"end\":625,\"start\":617},{\"end\":686,\"start\":683},{\"end\":750,\"start\":746},{\"end\":814,\"start\":808},{\"end\":882,\"start\":874},{\"end\":950,\"start\":942},{\"end\":1013,\"start\":1009},{\"end\":1015,\"start\":1014},{\"end\":91,\"start\":79},{\"end\":93,\"start\":92},{\"end\":159,\"start\":155},{\"end\":216,\"start\":212},{\"end\":218,\"start\":217},{\"end\":284,\"start\":279},{\"end\":349,\"start\":345},{\"end\":351,\"start\":350},{\"end\":416,\"start\":412},{\"end\":418,\"start\":417},{\"end\":486,\"start\":478},{\"end\":555,\"start\":548},{\"end\":625,\"start\":617},{\"end\":686,\"start\":683},{\"end\":750,\"start\":746},{\"end\":814,\"start\":808},{\"end\":882,\"start\":874},{\"end\":950,\"start\":942},{\"end\":1013,\"start\":1009},{\"end\":1015,\"start\":1014}]", "author_affiliation": "[{\"end\":153,\"start\":104},{\"end\":210,\"start\":161},{\"end\":277,\"start\":228},{\"end\":343,\"start\":294},{\"end\":410,\"start\":361},{\"end\":476,\"start\":427},{\"end\":546,\"start\":497},{\"end\":615,\"start\":566},{\"end\":681,\"start\":632},{\"end\":744,\"start\":695},{\"end\":806,\"start\":757},{\"end\":872,\"start\":823},{\"end\":940,\"start\":891},{\"end\":1007,\"start\":958},{\"end\":1076,\"start\":1027},{\"end\":153,\"start\":104},{\"end\":210,\"start\":161},{\"end\":277,\"start\":228},{\"end\":343,\"start\":294},{\"end\":410,\"start\":361},{\"end\":476,\"start\":427},{\"end\":546,\"start\":497},{\"end\":615,\"start\":566},{\"end\":681,\"start\":632},{\"end\":744,\"start\":695},{\"end\":806,\"start\":757},{\"end\":872,\"start\":823},{\"end\":940,\"start\":891},{\"end\":1007,\"start\":958},{\"end\":1076,\"start\":1027}]", "title": "[{\"end\":76,\"start\":1},{\"end\":1153,\"start\":1078},{\"end\":76,\"start\":1},{\"end\":1153,\"start\":1078}]", "venue": null, "abstract": "[{\"end\":2108,\"start\":1245},{\"end\":2108,\"start\":1245}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2393,\"start\":2390},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2396,\"start\":2393},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2399,\"start\":2396},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3547,\"start\":3546},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3582,\"start\":3581},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4242,\"start\":4238},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4246,\"start\":4242},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4250,\"start\":4246},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5527,\"start\":5525},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5541,\"start\":5539},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5605,\"start\":5603},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5705,\"start\":5703},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5772,\"start\":5768},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5776,\"start\":5772},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5780,\"start\":5776},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5784,\"start\":5780},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12269,\"start\":12266},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12272,\"start\":12269},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12274,\"start\":12272},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12850,\"start\":12848},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13854,\"start\":13852},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14945,\"start\":14942},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14948,\"start\":14945},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14951,\"start\":14948},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14955,\"start\":14951},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14959,\"start\":14955},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14963,\"start\":14959},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14967,\"start\":14963},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15385,\"start\":15382},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15387,\"start\":15385},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16614,\"start\":16611},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16618,\"start\":16614},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18646,\"start\":18643},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18648,\"start\":18646},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21831,\"start\":21829},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24324,\"start\":24322},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25011,\"start\":25009},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29048,\"start\":29046},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32400,\"start\":32398},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2393,\"start\":2390},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2396,\"start\":2393},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2399,\"start\":2396},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3547,\"start\":3546},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3582,\"start\":3581},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4242,\"start\":4238},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4246,\"start\":4242},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4250,\"start\":4246},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5527,\"start\":5525},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5541,\"start\":5539},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5605,\"start\":5603},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5705,\"start\":5703},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5772,\"start\":5768},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5776,\"start\":5772},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5780,\"start\":5776},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5784,\"start\":5780},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12269,\"start\":12266},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12272,\"start\":12269},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12274,\"start\":12272},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12850,\"start\":12848},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13854,\"start\":13852},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14945,\"start\":14942},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14948,\"start\":14945},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14951,\"start\":14948},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14955,\"start\":14951},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14959,\"start\":14955},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14963,\"start\":14959},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14967,\"start\":14963},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15385,\"start\":15382},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15387,\"start\":15385},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16614,\"start\":16611},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16618,\"start\":16614},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18646,\"start\":18643},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18648,\"start\":18646},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21831,\"start\":21829},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24324,\"start\":24322},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25011,\"start\":25009},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29048,\"start\":29046},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32400,\"start\":32398}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42308,\"start\":41540},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43703,\"start\":42309},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43795,\"start\":43704},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44806,\"start\":43796},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45640,\"start\":44807},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":46085,\"start\":45641},{\"attributes\":{\"id\":\"fig_0\"},\"end\":42308,\"start\":41540},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43703,\"start\":42309},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43795,\"start\":43704},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44806,\"start\":43796},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45640,\"start\":44807},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":46085,\"start\":45641}]", "paragraph": "[{\"end\":2878,\"start\":2110},{\"end\":3298,\"start\":2880},{\"end\":4424,\"start\":3300},{\"end\":5402,\"start\":4426},{\"end\":7384,\"start\":5404},{\"end\":8265,\"start\":7432},{\"end\":9466,\"start\":8267},{\"end\":10168,\"start\":9468},{\"end\":10647,\"start\":10170},{\"end\":11228,\"start\":10649},{\"end\":11936,\"start\":11230},{\"end\":12514,\"start\":11938},{\"end\":13660,\"start\":12516},{\"end\":14505,\"start\":13662},{\"end\":15877,\"start\":14507},{\"end\":16412,\"start\":15879},{\"end\":17911,\"start\":16414},{\"end\":18377,\"start\":17913},{\"end\":19246,\"start\":18379},{\"end\":20164,\"start\":19248},{\"end\":20745,\"start\":20166},{\"end\":21581,\"start\":20819},{\"end\":22284,\"start\":21583},{\"end\":24198,\"start\":22435},{\"end\":24235,\"start\":24219},{\"end\":24537,\"start\":24254},{\"end\":25366,\"start\":24539},{\"end\":26120,\"start\":25381},{\"end\":26880,\"start\":26122},{\"end\":27655,\"start\":26882},{\"end\":28602,\"start\":27657},{\"end\":30125,\"start\":28604},{\"end\":30362,\"start\":30144},{\"end\":30938,\"start\":30402},{\"end\":31656,\"start\":30940},{\"end\":32253,\"start\":31658},{\"end\":33184,\"start\":32255},{\"end\":33957,\"start\":33186},{\"end\":34845,\"start\":33959},{\"end\":35545,\"start\":34969},{\"end\":35868,\"start\":35606},{\"end\":36650,\"start\":35870},{\"end\":37380,\"start\":36652},{\"end\":37937,\"start\":37382},{\"end\":38535,\"start\":37939},{\"end\":39129,\"start\":38537},{\"end\":39546,\"start\":39131},{\"end\":40267,\"start\":39548},{\"end\":41128,\"start\":40269},{\"end\":41466,\"start\":41130},{\"end\":41539,\"start\":41523},{\"end\":2878,\"start\":2110},{\"end\":3298,\"start\":2880},{\"end\":4424,\"start\":3300},{\"end\":5402,\"start\":4426},{\"end\":7384,\"start\":5404},{\"end\":8265,\"start\":7432},{\"end\":9466,\"start\":8267},{\"end\":10168,\"start\":9468},{\"end\":10647,\"start\":10170},{\"end\":11228,\"start\":10649},{\"end\":11936,\"start\":11230},{\"end\":12514,\"start\":11938},{\"end\":13660,\"start\":12516},{\"end\":14505,\"start\":13662},{\"end\":15877,\"start\":14507},{\"end\":16412,\"start\":15879},{\"end\":17911,\"start\":16414},{\"end\":18377,\"start\":17913},{\"end\":19246,\"start\":18379},{\"end\":20164,\"start\":19248},{\"end\":20745,\"start\":20166},{\"end\":21581,\"start\":20819},{\"end\":22284,\"start\":21583},{\"end\":24198,\"start\":22435},{\"end\":24235,\"start\":24219},{\"end\":24537,\"start\":24254},{\"end\":25366,\"start\":24539},{\"end\":26120,\"start\":25381},{\"end\":26880,\"start\":26122},{\"end\":27655,\"start\":26882},{\"end\":28602,\"start\":27657},{\"end\":30125,\"start\":28604},{\"end\":30362,\"start\":30144},{\"end\":30938,\"start\":30402},{\"end\":31656,\"start\":30940},{\"end\":32253,\"start\":31658},{\"end\":33184,\"start\":32255},{\"end\":33957,\"start\":33186},{\"end\":34845,\"start\":33959},{\"end\":35545,\"start\":34969},{\"end\":35868,\"start\":35606},{\"end\":36650,\"start\":35870},{\"end\":37380,\"start\":36652},{\"end\":37937,\"start\":37382},{\"end\":38535,\"start\":37939},{\"end\":39129,\"start\":38537},{\"end\":39546,\"start\":39131},{\"end\":40267,\"start\":39548},{\"end\":41128,\"start\":40269},{\"end\":41466,\"start\":41130},{\"end\":41539,\"start\":41523}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":34968,\"start\":34846},{\"attributes\":{\"id\":\"formula_1\"},\"end\":35605,\"start\":35546},{\"attributes\":{\"id\":\"formula_0\"},\"end\":34968,\"start\":34846},{\"attributes\":{\"id\":\"formula_1\"},\"end\":35605,\"start\":35546}]", "table_ref": "[{\"end\":16745,\"start\":16738},{\"end\":17095,\"start\":17073},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19033,\"start\":19026},{\"end\":16745,\"start\":16738},{\"end\":17095,\"start\":17073},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19033,\"start\":19026}]", "section_header": "[{\"end\":7403,\"start\":7387},{\"end\":7420,\"start\":7406},{\"end\":7430,\"start\":7423},{\"end\":20764,\"start\":20748},{\"end\":20781,\"start\":20767},{\"end\":20800,\"start\":20784},{\"end\":20817,\"start\":20803},{\"end\":22433,\"start\":22287},{\"end\":24217,\"start\":24201},{\"end\":24252,\"start\":24238},{\"end\":25379,\"start\":25369},{\"end\":30142,\"start\":30128},{\"end\":30373,\"start\":30365},{\"end\":30390,\"start\":30376},{\"end\":30400,\"start\":30393},{\"end\":41485,\"start\":41469},{\"end\":41504,\"start\":41488},{\"end\":41521,\"start\":41507},{\"end\":42318,\"start\":42310},{\"end\":43805,\"start\":43797},{\"end\":45651,\"start\":45642},{\"end\":7403,\"start\":7387},{\"end\":7420,\"start\":7406},{\"end\":7430,\"start\":7423},{\"end\":20764,\"start\":20748},{\"end\":20781,\"start\":20767},{\"end\":20800,\"start\":20784},{\"end\":20817,\"start\":20803},{\"end\":22433,\"start\":22287},{\"end\":24217,\"start\":24201},{\"end\":24252,\"start\":24238},{\"end\":25379,\"start\":25369},{\"end\":30142,\"start\":30128},{\"end\":30373,\"start\":30365},{\"end\":30390,\"start\":30376},{\"end\":30400,\"start\":30393},{\"end\":41485,\"start\":41469},{\"end\":41504,\"start\":41488},{\"end\":41521,\"start\":41507},{\"end\":42318,\"start\":42310},{\"end\":43805,\"start\":43797},{\"end\":45651,\"start\":45642}]", "table": "[{\"end\":45640,\"start\":44858},{\"end\":46085,\"start\":45709},{\"end\":45640,\"start\":44858},{\"end\":46085,\"start\":45709}]", "figure_caption": "[{\"end\":42308,\"start\":41542},{\"end\":43703,\"start\":42320},{\"end\":43795,\"start\":43706},{\"end\":44806,\"start\":43807},{\"end\":44858,\"start\":44809},{\"end\":45709,\"start\":45653},{\"end\":42308,\"start\":41542},{\"end\":43703,\"start\":42320},{\"end\":43795,\"start\":43706},{\"end\":44806,\"start\":43807},{\"end\":44858,\"start\":44809},{\"end\":45709,\"start\":45653}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10911,\"start\":10902},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11096,\"start\":11087},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11679,\"start\":11670},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13420,\"start\":13411},{\"end\":13829,\"start\":13823},{\"end\":14013,\"start\":13992},{\"end\":14192,\"start\":14172},{\"end\":14345,\"start\":14325},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15258,\"start\":15249},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15310,\"start\":15301},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15490,\"start\":15481},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16128,\"start\":16118},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17132,\"start\":17123},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17699,\"start\":17690},{\"end\":18175,\"start\":18154},{\"end\":18233,\"start\":18213},{\"end\":19125,\"start\":19117},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10911,\"start\":10902},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11096,\"start\":11087},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11679,\"start\":11670},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13420,\"start\":13411},{\"end\":13829,\"start\":13823},{\"end\":14013,\"start\":13992},{\"end\":14192,\"start\":14172},{\"end\":14345,\"start\":14325},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15258,\"start\":15249},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15310,\"start\":15301},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15490,\"start\":15481},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16128,\"start\":16118},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17132,\"start\":17123},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17699,\"start\":17690},{\"end\":18175,\"start\":18154},{\"end\":18233,\"start\":18213},{\"end\":19125,\"start\":19117}]", "bib_author_first_name": "[{\"end\":58061,\"start\":58060},{\"end\":58071,\"start\":58070},{\"end\":58073,\"start\":58072},{\"end\":58283,\"start\":58282},{\"end\":58294,\"start\":58293},{\"end\":58305,\"start\":58304},{\"end\":58486,\"start\":58485},{\"end\":58704,\"start\":58703},{\"end\":58913,\"start\":58912},{\"end\":59141,\"start\":59140},{\"end\":59360,\"start\":59359},{\"end\":59586,\"start\":59585},{\"end\":59760,\"start\":59759},{\"end\":59762,\"start\":59761},{\"end\":59770,\"start\":59769},{\"end\":59772,\"start\":59771},{\"end\":59997,\"start\":59996},{\"end\":59999,\"start\":59998},{\"end\":60194,\"start\":60193},{\"end\":60392,\"start\":60391},{\"end\":60622,\"start\":60621},{\"end\":60632,\"start\":60631},{\"end\":60644,\"start\":60643},{\"end\":60653,\"start\":60652},{\"end\":60655,\"start\":60654},{\"end\":60977,\"start\":60976},{\"end\":61171,\"start\":61170},{\"end\":61346,\"start\":61345},{\"end\":61581,\"start\":61580},{\"end\":61596,\"start\":61595},{\"end\":61607,\"start\":61606},{\"end\":61937,\"start\":61936},{\"end\":61939,\"start\":61938},{\"end\":62229,\"start\":62228},{\"end\":62231,\"start\":62230},{\"end\":62245,\"start\":62244},{\"end\":62257,\"start\":62256},{\"end\":62271,\"start\":62270},{\"end\":62552,\"start\":62551},{\"end\":62554,\"start\":62553},{\"end\":62802,\"start\":62801},{\"end\":63121,\"start\":63120},{\"end\":63129,\"start\":63128},{\"end\":63434,\"start\":63433},{\"end\":63625,\"start\":63624},{\"end\":63968,\"start\":63967},{\"end\":64243,\"start\":64242},{\"end\":64667,\"start\":64666},{\"end\":64669,\"start\":64668},{\"end\":64680,\"start\":64679},{\"end\":64696,\"start\":64695},{\"end\":65022,\"start\":65021},{\"end\":65268,\"start\":65267},{\"end\":65284,\"start\":65280},{\"end\":65293,\"start\":65292},{\"end\":65295,\"start\":65294},{\"end\":65302,\"start\":65301},{\"end\":65315,\"start\":65314},{\"end\":65662,\"start\":65661},{\"end\":65676,\"start\":65675},{\"end\":66144,\"start\":66143},{\"end\":66158,\"start\":66157},{\"end\":66170,\"start\":66169},{\"end\":66174,\"start\":66171},{\"end\":66186,\"start\":66185},{\"end\":66194,\"start\":66193},{\"end\":66412,\"start\":66411},{\"end\":66421,\"start\":66420},{\"end\":66431,\"start\":66430},{\"end\":66531,\"start\":66530},{\"end\":66537,\"start\":66536},{\"end\":66549,\"start\":66548},{\"end\":66559,\"start\":66558},{\"end\":66575,\"start\":66570},{\"end\":66847,\"start\":66846},{\"end\":67116,\"start\":67115},{\"end\":67370,\"start\":67369},{\"end\":67377,\"start\":67376},{\"end\":67721,\"start\":67720},{\"end\":67980,\"start\":67976},{\"end\":67987,\"start\":67986},{\"end\":67995,\"start\":67994},{\"end\":68004,\"start\":68003},{\"end\":68006,\"start\":68005},{\"end\":68459,\"start\":68453},{\"end\":68686,\"start\":68685},{\"end\":68913,\"start\":68912},{\"end\":68922,\"start\":68921},{\"end\":69154,\"start\":69153},{\"end\":69398,\"start\":69397},{\"end\":69658,\"start\":69657},{\"end\":69671,\"start\":69667},{\"end\":69682,\"start\":69681},{\"end\":69696,\"start\":69695},{\"end\":69709,\"start\":69708},{\"end\":70040,\"start\":70039},{\"end\":70042,\"start\":70041},{\"end\":70263,\"start\":70259},{\"end\":70620,\"start\":70619},{\"end\":70626,\"start\":70625},{\"end\":70635,\"start\":70634},{\"end\":70642,\"start\":70641},{\"end\":70936,\"start\":70935},{\"end\":70942,\"start\":70941},{\"end\":70951,\"start\":70950},{\"end\":70958,\"start\":70957},{\"end\":71197,\"start\":71196},{\"end\":71406,\"start\":71405},{\"end\":71417,\"start\":71416},{\"end\":71426,\"start\":71425},{\"end\":71439,\"start\":71438},{\"end\":71441,\"start\":71440},{\"end\":71776,\"start\":71775},{\"end\":71783,\"start\":71782},{\"end\":71792,\"start\":71791},{\"end\":71799,\"start\":71798},{\"end\":71808,\"start\":71807},{\"end\":72082,\"start\":72081},{\"end\":72238,\"start\":72237},{\"end\":72245,\"start\":72244},{\"end\":72251,\"start\":72250},{\"end\":72263,\"start\":72262},{\"end\":72558,\"start\":72554},{\"end\":58061,\"start\":58060},{\"end\":58071,\"start\":58070},{\"end\":58073,\"start\":58072},{\"end\":58283,\"start\":58282},{\"end\":58294,\"start\":58293},{\"end\":58305,\"start\":58304},{\"end\":58486,\"start\":58485},{\"end\":58704,\"start\":58703},{\"end\":58913,\"start\":58912},{\"end\":59141,\"start\":59140},{\"end\":59360,\"start\":59359},{\"end\":59586,\"start\":59585},{\"end\":59760,\"start\":59759},{\"end\":59762,\"start\":59761},{\"end\":59770,\"start\":59769},{\"end\":59772,\"start\":59771},{\"end\":59997,\"start\":59996},{\"end\":59999,\"start\":59998},{\"end\":60194,\"start\":60193},{\"end\":60392,\"start\":60391},{\"end\":60622,\"start\":60621},{\"end\":60632,\"start\":60631},{\"end\":60644,\"start\":60643},{\"end\":60653,\"start\":60652},{\"end\":60655,\"start\":60654},{\"end\":60977,\"start\":60976},{\"end\":61171,\"start\":61170},{\"end\":61346,\"start\":61345},{\"end\":61581,\"start\":61580},{\"end\":61596,\"start\":61595},{\"end\":61607,\"start\":61606},{\"end\":61937,\"start\":61936},{\"end\":61939,\"start\":61938},{\"end\":62229,\"start\":62228},{\"end\":62231,\"start\":62230},{\"end\":62245,\"start\":62244},{\"end\":62257,\"start\":62256},{\"end\":62271,\"start\":62270},{\"end\":62552,\"start\":62551},{\"end\":62554,\"start\":62553},{\"end\":62802,\"start\":62801},{\"end\":63121,\"start\":63120},{\"end\":63129,\"start\":63128},{\"end\":63434,\"start\":63433},{\"end\":63625,\"start\":63624},{\"end\":63968,\"start\":63967},{\"end\":64243,\"start\":64242},{\"end\":64667,\"start\":64666},{\"end\":64669,\"start\":64668},{\"end\":64680,\"start\":64679},{\"end\":64696,\"start\":64695},{\"end\":65022,\"start\":65021},{\"end\":65268,\"start\":65267},{\"end\":65284,\"start\":65280},{\"end\":65293,\"start\":65292},{\"end\":65295,\"start\":65294},{\"end\":65302,\"start\":65301},{\"end\":65315,\"start\":65314},{\"end\":65662,\"start\":65661},{\"end\":65676,\"start\":65675},{\"end\":66144,\"start\":66143},{\"end\":66158,\"start\":66157},{\"end\":66170,\"start\":66169},{\"end\":66174,\"start\":66171},{\"end\":66186,\"start\":66185},{\"end\":66194,\"start\":66193},{\"end\":66412,\"start\":66411},{\"end\":66421,\"start\":66420},{\"end\":66431,\"start\":66430},{\"end\":66531,\"start\":66530},{\"end\":66537,\"start\":66536},{\"end\":66549,\"start\":66548},{\"end\":66559,\"start\":66558},{\"end\":66575,\"start\":66570},{\"end\":66847,\"start\":66846},{\"end\":67116,\"start\":67115},{\"end\":67370,\"start\":67369},{\"end\":67377,\"start\":67376},{\"end\":67721,\"start\":67720},{\"end\":67980,\"start\":67976},{\"end\":67987,\"start\":67986},{\"end\":67995,\"start\":67994},{\"end\":68004,\"start\":68003},{\"end\":68006,\"start\":68005},{\"end\":68459,\"start\":68453},{\"end\":68686,\"start\":68685},{\"end\":68913,\"start\":68912},{\"end\":68922,\"start\":68921},{\"end\":69154,\"start\":69153},{\"end\":69398,\"start\":69397},{\"end\":69658,\"start\":69657},{\"end\":69671,\"start\":69667},{\"end\":69682,\"start\":69681},{\"end\":69696,\"start\":69695},{\"end\":69709,\"start\":69708},{\"end\":70040,\"start\":70039},{\"end\":70042,\"start\":70041},{\"end\":70263,\"start\":70259},{\"end\":70620,\"start\":70619},{\"end\":70626,\"start\":70625},{\"end\":70635,\"start\":70634},{\"end\":70642,\"start\":70641},{\"end\":70936,\"start\":70935},{\"end\":70942,\"start\":70941},{\"end\":70951,\"start\":70950},{\"end\":70958,\"start\":70957},{\"end\":71197,\"start\":71196},{\"end\":71406,\"start\":71405},{\"end\":71417,\"start\":71416},{\"end\":71426,\"start\":71425},{\"end\":71439,\"start\":71438},{\"end\":71441,\"start\":71440},{\"end\":71776,\"start\":71775},{\"end\":71783,\"start\":71782},{\"end\":71792,\"start\":71791},{\"end\":71799,\"start\":71798},{\"end\":71808,\"start\":71807},{\"end\":72082,\"start\":72081},{\"end\":72238,\"start\":72237},{\"end\":72245,\"start\":72244},{\"end\":72251,\"start\":72250},{\"end\":72263,\"start\":72262},{\"end\":72558,\"start\":72554}]", "bib_author_last_name": "[{\"end\":58068,\"start\":58062},{\"end\":58081,\"start\":58074},{\"end\":58291,\"start\":58284},{\"end\":58302,\"start\":58295},{\"end\":58312,\"start\":58306},{\"end\":58501,\"start\":58487},{\"end\":58715,\"start\":58705},{\"end\":58920,\"start\":58914},{\"end\":59150,\"start\":59142},{\"end\":59365,\"start\":59361},{\"end\":59594,\"start\":59587},{\"end\":59767,\"start\":59763},{\"end\":59777,\"start\":59773},{\"end\":60004,\"start\":60000},{\"end\":60204,\"start\":60195},{\"end\":60398,\"start\":60393},{\"end\":60629,\"start\":60623},{\"end\":60641,\"start\":60633},{\"end\":60650,\"start\":60645},{\"end\":60665,\"start\":60656},{\"end\":60982,\"start\":60978},{\"end\":61175,\"start\":61172},{\"end\":61351,\"start\":61347},{\"end\":61593,\"start\":61582},{\"end\":61604,\"start\":61597},{\"end\":61612,\"start\":61608},{\"end\":61949,\"start\":61940},{\"end\":62242,\"start\":62232},{\"end\":62254,\"start\":62246},{\"end\":62268,\"start\":62258},{\"end\":62278,\"start\":62272},{\"end\":62562,\"start\":62555},{\"end\":62811,\"start\":62803},{\"end\":63126,\"start\":63122},{\"end\":63134,\"start\":63130},{\"end\":63437,\"start\":63435},{\"end\":63631,\"start\":63626},{\"end\":63976,\"start\":63969},{\"end\":64252,\"start\":64244},{\"end\":64677,\"start\":64670},{\"end\":64693,\"start\":64681},{\"end\":64709,\"start\":64697},{\"end\":65026,\"start\":65023},{\"end\":65278,\"start\":65269},{\"end\":65290,\"start\":65285},{\"end\":65299,\"start\":65296},{\"end\":65312,\"start\":65303},{\"end\":65321,\"start\":65316},{\"end\":65673,\"start\":65663},{\"end\":65682,\"start\":65677},{\"end\":66155,\"start\":66145},{\"end\":66167,\"start\":66159},{\"end\":66183,\"start\":66175},{\"end\":66191,\"start\":66187},{\"end\":66204,\"start\":66195},{\"end\":66418,\"start\":66413},{\"end\":66428,\"start\":66422},{\"end\":66438,\"start\":66432},{\"end\":66534,\"start\":66532},{\"end\":66546,\"start\":66538},{\"end\":66556,\"start\":66550},{\"end\":66568,\"start\":66560},{\"end\":66580,\"start\":66576},{\"end\":66858,\"start\":66848},{\"end\":67123,\"start\":67117},{\"end\":67374,\"start\":67371},{\"end\":67385,\"start\":67378},{\"end\":67730,\"start\":67722},{\"end\":67984,\"start\":67981},{\"end\":67992,\"start\":67988},{\"end\":68001,\"start\":67996},{\"end\":68012,\"start\":68007},{\"end\":68294,\"start\":68289},{\"end\":68694,\"start\":68687},{\"end\":68919,\"start\":68914},{\"end\":68930,\"start\":68923},{\"end\":69173,\"start\":69155},{\"end\":69405,\"start\":69399},{\"end\":69665,\"start\":69659},{\"end\":69679,\"start\":69672},{\"end\":69693,\"start\":69683},{\"end\":69706,\"start\":69697},{\"end\":69719,\"start\":69710},{\"end\":70052,\"start\":70043},{\"end\":70267,\"start\":70264},{\"end\":70623,\"start\":70621},{\"end\":70632,\"start\":70627},{\"end\":70639,\"start\":70636},{\"end\":70646,\"start\":70643},{\"end\":70939,\"start\":70937},{\"end\":70948,\"start\":70943},{\"end\":70955,\"start\":70952},{\"end\":70962,\"start\":70959},{\"end\":71202,\"start\":71198},{\"end\":71414,\"start\":71407},{\"end\":71423,\"start\":71418},{\"end\":71436,\"start\":71427},{\"end\":71447,\"start\":71442},{\"end\":71780,\"start\":71777},{\"end\":71789,\"start\":71784},{\"end\":71796,\"start\":71793},{\"end\":71805,\"start\":71800},{\"end\":71811,\"start\":71809},{\"end\":72094,\"start\":72083},{\"end\":72242,\"start\":72239},{\"end\":72248,\"start\":72246},{\"end\":72260,\"start\":72252},{\"end\":72267,\"start\":72264},{\"end\":72562,\"start\":72559},{\"end\":58068,\"start\":58062},{\"end\":58081,\"start\":58074},{\"end\":58291,\"start\":58284},{\"end\":58302,\"start\":58295},{\"end\":58312,\"start\":58306},{\"end\":58501,\"start\":58487},{\"end\":58715,\"start\":58705},{\"end\":58920,\"start\":58914},{\"end\":59150,\"start\":59142},{\"end\":59365,\"start\":59361},{\"end\":59594,\"start\":59587},{\"end\":59767,\"start\":59763},{\"end\":59777,\"start\":59773},{\"end\":60004,\"start\":60000},{\"end\":60204,\"start\":60195},{\"end\":60398,\"start\":60393},{\"end\":60629,\"start\":60623},{\"end\":60641,\"start\":60633},{\"end\":60650,\"start\":60645},{\"end\":60665,\"start\":60656},{\"end\":60982,\"start\":60978},{\"end\":61175,\"start\":61172},{\"end\":61351,\"start\":61347},{\"end\":61593,\"start\":61582},{\"end\":61604,\"start\":61597},{\"end\":61612,\"start\":61608},{\"end\":61949,\"start\":61940},{\"end\":62242,\"start\":62232},{\"end\":62254,\"start\":62246},{\"end\":62268,\"start\":62258},{\"end\":62278,\"start\":62272},{\"end\":62562,\"start\":62555},{\"end\":62811,\"start\":62803},{\"end\":63126,\"start\":63122},{\"end\":63134,\"start\":63130},{\"end\":63437,\"start\":63435},{\"end\":63631,\"start\":63626},{\"end\":63976,\"start\":63969},{\"end\":64252,\"start\":64244},{\"end\":64677,\"start\":64670},{\"end\":64693,\"start\":64681},{\"end\":64709,\"start\":64697},{\"end\":65026,\"start\":65023},{\"end\":65278,\"start\":65269},{\"end\":65290,\"start\":65285},{\"end\":65299,\"start\":65296},{\"end\":65312,\"start\":65303},{\"end\":65321,\"start\":65316},{\"end\":65673,\"start\":65663},{\"end\":65682,\"start\":65677},{\"end\":66155,\"start\":66145},{\"end\":66167,\"start\":66159},{\"end\":66183,\"start\":66175},{\"end\":66191,\"start\":66187},{\"end\":66204,\"start\":66195},{\"end\":66418,\"start\":66413},{\"end\":66428,\"start\":66422},{\"end\":66438,\"start\":66432},{\"end\":66534,\"start\":66532},{\"end\":66546,\"start\":66538},{\"end\":66556,\"start\":66550},{\"end\":66568,\"start\":66560},{\"end\":66580,\"start\":66576},{\"end\":66858,\"start\":66848},{\"end\":67123,\"start\":67117},{\"end\":67374,\"start\":67371},{\"end\":67385,\"start\":67378},{\"end\":67730,\"start\":67722},{\"end\":67984,\"start\":67981},{\"end\":67992,\"start\":67988},{\"end\":68001,\"start\":67996},{\"end\":68012,\"start\":68007},{\"end\":68294,\"start\":68289},{\"end\":68694,\"start\":68687},{\"end\":68919,\"start\":68914},{\"end\":68930,\"start\":68923},{\"end\":69173,\"start\":69155},{\"end\":69405,\"start\":69399},{\"end\":69665,\"start\":69659},{\"end\":69679,\"start\":69672},{\"end\":69693,\"start\":69683},{\"end\":69706,\"start\":69697},{\"end\":69719,\"start\":69710},{\"end\":70052,\"start\":70043},{\"end\":70267,\"start\":70264},{\"end\":70623,\"start\":70621},{\"end\":70632,\"start\":70627},{\"end\":70639,\"start\":70636},{\"end\":70646,\"start\":70643},{\"end\":70939,\"start\":70937},{\"end\":70948,\"start\":70943},{\"end\":70955,\"start\":70952},{\"end\":70962,\"start\":70959},{\"end\":71202,\"start\":71198},{\"end\":71414,\"start\":71407},{\"end\":71423,\"start\":71418},{\"end\":71436,\"start\":71427},{\"end\":71447,\"start\":71442},{\"end\":71780,\"start\":71777},{\"end\":71789,\"start\":71784},{\"end\":71796,\"start\":71793},{\"end\":71805,\"start\":71800},{\"end\":71811,\"start\":71809},{\"end\":72094,\"start\":72083},{\"end\":72242,\"start\":72239},{\"end\":72248,\"start\":72246},{\"end\":72260,\"start\":72252},{\"end\":72267,\"start\":72264},{\"end\":72562,\"start\":72559}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":24085261},\"end\":58239,\"start\":57983},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16640141},\"end\":58429,\"start\":58241},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":24673278},\"end\":58640,\"start\":58431},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":205420589},\"end\":58846,\"start\":58642},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49667613},\"end\":59055,\"start\":58848},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":23854526},\"end\":59300,\"start\":59057},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15326934},\"end\":59512,\"start\":59302},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3562008},\"end\":59726,\"start\":59514},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7602622},\"end\":59911,\"start\":59728},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6059802},\"end\":60148,\"start\":59913},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":17727860},\"end\":60336,\"start\":60150},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":205427260},\"end\":60520,\"start\":60338},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":139645007},\"end\":60906,\"start\":60522},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":56177573},\"end\":61114,\"start\":60908},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4441915},\"end\":61300,\"start\":61116},{\"attributes\":{\"doi\":\"10.1038/s41592-019-0403-1\",\"id\":\"b15\",\"matched_paper_id\":167211334},\"end\":61513,\"start\":61302},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3719281},\"end\":61830,\"start\":61515},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3640588},\"end\":62146,\"start\":61832},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5625548},\"end\":62461,\"start\":62148},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":90996315},\"end\":62720,\"start\":62463},{\"attributes\":{\"doi\":\"10.1101/580605\",\"id\":\"b20\"},\"end\":63003,\"start\":62722},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6538247},\"end\":63336,\"start\":63005},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2612052},\"end\":63620,\"start\":63338},{\"attributes\":{\"id\":\"b23\"},\"end\":63867,\"start\":63622},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":9747597},\"end\":64133,\"start\":63869},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14197771},\"end\":64585,\"start\":64135},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":52195631},\"end\":64890,\"start\":64587},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":46992249},\"end\":65265,\"start\":64892},{\"attributes\":{\"id\":\"b28\"},\"end\":65571,\"start\":65267},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3298854},\"end\":66091,\"start\":65573},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4246903},\"end\":66394,\"start\":66093},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1779661},\"end\":66528,\"start\":66396},{\"attributes\":{\"id\":\"b32\"},\"end\":66842,\"start\":66530},{\"attributes\":{\"id\":\"b33\"},\"end\":67023,\"start\":66844},{\"attributes\":{\"id\":\"b34\"},\"end\":67315,\"start\":67025},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":18195291},\"end\":67716,\"start\":67317},{\"attributes\":{\"id\":\"b36\"},\"end\":67893,\"start\":67718},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":195944196},\"end\":68285,\"start\":67895},{\"attributes\":{\"id\":\"b38\"},\"end\":68451,\"start\":68287},{\"attributes\":{\"id\":\"b39\"},\"end\":68617,\"start\":68453},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1806278},\"end\":68882,\"start\":68619},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14093430},\"end\":69067,\"start\":68884},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":39971417},\"end\":69336,\"start\":69069},{\"attributes\":{\"id\":\"b43\"},\"end\":69545,\"start\":69338},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":2334518},\"end\":69950,\"start\":69547},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":215779792},\"end\":70210,\"start\":69952},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":10716717},\"end\":70571,\"start\":70212},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":206594692},\"end\":70888,\"start\":70573},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":6447277},\"end\":71174,\"start\":70890},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":35602767},\"end\":71321,\"start\":71176},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":1023605},\"end\":71671,\"start\":71323},{\"attributes\":{\"id\":\"b51\"},\"end\":72028,\"start\":71673},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":2930547},\"end\":72235,\"start\":72030},{\"attributes\":{\"id\":\"b53\"},\"end\":72509,\"start\":72237},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":14113767},\"end\":72753,\"start\":72511},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":24085261},\"end\":58239,\"start\":57983},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16640141},\"end\":58429,\"start\":58241},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":24673278},\"end\":58640,\"start\":58431},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":205420589},\"end\":58846,\"start\":58642},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49667613},\"end\":59055,\"start\":58848},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":23854526},\"end\":59300,\"start\":59057},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15326934},\"end\":59512,\"start\":59302},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3562008},\"end\":59726,\"start\":59514},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7602622},\"end\":59911,\"start\":59728},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6059802},\"end\":60148,\"start\":59913},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":17727860},\"end\":60336,\"start\":60150},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":205427260},\"end\":60520,\"start\":60338},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206949135},\"end\":60906,\"start\":60522},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":56177573},\"end\":61114,\"start\":60908},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4441915},\"end\":61300,\"start\":61116},{\"attributes\":{\"doi\":\"10.1038/s41592-019-0403-1\",\"id\":\"b15\",\"matched_paper_id\":167211334},\"end\":61513,\"start\":61302},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3719281},\"end\":61830,\"start\":61515},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3640588},\"end\":62146,\"start\":61832},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5625548},\"end\":62461,\"start\":62148},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":90996315},\"end\":62720,\"start\":62463},{\"attributes\":{\"doi\":\"10.1101/580605\",\"id\":\"b20\"},\"end\":63003,\"start\":62722},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6538247},\"end\":63336,\"start\":63005},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2612052},\"end\":63620,\"start\":63338},{\"attributes\":{\"id\":\"b23\"},\"end\":63867,\"start\":63622},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":9747597},\"end\":64133,\"start\":63869},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14197771},\"end\":64585,\"start\":64135},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":52195631},\"end\":64890,\"start\":64587},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":46992249},\"end\":65265,\"start\":64892},{\"attributes\":{\"id\":\"b28\"},\"end\":65571,\"start\":65267},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3298854},\"end\":66091,\"start\":65573},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4246903},\"end\":66394,\"start\":66093},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1779661},\"end\":66528,\"start\":66396},{\"attributes\":{\"id\":\"b32\"},\"end\":66842,\"start\":66530},{\"attributes\":{\"id\":\"b33\"},\"end\":67023,\"start\":66844},{\"attributes\":{\"id\":\"b34\"},\"end\":67315,\"start\":67025},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":18195291},\"end\":67716,\"start\":67317},{\"attributes\":{\"id\":\"b36\"},\"end\":67893,\"start\":67718},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":195944196},\"end\":68285,\"start\":67895},{\"attributes\":{\"id\":\"b38\"},\"end\":68451,\"start\":68287},{\"attributes\":{\"id\":\"b39\"},\"end\":68617,\"start\":68453},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1806278},\"end\":68882,\"start\":68619},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14093430},\"end\":69067,\"start\":68884},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":39971417},\"end\":69336,\"start\":69069},{\"attributes\":{\"id\":\"b43\"},\"end\":69545,\"start\":69338},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":2334518},\"end\":69950,\"start\":69547},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":215779792},\"end\":70210,\"start\":69952},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":10716717},\"end\":70571,\"start\":70212},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":206594692},\"end\":70888,\"start\":70573},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":6447277},\"end\":71174,\"start\":70890},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":35602767},\"end\":71321,\"start\":71176},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":1023605},\"end\":71671,\"start\":71323},{\"attributes\":{\"id\":\"b51\"},\"end\":72028,\"start\":71673},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":2930547},\"end\":72235,\"start\":72030},{\"attributes\":{\"id\":\"b53\"},\"end\":72509,\"start\":72237},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":14113767},\"end\":72753,\"start\":72511}]", "bib_title": "[{\"end\":58058,\"start\":57983},{\"end\":58280,\"start\":58241},{\"end\":58483,\"start\":58431},{\"end\":58701,\"start\":58642},{\"end\":58910,\"start\":58848},{\"end\":59138,\"start\":59057},{\"end\":59357,\"start\":59302},{\"end\":59583,\"start\":59514},{\"end\":59757,\"start\":59728},{\"end\":59994,\"start\":59913},{\"end\":60191,\"start\":60150},{\"end\":60389,\"start\":60338},{\"end\":60619,\"start\":60522},{\"end\":60974,\"start\":60908},{\"end\":61168,\"start\":61116},{\"end\":61343,\"start\":61302},{\"end\":61578,\"start\":61515},{\"end\":61934,\"start\":61832},{\"end\":62226,\"start\":62148},{\"end\":62549,\"start\":62463},{\"end\":63118,\"start\":63005},{\"end\":63431,\"start\":63338},{\"end\":63965,\"start\":63869},{\"end\":64240,\"start\":64135},{\"end\":64664,\"start\":64587},{\"end\":65019,\"start\":64892},{\"end\":65659,\"start\":65573},{\"end\":66141,\"start\":66093},{\"end\":66409,\"start\":66396},{\"end\":67367,\"start\":67317},{\"end\":67974,\"start\":67895},{\"end\":68683,\"start\":68619},{\"end\":68910,\"start\":68884},{\"end\":69151,\"start\":69069},{\"end\":69655,\"start\":69547},{\"end\":70037,\"start\":69952},{\"end\":70257,\"start\":70212},{\"end\":70617,\"start\":70573},{\"end\":70933,\"start\":70890},{\"end\":71194,\"start\":71176},{\"end\":71403,\"start\":71323},{\"end\":71773,\"start\":71673},{\"end\":72079,\"start\":72030},{\"end\":72552,\"start\":72511},{\"end\":58058,\"start\":57983},{\"end\":58280,\"start\":58241},{\"end\":58483,\"start\":58431},{\"end\":58701,\"start\":58642},{\"end\":58910,\"start\":58848},{\"end\":59138,\"start\":59057},{\"end\":59357,\"start\":59302},{\"end\":59583,\"start\":59514},{\"end\":59757,\"start\":59728},{\"end\":59994,\"start\":59913},{\"end\":60191,\"start\":60150},{\"end\":60389,\"start\":60338},{\"end\":60619,\"start\":60522},{\"end\":60974,\"start\":60908},{\"end\":61168,\"start\":61116},{\"end\":61343,\"start\":61302},{\"end\":61578,\"start\":61515},{\"end\":61934,\"start\":61832},{\"end\":62226,\"start\":62148},{\"end\":62549,\"start\":62463},{\"end\":63118,\"start\":63005},{\"end\":63431,\"start\":63338},{\"end\":63965,\"start\":63869},{\"end\":64240,\"start\":64135},{\"end\":64664,\"start\":64587},{\"end\":65019,\"start\":64892},{\"end\":65659,\"start\":65573},{\"end\":66141,\"start\":66093},{\"end\":66409,\"start\":66396},{\"end\":67367,\"start\":67317},{\"end\":67974,\"start\":67895},{\"end\":68683,\"start\":68619},{\"end\":68910,\"start\":68884},{\"end\":69151,\"start\":69069},{\"end\":69655,\"start\":69547},{\"end\":70037,\"start\":69952},{\"end\":70257,\"start\":70212},{\"end\":70617,\"start\":70573},{\"end\":70933,\"start\":70890},{\"end\":71194,\"start\":71176},{\"end\":71403,\"start\":71323},{\"end\":71773,\"start\":71673},{\"end\":72079,\"start\":72030},{\"end\":72552,\"start\":72511}]", "bib_author": "[{\"end\":58070,\"start\":58060},{\"end\":58083,\"start\":58070},{\"end\":58293,\"start\":58282},{\"end\":58304,\"start\":58293},{\"end\":58314,\"start\":58304},{\"end\":58503,\"start\":58485},{\"end\":58717,\"start\":58703},{\"end\":58922,\"start\":58912},{\"end\":59152,\"start\":59140},{\"end\":59367,\"start\":59359},{\"end\":59596,\"start\":59585},{\"end\":59769,\"start\":59759},{\"end\":59779,\"start\":59769},{\"end\":60006,\"start\":59996},{\"end\":60206,\"start\":60193},{\"end\":60400,\"start\":60391},{\"end\":60631,\"start\":60621},{\"end\":60643,\"start\":60631},{\"end\":60652,\"start\":60643},{\"end\":60667,\"start\":60652},{\"end\":60984,\"start\":60976},{\"end\":61177,\"start\":61170},{\"end\":61353,\"start\":61345},{\"end\":61595,\"start\":61580},{\"end\":61606,\"start\":61595},{\"end\":61614,\"start\":61606},{\"end\":61951,\"start\":61936},{\"end\":62244,\"start\":62228},{\"end\":62256,\"start\":62244},{\"end\":62270,\"start\":62256},{\"end\":62280,\"start\":62270},{\"end\":62564,\"start\":62551},{\"end\":62813,\"start\":62801},{\"end\":63128,\"start\":63120},{\"end\":63136,\"start\":63128},{\"end\":63439,\"start\":63433},{\"end\":63633,\"start\":63624},{\"end\":63978,\"start\":63967},{\"end\":64254,\"start\":64242},{\"end\":64679,\"start\":64666},{\"end\":64695,\"start\":64679},{\"end\":64711,\"start\":64695},{\"end\":65028,\"start\":65021},{\"end\":65280,\"start\":65267},{\"end\":65292,\"start\":65280},{\"end\":65301,\"start\":65292},{\"end\":65314,\"start\":65301},{\"end\":65323,\"start\":65314},{\"end\":65675,\"start\":65661},{\"end\":65684,\"start\":65675},{\"end\":66157,\"start\":66143},{\"end\":66169,\"start\":66157},{\"end\":66185,\"start\":66169},{\"end\":66193,\"start\":66185},{\"end\":66206,\"start\":66193},{\"end\":66420,\"start\":66411},{\"end\":66430,\"start\":66420},{\"end\":66440,\"start\":66430},{\"end\":66536,\"start\":66530},{\"end\":66548,\"start\":66536},{\"end\":66558,\"start\":66548},{\"end\":66570,\"start\":66558},{\"end\":66582,\"start\":66570},{\"end\":66860,\"start\":66846},{\"end\":67125,\"start\":67115},{\"end\":67376,\"start\":67369},{\"end\":67387,\"start\":67376},{\"end\":67732,\"start\":67720},{\"end\":67986,\"start\":67976},{\"end\":67994,\"start\":67986},{\"end\":68003,\"start\":67994},{\"end\":68014,\"start\":68003},{\"end\":68296,\"start\":68289},{\"end\":68462,\"start\":68453},{\"end\":68696,\"start\":68685},{\"end\":68921,\"start\":68912},{\"end\":68932,\"start\":68921},{\"end\":69175,\"start\":69153},{\"end\":69407,\"start\":69397},{\"end\":69667,\"start\":69657},{\"end\":69681,\"start\":69667},{\"end\":69695,\"start\":69681},{\"end\":69708,\"start\":69695},{\"end\":69721,\"start\":69708},{\"end\":70054,\"start\":70039},{\"end\":70269,\"start\":70259},{\"end\":70625,\"start\":70619},{\"end\":70634,\"start\":70625},{\"end\":70641,\"start\":70634},{\"end\":70648,\"start\":70641},{\"end\":70941,\"start\":70935},{\"end\":70950,\"start\":70941},{\"end\":70957,\"start\":70950},{\"end\":70964,\"start\":70957},{\"end\":71204,\"start\":71196},{\"end\":71416,\"start\":71405},{\"end\":71425,\"start\":71416},{\"end\":71438,\"start\":71425},{\"end\":71449,\"start\":71438},{\"end\":71782,\"start\":71775},{\"end\":71791,\"start\":71782},{\"end\":71798,\"start\":71791},{\"end\":71807,\"start\":71798},{\"end\":71813,\"start\":71807},{\"end\":72096,\"start\":72081},{\"end\":72244,\"start\":72237},{\"end\":72250,\"start\":72244},{\"end\":72262,\"start\":72250},{\"end\":72269,\"start\":72262},{\"end\":72564,\"start\":72554},{\"end\":58070,\"start\":58060},{\"end\":58083,\"start\":58070},{\"end\":58293,\"start\":58282},{\"end\":58304,\"start\":58293},{\"end\":58314,\"start\":58304},{\"end\":58503,\"start\":58485},{\"end\":58717,\"start\":58703},{\"end\":58922,\"start\":58912},{\"end\":59152,\"start\":59140},{\"end\":59367,\"start\":59359},{\"end\":59596,\"start\":59585},{\"end\":59769,\"start\":59759},{\"end\":59779,\"start\":59769},{\"end\":60006,\"start\":59996},{\"end\":60206,\"start\":60193},{\"end\":60400,\"start\":60391},{\"end\":60631,\"start\":60621},{\"end\":60643,\"start\":60631},{\"end\":60652,\"start\":60643},{\"end\":60667,\"start\":60652},{\"end\":60984,\"start\":60976},{\"end\":61177,\"start\":61170},{\"end\":61353,\"start\":61345},{\"end\":61595,\"start\":61580},{\"end\":61606,\"start\":61595},{\"end\":61614,\"start\":61606},{\"end\":61951,\"start\":61936},{\"end\":62244,\"start\":62228},{\"end\":62256,\"start\":62244},{\"end\":62270,\"start\":62256},{\"end\":62280,\"start\":62270},{\"end\":62564,\"start\":62551},{\"end\":62813,\"start\":62801},{\"end\":63128,\"start\":63120},{\"end\":63136,\"start\":63128},{\"end\":63439,\"start\":63433},{\"end\":63633,\"start\":63624},{\"end\":63978,\"start\":63967},{\"end\":64254,\"start\":64242},{\"end\":64679,\"start\":64666},{\"end\":64695,\"start\":64679},{\"end\":64711,\"start\":64695},{\"end\":65028,\"start\":65021},{\"end\":65280,\"start\":65267},{\"end\":65292,\"start\":65280},{\"end\":65301,\"start\":65292},{\"end\":65314,\"start\":65301},{\"end\":65323,\"start\":65314},{\"end\":65675,\"start\":65661},{\"end\":65684,\"start\":65675},{\"end\":66157,\"start\":66143},{\"end\":66169,\"start\":66157},{\"end\":66185,\"start\":66169},{\"end\":66193,\"start\":66185},{\"end\":66206,\"start\":66193},{\"end\":66420,\"start\":66411},{\"end\":66430,\"start\":66420},{\"end\":66440,\"start\":66430},{\"end\":66536,\"start\":66530},{\"end\":66548,\"start\":66536},{\"end\":66558,\"start\":66548},{\"end\":66570,\"start\":66558},{\"end\":66582,\"start\":66570},{\"end\":66860,\"start\":66846},{\"end\":67125,\"start\":67115},{\"end\":67376,\"start\":67369},{\"end\":67387,\"start\":67376},{\"end\":67732,\"start\":67720},{\"end\":67986,\"start\":67976},{\"end\":67994,\"start\":67986},{\"end\":68003,\"start\":67994},{\"end\":68014,\"start\":68003},{\"end\":68296,\"start\":68289},{\"end\":68462,\"start\":68453},{\"end\":68696,\"start\":68685},{\"end\":68921,\"start\":68912},{\"end\":68932,\"start\":68921},{\"end\":69175,\"start\":69153},{\"end\":69407,\"start\":69397},{\"end\":69667,\"start\":69657},{\"end\":69681,\"start\":69667},{\"end\":69695,\"start\":69681},{\"end\":69708,\"start\":69695},{\"end\":69721,\"start\":69708},{\"end\":70054,\"start\":70039},{\"end\":70269,\"start\":70259},{\"end\":70625,\"start\":70619},{\"end\":70634,\"start\":70625},{\"end\":70641,\"start\":70634},{\"end\":70648,\"start\":70641},{\"end\":70941,\"start\":70935},{\"end\":70950,\"start\":70941},{\"end\":70957,\"start\":70950},{\"end\":70964,\"start\":70957},{\"end\":71204,\"start\":71196},{\"end\":71416,\"start\":71405},{\"end\":71425,\"start\":71416},{\"end\":71438,\"start\":71425},{\"end\":71449,\"start\":71438},{\"end\":71782,\"start\":71775},{\"end\":71791,\"start\":71782},{\"end\":71798,\"start\":71791},{\"end\":71807,\"start\":71798},{\"end\":71813,\"start\":71807},{\"end\":72096,\"start\":72081},{\"end\":72244,\"start\":72237},{\"end\":72250,\"start\":72244},{\"end\":72262,\"start\":72250},{\"end\":72269,\"start\":72262},{\"end\":72564,\"start\":72554}]", "bib_venue": "[{\"end\":65838,\"start\":65779},{\"end\":66696,\"start\":66643},{\"end\":67535,\"start\":67465},{\"end\":70407,\"start\":70342},{\"end\":71497,\"start\":71477},{\"end\":65838,\"start\":65779},{\"end\":66696,\"start\":66643},{\"end\":67535,\"start\":67465},{\"end\":70407,\"start\":70342},{\"end\":71497,\"start\":71477},{\"end\":58094,\"start\":58083},{\"end\":58318,\"start\":58314},{\"end\":58519,\"start\":58503},{\"end\":58729,\"start\":58717},{\"end\":58931,\"start\":58922},{\"end\":59162,\"start\":59152},{\"end\":59396,\"start\":59367},{\"end\":59604,\"start\":59596},{\"end\":59804,\"start\":59779},{\"end\":60014,\"start\":60006},{\"end\":60230,\"start\":60206},{\"end\":60412,\"start\":60400},{\"end\":60695,\"start\":60667},{\"end\":60996,\"start\":60984},{\"end\":61195,\"start\":61177},{\"end\":61390,\"start\":61378},{\"end\":61655,\"start\":61614},{\"end\":61968,\"start\":61951},{\"end\":62288,\"start\":62280},{\"end\":62575,\"start\":62564},{\"end\":62799,\"start\":62722},{\"end\":63157,\"start\":63136},{\"end\":63463,\"start\":63439},{\"end\":63699,\"start\":63633},{\"end\":63986,\"start\":63978},{\"end\":64272,\"start\":64254},{\"end\":64722,\"start\":64711},{\"end\":65060,\"start\":65028},{\"end\":65372,\"start\":65323},{\"end\":65749,\"start\":65684},{\"end\":66225,\"start\":66206},{\"end\":66446,\"start\":66440},{\"end\":66641,\"start\":66582},{\"end\":66888,\"start\":66860},{\"end\":67113,\"start\":67025},{\"end\":67463,\"start\":67387},{\"end\":67760,\"start\":67732},{\"end\":68069,\"start\":68014},{\"end\":68324,\"start\":68296},{\"end\":68495,\"start\":68462},{\"end\":68733,\"start\":68696},{\"end\":68963,\"start\":68932},{\"end\":69185,\"start\":69175},{\"end\":69395,\"start\":69338},{\"end\":69731,\"start\":69721},{\"end\":70065,\"start\":70054},{\"end\":70340,\"start\":70269},{\"end\":70713,\"start\":70648},{\"end\":70989,\"start\":70964},{\"end\":71231,\"start\":71204},{\"end\":71475,\"start\":71449},{\"end\":71832,\"start\":71813},{\"end\":72115,\"start\":72096},{\"end\":72318,\"start\":72269},{\"end\":72589,\"start\":72564},{\"end\":58094,\"start\":58083},{\"end\":58318,\"start\":58314},{\"end\":58519,\"start\":58503},{\"end\":58729,\"start\":58717},{\"end\":58931,\"start\":58922},{\"end\":59162,\"start\":59152},{\"end\":59396,\"start\":59367},{\"end\":59604,\"start\":59596},{\"end\":59804,\"start\":59779},{\"end\":60014,\"start\":60006},{\"end\":60230,\"start\":60206},{\"end\":60412,\"start\":60400},{\"end\":60695,\"start\":60667},{\"end\":60996,\"start\":60984},{\"end\":61195,\"start\":61177},{\"end\":61390,\"start\":61378},{\"end\":61655,\"start\":61614},{\"end\":61968,\"start\":61951},{\"end\":62288,\"start\":62280},{\"end\":62575,\"start\":62564},{\"end\":62799,\"start\":62722},{\"end\":63157,\"start\":63136},{\"end\":63463,\"start\":63439},{\"end\":63699,\"start\":63633},{\"end\":63986,\"start\":63978},{\"end\":64272,\"start\":64254},{\"end\":64722,\"start\":64711},{\"end\":65060,\"start\":65028},{\"end\":65372,\"start\":65323},{\"end\":65749,\"start\":65684},{\"end\":66225,\"start\":66206},{\"end\":66446,\"start\":66440},{\"end\":66641,\"start\":66582},{\"end\":66888,\"start\":66860},{\"end\":67113,\"start\":67025},{\"end\":67463,\"start\":67387},{\"end\":67760,\"start\":67732},{\"end\":68069,\"start\":68014},{\"end\":68324,\"start\":68296},{\"end\":68495,\"start\":68462},{\"end\":68733,\"start\":68696},{\"end\":68963,\"start\":68932},{\"end\":69185,\"start\":69175},{\"end\":69395,\"start\":69338},{\"end\":69731,\"start\":69721},{\"end\":70065,\"start\":70054},{\"end\":70340,\"start\":70269},{\"end\":70713,\"start\":70648},{\"end\":70989,\"start\":70964},{\"end\":71231,\"start\":71204},{\"end\":71475,\"start\":71449},{\"end\":71832,\"start\":71813},{\"end\":72115,\"start\":72096},{\"end\":72318,\"start\":72269},{\"end\":72589,\"start\":72564}]"}}}, "year": 2023, "month": 12, "day": 17}