{"id": 59523708, "updated": "2023-10-02 09:15:08.328", "metadata": {"title": "Noise2Self: Blind Denoising by Self-Supervision", "authors": "[{\"first\":\"Joshua\",\"last\":\"Batson\",\"middle\":[]},{\"first\":\"Loic\",\"last\":\"Royer\",\"middle\":[]}]", "venue": "ICML", "journal": "524-533", "publication_date": {"year": 2019, "month": 1, "day": 30}, "abstract": "We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement. Moreover, our framework is not restricted to a particular denoising model. We show how it can be used to calibrate any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. Finally, we prove a theoretical lower bound on the performance of an optimal denoiser. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1901.11365", "mag": "2949725501", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/BatsonR19", "doi": null}}, "content": {"source": {"pdf_hash": "dfed9adde0b2162f3554eb1f41be065aa0653e3d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1901.11365v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "51f1ae5f23dffb71f6ece81b2ca6689028729376", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/dfed9adde0b2162f3554eb1f41be065aa0653e3d.txt", "contents": "\nNoise2Self: Blind Denoising by Self-Supervision\n\n\nJoshua Batson \nLoic Royer \nNoise2Self: Blind Denoising by Self-Supervision\n\nWe propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement. Moreover, our framework is not restricted to a particular denoising model. We show how it can be used to calibrate any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. Finally, we prove a theoretical lower bound on the performance of an optimal denoiser. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.\n\nIntroduction\n\nWe would often like to reconstruct a signal from highdimensional measurements that are corrupted, undersampled, or otherwise noisy. Devices like high-resolution cameras, electron microscopes, and DNA sequencers are capable of producing measurements in the thousands to millions of feature dimensions. But when these devices are pushed to their limits, taking videos with ultra-fast frame rates at very low-illumination, probing individual molecules with electron microscopes, or sequencing tens of thousands of cells simultaneously, each individual feature can become quite noisy. Nevertheless, the objects being studied are often very structured and the values of different features are highly correlated. Speaking loosely, if the \"latent dimension\" of the space of objects under study is much lower than * Equal contribution 1 Chan-Zuckerberg Biohub. Correspondence to: Joshua Batson <joshua.batson@czbiohub.org>, Loic Royer <loic.royer@czbiohub.org>. the dimension of the measurement, it may be possible to implicitly learn that structure, denoise the measurements, and recover the signal without any prior knowledge of the signal or the noise.\n\nTraditional denoising methods each exploit a property of the noise, such as Gaussianity, or structure in the signal, such as spatiotemporal smoothness, self-similarity, or having low-rank. The performance of these methods is limited by the accuracy of their assumptions. For example, if the data are genuinely not low rank, then a low rank model will fit it poorly. This requires prior knowledge of the signal structure, which limits application to new domains and modalities. These methods also require calibration, as hyperparameters such as the degree of smoothness, the scale of self-similarity, or the rank of a matrix have dramatic impacts on performance.\n\nIn contrast, a data-driven prior, such as pairs (x i , y i ) of noisy and clean measurements of the same target, can be used to set up a supervised learning problem. A neural net trained to predict y i from x i may be used to denoise new noisy measurements (Weigert et al., 2018). As long as the new data are drawn from the same distribution, one can expect performance similar to that observed during training. Lehtinen et al. demonstrated that clean targets are unnecessary (2018). A neural net trained on pairs (x i , x i ) of independent noisy measurements of the same target will, under certain distributional assumptions, learn to predict the clean signal. These supervised approaches extend to image denoising the success of convolutional neural nets, which currently give state-of-the-art performance for a vast range of image-to-image tasks. Both of these methods require an experimental setup in which each target may be measured multiple times, which can be difficult in practice.\n\nIn this paper, we propose a framework for blind denoising based on self-supervision. We use groups of features whose noise is independent conditional on the true signal to predict one another. This allows us to learn denoising functions from single noisy measurements of each target, with performance close to that of supervised methods. The same approach can also be used to calibrate traditional image denoising methods such as median filters and non-local means, and, using a different independence structure, denoise highly under-sampled single-cell gene expression data. d Figure 1. (a) The box represents the dimensions of the measurement x. J is a subset of the dimensions, and f is a J-invariant function: it has the property that the value of f (x) restricted to dimensions in J, f (x)J , does not depend on the value of x restricted to J, xJ . This enables self-supervision when the noise in the data is conditionally independent between sets of dimensions. Here are 3 examples of dimension partitioning: (b) two independent image acquisitions, (c) independent pixels of a single image, (d) independently detected RNA molecules from a single cell.\n\nWe model the signal y and the noisy measurement x as a pair of random variables in R m . If J \u2282 {1, . . . , m} is a subset of the dimensions, we write x J for x restricted to J.\n\nDefinition. Let J be a partition of the dimensions {1, . . . , m} and let J \u2208 J . A function f :\nR m \u2192 R m is J-invariant if f (x) J does not depend on the value of x J . It is J -invariant if it is J-invariant for each J \u2208 J .\nWe propose minimizing the loss\nL(f ) = E f (x) \u2212 x 2 ,(1)\nover J -invariant functions f . Since f has to use information from outside of each subset of dimensions J to predict the values inside of J, it cannot merely be the identity.\n\nSuppose x is an unbiased estimator of y, that is, E[x|y] = y, and the noise in each subset J \u2208 J is independent from the noise in its complement J c , conditional on y. Then we can decompose the loss as\nL(f ) = E f (x) \u2212 y 2 + E x \u2212 y 2 ,(2)\nthe sum of the ordinary supervised loss and the variance of the noise. By minimizing the self-supervised loss L(f ), which we can compute from noisy data alone, we are able to find the actual optimal denoiser among any class of Jinvariant functions.\n\nFor example, if the signal is an image with independent, mean-zero noise on each pixel, we may choose J = {{1}, . . . , {m}} to be the singletons of each coordinate. In \u00a73, we show how to adapt any traditional image denoising function into a J -invariant one, and then calibrate it using this loss. For example, \"donut\" median filters, with a hole in the center, form a class of J -invariant functions. By comparing the value of the loss for different filter radii, we are able to select the optimal radius for denoising the image at hand.\n\nThe donut median filter has just one parameter and therefore limited ability to adapt to the data. At the other extreme, we may search over all J -invariant functions for the global optimum:\nProposition 1. The J -invariant function f * J minimizing (1) satisfies f * J (x) J = E[y J |x J c ] for each subset J \u2208 J .\nThat is, the optimal J -invariant predictor for the dimensions of y in some J \u2208 J is their expected value conditional on observing the dimensions of x outside of J.\n\nIn \u00a74, we use analytical examples to illustrate how the optimal J -invariant denoising function approaches the optimal general denoising function as the amount of correlation between features in the data increases.\n\nIn practice, we may attempt to approximate the optimal denoiser by searching over a very large class of functions, such as deep neural networks with millions of parameters. In \u00a75, we show that a deep convolutional network, modified to become J -invariant using a masking procedure, can achieve state-of-the-art denoising performance on three diverse datasets.\n\nSample code is available on GitHub 1 and deferred proofs are contained in the Supplement.\n\n\nRelated Work\n\nEach approach to blind denoising relies on assumptions about the structure of the signal and/or the noise. We review the major categories of assumption below, and the traditional and modern methods that utilize them. Most of the methods below are described in terms of application to image denoising, which has the richest literature, but some have natural extensions to other spatiotemporal signals and to generic measurements of vectors.\n\nSmoothness: Natural images and other spatiotemporal signals are often assumed to vary smoothly (Buades et al., 2005b). Local averaging, using a Gaussian, median, or some other filter, is a simple way to smooth out a noisy input. The degree of smoothing to use, e.g., the width of a filter, is a hyperparameter often tuned by visual inspection.\n\nSelf-Similarity: Natural images are often self-similar, in that each patch in an image is similar to many other patches from the same image. The classic non-local means algorithm replaces the center pixel of each patch with a weighted average of central pixels from similar patches (Buades et al., 2005a). The more robust BM3D algorithm makes stacks of similar patches, and performs thresholding in frequency space (Dabov et al., 2007). The hyperparameters of these methods have a large effect on performance; the 12 hyperparameters of BM3D were set in the reference implementation by evaluating performance on a few images with simulated Gaussian noise (Lebrun, 2012). On a new dataset with an unknown noise distribution it is computationally expensive to evaluate different hyperparameter choices and difficult to evaluate their effects in a principled way.\n\nConvolutional neural nets can produce images with another form of self-similarity, as linear combinations of the same small filters are used to produce each output. The \"deep image prior\" of (Ulyanov et al., 2017) exploits this by training a generative CNN to produce a single output image and stopping training before the loss goes to zero; the idea is that features shared in many places across the image are likely to be the signal, and will be learned before the net fits the noise.\n\nGenerative: Given a differentiable, generative model of the data, e.g. a neural net G trained using a generative adversarial loss, data can be denoised through projection onto the range of the net (Tripathi et al., 2018).\n\nGaussianity: Recent work (Zhussip et al., 2018;Metzler et al., 2018) uses a loss based on Stein's unbiased risk estimator to train denoising neural nets in the special case that noise is i.i.d. Gaussian.\n\nSparsity: Natural images are often close to sparse in e.g. a wavelet or DCT basis (Chang et al., 2000). Compression algorithms such as JPEG exploit this feature by thresholding small transform coefficients (Pennebaker & Mitchell, 1992). This is also a denoising strategy, but artifacts familiar from poor compression (like the ringing around sharp edges) may occur. Hyperparameters include the choice of basis and the degree of thresholding. Other methods learn an overcomplete dictionary from the data (Elad & Aharon, 2006;Papyan et al., 2017), and seek sparsity in that basis.\n\nCompressibility: A generic approach to denoising is to lossily compress and then decompress the data. The accuracy of this approach depends on the applicability of the compression scheme used to the signal at hand and its robustness to the form of noise. It also depends on choosing the degree of compression correctly: too much will lose important features of the signal, too little will preserve all of the noise. For the sparsity methods, this \"knob\" is the degree of sparsity, while for low-rank matrix factorizations, it is the rank of the matrix.\n\nAutoencoder architectures for neural nets provide a general framework for learnable compression. Each sample is mapped to a low-dimensional representation-the value of the neural net at the bottleneck layer-then back to the original space (Gallinari et al., 1987;Vincent et al., 2010). An autoencoder trained on noisy data may produce cleaner data as its output. The degree of compression is determined by the width of the bottleneck layer.\n\nUNet architectures, in which skip connections are added to a typical autoencoder architecture, can capture high-level spatially coarse representations and also reproduce fine detail; they can, in particular, learn the identity function . Trained directly on noisy data, they will do no denoising. Trained with clean targets, they can learn very accurate denoising functions (Weigert et al., 2018).\n\nStatistical Independence: Lehtinen et al. observed that a UNet trained to predict one noisy measurement of a signal from an independent noisy measurement of the same signal will in fact learn to predict the true signal (Lehtinen et al., 2018). We may reformulate the Noise2Noise procedure in terms of J -invariant functions: if x 1 = y + n 1 and x 2 = y + n 2 are the two measurements, we consider the composite measurement x = (x 1 , x 2 ) of a composite signal (y, y) in R 2m and set\nJ = {J 1 , J 2 } = {{1, . . . , m}, {m + 1, . . . , 2m}}. Then f * J (x) J2 = E[y|x 1\n]. An extension to video, in which one frame is used to compute the pullback under optical flow of another, was explored in (Ehret et al., 2018).\n\nIn concurrent work, Krull et al. describe an approach for learning to denoise from single noisy images with independent noise on each pixel (2018). They train a UNet to predict a collection of held-out pixels of an image from a version of that image with those pixels replaced. A key difference between their approach and our neural net examples in \u00a75 is in that their replacement strategy is not quite Jinvariant. (With some probability a given pixel is replaced by itself.) While their method lacks a theoretical guarantee against fitting the noise, it performs well in practice: On synthetically noised natural and microscopy images, they show performance similar to supervised methods, and on real cryo-TEM and fluorescence microscopy data, they show increased contrast and reduced speckle noise.\n\nFinally, we note that the \"fully emphasized denoising autoencoders\" in (Vincent et al., 2010) used the MSE between an autoencoder evaluated on masked input data and the true  Figure 2. Calibrating a median filter without ground truth. Different median filters may be obtained by varying the filter's radius. Which is optimal for a given image? The optimal parameter for J -invariant functions such as the donut median can be read off (red arrows) from the self-supervised loss.\n\nvalue of the masked pixels, but with the goal of learning robust representations, not denoising.\n\n\nCalibrating Traditional Models\n\nMany denoising models have hyperparameters controlling the extent or scale of the denoising-the size of a filter, the threshold for sparsity, the number of principal components, the width of a bottleneck. If ground truth data were available, the optimal parameter \u03b8 for a family f \u03b8 could be chosen by\nminimizing f \u03b8 (x) \u2212 y 2 .\nIn this section, we show how to make any traditional denoising function f \u03b8 into a J -invariant model g \u03b8 , whose optimal parameters can be found by minimizing the loss\nL(g \u03b8 ) = g \u03b8 (x) \u2212 x 2 .\nWe illustrate this process in Figure 2 for the median filter f r , which replaces each pixel with the median over a disk of radius r surrounding it. We consider an image with i.i.d. Gaussian noise, and let J = {{1}, . . . , {m}} be the partition into single pixels. The J -invariant function is the \"donut\" median g r , which replaces each pixel with the median over the same disk excluding the center. For the donut median, the minimum of the function g r (x) \u2212 x 2 (solid blue) sits directly above the minimum of the true reconstruction error g r (x) \u2212 y 2 (dashed blue), and selects the optimal radius r = 3. In contrast, the function f r (x) \u2212 x 2 (solid orange) is strictly increasing and tells us nothing about the true reconstruction error f r (x) \u2212 y 2 (dashed orange). Note that the median and donut median are genuinely different functions with slightly different performance, but while the former can only be tuned by inspecting the output images, the latter can be tuned using a principled loss.\n\nMore generally, let f \u03b8 be a classical denoiser, and consider some partition J of the pixels. Let s(x) be the function replacing each pixel with the average of its neighbors. Then the function g \u03b8 defined by\ng \u03b8 (x) J := f \u03b8 (1 J \u00b7 s(x) + 1 J c \u00b7 x) J ,(3)\nfor each J \u2208 J , is J -invariant.\n\nIn Supp. Figure 1, we show the corresponding loss curves for a wavelet filter, where we tune the threshold \u03c3, and NL-means, where we tune a cut-off distance h (Buades et al., 2005a;Chang et al., 2000). The partition J used is a 4x4 grid. Note that in all these examples, the function g \u03b8 is genuinely different than f \u03b8 , and, because the simple interpolation procedure may itself be helpful, it sometimes performs better.\n\nIn Table 1, we use the same loss to compare all three classes of denoising function. It selects the NL-means reconstruction as the best. Remarkably, this tuning process can be done on individual noisy images.\n\n\nSingle-Cell\n\nIn single-cell transcriptomic experiments, thousands of individual cells are isolated, lysed, and their mRNA are extracted, barcoded, and sequenced. Each mRNA molecule is mapped to a gene, and that \u223c20,000-dimensional vector of counts is an approximation to the gene expression of that cell. In modern, highly parallel experiments, only a few thousand of the hundreds of thousands of mRNA molecules present in a cell are successfully captured and sequenced (Milo et al., 2010). Thus the expression vectors are very undersampled, and genes expressed at low levels will appear as zeros. This makes simple relationships among genes, such as co-expression or transitions during development, difficult to see.\n\nIf we think of the measurement as a set of molecules captured from a given cell, then we may partition the molecules at random into two sets J 1 and J 2 . Summing (and normalizing) the gene counts in each set produces expression vectors x J1 and x J2 which are independent conditional on the true mRNA content y. We may now attempt to denoise x by training a model to predict x J2 from x J1 and vice versa.\n\nWe demonstrate this on a dataset of 2730 bone marrow cells from Paul et al. using principal component regression , where we use the self-supervised loss to find an optimal number of principal components. The data contain a population of stem cells which differentiate either into erythroid or myeloid lineages. The expression of genes preferentially expressed in each of these cell types is shown in Figure 3 for both the (normalized) noisy data and data denoised with too many, too few, and an optimal number of principal components. In the raw data, it is difficult to discern any population structure. When the data is under-corrected, the stem cell marker Ifitm1 is still not visible. When it is over-corrected, the stem population appears to express substantial amounts of Klf1 and Mpo. In the optimally corrected version, Ifitm1 expression coincides with low expression of the other markers, identifying the stem population, and its transition to the two more mature states is easy to see.\n\n\nPCA\n\nCross-validation for choosing the rank of a PCA requires some care, since adding more principal components will always produce a better fit, even on held-out samples (Bro et al., 2008). Owen and Perry recommend splitting the feature dimensions into two sets J 1 and J 2 as well as splitting the samples into train and validation sets (Owen & Perry, 2009). For a given k, they fit a rank k princi-pal component regression f k : X train,J1 \u2192 X train,J2 and evaluate its predictions on the validation set, computing f k (X valid,J1 ) \u2212 X valid,J2 2 . They repeat this, permuting train and validation sets and J 1 and J 2 . Simulations show that if X is actually a sum of a low-rank matrix plus Gaussian noise, then the k minimizing the total validation loss is often the optimal choice (Owen & Perry, 2009;Owen et al., 2016). This calculation corresponds to using the self-supervised loss to train and cross-validate a {J 1 , J 2 }invariant principal component regression.\n\n\nTheory\n\nIn an ideal situation for signal reconstruction, we have a prior p(y) for the signal and a probabilistic model of the noisy measurement process p(x|y). After observing some measurement x, the posterior distribution for y is given by Bayes' rule:\np(y|x) = p(x|y)p(y) p(x|y)p(y)dy .\nIn practice, one seeks some function f (x) approximating a relevant statistic of y|x, such as its mean or median. The mean is provided by the function minimizing the loss:\nE x f (x) \u2212 y 2 (\nThe L 1 norm would produce the median) .\n\nFix a partition J of the dimensions {1, . . . , n} of x and suppose that for each J \u2208 J , we have\np(x|y) = p(x J |y)p(x J c |y),\ni.e., x J and x J c are independent conditional on y. We consider the loss\nE x f (x) \u2212 x 2 = E x,y f (x) \u2212 y 2 + x \u2212 y 2 \u2212 2 f (x) \u2212 y, x \u2212 y\nIf f is J -invariant, then f (x) j and x j are independent random variables for all j, and the third term reduces to Then\nE y E x|y [f (x) \u2212 y], E x|y [x \u2212 y] ,L(f ) = J\u2208J E f J (x J c ) \u2212 x J 2 .\nThis is minimized at\nf * J (x J c ) = E[x J |x J c ] = E[y J |x J c ]\n. We bundle these functions into f * J . . The optimal J -invariant predictor converges to the optimal predictor. Example images for Gaussian processes of different length scales. We show how the gap in image quality between the two predictors tends to zero as the length scale increases.\n\n\nHow good is the optimum?\n\nHow much information do we lose by giving up x J when trying to predict y J ? Roughly speaking, the more the features in J are correlated with those outside of it, the closer f * J (x) will be to E[y|x] and the better both will estimate y. Figure 4 illustrates this phenomenon for the example of Gaussian Processes, a computationally tractable model of signals with correlated features. We consider a process on a 33 \u00d7 33 toroidal grid. The value of y at each node is standard normal and the correlation between the values at p and q depends on the distance between them: K p,q = exp(\u2212 p \u2212 q 2 /2 2 ), where is the length scale. The noisy measurement x = y + n, where n is white Gaussian noise with standard deviation 0.5.\n\n\nWhile\n\nE\ny \u2212 f * J (x) 2 \u2265 E y \u2212 E[y|x] 2\nfor all , the gap decreases quickly as the length scale increases.\n\nThe Gaussian process is more than a convenient example; it actually represents a worst case for the recovery error as a function of correlation.  Figure 5. For any dataset, the error of the optimal predictor (blue) is lower than that for a Gaussian Process (red) with the same covariance matrix. We show this for a dataset of noisy digits: the quality of the denoising is visibly better for the Alphabet than the Gaussian Process (samples at \u03c3 = 0.8).\n\nProof. See Supplement.\n\nGaussian processes represent a kind of local texture with no higher structure, and the functions f * J turn out to be linear .\n\nAt the other extreme is data drawn from finite collection of templates, like symbols in an alphabet. If the alphabet consists of {a 1 , . . . , a r } \u2208 R m and the noise is i.i.d. meanzero Gaussian with variance \u03c3 2 , then the optimal J-invariant prediction independent is a weighted sum of the letters from the alphabet. The weights\nw i = exp(\u2212 (a i \u2212 x) \u00b7 1 J c 2 /2\u03c3 2 )\ncorrespond to the posterior probabilities of each letter. When the noise is low, the output will concentrate on a copy of the closest letter; when the noise is high, the output will be the average of many letters.\n\nIn Figure 5, we demonstrate this phenomenon for an alphabet consisting of 30 16x16 handwritten digits drawn from MNIST (LeCun et al., 1998). Note that almost exact recovery is possible at much higher levels of noise than the Gaussian process with covariance matrix given by the empirical covariance matrix of the alphabet.\n\nAny real-world datasets will exhibit more structure than a Gaussian process, so nonlinear functions can generate significantly better predictions.\n\n\nDoing better\n\nIf f is J -invariant, then by definition f (x) j contains no information from x j , and the right linear combination \u03bbf (x) j + (1 \u2212 \u03bb)x j will produce an estimate of y j with lower variance than either. The optimal value of \u03bb can be computed from the variance of the noise, if known. The performance gain depends on the quality of f : for example, if f improves the PSNR by 10 dB, then mixing in the optimal amount of x will yield another 0.4 dB. (See Table 1 for an example and Supplement for proofs.)\n\n\nDeep Learning Denoisers\n\nThe self-supervised loss can also be used to train deep convolutional neural nets from single samples of noisy images. We show this on three datasets from different domains (see Figure 6) with strong synthetic noise applied independently to each pixel. We use a UNet architecture and a random partition of 25 subsets for J . We make the neural net Jinvariant as in Eq. 3, except we replace the masked pixels with random values instead of local averages. Table 2, we achieve superior performance to the classic unsupervised denoisers NLM and BM3D, and comparable performance to the same architecture trained with clean targets (Noise2Truth) and with independently noisy targets (Noise2Noise), and to a purely convolutional architecture with clean targets (DnCNN) (Zhang et al., 2017).\n\n\nAs shown in\n\nThe result of training is a neural net f \u03b8 , which, when converted into a J -invariant function g \u03b8 , has low selfsupervised loss. We found that applying f \u03b8 directly to the noisy input gave slightly better (0.5 dB) performance than using g \u03b8 . The images in Figure 6 correspond to applying f \u03b8 .\n\nExperiments were performed on a single NVIDIA Titan XP GPU.\n\n\nDiscussion\n\nWe have demonstrated a general framework for denoising high-dimensional measurements whose noise exhibits some conditional independence structure. We have shown how to use a self-supervised loss to calibrate or train any Jinvariant class of denoising functions.  Figure 6. Benchmarking of classic, supervised, and self-supervised denoising methods on natural images, Chinese characters, and fluorescence microscopy images. Baseline denoisers are: NLM (Buades et al., 2005a), BM3D (Dabov et al., 2007), and a supervised CNN-based denoiser, DnCNN (Zhang et al., 2017). We compare denoising performance of a UNet trained with three losses: supervised with truth (N2T), supervised with another noisy image (N2N), and self-supervised (N2S). There remain many open questions about the optimal choice of partition J for a given problem. The structure of J must reflect the patterns of dependence in the signal and independence in the noise. The relative sizes of each subset J \u2208 J and its complement creates a bias-variance tradeoff in the loss, exchanging information used to make a prediction for information about the quality of that prediction.\n\nFor example, the measurements of single-cell gene expression could be partitioned by molecule, gene, or even pathway, reflecting different assumptions about the kind of stochasticity occurring in transcription.\n\nWe hope this framework will find application to other domains, such as sensor networks in agriculture or geology, time series of whole brain neuronal activity, or telescope observations of distant celestial bodies. 001  002  003  004  005  006  007  008  009  010  011  012  013  014  015  016  017  018  019  020  021  022  023  024  025  026  027  028  029  030  031  032  033  034  035  036  037  038  039  040  041  042  043  044  045  046  047  048  049  050  051  052  053  054 Supplement to Noise2Self: Blind Denoising by Self-Supervision\n\n\nNotation\n\nFor a variables x \u2208 R m and J \u2282 {1, . . . , m}, we write x J for the restriction of x to the coordinates in J and x J c for the restriction of x to the coordinates in J c . If f : R m \u2192 R m is a function, we write f (x) J for the restriction of f (x) to the coordinates in J.\n\nA partition J of a set X is a set of disjoint subsets of X whose union is all of X.\n\nWhen J = {j} is a singleton, we write x \u2212j for x J c , the restriction of x to the coordinates not equal to j.\n\n\nGaussian Processes\n\nLet x and y be random variables. Then the estimator of y from x minimizing the expected mean-square error (MSE) is x \u2192 E[y|x]. The expected MSE of that estimator is simply the variance of y|x:\nE x y \u2212 E[y|x] 2 = E x Var(y|x)\n.\n\nIf x and y are jointly multivariate normal, then the righthand-side depends only on the covariance matrix \u03a3. If\n\u03a3 = \u03a3 xx \u03a3 yx \u03a3 xy \u03a3 yy ,\nthen then right-hand-side is in fact a constant independent of x:\nVar(y|x) = \u03a3 yy \u2212 \u03a3 yx \u03a3 \u22121 xx \u03a3 xy .\n(See Chapter 4 of .)\n\nLemma 1. Let \u03a3 be a symmetric, positive semi-definite matrix with block structure\n\u03a3 = \u03a3 11 \u03a3 12 \u03a3 21 \u03a3 22 .\nThen\n\u03a3 11 \u03a3 12 \u03a3 \u22121 22 \u03a3 21 .\nProof. Since \u03a3 is PSD, we may factorize it as a product X T X for some matrix X. (For example, take the spectral decomposition \u03a3 = V T \u039bV , with \u039b the diagonal matrix of eigenvalues, all of which are nonnegative since \u03a3 is PSD and V the matrix of eigenvectors.\nSet X = \u039b 1/2 V .) Write X = X 1 X 2 , so that \u03a3 ij = X T i X j .\nIf \u03c0 X2 is the projection operator onto the column-span of X 2 , then\nI \u03c0 X2 = X 2 (X T 2 X 2 ) \u22121 X T 2 .\nMultiplying on the left and right by X T 1 and X 1 yields\nX T 1 X 1 X T 1 X 2 (X T 2 X 2 ) \u22121 X T 2 X 1 \u03a3 11 \u03a3 12 \u03a3 \u22121 22 \u03a3 21 ,\nwhere the second line follows by grouping terms in the first.\n\nLemma 2. Let x, y be random variables and let x G and y G be Gaussian random variables with the same covariance matrix. Then\nE x y \u2212 E[y|x] 2 \u2264 E x G y G \u2212 E[y G |x G ] 2 .\nProof. These are in fact the expected variances of the conditional variables:\nE y \u2212 Ey|x 2 = E x E y|x y \u2212 Ey|x 2 = E x Var[y|x].\nUsing the formula above for the Gaussian process MSE, we now need to show that\nE x Var[y|x] \u2264 \u03a3 yy \u2212 \u03a3 yx \u03a3 \u22121 xx \u03a3 xy .\nBy the law of total variance,\nVar(y) = Var x (E[y|x]) + E x Var(y|x).\nSo it suffices to show that\n\n\nMasking\n\nIn this section, we discuss approaches to modifying the input to a neural net or other function f to create a J -invariant function.\n\nThe basic idea is to choose some interpolation function s(x) and then define g by\ng(x) J := f (1 J \u00b7 s(x) + 1 J c \u00b7 x) J ,\nwhere 1 J is the indicator function of the set J.\n\nIn Section 3 of the paper, on calibration, s is given by a local average, not containing the center. Explicitly, it is convolution with the kernel We also considered setting each entry of s(x) to a random variable uniform on [0, 1]. This produces a random Jinvariant function, ie, a distribution g(x) whose marginal g(x) J does not depend on x J .\n\n\nUniform Pixel Selection\n\nIn Krull et. al., the authors propose masking procedures that estimate a local distribution q(x) in the neighborhood of a pixel and then replace that pixel with a sample from the distribution. Because the value at that pixel is used to estimate the distribution, information about it leaks through and the resulting random functions are not genuinely Jinvariant.\n\nFor example, they propose a method called Uniform Pixel Selection (UPS) to train a neural net to predict x j from UPS j (x), where UPS j is the random function replacing the j th entry of x with the value of at a pixel k chosen uniformly at random from the r \u00d7 r neighborhood centered at j .\n\nWrite \u03b9 jk (x) is the vector x with the value x j replaced by x k .\n\nThe function f * minimizing the self-supervised loss This means that training using UPS masking can, given sufficient data and a sufficiently expressive network, produce a linear combination of the noisy input and the Noise2Self optimum. The smaller the region used for selecting the pixel, the larger the contribution of the noise will be. In practice, however, a convolutional neural net may not be able to learn to recognize when it was handed an interesting pixel x j and when it had been replaced (say by comparing the value at a pixel in UPS j (x) to each of its neighbors).\nE x f (UPS j (x)) j \u2212 x j 2 satisfies f * (x) j = E x [x j | UPS j (x)] = E x E k [x j |\u03b9 jk (x)] = 1 r 2 k E[x j |\u03b9 jk (x)] = 1 r 2 E[x j |\u03b9 jj (x)] + 1 r 2 k =j E[x j |\u03b9 jk (x)] = 1 r 2 x j + 1 r 2 k =j E[x j |x \u2212j ] = 1 r 2 x j + 1 \u2212 1 r 2 f * J (x) j , where f * J (x) j = E[x j |x \u2212j ] is\nOne attractive feature of UPS is that it keeps the same perpixel data distribution as the input. If, for example, the input is binary, then local averaging and random uniform replacements will both be substantial deviations. This may regularize the behavior of the network, making it more sensible to pass in an entire copy of x to the trained network later, rather than iteratively masking it.\n\nWe suggest a simple modification: exclude the value of x j when estimating the local distribution. For example, replace it with a random neighbor.\n\n\nLinear Combinations\n\nIn this section we note that if f is J -invariant, then f (x) j and x j give two uncorrelated estimators of y j for any coordinate j. Here we investigate the effect of taking a linear combination of them.\n\nGiven two uncorrelated and unbiased estimators u and v of some quantity y, we may form a linear combination:\nw \u03bb = \u03bbu + (1 \u2212 \u03bb)v.\nThe variance of this estimator is\n\u03bb 2 U + (1 \u2212 \u03bb) 2 V,\nwhere U and V are the variances of u and v respectively. This expression is minimized at\n\u03bb = V /(U + V ).\nThe variance of the mixed estimator w \u03bb is U V /(U + V ) = V 1 1+V /U . When the variance of v is much lower than that of u, we just get V out, but when they are the same the variance is exactly halved. Note that this is monotonic in V , so if estimators v 1 , . . . , v n are being compared, their rank will not change after the original signal is mixed in. In terms of PSNR, the new value is PSNR(w \u03bb , y) = 10 * log 10 1 + V /U V = PSNR(V ) + 10 * log 10 (1 + V /U ) \u2248 PSNR(V ) + 10 log 10 (e)\nV U \u2212 1 2 V 2 U 2 \u2248 PSNR(V ) + 4.3 \u00b7 V U\nIf we fix y, then x j and E[y j |x \u2212j ] are both independent estimators of y j , so the above reasoning applies. Note that the loss itself is the variance of x j |x \u2212j , whose two components are the variance of x j |y j and the variance of y j |x \u2212j .\n\nThe optimal value of \u03bb, then, is given by the variance of the noise divided by the value of the self-supervised loss.\n\nFor example the function f reduces the noise by a factor of 10 (ie, the variance of y j |x \u2212j is a tenth of the variance of x j |y j ), then \u03bb * = 1/11 and the linear combination has a PSNR 0.43 higher than that of f alone.\n\n\nCalibrating Traditional Denoising Methods\n\nThe image denoising methods were all demonstrated on the full camera image included in the scikit-image library for python (van der Walt et al., 2014). An inset from that image was displayed in the figures.\n\nWe also used the scikit-image implementations of the median filter, wavelet denoiser, and NL-means. The noise standard deviation was 0.1 on a [0, 1] scale.\n\nIn addition to the calibration plots for the median filter in the text, we show the same for the wavelet and NL-means denoisers in Supp. Figure 1. , and applied to each one substantial Gaussian (\u00b5 = 0, \u03c3 = 0.7) and Bernoulli (half pixels blacked out) noise. Each Chinese character appears 6 times in the whole dataset of 78174 images. We then split this dataset in a training and test set (90% versus 10%).\n\n\nNeural Net Examples\n\nCellNet We constructed a dataset of 34630 image tiles (128x128) obtained by random partitioning of a large collection of single channel fluorescence microscopy images of cultured cells. These images were downloaded from the Broad Bioimage Benchmark Collection (Ljosa et al., 2012). Before cropping, we first gently denoise the images using the non-local means algorithm. We do so in order to remove a very low and nearly imperceptible amount of noise already present in these images -indeed, the images have an excellent signal-to-noise ratio to start from. Next, we use a rich noise model to simulate typical noise on sCMOS scientific cameras. This noise model consists of: (i) spatially variant gain noise per pixel, (ii) Poisson noise, (iii) Cauchy distributed additive noise. We choose parameters so as to obtain a very aggressive noise regime.\n\nImageNet In order to generate a large collection of natural image tiles, we downloaded the ImageNet LSVRC 2013 Vali-167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217 218 219  dation Set consisting of 20121 RGB images -typically photographs. From these images we generated 60000 cropped images of dimension 128x128 with each RGB value within [0,255]. These images were mistreated by the strong combination of Poisson (\u03bb = 30), Gaussian (\u03c3 = 80), and Bernoulli noise (p = 0.2). In the case of Bernoulli noise, each pixel channel (R, G, or B) has probability p of being dark or hot, i.e. set to the value 0 or 255.\n\n\nArchitecture\n\nWe use a UNet architecture modelled after . The network has an hourglass shape with skip connections between layers of the same scale. Each convolutional block consists of two convolutional layers with 3x3 filters followed by an InstanceNorm. The number of channels is [32,64,128,256]. Downsampling uses strided  222  223  224  225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272  273  274 Supplement to Noise2Self: Blind Denoising by Self-Supervision convolutions and upsampling uses transposed convolutions. The network is implemented in PyTorch (Paszke et al., 2017) and the code is also included in the supplement.\n\n\nTraining\n\nWe convert a neural net f \u03b8 into a random J -invariant function:\nJ\u2208J 1 J \u00b7 f \u03b8 (1 J c \u00b7 x J + 1 J \u00b7 u)(1)\nwhere u is a vector of random numbers distributed uniformly on [0, 1]. To speed up training, we only compute the coordinates for one J per pass, and that J is chosen randomly for each batch with density 1/25. The loss is restricted to those coordinates.\n\nWe train with a batch size of 64 for H\u00e0nz\u00ec and CellNet and a batch size of 32 for ImageNet.\n\nWe train for 50 epochs for CellNet, 30 epochs for H\u00e0nz\u00ec and 1 epoch for ImageNet.\n\n\nInference\n\nWe considered two approaches for inference. In the first, we consider a partition J containing 25 sets and apply Equation (1) to produce a genuinely J -invariant function. This requires |J | applications of the network.\n\nIn the second, we just apply the trained network to the full noisy data. This will include the information from x j in the prediction f \u03b8 (x) j . While the information in this pixel was entirely redundant during training, some regularization induced by the convolutional structure of the net and the training procedure may have caused it to learn a function which uses that information in a sensible way. Indeed, on our three datasets, the direct application was about 0.5 dB better than the J -independent version.\n\n\nEvaluation\n\nWe evaluated each reconstruction method using the Peak Signal-to-Noise Ratio (PSNR). For two images with range [0, 1], this is a log-transformation of the mean-squared error:\n\nPSNR(x, y) = 10 * log 10 (1/ x \u2212 y 2 ).\n\nBecause of clipping, the noise on the image datasets is not conditionally mean-zero. (Any noise on a pixel with intensity 1, for example, must be negative.) This induces a bias: E[x|y] is shrunk slightly towards the mean intensity. For methods trained with clean targets, like Noise2Truth and DnCNN, this effect doesn't matter; the network can learn to produce the correct value. The outputs of the blind methods like Noise2Noise, Noise2Self, NL-means, and BM3D, will exhibit this shrinkage. To make up for this difference, we rescale the outputs of all methods to match the mean and variance of the ground truth.\n\nWe compute the PSNR for fully reconstructed images on hold-out test sets which were not part of the training or validation procedure.\n\n\nSingle-Cell Gene Expression\n\nThe lossy capture and sequencing process producing singlecell gene expression can be expressed as a Poisson distribution 1 . A given cell has a density \u03bb = (\u03bb 1 , . . . , \u03bb m ) over genes i \u2208 {1, . . . m}, with i \u03bb i = 1. If we sample N molecules, we get a multinomial distribution which can be approximated as x i \u223c Poisson(N \u03bb i ).\n\nWhile one would like to model molecular counts directly, the large dynamic range of gene expression (about 5 orders of magnitude) makes linear models difficult to fit directly. Instead, one typically introduces a normalized variable z, for example\nz i = \u03c1(N 0 * x i /N ),\nwhere N = i x i is the total number of molecules in a given cell, N 0 is a normalizing constant, and \u03c1 is some nonlinearity. Common values for \u03c1 include x \u2192 \u221a x and\n\nx \u2192 log(1 + x).\n\nOur analysis of the Paul et al. dataset  follows one from the tutorial for a diffusion-based denoiser called MAGIC, and we use the scprep package to perform normalization (Van Dijk et al., 2018). In the language above, N 0 is the median of the total molecule count per cell and \u03c1 is square root.\n\nBecause we work on the normalized variable z, the optimal denoiser would predict\nE[z i |\u03bb] \u2248 E xi\u223cPoisson N \u03bbi \u221a x i N 0 /N .\nThis function of \u03bb i is positive, monotonic and maps 0 to 0, so it is directionally informative. Since expectations do not commute with nonlinear functions, inverting it would not produce an unbiased estimate of \u03bb i . Nevertheless, it provides a quantitative estimate of gene expression which is well-adapted to the large dynamic range. 277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304  305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320  321  322  323  324  325  326  327  328  329 \n\nFigure 3 .\n3Self-supervised loss calibrates a linear denoiser for single cell data. (a) Raw expression of three genes: a myeloid cell marker (Mpo), an erythroid cell marker (Klf1), and a stem cell marker (Ifitm1). Each point corresponds to a cell. (e) Self-supervised loss for principal component regression. In (d) we show the the denoised data for the optimal number of principal components (17, red arrow). In (c) we show the result of using too few components and in (b) that of using too many. X-axes show square-root normalised counts.\n\n\nwhich vanishes when x is an unbiased estimator of y. (This is Proposition 1.) Any J -invariant function can be written as a collection of ordinary functions f J : R |J c | \u2192 R |J| , where we separate the output dimensions of f based on which input dimensions they depend on.\n\nFigure 4\n4Figure 4. The optimal J -invariant predictor converges to the optimal predictor. Example images for Gaussian processes of different length scales. We show how the gap in image quality between the two predictors tends to zero as the length scale increases.\n\n\nthe optimum of the selfsupervised loss among J -invariant functions.\n\nFigure 1 .\n1Calibrating a wavelet filter and Non-local means without ground truth. The optimal parameter for J -invariant (masked) versions can be read off (red arrows) from the self-supervised loss.\n\nTable 1 .\n1Comparison of optimally tuned J -invariant versions of classical denoising models. PSNR+ denotes the small performance boost from including an optimal amount of the noisy input ( \u00a74.2)METHOD \nLOSS \nPSNR PSNR+ \n\n(DONUT) MEDIAN 0.0107 \n27.5 \n28.2 \nWAVELET \n0.0113 \n26.0 \n26.9 \nNL-MEANS \n0.0098 \n30.4 \n30.8 \n\n\n\nTable 2 .\n2Comparison of different denoising models and training modalities. We compare the Peak Signal to Noise Ratio (PSNR) on held-out test data. Error bars from training five models.METHOD \nH\u00c0NZ\u00cc \nIMAGENET \nCELLNET \n\nRAW \n6.5 \n9.4 \n15.1 \nNLM \n8.4 \n15.7 \n29.0 \nBM3D \n9.3 \n15.5 \n31.4 \nDNCNN (N2T) 14.0 \u00b1 0.1 \n22.0 \n34.4 \u00b1 0.1 \nUNET (N2T) \n13.1 \u00b1 0.7 \n21.1 \n34.5 \u00b1 0.1 \nUNET (N2N) \n13.3 \u00b1 0.5 \n17.8 \n34.4 \u00b1 0.1 \nUNET (N2S) \n13.8 \u00b1 0.3 \n18.6 \n32.8 \u00b1 0.2 \n\n\n\n\nSupplement to Noise2Self: Blind Denoising by Self-Supervisionground truth \n\nself-supervised \nmasked \nclassic \n\nmasked \nclassic \n\nMean square error (MSE) \n\nSigma threshold \n\nground truth \n\nself-supervised \nmasked \nclassic \n\nmasked \nclassic \nMean square error (MSE) \n\nCut-off distance \n\nCalibrating Wavelet Denoiser \n\nCalibrating NL-means \n\n\nhttps://github.com/czbiohub/noise2self\nWhile the polymerase chain reaction (PCR) used to amplify the molecules for sequencing would introduce random multiplicative distortions, many modern datasets introduce unique molecular indentifiers (UMIs), barcodes attached to each molecule before amplification which can be used to deduplicate reads from the same original molecule.\nAcknowledgementsThank you to James Webber, Jeremy Freeman, David Dynerman, Nicholas Sofroniew, Jaakko Lehtinen, Jenny Folkesson, Anitha Krishnan, and Vedran Hadziosmanovic for valuable conversations. Thank you to Jack Kamm for discussions on Gaussian Processes and shrinkage estimators.\nCross-validation of component models: A critical look at current methods. R Bro, K Kjeldahl, A K Smilde, H A L Kiers, 10.1007/s00216-007-1790-1Analytical and Bioanalytical Chemistry. 3905Bro, R., Kjeldahl, K., Smilde, A. K., and Kiers, H. A. L. Cross-validation of component models: A critical look at current methods. Analytical and Bioanalytical Chem- istry, 390(5):1241-1251, March 2008. ISSN 1618- 2642, 1618-2650. doi: 10.1007/s00216-007-1790-1. URL http://link.springer.com/10.1007/ s00216-007-1790-1.\n\nA Non-Local Algorithm for Image Denoising. A Buades, B Coll, J.-M Morel, 10.1109/CVPR.2005.38IEEE2San Diego, CA, USABuades, A., Coll, B., and Morel, J.-M. A Non-Local Al- gorithm for Image Denoising. volume 2, pp. 60-65, San Diego, CA, USA, 2005a. IEEE. ISBN 978-0-7695- 2372-9. doi: 10.1109/CVPR.2005.38. URL http:// ieeexplore.ieee.org/document/1467423/.\n\nA Review of Image Denoising Algorithms, with a New One. A Buades, B Coll, J M Morel, 10.1137/040616024Multiscale Modeling & Simulation. 42Buades, A., Coll, B., and Morel, J. M. A Review of Im- age Denoising Algorithms, with a New One. Multiscale Modeling & Simulation, 4(2):490-530, January 2005b. ISSN 1540-3459, 1540-3467. doi: 10.1137/040616024. URL http://epubs.siam.org/doi/10.1137/ 040616024.\n\nAdaptive wavelet thresholding for image denoising and compression. S G Chang, B Yu, M Vetterli, IEEE transactions on image processing. 99Chang, S. G., Yu, B., and Vetterli, M. Adaptive wavelet thresholding for image denoising and compression. IEEE transactions on image processing, 9(9):1532-1546, 2000.\n\nImage Denoising by Sparse 3-D Transform-Domain Collaborative Filtering. K Dabov, A Foi, V Katkovnik, K Egiazarian, 10.1109/TIP.2007.901238IEEE Transactions on Image Processing. 168Dabov, K., Foi, A., Katkovnik, V., and Egiazarian, K. Image Denoising by Sparse 3-D Transform-Domain Collabora- tive Filtering. IEEE Transactions on Image Processing, 16(8):2080-2095, August 2007. ISSN 1057-7149. doi: 10.1109/TIP.2007.901238.\n\nModel-blind Video Denoising Via Frame-to-frame Training. T Ehret, A Davy, G Facciolo, J.-M Morel, P Arias, arXiv:1811.12766arXiv: 1811.12766Ehret, T., Davy, A., Facciolo, G., Morel, J.-M., and Arias, P. Model-blind Video Denoising Via Frame-to-frame Training. arXiv:1811.12766 [cs], November 2018. URL http://arxiv.org/abs/1811.12766. arXiv: 1811.12766.\n\nImage Denoising Via Sparse and Redundant Representations Over Learned Dictionaries. M Elad, M Aharon, 1057-7149. doi: 10. 1109/TIP.2006.881969IEEE Transactions on Image Processing. 1512Elad, M. and Aharon, M. Image Denoising Via Sparse and Redundant Representations Over Learned Dictionar- ies. IEEE Transactions on Image Processing, 15(12): 3736-3745, December 2006. ISSN 1057-7149. doi: 10. 1109/TIP.2006.881969. URL http://ieeexplore. ieee.org/document/4011956/.\n\nMemoires associatives distribuees: Une comparaison (Distributed associative memories: A comparison). P Gallinari, Y Lecun, S Thiria, F Soulie, Proceedings of COGNITIVA 87. COGNITIVA 87Paris, La VilletteGallinari, P., Lecun, Y., Thiria, S., and Soulie, F. Memoires associatives distribuees: Une comparaison (Distributed associative memories: A comparison). Proceedings of COGNITIVA 87, Paris, La Villette, May 1987, 1987.\n\nA Krull, T.-O Buchholz, F Jug, arXiv:1811.10980arXiv: 1811.10980Noise2void -Learning Denoising from Single Noisy Images. Krull, A., Buchholz, T.-O., and Jug, F. Noise2void -Learning Denoising from Single Noisy Images. arXiv:1811.10980 [cs], November 2018. URL http://arxiv.org/abs/1811.10980. arXiv: 1811.10980.\n\nAn Analysis and Implementation of the BM3d Image Denoising Method. M Lebrun, 10.5201/ipol.2012.l-bm3dImage Processing On Line. 2Lebrun, M. An Analysis and Implementation of the BM3d Image Denoising Method. Image Processing On Line, 2:175-213, August 2012. ISSN 2105-1232. doi: 10.5201/ipol.2012.l-bm3d. URL http://www.ipol. im/pub/art/2012/l-bm3d/.\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998.\n\nLearning Image Restoration without Clean Data. J Lehtinen, J Munkberg, J Hasselgren, S Laine, T Karras, M Aittala, Aila , T Noise2noise, 10Lehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras, T., Aittala, M., and Aila, T. Noise2noise: Learning Image Restoration without Clean Data. pp. 10, 2018.\n\nUnsupervised Learning with Stein's Unbiased Risk Estimator. C A Metzler, A Mousavi, R Heckel, R G Baraniuk, arXiv:1805.10531arXiv: 1805.10531cs, statMetzler, C. A., Mousavi, A., Heckel, R., and Baraniuk, R. G. Unsupervised Learning with Stein's Unbiased Risk Esti- mator. arXiv:1805.10531 [cs, stat], May 2018. URL http://arxiv.org/abs/1805.10531. arXiv: 1805.10531.\n\nBioNumbersthe database of key numbers in molecular and cell biology. R Milo, P Jorgensen, U Moran, G Weber, M Springer, 10.1093/nar/gkp889Nucleic Acids Research. 381supplMilo, R., Jorgensen, P., Moran, U., Weber, G., and Springer, M. BioNumbersthe database of key numbers in molecular and cell biology. Nucleic Acids Research, 38 (suppl 1):D750-D753, January 2010. ISSN 0305-1048, 1362-4962. doi: 10.1093/nar/gkp889. URL https:// academic.oup.com/nar/article-lookup/ doi/10.1093/nar/gkp889.\n\nAdaptive computation and machine learning series. K P Murphy, 978-0-262-01802-9MIT PressCambridge, MAMachine learning: a probabilistic perspectiveMurphy, K. P. Machine learning: a probabilistic perspective. Adaptive computation and machine learning series. MIT Press, Cambridge, MA, 2012. ISBN 978-0-262-01802-9.\n\nBi-cross-validation of the SVD and the nonnegative matrix factorization. A B Owen, P O Perry, 1932-6157. doi: 10.1214/ 08-AOAS227The Annals of Applied Statistics. 32Owen, A. B. and Perry, P. O. Bi-cross-validation of the SVD and the nonnegative matrix factoriza- tion. The Annals of Applied Statistics, 3(2):564- 594, June 2009. ISSN 1932-6157. doi: 10.1214/ 08-AOAS227. URL http://projecteuclid. org/euclid.aoas/1245676186.\n\nBi-cross-validation for factor analysis. A B Owen, J Wang, Statistical Science. 311Owen, A. B., Wang, J., and others. Bi-cross-validation for factor analysis. Statistical Science, 31(1):119-139, 2016.\n\nV Papyan, Y Romano, J Sulam, M Elad, arXiv:1705.03239Convolutional Dictionary Learning via Local Processing. Papyan, V., Romano, Y., Sulam, J., and Elad, M. Con- volutional Dictionary Learning via Local Processing. arXiv:1705.03239 [cs], May 2017. URL http:// arxiv.org/abs/1705.03239.\n\nTranscriptional Heterogeneity and Lineage Commitment in Noise2Self: Blind Denoising by Self-Supervision Myeloid Progenitors. F Paul, Y Arkin, A Giladi, D Jaitin, E Kenigsberg, H Keren-Shaul, D Winter, D Lara-Astiaso, M Gury, A Weiner, E David, N Cohen, F Lauridsen, S Haas, A Schlitzer, A Mildner, F Ginhoux, S Jung, A Trumpp, B Porse, A Tanay, Amit , I , 10.1016/j.cell.2015.11.013Cell. 1637Paul, F., Arkin, Y., Giladi, A., Jaitin, D., Kenigsberg, E., Keren-Shaul, H., Winter, D., Lara-Astiaso, D., Gury, M., Weiner, A., David, E., Cohen, N., Lauridsen, F., Haas, S., Schlitzer, A., Mildner, A., Ginhoux, F., Jung, S., Trumpp, A., Porse, B., Tanay, A., and Amit, I. Tran- scriptional Heterogeneity and Lineage Commitment in Noise2Self: Blind Denoising by Self-Supervision Myeloid Progenitors. Cell, 163(7):1663-1677, Decem- ber 2015. ISSN 00928674. doi: 10.1016/j.cell.2015. 11.013. URL https://linkinghub.elsevier. com/retrieve/pii/S0092867415014932.\n\nJPEG still image data compression standard. W B Pennebaker, J L Mitchell, 978-0-442-01272-4Van Nostrand ReinholdNew YorkPennebaker, W. B. and Mitchell, J. L. JPEG still image data compression standard. Van Nostrand Reinhold, New York, 1992. ISBN 978-0-442-01272-4.\n\nO Ronneberger, P Fischer, T Brox, U-Net, arXiv:1505.04597arXiv: 1505.04597Convolutional Networks for Biomedical Image Segmentation. Ronneberger, O., Fischer, P., and Brox, T. U-Net: Con- volutional Networks for Biomedical Image Segmen- tation. arXiv:1505.04597 [cs], May 2015. URL http://arxiv.org/abs/1505.04597. arXiv: 1505.04597.\n\nS Tripathi, Z C Lipton, T Q Nguyen, arXiv:1803.04477Correction by Projection: Denoising Images with Generative Adversarial Networks. Tripathi, S., Lipton, Z. C., and Nguyen, T. Q. Correction by Projection: Denoising Images with Generative Adversar- ial Networks. arXiv:1803.04477 [cs], March 2018. URL http://arxiv.org/abs/1803.04477.\n\n. D Ulyanov, A Vedaldi, V Lempitsky, Deep Image, Prior, arXiv:1711.10925arXiv: 1711.10925cs, statUlyanov, D., Vedaldi, A., and Lempitsky, V. Deep Image Prior. arXiv:1711.10925 [cs, stat], November 2017. URL http://arxiv.org/abs/1711.10925. arXiv: 1711.10925.\n\nStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. P Vincent, H Larochelle, I Lajoie, Y Bengio, P.-A Manzagol, Journal of machine learning research. 11Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Man- zagol, P.-A. Stacked denoising autoencoders: Learning useful representations in a deep network with a local de- noising criterion. Journal of machine learning research, 11(Dec):3371-3408, 2010.\n\nContent-Aware Image Restoration: Pushing the Limits of Fluorescence Microscopy. M Weigert, U Schmidt, T Boothe, A Mller, A Dibrov, A Jain, B Wilhelm, D Schmidt, C Broaddus, S Culley, M Rocha-Martins, F Segovia-Miranda, C Norden, R Henriques, M Zerial, M Solimena, J Rink, P Tomancak, L Royer, F Jug, E W Myers, 10.1101/236463Weigert, M., Schmidt, U., Boothe, T., Mller, A., Dibrov, A., Jain, A., Wilhelm, B., Schmidt, D., Broaddus, C., Culley, S., Rocha-Martins, M., Segovia-Miranda, F., Norden, C., Henriques, R., Zerial, M., Solimena, M., Rink, J., Tomancak, P., Royer, L., Jug, F., and My- ers, E. W. Content-Aware Image Restoration: Push- ing the Limits of Fluorescence Microscopy. July 2018. doi: 10.1101/236463. URL http://biorxiv.org/ lookup/doi/10.1101/236463.\n\nBeyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising. K Zhang, W Zuo, Y Chen, D Meng, L Zhang, 10.1109/TIP.2017.2662206IEEE Transactions on Image Processing. 267Zhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L. Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising. IEEE Transac- tions on Image Processing, 26(7):3142-3155, July 2017. ISSN 1057-7149, 1941-0042. doi: 10.1109/TIP.2017. 2662206. URL http://ieeexplore.ieee.org/ document/7839189/.\n\nTraining deep learning based image denoisers from undersampled measurements without ground truth and without image prior. M Zhussip, S Soltanayev, Chun , S Y , arXiv:1806.00961arXiv: 1806.00961Zhussip, M., Soltanayev, S., and Chun, S. Y. Training deep learning based image denoisers from undersam- pled measurements without ground truth and without image prior. arXiv:1806.00961 [cs], June 2018. URL http://arxiv.org/abs/1806.00961. arXiv: 1806.00961.\n\nA Krull, T.-O Buchholz, F Jug, arXiv:1811.10980arXiv: 1811.10980Noise2void -Learning Denoising from Single Noisy Images. Krull, A., Buchholz, T.-O., and Jug, F. Noise2void -Learning Denoising from Single Noisy Images. arXiv:1811.10980 [cs], November 2018. URL http://arxiv.org/abs/1811.10980. arXiv: 1811.10980.\n\nAnnotated high-throughput microscopy image sets for validation. V Ljosa, K L Sokolnicki, A E Carpenter, 10.1038/nmeth.2083Nature Methods. 97Ljosa, V., Sokolnicki, K. L., and Carpenter, A. E. Annotated high-throughput microscopy image sets for validation. Nature Methods, 9(7):637-637, July 2012. ISSN 1548- 7091, 1548-7105. doi: 10.1038/nmeth.2083. URL http: //www.nature.com/articles/nmeth.2083.\n\nAdaptive computation and machine learning series. K P Murphy, 978-0-262-01802-9MIT PressCambridge, MAMachine learning: a probabilistic perspectiveMurphy, K. P. Machine learning: a probabilistic perspective. Adaptive computation and machine learning series. MIT Press, Cambridge, MA, 2012. ISBN 978-0-262-01802-9.\n\nAutomatic differentiation in PyTorch. A Paszke, S Gross, S Chintala, G Chanan, E Yang, Z Devito, Z Lin, A Desmaison, L Antiga, A Lerer, Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. Automatic differentiation in PyTorch. In NIPS-W, 2017.\n\nTranscriptional Heterogeneity and Lineage Commitment in Myeloid Progenitors. F Paul, Y Arkin, A Giladi, D Jaitin, E Kenigsberg, H Keren-Shaul, D Winter, D Lara-Astiaso, M Gury, A Weiner, E David, N Cohen, F Lauridsen, S Haas, A Schlitzer, A Mildner, F Ginhoux, S Jung, A Trumpp, B Porse, A Tanay, Amit , I , 10.1016/j.cell.2015.11.013Cell. 1637Paul, F., Arkin, Y., Giladi, A., Jaitin, D., Kenigsberg, E., Keren-Shaul, H., Winter, D., Lara-Astiaso, D., Gury, M., Weiner, A., David, E., Cohen, N., Lauridsen, F., Haas, S., Schlitzer, A., Mildner, A., Ginhoux, F., Jung, S., Trumpp, A., Porse, B., Tanay, A., and Amit, I. Tran- scriptional Heterogeneity and Lineage Commitment in Myeloid Progenitors. Cell, 163(7):1663-1677, Decem- ber 2015. ISSN 00928674. doi: 10.1016/j.cell.2015. 11.013. URL https://linkinghub.elsevier. com/retrieve/pii/S0092867415014932.\n\nO Ronneberger, P Fischer, T Brox, U-Net, arXiv:1505.04597arXiv: 1505.04597Convolutional Networks for Biomedical Image Segmentation. Ronneberger, O., Fischer, P., and Brox, T. U-Net: Con- volutional Networks for Biomedical Image Segmen- tation. arXiv:1505.04597 [cs], May 2015. URL http://arxiv.org/abs/1505.04597. arXiv: 1505.04597.\n\nand contributors, t. s.-i. scikit-image: image processing in Python. PeerJ, 2:e453. S Van Der Walt, J L Schnberger, J Nunez-Iglesias, F Boulogne, J D Warner, N Yager, E Gouillart, T Yu, 10.7717/peerj.453van der Walt, S., Schnberger, J. L., Nunez-Iglesias, J., Boulogne, F., Warner, J. D., Yager, N., Gouillart, E., Yu, T., and contributors, t. s.-i. scikit-image: image processing in Python. PeerJ, 2:e453, 2014. ISSN 2167-8359. doi: 10.7717/peerj.453. URL https: //doi.org/10.7717/peerj.453.\n\nPattabiraman, D., and others. Recovering Gene Interactions from Single-Cell Data Using Data Diffusion. D Van Dijk, R Sharma, J Nainys, K Yim, P Kathail, A Carr, C Burdziak, K R Moon, C L Chaffer, Van Dijk, D., Sharma, R., Nainys, J., Yim, K., Kathail, P., Carr, A., Burdziak, C., Moon, K. R., Chaffer, C. L., Pat- tabiraman, D., and others. Recovering Gene Interactions from Single-Cell Data Using Data Diffusion. 2018.\n", "annotations": {"author": "[{\"end\":65,\"start\":51},{\"end\":77,\"start\":66}]", "publisher": null, "author_last_name": "[{\"end\":64,\"start\":58},{\"end\":76,\"start\":71}]", "author_first_name": "[{\"end\":57,\"start\":51},{\"end\":70,\"start\":66}]", "author_affiliation": null, "title": "[{\"end\":48,\"start\":1},{\"end\":125,\"start\":78}]", "venue": null, "abstract": "[{\"end\":1114,\"start\":127}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3221,\"start\":3199},{\"end\":3424,\"start\":3354},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8493,\"start\":8471},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9025,\"start\":9003},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9156,\"start\":9136},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9389,\"start\":9375},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9795,\"start\":9773},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10290,\"start\":10267},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10340,\"start\":10318},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10361,\"start\":10340},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10600,\"start\":10580},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10733,\"start\":10704},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11022,\"start\":11001},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11042,\"start\":11022},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11895,\"start\":11871},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11915,\"start\":11895},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12470,\"start\":12448},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12715,\"start\":12692},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13189,\"start\":13169},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14087,\"start\":14065},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16610,\"start\":16588},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16629,\"start\":16610},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17553,\"start\":17534},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19378,\"start\":19360},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19548,\"start\":19528},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19997,\"start\":19977},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20015,\"start\":19997},{\"end\":23710,\"start\":23684},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25371,\"start\":25351},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26233,\"start\":26211},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26260,\"start\":26240},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26325,\"start\":26305},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34632,\"start\":34606},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35558,\"start\":35538},{\"end\":36676,\"start\":36673},{\"end\":36680,\"start\":36676},{\"end\":37233,\"start\":37229},{\"end\":37236,\"start\":37233},{\"end\":37240,\"start\":37236},{\"end\":37244,\"start\":37240},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":37716,\"start\":37695},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":41059,\"start\":41036}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":42432,\"start\":41890},{\"attributes\":{\"id\":\"fig_1\"},\"end\":42709,\"start\":42433},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42976,\"start\":42710},{\"attributes\":{\"id\":\"fig_4\"},\"end\":43047,\"start\":42977},{\"attributes\":{\"id\":\"fig_5\"},\"end\":43248,\"start\":43048},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43567,\"start\":43249},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44025,\"start\":43568},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44367,\"start\":44026}]", "paragraph": "[{\"end\":2277,\"start\":1130},{\"end\":2940,\"start\":2279},{\"end\":3933,\"start\":2942},{\"end\":5092,\"start\":3935},{\"end\":5271,\"start\":5094},{\"end\":5369,\"start\":5273},{\"end\":5531,\"start\":5501},{\"end\":5734,\"start\":5559},{\"end\":5938,\"start\":5736},{\"end\":6227,\"start\":5978},{\"end\":6768,\"start\":6229},{\"end\":6960,\"start\":6770},{\"end\":7250,\"start\":7086},{\"end\":7466,\"start\":7252},{\"end\":7827,\"start\":7468},{\"end\":7918,\"start\":7829},{\"end\":8374,\"start\":7935},{\"end\":8719,\"start\":8376},{\"end\":9580,\"start\":8721},{\"end\":10068,\"start\":9582},{\"end\":10291,\"start\":10070},{\"end\":10496,\"start\":10293},{\"end\":11076,\"start\":10498},{\"end\":11630,\"start\":11078},{\"end\":12072,\"start\":11632},{\"end\":12471,\"start\":12074},{\"end\":12958,\"start\":12473},{\"end\":13190,\"start\":13045},{\"end\":13992,\"start\":13192},{\"end\":14471,\"start\":13994},{\"end\":14569,\"start\":14473},{\"end\":14905,\"start\":14604},{\"end\":15101,\"start\":14933},{\"end\":16135,\"start\":15128},{\"end\":16344,\"start\":16137},{\"end\":16427,\"start\":16394},{\"end\":16851,\"start\":16429},{\"end\":17061,\"start\":16853},{\"end\":17781,\"start\":17077},{\"end\":18189,\"start\":17783},{\"end\":19186,\"start\":18191},{\"end\":20163,\"start\":19194},{\"end\":20419,\"start\":20174},{\"end\":20626,\"start\":20455},{\"end\":20685,\"start\":20645},{\"end\":20784,\"start\":20687},{\"end\":20890,\"start\":20816},{\"end\":21079,\"start\":20958},{\"end\":21175,\"start\":21155},{\"end\":21513,\"start\":21225},{\"end\":22264,\"start\":21542},{\"end\":22275,\"start\":22274},{\"end\":22375,\"start\":22309},{\"end\":22828,\"start\":22377},{\"end\":22852,\"start\":22830},{\"end\":22980,\"start\":22854},{\"end\":23315,\"start\":22982},{\"end\":23569,\"start\":23356},{\"end\":23893,\"start\":23571},{\"end\":24041,\"start\":23895},{\"end\":24561,\"start\":24058},{\"end\":25372,\"start\":24589},{\"end\":25684,\"start\":25388},{\"end\":25745,\"start\":25686},{\"end\":26901,\"start\":25760},{\"end\":27113,\"start\":26903},{\"end\":27660,\"start\":27115},{\"end\":27948,\"start\":27673},{\"end\":28033,\"start\":27950},{\"end\":28145,\"start\":28035},{\"end\":28360,\"start\":28168},{\"end\":28394,\"start\":28393},{\"end\":28507,\"start\":28396},{\"end\":28599,\"start\":28534},{\"end\":28658,\"start\":28638},{\"end\":28741,\"start\":28660},{\"end\":28772,\"start\":28768},{\"end\":29058,\"start\":28798},{\"end\":29194,\"start\":29125},{\"end\":29289,\"start\":29232},{\"end\":29422,\"start\":29361},{\"end\":29548,\"start\":29424},{\"end\":29674,\"start\":29597},{\"end\":29805,\"start\":29727},{\"end\":29877,\"start\":29848},{\"end\":29945,\"start\":29918},{\"end\":30089,\"start\":29957},{\"end\":30172,\"start\":30091},{\"end\":30263,\"start\":30214},{\"end\":30612,\"start\":30265},{\"end\":31002,\"start\":30640},{\"end\":31295,\"start\":31004},{\"end\":31364,\"start\":31297},{\"end\":31946,\"start\":31366},{\"end\":32635,\"start\":32241},{\"end\":32783,\"start\":32637},{\"end\":33011,\"start\":32807},{\"end\":33121,\"start\":33013},{\"end\":33176,\"start\":33143},{\"end\":33286,\"start\":33198},{\"end\":33800,\"start\":33304},{\"end\":34093,\"start\":33842},{\"end\":34212,\"start\":34095},{\"end\":34437,\"start\":34214},{\"end\":34689,\"start\":34483},{\"end\":34846,\"start\":34691},{\"end\":35254,\"start\":34848},{\"end\":36126,\"start\":35278},{\"end\":36943,\"start\":36128},{\"end\":37765,\"start\":36960},{\"end\":37842,\"start\":37778},{\"end\":38137,\"start\":37884},{\"end\":38230,\"start\":38139},{\"end\":38313,\"start\":38232},{\"end\":38546,\"start\":38327},{\"end\":39063,\"start\":38548},{\"end\":39252,\"start\":39078},{\"end\":39293,\"start\":39254},{\"end\":39908,\"start\":39295},{\"end\":40043,\"start\":39910},{\"end\":40408,\"start\":40075},{\"end\":40657,\"start\":40410},{\"end\":40846,\"start\":40682},{\"end\":40863,\"start\":40848},{\"end\":41160,\"start\":40865},{\"end\":41242,\"start\":41162},{\"end\":41889,\"start\":41288}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5500,\"start\":5370},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5558,\"start\":5532},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5977,\"start\":5939},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7085,\"start\":6961},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13044,\"start\":12959},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14932,\"start\":14906},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15127,\"start\":15102},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16393,\"start\":16345},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20454,\"start\":20420},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20644,\"start\":20627},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20815,\"start\":20785},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20957,\"start\":20891},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21118,\"start\":21080},{\"attributes\":{\"id\":\"formula_13\"},\"end\":21154,\"start\":21118},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21224,\"start\":21176},{\"attributes\":{\"id\":\"formula_15\"},\"end\":22308,\"start\":22276},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23355,\"start\":23316},{\"attributes\":{\"id\":\"formula_17\"},\"end\":28392,\"start\":28361},{\"attributes\":{\"id\":\"formula_18\"},\"end\":28533,\"start\":28508},{\"attributes\":{\"id\":\"formula_19\"},\"end\":28637,\"start\":28600},{\"attributes\":{\"id\":\"formula_20\"},\"end\":28767,\"start\":28742},{\"attributes\":{\"id\":\"formula_21\"},\"end\":28797,\"start\":28773},{\"attributes\":{\"id\":\"formula_22\"},\"end\":29124,\"start\":29059},{\"attributes\":{\"id\":\"formula_23\"},\"end\":29231,\"start\":29195},{\"attributes\":{\"id\":\"formula_24\"},\"end\":29360,\"start\":29290},{\"attributes\":{\"id\":\"formula_25\"},\"end\":29596,\"start\":29549},{\"attributes\":{\"id\":\"formula_26\"},\"end\":29726,\"start\":29675},{\"attributes\":{\"id\":\"formula_27\"},\"end\":29847,\"start\":29806},{\"attributes\":{\"id\":\"formula_28\"},\"end\":29917,\"start\":29878},{\"attributes\":{\"id\":\"formula_29\"},\"end\":30213,\"start\":30173},{\"attributes\":{\"id\":\"formula_30\"},\"end\":32240,\"start\":31947},{\"attributes\":{\"id\":\"formula_31\"},\"end\":33142,\"start\":33122},{\"attributes\":{\"id\":\"formula_32\"},\"end\":33197,\"start\":33177},{\"attributes\":{\"id\":\"formula_33\"},\"end\":33303,\"start\":33287},{\"attributes\":{\"id\":\"formula_34\"},\"end\":33841,\"start\":33801},{\"attributes\":{\"id\":\"formula_35\"},\"end\":37883,\"start\":37843},{\"attributes\":{\"id\":\"formula_36\"},\"end\":40681,\"start\":40658},{\"attributes\":{\"id\":\"formula_37\"},\"end\":41287,\"start\":41243}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16863,\"start\":16856},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24518,\"start\":24511},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25050,\"start\":25043},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27598,\"start\":27330},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":36497,\"start\":36239},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37536,\"start\":37246},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41888,\"start\":41625}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1128,\"start\":1116},{\"attributes\":{\"n\":\"2.\"},\"end\":7933,\"start\":7921},{\"attributes\":{\"n\":\"3.\"},\"end\":14602,\"start\":14572},{\"attributes\":{\"n\":\"3.1.\"},\"end\":17075,\"start\":17064},{\"attributes\":{\"n\":\"3.2.\"},\"end\":19192,\"start\":19189},{\"attributes\":{\"n\":\"4.\"},\"end\":20172,\"start\":20166},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21540,\"start\":21516},{\"end\":22272,\"start\":22267},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24056,\"start\":24044},{\"attributes\":{\"n\":\"5.\"},\"end\":24587,\"start\":24564},{\"end\":25386,\"start\":25375},{\"attributes\":{\"n\":\"6.\"},\"end\":25758,\"start\":25748},{\"attributes\":{\"n\":\"1.\"},\"end\":27671,\"start\":27663},{\"attributes\":{\"n\":\"2.\"},\"end\":28166,\"start\":28148},{\"attributes\":{\"n\":\"3.\"},\"end\":29955,\"start\":29948},{\"attributes\":{\"n\":\"3.1.\"},\"end\":30638,\"start\":30615},{\"attributes\":{\"n\":\"3.2.\"},\"end\":32805,\"start\":32786},{\"attributes\":{\"n\":\"4.\"},\"end\":34481,\"start\":34440},{\"attributes\":{\"n\":\"5.\"},\"end\":35276,\"start\":35257},{\"attributes\":{\"n\":\"5.2.\"},\"end\":36958,\"start\":36946},{\"attributes\":{\"n\":\"5.3.\"},\"end\":37776,\"start\":37768},{\"attributes\":{\"n\":\"5.4.\"},\"end\":38325,\"start\":38316},{\"attributes\":{\"n\":\"5.5.\"},\"end\":39076,\"start\":39066},{\"attributes\":{\"n\":\"6.\"},\"end\":40073,\"start\":40046},{\"end\":41901,\"start\":41891},{\"end\":42719,\"start\":42711},{\"end\":43059,\"start\":43049},{\"end\":43259,\"start\":43250},{\"end\":43578,\"start\":43569}]", "table": "[{\"end\":43567,\"start\":43445},{\"end\":44025,\"start\":43755},{\"end\":44367,\"start\":44089}]", "figure_caption": "[{\"end\":42432,\"start\":41903},{\"end\":42709,\"start\":42435},{\"end\":42976,\"start\":42721},{\"end\":43047,\"start\":42979},{\"end\":43248,\"start\":43061},{\"end\":43445,\"start\":43261},{\"end\":43755,\"start\":43580},{\"end\":44089,\"start\":44028}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":4521,\"start\":4513},{\"end\":14177,\"start\":14169},{\"end\":15166,\"start\":15158},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":16446,\"start\":16438},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18599,\"start\":18591},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21790,\"start\":21782},{\"end\":22531,\"start\":22523},{\"end\":23582,\"start\":23574},{\"end\":24775,\"start\":24767},{\"end\":25655,\"start\":25647},{\"end\":26031,\"start\":26023},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34993,\"start\":34985}]", "bib_author_first_name": "[{\"end\":45104,\"start\":45103},{\"end\":45111,\"start\":45110},{\"end\":45123,\"start\":45122},{\"end\":45125,\"start\":45124},{\"end\":45135,\"start\":45134},{\"end\":45139,\"start\":45136},{\"end\":45582,\"start\":45581},{\"end\":45592,\"start\":45591},{\"end\":45603,\"start\":45599},{\"end\":45953,\"start\":45952},{\"end\":45963,\"start\":45962},{\"end\":45971,\"start\":45970},{\"end\":45973,\"start\":45972},{\"end\":46364,\"start\":46363},{\"end\":46366,\"start\":46365},{\"end\":46375,\"start\":46374},{\"end\":46381,\"start\":46380},{\"end\":46674,\"start\":46673},{\"end\":46683,\"start\":46682},{\"end\":46690,\"start\":46689},{\"end\":46703,\"start\":46702},{\"end\":47083,\"start\":47082},{\"end\":47092,\"start\":47091},{\"end\":47100,\"start\":47099},{\"end\":47115,\"start\":47111},{\"end\":47124,\"start\":47123},{\"end\":47465,\"start\":47464},{\"end\":47473,\"start\":47472},{\"end\":47949,\"start\":47948},{\"end\":47962,\"start\":47961},{\"end\":47971,\"start\":47970},{\"end\":47981,\"start\":47980},{\"end\":48270,\"start\":48269},{\"end\":48282,\"start\":48278},{\"end\":48294,\"start\":48293},{\"end\":48650,\"start\":48649},{\"end\":49014,\"start\":49013},{\"end\":49023,\"start\":49022},{\"end\":49033,\"start\":49032},{\"end\":49043,\"start\":49042},{\"end\":49264,\"start\":49263},{\"end\":49276,\"start\":49275},{\"end\":49288,\"start\":49287},{\"end\":49302,\"start\":49301},{\"end\":49311,\"start\":49310},{\"end\":49321,\"start\":49320},{\"end\":49335,\"start\":49331},{\"end\":49339,\"start\":49338},{\"end\":49584,\"start\":49583},{\"end\":49586,\"start\":49585},{\"end\":49597,\"start\":49596},{\"end\":49608,\"start\":49607},{\"end\":49618,\"start\":49617},{\"end\":49620,\"start\":49619},{\"end\":49961,\"start\":49960},{\"end\":49969,\"start\":49968},{\"end\":49982,\"start\":49981},{\"end\":49991,\"start\":49990},{\"end\":50000,\"start\":49999},{\"end\":50434,\"start\":50433},{\"end\":50436,\"start\":50435},{\"end\":50771,\"start\":50770},{\"end\":50773,\"start\":50772},{\"end\":50781,\"start\":50780},{\"end\":50783,\"start\":50782},{\"end\":51165,\"start\":51164},{\"end\":51167,\"start\":51166},{\"end\":51175,\"start\":51174},{\"end\":51326,\"start\":51325},{\"end\":51336,\"start\":51335},{\"end\":51346,\"start\":51345},{\"end\":51355,\"start\":51354},{\"end\":51738,\"start\":51737},{\"end\":51746,\"start\":51745},{\"end\":51755,\"start\":51754},{\"end\":51765,\"start\":51764},{\"end\":51775,\"start\":51774},{\"end\":51789,\"start\":51788},{\"end\":51804,\"start\":51803},{\"end\":51814,\"start\":51813},{\"end\":51830,\"start\":51829},{\"end\":51838,\"start\":51837},{\"end\":51848,\"start\":51847},{\"end\":51857,\"start\":51856},{\"end\":51866,\"start\":51865},{\"end\":51879,\"start\":51878},{\"end\":51887,\"start\":51886},{\"end\":51900,\"start\":51899},{\"end\":51911,\"start\":51910},{\"end\":51922,\"start\":51921},{\"end\":51930,\"start\":51929},{\"end\":51940,\"start\":51939},{\"end\":51949,\"start\":51948},{\"end\":51961,\"start\":51957},{\"end\":51965,\"start\":51964},{\"end\":52611,\"start\":52610},{\"end\":52613,\"start\":52612},{\"end\":52627,\"start\":52626},{\"end\":52629,\"start\":52628},{\"end\":52833,\"start\":52832},{\"end\":52848,\"start\":52847},{\"end\":52859,\"start\":52858},{\"end\":53167,\"start\":53166},{\"end\":53179,\"start\":53178},{\"end\":53181,\"start\":53180},{\"end\":53191,\"start\":53190},{\"end\":53193,\"start\":53192},{\"end\":53505,\"start\":53504},{\"end\":53516,\"start\":53515},{\"end\":53527,\"start\":53526},{\"end\":53879,\"start\":53878},{\"end\":53890,\"start\":53889},{\"end\":53904,\"start\":53903},{\"end\":53914,\"start\":53913},{\"end\":53927,\"start\":53923},{\"end\":54316,\"start\":54315},{\"end\":54327,\"start\":54326},{\"end\":54338,\"start\":54337},{\"end\":54348,\"start\":54347},{\"end\":54357,\"start\":54356},{\"end\":54367,\"start\":54366},{\"end\":54375,\"start\":54374},{\"end\":54386,\"start\":54385},{\"end\":54397,\"start\":54396},{\"end\":54409,\"start\":54408},{\"end\":54419,\"start\":54418},{\"end\":54436,\"start\":54435},{\"end\":54455,\"start\":54454},{\"end\":54465,\"start\":54464},{\"end\":54478,\"start\":54477},{\"end\":54488,\"start\":54487},{\"end\":54500,\"start\":54499},{\"end\":54508,\"start\":54507},{\"end\":54520,\"start\":54519},{\"end\":54529,\"start\":54528},{\"end\":54536,\"start\":54535},{\"end\":54538,\"start\":54537},{\"end\":55085,\"start\":55084},{\"end\":55094,\"start\":55093},{\"end\":55101,\"start\":55100},{\"end\":55109,\"start\":55108},{\"end\":55117,\"start\":55116},{\"end\":55627,\"start\":55626},{\"end\":55638,\"start\":55637},{\"end\":55655,\"start\":55651},{\"end\":55659,\"start\":55658},{\"end\":55661,\"start\":55660},{\"end\":55958,\"start\":55957},{\"end\":55970,\"start\":55966},{\"end\":55982,\"start\":55981},{\"end\":56335,\"start\":56334},{\"end\":56344,\"start\":56343},{\"end\":56346,\"start\":56345},{\"end\":56360,\"start\":56359},{\"end\":56362,\"start\":56361},{\"end\":56719,\"start\":56718},{\"end\":56721,\"start\":56720},{\"end\":57021,\"start\":57020},{\"end\":57031,\"start\":57030},{\"end\":57040,\"start\":57039},{\"end\":57052,\"start\":57051},{\"end\":57062,\"start\":57061},{\"end\":57070,\"start\":57069},{\"end\":57080,\"start\":57079},{\"end\":57087,\"start\":57086},{\"end\":57100,\"start\":57099},{\"end\":57110,\"start\":57109},{\"end\":57373,\"start\":57372},{\"end\":57381,\"start\":57380},{\"end\":57390,\"start\":57389},{\"end\":57400,\"start\":57399},{\"end\":57410,\"start\":57409},{\"end\":57424,\"start\":57423},{\"end\":57439,\"start\":57438},{\"end\":57449,\"start\":57448},{\"end\":57465,\"start\":57464},{\"end\":57473,\"start\":57472},{\"end\":57483,\"start\":57482},{\"end\":57492,\"start\":57491},{\"end\":57501,\"start\":57500},{\"end\":57514,\"start\":57513},{\"end\":57522,\"start\":57521},{\"end\":57535,\"start\":57534},{\"end\":57546,\"start\":57545},{\"end\":57557,\"start\":57556},{\"end\":57565,\"start\":57564},{\"end\":57575,\"start\":57574},{\"end\":57584,\"start\":57583},{\"end\":57596,\"start\":57592},{\"end\":57600,\"start\":57599},{\"end\":58154,\"start\":58153},{\"end\":58169,\"start\":58168},{\"end\":58180,\"start\":58179},{\"end\":58572,\"start\":58571},{\"end\":58588,\"start\":58587},{\"end\":58590,\"start\":58589},{\"end\":58604,\"start\":58603},{\"end\":58622,\"start\":58621},{\"end\":58634,\"start\":58633},{\"end\":58636,\"start\":58635},{\"end\":58646,\"start\":58645},{\"end\":58655,\"start\":58654},{\"end\":58668,\"start\":58667},{\"end\":59085,\"start\":59084},{\"end\":59097,\"start\":59096},{\"end\":59107,\"start\":59106},{\"end\":59117,\"start\":59116},{\"end\":59124,\"start\":59123},{\"end\":59135,\"start\":59134},{\"end\":59143,\"start\":59142},{\"end\":59155,\"start\":59154},{\"end\":59157,\"start\":59156},{\"end\":59165,\"start\":59164},{\"end\":59167,\"start\":59166}]", "bib_author_last_name": "[{\"end\":45108,\"start\":45105},{\"end\":45120,\"start\":45112},{\"end\":45132,\"start\":45126},{\"end\":45145,\"start\":45140},{\"end\":45589,\"start\":45583},{\"end\":45597,\"start\":45593},{\"end\":45609,\"start\":45604},{\"end\":45960,\"start\":45954},{\"end\":45968,\"start\":45964},{\"end\":45979,\"start\":45974},{\"end\":46372,\"start\":46367},{\"end\":46378,\"start\":46376},{\"end\":46390,\"start\":46382},{\"end\":46680,\"start\":46675},{\"end\":46687,\"start\":46684},{\"end\":46700,\"start\":46691},{\"end\":46714,\"start\":46704},{\"end\":47089,\"start\":47084},{\"end\":47097,\"start\":47093},{\"end\":47109,\"start\":47101},{\"end\":47121,\"start\":47116},{\"end\":47130,\"start\":47125},{\"end\":47470,\"start\":47466},{\"end\":47480,\"start\":47474},{\"end\":47959,\"start\":47950},{\"end\":47968,\"start\":47963},{\"end\":47978,\"start\":47972},{\"end\":47988,\"start\":47982},{\"end\":48276,\"start\":48271},{\"end\":48291,\"start\":48283},{\"end\":48298,\"start\":48295},{\"end\":48657,\"start\":48651},{\"end\":49020,\"start\":49015},{\"end\":49030,\"start\":49024},{\"end\":49040,\"start\":49034},{\"end\":49051,\"start\":49044},{\"end\":49273,\"start\":49265},{\"end\":49285,\"start\":49277},{\"end\":49299,\"start\":49289},{\"end\":49308,\"start\":49303},{\"end\":49318,\"start\":49312},{\"end\":49329,\"start\":49322},{\"end\":49351,\"start\":49340},{\"end\":49594,\"start\":49587},{\"end\":49605,\"start\":49598},{\"end\":49615,\"start\":49609},{\"end\":49629,\"start\":49621},{\"end\":49966,\"start\":49962},{\"end\":49979,\"start\":49970},{\"end\":49988,\"start\":49983},{\"end\":49997,\"start\":49992},{\"end\":50009,\"start\":50001},{\"end\":50443,\"start\":50437},{\"end\":50778,\"start\":50774},{\"end\":50789,\"start\":50784},{\"end\":51172,\"start\":51168},{\"end\":51180,\"start\":51176},{\"end\":51333,\"start\":51327},{\"end\":51343,\"start\":51337},{\"end\":51352,\"start\":51347},{\"end\":51360,\"start\":51356},{\"end\":51743,\"start\":51739},{\"end\":51752,\"start\":51747},{\"end\":51762,\"start\":51756},{\"end\":51772,\"start\":51766},{\"end\":51786,\"start\":51776},{\"end\":51801,\"start\":51790},{\"end\":51811,\"start\":51805},{\"end\":51827,\"start\":51815},{\"end\":51835,\"start\":51831},{\"end\":51845,\"start\":51839},{\"end\":51854,\"start\":51849},{\"end\":51863,\"start\":51858},{\"end\":51876,\"start\":51867},{\"end\":51884,\"start\":51880},{\"end\":51897,\"start\":51888},{\"end\":51908,\"start\":51901},{\"end\":51919,\"start\":51912},{\"end\":51927,\"start\":51923},{\"end\":51937,\"start\":51931},{\"end\":51946,\"start\":51941},{\"end\":51955,\"start\":51950},{\"end\":52624,\"start\":52614},{\"end\":52638,\"start\":52630},{\"end\":52845,\"start\":52834},{\"end\":52856,\"start\":52849},{\"end\":52864,\"start\":52860},{\"end\":52871,\"start\":52866},{\"end\":53176,\"start\":53168},{\"end\":53188,\"start\":53182},{\"end\":53200,\"start\":53194},{\"end\":53513,\"start\":53506},{\"end\":53524,\"start\":53517},{\"end\":53537,\"start\":53528},{\"end\":53549,\"start\":53539},{\"end\":53556,\"start\":53551},{\"end\":53887,\"start\":53880},{\"end\":53901,\"start\":53891},{\"end\":53911,\"start\":53905},{\"end\":53921,\"start\":53915},{\"end\":53936,\"start\":53928},{\"end\":54324,\"start\":54317},{\"end\":54335,\"start\":54328},{\"end\":54345,\"start\":54339},{\"end\":54354,\"start\":54349},{\"end\":54364,\"start\":54358},{\"end\":54372,\"start\":54368},{\"end\":54383,\"start\":54376},{\"end\":54394,\"start\":54387},{\"end\":54406,\"start\":54398},{\"end\":54416,\"start\":54410},{\"end\":54433,\"start\":54420},{\"end\":54452,\"start\":54437},{\"end\":54462,\"start\":54456},{\"end\":54475,\"start\":54466},{\"end\":54485,\"start\":54479},{\"end\":54497,\"start\":54489},{\"end\":54505,\"start\":54501},{\"end\":54517,\"start\":54509},{\"end\":54526,\"start\":54521},{\"end\":54533,\"start\":54530},{\"end\":54544,\"start\":54539},{\"end\":55091,\"start\":55086},{\"end\":55098,\"start\":55095},{\"end\":55106,\"start\":55102},{\"end\":55114,\"start\":55110},{\"end\":55123,\"start\":55118},{\"end\":55635,\"start\":55628},{\"end\":55649,\"start\":55639},{\"end\":55964,\"start\":55959},{\"end\":55979,\"start\":55971},{\"end\":55986,\"start\":55983},{\"end\":56341,\"start\":56336},{\"end\":56357,\"start\":56347},{\"end\":56372,\"start\":56363},{\"end\":56728,\"start\":56722},{\"end\":57028,\"start\":57022},{\"end\":57037,\"start\":57032},{\"end\":57049,\"start\":57041},{\"end\":57059,\"start\":57053},{\"end\":57067,\"start\":57063},{\"end\":57077,\"start\":57071},{\"end\":57084,\"start\":57081},{\"end\":57097,\"start\":57088},{\"end\":57107,\"start\":57101},{\"end\":57116,\"start\":57111},{\"end\":57378,\"start\":57374},{\"end\":57387,\"start\":57382},{\"end\":57397,\"start\":57391},{\"end\":57407,\"start\":57401},{\"end\":57421,\"start\":57411},{\"end\":57436,\"start\":57425},{\"end\":57446,\"start\":57440},{\"end\":57462,\"start\":57450},{\"end\":57470,\"start\":57466},{\"end\":57480,\"start\":57474},{\"end\":57489,\"start\":57484},{\"end\":57498,\"start\":57493},{\"end\":57511,\"start\":57502},{\"end\":57519,\"start\":57515},{\"end\":57532,\"start\":57523},{\"end\":57543,\"start\":57536},{\"end\":57554,\"start\":57547},{\"end\":57562,\"start\":57558},{\"end\":57572,\"start\":57566},{\"end\":57581,\"start\":57576},{\"end\":57590,\"start\":57585},{\"end\":58166,\"start\":58155},{\"end\":58177,\"start\":58170},{\"end\":58185,\"start\":58181},{\"end\":58192,\"start\":58187},{\"end\":58585,\"start\":58573},{\"end\":58601,\"start\":58591},{\"end\":58619,\"start\":58605},{\"end\":58631,\"start\":58623},{\"end\":58643,\"start\":58637},{\"end\":58652,\"start\":58647},{\"end\":58665,\"start\":58656},{\"end\":58671,\"start\":58669},{\"end\":59094,\"start\":59086},{\"end\":59104,\"start\":59098},{\"end\":59114,\"start\":59108},{\"end\":59121,\"start\":59118},{\"end\":59132,\"start\":59125},{\"end\":59140,\"start\":59136},{\"end\":59152,\"start\":59144},{\"end\":59162,\"start\":59158},{\"end\":59175,\"start\":59168}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1007/s00216-007-1790-1\",\"id\":\"b0\",\"matched_paper_id\":38397588},\"end\":45536,\"start\":45029},{\"attributes\":{\"doi\":\"10.1109/CVPR.2005.38\",\"id\":\"b1\"},\"end\":45894,\"start\":45538},{\"attributes\":{\"doi\":\"10.1137/040616024\",\"id\":\"b2\",\"matched_paper_id\":218466166},\"end\":46294,\"start\":45896},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":12479973},\"end\":46599,\"start\":46296},{\"attributes\":{\"doi\":\"10.1109/TIP.2007.901238\",\"id\":\"b4\",\"matched_paper_id\":1475121},\"end\":47023,\"start\":46601},{\"attributes\":{\"doi\":\"arXiv:1811.12766\",\"id\":\"b5\"},\"end\":47378,\"start\":47025},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6888534},\"end\":47845,\"start\":47380},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":65140756},\"end\":48267,\"start\":47847},{\"attributes\":{\"id\":\"b8\"},\"end\":48580,\"start\":48269},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":41118185},\"end\":48930,\"start\":48582},{\"attributes\":{\"id\":\"b10\"},\"end\":49214,\"start\":48932},{\"attributes\":{\"id\":\"b11\"},\"end\":49521,\"start\":49216},{\"attributes\":{\"id\":\"b12\"},\"end\":49889,\"start\":49523},{\"attributes\":{\"id\":\"b13\"},\"end\":50381,\"start\":49891},{\"attributes\":{\"id\":\"b14\"},\"end\":50695,\"start\":50383},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":408131},\"end\":51121,\"start\":50697},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5607641},\"end\":51323,\"start\":51123},{\"attributes\":{\"id\":\"b17\"},\"end\":51610,\"start\":51325},{\"attributes\":{\"id\":\"b18\"},\"end\":52564,\"start\":51612},{\"attributes\":{\"id\":\"b19\"},\"end\":52830,\"start\":52566},{\"attributes\":{\"id\":\"b20\"},\"end\":53164,\"start\":52832},{\"attributes\":{\"id\":\"b21\"},\"end\":53500,\"start\":53166},{\"attributes\":{\"id\":\"b22\"},\"end\":53760,\"start\":53502},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17804904},\"end\":54233,\"start\":53762},{\"attributes\":{\"id\":\"b24\"},\"end\":55003,\"start\":54235},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":996788},\"end\":55502,\"start\":55005},{\"attributes\":{\"id\":\"b26\"},\"end\":55955,\"start\":55504},{\"attributes\":{\"id\":\"b27\"},\"end\":56268,\"start\":55957},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":29890312},\"end\":56666,\"start\":56270},{\"attributes\":{\"id\":\"b29\"},\"end\":56980,\"start\":56668},{\"attributes\":{\"id\":\"b30\"},\"end\":57293,\"start\":56982},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":18038699},\"end\":58151,\"start\":57295},{\"attributes\":{\"id\":\"b32\"},\"end\":58485,\"start\":58153},{\"attributes\":{\"id\":\"b33\"},\"end\":58979,\"start\":58487},{\"attributes\":{\"id\":\"b34\"},\"end\":59400,\"start\":58981}]", "bib_title": "[{\"end\":45101,\"start\":45029},{\"end\":45950,\"start\":45896},{\"end\":46361,\"start\":46296},{\"end\":46671,\"start\":46601},{\"end\":47462,\"start\":47380},{\"end\":47946,\"start\":47847},{\"end\":48647,\"start\":48582},{\"end\":49958,\"start\":49891},{\"end\":50768,\"start\":50697},{\"end\":51162,\"start\":51123},{\"end\":51735,\"start\":51612},{\"end\":53876,\"start\":53762},{\"end\":55082,\"start\":55005},{\"end\":56332,\"start\":56270},{\"end\":57370,\"start\":57295}]", "bib_author": "[{\"end\":45110,\"start\":45103},{\"end\":45122,\"start\":45110},{\"end\":45134,\"start\":45122},{\"end\":45147,\"start\":45134},{\"end\":45591,\"start\":45581},{\"end\":45599,\"start\":45591},{\"end\":45611,\"start\":45599},{\"end\":45962,\"start\":45952},{\"end\":45970,\"start\":45962},{\"end\":45981,\"start\":45970},{\"end\":46374,\"start\":46363},{\"end\":46380,\"start\":46374},{\"end\":46392,\"start\":46380},{\"end\":46682,\"start\":46673},{\"end\":46689,\"start\":46682},{\"end\":46702,\"start\":46689},{\"end\":46716,\"start\":46702},{\"end\":47091,\"start\":47082},{\"end\":47099,\"start\":47091},{\"end\":47111,\"start\":47099},{\"end\":47123,\"start\":47111},{\"end\":47132,\"start\":47123},{\"end\":47472,\"start\":47464},{\"end\":47482,\"start\":47472},{\"end\":47961,\"start\":47948},{\"end\":47970,\"start\":47961},{\"end\":47980,\"start\":47970},{\"end\":47990,\"start\":47980},{\"end\":48278,\"start\":48269},{\"end\":48293,\"start\":48278},{\"end\":48300,\"start\":48293},{\"end\":48659,\"start\":48649},{\"end\":49022,\"start\":49013},{\"end\":49032,\"start\":49022},{\"end\":49042,\"start\":49032},{\"end\":49053,\"start\":49042},{\"end\":49275,\"start\":49263},{\"end\":49287,\"start\":49275},{\"end\":49301,\"start\":49287},{\"end\":49310,\"start\":49301},{\"end\":49320,\"start\":49310},{\"end\":49331,\"start\":49320},{\"end\":49338,\"start\":49331},{\"end\":49353,\"start\":49338},{\"end\":49596,\"start\":49583},{\"end\":49607,\"start\":49596},{\"end\":49617,\"start\":49607},{\"end\":49631,\"start\":49617},{\"end\":49968,\"start\":49960},{\"end\":49981,\"start\":49968},{\"end\":49990,\"start\":49981},{\"end\":49999,\"start\":49990},{\"end\":50011,\"start\":49999},{\"end\":50445,\"start\":50433},{\"end\":50780,\"start\":50770},{\"end\":50791,\"start\":50780},{\"end\":51174,\"start\":51164},{\"end\":51182,\"start\":51174},{\"end\":51335,\"start\":51325},{\"end\":51345,\"start\":51335},{\"end\":51354,\"start\":51345},{\"end\":51362,\"start\":51354},{\"end\":51745,\"start\":51737},{\"end\":51754,\"start\":51745},{\"end\":51764,\"start\":51754},{\"end\":51774,\"start\":51764},{\"end\":51788,\"start\":51774},{\"end\":51803,\"start\":51788},{\"end\":51813,\"start\":51803},{\"end\":51829,\"start\":51813},{\"end\":51837,\"start\":51829},{\"end\":51847,\"start\":51837},{\"end\":51856,\"start\":51847},{\"end\":51865,\"start\":51856},{\"end\":51878,\"start\":51865},{\"end\":51886,\"start\":51878},{\"end\":51899,\"start\":51886},{\"end\":51910,\"start\":51899},{\"end\":51921,\"start\":51910},{\"end\":51929,\"start\":51921},{\"end\":51939,\"start\":51929},{\"end\":51948,\"start\":51939},{\"end\":51957,\"start\":51948},{\"end\":51964,\"start\":51957},{\"end\":51968,\"start\":51964},{\"end\":52626,\"start\":52610},{\"end\":52640,\"start\":52626},{\"end\":52847,\"start\":52832},{\"end\":52858,\"start\":52847},{\"end\":52866,\"start\":52858},{\"end\":52873,\"start\":52866},{\"end\":53178,\"start\":53166},{\"end\":53190,\"start\":53178},{\"end\":53202,\"start\":53190},{\"end\":53515,\"start\":53504},{\"end\":53526,\"start\":53515},{\"end\":53539,\"start\":53526},{\"end\":53551,\"start\":53539},{\"end\":53558,\"start\":53551},{\"end\":53889,\"start\":53878},{\"end\":53903,\"start\":53889},{\"end\":53913,\"start\":53903},{\"end\":53923,\"start\":53913},{\"end\":53938,\"start\":53923},{\"end\":54326,\"start\":54315},{\"end\":54337,\"start\":54326},{\"end\":54347,\"start\":54337},{\"end\":54356,\"start\":54347},{\"end\":54366,\"start\":54356},{\"end\":54374,\"start\":54366},{\"end\":54385,\"start\":54374},{\"end\":54396,\"start\":54385},{\"end\":54408,\"start\":54396},{\"end\":54418,\"start\":54408},{\"end\":54435,\"start\":54418},{\"end\":54454,\"start\":54435},{\"end\":54464,\"start\":54454},{\"end\":54477,\"start\":54464},{\"end\":54487,\"start\":54477},{\"end\":54499,\"start\":54487},{\"end\":54507,\"start\":54499},{\"end\":54519,\"start\":54507},{\"end\":54528,\"start\":54519},{\"end\":54535,\"start\":54528},{\"end\":54546,\"start\":54535},{\"end\":55093,\"start\":55084},{\"end\":55100,\"start\":55093},{\"end\":55108,\"start\":55100},{\"end\":55116,\"start\":55108},{\"end\":55125,\"start\":55116},{\"end\":55637,\"start\":55626},{\"end\":55651,\"start\":55637},{\"end\":55658,\"start\":55651},{\"end\":55664,\"start\":55658},{\"end\":55966,\"start\":55957},{\"end\":55981,\"start\":55966},{\"end\":55988,\"start\":55981},{\"end\":56343,\"start\":56334},{\"end\":56359,\"start\":56343},{\"end\":56374,\"start\":56359},{\"end\":56730,\"start\":56718},{\"end\":57030,\"start\":57020},{\"end\":57039,\"start\":57030},{\"end\":57051,\"start\":57039},{\"end\":57061,\"start\":57051},{\"end\":57069,\"start\":57061},{\"end\":57079,\"start\":57069},{\"end\":57086,\"start\":57079},{\"end\":57099,\"start\":57086},{\"end\":57109,\"start\":57099},{\"end\":57118,\"start\":57109},{\"end\":57380,\"start\":57372},{\"end\":57389,\"start\":57380},{\"end\":57399,\"start\":57389},{\"end\":57409,\"start\":57399},{\"end\":57423,\"start\":57409},{\"end\":57438,\"start\":57423},{\"end\":57448,\"start\":57438},{\"end\":57464,\"start\":57448},{\"end\":57472,\"start\":57464},{\"end\":57482,\"start\":57472},{\"end\":57491,\"start\":57482},{\"end\":57500,\"start\":57491},{\"end\":57513,\"start\":57500},{\"end\":57521,\"start\":57513},{\"end\":57534,\"start\":57521},{\"end\":57545,\"start\":57534},{\"end\":57556,\"start\":57545},{\"end\":57564,\"start\":57556},{\"end\":57574,\"start\":57564},{\"end\":57583,\"start\":57574},{\"end\":57592,\"start\":57583},{\"end\":57599,\"start\":57592},{\"end\":57603,\"start\":57599},{\"end\":58168,\"start\":58153},{\"end\":58179,\"start\":58168},{\"end\":58187,\"start\":58179},{\"end\":58194,\"start\":58187},{\"end\":58587,\"start\":58571},{\"end\":58603,\"start\":58587},{\"end\":58621,\"start\":58603},{\"end\":58633,\"start\":58621},{\"end\":58645,\"start\":58633},{\"end\":58654,\"start\":58645},{\"end\":58667,\"start\":58654},{\"end\":58673,\"start\":58667},{\"end\":59096,\"start\":59084},{\"end\":59106,\"start\":59096},{\"end\":59116,\"start\":59106},{\"end\":59123,\"start\":59116},{\"end\":59134,\"start\":59123},{\"end\":59142,\"start\":59134},{\"end\":59154,\"start\":59142},{\"end\":59164,\"start\":59154},{\"end\":59177,\"start\":59164}]", "bib_venue": "[{\"end\":45210,\"start\":45172},{\"end\":45579,\"start\":45538},{\"end\":46030,\"start\":45998},{\"end\":46429,\"start\":46392},{\"end\":46776,\"start\":46739},{\"end\":47080,\"start\":47025},{\"end\":47559,\"start\":47522},{\"end\":48017,\"start\":47990},{\"end\":48388,\"start\":48333},{\"end\":48707,\"start\":48683},{\"end\":49011,\"start\":48932},{\"end\":49261,\"start\":49216},{\"end\":49581,\"start\":49523},{\"end\":50051,\"start\":50029},{\"end\":50431,\"start\":50383},{\"end\":50858,\"start\":50826},{\"end\":51201,\"start\":51182},{\"end\":51432,\"start\":51378},{\"end\":51998,\"start\":51994},{\"end\":52608,\"start\":52566},{\"end\":52962,\"start\":52906},{\"end\":53297,\"start\":53218},{\"end\":53974,\"start\":53938},{\"end\":54313,\"start\":54235},{\"end\":55186,\"start\":55149},{\"end\":55624,\"start\":55504},{\"end\":56076,\"start\":56021},{\"end\":56406,\"start\":56392},{\"end\":56716,\"start\":56668},{\"end\":57018,\"start\":56982},{\"end\":57633,\"start\":57629},{\"end\":58283,\"start\":58227},{\"end\":58569,\"start\":58487},{\"end\":59082,\"start\":58981},{\"end\":48049,\"start\":48019}]"}}}, "year": 2023, "month": 12, "day": 17}