{"id": 235359119, "updated": "2023-10-06 02:34:17.345", "metadata": {"title": "Multi-Target Domain Adaptation with Collaborative Consistency Learning", "authors": "[{\"first\":\"Takashi\",\"last\":\"Isobe\",\"middle\":[]},{\"first\":\"Xu\",\"last\":\"Jia\",\"middle\":[]},{\"first\":\"Shuaijun\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jianzhong\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Yongjie\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Jianzhuang\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Huchuan\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Shengjin\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 6, "day": 7}, "abstract": "Recently unsupervised domain adaptation for the semantic segmentation task has become more and more popular due to high-cost of pixel-level annotation on real-world images. However, most domain adaptation methods are only restricted to single-source-single-target pair, and can not be directly extended to multiple target domains. In this work, we propose a collaborative learning framework to achieve unsupervised multi-target domain adaptation. An unsupervised domain adaptation expert model is first trained for each source-target pair and is further encouraged to collaborate with each other through a bridge built between different target domains. These expert models are further improved by adding the regularization of making the consistent pixel-wise prediction for each sample with the same structured context. To obtain a single model that works across multiple target domains, we propose to simultaneously learn a student model which is trained to not only imitate the output of each expert on the corresponding target domain, but also to pull different expert close to each other with regularization on their weights. Extensive experiments demonstrate that the proposed method can effectively exploit rich structured information contained in both labeled source domain and multiple unlabeled target domains. Not only does it perform well across multiple target domains but also performs favorably against state-of-the-art unsupervised domain adaptation methods specially trained on a single source-target pair", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.03418", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/IsobeJCHSLLW21", "doi": "10.1109/cvpr46437.2021.00809"}}, "content": {"source": {"pdf_hash": "ba71b7005f2a94bf1519d88ae6f825e5b6aa8915", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.03418v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2106.03418", "status": "GREEN"}}, "grobid": {"id": "72a99bddc99279d74d81513802c14183b771e2f8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ba71b7005f2a94bf1519d88ae6f825e5b6aa8915.txt", "contents": "\nMulti-Target Domain Adaptation with Collaborative Consistency Learning\n\n\nTakashi Isobe \nDepartment of Electronic Engineering\nTsinghua University\n\n\nNoah's Ark Lab\nHuawei Technologies\n\n\nXu Jia xjia@dlut.edu.cn \nDalian University of Technology\n\n\nShuaijun Chen chenshuaijun@huawei.com \nNoah's Ark Lab\nHuawei Technologies\n\n\nJianzhong He jianzhong.he@huawei.com \nYongjie Shi \nNoah's Ark Lab\nHuawei Technologies\n\n\nKey Laboratory of Machine Perception (MOE)\nPeking University\n\n\nJianzhuang Liu liu.jianzhuang@huawei.com \nNoah's Ark Lab\nHuawei Technologies\n\n\nHuchuan Lu \nDalian University of Technology\n\n\nShengjin Wang \nDepartment of Electronic Engineering\nTsinghua University\n\n\nMulti-Target Domain Adaptation with Collaborative Consistency Learning\nCode is available at https://github.com/junpan19/MTDA. \u2020 The work was done in Noah's Ark Lab, Huawei Technologies. * Corresponding author\nRecently unsupervised domain adaptation for the semantic segmentation task has become more and more popular due to high-cost of pixel-level annotation on real-world images. However, most domain adaptation methods are only restricted to single-source-single-target pair, and can not be directly extended to multiple target domains. In this work, we propose a collaborative learning framework to achieve unsupervised multi-target domain adaptation. An unsupervised domain adaptation expert model is first trained for each source-target pair and is further encouraged to collaborate with each other through a bridge built between different target domains. These expert models are further improved by adding the regularization of making the consistent pixel-wise prediction for each sample with the same structured context. To obtain a single model that works across multiple target domains, we propose to simultaneously learn a student model which is trained to not only imitate the output of each expert on the corresponding target domain, but also to pull different expert close to each other with regularization on their weights. Extensive experiments demonstrate that the proposed method can effectively exploit rich structured information contained in both labeled source domain and multiple unlabeled target domains. Not only does it perform well across multiple target domains but also performs favorably against state-ofthe-art unsupervised domain adaptation methods specially trained on a single source-target pair.We first compare our method with the single-target domain adaptation (STDA) method on GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes with using ResNet-101 as backbone. The results are shown inTable 2. Our method performs favorably against state-of-the-art domain-specialized UDA methods on both GTA5-to-Cityscapes and SYNTHIAto-Cityscapes. However, it is noteworthy that with one round of training the proposed obtains a single model that achieves good performance on both Cityscapes and IDD. We also compare our method with DG and MTDA on \"GTA5 to Cityscapes and IDD\" and \"SYNTHIA to Cityscapes and\n\nIntroduction\n\nSemantic segmentation aims at interpreting an image by assigning each pixel to a semantic class [33,6,7,55,63]. Recently, semantic segmentation has achieved remarkable progress and is widely applied to intelligent systems such as autonomous driving, human-computer interaction and other low-level vision tasks [22,21,23]. Its success is mainly attributed to the supervised learning over large amounts of annotated data. However, human efforts on pixel-level annotations are expensive, which substantially limits the scalability of segmentation models. With large amounts of low-cost and diverse synthetic data simulated with game engines available, unsupervised domain adaptation (UDA) draws much attention to adapt the model learned on synthetic data to real-world data. Unsupervised domain adaptation methods [28,51,59,34,4,61,36,37] alleviate the issue of domain mismatch by training a model on both labeled source domain and unlabeled target domain.\n\nHowever, the setting of traditional unsupervised domain adaptation in semantic segmentation is usually restricted to single-source-single-target pair, as shown in Figure 1 (a). The learned model only works for a single target domain and can not be easily extended to multiple target domains, that is, multi-target domain adaptation (MTDA). With this setting, it is expected to learn a single model that is able to make full use of data from a single labeled source domain and multiple unlabeled target domains and performs well on multiple target domains simultaneously. This setting has great value in real-world applications. For example, in autonomous driving it is expected to have a model work in various environments with different lighting, weather and cityscapes. It is difficult to collect annotated data for such different environments but is easy to have large amounts of unlabeled data. There have been several works on MTDA [14,40,56], however, most of them focus on the classification task. Few works are developed to address the semantic segmentation task under the setting of multi-target domain adaptation. To the best of our knowledge, this is the first work to explore multi-target domain adaptation for semantic segmentation. The main challenge with this task are two folds: (1) lack of pixel-wise supervised information in multiple target domains poses great difficulty in mining inherent and transferable knowledge; (2) it is difficult to have a single model that works well on multiple target domains. There are two intuitive ways of extending the pair-wise DA to work on multiple target domains: (1) training multiple models individually for each target domain and (2) training a single model on combined data from multiple target domains. However, directly using multiple models would not play the model ensembling effect as in that in single domain. Inaccurate model dispatching would increase the risk of danger in practical applications. The model developed by direct data combination is likely to incur performance degradation due to the discrepancy between domains. Intuitively, a generic expert learned in a naive way might have inferior knowledge than the specialized expert for each target domain.\n\nIn this paper, we propose a novel collaborative consistency learning framework for multi-target domain adaptation, which includes collaborative consistency learning among multiple expert models and online knowledge distillation to obtain a single domain-generic student model. This work shows that once connection among domains is fully explored, i.e., connection between each source-target domain pair and among target domains, it can obtain even better performance than models learned with unsupervised domain adaptation methods for each source-target domain pair.\n\nIn the proposed collaborative consistency learning framework, data from all domains are first translated to the style of each target domain, respectively. In this way, we build a bridge between each pair of target domains, that is, images from the same domain are translated into different styles corresponding to different target domains. For each style, a semantic segmentation model is trained on both translated labeled data from source domain and translated unlabeled data from multiple target domains. Each network is a domainspecific expert and is trained with a kind of UDA loss and an additional consistency loss that align segmentation results of images of the same content but with different styles based on the bridge. Such collaborative consistency learning helps knowledge exchange among domain-specific experts.\n\nTo obtain a single model that works across multiple target domains, we design a student model whose weights are regularized by the weights of multiple experts and further teach it with multiple experts through knowledge distillation. In this way, the student model is able to learn common semantic knowledge from teachers across multiple domains.\n\nTo sum up, we make the following contributions:\n\n\u2022 To the best of our knowledge, this is the first work that explores the unsupervised multi-target domain adaptation task in semantic segmentation.\n\n\u2022 We propose a new collaborative consistency learning framework to handle the MTDA task for semantic segmentation, where unlabeled data in multiple target domains is fully leveraged to train a single model that works across all target domains.\n\n\u2022 Experimental results demonstrate the effectiveness of the proposed method. We can obtain a single model that not only works well across multiple target domains but also performs favorably against domain-specialized models on each target domain.\n\n\nRelated Work\n\n\nUnsupervised Domain Adaptation for Semantic Segmentation\n\nSingle-target Domain Adaptation. A typical practice for UDA in segmentation is to apply a model that is trained on a synthetic source domain to a real target domain. Unfortunately, the domain shift between the synthetic and real data would deteriorate the performance of model generalization [47,64,53]. There are three main categories of methods to seek a bridge the gap between the source and target domain. The first category is adversarial-based UDA [47,35,9,29,18,19,50,42] approaches which reduce domain discrepancy by maximizing the confusion between source and target in the feature [47,35,9,18,19] or entropy space [50,42]. The second category of methods attempt to learn domain-invariant representation by taking advantage of various image translation techniques [62,20], e.g. target-to-source translation in [53], bidirectional translation in [31] and texture-diversified translation in [26]. The third category of methods attempt to apply self-training [64,32,31,29,52,26,42] or model ensembling [54,50,8] for further improvement in the unlabeled target domain. Despite UDA for segmentation is a broadly studied topic, most of the previous works address address the UDA task under the setting of single-target domain adaptation (STDA), which has limitation in practical applications. Moreover, most of the previous works for STDA focus on fully utilizing the labeled data to improve the performance in unlabeled domain [19,3,53]. We argue that fully utilize the unlabeled data is also beneficial to explore the informative information within unlabeled data, thus improve the final performance on target domain. Based on these observations, multi-target domain adaptation (MTDA) is more realistic setting in real-world.\n\nMulti-target Domain Adaptation. There are two naive ways of directly extending domain-specialized UDA to work on multiple target domains, that are (1) training multiple models individually for each target domain (2) training a single model on combined data from multiple target domains. Unfortunately, these methods are not appropriate to handle MTDA problem because they would suffer from performance degradation due to the mismatching of multitarget domains. Despite several works have been done to address the MTDA task, they just focus on addressing classification task [14,40,56]. MTDA for segmentation is more challenging as it is in essence a dense pixel prediction task. The work most related to ours is [40], which also applies multiple teachers to obtain a common knowledge model for each target domain. However, in [40], unlabeled data from different target domains are not fully exploited to train stronger teachers and there is not any regularization in online knowledge distillation on both the student and teachers. Domain Generalization. The task of MTDA is also related to Domain generalization (DG), which attempts to generalize a model trained only on source domain to multiple unseen target domains by learning domain-invariant feature of source [25,12,1,58,30,57]. Khosla et al. [25] proposed removing the data bias by factoring out the domainspecific and domain-agnostic component during training on source domains. Yue et al. [30] proposed learning a domain-invariant feature representation via adversarial training. In [57], domain randomization and consistencyenforced training are both used to learn a domain-invariant network with synthetic images. Compared to the task of DG, where data from target domain is absent, the MTDA task aims at training a model for multiple target domains by fully exploring the unlabeled data.\n\n\nKnowledge Distillation\n\nKnowledge distillation (KD) has been widely studied for learning a compacting and fasting model for edge devices in real-world applications including face recognition, superresolution and object detection. The idea of KD is first proposed by [17], in which a student model is used to mimic the distribution of teacher's prediction. By transferring the knowledge from teacher to student, the student model is on par with or even better performance than the teacher model [13,38,16,41,24]. Rather than training a student to distill knowledge from a pretrained teacher, Zhang et al. [60] proposed to learn an ensemble of students which collaboratively teach each other throughout the training process. In this paper, we share similar philosophy as the general KD and adapt it to the MTDA task. Multiple domain-specific expert models with promising performance in each target domain are adopted as teacher, and a student is expected to perform well across all target domains. The student is taught simultaneously by multiple teachers, and also gives feedback to all teachers, all of which are implemented in an online fashion. gives rise to robust domain-invariant CNNs trained using synthetic images.\n\n\nMethodology\n\n\nOverview\n\nWe propose a novel framework to tackle the task of MTDA for semantic segmentation. Since only images from source domain have annotation maps, the key to this task is to make full use of given source domain data and to explore the way of mining rich structured information contained in unlabeled target domains. Our solution is to first train an expert model for each target domain, which is further encouraged to collaborate with each other simultaneously through a bridge built among different target domains. Since our final goal is to obtain a single model that works well on all target domains, we take the above expert models as teachers and additionally train a student model. It learns not only to imitate the output of each expert on the corresponding target domain but also to pulls different expert close to each other with regularization on their weights. The overall framework is illustrated in Figure 2. Note that all these are done in parallel at the same time.\n\nFormally, we denote data from source domain as D s = {(I s , y s )} and data from the m-th target domain as D tm = {I tm }, where I s and y s represent images and the associated pixel-wise annotation. The goal of our work is to adapt the knowledge from D s to M target domains D tm which are not associated with any annotation map.\n\n\nCollaborative Consistency Learning for MTDA\n\nLearning of multi-target domain experts. For each source-target domain pair, we train a domain adaptation model with most existing unsupervised domain adaptation method [50,47]. In this work, we train a model with a combination of cross-entropy loss on source domain D s for segmentation and adversarial loss for structure adapting, similar to [50,47]. However, instead of directly learning  Figure 2. Overview of the proposed Collaborative Consistency Learning (CCL) framework for MTDA in semantic segmentation. The framework is illustrated with M = 2 as example but it also holds for other numbers of target domains. Blue, yellow and green box represents the source, the 1-st and the 2-nd target domains, respectively.\n\nan expert with only data from each source-target pair, the proposed method would learn an expert with data available from all domains. Specifically, as for an expert of a particular target domain, style transfer method is first applied to translate data from all domains to the style of that target domain. In this way, discrepancy between different domains is reduced to some extent. With different semantic contexts but the same style helps learning a UDA expert model for a particular domain. In addition, re-styled data also works as a bridge to connect different target domains for knowledge exchange. The expert model for the m-th target domain is jointly optimized with supervised segmentation loss L m seg and adversarial loss L m adv as follows:\nL m = L m seg (P tm s , y s ) + \u03bb adv L m adv ,(1)\nwhere P is the output of the last layer of domain-specific expert. For I (\u00b7) (\u00b7) and P (\u00b7) (\u00b7) , superscript represents the translated style and subscript represents the corresponding domain. L m seg indicates the cross-entropy objective between the probability map and its pixel-level annotation map y s . \u03bb adv controls the weight of adversarial loss. L m adv is defined as:\nL m adv = E[log(1 \u2212 D m (P tm ))] + E[logD m (P tm s )] + M n=1 n =m E[log(1 \u2212 D m (P tm tn ))] + E[logD m (P tm s )],(2)\nwhich enforces the model to align multiple target domains with source domain and learn domain-invariant information with adversarial training. D m is a discriminator to classify the probability map whether from the source or the integrated target domain which is composed of multiple translated target domains. Note that all experts share the same network architecture but each one has a different set of weights.\n\nKnowledge exchange with collaborative consistency learning. The above expert domain adaptation models are able to give a reasonable performance on the corresponding domain adaptation task. However, power within data from multiple unlabeled target domains has not been fully exploited. As for data from a certain target domain, it has been translated into different styles of other target domains but with the same semantic context reserved. Multiple expert models are trained to make the consistent pixel-wise prediction for each sample with the same semantic context. Since different expert models are learned on samples of different styles, they learn the pixel-wise classification ability in different ways, and their predictions vary from each other. It is such different predictions that provide an opportunity to learn complementary knowledge from other experts and extract essential information that really matters to the performance of semantic segmentation. Therefore, we exploit collaborative learning for knowledge exchange among multiple expert models. The knowledge exchange with collaborative learning from other experts to the m-th expert can be formulated as:\nL m cl = 1 M \u2212 1 M n=1 n =m D KL (P tn ||P tm tn ),(3)\nwhere D KL is average of Kullback-Leibler (KL)-divergence between the probability map P tm tn and P tn . The expert of the domain m is trained to imitate the output distribution of other M -1 domain experts by L cl . Such knowledge exchange encourages each expert to make full use of unlabeled data in an unsupervised manner. The overall objective function of the m-th domain-specific expert is optimized by:\nL expert = 1 M M n=1 (L n + \u03bb cl L n cl ),(4)\nwhere \u03bb cl leverages the importance of consistency loss.\n\n\nOnline Knowledge Distillation from Multiple Experts\n\nWe have explained how to train multiple domainspecialized experts by making full use of available labeled and unlabeled data to improve their capability. However, our final purpose is to obtain a single model that performs well across multiple target domains. We propose to online distill knowledge from multiple expert models with additional regularization on their model weights. Specifically, a student network is added to the framework and is supervised with the output of multiple experts.\nL student okd = 1 M M n=1 D KL (P tn ||Q tn ),(5)\nwhere Q is the output of the last layer of the domain-generic student. Then, the overall optimization objective of domaingeneric student model can be defined as:\nL student = L student seg (Qs, ys) + \u03bb adv L student adv + \u03bb okd L student okd ,(6)\nwhere \u03bb okd is the weight factor to balance the training of online knowledge distillation and weights regularization, respectively. L student seg means the cross-entropy objective function between the probability map Q s and its pixel-level annotation map y s . The adversarial loss L student adv is expressed as:\nL student adv = 1 M M n=1 E[log(1 \u2212 D student (Q tn ))] + E[logD student (Q s )],(7)\nwhere D student is a discriminator for training domaingeneric student model. However, the performance of directly forcing a student to learn from multiple experts is limited due to diversity among multiple experts. The student might get confused in simultaneously distilling knowledge from very different experts. To address this issue, we propose to pull domain-specific experts a bit closer to the student. In this way, the gap between experts is reduced and it is easier for the student to distill common useful knowledge from these experts. The gap between domain-specific experts {F m expert } M m=1 and domain-generic student F student can be reduced with the following the weights regularization term:\nL wr = 1 M M m=1 ||\u03b8 m \u2212 \u03b8 student || 1 ,(8)\nwhere \u03b8 m and \u03b8 s represents the weights of the m-th domainspecific expert model and the domain-generic student model, respectively. The overall optimization objective of the CCL framework can be defined as:\nL = L student + L expert + \u03bb wr L wr ,(9)\nwhere \u03bb wr is the weighting parameters. Finally, the obtained domain-generic model is applied across M target domains.\n\n\nExperiments\n\nIn this section, we describe the experiment setting and implementation details of the proposed CCL. Extensive ablation studies and comparison with other MTDA and STDA methods are also provided. We show that our method can work well on multiple large scale urban driving datasets.\n\n\nDatasets\n\nUnder the MTDA experiment setting, synthetic datasets including GTA5 [44] and SYNTHIA [45] are used as source domain respectively, along with multiple real-world datasets Cityscapes [10], Indian Driving (IDD) [49] and Mapillary [39] as the target domains. The proposed CCL model is trained with labeled source data and unlabeled target data from various domains. Results on the validation sets of the datasets corresponding to the multiple target domains are used to evaluate its performance.\n\nGTA5 contains 24,966 synthetic images with a resolution of 1914\u00d71052 pixels that are collected from the video game GTA5 along with pixel-level annotations that are compatible with Cityscapes, IDD and Mapillary in 19 categories. Table 1. Performance comparison between our method and baseline models on adaptation from GTA5 to Cityscapes and IDD. The mIoU is calculated by the average of the intersection-over-union (IoU) among all 19 categories. \"R\" represents the ResNet101-based model and \"V\" represents the VGG16-based model. \"C\" and \"I\" indicate the target domain on Cityscapes and IDD, respectively. \"*\" represents the method with multiple models that are individually trained for each target domain.   Table 2. Comparison of our model with SOTA UDA methods, DG methods and MTDA methods with ResNet-101 as backbone.\n\nThe mIoU and mIoU* are evaluated over the 19 and 13 classes, respectively. \"G\", \"S\", \"C\" and \"I\" represent \"GTA5\", \"SYNTHIA\", \"Cityscapes\" and \"IDD\", respectively. \u2020 means the results of our implementation. All numbers correspond to the results without using pseudo labels or model ensembling as reported in the original papers.\n\nSetting Method mIoU mIoU* G \u2192 C G \u2192 I S \u2192 C S \u2192 I STDA AdaptSeg [47] 42. SYNTHIA is another synthetic dataset. The SYNTHIA-RAND-CITYSCAPES split of SYNTHIA, which contains 9,400 rendered images of 1280\u00d7760 resolution, is used as another source domain. We use the 16 common categories with Cityscapes, IDD and Mapillary for training and 13 common classes for testing.\n\nCityscapes is a real-world dataset with 5,000 street scenes taken from European cities and labeled into 19 classes. We use 2,975 images for training and 500 validation images.\n\nIDD is a more diverse dataset than Cityscapes which captures unstructured traffic on India's road. It contains a total of 10,003 images, with 6,993 images for training, 981 for validation and 2,029 for testing.\n\nMapillary provides 25,000 images collected from all around the world and diverse source of image capturing devices. It includes 18,000 images for training, 5,000 images for testing, and 2,000 images for validation.  \n\n\nTraining Details\n\nSimilar to [47] and [50], we use the DeepLab-v2 [5] model with ResNet-101 [15] and VGG-16 [46] as backbones and initialize them with models pre-trained on Ima-geNet [11]. For the discriminator, we also adopt the same network architecture as [47,50]. The semantic segmentation model parameters are optimized with SGD optimizer [2] where the weight decay and momentum are set to 0.9 and 5 \u00d7 10 \u22124 , respectively. The learning rate is initially set to 2.5 \u00d7 10 \u22124 . The polynomial procedure [5] is used as the learning rate schedule. The discriminator is optimized with Adam optimizer [27] with the momentum 0.9 and 0.99 with the learning rate is set to 10 \u22124 . We set \u03bb adv , \u03bb cl , \u03bb okd and \u03bb wr as 10 \u22123 . Here, we adopt a simple way to conduct image translation in gamut of LAB color space [43].\n\n\nComparison with Baseline Models\n\nWe compare the segmentation performance of the proposed CCL with three baselines: \"Individual Model\", \"Source Only\" and \"Data Combination\". \"Individual Model\", similar to [50], is to train multiple models for each corresponding target. \"Source Only\" and \"Data Combination\" are the MTDA setting which trains a single model across multiple target domains. \"Source Only\" is to train a model with the data only from source domain. \"Data combination\" is trained by directly combine data from multiple target domains as one domain. Here, we conduct the experiment with two target domains (i.e., M =2), but our method can be easily extended to the case of more number of target domains. The results of each method are reported in Table 1. In Table 1, the method of \"Individual Model\" that trains two models individually on Cityscapes and IDD achieves 43.3% and 43.6% mIoU on the corresponding domain. However, it requires two models for each domain. Compared to that, \"Source only\" use a single model but suffers considerable performance drops by 6.5% and 6.1% on Cityscapes and IDD because of the domain shift between the synthetic and real data. By directly combining the multiple target data as one domain, the model trained by \"Data Combination\" also suffers the performance degradation lagging behind the method of \"Individual Model\" by 3.0% and 1.6% mIoU on Cityscapes and IDD. Our method with a single model achieves 45.0% and 46.0% mIoU on Cityscapes and IDD, which significantly outperforms the \"Data Combination\" by +4.7% and +4.0%. By fully exploring unlabeled data from multiple target domains, the proposed CCL even works better than the \"Individual Model\", which adopts two models and trained on each target domain individually, by +1.7% and +2.4% mIoU on Cityscapes and IDD. The qualitative comparison between different baselines and the proposed CCL are provided in Figure 3. Table 4. Results of adapting GTA5 to different target domains with ResNet-101 as backbone. \"C\", \"I\" and \"M\" represent \"Cityscapes\", \"IDD\" and \"Mapillary\", respectively. IDD\". Compared to the method of DG, where the unlabeled data were not be used in [57] during training. We surpass [57] on both Cityscapes and IDD, respectively. We compare our method with two previous methods on MTDA.\n\nSince the previous works on MTDA only focus on the classification task, we carefully implement these methods in semantic segmentation with the same network. Compared to \"MTDA-ITA\", our method achieves significantly better performance on both domains. \"MT-MTDA\" is the method that adopts multiple teachers to alternatively teach a student in an offline knowledge distillation manner. However, the method also not consider to explore the information from different target domains. Our method achieves better performance than [40] on both Cityscapes and IDD.\n\n\nAblation Study\n\nIn this section, we evaluate each component in the proposed CCL framework by conducting ablation studies on GTA5 to Cityscapes and IDD task with ResNet-101 as backbone. Results are shown in Table 3.\n\nWe conduct a set of ablation study to examine the role of different components of the proposed method. A baseline (Model 1) here is designed as a method of directly applying adversarial loss to both target domains, i.e., \u03bb cl = \u03bb okd = \u03bb wr = 0. When online knowledge distillation loss \u03bb okd is switched on, Model 2 gains +1.0% mIoU improvement on IDD but suffers from 0.5% mIoU drops on Cityscapes. That could be explained by the confusion caused by the domain shift with expert models. When the weight regularization loss \u03bb wr is switched on, Model 3 gains evident improvement of +0.8% and +0.6% mIoU than the baseline on Cityscapes and IDD. Using \u03bb okd and \u03bb wr simultaneously improve the Model 1 by 1.7% and 1.8% mIoU on Cityscapes and IDD, and also outperforms \"Individual Model\" in both target domains. Consistent improvement over Model 2, Model 3 and Model 4 is gained when collaborative consistency learning is employed. Specifically, Model 7 gains evident 1.0% and 1.3% improvement from Model 4 on Cityscapes and IDD, simultaneously. \n\n\nGeneralization to Different Datasets\n\nSynthetic-to-real MTDA. Here, we conduct a set of experiments with different target domains. We consider the task of STDA as our baseline, that includes: (1) GTA5 to Cityscapes, (2) GTA5 to IDD and (3) GTA5 to Mapillary. Each STDA model is trained on the corresponding target domain, individually. In Table 4, three STDA baselines with three individually trained models achieve 43.3%, 43.6% and 45.8% mIoU on Cityscapes, IDD and Mapillary, respectively. It can also be extended to adaptation to all these three datasets. Experiment results show that our method with a single model consistently works better than the STDA baseline, which is individually trained on the corresponding target domains. Our method using a single model consistently works better than the STDA baseline on the corresponding target domains.\n\nReal-to-real MTDA. In Table 5, we also conduct a domain experiment from real-world datasets to real-world datasets. Here one of the Cityscapes, IDD and Mapillary is adopted as the source domain and the rest two are taken as the two target domains. Experimental results show that the proposed method not only works well on syn-to-real adaptation but also does a good job on the case of real-to-real.\n\n\nConclusion\n\nIn this work, we propose a novel collaborative consistency learning framework to achieve multi-target domain adaptation. The key idea is to first train a strong expert model for each target domain by simultaneously imposing consistency constraint among prediction from multiple expert models. They are further used as multiple teachers to collaboratively teach a student model in an online fashion such that a single model is able to work well across multiple target domains. Extensive experiments show that our method not only produces a single model that works well on multiple target domains but also achieves favorably performance against domain-specialized UDA methods on each domain.\n\nFigure 1 .\n1Comparison between the setting of single-target domain adaptation (STDA) and multi-target domain adaptation (MTDA). (a) Multiple STDA models with each one corresponding to a single target domain. (b) A single MTDA model working across multiple target domains.\n\nFigure 3 .\n3Qualitative results for GTA5 to Cityscapes and IDD.\n\n\nDomain-specific Expert 2Domain-generic Student \n\n\u2112 1 \n\u2112 \n\n\u2112 1 \n\n\u2112 1 \n\nPrediction Maps \n\n\u2112 \n\n\u2112 2 \n\nP t 2 \n\nt 1 \n\nStyle \n\nI s \n\nI t 2 \n\nI t 1 \n\nI s \n\nI t 2 \n\nI t 2 \n\nI s \n\nt 2 \n\nI t 1 \n\nt 2 \n\nDomain-specific Expert 1 \n\nI t 1 \n\nI s \n\nt 1 \n\nI t 2 \n\nt 1 \n\nStyle \n\nStyle \n\nStyle \n\n\u2112 2 \n\nI s \n\nI t 1 \n\n\u2112 2 \n\nQ t 1 \n\nQ t 2 \n\nP t 1 \n\nt 2 \n\nP t 2 \n\n\u2112 2 \n\n\u2112 \nTarget \nDomain 1 \n\nTarget \nDomain 2 \n\nSource \nDomain \n\nP s \n\nt 1 \n\nP s \n\nP s \n\nt 2 \n\nP t 1 \n\n\u2112 1 \n\n\n\n\nGTA5 \u2192 Cityscapes & IDDMethod \nModel \nTarget \nroad \nsidewalk \nbuilding \n\nwall \nfence \npole \nlight \nsign \nveg. \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotor \nbike \nmIoU \n\nIndividual Model* V \nC 88.4 30.8 78.4 29.8 25.9 20.5 17.6 11.2 79.2 30.3 65.1 46.6 9.1 81.2 22.9 29.9 0.1 11.9 0.5 35.8 \nI 68.8 2.5 61.4 29.2 20.8 24.9 7.3 34.3 75.6 29.3 91.2 39.8 28.3 63.6 35.8 38.8 0 39.2 7.8 36.8 \n\nSource only \nV \nC 64.0 16.8 67.0 22.6 18.9 22.1 20.6 13.3 76.8 14.8 63.9 47.9 5.7 72.5 12.3 12.9 9.5 19.1 2.3 30.7 \nI 50.9 2.3 45.8 21.8 20.5 26.8 6.8 39.6 76.1 28.3 82.0 38.6 28.8 69.2 38.2 16.6 0 49.1 9.7 34.3 \n\nData Combination V \nC 86.8 16.1 77.1 27.8 16.6 22.1 16.4 6.1 80.9 30.9 68.0 43.2 8.9 80.7 23.3 15.2 0 11.0 1.3 33.3 \nI 73.\n\nTable 3 .\n3Ablation studies of the proposed CCL framework on GTA5 to Cityscapes and IDD with ResNet-101 as backbone.Model # \nL cl \nL okd \nLwr \nC \nI \n\n1 \n42.3 \n42.9 \n\n2 \n\n41.8 \n43.9 \n3 \n\n43.1 \n43.5 \n4 \n\n\n44.0 \n44.7 \n\n5 \n\n\n42.4 \n45.2 \n6 \n\n\n44.2 \n44.9 \n7 \n\n\n\n45.0 \n46.0 \n\nIndividual Model \n43.3 \n43.6 \n\n\n\nTable 5 .\n5Results for real-to-real MTDA experiments.Souce \nTarget \nmIoU \nC \nI \nM \nC \nI \nM \n\nC \n\n\n-\n51.4 \n-\n\n-\n-\n49.6 \n\n\n-\n53.6 \n51.4 \n\nI \n\n\n46.5 \n-\n-\n\n-\n-\n49.0 \n\n\n46.8 \n-\n49.8 \n\nM \n\n\n57.9 \n-\n-\n\n-\n52.3 \n-\n\n\n58.5 \n54.1 \n-\n\n\n\nTowards domain generalization using metaregularization. Yogesh Balaji, Swami Sankaranarayanan, Rama Chellappa, Metareg, In NeurIPS. Yogesh Balaji, Swami Sankaranarayanan, and Rama Chel- lappa. Metareg: Towards domain generalization using meta- regularization. In NeurIPS, 2018.\n\nLarge-scale machine learning with stochastic gradient descent. L\u00e9on Bottou, Proceedings of COMPSTAT'2010. COMPSTAT'2010L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010. 2010.\n\nAll about structure: Adapting structural information across domains for boosting semantic segmentation. Wei-Lun Chang, Hui-Po Wang, Wen-Hsiao Peng, Wei-Chen Chiu, CVPR. Wei-Lun Chang, Hui-Po Wang, Wen-Hsiao Peng, and Wei- Chen Chiu. All about structure: Adapting structural informa- tion across domains for boosting semantic segmentation. In CVPR, 2019.\n\nProgressive feature alignment for unsupervised domain adaptation. Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, Junzhou Huang, CVPR. Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xing- hao Ding, Yue Huang, Tingyang Xu, and Junzhou Huang. Progressive feature alignment for unsupervised domain adap- tation. In CVPR, 2019.\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, TPAMI. 404Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. TPAMI, 40(4):834-848, 2017.\n\nRethinking atrous convolution for semantic image segmentation. Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam, abs/1706.05587CoRRLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017.\n\nEncoder-decoder with atrous separable convolution for semantic image segmentation. Yukun Liang-Chieh Chen, George Zhu, Florian Papandreou, Hartwig Schroff, Adam, ECCV. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.\n\nDomain adaptation for semantic segmentation with maximum squares loss. Minghao Chen, Hongyang Xue, Deng Cai, ICCV. Minghao Chen, Hongyang Xue, and Deng Cai. Domain adaptation for semantic segmentation with maximum squares loss. In ICCV, 2019.\n\nRoad: Reality oriented adaptation for semantic segmentation of urban scenes. Yuhua Chen, Wen Li, Luc Van Gool, CVPR. Yuhua Chen, Wen Li, and Luc Van Gool. Road: Reality ori- ented adaptation for semantic segmentation of urban scenes. In CVPR, 2018.\n\nUwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, CVPR. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\n\nDomain generalization via model-agnostic learning of semantic features. Qi Dou, Daniel Coelho De Castro, Konstantinos Kamnitsas, Ben Glocker, NeurIPS. Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning of semantic features. In NeurIPS, 2019.\n\nBorn again neural networks. Tommaso Furlanello, C Zachary, Michael Lipton, Laurent Tschannen, Anima Itti, Anandkumar, In ICML. Tommaso Furlanello, Zachary C Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In ICML. 2018.\n\nUnsupervised multitarget domain adaptation: An information theoretic approach. Pritish Behnam Gholami, Ognjen Sahu, Rudovic, IEEE Transactions on Image Processing. Konstantinos Bousmalis, and Vladimir PavlovicBehnam Gholami, Pritish Sahu, Ognjen Rudovic, Konstanti- nos Bousmalis, and Vladimir Pavlovic. Unsupervised multi- target domain adaptation: An information theoretic approach. IEEE Transactions on Image Processing, 2020.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nA comprehensive overhaul of feature distillation. Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, Jin Young Choi, ICCV. Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation. In ICCV, 2019.\n\nDistilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, NeurIPSW. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NeurIPSW, 2015.\n\nFcns in the wild: Pixel-level adversarial and constraint-based adaptation. Judy Hoffman, Dequan Wang, Fisher Yu, Trevor Darrell, abs/1612.02649CoRRJudy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. CoRR, abs/1612.02649, 2016.\n\nContextual-relation consistent domain adaptation for semantic segmentation. Jiaxing Huang, Shijian Lu, Dayan Guan, Xiaobing Zhang, ECCVJiaxing Huang, Shijian Lu, Dayan Guan, and Xiaobing Zhang. Contextual-relation consistent domain adaptation for semantic segmentation. ECCV, 2020.\n\nArbitrary style transfer in real-time with adaptive instance normalization. Xun Huang, Serge Belongie, ICCV. Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017.\n\nVideo super-resolution with recurrent structure-detail network. Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, Qi Tian, ECCV. Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, and Qi Tian. Video super-resolution with recurrent structure-detail network. In ECCV, 2020.\n\nVideo super-resolution with temporal group attention. Takashi Isobe, Songjiang Li, Xu Jia, Shanxin Yuan, Gregory Slabaugh, Chunjing Xu, Ya-Li Li, Shengjin Wang, Qi Tian, CVPR. Takashi Isobe, Songjiang Li, Xu Jia, Shanxin Yuan, Gregory Slabaugh, Chunjing Xu, Ya-Li Li, Shengjin Wang, and Qi Tian. Video super-resolution with temporal group attention. In CVPR, 2020.\n\nRevisiting temporal modeling for video super-resolution. Takashi Isobe, Fang Zhu, Shengjin Wang, BMVC. Takashi Isobe, Fang Zhu, and Shengjin Wang. Revisiting temporal modeling for video super-resolution. In BMVC, 2020.\n\nTowards oracle knowledge distillation with neural architecture search. Minsoo Kang, Jonghwan Mun, Bohyung Han, AAAI. Minsoo Kang, Jonghwan Mun, and Bohyung Han. Towards oracle knowledge distillation with neural architecture search. In AAAI, 2020.\n\nUndoing the damage of dataset bias. Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A Efros, Antonio Torralba, ECCV. Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A Efros, and Antonio Torralba. Undoing the damage of dataset bias. In ECCV, 2012.\n\nLearning texture invariant representation for domain adaptation of semantic segmentation. Myeongjin Kim, Hyeran Byun, CVPR. Myeongjin Kim and Hyeran Byun. Learning texture invariant representation for domain adaptation of semantic segmenta- tion. In CVPR, 2020.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n\nMohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for unsupervised domain adaptation. Chen-Yu Lee, Tanmay Batra, CVPR. Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for unsuper- vised domain adaptation. In CVPR, 2019.\n\nContent-consistent matching for domain adaptive semantic segmentation. Guangrui Li, Guoliang Kang, Wu Liu, Yunchao Wei, Yi Yang, ECCV. Guangrui Li, Guoliang Kang, Wu Liu, Yunchao Wei, and Yi Yang. Content-consistent matching for domain adaptive semantic segmentation. In ECCV, 2020.\n\nDeep domain generalization via conditional invariant adversarial networks. Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, Dacheng Tao, ECCV. Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generaliza- tion via conditional invariant adversarial networks. In ECCV, 2018.\n\nBidirectional learning for domain adaptation of semantic segmentation. Yunsheng Li, Lu Yuan, Nuno Vasconcelos, CVPR. Yunsheng Li, Lu Yuan, and Nuno Vasconcelos. Bidirectional learning for domain adaptation of semantic segmentation. In CVPR, 2019.\n\nConstructing self-motivated pyramid curriculums for crossdomain semantic segmentation: A non-adversarial approach. Qing Lian, Fengmao Lv, ICCV. Lixin Duan, and Boqing GongQing Lian, Fengmao Lv, Lixin Duan, and Boqing Gong. Constructing self-motivated pyramid curriculums for cross- domain semantic segmentation: A non-adversarial approach. In ICCV, 2019.\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, TPAMI. 394Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. TPAMI, 39(4):640-651, 2017.\n\nSignificance-aware information bottleneck for domain adaptive semantic segmentation. Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, Yi Yang, ICCV. Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, and Yi Yang. Significance-aware information bottleneck for domain adap- tive semantic segmentation. In ICCV, 2019.\n\nTaking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation. Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, Yi Yang, CVPR. Yawei Luo, Liang Zheng, Tao Guan, Junqing Yu, and Yi Yang. Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation. In CVPR, 2019.\n\nCross-domain semantic segmentation via domain-invariant interactive relation transfer. Fengmao Lv, Tao Liang, Xiang Chen, Guosheng Lin, CVPR. Fengmao Lv, Tao Liang, Xiang Chen, and Guosheng Lin. Cross-domain semantic segmentation via domain-invariant interactive relation transfer. In CVPR, 2020.\n\nInstance adaptive self-training for unsupervised domain adaptation. Ke Mei, Chuang Zhu, Jiaqi Zou, Shanghang Zhang, ECCV. Ke Mei, Chuang Zhu, Jiaqi Zou, and Shanghang Zhang. In- stance adaptive self-training for unsupervised domain adapta- tion. In ECCV, 2020.\n\nImproved knowledge distillation via teacher assistant. Mehrdad Seyed Iman Mirzadeh, Ang Farajtabar, Nir Li, Akihiro Levine, Hassan Matsukawa, Ghasemzadeh, AAAI. Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. Im- proved knowledge distillation via teacher assistant. In AAAI, 2020.\n\nThe mapillary vistas dataset for semantic understanding of street scenes. Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, Peter Kontschieder, Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In ICCV, 2017.\n\nUnsupervised multi-target domain adaptation through knowledge distillation. CoRR, abs. Madhu Le Thanh Nguyen-Meidine, Jose Kiran, Eric Dolz, Atif Granger, Louis-Antoine Bela, Blais-Morin, Le Thanh Nguyen-Meidine, Madhu Kiran, Jose Dolz, Eric Granger, Atif Bela, and Louis-Antoine Blais-Morin. Unsu- pervised multi-target domain adaptation through knowledge distillation. CoRR, abs/2007.07077, 2020.\n\nBoosting self-supervised learning via knowledge transfer. Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, Hamed Pirsiavash, CVPR. Mehdi Noroozi, Ananth Vinjimoor, Paolo Favaro, and Hamed Pirsiavash. Boosting self-supervised learning via knowledge transfer. In CVPR, 2018.\n\nSeokju Lee, and In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision. Fei Pan, Inkyu Shin, Francois Rameau, CVPR. Fei Pan, Inkyu Shin, Francois Rameau, Seokju Lee, and In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision. In CVPR, 2020.\n\nColor transfer between images. Erik Reinhard, Michael Adhikhmin, Bruce Gooch, Peter Shirley, IEEE Computer graphics and applications. 215Erik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. Color transfer between images. IEEE Computer graphics and applications, 21(5):34-41, 2001.\n\nPlaying for data: Ground truth from computer games. Vibhav Stephan R Richter, Stefan Vineet, Vladlen Roth, Koltun, ECCV. Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In ECCV, 2016.\n\nThe synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, Antonio M Lopez, CVPR. German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In CVPR, 2016.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, Karen Simonyan and Andrew Zisserman. Very deep convolu- tional networks for large-scale image recognition. 2015.\n\nLearning to adapt structured output space for semantic segmentation. Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Manmohan Chandraker, CVPR. Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learn- ing to adapt structured output space for semantic segmenta- tion. In CVPR, 2018.\n\nDomain adaptation for structured output via discriminative patch representations. Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, Manmohan Chandraker, ICCV. Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, and Manmo- han Chandraker. Domain adaptation for structured output via discriminative patch representations. In ICCV, 2019.\n\nIdd: A dataset for exploring problems of autonomous navigation in unconstrained environments. Girish Varma, Anbumani Subramanian, Anoop Namboodiri, Manmohan Chandraker, C V Jawahar, WACV. Girish Varma, Anbumani Subramanian, Anoop Namboodiri, Manmohan Chandraker, and CV Jawahar. Idd: A dataset for exploring problems of autonomous navigation in uncon- strained environments. In WACV, 2019.\n\nAdvent: Adversarial entropy minimization for domain adaptation in semantic segmentation. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, Patrick P\u00e9rez, CVPR. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P\u00e9rez. Advent: Adversarial entropy mini- mization for domain adaptation in semantic segmentation. In CVPR, 2019.\n\nDada: Depth-aware domain adaptation in semantic segmentation. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, Patrick P\u00e9rez, ICCV. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P\u00e9rez. Dada: Depth-aware domain adapta- tion in semantic segmentation. In ICCV, 2019.\n\nDifferential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation. Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-Mei Hwu, S Thomas, Honghui Huang, Shi, CVPR. Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-mei Hwu, Thomas S Huang, and Honghui Shi. Differential treatment for stuff and things: A simple unsuper- vised domain adaptation method for semantic segmentation. In CVPR, 2020.\n\nLabel-driven reconstruction for domain adaptation in semantic segmentation. Jinyu Yang, Weizhi An, Sheng Wang, Xinliang Zhu, Chaochao Yan, Junzhou Huang, ECCV. Jinyu Yang, Weizhi An, Sheng Wang, Xinliang Zhu, Chaochao Yan, and Junzhou Huang. Label-driven recon- struction for domain adaptation in semantic segmentation. In ECCV, 2020.\n\nFda: Fourier domain adaptation for semantic segmentation. Yanchao Yang, Stefano Soatto, CVPR. Yanchao Yang and Stefano Soatto. Fda: Fourier domain adaptation for semantic segmentation. In CVPR, 2020.\n\nContext prior for scene segmentation. Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, Nong Sang, CVPR. Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chun- hua Shen, and Nong Sang. Context prior for scene segmenta- tion. In CVPR, 2020.\n\nMulti-target unsupervised domain adaptation without exactly shared categories. Huanhuan Yu, Menglei Hu, Songcan Chen, abs/1809.00852CoRRHuanhuan Yu, Menglei Hu, and Songcan Chen. Multi-target unsupervised domain adaptation without exactly shared cate- gories. CoRR, abs/1809.00852, 2018.\n\nDomain randomization and pyramid consistency: Simulationto-real generalization without accessing target domain data. Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, Boqing Gong, ICCV. Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, and Boqing Gong. Domain randomization and pyramid consistency: Simulation- to-real generalization without accessing target domain data. In ICCV, 2019.\n\nGeneralizable semantic segmentation via model-agnostic learning and target-specific normalization. CoRR, abs. Jian Zhang, Lei Qi, Yinghuan Shi, Yang Gao, Jian Zhang, Lei Qi, Yinghuan Shi, and Yang Gao. Generaliz- able semantic segmentation via model-agnostic learning and target-specific normalization. CoRR, abs/2003.12296, 2020.\n\nA curriculum domain adaptation approach to the semantic segmentation of urban scenes. Yang Zhang, Philip David, Hassan Foroosh, Boqing Gong, TPAMI. 428Yang Zhang, Philip David, Hassan Foroosh, and Boqing Gong. A curriculum domain adaptation approach to the semantic segmentation of urban scenes. TPAMI, 42(8):1823-1841, 2020.\n\nDeep mutual learning. Ying Zhang, Tao Xiang, Timothy M Hospedales, Huchuan Lu, CVPR. Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In CVPR, 2018.\n\nMulti-source domain adaptation for semantic segmentation. Sicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua Chai, Kurt Keutzer, NeurIPS. Sicheng Zhao, Bo Li, Xiangyu Yue, Yang Gu, Pengfei Xu, Runbo Hu, Hua Chai, and Kurt Keutzer. Multi-source domain adaptation for semantic segmentation. In NeurIPS, 2019.\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017.\n\nAsymmetric non-local neural networks for semantic segmentation. Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, Xiang Bai, ICCV. Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xi- ang Bai. Asymmetric non-local neural networks for semantic segmentation. In ICCV, 2019.\n\nUnsupervised domain adaptation for semantic segmentation via class-balanced self-training. Yang Zou, Zhiding Yu, Jinsong Bvk Vijaya Kumar, Wang, ECCV. Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmen- tation via class-balanced self-training. In ECCV, 2018.\n", "annotations": {"author": "[{\"end\":184,\"start\":74},{\"end\":243,\"start\":185},{\"end\":319,\"start\":244},{\"end\":357,\"start\":320},{\"end\":470,\"start\":358},{\"end\":549,\"start\":471},{\"end\":595,\"start\":550},{\"end\":669,\"start\":596}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":82},{\"end\":191,\"start\":188},{\"end\":257,\"start\":253},{\"end\":332,\"start\":330},{\"end\":369,\"start\":366},{\"end\":485,\"start\":482},{\"end\":560,\"start\":558},{\"end\":609,\"start\":605}]", "author_first_name": "[{\"end\":81,\"start\":74},{\"end\":187,\"start\":185},{\"end\":252,\"start\":244},{\"end\":329,\"start\":320},{\"end\":365,\"start\":358},{\"end\":481,\"start\":471},{\"end\":557,\"start\":550},{\"end\":604,\"start\":596}]", "author_affiliation": "[{\"end\":146,\"start\":89},{\"end\":183,\"start\":148},{\"end\":242,\"start\":210},{\"end\":318,\"start\":283},{\"end\":406,\"start\":371},{\"end\":469,\"start\":408},{\"end\":548,\"start\":513},{\"end\":594,\"start\":562},{\"end\":668,\"start\":611}]", "title": "[{\"end\":71,\"start\":1},{\"end\":740,\"start\":670}]", "venue": null, "abstract": "[{\"end\":2997,\"start\":879}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3113,\"start\":3109},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3115,\"start\":3113},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3117,\"start\":3115},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3120,\"start\":3117},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":3123,\"start\":3120},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3327,\"start\":3323},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3330,\"start\":3327},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3333,\"start\":3330},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3828,\"start\":3824},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3831,\"start\":3828},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3834,\"start\":3831},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3837,\"start\":3834},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3839,\"start\":3837},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":3842,\"start\":3839},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3845,\"start\":3842},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3848,\"start\":3845},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4909,\"start\":4905},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4912,\"start\":4909},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4915,\"start\":4912},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5409,\"start\":5406},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9005,\"start\":9001},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9008,\"start\":9005},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9011,\"start\":9008},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9167,\"start\":9163},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9170,\"start\":9167},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9172,\"start\":9170},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9175,\"start\":9172},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9178,\"start\":9175},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9181,\"start\":9178},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9184,\"start\":9181},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9187,\"start\":9184},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9304,\"start\":9300},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9307,\"start\":9304},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9309,\"start\":9307},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9312,\"start\":9309},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9315,\"start\":9312},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9337,\"start\":9333},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9340,\"start\":9337},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":9486,\"start\":9482},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9489,\"start\":9486},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9532,\"start\":9528},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9567,\"start\":9563},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9611,\"start\":9607},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9678,\"start\":9674},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9681,\"start\":9678},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9684,\"start\":9681},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9687,\"start\":9684},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9690,\"start\":9687},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9693,\"start\":9690},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9696,\"start\":9693},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9721,\"start\":9717},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9724,\"start\":9721},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9726,\"start\":9724},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10144,\"start\":10140},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10146,\"start\":10144},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10149,\"start\":10146},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11019,\"start\":11015},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11022,\"start\":11019},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11025,\"start\":11022},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11157,\"start\":11153},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11271,\"start\":11267},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11711,\"start\":11707},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11714,\"start\":11711},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11716,\"start\":11714},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11719,\"start\":11716},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11722,\"start\":11719},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11725,\"start\":11722},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11745,\"start\":11741},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11894,\"start\":11890},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11988,\"start\":11984},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12564,\"start\":12560},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12792,\"start\":12788},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12795,\"start\":12792},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12798,\"start\":12795},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12801,\"start\":12798},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12804,\"start\":12801},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":12902,\"start\":12898},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15071,\"start\":15067},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15074,\"start\":15071},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15246,\"start\":15242},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15249,\"start\":15246},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21831,\"start\":21827},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21848,\"start\":21844},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21944,\"start\":21940},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21971,\"start\":21967},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21990,\"start\":21986},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23472,\"start\":23468},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24413,\"start\":24409},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24422,\"start\":24418},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24476,\"start\":24472},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24492,\"start\":24488},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24567,\"start\":24563},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24643,\"start\":24639},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24646,\"start\":24643},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24727,\"start\":24724},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24889,\"start\":24886},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24984,\"start\":24980},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25194,\"start\":25190},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25406,\"start\":25402},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":27370,\"start\":27366},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":27403,\"start\":27399},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28031,\"start\":28027}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31554,\"start\":31282},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31619,\"start\":31555},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32069,\"start\":31620},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32811,\"start\":32070},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33113,\"start\":32812},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33337,\"start\":33114}]", "paragraph": "[{\"end\":3966,\"start\":3013},{\"end\":6198,\"start\":3968},{\"end\":6766,\"start\":6200},{\"end\":7594,\"start\":6768},{\"end\":7942,\"start\":7596},{\"end\":7991,\"start\":7944},{\"end\":8140,\"start\":7993},{\"end\":8385,\"start\":8142},{\"end\":8633,\"start\":8387},{\"end\":10439,\"start\":8709},{\"end\":12291,\"start\":10441},{\"end\":13515,\"start\":12318},{\"end\":14517,\"start\":13542},{\"end\":14850,\"start\":14519},{\"end\":15618,\"start\":14898},{\"end\":16374,\"start\":15620},{\"end\":16802,\"start\":16426},{\"end\":17338,\"start\":16925},{\"end\":18515,\"start\":17340},{\"end\":18979,\"start\":18571},{\"end\":19082,\"start\":19026},{\"end\":19632,\"start\":19138},{\"end\":19844,\"start\":19683},{\"end\":20242,\"start\":19929},{\"end\":21036,\"start\":20328},{\"end\":21289,\"start\":21082},{\"end\":21450,\"start\":21332},{\"end\":21745,\"start\":21466},{\"end\":22250,\"start\":21758},{\"end\":23072,\"start\":22252},{\"end\":23402,\"start\":23074},{\"end\":23770,\"start\":23404},{\"end\":23947,\"start\":23772},{\"end\":24159,\"start\":23949},{\"end\":24377,\"start\":24161},{\"end\":25195,\"start\":24398},{\"end\":27502,\"start\":25231},{\"end\":28059,\"start\":27504},{\"end\":28276,\"start\":28078},{\"end\":29321,\"start\":28278},{\"end\":30177,\"start\":29362},{\"end\":30577,\"start\":30179},{\"end\":31281,\"start\":30592}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16425,\"start\":16375},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16924,\"start\":16803},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18570,\"start\":18516},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19025,\"start\":18980},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19682,\"start\":19633},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19928,\"start\":19845},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20327,\"start\":20243},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21081,\"start\":21037},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21331,\"start\":21290}]", "table_ref": "[{\"end\":22487,\"start\":22480},{\"end\":22967,\"start\":22960},{\"end\":25961,\"start\":25954},{\"end\":25973,\"start\":25966},{\"end\":27123,\"start\":27116},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28275,\"start\":28268},{\"end\":29670,\"start\":29663},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30208,\"start\":30201}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3011,\"start\":2999},{\"attributes\":{\"n\":\"2.\"},\"end\":8648,\"start\":8636},{\"attributes\":{\"n\":\"2.1.\"},\"end\":8707,\"start\":8651},{\"attributes\":{\"n\":\"2.2.\"},\"end\":12316,\"start\":12294},{\"attributes\":{\"n\":\"3.\"},\"end\":13529,\"start\":13518},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13540,\"start\":13532},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14896,\"start\":14853},{\"attributes\":{\"n\":\"3.3.\"},\"end\":19136,\"start\":19085},{\"attributes\":{\"n\":\"4.\"},\"end\":21464,\"start\":21453},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21756,\"start\":21748},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24396,\"start\":24380},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25229,\"start\":25198},{\"attributes\":{\"n\":\"4.5.\"},\"end\":28076,\"start\":28062},{\"attributes\":{\"n\":\"4.6.\"},\"end\":29360,\"start\":29324},{\"attributes\":{\"n\":\"5.\"},\"end\":30590,\"start\":30580},{\"end\":31293,\"start\":31283},{\"end\":31566,\"start\":31556},{\"end\":32822,\"start\":32813},{\"end\":33124,\"start\":33115}]", "table": "[{\"end\":32069,\"start\":31646},{\"end\":32811,\"start\":32095},{\"end\":33113,\"start\":32929},{\"end\":33337,\"start\":33168}]", "figure_caption": "[{\"end\":31554,\"start\":31295},{\"end\":31619,\"start\":31568},{\"end\":31646,\"start\":31622},{\"end\":32095,\"start\":32072},{\"end\":32929,\"start\":32824},{\"end\":33168,\"start\":33126}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4139,\"start\":4131},{\"end\":14457,\"start\":14449},{\"end\":15298,\"start\":15290},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27114,\"start\":27106}]", "bib_author_first_name": "[{\"end\":33401,\"start\":33395},{\"end\":33415,\"start\":33410},{\"end\":33438,\"start\":33434},{\"end\":33685,\"start\":33681},{\"end\":33964,\"start\":33957},{\"end\":33978,\"start\":33972},{\"end\":33994,\"start\":33985},{\"end\":34009,\"start\":34001},{\"end\":34280,\"start\":34274},{\"end\":34294,\"start\":34287},{\"end\":34307,\"start\":34300},{\"end\":34317,\"start\":34315},{\"end\":34331,\"start\":34324},{\"end\":34341,\"start\":34338},{\"end\":34357,\"start\":34349},{\"end\":34369,\"start\":34362},{\"end\":34700,\"start\":34689},{\"end\":34713,\"start\":34707},{\"end\":34733,\"start\":34726},{\"end\":34749,\"start\":34744},{\"end\":34762,\"start\":34758},{\"end\":34764,\"start\":34763},{\"end\":35089,\"start\":35078},{\"end\":35102,\"start\":35096},{\"end\":35122,\"start\":35115},{\"end\":35139,\"start\":35132},{\"end\":35416,\"start\":35411},{\"end\":35441,\"start\":35435},{\"end\":35454,\"start\":35447},{\"end\":35474,\"start\":35467},{\"end\":35756,\"start\":35749},{\"end\":35771,\"start\":35763},{\"end\":35781,\"start\":35777},{\"end\":36004,\"start\":35999},{\"end\":36014,\"start\":36011},{\"end\":36022,\"start\":36019},{\"end\":36285,\"start\":36279},{\"end\":36301,\"start\":36294},{\"end\":36318,\"start\":36309},{\"end\":36330,\"start\":36326},{\"end\":36346,\"start\":36340},{\"end\":36365,\"start\":36358},{\"end\":36658,\"start\":36655},{\"end\":36668,\"start\":36665},{\"end\":36682,\"start\":36675},{\"end\":36697,\"start\":36691},{\"end\":36705,\"start\":36702},{\"end\":36712,\"start\":36710},{\"end\":36942,\"start\":36940},{\"end\":36954,\"start\":36948},{\"end\":36985,\"start\":36973},{\"end\":37000,\"start\":36997},{\"end\":37219,\"start\":37212},{\"end\":37233,\"start\":37232},{\"end\":37250,\"start\":37243},{\"end\":37266,\"start\":37259},{\"end\":37283,\"start\":37278},{\"end\":37534,\"start\":37527},{\"end\":37557,\"start\":37551},{\"end\":37932,\"start\":37925},{\"end\":37944,\"start\":37937},{\"end\":37960,\"start\":37952},{\"end\":37970,\"start\":37966},{\"end\":38157,\"start\":38149},{\"end\":38169,\"start\":38163},{\"end\":38182,\"start\":38175},{\"end\":38194,\"start\":38188},{\"end\":38206,\"start\":38201},{\"end\":38216,\"start\":38213},{\"end\":38222,\"start\":38217},{\"end\":38439,\"start\":38431},{\"end\":38453,\"start\":38448},{\"end\":38467,\"start\":38463},{\"end\":38676,\"start\":38672},{\"end\":38692,\"start\":38686},{\"end\":38705,\"start\":38699},{\"end\":38716,\"start\":38710},{\"end\":38989,\"start\":38982},{\"end\":39004,\"start\":38997},{\"end\":39014,\"start\":39009},{\"end\":39029,\"start\":39021},{\"end\":39268,\"start\":39265},{\"end\":39281,\"start\":39276},{\"end\":39491,\"start\":39484},{\"end\":39501,\"start\":39499},{\"end\":39514,\"start\":39507},{\"end\":39528,\"start\":39519},{\"end\":39541,\"start\":39533},{\"end\":39550,\"start\":39548},{\"end\":39781,\"start\":39774},{\"end\":39798,\"start\":39789},{\"end\":39805,\"start\":39803},{\"end\":39818,\"start\":39811},{\"end\":39832,\"start\":39825},{\"end\":39851,\"start\":39843},{\"end\":39861,\"start\":39856},{\"end\":39874,\"start\":39866},{\"end\":39883,\"start\":39881},{\"end\":40150,\"start\":40143},{\"end\":40162,\"start\":40158},{\"end\":40176,\"start\":40168},{\"end\":40383,\"start\":40377},{\"end\":40398,\"start\":40390},{\"end\":40411,\"start\":40404},{\"end\":40596,\"start\":40590},{\"end\":40612,\"start\":40605},{\"end\":40625,\"start\":40619},{\"end\":40645,\"start\":40639},{\"end\":40647,\"start\":40646},{\"end\":40662,\"start\":40655},{\"end\":40917,\"start\":40908},{\"end\":40929,\"start\":40923},{\"end\":41126,\"start\":41125},{\"end\":41142,\"start\":41137},{\"end\":41369,\"start\":41362},{\"end\":41381,\"start\":41375},{\"end\":41628,\"start\":41620},{\"end\":41641,\"start\":41633},{\"end\":41650,\"start\":41648},{\"end\":41663,\"start\":41656},{\"end\":41671,\"start\":41669},{\"end\":41910,\"start\":41908},{\"end\":41921,\"start\":41915},{\"end\":41936,\"start\":41928},{\"end\":41949,\"start\":41943},{\"end\":41964,\"start\":41955},{\"end\":41973,\"start\":41970},{\"end\":41988,\"start\":41981},{\"end\":42262,\"start\":42254},{\"end\":42269,\"start\":42267},{\"end\":42280,\"start\":42276},{\"end\":42550,\"start\":42546},{\"end\":42564,\"start\":42557},{\"end\":42851,\"start\":42843},{\"end\":42862,\"start\":42858},{\"end\":42880,\"start\":42874},{\"end\":43126,\"start\":43121},{\"end\":43136,\"start\":43132},{\"end\":43145,\"start\":43142},{\"end\":43159,\"start\":43152},{\"end\":43166,\"start\":43164},{\"end\":43452,\"start\":43447},{\"end\":43463,\"start\":43458},{\"end\":43474,\"start\":43471},{\"end\":43488,\"start\":43481},{\"end\":43495,\"start\":43493},{\"end\":43786,\"start\":43779},{\"end\":43794,\"start\":43791},{\"end\":43807,\"start\":43802},{\"end\":43822,\"start\":43814},{\"end\":44060,\"start\":44058},{\"end\":44072,\"start\":44066},{\"end\":44083,\"start\":44078},{\"end\":44098,\"start\":44089},{\"end\":44314,\"start\":44307},{\"end\":44339,\"start\":44336},{\"end\":44355,\"start\":44352},{\"end\":44367,\"start\":44360},{\"end\":44382,\"start\":44376},{\"end\":44671,\"start\":44664},{\"end\":44687,\"start\":44681},{\"end\":44703,\"start\":44697},{\"end\":44708,\"start\":44704},{\"end\":44720,\"start\":44715},{\"end\":44992,\"start\":44987},{\"end\":45022,\"start\":45018},{\"end\":45034,\"start\":45030},{\"end\":45045,\"start\":45041},{\"end\":45068,\"start\":45055},{\"end\":45363,\"start\":45358},{\"end\":45379,\"start\":45373},{\"end\":45396,\"start\":45391},{\"end\":45410,\"start\":45405},{\"end\":45693,\"start\":45690},{\"end\":45704,\"start\":45699},{\"end\":45719,\"start\":45711},{\"end\":45941,\"start\":45937},{\"end\":45959,\"start\":45952},{\"end\":45976,\"start\":45971},{\"end\":45989,\"start\":45984},{\"end\":46259,\"start\":46253},{\"end\":46285,\"start\":46279},{\"end\":46301,\"start\":46294},{\"end\":46566,\"start\":46560},{\"end\":46577,\"start\":46572},{\"end\":46593,\"start\":46587},{\"end\":46612,\"start\":46607},{\"end\":46631,\"start\":46622},{\"end\":46920,\"start\":46915},{\"end\":46937,\"start\":46931},{\"end\":47140,\"start\":47132},{\"end\":47155,\"start\":47147},{\"end\":47168,\"start\":47162},{\"end\":47185,\"start\":47179},{\"end\":47202,\"start\":47192},{\"end\":47217,\"start\":47209},{\"end\":47517,\"start\":47509},{\"end\":47530,\"start\":47524},{\"end\":47543,\"start\":47537},{\"end\":47562,\"start\":47554},{\"end\":47851,\"start\":47845},{\"end\":47867,\"start\":47859},{\"end\":47886,\"start\":47881},{\"end\":47907,\"start\":47899},{\"end\":47921,\"start\":47920},{\"end\":47923,\"start\":47922},{\"end\":48240,\"start\":48231},{\"end\":48253,\"start\":48245},{\"end\":48266,\"start\":48260},{\"end\":48283,\"start\":48275},{\"end\":48297,\"start\":48290},{\"end\":48567,\"start\":48558},{\"end\":48580,\"start\":48572},{\"end\":48593,\"start\":48587},{\"end\":48610,\"start\":48602},{\"end\":48624,\"start\":48617},{\"end\":48923,\"start\":48915},{\"end\":48932,\"start\":48930},{\"end\":48944,\"start\":48937},{\"end\":48957,\"start\":48950},{\"end\":48971,\"start\":48965},{\"end\":48986,\"start\":48979},{\"end\":48993,\"start\":48992},{\"end\":49009,\"start\":49002},{\"end\":49356,\"start\":49351},{\"end\":49369,\"start\":49363},{\"end\":49379,\"start\":49374},{\"end\":49394,\"start\":49386},{\"end\":49408,\"start\":49400},{\"end\":49421,\"start\":49414},{\"end\":49676,\"start\":49669},{\"end\":49690,\"start\":49683},{\"end\":49859,\"start\":49850},{\"end\":49870,\"start\":49864},{\"end\":49885,\"start\":49877},{\"end\":49895,\"start\":49891},{\"end\":49907,\"start\":49900},{\"end\":49918,\"start\":49914},{\"end\":50155,\"start\":50147},{\"end\":50167,\"start\":50160},{\"end\":50179,\"start\":50172},{\"end\":50481,\"start\":50474},{\"end\":50491,\"start\":50487},{\"end\":50506,\"start\":50499},{\"end\":50520,\"start\":50513},{\"end\":50550,\"start\":50546},{\"end\":50566,\"start\":50560},{\"end\":50931,\"start\":50927},{\"end\":50942,\"start\":50939},{\"end\":50955,\"start\":50947},{\"end\":50965,\"start\":50961},{\"end\":51239,\"start\":51235},{\"end\":51253,\"start\":51247},{\"end\":51267,\"start\":51261},{\"end\":51283,\"start\":51277},{\"end\":51502,\"start\":51498},{\"end\":51513,\"start\":51510},{\"end\":51528,\"start\":51521},{\"end\":51530,\"start\":51529},{\"end\":51550,\"start\":51543},{\"end\":51725,\"start\":51718},{\"end\":51734,\"start\":51732},{\"end\":51746,\"start\":51739},{\"end\":51756,\"start\":51752},{\"end\":51768,\"start\":51761},{\"end\":51778,\"start\":51773},{\"end\":51786,\"start\":51783},{\"end\":51797,\"start\":51793},{\"end\":52074,\"start\":52067},{\"end\":52087,\"start\":52080},{\"end\":52101,\"start\":52094},{\"end\":52115,\"start\":52109},{\"end\":52117,\"start\":52116},{\"end\":52352,\"start\":52348},{\"end\":52364,\"start\":52358},{\"end\":52373,\"start\":52369},{\"end\":52387,\"start\":52379},{\"end\":52400,\"start\":52395},{\"end\":52651,\"start\":52647},{\"end\":52664,\"start\":52657},{\"end\":52676,\"start\":52669}]", "bib_author_last_name": "[{\"end\":33408,\"start\":33402},{\"end\":33432,\"start\":33416},{\"end\":33448,\"start\":33439},{\"end\":33457,\"start\":33450},{\"end\":33692,\"start\":33686},{\"end\":33970,\"start\":33965},{\"end\":33983,\"start\":33979},{\"end\":33999,\"start\":33995},{\"end\":34014,\"start\":34010},{\"end\":34285,\"start\":34281},{\"end\":34298,\"start\":34295},{\"end\":34313,\"start\":34308},{\"end\":34322,\"start\":34318},{\"end\":34336,\"start\":34332},{\"end\":34347,\"start\":34342},{\"end\":34360,\"start\":34358},{\"end\":34375,\"start\":34370},{\"end\":34705,\"start\":34701},{\"end\":34724,\"start\":34714},{\"end\":34742,\"start\":34734},{\"end\":34756,\"start\":34750},{\"end\":34771,\"start\":34765},{\"end\":35094,\"start\":35090},{\"end\":35113,\"start\":35103},{\"end\":35130,\"start\":35123},{\"end\":35144,\"start\":35140},{\"end\":35433,\"start\":35417},{\"end\":35445,\"start\":35442},{\"end\":35465,\"start\":35455},{\"end\":35482,\"start\":35475},{\"end\":35488,\"start\":35484},{\"end\":35761,\"start\":35757},{\"end\":35775,\"start\":35772},{\"end\":35785,\"start\":35782},{\"end\":36009,\"start\":36005},{\"end\":36017,\"start\":36015},{\"end\":36031,\"start\":36023},{\"end\":36292,\"start\":36286},{\"end\":36307,\"start\":36302},{\"end\":36324,\"start\":36319},{\"end\":36338,\"start\":36331},{\"end\":36356,\"start\":36347},{\"end\":36374,\"start\":36366},{\"end\":36663,\"start\":36659},{\"end\":36673,\"start\":36669},{\"end\":36689,\"start\":36683},{\"end\":36700,\"start\":36698},{\"end\":36708,\"start\":36706},{\"end\":36720,\"start\":36713},{\"end\":36946,\"start\":36943},{\"end\":36971,\"start\":36955},{\"end\":36995,\"start\":36986},{\"end\":37008,\"start\":37001},{\"end\":37230,\"start\":37220},{\"end\":37241,\"start\":37234},{\"end\":37257,\"start\":37251},{\"end\":37276,\"start\":37267},{\"end\":37288,\"start\":37284},{\"end\":37300,\"start\":37290},{\"end\":37549,\"start\":37535},{\"end\":37562,\"start\":37558},{\"end\":37571,\"start\":37564},{\"end\":37935,\"start\":37933},{\"end\":37950,\"start\":37945},{\"end\":37964,\"start\":37961},{\"end\":37974,\"start\":37971},{\"end\":38161,\"start\":38158},{\"end\":38173,\"start\":38170},{\"end\":38186,\"start\":38183},{\"end\":38199,\"start\":38195},{\"end\":38211,\"start\":38207},{\"end\":38227,\"start\":38223},{\"end\":38446,\"start\":38440},{\"end\":38461,\"start\":38454},{\"end\":38472,\"start\":38468},{\"end\":38684,\"start\":38677},{\"end\":38697,\"start\":38693},{\"end\":38708,\"start\":38706},{\"end\":38724,\"start\":38717},{\"end\":38995,\"start\":38990},{\"end\":39007,\"start\":39005},{\"end\":39019,\"start\":39015},{\"end\":39035,\"start\":39030},{\"end\":39274,\"start\":39269},{\"end\":39290,\"start\":39282},{\"end\":39497,\"start\":39492},{\"end\":39505,\"start\":39502},{\"end\":39517,\"start\":39515},{\"end\":39531,\"start\":39529},{\"end\":39546,\"start\":39542},{\"end\":39555,\"start\":39551},{\"end\":39787,\"start\":39782},{\"end\":39801,\"start\":39799},{\"end\":39809,\"start\":39806},{\"end\":39823,\"start\":39819},{\"end\":39841,\"start\":39833},{\"end\":39854,\"start\":39852},{\"end\":39864,\"start\":39862},{\"end\":39879,\"start\":39875},{\"end\":39888,\"start\":39884},{\"end\":40156,\"start\":40151},{\"end\":40166,\"start\":40163},{\"end\":40181,\"start\":40177},{\"end\":40388,\"start\":40384},{\"end\":40402,\"start\":40399},{\"end\":40415,\"start\":40412},{\"end\":40603,\"start\":40597},{\"end\":40617,\"start\":40613},{\"end\":40637,\"start\":40626},{\"end\":40653,\"start\":40648},{\"end\":40671,\"start\":40663},{\"end\":40921,\"start\":40918},{\"end\":40934,\"start\":40930},{\"end\":41135,\"start\":41127},{\"end\":41149,\"start\":41143},{\"end\":41153,\"start\":41151},{\"end\":41373,\"start\":41370},{\"end\":41387,\"start\":41382},{\"end\":41631,\"start\":41629},{\"end\":41646,\"start\":41642},{\"end\":41654,\"start\":41651},{\"end\":41667,\"start\":41664},{\"end\":41676,\"start\":41672},{\"end\":41913,\"start\":41911},{\"end\":41926,\"start\":41922},{\"end\":41941,\"start\":41937},{\"end\":41953,\"start\":41950},{\"end\":41968,\"start\":41965},{\"end\":41979,\"start\":41974},{\"end\":41992,\"start\":41989},{\"end\":42265,\"start\":42263},{\"end\":42274,\"start\":42270},{\"end\":42292,\"start\":42281},{\"end\":42555,\"start\":42551},{\"end\":42567,\"start\":42565},{\"end\":42856,\"start\":42852},{\"end\":42872,\"start\":42863},{\"end\":42888,\"start\":42881},{\"end\":43130,\"start\":43127},{\"end\":43140,\"start\":43137},{\"end\":43150,\"start\":43146},{\"end\":43162,\"start\":43160},{\"end\":43171,\"start\":43167},{\"end\":43456,\"start\":43453},{\"end\":43469,\"start\":43464},{\"end\":43479,\"start\":43475},{\"end\":43491,\"start\":43489},{\"end\":43500,\"start\":43496},{\"end\":43789,\"start\":43787},{\"end\":43800,\"start\":43795},{\"end\":43812,\"start\":43808},{\"end\":43826,\"start\":43823},{\"end\":44064,\"start\":44061},{\"end\":44076,\"start\":44073},{\"end\":44087,\"start\":44084},{\"end\":44104,\"start\":44099},{\"end\":44334,\"start\":44315},{\"end\":44350,\"start\":44340},{\"end\":44358,\"start\":44356},{\"end\":44374,\"start\":44368},{\"end\":44392,\"start\":44383},{\"end\":44405,\"start\":44394},{\"end\":44679,\"start\":44672},{\"end\":44695,\"start\":44688},{\"end\":44713,\"start\":44709},{\"end\":44733,\"start\":44721},{\"end\":45016,\"start\":44993},{\"end\":45028,\"start\":45023},{\"end\":45039,\"start\":45035},{\"end\":45053,\"start\":45046},{\"end\":45073,\"start\":45069},{\"end\":45086,\"start\":45075},{\"end\":45371,\"start\":45364},{\"end\":45389,\"start\":45380},{\"end\":45403,\"start\":45397},{\"end\":45421,\"start\":45411},{\"end\":45697,\"start\":45694},{\"end\":45709,\"start\":45705},{\"end\":45726,\"start\":45720},{\"end\":45950,\"start\":45942},{\"end\":45969,\"start\":45960},{\"end\":45982,\"start\":45977},{\"end\":45997,\"start\":45990},{\"end\":46277,\"start\":46260},{\"end\":46292,\"start\":46286},{\"end\":46306,\"start\":46302},{\"end\":46314,\"start\":46308},{\"end\":46570,\"start\":46567},{\"end\":46585,\"start\":46578},{\"end\":46605,\"start\":46594},{\"end\":46620,\"start\":46613},{\"end\":46637,\"start\":46632},{\"end\":46929,\"start\":46921},{\"end\":46947,\"start\":46938},{\"end\":47145,\"start\":47141},{\"end\":47160,\"start\":47156},{\"end\":47177,\"start\":47169},{\"end\":47190,\"start\":47186},{\"end\":47207,\"start\":47203},{\"end\":47228,\"start\":47218},{\"end\":47522,\"start\":47518},{\"end\":47535,\"start\":47531},{\"end\":47552,\"start\":47544},{\"end\":47573,\"start\":47563},{\"end\":47857,\"start\":47852},{\"end\":47879,\"start\":47868},{\"end\":47897,\"start\":47887},{\"end\":47918,\"start\":47908},{\"end\":47931,\"start\":47924},{\"end\":48243,\"start\":48241},{\"end\":48258,\"start\":48254},{\"end\":48273,\"start\":48267},{\"end\":48288,\"start\":48284},{\"end\":48303,\"start\":48298},{\"end\":48570,\"start\":48568},{\"end\":48585,\"start\":48581},{\"end\":48600,\"start\":48594},{\"end\":48615,\"start\":48611},{\"end\":48630,\"start\":48625},{\"end\":48928,\"start\":48924},{\"end\":48935,\"start\":48933},{\"end\":48948,\"start\":48945},{\"end\":48963,\"start\":48958},{\"end\":48977,\"start\":48972},{\"end\":48990,\"start\":48987},{\"end\":49000,\"start\":48994},{\"end\":49015,\"start\":49010},{\"end\":49020,\"start\":49017},{\"end\":49361,\"start\":49357},{\"end\":49372,\"start\":49370},{\"end\":49384,\"start\":49380},{\"end\":49398,\"start\":49395},{\"end\":49412,\"start\":49409},{\"end\":49427,\"start\":49422},{\"end\":49681,\"start\":49677},{\"end\":49697,\"start\":49691},{\"end\":49862,\"start\":49860},{\"end\":49875,\"start\":49871},{\"end\":49889,\"start\":49886},{\"end\":49898,\"start\":49896},{\"end\":49912,\"start\":49908},{\"end\":49923,\"start\":49919},{\"end\":50158,\"start\":50156},{\"end\":50170,\"start\":50168},{\"end\":50184,\"start\":50180},{\"end\":50485,\"start\":50482},{\"end\":50497,\"start\":50492},{\"end\":50511,\"start\":50507},{\"end\":50544,\"start\":50521},{\"end\":50558,\"start\":50551},{\"end\":50571,\"start\":50567},{\"end\":50937,\"start\":50932},{\"end\":50945,\"start\":50943},{\"end\":50959,\"start\":50956},{\"end\":50969,\"start\":50966},{\"end\":51245,\"start\":51240},{\"end\":51259,\"start\":51254},{\"end\":51275,\"start\":51268},{\"end\":51288,\"start\":51284},{\"end\":51508,\"start\":51503},{\"end\":51519,\"start\":51514},{\"end\":51541,\"start\":51531},{\"end\":51553,\"start\":51551},{\"end\":51730,\"start\":51726},{\"end\":51737,\"start\":51735},{\"end\":51750,\"start\":51747},{\"end\":51759,\"start\":51757},{\"end\":51771,\"start\":51769},{\"end\":51781,\"start\":51779},{\"end\":51791,\"start\":51787},{\"end\":51805,\"start\":51798},{\"end\":52078,\"start\":52075},{\"end\":52092,\"start\":52088},{\"end\":52107,\"start\":52102},{\"end\":52123,\"start\":52118},{\"end\":52356,\"start\":52353},{\"end\":52367,\"start\":52365},{\"end\":52377,\"start\":52374},{\"end\":52393,\"start\":52388},{\"end\":52404,\"start\":52401},{\"end\":52655,\"start\":52652},{\"end\":52667,\"start\":52665},{\"end\":52693,\"start\":52677},{\"end\":52699,\"start\":52695}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":33616,\"start\":33339},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":115963355},\"end\":33851,\"start\":33618},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":88522535},\"end\":34206,\"start\":33853},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":53763041},\"end\":34574,\"start\":34208},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3429309},\"end\":35013,\"start\":34576},{\"attributes\":{\"doi\":\"abs/1706.05587\",\"id\":\"b5\"},\"end\":35326,\"start\":35015},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3638670},\"end\":35676,\"start\":35328},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":203593505},\"end\":35920,\"start\":35678},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4715607},\"end\":36170,\"start\":35922},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":502946},\"end\":36600,\"start\":36172},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":57246310},\"end\":36866,\"start\":36602},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":202768984},\"end\":37182,\"start\":36868},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4110009},\"end\":37446,\"start\":37184},{\"attributes\":{\"id\":\"b13\"},\"end\":37877,\"start\":37448},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":38097,\"start\":37879},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":102483181},\"end\":38383,\"start\":38099},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":7200347},\"end\":38595,\"start\":38385},{\"attributes\":{\"doi\":\"abs/1612.02649\",\"id\":\"b17\"},\"end\":38904,\"start\":38597},{\"attributes\":{\"id\":\"b18\"},\"end\":39187,\"start\":38906},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6576859},\"end\":39418,\"start\":39189},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":220936542},\"end\":39718,\"start\":39420},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":219965026},\"end\":40084,\"start\":39720},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":221112213},\"end\":40304,\"start\":40086},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":208513309},\"end\":40552,\"start\":40306},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":9286850},\"end\":40816,\"start\":40554},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":211677777},\"end\":41079,\"start\":40818},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6628106},\"end\":41251,\"start\":41081},{\"attributes\":{\"id\":\"b27\"},\"end\":41547,\"start\":41253},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":226840200},\"end\":41831,\"start\":41549},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":52956008},\"end\":42181,\"start\":41833},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":129944996},\"end\":42429,\"start\":42183},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":201665971},\"end\":42785,\"start\":42431},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1629541},\"end\":43034,\"start\":42787},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":90262749},\"end\":43336,\"start\":43036},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52825087},\"end\":43690,\"start\":43338},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":219630886},\"end\":43988,\"start\":43692},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":221340970},\"end\":44250,\"start\":43990},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":212908749},\"end\":44588,\"start\":44252},{\"attributes\":{\"id\":\"b38\"},\"end\":44898,\"start\":44590},{\"attributes\":{\"id\":\"b39\"},\"end\":45298,\"start\":44900},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4525685},\"end\":45570,\"start\":45300},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":215786096},\"end\":45904,\"start\":45572},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":14088925},\"end\":46199,\"start\":45906},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":5844139},\"end\":46455,\"start\":46201},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206594095},\"end\":46845,\"start\":46457},{\"attributes\":{\"id\":\"b45\"},\"end\":47061,\"start\":46847},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":3556146},\"end\":47425,\"start\":47063},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":58014164},\"end\":47749,\"start\":47427},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":53759579},\"end\":48140,\"start\":47751},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":54216961},\"end\":48494,\"start\":48142},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":102354910},\"end\":48794,\"start\":48496},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":212747800},\"end\":49273,\"start\":48796},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":212644555},\"end\":49609,\"start\":49275},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":215745272},\"end\":49810,\"start\":49611},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":214794959},\"end\":50066,\"start\":49812},{\"attributes\":{\"doi\":\"abs/1809.00852\",\"id\":\"b55\"},\"end\":50355,\"start\":50068},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":202540251},\"end\":50815,\"start\":50357},{\"attributes\":{\"id\":\"b57\"},\"end\":51147,\"start\":50817},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":57189579},\"end\":51474,\"start\":51149},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":26071966},\"end\":51658,\"start\":51476},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":202766173},\"end\":51984,\"start\":51660},{\"attributes\":{\"id\":\"b61\"},\"end\":52282,\"start\":51986},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":201127121},\"end\":52554,\"start\":52284},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":52954862},\"end\":52872,\"start\":52556}]", "bib_title": "[{\"end\":33393,\"start\":33339},{\"end\":33679,\"start\":33618},{\"end\":33955,\"start\":33853},{\"end\":34272,\"start\":34208},{\"end\":34687,\"start\":34576},{\"end\":35409,\"start\":35328},{\"end\":35747,\"start\":35678},{\"end\":35997,\"start\":35922},{\"end\":36277,\"start\":36172},{\"end\":36653,\"start\":36602},{\"end\":36938,\"start\":36868},{\"end\":37210,\"start\":37184},{\"end\":37525,\"start\":37448},{\"end\":37923,\"start\":37879},{\"end\":38147,\"start\":38099},{\"end\":38429,\"start\":38385},{\"end\":39263,\"start\":39189},{\"end\":39482,\"start\":39420},{\"end\":39772,\"start\":39720},{\"end\":40141,\"start\":40086},{\"end\":40375,\"start\":40306},{\"end\":40588,\"start\":40554},{\"end\":40906,\"start\":40818},{\"end\":41123,\"start\":41081},{\"end\":41360,\"start\":41253},{\"end\":41618,\"start\":41549},{\"end\":41906,\"start\":41833},{\"end\":42252,\"start\":42183},{\"end\":42544,\"start\":42431},{\"end\":42841,\"start\":42787},{\"end\":43119,\"start\":43036},{\"end\":43445,\"start\":43338},{\"end\":43777,\"start\":43692},{\"end\":44056,\"start\":43990},{\"end\":44305,\"start\":44252},{\"end\":45356,\"start\":45300},{\"end\":45688,\"start\":45572},{\"end\":45935,\"start\":45906},{\"end\":46251,\"start\":46201},{\"end\":46558,\"start\":46457},{\"end\":47130,\"start\":47063},{\"end\":47507,\"start\":47427},{\"end\":47843,\"start\":47751},{\"end\":48229,\"start\":48142},{\"end\":48556,\"start\":48496},{\"end\":48913,\"start\":48796},{\"end\":49349,\"start\":49275},{\"end\":49667,\"start\":49611},{\"end\":49848,\"start\":49812},{\"end\":50472,\"start\":50357},{\"end\":51233,\"start\":51149},{\"end\":51496,\"start\":51476},{\"end\":51716,\"start\":51660},{\"end\":52346,\"start\":52284},{\"end\":52645,\"start\":52556}]", "bib_author": "[{\"end\":33410,\"start\":33395},{\"end\":33434,\"start\":33410},{\"end\":33450,\"start\":33434},{\"end\":33459,\"start\":33450},{\"end\":33694,\"start\":33681},{\"end\":33972,\"start\":33957},{\"end\":33985,\"start\":33972},{\"end\":34001,\"start\":33985},{\"end\":34016,\"start\":34001},{\"end\":34287,\"start\":34274},{\"end\":34300,\"start\":34287},{\"end\":34315,\"start\":34300},{\"end\":34324,\"start\":34315},{\"end\":34338,\"start\":34324},{\"end\":34349,\"start\":34338},{\"end\":34362,\"start\":34349},{\"end\":34377,\"start\":34362},{\"end\":34707,\"start\":34689},{\"end\":34726,\"start\":34707},{\"end\":34744,\"start\":34726},{\"end\":34758,\"start\":34744},{\"end\":34773,\"start\":34758},{\"end\":35096,\"start\":35078},{\"end\":35115,\"start\":35096},{\"end\":35132,\"start\":35115},{\"end\":35146,\"start\":35132},{\"end\":35435,\"start\":35411},{\"end\":35447,\"start\":35435},{\"end\":35467,\"start\":35447},{\"end\":35484,\"start\":35467},{\"end\":35490,\"start\":35484},{\"end\":35763,\"start\":35749},{\"end\":35777,\"start\":35763},{\"end\":35787,\"start\":35777},{\"end\":36011,\"start\":35999},{\"end\":36019,\"start\":36011},{\"end\":36033,\"start\":36019},{\"end\":36294,\"start\":36279},{\"end\":36309,\"start\":36294},{\"end\":36326,\"start\":36309},{\"end\":36340,\"start\":36326},{\"end\":36358,\"start\":36340},{\"end\":36376,\"start\":36358},{\"end\":36665,\"start\":36655},{\"end\":36675,\"start\":36665},{\"end\":36691,\"start\":36675},{\"end\":36702,\"start\":36691},{\"end\":36710,\"start\":36702},{\"end\":36722,\"start\":36710},{\"end\":36948,\"start\":36940},{\"end\":36973,\"start\":36948},{\"end\":36997,\"start\":36973},{\"end\":37010,\"start\":36997},{\"end\":37232,\"start\":37212},{\"end\":37243,\"start\":37232},{\"end\":37259,\"start\":37243},{\"end\":37278,\"start\":37259},{\"end\":37290,\"start\":37278},{\"end\":37302,\"start\":37290},{\"end\":37551,\"start\":37527},{\"end\":37564,\"start\":37551},{\"end\":37573,\"start\":37564},{\"end\":37937,\"start\":37925},{\"end\":37952,\"start\":37937},{\"end\":37966,\"start\":37952},{\"end\":37976,\"start\":37966},{\"end\":38163,\"start\":38149},{\"end\":38175,\"start\":38163},{\"end\":38188,\"start\":38175},{\"end\":38201,\"start\":38188},{\"end\":38213,\"start\":38201},{\"end\":38229,\"start\":38213},{\"end\":38448,\"start\":38431},{\"end\":38463,\"start\":38448},{\"end\":38474,\"start\":38463},{\"end\":38686,\"start\":38672},{\"end\":38699,\"start\":38686},{\"end\":38710,\"start\":38699},{\"end\":38726,\"start\":38710},{\"end\":38997,\"start\":38982},{\"end\":39009,\"start\":38997},{\"end\":39021,\"start\":39009},{\"end\":39037,\"start\":39021},{\"end\":39276,\"start\":39265},{\"end\":39292,\"start\":39276},{\"end\":39499,\"start\":39484},{\"end\":39507,\"start\":39499},{\"end\":39519,\"start\":39507},{\"end\":39533,\"start\":39519},{\"end\":39548,\"start\":39533},{\"end\":39557,\"start\":39548},{\"end\":39789,\"start\":39774},{\"end\":39803,\"start\":39789},{\"end\":39811,\"start\":39803},{\"end\":39825,\"start\":39811},{\"end\":39843,\"start\":39825},{\"end\":39856,\"start\":39843},{\"end\":39866,\"start\":39856},{\"end\":39881,\"start\":39866},{\"end\":39890,\"start\":39881},{\"end\":40158,\"start\":40143},{\"end\":40168,\"start\":40158},{\"end\":40183,\"start\":40168},{\"end\":40390,\"start\":40377},{\"end\":40404,\"start\":40390},{\"end\":40417,\"start\":40404},{\"end\":40605,\"start\":40590},{\"end\":40619,\"start\":40605},{\"end\":40639,\"start\":40619},{\"end\":40655,\"start\":40639},{\"end\":40673,\"start\":40655},{\"end\":40923,\"start\":40908},{\"end\":40936,\"start\":40923},{\"end\":41137,\"start\":41125},{\"end\":41151,\"start\":41137},{\"end\":41155,\"start\":41151},{\"end\":41375,\"start\":41362},{\"end\":41389,\"start\":41375},{\"end\":41633,\"start\":41620},{\"end\":41648,\"start\":41633},{\"end\":41656,\"start\":41648},{\"end\":41669,\"start\":41656},{\"end\":41678,\"start\":41669},{\"end\":41915,\"start\":41908},{\"end\":41928,\"start\":41915},{\"end\":41943,\"start\":41928},{\"end\":41955,\"start\":41943},{\"end\":41970,\"start\":41955},{\"end\":41981,\"start\":41970},{\"end\":41994,\"start\":41981},{\"end\":42267,\"start\":42254},{\"end\":42276,\"start\":42267},{\"end\":42294,\"start\":42276},{\"end\":42557,\"start\":42546},{\"end\":42569,\"start\":42557},{\"end\":42858,\"start\":42843},{\"end\":42874,\"start\":42858},{\"end\":42890,\"start\":42874},{\"end\":43132,\"start\":43121},{\"end\":43142,\"start\":43132},{\"end\":43152,\"start\":43142},{\"end\":43164,\"start\":43152},{\"end\":43173,\"start\":43164},{\"end\":43458,\"start\":43447},{\"end\":43471,\"start\":43458},{\"end\":43481,\"start\":43471},{\"end\":43493,\"start\":43481},{\"end\":43502,\"start\":43493},{\"end\":43791,\"start\":43779},{\"end\":43802,\"start\":43791},{\"end\":43814,\"start\":43802},{\"end\":43828,\"start\":43814},{\"end\":44066,\"start\":44058},{\"end\":44078,\"start\":44066},{\"end\":44089,\"start\":44078},{\"end\":44106,\"start\":44089},{\"end\":44336,\"start\":44307},{\"end\":44352,\"start\":44336},{\"end\":44360,\"start\":44352},{\"end\":44376,\"start\":44360},{\"end\":44394,\"start\":44376},{\"end\":44407,\"start\":44394},{\"end\":44681,\"start\":44664},{\"end\":44697,\"start\":44681},{\"end\":44715,\"start\":44697},{\"end\":44735,\"start\":44715},{\"end\":45018,\"start\":44987},{\"end\":45030,\"start\":45018},{\"end\":45041,\"start\":45030},{\"end\":45055,\"start\":45041},{\"end\":45075,\"start\":45055},{\"end\":45088,\"start\":45075},{\"end\":45373,\"start\":45358},{\"end\":45391,\"start\":45373},{\"end\":45405,\"start\":45391},{\"end\":45423,\"start\":45405},{\"end\":45699,\"start\":45690},{\"end\":45711,\"start\":45699},{\"end\":45728,\"start\":45711},{\"end\":45952,\"start\":45937},{\"end\":45971,\"start\":45952},{\"end\":45984,\"start\":45971},{\"end\":45999,\"start\":45984},{\"end\":46279,\"start\":46253},{\"end\":46294,\"start\":46279},{\"end\":46308,\"start\":46294},{\"end\":46316,\"start\":46308},{\"end\":46572,\"start\":46560},{\"end\":46587,\"start\":46572},{\"end\":46607,\"start\":46587},{\"end\":46622,\"start\":46607},{\"end\":46639,\"start\":46622},{\"end\":46931,\"start\":46915},{\"end\":46949,\"start\":46931},{\"end\":47147,\"start\":47132},{\"end\":47162,\"start\":47147},{\"end\":47179,\"start\":47162},{\"end\":47192,\"start\":47179},{\"end\":47209,\"start\":47192},{\"end\":47230,\"start\":47209},{\"end\":47524,\"start\":47509},{\"end\":47537,\"start\":47524},{\"end\":47554,\"start\":47537},{\"end\":47575,\"start\":47554},{\"end\":47859,\"start\":47845},{\"end\":47881,\"start\":47859},{\"end\":47899,\"start\":47881},{\"end\":47920,\"start\":47899},{\"end\":47933,\"start\":47920},{\"end\":48245,\"start\":48231},{\"end\":48260,\"start\":48245},{\"end\":48275,\"start\":48260},{\"end\":48290,\"start\":48275},{\"end\":48305,\"start\":48290},{\"end\":48572,\"start\":48558},{\"end\":48587,\"start\":48572},{\"end\":48602,\"start\":48587},{\"end\":48617,\"start\":48602},{\"end\":48632,\"start\":48617},{\"end\":48930,\"start\":48915},{\"end\":48937,\"start\":48930},{\"end\":48950,\"start\":48937},{\"end\":48965,\"start\":48950},{\"end\":48979,\"start\":48965},{\"end\":48992,\"start\":48979},{\"end\":49002,\"start\":48992},{\"end\":49017,\"start\":49002},{\"end\":49022,\"start\":49017},{\"end\":49363,\"start\":49351},{\"end\":49374,\"start\":49363},{\"end\":49386,\"start\":49374},{\"end\":49400,\"start\":49386},{\"end\":49414,\"start\":49400},{\"end\":49429,\"start\":49414},{\"end\":49683,\"start\":49669},{\"end\":49699,\"start\":49683},{\"end\":49864,\"start\":49850},{\"end\":49877,\"start\":49864},{\"end\":49891,\"start\":49877},{\"end\":49900,\"start\":49891},{\"end\":49914,\"start\":49900},{\"end\":49925,\"start\":49914},{\"end\":50160,\"start\":50147},{\"end\":50172,\"start\":50160},{\"end\":50186,\"start\":50172},{\"end\":50487,\"start\":50474},{\"end\":50499,\"start\":50487},{\"end\":50513,\"start\":50499},{\"end\":50546,\"start\":50513},{\"end\":50560,\"start\":50546},{\"end\":50573,\"start\":50560},{\"end\":50939,\"start\":50927},{\"end\":50947,\"start\":50939},{\"end\":50961,\"start\":50947},{\"end\":50971,\"start\":50961},{\"end\":51247,\"start\":51235},{\"end\":51261,\"start\":51247},{\"end\":51277,\"start\":51261},{\"end\":51290,\"start\":51277},{\"end\":51510,\"start\":51498},{\"end\":51521,\"start\":51510},{\"end\":51543,\"start\":51521},{\"end\":51555,\"start\":51543},{\"end\":51732,\"start\":51718},{\"end\":51739,\"start\":51732},{\"end\":51752,\"start\":51739},{\"end\":51761,\"start\":51752},{\"end\":51773,\"start\":51761},{\"end\":51783,\"start\":51773},{\"end\":51793,\"start\":51783},{\"end\":51807,\"start\":51793},{\"end\":52080,\"start\":52067},{\"end\":52094,\"start\":52080},{\"end\":52109,\"start\":52094},{\"end\":52125,\"start\":52109},{\"end\":52358,\"start\":52348},{\"end\":52369,\"start\":52358},{\"end\":52379,\"start\":52369},{\"end\":52395,\"start\":52379},{\"end\":52406,\"start\":52395},{\"end\":52657,\"start\":52647},{\"end\":52669,\"start\":52657},{\"end\":52695,\"start\":52669},{\"end\":52701,\"start\":52695}]", "bib_venue": "[{\"end\":33737,\"start\":33724},{\"end\":33469,\"start\":33459},{\"end\":33722,\"start\":33694},{\"end\":34020,\"start\":34016},{\"end\":34381,\"start\":34377},{\"end\":34778,\"start\":34773},{\"end\":35076,\"start\":35015},{\"end\":35494,\"start\":35490},{\"end\":35791,\"start\":35787},{\"end\":36037,\"start\":36033},{\"end\":36380,\"start\":36376},{\"end\":36726,\"start\":36722},{\"end\":37017,\"start\":37010},{\"end\":37309,\"start\":37302},{\"end\":37610,\"start\":37573},{\"end\":37980,\"start\":37976},{\"end\":38233,\"start\":38229},{\"end\":38482,\"start\":38474},{\"end\":38670,\"start\":38597},{\"end\":38980,\"start\":38906},{\"end\":39296,\"start\":39292},{\"end\":39561,\"start\":39557},{\"end\":39894,\"start\":39890},{\"end\":40187,\"start\":40183},{\"end\":40421,\"start\":40417},{\"end\":40677,\"start\":40673},{\"end\":40940,\"start\":40936},{\"end\":41159,\"start\":41155},{\"end\":41393,\"start\":41389},{\"end\":41682,\"start\":41678},{\"end\":41998,\"start\":41994},{\"end\":42298,\"start\":42294},{\"end\":42573,\"start\":42569},{\"end\":42895,\"start\":42890},{\"end\":43177,\"start\":43173},{\"end\":43506,\"start\":43502},{\"end\":43832,\"start\":43828},{\"end\":44110,\"start\":44106},{\"end\":44411,\"start\":44407},{\"end\":44662,\"start\":44590},{\"end\":44985,\"start\":44900},{\"end\":45427,\"start\":45423},{\"end\":45732,\"start\":45728},{\"end\":46038,\"start\":45999},{\"end\":46320,\"start\":46316},{\"end\":46643,\"start\":46639},{\"end\":46913,\"start\":46847},{\"end\":47234,\"start\":47230},{\"end\":47579,\"start\":47575},{\"end\":47937,\"start\":47933},{\"end\":48309,\"start\":48305},{\"end\":48636,\"start\":48632},{\"end\":49026,\"start\":49022},{\"end\":49433,\"start\":49429},{\"end\":49703,\"start\":49699},{\"end\":49929,\"start\":49925},{\"end\":50145,\"start\":50068},{\"end\":50577,\"start\":50573},{\"end\":50925,\"start\":50817},{\"end\":51295,\"start\":51290},{\"end\":51559,\"start\":51555},{\"end\":51814,\"start\":51807},{\"end\":52065,\"start\":51986},{\"end\":52410,\"start\":52406},{\"end\":52705,\"start\":52701}]"}}}, "year": 2023, "month": 12, "day": 17}