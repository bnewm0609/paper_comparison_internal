{"id": 14915460, "updated": "2022-07-12 12:21:41.129", "metadata": {"title": "Automatic Expansion of a Food Image Dataset Leveraging Existing Categories with Domain Adaptation", "authors": "[{\"first\":\"Yoshiyuki\",\"last\":\"Kawano\",\"middle\":[]},{\"first\":\"Keiji\",\"last\":\"Yanai\",\"middle\":[]}]", "venue": "ECCV Workshops", "journal": "3-17", "publication_date": {"year": 2014, "month": null, "day": null}, "abstract": ". In this paper, we propose a novel e\ufb00ective framework to expand an existing image dataset automatically leveraging existing categories and crowdsourcing. Especially, in this paper, we focus on expansion on food image data set. The number of food categories is uncountable, since foods are di\ufb00erent from a place to a place. If we have a Japanese food dataset, it does not help build a French food recognition system directly. That is why food data sets for di\ufb00erent food cultures have been built independently so far. Then, in this paper, we propose to leverage existing knowledge on food of other cultures by a generic \u201cfoodness\u201d classi\ufb01er and domain adaptation. This can enable us not only to built other-cultured food datasets based on an original food image dataset automatically, but also to save as much crowd-sourcing costs as possible. In the experiments, we show the e\ufb00ectiveness of the proposed method over the baselines.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "1030521295", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/KawanoY14", "doi": "10.1007/978-3-319-16199-0_1"}}, "content": {"source": {"pdf_hash": "87c8dd47bd004243e9788b8e70d7f065cf46ffff", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "10abacd36f7e8500d0f936d8e907f3b9516e1103", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/87c8dd47bd004243e9788b8e70d7f065cf46ffff.txt", "contents": "\nAutomatic Expansion of a Food Image Dataset Leveraging Existing Categories with Domain Adaptation\n\n\nYoshiyuki Kawano kawano-y@mm.inf.uec.ac.jp \nDepartment of Informatics\nThe University of Electro-Communications\n1-5-1 Chofugaoka, Chofu-shi182-8585TokyoJapan\n\nKeiji Yanai yanai@mm.inf.uec.ac.jp \nDepartment of Informatics\nThe University of Electro-Communications\n1-5-1 Chofugaoka, Chofu-shi182-8585TokyoJapan\n\nAutomatic Expansion of a Food Image Dataset Leveraging Existing Categories with Domain Adaptation\n10.1007/978-3-319-16199-0Dataset expansion \u00b7 Food image \u00b7 Foodness \u00b7 Domain adap- tation \u00b7 Crowd-sourcing \u00b7 Adaptive SVM\nIn this paper, we propose a novel effective framework to expand an existing image dataset automatically leveraging existing categories and crowdsourcing. Especially, in this paper, we focus on expansion on food image data set. The number of food categories is uncountable, since foods are different from a place to a place. If we have a Japanese food dataset, it does not help build a French food recognition system directly. That is why food data sets for different food cultures have been built independently so far. Then, in this paper, we propose to leverage existing knowledge on food of other cultures by a generic \"foodness\" classifier and domain adaptation. This can enable us not only to built other-cultured food datasets based on an original food image dataset automatically, but also to save as much crowd-sourcing costs as possible.In the experiments, we show the effectiveness of the proposed method over the baselines.\n\nIntroduction\n\nRecently, needs for food image recognition become larger, since food habit recording services for smartphones are spreading widely for everyday health care. For food habit recording, conventional ways such as inputing food names by texts or selecting food items from menus are very tedious, which sometimes prevent users from using such systems regularly. Then, several works on food recognition have been proposed so far [1][2][3][4][5] to make it easy to use food habit recording. In these works, the number of food categories is 100 at most, which is not enough for practical use. In fact, all of the foods we eat in our everyday life cannot be covered with only one hundred food categories, and the number of foods which can be recognized should be increased much more.\n\nOn the other hand, in these years, large-scale image classification is paid attention, and many methods for that have been proposed recently [6][7][8][9]. Due to these works, the number of categories to be recognized have been increased up to 1000. For example, in ImageNet Large Scale Visual Recognition Challenge (ILSVRC), the number of categories to be classified is 1000. The data set for ImageNet Challenge is a subset of ImageNet [10], which is known as the largest visual database where the number of categories are more than 20,000. Largescale image data sets such as ImageNet cannot be created by researchers by themselves. Most of them use crowd-sourcing Web services such as Amazon Mechanical Turk to build them semi-automatically.\n\nIn this paper, we propose a novel framework to expand an existing image dataset automatically leveraging existing categories. Especially, in this paper, we focus on expansion on food image data set.\n\nWhile ImageNet covers comprehensive concepts, our target is restricted to foods. In ImageNet, annotation of each concept is gathered independently. On the other hand, since foods look more similar to each other, visual knowledge on foods of a certain country is expected to help collect annotations of food photos of the other countries. Then, in this paper, we propose a novel effective framework which utilizes knowledge on food of other countries by domain adaptation.\n\nBasically, we gather food image candidates on novel food categories from the Web, and select good photos and add bounding boxes by using crowd-sourcing. In general, raw Web images include many noise images which are irrelevant to a given keyword. Especially, in this work, non-food images can be regarded as noise images. To exclude them from the gather images, we filter and re-rank Web images related to a given food category by using visual knowledge extracted from the existing food dataset.\n\nFirstly, we built a generic \"foodness\" classifier from a Japanese food data set, UEC-Food100 [4]. We cluster all the food categories in the exist food image set into several food groups the member of which are similar to each other in terms of image feature vectors, and we train SVMs regarding each food group independently. Then, we evaluate unknown images using the trained SVMs on the food groups, and regards the maximum value of the output values of all the SVM as the \"foodness\" value of the given image. We can decide if a given image of a unknown category is a food photo or not based on the \"foodness\" value. In addition, because we select the maximum value from all the output valued of food groups, we estimate the most related food group to a given photo.\n\nAfter \"foodness\" filtering, we obtain a food photo set. However, it might include food photos irrelevant to the given food keyword. Secondly, we select and re-rank more relevant images from the images judged as food photos by using transfer learning with visually similar categories in the source food photo data set. As a method of transfer learning, we use Adaptive SVM (A-SVM) [11] which can learn a discriminative hyper-plane in the target domain taking into account source-domain training data. In this work, the labeled data of the source categories which are visually similar to the target food photos are used as sourcedomain training data. As an initial target-domain training data, we use upperranked photos by a unsupervised image ranking method, VisualRank (VR) [12]. Then, we select food candidate images to be submitted for the crowd-sourcing by applying a trained A-SVM. By the experiments, the precision of the food candidate photos by A-SVM has been proved to outperformed the results by only VisualRank and by normal standard SVM.\n\nThe contributions of this paper are as follows:\n\n(1) Propose a novel framework to extend an existing image dataset with a generic \"foodness\" classifier and domain transfer learning. (2) Three-step crowd-sourcing: selecting representative sample images, excluding noise photos, and drawing bounding boxes. (3) Evaluate and compare accuracy of built food datasets and costs regarding the proposed method and two baselines. (4) Apply the proposed framework in a large scale, and build a new 256-category food dataset based on the existing 100-category food dataset automatically.\n\n\nRelated Works\n\nIn the existing work on food recognition, the target foods are limited to the foods which are common in a certain country. For example, US food [1,3,13], Chinese food [2] and Japanese food [4,14]. From this observation, it is assumed that these food datasets were built to implementing food recognition systems the target of which are only the foods in the specific countries. In addition, in the above-mentioned works, the number of target food categories is limited to 100 at most. From a practical point of view, 100 food categories is not enough for recognizing everyday foods for generic people. In fact, the number of foods we eat in our everyday life is much more than one hundred, and the number of foods which can be recognized should be increased much more.\n\nThen, in this work, to make it easy to add the number of food categories and to implement food image recognition systems for other country foods or all the country foods, we propose a method to use an existing food dataset to build additional or another food dataset automatically by applying transfer learning.\n\nOn the Web, there are various kinds and huge amounts of images. It is very easy to collect images associated with a given keyword using Web API such as Bing Image Search API, Flickr API and Twitter API. However, raw Web images contain many noise images which are irrelevant to the given keyword. Therefore, many works on re-rank Web images regarding the given keyword have been proposed since ten years ago [15,16]. Most of these works employed object recognition methods to select relevant images to given keywords from \"raw\" images collected from the Web using Web image search engines.\n\nAfter spreading Amazon Mechanical Turk (AMT) which is the world-largest crowd-sourcing Web platform, it is commonly used for a task to select relevant images. AMT enables us to build a very huge-scale image dataset such as Ima-geNet [10], to build a middle-or large-scale dataset with bounding boxes [17], and to add attributes to a large-scale dataset [18].\n\nIn some works, AMT was incorporated into object recognition procedures, which was called \"humans in the loop\". Vijayanarasimhan et al. [17] proposed to combine active learning of object detectors and AMT crowd-sourcing tasks to draw bounding boxes as a loop procedure to raise accuracy of object detection gradually. On the other hand, Branson et al. [19] proposed complementary use of AMT with object classifiers by giving AMT workers simple easy questions to tackle difficult fine-grained object classification.\n\nIn addition, thanks to crowd-sourcing, many kinds of image datasets have released such as \"bird\" [20], \"aircraft\" [21], and \"flower\" [22]. They are intended to be built for fine-grained visual categorization research.\n\nIn this work, we use AMT as a crowd-sourcing service to select relevant images and add bounding boxes to selected food images. The objective is similar to [17]. However, while Vijayanarasimhan et al. [17] collected relevant images and their bounding boxes on each category independently, we collect images using knowledge of the known categories in the existing database with a \"foodness\" classifier and transfer learning.\n\nIn addition, as a pre-step of image selection, we prepare a task to ask the best representative photos regarding the given category. Some small number of representative photos are used to be shown workers as example photos to raise the accuracy of the image selection step.\n\n\nProposed Method\n\nIn this paper, we propose a novel framework to expand an existing image dataset automatically. The proposed framework consists of two stages: (1) the image selection stage, and (2) the crowd-sourcing stage.\n\nIn the image selection stage, we collect images from the Web with the given category names, and filter out noise images using a \"foodness\" classifier and adaptive SVM [11], both of which we train using knowledge of the existing food image database.\n\nThen, in the crowd-sourcing stage, we crowdsource three kinds of tasks. First one is selecting representative images for the given new food category, the second one is discriminating relevant images from noise ones, and the third one is drawing bounding boxes on each of the selected images.\n\nThe processing flow of the proposed framework is shown in Fig.1. Each of the processing steps is explained as follows:\n\n(1) Collect target food images associated with the given new food category from the Web. (2) Evaluate \"foodness\" on each of the collected images, and select only high \"foodness\" images. (3) Rank the selected food images with VisualRank, and train adaptive SVMs(A-SVM) [11] with upper ranked images as pseudo positive samples. (4) Evaluate collected images again by A-SVM. (5) Crowdsource a task to select representative samples from the top 30 images in terms of A-SVM scores (6) Crowdsource a task to discriminate relevant images from noise images for the images ranked higher by A-SVM (7) Crowdsource a task to draw bounding boxes on the selected images. (8) Add the annotated food images to a food image dataset. \n\n\nFoodness Classifier\n\nWe construct a \"Foodness\" Classifier (FC) for discriminating and evaluating food images. FC evaluates if the given image is a food photo or not. We use FC to remove noise images from the images gathered from the Web.\n\nWe construct a FC from the existing multi-class food image dataset. Regarding feature extraction and coding, we adopt the same way as our mobile food recognition system [14]. Firstly, we train linear SVMs [23] in the one-vs-rest strategy for each category of the existing multi-class food image dataset. As image features, we adopt HOG patches [24] and color patches. Regarding foods, rotation and scale invariance is not so important. We regard fast extraction as more important, since we originally use these features for mobile food recognition. Both descriptors are coded by Fisher Vector (FV) [9,25], and they are integrated in the late fusion manner. We perform multi-class image classification in the cross-validation using the trained liner SVMs, and we build a confusion matrix according to the classification results.\n\nSecondly, we make some category groups based on confusion matrix of multiclass classification results. This is inspired by Bergamo et al.'s work [26]. They grouped a large number of categories into superordinate groups the member categories of which are confusing to each other recursively. In the same way, we perform confusion-matrix-based clustering for all the food categories. We intend to obtain superordinate categories such as meat, sandwiches, noodle and salad automatically.\n\nTo build a \"foodness\" classifier (FC), we train a linear SVM of each of the superordinate categories. The objective of FC is judging if a food photo candidate which never corresponds to any food category in the existing dataset is a food photo or not. Therefore, abstracted superordinate categories are desirable to be trained, rather than training of all the food categories directly. The output value of FC is the maximum value of SVM output of all the superordinate food groups.\n\nWhen training SVMs, we used all the images of the categories under the superordinate category as positive samples. For negative samples, we built a negative food image set in advance by gathering images using the Web image search engines with query keywords which are expected to related to noise images such as \"street stall\", \"kitchen\", \"dinner party\" and \"restaurant\" and excluding food photos by hand. All the images are represented by Fisher Vector of HOG patches and color patches. SVMs are trained in the late fusion manner with uniform weights. In the experiments, we will show the effectiveness of FC for evaluating \"foodness\" of food images of novel unknown categories based on visual knowledge of known food categories in the existing database.\n\n\nRe-ranking with Domain Transfer\n\nAfter \"foodness\" filtering, most of the remaining images are food images. However, they might includes other kinds of foods than the given food category. Since the objective of the proposed framework is collecting food images of novel unknown categories. To filtering out other food images than the target food category, we adopt discriminative approach with pseudo-positive samples in the similar way as Schroff et al. [16], since we have no labeled samples on the given novel category initially.\n\nTo get pseudo-positive samples, we rank the remaining images by the Visu-alRank [12] method. In addition, we use adaptive SVM [11] to leverage visual knowledge of the existing food image database. Because VisualRank is a unsupervised method to rank images which have many visually similar images in the upper ranking, it is useful to select relevant images from a noisy image dataset. However, it has drawback to narrow diversity of images. To compensate it, we use a domain adaptation method to leverage the existing food image database for classifying novel unknown food images.\n\n\nVisualRank.\n\nTo select pseudo-positive images, we apply VisualRank [12] to the top N images in term of \"foodness\" scores. We set N as 300 in the experiments. For computing VisualRank scores, we obtained similarity matrix S as being dot product of Fisher Vectors (concatenated vectors of HOG FV and Color FV) instead of the number of matched local features. It has been proved that dotproduct of FV can be regarded as a good similarity measure, and dot-product of L2-normalized vectors is equivalent to the cosine similarity [9,27]. In addition, according to the following equation, we normalize each element s of similarity matrix S so that s normalized varies within only [0, 1] range, because the value of elements of FV can be negative.\ns normalized = s \u2212 s min s max \u2212 s min(1)\nwhere s max and s min are maximum and minimum values among all the elements of similarity matrix S. Next, we column-normalized S for computation of Visual-Rank. Regarding a bias vector v, we assign only the top m images with uniform weights in the same way as [12]. We set m as 100 in the experiments. Note that, we use the same Gaussian Mixture Model(GMM)s for FV coding estimated in the the previous step. This mean that the descriptors of unknown category are not modeled independently. We calculate VisualRank score r in the following equation:\nr = \u03b1 * Sr + (1 \u2212 \u03b1) * v,(2)\nwhere \u03b1 is a damping factor where we set it to 0.85 according to [12].\n\n\nSelection of Source Domain Samples and Target Domain Samples.\n\nTo train and apply adaptive SVM [11], we need to prepare source-domain labeled samples as well as target-domain labeled samples. Because for both domains we need to prepare positive and negative samples, totally we prepare four kinds of samples for training of A-SVM. As target-domain positive samples, we use the top M images in terms of the VisualRank scores, while as target-domain negative samples, we use the images with lower \"foodness\" scores in the initial image set gathered from the Web. In the experiments, we selected at most 300 images the \"foodness\" score of which were less than -0.6. Because the objective of this A-SVM-based re-ranking step is excluding noise images from the initial image set, not classifying generic images into one of food categories, we use negative samples which are peculiar to the given food category.\n\nAs source-domain positive samples, we use all the samples in the most related food group to the given new food category. As mentioned in the previous subsection, a \"foodness\" classifier can estimate the most related food group as well as a \"foodness\" score. We select the most frequent food group among the top 100 \"foodness\" images in the initial image set as the most related food group to the given new food category. As source-domain negative sample, we use the same negative food image set used in the previous step of a \"foodness\" classifier.\n\nIn the next step, we select positive samples to exclude noise images, and select effective negative samples for training. Regarding source-domain samples, in general, the distribution of source-domain samples are wider than one of target-domain samples. Regarding target-domain samples, they are unreliable and tend to include outliers, since target-domain samples are selected automatically by a \"foodness\" classifier and VisualRank. Then we select the samples which are closed to target-domain samples as source-domain samples, and the samples which are closed to other target-domain samples as target-domain samples according to the following heuristics: This process is called \"Sample Selection (SS)\" in the section on experiments.\n\nAfter source/target-domain positive/negative samples are selected finally, we train the adaptive SVM, and apply the trained model to re-rank the images in the image set after filtering by the \"foodness\" classifier. We use only higher-ranked images for crowd-sourcing tasks.\n\n\nCrowd-Sourcing\n\nThe final objective is obtaining a novel food image dataset with bounding boxes. In the previous steps, we applied \"foodness\" filtering and adaptive SVM reranking. However, the obtained food imageset is not perfect, and has no bounding box information. As the final steps, we crowdsource the following three kinds of tasks: (1) selecting representative sample images, (2) removing irrelevant images, and (3) drawing bounding boxes. As a crowd-sourcing service, we use Amazon Mechanical Turk (AMT).\n\nRepresentative Image Selection Task. We assumes that AMT workers do not have knowledge about various kinds of foods. Therefore, it cannot be expected to obtain highly accurate results without any preparations. Then, we prepare a task to select representative sample images as a pre-process step.\n\nIn this task, we ask AMT workers to select less than 10 representative images to the given food category from the top 30 image of A-SVM output scores, after studying about the given food category by visiting Wikipedia, Google Web search and Google Web image search with the name of the given food category as a query word. We design the task page so that AMT workers cannot submit the results without clicking the Web links to Wikipedia and Google sites. After collecting results from 5 workers, we select the top 5 or 7 images as representative samples based on the number of votes by the workers. In the experiments, we set one HIT (Human Intelligence Task, which is a task unit in AMT.) of this task as 0.06$.\n\nNoise Removal Task. In this task, we ask AMT workers to annotate if the shown images are relevant to the given food category or not. In the task page, we show the representation images selected in the previous task. We believe this will be helpful for works who have never seen the target food. In one HIT, we use randomly-selected 25 images in the higher rank of the A-SVM scores. To prevent irresponsible worker, if there are more than four unchecked images, the result cannot be submitted. The results will be combined based on the majority voting. In the experiments, we requested each HIT for 5 workers. We set 1 HIT as 0.03$.\n\nDrawing Bounding Box Task. As the final task, we ask AMT workers to draw bounding boxes on the selected food images until the previous step. One HIT contains ten image annotation. In this step, worker can still mark irrelevant images as \"noise\" in the same way as the noise removal task, if they discover. After obtaining the results, we combine them by averaging the position of bounding boxes excluding images with no bounding boxes and too small bounding boxes. We add the finally selected images with bounding box to a new food image database as a ground-truth data. In the experiments, we requested each HIT for 4 workers. We set one HIT as 0.05$.\n\n\nExperiments\n\nIn this section, we perform the following three experiments to evaluate the effectiveness of the proposed method.\n\n-Performance comparison on food image filtering by a \"foodness\" classifier and adaptive SVM to leverage knowledge of the existing food dataset -Evaluation of the final results after crowd-sourcing and analysis of crowdsourcing cost.\n\nBefore evaluation, we describe a dataset, feature representation and initial food image collection from the Web. As an existing food dataset, we use \"UEC-Food100\" dataset [4] which consists 14361 food photos. Its number of food categories is 100, most of which are Japanese food categories. When building a \"foodness\" classifier (FC), we clustered 100 food categories into 13 food groups based on confusion matrix as shown in Tab.1. Note that the type of food groups in the table are named by hand for explanation.\n\nAs feature representation, we used 32-dim HOG local patches (8 orientations, 2x2) and 24-dim color local patches (mean and variance of RGB, 2x2) both of which are densely sampled from an image at difference 2 scales. After applying PCA, local descriptors are coded into Fisher Vector with GMM codebook (k=64) and a level-1 spatial pyramid (SPM) [28]. The GMM was estimated from the existing food dataset in advance.\n\nRegarding initial food image collection from Web, we collected food images via Flickr API, Twitter API and Bing Image Search API based on query words associated with the given food category. We collected more than 600 images for each category. As query words, we used the words of both local language and English. We excluded duplicated URLs using a URL hash table after putting together all the image URLs gathered from three different APIs. \n\n\nEvaluation on Image Filtering Results\n\nFor evaluation, we collected 35 categories of food image sets including 5 country foods with 7 categories for each country. All the 35 categories do not overlap with the categories in \"UEC-FOOD100\". We evaluated the precision of the top 300 food images (Precision@300) for each category. Note that we regarded badlyconditioned food images as being irrelevant. For example, an image with very small food region and an image in which only small portion of original food region is visible are not relevant. We compare Precision@300 after filtering by the following six methods: (1) VisualRank with Fisher Vector, (2) \"foodness\" classifier (FC), (3) normal SVM using only target-domain training samples without \"Sample Selection (SS)\" after FC filtering (4) normal SVM using only target-domain training samples with SS after FC filtering (5) adaptive SVM using both source/target-domain training samples without SS after FC filtering, and (6) adaptive SVM using both source/target-domain training samples with SS after FC filtering. The last method (FC + A-SVM(SS)) corresponds to the proposed methods. Note that \"Sample Selection (SS)\" means the step to select of training samples for A-SVM or SVM (see Sec. 3.2), and VisualRank is still used for positive sample selection in (4)(5)(6)(7).\n\nTab.2 shows average Precision@300 of the results after filtering by each of the seven methods over 5 country foods and all 35 kinds of foods. Overall, the proposed methods outperformed other six baseline methods for all the regional foods.\n\nCompared between VR and other supervised methods, the precision value by unsupervised VisualRank is not so good as the results by supervised discriminative classifiers such as FC and FC+A-SVM. In fact, FC improved Precision by about 20.0 points compared to VisualRank. This indicates that using existing categories helps improve filtering accuracy much, although they are different from the newly collected categories.\n\nTo use supervised methods such as SVM and A-SVM, we selected pseudopositive samples from the top 300 images ranked by FC with Visual Rank(VR), and we used the top 200 images ranked by VR as pseudo-positive samples in the after steps. Tab. 3 shows the precision at the top 200 images before and after applying VR. Compared with two results, Precision@200 was improved by 5.34 points, which shows the effectiveness of applying VR after FC.\n\nIn case of FC+SVM, we used only target-domain training samples where positive samples are selected by VisualRank from the unlabeled samples, and manually-constructed common negative samples are used as negative samples. Although FC+SVM employs supervised SVM, the step itself is unsupervised because positive samples are \"pseudo-positive\" samples collected automatically. Even without supervision, FC+SVM improved by about 6 points compared to FC in terms of Precision@300. After adding training sample selection (SS) for SVM, the Precision was slightly improved.\n\nFC+A-SVM and FC+A-SVM(SS) introduced a transfer learning method, adaptive SVM, which takes into account source-domain training samples as well. From their results, introducing domain transfer helps improve accuracy of image filtering, and the proposed method (FC+A-SVM(SS)) has achieved the best result, which proves the effectiveness of the proposed method. Fig.2 shows the top 3 food categories in terms of Precision@300 among all the 35 categories, \"mango pudding\", \"loco moco\" and \"fried shrimp with shell\" in the left column, and three source-domain samples in the corresponding food groups. The images in each food group are used as source-domain positive samples when training adaptive SVM. The target-domain images looks similar to source-domain images in terms of color, shape or ingredients. All the foods in the first row in the figure are light-yellow, the foods in the second row have brown-colored source, and the foods in the bottom row have fried ingredients.  From these results, the new category images can be classified with visually similar images of the existing categories in the most related food group by using transfer learning. This is a part of the contributions of this work.\n\n\nEvaluation Accuracy and Costs of Crowdsourcing\n\nWe evaluate the effectiveness of showing representative samples to workers, accuracy of obtained image sets and crowdsourcing costs.\n\nWorkers' Evaluation on Representative Samples. We prepared a task to select representative sample images as a pre-process step. Selected representation images were shown in the page of noise removal task and drawing bounding box task in order to teach workers what relevant food photos look like. To evaluate its effectiveness, we asked workers in each HIT if sample images shown in the HIT page are useful, so so or useless. As a result, 3495 and 5359 answers are obtained in noise removal task and annotation bounding box task. Tab.4 shows the ratio of each answer, which shows the effectiveness of showing representative samples in both noise removal task and annotation bounding box task.\n\n\nEvaluation of Accuracy and Costs.\n\nTo evaluate accuracy and costs including crowdsourcing, we constructed three kinds of datasets by the following different combination of filtering steps: (1) FC + drawing bounding box task (BB task),   (1) and (2), workers have to mark irrelevant images as \"noise\" in addition to drawing bounding boxes to relevant images in the BB task, because the noise removal task is not included. The combination (1) is the simplest, and in (2) adaptive SVM was added. The last combination where noise removal task is prepared as an independent task is equivalent to the proposed framework. Note that all the combination includes representative sample selection task. Tab.5 shows the precision of food images on the constructed dataset after crowsourcing by each combination. The precision by \"FC+BB task\" was 91.1%, while the precision by \"FC+A-SVM(SS)+BB task\" was 94.19%. Introducing A-SVM(SS) improved 3.09%, while it improved about 10% regarding the precision of filtered image sets before crowdsoucing. Although both combinations employs human annotation via crowdsourcing, the difference in precision appeared after crowdsourcing. This is estimated to come from the accuracy of dataset to supply workers. From this observation, to get more accurate results from crowdsourcing, more accurate data should be provided to crowdsourcing workers.\n\nCompared between \"FC+A-SVM(SS)+BB task\" and \"FC+A-SVM(SS)+NR task+BB task\", separating noise removal task from drawing bounding box improved the precision, although provided datasets are the same. This indicates that crowdsourcing tasks (HITs) should be include only one kinds of jobs. Of course, increase of the number of crowdsourcing steps means increase of economical costs. We compare costs among the three cases in the next.\n\nTab.6 shows the recovery ratio and cost for the three combinations. The recovery ratio means the ratio of the number of the images which were finally annotated with correct bounding boxes over the number of provided images to workers for crowdsourced annotation. If the recovery ratio is low, many irrelevant samples are provided to workers, which means economical costs increase. \"Costs\" shown in the table means the money ($) paid for AMT to get 100 annotated images. To avoid wasting money, the recovery rate should be high, hopefully close to 100%. \"FC+BB task\" was apparently a bad strategy, because the total cost is high and the accuracy of the obtained results shown in Tab.5 is worst. \"FC+A-SVM+BB\" performed the best among the three strategies in terms of cost. Adding the noise removal task, the cost increased, because the number of crowdsourcing steps also increased. However, the precision of the final obtained results was the best as shown in Tab.5. That shows that there is a trade-off between cost and accuracy. It depends on the policy when building a dataset. Of course, Tab.6 shows just one case. If a unit price for HIT of each task is changed, the result of cost analysis will be changed. Regarding cost, more accurate data should be provided to crowdsourcing workers to raise the recovery rate. To do that, introducing \"foodness\" classifier and adaptive SVM is very effective.\n\n\nConclusions\n\nIn this paper, we proposed a novel framework to expand an existing image dataset automatically employing generic classifiers and domain adaptation to leverage visual knowledge in the existing dataset. Especially, in this paper, we focused on expansion on food image data set. In the experiments, we showed the effectiveness of the proposed method over baselines in terms of the proposed image filtering methods and the proposed procedure for crowdsourcing.\n\nFor future work, we will make further analysis on the difference between a hand-collected food image dataset and an automatically collected dataset by the proposed framework. In addition, we plan to extend the framework to other categories than foods such as clothes and animal.\n\nFig. 1 .\n1Processing flow of the proposed framework\n\n-\nSelect the target-domain positive samples each of which has more than 3 positive samples among the nearest 5 samples over the space of all the targetdomain (positive and negative) samples. -Select the target-domain negative samples each of which has 5 negative samples among the nearest 5 samples over the target-domain space. -Select the source-domain positive samples which are included in the union set of 7 closest source-domain all (positive and negative) samples to each of the selected target-domain positive samples. -Select the source-domain negative samples which are included in the union set of 7 closest source-domain all samples to each of the selected targetdomain positive samples.\n\nFig. 2 .\n2The target-domain food images in the left column, and three source-domain samples in the most related food groups\n\n\nFC + A-SVM(SS) + BB task, and (3) FC + A-SVM(SS) + noise removal\n\nTable 1 .\n113 food groups and their member foodstype of food group \nfood categories \nnoodles \nudon nooles, dipping noodles, ramen \nyellow color \nomlet, potage, steamed egg hotchpotch \nsoup \nmiso soup, pork miso soup, Japaneses tofu and vegetable chowder \nfried \ntakoyaki, Japaneses-style pancake, fried noodle \ndeep fried \ncroquette, sirloin cutlet, fried chicken \nsalad \ngreen salad, macaroni salad, macaroni salad \nbread \nsandwiches, raisin bread, roll bread \nseafood \nsashimi, sashimi bowl, sushi \nrice \nrice, pilaf, fried rice \nfish \ngrilled salmon, grilled pacific saury, dried fish \nboiled \nseasoned beef with potatoes \nand \nsimmered ganmodoki \nseasoned \nseasoned beef with potatoes \nsauteed \nsauteed vegetables, go-ya chanpuru, kinpira-style sauteed burdock \nsauce \nstew, curry, stir-fried shrimp in chili sauce \n\n\n\nTable 2 .\n2Precision@300 of the food images ranked by six methods. The bottom method \u2020 is the proposed methods.no \nMethod \nAmerican Japanese Chinese Thai Indonesian Average \n(1) VisualRank(VR) \n58.47 \n54.95 \n60.66 62.19 \n58.71 \n59.00 \n(2) \"Foodness\" (FC) \n78.00 \n75.33 \n77.61 82.85 \n78.61 \n78.48 \n(3) \nFC + SVM \n84.52 \n82.90 \n84.80 88.80 \n81.95 \n84.60 \n(4) FC + SVM(SS) \n85.57 \n83.38 \n85.09 89.23 \n82.23 \n85.10 \n(5) FC + A-SVM \n86.95 \n85.71 \n86.19 89.66 \n82.71 \n86.24 \n(6) FC + A-SVM(SS) \u2020 89.61 \n87.76 87.76 91.38 \n84.09 \n88.12 \n\nTable 3. Precision@200 of \"pseudo-positive samples\" which is provided to SVM or \nA-SVM as positive target samples \n\nno Method American Japanese Chinese Thai Indonesian Average \n(A) only FC 79.21 \n77.00 \n80.21 83.78 \n83.00 \n80.64 \n(B) FC+VR 85.00 \n83.78 \n85.78 89.14 \n86.21 \n85.98 \n\n\n\nTable 4 .\n4Evaluation by workers on representative samples images (%)useful \nso so \nuseless \nnoise removal task \n89.59% 7.90% \n2.52 % \ndrawing bounding box task \n91.68% \n7.02% \n1.31 % \n\n\n\nTable 5 .Table 6 .\n56Precision of food images on dataset by difference 3 methods FC + A-SVM + BB task 94.19 +3.09 FC + A-SVM + NR task + BB task 97.83 +3.64 Recovery ratio(%) and costs($) to get annotated 100 images task (NR task) + BB task. In case ofprecision gain \nFC + BB task \n91.10 \n-\nnoise removal \nbounding box total \nrecovery ratio cost recovery ratio cost total \nFC + BB task \n-\n-\n64.2 \n3.11 3.11 \nFC + A-SVM + BB task \n-\n-\n74.7 \n2.68 2.68 \nFC + A-SVM + NR task + BB task \n80.9 \n0.74 \n86.7 \n2.31 3.16 \n\n\n\nFood recognition using statistics of pairwise local features. S Yang, M Chen, D Pomerleau, R Sukthankar, CVPRYang, S., Chen, M., Pomerleau, D., Sukthankar, R.: Food recognition using statis- tics of pairwise local features. In: CVPR (2010)\n\nAutomatic chinese food identification and quantity estimation. M Chen, Y Yang, C Ho, S Wang, S Liu, E Chang, C Yeh, M Ouhyoung, SIGGRAPH Asia 2012 Technical Briefs. Chen, M., Yang, Y., Ho, C., Wang, S., Liu, S., Chang, E., Yeh, C., Ouhyoung, M.: Automatic chinese food identification and quantity estimation. In: SIGGRAPH Asia 2012 Technical Briefs (2012)\n\nCombining global and local features for food identification in dietary assessment. M Bosch, F Zhu, N Khanna, C J Boushey, E J Delp, ICIPBosch, M., Zhu, F., Khanna, N., Boushey, C.J., Delp, E.J.: Combining global and local features for food identification in dietary assessment. In: ICIP (2011)\n\nMultiple-food recognition considering co-occurrence employing manifold ranking. Y Matsuda, K Yanai, ICPRMatsuda, Y., Yanai, K.: Multiple-food recognition considering co-occurrence employing manifold ranking. In: ICPR (2012)\n\nReal-time mobile food recognition system. Y Kawano, K Yanai, Proc. of IEEE CVPR International Workshop on Mobile Vision (IWMV). of IEEE CVPR International Workshop on Mobile Vision (IWMV)Kawano, Y., Yanai, K.: Real-time mobile food recognition system. In: Proc. of IEEE CVPR International Workshop on Mobile Vision (IWMV) (2013)\n\nEfficient additive kernels via explicit feature maps. A Vedaldi, A Zisserman, IEEE Trans. on PAMI. 343Vedaldi, A., Zisserman, A.: Efficient additive kernels via explicit feature maps. IEEE Trans. on PAMI 34(3), 480-492 (2012)\n\nLocality-constrained linear coding for image classification. J Wang, J Yang, K Yu, F Lv, T Huang, Y Gong, CVPR. Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., Gong, Y.: Locality-constrained linear coding for image classification. In: CVPR, pp. 3360-3367 (2010)\n\nImage classification using super-vector coding of local image descriptors. X Zhou, K Yu, T Zhang, T S Huang, ECCV 2010, Part V. Daniilidis, K., Maragos, P., Paragios, N.HeidelbergSpringer6315Zhou, X., Yu, K., Zhang, T., Huang, T.S.: Image classification using super-vector coding of local image descriptors. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part V. LNCS, vol. 6315, pp. 141-154. Springer, Heidelberg (2010)\n\nImproving the fisher kernel for large-scale image classification. F Perronnin, J S\u00e1nchez, T Mensink, ECCV 2010, Part IV. Daniilidis, K., Maragos, P., Paragios, N.HeidelbergSpringer6314Perronnin, F., S\u00e1nchez, J., Mensink, T.: Improving the fisher kernel for large-scale image classification. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part IV. LNCS, vol. 6314, pp. 143-156. Springer, Heidelberg (2010)\n\nImageNet: a large-scale hierarchical image database. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, CVPRDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: a large-scale hierarchical image database. In: CVPR (2009)\n\nCross-domain video concept detection using adaptive svms. J Yang, R Yan, A G Hauptmann, ACM MM. Yang, J., Yan, R., Hauptmann, A.G.: Cross-domain video concept detection using adaptive svms. In: ACM MM (2007)\n\nVisualrank: Applying pagerank to large-scale image search. Y Jing, S Baluja, IEEE Trans. on PAMI. Jing, Y., Baluja, S.: Visualrank: Applying pagerank to large-scale image search. IEEE Trans. on PAMI (2008)\n\nPFID: Pittsburgh fast-food image dataset. M Chen, K Dhingra, W Wu, L Yang, R Sukthankar, J Yang, ICIP. Chen, M., Dhingra, K., Wu, W., Yang, L., Sukthankar, R., Yang, J.: PFID: Pitts- burgh fast-food image dataset. In: ICIP, pp. 289-292 (2009)\n\nRapid mobile food recognition using fisher vector. Y Kawano, K Yanai, ACPRKawano, Y., Yanai, K.: Rapid mobile food recognition using fisher vector. In: ACPR (2013)\n\nProbabilistic web image gathering. K Yanai, K Barnard, ACM SIGMM WS Multimedia Information Retrieval. Yanai, K., Barnard, K.: Probabilistic web image gathering. In: ACM SIGMM WS Multimedia Information Retrieval, pp. 57-64 (2005)\n\nHarvesting image databases from the web. F Schroff, A Criminisi, A Zisserman, ICCVSchroff, F., Criminisi, A., Zisserman, A.: Harvesting image databases from the web. In: ICCV (2007)\n\nLarge-scale live active learning: training object detectors with crawled data and crowds. S Vijayanarasimhan, K Grauman, CVPR. Vijayanarasimhan, S., Grauman, K.: Large-scale live active learning: training object detectors with crawled data and crowds. In: CVPR, pp. 1449-1456 (2011)\n\nSun attribute database: discovering, annotating, and recognizing scene attributes. G Patterson, J Hays, CVPR. Patterson, G., Hays, J.: Sun attribute database: discovering, annotating, and rec- ognizing scene attributes. In: CVPR, pp. 2751-2758 (2012)\n\nVisual recognition with humans in the loop. S Branson, C Wah, F Schroff, B Babenko, P Welinder, P Perona, S Belongie, Computer Vision -ECCV 2010. Daniilidis, K., Maragos, P., Paragios, N.HeidelbergSpringer6314Branson, S., Wah, C., Schroff, F., Babenko, B., Welinder, P., Perona, P., Belongie, S.: Visual recognition with humans in the loop. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) Computer Vision -ECCV 2010, vol. 6314, pp. 438-451. Springer, Heidelberg (2010)\n\nP Welinder, S Branson, T Mita, C Wah, F Schroff, S Belongie, P Perona, Caltech-ucsd birds 200. California Institute of TechnologyTechnical reportWelinder, P., Branson, S., Mita, T., Wah, C., Schroff, F., Belongie, S., Perona, P.: Caltech-ucsd birds 200. Technical report, California Institute of Technology (2010)\n\nLIBLINEAR: A library for large linear classification. S Maji, J Kannala, E Rahtu, M Blaschko, A Vedaldi, R E Fan, K W Chang, C J Hsieh, X R Wang, C J Lin, Oxford flower 102. 22Technical reportFine-grained visual classification of aircraftMaji, S., Kannala, J., Rahtu, E., Blaschko, M., Vedaldi, A.: Fine-grained visual classification of aircraft. Technical report, arXiv (2013) 22. : Oxford flower 102. http://www.robots.ox.ac.uk/ \u223c vgg/data/flowers/ 23. Fan, R.E., Chang, K.W., Hsieh, C.J., Wang, X.R., Lin, C.J.: LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research 9, 1871-1874 (2008)\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, CVPRDalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In: CVPR (2005)\n\nFisher kernels on visual vocabularies for image categorization. F Perronnin, C Dance, CVPRPerronnin, F., Dance, C.: Fisher kernels on visual vocabularies for image catego- rization. In: CVPR (2007)\n\nMeta-class features for large-scale object categorization on a budget. A Bergamo, L Torresani, CVPRBergamo, A., Torresani, L.: Meta-class features for large-scale object categorization on a budget. In: CVPR (2012)\n\nLarge-scale image retrieval with compressed fisher vectors. F Perronnin, Y Liu, J S\u00e1nchez, H Poirier, CVPRPerronnin, F., Liu, Y., S\u00e1nchez, J., Poirier, H.: Large-scale image retrieval with compressed fisher vectors. In: CVPR (2010)\n\nBeyond bags of features: spatial pyramid matching for recognizing natural scene categories. S Lazebnik, C Schmid, J Ponce, CVPR. IEEE2Lazebnik, S., Schmid, C., Ponce, J.: Beyond bags of features: spatial pyramid matching for recognizing natural scene categories. In: CVPR. vol. 2, pp. 2169- 2178. IEEE (2006)\n", "annotations": {"author": "[{\"end\":258,\"start\":101},{\"end\":408,\"start\":259}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":111},{\"end\":270,\"start\":265}]", "author_first_name": "[{\"end\":110,\"start\":101},{\"end\":264,\"start\":259}]", "author_affiliation": "[{\"end\":257,\"start\":145},{\"end\":407,\"start\":295}]", "title": "[{\"end\":98,\"start\":1},{\"end\":506,\"start\":409}]", "venue": null, "abstract": "[{\"end\":1561,\"start\":628}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2002,\"start\":1999},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2005,\"start\":2002},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2008,\"start\":2005},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2011,\"start\":2008},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2014,\"start\":2011},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2496,\"start\":2493},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2499,\"start\":2496},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2502,\"start\":2499},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2505,\"start\":2502},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2792,\"start\":2788},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4362,\"start\":4359},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5420,\"start\":5416},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5814,\"start\":5810},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6827,\"start\":6824},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6829,\"start\":6827},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6832,\"start\":6829},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6850,\"start\":6847},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6872,\"start\":6869},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6875,\"start\":6872},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8173,\"start\":8169},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8176,\"start\":8173},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8589,\"start\":8585},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8656,\"start\":8652},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8709,\"start\":8705},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8851,\"start\":8847},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9067,\"start\":9063},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9328,\"start\":9324},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9345,\"start\":9341},{\"end\":9364,\"start\":9360},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9605,\"start\":9601},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9650,\"start\":9646},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10542,\"start\":10538},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11306,\"start\":11302},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12165,\"start\":12161},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12340,\"start\":12336},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12593,\"start\":12590},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12596,\"start\":12593},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12970,\"start\":12966},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15005,\"start\":15001},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15164,\"start\":15160},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15210,\"start\":15206},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15734,\"start\":15730},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16190,\"start\":16187},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16193,\"start\":16190},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16709,\"start\":16705},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17092,\"start\":17088},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17195,\"start\":17191},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22916,\"start\":22913},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23607,\"start\":23603}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33206,\"start\":33154},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33907,\"start\":33207},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34032,\"start\":33908},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34099,\"start\":34033},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34922,\"start\":34100},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35737,\"start\":34923},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35925,\"start\":35738},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36440,\"start\":35926}]", "paragraph": "[{\"end\":2350,\"start\":1577},{\"end\":3094,\"start\":2352},{\"end\":3294,\"start\":3096},{\"end\":3767,\"start\":3296},{\"end\":4264,\"start\":3769},{\"end\":5034,\"start\":4266},{\"end\":6084,\"start\":5036},{\"end\":6133,\"start\":6086},{\"end\":6662,\"start\":6135},{\"end\":7447,\"start\":6680},{\"end\":7760,\"start\":7449},{\"end\":8350,\"start\":7762},{\"end\":8710,\"start\":8352},{\"end\":9225,\"start\":8712},{\"end\":9444,\"start\":9227},{\"end\":9868,\"start\":9446},{\"end\":10143,\"start\":9870},{\"end\":10369,\"start\":10163},{\"end\":10619,\"start\":10371},{\"end\":10912,\"start\":10621},{\"end\":11032,\"start\":10914},{\"end\":11750,\"start\":11034},{\"end\":11990,\"start\":11774},{\"end\":12819,\"start\":11992},{\"end\":13305,\"start\":12821},{\"end\":13788,\"start\":13307},{\"end\":14545,\"start\":13790},{\"end\":15078,\"start\":14581},{\"end\":15660,\"start\":15080},{\"end\":16402,\"start\":15676},{\"end\":16993,\"start\":16445},{\"end\":17093,\"start\":17023},{\"end\":18001,\"start\":17159},{\"end\":18551,\"start\":18003},{\"end\":19288,\"start\":18553},{\"end\":19563,\"start\":19290},{\"end\":20079,\"start\":19582},{\"end\":20376,\"start\":20081},{\"end\":21090,\"start\":20378},{\"end\":21723,\"start\":21092},{\"end\":22377,\"start\":21725},{\"end\":22506,\"start\":22393},{\"end\":22740,\"start\":22508},{\"end\":23256,\"start\":22742},{\"end\":23673,\"start\":23258},{\"end\":24118,\"start\":23675},{\"end\":25446,\"start\":24160},{\"end\":25687,\"start\":25448},{\"end\":26107,\"start\":25689},{\"end\":26546,\"start\":26109},{\"end\":27111,\"start\":26548},{\"end\":28316,\"start\":27113},{\"end\":28499,\"start\":28367},{\"end\":29193,\"start\":28501},{\"end\":30567,\"start\":29231},{\"end\":30999,\"start\":30569},{\"end\":32401,\"start\":31001},{\"end\":32873,\"start\":32417},{\"end\":33153,\"start\":32875}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16444,\"start\":16403},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17022,\"start\":16994}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1575,\"start\":1563},{\"attributes\":{\"n\":\"2\"},\"end\":6678,\"start\":6665},{\"attributes\":{\"n\":\"3\"},\"end\":10161,\"start\":10146},{\"attributes\":{\"n\":\"3.1\"},\"end\":11772,\"start\":11753},{\"attributes\":{\"n\":\"3.2\"},\"end\":14579,\"start\":14548},{\"end\":15674,\"start\":15663},{\"end\":17157,\"start\":17096},{\"attributes\":{\"n\":\"3.3\"},\"end\":19580,\"start\":19566},{\"attributes\":{\"n\":\"4\"},\"end\":22391,\"start\":22380},{\"attributes\":{\"n\":\"4.1\"},\"end\":24158,\"start\":24121},{\"attributes\":{\"n\":\"4.2\"},\"end\":28365,\"start\":28319},{\"end\":29229,\"start\":29196},{\"attributes\":{\"n\":\"5\"},\"end\":32415,\"start\":32404},{\"end\":33163,\"start\":33155},{\"end\":33209,\"start\":33208},{\"end\":33917,\"start\":33909},{\"end\":34110,\"start\":34101},{\"end\":34933,\"start\":34924},{\"end\":35748,\"start\":35739},{\"end\":35945,\"start\":35927}]", "table": "[{\"end\":34922,\"start\":34149},{\"end\":35737,\"start\":35035},{\"end\":35925,\"start\":35808},{\"end\":36440,\"start\":36179}]", "figure_caption": "[{\"end\":33206,\"start\":33165},{\"end\":33907,\"start\":33210},{\"end\":34032,\"start\":33919},{\"end\":34099,\"start\":34035},{\"end\":34149,\"start\":34112},{\"end\":35035,\"start\":34935},{\"end\":35808,\"start\":35750},{\"end\":36179,\"start\":35948}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10977,\"start\":10972},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27477,\"start\":27472}]", "bib_author_first_name": "[{\"end\":36505,\"start\":36504},{\"end\":36513,\"start\":36512},{\"end\":36521,\"start\":36520},{\"end\":36534,\"start\":36533},{\"end\":36747,\"start\":36746},{\"end\":36755,\"start\":36754},{\"end\":36763,\"start\":36762},{\"end\":36769,\"start\":36768},{\"end\":36777,\"start\":36776},{\"end\":36784,\"start\":36783},{\"end\":36793,\"start\":36792},{\"end\":36800,\"start\":36799},{\"end\":37124,\"start\":37123},{\"end\":37133,\"start\":37132},{\"end\":37140,\"start\":37139},{\"end\":37150,\"start\":37149},{\"end\":37152,\"start\":37151},{\"end\":37163,\"start\":37162},{\"end\":37165,\"start\":37164},{\"end\":37416,\"start\":37415},{\"end\":37427,\"start\":37426},{\"end\":37603,\"start\":37602},{\"end\":37613,\"start\":37612},{\"end\":37945,\"start\":37944},{\"end\":37956,\"start\":37955},{\"end\":38179,\"start\":38178},{\"end\":38187,\"start\":38186},{\"end\":38195,\"start\":38194},{\"end\":38201,\"start\":38200},{\"end\":38207,\"start\":38206},{\"end\":38216,\"start\":38215},{\"end\":38455,\"start\":38454},{\"end\":38463,\"start\":38462},{\"end\":38469,\"start\":38468},{\"end\":38478,\"start\":38477},{\"end\":38480,\"start\":38479},{\"end\":38885,\"start\":38884},{\"end\":38898,\"start\":38897},{\"end\":38909,\"start\":38908},{\"end\":39295,\"start\":39294},{\"end\":39303,\"start\":39302},{\"end\":39311,\"start\":39310},{\"end\":39321,\"start\":39320},{\"end\":39323,\"start\":39322},{\"end\":39329,\"start\":39328},{\"end\":39335,\"start\":39334},{\"end\":39541,\"start\":39540},{\"end\":39549,\"start\":39548},{\"end\":39556,\"start\":39555},{\"end\":39558,\"start\":39557},{\"end\":39751,\"start\":39750},{\"end\":39759,\"start\":39758},{\"end\":39941,\"start\":39940},{\"end\":39949,\"start\":39948},{\"end\":39960,\"start\":39959},{\"end\":39966,\"start\":39965},{\"end\":39974,\"start\":39973},{\"end\":39988,\"start\":39987},{\"end\":40194,\"start\":40193},{\"end\":40204,\"start\":40203},{\"end\":40343,\"start\":40342},{\"end\":40352,\"start\":40351},{\"end\":40579,\"start\":40578},{\"end\":40590,\"start\":40589},{\"end\":40603,\"start\":40602},{\"end\":40811,\"start\":40810},{\"end\":40831,\"start\":40830},{\"end\":41088,\"start\":41087},{\"end\":41101,\"start\":41100},{\"end\":41301,\"start\":41300},{\"end\":41312,\"start\":41311},{\"end\":41319,\"start\":41318},{\"end\":41330,\"start\":41329},{\"end\":41341,\"start\":41340},{\"end\":41353,\"start\":41352},{\"end\":41363,\"start\":41362},{\"end\":41732,\"start\":41731},{\"end\":41744,\"start\":41743},{\"end\":41755,\"start\":41754},{\"end\":41763,\"start\":41762},{\"end\":41770,\"start\":41769},{\"end\":41781,\"start\":41780},{\"end\":41793,\"start\":41792},{\"end\":42101,\"start\":42100},{\"end\":42109,\"start\":42108},{\"end\":42120,\"start\":42119},{\"end\":42129,\"start\":42128},{\"end\":42141,\"start\":42140},{\"end\":42152,\"start\":42151},{\"end\":42154,\"start\":42153},{\"end\":42161,\"start\":42160},{\"end\":42163,\"start\":42162},{\"end\":42172,\"start\":42171},{\"end\":42174,\"start\":42173},{\"end\":42183,\"start\":42182},{\"end\":42185,\"start\":42184},{\"end\":42193,\"start\":42192},{\"end\":42195,\"start\":42194},{\"end\":42732,\"start\":42731},{\"end\":42741,\"start\":42740},{\"end\":42913,\"start\":42912},{\"end\":42926,\"start\":42925},{\"end\":43119,\"start\":43118},{\"end\":43130,\"start\":43129},{\"end\":43323,\"start\":43322},{\"end\":43336,\"start\":43335},{\"end\":43343,\"start\":43342},{\"end\":43354,\"start\":43353},{\"end\":43588,\"start\":43587},{\"end\":43600,\"start\":43599},{\"end\":43610,\"start\":43609}]", "bib_author_last_name": "[{\"end\":36510,\"start\":36506},{\"end\":36518,\"start\":36514},{\"end\":36531,\"start\":36522},{\"end\":36545,\"start\":36535},{\"end\":36752,\"start\":36748},{\"end\":36760,\"start\":36756},{\"end\":36766,\"start\":36764},{\"end\":36774,\"start\":36770},{\"end\":36781,\"start\":36778},{\"end\":36790,\"start\":36785},{\"end\":36797,\"start\":36794},{\"end\":36809,\"start\":36801},{\"end\":37130,\"start\":37125},{\"end\":37137,\"start\":37134},{\"end\":37147,\"start\":37141},{\"end\":37160,\"start\":37153},{\"end\":37170,\"start\":37166},{\"end\":37424,\"start\":37417},{\"end\":37433,\"start\":37428},{\"end\":37610,\"start\":37604},{\"end\":37619,\"start\":37614},{\"end\":37953,\"start\":37946},{\"end\":37966,\"start\":37957},{\"end\":38184,\"start\":38180},{\"end\":38192,\"start\":38188},{\"end\":38198,\"start\":38196},{\"end\":38204,\"start\":38202},{\"end\":38213,\"start\":38208},{\"end\":38221,\"start\":38217},{\"end\":38460,\"start\":38456},{\"end\":38466,\"start\":38464},{\"end\":38475,\"start\":38470},{\"end\":38486,\"start\":38481},{\"end\":38895,\"start\":38886},{\"end\":38906,\"start\":38899},{\"end\":38917,\"start\":38910},{\"end\":39300,\"start\":39296},{\"end\":39308,\"start\":39304},{\"end\":39318,\"start\":39312},{\"end\":39326,\"start\":39324},{\"end\":39332,\"start\":39330},{\"end\":39343,\"start\":39336},{\"end\":39546,\"start\":39542},{\"end\":39553,\"start\":39550},{\"end\":39568,\"start\":39559},{\"end\":39756,\"start\":39752},{\"end\":39766,\"start\":39760},{\"end\":39946,\"start\":39942},{\"end\":39957,\"start\":39950},{\"end\":39963,\"start\":39961},{\"end\":39971,\"start\":39967},{\"end\":39985,\"start\":39975},{\"end\":39993,\"start\":39989},{\"end\":40201,\"start\":40195},{\"end\":40210,\"start\":40205},{\"end\":40349,\"start\":40344},{\"end\":40360,\"start\":40353},{\"end\":40587,\"start\":40580},{\"end\":40600,\"start\":40591},{\"end\":40613,\"start\":40604},{\"end\":40828,\"start\":40812},{\"end\":40839,\"start\":40832},{\"end\":41098,\"start\":41089},{\"end\":41106,\"start\":41102},{\"end\":41309,\"start\":41302},{\"end\":41316,\"start\":41313},{\"end\":41327,\"start\":41320},{\"end\":41338,\"start\":41331},{\"end\":41350,\"start\":41342},{\"end\":41360,\"start\":41354},{\"end\":41372,\"start\":41364},{\"end\":41741,\"start\":41733},{\"end\":41752,\"start\":41745},{\"end\":41760,\"start\":41756},{\"end\":41767,\"start\":41764},{\"end\":41778,\"start\":41771},{\"end\":41790,\"start\":41782},{\"end\":41800,\"start\":41794},{\"end\":42106,\"start\":42102},{\"end\":42117,\"start\":42110},{\"end\":42126,\"start\":42121},{\"end\":42138,\"start\":42130},{\"end\":42149,\"start\":42142},{\"end\":42158,\"start\":42155},{\"end\":42169,\"start\":42164},{\"end\":42180,\"start\":42175},{\"end\":42190,\"start\":42186},{\"end\":42199,\"start\":42196},{\"end\":42738,\"start\":42733},{\"end\":42748,\"start\":42742},{\"end\":42923,\"start\":42914},{\"end\":42932,\"start\":42927},{\"end\":43127,\"start\":43120},{\"end\":43140,\"start\":43131},{\"end\":43333,\"start\":43324},{\"end\":43340,\"start\":43337},{\"end\":43351,\"start\":43344},{\"end\":43362,\"start\":43355},{\"end\":43597,\"start\":43589},{\"end\":43607,\"start\":43601},{\"end\":43616,\"start\":43611}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":36681,\"start\":36442},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14987220},\"end\":37038,\"start\":36683},{\"attributes\":{\"id\":\"b2\"},\"end\":37333,\"start\":37040},{\"attributes\":{\"id\":\"b3\"},\"end\":37558,\"start\":37335},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":12472196},\"end\":37888,\"start\":37560},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1440386},\"end\":38115,\"start\":37890},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6718692},\"end\":38377,\"start\":38117},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7405065},\"end\":38816,\"start\":38379},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":10402702},\"end\":39239,\"start\":38818},{\"attributes\":{\"id\":\"b9\"},\"end\":39480,\"start\":39241},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7263397},\"end\":39689,\"start\":39482},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10545157},\"end\":39896,\"start\":39691},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1548631},\"end\":40140,\"start\":39898},{\"attributes\":{\"id\":\"b13\"},\"end\":40305,\"start\":40142},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5225665},\"end\":40535,\"start\":40307},{\"attributes\":{\"id\":\"b15\"},\"end\":40718,\"start\":40537},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1721452},\"end\":41002,\"start\":40720},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8724974},\"end\":41254,\"start\":41004},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16647912},\"end\":41729,\"start\":41256},{\"attributes\":{\"id\":\"b19\"},\"end\":42044,\"start\":41731},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3116168},\"end\":42675,\"start\":42046},{\"attributes\":{\"id\":\"b21\"},\"end\":42846,\"start\":42677},{\"attributes\":{\"id\":\"b22\"},\"end\":43045,\"start\":42848},{\"attributes\":{\"id\":\"b23\"},\"end\":43260,\"start\":43047},{\"attributes\":{\"id\":\"b24\"},\"end\":43493,\"start\":43262},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2421251},\"end\":43803,\"start\":43495}]", "bib_title": "[{\"end\":36744,\"start\":36683},{\"end\":37600,\"start\":37560},{\"end\":37942,\"start\":37890},{\"end\":38176,\"start\":38117},{\"end\":38452,\"start\":38379},{\"end\":38882,\"start\":38818},{\"end\":39538,\"start\":39482},{\"end\":39748,\"start\":39691},{\"end\":39938,\"start\":39898},{\"end\":40340,\"start\":40307},{\"end\":40808,\"start\":40720},{\"end\":41085,\"start\":41004},{\"end\":41298,\"start\":41256},{\"end\":42098,\"start\":42046},{\"end\":43585,\"start\":43495}]", "bib_author": "[{\"end\":36512,\"start\":36504},{\"end\":36520,\"start\":36512},{\"end\":36533,\"start\":36520},{\"end\":36547,\"start\":36533},{\"end\":36754,\"start\":36746},{\"end\":36762,\"start\":36754},{\"end\":36768,\"start\":36762},{\"end\":36776,\"start\":36768},{\"end\":36783,\"start\":36776},{\"end\":36792,\"start\":36783},{\"end\":36799,\"start\":36792},{\"end\":36811,\"start\":36799},{\"end\":37132,\"start\":37123},{\"end\":37139,\"start\":37132},{\"end\":37149,\"start\":37139},{\"end\":37162,\"start\":37149},{\"end\":37172,\"start\":37162},{\"end\":37426,\"start\":37415},{\"end\":37435,\"start\":37426},{\"end\":37612,\"start\":37602},{\"end\":37621,\"start\":37612},{\"end\":37955,\"start\":37944},{\"end\":37968,\"start\":37955},{\"end\":38186,\"start\":38178},{\"end\":38194,\"start\":38186},{\"end\":38200,\"start\":38194},{\"end\":38206,\"start\":38200},{\"end\":38215,\"start\":38206},{\"end\":38223,\"start\":38215},{\"end\":38462,\"start\":38454},{\"end\":38468,\"start\":38462},{\"end\":38477,\"start\":38468},{\"end\":38488,\"start\":38477},{\"end\":38897,\"start\":38884},{\"end\":38908,\"start\":38897},{\"end\":38919,\"start\":38908},{\"end\":39302,\"start\":39294},{\"end\":39310,\"start\":39302},{\"end\":39320,\"start\":39310},{\"end\":39328,\"start\":39320},{\"end\":39334,\"start\":39328},{\"end\":39345,\"start\":39334},{\"end\":39548,\"start\":39540},{\"end\":39555,\"start\":39548},{\"end\":39570,\"start\":39555},{\"end\":39758,\"start\":39750},{\"end\":39768,\"start\":39758},{\"end\":39948,\"start\":39940},{\"end\":39959,\"start\":39948},{\"end\":39965,\"start\":39959},{\"end\":39973,\"start\":39965},{\"end\":39987,\"start\":39973},{\"end\":39995,\"start\":39987},{\"end\":40203,\"start\":40193},{\"end\":40212,\"start\":40203},{\"end\":40351,\"start\":40342},{\"end\":40362,\"start\":40351},{\"end\":40589,\"start\":40578},{\"end\":40602,\"start\":40589},{\"end\":40615,\"start\":40602},{\"end\":40830,\"start\":40810},{\"end\":40841,\"start\":40830},{\"end\":41100,\"start\":41087},{\"end\":41108,\"start\":41100},{\"end\":41311,\"start\":41300},{\"end\":41318,\"start\":41311},{\"end\":41329,\"start\":41318},{\"end\":41340,\"start\":41329},{\"end\":41352,\"start\":41340},{\"end\":41362,\"start\":41352},{\"end\":41374,\"start\":41362},{\"end\":41743,\"start\":41731},{\"end\":41754,\"start\":41743},{\"end\":41762,\"start\":41754},{\"end\":41769,\"start\":41762},{\"end\":41780,\"start\":41769},{\"end\":41792,\"start\":41780},{\"end\":41802,\"start\":41792},{\"end\":42108,\"start\":42100},{\"end\":42119,\"start\":42108},{\"end\":42128,\"start\":42119},{\"end\":42140,\"start\":42128},{\"end\":42151,\"start\":42140},{\"end\":42160,\"start\":42151},{\"end\":42171,\"start\":42160},{\"end\":42182,\"start\":42171},{\"end\":42192,\"start\":42182},{\"end\":42201,\"start\":42192},{\"end\":42740,\"start\":42731},{\"end\":42750,\"start\":42740},{\"end\":42925,\"start\":42912},{\"end\":42934,\"start\":42925},{\"end\":43129,\"start\":43118},{\"end\":43142,\"start\":43129},{\"end\":43335,\"start\":43322},{\"end\":43342,\"start\":43335},{\"end\":43353,\"start\":43342},{\"end\":43364,\"start\":43353},{\"end\":43599,\"start\":43587},{\"end\":43609,\"start\":43599},{\"end\":43618,\"start\":43609}]", "bib_venue": "[{\"end\":36502,\"start\":36442},{\"end\":36846,\"start\":36811},{\"end\":37121,\"start\":37040},{\"end\":37413,\"start\":37335},{\"end\":37686,\"start\":37621},{\"end\":37987,\"start\":37968},{\"end\":38227,\"start\":38223},{\"end\":38505,\"start\":38488},{\"end\":38937,\"start\":38919},{\"end\":39292,\"start\":39241},{\"end\":39576,\"start\":39570},{\"end\":39787,\"start\":39768},{\"end\":39999,\"start\":39995},{\"end\":40191,\"start\":40142},{\"end\":40407,\"start\":40362},{\"end\":40576,\"start\":40537},{\"end\":40845,\"start\":40841},{\"end\":41112,\"start\":41108},{\"end\":41400,\"start\":41374},{\"end\":41824,\"start\":41802},{\"end\":42218,\"start\":42201},{\"end\":42729,\"start\":42677},{\"end\":42910,\"start\":42848},{\"end\":43116,\"start\":43047},{\"end\":43320,\"start\":43262},{\"end\":43622,\"start\":43618},{\"end\":37747,\"start\":37688},{\"end\":38558,\"start\":38548},{\"end\":38990,\"start\":38980},{\"end\":41453,\"start\":41443}]"}}}, "year": 2023, "month": 12, "day": 17}