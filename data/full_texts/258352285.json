{"id": 258352285, "updated": "2023-10-05 01:53:50.552", "metadata": {"title": "Large Language Models are Strong Zero-Shot Retriever", "authors": "[{\"first\":\"Tao\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Guodong\",\"last\":\"Long\",\"middle\":[]},{\"first\":\"Xiubo\",\"last\":\"Geng\",\"middle\":[]},{\"first\":\"Chongyang\",\"last\":\"Tao\",\"middle\":[]},{\"first\":\"Tianyi\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Daxin\",\"last\":\"Jiang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "In this work, we propose a simple method that applies a large language model (LLM) to large-scale retrieval in zero-shot scenarios. Our method, the Language language model as Retriever (LameR), is built upon no other neural models but an LLM, while breaking brute-force combinations of retrievers with LLMs and lifting the performance of zero-shot retrieval to be very competitive on benchmark datasets. Essentially, we propose to augment a query with its potential answers by prompting LLMs with a composition of the query and the query's in-domain candidates. The candidates, regardless of correct or wrong, are obtained by a vanilla retrieval procedure on the target collection. As a part of the prompts, they are likely to help LLM generate more precise answers by pattern imitation or candidate summarization. Even if all the candidates are wrong, the prompts at least make LLM aware of in-collection patterns and genres. Moreover, due to the low performance of a self-supervised retriever, the LLM-based query augmentation becomes less effective as the retriever bottlenecks the whole pipeline. Therefore, we propose to leverage a non-parametric lexicon-based method (e.g., BM25) as the retrieval module to capture query-document overlap in a literal fashion. As such, LameR makes the retrieval procedure transparent to the LLM, thus circumventing the performance bottleneck.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2304.14233", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2304-14233", "doi": "10.48550/arxiv.2304.14233"}}, "content": {"source": {"pdf_hash": "718989761d0dc0f97727470f0dc23de7ea48c26d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.14233v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3423ce3a8fc121499b7978ac706f1d40a178a6a2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/718989761d0dc0f97727470f0dc23de7ea48c26d.txt", "contents": "\nLarge Language Models are Strong Zero-Shot Retriever\n\n\nTao Shen \nSchool of CS\nFEIT, UTS\nUniversity of Maryland\n\n\nGuodong Long guodong.long@uts.edu.aumicrosoftcooperation \nSchool of CS\nFEIT, UTS\nUniversity of Maryland\n\n\nXiubo Geng \nSchool of CS\nFEIT, UTS\nUniversity of Maryland\n\n\nChongyang Tao \nSchool of CS\nFEIT, UTS\nUniversity of Maryland\n\n\nDaxinTianyi Zhou zhou@umiacs.umd.edu \nSchool of CS\nFEIT, UTS\nUniversity of Maryland\n\n\nJiang Aaii \nSchool of CS\nFEIT, UTS\nUniversity of Maryland\n\n\nLarge Language Models are Strong Zero-Shot Retriever\n\nIn this work, we propose a simple method that applies a large language model (LLM) to large-scale retrieval in zero-shot scenarios. Our method, the Language language model as Retriever(LameR), is built upon no other neural models but an LLM, while breaking brute-force combinations of retrievers with LLMs and lifting the performance of zero-shot retrieval to be very competitive on benchmark datasets. Essentially, we propose to augment a query with its potential answers by prompting LLMs with a composition of the query and the query's in-domain candidates. The candidates, regardless of correct or wrong, are obtained by a vanilla retrieval procedure on the target collection. As a part of the prompts, they are likely to help LLM generate more precise answers by pattern imitation or candidate summarization. Even if all the candidates are wrong, the prompts at least make LLM aware of in-collection patterns and genres. Moreover, due to the low performance of a self-supervised retriever, the LLM-based query augmentation becomes less effective as the retriever bottlenecks the whole pipeline. Therefore, we propose to leverage a non-parametric lexicon-based method (e.g., BM25) as the retrieval module to capture query-document overlap in a literal fashion. As such, LameR makes the retrieval procedure transparent to the LLM, thus circumventing the performance bottleneck.Preprint. Under review.\n\nIntroduction\n\nLarge-scale retrieval, a.k.a. first-stage retrieval, is to fetch top relevant documents for a given text query from a huge collection with millions to billions of entries. It is indispensable in informationseeking tasks, such as open-domain question answering [5], web search [31], knowledge-grounded dialogue [43], etc. Recently, it is also leveraged as a core retrieval-augmenting module to enrich large language models (LLMs) with up-to-date or domain-specific knowledge [13,36], which reduces the hallucination problem [32] and improves the faithfulness of generated texts [14]. Thereby, large-scale retrieval is a long-term research problem, attracting research efforts from academia and industry.\n\nIn the last decade, large-scale retrieval relies heavily on deep representation learning techniques, from bag-of-words (BoW) [22] to pre-trained language models (PLMs) [9]. Compared to supervised representation learning [17,41] that requires labor-intensive annotations on query-document pairs, self-supervised (or zero-shot) learning [18,26,15,24] on in-domain pseudo pairs corpora can be readily generalized to any corpora without human-crafted annotations. Nonetheless, the zeroshot retriever usually results in an inferior retrieval quality [44], even worse than a non-parametric term-based BM25 retrieval [34,44].\n\nFortunately, recent surging LLMs provide a shortcut to reach zero-shot retrieval by augmenting a query with its potential answering elicited from the LLMs [12]. Coupled with a self-supervised retriever, Contriever [15], it delivers superior retrieval performance, even surpassing a number of supervisedly fine-tuned retrievers. But, such a brute-force combination of a self-supervised retriever with a versatile LLM leads to a major problem. The answer elicitation is merely based on prompting LLMs with short, intent-ambiguous, and domain-vague retrieval queries. Due to the ambiguity of user queries and unawareness of in-domain corpora, the LLMs are likely to generate spurious and out-of-domain answers to the queries [1], making the query augmentation even more toxic.\n\nTo circumvent this issue, we propose a brand-new and simple paradigm for large-scale retrieval, called LameR. Essentially, during eliciting LLMs for answers to a query, we inject the query's top answer candidates into the prompt, where the candidates are obtained by applying a vanilla retrieval procedure to the query. As such, the LLMs are prone to distinguish and imitate the candidates [3], while summarizing or/and re-writing new ones with internal knowledge of the LLMs. Moreover, despite correct or wrong candidates, they can at least provide demonstrations about in-domain patterns and knowledge [23,40,21].\n\nFurthermore, though the LLMs now generate more precise, and reliable query augmentations, the whole pipeline is likely to be bottlenecked by the weak retriever trained on pseudo data in a selfsupervised manner. Therefore, we also propose to get rid of any learnable parametric retrievers, while opting for non-parametric term-or lexicon-based retrieval methods (e.g., BM25 in our experiments) in our LameR. In contrast to model-specific compressed and/or latent embeddings from a deep retriever, the lexicon-based retrieval methods capture lexicon overlap between augmented queries and in-collection documents in a literal fashion, thus taking the outputs of LLMs in a transparent mode and bypassing the performance bottleneck problem.\n\nWe evaluate our LameR on several benchmark datasets of large-scale retrieval by following Gao et al. [12]. Our results show that our proposed method achieves the best retrieval qualities on most datasets compared to other zero-shot competitors. Meantime, it can surpass the LLM-based retriever with in-context labeled demonstrations and outperform the baseline retrievers fine-tuned on full datasets.\n\n\nRelated Work\n\nZero-Shot Large-scale Retrieval. In the last years, many research efforts have been dedicated to zero-short retrieval due to its independence of labor-intensive query-document annotations. In contrast to zero-shot transfer that supervisedly trains a retriever in one domain and then evaluates it in another domain [34], we focus on an extremer scenario where no supervised data but the raw target collection is accessible. To handle this scenario, previous works construct pseudo query-document pairs from a target retrieval collection, such as inverse cloze task [18], hyperlink prediction [44], bottlenecked autoencoder [31], etc. Given the mined pseudo pairs, they train a retriever upon pretrained language models, e.g., BERT and RoBERTa, via contrastive learning with stochastic negatives. However, the self-supervised retrievers are only comparable to the lightweight non-parametric termor lexicon-based retrievers, e.g., BM25 [28]. Even equipped with LLM-based augmentation [12], the self-supervised retrievers still lag behind the retrievers fine-tuned on supervised data. In this work, we discard the inferior self-supervised retrievers but choose the highly generalizable non-parametric retrievers, and propose a brand-new method that integrates LLMs into zero-shot retrieval.\n\nIn-context Learning (ICL). LLMs can be adapted to new tasks by learning input-label pairs (a.k.a. demonstrations) provided in context, without updates of parameters, which is dubbed in-context learning [3]. Furthermore, some works seek better in-context demonstrations through retrieval, based on an observation that the demonstrations close to the test input help ICL more effectively [20,29]. Empirically, ICL, with several demonstrations, remarkably outperforms zero-shot methods across a broad spectrum of tasks, however of a prerequisite for mandatory few-shot examples. Fortunately, recent works [40,27,23] suggest ICL demonstrations are mainly used to specify input-label domains and formats of the target task, rather than supervision signal only. Sharing a similar inspiration with these works, especially Z-ICL [21], we leverage a retriever for unsupervised demonstrations from a huge collection to specify the domain, intent, and unit. However, we stand with a clean-cut motivation: as we exactly target the retrieval task, the retrieved demonstrations are potential labels (answers), orthogonal to retrieving inputs in previous works [21,39]. As such, the demonstrations are likely to help generate correct answers by correction or/and summarization with a boosting inspiration.\n\nRetrieval & Rerank Pipeline. Our two-stage procedure is similar to the retrieval & rerank pipeline [4]. The retrieval & rerank pipeline first employs a high-efficient retriever to fetch top candidates from a collection and then uses a heavy but effective ranker to rerank the candidates for more precise ranking outputs [11,45]. But, besides requiring supervised data to train both modules, the rerank module is constrained by the upstream retrieval module. In contrast, LameR always lets its retrieval module direct interact with the collection, free of constraint.\n\nLLM for Information Retrieval. Although an LLM can directly generate relevant documents and even the final answer for a user query upon its parametric memory, such a generative informationseeking approach is limited by: i) out-of-date corpora are learned in the parametric memory, ii) unreliable, and hallucinative text is frequently generated, and iii) the domain of generated text cannot be specified as demand. In contrast, information retrieval aims to provide in-domain and reliable documents relevant to user queries, which still dominates people's daily information-seeking methods. Therefore, many research efforts have recently been dedicated to applying large language models (LLMs), such as the GPT series, to information retrieval tasks for superior search performance. The majority of these works are in few-shot or zero-shot scenarios. Yu et al. [42] proposed a generatethen-read pipeline instead of the traditional retrieve-then-read pipeline. Dai et al. [8] introduced a few-shot dense retrieval approach for different tasks with different retrieval intents. Dua et al. [10] proposed a data augmentation method for domain adaptation for open-domain QA, where a document is passed to LLM for the generation of its possible queries. Gao et al. [12] focused on zero-shot dense retrieval using the Hypothetical Document Embedding (HyDE) method to generate potential answers by LLM as query augmentation. Jeronymo et al. [16] and Boytsov et al. [2] leveraged a fine-tuned ranker (on MS-MARCO in a supervised manner) to filter LLM-generated data for better query-document quality and thus superior performance. Saad-Falcon et al. [30] designed a two-stage LLM pipeline for zero-shot query generation and reranker-distilled retriever. Wang et al. [39] utilized a few-shot query-document demonstration to generate documents for a new query as the query's augmentation.\n\nUnlike these works, we focus on the zero-shot retrieval scenario, and neither conduct any in-domain data augmentation for domain-specific retriever training nor introduce any other retrieval or/and intermediate models except for a frozen LLM.\n\n\nObservations\n\nIn our pilot experiments, we observed the brute-force combination of a versatile LLM with weak retriever leads to certain demerits, which primarily motivates this work. Bottleneck by Self-supervised Retriever. Due to the weakness of a self-supervised dense retriever in representing capability, the whole pipeline is bottlenecked by the retriever, even though correct answers are likely to be generated by the strong LLM. As illustrated in Figure 1, strengthening LLMs in QA-style query augmentation (i.e., HyDE [12], which elicits an LLM to generate answers as query augmentation) hardly improves retrieval performance. Here, 'd003' and '3.5t' denote text-davinci-003 and gpt-3.5-turbo by OpenAI, respectively. Mismatch w/ Term-based Retriever. Due to unawareness of in-domain corpora, LLM is likely to generate out-of-domain answers to a given context-short and intent-vague query, making the query augmentation even toxic. Thanks to the fuzzy capability of dense retrievers, such query augmentation still bring remarkable improvement in search quality. However, when it comes to lexicon-based retrieval (say BM25), the improvement will be reduced due to out-of-domain augmentations. Quantitatively, as in Figure 2, 'Contriever' is a SoTA self-supervised dense retriever while 'BM25' is a representative lexicon-based retrieval. It is observed that although BM25 can beat Contriever in the vanilla setting, HyDE brings twice more improvement to Contriever than BM25, making BM25 less competitive.  Figure 3: Large language model as Retriever (LameR). Please see Table 1 for the prompt formulation.\n\n\nResults\n\n\n$ !\n\n\nRetrieve Retrieve\n\n\nLanguage Language Model as Retriever (LameR)\n\nThis section begins with a task definition, followed by elaborations on three components to achieve LameR -non-parametric lexicon-based retriever ( \u00a74.1), candidate-prompted answer generation ( \u00a74.2), and answer-augmented large-scale retriever ( \u00a74.3). LameR's pipeline is illustrated in Figure 3.\n\nTask Definition: Zero-Shot Large-Scale Retrieval. Providing a huge collection consisting of many documents,\nD = {d i } |D| i=1\n, the goal of 'large-scale retrieval' is to rank the whole D in descending order according to the relevance score between a given text query q and each d i . The relevance score is usually derived by a high-efficient retrieval model that operates on a pre-indexed |D| and an on-the-fly q to satisfy real-time requirements. Meantime, 'zero-shot' means that there is no training set with labeled positive query-document pairs for supervised representation learning.\n\n\nNon-parametric Lexicon-based Retriever\n\nTo tackle zero-shot retrieval, a recent trend is to train a deep encoder (e.g., BERT) over pseudo query-document pairs in a self-supervised manner, where the pairs are heuristically mined from the target collection D. Although the self-supervised learning process is required to especially repeat or/and design for every retrieval collection [18,44], the resulting retrieval performance is still not satisfactory in most cases, lagging far behind fully-supervised retrievers.\n\nIn contrast, the non-parametric term-or lexicon-based retrieval methods, e.g., TF-IDF and BM25 1 , are free of training heavy neural networks, but depend on lexicon overlap with considering term and document frequency of the lexicons. Even so, the simple BM25 retrieval method can outperform the self-supervised retriever in many cases in zero-shot retrieval [44,34].\n\nTherefore, in this work we leverage the BM25 method [28] to perform large-scale retrieval. The core idea of BM25 is to rank documents according to their relevance to a given query by incorporating term frequency and inverse document frequency. In brief, its relevance score between a document d \u2208 D and a query q is defined as\nRel BM25 (d, q)= t\u2208q IDF(t)\u00b7 TF(t, d) \u00b7 (k 1 + 1) TF(t, d)+k 1 \u00b7(1\u2212b+b\u00b7 len(d) avgdl ) , where IDF(t)=log N \u2212n(t)+0.5 n(t) + 0.5 .(1)\nHere, t denotes a lexicon term in q, TF(t, d) is the term frequency of t in document d, and IDF(t) is the inverse document frequency of term t, N = |D| is the total number of documents in the collection, n(t) is the number of documents containing term t, len(d) is the length of d, and avgdl is the average document length across the collection. In the remainder, we define a retrieval procedure a\u015d\nD q = Retriever(q, D, K).(2)\nD q is a list of top-K retrieval candidates of q in descending order w.r.t relevance scores, so |D q | = K. Remark. When employing a strong, non-tunable, generative model, e.g., LLM, for explicit text augmentations of a query, a lexicon-based retrieval method has its own merit in not only high efficiency, but taking the exact augmentations for retrieval without compressed embedding. Therefore, using the lexicon-based method exposes LLMs' outputs to the retrieval collection literally, making the retrieval module transparent to LLMs. By comparison, the neural encoder, trained on heuristically mined pseudo data in self-supervised, is too weak to model the LLM-augmented queries, leaving a performance bottleneck here (see \u00a73).\n\n\nCandidate-Prompted Answer Generation\n\nGiven a query q, we augment it with its answer(s) a elicited from an LLM, which has been proven effective in improving zero-shot retrieval quality [12,39]. How to conduct the elicitation remains an open question. For example, in a straightforward way, [12] propose to prompt an LLM with a composition of a QA instruction and the query. However, as the LLM can only receive a short, intent-ambiguous query, joined with a broad and general QA instruction, it is not well instructed by the prompt with both the intent and domain of a query, leading to less precise answers. [39] add few-shot query-document examples as in-context demonstrations to the prompt for more reasonable answers, which, however, is unavailable in zero-shot settings.\n\nInstead, we propose a new prompt schema, called candidate-prompted answer generation, for query augmentation in large-scale retrieval. As shown in Table 1, besides a task instruction and a retrieval query, a list of top answering candidates is also included in the prompt for elicitation of an LLM. Here, the top candidates are obtained by directly applying a vanilla retrieval process to the query via the retriever ( \u00a74.1). Formally, we first retrieve top-M candidates for q from the whole D by\nC q = Retriever(q, D, M ),(3)\nwhere M is usually very small (e.g., < 10) to reduce computation overhead for downstream modules. Then, to elicit knowledge from an LLM, we construct a prompt with C q and then invoke the LLM for answer generation, i.e.,\nA q = {a q 1 . . . a q N |a q \u223c LLM (p (t, q, C q ))}(4)\nwhere p(\u00b7) composes the prompt using task an instruction t, the query q, and the retrieved candidates C q (see Table 1 for an example and Appendix A for prompts of all tasks). It is noteworthy that we generate multiple (i.e., N ) answers by sampling outputs of the LLM, because we'd like to provide as many potential answers as we can to prevent the 'vocabulary mismatch' problem.\n\nAs such, LLM(\u00b7) utilizes the answering candidates C q in two aspects: i) If one or many gold documents of q existing in C q , LLM(\u00b7) serves like a re-ranker and generates the answers A q by both summarizing the correct documents from C q and eliciting internal parameterized knowledge. ii) Regardless of the correctness of C q , LLM(\u00b7) also receives in-collection answering information about intents, domains, and units, which are prone to help the LLM generate more precise answers A q .\n\n\nAnswer-Augmented Large-Scale Retrieval\n\nGiven the generated answers A q of q, we use them to augment q and produce a new queryq. Attributed to the non-parametric lexicon-based retriever, we can perform the query augmentation in a very straightforward way, which operates on plain text rather than latent embeddings. That is, we can easily concatenate every a q \u2208 A q with the original q, i.e., q = Concat(q, a q 1 , q, a q 2 , . . . , q, a q N ),\n\nwhere Concat denotes a concatenation operation in text. Lastly, we simply use the augmented query, q, to conduct a large-scale retrieval,Dq = Retriever(q, D, K),\n\nwhereDq is a list of final retrieved documents for query q and K = 1000 for metric calculation. Thanks to the high efficiency of the lexicon-based retriever with an inverted index, the augmentation would not cause catastrophic overhead increases, which is still faster than a dense retriever.\n\n\nExperiment\n\nIn this section, we will conduct extensive experimental evaluations of the proposed retrieval method and compare it with strong competitors.\n\nDatasets and Metrics. Following the datasets used by Gao et al. [12], we first employ the widelyused passage retrieval datasets, MS-MARCO [25] and report performance on TREC Deep Learning 2019 [6] and TREC Deep Learning 2020 [7] test sets (DL19 and DL20 for short, respectively). Meantime, we also evaluate our method on BEIR benchmark [34]. Here, we follow Gao et al. [12] to consider low-resource datasets from the BEIR dataset, so we employ six datasets, consisting of one fact-checking task (Scifact), one question-answering task (FiQA), one bio-medical IR task (TREC-COVID), one news retrieval task (TREC-NEWS), one argument retrieval task (ArguAna), and one entity retrieval task (DBPedia). Note that, as a zero-shot retrieval setting, we do not use any training query-document pairs but directly evaluate our method in the test sets. Following previous works, we report MAP, nDCG@10 and Recall@1000 (R@1k) for both TREC Deep Learning 2019 and TREC Deep Learning 2020. And nDCG@10 is reported for all the datasets in the BEIR benchmark.\n\nExperimental Setup. As for the large language model, we use gpt-3.5-turbo as the LLM to perform answer generation by default. Meantime, we also involve gpt-4 to investigate whether stronger LLM will bring more improvement. And, the number of candidates, M in Eq. (3), is set to 10 in our main results, and the number of generated answers, N in Eq.(4) is set to 5. To ensure efficiency, we truncate each of the queries and passages/documents to 128 tokens.\n\n\nBaselines and Competitors.\n\nAs we focus on the zero-shot retrieval setting, our main baselines fall into the retrieval methods without dependency on annotated query-document pairs (i.e., w/o relevance judgment). In particular, we use BM25 [28] and Contriever [15] as strong baselines for zero-shot lexicon and dense retrieval, respectively. And, we also include HyDE [12] as the state-of-the-art competitor for LLM-based retrieval. Furthermore, we also employ some baselines not in zero-shot settings to verify the effectiveness of our method. On the one hand, we leverage Q2D+BM25 [39] as a few-shot baseline (i.e., w/ few-shot relevance judgment), where in-context gold query-document pairs are provided to help LLM generate answers for a query. On the other hand, we consider some popular fully-supervised retrieval models (i.e., w/ relevance judgment), including DPR [17], ANCE [41], fine-tuned Contriever [15], etc.\n\n\nMain Evaluation\n\nDL19 and DL20 Test Sets. As shown in Table 2, we compare our LameR with its baselines and competitors in both TREC Deep Learning 2019 and 2020 test sets. It is observed that our method achieves the best performance in the zero-shot setting, significantly outperforming its strong competitor, HyDE 2 . This clearly verifies the effectiveness of our candidate-prompted answer generation. It is also noteworthy that our LameR is based on a much faster BM25 retriever, in contrast to the heavy dense retriever, Contriever, in HyDE. Meantime, compared to the method (Q2D BM25 ) with few-shot relevance judgment and the methods (DPR, etc.) with full relevance judgment, our proposed LameR achieves the best on most retrieval evaluation metrics.\n\nBEIR Benchmark. Furthermore, we compare our retrieval method with the others on six lowresource tasks from the BEIR dataset. As shown in Table 3, our proposed method performs best  on four out of six datasets. It should be highlighted that our LameR achieves superior performance on two TREC retrieval datasets, i.e., TREC-COVID and TREC-NEWS, which verify our proposed method in web information-seeking tasks. Meantime, We found our LameR delivers poor results on 'Argunan', a dataset designed to retrieve counter-argument passages from a collection. Since the queries and documents in the dataset are usually over-long (> 256), this is possibly caused by applying aggressive truncation (cap at 128) to the long queries and passages in the dataset. Besides, we also noticed that the performance of FiQA in zero-shot settings is far from that in the few-shot or fully-supervised settings. This may be caused by the lack of financial knowledge in general LLM.\n\n\nAblation Study and Further Analysis\n\nNumber of Retrieved Demos. First, we investigate whether the number of retrieved passages (as in-context demonstration) affects query augmentation and thus retrieval quality. As shown in Figure 4(a), increasing M > 0 consistently brings improvement in answer-augmented large-scale retrieval, and the improvement becomes marginal when the number exceeds 10. Considering that increasing M inevitably causes more computation overheads, we use M = 10 for a better trade-off between performance and efficiency. Besides, an interesting point is that LameR with M = 0 is surprisingly better than both i) HyDE, which verifies the effectiveness of our query augmentation coupled with BM25 retrieval, i.e., Eq.(5-6), and ii) LameR with M = 1, which is likely caused by low recall performance in top-1 and more severe interference of error candidates.\n\nNumber of Answer Generations. We also investigate whether the number of answers generated by LLM will affect the performance of our LameR. As shown in Figure 4(b), the performance of retrieval grows along with the number of generated answers, but becomes fluctuating and saturated when N > 5. Therefore, we use N = 5 as the default in our experiments. Schemes to Obtain Demo-passages. It is de facto to leverage top-10 retrieved passages as demonstrations as they are likely to provide pivot query-related knowledge in a limited context window of LLMs. To empirically check this intuition, we propose three schemes for demo-passages: i) As shown in Figure 4(d), the performance consistently drops when we increase the sample range because the related knowledge and correct demonstrations are weakened gradually. ii) As shown in Figure 4(c), we fetch 10 consecutive passages from different start indices in BM25 results. Surprisingly, there is a U-shaped curve, which can be explained by 'hard negatives' widely presenting in IR: Basically, hard negatives in top candidates challenge LLMs' distinguishing capability between positives and hard negatives. What's worse, with increasing start indices, the correct passages scarcely appear in the 10 consecutive passages, making the LLMs lose contrastive samples and get fooled by the negatives.\n\niii) More interestingly, as the '\u226b1k' in both Figure 4(c) & 4(d), randomly sampling 10 entries from the whole collection as demo-passages results in surprisingly high results. This is because they are focused on providing useful information about the knowledge domain (e.g., web, news, Wikipedia, scientific, arguments), task intent (e.g., dialogue, question answering), answering format (e.g., unit, length, pattern), etc., while free from hard negatives or spurious answers. Exploring Extremes of LameR. As LameR is built upon BM25 retrieval system, the lower bound of LameR would be BM25. Go beyond, it is interesting to find out the upper bound of LameR, which can demonstrate the extreme performance that LameR may deliver. As shown in Table 4, we conduct an experiment called 'LameR-oracle', where 10 demo-passages are instead obtained by gold query-document pairs in the labeled test set. It's seen that compared to our LameR w/ default settings (i.e., dflt), LameR-oracle performs much higher, verifying i) the importance of the correctness of demonstrated passages and ii) a great improvement room left for further research. As an initial exploration, we propose a brute-force attempt that a 2nd-round LameR is applied to the retrieval results by default LameR, but to our surprise, the performance even drops by absolute 1.0% nDCG@10 (see the last row of Table 4). Sharing inspirations with error reinforcement, the query augmented by an LLM (in the 1st round) is prone to return spurious passages that especially confuse the LLM (i.e., hardly distinguished), resulting in wrong answers to poison BM25. This suggests that in the future, we should focus more on introducing multiple retrieval methods to achieve diversity. Power of Stronger LLM. To further verify if our LameR will benefit from stronger LLM, we involve the bleeding-edge LLM, GPT-4, in our LameR framework and apply it to DL20 dataset as its results in the main evaluation with GPT-3.5 is not superior enough. As shown in Table 5, after applying GPT-4, our retrieval method achieves significantly high performance and beats all the competitors even with full relevance judgment.  Overheads w/ LLMs. Similar to HyDE [12] and Q2D [39], using LLMs to generate query augmentations inevitably leads to high computation overheads. Optimistically speaking, such inference-only overheads do not increase with the scale of retrieval collection, and a recent trend is to make smaller LLMs competitive [35,33], which would benefit these methods.\n\n\nEfficiency Analysis\n\nIn the future, we will explore specializing in a smaller LLM to generate query augmentations. Besides, in HyDE and our LameR, introducing LLMs makes the whole retrieval system free from heavy query-document annotations and outperforms fully-supervised baselines. Specifically, as few-shot Q2D and our zero-shot LameR use extra passages in contrast to zero-shot HyDE, they outperform HyDE significantly. Comparing zero-shot LameR with few-shot Q2D, with similar LLM's overheads (i.e., reducing our retrieved candidates), the LameR achieves 66.7% nDCG@10 on DL19, still surpassing Q2D.\n\nOverheads in Retrieval. Moving to overheads in retrieval, we compare BM25-based zero-shot LameR with its counterpart, HyDE, equipped with zero-shot dense retriever. As in Figure 5, benefiting from highly-efficient BM25, LameR, with much higher zero-shot retrieval performance, wins in both retrieval latency and index size.\n\n\nLameR meets Dense Retriever\n\nGiven promising results w/ a simple BM25, we explore replacing the 2nd-stage BM25 w/ an encoder for dense retrieval. Compared to Eq.(5), the dense embedding of an augmented query is derived b\u0233 q = 1/N \u00b7 l\u2208[1,N ] (Enc(q; \u03b8 (den) ) + Enc(a q l ); \u03b8 (den) )/2, where \u03b8 (den) parameterizes Enc(\u00b7).\n\nConsistency across Paradigms. Recall the results in \u00a73: Applying HyDE leads to inconsistent improvement on zero-shot dense retrieval (i.e., Contriever) and term-based retriever (i.e., BM25). So, we'd like to check if LameR can overcome this issue by considering in-domain demonstrations. As listed in Table 6(top), applying LameR to Contriever and BM25 results in similar improvement, verifying its effectiveness in query augmentation by demonstrating in-domain knowledge.\n\nLameR w/ SoTA Retriever. To exploit the performance extreme of LameR, we incorporate a SoTA dense retriever, SimLM [38]. As shown in Table 6(bottom), LameR SimLM significantly improves the SoTA performance on DL19 and DL20 and achieves the best effectiveness. Meantime, compared to Q2D SimLM , our LameR brings significantly higher improvement to SimLM than Q2D (by 3.6% and 4.2% on DL19 and DL20, respectively), not to mention Q2D relying on few-shot demonstration.\n\n6 Limitations.\n\ni) Instruction sensitivity: Identical to other prompt-based LLM applications, this work would also be sensitive to the instructions with different LLMs, which may consume a lot of human effort on prompt writing. ii) Computation Overheads: As stated in 5.3, although the 2-stage retrieval procedure in LameR is very fast by inheriting BM25, LameR is constrained by calling the LLM for answer generation in terms of computation overheads. To overcome these limitations, in the future we will explore specializing in a relatively smaller LLM for query-augmentation purposes.\n\n\nConclusion\n\nIn this work, we propose a retrieval method based merely on a large language model (LLM) and a simple BM25 algorithm, without any dependence on learnable retrieval models. As such, all the operations are performed in the consistent interface of natural language (i.e., language-based query augmentation and lexicon-overlap retrieval relevance), without the performance bottleneck of a fragile self-supervised model-based retriever. The extensive experimental evaluations verify the effectiveness of the proposed LameR, supporting that the large language model can solely serve as a strong retriever without any in-domain annotated query-document pairs.\n\nFigure 1\n1Figure 1: nDCG@10 on DL19 for query augmentation w/ LLMs.\n\nFigure 2 :\n2HyDE improving Dense and Term-based Retrieval.\n\nFigure 4 :\n4Hyperparameter explorations and ablation studies, where the data points in dashed rectangles denote our default choices. (a) The number of retrieved passages as in-context demonstration for answer generation, i.e., M in Eq.(3). (b)The number of generated answers as query augmentations for large-scale retrieval, i.e., N in Eq.(4). (c) and (d) depict the schemes to obtain the 10 demo-passages, where the first is to fetch 10 consecutive passages from a start index of the BM25-retrieved passages and the second is to randomly sample 10 passages from top-N passages. Note that '\u226b1k' denotes randomly sampling 10 passages from the whole collection.\n\nFigure 5 :\n5Efficiency of LameR with HyDE in retrieval latency (QPS) and index size (GB). Numbers for LameR sum overheads in two stages, and the variants for each system are achieved by changing generation number.\n\nTable 1 :\n1Our simple QA prompt to elicit knowledge from LLM for information retrieval in our LameR. Here, the entry with '{\u00b7}' represents a placeholder for the corresponding text. c q l \u2208 C q denotes a retrieved candidate. Please see Appendix A for the prompts for all datasets.Candidate-prompted Instruction.Give a question \"{q}\" and its possible answering passages (most of these \npassages are wrong) enumerated as: \\n 1.{c q \n1 } \\n 2.{c q \n2 } \\n 3.{c q \n3 } . . . \nplease write a correct answering passage. \n\n\n\nTable 2 :\n2Results for web search on DL19/20. Best performing w/o relevance judgment is marked bold. DPR, ANCE and Contriever FT are in-domain supervised models that are finetuned on MS-MARCO training data.TREC Deep Leaning 2019 \nTREC Deep Leaning 2020 \nMAP nDCG@10 R@1k MAP nDCG@10 R@1k \n\nw/o relevance judgment (zero-shot retrieval) \nBM25 \n30.1 \n50.6 \n75.0 \n28.6 \n48.0 \n78.6 \nContriever \n24.0 \n44.5 \n74.6 \n24.0 \n42.1 \n75.4 \nHyDE \n41.8 \n61.3 \n88.0 \n38.2 \n57.9 \n84.4 \nLameR (ours) \n47.2 \n69.1 \n89.9 \n45.6 \n64.8 \n88.7 \n\nw/ few-shot relevance judgment (few-shot ICL for answer generation) \nQ2DBM25 \n-\n66.2 \n-\n-\n62.9 \n-\n\nw/ relevance judgment (fully-supervised fine-tuning) \nDPR \n36.5 \n62.2 \n76.9 \n41.8 \n65.3 \n81.4 \nANCE \n37.1 \n64.5 \n75.5 \n40.8 \n64.6 \n77.6 \nContriever FT \n41.7 \n62.1 \n83.6 \n43.6 \n63.2 \n85.8 \n\n\n\nTable 3 :\n3Low resource tasks from BEIR. Best performing w/o relevance judgment are marked bold.nDCG@10 \nScifact Arguana Trec-COVID FiQA DBPedia TREC-NEWS \n\nw/o relevance judgment \nBM25 \n67.9 \n39.7 \n59.5 \n23.6 \n31.8 \n39.5 \nContriever \n64.9 \n37.9 \n27.3 \n24.5 \n29.2 \n34.8 \nHyDE \n69.1 \n46.6 \n59.3 \n27.3 \n36.8 \n44.0 \nLameR (ours) \n73.5 \n40.2 \n75.8 \n25.8 \n39.0 \n50.3 \n\nw/ few-shot relevance judgment \nQ2DBM25 \n68.6 \n-\n72.2 \n-\n37.0 \n-\n\nw/ relevance judgment \nDPR \n31.8 \n17.5 \n33.2 \n29.5 \n26.3 \n16.1 \nANCE \n50.7 \n41.5 \n65.4 \n30.0 \n28.1 \n38.2 \nContriever FT \n67.7 \n44.6 \n59.6 \n32.9 \n41.3 \n42.8 \n\n\n\nTable 4 :\n4Exploring extremes of LameR.DL19 \nMAP nDCG@10 R@1k \n\nBM25 \n30.1 \n50.6 \n75.0 \nLameR (dflt) 47.2 \n69.1 \n89.9 \nLameR-oracle 60.7 \n84.0 \n93.8 \n\n\u2662 2nd Round 46.7 \n68.1 \n87.5 \n\n\n\nTable 5 :\n5LameR with GPT4.DL20 \nnDCG@10 \nBM25 \n48.0 \nHyDE \n57.9 \nDPR (supv.) \n65.3 \nLameR GPT-3.5 \n64.8 \nLameR GPT-4 \n65.9 \n\n\n\nTable 6 :\n6Results on DL19/20. \u2020Equipping with our implemented SimLM[38]. We mark the 'absolute improvement over base retriever' in superscript for key methods. Ref: DPR[17], SimLM[38], and E5[37].TREC Deep Leaning Track 2019 TREC Deep Leaning Track 2020MAP nDCG@10 \nR@1k \nMAP nDCG@10 \nR@1k \n\nZero-shot Retriever \nBM25 \n30.1 \n50.6 \n75.0 \n28.6 \n48.0 \n78.6 \nLameRbm25 (zero-shot) \n47.2 \n69.1 +18.5 \n89.9 \n45.6 \n64.8 +16.8 \n88.7 \nContriever \n24.0 \n44.5 \n74.6 \n24.0 \n42.1 \n75.4 \nLameRContriever (zero-shot) \n41.1 \n64.3 +19.8 \n87.3 \n38.3 \n58.2 +16.1 \n85.5 \n\nFully-supervised Retriever \nContriever FT \n41.7 \n62.1 \n83.6 \n43.6 \n63.2 \n85.8 \nDPR \n36.5 \n62.2 \n76.9 \n41.8 \n65.3 \n81.4 \nSimLM \n-\n71.4 \n-\n-\n69.7 \n-\nE5base \n-\n74.3 \n-\n-\n70.7 \n-\n\nLLM-augmented Fully-supervised Retriever \nHyDE Contriever FT \n-\n67.4 \n-\n-\n63.5 \n-\nQ2DDPR \n-\n68.7 \n-\n-\n67.1 \n-\nQ2DSimLM \n-\n72.9 +1.5 \n-\n-\n71.6 +1.9 \n-\nQ2DE5 base \n-\n74.9 +0.6 \n-\n-\n72.5 +1.8 \n-\nLameRSimLM \u2020 \n54.9 \n76.5 +5.1 \n91.1 \n55.7 \n75.8 +6.1 \n89.5 \n\n\nAlthough the two hyper-parameters, i.e., k1 and b, in BM25 algorithm can be tuned, for example, by grid search, we do not seek to tune them but keep them in defaults, i.e., k1 = 0.9 and b = 0.4, in Pyserini[19].\nAlthough HyDE uses text-davinci-003 as its LLM, we found updating it with gpt-3.5-turbo leads to similar retrieval performance. SeeFigure 1for details.\nPrompt for DL19 and DL20.Give a question \"{q}\" and its possible answering passages (most of these passages are wrong) enumerated as: \\n 1.{c q 1 } \\n 2.{c q 2 } \\n 3.{c q 3 } . . . please write a correct answering passage.Prompt for scifact.Give a question \"{q}\" and its possible scientific paper passages (most of these passages are wrong) enumerated as: \\n 1.{c q 1 } \\n 2.{c q 2 } \\n 3.{c q 3 } . . . please write a correct scientific paper passage.Prompt for arguana.Give a question \"{q}\" and its possible counter-argument passages (most of these passages are wrong) enumerated as:\n. Akari Asai, Timo Schick, S H Patrick, Xilun Lewis, Gautier Chen, Sebastian Izacard, Hannaneh Riedel, Wen-Tau Hajishirzi, Yih, 10.48550/arXiv.2211.09260Task-aware retrieval with instructions. CoRR, abs/2211.09260, 2022Akari Asai, Timo Schick, Patrick S. H. Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. CoRR, abs/2211.09260, 2022. doi: 10.48550/arXiv.2211.09260. URL https://doi.org/10.48550/ arXiv.2211.09260.\n\nInpars-light: Cost-effective unsupervised training of efficient rankers. CoRR, abs/2301.02998. Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, 10.48550/arXiv.2301.029982023Ramya Ramanathan, and Eric NybergLeonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ra- manathan, and Eric Nyberg. Inpars-light: Cost-effective unsupervised training of effi- cient rankers. CoRR, abs/2301.02998, 2023. doi: 10.48550/arXiv.2301.02998. URL https://doi.org/10.48550/arXiv.2301.02998.\n\n. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel MTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\n\nLanguage models are few-shot learners. Jeffrey Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott Litwin, Benjamin Gray, Jack Chess, Christopher Clark, Sam Berner, Alec Mccandlish, Ilya Radford, Dario Sutskever, Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020. Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin2020Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learn- ers. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6- 12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n\nSemantic models for the first-stage retrieval: A comprehensive review. Yinqiong Cai, Yixing Fan, Jiafeng Guo, Fei Sun, Ruqing Zhang, Xueqi Cheng, abs/2103.04831CoRRYinqiong Cai, Yixing Fan, Jiafeng Guo, Fei Sun, Ruqing Zhang, and Xueqi Cheng. Semantic models for the first-stage retrieval: A comprehensive review. CoRR, abs/2103.04831, 2021. URL https://arxiv.org/abs/2103.04831.\n\nReading wikipedia to answer open-domain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, 10.18653/v1/P17-1171Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Regina Barzilay and Min-Yen Kanthe 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -August 4, Volume 1: Long Papers, pages 1870-1879. Association for Computational Linguistics, 2017. doi: 10.18653/v1/P17-1171. URL https://doi.org/10. 18653/v1/P17-1171.\n\nOverview of the TREC 2019 deep learning track. CoRR, abs. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M Voorhees, Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. Overview of the TREC 2019 deep learning track. CoRR, abs/2003.07820, 2020. URL https://arxiv.org/abs/2003.07820.\n\nOverview of the TREC 2020 deep learning track. CoRR, abs/2102.07662, 2021. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. Overview of the TREC 2020 deep learning track. CoRR, abs/2102.07662, 2021. URL https://arxiv.org/abs/ 2102.07662.\n\nPromptagator: Few-shot dense retrieval from 8 examples. CoRR, abs/2209.11755. Zhuyun Dai, Y Vincent, Ji Zhao, Yi Ma, Jianmo Luan, Jing Ni, Anton Lu, Kelvin Bakalov, Keith B Guu, Ming-Wei Hall, Chang, 10.48550/arXiv.2209.117552022Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. CoRR, abs/2209.11755, 2022. doi: 10.48550/arXiv.2209.11755. URL https: //doi.org/10.48550/arXiv.2209.11755.\n\nBERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Jill Burstein, Christy Doran, and Thamar Soloriothe 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171- 4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL https://doi.org/10.18653/v1/n19-1423.\n\nTo adapt or to annotate: Challenges and interventions for domain adaptation in open-domain question answering. CoRR, abs/2212.10381. Dheeru Dua, Emma Strubell, Sameer Singh, Pat Verga, 10.48550/arXiv.2212.103812022Dheeru Dua, Emma Strubell, Sameer Singh, and Pat Verga. To adapt or to annotate: Chal- lenges and interventions for domain adaptation in open-domain question answering. CoRR, abs/2212.10381, 2022. doi: 10.48550/arXiv.2212.10381. URL https://doi.org/10.48550/ arXiv.2212.10381.\n\nLong document re-ranking with modular re-ranker. Luyu Gao, Jamie Callan ; Pablo Castells, Julio Gonzalo, Ben Carterette, SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. J. Shane Culpepper, and Gabriella KazaiMadrid, SpainEnrique Amig\u00f3,Luyu Gao and Jamie Callan. Long document re-ranking with modular re-ranker. In Enrique Amig\u00f3, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai, editors, SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 -15, 2022, pages 2371-2376.\n\n. 10.1145/3477495.3531860ACM2022ACM, 2022. doi: 10.1145/3477495.3531860. URL https://doi.org/10.1145/3477495. 3531860.\n\nPrecise zero-shot dense retrieval without relevance labels. CoRR, abs/2212.10496. Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan, 10.48550/arXiv.2212.104962022Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. CoRR, abs/2212.10496, 2022. doi: 10.48550/arXiv.2212.10496. URL https://doi.org/10.48550/arXiv.2212.10496.\n\nREALM: retrieval-augmented language model pre-training. CoRR, abs. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: retrieval-augmented language model pre-training. CoRR, abs/2002.08909, 2020. URL https: //arxiv.org/abs/2002.08909.\n\nRethinking with retrieval: Faithful large language model inference. Hangfeng He, Hongming Zhang, Dan Roth, 10.48550/arXiv.2301.003032023Hangfeng He, Hongming Zhang, and Dan Roth. Rethinking with retrieval: Faithful large language model inference. CoRR, abs/2301.00303, 2023. doi: 10.48550/arXiv.2301.00303. URL https://doi.org/10.48550/arXiv.2301.00303.\n\nTowards unsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, abs/2112.09118CoRRGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Towards unsupervised dense information retrieval with contrastive learning. CoRR, abs/2112.09118, 2021. URL https://arxiv.org/abs/2112.09118.\n\nRoberto de Alencar Lotufo, Jakub Zavrel, and Rodrigo Frassetto Nogueira. Inpars-v2: Large language models as efficient dataset generators for information retrieval. Vitor Jeronymo, Henrique Luiz, Hugo Bonifacio, Marzieh Abonizio, Fadaee, 10.48550/arXiv.2301.01820CoRR2023Vitor Jeronymo, Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto de Alen- car Lotufo, Jakub Zavrel, and Rodrigo Frassetto Nogueira. Inpars-v2: Large language models as efficient dataset generators for information retrieval. CoRR, abs/2301.01820, 2023. doi: 10.48550/arXiv.2301.01820. URL https://doi.org/10.48550/arXiv.2301.01820.\n\nDense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, S H Patrick, Ledell Lewis, Sergey Wu, Danqi Edunov, Wen-Tau Chen, Yih, 10.18653/v1/2020.emnlp-main.550doi: 10.18653/ v1/2020.emnlp-main.550Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liuthe 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Confer- ence on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 6769-6781. Association for Computational Linguistics, 2020. doi: 10.18653/ v1/2020.emnlp-main.550. URL https://doi.org/10.18653/v1/2020.emnlp-main.550.\n\nLatent retrieval for weakly supervised open domain question answering. Kenton Lee, Ming-Wei Chang, Kristina Toutanova, 10.18653/v1/p19-1612Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquezthe 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, ItalyAssociation for Computational Linguistics1Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers, pages 6086- 6096. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1612. URL https://doi.org/10.18653/v1/p19-1612.\n\nPyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, Rodrigo Nogueira, Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021). the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021), pages 2356-2362, 2021.\n\nWhat makes good in-context examples for gpt-3?. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, 10.18653/v1/2022.deelio-1.10Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online. Eneko Agirre, Marianna Apidianaki, and Ivan VulicDeep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and OnlineAssociation for Computational Linguistics2022Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Eneko Agirre, Marianna Apidianaki, and Ivan Vulic, editors, Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, pages 100-114. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.deelio-1.10. URL https://doi.org/10.18653/ v1/2022.deelio-1.10.\n\nZ-ICL: zero-shot in-context learning with pseudo-demonstrations. Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, Hannaneh Hajishirzi, 10.48550/arXiv.2212.09865CoRR2022Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. Z-ICL: zero-shot in-context learning with pseudo-demonstrations. CoRR, abs/2212.09865, 2022. doi: 10.48550/arXiv.2212.09865. URL https://doi.org/10.48550/arXiv.2212.09865.\n\nEfficient estimation of word representations in vector space. Tom\u00e1s Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, 1st International Conference on Learning Representations, ICLR 2013. Scottsdale, Arizona, USAWorkshop Track ProceedingsTom\u00e1s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In Yoshua Bengio and Yann LeCun, editors, 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, 2013. URL http://arxiv.org/abs/1301.3781.\n\nRethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Yoav Goldberg, Zornitsa Kozareva, and Yue Zhangthe 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11048-11064. Association for Computational Linguistics, 2022. URL https://aclanthology.org/2022.emnlp-main. 759.\n\nSGPT: GPT sentence embeddings for semantic search. CoRR, abs/2202.08904, 2022. Niklas Muennighoff, Niklas Muennighoff. SGPT: GPT sentence embeddings for semantic search. CoRR, abs/2202.08904, 2022. URL https://arxiv.org/abs/2202.08904.\n\nA human generated machine reading comprehension dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, Marco, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016). Tarek Richard Besold, Antoine Bordes, Artur S. d'Avila Garcez, and Greg Waynethe Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016)Barcelona, Spain1773CEUR Workshop Proceedings. CEUR-WS.orgTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d'Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_ 2016_paper9.pdf.\n\nLarge dual encoders are generalizable retrievers. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, Yinfei Yang, abs/2112.07899CoRRJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y. Zhao, Yi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable retrievers. CoRR, abs/2112.07899, 2021. URL https://arxiv.org/abs/2112. 07899.\n\nImpact of pretraining term frequencies on few-shot numerical reasoning. Yasaman Razeghi, Robert L Logan, I V , Matt Gardner, Sameer Singh, Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, and Yue ZhangAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsYasaman Razeghi, Robert L. Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot numerical reasoning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 840-854. Association for Com- putational Linguistics, 2022. URL https://aclanthology.org/2022.findings-emnlp. 59.\n\nThe probabilistic relevance framework: BM25 and beyond. E Stephen, Hugo Robertson, Zaragoza, 10.1561/1500000019Found. Trends Inf. Retr. 34Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333-389, 2009. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.\n\nLearning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, 10.18653/v1/2022.naacl-main.191Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022. Marine Carpuat, Marie-Catherine de Marneffe, and Iv\u00e1n Vladimir Meza Ru\u00edzthe 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022Seattle, WA, United StatesAssociation for Computational Linguistics2022Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Marine Carpuat, Marie-Catherine de Marneffe, and Iv\u00e1n Vladimir Meza Ru\u00edz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 2655-2671. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.naacl-main.191. URL https://doi.org/10.18653/v1/2022. naacl-main.191.\n\nUDAPDR: unsupervised domain adaptation via LLM prompting and distillation of rerankers. Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Sultan, Christopher Potts, 10.48550/arXiv.2303.00807CoRR2023Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md. Arafat Sultan, and Christopher Potts. UDAPDR: unsupervised domain adaptation via LLM prompting and distillation of rerankers. CoRR, abs/2303.00807, 2023. doi: 10.48550/arXiv.2303.00807. URL https://doi.org/10.48550/arXiv.2303.00807.\n\nLexmae: Lexicon-bottlenecked pretraining for large-scale retrieval. Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Xiaolong Huang, Binxing Jiao, Linjun Yang, and Daxin Jiang. Lexmae: Lexicon-bottlenecked pretraining for large-scale retrieval.\n\n. Corr, 10.48550/arXiv.2208.147542022CoRR, abs/2208.14754, 2022. doi: 10.48550/arXiv.2208.14754. URL https://doi.org/10. 48550/arXiv.2208.14754.\n\nRetrieval augmentation reduces hallucination in conversation. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston, 10.18653/v1/2021.findings-emnlp.320Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau YihPunta Cana, Dominican RepublicAssociation for Computational Linguistics2021Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmenta- tion reduces hallucination in conversation. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguis- tics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 3784-3803. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021. findings-emnlp.320. URL https://doi.org/10.18653/v1/2021.findings-emnlp.320.\n\nStanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\n\nBEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, Iryna Gurevych, abs/2104.08663CoRRNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. BEIR: A heterogenous benchmark for zero-shot evaluation of information retrieval models. CoRR, abs/2104.08663, 2021. URL https://arxiv.org/abs/2104.08663.\n\nLlama: Open and efficient foundation language models. CoRR, abs/2302.13971. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Aur\u00e9lien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, 10.48550/arXiv.2302.139712023Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. doi: 10.48550/arXiv.2302.13971. URL https://doi.org/10.48550/arXiv.2302.13971.\n\nInterleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. CoRR, abs/2212.10509. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, 10.48550/arXiv.2212.105092022Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. CoRR, abs/2212.10509, 2022. doi: 10.48550/arXiv.2212.10509. URL https://doi.org/10.48550/ arXiv.2212.10509.\n\n. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, Text embeddings by weakly-supervised contrastive pre-trainingLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training.\n\n. Corr, 10.48550/arXiv.2212.035332022CoRR, abs/2212.03533, 2022. doi: 10.48550/arXiv.2212.03533. URL https://doi.org/10. 48550/arXiv.2212.03533.\n\nSimlm: Pre-training with representation bottleneck for dense passage retrieval. CoRR, abs/2207.02578. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, 10.48550/arXiv.2207.025782022Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval. CoRR, abs/2207.02578, 2022. doi: 10.48550/arXiv.2207.02578. URL https://doi.org/10.48550/arXiv.2207.02578.\n\nQuery2doc: Query expansion with large language models. Liang Wang, Nan Yang, Furu Wei, 10.48550/arXiv.2303.076782023Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language models. CoRR, abs/2303.07678, 2023. doi: 10.48550/arXiv.2303.07678. URL https://doi. org/10.48550/arXiv.2303.07678.\n\nAn explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI.\n\nApproximate nearest neighbor negative contrastive learning for dense text retrieval. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N Bennett, Junaid Ahmed, Arnold Overwijk, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview. net/forum?id=zeFrfgyZln.\n\nGenerate rather than retrieve: Large language models are strong context generators. CoRR, abs/2209.10063. Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang, 10.48550/arXiv.2209.100632022Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models are strong context generators. CoRR, abs/2209.10063, 2022. doi: 10.48550/arXiv.2209.10063. URL https://doi.org/10.48550/arXiv.2209.10063.\n\nKnowledgegrounded dialogue generation with pre-trained language models. Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, Rui Yan, 10.18653/v1/2020.emnlp-main.272Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liuthe 2020 Conference on Empirical Methods in Natural Language ProcessingOnlineAssociation for Computational Linguistics2020Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, and Rui Yan. Knowledge- grounded dialogue generation with pre-trained language models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 3377-3390. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020. emnlp-main.272. URL https://doi.org/10.18653/v1/2020.emnlp-main.272.\n\nHyperlink-induced pre-training for passage retrieval in open-domain question answering. Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Lan Luo, Ke Zhan, Enrui Hu, Xinyu Zhang, Hao Jiang, Zhao Cao, Fan Yu, Xin Jiang, Qun Liu, Lei Chen, 10.18653/v1/2022.acl-long.493Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Smaranda Muresan, Preslav Nakov, and Aline Villavicenciothe 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics12022ACL 2022Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Lan Luo, Ke Zhan, Enrui Hu, Xinyu Zhang, Hao Jiang, Zhao Cao, Fan Yu, Xin Jiang, Qun Liu, and Lei Chen. Hyperlink-induced pre-training for passage retrieval in open-domain question answering. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 7135-7146. Association for Computational Linguistics, 2022. doi: 10.18653/v1/ 2022.acl-long.493. URL https://doi.org/10.18653/v1/2022.acl-long.493.\n\nTowards robust ranker for text retrieval. CoRR, abs/2206.08063. Yucheng Zhou, Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Guodong Long, Binxing Jiao, Daxin Jiang, 10.48550/arXiv.2206.080632022Yucheng Zhou, Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Guodong Long, Binxing Jiao, and Daxin Jiang. Towards robust ranker for text retrieval. CoRR, abs/2206.08063, 2022. doi: 10.48550/arXiv.2206.08063. URL https://doi.org/10.48550/arXiv.2206.08063.\n", "annotations": {"author": "[{\"end\":113,\"start\":56},{\"end\":219,\"start\":114},{\"end\":279,\"start\":220},{\"end\":342,\"start\":280},{\"end\":428,\"start\":343},{\"end\":488,\"start\":429}]", "publisher": null, "author_last_name": "[{\"end\":64,\"start\":60},{\"end\":126,\"start\":122},{\"end\":230,\"start\":226},{\"end\":293,\"start\":290},{\"end\":359,\"start\":355},{\"end\":439,\"start\":435}]", "author_first_name": "[{\"end\":59,\"start\":56},{\"end\":121,\"start\":114},{\"end\":225,\"start\":220},{\"end\":289,\"start\":280},{\"end\":354,\"start\":348},{\"end\":434,\"start\":429}]", "author_affiliation": "[{\"end\":112,\"start\":66},{\"end\":218,\"start\":172},{\"end\":278,\"start\":232},{\"end\":341,\"start\":295},{\"end\":427,\"start\":381},{\"end\":487,\"start\":441}]", "title": "[{\"end\":53,\"start\":1},{\"end\":541,\"start\":489}]", "venue": null, "abstract": "[{\"end\":1946,\"start\":543}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2225,\"start\":2222},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2242,\"start\":2238},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2276,\"start\":2272},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2440,\"start\":2436},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2443,\"start\":2440},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2489,\"start\":2485},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2543,\"start\":2539},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2795,\"start\":2791},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2837,\"start\":2834},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2890,\"start\":2886},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2893,\"start\":2890},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3005,\"start\":3001},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3008,\"start\":3005},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3011,\"start\":3008},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3014,\"start\":3011},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3215,\"start\":3211},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3280,\"start\":3276},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3283,\"start\":3280},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3445,\"start\":3441},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3504,\"start\":3500},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4011,\"start\":4008},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4454,\"start\":4451},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4669,\"start\":4665},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4672,\"start\":4669},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4675,\"start\":4672},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5520,\"start\":5516},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6150,\"start\":6146},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6400,\"start\":6396},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6427,\"start\":6423},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6458,\"start\":6454},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6769,\"start\":6765},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6817,\"start\":6813},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7325,\"start\":7322},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7510,\"start\":7506},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7513,\"start\":7510},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7726,\"start\":7722},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7729,\"start\":7726},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7732,\"start\":7729},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7945,\"start\":7941},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8270,\"start\":8266},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8273,\"start\":8270},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8514,\"start\":8511},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8736,\"start\":8732},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8739,\"start\":8736},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9844,\"start\":9840},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9953,\"start\":9950},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10070,\"start\":10066},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10242,\"start\":10238},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10416,\"start\":10412},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10439,\"start\":10436},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10624,\"start\":10620},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10740,\"start\":10736},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11633,\"start\":11629},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14079,\"start\":14075},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14082,\"start\":14079},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14573,\"start\":14569},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14576,\"start\":14573},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14635,\"start\":14631},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16391,\"start\":16387},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16394,\"start\":16391},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16496,\"start\":16492},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16815,\"start\":16811},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19786,\"start\":19782},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19860,\"start\":19856},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19914,\"start\":19911},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19946,\"start\":19943},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20058,\"start\":20054},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20091,\"start\":20087},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21463,\"start\":21459},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21483,\"start\":21479},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21591,\"start\":21587},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21806,\"start\":21802},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22095,\"start\":22091},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22106,\"start\":22102},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22134,\"start\":22130},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28277,\"start\":28273},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28290,\"start\":28286},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28553,\"start\":28549},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28556,\"start\":28553},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30444,\"start\":30440},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35354,\"start\":35350},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":35455,\"start\":35451},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":35466,\"start\":35462},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":35478,\"start\":35474},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":36475,\"start\":36471}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32116,\"start\":32048},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32176,\"start\":32117},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32837,\"start\":32177},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33052,\"start\":32838},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33569,\"start\":33053},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34378,\"start\":33570},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34968,\"start\":34379},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35152,\"start\":34969},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35280,\"start\":35153},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":36264,\"start\":35281}]", "paragraph": "[{\"end\":2664,\"start\":1962},{\"end\":3284,\"start\":2666},{\"end\":4059,\"start\":3286},{\"end\":4676,\"start\":4061},{\"end\":5413,\"start\":4678},{\"end\":5815,\"start\":5415},{\"end\":7118,\"start\":5832},{\"end\":8410,\"start\":7120},{\"end\":8978,\"start\":8412},{\"end\":10856,\"start\":8980},{\"end\":11100,\"start\":10858},{\"end\":12716,\"start\":11117},{\"end\":13098,\"start\":12801},{\"end\":13207,\"start\":13100},{\"end\":13690,\"start\":13227},{\"end\":14208,\"start\":13733},{\"end\":14577,\"start\":14210},{\"end\":14905,\"start\":14579},{\"end\":15438,\"start\":15040},{\"end\":16199,\"start\":15468},{\"end\":16978,\"start\":16240},{\"end\":17476,\"start\":16980},{\"end\":17727,\"start\":17507},{\"end\":18165,\"start\":17785},{\"end\":18655,\"start\":18167},{\"end\":19104,\"start\":18698},{\"end\":19267,\"start\":19106},{\"end\":19561,\"start\":19269},{\"end\":19716,\"start\":19576},{\"end\":20760,\"start\":19718},{\"end\":21217,\"start\":20762},{\"end\":22140,\"start\":21248},{\"end\":22898,\"start\":22160},{\"end\":23858,\"start\":22900},{\"end\":24738,\"start\":23898},{\"end\":26080,\"start\":24740},{\"end\":28592,\"start\":26082},{\"end\":29199,\"start\":28616},{\"end\":29524,\"start\":29201},{\"end\":29849,\"start\":29556},{\"end\":30323,\"start\":29851},{\"end\":30791,\"start\":30325},{\"end\":30807,\"start\":30793},{\"end\":31380,\"start\":30809},{\"end\":32047,\"start\":31395}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13226,\"start\":13208},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15039,\"start\":14906},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15467,\"start\":15439},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17506,\"start\":17477},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17784,\"start\":17728}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12688,\"start\":12681},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17134,\"start\":17127},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17903,\"start\":17896},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22204,\"start\":22197},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23044,\"start\":23037},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26830,\"start\":26823},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27454,\"start\":27447},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":28087,\"start\":28080},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30159,\"start\":30152},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30465,\"start\":30458}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1960,\"start\":1948},{\"attributes\":{\"n\":\"2\"},\"end\":5830,\"start\":5818},{\"attributes\":{\"n\":\"3\"},\"end\":11115,\"start\":11103},{\"end\":12726,\"start\":12719},{\"end\":12732,\"start\":12729},{\"end\":12752,\"start\":12735},{\"attributes\":{\"n\":\"4\"},\"end\":12799,\"start\":12755},{\"attributes\":{\"n\":\"4.1\"},\"end\":13731,\"start\":13693},{\"attributes\":{\"n\":\"4.2\"},\"end\":16238,\"start\":16202},{\"attributes\":{\"n\":\"4.3\"},\"end\":18696,\"start\":18658},{\"attributes\":{\"n\":\"5\"},\"end\":19574,\"start\":19564},{\"end\":21246,\"start\":21220},{\"attributes\":{\"n\":\"5.1\"},\"end\":22158,\"start\":22143},{\"attributes\":{\"n\":\"5.2\"},\"end\":23896,\"start\":23861},{\"attributes\":{\"n\":\"5.3\"},\"end\":28614,\"start\":28595},{\"attributes\":{\"n\":\"5.4\"},\"end\":29554,\"start\":29527},{\"attributes\":{\"n\":\"7\"},\"end\":31393,\"start\":31383},{\"end\":32057,\"start\":32049},{\"end\":32128,\"start\":32118},{\"end\":32188,\"start\":32178},{\"end\":32849,\"start\":32839},{\"end\":33063,\"start\":33054},{\"end\":33580,\"start\":33571},{\"end\":34389,\"start\":34380},{\"end\":34979,\"start\":34970},{\"end\":35163,\"start\":35154},{\"end\":35291,\"start\":35282}]", "table": "[{\"end\":33569,\"start\":33364},{\"end\":34378,\"start\":33777},{\"end\":34968,\"start\":34476},{\"end\":35152,\"start\":35009},{\"end\":35280,\"start\":35181},{\"end\":36264,\"start\":35536}]", "figure_caption": "[{\"end\":32116,\"start\":32059},{\"end\":32176,\"start\":32130},{\"end\":32837,\"start\":32190},{\"end\":33052,\"start\":32851},{\"end\":33364,\"start\":33065},{\"end\":33777,\"start\":33582},{\"end\":34476,\"start\":34391},{\"end\":35009,\"start\":34981},{\"end\":35181,\"start\":35165},{\"end\":35536,\"start\":35293}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11565,\"start\":11557},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12333,\"start\":12325},{\"end\":12625,\"start\":12617},{\"end\":13097,\"start\":13089},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24093,\"start\":24085},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24899,\"start\":24891},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25397,\"start\":25389},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25576,\"start\":25568},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26136,\"start\":26128},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29380,\"start\":29372}]", "bib_author_first_name": "[{\"end\":37222,\"start\":37217},{\"end\":37233,\"start\":37229},{\"end\":37243,\"start\":37242},{\"end\":37245,\"start\":37244},{\"end\":37260,\"start\":37255},{\"end\":37275,\"start\":37268},{\"end\":37291,\"start\":37282},{\"end\":37309,\"start\":37301},{\"end\":37325,\"start\":37318},{\"end\":37815,\"start\":37809},{\"end\":37832,\"start\":37825},{\"end\":37845,\"start\":37840},{\"end\":37861,\"start\":37855},{\"end\":37875,\"start\":37869},{\"end\":38244,\"start\":38241},{\"end\":38246,\"start\":38245},{\"end\":38262,\"start\":38254},{\"end\":38273,\"start\":38269},{\"end\":38288,\"start\":38281},{\"end\":38303,\"start\":38298},{\"end\":38320,\"start\":38312},{\"end\":38337,\"start\":38331},{\"end\":38813,\"start\":38806},{\"end\":38830,\"start\":38823},{\"end\":38846,\"start\":38835},{\"end\":38859,\"start\":38855},{\"end\":38871,\"start\":38867},{\"end\":38885,\"start\":38878},{\"end\":38899,\"start\":38894},{\"end\":38916,\"start\":38908},{\"end\":38927,\"start\":38923},{\"end\":38946,\"start\":38935},{\"end\":38957,\"start\":38954},{\"end\":38970,\"start\":38966},{\"end\":38987,\"start\":38983},{\"end\":39002,\"start\":38997},{\"end\":39956,\"start\":39948},{\"end\":39968,\"start\":39962},{\"end\":39981,\"start\":39974},{\"end\":39990,\"start\":39987},{\"end\":40002,\"start\":39996},{\"end\":40015,\"start\":40010},{\"end\":40314,\"start\":40309},{\"end\":40325,\"start\":40321},{\"end\":40338,\"start\":40333},{\"end\":40354,\"start\":40347},{\"end\":41147,\"start\":41143},{\"end\":41165,\"start\":41158},{\"end\":41178,\"start\":41173},{\"end\":41193,\"start\":41187},{\"end\":41207,\"start\":41202},{\"end\":41209,\"start\":41208},{\"end\":41495,\"start\":41491},{\"end\":41513,\"start\":41506},{\"end\":41526,\"start\":41521},{\"end\":41541,\"start\":41535},{\"end\":41812,\"start\":41806},{\"end\":41819,\"start\":41818},{\"end\":41831,\"start\":41829},{\"end\":41840,\"start\":41838},{\"end\":41851,\"start\":41845},{\"end\":41862,\"start\":41858},{\"end\":41872,\"start\":41867},{\"end\":41883,\"start\":41877},{\"end\":41898,\"start\":41893},{\"end\":41900,\"start\":41899},{\"end\":41914,\"start\":41906},{\"end\":42336,\"start\":42331},{\"end\":42353,\"start\":42345},{\"end\":42367,\"start\":42361},{\"end\":42381,\"start\":42373},{\"end\":43544,\"start\":43538},{\"end\":43554,\"start\":43550},{\"end\":43571,\"start\":43565},{\"end\":43582,\"start\":43579},{\"end\":43950,\"start\":43946},{\"end\":43961,\"start\":43956},{\"end\":43992,\"start\":43987},{\"end\":44005,\"start\":44002},{\"end\":44751,\"start\":44747},{\"end\":44765,\"start\":44757},{\"end\":44775,\"start\":44770},{\"end\":44786,\"start\":44781},{\"end\":45117,\"start\":45111},{\"end\":45129,\"start\":45123},{\"end\":45139,\"start\":45135},{\"end\":45154,\"start\":45146},{\"end\":45172,\"start\":45164},{\"end\":45453,\"start\":45445},{\"end\":45466,\"start\":45458},{\"end\":45477,\"start\":45474},{\"end\":45815,\"start\":45808},{\"end\":45833,\"start\":45825},{\"end\":45846,\"start\":45841},{\"end\":45866,\"start\":45857},{\"end\":45880,\"start\":45875},{\"end\":45899,\"start\":45893},{\"end\":45915,\"start\":45908},{\"end\":46373,\"start\":46368},{\"end\":46392,\"start\":46384},{\"end\":46403,\"start\":46399},{\"end\":46422,\"start\":46415},{\"end\":46889,\"start\":46881},{\"end\":46907,\"start\":46901},{\"end\":46919,\"start\":46914},{\"end\":46926,\"start\":46925},{\"end\":46928,\"start\":46927},{\"end\":46944,\"start\":46938},{\"end\":46958,\"start\":46952},{\"end\":46968,\"start\":46963},{\"end\":46984,\"start\":46977},{\"end\":47936,\"start\":47930},{\"end\":47950,\"start\":47942},{\"end\":47966,\"start\":47958},{\"end\":48876,\"start\":48871},{\"end\":48890,\"start\":48882},{\"end\":48906,\"start\":48895},{\"end\":48922,\"start\":48912},{\"end\":48934,\"start\":48929},{\"end\":48951,\"start\":48944},{\"end\":49636,\"start\":49628},{\"end\":49649,\"start\":49642},{\"end\":49661,\"start\":49656},{\"end\":49673,\"start\":49669},{\"end\":49689,\"start\":49681},{\"end\":49703,\"start\":49697},{\"end\":50774,\"start\":50769},{\"end\":50785,\"start\":50780},{\"end\":50793,\"start\":50791},{\"end\":50807,\"start\":50803},{\"end\":50829,\"start\":50821},{\"end\":51192,\"start\":51187},{\"end\":51205,\"start\":51202},{\"end\":51216,\"start\":51212},{\"end\":51233,\"start\":51226},{\"end\":51783,\"start\":51778},{\"end\":51794,\"start\":51789},{\"end\":51803,\"start\":51800},{\"end\":51819,\"start\":51814},{\"end\":51833,\"start\":51829},{\"end\":51849,\"start\":51841},{\"end\":51866,\"start\":51862},{\"end\":52764,\"start\":52758},{\"end\":52976,\"start\":52973},{\"end\":52988,\"start\":52985},{\"end\":53003,\"start\":53000},{\"end\":53018,\"start\":53010},{\"end\":53031,\"start\":53024},{\"end\":53046,\"start\":53040},{\"end\":53059,\"start\":53057},{\"end\":54256,\"start\":54250},{\"end\":54265,\"start\":54261},{\"end\":54274,\"start\":54270},{\"end\":54285,\"start\":54279},{\"end\":54298,\"start\":54291},{\"end\":54308,\"start\":54299},{\"end\":54319,\"start\":54317},{\"end\":54331,\"start\":54324},{\"end\":54333,\"start\":54332},{\"end\":54342,\"start\":54340},{\"end\":54354,\"start\":54349},{\"end\":54356,\"start\":54355},{\"end\":54371,\"start\":54363},{\"end\":54385,\"start\":54379},{\"end\":54755,\"start\":54748},{\"end\":54771,\"start\":54765},{\"end\":54773,\"start\":54772},{\"end\":54782,\"start\":54781},{\"end\":54784,\"start\":54783},{\"end\":54791,\"start\":54787},{\"end\":54807,\"start\":54801},{\"end\":55510,\"start\":55509},{\"end\":55524,\"start\":55520},{\"end\":55857,\"start\":55853},{\"end\":55873,\"start\":55865},{\"end\":55890,\"start\":55882},{\"end\":57010,\"start\":57007},{\"end\":57028,\"start\":57024},{\"end\":57044,\"start\":57038},{\"end\":57060,\"start\":57056},{\"end\":57076,\"start\":57070},{\"end\":57089,\"start\":57084},{\"end\":57104,\"start\":57098},{\"end\":57112,\"start\":57110},{\"end\":57132,\"start\":57121},{\"end\":57585,\"start\":57582},{\"end\":57597,\"start\":57592},{\"end\":57613,\"start\":57604},{\"end\":57622,\"start\":57619},{\"end\":57635,\"start\":57627},{\"end\":57650,\"start\":57643},{\"end\":57663,\"start\":57657},{\"end\":57675,\"start\":57670},{\"end\":58069,\"start\":58065},{\"end\":58086,\"start\":58079},{\"end\":58097,\"start\":58093},{\"end\":58109,\"start\":58104},{\"end\":58122,\"start\":58117},{\"end\":58991,\"start\":58986},{\"end\":59005,\"start\":58999},{\"end\":59023,\"start\":59017},{\"end\":59035,\"start\":59031},{\"end\":59051,\"start\":59044},{\"end\":59062,\"start\":59056},{\"end\":59078,\"start\":59073},{\"end\":59095,\"start\":59086},{\"end\":59097,\"start\":59096},{\"end\":59444,\"start\":59438},{\"end\":59457,\"start\":59453},{\"end\":59474,\"start\":59467},{\"end\":59491,\"start\":59483},{\"end\":59509,\"start\":59504},{\"end\":59860,\"start\":59856},{\"end\":59877,\"start\":59870},{\"end\":59893,\"start\":59886},{\"end\":59909,\"start\":59903},{\"end\":59930,\"start\":59920},{\"end\":59948,\"start\":59940},{\"end\":59963,\"start\":59958},{\"end\":59986,\"start\":59982},{\"end\":60000,\"start\":59994},{\"end\":60017,\"start\":60009},{\"end\":60031,\"start\":60025},{\"end\":60050,\"start\":60043},{\"end\":60068,\"start\":60059},{\"end\":60637,\"start\":60632},{\"end\":60655,\"start\":60647},{\"end\":60679,\"start\":60673},{\"end\":60692,\"start\":60686},{\"end\":61026,\"start\":61021},{\"end\":61036,\"start\":61033},{\"end\":61051,\"start\":61043},{\"end\":61066,\"start\":61059},{\"end\":61079,\"start\":61073},{\"end\":61091,\"start\":61086},{\"end\":61105,\"start\":61099},{\"end\":61120,\"start\":61116},{\"end\":61613,\"start\":61608},{\"end\":61623,\"start\":61620},{\"end\":61638,\"start\":61630},{\"end\":61653,\"start\":61646},{\"end\":61666,\"start\":61660},{\"end\":61678,\"start\":61673},{\"end\":61692,\"start\":61686},{\"end\":61707,\"start\":61703},{\"end\":62099,\"start\":62094},{\"end\":62109,\"start\":62106},{\"end\":62120,\"start\":62116},{\"end\":62429,\"start\":62425},{\"end\":62448,\"start\":62443},{\"end\":62467,\"start\":62462},{\"end\":62481,\"start\":62475},{\"end\":62983,\"start\":62980},{\"end\":62998,\"start\":62991},{\"end\":63008,\"start\":63006},{\"end\":63022,\"start\":63013},{\"end\":63035,\"start\":63029},{\"end\":63045,\"start\":63041},{\"end\":63047,\"start\":63046},{\"end\":63063,\"start\":63057},{\"end\":63077,\"start\":63071},{\"end\":63672,\"start\":63666},{\"end\":63680,\"start\":63677},{\"end\":63695,\"start\":63687},{\"end\":63709,\"start\":63702},{\"end\":63722,\"start\":63714},{\"end\":63733,\"start\":63727},{\"end\":63751,\"start\":63742},{\"end\":63764,\"start\":63757},{\"end\":63775,\"start\":63771},{\"end\":64205,\"start\":64197},{\"end\":64215,\"start\":64212},{\"end\":64223,\"start\":64220},{\"end\":64237,\"start\":64228},{\"end\":64250,\"start\":64243},{\"end\":64260,\"start\":64257},{\"end\":65151,\"start\":65145},{\"end\":65167,\"start\":65158},{\"end\":65178,\"start\":65172},{\"end\":65189,\"start\":65186},{\"end\":65197,\"start\":65195},{\"end\":65209,\"start\":65204},{\"end\":65219,\"start\":65214},{\"end\":65230,\"start\":65227},{\"end\":65242,\"start\":65238},{\"end\":65251,\"start\":65248},{\"end\":65259,\"start\":65256},{\"end\":65270,\"start\":65267},{\"end\":65279,\"start\":65276},{\"end\":66287,\"start\":66280},{\"end\":66297,\"start\":66294},{\"end\":66309,\"start\":66304},{\"end\":66325,\"start\":66316},{\"end\":66334,\"start\":66331},{\"end\":66346,\"start\":66339},{\"end\":66360,\"start\":66353},{\"end\":66372,\"start\":66367}]", "bib_author_last_name": "[{\"end\":37227,\"start\":37223},{\"end\":37240,\"start\":37234},{\"end\":37253,\"start\":37246},{\"end\":37266,\"start\":37261},{\"end\":37280,\"start\":37276},{\"end\":37299,\"start\":37292},{\"end\":37316,\"start\":37310},{\"end\":37336,\"start\":37326},{\"end\":37341,\"start\":37338},{\"end\":37823,\"start\":37816},{\"end\":37838,\"start\":37833},{\"end\":37853,\"start\":37846},{\"end\":37867,\"start\":37862},{\"end\":37881,\"start\":37876},{\"end\":38252,\"start\":38247},{\"end\":38267,\"start\":38263},{\"end\":38279,\"start\":38274},{\"end\":38296,\"start\":38289},{\"end\":38310,\"start\":38304},{\"end\":38329,\"start\":38321},{\"end\":38349,\"start\":38338},{\"end\":38821,\"start\":38814},{\"end\":38833,\"start\":38831},{\"end\":38853,\"start\":38847},{\"end\":38865,\"start\":38860},{\"end\":38876,\"start\":38872},{\"end\":38892,\"start\":38886},{\"end\":38906,\"start\":38900},{\"end\":38921,\"start\":38917},{\"end\":38933,\"start\":38928},{\"end\":38952,\"start\":38947},{\"end\":38964,\"start\":38958},{\"end\":38981,\"start\":38971},{\"end\":38995,\"start\":38988},{\"end\":39012,\"start\":39003},{\"end\":39020,\"start\":39014},{\"end\":39960,\"start\":39957},{\"end\":39972,\"start\":39969},{\"end\":39985,\"start\":39982},{\"end\":39994,\"start\":39991},{\"end\":40008,\"start\":40003},{\"end\":40021,\"start\":40016},{\"end\":40319,\"start\":40315},{\"end\":40331,\"start\":40326},{\"end\":40345,\"start\":40339},{\"end\":40361,\"start\":40355},{\"end\":41156,\"start\":41148},{\"end\":41171,\"start\":41166},{\"end\":41185,\"start\":41179},{\"end\":41200,\"start\":41194},{\"end\":41218,\"start\":41210},{\"end\":41504,\"start\":41496},{\"end\":41519,\"start\":41514},{\"end\":41533,\"start\":41527},{\"end\":41548,\"start\":41542},{\"end\":41816,\"start\":41813},{\"end\":41827,\"start\":41820},{\"end\":41836,\"start\":41832},{\"end\":41843,\"start\":41841},{\"end\":41856,\"start\":41852},{\"end\":41865,\"start\":41863},{\"end\":41875,\"start\":41873},{\"end\":41891,\"start\":41884},{\"end\":41904,\"start\":41901},{\"end\":41919,\"start\":41915},{\"end\":41926,\"start\":41921},{\"end\":42343,\"start\":42337},{\"end\":42359,\"start\":42354},{\"end\":42371,\"start\":42368},{\"end\":42391,\"start\":42382},{\"end\":43548,\"start\":43545},{\"end\":43563,\"start\":43555},{\"end\":43577,\"start\":43572},{\"end\":43588,\"start\":43583},{\"end\":43954,\"start\":43951},{\"end\":43985,\"start\":43962},{\"end\":44000,\"start\":43993},{\"end\":44016,\"start\":44006},{\"end\":44755,\"start\":44752},{\"end\":44768,\"start\":44766},{\"end\":44779,\"start\":44776},{\"end\":44793,\"start\":44787},{\"end\":45121,\"start\":45118},{\"end\":45133,\"start\":45130},{\"end\":45144,\"start\":45140},{\"end\":45162,\"start\":45155},{\"end\":45178,\"start\":45173},{\"end\":45456,\"start\":45454},{\"end\":45472,\"start\":45467},{\"end\":45482,\"start\":45478},{\"end\":45823,\"start\":45816},{\"end\":45839,\"start\":45834},{\"end\":45855,\"start\":45847},{\"end\":45873,\"start\":45867},{\"end\":45891,\"start\":45881},{\"end\":45906,\"start\":45900},{\"end\":45921,\"start\":45916},{\"end\":46382,\"start\":46374},{\"end\":46397,\"start\":46393},{\"end\":46413,\"start\":46404},{\"end\":46431,\"start\":46423},{\"end\":46439,\"start\":46433},{\"end\":46899,\"start\":46890},{\"end\":46912,\"start\":46908},{\"end\":46923,\"start\":46920},{\"end\":46936,\"start\":46929},{\"end\":46950,\"start\":46945},{\"end\":46961,\"start\":46959},{\"end\":46975,\"start\":46969},{\"end\":46989,\"start\":46985},{\"end\":46994,\"start\":46991},{\"end\":47940,\"start\":47937},{\"end\":47956,\"start\":47951},{\"end\":47976,\"start\":47967},{\"end\":48880,\"start\":48877},{\"end\":48893,\"start\":48891},{\"end\":48910,\"start\":48907},{\"end\":48927,\"start\":48923},{\"end\":48942,\"start\":48935},{\"end\":48960,\"start\":48952},{\"end\":49640,\"start\":49637},{\"end\":49654,\"start\":49650},{\"end\":49667,\"start\":49662},{\"end\":49679,\"start\":49674},{\"end\":49695,\"start\":49690},{\"end\":49708,\"start\":49704},{\"end\":50778,\"start\":50775},{\"end\":50789,\"start\":50786},{\"end\":50801,\"start\":50794},{\"end\":50819,\"start\":50808},{\"end\":50840,\"start\":50830},{\"end\":51200,\"start\":51193},{\"end\":51210,\"start\":51206},{\"end\":51224,\"start\":51217},{\"end\":51238,\"start\":51234},{\"end\":51787,\"start\":51784},{\"end\":51798,\"start\":51795},{\"end\":51812,\"start\":51804},{\"end\":51827,\"start\":51820},{\"end\":51839,\"start\":51834},{\"end\":51860,\"start\":51850},{\"end\":51878,\"start\":51867},{\"end\":52776,\"start\":52765},{\"end\":52983,\"start\":52977},{\"end\":52998,\"start\":52989},{\"end\":53008,\"start\":53004},{\"end\":53022,\"start\":53019},{\"end\":53038,\"start\":53032},{\"end\":53055,\"start\":53047},{\"end\":53064,\"start\":53060},{\"end\":53071,\"start\":53066},{\"end\":54259,\"start\":54257},{\"end\":54268,\"start\":54266},{\"end\":54277,\"start\":54275},{\"end\":54289,\"start\":54286},{\"end\":54315,\"start\":54309},{\"end\":54322,\"start\":54320},{\"end\":54338,\"start\":54334},{\"end\":54347,\"start\":54343},{\"end\":54361,\"start\":54357},{\"end\":54377,\"start\":54372},{\"end\":54390,\"start\":54386},{\"end\":54763,\"start\":54756},{\"end\":54779,\"start\":54774},{\"end\":54799,\"start\":54792},{\"end\":54813,\"start\":54808},{\"end\":55518,\"start\":55511},{\"end\":55534,\"start\":55525},{\"end\":55544,\"start\":55536},{\"end\":55863,\"start\":55858},{\"end\":55880,\"start\":55874},{\"end\":55897,\"start\":55891},{\"end\":57022,\"start\":57011},{\"end\":57036,\"start\":57029},{\"end\":57054,\"start\":57045},{\"end\":57068,\"start\":57061},{\"end\":57082,\"start\":57077},{\"end\":57096,\"start\":57090},{\"end\":57108,\"start\":57105},{\"end\":57119,\"start\":57113},{\"end\":57138,\"start\":57133},{\"end\":57590,\"start\":57586},{\"end\":57602,\"start\":57598},{\"end\":57617,\"start\":57614},{\"end\":57625,\"start\":57623},{\"end\":57641,\"start\":57636},{\"end\":57655,\"start\":57651},{\"end\":57668,\"start\":57664},{\"end\":57681,\"start\":57676},{\"end\":57863,\"start\":57859},{\"end\":58077,\"start\":58070},{\"end\":58091,\"start\":58087},{\"end\":58102,\"start\":58098},{\"end\":58115,\"start\":58110},{\"end\":58129,\"start\":58123},{\"end\":58997,\"start\":58992},{\"end\":59015,\"start\":59006},{\"end\":59029,\"start\":59024},{\"end\":59042,\"start\":59036},{\"end\":59054,\"start\":59052},{\"end\":59071,\"start\":59063},{\"end\":59084,\"start\":59079},{\"end\":59107,\"start\":59098},{\"end\":59451,\"start\":59445},{\"end\":59465,\"start\":59458},{\"end\":59481,\"start\":59475},{\"end\":59502,\"start\":59492},{\"end\":59518,\"start\":59510},{\"end\":59868,\"start\":59861},{\"end\":59884,\"start\":59878},{\"end\":59901,\"start\":59894},{\"end\":59918,\"start\":59910},{\"end\":59938,\"start\":59931},{\"end\":59956,\"start\":59949},{\"end\":59980,\"start\":59964},{\"end\":59992,\"start\":59987},{\"end\":60007,\"start\":60001},{\"end\":60023,\"start\":60018},{\"end\":60041,\"start\":60032},{\"end\":60057,\"start\":60051},{\"end\":60074,\"start\":60069},{\"end\":60082,\"start\":60076},{\"end\":60645,\"start\":60638},{\"end\":60671,\"start\":60656},{\"end\":60684,\"start\":60680},{\"end\":60702,\"start\":60693},{\"end\":61031,\"start\":61027},{\"end\":61041,\"start\":61037},{\"end\":61057,\"start\":61052},{\"end\":61071,\"start\":61067},{\"end\":61084,\"start\":61080},{\"end\":61097,\"start\":61092},{\"end\":61114,\"start\":61106},{\"end\":61124,\"start\":61121},{\"end\":61366,\"start\":61362},{\"end\":61618,\"start\":61614},{\"end\":61628,\"start\":61624},{\"end\":61644,\"start\":61639},{\"end\":61658,\"start\":61654},{\"end\":61671,\"start\":61667},{\"end\":61684,\"start\":61679},{\"end\":61701,\"start\":61693},{\"end\":61711,\"start\":61708},{\"end\":62104,\"start\":62100},{\"end\":62114,\"start\":62110},{\"end\":62124,\"start\":62121},{\"end\":62441,\"start\":62430},{\"end\":62460,\"start\":62449},{\"end\":62473,\"start\":62468},{\"end\":62484,\"start\":62482},{\"end\":62989,\"start\":62984},{\"end\":63004,\"start\":62999},{\"end\":63011,\"start\":63009},{\"end\":63027,\"start\":63023},{\"end\":63039,\"start\":63036},{\"end\":63055,\"start\":63048},{\"end\":63069,\"start\":63064},{\"end\":63086,\"start\":63078},{\"end\":63675,\"start\":63673},{\"end\":63685,\"start\":63681},{\"end\":63700,\"start\":63696},{\"end\":63712,\"start\":63710},{\"end\":63725,\"start\":63723},{\"end\":63740,\"start\":63734},{\"end\":63755,\"start\":63752},{\"end\":63769,\"start\":63765},{\"end\":63781,\"start\":63776},{\"end\":64210,\"start\":64206},{\"end\":64218,\"start\":64216},{\"end\":64226,\"start\":64224},{\"end\":64241,\"start\":64238},{\"end\":64255,\"start\":64251},{\"end\":64264,\"start\":64261},{\"end\":65156,\"start\":65152},{\"end\":65170,\"start\":65168},{\"end\":65184,\"start\":65179},{\"end\":65193,\"start\":65190},{\"end\":65202,\"start\":65198},{\"end\":65212,\"start\":65210},{\"end\":65225,\"start\":65220},{\"end\":65236,\"start\":65231},{\"end\":65246,\"start\":65243},{\"end\":65254,\"start\":65252},{\"end\":65265,\"start\":65260},{\"end\":65274,\"start\":65271},{\"end\":65284,\"start\":65280},{\"end\":66292,\"start\":66288},{\"end\":66302,\"start\":66298},{\"end\":66314,\"start\":66310},{\"end\":66329,\"start\":66326},{\"end\":66337,\"start\":66335},{\"end\":66351,\"start\":66347},{\"end\":66365,\"start\":66361},{\"end\":66378,\"start\":66373}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.48550/arXiv.2211.09260\",\"id\":\"b0\"},\"end\":37712,\"start\":37215},{\"attributes\":{\"doi\":\"10.48550/arXiv.2301.02998\",\"id\":\"b1\"},\"end\":38237,\"start\":37714},{\"attributes\":{\"id\":\"b2\"},\"end\":38765,\"start\":38239},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":39875,\"start\":38767},{\"attributes\":{\"doi\":\"abs/2103.04831\",\"id\":\"b4\"},\"end\":40256,\"start\":39877},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1171\",\"id\":\"b5\",\"matched_paper_id\":3618568},\"end\":41083,\"start\":40258},{\"attributes\":{\"id\":\"b6\"},\"end\":41414,\"start\":41085},{\"attributes\":{\"id\":\"b7\"},\"end\":41726,\"start\":41416},{\"attributes\":{\"doi\":\"10.48550/arXiv.2209.11755\",\"id\":\"b8\"},\"end\":42247,\"start\":41728},{\"attributes\":{\"doi\":\"10.18653/v1/n19-1423\",\"id\":\"b9\",\"matched_paper_id\":52967399},\"end\":43403,\"start\":42249},{\"attributes\":{\"doi\":\"10.48550/arXiv.2212.10381\",\"id\":\"b10\"},\"end\":43895,\"start\":43405},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":248571516},\"end\":44543,\"start\":43897},{\"attributes\":{\"doi\":\"10.1145/3477495.3531860\",\"id\":\"b12\"},\"end\":44663,\"start\":44545},{\"attributes\":{\"doi\":\"10.48550/arXiv.2212.10496\",\"id\":\"b13\"},\"end\":45042,\"start\":44665},{\"attributes\":{\"id\":\"b14\"},\"end\":45375,\"start\":45044},{\"attributes\":{\"doi\":\"10.48550/arXiv.2301.00303\",\"id\":\"b15\"},\"end\":45730,\"start\":45377},{\"attributes\":{\"doi\":\"abs/2112.09118\",\"id\":\"b16\"},\"end\":46201,\"start\":45732},{\"attributes\":{\"doi\":\"10.48550/arXiv.2301.01820\",\"id\":\"b17\"},\"end\":46819,\"start\":46203},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.550\",\"id\":\"b18\",\"matched_paper_id\":215737187},\"end\":47857,\"start\":46821},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":173990818},\"end\":48755,\"start\":47859},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":235366815},\"end\":49578,\"start\":48757},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":231632658},\"end\":50702,\"start\":49580},{\"attributes\":{\"id\":\"b22\"},\"end\":51123,\"start\":50704},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5959482},\"end\":51699,\"start\":51125},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":247155069},\"end\":52677,\"start\":51701},{\"attributes\":{\"id\":\"b25\"},\"end\":52914,\"start\":52679},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1289517},\"end\":54198,\"start\":52916},{\"attributes\":{\"id\":\"b27\"},\"end\":54674,\"start\":54200},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":256631096},\"end\":55451,\"start\":54676},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":207178704},\"end\":55797,\"start\":55453},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":245218561},\"end\":56917,\"start\":55799},{\"attributes\":{\"id\":\"b31\"},\"end\":57512,\"start\":56919},{\"attributes\":{\"id\":\"b32\"},\"end\":57855,\"start\":57514},{\"attributes\":{\"id\":\"b33\"},\"end\":58001,\"start\":57857},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":233240939},\"end\":58929,\"start\":58003},{\"attributes\":{\"id\":\"b35\"},\"end\":59347,\"start\":58931},{\"attributes\":{\"id\":\"b36\"},\"end\":59778,\"start\":59349},{\"attributes\":{\"id\":\"b37\"},\"end\":60507,\"start\":59780},{\"attributes\":{\"id\":\"b38\"},\"end\":61017,\"start\":60509},{\"attributes\":{\"id\":\"b39\"},\"end\":61358,\"start\":61019},{\"attributes\":{\"id\":\"b40\"},\"end\":61504,\"start\":61360},{\"attributes\":{\"id\":\"b41\"},\"end\":62037,\"start\":61506},{\"attributes\":{\"id\":\"b42\"},\"end\":62353,\"start\":62039},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":241035330},\"end\":62893,\"start\":62355},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":220302524},\"end\":63558,\"start\":62895},{\"attributes\":{\"id\":\"b45\"},\"end\":64123,\"start\":63560},{\"attributes\":{\"id\":\"b46\"},\"end\":65055,\"start\":64125},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":247447562},\"end\":66214,\"start\":65057},{\"attributes\":{\"id\":\"b48\"},\"end\":66661,\"start\":66216}]", "bib_title": "[{\"end\":38804,\"start\":38767},{\"end\":40307,\"start\":40258},{\"end\":42329,\"start\":42249},{\"end\":43944,\"start\":43897},{\"end\":46879,\"start\":46821},{\"end\":47928,\"start\":47859},{\"end\":48869,\"start\":48757},{\"end\":49626,\"start\":49580},{\"end\":51185,\"start\":51125},{\"end\":51776,\"start\":51701},{\"end\":52971,\"start\":52916},{\"end\":54746,\"start\":54676},{\"end\":55507,\"start\":55453},{\"end\":55851,\"start\":55799},{\"end\":58063,\"start\":58003},{\"end\":62423,\"start\":62355},{\"end\":62978,\"start\":62895},{\"end\":64195,\"start\":64125},{\"end\":65143,\"start\":65057}]", "bib_author": "[{\"end\":37229,\"start\":37217},{\"end\":37242,\"start\":37229},{\"end\":37255,\"start\":37242},{\"end\":37268,\"start\":37255},{\"end\":37282,\"start\":37268},{\"end\":37301,\"start\":37282},{\"end\":37318,\"start\":37301},{\"end\":37338,\"start\":37318},{\"end\":37343,\"start\":37338},{\"end\":37825,\"start\":37809},{\"end\":37840,\"start\":37825},{\"end\":37855,\"start\":37840},{\"end\":37869,\"start\":37855},{\"end\":37883,\"start\":37869},{\"end\":38254,\"start\":38241},{\"end\":38269,\"start\":38254},{\"end\":38281,\"start\":38269},{\"end\":38298,\"start\":38281},{\"end\":38312,\"start\":38298},{\"end\":38331,\"start\":38312},{\"end\":38351,\"start\":38331},{\"end\":38823,\"start\":38806},{\"end\":38835,\"start\":38823},{\"end\":38855,\"start\":38835},{\"end\":38867,\"start\":38855},{\"end\":38878,\"start\":38867},{\"end\":38894,\"start\":38878},{\"end\":38908,\"start\":38894},{\"end\":38923,\"start\":38908},{\"end\":38935,\"start\":38923},{\"end\":38954,\"start\":38935},{\"end\":38966,\"start\":38954},{\"end\":38983,\"start\":38966},{\"end\":38997,\"start\":38983},{\"end\":39014,\"start\":38997},{\"end\":39022,\"start\":39014},{\"end\":39962,\"start\":39948},{\"end\":39974,\"start\":39962},{\"end\":39987,\"start\":39974},{\"end\":39996,\"start\":39987},{\"end\":40010,\"start\":39996},{\"end\":40023,\"start\":40010},{\"end\":40321,\"start\":40309},{\"end\":40333,\"start\":40321},{\"end\":40347,\"start\":40333},{\"end\":40363,\"start\":40347},{\"end\":41158,\"start\":41143},{\"end\":41173,\"start\":41158},{\"end\":41187,\"start\":41173},{\"end\":41202,\"start\":41187},{\"end\":41220,\"start\":41202},{\"end\":41506,\"start\":41491},{\"end\":41521,\"start\":41506},{\"end\":41535,\"start\":41521},{\"end\":41550,\"start\":41535},{\"end\":41818,\"start\":41806},{\"end\":41829,\"start\":41818},{\"end\":41838,\"start\":41829},{\"end\":41845,\"start\":41838},{\"end\":41858,\"start\":41845},{\"end\":41867,\"start\":41858},{\"end\":41877,\"start\":41867},{\"end\":41893,\"start\":41877},{\"end\":41906,\"start\":41893},{\"end\":41921,\"start\":41906},{\"end\":41928,\"start\":41921},{\"end\":42345,\"start\":42331},{\"end\":42361,\"start\":42345},{\"end\":42373,\"start\":42361},{\"end\":42393,\"start\":42373},{\"end\":43550,\"start\":43538},{\"end\":43565,\"start\":43550},{\"end\":43579,\"start\":43565},{\"end\":43590,\"start\":43579},{\"end\":43956,\"start\":43946},{\"end\":43987,\"start\":43956},{\"end\":44002,\"start\":43987},{\"end\":44018,\"start\":44002},{\"end\":44757,\"start\":44747},{\"end\":44770,\"start\":44757},{\"end\":44781,\"start\":44770},{\"end\":44795,\"start\":44781},{\"end\":45123,\"start\":45111},{\"end\":45135,\"start\":45123},{\"end\":45146,\"start\":45135},{\"end\":45164,\"start\":45146},{\"end\":45180,\"start\":45164},{\"end\":45458,\"start\":45445},{\"end\":45474,\"start\":45458},{\"end\":45484,\"start\":45474},{\"end\":45825,\"start\":45808},{\"end\":45841,\"start\":45825},{\"end\":45857,\"start\":45841},{\"end\":45875,\"start\":45857},{\"end\":45893,\"start\":45875},{\"end\":45908,\"start\":45893},{\"end\":45923,\"start\":45908},{\"end\":46384,\"start\":46368},{\"end\":46399,\"start\":46384},{\"end\":46415,\"start\":46399},{\"end\":46433,\"start\":46415},{\"end\":46441,\"start\":46433},{\"end\":46901,\"start\":46881},{\"end\":46914,\"start\":46901},{\"end\":46925,\"start\":46914},{\"end\":46938,\"start\":46925},{\"end\":46952,\"start\":46938},{\"end\":46963,\"start\":46952},{\"end\":46977,\"start\":46963},{\"end\":46991,\"start\":46977},{\"end\":46996,\"start\":46991},{\"end\":47942,\"start\":47930},{\"end\":47958,\"start\":47942},{\"end\":47978,\"start\":47958},{\"end\":48882,\"start\":48871},{\"end\":48895,\"start\":48882},{\"end\":48912,\"start\":48895},{\"end\":48929,\"start\":48912},{\"end\":48944,\"start\":48929},{\"end\":48962,\"start\":48944},{\"end\":49642,\"start\":49628},{\"end\":49656,\"start\":49642},{\"end\":49669,\"start\":49656},{\"end\":49681,\"start\":49669},{\"end\":49697,\"start\":49681},{\"end\":49710,\"start\":49697},{\"end\":50780,\"start\":50769},{\"end\":50791,\"start\":50780},{\"end\":50803,\"start\":50791},{\"end\":50821,\"start\":50803},{\"end\":50842,\"start\":50821},{\"end\":51202,\"start\":51187},{\"end\":51212,\"start\":51202},{\"end\":51226,\"start\":51212},{\"end\":51240,\"start\":51226},{\"end\":51789,\"start\":51778},{\"end\":51800,\"start\":51789},{\"end\":51814,\"start\":51800},{\"end\":51829,\"start\":51814},{\"end\":51841,\"start\":51829},{\"end\":51862,\"start\":51841},{\"end\":51880,\"start\":51862},{\"end\":52778,\"start\":52758},{\"end\":52985,\"start\":52973},{\"end\":53000,\"start\":52985},{\"end\":53010,\"start\":53000},{\"end\":53024,\"start\":53010},{\"end\":53040,\"start\":53024},{\"end\":53057,\"start\":53040},{\"end\":53066,\"start\":53057},{\"end\":53073,\"start\":53066},{\"end\":54261,\"start\":54250},{\"end\":54270,\"start\":54261},{\"end\":54279,\"start\":54270},{\"end\":54291,\"start\":54279},{\"end\":54317,\"start\":54291},{\"end\":54324,\"start\":54317},{\"end\":54340,\"start\":54324},{\"end\":54349,\"start\":54340},{\"end\":54363,\"start\":54349},{\"end\":54379,\"start\":54363},{\"end\":54392,\"start\":54379},{\"end\":54765,\"start\":54748},{\"end\":54781,\"start\":54765},{\"end\":54787,\"start\":54781},{\"end\":54801,\"start\":54787},{\"end\":54815,\"start\":54801},{\"end\":55520,\"start\":55509},{\"end\":55536,\"start\":55520},{\"end\":55546,\"start\":55536},{\"end\":55865,\"start\":55853},{\"end\":55882,\"start\":55865},{\"end\":55899,\"start\":55882},{\"end\":57024,\"start\":57007},{\"end\":57038,\"start\":57024},{\"end\":57056,\"start\":57038},{\"end\":57070,\"start\":57056},{\"end\":57084,\"start\":57070},{\"end\":57098,\"start\":57084},{\"end\":57110,\"start\":57098},{\"end\":57121,\"start\":57110},{\"end\":57140,\"start\":57121},{\"end\":57592,\"start\":57582},{\"end\":57604,\"start\":57592},{\"end\":57619,\"start\":57604},{\"end\":57627,\"start\":57619},{\"end\":57643,\"start\":57627},{\"end\":57657,\"start\":57643},{\"end\":57670,\"start\":57657},{\"end\":57683,\"start\":57670},{\"end\":57865,\"start\":57859},{\"end\":58079,\"start\":58065},{\"end\":58093,\"start\":58079},{\"end\":58104,\"start\":58093},{\"end\":58117,\"start\":58104},{\"end\":58131,\"start\":58117},{\"end\":58999,\"start\":58986},{\"end\":59017,\"start\":58999},{\"end\":59031,\"start\":59017},{\"end\":59044,\"start\":59031},{\"end\":59056,\"start\":59044},{\"end\":59073,\"start\":59056},{\"end\":59086,\"start\":59073},{\"end\":59109,\"start\":59086},{\"end\":59453,\"start\":59438},{\"end\":59467,\"start\":59453},{\"end\":59483,\"start\":59467},{\"end\":59504,\"start\":59483},{\"end\":59520,\"start\":59504},{\"end\":59870,\"start\":59856},{\"end\":59886,\"start\":59870},{\"end\":59903,\"start\":59886},{\"end\":59920,\"start\":59903},{\"end\":59940,\"start\":59920},{\"end\":59958,\"start\":59940},{\"end\":59982,\"start\":59958},{\"end\":59994,\"start\":59982},{\"end\":60009,\"start\":59994},{\"end\":60025,\"start\":60009},{\"end\":60043,\"start\":60025},{\"end\":60059,\"start\":60043},{\"end\":60076,\"start\":60059},{\"end\":60084,\"start\":60076},{\"end\":60647,\"start\":60632},{\"end\":60673,\"start\":60647},{\"end\":60686,\"start\":60673},{\"end\":60704,\"start\":60686},{\"end\":61033,\"start\":61021},{\"end\":61043,\"start\":61033},{\"end\":61059,\"start\":61043},{\"end\":61073,\"start\":61059},{\"end\":61086,\"start\":61073},{\"end\":61099,\"start\":61086},{\"end\":61116,\"start\":61099},{\"end\":61126,\"start\":61116},{\"end\":61368,\"start\":61362},{\"end\":61620,\"start\":61608},{\"end\":61630,\"start\":61620},{\"end\":61646,\"start\":61630},{\"end\":61660,\"start\":61646},{\"end\":61673,\"start\":61660},{\"end\":61686,\"start\":61673},{\"end\":61703,\"start\":61686},{\"end\":61713,\"start\":61703},{\"end\":62106,\"start\":62094},{\"end\":62116,\"start\":62106},{\"end\":62126,\"start\":62116},{\"end\":62443,\"start\":62425},{\"end\":62462,\"start\":62443},{\"end\":62475,\"start\":62462},{\"end\":62486,\"start\":62475},{\"end\":62991,\"start\":62980},{\"end\":63006,\"start\":62991},{\"end\":63013,\"start\":63006},{\"end\":63029,\"start\":63013},{\"end\":63041,\"start\":63029},{\"end\":63057,\"start\":63041},{\"end\":63071,\"start\":63057},{\"end\":63088,\"start\":63071},{\"end\":63677,\"start\":63666},{\"end\":63687,\"start\":63677},{\"end\":63702,\"start\":63687},{\"end\":63714,\"start\":63702},{\"end\":63727,\"start\":63714},{\"end\":63742,\"start\":63727},{\"end\":63757,\"start\":63742},{\"end\":63771,\"start\":63757},{\"end\":63783,\"start\":63771},{\"end\":64212,\"start\":64197},{\"end\":64220,\"start\":64212},{\"end\":64228,\"start\":64220},{\"end\":64243,\"start\":64228},{\"end\":64257,\"start\":64243},{\"end\":64266,\"start\":64257},{\"end\":65158,\"start\":65145},{\"end\":65172,\"start\":65158},{\"end\":65186,\"start\":65172},{\"end\":65195,\"start\":65186},{\"end\":65204,\"start\":65195},{\"end\":65214,\"start\":65204},{\"end\":65227,\"start\":65214},{\"end\":65238,\"start\":65227},{\"end\":65248,\"start\":65238},{\"end\":65256,\"start\":65248},{\"end\":65267,\"start\":65256},{\"end\":65276,\"start\":65267},{\"end\":65286,\"start\":65276},{\"end\":66294,\"start\":66280},{\"end\":66304,\"start\":66294},{\"end\":66316,\"start\":66304},{\"end\":66331,\"start\":66316},{\"end\":66339,\"start\":66331},{\"end\":66353,\"start\":66339},{\"end\":66367,\"start\":66353},{\"end\":66380,\"start\":66367}]", "bib_venue": "[{\"end\":37807,\"start\":37714},{\"end\":39139,\"start\":39022},{\"end\":39946,\"start\":39877},{\"end\":40470,\"start\":40383},{\"end\":41141,\"start\":41085},{\"end\":41489,\"start\":41416},{\"end\":41804,\"start\":41728},{\"end\":42571,\"start\":42413},{\"end\":43536,\"start\":43405},{\"end\":44125,\"start\":44018},{\"end\":44745,\"start\":44665},{\"end\":45109,\"start\":45044},{\"end\":45443,\"start\":45377},{\"end\":45806,\"start\":45732},{\"end\":46366,\"start\":46203},{\"end\":47150,\"start\":47064},{\"end\":48091,\"start\":47998},{\"end\":49093,\"start\":48962},{\"end\":49912,\"start\":49738},{\"end\":50767,\"start\":50704},{\"end\":51307,\"start\":51240},{\"end\":51966,\"start\":51880},{\"end\":52756,\"start\":52679},{\"end\":53270,\"start\":53073},{\"end\":54248,\"start\":54200},{\"end\":54884,\"start\":54815},{\"end\":55587,\"start\":55564},{\"end\":56084,\"start\":55930},{\"end\":57005,\"start\":56919},{\"end\":57580,\"start\":57514},{\"end\":58250,\"start\":58166},{\"end\":58984,\"start\":58931},{\"end\":59436,\"start\":59349},{\"end\":59854,\"start\":59780},{\"end\":60630,\"start\":60509},{\"end\":61606,\"start\":61506},{\"end\":62092,\"start\":62039},{\"end\":62574,\"start\":62486},{\"end\":63170,\"start\":63088},{\"end\":63664,\"start\":63560},{\"end\":64383,\"start\":64297},{\"end\":65402,\"start\":65315},{\"end\":66278,\"start\":66216},{\"end\":40592,\"start\":40503},{\"end\":42784,\"start\":42621},{\"end\":44179,\"start\":44166},{\"end\":47279,\"start\":47202},{\"end\":48234,\"start\":48141},{\"end\":49211,\"start\":49095},{\"end\":50122,\"start\":49963},{\"end\":51333,\"start\":51309},{\"end\":52117,\"start\":52015},{\"end\":53547,\"start\":53349},{\"end\":54964,\"start\":54933},{\"end\":56323,\"start\":56158},{\"end\":58355,\"start\":58325},{\"end\":63179,\"start\":63172},{\"end\":64512,\"start\":64435},{\"end\":65547,\"start\":65460}]"}}}, "year": 2023, "month": 12, "day": 17}