{"id": 6423078, "updated": "2023-09-27 19:06:11.252", "metadata": {"title": "Holistically-Nested Edge Detection", "authors": "[{\"first\":\"Saining\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Zhuowen\",\"last\":\"Tu\",\"middle\":[]}]", "venue": "2015 IEEE International Conference on Computer Vision (ICCV)", "journal": "2015 IEEE International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2015, "month": 4, "day": 23}, "abstract": "We develop a new edge detection algorithm that tackles two important issues in this long-standing vision problem: (1) holistic image training and prediction; and (2) multi-scale and multi-level feature learning. Our proposed method, holistically-nested edge detection (HED), performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to approach the human ability resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of .782) and the NYU Depth dataset (ODS F-score of .746), and do so with an improved speed (0.4 second per image) that is orders of magnitude faster than some recent CNN-based edge detection algorithms.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1504.06375", "mag": "3081452934", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ijcv/XieT17", "doi": "10.1007/s11263-017-1004-z"}}, "content": {"source": {"pdf_hash": "4fbcec40b197db93d4754ffca02c49eba625f780", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1504.06375v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1504.06375", "status": "GREEN"}}, "grobid": {"id": "0692df3f222ada1b74b2d94474423eaf85e85d9e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4fbcec40b197db93d4754ffca02c49eba625f780.txt", "contents": "\nHolistically-Nested Edge Detection\n\n\nSaining Xie s9xie@eng.ucsd.edu \nDept. of CSE and Dept\nDept. of CogSci and Dept. of CSE\nCogSci University of California\nSan Diego 9500 Gilman Drive, La Jolla92093CA\n\nZhuowen Tu \nUniversity of California\nSan Diego 9500 Gilman Drive, La Jolla92093CA\n\nHolistically-Nested Edge Detection\n\nWe develop a new edge detection algorithm that tackles two important issues in this long-standing vision problem: (1) holistic image training and prediction; and (2) multi-scale and multi-level feature learning. Our proposed method, holistically-nested edge detection (HED), performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of .782) and the NYU Depth dataset (ODS F-score of .746), and do so with an improved speed (0.4 second per image) that is orders of magnitude faster than some recent CNN-based edge detection algorithms.\n\nIntroduction\n\nIn this paper, we tackle the problem of detecting edges and object boundaries in natural images, which is both fundamental and of great importance to a variety of computer vision areas ranging from traditional tasks such as visual saliency, segmentation, object detection/recognition, tracking and motion analysis, medical imaging, structure-frommotion and 3D reconstruction, to modern applications like autonomous driving, mobile computing, and image-to-text analysis. It has been long understood that precisely localizing edges in natural images involves visual perception of various \"levels\" [16,25]. A relatively comprehensive data collection and cognitive study [26] shows that while different subjects do have somewhat different preferences regarding where to place the edges and boundaries, there was nonetheless impressive consistency between subjects, e.g. reaching F-score 0.80 in the consistency study [26].\n\nThe history of computational edge detection is extremely (a) shows an example test image in the BSD500 dataset [26]; (b) shows its corresponding edges annotated by human subjects; (c) displays the result by HED. In the second row: (d), (e), and (f), respectively, show side edge responses from layers 2, 3, and 4 of our convolutional neural networks. In the third row: (g), (h), and (i), respectively, show edge responses from the Canny detector [4] at the scales \u03c3 = 2.0, \u03c3 = 4.0, and \u03c3 = 8.0. HED show its clear advantage in consistency over Canny. rich; we now highlight a few representative works that have proven to be of great practical importance. Broadly speaking, one may categorize works into a few groups such as I: early pioneering methods like the Sobel detector [18], zero-crossing [25,35], and the widely adopted Canny detector [4]; methods driven by II: information theory on top of features arrived at through careful manual design, such as Statistical Edges [20], Pb [26], and gPb [1]; and III: heavily learning-based methods that are still reliant on features of human design, such as BEL [5], Multi-scale [28], Sketch Tokens [22], and Structured Forests [6]. In addition, there has been a recent wave of development using Convolutional Neural Networks that emphasize the importance of automatic hierarchical feature learning, including N 4 -Fields [9], DeepContour [32], DeepEdge [2], and CSCNN [17]. Prior to this explosive development in deep learning, the Structured Forests method (typically abbreviated SE) [6] emerged as one of the most celebrated systems for edge detection, thanks to its state-of-the-art performance on the BSD500 dataset [26] (with, e.g., F-score of .746) and its practically significant speed of 2.5 frames per second. Recent CNN-based methods [9,32,2,17] have demonstrated promising F-score performance improvements over SE. Even so, there remains room for improvement in these CNN-based methods, certainly in F-score performance, but even more so in speed -at present, time to make a prediction ranges from several seconds [9] to a few hours [2] (even when using modern GPUs).\n\nHere, we develop an end-to-end edge detection system, holistically-nested edge detection (HED), that automatically learns the type of rich hierarchical features that are crucial if we are to approach the human ability to resolve ambiguity in natural image edge and object boundary detection. We acknowledge that it is slightly inaccurate to use the term \"nested\" in referring to those meaningful edge maps produced as side outputs -we intend to emphasize that many parts of the path along which each prediction is made are common to each of these edge maps, with successive edge maps including more. This integrated learning of hierarchical features is in distinction to previous multi-scale approaches [38,39,28] in which scale-space edge fields are neither automatically learned nor connected hierarchically. Figure 1 gives an illustration of an example image together with the human subject ground truth annotation, as well as results by the proposed HED edge detector (and also the responses in the individual layers), and results from the Canny edge detector with different scale parameters. Canny detection results at different scales are not only not directly connected, they also exhibit edge shift and inconsistency.\n\nThe proposed holistically-nested edge detection (HED) tackles two critical issues: (1) holistic image training and prediction; and (2) nested multi-scale feature learning, inspired by the fully convolutional neural networks [24] and by the deeply-supervised nets [21] that touch on, respectively, image-to-image classification and deep layer supervision (to \"guide\" early classification results). We find that the favorable characteristics of these underlying techniques manifest in HED being both accurate and computationally efficient.\n\n\nHolistically-Nested Edge Detection\n\nIn this section, we describe in detail the network structure of our proposed edge detection system. We start by discussing related neural-network-based approaches, particularly those that emphasize multi-scale and multi-level feature learning. The task of edge and object boundary detection is intrinsically challenging. After decades of research, there have emerged a number of properties that a researcher in the field might generally agree are key and that are likely to play a role in a successful system: (1) carefully designed and/or learned features [26,5], (2) multi-scale response fusion [38,30,28], (3) engagement of different levels of visual perception [16,25,37,15] such as mid-level Gestalt law information [7], (4) structural information [6] and context [36], (5) holistic image prediction [23], (6) 3D geometry [13], and (7) occlusion boundaries [14].\n\nThe Structured Forests method [6] primarily focusses on three of these aspects: using a large number of manually designed features (property 1), fusing multi-scale responses (property 2), and incorporating structural information (property 4). A recent wave of work using CNNs for patch-based edge prediction [9,32,2,17] share an alternative common thread that focusses on three aspects: automatic feature learning (property 1), multi-scale response fusion (property 2), and possible engagement of different levels of visual perception (property 3). However, due to the lack of deep supervision (that we include in our method), the multi-scale responses produced at the hidden layers in [2,17] are less semantically meaningful, since feedback must be back-propagated through the intermediate layers. More importantly, their patch-to-pixel or patchto-patch strategy results in significantly downgraded training and prediction efficiency. With some slight abuse of the term \"holistically-nested\", we emphasize here that we are producing an end-to-end edge detection system, a strategy inspired by fully convolutional neural networks [24], but with additional deep supervision on top of trimmed VGG nets [34] (shown in Figure 3). In the absence of deep supervision, a fully convolutional network [24] (FCN) produces a less satisfactory result (e.g. F-score .745 on BSD500) than HED, since edge detection demands highly accurate edge pixel localization. One thing worth mentioning is that our image-to-image training and prediction strategy still has not explicitly engaged contextual information, since constraints on the neighboring pixel labels are not directly enforced in HED. In addition to the speed gain over patchbased CNN edge detection methods, the performance gain is largely due to the following three aspects: (1) FCN-like image-to-image training allows us to simultaneously train on significantly larger amount of samples; (2) deep supervision in our model guides the learning of more transparent features; (3) upsampling in the side-output layers implicitly introduce guides the learning of more global information.\n\n\nExisting multi-scale and multi-level NN\n\nDue to the nature of hierarchical learning in the deep convolutional neural networks, the concept of multi-scale and multi-level learning might differ from situation to situation. For example, multi-scale learning can be \"inside\" the neural network, in the form of increasingly larger receptive fields and downsampled (strided) layers. In this \"inside\" case, the feature representations learned in each layer are naturally multi-scale. On the other hand, multi-scale learning can be \"outside\" of the neural network, for example by \"tweaking the scales\" of input images. While these two variants have some notable similarities, we have seen both of them applied to different tasks. We continue by next formalizing the possible configurations of multi-scale deep learning into 4 categories, namely, multi-stream learning, skip-net learning, a single model running on multiple inputs, and training of independent networks. Having these possibilities in mind will help make clearer the ways in which our proposed holistically-nested network approach differs from previous efforts and will help to highlight the important benefits in terms of representation and efficiency.\n\nMulti-stream learning [3] [27] A typical multi-stream learning architecture is illustrated in Fig 1 (a). Note that the multiple (parallel) network streams have different numbers of parameters and receptive field sizes, and thus correspond to multiple scales. Input data are simultaneously fed into multiple streams, after which the concatenated feature responses produced by the various streams are fed into a global output layer to produce the final result.\n\nSkip-layer network learning Examples of this form of network are [24][2][31] [9]. The key concept in \"skiplayer\" network learning is shown in Fig 1 (b). Instead of training multiple parallel streams, the topology for the skipnet architecture centers on a primary stream. Links are added to incorporate the feature responses from different levels of the primary network stream, and these responses are then combined in a shared output layer.\n\nThe common point in the two settings above is that, in both of the architectures, there is only one output layer so a single prediction is produced. However, in edge detection, it is often favorable (and, indeed, prevalent) to obtain multiple predictions and average the edge maps together.\n\nSingle model on multiple inputs To get multi-scale predictions, one can also run a single network on multiple (scaled) input images, illustrated in Fig 1 (c). This strategy can happen at both the training stage (as data augmentation) and at the testing stage (as \"ensemble testing\"). This approach is particularly common in non-deep-learning based methods [6]. Note that ensemble testing impairs the prediction efficiency of learning systems, with deeper models [2][9] possibly suffering more.\n\nTraining independent networks As an extreme variant to This is clearly impractical considering the factor by which this duplication would multiply the amount of resources required for training.\n\nHolistically-nested networks We list all of these variants to help clarify the distinction between existing approaches and our proposed holistically-nested network approach, illustrated in Fig 1 (e). As one may already notice, there is significant redundancy in existing approaches, both as regards representational power and computational efficiency. Our proposed holistically-nested network is a relatively simple variant that is able to produce predictions from multiple scales. The architecture can be interpreted as a \"holistically-nested\" version of the \"independent networks\" approach in Fig 1 (d); this perspective is what motivates our name. Our architecture comprises a single-stream deep network with multiple side outputs. We note that this architecture resembles several previous works, particularly the deeply-supervised net approach in which the authors show that hidden layer supervision can improve both optimization and generalization for image classification tasks.\n\n\nFormulations\n\nHere we formulate our approach for edge prediction. We denote our input training data set by S = {(I i , G i ), i = 1 . . . N }, where sample I i denotes the raw input image and G i denotes the corresponding ground truth binary edge map for sample I i . We subsequently drop the subscript i for notational simplicity, since we consider each image holistically and independently. The goal of our edge detection framework is to learn feature layers that minimize the side output layer classification error. Suppose in the network we have M side-output layers. Each side-output layer is associated with a classifier, in which the corresponding weights are denoted as w = (w (1) , . . . , w (M ) ). For simplicity, we denote the collection of all the other network layer parameters as W. We consider the objective function\nL side (I, G, W, w) = M m=1 \u03b1 m side (I, G, W, w (m) ) = M m=1 \u03b1 m \u2206(\u011c (m) , G; W, w),(1)\nwhere side denotes the image-level loss function for sideoutputs,\u011c (m) is the (predicted) edge map produced by side-output layer m, upsampled to original size when necessary, \u2206 is an \"energy function\", (e.g. cross-entropy) computing the loss of the predicted edge map over the ground truth target, and \u03b1 m is a hyper-parameter controlling the loss weight for each individual side-output layer.\n\nUnder the hood of image-to-image training, the loss function is computed over all pixels in an image sample I. For a typical natural image, the distribution of edge/nonedge pixels is heavily biased: 90% of the ground truth is non-edge. A cost-sensitive loss function is proposed in [17], where additional trade-off parameters are introduced for biased sampling. Here instead we use a simpler strategy to automatically balance the loss between positive/negative classes. We introduce a class-balancing weight \u03b2 j on a perpixel term basis, where index j is over the image spatial dimensions of image I. Then we use this class-balancing weight as a simple way to offset this imbalance between edge/non-edge. Specifically we define the following classbalanced cross-entropy loss function used in Equation (1) \n\u2206 = \u2212 \u03b2 j |I| j=1 G j log Pr(G j = 1|, I j , W, w) \u2212 (1 \u2212 \u03b2 j ) |I| j=1 (1 \u2212 G j ) log Pr(G j = 0|, I j , W, w),(2)\nwhere we denote |I|,|I| \u2212 and |I| + as total number of all pixels, non-edge (negative) pixels and edge (positive) pixels in image I, respectively. \u03b2 j = |I|\u2212 |I| (when I j is a positive) and 1 \u2212 \u03b2 j = |I|+ |I| (when I j is a negative).\n\n\nTrimmed Network for Edge Detection\n\nThe choice of hierarchy for our framework deserves some thought. We need the architecture (1) to be deep, so as to efficiently generate perceptually multi-level features; and (2) to have multiple stages with different strides, so as to capture the intrinsic scales of edge maps. We must also keep in mind the potential difficulty in training such deep neural networks with multiple stages when starting from scratch.\n\nRecently, VGGNet [34] achieved state-of-the-art performance in the ImageNet image classification challenge. We find that the VGGNet architecture design is a satisfactory starting point, since it is very deep (16 convolutional layers), dense (stride-1 convolutional kernels), and has multiple stages (five 2-strided downsampling layers). Other recent work [2] demonstrates that fine-tuning deep neural networks pre-trained on general image classification problem can be helpful to a low-level edge detection task. We find that the architecture designed in VGGNet is well-suited to our needs, although we do make the following modifica- tions: (a) we connect our side output layer to the last convolutional layer in each stage, respectively conv1 1, conv2 2, conv3 3, conv4 3, conv5 3. The side-output layer is implemented as a convolutional layer with kernel size 1 and number of outputs 1. Therefore the receptive field size of each of these convolutional layers is identical to the corresponding side-output layer; (b) we cut the last stage of VG-GNet, including all the fully connected layers and the 5th pooling layer. The reason for \"trimming\" the VGGNet is two-fold. Firstly, because we are expecting meaningful side outputs with different scales, a layer with stride 32 yields a too-small output plane with the consequence that the interpolated prediction map will be too fuzzy to utilize. Second, the fully connected layers (even when recast as convolutions) are computationally intensive, so that trimming pooling layer 5 and beyond can significantly reduce the memory/time cost during both training and testing. At the end of this process, our HED network architecture has 5 stages, with strides 1, 2, 4, 8 and 16, respectively, and with different receptive field sizes, all nested in the VGGNet. See Table 1 for a summary of configurations of receptive field and neuron stride.\n\n\nWeighted-fusion Layer\n\nOne natural question is how to best utilize the sideoutputs produced by our framework; while the most straightforward approach is to simply average all the edge map predictions, one can also learn a classifier to optimize the combination weights, either at pixel level (locally connected) or at image level. We add a \"weighted-fusion\" layer to the network and link all the side-output layer predictions and (simultaneously) learns the fusion weight during training. Denoting this fusion weight as h \u2208 R M , our loss function at the fusion layer L fuse becomes\nL fuse (I, G, W, w) = \u2206( M m=1 h m\u011c (m) , G; W, w)\nWith \u2206 still being the class-balancing cross-entropy loss function as defined in Equation (2). The overall loss function then becomes L(I, G, W, w) = L side (I, G, W, w) + L fuse (I, G, W, w)\n\nWe train this loss via standard back-propagation stochastic gradient descent. See section 3 for detailed hyper-parameter and experiment settings.\n\nHidden-layer supervision Since we incorporate a weighted-fusion output layer that connects each sideoutput layer, one might argue that the hidden supervision terms (specifically, side (I, G, W, w)) are unnecessary in this formulation, since now the whole network is path-connected and the output-layer parameters can be updated by back-propagation through the weighted-fusion layer error propagation path. In contrast to this view, we argue that deep supervision is important to obtain desired edge maps. The key characteristic of our proposed network is that each network layer is supposed to play a role as a singleton network responsible for producing an edge map at a certain scale. We show qualitative results based on the two variants discussed above: training with both weightedfusion supervision and deep supervision, or training with weighted-fusion supervision only. We observe that with deep supervision, the nested side-outputs are meaningful and agree with expectations, insofar as the successive edge map predictions are progressively coarse-to-fine, local-to-global. On the other hand, training with only the weighted-fusion output loss gives edge predictions that lack any such discernible order. We also find that many critical edges are lost at the high-level side output. Our claim is further supported quantitatively by the experimental results on benchmark dataset, which will be discussed later. Figure 4. Two examples illustrating how deep supervision helps sideoutput layers produce multi-scale dense predictions. Note that in the left column, the side outputs become progressively coarser and more \"global\", while critical object boundaries are preserved. In the right column, the predictions tends to lack any discernible order (e.g. in layers 1 and 2), and many boundaries are lost in later stages.\n\n\nw/o deep supervision w/ deep supervision w/o deep supervision w/ deep supervision\n\n\nExperimental Framework\n\nModel Parameters In contrast to fine-tuning a classification net to perform a high-level computer vision task like fine-grained image classification or semantic segmentation, adapting the classification net to a low-level edge detection task is much more challenging. Differences in data distribution, ground truth distribution, and loss function all contribute to difficulties in network convergence, even with the initialization of the pre-trained model. We first use a validation set and follow the evaluation strategy used in [6] to tune the deep model hyper-parameters. The parameters we tune (and the values we choose) are: the size of the minibatch (10), learning rate (1e-6), the loss-weight \u03b1 m for each side-output layer (1), momentum (0.9), initialization of the nested filters (0), initialization of fusion layer weight (1/5), weight decay (0.0002), training iterations (10,000; reduce learning rate by 1/10 after 5,000). We focus on the convergence behavior of the network, since we observe that whenever training converges, the deviations in F-score on the validation set tend to be very small. In order to investigate whether including additional nonlinearity helps, we also consider a setting in which we add an additional layer (with 50 filters and a ReLU) before each side-output layer; we find that performance here is worse. On another note, we observe that our nested multi-scale framework is insensitive to input image scales; during our training process, we take advantage of this by resizing all the images to 400 \u00d7 400 to reduce GPU memory usage and to take advantage of efficient batch processing. In later experiments, we fix the values of all parameters discussed above and explore other variants of HED on the fully-independent test set. Consensus Sampling In our approach, we duplicate the ground truth at each side-output layer and resize the (downsampled) side output to its original scale. Thus there exists a mismatch in the high-level side-outputs: The edge predictions are coarse and global, while the ground truth still contains many weak edges that could even be considered as noise. This issue leads to problematic convergence behavior, even with the help of a pre-trained model. We observe that this mismatch leads to back-propagated gradients that explode at the high-level side-output layers. We can, however, adjust how we make use of the ground truth labels in the BSDS dataset to combat this issue. Specifically, the ground truth labels are provided by multiple annotators and thus, implicitly, greater labeler consensus indicates stronger ground truth edges. We adopt a relatively brute-force solution: we assign a pixel a positive label if and only if it is labeled as positive by at least three annotators, and put all other positively labeled pixels with only one or two labeler consensus into the negative set. This addresses the problem of gradient explosion in high level side-output layers. For low level layers, reducing positive pixels brings additional robustness and prevents the network from being distracted by weak edges. Though not fully explored in our paper, we believe that careful handling of consensus levels of ground truth edge maps can lead to further improvement. Data Augmentation Data augmentation has proven to be a crucial technique in deep networks. We rotate the images to 16 different angles and crop the largest rectangle in the rotated image; we also flip the image at each angle, thus augmenting the training set by a factor of 32. We find that augmenting the data to different scales is unnecessary. During testing we operate on an image in its original scale. We also note that \"ensemble testing\" (testing on rotated/flipped images and averaging the prediction) yields no improvements in either F-score or average precision.\n\n\nArchitecture alternatives\n\nFCN and skip-layer architecture The topology used in the FCN model differs from our HED model in several aspects. As we have discussed, while FCN reinterprets the classification nets for pixel-wise prediction, it has only one single stream output. In FCN, the skip net structure is a DAG that combines coarse, high layer information with fine low layer information, without explicitly producing multi-scale output predictions. We explore how this architecture can be used for the edge detection task, under exactly the same experimental setup as our HED model. We first try to directly apply the FCN-8s model by replacing the loss function with cross-entropy for edge detection. This results in a model that achieves ODS=.725, OIS=.743 and AP = .680. This unsatisfactory result can be expected since this architecture is still not fine enough. We further explore whether the performance can be improved by adding even more links from low-level layers. We then create an FCN-2s network that adds additional links from the pool1 and pool2 layers, the performance improves to ODS=.745, OIS=.765, AP= .730. Still, directly applying FCN skip-net topology falls behind our proposed holistically nested architecture in edge detection tasks. We feel that, with heavy tweaking of the FCN, one might also be able to achieve competitive performance on edge detection, but we value the multi-scale side-outputs for their ability to provide additional flexibility, especially for edge detection. Different pooling function Previous work [2] suggests that different pooling functions can have a major impact on the performance for edge detection. We conduct a controlled experiment in which all pooling layers are replaced by average pooling. In contrast to the observation in [2], we find using average pooling decrease the performance to ODS=.741. In-network linear interpolation Built on top of [24], side-output prediction upsampling is implemented with innetwork deconvolutional layers. We fix all the deconvolutional layers to linear interpolation. Although [24] points out that one can learn arbitrary interpolation functions, we find that learned deconvolutions provide no noticeable improvements in our experiments. Training without deep supervision As we have discussed in previous section, the deep supervision plays an important role in HED. We test the model trained with/without deep supervision imposed. The model trained without deep supervision consistently performs worse (At least .01 in Fscore and .02 in AP).\n\n\nImplementation\n\nWe implement our framework in publicly available Caffe Library and build on top of the publicly available implementations of FCN [24] and DSN [21], thus relatively little engineering hacking is required. In our HED system the whole network is fine-tuned from an initialization with the VGG-16 Net pre-trained model. Running time Training takes about 7 hours on a single NVIDIA K40 GPU. Testing is also efficient. For an 320 \u00d7 480 image, HED takes 400 ms to make an edge prediction, including the interface overhead. Many edge detectors improve performance by sacrificing efficiency (for example, by testing on input images from multiple scales and averaging the results).\n\n\nResults\n\nIn this section we report the performance of our proposed algorithm.   Figure 5. Results on BSDS500 dataset. Our proposed HED framework achieve the best result (ODS=.782). Compared to several recent CNNbased edge detectors, our approach is also orders of magnitude faster. See Table 2 for detailed information.  [8] .610 .640 .560 10 BEL [5] .660 * --1/10 gPb-owt-ucm [1] .726 .757 .696 1/240 Sketch Tokens [22] .727 .746 .780 1 SCG [29] .739 .758 .773 1/280 SE-Var [6] .746 .767 .803 2.5 OEF [12] .749 .772 .817 -DeepNets [19] .738 .759 .758 1/5 \u2020 N4-Fields [9] .753 .769 .784 1/6 \u2020 DeepEdge [2] .753 .772 .807 1/10 3 \u2020 CSCNN [17] .756 .775 .798 -DeepContour [32] .  Figure 5 and Table 2.\n\nLate merging to boost average precision We find that the weighted-fusion layer output gives best performance in Fscore. However the average precision degrades compared to directly averaging all the side outputs. This is due to the fact that during training we are focusing on \"global\" object boundaries for the fusion-layer weight learning. However, because in HED all the side outputs are readily available after a single test, we can merge the fusion layer output with the directly averaged counterpart, at no extra cost, to compensate for the loss in average precision. This simple heuristic gives us the best performance across all measures that we report in Figure 5 and Table 2.\n\nMore training data Deep models drive advances in computer vision due to large learning capacity and huge training data. Unfortunately, in edge detection, we are limited by the number of training images available in the dataset. Here we want to explore whether adding more training data will allow us to further improve the results. To do this, we expand the training set by random sampling 100 images from the testing set. We then evaluate the result on the rest 100 test images. We report the averaged result over 5-fold trials. From the result we observe that by only adding 100 training images, the performance improves from ODS=.782 to ODS=.797 (\u00b1.003), and nearly touches the human benchmark. We believe that, with even larger annotated dataset, the performance can be further improved. Table 2 and Figure 5 lists the precisionrecall results obtained by HED relative to other competing methods. Table 3 summarizes the results produced by each individual side-output at different scales, as well as different combinations of the multi-scale edge maps. We want to emphasize that all the side-output predictions are obtained all in one pass, which enables us to fully investigate different configurations of combining the outputs at no extra cost. There are several interesting observations from the results. For instance, combination of predictions from multiple scales yields better performance, and all the side-output layers contribute to the result, either in F-score or averaged precision. To see this, in Table 3, the side-output layer 1 and layer 5 (the lowest and highest layers) achieve similar relatively low performance. One might expect these two side-output layers to not be useful in the averaged results. However this turns out not to be the case -for example, the Average 1-4 achieves ODS=.760 and incorporating the side-output layer 5, the averaged prediction achieves an ODS=.774. We find similar phenomenon when considering other ranges. As mentioned above, the predictions obtained using different combination strategies are complementary, and a late merging of the averaged predictions with learned fusion-layer predictions leads to the best result. NYUDv2 Dataset The NYU Depth (NYUD) dataset [33] has 1449 RGB-D images. The dataset was used for edge detection in [29] and [10]. Here we follow the [6] and perform the experiment on the data set processed by [10]. the NYUD dataset is split into 381 training, 414 validation, and 654 testing images. All images are cropped to the same dimension, thus we train our network on full resolution images. Finally following the experimental setup in [11] and [6], the maximum tolerance allowed for correct matches of edge predictions to ground truth during evaluation increases from .0075 to .011.   Table 4 for additional information.  [33] .658\n\n\nResults discussion\n\n.661 -1/360+ gPb+NG [10] .687 .716 .629 1/375 SE [6] .685 .699 .679 5 SE+NG+ [11] . Depth information encoding Following the success in [11] and [24], we leverage the depth information by utilizing the HHA feature, which the depth information is embedded into three-channel feature, i.e. horizontal disparity, height above ground, and the angle of the local surface normal with the inferred gravity direction. We use the same architectures and parameter settings as the experiments on BSDS 500. We train two different models in parallel, on RGB images and HHA feature images, and report the results respectively. Then we directly average the RGB and HHA predictions as our final result leveraging RGB-D information. We have tried other approaches to incorporate the depth information, for example, train on the raw depth channel, or concatenate the depth channel with the RGB channel before the first convolutional layer. None of these attempts yields competitive performance compared to using HHA. The effectiveness of HHA feature shows that, although deep neural networks are capable of automatic feature learning, for depth data, carefully hand-designed features are still necessary, especially when very limited training data is available Results discussion Table 4 and Figure 6 show the precision-recall evaluations of HED vs other competing methods. All the network structures for training are kept the same as for BSDS. During testing we use Average2-4 prediction instead of Fusion-layer output as it yields the best performance. We do not perform the late merging, since by combining two sources of edge map predictions (RGB and HHA), the average precision is already high. Note that the results achieved by only using RGB modality have already bypassed all the previous approaches.\n\n\nDiscussion and Conclusion\n\nIn this paper, we have developed a new convolutionalneural-network-based edge detection system that demonstrates state-of-the-art performance on natural images at a speed of practical importance (e.g. 0.4 second using GPU and 12 seconds on CPU). Our algorithm builds on top of the ideas of fully convolutional neural networks and deeplysupervised nets. We also initialize our network structure and parameters by adopting a pre-trained trimmed VG-GNets. Our method shows promising results in performing image-to-image learning and testing for edge detection by combining multi-scale and multi-level visual responses, even though explicit contextual and high-level information has not been enforced.\n\n\nAcknowledgment\n\nThis work is supported by NSF IIS-1216528 (IIS-1360566) and NSF award IIS-0844566 (IIS-1360568). We gratefully thank Patrick Gallagher for helping improve this manuscript. We are grateful for the generous donation of the GPUs by NVIDIA.\n\nFigure 1 .\n1Illustration of the proposed HED algorithm. In the first row:\n\nFigure 2 .\n2Illustration of different multi-scale deep learning settings. (a) is multi-stream architecture; (b) is skip-layer net architecture; (c) is a single model running on multi-scale inputs; (d) separate training of different networks; (e) is our proposed holistically-nested architectures, where multiple side outputs are added.\n\nFig 1 (\n1a), one might pursueFig 1 (d), in which multi-scale predictions are made by training multiple independent networks with different depths and different output loss layers.\n\nFigure 3 .\n3Illustration of the our network architecture for edge detection, highlighting the error backpropagation paths. Side-output layers are inserted after convolutional layers. Deep supervision is imposed at each sideoutput layer, guiding the side-outputs towards edge predictions with the characteristics we desire. The outputs of HED are multi-scale and multilevel, as the side-output plane size becomes smaller, and the receptive field size becomes larger. In the setting of edge detection, one weighted-fusion layer can be added to automatically learn how to combine outputs from multiple scales. The whole network is trained with multiple error propagation paths (dashed lines).\n\n\nBSDS500 dataset We perform the majority of the experiments on the Berkeley Segmentation Dataset and Bench-\n\nFigure 6 .\n6Precision/recall curves on NYUD dataset. Holistically-Edge Detection (HED) trained with RGB and HHA feature achieves the best result (ODS=.746). See\n\nTable 1 .\n1The receptive field and stride size in VGGNet[34]. The bolded convolutional layers are linked to additional side-output layers.layer c1 2 \np1 \nc2 2 \np2 \nc3 3 \nrf size \n5 \n6 \n14 \n16 \n40 \nstride \n1 \n2 \n2 \n4 \n4 \nlayer \np3 \nc4 3 \np4 \nc5 3 \np5 \nrf size \n44 \n92 \n100 \n196 \n212 \nstride \n8 \n8 \n16 \n16 \n32 \n\n\n\nTable 2 .\n2Results on BSDS500.  * BSDS300 results, \u2020GPU time \n\nODS OIS \nAP \nFPS \nHuman \n.80 \n.80 \n-\n-\nCanny \n.600 .640 .580 15 \nFelz-Hutt \n\nTable 3 .\n3Results of single and averaged side output in HED on \nthe BSDS 500 dataset. Each individual side output contributes to \nthe fused/averaged result. Note that the learned weighted-fusion \n(Fusion-output) achieves best F-score, while directly averaging all \nof the five layers (Average 1-5) produces better average precision. \nMerging those two readily available outputs further boost the per-\nformance. \n\nODS \nOIS \nAP \nSide-output 1 \n.595 \n.620 \n.582 \nSide-output 2 \n.697 \n.715 \n.673 \nSide-output 3 \n.738 \n.756 \n.717 \nSide-output 4 \n.740 \n.759 \n.672 \nSide-output 5 \n.606 \n.611 \n.429 \nFusion-output \n.782 \n.802 \n.787 \nAverage 1-4 \n.760 \n.784 \n.800 \nAverage 1-5 \n.774 \n.797 \n.822 \nAverage 2-4 \n.766 \n.788 \n.798 \nAverage 2-5 \n.777 \n.800 \n.814 \nMerged result \n.782 \n.804 \n.833 \n\n\n\n\nF=.746] HED (ours) [F=.710] SE+NG+ [F=.695] SE [F=.685] gPb+NG [F=.655] Silberman [F=.629] gPb\u2212owt\u2212ucm0 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \n0.6 \n0.7 \n0.8 \n0.9 \n1 \n0 \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\n0.7 \n\n0.8 \n\n0.9 \n\n1 \n\nRecall \n\nPrecision \n\n[\n\nTable 4 .\n4Results on the NYUD dataset[33] \u2020GPU timeODS \nOIS \nAP \nFPS \ngPb-ucm \n.632 \n.661 \n.562 \n1/360 \nSilberman \n\nContour detection and hierarchical image segmentation. P Arbelaez, M Maire, C Fowlkes, J Malik, PAMI33P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Con- tour detection and hierarchical image segmentation. PAMI, 33(5):898-916, 2011. 1, 7\n\nDeepedge: A multiscale bifurcated deep network for top-down contour detection. G Bertasius, J Shi, L Torresani, CVPR. 67G. Bertasius, J. Shi, and L. Torresani. Deepedge: A multi- scale bifurcated deep network for top-down contour detec- tion. In CVPR, 2015. 1, 2, 3, 4, 6, 7\n\nMultiscale convolutional neural networks for vision-based classification of cells. P Buyssens, A Elmoataz, O L\u00e9zoray, ACCV. 2013. 3P. Buyssens, A. Elmoataz, and O. L\u00e9zoray. Multiscale con- volutional neural networks for vision-based classification of cells. In ACCV. 2013. 3\n\nA computational approach to edge detection. J Canny, PAMI. 6J. Canny. A computational approach to edge detection. PAMI, (6):679-698, 1986. 1\n\nSupervised learning of edges and object boundaries. P Dollar, Z Tu, S Belongie, CVPR. IEEE2P. Dollar, Z. Tu, and S. Belongie. Supervised learning of edges and object boundaries. In CVPR, volume 2, pages 1964-1971. IEEE, 2006. 1, 2, 7\n\nFast edge detection using structured forests. P Doll\u00e1r, C L Zitnick, PAMI. 78P. Doll\u00e1r and C. L. Zitnick. Fast edge detection using struc- tured forests. PAMI, 2015. 1, 2, 3, 5, 7, 8\n\nEcological statistics of gestalt laws for the perceptual organization of contours. J H Elder, R M Goldberg, Journal of Vision. 24J. H. Elder and R. M. Goldberg. Ecological statistics of gestalt laws for the perceptual organization of contours. Journal of Vision, 2(4):5, 2002. 2\n\nEfficient graphbased image segmentation. P F Felzenszwalb, D P Huttenlocher, IJCV. 592P. F. Felzenszwalb and D. P. Huttenlocher. Efficient graph- based image segmentation. IJCV, 59(2):167-181, 2004. 7\n\nY Ganin, V Lempitsky, arXiv:1406.6558N4-fields: Neural network nearest neighbor fields for image transforms. arXiv preprintY. Ganin and V. Lempitsky. N4-fields: Neural network near- est neighbor fields for image transforms. arXiv preprint arXiv:1406.6558, 2014. 1, 2, 3, 7\n\nPerceptual organization and recognition of indoor scenes from rgb-d images. S Gupta, P Arbelaez, J Malik, CVPR. S. Gupta, P. Arbelaez, and J. Malik. Perceptual organiza- tion and recognition of indoor scenes from rgb-d images. In CVPR, 2013. 8\n\nLearning rich features from rgb-d images for object detection and segmentation. S Gupta, R Girshick, P Arbel\u00e1ez, J Malik, ECCV. S. Gupta, R. Girshick, P. Arbel\u00e1ez, and J. Malik. Learning rich features from rgb-d images for object detection and seg- mentation. In ECCV, 2014. 8\n\n. S Hallman, C C Fowlkes, arXiv:1412.4181Oriented edge forests for boundary detection. arXiv preprintS. Hallman and C. C. Fowlkes. Oriented edge forests for boundary detection. arXiv preprint arXiv:1412.4181, 2014. 7\n\nPutting objects in perspective. D Hoiem, A A Efros, M Hebert, IJCVD. Hoiem, A. A. Efros, and M. Hebert. Putting objects in perspective. IJCV, 80(1):3-15, 2008. 2\n\nRecovering occlusion boundaries from a single image. D Hoiem, A N Stein, A A Efros, M Hebert, ICCV. D. Hoiem, A. N. Stein, A. A. Efros, and M. Hebert. Recov- ering occlusion boundaries from a single image. In ICCV, 2007. 2\n\nBoundary detection benchmarking: Beyond f-measures. X Hou, A Yuille, C Koch, CVPR. X. Hou, A. Yuille, and C. Koch. Boundary detection bench- marking: Beyond f-measures. In CVPR, 2013. 2\n\nReceptive fields, binocular interaction and functional architecture in the cat's visual cortex. D H Hubel, T N Wiesel, The Journal of physiology. 1601D. H. Hubel and T. N. Wiesel. Receptive fields, binocular interaction and functional architecture in the cat's visual cor- tex. The Journal of physiology, 160(1):106-154, 1962. 1, 2\n\nPixel-wise deep learning for contour detection. ICLR. J.-J Hwang, T.-L Liu, J.-J. Hwang and T.-L. Liu. Pixel-wise deep learning for con- tour detection. ICLR, 2015. 1, 2, 4, 7\n\nOn the accuracy of the sobel edge detector. J Kittler, Image and Vision Computing. 11J. Kittler. On the accuracy of the sobel edge detector. Image and Vision Computing, 1(1):37-42, 1983. 1\n\nVisual boundary prediction: A deep neural prediction network and quality dissection. J J Kivinen, C K Williams, N Heess, D Technologies, AISTATS. J. J. Kivinen, C. K. Williams, N. Heess, and D. Technolo- gies. Visual boundary prediction: A deep neural prediction network and quality dissection. In AISTATS, 2014. 7\n\nStatistical edge detection: Learning and evaluating edge cues. S Konishi, A L Yuille, J M Coughlan, S C Zhu, PAMI25S. Konishi, A. L. Yuille, J. M. Coughlan, and S. C. Zhu. Sta- tistical edge detection: Learning and evaluating edge cues. PAMI, 25(1):57-74, 2003. 1\n\nDeeplysupervised nets. C.-Y Lee, S Xie, P Gallagher, Z Zhang, Z Tu, AISTATS. 26C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply- supervised nets. In AISTATS, 2015. 2, 6\n\nSketch tokens: A learned mid-level representation for contour and object detection. J J Lim, C L Zitnick, P Doll\u00e1r, CVPR. 7J. J. Lim, C. L. Zitnick, and P. Doll\u00e1r. Sketch tokens: A learned mid-level representation for contour and object de- tection. In CVPR, 2013. 1, 7\n\nNonparametric scene parsing via label transfer. C Liu, J Yuen, A Torralba, PAMI33C. Liu, J. Yuen, and A. Torralba. Nonparametric scene pars- ing via label transfer. PAMI, 33(12):2368-2382, 2011. 2\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. 68J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. CVPR, 2015. 2, 3, 6, 8\n\nTheory of edge detection. D Marr, E Hildreth, Proceedings of the Royal Society of London. Series B. Biological Sciences. the Royal Society of London. Series B. Biological Sciences207D. Marr and E. Hildreth. Theory of edge detection. Pro- ceedings of the Royal Society of London. Series B. Biological Sciences, 207(1167):187-217, 1980. 1, 2\n\nLearning to detect natural image boundaries using local brightness, color, and texture cues. D R Martin, C C Fowlkes, J Malik, PAMI. 265D. R. Martin, C. C. Fowlkes, and J. Malik. Learning to detect natural image boundaries using local brightness, color, and texture cues. PAMI, 26(5):530-549, 2004. 1, 2\n\nMultiscale deep learning for gesture detection and localization. N Neverova, C Wolf, G W Taylor, F Nebout, ECCV Workshops. N. Neverova, C. Wolf, G. W. Taylor, and F. Nebout. Multi- scale deep learning for gesture detection and localization. In ECCV Workshops, 2014. 3\n\nMulti-scale improves boundary detection in natural images. X Ren, ECCV. 1X. Ren. Multi-scale improves boundary detection in natural images. In ECCV. 2008. 1, 2\n\nDiscriminatively trained sparse code gradients for contour detection. X Ren, L Bo, NIPS. 7X. Ren and L. Bo. Discriminatively trained sparse code gra- dients for contour detection. In NIPS, pages 584-592, 2012. 7, 8\n\nStatistics of natural images: Scaling in the woods. D L Ruderman, W Bialek, Physical review letters. 736814D. L. Ruderman and W. Bialek. Statistics of natural images: Scaling in the woods. Physical review letters, 73(6):814, 1994. 2\n\nConvolutional neural networks applied to house numbers digit classification. P Sermanet, S Chintala, Y Lecun, ICPR. P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neu- ral networks applied to house numbers digit classification. In ICPR, 2012. 3\n\nDeepcontour: A deep convolutional feature learned by positivesharing loss for contour detection draft version. W Shen, X Wang, Y Wang, X Bai, Z Zhang, CVPR. W. Shen, X. Wang, Y. Wang, X. Bai, and Z. Zhang. Deep- contour: A deep convolutional feature learned by positive- sharing loss for contour detection draft version. In CVPR, 2015. 1, 2, 7\n\nIndoor segmentation and support inference from rgbd images. N Silberman, D Hoiem, P Kohli, R Fergus, ECCV. 2012. 8N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV. 2012. 8\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR. 5K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 2, 4, 5\n\nOn edge detection. V Torre, T A Poggio, PAMI. 2V. Torre and T. A. Poggio. On edge detection. PAMI, (2):147-163, 1986. 1\n\nAuto-context and its application to high-level vision tasks. Z Tu, CVPR. Z. Tu. Auto-context and its application to high-level vision tasks. In CVPR, 2008. 2\n\nNeural mechanisms of form and motion processing in the primate visual system. D C Van Essen, J L Gallant, Neuron. 131D. C. Van Essen and J. L. Gallant. Neural mechanisms of form and motion processing in the primate visual system. Neuron, 13(1):1-10, 1994. 2\n\nScale-space filtering: A new approach to multiscale description. A P Witkin, ICASSP. 9A. P. Witkin. Scale-space filtering: A new approach to multi- scale description. In ICASSP, volume 9, pages 150-153, 1984. 2\n\nScaling theorems for zero crossings. A L Yuille, T A Poggio, PAMI. 1A. L. Yuille and T. A. Poggio. Scaling theorems for zero crossings. PAMI, (1):15-25, 1986. 2\n", "annotations": {"author": "[{\"end\":202,\"start\":38},{\"end\":285,\"start\":203}]", "publisher": null, "author_last_name": "[{\"end\":49,\"start\":46},{\"end\":213,\"start\":211}]", "author_first_name": "[{\"end\":45,\"start\":38},{\"end\":210,\"start\":203}]", "author_affiliation": "[{\"end\":201,\"start\":70},{\"end\":284,\"start\":215}]", "title": "[{\"end\":35,\"start\":1},{\"end\":320,\"start\":286}]", "venue": null, "abstract": "[{\"end\":1238,\"start\":322}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1853,\"start\":1849},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1856,\"start\":1853},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1925,\"start\":1921},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2171,\"start\":2167},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2289,\"start\":2285},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2623,\"start\":2620},{\"end\":2724,\"start\":2718},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2954,\"start\":2950},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2974,\"start\":2970},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2977,\"start\":2974},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3020,\"start\":3017},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3154,\"start\":3150},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3163,\"start\":3159},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3176,\"start\":3173},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3285,\"start\":3282},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3303,\"start\":3299},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3323,\"start\":3319},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3351,\"start\":3348},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3545,\"start\":3542},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3563,\"start\":3559},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3577,\"start\":3574},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3593,\"start\":3589},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3709,\"start\":3706},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3845,\"start\":3841},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3968,\"start\":3965},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3971,\"start\":3968},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3973,\"start\":3971},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3976,\"start\":3973},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4249,\"start\":4246},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4268,\"start\":4265},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5008,\"start\":5004},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5011,\"start\":5008},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5014,\"start\":5011},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5756,\"start\":5752},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5795,\"start\":5791},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6665,\"start\":6661},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6667,\"start\":6665},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6705,\"start\":6701},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6708,\"start\":6705},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6711,\"start\":6708},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6773,\"start\":6769},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6776,\"start\":6773},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6779,\"start\":6776},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6782,\"start\":6779},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6828,\"start\":6825},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6860,\"start\":6857},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6877,\"start\":6873},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6913,\"start\":6909},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6935,\"start\":6931},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6970,\"start\":6966},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7006,\"start\":7003},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7284,\"start\":7281},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7287,\"start\":7284},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7289,\"start\":7287},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7292,\"start\":7289},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7662,\"start\":7659},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7665,\"start\":7662},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8107,\"start\":8103},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8177,\"start\":8173},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8269,\"start\":8265},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10338,\"start\":10335},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10842,\"start\":10838},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10853,\"start\":10850},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11866,\"start\":11863},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11972,\"start\":11969},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14788,\"start\":14784},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16137,\"start\":16133},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16474,\"start\":16471},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21449,\"start\":21446},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26279,\"start\":26276},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26518,\"start\":26515},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26640,\"start\":26636},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26806,\"start\":26802},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27419,\"start\":27415},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27432,\"start\":27428},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28284,\"start\":28281},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28310,\"start\":28307},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28340,\"start\":28337},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28380,\"start\":28376},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28406,\"start\":28402},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28438,\"start\":28435},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28466,\"start\":28462},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28496,\"start\":28492},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28531,\"start\":28528},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28565,\"start\":28562},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28600,\"start\":28596},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28633,\"start\":28629},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31568,\"start\":31564},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31639,\"start\":31635},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31648,\"start\":31644},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31672,\"start\":31669},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31733,\"start\":31729},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31967,\"start\":31963},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31975,\"start\":31972},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32154,\"start\":32150},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32206,\"start\":32202},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32234,\"start\":32231},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32263,\"start\":32259},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32322,\"start\":32318},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32331,\"start\":32327},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":36571,\"start\":36567},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":38031,\"start\":38027}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35029,\"start\":34955},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35366,\"start\":35030},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35547,\"start\":35367},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36238,\"start\":35548},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36347,\"start\":36239},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36509,\"start\":36348},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36821,\"start\":36510},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36961,\"start\":36822},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37747,\"start\":36962},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":37987,\"start\":37748},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38104,\"start\":37988}]", "paragraph": "[{\"end\":2172,\"start\":1254},{\"end\":4299,\"start\":2174},{\"end\":5526,\"start\":4301},{\"end\":6065,\"start\":5528},{\"end\":6971,\"start\":6104},{\"end\":9099,\"start\":6973},{\"end\":10311,\"start\":9143},{\"end\":10771,\"start\":10313},{\"end\":11213,\"start\":10773},{\"end\":11505,\"start\":11215},{\"end\":12000,\"start\":11507},{\"end\":12195,\"start\":12002},{\"end\":13181,\"start\":12197},{\"end\":14016,\"start\":13198},{\"end\":14500,\"start\":14107},{\"end\":15307,\"start\":14502},{\"end\":15659,\"start\":15424},{\"end\":16114,\"start\":15698},{\"end\":18003,\"start\":16116},{\"end\":18588,\"start\":18029},{\"end\":18831,\"start\":18640},{\"end\":18978,\"start\":18833},{\"end\":20805,\"start\":18980},{\"end\":24722,\"start\":20916},{\"end\":27267,\"start\":24752},{\"end\":27957,\"start\":27286},{\"end\":28658,\"start\":27969},{\"end\":29344,\"start\":28660},{\"end\":32159,\"start\":29346},{\"end\":33972,\"start\":32182},{\"end\":34699,\"start\":34002},{\"end\":34954,\"start\":34718}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14106,\"start\":14017},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15423,\"start\":15308},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18639,\"start\":18589}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17933,\"start\":17926},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28253,\"start\":28246},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28657,\"start\":28650},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29343,\"start\":29336},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30145,\"start\":30138},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30253,\"start\":30246},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30867,\"start\":30860},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32120,\"start\":32113},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":33451,\"start\":33444}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1252,\"start\":1240},{\"attributes\":{\"n\":\"2.\"},\"end\":6102,\"start\":6068},{\"attributes\":{\"n\":\"2.1.\"},\"end\":9141,\"start\":9102},{\"attributes\":{\"n\":\"2.2.\"},\"end\":13196,\"start\":13184},{\"attributes\":{\"n\":\"2.3.\"},\"end\":15696,\"start\":15662},{\"attributes\":{\"n\":\"2.4.\"},\"end\":18027,\"start\":18006},{\"end\":20889,\"start\":20808},{\"attributes\":{\"n\":\"3.\"},\"end\":20914,\"start\":20892},{\"attributes\":{\"n\":\"3.1.\"},\"end\":24750,\"start\":24725},{\"attributes\":{\"n\":\"3.2.\"},\"end\":27284,\"start\":27270},{\"attributes\":{\"n\":\"4.\"},\"end\":27967,\"start\":27960},{\"end\":32180,\"start\":32162},{\"attributes\":{\"n\":\"5.\"},\"end\":34000,\"start\":33975},{\"attributes\":{\"n\":\"6.\"},\"end\":34716,\"start\":34702},{\"end\":34966,\"start\":34956},{\"end\":35041,\"start\":35031},{\"end\":35375,\"start\":35368},{\"end\":35559,\"start\":35549},{\"end\":36359,\"start\":36349},{\"end\":36520,\"start\":36511},{\"end\":36832,\"start\":36823},{\"end\":36972,\"start\":36963},{\"end\":37998,\"start\":37989}]", "table": "[{\"end\":36821,\"start\":36649},{\"end\":36961,\"start\":36834},{\"end\":37747,\"start\":36974},{\"end\":37987,\"start\":37852},{\"end\":38104,\"start\":38041}]", "figure_caption": "[{\"end\":35029,\"start\":34968},{\"end\":35366,\"start\":35043},{\"end\":35547,\"start\":35377},{\"end\":36238,\"start\":35561},{\"end\":36347,\"start\":36241},{\"end\":36509,\"start\":36361},{\"end\":36649,\"start\":36522},{\"end\":37852,\"start\":37750},{\"end\":38041,\"start\":38000}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5120,\"start\":5112},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":8196,\"start\":8188},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10416,\"start\":10407},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10924,\"start\":10915},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11664,\"start\":11655},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12395,\"start\":12386},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12801,\"start\":12792},{\"end\":20406,\"start\":20398},{\"end\":28048,\"start\":28040},{\"end\":28645,\"start\":28637},{\"end\":29331,\"start\":29323},{\"end\":30158,\"start\":30150},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33464,\"start\":33456}]", "bib_author_first_name": "[{\"end\":38162,\"start\":38161},{\"end\":38174,\"start\":38173},{\"end\":38183,\"start\":38182},{\"end\":38194,\"start\":38193},{\"end\":38427,\"start\":38426},{\"end\":38440,\"start\":38439},{\"end\":38447,\"start\":38446},{\"end\":38707,\"start\":38706},{\"end\":38719,\"start\":38718},{\"end\":38731,\"start\":38730},{\"end\":38944,\"start\":38943},{\"end\":39094,\"start\":39093},{\"end\":39104,\"start\":39103},{\"end\":39110,\"start\":39109},{\"end\":39323,\"start\":39322},{\"end\":39333,\"start\":39332},{\"end\":39335,\"start\":39334},{\"end\":39544,\"start\":39543},{\"end\":39546,\"start\":39545},{\"end\":39555,\"start\":39554},{\"end\":39557,\"start\":39556},{\"end\":39782,\"start\":39781},{\"end\":39784,\"start\":39783},{\"end\":39800,\"start\":39799},{\"end\":39802,\"start\":39801},{\"end\":39943,\"start\":39942},{\"end\":39952,\"start\":39951},{\"end\":40293,\"start\":40292},{\"end\":40302,\"start\":40301},{\"end\":40314,\"start\":40313},{\"end\":40542,\"start\":40541},{\"end\":40551,\"start\":40550},{\"end\":40563,\"start\":40562},{\"end\":40575,\"start\":40574},{\"end\":40742,\"start\":40741},{\"end\":40753,\"start\":40752},{\"end\":40755,\"start\":40754},{\"end\":40990,\"start\":40989},{\"end\":40999,\"start\":40998},{\"end\":41001,\"start\":41000},{\"end\":41010,\"start\":41009},{\"end\":41174,\"start\":41173},{\"end\":41183,\"start\":41182},{\"end\":41185,\"start\":41184},{\"end\":41194,\"start\":41193},{\"end\":41196,\"start\":41195},{\"end\":41205,\"start\":41204},{\"end\":41397,\"start\":41396},{\"end\":41404,\"start\":41403},{\"end\":41414,\"start\":41413},{\"end\":41628,\"start\":41627},{\"end\":41630,\"start\":41629},{\"end\":41639,\"start\":41638},{\"end\":41641,\"start\":41640},{\"end\":41922,\"start\":41918},{\"end\":41934,\"start\":41930},{\"end\":42086,\"start\":42085},{\"end\":42317,\"start\":42316},{\"end\":42319,\"start\":42318},{\"end\":42330,\"start\":42329},{\"end\":42332,\"start\":42331},{\"end\":42344,\"start\":42343},{\"end\":42353,\"start\":42352},{\"end\":42611,\"start\":42610},{\"end\":42622,\"start\":42621},{\"end\":42624,\"start\":42623},{\"end\":42634,\"start\":42633},{\"end\":42636,\"start\":42635},{\"end\":42648,\"start\":42647},{\"end\":42650,\"start\":42649},{\"end\":42839,\"start\":42835},{\"end\":42846,\"start\":42845},{\"end\":42853,\"start\":42852},{\"end\":42866,\"start\":42865},{\"end\":42875,\"start\":42874},{\"end\":43079,\"start\":43078},{\"end\":43081,\"start\":43080},{\"end\":43088,\"start\":43087},{\"end\":43090,\"start\":43089},{\"end\":43101,\"start\":43100},{\"end\":43314,\"start\":43313},{\"end\":43321,\"start\":43320},{\"end\":43329,\"start\":43328},{\"end\":43520,\"start\":43519},{\"end\":43528,\"start\":43527},{\"end\":43541,\"start\":43540},{\"end\":43705,\"start\":43704},{\"end\":43713,\"start\":43712},{\"end\":44113,\"start\":44112},{\"end\":44115,\"start\":44114},{\"end\":44125,\"start\":44124},{\"end\":44127,\"start\":44126},{\"end\":44138,\"start\":44137},{\"end\":44390,\"start\":44389},{\"end\":44402,\"start\":44401},{\"end\":44410,\"start\":44409},{\"end\":44412,\"start\":44411},{\"end\":44422,\"start\":44421},{\"end\":44653,\"start\":44652},{\"end\":44825,\"start\":44824},{\"end\":44832,\"start\":44831},{\"end\":45023,\"start\":45022},{\"end\":45025,\"start\":45024},{\"end\":45037,\"start\":45036},{\"end\":45282,\"start\":45281},{\"end\":45294,\"start\":45293},{\"end\":45306,\"start\":45305},{\"end\":45569,\"start\":45568},{\"end\":45577,\"start\":45576},{\"end\":45585,\"start\":45584},{\"end\":45593,\"start\":45592},{\"end\":45600,\"start\":45599},{\"end\":45863,\"start\":45862},{\"end\":45876,\"start\":45875},{\"end\":45885,\"start\":45884},{\"end\":45894,\"start\":45893},{\"end\":46112,\"start\":46111},{\"end\":46124,\"start\":46123},{\"end\":46285,\"start\":46284},{\"end\":46294,\"start\":46293},{\"end\":46296,\"start\":46295},{\"end\":46448,\"start\":46447},{\"end\":46624,\"start\":46623},{\"end\":46626,\"start\":46625},{\"end\":46639,\"start\":46638},{\"end\":46641,\"start\":46640},{\"end\":46870,\"start\":46869},{\"end\":46872,\"start\":46871},{\"end\":47054,\"start\":47053},{\"end\":47056,\"start\":47055},{\"end\":47066,\"start\":47065},{\"end\":47068,\"start\":47067}]", "bib_author_last_name": "[{\"end\":38171,\"start\":38163},{\"end\":38180,\"start\":38175},{\"end\":38191,\"start\":38184},{\"end\":38200,\"start\":38195},{\"end\":38437,\"start\":38428},{\"end\":38444,\"start\":38441},{\"end\":38457,\"start\":38448},{\"end\":38716,\"start\":38708},{\"end\":38728,\"start\":38720},{\"end\":38739,\"start\":38732},{\"end\":38950,\"start\":38945},{\"end\":39101,\"start\":39095},{\"end\":39107,\"start\":39105},{\"end\":39119,\"start\":39111},{\"end\":39330,\"start\":39324},{\"end\":39343,\"start\":39336},{\"end\":39552,\"start\":39547},{\"end\":39566,\"start\":39558},{\"end\":39797,\"start\":39785},{\"end\":39815,\"start\":39803},{\"end\":39949,\"start\":39944},{\"end\":39962,\"start\":39953},{\"end\":40299,\"start\":40294},{\"end\":40311,\"start\":40303},{\"end\":40320,\"start\":40315},{\"end\":40548,\"start\":40543},{\"end\":40560,\"start\":40552},{\"end\":40572,\"start\":40564},{\"end\":40581,\"start\":40576},{\"end\":40750,\"start\":40743},{\"end\":40763,\"start\":40756},{\"end\":40996,\"start\":40991},{\"end\":41007,\"start\":41002},{\"end\":41017,\"start\":41011},{\"end\":41180,\"start\":41175},{\"end\":41191,\"start\":41186},{\"end\":41202,\"start\":41197},{\"end\":41212,\"start\":41206},{\"end\":41401,\"start\":41398},{\"end\":41411,\"start\":41405},{\"end\":41419,\"start\":41415},{\"end\":41636,\"start\":41631},{\"end\":41648,\"start\":41642},{\"end\":41928,\"start\":41923},{\"end\":41938,\"start\":41935},{\"end\":42094,\"start\":42087},{\"end\":42327,\"start\":42320},{\"end\":42341,\"start\":42333},{\"end\":42350,\"start\":42345},{\"end\":42366,\"start\":42354},{\"end\":42619,\"start\":42612},{\"end\":42631,\"start\":42625},{\"end\":42645,\"start\":42637},{\"end\":42654,\"start\":42651},{\"end\":42843,\"start\":42840},{\"end\":42850,\"start\":42847},{\"end\":42863,\"start\":42854},{\"end\":42872,\"start\":42867},{\"end\":42878,\"start\":42876},{\"end\":43085,\"start\":43082},{\"end\":43098,\"start\":43091},{\"end\":43108,\"start\":43102},{\"end\":43318,\"start\":43315},{\"end\":43326,\"start\":43322},{\"end\":43338,\"start\":43330},{\"end\":43525,\"start\":43521},{\"end\":43538,\"start\":43529},{\"end\":43549,\"start\":43542},{\"end\":43710,\"start\":43706},{\"end\":43722,\"start\":43714},{\"end\":44122,\"start\":44116},{\"end\":44135,\"start\":44128},{\"end\":44144,\"start\":44139},{\"end\":44399,\"start\":44391},{\"end\":44407,\"start\":44403},{\"end\":44419,\"start\":44413},{\"end\":44429,\"start\":44423},{\"end\":44657,\"start\":44654},{\"end\":44829,\"start\":44826},{\"end\":44835,\"start\":44833},{\"end\":45034,\"start\":45026},{\"end\":45044,\"start\":45038},{\"end\":45291,\"start\":45283},{\"end\":45303,\"start\":45295},{\"end\":45312,\"start\":45307},{\"end\":45574,\"start\":45570},{\"end\":45582,\"start\":45578},{\"end\":45590,\"start\":45586},{\"end\":45597,\"start\":45594},{\"end\":45606,\"start\":45601},{\"end\":45873,\"start\":45864},{\"end\":45882,\"start\":45877},{\"end\":45891,\"start\":45886},{\"end\":45901,\"start\":45895},{\"end\":46121,\"start\":46113},{\"end\":46134,\"start\":46125},{\"end\":46291,\"start\":46286},{\"end\":46303,\"start\":46297},{\"end\":46451,\"start\":46449},{\"end\":46636,\"start\":46627},{\"end\":46649,\"start\":46642},{\"end\":46879,\"start\":46873},{\"end\":47063,\"start\":47057},{\"end\":47075,\"start\":47069}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":38345,\"start\":38106},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1718856},\"end\":38621,\"start\":38347},{\"attributes\":{\"doi\":\"ACCV. 2013. 3\",\"id\":\"b2\"},\"end\":38897,\"start\":38623},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13284142},\"end\":39039,\"start\":38899},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6382669},\"end\":39274,\"start\":39041},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13874285},\"end\":39458,\"start\":39276},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14561136},\"end\":39738,\"start\":39460},{\"attributes\":{\"id\":\"b7\"},\"end\":39940,\"start\":39740},{\"attributes\":{\"doi\":\"arXiv:1406.6558\",\"id\":\"b8\"},\"end\":40214,\"start\":39942},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12061055},\"end\":40459,\"start\":40216},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13259596},\"end\":40737,\"start\":40461},{\"attributes\":{\"doi\":\"arXiv:1412.4181\",\"id\":\"b11\"},\"end\":40955,\"start\":40739},{\"attributes\":{\"id\":\"b12\"},\"end\":41118,\"start\":40957},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1750601},\"end\":41342,\"start\":41120},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8196149},\"end\":41529,\"start\":41344},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":17055992},\"end\":41862,\"start\":41531},{\"attributes\":{\"id\":\"b16\"},\"end\":42039,\"start\":41864},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":38666415},\"end\":42229,\"start\":42041},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7024747},\"end\":42545,\"start\":42231},{\"attributes\":{\"id\":\"b19\"},\"end\":42810,\"start\":42547},{\"attributes\":{\"id\":\"b20\"},\"end\":42992,\"start\":42812},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2792395},\"end\":43263,\"start\":42994},{\"attributes\":{\"id\":\"b22\"},\"end\":43461,\"start\":43265},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1629541},\"end\":43676,\"start\":43463},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2150419},\"end\":44017,\"start\":43678},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8165754},\"end\":44322,\"start\":44019},{\"attributes\":{\"id\":\"b26\"},\"end\":44591,\"start\":44324},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1930439},\"end\":44752,\"start\":44593},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14828052},\"end\":44968,\"start\":44754},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10210242},\"end\":45202,\"start\":44970},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6788752},\"end\":45455,\"start\":45204},{\"attributes\":{\"id\":\"b31\"},\"end\":45800,\"start\":45457},{\"attributes\":{\"doi\":\"ECCV. 2012. 8\",\"id\":\"b32\"},\"end\":46041,\"start\":45802},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14124313},\"end\":46263,\"start\":46043},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1065794},\"end\":46384,\"start\":46265},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52805831},\"end\":46543,\"start\":46386},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":17256399},\"end\":46802,\"start\":46545},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":11755124},\"end\":47014,\"start\":46804},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14815630},\"end\":47176,\"start\":47016}]", "bib_title": "[{\"end\":38424,\"start\":38347},{\"end\":38941,\"start\":38899},{\"end\":39091,\"start\":39041},{\"end\":39320,\"start\":39276},{\"end\":39541,\"start\":39460},{\"end\":39779,\"start\":39740},{\"end\":40290,\"start\":40216},{\"end\":40539,\"start\":40461},{\"end\":41171,\"start\":41120},{\"end\":41394,\"start\":41344},{\"end\":41625,\"start\":41531},{\"end\":42083,\"start\":42041},{\"end\":42314,\"start\":42231},{\"end\":42833,\"start\":42812},{\"end\":43076,\"start\":42994},{\"end\":43517,\"start\":43463},{\"end\":43702,\"start\":43678},{\"end\":44110,\"start\":44019},{\"end\":44387,\"start\":44324},{\"end\":44650,\"start\":44593},{\"end\":44822,\"start\":44754},{\"end\":45020,\"start\":44970},{\"end\":45279,\"start\":45204},{\"end\":45566,\"start\":45457},{\"end\":46109,\"start\":46043},{\"end\":46282,\"start\":46265},{\"end\":46445,\"start\":46386},{\"end\":46621,\"start\":46545},{\"end\":46867,\"start\":46804},{\"end\":47051,\"start\":47016}]", "bib_author": "[{\"end\":38173,\"start\":38161},{\"end\":38182,\"start\":38173},{\"end\":38193,\"start\":38182},{\"end\":38202,\"start\":38193},{\"end\":38439,\"start\":38426},{\"end\":38446,\"start\":38439},{\"end\":38459,\"start\":38446},{\"end\":38718,\"start\":38706},{\"end\":38730,\"start\":38718},{\"end\":38741,\"start\":38730},{\"end\":38952,\"start\":38943},{\"end\":39103,\"start\":39093},{\"end\":39109,\"start\":39103},{\"end\":39121,\"start\":39109},{\"end\":39332,\"start\":39322},{\"end\":39345,\"start\":39332},{\"end\":39554,\"start\":39543},{\"end\":39568,\"start\":39554},{\"end\":39799,\"start\":39781},{\"end\":39817,\"start\":39799},{\"end\":39951,\"start\":39942},{\"end\":39964,\"start\":39951},{\"end\":40301,\"start\":40292},{\"end\":40313,\"start\":40301},{\"end\":40322,\"start\":40313},{\"end\":40550,\"start\":40541},{\"end\":40562,\"start\":40550},{\"end\":40574,\"start\":40562},{\"end\":40583,\"start\":40574},{\"end\":40752,\"start\":40741},{\"end\":40765,\"start\":40752},{\"end\":40998,\"start\":40989},{\"end\":41009,\"start\":40998},{\"end\":41019,\"start\":41009},{\"end\":41182,\"start\":41173},{\"end\":41193,\"start\":41182},{\"end\":41204,\"start\":41193},{\"end\":41214,\"start\":41204},{\"end\":41403,\"start\":41396},{\"end\":41413,\"start\":41403},{\"end\":41421,\"start\":41413},{\"end\":41638,\"start\":41627},{\"end\":41650,\"start\":41638},{\"end\":41930,\"start\":41918},{\"end\":41940,\"start\":41930},{\"end\":42096,\"start\":42085},{\"end\":42329,\"start\":42316},{\"end\":42343,\"start\":42329},{\"end\":42352,\"start\":42343},{\"end\":42368,\"start\":42352},{\"end\":42621,\"start\":42610},{\"end\":42633,\"start\":42621},{\"end\":42647,\"start\":42633},{\"end\":42656,\"start\":42647},{\"end\":42845,\"start\":42835},{\"end\":42852,\"start\":42845},{\"end\":42865,\"start\":42852},{\"end\":42874,\"start\":42865},{\"end\":42880,\"start\":42874},{\"end\":43087,\"start\":43078},{\"end\":43100,\"start\":43087},{\"end\":43110,\"start\":43100},{\"end\":43320,\"start\":43313},{\"end\":43328,\"start\":43320},{\"end\":43340,\"start\":43328},{\"end\":43527,\"start\":43519},{\"end\":43540,\"start\":43527},{\"end\":43551,\"start\":43540},{\"end\":43712,\"start\":43704},{\"end\":43724,\"start\":43712},{\"end\":44124,\"start\":44112},{\"end\":44137,\"start\":44124},{\"end\":44146,\"start\":44137},{\"end\":44401,\"start\":44389},{\"end\":44409,\"start\":44401},{\"end\":44421,\"start\":44409},{\"end\":44431,\"start\":44421},{\"end\":44659,\"start\":44652},{\"end\":44831,\"start\":44824},{\"end\":44837,\"start\":44831},{\"end\":45036,\"start\":45022},{\"end\":45046,\"start\":45036},{\"end\":45293,\"start\":45281},{\"end\":45305,\"start\":45293},{\"end\":45314,\"start\":45305},{\"end\":45576,\"start\":45568},{\"end\":45584,\"start\":45576},{\"end\":45592,\"start\":45584},{\"end\":45599,\"start\":45592},{\"end\":45608,\"start\":45599},{\"end\":45875,\"start\":45862},{\"end\":45884,\"start\":45875},{\"end\":45893,\"start\":45884},{\"end\":45903,\"start\":45893},{\"end\":46123,\"start\":46111},{\"end\":46136,\"start\":46123},{\"end\":46293,\"start\":46284},{\"end\":46305,\"start\":46293},{\"end\":46453,\"start\":46447},{\"end\":46638,\"start\":46623},{\"end\":46651,\"start\":46638},{\"end\":46881,\"start\":46869},{\"end\":47065,\"start\":47053},{\"end\":47077,\"start\":47065}]", "bib_venue": "[{\"end\":38159,\"start\":38106},{\"end\":38463,\"start\":38459},{\"end\":38704,\"start\":38623},{\"end\":38956,\"start\":38952},{\"end\":39125,\"start\":39121},{\"end\":39349,\"start\":39345},{\"end\":39585,\"start\":39568},{\"end\":39821,\"start\":39817},{\"end\":40049,\"start\":39979},{\"end\":40326,\"start\":40322},{\"end\":40587,\"start\":40583},{\"end\":40987,\"start\":40957},{\"end\":41218,\"start\":41214},{\"end\":41425,\"start\":41421},{\"end\":41675,\"start\":41650},{\"end\":41916,\"start\":41864},{\"end\":42122,\"start\":42096},{\"end\":42375,\"start\":42368},{\"end\":42608,\"start\":42547},{\"end\":42887,\"start\":42880},{\"end\":43114,\"start\":43110},{\"end\":43311,\"start\":43265},{\"end\":43555,\"start\":43551},{\"end\":43797,\"start\":43724},{\"end\":44150,\"start\":44146},{\"end\":44445,\"start\":44431},{\"end\":44663,\"start\":44659},{\"end\":44841,\"start\":44837},{\"end\":45069,\"start\":45046},{\"end\":45318,\"start\":45314},{\"end\":45612,\"start\":45608},{\"end\":45860,\"start\":45802},{\"end\":46140,\"start\":46136},{\"end\":46309,\"start\":46305},{\"end\":46457,\"start\":46453},{\"end\":46657,\"start\":46651},{\"end\":46887,\"start\":46881},{\"end\":47081,\"start\":47077},{\"end\":43857,\"start\":43799}]"}}}, "year": 2023, "month": 12, "day": 17}