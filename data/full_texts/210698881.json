{"id": 210698881, "updated": "2023-11-07 19:49:11.473", "metadata": {"title": "Graph-Bert: Only Attention is Needed for Learning Graph Representations", "authors": "[{\"first\":\"Jiawei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Haopeng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Congying\",\"last\":\"Xia\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 1, "day": 15}, "abstract": "The dominant graph neural networks (GNNs) over-rely on the graph links, several serious performance problems with which have been witnessed already, e.g., suspended animation problem and over-smoothing problem. What's more, the inherently inter-connected nature precludes parallelization within the graph, which becomes critical for large-sized graph, as memory constraints limit batching across the nodes. In this paper, we will introduce a new graph neural network, namely GRAPH-BERT (Graph based BERT), solely based on the attention mechanism without any graph convolution or aggregation operators. Instead of feeding GRAPH-BERT with the complete large input graph, we propose to train GRAPH-BERT with sampled linkless subgraphs within their local contexts. GRAPH-BERT can be learned effectively in a standalone mode. Meanwhile, a pre-trained GRAPH-BERT can also be transferred to other application tasks directly or with necessary fine-tuning if any supervised label information or certain application oriented objective is available. We have tested the effectiveness of GRAPH-BERT on several graph benchmark datasets. Based the pre-trained GRAPH-BERT with the node attribute reconstruction and structure recovery tasks, we further fine-tune GRAPH-BERT on node classification and graph clustering tasks specifically. The experimental results have demonstrated that GRAPH-BERT can out-perform the existing GNNs in both the learning effectiveness and efficiency.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2001.05140", "mag": "3000577518", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2001-05140", "doi": null}}, "content": {"source": {"pdf_hash": "78542c2be9bb853a4e04642f2d315cfb0c6d94b3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2001.05140v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0f4ef33d5d7ac996b05a1103bab3a3454991390b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/78542c2be9bb853a4e04642f2d315cfb0c6d94b3.txt", "contents": "\nGRAPH-BERT: Only Attention is Needed for Learning Graph Representations\n\n\nJiawei Zhang \nHaopeng Zhang haopeng@ifmlab.org \n\u22c6 \nCongying Xia \nUniversity of Illinois at Chicago\nILUSA\n\nLi Sun l.sun@bupt.edu.cn \nBeijing University of Posts and Telecommunications\nBeijingChina\n\n\n\u22c6 IFM Lab\nFlorida State University\nTallahasseeFLUSA\n\nGRAPH-BERT: Only Attention is Needed for Learning Graph Representations\n\nThe dominant graph neural networks (GNNs) overrely on the graph links, several serious performance problems with which have been witnessed already, e.g., suspended animation problem and over-smoothing problem. What's more, the inherently inter-connected nature precludes parallelization within the graph, which becomes critical for large-sized graph, as memory constraints limit batching across the nodes. In this paper, we will introduce a new graph neural network, namely GRAPH-BERT (Graph based BERT), solely based on the attention mechanism without any graph convolution or aggregation operators. Instead of feeding GRAPH-BERT with the complete large input graph, we propose to train GRAPH-BERT with sampled linkless subgraphs within their local contexts. GRAPH-BERT can be learned effectively in a standalone mode. Meanwhile, a pre-trained GRAPH-BERT can also be transferred to other application tasks directly or with necessary fine-tuning if any supervised label information or certain application oriented objective is available. We have tested the effectiveness of GRAPH-BERT on several graph benchmark datasets. Based the pretrained GRAPH-BERT with the node attribute reconstruction and structure recovery tasks, we further fine-tune GRAPH-BERT on node classification and graph clustering tasks specifically. The experimental results have demonstrated that GRAPH-BERT can out-perform the existing GNNs in both the learning effectiveness and efficiency.\n\nIntroduction\n\nGraph provides a unified representation for many interconnected data in the real-world, which can model both the diverse attribute information of the node entities and the extensive connections among these nodes. For instance, the human brain imaging data, online social media and bio-medical molecules can all be represented as graphs, i.e., the brain graph , social graph [Ugander et al., 2011] and molecular graph [Jin et al., 2018], respectively. Traditional machine learning models can hardly be applied to the graph data directly, which usually take the feature vectors as the inputs. Viewed in such a perspective, learning the representations of the graph structured data is an important research task.\n\nIn recent years, great efforts have been devoted to designing new graph neural networks (GNNs) for effective graph representation learning. Besides the network embedding models, e.g., node2vec [Grover and Leskovec, 2016] and deepwalk [Perozzi et al., 2014a], the recent graph neural networks, e.g., GCN [Kipf and Welling, 2016], GAT [Veli\u010dkovi\u0107 et al., 2018] and LOOPYNET [Zhang, 2018], are also becoming much more important, which can further refine the learned representations for specific application tasks. Meanwhile, most of these existing graph representation learning models are still based on the graph structures, i.e., the links among the nodes. Via necessary neighborhood information aggregation or convolutional operators along the links, nodes' representations learned by such approaches can preserve the graph structure information.\n\nHowever, several serious learning performance problem, e.g., suspended animation problem  and over-smoothing problem [Li et al., 2018], with the existing GNN models have also been witnessed in recent years. According to , for the GNNs based on the approximated graph convolutional operators [Hammond et al., 2011], as the model architecture goes deeper and reaches certain limit, the model will not respond to the training data and suffers from the suspended animation problem. Meanwhile, the node representations obtained by such deep models tend to be over-smoothed and also become indistinguishable [Li et al., 2018]. Both of these two problems greatly hinder the applications of GNNs for deep graph representation learning tasks. What's more, the inherently interconnected nature precludes parallelization within the graph, which becomes critical for large-sized graph input, as memory constraints limit batching across the nodes.\n\nTo address the above problems, in this paper, we will propose a new graph neural network model, namely GRAPH-BERT (Graph based BERT). Inspired by , model GRAPH-BERT will be trained with sampled nodes together with their context (which are called linkless subgraphs in this paper) from the input large-sized graph data. Distinct from the existing GNN models, in the representation learning process, GRAPH-BERT utilizes no links in such sampled batches, which will be purely based on the attention mechanisms instead [Vaswani et al., 2017;Devlin et al., 2018]. Therefore, GRAPH-BERT can get rid of the aforementioned learning effectiveness and efficiency problems with existing GNN models promisingly.\n\nWhat's more, compared with computer vision [He et al., 2018] and natural language processing [Devlin et al., 2018], graph neural network pre-training and fine-tuning are still not common practice by this context so far. The main obstacles that prevent such operations can be due to the diverse input graph structures and the extensive connections among the nodes. Also the different learning task objectives also prevents the transfer of GNNs across different tasks. Since GRAPH-BERT doesn't really rely on the graph links at all, in this paper, we will investigate the transfer of pre-trained GRAPH-BERT on new learning tasks and other sequential models (with necessary fine-tuning), which will also help construct the functional pipeline of models in graph learning.\n\nWe summarize our contributions of this paper as follows:\n\n\u2022 New GNN Model: In this paper, we introduce a new GNN model GRAPH-BERT for graph data representation learning. GRAPH-BERT doesn't rely on the graph links for representation learning and can effectively address the suspended animation problems aforementioned. Also GRAPH-BERT is trainable with sampled linkless subgraphs (i.e., target node with context), which is more efficient than existing GNNs constructed for the complete input graph. To be more precise, the training cost of GRAPH-BERT is only decided by (1) training instance number, and (2) sampled subgraph size, which is uncorrelated with the input graph size at all.\n\n\u2022 Unsupervised Pre-Training: Given the input unlabeled graph, we will pre-train GRAPH-BERT based on to two common tasks in graph studies, i.e., node attribute reconstruction and graph structure recovery. Node attribute recovery ensures the learned node representations can capture the input attribute information; whereas graph structure recovery can further ensure GRAPH-BERT learned with linkless subgraphs can still maintain both the graph local and global structure properties.\n\n\u2022 Fine-Tuning and Transfer: Depending on the specific application task objectives, the GRAPH-BERT model can be further fine-tuned to adapt the learned representations to specific application requirements, e.g., node classification and graph clustering. Meanwhile, the pretrained GRAPH-BERT can also be transferred and applied to other sequential models, which allows the construction of functional pipelines for graph learning.\n\nThe remaining parts of this paper are organized as follows. We will introduce the related work in Section 2. Detailed information about the GRAPH-BERT model will be introduced in Section 3, whereas the pre-training and finetuning of GRAPH-BERT will be introduced in Section 4 in detail. The effectiveness of GRAPH-BERT will be tested in Section 5. Finally, we will conclude this paper in Section 6.\n\n\nRelated Work\n\nTo make this paper self-contained, we will introduce some related topics here on GNNs, TRANSFORMER and BERT.\n\n\nGraph Neural Network:\n\nRepresentative examples of GNNs proposed by present include GCN [Kipf and Welling, 2016], GraphSAGE [Hamilton et al., 2017] and LOOPYNET [Zhang, 2018], based on which various extended models [Veli\u010dkovi\u0107 et al., 2018;Sun et al., 2019;Klicpera et al., 2018] have been introduced as well. As mentioned above, GCN and its variant models are all based on the approximated graph convolutional operator [Hammond et al., 2011], which may lead to the suspended animation problem  and over-smoothing problem [Li et al., 2018] for deep model architectures. Theoretic analyses of the reasons are provided in [Li et al., 2018;G\u00fcrel et al., 2019]. To handle such problems,  generalizes the graph raw residual terms in [Zhang, 2018] and proposes a method based on graph residual learning; [Li et al., 2018] proposes to adopt residual/dense connections and dilated convolutions into the GCN architecture. Several other works [Sun et al., 2019;Huang and Carley, 2019] seek to involve the recurrent network for deep graph representation learning instead.\n\nBERT and TRANSFORMER: In NLP, the dominant sequence transduction models are based on complex recurrent [Hochreiter and Schmidhuber, 1997;Chung et al., 2014] or convolutional neural networks [Kim, 2014]. However, the inherently sequential nature precludes parallelization within training examples. Therefore, in [Vaswani et al., 2017], the authors propose a new network architecture, the TRANSFORMER, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. With TRANSFORMER, [Devlin et al., 2018] further introduces BERT for deep language understanding, which obtains new state-of-the-art results on eleven natural language processing tasks. In recent years, TRANS-FORMER and BERT based learning approaches have been used extensively in various learning tasks [Dai et al., 2019;Lan et al., 2019;Shang et al., 2019].\n\nReaders may also refer to page 1 and page 2 for more information on the state-of-the-art work on these topics.\n\n\nMethod\n\nIn this section, we will introduce the detailed information about the GRAPH-BERT model. As illustrated in Figure 1, GRAPH-BERT involves several parts: (1) linkless subgraph batching, (2) node input embedding, (3) graph-transformer based encoder, (4) representation fusion, and (5) the functional component. The results learned by the the graphtransformer model will be fused as the representation for the target nodes. In this section, we will introduce these key parts in great detail, whereas the pre-training and fine-tuning of GRAPH-BERT will be introduced in the following section.  l k 5 E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" h P + 6 L r U f 2 d 3 t Z a l d q a Q Q v E K M X y w = \" > A A A B 2 X i c b Z D N S g M x F I X v 1 L 8 6 V q 1 r N 8 E i u C o z b n Q p u H F Z w b Z C O 5 R M 5 k 4 b m s k M y R 2 h D H 0 B F 2 5 E f C 9 3 v o 3 p z 0 J b D w Q + z k n I v S c u l L Q U B N 9 e b W d 3 b / + g f u g f N f z j k 9 N m o 2 f z 0 g j s i l z l 5 j n m F p X U 2 C V J C p 8 L g z y L F f b j 6 f 0 i 7 7 + g s T L X T z Q r M M r 4 W M t U C k 7 O 6 o y a r a A d L M W 2 I V x D C 9 Y a N b + G S S 7 K D D U J x a 0 d h E F B U c U N S a F w 7 g 9 L i w U X U z 7 G g U P N M 7 R R t R x z z i 6 d k 7 A 0 N + 5 o Y k v 3 9 4 u K Z 9 b O s t j d z D h N 7 G a 2 M P / L B i W l t 1 E l d V E S a r H 6 K C 0 V o 5 w t d m a J N C h I z R x w Y a S b l Y k J N 1 y Q a 8 Z 3 H Y S b G 2 9 D 7 7 o d B u 3 w M Y A 6 n M M F X E E I N 3 A H D 9 C B L g h I 4 B X e v Y n 3 5 n 2 s u q p 5 6 9 L O 4 I + 8 z x 8 4 x I o 4 < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" m p k T r c G V i G 1 T / f Z O 9 E g c x E T M k S 8 = \" > A A A B 8 H i c b V A 9 T 8 M w F H z h s 5 Q C o S u L R Y t U l s p h g R G J p W O R 6 I f U h s p x n d a q 4 0 S 2 g 6 i i / B U W B h D i 1 7 D x b 3 D a D t B y k q X T 3 X t 6 5 w s S w b X B + N v Z 2 t 7 Z 3 d s v H Z Q P K 0 f H J + 5 p p a v j V F H W o b G I V T 8 g m g k u W c d w I 1 g / U Y x E g W C 9 Y H Z X + L 0 n p j S P 5 Y O Z J 8 y P y E T y k F N i r D R y q / V h R M w 0 C L N W / p g 1 8 G V e H 7 k 1 3 M Q L o E 3 i r U g N V m i P 3 K / h O K Z p x K S h g m g 9 8 H B i / I w o w 6 l g e X m Y a p Y Q O i M T N r B U k o h p P 1 t k z 9 G F V c Y o j J V 9 0 q C F + n s j I 5 H W 8 y i w k 0 V Q v e 4 V 4 n / e I D X h j Z 9 x m a S G S b o 8 F K Y C m R g V R a A x V 4 w a M b e E U M V t V k S n R B F q b F 1 l W 4 K 3 / u V N 0 r 1 q e r j p 3 W M o w R m c Q w M 8 u I Z b a E E b O k D h G V 7 g D d 6 d 3 H l 1 P p Z 1 b T m r 3 q r w B 8 7 n D 1 d + k i Y = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" m p k T r c G\nH (0) < l a t e x i t s h a 1 _ b a s e 6 4 = \" v D 5 a a F O u a D m q Z 1 m w s T 2 D S / u U D B o = \" > A A A B + 3 i c b V C 7 T s M w F L 3 h W c o r l J H F o k U q S 5 V 0 g b E S S 9 m K R B 9 S W y r H d V q r j h P Z D q K K 8 i s s D C D E y o + w 8 T c 4 b Q Z o O Z K l o 3 P u 1 T 0 + X s S Z 0 o 7 z b W 1 s b m 3 v 7 B b 2 i v s H h 0 f H 9 k m p o 8 J Y E t o m I Q 9 l z 8 O K c i Z o W z P N a S + S F A c e p 1 1 v d p P 5 3 U c q F Q v F v Z 5 H d B j g i W A + I 1 g b a W S X K oV i G 1 T / f Z O 9 E g c x E T M k S 8 = \" > A A A B 8 H i c b V A 9 T 8 M w F H z h s 5 Q C o S u L R Y t U l s p h g R G J p W O R 6 I f U h s p x n d a q 4 0 S 2 g 6 i i / B U W B h D i 1 7 D x b 3 D a D t B y k q X T 3 X t 6 5 w s S w b X B + N v Z 2 t 7 Z 3 d s v H Z Q P K 0 f H J + 5 p p a v j V F H W o b G I V T 8 g m g k u W c d w I 1 g / U Y x E g W C 9 Y H Z X + L 0 n p j S P 5 Y O Z J 8 y P y E T y k F N i r D R y q / V h R M w 0 C L N W / p g 1 8 G V e H 7 k 1 3 M Q L o E 3 i r U g N V m i P 3 K / h O K Z p x K S h g m g 9 8 H B i / I w o w 6 l g e X m Y a p Y Q O i M T N r B U k o h p P 1 t k z 9 G F V c Y o j J V 9 0 q C F + n s j I 5 H W 8 y i w k 0 V Q v e 4 V 4 n / e I D X h j Z 9 x m a S G S b o 8 F K Y C m R g V R a A x V 4 w a M b e E U M V t V k S n R B F q b F 1 l W 4 K 3 / u V N 0 r 1 q e r j p 3 W M o w R m c Q w M 8 u I Z b a E E b O k D h G V 7 g D d 6 d 3 H l 1 P p Z 1 b T m r 3 q r w B 8 7 n D 1 d + k i Y = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" v N q k 7 B o T W l 9 r u b J + a v e S 1 G 3 i h 5 8 = \" > A A A B + 3 i c b V C 7 T s M w F L 0 p r 1 J e o Y w s E S 1 S W S q H B c Z K L G U r E n 1 I b a k c 1 2 m t O k 5 k O 4 g q y q + w M I A Q K z / C x t / g t B m g 5 U i W j s 6 5 V / f 4 e B F n S i P 0 b R U 2 N r e 2 d 4 q 7 p b 3 9 g 8 M j + 7 j c U W E s C W 2 T k I e y 5 2 F F O R O 0 r Z n m t B d J i g O P 0 6 4 3 u 8 n 8 7 i O V i o X i X s 8 j O g z w R D C f E a y N N L L L 1 U G A 9 d T z k 2 b 6 k N T Q R V o d 2 R V U R w s 4 6 8 T N S Q V y t E b 2 1 2 A c k j i g Q h O O l e q 7 K N L D B E v N C K d p a R A r G m E y w x P a N 1 T g g K p h s s i e O u d G G T t + K M 0 T 2 l m o v z c S H C g 1 D z w z m Q V V q 1 4 m / u f 1 Y + 1 f D x M m o l h T Q Z a H / J g 7 O n S y I p w x k 5 R o P j c E E 8 l M V o d M s c R E m 7 p K p g R 3 9 c v r p H N Z d 1 H d v U O V x m 1 e R x F O 4 Q x q 4 M I V N K A J L W g D g S d 4 h l d 4 s 1 L r x X q 3 P p a j B S v f O Y E / s D 5 / A L H F k 4 8 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" v D 5 a a F O u a D m q Z 1 m w s T 2 D S / u U D B o = \" > A A A B + 3 i c b V C 7 T s M w F L 3 h W c o r l J H F o k U q S 5 V 0 g b E S S 9 m K R B 9 S W y r H d V q r j h P Z D q K K 8 i s s D C D E y o + w 8 T c 4 b Q Z o O Z K l o 3 P u 1 T 0 + X s S Z 0 o 7 z b W 1 s b m 3 v 7 B b 2 i v s H h 0 f H 9 k m p o 8 J Y E t o m I Q 9 l z 8 O K c i Z o W z P N a S + S F A c e p 1 1 v d p P 5 3 U c q F Q v F v Z 5 H d B j g i W A + I 1 g b a W S X K o M A 6 6 n n J 8 3 0 I a k 6 l 2 l l Z J e d m r M A W i d u T s q Q o z W y v w b j k M Q B F Z p w r F T f d S I 9 T L D U j H C a F g e x o h E m M z y h f U M F D q g a J o v s K b o w y h j 5 o T R P a L R Q f 2 8 k O F B q H n h m M g u q V r 1 M / M / r x 9 q / H i Z M R L G m g i w P + T F H O k R Z E W j M J C W a z w 3 B R D K T F Z E p l p h o U 1 f R l O C u f n m d d O o 1 1 6 m 5 d / V y 4 z a v o w B n c A 5 V c O E K G t C E F r S B w B M 8 w y u 8 W a n 1 Y r 1 b H 8 v R D S v f O Y U / s D 5 / A L J l k 5 E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" v D 5 a a F O u a D m q Z 1 m w s T 2 D S / u U D B o = \" > A A A B + 3 i c b V C 7 T s M w F L 3 h W c o r l J H F o k U q S 5 V 0 g b E S S 9 m K R B 9 S W y r H d V q r j h P Z D q K K 8 i s s D C D E y o + w 8 T c 4 b Q Z o O Z K l o 3 P u 1 T 0 + X s S Z 0 o 7 z b W 1 s b m 3 v 7 B b 2 i v s H h 0 f H 9 k m p o 8 J Y E t o m I Q 9 l z 8 O K c i Z o W z P N a S + S F A c e p 1 1 v d p P 5 3 U c q F Q v F v Z 5 H d B j g i W A + I 1 g b a W S X K o M A 6 6 n n J 8 3 0 I a k 6 l 2 l l Z J e d m r M A W i d u T s q Q o z W y v w b j k M Q B F Z p w r F T f d S I 9 T L D U j H C a F g e x o h E m M z y h f U M F D q g a J o v s K b o w y h j 5 o T R P a L R Q f 2 8 k O F B q H n h m M g u q V r 1 M / M / r x 9 q / H i Z M R L G m g i w P + T F H O k R Z E W j M J C W a z w 3 B R D K T F Z E p l p h o U 1 f R l O C u f n m d d O o 1 1 6 m 5 d / V y 4 z a v o w B n c A 5 V c O E K G t C E F r S B w B M 8 w y u 8 W a n 1 Y r 1 b H 8 v R D S v f O Y U / s D 5 / A L J l k 5 E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" v D 5 a a F O u a D m q Z 1 m w s T 2 D S / u U D B o = \" > A A A B + 3 i c b V C 7 T s M w F L 3 h W c o r l J H F o k U q S 5 V 0 g b E S S 9 m K R B 9 S W y r H d V q r j h P Z D q K K 8 i s s D C D E y o + w 8 T c 4 b Q Z o O Z K l o 3 P u 1 T 0 + X s S Z 0 o 7 z b W 1 s b m 3 v 7 B b 2 i v s H h 0 f H 9 k m p o 8 J Y E t o m I Q 9 l z 8 O K c i Z o W z P N a S + S F A c e p 1 1 v d p P 5 3 U c q F Q v F v Z 5 H d B j g i W A + I 1 g b a W S X K o M A 6 6 n n J 8 3 0 I a k 6 l 2 l l Z J e d m r M A W i d u T s q Q o z W y v w b j k M Q B F Z p w r F T f d S I 9 T L D U j H C a F g e x o h E m M z y h f U M F D q g a J o v s K b o w y h j 5 o T R P a L R Q f 2 8 k O F B q H n h m M g u q V r 1 M / M / r x 9 q / H i Z M R L G m g i w P + T F H O k R Z E W j M J C W a z w 3 B R D K T F Z E p l p h o U 1 f R l O C u f n m d d O o 1 1 6 m 5 d / V y 4 z a v o w B n c A 5 V c O E K G t C E F r S B w B M 8 w y u 8 W a n 1 Y r 1 b H 8 v R D S v f O Y U / s D 5 / A L J l k 5 E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" v D 5 a a F O u a D m q Z 1 m w s T 2 D S / u U D B o = \" > A A A B + 3 i c b V C 7 T s M w F L 3 h W c o r l J H F o k U q S 5 V 0 g b E S S 9 m K R B 9 S W y r H d V q r j h P Z D q K K 8 i s s D C D E y o + w 8 T c 4 b Q Z o O Z K l o 3 P u 1 T 0 + X s S Z 0 o 7 z b W 1 s b m 3 v 7 B b 2 i v s H h 0 f H 9 k m p o 8 J Y E t o m I Q 9 l z 8 O K c i Z o W z P N a S + S F A c e p 1 1 v d p P 5 3 U c q F Q v F v Z 5 H d B j g i W A + I 1 g b a W S X K o M A 6 6 n n J 8 3 0 I a k 6 l 2 l l Z J e d m r M A W i d u T s q Q o z W y v w b j k M Q B F Z p w r F T f d S I 9 T L D U j H C a F g e x o h E m M z y h f U M F D q g a J o v s K b o w y h j 5 o T R P a L R Q f 2 8 k O F B q H n h m M g u q V r 1 M / M / r x 9 q / H i Z M R L G m g i w P + T F H O k R Z E W j M J C W a z w 3 B R D K T F Z E p l p h o U 1 f R l O C u f n m d d O o 1 1 6 m 5 d / V y 4 z a v o w B n c A 5 V c O E K G t C E F r S B w B M 8 w y u 8 W a n 1 Y r 1 b H 8 v R D S v f O Y U / s D 5 / A L J l k 5 E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" v D 5 a a F O u a D m q Z 1 m w s T 2 D S / u U D B o = \" > A A A B + 3 i c b V C 7 T s M w F L 3 h W c o r l J H F o k U q S 5 V 0 g b E S S 9 m K R B 9 S W y r H d V q r j h P Z D q K K 8 i s s D C D E y o + w 8 T c 4 b Q Z o O Z K l o 3 P u 1 T 0 + X s S Z 0 o 7 z b W 1 s b m 3 v 7 B b 2 i v s H h 0 f H 9 k m p o 8 J Y E t o m I Q 9 l z 8 O K c i Z o W z P N a S + S F A c e p 1 1 v d p P 5 3 U c q F Q v F v Z 5 H d B j g i W A + I 1 g b a W S X K o M A 6 6 n n J 8 3 0 I a k 6 l 2 l l Z J e d m r M A W i d u T s q Q o z W y v w b j k M Q B F Z p w r F T f d S I 9 T L D U j H C a F g e x o h E m M z y h f U M F D q g a J o v s K b o w y h j 5 o T R P a L R Q f 2 8 k O F B q H n h m M g u q V r 1 M / M / r x 9 q / H i Z M R L G m g i w P + T F H O k R Z E W j M J C W a z w 3 B R D K T F Z E p l p h o U 1 f R l O C u f n m d d O o 1 1 6 m 5 d / V y 4 z a v o w B n c A 5 V c O E K G t C E F r S B w B M 8 w y u 8 W a n 1 Y r 1 b H 8 v R D S v f O Y U / s D 5 / A L J l k 5 E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" v D 5 a a F O u a D m q Z 1 m w s T 2 D S / u U D B o = \" > A A A B + 3 i c b V C 7 T s M w F L 3 h W c o r l J H F o k U q S 5 V 0 g b E S S 9 m K R B 9 S W y r H d V q r j h P Z D q K K 8 i s s D C D E y o + w 8 T c 4 b Q Z o O Z K l o 3 P u 1 T 0 + X s S Z 0 o 7 z b W 1 s b m 3 v 7 B b 2 i v s H h 0 f H 9 k m p o 8 J Y E t o m I Q 9 l z 8 O K c i Z o W z P N a S + S F A c e p 1 1 v d p P 5 3 U c q F Q v F v Z 5 H d B j g i W A + I 1 g b a W S X K o M A 6 6 n n J 8 3 0 I a k 6 l 2 l l Z J e d m r M A W i d u T s q Q o z W y v w b j k M Q B F Z p w r F T f d S I 9 T L D U j H C a F g e x o h E m M z y h f U M F D q g a J o v s K b o w y h j 5 o T R P a L R Q f 2 8 k O F B q H n h m M g u q V r 1 M / M / r x 9 q / H i Z M R L G m g i w P + T F H O k R Z E W j M J C W a z w 3 B R D K T F Z E p l p h o U 1 f R l O C u f n m d d O o 1 1 6 m 5 d / V y 4 z a v o w B n c A 5 V c O E K G t C E F r S B w B M 8 w y u 8 W a n 1 Y r 1 b H 8 v R D S v f O Y U / s D 5 / A L J l k 5 E = < / l a t e x i t > Part 5 z i < l a t e x i t s h a 1 _ b a s e 6 4 = \" 2 r L D q 3 3 E N a s T E g f b 1 4 u B H S C B J d A = \" > A A A B 9 X i c b V C 7 T s M w F L 0 p r 1 J e B U Y W i x a J q U q 6 w F j B w l g k + p D a U D m u 0 1 q 1 n c h 2 Q C X q f 7 A w g B A r / 8 L G 3 + C 0 G a D l S J a O z r l X 9 / g E M W f a u O 6 3 U 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 d t H S W K 0 B a J e K S 6 A d a U M 0 l b h h l O u 7 G i W A S c d o L J d e Z 3 H q j S L J J 3 Z h p T X + C R Z C E j 2 F j p v t o X 2 I y D M H 2 a D V h 1 U K 6 4 N X c O t E q 8 n F Q g R 3 N Q / u o P I 5 I I K g 3 h W O u e 5 8 b G T 7 E y j H A 6 K / U T T W N M J n h E e 5 Z K L K j 2 0 3 n q G T q z y h C F k b J P G j R X f 2 + k W G g 9 F Y G d z E L q Z S 8 T / / N 6 i Q k v / Z T J O D F U k s W h M O H I R C i r A A 2 Z o s T w q S W Y K G a z I j L G C h N j i y r Z E r z l L 6 + S d r 3 m u T X v t l 5 p X O V 1 F O E E T u E c P L i A B t x A E 1 p A Q M E z v M K b 8 + i 8 O O / O x 2 K 0 4 O Q 7 x / A H z u c P P 6 + S V A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 2 r L D q 3 3 E N a s T E g f b 1 4 u B H S C B J d A = \" > A A A B 9 X i c b V C 7 T s M w F L 0 p r 1 J e B U Y W i x a J q U q 6 w F j B w l g k + p D a U D m u 0 1 q 1 n c h 2 Q C X q f 7 A w g B A r / 8 L G 3 + C 0 G a D l S J a O z r l X 9 / g E M W f a u O 6 3 U 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 d t H S W K 0 B a J e K S 6 A d a U M 0 l b h h l O u 7 G i W A S c d o L J d e Z 3 H q j S L J J 3 Z h p T X + C R Z C E j 2 F j p v t o X 2 I y D M H 2 a D V h 1 U K 6 4 N X c O t E q 8 n F Q g R 3 N Q / u o P I 5 I I K g 3 h W O u e 5 8 b G T 7 E y j H A 6 K / U T T W N M J n h E e 5 Z K L K j 2 0 3 n q G T q z y h C F k b J P G j R X f 2 + k W G g 9 F Y G d z E L q Z S 8 T / / N 6 i Q k v / Z T J O D F U k s W h M O H I R C i r A A 2 Z o s T w q S W Y K G a z I j L G C h N j i y r Z E r z l L 6 + S d r 3 m u T X v t l 5 p X O V 1 F O E E T u E c P L i A B t x A E 1 p A Q M E z v M K b 8 + i 8 O O / O x 2 K 0 4 O Q 7\nx / A H z u c P P 6 + S V A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 2 r L D q 3 3 E N a s T E g f b 1 4 u B H S C B J d A = \" > A A A B 9 X i c b V C 7 T s M w F L 0 p r 1 J e B U Y W i x a J q U q 6 w F j B w l g k + p D a U D m u 0 1 q 1 n c h 2 Q C X q f 7 A w g B A r / 8 L G 3 + C 0 G a D l S J a O z r l X 9 / g E M W f a u O 6 3 U 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 d t H S W K 0 B a J e K S 6 A d a U M 0 l b h h l O u 7 G i W A S c d o L J d e Z 3 H q j S L J J 3 Z h p T X + C R Z C E j 2 F j p v t o X 2 I y D M H 2 a D V h 1 U K 6 4 N X c O t E q 8 n F Q g R 3 N Q / u o P I 5 I I K g 3 h W O u e 5 8 b G T 7 E y j H A 6 K / U T T W N M J n h E e 5 Z K L K j 2 0 3 n q G T q z y h C\nF k b J P G j R X f 2 + k W G g 9 F Y G d z E L q Z S 8 T / / N 6 i Q k v / Z T J O D F U k s W h M O H I R C i r A A 2 Z o s T w q S W Y K G a z I j L G C h N j i y r Z E r z l L 6 + S d r 3 m u T X v t l 5 p X O V 1 F O E E T u E c P L i A B t x A E 1 p A Q M E z v M K b 8 + i 8 O O / O x 2 K 0 4 O Q 7\nx / A H z u c P P 6 + S V A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 2 r L D q 3 3 E N a s T E g f b 1 4 u B H S C B J d A = \" > A A A B 9 X i c b V C 7 T s M w F L 0 p r 1 J e B U Y W i x a J q U q 6 w F j B w l g k + p D a U D m u 0 1 q 1 n c h 2 Q C X q f 7 A w g B A r / 8 L G 3 + C 0 G a D l S J a O z r l X 9 / g E M W f a u O 6 3 U 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 d t H S W K 0 B a J e K S 6 A d a U M 0 l b h h l O u 7 G i W A S c d o L J d e Z 3 H q j S L J J 3 Z h p T X + C R Z C E j 2 F j p v t o X 2 I y D M H 2 a D V h 1 U K 6 4 N X c O t E q 8 n F Q g R 3 N Q / u o P I 5 I I K g 3 h W O u e 5 8 b G T 7 E y j H A 6 K / U T T W N M J n h E e 5 Z K L K j 2 0 3 n q G T q z y h C\nF k b J P G j R X f 2 + k W G g 9 F Y G d z E L q Z S 8 T / / N 6 i Q k v / Z T J O D F U k s W h M O H I R C i r A A 2 Z o s T w q S W Y K G a z I j L G C h N j i y r Z E r z l L 6 + S d r 3 m u T X v t l 5 p X O V 1 F O E E T u E c P L i A B t x A E 1 p A Q M E z v M K b 8 + i 8 O O / O x 2 K 0 4 O Q 7\nx / A H z u c P P 6 + S V A = = < / l a t e x i t >\n\n\nHop based Relative Positional Embedding\n\nPart 4 H (D) < l a t e x i t s h a 1 _ b a s e 6 4 = \" 2 a Z i T n g 6 2 C f d E L H g Y k / L L P B m F N s = \" > A A A B + 3 i c b V D L T s J A F L 3 F F + K r 4 t L N R D D B D W n Z 6 J K o C 5 a Y y C O B S q b D F C Z M H 5 m Z G k n T X 3 H j Q m P c + i P u / B u n 0 I W C J 5 n k 5 J x 7 c 8 8 c N + J M K s v 6 N g o b m 1 v b O 8 X d 0 t 7 + w e G R e V z u y j A W h H Z I y E P R d 7 G k n A W 0 o 5 j i t B 8 J i n 2 X 0 5 4 7 u 8 n 8 3 i M V k o X B v Z p H 1 P H x J G A e I 1 h p a W S W q 0 M f q 6 n r J a 3 0 I a n d X q T V k V m x 6 t Y C a J 3 Y O a l A j v b I / B q O Q x L 7 N F C E Y y k H t h U p J 8 F C M c J p W h r G k k a Y z P C E D j Q N s E + l k y y y p + h c K 2 P k h U K / Q K G F + n s j w b 6 U c 9 / V k 1 l Q u e p l 4 n / e I F b e l Z O w I I o V D c j y k B d z p E K U F Y H G T F C i + F w T T A T T W R G Z Y o G J 0 n W V d A n 2 6 p f X S b d R t 6 2 6 f d e o N K / z O o p w C m d Q A x s u o Q k t a E M H C D z B M 7 z C m 5 E a L 8 a 7 8 b E c L R j 5 z g n 8 g f H 5 A 8 7 W k 5 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 2 a Z i T n g 6 2 C f d E L H g Y k / L L P B m F N s = \" > A A A B + 3 i c b V D L T s J A F L 3 F F + K r 4 t L N R D D B D W n Z 6 J K o C 5 a Y y C O B S q b D F C Z M H 5 m Z G k n T X 3 H j Q m P c + i P u / B u n 0 I W C J 5 n k 5 J x 7 c 8 8 c N + J M K s v 6 N g o b m 1 v b O 8 X d 0 t 7 + w e G R e V z u y j A W h H Z I y E P R d 7 G k n A W 0 o 5 j i t B 8 J i n 2 X 0 5 4 7 u 8 n 8 3 i M V k o X B v Z p H 1 P H x J G A e I 1 h p a W S W q 0 M f q 6 n r J a 3 0 I a n d X q T V k V m x 6 t Y C a J 3 Y O a l A j v b I / B q O Q x L 7 N F C E Y y k H t h U p J 8 F C M c J p W h r G k k a Y z P C E D j Q N s E + l k y y y p + h c K 2 P k h U K / Q K G F + n s j w b 6 U c 9 / V k 1 l Q u e p l 4 n / e I F b e l Z O w I I o V D c j y k B d z p E K U F Y H G T F C i + F w T T A T T W R G Z Y o G J 0 n W V d A n 2 6 p f X S b d R t 6 2 6 f d e o N K / z O o p w C m d Q A x s u o Q k t a E M H C D z B M 7 z C m 5 E a L 8 a 7 8 b E c L R j 5 z g n 8 g f H 5 A 8 7 W k 5 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 2 a Z i T n g 6 2 C f d E L H g Y k / L L P B m F N s = \" > A A A B + 3 i c b V D L T s J A F L 3 F F + K r 4 t L N R D D B D W n Z 6 J K o C 5 a Y y C O B S q b D F C Z M H 5 m Z G k n T X 3 H j Q m P c + i P u / B u n 0 I W C J 5 n k 5 J x 7 c 8 8 c N + J M K s v 6 N g o b m 1 v b O 8 X d 0 t 7 + w e G R e V z u y j A W h H Z I y E P R d 7 G k n A W 0 o 5 j i t B 8 J i n 2 X 0 5 4 7 u 8 n 8 3 i M V k o X B v Z p H 1 P H x J G A e I 1 h p a W S W q 0 M f q 6 n r J a 3 0 I a n d X q T V k V m x 6 t Y C a J 3 Y O a l A j v b I / B q O Q x L 7 N F C E Y y k H t h U p J 8 F C M c J p W h r G k k a Y z P C E D j Q N s E + l k y y y p + h c K 2 P k h U K / Q K G F + n s j w b 6 U c 9 / V k 1 l Q u e p l 4 n / e I F b e \n\n\nl Z O w I I o V D c j y k B d z p E K U F Y H G T F C i + F w T T A T T W R G Z Y o G J 0 n W V d A n 2 6 p f X S b d R t 6 2 6 f d e o N K / z O o p w C m d Q A x s u o Q k t a E M H C D z B M 7 z C m 5 E a L 8 a 7 8 b E c L R j 5 z g n 8 g f H 5 A 8 7 W k 5 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 2 a Z i T n g 6 2 C f d E L H g Y k / L L P B m F N s = \" > A A A B + 3 i c b V D L T s J A F L 3 F F + K r 4 t L N R D D B D W n Z 6 J K o C 5 a Y y C O B S q b D F C Z M H 5 m Z G k n T X 3 H j Q m P c + i P u / B u n 0 I W C J 5 n k 5 J x 7 c 8 8 c N + J M K s v 6 N g o b m 1 v b O 8 X d 0 t 7 + w e G R e V z u y j A W h H Z I y E P R d 7 G k n A W 0 o 5 j i t B 8 J i n 2 X 0 5 4 7 u 8 n 8 3 i M V k o X B v Z p H 1 P H x J G A e I 1 h p a W S W q 0 M f q 6 n r J a 3 0 I a n d X q T V k V m x 6 t Y C a J 3 Y O a l A j v b I / B q O Q x L 7 N F C E Y y k H t h U p J 8 F C M c J p W h r G k k a Y z P C E D j Q N s E + l k y y y p + h c K 2 P k h U K / Q K G F + n s j w b 6 U c 9 / V k 1 l Q u e p l 4 n / e I F b e l Z O w I I o V D c j y k B d z p E K U F Y H G T F C i + F w T T A T T W R G Z Y o G J 0 n W V d A n 2 6 p f X S b d R t 6 2 6 f d e o N K / z O o p w C m d Q A x s u o Q k t a E M H C D z B M 7 z C m 5 E a L 8 a 7 8 b E c L R j 5 z g n 8 g f H 5 A 8 7 W k 5 4 = < / l a t e x i t >\n\n\nNotations\n\nIn the sequel of this paper, we will use the lower case letters (e.g., x) to represent scalars, lower case bold letters (e.g., x) to denote column vectors, bold-face upper case letters (e.g., X) to denote matrices, and upper case calligraphic letters (e.g., X ) to denote sets or high-order tensors. Given a matrix X, we denote X(i, \u2236) and X(\u2236, j) as its i th row and j th column, respectively. The (i th , j th ) entry of matrix X can be denoted as either X(i, j). We use X \u22ba and x \u22ba to represent the transpose of matrix X and vector x. For vector x, we represent its L p -norm as x p = (\u2211 i x(i) p ) 1 p . The Frobenius-norm of matrix X is represented as X F = (\u2211 i,j X(i, j) 2 ) 1 2 . The element-wise product of vectors x and y of the same dimension is represented as x \u2297 y, whose concatenation is represented as x \u2294 y.\n\n\nLinkless Subgraph Batching\n\nPrior to talking about the subgraph batching method, we would like to present the problem settings first. Formally, we can represent the input graph data as G = (V, E, w, x, y), where V and E denote the sets of nodes and links in graph G, respectively. Mapping w \u2236 E \u2192 R projects links to their weight; whereas mappings x \u2236 V \u2192 X and y \u2236 V \u2192 Y can project the nodes to their raw features and labels. The graph size can be represented by the number of involved nodes, i.e., V . The above term defines a general graph concept. If the studied G is unweighted, we will have w(\nv i , v j ) = 1, \u2200(v i , v j ) \u2208 E; whereas \u2200(v i , v j ) \u2208 V \u00d7 V \u2216 E, we have w(v i , v j ) = 0.\nNotations X and Y denote feature space and label space, respectively. In this paper, we can simply represent X = R dx and Y = R dy . For node v i , we can also simplify its raw feature and label vector representations as\n\nx i = x(v i ) \u2208 R dx and y i = y(v i ) \u2208 R dy . The GRAPH-BERT model pre-training doesn't require any label supervision information actually, but partial of the labels will be used for the fine-tuning application task on node classification to be introduced later.\n\nInstead of working on the complete graph G, GRAPH-BERT will be trained with linkless subgraph batches sampled from the input graph instead. It will effectively enable the learning of GRAPH-BERT to parallelize (even though we will not study parallel computing of GRAPH-BERT in this paper) on extremely large-sized graphs that the existing graph neural networks cannot handle. Different approaches can be adopted here to sample the subgraphs from the input graph as studied in . However, to control the randomness involved in the sampling process, in this paper, we introduce the top-k intimacy sampling approach instead. Such a sampling algorithm works based on the graph intimacy matrix S \u2208 R V \u00d7 V , where entry S(i, j) measures the intimacy score between nodes v i and v j .\n\nThere exist different metrics to measure the intimacy scores among the nodes within the graph, e.g., Jaccard's coefficienty [Jaccard, 1901], Adamic/Adar [Adamic and Adar, 2003], Katz [Katz, 1953]. In this paper, we define matrix S based on the pagerank algorithm, which can be denoted as\nS = \u03b1 \u22c5 I \u2212 (1 \u2212 \u03b1) \u22c5\u0100 \u22121 ,(1)\nwhere factor \u03b1 \u2208 [0, 1] (which is usually set as 0.15). Term A = AD \u22121 denotes the colum-normalized adjacency matrix. In its representation, A is the adjacency matrix of the input graph, and D is its corresponding diagonal matrix with D(i, i) = \u2211 j A(i, j) on its diagonal. Formally, for any target node v i \u2208 V in the input graph, based on the intimacy matrix S, we can define its learning context as follows: DEFINITION 1. (Node Context): Given an input graph G and its intimacy matrix S, for node v i in the graph, we define its learning context as set \u0393(v i ) = {v j v j \u2208 V \u2216 {v i } \u2227 S(i, j) \u2265 \u03b8 i }. Here, the term \u03b8 i defines the minimum intimacy score threshold for nodes to involve in v i 's context.\n\nWe may need to add a remark: for all the nodes in v i ' learning context \u0393(v i ), they can cover both local neighbors of v i as well as the nodes which are far away. In this paper, we define the threshold \u03b8 i as the k th entry of sorted(S(i, \u2236)) (with v i being excluded), i.e., \u0393(v i ) covers the top-k intimate nodes of v i in graph G. Based on the node context concept, we can also represent the set of sampled graph batches for all the nodes as set G = {g 1 , g 2 , \u22ef, g V }, and g i denotes the subgraph sampled for v i (as the target node). Formally, g i can be represented as g i = (V i , \u2205), where the node set V i = {v i } \u222a \u0393(v i ) covers both v i and its context nodes and the link set is null. For large-sized input graphs, set G can further be decomposed into several mini-batches, i.e., B \u2286 G, which will be fed to train the GRAPH-BERT model.\n\n\nNode Input Vector Embeddings\n\nDifferent from image and text data, where the pixels and words/chars have their inherent orders, nodes in graphs are orderless. The GRAPH-BERT model to be learned in this paper doesn't require any node orders of the input sampled subgraph actually. Meanwhile, to simplify the presentations, we still propose to serialize the input subgraph nodes into certain ordered list instead. Formally, for all the nodes V i in the sampled linkless subgraph g i \u2208 B, we can denote them as a node list\n[v i , v i,1 , \u22ef, v i,k ], where v i,j will be placed ahead of v i,m if S(i, j) > S(i, m), \u2200v i,j , v i,m \u2208 V i .\nFor the remaining of this subsection, we will follow the identical node orders as indicated above by default to define their input vector embeddings.\n\nThe input vector embeddings to be fed to the graphtransformer model actually cover four parts: (1) raw feature vector embedding, (2) Weisfeiler-Lehman absolute role embedding, (3) intimacy based relative positional embedding, and (4) hop based relative distance embedding, respectively.\n\n\nRaw Feature Vector Embedding\n\nFormally, for each node v j \u2208 V i in the subgraph g i , we can embed its raw feature vector into a shared feature space (of the same dimension d h ) with its raw feature vector x j , which can be denoted as\ne (x) j = Embed (x j ) \u2208 R d h \u00d71 .\n(2) Depending on the input raw features properties, different models can be used to define the Embed(\u22c5) function. For instance, CNN can be used if x j denotes images; LSTM/BERT can be applied if x j denotes texts; and simple fully connected layers can also be used for simple attribute inputs.\n\n\nWeisfeiler-Lehman Absolute Role Embedding\n\nThe Weisfeiler-Lehman (WL) algorithm [Niepert et al., 2016] can label the nodes according to their structural roles in the graph data, where the nodes with the identical roles will be labeled with the same code (e.g., integer strings or node colors). Formally, for node v j \u2208 V i in the sampled subgraph, we can denote its WL code as WL(v j ) \u2208 N, which can be pre-computed based on the complete graph and is invariant for different sampled subgraphs. In this paper, we adopt the embedding approach proposed in [Vaswani et al., 2017] and define the nodes WL absolute role embedding vector as e (r)\nj = Position-Embed (WL(v j )) = sin WL(v j ) 10000 2l d h , cos WL(v j ) 10000 2l+1 d h \u230a d h 2 \u230b l=0 ,(3)\nwhere e (r) j \u2208 R d h \u00d71 . The index l iterates throughout all the entries in the above vector to compute the entry values with sin(\u22c5) and cos(\u22c5) functions for the node based on its WL code.\n\n\nIntimacy based Relative Positional Embedding\n\nThe WL based role embeddings can capture the global node role information in the representations. Here, we will introduce a relative positional embedding to extract the local information in the subgraph based on the placement orders of the serialized node list introduced at the beginning of this subsection. Formally, based on that serialized node list, we can denote the position of v j \u2208 V i as P (v j ). We know that P (v i ) = 0 by default and nodes closer to v i will have a small positional index. Furthermore, P (\u22c5) is a variant position index metric. For the identical node v j , its positional index P (v j ) will be different for different sampled subgraphs. Formally, for node v j , we can also extract its intimacy based relative positional embedding with the Position-Embed(\u22c5) function defined above as follows:\ne (p) j = Position-Embed (P(v j )) \u2208 R d h \u00d71 ,(4)\nwhich is quite close to the positional embedding in [Vaswani et al., 2017] for the relative positions in the word sequence.\n\n\nHop based Relative Distance Embedding\n\nThe hop based relative distance embedding can be treated as a balance between the absolute role embedding (for global information) and intimacy based relative positional embedding (for local information). Formally, for node v j \u2208 V i in the subgraph g i , we can denote its relative distance in hops to v i in the original input graph as H(v j ; v i ), which can be used to define its embedding vector as\ne (d) j = Position-Embed (H(v j ; v i )) \u2208 R d h \u00d71 . (5)\nIt it easy to observe that vector e (d) j will also be variant for the identical node v j in different subgraphs.\n\n\nGraph Transformer based Encoder\n\nBased on the computed embedding vectors defined above, we will be able to aggregate them together to define the initial input vectors for nodes, e.g., v j , in the subgraph g i as follows:\nh (0) j = Aggregate(e (x) j , e (r) j , e (p) j , e (d) j ).(6)\nIn this paper, we simply define the aggregation function as the vector summation. Furthermore, the initial input vectors for all the nodes in g i can organized into a matrix\nH (0) = [h (0) i , h (0) i,1 , \u22ef, h(0)\ni,k ] \u22ba \u2208 R (k+1)\u00d7d h . The graph-transformer based encoder to be introduced below will update the nodes' representations iteratively with multiple layers (D layers), and the output by the l th layer can be denoted as\nH (l) = G-Transformer H (l\u22121) = softmax QK \u22ba \u221a d h V + G-Res H (l\u22121) , X i ,(7)\nwhere\n\u23a7 \u23aa \u23aa \u23aa \u23aa \u23a8 \u23aa \u23aa \u23aa \u23aa \u23a9 Q = H (l\u22121) W (l) Q , K = H (l\u22121) W (l) K , V = H (l\u22121) W (l) V .(8)\nIn the above equations, W (l)\nQ , W (l) K , W (l) K \u2208 R d h \u00d7d h denote\nthe involved variables. To simplify the presentations in the paper, we assume nodes' hidden vectors in different layers have the same length. Notation G-Res H (l\u22121) , X i represents the graph residual term introduced in [Zhang and Meng, 2019], and X i \u2208 R (k+1)\u00d7dx is the raw features of all nodes in the subgraph g i . Also different from conventional residual learning, we will add the residual terms computed for the target node v i to the hidden state vectors of all the nodes in the subgraph at each layer. Based on the graph-transformer function defined above, we can represent the representation learning process of GRAPH-BERT as follows:\n\u23a7 \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23a8 \u23aa \u23aa \u23aa \u23aa \u23aa \u23aa \u23a9 H (0) = [h (0) i , h (0) i,1 , \u22ef, h (0) i,k ] \u22ba , H (l) = G-Transformer H (l\u22121) , \u2200l \u2208 {1, 2, \u22ef, D}, z i = Fusion H (D) .(9)\nDifferent from the application of conventional transformer model on NLP problems, which aims at learning the representations of all the input tokens. In this paper, we aim to apply the graph-transformer to get the representations of the target node only. In the above equation, function Fusion (\u22c5) will average the representations of all the nodes in input list, which defines the final state of the target v i , i.e., z i \u2208 R d h \u00d71 .\n\nBoth vector z i and matrix H (D) will be outputted to the following functional component attached to GRAPH-BERT. Depending on the application tasks, the functional component and learning objective (i.e., the loss function) will be different. We will show more detailed information in the following section on GRAPH-BERT pre-training and fine-tuning.\n\n\nGRAPH-BERT Learning\n\nWe propose to pre-train GRAPH-BERT with two tasks: (1) node attribute reconstruction, and (2) graph structure recovery. Meanwhile, depending on the objective application tasks, e.g., (1) node classification and (2) graph clustering as studied in this paper, GRAPH-BERT can be further fine-tuned to adapt both the model and the learned node representations accordingly to the new tasks.\n\n\nPre-training\n\nThe node raw attribute reconstruction task focuses on capturing the node attribute information in the learned representations, whereas the graph structure recovery task focuses more on the graph connection information instead.\n\nTask #1: Node Raw Attribute Reconstruction Formally, for the target node v i in the sampled subgraph g i , we have its learned representation by GRAPH-BERT to be z i . Via the fully connected layer (together with the activation function layer if necessary), we can denote the reconstructed raw attributes for node v i based on z i asx i = FC(z i ). To ensure the learned representations can capture the node raw attribute information, compared against the node raw features, e.g., x i for v i , we can define the node raw attribute reconstruction based loss term as follows:\n1 = 1 V vi\u2208V x i \u2212x i 2 .(10)\nTask #2: Graph Structure Recovery Furthermore, to ensure such representation vectors can also capture the graph structure information, the graph structure recovery task is also used as a pre-training task. Formally, for any two nodes v i and v j , based on their learned representations, we can denote the inferred connection score between them by computing their cosine similarity, i.e.,\u015d i,j = z \u22ba i zj zi zj . Compared against the ground truth graph intimacy matrix defined in Section 3.2, i.e., S, we can denote the introduced loss term as follows:\n2 = 1 V 2 S \u2212\u015c 2 F ,(11)\nwhere\u015c \u2208 R V \u00d7 V with entry\u015c(i, j) =\u015d i,j .\n\n\nModel Transfer and Fine-tuning\n\nIn applying the learned GRAPH-BERT into new learning tasks, the learned graph representations can be either fed into the new tasks directly or with necessary adjustment, i.e., finetuning. In this part, we can take the node classification and graph clustering tasks as the examples, where graph clustering can use the learned representations directly but finetuning will be necessary for the node classification task.\n\n\nTask # 1: Node Classification\n\nBased on the nodes learned representations, e.g., z i for v i , we can denote the inferred label for the node via the functional component as\u0177 i = softmax(FC(z i )). Compared with the nodes' true labels, we will be able to define the introduced node classification loss term on training batch T as\nnc = vi\u2208T dy m=1 \u2212y i (m) log\u0177 i (m).(12)\nBy re-training these stacked fully connected layers together with GRAPH-BERT (loaded from pre-training), we will be able to infer node class labels.\n\n\nTask # 2: Graph Clustering\n\nMeanwhile, for the graph clustering task, the main objective is to partition nodes in the graph into several different clusters, e.g., C = {C 1 , C 2 , \u22ef, C l } (l is a hyper-parameter prespecified in advance). For each objective cluster, e.g., C j \u2208 C, we can denote its center as a variable vector \u00b5 j = \u2211 vi\u2208Cj z i \u2208 R d h . For the graph clustering tasks, the main objective is to group similar nodes into the same cluster, whereas the different nodes will be partitioned into different clusters instead. Therefore, the objective function of graph clustering can be defined as follows:\nmin \u00b5 1 ,\u22ef,\u00b5 l min C l j=1 vi\u2208Cj z i \u2212 \u00b5 j 2 .(13)\nThe above objective function involves multiple variables to be learned concurrently, which can be trained with the EM algorithm much more effectively instead of error backpropagation. Therefore, instead of re-training the above graph clustering model together with GRAPH-BERT, we will only take the learned node representations as the node feature input for learning the graph clustering model instead.\n\n\nExperiments\n\nTo test the effectiveness of GRAPH-BERT in learning the graph representations, in this section, we will provide extensive experimental results of GRAPH-BERT on three realworld benchmark graph datasets, i.e., Cora, Citeseer and Pubmed , respectively. Reproducibility. Both the datasets and source code used can be accessed via link 3 . Detailed information about the server used to run the model can be found at the footnote 4 .\n\n\nDataset and Learning Settings\n\nThe graph benchmark datasets used in the experiments include Cora, Citeseer and Pubmed , which are used in most of the recent state-of-the-art graph neural network research works [Kipf and Welling, 2016;Veli\u010dkovi\u0107 et al., 2018;. Based on the input graph data, we will first pre-compute the node intimacy scores, based on which subgraph batches will be sampled subject to the subgraph size k \u2208 {1, 2, \u22ef, 10, 15, 20, \u22ef, 50}. In addition, we will also pre-compute the node pairwise hop distance and WL node codes. By minimizing the node raw feature reconstruction loss and graph structure recovery loss, GRAPH-BERT can be effectively pre-trained, whose learned (a) Training Accuracy (b) Testing Accuracy Figure 3: The learning performance of GRAPH-BERT on node classification with 1-layer, . . . , 5-layer, and 10-layer, \u22ef, 50-layer on the Cora dataset. The x axis denotes the iterations over the whole training set. The y axes denote the training and testing accuracy, respectively. variables will be transferred to the follow-up node classification and graph clustering tasks with/without fine-tuning. In the experiments, we first pre-train GRAPH-BERT based on the node attribute reconstruction task with 200 epochs, then load and pre-train the same GRAPH-BERT model again based on the graph structure recovery task with another 200 epochs.\n\nIn Figure 2, we show the learning performance of GRAPH-BERT on node attribute reconstruction and graph recovery, which converges very fast on both of these tasks.\n\n\nDefault Parameter Settings\n\nIf not clearly specified, the results reported in this paper are based on the following parameter settings of GRAPH-BERT: subgraph size: k = 7 (Cora), k = 5 (Citeseer) and k = 30   \n\n\nNode Classification without Pre-training\n\nGRAPH-BERT is a powerful mode and it can be applied to address various graph learning tasks in the standalone mode.\n\nTo show the effectiveness of GRAPH-BERT, we will first provide the experimental results of GRAPH-BERT on the node classification task without pre-training here, whereas the pretrained GRAPH-BERT based node classification results will be provided in Section 5.4 in more detail. Here, we will follow the identical train/validation/test set partitions used in the existing graph neural network papers  for fair comparisons.\n\nLearning Convergence of Deep GRAPH-BERT In Figure 3, we illustrate the training records of GRAPH-BERT for node classification on the Cora dataset. To show that GRAPH-BERT is different from other GNN models and GRAPH-BERT works with deep architectures, we also change the model depth with values from {1, 2, \u22ef, 5, 10, 20, \u22c5, 50}.\n\nAccording to the plots, GRAPH-BERT can converge very fast (with less than 10 epochs) on the training set. What's more, as the model depth increases, GRAPH-BERT will not suffer from the suspended animation problem. Even the very deep GRAPH-BERT (50 layers) can still respond effectively to the training data and achieve good learning performance.\n\n\nMain Results\n\nThe learning results of GRAPH-BERT (with different graph residual terms) on node classification are provided in Table 1. The comparison methods used here cover both classic and state-of-the-art GNN models. For the variant models which extend GCN and GAT (with new learning settings, include more training data, re-configure the graph structure  or use new optimization methods), we didn't compare them here. However, similar techniques proposed by these extension works can be used to further help improve GRAPH-BERT as well. According to the achieved scores, we observe that GRAPH-BERT can out-perform most of these baseline methods with a big improvement on both Cora and Pubmed. On Citeseer, its perofrmance is also among the top 3.\n\n\nSubgraph Size k Analysis\n\nAs illustrated in Table 2, we provide the learning performance analysis of GRAPH-BERT with different subgraph sizes, i.e., parameter k, on the Cora dataset. According to the results, parameter k affects the learning performance of GRAPH-BERT a lot, since it defines how many nearby nodes will be used to define the nodes' learning context. For the Cora dataset, we observe that the learning performance of GRAPH-BERT improves steadily as k increases from 1 to 7. After that, as k further increases, the performance will degrade dramatically. For the good scores with k = 1, partial contributions come from the graph residual terms in GRAPH-BERT. The time cost of GRAPH-BERT increases as k goes larger, which is very minor actually compared with other existing GNN models, like GCN and GAT. Similar results can be observed for the other two datasets, but the optimal k are different.\n\n\nGraph Residual Analysis\n\nWhat's more, in Table 3, we also provide the learning results of GRAPH-BERT with different graph residual terms. According to the scores, GRAPH-BERT with graph-raw residual term can outperform the other two, which is also consistent with the experimental observations on these different residual terms as reported in [Zhang and Meng, 2019].\n\n\nInitial Embedding Analysis\n\nAs shown in Table 4, we provide the learning performance of GRAPH-BERT on these three datasets, which takes different initial embeddings as the input. To better show the performance differences, the GRAPH-BERT used here doesn't Table 6: Performance comparison of GRAPH-BERT on fine-tuning tasks with/without pre-training. For all the models shown here, we will only use 1 5 of the normal training max epochs as used by GRAPH-BERT in involve any residual learning. According to the results, using the Weisfeiler-Lehman role embedding, hop based distance embedding and intimacy based positional embedding vectors along, GRAPH-BERT cannot work very well actually, whereas the raw feature embeddings do contribute a lot. Meanwhile, by incorporating such complementary embeddings into the raw feature embedding, the model can achieve better performance than using raw feature embedding only.\n\n\nGraph Clustering without Pre-Training\n\nIn Table 5, we show the learning results of GRAPH-BERT on graph clustering without any pre-training on the three datasets. Formally, the clustering component used in GRAPH-BERT is KMeans, which takes the nodes' raw feature vectors as the input. The results are evaluated with several different metrics shown above.\n\n\nPre-training vs. No Pre-training\n\nThe results reported in the previous subsections are all based on the GRAPH-BERT without pre-training actually.\n\nHere, we will provide the experimental results on GRAPH-BERT with pre-training to show their differences. According to the experiments, given enough training epochs, models with/without pre-training can both converge to very good learning results. Therefore, to highlight the differences, we will only use 1 5 of the normal training epochs here for finetuning GRAPH-BERT, and the results are provided in Table 6. We also show the performance of GRAPH-BERT without pretraining here for comparison.\n\nAccording to the scores, for most of the datasets, pretraining do give GRAPH-BERT a good initial state, which helps the model achieve better performance with only a very small number of fine-tuning epochs. On Cora and Citeseer, pre-training helps both the node classification and graph clustering tasks. Meanwhile, for Pubmed, pre-training helps node classification but degrades the results on graph clustering. Also pre-training with both node classification and graph recovery help the model to capture more information from the graph data, which also lead to higher scores than the models with single pre-training tasks.\n\n\nConclusion\n\nIn this paper, we have introduced the new GRAPH-BERT model for graph representation learning. Different from existing GNNs, GRAPH-BERT works well in deep architectures and will not suffer from the common problems with other GNNs. Based on a batch of linkless subgraphs sampled from the original graph data, GRAPH-BERT can effectively learn the representations of the target node with the extended graph-transformer layers introduced in this paper. GRAPH-BERT can serve as the graph representation learning component in graph learning pipeline. The pre-trained GRAPH-BERT can be transferred and applied to address new tasks either directly or with necessary fine-tuning.\n\nFigure 1 :\n1Architecture of the GRAPH-BERT Model. (Part 1: linkless subgraph batching; Part 2: node input vector embeddings; Part 3: graph transformer based encoder; Part 4: representation fusion; Part 5: functional component. Depending on the target application task, the function component will generate different output. In the sampled subgraphs, it covers both the target node and the surrounding context nodes.)\n\nFigure 2 :\n2Pre-training of GRAPH-BERT on node reconstruction and graph recovery tasks (x axis: iteration; y axis: training loss).\n\n\u2026 \u2026 \u2026\n\u2026Input GraphWL Absolute Positional Embedding Node Raw Feature Embedding Intimacy based Relative Positional Embedding Graph-Transformer(D layers)Representation Fusion Component \n\nPart 1 \n\nPart 2 \n\nPart 3 \n\nFused \nOutput \n\nFunctional Component \n\nActivation Function \n\nOutput \n\ntarget node \nsurrounding context \n\n\n\nTable 1 :\n1Learning Performance of GRAPH-BERT (based on different \ngraph residual terms) compared against existing baseline methods \non node classification. The results of GRAPH-BERT reported here \ndenotes the best observed scores obtained with subgraph size k \u2208 \n{1, 2, \u22ef, 10, 15, 20, \u22ef, 50}. \n\nMethods \nDatasets (Accuracy) \nCora Citeseer Pubmed \nLP ([Zhu et al., 2003]) \n0.680 0.453 \n0.630 \nICA ([Lu and Getoor, 2003]) \n0.751 0.691 \n0.739 \nManiReg ([Belkin et al., 2006]) \n0.595 0.601 \n0.707 \nSemiEmb ([Weston et al., 2008]) 0.590 0.596 \n0.711 \nDeepWalk ([Perozzi et al., 2014b]) 0.672 0.432 \n0.653 \nPlanetoid ([Yang et al., 2016]) \n0.757 0.647 \n0.772 \nMoNet ([Monti et al., 2016]) \n0.817 \n-\n0.788 \nGCN ([Kipf and Welling, 2016]) 0.815 0.703 \n0.790 \nGAT ([Veli\u010dkovi\u0107 et al., 2018]) \n0.830 0.725 \n0.790 \nLOOPYNET ([Zhang, 2018]) \n0.826 0.716 \n0.792 \nGRAPH-BERT \n0.843 0.712 \n0.793 \n\n\n\n\nPubmed); hidden size: 32; attention head number: 2; hidden layer number: D = 2; learning rate: 0.01 (Cora) and 0.001 (Citeseer) and 0.0005 (Pubmed); weight decay: 5e \u22124 ; intermediate size: 32; hidden dropout rate: 0.5; attention dropout rate: 0.3; graph residual term: graph-raw; training epoch: 150 (Cora), 500 (Pubmed), 2000 (Citeseer).\n\nTable 2 :\n2Analysis of subgraph size k on Cora for model performance (testing accuracy and testing loss) and total time cost.Cora Dataset \nk \nTest Accuracy Test Loss Total Time Cost (s) \n1 \n0.804 \n0.791 \n3.64 \n2 \n0.806 \n0.708 \n4.02 \n3 \n0.819 \n0.663 \n4.65 \n4 \n0.818 \n0.690 \n4.75 \n5 \n0.824 \n0.636 \n5.20 \n6 \n0.834 \n0.625 \n5.62 \n7 \n0.843 \n0.620 \n5.96 \n8 \n0.828 \n0.653 \n6.54 \n9 \n0.814 \n0.679 \n6.87 \n10 \n0.819 \n0.653 \n7.26 \n20 \n0.819 \n0.666 \n12.31 \n30 \n0.801 \n0.710 \n17.56 \n40 \n0.768 \n0.805 \n23.77 \n50 \n0.759 \n0.833 \n31.59 \n\n\n\nTable 3 :\n3Learning performance of GRAPH-BERT with different graph residual terms.Methods \nDatasets (Accuracy) \nModels \nResidual Cora \nCiteseer Pubmed \n\nGRAPH-BERT \n\nnone \n0.804 \n0.616 \n0.786 \nraw \n0.817 \n0.653 \n0.786 \ngraph-raw 0.843 \n0.712 \n0.793 \n\n\n\nTable 4 :\n4Learning performance of GRAPH-BERT with different initial embedding inputs.Methods \nDatasets (Accuracy) \nModels \nEmbedding Cora \nCiteseer Pubmed \n\nGRAPH-BERT \n\nhop distance 0.307 \n0.348 \n0.445 \nposition \n0.323 \n0.331 \n0.395 \nwl role \n0.457 \n0.345 \n0.443 \nraw feature 0.795 \n0.611 \n0.780 \nall \n0.804 \n0.616 \n0.786 \n\n\n\nTable 5 :\n5Clustering results of GRAPH-BERT without pre-training solely based on node raw features (MI: mutual information).Metrics \nDatasets \nCora Citeseer Pubmed \nRand \n0.080 0.249 \n0.281 \nAdjusted MI \n0.130 0.287 \n0.313 \nNormalized MI 0.133 0.289 \n0.313 \nHomogeneity 0.133 0.287 \n0.280 \nCompleteness 0.132 0.291 \n0.355 \n\n\n\nTable 1 .\n1For KMeans, the epoch denotes its max-iter parameter.Methods \nDatasets (Accuracy/Rand & Epoch) \nPre-Train Task \nFine-Tune Task \nCora \nCiteseer \nPubmed \n\nNode Reconstruction \nNode Classification 0.827 30 \n0.649 400 \n0.780 100 \nGraph Clustering \n0.400 30 \n0.312 400 \n0.027 100 \n\nStructural Recovery \nNode Classification 0.823 30 \n0.662 400 \n0.788 100 \nGraph Clustering \n0.123 30 \n0.090 400 \n0.132 100 \n\nBoth \nNode Classification 0.836 30 \n0.672 400 \n0.791 100 \nGraph Clustering \n0.177 30 \n0.203 400 \n0.159 100 \n\nNone \nNode Classification 0.805 30 \n0.654 400 \n0.786 100 \nGraph Clustering \n0.080 30 \n0.249 400 \n0.281 100 \n\n\nhttps://github.com/jwzhanggy/Graph-Bert 4 GPU Server: ASUS X99-E WS motherboard, Intel Core i7 CPU 6850K@3.6GHz (6 cores), 3 Nvidia GeForce GTX 1080 Ti GPU (11 GB buffer each), 128 GB DDR4 memory and 128 GB SSD swap.\n\nManifold regularization: A geometric framework for learning from labeled and unlabeled examples. Adar ; Eytan Adamic, Lada A Adar, Belkin, abs/1607.00653node2vec: Scalable feature learning for networks. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina ToutanovaGrover and Leskovec7BERT: pre-training of deep bidirectional transformers for language understanding. An anatomy of graph neural networks going deep via the lens of mutual information: Exponential decay vs. full preservation. ArXiv, abs/1910.04499and Adar, 2003] Eytan Adamic and Lada A. Adar. Friends and neighbors on the web. (3):211-230, July 2003. [Belkin et al., 2006] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled exam- ples. J. Mach. Learn. Res., 7:2399?2434, December 2006. [Chung et al., 2014] Junyoung Chung, \u00c7 aglar G\u00fcl\u00e7ehre, KyungHyun Cho, and Yoshua Bengio. Empirical evalua- tion of gated recurrent neural networks on sequence mod- eling. CoRR, abs/1412.3555, 2014. [Dai et al., 2019] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdi- nov. Transformer-xl: Attentive language models beyond a fixed-length context. CoRR, abs/1901.02860, 2019. [Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken- ton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understand- ing. CoRR, abs/1810.04805, 2018. [Grover and Leskovec, 2016] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. CoRR, abs/1607.00653, 2016. [G\u00fcrel et al., 2019] Nezihe Merve G\u00fcrel, Hansheng Ren, Yujing Wang, Hui Xue, Yaming Yang, and Ce Zhang. An anatomy of graph neural networks going deep via the lens of mutual information: Exponential decay vs. full preser- vation. ArXiv, abs/1910.04499, 2019.\n\nInductive representation learning on large graphs. [ Hamilton, abs/1706.02216CoRR[Hamilton et al., 2017] William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. CoRR, abs/1706.02216, 2017.\n\nWavelets on graphs via spectral graph theory. Applied and Computational Harmonic. [ Hammond, 129?150Analysis. 302[Hammond et al., 2011] David K. Hammond, Pierre Van- dergheynst, and Remi Gribonval. Wavelets on graphs via spectral graph theory. Applied and Computational Har- monic Analysis, 30(2):129?150, Mar 2011.\n\n. [ He, Rethinking imagenet pre-training. CoRR, abs/1811.08883[He et al., 2018] Kaiming He, Ross B. Girshick, and Pi- otr Doll\u00e1r. Rethinking imagenet pre-training. CoRR, abs/1811.08883, 2018.\n\nLong short-term memory. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 9[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8), November 1997.\n\n\u00c9tude comparative de la distribution florale dans une portion des alpes et des jura. Carley ; Binxuan Huang, Kathleen M Huang, Carley ; Paul Jaccard, abs/1904.08035Bulletin del la Soci\u00e9t\u00e9 Vaudoise des Sciences Naturelles. 37Inductive graph representation learning with recurrent graph neural networks. CoRR[Huang and Carley, 2019] Binxuan Huang and Kathleen M. Carley. Inductive graph representation learning with recur- rent graph neural networks. CoRR, abs/1904.08035, 2019. [Jaccard, 1901] Paul Jaccard.\u00c9tude comparative de la dis- tribution florale dans une portion des alpes et des jura. Bulletin del la Soci\u00e9t\u00e9 Vaudoise des Sciences Naturelles, 37:547-579, 1901.\n\nJunction tree variational autoencoder for molecular graph generation. [ Jin, abs/1802.04364CoRR[Jin et al., 2018] Wengong Jin, Regina Barzilay, and Tommi S. Jaakkola. Junction tree variational autoencoder for molecular graph generation. CoRR, abs/1802.04364, 2018.\n\nA new status index derived from sociometric analysis. Leo Katz, Katz, Psychometrika. 181Katz, 1953] Leo Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):39-43, Mar 1953.\n\nConvolutional neural networks for sentence classification. Kim ; Yoon Kim, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsKim, 2014] Yoon Kim. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Pro- cessing (EMNLP), pages 1746-1751, Doha, Qatar, Octo- ber 2014. Association for Computational Linguistics.\n\nSemi-supervised classification with graph convolutional networks. Kipf, N Welling ; Thomas, Max Kipf, Welling, abs/1609.02907CoRR[Kipf and Welling, 2016] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. CoRR, abs/1609.02907, 2016.\n\nPersonalized embedding propagation: Combining neural networks on graphs with personalized pagerank. [ Klicpera, abs/1810.05997CoRR[Klicpera et al., 2018] Johannes Klicpera, Aleksandar Bo- jchevski, and Stephan G\u00fcnnemann. Personalized embed- ding propagation: Combining neural networks on graphs with personalized pagerank. CoRR, abs/1810.05997, 2018.\n\nPiyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. [ Lan, [Lan et al., 2019] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori- cut. Albert: A lite bert for self-supervised learning of lan- guage representations, 2019.\n\nPre-training of graph augmented transformers for medication recommendation. CoRR, abs/1906.00346. [ Li, abs/1605.05273Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML'03. Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhinthe Twentieth International Conference on International Conference on Machine Learning, ICML'03New York, NY, USA; Jason Weston, Fr\u00e9d\u00e9ric Ratle, and Ronan Collobert; New York, NY, USAACM6652Proceedings of the 25th International Conference on Machine Learning, ICML'08. Association for Computing Machinery[Li et al., 2018] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. CoRR, abs/1801.07606, 2018. [Lu and Getoor, 2003] Qing Lu and Lise Getoor. Link-based classification. In Proceedings of the Twentieth Interna- tional Conference on International Conference on Ma- chine Learning, ICML'03, page 496?503. AAAI Press, 2003. [Meng and Zhang, 2019] Lin Meng and Jiawei Zhang. Isonn: Isomorphic neural network for graph representa- tion learning and classification. CoRR, abs/1907.09495, 2019. [Monti et al., 2016] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodol\u00e0, Jan Svoboda, and Michael M. Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. CoRR, abs/1611.08402, 2016. [Niepert et al., 2016] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. CoRR, abs/1605.05273, 2016. [Perozzi et al., 2014a] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre- sentations. In Proceedings of the 20th ACM SIGKDD In- ternational Conference on Knowledge Discovery and Data Mining, KDD '14, pages 701-710, New York, NY, USA, 2014. ACM. [Perozzi et al., 2014b] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social rep- resentations. CoRR, abs/1403.6652, 2014. [Shang et al., 2019] Junyuan Shang, Tengfei Ma, Cao Xiao, and Jimeng Sun. Pre-training of graph augmented transformers for medication recommendation. CoRR, abs/1906.00346, 2019. [Sun et al., 2019] Ke Sun, Zhouchen Lin, and Zhanxing Zhu. Adagcn: Adaboosting graph convolutional networks into deep models, 2019. [Ugander et al., 2011] Johan Ugander, Brian Karrer, Lars Backstrom, and Cameron Marlow. The anatomy of the facebook social graph. CoRR, abs/1111.4503, 2011. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. [Veli\u010dkovi\u0107 et al., 2018] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph Attention Networks. International Conference on Learning Representations, 2018. [Weston et al., 2008] Jason Weston, Fr\u00e9d\u00e9ric Ratle, and Ro- nan Collobert. Deep learning via semi-supervised embed- ding. In Proceedings of the 25th International Conference on Machine Learning, ICML'08, page 1168?1175, New York, NY, USA, 2008. Association for Computing Ma- chinery.\n\nGresnet: Graph residual network for reviving deep gnns from suspended animation. ArXiv, abs/1909.05729. abs/1805.07504Proceedings of the Twentieth International Conference on International Conference on Machine Learning, ICML'03. the Twentieth International Conference on International Conference on Machine Learning, ICML'03AAAI PressZhang and MengSemi-supervised learning using gaussian fields and harmonic functionset al., 2016] Zhilin Yang, William W. Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learn- ing with graph embeddings. CoRR, abs/1603.08861, 2016. [Zhang and Meng, 2019] Jiawei Zhang and Lin Meng. Gres- net: Graph residual network for reviving deep gnns from suspended animation. ArXiv, abs/1909.05729, 2019. [Zhang et al., 2018] Jiawei Zhang, Limeng Cui, and Fisher B. Gouza. SEGEN: sample-ensemble genetic evolutional network model. CoRR, abs/1803.08631, 2018. [Zhang, 2018] Jiawei Zhang. Deep loopy neural network model for graph structured data representation learning. CoRR, abs/1805.07504, 2018. [Zhu et al., 2003] Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the Twentieth International Conference on International Con- ference on Machine Learning, ICML'03, page 912?919. AAAI Press, 2003.\n", "annotations": {"author": "[{\"end\":88,\"start\":75},{\"end\":122,\"start\":89},{\"end\":125,\"start\":123},{\"end\":180,\"start\":126},{\"end\":271,\"start\":181},{\"end\":325,\"start\":272}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":82},{\"end\":102,\"start\":97},{\"end\":138,\"start\":135},{\"end\":187,\"start\":184}]", "author_first_name": "[{\"end\":81,\"start\":75},{\"end\":96,\"start\":89},{\"end\":124,\"start\":123},{\"end\":134,\"start\":126},{\"end\":183,\"start\":181}]", "author_affiliation": "[{\"end\":179,\"start\":140},{\"end\":270,\"start\":207},{\"end\":324,\"start\":273}]", "title": "[{\"end\":72,\"start\":1},{\"end\":397,\"start\":326}]", "venue": null, "abstract": "[{\"end\":1861,\"start\":399}]", "bib_ref": "[{\"end\":2273,\"start\":2251},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2312,\"start\":2294},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2808,\"start\":2781},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2845,\"start\":2822},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2915,\"start\":2891},{\"end\":2946,\"start\":2917},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2973,\"start\":2960},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3570,\"start\":3553},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3749,\"start\":3727},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4055,\"start\":4038},{\"end\":4909,\"start\":4887},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4929,\"start\":4909},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5133,\"start\":5116},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5187,\"start\":5166},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8079,\"start\":8055},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8114,\"start\":8091},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8141,\"start\":8128},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8207,\"start\":8182},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8224,\"start\":8207},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8246,\"start\":8224},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8409,\"start\":8387},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8506,\"start\":8489},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8604,\"start\":8587},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8623,\"start\":8604},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8708,\"start\":8695},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8782,\"start\":8765},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8918,\"start\":8900},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8941,\"start\":8918},{\"end\":9166,\"start\":9132},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9185,\"start\":9166},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9230,\"start\":9219},{\"end\":9362,\"start\":9340},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9560,\"start\":9539},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9842,\"start\":9824},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9859,\"start\":9842},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9878,\"start\":9859},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32844,\"start\":32829},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32880,\"start\":32858},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32900,\"start\":32888},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36339,\"start\":36317},{\"end\":36813,\"start\":36791},{\"end\":38175,\"start\":38153},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":46005,\"start\":45981},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46029,\"start\":46005}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":54453,\"start\":54036},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54585,\"start\":54454},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54903,\"start\":54586},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":55789,\"start\":54904},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":56131,\"start\":55790},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56652,\"start\":56132},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56905,\"start\":56653},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":57233,\"start\":56906},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":57559,\"start\":57234},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":58191,\"start\":57560}]", "paragraph": "[{\"end\":2586,\"start\":1877},{\"end\":3434,\"start\":2588},{\"end\":4370,\"start\":3436},{\"end\":5071,\"start\":4372},{\"end\":5841,\"start\":5073},{\"end\":5899,\"start\":5843},{\"end\":6528,\"start\":5901},{\"end\":7011,\"start\":6530},{\"end\":7440,\"start\":7013},{\"end\":7840,\"start\":7442},{\"end\":7965,\"start\":7857},{\"end\":9027,\"start\":7991},{\"end\":9879,\"start\":9029},{\"end\":9991,\"start\":9881},{\"end\":12671,\"start\":10002},{\"end\":24249,\"start\":23522},{\"end\":25283,\"start\":24556},{\"end\":25641,\"start\":25590},{\"end\":28564,\"start\":25685},{\"end\":30737,\"start\":29914},{\"end\":31340,\"start\":30768},{\"end\":31659,\"start\":31439},{\"end\":31925,\"start\":31661},{\"end\":32703,\"start\":31927},{\"end\":32992,\"start\":32705},{\"end\":33734,\"start\":33024},{\"end\":34592,\"start\":33736},{\"end\":35113,\"start\":34625},{\"end\":35377,\"start\":35228},{\"end\":35665,\"start\":35379},{\"end\":35904,\"start\":35698},{\"end\":36234,\"start\":35941},{\"end\":36877,\"start\":36280},{\"end\":37175,\"start\":36985},{\"end\":38049,\"start\":37224},{\"end\":38224,\"start\":38101},{\"end\":38670,\"start\":38266},{\"end\":38842,\"start\":38729},{\"end\":39066,\"start\":38878},{\"end\":39304,\"start\":39131},{\"end\":39561,\"start\":39344},{\"end\":39647,\"start\":39642},{\"end\":39768,\"start\":39739},{\"end\":40456,\"start\":39811},{\"end\":41047,\"start\":40612},{\"end\":41398,\"start\":41049},{\"end\":41807,\"start\":41422},{\"end\":42050,\"start\":41824},{\"end\":42626,\"start\":42052},{\"end\":43209,\"start\":42657},{\"end\":43278,\"start\":43235},{\"end\":43729,\"start\":43313},{\"end\":44060,\"start\":43763},{\"end\":44251,\"start\":44103},{\"end\":44871,\"start\":44282},{\"end\":45325,\"start\":44923},{\"end\":45768,\"start\":45341},{\"end\":47141,\"start\":45802},{\"end\":47305,\"start\":47143},{\"end\":47517,\"start\":47336},{\"end\":47677,\"start\":47562},{\"end\":48099,\"start\":47679},{\"end\":48429,\"start\":48101},{\"end\":48776,\"start\":48431},{\"end\":49528,\"start\":48793},{\"end\":50439,\"start\":49557},{\"end\":50807,\"start\":50467},{\"end\":51724,\"start\":50838},{\"end\":52080,\"start\":51766},{\"end\":52228,\"start\":52117},{\"end\":52726,\"start\":52230},{\"end\":53351,\"start\":52728},{\"end\":54035,\"start\":53366}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13175,\"start\":12672},{\"attributes\":{\"id\":\"formula_1\"},\"end\":23521,\"start\":13175},{\"attributes\":{\"id\":\"formula_2\"},\"end\":24555,\"start\":24250},{\"attributes\":{\"id\":\"formula_3\"},\"end\":25589,\"start\":25284},{\"attributes\":{\"id\":\"formula_4\"},\"end\":31438,\"start\":31341},{\"attributes\":{\"id\":\"formula_5\"},\"end\":33023,\"start\":32993},{\"attributes\":{\"id\":\"formula_6\"},\"end\":35227,\"start\":35114},{\"attributes\":{\"id\":\"formula_7\"},\"end\":35940,\"start\":35905},{\"attributes\":{\"id\":\"formula_8\"},\"end\":36984,\"start\":36878},{\"attributes\":{\"id\":\"formula_9\"},\"end\":38100,\"start\":38050},{\"attributes\":{\"id\":\"formula_10\"},\"end\":38728,\"start\":38671},{\"attributes\":{\"id\":\"formula_11\"},\"end\":39130,\"start\":39067},{\"attributes\":{\"id\":\"formula_12\"},\"end\":39343,\"start\":39305},{\"attributes\":{\"id\":\"formula_13\"},\"end\":39641,\"start\":39562},{\"attributes\":{\"id\":\"formula_14\"},\"end\":39738,\"start\":39648},{\"attributes\":{\"id\":\"formula_15\"},\"end\":39810,\"start\":39769},{\"attributes\":{\"id\":\"formula_16\"},\"end\":40611,\"start\":40457},{\"attributes\":{\"id\":\"formula_17\"},\"end\":42656,\"start\":42627},{\"attributes\":{\"id\":\"formula_18\"},\"end\":43234,\"start\":43210},{\"attributes\":{\"id\":\"formula_19\"},\"end\":44102,\"start\":44061},{\"attributes\":{\"id\":\"formula_20\"},\"end\":44922,\"start\":44872}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":49582,\"start\":49575},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":50490,\"start\":50483},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":50857,\"start\":50850},{\"end\":51073,\"start\":51066},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":51776,\"start\":51769},{\"end\":52641,\"start\":52634}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1875,\"start\":1863},{\"attributes\":{\"n\":\"2\"},\"end\":7855,\"start\":7843},{\"end\":7989,\"start\":7968},{\"attributes\":{\"n\":\"3\"},\"end\":10000,\"start\":9994},{\"end\":25683,\"start\":25644},{\"end\":29900,\"start\":28567},{\"attributes\":{\"n\":\"3.1\"},\"end\":29912,\"start\":29903},{\"attributes\":{\"n\":\"3.2\"},\"end\":30766,\"start\":30740},{\"attributes\":{\"n\":\"3.3\"},\"end\":34623,\"start\":34595},{\"end\":35696,\"start\":35668},{\"end\":36278,\"start\":36237},{\"end\":37222,\"start\":37178},{\"end\":38264,\"start\":38227},{\"attributes\":{\"n\":\"3.4\"},\"end\":38876,\"start\":38845},{\"attributes\":{\"n\":\"4\"},\"end\":41420,\"start\":41401},{\"attributes\":{\"n\":\"4.1\"},\"end\":41822,\"start\":41810},{\"attributes\":{\"n\":\"4.2\"},\"end\":43311,\"start\":43281},{\"end\":43761,\"start\":43732},{\"end\":44280,\"start\":44254},{\"attributes\":{\"n\":\"5\"},\"end\":45339,\"start\":45328},{\"attributes\":{\"n\":\"5.1\"},\"end\":45800,\"start\":45771},{\"end\":47334,\"start\":47308},{\"attributes\":{\"n\":\"5.2\"},\"end\":47560,\"start\":47520},{\"end\":48791,\"start\":48779},{\"end\":49555,\"start\":49531},{\"end\":50465,\"start\":50442},{\"end\":50836,\"start\":50810},{\"attributes\":{\"n\":\"5.3\"},\"end\":51764,\"start\":51727},{\"attributes\":{\"n\":\"5.4\"},\"end\":52115,\"start\":52083},{\"attributes\":{\"n\":\"6\"},\"end\":53364,\"start\":53354},{\"end\":54047,\"start\":54037},{\"end\":54465,\"start\":54455},{\"end\":54592,\"start\":54587},{\"end\":54914,\"start\":54905},{\"end\":56142,\"start\":56133},{\"end\":56663,\"start\":56654},{\"end\":56916,\"start\":56907},{\"end\":57244,\"start\":57235},{\"end\":57570,\"start\":57561}]", "table": "[{\"end\":54903,\"start\":54737},{\"end\":55789,\"start\":54916},{\"end\":56652,\"start\":56258},{\"end\":56905,\"start\":56736},{\"end\":57233,\"start\":56993},{\"end\":57559,\"start\":57359},{\"end\":58191,\"start\":57625}]", "figure_caption": "[{\"end\":54453,\"start\":54049},{\"end\":54585,\"start\":54467},{\"end\":54737,\"start\":54594},{\"end\":56131,\"start\":55792},{\"end\":56258,\"start\":56144},{\"end\":56736,\"start\":56665},{\"end\":56993,\"start\":56918},{\"end\":57359,\"start\":57246},{\"end\":57625,\"start\":57572}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10116,\"start\":10108},{\"end\":46511,\"start\":46503},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":47154,\"start\":47146},{\"end\":48152,\"start\":48144}]", "bib_author_first_name": "[{\"end\":58519,\"start\":58507},{\"end\":58532,\"start\":58528},{\"end\":58534,\"start\":58533},{\"end\":60321,\"start\":60320},{\"end\":60587,\"start\":60586},{\"end\":60824,\"start\":60823},{\"end\":61317,\"start\":61301},{\"end\":61333,\"start\":61325},{\"end\":61335,\"start\":61334},{\"end\":61957,\"start\":61956},{\"end\":62209,\"start\":62206},{\"end\":62424,\"start\":62414},{\"end\":63003,\"start\":63002},{\"end\":63025,\"start\":63022},{\"end\":63312,\"start\":63311},{\"end\":63675,\"start\":63674},{\"end\":63982,\"start\":63981}]", "bib_author_last_name": "[{\"end\":58526,\"start\":58520},{\"end\":58539,\"start\":58535},{\"end\":58547,\"start\":58541},{\"end\":60330,\"start\":60322},{\"end\":60595,\"start\":60588},{\"end\":60827,\"start\":60825},{\"end\":61323,\"start\":61318},{\"end\":61341,\"start\":61336},{\"end\":61364,\"start\":61343},{\"end\":61961,\"start\":61958},{\"end\":62214,\"start\":62210},{\"end\":62220,\"start\":62216},{\"end\":62428,\"start\":62425},{\"end\":63000,\"start\":62996},{\"end\":63020,\"start\":63004},{\"end\":63030,\"start\":63026},{\"end\":63039,\"start\":63032},{\"end\":63321,\"start\":63313},{\"end\":63679,\"start\":63676},{\"end\":63985,\"start\":63983}]", "bib_entry": "[{\"attributes\":{\"doi\":\"abs/1607.00653\",\"id\":\"b0\",\"matched_paper_id\":16902615},\"end\":60267,\"start\":58410},{\"attributes\":{\"doi\":\"abs/1706.02216\",\"id\":\"b1\"},\"end\":60502,\"start\":60269},{\"attributes\":{\"doi\":\"129?150\",\"id\":\"b2\",\"matched_paper_id\":5593503},\"end\":60819,\"start\":60504},{\"attributes\":{\"id\":\"b3\"},\"end\":61012,\"start\":60821},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1915014},\"end\":61214,\"start\":61014},{\"attributes\":{\"doi\":\"abs/1904.08035\",\"id\":\"b5\",\"matched_paper_id\":135345056},\"end\":61884,\"start\":61216},{\"attributes\":{\"doi\":\"abs/1802.04364\",\"id\":\"b6\"},\"end\":62150,\"start\":61886},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":121768822},\"end\":62353,\"start\":62152},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9672033},\"end\":62928,\"start\":62355},{\"attributes\":{\"doi\":\"abs/1609.02907\",\"id\":\"b9\"},\"end\":63209,\"start\":62930},{\"attributes\":{\"doi\":\"abs/1810.05997\",\"id\":\"b10\"},\"end\":63561,\"start\":63211},{\"attributes\":{\"id\":\"b11\"},\"end\":63881,\"start\":63563},{\"attributes\":{\"doi\":\"abs/1605.05273\",\"id\":\"b12\",\"matched_paper_id\":173990821},\"end\":67114,\"start\":63883},{\"attributes\":{\"doi\":\"abs/1805.07504\",\"id\":\"b13\",\"matched_paper_id\":202565918},\"end\":68438,\"start\":67116}]", "bib_title": "[{\"end\":58505,\"start\":58410},{\"end\":60584,\"start\":60504},{\"end\":61036,\"start\":61014},{\"end\":61299,\"start\":61216},{\"end\":62204,\"start\":62152},{\"end\":62412,\"start\":62355},{\"end\":63979,\"start\":63883},{\"end\":67218,\"start\":67116}]", "bib_author": "[{\"end\":58528,\"start\":58507},{\"end\":58541,\"start\":58528},{\"end\":58549,\"start\":58541},{\"end\":60332,\"start\":60320},{\"end\":60597,\"start\":60586},{\"end\":60829,\"start\":60823},{\"end\":61325,\"start\":61301},{\"end\":61343,\"start\":61325},{\"end\":61366,\"start\":61343},{\"end\":61963,\"start\":61956},{\"end\":62216,\"start\":62206},{\"end\":62222,\"start\":62216},{\"end\":62430,\"start\":62414},{\"end\":63002,\"start\":62996},{\"end\":63022,\"start\":63002},{\"end\":63032,\"start\":63022},{\"end\":63041,\"start\":63032},{\"end\":63323,\"start\":63311},{\"end\":63681,\"start\":63674},{\"end\":63987,\"start\":63981}]", "bib_venue": "[{\"end\":62616,\"start\":62526},{\"end\":64441,\"start\":64259},{\"end\":67441,\"start\":67346},{\"end\":58611,\"start\":58563},{\"end\":60318,\"start\":60269},{\"end\":60612,\"start\":60604},{\"end\":61076,\"start\":61038},{\"end\":61436,\"start\":61380},{\"end\":61954,\"start\":61886},{\"end\":62235,\"start\":62222},{\"end\":62524,\"start\":62430},{\"end\":62994,\"start\":62930},{\"end\":63309,\"start\":63211},{\"end\":63672,\"start\":63563},{\"end\":64111,\"start\":64001},{\"end\":67344,\"start\":67234}]"}}}, "year": 2023, "month": 12, "day": 17}