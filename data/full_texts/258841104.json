{"id": 258841104, "updated": "2023-10-31 11:48:13.742", "metadata": {"title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization", "authors": "[{\"first\":\"Jeonghoon\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Jung\",\"last\":\"Lee\",\"middle\":[\"Hyun\"]},{\"first\":\"Sungdong\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Joonsuk\",\"last\":\"Park\",\"middle\":[]},{\"first\":\"Kang\",\"last\":\"Yoo\",\"middle\":[\"Min\"]},{\"first\":\"Se\",\"last\":\"Kwon\",\"middle\":[\"Jung\"]},{\"first\":\"Dongsoo\",\"last\":\"Lee\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.14152", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-14152", "doi": "10.48550/arxiv.2305.14152"}}, "content": {"source": {"pdf_hash": "a10843d1349fff8d2a7d9722f800802187fef67f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.14152v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2305.14152", "status": "CLOSED"}}, "grobid": {"id": "e7dd7518d156ecdfed64cf6259629c1e19931905", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/a10843d1349fff8d2a7d9722f800802187fef67f.txt", "contents": "\nMemory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization\n28 Oct 2023\n\nJeonghoon Kim jeonghoon.samuel@gmail.com \nJung Hyun Lee \nSungdong Kim sungdong.kim@navercorp.com \nJoonsuk Park park@joonsuk.org \nMin Kang \nYoo \nSeJung Kwon sejung.kwon@navercorp.com \n\nNAVER Cloud\n\n\n\nNAVER Cloud\n\n\n\nNAVER Cloud\nKAIST AI\n\n\n\nNAVER AI Lab\nNAVER Cloud\n\n\n\nUniversity of Richmond\n\n\n\nNAVER Cloud\nSNU AI Center\n\n\n\nNAVER Cloud\n\n\nMemory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization\n28 Oct 2023AF41AA98F9BB354E3FD28A40448EA6D8arXiv:2305.14152v2[cs.LG]\nLarge language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs.While parameterefficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern.Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase.To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) -a simple yet effective method that combines the advantages of PEFT with quantized LLMs.By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions.Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state.Furthermore, it leverages the advantages of quantization to substantially reduce model sizes.Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage.We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters.To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset.Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.\n\nTask Switch\n\nStep 2\n\nFigure 1: Illustration of our proposed PEQA scheme where A \u2022 B indicates the element-wise product of A and B. PEQA is memory-efficient fine-tuning method for quantized large language models that updates only the quantization scale while keeping the integer matrix frozen.Notice a significant reduction in memory footprint when full-precision weights are converted into sub-4-bit integers.\n\n\nIntroduction\n\nLarge language models (LLMs) such as PaLM, LLaMA, and the GPT-series [1][2][3][4][5][6][7] have demonstrated unprecedented levels of task-generalization ability in various applications, including dialogue systems, question answering, summarization, and translation [8,9].While they can follow instructions and learn to solve tasks via in-context task descriptions or few-shot examples [10], fine-tuning allows LLMs to align their behavior with desirable traits, such as following instructions more precisely [11] or adhering to certain principles [12].Additionally, fine-tuning can improve the scaling curve by exposing the model to large collections of task-specific instruction datasets, leading to significant performance enhancements in various unseen downstream tasks [13][14][15][16][17].However, the immense computational cost of fully fine-tuning large-scale models presents challenges for researchers and developers, especially given that LLMs have billions or even trillions of parameters [18].\n\nIn response, several parameter-efficient fine-tuning (PEFT) methods have been introduced [19][20][21], which only update a small number of parameters compared to the pre-trained weights of LLMs.PEFT notably reduces the number of learnable parameters, making the fine-tuning of pre-trained LLMs viable by ensuring that the optimizer states' memory usage becomes negligible.These strategies lead to decreased memory usage during training and more efficient storage and seamless transitions of task-specifically fine-tuned parameters during deployment.Nonetheless, LLMs as a whole still demand significant memory, and further reductions are attainable through model compression.As outlined in Hu et al. [21], for instance, LoRA can cut the memory usage during the fine-tuning of GPT-3 175B from 1.2TB to 350GB.However, the model still requires approximately 350GB of memory for parameters in half-precision floating-point format.\n\nQuantization is a favorable method for both compressing and accelerating neural networks by discretizing parameters into low-bit integers while maintaining a shared high-precision scale within each parameter group (e.g., channel or layer).However, during training phases, quantization-aware training (QAT) [22][23][24][25] mandates updates for all parameters, rendering it not parameter-efficient.Since post-training quantization (PTQ) [26][27][28][29] is executed after training, most existing quantization schemes primarily target the deployment phases.Although PTQ can be integrated with PEFT, when PTQ follows PEFT, the model remains intact during fine-tuning, not decreasing the memory usage.Conversely, if PTQ precedes PEFT, while there's a reduction in memory usage during fine-tuning, no inference acceleration can be achieved due to the PEFT parameters during deployment.\n\nTo bridge the gap between PEFT and quantization, we introduce the Parameter-Efficient and Quantization-aware Adaptation (PEQA), a simple yet effective quantization-aware PEFT method.As illustrated in Figure 1, PEQA encompasses two steps: (a) Decomposition (Quantization) where the parameter matrix of each fully-connected layer is decomposed into a matrix of low-bit integers and quantization scales; and (b) Fine-tuning wherein, for each downstream task, the quantization scale is fine-tuned while the integer matrix remains unchanged.For the quantized LLMs, merely updating the quantization scale leverages the advantages of PEQA.As a result, PEQA maintains the merits of PEFT, such as fewer trainable parameters, along with efficient storage and swift switching of task-specific parameters.Concurrently, it provides the benefits of quantization, including reduced DRAM usage during both training and deployment, and inference acceleration due to fewer memory accesses at deployment.\n\nThrough this, we highlight the following:\n\n\u2022 We introduce PEQA, a method that fine-tunes only the quantization scales of quantized LLMs, keeping the integer matrix frozen.It bridges the gap between PEFT and quantization, offering advantages such as reduced memory consumption during both training and deployment phases, seamless task transitions, and faster inference.\u2022 To empirically validate the approach of solely fine-tuning the quantization scale while freezing the integer matrix, we compare the perplexity of LLMs fine-tuned with QAT, PEFT (+PTQ), and PEQA.The results indicate that PEQA delivers competitive performance in comparison to QAT and PEFT+PTQ, even at sub-4-bit precision.\u2022 To assess the scalability and comprehension performance of PEQA, we apply PEQA to taskspecific adaptation and instruction-tuning.Despite the reduction in model size by a factor of 4 to 5, PEQA demonstrates competitive performance up to a 65B LLM when compared to full-precision baselines.The results suggest that even when LLMs are quantized into low-bit precision, the overall comprehension capability of quantized LLMs can be effectively restored to their original performance using PEQA.\n\n\nRelated Work\n\nLarge Language Models and Alignment Learning.Although LLMs have demonstrated great generalization capabilities through their sheer scales [2,6,30] and the emergent mechanism known as in-context learning [1], they still require significant alignment to follow natural language instructions [13], adhere to ethical guidelines or steer towards harmlessness [12], utilize external tools [31], and to be grounded in knowledge to generate truthful answers [16,32].In particular, instruction-tuning has been pivotal in enabling LLMs to generalize instruction-following abilities, enabling them to solve seemingly any NLP task with only the description of the task in natural text [13,33,34], allowing the models to be accessed in an interactive manner.\n\nParameter-Efficient Fine-Tuning.Fine-tuning leverages the generalization capabilities elicited from the general pretraining to specialize in specific domains and tasks [35,36] or align the LLM with target behaviors [13].However, updating parameters in LLMs comes with a high computation cost and minimal compute environment required for gradient computation.As the hyper-scale era makes fine-tuning for LLMs prohibitively expensive, both efficient and effective alternatives to fine-tuning have received considerable attention.Specifically, inspired by the sensitivity of LLMs to prompts [37], a line of works has proposed introducing trainable prompt embeddings prepended to the input text while freezing the original LLM parameters [19,38,39].As another approach, adapter modules [20] introduce task-specific parameters, which are inserted between the pre-existing layers of the model Extending on this adapter-based approach, LoRA [21] employs the concept of low-rank bottleneck modules while demonstrating comparable performance to full fine-tuning.Subsequent works have unified the various versions and diverging approaches to PEFT [40,41] by formulating them in a single mathematical framework.These parameter-efficient methods have shown comparable performance to full model fine-tuning, presenting a cost-effective and efficient avenue for tailoring LLMs to specific tasks.\n\nHowever, even with the adoption of PEFT, the inherent model size of the LLM remains a challenge to handle.One immediate solution is to apply post-training quantization (PTQ), but its interaction with task-specific parameters is still an area of active research.There have been attempts to integrate PEFT and neural network quantization, including methods like Quadapter [42] and AlphaTuning [43].Yet, these methods have primarily been explored in smaller models of 1.3B or fewer parameters.Appendix J delineates the distinctions between our method and AlphaTuning.\n\nNeural Network Quantization.Neural network quantization consists largely of quantizationaware training (QAT) and PTQ.QAT methods [22][23][24][25] basically train not only quantization scales but all the parameters of a full-precision neural network to narrow the performance gap between the full-precision model and its quantized counterpart.Unfortunately, since QAT involves training all the weights of a full-precision network, it is not feasible to apply QAT to LLMs.To quantize LLMs, PTQ  3 Methodology\n\n\nProblem Setup\n\nMemory Demands in Fine-tuning.Given the significant computational demands associated with fully fine-tuning large language models, parameter-efficient fine-tuning (PEFT) methods have been introduced [19][20][21]46].One of the primary goals of PEFT methods is to reduce memory usage of the optimizer state during training, specifically by reducing the number of learnable parameters.While existing PEFT techniques do decrease memory consumption of the optimizer state and also narrow the accuracy gap between full fine-tuning and PEFT, the pre-trained weights of large language models still demand substantial memory space.When applying LoRA [21] to LLaMA-65B, for instance, even though storing optimizer states for trainable parameters consumes only 52MB (only query and value matrices are adapted with a LoRA rank of 4), the model size still occupies a huge portion of DRAM usage due to the frozen FP16 weights of a pre-trained model.To make PEFT more efficient, reducing the model size is an indispensable requisite.Given that LLMs mostly consist of fully-connected layers, compressing the weights of fully-connected layers is a key factor in compressing the model size and thus leading to more efficient PEFT.\n\nInference Latency of Large Language Models.During text generation inference, an autoregressive LLM generates tokens sequentially.A significant portion of the inference latency arises from matrix-vector multiplications, as opposed to matrix-matrix multiplications.Given that the batch size during inference is typically small [27,28], matrix-vector multiplications tend to be memory-bound.Specifically, accessing global memory, such as DRAM, is expensive on contemporary high-end GPUs.Thus, the number of weights loaded into registers profoundly impacts the speed of multiplication between a matrix and a vector.To decrease the number of weights (subsequently increasing the weights loaded into registers), quantization is a widely researched method for both compressing and speeding up neural networks [29,44,47].While quantization-aware training (QAT) imposes a significant load on both computation and memory, post-training quantization (PTQ) is often viewed as a fallback strategy among traditional quantization techniques to enhance the generation latency of LLMs.To both accelerate LLM inference and retain all advantages of PEFT, an innovative alternative approach should be pursued.Quantization reduces bit-precision for inference acceleration, less storage and increasing throughput.INT8 quantization, which lower the bit-precision for both activations and weights, utilize dedicated engine to effectively accelerate arithmetic computation [48].This is effective for large batches where computing speed matters but less so for smaller batches constrained by memory.To tackle this memory issue, weight-only quantization keeps high precision for activations (e.g., FP16) but compresses weights to 4-bit or less, targeting memory I/O enhancement in modern GPUs [28,47].For simplicity, we mainly focus on low-bit weight-only quantization in a linear asymmetric per-channel context in this paper.\n\nFor pre-trained weights of a fully-connected layer W 0 \u2208 R n\u00d7m , while PEQA can be applied to quantized LLMs, we first quantize W 0 .In other words, for a given bit-width b, quantized pre-trained weights W 0 can be written as , respectively, while per-channel scales and zero-points (namely, s 0 , z 0 \u2208 R n\u00d71 ) are initialized to minimize \u2225W 0 \u2212 W 0 \u2225 2 F .Notice that s 0 and z 0 are not related to any downstream task.Here, we freeze W 0 = clamp W0 s0 + z 0 , 0, 2 b \u2212 1 \u2212 z 0 , which is the integer quantization indices of W 0 , for every full-connected layer in a pre-trained LLM.And then we fine-tune only s 0 (residing outside the clamp function in Eq. 1) while sharing W 0 across all downstream tasks.Consequently, quantized pre-trained weights W 0 are adapted to a downstream task as follows:\nW 0 = s 0 \u2022 W 0 = s 0 \u2022 clamp W 0 s 0 + z 0 , 0, 2 b \u2212 1 \u2212 z 0 ,(1)W = (s 0 + \u2206s) \u2022 W 0 = (s 0 + \u2206s) \u2022 clamp W 0 s 0 + z 0 , 0, 2 b \u2212 1 \u2212 z 0 ,(2)\nwhere \u2206s \u2208 R n\u00d71 represents the gradient update of s 0 obtained by adaptation to a downstream task.We dub Eq. 2 as Parameter-Efficient and Quantization-aware Adaptation (PEQA).PEQA is a memory-efficient fine-tuning method dedicated to quantized LLMs by solely updating quantization scales s 0 .With W 0 being frozen and shared for all downstream tasks, s 0 + \u2206s are task-specific parameters in PEQA, which can be quickly and easily swapped when it is needed to switch to a different downstream task.Note that, PEQA can be seamlessly applied not only to weight-only quantized LLMs but also to weight-activation quantized ones.The overall procedure of PEQA is described in Figure 1 in detail.\n\n\nBenefits of PEQA Inherited from Bridging the Gap between PEFT and Quantization\n\nPEQA is designed to have the advantages of both existing PEFT methods [19,21,46] and quantized LLM [28,44,47,49].We summarize the benefits of PEQA in this subsection.Notably, the utilization of quantization scales s 0 + \u2206s allows PEQA to swiftly and effortlessly switch between task-specific parameters.This capability positions PEQA as ideally suited for deployment of quantized LLMs as a service, mirroring another key advantage of earlier PEFT methods.Note, however, that such a capability is not present in PEFT+PTQ (i.e., the case where PTQ is applied after PEFT) due to the non-reversible quantizers, such as the rounding function.\n\nBenefits of Quantization.Previous PEFT methods freeze the pre-trained weights W 0 and utilize additional learnable parameters to reduce the memory usage of optimizer state.Similarly, PEQA freezes quantized pre-trained weights W 0 (which are the integer quantization values of W 0 ) and fine-tunes quantization scales s 0 .Since W 0 is a b-bit integer matrix, not only can PEQA reduce the optimizer states' size but also the model size, leading to even greater efficiency in the PEFT scheme, as illustrated in Figure 2a.In addition, since W is a b-bit quantized matrix, PEQA can speed up token generation process at inference through dedicated kernels that accelerate the multiplication between a quantized weight matrix and a half-precision activation vector, as described by Frantar et al. [28], Lin et al. [47], and Park et al. [49].It is worth noticing that employing PTQ before fine-tuning (PTQ+PEFT) [50] allows for memory-efficient fine-tuning and seamless task transition for the quantized LLM; however, PTQ+PEFT is not able to inherit from inference acceleration of quantization.As a result, PEQA can achieve both model compression in the process of fine-tuning and inference acceleration for fine-tuned models with marginal performance degradation compared to LoRA, one of the state-of-the-art PEFT techniques, as shown in Figure 2b.\n\nThe comparison of PEQA with other methods using the LLaMA 65B is summarized in Table 1.\n\n\nExperiments\n\nIn this section, we empirically validate the effectiveness of our proposed PEQA method by examining its performance in both parameter-efficient fine-tuning (PEFT) and as a quantization method.We achieve this goal by using a series of benchmarks [52][53][54][55][56][57], datasets [51,58,59], and LLMs [4,6,60,61] that have been publicly introduced.In Section 4.1, to empirically confirm the validity of fine-tuning only the quantization scale while freezing the integer matrix, we compare the perplexity of fine-tuned LLMs through quantization-aware training (QAT), PEFT (+PTQ), and PEQA.In Section 4.2, to evaluate PEQA's scalability and task-specific adaptation performance, we fine-tune and assess LLMs on the Wikitext2 [51] and PennTreeBank [58] datasets using PEQA and LoRA [21].Section 4.3 is dedicated to showcasing PEQA's performance-restoring capability through instruction-tuning on the Alpaca [59] dataset after round-to-nearest (RTN) quantization over the full-precision original model.\n\nTo assess PEQA's performance as a PEFT method, we contrast PEQA with LoRA, which is currently recognized as one of the leading PEFT methods.As discussed in 3.1, we employ a baseline case that merges OPTQ [28], the state-of-the-art weight-only post-training quantization (PTQ) method for LLMs, with LoRA in order to evaluate PEQA's quantization capabilities.In the context of LoRA, QV4 signifies the application of query and value layer weights with a LoRA rank of 4, while QKVO16 indicates the application of query, key, value, and output projection layer weights with a LoRA rank of 16.For PEQA, we utilize round-to-nearest (RTN) for the initialization method of quantized LLM.\n\n4.1 Comparing Quantization Capabilities: PEQA vs. QAT vs. PEFT+PTQ Table 2 presents the perplexity when various quantized LLMs are fine-tuned using QAT, PEFT (+PTQ), and PEQA.To validate our approach, PEQA, which solely fine-tunes the quantization scale as outlined in Eq. 2 while simultaneously maintaining the integer matrix in a frozen state, we use QAT as an upper bound and PEFT+PTQ as a lower bound.Note that QAT, unlike PEQA, updates all parameters including pre-trained weights as well as quantization scales.Diving deeper into the comparison between QAT and PEQA, it is important to note that QAT minimizes the final task loss computed from a weight-only quantized model, as described in Eq. 1, with respect to both W 0 and s 0 .Note that QAT includes all pre-trained weights for training, resulting in the practical model size limitation of LLMs under investigation being capped at 13B in our experiments.Despite the fact that QAT also updates W 0 in Eq. 1, which is one of the most simple and straightforward approach though, we observe that the performance gap between QAT and PEQA narrows when the 4-bit association is introduced, especially as the size of LLMs increases.Impressively, PEQA can even outperform QAT in a 3-bit setting, a notably low-bit setting that challenges OPTQ in terms of quantizing LLMs.These findings suggest that the approach of PEQA, solely updating quantization scales while freezing the integer quantization values of pre-trained weights, can achieve performance comparable to that of QAT.\n\n\nTask-specific Adaptation with Wikitext2 and PennTreeBank Datasets\n\nTask-specific Adaptation and Scalability.We evaluate the task-specific adaptation performance and scalability of PEQA by employing GPT-Neo, GPT-J, and LLaMA models (up to 65B) on the Wikitext2 [51] and PennTreeBank (PTB) [58] datasets.The adaptation performance of PEQA is compared with LoRA, with its configuration set to QV4.As depicted in Table 3, we note a gradual convergence of PEQA's perplexity to that of full-precision LoRA, with only marginal PPL degradation as the model size expands.Thus, Table 3 demonstrates that PEQA, compared to a prominent PEFT technique that utilizes full-precision pre-trained language model (PLM), can maintain a competitive perplexity level in LLMs while concurrently reducing DRAM usage through low-bit quantized  Group-wise Quantization.Group-wise per-channel quantization [49], where weight groups in channels share quantization parameters, maintains accuracy at lower bits.In Table 5, we present that the performance incrementally improves as more learnable parameters are incorporated into PEQA.In particular, for Table 5, we examine various group sizes (denoted by g) when quantizing the weights [49,62].Through relatively straightforward grouping (employed to regulate the number of learnable parameters for PEQA), the perplexity incrementally decreases as more learnable parameters are utilized.Detailed settings are in Appendix G.\n\n\nInstruction-tuning with the Alpaca Dataset\n\nAlthough inference cost and training efficiency are crucial, the methods of PEFT and quantization have not been extensively explored.While LoRA and PEQA have been evaluated on adaptation performance for task-specific purposes as discussed in Section 4.2, it has not been widely corroborated that fine-tuning via PEFT can retain the performance for unseen tasks.Thus, given that RTN quantization results in a non-negligible performance degradation in LLMs, it is important to assess how much the performance of low-bit quantized LLMs is degraded on comprehensive tasks.To address these concerns, we conduct comprehensive experiments, benchmarking our techniques on prevalent instruction-following datasets and assessing the response quality of PEQA-tuned LLMs.Furthermore, to determine if PEQA can regain the performance of full-precision LLMs, we employ RTN quantization in conjunction with PEQA instruction-tuning across LLaMAs.\n\nExperimental Settings.We train LLaMAs [6,7] in various sizes on the Alpaca dataset [59], which is one of the popular instruction-following datasets generated from outputs of InstructGPT [11].Then, we test the models on other downstream tasks such as common-sense reasoning tasks [52][53][54][55] and massive multitask language understanding (MMLU) [56].Due to limited time and resources, we could not conduct an exhaustive search over hyper-parameters such as the learning rate or epoch.Instead, we followed the training recipe from Taori et al. [59].The LoRA configuration is set to QKVO16.Detailed settings can be found in Appendix H.\n\nCommon-Sense Reasoning.We conducted an experiment on five tasks [52][53][54][55] to assess whether the performance of common-sense reasoning and in-context learning can be sustained even after instruction-tuning LLMs on the Alpaca dataset via LoRA or PEQA.As depicted in Table 6, the results show that LLMs fine-tuned with LoRA or PEQA maintain a consistent trend in common-sense reasoning tasks.Furthermore, since PEQA's performance aligns closely with that of full-precision adaptation, this consistency is observed even when the model size has been reduced through low-bit weight quantization.We utilized the evaluation code from Eleuther AI's lm-evaluation-harness [63].\n\nMassive Multitask Language Understanding.To assess whether the performance of PEQAtuned models can be restored to the levels of full-precision model's performance, starting from RTN performance, we test our models on the MMLU benchmark containing 57 multiple-choice problems across various domains and levels of knowledge [56].In this experiment, we utilize the RTN results as a baseline to determine the extent of degradation on quantized LLM.As shown in Table 7, instruction-tuning with PEQA boosts the performance of RTN quantized models.This observation supports our claim that our approach enables LLMs to regain their few-shot in-context learning and understanding capabilities, even though they are significantly smaller than their original model size through quantization.Unfortunately, it seems that the PEQA-tuning does not achieve the best performance in fine-tuning larger models.This might be because PEQA-tuning did not been sufficiently explored different epochs or learning rates.Nonetheless, the observation that the performance of the quantized LLaMAs is restored through PEQA-tuning using an instructionfollowing dataset highlights the potential to further enhance the accuracy of PTQ methods.\n\n\nConclusion\n\nFine-tuning aligns large language models (LLMs) with specific purposes.To maintain the comprehensive capabilities of LLMs while effectively aligning them, we introduce PEQA, a method that seamlessly combines the advantages of parameter-efficient fine-tuning (PEFT) and quantization in LLMs.PEQA not only reduces DRAM consumption during fine-tuning but also accelerates inference latency for deployment by retaining weights in a low-bit quantized format.Through rigorous testing across various datasets and LLMs, we have found that PEQA can match the performance of fullprecision baselines in task-specific adaptations, even with a significant reduction in model size.When combined with instruction-tuning, PEQA's performance demonstrates its ability to both preserve and enhance comprehensive knowledge after the inherent compromises of quantization, recovering the performance of original model by simply updating the quantization scales of the quantized LLM.\n\n\nA Common Experimental Settings\n\nFor the common experimental settings, AdamW [64] optimizer and linear-decaying learning rate scheduler were used.We use Deepspeed repository [65]  C Experimental settings of Table 3 In Section 4.2, Table 3 show the scalability and task-specific adaptation performance of PEQA by comparing with LoRA and LoRA+OPTQ on Wikitext2 [51] and PennTreeBank (PTB) [58] datasets.Detailed experimental settings are as follows.LoRA configuration is set to QV4.For PTQ method, we utilize OPTQ [28] which is state-of-the-art low-bit weight-only PTQ method.We set input sequence length after tokenization (block size) to 1024 for under 65B models.For LLaMA 65B, input sequence length after tokenization is set to 768 due to memory issue.Batch size and epoch for all experiments are set to 128 and 15 respectively.Learning rates for Table 3 experiments are shown in Table 9.\n\n\nF LoRA Configuration Comparison on Wikitext2 Dataset\n\nAs shown in Table 11, the LoRA target module configuration of QV4 and QKVO16 has not much effect on perplexity on Wikitext2 experimental results.Table 11 shows equal tendency as mentioned in [21].We utilize QV4 configuration for Section 4.2 and QKVO16 configuration for Section 4.3 respectively.In Section 4.3, we use the Alpaca dataset [59] for instruction-tuning.We set learning rate, epoch, and quantization group size as in Table 13.The batch size is set to 128 for all experiments in this subsection.As mentioned in Section 4.3, due to limited time and resources, we couldn't conduct an exhaustive search over hyper-parameters such as learning rate or epoch.We believe that there are hyper-parameters that can perform better.For LLaMA 1 series (LLaMA 7, 13, and 30B), we truncate the prompt to the length of 2024 since their maximum sequence length is 2024 when evaluating the massive multitask lanugage understanding (MMLU) benchmark.Thus, for LLaMA2-70B, we set the tokenizer max length to 1024 on fine-tuning due to the resource limit.Otherwise, we use default max length of tokenizer 5 on training.For the evaluation, we use default tokenizer setting.For every experiment in this section, the configuration of PEQA is set to 4-bit RTN quantization.\n\n\nI LLaMA 7B and 13B on Natural Instruction\n\nTo evaluate the instruction-following ability of instruct-tuned models, we test them on another instruction-following dataset, Natural Instruction (NI) [57].Different from the Alpaca dataset, instructions of NI were collected by humans for existing 61 NLP tasks.For simplicity, we utilize evaluation splits consisting of 12 subtasks and restrict the maximum number of instances for each task to 200.At test time, the model should generate proper output for the given input with instruction for the target unseen task.As shown in Table 14, we find LLaMAs trained with PEQA show consistently better zero-shot task generalization performance (ROUGE-L) in NI for all parameter sizes compared to those from LoRA.\n\n\nJ Comparison with AlphaTuning\n\nWhen diving deeper into quantization scales, learnable parameters for both PEQA and AlphaTuning, it's worth noting that PEQA's adherence to uniform quantization means there's only one shared quantization scale for integer weight.Conversely, AlphaTuning's non-uniform approach means that for a b-bit quantization, there are b individual quantization scales for each weight matrix.Despite having multiple scales, AlphaTuning only fine-tunes one, leaving the rest static.As such, the number of trainable parameters are identical and AlphaTuning seems to offer a larger potential for a well-fitted model, but it can be easily seen that b \u2212 1 rest static scales introduced in AlphaTuning have limited usability, and thus the method may be prone to overfitting as evident through empirical results.\n\nIn Table 15, we conducted training on GPT-Neo and OPT 1.3B using the Wikitext2 dataset.Interestingly, PEQA, drawing from its methodological advantages, consistently demonstrates superior performance to AlphaTuning by at least 0.7 ppl on the Wikitext2 dataset.Both AlphaTuning and PEQA used channel-wise trainable parameters.Batch size of AlphaTuning is set to 32.\n\nFigure 2 :\n2\nFigure 2: (a) DRAM usage comparison of LLaMA-65B on various tuning methods and (b) perplexity over model size when tuning LLaMA models with LoRA and PEQA on Wikitext2 dataset.The size of a circle indicates the number of trainable parameters.For instance, the LLaMA-65B model with LoRA has a size of 131GB and 10.49M trainable parameters.Otherwise, LLaMA-65B with 4-bit PEQA has a model size of 33GB and 6.8M trainable parameters.\n\n\n\n\nwhere A \u2022 B, \u230a\u2022\u2309, and clamp(\u2022, a, b) indicate the element-wise product of A and B, the rounding function, and the clamping function into the range [a, b]\n\n\nFigure 3\n3\nFigure 3 illustrates the results of 3-bit and 4-bit PEQA's next token prediction performance on the Wikitext2 dataset.As shown in Figure 3, 3-bit performance of PEQA shows lower perplexity than 3-bit post-training quantized LoRA.The results from the 3-bit PEQA show that PEQA allows for continuity in model size options under DRAM usage constraints.\n\n\nFigure 3 :\n3\nFigure 3: The perplexity over model size of 3/4-bit performance of PEQA and LoRA+OPTQ\n\n\n5 H\n5\nExperimental Settings of Section 4.3\n\n\nTable 1 :\n1\nComparison of PEQA with other methods using LLaMA 65B on the DRAM usage and training time during fine-tuning, the DRAM storage for deployment, the inference acceleration, and task-switching efficiency.The DRAM usage estimation for PEFT is based on LoRA.PEFT+PTQ denotes PTQ after PEFT and PTQ+PEFT denotes PTQ before PEFT.\nDRAMDRAMInferenceTask-Method(Fine-Tuning) (Deployment)SpeedSwitchingFull Fine-Tuning457GB131GBSlowSlowPEFT131GB131GBSlowFastPEFT+PTQ131GB33GBFastSlowPTQ+PEFT33GB33GBSlowFastPEQA (Ours)33GB33GBFastFast3.2 Parameter-Efficient and Quantization-aware Adaptation (PEQA)\n\nTable 2 :\n2\n[28,49]rically confirm the validity of PEQA's approach, we compare the perplexity (PPL) of fine-tuned LLMs through QAT, PEFT+PTQ, and PEQA on Wikitext2[51]for GPT-Neo 2.7B, GPT-J 6B, LLaMA 7B, and LLaMA 13B.Weights are quantized into either 3-bit or 4-bit per channel, without a group size[28,49].LoRA configuration is set to QV4.The lower PPL, the better.Benefits of PEFT.By solely updating the quantization scales, PEQA substantially reduces the memory overhead associated with optimizer state, a feature consistent with other PEFT approaches.\nMethodW Bits GPT-Neo 2.7B GPT-J 6B LLaMA 7B LLaMA 13BQAT411.078.815.765.26LoRA + OPTQ412.098.917.135.31PEQA (Ours)411.388.845.845.30QAT312.379.606.145.59LoRA + OPTQ321.9311.2219.477.33PEQA (Ours)312.549.366.195.54\n\nTable 3 :\n3\nTo show scalability of PEQA, the perplexity (PPL) on Wikitext2 and PennTreeBank (PTB) was compared with LoRA and PEQA.In this comparison, only the weights were quantized into 3-bit and 4-bit per-channel without group size.LoRA configuration is set to QV4.A lower PPL value indicates better performance.\nMethodW BitsGPT-Neo 2.7BGPT-J 6BLLaMA 7BLLaMA 13BLLaMA 30BLLaMA 65BWikitext2LoRA1610.638.505.535.064.063.82LoRA+OPTQ412.098.917.135.314.394.10PEQA (Ours)411.388.845.845.304.364.02LoRA+OPTQ321.9311.2219.477.335.945.32PEQA (Ours)312.549.366.195.544.584.27PTBLoRA1615.9212.929.148.527.217.11LoRA+OPTQ418.8313.4611.228.837.557.46PEQA (Ours)416.5513.309.698.647.687.36\n\nTable 2\n2\nreveals the competitive performance of PEQA compared to QAT.Furthermore, our observations indicate that PEQA consistently outperforms the combination of LoRA and OPTQ for any selected model, regardless of whether a 3-bit or 4-bit setting is employed.Such superior performance can be attributed to PEQA's method of fine-tuning quantized LLMs, which minimizes the final task loss on the full training data, a capability that OPTQ lacks.Detailed settings are in Appendix B.\n\n\nTable 4 :\n4\nNumber of learnable parameters and model size of GPT-Neo, GPT-J and LLaMAs.PEQA configuration is set to 4-bit or 3-bit channel-wise quantization.\nMethodGPT-Neo 2.7BGPT-J 6BLLaMA 7BLLaMA 13BLLaMA 30BLLaMA 65B# ofLoRA (QV4)1.311.842.103.286.3910.49LearnableLoRA (QKVO16)5.247.348.3913.1125.5641.94Param. (M) PEQA (Ours)0.741.031.362.134.156.80ModelLoRA (QV4)5.3012.1013.4826.0365.06130.57SizePEQA (Ours, 4-bit)1.533.653.777.0116.9233.45(GB)PEQA (Ours, 3-bit)1.212.942.965.4212.9025.35\n\nTable 5 :\n5\n[49]i-scale (grouping) performance with PEQA-tuned LLaMA 7B and 13B on Wikitext2 where g indicates the group size[49].The perplexity consistently increases as PEQA take on more learnable parameters.\nModelW Bits Channel-Wise g256 g128 g64LLaMA 7B45.845.695.66 5.6436.195.965.91 5.89LLaMA 13B45.305.185.16 5.1635.545.405.37 5.34\n[4]ghts.Notably, for a 3-bit quantization, PEQA experiences less performance degradation as the model size decreases due to extreme low-bit quantization compared to the combined LoRA and OPTQ.To further elucidate our findings, we have provided figures illustrating the results of 3-bit and 4-bit PEQA in the Appendix D. The comprehensive results indicate that for the deployment stage, PEQA allows models with larger parameters to operate under DRAM usage constraints, outperforming full-precision PEFT methods.For instance, under a restricted DRAM footprint, large LLaMA models can be explored using PEQA, while full-precision LoRA permits only smaller LLaMA models.Additional results with OPT[4], ranging from 1.3B to 66B models are included in the Appendix E. The detailed experimental settings are also included in the Appendix C.Model Size and Number of Learnable Parameters.In an effort to estimate the DRAM usage necessitated by PEQA and LoRA during training and deployment, we outline the number of learnable parameters (for training) and the model size (expressed in gigabytes, GB, for deployment) in\n\n\nTable 4 .\n4\nAs demonstrated in Table4, PEQA involves fewer learnable parameters than LoRA when a quantization scale is assigned to each channel of pre-trained weights.For instance, PEQA has approximately 1.54 times fewer learnable parameters for LLaMA models than LoRA (QV4).In addition to having fewer learnable parameters, PEQA, through low-bit weight quantization, can also reduce the model size, which captures a huge amount of the DRAM footprint in fine-tuning LLMs.Remarkably, when fine-tuning LLaMA 30B using PEQA with 4-bit precision, the resulting model size is significantly smaller than that obtained by adapting 13B through LoRA, and slightly larger than the model adapted from LLaMA 7B using LoRA.Additionally, a comparison of the memory peak during training between PEQA and LoRA is provided in Appendix L.\n\n\nTable 6 :\n6\n[6]mon-sense reasoning and in-context learning performance of parameter-efficient instruction-tuned LLaMAs[6]using Alpaca datasets.LoRA configuration is set to QKVO16.Quantization precision of PEQA is set to 4-bit per-channel without group size.Note that ARC-C, ARC-E and OBQA stands for ARC-Challenge, ARC-Easy, and OpenBookQA respectively.\nMethod# ParamsModel Size (GB)PIQA HellaSwag ARC-C ARC-E OBQAAverageZero-Shot7B13.5GB77.373.041.452.542.457.3LLaMA13B26.1GB79.176.244.559.942.260.430B65.1GB80.179.245.558.942.061.17B13.5GB78.673.343.755.843.058.9(+1.6)+ LoRA13B26.1GB79.676.746.362.043.261.5(+1.1)30B65.1GB81.880.348.261.642.862.9(+1.8)7B3.8GB77.971.442.457.242.058.2(+0.9)+ PEQA13B7.0GB78.974.046.462.542.860.9(+0.5)30B16.9GB80.378.449.863.342.862.9(+1.8)Five-Shot7B13.5GB79.475.345.665.844.062.0LLaMA13B26.1GB80.078.450.470.847.265.430B65.1GB82.582.256.274.947.068.67B13.5GB79.975.246.466.547.263.0(+1.0)+ LoRA13B26.1GB81.178.853.572.447.066.6(+1.1)30B65.1GB84.183.359.579.250.671.4(+2.8)7B3.8GB78.973.245.165.444.061.3(\u22120.7)+ PEQA13B7.0GB80.776.050.971.648.065.5(+0.1)30B16.9GB82.780.256.875.547.668.6(+0.0)\n\nTable 7 :\n7\n[7]sive Multitask Language Understanding (MMLU) benchmark performance of PEQAtuned LLaMAs using Alpaca datasets.Five-shot accuracy is reported for the MMLU.Quantization precision of PEQA is set to 4-bit.When we quantize LLaMA[6]into 4-bit precision using the RTN method, no group size is applied.For LLaMA2[7], a group size of 256 is used with the RTN method.Note that RTN stands for round-to-nearest in the table.\n# ParamsModelHumanities STEMSocialOther AverageSizeSciencesLLaMA [6]7B13.5GB32.629.638.037.934.413B26.1GB42.836.153.353.246.130B65.1GB54.646.566.163.457.4+ RTN7B3.8GB28.425.626.931.828.3(w/o group size)13B7.0GB30.527.235.538.832.830B16.9GB39.634.046.149.742.1+ PEQA7B3.8GB35.730.938.240.035.813B7.0GB42.837.753.649.045.030B16.9GB51.144.162.460.754.3LLaMA2 [7]7B13.5GB43.337.051.852.445.913B26.0GB54.444.263.460.855.770B 138.0GB65.257.980.374.769.1+ RTN7B3.8GB39.535.549.349.943.2(g256)13B7.0GB50.242.661.359.753.270B35.3GB63.755.978.471.667.0+ PEQA7B3.8GB52.038.454.152.048.113B7.0GB60.545.063.357.055.370B35.3GB73.955.377.868.267.5\n\nTable 8 :\n8\n[28] FP16 and BF16 training.Additionally, we utilize Huggingface repository[66]3for training, evaluation code and dataset.We compare the perplexity when weights are quantized and adapted by quantization-aware training (QAT), LoRA with post-training quantization (PTQ), and PEQA, using the Wikitext2 dataset in Section 4.1.The LoRA configuration is set to QV4.For PTQ method, we utilize OPTQ[28]4which is state-of-the-art low-bit weight-only PTQ method.We set the model's maximum sequence length to 1024.Batch size and epoch for all experiments are set to 128 and 15 respectively.Te learning rates for the experiments of Table2are displayed in Table 8.Learning rates for LoRA and PEQA are shown in Appendix C. Learning rates of QAT in Table 2.\nB Experimental Settings of Section 4.1Method W Bits GPT-Neo 2.7B GPT-J 6B LLaMA 7B LLaMA 13BQAT44e-55e-61e-53e-5QAT36e-51e-52e-51e-5\n\nTable 9 :\n9\nLearning rate of LoRA and PEQA in Table3 onWikitext2 and PTB datasets.D The Perplexity of 3-bit and 4-bit PEQA on Wikitext2 Dataset\nMethodW BitsGPT-Neo 2.7BGPT-J 6BLLaMA 7BLLaMA 13BLLaMA 30BLLaMA 65BWikitext2LoRA165e-46e-41e-41e-42e-44e-5PEQA (Ours)45e-56e-66e-61e-51e-51e-5PEQA (Ours)36e-55e-52e-56e-53e-53e-5PTBLoRA162e-31e-38e-45e-44e-46e-4PEQA (Ours)43e-45e-55e-55e-53e-56e-5\n\nTable 10\n10\n[4]ws the perplexity of OPT[4]models adapted with PEQA and LoRA on the Wikitext2 dataset.The perplexity gap between LoRA and PEQA becomes smaller as the model size increases.\n\n\nTable 10 :\n10\nThe perplexity (PPL) on Wikitext2 for OPT 1.3B to 66B.In this comparison, only the weights were quantized into 4-bit.A lower PPL value indicates better performance.\nMethodW Bits OPT 1.3B OPT 2.7B OPT 6.7B OPT 13B OPT 30B OPT 66BLoRA(QV4)1611.5810.258.968.447.937.64PEQA(Ours)412.4010.789.348.748.117.86\n\nTable 11 :\n11\nThe perplexity (PPL) on Wikitext2 was compared with LoRA QV4 and QKVO16.A lower PPL value indicates better performance.In Section 4.2, Table5shows the perplexity of PEQA with grouping learnable parameters.We set model maximum sequence length to 1024.Batch size and epoch for all experiments are set to 128 and 15 respectively.Learning rates for experiments are shown in Table12.\nMethod# Bits GPT-Neo 2.7B GPT-J 6B LLaMA 7B LLaMA 13B LLaMA 30B LLaMA 65BLoRA(QV4)1610.638.505.535.064.063.82LoRA(QKVO16)1610.678.505.505.064.063.81\n\nTable 12 :\n12\nLearning rate for Table5.\n\n\nTable 13 :\n13\n[49]learning rate, epoch, quantization group size[49]for experiments on Section 4.3.the weights were quantized into 4-bit.\nHyper-parameter LLaMA 7B LLaMA 13B LLaMA 30B LLaMA2 7B LLaMA2 13B LLaMA2 70BEpoch335335Learning rate2e-52e-55e-65e-65e-65e-6Group sizePer-channel Per-channelPer-channel256256256\n\nTable 14 :\n14\nNatural Instruction benchmark performance of parameter-efficient instruction-tuned LLa-MAs using Alpaca datasets.Zero-shot performance (ROUGE-L) is reported for the NI.LoRA configuration is set to QKVO16.Quantization precisions of LoRA w/ OPTQ and PEQA are set to 4-bit.\n# Params LLaMA +LoRA +LoRA w/OPTQ +PEQA7B9.424.425.027.113B8.931.329.234.1\n\nTable 15 :\n15\nThe perplexity (PPL) of AlphaTuning and PEQA on Wikitext2 with OPT and GPT-Neo 1.3B.The lower PPL, the better.\nMethod# Bits OPT 1.3B GPT-Neo 1.3BAlphaTuning413.1515.03PEQA (Ours)412.4014.22AlphaTuning314.0017.25PEQA (Ours)313.4015.16\n\nTable 16 :\n16\nLearning rate of AlphaTuning in Table15.\nMethodW Bits OPT 1.3B GPT-Neo 1.3BAlphaTuning41e-45e-4AlphaTuning31e-41e-3\nhttps://github.com/microsoft/DeepSpeed\nhttps://github.com/huggingface/transformers/tree/main/examples/pytorch/ language-modeling\nhttps://github.com/IST-DASLab/gptq\ne.g. 'hf-internal-testing/llama-tokenizer' for LLaMA-1 series, 'meta-llama/Llama-2-70b-hf' for LLaMA-2 series.\nhttps://huggingface.co/docs/transformers/perf_train_gpu_one\nhttps://github.com/huggingface/transformers/blob/main/examples/pytorch/ language-modeling/run_clm_no_trainer.py\nK Choice of Updating Quantization Scales or Zero-PointsUniform quantization can represent both asymmetric and symmetric quantizations, hence it's not always necessary to mandate the use of zero-points.This is why adopting a strategy of only learning the scale factor serves as a fundamental and scalable baseline.We opted for this approach to clearly establish its advantages.To determine the efficacy of learning only the scaling factors, we have incorporated additional experiments.By referring to the table below, it's evident that merely optimizing zero-points does not yield effective learning outcomes.Moreover, simultaneously optimizing both zero-points and quantization scales does not present any significant improvement in accuracy either.L Memory Peak on TrainingThe memory consumption is not solely dictated by the model size but is also influenced by various other factors 6 .Our approach with PEQA inherently offers memory advantages during fine-tuning by striving to minimize both the model size and the number of training parameters.To provide a clear understanding of these benefits, we conducted tests using a single NVIDIA A100-80GB GPU and the causal language modeling code from the HuggingFace repository 7 .Both LoRA and PEQA fine-tuned the LLaMA-7B on the Wikitext2 dataset with a batch size of 2 without gradient accumulation.Our findings indicated that while LoRA peaked at a memory usage of 59GB during optimization, PEQA used just 43GB.Remarkably, this disparity (16GB, 7B) escalates as the model size increases; for instance, a 65B full-precision model under LoRA occupies 130GB, whereas PEQA remarkably uses just 33GB.Additionally, LoRA encountered Out-Of-Memory (OOM) issues at a batch size of 4, whereas PEQA, due to its efficiency, continued training seamlessly.\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, 202033\n\n. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck2022Jeff Dean, Slav Petrovand Noah Fiedel. Palm: Scaling language modeling with pathways\n\n. Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, Samuel Weinbach, Gpt-Neox, Large Scale Autoregressive Language Modeling in PyTorch. 8 2021\n\nOpt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint\n\n. OpenAI. Gpt-4 technical report. 2023\n\nThibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth\u00e9e Lachaux, Baptiste Lacroix, Naman Rozi\u00e8re, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023\n\nLlama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint\n\nLearning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033\n\nGpt-3: Its nature, scope, limits, and consequences. Minds Mach. Luciano Floridi, Massimo Chiriatti, 10.1007/s11023-020-09548-1dec 202030\n\nRethinking the role of demonstrations: What makes in-context learning work. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022\n\nMultitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Jos Neeraj, Abheesht Rozen, Andrea Sharma, Thibault Santilli, Jason Fevry, Alan Fries, Ryan Teehan, International Conference on Learning Representations. Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M RushTeven Le Scao2022\n\nFine-tuned language models are continual learners. Thomas Scialom, Tuhin Chakrabarty, Smaranda Muresan, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022\n\nUnifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, I Sida, Wang, arXiv:2201.059662022arXiv preprint\n\nTranscending scaling laws with 0.1% extra compute. Yi Tay, Jason Wei, Hyung Won Chung, David R Vinh Q Tran, Siamak So, Xavier Shakeri, Huaixiu Garcia, Jinfeng Steven Zheng, Aakanksha Rao, Chowdhery, arXiv:2210.113992022arXiv preprint\n\nSwitch transformers: Scaling to trillion parameter models with simple and efficient sparsity. William Fedus, Barret Zoph, Noam Shazeer, J. Mach. Learn. Res. 1532-4435231jan 2022\n\n. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang, 2021Gpt understands, too\n\nParameter-efficient transfer learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, Proceedings of the 36th International Conference on Machine Learning. Kamalika Chaudhuri, Ruslan Salakhutdinov, the 36th International Conference on Machine LearningPMLR09-15 Jun 201997\n\nLoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022\n\nLearning to quantize deep networks by optimizing quantization intervals with task loss. Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun Kwak, Sung Ju Hwang, Changkyu Choi, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2019\n\nLearned step size quantization. K Steven, Jeffrey L Esser, Deepika Mckinstry, Rathinakumar Bablani, Dharmendra S Appuswamy, Modha, International Conference on Learning Representations. 2020\n\nLinear symmetric quantization of neural networks for low-precision integer hardware. Xiandong Zhao, Ying Wang, Xuyi Cai, Cheng Liu, Lei Zhang, International Conference on Learning Representations. 2020\n\nCluster-promoting quantization with bit-drop for minimizing network quantization loss. Jung Hyun, Lee , Jihun Yun, Sung Ju Hwang, Eunho Yang, 2021\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.07339-bit matrix multiplication for transformers at scale. 20228arXiv preprintint8 (\n\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, arXiv:2206.018612022arXiv preprint\n\nOPTQ: Accurate quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, The Eleventh International Conference on Learning Representations. 2023\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han, arXiv:2211.104382022arXiv preprint\n\nTraining compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint\n\nToolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint\n\nWebgpt: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, John Schulman, CoRR, abs/2112.093322021\n\nSelf-instruct: Aligning language model with self generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, 10.48550/arXiv.2212.105602022\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019\n\nExploiting cloze questions for few shot text classification and natural language inference. Timo Schick, Hinrich Sch\u00fctze, arXiv:2001.076762020arXiv preprint\n\nPrefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, 10.18653/v1/2021.acl-long.353Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211Long Papers)\n\nThe power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, 10.18653/v1/2021.emnlp-main.243Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 2021Online and Punta Cana\n\nTowards a unified view of parameter-efficient transfer learning. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig, International Conference on Learning Representations. 2022\n\nUniPELT: A unified framework for parameter-efficient language model tuning. Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, Madian Khabsa, 10.18653/v1/2022.acl-long.433Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221\n\nQuadapter: Adapter for GPT-2 quantization. Minseop Park, Jaeseong You, Markus Nagel, Simyung Chang, Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 7-11, 2022. 2022\n\nAlphatuning: Quantization-aware parameter-efficient adaptation of large-scale pre-trained language models. Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Min Kang, Jin-Hwa Yoo, Baeseong Kim, Byeongwook Park, Jung-Woo Kim, Nako Ha, Dongsoo Sung, Lee, Findings of the Association for Computational Linguistics: EMNLP 2022. Yoav Goldberg, Zornitsa Kozareva, Yue Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 7-11, 2022. 2022\n\nFlexRound: Learnable rounding based on element-wise division for post-training quantization. Jung Hyun, Lee , Jeonghoon Kim, Se Jung Kwon, Dongsoo Lee, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningPMLRJul 2023202Proceedings of Machine Learning Research\n\nRethinking channel dimensions to isolate outliers for low-bit weight quantization of large language models. Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee, arXiv:2309.155312023arXiv preprint\n\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-of-the-art parameter-efficient fine-tuning methods. 2022\n\nAwq: Activation-aware weight quantization for llm compression and acceleration. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han, arXiv:2306.009782023arXiv preprint\n\nSmoothQuant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023\n\nLut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models. Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee, 2023\n\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. 2023arXiv preprint\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, 2016\n\nThink you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint\n\nReasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034\n\nHellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019\n\nCan a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, EMNLP. 2018\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint\n\nCross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, ACL. 2022\n\nBuilding a large annotated corpus of English: The Penn Treebank. Mitchell P Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, Computational Linguistics. 1921993\n\nStanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023\n\nGPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman, 10.5281/zenodo.5297715March 2021If you use this software, please cite it using these metadata\n\nLlama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao, arXiv:2303.161992023arXiv preprint\n\nQSGD: communication-efficient SGD via gradient quantization and encoding. Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, Milan Vojnovic, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Isabelle Guyon, Samy Ulrike Von Luxburg, Hanna M Bengio, Rob Wallach, S V N Fergus, Roman Vishwanathan, Garnett, Long Beach, CA, USADecember 4-9, 2017. 2017\n\nA framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, Andy Zou, 10.5281/zenodo.5371628September 2021\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAMay 6-9, 2019. 2019OpenReview.net\n\nDeepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, 10.1145/3394486.3406703KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event. Rajesh Gupta, Yan Liu, Jiliang Tang, B Aditya Prakash, CA, USAACMAugust 23-27, 2020. 2020\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsOctober 2020\n", "annotations": {"author": "[{\"end\":155,\"start\":114},{\"end\":170,\"start\":156},{\"end\":211,\"start\":171},{\"end\":242,\"start\":212},{\"end\":252,\"start\":243},{\"end\":257,\"start\":253},{\"end\":296,\"start\":258},{\"end\":311,\"start\":297},{\"end\":326,\"start\":312},{\"end\":350,\"start\":327},{\"end\":378,\"start\":351},{\"end\":404,\"start\":379},{\"end\":433,\"start\":405},{\"end\":448,\"start\":434}]", "publisher": null, "author_last_name": "[{\"end\":127,\"start\":124},{\"end\":169,\"start\":166},{\"end\":183,\"start\":180},{\"end\":224,\"start\":220},{\"end\":251,\"start\":247},{\"end\":256,\"start\":253},{\"end\":269,\"start\":265}]", "author_first_name": "[{\"end\":123,\"start\":114},{\"end\":160,\"start\":156},{\"end\":165,\"start\":161},{\"end\":179,\"start\":171},{\"end\":219,\"start\":212},{\"end\":246,\"start\":243},{\"end\":264,\"start\":260}]", "author_affiliation": "[{\"end\":310,\"start\":298},{\"end\":325,\"start\":313},{\"end\":349,\"start\":328},{\"end\":377,\"start\":352},{\"end\":403,\"start\":380},{\"end\":432,\"start\":406},{\"end\":447,\"start\":435}]", "title": "[{\"end\":100,\"start\":1},{\"end\":548,\"start\":449}]", "venue": null, "abstract": "[{\"end\":2316,\"start\":618}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2816,\"start\":2813},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2819,\"start\":2816},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2822,\"start\":2819},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2825,\"start\":2822},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2828,\"start\":2825},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2831,\"start\":2828},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2834,\"start\":2831},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3012,\"start\":3009},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3014,\"start\":3012},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3133,\"start\":3129},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3256,\"start\":3252},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3295,\"start\":3291},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3521,\"start\":3517},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3525,\"start\":3521},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3529,\"start\":3525},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3533,\"start\":3529},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3537,\"start\":3533},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3747,\"start\":3743},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3843,\"start\":3839},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3847,\"start\":3843},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3851,\"start\":3847},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4454,\"start\":4450},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4988,\"start\":4984},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4992,\"start\":4988},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4996,\"start\":4992},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5000,\"start\":4996},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5118,\"start\":5114},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5122,\"start\":5118},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5126,\"start\":5122},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5130,\"start\":5126},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7888,\"start\":7885},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7890,\"start\":7888},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7893,\"start\":7890},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7953,\"start\":7950},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8040,\"start\":8036},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8105,\"start\":8101},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8134,\"start\":8130},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8201,\"start\":8197},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8204,\"start\":8201},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8424,\"start\":8420},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8427,\"start\":8424},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8430,\"start\":8427},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8666,\"start\":8662},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8669,\"start\":8666},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8713,\"start\":8709},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9086,\"start\":9082},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9232,\"start\":9228},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9235,\"start\":9232},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9238,\"start\":9235},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9280,\"start\":9276},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9432,\"start\":9428},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9635,\"start\":9631},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9638,\"start\":9635},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10251,\"start\":10247},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10272,\"start\":10268},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10576,\"start\":10572},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10580,\"start\":10576},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10584,\"start\":10580},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10588,\"start\":10584},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11170,\"start\":11166},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11174,\"start\":11170},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11178,\"start\":11174},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11181,\"start\":11178},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11612,\"start\":11608},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12510,\"start\":12506},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12513,\"start\":12510},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12987,\"start\":12983},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12990,\"start\":12987},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12993,\"start\":12990},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13633,\"start\":13629},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13951,\"start\":13947},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13954,\"start\":13951},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15878,\"start\":15874},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15881,\"start\":15878},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15884,\"start\":15881},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15907,\"start\":15903},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15910,\"start\":15907},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15913,\"start\":15910},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":15916,\"start\":15913},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17238,\"start\":17234},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17255,\"start\":17251},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":17277,\"start\":17273},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17352,\"start\":17348},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":18139,\"start\":18135},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":18143,\"start\":18139},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":18147,\"start\":18143},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":18151,\"start\":18147},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":18155,\"start\":18151},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":18159,\"start\":18155},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":18174,\"start\":18170},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":18177,\"start\":18174},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":18180,\"start\":18177},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18194,\"start\":18191},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18196,\"start\":18194},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":18199,\"start\":18196},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":18202,\"start\":18199},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":18617,\"start\":18613},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":18639,\"start\":18635},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18673,\"start\":18669},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":18798,\"start\":18794},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19098,\"start\":19094},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21367,\"start\":21363},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":21395,\"start\":21391},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21987,\"start\":21983},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22314,\"start\":22310},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":22317,\"start\":22314},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23566,\"start\":23563},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23568,\"start\":23566},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23612,\"start\":23608},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23715,\"start\":23711},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23808,\"start\":23804},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23812,\"start\":23808},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":23816,\"start\":23812},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":23820,\"start\":23816},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":23877,\"start\":23873},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":24075,\"start\":24071},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":24231,\"start\":24227},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24235,\"start\":24231},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":24239,\"start\":24235},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":24243,\"start\":24239},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":24836,\"start\":24832},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":25165,\"start\":25161},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":27109,\"start\":27105},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":27206,\"start\":27202},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27391,\"start\":27387},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27419,\"start\":27415},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27544,\"start\":27540},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28170,\"start\":28166},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":28316,\"start\":28312},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":29434,\"start\":29430}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":31621,\"start\":31177},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31779,\"start\":31622},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32142,\"start\":31780},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32243,\"start\":32143},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32288,\"start\":32244},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32889,\"start\":32289},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33662,\"start\":32890},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34342,\"start\":33663},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34825,\"start\":34343},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35321,\"start\":34826},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":36772,\"start\":35322},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":37595,\"start\":36773},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":38726,\"start\":37596},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":39787,\"start\":38727},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":40676,\"start\":39788},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":41069,\"start\":40677},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":41258,\"start\":41070},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":41576,\"start\":41259},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":42119,\"start\":41577},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":42161,\"start\":42120},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":42477,\"start\":42162},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":42838,\"start\":42478},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":43087,\"start\":42839},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":43218,\"start\":43088}]", "paragraph": "[{\"end\":2337,\"start\":2331},{\"end\":2727,\"start\":2339},{\"end\":3748,\"start\":2744},{\"end\":4676,\"start\":3750},{\"end\":5558,\"start\":4678},{\"end\":6545,\"start\":5560},{\"end\":6588,\"start\":6547},{\"end\":7730,\"start\":6590},{\"end\":8492,\"start\":7747},{\"end\":9875,\"start\":8494},{\"end\":10441,\"start\":9877},{\"end\":10949,\"start\":10443},{\"end\":12179,\"start\":10967},{\"end\":14080,\"start\":12181},{\"end\":14883,\"start\":14082},{\"end\":15721,\"start\":15031},{\"end\":16441,\"start\":15804},{\"end\":17785,\"start\":16443},{\"end\":17874,\"start\":17787},{\"end\":18888,\"start\":17890},{\"end\":19568,\"start\":18890},{\"end\":21100,\"start\":19570},{\"end\":22547,\"start\":21170},{\"end\":23523,\"start\":22594},{\"end\":24161,\"start\":23525},{\"end\":24837,\"start\":24163},{\"end\":26051,\"start\":24839},{\"end\":27026,\"start\":26066},{\"end\":27918,\"start\":27061},{\"end\":29232,\"start\":27975},{\"end\":29985,\"start\":29278},{\"end\":30811,\"start\":30019},{\"end\":31176,\"start\":30813},{\"end\":31620,\"start\":31191},{\"end\":31778,\"start\":31625},{\"end\":32141,\"start\":31792},{\"end\":32242,\"start\":32157},{\"end\":32287,\"start\":32251},{\"end\":32624,\"start\":32302},{\"end\":33448,\"start\":32903},{\"end\":33978,\"start\":33676},{\"end\":34824,\"start\":34354},{\"end\":34984,\"start\":34839},{\"end\":35533,\"start\":35335},{\"end\":36771,\"start\":35662},{\"end\":37594,\"start\":36786},{\"end\":37950,\"start\":37609},{\"end\":39154,\"start\":38740},{\"end\":40543,\"start\":39801},{\"end\":40821,\"start\":40690},{\"end\":41257,\"start\":41083},{\"end\":41438,\"start\":41274},{\"end\":41970,\"start\":41592},{\"end\":42160,\"start\":42135},{\"end\":42299,\"start\":42177},{\"end\":42763,\"start\":42493},{\"end\":42964,\"start\":42854},{\"end\":43143,\"start\":43103}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14951,\"start\":14884},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15030,\"start\":14951}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17873,\"start\":17872},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19644,\"start\":19643},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21519,\"start\":21518},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21678,\"start\":21677},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22095,\"start\":22094},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22234,\"start\":22233},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":24441,\"start\":24440},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":25302,\"start\":25301},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27242,\"start\":27241},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27266,\"start\":27265},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27884,\"start\":27883},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":27917,\"start\":27916},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":27995,\"start\":27993},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":28128,\"start\":28126},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":28411,\"start\":28409},{\"attributes\":{\"ref_id\":\"tab_17\"},\"end\":29815,\"start\":29813},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":30824,\"start\":30822}]", "section_header": "[{\"end\":2329,\"start\":2318},{\"attributes\":{\"n\":\"1\"},\"end\":2742,\"start\":2730},{\"attributes\":{\"n\":\"2\"},\"end\":7745,\"start\":7733},{\"attributes\":{\"n\":\"3.1\"},\"end\":10965,\"start\":10952},{\"attributes\":{\"n\":\"3.3\"},\"end\":15802,\"start\":15724},{\"attributes\":{\"n\":\"4\"},\"end\":17888,\"start\":17877},{\"attributes\":{\"n\":\"4.2\"},\"end\":21168,\"start\":21103},{\"attributes\":{\"n\":\"4.3\"},\"end\":22592,\"start\":22550},{\"attributes\":{\"n\":\"5\"},\"end\":26064,\"start\":26054},{\"end\":27059,\"start\":27029},{\"end\":27973,\"start\":27921},{\"end\":29276,\"start\":29235},{\"end\":30017,\"start\":29988},{\"end\":31188,\"start\":31178},{\"end\":31789,\"start\":31781},{\"end\":32154,\"start\":32144},{\"end\":32248,\"start\":32245},{\"end\":32299,\"start\":32290},{\"end\":32900,\"start\":32891},{\"end\":33673,\"start\":33664},{\"end\":34351,\"start\":34344},{\"end\":34836,\"start\":34827},{\"end\":35332,\"start\":35323},{\"end\":36783,\"start\":36774},{\"end\":37606,\"start\":37597},{\"end\":38737,\"start\":38728},{\"end\":39798,\"start\":39789},{\"end\":40687,\"start\":40678},{\"end\":41079,\"start\":41071},{\"end\":41270,\"start\":41260},{\"end\":41588,\"start\":41578},{\"end\":42131,\"start\":42121},{\"end\":42173,\"start\":42163},{\"end\":42489,\"start\":42479},{\"end\":42850,\"start\":42840},{\"end\":43099,\"start\":43089}]", "table": "[{\"end\":32889,\"start\":32625},{\"end\":33662,\"start\":33449},{\"end\":34342,\"start\":33979},{\"end\":35321,\"start\":34985},{\"end\":35661,\"start\":35534},{\"end\":38726,\"start\":37951},{\"end\":39787,\"start\":39155},{\"end\":40676,\"start\":40544},{\"end\":41069,\"start\":40822},{\"end\":41576,\"start\":41439},{\"end\":42119,\"start\":41971},{\"end\":42477,\"start\":42300},{\"end\":42838,\"start\":42764},{\"end\":43087,\"start\":42965},{\"end\":43218,\"start\":43144}]", "figure_caption": "[{\"end\":31621,\"start\":31190},{\"end\":31779,\"start\":31624},{\"end\":32142,\"start\":31791},{\"end\":32243,\"start\":32156},{\"end\":32288,\"start\":32250},{\"end\":32625,\"start\":32301},{\"end\":33449,\"start\":32902},{\"end\":33979,\"start\":33675},{\"end\":34985,\"start\":34838},{\"end\":35534,\"start\":35334},{\"end\":37595,\"start\":36785},{\"end\":37951,\"start\":37608},{\"end\":39155,\"start\":38739},{\"end\":40544,\"start\":39800},{\"end\":40822,\"start\":40689},{\"end\":41258,\"start\":41082},{\"end\":41439,\"start\":41273},{\"end\":41971,\"start\":41591},{\"end\":42161,\"start\":42134},{\"end\":42300,\"start\":42176},{\"end\":42764,\"start\":42492},{\"end\":42965,\"start\":42853},{\"end\":43144,\"start\":43102}]", "figure_ref": "[{\"end\":2347,\"start\":2346},{\"end\":5768,\"start\":5767},{\"end\":15710,\"start\":15709},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16961,\"start\":16959},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17784,\"start\":17782}]", "bib_author_first_name": "[{\"end\":45503,\"start\":45500},{\"end\":45519,\"start\":45511},{\"end\":45530,\"start\":45526},{\"end\":45545,\"start\":45538},{\"end\":45560,\"start\":45555},{\"end\":45562,\"start\":45561},{\"end\":45579,\"start\":45571},{\"end\":45596,\"start\":45590},{\"end\":45616,\"start\":45610},{\"end\":45630,\"start\":45624},{\"end\":45645,\"start\":45639},{\"end\":45673,\"start\":45664},{\"end\":45691,\"start\":45685},{\"end\":45705,\"start\":45700},{\"end\":45721,\"start\":45714},{\"end\":45735,\"start\":45729},{\"end\":45748,\"start\":45744},{\"end\":45762,\"start\":45758},{\"end\":45776,\"start\":45771},{\"end\":45780,\"start\":45777},{\"end\":45795,\"start\":45788},{\"end\":45813,\"start\":45804},{\"end\":45830,\"start\":45824},{\"end\":45844,\"start\":45838},{\"end\":45855,\"start\":45850},{\"end\":45877,\"start\":45871},{\"end\":45894,\"start\":45886},{\"end\":45906,\"start\":45900},{\"end\":45917,\"start\":45915},{\"end\":45927,\"start\":45923},{\"end\":45942,\"start\":45937},{\"end\":45970,\"start\":45967},{\"end\":45980,\"start\":45977},{\"end\":45991,\"start\":45985},{\"end\":46009,\"start\":46004},{\"end\":46021,\"start\":46016},{\"end\":46039,\"start\":46032},{\"end\":46051,\"start\":46048},{\"end\":46068,\"start\":46059},{\"end\":46082,\"start\":46078},{\"end\":46094,\"start\":46088},{\"end\":46107,\"start\":46101},{\"end\":46124,\"start\":46118},{\"end\":46141,\"start\":46135},{\"end\":46153,\"start\":46147},{\"end\":46173,\"start\":46167},{\"end\":46187,\"start\":46182},{\"end\":46199,\"start\":46195},{\"end\":46215,\"start\":46210},{\"end\":46229,\"start\":46223},{\"end\":46241,\"start\":46236},{\"end\":46261,\"start\":46252},{\"end\":46274,\"start\":46268},{\"end\":46289,\"start\":46280},{\"end\":46300,\"start\":46296},{\"end\":46736,\"start\":46732},{\"end\":46754,\"start\":46747},{\"end\":46770,\"start\":46764},{\"end\":46784,\"start\":46781},{\"end\":46800,\"start\":46792},{\"end\":46810,\"start\":46807},{\"end\":46820,\"start\":46816},{\"end\":46835,\"start\":46831},{\"end\":46855,\"start\":46849},{\"end\":46868,\"start\":46863},{\"end\":46881,\"start\":46878},{\"end\":46897,\"start\":46890},{\"end\":46915,\"start\":46906},{\"end\":46928,\"start\":46925},{\"end\":46940,\"start\":46936},{\"end\":46953,\"start\":46947},{\"end\":47095,\"start\":47090},{\"end\":47110,\"start\":47103},{\"end\":47124,\"start\":47119},{\"end\":47137,\"start\":47132},{\"end\":47151,\"start\":47147},{\"end\":47165,\"start\":47158},{\"end\":47183,\"start\":47172},{\"end\":47195,\"start\":47191},{\"end\":47206,\"start\":47202},{\"end\":47213,\"start\":47211},{\"end\":47311,\"start\":47304},{\"end\":47333,\"start\":47326},{\"end\":47348,\"start\":47342},{\"end\":47368,\"start\":47358},{\"end\":47387,\"start\":47379},{\"end\":47405,\"start\":47397},{\"end\":47420,\"start\":47415},{\"end\":47434,\"start\":47430},{\"end\":47448,\"start\":47442},{\"end\":47465,\"start\":47457},{\"end\":47479,\"start\":47473},{\"end\":47653,\"start\":47649},{\"end\":47668,\"start\":47663},{\"end\":47682,\"start\":47677},{\"end\":47695,\"start\":47690},{\"end\":47709,\"start\":47704},{\"end\":47728,\"start\":47721},{\"end\":47744,\"start\":47737},{\"end\":47762,\"start\":47756},{\"end\":47778,\"start\":47770},{\"end\":47795,\"start\":47789},{\"end\":47889,\"start\":47884},{\"end\":47904,\"start\":47900},{\"end\":47920,\"start\":47913},{\"end\":47931,\"start\":47925},{\"end\":47945,\"start\":47941},{\"end\":47959,\"start\":47952},{\"end\":47970,\"start\":47966},{\"end\":47985,\"start\":47980},{\"end\":47998,\"start\":47994},{\"end\":48000,\"start\":47999},{\"end\":48065,\"start\":48064},{\"end\":48079,\"start\":48078},{\"end\":48090,\"start\":48089},{\"end\":48101,\"start\":48100},{\"end\":48103,\"start\":48102},{\"end\":48113,\"start\":48112},{\"end\":48220,\"start\":48213},{\"end\":48237,\"start\":48230},{\"end\":48368,\"start\":48363},{\"end\":48379,\"start\":48374},{\"end\":48388,\"start\":48385},{\"end\":48404,\"start\":48399},{\"end\":48418,\"start\":48414},{\"end\":48434,\"start\":48426},{\"end\":48451,\"start\":48447},{\"end\":48784,\"start\":48780},{\"end\":48800,\"start\":48793},{\"end\":48807,\"start\":48805},{\"end\":48820,\"start\":48815},{\"end\":48837,\"start\":48830},{\"end\":48856,\"start\":48850},{\"end\":48871,\"start\":48866},{\"end\":48887,\"start\":48879},{\"end\":48905,\"start\":48897},{\"end\":48917,\"start\":48913},{\"end\":48988,\"start\":48982},{\"end\":49000,\"start\":48994},{\"end\":49019,\"start\":49011},{\"end\":49033,\"start\":49027},{\"end\":49049,\"start\":49042},{\"end\":49063,\"start\":49059},{\"end\":49075,\"start\":49071},{\"end\":49086,\"start\":49082},{\"end\":49101,\"start\":49095},{\"end\":49121,\"start\":49114},{\"end\":49273,\"start\":49268},{\"end\":49286,\"start\":49279},{\"end\":49301,\"start\":49294},{\"end\":49314,\"start\":49308},{\"end\":49325,\"start\":49320},{\"end\":49329,\"start\":49326},{\"end\":49339,\"start\":49334},{\"end\":49351,\"start\":49348},{\"end\":49362,\"start\":49356},{\"end\":49364,\"start\":49363},{\"end\":49376,\"start\":49370},{\"end\":49514,\"start\":49508},{\"end\":49527,\"start\":49521},{\"end\":49541,\"start\":49536},{\"end\":49557,\"start\":49550},{\"end\":49571,\"start\":49564},{\"end\":49586,\"start\":49582},{\"end\":49604,\"start\":49597},{\"end\":49620,\"start\":49614},{\"end\":49635,\"start\":49631},{\"end\":49647,\"start\":49642},{\"end\":49659,\"start\":49653},{\"end\":49679,\"start\":49673},{\"end\":49690,\"start\":49684},{\"end\":49705,\"start\":49700},{\"end\":49728,\"start\":49721},{\"end\":49746,\"start\":49740},{\"end\":49757,\"start\":49752},{\"end\":49778,\"start\":49769},{\"end\":49794,\"start\":49786},{\"end\":49806,\"start\":49802},{\"end\":49828,\"start\":49825},{\"end\":49842,\"start\":49836},{\"end\":49854,\"start\":49849},{\"end\":49872,\"start\":49863},{\"end\":49886,\"start\":49879},{\"end\":49899,\"start\":49893},{\"end\":49914,\"start\":49908},{\"end\":49931,\"start\":49923},{\"end\":49941,\"start\":49938},{\"end\":49958,\"start\":49950},{\"end\":49972,\"start\":49966},{\"end\":49989,\"start\":49981},{\"end\":50005,\"start\":50000},{\"end\":50017,\"start\":50013},{\"end\":50029,\"start\":50025},{\"end\":50227,\"start\":50221},{\"end\":50242,\"start\":50237},{\"end\":50264,\"start\":50256},{\"end\":50632,\"start\":50625},{\"end\":50642,\"start\":50638},{\"end\":50648,\"start\":50643},{\"end\":50657,\"start\":50653},{\"end\":50668,\"start\":50663},{\"end\":50683,\"start\":50676},{\"end\":50702,\"start\":50693},{\"end\":50724,\"start\":50713},{\"end\":50733,\"start\":50729},{\"end\":50750,\"start\":50741},{\"end\":50757,\"start\":50756},{\"end\":50859,\"start\":50857},{\"end\":50870,\"start\":50865},{\"end\":50881,\"start\":50876},{\"end\":50885,\"start\":50882},{\"end\":50898,\"start\":50893},{\"end\":50900,\"start\":50899},{\"end\":50920,\"start\":50914},{\"end\":50931,\"start\":50925},{\"end\":50948,\"start\":50941},{\"end\":50964,\"start\":50957},{\"end\":50988,\"start\":50979},{\"end\":51142,\"start\":51135},{\"end\":51156,\"start\":51150},{\"end\":51167,\"start\":51163},{\"end\":51226,\"start\":51222},{\"end\":51237,\"start\":51232},{\"end\":51254,\"start\":51245},{\"end\":51263,\"start\":51259},{\"end\":51275,\"start\":51270},{\"end\":51288,\"start\":51282},{\"end\":51298,\"start\":51295},{\"end\":51382,\"start\":51378},{\"end\":51398,\"start\":51392},{\"end\":51417,\"start\":51408},{\"end\":51436,\"start\":51431},{\"end\":51453,\"start\":51446},{\"end\":51476,\"start\":51470},{\"end\":51491,\"start\":51487},{\"end\":51510,\"start\":51503},{\"end\":51596,\"start\":51588},{\"end\":51614,\"start\":51608},{\"end\":51758,\"start\":51757},{\"end\":51774,\"start\":51767},{\"end\":51785,\"start\":51779},{\"end\":51801,\"start\":51794},{\"end\":51818,\"start\":51813},{\"end\":51825,\"start\":51823},{\"end\":51838,\"start\":51832},{\"end\":52005,\"start\":51999},{\"end\":52021,\"start\":52012},{\"end\":52035,\"start\":52027},{\"end\":52047,\"start\":52041},{\"end\":52061,\"start\":52053},{\"end\":52075,\"start\":52067},{\"end\":52086,\"start\":52082},{\"end\":52089,\"start\":52087},{\"end\":52105,\"start\":52097},{\"end\":52222,\"start\":52221},{\"end\":52238,\"start\":52231},{\"end\":52240,\"start\":52239},{\"end\":52255,\"start\":52248},{\"end\":52279,\"start\":52267},{\"end\":52299,\"start\":52289},{\"end\":52301,\"start\":52300},{\"end\":52473,\"start\":52465},{\"end\":52484,\"start\":52480},{\"end\":52495,\"start\":52491},{\"end\":52506,\"start\":52501},{\"end\":52515,\"start\":52512},{\"end\":52674,\"start\":52670},{\"end\":52684,\"start\":52681},{\"end\":52692,\"start\":52687},{\"end\":52702,\"start\":52698},{\"end\":52705,\"start\":52703},{\"end\":52718,\"start\":52713},{\"end\":52734,\"start\":52731},{\"end\":52749,\"start\":52745},{\"end\":52763,\"start\":52757},{\"end\":52777,\"start\":52773},{\"end\":52987,\"start\":52981},{\"end\":52997,\"start\":52993},{\"end\":53023,\"start\":53017},{\"end\":53038,\"start\":53031},{\"end\":53051,\"start\":53043},{\"end\":53063,\"start\":53056},{\"end\":53178,\"start\":53173},{\"end\":53195,\"start\":53188},{\"end\":53215,\"start\":53212},{\"end\":53407,\"start\":53398},{\"end\":53416,\"start\":53414},{\"end\":53429,\"start\":53422},{\"end\":53444,\"start\":53438},{\"end\":53458,\"start\":53454},{\"end\":53554,\"start\":53548},{\"end\":53574,\"start\":53565},{\"end\":53591,\"start\":53585},{\"end\":53605,\"start\":53600},{\"end\":53625,\"start\":53619},{\"end\":53636,\"start\":53631},{\"end\":53654,\"start\":53649},{\"end\":53667,\"start\":53663},{\"end\":53672,\"start\":53668},{\"end\":53688,\"start\":53680},{\"end\":53705,\"start\":53700},{\"end\":53823,\"start\":53819},{\"end\":53836,\"start\":53832},{\"end\":53856,\"start\":53849},{\"end\":53871,\"start\":53864},{\"end\":53887,\"start\":53882},{\"end\":53900,\"start\":53896},{\"end\":53920,\"start\":53914},{\"end\":53937,\"start\":53931},{\"end\":54057,\"start\":54048},{\"end\":54071,\"start\":54066},{\"end\":54086,\"start\":54080},{\"end\":54099,\"start\":54095},{\"end\":54108,\"start\":54104},{\"end\":54126,\"start\":54117},{\"end\":54143,\"start\":54132},{\"end\":54159,\"start\":54151},{\"end\":54172,\"start\":54166},{\"end\":54190,\"start\":54183},{\"end\":54203,\"start\":54201},{\"end\":54215,\"start\":54211},{\"end\":54227,\"start\":54223},{\"end\":54246,\"start\":54238},{\"end\":54261,\"start\":54256},{\"end\":54277,\"start\":54270},{\"end\":54294,\"start\":54286},{\"end\":54306,\"start\":54302},{\"end\":54423,\"start\":54416},{\"end\":54437,\"start\":54430},{\"end\":54452,\"start\":54445},{\"end\":54466,\"start\":54461},{\"end\":54476,\"start\":54472},{\"end\":54478,\"start\":54477},{\"end\":54492,\"start\":54486},{\"end\":54511,\"start\":54503},{\"end\":54637,\"start\":54630},{\"end\":54653,\"start\":54646},{\"end\":54660,\"start\":54658},{\"end\":54670,\"start\":54666},{\"end\":54686,\"start\":54678},{\"end\":54694,\"start\":54691},{\"end\":54709,\"start\":54702},{\"end\":54723,\"start\":54717},{\"end\":54739,\"start\":54732},{\"end\":54754,\"start\":54748},{\"end\":54756,\"start\":54755},{\"end\":54770,\"start\":54767},{\"end\":54783,\"start\":54779},{\"end\":54785,\"start\":54784},{\"end\":54891,\"start\":54886},{\"end\":54908,\"start\":54900},{\"end\":54922,\"start\":54916},{\"end\":54936,\"start\":54928},{\"end\":55041,\"start\":55037},{\"end\":55058,\"start\":55051},{\"end\":55068,\"start\":55063},{\"end\":55081,\"start\":55076},{\"end\":55093,\"start\":55088},{\"end\":55106,\"start\":55102},{\"end\":55236,\"start\":55232},{\"end\":55252,\"start\":55245},{\"end\":55363,\"start\":55359},{\"end\":55376,\"start\":55371},{\"end\":55858,\"start\":55853},{\"end\":55871,\"start\":55867},{\"end\":55885,\"start\":55881},{\"end\":56253,\"start\":56246},{\"end\":56266,\"start\":56258},{\"end\":56279,\"start\":56273},{\"end\":56290,\"start\":56284},{\"end\":56315,\"start\":56309},{\"end\":56466,\"start\":56460},{\"end\":56479,\"start\":56472},{\"end\":56492,\"start\":56489},{\"end\":56503,\"start\":56498},{\"end\":56518,\"start\":56515},{\"end\":56529,\"start\":56523},{\"end\":56540,\"start\":56535},{\"end\":56552,\"start\":56546},{\"end\":56881,\"start\":56874},{\"end\":56896,\"start\":56888},{\"end\":56908,\"start\":56902},{\"end\":56923,\"start\":56916},{\"end\":57006,\"start\":57002},{\"end\":57025,\"start\":57017},{\"end\":57039,\"start\":57036},{\"end\":57255,\"start\":57253},{\"end\":57276,\"start\":57267},{\"end\":57289,\"start\":57282},{\"end\":57298,\"start\":57295},{\"end\":57312,\"start\":57305},{\"end\":57326,\"start\":57318},{\"end\":57342,\"start\":57332},{\"end\":57357,\"start\":57349},{\"end\":57367,\"start\":57363},{\"end\":57379,\"start\":57372},{\"end\":57466,\"start\":57462},{\"end\":57485,\"start\":57477},{\"end\":57499,\"start\":57496},{\"end\":57703,\"start\":57699},{\"end\":57713,\"start\":57710},{\"end\":57725,\"start\":57716},{\"end\":57733,\"start\":57731},{\"end\":57752,\"start\":57745},{\"end\":58050,\"start\":58046},{\"end\":58070,\"start\":58061},{\"end\":58084,\"start\":58076},{\"end\":58101,\"start\":58091},{\"end\":58109,\"start\":58107},{\"end\":58128,\"start\":58121},{\"end\":58176,\"start\":58170},{\"end\":58196,\"start\":58189},{\"end\":58405,\"start\":58403},{\"end\":58418,\"start\":58411},{\"end\":58432,\"start\":58425},{\"end\":58444,\"start\":58439},{\"end\":58457,\"start\":58451},{\"end\":58468,\"start\":58464},{\"end\":58609,\"start\":58600},{\"end\":58618,\"start\":58616},{\"end\":58631,\"start\":58624},{\"end\":58643,\"start\":58640},{\"end\":58654,\"start\":58648},{\"end\":58668,\"start\":58664},{\"end\":58931,\"start\":58926},{\"end\":58946,\"start\":58938},{\"end\":58959,\"start\":58953},{\"end\":58972,\"start\":58965},{\"end\":58987,\"start\":58978},{\"end\":59001,\"start\":58993},{\"end\":59010,\"start\":59008},{\"end\":59032,\"start\":59022},{\"end\":59046,\"start\":59038},{\"end\":59059,\"start\":59052},{\"end\":59074,\"start\":59071},{\"end\":59093,\"start\":59085},{\"end\":59106,\"start\":59103},{\"end\":59121,\"start\":59117},{\"end\":59258,\"start\":59251},{\"end\":59274,\"start\":59267},{\"end\":59287,\"start\":59282},{\"end\":59305,\"start\":59298},{\"end\":59405,\"start\":59400},{\"end\":59418,\"start\":59413},{\"end\":59431,\"start\":59427},{\"end\":59447,\"start\":59441},{\"end\":59460,\"start\":59454},{\"end\":59479,\"start\":59472},{\"end\":59497,\"start\":59491},{\"end\":59608,\"start\":59601},{\"end\":59620,\"start\":59615},{\"end\":59638,\"start\":59630},{\"end\":59649,\"start\":59644},{\"end\":59833,\"start\":59828},{\"end\":59846,\"start\":59843},{\"end\":59864,\"start\":59857},{\"end\":59874,\"start\":59871},{\"end\":59889,\"start\":59884},{\"end\":60157,\"start\":60152},{\"end\":60173,\"start\":60168},{\"end\":60187,\"start\":60181},{\"end\":60200,\"start\":60194},{\"end\":60228,\"start\":60225},{\"end\":60246,\"start\":60240},{\"end\":60260,\"start\":60254},{\"end\":60273,\"start\":60269},{\"end\":60285,\"start\":60279},{\"end\":60299,\"start\":60295},{\"end\":60311,\"start\":60306},{\"end\":60494,\"start\":60487},{\"end\":60509,\"start\":60503},{\"end\":60526,\"start\":60520},{\"end\":60542,\"start\":60534},{\"end\":60639,\"start\":60631},{\"end\":60641,\"start\":60640},{\"end\":60658,\"start\":60650},{\"end\":60674,\"start\":60670},{\"end\":60678,\"start\":60675},{\"end\":60790,\"start\":60785},{\"end\":60804,\"start\":60798},{\"end\":60822,\"start\":60816},{\"end\":60834,\"start\":60830},{\"end\":60850,\"start\":60843},{\"end\":60861,\"start\":60855},{\"end\":60877,\"start\":60872},{\"end\":60894,\"start\":60885},{\"end\":60896,\"start\":60895},{\"end\":60993,\"start\":60990},{\"end\":61004,\"start\":61001},{\"end\":61014,\"start\":61010},{\"end\":61027,\"start\":61021},{\"end\":61041,\"start\":61035},{\"end\":61235,\"start\":61229},{\"end\":61250,\"start\":61243},{\"end\":61261,\"start\":61256},{\"end\":61276,\"start\":61268},{\"end\":61287,\"start\":61281},{\"end\":61296,\"start\":61293},{\"end\":61310,\"start\":61301},{\"end\":61319,\"start\":61315},{\"end\":61327,\"start\":61325},{\"end\":61447,\"start\":61444},{\"end\":61464,\"start\":61458},{\"end\":61478,\"start\":61473},{\"end\":61488,\"start\":61483},{\"end\":61503,\"start\":61498},{\"end\":61641,\"start\":61633},{\"end\":61653,\"start\":61649},{\"end\":61679,\"start\":61674},{\"end\":61681,\"start\":61680},{\"end\":61693,\"start\":61690},{\"end\":61704,\"start\":61703},{\"end\":61708,\"start\":61705},{\"end\":61722,\"start\":61717},{\"end\":61846,\"start\":61843},{\"end\":61860,\"start\":61852},{\"end\":61872,\"start\":61866},{\"end\":61886,\"start\":61883},{\"end\":61901,\"start\":61894},{\"end\":61917,\"start\":61910},{\"end\":61934,\"start\":61926},{\"end\":61951,\"start\":61944},{\"end\":61961,\"start\":61957},{\"end\":61978,\"start\":61972},{\"end\":61997,\"start\":61992},{\"end\":62010,\"start\":62005},{\"end\":62025,\"start\":62021},{\"end\":62037,\"start\":62032},{\"end\":62048,\"start\":62045},{\"end\":62060,\"start\":62055},{\"end\":62071,\"start\":62067},{\"end\":62158,\"start\":62154},{\"end\":62176,\"start\":62171},{\"end\":62414,\"start\":62410},{\"end\":62429,\"start\":62423},{\"end\":62451,\"start\":62443},{\"end\":62467,\"start\":62460},{\"end\":62596,\"start\":62590},{\"end\":62607,\"start\":62604},{\"end\":62620,\"start\":62613},{\"end\":62628,\"start\":62627},{\"end\":62635,\"start\":62629},{\"end\":62747,\"start\":62741},{\"end\":62762,\"start\":62754},{\"end\":62776,\"start\":62770},{\"end\":62789,\"start\":62783},{\"end\":62807,\"start\":62800},{\"end\":62825,\"start\":62818},{\"end\":62838,\"start\":62831},{\"end\":62850,\"start\":62847},{\"end\":62862,\"start\":62858},{\"end\":62875,\"start\":62869},{\"end\":62890,\"start\":62887},{\"end\":62903,\"start\":62900},{\"end\":62919,\"start\":62914},{\"end\":62946,\"start\":62940},{\"end\":62957,\"start\":62951},{\"end\":62973,\"start\":62967},{\"end\":62984,\"start\":62979},{\"end\":62996,\"start\":62989},{\"end\":63013,\"start\":63006},{\"end\":63029,\"start\":63022},{\"end\":63046,\"start\":63037}]", "bib_author_last_name": "[{\"end\":45509,\"start\":45504},{\"end\":45524,\"start\":45520},{\"end\":45536,\"start\":45531},{\"end\":45553,\"start\":45546},{\"end\":45569,\"start\":45563},{\"end\":45588,\"start\":45580},{\"end\":45608,\"start\":45597},{\"end\":45622,\"start\":45617},{\"end\":45637,\"start\":45631},{\"end\":45652,\"start\":45646},{\"end\":45683,\"start\":45674},{\"end\":45698,\"start\":45692},{\"end\":45712,\"start\":45706},{\"end\":45727,\"start\":45722},{\"end\":45742,\"start\":45736},{\"end\":45756,\"start\":45749},{\"end\":45769,\"start\":45763},{\"end\":45786,\"start\":45781},{\"end\":45802,\"start\":45796},{\"end\":45822,\"start\":45814},{\"end\":45836,\"start\":45831},{\"end\":45848,\"start\":45845},{\"end\":45869,\"start\":45856},{\"end\":45884,\"start\":45878},{\"end\":45898,\"start\":45895},{\"end\":45913,\"start\":45907},{\"end\":45921,\"start\":45918},{\"end\":45935,\"start\":45928},{\"end\":45965,\"start\":45943},{\"end\":45975,\"start\":45971},{\"end\":45983,\"start\":45981},{\"end\":46002,\"start\":45992},{\"end\":46014,\"start\":46010},{\"end\":46030,\"start\":46022},{\"end\":46046,\"start\":46040},{\"end\":46057,\"start\":46052},{\"end\":46076,\"start\":46069},{\"end\":46086,\"start\":46083},{\"end\":46099,\"start\":46095},{\"end\":46116,\"start\":46108},{\"end\":46133,\"start\":46125},{\"end\":46145,\"start\":46142},{\"end\":46165,\"start\":46154},{\"end\":46180,\"start\":46174},{\"end\":46193,\"start\":46188},{\"end\":46208,\"start\":46200},{\"end\":46221,\"start\":46216},{\"end\":46234,\"start\":46230},{\"end\":46250,\"start\":46242},{\"end\":46266,\"start\":46262},{\"end\":46278,\"start\":46275},{\"end\":46294,\"start\":46290},{\"end\":46311,\"start\":46301},{\"end\":46320,\"start\":46313},{\"end\":46745,\"start\":46737},{\"end\":46762,\"start\":46755},{\"end\":46779,\"start\":46771},{\"end\":46790,\"start\":46785},{\"end\":46805,\"start\":46801},{\"end\":46814,\"start\":46811},{\"end\":46829,\"start\":46821},{\"end\":46847,\"start\":46836},{\"end\":46861,\"start\":46856},{\"end\":46876,\"start\":46869},{\"end\":46888,\"start\":46882},{\"end\":46904,\"start\":46898},{\"end\":46923,\"start\":46916},{\"end\":46934,\"start\":46929},{\"end\":46945,\"start\":46941},{\"end\":46962,\"start\":46954},{\"end\":46972,\"start\":46964},{\"end\":47101,\"start\":47096},{\"end\":47117,\"start\":47111},{\"end\":47130,\"start\":47125},{\"end\":47145,\"start\":47138},{\"end\":47156,\"start\":47152},{\"end\":47170,\"start\":47166},{\"end\":47189,\"start\":47184},{\"end\":47200,\"start\":47196},{\"end\":47209,\"start\":47207},{\"end\":47226,\"start\":47214},{\"end\":47324,\"start\":47312},{\"end\":47340,\"start\":47334},{\"end\":47356,\"start\":47349},{\"end\":47377,\"start\":47369},{\"end\":47395,\"start\":47388},{\"end\":47413,\"start\":47406},{\"end\":47428,\"start\":47421},{\"end\":47440,\"start\":47435},{\"end\":47455,\"start\":47449},{\"end\":47471,\"start\":47466},{\"end\":47489,\"start\":47480},{\"end\":47497,\"start\":47491},{\"end\":47661,\"start\":47654},{\"end\":47675,\"start\":47669},{\"end\":47688,\"start\":47683},{\"end\":47702,\"start\":47696},{\"end\":47719,\"start\":47710},{\"end\":47735,\"start\":47729},{\"end\":47754,\"start\":47745},{\"end\":47768,\"start\":47763},{\"end\":47787,\"start\":47779},{\"end\":47803,\"start\":47796},{\"end\":47898,\"start\":47890},{\"end\":47911,\"start\":47905},{\"end\":47923,\"start\":47921},{\"end\":47939,\"start\":47932},{\"end\":47950,\"start\":47946},{\"end\":47964,\"start\":47960},{\"end\":47978,\"start\":47971},{\"end\":47992,\"start\":47986},{\"end\":48011,\"start\":48001},{\"end\":48076,\"start\":48066},{\"end\":48087,\"start\":48080},{\"end\":48098,\"start\":48091},{\"end\":48110,\"start\":48104},{\"end\":48117,\"start\":48114},{\"end\":48228,\"start\":48221},{\"end\":48247,\"start\":48238},{\"end\":48372,\"start\":48369},{\"end\":48383,\"start\":48380},{\"end\":48397,\"start\":48389},{\"end\":48412,\"start\":48405},{\"end\":48424,\"start\":48419},{\"end\":48445,\"start\":48435},{\"end\":48463,\"start\":48452},{\"end\":48791,\"start\":48785},{\"end\":48803,\"start\":48801},{\"end\":48813,\"start\":48808},{\"end\":48828,\"start\":48821},{\"end\":48848,\"start\":48838},{\"end\":48864,\"start\":48857},{\"end\":48877,\"start\":48872},{\"end\":48895,\"start\":48888},{\"end\":48911,\"start\":48906},{\"end\":48921,\"start\":48918},{\"end\":48992,\"start\":48989},{\"end\":49009,\"start\":49001},{\"end\":49025,\"start\":49020},{\"end\":49040,\"start\":49034},{\"end\":49057,\"start\":49050},{\"end\":49069,\"start\":49064},{\"end\":49080,\"start\":49076},{\"end\":49093,\"start\":49087},{\"end\":49112,\"start\":49102},{\"end\":49130,\"start\":49122},{\"end\":49277,\"start\":49274},{\"end\":49292,\"start\":49287},{\"end\":49306,\"start\":49302},{\"end\":49318,\"start\":49315},{\"end\":49332,\"start\":49330},{\"end\":49346,\"start\":49340},{\"end\":49354,\"start\":49352},{\"end\":49368,\"start\":49365},{\"end\":49379,\"start\":49377},{\"end\":49519,\"start\":49515},{\"end\":49534,\"start\":49528},{\"end\":49548,\"start\":49542},{\"end\":49562,\"start\":49558},{\"end\":49580,\"start\":49572},{\"end\":49595,\"start\":49587},{\"end\":49612,\"start\":49605},{\"end\":49629,\"start\":49621},{\"end\":49640,\"start\":49636},{\"end\":49651,\"start\":49648},{\"end\":49671,\"start\":49660},{\"end\":49682,\"start\":49680},{\"end\":49698,\"start\":49691},{\"end\":49719,\"start\":49706},{\"end\":49738,\"start\":49729},{\"end\":49750,\"start\":49747},{\"end\":49767,\"start\":49758},{\"end\":49784,\"start\":49779},{\"end\":49800,\"start\":49795},{\"end\":49812,\"start\":49807},{\"end\":49823,\"start\":49814},{\"end\":49834,\"start\":49829},{\"end\":49847,\"start\":49843},{\"end\":49861,\"start\":49855},{\"end\":49877,\"start\":49873},{\"end\":49891,\"start\":49887},{\"end\":49906,\"start\":49900},{\"end\":49921,\"start\":49915},{\"end\":49936,\"start\":49932},{\"end\":49948,\"start\":49942},{\"end\":49964,\"start\":49959},{\"end\":49979,\"start\":49973},{\"end\":49998,\"start\":49990},{\"end\":50011,\"start\":50006},{\"end\":50023,\"start\":50018},{\"end\":50036,\"start\":50030},{\"end\":50235,\"start\":50228},{\"end\":50254,\"start\":50243},{\"end\":50272,\"start\":50265},{\"end\":50636,\"start\":50633},{\"end\":50651,\"start\":50649},{\"end\":50661,\"start\":50658},{\"end\":50674,\"start\":50669},{\"end\":50691,\"start\":50684},{\"end\":50711,\"start\":50703},{\"end\":50727,\"start\":50725},{\"end\":50739,\"start\":50734},{\"end\":50754,\"start\":50751},{\"end\":50762,\"start\":50758},{\"end\":50768,\"start\":50764},{\"end\":50863,\"start\":50860},{\"end\":50874,\"start\":50871},{\"end\":50891,\"start\":50886},{\"end\":50912,\"start\":50901},{\"end\":50923,\"start\":50921},{\"end\":50939,\"start\":50932},{\"end\":50955,\"start\":50949},{\"end\":50977,\"start\":50965},{\"end\":50992,\"start\":50989},{\"end\":51003,\"start\":50994},{\"end\":51148,\"start\":51143},{\"end\":51161,\"start\":51157},{\"end\":51175,\"start\":51168},{\"end\":51230,\"start\":51227},{\"end\":51243,\"start\":51238},{\"end\":51257,\"start\":51255},{\"end\":51268,\"start\":51264},{\"end\":51280,\"start\":51276},{\"end\":51293,\"start\":51289},{\"end\":51303,\"start\":51299},{\"end\":51390,\"start\":51383},{\"end\":51406,\"start\":51399},{\"end\":51429,\"start\":51418},{\"end\":51444,\"start\":51437},{\"end\":51468,\"start\":51454},{\"end\":51485,\"start\":51477},{\"end\":51501,\"start\":51492},{\"end\":51516,\"start\":51511},{\"end\":51606,\"start\":51597},{\"end\":51628,\"start\":51615},{\"end\":51765,\"start\":51759},{\"end\":51777,\"start\":51775},{\"end\":51792,\"start\":51786},{\"end\":51811,\"start\":51802},{\"end\":51821,\"start\":51819},{\"end\":51830,\"start\":51826},{\"end\":51843,\"start\":51839},{\"end\":51849,\"start\":51845},{\"end\":52010,\"start\":52006},{\"end\":52025,\"start\":52022},{\"end\":52039,\"start\":52036},{\"end\":52051,\"start\":52048},{\"end\":52065,\"start\":52062},{\"end\":52080,\"start\":52076},{\"end\":52095,\"start\":52090},{\"end\":52110,\"start\":52106},{\"end\":52229,\"start\":52223},{\"end\":52246,\"start\":52241},{\"end\":52265,\"start\":52256},{\"end\":52287,\"start\":52280},{\"end\":52311,\"start\":52302},{\"end\":52318,\"start\":52313},{\"end\":52478,\"start\":52474},{\"end\":52489,\"start\":52485},{\"end\":52499,\"start\":52496},{\"end\":52510,\"start\":52507},{\"end\":52521,\"start\":52516},{\"end\":52679,\"start\":52675},{\"end\":52696,\"start\":52693},{\"end\":52711,\"start\":52706},{\"end\":52723,\"start\":52719},{\"end\":52743,\"start\":52735},{\"end\":52755,\"start\":52750},{\"end\":52771,\"start\":52764},{\"end\":52789,\"start\":52778},{\"end\":52991,\"start\":52988},{\"end\":53015,\"start\":52998},{\"end\":53029,\"start\":53024},{\"end\":53041,\"start\":53039},{\"end\":53054,\"start\":53052},{\"end\":53066,\"start\":53064},{\"end\":53186,\"start\":53179},{\"end\":53210,\"start\":53196},{\"end\":53223,\"start\":53216},{\"end\":53233,\"start\":53225},{\"end\":53412,\"start\":53408},{\"end\":53420,\"start\":53417},{\"end\":53436,\"start\":53430},{\"end\":53452,\"start\":53445},{\"end\":53462,\"start\":53459},{\"end\":53563,\"start\":53555},{\"end\":53583,\"start\":53575},{\"end\":53598,\"start\":53592},{\"end\":53617,\"start\":53606},{\"end\":53629,\"start\":53626},{\"end\":53647,\"start\":53637},{\"end\":53661,\"start\":53655},{\"end\":53678,\"start\":53673},{\"end\":53698,\"start\":53689},{\"end\":53711,\"start\":53706},{\"end\":53718,\"start\":53713},{\"end\":53830,\"start\":53824},{\"end\":53847,\"start\":53837},{\"end\":53862,\"start\":53857},{\"end\":53880,\"start\":53872},{\"end\":53894,\"start\":53888},{\"end\":53912,\"start\":53901},{\"end\":53929,\"start\":53921},{\"end\":53945,\"start\":53938},{\"end\":54064,\"start\":54058},{\"end\":54078,\"start\":54072},{\"end\":54093,\"start\":54087},{\"end\":54102,\"start\":54100},{\"end\":54115,\"start\":54109},{\"end\":54130,\"start\":54127},{\"end\":54149,\"start\":54144},{\"end\":54164,\"start\":54160},{\"end\":54181,\"start\":54173},{\"end\":54199,\"start\":54191},{\"end\":54209,\"start\":54204},{\"end\":54221,\"start\":54216},{\"end\":54236,\"start\":54228},{\"end\":54254,\"start\":54247},{\"end\":54268,\"start\":54262},{\"end\":54284,\"start\":54278},{\"end\":54300,\"start\":54295},{\"end\":54315,\"start\":54307},{\"end\":54428,\"start\":54424},{\"end\":54443,\"start\":54438},{\"end\":54459,\"start\":54453},{\"end\":54470,\"start\":54467},{\"end\":54484,\"start\":54479},{\"end\":54501,\"start\":54493},{\"end\":54522,\"start\":54512},{\"end\":54644,\"start\":54638},{\"end\":54656,\"start\":54654},{\"end\":54664,\"start\":54661},{\"end\":54676,\"start\":54671},{\"end\":54689,\"start\":54687},{\"end\":54700,\"start\":54695},{\"end\":54715,\"start\":54710},{\"end\":54730,\"start\":54724},{\"end\":54746,\"start\":54740},{\"end\":54765,\"start\":54757},{\"end\":54777,\"start\":54771},{\"end\":54790,\"start\":54786},{\"end\":54898,\"start\":54892},{\"end\":54914,\"start\":54909},{\"end\":54926,\"start\":54923},{\"end\":54946,\"start\":54937},{\"end\":55049,\"start\":55042},{\"end\":55061,\"start\":55059},{\"end\":55074,\"start\":55069},{\"end\":55086,\"start\":55082},{\"end\":55100,\"start\":55094},{\"end\":55116,\"start\":55107},{\"end\":55243,\"start\":55237},{\"end\":55260,\"start\":55253},{\"end\":55369,\"start\":55364},{\"end\":55379,\"start\":55377},{\"end\":55386,\"start\":55381},{\"end\":55865,\"start\":55859},{\"end\":55879,\"start\":55872},{\"end\":55894,\"start\":55886},{\"end\":56256,\"start\":56254},{\"end\":56271,\"start\":56267},{\"end\":56282,\"start\":56280},{\"end\":56307,\"start\":56291},{\"end\":56322,\"start\":56316},{\"end\":56470,\"start\":56467},{\"end\":56487,\"start\":56480},{\"end\":56496,\"start\":56493},{\"end\":56513,\"start\":56504},{\"end\":56521,\"start\":56519},{\"end\":56533,\"start\":56530},{\"end\":56544,\"start\":56541},{\"end\":56559,\"start\":56553},{\"end\":56886,\"start\":56882},{\"end\":56900,\"start\":56897},{\"end\":56914,\"start\":56909},{\"end\":56929,\"start\":56924},{\"end\":57015,\"start\":57007},{\"end\":57034,\"start\":57026},{\"end\":57045,\"start\":57040},{\"end\":57265,\"start\":57256},{\"end\":57280,\"start\":57277},{\"end\":57293,\"start\":57290},{\"end\":57303,\"start\":57299},{\"end\":57316,\"start\":57313},{\"end\":57330,\"start\":57327},{\"end\":57347,\"start\":57343},{\"end\":57361,\"start\":57358},{\"end\":57370,\"start\":57368},{\"end\":57384,\"start\":57380},{\"end\":57389,\"start\":57386},{\"end\":57475,\"start\":57467},{\"end\":57494,\"start\":57486},{\"end\":57505,\"start\":57500},{\"end\":57708,\"start\":57704},{\"end\":57729,\"start\":57726},{\"end\":57743,\"start\":57734},{\"end\":57756,\"start\":57753},{\"end\":58059,\"start\":58051},{\"end\":58074,\"start\":58071},{\"end\":58089,\"start\":58085},{\"end\":58105,\"start\":58102},{\"end\":58119,\"start\":58110},{\"end\":58132,\"start\":58129},{\"end\":58187,\"start\":58177},{\"end\":58203,\"start\":58197},{\"end\":58409,\"start\":58406},{\"end\":58423,\"start\":58419},{\"end\":58437,\"start\":58433},{\"end\":58449,\"start\":58445},{\"end\":58462,\"start\":58458},{\"end\":58472,\"start\":58469},{\"end\":58614,\"start\":58610},{\"end\":58622,\"start\":58619},{\"end\":58638,\"start\":58632},{\"end\":58646,\"start\":58644},{\"end\":58662,\"start\":58655},{\"end\":58672,\"start\":58669},{\"end\":58936,\"start\":58932},{\"end\":58951,\"start\":58947},{\"end\":58963,\"start\":58960},{\"end\":58976,\"start\":58973},{\"end\":58991,\"start\":58988},{\"end\":59006,\"start\":59002},{\"end\":59020,\"start\":59011},{\"end\":59036,\"start\":59033},{\"end\":59050,\"start\":59047},{\"end\":59063,\"start\":59060},{\"end\":59083,\"start\":59075},{\"end\":59101,\"start\":59094},{\"end\":59115,\"start\":59107},{\"end\":59133,\"start\":59122},{\"end\":59140,\"start\":59135},{\"end\":59265,\"start\":59259},{\"end\":59280,\"start\":59275},{\"end\":59296,\"start\":59288},{\"end\":59312,\"start\":59306},{\"end\":59411,\"start\":59406},{\"end\":59425,\"start\":59419},{\"end\":59439,\"start\":59432},{\"end\":59452,\"start\":59448},{\"end\":59470,\"start\":59461},{\"end\":59489,\"start\":59480},{\"end\":59505,\"start\":59498},{\"end\":59613,\"start\":59609},{\"end\":59628,\"start\":59621},{\"end\":59642,\"start\":59639},{\"end\":59654,\"start\":59650},{\"end\":59841,\"start\":59834},{\"end\":59855,\"start\":59847},{\"end\":59869,\"start\":59865},{\"end\":59882,\"start\":59875},{\"end\":59894,\"start\":59890},{\"end\":60166,\"start\":60158},{\"end\":60179,\"start\":60174},{\"end\":60192,\"start\":60188},{\"end\":60210,\"start\":60201},{\"end\":60238,\"start\":60229},{\"end\":60252,\"start\":60247},{\"end\":60267,\"start\":60261},{\"end\":60277,\"start\":60274},{\"end\":60293,\"start\":60286},{\"end\":60304,\"start\":60300},{\"end\":60322,\"start\":60312},{\"end\":60501,\"start\":60495},{\"end\":60518,\"start\":60510},{\"end\":60532,\"start\":60527},{\"end\":60553,\"start\":60543},{\"end\":60648,\"start\":60642},{\"end\":60668,\"start\":60659},{\"end\":60692,\"start\":60679},{\"end\":60796,\"start\":60791},{\"end\":60814,\"start\":60805},{\"end\":60828,\"start\":60823},{\"end\":60841,\"start\":60835},{\"end\":60853,\"start\":60851},{\"end\":60870,\"start\":60862},{\"end\":60883,\"start\":60878},{\"end\":60906,\"start\":60897},{\"end\":60999,\"start\":60994},{\"end\":61008,\"start\":61005},{\"end\":61019,\"start\":61015},{\"end\":61033,\"start\":61028},{\"end\":61050,\"start\":61042},{\"end\":61241,\"start\":61236},{\"end\":61254,\"start\":61251},{\"end\":61266,\"start\":61262},{\"end\":61279,\"start\":61277},{\"end\":61291,\"start\":61288},{\"end\":61299,\"start\":61297},{\"end\":61313,\"start\":61311},{\"end\":61323,\"start\":61320},{\"end\":61332,\"start\":61328},{\"end\":61456,\"start\":61448},{\"end\":61471,\"start\":61465},{\"end\":61481,\"start\":61479},{\"end\":61496,\"start\":61489},{\"end\":61512,\"start\":61504},{\"end\":61647,\"start\":61642},{\"end\":61672,\"start\":61654},{\"end\":61688,\"start\":61682},{\"end\":61701,\"start\":61694},{\"end\":61715,\"start\":61709},{\"end\":61735,\"start\":61723},{\"end\":61744,\"start\":61737},{\"end\":61850,\"start\":61847},{\"end\":61864,\"start\":61861},{\"end\":61881,\"start\":61873},{\"end\":61892,\"start\":61887},{\"end\":61908,\"start\":61902},{\"end\":61924,\"start\":61918},{\"end\":61942,\"start\":61935},{\"end\":61955,\"start\":61952},{\"end\":61970,\"start\":61962},{\"end\":61990,\"start\":61979},{\"end\":62003,\"start\":61998},{\"end\":62019,\"start\":62011},{\"end\":62030,\"start\":62026},{\"end\":62043,\"start\":62038},{\"end\":62053,\"start\":62049},{\"end\":62065,\"start\":62061},{\"end\":62075,\"start\":62072},{\"end\":62169,\"start\":62159},{\"end\":62183,\"start\":62177},{\"end\":62421,\"start\":62415},{\"end\":62441,\"start\":62430},{\"end\":62458,\"start\":62452},{\"end\":62470,\"start\":62468},{\"end\":62602,\"start\":62597},{\"end\":62611,\"start\":62608},{\"end\":62625,\"start\":62621},{\"end\":62643,\"start\":62636},{\"end\":62752,\"start\":62748},{\"end\":62768,\"start\":62763},{\"end\":62781,\"start\":62777},{\"end\":62798,\"start\":62790},{\"end\":62816,\"start\":62808},{\"end\":62829,\"start\":62826},{\"end\":62845,\"start\":62839},{\"end\":62856,\"start\":62851},{\"end\":62867,\"start\":62863},{\"end\":62885,\"start\":62876},{\"end\":62898,\"start\":62891},{\"end\":62912,\"start\":62904},{\"end\":62938,\"start\":62920},{\"end\":62949,\"start\":62947},{\"end\":62965,\"start\":62958},{\"end\":62977,\"start\":62974},{\"end\":62987,\"start\":62985},{\"end\":63004,\"start\":62997},{\"end\":63020,\"start\":63014},{\"end\":63035,\"start\":63030},{\"end\":63053,\"start\":63047},{\"end\":63059,\"start\":63055}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":45660,\"start\":45461},{\"attributes\":{\"id\":\"b1\"},\"end\":46728,\"start\":45662},{\"attributes\":{\"id\":\"b2\"},\"end\":47037,\"start\":46730},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b3\"},\"end\":47262,\"start\":47039},{\"attributes\":{\"id\":\"b4\"},\"end\":47302,\"start\":47264},{\"attributes\":{\"id\":\"b5\"},\"end\":47594,\"start\":47304},{\"attributes\":{\"doi\":\"arXiv:2307.09288\",\"id\":\"b6\"},\"end\":47839,\"start\":47596},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":221665105},\"end\":48147,\"start\":47841},{\"attributes\":{\"doi\":\"10.1007/s11023-020-09548-1\",\"id\":\"b8\"},\"end\":48285,\"start\":48149},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":247155069},\"end\":48709,\"start\":48287},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":246426909},\"end\":48980,\"start\":48711},{\"attributes\":{\"doi\":\"arXiv:2212.08073\",\"id\":\"b11\"},\"end\":49216,\"start\":48982},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":237416585},\"end\":49439,\"start\":49218},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":239009562},\"end\":50168,\"start\":49441},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":252815378},\"end\":50518,\"start\":50170},{\"attributes\":{\"doi\":\"arXiv:2201.05966\",\"id\":\"b15\"},\"end\":50804,\"start\":50520},{\"attributes\":{\"doi\":\"arXiv:2210.11399\",\"id\":\"b16\"},\"end\":51039,\"start\":50806},{\"attributes\":{\"doi\":\"1532-4435\",\"id\":\"b17\",\"matched_paper_id\":231573431},\"end\":51218,\"start\":51041},{\"attributes\":{\"id\":\"b18\"},\"end\":51329,\"start\":51220},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":59599816},\"end\":51703,\"start\":51331},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":235458009},\"end\":51909,\"start\":51705},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":53719799},\"end\":52187,\"start\":51911},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":67788003},\"end\":52378,\"start\":52189},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":213700877},\"end\":52581,\"start\":52380},{\"attributes\":{\"id\":\"b24\"},\"end\":52729,\"start\":52583},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b25\"},\"end\":52886,\"start\":52731},{\"attributes\":{\"doi\":\"arXiv:2206.01861\",\"id\":\"b26\"},\"end\":53102,\"start\":52888},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":259298689},\"end\":53306,\"start\":53104},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b28\"},\"end\":53498,\"start\":53308},{\"attributes\":{\"doi\":\"arXiv:2203.15556\",\"id\":\"b29\"},\"end\":53754,\"start\":53500},{\"attributes\":{\"doi\":\"arXiv:2302.04761\",\"id\":\"b30\"},\"end\":53981,\"start\":53756},{\"attributes\":{\"doi\":\"CoRR, abs/2112.09332\",\"id\":\"b31\"},\"end\":54341,\"start\":53983},{\"attributes\":{\"doi\":\"10.48550/arXiv.2212.10560\",\"id\":\"b32\"},\"end\":54553,\"start\":54343},{\"attributes\":{\"id\":\"b33\"},\"end\":54802,\"start\":54555},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b34\"},\"end\":54982,\"start\":54804},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":160025533},\"end\":55138,\"start\":54984},{\"attributes\":{\"doi\":\"arXiv:2001.07676\",\"id\":\"b36\"},\"end\":55296,\"start\":55140},{\"attributes\":{\"doi\":\"10.18653/v1/2021.acl-long.353\",\"id\":\"b37\",\"matched_paper_id\":230433941},\"end\":55793,\"start\":55298},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.243\",\"id\":\"b38\",\"matched_paper_id\":233296808},\"end\":56179,\"start\":55795},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":238583580},\"end\":56382,\"start\":56181},{\"attributes\":{\"doi\":\"10.18653/v1/2022.acl-long.433\",\"id\":\"b40\",\"matched_paper_id\":238857301},\"end\":56829,\"start\":56384},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":254096429},\"end\":57144,\"start\":56831},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":252780574},\"end\":57604,\"start\":57146},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":258999931},\"end\":57936,\"start\":57606},{\"attributes\":{\"doi\":\"arXiv:2309.15531\",\"id\":\"b44\"},\"end\":58168,\"start\":57938},{\"attributes\":{\"id\":\"b45\"},\"end\":58321,\"start\":58170},{\"attributes\":{\"doi\":\"arXiv:2306.00978\",\"id\":\"b46\"},\"end\":58508,\"start\":58323},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":253708271},\"end\":58801,\"start\":58510},{\"attributes\":{\"id\":\"b48\"},\"end\":59069,\"start\":58803},{\"attributes\":{\"doi\":\"arXiv:2305.14314\",\"id\":\"b49\"},\"end\":59216,\"start\":59071},{\"attributes\":{\"id\":\"b50\"},\"end\":59318,\"start\":59218},{\"attributes\":{\"doi\":\"arXiv:1803.05457\",\"id\":\"b51\"},\"end\":59541,\"start\":59320},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":208290939},\"end\":59771,\"start\":59543},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":159041722},\"end\":60061,\"start\":59773},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":52183757},\"end\":60223,\"start\":60063},{\"attributes\":{\"doi\":\"arXiv:2009.03300\",\"id\":\"b55\"},\"end\":60410,\"start\":60225},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":237421373},\"end\":60564,\"start\":60412},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":252796},\"end\":60728,\"start\":60566},{\"attributes\":{\"id\":\"b58\"},\"end\":60912,\"start\":60730},{\"attributes\":{\"doi\":\"10.5281/zenodo.5297715\",\"id\":\"b59\"},\"end\":61145,\"start\":60914},{\"attributes\":{\"doi\":\"arXiv:2303.16199\",\"id\":\"b60\"},\"end\":61368,\"start\":61147},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":263894534},\"end\":61789,\"start\":61370},{\"attributes\":{\"doi\":\"10.5281/zenodo.5371628\",\"id\":\"b62\"},\"end\":62113,\"start\":61791},{\"attributes\":{\"doi\":\"ICLR 2019\",\"id\":\"b63\",\"matched_paper_id\":53592270},\"end\":62305,\"start\":62115},{\"attributes\":{\"doi\":\"10.1145/3394486.3406703\",\"id\":\"b64\",\"matched_paper_id\":221191193},\"end\":62679,\"start\":62307},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-demos.6\",\"id\":\"b65\",\"matched_paper_id\":208117506},\"end\":63349,\"start\":62681}]", "bib_title": "[{\"end\":47882,\"start\":47841},{\"end\":48361,\"start\":48287},{\"end\":48778,\"start\":48711},{\"end\":49266,\"start\":49218},{\"end\":49506,\"start\":49441},{\"end\":50219,\"start\":50170},{\"end\":51133,\"start\":51041},{\"end\":51376,\"start\":51331},{\"end\":51755,\"start\":51705},{\"end\":51997,\"start\":51911},{\"end\":52219,\"start\":52189},{\"end\":52463,\"start\":52380},{\"end\":53171,\"start\":53104},{\"end\":55035,\"start\":54984},{\"end\":55357,\"start\":55298},{\"end\":55851,\"start\":55795},{\"end\":56244,\"start\":56181},{\"end\":56458,\"start\":56384},{\"end\":56872,\"start\":56831},{\"end\":57251,\"start\":57146},{\"end\":57697,\"start\":57606},{\"end\":58598,\"start\":58510},{\"end\":59599,\"start\":59543},{\"end\":59826,\"start\":59773},{\"end\":60150,\"start\":60063},{\"end\":60485,\"start\":60412},{\"end\":60629,\"start\":60566},{\"end\":61442,\"start\":61370},{\"end\":62152,\"start\":62115},{\"end\":62408,\"start\":62307},{\"end\":62739,\"start\":62681}]", "bib_author": "[{\"end\":45511,\"start\":45500},{\"end\":45526,\"start\":45511},{\"end\":45538,\"start\":45526},{\"end\":45555,\"start\":45538},{\"end\":45571,\"start\":45555},{\"end\":45590,\"start\":45571},{\"end\":45610,\"start\":45590},{\"end\":45624,\"start\":45610},{\"end\":45639,\"start\":45624},{\"end\":45654,\"start\":45639},{\"end\":45685,\"start\":45664},{\"end\":45700,\"start\":45685},{\"end\":45714,\"start\":45700},{\"end\":45729,\"start\":45714},{\"end\":45744,\"start\":45729},{\"end\":45758,\"start\":45744},{\"end\":45771,\"start\":45758},{\"end\":45788,\"start\":45771},{\"end\":45804,\"start\":45788},{\"end\":45824,\"start\":45804},{\"end\":45838,\"start\":45824},{\"end\":45850,\"start\":45838},{\"end\":45871,\"start\":45850},{\"end\":45886,\"start\":45871},{\"end\":45900,\"start\":45886},{\"end\":45915,\"start\":45900},{\"end\":45923,\"start\":45915},{\"end\":45937,\"start\":45923},{\"end\":45967,\"start\":45937},{\"end\":45977,\"start\":45967},{\"end\":45985,\"start\":45977},{\"end\":46004,\"start\":45985},{\"end\":46016,\"start\":46004},{\"end\":46032,\"start\":46016},{\"end\":46048,\"start\":46032},{\"end\":46059,\"start\":46048},{\"end\":46078,\"start\":46059},{\"end\":46088,\"start\":46078},{\"end\":46101,\"start\":46088},{\"end\":46118,\"start\":46101},{\"end\":46135,\"start\":46118},{\"end\":46147,\"start\":46135},{\"end\":46167,\"start\":46147},{\"end\":46182,\"start\":46167},{\"end\":46195,\"start\":46182},{\"end\":46210,\"start\":46195},{\"end\":46223,\"start\":46210},{\"end\":46236,\"start\":46223},{\"end\":46252,\"start\":46236},{\"end\":46268,\"start\":46252},{\"end\":46280,\"start\":46268},{\"end\":46296,\"start\":46280},{\"end\":46313,\"start\":46296},{\"end\":46322,\"start\":46313},{\"end\":46747,\"start\":46732},{\"end\":46764,\"start\":46747},{\"end\":46781,\"start\":46764},{\"end\":46792,\"start\":46781},{\"end\":46807,\"start\":46792},{\"end\":46816,\"start\":46807},{\"end\":46831,\"start\":46816},{\"end\":46849,\"start\":46831},{\"end\":46863,\"start\":46849},{\"end\":46878,\"start\":46863},{\"end\":46890,\"start\":46878},{\"end\":46906,\"start\":46890},{\"end\":46925,\"start\":46906},{\"end\":46936,\"start\":46925},{\"end\":46947,\"start\":46936},{\"end\":46964,\"start\":46947},{\"end\":46974,\"start\":46964},{\"end\":47103,\"start\":47090},{\"end\":47119,\"start\":47103},{\"end\":47132,\"start\":47119},{\"end\":47147,\"start\":47132},{\"end\":47158,\"start\":47147},{\"end\":47172,\"start\":47158},{\"end\":47191,\"start\":47172},{\"end\":47202,\"start\":47191},{\"end\":47211,\"start\":47202},{\"end\":47228,\"start\":47211},{\"end\":47326,\"start\":47304},{\"end\":47342,\"start\":47326},{\"end\":47358,\"start\":47342},{\"end\":47379,\"start\":47358},{\"end\":47397,\"start\":47379},{\"end\":47415,\"start\":47397},{\"end\":47430,\"start\":47415},{\"end\":47442,\"start\":47430},{\"end\":47457,\"start\":47442},{\"end\":47473,\"start\":47457},{\"end\":47491,\"start\":47473},{\"end\":47499,\"start\":47491},{\"end\":47663,\"start\":47649},{\"end\":47677,\"start\":47663},{\"end\":47690,\"start\":47677},{\"end\":47704,\"start\":47690},{\"end\":47721,\"start\":47704},{\"end\":47737,\"start\":47721},{\"end\":47756,\"start\":47737},{\"end\":47770,\"start\":47756},{\"end\":47789,\"start\":47770},{\"end\":47805,\"start\":47789},{\"end\":47900,\"start\":47884},{\"end\":47913,\"start\":47900},{\"end\":47925,\"start\":47913},{\"end\":47941,\"start\":47925},{\"end\":47952,\"start\":47941},{\"end\":47966,\"start\":47952},{\"end\":47980,\"start\":47966},{\"end\":47994,\"start\":47980},{\"end\":48013,\"start\":47994},{\"end\":48230,\"start\":48213},{\"end\":48249,\"start\":48230},{\"end\":48374,\"start\":48363},{\"end\":48385,\"start\":48374},{\"end\":48399,\"start\":48385},{\"end\":48414,\"start\":48399},{\"end\":48426,\"start\":48414},{\"end\":48447,\"start\":48426},{\"end\":48465,\"start\":48447},{\"end\":48793,\"start\":48780},{\"end\":48805,\"start\":48793},{\"end\":48815,\"start\":48805},{\"end\":48830,\"start\":48815},{\"end\":48850,\"start\":48830},{\"end\":48866,\"start\":48850},{\"end\":48879,\"start\":48866},{\"end\":48897,\"start\":48879},{\"end\":48913,\"start\":48897},{\"end\":48923,\"start\":48913},{\"end\":48994,\"start\":48982},{\"end\":49011,\"start\":48994},{\"end\":49027,\"start\":49011},{\"end\":49042,\"start\":49027},{\"end\":49059,\"start\":49042},{\"end\":49071,\"start\":49059},{\"end\":49082,\"start\":49071},{\"end\":49095,\"start\":49082},{\"end\":49114,\"start\":49095},{\"end\":49132,\"start\":49114},{\"end\":49279,\"start\":49268},{\"end\":49294,\"start\":49279},{\"end\":49308,\"start\":49294},{\"end\":49320,\"start\":49308},{\"end\":49334,\"start\":49320},{\"end\":49348,\"start\":49334},{\"end\":49356,\"start\":49348},{\"end\":49370,\"start\":49356},{\"end\":49381,\"start\":49370},{\"end\":49521,\"start\":49508},{\"end\":49536,\"start\":49521},{\"end\":49550,\"start\":49536},{\"end\":49564,\"start\":49550},{\"end\":49582,\"start\":49564},{\"end\":49597,\"start\":49582},{\"end\":49614,\"start\":49597},{\"end\":49631,\"start\":49614},{\"end\":49642,\"start\":49631},{\"end\":49653,\"start\":49642},{\"end\":49673,\"start\":49653},{\"end\":49684,\"start\":49673},{\"end\":49700,\"start\":49684},{\"end\":49721,\"start\":49700},{\"end\":49740,\"start\":49721},{\"end\":49752,\"start\":49740},{\"end\":49769,\"start\":49752},{\"end\":49786,\"start\":49769},{\"end\":49802,\"start\":49786},{\"end\":49814,\"start\":49802},{\"end\":49825,\"start\":49814},{\"end\":49836,\"start\":49825},{\"end\":49849,\"start\":49836},{\"end\":49863,\"start\":49849},{\"end\":49879,\"start\":49863},{\"end\":49893,\"start\":49879},{\"end\":49908,\"start\":49893},{\"end\":49923,\"start\":49908},{\"end\":49938,\"start\":49923},{\"end\":49950,\"start\":49938},{\"end\":49966,\"start\":49950},{\"end\":49981,\"start\":49966},{\"end\":50000,\"start\":49981},{\"end\":50013,\"start\":50000},{\"end\":50025,\"start\":50013},{\"end\":50038,\"start\":50025},{\"end\":50237,\"start\":50221},{\"end\":50256,\"start\":50237},{\"end\":50274,\"start\":50256},{\"end\":50638,\"start\":50625},{\"end\":50653,\"start\":50638},{\"end\":50663,\"start\":50653},{\"end\":50676,\"start\":50663},{\"end\":50693,\"start\":50676},{\"end\":50713,\"start\":50693},{\"end\":50729,\"start\":50713},{\"end\":50741,\"start\":50729},{\"end\":50756,\"start\":50741},{\"end\":50764,\"start\":50756},{\"end\":50770,\"start\":50764},{\"end\":50865,\"start\":50857},{\"end\":50876,\"start\":50865},{\"end\":50893,\"start\":50876},{\"end\":50914,\"start\":50893},{\"end\":50925,\"start\":50914},{\"end\":50941,\"start\":50925},{\"end\":50957,\"start\":50941},{\"end\":50979,\"start\":50957},{\"end\":50994,\"start\":50979},{\"end\":51005,\"start\":50994},{\"end\":51150,\"start\":51135},{\"end\":51163,\"start\":51150},{\"end\":51177,\"start\":51163},{\"end\":51232,\"start\":51222},{\"end\":51245,\"start\":51232},{\"end\":51259,\"start\":51245},{\"end\":51270,\"start\":51259},{\"end\":51282,\"start\":51270},{\"end\":51295,\"start\":51282},{\"end\":51305,\"start\":51295},{\"end\":51392,\"start\":51378},{\"end\":51408,\"start\":51392},{\"end\":51431,\"start\":51408},{\"end\":51446,\"start\":51431},{\"end\":51470,\"start\":51446},{\"end\":51487,\"start\":51470},{\"end\":51503,\"start\":51487},{\"end\":51518,\"start\":51503},{\"end\":51767,\"start\":51757},{\"end\":51779,\"start\":51767},{\"end\":51794,\"start\":51779},{\"end\":51813,\"start\":51794},{\"end\":51823,\"start\":51813},{\"end\":51832,\"start\":51823},{\"end\":51845,\"start\":51832},{\"end\":51851,\"start\":51845},{\"end\":52012,\"start\":51999},{\"end\":52027,\"start\":52012},{\"end\":52041,\"start\":52027},{\"end\":52053,\"start\":52041},{\"end\":52067,\"start\":52053},{\"end\":52082,\"start\":52067},{\"end\":52097,\"start\":52082},{\"end\":52112,\"start\":52097},{\"end\":52231,\"start\":52221},{\"end\":52248,\"start\":52231},{\"end\":52267,\"start\":52248},{\"end\":52289,\"start\":52267},{\"end\":52313,\"start\":52289},{\"end\":52320,\"start\":52313},{\"end\":52480,\"start\":52465},{\"end\":52491,\"start\":52480},{\"end\":52501,\"start\":52491},{\"end\":52512,\"start\":52501},{\"end\":52523,\"start\":52512},{\"end\":52681,\"start\":52670},{\"end\":52687,\"start\":52681},{\"end\":52698,\"start\":52687},{\"end\":52713,\"start\":52698},{\"end\":52725,\"start\":52713},{\"end\":52745,\"start\":52731},{\"end\":52757,\"start\":52745},{\"end\":52773,\"start\":52757},{\"end\":52791,\"start\":52773},{\"end\":52993,\"start\":52981},{\"end\":53017,\"start\":52993},{\"end\":53031,\"start\":53017},{\"end\":53043,\"start\":53031},{\"end\":53056,\"start\":53043},{\"end\":53068,\"start\":53056},{\"end\":53188,\"start\":53173},{\"end\":53212,\"start\":53188},{\"end\":53225,\"start\":53212},{\"end\":53235,\"start\":53225},{\"end\":53414,\"start\":53398},{\"end\":53422,\"start\":53414},{\"end\":53438,\"start\":53422},{\"end\":53454,\"start\":53438},{\"end\":53464,\"start\":53454},{\"end\":53565,\"start\":53548},{\"end\":53585,\"start\":53565},{\"end\":53600,\"start\":53585},{\"end\":53619,\"start\":53600},{\"end\":53631,\"start\":53619},{\"end\":53649,\"start\":53631},{\"end\":53663,\"start\":53649},{\"end\":53680,\"start\":53663},{\"end\":53700,\"start\":53680},{\"end\":53713,\"start\":53700},{\"end\":53720,\"start\":53713},{\"end\":53832,\"start\":53819},{\"end\":53849,\"start\":53832},{\"end\":53864,\"start\":53849},{\"end\":53882,\"start\":53864},{\"end\":53896,\"start\":53882},{\"end\":53914,\"start\":53896},{\"end\":53931,\"start\":53914},{\"end\":53947,\"start\":53931},{\"end\":54066,\"start\":54048},{\"end\":54080,\"start\":54066},{\"end\":54095,\"start\":54080},{\"end\":54104,\"start\":54095},{\"end\":54117,\"start\":54104},{\"end\":54132,\"start\":54117},{\"end\":54151,\"start\":54132},{\"end\":54166,\"start\":54151},{\"end\":54183,\"start\":54166},{\"end\":54201,\"start\":54183},{\"end\":54211,\"start\":54201},{\"end\":54223,\"start\":54211},{\"end\":54238,\"start\":54223},{\"end\":54256,\"start\":54238},{\"end\":54270,\"start\":54256},{\"end\":54286,\"start\":54270},{\"end\":54302,\"start\":54286},{\"end\":54317,\"start\":54302},{\"end\":54430,\"start\":54416},{\"end\":54445,\"start\":54430},{\"end\":54461,\"start\":54445},{\"end\":54472,\"start\":54461},{\"end\":54486,\"start\":54472},{\"end\":54503,\"start\":54486},{\"end\":54524,\"start\":54503},{\"end\":54646,\"start\":54630},{\"end\":54658,\"start\":54646},{\"end\":54666,\"start\":54658},{\"end\":54678,\"start\":54666},{\"end\":54691,\"start\":54678},{\"end\":54702,\"start\":54691},{\"end\":54717,\"start\":54702},{\"end\":54732,\"start\":54717},{\"end\":54748,\"start\":54732},{\"end\":54767,\"start\":54748},{\"end\":54779,\"start\":54767},{\"end\":54792,\"start\":54779},{\"end\":54900,\"start\":54886},{\"end\":54916,\"start\":54900},{\"end\":54928,\"start\":54916},{\"end\":54948,\"start\":54928},{\"end\":55051,\"start\":55037},{\"end\":55063,\"start\":55051},{\"end\":55076,\"start\":55063},{\"end\":55088,\"start\":55076},{\"end\":55102,\"start\":55088},{\"end\":55118,\"start\":55102},{\"end\":55245,\"start\":55232},{\"end\":55262,\"start\":55245},{\"end\":55371,\"start\":55359},{\"end\":55381,\"start\":55371},{\"end\":55388,\"start\":55381},{\"end\":55867,\"start\":55853},{\"end\":55881,\"start\":55867},{\"end\":55896,\"start\":55881},{\"end\":56258,\"start\":56246},{\"end\":56273,\"start\":56258},{\"end\":56284,\"start\":56273},{\"end\":56309,\"start\":56284},{\"end\":56324,\"start\":56309},{\"end\":56472,\"start\":56460},{\"end\":56489,\"start\":56472},{\"end\":56498,\"start\":56489},{\"end\":56515,\"start\":56498},{\"end\":56523,\"start\":56515},{\"end\":56535,\"start\":56523},{\"end\":56546,\"start\":56535},{\"end\":56561,\"start\":56546},{\"end\":56888,\"start\":56874},{\"end\":56902,\"start\":56888},{\"end\":56916,\"start\":56902},{\"end\":56931,\"start\":56916},{\"end\":57267,\"start\":57253},{\"end\":57282,\"start\":57267},{\"end\":57295,\"start\":57282},{\"end\":57305,\"start\":57295},{\"end\":57318,\"start\":57305},{\"end\":57332,\"start\":57318},{\"end\":57349,\"start\":57332},{\"end\":57363,\"start\":57349},{\"end\":57372,\"start\":57363},{\"end\":57386,\"start\":57372},{\"end\":57391,\"start\":57386},{\"end\":57710,\"start\":57699},{\"end\":57716,\"start\":57710},{\"end\":57731,\"start\":57716},{\"end\":57745,\"start\":57731},{\"end\":57758,\"start\":57745},{\"end\":58061,\"start\":58046},{\"end\":58076,\"start\":58061},{\"end\":58091,\"start\":58076},{\"end\":58107,\"start\":58091},{\"end\":58121,\"start\":58107},{\"end\":58134,\"start\":58121},{\"end\":58189,\"start\":58170},{\"end\":58205,\"start\":58189},{\"end\":58411,\"start\":58403},{\"end\":58425,\"start\":58411},{\"end\":58439,\"start\":58425},{\"end\":58451,\"start\":58439},{\"end\":58464,\"start\":58451},{\"end\":58474,\"start\":58464},{\"end\":58616,\"start\":58600},{\"end\":58624,\"start\":58616},{\"end\":58640,\"start\":58624},{\"end\":58648,\"start\":58640},{\"end\":58664,\"start\":58648},{\"end\":58674,\"start\":58664},{\"end\":58938,\"start\":58926},{\"end\":58953,\"start\":58938},{\"end\":58965,\"start\":58953},{\"end\":58978,\"start\":58965},{\"end\":58993,\"start\":58978},{\"end\":59008,\"start\":58993},{\"end\":59022,\"start\":59008},{\"end\":59038,\"start\":59022},{\"end\":59052,\"start\":59038},{\"end\":59065,\"start\":59052},{\"end\":59085,\"start\":59071},{\"end\":59103,\"start\":59085},{\"end\":59117,\"start\":59103},{\"end\":59135,\"start\":59117},{\"end\":59142,\"start\":59135},{\"end\":59267,\"start\":59251},{\"end\":59282,\"start\":59267},{\"end\":59298,\"start\":59282},{\"end\":59314,\"start\":59298},{\"end\":59413,\"start\":59400},{\"end\":59427,\"start\":59413},{\"end\":59441,\"start\":59427},{\"end\":59454,\"start\":59441},{\"end\":59472,\"start\":59454},{\"end\":59491,\"start\":59472},{\"end\":59507,\"start\":59491},{\"end\":59615,\"start\":59601},{\"end\":59630,\"start\":59615},{\"end\":59644,\"start\":59630},{\"end\":59656,\"start\":59644},{\"end\":59843,\"start\":59828},{\"end\":59857,\"start\":59843},{\"end\":59871,\"start\":59857},{\"end\":59884,\"start\":59871},{\"end\":59896,\"start\":59884},{\"end\":60168,\"start\":60152},{\"end\":60181,\"start\":60168},{\"end\":60194,\"start\":60181},{\"end\":60212,\"start\":60194},{\"end\":60240,\"start\":60225},{\"end\":60254,\"start\":60240},{\"end\":60269,\"start\":60254},{\"end\":60279,\"start\":60269},{\"end\":60295,\"start\":60279},{\"end\":60306,\"start\":60295},{\"end\":60324,\"start\":60306},{\"end\":60503,\"start\":60487},{\"end\":60520,\"start\":60503},{\"end\":60534,\"start\":60520},{\"end\":60555,\"start\":60534},{\"end\":60650,\"start\":60631},{\"end\":60670,\"start\":60650},{\"end\":60694,\"start\":60670},{\"end\":60798,\"start\":60785},{\"end\":60816,\"start\":60798},{\"end\":60830,\"start\":60816},{\"end\":60843,\"start\":60830},{\"end\":60855,\"start\":60843},{\"end\":60872,\"start\":60855},{\"end\":60885,\"start\":60872},{\"end\":60908,\"start\":60885},{\"end\":61001,\"start\":60990},{\"end\":61010,\"start\":61001},{\"end\":61021,\"start\":61010},{\"end\":61035,\"start\":61021},{\"end\":61052,\"start\":61035},{\"end\":61243,\"start\":61229},{\"end\":61256,\"start\":61243},{\"end\":61268,\"start\":61256},{\"end\":61281,\"start\":61268},{\"end\":61293,\"start\":61281},{\"end\":61301,\"start\":61293},{\"end\":61315,\"start\":61301},{\"end\":61325,\"start\":61315},{\"end\":61334,\"start\":61325},{\"end\":61458,\"start\":61444},{\"end\":61473,\"start\":61458},{\"end\":61483,\"start\":61473},{\"end\":61498,\"start\":61483},{\"end\":61514,\"start\":61498},{\"end\":61852,\"start\":61843},{\"end\":61866,\"start\":61852},{\"end\":61883,\"start\":61866},{\"end\":61894,\"start\":61883},{\"end\":61910,\"start\":61894},{\"end\":61926,\"start\":61910},{\"end\":61944,\"start\":61926},{\"end\":61957,\"start\":61944},{\"end\":61972,\"start\":61957},{\"end\":61992,\"start\":61972},{\"end\":62005,\"start\":61992},{\"end\":62021,\"start\":62005},{\"end\":62032,\"start\":62021},{\"end\":62045,\"start\":62032},{\"end\":62055,\"start\":62045},{\"end\":62067,\"start\":62055},{\"end\":62077,\"start\":62067},{\"end\":62171,\"start\":62154},{\"end\":62185,\"start\":62171},{\"end\":62423,\"start\":62410},{\"end\":62443,\"start\":62423},{\"end\":62460,\"start\":62443},{\"end\":62472,\"start\":62460},{\"end\":62754,\"start\":62741},{\"end\":62770,\"start\":62754},{\"end\":62783,\"start\":62770},{\"end\":62800,\"start\":62783},{\"end\":62818,\"start\":62800},{\"end\":62831,\"start\":62818},{\"end\":62847,\"start\":62831},{\"end\":62858,\"start\":62847},{\"end\":62869,\"start\":62858},{\"end\":62887,\"start\":62869},{\"end\":62900,\"start\":62887},{\"end\":62914,\"start\":62900},{\"end\":62940,\"start\":62914},{\"end\":62951,\"start\":62940},{\"end\":62967,\"start\":62951},{\"end\":62979,\"start\":62967},{\"end\":62989,\"start\":62979},{\"end\":63006,\"start\":62989},{\"end\":63022,\"start\":63006},{\"end\":63037,\"start\":63022},{\"end\":63055,\"start\":63037},{\"end\":63061,\"start\":63055}]", "bib_venue": "[{\"end\":45498,\"start\":45461},{\"end\":47029,\"start\":46974},{\"end\":47088,\"start\":47039},{\"end\":47296,\"start\":47266},{\"end\":47588,\"start\":47499},{\"end\":47647,\"start\":47596},{\"end\":48062,\"start\":48013},{\"end\":48211,\"start\":48149},{\"end\":48551,\"start\":48465},{\"end\":48972,\"start\":48923},{\"end\":49196,\"start\":49148},{\"end\":49433,\"start\":49381},{\"end\":50090,\"start\":50038},{\"end\":50360,\"start\":50274},{\"end\":50623,\"start\":50520},{\"end\":50855,\"start\":50806},{\"end\":51196,\"start\":51177},{\"end\":51586,\"start\":51518},{\"end\":51903,\"start\":51851},{\"end\":52181,\"start\":52112},{\"end\":52372,\"start\":52320},{\"end\":52575,\"start\":52523},{\"end\":52668,\"start\":52583},{\"end\":52859,\"start\":52807},{\"end\":52979,\"start\":52888},{\"end\":53300,\"start\":53235},{\"end\":53396,\"start\":53308},{\"end\":53546,\"start\":53500},{\"end\":53817,\"start\":53756},{\"end\":54046,\"start\":53983},{\"end\":54414,\"start\":54343},{\"end\":54628,\"start\":54555},{\"end\":54884,\"start\":54804},{\"end\":55129,\"start\":55118},{\"end\":55230,\"start\":55140},{\"end\":55579,\"start\":55417},{\"end\":56013,\"start\":55927},{\"end\":56376,\"start\":56324},{\"end\":56677,\"start\":56590},{\"end\":56690,\"start\":56679},{\"end\":57000,\"start\":56931},{\"end\":57460,\"start\":57391},{\"end\":57826,\"start\":57758},{\"end\":58044,\"start\":57938},{\"end\":58315,\"start\":58205},{\"end\":58401,\"start\":58323},{\"end\":58742,\"start\":58674},{\"end\":58924,\"start\":58803},{\"end\":59196,\"start\":59158},{\"end\":59249,\"start\":59218},{\"end\":59398,\"start\":59320},{\"end\":59717,\"start\":59656},{\"end\":59983,\"start\":59896},{\"end\":60217,\"start\":60212},{\"end\":60390,\"start\":60340},{\"end\":60558,\"start\":60555},{\"end\":60719,\"start\":60694},{\"end\":60783,\"start\":60730},{\"end\":60988,\"start\":60914},{\"end\":61227,\"start\":61147},{\"end\":61631,\"start\":61514},{\"end\":61841,\"start\":61791},{\"end\":62250,\"start\":62194},{\"end\":62588,\"start\":62495},{\"end\":63200,\"start\":63091},{\"end\":48655,\"start\":48553},{\"end\":50151,\"start\":50092},{\"end\":50464,\"start\":50362},{\"end\":51683,\"start\":51630},{\"end\":55728,\"start\":55581},{\"end\":56104,\"start\":56015},{\"end\":56779,\"start\":56692},{\"end\":57078,\"start\":57047},{\"end\":57538,\"start\":57507},{\"end\":57881,\"start\":57828},{\"end\":58797,\"start\":58744},{\"end\":59765,\"start\":59719},{\"end\":60057,\"start\":59985},{\"end\":61765,\"start\":61746},{\"end\":62272,\"start\":62252},{\"end\":62652,\"start\":62645},{\"end\":63296,\"start\":63202}]"}}}, "year": 2023, "month": 12, "day": 17}