{"id": 173990717, "updated": "2023-10-07 02:06:21.23", "metadata": {"title": "Probabilistic Noise2Void: Unsupervised Content-Aware Denoising", "authors": "[{\"first\":\"Alexander\",\"last\":\"Krull\",\"middle\":[]},{\"first\":\"Tomas\",\"last\":\"Vicar\",\"middle\":[]},{\"first\":\"Florian\",\"last\":\"Jug\",\"middle\":[]}]", "venue": "Frontiers in Computer Science", "journal": "arXiv: Image and Video Processing", "publication_date": {"year": 2019, "month": 6, "day": 3}, "abstract": "Today, Convolutional Neural Networks (CNNs) are the leading method for image denoising. They are traditionally trained on pairs of images, which are often hard to obtain for practical applications. This motivates self-supervised training methods such as Noise2Void~(N2V) that operate on single noisy images. Self-supervised methods are, unfortunately, not competitive with models trained on image pairs. Here, we present 'Probabilistic Noise2Void' (PN2V), a method to train CNNs to predict per-pixel intensity distributions. Combining these with a suitable description of the noise, we obtain a complete probabilistic model for the noisy observations and true signal in every pixel. We evaluate PN2V on publicly available microscopy datasets, under a broad range of noise regimes, and achieve competitive results with respect to supervised state-of-the-art methods.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "1906.00651", "mag": "3007757852", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/fcomp/KrullVPLJ20", "doi": "10.3389/fcomp.2020.00005"}}, "content": {"source": {"pdf_hash": "7cca7a3754d6960618df3ee86c4fbb48a8dbc937", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1906.00651v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.frontiersin.org/articles/10.3389/fcomp.2020.00005/pdf", "status": "GOLD"}}, "grobid": {"id": "c5257bfa4621717e3e897b56f9da7a3feec0cdc4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7cca7a3754d6960618df3ee86c4fbb48a8dbc937.txt", "contents": "\nProbabilistic Noise2Void: Unsupervised Content-Aware Denoising\n4 Jun 2019\n\nAlexander Krull \nMPI-CBG/PKS (CSBD)\n\n\nTom\u00e1\u0161 Vi\u010dar \nBrno University of Technology\n\n\nFlorian Jug \nMPI-CBG/PKS (CSBD)\n\n\nProbabilistic Noise2Void: Unsupervised Content-Aware Denoising\n4 Jun 2019Denoising \u00b7 CARE \u00b7 Deep Learning \u00b7 Microscopy Data\nToday, Convolutional Neural Networks (CNNs)are the leading method for image denoising. They are traditionally trained on pairs of images, which are often hard to obtain for practical applications. This motivates self-supervised training methods such as Noise2Void (N2V) that operate on single noisy images. Self-supervised methods are, unfortunately, not competitive with models trained on image pairs. Here, we present Probabilistic Noise2Void (PN2V), a method to train CNNs to predict per-pixel intensity distributions. Combining these with a suitable description of the noise, we obtain a complete probabilistic model for the noisy observations and true signal in every pixel. We evaluate PN2V on publicly available microscopy datasets, under a broad range of noise regimes, and achieve competitive results with respect to supervised state-of-the-art methods.\n\nIntroduction\n\nImage restoration is the problem of reconstructing an image from a corrupted version of itself. Recent work shows how CNNs can be used to build powerful content-aware image restoration (CARE) pipelines [11,10,12,13,6,4,1,5]. However, for supervised CARE models, such as [10], pairs of clean and noisy images are required.\n\nFor many application areas, it is impractical or impossible to acquire clean ground-truth images [2]. In such cases, Noise2Noise (N2N) training [6] relaxes the problem, only requiring two noisy instances of the same data. Unfortunately, even the acquisition of two noisy realizations of the same image content is often difficult [2]. Self-supervised training methods, such as Noise2Void (N2V) [4], are a promising alternative, as they operate exclusively on single noisy images [4,1,5]. This is enabled by excluding/masking the center (blind-spot) of the network's receptive fields. Self-supervised training assumes that the noise is pixel-wise independent and that the true intensity of a pixel can be predicted from local image context, excluding before-mentioned blind-spots [4]. For many applications, especially in the context of microscopy images, the first assumption is fulfilled, but the second assumption offers room for improvements [5].\n\nHence, self-supervised models can often not compete with supervised training [4]. In concurrent work, by Laine et al. [5], this problem was elegantly addressed by assuming a Gaussian noise model and predicting Gaussian intensity distributions per pixel. The authors also showed that the same approach an be applied to other noise distributions, which can be approximated as Gaussian, or can be described analytically.\n\nHere, we introduce a new training approach called Probabilistic Noise2Void (PN2V). Similar to [5], PN2V proposes a way to leverage information of the network's blind-spots. However, PN2V is not restricted to Gaussian noise models or Gaussian intensity predictions. More precisely, to compute the posterior distribution of a pixel, we combine (i) a general noise model that can be represented as a histogram (observation likelihood), and (ii) a distribution of possible true pixel intensities (prior), represented by a set of predicted samples.\n\nHaving this complete probabilistic model for each pixel, we are now free to chose which statistical estimator to employ. In this work we use MMSE estimates for our final predictions and show that MMSE-PN2V consistently outperformes other self-supervised methods and, in many cases, leads to results that are competitive even with supervised state-of-the-art CARE networks (see below).\n\n\nBackground\n\nImage Formation and the Denoising Task: An image x = (x 1 , . . . , x n ) is the corrupted version of a clean image (signal) s = (s 1 , . . . , s n ). Our goal is to recover the original signal from x, thus implementing a function f (x) =\u015d \u2248 s.\n\nIn this paper, we assume that each observed pixel value x i is independently drawn from the conditional distribution p(x i |s i ) such that\np(x|s) = n i=1 p(x i |s i ).(1)\nWe will refer to p(x i |s i ) as observation likelihood. It is described by an arbitrary noise model. Traditional Training and Noise2Noise: The function f (x) can be implemented by a Fully Convolutional Network (FCN) [7] (see e.g. [11,10,12,6]), a type of CNN that takes an image as input and produces an entire (in this case denoised) image as output. However, in this setup every predicted output pixel s i depends only on a limited receptive field x RF(i) , i.e. a patch of input pixels surrounding it. FCN based image denoising in fact implements f (x) by producing independent predictions\u015d i = g(x RF(i) ; \u03b8) \u2248 s i for each pixel i, depending only on x RF(i) instead of on the entire image. The prediction is parametrized by the weights \u03b8 of the network. In traditional training, \u03b8 are learned from pairs of noisy x j and corresponding clean training images s j , which provide training examples (x j RF(i) , s j i ) consisting of noisy input patches x j RF(i) and their corresponding clean target values s j i .\n\nThe parameters \u03b8 are traditionally tuned to minimize an empirical risk function such as the average squared distance\nargmin \u03b8 n i=1 m j=1 (\u015d j i \u2212 s j i ) 2(2)\nover all training images j and pixels i. In Noise2Noise [6], Lehtinen et al. show that clean data is in fact not necessary for training and that the same training scheme can be used with noisy data alone. Noise2Noise uses pairs of corresponding noisy training images x j and x \u2032 j , which are based on the same signal s j , but are corrupted independently by noise (see Eq. 1). Such pairs can for example be acquired by imaging a static sample twice.\nNoise2Noise uses training examples (x j RF(i) , x \u2032 j i ), with the input patch x j RF(i)\ncropped from the first image x j and the noisy target x \u2032 j i extracted from the patch center in the second one x \u2032 . It is of course impossible for the network to predict the noisy pixel value x \u2032 j i from the independently corrupted input x j RF(i) . However, assuming the noise is zero centered, i.e. E x \u2032 j i = s j i , the best achievable prediction is the clean signal s j i and the network will learn to denoise the images it is presented with. Noise2Void Training: In Noise2Void, Krull et al. [4] show that training is still possible when not even noisy training pairs are available. They use single images to extract input and target for their networks. If this was done naively, the network would simply learn the identity transformation, directly outputting the value at the center of each pixel's receptive field. Krull et al. address the issue by effectively removing the central pixel from the networks receptive field. To achieve this, they mask the pixel during training, replacing it with a random value from the vicinity. Thus, a Noise2Void trained network can be seen as a function\u015d i =g(x RF(i) ; \u03b8) \u2248 s i , making a prediction for a single pixel based on the modified patchx RF(i) that excludes the central pixel. Such a network can no longer describe the identity, and can be trained from single noisy images.\n\nHowever, this ability comes at price. The accuracy of the predictions is reduced, as the network has to exclude the central pixel of its receptive field, thus having less information available.\n\nTo allow efficient training of a CNN with Noise2Void, Krull et al. simultaneously mask multiple pixels in larger training patches and jointly calculate their gradients.\n\n\nMethod\n\nMaximum Likelihood Training: In PN2V, we build on the idea of masking pixels [4] to obtain a prediction from the modified receptive fieldx RF(i) . However, instead of directly predicting an estimate for each pixel value, PN2V trains a CNN to describe a probability distribution p(s i |x RF(i) ; \u03b8). We will refer to p(s i |x RF(i) ; \u03b8) as prior, as it describes our knowledge of the pixel's signal considering only its surroundings, but not the observation at the pixel itself x i , since it has been excluded fromx RF(i) . We choose a sample based representation for this prior, which will be discussed below.\n\nRemembering that the observed pixels values are drawn independently (Eq. 1), we can combine Eq. 3 with our noise model, and obtain the joint distribution\np(x i , s i |x RF(i) ; \u03b8) = p(s i |x RF(i) ; \u03b8)p(x i |s i ).(4)\nBy integrating over all possible clean signals, we can derive\np(x i |x RF(i) ; \u03b8) = \u221e \u2212\u221e p(s i |x RF(i) ; \u03b8)p(x i |s i )ds i ,(5)\nthe probability of observing the pixel value x i , given we know its surroundingsx RF(i) . We can now view CNN training as an unsupervised learning task. Following the maximum likelihood approach, we tune \u03b8 to minimize\nargmin \u03b8 n i=1 \u2212 ln \u221e \u2212\u221e p(s i |x RF(i) ; \u03b8)p(x i |s i )ds i .(6)\nNote that in order to improve readability, we from here on omit the index j, and refrain from explicitly referring to the training image. Sample Based Prior: To allow an efficient optimization of Eq. 6 we choose a sample based representation of our prior p(s i |x RF(i) ; \u03b8). For every pixel i, our network directly predicts K = 800 output values s k i , which we interpret as independent samples, drawn from p(s i |x RF(i) ; \u03b8). We can now approximate Eq. 6 as\nargmin \u03b8 n i=1 \u2212 ln 1 K K k=1 p(x i |s k i ) .(7)\nDuring training we use Eq. 7 as loss function. Note that the summation over k can be efficiently performed on the GPU. Since every sample s k i is effectively a function of the parameters \u03b8, we can calculate the derivative with respect to any network parameter \u03b8 l as\n\u2202 \u2202\u03b8 l n i=1 \u2212 ln 1 K K k=1 p(x i |s k i ) = n i=1 \u2212 \uf8eb \uf8ed K k=1 \u2202 \u2202s k i p(x i |s k i ) \u2202s k i \u2202\u03b8 l K k=1 p(x i |s k i ) \uf8f6 \uf8f8 . (8)\nMinimal Mean Squared Error (MMSE) Inference: Assuming our network is sufficiently trained, we are now interested in processing images and finding sensible estimates for every pixel's signal s i . Based on our probabilistic model, we derive the MMSE estimate, which is defined as\ns MMSE i = argmin si E p(si|x RF(i) ) (\u015d i \u2212 s i ) 2 = E p(si|x RF(i) ) [s i ] ,(9)\nwhere p(s i |x RF(i) ) is the posterior distribution of the signal given the complete surrounding patch. The posterior is proportional to the joint distribution given in Eq. 4. We can thus approximate s MMSE i by weighing our predicted samples with the corresponding observation likelihood and calculating their average Figure 1 illustrates the process and shows the involved distributions for a concrete pixel in a real example.\ns MMSE i \u2248 K k=1 p(x i |s k i )s k i K k=1 p(x i |s k i ) .(10)\n\nExperiments\n\nThe results of our experiments can be found in Table 1. In Figure 2 we provide qualitative results on realistic test images. Datasets: We evaluate PN2V on datasets provided by Zhang et al. in [13]. Since PN2V is not yet implemented for multi-channel images, we use all available single-channel datasets. These datasets are recorded with different samples and under different imaging conditions. Each of them consists of a total of 20 fields of view (FOVs). One FOV is reserved for testing. The other 19 are used for training and validation.\n\nFor each FOV, the data is composed of 50 raw microscopy images, each containing different noise realizations of the same static sample. For every FOV, Zhang et al. additionally simulate four reduced noise regimes (NRs) by averaging different subsets of 2, 4, 8, and 16 raw images [13]. We will refer to the raw images as NR1 and to the regimes created through averaging 2, 4, 8, and 16 images as NR2, NR3, NR4, and NR5, respectively.\n\nWe find that in one of the datasets (Two-Photon Mice) the average pixel intensity fluctuates heavily over the course of the 50 images, even though it should be approximately constant for each FOV. Considering that a single ground truth image (the average) is used for the evaluation on all 50 images, this leads to fluctuations and distortions in the calculated PSNR values, which are also reflected in the comparatively high standard errors (SEMs) for all methods, see Table 1.\n\nTo account for this inconsistency in the data, we additionally use a variant of the PSNR calculation that is invariant to arbitrary shifts and linear transformations in the ground truth signal. These values are marked by an asterisk (*). Details can also be found in the supplementary material 1 . Acquiring Noise Models: In our experiments, we use a histogram based method to measure and describe the noise distribution p(x i |s i ). We start with corresponding pairs of clean s j and noisy x j images. Here, we use the available training data from [13] for this purpose. However, in general these images could show an arbitrary test pattern that covers the desired range of values and do not have to resemble the sample we are interested in. We construct a 2D histogram (256\u00d7256 bins), with the y-and x-axis corresponding to the clean s j i and noisy pixel values x j i , respectively. By normalizing every row, we obtain a a probability distribution for every signal. Considering Eq. 7, we require our model to be differentiable with respect to the s i . To ensure this differentiability, we linearly interpolate along the y-axis of the normalized histogram, obtaining a model for p(x i |s i ) that is continuous in s i . Evaluated Denoising Models/Methods: To put the denoising results of PN2V into perspective, we compare to various state-of-the-art baselines, including the strongest published numbers on the datasets. U-Net (PN2V): We use a standard U-Net [9]. Our network has a U-Net depth of 3, 1 input channel, and K = 800 output channels, which are interpreted as samples. We use a initial feature channel number of 64 in the fist U-Net layer. We train our network separately for each NR in each dataset. We use the same masking technique as [4]. Further details and training parameters can be found in the supplementary material 1 . U-Net (N2V): We use the same network architecture as for U-Net (PN2V) but modify the outputlayer to produce only a single prediction instead of K = 800. The network is trained using the Noise2Void scheme as described in [4]. All training parameters are identical to U-Net (PN2V). U-Net (trad.): We use the exact same architecture as U-Net (N2V), but train the network using the available ground-truth data and the standard MSE loss (see Eq. 2). All training parameters are identical to U-Net (PN2V) and U-Net (N2V). VST+BM3D: Numbers are taken from [13]. The authors fit a Poisson Gaussian noise model to the data and then apply a combination of variance-stabilizing transformation (VST) [8] and BM3D filtering [3]. DnCNN: Numbers are taken from [13]. DnCNN [12] is an established CNN based denoising architecture that is trained in a supervised fashion. N2N: Numbers are taken from [13]. The authors train a network according to the N2N scheme, using an architecture similar to the one presented in [6].   Table 1. Results of PN2V and baseline methods on three datasets from [13]. Comparisons are performed on five noise regimes (NR1-NR5). Numbers report PSNR (dB) \u00b1 2 SEM, averaged over all 50 images in each NR. We group all supervised/non-supervised methods and mark the highest values in bold. Rows marked by asterisk ( * ) use a scaleand shift-invariant PSNR calculation to address inconsistent acquisitions in the Two-Photon mice dataset (see main text). Comp. times: All CNN based methods required below 1s per image (NVIDIA TITAN Xp); VST+BM3D required on avg. 6.22s.\n\n\nDiscussion\n\nWe have introduced PN2V, a fully probabilistic approach extending self-supervised CARE training. PN2V makes use of an arbitrary noise model which can be determined by analyzing any set of available images that are subject to the same type of noise. This is a decisive advantage compared to state-of-the-art supervised methods and allows PN2V to be used for many practical applications. The much improved performance of PN2V lies consistently beyond selfsupervised training and can often compete with state-of-the-art supervised methods. We see a plethora of unique applications for PN2V, for example in challenging low-light conditions, where noise typically is the limiting factor for downstream analysis.\n\nFig. 1 .\n1Image denoising with PN2V. The final MMSE estimate (orange dashed line) for the true signal si of a pixel (position marked in the image insets on the right) corresponds to the center of mass of the posterior distribution (orange curve). Given an observed noisy input value xi (dashed green line), the posterior is proportional to the product of the prior (blue curve) and the observation likelihood (green curve). PN2V describes the prior by a set of samples predicted by our CNN. The likelihood is provided by an arbitrary noise model. Black dashed line is the true signal of the pixel (GT). Prior and posterior are visualized using a kernel density estimator.\n\nFig. 2 .\n2Qualitative results for three images (rows) from the datasets we used in this manuscript. Left to right: raw image (NR1), zoomed inset, predictions by U-Net (trad.), U-Net (N2V), U-Net (PN2V), and ground truth data.\n\n\n38\u00b10.01 32.44\u00b10.01 35.59\u00b10.01 38.90\u00b10.01 42.64\u00b10.03 35.79 U-Net (PN2V ) 38.24\u00b10.02 39.72\u00b10.03 41.34\u00b10.03 43.02\u00b10.04 45.11\u00b10.05 41.49 U-Net (N2V) 37.56\u00b10.02 38.78\u00b10.02 39.94\u00b10.02 41.01\u00b10.02 41.95\u00b10.Net (trad.) 38.38\u00b10.02 39.90\u00b10.03 41.37\u00b10.03 43.06\u00b10.04 45.16\u00b10.05 41.58 Two-Photon Mice Input 24.94\u00b10.07 27.83\u00b10.1 30.69\u00b10.15 33.67\u00b10.19 37.72\u00b10.14 30.97 U-Net (PN2V ) 33.67\u00b10.33 34.58\u00b10.39 35.42\u00b10.42 36.58\u00b10.37 39.78\u00b10.24 36.00 U-Net (N2V) 33.42\u00b10.31 34.31\u00b10.36 35.09\u00b10.38 36.08\u00b10.33 37.80\u00b10.14 35.Net (trad.) 34.35\u00b10.19 35.32+0.23 36.14\u00b10.27 37.48\u00b10.27 40.28\u00b10.2 36.72 * U-Net (PN2V ) 34.84\u00b10.06 36.02\u00b10.07 37.08\u00b10.08 38.28\u00b10.09 40.89\u00b10.07 37.42 * U-Net (N2V) 34.60\u00b10.09 35.77\u00b10.1 36.71\u00b10.1 37.64\u00b10.09 38.49\u00b10.05 36.64 * U-Net (trad.) 35.05\u00b10.05 36.22\u00b10.06 37.28\u00b10.07 38.78\u00b10.1 41.34\u00b10.07 37.73Confocal Mice \nNR1 \nNR2 \nNR3 \nNR4 \nNR5 \nMean \nInput \n29.02 39.85 \nVST+BM3D \n37.95 \n39.47 \n41.09 \n42.73 \n44.52 \n41.15 \nU-DnCNN \n38.15 \n39.78 \n41.41 \n43.11 \n45.20 \n41.53 \nN2N \n38.19 \n39.77 \n41.28 \n42.83 \n44.56 \n41.33 \nConfocal Zebrafish \nInput \n22.81\u00b10.02 25.89\u00b10.02 29.05\u00b10.03 32.39\u00b10.03 36.21\u00b10.04 29.27 \nU-Net (PN2V ) \n32.45\u00b10.02 33.96\u00b10.03 35.48\u00b10.05 37.07\u00b10.06 39.08\u00b10.07 35.61 \nU-Net (N2V) \n32.10\u00b10.02 33.34\u00b10.03 34.43\u00b10.04 35.39\u00b10.04 36.21\u00b10.03 34.30 \nVST+BM3D \n32.00 \n33.75 \n35.30 \n36.78 \n38.32 \n35.23 \nU-Net (trad.) \n32.93\u00b10.03 34.35\u00b10.04 35.67\u00b10.05 37.11+0.06 39.09\u00b10.07 35.83 \nDnCNN \n32.44 \n34.16 \n35.75 \n37.28 \n39.07 \n35.74 \nN2N \n32.93 \n34.37 \n35.71 \n37.06 \n38.65 \n35.74 \n34 \nVST+BM3D \n33.81 \n34.78 \n35.77 \n36.97 \n39.39 \n36.14 \nU-DnCNN \n33.67 \n34.95 \n36.10 \n37.43 \n40.30 \n36.49 \nN2N \n34.33 \n35.32 \n36.25 \n37.46 \n39.89 \n36.65 \n\n \nThe Supplement will be made available soon.\nAcknowledgementsWe thank Uwe Schmidt, Martin Weigert, and Tobias Pietzsch for the helpful discussions. We thank Tobias Pietzsch for proof reading. The computations were performed on an HPC Cluster at the Center for Information Services and High Performance Computing (ZIH) at TU Dresden.\nNoise2self: Blind denoising by self-supervision. J Batson, L Royer, arXiv:1901.11365arXiv preprintBatson, J., Royer, L.: Noise2self: Blind denoising by self-supervision. arXiv preprint arXiv:1901.11365 (2019)\n\nCryo-care: Content-aware image restoration for cryo-transmission electron microscopy data. T O Buchholz, M Jordan, G Pigino, F Jug, ISBIBuchholz, T.O., Jordan, M., Pigino, G., Jug, F.: Cryo-care: Content-aware image restoration for cryo-transmission electron microscopy data. In: ISBI (2019)\n\nImage denoising by sparse 3-d transform-domain collaborative filtering. K Dabov, A Foi, V Katkovnik, K Egiazarian, IEEE Transactions on Image Processing. 168Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.: Image denoising by sparse 3-d transform-domain collaborative filtering. IEEE Transactions on Image Processing 16(8), 2080-2095 (2007)\n\nNoise2void -learning denoising from single noisy images. A Krull, T O Buchholz, F Jug, CVPRKrull, A., Buchholz, T.O., Jug, F.: Noise2void -learning denoising from single noisy images. In: CVPR (2019)\n\nHigh-quality self-supervised deep image denoising. S Laine, T Karras, J Lehtinen, T Aila, arXiv:1901.10277arXiv preprintLaine, S., Karras, T., Lehtinen, J., Aila, T.: High-quality self-supervised deep image denoising. arXiv preprint arXiv:1901.10277 (2019)\n\nNoise2Noise: Learning image restoration without clean data. J Lehtinen, J Munkberg, J Hasselgren, S Laine, T Karras, M Aittala, T Aila, Lehtinen, J., Munkberg, J., Hasselgren, J., Laine, S., Karras, T., Aittala, M., Aila, T.: Noise2Noise: Learning image restoration without clean data. In: ICML. pp. 2965-2974 (2018)\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. pp. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: CVPR. pp. 3431-3440 (2015)\n\nOptimal inversion of the generalized anscombe transformation for poisson-gaussian noise. M Makitalo, A Foi, IEEE Transactions on Image Processing. 221Makitalo, M., Foi, A.: Optimal inversion of the generalized anscombe transforma- tion for poisson-gaussian noise. IEEE Transactions on Image Processing 22(1), 91-103 (2013)\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, MICCAIRonneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed- ical image segmentation. In: MICCAI (2015)\n\nContent-aware image restoration: Pushing the limits of fluorescence microscopy. M Weigert, Nature Methods. Weigert, M., , et al.: Content-aware image restora- tion: Pushing the limits of fluorescence microscopy. Na- ture Methods (2018).\n\n. 10.1038/s41592-018-0216-7https://doi.org/10.1038/s41592-018-0216-7, https://doi.org/10.1038/s41592-018-0216-7\n\nIsotropic reconstruction of 3d fluorescence microscopy images using convolutional neural networks. M Weigert, L Royer, F Jug, G Myers, Descoteaux, M., Maier-Hein, L., Franz, A., Jannin, P., Collins, D.L., Duchesne, S.Weigert, M., Royer, L., Jug, F., Myers, G.: Isotropic reconstruction of 3d fluores- cence microscopy images using convolutional neural networks. In: Descoteaux, M., Maier-Hein, L., Franz, A., Jannin, P., Collins, D.L., Duchesne, S. (eds.) MICCAI (2017)\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. K Zhang, W Zuo, Y Chen, D Meng, L Zhang, IEEE Transactions on Image Processing. 267Zhang, K., Zuo, W., Chen, Y., Meng, D., Zhang, L.: Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE Transactions on Image Processing 26(7), 3142-3155 (2017)\n\nY Zhang, Y Zhu, E Nichols, Q Wang, S Zhang, C Smith, S Howard, arXiv:1812.10366A poisson-gaussian denoising dataset with real fluorescence microscopy images. arXiv preprintZhang, Y., Zhu, Y., Nichols, E., Wang, Q., Zhang, S., Smith, C., Howard, S.: A poisson-gaussian denoising dataset with real fluorescence microscopy images. arXiv preprint arXiv:1812.10366 (2018)\n", "annotations": {"author": "[{\"end\":113,\"start\":76},{\"end\":158,\"start\":114},{\"end\":192,\"start\":159}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":86},{\"end\":125,\"start\":120},{\"end\":170,\"start\":167}]", "author_first_name": "[{\"end\":85,\"start\":76},{\"end\":119,\"start\":114},{\"end\":166,\"start\":159}]", "author_affiliation": "[{\"end\":112,\"start\":93},{\"end\":157,\"start\":127},{\"end\":191,\"start\":172}]", "title": "[{\"end\":63,\"start\":1},{\"end\":255,\"start\":193}]", "venue": null, "abstract": "[{\"end\":1179,\"start\":317}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1401,\"start\":1397},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1404,\"start\":1401},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1407,\"start\":1404},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1410,\"start\":1407},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1412,\"start\":1410},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1414,\"start\":1412},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1416,\"start\":1414},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1418,\"start\":1416},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1469,\"start\":1465},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1618,\"start\":1615},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1665,\"start\":1662},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1850,\"start\":1847},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1914,\"start\":1911},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1999,\"start\":1996},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2001,\"start\":1999},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2003,\"start\":2001},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2299,\"start\":2296},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2465,\"start\":2462},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2548,\"start\":2545},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2589,\"start\":2586},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2984,\"start\":2981},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4469,\"start\":4466},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4484,\"start\":4480},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4487,\"start\":4484},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4490,\"start\":4487},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4492,\"start\":4490},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5487,\"start\":5484},{\"end\":5504,\"start\":5489},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6473,\"start\":6470},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7756,\"start\":7753},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10898,\"start\":10894},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11528,\"start\":11524},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12713,\"start\":12709},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13625,\"start\":13622},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13915,\"start\":13912},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14227,\"start\":14224},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14557,\"start\":14553},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14695,\"start\":14692},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14718,\"start\":14715},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14754,\"start\":14750},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14766,\"start\":14762},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14891,\"start\":14887},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15007,\"start\":15004},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15084,\"start\":15080}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16974,\"start\":16302},{\"attributes\":{\"id\":\"fig_1\"},\"end\":17201,\"start\":16975},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":18836,\"start\":17202}]", "paragraph": "[{\"end\":1516,\"start\":1195},{\"end\":2466,\"start\":1518},{\"end\":2885,\"start\":2468},{\"end\":3430,\"start\":2887},{\"end\":3816,\"start\":3432},{\"end\":4075,\"start\":3831},{\"end\":4216,\"start\":4077},{\"end\":5266,\"start\":4249},{\"end\":5384,\"start\":5268},{\"end\":5878,\"start\":5428},{\"end\":7300,\"start\":5969},{\"end\":7495,\"start\":7302},{\"end\":7665,\"start\":7497},{\"end\":8286,\"start\":7676},{\"end\":8441,\"start\":8288},{\"end\":8567,\"start\":8506},{\"end\":8854,\"start\":8636},{\"end\":9382,\"start\":8921},{\"end\":9700,\"start\":9433},{\"end\":10109,\"start\":9831},{\"end\":10623,\"start\":10194},{\"end\":11242,\"start\":10702},{\"end\":11677,\"start\":11244},{\"end\":12157,\"start\":11679},{\"end\":15580,\"start\":12159},{\"end\":16301,\"start\":15595}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4248,\"start\":4217},{\"attributes\":{\"id\":\"formula_1\"},\"end\":5427,\"start\":5385},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5968,\"start\":5879},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8505,\"start\":8442},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8635,\"start\":8568},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8920,\"start\":8855},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9432,\"start\":9383},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9830,\"start\":9701},{\"attributes\":{\"id\":\"formula_8\"},\"end\":10193,\"start\":10110},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10687,\"start\":10624}]", "table_ref": "[{\"end\":10756,\"start\":10749},{\"end\":12156,\"start\":12149},{\"end\":15018,\"start\":15011}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1193,\"start\":1181},{\"attributes\":{\"n\":\"2\"},\"end\":3829,\"start\":3819},{\"attributes\":{\"n\":\"3\"},\"end\":7674,\"start\":7668},{\"attributes\":{\"n\":\"4\"},\"end\":10700,\"start\":10689},{\"attributes\":{\"n\":\"5\"},\"end\":15593,\"start\":15583},{\"end\":16311,\"start\":16303},{\"end\":16984,\"start\":16976}]", "table": "[{\"end\":18836,\"start\":17998}]", "figure_caption": "[{\"end\":16974,\"start\":16313},{\"end\":17201,\"start\":16986},{\"end\":17998,\"start\":17204}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10522,\"start\":10514},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10769,\"start\":10761}]", "bib_author_first_name": "[{\"end\":19219,\"start\":19218},{\"end\":19229,\"start\":19228},{\"end\":19471,\"start\":19470},{\"end\":19473,\"start\":19472},{\"end\":19485,\"start\":19484},{\"end\":19495,\"start\":19494},{\"end\":19505,\"start\":19504},{\"end\":19745,\"start\":19744},{\"end\":19754,\"start\":19753},{\"end\":19761,\"start\":19760},{\"end\":19774,\"start\":19773},{\"end\":20073,\"start\":20072},{\"end\":20082,\"start\":20081},{\"end\":20084,\"start\":20083},{\"end\":20096,\"start\":20095},{\"end\":20268,\"start\":20267},{\"end\":20277,\"start\":20276},{\"end\":20287,\"start\":20286},{\"end\":20299,\"start\":20298},{\"end\":20535,\"start\":20534},{\"end\":20547,\"start\":20546},{\"end\":20559,\"start\":20558},{\"end\":20573,\"start\":20572},{\"end\":20582,\"start\":20581},{\"end\":20592,\"start\":20591},{\"end\":20603,\"start\":20602},{\"end\":20849,\"start\":20848},{\"end\":20857,\"start\":20856},{\"end\":20870,\"start\":20869},{\"end\":21106,\"start\":21105},{\"end\":21118,\"start\":21117},{\"end\":21406,\"start\":21405},{\"end\":21421,\"start\":21420},{\"end\":21432,\"start\":21431},{\"end\":21652,\"start\":21651},{\"end\":22022,\"start\":22021},{\"end\":22033,\"start\":22032},{\"end\":22042,\"start\":22041},{\"end\":22049,\"start\":22048},{\"end\":22473,\"start\":22472},{\"end\":22482,\"start\":22481},{\"end\":22489,\"start\":22488},{\"end\":22497,\"start\":22496},{\"end\":22505,\"start\":22504},{\"end\":22749,\"start\":22748},{\"end\":22758,\"start\":22757},{\"end\":22765,\"start\":22764},{\"end\":22776,\"start\":22775},{\"end\":22784,\"start\":22783},{\"end\":22793,\"start\":22792},{\"end\":22802,\"start\":22801}]", "bib_author_last_name": "[{\"end\":19226,\"start\":19220},{\"end\":19235,\"start\":19230},{\"end\":19482,\"start\":19474},{\"end\":19492,\"start\":19486},{\"end\":19502,\"start\":19496},{\"end\":19509,\"start\":19506},{\"end\":19751,\"start\":19746},{\"end\":19758,\"start\":19755},{\"end\":19771,\"start\":19762},{\"end\":19785,\"start\":19775},{\"end\":20079,\"start\":20074},{\"end\":20093,\"start\":20085},{\"end\":20100,\"start\":20097},{\"end\":20274,\"start\":20269},{\"end\":20284,\"start\":20278},{\"end\":20296,\"start\":20288},{\"end\":20304,\"start\":20300},{\"end\":20544,\"start\":20536},{\"end\":20556,\"start\":20548},{\"end\":20570,\"start\":20560},{\"end\":20579,\"start\":20574},{\"end\":20589,\"start\":20583},{\"end\":20600,\"start\":20593},{\"end\":20608,\"start\":20604},{\"end\":20854,\"start\":20850},{\"end\":20867,\"start\":20858},{\"end\":20878,\"start\":20871},{\"end\":21115,\"start\":21107},{\"end\":21122,\"start\":21119},{\"end\":21418,\"start\":21407},{\"end\":21429,\"start\":21422},{\"end\":21437,\"start\":21433},{\"end\":21660,\"start\":21653},{\"end\":22030,\"start\":22023},{\"end\":22039,\"start\":22034},{\"end\":22046,\"start\":22043},{\"end\":22055,\"start\":22050},{\"end\":22479,\"start\":22474},{\"end\":22486,\"start\":22483},{\"end\":22494,\"start\":22490},{\"end\":22502,\"start\":22498},{\"end\":22511,\"start\":22506},{\"end\":22755,\"start\":22750},{\"end\":22762,\"start\":22759},{\"end\":22773,\"start\":22766},{\"end\":22781,\"start\":22777},{\"end\":22790,\"start\":22785},{\"end\":22799,\"start\":22794},{\"end\":22809,\"start\":22803}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1901.11365\",\"id\":\"b0\"},\"end\":19377,\"start\":19169},{\"attributes\":{\"id\":\"b1\"},\"end\":19670,\"start\":19379},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1475121},\"end\":20013,\"start\":19672},{\"attributes\":{\"id\":\"b3\"},\"end\":20214,\"start\":20015},{\"attributes\":{\"doi\":\"arXiv:1901.10277\",\"id\":\"b4\"},\"end\":20472,\"start\":20216},{\"attributes\":{\"id\":\"b5\"},\"end\":20790,\"start\":20474},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1629541},\"end\":21014,\"start\":20792},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206724566},\"end\":21338,\"start\":21016},{\"attributes\":{\"id\":\"b8\"},\"end\":21569,\"start\":21340},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":53749133},\"end\":21807,\"start\":21571},{\"attributes\":{\"doi\":\"10.1038/s41592-018-0216-7\",\"id\":\"b10\"},\"end\":21920,\"start\":21809},{\"attributes\":{\"id\":\"b11\"},\"end\":22391,\"start\":21922},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":996788},\"end\":22746,\"start\":22393},{\"attributes\":{\"doi\":\"arXiv:1812.10366\",\"id\":\"b13\"},\"end\":23114,\"start\":22748}]", "bib_title": "[{\"end\":19742,\"start\":19672},{\"end\":20846,\"start\":20792},{\"end\":21103,\"start\":21016},{\"end\":21649,\"start\":21571},{\"end\":22470,\"start\":22393}]", "bib_author": "[{\"end\":19228,\"start\":19218},{\"end\":19237,\"start\":19228},{\"end\":19484,\"start\":19470},{\"end\":19494,\"start\":19484},{\"end\":19504,\"start\":19494},{\"end\":19511,\"start\":19504},{\"end\":19753,\"start\":19744},{\"end\":19760,\"start\":19753},{\"end\":19773,\"start\":19760},{\"end\":19787,\"start\":19773},{\"end\":20081,\"start\":20072},{\"end\":20095,\"start\":20081},{\"end\":20102,\"start\":20095},{\"end\":20276,\"start\":20267},{\"end\":20286,\"start\":20276},{\"end\":20298,\"start\":20286},{\"end\":20306,\"start\":20298},{\"end\":20546,\"start\":20534},{\"end\":20558,\"start\":20546},{\"end\":20572,\"start\":20558},{\"end\":20581,\"start\":20572},{\"end\":20591,\"start\":20581},{\"end\":20602,\"start\":20591},{\"end\":20610,\"start\":20602},{\"end\":20856,\"start\":20848},{\"end\":20869,\"start\":20856},{\"end\":20880,\"start\":20869},{\"end\":21117,\"start\":21105},{\"end\":21124,\"start\":21117},{\"end\":21420,\"start\":21405},{\"end\":21431,\"start\":21420},{\"end\":21439,\"start\":21431},{\"end\":21662,\"start\":21651},{\"end\":22032,\"start\":22021},{\"end\":22041,\"start\":22032},{\"end\":22048,\"start\":22041},{\"end\":22057,\"start\":22048},{\"end\":22481,\"start\":22472},{\"end\":22488,\"start\":22481},{\"end\":22496,\"start\":22488},{\"end\":22504,\"start\":22496},{\"end\":22513,\"start\":22504},{\"end\":22757,\"start\":22748},{\"end\":22764,\"start\":22757},{\"end\":22775,\"start\":22764},{\"end\":22783,\"start\":22775},{\"end\":22792,\"start\":22783},{\"end\":22801,\"start\":22792},{\"end\":22811,\"start\":22801}]", "bib_venue": "[{\"end\":19216,\"start\":19169},{\"end\":19468,\"start\":19379},{\"end\":19824,\"start\":19787},{\"end\":20070,\"start\":20015},{\"end\":20265,\"start\":20216},{\"end\":20532,\"start\":20474},{\"end\":20888,\"start\":20880},{\"end\":21161,\"start\":21124},{\"end\":21403,\"start\":21340},{\"end\":21676,\"start\":21662},{\"end\":22019,\"start\":21922},{\"end\":22550,\"start\":22513},{\"end\":22904,\"start\":22827}]"}}}, "year": 2023, "month": 12, "day": 17}