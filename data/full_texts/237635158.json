{"id": 237635158, "updated": "2023-10-05 22:50:53.777", "metadata": {"title": "A Unified Treatment of Partial Stragglers and Sparse Matrices in Coded Matrix Computation", "authors": "[{\"first\":\"Anindya\",\"last\":\"Das\",\"middle\":[\"Bijoy\"]},{\"first\":\"Aditya\",\"last\":\"Ramamoorthy\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The overall execution time of distributed matrix computations is often dominated by slow worker nodes (stragglers) within the clusters. Recently, different coding techniques have been utilized to mitigate the effect of stragglers where worker nodes are assigned the job of processing encoded submatrices of the original matrices. In many machine learning or optimization problems the relevant matrices are often sparse. Several prior coded computation methods operate with dense linear combinations of the original submatrices; this can significantly increase the worker node computation times and consequently the overall job execution time. Moreover, several existing techniques treat the stragglers as failures (erasures) and discard their computations. In this work, we present a coding approach which operates with limited encoding of the original submatrices and utilizes the partial computations done by the slower workers. While our scheme can continue to have the optimal threshold of prior work, it also allows us to trade off the straggler resilience with the worker computation speed for sparse input matrices. Extensive numerical experiments done in AWS (Amazon Web Services) cluster confirm that the proposed approach enhances the speed of the worker computations (and thus the whole process) significantly.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2109.12070", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/itw/DasR21", "doi": "10.1109/itw48936.2021.9611400"}}, "content": {"source": {"pdf_hash": "75a29d90a6857ad2ffc7dbf4f8bba18207125d99", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2109.12070v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2109.12070", "status": "GREEN"}}, "grobid": {"id": "299c8073d9db748c02f3bacb68e7e948ef2fa1bf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/75a29d90a6857ad2ffc7dbf4f8bba18207125d99.txt", "contents": "\nA Unified Treatment of Partial Stragglers and Sparse Matrices in Coded Matrix Computation\nJun 2022 1\n\nAnindya Bijoy Das \nAditya Ramamoorthy \nA Unified Treatment of Partial Stragglers and Sparse Matrices in Coded Matrix Computation\nJun 2022 1\nThe overall execution time of distributed matrix computations is often dominated by slow worker nodes (stragglers) within the computation clusters. Recently, coding-theoretic techniques have been utilized to mitigate the effect of stragglers where worker nodes are assigned the job of processing encoded submatrices of the original matrices. In many machine learning or optimization problems the relevant matrices are often sparse. Several prior coded computation methods operate with dense linear combinations of the original submatrices; this can significantly increase the worker node computation times and consequently the overall job execution time. Moreover, several existing techniques treat the stragglers as failures (erasures) and discard their computations. In this work, we present a coding approach which operates with limited encoding of the original submatrices and utilizes the partial computations done by the slower workers. While our scheme can continue to have the optimal threshold of prior work, it also allows us to trade off the straggler resilience with the worker computation speed for sparse input matrices. Extensive numerical experiments done over cloud platforms confirm that the proposed approach enhances the speed of the worker computations (and thus the whole process) significantly.\n\nAbstract-The overall execution time of distributed matrix computations is often dominated by slow worker nodes (stragglers) within the computation clusters. Recently, coding-theoretic techniques have been utilized to mitigate the effect of stragglers where worker nodes are assigned the job of processing encoded submatrices of the original matrices. In many machine learning or optimization problems the relevant matrices are often sparse. Several prior coded computation methods operate with dense linear combinations of the original submatrices; this can significantly increase the worker node computation times and consequently the overall job execution time. Moreover, several existing techniques treat the stragglers as failures (erasures) and discard their computations. In this work, we present a coding approach which operates with limited encoding of the original submatrices and utilizes the partial computations done by the slower workers. While our scheme can continue to have the optimal threshold of prior work, it also allows us to trade off the straggler resilience with the worker computation speed for sparse input matrices. Extensive numerical experiments done over cloud platforms confirm that the proposed approach enhances the speed of the worker computations (and thus the whole process) significantly.\n\n\nI. INTRODUCTION\n\nMatrix computations are an indispensable part of several machine learning and optimization problems. The large scale dimensions of the matrices in these problems necessitates the usage of distributed computation where the whole job is subdivided into smaller tasks and assigned to multiple worker nodes. In these systems, the overall job execution time can be dominated by slower (or failed) worker nodes, which are referred to as stragglers. Recently, a number of coding theory techniques [1]- [11] have been proposed to mitigate the effect of stragglers for matrix-vector and matrix-matrix multiplications (see [12] for a tutorial overview). For example, [1] proposes to compute A T x, where A \u2208 R t\u00d7r and x \u2208 R t , in a distributed fashion by partitioning the matrix A into two block-columns as A = [A 0 | A 1 ], and assigning the job of computing A T 0 x, A T 1 x and (A 0 + A 1 ) T x, respectively, to three different workers. Thus, we can recover A T x if any two out of three workers return their results. This implies that This work was supported in part by the National Science Foundation (NSF) under grants CCF-1910840 and CCF-2115200. The material in this work has appeared in part at the 2021 Information Theory Workshop (ITW), Kanazawa, Japan and the 2022 IEEE International Symposium on Information Theory (ISIT), Aalto University in Espoo, Finland.\n\nAnindya Bijoy Das  the system is resilient to one straggler, while each of those workers has effectively half of the overall computational load. In general, we define the recovery threshold as the minimum number of workers (\u03c4 ) that need to finish their jobs such that the result A T x (for matrix-vector multiplication), or A T B (for matrix-matrix multiplication; where B \u2208 R t\u00d7w ) can be recovered from any subset of \u03c4 worker nodes.\n\nWhile the recovery threshold is an important metric for coded computation, there are other important issues that also need to be considered. First, in many of the practical examples, the matrices A and/or B can be sparse. If we linearly combine m submatrices of A, then the density of the nonzero entries in the encoded matrices can be up to m-times higher than the density of A; the actual value will depend on the underlying sparsity pattern. This can result in a large increase in the worker node computation time. As noted in [13], the overall job execution time can actually go up rather than down. Thus, designing schemes that combine relatively few submatrices while continuing to have a good threshold is important. Second, much of prior work (see [13]- [20] for some exceptions) treats stragglers as erasures, and thus discards the computations done by the slower workers. But a slower worker may not be a useless worker and efficiently utilizing the partial computations done by the slower workers is of interest.\n\nIn this work, we propose an approach for both distributed matrix-vector and matrix-matrix multiplications which makes progress on both of the issues mentioned above. In our approach, many of the assigned submatrices within a worker node are uncoded; this makes the worker computations significantly faster. Moreover, our proposed approach can exploit the partial computations done by the slower workers. We emphasize that our approach continues to enjoy the optimal recovery threshold (see [3]) for storage fractions of the form 1/k A and 1/k B (for the worker nodes). Furthermore, we also present approaches to trade off the straggler resilience with number of submatrices of A and B that are encoded. This paper is organized as follows. In Section II, we discuss the problem setup, related works and summarize our contributions. Then, in Section III, we present the details of our scheme and discuss our results about straggler resilience, suitability for sparse input matrices and utilization of partial computation. Next, in Section IV, we show the results of extensive numerical experiments and compare the performance of our proposed method with other methods. Finally, Section V concludes the paper with a discussion of possible future directions.\n\n\nII. PROBLEM SETUP, RELATED WORK AND SUMMARY OF CONTRIBUTIONS\n\nIn this work by matrix-computations we refer to either matrix-vector multiplication or matrix-matrix multiplication. In the case of matrix-vector multiplication, we typically partition matrix A into submatrices, generate encoded submatrices and distribute a certain number of these encoded submatrices and the vector x to n worker nodes depending on their storage capacities. Each worker node computes (in a specified order) the product of its assigned submatrices and the vector x. Each time a product is computed, it is communicated to the central node. The central node is responsible for recovering A T x once enough products have been obtained from the workers.\n\nMatrix-matrix multiplication is a more general and challenging problem. Here, we first perform a block decomposition of matrices A and B with sizes q \u00d7 u and q \u00d7 v respectively. The (i, j)-th block of A is denoted A i,j , 0 \u2264 i \u2264 q \u2212 1, 0 \u2264 j \u2264 u \u2212 1 (similar notation holds for the blocks of B). The central node carries out encoding on both the A i,j submatrices and the B i,j submatrices. In particular, although it calculates scalar linear combinations of the submatrices, it is not responsible for any of the computationally intensive matrix operations. Following this, it sends the coded submatrices of A and B to the workers. The worker nodes compute the corresponding pair-wise products (either all or some subset thereof) of the submatrices assigned to them in a specified order and send the results back to the central node which performs appropriate decoding to recover A T B.\n\n\nA. Problem Setup\n\nWe assume that the system has n workers each of which can store the equivalent of \u03b3 A = 1 kA and \u03b3 B = 1 kB fractions of matrices A and B. It is assumed that some of the worker nodes will fail or will be too slow (which is often the case in real-life clusters); the number of such nodes is assumed to be s (or less). Definition 1. We define the recovery threshold as the minimum number of workers (\u03c4 ) that need to finish their jobs such that the result A T x (for matrix-vector multiplication, where A \u2208 R t\u00d7r and x \u2208 R t ), or A T B (for matrix-matrix multiplication; where A \u2208 R t\u00d7r and B \u2208 R t\u00d7w ) can be recovered from any subset of \u03c4 worker nodes.\n\nThe recovery threshold of a scheme is said to be optimal if it is the lowest possible given the storage constraints of the worker nodes.\n\nIn our approach, we partition matrix A into \u2206 A = LCM(n, k A ) submatrices (block-columns) as A 0 , A 1 , A 2 , . . . , A \u2206A\u22121 where LCM indicates the least common multiple. We also partition matrix B into \u2206 B = k B submatrices (block-columns) as B 0 , B 1 , B 2 , . . . , B \u2206B \u22121 and set \u2206 = \u2206 A \u2206 B . We denote the number of assigned submatrices from A and B to any worker as \u2113 A and \u2113 B respectively, so \u2113 A = \u2206A kA and \u2113 B = \u2206B kB = 1. Any worker will compute all pairwise block-products, thus the worker will be responsible for computing \u2113 = \u2113 A \u2113 B = \u2113 A block-products.\n\nWe say that any submatrix A i , for i = 0, 1, . . . , \u2206 A \u2212 1, appears within a worker node as an uncoded block if A i is assigned to that worker as an uncoded submatrix. Similarly, A i is said to appear within a worker node in a coded block if a random linear combination of some submatrices including A i is assigned to that worker. At this point, we define the \"weight\" of the encoded submatrices as it will serve as an important metric for working with sparse input matrices.\n\nDefinition 2. We define the \"weight\" of the encoding as the number of submatrices that are linearly combined to arrive at the encoded submatrix.\n\nIf the \"input\" submatrices A and/or B are sparse, the encoded submatrices will be denser and the density is proportional to the weight of the encoding. Computing the product of two dense matrices is more computationally expensive than computing the product of two sparse matrices. Thus, low weight encodings are desirable to enhance the speed of overall computation in case of sparse matrices.\n\nIn each worker node there are locations numbered 0, 1, . . . , \u2113 \u2212 1 where 0 indicates the top location and \u2113 \u2212 1 the bottom location. The worker node starts processing the assigned submatrix product at the top (location 0) and then proceeds downwards to location \u2113 \u2212 1. For this system, if the central node can decode A T B from any Q block products (respecting the top-to-bottom computation order), we say that the scheme has the corresponding Q/\u2206 value. A smaller Q/\u2206 value of a system indicates that the system can utilize the partial computations of the slower workers more efficiently than a system with higher Q/\u2206 value. It is to be noted that in our problem there are a total of \u2206 submatrix products to be recovered. Hence, Q/\u2206 \u2265 1.\n\nWe shall use the terminology of symbols and submatrices (or submatrix-products) interchangeably at several places.\n\n\nB. Discussion of Related Work\n\nSeveral coded computation approaches [1]- [11] have been introduced in the literature of distributed matrix multiplication in recent years. Many of these ideas are presented in a tutorial fashion in [12]. We compare the properties of different coded matrix-multiplication schemes in Table I. Moreover in Table  II, we outline the notations used in this paper.\n\nWith storage fractions \u03b3 A = 1/k A and \u03b3 B = 1/k B and q = 1, the approach in [3] has a threshold \u03c4 = k A k B which is shown to be optimal [3]. It proceeds by creating encoded matrix polynomials whose coefficients correspond to the blocks of the input matrices and subsequently evaluating them at distinct points. The decoding process corresponds to polynomial interpolation. Moreover, there are other variants of polynomial code-based works [7], [20], random code-based approaches [8], or convolutional code-based methods [6], [10] which are also resilient to optimal number of stragglers. It should be noted that there are several other approaches [13], [16], [23] which are sub-optimal in terms of straggler resilience.\n\nThere are some works [2], [4], [24], [25] which consider the case of q > 1 in the block decomposition of the input TABLE I: Comparison with existing works on distributed matrix-matrix multiplications. We did not add the works of [6], [16], [21], [22] in this table since they are applicable for matrix-vector multiplication only.   CODES  OPTIMAL  NUMERICAL  PARTIAL  SPARSELY  THRESHOLD? STABILITY? COMPUTATION?  CODED? REPETITION CODES \u2717 \u2713 \u2717 \u2713 PROD. CODES [23], FACTORED CODES [11] \u2717 \u2713 \u2717 \u2717 POLYNOMIAL CODES [3] \u2713 \u2717 \u2717 \u2717 BIV. HERMITIAN POLY. CODE [20] \u2713 \u2717 \u2713 \u2717 ORTHOPOLY [7], RKRP CODE [8] \u2713 \u2713 \u2717 \u2717 CONV. CODE [10], CIRC. & ROT. MAT. [9] \u03b2-LEVEL CODING [14] \u2717 \u2713 \u2713 \u2713 SCS OPTIMAL SCHEME [14] \u2713  \u2113u + \u2113c = \u2206A/kA matrices. While this can reduce the recovery threshold as compared to the case of q = 1, it comes at the cost of increased computational load at each of the workers. Moreover the communication load between the worker node and the central node also increases.\n\u2713 \u2713 \u2713 Proposed Scheme \u2713 \u2713 \u2713 \u2713s = n \u2212 \u03c4 x RELAXATION IN NUMBER OF STRAGGLERS x = sm \u2212 s y REDUCTION OF WEIGHTS IN CODED SUBMATRICES OF A y = \u230a k A x sm \u230b Q NUMBER\nWhile much of the initial work on coded computation focused on the recovery threshold, subsequent work has identified other metrics that need to be considered as well. Here we discuss several such other concerns that have been discussed in this literature. Sparsity of the \"input\" matrices: There are practical applications in machine learning, optimization and other areas where the underlying matrices A and B are sparse. If we want to compute the inner product of two n-length vectors a and x where a has around \u03b4n (0 < \u03b4 \u226a 1) non-zero entries, it takes \u2248 2\u03b4n floating point operations (flops) as compared to \u2248 2n flops in the dense case where \u03b4 \u2248 1. The linear encoding in several prior approaches [3], [4], [7], [8], [10], [20] is dense, i.e., it significantly increases the number of non-zero entries in the encoded matrices which are assigned to the worker nodes. For example, in the approach of [3] the encoded matrices of A (respectively B) are obtained by linearly combining k A (respectively k B ) submatrices. This in turn can cause the worker computation time to increase by up to k A k B times, i.e., the advantage of distributing the computation may be lost. This underscores the necessity of considering schemes where the encoding only combines a limited number of submatrices. Numerical stability: Another major issue of the polynomial based approaches [3] is numerical instability. It should be noted that the encoding and decoding algorithms within coded computation operate over the real field, although the corresponding techniques are borrowed from classical coding theory. Unlike the finite field, the recovery from a system of equations can be quite inaccurate in the real field if the corresponding system matrix is ill-conditioned. The polynomial code approach [3] uses Vandermonde matrices for the encoding process which are well-recognized to be illconditioned [26].\n\nA number of prior works [7]- [10], [19], [27] have addressed this issue and emphasized that the worst case condition number (\u03ba worst ) of the decoding matrices over all different choices of s stragglers is an important metric to be optimized. The work of [7] presents an approach within the basis of orthogonal polynomials, and demonstrates that \u03ba worst of their schemes is at most O(n 2s ). The approach in [10] proposes a random convolutional code based approach, and provides a computable upper bound on \u03ba worst , while the work of [9] leverages the properties of rotation matrices and circulant permutation matrices to upper bound \u03ba worst by O(n s+5.5 ). In terms of numerical stability, [9] provides the best results in numerical experiments. The work in [8] presents an approach where they take random linear combinations of the submatrices to generate the coded submatrices (this was also suggested in Remark 8 of [3]) and shows the improvement over the polynomial code approach. Some other approaches [6], [19] address this issue, but they are applicable to matrix-vector multiplication only. Partial Stragglers: The third issue in distributed computations is that several approaches [3], [7]- [10] treat stragglers as erasures; in other words they assume that no useful information can be obtained from the slower worker nodes. But some recent works [14], [15], [20], [22] consider that a slow worker may not be a useless worker; rather exploiting these partial computations can enhance the speed of the overall job. In these approaches, multiple jobs are assigned to each of the worker nodes, so that the central node can leverage the partial computations. This naturally leads to the Q/\u2206-metric discussed previously; it was introduced in [22] and discussed in-depth in [14]. We present a detailed comparison with [14] in Section IV.\n\n\nC. Summary of Contributions\n\nThe contributions of our work can be summarized as follows.\n\n\u2022 For a system with n workers each of which can store \u03b3 A = 1 kA fraction of matrix A and \u03b3 B = 1 kB fraction of matrix B, we propose a coded matrix-matrix multiplication scheme which (i) is optimal in terms of straggler resilience (s = n \u2212 k A k B ); (ii) can utilize the partial computations done by the slower worker nodes; and (iii) enhances the worker computation speed when the \"input\" matrices A and B are sparse. Specifically, several of the assigned submatrices in our scheme our uncoded. \u2022 Our work allows us to trade off the straggler resilience with the weight of the encoding scheme. If the recovery threshold is relaxed to \u03c4 = k A k B + x, then we can further reduce the weight of the encoded A submatrices while ensuring that the number of uncoded A submatrices remains the same (as in the optimal threshold case). We show that the coded submatrices will be linear combinations of k A \u2212 y uncoded submatrices; where y = \u230a kAx sm \u230b. Thus the worker computation speed can be enhanced in comparison to the case of \u03c4 = k A k B when x = 0. \u2022 We provide upper and lower bounds on the value of Q for our scheme. We show that for x = 0, the bounds are the same. Moreover, we have demonstrated several numerical examples which show that the difference between the bounds is small even when x > 0. \u2022 Our theoretical results are supported by extensive numerical experiments conducted on AWS clusters. Fig. 1, depicts such a comparison in terms of overall computation time required by different approaches for sparse input matrices in a system of n = 24 worker nodes. We have simulated the stragglers in such a way that the slower workers have one-fifth of the speed of the non-straggling nodes. From Fig. 1, it can be verified that our proposed approach requires significantly less overall computation time than the dense coded approaches when the \"input\" matrices are sparse.\n\n\nD. Motivating Example\n\nExample 1. Consider distributed matrix-matrix multiplication with n = 5 workers each of which can store \u03b3 A = \u03b3 B = 1 2 portions of matrices A and B, respectively. The job assignment according to our proposed approach is shown in Fig. 2, where matrices A and B are partitioned into \u2206 A = 10 and \u2206 B = 2 submatrices, respectively. In this case, each worker is responsible for computing all its corresponding pairwise block-products in a natural top-to-bottom order, e.g., worker\nW 0 computes A T 0 (r 0 B 0 + r 1 B 1 ) followed by A T 1 (r 0 B 0 + r 1 B 1 ), . . . , (c 0 A 4 +c 1 A 9 ) T (r 0 B 0 +r 1 B 1 )\n. Thus, each worker computes four uncoded-coded and one coded-coded block products. It can be verified that for these parameters, both the polynomial code approach and the approach in Fig. 2 are resilient to one straggler.\n\nWe emphasize that the approach in Fig. 2 has some notable advantages over the polynomial code approach. Suppose that matrices A and B are sparse. In the polynomial code approach, the encoded submatrices are approximately twice as dense as the original matrices. Thus, computing the product of the corresponding encoded matrices will take about twice as much time as compared to uncoded-coded block products in Fig. 2.\n\nMoreover, the latter approach can utilize the partial calculations done by the slower workers. The polynomial code approach needs at least four workers to fully finish their respective assigned jobs. On the other hand, in Fig. 2, the final result can be recovered as soon as any Q = 23 block products are computed over all the workers according to the assigned computation order, where we recall that the central node needs to recover twenty submatrix products of the form\nA T i B j , 0 \u2264 i \u2264 9, 0 \u2264 j \u2264 1.\nThus, our approach allows us to leverage partial computations in scenarios where workers have differing speeds. It should be noted that the approach in [14] provides Q = 21 for the same system, however, the coding for matrix A is denser in case of [14], which can lead to higher worker computation time.\n\n\nIII. MATRIX-MATRIX MULTIPLICATION SCHEME\n\nWe now describe our proposed matrix-matrix multiplication scheme, beginning with an illustrative example that describes the encoding scheme and outlines the decoding scheme in case of worker node failures.\n\n\nA. An illustrative example\n\nConsider the example in Fig. 3 where the system has 12 worker nodes each of which can store 1/3-rd fraction of A and 1/3-rd fraction of B. First we partition A into 12 blockcolumns, and assign three uncoded and one coded submatrix of A to each of the worker nodes. The coded submatrices are always linear combinations of three uncoded submatrices. Similarly, we partition B into three block-columns, and assign \n\n\nNumber of slower workers\n\nOverall computation time (in s) Polynomial code [3] Ortho-Poly Code [7] RKRP Code [8] SCS optimal Scheme [14] Proposed Scheme \n\n\nNumber of slower workers\n\nOverall computation time (in s) Polynomial code [3] Ortho-Poly Code [7] RKRP Code [8] SCS optimal Scheme [14] Proposed Scheme Fig. 1: Comparison among different coded approaches in terms of overall computation time for different number of slower worker nodes when the \"input\" matrices are 98% sparse (left) or 95% sparse (right). The system has n = 24 worker nodes each of which can store \u03b3A = 1/4 and \u03b3B = 1/5 fraction of matrices A and B, respectively, so the recovery threshold, \u03c4 = 20. The slower workers are simulated in such a way so that they have one-fifth of the speed of the non-straggling workers. only one coded B submatrix to each of the worker nodes; these are always linear combinations of two uncoded B submatrices. There is a particular top-to-bottom order in which the tasks are executed within each worker node, e.g., in W 0 it is\nW 0 W 1 W 2 W 3 W 4 A 0 A 1 A 2 A 3 c 0 A 4 + c 1 A 9 r 0 B 0 + r 1 B 1 A 2 A 3 A 4 A 5 c 2 A 6 + c 3 A 1 r 2 B 0 + r 3 B 1 A 4 A 5 A 6 A 7 c 4 A 8 + c 5 A 3 r 4 B 0 + r 5 B 1 A 6 A 7 A 8 A 9 c 6 A 0 + c 7 A 5 r 6 B 0 + r 7 B 1 A 8 A 9 A 0 A 1 c 8 A 2 + c 9 A 7 r 8 B 0 + r 9 B 1A T 0 (r 0 B 0 +r 1 B 1 ), A T 1 (r 0 B 0 +r 1 B 1 ), A T 2 (r 0 B 0 +r 1 B 1 ) and finally (c 00 A 3 + c 01 A 7 + c 02 A 11 ) T (r 0 B 0 + r 1 B 1 ),\nwhere c 00 , c 01 and c 02 represent the random coefficients for the encoded A submatrix in worker W 0 .\n\nTo better understand the structure of the encoding scheme, consider the following class decomposition of A submatrices:\nC 0 = {A 0 , A 4 , A 8 }, C 1 = {A 1 , A 5 , A 9 }, C 2 = {A 2 , A 6 , A 10 }, C 3 = {A 3 , A 7 , A 11 }.\nThe assignment of encoded A submatrices to each worker node are performed in a \"cyclic\" fashion, whereby a representative from each class is chosen for an uncoded assignment and a random linear combination of all members of the class is chosen for the coded assignment. For, instance in W 0 the three uncoded assignments are from (top-to-bottom) from classes C 0 , C 1 and C 2 whereas the coded assignment is a random linear combination of the members of C 3 . The sequence is shifted cyclically in W 1 and continues in a similar manner. The encoding of the B submatrices also follows a cyclic-pattern whereby W 0 contains a random linear combination of B 0 and B 1 , W 1 contains a random linear combination of B 1 and B 2 and so on.\nW0 W1 W2 W3 W4 W5 A0 A1 A2 {A3, A7, A11} r0B0 + r1B1 A1 A2 A3 {A4, A8, A0} r2B1 + r3B2 A2 A3 A4 {A5, A9, A1} r4B2 + r5B0 A3 A4 A5 {A6, A10, A2} r6B0 + r7B1 A4 A5 A6 {A7, A11, A3} r8B1 + r9B2 A5 A6 A7 {A8, A0, A4} r10B2 + r11B0 W6 W7 W8 W9 W10 W11 A6 A7 A8 {A9, A1, A5} r12B0 + r13B1 A7 A8 A9 {A10, A2, A6} r14B1 + r15B2 A8 A9 A10 {A11, A3, A7} r16B2 + r17B0 A9 A10 A11 {A0, A4, A8} r18B0 + r19B1 A10 A11 A0 {A1, A5, A9} r20B1 + r21B2 A11 A0 A1 {A2, A6, A10} r22B2 + r23B0\nIt can be verified that each class (for example, C 0 ) appears in each worker node either as an uncoded assignment or as part of a coded assignment.\n\nIt turns out that this scheme is resilient to the failure of any s = 3 workers. While a general proof requires more ideas, we illustrate the recovery process by means of an example. Suppose W 0 , W 10 and W 11 are failed so that all uncoded assignments of A 0 are lost. Note that A 0 \u2208 C 0 and since we only encode the A submatrices within a certain class, it suffices for us to examine those submatrix products where class C 0 participates. From Fig. 3 we can see that A 4 \u2208 C 0 and A 8 \u2208 C 0 participate in an uncoded manner in W 2 , W 3 , W 4 and W 6 , W 7 , W 8 respectively. The coded assigments corresponding to C 0 appear in W 1 , W 5 and W 9 .\n\nConsider the following encoding matrices (with permuted columns for ease of viewing) that represent the coding coefficients for the submatrices of A and B respectively. The columns in left-to-right order correspond to worker nodes specified in the expression for W in (3) below.\nG A = \uf8ee \uf8f0 Uncoded A0 1 1 1 Uncoded A4 0 0 0 Uncoded A8 0 0 0 Coded C0 * * * 0 0 0 1 1 1 0 0 0 * * * 0 0 0 0 0 0 1 1 1 * * * \uf8f9 \uf8fb ,(1)G B = \uf8ee \uf8f0 * 0 * * * 0 * 0 * 0 * * * * 0 0 * * * * 0 * 0 * 0 * * * 0 * 0 * * * * 0 \uf8f9 \uf8fb ,\n(2) W = 0 10 11 2 3 4 6 7 8\n1 5 9 .(3)\nThe asterisks in the above equations represent i.i.d. random choices from a continuous distribution. Suppose for instance that W 0 , W 10 and W 11 are stragglers. In this case one can observe that from W 2 , W 3 and W 4 we can recover the products A T 4 B j for j = 0, 1, 2. This is because the corresponding system of equations for these three unknowns looks like \uf8ee\n\uf8f0 * * 0 0 * * * 0 * \uf8f9 \uf8fb ,(4)\nwhere the asterisks represent the chosen i.i.d. random coefficients. It can be seen that this matrix will be full rank with probability-1. In a similar manner we can argue that from W 6 , W 7 and W 8 we can recover the products A T 8 B j , j = 0, 1, 2. Following this, workers W 1 , W 5 and W 9 allow the recovery of A T 0 B j , j = 0, 1, 2. A more involved argument made in Section III-E shows that in fact under any pattern of three stragglers the product A T B can be recovered.\n\nThe Q/\u2206 analysis is a bit more subtle. We divide the workers into three groups:\nG 0 = {W 0 , W 1 , W 2 , W 3 }, G 1 = {W 4 , W 5 , W 6 , W 7 } and G 3 = {W 8 , W 9 , W 10 , W 11 }.\nWithin each group each class appears at all possible locations, e.g. within G 0 , C 0 appears at location-0 in W 0 , location-3 in W 1 , location-2 in W 2 and location-1 in W 3 and so on. The appearances of C m in all different locations within a group allows us to leverage the properties of the cyclic assignment as done previously in [14], [22] and arrive at corresponding upper and lower bounds for Q/\u2206. We note here that the bounds match for x = 0. As this is much more involved, we defer the argument to Section III-E.\n\n\nB. Overview of Alg. 1\n\nWe now discuss the scheme specified formally in Algorithm 1. The symbols and notation introduced in Algorithm 1 are summarized in Table II.\n\n\nWeight of the linear combination of A and B submatrices:\n\nNote that s m = n \u2212 k A k B is the maximum number of stragglers that the scheme can be resilient to, whereas we want resilience to s \u2264 s m stragglers. Line 1 in Alg. 1 sets Algorithm 1: Proposed scheme for distributed matrixmatrix multiplication Input : Matrices A and B, n-number of worker nodes, s-number of stragglers, storage\nfraction \u03b3 A = 1 kA and \u03b3 B = 1 kB ; s \u2264 s m = n \u2212 k A k B . 1 Set x = s m \u2212 s and y = \u230a kAx sm \u230b; 2 Set \u2206 A = LCM(n, k A ) and \u2206 B = k B and Partition A and B into \u2206 A and \u2206 B block-columns, respectively; 3 Set \u2206 = \u2206 A \u2206 B , p = \u2206 n and \u2113 = \u2206A kA ; 4 Number of coded submatrices of A in each worker node, \u2113 c = \u2113 \u2212 p; 5 Set \u03c9 = 1 + \u2308 sm kB \u2309 and \u03b6 = 1 + k B \u2212 kB \u03c9 ; 6 Define C i = A i , A \u2113+i , . . . , A (kA\u22121)\u2113+i , and \u03bb i = 0, for i = 0, 1, . . . , \u2113 \u2212 1; 7 for i \u2190 0 to n \u2212 1 do 8 u \u2190 i \u00d7 \u2206A n ; 9 Define T = {u, u + 1, . . . , u + p \u2212 1} (modulo \u2206 A ); 10\nAssign all A m 's sequentially from top to bottom to worker node i, where m \u2208 T ;\n11 for j \u2190 0 to \u2113 c \u2212 1 do 12 v \u2190 u + p + j (mod \u2113); 13\nDenote Y \u2208 C v as the set of the element submatrices at locations (modulo k A )\n\u03bb v , \u03bb v + 1, \u03bb v + 2, . . . , \u03bb v + k A \u2212 y \u2212 1 of C v ; 14 Assign a random linear combination of A q 's where A q \u2208 Y; 15 \u03bb v \u2190 \u03bb v + k A \u2212 y (modulo k A ); 16 end 17 Define V = {i, i + 1, . . . , i + \u03b6 \u2212 1} (modulo \u2206 B ); 18\nAssign a random linear combination of B q 's where B q \u2208 V; 19 end Output : n, \u03b3 A , \u03b3 B -scheme for distributed matrix-matrix multiplication.\n\nthe parameter x = s m \u2212 s. Thus, x measures the relaxation of the straggler resilience that we are able to tolerate. This allows us to reduce the weight of the linear combination of the A submatrices. In particular, let y = \u230a kAx sm \u230b. Then, our algorithm combines at most k A \u2212 y submatrices of A.\n\nThe encoded submatrices of B are obtained by combining \u03b6 submatrices of {B 0 , B 1 , . . . , B \u2206B\u22121 }. Line 5 specifies the assignment of \u03b6; it can be observed that\n\u03b6 \u2264 \u2206 B = k B . Assignment of encoded submatrices of A: We further di- vide the set {A 0 , A 1 , . . . , A \u2206A\u22121 } into \u2113 disjoint classes C 0 , C 1 , . . . , C \u2113\u22121 , i.e., C m = A m , A \u2113+m , A 2\u2113+m , . . . , A (kA\u22121)\u2113+m . (5)\nThis implies that |C m | = k A , for m = 0, 1, . . . , \u2113 \u2212 1, and submatrix A i belongs to C i (mod \u2113) .\n\nThe worker nodes are assigned submatrices from each class C m , 0 \u2264 m \u2264 \u2113 \u2212 1 in a block-cyclic fashion; the block shift is specified by \u2206 A /n (line 8). In each worker node, the first p = \u2206/n assignments are uncoded, i.e., they correspond to a specific element of the corresponding class. The remaining \u2113 c = \u2113 \u2212 p assignments are coded. Each coded assignment corresponds to random linear combination of an appropriate (k A \u2212 y)-sized subset of the corresponding class. This is discussed in line 8 -16 in Alg. 1.\n\nAs each location of every worker node is populated by a submatrix from a class C m where 0 \u2264 m \u2264 \u2113 \u2212 1, we will occasionally say that the class C m appears at a certain location (between 0 to \u2113 \u2212 1) at a certain worker node. To ensure that each submatrix of C m participates in \"almost\" the same number of coded assignments, we use a counter \u03bb i to keep track of the linear combination that will be formed from the corresponding class\nC i , 0 \u2264 i \u2264 \u2113 \u2212 1 (lines 6, 13 -15 in Alg. 1).\nIn Section III-A, we discussed an example where x = 0 so that y = 0. Therefore, the encoded A matrices combine all the three submatrices within the respective classes. There are p = \u2206/n = 3 uncoded A submatrices and one coded A submatrix in each worker node.\n\n\nAssignment of encoded submatrices of B: For worker\nW i , consider the set V = {i, i + 1, . . . , i + \u03b6 \u2212 1} (mod \u2206 B ).\nA random linear combination of B k for k \u2208 V is assigned to worker W i . We note here that \u03b6 \u2264 k B and can in fact be as small as \u2308k B /2\u2309 depending upon the values of k B and s m (see line 5 of Alg. 1).\n\nFor instance, in Fig. 3, s m = 3 and k B = 3, so that \u03c9 = 2 which implies that \u03b6 = 2. Thus, for instance for worker W 8 ,\nthe set V = {8, 9} (mod \u2206 B ) = {2, 0} (where \u2206 B = k B )\nand it is assigned a random linear combination of B 2 and B 0 . Moreover, the patterns repeats periodically. Order of jobs: Note that each worker node is only assigned one encoded B submatrix. Each worker node computes the product of its assigned A submatrices with the corresponding encoded B submatrix in the top to bottom order.\n\nIn the following subsections we point out certain \"structural\" properties of our scheme. In the presence of s stragglers, suppose that there is a submatrix A T i B j where A i \u2208 C m that we cannot decode. Our scheme is such that we can just focus on the equations where C m participates. This provides a manageable subset of equations where we can focus our attention. Different properties of the scheme (Lemma 1 and Claim 2) allow us to assert that the overall system of equations seen by submatrices A T i B j where A i \u2208 C m and j = 0, 1, . . . , \u2206 B \u2212 1 is full-rank even in the presence of s stragglers (Theorem 1).\n\n\nC. Coding for Matrix A\n\nLet U i denote the subset of worker nodes where A i appears in an uncoded block, for i = 0, 1, . . . , \u2206 A \u2212 1. Likewise, V i denotes the subset of worker nodes where A i appears in a coded block. Our first claim states that the number of coded appearances of any two submatrices in a class can differ by at most one. The detailed proof is given in Appendix A. Claim 1. If the jobs are assigned to the workers according to Alg. 1, for any A i , A j \u2208 C m , Example 2. To clarify the idea of the proof of Lemma 1(ii), We consider an example with distributed matrix-vector multiplication (which is equivalent to k B = 1 in distributed matrix-matrix multiplication) in Fig. 4 where we have a system with n = 5 workers, \u03b3 A = 1 3 and s = 1. We consider the class, C 0 = {A 0 , A 5 , A 10 }. It can be verified from Fig. 4 that\n|V i | \u2212 |V j | \u2264 1. W 0 W 1 W 2 W 3 W 4 A0 A1 A2 {A3, A8} {A4, A9} x A3 A4 A5 {A1, A6} {A2, A7} x A6 A7 A8 {A14, A4} {A0, A5} x A9 A10 A11 {A12, A2} {A13, A3} x A12 A13 A14 {A10, A0} {A11,\u00b5 0 = |V 0 | + |V 5 | + |V 10 | 3 = 2 + 1 + 1 3 = 4 3 .\nSo, \u230a\u00b5 0 \u230b = 1 and \u2308\u00b5 0 \u2309 = 2; which satisfies the inequality \u230a\u00b5 0 \u230b \u2264 |V 0 |, |V 5 |, |V 10 | \u2264 \u2308\u00b5 0 \u2309.\n\nThus, |V i | \u2265 1 = s; for any i = 0, 1, . . . , \u2206 A \u2212 1.\n\nThe following corollary states that the submatrices in C m are assigned to k A k B distinct workers as uncoded blocks and to the remaining s m = n \u2212 k A k B workers as coded blocks. The proof appears in Appendix C.\nCorollary 1. If C m = A m , A \u2113+m , . . . , A (kA\u22121)\u2113+m , then (i) \u222a i:Ai\u2208Cm U i = k A k B and \u222a i:Ai\u2208Cm V i = s m ; (ii) \u222a i:Ai\u2208Cm U i \u2229 \u222a i:Ai\u2208Cm V i = \u2205 .\n\nD. Coding for Matrix B\n\nTo discuss the coding for matrix B, first we consider a k B \u00d7 n matrix, where each column has \u03b6 \u2264 k B non-zero entries which are chosen i.i.d. from a continuous distribution. Moreover, the indices of non-zero entries are consecutive and shifted in a cyclic fashion, reduced modulo k B . For example, if we have a system with n = 12 workers with k A = 2 and k B = 5, then \u03b6 = 3 and the corresponding coding matrix for B, denoted as R B kB ,n , can be written as\nR B kB ,n = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 * 0 0 * * * 0 0 * * * 0 * * 0 0 * * * 0 0 * * * * * * 0 0 * * * 0 0 * * 0 * * * 0 0 * * * 0 0 * 0 0 * * * 0 0 * * * 0 0 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb ;(6)\nwhere * indicates the non-zero entries. The entries at indices i, i + 1, . . . , i + \u03b6 \u2212 1 (reduced modulo k B ) are non-zero (chosen i.i.d. from a continuous distribution) within column i of R B kB ,n and the other entries are set to zero. The non-zero coefficients are used to specify the random linear combination of the submatrices of B assigned to worker W i .\n\n\nDefinition 3.\n\nA type i submatrix, for i = 0, 1, 2 . . . , k B \u2212 1, is a random linear combination of the submatrices, B i , B i+1 , . . . , B i+\u03b6\u22121 (indices reduced modulo k B ). Thus we can say that worker node W j is assigned a type j (mod k B ) submatrix (line 18 of Alg. 1).\n\n\nConsider the case of x = 0 and any\nA i , i = 0, 1, 2, . . . , \u2206 A \u2212 1. From Lemma 1, we know |U i | = k B and |V i | = s m (since x = 0, s = s m ).\nThus A i appears at \u03c3 = k B + s m worker nodes. We now investigate the types (cf. Def. 3) of the coded submatrices of B in those \u03c3 worker nodes. The following claim specifies the types of those B submatrices, and the proof is given in Appendix D.\n\nClaim 2. Consider the construction in Alg. 1 with x = 0 and let k be the minimum index of the worker node where A i appears (uncoded or coded) and consider the worker nodes in U i \u222a V i . The assigned submatrices of B for those worker nodes are, respectively, from types k, k +1, k +2, . . . , k +\u03c3 \u22121 (reduced modulo k B ), which are \u03c3 consecutive types. \n= k B + s m . If \u03b6 > k B \u2212 kB \u03c9 , any k B \u00d7 k B submatrix of R i is full rank, where \u03c9 = 1 + \u2308 sm kB \u2309. Proof.\nThe proof is given in Appendix E.\n\nFor any class C m , the encoded submatrices of A within different worker nodes can be specified in terms of a k A \u00d7 n \"generator\" matrix, e.g., in Fig. 2, the column of the generator matrix for worker W 0 corresponding to C 4 = {A 4 , A 9 } will be [c 0 c 1 ] T . Similarly the encoded submatrices of B within different worker nodes can be specified in terms of a k B \u00d7 n \"generator\" matrix, e.g., in Fig. 2, the column of the generator matrix for worker W 0 will be [r 0 r 1 ] T . We use this formalism in the discussion below. Theorem 1. Assume that a system has n worker nodes each of which can store 1/k A and 1/k B fraction of matrices A and B, respectively. In each worker, according to Alg. 1, we assign some uncoded A submatrices and some coded A submatrices with weight k A \u2212 y , where s m = n \u2212 k A k B and y = \u230a kAx sm \u230b. We also assign a coded B submatrix to each worker which has a weight \u03b6, as described in Lemma 2. Then, this distributed matrix-matrix multiplication scheme will be resilient to s = s m \u2212 x stragglers.\n\nProof. We assume that there are s = s m \u2212 x stragglers, and we cannot recover an unknown A T i B j from the remaining \u03c4 = k A k B + x workers. Let A i \u2208 C m , for some m = 0, 1, 2, . . . , \u2113 \u2212 1. From Lemma 1 part (i), |U m | = |U \u2113+m | = \u00b7 \u00b7 \u00b7 = |U (kA\u22121)\u2113+m | = k B . Thus, from Corollary 1, without loss of generality, by permuting the columns appropriately, the k A \u00d7 n generator matrix for the corresponding submatrices of C m can be expressed as\nG A = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 1 kB 0 . . . 0 0 1 kB . . . 0 0 0 . . . 0 R A kA,sm . . . . . . . . . . . . 0 0 . . . 1 kB \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb ,\nwhere 1 kB represents an all-one row-vector of length k B . An example of this is shown in Section III-A. Here R A kA,sm is a random matrix of size k A \u00d7 s m whose each column has k A \u2212 y non-zero entries (and each row has at least s = s m \u2212 x non-zero entries) where y is defined in Line 1 in Alg. 1. The first k A k B columns of G A denote the uncoded submatrices of C m and the next s m = n \u2212 k A k B columns denote the coded submatrices. Similarly, the generator matrix for the corresponding coded submatrices of B is G B = R B kB ,n (as mentioned in (6)). Thus, the generator matrix for the unknowns of the form A T \u03b1 B \u03b2 (where A \u03b1 \u2208 C m , \u03b2 = 0, 1, . . . , \u2206 B \u22121) is given by G = G A \u2299 G B (\u2299 denotes the Khatri-Rao product [28] which corresponds to column-wise Kronecker product) which is of size k A k B \u00d7 n. The following lemma states a relevant rank property of G, and the corresponding proof is given in Appendix F.\nLemma 3. Any k A k B \u00d7 \u03c4 submatrix of G has a rank k A k B with probability 1, where \u03c4 = k A k B + x.\nThe unknowns corresponding to C m can be represented in terms of the following Kronecker product as\ny = A T m A T \u2113+m . . . A T (kA\u22121)\u2113+m \u2297 B 0 B 1 . . . B kB \u22121 = A T m B 0 A T m B 1 . . . . . . A T (kA\u22121)\u2113+m B kB \u22121 ,\nthus there are k A k B such unknowns of the form A T \u03b1 B \u03b2 , where A \u03b1 \u2208 C m . Note that A T i B j is also one of them which is assumed to be not decodable from the \u03c4 workers. But from Lemma 3, we can show that any k A k B \u00d7 \u03c4 submatrix of G has a rank k A k B with probability 1, which indicates that all k A k B unknowns corresponding to C m can be recovered from any \u03c4 workers. This contradicts our assumption that A T i B j is not decodable. Let us consider the example in Fig. 2, and apply the argument for unknown A T 0 B 0 . Here we have \u2113 = 5, so A 0 \u2208 C 0 = {A 0 , A 5 }. Thus to recover the unknowns of the form A T \u03b1 B \u03b2 corresponding to C 0 , we have the corresponding generator matrices as G A = 1 1 0 0 c 6 0 0 1 1 c 7 , and G B = r 0 r 8 r 2 r 4 r 6 r 1 r 9 r 3 r 5 r 7 ;\n\nwhere the columns correspond to W 0 , W 4 , W 1 , W 2 and W 3 , respectively. Next we can have the generator matrix G = G A \u2299 G B having a size 4 \u00d7 5 whose any 4 \u00d7 4 square submatrix is full-rank. Thus we can recover the unknowns,\nA T 0 B 0 , A T 0 B 1 , A T 5 B 0 and A T 5 B 1 from\n\nthe results returned by any four workers.\n\nWe now present the result of our work on utilizing the partial computations. It provides the calculation of the value of Q for our scheme for different system parameters. Before stating the corresponding theorem, we state the following claim, which follows from [14]. The proof is detailed in Appendix G.\n\nClaim 3. Assume that the jobs are assigned to the workers according to Alg. 1, and consider any class C m , for m = 0, 1, . . . , \u2113 \u2212 1. The maximum number of submatrix-products that can be acquired from the job assignments where C m appears exactly \u03ba \u2212 1 times is\n\u03b7 = n(\u2113 \u2212 1) 2 + c c1\u22121 i=0 (\u2113 \u2212 i) + c 2 (\u2113 \u2212 c 1 ) ;\nwhere c = n \u2113 , c 1 = \u230a \u03ba\u22121 c \u230b, and c 2 = \u03ba \u2212 1 \u2212 cc 1 .\n\nTheorem 2. Alg. 1 proposes a distributed matrix-matrix multiplication scheme which provides Q such that Q lb \u2264 Q \u2264 Q ub .\n\nHere the bounds are given by\nQ ub = n(\u2113 \u2212 1) 2 + c c x 1 \u22121 i=0 (\u2113 \u2212 i) + c x 2 (\u2113 \u2212 c x 1 ) + 1 ; and Q lb = n(\u2113 \u2212 1) 2 + c c 0 1 \u22121 i=0 (\u2113 \u2212 i) + c 0 2 (\u2113 \u2212 c 0 1 ) + s m y k A + 1 ; where c = n \u2113 , c x 1 = kAkB +x\u22121 c , c x 2 = k A k B + x \u2212 1 \u2212 cc x 1 and y = kAx sm .\nProof. The proof of this theorem is given in Appendix H.\n\nWhen x = 0, then \u03c4 = k A k B , c x 1 = c 0 1 and c x 2 = c 0 2 , hence Q lb = Q ub = Q. Let us consider a special case, where n and k A are co-prime. In that case, n and c = 1. Moreover, c 0 1 = k A k B \u2212 1 and c 0 2 = 0. Thus, we have\n\u2206 A = n\u00d7k A , so \u2113 = \u2206 A /k A =Q = n(n \u2212 1) 2 + kAkB \u22121 i=0 (n \u2212 i) + 1 = n(n \u2212 1) 2 + nk A k B \u2212 k A k B (k A k B \u2212 1) 2 + 1 = nk A k B + (n \u2212 k A k B )(n + k A k B \u2212 1) 2 + 1 \u2248 \u2206 + s m \u00d7 2k A k B 2 + 1 when s m = n \u2212 k A k B is very small. Thus, we have Q \u2206 \u2248 1 + sm\nn . If s m \u226a n, then Q/\u2206 is very close to 1, which indicates that, in this special case, the proposed scheme can efficiently utilize the partial computations done by the slower workers.\n\nIt should be noted that the trivial lower bound of Q is \u2206. This can be achieved directly by assigning multiple evaluations in many of the dense coded approaches [3], [7], [8]. But, the issue here is sparsity. The weights of the encoded A and B matrices in those approaches are k A and k B , respectively, which can destroy the inherent sparsity of the matrices, so the worker computation time can go up significantly.\n\nIn our proposed approach, the value of Q is slightly more than \u2206. However, we assign many uncoded A submatrices and we reduce the weight of B submatrices, which help to preserve the sparsity of A and B and can reduce the worker computation time. Thus we can gain in overall computation time as shown in Fig. 1 even though we lose a small amount in the Q/\u2206 metric.  Table III for different values of x. For x = 0, the recovery threshold is 6, and Q lb = Q ub = Q = 59. Moreover, for x = 1, the recovery threshold is 7 and Q lb \u2264 Q \u2264 Q ub where Q ub \u2212 Q lb = 2.\n\n\nF. Dealing with sparse input matrices\n\nWe now discuss the performance of our scheme when the input matrices are sparse. In our algorithm, among the \u2113 submatrices of A, we assign \u2113 u = \u2206A\u2206B n uncoded submatrices, where \u2206 B = k B ; the rest \u2113 c = \u2113 \u2212 \u2113 u submatrices are coded, i.e.,\n\u2113 c \u2113 = 1 \u2212 \u2113 u \u2113 = 1 \u2212 \u2206 A \u2206 B /n \u2206 A /k A = 1 \u2212 k A k B n = s m n .\nThe usual assumption is that s m \u226a n. This indicates that a small portion of the whole storage capacity for A is allocated for the coded submatrices. Thus, the worker nodes will take less time to compute their assigned block-products in our proposed approach. We clarify this with an example below. Consider that A \u2208 R t\u00d7r and B \u2208 R t\u00d7w are two sparse random matrices, where the entries are chosen independently to be non-zero with probability \u03b7. Thus, when we obtain a coded submatrix as the linear combination of k A submatrices of A, the probability of any entry to be non-zero is approximately k A \u03b7 (here we assume \u03b7 is small). Similarly, the probability of any entry in a coded submatrix of B to be non-zero is approximately k B \u03b7, if it is obtained by a linear combination of k B submatrices. Now for the dense coded approaches [3], [7], [8], every worker node stores 1/k A and 1/k B fractions of matrices A and B, and thus the computational complexity of every worker node is approximately\nO (\u03b7k A \u03b7k B \u00d7 t) \u00d7 r kA w kB = O \u03b7 2 \u00d7 rwt .\nIn our proposed approach with x = 0, the fraction of uncoded A submatrices is kAkB n and the remaining sm n fraction is coded and obtained from linear combination of k A submatrices. Moreover, the coded submatrix for B is obtained by a random linear combination of \u03b6 uncoded submatrices. Thus, the computational complexity for a worker node to compute the block product between an uncoded\nA and coded B submatrix is O (\u03b7 \u00d7 \u03b7\u03b6 \u00d7 t) r \u2206A w kB = O \u03b7 2 \u00d7 rwt \u00d7 \u03b6 \u2206A\u2206B .\nSimilarly, the computational complexity for a worker node to compute the block product between a coded A and coded B submatrix is\nO (\u03b7k A \u00d7 \u03b7\u03b6 \u00d7 t) r \u2206A w kB = O \u03b7 2 \u00d7 rwt \u00d7 \u03b6kA \u2206A\u2206B .\nSince the workers need to compute p uncoded-coded and \u2113\u2212p codedcoded block products, the total computational complexity for every worker node in our approach is approximately\np \u00d7 O \u03b7 2 \u00d7 rwt \u00d7 \u03b6 \u2206 A \u2206 B + (\u2113 \u2212 p) \u00d7 O \u03b7 2 \u00d7 rwt \u00d7 \u03b6k A \u2206 A \u2206 B =O \u03b7 2 \u00d7 rwt \u00d7 \u03b6 n + \u03b6s m nk B .\nThus, the computational complexity of every worker node of our approach is around O \u03b6 n 1 + sm kB times smaller than that of the dense coded approaches. So we claim that our proposed approach is much more suited to sparse input matrices than the dense coded approaches in [3], [7], [8]. For example, the worker computation speed in our proposed scheme is expected to be approximately 3\u00d7 faster than the dense coded approaches in case of the system in Fig. 3; where n = 12, k B = 3, \u03b6 = 2 and s m = 3. The worker computation speed can be further improved in our proposed approach if we consider x > 0, when we combine k A \u2212 y submatrices to obtain the coded submatrices of A.\n\nIt should be noted that there are certain approaches [8], [10] where there are some \"systematic\" worker nodes which are responsible for computing only the uncoded block-products. The computational complexity of every such worker node is\napproximately O (\u03b7 \u00d7 \u03b7 \u00d7 t) \u00d7 r kA w kB = O \u03b7 2 \u00d7 rwt\nkAkB , which is certainly lesser than that of any worker node in our scheme. However, in the approaches of [8] and [10], there are \u03c4 systematic worker nodes all of which are assigned uncoded submatrices and s m parity worker nodes all of which are assigned dense encoded submatrices. It can often be the case that one of the systematic worker nodes is a full straggler (failure). In that case, the master node requires results from at least one parity worker node, where all the assigned encoded submatrices are dense. That parity worker node will require much more time to complete its job, hence the overall computation time will be higher.\n\nOn the other hand, in our scheme we have assigned \u2113 u uncoded and \u2113 c coded A submatrices to each worker node, thus unlike [8] or [10], there is an underlying symmetry in our scheme so that every non-straggling worker node takes similar amount of computation time. Thus the overall computation will be faster, as demonstrated in Fig. 1.\n\n\nRemark 1.\n\nOur scheme is also applicable for distributed matrix-vector multiplication. In that case, the usual assumption is that each worker can store the whole vector x, and we can prove similar theorems by substituting \u03b3 B = 1 (or k B = 1).\n\n\nIV. NUMERICAL EXPERIMENTS AND COMPARISONS\n\nIn this section, we compare the performance of our approach with competing methods [3], [7], [8], [10], [14] via exhaustive numerical experiments in AWS (Amazon Web Services) cluster where a t2.2xlarge machine is used as the central node and t2.small machines as the worker nodes. All the corresponding software codes related to the numerical experiments have been made publicly available [29] for ensuring reproducibility of the results.\n\nWorker Computation Time: We consider a distributed matrix multiplication system with n = 24 workers, each of which can store \u03b3 A = 1 4 fraction of matrix A and \u03b3 B = 1 5 fraction of matrix B. The input matrices A and B, of sizes 12000 \u00d7 15000 and 12000 \u00d7 13500, are assumed to be sparse. We assume three different cases where sparsity (\u00b5) of the input matrices are 95%, 98% and 99%, respectively, which indicates that randomly chosen 95%, 98% and 99% entries of both of matrices A and B are zero. Table IV shows the corresponding comparison of the different methods for the worker computation time for this example. We note that in real world problems, it is common that the corresponding data matrices exhibit this level of sparsity (examples can be found in [30]). It should be noted that we have mentioned the average value of worker computation time over all the workers in case of [3], [7], [14] and our proposed approach. However, in case of [8] and [10], we have shown the average worker computation time over the parity workers only because the remaining worker nodes are message workers where there is no coding involved.\n\nIt can easily be verified from the table that the workers take significantly less time to compute the submatrix products for our proposed approach than the other methods [3], [7], [8], [10]. This is because in the other methods the coded submatrices are linear combinations of all k A = 4 submatrices from A (or k B = 5 submatrices from B). SCS optimal scheme [14] Proposed Scheme x = 0 Proposed Scheme x = 2 The work most closely related to our approach is our prior work in [14] (SCS optimal scheme, see Section V in [14]). Both approaches partition A and B into \u2206 A = LCM(n, k A ) and \u2206 B = k B submatrices, respectively. Moreover, both approaches assign some uncoded submatrices of A and then some coded submatrices of A; and assign a coded submatrix of B to each of the worker nodes.\n\nHowever, there are some crucial differences. [14] requires the weight of the encoding of the A submatrices to be \u2206 A \u2212 \u2113 u which is much higher than k A \u2212 y. Furthermore [14] does not allow for a trade-off between the number of stragglers and the weight of the coded A submatrices; this is a salient feature of our approach. Moreover, for the coding of B, SCS optimal scheme in [14] assigns linear combinations of k B submatrices, whereas in our proposed approach we assign linear combinations of \u03b6 submatrices where \u03b6 can be significantly smaller than k B . We emphasize that the our proposed approach continues to enjoy the optimal straggler resilience when x = 0. However, we point out that we lose a small amount in the Q/\u2206 metric, with respect to SCS optimal scheme in [14]. Table V shows the weight of the coding matrices for all  ) and \u03c9 = 1 + sm k B . The uncoded submatrices for these approaches are assigned in the same way.\n\n\nMETHODS\n\n\nWEIGHT OF\nA WEIGHT OF B SCS OPT. [14] \u2206A \u2212 \u2206 A k B n = 19 kB = 5 PROP. (x = 0) kA = 4 1 + kB \u2212 k B \u03c9 = 3 PROP. (x = 2) kA \u2212 k A x sm = 2 1 + kB \u2212 k B \u03c9 = 3\nthese approaches. In this example, to obtain the coded submatrix of A and B in the SCS optimal scheme in [14], we need to have random linear combination of 19 and 5 submatrices, respectively; whereas our proposed method assigns linear combinations of 4 and 3 submatrices while having the optimal straggler resilience. Moreover, in our proposed approach, if we consider having a larger recovery threshold with x = 2, then the weight of coding for A is even lesser. Thus, the worker computation for our proposed approach is faster than the case in [14] as shown in Table IV. This is further clarified using Fig. 5 which shows a detailed comparison of worker computation for computing uncodedcoded and coded-coded matrix products. The proposed approach is faster than [14] when computing the uncoded A -coded B products because of the reduced weight of encoded B submatrices. Moreover, our approach is also faster in computing the coded A -coded B products because of the reduced weight of encoded A submatrices. Because of these two differences in coding, worker computation is faster in our proposed approach in comparison to [14].\n\nOverall Computation Time: Now we compare different approaches in terms of overall computation time to recover A T B. Overall computation time is the time required by the worker nodes to compute the products so that the master node is able to decode all the unknowns corresponding to A T B. Note that it is different than the worker computation time discussed above. For example, we have 24 worker nodes, and assume that in polynomial code approach [3] (with recovery threshold 20), these worker nodes require t 0 \u2264 t 1 \u2264 t 2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 t 23 to compute their respective block-products. Then the overall computation time for this approach is t 19 .\n\nFirst, we consider the same system of n = 24 worker nodes and sparse matrices with 98% or 95% sparsity. We assume that there can be partial stragglers (slower workers) among these n worker nodes where the slower workers have onefifth of the speed of the non-straggling nodes. We carry out the simulations for different approaches for different number of partial stragglers.\n\nThe results are demonstrated in Fig. 1 where we can see that our proposed approach is significantly faster in terms of overall computation time in comparison to the dense coded approaches for different number of slower workers. This is because our proposed approach utilizes the partial stragglers and deals with sparsity quite well. It should be noted that there are peaks for [3], [7] when there are five slower workers. The Polynomial code [3] Ortho-Poly Code [7] RKRP Code [8] SCS optimal Scheme [14] Proposed Scheme Fig. 6: Comparison among different coded approaches in terms of overall computation time for different number of slower worker nodes when the \"input\" matrices are fully dense. The system has n = 24 worker nodes each of which can store \u03b3A = 1/4 and \u03b3B = 1/5 fraction of matrices A and B, respectively, so the recovery threshold, \u03c4 = 20. The slower workers are simulated in such a way so that they have half of the speed of the non-straggling workers. reason is that these codes are designed for four full stragglers while they do not take account the partial computations of the slower workers.\n\nNext we simulate another example with dense \"input\" matrices in the same system having n = 24 worker nodes which include some slower worker nodes which have half of the speed of the non-straggling nodes. The result for overall computation time is demonstrated in Fig. 6. Although, the approach in [14] is slightly faster than our proposed approach (since it has a slightly smaller Q/\u2206 than ours), our proposed approach still outperforms the dense coded approaches in [3], [7], [8] in most of the cases. Now in order to clarify the trade-off between Q/\u2206 and the sparsity of the code, we emphasize that there are two fold gains in computation time in our scheme: utilization of partial stragglers and dealing with sparse matrices. Because of these two fold gains, when the \"input\" matrices are sparse, our approach outperforms all the approaches in [3], [7], [8] and [14] as shown in Fig. 1. On the other hand, if the \"input\" matrices are dense, we have the advantage in computation time because of utilization of partial stragglers only. In that case, since the approach in [14] has a slightly smaller Q/\u2206 than ours, its overall computation is slightly faster, (see Fig. 6).\n\nValue of Q/\u2206: Many of the available approaches in coded matrix computations literature [3], [7], [8], [10] cannot leverage the slow workers, because they assign exactly one job to each of the worker nodes. On the other hand, the proposed approach and [14] assign multiple jobs to each of the worker nodes. This allows the opportunity to leverage partial stragglers.\n\nWe consider the same example of n = 24 worker nodes and show the comparison in Table VI in terms of Q/\u2206. We can see that our approach has a slightly higher Q/\u2206 than the approach in [14] and the value of Q/\u2206 can increase for the choice of x > 0. However our proposed approach has the same straggler resilience as [14] and has a significant gain over that approach in terms of worker computation speed as shown in N/A 1.96 \u00d7 10 6 RKRP CODE [8] N/A 2.83 \u00d7 10 5 CONV CODE* [10] N/A 2.65 \u00d7 10 4 SCS OPT. SCH. [14] 124  Table IV.\n\nIt should be noted that the approaches in [3], [7], [8] can directly be extended to utilizing the partial computations of the stragglers, and the corresponding Q/\u2206 value can be improved to 1. However, in that case, the size of corresponding system matrix increases significantly and that can lead to a very high worst case condition number. It indicates that the systems can be numerically unstable and the recovered results can be quite inaccurate. This is also discussed in Table IX in [14] with numerical examples.\n\nNumerical Stability: For the same system we find the worst case condition number (\u03ba worst ) of the decoding matrices over all different choices of s stragglers for all the approaches and present them in Table VI. As expected, the polynomial code approach [3] has a very high \u03ba worst . The works in [7], [8], [10] have significantly smaller \u03ba worst ; however they cannot leverage the partial computations done by the slower worker nodes. The method in [14] can utilize the partial stragglers and also provide \u03ba worst in the similar range in comparison to [6]. Now from Table VI, we can see that our approach for x = 0 also provides comparable values for worst case condition numbers, which indicates that the corresponding decoding matrix will not be ill-conditioned. The value of \u03ba worst is further reduced when we consider the case of x = 2. Although for x = 2, we have resilience to less number of stragglers, it has significant advantage on worker computation time as we have shown in Table IV.\n\n\nV. CONCLUSION\n\nIn this work, we have proposed a distributed coded matrixmultiplication scheme which (i) is resilient to optimal number of stragglers, (ii) leverages the partial computations done by the slower worker nodes and (iii) allows limited amount of coding so that the scheme is suited specifically to the case when the \"input\" matrices are sparse.\n\nIn our scheme, most of the assigned A submatrices are uncoded which can preserve the sparsity of the input matrix A. Thus, the worker computation speed is significantly faster in our method in comparison to some other prior works. Moreover, our proposed scheme also allows a trade off between the straggler resilience and the worker computation speed. Comprehensive numerical experiments on Amazon Web Services (AWS) cluster support our findings.\n\nThere are several directions for the future work of this paper. It may be interesting to examine if multiple B submatrices can be assigned to a worker node to reduce the overall worker computation time while maintaining all the desirable properties of the scheme. Furthermore, we have the defined Q as the worst case number of symbols to recover the final result; analysis on the average number of symbols can be of interest.\n\n\nAPPENDIX\n\n\nA. Proof of Claim 1\n\nProof. We know |C m | = k A , and we use the random linear combinations of k A \u2212 y uncoded submatrices from matrix A to obtain the coded symbols. Now consider the indices of the submatrices of class C m using the row vector\nz = m \u2113 + m 2\u2113 + m . . . (k A \u2212 1)\u2113 + m ; (7)\nand consider the index parameter for C m as \u03bb m . According to lines 12 and 13 of Alg. 1, we assign any coded submatrix of C m using the random linear combinations of k A \u2212 y submatrices from C m . The indices of these submatrices are the consecutive k A \u2212 y entries ofz in (7), starting from \u03bb m . Next we shift the index parameter right by k A \u2212 y in a cyclic fashion. In this way, after assigning all the coded symbols corresponding to C m , let us assume that the index parameter is \u03bb end m . Now without loss of generality, assume that i < j, and we have the following three cases.\nCase 1: If \u03bb end m \u2264 i < j, |V i | = |V j |. Case 2: If i < \u03bb end m \u2264 j, |V i | \u2212 |V j | = 1. Case 3: If i < j < \u03bb end m , |V i | = |V j |.\nThus in all three cases, we have |V i | \u2212 |V j | \u2264 1.\n\n\nB. Proof of Lemma 1\n\nProof. Letw = \u2206A n , so we can write i = \u03b4w + \u03b1 for any A i where 0 \u2264 \u03b1 \u2264w \u2212 1 and 0 \u2264 \u03b4 \u2264 n \u2212 1. Letf (z) for z \u2208 Z be the function defined below.\nf (z) = z if z \u2265 0, n + z otherwise.\nwhere \u2212n < z < n. With p = \u2206 n = k Bw and from the definition of T in Alg. 1, the uncoded submatrices assigned to the worker node W j are given by A jw , A jw+1 , A jw+2 , . . . , A jw+p\u22121 (indices reduced modulo \u2206 A ). This implies that A i is assigned to worker nodes Wf (\u03b4) , Wf (\u03b4\u22121) , . . . , Wf (\u03b4\u2212kB +1) as an uncoded block. The functionf (\u00b7) handles negative indices. So, we have |U i | = k B , which proves part (i) of the lemma.\n\nNext, we observe that the uncoded assignment to W j , namely A jw , A jw+1 , . . . , A jw+p\u22121 , belong to the classes C jw , C jw+1 , . . . , C jw+p\u22121 and \u2113 c = \u2113 \u2212 p coded submatrices consist of the elements from C jw+p , C jw+p+1 , . . . , C jw+\u2113\u22121 , respectively. In particular, it follows that the assigned submatrices to any worker node W j belong to the classes C jw , C jw+1 , . . . , C jw+\u2113\u22121 , so that each worker node contains an assignment from each class C m , 0 \u2264 m \u2264 \u2113 \u2212 1. Thus, any submatrix cannot appear more than once within any particular worker whether in an uncoded or a coded block. This shows that U i \u2229 V i = \u2205. Now, consider the j-th symbols (i.e., the j-th A submatrices) of the consecutive \u2113 worker nodes within a worker group G \u03bb , for \u03bb \u2208 {0, 1, . . . , c\u22121}. We can show that all the \u2113 classes are represented over those j-th symbols of these corresponding \u2113 worker nodes. To prove it by contradiction, we assume that there are two different workers, W i1 and W i2 , where the corresponding symbols come from the same class. Since the assignments to W i are C iw , C iw+1 , C iw+2 , . . . , C iw+p\u22121 , this is only possible if [(i 1 \u2212 i 2 )w] mod \u2113 = 0. But it is not possible since |(i 1 \u2212 i 2 )| < \u2113 andw and \u2113 are co-prime (since \u2206 A = LCM(n, k A )). Thus the j-th symbols of \u2113 workers of G \u03bb come from \u2113 different C m 's. Together with the fact that each worker node contains all C m 's, we can conclude part (iii).\n\nApplying the above argument to the j-th coded symbols of \u2113 consecutive workers within a group, we conclude that these come from \u2113 different C m 's. But we have c such worker groups each consisting of \u2113 consecutive workers. Moreover, every worker has \u2113 c coded symbols, thus there are, in total, \u2113 c \u00d7 c coded symbols from any class C m . Since each of the coded symbols consists of k A \u2212 y submatrices from C m , we have i:Ai\u2208Cm\n|V i | = \u2113 c \u00d7 n \u2113 \u00d7 (k A \u2212 y) = \u2206 A k A \u2212 \u2206 A \u2206 B n \u00d7 n \u2206 A /k A \u00d7 (k A \u2212 y) = s m (k A \u2212 y) .\nThus, the average of those corresponding |V i |'s is given by\n\u00b5 m = i:Ai\u2208Cm |V i | k A = s m (k A \u2212 y) k A .(8)\nBut according to Claim 1, |V i | \u2212 |V j | \u2264 1; for any i, j such that A i , A j \u2208 C m . Thus \u230a\u00b5 m \u230b \u2264 |V i | \u2264 \u2308\u00b5 m \u2309, so that\n|V i | \u2265 s m \u2212 s m y k A = s m \u2212 s m k A k A x s m \u2265 s m \u2212 s m k A k A x s m = s m \u2212 x = s .\nThis concludes the proof for part (ii).\n\n\nC. Proof of Corollary 1\n\nProof. Consider two elements,\nA u , A v \u2208 C m where u = v.\nFrom Lemma 1, we know that |U u | = |U v | = k B . Now from the proof of Lemma 1, we also know that each of the \u2113 submatrices assigned to any worker node comes from a different equivalence class (indices modulo \u2113), i.e., U u \u2229 U v = \u2205. It can be proved similarly for any two arbitrary elements of C m .\nNow, since |C m | = k A , \u222a i:Ai\u2208Cm U i = k A k B .\nNow consider any i such that A i \u2208 C m . According to the proof of Lemma 1, each worker node contains exactly one assignment of C m , i.e.,\nU i \u2229 V j\u2113+m = \u2205; if A j\u2113+m \u2208 C m . Thus we have \u222a i:Ai\u2208Cm U i \u2229 \u222a i:Ai\u2208Cm V i = \u2205.\nBut C m appears at every worker node, so we can say \nV i = n \u2212 k A k B = s m .\n\nD. Proof of Claim 2\n\nProof. According to Definition 3, the assigned submatrix of B for worker node W k (minimum node index where A i appears) is of type k. Now suppose that A i continues to appear at worker nodes W k+1 , W k+2 , . . . , W k+d where d \u2265 0 (either coded or uncoded). Thus, the corresponding assigned submatrices of B are from types k+1, k+2, . . . , k+d (reduced modulo k B ). Assume A i \u2208 C m . Since A i does not appear at W k+d+1 , this implies that the assignment corresponding to C m in W k+d+1 is uncoded. This is because we have x = 0, so that y = 0, i.e. all A i \u2208 C m participate in a coded block. Thus, suppose A j \u2208 C m appears in W k+d+1 as an uncoded block. We point out that according to the proof of Lemma 1, A j appears at consecutive k B worker nodes in an uncoded fashion.\n\nThis means that the next worker node that where A i can potentially appear is W k+d+kB +1 , which has a submatrix of type k + d + k B + 1 (mod k B ), which is the same as type k+d+1 (mod k B ). On the other hand, if A i does not appear at W k+d+kB +1 , another member of C m will appear in the next k B worker nodes. Then we would need to move to W k+d+2kB +1 for the next potential appearance of A i , where we have the submatrix from type k + d + 2k B + 1 (mod k B ), which is the same as type k + d + 1 (mod k B ).\n\nThe above argument shows that after having the B submatrices of types k, k + 1, k + 2, . . . , k + d (mod k B ) from W k , W k+1 , W k+2 , . . . , W k+d , we will have a submatrix of type k + d + 1 (mod k B ). Applying the argument recursively, we can conclude the required result.\n\n\nE. Proof of Lemma 2\n\nProof. Let b represent a vector with k B unknowns so that\nb T = b 0 b 1 b 2 . . . b kB \u22121 . Let c T = b T R i , where c is of length \u03c3.\nIn order to prove the lemma, we need to show that we can decode all k B unknowns of b from any k B entries of c. Consider a bipartite graph G = C \u222a B whose vertex set consists of two sets, C (representing any k B entries of c) and B (representing the entries of b). An edge connects c i to b j if b j participates in the corresponding equation. Thus, a columns of R i corresponds to a vertex in C and the non-zero entries of the column correspond to the edges incident on the vertex.\n\nIn the argument below, we suppose that the random linear coefficients are indeterminates and we argue that there exists a matching in G where all the unknowns in B are matched. Thus, according to Hall's marriage theorem [31], we need to argue that for anyC \u2282 C, the cardinality of the neighbourhood ofC, denoted as N (C) \u2282 B, is at least as large as |C|.\n\nTo argue this, we partition the columns of R i into \u03c9 disjoint sets, \u2126 0 , \u2126 1 , . . . , \u2126 \u03c9\u22121 , where each of the sets, \u2126 0 , \u2126 1 , . . . , \u2126 \u03c9\u22122 , have k B columns each, and \u2126 \u03c9\u22121 has the remaining \u03c3 \u2212 k B (\u03c9 \u2212 1) \u2264 k B columns. According to Claim 2, for any A i , since the corresponding B submatrices come from \u03c3 consecutive types, we can partition it in such a way so that each set \u2126 0 , \u2126 1 , . . . , \u2126 \u03c9\u22122 has exactly one column corresponding to every submatrix type, and \u2126 \u03c9\u22121 has the remaining ones. So, by permuting some columns of R i we can equivalently state\nc T = b T [\u2126 0 \u2126 1 . . . \u2126 \u03c9\u22122 \u2126 \u03c9\u22121 ]\nwhere \u03c9 = \u03c3 kB = 1 + sm kB . Note that the non-zero entries in the columns in any \u2126 k are cyclically shifted. We are trying to determine the neighborhood of any m elements of C. Using the arguments in the proof of Theorems 7 and 8 in [14], we can show that forC, such that |C| = m, we have\n|N (C)| = min \u03b6 + m \u03c9 \u2212 1, k B .\nHowever, we need |N (C)| \u2265 m; which indicates that\n\u03b6 \u2265 1 + m \u2212 m \u03c9 ; for m = 1, 2, . . . , k B .\nSince s m and k B are given, \u03c9 = 1 + sm kB is constant for any given parameters. Now 1+m\u2212 m \u03c9 is an increasing sequence for integer m \u2264 k B , so we need to set \u03b6 \u2265 1 + k B \u2212 kB \u03c9 . Thus for anyC, we have |N (C)| \u2265 |C|, which indicates that we have a matching. Since the entries are chosen randomly from a continuous distribution, we can say that any k B \u00d7 k B submatrix of R i is full rank.\n\n\nF. Proof of Lemma 3\n\nProof. The matrix G = G A \u2299 G B has a size k A k B \u00d7 n. We can pick any arbitrary \u03c4 columns of G which provides us with a k A k B \u00d7 \u03c4 submatrix, G sub . Let us choose the corresponding columns from G A and G B to form G A sub and G B sub , respectively, so that\nG sub = G A sub \u2299 G B sub .\nFirst we partition G A into k A + 1 block columns, and denote them as b 0 , b 1 , . . . , b kA , where each of the first k A block columns have k B columns each and b kA has s m columns. We denote e i (for i = 0, 1, . . . , k A \u2212 1) as a vector of length k A , whose i-th element is 1 and other elements are 0. Thus for i = 0, 1, . . . , k A \u2212 1, we can say that the columns in b i are e i . We assume that \u03b4 i is the number of columns which are missing from the block column b i of G A to form G A sub , so\nkA\u22121 i=0 \u03b4 i \u2264 s. Thus we have \u03b7 = kA\u22121 i=0 \u03b4 i + x columns in G A sub from b kA of G A .\nNext from Lemma 1, part (ii), we have at least s = s m \u2212 x appearances of any A i \u2208 C m in block b kA in G A . Thus, out of those chosen \u03b7 columns in block b kA , every such A i will appear in at least \u03b7 0 = \u03b7 \u2212 x = kA\u22121 i=0 \u03b4 i columns. Now, let us choose the columns of G A sub which are from b 0 . Next choose \u03b4 0 \u2264 \u03b7 0 such columns of G A sub which are from b kA of G A having non-zero entries at index 0. We set each of those \u03b4 0 columns as e 0 . Thus we have k B columns each of which is e 0 , and after the Khatri-Rao Product with the corresponding k B columns from G B sub , we have a matrix having the following form u 0 = R 0 kB ,kB 0 kB ,kB 0 kB ,kB . . . 0 kB ,kB T kA blocks where R 0 kB ,kB is obtained by taking some k B columns from \u03c3 = k B + s m columns corresponding A m0 (which is the first member of C m ), for the case x = 0, as described in Claim 2 and Lemma 2. So, R 0 kB ,kB is full rank. Similarly, we can choose the columns of G A sub which are from b 1 . Next choose \u03b4 1 \u2264 \u03b7 0 \u2212 \u03b4 0 more columns of G A sub which are from b kA of G A having non-zero entries at index 1. We set each of those \u03b4 1 columns as e 1 . Thus we have k B columns each of which is e 1 , and after the Khatri-Rao Product with the corresponding k B columns from G B sub , we have a matrix having the following form\nu 1 = 0 kB ,kB R 1 kB ,kB 0 kB ,kB . . . 0 kB ,kB T kA blocks where R 1\nkB ,kB is obtained by taking some k B columns from \u03c3 = k B +s m columns for A 1 , for the case x = 0, as described in Claim 2 and Lemma 2. So, R 1 kB ,kB is full rank. We can continue the similar process for b 2 , b 3 , . . . , b kA\u22121 , and we can show that we will have enough remaining columns even for b kA\u22121 , since \u03b7 0 = kA\u22121 i=0 \u03b4 i . Thus in this way, we can show that we have a k A \u00d7 k A block diagonal matrix, u = [u 0 u 1 . . . u kA\u22121 ], each of whose diagonal blocks is of size k B \u00d7 k B and of full rank, thus the whole block diagonal matrix is full rank.\n\nFinally, as there exists a choice of values that makes the chosen k A k B \u00d7 k A k B submatrix of G sub nonsingular, it continues to be nonsingular with probability 1 under a random choice. It should be noted that we will have some additional x = \u03c4 \u2212 k A k B columns in G sub , and thus any k A k B \u00d7 \u03c4 submatrix of G has a rank k A k B with probability 1.\n\n\nG. Proof of Claim 3\n\nProof. Consider any worker group G \u03bb , \u03bb = 0, 1, . . . , c \u2212 1 consisting of \u2113 worker nodes. The submatrices corresponding to C m appear in all different locations, 0, 1, . . . , \u2113 \u2212 1 (cf. Lemma 1), in G \u03bb . Let \u03b1 0 be the maximum number of submatrix-products that can be processed across all worker nodes of G \u03bb such that none of those submatrices from C m is processed even once. Then, from Fig. 7 it can be seen that \u03b1 0 = 0 + 1 + 2 + \u00b7 \u00b7 \u00b7 + \u2113 \u2212 1 = \u2113(\u2113 \u2212 1) 2 .\n\nNow we know that we can process \u03b1 0 submatrix-products from each of the worker groups without processing any submatrix-product corresponding to C m . Any additional processing will necessarily process a submatrix-product corresponding to C m . Suppose we choose any particular worker, where the position index of C m is i. In that case, we can acquire at most \u2113 \u2212 1 \u2212 i more symbols (i.e., submatrix-products)\nW 0 W 1 . . . W \u2113\u22122 W \u2113\u22121 C 0 C 1 . . . C \u2113\u22123 C \u2113\u22122 C \u2113\u22121 C 1 C 2\n. . . . . . Fig. 7: We partition the given n worker nodes into c = n/\u2113 disjoint groups of worker nodes. We show a group of \u2113 worker nodes (from W0 to W \u2113\u22121 , shown in green) where any class (without loss of generality, assume C \u2113\u22121 ) appears exactly \u2113 times, in all \u2113 different locations. Once we acquire the symbol C \u2113\u22121 from location i, we can acquire (\u2113 \u2212 1 \u2212 i) more symbols from that worker node without any further appearances of C \u2113\u22121 . We show the corresponding numbers for the selected worker group.\nC \u2113\u22122 C \u2113\u22121 . . . C \u2113\u22125 C \u2113\u22124 C \u2113\u22123 C \u2113\u22121 C 0 . . . C \u2113\u22124 C \u2113\u22123 C \u2113\u22122 0 1 \u2113 \u2212 1 \u2212 i \u2113 \u2212 2 \u2113 \u2212 1\nfrom that particular worker without any more appearances of C m . A corresponding example is shown in Fig. 7. This is true for all c = n/\u2113 worker groups, since in each group any class appears at all locations. Thus, the maximum number of submatrix-products that can be processed for each additional appearance of C m can be expressed by the following vector. \n\nHere z is a non-increasing sequence, so in order to obtain the maximum number of submatrix-products where C m appears at most \u03ba \u2212 1 times, we need to acquire submatrixproducts sequentially as mentioned in z. Let c 1 = \u230a \u03ba\u22121 c \u230b and c 2 = \u03ba \u2212 1 \u2212 cc 1 ; so cc 1 + c 2 = \u03ba \u2212 1. Then we can choose the first \u03ba \u2212 1 workers (as mentioned in z) so that we can have \u03b7 symbols where C m appears exactly \u03ba \u2212 1 times, so\n\u03b7 = c\u03b1 0 + \u03ba\u22122 i=0 z[i] = n(\u2113 \u2212 1) 2 + c c1\u22121 i=0 (\u2113 \u2212 i) + c 2 (\u2113 \u2212 c 1 ).\nThis concludes the proof.\n\n\nH. Proof of Theorem 2\n\nProof. We assume that there is such A T i B j which can still not be recovered after some certain Q ub symbols are acquired, where Q ub is defined in the theorem statement. Assume that A i \u2208 C m . Now from Theorem 1, we know that any \u03c4 = k A k B + x out of those n submatrix-products corresponding to C m are enough to recover all the corresponding unknowns. So, to prove the upper bound for Q, we try to find the maximum number of block products (Q \u2032 ) which can be acquired over all the worker groups where there are at most \u03c4 \u2212 1 appearances of C m . According to Claim 3, Q \u2032 is given by\nQ \u2032 = n(\u2113 \u2212 1) 2 + c c x 1 \u22121 i=0 (\u2113 \u2212 i) + c x 2 (\u2113 \u2212 c x 1 );\nwhere c x 1 = \u230a \u03c4 \u22121 c \u230b and c x 2 = \u03c4 \u2212 1 \u2212 cc x 1 with \u03c4 = k A k B + x. It indicates that we can recover all k A k B such unknowns corresponding to C m if we acquire any Q = Q \u2032 + 1 submatrixproducts over n workers. Thus according to Theorem 1, we can recover the unknown A T i B j from any Q ub submatrixproducts, which concludes the proof for upper bound.\n\nOn the other hand, to prove the lower bound, we pick a particular submatrix A \u2206A\u22121 and show a certain pattern of Q lb \u2212 1 block-products from which A T \u2206A\u22121 B j cannot be decoded, for any j = 0, 1, 2, . . . , k B \u2212 1. Note that A \u2206A\u22121 \u2208 C \u2113\u22121 (since \u2113 divides \u2206 A ).\n\nTo form that pattern of Q lb \u2212 1 block-products, first we choose c\u03b1 0 = n(\u2113\u22121) 2 block-products from all c worker groups where C \u2113\u22121 does not appear. Since according to Corollary 1(i), C \u2113\u22121 appears at k A k B worker nodes in an uncoded fashion, where we know that the uncoded locations of C \u2113\u22121 in every worker group are 0, 1, . . . , p\u2212 1. Thus, we can acquire at most M more symbols where M = [\u2113 + (\u2113 \u2212 1) + (\u2113 \u2212 2) + \u00b7 \u00b7 \u00b7 + (\u2113 \u2212 p + 1)] c such that C \u2113\u22121 appears only in an uncoded fashion. This is because c \u00d7 p = k A k B , where p = \u2206/n. However, we know that A \u2206A\u22121 appears at location p \u2212 1 at worker node W n\u2212kB according to the scheme. Thus, we can construct a pattern where A \u2206A\u22121 appears exactly k B \u2212 1 times by removing \u2113 \u2212 p + 1 symbols from worker W n\u2212kB . Let Then, we have shown a pattern such that from Q \u2032\u2032 symbols we cannot recover the unknowns corresponding to A \u2206A\u22121 . Now according to Claim 1 and the proof of Lemma 1, A \u2206A\u22121 appears at \u230a\u00b5 \u2113\u22121 \u230b = s m \u2212 smy kA coded submatrices of C \u2113\u22121 (\u00b5 m is defined as the average of the coded appearances of all the submatrices of class C m and its value is given in (8)). Since C \u2113\u22121 appears at s m locations, it indicates that there are an additional s m \u2212 \u230a\u00b5 m \u230b = smy kA coded submatrices where A \u2206A\u22121 does not appear.\n\nThus, we finally form a pattern of Q \u2032\u2032 + smy kA = Q lb \u2212 1 symbols where A \u2206A\u22121 appears exactly k B \u2212 1 times, where y = kAx sm . But there are k B unknowns corresponding to A \u2206A \u2212 1 in the form of A T \u2206A\u22121 B j , so we cannot decode the A T \u2206A\u22121 B j 's from this specific pattern of Q lb \u2212 1 symbols. This concludes the proof for the lower bound of Q.\n\nFig. 2 :\n2Matrices A and B are partitioned into ten and two blockcolumns, respectively. Each worker is assigned four uncoded-coded and one coded-coded block-products. The coefficients ri's and ci's are chosen i.i.d. at random from a continuous distribution.\n\nFig. 3 :\n3Distributed matrix multiplication over n = 12 worker nodes with \u03b3A = \u03b3B = 1 3 ; so \u2206A = 12 and \u2206B = 3. In this figure, any {Ai, Aj, A k } means a random linear combination of those submatrices. The coefficients ri's are chosen i.i.d. at random from a continuous distribution.\n\nFig. 4 :\n4Matrix-vector case with n = 5 workers and s = 1 stragglers, with \u03b3A = 1 3 . Here, sm = n \u2212 1 \u03b3 A = 2. In this figure, any {Ai, Aj} means a linear combination of those submatrices, where the coefficients are chosen i.i.d. at random from a continuous distribution. We now present a lemma which outlines the key properties of the structure of encoding submatrices of A. It includes the details on how a given submatrix A i and the different classes appear at different locations over all the worker nodes. The proof of this lemma is detailed in Appendix B. Lemma 1. Assume that the jobs are assigned to the workers according to Alg. 1, and consider any submatrix A i , for i = 0, 1, 2, . . . , \u2206 A \u2212 1. Then (i) |U i | = k B , (ii) |V i | \u2265 s and U i \u2229 V i = \u2205, and (iii) a given class C m , where 0 \u2264 m \u2264 \u2113\u22121, appears at all different locations 0, 1, . . . , \u2113\u22121 within the worker nodes of any worker group G \u03bb , where 0 \u2264 \u03bb \u2264 c \u2212 1.\n\n\nE. Straggler Resilience and bounds on Q/\u2206Lemma 2. Consider anyA i , i = 0, 1, 2, . . . , \u2206 A \u2212 1, for the case of x = 0. Construct a k B \u00d7 \u03c3 matrix R iwhere the columns of R i correspond to the coefficients for coded B submatrices of the worker nodes in U i \u222aV i and \u03c3\n\nExample 3 .\n3We consider an example with n = 8 and \u03b3 A =1 3 , \u03b3 B = 1 2 . So, we partition A into \u2206 A = LCM(n, k A ) = 24 submatrices and B into \u2206 B = k B = 2 submatrices. The properties of this scheme are discussed in\n\nFig. 5 :\n5The comparison of worker computation time for the case of sparse matrices with \u00b5 = 98%. We split the total time into two parts: time required for multiplying p uncoded submatrices of A with the coded submatrix of B and the time required for multiplying (\u2113 \u2212 p) coded submatrices from A and with the coded submatrix of B.\n\n\n1, . . . , \u2113 \u2212 1, . . . , c 1, 1, . . . , 1) .\n\nQ\n\u2032\u2032 = c\u03b1 0 + M \u2212 (\u2113 \u2212 p + 1) = c\u03b1 0 + \u2113 + (\u2113 \u2212 1) + (\u2113 \u2212 2) + \u00b7 \u00b7 \u00b7 + (\u2113 \u2212 p + 2)c + (c \u2212 1)(\u2113 \u2212 p k A k B \u2212 1 \u2212 cc 0 1 = c \u2212 1.\n\n\n(das207@purdue.edu) was with the Dept. of Electrical and Computer Engineering at Iowa State University. He is now with the Dept. of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47907 USA. Aditya Ramamoorthy (adityar@iastate.edu) is with the Dept. of Electrical and Computer Engineering, Iowa State University, Ames, IA 50011 USA.\n\nTABLE II :\nIINotation Table NUMBER OF MAXIMUM POSSIBLE FULL STRAGGLERS sm = n \u2212 kAkB \u2206A, \u2206B NUMBER OF BLOCK-COLUMNS THAT \u2206A = LCM(n, kA) A AND B, RESPECTIVELY, ARE PARTITIONED INTO AND \u2206B = kB \u2206 TOTAL NUMBER OF UNKNOWNS THAT NEED TO BE RECOVERED \u2206 = \u2206A\u2206B \u03c4 RECOVERY THRESHOLD OF THE SCHEME \u03c4 \u2265 kAkB s NUMBER OF STRAGGLERSNOTATION \nDEFINITION \nDESCRIPTION \n\n\u03b3A, \u03b3B \nSTORAGE FRACTION FOR A AND B, RESPECTIVELY \n\u03b3A = 1 \nk A , \u03b3B = 1 \n\nk B \n\nn \n\nNUMBER OF TOTAL WORKER NODES \n\nn \u2265 kAkB \nsm \n\n\n\n\nOF SUBMATRIX PRODUCTS THAT HAVE TO BE COMPUTED Q \u2265 \u2206 IN THE WORST CASE TO RECOVER THE INTENDED RESULT\u03b6 \nWEIGHT FOR THE ENCODING OF B \n\u03b6 \u2264 kB \n\n\u2113u, \u2113c \nNUMBER OF UNCODED AND CODED SUBMATRICES OF A \n\u2113u = \u2206 \n\nn AND \nASSIGNED TO EVERY WORKER NODE \n\n\n\nTABLE III :\nIIIComparison of properties of the system with n = 8and \u03b3A = 1 \n3 and \u03b3B = 1 \n2 for different values of x \n\nx \ny \n\u03c4 \nQ lb \nQ ub \nQ \n\n0 \n0 \n6 \n59 \n59 \n59 \n1 \n1 \n7 \n60 \n62 \n61 \n\n\n\nTABLE IV :\nIVComparison of worker computation time (in seconds) for matrix-matrix multiplication for n = 24, \u03b3A = 1 98% and 99% entries of both of matrices A and B are zero.4 and \u03b3B = 1 \n\n5 \n\n(*for [10], we assume \u03b3A = 2 \n5 and \u03b3B = 1 \n4 ) when randomly chosen \n95%, METHODS \n\nS \n\nWORKER COMP. TIME (S) \n\u00b5 = 99% \u00b5 = 98% \u00b5 = 95% \n\nPOLY CODE [3] \n4 \n1.23 \n3.10 \n8.21 \nORTHO POLY [7] \n4 \n1.25 \n3.13 \n8.14 \nRKRP CODE [8] \n4 \n1.21 \n3.09 \n8.10 \nCONV. CODE* [10] \n4 \n1.92 \n5.07 \n10.72 \nSCS OPT. SCH. [14] \n4 \n0.91 \n1.89 \n4.67 \nPROP. SCH. (x = 0) \n4 \n0.54 \n0.97 \n3.68 \nPROP. SCH. (x = 2) \n2 \n0.45 \n0.81 \n3.21 \n\nuncoded A -coded B \ncoded A -coded B \n\n0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1 \n\n1.2 \n\n1.4 \n\nWorker Computation Time (in sec) \n\n\n\nTABLE V :\nVWeight of coding for the coded submatrices A and B for \n\ndifferent approaches where \u2206A =LCM(n, kA\n\nTABLE VI :\nVIComparison of utilization of partial stragglers andnumerical stability among different approaches \n\nMETHODS \nQ/\u2206 \n\u03baworst \n\nPOLY CODE [3] \nN/A \n2.40 \u00d7 10 10 \nORTHO-POLY [7] \n\n\nSpeeding up distributed machine learning using codes. K Lee, M Lam, R Pedarsani, D Papailiopoulos, K Ramchandran, IEEE Trans. on Info. Th. 643K. Lee, M. Lam, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran, \"Speeding up distributed machine learning using codes,\" IEEE Trans. on Info. Th., vol. 64, no. 3, pp. 1514-1529, 2018.\n\nShort-dot: Computing large linear transforms distributedly using coded short dot products. S Dutta, V Cadambe, P Grover, Proc. of Adv. in Neur. Inf. Proc. Syst. (NIPS). of Adv. in Neur. Inf. . Syst. (NIPS)S. Dutta, V. Cadambe, and P. Grover, \"Short-dot: Computing large linear transforms distributedly using coded short dot products,\" in Proc. of Adv. in Neur. Inf. Proc. Syst. (NIPS), 2016, pp. 2100-2108.\n\nPolynomial codes: an optimal design for high-dimensional coded matrix multiplication. Q Yu, M Maddah-Ali, S Avestimehr, Proc. of Adv. in Neur. Inf. Proc. Syst. (NIPS. of Adv. in Neur. Inf. . Syst. (NIPSQ. Yu, M. Maddah-Ali, and S. Avestimehr, \"Polynomial codes: an optimal design for high-dimensional coded matrix multiplication,\" in Proc. of Adv. in Neur. Inf. Proc. Syst. (NIPS), 2017, pp. 4403-4413.\n\nStraggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding. Q Yu, M A Maddah-Ali, A S Avestimehr, IEEE Trans. on Info. Th. 663Q. Yu, M. A. Maddah-Ali, and A. S. Avestimehr, \"Straggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding,\" IEEE Trans. on Info. Th., vol. 66, no. 3, pp. 1920-1933, 2020.\n\nGradient coding: Avoiding stragglers in distributed learning. R Tandon, Q Lei, A G Dimakis, N Karampatziakis, Proc. of Intl. Conf. on Machine Learning (ICML). of Intl. Conf. on Machine Learning (ICML)R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis, \"Gradient coding: Avoiding stragglers in distributed learning,\" in Proc. of Intl. Conf. on Machine Learning (ICML), 2017, pp. 3368-3376.\n\nDistributed matrix-vector multiplication: A convolutional coding approach. A B Das, A Ramamoorthy, Proc. of IEEE Intl. Symp. on Info. Th. of IEEE Intl. Symp. on Info. ThA. B. Das and A. Ramamoorthy, \"Distributed matrix-vector multiplica- tion: A convolutional coding approach,\" in Proc. of IEEE Intl. Symp. on Info. Th., 2019, pp. 3022-3026.\n\nNumerically stable polynomially coded computing. M Fahim, V R Cadambe, IEEE Trans. on Info. Th. 675M. Fahim and V. R. Cadambe, \"Numerically stable polynomially coded computing,\" IEEE Trans. on Info. Th., vol. 67, no. 5, pp. 2758-2785, 2021.\n\nRandom Khatri-Rao-product codes for numerically-stable distributed matrix multiplication. A M Subramaniam, A Heidarzadeh, K R Narayanan, Proc. of Annual Conf. on Comm., Control, and Computing (Allerton). of Annual Conf. on Comm., Control, and Computing (Allerton)A. M. Subramaniam, A. Heidarzadeh, and K. R. Narayanan, \"Random Khatri-Rao-product codes for numerically-stable distributed matrix mul- tiplication,\" in Proc. of Annual Conf. on Comm., Control, and Computing (Allerton), Sep. 2019, pp. 253-259.\n\nNumerically stable coded matrix computations via circulant and rotation matrix embeddings. A Ramamoorthy, L Tang, IEEE Trans. on Info. Th. 684A. Ramamoorthy and L. Tang, \"Numerically stable coded matrix com- putations via circulant and rotation matrix embeddings,\" IEEE Trans. on Info. Th., vol. 68, no. 4, pp. 2684-2703, 2022.\n\nEfficient and robust distributed matrix computations via convolutional coding. A B Das, A Ramamoorthy, N Vaswani, IEEE Trans. on Info. Th. 679A. B. Das, A. Ramamoorthy, and N. Vaswani, \"Efficient and robust distributed matrix computations via convolutional coding,\" IEEE Trans. on Info. Th., vol. 67, no. 9, pp. 6266-6282, 2021.\n\nFactored LT and factored raptor codes for large-scale distributed matrix multiplication. A K Pradhan, A Heidarzadeh, K R Narayanan, IEEE J. Select. Areas Info. Th. 23A. K. Pradhan, A. Heidarzadeh, and K. R. Narayanan, \"Factored LT and factored raptor codes for large-scale distributed matrix multiplication,\" IEEE J. Select. Areas Info. Th., vol. 2, no. 3, pp. 893-906, 2021.\n\nStraggler-resistant distributed matrix computation via coding theory: Removing a bottleneck in largescale data processing. A Ramamoorthy, A B Das, L Tang, IEEE Sig. Proc. Mag. 373A. Ramamoorthy, A. B. Das, and L. Tang, \"Straggler-resistant distributed matrix computation via coding theory: Removing a bottleneck in large- scale data processing,\" IEEE Sig. Proc. Mag., vol. 37, no. 3, pp. 136-145, 2020.\n\nCoded sparse matrix multiplication. S Wang, J Liu, N Shroff, Proc. of Intl. Conf. on Machine Learning (ICML). of Intl. Conf. on Machine Learning (ICML)S. Wang, J. Liu, and N. Shroff, \"Coded sparse matrix multiplication,\" in Proc. of Intl. Conf. on Machine Learning (ICML), 2018, pp. 5152- -5160.\n\nCoded sparse matrix computation schemes that leverage partial stragglers. A B Das, A Ramamoorthy, IEEE Trans. on Info. Th. 686A. B. Das and A. Ramamoorthy, \"Coded sparse matrix computation schemes that leverage partial stragglers,\" IEEE Trans. on Info. Th., vol. 68, no. 6, pp. 4156-4181, 2022.\n\nExploitation of stragglers in coded computation. S Kiani, N Ferdinand, S C Draper, Proc. of IEEE Intl. of IEEE IntlS. Kiani, N. Ferdinand, and S. C. Draper, \"Exploitation of stragglers in coded computation,\" in Proc. of IEEE Intl. Symp. on Info. Th., 2018, pp. 1988-1992.\n\nRateless codes for near-perfect load balancing in distributed matrixvector multiplication. A Mallick, M Chaudhari, U Sheth, G Palanikumar, G Joshi, Proceedings of the ACM on Meas. and Analysis of Comp. Syst. 33A. Mallick, M. Chaudhari, U. Sheth, G. Palanikumar, and G. Joshi, \"Rateless codes for near-perfect load balancing in distributed matrix- vector multiplication,\" Proceedings of the ACM on Meas. and Analysis of Comp. Syst., vol. 3, no. 3, pp. 1-40, 2019.\n\nHierarchical coded matrix multiplication. S Kianidehkordi, N Ferdinand, S C Draper, IEEE Trans. on Info. Th. 672S. Kianidehkordi, N. Ferdinand, and S. C. Draper, \"Hierarchical coded matrix multiplication,\" IEEE Trans. on Info. Th., vol. 67, no. 2, pp. 726- 754, 2021.\n\nDistributed gradient descent with coded partial gradient computations. E Ozfatura, S Ulukus, D G\u00fcnd\u00fcz, Proc. of IEEE Intl. Conf. on Acoustics, Speech and Sig. Proc. (ICASSP). of IEEE Intl. Conf. on Acoustics, Speech and Sig. . (ICASSP)E. Ozfatura, S. Ulukus, and D. G\u00fcnd\u00fcz, \"Distributed gradient descent with coded partial gradient computations,\" in Proc. of IEEE Intl. Conf. on Acoustics, Speech and Sig. Proc. (ICASSP), 2019, pp. 3492-3496.\n\nUniversally decodable matrices for distributed matrix-vector multiplication. A Ramamoorthy, L Tang, P O Vontobel, Proc. of IEEE Intl. of IEEE IntlA. Ramamoorthy, L. Tang, and P. O. Vontobel, \"Universally decodable matrices for distributed matrix-vector multiplication,\" in Proc. of IEEE Intl. Symp. on Info. Th., 2019, pp. 1777-1781.\n\nBivariate hermitian polynomial coding for efficient distributed matrix multiplication. B Has\u0131rc\u0131oglu, J G\u00f3mez-Vilardeb\u00f3, D G\u00fcnd\u00fcz, Proc. of IEEE Glob. Comm. Conf. (GLOBECOM). of IEEE Glob. Comm. Conf. (GLOBECOM)B. Has\u0131rc\u0131oglu, J. G\u00f3mez-Vilardeb\u00f3, and D. G\u00fcnd\u00fcz, \"Bivariate hermi- tian polynomial coding for efficient distributed matrix multiplication,\" in Proc. of IEEE Glob. Comm. Conf. (GLOBECOM), 2020, pp. 1-6.\n\nDynamic heterogeneityaware coded cooperative computation at the edge. Y Keshtkarjahromi, Y Xing, H Seferoglu, Proc. of IEEE Intl. Conf. on Network Protocols (ICNP). of IEEE Intl. Conf. on Network Protocols (ICNP)Y. Keshtkarjahromi, Y. Xing, and H. Seferoglu, \"Dynamic heterogeneity- aware coded cooperative computation at the edge,\" in Proc. of IEEE Intl. Conf. on Network Protocols (ICNP), 2018, pp. 23-33.\n\nC 3 LES : Codes for coded computation that leverage stragglers. A B Das, L Tang, A Ramamoorthy, Proc. of IEEE Info. of IEEE InfoA. B. Das, L. Tang, and A. Ramamoorthy, \"C 3 LES : Codes for coded computation that leverage stragglers,\" in Proc. of IEEE Info. Th. Workshop, 2018, pp. 1-5.\n\nHigh-dimensional coded matrix multiplication. K Lee, C Suh, K Ramchandran, Proc. of IEEE Intl. Symp. on Info. Th. of IEEE Intl. Symp. on Info. ThK. Lee, C. Suh, and K. Ramchandran, \"High-dimensional coded matrix multiplication,\" in Proc. of IEEE Intl. Symp. on Info. Th., 2017, pp. 2418-2422.\n\nOn the optimal recovery threshold of coded matrix multiplication. S Dutta, M Fahim, F Haddadpour, H Jeong, V Cadambe, P Grover, IEEE Trans. on Info. Th. 661S. Dutta, M. Fahim, F. Haddadpour, H. Jeong, V. Cadambe, and P. Grover, \"On the optimal recovery threshold of coded matrix multi- plication,\" IEEE Trans. on Info. Th., vol. 66, no. 1, pp. 278-301, 2019.\n\nErasure coding for distributed matrix multiplication for matrices with bounded entries. L Tang, K Konstantinidis, A Ramamoorthy, IEEE Comm. Letters. 231L. Tang, K. Konstantinidis, and A. Ramamoorthy, \"Erasure coding for distributed matrix multiplication for matrices with bounded entries,\" IEEE Comm. Letters, vol. 23, no. 1, pp. 8-11, 2019.\n\nHow Bad Are Vandermonde Matrices?. V Pan, SIAM Journal on Matrix Analysis and Applications. 372V. Pan, \"How Bad Are Vandermonde Matrices?\" SIAM Journal on Matrix Analysis and Applications, vol. 37, no. 2, pp. 676-694, 2016.\n\nProduct lagrange coded computing. A M Subramaniam, A Heidarzadeh, A K Pradhan, K R Narayanan, Proc. of IEEE Intl. of IEEE IntlA. M. Subramaniam, A. Heidarzadeh, A. K. Pradhan, and K. R. Narayanan, \"Product lagrange coded computing,\" in Proc. of IEEE Intl. Symp. on Info. Th., 2020, pp. 197-202.\n\nX.-D Zhang, Matrix analysis and applications. Cambridge University PressX.-D. Zhang, Matrix analysis and applications. Cambridge University Press, 2017.\n\nUnified Treatment of Partial Stragglers and Sparse Matrices. Unified Treatment of Partial Stragglers and Sparse Matrices. [Online].\n\nSuiteSparse Matrix Collection. SuiteSparse Matrix Collection. [Online].\n\nHall, Combinatorial theory. J Marshall, John Wiley & SonsJ. Marshall. Hall, Combinatorial theory. John Wiley & Sons, 1986.\n", "annotations": {"author": "[{\"end\":121,\"start\":103},{\"end\":141,\"start\":122}]", "publisher": null, "author_last_name": "[{\"end\":120,\"start\":117},{\"end\":140,\"start\":129}]", "author_first_name": "[{\"end\":110,\"start\":103},{\"end\":116,\"start\":111},{\"end\":128,\"start\":122}]", "author_affiliation": null, "title": "[{\"end\":90,\"start\":1},{\"end\":231,\"start\":142}]", "venue": null, "abstract": "[{\"end\":1560,\"start\":243}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3401,\"start\":3398},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3407,\"start\":3403},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3525,\"start\":3521},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3568,\"start\":3565},{\"end\":4290,\"start\":4287},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5244,\"start\":5240},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5470,\"start\":5466},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5476,\"start\":5472},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6228,\"start\":6225},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11953,\"start\":11950},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11959,\"start\":11955},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12116,\"start\":12112},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12355,\"start\":12352},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12416,\"start\":12413},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12719,\"start\":12716},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12725,\"start\":12721},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12759,\"start\":12756},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12800,\"start\":12797},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12806,\"start\":12802},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12928,\"start\":12924},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12934,\"start\":12930},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12940,\"start\":12936},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13022,\"start\":13019},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13027,\"start\":13024},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13033,\"start\":13029},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13039,\"start\":13035},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13230,\"start\":13227},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13236,\"start\":13232},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13242,\"start\":13238},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13248,\"start\":13244},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13460,\"start\":13456},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13481,\"start\":13477},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13510,\"start\":13507},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13549,\"start\":13545},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13571,\"start\":13568},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13586,\"start\":13583},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13610,\"start\":13606},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13633,\"start\":13630},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13653,\"start\":13649},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13685,\"start\":13681},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14831,\"start\":14828},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14836,\"start\":14833},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14841,\"start\":14838},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14846,\"start\":14843},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14852,\"start\":14848},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14858,\"start\":14854},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15032,\"start\":15029},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15499,\"start\":15496},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15916,\"start\":15913},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16019,\"start\":16015},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16049,\"start\":16046},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16055,\"start\":16051},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16061,\"start\":16057},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16067,\"start\":16063},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16280,\"start\":16277},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16434,\"start\":16430},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16560,\"start\":16557},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16717,\"start\":16714},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16785,\"start\":16782},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16946,\"start\":16943},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17034,\"start\":17031},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17040,\"start\":17036},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17217,\"start\":17214},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17222,\"start\":17219},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17228,\"start\":17224},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17385,\"start\":17381},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17391,\"start\":17387},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17397,\"start\":17393},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17403,\"start\":17399},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17775,\"start\":17771},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17806,\"start\":17802},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17850,\"start\":17846},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21778,\"start\":21774},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21874,\"start\":21870},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22697,\"start\":22694},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22717,\"start\":22714},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22731,\"start\":22728},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22755,\"start\":22751},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22852,\"start\":22849},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22872,\"start\":22869},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22886,\"start\":22883},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22910,\"start\":22906},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28362,\"start\":28358},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28368,\"start\":28364},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":39945,\"start\":39941},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":41842,\"start\":41838},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":43571,\"start\":43568},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":43576,\"start\":43573},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":43581,\"start\":43578},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":45578,\"start\":45575},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":45583,\"start\":45580},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":45588,\"start\":45585},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":46985,\"start\":46982},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":46990,\"start\":46987},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":46995,\"start\":46992},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":47442,\"start\":47439},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":47448,\"start\":47444},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":47787,\"start\":47784},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":47796,\"start\":47792},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":48447,\"start\":48444},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":48455,\"start\":48451},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":49035,\"start\":49032},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":49040,\"start\":49037},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":49045,\"start\":49042},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49051,\"start\":49047},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":49057,\"start\":49053},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":49342,\"start\":49338},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":50153,\"start\":50149},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":50278,\"start\":50275},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":50283,\"start\":50280},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":50289,\"start\":50285},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":50340,\"start\":50337},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":50349,\"start\":50345},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":50694,\"start\":50691},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":50699,\"start\":50696},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":50704,\"start\":50701},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":50710,\"start\":50706},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":50885,\"start\":50881},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":51001,\"start\":50997},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":51044,\"start\":51040},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":51360,\"start\":51356},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":51485,\"start\":51481},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":51693,\"start\":51689},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":52089,\"start\":52085},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":52523,\"start\":52519},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":52964,\"start\":52960},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":53183,\"start\":53179},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":53543,\"start\":53539},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":53997,\"start\":53994},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":54949,\"start\":54946},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":54954,\"start\":54951},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":55014,\"start\":55011},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":55034,\"start\":55031},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":55048,\"start\":55045},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":55072,\"start\":55068},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":55985,\"start\":55981},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":56154,\"start\":56151},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":56159,\"start\":56156},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":56164,\"start\":56161},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":56534,\"start\":56531},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":56539,\"start\":56536},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":56544,\"start\":56541},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":56553,\"start\":56549},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":56761,\"start\":56757},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":56949,\"start\":56946},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":56954,\"start\":56951},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":56959,\"start\":56956},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":56965,\"start\":56961},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":57114,\"start\":57110},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":57411,\"start\":57407},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":57542,\"start\":57538},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":57667,\"start\":57664},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":57699,\"start\":57695},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":57734,\"start\":57730},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":57796,\"start\":57793},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":57801,\"start\":57798},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":57806,\"start\":57803},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":58243,\"start\":58239},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":58528,\"start\":58525},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":58571,\"start\":58568},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":58576,\"start\":58573},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":58582,\"start\":58578},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":58725,\"start\":58721},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":58827,\"start\":58824},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":67804,\"start\":67800},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":68785,\"start\":68781}]", "figure": "[{\"attributes\":{\"id\":\"fig_2\"},\"end\":78235,\"start\":77977},{\"attributes\":{\"id\":\"fig_3\"},\"end\":78522,\"start\":78236},{\"attributes\":{\"id\":\"fig_4\"},\"end\":79465,\"start\":78523},{\"attributes\":{\"id\":\"fig_5\"},\"end\":79736,\"start\":79466},{\"attributes\":{\"id\":\"fig_6\"},\"end\":79956,\"start\":79737},{\"attributes\":{\"id\":\"fig_7\"},\"end\":80288,\"start\":79957},{\"attributes\":{\"id\":\"fig_11\"},\"end\":80337,\"start\":80289},{\"attributes\":{\"id\":\"fig_12\"},\"end\":80468,\"start\":80338},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":80830,\"start\":80469},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":81320,\"start\":80831},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":81568,\"start\":81321},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":81758,\"start\":81569},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":82481,\"start\":81759},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":82591,\"start\":82482},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":82779,\"start\":82592}]", "paragraph": "[{\"end\":2888,\"start\":1562},{\"end\":4271,\"start\":2908},{\"end\":4708,\"start\":4273},{\"end\":5733,\"start\":4710},{\"end\":6989,\"start\":5735},{\"end\":7720,\"start\":7054},{\"end\":8609,\"start\":7722},{\"end\":9283,\"start\":8630},{\"end\":9421,\"start\":9285},{\"end\":9999,\"start\":9423},{\"end\":10480,\"start\":10001},{\"end\":10626,\"start\":10482},{\"end\":11021,\"start\":10628},{\"end\":11763,\"start\":11023},{\"end\":11879,\"start\":11765},{\"end\":12272,\"start\":11913},{\"end\":12996,\"start\":12274},{\"end\":13963,\"start\":12998},{\"end\":16020,\"start\":14126},{\"end\":17865,\"start\":16022},{\"end\":17956,\"start\":17897},{\"end\":19838,\"start\":17958},{\"end\":20341,\"start\":19864},{\"end\":20694,\"start\":20472},{\"end\":21113,\"start\":20696},{\"end\":21587,\"start\":21115},{\"end\":21925,\"start\":21622},{\"end\":22175,\"start\":21970},{\"end\":22617,\"start\":22206},{\"end\":22772,\"start\":22646},{\"end\":23650,\"start\":22801},{\"end\":24185,\"start\":24081},{\"end\":24306,\"start\":24187},{\"end\":25147,\"start\":24413},{\"end\":25768,\"start\":25620},{\"end\":26421,\"start\":25770},{\"end\":26701,\"start\":26423},{\"end\":26949,\"start\":26922},{\"end\":27327,\"start\":26961},{\"end\":27838,\"start\":27357},{\"end\":27919,\"start\":27840},{\"end\":28545,\"start\":28021},{\"end\":28710,\"start\":28571},{\"end\":29100,\"start\":28771},{\"end\":29745,\"start\":29664},{\"end\":29881,\"start\":29802},{\"end\":30253,\"start\":30111},{\"end\":30553,\"start\":30255},{\"end\":30719,\"start\":30555},{\"end\":31051,\"start\":30947},{\"end\":31566,\"start\":31053},{\"end\":32002,\"start\":31568},{\"end\":32310,\"start\":32052},{\"end\":32636,\"start\":32433},{\"end\":32759,\"start\":32638},{\"end\":33149,\"start\":32818},{\"end\":33771,\"start\":33151},{\"end\":34620,\"start\":33798},{\"end\":34970,\"start\":34866},{\"end\":35028,\"start\":34972},{\"end\":35244,\"start\":35030},{\"end\":35888,\"start\":35428},{\"end\":36415,\"start\":36050},{\"end\":36697,\"start\":36433},{\"end\":37094,\"start\":36848},{\"end\":37452,\"start\":37096},{\"end\":37597,\"start\":37564},{\"end\":38632,\"start\":37599},{\"end\":39085,\"start\":38634},{\"end\":40137,\"start\":39209},{\"end\":40339,\"start\":40240},{\"end\":41246,\"start\":40460},{\"end\":41478,\"start\":41248},{\"end\":41880,\"start\":41576},{\"end\":42146,\"start\":41882},{\"end\":42259,\"start\":42202},{\"end\":42382,\"start\":42261},{\"end\":42412,\"start\":42384},{\"end\":42713,\"start\":42657},{\"end\":42950,\"start\":42715},{\"end\":43405,\"start\":43220},{\"end\":43824,\"start\":43407},{\"end\":44385,\"start\":43826},{\"end\":44669,\"start\":44427},{\"end\":45737,\"start\":44740},{\"end\":46172,\"start\":45784},{\"end\":46379,\"start\":46250},{\"end\":46609,\"start\":46435},{\"end\":47384,\"start\":46710},{\"end\":47622,\"start\":47386},{\"end\":48319,\"start\":47677},{\"end\":48657,\"start\":48321},{\"end\":48903,\"start\":48671},{\"end\":49387,\"start\":48949},{\"end\":50519,\"start\":49389},{\"end\":51309,\"start\":50521},{\"end\":52245,\"start\":51311},{\"end\":53544,\"start\":52414},{\"end\":54191,\"start\":53546},{\"end\":54566,\"start\":54193},{\"end\":55682,\"start\":54568},{\"end\":56857,\"start\":55684},{\"end\":57224,\"start\":56859},{\"end\":57749,\"start\":57226},{\"end\":58268,\"start\":57751},{\"end\":59267,\"start\":58270},{\"end\":59625,\"start\":59285},{\"end\":60073,\"start\":59627},{\"end\":60500,\"start\":60075},{\"end\":60758,\"start\":60535},{\"end\":61391,\"start\":60805},{\"end\":61585,\"start\":61532},{\"end\":61756,\"start\":61609},{\"end\":62232,\"start\":61794},{\"end\":63684,\"start\":62234},{\"end\":64114,\"start\":63686},{\"end\":64272,\"start\":64211},{\"end\":64449,\"start\":64323},{\"end\":64582,\"start\":64543},{\"end\":64639,\"start\":64610},{\"end\":64971,\"start\":64669},{\"end\":65163,\"start\":65024},{\"end\":65300,\"start\":65248},{\"end\":66133,\"start\":65349},{\"end\":66652,\"start\":66135},{\"end\":66935,\"start\":66654},{\"end\":67016,\"start\":66959},{\"end\":67578,\"start\":67095},{\"end\":67934,\"start\":67580},{\"end\":68507,\"start\":67936},{\"end\":68836,\"start\":68547},{\"end\":68920,\"start\":68870},{\"end\":69357,\"start\":68967},{\"end\":69642,\"start\":69381},{\"end\":70178,\"start\":69671},{\"end\":71581,\"start\":70269},{\"end\":72221,\"start\":71654},{\"end\":72578,\"start\":72223},{\"end\":73069,\"start\":72602},{\"end\":73480,\"start\":73071},{\"end\":74055,\"start\":73547},{\"end\":74511,\"start\":74152},{\"end\":74923,\"start\":74513},{\"end\":75025,\"start\":75000},{\"end\":75642,\"start\":75051},{\"end\":76066,\"start\":75707},{\"end\":76334,\"start\":76068},{\"end\":77622,\"start\":76336},{\"end\":77976,\"start\":77624}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13993,\"start\":13964},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14125,\"start\":13993},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20471,\"start\":20342},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21621,\"start\":21588},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23930,\"start\":23651},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24080,\"start\":23930},{\"attributes\":{\"id\":\"formula_6\"},\"end\":24412,\"start\":24307},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25619,\"start\":25148},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26834,\"start\":26702},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26921,\"start\":26834},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26960,\"start\":26950},{\"attributes\":{\"id\":\"formula_11\"},\"end\":27356,\"start\":27328},{\"attributes\":{\"id\":\"formula_12\"},\"end\":28020,\"start\":27920},{\"attributes\":{\"id\":\"formula_13\"},\"end\":29663,\"start\":29101},{\"attributes\":{\"id\":\"formula_14\"},\"end\":29801,\"start\":29746},{\"attributes\":{\"id\":\"formula_15\"},\"end\":30110,\"start\":29882},{\"attributes\":{\"id\":\"formula_16\"},\"end\":30946,\"start\":30720},{\"attributes\":{\"id\":\"formula_17\"},\"end\":32051,\"start\":32003},{\"attributes\":{\"id\":\"formula_18\"},\"end\":32432,\"start\":32364},{\"attributes\":{\"id\":\"formula_19\"},\"end\":32817,\"start\":32760},{\"attributes\":{\"id\":\"formula_20\"},\"end\":34810,\"start\":34621},{\"attributes\":{\"id\":\"formula_21\"},\"end\":34865,\"start\":34810},{\"attributes\":{\"id\":\"formula_22\"},\"end\":35402,\"start\":35245},{\"attributes\":{\"id\":\"formula_23\"},\"end\":36049,\"start\":35889},{\"attributes\":{\"id\":\"formula_24\"},\"end\":36847,\"start\":36735},{\"attributes\":{\"id\":\"formula_25\"},\"end\":37563,\"start\":37453},{\"attributes\":{\"id\":\"formula_26\"},\"end\":39208,\"start\":39086},{\"attributes\":{\"id\":\"formula_27\"},\"end\":40239,\"start\":40138},{\"attributes\":{\"id\":\"formula_28\"},\"end\":40459,\"start\":40340},{\"attributes\":{\"id\":\"formula_29\"},\"end\":41531,\"start\":41479},{\"attributes\":{\"id\":\"formula_30\"},\"end\":42201,\"start\":42147},{\"attributes\":{\"id\":\"formula_31\"},\"end\":42656,\"start\":42413},{\"attributes\":{\"id\":\"formula_32\"},\"end\":42982,\"start\":42951},{\"attributes\":{\"id\":\"formula_33\"},\"end\":43219,\"start\":42982},{\"attributes\":{\"id\":\"formula_34\"},\"end\":44739,\"start\":44670},{\"attributes\":{\"id\":\"formula_35\"},\"end\":45783,\"start\":45738},{\"attributes\":{\"id\":\"formula_36\"},\"end\":46249,\"start\":46173},{\"attributes\":{\"id\":\"formula_37\"},\"end\":46434,\"start\":46380},{\"attributes\":{\"id\":\"formula_38\"},\"end\":46709,\"start\":46610},{\"attributes\":{\"id\":\"formula_39\"},\"end\":47676,\"start\":47623},{\"attributes\":{\"id\":\"formula_40\"},\"end\":52413,\"start\":52268},{\"attributes\":{\"id\":\"formula_41\"},\"end\":60804,\"start\":60759},{\"attributes\":{\"id\":\"formula_42\"},\"end\":61531,\"start\":61392},{\"attributes\":{\"id\":\"formula_43\"},\"end\":61793,\"start\":61757},{\"attributes\":{\"id\":\"formula_44\"},\"end\":64210,\"start\":64115},{\"attributes\":{\"id\":\"formula_45\"},\"end\":64322,\"start\":64273},{\"attributes\":{\"id\":\"formula_46\"},\"end\":64542,\"start\":64450},{\"attributes\":{\"id\":\"formula_47\"},\"end\":64668,\"start\":64640},{\"attributes\":{\"id\":\"formula_48\"},\"end\":65023,\"start\":64972},{\"attributes\":{\"id\":\"formula_49\"},\"end\":65247,\"start\":65164},{\"attributes\":{\"id\":\"formula_50\"},\"end\":65326,\"start\":65301},{\"attributes\":{\"id\":\"formula_51\"},\"end\":67094,\"start\":67017},{\"attributes\":{\"id\":\"formula_52\"},\"end\":68546,\"start\":68508},{\"attributes\":{\"id\":\"formula_53\"},\"end\":68869,\"start\":68837},{\"attributes\":{\"id\":\"formula_54\"},\"end\":68966,\"start\":68921},{\"attributes\":{\"id\":\"formula_55\"},\"end\":69670,\"start\":69643},{\"attributes\":{\"id\":\"formula_56\"},\"end\":70268,\"start\":70179},{\"attributes\":{\"id\":\"formula_57\"},\"end\":71653,\"start\":71582},{\"attributes\":{\"id\":\"formula_59\"},\"end\":73546,\"start\":73481},{\"attributes\":{\"id\":\"formula_60\"},\"end\":74151,\"start\":74056},{\"attributes\":{\"id\":\"formula_62\"},\"end\":74999,\"start\":74924},{\"attributes\":{\"id\":\"formula_63\"},\"end\":75706,\"start\":75643}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12226,\"start\":12196},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":13418,\"start\":13252},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28709,\"start\":28701},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44200,\"start\":44191},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":49894,\"start\":49886},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":52098,\"start\":52091},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":52985,\"start\":52977},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":57313,\"start\":57305},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":57748,\"start\":57740},{\"end\":58235,\"start\":58227},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":58481,\"start\":58473},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":59266,\"start\":59258}]", "section_header": "[{\"end\":2906,\"start\":2891},{\"end\":7052,\"start\":6992},{\"end\":8628,\"start\":8612},{\"end\":11911,\"start\":11882},{\"end\":17895,\"start\":17868},{\"end\":19862,\"start\":19841},{\"end\":21968,\"start\":21928},{\"end\":22204,\"start\":22178},{\"end\":22644,\"start\":22620},{\"end\":22799,\"start\":22775},{\"end\":28569,\"start\":28548},{\"end\":28769,\"start\":28713},{\"end\":32363,\"start\":32313},{\"end\":33796,\"start\":33774},{\"end\":35426,\"start\":35404},{\"end\":36431,\"start\":36418},{\"end\":36734,\"start\":36700},{\"end\":41574,\"start\":41533},{\"end\":44425,\"start\":44388},{\"end\":48669,\"start\":48660},{\"end\":48947,\"start\":48906},{\"end\":52255,\"start\":52248},{\"end\":52267,\"start\":52258},{\"end\":59283,\"start\":59270},{\"end\":60511,\"start\":60503},{\"end\":60533,\"start\":60514},{\"end\":61607,\"start\":61588},{\"end\":64608,\"start\":64585},{\"end\":65347,\"start\":65328},{\"end\":66957,\"start\":66938},{\"end\":69379,\"start\":69360},{\"end\":72600,\"start\":72581},{\"end\":75049,\"start\":75028},{\"end\":77986,\"start\":77978},{\"end\":78245,\"start\":78237},{\"end\":78532,\"start\":78524},{\"end\":79749,\"start\":79738},{\"end\":79966,\"start\":79958},{\"end\":80340,\"start\":80339},{\"end\":80842,\"start\":80832},{\"end\":81581,\"start\":81570},{\"end\":81770,\"start\":81760},{\"end\":82492,\"start\":82483},{\"end\":82603,\"start\":82593}]", "table": "[{\"end\":81320,\"start\":81153},{\"end\":81568,\"start\":81424},{\"end\":81758,\"start\":81634},{\"end\":82481,\"start\":81933},{\"end\":82591,\"start\":82494},{\"end\":82779,\"start\":82657}]", "figure_caption": "[{\"end\":78235,\"start\":77988},{\"end\":78522,\"start\":78247},{\"end\":79465,\"start\":78534},{\"end\":79736,\"start\":79468},{\"end\":79956,\"start\":79751},{\"end\":80288,\"start\":79968},{\"end\":80337,\"start\":80291},{\"end\":80468,\"start\":80341},{\"end\":80830,\"start\":80471},{\"end\":81153,\"start\":80845},{\"end\":81424,\"start\":81323},{\"end\":81634,\"start\":81585},{\"end\":81933,\"start\":81773},{\"end\":82657,\"start\":82606}]", "figure_ref": "[{\"end\":19369,\"start\":19363},{\"end\":19668,\"start\":19662},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20100,\"start\":20094},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20662,\"start\":20656},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20736,\"start\":20730},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21112,\"start\":21106},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21343,\"start\":21337},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22236,\"start\":22230},{\"end\":22933,\"start\":22927},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26223,\"start\":26217},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32661,\"start\":32655},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34470,\"start\":34464},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34615,\"start\":34609},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":37752,\"start\":37746},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":38006,\"start\":38000},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40943,\"start\":40937},{\"end\":44135,\"start\":44129},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":47167,\"start\":47161},{\"end\":48656,\"start\":48650},{\"end\":52149,\"start\":52148},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":53025,\"start\":53019},{\"end\":54606,\"start\":54600},{\"end\":55095,\"start\":55089},{\"end\":55953,\"start\":55947},{\"end\":56572,\"start\":56566},{\"end\":56855,\"start\":56849},{\"end\":73002,\"start\":72996},{\"end\":73565,\"start\":73559},{\"end\":74260,\"start\":74254}]", "bib_author_first_name": "[{\"end\":82836,\"start\":82835},{\"end\":82843,\"start\":82842},{\"end\":82850,\"start\":82849},{\"end\":82863,\"start\":82862},{\"end\":82881,\"start\":82880},{\"end\":83204,\"start\":83203},{\"end\":83213,\"start\":83212},{\"end\":83224,\"start\":83223},{\"end\":83607,\"start\":83606},{\"end\":83613,\"start\":83612},{\"end\":83627,\"start\":83626},{\"end\":84023,\"start\":84022},{\"end\":84029,\"start\":84028},{\"end\":84031,\"start\":84030},{\"end\":84045,\"start\":84044},{\"end\":84047,\"start\":84046},{\"end\":84362,\"start\":84361},{\"end\":84372,\"start\":84371},{\"end\":84379,\"start\":84378},{\"end\":84381,\"start\":84380},{\"end\":84392,\"start\":84391},{\"end\":84770,\"start\":84769},{\"end\":84772,\"start\":84771},{\"end\":84779,\"start\":84778},{\"end\":85087,\"start\":85086},{\"end\":85096,\"start\":85095},{\"end\":85098,\"start\":85097},{\"end\":85370,\"start\":85369},{\"end\":85372,\"start\":85371},{\"end\":85387,\"start\":85386},{\"end\":85402,\"start\":85401},{\"end\":85404,\"start\":85403},{\"end\":85879,\"start\":85878},{\"end\":85894,\"start\":85893},{\"end\":86196,\"start\":86195},{\"end\":86198,\"start\":86197},{\"end\":86205,\"start\":86204},{\"end\":86220,\"start\":86219},{\"end\":86536,\"start\":86535},{\"end\":86538,\"start\":86537},{\"end\":86549,\"start\":86548},{\"end\":86564,\"start\":86563},{\"end\":86566,\"start\":86565},{\"end\":86947,\"start\":86946},{\"end\":86962,\"start\":86961},{\"end\":86964,\"start\":86963},{\"end\":86971,\"start\":86970},{\"end\":87264,\"start\":87263},{\"end\":87272,\"start\":87271},{\"end\":87279,\"start\":87278},{\"end\":87599,\"start\":87598},{\"end\":87601,\"start\":87600},{\"end\":87608,\"start\":87607},{\"end\":87870,\"start\":87869},{\"end\":87879,\"start\":87878},{\"end\":87892,\"start\":87891},{\"end\":87894,\"start\":87893},{\"end\":88185,\"start\":88184},{\"end\":88196,\"start\":88195},{\"end\":88209,\"start\":88208},{\"end\":88218,\"start\":88217},{\"end\":88233,\"start\":88232},{\"end\":88600,\"start\":88599},{\"end\":88617,\"start\":88616},{\"end\":88630,\"start\":88629},{\"end\":88632,\"start\":88631},{\"end\":88898,\"start\":88897},{\"end\":88910,\"start\":88909},{\"end\":88920,\"start\":88919},{\"end\":89348,\"start\":89347},{\"end\":89363,\"start\":89362},{\"end\":89371,\"start\":89370},{\"end\":89373,\"start\":89372},{\"end\":89693,\"start\":89692},{\"end\":89708,\"start\":89707},{\"end\":89727,\"start\":89726},{\"end\":90092,\"start\":90091},{\"end\":90111,\"start\":90110},{\"end\":90119,\"start\":90118},{\"end\":90495,\"start\":90494},{\"end\":90497,\"start\":90496},{\"end\":90504,\"start\":90503},{\"end\":90512,\"start\":90511},{\"end\":90764,\"start\":90763},{\"end\":90771,\"start\":90770},{\"end\":90778,\"start\":90777},{\"end\":91078,\"start\":91077},{\"end\":91087,\"start\":91086},{\"end\":91096,\"start\":91095},{\"end\":91110,\"start\":91109},{\"end\":91119,\"start\":91118},{\"end\":91130,\"start\":91129},{\"end\":91460,\"start\":91459},{\"end\":91468,\"start\":91467},{\"end\":91486,\"start\":91485},{\"end\":91750,\"start\":91749},{\"end\":91974,\"start\":91973},{\"end\":91976,\"start\":91975},{\"end\":91991,\"start\":91990},{\"end\":92006,\"start\":92005},{\"end\":92008,\"start\":92007},{\"end\":92019,\"start\":92018},{\"end\":92021,\"start\":92020},{\"end\":92239,\"start\":92235},{\"end\":92624,\"start\":92623}]", "bib_author_last_name": "[{\"end\":82840,\"start\":82837},{\"end\":82847,\"start\":82844},{\"end\":82860,\"start\":82851},{\"end\":82878,\"start\":82864},{\"end\":82893,\"start\":82882},{\"end\":83210,\"start\":83205},{\"end\":83221,\"start\":83214},{\"end\":83231,\"start\":83225},{\"end\":83610,\"start\":83608},{\"end\":83624,\"start\":83614},{\"end\":83638,\"start\":83628},{\"end\":84026,\"start\":84024},{\"end\":84042,\"start\":84032},{\"end\":84058,\"start\":84048},{\"end\":84369,\"start\":84363},{\"end\":84376,\"start\":84373},{\"end\":84389,\"start\":84382},{\"end\":84407,\"start\":84393},{\"end\":84776,\"start\":84773},{\"end\":84791,\"start\":84780},{\"end\":85093,\"start\":85088},{\"end\":85106,\"start\":85099},{\"end\":85384,\"start\":85373},{\"end\":85399,\"start\":85388},{\"end\":85414,\"start\":85405},{\"end\":85891,\"start\":85880},{\"end\":85899,\"start\":85895},{\"end\":86202,\"start\":86199},{\"end\":86217,\"start\":86206},{\"end\":86228,\"start\":86221},{\"end\":86546,\"start\":86539},{\"end\":86561,\"start\":86550},{\"end\":86576,\"start\":86567},{\"end\":86959,\"start\":86948},{\"end\":86968,\"start\":86965},{\"end\":86976,\"start\":86972},{\"end\":87269,\"start\":87265},{\"end\":87276,\"start\":87273},{\"end\":87286,\"start\":87280},{\"end\":87605,\"start\":87602},{\"end\":87620,\"start\":87609},{\"end\":87876,\"start\":87871},{\"end\":87889,\"start\":87880},{\"end\":87901,\"start\":87895},{\"end\":88193,\"start\":88186},{\"end\":88206,\"start\":88197},{\"end\":88215,\"start\":88210},{\"end\":88230,\"start\":88219},{\"end\":88239,\"start\":88234},{\"end\":88614,\"start\":88601},{\"end\":88627,\"start\":88618},{\"end\":88639,\"start\":88633},{\"end\":88907,\"start\":88899},{\"end\":88917,\"start\":88911},{\"end\":88927,\"start\":88921},{\"end\":89360,\"start\":89349},{\"end\":89368,\"start\":89364},{\"end\":89382,\"start\":89374},{\"end\":89705,\"start\":89694},{\"end\":89724,\"start\":89709},{\"end\":89734,\"start\":89728},{\"end\":90108,\"start\":90093},{\"end\":90116,\"start\":90112},{\"end\":90129,\"start\":90120},{\"end\":90501,\"start\":90498},{\"end\":90509,\"start\":90505},{\"end\":90524,\"start\":90513},{\"end\":90768,\"start\":90765},{\"end\":90775,\"start\":90772},{\"end\":90790,\"start\":90779},{\"end\":91084,\"start\":91079},{\"end\":91093,\"start\":91088},{\"end\":91107,\"start\":91097},{\"end\":91116,\"start\":91111},{\"end\":91127,\"start\":91120},{\"end\":91137,\"start\":91131},{\"end\":91465,\"start\":91461},{\"end\":91483,\"start\":91469},{\"end\":91498,\"start\":91487},{\"end\":91754,\"start\":91751},{\"end\":91988,\"start\":91977},{\"end\":92003,\"start\":91992},{\"end\":92016,\"start\":92009},{\"end\":92031,\"start\":92022},{\"end\":92245,\"start\":92240},{\"end\":92633,\"start\":92625}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3442617},\"end\":83110,\"start\":82781},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9028807},\"end\":83518,\"start\":83112},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":20729541},\"end\":83922,\"start\":83520},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":22184330},\"end\":84297,\"start\":83924},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":33632433},\"end\":84692,\"start\":84299},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":59291914},\"end\":85035,\"start\":84694},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":84187068},\"end\":85277,\"start\":85037},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":196622289},\"end\":85785,\"start\":85279},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":204578346},\"end\":86114,\"start\":85787},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219179466},\"end\":86444,\"start\":86116},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":198901601},\"end\":86821,\"start\":86446},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":211069710},\"end\":87225,\"start\":86823},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":28708280},\"end\":87522,\"start\":87227},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":228372945},\"end\":87818,\"start\":87524},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":49487227},\"end\":88091,\"start\":87820},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":248384907},\"end\":88555,\"start\":88093},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":198147872},\"end\":88824,\"start\":88557},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53717250},\"end\":89268,\"start\":88826},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":59413846},\"end\":89603,\"start\":89270},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":231725175},\"end\":90019,\"start\":89605},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":49868397},\"end\":90428,\"start\":90021},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52284270},\"end\":90715,\"start\":90430},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6938356},\"end\":91009,\"start\":90717},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":19323670},\"end\":91369,\"start\":91011},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53231036},\"end\":91712,\"start\":91371},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":39124975},\"end\":91937,\"start\":91714},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":221386704},\"end\":92233,\"start\":91939},{\"attributes\":{\"id\":\"b27\"},\"end\":92387,\"start\":92235},{\"attributes\":{\"id\":\"b28\"},\"end\":92520,\"start\":92389},{\"attributes\":{\"id\":\"b29\"},\"end\":92593,\"start\":92522},{\"attributes\":{\"id\":\"b30\"},\"end\":92717,\"start\":92595}]", "bib_title": "[{\"end\":82833,\"start\":82781},{\"end\":83201,\"start\":83112},{\"end\":83604,\"start\":83520},{\"end\":84020,\"start\":83924},{\"end\":84359,\"start\":84299},{\"end\":84767,\"start\":84694},{\"end\":85084,\"start\":85037},{\"end\":85367,\"start\":85279},{\"end\":85876,\"start\":85787},{\"end\":86193,\"start\":86116},{\"end\":86533,\"start\":86446},{\"end\":86944,\"start\":86823},{\"end\":87261,\"start\":87227},{\"end\":87596,\"start\":87524},{\"end\":87867,\"start\":87820},{\"end\":88182,\"start\":88093},{\"end\":88597,\"start\":88557},{\"end\":88895,\"start\":88826},{\"end\":89345,\"start\":89270},{\"end\":89690,\"start\":89605},{\"end\":90089,\"start\":90021},{\"end\":90492,\"start\":90430},{\"end\":90761,\"start\":90717},{\"end\":91075,\"start\":91011},{\"end\":91457,\"start\":91371},{\"end\":91747,\"start\":91714},{\"end\":91971,\"start\":91939}]", "bib_author": "[{\"end\":82842,\"start\":82835},{\"end\":82849,\"start\":82842},{\"end\":82862,\"start\":82849},{\"end\":82880,\"start\":82862},{\"end\":82895,\"start\":82880},{\"end\":83212,\"start\":83203},{\"end\":83223,\"start\":83212},{\"end\":83233,\"start\":83223},{\"end\":83612,\"start\":83606},{\"end\":83626,\"start\":83612},{\"end\":83640,\"start\":83626},{\"end\":84028,\"start\":84022},{\"end\":84044,\"start\":84028},{\"end\":84060,\"start\":84044},{\"end\":84371,\"start\":84361},{\"end\":84378,\"start\":84371},{\"end\":84391,\"start\":84378},{\"end\":84409,\"start\":84391},{\"end\":84778,\"start\":84769},{\"end\":84793,\"start\":84778},{\"end\":85095,\"start\":85086},{\"end\":85108,\"start\":85095},{\"end\":85386,\"start\":85369},{\"end\":85401,\"start\":85386},{\"end\":85416,\"start\":85401},{\"end\":85893,\"start\":85878},{\"end\":85901,\"start\":85893},{\"end\":86204,\"start\":86195},{\"end\":86219,\"start\":86204},{\"end\":86230,\"start\":86219},{\"end\":86548,\"start\":86535},{\"end\":86563,\"start\":86548},{\"end\":86578,\"start\":86563},{\"end\":86961,\"start\":86946},{\"end\":86970,\"start\":86961},{\"end\":86978,\"start\":86970},{\"end\":87271,\"start\":87263},{\"end\":87278,\"start\":87271},{\"end\":87288,\"start\":87278},{\"end\":87607,\"start\":87598},{\"end\":87622,\"start\":87607},{\"end\":87878,\"start\":87869},{\"end\":87891,\"start\":87878},{\"end\":87903,\"start\":87891},{\"end\":88195,\"start\":88184},{\"end\":88208,\"start\":88195},{\"end\":88217,\"start\":88208},{\"end\":88232,\"start\":88217},{\"end\":88241,\"start\":88232},{\"end\":88616,\"start\":88599},{\"end\":88629,\"start\":88616},{\"end\":88641,\"start\":88629},{\"end\":88909,\"start\":88897},{\"end\":88919,\"start\":88909},{\"end\":88929,\"start\":88919},{\"end\":89362,\"start\":89347},{\"end\":89370,\"start\":89362},{\"end\":89384,\"start\":89370},{\"end\":89707,\"start\":89692},{\"end\":89726,\"start\":89707},{\"end\":89736,\"start\":89726},{\"end\":90110,\"start\":90091},{\"end\":90118,\"start\":90110},{\"end\":90131,\"start\":90118},{\"end\":90503,\"start\":90494},{\"end\":90511,\"start\":90503},{\"end\":90526,\"start\":90511},{\"end\":90770,\"start\":90763},{\"end\":90777,\"start\":90770},{\"end\":90792,\"start\":90777},{\"end\":91086,\"start\":91077},{\"end\":91095,\"start\":91086},{\"end\":91109,\"start\":91095},{\"end\":91118,\"start\":91109},{\"end\":91129,\"start\":91118},{\"end\":91139,\"start\":91129},{\"end\":91467,\"start\":91459},{\"end\":91485,\"start\":91467},{\"end\":91500,\"start\":91485},{\"end\":91756,\"start\":91749},{\"end\":91990,\"start\":91973},{\"end\":92005,\"start\":91990},{\"end\":92018,\"start\":92005},{\"end\":92033,\"start\":92018},{\"end\":92247,\"start\":92235},{\"end\":92635,\"start\":92623}]", "bib_venue": "[{\"end\":82918,\"start\":82895},{\"end\":83279,\"start\":83233},{\"end\":83685,\"start\":83640},{\"end\":84083,\"start\":84060},{\"end\":84456,\"start\":84409},{\"end\":84830,\"start\":84793},{\"end\":85131,\"start\":85108},{\"end\":85481,\"start\":85416},{\"end\":85924,\"start\":85901},{\"end\":86253,\"start\":86230},{\"end\":86608,\"start\":86578},{\"end\":86997,\"start\":86978},{\"end\":87335,\"start\":87288},{\"end\":87645,\"start\":87622},{\"end\":87921,\"start\":87903},{\"end\":88299,\"start\":88241},{\"end\":88664,\"start\":88641},{\"end\":88999,\"start\":88929},{\"end\":89402,\"start\":89384},{\"end\":89778,\"start\":89736},{\"end\":90184,\"start\":90131},{\"end\":90544,\"start\":90526},{\"end\":90829,\"start\":90792},{\"end\":91162,\"start\":91139},{\"end\":91518,\"start\":91500},{\"end\":91804,\"start\":91756},{\"end\":92051,\"start\":92033},{\"end\":92279,\"start\":92247},{\"end\":92448,\"start\":92389},{\"end\":92551,\"start\":92522},{\"end\":92621,\"start\":92595},{\"end\":83317,\"start\":83281},{\"end\":83722,\"start\":83687},{\"end\":84499,\"start\":84458},{\"end\":84863,\"start\":84832},{\"end\":85542,\"start\":85483},{\"end\":87378,\"start\":87337},{\"end\":87935,\"start\":87923},{\"end\":89061,\"start\":89001},{\"end\":89416,\"start\":89404},{\"end\":89816,\"start\":89780},{\"end\":90233,\"start\":90186},{\"end\":90558,\"start\":90546},{\"end\":90862,\"start\":90831},{\"end\":92065,\"start\":92053}]"}}}, "year": 2023, "month": 12, "day": 17}