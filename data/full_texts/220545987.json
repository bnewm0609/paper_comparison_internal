{"id": 220545987, "updated": "2023-10-06 12:53:29.107", "metadata": {"title": "Self-Supervised Nuclei Segmentation in Histopathological Images Using Attention", "authors": "[{\"first\":\"Mihir\",\"last\":\"Sahasrabudhe\",\"middle\":[]},{\"first\":\"Stergios\",\"last\":\"Christodoulidis\",\"middle\":[]},{\"first\":\"Roberto\",\"last\":\"Salgado\",\"middle\":[]},{\"first\":\"Stefan\",\"last\":\"Michiels\",\"middle\":[]},{\"first\":\"Sherene\",\"last\":\"Loi\",\"middle\":[]},{\"first\":\"Fabrice\",\"last\":\"Andr'e\",\"middle\":[]},{\"first\":\"Nikos\",\"last\":\"Paragios\",\"middle\":[]},{\"first\":\"Maria\",\"last\":\"Vakalopoulou\",\"middle\":[]}]", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2020", "journal": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2020", "publication_date": {"year": 2020, "month": 7, "day": 16}, "abstract": "Segmentation and accurate localization of nuclei in histopathological images is a very challenging problem, with most existing approaches adopting a supervised strategy. These methods usually rely on manual annotations that require a lot of time and effort from medical experts. In this study, we present a self-supervised approach for segmentation of nuclei for whole slide histopathology images. Our method works on the assumption that the size and texture of nuclei can determine the magnification at which a patch is extracted. We show that the identification of the magnification level for tiles can generate a preliminary self-supervision signal to locate nuclei. We further show that by appropriately constraining our model it is possible to retrieve meaningful segmentation maps as an auxiliary output to the primary magnification identification task. Our experiments show that with standard post-processing, our method can outperform other unsupervised nuclei segmentation approaches and report similar performance with supervised ones on the publicly available MoNuSeg dataset. Our code and models are available online to facilitate further research.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2007.08373", "mag": "3043037655", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/miccai/SahasrabudheCSM20", "doi": "10.1007/978-3-030-59722-1_38"}}, "content": {"source": {"pdf_hash": "a15d62232ee99bec9cc90c0a037e2765f0f03c48", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.08373v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://hal.archives-ouvertes.fr/hal-03087006/file/arxiv.pdf", "status": "GREEN"}}, "grobid": {"id": "e99a221f7b17747b817a6734e292bedaef381259", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a15d62232ee99bec9cc90c0a037e2765f0f03c48.txt", "contents": "\nSelf-Supervised Nuclei Segmentation in Histopathological Images Using Attention\n\n\nMihir Sahasrabudhe \nMath\u00e9matiques et Informatique pour la Complexit\u00e9 et les Syst\u00e8mes\nUniversit\u00e9 Paris-Saclay\n91190CentraleSup\u00e9lec, Gif-sur-YvetteFrance\n\nInria Saclay\n91190Gif-sur-YvetteFrance\n\nStergios Christodoulidis \nInstitut Gustave Roussy\n94800VillejuifFrance\n\nRoberto Salgado \nDivision of Research\nPeter MacCallum Cancer Centre\nMelbourneAustralia\n\nDepartment of Pathology\nGZA-ZNA Hospitals\n2050AntwerpBelgium\n\nStefan Michiels \nInstitut Gustave Roussy\n94800VillejuifFrance\n\nService de Biostatistique et d'Epid\u00e9miologie\nGustave Roussy\nUniversit\u00e9 ParisSud\nUniversit\u00e9 ParisSaclay\nCESP U108VillejuifFrance\n\nSherene Loi \nDivision of Research\nPeter MacCallum Cancer Centre\nMelbourneAustralia\n\nFabrice Andr\u00e9 \nInstitut Gustave Roussy\n94800VillejuifFrance\n\nNikos Paragios \nMath\u00e9matiques et Informatique pour la Complexit\u00e9 et les Syst\u00e8mes\nUniversit\u00e9 Paris-Saclay\n91190CentraleSup\u00e9lec, Gif-sur-YvetteFrance\n\nTherapanacea\n75014ParisFrance\n\nMaria Vakalopoulou \nMath\u00e9matiques et Informatique pour la Complexit\u00e9 et les Syst\u00e8mes\nUniversit\u00e9 Paris-Saclay\n91190CentraleSup\u00e9lec, Gif-sur-YvetteFrance\n\nInria Saclay\n91190Gif-sur-YvetteFrance\n\nInstitut Gustave Roussy\n94800VillejuifFrance\n\nSelf-Supervised Nuclei Segmentation in Histopathological Images Using Attention\nPathologyWhole Slide ImagesNuclei SegmentationDeep LearningSelf-SupervisionAttention Models\nSegmentation and accurate localization of nuclei in histopathological images is a very challenging problem, with most existing approaches adopting a supervised strategy. These methods usually rely on manual annotations that require a lot of time and effort from medical experts. In this study, we present a self-supervised approach for segmentation of nuclei for whole slide histopathology images. Our method works on the assumption that the size and texture of nuclei can determine the magnification at which a patch is extracted. We show that the identification of the magnification level for tiles can generate a preliminary self-supervision signal to locate nuclei. We further show that by appropriately constraining our model it is possible to retrieve meaningful segmentation maps as an auxiliary output to the primary magnification identification task. Our experiments show that with standard post-processing, our method can outperform other unsupervised nuclei segmentation approaches and report similar performance with supervised ones on the publicly available MoNuSeg dataset. Our code and models are available online to facilitate further research. 3x3 Conv, s=1, p=1, d=1 3x3 ResConv, s=1, p=1, d=1 3x3 ResConv, s=1, p=2, d=2 3x3 ResConv, s=1, p=3, d=3 3x3 ResConv, s=1, p=5, d=5 3x3 ResConv, s=1, p=10, d=10 3x3 ResConv, s=1, p=20, d=20 Patch, I X Attention network, F ResNet34 Scale Attention, A Scale network, G J\n\nIntroduction\n\nHistology images are the gold standard in diagnosing a considerable number of diseases including almost all types of cancer. For example, the count of nuclei on whole-slide images (WSIs) can have diagnostic significance for numerous cancerous conditions [20]. The proliferation of digital pathology and highthroughput tissue imaging leads to the adoption in clinical practice of digitized https://github.com/msahasrabudhe/miccai2020_self_sup_nuclei_seg arXiv:2007.08373v1 [eess.IV] 16 Jul 2020 histopathological images that are utilized and archived every day. Such WSIs are acquired from glass histology slides using dedicated scanning devices after a staining process. In each WSI, thousands of nuclei from various types of cell can be identified. The detection of such nuclei is crucial for the identification of tissue structures, which can be further analyzed in a systematic manner and used for various clinical tasks. Presence, extent, size, shape, and other morphological characteristics of such structures are important indicators of the severity of different diseases [6]. Moreover, a quantitative analysis of digital pathology is important, to understand the underlying biological reasons for diseases [21].\n\nManual segmentation or estimation of nuclei on a WSI is an extremely time consuming process which suffers from high inter-observer variability [1]. On the other hand, data-driven methods that perform well on a specific histopathological datasets report poor performance on other datasets due again to the high variability in acquisition parameters and biological properties of cells in different organs and diseases [13]. To deal with this problem, datasets integrating different organs [13,4] based on images from The Cancer Genome Atlas (TCGA) provide pixelwise annotations for nuclei from variety of organs. Yet, these datasets provide access to only a limited range of annotations, making the generalization of these techniques ambiguous and emphasizing the need for novel segmentation algorithms without relying purely on manual annotations.\n\nTo this end, in this paper, we propose a self-supervised approach for nuclei segmentation without requiring annotations. The contributions of this paper are threefold: (i) we propose using scale classification as a self-supervision signal under the assumption that nuclei are a discriminative feature for this task; (ii) we employ a fully convolutional attention network based on dilated filters that generates segmentation maps for nuclei in the image space; and (iii) we investigate regularization constraints on the output of the attention network in order to generate semantically meaningful segmentation maps.\n\n\nRelated Work\n\nHematoxylin and eosin (H&E) staining is one of the most common and inexpensive staining schemes for WSI acquisition. A number of different tissue structures can be identified in H&E images such as glands, lumen (ducts within glands), adipose (fat), and stroma (connective tissue). The building blocks of such structures are a number of different cells. During the staining process, hematoxylin renders cell nuclei dark blueish purple and the epithelium light purple, while eosin renders stroma pink. A variety of standard image analysis methods are based on hematoxylin in order to extract nuclei [24,2] reporting very promising results, albeit evaluated mostly on single organs. A lot of research on the segmentation of nuclei in WSI images has been presented over the past few decades. Methodologies that integrate thresholding, clustering, watershed algorithms, active contours, and variants along with a variety of pre-and post-processing techniques have been extensively studied [8]. A common problem among the aforementioned algorithmic approaches is the poor generalization across the wide spectrum of tissue morphologies introducing a lot of false positives.\n\nTo counter this, a number of learning-based approaches have been investigated in order to better tackle the variation over nuclei shape and color. One group of learning-based methods includes hand-engineered representations such as filter bank responses, geometric features, texture descriptors or other first order statistics paired with a classification algorithm [12,18]. Recent success of deep learning-based methods and the introduction of publicly available datasets [13,4] formed a second learning-based group of supervised approaches. In particular, [13] summarises some of these supervised approaches that are developed for multi-organ nuclei segmentation, most of them based on convolutional neural networks. Among them the best performing method proposes a multi-task scheme based on an FCN [14] architecture using a ResNet [9] backbone encoder with one branch to perform nuclei segmentation and a second one for contour segmentation. Yet, the emergence of self-supervised approaches in computer vision [17,5] has not successfully translated to applications in histopathology. In this paper, we proposed a self-supervised method for nuclei segmentation exploiting magnification level determination as a self-supervision signal.\n\n\nMethodology\n\nThe main idea behind our approach is that given a patch extracted from a WSI viewed at a certain magnification, the level of magnification can be ascertained by looking at the size and texture of the nuclei in the patch. By extension, we further assume that the nuclei are enough to determine the level of magnification, and other artefacts in the image are not necessary for this task.\n\nContrary to several concurrent computer vision pipelines which propose to train and evaluate models by feeding images sampled at several scales in order for them to learn multi-scale features [9] or models which specifically train for scale equivariance [23], we posit learning a scale-sensitive network which specifically trains for discriminative features for correct scale classification . Given a set of WSIs, we extract all tissue patches (tiles) from them at a fixed set of magnifications C. We consider only these tiles, with the \"ground-truth\" knowledge for each tile being at what magnification level it was extracted. Following our earlier reasoning, if nuclei in a given tile I \u2208 R 3\u00d7H\u00d7W are enough to predict the level of magnification, we assume that there exists a corresponding attention map A, so that A I is also enough to determine the magnification, where represents element-wise multiplication, and A \u2208 [0, 1] 1\u00d7H\u00d7W is a single channel attention image that focuses on the nuclei in the input tile ( Figure 1).\n\nWe design a fully-convolutional feature extractor F to predict the attention map A from the patch I. Our feature extractor consists of several layers of convolution operations with a gradual increase in the dilation of the kernels so as to incorporate information from a large neighborhood around every pixel. This feature extractor F regresses a confidence map a = F (I) \u2208 R 1\u00d7H\u00d7W , which is activated by a compressed and biased sigmoid function so that A = \u03c3 (a). In order to force the attention map to focus only on parts of the input patch, we apply a sparsity regularizer on A. This regularizer follows the idea and implementation of a concurrent work on unsupervised separation of nuclei and background [10]. Sparsity is imposed by picking the \u03b7-th percentile value in the confidence map a for all images in the batch, and choosing a threshold \u03c4 equal to the average of this percentile over an entire training batch. Formally,\n\u03c4 = 1 B B b=1 a (\u03b7) b ,(1)\nwhere a\n(\u03b7) b\nrepresents the \u03b7 100 \u00b7 HW -th largest value in the confidence map a b for the b-th image in the training batch of B images. The sigmoid is then defined as \u03c3(x) = 1 1+exp(\u2212r(x\u2212\u03c4 )) . It is compressed in order to force sharp transitions in the activated attention map, the compression being determined by r. We use r = 20 in our experiments.\n\nThe \"attended\" image J = A I is now enough for magnification or scale classification. We train a scale classification network G, which we initialize as a ResNet-34 [9], to predict the magnification level for each input tile J. The output of this network is scores for each magnification level, which is converted to probabilities using a softmax activation. The resulting model (Figure 1) is trainable in an end-to-end manner. We use negative log-likelihood to train the scale classification network G, and in turn the attention network F-\nL scale (p, l) = \u2212 logp l ;p i = [softmax (\u015d)] i ;\u015d = G (J) ; 1 \u2264 i \u2264 N C ,(2)\nwhere l is the scale ground-truth, and N C = |C| .\n\nNote that the terms scale (in the context of computer vision) and magnification (in the context of histopathology) are semantically equivalent and used interchangeably.\n\n\nSmoothness Regularization\n\nWe wish A to be semantically meaningful and smooth with blobs focusing on nuclei instead of having high frequency components. To this end, we incorporate a smoothness regularizer on the attention maps. The smoothness regularizer attempts simply to reduce the high frequency component that might appear in the attention map because of the compressed sigmoid. We employ a standard smoothness regularizer based on spatial gradients defined as\nL smooth = 1 (H \u2212 1)(W \u2212 1) i,j A i+1,j \u2212 A i,j 1 + A i,j+1 \u2212 A i,j 1 . (3)\n\nTransformation Equivariance\n\nEquivariance is a commonly used constraint on feature extractors for imposing semantic consistency [22,3]. A feature extractor f is equivariant to a transformation g if g is replicated in the feature vector produced by f , i.e., f (g(x)) = g(f (x)) , for an image x. In the given context, we want the attention map obtained from F to be equivariant to a set T of certain rigid transforms. We impose equivariance to these transformations through a simple mean squared error loss on A. Formally, we define the equivariance constraint as\nL equiv = 1 HW \u03c3 (t (F (I))) \u2212 \u03c3 (F (t (I))) 2 2 ,(4)\nfor a transformation t \u2208 T . We set T to include horizontal and vertical flips, matrix transpose, and rotations by 90, 180, and 270 degrees.Each training batch uses a random t \u2208 T .\n\n\nTraining\n\nThe overall model is trained in an end-to-end fashion, with L scale being the guiding self-supervision loss. For models incorporating all constraints, i.e., smoothness, sparsity, and equivariance, the total loss is L total = L scale + L smooth + L equiv .\n\nWe refer to a model trained with all these components together as M proposed . We also test models without one of these losses to demonstrate how each loss contributes to the learning. More specifically, we define the following models: We set the sparsity parameter \u03b7 empirically in order to choose the 93-rd percentile value for sparsity regularization. This is equivalent to assuming that, on an average, 7% of the pixels in a tile represent nuclei.\n\n\nPost Processing, Validation, and Model Selection\n\nIn order to retrieve the final instance segmentation from the attention image we employ a post processing pipeline that consists of 3 consequent steps. Firstly, two binary opening and closing morphological operations are sequentially performed using a coarse and a fine circular element (r = 2, r = 1). Next, the distance transform is calculated and smoothed using a Gaussian blur (\u03c3 = 1) on the new attention image and the local maxima are identified in a circular window (r = 7). Lastly, a marker driven watershed algorithm is applied using the inverse of the distance transform and the local maxima as markers.\n\nAs our model does not explicitly train for segmentation of nuclei, we require a validation set to determine which model is finally best-suited for our objective. To this end, we record the Dice score between the attention map and the ground truth on the validation set (see Section 4.1) at intermediate training epochs, and choose the epoch which performs the best. We noticed that, in general, performance increases initially on the validation, but flattens after \u223c30 epochs.\n\n\nExperimental Setup and Results\n\n\nDataset\n\nFor the purposes of this study we used the MoNuSeg database [13]. This dataset contains thirty 1000 \u00d7 1000 annotated patches extracted from thirty WSIs from different patients suffering from different cancer types from The Cancer Genomic Atlas (TCGA). We downloaded the WSIs corresponding to patients included in the training split and extracted tiles of size 224 \u00d7 224 from three different magnifications, namely 10\u00d7 , 20\u00d7 , and 40\u00d7 . For each extracted tile, we perform a simple thresholding in the HSV color space to determine whether the tile contains tissue or not. Tiles with less then 70% tissue cover are not used. Furthermore, a stain normalization step was performed using the color transfer approach described in [19]. Finally, a total of 1 125 737 tiles from the three aforementioned scales were selected and paired with the corresponding magnification level. The MoNuSeg train and test splits were employed, while the MoNuSeg train set was further split into training and validation as 19 and 11 examples, respectively. The annotations provided by MoNuSeg on the validation set were utilized for determining the four post processing parameters (Section 3.4) and for the final evaluation. For the model M \u00acWSI , which does not use whole slide images, we use the MoNuSeg patches instead for training, using the same strategy to split training and validation. We further evaluated the performance of our model that was trained on the MoNuSeg training set on the TNBC [15] and CoNSeP [7] datasets.\n\n\nImplementation\n\nWe use the PyTorch [16] library for our code. We use the Adam [11] optimizer in all our experiments, with an initial learning rate of 0.0002 , a weight decay of 0.0001 , and \u03b2 1 = 0.9 . We use a batch size of 32 , 100 minibatches per epoch, and randomly crop patches of size 160 \u00d7 160 from training images to use as inputs to our models. Furthermore, as there is a high imbalance among the number of tiles for each of the magnification level (images are about 4 times more in number for a one step increase in the magnification level), we force a per-batch sampling of images that is uniform over the magnification levels, i.e., each training batch is sampled so that images are divided equally over the magnification levels. This is important to prevent learning a biased model.\n\n\nResults\n\nTo highlight the potentials of our method we compare its performance with supervised and unsupervised methods on the MoNuSeg testset presented in [13]. In particular, in Table 1 we summarize the performance of three supervised methods (CNN2,CNN3 and Best Supervised) and two completely unsupervised methods (Fiji and CellProfiler) together with different variations of our proposed method. Our method outperforms the unsupervised methods, and it reports similar performance with CNN2 [13] and CNN3 [13] on the same dataset. While it reports lower performance than the best supervised method from [13], our formulation is quite modular and able to adapt multi-task schemes similar to the one adapted by the winning method of [13].\n\nOn the TNBC and CoNSeP datasets, our method is strongly competitive among the unsupervised methods. We should emphasize that these results have been obtained without retraining on these datasets. The CoNSeP dataset consists mainly of colorectal adenocarcinoma which is under-represented in the training set of MoNuSeg, proving very good generalization of our method.\n\nMoreover, from our ablation study (Table 1), it is clear that all components of the proposed model are essential. Sparsity is the most important as by removing it, the network regresses an attention map that is too smooth and not necessarily concentrating on nuclei, thus being semantically meaningless. Qualitatively, we observed that L smooth allows the network to focus on only on nuclei by removing attention over adjacent tissue regions, while L equiv further refines the attention maps by imposing geometric symmetry. Finally, in Figure 2 the segmentation map for one test image is presented. Results obtained from the M proposed attention network together with the nuclei segmentation after the performed post-processing are summarised.\n\n\nConclusion\n\nIn this paper, we propose and investigate a self-supervised method for nuclei segmentation of multi-organ histopathological images. In particular, we propose the use of the scale classification as a guiding self-supervision signal to train an attention network. We propose regularizers in order to regress attention maps that are semantically meaningful. Promising results comparable with supervised methods tested on the publicly available MoNuSeg dataset indicate the potentials of our method. We show also via. experiments on TNBC and ConSeP that our model generalizes well on new datasets. In the future, we aim to investigate the integration of our results within a treatment selection strategy. Nuclei presence is often a strong bio-marker as it concerns emerging cancer treatments (immunotherapy). Therefore, the end-to-end integration coupling histopathology and treatment outcomes could lead to prognostic tools as it concerns treatment response. Parallelly, other domains in medical imaging share concept similarities with the proposed concept.\n\nFig. 1 .\n1A diagram of our approach. Each patch I is fed to the attention network F generating an attention map A. The \"attended\" image J is then given to the scale classification network G. Both networks are trained in an end-to-end fashion. s, p, and d for convolution blocks refer to stride, padding, and dilation.\n\n1 .\n1M \u00acsmooth : does not include L smooth . 2. M \u00acequiv : does not include L equiv . 3. M \u00acsparse : does not include a sparsity regularizer on the attention map. In this case, the sigmoid is simply defined as \u03c3(x) = 1 1+exp(\u2212x) . 4. M \u00acWSI : a model which does not sample images from WSIs, but instead from a set of pre-extracted patches (see Section 4.1).\n\nFig. 2 .\n2Input, intermediate results and output of the post processing pipeline. From left to right: the input image; the attention map obtained from M proposed after the post-processing; the distance transform together with local maxima over-imposed in red; and the final result after the marker driven watershed.\n\n\nTable 1. Quantitative results of the different benchmarked methods on three different public available datasets. AJI, AHD, and ADC stand for Aggregated Jaccard Index, Average Hausdorff Distance, and Average Dice Coefficient, respectively. Methods marked with \u2020 are supervised.Test dataset \nMethod \nAJI [13] \nAHD \nADC \n\nMoNuSeg test \n\nCNN2 [13]  \u2020 \n0.3482 \n8.6924 \n0.6928 \nCNN3 [13]  \u2020 \n0.5083 \n7.6615 \n0.7623 \n\nBest Supervised [13]  \u2020 \n0.691 \n-\n-\n\nCellProfiler [13] \n0.1232 \n9.2771 \n0.5974 \nFiji [13] \n0.2733 \n8.9507 \n0.6493 \n\nM\u00acsparse \n0.0312 \n13.1415 \n0.2283 \nM \u00acsmooth \n0.1929 \n8.8166 \n0.4789 \nM\u00acWSI \n0.3025 \n8.2853 \n0.6209 \n\nM\u00acequiv \n0.4938 \n8.0091 \n0.7136 \nM proposed \n0.5354 \n7.7502 \n0.7477 \n\nTNBC[15] \n\nU-Net[7]  \u2020 \n0.514 \n-\n0.681 \nSegNet+WS[7]  \u2020 \n0.559 \n-\n0.758 \nHoverNet[7]  \u2020 \n0.590 \n-\n0.749 \n\nCellProfiler \n0.2080 \n-\n0.4157 \n\nM proposed \n0.2656 \n-\n0.5139 \n\nCoNSeP[7] \n\nSegNet[7]  \u2020 \n0.194 \n-\n0.796 \nU-Net[7]  \u2020 \n0.482 \n-\n0.724 \n\nCellProfiler[7] \n0.202 \n-\n0.434 \nQuPath[7] \n0.249 \n-\n0.588 \n\nM proposed \n0.1980 \n-\n0.587 \n\n\nMalignant mesothelioma of the pleura: interobserver variability. A Andrion, C Magnani, P Betta, A Donna, F Mollo, M Scelsi, P Bernardi, M Botta, B Terracini, Journal of clinical pathology. 489Andrion, A., Magnani, C., Betta, P., Donna, A., Mollo, F., Scelsi, M., Bernardi, P., Botta, M., Terracini, B.: Malignant mesothelioma of the pleura: interobserver variability. Journal of clinical pathology 48(9), 856-860 (1995)\n\nThe prognostic significance of the aberrant extremes of p53 immunophenotypes in breast cancer. D P Boyle, D G Mcart, G Irwin, C S Wilhelm-Benartzi, T F Lioe, E Sebastian, S Mcquaid, P W Hamilton, J A James, P B Mullan, Histopathology. 653Boyle, D.P., McArt, D.G., Irwin, G., Wilhelm-Benartzi, C.S., Lioe, T.F., Sebastian, E., McQuaid, S., Hamilton, P.W., James, J.A., Mullan, P.B., et al.: The prognostic significance of the aberrant extremes of p53 immunophenotypes in breast cancer. Histopathology 65(3), 340-352 (2014)\n\nT S Cohen, M Weiler, B Kicanaoglu, M Welling, arXiv:1902.04615Gauge equivariant convolutional networks and the icosahedral cnn. arXiv preprintCohen, T.S., Weiler, M., Kicanaoglu, B., Welling, M.: Gauge equivariant convolu- tional networks and the icosahedral cnn. arXiv preprint arXiv:1902.04615 (2019)\n\nPannuke: An open pan-cancer histology dataset for nuclei instance segmentation and classification. J Gamper, N Koohbanani, K Benet, A Khuram, N Rajpoot, Digital Pathology. ChamSpringer International PublishingGamper, J., Alemi Koohbanani, N., Benet, K., Khuram, A., Rajpoot, N.: Pannuke: An open pan-cancer histology dataset for nuclei instance segmentation and classi- fication. In: Digital Pathology. Springer International Publishing, Cham (2019)\n\nUnsupervised representation learning by predicting image rotations. S Gidaris, P Singh, N Komodakis, arXiv:1803.07728arXiv preprintGidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728 (2018)\n\nHistologic grading of prostate cancer: a perspective. D F Gleason, Human pathology. 233Gleason, D.F.: Histologic grading of prostate cancer: a perspective. Human pathol- ogy 23(3), 273-279 (1992)\n\nHover-net: Simultaneous segmentation and classification of nuclei in multitissue histology images. S Graham, Q D Vu, S E A Raza, A Azam, Y W Tsang, J T Kwak, N Rajpoot, Medical Image Analysis. 58101563Graham, S., Vu, Q.D., Raza, S.E.A., Azam, A., Tsang, Y.W., Kwak, J.T., Rajpoot, N.: Hover-net: Simultaneous segmentation and classification of nuclei in multi- tissue histology images. Medical Image Analysis 58, 101563 (2019)\n\nHistopathological image analysis: A review. M N Gurcan, L E Boucheron, A Can, A Madabhushi, N M Rajpoot, B Yener, IEEE reviews in biomedical engineering. 2Gurcan, M.N., Boucheron, L.E., Can, A., Madabhushi, A., Rajpoot, N.M., Yener, B.: Histopathological image analysis: A review. IEEE reviews in biomedical engi- neering 2, 147-171 (2009)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770-778 (2016)\n\nSparse autoencoder for unsupervised nucleus detection and representation in histopathology images. L Hou, V Nguyen, A B Kanevsky, D Samaras, T M Kurc, T Zhao, R R Gupta, Y Gao, Pattern recognition. 86Hou, L., Nguyen, V., Kanevsky, A.B., Samaras, D., Kurc, T.M., Zhao, T., Gupta, R.R., Gao, Y., et al.: Sparse autoencoder for unsupervised nucleus detection and representation in histopathology images. Pattern recognition 86 (2019)\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)\n\nPartitioning histopathological images: an integrated framework for supervised color-texture segmentation and cell splitting. H Kong, M Gurcan, K Belkacem-Boussaid, IEEE transactions on medical imaging. 309Kong, H., Gurcan, M., Belkacem-Boussaid, K.: Partitioning histopathological im- ages: an integrated framework for supervised color-texture segmentation and cell splitting. IEEE transactions on medical imaging 30(9), 1661-1677 (2011)\n\nA dataset and a technique for generalized nuclear segmentation for computational pathology. N Kumar, R Verma, S Sharma, S Bhargava, A Vahadane, A Sethi, IEEE transactions on medical imaging. 367Kumar, N., Verma, R., Sharma, S., Bhargava, S., Vahadane, A., Sethi, A.: A dataset and a technique for generalized nuclear segmentation for computational pathology. IEEE transactions on medical imaging 36(7), 1550-1560 (2017)\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionLong, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 3431-3440 (2015)\n\nSegmentation of nuclei in histopathology images by deep regression of the distance map. P Naylor, M La\u00e9, F Reyal, T Walter, IEEE transactions on medical imaging. 382Naylor, P., La\u00e9, M., Reyal, F., Walter, T.: Segmentation of nuclei in histopathology images by deep regression of the distance map. IEEE transactions on medical imaging 38(2), 448-459 (2018)\n\nA Paszke, S Gross, S Chintala, G Chanan, E Yang, Z Devito, Z Lin, A Desmaison, L Antiga, A Lerer, Automatic differentiation in pytorch. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A.: Automatic differentiation in pytorch (2017)\n\nCuriosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsPathak, D., Agrawal, P., Efros, A.A., Darrell, T.: Curiosity-driven exploration by self-supervised prediction. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. pp. 16-17 (2017)\n\nOverlapping cell nuclei segmentation using a spatially adaptive active physical model. M E Plissiti, C Nikou, IEEE Transactions on Image Processing. 2111Plissiti, M.E., Nikou, C.: Overlapping cell nuclei segmentation using a spatially adaptive active physical model. IEEE Transactions on Image Processing 21(11) (2012)\n\nColor transfer between images. E Reinhard, M Adhikhmin, B Gooch, P Shirley, IEEE Computer graphics and applications. 215Reinhard, E., Adhikhmin, M., Gooch, B., Shirley, P.: Color transfer between im- ages. IEEE Computer graphics and applications 21(5), 34-41 (2001)\n\nPredictive value of tumor-infiltrating lymphocytes to pathological complete response in neoadjuvant treated triple-negative breast cancers. M Ruan, T Tian, J Rao, X Xu, B Yu, W Yang, R Shui, Diagnostic pathology. 13166Ruan, M., Tian, T., Rao, J., Xu, X., Yu, B., Yang, W., Shui, R.: Predictive value of tumor-infiltrating lymphocytes to pathological complete response in neoadjuvant treated triple-negative breast cancers. Diagnostic pathology 13(1), 66 (2018)\n\nRubin's pathology: clinicopathologic foundations of medicine. R Rubin, D S Strayer, E Rubin, Lippincott Williams & WilkinsRubin, R., Strayer, D.S., Rubin, E., et al.: Rubin's pathology: clinicopathologic foundations of medicine. Lippincott Williams & Wilkins (2008)\n\nUnsupervised learning of object landmarks by factorized spatial embeddings. J Thewlis, H Bilen, A Vedaldi, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionThewlis, J., Bilen, H., Vedaldi, A.: Unsupervised learning of object landmarks by factorized spatial embeddings. In: Proceedings of the IEEE International Confer- ence on Computer Vision. pp. 5916-5925 (2017)\n\nDeep scale-spaces: Equivariance over scale. D Worrall, M Welling, Advances in Neural Information Processing Systems. Worrall, D., Welling, M.: Deep scale-spaces: Equivariance over scale. In: Advances in Neural Information Processing Systems (2019)\n\nAutomatic extraction of cell nuclei from H&E-stained histopathological images. F Yi, J Huang, L Yang, Y Xie, G Xiao, Journal of Medical Imaging. 42Yi, F., Huang, J., Yang, L., Xie, Y., Xiao, G.: Automatic extraction of cell nu- clei from H&E-stained histopathological images. Journal of Medical Imaging 4(2) (2017)\n", "annotations": {"author": "[{\"end\":275,\"start\":83},{\"end\":347,\"start\":276},{\"end\":497,\"start\":348},{\"end\":689,\"start\":498},{\"end\":773,\"start\":690},{\"end\":834,\"start\":774},{\"end\":1014,\"start\":835},{\"end\":1253,\"start\":1015}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":89},{\"end\":300,\"start\":285},{\"end\":363,\"start\":356},{\"end\":513,\"start\":505},{\"end\":701,\"start\":698},{\"end\":787,\"start\":782},{\"end\":849,\"start\":841},{\"end\":1033,\"start\":1021}]", "author_first_name": "[{\"end\":88,\"start\":83},{\"end\":284,\"start\":276},{\"end\":355,\"start\":348},{\"end\":504,\"start\":498},{\"end\":697,\"start\":690},{\"end\":781,\"start\":774},{\"end\":840,\"start\":835},{\"end\":1020,\"start\":1015}]", "author_affiliation": "[{\"end\":234,\"start\":103},{\"end\":274,\"start\":236},{\"end\":346,\"start\":302},{\"end\":434,\"start\":365},{\"end\":496,\"start\":436},{\"end\":559,\"start\":515},{\"end\":688,\"start\":561},{\"end\":772,\"start\":703},{\"end\":833,\"start\":789},{\"end\":982,\"start\":851},{\"end\":1013,\"start\":984},{\"end\":1166,\"start\":1035},{\"end\":1206,\"start\":1168},{\"end\":1252,\"start\":1208}]", "title": "[{\"end\":80,\"start\":1},{\"end\":1333,\"start\":1254}]", "venue": null, "abstract": "[{\"end\":2855,\"start\":1426}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3129,\"start\":3125},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3355,\"start\":3353},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3952,\"start\":3949},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4088,\"start\":4084},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4237,\"start\":4234},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4511,\"start\":4507},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4582,\"start\":4578},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4584,\"start\":4582},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6171,\"start\":6167},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6173,\"start\":6171},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6557,\"start\":6554},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7108,\"start\":7104},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7111,\"start\":7108},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7215,\"start\":7211},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7217,\"start\":7215},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7300,\"start\":7296},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7544,\"start\":7540},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7576,\"start\":7573},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7756,\"start\":7752},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7758,\"start\":7756},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8575,\"start\":8572},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8638,\"start\":8634},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10124,\"start\":10120},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10893,\"start\":10890},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12244,\"start\":12240},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12246,\"start\":12244},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14885,\"start\":14881},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15549,\"start\":15545},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16302,\"start\":16298},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16317,\"start\":16314},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16369,\"start\":16365},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16412,\"start\":16408},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17287,\"start\":17283},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17625,\"start\":17621},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17639,\"start\":17635},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17737,\"start\":17733},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17865,\"start\":17861}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20367,\"start\":20049},{\"attributes\":{\"id\":\"fig_1\"},\"end\":20726,\"start\":20368},{\"attributes\":{\"id\":\"fig_2\"},\"end\":21043,\"start\":20727},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":22077,\"start\":21044}]", "paragraph": "[{\"end\":4089,\"start\":2871},{\"end\":4937,\"start\":4091},{\"end\":5553,\"start\":4939},{\"end\":6736,\"start\":5570},{\"end\":7976,\"start\":6738},{\"end\":8378,\"start\":7992},{\"end\":9409,\"start\":8380},{\"end\":10343,\"start\":9411},{\"end\":10378,\"start\":10371},{\"end\":10724,\"start\":10385},{\"end\":11265,\"start\":10726},{\"end\":11395,\"start\":11345},{\"end\":11565,\"start\":11397},{\"end\":12034,\"start\":11595},{\"end\":12675,\"start\":12141},{\"end\":12911,\"start\":12730},{\"end\":13179,\"start\":12924},{\"end\":13632,\"start\":13181},{\"end\":14298,\"start\":13685},{\"end\":14776,\"start\":14300},{\"end\":16327,\"start\":14821},{\"end\":17125,\"start\":16346},{\"end\":17866,\"start\":17137},{\"end\":18234,\"start\":17868},{\"end\":18979,\"start\":18236},{\"end\":20048,\"start\":18994}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10370,\"start\":10344},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10384,\"start\":10379},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11344,\"start\":11266},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12110,\"start\":12035},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12729,\"start\":12676}]", "table_ref": "[{\"end\":17314,\"start\":17307},{\"end\":18279,\"start\":18270}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2869,\"start\":2857},{\"attributes\":{\"n\":\"2\"},\"end\":5568,\"start\":5556},{\"attributes\":{\"n\":\"3\"},\"end\":7990,\"start\":7979},{\"attributes\":{\"n\":\"3.1\"},\"end\":11593,\"start\":11568},{\"attributes\":{\"n\":\"3.2\"},\"end\":12139,\"start\":12112},{\"attributes\":{\"n\":\"3.3\"},\"end\":12922,\"start\":12914},{\"attributes\":{\"n\":\"3.4\"},\"end\":13683,\"start\":13635},{\"attributes\":{\"n\":\"4\"},\"end\":14809,\"start\":14779},{\"attributes\":{\"n\":\"4.1\"},\"end\":14819,\"start\":14812},{\"attributes\":{\"n\":\"4.2\"},\"end\":16344,\"start\":16330},{\"attributes\":{\"n\":\"4.3\"},\"end\":17135,\"start\":17128},{\"attributes\":{\"n\":\"5\"},\"end\":18992,\"start\":18982},{\"end\":20058,\"start\":20050},{\"end\":20372,\"start\":20369},{\"end\":20736,\"start\":20728}]", "table": "[{\"end\":22077,\"start\":21322}]", "figure_caption": "[{\"end\":20367,\"start\":20060},{\"end\":20726,\"start\":20374},{\"end\":21043,\"start\":20738},{\"end\":21322,\"start\":21046}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9407,\"start\":9399},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11113,\"start\":11104},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18780,\"start\":18772}]", "bib_author_first_name": "[{\"end\":22145,\"start\":22144},{\"end\":22156,\"start\":22155},{\"end\":22167,\"start\":22166},{\"end\":22176,\"start\":22175},{\"end\":22185,\"start\":22184},{\"end\":22194,\"start\":22193},{\"end\":22204,\"start\":22203},{\"end\":22216,\"start\":22215},{\"end\":22225,\"start\":22224},{\"end\":22596,\"start\":22595},{\"end\":22598,\"start\":22597},{\"end\":22607,\"start\":22606},{\"end\":22609,\"start\":22608},{\"end\":22618,\"start\":22617},{\"end\":22627,\"start\":22626},{\"end\":22629,\"start\":22628},{\"end\":22649,\"start\":22648},{\"end\":22651,\"start\":22650},{\"end\":22659,\"start\":22658},{\"end\":22672,\"start\":22671},{\"end\":22683,\"start\":22682},{\"end\":22685,\"start\":22684},{\"end\":22697,\"start\":22696},{\"end\":22699,\"start\":22698},{\"end\":22708,\"start\":22707},{\"end\":22710,\"start\":22709},{\"end\":23024,\"start\":23023},{\"end\":23026,\"start\":23025},{\"end\":23035,\"start\":23034},{\"end\":23045,\"start\":23044},{\"end\":23059,\"start\":23058},{\"end\":23427,\"start\":23426},{\"end\":23437,\"start\":23436},{\"end\":23451,\"start\":23450},{\"end\":23460,\"start\":23459},{\"end\":23470,\"start\":23469},{\"end\":23847,\"start\":23846},{\"end\":23858,\"start\":23857},{\"end\":23867,\"start\":23866},{\"end\":24111,\"start\":24110},{\"end\":24113,\"start\":24112},{\"end\":24353,\"start\":24352},{\"end\":24363,\"start\":24362},{\"end\":24365,\"start\":24364},{\"end\":24371,\"start\":24370},{\"end\":24375,\"start\":24372},{\"end\":24383,\"start\":24382},{\"end\":24391,\"start\":24390},{\"end\":24393,\"start\":24392},{\"end\":24402,\"start\":24401},{\"end\":24404,\"start\":24403},{\"end\":24412,\"start\":24411},{\"end\":24726,\"start\":24725},{\"end\":24728,\"start\":24727},{\"end\":24738,\"start\":24737},{\"end\":24740,\"start\":24739},{\"end\":24753,\"start\":24752},{\"end\":24760,\"start\":24759},{\"end\":24774,\"start\":24773},{\"end\":24776,\"start\":24775},{\"end\":24787,\"start\":24786},{\"end\":25069,\"start\":25068},{\"end\":25075,\"start\":25074},{\"end\":25084,\"start\":25083},{\"end\":25091,\"start\":25090},{\"end\":25524,\"start\":25523},{\"end\":25531,\"start\":25530},{\"end\":25541,\"start\":25540},{\"end\":25543,\"start\":25542},{\"end\":25555,\"start\":25554},{\"end\":25566,\"start\":25565},{\"end\":25568,\"start\":25567},{\"end\":25576,\"start\":25575},{\"end\":25584,\"start\":25583},{\"end\":25586,\"start\":25585},{\"end\":25595,\"start\":25594},{\"end\":25901,\"start\":25900},{\"end\":25903,\"start\":25902},{\"end\":25913,\"start\":25912},{\"end\":26178,\"start\":26177},{\"end\":26186,\"start\":26185},{\"end\":26196,\"start\":26195},{\"end\":26584,\"start\":26583},{\"end\":26593,\"start\":26592},{\"end\":26602,\"start\":26601},{\"end\":26612,\"start\":26611},{\"end\":26624,\"start\":26623},{\"end\":26636,\"start\":26635},{\"end\":26969,\"start\":26968},{\"end\":26977,\"start\":26976},{\"end\":26990,\"start\":26989},{\"end\":27429,\"start\":27428},{\"end\":27439,\"start\":27438},{\"end\":27446,\"start\":27445},{\"end\":27455,\"start\":27454},{\"end\":27698,\"start\":27697},{\"end\":27708,\"start\":27707},{\"end\":27717,\"start\":27716},{\"end\":27729,\"start\":27728},{\"end\":27739,\"start\":27738},{\"end\":27747,\"start\":27746},{\"end\":27757,\"start\":27756},{\"end\":27764,\"start\":27763},{\"end\":27777,\"start\":27776},{\"end\":27787,\"start\":27786},{\"end\":28057,\"start\":28056},{\"end\":28067,\"start\":28066},{\"end\":28078,\"start\":28077},{\"end\":28080,\"start\":28079},{\"end\":28089,\"start\":28088},{\"end\":28570,\"start\":28569},{\"end\":28572,\"start\":28571},{\"end\":28584,\"start\":28583},{\"end\":28834,\"start\":28833},{\"end\":28846,\"start\":28845},{\"end\":28859,\"start\":28858},{\"end\":28868,\"start\":28867},{\"end\":29210,\"start\":29209},{\"end\":29218,\"start\":29217},{\"end\":29226,\"start\":29225},{\"end\":29233,\"start\":29232},{\"end\":29239,\"start\":29238},{\"end\":29245,\"start\":29244},{\"end\":29253,\"start\":29252},{\"end\":29594,\"start\":29593},{\"end\":29603,\"start\":29602},{\"end\":29605,\"start\":29604},{\"end\":29616,\"start\":29615},{\"end\":29875,\"start\":29874},{\"end\":29886,\"start\":29885},{\"end\":29895,\"start\":29894},{\"end\":30281,\"start\":30280},{\"end\":30292,\"start\":30291},{\"end\":30565,\"start\":30564},{\"end\":30571,\"start\":30570},{\"end\":30580,\"start\":30579},{\"end\":30588,\"start\":30587},{\"end\":30595,\"start\":30594}]", "bib_author_last_name": "[{\"end\":22153,\"start\":22146},{\"end\":22164,\"start\":22157},{\"end\":22173,\"start\":22168},{\"end\":22182,\"start\":22177},{\"end\":22191,\"start\":22186},{\"end\":22201,\"start\":22195},{\"end\":22213,\"start\":22205},{\"end\":22222,\"start\":22217},{\"end\":22235,\"start\":22226},{\"end\":22604,\"start\":22599},{\"end\":22615,\"start\":22610},{\"end\":22624,\"start\":22619},{\"end\":22646,\"start\":22630},{\"end\":22656,\"start\":22652},{\"end\":22669,\"start\":22660},{\"end\":22680,\"start\":22673},{\"end\":22694,\"start\":22686},{\"end\":22705,\"start\":22700},{\"end\":22717,\"start\":22711},{\"end\":23032,\"start\":23027},{\"end\":23042,\"start\":23036},{\"end\":23056,\"start\":23046},{\"end\":23067,\"start\":23060},{\"end\":23434,\"start\":23428},{\"end\":23448,\"start\":23438},{\"end\":23457,\"start\":23452},{\"end\":23467,\"start\":23461},{\"end\":23478,\"start\":23471},{\"end\":23855,\"start\":23848},{\"end\":23864,\"start\":23859},{\"end\":23877,\"start\":23868},{\"end\":24121,\"start\":24114},{\"end\":24360,\"start\":24354},{\"end\":24368,\"start\":24366},{\"end\":24380,\"start\":24376},{\"end\":24388,\"start\":24384},{\"end\":24399,\"start\":24394},{\"end\":24409,\"start\":24405},{\"end\":24420,\"start\":24413},{\"end\":24735,\"start\":24729},{\"end\":24750,\"start\":24741},{\"end\":24757,\"start\":24754},{\"end\":24771,\"start\":24761},{\"end\":24784,\"start\":24777},{\"end\":24793,\"start\":24788},{\"end\":25072,\"start\":25070},{\"end\":25081,\"start\":25076},{\"end\":25088,\"start\":25085},{\"end\":25095,\"start\":25092},{\"end\":25528,\"start\":25525},{\"end\":25538,\"start\":25532},{\"end\":25552,\"start\":25544},{\"end\":25563,\"start\":25556},{\"end\":25573,\"start\":25569},{\"end\":25581,\"start\":25577},{\"end\":25592,\"start\":25587},{\"end\":25599,\"start\":25596},{\"end\":25910,\"start\":25904},{\"end\":25916,\"start\":25914},{\"end\":26183,\"start\":26179},{\"end\":26193,\"start\":26187},{\"end\":26214,\"start\":26197},{\"end\":26590,\"start\":26585},{\"end\":26599,\"start\":26594},{\"end\":26609,\"start\":26603},{\"end\":26621,\"start\":26613},{\"end\":26633,\"start\":26625},{\"end\":26642,\"start\":26637},{\"end\":26974,\"start\":26970},{\"end\":26987,\"start\":26978},{\"end\":26998,\"start\":26991},{\"end\":27436,\"start\":27430},{\"end\":27443,\"start\":27440},{\"end\":27452,\"start\":27447},{\"end\":27462,\"start\":27456},{\"end\":27705,\"start\":27699},{\"end\":27714,\"start\":27709},{\"end\":27726,\"start\":27718},{\"end\":27736,\"start\":27730},{\"end\":27744,\"start\":27740},{\"end\":27754,\"start\":27748},{\"end\":27761,\"start\":27758},{\"end\":27774,\"start\":27765},{\"end\":27784,\"start\":27778},{\"end\":27793,\"start\":27788},{\"end\":28064,\"start\":28058},{\"end\":28075,\"start\":28068},{\"end\":28086,\"start\":28081},{\"end\":28097,\"start\":28090},{\"end\":28581,\"start\":28573},{\"end\":28590,\"start\":28585},{\"end\":28843,\"start\":28835},{\"end\":28856,\"start\":28847},{\"end\":28865,\"start\":28860},{\"end\":28876,\"start\":28869},{\"end\":29215,\"start\":29211},{\"end\":29223,\"start\":29219},{\"end\":29230,\"start\":29227},{\"end\":29236,\"start\":29234},{\"end\":29242,\"start\":29240},{\"end\":29250,\"start\":29246},{\"end\":29258,\"start\":29254},{\"end\":29600,\"start\":29595},{\"end\":29613,\"start\":29606},{\"end\":29622,\"start\":29617},{\"end\":29883,\"start\":29876},{\"end\":29892,\"start\":29887},{\"end\":29903,\"start\":29896},{\"end\":30289,\"start\":30282},{\"end\":30300,\"start\":30293},{\"end\":30568,\"start\":30566},{\"end\":30577,\"start\":30572},{\"end\":30585,\"start\":30581},{\"end\":30592,\"start\":30589},{\"end\":30600,\"start\":30596}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":18022822},\"end\":22498,\"start\":22079},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1218978},\"end\":23021,\"start\":22500},{\"attributes\":{\"doi\":\"arXiv:1902.04615\",\"id\":\"b2\"},\"end\":23325,\"start\":23023},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":195785143},\"end\":23776,\"start\":23327},{\"attributes\":{\"doi\":\"arXiv:1803.07728\",\"id\":\"b4\"},\"end\":24054,\"start\":23778},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":25758567},\"end\":24251,\"start\":24056},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":195583959},\"end\":24679,\"start\":24253},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8575576},\"end\":25020,\"start\":24681},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206594692},\"end\":25422,\"start\":25022},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16460672},\"end\":25854,\"start\":25424},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b10\"},\"end\":26050,\"start\":25856},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10035420},\"end\":26489,\"start\":26052},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5162860},\"end\":26910,\"start\":26491},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1629541},\"end\":27338,\"start\":26912},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":59601271},\"end\":27695,\"start\":27340},{\"attributes\":{\"id\":\"b15\"},\"end\":27994,\"start\":27697},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":20045336},\"end\":28480,\"start\":27996},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4664881},\"end\":28800,\"start\":28482},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14088925},\"end\":29067,\"start\":28802},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52141908},\"end\":29529,\"start\":29069},{\"attributes\":{\"id\":\"b20\"},\"end\":29796,\"start\":29531},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":19833076},\"end\":30234,\"start\":29798},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":167217644},\"end\":30483,\"start\":30236},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5027896},\"end\":30799,\"start\":30485}]", "bib_title": "[{\"end\":22142,\"start\":22079},{\"end\":22593,\"start\":22500},{\"end\":23424,\"start\":23327},{\"end\":24108,\"start\":24056},{\"end\":24350,\"start\":24253},{\"end\":24723,\"start\":24681},{\"end\":25066,\"start\":25022},{\"end\":25521,\"start\":25424},{\"end\":26175,\"start\":26052},{\"end\":26581,\"start\":26491},{\"end\":26966,\"start\":26912},{\"end\":27426,\"start\":27340},{\"end\":28054,\"start\":27996},{\"end\":28567,\"start\":28482},{\"end\":28831,\"start\":28802},{\"end\":29207,\"start\":29069},{\"end\":29872,\"start\":29798},{\"end\":30278,\"start\":30236},{\"end\":30562,\"start\":30485}]", "bib_author": "[{\"end\":22155,\"start\":22144},{\"end\":22166,\"start\":22155},{\"end\":22175,\"start\":22166},{\"end\":22184,\"start\":22175},{\"end\":22193,\"start\":22184},{\"end\":22203,\"start\":22193},{\"end\":22215,\"start\":22203},{\"end\":22224,\"start\":22215},{\"end\":22237,\"start\":22224},{\"end\":22606,\"start\":22595},{\"end\":22617,\"start\":22606},{\"end\":22626,\"start\":22617},{\"end\":22648,\"start\":22626},{\"end\":22658,\"start\":22648},{\"end\":22671,\"start\":22658},{\"end\":22682,\"start\":22671},{\"end\":22696,\"start\":22682},{\"end\":22707,\"start\":22696},{\"end\":22719,\"start\":22707},{\"end\":23034,\"start\":23023},{\"end\":23044,\"start\":23034},{\"end\":23058,\"start\":23044},{\"end\":23069,\"start\":23058},{\"end\":23436,\"start\":23426},{\"end\":23450,\"start\":23436},{\"end\":23459,\"start\":23450},{\"end\":23469,\"start\":23459},{\"end\":23480,\"start\":23469},{\"end\":23857,\"start\":23846},{\"end\":23866,\"start\":23857},{\"end\":23879,\"start\":23866},{\"end\":24123,\"start\":24110},{\"end\":24362,\"start\":24352},{\"end\":24370,\"start\":24362},{\"end\":24382,\"start\":24370},{\"end\":24390,\"start\":24382},{\"end\":24401,\"start\":24390},{\"end\":24411,\"start\":24401},{\"end\":24422,\"start\":24411},{\"end\":24737,\"start\":24725},{\"end\":24752,\"start\":24737},{\"end\":24759,\"start\":24752},{\"end\":24773,\"start\":24759},{\"end\":24786,\"start\":24773},{\"end\":24795,\"start\":24786},{\"end\":25074,\"start\":25068},{\"end\":25083,\"start\":25074},{\"end\":25090,\"start\":25083},{\"end\":25097,\"start\":25090},{\"end\":25530,\"start\":25523},{\"end\":25540,\"start\":25530},{\"end\":25554,\"start\":25540},{\"end\":25565,\"start\":25554},{\"end\":25575,\"start\":25565},{\"end\":25583,\"start\":25575},{\"end\":25594,\"start\":25583},{\"end\":25601,\"start\":25594},{\"end\":25912,\"start\":25900},{\"end\":25918,\"start\":25912},{\"end\":26185,\"start\":26177},{\"end\":26195,\"start\":26185},{\"end\":26216,\"start\":26195},{\"end\":26592,\"start\":26583},{\"end\":26601,\"start\":26592},{\"end\":26611,\"start\":26601},{\"end\":26623,\"start\":26611},{\"end\":26635,\"start\":26623},{\"end\":26644,\"start\":26635},{\"end\":26976,\"start\":26968},{\"end\":26989,\"start\":26976},{\"end\":27000,\"start\":26989},{\"end\":27438,\"start\":27428},{\"end\":27445,\"start\":27438},{\"end\":27454,\"start\":27445},{\"end\":27464,\"start\":27454},{\"end\":27707,\"start\":27697},{\"end\":27716,\"start\":27707},{\"end\":27728,\"start\":27716},{\"end\":27738,\"start\":27728},{\"end\":27746,\"start\":27738},{\"end\":27756,\"start\":27746},{\"end\":27763,\"start\":27756},{\"end\":27776,\"start\":27763},{\"end\":27786,\"start\":27776},{\"end\":27795,\"start\":27786},{\"end\":28066,\"start\":28056},{\"end\":28077,\"start\":28066},{\"end\":28088,\"start\":28077},{\"end\":28099,\"start\":28088},{\"end\":28583,\"start\":28569},{\"end\":28592,\"start\":28583},{\"end\":28845,\"start\":28833},{\"end\":28858,\"start\":28845},{\"end\":28867,\"start\":28858},{\"end\":28878,\"start\":28867},{\"end\":29217,\"start\":29209},{\"end\":29225,\"start\":29217},{\"end\":29232,\"start\":29225},{\"end\":29238,\"start\":29232},{\"end\":29244,\"start\":29238},{\"end\":29252,\"start\":29244},{\"end\":29260,\"start\":29252},{\"end\":29602,\"start\":29593},{\"end\":29615,\"start\":29602},{\"end\":29624,\"start\":29615},{\"end\":29885,\"start\":29874},{\"end\":29894,\"start\":29885},{\"end\":29905,\"start\":29894},{\"end\":30291,\"start\":30280},{\"end\":30302,\"start\":30291},{\"end\":30570,\"start\":30564},{\"end\":30579,\"start\":30570},{\"end\":30587,\"start\":30579},{\"end\":30594,\"start\":30587},{\"end\":30602,\"start\":30594}]", "bib_venue": "[{\"end\":22266,\"start\":22237},{\"end\":22733,\"start\":22719},{\"end\":23149,\"start\":23085},{\"end\":23497,\"start\":23480},{\"end\":23844,\"start\":23778},{\"end\":24138,\"start\":24123},{\"end\":24444,\"start\":24422},{\"end\":24833,\"start\":24795},{\"end\":25174,\"start\":25097},{\"end\":25620,\"start\":25601},{\"end\":25898,\"start\":25856},{\"end\":26252,\"start\":26216},{\"end\":26680,\"start\":26644},{\"end\":27077,\"start\":27000},{\"end\":27500,\"start\":27464},{\"end\":27831,\"start\":27795},{\"end\":28186,\"start\":28099},{\"end\":28629,\"start\":28592},{\"end\":28917,\"start\":28878},{\"end\":29280,\"start\":29260},{\"end\":29591,\"start\":29531},{\"end\":29972,\"start\":29905},{\"end\":30351,\"start\":30302},{\"end\":30628,\"start\":30602},{\"end\":23503,\"start\":23499},{\"end\":25238,\"start\":25176},{\"end\":27141,\"start\":27079},{\"end\":28260,\"start\":28188},{\"end\":30026,\"start\":29974}]"}}}, "year": 2023, "month": 12, "day": 17}