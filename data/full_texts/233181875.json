{"id": 233181875, "updated": "2023-10-06 04:53:08.435", "metadata": {"title": "Learning optical flow from still images", "authors": "[{\"first\":\"Filippo\",\"last\":\"Aleotti\",\"middle\":[]},{\"first\":\"Matteo\",\"last\":\"Poggi\",\"middle\":[]},{\"first\":\"Stefano\",\"last\":\"Mattoccia\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 4, "day": 8}, "abstract": "This paper deals with the scarcity of data for training optical flow networks, highlighting the limitations of existing sources such as labeled synthetic datasets or unlabeled real videos. Specifically, we introduce a framework to generate accurate ground-truth optical flow annotations quickly and in large amounts from any readily available single real picture. Given an image, we use an off-the-shelf monocular depth estimation network to build a plausible point cloud for the observed scene. Then, we virtually move the camera in the reconstructed environment with known motion vectors and rotation angles, allowing us to synthesize both a novel view and the corresponding optical flow field connecting each pixel in the input image to the one in the new frame. When trained with our data, state-of-the-art optical flow networks achieve superior generalization to unseen real data compared to the same models trained either on annotated synthetic datasets or unlabeled videos, and better specialization if combined with synthetic images.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.03965", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/AleottiPM21", "doi": "10.1109/cvpr46437.2021.01495"}}, "content": {"source": {"pdf_hash": "6d453f5df60963a232fdc62f5c5f70b4c55f5185", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.03965v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2104.03965", "status": "GREEN"}}, "grobid": {"id": "46644cbd3c36498eb36ade9998a61d5b406e4b7f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6d453f5df60963a232fdc62f5c5f70b4c55f5185.txt", "contents": "\nLearning optical flow from still images\n\n\nFilippo Aleotti filippo.aleotti2@unibo.it \nDepartment of Computer Science and Engineering (DISI)\nUniversity of Bologna\nItaly\n\nMatteo Poggi m.poggi@unibo.it \nDepartment of Computer Science and Engineering (DISI)\nUniversity of Bologna\nItaly\n\nStefano Mattoccia stefano.mattoccia@unibo.it \nDepartment of Computer Science and Engineering (DISI)\nUniversity of Bologna\nItaly\n\nLearning optical flow from still images\n\nThis paper deals with the scarcity of data for training optical flow networks, highlighting the limitations of existing sources such as labeled synthetic datasets or unlabeled real videos. Specifically, we introduce a framework to generate accurate ground-truth optical flow annotations quickly and in large amounts from any readily available single real picture. Given an image, we use an off-the-shelf monocular depth estimation network to build a plausible point cloud for the observed scene. Then, we virtually move the camera in the reconstructed environment with known motion vectors and rotation angles, allowing us to synthesize both a novel view and the corresponding optical flow field connecting each pixel in the input image to the one in the new frame. When trained with our data, state-of-the-art optical flow networks achieve superior generalization to unseen real data compared to the same models trained either on annotated synthetic datasets or unlabeled videos, and better specialization if combined with synthetic images.\n\nIntroduction\n\nThe problem of estimating per-pixel motion between video frames, also known as optical flow [50], has a long history in computer vision and remains far from being solved. On top of it, several higher-level tasks such as tracking, action recognition and more are typically performed. Among the main challenges for optical flow systems, there are occlusions, motion blur and lack of texture.\n\nDeep learning has played a crucial role in the latest years of research on this topic, at first to learn a data term [1,65] and then to directly infer the dense optical flow field in end-to-end manner [7,20,51,52,18,19,17,53], currently representing the state-of-the-art in this field. This achievement has been made possible by the availability of extensive training data labeled with ground-truth optical flow fields, most of them obtained through computer graphics * Joint first authorship. a) b) c) d) Figure 1. Depthstillation from still images. From left to right: a) single input image, b) estimated depth map, c) optical flow field consequence of virtual camera motion, d) virtual view. We show b) as inverse depth to improve visualization. [5,7,20]. Unfortunately, these large datasets alone are not enough to train a neural network for its deployment in real environments, because of the well-known domain shift occurring when moving from synthetic images to real ones. A notable example is represented by the KITTI optical flow benchmarks [10,35], over which deep networks that have been trained only on synthetic data perform poorly, as witnessed by recent works [20,51,53]. This problem is known in literature and has been faced for other tasks such as semantic segmentation [15,39,43,55] or stereo depth estimation [56,57,68,61]. To fully restore a level of accuracy comparable to the one achieved on synthetic data, fine-tuning on imagery similar to the testing domain is usually required. Anyway, obtaining ground-truth optical flow labels for real images is particularly challenging because there exists virtually no sensor capable of acquiring ground-truth correspondences between points in challenging real-world scenes [37]. A viable strategy consists into passing through depth sensors (e.g., LiDARs), indeed optical flow fields can be obtained by projecting the 3D points from a given frame into the next frame [10], although it cannot take into account independently moving objects, for which manual post-processing or annotation remains necessary [35,37]. The literature is rich of self-supervised strate-gies [34,30,28,23] from unlabeled videos to soften this constraint, but they mostly excel when deployed on data similar to those observed for training, a scenario unlikely to occur in most real applications. Given both the aforementioned domain shift issue and the lack of real imagery annotated for optical flow, we propose an alternative scheme to distill proxy labels from real images for effective training of optical flow estimation networks. Following the observation that depth is required to obtain dense matching across views through reprojection [10,35,37], we use a monocular depth estimation network to revert the annotation process: given a single image and its estimated depth, we suppose a virtual motion of the camera to compute a dense optical flow field and, consequently, synthesize a new virtual image accordingly. For instance, in Figure 1 from a) pictures of a person and a cat, we estimate b) monocular depth and generate c) a flow field used to synthesize d) a novel view. We dub this process Depthstillation, and any single image is eligible for producing optical flow annotations through it.\n\nExperiments carried out on synthetic (Sintel) and real (KITTI 2012 and 2015) datasets support our main claims:\n\n\u2022 We show that it is possible to train an optical flow network on a collection of unrelated images, e.g. single pictures readily available online \u2022 Using real images through our technique allows us to train networks that better transfer to real data than their counterparts trained on synthetic images, while finetuning these latter on dephtstilled frames and then on real data improves specialization \u2022 Networks trained on our dephtstilled frames and flow labels better transfer to new real datasets than state-ofthe-art self-supervised strategies using real videos [23] \n\n\nRelated Work\n\nIn this section, we review the literature relevant to the research topics touched by our work.\n\nOptical Flow -Energy Minimization models. For a long time, optical flow has been cast as a continuous optimization problem through variational frameworks [16,3,67]. These approaches involve a data term coupled with regularization terms, and improvements to the former [4,63] or the latter [44] represented the primary strategy to increase optical flow accuracy for years [50]. More recent strategies consider optical flow as a discrete optimization problem, despite managing the sizeable 2D search space required to determine corresponding pixels between images [36,6,65] is challenging. Until a few years ago [7], early attempts to improve optical flow with deep networks mainly focused on learning more robust data terms by training CNNs to match patches across images [63,1,65].\n\nEnd-to-end Optical Flow. FlowNet [7] is the first endto-end deep architecture proposed for optical flow. Concurrently, to satisfy the massive amount of training data required, synthetic datasets with dense optical flow groundtruth labels were made available [7,33]. Eventually, other architectures [20,51,52,18,19,17,53] further improved accuracy on popular synthetic [5,33] and real [35,10] benchmarks, with RAFT [53] representing state-of-the-art.\n\nFor most existing networks, generalization remains a cause of concerns, in particular when moving from synthetic [7,33] to real images [10,35]. With our work, we show how to generate plausible training samples from real, unrelated images allowing for superior generalization.\n\nSelf-supervised Optical Flow. Being ground-truths hard to obtain for real data, self-supervised strategies allow to relax this requirement [21,46,34]. More recent advances introduced teaching-student frameworks [29], occlusion generation [30] and transformed data from augmentation [28]. Jonschkowski et al. [23] highlighted the key components to achieve state-of-the-art results in this setting.\n\nMost of these approaches train on unlabeled videos (e.g., from the KITTI 2015 multiview dataset [35]) from the same domain where the evaluation is carried out (e.g., the KITTI 2015 optical flow benchmark). In contrast, in our work, we relax both constraints of having i) organized video collections and ii) taken in similar domains, achieving superior generalization compared to self-supervised networks.\n\nSingle Image Depth Estimation. In parallel to supervised approaches [64,25,9], many works focused on selfsupervised strategies, aimed at replacing ground-truth labels with collections of images, either relying on stereo pairs [11,58,62] or monocular videos [69,12,13,59]. To improve generalization, recent works [26,45] exploited supervision from a large variety of images and auxiliary strategies such as Multi-View Stereo methods [48].\n\nShared by all these methods is the assumption of static scenes, required for reprojection across multiple views. In this paper, we show how a network trained according to such a strategy allows for generating, from still images, training data that well model motions, to train optical flow networks that are effective in presence of moving objects.\n\nNovel View Synthesis. View synthesis aims at creating new images observed from arbitrary viewpoints starting from a given scene. It is gaining an ever increasing interest in computer vision [66,38,8,60,47], and it is a fundamental step to address many other tasks, such as video interpolation [22,2,40] or 3D effects [49,70,41].\n\nConversely, we focus on creating image pairs and corresponding ground-truth pixel displacements rather than visually pleasant videos. While some of the techniques mentioned above rely on pre-trained flow networks [40], our goal is to generate data to train these latter.\n\nData distillation through depth estimation. Strictly re-Flow colors Depth colors Figure 2. Overview of the proposed depthstillation pipeline. Given a single image I0 and its estimated depth map D0, we place the camera in c0 and virtually move it (red arrow) towards a new viewpoint c1. From the depth and virtual ego-motion, we obtain optical flow labels F0\u21921 and a novel I1 through forward warping. lated to our work is [61], estimating depth from single images to synthesize virtual right views and thus obtain stereo pairs, used to train deep stereo networks.\n\nDespite the analogy of using single image depth estimation, we point out that our goal differs from [61] since we aim at modeling arbitrary motions in the scene (i.e., optical flow) rather than a horizontal pixel displacement between synchronized images (i.e., disparity). Purposely, we will describe the additional strategies required to attain, from single still images, the best training data for optical flow networks.\n\n\nDepthstillation pipeline\n\nIn this section we illustrate our proposed framework to generate new virtual views I 1 from single images I 0 , with corresponding dense optical flow ground-truth maps F 0\u21921 . An overview of our pipeline is shown in Figure 2.\n\nVirtual camera motion engine. Given I 0 , an off-theshelf monocular depth network \u03a6 is used to estimate its depth map D 0\nD 0 = \u03a6(I 0 )(1)\nused to project pixels in I 0 to 3D space according to some plausible inverse intrinsics matrix K \u22121 . In case the network estimates inverse depth, we bring it to the depth domain first. D 0 usually shows blurred edges [61,49], causing flying pixels in the 3D space that can be easily sharpened via edge-preserving filters [32]. We now assume the camera used to frame image I 0 to be at 3D location c 0 and apply an arbitrary virtual motion, moving it towards a new position c 1 . To this aim, we generate a plausible rotation R 1 by sampling a random triplet of Euler angles and a plausible translation t 1 by sampling a random 3D vector. Then, we obtain the transformation matrix T 0\u21921 = (R 1 |t 1 ) corresponding to such rototranslation. Thus, we can project our 3D points to the image space through K in order to obtain a new image I 1 . This allows to obtain, for each pixel p 0 in I 0 , the coordinates p 1 of its corresponding pixel in I 1 acquired from viewpoint c 1\na) b) c) d) e)p 1 \u223c KT 0\u21921 D 0 (p 0 )K \u22121 p 0(2)\nand flow F 0\u21921 is obtained as the difference between p 1 and p 0 . We point out that F 0\u21921 only models the virtual camera ego-motion, i.e. no object has moved independently. Finally, we obtain the new image I 1 through forward warping. Forward warping suffers from two well-known problems [61], that are collisions (i.e., multiple pixels from I 0 being warped to the same location in I 1 ) and holes (i.e., pixels in I 1 over which no pixel from I 0 is projected). To handle collisions, we keep track of pixels p 1 having multiple projections p 0 in a binary collision mask M (i.e., collisions are labeled as 1, other pixels as 0) and select, for each, the one having minimum depth according to camera in position c 1 , i.e. the closest, to be displayed in I 1 .\n\nHole filling. Artefacts introduced by holes are more subtle to be solved. Moreover, applying a 6DoF transformation to the camera plane vastly increases the chance of occurrence of holes compared to the case of 1D camera translations applied to distill stereo pairs [61]. In particular, in case of larger camera motion/rotations some stretching artefacts occur on the foreground objects (and, occasionally, in the background as well) as shown in Figure 3 a). To remove these holes, we build a binary hole mask H, as in Figure 3 b), where we label pixels in I 1 for which no pixel in I 0 is reprojecting on to with 0. Then, a simple inpainting strategy [54] is usually sufficient to fill them, as reported in Figure  3 c) on the girl's face. Unfortunately, this is not enough in the case of stretching artefacts occurring in a foreground object overlapping a background one. Indeed, in this case, it is very likely that the holes induced by the stretching of the foreground object are filled by pixels in the background. These pixels are not detected by H, causing the bleeding effect shown in Figure 3 c), where the hair merges with the background umbrella. Since most of these artefacts occur in non-colliding pixels surrounded by colliding ones, i.e. in M they are labeled as 0 and surrounded by pixels labeled as 1, we can detect them by dilating M into M . Then, we define the binary mask P assigning 1 to pixels having the same label in (M , M) and 0 to the remaining (i.e. those that become 1 in M ). We finally obtain H by multiplying H and P\nP = (M == M), H = H \u00b7 P(3)\nWe can apply the inpainting algorithm to pixels labeled with 0 in H , shown in Figure 3  We point out how, in large dis-occluded area (i.e., in the proximity of depth boundaries, as shown in Figure 3 on the left of the person), the inpainting method produces blurred content, as shown in Figure 3 c) and e). Despite these artefacts, our experiments will prove that hole filling improves the accuracy of trained networks significantly. We report in the supplementary material additional qualitative results concerning the design choices discussed so far.\n\nIndependent motions. The pipeline sketched so far models the optical flow field occurring between images acquired in a static environment, i.e. consequence of the camera motion, not taking into account possible independently moving objects, very likely to occur in real contexts [35]. In order to model more realistic simulations, we introduce the possibility of applying different virtual motions to objects extracted from the scene by leveraging an instance segmentation network \u2126 for extracting N objects\n\u03a0 i , i \u2208 [1,N] \u03a0 = {\u03a0 i , i \u2208 [1, N]} = \u2126(I 0 )(4)\nThen, to simulate a motion of the object in the scene, we randomly move the camera from c 0 towards a point c \u03c0i = c 1 and its corresponding transformation T 0\u2192\u03c0i to be applied to object \u03a0 i . Then, we reproject pixels from I 0 on the image planes of the different cameras. Pixel coordinates in I 1 will be selected according to their belonging to segmented objects or the background as\np 1 \u223c KT 0\u21921 D 0 (p 0 )K \u22121 p 0 if p 0 / \u2208 \u03a0 KT 0\u2192\u03c0i D 0 (p 0 )K \u22121 p 0 if p 0 \u2208 \u03a0 i(5)\nWe handle collisions as outlined before, keeping pixels whose depth results lower after motion. Finally, we obtain optical flow F 0\u21921 and image I 1 as aforementioned.\n\nTo be robust to noisy/false detections, e.g. in case of tiny blobs accidentally labeled as objects, we rank the objects according to their size, i.e. number of pixels, and keep in \u03a0 only the n <N largest objects. Figure 4 shows two qualitative comparisons between images and flow distilled by merely applying a virtual camera motion, a) and b), and those obtained by segmenting the cat or the person in the foreground and simulating an independent motion, c) and d). Although our formulation simulates moving objects by moving virtual cameras instead, we can notice how the final effect on I 1 and F 0\u21921 is equivalent for our purposes. We point out that, by increasing the number of moving objects, collisions and holes increase. In particular, a higher number of dis-occlusions might appear after applying independent motions, leading to blurry inpainted content, as shown in Figure 4 c) on the top row, on the right of the cat. Besides, shape boundaries may be inconsistent across depth and segmentation predictions, afflicting the truthfulness of the generated image and introducing artefacts (e.g., background pixels moved as part of the foreground). We will see how, although helpful, this approach yields minor improvements compared to the previous two steps performed in our framework, that result crucial for dephtstilling reliable training data. Moreover, segmenting object instances requires an additional network \u2126 trained in a supervised manner conversely to single-image depth estimation networks, whereby an extensive literature of self/weakly-supervised approaches exists [11,12,58,62].\na) b) c) d)\n\nExperimental results\n\nIn this section, we describe the experimental setup used to validate our depthstillation pipeline. The source code is available at https://github.com/mattpoggi/ depthstillation.\n\n\nTraining datasets\n\nAt first, we describe the datasets used to train the networks considered in our experiments.\n\nChairs (Ch). FlyingChairs [7] is a popular synthetic dataset used to train optical flow models. It contains 22232 images of chairs moving according to 2D displacement vectors over random backgrounds sampled from Flickr.\n\nThings (Th). The FlyingThings3D dataset [20] is a collection of 3D synthetic scenes belonging to the SceneFlow dataset [33] and contains a training split made of 19635 images. Differently from Chairs, objects move in the scene with more complex 3D motions. State-of-the-art networks usually train in sequence over Chairs and Things (Ch\u2192Th).\n\nCOCO dataset. The COCO dataset [27] is a collection of single still images (it provides I 0 only) and ground-truth with labels for tasks such as object detection or panoptic segmentation, but lacks any depth or optical flow annotation. We sample images from the train2017 split, which contains 118288 pictures, to generate virtual images and optical flow maps. We dub dephtstilled COCO (dCOCO) the training set obtained in such a manner.\n\nDAVIS. The DAVIS dataset [42] provides highresolution videos and it is widely used for video object segmentation. Since it does not provide optical flow groundtruth labels, we use all the 10581 images of the unsupervised 2019 challenge to generate dDAVIS and compare with the state-of-the-art in self-supervised optical flow [23].\n\n\nTesting datasets\n\nWe describe here the testing imagery used to evaluate the networks trained on the datasets mentioned above. As metrics, we report the average End-Point Error (EPE) and two error rates, respectively the percentage of pixels with absolute error greater than 3 (> 3) or both absolute and relative errors greater than 3 and 5% respectively (Fl), as defined in [35], on All pixels. In every experiment, we will highlight the best results in bold and underline the secondbest among methods trained in fair conditions. Sintel. Sintel [5] is a synthetic dataset with ground-truth optical flow maps. We use its training split, counting 1041 images for both Clean and Final passes, for evaluation.\n\nKITTI. The KITTI dataset is a popular dataset for autonomous driving with sparse ground-truth values for both depth and optical flow tasks. Two versions exist, KITTI 2012 [10] counting 194 images framing static scenes and KITTI 2015 [35] made of 200 images framing moving objects, in both cases gathered by a car in motion.\n\n\nImplementation Details\n\nWe describe next our pipeline and the networks used for depth estimation and learning optical flow.\n\nDepth estimation models. To obtain dense depth maps from single RGB images, we select two models, respectively MiDaS [45] and MegaDepth [26], the former because represents the state-of-the-art for depth estimation in-thewild and the latter because trained with weaker supervision than MiDaS 1 . Next, we will show how the accuracy of net- 1 The reader might argue that MiDaS has been trained on labels pro-works trained on our data is affected by the depth estimator.\n\nDepthstillation pipeline. To generate virtual images, we convert predicted depths into [1,100]. Given a single image of resolution W\u00d7H, we assume a virtual camera having fixed K, with focals (f x , f y ) = 0.58(W,H) and optical center (c x , c y ) = 0.5(W,H). To generate T 0\u21921 , we build t 1 by sampling three scalars t x , t y , t z in [\u22120.2, 0.2] and R 1 by sampling three Euler angles in [\u2212 \u03c0 18 , \u03c0 18 ]. To simulate moving objects, we run pre-trained Mask-RCNN [14] to select n = 2 instance masks and generate t i and R i sampling respectively in [\u22120.1, 0.1] and [\u2212 \u03c0 36 , \u03c0 36 ] and add them to R 1 and t 1 . Depth maps are sharpened by means of 2 iterations of a 5\u00d75 bilateral filter, while we dilate M with a 3\u00d73 kernel. We can generate multiple camera motions for any given single image and thus a variety of pairs and ground-truth labels. We will see how playing with the number of images and motions impacts optical flow network accuracy.\n\nOptical Flow networks. To evaluate how effective our distilled images are at training optical flow models, we select two main architectures: RAFT [53] and PWC-Net [51]. The first because it represents state-of-the-art architecture for supervised optical flow, already enabling excellent generalization capability. The second because it achieves the best results among self-supervised methods (e.g., UFlow [23]). By deploying both architectures, we aim to prove that our method is general and significantly improves generalization in supervised and self-supervised optical flow. When not otherwise specified, we train RAFT on depthstilled data for 100K steps with a learning rate of 4\u00d710 \u22124 and weight decay of 10 \u22124 , batch size of 6 and 496\u00d7368 image crops. This configuration is the largest one fitting into a single NVIDIA Titan X GPU. Following [53], we adopted AdamW as optimizer [31] and applied the same data augmentations and loss functions, while we set 12 as the number of iterative updates. To train PWCNet, we used as optimizer Adam [24], with an initial learning rate of 1e \u22124 and halved after 400K, 600K and 800K steps. We trained our model for 1M steps with a batch size of 8, adopting the multi-scale loss used in [51] for the synthetic pre-training, with the same augmentations and crop size used for RAFT.\n\n\nAblation Study\n\nIn this section, we assess the impact of the different components of our pipeline.\n\nDepth, hole filling and moving objects. We start by ablating our pipeline to measure the impact of i) estimating depth, ii) applying hole filling to generated images and iii) simulating objects moving independently. This study is carried out by generating virtual views from 20K COCO imduced by pre-trained optical flow networks, introducing biases into images generated with our pipeline. However, we point out that optical flow networks are used only to handle negative disparities in stereo images and would not be necessary if, given the minimum negative disparity d min , the right image is shifted left by |d min |, thus making d min = 0.   [45] to estimate depth during the depthstillation process, we can notice considerable improvements in all metrics and datasets, with Fl score often more than halved. Nonetheless, generated images are affected by large holes and this does not allow for optimal performance. By enabling hole filling (C), the trained RAFT further improves its accuracy on real datasets. Finally, in (D), we show results by simulating objects moving independently, that further improves the results on Sintel. The benefit of this latter strategy is consistent on most metrics, although minor on real datasets such as KITTI 2012 and 2015 compared to the improvements obtained by (B) and (C), proving that the simple camera motion combined with depth is enough to obtain a robust optical flow network capable of generalizing to real environments. Moreover, as already pointed out, (D) also requires a trained instance segmentation network, which is hard to obtain for any possible dataset and would consequently constrain our pipeline. Thus, since our primary focus is on real environments, we choose (C) as the configuration for the following experiments. Depth estimation network. We measure the impact of the depth estimator on our overall data generation pipeline.\n\nTo this aim, we follow the same protocol of the previous experiments, replacing MiDaS with MegaDepth [26] during the depth estimation step. Table 2 shows the results of this experiment. We can notice how images generated through MegaDepth (B) allow for training a RAFT model that places in between the one trained on images generated without depth (A) and using MiDaS (C), being much closer to the latter than to the former. This proves that depth is a crucial cue in our pipeline and the accuracy of the optical flow network, as we might expect, increases with the quality of the estimated depth maps, although with minor gains. Amount of generated images. We can increase the amount of data we generate acting on two orthogonal dimensions: the number of images I 0 and the number of virtual motions we simulate for each. Table 3 collects the results achieved by several RAFT models trained on a different number of images, obtained by varying the parameters mentioned above. By assuming 4K input images, we can notice how applying 5 virtual motions to each (B) allows a consistent boost on Sintel and KITTI 2012 compared to simulating a single motion each (A), while not improving on KITTI 2015. Interestingly, 4K images already allow for strong generalization to real domains, outperforming the results achieved using synthetic datasets shown in detail in the next section. On the other hand, increasing the input images by the same factor \u00d75, yet simulating a single motion (C) leads worse results on Sintel while achieving some improvement on KITTI compared to (A) and (B). This fact highlights that a more variegate image content in the training dataset may be beneficial only for generalization to real environments. By depthstilling 5 motions, for a total of 100K training samples (D), yields further improvements on Sintel, again with minor impact on KITTI. To carry out a fair comparison with synthetic datasets, counting about 20K images each, we will use 20K images and a single virtual motion to depthstill our training data from now on.\n\n\nComparison with synthetic datasets\n\nIn this section, we evaluate the effectiveness of our depthstilled data versus synthetic datasets [7,33].\n\nGeneralization to real environments. We start by evaluating the robustness of a network trained on our data when deployed on real datasets. Table 4 shows the performance achieved by RAFT when trained on Chairs (A) and finetuned on Things (B) with crop size and settings described in [53] to fit in a single GPU, compared to a variant trained on dCOCO, a split of 20K image pairs depthstilled from COCO (C). For completeness, we also report the performance of RAFT models provided by the authors (A \u2020) and (B \u2020), trained on 2\u00d7 GPUs and thus not directly comparable with our setting. We can notice how training on dCOCO (C) allows for much higher generalization on real datasets such    Table 4. Comparison with synthetic datasets -generalization. Generalization achieved by RAFT when trained on synthetic data (A),(B), on our dCOCO dataset (C) and a combination of both (D). \u2020 are obtained with publicly available weights by [53] (2\u00d7 GPUs).\n\nas KITTI 2012 and 2015, at the cost of worse performance on the Sintel synthetic dataset. This latter result is not surprising because the images in Things are generated through computer graphics as those in Sintel, while generating virtual images from a real dataset (COCO) leads to superior generalization on real datasets (KITTI 2012 and 2015), also outperforming (A \u2020) and (B \u2020) despite the single GPU.\n\nWe also train RAFT sequentially on Chairs, Things and dCOCO (D). This setting improves the EPE achieved by (C) on KITTI 2012 and 2015 and turns out much more effective on Sintel with both metrics. This fact suggests that a combination of synthetic images with perfect ground-truth and virtual images with depthstilled labels might be beneficial for generalization purposes. Figure 5 shows some qualitative optical flow predictions and corresponding error maps obtained from the RAFT variants considered in Table 4. We report additional examples in the supplementary material.\n\nFine-tuning on real data. We evaluate the effect of pretraining on synthetic images or our generated frames when fine-tuning on a few real data with accurate ground-truth. To this aim, we fine-tune RAFT variants on the first 160 images of the KITTI 2015 training set and evaluate on the remaining 40 and KITTI 2012. We train with a learning rate of 10 \u22124 and weight decay of 10 \u22125 , batch size of 3 and 960\u00d7288 image crops, converging after 20K iterations.   Table 6. Impact of depthstillation on different architectures. Evaluation on PWCNet and RAFT. Entries with \"-\" are not provided in the original paper. notice how variants (A) and (B) trained on synthetic data are greatly improved by the fine-tuning, while (C) achieves slightly lower accuracy after fine-tuning. Despite allowing for much higher generalization to real images, the supervision allowed by our method is weaker than the one obtained through real image pairs and perfect ground-truth. Thus, it is not surprising that networks trained from scratch to the end on perfect ground-truth might yield better accuracy. Nonetheless, combining synthetic data with our depthstilled images (D) allows for the best performance, confirming the findings from our previous experiments that a combination of the two worlds -synthetic data with perfect labels and realistic yet imperfect images and labels -is beneficial.\n\nImpact on different optical flow networks. To prove that the superior generalization we achieve is enabled by our data rather than a specific architecture such as RAFT, we also train PWCNet [51] on the 20K images generated from COCO. Table 6 shows how PWCNet trained on dCOCO (C) dramatically outperforms the original variants trained on Chairs (A) and fine-tuned on Things (B) when testing on real data, at the cost of lower performance on Sintel synthetic images, substantially confirming our findings from previous experiments with RAFT, reported in the table for comparison (D). This fact proves that our data, generated from single yet realistic still images, significantly improves generalization to real data independently from the optical flow model trained.\n\n\nComparison with self-supervision from videos\n\nGiven the rich literature about self-supervised optical flow [34,29,30,23], we compare our strategy with stateof-the-art practises for self-supervised optical flow [23].\n\nGeneralization. In contrast to most works in this field that train and test in the same domain [34,29,30,23], we inquire about how well networks trained in a self-supervised manner or leveraging our proposal transfer across different real datasets. To this aim, we adopt DAVIS [42] for training and evaluate on KITTI 2012 and 2015 as in the previous experiments. To train UFlow [23], we use the official code provided by the authors. In particular, we trained the model on the entire DAVIS dataset for 1M steps, using a batch size of 1 as suggested in [23], 512\u00d7384 resized images and letting unchanged other configuration parameters in order to replicate the authors' settings. Being UFlow based on PWCNet, we train from scratch another instance of PWC-Net on dDAVIS for the same number of steps with a batch of 8 over depthstilled images and labels. The learning rate scheduling is the same highlighted in section 4.3, while the crop is 512\u00d7384. This way, we evaluate how well a PWC-Net trained on depthstilled data transfers to other datasets compared to a model trained on real videos framing the same image content of the depthstilled images. Table 7 collects the outcome of this experiment. We can notice how the PWCNet model trained on dDAVIS (B) transfers much better to the KITTI 2012 and 2015 datasets compared to UFlow trained on the real DAVIS (A), thanks to the stronger supervision from the distilled optical flow labels. For the sake of completeness, we also report the results achieved by RAFT (C) trained on the same data, confirming to be superior.\n\nLimitations. Our pipeline has some obvious limitations. Indeed, the training samples we generate are far from being utterly realistic because cannot model some behaviors, such as the large 3D rotation of objects in the scene, frequently found in real videos. Thus, despite the strong generalization we achieve compared to self-supervision, real videos allow for much better specialization when training and testing in the same domain. As shown in Table 8 Table 8. Comparison between self-supervision and depthstillation -specialization. Effectiveness of the two strategies when training and testing on similar data (KITTI 2015). Entries with \"-\" are not provided in the original paper. forms much better than PWCNet trained on 960\u00d7288 crops from dKITTI (B), a set of about 4K images depthstilled from KITTI 2015 multiview testing set. On the other hand, RAFT trained on dKITTI with the same crop size (C) gets closer to UFlow, thanks to the more effective architecture.\n\nThis lower specialization is also due to the completely random motions we depthstill. In contrast, KITTI motions consist of a much smaller subset (i.e. mostly forward translations or steerings) dominant in the real KITTI multiview split, yet rarely occurring in dKITTI.\n\nAs take-home message, our depthstillation strategy effectively addresses the scarcity of training data, e.g. when annotated images or not-annotated videos of the target environment are not available, yielding superior generalization compared to existing practices. Moreover, it is complementary to domain-specific real training data with labels, seldom ever available in practice.\n\n\nConclusion\n\nWe proposed a new strategy named, Depthstillation, to distill dense optical flow ground-truth maps from single still images and create novel virtual views, by leveraging the depth provided by a pre-trained monocular network. Through extensive experiments, we showed how it allows for training state-of-the-art optical flow networks [51,53], leading to models that better generalize to real data compared to the use of synthetic images or self-supervision from videos framing different content, while suffering at specialization. Depthstillation is a powerful solution when domain-specific training data is not available, as occurs in most practical applications in-the-wild.\n\nFigure 3 .\n3Hole filling strategies. From left to right: a) forwardwarped image affected by stretching artefacts, b) holes mask H c) inpainted image, d) collision-augmented holes mask H and e) improved inpainted image. Black pixels in H and H are those to be inpainted.\n\n\nd), to obtain Figure 3 e), where the foreground-background bleeding does not occur. We report more qualitative examples regarding the different masks in the supplementary material.\n\nFigure 4 .\n4Independent motions modeling. From left to right: a) image generated by only modeling camera motion and b) corresponding optical flow field, c) image generated after segmenting the foreground, which is now subject to a different motion yielding d) a more complex optical flow field.\n\nFigure 5 .\n5Qualitative results on the KITTI 2015 training set. On two rows: a) reference frame (top) and ground-truth flow (bottom), optical flow maps (top) by RAFT trained on b) Ch, c) Ch\u2192Th, d) dCOCO and e) Ch\u2192Th\u2192dCOCO and error maps (bottom).\n\nTable 3 .\n3Impact of images and virtual motions. We train several RAFT models by changing the number of input images taken from COCO and the number of motions depthstilled for each one.ages, applying a single virtual camera motion for each, by \ntraining RAFT [53] on them and evaluating the final model \non Sintel, KITTI 2012 and KITTI 2015. Table 1 collects the \noutcome of this evaluation. On row (A), we show the per-\nformance achieved by generating images without estimat-\ning their depth, thus assuming a constant depth value for all \npixels in any image. By moving to row (B), for which we \nuse MiDaS \n\nTable 5\n5collects the outcome of this experiment. We canPre-training \nFine-tuning \nKITTI12 \nKITTI15 \nEPE \nFl \nEPE \nFl \n(A) \nCh \n\n5.14 34.64 15.56 47.29 \nCh \n\n1.42 4.86 2.40 8.49 \n(B) \nCh\u2192Th \n\n2.40 10.49 9.04 25.53 \nCh\u2192Th \n\n1.36 4.67 2.22 \n8.09 \n(C) \ndCOCO \n\n1.82 6.62 5.09 16.72 \ndCOCO \n\n1.37 4.70 2.76 \n9.15 \n(D) Ch\u2192Th\u2192dCOCO \n\n1.78 7.00 4.82 18.03 \nCh\u2192Th\u2192dCOCO \n\n1.32 4.54 2.21 \n7.93 \n\n\n\nTable 5 .\n5Comparison with synthetic datasets -fine-tuning. PWCNet dCOCO 4.14 11.54 5.57 15.58 3.16 13.30 8.49 26.06 (D) RAFT dCOCO 2.63 7.00 3.90 11.31 1.82 6.62 3.81 12.42Per-\n\n\nTable 7 .\n7, UFlow trained on the 4K images of the KITTI multiview dataset (A) per-UFlow DAVIS 3.49 14.54 9.52 25.52 (B) PWCNet dDAVIS 2.81 11.29 6.88 21.87 (C) RAFT dDAVIS 1.78 6.85 3.80 13.22 Comparison between self-supervision and depthstillation -generalization. Effectiveness of the two strategies when evaluated on unseen data (KITTI 2012 and 2015). PWCNet dKITTI 2.64 9.43 7.92 22.17 (C) RAFT dKITTI 1.76 5.91 4.01 13.35Model Dataset \nKITTI12 KITTI15 \nEPE Fl EPE Fl \n(A) Model Dataset KITTI12 KITTI15 \nEPE Fl EPE Fl \n(A) UFlow KITTI \n-\n-3.08 10.00 \n(B) \nAcknowledgement. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.\nExploiting semantic information and deep matching for optical flow. Min Bai, Wenjie Luo, Kaustav Kundu, Raquel Urtasun, European Conference on Computer Vision. Springer1Min Bai, Wenjie Luo, Kaustav Kundu, and Raquel Urtasun. Exploiting semantic information and deep matching for opti- cal flow. In European Conference on Computer Vision, pages 154-170. Springer, 2016. 1, 2\n\nDepth-aware video frame interpolation. Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, Ming-Hsuan Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depth-aware video frame interpolation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3703- 3712, 2019. 2\n\nA framework for the robust estimation of optical flow. J Michael, Padmanabhan Black, Anandan, 1993 (4th) International Conference on Computer Vision. Michael J Black and Padmanabhan Anandan. A framework for the robust estimation of optical flow. In 1993 (4th) In- ternational Conference on Computer Vision, pages 231-236. IEEE, 1993. 2\n\nLarge displacement optical flow. Thomas Brox, Christoph Bregler, Jitendra Malik, 2009 IEEE Conference on Computer Vision and Pattern Recognition. Thomas Brox, Christoph Bregler, and Jitendra Malik. Large displacement optical flow. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 41-48. IEEE, 2009. 2\n\nA naturalistic open source movie for optical flow evaluation. D J Butler, J Wulff, G B Stanley, M J Black, editor, European Conf. on Computer Vision (ECCV), Part IV. A. Fitzgibbon et al.Springer-Verlag75775D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A nat- uralistic open source movie for optical flow evaluation. In A. Fitzgibbon et al. (Eds.), editor, European Conf. on Com- puter Vision (ECCV), Part IV, LNCS 7577, pages 611-625. Springer-Verlag, Oct. 2012. 1, 2, 5\n\nFull flow: Optical flow estimation by global optimization over regular grids. Qifeng Chen, Vladlen Koltun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionQifeng Chen and Vladlen Koltun. Full flow: Optical flow estimation by global optimization over regular grids. In Pro- ceedings of the IEEE conference on computer vision and pat- tern recognition, pages 4706-4714, 2016. 2\n\nFlownet: Learning optical flow with convolutional networks. Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der, Daniel Smagt, Thomas Cremers, Brox, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision6Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Pro- ceedings of the IEEE international conference on computer vision, pages 2758-2766, 2015. 1, 2, 4, 6\n\nDeepview: View synthesis with learned gradient descent. John Flynn, Michael Broxton, Paul Debevec, Matthew Du-Vall, Graham Fyffe, Ryan Overbeck, Noah Snavely, Richard Tucker, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJohn Flynn, Michael Broxton, Paul Debevec, Matthew Du- Vall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and Richard Tucker. Deepview: View synthesis with learned gradient descent. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2367- 2376, 2019. 2\n\nDeep ordinal regression network for monocular depth estimation. Huan Fu, Mingming Gong, Chaohui Wang, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Kayhan Batmanghelich, and Dacheng TaoHuan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat- manghelich, and Dacheng Tao. Deep ordinal regression net- work for monocular depth estimation. In The IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), 2018. 2\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. Andreas Geiger, Philip Lenz, Raquel Urtasun, Conference on Computer Vision and Pattern Recognition (CVPR). Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recog- nition (CVPR), 2012. 1, 2, 5\n\nUnsupervised monocular depth estimation with leftright consistency. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J Brostow, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition24Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Bros- tow. Unsupervised monocular depth estimation with left- right consistency. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 270-279, 2017. 2, 4\n\nDigging into self-supervised monocular depth prediction. Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, Gabriel J Brostow, The International Conference on Computer Vision (ICCV). 24Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J. Brostow. Digging into self-supervised monocular depth prediction. In The International Conference on Com- puter Vision (ICCV), October 2019. 2, 4\n\nAllan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. Vitor Guizilini, Rares Ambrus, Sudeep Pillai, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2020Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raven- tos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. In IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), 2020. 2\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017. 5\n\nCycada: Cycle-consistent adversarial domain adaptation. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, Trevor Darrell, PMLRInternational conference on machine learning. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International conference on machine learning, pages 1989- 1998. PMLR, 2018. 1\n\nDetermining optical flow. K P Berthold, Brian G Horn, Schunck, Techniques and Applications of Image Understanding. 281Berthold KP Horn and Brian G Schunck. Determining op- tical flow. In Techniques and Applications of Image Under- standing, volume 281, pages 319-331. International Society for Optics and Photonics, 1981. 2\n\nLiteFlowNet3: Resolving Correspondence Ambiguity for More Accurate Optical Flow Estimation. Tak-Wai Hui, Chen Change Loy, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Tak-Wai Hui and Chen Change Loy. LiteFlowNet3: Resolv- ing Correspondence Ambiguity for More Accurate Optical Flow Estimation. In Proceedings of the European Confer- ence on Computer Vision (ECCV), 2020. 1, 2\n\nLite-FlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation. Tak-Wai Hui, Xiaoou Tang, Chen Change Loy, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)1Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Lite- FlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation. In Proceedings of IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 8981-8989, 2018. 1, 2\n\nA lightweight optical flow cnn -revisiting data fidelity and regularization. Tak-Wai Hui, Xiaoou Tang, Chen Change Loy, IEEE Transactions on Pattern Analysis and Machine Intelligence. 1Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. A lightweight optical flow cnn -revisiting data fidelity and reg- ularization. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 1, 2\n\nFlownet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 25E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR), Jul 2017. 1, 2, 5\n\nBack to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness. Adam W Yu Jason, Konstantinos G Harley, Derpanis, ECCV Workshops. J Yu Jason, Adam W Harley, and Konstantinos G Derpa- nis. Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness. In ECCV Workshops (3), 2016. 2\n\nSuper slomo: High quality estimation of multiple intermediate frames for video interpolation. Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, Jan Kautz, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHuaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, and Jan Kautz. Super slomo: High quality estimation of multiple intermediate frames for video interpolation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9000- 9008, 2018. 2\n\nWhat matters in unsupervised optical flow. Rico Jonschkowski, Austin Stone, Jon Barron, Ariel Gordon, Kurt Konolige, Anelia Angelova, ECCV. 8Rico Jonschkowski, Austin Stone, Jon Barron, Ariel Gordon, Kurt Konolige, and Anelia Angelova. What matters in unsu- pervised optical flow. ECCV, 2020. 2, 5, 8\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5\n\nVasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. Iro Laina, Christian Rupprecht, 3Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed- erico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In 3DV, 2016. 2\n\nMegadepth: Learning singleview depth prediction from internet photos. Zhengqi Li, Noah Snavely, Computer Vision and Pattern Recognition (CVPR). 56Zhengqi Li and Noah Snavely. Megadepth: Learning single- view depth prediction from internet photos. In Computer Vision and Pattern Recognition (CVPR), 2018. 2, 5, 6\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755.\n\n. Springer, Springer, 2014. 5\n\nLearning by analogy: Reliable supervision from transformations for unsupervised optical flow estimation. Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao Wang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, Feiyue Huang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLiang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao Wang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, and Feiyue Huang. Learning by analogy: Reliable super- vision from transformations for unsupervised optical flow estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6489- 6498, 2020. 2\n\nDdflow: Learning optical flow with unlabeled data distillation. Pengpeng Liu, Irwin King, Jia Michael R Lyu, Xu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Pengpeng Liu, Irwin King, Michael R Lyu, and Jia Xu. Ddflow: Learning optical flow with unlabeled data distilla- tion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8770-8777, 2019. 2, 8\n\nSelflow: Self-supervised learning of optical flow. Pengpeng Liu, Michael Lyu, Irwin King, Jia Xu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2Pengpeng Liu, Michael Lyu, Irwin King, and Jia Xu. Self- low: Self-supervised learning of optical flow. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4571-4580, 2019. 2, 8\n\n. Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. arXiv preprintIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5\n\nConstant time weighted median filtering for stereo matching and beyond. Ziyang Ma, Kaiming He, Yichen Wei, Jian Sun, Enhua Wu, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZiyang Ma, Kaiming He, Yichen Wei, Jian Sun, and En- hua Wu. Constant time weighted median filtering for stereo matching and beyond. In Proceedings of the IEEE Interna- tional Conference on Computer Vision, pages 49-56, 2013. 3\n\nA large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, Thomas Brox, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition6Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 4040-4048, 2016. 2, 5, 6\n\nUnFlow: Unsupervised learning of optical flow with a bidirectional census loss. Simon Meister, Junhwa Hur, Stefan Roth, AAAI. 2Simon Meister, Junhwa Hur, and Stefan Roth. UnFlow: Un- supervised learning of optical flow with a bidirectional cen- sus loss. In AAAI, 2018. 2, 8\n\nObject scene flow for autonomous vehicles. Moritz Menze, Andreas Geiger, Conference on Computer Vision and Pattern Recognition (CVPR). Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 1, 2, 4, 5\n\nDiscrete optimization for optical flow. Moritz Menze, Christian Heipke, Andreas Geiger, German Conference on Pattern Recognition. SpringerMoritz Menze, Christian Heipke, and Andreas Geiger. Dis- crete optimization for optical flow. In German Conference on Pattern Recognition, pages 16-28. Springer, 2015. 2\n\nObject scene flow. Moritz Menze, Christian Heipke, Andreas Geiger, ISPRS Journal of Photogrammetry and Remote Sensing (JPRS). 12Moritz Menze, Christian Heipke, and Andreas Geiger. Ob- ject scene flow. ISPRS Journal of Photogrammetry and Re- mote Sensing (JPRS), 2018. 1, 2\n\nNerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, ECCV. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view syn- thesis. In ECCV, 2020. 2\n\nImage to image translation for domain adaptation. Zak Murez, Soheil Kolouri, David Kriegman, Ravi Ramamoorthi, Kyungnam Kim, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZak Murez, Soheil Kolouri, David Kriegman, Ravi Ra- mamoorthi, and Kyungnam Kim. Image to image translation for domain adaptation. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 4500-4509, 2018. 1\n\nSoftmax splatting for video frame interpolation. Simon Niklaus, Feng Liu, IEEE International Conference on Computer Vision. Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In IEEE International Conference on Computer Vision, 2020. 2\n\n3d ken burns effect from a single image. Simon Niklaus, Long Mai, Jimei Yang, Feng Liu, ACM Transactions on Graphics (TOG). 386Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d ken burns effect from a single image. ACM Transactions on Graphics (TOG), 38(6):1-15, 2019. 2\n\nA benchmark dataset and evaluation methodology for video object segmentation. F Perazzi, J Pont-Tuset, B Mcwilliams, L Van Gool, M Gross, A Sorkine-Hornung, Computer Vision and Pattern Recognition. 5F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In Computer Vision and Pattern Recognition, 2016. 5, 8\n\nLearning across tasks and domains. Alessio Pierluigi Zama Ramirez, Samuele Tonioni, Luigi Di Salti, Stefano, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionPierluigi Zama Ramirez, Alessio Tonioni, Samuele Salti, and Luigi Di Stefano. Learning across tasks and domains. In Pro- ceedings of the IEEE International Conference on Computer Vision, pages 8110-8119, 2019. 1\n\nNon-local total generalized variation for optical flow estimation. Ren\u00e9 Ranftl, Kristian Bredies, Thomas Pock, European Conference on Computer Vision. SpringerRen\u00e9 Ranftl, Kristian Bredies, and Thomas Pock. Non-local total generalized variation for optical flow estimation. In European Conference on Computer Vision, pages 439-454. Springer, 2014. 2\n\nTowards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 56Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence (TPAMI), 2020. 2, 5, 6\n\nUnsupervised deep learning for optical flow estimation. Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang, Hongyuan Zha, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence31Zhe Ren, Junchi Yan, Bingbing Ni, Bin Liu, Xiaokang Yang, and Hongyuan Zha. Unsupervised deep learning for optical flow estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017. 2\n\nFree view synthesis. Gernot Riegler, Vladlen Koltun, European Conference on Computer Vision. Gernot Riegler and Vladlen Koltun. Free view synthesis. In European Conference on Computer Vision, 2020. 2\n\nStructure-from-motion revisited. Johannes Lutz Sch\u00f6nberger, Jan-Michael Frahm, Conference on Computer Vision and Pattern Recognition (CVPR). Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Com- puter Vision and Pattern Recognition (CVPR), 2016. 2\n\n3d photography using context-aware layered depth inpainting. Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 23Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 3d photography using context-aware layered depth inpainting. In IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), 2020. 2, 3\n\nSecrets of optical flow estimation and their principles. Deqing Sun, Stefan Roth, Michael J Black, IEEE computer society conference on computer vision and pattern recognition. IEEE1Deqing Sun, Stefan Roth, and Michael J Black. Secrets of optical flow estimation and their principles. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 2432-2439. IEEE, 2010. 1, 2\n\nPwc-net: Cnns for optical flow using pyramid, warping, and cost volume. Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionDeqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8934-8943, 2018. 1, 2, 5, 8\n\nModels matter, so does training: An empirical study of cnns for optical flow estimation. Deqing Sun, Xiaodong Yang, Ming-Yu Liu, Jan Kautz, IEEE transactions on pattern analysis and machine intelligence. 42Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Models matter, so does training: An empirical study of cnns for optical flow estimation. IEEE transactions on pattern analysis and machine intelligence, 42(6):1408-1423, 2019. 1, 2\n\nRaft: Recurrent all-pairs field transforms for optical flow. Zachary Teed, Jia Deng, European Conference on Computer Vision (ECCV). 7Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European Conference on Computer Vision (ECCV), 2020. 1, 2, 5, 6, 7, 8\n\nAn image inpainting technique based on the fast marching method. Alexandru Telea, Journal of graphics tools. 91Alexandru Telea. An image inpainting technique based on the fast marching method. Journal of graphics tools, 9(1):23-34, 2004. 3\n\nUnsupervised domain adaptation in semantic segmentation: a review. Marco Toldo, Andrea Maracani, Umberto Michieli, Pietro Zanuttigh, arXiv:2005.10876arXiv preprintMarco Toldo, Andrea Maracani, Umberto Michieli, and Pietro Zanuttigh. Unsupervised domain adaptation in semantic segmentation: a review. arXiv preprint arXiv:2005.10876, 2020. 1\n\nUnsupervised adaptation for deep stereo. Alessio Tonioni, Matteo Poggi, Stefano Mattoccia, Luigi Di Stefano, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionAlessio Tonioni, Matteo Poggi, Stefano Mattoccia, and Luigi Di Stefano. Unsupervised adaptation for deep stereo. In Pro- ceedings of the IEEE International Conference on Computer Vision, pages 1605-1613, 2017. 1\n\nReal-time self-adaptive deep stereo. Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mattoccia, Luigi Di Stefano, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mat- toccia, and Luigi Di Stefano. Real-time self-adaptive deep stereo. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR), June 2019. 1\n\nLearning monocular depth estimation infusing traditional stereo knowledge. Fabio Tosi, Filippo Aleotti, Matteo Poggi, Stefano Mattoccia, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition24Fabio Tosi, Filippo Aleotti, Matteo Poggi, and Stefano Mat- toccia. Learning monocular depth estimation infusing tradi- tional stereo knowledge. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 9799-9809, 2019. 2, 4\n\nDistilled semantics for comprehensive scene understanding from videos. Fabio Tosi, Filippo Aleotti, Matteo Pierluigi Zama Ramirez, Samuele Poggi, Luigi Di Salti, Stefano Stefano, Mattoccia, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionFabio Tosi, Filippo Aleotti, Pierluigi Zama Ramirez, Matteo Poggi, Samuele Salti, Luigi Di Stefano, and Stefano Mattoc- cia. Distilled semantics for comprehensive scene understand- ing from videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 2\n\nSingle-view view synthesis with multiplane images. Richard Tucker, Noah Snavely, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Richard Tucker and Noah Snavely. Single-view view synthe- sis with multiplane images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. 2\n\nLearning stereo from single images. Jamie Watson, Oisin Mac Aodha, Daniyar Turmukhambetov, Gabriel J Brostow, Michael Firman, European Conference on Computer Vision (ECCV). 13Jamie Watson, Oisin Mac Aodha, Daniyar Turmukhambetov, Gabriel J. Brostow, and Michael Firman. Learning stereo from single images. In European Conference on Computer Vision (ECCV), 2020. 1, 3\n\nSelf-supervised monocular depth hints. Jamie Watson, Michael Firman, J Gabriel, Daniyar Brostow, Turmukhambetov, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision24Jamie Watson, Michael Firman, Gabriel J Brostow, and Daniyar Turmukhambetov. Self-supervised monocular depth hints. In Proceedings of the IEEE International Conference on Computer Vision, pages 2162-2171, 2019. 2, 4\n\nDeepflow: Large displacement optical flow with deep matching. Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, Cordelia Schmid, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionPhilippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepflow: Large displacement optical flow with deep matching. In Proceedings of the IEEE inter- national conference on computer vision, pages 1385-1392, 2013. 2\n\nStructured attention guided convolutional neural fields for monocular depth estimation. Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, Elisa Ricci, CVPR. Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and Elisa Ricci. Structured attention guided convolutional neural fields for monocular depth estimation. In CVPR, 2018. 2\n\nAccurate optical flow via direct cost volume processing. Jia Xu, Ren\u00e9 Ranftl, Vladlen Koltun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition1Jia Xu, Ren\u00e9 Ranftl, and Vladlen Koltun. Accurate optical flow via direct cost volume processing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 1289-1297, 2017. 1, 2\n\nNovel view synthesis of dynamic scenes with globally coherent depths from a monocular camera. Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, Jan Kautz, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5336-5345, 2020. 2\n\nA duality based approach for realtime tv-l 1 optical flow. Christopher Zach, Thomas Pock, Horst Bischof, Joint pattern recognition symposium. SpringerChristopher Zach, Thomas Pock, and Horst Bischof. A du- ality based approach for realtime tv-l 1 optical flow. In Joint pattern recognition symposium, pages 214-223. Springer, 2007. 2\n\nDomain-invariant stereo matching networks. Feihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu, Benjamin Wah, Philip Torr, arXiv:1911.13287arXiv preprintFeihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu, Benjamin Wah, and Philip Torr. Domain-invariant stereo matching networks. arXiv preprint arXiv:1911.13287, 2019. 1\n\nUnsupervised learning of depth and ego-motion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1851-1858, 2017. 2\n\nStereo magnification: Learning view synthesis using multiplane images. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Noah Snavely, SIGGRAPH. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view syn- thesis using multiplane images. In SIGGRAPH, 2018. 2\n", "annotations": {"author": "[{\"end\":168,\"start\":43},{\"end\":282,\"start\":169},{\"end\":411,\"start\":283}]", "publisher": null, "author_last_name": "[{\"end\":58,\"start\":51},{\"end\":181,\"start\":176},{\"end\":300,\"start\":291}]", "author_first_name": "[{\"end\":50,\"start\":43},{\"end\":175,\"start\":169},{\"end\":290,\"start\":283}]", "author_affiliation": "[{\"end\":167,\"start\":86},{\"end\":281,\"start\":200},{\"end\":410,\"start\":329}]", "title": "[{\"end\":40,\"start\":1},{\"end\":451,\"start\":412}]", "venue": null, "abstract": "[{\"end\":1494,\"start\":453}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b50\"},\"end\":1606,\"start\":1602},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2021,\"start\":2018},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":2024,\"start\":2021},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2105,\"start\":2102},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2108,\"start\":2105},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2111,\"start\":2108},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2114,\"start\":2111},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2117,\"start\":2114},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2120,\"start\":2117},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2123,\"start\":2120},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2126,\"start\":2123},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2653,\"start\":2650},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2655,\"start\":2653},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2658,\"start\":2655},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2955,\"start\":2951},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2958,\"start\":2955},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3080,\"start\":3076},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3083,\"start\":3080},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3086,\"start\":3083},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3193,\"start\":3189},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3196,\"start\":3193},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3199,\"start\":3196},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3202,\"start\":3199},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3234,\"start\":3230},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":3237,\"start\":3234},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":3240,\"start\":3237},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3243,\"start\":3240},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3644,\"start\":3640},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3838,\"start\":3834},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3976,\"start\":3972},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3979,\"start\":3976},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4039,\"start\":4035},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4042,\"start\":4039},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4045,\"start\":4042},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4048,\"start\":4045},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4590,\"start\":4586},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4593,\"start\":4590},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4596,\"start\":4593},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5832,\"start\":5828},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6104,\"start\":6100},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6106,\"start\":6104},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":6109,\"start\":6106},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6217,\"start\":6214},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":6220,\"start\":6217},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6239,\"start\":6235},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6321,\"start\":6317},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6512,\"start\":6508},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6514,\"start\":6512},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":6517,\"start\":6514},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6559,\"start\":6556},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":6721,\"start\":6717},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6723,\"start\":6721},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":6726,\"start\":6723},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6765,\"start\":6762},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6990,\"start\":6987},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6993,\"start\":6990},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7031,\"start\":7027},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7034,\"start\":7031},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7037,\"start\":7034},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7040,\"start\":7037},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7043,\"start\":7040},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7046,\"start\":7043},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7049,\"start\":7046},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7100,\"start\":7097},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7103,\"start\":7100},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7117,\"start\":7113},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7120,\"start\":7117},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7147,\"start\":7143},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7296,\"start\":7293},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7299,\"start\":7296},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7319,\"start\":7315},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7322,\"start\":7319},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7600,\"start\":7596},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7603,\"start\":7600},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7606,\"start\":7603},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7672,\"start\":7668},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7699,\"start\":7695},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7743,\"start\":7739},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7769,\"start\":7765},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7955,\"start\":7951},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":8333,\"start\":8329},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8336,\"start\":8333},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8338,\"start\":8336},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8491,\"start\":8487},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8494,\"start\":8491},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":8497,\"start\":8494},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":8522,\"start\":8518},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8525,\"start\":8522},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8528,\"start\":8525},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8531,\"start\":8528},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8577,\"start\":8573},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8580,\"start\":8577},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8697,\"start\":8693},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":9244,\"start\":9240},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9247,\"start\":9244},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9249,\"start\":9247},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9252,\"start\":9249},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9255,\"start\":9252},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9347,\"start\":9343},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9349,\"start\":9347},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9352,\"start\":9349},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9371,\"start\":9367},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":9374,\"start\":9371},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9377,\"start\":9374},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9597,\"start\":9593},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10077,\"start\":10073},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10320,\"start\":10316},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":11256,\"start\":11252},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11259,\"start\":11256},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11360,\"start\":11356},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":12350,\"start\":12346},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":13090,\"start\":13086},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13476,\"start\":13472},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15235,\"start\":15231},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17746,\"start\":17742},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17749,\"start\":17746},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17752,\"start\":17749},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":17755,\"start\":17752},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18114,\"start\":18111},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18350,\"start\":18346},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18429,\"start\":18425},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18683,\"start\":18679},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19116,\"start\":19112},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19416,\"start\":19412},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19798,\"start\":19794},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19968,\"start\":19965},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20302,\"start\":20298},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20364,\"start\":20360},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20699,\"start\":20695},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20718,\"start\":20714},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20918,\"start\":20917},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21137,\"start\":21134},{\"end\":21141,\"start\":21137},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21518,\"start\":21514},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22149,\"start\":22145},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":22166,\"start\":22162},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22408,\"start\":22404},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22852,\"start\":22848},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22888,\"start\":22884},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23048,\"start\":23044},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23233,\"start\":23229},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24076,\"start\":24072},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25425,\"start\":25421},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27510,\"start\":27507},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27513,\"start\":27510},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":27803,\"start\":27799},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":28444,\"start\":28440},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31012,\"start\":31008},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31698,\"start\":31694},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31701,\"start\":31698},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31704,\"start\":31701},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31707,\"start\":31704},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31801,\"start\":31797},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31903,\"start\":31899},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31906,\"start\":31903},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31909,\"start\":31906},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31912,\"start\":31909},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":32085,\"start\":32081},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32186,\"start\":32182},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32360,\"start\":32356},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":35345,\"start\":35341},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":35348,\"start\":35345}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35954,\"start\":35684},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36137,\"start\":35955},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36433,\"start\":36138},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36681,\"start\":36434},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37290,\"start\":36682},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37679,\"start\":37291},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":37859,\"start\":37680},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38421,\"start\":37860}]", "paragraph": "[{\"end\":1899,\"start\":1510},{\"end\":5147,\"start\":1901},{\"end\":5259,\"start\":5149},{\"end\":5833,\"start\":5261},{\"end\":5944,\"start\":5850},{\"end\":6727,\"start\":5946},{\"end\":7178,\"start\":6729},{\"end\":7455,\"start\":7180},{\"end\":7853,\"start\":7457},{\"end\":8259,\"start\":7855},{\"end\":8698,\"start\":8261},{\"end\":9048,\"start\":8700},{\"end\":9378,\"start\":9050},{\"end\":9650,\"start\":9380},{\"end\":10214,\"start\":9652},{\"end\":10638,\"start\":10216},{\"end\":10892,\"start\":10667},{\"end\":11015,\"start\":10894},{\"end\":12007,\"start\":11033},{\"end\":12819,\"start\":12057},{\"end\":14369,\"start\":12821},{\"end\":14950,\"start\":14397},{\"end\":15459,\"start\":14952},{\"end\":15898,\"start\":15512},{\"end\":16153,\"start\":15987},{\"end\":17756,\"start\":16155},{\"end\":17969,\"start\":17792},{\"end\":18083,\"start\":17991},{\"end\":18304,\"start\":18085},{\"end\":18646,\"start\":18306},{\"end\":19085,\"start\":18648},{\"end\":19417,\"start\":19087},{\"end\":20125,\"start\":19438},{\"end\":20450,\"start\":20127},{\"end\":20576,\"start\":20477},{\"end\":21045,\"start\":20578},{\"end\":21997,\"start\":21047},{\"end\":23322,\"start\":21999},{\"end\":23423,\"start\":23341},{\"end\":25318,\"start\":23425},{\"end\":27370,\"start\":25320},{\"end\":27514,\"start\":27409},{\"end\":28455,\"start\":27516},{\"end\":28863,\"start\":28457},{\"end\":29440,\"start\":28865},{\"end\":30816,\"start\":29442},{\"end\":31584,\"start\":30818},{\"end\":31802,\"start\":31633},{\"end\":33370,\"start\":31804},{\"end\":34341,\"start\":33372},{\"end\":34612,\"start\":34343},{\"end\":34994,\"start\":34614},{\"end\":35683,\"start\":35009}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11032,\"start\":11016},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12022,\"start\":12008},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12056,\"start\":12022},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14396,\"start\":14370},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15511,\"start\":15460},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15986,\"start\":15899},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17768,\"start\":17757}]", "table_ref": "[{\"end\":25467,\"start\":25460},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26150,\"start\":26143},{\"end\":27663,\"start\":27656},{\"end\":28208,\"start\":28201},{\"end\":29378,\"start\":29371},{\"end\":29908,\"start\":29901},{\"end\":31059,\"start\":31052},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":32959,\"start\":32952},{\"end\":33826,\"start\":33819},{\"end\":33834,\"start\":33827}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1508,\"start\":1496},{\"attributes\":{\"n\":\"2.\"},\"end\":5848,\"start\":5836},{\"attributes\":{\"n\":\"3.\"},\"end\":10665,\"start\":10641},{\"attributes\":{\"n\":\"4.\"},\"end\":17790,\"start\":17770},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17989,\"start\":17972},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19436,\"start\":19420},{\"attributes\":{\"n\":\"4.3.\"},\"end\":20475,\"start\":20453},{\"attributes\":{\"n\":\"4.4.\"},\"end\":23339,\"start\":23325},{\"attributes\":{\"n\":\"4.5.\"},\"end\":27407,\"start\":27373},{\"attributes\":{\"n\":\"4.6.\"},\"end\":31631,\"start\":31587},{\"attributes\":{\"n\":\"5.\"},\"end\":35007,\"start\":34997},{\"end\":35695,\"start\":35685},{\"end\":36149,\"start\":36139},{\"end\":36445,\"start\":36435},{\"end\":36692,\"start\":36683},{\"end\":37299,\"start\":37292},{\"end\":37690,\"start\":37681},{\"end\":37870,\"start\":37861}]", "table": "[{\"end\":37290,\"start\":36868},{\"end\":37679,\"start\":37348},{\"end\":37859,\"start\":37854},{\"end\":38421,\"start\":38288}]", "figure_caption": "[{\"end\":35954,\"start\":35697},{\"end\":36137,\"start\":35957},{\"end\":36433,\"start\":36151},{\"end\":36681,\"start\":36447},{\"end\":36868,\"start\":36694},{\"end\":37348,\"start\":37301},{\"end\":37854,\"start\":37692},{\"end\":38288,\"start\":37872}]", "figure_ref": "[{\"end\":2415,\"start\":2407},{\"end\":4890,\"start\":4882},{\"end\":9741,\"start\":9733},{\"end\":10891,\"start\":10883},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13274,\"start\":13266},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13347,\"start\":13339},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13537,\"start\":13528},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13921,\"start\":13913},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14484,\"start\":14476},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14596,\"start\":14588},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14693,\"start\":14685},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16376,\"start\":16368},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17040,\"start\":17032},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29247,\"start\":29239}]", "bib_author_first_name": "[{\"end\":38632,\"start\":38629},{\"end\":38644,\"start\":38638},{\"end\":38657,\"start\":38650},{\"end\":38671,\"start\":38665},{\"end\":38980,\"start\":38975},{\"end\":38995,\"start\":38986},{\"end\":39005,\"start\":39001},{\"end\":39017,\"start\":39010},{\"end\":39032,\"start\":39025},{\"end\":39048,\"start\":39038},{\"end\":39484,\"start\":39483},{\"end\":39505,\"start\":39494},{\"end\":39804,\"start\":39798},{\"end\":39820,\"start\":39811},{\"end\":39838,\"start\":39830},{\"end\":40155,\"start\":40154},{\"end\":40157,\"start\":40156},{\"end\":40167,\"start\":40166},{\"end\":40176,\"start\":40175},{\"end\":40178,\"start\":40177},{\"end\":40189,\"start\":40188},{\"end\":40191,\"start\":40190},{\"end\":40658,\"start\":40652},{\"end\":40672,\"start\":40665},{\"end\":41110,\"start\":41104},{\"end\":41131,\"start\":41124},{\"end\":41145,\"start\":41141},{\"end\":41157,\"start\":41151},{\"end\":41172,\"start\":41167},{\"end\":41191,\"start\":41183},{\"end\":41207,\"start\":41200},{\"end\":41223,\"start\":41217},{\"end\":41237,\"start\":41231},{\"end\":41756,\"start\":41752},{\"end\":41771,\"start\":41764},{\"end\":41785,\"start\":41781},{\"end\":41802,\"start\":41795},{\"end\":41818,\"start\":41812},{\"end\":41830,\"start\":41826},{\"end\":41845,\"start\":41841},{\"end\":41862,\"start\":41855},{\"end\":42369,\"start\":42365},{\"end\":42382,\"start\":42374},{\"end\":42396,\"start\":42389},{\"end\":42819,\"start\":42812},{\"end\":42834,\"start\":42828},{\"end\":42847,\"start\":42841},{\"end\":43196,\"start\":43189},{\"end\":43210,\"start\":43205},{\"end\":43229,\"start\":43222},{\"end\":43231,\"start\":43230},{\"end\":43685,\"start\":43678},{\"end\":43699,\"start\":43694},{\"end\":43703,\"start\":43700},{\"end\":43718,\"start\":43711},{\"end\":43734,\"start\":43727},{\"end\":43736,\"start\":43735},{\"end\":44114,\"start\":44109},{\"end\":44131,\"start\":44126},{\"end\":44146,\"start\":44140},{\"end\":44501,\"start\":44494},{\"end\":44513,\"start\":44506},{\"end\":44880,\"start\":44876},{\"end\":44894,\"start\":44890},{\"end\":44909,\"start\":44902},{\"end\":44923,\"start\":44916},{\"end\":44936,\"start\":44929},{\"end\":44948,\"start\":44944},{\"end\":44963,\"start\":44957},{\"end\":44977,\"start\":44971},{\"end\":45317,\"start\":45316},{\"end\":45319,\"start\":45318},{\"end\":45335,\"start\":45330},{\"end\":45337,\"start\":45336},{\"end\":45714,\"start\":45707},{\"end\":45731,\"start\":45720},{\"end\":46155,\"start\":46148},{\"end\":46167,\"start\":46161},{\"end\":46185,\"start\":46174},{\"end\":46673,\"start\":46666},{\"end\":46685,\"start\":46679},{\"end\":46703,\"start\":46692},{\"end\":47050,\"start\":47049},{\"end\":47057,\"start\":47056},{\"end\":47066,\"start\":47065},{\"end\":47076,\"start\":47075},{\"end\":47086,\"start\":47085},{\"end\":47101,\"start\":47100},{\"end\":47513,\"start\":47509},{\"end\":47515,\"start\":47514},{\"end\":47540,\"start\":47526},{\"end\":47867,\"start\":47861},{\"end\":47881,\"start\":47875},{\"end\":47892,\"start\":47887},{\"end\":47912,\"start\":47902},{\"end\":47923,\"start\":47919},{\"end\":47943,\"start\":47940},{\"end\":48436,\"start\":48432},{\"end\":48457,\"start\":48451},{\"end\":48468,\"start\":48465},{\"end\":48482,\"start\":48477},{\"end\":48495,\"start\":48491},{\"end\":48512,\"start\":48506},{\"end\":48736,\"start\":48735},{\"end\":48752,\"start\":48747},{\"end\":49041,\"start\":49038},{\"end\":49058,\"start\":49049},{\"end\":49326,\"start\":49319},{\"end\":49335,\"start\":49331},{\"end\":49613,\"start\":49605},{\"end\":49626,\"start\":49619},{\"end\":49639,\"start\":49634},{\"end\":49655,\"start\":49650},{\"end\":49668,\"start\":49662},{\"end\":49681,\"start\":49677},{\"end\":49696,\"start\":49691},{\"end\":49715,\"start\":49705},{\"end\":50132,\"start\":50127},{\"end\":50147,\"start\":50138},{\"end\":50161,\"start\":50155},{\"end\":50170,\"start\":50166},{\"end\":50182,\"start\":50176},{\"end\":50193,\"start\":50189},{\"end\":50206,\"start\":50199},{\"end\":50220,\"start\":50212},{\"end\":50232,\"start\":50227},{\"end\":50243,\"start\":50237},{\"end\":50820,\"start\":50812},{\"end\":50831,\"start\":50826},{\"end\":50841,\"start\":50838},{\"end\":51256,\"start\":51248},{\"end\":51269,\"start\":51262},{\"end\":51280,\"start\":51275},{\"end\":51290,\"start\":51287},{\"end\":51660,\"start\":51656},{\"end\":51678,\"start\":51673},{\"end\":51949,\"start\":51943},{\"end\":51961,\"start\":51954},{\"end\":51972,\"start\":51966},{\"end\":51982,\"start\":51978},{\"end\":51993,\"start\":51988},{\"end\":52460,\"start\":52452},{\"end\":52472,\"start\":52468},{\"end\":52484,\"start\":52478},{\"end\":52501,\"start\":52494},{\"end\":52517,\"start\":52511},{\"end\":52533,\"start\":52527},{\"end\":52553,\"start\":52547},{\"end\":53119,\"start\":53114},{\"end\":53135,\"start\":53129},{\"end\":53147,\"start\":53141},{\"end\":53359,\"start\":53353},{\"end\":53374,\"start\":53367},{\"end\":53650,\"start\":53644},{\"end\":53667,\"start\":53658},{\"end\":53683,\"start\":53676},{\"end\":53938,\"start\":53932},{\"end\":53955,\"start\":53946},{\"end\":53971,\"start\":53964},{\"end\":54262,\"start\":54259},{\"end\":54276,\"start\":54275},{\"end\":54292,\"start\":54285},{\"end\":54313,\"start\":54305},{\"end\":54315,\"start\":54314},{\"end\":54328,\"start\":54324},{\"end\":54340,\"start\":54337},{\"end\":54613,\"start\":54610},{\"end\":54627,\"start\":54621},{\"end\":54642,\"start\":54637},{\"end\":54657,\"start\":54653},{\"end\":54679,\"start\":54671},{\"end\":55121,\"start\":55116},{\"end\":55135,\"start\":55131},{\"end\":55376,\"start\":55371},{\"end\":55390,\"start\":55386},{\"end\":55401,\"start\":55396},{\"end\":55412,\"start\":55408},{\"end\":55685,\"start\":55684},{\"end\":55696,\"start\":55695},{\"end\":55710,\"start\":55709},{\"end\":55724,\"start\":55723},{\"end\":55736,\"start\":55735},{\"end\":55745,\"start\":55744},{\"end\":56070,\"start\":56063},{\"end\":56102,\"start\":56095},{\"end\":56117,\"start\":56112},{\"end\":56120,\"start\":56118},{\"end\":56542,\"start\":56538},{\"end\":56559,\"start\":56551},{\"end\":56575,\"start\":56569},{\"end\":56923,\"start\":56919},{\"end\":56938,\"start\":56932},{\"end\":56954,\"start\":56949},{\"end\":56969,\"start\":56963},{\"end\":56988,\"start\":56981},{\"end\":57398,\"start\":57395},{\"end\":57410,\"start\":57404},{\"end\":57424,\"start\":57416},{\"end\":57432,\"start\":57429},{\"end\":57446,\"start\":57438},{\"end\":57461,\"start\":57453},{\"end\":57823,\"start\":57817},{\"end\":57840,\"start\":57833},{\"end\":58038,\"start\":58030},{\"end\":58068,\"start\":58057},{\"end\":58364,\"start\":58357},{\"end\":58380,\"start\":58371},{\"end\":58393,\"start\":58385},{\"end\":58407,\"start\":58400},{\"end\":58754,\"start\":58748},{\"end\":58766,\"start\":58760},{\"end\":58782,\"start\":58773},{\"end\":59173,\"start\":59167},{\"end\":59187,\"start\":59179},{\"end\":59201,\"start\":59194},{\"end\":59210,\"start\":59207},{\"end\":59698,\"start\":59692},{\"end\":59712,\"start\":59704},{\"end\":59726,\"start\":59719},{\"end\":59735,\"start\":59732},{\"end\":60114,\"start\":60107},{\"end\":60124,\"start\":60121},{\"end\":60415,\"start\":60406},{\"end\":60654,\"start\":60649},{\"end\":60668,\"start\":60662},{\"end\":60686,\"start\":60679},{\"end\":60703,\"start\":60697},{\"end\":60972,\"start\":60965},{\"end\":60988,\"start\":60982},{\"end\":61003,\"start\":60996},{\"end\":61020,\"start\":61015},{\"end\":61023,\"start\":61021},{\"end\":61411,\"start\":61404},{\"end\":61426,\"start\":61421},{\"end\":61439,\"start\":61433},{\"end\":61454,\"start\":61447},{\"end\":61471,\"start\":61466},{\"end\":61474,\"start\":61472},{\"end\":61959,\"start\":61954},{\"end\":61973,\"start\":61966},{\"end\":61989,\"start\":61983},{\"end\":62004,\"start\":61997},{\"end\":62493,\"start\":62488},{\"end\":62507,\"start\":62500},{\"end\":62523,\"start\":62517},{\"end\":62555,\"start\":62548},{\"end\":62568,\"start\":62563},{\"end\":62571,\"start\":62569},{\"end\":62586,\"start\":62579},{\"end\":63095,\"start\":63088},{\"end\":63108,\"start\":63104},{\"end\":63515,\"start\":63510},{\"end\":63529,\"start\":63524},{\"end\":63548,\"start\":63541},{\"end\":63572,\"start\":63565},{\"end\":63574,\"start\":63573},{\"end\":63591,\"start\":63584},{\"end\":63886,\"start\":63881},{\"end\":63902,\"start\":63895},{\"end\":63912,\"start\":63911},{\"end\":63929,\"start\":63922},{\"end\":64365,\"start\":64357},{\"end\":64385,\"start\":64379},{\"end\":64398,\"start\":64394},{\"end\":64418,\"start\":64410},{\"end\":64875,\"start\":64872},{\"end\":64883,\"start\":64880},{\"end\":64893,\"start\":64890},{\"end\":64904,\"start\":64900},{\"end\":64914,\"start\":64910},{\"end\":64926,\"start\":64921},{\"end\":65172,\"start\":65169},{\"end\":65181,\"start\":65177},{\"end\":65197,\"start\":65190},{\"end\":65656,\"start\":65653},{\"end\":65674,\"start\":65668},{\"end\":65686,\"start\":65680},{\"end\":65698,\"start\":65694},{\"end\":65702,\"start\":65699},{\"end\":65712,\"start\":65709},{\"end\":66218,\"start\":66207},{\"end\":66231,\"start\":66225},{\"end\":66243,\"start\":66238},{\"end\":66531,\"start\":66526},{\"end\":66547,\"start\":66539},{\"end\":66559,\"start\":66552},{\"end\":66572,\"start\":66566},{\"end\":66593,\"start\":66585},{\"end\":66605,\"start\":66599},{\"end\":66882,\"start\":66875},{\"end\":66896,\"start\":66889},{\"end\":66908,\"start\":66904},{\"end\":66925,\"start\":66918},{\"end\":67378,\"start\":67371},{\"end\":67392,\"start\":67385},{\"end\":67405,\"start\":67401},{\"end\":67419,\"start\":67413},{\"end\":67431,\"start\":67427}]", "bib_author_last_name": "[{\"end\":38636,\"start\":38633},{\"end\":38648,\"start\":38645},{\"end\":38663,\"start\":38658},{\"end\":38679,\"start\":38672},{\"end\":38984,\"start\":38981},{\"end\":38999,\"start\":38996},{\"end\":39008,\"start\":39006},{\"end\":39023,\"start\":39018},{\"end\":39036,\"start\":39033},{\"end\":39053,\"start\":39049},{\"end\":39492,\"start\":39485},{\"end\":39511,\"start\":39506},{\"end\":39520,\"start\":39513},{\"end\":39809,\"start\":39805},{\"end\":39828,\"start\":39821},{\"end\":39844,\"start\":39839},{\"end\":40164,\"start\":40158},{\"end\":40173,\"start\":40168},{\"end\":40186,\"start\":40179},{\"end\":40197,\"start\":40192},{\"end\":40663,\"start\":40659},{\"end\":40679,\"start\":40673},{\"end\":41122,\"start\":41111},{\"end\":41139,\"start\":41132},{\"end\":41149,\"start\":41146},{\"end\":41165,\"start\":41158},{\"end\":41181,\"start\":41173},{\"end\":41198,\"start\":41192},{\"end\":41215,\"start\":41208},{\"end\":41229,\"start\":41224},{\"end\":41245,\"start\":41238},{\"end\":41251,\"start\":41247},{\"end\":41762,\"start\":41757},{\"end\":41779,\"start\":41772},{\"end\":41793,\"start\":41786},{\"end\":41810,\"start\":41803},{\"end\":41824,\"start\":41819},{\"end\":41839,\"start\":41831},{\"end\":41853,\"start\":41846},{\"end\":41869,\"start\":41863},{\"end\":42372,\"start\":42370},{\"end\":42387,\"start\":42383},{\"end\":42401,\"start\":42397},{\"end\":42826,\"start\":42820},{\"end\":42839,\"start\":42835},{\"end\":42855,\"start\":42848},{\"end\":43203,\"start\":43197},{\"end\":43220,\"start\":43211},{\"end\":43239,\"start\":43232},{\"end\":43692,\"start\":43686},{\"end\":43709,\"start\":43704},{\"end\":43725,\"start\":43719},{\"end\":43744,\"start\":43737},{\"end\":44124,\"start\":44115},{\"end\":44138,\"start\":44132},{\"end\":44153,\"start\":44147},{\"end\":44504,\"start\":44502},{\"end\":44522,\"start\":44514},{\"end\":44888,\"start\":44881},{\"end\":44900,\"start\":44895},{\"end\":44914,\"start\":44910},{\"end\":44927,\"start\":44924},{\"end\":44942,\"start\":44937},{\"end\":44955,\"start\":44949},{\"end\":44969,\"start\":44964},{\"end\":44985,\"start\":44978},{\"end\":45328,\"start\":45320},{\"end\":45342,\"start\":45338},{\"end\":45351,\"start\":45344},{\"end\":45718,\"start\":45715},{\"end\":45735,\"start\":45732},{\"end\":46159,\"start\":46156},{\"end\":46172,\"start\":46168},{\"end\":46189,\"start\":46186},{\"end\":46677,\"start\":46674},{\"end\":46690,\"start\":46686},{\"end\":46707,\"start\":46704},{\"end\":47054,\"start\":47051},{\"end\":47063,\"start\":47058},{\"end\":47073,\"start\":47067},{\"end\":47083,\"start\":47077},{\"end\":47098,\"start\":47087},{\"end\":47106,\"start\":47102},{\"end\":47524,\"start\":47516},{\"end\":47547,\"start\":47541},{\"end\":47557,\"start\":47549},{\"end\":47873,\"start\":47868},{\"end\":47885,\"start\":47882},{\"end\":47900,\"start\":47893},{\"end\":47917,\"start\":47913},{\"end\":47938,\"start\":47924},{\"end\":47949,\"start\":47944},{\"end\":48449,\"start\":48437},{\"end\":48463,\"start\":48458},{\"end\":48475,\"start\":48469},{\"end\":48489,\"start\":48483},{\"end\":48504,\"start\":48496},{\"end\":48521,\"start\":48513},{\"end\":48745,\"start\":48737},{\"end\":48759,\"start\":48753},{\"end\":48763,\"start\":48761},{\"end\":49047,\"start\":49042},{\"end\":49068,\"start\":49059},{\"end\":49329,\"start\":49327},{\"end\":49343,\"start\":49336},{\"end\":49617,\"start\":49614},{\"end\":49632,\"start\":49627},{\"end\":49648,\"start\":49640},{\"end\":49660,\"start\":49656},{\"end\":49675,\"start\":49669},{\"end\":49689,\"start\":49682},{\"end\":49703,\"start\":49697},{\"end\":49723,\"start\":49716},{\"end\":50001,\"start\":49993},{\"end\":50136,\"start\":50133},{\"end\":50153,\"start\":50148},{\"end\":50164,\"start\":50162},{\"end\":50174,\"start\":50171},{\"end\":50187,\"start\":50183},{\"end\":50197,\"start\":50194},{\"end\":50210,\"start\":50207},{\"end\":50225,\"start\":50221},{\"end\":50235,\"start\":50233},{\"end\":50249,\"start\":50244},{\"end\":50824,\"start\":50821},{\"end\":50836,\"start\":50832},{\"end\":50855,\"start\":50842},{\"end\":50859,\"start\":50857},{\"end\":51260,\"start\":51257},{\"end\":51273,\"start\":51270},{\"end\":51285,\"start\":51281},{\"end\":51293,\"start\":51291},{\"end\":51671,\"start\":51661},{\"end\":51685,\"start\":51679},{\"end\":51952,\"start\":51950},{\"end\":51964,\"start\":51962},{\"end\":51976,\"start\":51973},{\"end\":51986,\"start\":51983},{\"end\":51996,\"start\":51994},{\"end\":52466,\"start\":52461},{\"end\":52476,\"start\":52473},{\"end\":52492,\"start\":52485},{\"end\":52509,\"start\":52502},{\"end\":52525,\"start\":52518},{\"end\":52545,\"start\":52534},{\"end\":52558,\"start\":52554},{\"end\":53127,\"start\":53120},{\"end\":53139,\"start\":53136},{\"end\":53152,\"start\":53148},{\"end\":53365,\"start\":53360},{\"end\":53381,\"start\":53375},{\"end\":53656,\"start\":53651},{\"end\":53674,\"start\":53668},{\"end\":53690,\"start\":53684},{\"end\":53944,\"start\":53939},{\"end\":53962,\"start\":53956},{\"end\":53978,\"start\":53972},{\"end\":54273,\"start\":54263},{\"end\":54283,\"start\":54277},{\"end\":54303,\"start\":54293},{\"end\":54322,\"start\":54316},{\"end\":54335,\"start\":54329},{\"end\":54352,\"start\":54341},{\"end\":54356,\"start\":54354},{\"end\":54619,\"start\":54614},{\"end\":54635,\"start\":54628},{\"end\":54651,\"start\":54643},{\"end\":54669,\"start\":54658},{\"end\":54683,\"start\":54680},{\"end\":55129,\"start\":55122},{\"end\":55139,\"start\":55136},{\"end\":55384,\"start\":55377},{\"end\":55394,\"start\":55391},{\"end\":55406,\"start\":55402},{\"end\":55416,\"start\":55413},{\"end\":55693,\"start\":55686},{\"end\":55707,\"start\":55697},{\"end\":55721,\"start\":55711},{\"end\":55733,\"start\":55725},{\"end\":55742,\"start\":55737},{\"end\":55761,\"start\":55746},{\"end\":56093,\"start\":56071},{\"end\":56110,\"start\":56103},{\"end\":56126,\"start\":56121},{\"end\":56135,\"start\":56128},{\"end\":56549,\"start\":56543},{\"end\":56567,\"start\":56560},{\"end\":56580,\"start\":56576},{\"end\":56930,\"start\":56924},{\"end\":56947,\"start\":56939},{\"end\":56961,\"start\":56955},{\"end\":56979,\"start\":56970},{\"end\":56995,\"start\":56989},{\"end\":57402,\"start\":57399},{\"end\":57414,\"start\":57411},{\"end\":57427,\"start\":57425},{\"end\":57436,\"start\":57433},{\"end\":57451,\"start\":57447},{\"end\":57465,\"start\":57462},{\"end\":57831,\"start\":57824},{\"end\":57847,\"start\":57841},{\"end\":58055,\"start\":58039},{\"end\":58074,\"start\":58069},{\"end\":58369,\"start\":58365},{\"end\":58383,\"start\":58381},{\"end\":58398,\"start\":58394},{\"end\":58413,\"start\":58408},{\"end\":58758,\"start\":58755},{\"end\":58771,\"start\":58767},{\"end\":58788,\"start\":58783},{\"end\":59177,\"start\":59174},{\"end\":59192,\"start\":59188},{\"end\":59205,\"start\":59202},{\"end\":59216,\"start\":59211},{\"end\":59702,\"start\":59699},{\"end\":59717,\"start\":59713},{\"end\":59730,\"start\":59727},{\"end\":59741,\"start\":59736},{\"end\":60119,\"start\":60115},{\"end\":60129,\"start\":60125},{\"end\":60421,\"start\":60416},{\"end\":60660,\"start\":60655},{\"end\":60677,\"start\":60669},{\"end\":60695,\"start\":60687},{\"end\":60713,\"start\":60704},{\"end\":60980,\"start\":60973},{\"end\":60994,\"start\":60989},{\"end\":61013,\"start\":61004},{\"end\":61031,\"start\":61024},{\"end\":61419,\"start\":61412},{\"end\":61431,\"start\":61427},{\"end\":61445,\"start\":61440},{\"end\":61464,\"start\":61455},{\"end\":61482,\"start\":61475},{\"end\":61964,\"start\":61960},{\"end\":61981,\"start\":61974},{\"end\":61995,\"start\":61990},{\"end\":62014,\"start\":62005},{\"end\":62498,\"start\":62494},{\"end\":62515,\"start\":62508},{\"end\":62546,\"start\":62524},{\"end\":62561,\"start\":62556},{\"end\":62577,\"start\":62572},{\"end\":62594,\"start\":62587},{\"end\":62605,\"start\":62596},{\"end\":63102,\"start\":63096},{\"end\":63116,\"start\":63109},{\"end\":63522,\"start\":63516},{\"end\":63539,\"start\":63530},{\"end\":63563,\"start\":63549},{\"end\":63582,\"start\":63575},{\"end\":63598,\"start\":63592},{\"end\":63893,\"start\":63887},{\"end\":63909,\"start\":63903},{\"end\":63920,\"start\":63913},{\"end\":63937,\"start\":63930},{\"end\":63953,\"start\":63939},{\"end\":64377,\"start\":64366},{\"end\":64392,\"start\":64386},{\"end\":64408,\"start\":64399},{\"end\":64425,\"start\":64419},{\"end\":64878,\"start\":64876},{\"end\":64888,\"start\":64884},{\"end\":64898,\"start\":64894},{\"end\":64908,\"start\":64905},{\"end\":64919,\"start\":64915},{\"end\":64932,\"start\":64927},{\"end\":65175,\"start\":65173},{\"end\":65188,\"start\":65182},{\"end\":65204,\"start\":65198},{\"end\":65666,\"start\":65657},{\"end\":65678,\"start\":65675},{\"end\":65692,\"start\":65687},{\"end\":65707,\"start\":65703},{\"end\":65718,\"start\":65713},{\"end\":66223,\"start\":66219},{\"end\":66236,\"start\":66232},{\"end\":66251,\"start\":66244},{\"end\":66537,\"start\":66532},{\"end\":66550,\"start\":66548},{\"end\":66564,\"start\":66560},{\"end\":66583,\"start\":66573},{\"end\":66597,\"start\":66594},{\"end\":66610,\"start\":66606},{\"end\":66887,\"start\":66883},{\"end\":66902,\"start\":66897},{\"end\":66916,\"start\":66909},{\"end\":66930,\"start\":66926},{\"end\":67383,\"start\":67379},{\"end\":67399,\"start\":67393},{\"end\":67411,\"start\":67406},{\"end\":67425,\"start\":67420},{\"end\":67439,\"start\":67432}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16788989},\"end\":38934,\"start\":38561},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":90239045},\"end\":39426,\"start\":38936},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14930294},\"end\":39763,\"start\":39428},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3243550},\"end\":40090,\"start\":39765},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4637111},\"end\":40572,\"start\":40092},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9489832},\"end\":41042,\"start\":40574},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":12552176},\"end\":41694,\"start\":41044},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":190000375},\"end\":42299,\"start\":41696},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":46968214},\"end\":42739,\"start\":42301},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6724907},\"end\":43119,\"start\":42741},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206596513},\"end\":43619,\"start\":43121},{\"attributes\":{\"id\":\"b11\"},\"end\":44013,\"start\":43621},{\"attributes\":{\"id\":\"b12\"},\"end\":44447,\"start\":44015},{\"attributes\":{\"id\":\"b13\"},\"end\":44818,\"start\":44449},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b14\",\"matched_paper_id\":7646250},\"end\":45288,\"start\":44820},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1371968},\"end\":45613,\"start\":45290},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":220647088},\"end\":46060,\"start\":45615},{\"attributes\":{\"id\":\"b17\"},\"end\":46587,\"start\":46062},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":81977229},\"end\":46977,\"start\":46589},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3759573},\"end\":47405,\"start\":46979},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6247123},\"end\":47765,\"start\":47407},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10817557},\"end\":48387,\"start\":47767},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":219558670},\"end\":48689,\"start\":48389},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b23\"},\"end\":48909,\"start\":48691},{\"attributes\":{\"id\":\"b24\"},\"end\":49247,\"start\":48911},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4572038},\"end\":49560,\"start\":49249},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14113767},\"end\":49989,\"start\":49562},{\"attributes\":{\"id\":\"b27\"},\"end\":50020,\"start\":49991},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":214713881},\"end\":50746,\"start\":50022},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":57333758},\"end\":51195,\"start\":50748},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":125947273},\"end\":51652,\"start\":51197},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b31\"},\"end\":51869,\"start\":51654},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":8000286},\"end\":52346,\"start\":51871},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":206594275},\"end\":53032,\"start\":52348},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":19160323},\"end\":53308,\"start\":53034},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":12986049},\"end\":53602,\"start\":53310},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":984034},\"end\":53911,\"start\":53604},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":4465280},\"end\":54185,\"start\":53913},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":213175590},\"end\":54558,\"start\":54187},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":13815143},\"end\":55065,\"start\":54560},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":212675709},\"end\":55328,\"start\":55067},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":202565675},\"end\":55604,\"start\":55330},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1949934},\"end\":56026,\"start\":55606},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":104292122},\"end\":56469,\"start\":56028},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":5735449},\"end\":56820,\"start\":56471},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":195776274},\"end\":57337,\"start\":56822},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":29151213},\"end\":57794,\"start\":57339},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":221112229},\"end\":57995,\"start\":57796},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1728538},\"end\":58294,\"start\":57997},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":215548442},\"end\":58689,\"start\":58296},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":206591220},\"end\":59093,\"start\":58691},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":30824366},\"end\":59601,\"start\":59095},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":52285316},\"end\":60044,\"start\":59603},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":214667893},\"end\":60339,\"start\":60046},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":5908881},\"end\":60580,\"start\":60341},{\"attributes\":{\"doi\":\"arXiv:2005.10876\",\"id\":\"b55\"},\"end\":60922,\"start\":60582},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":9168906},\"end\":61365,\"start\":60924},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":52978720},\"end\":61877,\"start\":61367},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":102351899},\"end\":62415,\"start\":61879},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":214727592},\"end\":63035,\"start\":62417},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":216080881},\"end\":63472,\"start\":63037},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":220961565},\"end\":63840,\"start\":63474},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":202676783},\"end\":64293,\"start\":63842},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":206769904},\"end\":64782,\"start\":64295},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":4469249},\"end\":65110,\"start\":64784},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":9280953},\"end\":65557,\"start\":65112},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":214795169},\"end\":66146,\"start\":65559},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":15250191},\"end\":66481,\"start\":66148},{\"attributes\":{\"doi\":\"arXiv:1911.13287\",\"id\":\"b68\"},\"end\":66815,\"start\":66483},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":11977588},\"end\":67298,\"start\":66817},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":219893035},\"end\":67618,\"start\":67300}]", "bib_title": "[{\"end\":38627,\"start\":38561},{\"end\":38973,\"start\":38936},{\"end\":39481,\"start\":39428},{\"end\":39796,\"start\":39765},{\"end\":40152,\"start\":40092},{\"end\":40650,\"start\":40574},{\"end\":41102,\"start\":41044},{\"end\":41750,\"start\":41696},{\"end\":42363,\"start\":42301},{\"end\":42810,\"start\":42741},{\"end\":43187,\"start\":43121},{\"end\":43676,\"start\":43621},{\"end\":44107,\"start\":44015},{\"end\":44492,\"start\":44449},{\"end\":44874,\"start\":44820},{\"end\":45314,\"start\":45290},{\"end\":45705,\"start\":45615},{\"end\":46146,\"start\":46062},{\"end\":46664,\"start\":46589},{\"end\":47047,\"start\":46979},{\"end\":47507,\"start\":47407},{\"end\":47859,\"start\":47767},{\"end\":48430,\"start\":48389},{\"end\":49317,\"start\":49249},{\"end\":49603,\"start\":49562},{\"end\":50125,\"start\":50022},{\"end\":50810,\"start\":50748},{\"end\":51246,\"start\":51197},{\"end\":51941,\"start\":51871},{\"end\":52450,\"start\":52348},{\"end\":53112,\"start\":53034},{\"end\":53351,\"start\":53310},{\"end\":53642,\"start\":53604},{\"end\":53930,\"start\":53913},{\"end\":54257,\"start\":54187},{\"end\":54608,\"start\":54560},{\"end\":55114,\"start\":55067},{\"end\":55369,\"start\":55330},{\"end\":55682,\"start\":55606},{\"end\":56061,\"start\":56028},{\"end\":56536,\"start\":56471},{\"end\":56917,\"start\":56822},{\"end\":57393,\"start\":57339},{\"end\":57815,\"start\":57796},{\"end\":58028,\"start\":57997},{\"end\":58355,\"start\":58296},{\"end\":58746,\"start\":58691},{\"end\":59165,\"start\":59095},{\"end\":59690,\"start\":59603},{\"end\":60105,\"start\":60046},{\"end\":60404,\"start\":60341},{\"end\":60963,\"start\":60924},{\"end\":61402,\"start\":61367},{\"end\":61952,\"start\":61879},{\"end\":62486,\"start\":62417},{\"end\":63086,\"start\":63037},{\"end\":63508,\"start\":63474},{\"end\":63879,\"start\":63842},{\"end\":64355,\"start\":64295},{\"end\":64870,\"start\":64784},{\"end\":65167,\"start\":65112},{\"end\":65651,\"start\":65559},{\"end\":66205,\"start\":66148},{\"end\":66873,\"start\":66817},{\"end\":67369,\"start\":67300}]", "bib_author": "[{\"end\":38638,\"start\":38629},{\"end\":38650,\"start\":38638},{\"end\":38665,\"start\":38650},{\"end\":38681,\"start\":38665},{\"end\":38986,\"start\":38975},{\"end\":39001,\"start\":38986},{\"end\":39010,\"start\":39001},{\"end\":39025,\"start\":39010},{\"end\":39038,\"start\":39025},{\"end\":39055,\"start\":39038},{\"end\":39494,\"start\":39483},{\"end\":39513,\"start\":39494},{\"end\":39522,\"start\":39513},{\"end\":39811,\"start\":39798},{\"end\":39830,\"start\":39811},{\"end\":39846,\"start\":39830},{\"end\":40166,\"start\":40154},{\"end\":40175,\"start\":40166},{\"end\":40188,\"start\":40175},{\"end\":40199,\"start\":40188},{\"end\":40665,\"start\":40652},{\"end\":40681,\"start\":40665},{\"end\":41124,\"start\":41104},{\"end\":41141,\"start\":41124},{\"end\":41151,\"start\":41141},{\"end\":41167,\"start\":41151},{\"end\":41183,\"start\":41167},{\"end\":41200,\"start\":41183},{\"end\":41217,\"start\":41200},{\"end\":41231,\"start\":41217},{\"end\":41247,\"start\":41231},{\"end\":41253,\"start\":41247},{\"end\":41764,\"start\":41752},{\"end\":41781,\"start\":41764},{\"end\":41795,\"start\":41781},{\"end\":41812,\"start\":41795},{\"end\":41826,\"start\":41812},{\"end\":41841,\"start\":41826},{\"end\":41855,\"start\":41841},{\"end\":41871,\"start\":41855},{\"end\":42374,\"start\":42365},{\"end\":42389,\"start\":42374},{\"end\":42403,\"start\":42389},{\"end\":42828,\"start\":42812},{\"end\":42841,\"start\":42828},{\"end\":42857,\"start\":42841},{\"end\":43205,\"start\":43189},{\"end\":43222,\"start\":43205},{\"end\":43241,\"start\":43222},{\"end\":43694,\"start\":43678},{\"end\":43711,\"start\":43694},{\"end\":43727,\"start\":43711},{\"end\":43746,\"start\":43727},{\"end\":44126,\"start\":44109},{\"end\":44140,\"start\":44126},{\"end\":44155,\"start\":44140},{\"end\":44506,\"start\":44494},{\"end\":44524,\"start\":44506},{\"end\":44890,\"start\":44876},{\"end\":44902,\"start\":44890},{\"end\":44916,\"start\":44902},{\"end\":44929,\"start\":44916},{\"end\":44944,\"start\":44929},{\"end\":44957,\"start\":44944},{\"end\":44971,\"start\":44957},{\"end\":44987,\"start\":44971},{\"end\":45330,\"start\":45316},{\"end\":45344,\"start\":45330},{\"end\":45353,\"start\":45344},{\"end\":45720,\"start\":45707},{\"end\":45737,\"start\":45720},{\"end\":46161,\"start\":46148},{\"end\":46174,\"start\":46161},{\"end\":46191,\"start\":46174},{\"end\":46679,\"start\":46666},{\"end\":46692,\"start\":46679},{\"end\":46709,\"start\":46692},{\"end\":47056,\"start\":47049},{\"end\":47065,\"start\":47056},{\"end\":47075,\"start\":47065},{\"end\":47085,\"start\":47075},{\"end\":47100,\"start\":47085},{\"end\":47108,\"start\":47100},{\"end\":47526,\"start\":47509},{\"end\":47549,\"start\":47526},{\"end\":47559,\"start\":47549},{\"end\":47875,\"start\":47861},{\"end\":47887,\"start\":47875},{\"end\":47902,\"start\":47887},{\"end\":47919,\"start\":47902},{\"end\":47940,\"start\":47919},{\"end\":47951,\"start\":47940},{\"end\":48451,\"start\":48432},{\"end\":48465,\"start\":48451},{\"end\":48477,\"start\":48465},{\"end\":48491,\"start\":48477},{\"end\":48506,\"start\":48491},{\"end\":48523,\"start\":48506},{\"end\":48747,\"start\":48735},{\"end\":48761,\"start\":48747},{\"end\":48765,\"start\":48761},{\"end\":49049,\"start\":49038},{\"end\":49070,\"start\":49049},{\"end\":49331,\"start\":49319},{\"end\":49345,\"start\":49331},{\"end\":49619,\"start\":49605},{\"end\":49634,\"start\":49619},{\"end\":49650,\"start\":49634},{\"end\":49662,\"start\":49650},{\"end\":49677,\"start\":49662},{\"end\":49691,\"start\":49677},{\"end\":49705,\"start\":49691},{\"end\":49725,\"start\":49705},{\"end\":50003,\"start\":49993},{\"end\":50138,\"start\":50127},{\"end\":50155,\"start\":50138},{\"end\":50166,\"start\":50155},{\"end\":50176,\"start\":50166},{\"end\":50189,\"start\":50176},{\"end\":50199,\"start\":50189},{\"end\":50212,\"start\":50199},{\"end\":50227,\"start\":50212},{\"end\":50237,\"start\":50227},{\"end\":50251,\"start\":50237},{\"end\":50826,\"start\":50812},{\"end\":50838,\"start\":50826},{\"end\":50857,\"start\":50838},{\"end\":50861,\"start\":50857},{\"end\":51262,\"start\":51248},{\"end\":51275,\"start\":51262},{\"end\":51287,\"start\":51275},{\"end\":51295,\"start\":51287},{\"end\":51673,\"start\":51656},{\"end\":51687,\"start\":51673},{\"end\":51954,\"start\":51943},{\"end\":51966,\"start\":51954},{\"end\":51978,\"start\":51966},{\"end\":51988,\"start\":51978},{\"end\":51998,\"start\":51988},{\"end\":52468,\"start\":52452},{\"end\":52478,\"start\":52468},{\"end\":52494,\"start\":52478},{\"end\":52511,\"start\":52494},{\"end\":52527,\"start\":52511},{\"end\":52547,\"start\":52527},{\"end\":52560,\"start\":52547},{\"end\":53129,\"start\":53114},{\"end\":53141,\"start\":53129},{\"end\":53154,\"start\":53141},{\"end\":53367,\"start\":53353},{\"end\":53383,\"start\":53367},{\"end\":53658,\"start\":53644},{\"end\":53676,\"start\":53658},{\"end\":53692,\"start\":53676},{\"end\":53946,\"start\":53932},{\"end\":53964,\"start\":53946},{\"end\":53980,\"start\":53964},{\"end\":54275,\"start\":54259},{\"end\":54285,\"start\":54275},{\"end\":54305,\"start\":54285},{\"end\":54324,\"start\":54305},{\"end\":54337,\"start\":54324},{\"end\":54354,\"start\":54337},{\"end\":54358,\"start\":54354},{\"end\":54621,\"start\":54610},{\"end\":54637,\"start\":54621},{\"end\":54653,\"start\":54637},{\"end\":54671,\"start\":54653},{\"end\":54685,\"start\":54671},{\"end\":55131,\"start\":55116},{\"end\":55141,\"start\":55131},{\"end\":55386,\"start\":55371},{\"end\":55396,\"start\":55386},{\"end\":55408,\"start\":55396},{\"end\":55418,\"start\":55408},{\"end\":55695,\"start\":55684},{\"end\":55709,\"start\":55695},{\"end\":55723,\"start\":55709},{\"end\":55735,\"start\":55723},{\"end\":55744,\"start\":55735},{\"end\":55763,\"start\":55744},{\"end\":56095,\"start\":56063},{\"end\":56112,\"start\":56095},{\"end\":56128,\"start\":56112},{\"end\":56137,\"start\":56128},{\"end\":56551,\"start\":56538},{\"end\":56569,\"start\":56551},{\"end\":56582,\"start\":56569},{\"end\":56932,\"start\":56919},{\"end\":56949,\"start\":56932},{\"end\":56963,\"start\":56949},{\"end\":56981,\"start\":56963},{\"end\":56997,\"start\":56981},{\"end\":57404,\"start\":57395},{\"end\":57416,\"start\":57404},{\"end\":57429,\"start\":57416},{\"end\":57438,\"start\":57429},{\"end\":57453,\"start\":57438},{\"end\":57467,\"start\":57453},{\"end\":57833,\"start\":57817},{\"end\":57849,\"start\":57833},{\"end\":58057,\"start\":58030},{\"end\":58076,\"start\":58057},{\"end\":58371,\"start\":58357},{\"end\":58385,\"start\":58371},{\"end\":58400,\"start\":58385},{\"end\":58415,\"start\":58400},{\"end\":58760,\"start\":58748},{\"end\":58773,\"start\":58760},{\"end\":58790,\"start\":58773},{\"end\":59179,\"start\":59167},{\"end\":59194,\"start\":59179},{\"end\":59207,\"start\":59194},{\"end\":59218,\"start\":59207},{\"end\":59704,\"start\":59692},{\"end\":59719,\"start\":59704},{\"end\":59732,\"start\":59719},{\"end\":59743,\"start\":59732},{\"end\":60121,\"start\":60107},{\"end\":60131,\"start\":60121},{\"end\":60423,\"start\":60406},{\"end\":60662,\"start\":60649},{\"end\":60679,\"start\":60662},{\"end\":60697,\"start\":60679},{\"end\":60715,\"start\":60697},{\"end\":60982,\"start\":60965},{\"end\":60996,\"start\":60982},{\"end\":61015,\"start\":60996},{\"end\":61033,\"start\":61015},{\"end\":61421,\"start\":61404},{\"end\":61433,\"start\":61421},{\"end\":61447,\"start\":61433},{\"end\":61466,\"start\":61447},{\"end\":61484,\"start\":61466},{\"end\":61966,\"start\":61954},{\"end\":61983,\"start\":61966},{\"end\":61997,\"start\":61983},{\"end\":62016,\"start\":61997},{\"end\":62500,\"start\":62488},{\"end\":62517,\"start\":62500},{\"end\":62548,\"start\":62517},{\"end\":62563,\"start\":62548},{\"end\":62579,\"start\":62563},{\"end\":62596,\"start\":62579},{\"end\":62607,\"start\":62596},{\"end\":63104,\"start\":63088},{\"end\":63118,\"start\":63104},{\"end\":63524,\"start\":63510},{\"end\":63541,\"start\":63524},{\"end\":63565,\"start\":63541},{\"end\":63584,\"start\":63565},{\"end\":63600,\"start\":63584},{\"end\":63895,\"start\":63881},{\"end\":63911,\"start\":63895},{\"end\":63922,\"start\":63911},{\"end\":63939,\"start\":63922},{\"end\":63955,\"start\":63939},{\"end\":64379,\"start\":64357},{\"end\":64394,\"start\":64379},{\"end\":64410,\"start\":64394},{\"end\":64427,\"start\":64410},{\"end\":64880,\"start\":64872},{\"end\":64890,\"start\":64880},{\"end\":64900,\"start\":64890},{\"end\":64910,\"start\":64900},{\"end\":64921,\"start\":64910},{\"end\":64934,\"start\":64921},{\"end\":65177,\"start\":65169},{\"end\":65190,\"start\":65177},{\"end\":65206,\"start\":65190},{\"end\":65668,\"start\":65653},{\"end\":65680,\"start\":65668},{\"end\":65694,\"start\":65680},{\"end\":65709,\"start\":65694},{\"end\":65720,\"start\":65709},{\"end\":66225,\"start\":66207},{\"end\":66238,\"start\":66225},{\"end\":66253,\"start\":66238},{\"end\":66539,\"start\":66526},{\"end\":66552,\"start\":66539},{\"end\":66566,\"start\":66552},{\"end\":66585,\"start\":66566},{\"end\":66599,\"start\":66585},{\"end\":66612,\"start\":66599},{\"end\":66889,\"start\":66875},{\"end\":66904,\"start\":66889},{\"end\":66918,\"start\":66904},{\"end\":66932,\"start\":66918},{\"end\":67385,\"start\":67371},{\"end\":67401,\"start\":67385},{\"end\":67413,\"start\":67401},{\"end\":67427,\"start\":67413},{\"end\":67441,\"start\":67427}]", "bib_venue": "[{\"end\":39196,\"start\":39134},{\"end\":40822,\"start\":40760},{\"end\":41374,\"start\":41322},{\"end\":42012,\"start\":41950},{\"end\":43382,\"start\":43320},{\"end\":44645,\"start\":44593},{\"end\":45852,\"start\":45803},{\"end\":46338,\"start\":46273},{\"end\":48092,\"start\":48030},{\"end\":50400,\"start\":50334},{\"end\":50970,\"start\":50924},{\"end\":51436,\"start\":51374},{\"end\":52119,\"start\":52067},{\"end\":52701,\"start\":52639},{\"end\":54826,\"start\":54764},{\"end\":56258,\"start\":56206},{\"end\":57576,\"start\":57530},{\"end\":59359,\"start\":59297},{\"end\":61154,\"start\":61102},{\"end\":61647,\"start\":61574},{\"end\":62157,\"start\":62095},{\"end\":62748,\"start\":62686},{\"end\":63281,\"start\":63208},{\"end\":64076,\"start\":64024},{\"end\":64548,\"start\":64496},{\"end\":65347,\"start\":65285},{\"end\":65869,\"start\":65803},{\"end\":67073,\"start\":67011},{\"end\":38719,\"start\":38681},{\"end\":39132,\"start\":39055},{\"end\":39576,\"start\":39522},{\"end\":39909,\"start\":39846},{\"end\":40256,\"start\":40199},{\"end\":40758,\"start\":40681},{\"end\":41320,\"start\":41253},{\"end\":41948,\"start\":41871},{\"end\":42472,\"start\":42403},{\"end\":42917,\"start\":42857},{\"end\":43318,\"start\":43241},{\"end\":43800,\"start\":43746},{\"end\":44220,\"start\":44155},{\"end\":44591,\"start\":44524},{\"end\":45035,\"start\":44991},{\"end\":45403,\"start\":45353},{\"end\":45801,\"start\":45737},{\"end\":46271,\"start\":46191},{\"end\":46771,\"start\":46709},{\"end\":47173,\"start\":47108},{\"end\":47573,\"start\":47559},{\"end\":48028,\"start\":47951},{\"end\":48527,\"start\":48523},{\"end\":48733,\"start\":48691},{\"end\":49036,\"start\":48911},{\"end\":49391,\"start\":49345},{\"end\":49763,\"start\":49725},{\"end\":50332,\"start\":50251},{\"end\":50922,\"start\":50861},{\"end\":51372,\"start\":51295},{\"end\":52065,\"start\":51998},{\"end\":52637,\"start\":52560},{\"end\":53158,\"start\":53154},{\"end\":53443,\"start\":53383},{\"end\":53732,\"start\":53692},{\"end\":54037,\"start\":53980},{\"end\":54362,\"start\":54358},{\"end\":54762,\"start\":54685},{\"end\":55189,\"start\":55141},{\"end\":55452,\"start\":55418},{\"end\":55802,\"start\":55763},{\"end\":56204,\"start\":56137},{\"end\":56620,\"start\":56582},{\"end\":57067,\"start\":56997},{\"end\":57528,\"start\":57467},{\"end\":57887,\"start\":57849},{\"end\":58136,\"start\":58076},{\"end\":58480,\"start\":58415},{\"end\":58865,\"start\":58790},{\"end\":59295,\"start\":59218},{\"end\":59805,\"start\":59743},{\"end\":60176,\"start\":60131},{\"end\":60448,\"start\":60423},{\"end\":60647,\"start\":60582},{\"end\":61100,\"start\":61033},{\"end\":61572,\"start\":61484},{\"end\":62093,\"start\":62016},{\"end\":62684,\"start\":62607},{\"end\":63206,\"start\":63118},{\"end\":63645,\"start\":63600},{\"end\":64022,\"start\":63955},{\"end\":64494,\"start\":64427},{\"end\":64938,\"start\":64934},{\"end\":65283,\"start\":65206},{\"end\":65801,\"start\":65720},{\"end\":66288,\"start\":66253},{\"end\":66524,\"start\":66483},{\"end\":67009,\"start\":66932},{\"end\":67449,\"start\":67441}]"}}}, "year": 2023, "month": 12, "day": 17}