{"id": 51868339, "updated": "2023-09-30 19:06:26.744", "metadata": {"title": "Towards one-shot learning for rare-word translation with external experts", "authors": "[{\"first\":\"Ngoc-Quan\",\"last\":\"Pham\",\"middle\":[]},{\"first\":\"Jan\",\"last\":\"Niehues\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Waibel\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Neural machine translation (NMT) has significantly improved the quality of automatic translation models. One of the main challenges in current systems is the translation of rare words. We present a generic approach to address this weakness by having external models annotate the training data as Experts, and control the model-expert interaction with a pointer network and reinforcement learning. Our experiments using phrase-based models to simulate Experts to complement neural machine translation models show that the model can be trained to copy the annotations into the output consistently. We demonstrate the benefit of our proposed framework in outof domain translation scenarios with only lexical resources, improving more than 1.0 BLEU point in both translation directions English-Spanish and German-English.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1809.03182", "mag": "2953385323", "acl": "W18-2712", "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1809-03182", "doi": "10.18653/v1/w18-2712"}}, "content": {"source": {"pdf_hash": "fabb20386bfcc6757a95d5746d9fd5c7f9e2ff55", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1809.03182v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/W18-2712.pdf", "status": "HYBRID"}}, "grobid": {"id": "dbaacbe60c13ff71772ef34a150750c87df748d0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fabb20386bfcc6757a95d5746d9fd5c7f9e2ff55.txt", "contents": "\nTowards one-shot learning for rare-word translation with external experts\n\n\nNgoc-Quan Pham \nKarlsruhe Institute of Technology\n\n\nJan Niehues jan.niehues@kit.edualex.waibel@kit.edu \nKarlsruhe Institute of Technology\n\n\nAlex Waibel \nKarlsruhe Institute of Technology\n\n\nTowards one-shot learning for rare-word translation with external experts\n\nNeural machine translation (NMT) has significantly improved the quality of automatic translation models. One of the main challenges in current systems is the translation of rare words. We present a generic approach to address this weakness by having external models annotate the training data as Experts, and control the model-expert interaction with a pointer network and reinforcement learning. Our experiments using phrase-based models to simulate Experts to complement neural machine translation models show that the model can be trained to copy the annotations into the output consistently. We demonstrate the benefit of our proposed framework in outof-domain translation scenarios with only lexical resources, improving more than 1.0 BLEU point in both translation directions English\u2192Spanish and German\u2192English.\n\nIntroduction\n\nSequence to sequence models have recently become the state-of-the-art approach for machine translation (Luong et al., 2015;Vaswani et al., 2017). This model architecture can directly approximate the conditional probability of the target sequence given a source sequence using neural networks (Kalchbrenner and Blunsom, 2013). As a result, not only do they model a smoother probability distribution (Bengio et al., 2003) than the sparse phrase tables in statistical machine translation (Koehn et al., 2003), but they can also jointly learn translation models, language models and even alignments in a single model (Bahdanau et al., 2014).\n\nOne of the main weaknesses of neural machine translation models is poor handling of low frequency events. Neural models tend to prioritize output fluency over translation adequacy, and faced with rare words either silently ignore input (Koehn and Knowles, 2017) or fall into underor over-translation (Tu et al., 2016). Examples of these situations include named entities, dates, and rare morphological forms. Improper handling of rare events can be harmful to industrial systems (Wu et al., 2016), where translation mistakes can have serious ramifications. Similarly, translating in specific domains such as information technology or biology, a slight change in vocabulary can drastically alter meaning. It is important, then, to address translation of rare words. While domain-specific parallel corpora can be used to adapt translation models efficiently (Luong and Manning, 2015), parallel corpora for many domains can be difficult to collect, and this requires continued training. Translation lexicons, however, are much more commonly available. In this work, we introduce a strategy to incorporate external lexical knowledge, dubbed \"Expert annotation,\" into neural machine translation models. First, we annotate the lexical translations directly into the source side of the parallel data, so that the information is exposed during both training and inference. Second, inspired by CopyNet (Gu et al., 2016), we utilize a pointer network (Vinyals et al., 2015) to introduce a copy distribution over the source sentence, to increase the generation probability of rare words. Given that the expert annotation can differ from the reference, in order to encourage the model to copy the annotation we use reinforcement learning to guide the search, giving rewards when the annotation is used. Our work is motivated to be able to achieve One-Shot learning, which can help the model to accurately translate the events that are annotated during inference. Such ability can be transferred from an Expert which is capable of learning to translate lexically with one or few examples, such as dictionaries, or phrase-tables, or even human annotators.\n\nWe realize our proposed framework with experiments on English\u2192Spanish and German\u2192English translation tasks. We focus on translation of rare events using translation suggestions from an Expert, here simulated by an additional phrase table.\n\nSpecifically, we annotate rare words in our parallel data with best candidates from a phrase table before training, so that rare events are provided with suggested translations. Our model can be explicitly trained to copy the annotation approximately 90% of the time, and it outperformed the baselines on translation accuracy of rare words, reaching up to 97% accuracy. Also importantly, this performance is maintained when translating data in a different domain. Further analysis was done to verify the potential of our proposed framework.\n\n\nBackground -Neural Machine Translation\n\nNeural machine translation (NMT) consists of an encoder and a decoder Vaswani et al., 2017) that directly approximate the conditional probability of a target sequence Y = y 1 , y 2 , \u00b7 \u00b7 \u00b7 , y T given a source sequence X = x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x M . The model is normally trained to maximize the log-likelihood of each target token given the previous words as well as the source sequence with respect to model parameters \u03b8 as in Equation 1: log P (Y |X; \u03b8) = \u03a3 T t=1 (log P (y t |X, y 1 , y 2 , \u00b7 \u00b7 \u00b7 , y t \u2212 1))\n\n(1)\n\nThe advantages of NMT compared to phrasedbased machine translation come from the neural architecture components:\n\n\u2022 The embedding layers, which are shared between samples, allow the model to continuously represent discrete words and effectively capture word relationship (Bengio et al., 2003;Mikolov et al., 2013). Notably we refer to two different embedding layers being used in most models, one for the first input layer of the encoder/decoder, and another one at the decoder output layer that is used to compute the probability distribution (Equation 1). Figure 1: A generic illustration of our framework. The source sentence is annotated with experts before learning. The model learns to utilize the annotation by using them directly in the translation)\n\n\u2022 Complex neural architectures like LSTMs (Hochreiter and Schmidhuber, 1997) or Transformers (Vaswani et al., 2017) can represent structural sequences (sentences, phrases) effectively.\n\n\u2022 Attention models (Bahdanau et al., 2014;Luong et al., 2015) are capable of hierarchically modeling the translation mapping between sentence pairs.\n\nThe challenges of NMT These models are often attacked over their inability to learn to translate rare events, which are often named entities and rare morphological variants (Arthur et al., 2016;Koehn and Knowles, 2017;Nguyen and Chiang, 2017). Learning from rare events is difficult due to the fact that the model parameters are not adequately updated. For example, the embeddings of the rare words are only updated a few times during training, and similarly for the patterns learned by the recurrent structures in the encoders / decoders and attention models.\n\n\nExpert framework description\n\nHuman translators can benefit from external knowledge such as dictionaries, particularly in specific domains. Similarly, the idea behind our framework is to rely on external models to annotate extra input into the source side of the training data, which we refer as Experts. Such expert models would not necessarily outperform NMT models themselves, but rather complement them and compensate for their weaknesses.\n\nThe illustration of the proposed framework is given in Figure 1. Before the learning process, the source sentence is annotated by one or several expert models, which we abstract as any model that can show additional data perspectives. For example, these experts could be a terminology list or a statistical phrase-based system to generate translations for specific phrases, but it can also be used in various other situations. For example, we might use it to integrate a model that can do metric conversion or handling of links to web addresses, which can be useful for certain applications. Then NMT model then learns to translate to the target sentence using the annotated source.\n\n\nAnnotation\n\nThe aforementioned idea of Experts in our work is inspired by the fact that human translators can benefit from domain experts when translating domainspecific content. Accordingly, we design the annotation and training process as follows:\n\n\u2022 Words are identified as candidates for annotation using a frequency threshold.\n\n\u2022 Look up possible translations of the candidates from the Expert and annotate them directly next to the candidates. We use special bounding symbols to help guide the model to copy the annotation during translation.\n\n\u2022 Train a neural machine translation model using these annotated sentences.\n\n\u2022 During inference, we annotate the source sentence in the same fashion as in training.\n\nByte-Pair encoding We consider BPE (Sennrich et al., 2016) one of the crucial factors for annotation in order to efficiently represent words that do not appear in the training data. The rare words (and their translation suggestions, which can be rare as well) are split into smaller segments, alleviating the problem of dealing with U N K tokens (Luong et al., 2014).\n\nEmbedding sharing Our annotation method includes target language tokens directly in the source sentence. In order to make the model perceive these words the same way in the source and the target, we create a joint vocabulary of the source and target language and simply tie the embedding projection matrices of the source encoder, target encoder and target decoder. This practice has been explored in various language modeling works (Press and Wolf, 2016;Inan et al., 2016) to improve regularisation.\n\n\nCopy-Generator\n\nHypothetically, the model could learn to simply ignore the annotation during optimization because it contains strange symbols (the target language) in source language sentences. If this were the case, adding annotations would not help translate rare events. Therefore, inspired by the CopyNet (Gu et al., 2016;Gulcehre et al., 2016), which originates from pointer networks (Vinyals et al., 2015) that learn to pick the tokens that appeared in the memory of the models, we incorporate the copymechanism into the neural translation model so that the annotations can be simply pasted into the translation. Explicitly, the conditional probability is now presented as a mixture of two distributions: copy and generated.\nP (Y |X; \u03b8) = \u03a3 T t=1 [\u03b3P G (y t |X, y 1 , y 2 , \u00b7 \u00b7 \u00b7 , y t \u2212 1) +(1 \u2212 \u03b3)P C (y t |X, y 1 , y 2 , \u00b7 \u00b7 \u00b7 , y t \u2212 1)](2)\nThe distribution over the whole vocabulary P G is estimated from the softmax layer using equation 1, and the copy distribution P C is used from the attention layer from the decoder state over the context (dubbed 'alignment' in previous works (Bahdanau et al., 2014)). The mixture coefficient \u03b3 controls the bias between the mixtures and is estimated using a feed-forward neural network layer with a sigmoid function, which is placed on top of the decoder hidden state (before the final output softmax layer 1 ). Ideally, the model learns to adjust between copying the input annotation or generating a translation.\n\nIt is important to note that, in previous works the authors had to build dynamic vocabulary for each sample due to the vocabulary mismatch between the source and target (Gu et al., 2016). Since we tied the embeddings of source and target languages, it becomes trivial to combine the two distributions. The use of byte-pair encodings also helps to eliminate unknown words on both sides, alleviating the task of excluding copying unknown tokens.\n\n\nReinforcement Learning\n\nWhy reinforcement learning While our annotation provides target language tokens that can be directly copied to the generated output, and the copy generator allows a direct gradient path from the output to the annotation, the annotation is not guaranteed to be in the reference. When this is the case, the model does not receive the learning signal to copy the annotation.\n\nIn order to remedy this, we propose to cast the problem as a reinforcement learning task (Ranzato et al., 2015) in which we have the model sample and provide a learning signal by rewarding the model if it copies the annotation into the target, as seen in the loss function in Equation 3:\nL(\u03b8) = \u2212E W \u223cp \u03b8 (r(W, REF ))(3)\n.\n\nReward function For this purpose, we designed a reward function that can encourage the model to prioritize copying the annotation into the target, but still maintain a reasonable translation quality.\n\nFor suggestion utilization, we denote HIT as the score function that gives rewards for every overlap of the output and the suggestion. If all annotated words are used then HIT (W, REF ) = 1.0, otherwise the percentage of the copied words. For the translation score, we use the GLEU function (Wu et al., 2016) -the minimum of recall and precision of the n-grams up to 4-gram between the sample and the reference, which has been reported to correspond well with corpus-level translation metrics such as BLEU (Papineni et al., 2002). The reward function is defined as in Equation 4:\nr(W, REF ) = \u03b1HIT (W, REF )+ (1 \u2212 \u03b1)GLEU (W, REF )(4)\nVariance reduction The use of reinforcement learning with translation models has been explored in various works (Ranzato et al., 2015;Bahdanau et al., 2016;Rennie et al., 2016;, in which the models are difficult to train due to the high variance of the gradients (Schulman et al., 2017). To tackle this problem, we follow the Self-Critical model proposed by (Rennie et al., 2016) for variance reduction:\n\n\u2022 Pre-training the model using cross-entropy loss (Eq. 1) to obtain a solid initialization presearch, which allows the model to achieve reasonable rewards to learn faster.\n\n\u2022 During the reinforcement phase, for each sample/mini-batch, the decoder explores the search space with Markov chain Monte Carlo sampling, and at the same time performs a greedy search for a 'baseline' performance. We encourage the model to perform better than baseline, which is used to decide the sign of the gradients (Williams, 1992).\n\nNotably, there is no gradient flowing in the baseline subgraph since the argmax operators used in the greedy search are not differentiable.\n\n\nExperiment setup\n\nIn the experiments, we realise the generic framework described in Section 3 with the tasks of translating from English\u2192Spanish and German\u2192English.\n\nFor both language pairs, we used data from Europarl (version 7) (Koehn, 2005) and IWSLT17 (Cettolo et al., 2012) to train our neural networks. For validation, we use the IWSLT validation set (dev2010) to select the best models based on perplexity (for cross-entropy loss) and BLEU score (for reinforcement learning). For evaluation, we use IWSLT tst2010 as the indomain test set. We also evaluate our models on out-of-domain corpora. For English\u2192Spanish an additional Business dataset is used. The corpus statistics can be seen on Table 1. The out-ofdomain experiments for the German\u2192English are carried out on the medical domain, in which we use the UFAL Medical Corpus v1.0 corpus (2.2 million sentences) to train the Expert and the Oracle system. The test data for this task is the HIML2017 dataset with 1517 sentences. We preprocess all the data using standard tokenization, true-casing and BPE splitting with 40K joined operations.\n\n\nImplementation details\n\nOur base neural machine translation follows the neural machine translation with global attention model described in (Luong et al., 2015) 2 . The encoder is a bidirectional LSTM network, while the decoder is an LSTM with attention, which is a 2-layer feed-forward neural network (Bahdanau et al., 2014). We also use the input-feeding method (Luong et al., 2015) and context-gate (Tu et al., 2016) to improve model coverage. All networks in our experiments have layer size (embedding and hidden) of 512 (English\u2192Spanish) and 1024 (German\u2192English) with 2 LSTM layers. Dropout is put vertically between LSTM layers to improve regularization (Pham et al., 2014). We create mini-batches with maximum 128 sentence pairs of the same source size. For crossentropy training, the parameters are optimized using Adam (Kingma and Ba, 2014) with a learning rate annealing schedule suggested in (Denkowski and Neubig, 2017), starting from 0.001 until 0.00025. After reaching convergence on the training data, we fine-tune the models on the IWSLT training set with learning rate of 0.0002. Finally, we use our best models on the validation data as the initialization for reinforcement learning using a learning rate of 0.0001, which is done on the IWSLT set for 50 epochs. Beam search is used for decoding.\n\n\nPhrase-based Experts\n\nWe selected phrase tables for the Experts in our experiments. While other resources like terminology lists can also be used for the translation annotations, our motivation here is that the phrasetables can additionally capture multi-word phrase pairs, and additionally can better capture the distribution tail of rare phrases as compared to neural models (Koehn and Knowles, 2017). We selected the translation with the highest average probabilities in the 4 phrase table scores for annotation.\n\nOn the English\u2192Spanish task, the phrase tables are trained on the same data as the NMT model, while on the German\u2192English direction, we simulate the situation when the expert is not in the same domain as the test data to observe the potentials. Therefore, we train an additional table on the UFAL Medical Corpus v.1.0 corpus (which is not observed by the NMT model) to for the outof-domain annotation.\n\n\nEvaluation\n\n\nResearch questions\n\nWe aim to find the answers to the following research questions:\n\n\u2022 Given the annotation quality being imperfect, how much does it affect the overall translation quality?\n\n\u2022 How much does annotation participate in translating rare words, and how consistently can the model learn to copy the annotation?\n\n\u2022 How will the model perform in a new domain? The copy mechanism does not depend on the domain of the training or adaptation data, which is optimal.\n\n\nEvaluation Metrics\n\nTo serve the research questions above, we use the following evaluation metrics:\n\n\u2022 BLEU: score for general translation quality.\n\n\u2022 SUGGESTION (SUG): The overlap between the hypothesis and the phrase-table (on word level), showing how much the expert content is used by the model.\n\n\u2022 SUGGESTION ACCURACY (SAC): The intersection between the hypothesis, the phrase-table suggestions and the reference. This metrics shows us the accuracy of the system on the rare-words which are suggested by the phrase-table.\n\nDiscussion The SUG metric shows the consistency of the model on the copy mechanism. Models with lower SUG are not necessarily worse, and models with high SUG can potentially have very low recall on rare-word translation by systematically copying bad suggestions and failing to translate rare-words where the annotator is incorrect. However, we argue that a high SUG system can be used reliably with a high quality expert. For example, in censorship management or name translation which is strictly sensitive, this quality can help reducing output inconsistency. On the other hand, the SAC metrics show improvement on rareword translation, but only on the intersection of the phrase table and the reference. This subset is our main focus. General rare-word translation quality requires additional effort to find the reference aligned to the rare words in the source sentences, which we consider for future work.\n\n\nExperimental results\n\nEnglish\u2192Spanish Results for this task are presented on table 2. First, the main difference between the settings is the SUG and SAC figures for all test sets. Both of them increase dramatically from baseline to annotation, and also increase according to the level of supervision in our model proposals. While the copy mechanism can help us to copy more from the annotation, the REINFORCE models are successfully trained to   Table 3: The results of German\u2192English on various domains: TEDTalks and Biomedical. We use AN for using annotations from the phrase table, RF for using REINFORCE (\u03b1= 0.5) and CP for using the Copy mechanism.\n\nmake the model copy more consistently. Their combination helps us achieve the desired behavior, in which almost all of the annotations given are copied, and we achieve 100% accuracy on the rare-words section that the phrase table covers. As mentioned in the discussion above, the SAC and SUG figures, while being not enough to quantitatively prove that the total number of rare words translated, show that the phrase table is complementary to the neural machine translation, and the more coverage the expert has, the more benefit this method can bring.\n\nWe notice an improvement of 1 BLEU point on dev2010 but only slight changes compared to the baseline on tst2010. On the out-of-domain set, however, the improved rare-word performance leads to an increase of 1.7 BLEU points over the baseline without annotation. Our models, despite training on a noisier dataset, are able to improve translation quality.\n\nGerman\u2192English Results are shown in Table 3. On the dev2010 and tst2010 in-domain datasets, we observe similar phenomena to the En-Es direction. Rare-word performance increases with the number of words copied, and the combination of the copy mechanism and REINFORCE help us copy consistently. Surprisingly, however, the BLEU score drops with annotations. This may be because of the relative morphologically complexity of the German words compared to the English, making it harder to generate the correct word form.\n\nIn the experiments with an out-of-domain test set (HIML), we use annotations from that domain to simulate a domain-expert. For comparison, we also trained an NMT model adapted to the UFAL corpus, which we call the Oracle model. In this domain, our models show the same behavior, in which almost every word annotated is copied to the output. The annotation efficiently improves translation quality by 1.7 BLEU points over the baseline without annotation. The adapted model has a higher BLEU score, but here performs worse than our annotated model in terms of phrase-table overlap and rare-word translation accuracy for words in this set. Our model shows significantly better rare word handling than the baseline. Though the best obtainable system is adapted to the in-domain data, this requires parallel text: this experiment shows the high potential to improve NMT on out-of-domain scenarios using only lexical-level materials. We notice a surprising drop of 1.0 BLEU points for the RE-INFORCE model. Possible reasons include inefficient beam search on REINFORCE models, or the GLEU signal was out-weighted by the HIT one during training, which is known for the difficulty (Zaremba and Sutskever, 2015).\n\n\nFurther Analysis\n\nName translation Names can often be translated by BPE, but it is noticeable about examples of the inconsistency, which can be alleviated using annotations, as illustrated in Figure 2-Top.\n\nCopying long phrases We find that with very high supervision, the model can learn to copy even phrases completely into the output, as in Figure 2-Bottom. Though this is potentially dangerous, as the output may the lose the additional fluency which comes from NMT, it is controllable by combining RL and cross entropy loss (Paulus et al., 2017).\n\nAttention Plotting the attention map for the decoded sequences we notice that, while we marked the beginning and end of annotated sections and the separation between the source and the suggestion with # and ## tokens, those positions received very little weight from the decoder. One possible explanation is that these tokens do not contribute to the translation when decoding, and the annotations may useful without bounding tags. For the annotations used in the translation, we identified two prominent cases; for the rare words whose annotation need only be identically copied to the target, the attention map focuses evenly on both source and annotation, while the heat map typically heavily emphasizes only the annotation otherwise. An example is illustrated in figure 3.\n\nEffect of \u03b1 The full results with respect to different \u03b1 values which are used in Equation 3 for reward weighting can be seen in Table 4. Higher \u03b1 values emphasize the signal to copy the source annotation, as can be seen from the increase in terms of Accuracy and Suggestion utilization across the values. As expected, as \u03b1 goes toward 1.0, the model gradually loses the signal needed to maintain translation quality and finally diverges.  \n\n\nRelated Work\n\nTranslating rare words in neural machine translation is a rich and active topic, particularly when translating morphologically rich languages or translating named entities. Sub-word unit decomposition or BPE (Sennrich et al., 2016) has become the de-facto standard in most neural translation systems (Wu et al., 2016). Using phrase tables to handle rare words was previously explored in (Luong et al., 2014), but was not compatible with BPE. (Gulcehre et al., 2016) explored using pointer networks to copy source words to the translation output, which could benefit from our design but would require significant changes to the architecture and likely be limited to copying only. Additionally, models that can learn to remember rare events were explored in . Our work builds on the idea of using a phrasebased neural machine translation to augment source data, (Niehues et al., 2016;Denkowski and Neubig, 2017), but can be extended to any annotation type without complicated hybrid phrasebased neural machine translation systems. We were additionally inspired by the use of feature functions with lexical-based features from dictio- Figure 2: Top: Examples of name annotations with our framework from tst2010. The name Kean is originally split by BPE into 'K' and 'ean'. This is incorrectly translated without annotation (in blue) and corrected with the annotation (in red). Bottom: An example of phrase copying, in which the German word is translated into a long English phrase. naries and phrase-tables in (Zhang et al., 2017). They also rely on sample-based techniques, (Shen et al., 2015), to train their networks, but their computation is more expensive than the self-critical network in our work. We focus here on rare events, with the possibility to construct interactive models for fast updating without retraining. We also use the ideas of using REINFORCE to train sequence generators for arbitrary rewards (Ranzato et al., 2015;Bahdanau et al., 2016). While this method remains difficult to train, it is promising to use to achieve non-probabilistic features for neural models: for example enforcing formality in outputs in German, or censoring undesired outputs.\n\n\nConclusion\n\nIn this work, we presented a framework to alleviate the weaknesses of neural machine transla-tion models by incorporating external knowledge as Experts and training the models to use their annotations using reinforcement learning and a pointer network. We show improvements over the unannotated model on both in-and out-ofdomain datasets. When only lexical resources are available and in-domain fine-tuning cannot be performed, our framework can improve performance. The annotator might potentially be trained together with the main model to balance translation quality with copying annotations, which our current framework seems to be biased to.\n\nFigure 3 :\n3An attention heat map of an English-Spanish sentence pair (source on X-axis, target on Y-axis) with annotated sections in red rectangles. Annotations and their source are bounded by # characters.\n\nTable 2 :\n2The results of English -Spanish on various domains: TEDTalks and Business. We use AN for \nusing annotations from the phrase table, RF for using REINFORCE (\u03b1= 0.5) and CP for using the Copy \nmechanism. \n\nSystem-Data \ndev2010 \ntst2010 \nHIML \nBLEU SAC SUG BLEU SAC SUG BLEU SAC SUG \n\n1. Baseline \n37.5 \n66 \n45 \n36.14 66.9 45.1 \n32.4 \n46.3 \n37.2 \n2. + AN \n37.1 \n93 \n84.1 \n35.6 \n91.9 84.4 33.99 \n87.1 \n85.1 \n3. + AN-CP \n37.2 \n96 \n88.2 35.89 94.1 90.7 \n34.1 \n96.5 \n95.0 \n4. + AN-CP-RF \n36.6 \n97 \n92.9 35.89 98.5 95.5 \n33.1 \n98.0 \n97.6 \nBiomedical-Oracle \n-\n-\n-\n-\n-\n-\n37.82 81.77 65.44 \n\n\n\nTable 4 :\n4Performances w.r.t to different alpha values. Metrics shown are BLEU (BL), ACCURACY (AC) and SUGGESTION (SUG)\nUsing an additional attention layer yields similar result.\nThe framework is implemented in PyTorch and can be found at https://github.com/quanpn90/OpenNMT-py\nProceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1. Association for Computational Linguistics, pages 48-54.\nAcknowledgmentsThis work was supported by the Carl-Zeiss-Stiftung. We thank Elizabeth Salesky for the constructive comments.\nIncorporating discrete translation lexicons into neural machine translation. Philip Arthur, Graham Neubig, Satoshi Nakamura, arXiv:1606.02006arXiv preprintPhilip Arthur, Graham Neubig, and Satoshi Naka- mura. 2016. Incorporating discrete translation lexi- cons into neural machine translation. arXiv preprint arXiv:1606.02006 .\n\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, CoRR abs/1409.0473D. Bahdanau, K. Cho, and Y. Bengio. 2014. Neural machine translation by jointly learning to align and translate. CoRR abs/1409.0473.\n\nAn actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, arXiv:1607.07086arXiv preprintDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2016. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086 .\n\nA neural probabilistic language model. Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, Christian Jauvin, Journal of machine learning research. 3Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic lan- guage model. Journal of machine learning research 3(Feb):1137-1155.\n\nWit: Web inventory of transcribed and translated talks. M Cettolo, C Girardi, M Federico, Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT). the 16th Conference of the European Association for Machine Translation (EAMT)M. Cettolo, C. Girardi, and M. Federico. 2012. Wit: Web inventory of transcribed and translated talks. In Proceedings of the 16th Conference of the Euro- pean Association for Machine Translation (EAMT).\n\nMichael Denkowski, Graham Neubig, arXiv:1706.09733Stronger baselines for trustable results in neural machine translation. arXiv preprintMichael Denkowski and Graham Neubig. 2017. Stronger baselines for trustable results in neural ma- chine translation. arXiv preprint arXiv:1706.09733 .\n\nIncorporating copying mechanism in sequence-to-sequence learning. Jiatao Gu, Zhengdong Lu, Hang Li, O K Victor, Li, arXiv:1603.06393arXiv preprintJiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. arXiv preprint arXiv:1603.06393 .\n\nCaglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, Yoshua Bengio, arXiv:1603.08148Pointing the unknown words. arXiv preprintCaglar Gulcehre, Sungjin Ahn, Ramesh Nallap- ati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. arXiv preprint arXiv:1603.08148 .\n\nLong shortterm memory. S Hochreiter, J Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Comput. 98S. Hochreiter and J. Schmidhuber. 1997. Long short- term memory. Neural Comput. 9(8):1735-1780. https://doi.org/10.1162/neco.1997.9.8.1735.\n\nTying word vectors and word classifiers: A loss framework for language modeling. Khashayar Hakan Inan, Richard Khosravi, Socher, arXiv:1611.01462arXiv preprintHakan Inan, Khashayar Khosravi, and Richard Socher. 2016. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462 .\n\nLearning to remember rare events. Lukasz Kaiser, Ofir Nachum, Aurko Roy, Samy Bengio, CoRR abs/1703.03129Lukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. 2017. Learning to remem- ber rare events. CoRR abs/1703.03129. http://arxiv.org/abs/1703.03129.\n\nRecurrent continuous translation models. Nal Kalchbrenner, Phil Blunsom, In EMNLP. 3413Nal Kalchbrenner and Phil Blunsom. 2013. Recur- rent continuous translation models. In EMNLP. vol- ume 3, page 413.\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, arXiv:1412.6980arXiv preprintDiederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 .\n\nEuroparl: A parallel corpus for statistical machine translation. Philipp Koehn, MT summit. 5Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit. vol- ume 5, pages 79-86.\n\nSix challenges for neural machine translation. Philipp Koehn, Rebecca Knowles, arXiv:1706.03872arXiv preprintPhilipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. arXiv preprint arXiv:1706.03872 .\n\nStanford neural machine translation systems for spoken language domains. Philipp Koehn, Franz Josef Och, Daniel Marcu, Proceedings of the International Workshop on Spoken Language Translation. Minh-Thang Luong and Christopher D Manningthe International Workshop on Spoken Language TranslationStatistical phrase-based translationPhilipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Minh-Thang Luong and Christopher D Manning. 2015. Stanford neural machine translation systems for spo- ken language domains. In Proceedings of the In- ternational Workshop on Spoken Language Transla- tion. pages 76-79.\n\nEffective approaches to attentionbased neural machine translation. Minh-Thang Luong, Hieu Pham, Christopher D Manning, arXiv:1508.04025arXiv preprintMinh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025 .\n\nAddressing the rare word problem in neural machine translation. Minh-Thang Luong, Ilya Sutskever, V Quoc, Oriol Le, Wojciech Vinyals, Zaremba, arXiv:1410.8206arXiv preprintMinh-Thang Luong, Ilya Sutskever, Quoc V Le, Oriol Vinyals, and Wojciech Zaremba. 2014. Addressing the rare word problem in neural machine translation. arXiv preprint arXiv:1410.8206 .\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems. pages 3111-3119.\n\nReinforcement learning for bandit neural machine translation with simulated human feedback. Khanh Nguyen, Hal Daum\u00e9, Iii , Jordan Boyd-Graber, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingKhanh Nguyen, Hal Daum\u00e9 III, and Jordan Boyd- Graber. 2017. Reinforcement learning for bandit neural machine translation with simulated human feedback. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing. pages 1465-1475.\n\nImproving lexical choice in neural machine translation. Q Toan, David Nguyen, Chiang, arXiv:1710.01329arXiv preprintToan Q Nguyen and David Chiang. 2017. Improving lexical choice in neural machine translation. arXiv preprint arXiv:1710.01329 .\n\nPre-translation for neural machine translation. Jan Niehues, Eunah Cho, Thanh-Le Ha, Alex Waibel, arXiv:1610.05243arXiv preprintJan Niehues, Eunah Cho, Thanh-Le Ha, and Alex Waibel. 2016. Pre-translation for neural machine translation. arXiv preprint arXiv:1610.05243 .\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting on association for compu- tational linguistics. Association for Computational Linguistics, pages 311-318.\n\nA deep reinforced model for abstractive summarization. Romain Paulus, Caiming Xiong, Richard Socher, arXiv:1705.04304arXiv preprintRomain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive sum- marization. arXiv preprint arXiv:1705.04304 .\n\nDropout improves recurrent neural networks for handwriting recognition. Vu Pham, Th\u00e9odore Bluche, Christopher Kermorvant, J\u00e9r\u00f4me Louradour, Frontiers in Handwriting Recognition (ICFHR). 14th International Conference on. IEEEVu Pham, Th\u00e9odore Bluche, Christopher Kermorvant, and J\u00e9r\u00f4me Louradour. 2014. Dropout improves re- current neural networks for handwriting recognition. In Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on. IEEE, pages 285-290.\n\nUsing the output embedding to improve language models. Ofir Press, Lior Wolf, arXiv:1608.05859arXiv preprintOfir Press and Lior Wolf. 2016. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859 .\n\nAurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level train- ing with recurrent neural networks. arXiv preprint arXiv:1511.06732 .\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jarret Mroueh, Vaibhava Ross, Goel, arXiv:1612.00563arXiv preprintSteven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. 2016. Self-critical sequence training for image captioning. arXiv preprint arXiv:1612.00563 .\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proxi- mal policy optimization algorithms. arXiv preprint arXiv:1707.06347 .\n\nNeural machine translation of rare words with subword units. R Sennrich, B Haddow, A Birch, Proceedings of the 54st Annual Meeting of the Association for Computational Linguistics. the 54st Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyR. Sennrich, B. Haddow, and A. Birch. 2016. Neu- ral machine translation of rare words with subword units. In Proceedings of the 54st Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, Yang Liu, arXiv:1512.02433Minimum risk training for neural machine translation. arXiv preprintShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2015. Minimum risk training for neural machine translation. arXiv preprint arXiv:1512.02433 .\n\nSequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Quebec, CanadaI. Sutskever, O. Vinyals, and Q. V. Le. 2014. Se- quence to sequence learning with neural networks. In Advances in Neural Information Processing Sys- tems 27: Annual Conference on Neural Information Processing Systems 2014. Quebec, Canada, pages 3104-3112.\n\nModeling coverage for neural machine translation. Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, Hang Li, arXiv:1601.04811arXiv preprintZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Modeling coverage for neural machine translation. arXiv preprint arXiv:1601.04811 .\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv:1706.03762Attention is all you need. arXiv preprintAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762 .\n\nPointer networks. Oriol Vinyals, Meire Fortunato, Navdeep Jaitly, Advances in Neural Information Processing Systems. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in Neural In- formation Processing Systems. pages 2692-2700.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. J Ronald, Williams, Machine learning. 83-4Ronald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine learning 8(3-4):229-256.\n\nGoogle's neural machine translation system. Yonghui Wu, Mike Schuster, Zhifeng Chen, V Quoc, Mohammad Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.08144Bridging the gap between human and machine translation. arXiv preprintYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural ma- chine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144 .\n\nWojciech Zaremba, Ilya Sutskever, arXiv:1505.00521Reinforcement learning neural turing machines-revised. arXiv preprintWojciech Zaremba and Ilya Sutskever. 2015. Rein- forcement learning neural turing machines-revised. arXiv preprint arXiv:1505.00521 .\n\nPrior knowledge integration for neural machine translation using posterior regularization pages. Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu, Maosong Sun, Jiacheng Zhang, Yang Liu, Huanbo Luan, Jingfang Xu, and Maosong Sun. 2017. Prior knowledge integra- tion for neural machine translation using posterior regularization pages 1514-1523.\n", "annotations": {"author": "[{\"end\":128,\"start\":77},{\"end\":216,\"start\":129},{\"end\":265,\"start\":217}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":87},{\"end\":140,\"start\":133},{\"end\":228,\"start\":222}]", "author_first_name": "[{\"end\":86,\"start\":77},{\"end\":132,\"start\":129},{\"end\":221,\"start\":217}]", "author_affiliation": "[{\"end\":127,\"start\":93},{\"end\":215,\"start\":181},{\"end\":264,\"start\":230}]", "title": "[{\"end\":74,\"start\":1},{\"end\":339,\"start\":266}]", "venue": null, "abstract": "[{\"end\":1158,\"start\":341}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1297,\"start\":1277},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1318,\"start\":1297},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1498,\"start\":1466},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1593,\"start\":1572},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1679,\"start\":1659},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1810,\"start\":1787},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2074,\"start\":2049},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2130,\"start\":2113},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2309,\"start\":2292},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2694,\"start\":2669},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3223,\"start\":3206},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3276,\"start\":3254},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4870,\"start\":4849},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5589,\"start\":5568},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5610,\"start\":5589},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6132,\"start\":6098},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6171,\"start\":6149},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6284,\"start\":6261},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6303,\"start\":6284},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6586,\"start\":6565},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6610,\"start\":6586},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6634,\"start\":6610},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9167,\"start\":9147},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9625,\"start\":9603},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9643,\"start\":9625},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9999,\"start\":9982},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10021,\"start\":9999},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10084,\"start\":10062},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10789,\"start\":10766},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11325,\"start\":11308},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12093,\"start\":12071},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12814,\"start\":12798},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13036,\"start\":13013},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13275,\"start\":13253},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13297,\"start\":13275},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13317,\"start\":13297},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13427,\"start\":13404},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13520,\"start\":13499},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14057,\"start\":14041},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14445,\"start\":14432},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14480,\"start\":14458},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15467,\"start\":15447},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15632,\"start\":15609},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15691,\"start\":15671},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15726,\"start\":15709},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15987,\"start\":15968},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16156,\"start\":16136},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16239,\"start\":16211},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17026,\"start\":17001},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22754,\"start\":22725},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23308,\"start\":23287},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24777,\"start\":24754},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24863,\"start\":24846},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24953,\"start\":24933},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25011,\"start\":24988},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25428,\"start\":25406},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25455,\"start\":25428},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26073,\"start\":26053},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26137,\"start\":26118},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26483,\"start\":26461},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26505,\"start\":26483}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":27588,\"start\":27380},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28182,\"start\":27589},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28304,\"start\":28183}]", "paragraph": "[{\"end\":1811,\"start\":1174},{\"end\":3954,\"start\":1813},{\"end\":4194,\"start\":3956},{\"end\":4736,\"start\":4196},{\"end\":5290,\"start\":4779},{\"end\":5295,\"start\":5292},{\"end\":5409,\"start\":5297},{\"end\":6054,\"start\":5411},{\"end\":6240,\"start\":6056},{\"end\":6390,\"start\":6242},{\"end\":6952,\"start\":6392},{\"end\":7398,\"start\":6985},{\"end\":8082,\"start\":7400},{\"end\":8334,\"start\":8097},{\"end\":8416,\"start\":8336},{\"end\":8633,\"start\":8418},{\"end\":8710,\"start\":8635},{\"end\":8799,\"start\":8712},{\"end\":9168,\"start\":8801},{\"end\":9670,\"start\":9170},{\"end\":10403,\"start\":9689},{\"end\":11137,\"start\":10524},{\"end\":11582,\"start\":11139},{\"end\":11980,\"start\":11609},{\"end\":12269,\"start\":11982},{\"end\":12304,\"start\":12303},{\"end\":12505,\"start\":12306},{\"end\":13086,\"start\":12507},{\"end\":13544,\"start\":13141},{\"end\":13717,\"start\":13546},{\"end\":14058,\"start\":13719},{\"end\":14199,\"start\":14060},{\"end\":14366,\"start\":14220},{\"end\":15304,\"start\":14368},{\"end\":16621,\"start\":15331},{\"end\":17139,\"start\":16646},{\"end\":17542,\"start\":17141},{\"end\":17641,\"start\":17578},{\"end\":17747,\"start\":17643},{\"end\":17879,\"start\":17749},{\"end\":18029,\"start\":17881},{\"end\":18131,\"start\":18052},{\"end\":18179,\"start\":18133},{\"end\":18331,\"start\":18181},{\"end\":18558,\"start\":18333},{\"end\":19470,\"start\":18560},{\"end\":20126,\"start\":19495},{\"end\":20680,\"start\":20128},{\"end\":21034,\"start\":20682},{\"end\":21550,\"start\":21036},{\"end\":22755,\"start\":21552},{\"end\":22963,\"start\":22776},{\"end\":23309,\"start\":22965},{\"end\":24087,\"start\":23311},{\"end\":24529,\"start\":24089},{\"end\":26718,\"start\":24546},{\"end\":27379,\"start\":26733}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10523,\"start\":10404},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12302,\"start\":12270},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13140,\"start\":13087}]", "table_ref": "[{\"end\":14906,\"start\":14899},{\"end\":19926,\"start\":19919},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24225,\"start\":24218}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1172,\"start\":1160},{\"attributes\":{\"n\":\"2\"},\"end\":4777,\"start\":4739},{\"attributes\":{\"n\":\"3\"},\"end\":6983,\"start\":6955},{\"attributes\":{\"n\":\"3.1\"},\"end\":8095,\"start\":8085},{\"attributes\":{\"n\":\"3.2\"},\"end\":9687,\"start\":9673},{\"attributes\":{\"n\":\"3.3\"},\"end\":11607,\"start\":11585},{\"attributes\":{\"n\":\"4\"},\"end\":14218,\"start\":14202},{\"attributes\":{\"n\":\"4.1\"},\"end\":15329,\"start\":15307},{\"attributes\":{\"n\":\"4.2\"},\"end\":16644,\"start\":16624},{\"attributes\":{\"n\":\"5\"},\"end\":17555,\"start\":17545},{\"attributes\":{\"n\":\"5.1\"},\"end\":17576,\"start\":17558},{\"attributes\":{\"n\":\"5.2\"},\"end\":18050,\"start\":18032},{\"attributes\":{\"n\":\"5.3\"},\"end\":19493,\"start\":19473},{\"attributes\":{\"n\":\"5.4\"},\"end\":22774,\"start\":22758},{\"attributes\":{\"n\":\"6\"},\"end\":24544,\"start\":24532},{\"attributes\":{\"n\":\"7\"},\"end\":26731,\"start\":26721},{\"end\":27391,\"start\":27381},{\"end\":27599,\"start\":27590},{\"end\":28193,\"start\":28184}]", "table": "[{\"end\":28182,\"start\":27601}]", "figure_caption": "[{\"end\":27588,\"start\":27393},{\"end\":28304,\"start\":28195}]", "figure_ref": "[{\"end\":5863,\"start\":5855},{\"end\":7463,\"start\":7455},{\"end\":22958,\"start\":22950},{\"end\":23110,\"start\":23102},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24086,\"start\":24078},{\"end\":25686,\"start\":25678}]", "bib_author_first_name": "[{\"end\":28880,\"start\":28874},{\"end\":28895,\"start\":28889},{\"end\":28911,\"start\":28904},{\"end\":29198,\"start\":29197},{\"end\":29210,\"start\":29209},{\"end\":29217,\"start\":29216},{\"end\":29436,\"start\":29429},{\"end\":29455,\"start\":29447},{\"end\":29470,\"start\":29464},{\"end\":29482,\"start\":29475},{\"end\":29494,\"start\":29490},{\"end\":29507,\"start\":29501},{\"end\":29521,\"start\":29516},{\"end\":29539,\"start\":29533},{\"end\":29838,\"start\":29832},{\"end\":29853,\"start\":29847},{\"end\":29870,\"start\":29864},{\"end\":29889,\"start\":29880},{\"end\":30167,\"start\":30166},{\"end\":30178,\"start\":30177},{\"end\":30189,\"start\":30188},{\"end\":30584,\"start\":30577},{\"end\":30602,\"start\":30596},{\"end\":30937,\"start\":30931},{\"end\":30951,\"start\":30942},{\"end\":30960,\"start\":30956},{\"end\":30966,\"start\":30965},{\"end\":30968,\"start\":30967},{\"end\":31176,\"start\":31170},{\"end\":31194,\"start\":31187},{\"end\":31206,\"start\":31200},{\"end\":31223,\"start\":31218},{\"end\":31236,\"start\":31230},{\"end\":31477,\"start\":31476},{\"end\":31491,\"start\":31490},{\"end\":31779,\"start\":31770},{\"end\":31799,\"start\":31792},{\"end\":32062,\"start\":32056},{\"end\":32075,\"start\":32071},{\"end\":32089,\"start\":32084},{\"end\":32099,\"start\":32095},{\"end\":32324,\"start\":32321},{\"end\":32343,\"start\":32339},{\"end\":32536,\"start\":32528},{\"end\":32550,\"start\":32545},{\"end\":32770,\"start\":32763},{\"end\":32970,\"start\":32963},{\"end\":32985,\"start\":32978},{\"end\":33228,\"start\":33221},{\"end\":33241,\"start\":33236},{\"end\":33247,\"start\":33242},{\"end\":33259,\"start\":33253},{\"end\":33870,\"start\":33860},{\"end\":33882,\"start\":33878},{\"end\":33902,\"start\":33889},{\"end\":34182,\"start\":34172},{\"end\":34194,\"start\":34190},{\"end\":34207,\"start\":34206},{\"end\":34219,\"start\":34214},{\"end\":34232,\"start\":34224},{\"end\":34548,\"start\":34543},{\"end\":34562,\"start\":34558},{\"end\":34577,\"start\":34574},{\"end\":34588,\"start\":34584},{\"end\":34590,\"start\":34589},{\"end\":34604,\"start\":34600},{\"end\":34992,\"start\":34987},{\"end\":35004,\"start\":35001},{\"end\":35015,\"start\":35012},{\"end\":35024,\"start\":35018},{\"end\":35517,\"start\":35516},{\"end\":35529,\"start\":35524},{\"end\":35756,\"start\":35753},{\"end\":35771,\"start\":35766},{\"end\":35785,\"start\":35777},{\"end\":35794,\"start\":35790},{\"end\":36047,\"start\":36040},{\"end\":36063,\"start\":36058},{\"end\":36076,\"start\":36072},{\"end\":36091,\"start\":36083},{\"end\":36635,\"start\":36629},{\"end\":36651,\"start\":36644},{\"end\":36666,\"start\":36659},{\"end\":36927,\"start\":36925},{\"end\":36942,\"start\":36934},{\"end\":36962,\"start\":36951},{\"end\":36981,\"start\":36975},{\"end\":37398,\"start\":37394},{\"end\":37410,\"start\":37406},{\"end\":37576,\"start\":37569},{\"end\":37588,\"start\":37583},{\"end\":37605,\"start\":37598},{\"end\":37622,\"start\":37614},{\"end\":37950,\"start\":37949},{\"end\":37966,\"start\":37959},{\"end\":37982,\"start\":37975},{\"end\":38000,\"start\":37994},{\"end\":38017,\"start\":38009},{\"end\":38243,\"start\":38239},{\"end\":38259,\"start\":38254},{\"end\":38276,\"start\":38268},{\"end\":38291,\"start\":38287},{\"end\":38305,\"start\":38301},{\"end\":38610,\"start\":38609},{\"end\":38622,\"start\":38621},{\"end\":38632,\"start\":38631},{\"end\":39049,\"start\":39044},{\"end\":39060,\"start\":39056},{\"end\":39076,\"start\":39068},{\"end\":39084,\"start\":39081},{\"end\":39092,\"start\":39089},{\"end\":39104,\"start\":39097},{\"end\":39114,\"start\":39110},{\"end\":39432,\"start\":39431},{\"end\":39445,\"start\":39444},{\"end\":39456,\"start\":39455},{\"end\":39458,\"start\":39457},{\"end\":39907,\"start\":39899},{\"end\":39921,\"start\":39912},{\"end\":39930,\"start\":39926},{\"end\":39943,\"start\":39936},{\"end\":39953,\"start\":39949},{\"end\":40148,\"start\":40142},{\"end\":40162,\"start\":40158},{\"end\":40176,\"start\":40172},{\"end\":40190,\"start\":40185},{\"end\":40207,\"start\":40202},{\"end\":40220,\"start\":40215},{\"end\":40222,\"start\":40221},{\"end\":40236,\"start\":40230},{\"end\":40250,\"start\":40245},{\"end\":40536,\"start\":40531},{\"end\":40551,\"start\":40546},{\"end\":40570,\"start\":40563},{\"end\":40871,\"start\":40870},{\"end\":41116,\"start\":41109},{\"end\":41125,\"start\":41121},{\"end\":41143,\"start\":41136},{\"end\":41151,\"start\":41150},{\"end\":41166,\"start\":41158},{\"end\":41179,\"start\":41171},{\"end\":41194,\"start\":41189},{\"end\":41209,\"start\":41205},{\"end\":41221,\"start\":41218},{\"end\":41232,\"start\":41227},{\"end\":41630,\"start\":41622},{\"end\":41644,\"start\":41640},{\"end\":41981,\"start\":41973},{\"end\":41993,\"start\":41989},{\"end\":42005,\"start\":41999},{\"end\":42020,\"start\":42012},{\"end\":42032,\"start\":42025}]", "bib_author_last_name": "[{\"end\":28887,\"start\":28881},{\"end\":28902,\"start\":28896},{\"end\":28920,\"start\":28912},{\"end\":29207,\"start\":29199},{\"end\":29214,\"start\":29211},{\"end\":29224,\"start\":29218},{\"end\":29445,\"start\":29437},{\"end\":29462,\"start\":29456},{\"end\":29473,\"start\":29471},{\"end\":29488,\"start\":29483},{\"end\":29499,\"start\":29495},{\"end\":29514,\"start\":29508},{\"end\":29531,\"start\":29522},{\"end\":29546,\"start\":29540},{\"end\":29845,\"start\":29839},{\"end\":29862,\"start\":29854},{\"end\":29878,\"start\":29871},{\"end\":29896,\"start\":29890},{\"end\":30175,\"start\":30168},{\"end\":30186,\"start\":30179},{\"end\":30198,\"start\":30190},{\"end\":30594,\"start\":30585},{\"end\":30609,\"start\":30603},{\"end\":30940,\"start\":30938},{\"end\":30954,\"start\":30952},{\"end\":30963,\"start\":30961},{\"end\":30975,\"start\":30969},{\"end\":30979,\"start\":30977},{\"end\":31185,\"start\":31177},{\"end\":31198,\"start\":31195},{\"end\":31216,\"start\":31207},{\"end\":31228,\"start\":31224},{\"end\":31243,\"start\":31237},{\"end\":31488,\"start\":31478},{\"end\":31503,\"start\":31492},{\"end\":31790,\"start\":31780},{\"end\":31808,\"start\":31800},{\"end\":31816,\"start\":31810},{\"end\":32069,\"start\":32063},{\"end\":32082,\"start\":32076},{\"end\":32093,\"start\":32090},{\"end\":32106,\"start\":32100},{\"end\":32337,\"start\":32325},{\"end\":32351,\"start\":32344},{\"end\":32543,\"start\":32537},{\"end\":32553,\"start\":32551},{\"end\":32776,\"start\":32771},{\"end\":32976,\"start\":32971},{\"end\":32993,\"start\":32986},{\"end\":33234,\"start\":33229},{\"end\":33251,\"start\":33248},{\"end\":33265,\"start\":33260},{\"end\":33876,\"start\":33871},{\"end\":33887,\"start\":33883},{\"end\":33910,\"start\":33903},{\"end\":34188,\"start\":34183},{\"end\":34204,\"start\":34195},{\"end\":34212,\"start\":34208},{\"end\":34222,\"start\":34220},{\"end\":34240,\"start\":34233},{\"end\":34249,\"start\":34242},{\"end\":34556,\"start\":34549},{\"end\":34572,\"start\":34563},{\"end\":34582,\"start\":34578},{\"end\":34598,\"start\":34591},{\"end\":34609,\"start\":34605},{\"end\":34999,\"start\":34993},{\"end\":35010,\"start\":35005},{\"end\":35036,\"start\":35025},{\"end\":35522,\"start\":35518},{\"end\":35536,\"start\":35530},{\"end\":35544,\"start\":35538},{\"end\":35764,\"start\":35757},{\"end\":35775,\"start\":35772},{\"end\":35788,\"start\":35786},{\"end\":35801,\"start\":35795},{\"end\":36056,\"start\":36048},{\"end\":36070,\"start\":36064},{\"end\":36081,\"start\":36077},{\"end\":36095,\"start\":36092},{\"end\":36642,\"start\":36636},{\"end\":36657,\"start\":36652},{\"end\":36673,\"start\":36667},{\"end\":36932,\"start\":36928},{\"end\":36949,\"start\":36943},{\"end\":36973,\"start\":36963},{\"end\":36991,\"start\":36982},{\"end\":37404,\"start\":37399},{\"end\":37415,\"start\":37411},{\"end\":37581,\"start\":37577},{\"end\":37596,\"start\":37589},{\"end\":37612,\"start\":37606},{\"end\":37627,\"start\":37623},{\"end\":37636,\"start\":37629},{\"end\":37957,\"start\":37951},{\"end\":37973,\"start\":37967},{\"end\":37992,\"start\":37983},{\"end\":38007,\"start\":38001},{\"end\":38022,\"start\":38018},{\"end\":38028,\"start\":38024},{\"end\":38252,\"start\":38244},{\"end\":38266,\"start\":38260},{\"end\":38285,\"start\":38277},{\"end\":38299,\"start\":38292},{\"end\":38312,\"start\":38306},{\"end\":38619,\"start\":38611},{\"end\":38629,\"start\":38623},{\"end\":38638,\"start\":38633},{\"end\":39054,\"start\":39050},{\"end\":39066,\"start\":39061},{\"end\":39079,\"start\":39077},{\"end\":39087,\"start\":39085},{\"end\":39095,\"start\":39093},{\"end\":39108,\"start\":39105},{\"end\":39118,\"start\":39115},{\"end\":39442,\"start\":39433},{\"end\":39453,\"start\":39446},{\"end\":39461,\"start\":39459},{\"end\":39910,\"start\":39908},{\"end\":39924,\"start\":39922},{\"end\":39934,\"start\":39931},{\"end\":39947,\"start\":39944},{\"end\":39956,\"start\":39954},{\"end\":40156,\"start\":40149},{\"end\":40170,\"start\":40163},{\"end\":40183,\"start\":40177},{\"end\":40200,\"start\":40191},{\"end\":40213,\"start\":40208},{\"end\":40228,\"start\":40223},{\"end\":40243,\"start\":40237},{\"end\":40261,\"start\":40251},{\"end\":40544,\"start\":40537},{\"end\":40561,\"start\":40552},{\"end\":40577,\"start\":40571},{\"end\":40878,\"start\":40872},{\"end\":40888,\"start\":40880},{\"end\":41119,\"start\":41117},{\"end\":41134,\"start\":41126},{\"end\":41148,\"start\":41144},{\"end\":41156,\"start\":41152},{\"end\":41169,\"start\":41167},{\"end\":41187,\"start\":41180},{\"end\":41203,\"start\":41195},{\"end\":41216,\"start\":41210},{\"end\":41225,\"start\":41222},{\"end\":41236,\"start\":41233},{\"end\":41246,\"start\":41238},{\"end\":41638,\"start\":41631},{\"end\":41654,\"start\":41645},{\"end\":41987,\"start\":41982},{\"end\":41997,\"start\":41994},{\"end\":42010,\"start\":42006},{\"end\":42023,\"start\":42021},{\"end\":42036,\"start\":42033}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1606.02006\",\"id\":\"b0\"},\"end\":29124,\"start\":28797},{\"attributes\":{\"doi\":\"CoRR abs/1409.0473\",\"id\":\"b1\"},\"end\":29376,\"start\":29126},{\"attributes\":{\"doi\":\"arXiv:1607.07086\",\"id\":\"b2\"},\"end\":29791,\"start\":29378},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":221275765},\"end\":30108,\"start\":29793},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":26275404},\"end\":30575,\"start\":30110},{\"attributes\":{\"doi\":\"arXiv:1706.09733\",\"id\":\"b5\"},\"end\":30863,\"start\":30577},{\"attributes\":{\"doi\":\"arXiv:1603.06393\",\"id\":\"b6\"},\"end\":31168,\"start\":30865},{\"attributes\":{\"doi\":\"arXiv:1603.08148\",\"id\":\"b7\"},\"end\":31451,\"start\":31170},{\"attributes\":{\"doi\":\"10.1162/neco.1997.9.8.1735\",\"id\":\"b8\"},\"end\":31687,\"start\":31453},{\"attributes\":{\"doi\":\"arXiv:1611.01462\",\"id\":\"b9\"},\"end\":32020,\"start\":31689},{\"attributes\":{\"doi\":\"CoRR abs/1703.03129\",\"id\":\"b10\"},\"end\":32278,\"start\":32022},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":12639289},\"end\":32482,\"start\":32280},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b12\"},\"end\":32696,\"start\":32484},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":38407095},\"end\":32914,\"start\":32698},{\"attributes\":{\"doi\":\"arXiv:1706.03872\",\"id\":\"b14\"},\"end\":33146,\"start\":32916},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14646537},\"end\":33791,\"start\":33148},{\"attributes\":{\"doi\":\"arXiv:1508.04025\",\"id\":\"b16\"},\"end\":34106,\"start\":33793},{\"attributes\":{\"doi\":\"arXiv:1410.8206\",\"id\":\"b17\"},\"end\":34464,\"start\":34108},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16447573},\"end\":34893,\"start\":34466},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":215824512},\"end\":35458,\"start\":34895},{\"attributes\":{\"doi\":\"arXiv:1710.01329\",\"id\":\"b20\"},\"end\":35703,\"start\":35460},{\"attributes\":{\"doi\":\"arXiv:1610.05243\",\"id\":\"b21\"},\"end\":35974,\"start\":35705},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":11080756},\"end\":36572,\"start\":35976},{\"attributes\":{\"doi\":\"arXiv:1705.04304\",\"id\":\"b23\"},\"end\":36851,\"start\":36574},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":9919769},\"end\":37337,\"start\":36853},{\"attributes\":{\"doi\":\"arXiv:1608.05859\",\"id\":\"b25\"},\"end\":37567,\"start\":37339},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b26\"},\"end\":37893,\"start\":37569},{\"attributes\":{\"doi\":\"arXiv:1612.00563\",\"id\":\"b27\"},\"end\":38237,\"start\":37895},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b28\"},\"end\":38546,\"start\":38239},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1114678},\"end\":39042,\"start\":38548},{\"attributes\":{\"doi\":\"arXiv:1512.02433\",\"id\":\"b30\"},\"end\":39377,\"start\":39044},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7961699},\"end\":39847,\"start\":39379},{\"attributes\":{\"doi\":\"arXiv:1601.04811\",\"id\":\"b32\"},\"end\":40140,\"start\":39849},{\"attributes\":{\"doi\":\"arXiv:1706.03762\",\"id\":\"b33\"},\"end\":40511,\"start\":40142},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5692837},\"end\":40778,\"start\":40513},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2332513},\"end\":41063,\"start\":40780},{\"attributes\":{\"doi\":\"arXiv:1609.08144\",\"id\":\"b36\",\"matched_paper_id\":3603249},\"end\":41620,\"start\":41065},{\"attributes\":{\"doi\":\"arXiv:1505.00521\",\"id\":\"b37\"},\"end\":41874,\"start\":41622},{\"attributes\":{\"id\":\"b38\"},\"end\":42221,\"start\":41876}]", "bib_title": "[{\"end\":29830,\"start\":29793},{\"end\":30164,\"start\":30110},{\"end\":31474,\"start\":31453},{\"end\":32319,\"start\":32280},{\"end\":32761,\"start\":32698},{\"end\":33219,\"start\":33148},{\"end\":34541,\"start\":34466},{\"end\":34985,\"start\":34895},{\"end\":36038,\"start\":35976},{\"end\":36923,\"start\":36853},{\"end\":38607,\"start\":38548},{\"end\":39429,\"start\":39379},{\"end\":40529,\"start\":40513},{\"end\":40868,\"start\":40780},{\"end\":41107,\"start\":41065}]", "bib_author": "[{\"end\":28889,\"start\":28874},{\"end\":28904,\"start\":28889},{\"end\":28922,\"start\":28904},{\"end\":29209,\"start\":29197},{\"end\":29216,\"start\":29209},{\"end\":29226,\"start\":29216},{\"end\":29447,\"start\":29429},{\"end\":29464,\"start\":29447},{\"end\":29475,\"start\":29464},{\"end\":29490,\"start\":29475},{\"end\":29501,\"start\":29490},{\"end\":29516,\"start\":29501},{\"end\":29533,\"start\":29516},{\"end\":29548,\"start\":29533},{\"end\":29847,\"start\":29832},{\"end\":29864,\"start\":29847},{\"end\":29880,\"start\":29864},{\"end\":29898,\"start\":29880},{\"end\":30177,\"start\":30166},{\"end\":30188,\"start\":30177},{\"end\":30200,\"start\":30188},{\"end\":30596,\"start\":30577},{\"end\":30611,\"start\":30596},{\"end\":30942,\"start\":30931},{\"end\":30956,\"start\":30942},{\"end\":30965,\"start\":30956},{\"end\":30977,\"start\":30965},{\"end\":30981,\"start\":30977},{\"end\":31187,\"start\":31170},{\"end\":31200,\"start\":31187},{\"end\":31218,\"start\":31200},{\"end\":31230,\"start\":31218},{\"end\":31245,\"start\":31230},{\"end\":31490,\"start\":31476},{\"end\":31505,\"start\":31490},{\"end\":31792,\"start\":31770},{\"end\":31810,\"start\":31792},{\"end\":31818,\"start\":31810},{\"end\":32071,\"start\":32056},{\"end\":32084,\"start\":32071},{\"end\":32095,\"start\":32084},{\"end\":32108,\"start\":32095},{\"end\":32339,\"start\":32321},{\"end\":32353,\"start\":32339},{\"end\":32545,\"start\":32528},{\"end\":32555,\"start\":32545},{\"end\":32778,\"start\":32763},{\"end\":32978,\"start\":32963},{\"end\":32995,\"start\":32978},{\"end\":33236,\"start\":33221},{\"end\":33253,\"start\":33236},{\"end\":33267,\"start\":33253},{\"end\":33878,\"start\":33860},{\"end\":33889,\"start\":33878},{\"end\":33912,\"start\":33889},{\"end\":34190,\"start\":34172},{\"end\":34206,\"start\":34190},{\"end\":34214,\"start\":34206},{\"end\":34224,\"start\":34214},{\"end\":34242,\"start\":34224},{\"end\":34251,\"start\":34242},{\"end\":34558,\"start\":34543},{\"end\":34574,\"start\":34558},{\"end\":34584,\"start\":34574},{\"end\":34600,\"start\":34584},{\"end\":34611,\"start\":34600},{\"end\":35001,\"start\":34987},{\"end\":35012,\"start\":35001},{\"end\":35018,\"start\":35012},{\"end\":35038,\"start\":35018},{\"end\":35524,\"start\":35516},{\"end\":35538,\"start\":35524},{\"end\":35546,\"start\":35538},{\"end\":35766,\"start\":35753},{\"end\":35777,\"start\":35766},{\"end\":35790,\"start\":35777},{\"end\":35803,\"start\":35790},{\"end\":36058,\"start\":36040},{\"end\":36072,\"start\":36058},{\"end\":36083,\"start\":36072},{\"end\":36097,\"start\":36083},{\"end\":36644,\"start\":36629},{\"end\":36659,\"start\":36644},{\"end\":36675,\"start\":36659},{\"end\":36934,\"start\":36925},{\"end\":36951,\"start\":36934},{\"end\":36975,\"start\":36951},{\"end\":36993,\"start\":36975},{\"end\":37406,\"start\":37394},{\"end\":37417,\"start\":37406},{\"end\":37583,\"start\":37569},{\"end\":37598,\"start\":37583},{\"end\":37614,\"start\":37598},{\"end\":37629,\"start\":37614},{\"end\":37638,\"start\":37629},{\"end\":37959,\"start\":37949},{\"end\":37975,\"start\":37959},{\"end\":37994,\"start\":37975},{\"end\":38009,\"start\":37994},{\"end\":38024,\"start\":38009},{\"end\":38030,\"start\":38024},{\"end\":38254,\"start\":38239},{\"end\":38268,\"start\":38254},{\"end\":38287,\"start\":38268},{\"end\":38301,\"start\":38287},{\"end\":38314,\"start\":38301},{\"end\":38621,\"start\":38609},{\"end\":38631,\"start\":38621},{\"end\":38640,\"start\":38631},{\"end\":39056,\"start\":39044},{\"end\":39068,\"start\":39056},{\"end\":39081,\"start\":39068},{\"end\":39089,\"start\":39081},{\"end\":39097,\"start\":39089},{\"end\":39110,\"start\":39097},{\"end\":39120,\"start\":39110},{\"end\":39444,\"start\":39431},{\"end\":39455,\"start\":39444},{\"end\":39463,\"start\":39455},{\"end\":39912,\"start\":39899},{\"end\":39926,\"start\":39912},{\"end\":39936,\"start\":39926},{\"end\":39949,\"start\":39936},{\"end\":39958,\"start\":39949},{\"end\":40158,\"start\":40142},{\"end\":40172,\"start\":40158},{\"end\":40185,\"start\":40172},{\"end\":40202,\"start\":40185},{\"end\":40215,\"start\":40202},{\"end\":40230,\"start\":40215},{\"end\":40245,\"start\":40230},{\"end\":40263,\"start\":40245},{\"end\":40546,\"start\":40531},{\"end\":40563,\"start\":40546},{\"end\":40579,\"start\":40563},{\"end\":40880,\"start\":40870},{\"end\":40890,\"start\":40880},{\"end\":41121,\"start\":41109},{\"end\":41136,\"start\":41121},{\"end\":41150,\"start\":41136},{\"end\":41158,\"start\":41150},{\"end\":41171,\"start\":41158},{\"end\":41189,\"start\":41171},{\"end\":41205,\"start\":41189},{\"end\":41218,\"start\":41205},{\"end\":41227,\"start\":41218},{\"end\":41238,\"start\":41227},{\"end\":41248,\"start\":41238},{\"end\":41640,\"start\":41622},{\"end\":41656,\"start\":41640},{\"end\":41989,\"start\":41973},{\"end\":41999,\"start\":41989},{\"end\":42012,\"start\":41999},{\"end\":42025,\"start\":42012},{\"end\":42038,\"start\":42025}]", "bib_venue": "[{\"end\":28872,\"start\":28797},{\"end\":29195,\"start\":29126},{\"end\":29427,\"start\":29378},{\"end\":29934,\"start\":29898},{\"end\":30293,\"start\":30200},{\"end\":30697,\"start\":30627},{\"end\":30929,\"start\":30865},{\"end\":31287,\"start\":31261},{\"end\":31544,\"start\":31531},{\"end\":31768,\"start\":31689},{\"end\":32054,\"start\":32022},{\"end\":32361,\"start\":32353},{\"end\":32526,\"start\":32484},{\"end\":32787,\"start\":32778},{\"end\":32961,\"start\":32916},{\"end\":33339,\"start\":33267},{\"end\":33858,\"start\":33793},{\"end\":34170,\"start\":34108},{\"end\":34660,\"start\":34611},{\"end\":35124,\"start\":35038},{\"end\":35514,\"start\":35460},{\"end\":35751,\"start\":35705},{\"end\":36180,\"start\":36097},{\"end\":36627,\"start\":36574},{\"end\":37037,\"start\":36993},{\"end\":37392,\"start\":37339},{\"end\":37708,\"start\":37654},{\"end\":37947,\"start\":37895},{\"end\":38369,\"start\":38330},{\"end\":38727,\"start\":38640},{\"end\":39188,\"start\":39136},{\"end\":39575,\"start\":39463},{\"end\":39897,\"start\":39849},{\"end\":40304,\"start\":40279},{\"end\":40628,\"start\":40579},{\"end\":40906,\"start\":40890},{\"end\":41318,\"start\":41264},{\"end\":41725,\"start\":41672},{\"end\":41971,\"start\":41876},{\"end\":30373,\"start\":30295},{\"end\":33440,\"start\":33383},{\"end\":35197,\"start\":35126},{\"end\":36250,\"start\":36182},{\"end\":38816,\"start\":38729},{\"end\":39591,\"start\":39577}]"}}}, "year": 2023, "month": 12, "day": 17}