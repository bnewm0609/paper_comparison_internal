{"id": 226222256, "updated": "2023-10-06 09:23:17.803", "metadata": {"title": "CliniQG4QA: Generating Diverse Questions for Domain Adaptation of Clinical Question Answering", "authors": "[{\"first\":\"Xiang\",\"last\":\"Yue\",\"middle\":[]},{\"first\":\"Xinliang\",\"last\":\"Zhang\",\"middle\":[\"Frederick\"]},{\"first\":\"Ziyu\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Simon\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Huan\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Clinical question answering (QA) aims to automatically answer questions from medical professionals based on clinical texts. Studies show that neural QA models trained on one corpus may not generalize well to new clinical texts from a different institute or a different patient group, where large-scale QA pairs are not readily available for model retraining. To address this challenge, we propose a simple yet effective framework, CliniQG4QA, which leverages question generation (QG) to synthesize QA pairs on new clinical contexts and boosts QA models without requiring manual annotations. In order to generate diverse types of questions that are essential for training QA models, we further introduce a seq2seq-based question phrase prediction (QPP) module that can be used together with most existing QG models to diversify the generation. Our comprehensive experiment results show that the QA corpus generated by our framework can improve QA models on the new contexts (up to 8% absolute gain in terms of Exact Match), and that the QPP module plays a crucial role in achieving the gain.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.16021", "mag": "3095463793", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/bibm/YueZYLS21", "doi": "10.1109/bibm52615.2021.9669300"}}, "content": {"source": {"pdf_hash": "e5120fa67a12848c957122e99cc896de9614dff1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.16021v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4133c34526b6533fe568d3858c5589b82b2500d2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e5120fa67a12848c957122e99cc896de9614dff1.txt", "contents": "\nCliniQG4QA: Generating Diverse Questions for Domain Adaptation of Clinical Question Answering\n\n\nXiang Yue \nThe Ohio State University\n\n\nXinliang Frederick Zhang \nThe Ohio State University\n\n\nUniversity of Michigan\n\n\nZiyu Yao ziyuyao@gmu.edu \nThe Ohio State University\n\n\nGeorge Mason University\n\n\nSimon Lin simon.lin@nationwidechildrens.org \nAbigail Wexner Research Institute at Nationwide Children's Hospital\n\n\nHuan Sun \nThe Ohio State University\n\n\nCliniQG4QA: Generating Diverse Questions for Domain Adaptation of Clinical Question Answering\n* These two authors contributed equallyIndex Terms-Clinical Question AnsweringClinical Question GenerationNatural Language ProcessingDomain AdaptationClinical Text\nClinical question answering (QA) aims to automatically answer questions from medical professionals based on clinical texts. Studies show that neural QA models trained on one corpus may not generalize well to new clinical texts from a different institute or a different patient group, where largescale QA pairs are not readily available for model retraining.To address this challenge, we propose a simple yet effective framework, CliniQG4QA, which leverages question generation (QG) to synthesize QA pairs on new clinical contexts and boosts QA models without requiring manual annotations. In order to generate diverse types of questions that are essential for training QA models, we further introduce a seq2seq-based question phrase prediction (QPP) module that can be used together with most existing QG models to diversify the generation. Our comprehensive experiment results show that the QA corpus generated by our framework can improve QA models on the new contexts (up to 8% absolute gain in terms of Exact Match), and that the QPP module plays a crucial role in achieving the gain. 1\n\nI. INTRODUCTION\n\nClinical question answering (QA), which aims to automatically answer natural language questions based on clinical texts in Electronic Medical Records (EMR), has been identified as an important task to assist clinical practitioners [1]- [5]. Neural QA models in recent years [5]- [7] show promising results in this research. However, answering clinical questions still remains challenging in real-world scenarios, because well-trained QA systems may not generalize well to new clinical contexts from a different institute or a different patient group. For example, as pointed out in [8], when a clinical QA model that was trained on the emrQA dataset [3] is deployed to answer questions based on MIMIC-III clinical texts [9], its performance drops dramatically by around 30% even on the questions that are similar to those in training, simply because clinical texts of the 1 Our dataset and code are available at: https://github.com/sunlabosu/CliniQG4QA/. two datasets are different (e.g., different topics, note structures, writing styles).\n\nOne straightforward solution is to annotate QA pairs on new contexts and retrain a QA model. However, manually creating large-scale QA pairs in clinical domain is extremely challenging due to the requirement of tremendous expert effort, data privacy concerns and other ethical issues.\n\nIn this work, we study the problem of constructing clinical QA models on new contexts without human-annotated QA pairs (which is referred to as domain adaptation). We assume the availability of a large set of QA pairs on source contexts, and our goal is to better answer questions on new documents (target contexts 2 ), where only unlabeled documents are provided.\n\nTo this end, we introduce our framework, CliniQG4QA, which leverages question generation (QG), a recent technique of automatically generating questions from given contexts [10], to synthesize clinical QA pairs on target contexts to facilitate the QA model training ( Figure 2). The QG model is built up by reusing the QA pairs on source contexts as training data. To apply QG to target contexts, our framework also includes an answer evidence extractor (AEE) to extract meaningful text spans, which are worthwhile to ask questions about, from the clinical documents. Intrinsically, our framework is backed by the observation that questions in the clinical domain generally follow similar patterns even across different contexts, and clinical QG suffers less from the context shift compared with clinical QA. This allows us to utilize QG models trained on source clinical contexts to boost QA models on target contexts.\n\nHowever, our preliminary studies find that many existing QG models often fall short on generating questions that are diverse enough to serve as useful training data for clinical QA models. To tackle the problem, we introduce a question phrase prediction (QPP) module, which takes an answer evidence as input and sequentially predicts potential question phrases (e.g., \"What treatment\", \"How often\") that signify what types of questions humans would likely ask about the answer evidence. By directly forcing a QG model to produce specified question phrases in the beginning of the question generation process (both in training and inference), QPP enables diverse questions to be generated.\n\nDue to the lack of publicly-available clinical QA pairs for our proposed domain adaptation evaluation setting, we ask clinical experts to annotate a new test set on the sampled MIMIC-III [9] clinical texts. We conduct extensive experiments to evaluate CliniQG4QA, using emrQA [3] as the source contexts and our annotated MIMIC-III [9] as the target ones. We instantiate our framework with a variety of widely adopted base QG models and base QA models.\n\nBy performing comprehensive analyses, we show that the proposed QPP module can substantially help generate much more diverse types of questions (e.g., \"When\" and \"Why\" questions). More importantly, we systematically demonstrate the strong capability of CliniQG4QA for improving QA performance on new contexts by evaluating it on our constructed MIMIC-III QA dataset. When using QA pairs automatically synthesized by our QPP-enhanced QG models as the training corpus, we are able to boost QA models' performance by up to 8% in terms of Exact Match (EM), compared with their counterparts directly trained on the emrQA dataset. To further investigate why QG boosts QA, we provide both quantitative and qualitative analyses, indicating that QA models can benefit from seeing more target contexts as well as more diverse questions generated on them.\n\n\nII. PRELIMINARY AND RELATED WORK\n\nClinical Question Answering aims to extract a text span (a sentence or multiple sentences) as the answer from a patient clinical note given a question ( Fig. 1 left) [8]. Though many neural models [5]- [7], [11], [12] have achieved impressive results on this task, their performance on new clinical contexts, whose data distributions could be different from the ones that these models were trained on, is still far from satisfactory [8]. Though one can improve the performance by adding more QA pairs on new contexts into training, however, manually creating large-scale QA pairs in the clinical domain often involves tremendous expert effort and data privacy concerns. Moreover, during the pandemic, clinical QA models can also be deployed to answer COVID-19 related questions [13], [14]. Question Generation seeks to automatically generate questions given a sentence or paragraph (Fig. 1 right). Existing QG models [10], [15]- [23] in the open domain usually adopt a seq2seq (encoder-decoder) architecture. One of the drawback of such models is that they can only generate one question given one input and fail to generate multiple diverse questions, which we find is crucial to the QA task. Some recent work [24]- [26] explores the diverse QG in the open domain, but they cannot be directly applied to the clinical domain as their models usually require a short answer (e.g., an entity) as input but that information sometimes is not available in the clinical QA dataset (e.g. emrQA [3]), rendering the difficulty of directly deploying their model on the clinical QA.  In the clinical and medical domain, [27] and [28], [29] apply Variational Autoencoder (VAE) models to generate or paraphrase medical or clinical questions. However, none of them explore leveraging QG to improve QA performance on new contexts. Our aim is to improve clinical QA on new clinical texts (i.e., domain adaptation of clinical QA). We assume the availability of a large set of QA pairs and corresponding clinical documents (source contexts), and our goal is to better answer questions on new documents (target contexts) where only unlabeled documents are provided. We leverage a QG model to synthesize diverse QA pairs to save medical experts annotation efforts and improve QA performance without requiring extra annotations. Our setting is very practical in the real-world scenario, since it is infeasible to always annotate QA pairs on new clinical texts when deploying a QA system into a new environment.\n\n\nIII. METHODS\n\n\nA. Overview of Our Framework\n\nWe first give an overview of our CliniQG4QA framework (Fig 2). CliniQG4QA improves clinical QA on new contexts by automatically synthesizing QA pairs for new clinical contexts. To approach this, we first leverage an answer evidence extractor to extract meaningful text spans from unlabeled documents, based on which a QG model can be applied to generate questions.\n\nIn order to encourage diverse questions, we reformulate the question generation process as two-stage. In the first stage, we propose a question phrase prediction module to predict a set of question phrases, which represent the types of questions humans would ask, given an answer evidence. In the second stage, following a specific question phrase predicted by our QPP, a QG model is used to complete the rest of the question. Therefore, our framework CliniQG4QA is able to produce questions of more diverse types. The generated QA pairs by QG models are finally used to train QA models on new contexts.\n\n\nB. Answer Evidence Extractor (AEE)\n\nWhen human annotators create questions, they first read a document and then select a text span to ask questions about. To imitate this process, we implement an answer evidence extractor to extract possible text spans from a document. Following [3], [8], we focus on longer text spans (as answer evidences) instead of short answers (e.g., a single named entity), since longer text spans often contain richer information compared with short ones, which are very important for the clinical QA task. More formally, given a document (context) p = {p 1 , p 2 , ..., p m }, where p i is the i-th token of the document and m is the total number of tokens, we aim to extract potential evidence sequences. Since the answer evidence is not always a single sentence (sometimes could be multiple sentences), instead of treating it as a sentence selection task, we formulate it as a sequence labeling (or tagging) task. We follow the BIO tagging (short for beginning, inside, outside), a commonly used sequence labeling scheme [30], to label answer evidences.\n\nFirstly, we adopt the ClinicalBERT model [31] to encode the document:\nU = ClinicalBERT{p 1 , ..., p m }.(1)\nwhere U \u2208 R m\u00d7d , and d is size of the dimension. Following the same paradigm of the BERT model for the sequence labeling task [7], we use a linear layer on top of the hidden states output by ClinicalBERT followed by a softmax function to do the classification:\nPr(a j |p i ) = softmax(U \u00b7 W + b), \u2200p i \u2208 p (2)\nwhere a j is the predicted BIO tag. After prediction, we observe that the extracted answer evidences sometimes are broken sentences due to the noisy nature and uninformative language (e.g., acronyms) of clinical texts. To make sure that the extracted evidences are meaningful, we designed a \"merge-and-drop\" heuristic rule to further improve the extractor's accuracy. Specifically, for each extracted evidence candidate, we first examine the length (number of tokens) of the extracted evidence. If the length is larger than the threshold \u03b7, we keep this evidence; otherwise, we compute the distance, i.e., the number of tokens between the current candidate span and another closest candidate span If the distance is smaller than the threshold \u03b3, we merge these two \"close-sitting\" spans; otherwise, we drop this overly-short evidence span. In our experiments, we set \u03b7 and \u03b3 to be 3 and 3, respectively, since they help achieve the best performance on the dev set.\n\n\nC. Question Phrase Prediction (QPP)\n\nExisting QG models are often biased to generate limited types of questions. To address this problem, we introduce our question phrase prediction module that can be used to diversify the generation of existing QG models.\n\nFormally, denote V l = {s 1 , ..., s L } as the vocabulary of all available question phrases of length l in the training data and L = |V l | as its size. V l can be obtained by collecting the first n-gram words in the questions. We set n = 2 in our experiment as it achieves the best performance on the dev set. Given an answer evidence a, the goal of QPP is to map a \u2192 y = (y 1 , ..., y L ) \u2208 {0, 1} L , where y i = 1 indicates predicting s i in V l as a question phrase for the evidence a. Instead of treating it as a common multi-label classification problem, we formulate the task as a sequence prediction problem and adopt a commonly used seq2seq model with an attention mechanism [32] to predict a sequence of question phrases s = (s j1 , ..., s j |s| ) (e.g., \"What treatment\" (s j1 ) \u2192 \"How often\" (s j2 ) \u2192 \"What dosage\" (s j3 ), with |s| = 3).\n\nDuring training, we assume that the set of question phrases is arranged in a pre-defined order. Such orderings can be obtained with some heuristic methods, e.g., using a descending order based on question phrase frequency in the corpus 3 . In the inference stage, QPP can dynamically decide the number of question phrases for each answer evidence by predicting a special [STOP] token. By decomposing QG into two steps (diversification followed by generation), the implemented QPP can increase the diversity in a more controllable way.\n\n\nD. Training\n\nAlgorithm 1 illustrates the pretraining and training procedure of our CliniQG4QA.\n\nDuring the pretraining stage, we first train the answer evidence extractor (AEE) module on the source contexts by minimizing the negative log-likelihood loss:\nL AEE = \u2212 i log P (a|p; \u03c6)(3)\nwhere \u03c6 represents all the parameters of the answer evidence extractor. For the supervision signals, we identify all evidences in the source data as ground-truth chunks which are marked using the BIO scheme. Moving to the Question Phrase Prediction (QPP) module, given an answer evidence a, we aim to predict a question phrase sequence y and minimize:\nL QP P = \u2212 i log P (y|a; \u03b8)(4)\nwhere \u03b8 denotes all the parameters of QPP. Then we can train any QG model (e.g, NQG [10]) on source data by minimizing:\nL QG = \u2212 i log P (q|a, y; \u00b5)(5)\nwhere \u00b5 denotes all parameters of the QG model. During the training stage, given unlabeled target clinical documents, we first extract answer evidences, based on which QPP can be \"plugged\" into the QG model to generate diverse questions. Finally, a QA model (e.g., DocReader [6]) can be trained on the generated QA pairs of the target documents:\nL QA = \u2212 i log P (a|q, p; \u03b4)(6)\nwhere \u03b4 denotes all parameters of the QA model.  [3], which was generated based on medical expert-made question templates and existing annotations on n2c2 challenge datasets [33], is a commonly adopted dataset for clinical reading comprehension.\n\n\nIV. GENERALIZABILITY TEST SET CONSTRUCTION\n\nHowever, all the QA pairs in emrQA are based on n2c2 clinical texts and thus not suitable for our generalization setting.\n\n[8] studied a similar problem and annotated a test set on MIMIC-III clinical texts [9]. However, their test set is too small (only 50 QA pairs) and not publicly available. Given the lack of a reasonably large clinical QA test set for studying generalization, with the help of three clinical experts, we create 1287 QA pairs on a sampled set of MIMIC-III [9] clinical notes, which have been reviewed and approved by PhysioNet 4 and is downloadable by following the instructions 5 . Annotation Process. We sample 36 MIMIC-III clinical notes as contexts. When sampling MIMIC-III notes, we ensure that all the sampled clinical texts do not appear in emrQA, acknowledging that there is a small overlap between the two datasets. For each context, clinical experts can ask any questions as long as an answer can be extracted from the context. To save annotation effort, QA pairs generated by QG models (i.e., all base QG models and their diversity-enhanced variants; see Section V-A) are provided as references, and duplicates are removed. Meanwhile, clinical experts are highly encouraged to create new questions based on the given clinical text (which are marked as \"human-generated\"/\"HG\"). But if they do find the machine-generated questions sound natural and match the provided answer, they can keep them (which are marked as \"human-verified\"/\"HV\"). After obtaining the annotated questions, we ask another clinical expert to do a final pass of the questions in order to further ensure the quality of the test set. The final test set consists of 1287 questions (of which 975 are \"human-verified\" and 312 are \"human-generated\").\n\nWe understand that there might be potential bias when evaluating QA models on the HV set (i.e, a QG model which is used to generate training questions for a QA model also contributes questions to the HV set as well). However, such bias might exist in human annotated data as well (e.g., the same set of humans create both training and testing dataset). Note that the contexts used to generate questions in HV/HG are separated from those to generate training questions. Besides, due to the relatively limited language patterns in clinical domain, we find most questions in HV set sound like what humans would ask. As such, we still deem it as a valuable asset and potential future research could leverage our HV set as their dev set to tune hyper-parameters.\n\nTo help tune the model, we also construct dev set of MIMIC- III by sampling generated questions from QG models and their variants and is used to tune the hyper-parameters. In the following sections, we consider emrQA as the source dataset and our annotated MIMIC-III QA dataset as the target data. Detailed statistics of the two datasets are in Table I.\n\n\nV. EXPERIMENTAL SETUP A. Base QG models\n\nWe instantiate our CliniQG4QA framework using three base QG models:\n\n\u2022 NQG [10] is the first seq2seq model with a global attention mechanism [32] for question generation.\n\n\u2022 NQG++ [15] is one of the most commonly adopted QG baselines with a feature-enriched encoder (e.g., lexical features) and a copy mechanism [35].\n\n\u2022 BERT-SQG [34] uses a pretrained BERT model (we use ClinicalBERT [31] to accommodate clinical setting) as the encoder and formulates the decoding as a \"MASK\" token prediction problem.\n\nIt has been studied that beam search and sampling strategies show competitive performance in diversifying generations [36], [37]. We thus include Top-k [38] and Nucleus samplings [39] as representative sampling strategies in our experiments.\n\nAs such, to investigate the effectiveness of diverse QG for QA, we consider the following variants of each base QG model: \n\n\nB. Base QA models\n\nFor QA, we instantiate CliniQG4QA with two base models, DocReader [6] and ClinicalBERT [31]. When training a QA model, we only use the synthetic data on the target contexts and do not combine the synthetic data with the source data since the combination does not help in our preliminary studies.\n\nNote that more complex QG/QA models and training strategies can also be used in our framework. As this work focuses on exploring how diverse questions help QA on target contexts, we adopt fundamental QG/QA models and training strategies, and leave more advanced ones that are complementary to our framework as future work.\n\n\nC. Evaluation Metrics\n\nFor QA evaluation, we report exact match (EM) (percentage of predictions that match the ground truth answers exactly) and F1 (average overlap between the predictions and ground truth answers) as in [40]. Since our main goal is to evaluate whether the generated questions are useful to improve the QA performance on the target contexts, the common language generation metrics such as BLEU [41] and ROUGE-L [42] are not suitable to reflect the quality of the generated questions, and thus we do not adopt these metrics in our experiments.\n\n\nD. Implementation Details\n\nBase QG Models: We re-implement three base QG models using Pytorch and have ensured that they achieve comparable performance as originally reported. Best QG models are selected using the per-token accuracy of both the QPP module (if applicable) and QG on dev set. Base QA Models: We use the open-sourced implementation. 6 Best QA models are selected using EM and F1 on dev set. Hyperparameters Search: Hyperparameters of QG models are set to be the same as in original papers and hyperparameters of QA models are set according to [8]. Specifically, we train NQG and NQG++ up to 20 epochs, BERT-SQG up to 5 epochs, DocReader up to 5 epochs and ClinicalBERT up to 3 epochs.\n\n\nVI. EXPERIMENTAL RESULTS\n\nA. Can Generated Questions Help QA on New Contexts? Table II summarizes the performance of two widely used QA models, DocReader [6] and ClinicalBERT [31], on the MIMIC-III testing set. The QA models are trained based on different corpora, including the emrQA dataset as well as QA pairs generated by different models. For a fair comparison, we keep the total number of generated QA pairs roughly the same as emrQA. As can be seen from the table, the QA models based on the corpora that are generated using the three base QG models can only achieve roughly the same or even worse performance compared with the QA models trained on the emrQA dataset. Though the Beam Search and sampling strategies could boost the diversity of generated questions to some extent, and thus lead to the improvement of QA models, our proposed QPP module can improve the QA performance by a larger margin. For example, training DocReader using questions generated by NQG++ with our QPP module outperforms that using the emrQA dataset by around 8% under EM and 4% under F1 on the overall test set. Moreover, the results on human-generated portion are consistently better than that on human-verified. It's attributed to the fact that human-created questions are more readable and sensible while human-verified questions are a bit of less natural though correctness is ensured.\n\nAll these results indicate that generating a diverse QA corpus is useful for downstream QA on new contexts, and our simple QPP module can help existing QG models achieve such a goal.\n\n\nB. Why QG Boosts QA on New Contexts?\n\nTo further explore why QG can boost QA, we consider three major factors when generating a QA corpus: the number of documents, the number of answer evidences per document, and the number of generated questions per answer evidence. When we test one factor, we fix the other two. For example, we fix the number of answer evidences and questions at 20 and 6 when we test the influence of the number of documents. We use NQG++ and DocReader as our base QG and QA models to instantiate our CliniQG4QA framework and report the performance on the Dev set.\n\nAs can seen from Fig 4, the performance steadily increases when we use more documents and more answer evidences during QA corpus generation. This can demonstrate the first hypothesis: The generated corpus enables a QA model to see more new contexts during training, which can help the QA model get a better understanding of similar contexts during testing. The more contexts it sees, the more benefits it could obtain. We can also see that with the increase of the number of generated questions per evidence, the performance generally rises up. This indicates that multiple diverse questions are essential for boosting QA performance. A Closer Look at Generated Question Types. To further demonstrate QPP module can help generate diverse questions, we show the distribution over the types of questions generated by NQG-based models in Fig 3. We observe that questions generated by base NQG and NQG+BeamSearch are limited in terms of the question types. However, more types of questions (e.g., \"How\", \"Why\") can be generated when enabling sampling strategies. Furthermore, when being equipped with our QPP module, the NQG model can even generate questions of an extremely rare type, i.e., \"When\" questions. Though Top-k and Nucleus sampling methods also generate questions of less frequent types, our QPP module could cover even more types.\n\nIn summary, we think seeing many new contexts and diverse questions are the two main reasons why QA models are boosted.\n\nC. Diverse Questions Really Matter for QA: Two Real Cases.\n\nIn Fig 5, we present a QA example and a QG example from MIMIC-III for qualitative analysis.\n\nIn the QA example, this \"why\" question can be correctly answered by the QA model (DocReader) trained on the \"NQG+QPP\" generated corpus while the QA models trained on other generated corpora fail. This is because, as shown in Fig 3, the NQG model and \"NQG+BeamSearch\" cannot generate any \"why\" questions and sampling strategies could only help generate a limited number of \"why\" questions. Thus QA models trained on such corpora cannot answer questions of less frequent types. Though the emrQA dataset contains diverse questions (including \"why\" questions), its contexts might be different from MIMIC-III in terms of topic, note structures, writing styles, etc. So the model trained on emrQA struggles to answer some questions as well.\n\nIn the QG example, the base model NQG can only generate one question. Though utilizing the Beam Search enables the model to explore multiple candidates, the generated questions are quite similar and are less likely to help improve QA. Sampling strategies, though further diversifying the generation during decoding, suffer from generating irrelevant contents (e.g., \"NQG+Nucleus\" generates a irrelevant \"morphine\" token). Enabling our QPP module helps generate relevant and diverse questions including \"Why\", \"What\", \"How\", etc.\n\n\nD. Ablation Study\n\nPerformance of QPP with Sampling Strategies. Since our QPP is compatible with sampling strategies, we further study the performance after combining these two techniques.  lead to further improvement compared with using QPP only. This demonstrate that our QPP module is good enough to generate diverse useful questions for improving QA. Alternative Approaches for QPP. There are many model options for the QPP task, e.g., those for multi-label classification. To justify our choice of a seq2seq model, we compare it with two commonly-adopted multi-label classification methods: binary relevance (BR) and classifier chain (CC) [43], [44]. BR develops multiple binary classifiers independently while CC builds a chain of classifiers and predicts labels sequentially. We use multi-layer perceptron as the base model for both BR and CC. For each answer evidence, the input is the representation from the same LSTM encoder as our QPP module.\n\nFrom Table IV, we can see: (1) The seq2seq design in our QPP module performs better overall and especially in terms of Recall, which is particularly important since we aim for generating diverse question types; (2) A simple seq2seq model achieves great performance across all metrics, which renders Context: ... he was guaiac negative on admission. hematocrit remained stable overnight. 5. abd pain: suspect secondary to chronic pancreatitis. amylase unchanged from previous levels. ...  \n\nFig. 1 :\n1Illustration of Clinical Question Answering (QA) and Question Generation (QG) task.\n\nAlgorithm 1\n1CliniQG4QA training procedure Input: labeled source data {(P S , A S , Q S )}, unlabeled target data {P T } Output: Generated QA pairs {(A T , Q T )} on target contexts; An optimized QA model for answering questions on target contexts; Pretraining Stage 1: Train Answer Evidence Extractor based on the source data {(P S , A S )} using Eq. 3 2: Obtain question phrase data Y S from Q S and train Question Phrase Prediction module on the source data {(A S , Y S )} using Eq. 4 3: Train a QPP-enhanced QG model on the source data {(A S , Y S , Q S )} using Eq. 5 Training Stage 4: Use AEE to extract potential answer evidences {A T } on the target contexts {P T } 5: Use QPP to predict potential question phrases set {Y T } on {A T } 6: Use QPP-enhanced QG to generate diverse questions {Q T } based on {(A T , Y T )} 7: Train a QA model on synthetic target data {(P T , A T , Q T )} using Eq. 6\n\n( 1 )\n1Base Model: Inference with greedy search; (2) Base Model + Beam Search: Inference with Beam Search of beam size K and keep top K beams (K = 3); (3) Base Model + Top-k sampling: Inference with sampling from top-k tokens (k = 20); (4) Base Model + Nucleus sampling: Inference with sampling from top-p tokens (p = 0.95); (5) Base Model + QPP: Inference with greedy search for both QPP module and Base model.\n\n\n-emrQA: 5. abd pain -NQG: 5. abd pain: -NQG+BeamSearch: 5. abd pain: -NQG+Top-k: 5. abd pain: -NQG+Nucleus: 5. abd pain: -NQG+QPP: 5. abd pain: suspect secondary to chronic pancreatitis. QA Example from MIMIC-III Question: Why did the patient get abd pain? Answer by QA model trained on -NQG: Does the patient have any pain? -NQG+BeamSearch: Does the patient have any pain history? Does the patient have pain? Does the patient have any pain? -NQG+Top-k: Has the patient ever had any pain? Has the patient ever reported pain? Does the patient have a history pain? -NQG+Nucleus: Has the patient ever gone into pain? What happened when she was given morphine? Is there mention pain anywhere in the record? -NQG+QPP: Why did the patient have acetaminophen? What treatment has the patient had for his pain? How was pain treated? Does the patient have any pain? ... QG Example from MIMIC-III Context: ... the patient was taking at home prior to admission were not restarted. 25. acetaminophen 325-650 mg po/ng q6h:prn pain 26. dabigatran etexilate 150 mg po bid... Questions generated by Fig. 5: QA and QG examples. The red parts in contexts are ground-truth answer evidences.\n\n\nFor HTN control ... BPWhy has the patient ...Context: ... For HTN control, \npt was given HCTZ and \nlopressor which sufficiently \ncontrolled his BP. Pt was sent \nhome on HCTZ 25mg daily and \natenolol 50mg daily. \n... \nADDITIONAL COMMENTS: \n1.) Take hydrochlo-rothiazide \n25mg daily and atenolol 50mg \ndaily for your blood pressure. \n... \n\nRECORD #992321, Date: 2145-09-22 \n\nQA \nModel \n\nFor HTN control, pt was given \nHCTZ and lopressor which \nsufficiently controlled his BP. \n\nWhy has the patient \nbeen prescribed hctz? \n\nRead \nText \n\nQuestion \n\nExtract Answer \n\nEncoder \n\nDecoder \n\n\n\n\nUnlike open domain, there are very few publicly available QA datasets in the clinical domain. EmrQA dataset\n\nTABLE I :\nIStatistics of the datasets. We synthesize a machinegenerated dev set and ask human experts to annotate a test set for MIMIC-III.(Question / Context) \nemrQA \nMIMIC-III \n# Train \n781,857 / 337 \n-/ 337 \n# Dev \n86,663 / 41 \n8,824 / 40 \n# Test \n98,994 / 42 \n1,287 / 36 \n# Total \n967,514 / 420 \n-/ 413 \n\nfor purpose of \nQG & QA \n(source) \n\nQA \n(target) \n\n\n\nTABLE II :\nIIThe QA performance on MIMIC-III test set. emrQA is also included as a baseline dataset to help illustrate the generated diverse questions on MIMIC-III are useful to improve the QA model performance on new contexts.QA Datasets \n\nDocReader [6] \nClinicalBERT [31] \nHuman \nGenerated \n\nHuman \nVerified \n\nOverall \nTest \n\nHuman \nGenerated \n\nHuman \nVerified \n\nOverall \nTest \nEM \nF1 \nEM \nF1 \nEM \nF1 \nEM \nF1 \nEM \nF1 \nEM \nF1 \nemrQA [3] \n69.87 83.66 61.44 78.82 63.48 79.99 69.23 82.83 61.23 78.56 63.17 79.59 \nNQG [10] \n66.99 79.67 64.71 79.36 65.26 79.43 67.30 82.59 59.49 76.68 61.38 78.11 \n+ BeamSearch \n71.15 83.07 67.07 81.21 68.07 81.66 68.91 84.26 63.17 79.17 64.56 80.40 \n+ Top-k Sampling \n71.58 83.48 66.77 80.45 67.94 81.19 67.74 81.96 60.82 78.16 62.50 79.08 \n+ Nucleus Sampling 70.62 83.68 67.16 80.37 68.00 81.17 68.70 83.21 62.36 77.89 63.90 79.18 \n+ QPP (Ours) \n74.36 85.18 68.82 82.89 70.09 83.44 69.23 84.33 63.79 79.56 65.11 80.72 \nNQG++ [15] \n66.34 81.34 65.94 78.71 66.04 79.35 65.06 80.11 59.59 75.85 60.92 76.88 \n+ BeamSearch \n72.11 84.56 68.10 80.09 69.07 81.17 68.26 83.70 64.61 80.30 65.50 81.12 \n+ Top-k Sampling \n73.29 85.56 69.11 82.38 69.41 83.35 70.19 85.61 62.84 79.77 64.62 81.19 \n+ Nucleus Sampling 73.34 84.95 68.94 81.72 70.01 82.51 70.19 84.72 63.93 79.54 65.45 80.80 \n+ QPP (Ours) \n74.68 85.92 70.05 83.47 71.10 84.06 70.83 85.76 65.33 80.64 66.67 81.88 \nBERT-SQG [34] \n70.19 81.47 66.05 79.64 67.05 80.08 65.06 82.20 59.59 78.04 60.92 79.05 \n+ BeamSearch \n73.71 84.44 68.71 81.98 69.93 82.58 67.31 82.54 61.94 79.02 63.25 79.88 \n+ Top-k Sampling \n72.81 84.16 69.20 82.24 70.07 82.71 69.12 84.20 60.44 78.27 62.55 79.71 \n+ Nucleus Sampling 70.73 83.60 68.56 81.80 69.09 82.24 67.74 83.16 61.61 78.74 63.09 79.81 \n+ QPP (Ours) \n74.36 85.53 70.77 83.60 71.64 84.07 69.23 85.38 64.21 80.53 65.43 81.71 \n\n\n\nTable III shows\nIIIthe results, which indicate that combining two techniques can improve the sampling strategies' performance but do notFig. 3: Distributions over types of questions generated by NQG models.QA model's Performance on MIMIC-III Dev SetFig. 4: Influence of the number of documents, number of evidences per document, number of QA pairs per evidence on QA performance.0 \n\n29.94 \n13.76 \n13.79 \n8.39 \n\n0 \n0 \n0.00 \n0.00 \n0.01 \n3.95 \n5.54 \n27.01 \n26.93 \n25.37 \n\n0 \n0 \n1.93 \n1.94 \n4.44 \n\n0 \n0 \n0.48 \n0.46 \n0.91 \n0 \n0 \n0.91 \n0.95 \n0.87 \n0 \n1.38 \n13.38 \n13.24 \n9.09 \n\n0 \n0 \n0.12 \n0.09 \n0.27 \n0 \n0 \n1.29 \n1.29 \n4.03 \n\n0 \n0 \n4.15 \n4.06 \n12.72 \n\n96.05 \n\n63.14 \n\n36.94 \n37.15 \n33.89 \n\nN Q G \nN Q G + B e a m S e a r c h \nN Q G + T o p -k \nN Q G + N uc l e us \nN Q G + Q P P ( O ur s ) \n\nWhat \nWhen \nHas \nWas \nWhy \nHow \nIs \nDid \nCan \nAny \nDoes \n\n100 300 500 750 1000 \n# of documents \n\n60 \n\n70 \n\n80 \n\n90 \n\nEM \nF1 \n\n5 10 20 30 50 \n# of evidences / doc \n\nEM \nF1 \n\n1 3 6 9 12 \n# of ques / evidence \n\nEM \nF1 \n\n\n\nTABLE III :\nIIIThe QA performance on MIMIC-III test set when QPP is employed with sampling strategies Nucleus + QPP 74.12 85.08 68.10 81.36 69.56 82.26QA Datasets \n\nDocReader [6] \nHuman \nGenerated \n\nHuman \nVerified \n\nOverall \nTest \nEM \nF1 \nEM \nF1 \nEM \nF1 \nNQG \n66.99 79.67 64.71 79.36 65.26 79.43 \n+ QPP \n74.36 85.18 68.82 82.89 70.09 83.44 \n+ Top-k \n71.58 83.48 66.77 80.45 67.94 81.19 \n+ Tok-k + QPP \n72.52 84.98 67.67 81.79 68.84 82.56 \n+ Nucleus \n70.62 83.68 67.16 80.37 68.00 81.17 \n+ \n\nTABLE IV :\nIVChoosing seq2seq-based QPP over alternative multi-label classification methods. HL: Hamming Loss. developing more complex models for this task less necessary.VII. CONCLUSION This paper proposes a simple yet effective framework for improving clinical QA on new contexts. It leverages a seq2seqbased question phrase prediction module to enable QG models to generate diverse questions. Our comprehensive experiments and analyses allow for a better understanding of why diverse question generation can help QA on new clinical documents.Models \nHL \nPrecision Recall \nF1 \nBinary Relevance 0.0524 \n99.22 \n90.89 94.87 \nClassifier Chain \n0.0524 \n99.22 \n90.89 94.87 \nQPP \n0.0346 \n97.28 \n96.20 \n96.74 \n\n\nWe use \"new\" and \"target\" contexts interchangeably.\nIn our dataset, each answer evidence is tied with multiple questions, which allows the training for QPP.\nhttps://physionet.org/. PhysioNet is a resource center with missions to conduct and catalyze for biomedical research, which offers free access to large collections of physiological and clinical data, such as MIMIC-III[9]. 5 https://physionet.org/content/mimic-iii-question-answer/1.0.0/.\nDocReader: https://github.com/facebookresearch/DrQA. ClinicalBERT: https://github.com/EmilyAlsentzer/clinicalBERT.\nACKNOWLEDGMENTThe authors would like to thank all the constructive reviews. The research is sponsored in part by the PCORI Funding ME-2017C1-6413, the Army Research Office under cooperative agreements W911NF-17-1-0412, NSF Grant IIS1815674, NSF CAREER #1942980, and Ohio Supercomputer Center[45]. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.\nAn ontology for clinical questions about the contents of patient notes. J Patrick, M Li, JBI. 452J. Patrick and M. Li, \"An ontology for clinical questions about the contents of patient notes,\" JBI, vol. 45, no. 2, pp. 292-306, 2012.\n\nAnnotating electronic medical records for question answering. P Raghavan, S Patwardhan, J J Liang, M V Devarakonda, arXiv:1805.06816arXiv preprintP. Raghavan, S. Patwardhan, J. J. Liang, and M. V. Devarakonda, \"Annotating electronic medical records for question answering,\" arXiv preprint arXiv:1805.06816, 2018.\n\nemrqa: A large corpus for question answering on electronic medical records. A Pampari, P Raghavan, J Liang, J Peng, EMNLP. 18A. Pampari, P. Raghavan, J. Liang, and J. Peng, \"emrqa: A large corpus for question answering on electronic medical records,\" in EMNLP'18, 2018, pp. 2357-2368.\n\nAnnotating and characterizing clinical sentences with explicit why-qa cues. J Fan, NAACL Clinical NLP Workshop. J. Fan, \"Annotating and characterizing clinical sentences with explicit why-qa cues,\" in NAACL Clinical NLP Workshop, 2019, pp. 101-106.\n\nEntityenriched neural models for clinical question answering. B P S Rawat, W.-H Weng, P Raghavan, P Szolovits, arXiv:2005.06587arXiv preprintB. P. S. Rawat, W.-H. Weng, P. Raghavan, and P. Szolovits, \"Entity- enriched neural models for clinical question answering,\" arXiv preprint arXiv:2005.06587, 2020.\n\nReading wikipedia to answer open-domain questions. D Chen, A Fisch, J Weston, A Bordes, ACL'17. D. Chen, A. Fisch, J. Weston, and A. Bordes, \"Reading wikipedia to answer open-domain questions,\" in ACL'17, 2017, pp. 1870-1879.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, NAACL-HLT'19. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training of deep bidirectional transformers for language understanding,\" in NAACL- HLT'19, 2019, pp. 4171-4186.\n\nClinical reading comprehension: A thorough analysis of the emrqa dataset. X Yue, B J Gutierrez, H Sun, ACL'20. X. Yue, B. J. Gutierrez, and H. Sun, \"Clinical reading comprehension: A thorough analysis of the emrqa dataset,\" in ACL'20, 2020.\n\nMimic-iii, a freely accessible critical care database. A E Johnson, T J Pollard, L Shen, H L Li-Wei, M Feng, M Ghassemi, B Moody, P Szolovits, L A Celi, R G Mark, Scientific data. 3160035A. E. Johnson, T. J. Pollard, L. Shen, H. L. Li-wei, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark, \"Mimic-iii, a freely accessible critical care database,\" Scientific data, vol. 3, p. 160035, 2016.\n\nLearning to ask: Neural question generation for reading comprehension. X Du, J Shao, C Cardie, ACL'17. X. Du, J. Shao, and C. Cardie, \"Learning to ask: Neural question generation for reading comprehension,\" in ACL'17, 2017, pp. 1342-1352.\n\nBidirectional attention flow for machine comprehension. M Seo, A Kembhavi, A Farhadi, H Hajishirzi, ICLR'17. M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, \"Bidirectional attention flow for machine comprehension,\" in ICLR'17, 2017.\n\nAdapting and evaluating a deep learning language model for clinical why-question answering. A Wen, M Y Elwazir, S Moon, J Fan, JAMIA Open. 31A. Wen, M. Y. Elwazir, S. Moon, and J. Fan, \"Adapting and evaluating a deep learning language model for clinical why-question answering,\" JAMIA Open, vol. 3, no. 1, pp. 16-20, 2020.\n\nCollecting verified COVID-19 question answer pairs. A Poliak, M Fleming, C Costello, K W Murray, M Yarmohammadi, S Pandya, D Irani, M Agarwal, U Sharma, S Sun, N Ivanov, L Shang, K Srinivasan, S Lee, X Han, S Agarwal, J Sedoc, Proceedings of the 1st Workshop on NLP for COVID-19@ EMNLP 2020. the 1st Workshop on NLP for COVID-19@ EMNLP 2020OnlineAssociation for Computational LinguisticsA. Poliak, M. Fleming, C. Costello, K. W. Murray, M. Yarmohammadi, S. Pandya, D. Irani, M. Agarwal, U. Sharma, S. Sun, N. Ivanov, L. Shang, K. Srinivasan, S. Lee, X. Han, S. Agarwal, and J. Sedoc, \"Collecting verified COVID-19 question answer pairs,\" in Proceedings of the 1st Workshop on NLP for COVID-19@ EMNLP 2020, Online, December 2020. Association for Computational Linguistics, 2020.\n\nCOUGH: A challenge dataset and models for COVID-19 FAQ retrieval. X F Zhang, H Sun, X Yue, S M Lin, H Sun, EMNLP 2021. Association for Computational LinguisticsX. F. Zhang, H. Sun, X. Yue, S. M. Lin, and H. Sun, \"COUGH: A challenge dataset and models for COVID-19 FAQ retrieval,\" in EMNLP 2021. Association for Computational Linguistics, 2021, pp. 3759-3769.\n\nNeural question generation from text: A preliminary study. Q Zhou, N Yang, F Wei, C Tan, H Bao, M Zhou, NLPCC'17. SpringerQ. Zhou, N. Yang, F. Wei, C. Tan, H. Bao, and M. Zhou, \"Neural question generation from text: A preliminary study,\" in NLPCC'17. Springer, 2017, pp. 662-671.\n\nAnswer-focused and position-aware neural question generation. X Sun, J Liu, Y Lyu, W He, Y Ma, S Wang, EMNLP. 18X. Sun, J. Liu, Y. Lyu, W. He, Y. Ma, and S. Wang, \"Answer-focused and position-aware neural question generation,\" in EMNLP'18, 2018, pp. 3930-3939.\n\nParagraph-level neural question generation with maxout pointer and gated self-attention networks. Y Zhao, X Ni, Y Ding, Q Ke, EMNLP. 18Y. Zhao, X. Ni, Y. Ding, and Q. Ke, \"Paragraph-level neural question generation with maxout pointer and gated self-attention networks,\" in EMNLP'18, 2018, pp. 3901-3910.\n\nLet's ask again: Refine network for automatic question generation. P Nema, A K Mohankumar, M M Khapra, B V Srinivasan, B Ravindran, EMNLP-IJCNLP'19. P. Nema, A. K. Mohankumar, M. M. Khapra, B. V. Srinivasan, and B. Ravindran, \"Let's ask again: Refine network for automatic question generation,\" in EMNLP-IJCNLP'19, 2019, pp. 3305-3314.\n\nCapturing greater context for question generation. L A Tuan, D J Shah, R Barzilay, AAAI'20. L. A. Tuan, D. J. Shah, and R. Barzilay, \"Capturing greater context for question generation,\" in AAAI'20, 2020.\n\nSemi-supervised qa with generative domain-adaptive nets. Z Yang, J Hu, R Salakhutdinov, W Cohen, ACL'17. Z. Yang, J. Hu, R. Salakhutdinov, and W. Cohen, \"Semi-supervised qa with generative domain-adaptive nets,\" in ACL'17, 2017, pp. 1040-1050.\n\nHarvesting paragraph-level question-answer pairs from wikipedia. X Du, C Cardie, ACL'18. X. Du and C. Cardie, \"Harvesting paragraph-level question-answer pairs from wikipedia,\" in ACL'18, 2018, pp. 1907-1917.\n\nSynthetic qa corpora generation with roundtrip consistency. C Alberti, D Andor, E Pitler, J Devlin, M Collins, ACL'19. C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \"Synthetic qa corpora generation with roundtrip consistency,\" in ACL'19, 2019, pp. 6168-6173.\n\nAddressing semantic drift in question generation for semi-supervised question answering. S Zhang, M Bansal, EMNLP-IJCNLP'19. S. Zhang and M. Bansal, \"Addressing semantic drift in question genera- tion for semi-supervised question answering,\" in EMNLP-IJCNLP'19, 2019, pp. 2495-2509.\n\nLet me know what to ask: Interrogativeword-aware question generation. J Kang, H P San Roman, Proceedings of the 2nd Workshop on Machine Reading for Question Answering. the 2nd Workshop on Machine Reading for Question AnsweringJ. Kang, H. P. San Roman et al., \"Let me know what to ask: Interrogative- word-aware question generation,\" in Proceedings of the 2nd Workshop on Machine Reading for Question Answering, 2019, pp. 163-171.\n\nMixture content selection for diverse sequence generation. J Cho, M Seo, H Hajishirzi, EMNLP-IJCNLP'19. J. Cho, M. Seo, and H. Hajishirzi, \"Mixture content selection for diverse sequence generation,\" in EMNLP-IJCNLP'19, 2019, pp. 3112-3122.\n\nAsking questions the human way: Scalable question-answer generation from text corpus. B Liu, H Wei, D Niu, H Chen, Y He, in WWW'20, 2020B. Liu, H. Wei, D. Niu, H. Chen, and Y. He, \"Asking questions the human way: Scalable question-answer generation from text corpus,\" in WWW'20, 2020, pp. 2032-2043.\n\nOn the generation of medical question-answer pairs. S Shen, Y Li, N Du, X Wu, Y Xie, S Ge, T Yang, K Wang, X Liang, W Fan, AAAI. S. Shen, Y. Li, N. Du, X. Wu, Y. Xie, S. Ge, T. Yang, K. Wang, X. Liang, and W. Fan, \"On the generation of medical question-answer pairs.\" in AAAI, 2020, pp. 8822-8829.\n\nA paraphrase generation system for ehr question answering. S Soni, K Roberts, 18th BioNLP Workshop. S. Soni and K. Roberts, \"A paraphrase generation system for ehr question answering,\" in 18th BioNLP Workshop, 2019, pp. 20-29.\n\nParaphrasing to improve the performance of electronic health records question answering. AMIA Summits. 2020626--, \"Paraphrasing to improve the performance of electronic health records question answering,\" AMIA Summits, vol. 2020, p. 626, 2020.\n\nText chunking using transformationbased learning,\" in Natural language processing using very large corpora. L A Ramshaw, M P Marcus, SpringerL. A. Ramshaw and M. P. Marcus, \"Text chunking using transformation- based learning,\" in Natural language processing using very large corpora. Springer, 1999, pp. 157-176.\n\nPublicly available clinical bert embeddings. E Alsentzer, J R Murphy, W Boag, W.-H Weng, D Jin, T Naumann, M Mcdermott, NAACL Clinical NLP Workshop. E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, and M. McDermott, \"Publicly available clinical bert embeddings,\" NAACL Clinical NLP Workshop 2019, 2019.\n\nEffective approaches to attention-based neural machine translation. M.-T Luong, H Pham, C D Manning, EMNLP'15. M.-T. Luong, H. Pham, and C. D. Manning, \"Effective approaches to attention-based neural machine translation,\" in EMNLP'15, 2015, pp. 1412-1421.\n\nn2c2 nlp research data sets. portal.dbmi.hms.harvard.edun2c2, \"n2c2 nlp research data sets,\" portal.dbmi.hms.harvard.edu, 2006. [Online]. Available: https://portal.dbmi.hms.harvard.edu/projects/ n2c2-nlp/\n\nA recurrent bert-based model for question generation. Y.-H Chan, Y.-C Fan, Proceedings of the 2nd Workshop on Machine Reading for Question Answering. the 2nd Workshop on Machine Reading for Question AnsweringY.-H. Chan and Y.-C. Fan, \"A recurrent bert-based model for question generation,\" in Proceedings of the 2nd Workshop on Machine Reading for Question Answering, 2019, pp. 154-162.\n\nPointing the unknown words. C Gulcehre, S Ahn, R Nallapati, B Zhou, Y Bengio, ACL'16. C. Gulcehre, S. Ahn, R. Nallapati, B. Zhou, and Y. Bengio, \"Pointing the unknown words,\" in ACL'16, 2016, pp. 140-149.\n\nComparison of diverse decoding methods from conditional language models. D Ippolito, R Kriz, J Sedoc, M Kustikova, C Callison-Burch, ACL '19. Association for Computational LinguisticsD. Ippolito, R. Kriz, J. Sedoc, M. Kustikova, and C. Callison-Burch, \"Comparison of diverse decoding methods from conditional language models,\" in ACL '19. Association for Computational Linguistics, 2019, pp. 3752-3762.\n\nOn the importance of diversity in question generation for QA. M A Sultan, S Chandel, R F Astudillo, V Castelli, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreaultthe 58th Annual Meeting of the Association for Computational LinguisticsM. A. Sultan, S. Chandel, R. F. Astudillo, and V. Castelli, \"On the importance of diversity in question generation for QA,\" in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds., 2020, pp. 5651-5656.\n\nHierarchical neural story generation. A Fan, M Lewis, Y N Dauphin, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsA. Fan, M. Lewis, and Y. N. Dauphin, \"Hierarchical neural story generation,\" in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, 2018, pp. 889-898.\n\nThe curious case of neural text degeneration. A Holtzman, J Buys, L Du, M Forbes, Y Choi, International Conference on Learning Representations. A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, \"The curious case of neural text degeneration,\" in International Conference on Learning Representations, 2019.\n\nSquad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, EMNLP'16. P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \"Squad: 100,000+ questions for machine comprehension of text,\" in EMNLP'16, 2016, pp. 2383-2392.\n\nBleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, ACL'02. Association for Computational LinguisticsK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \"Bleu: a method for automatic evaluation of machine translation,\" in ACL'02. Association for Computational Linguistics, 2002, pp. 311-318.\n\nROUGE: A package for automatic evaluation of summaries. C.-Y. Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsC.-Y. Lin, \"ROUGE: A package for automatic evaluation of summaries,\" in Text Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics, Jul. 2004, pp. 74-81.\n\nLearning multi-label scene classification. M R Boutell, J Luo, X Shen, C M Brown, Pattern Recognition. M. R. Boutell, J. Luo, X. Shen, and C. M. Brown, \"Learning multi-label scene classification,\" Pattern Recognition, p. 1757-1771, 2004.\n\nClassifier chains for multi-label classification. J Read, B Pfahringer, G Holmes, E Frank, Machine learning. 853333J. Read, B. Pfahringer, G. Holmes, and E. Frank, \"Classifier chains for multi-label classification,\" Machine learning, vol. 85, no. 3, p. 333, 2011.\n\nOhio supercomputer center. O S Center, O. S. Center, \"Ohio supercomputer center,\" 1987. [Online]. Available: http://osc.edu/ark:/19495/f5s1ph73\n", "annotations": {"author": "[{\"end\":135,\"start\":97},{\"end\":214,\"start\":136},{\"end\":294,\"start\":215},{\"end\":409,\"start\":295},{\"end\":447,\"start\":410}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":103},{\"end\":160,\"start\":155},{\"end\":223,\"start\":220},{\"end\":304,\"start\":301},{\"end\":418,\"start\":415}]", "author_first_name": "[{\"end\":102,\"start\":97},{\"end\":144,\"start\":136},{\"end\":154,\"start\":145},{\"end\":219,\"start\":215},{\"end\":300,\"start\":295},{\"end\":414,\"start\":410}]", "author_affiliation": "[{\"end\":134,\"start\":108},{\"end\":188,\"start\":162},{\"end\":213,\"start\":190},{\"end\":267,\"start\":241},{\"end\":293,\"start\":269},{\"end\":408,\"start\":340},{\"end\":446,\"start\":420}]", "title": "[{\"end\":94,\"start\":1},{\"end\":541,\"start\":448}]", "venue": null, "abstract": "[{\"end\":1796,\"start\":706}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2049,\"start\":2046},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2054,\"start\":2051},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2092,\"start\":2089},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2097,\"start\":2094},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2400,\"start\":2397},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2468,\"start\":2465},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2538,\"start\":2535},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2688,\"start\":2687},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3685,\"start\":3681},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5309,\"start\":5306},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5398,\"start\":5395},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5453,\"start\":5450},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6622,\"start\":6619},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6653,\"start\":6650},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6658,\"start\":6655},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6664,\"start\":6660},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6670,\"start\":6666},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6889,\"start\":6886},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7235,\"start\":7231},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7241,\"start\":7237},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7374,\"start\":7370},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7380,\"start\":7376},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7386,\"start\":7382},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7668,\"start\":7664},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7674,\"start\":7670},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7942,\"start\":7939},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8065,\"start\":8061},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8074,\"start\":8070},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8080,\"start\":8076},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10244,\"start\":10241},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10249,\"start\":10246},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11014,\"start\":11010},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11089,\"start\":11085},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11282,\"start\":11279},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13378,\"start\":13374},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13780,\"start\":13779},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14836,\"start\":14832},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15178,\"start\":15175},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15330,\"start\":15327},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15456,\"start\":15452},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15779,\"start\":15776},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16050,\"start\":16047},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18553,\"start\":18549},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18619,\"start\":18615},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18658,\"start\":18654},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18790,\"start\":18786},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18808,\"start\":18804},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18863,\"start\":18859},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19101,\"start\":19097},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19107,\"start\":19103},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19135,\"start\":19131},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19162,\"start\":19158},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19435,\"start\":19432},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19457,\"start\":19453},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20213,\"start\":20209},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20403,\"start\":20399},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20420,\"start\":20416},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20898,\"start\":20897},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21110,\"start\":21107},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21408,\"start\":21405},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21430,\"start\":21426},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26932,\"start\":26928},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26938,\"start\":26934},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27270,\"start\":27267},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35798,\"start\":35795}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27823,\"start\":27729},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28730,\"start\":27824},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29143,\"start\":28731},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30316,\"start\":29144},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30901,\"start\":30317},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31011,\"start\":30902},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31373,\"start\":31012},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33215,\"start\":31374},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34221,\"start\":33216},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":34713,\"start\":34222},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":35420,\"start\":34714}]", "paragraph": "[{\"end\":2855,\"start\":1815},{\"end\":3141,\"start\":2857},{\"end\":3507,\"start\":3143},{\"end\":4427,\"start\":3509},{\"end\":5117,\"start\":4429},{\"end\":5570,\"start\":5119},{\"end\":6416,\"start\":5572},{\"end\":8941,\"start\":6453},{\"end\":9353,\"start\":8989},{\"end\":9958,\"start\":9355},{\"end\":11042,\"start\":9997},{\"end\":11113,\"start\":11044},{\"end\":11413,\"start\":11152},{\"end\":12427,\"start\":11463},{\"end\":12686,\"start\":12467},{\"end\":13541,\"start\":12688},{\"end\":14077,\"start\":13543},{\"end\":14174,\"start\":14093},{\"end\":14334,\"start\":14176},{\"end\":14716,\"start\":14365},{\"end\":14867,\"start\":14748},{\"end\":15245,\"start\":14900},{\"end\":15523,\"start\":15278},{\"end\":15691,\"start\":15570},{\"end\":17316,\"start\":15693},{\"end\":18075,\"start\":17318},{\"end\":18430,\"start\":18077},{\"end\":18541,\"start\":18474},{\"end\":18644,\"start\":18543},{\"end\":18791,\"start\":18646},{\"end\":18977,\"start\":18793},{\"end\":19220,\"start\":18979},{\"end\":19344,\"start\":19222},{\"end\":19661,\"start\":19366},{\"end\":19985,\"start\":19663},{\"end\":20547,\"start\":20011},{\"end\":21248,\"start\":20577},{\"end\":22628,\"start\":21277},{\"end\":22812,\"start\":22630},{\"end\":23400,\"start\":22853},{\"end\":24741,\"start\":23402},{\"end\":24862,\"start\":24743},{\"end\":24922,\"start\":24864},{\"end\":25015,\"start\":24924},{\"end\":25751,\"start\":25017},{\"end\":26281,\"start\":25753},{\"end\":27238,\"start\":26303},{\"end\":27728,\"start\":27240}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11151,\"start\":11114},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11462,\"start\":11414},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14364,\"start\":14335},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14747,\"start\":14717},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14899,\"start\":14868},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15277,\"start\":15246}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18429,\"start\":18422},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21337,\"start\":21329},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27253,\"start\":27245}]", "section_header": "[{\"end\":1813,\"start\":1798},{\"end\":6451,\"start\":6419},{\"end\":8956,\"start\":8944},{\"end\":8987,\"start\":8959},{\"end\":9995,\"start\":9961},{\"end\":12465,\"start\":12430},{\"end\":14091,\"start\":14080},{\"end\":15568,\"start\":15526},{\"end\":18472,\"start\":18433},{\"end\":19364,\"start\":19347},{\"end\":20009,\"start\":19988},{\"end\":20575,\"start\":20550},{\"end\":21275,\"start\":21251},{\"end\":22851,\"start\":22815},{\"end\":26301,\"start\":26284},{\"end\":27738,\"start\":27730},{\"end\":27836,\"start\":27825},{\"end\":28737,\"start\":28732},{\"end\":31022,\"start\":31013},{\"end\":31385,\"start\":31375},{\"end\":33232,\"start\":33217},{\"end\":34234,\"start\":34223},{\"end\":34725,\"start\":34715}]", "table": "[{\"end\":30901,\"start\":30364},{\"end\":31373,\"start\":31152},{\"end\":33215,\"start\":31602},{\"end\":34221,\"start\":33596},{\"end\":34713,\"start\":34374},{\"end\":35420,\"start\":35260}]", "figure_caption": "[{\"end\":27823,\"start\":27740},{\"end\":28730,\"start\":27838},{\"end\":29143,\"start\":28739},{\"end\":30316,\"start\":29146},{\"end\":30364,\"start\":30319},{\"end\":31011,\"start\":30904},{\"end\":31152,\"start\":31024},{\"end\":31602,\"start\":31388},{\"end\":33596,\"start\":33236},{\"end\":34374,\"start\":34238},{\"end\":35260,\"start\":34728}]", "figure_ref": "[{\"end\":3784,\"start\":3776},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6618,\"start\":6606},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7349,\"start\":7335},{\"end\":9050,\"start\":9043},{\"end\":23425,\"start\":23419},{\"end\":24243,\"start\":24237},{\"end\":24933,\"start\":24927},{\"end\":25248,\"start\":25242}]", "bib_author_first_name": "[{\"end\":36709,\"start\":36708},{\"end\":36720,\"start\":36719},{\"end\":36933,\"start\":36932},{\"end\":36945,\"start\":36944},{\"end\":36959,\"start\":36958},{\"end\":36961,\"start\":36960},{\"end\":36970,\"start\":36969},{\"end\":36972,\"start\":36971},{\"end\":37261,\"start\":37260},{\"end\":37272,\"start\":37271},{\"end\":37284,\"start\":37283},{\"end\":37293,\"start\":37292},{\"end\":37547,\"start\":37546},{\"end\":37783,\"start\":37782},{\"end\":37787,\"start\":37784},{\"end\":37799,\"start\":37795},{\"end\":37807,\"start\":37806},{\"end\":37819,\"start\":37818},{\"end\":38078,\"start\":38077},{\"end\":38086,\"start\":38085},{\"end\":38095,\"start\":38094},{\"end\":38105,\"start\":38104},{\"end\":38336,\"start\":38335},{\"end\":38349,\"start\":38345},{\"end\":38358,\"start\":38357},{\"end\":38365,\"start\":38364},{\"end\":38640,\"start\":38639},{\"end\":38647,\"start\":38646},{\"end\":38649,\"start\":38648},{\"end\":38662,\"start\":38661},{\"end\":38863,\"start\":38862},{\"end\":38865,\"start\":38864},{\"end\":38876,\"start\":38875},{\"end\":38878,\"start\":38877},{\"end\":38889,\"start\":38888},{\"end\":38897,\"start\":38896},{\"end\":38899,\"start\":38898},{\"end\":38909,\"start\":38908},{\"end\":38917,\"start\":38916},{\"end\":38929,\"start\":38928},{\"end\":38938,\"start\":38937},{\"end\":38951,\"start\":38950},{\"end\":38953,\"start\":38952},{\"end\":38961,\"start\":38960},{\"end\":38963,\"start\":38962},{\"end\":39293,\"start\":39292},{\"end\":39299,\"start\":39298},{\"end\":39307,\"start\":39306},{\"end\":39518,\"start\":39517},{\"end\":39525,\"start\":39524},{\"end\":39537,\"start\":39536},{\"end\":39548,\"start\":39547},{\"end\":39792,\"start\":39791},{\"end\":39799,\"start\":39798},{\"end\":39801,\"start\":39800},{\"end\":39812,\"start\":39811},{\"end\":39820,\"start\":39819},{\"end\":40076,\"start\":40075},{\"end\":40086,\"start\":40085},{\"end\":40097,\"start\":40096},{\"end\":40109,\"start\":40108},{\"end\":40111,\"start\":40110},{\"end\":40121,\"start\":40120},{\"end\":40137,\"start\":40136},{\"end\":40147,\"start\":40146},{\"end\":40156,\"start\":40155},{\"end\":40167,\"start\":40166},{\"end\":40177,\"start\":40176},{\"end\":40184,\"start\":40183},{\"end\":40194,\"start\":40193},{\"end\":40203,\"start\":40202},{\"end\":40217,\"start\":40216},{\"end\":40224,\"start\":40223},{\"end\":40231,\"start\":40230},{\"end\":40242,\"start\":40241},{\"end\":40869,\"start\":40868},{\"end\":40871,\"start\":40870},{\"end\":40880,\"start\":40879},{\"end\":40887,\"start\":40886},{\"end\":40894,\"start\":40893},{\"end\":40896,\"start\":40895},{\"end\":40903,\"start\":40902},{\"end\":41222,\"start\":41221},{\"end\":41230,\"start\":41229},{\"end\":41238,\"start\":41237},{\"end\":41245,\"start\":41244},{\"end\":41252,\"start\":41251},{\"end\":41259,\"start\":41258},{\"end\":41506,\"start\":41505},{\"end\":41513,\"start\":41512},{\"end\":41520,\"start\":41519},{\"end\":41527,\"start\":41526},{\"end\":41533,\"start\":41532},{\"end\":41539,\"start\":41538},{\"end\":41804,\"start\":41803},{\"end\":41812,\"start\":41811},{\"end\":41818,\"start\":41817},{\"end\":41826,\"start\":41825},{\"end\":42079,\"start\":42078},{\"end\":42087,\"start\":42086},{\"end\":42089,\"start\":42088},{\"end\":42103,\"start\":42102},{\"end\":42105,\"start\":42104},{\"end\":42115,\"start\":42114},{\"end\":42117,\"start\":42116},{\"end\":42131,\"start\":42130},{\"end\":42400,\"start\":42399},{\"end\":42402,\"start\":42401},{\"end\":42410,\"start\":42409},{\"end\":42412,\"start\":42411},{\"end\":42420,\"start\":42419},{\"end\":42611,\"start\":42610},{\"end\":42619,\"start\":42618},{\"end\":42625,\"start\":42624},{\"end\":42642,\"start\":42641},{\"end\":42864,\"start\":42863},{\"end\":42870,\"start\":42869},{\"end\":43069,\"start\":43068},{\"end\":43080,\"start\":43079},{\"end\":43089,\"start\":43088},{\"end\":43099,\"start\":43098},{\"end\":43109,\"start\":43108},{\"end\":43372,\"start\":43371},{\"end\":43381,\"start\":43380},{\"end\":43637,\"start\":43636},{\"end\":43645,\"start\":43644},{\"end\":43647,\"start\":43646},{\"end\":44057,\"start\":44056},{\"end\":44064,\"start\":44063},{\"end\":44071,\"start\":44070},{\"end\":44326,\"start\":44325},{\"end\":44333,\"start\":44332},{\"end\":44340,\"start\":44339},{\"end\":44347,\"start\":44346},{\"end\":44355,\"start\":44354},{\"end\":44593,\"start\":44592},{\"end\":44601,\"start\":44600},{\"end\":44607,\"start\":44606},{\"end\":44613,\"start\":44612},{\"end\":44619,\"start\":44618},{\"end\":44626,\"start\":44625},{\"end\":44632,\"start\":44631},{\"end\":44640,\"start\":44639},{\"end\":44648,\"start\":44647},{\"end\":44657,\"start\":44656},{\"end\":44899,\"start\":44898},{\"end\":44907,\"start\":44906},{\"end\":45421,\"start\":45420},{\"end\":45423,\"start\":45422},{\"end\":45434,\"start\":45433},{\"end\":45436,\"start\":45435},{\"end\":45672,\"start\":45671},{\"end\":45685,\"start\":45684},{\"end\":45687,\"start\":45686},{\"end\":45697,\"start\":45696},{\"end\":45708,\"start\":45704},{\"end\":45716,\"start\":45715},{\"end\":45723,\"start\":45722},{\"end\":45734,\"start\":45733},{\"end\":46022,\"start\":46018},{\"end\":46031,\"start\":46030},{\"end\":46039,\"start\":46038},{\"end\":46041,\"start\":46040},{\"end\":46471,\"start\":46467},{\"end\":46482,\"start\":46478},{\"end\":46830,\"start\":46829},{\"end\":46842,\"start\":46841},{\"end\":46849,\"start\":46848},{\"end\":46862,\"start\":46861},{\"end\":46870,\"start\":46869},{\"end\":47081,\"start\":47080},{\"end\":47093,\"start\":47092},{\"end\":47101,\"start\":47100},{\"end\":47110,\"start\":47109},{\"end\":47123,\"start\":47122},{\"end\":47474,\"start\":47473},{\"end\":47476,\"start\":47475},{\"end\":47486,\"start\":47485},{\"end\":47497,\"start\":47496},{\"end\":47499,\"start\":47498},{\"end\":47512,\"start\":47511},{\"end\":48093,\"start\":48092},{\"end\":48100,\"start\":48099},{\"end\":48109,\"start\":48108},{\"end\":48111,\"start\":48110},{\"end\":48528,\"start\":48527},{\"end\":48540,\"start\":48539},{\"end\":48548,\"start\":48547},{\"end\":48554,\"start\":48553},{\"end\":48564,\"start\":48563},{\"end\":48852,\"start\":48851},{\"end\":48865,\"start\":48864},{\"end\":48874,\"start\":48873},{\"end\":48885,\"start\":48884},{\"end\":49116,\"start\":49115},{\"end\":49128,\"start\":49127},{\"end\":49138,\"start\":49137},{\"end\":49149,\"start\":49145},{\"end\":49453,\"start\":49448},{\"end\":49782,\"start\":49781},{\"end\":49784,\"start\":49783},{\"end\":49795,\"start\":49794},{\"end\":49802,\"start\":49801},{\"end\":49810,\"start\":49809},{\"end\":49812,\"start\":49811},{\"end\":50028,\"start\":50027},{\"end\":50036,\"start\":50035},{\"end\":50050,\"start\":50049},{\"end\":50060,\"start\":50059},{\"end\":50270,\"start\":50269},{\"end\":50272,\"start\":50271}]", "bib_author_last_name": "[{\"end\":36717,\"start\":36710},{\"end\":36723,\"start\":36721},{\"end\":36942,\"start\":36934},{\"end\":36956,\"start\":36946},{\"end\":36967,\"start\":36962},{\"end\":36984,\"start\":36973},{\"end\":37269,\"start\":37262},{\"end\":37281,\"start\":37273},{\"end\":37290,\"start\":37285},{\"end\":37298,\"start\":37294},{\"end\":37551,\"start\":37548},{\"end\":37793,\"start\":37788},{\"end\":37804,\"start\":37800},{\"end\":37816,\"start\":37808},{\"end\":37829,\"start\":37820},{\"end\":38083,\"start\":38079},{\"end\":38092,\"start\":38087},{\"end\":38102,\"start\":38096},{\"end\":38112,\"start\":38106},{\"end\":38343,\"start\":38337},{\"end\":38355,\"start\":38350},{\"end\":38362,\"start\":38359},{\"end\":38375,\"start\":38366},{\"end\":38644,\"start\":38641},{\"end\":38659,\"start\":38650},{\"end\":38666,\"start\":38663},{\"end\":38873,\"start\":38866},{\"end\":38886,\"start\":38879},{\"end\":38894,\"start\":38890},{\"end\":38906,\"start\":38900},{\"end\":38914,\"start\":38910},{\"end\":38926,\"start\":38918},{\"end\":38935,\"start\":38930},{\"end\":38948,\"start\":38939},{\"end\":38958,\"start\":38954},{\"end\":38968,\"start\":38964},{\"end\":39296,\"start\":39294},{\"end\":39304,\"start\":39300},{\"end\":39314,\"start\":39308},{\"end\":39522,\"start\":39519},{\"end\":39534,\"start\":39526},{\"end\":39545,\"start\":39538},{\"end\":39559,\"start\":39549},{\"end\":39796,\"start\":39793},{\"end\":39809,\"start\":39802},{\"end\":39817,\"start\":39813},{\"end\":39824,\"start\":39821},{\"end\":40083,\"start\":40077},{\"end\":40094,\"start\":40087},{\"end\":40106,\"start\":40098},{\"end\":40118,\"start\":40112},{\"end\":40134,\"start\":40122},{\"end\":40144,\"start\":40138},{\"end\":40153,\"start\":40148},{\"end\":40164,\"start\":40157},{\"end\":40174,\"start\":40168},{\"end\":40181,\"start\":40178},{\"end\":40191,\"start\":40185},{\"end\":40200,\"start\":40195},{\"end\":40214,\"start\":40204},{\"end\":40221,\"start\":40218},{\"end\":40228,\"start\":40225},{\"end\":40239,\"start\":40232},{\"end\":40248,\"start\":40243},{\"end\":40877,\"start\":40872},{\"end\":40884,\"start\":40881},{\"end\":40891,\"start\":40888},{\"end\":40900,\"start\":40897},{\"end\":40907,\"start\":40904},{\"end\":41227,\"start\":41223},{\"end\":41235,\"start\":41231},{\"end\":41242,\"start\":41239},{\"end\":41249,\"start\":41246},{\"end\":41256,\"start\":41253},{\"end\":41264,\"start\":41260},{\"end\":41510,\"start\":41507},{\"end\":41517,\"start\":41514},{\"end\":41524,\"start\":41521},{\"end\":41530,\"start\":41528},{\"end\":41536,\"start\":41534},{\"end\":41544,\"start\":41540},{\"end\":41809,\"start\":41805},{\"end\":41815,\"start\":41813},{\"end\":41823,\"start\":41819},{\"end\":41829,\"start\":41827},{\"end\":42084,\"start\":42080},{\"end\":42100,\"start\":42090},{\"end\":42112,\"start\":42106},{\"end\":42128,\"start\":42118},{\"end\":42141,\"start\":42132},{\"end\":42407,\"start\":42403},{\"end\":42417,\"start\":42413},{\"end\":42429,\"start\":42421},{\"end\":42616,\"start\":42612},{\"end\":42622,\"start\":42620},{\"end\":42639,\"start\":42626},{\"end\":42648,\"start\":42643},{\"end\":42867,\"start\":42865},{\"end\":42877,\"start\":42871},{\"end\":43077,\"start\":43070},{\"end\":43086,\"start\":43081},{\"end\":43096,\"start\":43090},{\"end\":43106,\"start\":43100},{\"end\":43117,\"start\":43110},{\"end\":43378,\"start\":43373},{\"end\":43388,\"start\":43382},{\"end\":43642,\"start\":43638},{\"end\":43657,\"start\":43648},{\"end\":44061,\"start\":44058},{\"end\":44068,\"start\":44065},{\"end\":44082,\"start\":44072},{\"end\":44330,\"start\":44327},{\"end\":44337,\"start\":44334},{\"end\":44344,\"start\":44341},{\"end\":44352,\"start\":44348},{\"end\":44358,\"start\":44356},{\"end\":44598,\"start\":44594},{\"end\":44604,\"start\":44602},{\"end\":44610,\"start\":44608},{\"end\":44616,\"start\":44614},{\"end\":44623,\"start\":44620},{\"end\":44629,\"start\":44627},{\"end\":44637,\"start\":44633},{\"end\":44645,\"start\":44641},{\"end\":44654,\"start\":44649},{\"end\":44661,\"start\":44658},{\"end\":44904,\"start\":44900},{\"end\":44915,\"start\":44908},{\"end\":45431,\"start\":45424},{\"end\":45443,\"start\":45437},{\"end\":45682,\"start\":45673},{\"end\":45694,\"start\":45688},{\"end\":45702,\"start\":45698},{\"end\":45713,\"start\":45709},{\"end\":45720,\"start\":45717},{\"end\":45731,\"start\":45724},{\"end\":45744,\"start\":45735},{\"end\":46028,\"start\":46023},{\"end\":46036,\"start\":46032},{\"end\":46049,\"start\":46042},{\"end\":46476,\"start\":46472},{\"end\":46486,\"start\":46483},{\"end\":46839,\"start\":46831},{\"end\":46846,\"start\":46843},{\"end\":46859,\"start\":46850},{\"end\":46867,\"start\":46863},{\"end\":46877,\"start\":46871},{\"end\":47090,\"start\":47082},{\"end\":47098,\"start\":47094},{\"end\":47107,\"start\":47102},{\"end\":47120,\"start\":47111},{\"end\":47138,\"start\":47124},{\"end\":47483,\"start\":47477},{\"end\":47494,\"start\":47487},{\"end\":47509,\"start\":47500},{\"end\":47521,\"start\":47513},{\"end\":48097,\"start\":48094},{\"end\":48106,\"start\":48101},{\"end\":48119,\"start\":48112},{\"end\":48537,\"start\":48529},{\"end\":48545,\"start\":48541},{\"end\":48551,\"start\":48549},{\"end\":48561,\"start\":48555},{\"end\":48569,\"start\":48565},{\"end\":48862,\"start\":48853},{\"end\":48871,\"start\":48866},{\"end\":48882,\"start\":48875},{\"end\":48891,\"start\":48886},{\"end\":49125,\"start\":49117},{\"end\":49135,\"start\":49129},{\"end\":49143,\"start\":49139},{\"end\":49153,\"start\":49150},{\"end\":49457,\"start\":49454},{\"end\":49792,\"start\":49785},{\"end\":49799,\"start\":49796},{\"end\":49807,\"start\":49803},{\"end\":49818,\"start\":49813},{\"end\":50033,\"start\":50029},{\"end\":50047,\"start\":50037},{\"end\":50057,\"start\":50051},{\"end\":50066,\"start\":50061},{\"end\":50279,\"start\":50273}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9855359},\"end\":36868,\"start\":36636},{\"attributes\":{\"doi\":\"arXiv:1805.06816\",\"id\":\"b1\"},\"end\":37182,\"start\":36870},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52158121},\"end\":37468,\"start\":37184},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":195742686},\"end\":37718,\"start\":37470},{\"attributes\":{\"doi\":\"arXiv:2005.06587\",\"id\":\"b4\"},\"end\":38024,\"start\":37720},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3618568},\"end\":38251,\"start\":38026},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52967399},\"end\":38563,\"start\":38253},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":218486765},\"end\":38805,\"start\":38565},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":33285731},\"end\":39219,\"start\":38807},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2172129},\"end\":39459,\"start\":39221},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8535316},\"end\":39697,\"start\":39461},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":207930303},\"end\":40021,\"start\":39699},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":229183056},\"end\":40800,\"start\":40023},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":225066685},\"end\":41160,\"start\":40802},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9745861},\"end\":41441,\"start\":41162},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53083677},\"end\":41703,\"start\":41443},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53081407},\"end\":42009,\"start\":41705},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":202565869},\"end\":42346,\"start\":42011},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":204838005},\"end\":42551,\"start\":42348},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15164488},\"end\":42796,\"start\":42553},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":21702856},\"end\":43006,\"start\":42798},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":189762081},\"end\":43280,\"start\":43008},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":202572810},\"end\":43564,\"start\":43282},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":204960839},\"end\":43995,\"start\":43566},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202538019},\"end\":44237,\"start\":43997},{\"attributes\":{\"id\":\"b25\"},\"end\":44538,\"start\":44239},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53301248},\"end\":44837,\"start\":44540},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":199379787},\"end\":45065,\"start\":44839},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":214665604},\"end\":45310,\"start\":45067},{\"attributes\":{\"id\":\"b29\"},\"end\":45624,\"start\":45312},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":102352093},\"end\":45948,\"start\":45626},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1998416},\"end\":46205,\"start\":45950},{\"attributes\":{\"id\":\"b32\"},\"end\":46411,\"start\":46207},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":208232005},\"end\":46799,\"start\":46413},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":969555},\"end\":47005,\"start\":46801},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":189927977},\"end\":47409,\"start\":47007},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":220047812},\"end\":48052,\"start\":47411},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":44134226},\"end\":48479,\"start\":48054},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":127986954},\"end\":48788,\"start\":48481},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11816014},\"end\":49049,\"start\":48790},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":11080756},\"end\":49390,\"start\":49051},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":964287},\"end\":49736,\"start\":49392},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":9404152},\"end\":49975,\"start\":49738},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":7679549},\"end\":50240,\"start\":49977},{\"attributes\":{\"id\":\"b44\"},\"end\":50385,\"start\":50242}]", "bib_title": "[{\"end\":36706,\"start\":36636},{\"end\":37258,\"start\":37184},{\"end\":37544,\"start\":37470},{\"end\":38075,\"start\":38026},{\"end\":38333,\"start\":38253},{\"end\":38637,\"start\":38565},{\"end\":38860,\"start\":38807},{\"end\":39290,\"start\":39221},{\"end\":39515,\"start\":39461},{\"end\":39789,\"start\":39699},{\"end\":40073,\"start\":40023},{\"end\":40866,\"start\":40802},{\"end\":41219,\"start\":41162},{\"end\":41503,\"start\":41443},{\"end\":41801,\"start\":41705},{\"end\":42076,\"start\":42011},{\"end\":42397,\"start\":42348},{\"end\":42608,\"start\":42553},{\"end\":42861,\"start\":42798},{\"end\":43066,\"start\":43008},{\"end\":43369,\"start\":43282},{\"end\":43634,\"start\":43566},{\"end\":44054,\"start\":43997},{\"end\":44590,\"start\":44540},{\"end\":44896,\"start\":44839},{\"end\":45154,\"start\":45067},{\"end\":45669,\"start\":45626},{\"end\":46016,\"start\":45950},{\"end\":46465,\"start\":46413},{\"end\":46827,\"start\":46801},{\"end\":47078,\"start\":47007},{\"end\":47471,\"start\":47411},{\"end\":48090,\"start\":48054},{\"end\":48525,\"start\":48481},{\"end\":48849,\"start\":48790},{\"end\":49113,\"start\":49051},{\"end\":49446,\"start\":49392},{\"end\":49779,\"start\":49738},{\"end\":50025,\"start\":49977}]", "bib_author": "[{\"end\":36719,\"start\":36708},{\"end\":36725,\"start\":36719},{\"end\":36944,\"start\":36932},{\"end\":36958,\"start\":36944},{\"end\":36969,\"start\":36958},{\"end\":36986,\"start\":36969},{\"end\":37271,\"start\":37260},{\"end\":37283,\"start\":37271},{\"end\":37292,\"start\":37283},{\"end\":37300,\"start\":37292},{\"end\":37553,\"start\":37546},{\"end\":37795,\"start\":37782},{\"end\":37806,\"start\":37795},{\"end\":37818,\"start\":37806},{\"end\":37831,\"start\":37818},{\"end\":38085,\"start\":38077},{\"end\":38094,\"start\":38085},{\"end\":38104,\"start\":38094},{\"end\":38114,\"start\":38104},{\"end\":38345,\"start\":38335},{\"end\":38357,\"start\":38345},{\"end\":38364,\"start\":38357},{\"end\":38377,\"start\":38364},{\"end\":38646,\"start\":38639},{\"end\":38661,\"start\":38646},{\"end\":38668,\"start\":38661},{\"end\":38875,\"start\":38862},{\"end\":38888,\"start\":38875},{\"end\":38896,\"start\":38888},{\"end\":38908,\"start\":38896},{\"end\":38916,\"start\":38908},{\"end\":38928,\"start\":38916},{\"end\":38937,\"start\":38928},{\"end\":38950,\"start\":38937},{\"end\":38960,\"start\":38950},{\"end\":38970,\"start\":38960},{\"end\":39298,\"start\":39292},{\"end\":39306,\"start\":39298},{\"end\":39316,\"start\":39306},{\"end\":39524,\"start\":39517},{\"end\":39536,\"start\":39524},{\"end\":39547,\"start\":39536},{\"end\":39561,\"start\":39547},{\"end\":39798,\"start\":39791},{\"end\":39811,\"start\":39798},{\"end\":39819,\"start\":39811},{\"end\":39826,\"start\":39819},{\"end\":40085,\"start\":40075},{\"end\":40096,\"start\":40085},{\"end\":40108,\"start\":40096},{\"end\":40120,\"start\":40108},{\"end\":40136,\"start\":40120},{\"end\":40146,\"start\":40136},{\"end\":40155,\"start\":40146},{\"end\":40166,\"start\":40155},{\"end\":40176,\"start\":40166},{\"end\":40183,\"start\":40176},{\"end\":40193,\"start\":40183},{\"end\":40202,\"start\":40193},{\"end\":40216,\"start\":40202},{\"end\":40223,\"start\":40216},{\"end\":40230,\"start\":40223},{\"end\":40241,\"start\":40230},{\"end\":40250,\"start\":40241},{\"end\":40879,\"start\":40868},{\"end\":40886,\"start\":40879},{\"end\":40893,\"start\":40886},{\"end\":40902,\"start\":40893},{\"end\":40909,\"start\":40902},{\"end\":41229,\"start\":41221},{\"end\":41237,\"start\":41229},{\"end\":41244,\"start\":41237},{\"end\":41251,\"start\":41244},{\"end\":41258,\"start\":41251},{\"end\":41266,\"start\":41258},{\"end\":41512,\"start\":41505},{\"end\":41519,\"start\":41512},{\"end\":41526,\"start\":41519},{\"end\":41532,\"start\":41526},{\"end\":41538,\"start\":41532},{\"end\":41546,\"start\":41538},{\"end\":41811,\"start\":41803},{\"end\":41817,\"start\":41811},{\"end\":41825,\"start\":41817},{\"end\":41831,\"start\":41825},{\"end\":42086,\"start\":42078},{\"end\":42102,\"start\":42086},{\"end\":42114,\"start\":42102},{\"end\":42130,\"start\":42114},{\"end\":42143,\"start\":42130},{\"end\":42409,\"start\":42399},{\"end\":42419,\"start\":42409},{\"end\":42431,\"start\":42419},{\"end\":42618,\"start\":42610},{\"end\":42624,\"start\":42618},{\"end\":42641,\"start\":42624},{\"end\":42650,\"start\":42641},{\"end\":42869,\"start\":42863},{\"end\":42879,\"start\":42869},{\"end\":43079,\"start\":43068},{\"end\":43088,\"start\":43079},{\"end\":43098,\"start\":43088},{\"end\":43108,\"start\":43098},{\"end\":43119,\"start\":43108},{\"end\":43380,\"start\":43371},{\"end\":43390,\"start\":43380},{\"end\":43644,\"start\":43636},{\"end\":43659,\"start\":43644},{\"end\":44063,\"start\":44056},{\"end\":44070,\"start\":44063},{\"end\":44084,\"start\":44070},{\"end\":44332,\"start\":44325},{\"end\":44339,\"start\":44332},{\"end\":44346,\"start\":44339},{\"end\":44354,\"start\":44346},{\"end\":44360,\"start\":44354},{\"end\":44600,\"start\":44592},{\"end\":44606,\"start\":44600},{\"end\":44612,\"start\":44606},{\"end\":44618,\"start\":44612},{\"end\":44625,\"start\":44618},{\"end\":44631,\"start\":44625},{\"end\":44639,\"start\":44631},{\"end\":44647,\"start\":44639},{\"end\":44656,\"start\":44647},{\"end\":44663,\"start\":44656},{\"end\":44906,\"start\":44898},{\"end\":44917,\"start\":44906},{\"end\":45433,\"start\":45420},{\"end\":45445,\"start\":45433},{\"end\":45684,\"start\":45671},{\"end\":45696,\"start\":45684},{\"end\":45704,\"start\":45696},{\"end\":45715,\"start\":45704},{\"end\":45722,\"start\":45715},{\"end\":45733,\"start\":45722},{\"end\":45746,\"start\":45733},{\"end\":46030,\"start\":46018},{\"end\":46038,\"start\":46030},{\"end\":46051,\"start\":46038},{\"end\":46478,\"start\":46467},{\"end\":46488,\"start\":46478},{\"end\":46841,\"start\":46829},{\"end\":46848,\"start\":46841},{\"end\":46861,\"start\":46848},{\"end\":46869,\"start\":46861},{\"end\":46879,\"start\":46869},{\"end\":47092,\"start\":47080},{\"end\":47100,\"start\":47092},{\"end\":47109,\"start\":47100},{\"end\":47122,\"start\":47109},{\"end\":47140,\"start\":47122},{\"end\":47485,\"start\":47473},{\"end\":47496,\"start\":47485},{\"end\":47511,\"start\":47496},{\"end\":47523,\"start\":47511},{\"end\":48099,\"start\":48092},{\"end\":48108,\"start\":48099},{\"end\":48121,\"start\":48108},{\"end\":48539,\"start\":48527},{\"end\":48547,\"start\":48539},{\"end\":48553,\"start\":48547},{\"end\":48563,\"start\":48553},{\"end\":48571,\"start\":48563},{\"end\":48864,\"start\":48851},{\"end\":48873,\"start\":48864},{\"end\":48884,\"start\":48873},{\"end\":48893,\"start\":48884},{\"end\":49127,\"start\":49115},{\"end\":49137,\"start\":49127},{\"end\":49145,\"start\":49137},{\"end\":49155,\"start\":49145},{\"end\":49459,\"start\":49448},{\"end\":49794,\"start\":49781},{\"end\":49801,\"start\":49794},{\"end\":49809,\"start\":49801},{\"end\":49820,\"start\":49809},{\"end\":50035,\"start\":50027},{\"end\":50049,\"start\":50035},{\"end\":50059,\"start\":50049},{\"end\":50068,\"start\":50059},{\"end\":50281,\"start\":50269}]", "bib_venue": "[{\"end\":40369,\"start\":40315},{\"end\":43792,\"start\":43734},{\"end\":46621,\"start\":46563},{\"end\":47744,\"start\":47672},{\"end\":48282,\"start\":48210},{\"end\":49508,\"start\":49492},{\"end\":36728,\"start\":36725},{\"end\":36930,\"start\":36870},{\"end\":37305,\"start\":37300},{\"end\":37580,\"start\":37553},{\"end\":37780,\"start\":37720},{\"end\":38120,\"start\":38114},{\"end\":38389,\"start\":38377},{\"end\":38674,\"start\":38668},{\"end\":38985,\"start\":38970},{\"end\":39322,\"start\":39316},{\"end\":39568,\"start\":39561},{\"end\":39836,\"start\":39826},{\"end\":40313,\"start\":40250},{\"end\":40919,\"start\":40909},{\"end\":41274,\"start\":41266},{\"end\":41551,\"start\":41546},{\"end\":41836,\"start\":41831},{\"end\":42158,\"start\":42143},{\"end\":42438,\"start\":42431},{\"end\":42656,\"start\":42650},{\"end\":42885,\"start\":42879},{\"end\":43125,\"start\":43119},{\"end\":43405,\"start\":43390},{\"end\":43732,\"start\":43659},{\"end\":44099,\"start\":44084},{\"end\":44323,\"start\":44239},{\"end\":44667,\"start\":44663},{\"end\":44937,\"start\":44917},{\"end\":45168,\"start\":45156},{\"end\":45418,\"start\":45312},{\"end\":45773,\"start\":45746},{\"end\":46059,\"start\":46051},{\"end\":46234,\"start\":46207},{\"end\":46561,\"start\":46488},{\"end\":46885,\"start\":46879},{\"end\":47147,\"start\":47140},{\"end\":47610,\"start\":47523},{\"end\":48208,\"start\":48121},{\"end\":48623,\"start\":48571},{\"end\":48901,\"start\":48893},{\"end\":49161,\"start\":49155},{\"end\":49490,\"start\":49459},{\"end\":49839,\"start\":49820},{\"end\":50084,\"start\":50068},{\"end\":50267,\"start\":50242}]"}}}, "year": 2023, "month": 12, "day": 17}