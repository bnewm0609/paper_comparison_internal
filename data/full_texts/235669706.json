{"id": 235669706, "updated": "2023-10-06 01:25:33.084", "metadata": {"title": "Making the most of small Software Engineering datasets with modern machine learning", "authors": "[{\"first\":\"Julian\",\"last\":\"Prenner\",\"middle\":[\"Aron\"]},{\"first\":\"Romain\",\"last\":\"Robbes\",\"middle\":[]}]", "venue": "IEEE Transactions on Software Engineering", "journal": "IEEE Transactions on Software Engineering", "publication_date": {"year": 2021, "month": 6, "day": 29}, "abstract": "This paper provides a starting point for Software Engineering (SE) researchers and practitioners faced with the problem of training machine learning models on small datasets. Due to the high costs associated with labeling data, in Software Engineering,there exist many small (<1 000 samples) and medium-sized (<100 000 samples) datasets. While deep learning has set the state of the art in many machine learning tasks, it is only recently that it has proven effective on small-sized datasets, primarily thanks to pre-training, a semi-supervised learning technique that leverages abundant unlabelled data alongside scarce labelled data.In this work, we evaluate pre-trained Transformer models on a selection of 13 smaller datasets from the SE literature, covering both,source code and natural language. Our results suggest that pre-trained Transformers are competitive and in some cases superior to previous models, especially for tasks involving natural language; whereas for source code tasks, in particular for very small datasets,traditional machine learning methods often has the edge.In addition, we experiment with several techniques that ought to aid training on small datasets, including active learning, data augmentation, soft labels, self-training and intermediate-task fine-tuning, and issue recommendations on when they are effective. We also release all the data, scripts, and most importantly pre-trained models for the community to reuse on their own datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.15209", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tse/PrennerR22", "doi": "10.1109/tse.2021.3135465"}}, "content": {"source": {"pdf_hash": "c65dcc599f4539295c83b8d12f7bbf9b361e4697", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.15209v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2106.15209", "status": "GREEN"}}, "grobid": {"id": "1f2f68f8cf462f9d383e5264d40c7b85b7b41d68", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c65dcc599f4539295c83b8d12f7bbf9b361e4697.txt", "contents": "\nMaking the most of small Software Engineering datasets with modern machine learning\n\n\nJulian Aron Prenner \nRomain Robbes \nMaking the most of small Software Engineering datasets with modern machine learning\n1Index Terms-Small DatasetsTransformerBERTRoBERTAPre-trainingFine-TuningData AugmentationBack TranslationSoft LabelsActive Learning\nThis paper provides a starting point for Software Engineering (SE) researchers and practitioners faced with the problem of training machine learning models on small datasets. Due to the high costs associated with labeling data, in Software Engineering, there exist many small (< 1 000 samples) and medium-sized (< 100 000 samples) datasets. While deep learning has set the state of the art in many machine learning tasks, it is only recently that it has proven effective on small-sized datasets, primarily thanks to pre-training, a semi-supervised learning technique that leverages abundant unlabelled data alongside scarce labelled data. In this work, we evaluate pre-trained Transformer models on a selection of 13 smaller datasets from the SE literature, covering both, source code and natural language. Our results suggest that pre-trained Transformers are competitive and in some cases superior to previous models, especially for tasks involving natural language; whereas for source code tasks, in particular for very small datasets, traditional machine learning methods often has the edge. In addition, we experiment with several techniques that ought to aid training on small datasets, including active learning, data augmentation, soft labels, self-training and intermediate-task fine-tuning, and issue recommendations on when they are effective. We also release all the data, scripts, and most importantly pre-trained models for the community to reuse on their own datasets.\n\nINTRODUCTION\n\nS MALL datasets are commonplace for many Software Engineering problems. While the creation of a labelled dataset is always a significant undertaking, this is even more the case for Software Engineering. In many cases, significant expert knowledge is required to label Software Engineering data, making it difficult to use crowd-sourcing techniques, as is often done in other fields such as in computer vision [1]. Moreover, some labelling tasks involve detailed (text or source code) understanding, making the labelling of a single example time consuming. Dataset size may be further reduced by the need to label the same examples multiple times and to compute inter-rater agreement. Due to all these factors, it is thus not uncommon for hand-labelled Software Engineering datasets to number only a few thousands or even hundreds of samples. For instance, the 13 datasets used in this work (described in Section 2) range from 200 to 62,275 samples, with three datasets having more than 5,000 samples, and four having less than 1,000.\n\nHistorically, small Software Engineering datasets were used with traditional machine learning algorithms, such as Support Vector Machines (SVMs), Logistic Regression or Random Forests, often combined with manual feature engineering. In recent years, early experiments with deep learning architectures [2]- [5], such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNSs), showed mixed results, suggesting that for many tasks where training data is scarce, deep learning does not provide a clear benefit, especially in light of its considerably higher computational costs.\n\nWhether deep learning is in fact not well suited for these small datasets, and if so, for which kind of tasks and dataset Julian Aron Prenner and Romain Robbes are with the Faculty of Computer Science, Free University of Bozen-Bolzano, Italy sizes is the central question of this paper. The motivation to take a second look at this problem is the recent advent of semi-supervised learning [6]. Semi-supervised learning is a machine learning paradigm in which both labelled data and unlabelled data are leveraged in the learning process, with the latter being much cheaper to acquire. While prevalent in computer vision, it is only since 2018 that semi-supervised learning has become viable in the NLP domain [7], [8], in the form of pre-training. Since 2019, pre-trained Transformerbased models such as BERT [9] or RoBERTa [10] have set many records in NLP and related fields, and considerably improved the state of the art on important benchmarks such as GLUE [11] and SQuAD [12], in which there are small datasets. In addition to pre-training, several additional techniques have the potential to benefit small datasets, including Domain-specific Fine-Tuning, Intermediate-Task fine-tuning, Active Learning, Self-Training, Data Augmentation, and Soft Labels. Thus, a better understanding of when to combine these techniques is necessary. We provide background on the Transformer Architecture, Semi-supervised learning via pre-training and other techniques in Section 3.\n\nWhile previous work showed promising results in applying pre-training to SE problems [13]- [17], this work examines this phenomenon in more depth, by applying the pre-training paradigm on thirteen different small and medium-sized Software Engineering datasets selected from the literature. These datasets span natural language, source code, and source code comments, in a variety of domains (several sentiment analysis tasks, several app review classification tasks, technical debt detection, comment classification, code comment coherence, code smell detection, code readability, code complexity). In addition, and unlike previous work, we also investigate the impact of the additional techniques mentioned above, when they are rele-vant. Section 4 presents methodological details such as preprocessing, baselines, and training, testing and validation modalities, for all the scenarios we consider. This section also presents the pre-trained and fine-tuned models we use in this work, including StackOBERTflow, a Transformer model pre-trained on 26 million Stack Overflow comments.\n\nSection 5 presents the results of the paper, answering the following research questions:\n\n\u2022 RQ1: For which domains and tasks does the pretraining paradigm outperform the baselines? We find that pre-training is effective for tasks working on natural language and source code comments, but is not as effective for tasks working on source code yet. \u2022 RQ2: Which additional techniques are effective, and if so in which circumstances? We find that some techniques, such as domain-specific pre-training, and data augmentation are effective in some (but not all) settings, while we find limited evidence for the effectiveness of others, such as active learning. Finally, we close the paper by documenting the limitations of our study, and the opportunities for additional studies in Section 6. We conclude the work in Section 7, summarizing initial recommendations on the effectiveness of pre-training and the additional techniques. Additional material can be found in three appendices: Appendix A provides additional information on datasets; Appendix B provides results; last but not least, Appendix C provides instructions on how to access the data, scripts, and pretrained models we used in our experiments. These models can be fine-tuned for a wide range of tasks relating to software artifacts; we hope that they will prove useful to other researchers in the field.\n\n\nDATASETS AND RELATED WORK\n\nWe selected thirteen datasets introduced in the Software Engineering literature in recent years, aiming for both variety in terms of artifacts, dataset size, and classification tasks [2], [3], [18]- [26]. Another selection factor was the availability of a comparable baseline or a way to reproduce the initial experiment. Seven datasets involve natural language, two code comments, and the remaining four source code (one with comments). They vary from 341 to 62,275 examples, and from 2 to 16 classes. The datasets cover nine different tasks: a) sentiment classification of software artifacts, such as Stack Overflow comments, app and code reviews b) detection of informative app reviews c) classification of app reviews d) detection of self-admitted technical debt through code comments e) classification of code comments f) prediction of code-comment coherence g) detection of linguistic code smells h) prediction of code runtime complexity and i) prediction of code readability.\n\nNext, we discuss each of these tasks in greater detail. For a more concise overview refer to Table 1.\n\n\nNatural Language Datasets\n\nThese datasets contain mainly natural language, but may occasionally contain some source code identifiers. They are the closest to the original setting for models pre-trained on a generic English corpus, although they come from very specific domains.  \n\n\nSentiment Classification\n\nIn sentiment classification, a model assigns a sentiment class (e.g., one of positive, negative, neutral) to a sentence or short piece of text; in our case the text's domain is related to software development. The dataset compiled by Calefato et al. (Senti4SD) contains 3 097 training and 1 326 test samples each labeled as either positive, negative or neutral; all samples were extracted from questions, answers and comments of Stack Overflow posts. Along with their dataset, the authors also released individual rater annotations, i.e., three rater labels per sample, from which the final labels were obtained by applying a majority vote rule. The dataset is used to evaluate an SVM classifier using word embeddings trained on a Stack Overflow corpus as features; we include this SVM as a baseline (see Table 8).\n\nLin et al. created a sentiment classification datasets consisting of 1,500 sentences (from 178 text fragments) extracted from Stack Overflow discussions. They also adapted two previous datasets of 636 Jira issues (926 sentences), and 130 app reviews (341 sentences) [28], [29]. The JIRA issues dataset has only two sentiment classes (positive and negative) while the other two have an additional neutral class. All three datasets were used in a study of several sentiment analysis tools, and a novel model introduced in the same work [3]; we include all of them as baselines in our compari-son (Table 8). To allow for a comparison with these baselines, all three datasets are only used as test sets in this work.\n\n\nInformative App Review Detection\n\nAn app review is considered informative if it contains valuable information for the application developer, such as feature suggestions or bug reports. The dataset presented by Chen et al. [20] contains 12 000 app reviews belonging to four popular mobile apps from the Google Play Store. The dataset is partitioned into predefined test (2000 samples per app) and train sets (1000 samples per app). Three raters annotated each review as either informative or non-informative, with a majority vote to determine the final label. This work also presented AR-MINER, a tool based on an expectation maximization with Naive Bayes (EMNB) classifier to detect such informative app reviews; this tool is our baseline.\n\n\nApp Review Classification\n\nMaalej et al. [21] compiled a dataset of 4 400 reviews crawled from Apple's App Store and Google Play. Each review was categorized by two raters into one of four classes (bug report, feature request, user experience or rating); reviews with rater disagreement were discarded. The authors experimented with various types of classifiers, finding that an ensemble of binary classifiers performs considerably better than a single multiclass classifier. We only study multiclass classification, for comparability reasons, use the author's multiclass classifier as a baseline. Scalabrino et al. [27] and Villarroel et al. [29] introduce CLAP, a tool for automatic classification and clustering of app reviews that is evaluated on a dataset of 3 000 app reviews created by the same authors. We use the CLAP dataset in a data augmentation experiment.\n\n\nRelated Work\n\nThere is a vast literature on sentiment analysis and classification. For a general overview see e.g., M\u00e4ntyl\u00e4 et al. [30]. Zhang et al. [15] provide an in-depth comparison of pre-trained Transformers with six different sentiment analysis tools, including both, tools for general sentiment analysis and tools specifically targeted towards Software Engineering. The study uses some datasets that we also use (e.g., from Lin et al. [3]), they defined custom training and testing sets while we used them solely for testing, which makes comparisons difficult. While they explored additional Transformer architectures (XLNet [31] and ALBERT [32]), their study was limited to only fine-tuning: they did not investigate the use of task-specific pre-training, nor any other of the additional methods that we study. Ahmed et al. [33], introduced SentiCR, a sentiment analysis tool that uses of Part Of Speech (POS) tags and Gradient Boosting Trees, while Chen et al. [34] present SEntiMoji, a model that leverages emojis to improve SE sentiment classification.\n\nDhinakaran et al. [35] investigated the application of active learning for app review analysis, using the dataset by Maalej et al. They employ traditional machine learning algorithms (naive Bayes, logistic regression, and SVM). A similar experiment, carried out on the same dataset can also be found in this work.\n\n\nDatasets of Code Comments\n\nSource code comments somewhat differ from natural language: they may often contain source code identifiers, code annotations, and specific idioms common in source code documentation.\n\n\nSelf-Admitted Technical Debt Detection\n\nSelf-admitted technical debt (SATD) is technical debt known to and acknowledged by the author. It is often expressed in code comments with a short description of a flaw or shortcoming and sometimes, but not always, marked with specific keywords, such as FIXME, TODO or HACK. Detection of such comments can be useful to assess software quality, aid decision-making or direct further development.\n\nThe dataset by S. Maldonado et al. contains SATD comments extracted from 10 prominent Java projects (for more details, see Table 15 in Appendix A). With over 60 000 samples, it is by far the largest dataset used in this work. Each dataset sample is assigned to one of five SATD categories informed by an established ontology of technical debt [36] (design debt, requirement debt, defect debt, documentation debt or test debt) or labeled as not containing any SATD at all. In this work, we concentrate on the binary version of the problem (detecting presence of SATD), for two reasons: (1) because there are large class imbalances (i.e., the document debt class makes up less than 0.1% of the total data), and (2) to compare performance to binary classifiers from previous work. In addition to introducing this dataset, the authors also perform various SATD detection and classification experiments using traditional NLP and machine learning methods; we use their SATD detection model as a baseline. We also include the CNN from Ren et al. [4] as a second baseline.\n\n\nComment Classification\n\nPascarella and Bacchelli [23] released a dataset of over 11 000 comments from open-source Java projects classified according to a taxonomy of 16 different comment categories. They evaluate their dataset on a multinomial Naive Bayes classifier which serves as our baseline for this task.\n\n\nRelated Work\n\nFor a more general survey on self-admitted technical debt see e.g., Sierra et al. [37]. Santos et al. [5] use a Long short-term memory network (LSTM) to classify SATD, also making use of the dataset by S. Maldonado et al.; since we confined ourselves to SATD detection this work was not included. A text-mining based approach to SATD detection can be found in [38], [39]; in this work, unfortunately, only a subset of the dataset was used and results are thus not comparable. In a closely related comment classification work, Pascarella [40] focus on comment classification in mobile applications.\n\n\nDatasets of Source Code\n\nThese datasets significantly differ from natural language: on the one hand, code has a very specific and unambiguous syntax and are much more repetitive than natural language [41]; on the other hand, code has very complex semantics, and has many identifiers, leading to vocabulary issues [42].\n\n\nCode-Comment Coherence Prediction\n\nCorazza et al. [24], [43] and Cimasa et al. [44] examine the concept of code-comment coherence, i.e., the \"relatedness\" of a method's code and its lead comment. The authors introduce a dataset of 2 883 Java methods along with their leading comment and a binary label indicating whether coherence exists between the two or not [24].In follow-up work, the authors trained an SVM classifier on their dataset using features based on tf-idf [43] and later word embedings [44]. We include both of these models as baselines.\n\n\nLinguistic Smell Detection\n\nFakhoury et al. [2] examine the automatic detection of linguistic code smells (also known as linguistic antipatterns), that is, code smells emerging from the use of misleading identifier names or the violation of common naming conventions. Examples of this are variable names as if they were lists or arrays when in fact they have a scalar type, or getter methods with side effects.\n\nThey labeled a dataset of roughly 1 700 code snippets, following a taxonomy of linguistic smells [45], which comprises 18 different types of linguistic antipatterns. They then trained a number of models on this dataset and compared their performances. These models include CNNs in various configurations, SVMs with different kernels and a Random Forest classifier. All models are binary, that is, they only determine whether a given sample is \"smelly\" or not and do distinguish between different types of antipatterns. Interestingly, the authors found that a thoroughly tuned SVM model outstrips the CNN in all its configurations. This not only in terms of performance metrics but also in terms of memory consumption, training time and ease of use. Their best-performing models were selected as baselines.\n\n\nCode Runtime Complexity Classification\n\nSikka et al. [25] investigate the use of machine learning to automatically predict the code runtime complexity class (e.g., O(n 2 )) of short programs. To this end, they collected 933 Java implementations of various algorithms from a competitive programming platform and annotated each with the corresponding complexity class (i.e., one of O(1), O(log n), O(n), O(n log n), O(n 2 )). They experiment with various traditional machine learning approaches, training, such as Random Forests and SVMs on manually engineered features (such as numbers of loops, numbers of variables etc.) and code embeddings obtained from the programs' abstract syntax tree through graph2vec [46].\n\n\nCode Readability Prediction\n\nWhat constitutes readable code and what does not, seems to be largely a matter of personal taste. Notwithstanding this, research by Buse and Weimer [47] suggests that code readability can, at least in part, be measured objectively.\n\nA relatively small number of papers [26], [47]- [50] examine models for automatic code readability estimation. Most recently, Scalabrino et al. [26], [50] compiled a dataset by letting 30 Computer Science students rate the readability of 200 methods, previously selected from well-known Java projects. Each method received 9 readability ratings: these ratings were then averaged and compared against a threshold value to assign a single binary readability label. Further, they developed a logistic regression model for code readability estimation by combining structural readability features proposed in Buse and Weimer [47] and Dorn [49] with novel textual features. We base all of our experiments on above dataset; previous datasets by Buse and Weimer [47] and Dorn [49] were not included due to lack of baselines suitable for comparison.\n\n\nRelated Work\n\nWang et al. [51] analyzed code-comment coherence by means of a Bi-LSTM model which was evaluated also on the above dataset. However, because a different evaluation methodology was used, a direct comparison was not possible. Arnaoudova et al. [45], in addition to introducing the already mentioned code smell taxonomy, provide an exhaustive treatment of this subject, including also an in-depth empirical study of how developers perceive such smells. A system to detect linguistic code smells in infrastructure as code scripts was developed by Borovits et al. [52]. There exists further literature on smell detection in a broader context: for instance, Fontana et al. [53] presented a machine learning approach to code smell detection whose results, however, were called into question in a later replication study [54]. More recently, Sharma et al. [55] used deep learning models such as CNNs and LSTM networks to detect code smells in C# and Java code. Also related, is work done by Arcelli Fontana and Zanoni [56], who experiment with machine learning models for code smell severity prediction, which can be considered an extension of the simpler detection task.\n\n\nBACKGROUND\n\nThis section provides an overview of the various machine learning techniques that we investigate in order to evaluate their effectiveness starting with the pre-training paradigm, then covering self-training, data augmentation, active learning, and soft labels. We also highlight the uses of pretraining in software engineering.\n\n\nPre-training with The Transformer and BERT\n\nThe Transformer architecture Transformer [57] networks are a relatively recent architecture, particularly popular in the domain of natural language processing (NLP). They have replaced Long short-term memory (LSTM) networks as the prevailing architecture for text-based data. Transformers are based on attention, a mechanism previously used in LSTM networks to align the information flow between the encoder and decoder part of the network [58]. Attention allows a model to connect related parts of a sentence and form complex structures of interdependence between them. Unlike LSTMs, Transformer networks are not recurrent and instead have a fixed-size input window of tokens (typically 512 tokens): this allows for more efficient training and avoids vanishing-gradient or long-dependency problems extant in recurrent architectures. To summarize, each layer of the Transformer uses the attention mechanism to learn relationships between its inputs, which, in the case of the first layer are the input tokens; when used for classification, a final fully-connected layer is used as output layer.\n\n\nPre-training via Language Modelling\n\nBERT [9] (Bi-Directional Encoder Representations from Transformers) is an extension of the Transformer architecture and comes with a specific semi-supervised learning training regimen: BERT heavily relies on pre-training, a form of unsupervised learning, before being fine-tuned on a downstream task in a classical supervised fashion.\n\nDuring pre-training, BERT is trained on large amounts of unlabeled data via Mask Language Modelling (MLM). MLM is a prediction task where some of the input tokens are randomly replaced by blanks (\"masked\") and the model is trained to predict the tokens behind these blanks, taking into account the textual context on both sides of the blank (see the BERT paper for more details on the pre-training itself [9]). Intuitively, this general task is supposed to initialize the weights to a state in which certain general concepts and relationships useful for a large number of downstream tasks are already present: BERT learns a Representation of the tokens. Unlike word embeddings [59], these are contextual representations: they depend both on the token, and its surrounding tokens.\n\nOf note, earlier work also used Language Modelling as a pre-training task (ELMo and ULMFit [7], [8]) with LSTMs, and were used with some varying amount of success in Software Engineering [13], [14]. BERT's pre-training is more efficient for two reasons: BERT's bidirectional architecture uses the context before and after the token, whereas LSTMs use only the context before the token; and BERT uses Byte-Pair Encoding (BPE) [60] to tokenise text in subwords rather than entire words, leading to better modelling of the vocabulary (see previous work by Karampatsis et al. for an extended discussion of this aspect for source code [42]).\n\nRoBERTa [10] is a refinement of BERT, in particular relating to its pre-training regimen (e.g., RoBERTa uses a larger pre-training corpus, dynamic masking, and a variation of the pre-training task) and with only minor architectural changes (RoBERTa uses Byte-level BPE tokenization, rather than character-level BPE).\n\n\nFine-tuning\n\nBoth BERT and RoBERTa are hardly ever trained from scratch. Instead, starting from a pre-trained model with pre-initialized weights, the model weights are further finetuned by training on task-specific labeled data (called a downstream task). This involves replacing the last layer of the model (useful for the pre-training task), with a taskspecific layer, and resuming training. The model can leverage the pre-trained representations to be able to learn the downstream task effectively, even with a limited amount of data, allowing BERT and RoBERTa to set the state of the art on NLP benchmarks, even on tasks with limited data (the GLUE benchmark [11] includes several task with less than 10,000 examples).\n\n\nImpact of the Pre-training corpora\n\nThe standard BERT and RoBERTa models have both been pre-trained on a large English natural language corpus, with several models available in various sizes. There exist pretrained BERT models for many other natural languages and even programming languages [61]. Intuitively, one would expect a generic pre-training corpus to be a \"jack of all trades, master of none\", with a more specific pre-training corpus to be more suited for more specific domains (such as software engineering). There is evidence of this for word embeddings in Software Engineering [62], but how much of an impact a domain-specific pre-training corpus has for a BERT or RoBERTa model is still an open question, which we investigate. Of note, the ULMFit approach [8] continues the pre-training task on the task-specific data (without using labels), before the actual fine-tuning, finding that it does improve performance.\n\n\nAdditional Techniques\n\nIntermediate-Task Fine-Tuning Intermediate-task fine-tuning (ITT), also known as two stage fine-tuning, STILTs [63], or TANDA [64] is a technique whereby the model is fine-tuned twice (with labeled data): first on an intermediate task, a task different from but closely related to the target task, and finally on the actual target task (e.g., training for sentiment analysis on movies, before switching to sentiment analysis on books). This is particularly attractive whenever only little data is available for the target task whilst large amounts of data are available for a similar, possibly slightly simpler, but different intermediate task. The idea is that the target task might benefit from \"knowledge\" that the model acquired during intermediatetask training. Pruksachatkun et al. [65] presents a survey on when this method offers good prospects in NLP.\n\n\nSelf-Training\n\nSelf-training (also known as self-labelling or selflearning) [66], [67], is a very simple semi-supervised learning method. It can be explained as follows: A model is first trained on a (possibly too small) labeled dataset. Next, this model is used to evaluate a number of additional unlabeled samples. The model's predictions for these unlabeled samples are then simply used as their gold labels. We now have additional labeled data, albeit noisier ones; after adding it to the original dataset we retrain the model. Predictions can be filtered by confidence to reduce the probability of introducing noise into the training set.\n\n\nData Augmentation and Back-Translation\n\nData augmentation is a well-known technique to increase the amount of labeled data without any human labeling effort, which is especially valuable in cases where training data is in short supply. It works by adding slightly varied copies of already existing, labeled samples to the dataset, assuming the variations do not affect the label. The technique was first used in computer vision, where data augmentations are easier to define, such as flipping images horizontally (a dog looking left instead of right), or cropping images randomly (a closeup of the dog's head should still be classified as a dog). For text data, several such methods for augmentation have been proposed in recent years, among others: a) replacing words with synonyms [68], [69], b) replacing, adding or deleting words randomly [69], c) replacing words with the nearest neighbor in an embedding space [70], [71], d) replacing words with predictions from a masked language model such as BERT [72], e) translating into an intermediate language and then back into the source language (back-translation) [73]. Augmentation is typically applied at training time by simply adding the augmented samples to the training set and then proceeding as usual. Alternatively, augmentation can also be carried out at test time by aggregating (e.g., averaging) the prediction for an original test sample with the predictions for its augmented copies, thus obtaining potentially more stable or more accurate predictions. Figure 1 shows an example of augmentation through back-translation: a sample in the dataset (here an English sentence) is translated into German and French, then back into English, causing slight variations.\n\n\nActive Learning\n\nThe goal of active learning is to make the process of manual data labeling more efficient. Active learning avoids presenting samples to the rater that the model is likely to classify correctly and thus provide little new information.\n\nInitially, a human rater labels a small number of samples, called the seed. There is also a second, larger set of yet unlabeled samples, called the pool. A model is first trained on the seed. In a next step, this model is used to select those samples from the pool that the model found most \"difficult\" to classify. \"Difficulty\" is measured by means of a confidence or acquisition function which calculates a confidence score from the model's prediction. In classification, this is usually a distribution over the target classes and acquisition functions are thus applied to class probabilities. Selecting samples by confidence score is called confidence sampling. The rater then labels the selected samples, which are then removed from the pool and added to the model's training set. This process is repeated until a satisfying number of samples have been labeled or the model reaches a particular target accuracy. A possible problem with confidence sampling is that selected samples, albeit being difficult for the model, might all be very similar, reducing the efficiency of the process. Confidence sampling is often paired with diversity sampling: selected samples are subsequently filtered for diversity, for instance using a clustering algorithm such as k-means. A common way to evaluate and compare active learning approaches is a simulation with an already labeled dataset. See Settles [74] for an overview of variants and extensions of active learning.\n\n\nSoft Labels\n\nIn classification, usually, every sample is associated with a single target class. For many machine learning algorithms, in particular for neural networks, the target label of a sample is represented as a probability distribution over classes. While optimizations exist for handling the common singlelabel case, conceptually we can say that the target label is denoted by a distribution vector which assigns probability one to the class it belongs to and probability zero to all other classes. Take, for example, a classification problem where each sample belongs to either class A, B or C. A sample will have the target vector (A: 0, B: 1, C: 0) if it belongs to class B and (A: 0, B: 0, C: 1) if it belongs to class C.\n\nThe term soft label is used when this distribution vector is fuzzy, i.e., is not comprised of a single one and many zeros. Intuitively, this means that a sample can belong to multiple classes, with a degree expressed by the class probabilities: a target distribution such as (A: 0.4, B: 0.6, C:0) belongs to both, class B and class A. Most datasets do not come with soft-labels. In cases where each sample in a dataset was classified by multiple raters (which is common in order to compute inter-rater agreement), instead of using a majority vote, the rater's votes can be converted into a soft-label. Intuitively, an example in which raters disagree can be seen as more ambiguous. Providing this information to the model can help it differentiate between \"easy\" examples and \"hard\" examples.\n\n\nUses of Pre-training and Transformers in Software Engineering\n\nIn previous work, we investigated the usefulness of the pre-training paradigm (using the earlier ULMFit approach [8]), finding it promising in limited sentiment analysis experiments [13]. Mahadi et al. [14] experimented with crossdataset classification of design discussions, but had mixed results.\n\nZhang et al. [15] provide a detailed study on the use of Transformers for sentiment analysis of Software Engineering artifacts, comparing existing sentiment analysis tools with Transformer models (BERT, RoBERTa, XLNet). Biswas et al. [17] pursue a similar avenue, training BERT on a newly compiled dataset of 4 000 sentences from Stack Overflow discussions and comparing results with recurrent models.\n\nHey et al. [16] present NoRBERT, a BERT model finetuned to classify functional and non-functional requirements that achieves results competitive to state-of-the-art models on the PROMISE NFR dataset.\n\nKeim et al. [75] attempt to use a standard BERT (i.e., pretrained on English natural language) for the detection of architectural tactics in Java code and report mixed results that lag behind state-of-the-art approaches in one case study.\n\nSvyatkovskiy et al. [76] introduce IntelliCode compose, a system for intelligent code completion based on the GPT-2 Transformer language model. Finally, Feng et al. [61] present CodeBERT, a RoBERTabased Transformer that was trained on natural language and code (bimodal), allowing for code-related tasks that also involve natural language, such as code search or documentation generation.\n\n\nMETHODOLOGY\n\nThis section covers general aspects of the methodology, that apply to all the experiments. To ease readability, methodological details that refer to a specific technique (e.g., active learning) are described jointly with the results of this technique in Section 5.\n\n\nPre-Trained models\n\nOff-the-shelf models We use several \"off-the-shelf\" pre-trained model, which were trained on a corpus of generic English text.\n\n\u2022 BERT-base, a 12 layer Transformer model ( 110 million parameters), pre-trained on a 3.3 billion words from books and Wikipedia [9]. \u2022 BERT-large, a 24 layer Transformer model ( 340 million parameters), pre-trained on the same corpus [9]. \u2022 DistillRoBERTa, a 6 layer Transformer model ( 82 million parameters), a compressed version of the larger RoBERTa model.\n\n\nDomain-specific models\n\nWe pre-train a range of Transformers on data from different domains and of different sizes, to gain insights on the effectiveness of pre-training on domain-specific data. One model (StackOBERTflow) is entirely trained on domainspecific data, while the others are \"off-the-shelf\" models pre-trained on English, that are further pre-trained on some domain-specific data. \n\n\nSource code models\n\nFor the code tasks, we use the following two models: model takes as input pairs of natural language and code and primarily targets code-related tasks that also involve natural language (e.g., code search or summarization). Since none of our code experiments involves natural language, we use an empty string as natural language input, except for the code-comment coherence task, where, after stripping comment markers, comments are treated as natural language.\n\u2022 CodeBERTa 2 ,\n\nScratch model\n\nTo get a rough estimate of the effect of pre-training, we also train, for the sentiment classification task, a randomly initialized small RoBERTa model solely on the training set.\n\n\nPreprocessing\n\nPre-trained models do not require extensive pre-processing, such as stemming or removing stop words. In fact, these may be harmful to performance, as the models were pretrained on data that was not pre-processed. In addition, large neural networks have enough parameter capacity to pick up on subtleties such as word order and negation. Thus for most of the datasets, in line with the practice, we did very little preprocessing. For the sentiment analysis and app review datasets, we used raw, unpreprocessed input. For the app review classification tasks we concatenated the review title and body and prepended the review's rating (a number in the range 1-5). We applied heavier preprocessing for tasks with code comment input. Here, similarly to [22], we removed newlines, comment delimiters (such as //, / * , * /), stripped HTML tags and removed all punctuation except periods and question marks as well as repeated whitespace characters. Our goal was to reduce the length of the comments to fit in the Transformer's input window: each punctuation mark is treated as an additional token, taking away a spot in the window. Preprocessing was also necessary for some source code tasks. In the code-comment coherence task, we simply took the concatenation of the lead comment and the method body as input. Moreover, we reformated all code files in the complexity prediction dataset using Google's Java code formatter 3 such that, for instance, all of them use the same indentation width. For the remaining dataset and tasks no preprocessing was done.\n\nAfter this, we used each pre-trained model's tokenizer to properly segment the data in the subword units specific to this model, as each model may have a different vocabulary.\n\n\nDataset Partitioning and Evaluation\n\nWe tried to replicate the baseline models' training and evaluation methodology as closely as possible. Independent of the different evaluation strategies, we repeat all of our experiments at least three times with varying random seeds and average results to reduce noise.\n\n\nSentiment classification.\n\nWe trained our models on the predefined training set of the Senti4SD dataset [18], 30% of which we use for validation; the corresponding test set was used for testing. All the remaining sentiment analysis 3. https://github.com/google/google-java-format datasets were solely used as test sets (as was done in previous work). Whenever a test set lacked a neutral sentiment class, as was the case for the JIRA issues dataset, we treated neutral predictions from the model as negative.\n\nInformative app reviews. A predefined train-test split is also given for the informative app reviews detection task. Here, the test set is actually larger than the training set (2 000 and 1 000 samples, respectively). We used 15% of the training set for validation.\n\nApp review classification. We used Monte Carlo crossvalidation: we split the dataset in 10 random training and validation partitions with a ratio of 70:30. Reported results are averages over 10 runs.\n\n\nSATD.\n\nWe use cross-validation, with a 9 \u2192 1 cross-project setting: we train on 9 out of the 10 total projects; the remaining project acts a test set.\n\nCode-comment coherence. The model in [43] was trained on 75% of the dataset while the remaining 25% were used for testing. Because this train-test split was chosen randomly, an exact comparison is not possible. To obtain more stable performance metrics, we re-evaluated this baseline model with three random train-splits and averaged the results. We train our own model in the same way, and use 10% of the training data for validation.\n\nLinguistic code smells. As pointed out in the initial work, the leave-one-out cross validation strategy used for training the linguistic smell detection baselines is prohibitively expensive for a deep neural network. We resorted to 15-fold cross validation, putting the baselines at a slight advantage: they were trained on over 99% of the entire dataset, while our model uses only 93% of the data.\n\nOther datasets. Finally, the code runtime complexity prediction, comment classification and code readability prediction datasets were evaluated using k-fold cross-validation with k equal to 5, 10 and 10 respectively.\n\n\nFine-Tuning and Testing\n\nWe tried several different hyper-parameters on the Senti4SD dataset, varying learning rate (2e-5, 4e-5, 5e-5, 5e-5), batch size (8,12,16,32), and drop-out rate (0.05, 0.07, 0.1). We found that general recommendations give good results in most cases (e.g., a learning rate of 5e-5 for fine-tuning BERT). Interestingly, on Senti4SD a configuration with a relatively small batch size of 12 worked best. On the remaining tasks batch size was selected so as to fill the available GPU memory (16-48 depending on GPU and task). This means that for medium-sized models, the batch size was halved with respect to small models, such as CodeBERTa or StackOBERTflow. We also reduced the Transformers input window size from 512 to 256 tokens on datasets where input sentences where so short that the bulk of them fit this narrower window; a smaller window reduces memory consumption (and thus allows for a larger batch size) and speeds up training. Exceptionally long samples that occurred occasionally were truncated to the used window size. We stopped training after validation performance converged, which usually happened after 4-6 epochs. Sentiment Classification [3], [18], [19] Informative App Review Detection [20] App Review Classification [21], [27] Self-Admitted Technical Debt Detection [22] Comment Classification [23] Code-Comment Coherence Prediction [24] Linguistic Smell Detection [2] Code Runtime Complexity Classification [25] Code Readability Prediction [26] Outcome Domain-specific pre-training Active Learning\n\nBack-translation Augmentation\n\n\nSelf-Training\n\nIntermediate-Task Fine-tuning\n\n\nLabel-Smoothing & Soft-Labels\n\nTask-Specific Tokens = below baseline = close to baseline = above baseline = no clear benefit = little benefit = likely benefit \n\n\nRESULTS AND DISCUSSION\n\nWe first start by giving an overview of the performance on each task compared to the available baseline, before diving into the details of the impact on the performance of each of the techniques that we investigate. Of note, due to limitations in the datasets and the run-time needed for each experiment, we were limited in the number of experiments we could run for each additional technique. For an overview of our results, refer to Table 2.\n\n\nComparison with Baselines\n\nSentiment Analysis. On the sentiment analysis datasets, Transformers are ahead of previous methods on most datasets (Table 8). In particular, this is also true for the slightly out-of-domain datasets, such as the JIRA dataset, which the Transformers were not directly trained on, with one exception: Transformers lag behind SentiStrength on the second Stack Overflow test set, but only in terms of F1 (by less than 1%), not accuracy.\n\n\nApp Review Analysis.\n\nIn both, app review classification (Table 10) and informative app review detection (Table 3) Transformer models clearly outperform baselines. The BERT model that was further pre-trained on app reviews is in the lead, but all of the Transformers manage to improve upon baselines. Finally, also on the CLAP dataset StackOBERTflow-comments was able to achieve a 5% higher macro F1 score over the previous random forest model proposed by Scalabrino et al. [27] (Table 11).\n\nSATD. The Transformer models are able to outperform the CNN model by Ren et al. [4], which, to the best of our knowledge, represents the current state of the art (see Table 6). The dataset contains a considerable number of exact duplicates and near duplicates (those arising after preprocessing): we report results with and without removal of such duplicates; we do not know whether baseline have been trained with or without such duplicates. Comment Classification. The Naive Bayes baseline lags behind all three Transformer models (StackOBERTflowcomments, standard BERT, and a domain-specific pretrained BERT) by a margin of 4% (Table 7). A BERT model further pre-trained on task-specific data (i.e., Java comments) performed slightly worse than standard BERT.\n\nCode-Comment Coherence. The SVM baseline by Corazza et al. [43] performs better than the CodeBERTa Transformer, even when employing intermediate-task training (+1% accuracy, Table 13). In a later work, Cimasa et al. [44] experiment with word embeddings: the resulting baseline is weaker than their first and outperformed by the Transformer.\n\n\nLinguistic Smell Detection.\n\nWe compare with the baselines established by Fakhoury et al. [2] in Table 4. CodeBERTa is able to outperform the manually tuned SVM and the also CNN but clearly remains behind the SMO (sequential minimal optimization) model that was automatically tuned using Bayesian optimization (through Auto-Weka [78]).\n\n\nRuntime Complexity Classification.\n\nThe complexity classification task was the only code task where the Transformer exceeded all baselines (Table 5), including the Random Forest classifier and the SVM trained on AST embeddings.\n\n\nCode Readability Prediction.\n\nThe logistic regression baseline trained on manually engineered features by Scalabrino et al. [50] is out of reach for the Transformer: the accuracy achieved by the baseline is over 10% higher (Table 17). From all the selected tasks, the readability prediction task was the hardest for the Transformer.  \n\n\nModel\n\n\nFacebook\n\n\nPre-Training\n\nOur results suggest that, in particular for natural language tasks, the most promising approach seems to be to further pre-train models already pre-trained on general English. When available, in-domain data should be used for pretraining, but even close-to-domain data can yield good improvements. For instance, pre-training BERT on Stack Overflow comments helped to improve accuracy also on the app review dataset (Table 8). Further pre-training an already pre-trained model should also be preferred over pre-training from scratch: The further pre-trained models outperformed our model pre-trained from scratch for most tasks and metrics even though pre-training from scratch required considerably more training time and (unlabeled) training data. This comparison comes with a grain of salt: our further pre-trained models have twice as many layers as our pre-trained-from-scratch model, which, in turn, has a much larger vocabulary (52 000 vs 30 522). While not having a larger model pre-trained from scratch is a limitation of this work, it also highlights how expensive it is. What speaks for our small model, and for small models in general, is of course their size: with only half the layers, training and evaluation is roughly twice as fast, the memory footprint is much smaller and, depending on the task, the performance hit may be acceptable.\n\nOur experiments also indicate that further pre-training is effective even with relatively small amounts data. In the sentiment classification task as little as 150 MB (1 million samples) of pre-training data seems to be sufficient and able to \"saturate\" the model. Doubling the amount of pretraining data resulted in virtually negligible improvements (see BERT-SO-1M versus BERT-SO-2M in Table 8). Similarly, for our large model (BERT-SO-1M-large) improvements are marginal: on the Senti4SD test set, i.e., the test set that \"matches\" the training set, it outperforms the base-sized models by only 0.6%, while on the other test tests it lags behind them.  Interestingly, our BERT-comments model, a general English model further pre-trained on Java comments, performs slightly worse than the same model without this taskspecific pre-training (i.e., a standard BERT) on both datasets it was applied to (Tables 7 and 6). As to why this is the case we can only speculate: A possible explanation is that the comments in our pre-training dataset are very repetitive and have low linguistic diversity (e.g., Java docstrings). Thus, the model might have unlearned some of its general language capabilities during task-specific pre-training. Figure 2 demonstrates that pre-training is essential: a randomly initialized model not only converges much slower, it also has higher variance and typically reaches much lower peak performance. In sum, our experiments show that there is very little reason not to use an already pre-trained, general natural language model as the basis for further domain-specific pre-training and should in most cases be preferred over pre-training from scratch, which, in relation to training time, hardly seems worth the effort.\n\nPre-training is essential. Further pre-training a generic model on domain-specific data is often beneficial, and is much more effective than pre-training from scratch.\n\n\nSoft Labels\n\nSince Calefato et al. [18] released multi-rater labels (three per sample) along with the majority label, we conducted a soft label experiment. For instance, if one voter assigned the positive label to a sentence, while two raters assigned neutral, majority voting would label it as neutral. Instead, the soft label captures all three rater labels, assigning the distribution: (positive: 0.33, negative: 0, neutral: 0.66) to the sentence.\n\nWe train a subset of the models with hard-labels and softlabels on Senti4SD, and evaluate on all sentiment analysis datasets (see Table 9). The results look rather promising: on the Senti4SD test set, training with all three rater labels resulted in an increase of 0.5%. On the JIRA test set, soft labels yielded an improvement of 1%. On the other hand, performance dropped on the second Stack Overflow test set. While these improvements are not certain, and might seem modest, they come almost for free. Whenever multirater labels are available, we recommend to tentatively use them in this way and encourage creators of datasets to also release labels of individual raters.\n\nWhen available, individual rater labels may improve performance at very low cost. \n\n\nBack-Translation Augmentation\n\nWe performed back-translation experiments on the sentiment and app review classification tasks by translating the entire datasets into French, German and Russian using Google Translate and from these languages back into English (see Figure 1 for an example). For the CLAP and Senti4SD datasets we do training-time and test-time augmentation, both, separately and combined, using the Stack-OBERTflow model (Table 11). On the other app review dataset [21] we do training-time augmentation alone, and combine it with test-time augmentation: here the the experiment is carried out on several different models (  Back-translation augmentation led to a clear increase in F1 and accuracy on the CLAP app review dataset, in particular when training and testing time augmentation were combined (+1.1% accuracy, Table 11). On Senti4SD, data augmentation yields modest improvements (+0.3%); in fact, augmenting at test time only caused a slight drop in performance. Table 10 suggests that the effect of backtranslation augmentation depends on the model and pretraining choice. With train-time augmentation only, we see a modest increase of 0.6% in F1 for our small StackOBERTflow model and 1.1% on a general BERT model, while BERTreviews shows better performance without augmentation. The latter does however benefit from combined augmentation (+0.4%). However, the question of whether in general task-specific pre-training diminishes the effects of data augmentation cannot be answered given this limited data and would require further experiments.\n\nWhen possible, back translation yields improvements, particularly if used at both training and test time.\n\n\nActive Learning\nCLC (x)= n n\u22121 (1 \u2212 P \u03b8 (y * 1 |x)) CMC (x)=1 \u2212 (P \u03b8 (y * 1 |x) \u2212 P \u03b8 (y * 2 |x)) CRC (x)= P \u03b8 (y * 1 |x) P \u03b8 (y * 2 |x) CE(x)=\u2212 1 log 2 (n) n i=1 P \u03b8 (yi|x)log2(P \u03b8 (yi|x)) C rand (x)=rand([0, 1])\n\nTABLE 12\n\nAcquisition functions used in our active learning experiment, adapted from [79]: least confidence (C LC ), margin of confidence (C M C ), ratio of confidence (C RC ), entropy (C E ) and random confidence (C rand ). y * 1 and y * 2 are the classes with highest and second highest probabilities, respectively; n is the number of classes. All functions have range [0, 1].\n\nWe try active learning on the Senti4SD sentiment analysis dataset and an app review dataset. In both cases we compare several acquisition functions (Table 12). We carry out the experiment as follows: initially we split the training set into the seed set D seed containing 5% of all samples and the pool set D pool , containing the remaining samples. We let D train := D seed and train the model. Then we evaluate D pool as well as the test set on this model. Next, for each x \u2208 D pool we calculate a confidence score by applying the acquisition function (from Table 12). After that, we let D top be the k = 180 samples with the highest confidence score. We remove these samples from D pool and add them to D train This procedure is repeated until D pool is empty. Test Set (c = 3) Fig. 3. Accuracy of D pool and the test set at each iteration of the active learning process for different acquisition functions with (c = 3) and without (c = 1) diversity sampling for the sentiment classification task (Senti4SD). Error bands are 95% confidence intervals. Same plot for the review classification task can be found in the appendix ( Figure 5)\nCLC CMC CRC CE Crand\nWe also combine confidence sampling with diversity sampling to avoid introducing similar samples into the pool. Instead of k samples, we select c \u00b7 k samples from the pool, where c determines the cluster size. We use the k-means algorithm to cluster the c \u00b7 k samples into k clusters, each of size c. We then select a single sample from each cluster, for a total of k samples. Then, we proceed as above. Figure 3 shows evaluation results at each iteration step with a cluster size of c = 3 for the sentiment classification task. The outcome of our active learning experiments remained behind expectations: in both tasks, neither confidence sampling alone nor confidence and diversity sampling combined showed an appreciable advantage over the random baseline. The choice of acquisition function did not seem crucial, but a more systematic study would be needed to draw more solid conclusions. On the other hand, the plot of pool accuracy (top left) indicates that the active learning process worked as expected: a random acquisition function without diversity sampling had constant performance.\n\nActive learning simulations did not improve results.\n\n\nSelf-Training\n\nWe investigate the use of self-training for the SATD detection task. We extract 350 000 comments from various popular Java libraries and frameworks. Then we train a classifier model on the entire original dataset, which we use to classify the 350 000 comments as either technical debt or not technical debt. We only keep the 7 904 positive comments, i.e., those classified as technical debt, and discard all other samples to avoid increasing the class imbalance already extant in the original dataset. For each positive sample we calculate a confidence score using C LC (Table 12). We take the top 5%, and 80% most confidently classified comments, equal to 6 092 and 7 880 additional comments, respectively, and add them to the original training set. Finally, the model is trained and evaluated on this extended training set. Recall 0% 5% 80% Fig. 4. Precision, recall and F1 under different self-training settings for the Apache Ant project. Error bands are 95% confidence intervals. Figure 4 shows that self-training increases recall (and possibly F1) but causes precision to drop. This is, of course, not surprising: the added samples increase dataset variance which likely explains an increased recall. Similarly, the precision drop can be explained by the lower quality selftraining labels. Thus, one can tune the precision-recall tradeoff according to task-specific needs, such as when a recall is more important than precision, or the model's precision is high enough to be partly sacrificed for better recall. Figure 4 shows different self-training settings for Apache Ant: F1 score went up 4% (precision: -1%, recall +10%), when using a 5% confidence threshold. The change in F1 strongly depends on the confidence threshold and varies across projects: EMF sees an 8% F1 drop (precision: \u221228%, recall: +15%); other projects range from \u22121% to +4%.\n\nSelf-training increases recall at the expense of precision, but the confidence threshold should be tuned carefully.\n\n\nIntermediate-Task Training\n\nWe evaluate Intermediate-Task Training (ITT) on the codecomment coherence task, as it is the only setting for which we could define such an intermediate task. We use 38 000 Java methods along with their lead comments from the CodeSearchNet [77] dataset. We assign half of the methods to their actual lead comments (assumed to be coherent) and shuffle the other half randomly (thus assumed to be incoherent). The model is then fine-tuned on the intermediate task of detecting whether a method was paired with its true lead comment or a random one. Finally, we fine-tune on the code-comment coherence dataset as usual.\n\n\nModel\n\nAcc. AUC  ITT improved the performance of the Transformer model on the code-comment coherence task (Table 13): we observed a modest rise in AUC of 0.8%, and a slightly higher increase in accuracy (+1.1%). If an appropriate intermediate task for the task at hand can be found, ITT can be done with relatively little effort: in our case, the training procedure for the intermediate-task was mostly identical to the one for the target task; the bulk of the work consisted in generating the intermediate-task dataset (e.g. selecting, shuffling and preprocessing the data from CodeSearchNet).\n\nWhen applicable, ITT may improve performance; however finding a suitable intermediate task may be difficult.\n\n\nDISCUSSION\n\n\nSummary and implications of the results\n\n\nTypes of datasets.\n\nOverall, we see that Transformers work very well for natural language datasets, but that performance on source code is \"hit or miss\". This comes with caveats: the models used so far are multilingual, which might reduce performance. They are also trained with less computational resources, and on an order of magnitude less data: While CodeSearchNet is around 1.7 GB, the training data for BERT is around 16GB, while it is 160GB for RoBERTa (CodeSearchNet). Models where also small (6 layers), or had a dual input (text and code). We would expect a Java-specific model trained on a similar size corpus as BERT to perform better. Moreover, source code is quite different from natural language: code snippets are often larger than sentences, and much more structured. This might pose limits on what an unstructured model might achieve. Recent adaptations of the Transformer architecture (e.g. [80], [81]) allow the model to better make use of the tree-like structure of code. Investigating such code-specific architectures in connection with small datasets remains an issue for future research.\n\nDomain-specific pre-training. proved effective in natural language settings, improving performance at a moderate cost in terms of computation and data. The only case where it did not work well was for code comments. While we are not sure why, one reason could be that code comments are too far way from regular English (needing a specific model instead), or that careful curation of the data set (avoiding too many duplicates) is needed. Both cases could lead to catastrophic forgetting [82] of the initial pretraining. Leveraging the resources that were used to train BERT and fine-tuning it further proved much more effective than training a model from scratch. We have not evaluated domain-specific training from English to source code, as we hypothesized that the two domains are very differentthe tokenization alone might differ significantly [42]. This intuition is supported by the literature, which reports an example where an English BERT was applied to source code, with underwhelming results [75].\n\n\nBack-translation.\n\nWhile data augmentation is effective for natural language, it is not immediately applicable to source code. Source code can not be \"back translated\" easily. Specific data augmentations for code should be investigated, but may not be trivial (e.g., renaming identifiers could be investigated, but what should be done with API methods?).\n\n\nIntermediate Task Training.\n\nAnother alternative is to define suitable intermediate training tasks. We have found initial evidence of this, and a recent paper adds further evidence, in the context of traceability [83]. However, it used a very similar task and dataset. Thus, the challenge here is not whether intermediate task training helps, but rather whether a suitable task exists for a given problem.\n\nSoft Labels. Soft labels that reflect the uncertainty of raters (and thus the difficulty of the samples) can be useful as well, and at a minimal cost. However these are not common, as of now. We call on dataset builders to release them alongside the majority label, as was done by Calefato [18].\n\n\nLimitations of this work\n\nLimited Number of experiments. While we try to report results as extensively as possible to increase their generalisability, we are limited for two main reasons: 1) we have limited computational resources, and 2) some techniques are specific to some settings.\n\n\nLimited resources.\n\nDeep learning is famously resource intensive. While fine-tuning is less resource intensive than training models from scratch, it still requires significant time on one or more dedicated GPUs, particularly for larger models. A single run is measured in hours. This limits the number of experiments, particularly as we repeat experiments several times with different random seeds.\n\nResults in specific settings. While resources are limited, we still wanted to try each technique on at least two datasets. However, some techniques were applied to a single dataset. For soft labels, we needed multiple ratings: only a single sentiment analysis dataset had the required three ratings per sample. We could only define a reasonable intermediate task for code-comment readability prediction. We considered using self-training for comment classification, but did not, due to the large number of imbalanced classes.\n\n\nHyper-parameters.\n\nLimited resources also impact the extent to which we perform hyper-parameter optimisation, as thorough parameter searches (whether by grid, random or bayesian methods) would be prohibitively expensive. A second limit is that some hyper-parameters are fixed by the usage of a pre-trained model (e.g. number of layers, number of attention heads, embedding size, vocabulary size). A silver lining is that, given the interest in pre-trained models, general recommendations for hyper-parameters exist and are broadly applicable. Thus, we started with these recommendations, and investigated some variations of the hyper-parameters on the Senti4SD dataset, confirming that the recommendations worked well. We then applied those hyper-parameters on other experiments, varying only the most important ones in some cases (learning rate, batch size). Cross-validation also makes evaluation and hyperparameter tuning more complex and resource intensive. Since we limited hyper-parameter tuning, we are not at risk of overfitting to the test fold when doing cross validation. An alternative would be to use doubly nested cross validation, but this further increases the resource needed. We note that dedicated test sets ease this considerably.\n\nComparisons with previous work. We do our best to provide a fair comparison with previous work, while avoiding methodological issue (e.g. averaging seeds). We do not always exactly know how previous work was evaluated evaluation (e.g., hyper-parameter selection strategy, whether simple or nested cross-validation was used, or whether some data points were excluded) as code is not always released. In some other cases, other factor presents us to make an exact comparison (e.g. use of leave-one-out cross validation is not practical for our setting). To alleviate this in the future, we release our source code (see Appendix C).\n\nActive Learning. While we could not see an advantage to active learning, this is not in line with previous work by Dhinakaran et al. [35] and Tu et al. [84].Of note, our results are obtained through simulation based on an existing labelled dataset. While this is a practice often used to evaluate active learning methods, a realistic application of active learning on a larger set of unlabelled data would lead to a different training set, which may be substantially more varied, and thus more effective. But it is also possible that the impact of active learning is less visible when pre-training is used.\n\nRandom Seeds. Dodge et al. [85] found that the choice of the random seed can have a substantial impact on performance, especially for small datasets. We ran most of our experiments five times and all of them at least three times, with different seeds. While this surely mitigates the problem, it might not fully clear it up . Implementation Bugs. Our implementations are based on Hugging Face's transformers Python package [86], a high quality implementation of common Transformer models. However, despite careful reviews we cannot fully preclude errors in our own code and adaptations.\n\n\nCONCLUSIONS\n\nSoftware Engineering datasets are often small, by necessity. In this work, we trained various Transformer models on 13 small and medium-sized dataset selected from the recent Software Engineering literature. We not only compared Transformers of different size and different pre-training regimes but also applied several machine learning techniques that promised a possible benefit for small datasets. These techniques were data augmentation, self-training, intermediate-task training, active learning and soft labels.\n\nOverall, we found that on natural language tasks, Transformers usually outperform existing baselines. On source code tasks, however, results were mixed. Significant work lies ahead to define effective pre-trained source code models either by training larger models on more data, or by incorporating more structural information during training.\n\nIn general, we advise against pre-training a new model from scratch as it is extremely resource intensive, for mixed results. Instead, an already pre-trained model can be further pre-trained on task-specific data. If such task-specific data is unavailable, training on close-to-domain data is worth a try. We provide several such pre-trained models in Appendix C.\n\nSeveral additional techniques were useful at a relatively low cost. We particularly recommend the use of soft labels derived from multi-rater labels if available, and call on dataset authors to release these multi-rater labels. Backtranslation is similarly useful, if more expensive. It is unfortunately not easily applicable to source code.\n\nOther techniques were less applicable. We find that selftraining is advisable only in cases where the user wants to boost recall and is willing to sacrifice precision. If circumstances allow it, intermediate task training seems promising, but it seems rarely applicable, and has a much higher cost. Finally, our active learning experiments were inconclusive; a wider study on a larger set of dataset might be required to draw a clearer picture.\n\nWhile these general guidelines are useful on their own, their applicability is limited. To this extent, we release all the scripts and pretrained models that were built as part of this work, so that the community can easily fine-tune the models on their own Software Engineering datasets, and apply additional techniques as they see fit (see Appendix C). \n\n\nAPPENDIX A ADDITIONAL INFORMATION ON THE DATASETS\n\nData Quality. On the SATD dataset we noticed that false positives often contain keywords such as FIXME, TODO, or HACK; while this might raise questions about quality, we found that only 1.2% of the negative instances contain such keywords, compared to 6% among positives. Similarly, in the app review dataset by Maalej et al. [21] we found that 9% of the samples appear twice, with different labels. We left these duplicates in the dataset as we did not know how this issue was handled by previous work, nor which duplicates to remove and which to keep.\n\nAdditional examples and Statistics. Table 14 shows a representative example of each dataset, alongside with its class. Table 15 shows detailed statistics (per project) on the Self-Admited Technical Debt Dataset. Figure 5 shows the results of active learning on the app review classification dataset. On this dataset, all pool curves show a noticeable drop at the end. A possible explanation for this might be data quality: we observed that the app review dataset by Maalej et al. [21] contains a number of duplicates with conflicting labels. Having learned one of the duplicate samples, all its copies will be considered very \"easy\" and not be selected until the very end, at which point the model will predict the label of the duplicate selected first, which will, as labels are conflicting, be wrong, causing accuracy to drop towards the end of the active learning process.\n\n\nAPPENDIX B ADDITIONAL RESULTS\n\n\nB.1 Active Learning\n\n\nB.2 Label Smoothing\n\nA method to obtain soft-labels that does not require any additional information is label smoothing [87], [88]. In label smoothing, the original target distribution is mixed with the uniform distribution over all classes: For a given target vector y, its smoothed version is calculated as: y smooth = (1 \u2212 \u03b1) \u00b7 y + \u03b1 \u00b7 1 K , where K is the number of classes and \u03b1 controls the smoothing strength. As an example, smoothing the target vector (A: 0, B: 1, C: 0) with \u03b1 = 0.2 results in (A: 0.06, B: 0.86, C: 0.06), now a soft-label. Label smoothing is a form of regulation: intuitively, it dampens the model's prediction confidence, forcing it to make more \"cautious\" predictions.\n\nWe carried out a label-smoothing experiment on the Senti4SD dataset in addition to soft labels. We train a subset of our Transformer models with hard-labels, soft-labels and different degrees of label-smoothing (\u03b1 = 0.1, 0.05 and 0.03) respectively, and evaluate on all sentiment analysis datasets; refer to Table 16 for a comparison of the results. While label smoothing can occasionally improve performance (e.g. on Jira issues), it is more likely to either degrade performance, or not affecting it significantly.\n\n\nB.3 Task-Specific Tokens\n\nIn the code readability task, our Transformer models cannot compete with manually engineered features used by the baseline. Since Buse and Weimer [47] found that line length is one of the most important features for predicting code readability, we attempt to provide this information explicitly to our model in form the of special line length tokens, added to line ends. These tokens range from <l1>, indicating a short line, up to <l10> for very long lines and are inserted before newline tokens. We fine-tuned the same model with and without these special tokens.\n\nLine length tokens failed to improve the performance. In fact, they seem to hurt performance (see Table 17). The logistic regression model by Scalabrino et al. outperforms our Transformer model by a wide margin. We were able to successfully reproduce the results of Scalabrino's model, which was implemented using Weka [89]. We found that for their model, attribute selection is crucial; without it, in our experiments, accuracy dropped significantly (below 60%).\n\nA simple logistic regression model implemented using scikit-learn [90], even with attribute selection, was similarly unable to beat their Weka model; neither was TPOT [91], a framework for automated machine learning, that automatically evaluates a large number of combinations of different machine learning algorithms. By first using Weka's attribute selection algorithm and feeding selected attribute to a scikit-learn logistic regression model we were eventually able to obtain results close to the Weka-only model.\n\n\nB.4 Sentiment Analysis\n\nFull results, including per-class precision and recall for sentiment classification (Table 18). Finally, as can be seen in table 19, sentences that are most confusing to the model are hard to classify even for humans. Similar observations can be made in other datasets (e.g., SATD, not shown here).\n\n\nName Sample Class\n\nSentiment Classification (Stack Overflow) [18] I want them to resize based on the length of the data they're showing. neutral Sentiment Classification (Stack Overflow) [19] When I run my client, it throws the following exception. negative Sentiment Classification (JIRA Issues) [3] This is always a really bad way to design software. negative Sentiment Classification (App Reviews) [19] amazing! a must have app positive\n\nInformative App Review Detection [20] not able to download any pictures please fix these bugs immediately informative App Review Classification [21] Best game I've played on Android rating App Review Classification [27] good but... it has ads...please remove ads from this... usability\n\nSelf-Admitted Technical Debt Detection [22] // FIXME: Is \"No Namespace is Empty Namespace\" really OK? SATD   TABLE 17 Results for the code readability prediction task. As far as our number are concerned, values are means over five runs with different seeds.\n\n\nAPPENDIX C FURTHER DETAILS ON IMPLEMENTATION, RUN-TIME, REPLICATION\n\nObtaining back-translation data. While the original works introducing back-translation used an ad-hoc neural translation model, we found that the most efficient way to obtain back-translations is to load the dataset into Google Sheet and use the GOOGLETRANSLATE macro. An example is available online 4 .\n\n\nModel implementations.\n\nFor all our experiments we use HuggingFace's transformers package [86], a Python library based on pytorch that implements many different Transformer architectures, including BERT and RoBERTa.\n\nRuntime considerations. All of our experiments were carried out either on an NVIDIA V100 GPU with 32 GB of memory or on up to three NVIDIA RTX 2080TIs with 10 GB memory each.\n\nOur pre-training regimes are generally affordable even with relatively modest computational budget, although an extensive hyper-parameter search is hardly feasible. We thus followed common recommendations and only tried a few parameter combinations. With a training time of two weeks, pre-training StackOBERTflow from scratch was by far the most expensive (especially considering that this was clearly not enough, as it ended up being out-performed by the further pre-trained models). Further pre-training the 12-layer models required considerably less training time, usually below 24 hours (on an NVIDIA V100 GPU).\n\nUsing the pre-trained models. Our models are publicly available: the StackOBERTflow model can be obtained through the Huggingface Model Hub 5 ; our fine-tuned BERT and RoBERTa models can be downloaded from GitHub 6 .\n\nYou can download our pre-trained models and use them for your own experiments. Our StackOBERTflow model can be automatically downloaded using the transformers library. You can instantiate a classification model using model = AutoModel.from_pretrained ('giganticode/StackOBERTflow-comments-small-v1'), and then fine-tune on your task-specific data. You can also use our other models: first download the model as ZIP archive from our GitHub page and unpack it; then, likewise, load them as follows: model = AutoModel.from_pretrained('/path/to/model')\n\n\nRerunning experiments.\n\nFirst clone our GitHub repository 7 ; then run python -mdl4se.experiments.<experiment>.default -seeds 100 200 300 400 500 -out_file=result_file.csv, where experiment is one of the experiments listed in Table 20. Configuration options and default hyper-parameters can for each experiment be found in /dl4se/config/<experiment>.py,\n\n\u2022\nBERT-reviews, a 12-layer (base) BERT model trained on 169 097 (8.4 MB) unlabeled app reviews from the AR-MINER dataset. \u2022 BERT-comments, a 12-layer (base) BERT model trained on 487 693 (48 MB) comments extracted from wellknown Java projects. \u2022 BERT-SO-1M and BERT-SO-2M, two 12-layer (base) BERT models trained on one (147 MB) and two millions (304 MB) of Stack Overflow comments, respectively, taken from the Stack Exchange Data Dump 1 . \u2022 BERT-SO-1M-large, a 24-layer (large) BERT model trained on one million Stack Overflow comments (as above), used, however, only in the sentiment classification experiments. \u2022 StackOBERTflow, a 6-layer (small) RoBERTa model trained from scratch on 26.2 million Stack Overflow comments (3.6 GB). The Stack Overflow corpus was tokenized using byte pair encoding (BPE) subwords and a large vocabulary size of 52 000.\n\nFig. 2 .\n2F1 score on different sentiment classification datasets with and without pre-training. Number of optimization steps is shown on the xaxis; error bands are 95% confidence intervals.\n\nFig. 5 .\n5Accuracy of D pool and the test set at each iteration of the active learning process for different acquisition functions with (c = 3) and without (c = 1) diversity sampling for the review classification task. Error bands are 95% confidence intervals.\n\nTABLE 1\n1Datasets considered in this work, along with their size, number of classes and usage.\n\n\nEN \u2192DE Leppie, das sind gro\u00dfartige Neuigkeiten! Ich freue mich darauf, IronScheme auszuprobieren! DE \u2192EN leppie, those are great news! I am looking forward to try out IronScheme!EN \nLeppie, that's great news! I look forward to trying \nIronScheme! \n\nEN \u2192FR \nLeppie, c'est une excellente nouvelle! J'ai h\u00e2te \nd'essayer IronScheme! \nFR \u2192EN \nleppie, this is great news! I can't wait to try Iron-\nScheme! \n\nFig. 1. Example of back-translation. The original English sentence is \nfirst translated to German and French, then translated back into En-\nglish; resulting variation underlined. Google Translate was used for the \ntranslation. \n\n\n\n\na small (6-layer) RoBERTa model trained on the polyglot CodeSearchNet [77] source code corpus and released by Hugging Face. The model supports multiple programming languages: Go, Java, Javascript, PHP, Python and Ruby. \u2022 CodeBERT [61], a larger 12-layer model trained on the same corpus, but with a bimodal training regimen: the 1. https://archive.org/details/stackexchange 2. https://huggingface.co/huggingface/CodeBERTa-small-v1\n\nTABLE 2\n2Overview of tasks, experiments, and results in this work.\n\n\nResults (macro F1) for four apps in the AR-MINER dataset. Our numbers are averages over five runs with different seeds.Tap \nFish \n\nTemple \nRun2 \n\nSwift-\nKey \nAvg. \n\nStackOBERTflow 90.9 89.4 \n88.6 \n85.1 88.5 \nBERT (base) \n90.6 88.2 \n89.2 \n85.4 88.4 \nBERT-SO-1M \n92.1 90.0 \n91.1 \n87.5 90.2 \nBERT-reviews \n93.3 91.4 \n91.3 \n89.9 91.5 \nEMNB [20] \n87.7 76.1 \n79.7 \n76.4 80.0 \n\nTABLE 3 \nPre-trained transformers were able to outperform base-\nlines on domains closer to natural language; for source \ncode, results were mixed. \n\nModel \nF1 \nPrec. Rec. \n\nCodeBERTa 1 \n81.1 \n81.5 \n81.1 \nCodeBERT 1 \n71.2 \n73.7 \n71.5 \nSMO Poly 2 [2] 88.8 \n91.8 \n86.0 \nSVM RBF 2 [2] \n74.8 \n76.2 \n73.4 \nCNN 2 [2] \n74.5 \n75.6 \n73.5 \n\n1 15-fold cross validation \n2 leave-one-out cross validation \n\n\n\nTABLE 4 Macro\n4F1, precision and recall for the linguistic smell detection task. Our numbers are averages over three runs with different seeds. Results for the code runtime complexity prediction task. Our numbers are means over five runs with different seeds.Model \nAcc. \n\nCodeBERTa \n78.2 \nCodeBERT \n78.4 \nRandom Forest [25] \n74.3 \nLogistic Regression [25] \n73.2 \nSVM [25] \n73.0 \nSVM+graph2vec [25] \n73.9 \n\nTABLE 5 \n\n\n\nMacro F1 scores for the SATD detection task. As far as our results are concerned, numbers are means over five runs, each with different seed.without duplicates \nwith duplicates \nBERT-\nSO-1M \n\nBERT-\ncomments \n\nBERT \n(base) \n\nStack-\nOBERTflow \n\nBERT-\nSO-1M \n\nBERT-\ncomments \n\nBERT \n(base) \n\nStack-\nOBERTflow \nCNN NLP \n\nApache Ant 70.2 \n66.9 \n65.4 \n67.5 \n70.3 \n68.8 \n67.2 \n69.0 \n66.0 51.2 \nArgoUML \n89.8 \n90.0 \n89.7 \n89.3 \n89.4 \n90.0 \n89.7 \n89.0 \n87.8 81.9 \nColumba \n90.9 \n90.4 \n91.0 \n90.0 \n91.4 \n91.4 \n91.9 \n90.6 \n85.2 75.0 \nEMF \n73.5 \n72.4 \n73.2 \n69.2 \n72.5 \n69.5 \n74.4 \n71.0 \n67.9 46.2 \nHibernate \n88.6 \n87.5 \n88.2 \n87.4 \n88.7 \n88.3 \n88.9 \n87.7 \n82.6 76.3 \nJEdit \n72.7 \n70.6 \n71.9 \n73.9 \n72.0 \n70.8 \n70.5 \n72.2 \n59.9 46.1 \nJFreeChart \n77.7 \n80.7 \n79.2 \n77.6 \n64.1 \n64.7 \n63.8 \n63.0 \n73.9 51.3 \nJMeter \n87.0 \n87.5 \n87.4 \n86.6 \n86.4 \n85.8 \n86.2 \n84.3 \n82.8 71.5 \nJRuby \n91.1 \n91.2 \n90.8 \n90.5 \n92.1 \n92.4 \n92.3 \n91.4 \n86.3 77.3 \nSQuirrel \n79.1 \n78.2 \n78.8 \n78.6 \n80.5 \n79.5 \n80.5 \n78.4 \n73.9 59.3 \n\nAverage \n82.1 \n81.5 \n81.6 \n81.1 \n80.7 \n80.1 \n80.5 \n79.7 \n76.6 63.6 \n\nTABLE 6 \nModel \nF1 \nPrec. Rec. \n\nStackOBERTflow \n88.4 \n89.6 \n90.1 \nBERT-comments \n88.3 \n90.6 \n89.0 \nBERT \n88.4 \n90.5 \n89.1 \nNaive Bayes Multinomial [23] 84.3 \n82.0 \n87.2 \n\n\n\nTABLE 7\n7Results for the comment classification task. Task-specific pre-training failed to improve performance. As far as our models are concerned, results are averages over three runs with different seeds.\n\n\nAccuracy, macro F1 and per-class precision and recall for different models and datasets. Values reported are means over five runs, each with different seed (only our models). All models were trained on the Senti4SD[18] Stack Overflow dataset. Macro F1 for different label types and datasets: mean, maximum and standard deviation over five runs, each with different seed. All models were trained on the Senti4SD[18] Stack Overflow dataset.Dataset \nModel \nAcc. F1 \n\nSenti4SD [18] \n(Test Set) \n\nBERT (base) \n87.9 87.8 \nBERT-SO-1M-large \n89.4 89.4 \nBERT-SO-2M \n88.8 88.7 \nBERT-SO-1M \n88.8 88.7 \nDistilRoBERTa (small) \n87.4 87.3 \nRoBERTa (small, no pretr.) 78.4 77.8 \nSenti4SD [18] \n-\n86.0 \nSentiCR [18] \n-\n82.0 \nSentiStrengthSE [18] \n-\n80.0 \nSentiStrength [18] \n-\n84.0 \nStackOBERTflow \n88.7 88.6 \n\nApp Reviews [19] \n\nBERT (base) \n64.9 50.2 \nBERT-SO-1M-large \n66.0 51.9 \nBERT-SO-2M \n67.7 53.1 \nBERT-SO-1M \n68.3 53.7 \nDistilRoBERTa (small) \n60.9 48.5 \nNLTK [3] \n54.0 40.8 StackOBERTflow \n72.0 57.4 \nStanford CoreNLP SO [3] 41.6 35.5 \nStanford CoreNLP [3] \n69.5 56.0 \n\nJIRA Issues [3] \n\nBERT (base) \n95.7 95.0 \nBERT-SO-1M-large \n94.8 93.8 \nBERT-SO-2M \n95.2 94.5 \nBERT-SO-1M \n95.1 94.2 \nDistilRoBERTa (small) \n93.7 92.7 \nNLTK [3] \n29.8 46.5 StackOBERTflow \n93.5 92.5 \nStanford CoreNLP SO [3] 36.0 44.2 \nStanford CoreNLP [3] \n67.6 73.7 \n\nStackOverflow [19] \n\nBERT (base) \n79.9 47.6 \nBERT-SO-1M-large \n79.2 43.3 \nBERT-SO-2M \n80.3 48.6 \nBERT-SO-1M \n80.1 47.9 \nDistilRoBERTa (small) \n78.8 44.3 \nNLTK [3] \n77.9 43.2 StackOBERTflow \n79.3 44.1 \nStanford CoreNLP SO [3] 75.9 47.5 \nStanford CoreNLP [3] \n40.3 35.5 \n\nTABLE 8 \nDataset \nLabel Type \nF1 \n\u00b5 \u00b1 \u03c3 max \n\nApp Reviews [19] \nAll Label Votes 57.5 \u00b1 2.2 59.1 \nMajority Label 57.4 \u00b1 1.6 59.7 \n\nJIRA Issues [3] \nAll Label Votes 93.5 \u00b1 1.0 94.7 \nMajority Label 92.5 \u00b1 0.5 93.2 \n\nStackOverflow [19] \nAll Label Votes 42.4 \u00b1 1.0 43.8 \nMajority Label 44.1 \u00b1 2.9 46.4 \n\nSenti4SD [18] \nAll Label Votes 89.1 \u00b1 0.5 89.6 \nMajority Label 88.6 \u00b1 0.5 89.1 \n\nTABLE 9 \n\n\nTable 10 ).\n10Bug \nreports \n\nFeature \nrequest \nRatings \nUser \nexperience \nAvg. \n\nStackOBERTflow \n61.1 \n42.0 \n79.4 \n49.8 \n58.1 \nStackOBERTflow+BT \n62.5 \n43.3 \n79.0 \n50.2 \n58.7 \nStackOBERTflow+BTT \n62.7 \n44.3 \n79.6 \n50.6 \n59.3 \nBERT (base) \n61.8 \n39.7 \n79.4 \n49.4 \n57.6 \nBERT (base)+BT \n62.0 \n43.1 \n79.4 \n50.2 \n58.7 \nBERT (base)+BTT \n62.5 \n42.6 \n80.0 \n50.1 \n58.8 \nBERT-reviews \n64.1 \n44.7 \n80.2 \n51.0 \n60.0 \nBERT-reviews+BT \n63.2 \n43.9 \n79.5 \n51.0 \n59.4 \nBERT-reviews+BTT \n63.7 \n46.7 \n80.1 \n51.2 \n60.4 \nDecision Tree 1 [21] \n62.0 \n42.0 \n54.0 \n50.0 \n52.0 \nNaive Bayes 1 [21] \n62.0 \n47.0 \n54.0 \n53.0 \n54.0 \n\n1 multiclass, bag of words + metadata \n\n\n\nTABLE 10 F1\n10scores for app review classification, reported with training-time back-translation augmentation (+BT), with training-time and test-time back-translation augmentation (+BTT) and without any augmentation.Results are averages over three runs with different seeds.TABLE 11 Macro F1, precision and recall for back-translation augmentation at training and test time on the Senti4SD (sentiment classification) and CLAP (app review classification) datasets using the StackOBERTflow model. Results are averages over three runs with different seeds.Dataset \nAugmentation Acc. F1 Prec. Rec. \n\nCLAP \n\nTrain+Test \n89.2 81.4 84.2 82.3 \nNone \n88.1 79.7 80.6 81.7 \nTest \n88.3 80.2 81.8 81.5 \nTrain \n88.9 81.1 83.7 82.0 \n\nSenti4SD \n\nTrain+Test \n88.7 88.6 88.7 88.5 \nNone \n88.4 88.2 88.2 88.4 \nTest \n88.2 88.0 88.1 88.0 \nTrain \n88.4 88.3 88.3 88.5 \n\n\n\nTABLE 13\n13Results on the comment-code coherence dataset. CodeBERTa was \nevaluated with and without intermediate-task training (ITT). Our \nnumbers are averages over five runs with different seeds. \n\n\n\n\n/ ** * Returns the current number of milk units in * the inventory. class GFG { static int minJumps(int arr[], int n) { int[] jumps = new int[n]; int min; jumps[n -1] = 0; for (int i = n -2; i >= 0; i--) { if (arr[i] == 0) jumps[i] = Integer.MAX_VALUE; else if (arr[i] >= n -i -1) jumps[i] = 1; else { ... } } return jumps[0]; } public static void main(String[] args) {...} } O(n log n) @Override public void configure(Configuration cfg) { super.configure(cfg); cfg.setProperty(Environment.USE_SECOND_LEVEL_CACHE, ...); cfg.setProperty(Environment.GENERATE_STATISTICS, ...); cfg.setProperty(Environment.USE_QUERY_CACHE, \"false\" ); ... // more cfg.setProperty calls } readableTABLE 14Arbitrarily selected examples from the datasets along with their class. Some of the code samples have been shortened with ellipses.Comment Classification \n[23] \n@return a string for throwing \nusage \n\nCode-Comment Coherence \nPrediction [24] \n\n* @return int \n* / \npublic int getMilk() { \nreturn milk; \n} \n\ncoherent \n\nLinguistic Smell \nDetection [2] \n\npublic void ToSource(StringBuilder sb) { \nsb.append(\";\"); \nthis.NewLine(sb); \n} \n\nsmelly \n(transform method \ndoes not return) \n\nCode Runtime Complexity \nClassification [25] \n\nCode Readability \nPrediction [26] \n\n\n\nTABLE 15 TABLE 16 Macro\n1516Projects in the SATD dataset along with the number of samples classified as self-admitted technical debt. App Reviews [19] All Label Votes (no smooth.) 57.5 \u00b1 2.2 59.1 Majority Label (no smooth.) 57.4 \u00b1 1.6 59.7 JIRA Issues [3] All Label Votes (\u03b1 = 0.03) 92.7 \u00b1 1.4 94.2 All Label Votes (\u03b1 = 0.05) 93.2 \u00b1 0.7 94.0 All Label Votes (\u03b1 = 0.1) 93.1 \u00b1 1.1 94.2 All Label Votes (no smooth.) 93.5 \u00b1 1.0 94.7 Majority Label (\u03b1 = 0.03) 91.6 \u00b1 1.9 94.3 Majority Label (\u03b1 = 0.05) 92.4 \u00b1 1.3 93.6 Majority Label (\u03b1 = 0.1) 93.5 \u00b1 0.7 94.3 Majority Label (no smooth.) 92.5 \u00b1 0.5 93.2 StackOverflow [19] All Label Votes (\u03b1 = 0.03) 41.6 \u00b1 1.3 43.6 All Label Votes (\u03b1 = 0.05) 42.5 \u00b1 2.4 46.1 All Label Votes (\u03b1 = 0.1) 41.0 \u00b1 1.4 42.7 All Label Votes (no smooth.) 42.4 \u00b1 1.0 43.8 Majority Label (\u03b1 = 0.03) 42.8 \u00b1 3.5 47.4 Majority Label (\u03b1 = 0.05) 42.7 \u00b1 2.3 45.6 Majority Label (\u03b1 = 0.1) 43.1 \u00b1 1.4 45.2 Majority Label (no smooth.) 44.1 \u00b1 2.9 46.4 All Label Votes (\u03b1 = 0.05) 89.0 \u00b1 0.6 89.9 All Label Votes (\u03b1 = 0.1) 89.0 \u00b1 0.4 89.5 All Label Votes (no smooth.) 89.1 \u00b1 0.5 89.6 Majority Label (\u03b1 = 0.03) 88.4 \u00b1 0.5 89.0 Majority Label (\u03b1 = 0.05) 88.4 \u00b1 0.4 89.1 Majority Label (\u03b1 = 0.1) 88.5 \u00b1 0.3 88.8 Majority Label (no smooth.) 88.6 \u00b1 0.5 89.1 F1 for different label types and datasets: mean, maximum and standard deviation over five runs, each with different seed. All models were trained on the training set of the Stack Overflow dataset from Calefato et al. [18].Dataset \nLabel Type \nF1 \n\u00b5 \u00b1 \u03c3 max \n\nAll Label Votes (\u03b1 = 0.03) \n56.9 \u00b1 3.3 61.6 \nAll Label Votes (\u03b1 = 0.05) \n56.2 \u00b1 2.9 58.8 \nAll Label Votes (\u03b1 = 0.1) \n56.4 \u00b1 2.6 59.0 \nMajority Label (\u03b1 = 0.03) \n55.7 \u00b1 3.8 60.4 \nMajority Label (\u03b1 = 0.05) \n55.9 \u00b1 1.5 57.6 \nMajority Label (\u03b1 = 0.1) \n56.7 \u00b1 1.6 58.8 \nSenti4SD [18] \n(Test Set) \n\nAll Label Votes (\u03b1 = 0.03) \n88.9 \u00b1 0.2 89.2 \nModel \nAcc. \n\nCodeBERTa \n73.1 \nCodeBERTa+LLT \n72.5 \nCodeBERT \n69.3 \nLogistic Regression [50] 84.0 \n\n\nRoBERTa (small, no pretr.) 48.0 42.2 SentiStrength+AC0-SE [3] 58.9 46.6 SentiStrength [3] 62.5 48.2\nRoBERTa (small, no pretr.) 84.0 80.1 SentiStrength+AC0-SE [3] 76.0 87.0 SentiStrength [3] 77.1 85.4\nRoBERTa (small, no pretr.) 75.5 42.2 SentiStrength+AC0-SE [3] 78.0 46.8 SentiStrength [3] 69.5 49.5\nSATD not SATD Totaldataset loading and pre-processing code lies in /dl4se/datasets/<experiment>.py Note that you cannot use the original datasets, as datasets need to adhere to a specific format. We will provide all of the datasets upon request.SampleAct. Label Pred. Label Agr.This worked for me. positive neutral noIn addition to firebug (which should be your first port of call), the will also tell you where a given style is sourced from, just in case IE -shock, horror -should be different. negative positive yes I understand its not a desirable circumstance, however if I NEEDED to have some kind of HTML within JSON tags, e.g.: is this possible to do in Python without requiring to to be escaped beforehand? It will be a string initially so I was thinking about writing a regular expression to attempt to match and escape these prior to processing, but I just want to make sure there isn't an easier way.\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A large-scale hierarchical image database,\" in 2009 IEEE conference on computer vision and pattern recognition, Ieee, 2009, pp. 248-255.\n\nKeep it simple: Is deep learning good for linguistic smell detection. S Fakhoury, V Arnaoudova, C Noiseux, F Khomh, G Antoniol, 10.1109/SANER.2018.8330265DOI: 10. 1109/SANER.2018.8330265IEEE 25th International Conference on Software Analysis. S. Fakhoury, V. Arnaoudova, C. Noiseux, F. Khomh, and G. Antoniol, \"Keep it simple: Is deep learning good for linguistic smell detection?\" In 2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER), 2018, pp. 602-611. DOI: 10. 1109/SANER.2018.8330265.\n\nSentiment analysis for software engineering: How far can we go. B Lin, F Zampetti, G Bavota, M Di Penta, M Lanza, R Oliveto, 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). B. Lin, F. Zampetti, G. Bavota, M. Di Penta, M. Lanza, and R. Oliveto, \"Sentiment analysis for software engi- neering: How far can we go?\" In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE), 2018, pp. 94-104.\n\nNeural network-based detection of selfadmitted technical debt: From performance to explainability. X Ren, Z Xing, X Xia, D Lo, X Wang, J Grundy, ACM Transactions on Software Engineering and Methodology (TOSEM). 283X. Ren, Z. Xing, X. Xia, D. Lo, X. Wang, and J. Grundy, \"Neural network-based detection of self- admitted technical debt: From performance to ex- plainability,\" ACM Transactions on Software Engineering and Methodology (TOSEM), vol. 28, no. 3, pp. 1-45, 2019.\n\nSelf-admitted technical debt classification using lstm neural network. R M Santos, M C R Junior, M G De Mendon\u00e7a, Neto, 17th International Conference on Information Technology-New Generations (ITNG 2020). SpringerR. M. Santos, M. C. R. Junior, and M. G. de Men- don\u00e7a Neto, \"Self-admitted technical debt classifica- tion using lstm neural network,\" in 17th International Conference on Information Technology-New Generations (ITNG 2020), Springer, 2020, pp. 679-685.\n\nIntroduction to semisupervised learning. X Zhu, A B Goldberg, Synthesis lectures on artificial intelligence and machine learning. 31X. Zhu and A. B. Goldberg, \"Introduction to semi- supervised learning,\" Synthesis lectures on artificial in- telligence and machine learning, vol. 3, no. 1, pp. 1-130, 2009.\n\nM E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, arXiv:1802.05365Deep contextualized word representations. arXiv preprintM. E. Peters, M. Neumann, M. Iyyer, M. Gard- ner, C. Clark, K. Lee, and L. Zettlemoyer, \"Deep contextualized word representations,\" arXiv preprint arXiv:1802.05365, 2018.\n\nUniversal language model fine-tuning for text classification. J Howard, S Ruder, arXiv:1801.06146arXiv preprintJ. Howard and S. Ruder, \"Universal language model fine-tuning for text classification,\" arXiv preprint arXiv:1801.06146, 2018.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805cs.CLJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2018. arXiv: 1810 . 04805 [cs.CL].\n\nY Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. cs.CLY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, Roberta: A robustly optimized bert pretraining approach, 2019. arXiv: 1907.11692 [cs.CL].\n\nGlue: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, arXiv:1804.07461cs.CLA. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019. arXiv: 1804.07461 [cs.CL].\n\nSquad: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, arXiv:1606.05250cs.CLP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, Squad: 100,000+ questions for machine comprehension of text, 2016. arXiv: 1606.05250 [cs.CL].\n\nLeveraging small software engineering data sets with pre-trained neural networks. R Robbes, A Janes, 2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER). IEEER. Robbes and A. Janes, \"Leveraging small software engineering data sets with pre-trained neural net- works,\" in 2019 IEEE/ACM 41st International Confer- ence on Software Engineering: New Ideas and Emerging Results (ICSE-NIER), IEEE, 2019, pp. 29-32.\n\nCross-dataset design discussion mining. A Mahadi, K Tongay, N A Ernst, 2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEEA. Mahadi, K. Tongay, and N. A. Ernst, \"Cross-dataset design discussion mining,\" in 2020 IEEE 27th Inter- national Conference on Software Analysis, Evolution and Reengineering (SANER), IEEE, 2020, pp. 149-160.\n\nSentiment analysis for software engineering: How far can pre-trained transformer models go?. T Zhang, B Xu, F Thung, S A Haryono, D Lo, L Jiang, 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEET. Zhang, B. Xu, F. Thung, S. A. Haryono, D. Lo, and L. Jiang, \"Sentiment analysis for software engineering: How far can pre-trained transformer models go?\" In 2020 IEEE International Conference on Software Mainte- nance and Evolution (ICSME), IEEE, pp. 70-80.\n\nNorbert: Transfer learning for requirements classification. T Hey, J Keim, A Koziolek, W F Tichy, 2020 IEEE 28th International Requirements Engineering Conference. IEEET. Hey, J. Keim, A. Koziolek, and W. F. Tichy, \"Nor- bert: Transfer learning for requirements classifica- tion,\" in 2020 IEEE 28th International Requirements Engineering Conference (RE), IEEE, pp. 169-179.\n\nAchieving reliable sentiment analysis in the software engineering domain using bert. E Biswas, M E Karabulut, L Pollock, K Vijay-Shanker, 10.1109/ICSME46990.2020.00025DOI: 10 . 1109/ICSME46990.2020.000252020 IEEE International Conference on Software Maintenance and Evolution (ICSME). E. Biswas, M. E. Karabulut, L. Pollock, and K. Vijay- Shanker, \"Achieving reliable sentiment analysis in the software engineering domain using bert,\" in 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME), 2020, pp. 162-173. DOI: 10 . 1109/ICSME46990.2020.00025.\n\nSentiment polarity detection for software development. F Calefato, F Lanubile, F Maiorano, N Novielli, Empirical Software Engineering. 233F. Calefato, F. Lanubile, F. Maiorano, and N. Novielli, \"Sentiment polarity detection for software develop- ment,\" Empirical Software Engineering, vol. 23, no. 3, pp. 1352-1382, 2018.\n\nTwo datasets for sentiment analysis in software engineering. B Lin, F Zampetti, R Oliveto, M Di Penta, M Lanza, G Bavota, 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME). B. Lin, F. Zampetti, R. Oliveto, M. Di Penta, M. Lanza, and G. Bavota, \"Two datasets for sentiment analysis in software engineering,\" in 2018 IEEE International Con- ference on Software Maintenance and Evolution (ICSME), 2018, pp. 712-712.\n\nAr-miner: Mining informative reviews for developers from mobile app marketplace. N Chen, J Lin, S C Hoi, X Xiao, B Zhang, Proceedings of the 36th international conference on software engineering. the 36th international conference on software engineeringN. Chen, J. Lin, S. C. Hoi, X. Xiao, and B. Zhang, \"Ar-miner: Mining informative reviews for develop- ers from mobile app marketplace,\" in Proceedings of the 36th international conference on software engineering, 2014, pp. 767-778.\n\nOn the automatic classification of app reviews. W Maalej, Z Kurtanovi\u0107, H Nabil, C Stanik, Requirements Engineering. 213W. Maalej, Z. Kurtanovi\u0107, H. Nabil, and C. Stanik, \"On the automatic classification of app reviews,\" Require- ments Engineering, vol. 21, no. 3, pp. 311-331, 2016.\n\nUsing natural language processing to automatically detect self-admitted technical debt. E D S Maldonado, E Shihab, N Tsantalis, IEEE Transactions on Software Engineering. 4311E. d. S. Maldonado, E. Shihab, and N. Tsantalis, \"Using natural language processing to automatically detect self-admitted technical debt,\" IEEE Transactions on Software Engineering, vol. 43, no. 11, pp. 1044-1062, 2017.\n\nClassifying code comments in java open-source software systems. L Pascarella, A Bacchelli, 2017L. Pascarella and A. Bacchelli, \"Classifying code com- ments in java open-source software systems,\" in 2017\n\nIEEE/ACM 14th International Conference on Mining Software Repositories (MSR). IEEEIEEE/ACM 14th International Conference on Mining Soft- ware Repositories (MSR), IEEE, 2017, pp. 227-237.\n\nA new dataset for source code comment coherence. A Corazza, V Maggio, B Kessler, G Scanniello, CLiC it. 100A. Corazza, V. Maggio, B. Kessler, and G. Scanniello, \"A new dataset for source code comment coherence,\" CLiC it, p. 100, 2016.\n\nLearning based methods for code runtime complexity prediction. J Sikka, K Satya, Y Kumar, S Uppal, R R Shah, R Zimmermann, ISBN: 978-3-030-45439-5Advances in Information Retrieval. J. M. Jose, E. Yilmaz, J. Magalh\u00e3es, P. Castells, N. Ferro, M. J. Silva, and F. MartinsChamSpringer International PublishingJ. Sikka, K. Satya, Y. Kumar, S. Uppal, R. R. Shah, and R. Zimmermann, \"Learning based methods for code runtime complexity prediction,\" in Advances in Information Retrieval, J. M. Jose, E. Yilmaz, J. Magal- h\u00e3es, P. Castells, N. Ferro, M. J. Silva, and F. Martins, Eds., Cham: Springer International Publishing, 2020, pp. 313-325, ISBN: 978-3-030-45439-5.\n\nImproving code readability models with textual features. S Scalabrino, M Linares-Vasquez, D Poshyvanyk, R Oliveto, 2016 IEEE 24th International Conference on Program Comprehension (ICPC). IEEES. Scalabrino, M. Linares-Vasquez, D. Poshyvanyk, and R. Oliveto, \"Improving code readability models with textual features,\" in 2016 IEEE 24th International Conference on Program Comprehension (ICPC), IEEE, 2016, pp. 1-10.\n\nListening to the crowd for the release planning of mobile apps. S Scalabrino, G Bavota, B Russo, M D Penta, R Oliveto, 10.1109/TSE.2017.2759112IEEE Transactions on Software Engineering. 451S. Scalabrino, G. Bavota, B. Russo, M. D. Penta, and R. Oliveto, \"Listening to the crowd for the release planning of mobile apps,\" IEEE Transactions on Soft- ware Engineering, vol. 45, no. 1, pp. 68-86, 2019. DOI: 10.1109/TSE.2017.2759112.\n\nThe emotional side of software developers in jira. M Ortu, A Murgia, G Destefanis, P Tourani, R Tonelli, M Marchesi, B Adams, 10.1145/2901739.2903505ISBN: 9781450341868. DOI: 10 . 1145 / 2901739 . 2903505Proceedings of the 13th International Conference on Mining Software Repositories, ser. MSR '16. the 13th International Conference on Mining Software Repositories, ser. MSR '16Austin, TexasAssociation for Computing MachineryOnlineM. Ortu, A. Murgia, G. Destefanis, P. Tourani, R. Tonelli, M. Marchesi, and B. Adams, \"The emotional side of software developers in jira,\" in Proceedings of the 13th International Conference on Mining Software Repositories, ser. MSR '16, Austin, Texas: Association for Computing Machinery, 2016, pp. 480-483, ISBN: 9781450341868. DOI: 10 . 1145 / 2901739 . 2903505. [On- line]. Available: https : / / doi . org / 10 . 1145 / 2901739 . 2903505.\n\nRelease planning of mobile apps based on user reviews. L Villarroel, G Bavota, B Russo, R Oliveto, M Di Penta, https:/doi-org.libproxy.unibz.it/10.1145/2884781.2884818ISBN: 9781450339001. DOI: 10.1145/ 2884781 . 2884818Proceedings of the 38th International Conference on Software Engineering, ser. ICSE '16. the 38th International Conference on Software Engineering, ser. ICSE '16Austin, TexasAssociation for Computing MachineryL. Villarroel, G. Bavota, B. Russo, R. Oliveto, and M. Di Penta, \"Release planning of mobile apps based on user reviews,\" in Proceedings of the 38th Interna- tional Conference on Software Engineering, ser. ICSE '16, Austin, Texas: Association for Computing Machinery, 2016, pp. 14-24, ISBN: 9781450339001. DOI: 10.1145/ 2884781 . 2884818. [Online]. Available: https : / / doi - org.libproxy.unibz.it/10.1145/2884781.2884818.\n\nThe evolution of sentiment analysis-a review of research topics, venues, and top cited papers. M V M\u00e4ntyl\u00e4, D Graziotin, M Kuutila, 10.1016/j.cosrev.2017.10.002Computer Science Review. 27M. V. M\u00e4ntyl\u00e4, D. Graziotin, and M. Kuutila, \"The evolution of sentiment analysis-a review of research topics, venues, and top cited papers,\" Computer Science Review, vol. 27, pp. 16-32, 2018, ISSN: 1574-0137. DOI: https : / / doi . org / 10 . 1016 / j . cosrev. 2017 . 10 . 002. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S1574013717300606.\n\nXlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, Inc32Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdi- nov, and Q. V. Le, \"Xlnet: Generalized autoregressive pretraining for language understanding,\" in Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, Eds., vol. 32, Curran Associates, Inc., 2019, pp. 5753-5763. [Online]. Available: https : / / proceedings . neurips . cc / paper / 2019 / file / dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf.\n\nAlbert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, arXiv:1909.11942cs.CLZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, Albert: A lite bert for self-supervised learning of language representations, 2020. arXiv: 1909 . 11942 [cs.CL].\n\nSenticr: A customized sentiment analysis tool for code review interactions. T Ahmed, A Bosu, A Iqbal, S Rahimi, 32nd IEEE/ACM International Conference on Automated Software Engineering (NIER track), ser. ASE '17. T. Ahmed, A. Bosu, A. Iqbal, and S. Rahimi, \"Senticr: A customized sentiment analysis tool for code review interactions,\" in 32nd IEEE/ACM International Confer- ence on Automated Software Engineering (NIER track), ser. ASE '17, 2017.\n\nEmoji-powered sentiment and emotion detection from software developers' communication data. Z Chen, Y Cao, H Yao, X Lu, X Peng, H Mei, X Liu, https:/doi-org.libproxy.unibz.it/10.1145/3424308ISSN: 1049-331X. DOI: 10 . 1145 / 3424308ACM Trans. Softw. Eng. Methodol. 302Z. Chen, Y. Cao, H. Yao, X. Lu, X. Peng, H. Mei, and X. Liu, \"Emoji-powered sentiment and emotion detection from software developers' communication data,\" ACM Trans. Softw. Eng. Methodol., vol. 30, no. 2, Jan. 2021, ISSN: 1049-331X. DOI: 10 . 1145 / 3424308. [Online]. Available: https://doi-org.libproxy.unibz. it/10.1145/3424308.\n\nApp review analysis via active learning: Reducing supervision effort without compromising classification accuracy. V T Dhinakaran, R Pulle, N Ajmeri, P K Murukannaiah, 10.1109/RE.2018.000262018 IEEE 26th International Requirements Engineering Conference (RE). V. T. Dhinakaran, R. Pulle, N. Ajmeri, and P. K. Mu- rukannaiah, \"App review analysis via active learning: Reducing supervision effort without compromising classification accuracy,\" in 2018 IEEE 26th Interna- tional Requirements Engineering Conference (RE), 2018, pp. 170-181. DOI: 10.1109/RE.2018.00026.\n\nTowards an ontology of terms on technical debt. N S R Alves, L F Ribeiro, V Caires, T S Mendes, R O Sp\u00ednola, 2014 Sixth International Workshop on Managing Technical Debt. N. S. R. Alves, L. F. Ribeiro, V. Caires, T. S. Mendes, and R. O. Sp\u00ednola, \"Towards an ontology of terms on technical debt,\" in 2014 Sixth International Workshop on Managing Technical Debt, 2014, pp. 1-7.\n\nA survey of self-admitted technical debt. G Sierra, E Shihab, Y Kamei, 10.1016/j.jss.2019.02.056Journal of Systems and Software. 152G. Sierra, E. Shihab, and Y. Kamei, \"A survey of self-admitted technical debt,\" Journal of Systems and Software, vol. 152, pp. 70-82, 2019, ISSN: 0164-1212. DOI: https : / / doi . org / 10 . 1016 / j . jss . 2019 . 02 . 056. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S0164121219300457.\n\nIdentifying self-admitted technical debt in open source projects using text mining. Q Huang, E Shihab, X Xia, D Lo, S Li, Empirical Software Engineering. 231Q. Huang, E. Shihab, X. Xia, D. Lo, and S. Li, \"Iden- tifying self-admitted technical debt in open source projects using text mining,\" Empirical Software Engi- neering, vol. 23, no. 1, pp. 418-451, 2018.\n\nSatd detector: A text-mining-based self-admitted technical debt detection tool. Z Liu, Q Huang, X Xia, E Shihab, D Lo, S Li, 2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion). Z. Liu, Q. Huang, X. Xia, E. Shihab, D. Lo, and S. Li, \"Satd detector: A text-mining-based self-admitted technical debt detection tool,\" in 2018 IEEE/ACM 40th International Conference on Software Engineering: Com- panion (ICSE-Companion), 2018, pp. 9-12.\n\nClassifying code comments in java mobile applications. L Pascarella, 2018 IEEE/ACM 5th International Conference on Mobile Software Engineering and Systems (MOBILESoft). L. Pascarella, \"Classifying code comments in java mo- bile applications,\" in 2018 IEEE/ACM 5th International Conference on Mobile Software Engineering and Systems (MOBILESoft), 2018, pp. 39-40.\n\nOn the naturalness of software. A Hindle, E T Barr, M Gabel, Z Su, P Devanbu, Communications of the ACM. 595A. Hindle, E. T. Barr, M. Gabel, Z. Su, and P. Devanbu, \"On the naturalness of software,\" Communications of the ACM, vol. 59, no. 5, pp. 122-131, 2016.\n\nBig code!= big vocabulary: Open-vocabulary models for source code. R.-M Karampatsis, H Babii, R Robbes, C Sutton, A Janes, 2020R.-M. Karampatsis, H. Babii, R. Robbes, C. Sut- ton, and A. Janes, \"Big code!= big vocabulary: Open-vocabulary models for source code,\" in 2020\n\nIEEE/ACM 42nd International Conference on Software Engineering (ICSE). IEEEIEEE/ACM 42nd International Conference on Software Engineering (ICSE), IEEE, 2020, pp. 1073-1085.\n\nCoherence of comments and method implementations: A dataset and an empirical investigation. A Corazza, V Maggio, G Scanniello, 10.1007/s11219-016-9347-1DOI: 10 . 1007 / s11219 -016 -9347 -1Software Quality Journal. 262A. Corazza, V. Maggio, and G. Scanniello, \"Coherence of comments and method implementations: A dataset and an empirical investigation,\" Software Quality Jour- nal, vol. 26, no. 2, pp. 751-777, Jun. 2018, ISSN: 0963- 9314. DOI: 10 . 1007 / s11219 -016 -9347 -1. [Online].\n\n. 10.1007/s11219-016-9347-1Available: https : / / doi . org / 10 . 1007 / s11219 -016 - 9347-1.\n\nWord embeddings for comment coherence. A Cimasa, A Corazza, C Coviello, G Scanniello, 2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA). A. Cimasa, A. Corazza, C. Coviello, and G. Scanniello, \"Word embeddings for comment coherence,\" in 2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA), 2019, pp. 244-251.\n\nLinguistic antipatterns: What they are and how developers perceive them. V Arnaoudova, M Di Penta, G Antoniol, Empirical Software Engineering. 211V. Arnaoudova, M. Di Penta, and G. Antoniol, \"Lin- guistic antipatterns: What they are and how devel- opers perceive them,\" Empirical Software Engineering, vol. 21, no. 1, pp. 104-158, 2016.\n\nA Narayanan, M Chandramohan, R Venkatesan, L Chen, Y Liu, S , arXiv:1707.05005Graph2vec: Learning distributed representations of graphs. cs.AIA. Narayanan, M. Chandramohan, R. Venkatesan, L. Chen, Y. Liu, and S. Jaiswal, Graph2vec: Learning distributed representations of graphs, 2017. arXiv: 1707 . 05005 [cs.AI].\n\nLearning a metric for code readability. R P Buse, W R Weimer, IEEE Transactions on Software Engineering. 364R. P. Buse and W. R. Weimer, \"Learning a metric for code readability,\" IEEE Transactions on Software Engineering, vol. 36, no. 4, pp. 546-558, 2009.\n\nA simpler model of software readability. D Posnett, A Hindle, P Devanbu, https:/doi-org.libproxy.unibz.it/10.1145/1985441.1985454ISBN: 9781450305747. DOI: 10 . 1145 / 1985441 . 1985454Proceedings of the 8th Working Conference on Mining Software Repositories, ser. MSR '11. the 8th Working Conference on Mining Software Repositories, ser. MSR '11Waikiki, Honolulu, HI, USAAssociation for Computing Machinery. OnlineD. Posnett, A. Hindle, and P. Devanbu, \"A simpler model of software readability,\" in Proceedings of the 8th Working Conference on Mining Software Repositories, ser. MSR '11, Waikiki, Honolulu, HI, USA: Associa- tion for Computing Machinery, 2011, pp. 73-82, ISBN: 9781450305747. DOI: 10 . 1145 / 1985441 . 1985454. [On- line]. Available: https://doi-org.libproxy.unibz.it/10. 1145/1985441.1985454.\n\nA general software readability model. J Dorn, J. Dorn, \"A general software readability model,\" 2012.\n\nA comprehensive model for code readability. S Scalabrino, M Linares-V\u00e1squez, R Oliveto, D Poshyvanyk, Journal of Software: Evolution and Process. 306S. Scalabrino, M. Linares-V\u00e1squez, R. Oliveto, and D. Poshyvanyk, \"A comprehensive model for code readability,\" Journal of Software: Evolution and Process, vol. 30, no. 6, e1958, 2018.\n\nDeep code-comment understanding and assessment. D Wang, Y Guo, W Dong, Z Wang, H Liu, S Li, 10.1109/ACCESS.2019.2957424IEEE Access. 7D. Wang, Y. Guo, W. Dong, Z. Wang, H. Liu, and S. Li, \"Deep code-comment understanding and assess- ment,\" IEEE Access, vol. 7, pp. 174 200-174 209, 2019. DOI: 10.1109/ACCESS.2019.2957424.\n\nDeepiac: Deep learning-based linguistic anti-pattern detection in iac. N Borovits, I Kumara, P Krishnan, S D Palma, D Di Nucci, F Palomba, D A Tamburri, W.-J Van Den, Heuvel, 10.1145/3416505.3423564ISBN: 9781450381246. DOI: 10 . 1145 / 3416505 . 3423564Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation, ser. MaLTeSQuE 2020, Virtual, USA: Association for Computing Machinery. the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation, ser. MaLTeSQuE 2020, Virtual, USA: Association for Computing MachineryOnlineN. Borovits, I. Kumara, P. Krishnan, S. D. Palma, D. Di Nucci, F. Palomba, D. A. Tamburri, and W.-J. van den Heuvel, \"Deepiac: Deep learning-based lin- guistic anti-pattern detection in iac,\" in Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Eval- uation, ser. MaLTeSQuE 2020, Virtual, USA: Associa- tion for Computing Machinery, 2020, pp. 7-12, ISBN: 9781450381246. DOI: 10 . 1145 / 3416505 . 3423564. [On- line]. Available: https : / / doi . org / 10 . 1145 / 3416505 . 3423564.\n\nComparing and experimenting machine learning techniques for code smell detection. F A Fontana, M V M\u00e4ntyl\u00e4, M Zanoni, A Marino, Empirical Software Engineering. 213F. A. Fontana, M. V. M\u00e4ntyl\u00e4, M. Zanoni, and A. Marino, \"Comparing and experimenting machine learning techniques for code smell detection,\" Empir- ical Software Engineering, vol. 21, no. 3, pp. 1143-1191, 2016.\n\nDetecting code smells using machine learning techniques: Are we there yet?. D Di Nucci, F Palomba, D A Tamburri, A Serebrenik, A De Lucia, In 2018 ieee 25th international conference on software analysis, evolution and reengineering (saner). IEEED. Di Nucci, F. Palomba, D. A. Tamburri, A. Sere- brenik, and A. De Lucia, \"Detecting code smells us- ing machine learning techniques: Are we there yet?\" In 2018 ieee 25th international conference on software analysis, evolution and reengineering (saner), IEEE, 2018, pp. 612-621.\n\nOn the feasibility of transfer-learning code smells using deep learning. T Sharma, V Efstathiou, P Louridas, D Spinellis, arXiv:1904.03031arXiv preprintT. Sharma, V. Efstathiou, P. Louridas, and D. Spinellis, \"On the feasibility of transfer-learning code smells using deep learning,\" arXiv preprint arXiv:1904.03031, 2019.\n\nCode smell severity classification using machine learning techniques. F , Arcelli Fontana, M Zanoni, 10.1016/j.knosys.2017.04.014128Knowledge-Based SystemsF. Arcelli Fontana and M. Zanoni, \"Code smell sever- ity classification using machine learning techniques,\" Knowledge-Based Systems, vol. 128, pp. 43-58, 2017, ISSN: 0950-7051. DOI: https : / / doi . org / 10 . 1016 / j . knosys . 2017 . 04 . 014. [Online]. Available: http : / / www . sciencedirect . com / science / article / pii / S0950705117301880.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in neural information processing systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Advances in neural information processing systems, 2017, pp. 5998-6008.\n\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, English (US). arXivD. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine translation by jointly learning to align and translate,\" English (US), arXiv, 2014.\n\nT Mikolov, I Sutskever, K Chen, G Corrado, J Dean, arXiv:1310.4546Distributed representations of words and phrases and their compositionality. arXiv preprintT. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, \"Distributed representations of words and phrases and their compositionality,\" arXiv preprint arXiv:1310.4546, 2013.\n\nNeural machine translation of rare words with subword units. R Sennrich, B Haddow, A Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsLong Papers1R. Sennrich, B. Haddow, and A. Birch, \"Neural ma- chine translation of rare words with subword units,\" in Proceedings of the 54th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), 2016, pp. 1715-1725.\n\nZ Feng, D Guo, D Tang, N Duan, X Feng, M Gong, L Shou, B Qin, T Liu, D Jiang, M Zhou, arXiv:2002.08155Codebert: A pre-trained model for programming and natural languages. 2020cs.CLZ. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, Code- bert: A pre-trained model for programming and natural languages, 2020. arXiv: 2002.08155 [cs.CL].\n\nWord embeddings for the software engineering domain. V Efstathiou, C Chatzilenas, D Spinellis, Proceedings of the 15th international conference on mining software repositories. the 15th international conference on mining software repositoriesV. Efstathiou, C. Chatzilenas, and D. Spinellis, \"Word embeddings for the software engineering domain,\" in Proceedings of the 15th international conference on mining software repositories, 2018, pp. 38-41.\n\nSentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. J Phang, T F\u00e9vry, S R Bowman, arXiv:1811.01088arXiv preprintJ. Phang, T. F\u00e9vry, and S. R. Bowman, \"Sen- tence encoders on stilts: Supplementary training on intermediate labeled-data tasks,\" arXiv preprint arXiv:1811.01088, 2018.\n\nTanda: Transfer and adapt pre-trained transformer models for answer sentence selection. S Garg, T Vu, A Moschitti, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34S. Garg, T. Vu, and A. Moschitti, \"Tanda: Transfer and adapt pre-trained transformer models for answer sen- tence selection,\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, 2020, pp. 7780-7788.\n\nBowman, Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work?. Y Pruksachatkun, J Phang, H Liu, P M Htut, X Zhang, R Y Pang, C Vania, K Kann, S R , arXiv:2005.00628cs.CLY. Pruksachatkun, J. Phang, H. Liu, P. M. Htut, X. Zhang, R. Y. Pang, C. Vania, K. Kann, and S. R. Bow- man, Intermediate-task transfer learning with pretrained models for natural language understanding: When and why does it work? 2020. arXiv: 2005.00628 [cs.CL].\n\nProbability of error of some adaptive pattern-recognition machines. H Scudder, IEEE Transactions on Information Theory. 113H. Scudder, \"Probability of error of some adaptive pattern-recognition machines,\" IEEE Transactions on Information Theory, vol. 11, no. 3, pp. 363-371, 1965.\n\nUnsupervised word sense disambiguation rivaling supervised methods. D Yarowsky, 10.3115/981658.981684DOI: 10 . 3115 / 981658 . 98168433rd Annual Meeting of the Association for Computational Linguistics. Cambridge, Massachusetts, USAAssociation for Computational LinguisticsD. Yarowsky, \"Unsupervised word sense disambigua- tion rivaling supervised methods,\" in 33rd Annual Meeting of the Association for Computational Linguis- tics, Cambridge, Massachusetts, USA: Association for Computational Linguistics, Jun. 1995, pp. 189-196. DOI: 10 . 3115 / 981658 . 981684. [Online]. Available: https://www.aclweb.org/anthology/P95-1026.\n\nCharacter-level convolutional networks for text classification. X Zhang, J Zhao, Y Lecun, arXiv:1509.01626cs.LGX. Zhang, J. Zhao, and Y. LeCun, Character-level convo- lutional networks for text classification, 2016. arXiv: 1509. 01626 [cs.LG].\n\nEda: Easy data augmentation techniques for boosting performance on text classification tasks. J Wei, K Zou, arXiv:1901.11196cs.CLJ. Wei and K. Zou, Eda: Easy data augmentation tech- niques for boosting performance on text classification tasks, 2019. arXiv: 1901.11196 [cs.CL].\n\nX Jiao, Y Yin, L Shang, X Jiang, X Chen, L Li, F Wang, Q Liu, arXiv:1909.10351Tinybert: Distilling bert for natural language understanding. cs.CLX. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu, Tinybert: Distilling bert for nat- ural language understanding, 2020. arXiv: 1909 . 10351 [cs.CL].\n\nThat's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets. W Y Wang, D Yang, 10.18653/v1/D15-1306Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsW. Y. Wang and D. Yang, \"That's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets,\" in Proceedings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing, Lisbon, Portugal: Association for Computational Linguistics, Sep. 2015, pp. 2557-2563. DOI: 10.18653/v1/D15-1306. [Online].\n\nBae: Bert-based adversarial examples for text classification. S Garg, G Ramakrishnan, arXiv:2004.019702020cs.CLS. Garg and G. Ramakrishnan, Bae: Bert-based adver- sarial examples for text classification, 2020. arXiv: 2004. 01970 [cs.CL].\n\nImproving neural machine translation models with monolingual data. R Sennrich, B Haddow, A Birch, arXiv:1511.06709cs.CLR. Sennrich, B. Haddow, and A. Birch, Improving neural machine translation models with monolingual data, 2016. arXiv: 1511.06709 [cs.CL].\n\nActive learning literature survey. B Settles, 1648Computer SciencesTechnical ReportUniversity of Wisconsin-MadisonB. Settles, \"Active learning literature survey,\" Univer- sity of Wisconsin-Madison, Computer Sciences Tech- nical Report 1648, 2009.\n\nDoes bert understand code? -an exploratory study on the detection of architectural tactics in code. J Keim, A Kaplan, A Koziolek, M Mirakhorli, ISBN: 978-3-030-58923-3Software Architecture, A. Jansen, I. Malavolta, H. Muccini, I. Ozkaya, and O. ZimmermannSpringer International PublishingChamJ. Keim, A. Kaplan, A. Koziolek, and M. Mirakhorli, \"Does bert understand code? -an exploratory study on the detection of architectural tactics in code,\" in Software Architecture, A. Jansen, I. Malavolta, H. Muc- cini, I. Ozkaya, and O. Zimmermann, Eds., Cham: Springer International Publishing, 2020, pp. 220-228, ISBN: 978-3-030-58923-3.\n\nIntellicode compose: Code generation using transformer. A Svyatkovskiy, S K Deng, S Fu, N Sundaresan, https:/doi-org.libproxy.unibz.it/10.1145/3368089.3417058Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringNew York, NY, USAAssociation for Computing Machinery9781450370431A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sun- daresan, \"Intellicode compose: Code generation using transformer,\" in Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineer- ing. New York, NY, USA: Association for Computing Machinery, 2020, pp. 1433-1443, ISBN: 9781450370431. [Online]. Available: https://doi-org.libproxy.unibz. it/10.1145/3368089.3417058.\n\nCodeSearchNet Challenge: Evaluating the State of Semantic Code Search. H Husain, H.-H Wu, T Gazit, M Allamanis, M Brockschmidt, arXiv:1909.09436arXiv: 1909.09436cs, stat. visited on 03/12/2020H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt, \"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search,\" arXiv:1909.09436 [cs, stat], Sep. 2019, arXiv: 1909.09436. [Online]. Available: http://arxiv.org/abs/1909.09436 (visited on 03/12/2020).\n\nAuto-weka: Automatic model selection and hyperparameter optimization in weka. L Kotthoff, C Thornton, H H Hoos, F Hutter, K Leyton-Brown, Automated Machine Learning. ChamSpringerL. Kotthoff, C. Thornton, H. H. Hoos, F. Hutter, and K. Leyton-Brown, \"Auto-weka: Automatic model se- lection and hyperparameter optimization in weka,\" in Automated Machine Learning, Springer, Cham, 2019, pp. 81-95.\n\nHuman-in-the-Loop Machine Learning. R Munro, Manning Publications Co9781617296741R. Munro, Human-in-the-Loop Machine Learning. Man- ning Publications Co., 2021, ISBN: 9781617296741.\n\nCode prediction by feeding trees to transformers. S Kim, J Zhao, Y Tian, S Chandra, arXiv:2003.13848arXiv preprintS. Kim, J. Zhao, Y. Tian, and S. Chandra, \"Code predic- tion by feeding trees to transformers,\" arXiv preprint arXiv:2003.13848, 2020.\n\nNovel positional encodings to enable tree-based transformers. V Shiv, C Quirk, Advances in Neural Information Processing Systems. 1291V. Shiv and C. Quirk, \"Novel positional encodings to enable tree-based transformers,\" in Advances in Neural Information Processing Systems, 2019, pp. 12 081-12 091.\n\nAn empirical investigation of catastrophic forgetting in gradient-based neural networks. I J Goodfellow, M Mirza, D Xiao, A Courville, Y Bengio, arXiv:1312.6211stat.MLI. J. Goodfellow, M. Mirza, D. Xiao, A. Courville, and Y. Bengio, An empirical investigation of catastrophic forgetting in gradient-based neural networks, 2015. arXiv: 1312.6211 [stat.ML].\n\nTraceability transformed: Generating more accurate links with pre-trained bert models. J Lin, Y Liu, Q Zeng, M Jiang, J Cleland-Huang, 2021J. Lin, Y. Liu, Q. Zeng, M. Jiang, and J. Cleland- Huang, \"Traceability transformed: Generating more accurate links with pre-trained bert models,\" in 2021\n\n10.1109/ICSE43902.2021.00040DOI: 10.1109/ ICSE43902.2021.00040IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE/ACM 43rd International Conference on Software Engineering (ICSE), 2021, pp. 324-335. DOI: 10.1109/ ICSE43902.2021.00040.\n\nBetter data labelling with emblem (and how that impacts defect prediction). H Tu, Z Yu, T Menzies, IEEE Transactions on Software Engineering. H. Tu, Z. Yu, and T. Menzies, \"Better data labelling with emblem (and how that impacts defect predic- tion),\" IEEE Transactions on Software Engineering, 2020.\n\nFine-tuning pretrained language models: Weight initializations, data orders, and early stopping. J Dodge, G Ilharco, R Schwartz, A Farhadi, H Hajishirzi, N Smith, arXiv:2002.06305arXiv preprintJ. Dodge, G. Ilharco, R. Schwartz, A. Farhadi, H. Hajishirzi, and N. Smith, \"Fine-tuning pretrained lan- guage models: Weight initializations, data orders, and early stopping,\" arXiv preprint arXiv:2002.06305, 2020.\n\nHuggingface's transformers: State-ofthe-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T L Scao, S Gugger, M Drame, Q Lhoest, A M Rush, arXiv:1910.037712020cs.CLT. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, Huggingface's transformers: State-of- the-art natural language processing, 2020. arXiv: 1910 . 03771 [cs.CL].\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking the inception architecture for computer vision,\" in Proceedings of the IEEE confer- ence on computer vision and pattern recognition, 2016, pp. 2818-2826.\n\nWhen does label smoothing help?. R M\u00fcller, S Kornblith, G E Hinton, Advances in Neural Information Processing Systems. R. M\u00fcller, S. Kornblith, and G. E. Hinton, \"When does label smoothing help?\" In Advances in Neural Information Processing Systems, 2019, pp. 4694-4703.\n\nWeka: A machine learning workbench for data mining.,\" in Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers. E Frank, M A Hall, G Holmes, R Kirkby, B Pfahringer, I H Witten, O. Maimon and L. RokachSpringerE. Frank, M. A. Hall, G. Holmes, R. Kirkby, B. Pfahringer, and I. H. Witten, \"Weka: A machine learning workbench for data mining.,\" in Data Min- ing and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers, O. Maimon and L. Rokach, Eds. Berlin: Springer, 2005, pp. 1305- 1314. [Online]. Available: http : / / researchcommons . waikato.ac.nz/handle/10289/1497.\n\nScikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour- napeau, M. Brucher, M. Perrot, and E. Duchesnay, \"Scikit-learn: Machine learning in Python,\" Journal of Machine Learning Research, vol. 12, pp. 2825-2830, 2011.\n\nScaling tree-based automated machine learning to biomedical big data with a feature set selector. T T Le, W Fu, J H Moore, Bioinformatics. 361T. T. Le, W. Fu, and J. H. Moore, \"Scaling tree-based automated machine learning to biomedical big data with a feature set selector,\" Bioinformatics, vol. 36, no. 1, pp. 250-256, 2020.\n", "annotations": {"author": "[{\"end\":107,\"start\":87},{\"end\":122,\"start\":108}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":99},{\"end\":121,\"start\":115}]", "author_first_name": "[{\"end\":93,\"start\":87},{\"end\":98,\"start\":94},{\"end\":114,\"start\":108}]", "author_affiliation": null, "title": "[{\"end\":84,\"start\":1},{\"end\":206,\"start\":123}]", "venue": null, "abstract": "[{\"end\":1822,\"start\":339}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2250,\"start\":2247},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3177,\"start\":3174},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3182,\"start\":3179},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3860,\"start\":3857},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4179,\"start\":4176},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4184,\"start\":4181},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4279,\"start\":4276},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4295,\"start\":4291},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4433,\"start\":4429},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4448,\"start\":4444},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5029,\"start\":5025},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5035,\"start\":5031},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7603,\"start\":7600},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7608,\"start\":7605},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7614,\"start\":7610},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7620,\"start\":7616},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7879,\"start\":7877},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9899,\"start\":9895},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9905,\"start\":9901},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10166,\"start\":10163},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10570,\"start\":10566},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11131,\"start\":11127},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11706,\"start\":11702},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11733,\"start\":11729},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12093,\"start\":12089},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12112,\"start\":12108},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12404,\"start\":12401},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12595,\"start\":12591},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12611,\"start\":12607},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12795,\"start\":12791},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12933,\"start\":12929},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13046,\"start\":13042},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14335,\"start\":14331},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14576,\"start\":14573},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14700,\"start\":14697},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15030,\"start\":15027},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15108,\"start\":15104},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15468,\"start\":15464},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15487,\"start\":15484},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15746,\"start\":15742},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15752,\"start\":15748},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15923,\"start\":15919},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16186,\"start\":16182},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16299,\"start\":16295},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16357,\"start\":16353},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16363,\"start\":16359},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16386,\"start\":16382},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16668,\"start\":16664},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16778,\"start\":16774},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16808,\"start\":16804},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16905,\"start\":16902},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":17371,\"start\":17367},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18135,\"start\":18131},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":18791,\"start\":18787},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":18976,\"start\":18972},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19097,\"start\":19093},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19103,\"start\":19099},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":19109,\"start\":19105},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19205,\"start\":19201},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":19211,\"start\":19207},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19681,\"start\":19677},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19695,\"start\":19691},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19815,\"start\":19811},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19829,\"start\":19825},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":19930,\"start\":19926},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20160,\"start\":20156},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":20477,\"start\":20473},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":20585,\"start\":20581},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":20731,\"start\":20727},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":20766,\"start\":20762},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":20928,\"start\":20924},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":21511,\"start\":21507},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":21910,\"start\":21906},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22608,\"start\":22605},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23344,\"start\":23341},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":23617,\"start\":23613},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23811,\"start\":23808},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23816,\"start\":23813},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23908,\"start\":23904},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23914,\"start\":23910},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":24146,\"start\":24142},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24351,\"start\":24347},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24367,\"start\":24363},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25341,\"start\":25337},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":25694,\"start\":25690},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":25993,\"start\":25989},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26172,\"start\":26169},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":26468,\"start\":26464},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":26483,\"start\":26479},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":27145,\"start\":27141},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":27296,\"start\":27292},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":27302,\"start\":27298},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":28649,\"start\":28645},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":28655,\"start\":28651},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":28709,\"start\":28705},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":28782,\"start\":28778},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":28788,\"start\":28784},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":28872,\"start\":28868},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":28981,\"start\":28977},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":31240,\"start\":31236},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33015,\"start\":33012},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33085,\"start\":33081},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33105,\"start\":33101},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33216,\"start\":33212},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":33437,\"start\":33433},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33617,\"start\":33613},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":33819,\"start\":33815},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":34067,\"start\":34063},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":34212,\"start\":34208},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34994,\"start\":34991},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35100,\"start\":35097},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":37084,\"start\":37080},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":38481,\"start\":38477},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":39545,\"start\":39541},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":40716,\"start\":40713},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":40719,\"start\":40716},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40722,\"start\":40719},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":40725,\"start\":40722},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":41744,\"start\":41741},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":41750,\"start\":41746},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":41756,\"start\":41752},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":41794,\"start\":41790},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":41825,\"start\":41821},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":41831,\"start\":41827},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":41875,\"start\":41871},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":41903,\"start\":41899},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":41942,\"start\":41938},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":41973,\"start\":41970},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":42017,\"start\":42013},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":42050,\"start\":42046},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":43757,\"start\":43753},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":43854,\"start\":43851},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":44598,\"start\":44594},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":44755,\"start\":44751},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":44971,\"start\":44968},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":45211,\"start\":45207},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":45574,\"start\":45570},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":49127,\"start\":49123},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":50786,\"start\":50782},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":52285,\"start\":52281},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":57149,\"start\":57145},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":59200,\"start\":59196},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":59206,\"start\":59202},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":59890,\"start\":59886},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":60251,\"start\":60247},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":60406,\"start\":60402},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":60984,\"start\":60980},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":61468,\"start\":61464},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":64708,\"start\":64704},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":64727,\"start\":64723},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":65210,\"start\":65206},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":65606,\"start\":65602},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":68538,\"start\":68534},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":69247,\"start\":69243},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":69819,\"start\":69815},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":69825,\"start\":69821},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":71088,\"start\":71084},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":71828,\"start\":71824},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":72040,\"start\":72036},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":72141,\"start\":72137},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":72645,\"start\":72643},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":72880,\"start\":72876},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":73006,\"start\":73002},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":73115,\"start\":73112},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":73220,\"start\":73216},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":73293,\"start\":73289},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":73404,\"start\":73400},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":73475,\"start\":73471},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":73586,\"start\":73582},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":74272,\"start\":74268},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":81706,\"start\":81702},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":81902,\"start\":81898}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":77166,\"start\":76311},{\"attributes\":{\"id\":\"fig_1\"},\"end\":77358,\"start\":77167},{\"attributes\":{\"id\":\"fig_4\"},\"end\":77620,\"start\":77359},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":77716,\"start\":77621},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":78350,\"start\":77717},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":78783,\"start\":78351},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":78851,\"start\":78784},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":79618,\"start\":78852},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":80036,\"start\":79619},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":81277,\"start\":80037},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":81485,\"start\":81278},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":83476,\"start\":81486},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":84122,\"start\":83477},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":84970,\"start\":84123},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":85171,\"start\":84971},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":86417,\"start\":85172},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":88374,\"start\":86418}]", "paragraph": "[{\"end\":2871,\"start\":1838},{\"end\":3466,\"start\":2873},{\"end\":4938,\"start\":3468},{\"end\":6022,\"start\":4940},{\"end\":6112,\"start\":6024},{\"end\":7387,\"start\":6114},{\"end\":8399,\"start\":7417},{\"end\":8502,\"start\":8401},{\"end\":8784,\"start\":8532},{\"end\":9627,\"start\":8813},{\"end\":10341,\"start\":9629},{\"end\":11083,\"start\":10378},{\"end\":11955,\"start\":11113},{\"end\":13022,\"start\":11972},{\"end\":13337,\"start\":13024},{\"end\":13549,\"start\":13367},{\"end\":13986,\"start\":13592},{\"end\":15052,\"start\":13988},{\"end\":15365,\"start\":15079},{\"end\":15979,\"start\":15382},{\"end\":16300,\"start\":16007},{\"end\":16855,\"start\":16338},{\"end\":17268,\"start\":16886},{\"end\":18075,\"start\":17270},{\"end\":18792,\"start\":18118},{\"end\":19055,\"start\":18824},{\"end\":19897,\"start\":19057},{\"end\":21077,\"start\":19914},{\"end\":21419,\"start\":21092},{\"end\":22560,\"start\":21466},{\"end\":22934,\"start\":22600},{\"end\":23715,\"start\":22936},{\"end\":24353,\"start\":23717},{\"end\":24671,\"start\":24355},{\"end\":25396,\"start\":24687},{\"end\":26327,\"start\":25435},{\"end\":27213,\"start\":26353},{\"end\":27859,\"start\":27231},{\"end\":29587,\"start\":27902},{\"end\":29840,\"start\":29607},{\"end\":31303,\"start\":29842},{\"end\":32039,\"start\":31319},{\"end\":32833,\"start\":32041},{\"end\":33197,\"start\":32899},{\"end\":33600,\"start\":33199},{\"end\":33801,\"start\":33602},{\"end\":34041,\"start\":33803},{\"end\":34431,\"start\":34043},{\"end\":34711,\"start\":34447},{\"end\":34860,\"start\":34734},{\"end\":35223,\"start\":34862},{\"end\":35619,\"start\":35250},{\"end\":36102,\"start\":35642},{\"end\":36314,\"start\":36135},{\"end\":37882,\"start\":36332},{\"end\":38059,\"start\":37884},{\"end\":38370,\"start\":38099},{\"end\":38881,\"start\":38400},{\"end\":39148,\"start\":38883},{\"end\":39349,\"start\":39150},{\"end\":39502,\"start\":39359},{\"end\":39939,\"start\":39504},{\"end\":40339,\"start\":39941},{\"end\":40557,\"start\":40341},{\"end\":42103,\"start\":40585},{\"end\":42134,\"start\":42105},{\"end\":42181,\"start\":42152},{\"end\":42343,\"start\":42215},{\"end\":42813,\"start\":42370},{\"end\":43276,\"start\":42843},{\"end\":43769,\"start\":43301},{\"end\":44533,\"start\":43771},{\"end\":44875,\"start\":44535},{\"end\":45213,\"start\":44907},{\"end\":45443,\"start\":45252},{\"end\":45780,\"start\":45476},{\"end\":47168,\"start\":45816},{\"end\":48916,\"start\":47170},{\"end\":49085,\"start\":48918},{\"end\":49538,\"start\":49101},{\"end\":50215,\"start\":49540},{\"end\":50299,\"start\":50217},{\"end\":51871,\"start\":50333},{\"end\":51978,\"start\":51873},{\"end\":52574,\"start\":52206},{\"end\":53715,\"start\":52576},{\"end\":54831,\"start\":53737},{\"end\":54885,\"start\":54833},{\"end\":56757,\"start\":54903},{\"end\":56874,\"start\":56759},{\"end\":57521,\"start\":56905},{\"end\":58118,\"start\":57531},{\"end\":58228,\"start\":58120},{\"end\":59397,\"start\":58306},{\"end\":60407,\"start\":59399},{\"end\":60764,\"start\":60429},{\"end\":61172,\"start\":60796},{\"end\":61469,\"start\":61174},{\"end\":61757,\"start\":61498},{\"end\":62158,\"start\":61780},{\"end\":62685,\"start\":62160},{\"end\":63938,\"start\":62707},{\"end\":64569,\"start\":63940},{\"end\":65177,\"start\":64571},{\"end\":65765,\"start\":65179},{\"end\":66298,\"start\":65781},{\"end\":66643,\"start\":66300},{\"end\":67008,\"start\":66645},{\"end\":67351,\"start\":67010},{\"end\":67797,\"start\":67353},{\"end\":68154,\"start\":67799},{\"end\":68761,\"start\":68208},{\"end\":69638,\"start\":68763},{\"end\":70392,\"start\":69716},{\"end\":70909,\"start\":70394},{\"end\":71503,\"start\":70938},{\"end\":71968,\"start\":71505},{\"end\":72487,\"start\":71970},{\"end\":72812,\"start\":72514},{\"end\":73254,\"start\":72834},{\"end\":73541,\"start\":73256},{\"end\":73800,\"start\":73543},{\"end\":74175,\"start\":73872},{\"end\":74393,\"start\":74202},{\"end\":74569,\"start\":74395},{\"end\":75186,\"start\":74571},{\"end\":75404,\"start\":75188},{\"end\":75954,\"start\":75406},{\"end\":76310,\"start\":75981}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":36118,\"start\":36103},{\"attributes\":{\"id\":\"formula_1\"},\"end\":52194,\"start\":51997},{\"attributes\":{\"id\":\"formula_2\"},\"end\":53736,\"start\":53716}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8501,\"start\":8494},{\"end\":9625,\"start\":9618},{\"end\":10232,\"start\":10223},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14119,\"start\":14111},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":42812,\"start\":42805},{\"end\":42967,\"start\":42959},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":43345,\"start\":43336},{\"end\":43393,\"start\":43384},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":43768,\"start\":43758},{\"end\":43945,\"start\":43938},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":44410,\"start\":44401},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44717,\"start\":44709},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":44982,\"start\":44975},{\"end\":45364,\"start\":45355},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45679,\"start\":45669},{\"end\":46239,\"start\":46231},{\"end\":47565,\"start\":47558},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":48085,\"start\":48070},{\"end\":49677,\"start\":49670},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":50747,\"start\":50738},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":51143,\"start\":51135},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":51296,\"start\":51288},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":52733,\"start\":52724},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":53144,\"start\":53136},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":55483,\"start\":55473},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":57640,\"start\":57630},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":68807,\"start\":68799},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":68890,\"start\":68882},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":70710,\"start\":70702},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":71611,\"start\":71603},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":72608,\"start\":72598},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":73660,\"start\":73652},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":76191,\"start\":76183}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1836,\"start\":1824},{\"attributes\":{\"n\":\"2\"},\"end\":7415,\"start\":7390},{\"attributes\":{\"n\":\"2.1\"},\"end\":8530,\"start\":8505},{\"end\":8811,\"start\":8787},{\"end\":10376,\"start\":10344},{\"end\":11111,\"start\":11086},{\"end\":11970,\"start\":11958},{\"attributes\":{\"n\":\"2.2\"},\"end\":13365,\"start\":13340},{\"end\":13590,\"start\":13552},{\"end\":15077,\"start\":15055},{\"end\":15380,\"start\":15368},{\"attributes\":{\"n\":\"2.3\"},\"end\":16005,\"start\":15982},{\"end\":16336,\"start\":16303},{\"end\":16884,\"start\":16858},{\"end\":18116,\"start\":18078},{\"end\":18822,\"start\":18795},{\"end\":19912,\"start\":19900},{\"attributes\":{\"n\":\"3\"},\"end\":21090,\"start\":21080},{\"attributes\":{\"n\":\"3.1\"},\"end\":21464,\"start\":21422},{\"end\":22598,\"start\":22563},{\"end\":24685,\"start\":24674},{\"end\":25433,\"start\":25399},{\"attributes\":{\"n\":\"3.2\"},\"end\":26351,\"start\":26330},{\"end\":27229,\"start\":27216},{\"end\":27900,\"start\":27862},{\"end\":29605,\"start\":29590},{\"end\":31317,\"start\":31306},{\"attributes\":{\"n\":\"3.3\"},\"end\":32897,\"start\":32836},{\"attributes\":{\"n\":\"4\"},\"end\":34445,\"start\":34434},{\"attributes\":{\"n\":\"4.1\"},\"end\":34732,\"start\":34714},{\"end\":35248,\"start\":35226},{\"end\":35640,\"start\":35622},{\"end\":36133,\"start\":36120},{\"attributes\":{\"n\":\"4.2\"},\"end\":36330,\"start\":36317},{\"attributes\":{\"n\":\"4.3\"},\"end\":38097,\"start\":38062},{\"end\":38398,\"start\":38373},{\"end\":39357,\"start\":39352},{\"attributes\":{\"n\":\"4.4\"},\"end\":40583,\"start\":40560},{\"end\":42150,\"start\":42137},{\"end\":42213,\"start\":42184},{\"attributes\":{\"n\":\"5\"},\"end\":42368,\"start\":42346},{\"attributes\":{\"n\":\"5.1\"},\"end\":42841,\"start\":42816},{\"end\":43299,\"start\":43279},{\"end\":44905,\"start\":44878},{\"end\":45250,\"start\":45216},{\"end\":45474,\"start\":45446},{\"end\":45788,\"start\":45783},{\"end\":45799,\"start\":45791},{\"attributes\":{\"n\":\"5.2\"},\"end\":45814,\"start\":45802},{\"attributes\":{\"n\":\"5.3\"},\"end\":49099,\"start\":49088},{\"attributes\":{\"n\":\"5.4\"},\"end\":50331,\"start\":50302},{\"attributes\":{\"n\":\"5.5\"},\"end\":51996,\"start\":51981},{\"end\":52204,\"start\":52196},{\"attributes\":{\"n\":\"5.6\"},\"end\":54901,\"start\":54888},{\"attributes\":{\"n\":\"5.7\"},\"end\":56903,\"start\":56877},{\"end\":57529,\"start\":57524},{\"attributes\":{\"n\":\"6\"},\"end\":58241,\"start\":58231},{\"attributes\":{\"n\":\"6.1\"},\"end\":58283,\"start\":58244},{\"end\":58304,\"start\":58286},{\"end\":60427,\"start\":60410},{\"end\":60794,\"start\":60767},{\"attributes\":{\"n\":\"6.2\"},\"end\":61496,\"start\":61472},{\"end\":61778,\"start\":61760},{\"end\":62705,\"start\":62688},{\"attributes\":{\"n\":\"7\"},\"end\":65779,\"start\":65768},{\"end\":68206,\"start\":68157},{\"end\":69670,\"start\":69641},{\"end\":69692,\"start\":69673},{\"end\":69714,\"start\":69695},{\"end\":70936,\"start\":70912},{\"end\":72512,\"start\":72490},{\"end\":72832,\"start\":72815},{\"end\":73870,\"start\":73803},{\"end\":74200,\"start\":74178},{\"end\":75979,\"start\":75957},{\"end\":76313,\"start\":76312},{\"end\":77176,\"start\":77168},{\"end\":77368,\"start\":77360},{\"end\":77629,\"start\":77622},{\"end\":78792,\"start\":78785},{\"end\":79633,\"start\":79620},{\"end\":81286,\"start\":81279},{\"end\":83489,\"start\":83478},{\"end\":84135,\"start\":84124},{\"end\":84980,\"start\":84972},{\"end\":86442,\"start\":86419}]", "table": "[{\"end\":78350,\"start\":77897},{\"end\":79618,\"start\":78973},{\"end\":80036,\"start\":79879},{\"end\":81277,\"start\":80180},{\"end\":83476,\"start\":81926},{\"end\":84122,\"start\":83492},{\"end\":84970,\"start\":84677},{\"end\":85171,\"start\":84983},{\"end\":86417,\"start\":85988},{\"end\":88374,\"start\":87899}]", "figure_caption": "[{\"end\":77166,\"start\":76314},{\"end\":77358,\"start\":77178},{\"end\":77620,\"start\":77370},{\"end\":77716,\"start\":77631},{\"end\":77897,\"start\":77719},{\"end\":78783,\"start\":78353},{\"end\":78851,\"start\":78794},{\"end\":78973,\"start\":78854},{\"end\":79879,\"start\":79635},{\"end\":80180,\"start\":80039},{\"end\":81485,\"start\":81288},{\"end\":81926,\"start\":81488},{\"end\":84677,\"start\":84138},{\"end\":85988,\"start\":85174},{\"end\":87899,\"start\":86447}]", "figure_ref": "[{\"end\":29388,\"start\":29380},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":48411,\"start\":48403},{\"end\":50574,\"start\":50566},{\"end\":53363,\"start\":53357},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":53714,\"start\":53706},{\"end\":54149,\"start\":54141},{\"end\":55752,\"start\":55746},{\"end\":55896,\"start\":55888},{\"end\":56429,\"start\":56421},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":68983,\"start\":68975}]", "bib_author_first_name": "[{\"end\":89641,\"start\":89640},{\"end\":89649,\"start\":89648},{\"end\":89657,\"start\":89656},{\"end\":89670,\"start\":89666},{\"end\":89676,\"start\":89675},{\"end\":89682,\"start\":89681},{\"end\":90043,\"start\":90042},{\"end\":90055,\"start\":90054},{\"end\":90069,\"start\":90068},{\"end\":90080,\"start\":90079},{\"end\":90089,\"start\":90088},{\"end\":90575,\"start\":90574},{\"end\":90582,\"start\":90581},{\"end\":90594,\"start\":90593},{\"end\":90604,\"start\":90603},{\"end\":90607,\"start\":90605},{\"end\":90616,\"start\":90615},{\"end\":90625,\"start\":90624},{\"end\":91048,\"start\":91047},{\"end\":91055,\"start\":91054},{\"end\":91063,\"start\":91062},{\"end\":91070,\"start\":91069},{\"end\":91076,\"start\":91075},{\"end\":91084,\"start\":91083},{\"end\":91494,\"start\":91493},{\"end\":91496,\"start\":91495},{\"end\":91506,\"start\":91505},{\"end\":91510,\"start\":91507},{\"end\":91520,\"start\":91519},{\"end\":91522,\"start\":91521},{\"end\":91931,\"start\":91930},{\"end\":91938,\"start\":91937},{\"end\":91940,\"start\":91939},{\"end\":92197,\"start\":92196},{\"end\":92199,\"start\":92198},{\"end\":92209,\"start\":92208},{\"end\":92220,\"start\":92219},{\"end\":92229,\"start\":92228},{\"end\":92240,\"start\":92239},{\"end\":92249,\"start\":92248},{\"end\":92256,\"start\":92255},{\"end\":92577,\"start\":92576},{\"end\":92587,\"start\":92586},{\"end\":92836,\"start\":92835},{\"end\":92849,\"start\":92845},{\"end\":92858,\"start\":92857},{\"end\":92865,\"start\":92864},{\"end\":93067,\"start\":93066},{\"end\":93074,\"start\":93073},{\"end\":93081,\"start\":93080},{\"end\":93090,\"start\":93089},{\"end\":93096,\"start\":93095},{\"end\":93105,\"start\":93104},{\"end\":93113,\"start\":93112},{\"end\":93121,\"start\":93120},{\"end\":93130,\"start\":93129},{\"end\":93145,\"start\":93144},{\"end\":93517,\"start\":93516},{\"end\":93525,\"start\":93524},{\"end\":93534,\"start\":93533},{\"end\":93545,\"start\":93544},{\"end\":93553,\"start\":93552},{\"end\":93561,\"start\":93560},{\"end\":93563,\"start\":93562},{\"end\":93843,\"start\":93842},{\"end\":93856,\"start\":93855},{\"end\":93865,\"start\":93864},{\"end\":93876,\"start\":93875},{\"end\":94133,\"start\":94132},{\"end\":94143,\"start\":94142},{\"end\":94561,\"start\":94560},{\"end\":94571,\"start\":94570},{\"end\":94581,\"start\":94580},{\"end\":94583,\"start\":94582},{\"end\":94999,\"start\":94998},{\"end\":95008,\"start\":95007},{\"end\":95014,\"start\":95013},{\"end\":95023,\"start\":95022},{\"end\":95025,\"start\":95024},{\"end\":95036,\"start\":95035},{\"end\":95042,\"start\":95041},{\"end\":95459,\"start\":95458},{\"end\":95466,\"start\":95465},{\"end\":95474,\"start\":95473},{\"end\":95486,\"start\":95485},{\"end\":95488,\"start\":95487},{\"end\":95859,\"start\":95858},{\"end\":95869,\"start\":95868},{\"end\":95871,\"start\":95870},{\"end\":95884,\"start\":95883},{\"end\":95895,\"start\":95894},{\"end\":96407,\"start\":96406},{\"end\":96419,\"start\":96418},{\"end\":96431,\"start\":96430},{\"end\":96443,\"start\":96442},{\"end\":96736,\"start\":96735},{\"end\":96743,\"start\":96742},{\"end\":96755,\"start\":96754},{\"end\":96766,\"start\":96765},{\"end\":96769,\"start\":96767},{\"end\":96778,\"start\":96777},{\"end\":96787,\"start\":96786},{\"end\":97201,\"start\":97200},{\"end\":97209,\"start\":97208},{\"end\":97216,\"start\":97215},{\"end\":97218,\"start\":97217},{\"end\":97225,\"start\":97224},{\"end\":97233,\"start\":97232},{\"end\":97654,\"start\":97653},{\"end\":97664,\"start\":97663},{\"end\":97678,\"start\":97677},{\"end\":97687,\"start\":97686},{\"end\":97979,\"start\":97978},{\"end\":97983,\"start\":97980},{\"end\":97996,\"start\":97995},{\"end\":98006,\"start\":98005},{\"end\":98351,\"start\":98350},{\"end\":98365,\"start\":98364},{\"end\":98728,\"start\":98727},{\"end\":98739,\"start\":98738},{\"end\":98749,\"start\":98748},{\"end\":98760,\"start\":98759},{\"end\":98978,\"start\":98977},{\"end\":98987,\"start\":98986},{\"end\":98996,\"start\":98995},{\"end\":99005,\"start\":99004},{\"end\":99014,\"start\":99013},{\"end\":99016,\"start\":99015},{\"end\":99024,\"start\":99023},{\"end\":99634,\"start\":99633},{\"end\":99648,\"start\":99647},{\"end\":99667,\"start\":99666},{\"end\":99681,\"start\":99680},{\"end\":100057,\"start\":100056},{\"end\":100071,\"start\":100070},{\"end\":100081,\"start\":100080},{\"end\":100090,\"start\":100089},{\"end\":100092,\"start\":100091},{\"end\":100101,\"start\":100100},{\"end\":100474,\"start\":100473},{\"end\":100482,\"start\":100481},{\"end\":100492,\"start\":100491},{\"end\":100506,\"start\":100505},{\"end\":100517,\"start\":100516},{\"end\":100528,\"start\":100527},{\"end\":100540,\"start\":100539},{\"end\":101356,\"start\":101355},{\"end\":101370,\"start\":101369},{\"end\":101380,\"start\":101379},{\"end\":101389,\"start\":101388},{\"end\":101400,\"start\":101399},{\"end\":102250,\"start\":102249},{\"end\":102252,\"start\":102251},{\"end\":102263,\"start\":102262},{\"end\":102276,\"start\":102275},{\"end\":102787,\"start\":102786},{\"end\":102795,\"start\":102794},{\"end\":102802,\"start\":102801},{\"end\":102810,\"start\":102809},{\"end\":102823,\"start\":102822},{\"end\":102825,\"start\":102824},{\"end\":102842,\"start\":102841},{\"end\":102844,\"start\":102843},{\"end\":103564,\"start\":103563},{\"end\":103571,\"start\":103570},{\"end\":103579,\"start\":103578},{\"end\":103590,\"start\":103589},{\"end\":103600,\"start\":103599},{\"end\":103610,\"start\":103609},{\"end\":103899,\"start\":103898},{\"end\":103908,\"start\":103907},{\"end\":103916,\"start\":103915},{\"end\":103925,\"start\":103924},{\"end\":104363,\"start\":104362},{\"end\":104371,\"start\":104370},{\"end\":104378,\"start\":104377},{\"end\":104385,\"start\":104384},{\"end\":104391,\"start\":104390},{\"end\":104399,\"start\":104398},{\"end\":104406,\"start\":104405},{\"end\":104986,\"start\":104985},{\"end\":104988,\"start\":104987},{\"end\":105002,\"start\":105001},{\"end\":105011,\"start\":105010},{\"end\":105021,\"start\":105020},{\"end\":105023,\"start\":105022},{\"end\":105485,\"start\":105484},{\"end\":105489,\"start\":105486},{\"end\":105498,\"start\":105497},{\"end\":105500,\"start\":105499},{\"end\":105511,\"start\":105510},{\"end\":105521,\"start\":105520},{\"end\":105523,\"start\":105522},{\"end\":105533,\"start\":105532},{\"end\":105535,\"start\":105534},{\"end\":105856,\"start\":105855},{\"end\":105866,\"start\":105865},{\"end\":105876,\"start\":105875},{\"end\":106346,\"start\":106345},{\"end\":106355,\"start\":106354},{\"end\":106365,\"start\":106364},{\"end\":106372,\"start\":106371},{\"end\":106378,\"start\":106377},{\"end\":106704,\"start\":106703},{\"end\":106711,\"start\":106710},{\"end\":106720,\"start\":106719},{\"end\":106727,\"start\":106726},{\"end\":106737,\"start\":106736},{\"end\":106743,\"start\":106742},{\"end\":107157,\"start\":107156},{\"end\":107498,\"start\":107497},{\"end\":107508,\"start\":107507},{\"end\":107510,\"start\":107509},{\"end\":107518,\"start\":107517},{\"end\":107527,\"start\":107526},{\"end\":107533,\"start\":107532},{\"end\":107797,\"start\":107793},{\"end\":107812,\"start\":107811},{\"end\":107821,\"start\":107820},{\"end\":107831,\"start\":107830},{\"end\":107841,\"start\":107840},{\"end\":108265,\"start\":108264},{\"end\":108276,\"start\":108275},{\"end\":108286,\"start\":108285},{\"end\":108799,\"start\":108798},{\"end\":108809,\"start\":108808},{\"end\":108820,\"start\":108819},{\"end\":108832,\"start\":108831},{\"end\":109216,\"start\":109215},{\"end\":109230,\"start\":109229},{\"end\":109233,\"start\":109231},{\"end\":109242,\"start\":109241},{\"end\":109481,\"start\":109480},{\"end\":109494,\"start\":109493},{\"end\":109510,\"start\":109509},{\"end\":109524,\"start\":109523},{\"end\":109532,\"start\":109531},{\"end\":109539,\"start\":109538},{\"end\":109837,\"start\":109836},{\"end\":109839,\"start\":109838},{\"end\":109847,\"start\":109846},{\"end\":109849,\"start\":109848},{\"end\":110096,\"start\":110095},{\"end\":110107,\"start\":110106},{\"end\":110117,\"start\":110116},{\"end\":110906,\"start\":110905},{\"end\":111014,\"start\":111013},{\"end\":111028,\"start\":111027},{\"end\":111047,\"start\":111046},{\"end\":111058,\"start\":111057},{\"end\":111353,\"start\":111352},{\"end\":111361,\"start\":111360},{\"end\":111368,\"start\":111367},{\"end\":111376,\"start\":111375},{\"end\":111384,\"start\":111383},{\"end\":111391,\"start\":111390},{\"end\":111698,\"start\":111697},{\"end\":111710,\"start\":111709},{\"end\":111720,\"start\":111719},{\"end\":111732,\"start\":111731},{\"end\":111734,\"start\":111733},{\"end\":111743,\"start\":111742},{\"end\":111746,\"start\":111744},{\"end\":111755,\"start\":111754},{\"end\":111766,\"start\":111765},{\"end\":111768,\"start\":111767},{\"end\":111783,\"start\":111779},{\"end\":112884,\"start\":112883},{\"end\":112886,\"start\":112885},{\"end\":112897,\"start\":112896},{\"end\":112899,\"start\":112898},{\"end\":112910,\"start\":112909},{\"end\":112920,\"start\":112919},{\"end\":113253,\"start\":113252},{\"end\":113256,\"start\":113254},{\"end\":113265,\"start\":113264},{\"end\":113276,\"start\":113275},{\"end\":113278,\"start\":113277},{\"end\":113290,\"start\":113289},{\"end\":113304,\"start\":113303},{\"end\":113777,\"start\":113776},{\"end\":113787,\"start\":113786},{\"end\":113801,\"start\":113800},{\"end\":113813,\"start\":113812},{\"end\":114098,\"start\":114097},{\"end\":114108,\"start\":114101},{\"end\":114119,\"start\":114118},{\"end\":114564,\"start\":114563},{\"end\":114575,\"start\":114574},{\"end\":114586,\"start\":114585},{\"end\":114596,\"start\":114595},{\"end\":114609,\"start\":114608},{\"end\":114618,\"start\":114617},{\"end\":114620,\"start\":114619},{\"end\":114629,\"start\":114628},{\"end\":114639,\"start\":114638},{\"end\":114982,\"start\":114981},{\"end\":114994,\"start\":114993},{\"end\":115001,\"start\":115000},{\"end\":115167,\"start\":115166},{\"end\":115178,\"start\":115177},{\"end\":115191,\"start\":115190},{\"end\":115199,\"start\":115198},{\"end\":115210,\"start\":115209},{\"end\":115563,\"start\":115562},{\"end\":115575,\"start\":115574},{\"end\":115585,\"start\":115584},{\"end\":116010,\"start\":116009},{\"end\":116018,\"start\":116017},{\"end\":116025,\"start\":116024},{\"end\":116033,\"start\":116032},{\"end\":116041,\"start\":116040},{\"end\":116049,\"start\":116048},{\"end\":116057,\"start\":116056},{\"end\":116065,\"start\":116064},{\"end\":116072,\"start\":116071},{\"end\":116079,\"start\":116078},{\"end\":116088,\"start\":116087},{\"end\":116449,\"start\":116448},{\"end\":116463,\"start\":116462},{\"end\":116478,\"start\":116477},{\"end\":116933,\"start\":116932},{\"end\":116942,\"start\":116941},{\"end\":116951,\"start\":116950},{\"end\":116953,\"start\":116952},{\"end\":117251,\"start\":117250},{\"end\":117259,\"start\":117258},{\"end\":117265,\"start\":117264},{\"end\":117743,\"start\":117742},{\"end\":117760,\"start\":117759},{\"end\":117769,\"start\":117768},{\"end\":117776,\"start\":117775},{\"end\":117778,\"start\":117777},{\"end\":117786,\"start\":117785},{\"end\":117795,\"start\":117794},{\"end\":117797,\"start\":117796},{\"end\":117805,\"start\":117804},{\"end\":117814,\"start\":117813},{\"end\":117822,\"start\":117821},{\"end\":117824,\"start\":117823},{\"end\":118182,\"start\":118181},{\"end\":118464,\"start\":118463},{\"end\":119090,\"start\":119089},{\"end\":119099,\"start\":119098},{\"end\":119107,\"start\":119106},{\"end\":119365,\"start\":119364},{\"end\":119372,\"start\":119371},{\"end\":119549,\"start\":119548},{\"end\":119557,\"start\":119556},{\"end\":119564,\"start\":119563},{\"end\":119573,\"start\":119572},{\"end\":119582,\"start\":119581},{\"end\":119590,\"start\":119589},{\"end\":119596,\"start\":119595},{\"end\":119604,\"start\":119603},{\"end\":120037,\"start\":120036},{\"end\":120039,\"start\":120038},{\"end\":120047,\"start\":120046},{\"end\":120766,\"start\":120765},{\"end\":120774,\"start\":120773},{\"end\":121010,\"start\":121009},{\"end\":121022,\"start\":121021},{\"end\":121032,\"start\":121031},{\"end\":121236,\"start\":121235},{\"end\":121549,\"start\":121548},{\"end\":121557,\"start\":121556},{\"end\":121567,\"start\":121566},{\"end\":121579,\"start\":121578},{\"end\":122138,\"start\":122137},{\"end\":122154,\"start\":122153},{\"end\":122156,\"start\":122155},{\"end\":122164,\"start\":122163},{\"end\":122170,\"start\":122169},{\"end\":123091,\"start\":123090},{\"end\":123104,\"start\":123100},{\"end\":123110,\"start\":123109},{\"end\":123119,\"start\":123118},{\"end\":123132,\"start\":123131},{\"end\":123567,\"start\":123566},{\"end\":123579,\"start\":123578},{\"end\":123591,\"start\":123590},{\"end\":123593,\"start\":123592},{\"end\":123601,\"start\":123600},{\"end\":123611,\"start\":123610},{\"end\":123920,\"start\":123919},{\"end\":124117,\"start\":124116},{\"end\":124124,\"start\":124123},{\"end\":124132,\"start\":124131},{\"end\":124140,\"start\":124139},{\"end\":124379,\"start\":124378},{\"end\":124387,\"start\":124386},{\"end\":124706,\"start\":124705},{\"end\":124708,\"start\":124707},{\"end\":124722,\"start\":124721},{\"end\":124731,\"start\":124730},{\"end\":124739,\"start\":124738},{\"end\":124752,\"start\":124751},{\"end\":125061,\"start\":125060},{\"end\":125068,\"start\":125067},{\"end\":125075,\"start\":125074},{\"end\":125083,\"start\":125082},{\"end\":125092,\"start\":125091},{\"end\":125605,\"start\":125604},{\"end\":125611,\"start\":125610},{\"end\":125617,\"start\":125616},{\"end\":125928,\"start\":125927},{\"end\":125937,\"start\":125936},{\"end\":125948,\"start\":125947},{\"end\":125960,\"start\":125959},{\"end\":125971,\"start\":125970},{\"end\":125985,\"start\":125984},{\"end\":126314,\"start\":126313},{\"end\":126322,\"start\":126321},{\"end\":126331,\"start\":126330},{\"end\":126339,\"start\":126338},{\"end\":126351,\"start\":126350},{\"end\":126363,\"start\":126362},{\"end\":126370,\"start\":126369},{\"end\":126380,\"start\":126379},{\"end\":126389,\"start\":126388},{\"end\":126397,\"start\":126396},{\"end\":126410,\"start\":126409},{\"end\":126421,\"start\":126420},{\"end\":126433,\"start\":126432},{\"end\":126447,\"start\":126446},{\"end\":126453,\"start\":126452},{\"end\":126464,\"start\":126463},{\"end\":126471,\"start\":126470},{\"end\":126477,\"start\":126476},{\"end\":126479,\"start\":126478},{\"end\":126487,\"start\":126486},{\"end\":126497,\"start\":126496},{\"end\":126506,\"start\":126505},{\"end\":126516,\"start\":126515},{\"end\":126518,\"start\":126517},{\"end\":126961,\"start\":126960},{\"end\":126972,\"start\":126971},{\"end\":126985,\"start\":126984},{\"end\":126994,\"start\":126993},{\"end\":127004,\"start\":127003},{\"end\":127415,\"start\":127414},{\"end\":127425,\"start\":127424},{\"end\":127438,\"start\":127437},{\"end\":127440,\"start\":127439},{\"end\":127809,\"start\":127808},{\"end\":127818,\"start\":127817},{\"end\":127820,\"start\":127819},{\"end\":127828,\"start\":127827},{\"end\":127838,\"start\":127837},{\"end\":127848,\"start\":127847},{\"end\":127862,\"start\":127861},{\"end\":127864,\"start\":127863},{\"end\":128336,\"start\":128335},{\"end\":128349,\"start\":128348},{\"end\":128362,\"start\":128361},{\"end\":128374,\"start\":128373},{\"end\":128384,\"start\":128383},{\"end\":128395,\"start\":128394},{\"end\":128405,\"start\":128404},{\"end\":128416,\"start\":128415},{\"end\":128432,\"start\":128431},{\"end\":128441,\"start\":128440},{\"end\":128452,\"start\":128451},{\"end\":128466,\"start\":128465},{\"end\":128476,\"start\":128475},{\"end\":128490,\"start\":128489},{\"end\":128501,\"start\":128500},{\"end\":128511,\"start\":128510},{\"end\":128985,\"start\":128984},{\"end\":128987,\"start\":128986},{\"end\":128993,\"start\":128992},{\"end\":128999,\"start\":128998},{\"end\":129001,\"start\":129000}]", "bib_author_last_name": "[{\"end\":89646,\"start\":89642},{\"end\":89654,\"start\":89650},{\"end\":89664,\"start\":89658},{\"end\":89673,\"start\":89671},{\"end\":89679,\"start\":89677},{\"end\":89690,\"start\":89683},{\"end\":90052,\"start\":90044},{\"end\":90066,\"start\":90056},{\"end\":90077,\"start\":90070},{\"end\":90086,\"start\":90081},{\"end\":90098,\"start\":90090},{\"end\":90579,\"start\":90576},{\"end\":90591,\"start\":90583},{\"end\":90601,\"start\":90595},{\"end\":90613,\"start\":90608},{\"end\":90622,\"start\":90617},{\"end\":90633,\"start\":90626},{\"end\":91052,\"start\":91049},{\"end\":91060,\"start\":91056},{\"end\":91067,\"start\":91064},{\"end\":91073,\"start\":91071},{\"end\":91081,\"start\":91077},{\"end\":91091,\"start\":91085},{\"end\":91503,\"start\":91497},{\"end\":91517,\"start\":91511},{\"end\":91534,\"start\":91523},{\"end\":91540,\"start\":91536},{\"end\":91935,\"start\":91932},{\"end\":91949,\"start\":91941},{\"end\":92206,\"start\":92200},{\"end\":92217,\"start\":92210},{\"end\":92226,\"start\":92221},{\"end\":92237,\"start\":92230},{\"end\":92246,\"start\":92241},{\"end\":92253,\"start\":92250},{\"end\":92268,\"start\":92257},{\"end\":92584,\"start\":92578},{\"end\":92593,\"start\":92588},{\"end\":92843,\"start\":92837},{\"end\":92855,\"start\":92850},{\"end\":92862,\"start\":92859},{\"end\":92875,\"start\":92866},{\"end\":93071,\"start\":93068},{\"end\":93078,\"start\":93075},{\"end\":93087,\"start\":93082},{\"end\":93093,\"start\":93091},{\"end\":93102,\"start\":93097},{\"end\":93110,\"start\":93106},{\"end\":93118,\"start\":93114},{\"end\":93127,\"start\":93122},{\"end\":93142,\"start\":93131},{\"end\":93154,\"start\":93146},{\"end\":93522,\"start\":93518},{\"end\":93531,\"start\":93526},{\"end\":93542,\"start\":93535},{\"end\":93550,\"start\":93546},{\"end\":93558,\"start\":93554},{\"end\":93570,\"start\":93564},{\"end\":93853,\"start\":93844},{\"end\":93862,\"start\":93857},{\"end\":93873,\"start\":93866},{\"end\":93882,\"start\":93877},{\"end\":94140,\"start\":94134},{\"end\":94149,\"start\":94144},{\"end\":94568,\"start\":94562},{\"end\":94578,\"start\":94572},{\"end\":94589,\"start\":94584},{\"end\":95005,\"start\":95000},{\"end\":95011,\"start\":95009},{\"end\":95020,\"start\":95015},{\"end\":95033,\"start\":95026},{\"end\":95039,\"start\":95037},{\"end\":95048,\"start\":95043},{\"end\":95463,\"start\":95460},{\"end\":95471,\"start\":95467},{\"end\":95483,\"start\":95475},{\"end\":95494,\"start\":95489},{\"end\":95866,\"start\":95860},{\"end\":95881,\"start\":95872},{\"end\":95892,\"start\":95885},{\"end\":95909,\"start\":95896},{\"end\":96416,\"start\":96408},{\"end\":96428,\"start\":96420},{\"end\":96440,\"start\":96432},{\"end\":96452,\"start\":96444},{\"end\":96740,\"start\":96737},{\"end\":96752,\"start\":96744},{\"end\":96763,\"start\":96756},{\"end\":96775,\"start\":96770},{\"end\":96784,\"start\":96779},{\"end\":96794,\"start\":96788},{\"end\":97206,\"start\":97202},{\"end\":97213,\"start\":97210},{\"end\":97222,\"start\":97219},{\"end\":97230,\"start\":97226},{\"end\":97239,\"start\":97234},{\"end\":97661,\"start\":97655},{\"end\":97675,\"start\":97665},{\"end\":97684,\"start\":97679},{\"end\":97694,\"start\":97688},{\"end\":97993,\"start\":97984},{\"end\":98003,\"start\":97997},{\"end\":98016,\"start\":98007},{\"end\":98362,\"start\":98352},{\"end\":98375,\"start\":98366},{\"end\":98736,\"start\":98729},{\"end\":98746,\"start\":98740},{\"end\":98757,\"start\":98750},{\"end\":98771,\"start\":98761},{\"end\":98984,\"start\":98979},{\"end\":98993,\"start\":98988},{\"end\":99002,\"start\":98997},{\"end\":99011,\"start\":99006},{\"end\":99021,\"start\":99017},{\"end\":99035,\"start\":99025},{\"end\":99645,\"start\":99635},{\"end\":99664,\"start\":99649},{\"end\":99678,\"start\":99668},{\"end\":99689,\"start\":99682},{\"end\":100068,\"start\":100058},{\"end\":100078,\"start\":100072},{\"end\":100087,\"start\":100082},{\"end\":100098,\"start\":100093},{\"end\":100109,\"start\":100102},{\"end\":100479,\"start\":100475},{\"end\":100489,\"start\":100483},{\"end\":100503,\"start\":100493},{\"end\":100514,\"start\":100507},{\"end\":100525,\"start\":100518},{\"end\":100537,\"start\":100529},{\"end\":100546,\"start\":100541},{\"end\":101367,\"start\":101357},{\"end\":101377,\"start\":101371},{\"end\":101386,\"start\":101381},{\"end\":101397,\"start\":101390},{\"end\":101409,\"start\":101401},{\"end\":102260,\"start\":102253},{\"end\":102273,\"start\":102264},{\"end\":102284,\"start\":102277},{\"end\":102792,\"start\":102788},{\"end\":102799,\"start\":102796},{\"end\":102807,\"start\":102803},{\"end\":102820,\"start\":102811},{\"end\":102839,\"start\":102826},{\"end\":102847,\"start\":102845},{\"end\":103568,\"start\":103565},{\"end\":103576,\"start\":103572},{\"end\":103587,\"start\":103580},{\"end\":103597,\"start\":103591},{\"end\":103607,\"start\":103601},{\"end\":103618,\"start\":103611},{\"end\":103905,\"start\":103900},{\"end\":103913,\"start\":103909},{\"end\":103922,\"start\":103917},{\"end\":103932,\"start\":103926},{\"end\":104368,\"start\":104364},{\"end\":104375,\"start\":104372},{\"end\":104382,\"start\":104379},{\"end\":104388,\"start\":104386},{\"end\":104396,\"start\":104392},{\"end\":104403,\"start\":104400},{\"end\":104410,\"start\":104407},{\"end\":104999,\"start\":104989},{\"end\":105008,\"start\":105003},{\"end\":105018,\"start\":105012},{\"end\":105036,\"start\":105024},{\"end\":105495,\"start\":105490},{\"end\":105508,\"start\":105501},{\"end\":105518,\"start\":105512},{\"end\":105530,\"start\":105524},{\"end\":105543,\"start\":105536},{\"end\":105863,\"start\":105857},{\"end\":105873,\"start\":105867},{\"end\":105882,\"start\":105877},{\"end\":106352,\"start\":106347},{\"end\":106362,\"start\":106356},{\"end\":106369,\"start\":106366},{\"end\":106375,\"start\":106373},{\"end\":106381,\"start\":106379},{\"end\":106708,\"start\":106705},{\"end\":106717,\"start\":106712},{\"end\":106724,\"start\":106721},{\"end\":106734,\"start\":106728},{\"end\":106740,\"start\":106738},{\"end\":106746,\"start\":106744},{\"end\":107168,\"start\":107158},{\"end\":107505,\"start\":107499},{\"end\":107515,\"start\":107511},{\"end\":107524,\"start\":107519},{\"end\":107530,\"start\":107528},{\"end\":107541,\"start\":107534},{\"end\":107809,\"start\":107798},{\"end\":107818,\"start\":107813},{\"end\":107828,\"start\":107822},{\"end\":107838,\"start\":107832},{\"end\":107847,\"start\":107842},{\"end\":108273,\"start\":108266},{\"end\":108283,\"start\":108277},{\"end\":108297,\"start\":108287},{\"end\":108806,\"start\":108800},{\"end\":108817,\"start\":108810},{\"end\":108829,\"start\":108821},{\"end\":108843,\"start\":108833},{\"end\":109227,\"start\":109217},{\"end\":109239,\"start\":109234},{\"end\":109251,\"start\":109243},{\"end\":109491,\"start\":109482},{\"end\":109507,\"start\":109495},{\"end\":109521,\"start\":109511},{\"end\":109529,\"start\":109525},{\"end\":109536,\"start\":109533},{\"end\":109844,\"start\":109840},{\"end\":109856,\"start\":109850},{\"end\":110104,\"start\":110097},{\"end\":110114,\"start\":110108},{\"end\":110125,\"start\":110118},{\"end\":110911,\"start\":110907},{\"end\":111025,\"start\":111015},{\"end\":111044,\"start\":111029},{\"end\":111055,\"start\":111048},{\"end\":111069,\"start\":111059},{\"end\":111358,\"start\":111354},{\"end\":111365,\"start\":111362},{\"end\":111373,\"start\":111369},{\"end\":111381,\"start\":111377},{\"end\":111388,\"start\":111385},{\"end\":111394,\"start\":111392},{\"end\":111707,\"start\":111699},{\"end\":111717,\"start\":111711},{\"end\":111729,\"start\":111721},{\"end\":111740,\"start\":111735},{\"end\":111752,\"start\":111747},{\"end\":111763,\"start\":111756},{\"end\":111777,\"start\":111769},{\"end\":111791,\"start\":111784},{\"end\":111799,\"start\":111793},{\"end\":112894,\"start\":112887},{\"end\":112907,\"start\":112900},{\"end\":112917,\"start\":112911},{\"end\":112927,\"start\":112921},{\"end\":113262,\"start\":113257},{\"end\":113273,\"start\":113266},{\"end\":113287,\"start\":113279},{\"end\":113301,\"start\":113291},{\"end\":113313,\"start\":113305},{\"end\":113784,\"start\":113778},{\"end\":113798,\"start\":113788},{\"end\":113810,\"start\":113802},{\"end\":113823,\"start\":113814},{\"end\":114116,\"start\":114109},{\"end\":114126,\"start\":114120},{\"end\":114572,\"start\":114565},{\"end\":114583,\"start\":114576},{\"end\":114593,\"start\":114587},{\"end\":114606,\"start\":114597},{\"end\":114615,\"start\":114610},{\"end\":114626,\"start\":114621},{\"end\":114636,\"start\":114630},{\"end\":114650,\"start\":114640},{\"end\":114991,\"start\":114983},{\"end\":114998,\"start\":114995},{\"end\":115008,\"start\":115002},{\"end\":115175,\"start\":115168},{\"end\":115188,\"start\":115179},{\"end\":115196,\"start\":115192},{\"end\":115207,\"start\":115200},{\"end\":115215,\"start\":115211},{\"end\":115572,\"start\":115564},{\"end\":115582,\"start\":115576},{\"end\":115591,\"start\":115586},{\"end\":116015,\"start\":116011},{\"end\":116022,\"start\":116019},{\"end\":116030,\"start\":116026},{\"end\":116038,\"start\":116034},{\"end\":116046,\"start\":116042},{\"end\":116054,\"start\":116050},{\"end\":116062,\"start\":116058},{\"end\":116069,\"start\":116066},{\"end\":116076,\"start\":116073},{\"end\":116085,\"start\":116080},{\"end\":116093,\"start\":116089},{\"end\":116460,\"start\":116450},{\"end\":116475,\"start\":116464},{\"end\":116488,\"start\":116479},{\"end\":116939,\"start\":116934},{\"end\":116948,\"start\":116943},{\"end\":116960,\"start\":116954},{\"end\":117256,\"start\":117252},{\"end\":117262,\"start\":117260},{\"end\":117275,\"start\":117266},{\"end\":117757,\"start\":117744},{\"end\":117766,\"start\":117761},{\"end\":117773,\"start\":117770},{\"end\":117783,\"start\":117779},{\"end\":117792,\"start\":117787},{\"end\":117802,\"start\":117798},{\"end\":117811,\"start\":117806},{\"end\":117819,\"start\":117815},{\"end\":118190,\"start\":118183},{\"end\":118473,\"start\":118465},{\"end\":119096,\"start\":119091},{\"end\":119104,\"start\":119100},{\"end\":119113,\"start\":119108},{\"end\":119369,\"start\":119366},{\"end\":119376,\"start\":119373},{\"end\":119554,\"start\":119550},{\"end\":119561,\"start\":119558},{\"end\":119570,\"start\":119565},{\"end\":119579,\"start\":119574},{\"end\":119587,\"start\":119583},{\"end\":119593,\"start\":119591},{\"end\":119601,\"start\":119597},{\"end\":119608,\"start\":119605},{\"end\":120044,\"start\":120040},{\"end\":120052,\"start\":120048},{\"end\":120771,\"start\":120767},{\"end\":120787,\"start\":120775},{\"end\":121019,\"start\":121011},{\"end\":121029,\"start\":121023},{\"end\":121038,\"start\":121033},{\"end\":121244,\"start\":121237},{\"end\":121554,\"start\":121550},{\"end\":121564,\"start\":121558},{\"end\":121576,\"start\":121568},{\"end\":121590,\"start\":121580},{\"end\":122151,\"start\":122139},{\"end\":122161,\"start\":122157},{\"end\":122167,\"start\":122165},{\"end\":122181,\"start\":122171},{\"end\":123098,\"start\":123092},{\"end\":123107,\"start\":123105},{\"end\":123116,\"start\":123111},{\"end\":123129,\"start\":123120},{\"end\":123145,\"start\":123133},{\"end\":123576,\"start\":123568},{\"end\":123588,\"start\":123580},{\"end\":123598,\"start\":123594},{\"end\":123608,\"start\":123602},{\"end\":123624,\"start\":123612},{\"end\":123926,\"start\":123921},{\"end\":124121,\"start\":124118},{\"end\":124129,\"start\":124125},{\"end\":124137,\"start\":124133},{\"end\":124148,\"start\":124141},{\"end\":124384,\"start\":124380},{\"end\":124393,\"start\":124388},{\"end\":124719,\"start\":124709},{\"end\":124728,\"start\":124723},{\"end\":124736,\"start\":124732},{\"end\":124749,\"start\":124740},{\"end\":124759,\"start\":124753},{\"end\":125065,\"start\":125062},{\"end\":125072,\"start\":125069},{\"end\":125080,\"start\":125076},{\"end\":125089,\"start\":125084},{\"end\":125106,\"start\":125093},{\"end\":125608,\"start\":125606},{\"end\":125614,\"start\":125612},{\"end\":125625,\"start\":125618},{\"end\":125934,\"start\":125929},{\"end\":125945,\"start\":125938},{\"end\":125957,\"start\":125949},{\"end\":125968,\"start\":125961},{\"end\":125982,\"start\":125972},{\"end\":125991,\"start\":125986},{\"end\":126319,\"start\":126315},{\"end\":126328,\"start\":126323},{\"end\":126336,\"start\":126332},{\"end\":126348,\"start\":126340},{\"end\":126360,\"start\":126352},{\"end\":126367,\"start\":126364},{\"end\":126377,\"start\":126371},{\"end\":126386,\"start\":126381},{\"end\":126394,\"start\":126390},{\"end\":126407,\"start\":126398},{\"end\":126418,\"start\":126411},{\"end\":126430,\"start\":126422},{\"end\":126444,\"start\":126434},{\"end\":126450,\"start\":126448},{\"end\":126461,\"start\":126454},{\"end\":126468,\"start\":126465},{\"end\":126474,\"start\":126472},{\"end\":126484,\"start\":126480},{\"end\":126494,\"start\":126488},{\"end\":126503,\"start\":126498},{\"end\":126513,\"start\":126507},{\"end\":126523,\"start\":126519},{\"end\":126969,\"start\":126962},{\"end\":126982,\"start\":126973},{\"end\":126991,\"start\":126986},{\"end\":127001,\"start\":126995},{\"end\":127010,\"start\":127005},{\"end\":127422,\"start\":127416},{\"end\":127435,\"start\":127426},{\"end\":127447,\"start\":127441},{\"end\":127815,\"start\":127810},{\"end\":127825,\"start\":127821},{\"end\":127835,\"start\":127829},{\"end\":127845,\"start\":127839},{\"end\":127859,\"start\":127849},{\"end\":127871,\"start\":127865},{\"end\":128346,\"start\":128337},{\"end\":128359,\"start\":128350},{\"end\":128371,\"start\":128363},{\"end\":128381,\"start\":128375},{\"end\":128392,\"start\":128385},{\"end\":128402,\"start\":128396},{\"end\":128413,\"start\":128406},{\"end\":128429,\"start\":128417},{\"end\":128438,\"start\":128433},{\"end\":128449,\"start\":128442},{\"end\":128463,\"start\":128453},{\"end\":128473,\"start\":128467},{\"end\":128487,\"start\":128477},{\"end\":128498,\"start\":128491},{\"end\":128508,\"start\":128502},{\"end\":128521,\"start\":128512},{\"end\":128990,\"start\":128988},{\"end\":128996,\"start\":128994},{\"end\":129007,\"start\":129002}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":57246310},\"end\":89970,\"start\":89587},{\"attributes\":{\"doi\":\"10.1109/SANER.2018.8330265\",\"id\":\"b1\",\"matched_paper_id\":4657352},\"end\":90508,\"start\":89972},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":49664545},\"end\":90946,\"start\":90510},{\"attributes\":{\"id\":\"b3\"},\"end\":91420,\"start\":90948},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":219409706},\"end\":91887,\"start\":91422},{\"attributes\":{\"id\":\"b5\"},\"end\":92194,\"start\":91889},{\"attributes\":{\"id\":\"b6\"},\"end\":92512,\"start\":92196},{\"attributes\":{\"id\":\"b7\"},\"end\":92751,\"start\":92514},{\"attributes\":{\"id\":\"b8\"},\"end\":93064,\"start\":92753},{\"attributes\":{\"id\":\"b9\"},\"end\":93427,\"start\":93066},{\"attributes\":{\"id\":\"b10\"},\"end\":93779,\"start\":93429},{\"attributes\":{\"id\":\"b11\"},\"end\":94048,\"start\":93781},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":174804114},\"end\":94518,\"start\":94050},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":209862846},\"end\":94903,\"start\":94520},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":226267095},\"end\":95396,\"start\":94905},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":222297517},\"end\":95771,\"start\":95398},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":226265659},\"end\":96349,\"start\":95773},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":19708400},\"end\":96672,\"start\":96351},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53284826},\"end\":97117,\"start\":96674},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3078225},\"end\":97603,\"start\":97119},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14501274},\"end\":97888,\"start\":97605},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10396446},\"end\":98284,\"start\":97890},{\"attributes\":{\"id\":\"b22\"},\"end\":98488,\"start\":98286},{\"attributes\":{\"id\":\"b23\"},\"end\":98676,\"start\":98490},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14200259},\"end\":98912,\"start\":98678},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":207780208},\"end\":99574,\"start\":98914},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1489233},\"end\":99990,\"start\":99576},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":57763210},\"end\":100420,\"start\":99992},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":193927},\"end\":101298,\"start\":100422},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":397662},\"end\":102152,\"start\":101300},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206990340},\"end\":102710,\"start\":102154},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":195069387},\"end\":103483,\"start\":102712},{\"attributes\":{\"id\":\"b32\"},\"end\":103820,\"start\":103485},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8993699},\"end\":104268,\"start\":103822},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":231638713},\"end\":104868,\"start\":104270},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206524309},\"end\":105434,\"start\":104870},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9524394},\"end\":105811,\"start\":105436},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":86694144},\"end\":106259,\"start\":105813},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":207144767},\"end\":106621,\"start\":106261},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":49294647},\"end\":107099,\"start\":106623},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":215808484},\"end\":107463,\"start\":107101},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2846066},\"end\":107724,\"start\":107465},{\"attributes\":{\"id\":\"b42\"},\"end\":107996,\"start\":107726},{\"attributes\":{\"id\":\"b43\"},\"end\":108170,\"start\":107998},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":14358688},\"end\":108660,\"start\":108172},{\"attributes\":{\"id\":\"b45\"},\"end\":108757,\"start\":108662},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":208209348},\"end\":109140,\"start\":108759},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":16034273},\"end\":109478,\"start\":109142},{\"attributes\":{\"id\":\"b48\"},\"end\":109794,\"start\":109480},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":650129},\"end\":110052,\"start\":109796},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":7200084},\"end\":110865,\"start\":110054},{\"attributes\":{\"id\":\"b51\"},\"end\":110967,\"start\":110867},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":51970521},\"end\":111302,\"start\":110969},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":209335843},\"end\":111624,\"start\":111304},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":221856721},\"end\":112799,\"start\":111626},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":16222152},\"end\":113174,\"start\":112801},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":4656410},\"end\":113701,\"start\":113176},{\"attributes\":{\"id\":\"b57\"},\"end\":114025,\"start\":113703},{\"attributes\":{\"id\":\"b58\"},\"end\":114534,\"start\":114027},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":13756489},\"end\":114908,\"start\":114536},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":11212020},\"end\":115164,\"start\":114910},{\"attributes\":{\"id\":\"b61\"},\"end\":115499,\"start\":115166},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":1114678},\"end\":116007,\"start\":115501},{\"attributes\":{\"id\":\"b63\"},\"end\":116393,\"start\":116009},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":50775826},\"end\":116842,\"start\":116395},{\"attributes\":{\"id\":\"b65\"},\"end\":117160,\"start\":116844},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":207853043},\"end\":117609,\"start\":117162},{\"attributes\":{\"id\":\"b67\"},\"end\":118111,\"start\":117611},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":30807376},\"end\":118393,\"start\":118113},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":1487550},\"end\":119023,\"start\":118395},{\"attributes\":{\"id\":\"b70\"},\"end\":119268,\"start\":119025},{\"attributes\":{\"id\":\"b71\"},\"end\":119546,\"start\":119270},{\"attributes\":{\"id\":\"b72\"},\"end\":119865,\"start\":119548},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":3257353},\"end\":120701,\"start\":119867},{\"attributes\":{\"id\":\"b74\"},\"end\":120940,\"start\":120703},{\"attributes\":{\"id\":\"b75\"},\"end\":121198,\"start\":120942},{\"attributes\":{\"id\":\"b76\"},\"end\":121446,\"start\":121200},{\"attributes\":{\"id\":\"b77\"},\"end\":122079,\"start\":121448},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":218673683},\"end\":123017,\"start\":122081},{\"attributes\":{\"id\":\"b79\"},\"end\":123486,\"start\":123019},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":11493856},\"end\":123881,\"start\":123488},{\"attributes\":{\"id\":\"b81\"},\"end\":124064,\"start\":123883},{\"attributes\":{\"id\":\"b82\"},\"end\":124314,\"start\":124066},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":202784970},\"end\":124614,\"start\":124316},{\"attributes\":{\"id\":\"b84\"},\"end\":124971,\"start\":124616},{\"attributes\":{\"id\":\"b85\"},\"end\":125266,\"start\":124973},{\"attributes\":{\"id\":\"b86\"},\"end\":125526,\"start\":125268},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":208006436},\"end\":125828,\"start\":125528},{\"attributes\":{\"id\":\"b88\"},\"end\":126238,\"start\":125830},{\"attributes\":{\"id\":\"b89\"},\"end\":126899,\"start\":126240},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":206593880},\"end\":127379,\"start\":126901},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":174802983},\"end\":127651,\"start\":127381},{\"attributes\":{\"id\":\"b92\"},\"end\":128291,\"start\":127653},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":10659969},\"end\":128884,\"start\":128293},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":174812589},\"end\":129212,\"start\":128886}]", "bib_title": "[{\"end\":89638,\"start\":89587},{\"end\":90040,\"start\":89972},{\"end\":90572,\"start\":90510},{\"end\":91045,\"start\":90948},{\"end\":91491,\"start\":91422},{\"end\":91928,\"start\":91889},{\"end\":94130,\"start\":94050},{\"end\":94558,\"start\":94520},{\"end\":94996,\"start\":94905},{\"end\":95456,\"start\":95398},{\"end\":95856,\"start\":95773},{\"end\":96404,\"start\":96351},{\"end\":96733,\"start\":96674},{\"end\":97198,\"start\":97119},{\"end\":97651,\"start\":97605},{\"end\":97976,\"start\":97890},{\"end\":98725,\"start\":98678},{\"end\":98975,\"start\":98914},{\"end\":99631,\"start\":99576},{\"end\":100054,\"start\":99992},{\"end\":100471,\"start\":100422},{\"end\":101353,\"start\":101300},{\"end\":102247,\"start\":102154},{\"end\":102784,\"start\":102712},{\"end\":103896,\"start\":103822},{\"end\":104360,\"start\":104270},{\"end\":104983,\"start\":104870},{\"end\":105482,\"start\":105436},{\"end\":105853,\"start\":105813},{\"end\":106343,\"start\":106261},{\"end\":106701,\"start\":106623},{\"end\":107154,\"start\":107101},{\"end\":107495,\"start\":107465},{\"end\":108262,\"start\":108172},{\"end\":108796,\"start\":108759},{\"end\":109213,\"start\":109142},{\"end\":109834,\"start\":109796},{\"end\":110093,\"start\":110054},{\"end\":111011,\"start\":110969},{\"end\":111350,\"start\":111304},{\"end\":111695,\"start\":111626},{\"end\":112881,\"start\":112801},{\"end\":113250,\"start\":113176},{\"end\":114561,\"start\":114536},{\"end\":114979,\"start\":114910},{\"end\":115560,\"start\":115501},{\"end\":116446,\"start\":116395},{\"end\":117248,\"start\":117162},{\"end\":118179,\"start\":118113},{\"end\":118461,\"start\":118395},{\"end\":120034,\"start\":119867},{\"end\":122135,\"start\":122081},{\"end\":123564,\"start\":123488},{\"end\":124376,\"start\":124316},{\"end\":125602,\"start\":125528},{\"end\":126958,\"start\":126901},{\"end\":127412,\"start\":127381},{\"end\":128333,\"start\":128293},{\"end\":128982,\"start\":128886}]", "bib_author": "[{\"end\":89648,\"start\":89640},{\"end\":89656,\"start\":89648},{\"end\":89666,\"start\":89656},{\"end\":89675,\"start\":89666},{\"end\":89681,\"start\":89675},{\"end\":89692,\"start\":89681},{\"end\":90054,\"start\":90042},{\"end\":90068,\"start\":90054},{\"end\":90079,\"start\":90068},{\"end\":90088,\"start\":90079},{\"end\":90100,\"start\":90088},{\"end\":90581,\"start\":90574},{\"end\":90593,\"start\":90581},{\"end\":90603,\"start\":90593},{\"end\":90615,\"start\":90603},{\"end\":90624,\"start\":90615},{\"end\":90635,\"start\":90624},{\"end\":91054,\"start\":91047},{\"end\":91062,\"start\":91054},{\"end\":91069,\"start\":91062},{\"end\":91075,\"start\":91069},{\"end\":91083,\"start\":91075},{\"end\":91093,\"start\":91083},{\"end\":91505,\"start\":91493},{\"end\":91519,\"start\":91505},{\"end\":91536,\"start\":91519},{\"end\":91542,\"start\":91536},{\"end\":91937,\"start\":91930},{\"end\":91951,\"start\":91937},{\"end\":92208,\"start\":92196},{\"end\":92219,\"start\":92208},{\"end\":92228,\"start\":92219},{\"end\":92239,\"start\":92228},{\"end\":92248,\"start\":92239},{\"end\":92255,\"start\":92248},{\"end\":92270,\"start\":92255},{\"end\":92586,\"start\":92576},{\"end\":92595,\"start\":92586},{\"end\":92845,\"start\":92835},{\"end\":92857,\"start\":92845},{\"end\":92864,\"start\":92857},{\"end\":92877,\"start\":92864},{\"end\":93073,\"start\":93066},{\"end\":93080,\"start\":93073},{\"end\":93089,\"start\":93080},{\"end\":93095,\"start\":93089},{\"end\":93104,\"start\":93095},{\"end\":93112,\"start\":93104},{\"end\":93120,\"start\":93112},{\"end\":93129,\"start\":93120},{\"end\":93144,\"start\":93129},{\"end\":93156,\"start\":93144},{\"end\":93524,\"start\":93516},{\"end\":93533,\"start\":93524},{\"end\":93544,\"start\":93533},{\"end\":93552,\"start\":93544},{\"end\":93560,\"start\":93552},{\"end\":93572,\"start\":93560},{\"end\":93855,\"start\":93842},{\"end\":93864,\"start\":93855},{\"end\":93875,\"start\":93864},{\"end\":93884,\"start\":93875},{\"end\":94142,\"start\":94132},{\"end\":94151,\"start\":94142},{\"end\":94570,\"start\":94560},{\"end\":94580,\"start\":94570},{\"end\":94591,\"start\":94580},{\"end\":95007,\"start\":94998},{\"end\":95013,\"start\":95007},{\"end\":95022,\"start\":95013},{\"end\":95035,\"start\":95022},{\"end\":95041,\"start\":95035},{\"end\":95050,\"start\":95041},{\"end\":95465,\"start\":95458},{\"end\":95473,\"start\":95465},{\"end\":95485,\"start\":95473},{\"end\":95496,\"start\":95485},{\"end\":95868,\"start\":95858},{\"end\":95883,\"start\":95868},{\"end\":95894,\"start\":95883},{\"end\":95911,\"start\":95894},{\"end\":96418,\"start\":96406},{\"end\":96430,\"start\":96418},{\"end\":96442,\"start\":96430},{\"end\":96454,\"start\":96442},{\"end\":96742,\"start\":96735},{\"end\":96754,\"start\":96742},{\"end\":96765,\"start\":96754},{\"end\":96777,\"start\":96765},{\"end\":96786,\"start\":96777},{\"end\":96796,\"start\":96786},{\"end\":97208,\"start\":97200},{\"end\":97215,\"start\":97208},{\"end\":97224,\"start\":97215},{\"end\":97232,\"start\":97224},{\"end\":97241,\"start\":97232},{\"end\":97663,\"start\":97653},{\"end\":97677,\"start\":97663},{\"end\":97686,\"start\":97677},{\"end\":97696,\"start\":97686},{\"end\":97995,\"start\":97978},{\"end\":98005,\"start\":97995},{\"end\":98018,\"start\":98005},{\"end\":98364,\"start\":98350},{\"end\":98377,\"start\":98364},{\"end\":98738,\"start\":98727},{\"end\":98748,\"start\":98738},{\"end\":98759,\"start\":98748},{\"end\":98773,\"start\":98759},{\"end\":98986,\"start\":98977},{\"end\":98995,\"start\":98986},{\"end\":99004,\"start\":98995},{\"end\":99013,\"start\":99004},{\"end\":99023,\"start\":99013},{\"end\":99037,\"start\":99023},{\"end\":99647,\"start\":99633},{\"end\":99666,\"start\":99647},{\"end\":99680,\"start\":99666},{\"end\":99691,\"start\":99680},{\"end\":100070,\"start\":100056},{\"end\":100080,\"start\":100070},{\"end\":100089,\"start\":100080},{\"end\":100100,\"start\":100089},{\"end\":100111,\"start\":100100},{\"end\":100481,\"start\":100473},{\"end\":100491,\"start\":100481},{\"end\":100505,\"start\":100491},{\"end\":100516,\"start\":100505},{\"end\":100527,\"start\":100516},{\"end\":100539,\"start\":100527},{\"end\":100548,\"start\":100539},{\"end\":101369,\"start\":101355},{\"end\":101379,\"start\":101369},{\"end\":101388,\"start\":101379},{\"end\":101399,\"start\":101388},{\"end\":101411,\"start\":101399},{\"end\":102262,\"start\":102249},{\"end\":102275,\"start\":102262},{\"end\":102286,\"start\":102275},{\"end\":102794,\"start\":102786},{\"end\":102801,\"start\":102794},{\"end\":102809,\"start\":102801},{\"end\":102822,\"start\":102809},{\"end\":102841,\"start\":102822},{\"end\":102849,\"start\":102841},{\"end\":103570,\"start\":103563},{\"end\":103578,\"start\":103570},{\"end\":103589,\"start\":103578},{\"end\":103599,\"start\":103589},{\"end\":103609,\"start\":103599},{\"end\":103620,\"start\":103609},{\"end\":103907,\"start\":103898},{\"end\":103915,\"start\":103907},{\"end\":103924,\"start\":103915},{\"end\":103934,\"start\":103924},{\"end\":104370,\"start\":104362},{\"end\":104377,\"start\":104370},{\"end\":104384,\"start\":104377},{\"end\":104390,\"start\":104384},{\"end\":104398,\"start\":104390},{\"end\":104405,\"start\":104398},{\"end\":104412,\"start\":104405},{\"end\":105001,\"start\":104985},{\"end\":105010,\"start\":105001},{\"end\":105020,\"start\":105010},{\"end\":105038,\"start\":105020},{\"end\":105497,\"start\":105484},{\"end\":105510,\"start\":105497},{\"end\":105520,\"start\":105510},{\"end\":105532,\"start\":105520},{\"end\":105545,\"start\":105532},{\"end\":105865,\"start\":105855},{\"end\":105875,\"start\":105865},{\"end\":105884,\"start\":105875},{\"end\":106354,\"start\":106345},{\"end\":106364,\"start\":106354},{\"end\":106371,\"start\":106364},{\"end\":106377,\"start\":106371},{\"end\":106383,\"start\":106377},{\"end\":106710,\"start\":106703},{\"end\":106719,\"start\":106710},{\"end\":106726,\"start\":106719},{\"end\":106736,\"start\":106726},{\"end\":106742,\"start\":106736},{\"end\":106748,\"start\":106742},{\"end\":107170,\"start\":107156},{\"end\":107507,\"start\":107497},{\"end\":107517,\"start\":107507},{\"end\":107526,\"start\":107517},{\"end\":107532,\"start\":107526},{\"end\":107543,\"start\":107532},{\"end\":107811,\"start\":107793},{\"end\":107820,\"start\":107811},{\"end\":107830,\"start\":107820},{\"end\":107840,\"start\":107830},{\"end\":107849,\"start\":107840},{\"end\":108275,\"start\":108264},{\"end\":108285,\"start\":108275},{\"end\":108299,\"start\":108285},{\"end\":108808,\"start\":108798},{\"end\":108819,\"start\":108808},{\"end\":108831,\"start\":108819},{\"end\":108845,\"start\":108831},{\"end\":109229,\"start\":109215},{\"end\":109241,\"start\":109229},{\"end\":109253,\"start\":109241},{\"end\":109493,\"start\":109480},{\"end\":109509,\"start\":109493},{\"end\":109523,\"start\":109509},{\"end\":109531,\"start\":109523},{\"end\":109538,\"start\":109531},{\"end\":109542,\"start\":109538},{\"end\":109846,\"start\":109836},{\"end\":109858,\"start\":109846},{\"end\":110106,\"start\":110095},{\"end\":110116,\"start\":110106},{\"end\":110127,\"start\":110116},{\"end\":110913,\"start\":110905},{\"end\":111027,\"start\":111013},{\"end\":111046,\"start\":111027},{\"end\":111057,\"start\":111046},{\"end\":111071,\"start\":111057},{\"end\":111360,\"start\":111352},{\"end\":111367,\"start\":111360},{\"end\":111375,\"start\":111367},{\"end\":111383,\"start\":111375},{\"end\":111390,\"start\":111383},{\"end\":111396,\"start\":111390},{\"end\":111709,\"start\":111697},{\"end\":111719,\"start\":111709},{\"end\":111731,\"start\":111719},{\"end\":111742,\"start\":111731},{\"end\":111754,\"start\":111742},{\"end\":111765,\"start\":111754},{\"end\":111779,\"start\":111765},{\"end\":111793,\"start\":111779},{\"end\":111801,\"start\":111793},{\"end\":112896,\"start\":112883},{\"end\":112909,\"start\":112896},{\"end\":112919,\"start\":112909},{\"end\":112929,\"start\":112919},{\"end\":113264,\"start\":113252},{\"end\":113275,\"start\":113264},{\"end\":113289,\"start\":113275},{\"end\":113303,\"start\":113289},{\"end\":113315,\"start\":113303},{\"end\":113786,\"start\":113776},{\"end\":113800,\"start\":113786},{\"end\":113812,\"start\":113800},{\"end\":113825,\"start\":113812},{\"end\":114101,\"start\":114097},{\"end\":114118,\"start\":114101},{\"end\":114128,\"start\":114118},{\"end\":114574,\"start\":114563},{\"end\":114585,\"start\":114574},{\"end\":114595,\"start\":114585},{\"end\":114608,\"start\":114595},{\"end\":114617,\"start\":114608},{\"end\":114628,\"start\":114617},{\"end\":114638,\"start\":114628},{\"end\":114652,\"start\":114638},{\"end\":114993,\"start\":114981},{\"end\":115000,\"start\":114993},{\"end\":115010,\"start\":115000},{\"end\":115177,\"start\":115166},{\"end\":115190,\"start\":115177},{\"end\":115198,\"start\":115190},{\"end\":115209,\"start\":115198},{\"end\":115217,\"start\":115209},{\"end\":115574,\"start\":115562},{\"end\":115584,\"start\":115574},{\"end\":115593,\"start\":115584},{\"end\":116017,\"start\":116009},{\"end\":116024,\"start\":116017},{\"end\":116032,\"start\":116024},{\"end\":116040,\"start\":116032},{\"end\":116048,\"start\":116040},{\"end\":116056,\"start\":116048},{\"end\":116064,\"start\":116056},{\"end\":116071,\"start\":116064},{\"end\":116078,\"start\":116071},{\"end\":116087,\"start\":116078},{\"end\":116095,\"start\":116087},{\"end\":116462,\"start\":116448},{\"end\":116477,\"start\":116462},{\"end\":116490,\"start\":116477},{\"end\":116941,\"start\":116932},{\"end\":116950,\"start\":116941},{\"end\":116962,\"start\":116950},{\"end\":117258,\"start\":117250},{\"end\":117264,\"start\":117258},{\"end\":117277,\"start\":117264},{\"end\":117759,\"start\":117742},{\"end\":117768,\"start\":117759},{\"end\":117775,\"start\":117768},{\"end\":117785,\"start\":117775},{\"end\":117794,\"start\":117785},{\"end\":117804,\"start\":117794},{\"end\":117813,\"start\":117804},{\"end\":117821,\"start\":117813},{\"end\":117827,\"start\":117821},{\"end\":118192,\"start\":118181},{\"end\":118475,\"start\":118463},{\"end\":119098,\"start\":119089},{\"end\":119106,\"start\":119098},{\"end\":119115,\"start\":119106},{\"end\":119371,\"start\":119364},{\"end\":119378,\"start\":119371},{\"end\":119556,\"start\":119548},{\"end\":119563,\"start\":119556},{\"end\":119572,\"start\":119563},{\"end\":119581,\"start\":119572},{\"end\":119589,\"start\":119581},{\"end\":119595,\"start\":119589},{\"end\":119603,\"start\":119595},{\"end\":119610,\"start\":119603},{\"end\":120046,\"start\":120036},{\"end\":120054,\"start\":120046},{\"end\":120773,\"start\":120765},{\"end\":120789,\"start\":120773},{\"end\":121021,\"start\":121009},{\"end\":121031,\"start\":121021},{\"end\":121040,\"start\":121031},{\"end\":121246,\"start\":121235},{\"end\":121556,\"start\":121548},{\"end\":121566,\"start\":121556},{\"end\":121578,\"start\":121566},{\"end\":121592,\"start\":121578},{\"end\":122153,\"start\":122137},{\"end\":122163,\"start\":122153},{\"end\":122169,\"start\":122163},{\"end\":122183,\"start\":122169},{\"end\":123100,\"start\":123090},{\"end\":123109,\"start\":123100},{\"end\":123118,\"start\":123109},{\"end\":123131,\"start\":123118},{\"end\":123147,\"start\":123131},{\"end\":123578,\"start\":123566},{\"end\":123590,\"start\":123578},{\"end\":123600,\"start\":123590},{\"end\":123610,\"start\":123600},{\"end\":123626,\"start\":123610},{\"end\":123928,\"start\":123919},{\"end\":124123,\"start\":124116},{\"end\":124131,\"start\":124123},{\"end\":124139,\"start\":124131},{\"end\":124150,\"start\":124139},{\"end\":124386,\"start\":124378},{\"end\":124395,\"start\":124386},{\"end\":124721,\"start\":124705},{\"end\":124730,\"start\":124721},{\"end\":124738,\"start\":124730},{\"end\":124751,\"start\":124738},{\"end\":124761,\"start\":124751},{\"end\":125067,\"start\":125060},{\"end\":125074,\"start\":125067},{\"end\":125082,\"start\":125074},{\"end\":125091,\"start\":125082},{\"end\":125108,\"start\":125091},{\"end\":125610,\"start\":125604},{\"end\":125616,\"start\":125610},{\"end\":125627,\"start\":125616},{\"end\":125936,\"start\":125927},{\"end\":125947,\"start\":125936},{\"end\":125959,\"start\":125947},{\"end\":125970,\"start\":125959},{\"end\":125984,\"start\":125970},{\"end\":125993,\"start\":125984},{\"end\":126321,\"start\":126313},{\"end\":126330,\"start\":126321},{\"end\":126338,\"start\":126330},{\"end\":126350,\"start\":126338},{\"end\":126362,\"start\":126350},{\"end\":126369,\"start\":126362},{\"end\":126379,\"start\":126369},{\"end\":126388,\"start\":126379},{\"end\":126396,\"start\":126388},{\"end\":126409,\"start\":126396},{\"end\":126420,\"start\":126409},{\"end\":126432,\"start\":126420},{\"end\":126446,\"start\":126432},{\"end\":126452,\"start\":126446},{\"end\":126463,\"start\":126452},{\"end\":126470,\"start\":126463},{\"end\":126476,\"start\":126470},{\"end\":126486,\"start\":126476},{\"end\":126496,\"start\":126486},{\"end\":126505,\"start\":126496},{\"end\":126515,\"start\":126505},{\"end\":126525,\"start\":126515},{\"end\":126971,\"start\":126960},{\"end\":126984,\"start\":126971},{\"end\":126993,\"start\":126984},{\"end\":127003,\"start\":126993},{\"end\":127012,\"start\":127003},{\"end\":127424,\"start\":127414},{\"end\":127437,\"start\":127424},{\"end\":127449,\"start\":127437},{\"end\":127817,\"start\":127808},{\"end\":127827,\"start\":127817},{\"end\":127837,\"start\":127827},{\"end\":127847,\"start\":127837},{\"end\":127861,\"start\":127847},{\"end\":127873,\"start\":127861},{\"end\":128348,\"start\":128335},{\"end\":128361,\"start\":128348},{\"end\":128373,\"start\":128361},{\"end\":128383,\"start\":128373},{\"end\":128394,\"start\":128383},{\"end\":128404,\"start\":128394},{\"end\":128415,\"start\":128404},{\"end\":128431,\"start\":128415},{\"end\":128440,\"start\":128431},{\"end\":128451,\"start\":128440},{\"end\":128465,\"start\":128451},{\"end\":128475,\"start\":128465},{\"end\":128489,\"start\":128475},{\"end\":128500,\"start\":128489},{\"end\":128510,\"start\":128500},{\"end\":128523,\"start\":128510},{\"end\":128992,\"start\":128984},{\"end\":128998,\"start\":128992},{\"end\":129009,\"start\":128998}]", "bib_venue": "[{\"end\":97372,\"start\":97315},{\"end\":99186,\"start\":99182},{\"end\":100814,\"start\":100722},{\"end\":101693,\"start\":101608},{\"end\":110425,\"start\":110327},{\"end\":112250,\"start\":112073},{\"end\":115754,\"start\":115682},{\"end\":116637,\"start\":116572},{\"end\":117386,\"start\":117340},{\"end\":118627,\"start\":118598},{\"end\":120249,\"start\":120162},{\"end\":122527,\"start\":122383},{\"end\":123658,\"start\":123654},{\"end\":127153,\"start\":127091},{\"end\":89755,\"start\":89692},{\"end\":90213,\"start\":90158},{\"end\":90709,\"start\":90635},{\"end\":91157,\"start\":91093},{\"end\":91625,\"start\":91542},{\"end\":92017,\"start\":91951},{\"end\":92326,\"start\":92286},{\"end\":92574,\"start\":92514},{\"end\":92833,\"start\":92753},{\"end\":93227,\"start\":93172},{\"end\":93514,\"start\":93429},{\"end\":93840,\"start\":93781},{\"end\":94262,\"start\":94151},{\"end\":94688,\"start\":94591},{\"end\":95130,\"start\":95050},{\"end\":95560,\"start\":95496},{\"end\":96056,\"start\":95976},{\"end\":96484,\"start\":96454},{\"end\":96876,\"start\":96796},{\"end\":97313,\"start\":97241},{\"end\":97720,\"start\":97696},{\"end\":98059,\"start\":98018},{\"end\":98348,\"start\":98286},{\"end\":98566,\"start\":98490},{\"end\":98780,\"start\":98773},{\"end\":99093,\"start\":99060},{\"end\":99762,\"start\":99691},{\"end\":100176,\"start\":100135},{\"end\":100720,\"start\":100626},{\"end\":101606,\"start\":101519},{\"end\":102337,\"start\":102314},{\"end\":102898,\"start\":102849},{\"end\":103561,\"start\":103485},{\"end\":104033,\"start\":103934},{\"end\":104532,\"start\":104501},{\"end\":105128,\"start\":105059},{\"end\":105605,\"start\":105545},{\"end\":105940,\"start\":105909},{\"end\":106413,\"start\":106383},{\"end\":106843,\"start\":106748},{\"end\":107268,\"start\":107170},{\"end\":107568,\"start\":107543},{\"end\":107791,\"start\":107726},{\"end\":108067,\"start\":107998},{\"end\":108385,\"start\":108361},{\"end\":108932,\"start\":108845},{\"end\":109283,\"start\":109253},{\"end\":109615,\"start\":109558},{\"end\":109899,\"start\":109858},{\"end\":110325,\"start\":110238},{\"end\":110903,\"start\":110867},{\"end\":111113,\"start\":111071},{\"end\":111434,\"start\":111423},{\"end\":112071,\"start\":111879},{\"end\":112959,\"start\":112929},{\"end\":113415,\"start\":113315},{\"end\":113774,\"start\":113703},{\"end\":114095,\"start\":114027},{\"end\":114701,\"start\":114652},{\"end\":115022,\"start\":115010},{\"end\":115307,\"start\":115232},{\"end\":115680,\"start\":115593},{\"end\":116178,\"start\":116111},{\"end\":116570,\"start\":116490},{\"end\":116930,\"start\":116844},{\"end\":117338,\"start\":117277},{\"end\":117740,\"start\":117611},{\"end\":118231,\"start\":118192},{\"end\":118596,\"start\":118528},{\"end\":119087,\"start\":119025},{\"end\":119362,\"start\":119270},{\"end\":119686,\"start\":119626},{\"end\":120160,\"start\":120074},{\"end\":120763,\"start\":120703},{\"end\":121007,\"start\":120942},{\"end\":121233,\"start\":121200},{\"end\":121546,\"start\":121448},{\"end\":122381,\"start\":122239},{\"end\":123088,\"start\":123019},{\"end\":123652,\"start\":123626},{\"end\":123917,\"start\":123883},{\"end\":124114,\"start\":124066},{\"end\":124444,\"start\":124395},{\"end\":124703,\"start\":124616},{\"end\":125058,\"start\":124973},{\"end\":125399,\"start\":125330},{\"end\":125668,\"start\":125627},{\"end\":125925,\"start\":125830},{\"end\":126311,\"start\":126240},{\"end\":127089,\"start\":127012},{\"end\":127498,\"start\":127449},{\"end\":127806,\"start\":127653},{\"end\":128559,\"start\":128523},{\"end\":129023,\"start\":129009}]"}}}, "year": 2023, "month": 12, "day": 17}