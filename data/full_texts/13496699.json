{"id": 13496699, "updated": "2023-09-27 22:52:53.941", "metadata": {"title": "Fairness Through Awareness", "authors": "[{\"first\":\"Cynthia\",\"last\":\"Dwork\",\"middle\":[]},{\"first\":\"Moritz\",\"last\":\"Hardt\",\"middle\":[]},{\"first\":\"Toniann\",\"last\":\"Pitassi\",\"middle\":[]},{\"first\":\"Omer\",\"last\":\"Reingold\",\"middle\":[]},{\"first\":\"Rich\",\"last\":\"Zemel\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2011, "month": 4, "day": 19}, "abstract": "We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of\"fair affirmative action,\"which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1104.3913", "mag": "2949135123", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/innovations/DworkHPRZ12", "doi": "10.1145/2090236.2090255"}}, "content": {"source": {"pdf_hash": "293af2dc96ffed5435051e0622d6991411690da9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1104.3913v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1104.3913.pdf", "status": "GREEN"}}, "grobid": {"id": "bc3230ed793c38720f560d4e0836bf8067e0c6f9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/293af2dc96ffed5435051e0622d6991411690da9.txt", "contents": "\nFairness Through Awareness\nJanuary 20, 2013 29 Nov 2011\n\nCynthia Dwork dwork@microsoft.com \nMoritz Hardt mhardt@us.ibm.com. \nIBM Research Almaden\nSan JoseCAUSA\n\nDepartment of Computer Science\nDepartment of Computer Science\nUniversity of Toronto\nUniversity of Toronto\n\n\nToniann Pitassi \nOmer Reingold omer.reingold@microsoft.com \nRichard Zemel zemel@cs.toronto.edu. \nFairness Through Awareness\nJanuary 20, 2013 29 Nov 2011* Microsoft Research Silicon Valley, Mountain View, CA, USA. Part of this work has been done while the author visited Microsoft Research Silicon Valley. Part of this work has been done while the author visited Microsoft Research Silicon Valley. \u00a7 Microsoft Research Silicon Valley, Mountain View, CA, USA. Part of this work has been done while the author visited Microsoft Research Silicon Valley.\nWe study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of \"fair affirmative action,\" which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.\n\nIntroduction\n\nIn this work, we study fairness in classification. Nearly all classification tasks face the challenge of achieving utility in classification for some purpose, while at the same time preventing discrimination against protected population subgroups. A motivating example is membership in a racial minority in the context of banking. An article in The Wall Street Journal (8/4/2010) describes the practices of a credit card company and its use of a tracking network to learn detailed demographic information about each visitor to the site, such as approximate income, where she shops, the fact that she rents children's videos, and so on. According to the article, this information is used to \"decide which credit cards to show first-time visitors\" to the web site, raising the concern of steering, namely the (illegal) practice of guiding members of minority groups into less advantageous credit offerings [SA10].\n\nWe provide a normative approach to fairness in classification and a framework for achieving it. Our framework permits us to formulate the question as an optimization problem that can be solved by a linear program. In keeping with the motivation of fairness in online advertising, our approach will permit the entity that needs to classify individuals, which we call the vendor, as much freedom as possible, without knowledge of or trust in this party. This allows the vendor to benefit from investment in data mining and market research in designing its classifier, while our absolute guarantee of fairness frees the vendor from regulatory concerns.\n\nOur approach is centered around the notion of a task-specific similarity metric describing the extent to which pairs of individuals should be regarded as similar for the classification task at hand. 1 The similarity metric expresses ground truth. When ground truth is unavailable, the metric may reflect the \"best\" available approximation as agreed upon by society. Following established tradition [Raw01], the metric is assumed to be public and open to discussion and continual refinement. Indeed, we envision that, typically, the distance metric would be externally imposed, for example, by a regulatory body, or externally proposed, by a civil rights organization.\n\nThe choice of a metric need not determine (or even suggest) a particular classification scheme. There can be many classifiers consistent with a single metric. Which classification scheme is chosen in the end is a matter of the vendor's utility function which we take into account. To give a concrete example, consider a metric that expresses which individuals have similar credit worthiness. One advertiser may wish to target a specific product to individuals with low credit, while another advertiser may seek individuals with good credit.\n\n\nKey Elements of Our Framework\n\nTreating similar individuals similarly. We capture fairness by the principle that any two individuals who are similar with respect to a particular task should be classified similarly. In order to accomplish this individual-based fairness, we assume a distance metric that defines the similarity between the individuals. This is the source of \"awareness\" in the title of this paper. We formalize this guiding principle as a Lipschitz condition on the classifier. In our approach a classifier is a randomized mapping from individuals to outcomes, or equivalently, a mapping from individuals to distributions over outcomes. The Lipschitz condition requires that any two individuals x, y that are at distance d(x, y) \u2208 [0, 1] map to distributions M(x) and M(y), respectively, such that the statistical distance between M(x) and M(y) is at most d(x, y). In other words, the distributions over outcomes observed by x and y are indistinguishable up to their distance d(x, y).\n\nFormulation as an optimization problem. We consider the natural optimization problem of constructing fair (i.e., Lipschitz) classifiers that minimize the expected utility loss of the vendor. We observe that this optimization problem can be expressed as a linear program and hence solved efficiently. Moreover, this linear program and its dual interpretation will be used heavily throughout our work.\n\nConnection between individual fairness and group fairness. Statistical parity is the property that the demographics of those receiving positive (or negative) classifications are identical to the demographics of the population as a whole. Statistical parity speaks to group fairness rather than individual fairness, and appears desirable, as it equalizes outcomes across protected and non-protected groups. However, we demonstrate its inadequacy as a notion of fairness through several examples in which statistical parity is maintained, but from the point of view of an individual, the outcome is blatantly unfair. While statistical parity (or group fairness) is insufficient by itself, we investigate conditions under which our notion of fairness implies statistical parity. In Section 3, we give conditions on the similarity metric, via an Earthmover distance, such that fairness for individuals (the Lipschitz condition) yields group fairness (statistical parity). More precisely, we show that the Lipschitz condition implies statistical parity between two groups if and only if the Earthmover distance between the two groups is small. This characterization is an important tool in understanding the consequences of imposing the Lipschitz condition.\n\nFair affirmative action. In Section 4, we give techniques for forcing statistical parity when it is not implied by the Lipschitz condition (the case of preferential treatment), while preserving as much fairness for individuals as possible. We interpret these results as providing a way of achieving fair affirmative action.\n\nA close relationship to privacy. We observe that our definition of fairness is a generalization of the notion of differential privacy [Dwo06,DMNS06]. We draw an analogy between individuals in the setting of fairness and databases in the setting of differential privacy. In Section 5 we build on this analogy and exploit techniques from differential privacy to develop a more efficient variation of our fairness mechanism. We prove that our solution has small error when the metric space of individuals has small doubling dimension, a natural condition arising in machine learning applications. We also prove a lower bound showing that any mapping satisfying the Lipschitz condition has error that scales with the doubling dimension. Interestingly, these results also demonstrate a quantiative trade-off between fairness and utility. Finally, we touch on the extent to which fairness can hide information from the advertiser in the context of online advertising.\n\nPrevention of certain evils. We remark that our notion of fairness interdicts a catalogue of discriminatory practices including the following, described in Appendix A: redlining; reverse redlining; discrimination based on redundant encodings of membership in the protected set; cutting off business with a segment of the population in which membership in the protected set is disproportionately high; doing business with the \"wrong\" subset of the protected set (possibly in order to prove a point); and \"reverse tokenism.\"\n\n\nDiscussion: The Metric\n\nAs noted above, the metric should (ideally) capture ground truth. Justifying the availability of or access to the distance metric in various settings is one of the most challenging aspects of our framework, and in reality the metric used will most likely only be society's current best approximation to the truth. Of course, metrics are employed, implicitly or explicitly, in many classification settings, such as college admissions procedures, advertising (\"people who buy X and live in zipcode Y are similar to people who live in zipcode Z and buy W\"), and loan applications (credit scores). Our work advocates for making these metrics public.\n\nAn intriguing example of an existing metric designed for the health care setting is part of the AALIM project [AAL], whose goal is to provide a decision support system for cardiology that helps a physician in finding a suitable diagnosis for a patient based on the consensus opinions of other physicians who have looked at similar patients in the past. Thus the system requires an accurate understanding of which patients are similar based on information from multiple domains such as cardiac echo videos, heart sounds, ECGs and physicians' reports. AALIM seeks to ensure that individuals with similar health characteristics receive similar treatments from physicians. This work could serve as a starting point in the fairness setting, although it does not (yet?) provide the distance metric that our approach requires. We discuss this further in Section 6.1.\n\nFinally, we can envision classification situations in which it is desirable to \"adjust\" or otherwise \"make up\" a metric, and use this synthesized metric as a basis for determining which pairs of individuals should be classified similarly. 2 Our machinery is agnostic as to the \"correctness\" of the metric, and so can be employed in these settings as well.\n\n\nRelated Work\n\nThere is a broad literature on fairness, notably in social choice theory, game theory, economics, and law. Among the most relevant are theories of fairness and algorithmic approaches to apportionment; see, for example, the following books: H. Peyton Young's Equity, John Roemer's Equality of Opportunity and Theories of Distributive Justice, as well as John Rawls' A Theory of Justice and Justice as Fairness: A Restatement. Calsamiglia [Cal05] explains, \"Equality of opportunity defines an important welfare criterion in political philosophy and policy analysis. Philosophers define equality of opportunity as the requirement that an individual's well being be independent of his or her irrelevant characteristics. The difference among philosophers is mainly about which characteristics should be considered irrelevant. Policymakers, however, are often called upon to address more specific questions: How should admissions policies be designed so as to provide equal opportunities for college? Or how should tax schemes be designed so as to equalize opportunities for income? These are called local distributive justice problems, because each policymaker is in charge of achieving equality of opportunity to a specific issue.\"\n\nIn general, local solutions do not, taken together, solve the global problem: \"There is no mechanism comparable to the invisible hand of the market for coordinating distributive justice at the micro into just outcomes at the macro level\" [You95], (although Calsamiglia's work treats exactly this problem [Cal05]). Nonetheless, our work is decidedly \"local,\" both in the aforementioned sense and in our definition of fairness. To our knowledge, our approach differs from much of the literature in our fundamental skepticism regarding the vendor; we address this by separating the vendor from the data owner, leaving classification to the latter.\n\nConcerns for \"fairness\" also arise in many contexts in computer science, game theory, and economics. For example, in the distributed computing literature, one meaning of fairness is that a process that attempts infinitely often to make progress eventually makes progress. One quantitative meaning of unfairness in scheduling theory is the maximum, taken over all members of a set of longlived processes, of the difference between the actual load on the process and the so-called desired load (the desired load is a function of the tasks in which the process participates) [AAN + 98]; other notions of fairness appear in [BS06,Fei08,FT11], to name a few. For an example of work incorporating fairness into game theory and economics see the eponymous paper [Rab93].\n\n\nFormulation of the Problem\n\nIn this section we describe our setup in its most basic form. We shall later see generalizations of this basic formulation. Individuals are the objects to be classified; we denote the set of individuals by V. In this paper we consider classifiers that map individuals to outcomes. We denote the set of outcomes by A. In the simplest non-trivial case A = {0, 1} . To ensure fairness, we will consider randomized classifiers mapping individuals to distributions over outcomes. To introduce our notion of fairness we assume the existence of a metric on individuals d : V \u00d7 V \u2192 . We will consider randomized mappings M : V \u2192 \u2206(A) from individuals to probability distributions over outcomes. Such a mapping naturally describes a randomized classification procedure: to classify x \u2208 V choose an outcome a according to the distribution M(x). We interpret the goal of \"mapping similar people similarly\" to mean that the distributions assigned to similar people are similar. Later we will discuss two specific measures of similarity of distributions, D \u221e and D tv , of interest in this work. \n\nWhen D and d are clear from the context we will refer to this simply as the Lipschitz property.\n\nWe note that there always exists a Lipschitz classifier, for example, by mapping all individuals to the same distribution over A. Which classifier we shall choose thus depends on a notion of utility. We capture utility using a loss function L : V \u00d7 A \u2192 . This setup naturally leads to the optimization problem:\n\nFind a mapping from individuals to distributions over outcomes that minimizes expected loss subject to the Lipschitz condition.\n\n\nAchieving Fairness\n\nOur fairness definition leads to an optimization problem in which we minimize an arbitrary loss function L : V \u00d7 A \u2192 while achieving the (D, d)-Lipschitz property for a given metric d : V \u00d7 V \u2192 . We denote by I an instance of our problem consisting of a metric d : V \u00d7 V \u2192 , and a loss function L : V \u00d7A \u2192 . We denote the optimal value of the minimization problem by opt(I), as formally defined in Figure 1. We will also write the mapping M : Figure 1: The Fairness LP: Loss minimization subject to fairness constraint Probability Metrics The first choice for D that may come to mind is the statistical distance: Let P, Q denote probability measures on a finite domain A. The statistical distance or total variation norm between P and Q is denoted by\nV \u2192 \u2206(A) as M = {\u00b5 x } x\u2208V where \u00b5 x = M(x) \u2208 \u2206(A). opt(I) def = min {\u00b5 x } x\u2208V \u00be x\u223cV \u00be a\u223c\u00b5 x L(x, a) (2) subject to \u2200x, y \u2208 V, : D(\u00b5 x , \u00b5 y ) \u2264 d(x, y) (3) \u2200x \u2208 V : \u00b5 x \u2208 \u2206(A)(4)D tv (P, Q) = 1 2 a\u2208A |P(a) \u2212 Q(a)| .(5)\nThe following lemma is easily derived from the definitions of opt(I) and D tv .\n\nLemma 2.1. Let D = D tv . Given an instance I we can compute opt(I) with a linear program of size poly(|V|, |A|).\n\nRemark 2.1. When dealing with the set V, we have assumed that V is the set of real individuals (rather than the potentially huge set of all possible encodings of individuals). More generally, we may only have access to a subsample from the set of interest. In such a case, there is the additional challenge of extrapolating a classifier over the entire set.\n\nA weakness of using D tv as the distance measure on distributions, it that we should then assume that the distance metric (measuring distance between individuals) is scaled such that for similar individuals d(x, y) is very close to zero, while for very dissimilar individuals d(x, y) is close to one. A potentially better choice for D in this respect is sometimes called relative \u221e metric:\nD \u221e (P, Q) = sup a\u2208A log max P(a) Q(a) , Q(a) P(a) .(6)\nWith this choice we think of two individuals x, y as similar if d(x, y) 1. In this case, the Lipschitz condition in Equation 1 ensures that x and y map to similar distributions over A. On the other hand, when x, y are very dissimilar, i.e., d(x, y) 1, the condition imposes only a weak constraint on the two corresponding distributions over outcomes. Proof. We note that the objective function and the first constraint are indeed linear in the variables \u00b5 x (a), as the first constraint boils down to requirements of the form \u00b5 x (a) \u2264 e d(x,y) \u00b5 y (a). The second constraint \u00b5 x \u2208 \u2206(A) can easily be rewritten as a set of linear constraints.\n\nNotation. Recall that we often write the mapping M :\nV \u2192 \u2206(A) as M = {\u00b5 x } x\u2208V where \u00b5 x = M(x) \u2208 \u2206(A).\nIn this case, when S is a distribution over V we denote by \u00b5 S the distribution over A defined as\n\u00b5 S (a) = \u00be x\u223cS \u00b5 x (a) where a \u2208 A .\nUseful Facts It is not hard to check that both D tv and D \u221e are metrics with the following properties.\nLemma 2.3. D tv (P, Q) \u2264 1 \u2212 exp(\u2212D \u221e (P, Q)) \u2264 D \u221e (P, Q)\nFact 2.1. For any three distributions P, Q, R and non-negative numbers \u03b1, \u03b2 \u2265 0 such that \u03b1 + \u03b2 = 1, we have D tv (\u03b1P + \u03b2Q, R) \u2264 \u03b1D tv (P, R) + \u03b2D tv (Q, R).\n\nPost-Processing. An important feature of our definition is that it behaves well with respect to postprocessing.\nSpecifically, if M : V \u2192 \u2206(A) is (D, d)-Lipschitz for D \u2208 {D tv , D \u221e } and f : A \u2192 B is any possibly randomized function from A to another set B, then the composition f \u2022 M : V \u2192 \u2206(B)\nis a (D, d)-Lipschitz mapping. This would in particular be useful in the setting of the example in Section 2.2.\n\n\nExample: Ad network\n\nHere we expand on the example of an advertising network mentioned in the Introduction. We explain how the Fairness LP provides a fair solution protecting against the evils described in Appendix A. The Wall Street Journal article [SA10] describes how the [x+1] tracking network collects demographic information about individuals, such as their browsing history, geographical location, and shopping behavior, and utilizes this to assign a person to one of 66 groups. For example, one of these groups is \"White Picket Fences,\" a market segment with median household income of just over $50,000, aged 25 to 44 with kids, with some college education, etc. Based on this assignment to a group, CapitalOne decides which credit card, with particular terms of credit, to show the individual. In general we view a classification task as involving two distinct parties: the data owner is a trusted party holding the data of individuals, and the vendor is the party that wishes to classify individuals. The loss function may be defined solely by either party or by both parties in collaboration. In this example, the data owner is the ad network [x+1], and the vendor is CapitalOne.\n\nThe ad network ([x+1]) maintains a mapping from individuals into categories. We can think of these categories as outcomes, as they determine which ads will be shown to an individual. In order to comply with our fairness requirement, the mapping from individuals into categories (or outcomes) will have to be randomized and satisfy the Lipschitz property introduced above. Subject to the Lipschitz constraint, the vendor can still express its own belief as to how individuals should be assigned to categories using the loss function. However, since the Lipschitz condition is a hard constraint there is no possibility of discriminating between individuals that are deemed similar by the metric. In particular, this will disallow arbitrary distinctions between protected individuals, thus preventing both reverse tokenism and the self-fulfilling prophecy (see Appendix A). In addition, the metric can eliminate the existence of redundant encodings of certain attributes thus also preventing redlining of those attributes. In Section 3 we will see a characterization of which attributes are protected by the metric in this way.\n\n\nConnection to Differential Privacy\n\nOur notion of fairness may be viewed as a generalization of differential privacy [Dwo06,DMNS06]. As it turns out our notion can be seen as a generalization of differential privacy. To see this, consider a simple setting of differential privacy where a database curator maintains a database x (thought of as a subset of some universe U) and a data analyst is allowed to ask a query F : V \u2192 A on the database. Here we denote the set of databases by V = 2 U and the range of the query by A. A mapping M : V \u2192 \u2206(A) satisfies -differential privacy if and only if M satisfies the (D \u221e , d)-Lipschitz property, where, letting x y denote the symmetric difference between x and y, we define d(x, y) def = |x y|. The utility loss of the analyst for getting an answer a \u2208 A from the mechanism is defined as L(x, a) = d A (F x, a), that is distance of the true answer from the given answer. Here distance refers to some distance measure in A that we described using the notation d A . For example, when A = , this could simply be d A (a, b) = |a \u2212 b|. The optimization problem (2) in Figure 1 (i.e., opt(I) def = min \u00be x\u223cV \u00be a\u223c\u00b5 x L(x, a)) now defines the optimal differentially private mechanism in this setting. We can draw a conceptual analogy between the utility model in differential privacy and that in fairness. If we think of outcomes as representing information about an individual, then the vendor wishes to receive what she believes is the most \"accurate\" representation of an individual. This is quite similar to the goal of the analyst in differential privacy.\n\nIn the current work we deal with more general metric spaces than in differential privacy. Nevertheless, we later see (specifically in Section 5) that some of the techniques used in differential privacy carry over to the fairness setting.\n\n\nRelationship between Lipschitz property and statistical parity\n\nIn this section we discuss the relationship between the Lipschitz property articulated in Definition 2.1 and statistical parity. As we discussed earlier, statistical parity is insufficient as a general notion of fairness. Nevertheless statistical parity can have several desirable features, e.g., as described in Proposition 3.1 below. In this section we demonstrate that the Lipschitz condition naturally implies statistical parity between certain subsets of the population.\n\nFormally, statistical parity is the following property. \nD tv (\u00b5 S , \u00b5 T ) \u2264 .(7)\nProposition 3.1. Let M : V \u2192 \u2206(A) be a mapping that satisfies statistical parity between two sets S and T up to bias . Then, for every set of outcomes O \u2286 A, we have the following two properties.\n1. | r {M(x) \u2208 O | x \u2208 S } \u2212 r {M(x) \u2208 O | x \u2208 T }| \u2264 , 2. | r {x \u2208 S | M(x) \u2208 O} \u2212 r {x \u2208 T | M(x) \u2208 O})| \u2264 .\nIntuitively, this proposition says that if M satisfies statistical parity, then members of S are equally likely to observe a set of outcomes as are members of T. Furthermore, the fact that an individual observed a particular outcome provides no information as to whether the individual is a member of S or a member of T. We can always choose T = S c in which case we compare S to the general population.\n\n\nWhy is statistical parity insufficient?\n\nAlthough in some cases statistical parity appears to be desirable -in particular, it neutralizes redundant encodings -we now argue its inadequacy as a notion of fairness, presenting three examples in which statistical parity is maintained, but from the point of view of an individual, the outcome is blatantly unfair. In describing these examples, we let S denote the protected set and S c its complement.\n\nExample 1: Reduced Utility. Consider the following scenario. Suppose in the culture of S the most talented students are steered toward science and engineering and the less talented are steered toward finance, while in the culture of S c the situation is reversed: the most talented are steered toward finance and those with less talent are steered toward engineering. An organization ignorant of the culture of S and seeking the most talented people may select for \"economics,\" arguably choosing the wrong subset of S , even while maintaining parity. Note that this poor outcome can occur in a \"fairness through blindness\" approach -the errors come from ignoring membership in S .\n\nExample 2: Self-fulfilling prophecy. This is when unqualified members of S are chosen, in order to \"justify\" future discrimination against S (building a case that there is no point in \"wasting\" resources on S ). Although senseless, this is an example of something pernicious that is not ruled out by statistical parity, showing the weakness of this notion. A variant of this apparently occurs in selecting candidates for interviews: the hiring practices of certain firms are audited to ensure sufficiently many interviews of minority candidates, but less care is taken to ensure that the best minorities -those that might actually compete well with the better non-minority candidatesare invited [Zar11].\n\nExample 3: Subset Targeting. Statistical parity for S does not imply statistical parity for subsets of S . This can be maliciously exploited in many ways. For example, consider an advertisement for a product X which is targeted to members of S that are likely to be interested in X and to members of S c that are very unlikely to be interested in X. Clicking on such an ad may be strongly correlated with membership in S (even if exposure to the ad obeys statistical parity).\n\n\nEarthmover distance: Lipschitz versus statistical parity\n\nA fundamental question that arises in our approach is: When does the Lipschitz condition imply statistical parity between two distributions S and T on V? We will see that the answer to this question is closely related to the Earthmover distance between S and T , which we will define shortly. The next definition formally introduces the quantity that we will study, that is, the extent to which any Lipschitz mapping can violate statistical parity. In other words, we answer the question, \"How biased with respect to S and T might the solution of the fairness LP be, in the worst case?\" Definition 3.2 (Bias). We define\nbias D,d (S , T ) def = max \u00b5 S (0) \u2212 \u00b5 T (0) ,(8)\nwhere the maximum is taken over all (D, d)-\nLipschitz mappings M = {\u00b5 x } x\u2208V mapping V into \u2206({0, 1}).\nNote that bias D,d (S , T ) \u2208 [0, 1]. Even though in the definition we restricted ourselves to mappings into distributions over {0, 1}, it turns out that this is without loss of generality, as we show next. Indeed, let A S = {a \u2208 A : \u00b5 S (a) > \u00b5 T (a)} and let A T = A c S . Put \u00b5 x (0) = \u00b5 x (A S ) and \u00b5 x (1) = \u00b5 x (A T ). We claim that M = {\u00b5 x } x\u2208V is a (D, d)-Lipschitz mapping. In both cases D \u2208 {D tv , D \u221e } this follows directly from the definition. On the other hand, it is easy to see that\nD tv (\u00b5 S , \u00b5 T ) = D tv (\u00b5 S , \u00b5 T ) = \u00b5 S (0) \u2212 \u00b5 T (0) \u2264 bias D,d (S , T ) .\nEarthmover Distance. We will presently relate bias D,d (S , T ) for D \u2208 {D tv , D \u221e } to certain Earthmover distances between S and T , which we define next. Definition 3.3 (Earthmover distance). Let \u03c3 : V \u00d7 V \u2192 be a nonnegative distance function. The \u03c3-Earthmover distance between two distributions S and T , denoted \u03c3 EM (S , T ), is defined as the value of the so-called Earthmover LP:\n\u03c3 EM (S , T ) def = min x,y\u2208V h(x, y)\u03c3(x, y) subject to y\u2208V h(x, y) = S (x) y\u2208V h(y, x) = T (x) h(x, y) \u2265 0\nWe will need the following standard lemma, which simplifies the definition of the Earthmover distance in the case where \u03c3 is a metric. \nbias D tv , d (S , T ) \u2264 d EM (S , T ) .(9)\nIf furthermore d(x, y) \u2264 1 for all x, y, then we have\nbias D tv , d (S , T ) \u2265 d EM (S , T ) .(10)\nProof. The proof is by linear programming duality. We can express bias D tv , d (S , T ) as the following linear program:\nbias(S , T ) = max x\u2208V S (x)\u00b5 x (0) \u2212 x\u2208V T (x)\u00b5 x (0) subject to \u00b5 x (0) \u2212 \u00b5 y (0) \u2264 d(x, y) \u00b5 x (0) + \u00b5 x (1) = 1 \u00b5 x (a) \u2265 0\nHere, we used the fact that\nD tv (\u00b5 x , \u00b5 y ) \u2264 d(x, y) \u21d0\u21d2 \u00b5 x (0) \u2212 \u00b5 y (0) \u2264 d(x, y) .\nThe constraint on the RHS is enforced in the linear program above by the two constraints \u00b5 x (0)\u2212\u00b5 y (0) \u2264 d(x, y) and \u00b5 y (0) \u2212 \u00b5 x (0) \u2264 d(x, y).\n\nWe can now prove (9). Since d is a metric, we can apply Lemma 3.2. Let { f (x, y)} x,y\u2208V be a solution to the LP defined in Lemma 3.2. By putting x = 0 for all x \u2208 V, we can extend this to a feasible solution to the LP defining bias(S , T ) achieving the same objective value. Hence, we have bias(S , T ) \u2264 d EM (S , T ).\n\nLet us now prove (10), using the assumption that d(x, y) \u2264 1. To do so, consider dropping the constraint that \u00b5 x (0) + \u00b5 x (1) = 1 and denote by \u03b2(S , T ) the resulting LP:\n\u03b2(S , T ) def = max x\u2208V S (x)\u00b5 x (0) \u2212 x\u2208V T (x)\u00b5 x (0) subject to \u00b5 x (0) \u2212 \u00b5 y (0) \u2264 d(x, y) \u00b5 x (0) \u2265 0\nIt is clear that \u03b2(S , T ) \u2265 bias(S , T ) and we claim that in fact bias(S , T ) \u2265 \u03b2(S , T ). To see this, consider any solution {\u00b5 x (0)} x\u2208V to \u03b2(S , T ). Without changing the objective value we may assume that min x\u2208V \u00b5 x (0) = 0. By our assumption that d(x, y) \u2264 1 this means that max x\u2208V \u00b5\nx (0) \u2264 1. Now put \u00b5 x (1) = 1 \u2212 \u00b5 x (0) \u2208 [0, 1].\nThis gives a solution to bias(S , T ) achieving the same objective value. We therefore have, bias(S , T ) = \u03b2(S , T ) .\n\nOn the other hand, by strong LP duality, we have\n\u03b2(S , T ) = min x,y\u2208V h(x, y)d(x, y) subject to y\u2208V h(x, y) \u2265 y\u2208V h(y, x) + S (x) \u2212 T (x) h(x, y) \u2265 0\nIt is clear that in the first constraint we must have equality in any optimal solution. Otherwise we can improve the objective value by decreasing some variable h(x, y) without violating any constraints. Since d is a metric we can now apply Lemma 3.2 to conclude that \u03b2(S , T ) = d EM (S , T ) and thus bias(S , T ) = d EM (S , T ).  d(x, y). Recall, a coupling is a distribution (X, Y) over V \u00d7 V such that the marginal distributions are S and T, respectively. The cost of the coupling is \u00be d(X, Y). It is not difficult to argue directly that any such coupling gives an upper bound on bias D tv , d (S , T ). We chose the linear programming proof since it leads to additional insight into the tightness of the theorem.\n\nThe situation for bias D \u221e , d is somewhat more complicated and we do not get a tight characterization in terms of an Earthmover distance. We do however have the following upper bound. Proof. By Lemma 2.3, we have D tv (\u00b5 x , \u00b5 y ) \u2264 D \u221e (\u00b5 x , \u00b5 y ) for any two distributions \u00b5 x , \u00b5 y . Hence, every (D \u221e , d)-Lipschitz mapping is also (D tv , d)-Lipschitz. Therefore, bias D tv , d (S , T ) is a relaxation of bias D \u221e , d (S , T ).\nCorollary 3.5. bias D \u221e , d (S , T ) \u2264 d EM (S , T )(12)\nFor completeness we note the dual linear program obtained from the definition of bias D \u221e , d (S , T ) :\nbias D \u221e , d (S , T ) = min x\u2208V x subject to y\u2208V f (x, y) + x \u2265 y\u2208V f (y, x)e d(x,y) + S (x) \u2212 T (x) (13) y\u2208V g(x, y) + x \u2265 y\u2208V g(y, x)e d(x,y)(14)\nf (x, y), g(x, y) \u2265 0\n\nSimilar to the proof of Theorem 3.3, we may interpret this program as a flow problem. The variables f (x, y), g(x, y) represent a nonnegative flow from x to y and x are slack variables. Note that the variables x are unrestricted as they correspond to an equality constraint. The first constraint requires that x has at least S (x) \u2212 T (x) outgoing units of flow in f. The RHS of the constraints states that the penalty for receiving a unit of flow from y is e d(x,y) . However, it is no longer clear that we can get rid of the variables x , g(x, y).\n\nOpen Question 3.1. Can we achieve a tight characterization of when (D \u221e , d)-Lipschitz implies statistical parity?\n\n\nFair Affirmative Action\n\nIn this section, we explore how to implement what may be called fair affirmative action. Indeed, a typical question when we discuss fairness is, \"What if we want to ensure statistical parity between two groups S and T, but members of S are less likely to be \"qualified\"? In Section 3, we have seen that when S and T are \"similar\" then the Lipschitz condition implies statistical parity. Here we consider the complementary case where S and T are very different and imposing statistical parity corresponds to preferential treatment. This is a cardinal question, which we examine with a concrete example illustrated in Figure 2. For simplicity, let T = S c . Assume |S |/|T \u222aS | = 1/10, so S is only 10% of the population. Suppose that our task-specific metric partitions S \u222a T into two groups, call them G 0 and G 1 , where members of G i are very close to one another and very far from all members of G 1\u2212i . Let S i , respectively T i , denote the intersection S \u2229 G i , respectively T \u2229 G i , for i = 0, 1. Finally, assume |S 0 | = |T 0 | = 9|S |/10. Thus, G 0 contains less than 20% of the total population, and is equally divided between S and T .\n\nThe Lipschitz condition requires that members of each G i be treated similarly to one another, but there is no requirement that members of G 0 be treated similarly to members of G 1 . The treatment of\nG 0 G 1 S 0 T 0 S 1 T 1 Figure 2: S 0 = G 0 \u2229 S , T 0 = G 0 \u2229 T\nmembers of S , on average, may therefore be very different from the treatment, on average, of members of T , since members of S are over-represented in G 0 and under-represented in G 1 . Thus the Lipschitz condition says nothing about statistical parity in this case. Suppose the members of G i are to be shown an advertisement ad i for a loan offering, where the terms in ad 1 are superior to those in ad 0 . Suppose further that the distance metric has partitioned the population according to (something correlated with) credit score, with those in G 1 having higher scores than those in G 0 .\n\nOn the one hand, this seems fair: people with better ability to repay are being shown a more attractive product. Now we ask two questions: \"What is the effect of imposing statistical parity?\" and \"What is the effect of failing to impose statistical parity?\" Imposing Statistical Parity. Essentially all of S is in G 0 , so for simplicity let us suppose that indeed S 0 = S \u2282 G 0 . In this case, to ensure that members of S have comparable chance of seeing ad 1 as do members of T , members of S must be treated, for the most part, like those in T 1 . In addition, by the Lipschitz condition, members of T 0 must be treated like members of S 0 = S , so these, also, are treated like T 1 , and the space essentially collapses, leaving only trivial solutions such as assigning a fixed probability distribution on the advertisements (ad 0 , ad 1 ) and showing ads according to this distribution to each individual, or showing all individuals ad i for some fixed i. However, while fair (all individuals are treated identically), these solutions fail to take the vendor's loss function into account.\n\nFailing to Impose Statistical Parity. The demographics of the groups G i differ from the demographics of the general population. Even though half the individuals shown ad 0 are members of S and half are members of T , this in turn can cause a problem with fairness: an \"anti-S \" vendor can effectively eliminate most members of S by replacing the \"reasonable\" advertisement ad 0 offering less good terms, with a blatantly hostile message designed to drive away customers. This eliminates essentially all business with members of S , while keeping intact most business with members of T . Thus, if members of S are relatively far from the members of T according to the distance metric, then satisfying the Lipschitz condition may fail to prevent some of the unfair practices.\n\n\nAn alternative optimization problem\n\nWith the above discussion in mind, we now suggest a different approach, in which we insist on statistical parity, but we relax the Lipschitz condition between elements of S and elements of S c . This is consistent with the essence of preferential treatment, which implies that elements in S are treated differently than elements in T . The approach is inspired by the use of the Earthmover relaxation in the context of metric labeling and 0-extension [KT02,CKNZ04]. Relaxing the S \u00d7 T Lipschitz constraints also makes sense if the information about the distances between members of S and members of T is of lower quality, or less reliable, than the internal distance information within these two sets.\n\nWe proceed in two steps:\n\n1. (a) First we compute a mapping from elements in S to distributions over T which transports the uniform distribution over S to the uniform distribution over T , while minimizing the total distance traveled. Additionally the mapping preserves the Lipschitz condition between elements within S . where {\u00b5 x } x\u2208S denotes the mapping computed in step (a). L can be viewed as a reweighting of the loss function L, taking into account the loss on S (indirectly through its mapping to T ).\n\n2. Run the Fairness LP only on T , using the new loss function L .\n\nComposing these two steps yields a a mapping from V = S \u222a T into A.\n\nFormally, we can express the first step of this alternative approach as a restricted Earthmover problem defined as\nd EM+L (S , T ) def = min \u00be x\u2208S \u00be y\u223c\u00b5 x d(x, y) (15) subject to D(\u00b5 x , \u00b5 x ) \u2264 d(x, x ) for all x, x \u2208 S D tv (\u00b5 S , U T ) \u2264 \u00b5 x \u2208 \u2206(T ) for all x \u2208 S\nHere, U T denotes the uniform distribution over T. Given {\u00b5 x } x\u2208S which minimizes (15) and {\u03bd x } x\u2208T which minimizes the original fairness LP (2) restricted to T, we define the mapping M : V \u2192 \u2206(A) by putting\nM(x) = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u03bd x x \u2208 T \u00be y\u223c\u00b5 x \u03bd y x \u2208 S .(16)\nBefore stating properties of the mapping M we make some remarks.\n\n1. Fundamentally, this new approach shifts from minimizing loss, subject to the Lipschitz constraints, to minimizing loss and disruption of S \u00d7 T Lipschitz requirement, subject to the parity and S \u00d7 S and T \u00d7 T Lipschitz constraints. This gives us a bicriteria optimization problem, with a wide range of options.\n\n2. We also have some flexibility even in the current version. For example, we can eliminate the re-weighting, prohibiting the vendor from expressing any opinion about the fate of elements in S . This makes sense in several settings. For example, the vendor may request this due to ignorance (e.g., lack of market research) about S , or the vendor may have some (hypothetical) special legal status based on past discrimination against S .\n\n3. It is instructive to compare the alternative approach to a modification of the Fairness LP in which we enforce statistical parity and eliminate the Lipschitz requirement on S \u00d7 T . The alternative approach is more faithful to the S \u00d7 T distances, providing protection against the self-fulfilling prophecy discussed in the Introduction, in which the vendor deliberately selects the \"wrong\" subset of S while still maintaining statistical parity.\n\n\n4.\n\nA related approach to addressing preferential treatment involves adjusting the metric in such a way that the Lipschitz condition will imply statistical parity. This coincides with at least one philosophy behind affirmative action: that the metric does not fully reflect potential that may be undeveloped because of unequal access to resources. Therefore, when we consider one of the strongest individuals in S , affirmative action suggests it is more appropriate to consider this individual as similar to one of the strongest individuals of T (rather than to an individual of T which is close according to the original distance metric). In this case, it is natural to adjust the distances between elements in S and T rather than inside each one of the populations (other than possibly re-scaling). This gives rise to a family of optimization problems:  Proof. The first property follows since\nD tv (M(S ), M(T )) = D tv \u00be x\u2208S \u00be y\u223c\u00b5 x \u03bd y , \u00be x\u2208T \u03bd x \u2264 D tv (\u00b5 S , U T ) \u2264 .\nThe second claim is trivial for (x, y) \u2208 T \u00d7 T. So, let (x, y) \u2208 S \u00d7 S . Then,\nD(M(x), M(y)) \u2264 D(\u00b5 x , \u00b5 y ) \u2264 d(x, y) .\nWe have given up the Lipschitz condition between S and T , instead relying on the terms d(x, y) in the objective function to discourage mapping x to distant y's. It turns out that the Lipschitz condition between elements x \u2208 S and y \u2208 T is still maintained on average and that the expected violation is given by d EM+L (S , T ) as shown next. Proof. For every x \u2208 S and y \u2208 T we have\nD tv (M(x), M(y)) = D tv \u00be z\u223c\u00b5 x M(z), M(y) \u2264 \u00be z\u223c\u00b5 x D tv (M(z), M(y)) (by Fact 2.1) \u2264 \u00be z\u223c\u00b5 x d(z, y) (Proposition 4.1 since z, y \u2208 T ) \u2264 d(x, y) + \u00be z\u223c\u00b5 x d(x, z) (by triangle inequalities)\nThe proof is completed by taking the expectation over x \u2208 S .\n\nAn interesting challenge for future work is handling preferential treatment of multiple protected subsets that are not mutually disjoint. The case of disjoint subsets seems easier and in particular amenable to our approach.\n\n\nSmall loss in bounded doubling dimension\n\nThe general LP shows that given an instance I, it is possible to find an \"optimally fair\" mapping in polynomial time. The result however does not give a concrete quantitative bound on the resulting loss. Further, when the instance is very large, it is desirable to come up with more efficient methods to define the mapping.\n\nWe now give a fairness mechanism for which we can prove a bound on the loss that it achieves in a natural setting. Moreover, the mechanism is significantly more efficient than the general linear program. Our mechanism is based on the exponential mechanism [MT07], first considered in the context of differential privacy.\n\nWe will describe the method in the natural setting where the mapping M maps elements of V to distributions over V itself. The method could be generalized to a different set A as long as we also have a distance function defined over A and some distance preserving embedding of V into A. A natural loss function to minimize in the setting where V is mapped into distributions over V is given by the metric d itself. In this setting we will give an explicit Lipschitz mapping and show that under natural assumptions on the metric space (V, d) the mapping achieves small loss. (x,y) . MT07]). The exponential mechanism is (D \u221e , d)-Lipschitz.\nDefinition 5.1. Given a metric d : V \u00d7 V \u2192 the exponential mechanism E : V \u2192 \u2206(V) is defined by putting E(x) def = [Z \u22121 x e \u2212d(x,y) ] y\u2208V , where Z x = y\u2208V e \u2212dLemma 5.1 ([\nOne cannot in general expect the exponential mechanism to achieve small loss. However, this turns out to be true in the case where (V, d) has small doubling dimension. It is important to note that in differential privacy, the space of databases does not have small doubling dimension. The situation in fairness is quite different. Many metric spaces arising in machine learning applications do have bounded doubling dimension. Hence the theorem that we are about to prove applies in many natural settings.\n\nDefinition 5.2. The doubling dimension of a metric space (V, d) is the smallest number k such that for every x \u2208 V and every R \u2265 0 the ball of radius R around x, denoted B(x, R) = {y \u2208 V : d(x, y) \u2264 R} can be covered by 2 k balls of radius R/2.\n\nWe will also need that points in the metric space are not too close together. Proof. Suppose d has doubling dimension k. It was shown in [CG08] that doubling dimension k implies for every\nR \u2265 0 that \u00be x\u2208V |B(x, 2R)| \u2264 2 k \u00be x\u2208V |B(x, R)| ,(17)\nwhere k = O(k). It follows from this condition and the assumption on (V, d) that for some positive > 0,\n\u00be x\u2208V |B(x, 1)| \u2264 1 k \u00be x\u2208V |B(x, )| = 2 O(k) .(18)\nThen,\n\u00be x\u2208V \u00be y\u223cE(x) d(x, y) \u2264 1 + \u00be x\u2208V \u221e 1 re \u2212r Z x |B(x, r)|dr \u2264 1 + \u00be x\u2208V \u221e 1 re \u2212r |B(x, r)|dr (since Z x \u2265 e \u2212d(x,x) = 1) = 1 + \u221e 1 re \u2212r \u00be x\u2208V |B(x, r)|dr \u2264 1 + \u221e 1 re \u2212r r k \u00be x\u2208V\n|B(x, 1)|dr (using (18))\n\u2264 1 + 2 O(k) \u221e 0 r k +1 e \u2212r dr \u2264 1 + 2 O(k) (k + 2)! .\nAs we assumed that k = O(1), we conclude The proof of Theorem 5.2 shows an exponential dependence on the doubling dimension k of the underlying space in the error of the exponential mechanism. The next theorem shows that the loss of any Lipschitz mapping has to scale at least linearly with k. The proof follows from a packing argument similar to that in [HT10]. The argument is slightly complicated by the fact that we need to give a lower bound on the average error (over x \u2208 V) of any mechanism.\n\u00be x\u2208V \u00be y\u223cE(x) d(x, y) \u2264 2 O(k) (k + 2)! \u2264 O(1) .Definition 5.4. A set B \u2286 V is called an R-packing if d(x, y) > R for all x, y \u2208 B.\nHere we give a lower bound using a metric space that may not be well-separated. However, following Remark 5.1, this also shows that any mapping defined on a well-separated subset of the metric space must have large error up to a small additive loss. \nV \u2192 \u2206(V) must satisfy \u00be x\u2208V \u00be y\u223cM(x) d(x, y) \u2265 \u2126(k) .\nProof. Construct V by randomly picking n points from a r-dimensional sphere of radius 100k. We will choose n sufficiently large and r = O(k). Endow V with the Euclidean distance d. Since V \u2286 r and r = O(k) it follows from a well-known fact that the doubling dimension of (V, d) is bounded by O(k).\n\nClaim 5.4. Let X be the distribution obtained by choosing a random x \u2208 V and outputting a random y \u2208 B(x, k). Then, for sufficiently large n, the distribution X has statistical distance at most 1/100 from the uniform distribution over V.\n\nProof. The claim follows from standard arguments showing that for large enough n every point y \u2208 V is contained in approximately equally many balls of radius k.\n\nLet M denote any (D \u221e , d)-Lipschitz mapping and denote its error on a point x \u2208 V by\nR(x) = \u00be y\u223cM(x)\nd(x, y) . and put R = \u00be x\u2208V R(x). Let G = {x \u2208 V : R(x) \u2264 2R}. By Markov's inequality |G| \u2265 n/2. Now, pick x \u2208 V uniformly at random and choose a set P x of 2 2k random points (with replacement) from B(x, k). For sufficiently large dimension r = O(k), it follows from concentration of measure on the sphere that P x forms a k/2-packing with probability, say, 1/10. Moreover, by Claim 5.4, for random x \u2208 V and random y \u2208 B(x, k), the probability that y \u2208 G is at least |G|/|V| \u2212 1/100 \u2265 1/3. Hence, with high probability,\n|P x \u2229 G| \u2265 2 2k /10 .(19)\nNow, suppose M satisfies R \u2264 k/100. We will lead this to a contradiction thus showing that M has average error at least k/100. Indeed, under the assumption that R \u2264 k/100, we have that for every y \u2208 G,\nr {M(y) \u2208 B(y, k/50)} \u2265 1 2 ,(20)\nand therefore\n1 \u2265 r M(x) \u2208 \u222a y\u2208P x \u2229G B(y, k/2) = y\u2208P x \u2229G r {M(x) \u2208 B(y, k/2)} (since P x is a k/2-packing) \u2265 y\u2208P x \u2229G exp(\u2212k) r(M(y) \u2208 B(y, k/2))\n(by the Lipschitz condition)\n= 2 2k 10 \u00b7 exp(\u2212k) 2 > 1 .\nThis is a contradiction which shows that R > k/100.\n\nOpen Question 5.1. Can we improve the exponential dependence on the doubling dimension in our upper bound?\n\n\nDiscussion and Future Directions\n\nIn this paper we introduced a framework for characterizing fairness in classification. The key element in this framework is a requirement that similar people be treated similarly in the classification. We developed an optimization approach which balanced these similarity constraints with a vendor's loss function. and analyzed when this local fairness condition implies statistical parity, a strong notion of equal treatment. We also presented an alternative formulation enforcing statistical parity, which is especially useful to allow preferential treatment of individuals from some group. We remark that although we have focused on using the metric as a method of defining and enforcing fairness, one can also use our approach to certify fairness (or to detect unfairness). This permits us to evaluate classifiers even when fairness is defined based on data that simply isn't available to the classification algorithm 3 . Below we consider some open questions and directions for future work.\n\n\nOn the Similarity Metric\n\nAs noted above, one of the most challenging aspects of our work is justifying the availability of a distance metric. We argue here that the notion of a metric already exists in many classification problems, and we consider some approaches to building such a metric.\n\n\nDefining a metric on individuals\n\nThe imposition of a metric already occurs in many classification processes. Examples include credit scores 4 for loan applications, and combinations of test scores and grades for some college admissions. In some cases, for reasons of social engineering, metrics may be adjusted based on membership in various groups, for example, to increase geographic and ethnic diversity. The construction of a suitable metric can be partially automated using existing machine learning techniques. This is true in particular for distances d(x, y) where x and y are both in the same protected set or both in the general population. When comparing individuals from different groups, we may need human insight and domain information. This is discussed further in Section 6.1.2.\n\nAnother direction, which intrigues us but which have not yet pursued, is particularly relevant to the context of on-line services (or advertising): allow users to specify attributes they do or do not want to have taken into account in classifying content of interest. The risk, as noted early on in this work, is that attributes may have redundant encodings in other attributes, including encodings of which the user, the ad network, and the advertisers may all be unaware. Our notion of fairness can potentially give a refinement of the \"user empowerment\" approach by allowing a user to participate in defining the metric that is used when providing services to this user (one can imagine for example a menu of metrics each one supposed to protect some subset of attributes). Further research into the feasibility of this approach is needed, in particular, our discussion throughout this paper has assumed that a single metric is used across the board. Can we make sense out of the idea of applying different metrics to different users?\n\n\nBuilding a metric via metric labeling\n\nOne approach to building the metric is to first build a metric on S c , say, using techniques from machine learning, and then \"inject\" members of S into the metric by mapping them to members of S in a fashion consistent with observed information. In our case, this observed information would come from the human insight and domain information mentioned above. Formally, this can be captured by the problem of metric labeling [KT02]: we have a collection of |S c | labels for which a metric is defined, together with |S | objects, each of which is to be assigned a label.\n\nIt may be expensive to access this extra information needed for metric labeling. We may ask the question of how much information do we need in order to approximate the result we would get were we to have all this information. This is related to our next question.\n\n\nHow much information is needed?\n\nSuppose there is an unknown metric d * (the right metric) that we are trying to find. We can ask an expert panel to tell us d * (x, y) given (x, y) \u2208 V 2 . The experts are costly and we are trying to minimize the number of calls we need to make. The question is: How many queries q do we need to make to be able to compute a metric d : V \u00d7 V \u2192 such that the distortion between d and d * is at most C, i.e.,\nsup x,y\u2208V max d(x, y) d * (x, y) , d * (x, y) d(x, y) \u2264 C .(21)\nThe problem can be seen as a variant of the well-studied question of constructing spanners. A spanner is a small implicit representation of a metric d * . While this is not exactly what we want, it seems that certain spanner constructions work in our setting as well,are willing to relax the embedding problem by permitting a certain fraction of the embedded edges to have arbitrary distortion, as any finite metric can be embedded, with constant slack and constant distortion, into constant-dimensional Euclidean space [ABC + 05].\n\n\nCase Study on Applications in Health Care\n\nAn interesting direction for a case study is suggested by another Wall Street Journal article (11/19/2010) that describes the (currently experimental) practice of insurance risk assessment via online tracking.\n\nFor example, food purchases and exercise habits correlate with certain diseases. This is a stimulating, albeit alarming, development. In the most individual-friendly interpretation described in the article, this provides a method for assessing risk that is faster and less expensive than the current practice of testing blood and urine samples. \"Deloitte and the life insurers stress the databases wouldn't be used to make final decisions about applicants. Rather, the process would simply speed up applications from people who look like good risks. Other people would go through the traditional assessment process.\" [SM10] Nonetheless, there are risks to the insurers, and preventing discrimination based on protected status should therefore be of interest: \"The information sold by marketing-database firms is lightly regulated. But using it in the life-insurance application process would \"raise questions\" about whether the data would be subject to the federal Fair Credit Reporting Act, says Rebecca Kuehn of the Federal Trade Commission's division of privacy and identity protection. The law's provisions kick in when \"adverse action\" is taken against a person, such as a decision to deny insurance or increase rates.\"\n\nAs mentioned in the introduction, the AALIM project [AAL] provides similarity information suitable for the health care setting. While their work is currently restricted to the area of cardiology, future work may extend to other medical domains. Such similarity information may be used to assemble a metric that decides which individual have similar medical conditions. Our framework could then employ this metric to ensure that similar patients receive similar health care policies. This would help to address the concerns articulated above. We pose it as an interesting direction for future work to investigate how a suitable fairness metric could be extracted from the AALIM system.\n\n\nDoes Fairness Hide Information?\n\nWe have already discussed the need for hiding (non-)membership in S in ensuring fairness. We now ask a converse question: Does fairness in the context of advertising hide information from the advertiser? Statistical parity has the interesting effect that it eliminates redundant encodings of S in terms of A, in the sense that after applying M, there is no f : A \u2192 {0, 1} that can be biased against S in any way. This prevents certain attacks that aim to determine membership in S .\n\nUnfortunately, this property is not hereditary. Indeed, suppose that the advertiser wishes to target HIV-positive people. If the set of HIV-positive people is protected, then the advertiser is stymied by the statistical parity constraint. However, suppose it so happens that the advertiser's utility function is extremely high on people who are not only HIV-positive but who also have AIDS. Consider a mapping that satisfies statistical parity for \"HIV-positive,\" but also maximizes the advertiser's utility. We expect that the necessary error of such a mapping will be on members of \"HIV\\AIDS,\" that is, people who are HIV-positive but who do not have AIDS. In particular, we don't expect the mapping to satisfy statistical parity for \"AIDS\" -the fraction of people with AIDS seeing the advertisement may be much higher than the fraction of people with AIDS in the population as a whole. Hence, the advertiser can in fact target \"AIDS\".\n\nAlternatively, suppose people with AIDS are mapped to a region B \u2282 A, as is a |AIDS|/|HIV positive| fraction of HIV-negative individuals. Thus, being mapped to B maintains statistical parity for the set of HIV-positive individuals, meaning that the probability that a random HIV-positive individual is mapped to B is the same as the probability that a random member of the whole population is mapped to B. Assume further that mappings to A \\ B also maintains parity. Now the advertiser can refuse to do business with all people with AIDS, sacrificing just a small amount of business in the HIV-negative community.\n\nThese examples show that statistical parity is not a good method of hiding sensitive information in targeted advertising. A natural question, not yet pursued, is whether we can get better protection using the Lipschitz property with a suitable metric.\n\nDefinition 2 . 1 (\n21Lipschitz mapping). A mapping M : V \u2192 \u2206(A) satisfies the (D, d)-Lipschitz property if for every x, y \u2208 V, we have D(Mx, My) \u2264 d(x, y) .\n\nLemma 2. 2 .\n2Let D = D \u221e . Given an instance I we can compute opt(I) with a linear program of size poly(|V|, |A|).\n\nDefinition 3. 1 (\n1Statistical parity). We say that a mapping M : V \u2192 \u2206(A) satisfies statistical parity between distributions S and T up to bias if\n\nLemma 3. 1 .\n1Let D \u2208 {D tv , D \u221e } and let M : V \u2192 \u2206(A) be any (D, d)-Lipschitz mapping. Then, M satisfies statistical parity between S and T up to bias D,d (S , T ). Proof. Let M = {\u00b5 x } x\u2208V be any (D, d)-Lipschitz mapping into A. We will construct a (D, d)-Lipschitz mapping M : V \u2192 \u2206({0, 1}) which has the same bias between S and T as M.\n\nLemma 3. 2 .\n2Let d : V \u00d7 V \u2192 be a metric. Then, d EM (S , T ) = min y, x) + S (x) \u2212 T (x) h(x, y) \u2265 0 Theorem 3.3. Let d be a metric. Then,\n\nRemark 3. 1 .\n1Here we point out a different proof of the fact that bias D tv , d (S , T ) \u2264 d EM (S , T ) which does not involve LP duality. Indeed d EM (S , T ) can be interpreted as giving the cost of the best coupling between the two distributions S and T subject to the penalty function\n\n\n\u221e , d (S , T ) \u2264 bias D tv , d (S , T ) (11)\n\n( b )\nbThis mapping gives us the following new loss function for elements of T : For y \u2208 T and a \u2208 A we define a new loss, L (y, a), as L (y, a) = x\u2208S \u00b5 x (y)L(x, a) + L(y, a) ,\n\nFind\na new distance metric d which \"best approximates\" d under the condition that S and T have small Earthmover distance under d , where we have the flexibility of choosing the measure of quality to how well d approximates d. Let M be the mapping of Equation 16. The following properties of M are easy to verify.\n\nProposition 4. 1 .\n1The mapping M defined in (16) satisfies 1. statistical parity between S and T up to bias , 2. the Lipschitz condition for every pair (x, y) \u2208 (S \u00d7 S ) \u222a (T \u00d7 T ).\n\n\n\u2212 d(x, y) \u2264 d EM+L (S , T ) .\n\nDefinition 5 . 3 .\n53We call a metric space (V, d) well separated if there is a positive constant > 0 such that |B(x, )| = 1 for all x \u2208 V. Theorem 5.2. Let d be a well separated metric space of bounded doubling dimension. Then the exponential mechanism satisfies \u00be x\u2208V \u00be y\u223cE(x) d(x, y) = O(1) .\n\nRemark 5. 1 .\n1If (V, d) is not well-separated, then for every constant > 0, it must contain a wellseparated subset V \u2286 V such that every point x \u2208 V has a neighbor x \u2208 V such that d(x, x ) \u2264 .A Lipschitz mapping M defined on V naturally extends to all of V by putting M(x) = M (x )where x is the nearest neighbor of x in V . It is easy to see that the expected loss of M is only an additive worse than that of M . Similarly, the Lipschitz condition deteriorates by an additive 2 , i.e., D \u221e (M(x), M(y)) \u2264 d(x, y) + 2 . Indeed, denoting the nearest neighbors in V of x, y by x , y respectively, we have D \u221e (M(x), M(y)) = D \u221e (M (x ), M (y )) \u2264 d(x , y ) \u2264 d(x, y)+d(x, x )+d(y, y ) \u2264 d(x, y) + 2 . Here, we used the triangle inequality.\n\nTheorem 5 . 3 .\n53For every k \u2265 2 and every large enough n \u2265 n 0 (k) there exists an n-point metric space of doubling dimension O(k) such that any (D \u221e , d)-Lipschitz mapping M :\nStrictly speaking, we only require a function d : V \u00d7V \u2192 where V is the set of individuals, d(x, y) \u2265 0, d(x, y) = d(y, x) and d(x, x) = 0.\nThis is consistent with the practice, in some college admissions offices, of adding a certain number of points to SAT scores of students in disadvantaged groups.\nThis observation is due to Boaz Barak.4 We remark that the credit score is a one-dimensional metric that suggests an obvious interpretation as a measure of quality rather than a measure of similarity. When the metric is defined over multiple attributes such an interpretation is no longer clear.\nAcknowledgmentsWe would like to thank Amos Fiat for a long and wonderful discussion which started this project. We also thank Ittai Abraham, Boaz Barak, Mike Hintze, Jon Kleinberg, Robi Krauthgamer, Deirdre Mulligan, Ofer Neiman, Kobbi Nissim, Aaron Roth, and Tal Zarsky for helpful discussions. Finally, we are deeply grateful to Micah Altman for bringing to our attention key philosophical and economics works.A Catalog of EvilsWe briefly summarize here behaviors against which we wish to protect. We make no attempt to be formal. Let S be a protected set.1. Blatant explicit discrimination. This is when membership in S is explicitly tested for and a \"worse\" outcome is given to members of S than to members of S c .Discrimination Based on Redundant Encoding.Here the explicit test for membership in S is replaced by a test that is, in practice, essentially equivalent. This is a successful attack against \"fairness through blindness,\" in which the idea is to simply ignore protected attributes such as sex or race. However, when personalization and advertising decisions are based on months or years of on-line activity, there is a very real possibility that membership in a given demographic group is embedded holographically in the history. Simply deleting, say, the Facebook \"sex\" and \"Interested in men/women\" bits almost surely does not hide homosexuality. This point was argued by the (somewhat informal) \"Gaydar\" study[JM09]in which a threshold was found for predicting, based on the sexual preferences of his male friends, whether or not a given male is interested in men. Such redundant encodings of sexual preference and other attributes need not be explicitly known or recognized as such, and yet can still have a discriminatory effect.\nAALIM. AALIM. http://www.almaden.ibm.com/cs/projects/aalim/.\n\nFairness in scheduling. Miklos Ajtai, James Aspnes, Moni Naor, Yuval Rabani, Leonard J Schulman, Orli Waarts, Journal of Algorithms. 292AAN + 98[AAN + 98] Miklos Ajtai, James Aspnes, Moni Naor, Yuval Rabani, Leonard J. Schulman, and Orli Waarts. Fairness in scheduling. Journal of Algorithms, 29(2):306-357, November 1998.\n\nMetric embeddings with relaxed guarantees. Ittai Abraham, Yair Bartal, T.-H Hubert, Kedar Chan, Anupam Dhamdhere, Jon M Gupta, Kleinberg, Ofer Neiman, and Aleksandrs Slivkins. IEEEFOCS[ABC + 05] Ittai Abraham, Yair Bartal, Hubert T.-H. Chan, Kedar Dhamdhere, Anupam Gupta, Jon M. Kleinberg, Ofer Neiman, and Aleksandrs Slivkins. Metric embeddings with relaxed guarantees. In FOCS, pages 83-100. IEEE, 2005.\n\nThe santa claus problem. Nikhil Bansal, Maxim Sviridenko, Proc. 38th STOC. 38th STOCACMNikhil Bansal and Maxim Sviridenko. The santa claus problem. In Proc. 38th STOC, pages 31-40. ACM, 2006.\n\nDecentralizing equality of opportunity and issues concerning the equality of educational opportunity. Catarina Calsamiglia, Doctoral Dissertation, Yale UniversityCatarina Calsamiglia. Decentralizing equality of opportunity and issues concerning the equality of educational opportunity, 2005. Doctoral Dissertation, Yale University.\n\nApproximating TSP on metrics with bounded global growth. T.-H. Hubert Chan, Anupam Gupta, Proc. 19th Symposium on Discrete Algorithms (SODA). 19th Symposium on Discrete Algorithms (SODA)ACM-SIAMT.-H. Hubert Chan and Anupam Gupta. Approximating TSP on metrics with bounded global growth. In Proc. 19th Symposium on Discrete Algorithms (SODA), pages 690-699. ACM-SIAM, 2008.\n\nA linear programming formulation and approximation algorithms for the metric labeling problem. Chandra Chekuri, Sanjeev Khanna, Joseph Naor, Leonid Zosin, SIAM J. Discrete Math. 183Chandra Chekuri, Sanjeev Khanna, Joseph Naor, and Leonid Zosin. A linear programming formulation and approximation algorithms for the metric labeling problem. SIAM J. Discrete Math., 18(3):608-625, 2004.\n\nCalibrating noise to sensitivity in private data analysis. Cynthia Dwork, Frank Mcsherry, Kobbi Nissim, Adam Smith, Proc. 3rd TCC. 3rd TCCSpringerCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Proc. 3rd TCC, pages 265-284. Springer, 2006.\n\nDifferential privacy. Cynthia Dwork, Proc. 33rd ICALP. 33rd ICALPSpringerCynthia Dwork. Differential privacy. In Proc. 33rd ICALP, pages 1-12. Springer, 2006.\n\nOn allocations that maximize fairness. Uri Feige, Proc. 19th Symposium on Discrete Algorithms (SODA). 19th Symposium on Discrete Algorithms (SODA)ACM-SIAMUri Feige. On allocations that maximize fairness. In Proc. 19th Symposium on Discrete Algorithms (SODA), pages 287-293. ACM-SIAM, 2008.\n\nMechanism design with uncertain inputs (to err is human, to forgive divine). Uri Feige, Moshe Tennenholtz, Proc. 43rd STOC. 43rd STOCACMUri Feige and Moshe Tennenholtz. Mechanism design with uncertain inputs (to err is human, to forgive divine). In Proc. 43rd STOC, pages 549-558. ACM, 2011.\n\nOn the geometry of differential privacy. Moritz Hardt, Kunal Talwar, Proc. 42nd STOC. 42nd STOCACMMoritz Hardt and Kunal Talwar. On the geometry of differential privacy. In Proc. 42nd STOC. ACM, 2010.\n\nRedlining. Encyclopedia of Chicago. D , Bradford Hunt, D. Bradford Hunt. Redlining. Encyclopedia of Chicago, 2005.\n\nGaydar: Facebook friendships expose sexual orientation. Carter Jernigan, F T Behram, Mistree, First Monday. 1410Carter Jernigan and Behram F.T. Mistree. Gaydar: Facebook friendships expose sexual orientation. First Monday, 14(10), 2009.\n\nApproximation algorithms for classification problems with pairwise relationships: metric labeling and markov random fields. Jon M Kleinberg, Tardos, Journal of the ACM (JACM). 495Jon M. Kleinberg and\u00c9va Tardos. Approximation algorithms for classification problems with pairwise relationships: metric labeling and markov random fields. Journal of the ACM (JACM), 49(5):616-639, 2002.\n\nMechanism design via differential privacy. Frank Mcsherry, Kunal Talwar, Proc. 48th Foundations of Computer Science (FOCS). 48th Foundations of Computer Science (FOCS)IEEEFrank McSherry and Kunal Talwar. Mechanism design via differential privacy. In Proc. 48th Foundations of Computer Science (FOCS), pages 94-103. IEEE, 2007.\n\nIncorporating fairness into game theory and economics. M Rabin, The American Economic Review. 83M. Rabin. Incorporating fairness into game theory and economics. The American Economic Review, 83:1281-1302, 1993.\n\nJustice as Fairness, A Restatement. John Rawls, Belknap PressJohn Rawls. Justice as Fairness, A Restatement. Belknap Press, 2001.\n\nOn the web's cutting edge, anonymity in name only. Emily Steel, Julia Angwin, The Wall Street Journal. Emily Steel and Julia Angwin. On the web's cutting edge, anonymity in name only. The Wall Street Journal, 2010.\n\nInsurers test data profiles to identify risky clients. Leslie Scism, Mark Maremont, The Wall Street Journal. Leslie Scism and Mark Maremont. Insurers test data profiles to identify risky clients. The Wall Street Journal, 2010.\n\nEquity. H , Peyton Young, Princeton University PressH. Peyton Young. Equity. Princeton University Press, 1995.\n\n. Tal Zarsky, Private communicationTal Zarsky. Private communication. 2011.\n\nA well-known form of discrimination based on redundant encoding. The following definition appears in an article by [Hun05], which contains the history of the term, the practice, and its consequences. Redlining, Redlining is the practice of arbitrarily denying or limiting financial services to specific neighborhoods, generally because its residents are people of color or are poorRedlining. A well-known form of discrimination based on redundant encoding. The following definition appears in an article by [Hun05], which contains the history of the term, the practice, and its consequences: \"Redlining is the practice of arbitrarily denying or limiting financial services to specific neighborhoods, generally because its residents are people of color or are poor.\"\n\nCutting off business with a segment of the population in which membership in the protected set is disproportionately high. A generalization of redlining, in which members of S need not be a majority of the redlined population. instead, the fraction of the redlined population belonging to S may simply exceed the fraction of S in the population as a wholeCutting off business with a segment of the population in which membership in the protected set is disproportionately high. A generalization of redlining, in which members of S need not be a majority of the redlined population; instead, the fraction of the redlined population belonging to S may simply exceed the fraction of S in the population as a whole.\n\nHere the vendor advertiser is willing to cut off its nose to spite its face, deliberately choosing the \"wrong\" members of S in order to build a bad \"track record\" for S . A less malicious vendor may simply select random members of S rather than qualified members, thus inadvertently building a bad track record for S. Self-fulfilling prophecySelf-fulfilling prophecy. Here the vendor advertiser is willing to cut off its nose to spite its face, deliberately choosing the \"wrong\" members of S in order to build a bad \"track record\" for S . A less malicious vendor may simply select random members of S rather than qualified members, thus inadvertently building a bad track record for S .\n\nThis might be compelling, but by sacrificing one really good candidate c \u2208 S c the bank could refute all charges of discrimination against S . That is, c is a token rejectee; hence the term \"reverse tokenism\" (\"tokenism\" usually refers to accepting a token member of S ). We remark that the general question of explaining decisions seems quite difficult. One possible refutation might be the exhibition of an \"obviously more qualified\" member of S c who is also denied a loan. a situation only made worse by the existence of redundant encodings of attributesReverse tokenism. This concept arose in the context of imagining what might be a convincing refutation to the claim \"The bank denied me a loan because I am a member of S .\" One possible refutation might be the exhibition of an \"obviously more qualified\" member of S c who is also denied a loan. This might be compelling, but by sacrificing one really good candidate c \u2208 S c the bank could refute all charges of discrimination against S . That is, c is a token rejectee; hence the term \"reverse tokenism\" (\"tokenism\" usually refers to accepting a token member of S ). We remark that the general question of explaining decisions seems quite difficult, a situation only made worse by the existence of redundant encodings of attributes.\n", "annotations": {"author": "[{\"end\":92,\"start\":58},{\"end\":269,\"start\":93},{\"end\":286,\"start\":270},{\"end\":329,\"start\":287},{\"end\":366,\"start\":330}]", "publisher": null, "author_last_name": "[{\"end\":71,\"start\":66},{\"end\":105,\"start\":100},{\"end\":285,\"start\":278},{\"end\":300,\"start\":292},{\"end\":343,\"start\":338}]", "author_first_name": "[{\"end\":65,\"start\":58},{\"end\":99,\"start\":93},{\"end\":277,\"start\":270},{\"end\":291,\"start\":287},{\"end\":337,\"start\":330}]", "author_affiliation": "[{\"end\":160,\"start\":126},{\"end\":268,\"start\":162}]", "title": "[{\"end\":27,\"start\":1},{\"end\":393,\"start\":367}]", "venue": null, "abstract": "[{\"end\":1993,\"start\":820}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2919,\"start\":2913},{\"end\":3773,\"start\":3772},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3978,\"start\":3971},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7907,\"start\":7900},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7914,\"start\":7907},{\"end\":10040,\"start\":10035},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11602,\"start\":11595},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12632,\"start\":12625},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12698,\"start\":12691},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13659,\"start\":13653},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13665,\"start\":13659},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13670,\"start\":13665},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13795,\"start\":13788},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19317,\"start\":19311},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21505,\"start\":21498},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21512,\"start\":21505},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26388,\"start\":26381},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37334,\"start\":37328},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37341,\"start\":37334},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":42627,\"start\":42621},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":43273,\"start\":43268},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":44396,\"start\":44390},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":45284,\"start\":45278},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":51446,\"start\":51440},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":53768,\"start\":53762},{\"end\":54428,\"start\":54423},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":60898,\"start\":60897}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":57538,\"start\":57381},{\"attributes\":{\"id\":\"fig_1\"},\"end\":57655,\"start\":57539},{\"attributes\":{\"id\":\"fig_2\"},\"end\":57804,\"start\":57656},{\"attributes\":{\"id\":\"fig_3\"},\"end\":58148,\"start\":57805},{\"attributes\":{\"id\":\"fig_4\"},\"end\":58290,\"start\":58149},{\"attributes\":{\"id\":\"fig_5\"},\"end\":58583,\"start\":58291},{\"attributes\":{\"id\":\"fig_6\"},\"end\":58630,\"start\":58584},{\"attributes\":{\"id\":\"fig_7\"},\"end\":58809,\"start\":58631},{\"attributes\":{\"id\":\"fig_8\"},\"end\":59123,\"start\":58810},{\"attributes\":{\"id\":\"fig_9\"},\"end\":59307,\"start\":59124},{\"attributes\":{\"id\":\"fig_10\"},\"end\":59339,\"start\":59308},{\"attributes\":{\"id\":\"fig_11\"},\"end\":59636,\"start\":59340},{\"attributes\":{\"id\":\"fig_12\"},\"end\":60376,\"start\":59637},{\"attributes\":{\"id\":\"fig_13\"},\"end\":60556,\"start\":60377}]", "paragraph": "[{\"end\":2920,\"start\":2009},{\"end\":3571,\"start\":2922},{\"end\":4240,\"start\":3573},{\"end\":4782,\"start\":4242},{\"end\":5784,\"start\":4816},{\"end\":6185,\"start\":5786},{\"end\":7439,\"start\":6187},{\"end\":7764,\"start\":7441},{\"end\":8727,\"start\":7766},{\"end\":9251,\"start\":8729},{\"end\":9923,\"start\":9278},{\"end\":10784,\"start\":9925},{\"end\":11141,\"start\":10786},{\"end\":12385,\"start\":11158},{\"end\":13031,\"start\":12387},{\"end\":13796,\"start\":13033},{\"end\":14910,\"start\":13827},{\"end\":15007,\"start\":14912},{\"end\":15319,\"start\":15009},{\"end\":15448,\"start\":15321},{\"end\":16221,\"start\":15471},{\"end\":16522,\"start\":16443},{\"end\":16637,\"start\":16524},{\"end\":16996,\"start\":16639},{\"end\":17387,\"start\":16998},{\"end\":18086,\"start\":17444},{\"end\":18140,\"start\":18088},{\"end\":18290,\"start\":18193},{\"end\":18431,\"start\":18329},{\"end\":18648,\"start\":18491},{\"end\":18761,\"start\":18650},{\"end\":19058,\"start\":18947},{\"end\":20252,\"start\":19082},{\"end\":21378,\"start\":20254},{\"end\":22978,\"start\":21417},{\"end\":23217,\"start\":22980},{\"end\":23759,\"start\":23284},{\"end\":23817,\"start\":23761},{\"end\":24038,\"start\":23843},{\"end\":24553,\"start\":24150},{\"end\":25002,\"start\":24597},{\"end\":25684,\"start\":25004},{\"end\":26389,\"start\":25686},{\"end\":26866,\"start\":26391},{\"end\":27546,\"start\":26927},{\"end\":27641,\"start\":27598},{\"end\":28204,\"start\":27702},{\"end\":28673,\"start\":28285},{\"end\":28917,\"start\":28782},{\"end\":29015,\"start\":28962},{\"end\":29182,\"start\":29061},{\"end\":29338,\"start\":29311},{\"end\":29547,\"start\":29400},{\"end\":29870,\"start\":29549},{\"end\":30045,\"start\":29872},{\"end\":30447,\"start\":30153},{\"end\":30618,\"start\":30499},{\"end\":30668,\"start\":30620},{\"end\":31490,\"start\":30771},{\"end\":31927,\"start\":31492},{\"end\":32089,\"start\":31985},{\"end\":32259,\"start\":32238},{\"end\":32810,\"start\":32261},{\"end\":32926,\"start\":32812},{\"end\":34104,\"start\":32954},{\"end\":34306,\"start\":34106},{\"end\":34966,\"start\":34371},{\"end\":36061,\"start\":34968},{\"end\":36837,\"start\":36063},{\"end\":37578,\"start\":36877},{\"end\":37604,\"start\":37580},{\"end\":38091,\"start\":37606},{\"end\":38159,\"start\":38093},{\"end\":38228,\"start\":38161},{\"end\":38344,\"start\":38230},{\"end\":38708,\"start\":38497},{\"end\":38828,\"start\":38764},{\"end\":39142,\"start\":38830},{\"end\":39581,\"start\":39144},{\"end\":40030,\"start\":39583},{\"end\":40929,\"start\":40037},{\"end\":41089,\"start\":41011},{\"end\":41515,\"start\":41132},{\"end\":41770,\"start\":41709},{\"end\":41995,\"start\":41772},{\"end\":42363,\"start\":42040},{\"end\":42685,\"start\":42365},{\"end\":43325,\"start\":42687},{\"end\":44005,\"start\":43500},{\"end\":44251,\"start\":44007},{\"end\":44440,\"start\":44253},{\"end\":44600,\"start\":44497},{\"end\":44658,\"start\":44653},{\"end\":44866,\"start\":44842},{\"end\":45421,\"start\":44923},{\"end\":45805,\"start\":45555},{\"end\":46157,\"start\":45860},{\"end\":46396,\"start\":46159},{\"end\":46558,\"start\":46398},{\"end\":46645,\"start\":46560},{\"end\":47183,\"start\":46662},{\"end\":47412,\"start\":47211},{\"end\":47460,\"start\":47447},{\"end\":47623,\"start\":47595},{\"end\":47703,\"start\":47652},{\"end\":47811,\"start\":47705},{\"end\":48843,\"start\":47848},{\"end\":49137,\"start\":48872},{\"end\":49934,\"start\":49174},{\"end\":50973,\"start\":49936},{\"end\":51585,\"start\":51015},{\"end\":51850,\"start\":51587},{\"end\":52292,\"start\":51886},{\"end\":52888,\"start\":52357},{\"end\":53143,\"start\":52934},{\"end\":54369,\"start\":53145},{\"end\":55055,\"start\":54371},{\"end\":55573,\"start\":55091},{\"end\":56512,\"start\":55575},{\"end\":57127,\"start\":56514},{\"end\":57380,\"start\":57129}]", "formula": "[{\"attributes\":{\"id\":\"formula_1\"},\"end\":16402,\"start\":16222},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16442,\"start\":16402},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17443,\"start\":17388},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18192,\"start\":18141},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18328,\"start\":18291},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18490,\"start\":18432},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18946,\"start\":18762},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23842,\"start\":23818},{\"attributes\":{\"id\":\"formula_9\"},\"end\":24149,\"start\":24039},{\"attributes\":{\"id\":\"formula_10\"},\"end\":27597,\"start\":27547},{\"attributes\":{\"id\":\"formula_11\"},\"end\":27701,\"start\":27642},{\"attributes\":{\"id\":\"formula_12\"},\"end\":28284,\"start\":28205},{\"attributes\":{\"id\":\"formula_13\"},\"end\":28781,\"start\":28674},{\"attributes\":{\"id\":\"formula_14\"},\"end\":28961,\"start\":28918},{\"attributes\":{\"id\":\"formula_15\"},\"end\":29060,\"start\":29016},{\"attributes\":{\"id\":\"formula_16\"},\"end\":29310,\"start\":29183},{\"attributes\":{\"id\":\"formula_17\"},\"end\":29399,\"start\":29339},{\"attributes\":{\"id\":\"formula_18\"},\"end\":30152,\"start\":30046},{\"attributes\":{\"id\":\"formula_19\"},\"end\":30498,\"start\":30448},{\"attributes\":{\"id\":\"formula_20\"},\"end\":30770,\"start\":30669},{\"attributes\":{\"id\":\"formula_21\"},\"end\":31984,\"start\":31928},{\"attributes\":{\"id\":\"formula_22\"},\"end\":32237,\"start\":32090},{\"attributes\":{\"id\":\"formula_23\"},\"end\":34370,\"start\":34307},{\"attributes\":{\"id\":\"formula_24\"},\"end\":38496,\"start\":38345},{\"attributes\":{\"id\":\"formula_25\"},\"end\":38763,\"start\":38709},{\"attributes\":{\"id\":\"formula_26\"},\"end\":41010,\"start\":40930},{\"attributes\":{\"id\":\"formula_27\"},\"end\":41131,\"start\":41090},{\"attributes\":{\"id\":\"formula_28\"},\"end\":41708,\"start\":41516},{\"attributes\":{\"id\":\"formula_29\"},\"end\":43487,\"start\":43326},{\"attributes\":{\"id\":\"formula_30\"},\"end\":43499,\"start\":43487},{\"attributes\":{\"id\":\"formula_31\"},\"end\":44496,\"start\":44441},{\"attributes\":{\"id\":\"formula_32\"},\"end\":44652,\"start\":44601},{\"attributes\":{\"id\":\"formula_33\"},\"end\":44841,\"start\":44659},{\"attributes\":{\"id\":\"formula_34\"},\"end\":44922,\"start\":44867},{\"attributes\":{\"id\":\"formula_35\"},\"end\":45471,\"start\":45422},{\"attributes\":{\"id\":\"formula_36\"},\"end\":45554,\"start\":45471},{\"attributes\":{\"id\":\"formula_37\"},\"end\":45859,\"start\":45806},{\"attributes\":{\"id\":\"formula_38\"},\"end\":46661,\"start\":46646},{\"attributes\":{\"id\":\"formula_39\"},\"end\":47210,\"start\":47184},{\"attributes\":{\"id\":\"formula_40\"},\"end\":47446,\"start\":47413},{\"attributes\":{\"id\":\"formula_41\"},\"end\":47594,\"start\":47461},{\"attributes\":{\"id\":\"formula_42\"},\"end\":47651,\"start\":47624},{\"attributes\":{\"id\":\"formula_43\"},\"end\":52356,\"start\":52293}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2007,\"start\":1995},{\"attributes\":{\"n\":\"1.1\"},\"end\":4814,\"start\":4785},{\"attributes\":{\"n\":\"1.2\"},\"end\":9276,\"start\":9254},{\"attributes\":{\"n\":\"1.3\"},\"end\":11156,\"start\":11144},{\"attributes\":{\"n\":\"2\"},\"end\":13825,\"start\":13799},{\"attributes\":{\"n\":\"2.1\"},\"end\":15469,\"start\":15451},{\"attributes\":{\"n\":\"2.2\"},\"end\":19080,\"start\":19061},{\"attributes\":{\"n\":\"2.3\"},\"end\":21415,\"start\":21381},{\"attributes\":{\"n\":\"3\"},\"end\":23282,\"start\":23220},{\"attributes\":{\"n\":\"3.1\"},\"end\":24595,\"start\":24556},{\"attributes\":{\"n\":\"3.2\"},\"end\":26925,\"start\":26869},{\"attributes\":{\"n\":\"4\"},\"end\":32952,\"start\":32929},{\"attributes\":{\"n\":\"4.1\"},\"end\":36875,\"start\":36840},{\"end\":40035,\"start\":40033},{\"attributes\":{\"n\":\"5\"},\"end\":42038,\"start\":41998},{\"attributes\":{\"n\":\"6\"},\"end\":47846,\"start\":47814},{\"attributes\":{\"n\":\"6.1\"},\"end\":48870,\"start\":48846},{\"attributes\":{\"n\":\"6.1.1\"},\"end\":49172,\"start\":49140},{\"attributes\":{\"n\":\"6.1.2\"},\"end\":51013,\"start\":50976},{\"attributes\":{\"n\":\"6.1.3\"},\"end\":51884,\"start\":51853},{\"attributes\":{\"n\":\"6.2\"},\"end\":52932,\"start\":52891},{\"attributes\":{\"n\":\"6.3\"},\"end\":55089,\"start\":55058},{\"end\":57400,\"start\":57382},{\"end\":57552,\"start\":57540},{\"end\":57674,\"start\":57657},{\"end\":57818,\"start\":57806},{\"end\":58162,\"start\":58150},{\"end\":58305,\"start\":58292},{\"end\":58637,\"start\":58632},{\"end\":58815,\"start\":58811},{\"end\":59143,\"start\":59125},{\"end\":59359,\"start\":59341},{\"end\":59651,\"start\":59638},{\"end\":60393,\"start\":60378}]", "table": null, "figure_caption": "[{\"end\":57538,\"start\":57403},{\"end\":57655,\"start\":57554},{\"end\":57804,\"start\":57676},{\"end\":58148,\"start\":57820},{\"end\":58290,\"start\":58164},{\"end\":58583,\"start\":58307},{\"end\":58630,\"start\":58586},{\"end\":58809,\"start\":58639},{\"end\":59123,\"start\":58816},{\"end\":59307,\"start\":59145},{\"end\":59339,\"start\":59310},{\"end\":59636,\"start\":59362},{\"end\":60376,\"start\":59653},{\"end\":60556,\"start\":60396}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15877,\"start\":15869},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15922,\"start\":15914},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22497,\"start\":22489},{\"end\":31112,\"start\":31105},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33578,\"start\":33570},{\"end\":43265,\"start\":43260}]", "bib_author_first_name": "[{\"end\":62999,\"start\":62993},{\"end\":63012,\"start\":63007},{\"end\":63025,\"start\":63021},{\"end\":63037,\"start\":63032},{\"end\":63053,\"start\":63046},{\"end\":63055,\"start\":63054},{\"end\":63070,\"start\":63066},{\"end\":63341,\"start\":63336},{\"end\":63355,\"start\":63351},{\"end\":63368,\"start\":63364},{\"end\":63382,\"start\":63377},{\"end\":63395,\"start\":63389},{\"end\":63410,\"start\":63407},{\"end\":63412,\"start\":63411},{\"end\":63732,\"start\":63726},{\"end\":63746,\"start\":63741},{\"end\":64004,\"start\":63996},{\"end\":64296,\"start\":64284},{\"end\":64309,\"start\":64303},{\"end\":64703,\"start\":64696},{\"end\":64720,\"start\":64713},{\"end\":64735,\"start\":64729},{\"end\":64748,\"start\":64742},{\"end\":65053,\"start\":65046},{\"end\":65066,\"start\":65061},{\"end\":65082,\"start\":65077},{\"end\":65095,\"start\":65091},{\"end\":65332,\"start\":65325},{\"end\":65505,\"start\":65502},{\"end\":65834,\"start\":65831},{\"end\":65847,\"start\":65842},{\"end\":66094,\"start\":66088},{\"end\":66107,\"start\":66102},{\"end\":66286,\"start\":66285},{\"end\":66297,\"start\":66289},{\"end\":66427,\"start\":66421},{\"end\":66439,\"start\":66438},{\"end\":66441,\"start\":66440},{\"end\":66730,\"start\":66727},{\"end\":66732,\"start\":66731},{\"end\":67035,\"start\":67030},{\"end\":67051,\"start\":67046},{\"end\":67371,\"start\":67370},{\"end\":67567,\"start\":67563},{\"end\":67714,\"start\":67709},{\"end\":67727,\"start\":67722},{\"end\":67935,\"start\":67929},{\"end\":67947,\"start\":67943},{\"end\":68111,\"start\":68110},{\"end\":68120,\"start\":68114},{\"end\":68219,\"start\":68216}]", "bib_author_last_name": "[{\"end\":63005,\"start\":63000},{\"end\":63019,\"start\":63013},{\"end\":63030,\"start\":63026},{\"end\":63044,\"start\":63038},{\"end\":63064,\"start\":63056},{\"end\":63077,\"start\":63071},{\"end\":63349,\"start\":63342},{\"end\":63362,\"start\":63356},{\"end\":63375,\"start\":63369},{\"end\":63387,\"start\":63383},{\"end\":63405,\"start\":63396},{\"end\":63418,\"start\":63413},{\"end\":63429,\"start\":63420},{\"end\":63739,\"start\":63733},{\"end\":63757,\"start\":63747},{\"end\":64016,\"start\":64005},{\"end\":64301,\"start\":64297},{\"end\":64315,\"start\":64310},{\"end\":64711,\"start\":64704},{\"end\":64727,\"start\":64721},{\"end\":64740,\"start\":64736},{\"end\":64754,\"start\":64749},{\"end\":65059,\"start\":65054},{\"end\":65075,\"start\":65067},{\"end\":65089,\"start\":65083},{\"end\":65101,\"start\":65096},{\"end\":65338,\"start\":65333},{\"end\":65511,\"start\":65506},{\"end\":65840,\"start\":65835},{\"end\":65859,\"start\":65848},{\"end\":66100,\"start\":66095},{\"end\":66114,\"start\":66108},{\"end\":66302,\"start\":66298},{\"end\":66436,\"start\":66428},{\"end\":66448,\"start\":66442},{\"end\":66457,\"start\":66450},{\"end\":66742,\"start\":66733},{\"end\":66750,\"start\":66744},{\"end\":67044,\"start\":67036},{\"end\":67058,\"start\":67052},{\"end\":67377,\"start\":67372},{\"end\":67573,\"start\":67568},{\"end\":67720,\"start\":67715},{\"end\":67734,\"start\":67728},{\"end\":67941,\"start\":67936},{\"end\":67956,\"start\":67948},{\"end\":68126,\"start\":68121},{\"end\":68226,\"start\":68220},{\"end\":68500,\"start\":68491}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":62967,\"start\":62907},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1569319},\"end\":63291,\"start\":62969},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7475027},\"end\":63699,\"start\":63293},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4474368},\"end\":63892,\"start\":63701},{\"attributes\":{\"id\":\"b4\"},\"end\":64225,\"start\":63894},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7352306},\"end\":64599,\"start\":64227},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":13275863},\"end\":64985,\"start\":64601},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2468323},\"end\":65301,\"start\":64987},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2565493},\"end\":65461,\"start\":65303},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":17452498},\"end\":65752,\"start\":65463},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2865011},\"end\":66045,\"start\":65754},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6104125},\"end\":66247,\"start\":66047},{\"attributes\":{\"id\":\"b12\"},\"end\":66363,\"start\":66249},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":42262266},\"end\":66601,\"start\":66365},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":16241328},\"end\":66985,\"start\":66603},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":17809969},\"end\":67313,\"start\":66987},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11831549},\"end\":67525,\"start\":67315},{\"attributes\":{\"id\":\"b17\"},\"end\":67656,\"start\":67527},{\"attributes\":{\"id\":\"b18\"},\"end\":67872,\"start\":67658},{\"attributes\":{\"id\":\"b19\"},\"end\":68100,\"start\":67874},{\"attributes\":{\"id\":\"b20\"},\"end\":68212,\"start\":68102},{\"attributes\":{\"id\":\"b21\"},\"end\":68289,\"start\":68214},{\"attributes\":{\"id\":\"b22\"},\"end\":69056,\"start\":68291},{\"attributes\":{\"id\":\"b23\"},\"end\":69769,\"start\":69058},{\"attributes\":{\"id\":\"b24\"},\"end\":70457,\"start\":69771},{\"attributes\":{\"id\":\"b25\"},\"end\":71749,\"start\":70459}]", "bib_title": "[{\"end\":62991,\"start\":62969},{\"end\":63334,\"start\":63293},{\"end\":63724,\"start\":63701},{\"end\":64282,\"start\":64227},{\"end\":64694,\"start\":64601},{\"end\":65044,\"start\":64987},{\"end\":65323,\"start\":65303},{\"end\":65500,\"start\":65463},{\"end\":65829,\"start\":65754},{\"end\":66086,\"start\":66047},{\"end\":66419,\"start\":66365},{\"end\":66725,\"start\":66603},{\"end\":67028,\"start\":66987},{\"end\":67368,\"start\":67315},{\"end\":67707,\"start\":67658},{\"end\":67927,\"start\":67874}]", "bib_author": "[{\"end\":63007,\"start\":62993},{\"end\":63021,\"start\":63007},{\"end\":63032,\"start\":63021},{\"end\":63046,\"start\":63032},{\"end\":63066,\"start\":63046},{\"end\":63079,\"start\":63066},{\"end\":63351,\"start\":63336},{\"end\":63364,\"start\":63351},{\"end\":63377,\"start\":63364},{\"end\":63389,\"start\":63377},{\"end\":63407,\"start\":63389},{\"end\":63420,\"start\":63407},{\"end\":63431,\"start\":63420},{\"end\":63741,\"start\":63726},{\"end\":63759,\"start\":63741},{\"end\":64018,\"start\":63996},{\"end\":64303,\"start\":64284},{\"end\":64317,\"start\":64303},{\"end\":64713,\"start\":64696},{\"end\":64729,\"start\":64713},{\"end\":64742,\"start\":64729},{\"end\":64756,\"start\":64742},{\"end\":65061,\"start\":65046},{\"end\":65077,\"start\":65061},{\"end\":65091,\"start\":65077},{\"end\":65103,\"start\":65091},{\"end\":65340,\"start\":65325},{\"end\":65513,\"start\":65502},{\"end\":65842,\"start\":65831},{\"end\":65861,\"start\":65842},{\"end\":66102,\"start\":66088},{\"end\":66116,\"start\":66102},{\"end\":66289,\"start\":66285},{\"end\":66304,\"start\":66289},{\"end\":66438,\"start\":66421},{\"end\":66450,\"start\":66438},{\"end\":66459,\"start\":66450},{\"end\":66744,\"start\":66727},{\"end\":66752,\"start\":66744},{\"end\":67046,\"start\":67030},{\"end\":67060,\"start\":67046},{\"end\":67379,\"start\":67370},{\"end\":67575,\"start\":67563},{\"end\":67722,\"start\":67709},{\"end\":67736,\"start\":67722},{\"end\":67943,\"start\":67929},{\"end\":67958,\"start\":67943},{\"end\":68114,\"start\":68110},{\"end\":68128,\"start\":68114},{\"end\":68228,\"start\":68216},{\"end\":68502,\"start\":68491}]", "bib_venue": "[{\"end\":62912,\"start\":62907},{\"end\":63100,\"start\":63079},{\"end\":63467,\"start\":63431},{\"end\":63774,\"start\":63759},{\"end\":63994,\"start\":63894},{\"end\":64367,\"start\":64317},{\"end\":64777,\"start\":64756},{\"end\":65116,\"start\":65103},{\"end\":65356,\"start\":65340},{\"end\":65563,\"start\":65513},{\"end\":65876,\"start\":65861},{\"end\":66131,\"start\":66116},{\"end\":66283,\"start\":66249},{\"end\":66471,\"start\":66459},{\"end\":66777,\"start\":66752},{\"end\":67109,\"start\":67060},{\"end\":67407,\"start\":67379},{\"end\":67561,\"start\":67527},{\"end\":67759,\"start\":67736},{\"end\":67981,\"start\":67958},{\"end\":68108,\"start\":68102},{\"end\":68489,\"start\":68291},{\"end\":69283,\"start\":69058},{\"end\":70087,\"start\":69771},{\"end\":70812,\"start\":70459},{\"end\":63785,\"start\":63776},{\"end\":64413,\"start\":64369},{\"end\":65125,\"start\":65118},{\"end\":65368,\"start\":65358},{\"end\":65609,\"start\":65565},{\"end\":65887,\"start\":65878},{\"end\":66142,\"start\":66133},{\"end\":67154,\"start\":67111}]"}}}, "year": 2023, "month": 12, "day": 17}