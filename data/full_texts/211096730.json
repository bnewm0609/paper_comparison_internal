{"id": 211096730, "updated": "2023-10-06 18:46:25.722", "metadata": {"title": "A Simple Framework for Contrastive Learning of Visual Representations", "authors": "[{\"first\":\"Ting\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Simon\",\"last\":\"Kornblith\",\"middle\":[]},{\"first\":\"Mohammad\",\"last\":\"Norouzi\",\"middle\":[]},{\"first\":\"Geoffrey\",\"last\":\"Hinton\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 2, "day": 13}, "abstract": "This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2002.05709", "mag": "3034978746", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/ChenK0H20", "doi": null}}, "content": {"source": {"pdf_hash": "7af72a461ed7cda180e7eab878efd5f35d79bbf4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2002.05709v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9a23e3c33b9f59fe38e65f56b7aea73e32dbe539", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7af72a461ed7cda180e7eab878efd5f35d79bbf4.txt", "contents": "\nA Simple Framework for Contrastive Learning of Visual Representations\n\n\nTing Chen \nSimon Kornblith \nMohammad Norouzi \nGeoffrey Hinton \nA Simple Framework for Contrastive Learning of Visual Representations\n\nThis paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive selfsupervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by Sim-CLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-ofthe-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100\u00d7 fewer labels.\n\nIntroduction\n\nLearning effective visual representations without human supervision is a long-standing problem. Most mainstream approaches fall into one of two classes: generative or discriminative. Generative approaches learn to generate or otherwise model pixels in the input space (Hinton et al., 2006;Kingma & Welling, 2013;Goodfellow et al., 2014). However, pixel-level generation is computationally expensive and may not be necessary for representation learning. tive functions similar to those used for supervised learning, but train networks to perform pretext tasks where both the inputs and labels are derived from an unlabeled dataset. Many such approaches have relied on heuristics to design pretext tasks (Doersch et al., 2015;Zhang et al., 2016;Noroozi & Favaro, 2016;Gidaris et al., 2018), which could limit the generality of the learned representations. Discriminative approaches based on contrastive learning in the latent space have recently shown great promise, achieving state-of-theart results (Hadsell et al., 2006;Dosovitskiy et al., 2014;Oord et al., 2018;Bachman et al., 2019).\n\nIn this work, we introduce a simple framework for contrastive learning of visual representations, which we call SimCLR. Not only does SimCLR outperform previous work ( Figure 1), but it is also simpler, requiring neither specialized architectures (Bachman et al., 2019;H\u00e9naff et al., 2019) nor a memory bank (Wu et al., 2018;Tian et al., 2019;He et al., 2019a;Misra & van der Maaten, 2019).\n\nIn order to understand what enables good contrastive representation learning, we systematically study the major components of our framework and show that:\n\n\u2022 Composition of multiple data augmentation operations is crucial in defining the contrastive prediction tasks that yield effective representations. In addition, unsupervised arXiv:2002.05709v1 [cs.LG] 13 Feb 2020 contrastive learning benefits from stronger data augmentation than supervised learning.\n\n\u2022 Introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations.\n\n\u2022 Representation learning with contrastive cross entropy loss benefits from normalized embeddings and an appropriately adjusted temperature parameter.\n\n\u2022 Contrastive learning benefits from larger batch sizes and longer training compared to its supervised counterpart. Like supervised learning, contrastive learning benefits from deeper and wider networks.\n\nWe combine these findings to achieve a new state-of-the-art in self-supervised and semi-supervised learning on Ima-geNet ILSVRC-2012 (Russakovsky et al., 2015). Under the linear evaluation protocol, SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art (H\u00e9naff et al., 2019). When fine-tuned with only 1% of the ImageNet labels, SimCLR achieves 85.8% top-5 accuracy, a relative improvement of 10% (H\u00e9naff et al., 2019). When fine-tuned on other natural image classification datasets, SimCLR performs on par with or better than a strong supervised baseline (Kornblith et al., 2019) on 10 out of 12 datasets.\n\n\nMethod\n\n\nThe Contrastive Learning Framework\n\nInspired by recent contrastive learning algorithms (see Section 7 for an overview), SimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space. As illustrated in Figure 2, this framework comprises the following four major components.\n\n\u2022 A stochastic data augmentation module that transforms any given data example randomly resulting in two correlated views of the same example, denotedx i andx j , which we consider as a positive pair. In this work, we sequentially apply three simple augmentations: random cropping followed by resize back to the original size, random color distortions, and random Gaussian blur. As shown in Section 3, the combination of random crop and color distortion is crucial to achieve a good performance.\n\n\u2022 A neural network base encoder f (\u00b7) that extracts representation vectors from augmented data examples. Our framework allows various choices of the network architecture without any constraints. We opt for simplicity and adopt the commonly used ResNet (He et al., 2016) to obtain h i = f (x i ) = ResNet(x i ) where h i \u2208 R d is the output after the average pooling layer.\n\n\u2022 A small neural network projection head g(\u00b7) that maps\n\u2190\u2212 Representation \u2212\u2192 x xixj hi hj zi zj t \u223c T t \u223c T f (\u00b7) f (\u00b7) g(\u00b7) g(\u00b7)\nMaximize agreement Figure 2. A simple framework for contrastive learning of visual representations. Two separate data augmentation operators are sampled from the same family of augmentations (t \u223c T and t \u223c T ) and applied to each data example to obtain two correlated views. A base encoder network f (\u00b7) and a projection head g(\u00b7) are trained to maximize agreement using a contrastive loss. After training is completed, we through away the projection head g(\u00b7)\n\nand use encoder f (\u00b7) and representation h for downstream tasks.\n\nrepresentations to the space where contrastive loss is applied. We use a MLP with one hidden layer to obtain\nz i = g(h i ) = W (2) \u03c3(W (1) h i )\nwhere \u03c3 is a ReLU nonlinearity. As shown in section 4, we find it beneficial to define the contrastive loss on z i 's rather than h i 's.\n\n\u2022 A contrastive loss function defined for a contrastive prediction task. Given a set {x k } including a positive pair of examplesx i andx j , the contrastive prediction task aims to identifyx j in {x k } k =i for a givenx i .\n\nWe randomly sample a minibatch of N examples and define the contrastive prediction task on pairs of augmented examples derived from the minibatch, resulting in 2N data points. We do not sample negative examples explicitly. Instead, given a positive pair, similar to (Chen et al., 2017), we treat the other 2(N \u2212 1) augmented examples within a minibatch as negative examples. Let sim(u, v) = u v/ u v denote the cosine similarity between two vectors u and v. Then the loss function for a positive pair of examples (i, j) is defined as\ni,j = \u2212 log exp(sim(z i , z j )/\u03c4 ) 2N k=1 1 [k =i] exp(sim(z i , z k )/\u03c4 ) ,(1)\nwhere 1 [k =i] \u2208 {0, 1} is an indicator function evaluating to 1 iff k = i and \u03c4 denotes a temperature parameter. The final loss is computed across all positive pairs, both (i, j) and (j, i), in a mini-batch. This loss has been used in previous work (Sohn, 2016;Wu et al., 2018;Oord et al., 2018); for convenience, we term it NT-Xent (the normalized temperature-scaled cross entropy loss).\n\nAlgorithm 1 summarizes the proposed method.\n\nAlgorithm 1 SimCLR's main learning algorithm.\n\ninput: batch size N , temperature \u03c4 , structure of f , g, T .\n\nfor sampled minibatch {x k } N k=1 do for all k \u2208 {1, . . . , N } do draw two augmentation functions t \u223c T , t \u223c T # the first augmentatio\u00f1\nx 2k\u22121 = t(x k ) h 2k\u22121 = f (x 2k\u22121 ) # representation z 2k\u22121 = g(h 2k\u22121 ) # projection # the second augmentatio\u00f1 x 2k = t (x k ) h 2k = f (x 2k ) # representation z 2k = g(h 2k ) # projection end for for all i \u2208 {1, . . . , 2N } and j \u2208 {1, . . . , 2N } do s i,j = z i z j /(\u03c4 z i z j ) # pairwise similarity end for define (i, j) as (i, j) = \u2212 log exp(si,j ) 2N k=1 1 [k =i] exp(s i,k ) L = 1 2N N k=1 [ (2k\u22121, 2k) + (2k, 2k\u22121)]\nupdate networks f and g to minimize L end for return encoder network f\n\n\nTraining with Large Batch Size\n\nWe do not train the model with a memory bank (Wu et al., 2018). Instead, we vary the training batch size N from 256 to 8192. A batch size of 8192 gives us 16382 negative examples per positive pair from both augmentation views. Training with large batch size may be unstable when using standard SGD/Momentum with linear learning rate scaling (Goyal et al., 2017). To stabilize the training, we use the LARS optimizer (You et al., 2017) for all batch sizes. We train our model with Cloud TPUs, using 32 to 128 cores depending on the batch size. 1 Global BN. Standard ResNets use batch normalization (Ioffe & Szegedy, 2015). In distributed training with data parallelism, the BN mean and variance are typically aggregated locally per device. In our contrastive learning, as positive pairs are computed in the same device, the model can exploit the local information leakage to improve prediction accuracy without improving representations. We address this issue by aggregating BN mean and variance over all devices during the training. Other approaches include shuffling data examples (He et al., 2019a), or replacing BN with layer norm (H\u00e9naff et al., 2019).\n\n\nEvaluation Protocol\n\nHere we lay out the protocol for our empirical studies, which aim to understand different design choices in our framework. 1 With 128 TPU v3 cores, it takes \u223c1.5 hours to train our ResNet-50 with a batch size of 4096 for 100 epochs.   Dataset and Metrics. Most of our study for unsupervised pretraining (learning encoder network f without labels) is done using the ImageNet ILSVRC-2012 dataset (Russakovsky et al., 2015). Some additional pretraining experiments on CIFAR-10 (Krizhevsky & Hinton, 2009) can be found in Appendix B.7. We also test the pretrained results on a wide range of datasets for transfer learning. To evaluate the learned representations, we follow the widely used linear evaluation protocol Oord et al., 2018;Bachman et al., 2019;Kolesnikov et al., 2019), where a linear classifier is trained on top of the frozen base network, and test accuracy is used as a proxy for representation quality. Beyond linear evaluation, we also compare against state-of-the-art on semi-supervised and transfer learning.\n\nDefault setting. Unless otherwise specified, for data augmentation we use random crop and resize (with random flip), color distortions, and Gaussian blur (for details, see Appendix A). We use ResNet-50 as the base encoder network, and a 2-layer MLP projection head to project the representation to a 128-dimensional latent space. As the loss, we use NT-Xent, optimized using LARS with linear learning rate scaling (i.e. LearningRate = 0.3 \u00d7 BatchSize/256) and weight decay of 10 \u22126 . We train at batch size 4096 for 100 epochs. 2 Furthermore, we use linear warmup for the first 10 epochs, and decay the learning rate with the cosine decay schedule without restarts (Loshchilov & Hutter, 2016).\n\n\nData Augmentation for Contrastive Representation Learning\n\nData augmentation defines predictive tasks. While data augmentation has been widely used in both supervised and unsupervised representation learning (Krizhevsky et al., 2012;H\u00e9naff et al., 2019;Bachman et al., 2019), it has not been considered as a systematic way to define the contrastive prediction task. Many existing approaches define contrastive prediction tasks by changing the architecture.  prediction via a fixed image splitting procedure and a context aggregation network. We show that this complexity can be avoided by performing simple random cropping (with resizing) of target images, which creates a family of predictive tasks subsuming the above mentioned two, as shown in Figure 3. This simple design choice conveniently decouples the predictive task from other components such as the neural network architecture. Broader contrastive prediction tasks can be defined by extending the family of augmentations and composing them stochastically.\n\n\nComposition of data augmentation operations is crucial for learning good representations\n\nTo systematically study the impact of data augmentation, we consider several common augmentations here. One type of augmentation involves spatial/geometric transformation of data, such as cropping and resizing (with horizontal flipping), rotation (Gidaris et al., 2018) and cutout (De-Vries & Taylor, 2017). The other type of augmentation involves appearance transformation, such as color distortion (including color dropping, brightness, contrast, saturation, hue) (Howard, 2013;Szegedy et al., 2015), Gaussian blur, and Sobel filtering. Figure 4 visualizes the augmentations that we study in this work.\n\nTo understand the effects of individual data augmentations and the importance of augmentation composition, we investigate the performance of our framework when applying  Figure 5. Linear evaluation (ImageNet top-1 accuracy) under individual or composition of data augmentations, applied only to one branch. For all columns but the last, diagonal entries correspond to single transformation, and off-diagonals correspond to composition of two transformations (applied sequentially). The last column reflects the average over the row.\n\naugmentations individually or in pairs. Since ImageNet images are of different sizes, we always apply crop and resize images (Krizhevsky et al., 2012;Szegedy et al., 2015), which makes it difficult to study other augmentations in the absence of cropping. To eliminate this confound, we consider an asymmetric data transformation setting for this ablation. Specifically, we always first randomly crop images and resize them to the same resolution, and we then apply the targeted transformation(s) only to one branch of the framework in Figure 2, while leaving the other branch as the identity (i.e. t(x i ) = x i ). Note that this asymmetric data augmentation hurts the performance. Nonetheless, this setup should not substantively change the impact of individual data augmentations or their compositions.    Figure 5 shows linear evaluation results under individual and composition of transformations. We observe that no single transformation suffices to learn good representations, even though the model can almost perfectly identify the positive pairs in the contrastive task. When composing augmentations, the contrastive prediction task becomes harder, but the quality of representation improves dramatically.\n\nOne composition of augmentations stands out: random cropping and random color distortion. We conjecture that one serious issue when using only random cropping as data augmentation is that most patches from an image share a similar color distribution. Figure 6 shows that color histograms alone suffice to distinguish images. Neural nets may exploit this shortcut to solve the predictive task. Therefore, it is critical to compose cropping with color distortion in order to learn generalizable features.\n\n\nContrastive learning needs stronger data augmentation than supervised learning\n\nTo further demonstrate the importance of the color augmentation, we adjust the strength of color augmentation as shown in Table 1. Stronger color augmentation substantially improves the linear evaluation of the learned unsupervised models. In this context, AutoAugment (Cubuk et al., 2019), a sophisticated augmentation policy found using supervised learning, does not work better than simple cropping + (stronger) color distortion. When training su-  pervised models with the same set of augmentations, we observe that stronger color augmentation does not improve or even hurts their performance. Thus, our experiments show that unsupervised contrastive learning benefits from stronger (color) data augmentation than supervised learning.\nR101 R101(2x) R152 R152(2x) R18 R18(2x) R18(4x) R34 R34(2x) R34(4x) R50 R50(2x) R50(4x) Sup. R50 Sup. R50(2x) Sup. R50(4x) R50* R50(2x)* R50(4x)*\nAlthough previous work has reported that data augmentation is useful for self-supervised learning (Doersch et al., 2015;Bachman et al., 2019;H\u00e9naff et al., 2019), we show that data augmentation that does not yield accuracy benefits for supervised learning can still help considerably with contrastive learning.\n\n\nArchitectures for Encoder and Head\n\n4.1. Unsupervised contrastive learning benefits (more) from bigger models Figure 7 shows, perhaps unsurprisingly, that increasing depth and width both improve performance. While similar findings hold for supervised learning (He et al., 2016), we find the gap between supervised models and linear classifiers trained on unsupervised models shrinks as the model size increases, suggesting that unsupervised learning benefits more from bigger models than its supervised counterpart.\n\n\nA nonlinear projection head improves the representation quality of the layer before it\n\nWe then study the importance of including a projection head, i.e. g(h). Figure 8 shows linear evaluation results using three different architecture for the head: (1) identity Name Negative loss function Gradient w.r.t. u   mapping;\n\n\nNT-Xent\nu T v + /\u03c4 \u2212 log v\u2208{v + ,v \u2212 } exp(u T v/\u03c4 ) (1 \u2212 exp(u T v + /\u03c4 ) Z(u) )/\u03c4 v + \u2212 v\u2208{v + ,v \u2212 } exp(u T v/\u03c4 ) Z(u) /\u03c4 v NT-Logistic log \u03c3(u T v + /\u03c4 ) + log \u03c3(\u2212u T v \u2212 /\u03c4 ) (\u03c3(\u2212u T v + /\u03c4 ))/\u03c4 v + \u2212 \u03c3(u T v \u2212 /\u03c4 )/\u03c4 v \u2212 Margin Triplet \u2212 max(u T v \u2212 \u2212 u T v + + m, 0) v + \u2212 v \u2212 if u T v + \u2212 u T v \u2212 < m else 0\n(2) linear projection, as used by several previous approaches (Wu et al., 2018); and (3) the default nonlinear projection with one additional hidden layer (and ReLU activation), similar to Bachman et al. (2019). We observe that a nonlinear projection is better than a linear projection (+3%), and much better than no projection (>10%). When a projection head is used, similar results are observed regardless of output dimension. Furthermore, even when nonlinear projection is used, the layer before the projection head, h, is still much better (>10%) than the layer after, z = g(h), which shows that the hidden layer before the projection head is a better representation than the layer after.\n\nWe conjecture that the importance of using the representation before the nonlinear projection is due to loss of information induced by the contrastive loss. In particular, z = g(h) is trained to be invariant to data transformation. Thus, g can remove information that may be useful for the downstream task, such as the color or orientation of objects. By leveraging the nonlinear transformation g(\u00b7), more information can be formed and maintained in h. To verify this hypothesis, we conduct experiments that use either h or g(h) to learn to predict the transformation applied during the pretraining.\n\nHere we set g(h) = W (2) \u03c3(W (1) h), with the same input and output dimensionality (i.e. 2048). Table 3 shows h contains much more information about the transformation applied, while g(h) loses information.\n\n\nLoss Functions and Batch Size\n\n\nNormalized cross entropy loss with adjustable temperature works better than alternatives\n\nWe compare the NT-Xent loss against other commonly used contrastive loss functions, such as logistic loss (Mikolov et al., 2013), and margin loss (Schroff et al., 2015). Table 2 shows the objective function as well as the gradient to the input of the loss function. Looking at the gradient, we observe 1) 2 normalization along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives; and 2) unlike cross-entropy, other objective functions do not weigh the negatives by their relative hardness. As a result, one must apply semi-hard negative mining (Schroff et al., 2015) for these loss functions: instead of computing the gradient over all loss terms, one can compute the gradient using semi-hard negative terms (i.e., those that are within the loss margin and closest in distance, but farther than positive examples).\n\nTo make the comparisons fair, we use the same 2 normalization for all loss functions, and we tune the hyperparameters, and report their best results. 7 Table 4 shows that, while (semi-hard) negative mining helps, the best result is still much worse than our default NT-Xent loss.\n\nWe next test the importance of the 2 normalization and temperature \u03c4 in our default NT-Xent loss. Table 5 shows that without normalization and proper temperature scaling, performance is significantly worse. Without 2 normalization, the contrastive task accuracy is higher, but the resulting representation is worse under linear evaluation.\n\n\n5.2.\n\nContrastive learning benefits (more) from larger batch sizes and longer training Figure 9 shows the impact of batch size when models are trained for different numbers of epochs. We find that, when the number of training epochs is small (e.g. 100 epochs), larger batch sizes have a significant advantage over the smaller ones. With more training steps/epochs, the gaps between different batch sizes decrease or disappear, provided the batches are randomly resampled. In contrast to supervised learning (Goyal et al., 2017) Table 7. ImageNet accuracy of models trained with few labels.\n\nwidths (width multipliers of 1\u00d7, 2\u00d7, and 4\u00d7). For better convergence, our models here are trained for 1000 epochs.\n\nLinear evaluation. Semi-supervised learning. We follow Zhai et al. (2019) and sample 1% or 10% of the labeled ILSVRC-12 training datasets in a class-balanced way (i.e. around 12.8 and 128 images per class respectively). We simply fine-tune the whole base network on the labeled data without regulariza- Transfer learning. We evaluate transfer learning performance across 12 natural image datasets in both linear evaluation (fixed feature extractor) and fine-tuning settings. Following Kornblith et al. (2019), we perform hyperparameter tuning for each model-dataset combination and select the best hyperparameters on a validation set. Table 8 shows results with the ResNet-50 (4\u00d7) model. When fine-tuned, our self-supervised model significantly outperforms the supervised baseline on 5 datasets, whereas the supervised baseline is superior on only 2 (i.e. Pets and Flowers). On the remaining 5 datasets, the models are statistically tied. Full experimental details as well as results with the standard ResNet-50 architecture are provided in Appendix B.6.\n\nWe note that the superiority of our framework relative to previous work is not explained by any single design choice, but by their composition. We provide a comprehensive comparison of our design choices with those of previous work in Appendix C.\n\n\nRelated Work\n\nThe idea of making representations of an image agree with each other under small transformations dates back to Becker & Hinton (1992). We extend this idea by leveraging recent advances in data augmentation, network architecture and contrastive losses. A similar consistency idea has been explored in other contexts such as semi-supervised learning (Xie et al., 2019;Berthelot et al., 2019).\n\nHandcrafted pretext tasks. The recent renaissance of selfsupervised learning began with artificially designed pretext tasks, such as relative patch prediction (Doersch et al., 2015), solving jigsaw puzzles (Noroozi & Favaro, 2016), colorization  and rotation prediction (Gidaris et al., 2018). Although good results can be obtained with bigger networks and longer training , these pretext tasks rely on somewhat ad-hoc heuristics, which limits the generality of learned representations.\n\nContrastive visual representation learning. Dating back to Hadsell et al. (2006), these approaches learn representations by contrasting positive pairs against negative pairs. Along these lines, Dosovitskiy et al. (2014) proposes to treat each instance as a class represented by a feature vector (in a parametric form). Wu et al. (2018) proposes to use a memory bank to store the instance class representation vector, an approach adopted and extended in several recent papers . However, it is not clear if the success of contrastive approaches is determined by the mutual information, or by the specific form of the contrastive loss (Tschannen et al., 2019). Further comparison of our method to related methods are in Appendix C.\n\n\nConclusion\n\nIn this work, we present a simple framework and its instantiation for contrastive visual representation learning. We carefully study its components, and show the effects of different design choices. By combining our findings, we improve considerably over previous methods for selfsupervised, semi-supervised, and transfer learning.\n\nOur results show that the complexity of some previous methods for self-supervised learning is not necessary to achieve good performance. Our approach differs from standard supervised learning on ImageNet only in the choice of data augmentation, the use of a nonlinear head at the end of the network, and the loss function. The strength of this simple framework suggests that, despite a recent surge in interest, self-supervised learning remains undervalued.\n\n\nA. Data Augmentation Details\n\nIn our default pre-training setting (which is used to train our best models), we utilize random crop (with resize and random flip), random color distortion, and random Gaussian blur as the data augmentations. The details of these three augmentations are provided below.\n\nRandom crop and resize to 224x224 We use standard Inception-style random cropping . The crop of random size (uniform from 0.08 to 1.0 in area) of the original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop is finally resized to the original size. This has been implemented in Tensorflow as \"slim.preprocessing.inception_preprocessing.distorted_bounding_box_crop\", or in Pytorch as \"torchvision.transforms.RandomResizedCrop\". Additionally, the random crop (with resize) is always followed by a random horizontal/left-to-right flip with 50% probability. This is helpful but not essential. By removing this from our default augmentation policy, the top-1 linear evaluation drops from 64.5% to 63.4% for our ResNet-50 model trained in 100 epochs.\n\nColor distortion Color distortion is composed by color jittering and color dropping. We find stronger color jittering usually helps, so we set a strength parameter.\n\nA pseudo-code for color distortion using TensorFlow is as follows. Gaussian blur This augmentation is in our default policy. We find it helpful, as it improves our ResNet-50 trained for 100 epochs from 63.2% to 64.5%. We blur the image 50% of the time using a Gaussian kernel. We randomly sample \u03c3 \u2208 [0.1, 2.0], and the kernel size is set to be 10% of the image height/width.\n\n\nB. Additional Experimental Results\n\n\nB.1. Effects of Longer Training for Supervised Models\n\nHere we perform experiments to see how training steps and stronger data augmentation affect supervised training. We test ResNet-50 and ResNet-50 (4\u00d7) under the same set of data augmentations (random crops, color distortion, 50% Gaussian blur) as used in our unsupervised models. Figure B.1 shows the top-1 accuracy. We observe that there is no significant benefit from training supervised models longer on ImageNet. Stronger data augmentation slightly improves the accuracy of ResNet-50 (4\u00d7) but does not help on ResNet-50. When stronger data augmentation is applied, ResNet-50 generally requires longer training (e.g. 500 epochs) to obtain the optimal result, while ResNet-50 (4\u00d7) does not benefit from longer training.       \n\n\nB.2. Understanding The Non-Linear Projection Head\n\n\nB.3. Tuning For Other Loss Functions\n\nThe learning rate that works best for NT-Xent loss may not be a good learning rate for other loss functions. To ensure a fair comparison, we also tune hyperparameters for both margin loss and logistic loss. Specifically, we tune learning rate in {0.01, 0.1, 0.3, 0.5, 1.0} for both loss functions. We further tune the margin in {0, 0.4, 0.8, 1.6} for margin loss, the temperature in {0.1, 0.2, 0.5, 1.0} for logistic loss.\n\n\nB.4. Batch Size and Training\n\nSteps Figure B.3 shows the top-5 accuracy on linear evaluation when trained with different batch sizes and training epochs. The conclusion is very similar to top-1 accuracy shown before, except that the differences between different batch sizes and training steps seems slightly smaller here.\n\n\nB.5. Semi-supervised Learning\n\nFine-tuning Procedure We fine-tune using the Nesterov momentum optimizer with a batch size of 4096, momentum of 0.9, and a learning rate of 0.8 (following LearningRate = 0.05 \u00d7 BatchSize/256) without warmup. Only random cropping (with random left-to-right flipping and resizing to 224x224) is used for preprocessing. We do not use any regularization (including weight decay). For 1% labeled data we fine-tune for 60 epochs, and for 10% Labeled data we fine-tune for 30 epochs. For the inference, we resize the given image to 256x256, and take a single center crop of 224x224. Table B.4 shows the comparisons of top-1 accuracy for different methods for semi-supervised learning. Our models significantly improve state-of-the-art.  Table 7 for top-5 accuracy.\n\n\nB.6. Transfer Learning\n\nWe evaluated the performance of our self-supervised representation for transfer learning in two settings: linear evaluation, where a logistic regression classifier is trained to classify a new dataset based on the self-supervised representation learned on ImageNet, and fine-tuning, where we allow all weights to vary during training. In both cases, we follow the approach described by Kornblith et al. (2019), although our preprocessing differs slightly.\n\nB.6.1. METHODS\n\n\nDatasets\n\nWe investigated transfer learning performance on the Food-101 dataset (Bossard et al., 2014), CIFAR-10 and CIFAR-100 (Krizhevsky & Hinton, 2009), Birdsnap (Berg et al., 2014), the SUN397 scene dataset (Xiao et al., 2010), Stanford Cars (Krause et al., 2013), FGVC Aircraft (Maji et al., 2013), the PASCAL VOC 2007 classification task (Everingham et al., 2010), the Describable Textures Dataset (DTD) (Cimpoi et al., 2014), Oxford-IIIT Pets (Parkhi et al., 2012), Caltech-101 (Fei-Fei et al., 2004, and Oxford 102 Flowers (Nilsback & Zisserman, 2008). We follow the evaluation protocols in the papers introducing these datasets, i.e., we report top-1 accuracy for Food-101, CIFAR-10, CIFAR-100, Birdsnap, SUN397, Stanford Cars, and DTD; mean per-class accuracy for FGVC Aircraft, Oxford-IIIT Pets, Caltech-101, and Oxford 102 Flowers; and the 11-point mAP metric as defined in Everingham et al. (2010) for PASCAL VOC 2007. For DTD and SUN397, the dataset creators defined multiple train/test splits; we report results only for the first split. Caltech-101 defines no train/test split, so we randomly chose 30 images per class and test on the remainder, for fair comparison with previous work (Donahue et al., 2014;Simonyan & Zisserman, 2014).\n\nWe used the validation sets specified by the dataset creators to select hyperparameters for FGVC Aircraft, PASCAL VOC 2007, DTD, and Oxford 102 Flowers. For other datasets, we held out a subset of the training set for validation while performing hyperparameter tuning. After selecting the optimal hyperparameters on the validation set, we retrained the model using the selected parameters using all training and validation images. We report accuracy on the test set.\n\n\nLinear Evaluation\n\nWe trained an 2 -regularized multinomial logistic regression classifier on features extracted from the frozen pretrained network. We used L-BFGS to optimize the softmax cross-entropy objective and we did not apply data augmentation. As preprocessing, all images were resized to 224 pixels along the shorter side using bicubic resampling, after which we took a 224 \u00d7 224 center crop. We selected the 2 regularization parameter from a range of 45 logarithmically spaced values between 10 \u22126 and 10 5 .\n\nFine-Tuning We fine-tuned the entire network using the weights of the pretrained network as initialization. We trained for 20,000 steps at a batch size of 256 using SGD with Nesterov momentum with a momentum parameter of 0.9. We set the momentum parameter for the batch normalization statistics to max(1 \u2212 10/s, 0.9) where s is the number of steps per epoch.\n\nAs data augmentation during fine-tuning, we performed only random crops with resize and flips; in contrast to pretraining, we did not perform color augmentation or blurring. At test time, we resized images to 256 pixels along the shorter side and took a 224 \u00d7 224 center crop. (Additional accuracy improvements may be possible with further optimization of data augmentation, particularly on the CIFAR-10 and CIFAR-100 datasets.) We selected the learning rate and weight decay, with a grid of 7 logarithmically spaced learning rates between 0.0001 and 0.1 and 7 logarithmically spaced values of weight decay between 10 \u22126 and 10 \u22123 , as well as no weight decay. We divide these values of weight decay by the learning rate.\n\nTraining from Random Initialization We trained the network from random initialization using the same procedure as for fine-tuning, but for longer, and with an altered hyperparameter grid. We chose hyperparameters from a grid of 7 logarithmically spaced learning rates between 0.001 and 1.0 and 8 logarithmically spaced values of weight decay between 10 \u22125 and 10 \u22121.5 .\n\nImportantly, our random initialization baselines are trained for 40,000 steps, which is sufficiently long to achieve nearmaximal accuracy. He et al. (2019a) primarily report fine-tuning results on datasets where fine-tuning accelerates training but provides no benefit over training longer from random initialization (He et al., 2019b). Thus, they train all models, including random initialization baselines, for relatively few epochs. By contrast, our fine-tuning experiments are intended to assess accuracy improvements due to pretraining, not merely acceleration of training. Figure 8 of Kornblith et al. (2019) indicates that 40,000 steps is sufficient for this purpose.\n\nOn Birdsnap, there are no statistically significant differences among methods, and on Food-101, Stanford Cars, and FGVC Aircraft datasets, fine-tuning provides only a small advantage over training from random initialization. However, on the remaining 8 datasets, pretraining has clear advantages.\n\n\nSupervised Baselines\n\nWe compare against architecturally identical ResNet models trained on ImageNet with standard cross-entropy loss. These models are trained with the same data augmentation as our self-supervised models (crops, strong color augmentation, and blur) and are also trained for 1000 epochs. We found that, although stronger data augmentation and longer training time do not benefit accuracy on ImageNet, these models performed significantly better than a supervised baseline trained for 90 epochs and ordinary data augmentation for linear evaluation on a subset of transfer datasets. The supervised ResNet-50 baseline achieves 76.3% top-1 accuracy on ImageNet, vs. 69.3% for the self-supervised counterpart, while the ResNet-50 (4\u00d7) baseline achieves 78.3%, vs. 76.5% for the self-supervised model.\n\n\nStatistical Significance Testing\n\nWe test for the significance of differences between model with a permutation test. Given predictions of two models, we generate 100,000 samples from the null distribution by randomly exchanging predictions for each example and computing the difference in accuracy after performing this randomization. We then compute the percentage of samples from the null distribution that are more extreme than the observed difference in predictions. For top-1 accuracy, this procedure yields the same result as the exact McNemar test. The assumption of exchangeability under the null hypothesis is also valid for mean per-class accuracy, but not when computing average precision curves. Thus, we perform significance testing for a difference in accuracy on VOC 2007 rather than a difference in mAP. A caveat of this procedure is that it does not consider run-to-run variability when training the models, only variability arising from using a finite sample of images for evaluation.  Table 8 of the text show no clear advantage to the supervised or self-supervised models.\n\nWith the narrower ResNet-50 architecture, however, supervised learning maintains a clear advantage over self-supervised learning. The supervised ResNet-50 model outperforms the self-supervised model on all datasets with linear evaluation, and most (10 of 12) datasets with fine-tuning. The weaker performance of the ResNet model compared to the ResNet (4\u00d7) model may relate to the accuracy gap between the supervised and self-supervised models on ImageNet. The self-supervised ResNet gets 69.3% top-1 accuracy, 6.8% worse than the supervised model in absolute terms, whereas the self-supervised ResNet (4\u00d7) model gets 76.5%, which is only 1.8% worse than the supervised model.\n\n\nB.7. CIFAR-10\n\nWhile we focus on using ImageNet as the main dataset for pre-training our unsupervised model, our method also works with other datasets. We demonstrate it by testing on CIFAR-10 as follows.\n\nSetup As our goal is not to optimize CIFAR-10 performance, but rather to provide further confirmation of our observations on ImageNet, we use the same architecture (ResNet-50) for CIFAR-10 experiments. Because CIFAR-10 images are much smaller than ImageNet images, we replace the first 7x7 Conv of stride 2 with 3x3 Conv of stride 1, and also remove the first max pooling operation. For data augmentation, we use the same Inception crop (flip and resize to 32x32) as ImageNet, 8 and color distortion (strength=0.5), leaving out Gaussian blur. We pretrain with learning rate in {0.5, 1.0, 1.5}, temperature in {0.1, 0.5, 1.0}, and batch size in {256, 512, 1024, 2048, 4096}. The rest of the settings (including optimizer, weight decay, etc.) are the same as our ImageNet training.\n\nOur best model trained with batch size 1024 can achieve a linear evaluation accuracy of 94.0%, compared to 95.1% from the supervised baseline using the same architecture and batch size. The best self-supervised model that reports linear evaluation result on CIFAR-10 is AMDIM (Bachman et al., 2019), which achieves 91.2% with a model 25\u00d7 larger than ours. We note that our model can be improved by incorporating extra data augmentations as well as using a more suitable base network.\n\nPerformance under different batch sizes and training steps Figure B.5 shows the linear evaluation performance under different batch sizes and training steps. The results are consistent with our observations on ImageNet, although the largest batch size of 4096 seems to cause a small degradation in performance on CIFAR-10.\n\nOptimal temperature under different batch sizes Figure B.6 shows the linear evaluation of model trained with three different temperatures under various batch sizes. We find that when training to convergence (e.g. training epochs > 300), the optimal temperature in {0.1, 0.5, 1.0} is 0.5 and seems consistent regardless of the batch sizes. However, the performance with \u03c4 = 0.1 improves as batch size increases, which may suggest a small shift of optimal temperature towards 0.1.\n\n\nC. Further Comparison to Related Methods\n\nHere we provide an in-depth comparison of our method to the recently proposed contrastive representation learning methods: \u2022 DIM/AMDIM (Hjelm et al., 2018;Bachman et al., 2019) achieve global-to-local/local-to-neighbor prediction by predicting the middle layer of ConvNet. The ConvNet is a ResNet that has bewen modified to place significant constraints on the receptive fields of the network (e.g. replacing many 3x3 Convs with 1x1 Convs). In our framework, we decouple the prediction task and encoder architecture, by random cropping (with resizing) and using the final representations of two augmented views for prediction, so we can use standard and more powerful ResNets. Our NT-Xent loss function leverages normalization and temperature to restrict the range of similarity scores, whereas they use a tanh function with regularization. We use a simpler data augmentation policy, while they use FastAutoAugment for their best result.\n\n\u2022 CPC v1 and v2 (Oord et al., 2018;H\u00e9naff et al., 2019) define the context prediction task using a deterministic strategy to split examples into patches, and a context aggregation network (a PixelCNN) to aggregate these patches. The base encoder network sees only patches, which are considerably smaller than the original image. We decouple the prediction task and the encoder architecture, so we do not require a context aggregation network, and our encoder can look at the images of wider spectrum of resolutions. In addition, we use the NT-Xent loss function, which leverages normalization and temperature, whereas they use an unnormalized cross-entropy-based objective. We use simpler data augmentation.\n\n\u2022 InstDisc, MoCo, PIRL (Wu et al., 2018;He et al., 2019a;Misra & van der Maaten, 2019) generalize the Exemplar approach originally proposed by Dosovitskiy et al. (2014) and leverage an explicit memory bank. We do not use a memory bank; we find that, with a larger batch size, in-batch negative example sampling suffices. We also utilize a nonlinear projection head, and use the representation before the projection head. Although we use similar types of augmentations (e.g., random crop and color distortion), we expect specific parameters may be different.\n\n\u2022 CMC (Tian et al., 2019) uses a separated network for each view, while we simply use a single network shared for all randomly augmented views. The data augmentation, projection head and loss function are also different. We use larger batch size instead of a memory bank.\n\n\u2022 Whereas Ye et al. (2019) maximize similarity between augmented and unaugmented copies of the same image, we apply data augmentation symmetrically to both branches of our framework (Figure 2). We also apply a nonlinear projection on the output of base feature network, and use the representation before projection network, whereas Ye et al. (2019) use the linearly projected final hidden vector as the representation. When training with large batch sizes using multiple accelerators, we use global BN to avoid shortcuts that can greatly decrease representation quality. Table C.1 provides a high-level comparison of the design choices of our method with those of previous methods. The superiority of our results over results obtained by previous methods is not due to any single design choice, but to their combination. Compared with previous work, our design choices are generally simpler.\n\nFigure 1 .\n1ImageNet top-1 accuracy of linear classifiers trained on representations learned with different self-supervised methods (pretrained on ImageNet). Gray cross indicates supervised ResNet-50. Our method, SimCLR, is shown in bold.\n\nFigure 3 .\n3Solid rectangles are images, dashed rectangles are random crops. By randomly cropping images, we sample contrastive prediction tasks that include global to local view (B \u2192 A) or adjacent view (D \u2192 C) prediction.\n\n\nCrop and resize (c) Crop, resize (and flip) (d) Color distort. (drop) (e) Color distort. (jitter) (f) Rotate {90 \u2022 , 180 \u2022 , 270 \u2022 }\n\nFigure 4 .\n4Illustrations of the studied data augmentation operators. Each augmentation can transform data stochastically with some internal parameters (e.g. rotation degree, noise level). Note that we only test these operators in ablation, the augmentation policy used to train our models only includes random crop (with flip and resize), color distortion, and Gaussian blur.(Original image cc-by: Von.grzanka) For example, Hjelm et al. (2018); Bachman et al. (2019) achieve global-to-local view prediction via constraining the receptive field in the network architecture, whereas Oord et al. (2018); H\u00e9naff et al. (2019) achieve neighboring view\n\nFigure 6 .\n6Histograms of pixel intensities (over all channels) for different crops of two different images (i.e. two rows). The image for the first row is fromFigure 4. All axes have the same range.\n\nFigure 7 .\n7Linear evaluation of models with varied depth and width. Models in blue dots are ours trained for 100 epochs, models in red stars are ours trained for 1000 epochs, and models in green crosses are supervised ResNets trained for 90 epochs 6(He et al., 2016).\n\nFigure 8 .\n8Linear evaluation of representations with different projection heads g(\u00b7) and various dimensions of z = g(h\n\nFigure 9 .\n9Linear evaluation models (ResNet-50) trained with different batch size and epochs. Each bar is a single run from scratch.\n\n(\nZhuang et al., 2019;Tian et al., 2019;He et al., 2019a;Misra & van der Maaten, 2019). Other work explores the use of in-batch samples for negative sampling instead of a memory bank(Ye et al., 2019;Ji et al., 2019).Recent literature has attempted to relate the success of their methods to maximization of mutual information between latent representations(Oord et al., 2018;H\u00e9naff et al., 2019;Hjelm et al., 2018;Bachman et al., 2019)\n\nFigure B. 1 .\n1Squared real eigenvalue distribution of linear projection matrix W \u2208 R 2048\u00d72048 used to compute g(h) = W h.\n\nFigure B. 2 .\n2t-SNE visualizations of hidden vectors of images from a randomly selected 10 classes in the validation set.\n\nFigure B. 1\n1shows the eigenvalue distribution of linear projection matrix W \u2208 R 2048\u00d72048 used to compute z = W h. This matrix has relatively few large eigenvalues, indicating that it is approximately low-rank.\n\nFigure B. 2\n2shows t-SNE(Maaten & Hinton, 2008) visualizations of h and z = g(h) for randomly selected 10 classes by our best ResNet-50 (top-1 linear evaluation 69.3%). Classes represented by h are better separated compared to z.\n\nFigure B. 3 .Figure B. 4 .\n34Linear evaluation (top-5) of ResNet-50 trained with different batch sizes and epochs. Each bar is a single run from scratch. SeeFigure 9for top-ImageNet top-1 accuracy of models trained with few labels. See\n\nFigure B. 6 .\n6Linear evaluation of the model (ResNet-50) trained with three temperatures on different batch sizes on CIFAR-10. Each bar is averaged over multiple runs with different learning rates and total train epochs. Error bar denotes standard deviation.\n\nTable 2 .\n2Negative loss functions and their gradients. All input vectors, i.e. u, v + , v \u2212 , are 2 normalized. NT-Xent is an abbreviation for \"Normalized Temperature-scaled Cross Entropy\". Different loss functions impose different weightings of positive and negative examples.\n\n\n). The representation h (before projection) is 2048-dimensional here.Table 3. Accuracy of training additional MLPs on different representations to predict the transformation applied. Other than crop and color augmentation, we additionally and independently add rotation (one of {0 \u2022 , 90 \u2022 , 180 \u2022 , 270 \u2022 }), Gaussian noise, and Sobel filtering transformation during the pretraining for the last three rows. Both h and g(h) are of the same dimensionality, i.e. 2048.What to predict? \nRandom guess \nRepresentation \nh \ng(h) \n\nColor vs grayscale \n80 \n99.3 \n97.4 \nRotation \n25 \n67.6 \n25.6 \nOrig. vs corrupted \n50 \n99.5 \n59.6 \nOrig. vs Sobel filtered \n50 \n96.6 \n56.3 \n\n\n\n\nTable 4. Linear evaluation (top-1) for models trained with different loss functions. \"sh\" means using semi-hard negative mining.Table 5. Linear evaluation for models trained with different choices of 2 norm and temperature \u03c4 for NT-Xent loss. The contrastive distribution is over 4096 examples.Margin NT-Logi. Margin (sh) NT-Logi.(sh) NT-Xent \n\n50.9 \n51.6 \n57.5 \n57.9 \n63.9 \n\n2 norm? \n\u03c4 \nEntropy Contrastive acc. Top 1 \n\nYes \n\n0.05 \n1.0 \n90.5 \n59.7 \n0.1 \n4.5 \n87.8 \n64.4 \n0.5 \n8.2 \n68.2 \n60.7 \n1 \n8.3 \n59.1 \n58.0 \n\nNo \n10 \n0.5 \n91.7 \n57.2 \n100 \n0.5 \n92.1 \n57.0 \n\n\n\nTable 6\n6compares our results with previ-\n\n\nTable B . 1 .\nB1Top-1 accuracy of supervised models trained longer under various data augmentation procedures (from the same set of data augmentations for contrastive learning).\n\n\nTable B.2. Comparison of transfer learning performance of our self-supervised approach with supervised baselines across 12 natural image datasets, using ImageNet-pretrained ResNet models. See alsoFigure 8for results with the ResNet (4\u00d7) architecture.Linear evaluation: \nSimCLR (ours) 68.4 \n90.6 \n71.6 \n37.4 \n58.8 \n50.3 50.3 \n80.5 \n74.5 83.6 \n90.3 \n91.2 \nSupervised \n72.3 \n93.6 \n78.3 \n53.7 \n61.9 \n66.7 61.0 \n82.8 \n74.9 91.5 \n94.5 \n94.7 \n\nFine-tuned: \nSimCLR (ours) 88.2 \n97.7 \n85.9 \n75.9 \n63.5 \n91.3 88.1 \n84.1 \n73.2 89.2 \n92.1 \n97.0 \nSupervised \n88.3 \n97.5 \n86.4 \n75.8 \n64.3 \n92.1 86.0 \n85.0 \n74.6 92.1 \n93.3 \n97.6 \nRandom init \n86.9 \n95.9 \n80.2 \n76.1 \n53.6 \n91.4 85.9 \n67.3 \n64.8 81.5 \n72.6 \n92.0 \n\nB.6.2. RESULTS WITH STANDARD RESNET \n\nThe ResNet-50 (4\u00d7) results shown in \nGoogle Research, Brain Team. Correspondence to: Ting Chen <iamtingchen@google.com>.\nAlthough max performance is not reached in 100 epochs, reasonable results are achieved, allowing fair and efficient ablations.\nSupervised models are trained for 90 epochs; longer training improves performance of stronger augmentation by \u223c 0.5%.\nTraining longer does not improve supervised ResNets (see Appendix B.1).\nDetails of tuning can be found in Appendix B.3. For simplicity, we only consider the negatives from one augmentation view.\nIt is worth noting that, although CIFAR-10 images are much smaller than ImageNet images and image size does not differ among examples, cropping with resizing is still a very effective augmentation for contrastive learning.\nAcknowledgementsWe would like to thank Xiaohua Zhai, Rafael M\u00fcller and Yani Ioannou for their feedback on the draft. We are also grateful for general support from Google Research teams in Toronto and elsewhere.ModelData\nLearning representations by maximizing mutual information across views. P Bachman, R D Hjelm, W Buchwalter, Advances in Neural Information Processing Systems. Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning rep- resentations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. 15509-15519, 2019.\n\nSelf-organizing neural network that discovers surfaces in random-dot stereograms. S Becker, G E Hinton, Nature. 3556356Becker, S. and Hinton, G. E. Self-organizing neural network that discovers surfaces in random-dot stereograms. Nature, 355 (6356):161-163, 1992.\n\nLarge-scale fine-grained visual categorization of birds. T Berg, J Liu, S W Lee, M L Alexander, D W Jacobs, P N Belhumeur, Birdsnap, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEBerg, T., Liu, J., Lee, S. W., Alexander, M. L., Jacobs, D. W., and Belhumeur, P. N. Birdsnap: Large-scale fine-grained visual categorization of birds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2019-2026. IEEE, 2014.\n\nA holistic approach to semisupervised learning. D Berthelot, N Carlini, I Goodfellow, N Papernot, A Oliver, C A Raffel, Mixmatch, Advances in Neural Information Processing Systems. Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and Raffel, C. A. Mixmatch: A holistic approach to semi- supervised learning. In Advances in Neural Information Pro- cessing Systems, pp. 5050-5060, 2019.\n\nFood-101-mining discriminative components with random forests. L Bossard, M Guillaumin, L Van Gool, European conference on computer vision. SpringerBossard, L., Guillaumin, M., and Van Gool, L. Food-101-mining discriminative components with random forests. In European conference on computer vision, pp. 446-461. Springer, 2014.\n\nOn sampling strategies for neural network-based collaborative filtering. T Chen, Y Sun, Y Shi, L Hong, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningChen, T., Sun, Y., Shi, Y., and Hong, L. On sampling strategies for neural network-based collaborative filtering. In Proceed- ings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 767-776, 2017.\n\nDescribing textures in the wild. M Cimpoi, S Maji, I Kokkinos, S Mohamed, A Vedaldi, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEECimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3606- 3613. IEEE, 2014.\n\nLearning augmentation strategies from data. E D Cubuk, B Zoph, D Mane, V Vasudevan, Q V Le, Autoaugment, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionCubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 113-123, 2019.\n\nImproved regularization of convolutional neural networks with cutout. T Devries, G W Taylor, arXiv:1708.04552arXiv preprintDeVries, T. and Taylor, G. W. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\n\nUnsupervised visual representation learning by context prediction. C Doersch, A Gupta, A A Efros, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionDoersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422-1430, 2015.\n\nLarge scale adversarial representation learning. J Donahue, K Simonyan, Advances in Neural Information Processing Systems. Donahue, J. and Simonyan, K. Large scale adversarial representa- tion learning. In Advances in Neural Information Processing Systems, pp. 10541-10551, 2019.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, Darrell , T , International Conference on Machine Learning. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, pp. 647-655, 2014.\n\nDiscriminative unsupervised feature learning with convolutional neural networks. A Dosovitskiy, J T Springenberg, M Riedmiller, T Brox, Advances in neural information processing systems. Dosovitskiy, A., Springenberg, J. T., Riedmiller, M., and Brox, T. Discriminative unsupervised feature learning with convolutional neural networks. In Advances in neural information processing systems, pp. 766-774, 2014.\n\nThe pascal visual object classes (voc) challenge. M Everingham, L Van Gool, C K Williams, J Winn, A Zisserman, International Journal of Computer Vision. 882Everingham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303-338, 2010.\n\nLearning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. L Fei-Fei, R Fergus, P Perona, IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop on Generative-Model Based Vision. Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop on Generative-Model Based Vision, 2004.\n\nUnsupervised representation learning by predicting image rotations. S Gidaris, P Singh, N Komodakis, arXiv:1803.07728arXiv preprintGidaris, S., Singh, P., and Komodakis, N. Unsupervised represen- tation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde- Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672-2680, 2014.\n\nP Goyal, P Doll\u00e1r, R Girshick, P Noordhuis, L Wesolowski, A Kyrola, A Tulloch, Y Jia, K He, Accurate, arXiv:1706.02677large minibatch sgd: Training imagenet in 1 hour. arXiv preprintGoyal, P., Doll\u00e1r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\nDimensionality reduction by learning an invariant mapping. R Hadsell, S Chopra, Y Lecun, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06). IEEE2Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer So- ciety Conference on Computer Vision and Pattern Recognition (CVPR'06), volume 2, pp. 1735-1742. IEEE, 2006.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\nMomentum contrast for unsupervised visual representation learning. K He, H Fan, Y Wu, S Xie, R Girshick, arXiv:1911.05722arXiv preprintHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019a.\n\nRethinking ImageNet pretraining. K He, R Girshick, P Doll\u00e1r, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionHe, K., Girshick, R., and Doll\u00e1r, P. Rethinking ImageNet pre- training. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4918-4927, 2019b.\n\nO J H\u00e9naff, A Razavi, C Doersch, S Eslami, A V Oord, arXiv:1905.09272Data-efficient image recognition with contrastive predictive coding. arXiv preprintH\u00e9naff, O. J., Razavi, A., Doersch, C., Eslami, S., and Oord, A. v. d. Data-efficient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019.\n\nA fast learning algorithm for deep belief nets. G E Hinton, S Osindero, Y.-W Teh, Neural computation. 187Hinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning al- gorithm for deep belief nets. Neural computation, 18(7):1527- 1554, 2006.\n\nLearning deep representations by mutual information estimation and maximization. R D Hjelm, A Fedorov, S Lavoie-Marchildon, K Grewal, P Bachman, A Trischler, Y Bengio, arXiv:1808.06670arXiv preprintHjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep repre- sentations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.\n\nSome improvements on deep convolutional neural network based image classification. A G Howard, arXiv:1312.5402arXiv preprintHoward, A. G. Some improvements on deep convolutional neural network based image classification. arXiv preprint arXiv:1312.5402, 2013.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, arXiv:1502.03167arXiv preprintIoffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n\nInvariant information clustering for unsupervised image classification and segmentation. X Ji, J F Henriques, A Vedaldi, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJi, X., Henriques, J. F., and Vedaldi, A. Invariant information clustering for unsupervised image classification and segmenta- tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 9865-9874, 2019.\n\nD P Kingma, M Welling, arXiv:1312.6114Auto-encoding variational bayes. arXiv preprintKingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\nRevisiting self-supervised visual representation learning. A Kolesnikov, X Zhai, L Beyer, Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. the IEEE conference on Computer Vision and Pattern RecognitionKolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-supervised visual representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1920-1929, 2019.\n\nDo better ImageNet models transfer better?. S Kornblith, J Shlens, Q V Le, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKornblith, S., Shlens, J., and Le, Q. V. Do better ImageNet models transfer better? In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2661-2671, 2019.\n\nCollecting a large-scale dataset of fine-grained cars. J Krause, J Deng, M Stark, L Fei-Fei, Second Workshop on Fine-Grained Visual Categorization. Krause, J., Deng, J., Stark, M., and Fei-Fei, L. Collecting a large-scale dataset of fine-grained cars. In Second Workshop on Fine-Grained Visual Categorization, 2013.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, University of TorontoTechnical reportKrizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. URL https://www.cs.toronto.edu/~kriz/ learning-features-2009-TR.pdf.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classifi- cation with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.\n\nI Loshchilov, F Hutter, Sgdr, arXiv:1608.03983Stochastic gradient descent with warm restarts. arXiv preprintLoshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.\n\nJournal of machine learning research. L V Maaten, G Hinton, 9Visualizing data using t-sneMaaten, L. v. d. and Hinton, G. Visualizing data using t-sne. Jour- nal of machine learning research, 9(Nov):2579-2605, 2008.\n\nFine-grained visual classification of aircraft. S Maji, J Kannala, E Rahtu, M Blaschko, A Vedaldi, Technical reportMaji, S., Kannala, J., Rahtu, E., Blaschko, M., and Vedaldi, A. Fine-grained visual classification of aircraft. Technical report, 2013.\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintMikolov, T., Chen, K., Corrado, G., and Dean, J. Efficient esti- mation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nSelf-supervised learning of pretext-invariant representations. I Misra, L Van Der Maaten, arXiv:1912.01991arXiv preprintMisra, I. and van der Maaten, L. Self-supervised learn- ing of pretext-invariant representations. arXiv preprint arXiv:1912.01991, 2019.\n\nAutomated flower classification over a large number of classes. M.-E Nilsback, A Zisserman, Computer Vision, Graphics & Image Processing. IEEEICVGIP'08. Sixth Indian Conference onNilsback, M.-E. and Zisserman, A. Automated flower classification over a large number of classes. In Computer Vision, Graphics & Image Processing, 2008. ICVGIP'08. Sixth Indian Conference on, pp. 722-729. IEEE, 2008.\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. M Noroozi, P Favaro, European Conference on Computer Vision. SpringerNoroozi, M. and Favaro, P. Unsupervised learning of visual repre- sentations by solving jigsaw puzzles. In European Conference on Computer Vision, pp. 69-84. Springer, 2016.\n\nA Oord, Y Li, O Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintOord, A. v. d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nCats and dogs. O M Parkhi, A Vedaldi, A Zisserman, Jawahar , C , IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEParkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3498-3505. IEEE, 2012.\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International journal of computer vision. 1153Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211-252, 2015.\n\nFacenet: A unified embedding for face recognition and clustering. F Schroff, D Kalenichenko, J Philbin, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSchroff, F., Kalenichenko, D., and Philbin, J. Facenet: A unified embedding for face recognition and clustering. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pp. 815-823, 2015.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintSimonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nImproved deep metric learning with multi-class n-pair loss objective. K Sohn, Advances in neural information processing systems. Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. In Advances in neural information processing systems, pp. 1857-1865, 2016.\n\nK Sohn, D Berthelot, C.-L Li, Z Zhang, N Carlini, E D Cubuk, A Kurakin, H Zhang, C Raffel, Fixmatch, arXiv:2001.07685Simplifying semi-supervised learning with consistency and confidence. arXiv preprintSohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C. Fixmatch: Simpli- fying semi-supervised learning with consistency and confidence. arXiv preprint arXiv:2001.07685, 2020.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.\n\nY Tian, D Krishnan, P Isola, arXiv:1906.05849Contrastive multiview coding. arXiv preprintTian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding. arXiv preprint arXiv:1906.05849, 2019.\n\nOn mutual information maximization for representation learning. M Tschannen, J Djolonga, P K Rubenstein, S Gelly, M Lucic, arXiv:1907.13625arXiv preprintTschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S., and Lu- cic, M. On mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019.\n\nUnsupervised feature learning via non-parametric instance discrimination. Z Wu, Y Xiong, S X Yu, Lin , D , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised feature learning via non-parametric instance discrimination. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3733-3742, 2018.\n\nSun database: Large-scale scene recognition from abbey to zoo. J Xiao, J Hays, K A Ehinger, A Oliva, A Torralba, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEEXiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun database: Large-scale scene recognition from abbey to zoo. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3485-3492. IEEE, 2010.\n\nUnsupervised data augmentation. Q Xie, Z Dai, E Hovy, M.-T Luong, Q V Le, arXiv:1904.12848arXiv preprintXie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. Unsu- pervised data augmentation. arXiv preprint arXiv:1904.12848, 2019.\n\nUnsupervised embedding learning via invariant and spreading instance feature. M Ye, X Zhang, P C Yuen, Chang , S.-F , Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYe, M., Zhang, X., Yuen, P. C., and Chang, S.-F. Unsupervised embedding learning via invariant and spreading instance feature. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6210-6219, 2019.\n\nLarge batch training of convolutional networks. Y You, I Gitman, B Ginsburg, arXiv:1708.03888arXiv preprintYou, Y., Gitman, I., and Ginsburg, B. Large batch training of con- volutional networks. arXiv preprint arXiv:1708.03888, 2017.\n\nS4l: Selfsupervised semi-supervised learning. X Zhai, A Oliver, A Kolesnikov, L Beyer, The IEEE International Conference on Computer Vision (ICCV). Zhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S4l: Self- supervised semi-supervised learning. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n\nColorful image colorization. R Zhang, P Isola, A A Efros, European conference on computer vision. SpringerZhang, R., Isola, P., and Efros, A. A. Colorful image coloriza- tion. In European conference on computer vision, pp. 649-666. Springer, 2016.\n\nLocal aggregation for unsupervised learning of visual embeddings. C Zhuang, A L Zhai, Yamins , D , Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZhuang, C., Zhai, A. L., and Yamins, D. Local aggregation for unsupervised learning of visual embeddings. In Proceedings of the IEEE International Conference on Computer Vision, pp. 6002-6012, 2019.\n", "annotations": {"author": "[{\"end\":83,\"start\":73},{\"end\":100,\"start\":84},{\"end\":118,\"start\":101},{\"end\":135,\"start\":119}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":78},{\"end\":99,\"start\":90},{\"end\":117,\"start\":110},{\"end\":134,\"start\":128}]", "author_first_name": "[{\"end\":77,\"start\":73},{\"end\":89,\"start\":84},{\"end\":109,\"start\":101},{\"end\":127,\"start\":119}]", "author_affiliation": null, "title": "[{\"end\":70,\"start\":1},{\"end\":205,\"start\":136}]", "venue": null, "abstract": "[{\"end\":1512,\"start\":207}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1817,\"start\":1796},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1840,\"start\":1817},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1863,\"start\":1840},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2252,\"start\":2230},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2271,\"start\":2252},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2294,\"start\":2271},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2315,\"start\":2294},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2549,\"start\":2527},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2574,\"start\":2549},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2592,\"start\":2574},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2613,\"start\":2592},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2885,\"start\":2863},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2905,\"start\":2885},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2941,\"start\":2924},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2959,\"start\":2941},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2976,\"start\":2959},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3005,\"start\":2976},{\"end\":3362,\"start\":3358},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4154,\"start\":4128},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4319,\"start\":4298},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4463,\"start\":4442},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4625,\"start\":4601},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5799,\"start\":5782},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7358,\"start\":7339},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7950,\"start\":7938},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7966,\"start\":7950},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7984,\"start\":7966},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8972,\"start\":8955},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9271,\"start\":9251},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9344,\"start\":9326},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9530,\"start\":9507},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10010,\"start\":9992},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10065,\"start\":10044},{\"end\":10214,\"start\":10213},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10510,\"start\":10484},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10591,\"start\":10564},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10821,\"start\":10803},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10842,\"start\":10821},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10866,\"start\":10842},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11807,\"start\":11780},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12044,\"start\":12019},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12064,\"start\":12044},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12085,\"start\":12064},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13189,\"start\":13167},{\"end\":13226,\"start\":13201},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13400,\"start\":13386},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13421,\"start\":13400},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14210,\"start\":14185},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14231,\"start\":14210},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16149,\"start\":16129},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16865,\"start\":16843},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16886,\"start\":16865},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16906,\"start\":16886},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17335,\"start\":17318},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":18294,\"start\":18277},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18425,\"start\":18404},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19969,\"start\":19947},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20009,\"start\":19987},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20488,\"start\":20466},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21888,\"start\":21868},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":22141,\"start\":22123},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23520,\"start\":23498},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":23753,\"start\":23735},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23776,\"start\":23753},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23960,\"start\":23938},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24009,\"start\":23985},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24071,\"start\":24049},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24347,\"start\":24326},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24486,\"start\":24461},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":24602,\"start\":24586},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":24923,\"start\":24899},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30335,\"start\":30312},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30502,\"start\":30480},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":30553,\"start\":30527},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30584,\"start\":30565},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":30630,\"start\":30611},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30667,\"start\":30646},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":30702,\"start\":30683},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30769,\"start\":30744},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30831,\"start\":30810},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30870,\"start\":30850},{\"end\":30906,\"start\":30870},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30959,\"start\":30931},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31310,\"start\":31286},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31623,\"start\":31601},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":31650,\"start\":31623},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":34252,\"start\":34235},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34431,\"start\":34413},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":34710,\"start\":34687},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38944,\"start\":38922},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":40133,\"start\":40113},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":40154,\"start\":40133},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":40952,\"start\":40933},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":40971,\"start\":40952},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":41666,\"start\":41649},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":41683,\"start\":41666},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":41712,\"start\":41683},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":41794,\"start\":41769},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":42210,\"start\":42191},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":42484,\"start\":42468},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":42806,\"start\":42790},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":45068,\"start\":45051},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":45349,\"start\":45329},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":45367,\"start\":45349},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":45384,\"start\":45367},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":45413,\"start\":45384},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":45526,\"start\":45509},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":45542,\"start\":45526},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":45701,\"start\":45682},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":45721,\"start\":45701},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":45740,\"start\":45721},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":45761,\"start\":45740},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":46272,\"start\":46249}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43589,\"start\":43350},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43814,\"start\":43590},{\"attributes\":{\"id\":\"fig_4\"},\"end\":43949,\"start\":43815},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44598,\"start\":43950},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44799,\"start\":44599},{\"attributes\":{\"id\":\"fig_9\"},\"end\":45069,\"start\":44800},{\"attributes\":{\"id\":\"fig_10\"},\"end\":45190,\"start\":45070},{\"attributes\":{\"id\":\"fig_11\"},\"end\":45325,\"start\":45191},{\"attributes\":{\"id\":\"fig_12\"},\"end\":45761,\"start\":45326},{\"attributes\":{\"id\":\"fig_14\"},\"end\":45886,\"start\":45762},{\"attributes\":{\"id\":\"fig_15\"},\"end\":46010,\"start\":45887},{\"attributes\":{\"id\":\"fig_16\"},\"end\":46223,\"start\":46011},{\"attributes\":{\"id\":\"fig_17\"},\"end\":46454,\"start\":46224},{\"attributes\":{\"id\":\"fig_18\"},\"end\":46691,\"start\":46455},{\"attributes\":{\"id\":\"fig_19\"},\"end\":46952,\"start\":46692},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":47232,\"start\":46953},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47900,\"start\":47233},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":48466,\"start\":47901},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":48510,\"start\":48467},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":48689,\"start\":48511},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":49466,\"start\":48690}]", "paragraph": "[{\"end\":2614,\"start\":1528},{\"end\":3006,\"start\":2616},{\"end\":3162,\"start\":3008},{\"end\":3465,\"start\":3164},{\"end\":3636,\"start\":3467},{\"end\":3788,\"start\":3638},{\"end\":3993,\"start\":3790},{\"end\":4651,\"start\":3995},{\"end\":5031,\"start\":4699},{\"end\":5528,\"start\":5033},{\"end\":5902,\"start\":5530},{\"end\":5959,\"start\":5904},{\"end\":6494,\"start\":6034},{\"end\":6560,\"start\":6496},{\"end\":6670,\"start\":6562},{\"end\":6844,\"start\":6707},{\"end\":7071,\"start\":6846},{\"end\":7606,\"start\":7073},{\"end\":8077,\"start\":7688},{\"end\":8122,\"start\":8079},{\"end\":8169,\"start\":8124},{\"end\":8232,\"start\":8171},{\"end\":8373,\"start\":8234},{\"end\":8875,\"start\":8805},{\"end\":10066,\"start\":8910},{\"end\":11113,\"start\":10090},{\"end\":11808,\"start\":11115},{\"end\":12827,\"start\":11870},{\"end\":13524,\"start\":12920},{\"end\":14058,\"start\":13526},{\"end\":15273,\"start\":14060},{\"end\":15777,\"start\":15275},{\"end\":16598,\"start\":15860},{\"end\":17055,\"start\":16745},{\"end\":17573,\"start\":17094},{\"end\":17895,\"start\":17664},{\"end\":18907,\"start\":18215},{\"end\":19508,\"start\":18909},{\"end\":19716,\"start\":19510},{\"end\":20736,\"start\":19841},{\"end\":21017,\"start\":20738},{\"end\":21358,\"start\":21019},{\"end\":21950,\"start\":21367},{\"end\":22066,\"start\":21952},{\"end\":23122,\"start\":22068},{\"end\":23370,\"start\":23124},{\"end\":23777,\"start\":23387},{\"end\":24265,\"start\":23779},{\"end\":24995,\"start\":24267},{\"end\":25341,\"start\":25010},{\"end\":25800,\"start\":25343},{\"end\":26102,\"start\":25833},{\"end\":26903,\"start\":26104},{\"end\":27069,\"start\":26905},{\"end\":27446,\"start\":27071},{\"end\":28268,\"start\":27541},{\"end\":28783,\"start\":28361},{\"end\":29108,\"start\":28816},{\"end\":29899,\"start\":29142},{\"end\":30381,\"start\":29926},{\"end\":30397,\"start\":30383},{\"end\":31651,\"start\":30410},{\"end\":32119,\"start\":31653},{\"end\":32640,\"start\":32141},{\"end\":33000,\"start\":32642},{\"end\":33723,\"start\":33002},{\"end\":34094,\"start\":33725},{\"end\":34770,\"start\":34096},{\"end\":35068,\"start\":34772},{\"end\":35883,\"start\":35093},{\"end\":36978,\"start\":35920},{\"end\":37656,\"start\":36980},{\"end\":37863,\"start\":37674},{\"end\":38644,\"start\":37865},{\"end\":39129,\"start\":38646},{\"end\":39453,\"start\":39131},{\"end\":39933,\"start\":39455},{\"end\":40915,\"start\":39978},{\"end\":41624,\"start\":40917},{\"end\":42183,\"start\":41626},{\"end\":42456,\"start\":42185},{\"end\":43349,\"start\":42458}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6033,\"start\":5960},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6706,\"start\":6671},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7687,\"start\":7607},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8804,\"start\":8374},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16744,\"start\":16599},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18214,\"start\":17906}]", "table_ref": "[{\"end\":15989,\"start\":15982},{\"end\":19613,\"start\":19606},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20018,\"start\":20011},{\"end\":20897,\"start\":20890},{\"end\":21124,\"start\":21117},{\"end\":21896,\"start\":21889},{\"end\":22710,\"start\":22703},{\"end\":29725,\"start\":29718},{\"end\":29879,\"start\":29872},{\"end\":36897,\"start\":36890},{\"end\":43036,\"start\":43029}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1526,\"start\":1514},{\"attributes\":{\"n\":\"2.\"},\"end\":4660,\"start\":4654},{\"attributes\":{\"n\":\"2.1.\"},\"end\":4697,\"start\":4663},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8908,\"start\":8878},{\"attributes\":{\"n\":\"2.3.\"},\"end\":10088,\"start\":10069},{\"attributes\":{\"n\":\"3.\"},\"end\":11868,\"start\":11811},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12918,\"start\":12830},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15858,\"start\":15780},{\"attributes\":{\"n\":\"4.\"},\"end\":17092,\"start\":17058},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17662,\"start\":17576},{\"end\":17905,\"start\":17898},{\"attributes\":{\"n\":\"5.\"},\"end\":19748,\"start\":19719},{\"attributes\":{\"n\":\"5.1.\"},\"end\":19839,\"start\":19751},{\"end\":21365,\"start\":21361},{\"attributes\":{\"n\":\"7.\"},\"end\":23385,\"start\":23373},{\"attributes\":{\"n\":\"8.\"},\"end\":25008,\"start\":24998},{\"end\":25831,\"start\":25803},{\"end\":27483,\"start\":27449},{\"end\":27539,\"start\":27486},{\"end\":28320,\"start\":28271},{\"end\":28359,\"start\":28323},{\"end\":28814,\"start\":28786},{\"end\":29140,\"start\":29111},{\"end\":29924,\"start\":29902},{\"end\":30408,\"start\":30400},{\"end\":32139,\"start\":32122},{\"end\":35091,\"start\":35071},{\"end\":35918,\"start\":35886},{\"end\":37672,\"start\":37659},{\"end\":39976,\"start\":39936},{\"end\":43361,\"start\":43351},{\"end\":43601,\"start\":43591},{\"end\":43961,\"start\":43951},{\"end\":44610,\"start\":44600},{\"end\":44811,\"start\":44801},{\"end\":45081,\"start\":45071},{\"end\":45202,\"start\":45192},{\"end\":45328,\"start\":45327},{\"end\":45776,\"start\":45763},{\"end\":45901,\"start\":45888},{\"end\":46023,\"start\":46012},{\"end\":46236,\"start\":46225},{\"end\":46482,\"start\":46456},{\"end\":46706,\"start\":46693},{\"end\":46963,\"start\":46954},{\"end\":48475,\"start\":48468},{\"end\":48525,\"start\":48512}]", "table": "[{\"end\":47900,\"start\":47702},{\"end\":48466,\"start\":48197},{\"end\":48510,\"start\":48477},{\"end\":49466,\"start\":48942}]", "figure_caption": "[{\"end\":43589,\"start\":43363},{\"end\":43814,\"start\":43603},{\"end\":43949,\"start\":43817},{\"end\":44598,\"start\":43963},{\"end\":44799,\"start\":44612},{\"end\":45069,\"start\":44813},{\"end\":45190,\"start\":45083},{\"end\":45325,\"start\":45204},{\"end\":45761,\"start\":45329},{\"end\":45886,\"start\":45778},{\"end\":46010,\"start\":45903},{\"end\":46223,\"start\":46025},{\"end\":46454,\"start\":46238},{\"end\":46691,\"start\":46485},{\"end\":46952,\"start\":46708},{\"end\":47232,\"start\":46965},{\"end\":47702,\"start\":47235},{\"end\":48197,\"start\":47903},{\"end\":48689,\"start\":48528},{\"end\":48942,\"start\":48692}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2792,\"start\":2784},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":4968,\"start\":4960},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":6061,\"start\":6053},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12566,\"start\":12558},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":13467,\"start\":13459},{\"end\":13704,\"start\":13696},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":14603,\"start\":14595},{\"end\":14876,\"start\":14868},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":15534,\"start\":15526},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":17176,\"start\":17168},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":17744,\"start\":17736},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":21456,\"start\":21448},{\"end\":27828,\"start\":27820},{\"end\":28830,\"start\":28822},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":34683,\"start\":34675},{\"end\":39198,\"start\":39190},{\"end\":39511,\"start\":39503},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":42650,\"start\":42640}]", "bib_author_first_name": "[{\"end\":50507,\"start\":50506},{\"end\":50518,\"start\":50517},{\"end\":50520,\"start\":50519},{\"end\":50529,\"start\":50528},{\"end\":50874,\"start\":50873},{\"end\":50884,\"start\":50883},{\"end\":50886,\"start\":50885},{\"end\":51114,\"start\":51113},{\"end\":51122,\"start\":51121},{\"end\":51129,\"start\":51128},{\"end\":51131,\"start\":51130},{\"end\":51138,\"start\":51137},{\"end\":51140,\"start\":51139},{\"end\":51153,\"start\":51152},{\"end\":51155,\"start\":51154},{\"end\":51165,\"start\":51164},{\"end\":51167,\"start\":51166},{\"end\":51559,\"start\":51558},{\"end\":51572,\"start\":51571},{\"end\":51583,\"start\":51582},{\"end\":51597,\"start\":51596},{\"end\":51609,\"start\":51608},{\"end\":51619,\"start\":51618},{\"end\":51621,\"start\":51620},{\"end\":51981,\"start\":51980},{\"end\":51992,\"start\":51991},{\"end\":52006,\"start\":52005},{\"end\":52321,\"start\":52320},{\"end\":52329,\"start\":52328},{\"end\":52336,\"start\":52335},{\"end\":52343,\"start\":52342},{\"end\":52806,\"start\":52805},{\"end\":52816,\"start\":52815},{\"end\":52824,\"start\":52823},{\"end\":52836,\"start\":52835},{\"end\":52847,\"start\":52846},{\"end\":53170,\"start\":53169},{\"end\":53172,\"start\":53171},{\"end\":53181,\"start\":53180},{\"end\":53189,\"start\":53188},{\"end\":53197,\"start\":53196},{\"end\":53210,\"start\":53209},{\"end\":53212,\"start\":53211},{\"end\":53664,\"start\":53663},{\"end\":53675,\"start\":53674},{\"end\":53677,\"start\":53676},{\"end\":53924,\"start\":53923},{\"end\":53935,\"start\":53934},{\"end\":53944,\"start\":53943},{\"end\":53946,\"start\":53945},{\"end\":54327,\"start\":54326},{\"end\":54338,\"start\":54337},{\"end\":54638,\"start\":54637},{\"end\":54649,\"start\":54648},{\"end\":54656,\"start\":54655},{\"end\":54667,\"start\":54666},{\"end\":54678,\"start\":54677},{\"end\":54687,\"start\":54686},{\"end\":54702,\"start\":54695},{\"end\":54706,\"start\":54705},{\"end\":55071,\"start\":55070},{\"end\":55086,\"start\":55085},{\"end\":55088,\"start\":55087},{\"end\":55104,\"start\":55103},{\"end\":55118,\"start\":55117},{\"end\":55449,\"start\":55448},{\"end\":55463,\"start\":55462},{\"end\":55475,\"start\":55474},{\"end\":55477,\"start\":55476},{\"end\":55489,\"start\":55488},{\"end\":55497,\"start\":55496},{\"end\":55872,\"start\":55871},{\"end\":55883,\"start\":55882},{\"end\":55893,\"start\":55892},{\"end\":56367,\"start\":56366},{\"end\":56378,\"start\":56377},{\"end\":56387,\"start\":56386},{\"end\":56611,\"start\":56610},{\"end\":56625,\"start\":56624},{\"end\":56642,\"start\":56641},{\"end\":56651,\"start\":56650},{\"end\":56657,\"start\":56656},{\"end\":56673,\"start\":56672},{\"end\":56682,\"start\":56681},{\"end\":56695,\"start\":56694},{\"end\":56975,\"start\":56974},{\"end\":56984,\"start\":56983},{\"end\":56994,\"start\":56993},{\"end\":57006,\"start\":57005},{\"end\":57019,\"start\":57018},{\"end\":57033,\"start\":57032},{\"end\":57043,\"start\":57042},{\"end\":57054,\"start\":57053},{\"end\":57061,\"start\":57060},{\"end\":57429,\"start\":57428},{\"end\":57440,\"start\":57439},{\"end\":57450,\"start\":57449},{\"end\":57835,\"start\":57834},{\"end\":57841,\"start\":57840},{\"end\":57850,\"start\":57849},{\"end\":57857,\"start\":57856},{\"end\":58260,\"start\":58259},{\"end\":58266,\"start\":58265},{\"end\":58273,\"start\":58272},{\"end\":58279,\"start\":58278},{\"end\":58286,\"start\":58285},{\"end\":58520,\"start\":58519},{\"end\":58526,\"start\":58525},{\"end\":58538,\"start\":58537},{\"end\":58836,\"start\":58835},{\"end\":58838,\"start\":58837},{\"end\":58848,\"start\":58847},{\"end\":58858,\"start\":58857},{\"end\":58869,\"start\":58868},{\"end\":58879,\"start\":58878},{\"end\":58881,\"start\":58880},{\"end\":59216,\"start\":59215},{\"end\":59218,\"start\":59217},{\"end\":59228,\"start\":59227},{\"end\":59243,\"start\":59239},{\"end\":59493,\"start\":59492},{\"end\":59495,\"start\":59494},{\"end\":59504,\"start\":59503},{\"end\":59515,\"start\":59514},{\"end\":59536,\"start\":59535},{\"end\":59546,\"start\":59545},{\"end\":59557,\"start\":59556},{\"end\":59570,\"start\":59569},{\"end\":59921,\"start\":59920},{\"end\":59923,\"start\":59922},{\"end\":60192,\"start\":60191},{\"end\":60201,\"start\":60200},{\"end\":60491,\"start\":60490},{\"end\":60497,\"start\":60496},{\"end\":60499,\"start\":60498},{\"end\":60512,\"start\":60511},{\"end\":60871,\"start\":60870},{\"end\":60873,\"start\":60872},{\"end\":60883,\"start\":60882},{\"end\":61117,\"start\":61116},{\"end\":61131,\"start\":61130},{\"end\":61139,\"start\":61138},{\"end\":61536,\"start\":61535},{\"end\":61549,\"start\":61548},{\"end\":61559,\"start\":61558},{\"end\":61561,\"start\":61560},{\"end\":61951,\"start\":61950},{\"end\":61961,\"start\":61960},{\"end\":61969,\"start\":61968},{\"end\":61978,\"start\":61977},{\"end\":62268,\"start\":62267},{\"end\":62282,\"start\":62281},{\"end\":62596,\"start\":62595},{\"end\":62610,\"start\":62609},{\"end\":62623,\"start\":62622},{\"end\":62625,\"start\":62624},{\"end\":62878,\"start\":62877},{\"end\":62892,\"start\":62891},{\"end\":63148,\"start\":63147},{\"end\":63150,\"start\":63149},{\"end\":63160,\"start\":63159},{\"end\":63374,\"start\":63373},{\"end\":63382,\"start\":63381},{\"end\":63393,\"start\":63392},{\"end\":63402,\"start\":63401},{\"end\":63414,\"start\":63413},{\"end\":63640,\"start\":63639},{\"end\":63651,\"start\":63650},{\"end\":63659,\"start\":63658},{\"end\":63670,\"start\":63669},{\"end\":63922,\"start\":63921},{\"end\":63931,\"start\":63930},{\"end\":64184,\"start\":64180},{\"end\":64196,\"start\":64195},{\"end\":64589,\"start\":64588},{\"end\":64600,\"start\":64599},{\"end\":64833,\"start\":64832},{\"end\":64841,\"start\":64840},{\"end\":64847,\"start\":64846},{\"end\":65103,\"start\":65102},{\"end\":65105,\"start\":65104},{\"end\":65115,\"start\":65114},{\"end\":65126,\"start\":65125},{\"end\":65145,\"start\":65138},{\"end\":65149,\"start\":65148},{\"end\":65447,\"start\":65446},{\"end\":65462,\"start\":65461},{\"end\":65470,\"start\":65469},{\"end\":65476,\"start\":65475},{\"end\":65486,\"start\":65485},{\"end\":65498,\"start\":65497},{\"end\":65504,\"start\":65503},{\"end\":65513,\"start\":65512},{\"end\":65525,\"start\":65524},{\"end\":65535,\"start\":65534},{\"end\":65904,\"start\":65903},{\"end\":65915,\"start\":65914},{\"end\":65931,\"start\":65930},{\"end\":66368,\"start\":66367},{\"end\":66380,\"start\":66379},{\"end\":66630,\"start\":66629},{\"end\":66844,\"start\":66843},{\"end\":66852,\"start\":66851},{\"end\":66868,\"start\":66864},{\"end\":66874,\"start\":66873},{\"end\":66883,\"start\":66882},{\"end\":66894,\"start\":66893},{\"end\":66896,\"start\":66895},{\"end\":66905,\"start\":66904},{\"end\":66916,\"start\":66915},{\"end\":66925,\"start\":66924},{\"end\":67312,\"start\":67311},{\"end\":67323,\"start\":67322},{\"end\":67330,\"start\":67329},{\"end\":67337,\"start\":67336},{\"end\":67349,\"start\":67348},{\"end\":67357,\"start\":67356},{\"end\":67369,\"start\":67368},{\"end\":67378,\"start\":67377},{\"end\":67391,\"start\":67390},{\"end\":67790,\"start\":67789},{\"end\":67798,\"start\":67797},{\"end\":67810,\"start\":67809},{\"end\":68051,\"start\":68050},{\"end\":68064,\"start\":68063},{\"end\":68076,\"start\":68075},{\"end\":68078,\"start\":68077},{\"end\":68092,\"start\":68091},{\"end\":68101,\"start\":68100},{\"end\":68393,\"start\":68392},{\"end\":68399,\"start\":68398},{\"end\":68408,\"start\":68407},{\"end\":68410,\"start\":68409},{\"end\":68418,\"start\":68415},{\"end\":68422,\"start\":68421},{\"end\":68852,\"start\":68851},{\"end\":68860,\"start\":68859},{\"end\":68868,\"start\":68867},{\"end\":68870,\"start\":68869},{\"end\":68881,\"start\":68880},{\"end\":68890,\"start\":68889},{\"end\":69230,\"start\":69229},{\"end\":69237,\"start\":69236},{\"end\":69244,\"start\":69243},{\"end\":69255,\"start\":69251},{\"end\":69264,\"start\":69263},{\"end\":69266,\"start\":69265},{\"end\":69510,\"start\":69509},{\"end\":69516,\"start\":69515},{\"end\":69525,\"start\":69524},{\"end\":69527,\"start\":69526},{\"end\":69539,\"start\":69534},{\"end\":69546,\"start\":69542},{\"end\":69970,\"start\":69969},{\"end\":69977,\"start\":69976},{\"end\":69987,\"start\":69986},{\"end\":70203,\"start\":70202},{\"end\":70211,\"start\":70210},{\"end\":70221,\"start\":70220},{\"end\":70235,\"start\":70234},{\"end\":70513,\"start\":70512},{\"end\":70522,\"start\":70521},{\"end\":70531,\"start\":70530},{\"end\":70533,\"start\":70532},{\"end\":70799,\"start\":70798},{\"end\":70809,\"start\":70808},{\"end\":70811,\"start\":70810},{\"end\":70824,\"start\":70818},{\"end\":70828,\"start\":70827}]", "bib_author_last_name": "[{\"end\":50515,\"start\":50508},{\"end\":50526,\"start\":50521},{\"end\":50540,\"start\":50530},{\"end\":50881,\"start\":50875},{\"end\":50893,\"start\":50887},{\"end\":51119,\"start\":51115},{\"end\":51126,\"start\":51123},{\"end\":51135,\"start\":51132},{\"end\":51150,\"start\":51141},{\"end\":51162,\"start\":51156},{\"end\":51177,\"start\":51168},{\"end\":51187,\"start\":51179},{\"end\":51569,\"start\":51560},{\"end\":51580,\"start\":51573},{\"end\":51594,\"start\":51584},{\"end\":51606,\"start\":51598},{\"end\":51616,\"start\":51610},{\"end\":51628,\"start\":51622},{\"end\":51638,\"start\":51630},{\"end\":51989,\"start\":51982},{\"end\":52003,\"start\":51993},{\"end\":52015,\"start\":52007},{\"end\":52326,\"start\":52322},{\"end\":52333,\"start\":52330},{\"end\":52340,\"start\":52337},{\"end\":52348,\"start\":52344},{\"end\":52813,\"start\":52807},{\"end\":52821,\"start\":52817},{\"end\":52833,\"start\":52825},{\"end\":52844,\"start\":52837},{\"end\":52855,\"start\":52848},{\"end\":53178,\"start\":53173},{\"end\":53186,\"start\":53182},{\"end\":53194,\"start\":53190},{\"end\":53207,\"start\":53198},{\"end\":53215,\"start\":53213},{\"end\":53228,\"start\":53217},{\"end\":53672,\"start\":53665},{\"end\":53684,\"start\":53678},{\"end\":53932,\"start\":53925},{\"end\":53941,\"start\":53936},{\"end\":53952,\"start\":53947},{\"end\":54335,\"start\":54328},{\"end\":54347,\"start\":54339},{\"end\":54646,\"start\":54639},{\"end\":54653,\"start\":54650},{\"end\":54664,\"start\":54657},{\"end\":54675,\"start\":54668},{\"end\":54684,\"start\":54679},{\"end\":54693,\"start\":54688},{\"end\":55083,\"start\":55072},{\"end\":55101,\"start\":55089},{\"end\":55115,\"start\":55105},{\"end\":55123,\"start\":55119},{\"end\":55460,\"start\":55450},{\"end\":55472,\"start\":55464},{\"end\":55486,\"start\":55478},{\"end\":55494,\"start\":55490},{\"end\":55507,\"start\":55498},{\"end\":55880,\"start\":55873},{\"end\":55890,\"start\":55884},{\"end\":55900,\"start\":55894},{\"end\":56375,\"start\":56368},{\"end\":56384,\"start\":56379},{\"end\":56397,\"start\":56388},{\"end\":56622,\"start\":56612},{\"end\":56639,\"start\":56626},{\"end\":56648,\"start\":56643},{\"end\":56654,\"start\":56652},{\"end\":56670,\"start\":56658},{\"end\":56679,\"start\":56674},{\"end\":56692,\"start\":56683},{\"end\":56702,\"start\":56696},{\"end\":56981,\"start\":56976},{\"end\":56991,\"start\":56985},{\"end\":57003,\"start\":56995},{\"end\":57016,\"start\":57007},{\"end\":57030,\"start\":57020},{\"end\":57040,\"start\":57034},{\"end\":57051,\"start\":57044},{\"end\":57058,\"start\":57055},{\"end\":57064,\"start\":57062},{\"end\":57074,\"start\":57066},{\"end\":57437,\"start\":57430},{\"end\":57447,\"start\":57441},{\"end\":57456,\"start\":57451},{\"end\":57838,\"start\":57836},{\"end\":57847,\"start\":57842},{\"end\":57854,\"start\":57851},{\"end\":57861,\"start\":57858},{\"end\":58263,\"start\":58261},{\"end\":58270,\"start\":58267},{\"end\":58276,\"start\":58274},{\"end\":58283,\"start\":58280},{\"end\":58295,\"start\":58287},{\"end\":58523,\"start\":58521},{\"end\":58535,\"start\":58527},{\"end\":58545,\"start\":58539},{\"end\":58845,\"start\":58839},{\"end\":58855,\"start\":58849},{\"end\":58866,\"start\":58859},{\"end\":58876,\"start\":58870},{\"end\":58886,\"start\":58882},{\"end\":59225,\"start\":59219},{\"end\":59237,\"start\":59229},{\"end\":59247,\"start\":59244},{\"end\":59501,\"start\":59496},{\"end\":59512,\"start\":59505},{\"end\":59533,\"start\":59516},{\"end\":59543,\"start\":59537},{\"end\":59554,\"start\":59547},{\"end\":59567,\"start\":59558},{\"end\":59577,\"start\":59571},{\"end\":59930,\"start\":59924},{\"end\":60198,\"start\":60193},{\"end\":60209,\"start\":60202},{\"end\":60494,\"start\":60492},{\"end\":60509,\"start\":60500},{\"end\":60520,\"start\":60513},{\"end\":60880,\"start\":60874},{\"end\":60891,\"start\":60884},{\"end\":61128,\"start\":61118},{\"end\":61136,\"start\":61132},{\"end\":61145,\"start\":61140},{\"end\":61546,\"start\":61537},{\"end\":61556,\"start\":61550},{\"end\":61564,\"start\":61562},{\"end\":61958,\"start\":61952},{\"end\":61966,\"start\":61962},{\"end\":61975,\"start\":61970},{\"end\":61986,\"start\":61979},{\"end\":62279,\"start\":62269},{\"end\":62289,\"start\":62283},{\"end\":62607,\"start\":62597},{\"end\":62620,\"start\":62611},{\"end\":62632,\"start\":62626},{\"end\":62889,\"start\":62879},{\"end\":62899,\"start\":62893},{\"end\":62905,\"start\":62901},{\"end\":63157,\"start\":63151},{\"end\":63167,\"start\":63161},{\"end\":63379,\"start\":63375},{\"end\":63390,\"start\":63383},{\"end\":63399,\"start\":63394},{\"end\":63411,\"start\":63403},{\"end\":63422,\"start\":63415},{\"end\":63648,\"start\":63641},{\"end\":63656,\"start\":63652},{\"end\":63667,\"start\":63660},{\"end\":63675,\"start\":63671},{\"end\":63928,\"start\":63923},{\"end\":63946,\"start\":63932},{\"end\":64193,\"start\":64185},{\"end\":64206,\"start\":64197},{\"end\":64597,\"start\":64590},{\"end\":64607,\"start\":64601},{\"end\":64838,\"start\":64834},{\"end\":64844,\"start\":64842},{\"end\":64855,\"start\":64848},{\"end\":65112,\"start\":65106},{\"end\":65123,\"start\":65116},{\"end\":65136,\"start\":65127},{\"end\":65459,\"start\":65448},{\"end\":65467,\"start\":65463},{\"end\":65473,\"start\":65471},{\"end\":65483,\"start\":65477},{\"end\":65495,\"start\":65487},{\"end\":65501,\"start\":65499},{\"end\":65510,\"start\":65505},{\"end\":65522,\"start\":65514},{\"end\":65532,\"start\":65526},{\"end\":65545,\"start\":65536},{\"end\":65912,\"start\":65905},{\"end\":65928,\"start\":65916},{\"end\":65939,\"start\":65932},{\"end\":66377,\"start\":66369},{\"end\":66390,\"start\":66381},{\"end\":66635,\"start\":66631},{\"end\":66849,\"start\":66845},{\"end\":66862,\"start\":66853},{\"end\":66871,\"start\":66869},{\"end\":66880,\"start\":66875},{\"end\":66891,\"start\":66884},{\"end\":66902,\"start\":66897},{\"end\":66913,\"start\":66906},{\"end\":66922,\"start\":66917},{\"end\":66932,\"start\":66926},{\"end\":66942,\"start\":66934},{\"end\":67320,\"start\":67313},{\"end\":67327,\"start\":67324},{\"end\":67334,\"start\":67331},{\"end\":67346,\"start\":67338},{\"end\":67354,\"start\":67350},{\"end\":67366,\"start\":67358},{\"end\":67375,\"start\":67370},{\"end\":67388,\"start\":67379},{\"end\":67402,\"start\":67392},{\"end\":67795,\"start\":67791},{\"end\":67807,\"start\":67799},{\"end\":67816,\"start\":67811},{\"end\":68061,\"start\":68052},{\"end\":68073,\"start\":68065},{\"end\":68089,\"start\":68079},{\"end\":68098,\"start\":68093},{\"end\":68107,\"start\":68102},{\"end\":68396,\"start\":68394},{\"end\":68405,\"start\":68400},{\"end\":68413,\"start\":68411},{\"end\":68857,\"start\":68853},{\"end\":68865,\"start\":68861},{\"end\":68878,\"start\":68871},{\"end\":68887,\"start\":68882},{\"end\":68899,\"start\":68891},{\"end\":69234,\"start\":69231},{\"end\":69241,\"start\":69238},{\"end\":69249,\"start\":69245},{\"end\":69261,\"start\":69256},{\"end\":69269,\"start\":69267},{\"end\":69513,\"start\":69511},{\"end\":69522,\"start\":69517},{\"end\":69532,\"start\":69528},{\"end\":69974,\"start\":69971},{\"end\":69984,\"start\":69978},{\"end\":69996,\"start\":69988},{\"end\":70208,\"start\":70204},{\"end\":70218,\"start\":70212},{\"end\":70232,\"start\":70222},{\"end\":70241,\"start\":70236},{\"end\":70519,\"start\":70514},{\"end\":70528,\"start\":70523},{\"end\":70539,\"start\":70534},{\"end\":70806,\"start\":70800},{\"end\":70816,\"start\":70812}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":173990164},\"end\":50789,\"start\":50434},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4332326},\"end\":51054,\"start\":50791},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":10860374},\"end\":51508,\"start\":51056},{\"attributes\":{\"id\":\"b3\"},\"end\":51915,\"start\":51510},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":12726540},\"end\":52245,\"start\":51917},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9653504},\"end\":52770,\"start\":52247},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4309276},\"end\":53123,\"start\":52772},{\"attributes\":{\"id\":\"b7\"},\"end\":53591,\"start\":53125},{\"attributes\":{\"doi\":\"arXiv:1708.04552\",\"id\":\"b8\"},\"end\":53854,\"start\":53593},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9062671},\"end\":54275,\"start\":53856},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":195820291},\"end\":54556,\"start\":54277},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6161478},\"end\":54987,\"start\":54558},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3244218},\"end\":55396,\"start\":54989},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4246903},\"end\":55741,\"start\":55398},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2156851},\"end\":56296,\"start\":55743},{\"attributes\":{\"doi\":\"arXiv:1803.07728\",\"id\":\"b15\"},\"end\":56579,\"start\":56298},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1033682},\"end\":56972,\"start\":56581},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b17\"},\"end\":57367,\"start\":56974},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":8281592},\"end\":57786,\"start\":57369},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206594692},\"end\":58190,\"start\":57788},{\"attributes\":{\"doi\":\"arXiv:1911.05722\",\"id\":\"b20\"},\"end\":58484,\"start\":58192},{\"attributes\":{\"id\":\"b21\"},\"end\":58833,\"start\":58486},{\"attributes\":{\"doi\":\"arXiv:1905.09272\",\"id\":\"b22\"},\"end\":59165,\"start\":58835},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2309950},\"end\":59409,\"start\":59167},{\"attributes\":{\"doi\":\"arXiv:1808.06670\",\"id\":\"b24\"},\"end\":59835,\"start\":59411},{\"attributes\":{\"doi\":\"arXiv:1312.5402\",\"id\":\"b25\"},\"end\":60095,\"start\":59837},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b26\"},\"end\":60399,\"start\":60097},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":85517234},\"end\":60868,\"start\":60401},{\"attributes\":{\"doi\":\"arXiv:1312.6114\",\"id\":\"b28\"},\"end\":61055,\"start\":60870},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":59292019},\"end\":61489,\"start\":61057},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":43928547},\"end\":61893,\"start\":61491},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":16632981},\"end\":62210,\"start\":61895},{\"attributes\":{\"id\":\"b32\"},\"end\":62528,\"start\":62212},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":195908774},\"end\":62875,\"start\":62530},{\"attributes\":{\"doi\":\"arXiv:1608.03983\",\"id\":\"b34\"},\"end\":63107,\"start\":62877},{\"attributes\":{\"id\":\"b35\"},\"end\":63323,\"start\":63109},{\"attributes\":{\"id\":\"b36\"},\"end\":63575,\"start\":63325},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b37\"},\"end\":63856,\"start\":63577},{\"attributes\":{\"doi\":\"arXiv:1912.01991\",\"id\":\"b38\"},\"end\":64114,\"start\":63858},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":15193013},\"end\":64511,\"start\":64116},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":187547},\"end\":64830,\"start\":64513},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b41\"},\"end\":65085,\"start\":64832},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":383200},\"end\":65393,\"start\":65087},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2930547},\"end\":65835,\"start\":65395},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206592766},\"end\":66297,\"start\":65837},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b45\"},\"end\":66557,\"start\":66299},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":911406},\"end\":66841,\"start\":66559},{\"attributes\":{\"doi\":\"arXiv:2001.07685\",\"id\":\"b47\"},\"end\":67277,\"start\":66843},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206592484},\"end\":67787,\"start\":67279},{\"attributes\":{\"doi\":\"arXiv:1906.05849\",\"id\":\"b49\"},\"end\":67984,\"start\":67789},{\"attributes\":{\"doi\":\"arXiv:1907.13625\",\"id\":\"b50\"},\"end\":68316,\"start\":67986},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":4591284},\"end\":68786,\"start\":68318},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1309931},\"end\":69195,\"start\":68788},{\"attributes\":{\"doi\":\"arXiv:1904.12848\",\"id\":\"b53\"},\"end\":69429,\"start\":69197},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":102350974},\"end\":69919,\"start\":69431},{\"attributes\":{\"doi\":\"arXiv:1708.03888\",\"id\":\"b55\"},\"end\":70154,\"start\":69921},{\"attributes\":{\"id\":\"b56\"},\"end\":70481,\"start\":70156},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":50698},\"end\":70730,\"start\":70483},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":88523028},\"end\":71150,\"start\":70732}]", "bib_title": "[{\"end\":50504,\"start\":50434},{\"end\":50871,\"start\":50791},{\"end\":51111,\"start\":51056},{\"end\":51556,\"start\":51510},{\"end\":51978,\"start\":51917},{\"end\":52318,\"start\":52247},{\"end\":52803,\"start\":52772},{\"end\":53167,\"start\":53125},{\"end\":53921,\"start\":53856},{\"end\":54324,\"start\":54277},{\"end\":54635,\"start\":54558},{\"end\":55068,\"start\":54989},{\"end\":55446,\"start\":55398},{\"end\":55869,\"start\":55743},{\"end\":56608,\"start\":56581},{\"end\":57426,\"start\":57369},{\"end\":57832,\"start\":57788},{\"end\":58517,\"start\":58486},{\"end\":59213,\"start\":59167},{\"end\":60488,\"start\":60401},{\"end\":61114,\"start\":61057},{\"end\":61533,\"start\":61491},{\"end\":61948,\"start\":61895},{\"end\":62593,\"start\":62530},{\"end\":64178,\"start\":64116},{\"end\":64586,\"start\":64513},{\"end\":65100,\"start\":65087},{\"end\":65444,\"start\":65395},{\"end\":65901,\"start\":65837},{\"end\":66627,\"start\":66559},{\"end\":67309,\"start\":67279},{\"end\":68390,\"start\":68318},{\"end\":68849,\"start\":68788},{\"end\":69507,\"start\":69431},{\"end\":70200,\"start\":70156},{\"end\":70510,\"start\":70483},{\"end\":70796,\"start\":70732}]", "bib_author": "[{\"end\":50517,\"start\":50506},{\"end\":50528,\"start\":50517},{\"end\":50542,\"start\":50528},{\"end\":50883,\"start\":50873},{\"end\":50895,\"start\":50883},{\"end\":51121,\"start\":51113},{\"end\":51128,\"start\":51121},{\"end\":51137,\"start\":51128},{\"end\":51152,\"start\":51137},{\"end\":51164,\"start\":51152},{\"end\":51179,\"start\":51164},{\"end\":51189,\"start\":51179},{\"end\":51571,\"start\":51558},{\"end\":51582,\"start\":51571},{\"end\":51596,\"start\":51582},{\"end\":51608,\"start\":51596},{\"end\":51618,\"start\":51608},{\"end\":51630,\"start\":51618},{\"end\":51640,\"start\":51630},{\"end\":51991,\"start\":51980},{\"end\":52005,\"start\":51991},{\"end\":52017,\"start\":52005},{\"end\":52328,\"start\":52320},{\"end\":52335,\"start\":52328},{\"end\":52342,\"start\":52335},{\"end\":52350,\"start\":52342},{\"end\":52815,\"start\":52805},{\"end\":52823,\"start\":52815},{\"end\":52835,\"start\":52823},{\"end\":52846,\"start\":52835},{\"end\":52857,\"start\":52846},{\"end\":53180,\"start\":53169},{\"end\":53188,\"start\":53180},{\"end\":53196,\"start\":53188},{\"end\":53209,\"start\":53196},{\"end\":53217,\"start\":53209},{\"end\":53230,\"start\":53217},{\"end\":53674,\"start\":53663},{\"end\":53686,\"start\":53674},{\"end\":53934,\"start\":53923},{\"end\":53943,\"start\":53934},{\"end\":53954,\"start\":53943},{\"end\":54337,\"start\":54326},{\"end\":54349,\"start\":54337},{\"end\":54648,\"start\":54637},{\"end\":54655,\"start\":54648},{\"end\":54666,\"start\":54655},{\"end\":54677,\"start\":54666},{\"end\":54686,\"start\":54677},{\"end\":54695,\"start\":54686},{\"end\":54705,\"start\":54695},{\"end\":54709,\"start\":54705},{\"end\":55085,\"start\":55070},{\"end\":55103,\"start\":55085},{\"end\":55117,\"start\":55103},{\"end\":55125,\"start\":55117},{\"end\":55462,\"start\":55448},{\"end\":55474,\"start\":55462},{\"end\":55488,\"start\":55474},{\"end\":55496,\"start\":55488},{\"end\":55509,\"start\":55496},{\"end\":55882,\"start\":55871},{\"end\":55892,\"start\":55882},{\"end\":55902,\"start\":55892},{\"end\":56377,\"start\":56366},{\"end\":56386,\"start\":56377},{\"end\":56399,\"start\":56386},{\"end\":56624,\"start\":56610},{\"end\":56641,\"start\":56624},{\"end\":56650,\"start\":56641},{\"end\":56656,\"start\":56650},{\"end\":56672,\"start\":56656},{\"end\":56681,\"start\":56672},{\"end\":56694,\"start\":56681},{\"end\":56704,\"start\":56694},{\"end\":56983,\"start\":56974},{\"end\":56993,\"start\":56983},{\"end\":57005,\"start\":56993},{\"end\":57018,\"start\":57005},{\"end\":57032,\"start\":57018},{\"end\":57042,\"start\":57032},{\"end\":57053,\"start\":57042},{\"end\":57060,\"start\":57053},{\"end\":57066,\"start\":57060},{\"end\":57076,\"start\":57066},{\"end\":57439,\"start\":57428},{\"end\":57449,\"start\":57439},{\"end\":57458,\"start\":57449},{\"end\":57840,\"start\":57834},{\"end\":57849,\"start\":57840},{\"end\":57856,\"start\":57849},{\"end\":57863,\"start\":57856},{\"end\":58265,\"start\":58259},{\"end\":58272,\"start\":58265},{\"end\":58278,\"start\":58272},{\"end\":58285,\"start\":58278},{\"end\":58297,\"start\":58285},{\"end\":58525,\"start\":58519},{\"end\":58537,\"start\":58525},{\"end\":58547,\"start\":58537},{\"end\":58847,\"start\":58835},{\"end\":58857,\"start\":58847},{\"end\":58868,\"start\":58857},{\"end\":58878,\"start\":58868},{\"end\":58888,\"start\":58878},{\"end\":59227,\"start\":59215},{\"end\":59239,\"start\":59227},{\"end\":59249,\"start\":59239},{\"end\":59503,\"start\":59492},{\"end\":59514,\"start\":59503},{\"end\":59535,\"start\":59514},{\"end\":59545,\"start\":59535},{\"end\":59556,\"start\":59545},{\"end\":59569,\"start\":59556},{\"end\":59579,\"start\":59569},{\"end\":59932,\"start\":59920},{\"end\":60200,\"start\":60191},{\"end\":60211,\"start\":60200},{\"end\":60496,\"start\":60490},{\"end\":60511,\"start\":60496},{\"end\":60522,\"start\":60511},{\"end\":60882,\"start\":60870},{\"end\":60893,\"start\":60882},{\"end\":61130,\"start\":61116},{\"end\":61138,\"start\":61130},{\"end\":61147,\"start\":61138},{\"end\":61548,\"start\":61535},{\"end\":61558,\"start\":61548},{\"end\":61566,\"start\":61558},{\"end\":61960,\"start\":61950},{\"end\":61968,\"start\":61960},{\"end\":61977,\"start\":61968},{\"end\":61988,\"start\":61977},{\"end\":62281,\"start\":62267},{\"end\":62291,\"start\":62281},{\"end\":62609,\"start\":62595},{\"end\":62622,\"start\":62609},{\"end\":62634,\"start\":62622},{\"end\":62891,\"start\":62877},{\"end\":62901,\"start\":62891},{\"end\":62907,\"start\":62901},{\"end\":63159,\"start\":63147},{\"end\":63169,\"start\":63159},{\"end\":63381,\"start\":63373},{\"end\":63392,\"start\":63381},{\"end\":63401,\"start\":63392},{\"end\":63413,\"start\":63401},{\"end\":63424,\"start\":63413},{\"end\":63650,\"start\":63639},{\"end\":63658,\"start\":63650},{\"end\":63669,\"start\":63658},{\"end\":63677,\"start\":63669},{\"end\":63930,\"start\":63921},{\"end\":63948,\"start\":63930},{\"end\":64195,\"start\":64180},{\"end\":64208,\"start\":64195},{\"end\":64599,\"start\":64588},{\"end\":64609,\"start\":64599},{\"end\":64840,\"start\":64832},{\"end\":64846,\"start\":64840},{\"end\":64857,\"start\":64846},{\"end\":65114,\"start\":65102},{\"end\":65125,\"start\":65114},{\"end\":65138,\"start\":65125},{\"end\":65148,\"start\":65138},{\"end\":65152,\"start\":65148},{\"end\":65461,\"start\":65446},{\"end\":65469,\"start\":65461},{\"end\":65475,\"start\":65469},{\"end\":65485,\"start\":65475},{\"end\":65497,\"start\":65485},{\"end\":65503,\"start\":65497},{\"end\":65512,\"start\":65503},{\"end\":65524,\"start\":65512},{\"end\":65534,\"start\":65524},{\"end\":65547,\"start\":65534},{\"end\":65914,\"start\":65903},{\"end\":65930,\"start\":65914},{\"end\":65941,\"start\":65930},{\"end\":66379,\"start\":66367},{\"end\":66392,\"start\":66379},{\"end\":66637,\"start\":66629},{\"end\":66851,\"start\":66843},{\"end\":66864,\"start\":66851},{\"end\":66873,\"start\":66864},{\"end\":66882,\"start\":66873},{\"end\":66893,\"start\":66882},{\"end\":66904,\"start\":66893},{\"end\":66915,\"start\":66904},{\"end\":66924,\"start\":66915},{\"end\":66934,\"start\":66924},{\"end\":66944,\"start\":66934},{\"end\":67322,\"start\":67311},{\"end\":67329,\"start\":67322},{\"end\":67336,\"start\":67329},{\"end\":67348,\"start\":67336},{\"end\":67356,\"start\":67348},{\"end\":67368,\"start\":67356},{\"end\":67377,\"start\":67368},{\"end\":67390,\"start\":67377},{\"end\":67404,\"start\":67390},{\"end\":67797,\"start\":67789},{\"end\":67809,\"start\":67797},{\"end\":67818,\"start\":67809},{\"end\":68063,\"start\":68050},{\"end\":68075,\"start\":68063},{\"end\":68091,\"start\":68075},{\"end\":68100,\"start\":68091},{\"end\":68109,\"start\":68100},{\"end\":68398,\"start\":68392},{\"end\":68407,\"start\":68398},{\"end\":68415,\"start\":68407},{\"end\":68421,\"start\":68415},{\"end\":68425,\"start\":68421},{\"end\":68859,\"start\":68851},{\"end\":68867,\"start\":68859},{\"end\":68880,\"start\":68867},{\"end\":68889,\"start\":68880},{\"end\":68901,\"start\":68889},{\"end\":69236,\"start\":69229},{\"end\":69243,\"start\":69236},{\"end\":69251,\"start\":69243},{\"end\":69263,\"start\":69251},{\"end\":69271,\"start\":69263},{\"end\":69515,\"start\":69509},{\"end\":69524,\"start\":69515},{\"end\":69534,\"start\":69524},{\"end\":69542,\"start\":69534},{\"end\":69549,\"start\":69542},{\"end\":69976,\"start\":69969},{\"end\":69986,\"start\":69976},{\"end\":69998,\"start\":69986},{\"end\":70210,\"start\":70202},{\"end\":70220,\"start\":70210},{\"end\":70234,\"start\":70220},{\"end\":70243,\"start\":70234},{\"end\":70521,\"start\":70512},{\"end\":70530,\"start\":70521},{\"end\":70541,\"start\":70530},{\"end\":70808,\"start\":70798},{\"end\":70818,\"start\":70808},{\"end\":70827,\"start\":70818},{\"end\":70831,\"start\":70827}]", "bib_venue": "[{\"end\":50591,\"start\":50542},{\"end\":50901,\"start\":50895},{\"end\":51254,\"start\":51189},{\"end\":51689,\"start\":51640},{\"end\":52055,\"start\":52017},{\"end\":52448,\"start\":52350},{\"end\":52922,\"start\":52857},{\"end\":53307,\"start\":53230},{\"end\":53661,\"start\":53593},{\"end\":54021,\"start\":53954},{\"end\":54398,\"start\":54349},{\"end\":54753,\"start\":54709},{\"end\":55174,\"start\":55125},{\"end\":55549,\"start\":55509},{\"end\":56009,\"start\":55902},{\"end\":56364,\"start\":56298},{\"end\":56753,\"start\":56704},{\"end\":57140,\"start\":57092},{\"end\":57548,\"start\":57458},{\"end\":57940,\"start\":57863},{\"end\":58257,\"start\":58192},{\"end\":58614,\"start\":58547},{\"end\":58971,\"start\":58904},{\"end\":59267,\"start\":59249},{\"end\":59490,\"start\":59411},{\"end\":59918,\"start\":59837},{\"end\":60189,\"start\":60097},{\"end\":60589,\"start\":60522},{\"end\":60939,\"start\":60908},{\"end\":61224,\"start\":61147},{\"end\":61643,\"start\":61566},{\"end\":62041,\"start\":61988},{\"end\":62265,\"start\":62212},{\"end\":62683,\"start\":62634},{\"end\":62969,\"start\":62923},{\"end\":63145,\"start\":63109},{\"end\":63371,\"start\":63325},{\"end\":63637,\"start\":63577},{\"end\":63919,\"start\":63858},{\"end\":64252,\"start\":64208},{\"end\":64647,\"start\":64609},{\"end\":64931,\"start\":64873},{\"end\":65217,\"start\":65152},{\"end\":65587,\"start\":65547},{\"end\":66018,\"start\":65941},{\"end\":66365,\"start\":66299},{\"end\":66686,\"start\":66637},{\"end\":67028,\"start\":66960},{\"end\":67481,\"start\":67404},{\"end\":67862,\"start\":67834},{\"end\":68048,\"start\":67986},{\"end\":68502,\"start\":68425},{\"end\":68966,\"start\":68901},{\"end\":69227,\"start\":69197},{\"end\":69626,\"start\":69549},{\"end\":69967,\"start\":69921},{\"end\":70302,\"start\":70243},{\"end\":70579,\"start\":70541},{\"end\":70898,\"start\":70831},{\"end\":52533,\"start\":52450},{\"end\":53371,\"start\":53309},{\"end\":54075,\"start\":54023},{\"end\":58004,\"start\":57942},{\"end\":58668,\"start\":58616},{\"end\":60643,\"start\":60591},{\"end\":61288,\"start\":61226},{\"end\":61707,\"start\":61645},{\"end\":66082,\"start\":66020},{\"end\":67545,\"start\":67483},{\"end\":68566,\"start\":68504},{\"end\":69690,\"start\":69628},{\"end\":70952,\"start\":70900}]"}}}, "year": 2023, "month": 12, "day": 17}