{"id": 15280949, "updated": "2023-09-28 16:01:21.898", "metadata": {"title": "A Structured Self-attentive Sentence Embedding", "authors": "[{\"first\":\"Zhouhan\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Minwei\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Cicero\",\"last\":\"Santos\",\"middle\":[\"Nogueira\",\"dos\"]},{\"first\":\"Mo\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Bing\",\"last\":\"Xiang\",\"middle\":[]},{\"first\":\"Bowen\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Yoshua\",\"last\":\"Bengio\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 3, "day": 9}, "abstract": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1703.03130", "mag": "2963386218", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/LinFSYXZB17", "doi": null}}, "content": {"source": {"pdf_hash": "204a4a70428f3938d2c538a4d74c7ae0416306d8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1703.03130v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b1d7be83982923f9bd96682fb3f73a2f6fd5d2ff", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/204a4a70428f3938d2c538a4d74c7ae0416306d8.txt", "contents": "\nA STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING\n\n\nZhouhan Lin lin.zhouhan@gmail.com \nMontreal Institute for Learning Algorithms (MILA)\nUniversit\u00e9 de Montr\u00e9al \u2020 CIFAR Senior Fellow\n\n\nMinwei Feng mfeng@us.ibm.com \nMontreal Institute for Learning Algorithms (MILA)\nUniversit\u00e9 de Montr\u00e9al \u2020 CIFAR Senior Fellow\n\n\nCicero Nogueira cicerons@us.ibm.com \nMontreal Institute for Learning Algorithms (MILA)\nUniversit\u00e9 de Montr\u00e9al \u2020 CIFAR Senior Fellow\n\n\nMo Santos \nMontreal Institute for Learning Algorithms (MILA)\nUniversit\u00e9 de Montr\u00e9al \u2020 CIFAR Senior Fellow\n\n\nBing Yu \nMontreal Institute for Learning Algorithms (MILA)\nUniversit\u00e9 de Montr\u00e9al \u2020 CIFAR Senior Fellow\n\n\nBowen Xiang \nMontreal Institute for Learning Algorithms (MILA)\nUniversit\u00e9 de Montr\u00e9al \u2020 CIFAR Senior Fellow\n\n\nYoshua Zhou zhou@us.ibm.com \nMontreal Institute for Learning Algorithms (MILA)\nUniversit\u00e9 de Montr\u00e9al \u2020 CIFAR Senior Fellow\n\n\nBengio \nMontreal Institute for Learning Algorithms (MILA)\nUniversit\u00e9 de Montr\u00e9al \u2020 CIFAR Senior Fellow\n\n\nIbm Watson \nMontreal Institute for Learning Algorithms (MILA)\nUniversit\u00e9 de Montr\u00e9al \u2020 CIFAR Senior Fellow\n\n\nA STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING\nPublished as a conference paper at ICLR 2017\nThis paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks. * This work has been done during the 1st author's internship with IBM Watson.\n\nINTRODUCTION\n\nMuch progress has been made in learning semantically meaningful distributed representations of individual words, also known as word embeddings (Bengio et al., 2001;Mikolov et al., 2013). On the other hand, much remains to be done to obtain satisfying representations of phrases and sentences. Those methods generally fall into two categories. The first consists of universal sentence embeddings usually trained by unsupervised learning (Hill et al., 2016). This includes SkipThought vectors , ParagraphVector (Le & Mikolov, 2014), recursive auto-encoders (Socher et al., 2011;2013), Sequential Denoising Autoencoders (SDAE), FastSent (Hill et al., 2016), etc.\n\nThe other category consists of models trained specifically for a certain task. They are usually combined with downstream applications and trained by supervised learning. One generally finds that specifically trained sentence embeddings perform better than generic ones, although generic ones can be used in a semi-supervised setting, exploiting large unlabeled corpora. Several models have been proposed along this line, by using recurrent networks (Hochreiter & Schmidhuber, 1997;Chung et al., 2014), recursive networks (Socher et al., 2013) and convolutional networks (Kalchbrenner et al., 2014;dos Santos & Gatti, 2014;Kim, 2014) as an intermediate step in creating sentence representations to solve a wide variety of tasks including classification and ranking (Yin & Sch\u00fctze, 2015;Palangi et al., 2016;Feng et al., 2015). A common approach in previous methods consists in creating a simple vector representation by using the final hidden state of the RNN or the max (or average) pooling from either RNNs hidden states or convolved n-grams. Additional works have also been done in exploiting linguistic structures such as parse and dependence trees to improve sentence representations (Ma et al., 2015;Mou et al., 2015b;Tai et al., 2015).\n\nFor some tasks people propose to use attention mechanism on top of the CNN or LSTM model to introduce extra source of information to guide the extraction of sentence embedding . However, for some other tasks like sentiment classification, this is not directly applicable since there is no such extra information: the model is only given one single sentence as input. In those cases, the most common way is to add a max pooling or averaging step across all time steps   Figure 1: A sample model structure showing the sentence embedding model combined with a fully connected and softmax layer for sentiment analysis (a). The sentence embedding M is computed as multiple weighted sums of hidden states from a bidirectional LSTM (h 1 , ..., h n ), where the summation weights (A i1 , ..., A in ) are computed in a way illustrated in (b). Blue colored shapes stand for hidden representations, and red colored shapes stand for weights, annotations, or input/output. (Lee & Dernoncourt, 2016), or just pick up the hidden representation at the last time step as the encoded embedding (Margarit & Subramaniam, 2016).\n\nA common approach in many of the aforementioned methods consists of creating a simple vector representation by using the final hidden state of the RNN or the max (or average) pooling from either RNNs hidden states or convolved n-grams. We hypothesize that carrying the semantics along all time steps of a recurrent model is relatively hard and not necessary. We propose a self-attention mechanism for these sequential models to replace the max pooling or averaging step. Different from previous approaches, the proposed self-attention mechanism allows extracting different aspects of the sentence into multiple vector representations. It is performed on top of an LSTM in our sentence embedding model. This enables attention to be used in those cases when there are no extra inputs. In addition, due to its direct access to hidden representations from previous time steps, it relieves some long-term memorization burden from LSTM. As a side effect coming together with our proposed self-attentive sentence embedding, interpreting the extracted embedding becomes very easy and explicit.\n\nSection 2 details on our proposed self-attentive sentence embedding model, as well as a regularization term we proposed for this model, which is described in Section 2.2. We also provide a visualization method for this sentence embedding in section 2.3. We then evaluate our model in author profiling, sentiment classification and textual entailment tasks in Section 4.\n\n\nAPPROACH\n\n\nMODEL\n\nThe proposed sentence embedding model consists of two parts. The first part is a bidirectional LSTM, and the second part is the self-attention mechanism, which provides a set of summation weight vectors for the LSTM hidden states. These set of summation weight vectors are dotted with the LSTM hidden states, and the resulting weighted LSTM hidden states are considered as an embedding for the sentence. It can be combined with, for example, a multilayer perceptron to be applied on a downstream application. Figure 1 shows an example when the proposed sentence embedding model is applied to sentiment analysis, combined with a fully connected layer and a softmax layer. Besides using a fully connected layer, we also proposes an approach that prunes weight connections by utilizing the 2-D structure of matrix sentence embedding, which is detailed in Appendix A. For this section, we will use Figure 1 to describe our model.\n\nSuppose we have a sentence, which has n tokens, represented in a sequence of word embeddings.\nS = (w 1 , w 2 , \u00b7 \u00b7 \u00b7 w n )(1)\nHere w i is a vector standing for a d dimentional word embedding for the i-th word in the sentence. S is thus a sequence represented as a 2-D matrix, which concatenates all the word embeddings together. S should have the shape n-by-d.\n\nNow each entry in the sequence S are independent with each other. To gain some dependency between adjacent words within a single sentence, we use a bidirectional LSTM to process the sentence:\n\u2212 \u2192 h t = \u2212\u2212\u2212\u2212\u2192 LST M (w t , \u2212 \u2212 \u2192 h t\u22121 ) (2) \u2190 \u2212 h t = \u2190\u2212\u2212\u2212\u2212 LST M (w t , \u2190 \u2212 \u2212 h t+1 )(3)\nAnd we concatenate each \u2212 \u2192 h t with \u2190 \u2212 h t to obtain a hidden state h t . Let the hidden unit number for each unidirectional LSTM be u. For simplicity, we note all the n h t s as H, who have the size n-by-2u.\nH = (h 1 , h 2 , \u00b7 \u00b7 \u00b7 h n )(4)\nOur aim is to encode a variable length sentence into a fixed size embedding. We achieve that by choosing a linear combination of the n LSTM hidden vectors in H. Computing the linear combination requires the self-attention mechanism. The attention mechanism takes the whole LSTM hidden states H as input, and outputs a vector of weights a:\na = sof tmax w s2 tanh W s1 H T(5)\nHere W s1 is a weight matrix with a shape of d a -by-2u. and w s2 is a vector of parameters with size d a , where d a is a hyperparameter we can set arbitrarily. Since H is sized n-by-2u, the annotation vector a will have a size n. the sof tmax() ensures all the computed weights sum up to 1. Then we sum up the LSTM hidden states H according to the weight provided by a to get a vector representation m of the input sentence.\n\nThis vector representation usually focuses on a specific component of the sentence, like a special set of related words or phrases. So it is expected to reflect an aspect, or component of the semantics in a sentence. However, there can be multiple components in a sentence that together forms the overall semantics of the whole sentence, especially for long sentences. (For example, two clauses linked together by an \"and.\") Thus, to represent the overall semantics of the sentence, we need multiple m's that focus on different parts of the sentence. Thus we need to perform multiple hops of attention. Say we want r different parts to be extracted from the sentence, with regard to this, we extend the w s2 into a r-by-d a matrix, note it as W s2 , and the resulting annotation vector a becomes annotation matrix A. Formally,\nA = sof tmax W s2 tanh W s1 H T(6)\nHere the sof tmax() is performed along the second dimension of its input. We can deem Equation 6 as a 2-layer MLP without bias, whose hidden unit numbers is d a , and parameters are {W s2 , W s1 }.\n\nThe embedding vector m then becomes an r-by-2u embedding matrix M . We compute the r weighted sums by multiplying the annotation matrix A and LSTM hidden states H, the resulting matrix is the sentence embedding:\nM = AH(7)\n\nPENALIZATION TERM\n\nThe embedding matrix M can suffer from redundancy problems if the attention mechanism always provides similar summation weights for all the r hops. Thus we need a penalization term to encourage the diversity of summation weight vectors across different hops of attention.\n\nThe best way to evaluate the diversity is definitely the Kullback Leibler divergence between any 2 of the summation weight vectors. However, we found that not very stable in our case. We conjecture it is because we are maximizing a set of KL divergence (instead of minimizing only one, which is the usual case), we are optimizing the annotation matrix A to have a lot of sufficiently small or even zero values at different softmax output units, and these vast amount of zeros is making the training unstable. There is another feature that KL doesn't provide but we want, which is, we want each individual row to focus on a single aspect of semantics, so we want the probability mass in the annotation softmax output to be more focused. but with KL penalty we cant encourage that.\n\nWe hereby introduce a new penalization term which overcomes the aforementioned shortcomings. Compared to the KL divergence penalization, this term consumes only one third of the computation. We use the dot product of A and its transpose, subtracted by an identity matrix, as a measure of redundancy.\nP = AA T \u2212 I F 2(8)\nHere \u2022 F stands for the Frobenius norm of a matrix. Similar to adding an L2 regularization term, this penalization term P will be multiplied by a coefficient, and we minimize it together with the original loss, which is dependent on the downstream application.\n\nLet's consider two different summation vectors a i and a j in A. Because of the softmax, all entries within any summation vector in A should sum up to 1. Thus they can be deemed as probability masses in a discrete probability distribution. For any non-diagonal elements a ij (i = j) in the AA T matrix, it corresponds to a summation over elementwise product of two distributions:\n0 < a ij = n k=1 a i k a j k < 1 (9)\nwhere a i k and a j k are the k-th element in the a i and a j vectors, respectively. In the most extreme case, where there is no overlap between the two probability distributions a i and a j , the correspond a ij will be 0. Otherwise, it will have a positive value. On the other extreme end, if the two distributions are identical and all concentrates on one single word, it will have a maximum value of 1. We subtract an identity matrix from AA T so that forces the elements on the diagonal of AA T to approximate 1, which encourages each summation vector a i to focus on as few number of words as possible, forcing each vector to be focused on a single aspect, and all other elements to 0, which punishes redundancy between different summation vectors.\n\n\nVISUALIZATION\n\nThe interpretation of the sentence embedding is quite straight forward because of the existence of annotation matrix A. For each row in the sentence embedding matrix M , we have its corresponding annotation vector a i . Each element in this vector corresponds to how much contribution the LSTM hidden state of a token on that position contributes to. We can thus draw a heat map for each row of the embedding matrix M This way of visualization gives hints on what is encoded in each part of the embedding, adding an extra layer of interpretation. (See Figure 3a and 3b).\n\nThe second way of visualization can be achieved by summing up over all the annotation vectors, and then normalizing the resulting weight vector to sum up to 1. Since it sums up all aspects of semantics of a sentence, it yields a general view of what the embedding mostly focuses on. We can figure out which words the embedding takes into account a lot, and which ones are skipped by the embedding. See Figure 3c and 3d.\n\n\nRELATED WORK\n\nVarious supervised and unsupervised sentence embedding models have been mentioned in Section 1. Different from those models, our proposed method uses a new self-attention mechanism that allows it to extract different aspects of the sentence into multiple vector-representations. The matrix structure together with the penalization term gives our model a greater capacity to disentangle the latent information from the input sentence. We also do not use linguistic structures to guide our sentence representation model. Additionally, using our method we can easily create visualizations that can help in the interpretation of the learned representations.\n\nSome recent work have also proposed supervised methods that use intra/self-sentence attention. Ling et al. (2015) proposed an attention based model for word embedding, which calculates an attention weight for each word at each possible position in the context window. However this method cannot be extended to sentence level embeddings since one cannot exhaustively enumerate all possible sentences. Liu et al. (2016a) proposes a sentence level attention which has a similar motivation but done differently. They utilize the mean pooling over LSTM states as the attention source, and use that to re-weight the pooled vector representation of the sentence.\n\nApart from the previous 2 variants, we want to note that  proposed a same self attention mechanism for question encoding in their factoid QA model, which is concurrent to our work. The difference lies in that their encoding is still presented as a vector, but our attention produces a matrix representation instead, with a specially designed penalty term. We applied the model for sentiment anaysis and entailment, and their model is for factoid QA.\n\nThe LSTMN model (Cheng et al., 2016) also proposed a very successful intra-sentence level attention mechanism, which is later used by Parikh et al. (2016). We see our attention and theirs as having different granularities. LSTMN produces an attention vector for each of its hidden states during the recurrent iteration, which is sort of an \"online updating\" attention. It's more fine-grained, targeting at discovering lexical correlations between a certain word and its previous words. On the contrary, our attention mechanism is only performed once, focuses directly on the semantics that makes sense for discriminating the targets. It is less focused on relations between words, but more on the semantics of the whole sentence that each word contributes to. Computationally, our method also scales up with the sentence length better, since it doesn't require the LSTM to compute an annotation vector over all of its previous words each time when the LSTMN computes its next step.\n\n\nEXPERIMENTAL RESULTS\n\nWe first evaluate our sentence embedding model by applying it to 3 different datasets: the Age dataset, the Yelp dataset, and the Stanford Natural Language Inference (SNLI) Corpus. These 3 datasets fall into 3 different tasks, corresponding to author profiling, sentiment analysis, and textual entailment, respectively. Then we also perform a set of exploratory experiments to validate properties of various aspects for our sentence embedding model.\n\n\nAUTHOR PROFILING\n\nThe Author Profiling dataset 1 consists of Twitter tweets in English, Spanish, and Dutch. For some of the tweets, it also provides an age and gender of the user when writing the tweet. The age range are split into 5 classes: 18-24, 25-34, 35-49, 50-64, 65+. We use English tweets as input, and use those tweets to predict the age range of the user. Since we are predicting the age of users, we refer to it as Age dataset in the rest of our paper. We randomly selected 68485 tweets as training set, 4000 for development set, and 4000 for test set. Performances are also chosen to be classification accuracy.  norm of gradients to be between -0.5 and 0.5. We searched hyperparameters in a wide range and find the aforementioned set of hyperparameters yields the highest accuracy.\n\nFor our model, we use the same settings as what we did in biLSTM. We also use a 2-layer ReLU output MLP, but with 2000 hidden units. In addition, our self-attention MLP has a hidden layer with 350 units (the d a in Section 2), we choose the matrix embedding to have 30 rows (the r), and a coefficient of 1 for the penalization term.\n\nWe train all the three models until convergence and select the corresponding test set performance according to the best development set performance. Our results show that the model outperforms both of the biLSTM and CNN baselines by a significant margin.\n\n\nSENTIMENT ANALYSIS\n\nWe choose the Yelp dataset 2 for sentiment analysis task. It consists of 2.7M yelp reviews, we take the review as input and predict the number of stars the user who wrote that review assigned to the corresponding business store. We randomly select 500K review-star pairs as training set, and 2000 for development set, 2000 for test set. We tokenize the review texts by Stanford tokenizer. We use 100 dimensional word2vec as initialization for word embeddings, and tune the embedding during training across all of our experiments. The target number of stars is an integer number in the range of [1, 5], inclusive. We are treating the task as a classification task, i.e., classify a review text into one of the 5 classes. We use classification accuracy as a measurement.\n\nFor the two baseline models, we use the same setting as what we used for Author Profiling dataset, except that we are using a batch size of 32 instead. For our model, we are also using the same setting, except that we choose the hidden unit numbers in the output MLP to be 3000 instead. We also observe a significant performance gain comparining to the two baselines. (Table 1) As an interpretation of the learned sentence embedding, we use the second way of visualization described in Section 2.3 to plot heat maps for some of the reviews in the dataset. We randomly select 5 examples of negative (1 star) and positive (5 stars) reviews from the test set, when the model has a high confidence (> 0.8) in predicting the label. As shown in Figure 2, we find that the model majorly learns to capture some key factors in the review that indicate strongly on the sentiment behind the sentence. For most of the short reviews, the model manages to capture all the key factors that contribute to an extreme score, but for longer reviews, the model is still not able to capture all related factors. For example, in the 3rd review in Figure 2b), it seems that a lot of focus is spent on one single factor, i.e., the \"so much fun\", and the model puts a little amount of attention on other key points like \"highly recommend\", \"amazing food\", etc.\n\n\nTEXTUAL ENTAILMENT\n\nWe use the biggest dataset in textual entailment, the SNLI corpus (Bowman et al., 2015) for our evaluation on this task. SNLI is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral. The model will be given a pair of sentences, called hypothesis and premise respectively, and asked to tell if the semantics in the two sentences are contradicting with each other or not. It is also a classification task, so we measure the performance by accuracy.\n\nWe process the hypothesis and premise independently, and then extract the relation between the two sentence embeddings by using multiplicative interactions proposed in Memisevic (2013) (see Appendix B for details), and use a 2-layer ReLU output MLP with 4000 hidden units to map the hidden representation into classification results. Parameters of biLSTM and attention MLP are shared across hypothesis and premise. The biLSTM is 300 dimension in each direction, the attention MLP has 150 hidden units instead, and both sentence embeddings for hypothesis and premise have 30 rows (the r). The penalization term coefficient is set to 0.3. We use 300 dimensional GloVe (Pennington et al., 2014) word embedding to initialize word embeddings. We use AdaGrad as the optimizer, with a learning rate of 0.01. We don't use any extra regularization methods, like dropout or L2 normalization. Training converges after 4 epochs, which is relatively fast.\n\nThis task is a bit different from previous two tasks, in that it has 2 sentences as input. There are a bunch of ways to add inter-sentence level attention, and those attentions bring a lot of benefits.\n\nTo make the comparison focused and fair, we only compare methods that fall into the sentence encoding-based models. i.e., there is no information exchanged between the hypothesis and premise before they are encoded into some distributed encoding. \n\n\nModel\n\nTest Accuracy 300D LSTM encoders (Bowman et al., 2016) 80.6% 600D (300+300) BiLSTM encoders (Liu et al., 2016b) 83.3% 300D Tree-based CNN encoders (Mou et al., 2015a) 82.1% 300D SPINN-PI encoders (Bowman et al., 2016) 83.2% 300D NTI-SLSTM-LSTM encoders (Munkhdalai & Yu, 2016a) 83.4% 1024D GRU encoders with SkipThoughts pre-training (Vendrov et al., 2015) 81.4% 300D NSE encoders (Munkhdalai & Yu, 2016b) 84.6% Our method 84.4%\n\nWe find that compared to other published approaches, our method shows a significant gain (\u2265 1%) to them, except for the 300D NSE encoders, which is the state-of-the-art in this category. However, the 0.2% different is relatively small compared to the differences between other methods.\n\n\nEXPLORATORY EXPERIMENTS\n\nIn this subsection we are going to do a set of exploratory experiments to study the relative effect of each component in our model.\n\n\nEFFECT OF PENALIZATION TERM\n\nSince the purpose of introducing the penalization term P is majorly to discourage the redundancy in the embedding, we first directly visualize the heat maps of each row when the model is presented with a sentence. We compare two identical models with the same size as detailed in Section 4.1 trained separately on Age dataset, one with this penalization term (where the penalization coefficient is set to 1.0) and the other with no penalty. We randomly select one tweet from the test set and compare the two models by plotting a heat map for each hop of attention on that single tweet. Since there are 30 hops of attention for each model, which makes plotting all of them quite redundant, we only plot 6 of them. These 6 hops already reflect the situation in all of the 30 hops.  From the figure we can tell that the model trained without the penalization term have lots of redundancies between different hops of attention (Figure 3a), resulting in putting lot of focus on the word \"it\" (Figure 3c), which is not so relevant to the age of the author. However in the right column, the model shows more variations between different hops, and as a result, the overall embedding focuses on \"mail-replies spam\" instead. (Figure 3d) For the Yelp dataset, we also observe a similar phenomenon. To make the experiments more explorative, we choose to plot heat maps of overall attention heat maps for more samples, instead of plotting detailed heat maps for a single sample again. Figure 4 shows overall focus of the sentence embedding on three different reviews. We observe that with the penalization term, the model tends to be more focused on important parts of the review. We think it is because that we are encouraging it to be focused, in the diagonals of matrix AA T (Equation 8).\n\nTo validate if these differences result in performance difference, we evaluate four models trained on Yelp and Age datasets, both with and without the penalization term. Results are shown in Table  3. Consistent with what expected, models trained with the penalization term outperforms their counterpart trained without.\n\nIn SNLI dataset, although we observe that introducing the penalization term still contributes to encouraging the diversity of different rows in the matrix sentence embedding, and forcing the network to be more focused on the sentences, the quantitative effect of this penalization term is not so obvious on SNLI dataset. Both models yield similar test set accuracies.\n\n\nEFFECT OF MULTIPLE VECTORS\n\nHaving multiple rows in the sentence embedding is expected to provide more abundant information about the encoded content. It makes sence to evaluate how significant the improvement can be brought by r. Taking the models we used for Age and SNLI dataset as an example, we vary r from 1 to 30 for each task, and train the resulting 10 models independently ( Figure 5). Note that when r = 1, the sentence embedding reduces to a normal vector form.\n\nFrom this figure we can find that, without having multiple rows, the model performs on-par with its competitiors which use other forms of vector sentence embeddings. But there is significant difference between having only one vector for the sentence embedding and multiple vectors. The models are also quite invariant with respect to r, since in the two figures a wide range of values between 10 to 30 are all generating comparable curves.\n\n\nCONCLUSION AND DISCUSSION\n\nIn this paper, we introduced a fixed size, matrix sentence embedding with a self-attention mechanism. Because of this attention mechanism, there is a way to interpret the sentence embedding in depth in our model. Experimental results over 3 different tasks show that the model outperforms other sentence embedding models by a significant margin.\n\nIntroducing attention mechanism allows the final sentence embedding to directly access previous LSTM hidden states via the attention summation. Thus the LSTM doesn't need to carry every piece of information towards its last hidden state. Instead, each LSTM hidden state is only expected to provide shorter term context information around each word, while the higher level semantics, which requires longer term dependency, can be picked up directly by the attention mechanism. This setting reliefs the burden of LSTM to carry on long term dependencies. Our experiments also support that, as we observed that our model has a bigger advantage when the contents are longer. Further more, the notion of summing up elements in the attention mechanism is very primitive, it can be something more complex than that, which will allow more operations on the hidden states of LSTM.\n\nThe model is able to encode any sequence with variable length into a fixed size representation, without suffering from long-term dependency problems. This brings a lot of scalability to the model: without any modification, it can be applied directly to longer contents like paragraphs, articles, etc. Though this is beyond the focus of this paper, it remains an interesting direction to explore as a future work.\n\nAs a downside of our proposed model, the current training method heavily relies on downstream applications, thus we are not able to train it in an unsupervised way. The major obstacle towards enabling unsupervised learning in this model is that during decoding, we don't know as prior how the different rows in the embedding should be divided and reorganized. Exploring all those possible divisions by using a neural network could easily end up with overfitting. Although we can still do unsupervised learning on the proposed model by using a sequential decoder on top of the sentence embedding, it merits more to find some other structures as a decoder.\n\n\nAPPENDIX A PRUNED MLP FOR STRUCTURED MATRIX SENTENCE EMBEDDING\n\nAs a side effect of having multiple vectors to represent a sentence, the matrix sentence embedding is usually several times larger than vector sentence embeddings. This results in needing more parameters in the subsequent fully connected layer, which connects every hidden units to every units in the matrix sentence embedding. Actually in the example shown in Figure 1, this fully connected layer takes around 90% percent of the parameters. See Table 4. In this appendix we are going to introduce a weight pruning method which, by utilizing the 2D structure of matrix embedding, is able to drastically reduce the number of parameters in the fully connected hidden layer.\n\nInheriting the notation used in the main paper, let the matrix embedding M has a shape of r by u, and let the fully connected hidden layer has b units. The normal fully connected hidden layer will require each hidden unit to be connected to every unit in the matrix embedding, as shown in Figure  1. This ends up with r \u00d7 u \u00d7 b parameters in total.\n\nHowever there are 2-D structures in the matrix embedding, which we should make use of. Each row (m i in Figure 1) in the matrix is computed from a weighted sum of LSTM hidden states, which means they share some similarities\n\nTo reflect these similarity in the fully connected layer, we split the hidden states into r equally sized groups, with each group having p units. The i-th group is only fully connected to the i-th row in the matrix representation. All connections that connects the i-th group hidden units to other rows of the matrix are pruned away. In this way, Simillarity between different rows of matrix embedding are reflected as symmetry of connecting type in the hidden layer. As a result, the hidden layer can be interperated as also having a 2-D structute, with the number (r) and size (p) of groups as its two dimensions (The M v in Figure 6). When the total number of hidden units are the same (i.e.,  r \u00d7 p = b), this process prunes away (r \u2212 1)/r of weight values, which is a fairly large portion when r is large.\n\nOn the other dimension, another form of similarity exists too. For each vector representation m i in M , the j-th element m ij is a weighted sum of an LSTM hidden unit at different time steps. And for a certain j-th element in all vector representations, they are summed up from a same LSTM hidden unit. We can also reflect this similarity into the symmetry of weight connections by using the same pruning method we did above. Thus we will have another 2-D structured hidden states sized u-by-q, noted as M h in Figure 6. Table 4 takes the model we use for yelp dataset as a concrete example, and compared the number of parameters in each part of the model, both before and after pruning. We can see the above pruning method drastically reduces the model size. Note that the p and q in this structure can be adjusted freely as hyperparameters. Also, we can continue the corresponding pruning process on top of M v and M h over and over again, and end up with having a stack of structured hidden layers, just like stacking fully connected layers.\n\nThe subsequent softmax layer will be fully connected to both M v and M h , i.e., each unit in the softmax layer is connected to all units in M v and M h . This is not a problem since the speed of softmax is largely dependent of the number of softmax units, which is not changed.In addition, for applications like sentiment analysis and textural entailment, the softmax layer is so tiny that only contains several units.\n\nExperimental results in the three datasets has shown that, this pruning mechanism lowers performances a bit, but still allows all three models to perform comparable or better than other models compared in the paper.\n\n\nB DETAILED STRUCTURE OF THE MODEL FOR SNLI DATASET\n\nIn Section 4.3 we tested our matrix sentence embedding model for the textual entailment task on the SNLI dataset. Different from the former two tasks, the textual entailment task consists of a pair of sentences as input. We propose to use a set of multiplicative interactions to combine the two  matrix embeddings extracted for each sentence. The form of multiplicative interaction is inspired by Factored Gated Autoencoder (Memisevic, 2013).\n\nThe overall structure of our model for SNLI is dipicted in Figure 7. For both hypothesis and premise, we extract their embeddings (M h and M p in the figure) independently, with a same LSTM and attention mechanism. The parameters of this part of model are shared (rectangles with dashed orange line in the figure).\n\nComparing the two matrix embeddings corresponds to the green dashed rectangle part in the figure, which computes a single matrix embedding (F r ) as the factor of semantic relation between the two sentences. To represent the relation between M h and M p , F r can be connected to M h and M p through a three-way multiplicative interaction. In a three-way multiplicative interaction, the value of anyone of F r , M h and M p is a function of the product of the others. This type of connection is originally introduced to extract relation between images (Memisevic, 2013). Since here we are just computing the factor of relations (F r ) from M h and M p , it corresponds to the encoder part in the Factored Gated Autoencoder in Memisevic (2013). We call it Gated Encoder in Figure 7.\n\nFirst we multiply each row in the matrix embedding by a different weight matrix. Repeating it over all rows, corresponds to a batched dot product between a 2-D matrix and a 3-D weight tensor. Inheriting the name in (Memisevic, 2013), we call the resulting matrix as factor. Doing the batched dot for both hypothesis embedding and premise embedding, we have F h and F p , respectively.\nF h = batcheddot(M h , W f h ) (10) F p = batcheddot(M p , W f p )(11)\nHere W f h and W f p are the two weight tensors for hypothesis embedding and premise embedding.\n\nThe factor of the relation (F r ) is just an element-wise product of F h and F p (the triangle in the middle of Figure 7):\nF r = F h F p(12)\nHere stands for element-wise product. After the F r layer, we then use an MLP with softmax output to classify the relation into different categlories.\n\nFigure 2 :\n2Heatmap of Yelp reviews with the two extreme score.\n\nFigure 3 :Figure 4 :\n34Heat maps for 2 models trained on Age dataset. The left column is trained without the penalization term, and the right column is trained with 1.0 penalization. (a) and (b) shows detailed attentions taken by 6 out of 30 rows of the matrix embedding, while (c) and (d) shows the overall attention by summing up all 30 attention weight vectors.(a) Yelp without penalization (b) Yelp with penalization Attention of sentence embedding on 3 different Yelp reviews. The left one is trained without penalization, and the right one is trained with 1.0 penalization.\n\nFigure 5 :\n5Effect of the number of rows (r) in matrix sentence embedding. The vertical axes indicates test set accuracy and the horizontal axes indicates training epoches. Numbers in the legends stand for the corresponding values of r. (a) is conducted in Age dataset and (b) is conducted in SNLI dataset.\n\nFigure 6 :\n6Hidden layer with pruned weight connections. M is the matrix sentence embedding, M v and M h are the structured hidden representation computed by pruned weights.\n\nFigure 7 :\n7Model structure used for textual entailment task.\n\n\n... ......w 1 \nw 2 \nw 3 \nw n \nw 4 \n\nh 1 \nh 2 \nh 3 \nh 4 \n... \nh n \n\nM \n\nA i1 \nA i2 \nA i3 \nA i4 \n... \nA in \n\nm 1 m 2 \nm r \nm i \n\n... \n... \n\n... \n\n(a) \n\nh 1 h 2 \nh n \n... \n\nW s1 \n\nn \n\nn \n\ntanh \n\nsoftmax \n\nW s2 \n\nA \n\nd a \n\nr \n\nr \n\n2u \n\n(b) \n\n\n\nTable 1 :\n1Performance Comparision of Different Models on Yelp and Age DatasetModels \nYelp \nAge \nBiLSTM + Max Pooling + MLP 61.99% 77.40% \nCNN + Max Pooling + MLP \n62.05% 78.15% \nOur Model \n64.21% 80.45% \n\nWe compare our model with two baseline models: biLSTM and CNN. For the two baseline models. \nThe biLSTM model uses a bidirectional LSTM with 300 dimensions in each direction, and use max \npooling across all LSTM hidden states to get the sentence embedding vector, then use a 2-layer \nReLU output MLP with 3000 hidden states to output the classification result. The CNN model \nuses the same scheme, but substituting biLSTM with 1 layer of 1-D convolutional network. During \ntraining we use 0.5 dropout on the MLP and 0.0001 L2 regularization. We use stochastic gradient \ndescent as the optimizer, with a learning rate of 0.06, batch size 16. For biLSTM, we also clip the \n\n\nTable 2 :\n2Test Set Performance Compared to other Sentence Encoding Based Methods in SNLI Datset\n\nTable 3 :\n3Performance comparision regarding the penalization termPenalization coefficient \nYelp \nAge \n1.0 \n64.21% 80.45% \n0.0 \n61.74% 79.27% \n\n\n\nTable 4 :\n4Model Size Comparison Before and After PruningHidden layer Softmax Other Parts TotalAccuracy \n\n\n\n... ......w1 \nw2 \nw3 \nwn \nw4 \n\nM h \n\n... \n... \nw1 \nw2 \nw3 \nwn \nw4 \n\nM p \n\n... \n... \n\nF h \nF p \n\nGated Encoder \n\nHypothesis \nPremise \n\nF r \n\n\nhttp://pan.webis.de/clef16/pan16-web/author-profiling.html\nhttps://www.yelp.com/dataset challenge\nACKNOWLEDGMENTSThe authors would like to acknowledge the developers of Theano (Theano Development Team, 2016) and Lasagne. The first author would also like to thank IBM Watson for providing resources, fundings and valuable discussions to make this project possible, and Caglar Gulcehre for helpful discussions.\nA neural probabilistic language model. Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, Advances in Neural Information Processing Systems. Yoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language model. In Advances in Neural Information Processing Systems, pp. 932-938, 2001.\n\nA large annotated corpus for learning natural language inference. Gabor Samuel R Bowman, Christopher Angeli, Christopher D Potts, Manning, arXiv:1508.05326arXiv preprintSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large anno- tated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015.\n\nA fast unified model for parsing and sentence understanding. Jon Samuel R Bowman, Abhinav Gauthier, Raghav Rastogi, Gupta, D Christopher, Christopher Manning, Potts, arXiv:1603.06021arXiv preprintSamuel R Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D Manning, and Christopher Potts. A fast unified model for parsing and sentence understanding. arXiv preprint arXiv:1603.06021, 2016.\n\nLong short-term memory-networks for machine reading. Jianpeng Cheng, Li Dong, Mirella Lapata, Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. In Conference on Empirical Methods in Natural Language Processing (EMNLP). Asso- ciation for Computational Linguistics, 2016.\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, arXiv:1412.3555arXiv preprintJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n\nDeep convolutional neural networks for sentiment analysis of short texts. Santos Cicero Dos, Maira Gatti, Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. COLING 2014, the 25th International Conference on Computational Linguistics: Technical PapersCicero dos Santos and Maira Gatti. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of COLING 2014, the 25th International Conference on Computa- tional Linguistics: Technical Papers, pp. 69-78, 2014.\n\nMing Cicero Dos Santos, Bing Tan, Bowen Xiang, Zhou, arXiv:1602.03609Attentive pooling networks. arXiv preprintCicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. Attentive pooling networks. arXiv preprint arXiv:1602.03609, 2016.\n\nApplying deep learning to answer selection: a study and an open task. Minwei Feng, Bing Xiang, Michael R Glass, Lidan Wang, Bowen Zhou, 2015 IEEE Workshop on Automatic Speech Recognition and Understanding. Scottsdale, AZ, USAMinwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, and Bowen Zhou. Applying deep learn- ing to answer selection: a study and an open task. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2015, Scottsdale, AZ, USA, December 13-17, 2015, pp. 813-820, 2015.\n\nLearning distributed representations of sentences from unlabelled data. Felix Hill, Kyunghyun Cho, Anna Korhonen, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1367- 1377, San Diego, California, June 2016. Association for Computational Linguistics. URL http: //www.aclweb.org/anthology/N16-1162.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.\n\nA convolutional neural network for modelling sentences. Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom, arXiv:1404.2188arXiv preprintNal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. arXiv preprint arXiv:1404.2188, 2014.\n\nConvolutional neural networks for sentence classification. Yoon Kim, arXiv:1408.5882arXiv preprintYoon Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.\n\nAntonio Torralba, and Sanja Fidler. Skip-thought vectors. Ryan Kiros, Yukun Zhu, R Ruslan, Richard Salakhutdinov, Raquel Zemel, Urtasun, Advances in neural information processing systems. Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor- ralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing systems, pp. 3294-3302, 2015.\n\nDistributed representations of sentences and documents. V Quoc, Tomas Le, Mikolov, ICML. 14Quoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. In ICML, volume 14, pp. 1188-1196, 2014.\n\nSequential short-text classification with recurrent and convolutional neural networks. Ji Young Lee, Franck Dernoncourt, arXiv:1603.03827arXiv preprintJi Young Lee and Franck Dernoncourt. Sequential short-text classification with recurrent and con- volutional neural networks. arXiv preprint arXiv:1603.03827, 2016.\n\nDataset and neural recurrent sequence labeling model for open-domain factoid question answering. Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, Wei Xu, arXiv:1607.06275arXiv preprintPeng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. Dataset and neural recurrent sequence labeling model for open-domain factoid question answering. arXiv preprint arXiv:1607.06275, 2016.\n\nNot all contexts are created equal: Better word representations with variable attention. Wang Ling, Lin Chu-Cheng, Yulia Tsvetkov, Silvio Amir, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsWang Ling, Lin Chu-Cheng, Yulia Tsvetkov, and Silvio Amir. Not all contexts are created equal: Better word representations with variable attention. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1367-1372, Lisbon, Portugal, Septem- ber 2015. Association for Computational Linguistics.\n\nLearning natural language inference using bidirectional LSTM model and inner-attention. Yang Liu, Chengjie Sun, Lei Lin, Xiaolong Wang, abs/1605.09090CoRRYang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. Learning natural language inference using bidirectional LSTM model and inner-attention. CoRR, abs/1605.09090, 2016a.\n\nLearning natural language inference using bidirectional lstm model and inner-attention. Yang Liu, Chengjie Sun, Lei Lin, Xiaolong Wang, arXiv:1605.09090arXiv preprintYang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. Learning natural language inference using bidirectional lstm model and inner-attention. arXiv preprint arXiv:1605.09090, 2016b.\n\nDependency-based convolutional neural networks for sentence embedding. Mingbo Ma, Liang Huang, Bing Xiang, Bowen Zhou, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing2Mingbo Ma, Liang Huang, Bing Xiang, and Bowen Zhou. Dependency-based convolutional neural networks for sentence embedding. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, volume 2, pp. 174-179, 2015.\n\nA batch-normalized recurrent network for sentiment classification. Horia Margarit, Raghav Subramaniam, Advances in Neural Information Processing Systems. Horia Margarit and Raghav Subramaniam. A batch-normalized recurrent network for sentiment classification. In Advances in Neural Information Processing Systems, 2016.\n\nLearning to relate images. Roland Memisevic, IEEE transactions on pattern analysis and machine intelligence. 35Roland Memisevic. Learning to relate images. IEEE transactions on pattern analysis and machine intelligence, 35(8):1829-1846, 2013.\n\nEfficient estimation of word representations in vector space. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1301.3781arXiv preprintTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen- tations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nNatural language inference by tree-based convolution and heuristic matching. Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, Zhi Jin, arXiv:1512.08422arXiv preprintLili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. Natural language inference by tree-based convolution and heuristic matching. arXiv preprint arXiv:1512.08422, 2015a.\n\nDiscriminative neural sentence modeling by tree-based convolution. Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, Zhi Jin, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsLili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. Discriminative neural sentence model- ing by tree-based convolution. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 2315-2325, Lisbon, Portugal, September 2015b. Association for Computational Linguistics. URL http://aclweb.org/anthology/D15-1279.\n\nTsendsuren Munkhdalai, Hong Yu, arXiv:1607.04492Neural tree indexers for text understanding. arXiv preprintTsendsuren Munkhdalai and Hong Yu. Neural tree indexers for text understanding. arXiv preprint arXiv:1607.04492, 2016a.\n\nNeural semantic encoders. Tsendsuren Munkhdalai, Hong Yu, arXiv:1607.04315arXiv preprintTsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. arXiv preprint arXiv:1607.04315, 2016b.\n\nDeep sentence embedding using long short-term memory networks: Analysis and application to information retrieval. Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, Rabab Ward, IEEE/ACM Transactions on Audio, Speech, and Language Processing. 244Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward. Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval. IEEE/ACM Transactions on Audio, Speech, and Lan- guage Processing, 24(4):694-707, 2016.\n\nA decomposable attention model for natural language inference. P Ankur, Oscar Parikh, Dipanjan Tackstrom, Jakob Das, Uszkoreit, Proceedings of EMNLP. EMNLPAnkur P. Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model for natural language inference. In Proceedings of EMNLP, 2016.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, EMNLP. 14Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pp. 1532-43, 2014.\n\nSemi-supervised recursive autoencoders for predicting sentiment distributions. Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, Christopher D Manning, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingEdinburgh, Scotland, UK.Association for Computational LinguisticsRichard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pp. 151-161, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics. URL http: //www.aclweb.org/anthology/D11-1014.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Y Jean, Jason Wu, Chuang, D Christopher, Manning, Y Andrew, Christopher Ng, Potts, Proceedings of the conference on empirical methods in natural language processing. the conference on empirical methods in natural language processingCiteseer16311642Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, pp. 1642. Citeseer, 2013.\n\nImproved semantic representations from tree-structured long short-term memory networks. Kai Sheng Tai, Richard Socher, Christopher D Manning, Proceedings of ACL. ACLKai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of ACL, pp. 1556-1566, 2015.\n\nImproved representation learning for question answer matching. Ming Tan, Bing Cicero Dos Santos, Bowen Xiang, Zhou, Proceedings of ACL. ACLBerlin, GermanyAssociation for Computational LinguisticsMing Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. Improved representation learning for question answer matching. In Proceedings of ACL, pp. 464-473, Berlin, Germany, August 2016. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ P16-1044.\n\nTheano: A {Python} framework for fast computation of mathematical expressions. abs/1605.0Theano Development Team. Theano Development Team. Theano: A {Python} framework for fast computation of mathemati- cal expressions. arXiv e-prints, abs/1605.0, 2016. URL http://arxiv.org/abs/1605. 02688.\n\nIvan Vendrov, Ryan Kiros, arXiv:1511.06361Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. arXiv preprintIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. arXiv preprint arXiv:1511.06361, 2015.\n\nConvolutional neural network for paraphrase identification. Wenpeng Yin, Hinrich Sch\u00fctze, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesWenpeng Yin and Hinrich Sch\u00fctze. Convolutional neural network for paraphrase identification. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 901-911, 2015.\n", "annotations": {"author": "[{\"end\":181,\"start\":50},{\"end\":308,\"start\":182},{\"end\":442,\"start\":309},{\"end\":550,\"start\":443},{\"end\":656,\"start\":551},{\"end\":766,\"start\":657},{\"end\":892,\"start\":767},{\"end\":997,\"start\":893},{\"end\":1106,\"start\":998}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":58},{\"end\":193,\"start\":189},{\"end\":324,\"start\":316},{\"end\":452,\"start\":446},{\"end\":558,\"start\":556},{\"end\":668,\"start\":663},{\"end\":778,\"start\":774},{\"end\":899,\"start\":893},{\"end\":1008,\"start\":1002}]", "author_first_name": "[{\"end\":57,\"start\":50},{\"end\":188,\"start\":182},{\"end\":315,\"start\":309},{\"end\":445,\"start\":443},{\"end\":555,\"start\":551},{\"end\":662,\"start\":657},{\"end\":773,\"start\":767},{\"end\":1001,\"start\":998}]", "author_affiliation": "[{\"end\":180,\"start\":85},{\"end\":307,\"start\":212},{\"end\":441,\"start\":346},{\"end\":549,\"start\":454},{\"end\":655,\"start\":560},{\"end\":765,\"start\":670},{\"end\":891,\"start\":796},{\"end\":996,\"start\":901},{\"end\":1105,\"start\":1010}]", "title": "[{\"end\":47,\"start\":1},{\"end\":1153,\"start\":1107}]", "venue": null, "abstract": "[{\"end\":2015,\"start\":1199}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2195,\"start\":2174},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2216,\"start\":2195},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2486,\"start\":2467},{\"end\":2560,\"start\":2524},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2607,\"start\":2586},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2612,\"start\":2607},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2684,\"start\":2665},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3173,\"start\":3141},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3192,\"start\":3173},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3234,\"start\":3213},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3289,\"start\":3262},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3314,\"start\":3289},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3324,\"start\":3314},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3477,\"start\":3456},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3498,\"start\":3477},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3516,\"start\":3498},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3897,\"start\":3880},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3915,\"start\":3897},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3932,\"start\":3915},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4920,\"start\":4895},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5041,\"start\":5011},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15044,\"start\":15026},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15349,\"start\":15331},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16075,\"start\":16055},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16193,\"start\":16173},{\"end\":17771,\"start\":17740},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22277,\"start\":22252},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23044,\"start\":23023},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23101,\"start\":23082},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23156,\"start\":23137},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23207,\"start\":23186},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23267,\"start\":23243},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23346,\"start\":23324},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23395,\"start\":23371},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33905,\"start\":33888},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34793,\"start\":34776},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34966,\"start\":34950},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35239,\"start\":35222}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":35916,\"start\":35852},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36497,\"start\":35917},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36805,\"start\":36498},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36980,\"start\":36806},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37043,\"start\":36981},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37284,\"start\":37044},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38163,\"start\":37285},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38261,\"start\":38164},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38407,\"start\":38262},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38514,\"start\":38408},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38657,\"start\":38515}]", "paragraph": "[{\"end\":2690,\"start\":2031},{\"end\":3933,\"start\":2692},{\"end\":5042,\"start\":3935},{\"end\":6129,\"start\":5044},{\"end\":6500,\"start\":6131},{\"end\":7446,\"start\":6521},{\"end\":7541,\"start\":7448},{\"end\":7808,\"start\":7574},{\"end\":8001,\"start\":7810},{\"end\":8305,\"start\":8095},{\"end\":8676,\"start\":8338},{\"end\":9138,\"start\":8712},{\"end\":9966,\"start\":9140},{\"end\":10199,\"start\":10002},{\"end\":10412,\"start\":10201},{\"end\":10714,\"start\":10443},{\"end\":11495,\"start\":10716},{\"end\":11796,\"start\":11497},{\"end\":12077,\"start\":11817},{\"end\":12458,\"start\":12079},{\"end\":13250,\"start\":12496},{\"end\":13838,\"start\":13268},{\"end\":14259,\"start\":13840},{\"end\":14929,\"start\":14276},{\"end\":15586,\"start\":14931},{\"end\":16037,\"start\":15588},{\"end\":17020,\"start\":16039},{\"end\":17494,\"start\":17045},{\"end\":18292,\"start\":17515},{\"end\":18626,\"start\":18294},{\"end\":18882,\"start\":18628},{\"end\":19673,\"start\":18905},{\"end\":21010,\"start\":19675},{\"end\":21584,\"start\":21033},{\"end\":22528,\"start\":21586},{\"end\":22731,\"start\":22530},{\"end\":22980,\"start\":22733},{\"end\":23418,\"start\":22990},{\"end\":23705,\"start\":23420},{\"end\":23864,\"start\":23733},{\"end\":25674,\"start\":23896},{\"end\":25996,\"start\":25676},{\"end\":26365,\"start\":25998},{\"end\":26841,\"start\":26396},{\"end\":27282,\"start\":26843},{\"end\":27657,\"start\":27312},{\"end\":28529,\"start\":27659},{\"end\":28943,\"start\":28531},{\"end\":29599,\"start\":28945},{\"end\":30337,\"start\":29666},{\"end\":30687,\"start\":30339},{\"end\":30912,\"start\":30689},{\"end\":31724,\"start\":30914},{\"end\":32771,\"start\":31726},{\"end\":33192,\"start\":32773},{\"end\":33409,\"start\":33194},{\"end\":33906,\"start\":33464},{\"end\":34222,\"start\":33908},{\"end\":35005,\"start\":34224},{\"end\":35391,\"start\":35007},{\"end\":35558,\"start\":35463},{\"end\":35682,\"start\":35560},{\"end\":35851,\"start\":35701}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7573,\"start\":7542},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8094,\"start\":8002},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8337,\"start\":8306},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8711,\"start\":8677},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10001,\"start\":9967},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10422,\"start\":10413},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11816,\"start\":11797},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12495,\"start\":12459},{\"attributes\":{\"id\":\"formula_8\"},\"end\":35462,\"start\":35392},{\"attributes\":{\"id\":\"formula_9\"},\"end\":35700,\"start\":35683}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20052,\"start\":20043},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25875,\"start\":25867},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30119,\"start\":30112},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32255,\"start\":32248}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2029,\"start\":2017},{\"attributes\":{\"n\":\"2\"},\"end\":6511,\"start\":6503},{\"attributes\":{\"n\":\"2.1\"},\"end\":6519,\"start\":6514},{\"attributes\":{\"n\":\"2.2\"},\"end\":10441,\"start\":10424},{\"attributes\":{\"n\":\"2.3\"},\"end\":13266,\"start\":13253},{\"attributes\":{\"n\":\"3\"},\"end\":14274,\"start\":14262},{\"attributes\":{\"n\":\"4\"},\"end\":17043,\"start\":17023},{\"attributes\":{\"n\":\"4.1\"},\"end\":17513,\"start\":17497},{\"attributes\":{\"n\":\"4.2\"},\"end\":18903,\"start\":18885},{\"attributes\":{\"n\":\"4.3\"},\"end\":21031,\"start\":21013},{\"end\":22988,\"start\":22983},{\"attributes\":{\"n\":\"4.4\"},\"end\":23731,\"start\":23708},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":23894,\"start\":23867},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":26394,\"start\":26368},{\"attributes\":{\"n\":\"5\"},\"end\":27310,\"start\":27285},{\"end\":29664,\"start\":29602},{\"end\":33462,\"start\":33412},{\"end\":35863,\"start\":35853},{\"end\":35938,\"start\":35918},{\"end\":36509,\"start\":36499},{\"end\":36817,\"start\":36807},{\"end\":36992,\"start\":36982},{\"end\":37295,\"start\":37286},{\"end\":38174,\"start\":38165},{\"end\":38272,\"start\":38263},{\"end\":38418,\"start\":38409}]", "table": "[{\"end\":37284,\"start\":37056},{\"end\":38163,\"start\":37364},{\"end\":38407,\"start\":38329},{\"end\":38514,\"start\":38504},{\"end\":38657,\"start\":38527}]", "figure_caption": "[{\"end\":35916,\"start\":35865},{\"end\":36497,\"start\":35941},{\"end\":36805,\"start\":36511},{\"end\":36980,\"start\":36819},{\"end\":37043,\"start\":36994},{\"end\":37056,\"start\":37046},{\"end\":37364,\"start\":37297},{\"end\":38261,\"start\":38176},{\"end\":38329,\"start\":38274},{\"end\":38504,\"start\":38420},{\"end\":38527,\"start\":38517}]", "figure_ref": "[{\"end\":4412,\"start\":4404},{\"end\":7038,\"start\":7030},{\"end\":7423,\"start\":7415},{\"end\":13829,\"start\":13820},{\"end\":14251,\"start\":14242},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20422,\"start\":20414},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20810,\"start\":20800},{\"end\":24830,\"start\":24819},{\"end\":24893,\"start\":24883},{\"end\":25122,\"start\":25111},{\"end\":25376,\"start\":25368},{\"end\":25672,\"start\":25661},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26761,\"start\":26753},{\"end\":30035,\"start\":30027},{\"end\":30637,\"start\":30628},{\"end\":30801,\"start\":30793},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31549,\"start\":31541},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32246,\"start\":32238},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33975,\"start\":33967},{\"end\":34221,\"start\":34214},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35004,\"start\":34996},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35681,\"start\":35672}]", "bib_author_first_name": "[{\"end\":39112,\"start\":39106},{\"end\":39127,\"start\":39121},{\"end\":39144,\"start\":39138},{\"end\":39441,\"start\":39436},{\"end\":39470,\"start\":39459},{\"end\":39492,\"start\":39479},{\"end\":39788,\"start\":39785},{\"end\":39813,\"start\":39806},{\"end\":39830,\"start\":39824},{\"end\":39848,\"start\":39847},{\"end\":39873,\"start\":39862},{\"end\":40190,\"start\":40182},{\"end\":40200,\"start\":40198},{\"end\":40214,\"start\":40207},{\"end\":40649,\"start\":40641},{\"end\":40663,\"start\":40657},{\"end\":40683,\"start\":40674},{\"end\":40695,\"start\":40689},{\"end\":40997,\"start\":40991},{\"end\":41015,\"start\":41010},{\"end\":41472,\"start\":41468},{\"end\":41496,\"start\":41492},{\"end\":41507,\"start\":41502},{\"end\":41780,\"start\":41774},{\"end\":41791,\"start\":41787},{\"end\":41806,\"start\":41799},{\"end\":41808,\"start\":41807},{\"end\":41821,\"start\":41816},{\"end\":41833,\"start\":41828},{\"end\":42296,\"start\":42291},{\"end\":42312,\"start\":42303},{\"end\":42322,\"start\":42318},{\"end\":43100,\"start\":43096},{\"end\":43119,\"start\":43113},{\"end\":43322,\"start\":43319},{\"end\":43343,\"start\":43337},{\"end\":43362,\"start\":43358},{\"end\":43616,\"start\":43612},{\"end\":43821,\"start\":43817},{\"end\":43834,\"start\":43829},{\"end\":43841,\"start\":43840},{\"end\":43857,\"start\":43850},{\"end\":43879,\"start\":43873},{\"end\":44218,\"start\":44217},{\"end\":44230,\"start\":44225},{\"end\":44468,\"start\":44466},{\"end\":44474,\"start\":44469},{\"end\":44486,\"start\":44480},{\"end\":44797,\"start\":44793},{\"end\":44805,\"start\":44802},{\"end\":44818,\"start\":44810},{\"end\":44830,\"start\":44823},{\"end\":44841,\"start\":44837},{\"end\":44850,\"start\":44847},{\"end\":44860,\"start\":44857},{\"end\":45201,\"start\":45197},{\"end\":45211,\"start\":45208},{\"end\":45228,\"start\":45223},{\"end\":45245,\"start\":45239},{\"end\":45894,\"start\":45890},{\"end\":45908,\"start\":45900},{\"end\":45917,\"start\":45914},{\"end\":45931,\"start\":45923},{\"end\":46218,\"start\":46214},{\"end\":46232,\"start\":46224},{\"end\":46241,\"start\":46238},{\"end\":46255,\"start\":46247},{\"end\":46550,\"start\":46544},{\"end\":46560,\"start\":46555},{\"end\":46572,\"start\":46568},{\"end\":46585,\"start\":46580},{\"end\":47293,\"start\":47288},{\"end\":47310,\"start\":47304},{\"end\":47575,\"start\":47569},{\"end\":47853,\"start\":47848},{\"end\":47866,\"start\":47863},{\"end\":47877,\"start\":47873},{\"end\":47894,\"start\":47887},{\"end\":48171,\"start\":48167},{\"end\":48180,\"start\":48177},{\"end\":48188,\"start\":48186},{\"end\":48196,\"start\":48193},{\"end\":48203,\"start\":48201},{\"end\":48214,\"start\":48211},{\"end\":48223,\"start\":48220},{\"end\":48514,\"start\":48510},{\"end\":48523,\"start\":48520},{\"end\":48532,\"start\":48530},{\"end\":48540,\"start\":48537},{\"end\":48547,\"start\":48545},{\"end\":48558,\"start\":48555},{\"end\":49144,\"start\":49134},{\"end\":49161,\"start\":49157},{\"end\":49398,\"start\":49388},{\"end\":49415,\"start\":49411},{\"end\":49671,\"start\":49666},{\"end\":49683,\"start\":49681},{\"end\":49696,\"start\":49690},{\"end\":49711,\"start\":49703},{\"end\":49725,\"start\":49717},{\"end\":49737,\"start\":49730},{\"end\":49751,\"start\":49744},{\"end\":49763,\"start\":49758},{\"end\":50213,\"start\":50212},{\"end\":50226,\"start\":50221},{\"end\":50243,\"start\":50235},{\"end\":50260,\"start\":50255},{\"end\":50522,\"start\":50515},{\"end\":50542,\"start\":50535},{\"end\":50564,\"start\":50551},{\"end\":50820,\"start\":50813},{\"end\":50836,\"start\":50829},{\"end\":50853,\"start\":50849},{\"end\":50855,\"start\":50854},{\"end\":50869,\"start\":50863},{\"end\":50871,\"start\":50870},{\"end\":50887,\"start\":50876},{\"end\":50889,\"start\":50888},{\"end\":51613,\"start\":51606},{\"end\":51626,\"start\":51622},{\"end\":51639,\"start\":51638},{\"end\":51651,\"start\":51646},{\"end\":51665,\"start\":51664},{\"end\":51689,\"start\":51688},{\"end\":51709,\"start\":51698},{\"end\":52312,\"start\":52303},{\"end\":52325,\"start\":52318},{\"end\":52345,\"start\":52334},{\"end\":52347,\"start\":52346},{\"end\":52639,\"start\":52635},{\"end\":52649,\"start\":52645},{\"end\":52674,\"start\":52669},{\"end\":53341,\"start\":53337},{\"end\":53355,\"start\":53351},{\"end\":53676,\"start\":53669},{\"end\":53689,\"start\":53682}]", "bib_author_last_name": "[{\"end\":39119,\"start\":39113},{\"end\":39136,\"start\":39128},{\"end\":39152,\"start\":39145},{\"end\":39457,\"start\":39442},{\"end\":39477,\"start\":39471},{\"end\":39498,\"start\":39493},{\"end\":39507,\"start\":39500},{\"end\":39804,\"start\":39789},{\"end\":39822,\"start\":39814},{\"end\":39838,\"start\":39831},{\"end\":39845,\"start\":39840},{\"end\":39860,\"start\":39849},{\"end\":39881,\"start\":39874},{\"end\":39888,\"start\":39883},{\"end\":40196,\"start\":40191},{\"end\":40205,\"start\":40201},{\"end\":40221,\"start\":40215},{\"end\":40655,\"start\":40650},{\"end\":40672,\"start\":40664},{\"end\":40687,\"start\":40684},{\"end\":40702,\"start\":40696},{\"end\":41008,\"start\":40998},{\"end\":41021,\"start\":41016},{\"end\":41490,\"start\":41473},{\"end\":41500,\"start\":41497},{\"end\":41513,\"start\":41508},{\"end\":41519,\"start\":41515},{\"end\":41785,\"start\":41781},{\"end\":41797,\"start\":41792},{\"end\":41814,\"start\":41809},{\"end\":41826,\"start\":41822},{\"end\":41838,\"start\":41834},{\"end\":42301,\"start\":42297},{\"end\":42316,\"start\":42313},{\"end\":42331,\"start\":42323},{\"end\":43111,\"start\":43101},{\"end\":43131,\"start\":43120},{\"end\":43335,\"start\":43323},{\"end\":43356,\"start\":43344},{\"end\":43370,\"start\":43363},{\"end\":43620,\"start\":43617},{\"end\":43827,\"start\":43822},{\"end\":43838,\"start\":43835},{\"end\":43848,\"start\":43842},{\"end\":43871,\"start\":43858},{\"end\":43885,\"start\":43880},{\"end\":43894,\"start\":43887},{\"end\":44223,\"start\":44219},{\"end\":44233,\"start\":44231},{\"end\":44242,\"start\":44235},{\"end\":44478,\"start\":44475},{\"end\":44498,\"start\":44487},{\"end\":44800,\"start\":44798},{\"end\":44808,\"start\":44806},{\"end\":44821,\"start\":44819},{\"end\":44835,\"start\":44831},{\"end\":44845,\"start\":44842},{\"end\":44855,\"start\":44851},{\"end\":44863,\"start\":44861},{\"end\":45206,\"start\":45202},{\"end\":45221,\"start\":45212},{\"end\":45237,\"start\":45229},{\"end\":45250,\"start\":45246},{\"end\":45898,\"start\":45895},{\"end\":45912,\"start\":45909},{\"end\":45921,\"start\":45918},{\"end\":45936,\"start\":45932},{\"end\":46222,\"start\":46219},{\"end\":46236,\"start\":46233},{\"end\":46245,\"start\":46242},{\"end\":46260,\"start\":46256},{\"end\":46553,\"start\":46551},{\"end\":46566,\"start\":46561},{\"end\":46578,\"start\":46573},{\"end\":46590,\"start\":46586},{\"end\":47302,\"start\":47294},{\"end\":47322,\"start\":47311},{\"end\":47585,\"start\":47576},{\"end\":47861,\"start\":47854},{\"end\":47871,\"start\":47867},{\"end\":47885,\"start\":47878},{\"end\":47899,\"start\":47895},{\"end\":48175,\"start\":48172},{\"end\":48184,\"start\":48181},{\"end\":48191,\"start\":48189},{\"end\":48199,\"start\":48197},{\"end\":48209,\"start\":48204},{\"end\":48218,\"start\":48215},{\"end\":48227,\"start\":48224},{\"end\":48518,\"start\":48515},{\"end\":48528,\"start\":48524},{\"end\":48535,\"start\":48533},{\"end\":48543,\"start\":48541},{\"end\":48553,\"start\":48548},{\"end\":48562,\"start\":48559},{\"end\":49155,\"start\":49145},{\"end\":49164,\"start\":49162},{\"end\":49409,\"start\":49399},{\"end\":49418,\"start\":49416},{\"end\":49679,\"start\":49672},{\"end\":49688,\"start\":49684},{\"end\":49701,\"start\":49697},{\"end\":49715,\"start\":49712},{\"end\":49728,\"start\":49726},{\"end\":49742,\"start\":49738},{\"end\":49756,\"start\":49752},{\"end\":49768,\"start\":49764},{\"end\":50219,\"start\":50214},{\"end\":50233,\"start\":50227},{\"end\":50253,\"start\":50244},{\"end\":50264,\"start\":50261},{\"end\":50275,\"start\":50266},{\"end\":50533,\"start\":50523},{\"end\":50549,\"start\":50543},{\"end\":50572,\"start\":50565},{\"end\":50827,\"start\":50821},{\"end\":50847,\"start\":50837},{\"end\":50861,\"start\":50856},{\"end\":50874,\"start\":50872},{\"end\":50897,\"start\":50890},{\"end\":51620,\"start\":51614},{\"end\":51636,\"start\":51627},{\"end\":51644,\"start\":51640},{\"end\":51654,\"start\":51652},{\"end\":51662,\"start\":51656},{\"end\":51677,\"start\":51666},{\"end\":51686,\"start\":51679},{\"end\":51696,\"start\":51690},{\"end\":51712,\"start\":51710},{\"end\":51719,\"start\":51714},{\"end\":52316,\"start\":52313},{\"end\":52332,\"start\":52326},{\"end\":52355,\"start\":52348},{\"end\":52643,\"start\":52640},{\"end\":52667,\"start\":52650},{\"end\":52680,\"start\":52675},{\"end\":52686,\"start\":52682},{\"end\":53349,\"start\":53342},{\"end\":53361,\"start\":53356},{\"end\":53680,\"start\":53677},{\"end\":53697,\"start\":53690}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":221275765},\"end\":39368,\"start\":39067},{\"attributes\":{\"doi\":\"arXiv:1508.05326\",\"id\":\"b1\"},\"end\":39722,\"start\":39370},{\"attributes\":{\"doi\":\"arXiv:1603.06021\",\"id\":\"b2\"},\"end\":40127,\"start\":39724},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6506243},\"end\":40561,\"start\":40129},{\"attributes\":{\"doi\":\"arXiv:1412.3555\",\"id\":\"b4\"},\"end\":40915,\"start\":40563},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15874232},\"end\":41466,\"start\":40917},{\"attributes\":{\"doi\":\"arXiv:1602.03609\",\"id\":\"b6\"},\"end\":41702,\"start\":41468},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3477924},\"end\":42217,\"start\":41704},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2937095},\"end\":43070,\"start\":42219},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1915014},\"end\":43261,\"start\":43072},{\"attributes\":{\"doi\":\"arXiv:1404.2188\",\"id\":\"b10\"},\"end\":43551,\"start\":43263},{\"attributes\":{\"doi\":\"arXiv:1408.5882\",\"id\":\"b11\"},\"end\":43757,\"start\":43553},{\"attributes\":{\"id\":\"b12\"},\"end\":44159,\"start\":43759},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2407601},\"end\":44377,\"start\":44161},{\"attributes\":{\"doi\":\"arXiv:1603.03827\",\"id\":\"b14\"},\"end\":44694,\"start\":44379},{\"attributes\":{\"doi\":\"arXiv:1607.06275\",\"id\":\"b15\"},\"end\":45106,\"start\":44696},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1144632},\"end\":45800,\"start\":45108},{\"attributes\":{\"doi\":\"abs/1605.09090\",\"id\":\"b17\"},\"end\":46124,\"start\":45802},{\"attributes\":{\"doi\":\"arXiv:1605.09090\",\"id\":\"b18\"},\"end\":46471,\"start\":46126},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1727568},\"end\":47219,\"start\":46473},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":29193636},\"end\":47540,\"start\":47221},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":7197251},\"end\":47784,\"start\":47542},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b22\"},\"end\":48088,\"start\":47786},{\"attributes\":{\"doi\":\"arXiv:1512.08422\",\"id\":\"b23\"},\"end\":48441,\"start\":48090},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10728540},\"end\":49132,\"start\":48443},{\"attributes\":{\"doi\":\"arXiv:1607.04492\",\"id\":\"b25\"},\"end\":49360,\"start\":49134},{\"attributes\":{\"doi\":\"arXiv:1607.04315\",\"id\":\"b26\"},\"end\":49550,\"start\":49362},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3337266},\"end\":50147,\"start\":49552},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":8495258},\"end\":50466,\"start\":50149},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1957433},\"end\":50732,\"start\":50468},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3116311},\"end\":51525,\"start\":50734},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":990233},\"end\":52213,\"start\":51527},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3033526},\"end\":52570,\"start\":52215},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12320170},\"end\":53042,\"start\":52572},{\"attributes\":{\"doi\":\"abs/1605.0\",\"id\":\"b34\",\"matched_paper_id\":8993325},\"end\":53335,\"start\":53044},{\"attributes\":{\"doi\":\"arXiv:1511.06361\",\"id\":\"b35\"},\"end\":53607,\"start\":53337},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":17578970},\"end\":54228,\"start\":53609}]", "bib_title": "[{\"end\":39104,\"start\":39067},{\"end\":40180,\"start\":40129},{\"end\":40989,\"start\":40917},{\"end\":41772,\"start\":41704},{\"end\":42289,\"start\":42219},{\"end\":43094,\"start\":43072},{\"end\":43815,\"start\":43759},{\"end\":44215,\"start\":44161},{\"end\":45195,\"start\":45108},{\"end\":46542,\"start\":46473},{\"end\":47286,\"start\":47221},{\"end\":47567,\"start\":47542},{\"end\":48508,\"start\":48443},{\"end\":49664,\"start\":49552},{\"end\":50210,\"start\":50149},{\"end\":50513,\"start\":50468},{\"end\":50811,\"start\":50734},{\"end\":51604,\"start\":51527},{\"end\":52301,\"start\":52215},{\"end\":52633,\"start\":52572},{\"end\":53121,\"start\":53044},{\"end\":53667,\"start\":53609}]", "bib_author": "[{\"end\":39121,\"start\":39106},{\"end\":39138,\"start\":39121},{\"end\":39154,\"start\":39138},{\"end\":39459,\"start\":39436},{\"end\":39479,\"start\":39459},{\"end\":39500,\"start\":39479},{\"end\":39509,\"start\":39500},{\"end\":39806,\"start\":39785},{\"end\":39824,\"start\":39806},{\"end\":39840,\"start\":39824},{\"end\":39847,\"start\":39840},{\"end\":39862,\"start\":39847},{\"end\":39883,\"start\":39862},{\"end\":39890,\"start\":39883},{\"end\":40198,\"start\":40182},{\"end\":40207,\"start\":40198},{\"end\":40223,\"start\":40207},{\"end\":40657,\"start\":40641},{\"end\":40674,\"start\":40657},{\"end\":40689,\"start\":40674},{\"end\":40704,\"start\":40689},{\"end\":41010,\"start\":40991},{\"end\":41023,\"start\":41010},{\"end\":41492,\"start\":41468},{\"end\":41502,\"start\":41492},{\"end\":41515,\"start\":41502},{\"end\":41521,\"start\":41515},{\"end\":41787,\"start\":41774},{\"end\":41799,\"start\":41787},{\"end\":41816,\"start\":41799},{\"end\":41828,\"start\":41816},{\"end\":41840,\"start\":41828},{\"end\":42303,\"start\":42291},{\"end\":42318,\"start\":42303},{\"end\":42333,\"start\":42318},{\"end\":43113,\"start\":43096},{\"end\":43133,\"start\":43113},{\"end\":43337,\"start\":43319},{\"end\":43358,\"start\":43337},{\"end\":43372,\"start\":43358},{\"end\":43622,\"start\":43612},{\"end\":43829,\"start\":43817},{\"end\":43840,\"start\":43829},{\"end\":43850,\"start\":43840},{\"end\":43873,\"start\":43850},{\"end\":43887,\"start\":43873},{\"end\":43896,\"start\":43887},{\"end\":44225,\"start\":44217},{\"end\":44235,\"start\":44225},{\"end\":44244,\"start\":44235},{\"end\":44480,\"start\":44466},{\"end\":44500,\"start\":44480},{\"end\":44802,\"start\":44793},{\"end\":44810,\"start\":44802},{\"end\":44823,\"start\":44810},{\"end\":44837,\"start\":44823},{\"end\":44847,\"start\":44837},{\"end\":44857,\"start\":44847},{\"end\":44865,\"start\":44857},{\"end\":45208,\"start\":45197},{\"end\":45223,\"start\":45208},{\"end\":45239,\"start\":45223},{\"end\":45252,\"start\":45239},{\"end\":45900,\"start\":45890},{\"end\":45914,\"start\":45900},{\"end\":45923,\"start\":45914},{\"end\":45938,\"start\":45923},{\"end\":46224,\"start\":46214},{\"end\":46238,\"start\":46224},{\"end\":46247,\"start\":46238},{\"end\":46262,\"start\":46247},{\"end\":46555,\"start\":46544},{\"end\":46568,\"start\":46555},{\"end\":46580,\"start\":46568},{\"end\":46592,\"start\":46580},{\"end\":47304,\"start\":47288},{\"end\":47324,\"start\":47304},{\"end\":47587,\"start\":47569},{\"end\":47863,\"start\":47848},{\"end\":47873,\"start\":47863},{\"end\":47887,\"start\":47873},{\"end\":47901,\"start\":47887},{\"end\":48177,\"start\":48167},{\"end\":48186,\"start\":48177},{\"end\":48193,\"start\":48186},{\"end\":48201,\"start\":48193},{\"end\":48211,\"start\":48201},{\"end\":48220,\"start\":48211},{\"end\":48229,\"start\":48220},{\"end\":48520,\"start\":48510},{\"end\":48530,\"start\":48520},{\"end\":48537,\"start\":48530},{\"end\":48545,\"start\":48537},{\"end\":48555,\"start\":48545},{\"end\":48564,\"start\":48555},{\"end\":49157,\"start\":49134},{\"end\":49166,\"start\":49157},{\"end\":49411,\"start\":49388},{\"end\":49420,\"start\":49411},{\"end\":49681,\"start\":49666},{\"end\":49690,\"start\":49681},{\"end\":49703,\"start\":49690},{\"end\":49717,\"start\":49703},{\"end\":49730,\"start\":49717},{\"end\":49744,\"start\":49730},{\"end\":49758,\"start\":49744},{\"end\":49770,\"start\":49758},{\"end\":50221,\"start\":50212},{\"end\":50235,\"start\":50221},{\"end\":50255,\"start\":50235},{\"end\":50266,\"start\":50255},{\"end\":50277,\"start\":50266},{\"end\":50535,\"start\":50515},{\"end\":50551,\"start\":50535},{\"end\":50574,\"start\":50551},{\"end\":50829,\"start\":50813},{\"end\":50849,\"start\":50829},{\"end\":50863,\"start\":50849},{\"end\":50876,\"start\":50863},{\"end\":50899,\"start\":50876},{\"end\":51622,\"start\":51606},{\"end\":51638,\"start\":51622},{\"end\":51646,\"start\":51638},{\"end\":51656,\"start\":51646},{\"end\":51664,\"start\":51656},{\"end\":51679,\"start\":51664},{\"end\":51688,\"start\":51679},{\"end\":51698,\"start\":51688},{\"end\":51714,\"start\":51698},{\"end\":51721,\"start\":51714},{\"end\":52318,\"start\":52303},{\"end\":52334,\"start\":52318},{\"end\":52357,\"start\":52334},{\"end\":52645,\"start\":52635},{\"end\":52669,\"start\":52645},{\"end\":52682,\"start\":52669},{\"end\":52688,\"start\":52682},{\"end\":53351,\"start\":53337},{\"end\":53363,\"start\":53351},{\"end\":53682,\"start\":53669},{\"end\":53699,\"start\":53682}]", "bib_venue": "[{\"end\":41226,\"start\":41133},{\"end\":41929,\"start\":41910},{\"end\":42625,\"start\":42477},{\"end\":45427,\"start\":45340},{\"end\":46901,\"start\":46755},{\"end\":48739,\"start\":48652},{\"end\":50304,\"start\":50299},{\"end\":51082,\"start\":50987},{\"end\":51870,\"start\":51804},{\"end\":52380,\"start\":52377},{\"end\":52726,\"start\":52708},{\"end\":53970,\"start\":53843},{\"end\":39203,\"start\":39154},{\"end\":39434,\"start\":39370},{\"end\":39783,\"start\":39724},{\"end\":40336,\"start\":40223},{\"end\":40639,\"start\":40563},{\"end\":41131,\"start\":41023},{\"end\":41563,\"start\":41537},{\"end\":41908,\"start\":41840},{\"end\":42475,\"start\":42333},{\"end\":43151,\"start\":43133},{\"end\":43317,\"start\":43263},{\"end\":43610,\"start\":43553},{\"end\":43945,\"start\":43896},{\"end\":44248,\"start\":44244},{\"end\":44464,\"start\":44379},{\"end\":44791,\"start\":44696},{\"end\":45338,\"start\":45252},{\"end\":45888,\"start\":45802},{\"end\":46212,\"start\":46126},{\"end\":46753,\"start\":46592},{\"end\":47373,\"start\":47324},{\"end\":47649,\"start\":47587},{\"end\":47846,\"start\":47786},{\"end\":48165,\"start\":48090},{\"end\":48650,\"start\":48564},{\"end\":49225,\"start\":49182},{\"end\":49386,\"start\":49362},{\"end\":49833,\"start\":49770},{\"end\":50297,\"start\":50277},{\"end\":50579,\"start\":50574},{\"end\":50985,\"start\":50899},{\"end\":51802,\"start\":51721},{\"end\":52375,\"start\":52357},{\"end\":52706,\"start\":52688},{\"end\":53156,\"start\":53133},{\"end\":53452,\"start\":53379},{\"end\":53841,\"start\":53699}]"}}}, "year": 2023, "month": 12, "day": 17}