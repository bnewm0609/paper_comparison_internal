{"id": 201058721, "updated": "2023-10-06 23:42:48.814", "metadata": {"title": "Context-Aware Emotion Recognition Networks", "authors": "[{\"first\":\"Jiyoung\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Seungryong\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Sunok\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Jungin\",\"last\":\"Park\",\"middle\":[]},{\"first\":\"Kwanghoon\",\"last\":\"Sohn\",\"middle\":[]}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": 8, "day": 16}, "abstract": "Traditional techniques for emotion recognition have focused on the facial expression analysis only, thus providing limited ability to encode context that comprehensively represents the emotional responses. We present deep networks for context-aware emotion recognition, called CAER-Net, that exploit not only human facial expression but also context information in a joint and boosting manner. The key idea is to hide human faces in a visual scene and seek other contexts based on an attention mechanism. Our networks consist of two sub-networks, including two-stream encoding networks to seperately extract the features of face and context regions, and adaptive fusion networks to fuse such features in an adaptive fashion. We also introduce a novel benchmark for context-aware emotion recognition, called CAER, that is more appropriate than existing benchmarks both qualitatively and quantitatively. On several benchmarks, CAER-Net proves the effect of context for emotion recognition. Our dataset is available at http://caer-dataset.github.io.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1908.05913", "mag": "3001529617", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/LeeKKPS19", "doi": "10.1109/iccv.2019.01024"}}, "content": {"source": {"pdf_hash": "7f7618f2f315cb06b65925b4d497da30448c4f4d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1908.05913v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1908.05913", "status": "GREEN"}}, "grobid": {"id": "8994d0a65e1d362d8e080290f4d78ce5b65ee66c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7f7618f2f315cb06b65925b4d497da30448c4f4d.txt", "contents": "\nContext-Aware Emotion Recognition Networks\n\n\nJiyoung Lee \n2\u00c9 cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)\nYonsei University\n\n\nSeungryong Kim seungryong.kim@epfl.ch \n2\u00c9 cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)\nYonsei University\n\n\nSunok Kim \n2\u00c9 cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)\nYonsei University\n\n\nJungin Park \n2\u00c9 cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)\nYonsei University\n\n\nKwanghoon Sohn khsohn@yonsei.ac.kr \n2\u00c9 cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)\nYonsei University\n\n\nContext-Aware Emotion Recognition Networks\n\nTraditional techniques for emotion recognition have focused on the facial expression analysis only, thus providing limited ability to encode context that comprehensively represents the emotional responses. We present deep networks for context-aware emotion recognition, called CAER-Net, that exploit not only human facial expression but also context information in a joint and boosting manner. The key idea is to hide human faces in a visual scene and seek other contexts based on an attention mechanism. Our networks consist of two sub-networks, including two-stream encoding networks to separately extract the features of face and context regions, and adaptive fusion networks to fuse such features in an adaptive fashion. We also introduce a novel benchmark for context-aware emotion recognition, called CAER, that is more appropriate than existing benchmarks both qualitatively and quantitatively. On several benchmarks, CAER-Net proves the effect of context for emotion recognition. Our dataset is available at\n\nIntroduction\n\nRecognizing human emotions from visual contents has attracted significant attention in numerous computer vision applications such as health care and human-computer interaction systems [1,2,3].\n\nPrevious researches for emotion recognition based on handcrafted features [4,5] or deep networks [6,7,8] have mainly focused on the perception of the facial expression, based on the assumption that facial images are one of the most discriminative features of emotional responses. In this regard, the most widely used datasets, such as the AFEW [9] and FER2013 [10], only provide the cropped and aligned facial images. However, those conventional methods with the facial image dataset frequently fail to provide satisfactory performance when the emotional signals in the faces are indistinguishable and ambiguous. Meanwhile, people recognize the emotion of others from not only their faces but also surrounding contexts, such as action, interaction with others, and place [11,12]. Given untrimmed videos as in Fig. 1(a), could we catch how a woman feels solely from her facial expression as in Fig. 1(b)? It is ambiguous to estimate the emotion only with cropped facial videos. However, we could easily guess the emotion as \"surprise\" with her facial expression and contexts that an another woman comes close to her as shown in Fig. 1(c). Nevertheless, such contexts have been rarely considered in most existing emotion recognition methods and benchmarks. Some methods [13,14] have shown that emotion recognition performance can be significantly boosted by considering context information such as gesture and place [13,14]. In addition, in visual sentimental analysis [15,16] that recognizes the sentiment of an image, similar to emotion recognition but not tailored to humans, the holistic visual appearance was used to encode such contexts. However, these approaches are not practical for extracting the salient context information from visual contents. Moreover, large-scale emotion recognition datasets, including various context information close in real environments, are absence.\n\nTo overcome these limitations, we present a novel framework, called Context-Aware Emotion Recogntion Networks (CAER-Net), to recognize human emotion from images and videos by exploiting not only human facial expression but also scene contexts in a joint and boosting manner, instead of only focusing on the facial regions as in most existing methods [4,5,6,7,8]. The networks are designed in a twostream architecture, including two feature encoding stream; face encoding and context encoding streams. Our key ingradient is to seek other relevant contexts by hiding human faces based on an attention mechanism, which enables the networks to reduce an ambiguity and improve an accuracy in emotion recognition. The face and context features are then fused to predict the emotion class in an adaptive fusion network by inferring an optimal fusion weight among the two-stream features.\n\nIn addition, we build a novel database, called Context-Aware Emotion Recognition (CAER), by collecting a large amount of video clips from TV shows and annotating the ground-truth emotion category. Experimental results show that CAER-Net outperforms baseline networks for contextaware emotion recognition on several benchmarks, including AFEW [9] and our CAER dataset.\n\n\nRelated Work\n\nEmotion recognition approaches. Most approaches to recognize human emotion have focused on facial expression analysis [4,5,6,7,8]. Some methods are based on the facial action coding system [17,18], where a set of localized movements of the face is used to encode facial expression. Compared to conventional methods that have relied on handcrafted features and shallow classifiers [4,5], recent deep convolutional neural networks (CNNs) based approaches have made significant progress [6]. Various techniques to capture temporal dynamics in videos have also been proposed making connections across the time using recurrent neural networks (RNNs) or deep 3D-CNNs [19,20]. However, most works have been relied on human face analysis, and thus they have limited ability to exploit context information for emotion recognition in the wild.\n\nTo solve these limitations, some approaches using other visual clues have been proposed [21,22,13,14]. Nicolaou et al. [21] used the location of shoulders and Schindler et al. [22] used the body pose to recognize six emotion categories under controlled conditions. Chen et al. [13] detected events, objects, and scenes using pre-learned CNNs and fused each score with context fusion. In [14], manually annotated body bounding boxes and holistic images were leveraged. However, [14] have a limited ability to encode dynamic signals (i.e., video) to estimate the emotion. Moreover, the aforementioned methods are a lack of prac-tical solutions to extract the sailent context information and exploit it to context-aware emotion recognition.\n\nEmotion recognition datasets. Most of the datasets that focus on detecting occurrence of expressions, such as CK+ [23] and MMI [24], have been taken in lab-controlled environments. Recently, datasets recorded in the wild condition for including naturalistic emotion states [9,25,26] have attracted much attention. AFEW benchmark [9] of the EMOTIW challenge [27] provides video frames extracted from movies and TV shows, while SFEW database [25] has been built as a static subset of the AFEW. FER-Wild [26] database contains 24,000 images that are obtained by querying emotion-related terms from search engines. MS-COCO database [28] has been recently annotated with object attributes, including some emotion categories for human, but the attributes are not intended to be exhaustive for emotion recognition, and not all people are annotated with emotion attributes. Some studies [29,30] built the database consisting of a spontaneous subset acquired under a restrictive setting to establish the relationship between emotion and body posture. EMOTIC database [14] has been introduced providing the manually annotated body regions which contains emotional state. Although these datasets investigate a different aspect of emotion recognition with contexts, a large-scale dataset for context-aware emotion recognition is absence that contains various context information.\n\nAttention inference. Since deep CNNs have achieved a great success in many computer vision areas [31,32,33], numerous attention inference models [34,35] have been investigated to identify discriminative regions where the networks attend, by mining discriminative regions [36], implicitly analyzing the higher-layer activation maps [34,35], and designing different architecture of attention modules [37,38]. Although the attention produced by these conventional methods could be used as a prior for various tasks, it only covers most discriminative regions of the object, and thus frequently fails to capture other discriminative parts that can help performance improvement.\n\nMost related methods to our work discover attentive areas for visual sentiment recognition [16,39]. Although those produce the emotion sentiment map using deep CNNs, it only focuses on image-level sentiment analysis, not human-centric emotion like us.\n\n\nProposed Method\n\n\nMotivation and Overview\n\nIn this section, we describe a simple yet effective framework for context-aware emotion recognition in images and videos that exploits the facial expression and context information in a boosting and synergistic manner. A simple solution is to use the holistic visual appearance similar to [14,13], but such a model cannot encode salient contextual regions well. Based on the intuition that emotions can be recognized by understanding the context components of scene, as well as facial expression together, we present an attention inference module that estimates the context information in images and videos. By hiding the facial regions in inputs and seeking the attention regions, our networks localize more discriminative context regions that are used to improve emotion recognition accuracy in a context-aware manner.\n\nConcretely, let us denote an image and a video that consists of a sequence of T images as I and V = {I 1 , . . . , I T }, respectively. Our objective is to infer the discrete emotion label y among K emotion labels {y 1 , . . . , y K } of the image I or video clip V with deep CNNs. To solve this problem, we present a network architecture consisting of two subnetworks, including a two-stream encoding network and an adaptive fusion network, as illustrated in Fig. 2. The twostream encoding networks consist of face stream and context stream in which facial expression and context information are encoded in the separate networks. By combining two features in the adaptive fusion network, our method attains an optimal performance for context-aware emotion recognition.\n\n\nNetwork Architectures\n\n\nTwo-stream Encoding Networks\n\nIn this section, we first present a dynamic model of our networks for analyzing videos, and then present a static model for analyzing images.\n\nFace encoding stream. As in existing facial expression analysis approaches [6,20,40], our networks also have the facial expression encoding module. We first detect and crop the facial regions using the off-the-shelf face detectors [41] to build input of face stream V F . The facial expression encoding module is designed to extract the facial expres-sion features denoted as X F from temporally stacked facecropped inputs V F by feed-foward process such that\nX F = F(V F ; W F ),(1)\nwith face stream parameters W F . The facial expression encoding module is designed based on the basic operations of 3D-CNNs which are well-suited for spatiotemporal feature representation. Compared to 2D-CNNs, 3D-CNNs have the better ability to model temporal information for videos using 3D convolution and 3D pooling operations. Specifically, the face encoding module consist of 5 convolutional layers with 3 \u00d7 3 \u00d7 3 kernels followed by batch normalization (BN), rectified linear unit (ReLU) layers and 4 max-pooling layers with stride 2 \u00d7 2 \u00d7 2 except for the first layer. The first pooling layer has a kernel size 1 \u00d7 2 \u00d7 2 with the intention of not to merge the temporal signal too early. The number of kernels for five convolution layers are 32, 64, 128, 256 and 256, respectively. The final feature X F is spatially averaged in the average-pooling layer.\n\nContext encoding stream. In comparison to the face encoding stream, the context encoding stream includes a context encoding module and an attention inference module. To extract the context information except the facial expression, we present a novel strategy that hides the faces and seeks contexts based on the attention mechanisms. Specifically, the context encoding module is designed to extract the context features denoted as X C from temporally stacked facehidden inputs V C by feed-foward process:\nX C = F(V C ; W C ),(2)\nwith context stream parameters W C . In addition, an attention inference module is learned to extract attention regions of input, enabling the context encoding stream to focus on the sailent contexts. Concretely, the attention inference module takes an intermediate feature X C as input to infer the attention A \u2208 R H\u00d7W , where H \u00d7 W is the spatial resolution of the X C . To make the sum of attention for each pixel to be 1, we spatially normalize the attention A by using the spatial softmax [42] as follows:\n\u00c2 i = exp(A i ) j exp(A j ) ,(3)\nwhere\u00c2 is the attention for context at each pixel i and j \u2208 {1, \u00b7 \u00b7 \u00b7 , H \u00d7 W }. Since we temporally aggregate the features using 3D-CNNs, we only normalize the attention weight across spatial axises not temporal axis. Note that the attention is implicitly learned in an unsupervised manner. Attention\u00c2 is then applied to the feature X C to make the attention-boosted featureX C as follows:\nX C =\u00c2 X C ,(4)\nwhere is an element-wise multiplication operator. Specifically, we use five convolution layers to extract intermediate feature volumes X C followed by BN and ReLU, and 4 max-pooling layers. All max-pooling layers except for the first layer have 2 \u00d7 2 \u00d7 2 kernel with stride 2. The first pooling layer has kernel size 1 \u00d7 2 \u00d7 2 similar to facial expression encoding stream. The number of filters for five convolution layers are 32, 64, 128, and 256, respectively. In the attention inference module, we use two convolution layers with 3 \u00d7 3 \u00d7 3 kernels producing 128 and 1 feature channels, followed by BN and ReLU layers. The final fea-tureX C is spatially averaged in the average-pooling layer.\n\nStatic model. Dynamic model described above can be simplified for emotion recognition in images. A static model, called CAER-Net-S, takes both a single frame facecropped image I F and face-hidden image I C as input. In networks, all 3D convolution layers and 3D max-pooling layers are replaced with 2D convolution layers and 2D maxpooling layers, respectively. Thus, our two types of models can be applied in various environments regardless of the data type. Fig. 3 visualizes the attention maps of static and dynamic models. As expected, our networks both with static and dynamic models localize the context information well, except for the face expression. By exploiting the temporal connectivity, the dynamic model can localize more sailent regions compared to the static model.  \n\n\nAdaptive Fusion Networks\n\nTo recognize the emotion by using the face and context information in a joint manner, the features extracted from two modules should be combined. However, a direct concatenation of different features [14] often fails to provide optimal performance. To alleviate this limitation, we build the adaptive fusion networks with an attention model for inferring an optimal fusion weight for each feature X F andX C . The attentions are learned such that \u03bb F = F(X F ; W D ) and \u03bb C = F(X C ; W E ) with network parameters W D and W E , respectively. Softmax function make the sum of these attentions to be 1, i.e., \u03bb F + \u03bb C = 1. Fig. 4 shows some examples of the attention weights, i.e., \u03bb F and \u03bb C , in CAER-Net. According to contents, the attention weights are adaptively determined to yield an optimal solution.\n\nUnlike methods using the simple concatenation [14], the learned attentions are applied to inputs as\nX A = \u03a0(X F \u03bb F ,X C \u03bb C ),(5)\nwhere \u03a0 is a concatenation operator. We then estimate the final output y for emotion category by classifier:\ny = F(X A ; W G ),(6)\nwhere W G represents the remainder parameters of the adaptive fusion networks. Specifically, the fusion networks consist of 6 convolution layers with 1 \u00d7 1 kernels. The four layers use to produce fusion attention \u03bb F and \u03bb C . While the intermediate two layers that receive each stream feature as input produce 128 channel feature, the remaining two layers produce 1 channel attention for facial and contextual features. For the two layers that act as final classifiers, the first convolution layer produces 128 channel feature followed by ReLU and dropout layers to prevent the problem of the network overfitting, and the second convolution layer produces K channel feature to estimated the emotional category. Step 1)\n\nStep 2)\n\nStep 3)\n\nUncollected clip Collected clip Figure 5. Procedure for building CAER benchmark: we divide the video clips to the shot with shot boundary detection method, and remove face-undetected shots, group-level and ambiguous shots to estimate the emotion. Finally, we annotate the emotion category.\n\n\nThe CAER Benchmark\n\nMost existing datasets [10,43] have focused on the human facial analysis, and thus they are inappropriate for context-aware emotion recogntion. In this section, we introduce a benchmark by collecting large-scale video clips from TV shows and annotating them for context-aware emotion recogntion.\n\n\nAnnotation\n\nWe first collected the video clips from 79 TV shows and then refined them using the shot boundary detector, face detector/tracking and feature clustering 1 . Each video clip was manually annotated with six emotion categories, including \"anger\", \"disgust\", \"fear\", \"happy\", \"sad\", and \"surprise\", as well as \"neutral\". Six annotators were recruited to assign the emotion category on the 20,484 clips of the initial collection. Since all the video clips have audio and visual tracks, the annotators labeled them while listening to the audio tracks for more accurate annotations. Each clip was evaluated by three different annotators. The annotation was performed blindly and independently, i.e. the annotators were not aware of the other annotator's response. Importantly, in comparison of existing datasets [9,14], confidence scores were annotated as well as emotion category, which can be thought as the probability of the annotation reliability. If two more annotators assigned the same emotion categories, the clip was remained in the database. We also removed the clips which have lower confidence average under the 0.5. Finally, 13,201 clips and about 1.1M frames were available. The videos range from short (around 30 frames) to longer clips (more than 120 frames). The average of sequence length is 90 frames. In addition, we extracted about 70K static images from CAER to create a static image subset, called CAER-S. The dataset is randomly split into training (70%), validation (10%), and testing (20%) sets. Overall stage of data acquisition and annotation is illustrated in Fig. 5. Table 1 summarizes the number of clips per each cateogry in the CAER benchmark.\n\n\nAnalysis\n\nWe compare CAER and CAER-S datasets with other widely used datasets, such as EMOTIC [14], Affect-Net [43], AFEW [44], and Video Emotion datasets [45], as shown in Table 2. According to the data type, the datasets are grouped into the static and dynamic. Even if static databases for facial expression analysis such as Af-fectNet [43] and FER-Wild [26] collect a large amount of facial expression images from the web, they have only facecropped images not including surrounding context. In addition, EMOTIC [14] do not contain human facial images, as exampled in Fig. 6, thus causing subjective and ambiguous labelling from observers. On the other hand, commonly used video emotion recognition datasets had insufficient amount of data than image-based datasets [45,46]. Compared to these datasets, the CAER dataset provides the large-scale video clips which are sufficient amount to learn the machine learning algorithms for context-aware emotion recognition.\n\n\nExperiments\n\n\nImplementation Details\n\nCAER-Net was implemented with PyTorch library [47]. We trained CAER-Net from scratch with learning rate initialized as 5 \u00d7 10 \u22123 and dropped by a factor of 10 every 4 epochs. CAER-Net was learned with the cross-entropy loss function [48] with ground-truth emotion labels with batch size to 32. As CAER dataset has various length of (a) EMOTIC [14] (b) AffectNet [43] (c) CAER Figure 6. Examples in the EMOTIC [14], AffectNet [43] and CAER. While EMOTIC includes face-unvisible images to yeild ambiguous emotion recognition, AffectNet includes face-cropped images which have limited to use of context.  Table 2. Comparison of the CAER with existing emotion recognition datasets such as EMOTIC [14], AffectNet [43], AFEW [44], and Video Emotion [45] datasets. Compared to existing datasets, CAER contains large amount of video clips for context-aware emotion recognition.\n\nvideos, we randomly extracted single non-overlapped consecutive 16 frame clips from every training video which sampled at 10 frames per second. While the clips of facial V F are resized to have the frame size of 96 \u00d7 96, the clips of contextual parts V C are resized to have the frame size of 128 \u00d7 171 and randomly cropped into 112 \u00d7 112 at training stage. We also trained static model of CAER-Net-S with CAER-S dataset with the input size of 224 \u00d7 224. To reduce the effects of overfitting, we employed the dropout scheme with the ratio of 0.5 between 1 \u00d7 1 convolution layers, and data augmentation schemes such as flips, contrast, and color changes. At testing phase, we used a single center crop per contextual parts clips. For video predictions, we split a video into 16 frame clips with a 8 frame overlap between two consecutive clips then average clip predictions of all clips.\n\n\nExperimental Settings\n\nWe evaluated CAER-Net on the CAER and AFEW dataset [9], respectively. For evaluation of the proposed networks quantitatively, we measured the emotion recognition performance by classification accuracy as used in [27]. We reproduced four classical deep network architectures before the fully-connected layers, including AlexNet [31], VGGNet [32], ResNet [33], and C3D [49], as the baseline methods. We adopt two fully-connected layers as classifiers for the baseline methods. We initialized the feature extraction modules of all the baselines using pretrained mod-  Table 3. Ablation study of CAER-Net-S and CAER-Net on the CAER-S and CAER datasets, respectively. 'F', 'C', 'cA', and 'fA' denote face encoding stream, context encoding stream, context attention module and fusion attention module, respectively. els from two large-scale classification datasets such as Im-ageNet [50] and Sports-1M [51], and fine-tuned whole networks on CAER benchmark. We trained all parameters of learning rate 10 \u22124 for fine-tuned models.\n\n\nResults on the CAER dataset\n\nAblation study. We analyzed CAER-Net-S and CAER-Net with ablation studies as varying the combination of different inputs such as cropped face and context, and attention modules such as context and fusion attention modules. For all those experiments, CAER-Net-S and CAER-Net were trained and tested on the CAER-S and CAER datasets, respectively. For quantitative analysis of ablation study, we examined the classification accuracy on the CAER benchmark as shown in Table 3. The results show that the best result can be obtained when both the face and context are used as inputs. As our baseline, CAER-Net w/F that considers facial expression only for emotion recognition provides the accuracy 74.13 %. Compared to this, our CAER-Net that fully makes use of both face and context shows the best performance. When we compared the static and dynamic models, CAER-Net shows 3.53 % improvement than CAER-Net-S, which shows the importance to consider the temporal dynamic inputs for context-aware emotion recognition. Fig. 7 demonstrates the confusion matrix of CAER-Net w/F and CAER-Net, which also verify that compared to the model that only focuses on facial stream only, a joint model that considers facial stream and context stream simultaneously can highly boost the emotion recognition performance. Happy and neutral accuracies were increased by 7.48% and 5.65%, respectively, which clearly shows that context information helps distinguishing these two categories rather than only using facial expression. Finally, we conducted an ablation study for the context attention module. First of all, when we trained CAER-Net-S and CAER-Net without hiding the face, they tended to focus on the most discriminative parts only (i.e., faces) as depicted in the preceding two columns Fig. 8. Secondly, we conducted \n\n\nMethods\n\nAcc. (%) ImageNet-AlexNet [31] 47.36 ImageNet-VGGNet [32] 49.89 ImageNet-ResNet [33] 57.33 Fine-tuned AlexNet [31] 61.73 Fine-tuned VGGNet [32] 64.85 Fine-tuned ResNet [33] 68.46 CAER-Net-S 73.51 Table 4. Quantitative evaluation of CAER-Net-S in comparison to baseline methods on the CAER-S benchmark .\n\nanother experiment on actionless frames as depicted in the second and last columns. As shown in the last two columns Fig. 8, both CAER-Net-S and CAER-Net attend to not only \"things that move\" but also the salient scene that can be an emotion signals. To summarize, our context encoding stream enables the networks to attend salient context that boost performance for both images and videos.\n\nComparison to baseline methods. In Fig. 9 and Table 4, we evaluated CAER-Net-S with baseline 2D CNNs based approaches. The standard networks including AlexNet [31], VGGNet [32], and ResNet [33] pretrained with ImageNet were reproduced for comparison with CAER-Net-S. In addition, we also fine-tuned these networks on the CAER-S dataset. Compared to these baseline methods, our CAER-Net-S improves the classification performance than finetuned ResNet by 5.05%. Moreover, CAER-Net-S consistently performs favorably against baseline deep networks on each category in the CAER-S benchmark, which illustrates that CAER-Net can learn more discriminative representation for this task. In addition, we evaluated CAER-Net with a baseline 3D CNNs based approach in Table 5. Compared to C3D [49], our CAER-Net has shown the state-of-the-art performance on the CAER benchmark.\n\nFinally, Fig. 10 shows the qualitative results with learned attention maps obtained by CAM [34] with fine-tuned VG-GNet and in context encoding stream of CAER-Net-S. Note that images in Fig. 10 were correctly classified to groundtruth emotion categories both with fine-tuned VGGNet and \n\n\nMethods\n\nAcc. (%) Sports-1M-C3D [49] 66.38 Fine-tuned C3D [49] 71.02 CAER-Net 77.04 Table 5. Quantitative evaluation of CAER-Net in comparison to C3D [49] on the CAER benchmark .\n\nCAER-Net-S. Unlike CAM [34] that only considers facial expressions, the attention mechanism in CAER-Net-S localizes context information well that can boost the emotion recognition performance in a context-aware manner.\n\n\nResults on the AFEW dataset\n\nWe conducted an additional experiment to verify the effectiveness of the CAER dataset compared to the AFEW dataset [9]. When we trained CAER-Net on the combination of CAER and AFEW datasets, the highly improvement was attained. It demonstrates that CAER dataset could be complement data distribution of the AFEW dataset. It should be noted that Fan et al. [40] has shown the better performance, they are formulated the networks with the ensemble of various networks to maximize the performance in EmotiW challenge. Unlike this, we focused on investigating how context information helps to improve the emotion recognition performance. For this purpose, we choice shallow architecture rather than Fan et al. [40]. If the face encoding stream adopt more complicated networks such  Table 6. Quantitative evaluation of CAER-Net on the AFEW [9] benchmark, as varying training datasets.\n\nFan et al. [40], the performance of CAER-Net also will be highly boosted. We reserve this as further works.\n\n\nConclusion\n\nWe presented CAER-Net that jointly exploits human facial expression and context for context-aware emotion recognition. The key idea of this approach is to seek sailent context information by hiding the facial regions with an attention mechanism, and utilize this to estimate the emotion from contexts, as well as the facial information together. We also introduced the CAER benchmark that is more appropriate for context-aware emotion recognition than existing benchmarks both qualitatively and quantitatively. We hope that the results of this study will facilitate further advances in context-aware emotion recognition and its related tasks.\n\nFigure 1 .\n1This research was supported by Next-Generation Information Computing Development Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Science and ICT (NRF-2017M3C4A7069370). Intuition of CAER-Net: for untrimmed videos as in (a), conventional methods that leverage the facial regions only as in (b) often fail to recognize emotion. Unlike these methods, CAER-Net focuses on both face and attentive context regions as in (c).\n\nFigure 2 .\n2Network configuration of CAER-Net, consisting of two-stream encoding networks and adaptive fusion networks.\n\nFigure 3 .\n3Visualization of the attention maps of (b) static and (c) dynamic context encoding models of CAER-Net.\n\nFigure 4 .\n4Some examples of the attention weights, i.e., \u03bbF and \u03bbC , in our networks.\n\nFigure 7 .Figure 8 .\n78Confusion matrix of CAER-Net with face stream only and with face and context streams on the CAER benchmark. Visualization of the attention: (from top to bottom) inputs, attention maps of CAER-Net-S and CAER-Net. (a) and (b) are results of ablation study without hiding the face during training, (c) and (d) with hiding the face.\n\nFigure 9 .\n9Quantitative evaluation of CAER-Net-S in comparison to baseline methods on each category in the CAER-S benchmark.\n\nFigure 10 .\n10Visualization of learned attention maps in CAER-Net-S: (from top to bottom) inputs, attention maps of CAM[34], inputs of context encoding stream, attention maps in context encoding stream. Note that red color indicates attentive regions and blue color indicates suppressed regions. Best viewed in color.\n\n\n1 https://github.com/pyannote/pyannote-videoTable 1. Amount of video clips in each category on CAER dataset.Category \n# of clips \n# of frames \n% \nAnger \n1,628 \n139,681 \n12.33 \nDisgust \n719 \n59,630 \n5.44 \nFear \n514 \n46,441 \n3.89 \nHappy \n2,726 \n219,377 \n20.64 \nNeutral \n4,579 \n377,276 \n34.69 \nSad \n1,473 \n138,599 \n11.16 \nSurprise \n1,562 \n126,873 \n11.83 \nTotal \n13,201 \n1,107,877 \n100 \n\n\n\n\nMethodsTraining data Acc. (%) VielZeuf et al.[52] w/F FER+AFEW 48.60 Fan et al. [19] w/F FER+AFEW 48.30 Hu et al. [53] w/F AFEW 42.55 Fan et al. [40] w/FFER+AFEW \n57.43 \nCAER-Net w/F \nAFEW \n41.86 \nCAER-Net \nCAER \n38.65 \nCAER-Net \nAFEW \n43.12 \nCAER-Net \nCAER+AFEW \n51.68 \n\n\n\nToward an affect-sensitive autotutor. D&apos; Sidney, Rosalind W Mello, Arthur Picard, Graesser, IEEE Int. Systems. Sidney D'Mello, Rosalind W Picard, and Arthur Graesser. Toward an affect-sensitive autotutor. IEEE Int. Systems, 2007.\n\nDeveloping multimodal intelligent affective interfaces for tele-home health care. Christina Lisetti, Fatma Nasoz, Cynthia Lerouge, Onur Ozyer, Kaye Alvarez, Int. Jou. of Hum.-Comp. Stud. Christina Lisetti, Fatma Nasoz, Cynthia LeRouge, Onur Ozyer, and Kaye Alvarez. Developing multimodal intelli- gent affective interfaces for tele-home health care. Int. Jou. of Hum.-Comp. Stud., 2003.\n\nExperiencedriven procedural content generation. N Georgios, Julian Yannakakis, Togelius, IEEE Trans. AC. Georgios N Yannakakis and Julian Togelius. Experience- driven procedural content generation. IEEE Trans. AC, 2011.\n\nFacial expression recognition based on local binary patterns: A comprehensive study. Caifeng Shan, Shaogang Gong, Peter W Mcowan, Image and Vis. Comput. Caifeng Shan, Shaogang Gong, and Peter W McOwan. Fa- cial expression recognition based on local binary patterns: A comprehensive study. Image and Vis. Comput., 2009.\n\nLearning active facial patches for expression analysis. Lin Zhong, Qingshan Liu, Peng Yang, Bo Liu, Junzhou Huang, Dimitris N Metaxas, CVPRLin Zhong, Qingshan Liu, Peng Yang, Bo Liu, Junzhou Huang, and Dimitris N Metaxas. Learning active facial patches for expression analysis. In: CVPR, 2012.\n\nEmotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild. C Fabian Benitez-Quiroz, Ramprakash Srinivasan, Aleix M Martinez, CVPRC Fabian Benitez-Quiroz, Ramprakash Srinivasan, and Aleix M Martinez. Emotionet: An accurate, real-time al- gorithm for the automatic annotation of a million facial ex- pressions in the wild. In: CVPR, 2016.\n\nOcclusion aware facial expression recognition using cnn with attention mechanism. Yong Li, Jiabei Zeng, Shiguang Shan, Xilin Chen, IEEE Trans. IP. Yong Li, Jiabei Zeng, Shiguang Shan, and Xilin Chen. Oc- clusion aware facial expression recognition using cnn with attention mechanism. IEEE Trans. IP, 2018.\n\nReliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. Shan Li, Weihong Deng, Junping Du, Shan Li, Weihong Deng, and JunPing Du. Reliable crowd- sourcing and deep locality-preserving learning for expres- sion recognition in the wild. 2017.\n\nActed facial expressions in the wild database. Abhinav Dhall, Roland Goecke, Simon Lucey, Tom Gedeon, TR-CS-11Technical ReportAbhinav Dhall, Roland Goecke, Simon Lucey, and Tom Gedeon. Acted facial expressions in the wild database. Tech- nical Report TR-CS-11, 2011.\n\nChallenges in representation learning: A report on three machine learning contests. J Ian, Dumitru Goodfellow, Pierre Luc Erhan, Aaron Carrier, Mehdi Courville, Ben Mirza, Will Hamner, Yichuan Cukierski, David Tang, Dong-Hyun Thaler, Lee, ICONIPIan J Goodfellow, Dumitru Erhan, Pierre Luc Carrier, Aaron Courville, Mehdi Mirza, Ben Hamner, Will Cukierski, Yichuan Tang, David Thaler, Dong-Hyun Lee, et al. Chal- lenges in representation learning: A report on three machine learning contests. In: ICONIP, 2013.\n\nContext in emotion perception. Lisa Feldman Barrett, Batja Mesquita, Maria Gendron, Curr. Dir. in Psych. Science. Lisa Feldman Barrett, Batja Mesquita, and Maria Gendron. Context in emotion perception. Curr. Dir. in Psych. Science, 2011.\n\nThe role of the parahippocampal cortex in cognition. Kestutis Elissa M Aminoff, Moshe Kveraga, Bar, Trends in cognitive sciences. Elissa M Aminoff, Kestutis Kveraga, and Moshe Bar. The role of the parahippocampal cortex in cognition. Trends in cognitive sciences, 2013.\n\nEmotion in context: Deep semantic feature fusion for video emotion recognition. Chen Chen, Zuxuan Wu, Yu-Gang Jiang, MMChen Chen, Zuxuan Wu, and Yu-Gang Jiang. Emotion in context: Deep semantic feature fusion for video emotion recognition. In: MM, 2016.\n\nEmotion recognition in context. Ronak Kosti, M Jose, Adria Alvarez, Agata Recasens, Lapedriza, CVPRRonak Kosti, Jose M Alvarez, Adria Recasens, and Agata Lapedriza. Emotion recognition in context. In: CVPR, 2017.\n\nContext-aware affective images classification based on bilayer sparse representation. Bing Li, Weihua Xiong, Weiming Hu, Xinmiao Ding, MMBing Li, Weihua Xiong, Weiming Hu, and Xinmiao Ding. Context-aware affective images classification based on bi- layer sparse representation. In: MM, 2012.\n\nWeakly supervised coupled networks for visual sentiment analysis. Jufeng Yang, Dongyu She, Yu-Kun Lai, Ming-Hsuan Paul L Rosin, Yang, CVPRJufeng Yang, Dongyu She, Yu-Kun Lai, Paul L Rosin, and Ming-Hsuan Yang. Weakly supervised coupled networks for visual sentiment analysis. In: CVPR, 2018.\n\nFacial action coding system: a technique for the measurement of facial movement. E Friesen, Paul Ekman, Palo AltoE Friesen and Paul Ekman. Facial action coding system: a technique for the measurement of facial movement. Palo Alto, 1978.\n\nDiscriminative shared gaussian processes for multiview and view-invariant facial expression recognition. Stefanos Eleftheriadis, Ognjen Rudovic, Maja Pantic, IEEE Trans. IP. Stefanos Eleftheriadis, Ognjen Rudovic, and Maja Pantic. Discriminative shared gaussian processes for multiview and view-invariant facial expression recognition. IEEE Trans. IP, 2015.\n\nVideo-based emotion recognition using cnn-rnn and c3d hybrid networks. Yin Fan, Xiangju Lu, Dian Li, Yuanliu Liu, ICMIYin Fan, Xiangju Lu, Dian Li, and Yuanliu Liu. Video-based emotion recognition using cnn-rnn and c3d hybrid networks. In: ICMI, 2016.\n\nSpatiotemporal attention based deep neural networks for emotion recognition. Jiyoung Lee, Sunok Kim, Seungryong Kim, Kwanghoon Sohn, ICASSPJiyoung Lee, Sunok Kim, Seungryong Kim, and Kwanghoon Sohn. Spatiotemporal attention based deep neural networks for emotion recognition. In: ICASSP, 2018.\n\nContinuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space. A Mihalis, Hatice Nicolaou, Maja Gunes, Pantic, IEEE Trans. AC. Mihalis A Nicolaou, Hatice Gunes, and Maja Pantic. Con- tinuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space. IEEE Trans. AC, 2011.\n\nRecognizing emotions expressed by body pose: A biologically inspired neural model. Konrad Schindler, Luc Van Gool, Beatrice De Gelder, Neur. Net. Konrad Schindler, Luc Van Gool, and Beatrice de Gelder. Recognizing emotions expressed by body pose: A biologi- cally inspired neural model. Neur. Net., 2008.\n\nThe extended cohnkanade dataset (ck+): A complete dataset for action unit and emotion-specified expression. Patrick Lucey, F Jeffrey, Takeo Cohn, Jason Kanade, Zara Saragih, Iain Ambadar, Matthews, CVPR Work. Patrick Lucey, Jeffrey F Cohn, Takeo Kanade, Jason Saragih, Zara Ambadar, and Iain Matthews. The extended cohn- kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression. In: CVPR Work., 2010.\n\nWeb-based database for facial expression analysis. Maja Pantic, Michel Valstar, Ron Rademaker, Ludo Maat, ICMEMaja Pantic, Michel Valstar, Ron Rademaker, and Ludo Maat. Web-based database for facial expression analysis. In: ICME, 2005.\n\nStatic facial expression analysis in tough conditions: Data, evaluation protocol and benchmark. Abhinav Dhall, Roland Goecke, Simon Lucey, Tom Gedeon, ICCV Work. Abhinav Dhall, Roland Goecke, Simon Lucey, and Tom Gedeon. Static facial expression analysis in tough condi- tions: Data, evaluation protocol and benchmark. In: ICCV Work., 2011.\n\nFacial expression recognition from world wild web. Ali Mollahosseini, Behzad Hasani, Michelle J Salvador, Hojjat Abdollahi, David Chan, Mohammad H Mahoor, CVPR Work. Ali Mollahosseini, Behzad Hasani, Michelle J Salvador, Ho- jjat Abdollahi, David Chan, and Mohammad H Mahoor. Fa- cial expression recognition from world wild web. In: CVPR Work., 2016.\n\nEmotiw 2016: Video and group-level emotion recognition challenges. Abhinav Dhall, Roland Goecke, Jyoti Joshi, Jesse Hoey, Tom Gedeon, ICMIAbhinav Dhall, Roland Goecke, Jyoti Joshi, Jesse Hoey, and Tom Gedeon. Emotiw 2016: Video and group-level emotion recognition challenges. In: ICMI, 2016.\n\nCoco attributes: Attributes for people, animals, and objects. G Patterson, J Hays, ECCVG. Patterson and J. Hays. Coco attributes: Attributes for people, animals, and objects. In: ECCV, 2016.\n\nRecognizing affective dimensions from body posture. Andrea Kleinsmith, Nadia Bianchi-Berthouze, ACIIAndrea Kleinsmith and Nadia Bianchi-Berthouze. Recog- nizing affective dimensions from body posture. In: ACII, 2007.\n\nAutomatic recognition of non-acted affective postures. A Kleinsmith, N Bianchi-Berthouze, A Steed, IEEE Trans. Systems. A. Kleinsmith, N. Bianchi-Berthouze, and A. Steed. Au- tomatic recognition of non-acted affective postures. IEEE Trans. Systems, 2011.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NeurIPSAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. In: NeurIPS, 2012.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPRKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In: CVPR, 2016.\n\nLearning deep features for discriminative localization. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, CVPRBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discrimi- native localization. In: CVPR, 2016.\n\nGrad-cam: Visual explanations from deep networks via gradient-based localization. R Ramprasaath, Michael Selvaraju, Abhishek Cogswell, Ramakrishna Das, Devi Vedantam, Dhruv Parikh, Batra, Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In: ICCV, 2017.\n\nTrack and transfer: Watching videos to simulate strong human supervision for weakly-supervised object detection. Krishna Kumar Singh, Fanyi Xiao, Yong Jae Lee, CVPRKrishna Kumar Singh, Fanyi Xiao, and Yong Jae Lee. Track and transfer: Watching videos to simulate strong human su- pervision for weakly-supervised object detection. In: CVPR, 2016.\n\nJoon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. Sanghyun Woo, Jongchan Park, ECCVSanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. In: ECCV, 2018.\n\nSqueeze-and-excitation networks. Jie Hu, Li Shen, Gang Sun, CVPRJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net- works. In: CVPR, 2018.\n\nVisual sentiment analysis by attending on local image regions. Quanzeng You, Jin Hailin, Jiebo Luo, AAAIQuanzeng You, Hailin Jin, and Jiebo Luo. Visual sentiment analysis by attending on local image regions. In: AAAI, 2017.\n\nVideobased emotion recognition using deeply-supervised neural networks. Yingruo Fan, C K Jacqueline, Lam, O K Victor, Li, In: ICMI. Yingruo Fan, Jacqueline CK Lam, and Victor OK Li. Video- based emotion recognition using deeply-supervised neural networks. In: ICMI, 2018.\n\nDlib-ml: A machine learning toolkit. E Davis, King, Joul. of Mach. Learn. Res. Davis E King. Dlib-ml: A machine learning toolkit. Joul. of Mach. Learn. Res., 2009.\n\nAction recognition using visual attention. Shikhar Sharma, Ryan Kiros, Ruslan Salakhutdinov, arXiv:1511.04119Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Ac- tion recognition using visual attention. arXiv:1511.04119, 2015.\n\nAffectnet: A database for facial expression, valence, and arousal computing in the wild. Ali Mollahosseini, Behzad Hasani, Mohammad H Mahoor, IEEE Trans. AC. Ali Mollahosseini, Behzad Hasani, and Mohammad H Ma- hoor. Affectnet: A database for facial expression, valence, and arousal computing in the wild. IEEE Trans. AC.\n\nCollecting large, richly annotated facial-expression databases from movies. Abhinav Dhall, Roland Goecke, Simon Lucey, Tom Gedeon, IEEE MultiAbhinav Dhall, Roland Goecke, Simon Lucey, Tom Gedeon, et al. Collecting large, richly annotated facial-expression databases from movies. IEEE Multi., 2012.\n\nPredicting emotions in user-generated videos. Yu-Gang Jiang, Baohan Xu, Xiangyang Xue, AAAIYu-Gang Jiang, Baohan Xu, and Xiangyang Xue. Predicting emotions in user-generated videos. In: AAAI, 2014.\n\nAfew-va database for valence and arousal estimation in-the-wild. Image and Vis. Comput. Jean Kossaifi, Georgios Tzimiropoulos, Sinisa Todorovic, Maja Pantic, Jean Kossaifi, Georgios Tzimiropoulos, Sinisa Todorovic, and Maja Pantic. Afew-va database for valence and arousal estimation in-the-wild. Image and Vis. Comput., 2017.\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al- ban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\nUnified confidence estimation networks for robust stereo matching. Sunok Kim, Dongbo Min, Seungryong Kim, Kwanghoon Sohn, IEEE Trans. IP. Sunok Kim, Dongbo Min, Seungryong Kim, and Kwanghoon Sohn. Unified confidence estimation net- works for robust stereo matching. IEEE Trans. IP, 2018.\n\nLearning spatiotemporal features with 3d convolutional networks. Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar Paluri, ICCVDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In: ICCV, 2015.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPRJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In: CVPR, 2009.\n\nLarge-scale video classification with convolutional neural networks. Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei, CVPRAndrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In: CVPR, 2014.\n\nTemporal multimodal fusion for video emotion classification in the wild. Valentin Vielzeuf, St\u00e9phane Pateux, Fr\u00e9d\u00e9ric Jurie, ICMIValentin Vielzeuf, St\u00e9phane Pateux, and Fr\u00e9d\u00e9ric Jurie. Tem- poral multimodal fusion for video emotion classification in the wild. In: ICMI, 2017.\n\nLearning supervised scoring ensemble for emotion recognition in the wild. Ping Hu, Dongqi Cai, Shandong Wang, Anbang Yao, Yurong Chen, ICMIPing Hu, Dongqi Cai, Shandong Wang, Anbang Yao, and Yurong Chen. Learning supervised scoring ensemble for emotion recognition in the wild. In: ICMI, 2017.\n", "annotations": {"author": "[{\"end\":128,\"start\":46},{\"end\":237,\"start\":129},{\"end\":318,\"start\":238},{\"end\":401,\"start\":319},{\"end\":507,\"start\":402}]", "publisher": null, "author_last_name": "[{\"end\":57,\"start\":54},{\"end\":143,\"start\":140},{\"end\":247,\"start\":244},{\"end\":330,\"start\":326},{\"end\":416,\"start\":412}]", "author_first_name": "[{\"end\":53,\"start\":46},{\"end\":139,\"start\":129},{\"end\":243,\"start\":238},{\"end\":325,\"start\":319},{\"end\":411,\"start\":402}]", "author_affiliation": "[{\"end\":127,\"start\":59},{\"end\":236,\"start\":168},{\"end\":317,\"start\":249},{\"end\":400,\"start\":332},{\"end\":506,\"start\":438}]", "title": "[{\"end\":43,\"start\":1},{\"end\":550,\"start\":508}]", "venue": null, "abstract": "[{\"end\":1567,\"start\":552}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1770,\"start\":1767},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1772,\"start\":1770},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1774,\"start\":1772},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1854,\"start\":1851},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1856,\"start\":1854},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1877,\"start\":1874},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1879,\"start\":1877},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1881,\"start\":1879},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2124,\"start\":2121},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2141,\"start\":2137},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2552,\"start\":2548},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2555,\"start\":2552},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3049,\"start\":3045},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3052,\"start\":3049},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3195,\"start\":3191},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3198,\"start\":3195},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3248,\"start\":3244},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3251,\"start\":3248},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4017,\"start\":4014},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4019,\"start\":4017},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4021,\"start\":4019},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4023,\"start\":4021},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4025,\"start\":4023},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4891,\"start\":4888},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5051,\"start\":5048},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5053,\"start\":5051},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5055,\"start\":5053},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5057,\"start\":5055},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5059,\"start\":5057},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5123,\"start\":5119},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5126,\"start\":5123},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5313,\"start\":5310},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5315,\"start\":5313},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5417,\"start\":5414},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5595,\"start\":5591},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5598,\"start\":5595},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5857,\"start\":5853},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5860,\"start\":5857},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5863,\"start\":5860},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5866,\"start\":5863},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5888,\"start\":5884},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5945,\"start\":5941},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6046,\"start\":6042},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6156,\"start\":6152},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6246,\"start\":6242},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6622,\"start\":6618},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6635,\"start\":6631},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6780,\"start\":6777},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6783,\"start\":6780},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6786,\"start\":6783},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6836,\"start\":6833},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6865,\"start\":6861},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6948,\"start\":6944},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7009,\"start\":7005},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7136,\"start\":7132},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7387,\"start\":7383},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7390,\"start\":7387},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7566,\"start\":7562},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7974,\"start\":7970},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7977,\"start\":7974},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7980,\"start\":7977},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8022,\"start\":8018},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8025,\"start\":8022},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8148,\"start\":8144},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8208,\"start\":8204},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8211,\"start\":8208},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8275,\"start\":8271},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8278,\"start\":8275},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8643,\"start\":8639},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8646,\"start\":8643},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9138,\"start\":9134},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9141,\"start\":9138},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10714,\"start\":10711},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10717,\"start\":10714},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10720,\"start\":10717},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10871,\"start\":10867},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13011,\"start\":13007},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15176,\"start\":15172},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15833,\"start\":15829},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17123,\"start\":17119},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17126,\"start\":17123},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18215,\"start\":18212},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18218,\"start\":18215},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19178,\"start\":19174},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19195,\"start\":19191},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19206,\"start\":19202},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19239,\"start\":19235},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19423,\"start\":19419},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19441,\"start\":19437},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19600,\"start\":19596},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19854,\"start\":19850},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19857,\"start\":19854},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20139,\"start\":20135},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20326,\"start\":20322},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20436,\"start\":20432},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20455,\"start\":20451},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20502,\"start\":20498},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20518,\"start\":20514},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20785,\"start\":20781},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20801,\"start\":20797},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20812,\"start\":20808},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20836,\"start\":20832},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21925,\"start\":21922},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22087,\"start\":22083},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22202,\"start\":22198},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22215,\"start\":22211},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22228,\"start\":22224},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22242,\"start\":22238},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22752,\"start\":22748},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":22771,\"start\":22767},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24771,\"start\":24767},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24798,\"start\":24794},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24825,\"start\":24821},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24855,\"start\":24851},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24884,\"start\":24880},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24913,\"start\":24909},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25600,\"start\":25596},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25613,\"start\":25609},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25630,\"start\":25626},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26221,\"start\":26217},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26398,\"start\":26394},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26628,\"start\":26624},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26654,\"start\":26650},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":26746,\"start\":26742},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26799,\"start\":26795},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27140,\"start\":27137},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27382,\"start\":27378},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27732,\"start\":27728},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27860,\"start\":27857},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27918,\"start\":27914},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30070,\"start\":30066},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":30703,\"start\":30699}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29140,\"start\":28668},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29261,\"start\":29141},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29377,\"start\":29262},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29465,\"start\":29378},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29818,\"start\":29466},{\"attributes\":{\"id\":\"fig_7\"},\"end\":29945,\"start\":29819},{\"attributes\":{\"id\":\"fig_8\"},\"end\":30264,\"start\":29946},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30651,\"start\":30265},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30926,\"start\":30652}]", "paragraph": "[{\"end\":1775,\"start\":1583},{\"end\":3662,\"start\":1777},{\"end\":4544,\"start\":3664},{\"end\":4913,\"start\":4546},{\"end\":5763,\"start\":4930},{\"end\":6502,\"start\":5765},{\"end\":7871,\"start\":6504},{\"end\":8546,\"start\":7873},{\"end\":8799,\"start\":8548},{\"end\":9665,\"start\":8845},{\"end\":10436,\"start\":9667},{\"end\":10634,\"start\":10493},{\"end\":11095,\"start\":10636},{\"end\":11982,\"start\":11120},{\"end\":12488,\"start\":11984},{\"end\":13023,\"start\":12513},{\"end\":13447,\"start\":13057},{\"end\":14158,\"start\":13464},{\"end\":14943,\"start\":14160},{\"end\":15781,\"start\":14972},{\"end\":15882,\"start\":15783},{\"end\":16022,\"start\":15914},{\"end\":16764,\"start\":16045},{\"end\":16773,\"start\":16766},{\"end\":16782,\"start\":16775},{\"end\":17073,\"start\":16784},{\"end\":17391,\"start\":17096},{\"end\":19077,\"start\":17406},{\"end\":20048,\"start\":19090},{\"end\":20958,\"start\":20089},{\"end\":21845,\"start\":20960},{\"end\":22893,\"start\":21871},{\"end\":24729,\"start\":22925},{\"end\":25043,\"start\":24741},{\"end\":25435,\"start\":25045},{\"end\":26301,\"start\":25437},{\"end\":26589,\"start\":26303},{\"end\":26770,\"start\":26601},{\"end\":26990,\"start\":26772},{\"end\":27901,\"start\":27022},{\"end\":28010,\"start\":27903},{\"end\":28667,\"start\":28025}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11119,\"start\":11096},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12512,\"start\":12489},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13056,\"start\":13024},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13463,\"start\":13448},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15913,\"start\":15883},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16044,\"start\":16023}]", "table_ref": "[{\"end\":19005,\"start\":18998},{\"end\":19260,\"start\":19253},{\"end\":20698,\"start\":20691},{\"end\":22443,\"start\":22436},{\"end\":23396,\"start\":23389},{\"end\":24944,\"start\":24937},{\"end\":25490,\"start\":25483},{\"end\":26199,\"start\":26192},{\"end\":26683,\"start\":26676},{\"end\":27807,\"start\":27800}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1581,\"start\":1569},{\"attributes\":{\"n\":\"2.\"},\"end\":4928,\"start\":4916},{\"attributes\":{\"n\":\"3.\"},\"end\":8817,\"start\":8802},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8843,\"start\":8820},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10460,\"start\":10439},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":10491,\"start\":10463},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":14970,\"start\":14946},{\"attributes\":{\"n\":\"4.\"},\"end\":17094,\"start\":17076},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17404,\"start\":17394},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19088,\"start\":19080},{\"attributes\":{\"n\":\"5.\"},\"end\":20062,\"start\":20051},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20087,\"start\":20065},{\"attributes\":{\"n\":\"5.2.\"},\"end\":21869,\"start\":21848},{\"attributes\":{\"n\":\"5.3.\"},\"end\":22923,\"start\":22896},{\"end\":24739,\"start\":24732},{\"end\":26599,\"start\":26592},{\"attributes\":{\"n\":\"5.4.\"},\"end\":27020,\"start\":26993},{\"attributes\":{\"n\":\"6.\"},\"end\":28023,\"start\":28013},{\"end\":28679,\"start\":28669},{\"end\":29152,\"start\":29142},{\"end\":29273,\"start\":29263},{\"end\":29389,\"start\":29379},{\"end\":29487,\"start\":29467},{\"end\":29830,\"start\":29820},{\"end\":29958,\"start\":29947}]", "table": "[{\"end\":30651,\"start\":30375},{\"end\":30926,\"start\":30807}]", "figure_caption": "[{\"end\":29140,\"start\":28681},{\"end\":29261,\"start\":29154},{\"end\":29377,\"start\":29275},{\"end\":29465,\"start\":29391},{\"end\":29818,\"start\":29490},{\"end\":29945,\"start\":29832},{\"end\":30264,\"start\":29961},{\"end\":30375,\"start\":30267},{\"end\":30807,\"start\":30654}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2592,\"start\":2586},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2676,\"start\":2670},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2913,\"start\":2904},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10133,\"start\":10127},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14625,\"start\":14619},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15601,\"start\":15595},{\"end\":16824,\"start\":16816},{\"end\":18996,\"start\":18990},{\"end\":19658,\"start\":19652},{\"end\":20473,\"start\":20465},{\"end\":23942,\"start\":23936},{\"end\":24704,\"start\":24698},{\"end\":25168,\"start\":25162},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":25478,\"start\":25472},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26319,\"start\":26312},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26496,\"start\":26489}]", "bib_author_first_name": "[{\"end\":30973,\"start\":30966},{\"end\":30990,\"start\":30982},{\"end\":30992,\"start\":30991},{\"end\":31006,\"start\":31000},{\"end\":31255,\"start\":31246},{\"end\":31270,\"start\":31265},{\"end\":31285,\"start\":31278},{\"end\":31299,\"start\":31295},{\"end\":31311,\"start\":31307},{\"end\":31601,\"start\":31600},{\"end\":31618,\"start\":31612},{\"end\":31865,\"start\":31858},{\"end\":31880,\"start\":31872},{\"end\":32152,\"start\":32149},{\"end\":32168,\"start\":32160},{\"end\":32178,\"start\":32174},{\"end\":32187,\"start\":32185},{\"end\":32200,\"start\":32193},{\"end\":32216,\"start\":32208},{\"end\":32218,\"start\":32217},{\"end\":32514,\"start\":32506},{\"end\":32541,\"start\":32531},{\"end\":32561,\"start\":32554},{\"end\":32871,\"start\":32867},{\"end\":32882,\"start\":32876},{\"end\":32897,\"start\":32889},{\"end\":32909,\"start\":32904},{\"end\":33197,\"start\":33193},{\"end\":33209,\"start\":33202},{\"end\":33223,\"start\":33216},{\"end\":33433,\"start\":33426},{\"end\":33447,\"start\":33441},{\"end\":33461,\"start\":33456},{\"end\":33472,\"start\":33469},{\"end\":33732,\"start\":33731},{\"end\":33745,\"start\":33738},{\"end\":33764,\"start\":33758},{\"end\":33768,\"start\":33765},{\"end\":33781,\"start\":33776},{\"end\":33796,\"start\":33791},{\"end\":33811,\"start\":33808},{\"end\":33823,\"start\":33819},{\"end\":33839,\"start\":33832},{\"end\":33856,\"start\":33851},{\"end\":33872,\"start\":33863},{\"end\":34201,\"start\":34189},{\"end\":34216,\"start\":34211},{\"end\":34232,\"start\":34227},{\"end\":34458,\"start\":34450},{\"end\":34482,\"start\":34477},{\"end\":34752,\"start\":34748},{\"end\":34765,\"start\":34759},{\"end\":34777,\"start\":34770},{\"end\":34960,\"start\":34955},{\"end\":34969,\"start\":34968},{\"end\":34981,\"start\":34976},{\"end\":34996,\"start\":34991},{\"end\":35227,\"start\":35223},{\"end\":35238,\"start\":35232},{\"end\":35253,\"start\":35246},{\"end\":35265,\"start\":35258},{\"end\":35502,\"start\":35496},{\"end\":35515,\"start\":35509},{\"end\":35527,\"start\":35521},{\"end\":35543,\"start\":35533},{\"end\":35805,\"start\":35804},{\"end\":35819,\"start\":35815},{\"end\":36074,\"start\":36066},{\"end\":36096,\"start\":36090},{\"end\":36110,\"start\":36106},{\"end\":36394,\"start\":36391},{\"end\":36407,\"start\":36400},{\"end\":36416,\"start\":36412},{\"end\":36428,\"start\":36421},{\"end\":36657,\"start\":36650},{\"end\":36668,\"start\":36663},{\"end\":36684,\"start\":36674},{\"end\":36699,\"start\":36690},{\"end\":36973,\"start\":36972},{\"end\":36989,\"start\":36983},{\"end\":37004,\"start\":37000},{\"end\":37305,\"start\":37299},{\"end\":37320,\"start\":37317},{\"end\":37339,\"start\":37331},{\"end\":37637,\"start\":37630},{\"end\":37646,\"start\":37645},{\"end\":37661,\"start\":37656},{\"end\":37673,\"start\":37668},{\"end\":37686,\"start\":37682},{\"end\":37700,\"start\":37696},{\"end\":38012,\"start\":38008},{\"end\":38027,\"start\":38021},{\"end\":38040,\"start\":38037},{\"end\":38056,\"start\":38052},{\"end\":38297,\"start\":38290},{\"end\":38311,\"start\":38305},{\"end\":38325,\"start\":38320},{\"end\":38336,\"start\":38333},{\"end\":38590,\"start\":38587},{\"end\":38612,\"start\":38606},{\"end\":38629,\"start\":38621},{\"end\":38631,\"start\":38630},{\"end\":38648,\"start\":38642},{\"end\":38665,\"start\":38660},{\"end\":38680,\"start\":38672},{\"end\":38682,\"start\":38681},{\"end\":38962,\"start\":38955},{\"end\":38976,\"start\":38970},{\"end\":38990,\"start\":38985},{\"end\":39003,\"start\":38998},{\"end\":39013,\"start\":39010},{\"end\":39244,\"start\":39243},{\"end\":39257,\"start\":39256},{\"end\":39431,\"start\":39425},{\"end\":39449,\"start\":39444},{\"end\":39647,\"start\":39646},{\"end\":39661,\"start\":39660},{\"end\":39682,\"start\":39681},{\"end\":39916,\"start\":39912},{\"end\":39933,\"start\":39929},{\"end\":39953,\"start\":39945},{\"end\":39955,\"start\":39954},{\"end\":40187,\"start\":40182},{\"end\":40204,\"start\":40198},{\"end\":40444,\"start\":40437},{\"end\":40456,\"start\":40449},{\"end\":40472,\"start\":40464},{\"end\":40482,\"start\":40478},{\"end\":40671,\"start\":40666},{\"end\":40684,\"start\":40678},{\"end\":40698,\"start\":40693},{\"end\":40714,\"start\":40710},{\"end\":40729,\"start\":40722},{\"end\":40980,\"start\":40979},{\"end\":41001,\"start\":40994},{\"end\":41021,\"start\":41013},{\"end\":41043,\"start\":41032},{\"end\":41053,\"start\":41049},{\"end\":41069,\"start\":41064},{\"end\":41413,\"start\":41406},{\"end\":41419,\"start\":41414},{\"end\":41432,\"start\":41427},{\"end\":41443,\"start\":41439},{\"end\":41447,\"start\":41444},{\"end\":41725,\"start\":41717},{\"end\":41739,\"start\":41731},{\"end\":41909,\"start\":41906},{\"end\":41916,\"start\":41914},{\"end\":41927,\"start\":41923},{\"end\":42091,\"start\":42083},{\"end\":42100,\"start\":42097},{\"end\":42114,\"start\":42109},{\"end\":42324,\"start\":42317},{\"end\":42331,\"start\":42330},{\"end\":42333,\"start\":42332},{\"end\":42352,\"start\":42351},{\"end\":42354,\"start\":42353},{\"end\":42556,\"start\":42555},{\"end\":42733,\"start\":42726},{\"end\":42746,\"start\":42742},{\"end\":42760,\"start\":42754},{\"end\":43008,\"start\":43005},{\"end\":43030,\"start\":43024},{\"end\":43047,\"start\":43039},{\"end\":43049,\"start\":43048},{\"end\":43322,\"start\":43315},{\"end\":43336,\"start\":43330},{\"end\":43350,\"start\":43345},{\"end\":43361,\"start\":43358},{\"end\":43591,\"start\":43584},{\"end\":43605,\"start\":43599},{\"end\":43619,\"start\":43610},{\"end\":43829,\"start\":43825},{\"end\":43848,\"start\":43840},{\"end\":43870,\"start\":43864},{\"end\":43886,\"start\":43882},{\"end\":44107,\"start\":44103},{\"end\":44119,\"start\":44116},{\"end\":44134,\"start\":44127},{\"end\":44152,\"start\":44145},{\"end\":44167,\"start\":44161},{\"end\":44181,\"start\":44174},{\"end\":44196,\"start\":44190},{\"end\":44207,\"start\":44202},{\"end\":44223,\"start\":44219},{\"end\":44236,\"start\":44232},{\"end\":44508,\"start\":44503},{\"end\":44520,\"start\":44514},{\"end\":44536,\"start\":44526},{\"end\":44551,\"start\":44542},{\"end\":44792,\"start\":44790},{\"end\":44806,\"start\":44799},{\"end\":44819,\"start\":44816},{\"end\":44835,\"start\":44828},{\"end\":44854,\"start\":44847},{\"end\":45082,\"start\":45079},{\"end\":45092,\"start\":45089},{\"end\":45106,\"start\":45099},{\"end\":45121,\"start\":45115},{\"end\":45129,\"start\":45126},{\"end\":45136,\"start\":45134},{\"end\":45366,\"start\":45360},{\"end\":45383,\"start\":45377},{\"end\":45401,\"start\":45394},{\"end\":45416,\"start\":45410},{\"end\":45429,\"start\":45424},{\"end\":45444,\"start\":45442},{\"end\":45723,\"start\":45715},{\"end\":45742,\"start\":45734},{\"end\":45759,\"start\":45751},{\"end\":45997,\"start\":45993},{\"end\":46008,\"start\":46002},{\"end\":46022,\"start\":46014},{\"end\":46035,\"start\":46029},{\"end\":46047,\"start\":46041}]", "bib_author_last_name": "[{\"end\":30980,\"start\":30974},{\"end\":30998,\"start\":30993},{\"end\":31013,\"start\":31007},{\"end\":31023,\"start\":31015},{\"end\":31263,\"start\":31256},{\"end\":31276,\"start\":31271},{\"end\":31293,\"start\":31286},{\"end\":31305,\"start\":31300},{\"end\":31319,\"start\":31312},{\"end\":31610,\"start\":31602},{\"end\":31629,\"start\":31619},{\"end\":31639,\"start\":31631},{\"end\":31870,\"start\":31866},{\"end\":31885,\"start\":31881},{\"end\":31901,\"start\":31887},{\"end\":32158,\"start\":32153},{\"end\":32172,\"start\":32169},{\"end\":32183,\"start\":32179},{\"end\":32191,\"start\":32188},{\"end\":32206,\"start\":32201},{\"end\":32226,\"start\":32219},{\"end\":32529,\"start\":32515},{\"end\":32552,\"start\":32542},{\"end\":32570,\"start\":32562},{\"end\":32874,\"start\":32872},{\"end\":32887,\"start\":32883},{\"end\":32902,\"start\":32898},{\"end\":32914,\"start\":32910},{\"end\":33200,\"start\":33198},{\"end\":33214,\"start\":33210},{\"end\":33226,\"start\":33224},{\"end\":33439,\"start\":33434},{\"end\":33454,\"start\":33448},{\"end\":33467,\"start\":33462},{\"end\":33479,\"start\":33473},{\"end\":33736,\"start\":33733},{\"end\":33756,\"start\":33746},{\"end\":33774,\"start\":33769},{\"end\":33789,\"start\":33782},{\"end\":33806,\"start\":33797},{\"end\":33817,\"start\":33812},{\"end\":33830,\"start\":33824},{\"end\":33849,\"start\":33840},{\"end\":33861,\"start\":33857},{\"end\":33879,\"start\":33873},{\"end\":33884,\"start\":33881},{\"end\":34209,\"start\":34202},{\"end\":34225,\"start\":34217},{\"end\":34240,\"start\":34233},{\"end\":34475,\"start\":34459},{\"end\":34490,\"start\":34483},{\"end\":34495,\"start\":34492},{\"end\":34757,\"start\":34753},{\"end\":34768,\"start\":34766},{\"end\":34783,\"start\":34778},{\"end\":34966,\"start\":34961},{\"end\":34974,\"start\":34970},{\"end\":34989,\"start\":34982},{\"end\":35005,\"start\":34997},{\"end\":35016,\"start\":35007},{\"end\":35230,\"start\":35228},{\"end\":35244,\"start\":35239},{\"end\":35256,\"start\":35254},{\"end\":35270,\"start\":35266},{\"end\":35507,\"start\":35503},{\"end\":35519,\"start\":35516},{\"end\":35531,\"start\":35528},{\"end\":35556,\"start\":35544},{\"end\":35562,\"start\":35558},{\"end\":35813,\"start\":35806},{\"end\":35825,\"start\":35820},{\"end\":36088,\"start\":36075},{\"end\":36104,\"start\":36097},{\"end\":36117,\"start\":36111},{\"end\":36398,\"start\":36395},{\"end\":36410,\"start\":36408},{\"end\":36419,\"start\":36417},{\"end\":36432,\"start\":36429},{\"end\":36661,\"start\":36658},{\"end\":36672,\"start\":36669},{\"end\":36688,\"start\":36685},{\"end\":36704,\"start\":36700},{\"end\":36981,\"start\":36974},{\"end\":36998,\"start\":36990},{\"end\":37010,\"start\":37005},{\"end\":37018,\"start\":37012},{\"end\":37315,\"start\":37306},{\"end\":37329,\"start\":37321},{\"end\":37349,\"start\":37340},{\"end\":37643,\"start\":37638},{\"end\":37654,\"start\":37647},{\"end\":37666,\"start\":37662},{\"end\":37680,\"start\":37674},{\"end\":37694,\"start\":37687},{\"end\":37708,\"start\":37701},{\"end\":37718,\"start\":37710},{\"end\":38019,\"start\":38013},{\"end\":38035,\"start\":38028},{\"end\":38050,\"start\":38041},{\"end\":38061,\"start\":38057},{\"end\":38303,\"start\":38298},{\"end\":38318,\"start\":38312},{\"end\":38331,\"start\":38326},{\"end\":38343,\"start\":38337},{\"end\":38604,\"start\":38591},{\"end\":38619,\"start\":38613},{\"end\":38640,\"start\":38632},{\"end\":38658,\"start\":38649},{\"end\":38670,\"start\":38666},{\"end\":38689,\"start\":38683},{\"end\":38968,\"start\":38963},{\"end\":38983,\"start\":38977},{\"end\":38996,\"start\":38991},{\"end\":39008,\"start\":39004},{\"end\":39020,\"start\":39014},{\"end\":39254,\"start\":39245},{\"end\":39262,\"start\":39258},{\"end\":39442,\"start\":39432},{\"end\":39467,\"start\":39450},{\"end\":39658,\"start\":39648},{\"end\":39679,\"start\":39662},{\"end\":39688,\"start\":39683},{\"end\":39927,\"start\":39917},{\"end\":39943,\"start\":39934},{\"end\":39962,\"start\":39956},{\"end\":40196,\"start\":40188},{\"end\":40214,\"start\":40205},{\"end\":40447,\"start\":40445},{\"end\":40462,\"start\":40457},{\"end\":40476,\"start\":40473},{\"end\":40486,\"start\":40483},{\"end\":40676,\"start\":40672},{\"end\":40691,\"start\":40685},{\"end\":40708,\"start\":40699},{\"end\":40720,\"start\":40715},{\"end\":40738,\"start\":40730},{\"end\":40992,\"start\":40981},{\"end\":41011,\"start\":41002},{\"end\":41030,\"start\":41022},{\"end\":41047,\"start\":41044},{\"end\":41062,\"start\":41054},{\"end\":41076,\"start\":41070},{\"end\":41083,\"start\":41078},{\"end\":41425,\"start\":41420},{\"end\":41437,\"start\":41433},{\"end\":41451,\"start\":41448},{\"end\":41729,\"start\":41726},{\"end\":41744,\"start\":41740},{\"end\":41912,\"start\":41910},{\"end\":41921,\"start\":41917},{\"end\":41931,\"start\":41928},{\"end\":42095,\"start\":42092},{\"end\":42107,\"start\":42101},{\"end\":42118,\"start\":42115},{\"end\":42328,\"start\":42325},{\"end\":42344,\"start\":42334},{\"end\":42349,\"start\":42346},{\"end\":42361,\"start\":42355},{\"end\":42365,\"start\":42363},{\"end\":42562,\"start\":42557},{\"end\":42568,\"start\":42564},{\"end\":42740,\"start\":42734},{\"end\":42752,\"start\":42747},{\"end\":42774,\"start\":42761},{\"end\":43022,\"start\":43009},{\"end\":43037,\"start\":43031},{\"end\":43056,\"start\":43050},{\"end\":43328,\"start\":43323},{\"end\":43343,\"start\":43337},{\"end\":43356,\"start\":43351},{\"end\":43368,\"start\":43362},{\"end\":43597,\"start\":43592},{\"end\":43608,\"start\":43606},{\"end\":43623,\"start\":43620},{\"end\":43838,\"start\":43830},{\"end\":43862,\"start\":43849},{\"end\":43880,\"start\":43871},{\"end\":43893,\"start\":43887},{\"end\":44114,\"start\":44108},{\"end\":44125,\"start\":44120},{\"end\":44143,\"start\":44135},{\"end\":44159,\"start\":44153},{\"end\":44172,\"start\":44168},{\"end\":44188,\"start\":44182},{\"end\":44200,\"start\":44197},{\"end\":44217,\"start\":44208},{\"end\":44230,\"start\":44224},{\"end\":44242,\"start\":44237},{\"end\":44512,\"start\":44509},{\"end\":44524,\"start\":44521},{\"end\":44540,\"start\":44537},{\"end\":44556,\"start\":44552},{\"end\":44797,\"start\":44793},{\"end\":44814,\"start\":44807},{\"end\":44826,\"start\":44820},{\"end\":44845,\"start\":44836},{\"end\":44861,\"start\":44855},{\"end\":45087,\"start\":45083},{\"end\":45097,\"start\":45093},{\"end\":45113,\"start\":45107},{\"end\":45124,\"start\":45122},{\"end\":45132,\"start\":45130},{\"end\":45144,\"start\":45137},{\"end\":45375,\"start\":45367},{\"end\":45392,\"start\":45384},{\"end\":45408,\"start\":45402},{\"end\":45422,\"start\":45417},{\"end\":45440,\"start\":45430},{\"end\":45452,\"start\":45445},{\"end\":45732,\"start\":45724},{\"end\":45749,\"start\":45743},{\"end\":45765,\"start\":45760},{\"end\":46000,\"start\":45998},{\"end\":46012,\"start\":46009},{\"end\":46027,\"start\":46023},{\"end\":46039,\"start\":46036},{\"end\":46052,\"start\":46048}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14758855},\"end\":31162,\"start\":30928},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3763419},\"end\":31550,\"start\":31164},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3068042},\"end\":31771,\"start\":31552},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":589516},\"end\":32091,\"start\":31773},{\"attributes\":{\"id\":\"b4\"},\"end\":32386,\"start\":32093},{\"attributes\":{\"id\":\"b5\"},\"end\":32783,\"start\":32388},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":58560770},\"end\":33090,\"start\":32785},{\"attributes\":{\"id\":\"b7\"},\"end\":33377,\"start\":33092},{\"attributes\":{\"doi\":\"TR-CS-11\",\"id\":\"b8\"},\"end\":33645,\"start\":33379},{\"attributes\":{\"id\":\"b9\"},\"end\":34156,\"start\":33647},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":35713636},\"end\":34395,\"start\":34158},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":38429476},\"end\":34666,\"start\":34397},{\"attributes\":{\"id\":\"b12\"},\"end\":34921,\"start\":34668},{\"attributes\":{\"id\":\"b13\"},\"end\":35135,\"start\":34923},{\"attributes\":{\"id\":\"b14\"},\"end\":35428,\"start\":35137},{\"attributes\":{\"id\":\"b15\"},\"end\":35721,\"start\":35430},{\"attributes\":{\"id\":\"b16\"},\"end\":35959,\"start\":35723},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8072069},\"end\":36318,\"start\":35961},{\"attributes\":{\"id\":\"b18\"},\"end\":36571,\"start\":36320},{\"attributes\":{\"id\":\"b19\"},\"end\":36866,\"start\":36573},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9296806},\"end\":37214,\"start\":36868},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13821711},\"end\":37520,\"start\":37216},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3329621},\"end\":37955,\"start\":37522},{\"attributes\":{\"id\":\"b23\"},\"end\":38192,\"start\":37957},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":15387778},\"end\":38534,\"start\":38194},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":17075748},\"end\":38886,\"start\":38536},{\"attributes\":{\"id\":\"b26\"},\"end\":39179,\"start\":38888},{\"attributes\":{\"id\":\"b27\"},\"end\":39371,\"start\":39181},{\"attributes\":{\"id\":\"b28\"},\"end\":39589,\"start\":39373},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":27724558},\"end\":39845,\"start\":39591},{\"attributes\":{\"id\":\"b30\"},\"end\":40112,\"start\":39847},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b31\"},\"end\":40389,\"start\":40114},{\"attributes\":{\"id\":\"b32\"},\"end\":40608,\"start\":40391},{\"attributes\":{\"id\":\"b33\"},\"end\":40895,\"start\":40610},{\"attributes\":{\"id\":\"b34\"},\"end\":41291,\"start\":40897},{\"attributes\":{\"id\":\"b35\"},\"end\":41638,\"start\":41293},{\"attributes\":{\"id\":\"b36\"},\"end\":41871,\"start\":41640},{\"attributes\":{\"id\":\"b37\"},\"end\":42018,\"start\":41873},{\"attributes\":{\"id\":\"b38\"},\"end\":42243,\"start\":42020},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":52902693},\"end\":42516,\"start\":42245},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6155330},\"end\":42681,\"start\":42518},{\"attributes\":{\"doi\":\"arXiv:1511.04119\",\"id\":\"b41\"},\"end\":42914,\"start\":42683},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":37515850},\"end\":43237,\"start\":42916},{\"attributes\":{\"id\":\"b43\"},\"end\":43536,\"start\":43239},{\"attributes\":{\"id\":\"b44\"},\"end\":43735,\"start\":43538},{\"attributes\":{\"id\":\"b45\"},\"end\":44063,\"start\":43737},{\"attributes\":{\"id\":\"b46\"},\"end\":44434,\"start\":44065},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":53098789},\"end\":44723,\"start\":44436},{\"attributes\":{\"id\":\"b48\"},\"end\":45024,\"start\":44725},{\"attributes\":{\"id\":\"b49\"},\"end\":45289,\"start\":45026},{\"attributes\":{\"id\":\"b50\"},\"end\":45640,\"start\":45291},{\"attributes\":{\"id\":\"b51\"},\"end\":45917,\"start\":45642},{\"attributes\":{\"id\":\"b52\"},\"end\":46212,\"start\":45919}]", "bib_title": "[{\"end\":30964,\"start\":30928},{\"end\":31244,\"start\":31164},{\"end\":31598,\"start\":31552},{\"end\":31856,\"start\":31773},{\"end\":32865,\"start\":32785},{\"end\":34187,\"start\":34158},{\"end\":34448,\"start\":34397},{\"end\":36064,\"start\":35961},{\"end\":36970,\"start\":36868},{\"end\":37297,\"start\":37216},{\"end\":37628,\"start\":37522},{\"end\":38288,\"start\":38194},{\"end\":38585,\"start\":38536},{\"end\":39644,\"start\":39591},{\"end\":42315,\"start\":42245},{\"end\":42553,\"start\":42518},{\"end\":43003,\"start\":42916},{\"end\":44501,\"start\":44436}]", "bib_author": "[{\"end\":30982,\"start\":30966},{\"end\":31000,\"start\":30982},{\"end\":31015,\"start\":31000},{\"end\":31025,\"start\":31015},{\"end\":31265,\"start\":31246},{\"end\":31278,\"start\":31265},{\"end\":31295,\"start\":31278},{\"end\":31307,\"start\":31295},{\"end\":31321,\"start\":31307},{\"end\":31612,\"start\":31600},{\"end\":31631,\"start\":31612},{\"end\":31641,\"start\":31631},{\"end\":31872,\"start\":31858},{\"end\":31887,\"start\":31872},{\"end\":31903,\"start\":31887},{\"end\":32160,\"start\":32149},{\"end\":32174,\"start\":32160},{\"end\":32185,\"start\":32174},{\"end\":32193,\"start\":32185},{\"end\":32208,\"start\":32193},{\"end\":32228,\"start\":32208},{\"end\":32531,\"start\":32506},{\"end\":32554,\"start\":32531},{\"end\":32572,\"start\":32554},{\"end\":32876,\"start\":32867},{\"end\":32889,\"start\":32876},{\"end\":32904,\"start\":32889},{\"end\":32916,\"start\":32904},{\"end\":33202,\"start\":33193},{\"end\":33216,\"start\":33202},{\"end\":33228,\"start\":33216},{\"end\":33441,\"start\":33426},{\"end\":33456,\"start\":33441},{\"end\":33469,\"start\":33456},{\"end\":33481,\"start\":33469},{\"end\":33738,\"start\":33731},{\"end\":33758,\"start\":33738},{\"end\":33776,\"start\":33758},{\"end\":33791,\"start\":33776},{\"end\":33808,\"start\":33791},{\"end\":33819,\"start\":33808},{\"end\":33832,\"start\":33819},{\"end\":33851,\"start\":33832},{\"end\":33863,\"start\":33851},{\"end\":33881,\"start\":33863},{\"end\":33886,\"start\":33881},{\"end\":34211,\"start\":34189},{\"end\":34227,\"start\":34211},{\"end\":34242,\"start\":34227},{\"end\":34477,\"start\":34450},{\"end\":34492,\"start\":34477},{\"end\":34497,\"start\":34492},{\"end\":34759,\"start\":34748},{\"end\":34770,\"start\":34759},{\"end\":34785,\"start\":34770},{\"end\":34968,\"start\":34955},{\"end\":34976,\"start\":34968},{\"end\":34991,\"start\":34976},{\"end\":35007,\"start\":34991},{\"end\":35018,\"start\":35007},{\"end\":35232,\"start\":35223},{\"end\":35246,\"start\":35232},{\"end\":35258,\"start\":35246},{\"end\":35272,\"start\":35258},{\"end\":35509,\"start\":35496},{\"end\":35521,\"start\":35509},{\"end\":35533,\"start\":35521},{\"end\":35558,\"start\":35533},{\"end\":35564,\"start\":35558},{\"end\":35815,\"start\":35804},{\"end\":35827,\"start\":35815},{\"end\":36090,\"start\":36066},{\"end\":36106,\"start\":36090},{\"end\":36119,\"start\":36106},{\"end\":36400,\"start\":36391},{\"end\":36412,\"start\":36400},{\"end\":36421,\"start\":36412},{\"end\":36434,\"start\":36421},{\"end\":36663,\"start\":36650},{\"end\":36674,\"start\":36663},{\"end\":36690,\"start\":36674},{\"end\":36706,\"start\":36690},{\"end\":36983,\"start\":36972},{\"end\":37000,\"start\":36983},{\"end\":37012,\"start\":37000},{\"end\":37020,\"start\":37012},{\"end\":37317,\"start\":37299},{\"end\":37331,\"start\":37317},{\"end\":37351,\"start\":37331},{\"end\":37645,\"start\":37630},{\"end\":37656,\"start\":37645},{\"end\":37668,\"start\":37656},{\"end\":37682,\"start\":37668},{\"end\":37696,\"start\":37682},{\"end\":37710,\"start\":37696},{\"end\":37720,\"start\":37710},{\"end\":38021,\"start\":38008},{\"end\":38037,\"start\":38021},{\"end\":38052,\"start\":38037},{\"end\":38063,\"start\":38052},{\"end\":38305,\"start\":38290},{\"end\":38320,\"start\":38305},{\"end\":38333,\"start\":38320},{\"end\":38345,\"start\":38333},{\"end\":38606,\"start\":38587},{\"end\":38621,\"start\":38606},{\"end\":38642,\"start\":38621},{\"end\":38660,\"start\":38642},{\"end\":38672,\"start\":38660},{\"end\":38691,\"start\":38672},{\"end\":38970,\"start\":38955},{\"end\":38985,\"start\":38970},{\"end\":38998,\"start\":38985},{\"end\":39010,\"start\":38998},{\"end\":39022,\"start\":39010},{\"end\":39256,\"start\":39243},{\"end\":39264,\"start\":39256},{\"end\":39444,\"start\":39425},{\"end\":39469,\"start\":39444},{\"end\":39660,\"start\":39646},{\"end\":39681,\"start\":39660},{\"end\":39690,\"start\":39681},{\"end\":39929,\"start\":39912},{\"end\":39945,\"start\":39929},{\"end\":39964,\"start\":39945},{\"end\":40198,\"start\":40182},{\"end\":40216,\"start\":40198},{\"end\":40449,\"start\":40437},{\"end\":40464,\"start\":40449},{\"end\":40478,\"start\":40464},{\"end\":40488,\"start\":40478},{\"end\":40678,\"start\":40666},{\"end\":40693,\"start\":40678},{\"end\":40710,\"start\":40693},{\"end\":40722,\"start\":40710},{\"end\":40740,\"start\":40722},{\"end\":40994,\"start\":40979},{\"end\":41013,\"start\":40994},{\"end\":41032,\"start\":41013},{\"end\":41049,\"start\":41032},{\"end\":41064,\"start\":41049},{\"end\":41078,\"start\":41064},{\"end\":41085,\"start\":41078},{\"end\":41427,\"start\":41406},{\"end\":41439,\"start\":41427},{\"end\":41453,\"start\":41439},{\"end\":41731,\"start\":41717},{\"end\":41746,\"start\":41731},{\"end\":41914,\"start\":41906},{\"end\":41923,\"start\":41914},{\"end\":41933,\"start\":41923},{\"end\":42097,\"start\":42083},{\"end\":42109,\"start\":42097},{\"end\":42120,\"start\":42109},{\"end\":42330,\"start\":42317},{\"end\":42346,\"start\":42330},{\"end\":42351,\"start\":42346},{\"end\":42363,\"start\":42351},{\"end\":42367,\"start\":42363},{\"end\":42564,\"start\":42555},{\"end\":42570,\"start\":42564},{\"end\":42742,\"start\":42726},{\"end\":42754,\"start\":42742},{\"end\":42776,\"start\":42754},{\"end\":43024,\"start\":43005},{\"end\":43039,\"start\":43024},{\"end\":43058,\"start\":43039},{\"end\":43330,\"start\":43315},{\"end\":43345,\"start\":43330},{\"end\":43358,\"start\":43345},{\"end\":43370,\"start\":43358},{\"end\":43599,\"start\":43584},{\"end\":43610,\"start\":43599},{\"end\":43625,\"start\":43610},{\"end\":43840,\"start\":43825},{\"end\":43864,\"start\":43840},{\"end\":43882,\"start\":43864},{\"end\":43895,\"start\":43882},{\"end\":44116,\"start\":44103},{\"end\":44127,\"start\":44116},{\"end\":44145,\"start\":44127},{\"end\":44161,\"start\":44145},{\"end\":44174,\"start\":44161},{\"end\":44190,\"start\":44174},{\"end\":44202,\"start\":44190},{\"end\":44219,\"start\":44202},{\"end\":44232,\"start\":44219},{\"end\":44244,\"start\":44232},{\"end\":44514,\"start\":44503},{\"end\":44526,\"start\":44514},{\"end\":44542,\"start\":44526},{\"end\":44558,\"start\":44542},{\"end\":44799,\"start\":44790},{\"end\":44816,\"start\":44799},{\"end\":44828,\"start\":44816},{\"end\":44847,\"start\":44828},{\"end\":44863,\"start\":44847},{\"end\":45089,\"start\":45079},{\"end\":45099,\"start\":45089},{\"end\":45115,\"start\":45099},{\"end\":45126,\"start\":45115},{\"end\":45134,\"start\":45126},{\"end\":45146,\"start\":45134},{\"end\":45377,\"start\":45360},{\"end\":45394,\"start\":45377},{\"end\":45410,\"start\":45394},{\"end\":45424,\"start\":45410},{\"end\":45442,\"start\":45424},{\"end\":45454,\"start\":45442},{\"end\":45734,\"start\":45715},{\"end\":45751,\"start\":45734},{\"end\":45767,\"start\":45751},{\"end\":46002,\"start\":45993},{\"end\":46014,\"start\":46002},{\"end\":46029,\"start\":46014},{\"end\":46041,\"start\":46029},{\"end\":46054,\"start\":46041}]", "bib_venue": "[{\"end\":31042,\"start\":31025},{\"end\":31349,\"start\":31321},{\"end\":31655,\"start\":31641},{\"end\":31924,\"start\":31903},{\"end\":32147,\"start\":32093},{\"end\":32504,\"start\":32388},{\"end\":32930,\"start\":32916},{\"end\":33191,\"start\":33092},{\"end\":33424,\"start\":33379},{\"end\":33729,\"start\":33647},{\"end\":34270,\"start\":34242},{\"end\":34525,\"start\":34497},{\"end\":34746,\"start\":34668},{\"end\":34953,\"start\":34923},{\"end\":35221,\"start\":35137},{\"end\":35494,\"start\":35430},{\"end\":35802,\"start\":35723},{\"end\":36133,\"start\":36119},{\"end\":36389,\"start\":36320},{\"end\":36648,\"start\":36573},{\"end\":37034,\"start\":37020},{\"end\":37360,\"start\":37351},{\"end\":37729,\"start\":37720},{\"end\":38006,\"start\":37957},{\"end\":38354,\"start\":38345},{\"end\":38700,\"start\":38691},{\"end\":38953,\"start\":38888},{\"end\":39241,\"start\":39181},{\"end\":39423,\"start\":39373},{\"end\":39709,\"start\":39690},{\"end\":39910,\"start\":39847},{\"end\":40180,\"start\":40114},{\"end\":40435,\"start\":40391},{\"end\":40664,\"start\":40610},{\"end\":40977,\"start\":40897},{\"end\":41404,\"start\":41293},{\"end\":41715,\"start\":41640},{\"end\":41904,\"start\":41873},{\"end\":42081,\"start\":42020},{\"end\":42375,\"start\":42367},{\"end\":42595,\"start\":42570},{\"end\":42724,\"start\":42683},{\"end\":43072,\"start\":43058},{\"end\":43313,\"start\":43239},{\"end\":43582,\"start\":43538},{\"end\":43823,\"start\":43737},{\"end\":44101,\"start\":44065},{\"end\":44572,\"start\":44558},{\"end\":44788,\"start\":44725},{\"end\":45077,\"start\":45026},{\"end\":45358,\"start\":45291},{\"end\":45713,\"start\":45642},{\"end\":45991,\"start\":45919}]"}}}, "year": 2023, "month": 12, "day": 17}