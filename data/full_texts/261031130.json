{"id": 261031130, "updated": "2023-12-16 15:05:34.007", "metadata": {"title": "Towards Personalized Federated Learning via Heterogeneous Model Reassembly", "authors": "[{\"first\":\"Jiaqi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Xingyi\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Suhan\",\"last\":\"Cui\",\"middle\":[]},{\"first\":\"Liwei\",\"last\":\"Che\",\"middle\":[]},{\"first\":\"Lingjuan\",\"last\":\"Lyu\",\"middle\":[]},{\"first\":\"Dongkuan\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Fenglong\",\"last\":\"Ma\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "This paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures. To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning. In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side. Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention. Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent. Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings. Additionally, pFedHR effectively reduces the adverse impact of using different public data and dynamically generates diverse personalized models in an automated manner.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2308-08643", "doi": "10.48550/arxiv.2308.08643"}}, "content": {"source": {"pdf_hash": "99a0f0f6ada73216389307133e04e9468639fbb2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.08643v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2308.08643", "status": "CLOSED"}}, "grobid": {"id": "c3d386aa60aed9652473bccc6ec00176934273f7", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/99a0f0f6ada73216389307133e04e9468639fbb2.txt", "contents": "\nTowards Personalized Federated Learning via Heterogeneous Model Reassembly\n27 Oct 2023\n\nJiaqi Wang jqwang@psu.edu \nThe Pennsylvania State University\n\n\nXingyi Yang xyang@u.nus.edu \nNational University of Singapore\n\n\nSuhan Cui \nThe Pennsylvania State University\n\n\nLiwei Che \nThe Pennsylvania State University\n\n\nLingjuan Lyu lingjuan.lv@sony.com \nSony AI\n\n\nDongkuan Xu \nNorth Carolina State University\n\n\nFenglong Ma fenglong@psu.edu \nThe Pennsylvania State University\n\n\nTowards Personalized Federated Learning via Heterogeneous Model Reassembly\n27 Oct 20238CD6F0A80158BA0340C046B070441D3BarXiv:2308.08643v3[cs.LG]\nThis paper focuses on addressing the practical yet challenging problem of model heterogeneity in federated learning, where clients possess models with different network structures.To track this problem, we propose a novel framework called pFedHR, which leverages heterogeneous model reassembly to achieve personalized federated learning.In particular, we approach the problem of heterogeneous model personalization as a model-matching optimization task on the server side.Moreover, pFedHR automatically and dynamically generates informative and diverse personalized candidates with minimal human intervention.Furthermore, our proposed heterogeneous model reassembly technique mitigates the adverse impact introduced by using public data with different distributions from the client data to a certain extent.Experimental results demonstrate that pFedHR outperforms baselines on three datasets under both IID and Non-IID settings.Additionally, pFedHR effectively reduces the adverse impact of using different public data and dynamically generates diverse personalized models in an automated manner 2 .\n\nIntroduction\n\nFederated learning (FL) aims to enable collaborative machine learning without the need to share clients' data with others, thereby upholding data privacy [1][2][3].However, traditional federated learning approaches [2,[4][5][6][7][8][9][10][11][12] typically enforce the use of an identical model structure for all clients during training.This constraint poses challenges in achieving personalized learning within the FL framework.In real-world scenarios, clients such as data centers, institutes, or companies often possess their own distinct models, which may have varying structures.Training on top of their original models should be a better solution than deploying new ones for collaborative purposes.Therefore, a practical solution lies in fostering heterogeneous model cooperation within FL, while preserving individual model structures.Only a few studies have attempted to address the challenging problem of heterogeneous model cooperation in FL [13][14][15][16][17], and most of them incorporate the use of a public dataset to facilitate both cooperation and personalization [14][15][16][17].However, these approaches still face several key issues:\n\n\u2022 Undermining personalization through consensus: Existing methods often generate consensual side information, such as class information [14], logits [15,18], and label-wise representations [19], using public data.This information is then exchanged and used to conduct average operations on the server, resulting in a consensus representation.However, this approach poses privacy and security concerns due to the exchange of side information [20].Furthermore, the averaging process \u2022 Excessive reliance on prior knowledge for distillation-based approaches: Distillation-based techniques, such as knowledge distillation (KD), are commonly employed for heterogeneous model aggregation in FL [16,17,21].However, these techniques necessitate the predefinition of a shared model structure based on prior knowledge [17].This shared model is then downloaded to clients to guide their training process.Consequently, handcrafted models can heavily influence local model personalization.Additionally, a fixed shared model structure may be insufficient for effectively guiding personalized learning when dealing with a large number of clients with non-IID data.Thus, it is crucial to explore methods that can automatically and dynamically generate client-specific personalized models as guidance.\n\n\u2022 Sensitivity to the choice of public datasets: Most existing approaches use public data to obtain guidance information, such as logits [15,18] or a shared model [17], for local model personalization.\n\nThe design of these approaches makes public data and model personalization tightly bound together.Thus, they usually choose the public data with the same distribution as the client data.Therefore, using public data with different distributions from client data will cause a significant performance drop in existing models.Figure 1 illustrates the performance variations of different models trained on the SVHN dataset with different public datasets (detailed experimental information can be found in Section 4.4).The figure demonstrates a significant performance drop when using alternative public datasets.Consequently, mitigating the adverse impact of employing diverse public data remains a critical yet practical research challenge in FL.\n\nMotivation & Challenges.In fact, both consensus-based and distillation-based approaches aim to learn aggregated and shared information used as guidance in personalized local model training, which is not an optimal way to achieve personalization.An ideal solution is to generate a personalized model for the corresponding client, which is significantly challenging since the assessable information on the server side can only include the uploaded client models and the public data.To avoid the issue of public data sensitivity, only client models can be used.These constraints motivate us to employ the model reassembly technique [22] to generate models first and then select the most matchable personalized model for a specific client from the generations.\n\nTo this end, we will face several new challenges.(C1) Applying the model reassembly technique will result in many candidates.Thus, the first challenge is how to get the optimal candidates.(C2) The layers of the generated candidates are usually from different client models, and the output dimension size of the first layer may not align with the input dimension size of the second layer, which leads to the necessity of network layer stitching.However, the parameters of the stitched layers are unknown.Therefore, the second challenge is how to learn those unknown parameters in the stitched models.(C3) Even with well-trained stitched models, digging out the best match between a client model and a stitched model remains a big challenge.\n\nOur Approach.To simultaneously tackle all the aforementioned challenges, we present a novel framework called pFedHR, which aims to achieve personalized federated learning and address the issue of heterogeneous model cooperation (as depicted in Figure 2).The pFedHR framework comprises two key updates: the server update and the client update.In particular, to tackle C3, we approach the issue of heterogeneous model personalization from a model-matching optimization perspective on the server side (see Section 3.1.1).To solve this problem, we introduce a novel heterogeneous model reassembly technique in Section 3.1.2to assemble models uploaded from clients, i.e., {w  and diverse model candidates using clustering results.Importantly, all layers in each candidate are derived from the uploaded client models.\n\nPrior to matching the client model with candidates, we perform network layer stitching while maximizing the retention of information from the original client models (Section 3.1.3).To tackle C2, we introduce public data D p to help the finetuning of the stitched candidates, i.e., {c 1 t , \u2022 \u2022 \u2022 , cM t }, where M is the number of generated candidates.Specifically, we employ labeled OR unlabeled public data to fine-tune the stitched and client models and then calculate similarities based on model outputs.Intuitively, if two models are highly related to each other, their outputs should also be similar.Therefore, we select the candidate with the highest similarity as the personalized model of the corresponding client, which results in matched pairs 2. In the client update (Section 3.2), we treat the matched personalized model as a guidance mechanism for client parameter learning using knowledge distillation 3 .\n{{w 1 t , ci t }, \u2022 \u2022 \u2022 , {w B t , cm t }} in Figure\nIt is worth noting that we minimally use public data during our model learning to reduce their adverse impact.In our model design, the public data are used for clustering layers, fine-tuning the stitched candidates, and guiding model matching.Clustering and matching stages use public data to obtain the feedforward outputs as guidance and do not involve model parameter updates.Only in the fine-tuning stage, the stitched models' parameters will be updated based on public data.To reduce its impact as much as possible, we limit the number of finetuning epochs during the model implementation.Although we cannot thoroughly break the tie between model training and public data, such a design at least greatly alleviates the problem of public data sensitivity in FL.\n\nContributions.Our work makes the following key contributions: (1) We introduce the first personalized federated learning framework based on model reassembly, specifically designed to address the challenges of heterogeneous model cooperation.(2) The proposed pFedHR framework demonstrates the ability to automatically and dynamically generate personalized candidates that are both informative and diverse, requiring minimal human intervention.(3) We present a novel heterogeneous model reassembly technique, which effectively mitigates the adverse impact caused by using public data with distributions different from client data.( 4) Experimental results show that the pFedHR framework achieves state-of-the-art performance on three datasets, exhibiting superior performance under both IID and Non-IID settings when compared to baselines employing labeled and unlabeled public datasets.\n\n\nRelated Work\n\nModel Heterogeneity in Federated Learning.Although many federated learning models, such as FedAvg [4], FedProx [2], Per-FedAvg [23], PFedMe [24], and PFedBayes [25], have been proposed recently, the focus on heterogeneous model cooperation, where clients possess models with diverse structures, remains limited.It is worth noting that in this context, the client models are originally distinct and not derived from a shared, large global model through the distillation of subnetworks [26,27].Existing studies on heterogeneous model cooperation can be broadly categorized based on whether they utilize public data for model training.FedKD [13] aims to achieve personalized models for each client without employing public data by simultaneously maintaining a large heterogeneous model and a small homogeneous model on each client, which incurs high computational costs.\n\nMost existing approaches leverage public data to facilitate model training.Among them, FedDF [16], FedKEMF [17], and FCCL [15] employ unlabeled public data.However, FedDF trains a global model with different settings compared to our approach.FedKEMF performs mutual knowledge distillation learning on the server side to achieve model personalization, requiring predefined model structures.FCCL averages the logits provided by each client and utilizes a consensus logit as guidance during local model training.It is worth noting that the use of logits raises concerns regarding privacy and security [20,28].FedMD [14] and FedGH [19] employ labeled public data.These approaches exchange class information or representations between the server and clients and perform aggregation to address the model heterogeneity issue.However, similar to FCCL, these methods also introduce privacy leakage concerns.In contrast to existing work, we propose a general framework capable of utilizing either labeled OR unlabeled public data to learn personalized models through heterogeneous model reassembly.We summarize the distinctions between existing approaches and our framework in Table 1.\n\nNeural Network Reassembly and Stitching.As illustrated in Table 1, conventional methods are primarily employed in existing federated learning approaches to obtain personalized client models, with limited exploration of model reassembly.Additionally, there is a lack of research investigating neural network reassembly and stitching [22,[29][30][31][32] within the context of federated learning.For instance, the work presented in [30] proposes three algorithms to merge two models within the weight space, but it is limited to handling only two models as input.In our setting, multiple models need to be incorporated into the model reassembly or aggregation process.Furthermore, both [22] and [31] focus on pre-trained models, which differ from our specific scenario.\n\n\nMethodology\n\nOur model pFedHR incorporates two key updates: the server update and the local update, as depicted in Figure 2. Next, we provide the details of our model design starting with the server update.\n\n\nServer Update\n\nDuring each communication round t, the server will receive B heterogeneous client models with parameters denoted as\n{w 1 t , w 2 t , \u2022 \u2022 \u2022 , w B t }.\nAs we discussed in Section 1, traditional approaches have limitations when applied in this context.To overcome these limitations and learn a personalized model \u0175n t that can be distributed to the corresponding n-th client, we propose a novel approach that leverages the publicly available data D p stored on the server to find the most similar aggregated models learned from {w 1 t , w 2 t , \u2022 \u2022 \u2022 , w B t } for w n t .\n\n\nSimilarity-based Model Matching\n\nLet g(\u2022, \u2022) denote the model aggregation function, which can automatically and dynamically obtain M aggregated model candidates as follows:\n{c 1 t , \u2022 \u2022 \u2022 , c M t } = g({w 1 t , w 2 t , \u2022 \u2022 \u2022 , w B t }, D p ),(1)\nwhere g(\u2022, \u2022) will be detailed in Section 3.1.2,and c m k is the m-th generated model candidate learned by g(\u2022, \u2022).Note that c m k denotes the model before network stitching.M is the total number of candidates, which is not a fixed number and is estimated by g(\u2022, \u2022).In such a way, our goal is to optimize the following function:\nc * t = arg max c m t ;m\u2208[1,M ] {sim(w n t , c 1 t ; D p ), \u2022 \u2022 \u2022 , sim(w n t , c M t ; D p )}, \u2200n \u2208 [1, B],(2)\nAlgorithm 1: Reassembly Candidate Search\ninput :Layer clusters {G 1 t , G 2 t , \u2022 \u2022 \u2022 , G K t }, operation type set O, rule set R output :{c 1 t , \u2022 \u2022 \u2022 , c M t } Initialize Ct = \u00d8; for k \u2190 1, \u2022 \u2022 \u2022 , K do // Q k is the number of operation-layer pairs in G k t for q \u2190 1, \u2022 \u2022 \u2022 , Q k do\nInitialize an empty candidate cq = [], operation type set Oq = \u00d8, group id set Kq = \u00d8; Select the q-th layer-operation pair\n(L n t,i , O n i ) from group G k Add (L n t,i , O n i ) to cq, add O n i to Oq, add k to Kq; for k \u2032 \u2190 1, \u2022 \u2022 \u2022 , K do 9\nCheck a pair from G k \u2032 whether it satisfies:\nreturn :Ct = {c 1 t , \u2022 \u2022 \u2022 , c M t }\nwhere c * t is the best matched model for w n t , which is also denoted as \u0175n t = c * t .sim(\u2022, \u2022) is the similarity function between two models, which will be detailed in Section 3.1.3.\n\n\nHeterogeneous Model Reassemblyg(\u2022, \u2022)\n\nTo optimize Eq. ( 2), we need to obtain M candidates using the heterogenous model aggregation function g(\u2022) in Eq. (1).To avoid the issue of predefined model architectures in the knowledge distillation approaches, we aim to automatically and dynamically learn the candidate architectures via a newly designed function g(\u2022, \u2022).In particular, we propose a decomposition-grouping-reassembly method as g(\u2022, \u2022), including layer-wise decomposition, function-driven layer grouping, and reassembly candidate generation.\n\nLayer-wise Decomposition.Assume that each uploaded client model w n t contains H layers, i.e.,\nw n t = [(L n t,1 , O n 1 ), \u2022 \u2022 \u2022 , (L n t,H , O n E )],\nwhere each layer L n t,h is associated with an operation type O n e .For example, a plain convolutional neural network (CNN) usually has three operations: convolution, pooling, and fully connected layers.For different client models, H may be different.The decomposition step aims to obtain these layers and their corresponding operation types.\n\nFunction-driven Layer Grouping.After decomposing layers of client models, we group these layers based on their functional similarities.Due to the model structure heterogeneity in our setting, the dimension size of the output representations from layers by feeding the public data D p to different models will be different.Thus, measuring the similarity between a pair of layers is challenging, which can be resolved by applying the commonly used centered kernel alignment (CKA) technique [33].In particular, we define the distance metric between any pair of layers as follows:\ndis(L n t,i , L b t,j ) = (CKA(X n t,i , X b t,i ) + CKA(L n t,i (X n t,i ), L b t,i (X b t,i ))) \u22121 ,(3)\nwhere X n t,i is the input data of L n t,i , and L n t,i (X n t,i ) denotes the output data from L n t,i .This metric uses CKA(\u2022, \u2022) to calculate the similarity between both input and output data of two layers.\n\nBased on the defined distance metric, we conduct the K-means-style algorithm to group the layers of B models into K clusters.This optimization process aims to minimize the sum of distances between all pairs of layers, denoted as L t .The procedure can be described as follows:\nmin L t = min \u03b4 a b,h \u2208{0,1} K k=1 B b=1 H h=1 \u03b4 k b,h (dis(L k t , L b t,h )),(4)\nwhere L k t is the center of the k-th cluster.\u03b4 k b,h is the indicator.If the h-th layer of w b t belongs to the k-th cluster, then \u03b4 k b,h = 1.Otherwise, \u03b4 k b,h = 0.After the grouping process, we obtain K layer clusters denoted as\n{G 1 t , G 2 t , \u2022 \u2022 \u2022 , G K t }.\nThere are multiple layers in each group, which have similar functions.Besides, each layer is associated with an operation type.\n\nReassembly Candidate Generation.The last step for obtaining personalized candidates\n{c 1 t , \u2022 \u2022 \u2022 , c M t } is to assemble the learned layer-wise groups {G 1 t , G 2 t , \u2022 \u2022 \u2022 , G K t }\nbased on their functions.To this end, we design a heuristic rule-based search strategy as shown in Algorithm 1.Our goal is to automatically generate informative and diverse candidates.\n\nGenerally, an informative candidate needs to follow the design of handcrafted network structures.This is challenging since the candidates are automatically generated without human interventions and prior knowledge.To satisfy this condition, we require the layer orders to be guaranteed (R 1 in Line 10).For example, the i-th layer from the n-th model, i.e., L n t,i , in a candidate must be followed by a layer with an index j > i from other models or itself.Besides, the operation type also determines the quality of a model.For a CNN model, the fully connected layer is usually used after the convolution layer, which motivates us to design the R 2 operation order rule in Line 11.\n\nOnly taking the informativeness principle into consideration, we may generate a vast number of candidates with different sizes of layers.Some candidates may be a subset of others and even worse with low quality.Besides, the large number of candidates will increase the computational burden of the server.To avoid these issues and further obtain high-quality candidates, we use the diversity principle as the filtering rule.A diverse and informative model should contain all the operation types, i.e., the R 3 complete operation rule in Line 16. Besides, the groups {G 1 t , \u2022 \u2022 \u2022 , G K t } are clustered based on their layer functions.The requirement that layers of candidates must be from different groups should significantly increase the diversity of model functions, which motivates us to design the R 4 diverse group rule in Line 17.\n\n\nSimilarity Learning with Layer Stitching -sim(\u2022, \u2022)\n\nAfter obtaining a set of candidate models {c 1 t , \u2022 \u2022 \u2022 , c M t }, to optimize Eq. ( 2), we need to calculate the similary bettwen each client model w n t and all the cadidates {c 1 t , \u2022 \u2022 \u2022 , c M t } using the public data D p .However, this is non-trivial since c m t is assembled by layers from different client models, which is not a complete model architecture.We have to stitch these layers together before using c m t .Layer Stitching.Assume that L n t,i and L b t,j are any two consecutive layers in the candidate model c m t .Let d i denote the output dimension of L n t,i and d j denote the input dimension of L b t,j .d i is usually not equal to d j .To stitch these two layers, we follow existing work [31] by adding a nonlinear activation function ReLU(\u2022) on top of a linear layer, i.e., ReLU(W \u22a4 X + b), where W \u2208 R di\u00d7dj , b \u2208 R dj , and X represents the output data from the first layer.In such a way, we can obtain a stitched candidate cm t .The reasons that we apply this simple layer as the stitch are twofold.On the one hand, even adding a simple linear layer between any two consecutive layers, the model will increase d i * (d j + 1) parameters.Since a candidate c m t may contain several layers, if using more complicated layers as the stitch, the number of parameters will significantly increase, which makes the new candidate model hard to be trained.On the other hand, using a simple layer with a few parameters may be helpful for the new candidate model to maintain more information from the original models.This is of importance for the similarity calculation in the next step.\n\nSimilarity Calculation.We propose to use the cosine score cos(\u2022, \u2022) to calculate the similarity between a pair of models (w n t , cm t ) as follows:\nsim(w n t , c m t ; D p ) = sim(w n t , cm t ; D p ) = 1 P P p=1 cos(\u03b1 n t (x p ), \u03b1 m t (x p )),(5)\nwhere P denotes the number of data in the public dataset D p and x p is the p-th data in D p .\u03b1 n t (x p ) and \u03b1 m t (x p ) are the logits output from models w n t and cm t , respectively.To obtain the logits, we need to finetune w n t and cm t using D p first.In our design, we can use both labeled and unlabeled data to finetune models but with different loss functions.If D p is labeled, then we use the supervised cross-entropy (CE) loss to finetune the model.If D p is unlabeled, then we apply the self-supervised contrastive loss to finetune them following [34].\n\n\nClient Update\n\nThe obtained personalized model \u0175n t (i.e., c * t in Eq. ( 2)) will be distributed to the n-th client if it is selected in the next communication round t + 1. \u0175n t is a reassembled model that carries external knowledge from other clients, but its network structure is different from the original w n t .To incorporate the new knowledge without training w n t from scratch, we propose to apply knowledge distillation on the client following [35].\n\nLet D n = {(x n i , y n i )} denote the labeled data, where x n i is the data feature and y n i is the coresponding ground truth vector.The loss of training local model with knowledge distillation is defined as follows:\nJ n = 1 |D n | |Dn| i=1 [CE(w n t (x n i ), y n i ) + \u03bbKL(\u03b1 n t (x n i ), \u03b1n t (x n i ))] ,(6)\nwhere Baselines.We compare pFedHR with the baselines under two settings.(1) Heterogenous setting.\n|D\nIn this setting, clients are allowed to have different model structures.The proposed pFedHR is general and can use both labeled and unlabeled public datasets to conduct heterogeneous model cooperation.To make fair comparisons, we use FedMD [14] and FedGH [19] as baselines when using the labeled public data, and FCCL [15] and FedKEMF [17] when testing the unlabeled public data.\n\n(2) Homogenous setting.In this setting, clients share an identical model structure.We use traditional and personalized FL models as baselines, including FedAvg [4], FedProx [2], Per-FedAvg [23], PFedMe [24], and PFedBayes [25].\n\n\nHeterogenous Setting Evaluation\n\nSmall Number of Clients.Similar to existing work [15], to test the performance with a small number of clients, we set the client number N = 12 and active client number B = 4 in each communication round.We design 4 types of models with different structures and randomly assign each type of model to 3 clients.The Conv operation contains convolution, max pooling, batch normalization, and ReLu, and the FC layer contains fully connected mapping, ReLU, and dropout.We set the number of clusters K = 4. Then local training epoch and the server finetuning epoch are equal to 10 and 3, respectively.The public data and client data are from the same dataset.Table 2 shows the experimental results for the heterogeneous setting using both labeled and unlabeled public data.We can observe that the proposed pFedHR achieves state-of-the-art performance on all datasets and settings.We also find the methods using labeled public datasets can boost the performance compared with unlabeled public ones in general, which aligns with our expectations and experiences.\n\nLarge Number of Clients.We also test the performance of models with a large number of clients.When the number of active clients B is large, calculating layer pair-wise distance values using Eq.(3) will be highly time-consuming.To avoid this issue, a straightforward solution is to conduct FedAvg [4] for averaging the models with the same structures first and then do the function-driven layer grouping based on the averaged structures via Eq.( 4).The following operations are the same as pFedHR.In this experiment, we set N = 100 clients and the active number of clients B = 10.\n\nOther settings are the same as those in the small number client experiment.Table 3 shows the results on the SVHN dataset for testing the proposed pFedHR for a large number of clients setting.The results show similar patterns as those listed in Table 2, where the proposed pFedHR achieves the best performance under IID and Non-IID settings whether it uses labeled or unlabeled public datasets.Compared to the results on the SVHN dataset in Table 2, we can find that the performance of all the baselines and our models drops.Because the number of training data is fixed, allocating these data to 100 clients will make each client use fewer data for training, which leads to a performance drop.The results on both small and large numbers of clients clearly demonstrate the effectiveness of our model for addressing the heterogeneous model cooperation issue in federated learning.\n\n\nHomogeneous Setting Evaluation\n\nIn this experiment, all the clients use the same model structure.We test the performance of our proposed pFedHR on representative models with the smallest (M1) and largest (M4) number of layers, compared with state-of-the-art homogeneous federated learning models.Except for the identical model structure, other experimental settings are the same as those used in the scenario of the small number of clients.Note that for pFedHR, we report the results using the labeled public data in this experiment.The results are shown in Table 4.We can observe that using a simple model (M1) can make models achieve relatively high performance since the MNIST dataset is easy.However, for complicated datasets, i.e., SVHN and CIFAR-10, using a complex model structure (i.e., M4) is helpful for all approaches to improve their performance significantly.Our proposed pFedHR outperforms all baselines on these two datasets, even equipping with a simple model M1.Note that our model is proposed to address the heterogeneous model cooperation problem instead of the homogeneous personalization in FL.Thus, it is a practical approach and can achieve personalization.Still, we also need to mention that it needs extra public data on the server, which is different from baselines.\n\n\nPublic Dataset Analysis\n\nSensitivity to the Public Data Selection.In the previous experiments, the public and client data are from the same dataset, i.e., having the same distribution.To validate the effect of using different public data during model learning for all baselines and our model, we conduct experiments by choosing public data from different datasets and report the results on the SVHN dataset.Other experimental settings are the same as those in the scenario of the small number of clients.\n\nFigure 1 shows the experimental results for all approaches using labeled and unlabeled public datasets.We can observe that replacing the public data will make all approaches decrease performance.This is reasonable since the data distributions between public and client data are different.However, compared with baselines, the proposed pFedHR has the lowest performance drop.Even using other public data, pFedHR can achieve comparable or better performance with baselines using SVHN as the public data.This advantage stems from our model design.As described in Section 3.1.3,we keep more information from original client models by using a simple layer as the stitch.Besides, we aim to search for the most similar personalized candidate with a client model.We propose to calculate the average logits in Eq. ( 5) as the criteria.To obtain the logits, we do not need to finetune the models many times.In our experiments, we set the number of finetuning epochs as 3.This strategy can also help the model reduce the adverse impact of public data during model training.Toward this end, we still use the small number of clients setting, i.e., the client and public data are from the SVHN dataset, but we adjust the percentage of public data.In the original experiment, we used 10% data as the public data.Now, we reduce this percentage to 2% and 5%.The results are shown in Figure 3.We can observe that with the increase in the percentage of public data, the performance of the proposed pFedHR also improves.These results align with our expectations since more public data used for finetuning can help pFedHR obtain more accurately matched personalized models, further enhancing the final accuracy.\n\n\nExperiment results with Different Numbers of Clusters\n\nIn our model design, we need to group functional layers into K groups by optimizing Eq. ( 4).Where K is a predefined hyperparameter.In this experiment, we aim to investigate the performance influence with regard to K. In particular, we conduct the experiments on the SVHN dataset with 12 local clients, and the public data are also the SVHN data.\n\nFigure 4 shows the results on both IID and Non-IID settings with labeled and unlabeled public data.X-axis represents the number of clusters, and Y -axis denotes the accuracy values.We can observe that with the increase of K, the performance will also increase.However, in the experiments, we do not recommend setting a large K since a trade-off balance exists between K and M , where M is the number of candidates automatically generated by Algorithm 1 in the main manuscript.If K is large, then M will be small due to Rule R 4 .In other words, a larger K may make the empty C t returned by Algorithm 1 in the main manuscript.\n\n\nLayer Stitching Study\n\nOne of our major contributions is to develop a new layer stitching strategy to reduce the adverse impacts of introducing public data, even with different distributions from the client data.Our proposed strategy includes two aspects: (1) using a simple layer to stitch layers and (2) reducing the number of finetuning epochs.To validate the correctness of these assumptions, we conduct the following experiments on SVHN with 12 clients, where both client data and labeled public data are extracted from SVHN.\n\nStitching Layer Numbers.In this experiment, we add the complexity of layers for stitching.In our model design, we only use ReLU(W \u22a4 X + b).Now, we increase the number of linear layers from 1 to 2 to 3. The results are depicted in Figure 5.We can observe that under both IID and Non-IID settings, the performance will decrease with the increase of the complexity of the stitching layers.These results demonstrate our assumption that more complex stitching layers will introduce more information about public data but reduce the personalized information of each client model maintained.Thus, using a simple layer to stitch layers is a reasonable choice.\n\nStitched Model Finetuning Numbers.We further explore the influence of the number of finetuning epochs on the stitched model.The results are shown in Figure 6.We can observe that increasing the number of finetuning epochs can also introduce more public data information and reduce model performance.Thus, setting a small number of finetuning epochs benefits keeping the model's performance.\n\n1.74% 3.92% 2.57% 5.31%  pFedHR can automatically and dynamically generate candidates for clients using the proposed heterogeneous model reassembly techniques in Section 3.1.2.We visualize the generated models for a client at two different epochs (t and t \u2032 ) to make a comparison in Figure 7.We can observe that at epoch t, the layers are \"[Conv2 from M1, Conv3 from M2, Conv4 from M3, Conv5 from M3, FC3 from M2]\", which is significantly different the model structure at epoch t \u2032 .These models automatically reassemble different layers from different models learned by the proposed pFedHR instead of using predefined structures or consensus information.\n\n\nConclusion\n\nModel heterogeneity is a crucial and practical challenge in federated learning.While a few studies have addressed this problem, existing models still encounter various issues.To bridge this research gap, we propose a novel framework, named pFedHR, for personalized federated learning, focusing on solving the problem of heterogeneous model cooperation.The experimental results conducted on three datasets, under both IID and Non-IID settings, have verified the effectiveness of our proposed pFedHR framework in addressing the model heterogeneity issue in federated learning.The achieved state-of-the-art performance serves as evidence of the efficacy and practicality of our approach.\n\nFigure 1 :\n1\nFigure 1: Performance changes when using different public data.pFedHR is our proposed model.significantly diminishes the unique characteristics of individual local models, thereby hampering model personalization.Consequently, there is a need to explore approaches that can achieve local model personalization without relying on consensus-based techniques.\n\n\nFigure 2 :\n2\nFigure 2: Overview of the proposed pFedHR.K is the number of clusters.\n\n\nFigure 3 :\n3\nFigure 3: Performance change w.r.t. the percentage of public data.Sensitivity to the Percentage of Public Data.Several factors can affect model performance.In this experiment, we aim to investigate whether the percentage of public data is a key factor in influencing performance change.Toward this end, we still use the small number of clients setting, i.e., the client and public data are from the SVHN dataset, but we adjust the percentage of public data.In the original experiment, we used 10% data as the public data.Now, we reduce this percentage to 2% and 5%.The results are shown in Figure3.We can observe that with the increase in the percentage of public data, the performance of the proposed pFedHR also improves.These results align with our expectations since more public data used for finetuning can help pFedHR obtain more accurately matched personalized models, further enhancing the final accuracy.\n\n\nFigure 4 :\n4\nFigure 4: Results on different number of clusters K's.\n\n\nFigure 5 :\n5\nFigure 5: Stitching layer number study.\n\n\nFigure 6 :Figure 7 :\n67\nFigure 6: Server finetuning number study.\n\n\n\n\nThis technique involves the dynamic grouping of model layers based on their functions, i.e., layer-wise decomposition and function-driven layer grouping in Figure2.To handle C1, a heuristic rule-based search strategy is proposed in reassembly candidate generation to assemble informative\n\n1 t , \u2022 \u2022 \u2022 , w B t }, where B is the number of active clients in the t-the communication round.\n\n\nTable 1 :\n1\nA comparison between existing heterogeneous model cooperation works and our pFedHR.\nApproachPublic DatasetModel CharacteristicsW. Label W.o. LabelUpload and DownloadAggregationPersonalizationFedDF [16]\u2717\u2713parametersensemble distillation\u2717FedKEMF [17]\u2717\u2713parametersmutual learning\u2713FCCL [15]\u2717\u2713logitsaverage\u2713FedMD [14]\u2713\u2717class scoresaverage\u2713FedGH [19]\u2713\u2717label-wise representationsaverage\u2713pFedHR\u2713\u2713parametersmodel reassembly\u2713\n\n10\n\nR1 (layer order): the layer index should be larger than that of the last layer added to cq, and 11 R2 (operation order): the operation type should be followed by the previous type in cq;Add the pair to cq, add its operation type to Oq, add k \u2032 to Kq;\n12if True then13\n14 Move to the next pair; Check Oq and Kq with: R3 (complete operation): the size of Oq should be equal to that of O, and R4 (diverse group): the size of Kq should be equal to K; if True then 19 Add the candidate cq to Ct;\n\n\n\n\nn | denotes the number of data in D n , w n t (x n i ) means the predicted label distribution, \u03bb is a hyperparameter, KL(\u2022, \u2022) is the Kullback-Leibler divergence, and \u03b1 n We conduct experiments for the image classification task on MNIST, SVHN, and CIFAR-10 datasets under both IID and non-IID data distribution settings, respectively.We split the datasets into 80% for training and 20% for testing.During training, we randomly sample 10% training data to put in the server as D p and the remaining 90% to distribute to the clients.The training and testing datasets are randomly sampled for the IID setting.For the non-IID setting, each client randomly holds two classes of data.To test the personalization effectiveness, we sample the testing dataset following the label distribution as the training dataset.We also conduct experiments to test models using different public data in Section 4.4.\nt (x n i ) and \u03b1n t (x n i ) are the logits t and the downloaded personalized model \u0175n from the local model w n t , respevtively.4 Experiments4.1 Experiment SetupsDatasets.\n\nTable 2 :\n2\nPerformance comparison with baselines under the heterogeneous setting.\nPublicDatasetMNISTSVHNCIFAR-10DataModelIIDNon-IIDIIDNon-IIDIIDNon-IIDFedMD [14]93.08% 91.44%81.55% 78.39%68.22% 66.13%LabeledFedGH [19]94.10% 93.27%81.94% 81.06%72.69% 70.27%pFedHR94.55% 94.41% 83.68% 83.40% 73.88% 71.74%FedKEMF [17] 93.01% 91.66%80.41% 79.33%67.12% 66.93%UnlabeledFCCL [15]93.62% 92.88%82.03% 79.75%68.77% 66.49%pFedHR93.89% 93.76% 83.15% 80.24%69.38% 68.01%\n\nTable 3 :\n3\nEvaluation using a large number of clients on the SVHN dataset (N = 100).\nPublic Data ModelIIDNon-IIDFedMD78.16%74.34%LabeledFedGH76.27%72.78%pFedHR80.02% 77.63%FedKEAF 76.27%74.61%UnlabeledFCCL75.03%71.54%pFedHR78.98% 75.77%\n\nTable 4 :\n4\nHomogeneous model comparison with baselines.\nModelDataset SettingMNIST IID Non-IIDIIDSVHN Non-IIDCIFAR-10 IID Non-IIDFedAvg [4]91.23% 90.04%53.45% 51.33%43.05% 33.39%FedProx [2]92.66% 92.47%54.86% 53.09%43.62% 35.06%M1Per-FedAvg [23] 93.23% 93.04% PFedMe [24] 93.57% 92.00%54.29% 52.04% 55.01% 53.78%44.14% 42.02% 45.01% 43.65%PFedBayes [25]94.39% 93.32% 58.49% 55.74%46.12% 44.49%pFedHR94.26% 93.26% 61.72% 59.23% 54.38% 48.44%FedAvg [4]94.24% 92.16%83.26% 82.77%67.68% 58.92%FedProx [2]94.22% 93.22%84.72% 83.00%71.24% 63.98%M4Per-FedAvg [23] 95.77% 93.67% PFedMe [24] 95.71% 94.02% 87.63% 85.33% 85.99% 84.01%79.56% 76.23% 79.88% 77.56%PFedBayes [25]95.64% 93.23%88.34% 86.28%80.06% 77.93%pFedHR94.88% 93.77% 89.87% 87.94% 81.54% 79.45%\nNote that the network architecture of both local model and personalized model are known, and thus, there is no human intervention in the client update.\nAcknowledgements This work is partially supported by the National Science Foundation under Grant No. 2212323 and 2238275.\nAdvances and open problems in federated learning. Peter Kairouz, Brendan Mcmahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Nitin Arjun, Kallista Bhagoji, Zachary Bonawitz, Graham Charles, Rachel Cormode, Cummings, Foundations and Trends\u00ae in Machine Learning. 202114\n\nFederated optimization in heterogeneous networks. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, Virginia Smith, Proceedings of Machine learning and systems. Machine learning and systems20202\n\nA survey on security and privacy of federated learning. Reza M Viraaji Mothukuri, Seyedamin Parizi, Yan Pouriyeh, Ali Huang, Gautam Dehghantanha, Srivastava, Future Generation Computer Systems. 1152021\n\nCommunication-efficient learning of deep networks from decentralized data. Brendan Mcmahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Aguera Y Arcas, Artificial intelligence and statistics. PMLR2017\n\nFederated learning from pre-trained models: A contrastive learning approach. Yue Tan, Guodong Long, Jie Ma, Lu Liu, Tianyi Zhou, Jing Jiang, arXiv:2209.100832022arXiv preprint\n\nFedproto: Federated prototype learning across heterogeneous clients. Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, Chengqi Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236\n\nFederated learning with partial model personalization. Krishna Pillutla, Kshitiz Malik, Abdel-Rahman Mohamed, Mike Rabbat, Maziar Sanjabi, Lin Xiao, International Conference on Machine Learning. PMLR2022\n\nAn efficient framework for clustered federated learning. Avishek Ghosh, Jichan Chung, Dong Yin, Kannan Ramchandran, Advances in Neural Information Processing Systems. 202033\n\nTianfei Zhou, Ender Konukoglu, arXiv:2301.12995Fedfa: Federated feature augmentation. 2023arXiv preprint\n\nPersonalized federated learning with graph. Fengwen Chen, Guodong Long, Zonghan Wu, Tianyi Zhou, Jing Jiang, arXiv:2203.008292022arXiv preprint\n\nTackling the objective inconsistency problem in heterogeneous federated optimization. Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, Vincent Poor, Advances in neural information processing systems. 202033\n\nPersonalized federated learning under mixture of distributions. Yue Wu, Shuaicheng Zhang, Wenchao Yu, Yanchi Liu, Quanquan Gu, Dawei Zhou, Haifeng Chen, Wei Cheng, arXiv:2305.010682023arXiv preprint\n\nCommunicationefficient federated learning via knowledge distillation. Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, Xing Xie, Nature communications. 13120322022\n\nFedmd: Heterogenous federated learning via model distillation. Daliang Li, Junpu Wang, arXiv:1910.035812019arXiv preprint\n\nLearn from others and be yourself in heterogeneous federated learning. Wenke Huang, Mang Ye, Bo Du, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022\n\nEnsemble distillation for robust model fusion in federated learning. Tao Lin, Lingjing Kong, Sebastian U Stich, Martin Jaggi, Advances in Neural Information Processing Systems. 202033\n\nResource-aware federated learning using knowledge extraction and multi-model fusion. Sixing Yu, Wei Qian, Ali Jannesari, arXiv:2208.079782022arXiv preprint\n\nFederated model distillation with noise-free differential privacy. Lichao Sun, Lingjuan Lyu, arXiv:2009.055372020arXiv preprint\n\nFedgh: Heterogeneous federated learning with generalized global header. Liping Yi, Gang Wang, Xiaoguang Liu, Zhuan Shi, Han Yu, arXiv:2303.131372023arXiv preprint\n\nPrivacy and robustness in federated learning: Attacks and defenses. Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao, Qiang Yang, Philip Yu, IEEE transactions on neural networks and learning systems. 2022\n\nKnowledge-enhanced semi-supervised federated learning for aggregating heterogeneous lightweight clients in iot. Jiaqi Wang, Shenglai Zeng, Zewei Long, Yaqing Wang, Houping Xiao, Fenglong Ma, Proceedings of the 2023 SIAM International Conference on Data Mining (SDM). the 2023 SIAM International Conference on Data Mining (SDM)SIAM2023\n\nDeep model reassembly. Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, Xinchao Wang, Advances in neural information processing systems. 202235\n\nPersonalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. Alireza Fallah, Aryan Mokhtari, Asuman Ozdaglar, Advances in Neural Information Processing Systems. 202033\n\nPersonalized federated learning with moreau envelopes. Nguyen Canh T Dinh, Josh Tran, Nguyen, Advances in Neural Information Processing Systems. 202033\n\nPersonalized federated learning via variational bayesian inference. Xu Zhang, Yinchuan Li, Wenpeng Li, Kaiyang Guo, Yunfeng Shao, International Conference on Machine Learning. PMLR2022\n\nHeterofl: Computation and communication efficient federated learning for heterogeneous clients. Enmao Diao, Jie Ding, Vahid Tarokh, arXiv:2010.012642020arXiv preprint\n\nHeterogeneous model fusion federated learning mechanism based on model mapping. Xiaofeng Lu, Yuying Liao, Chao Liu, Pietro Lio, Pan Hui, IEEE Internet of Things Journal. 982021\n\nVertical federated learning without revealing intersection membership. Jiankai Sun, Xin Yang, Yuanshun Yao, Aonan Zhang, Weihao Gao, Junyuan Xie, Chong Wang, arXiv:2106.055082021arXiv preprint\n\nRevisiting model stitching to compare neural representations. Yamini Bansal, Preetum Nakkiran, Boaz Barak, Advances in neural information processing systems. 202134\n\nGit re-basin: Merging models modulo permutation symmetries. Jonathan Samuel K Ainsworth, Siddhartha Hayase, Srinivasa, arXiv:2209.048362022arXiv preprint\n\nZizheng Pan, Jianfei Cai, Bohan Zhuang, arXiv:2302.06586Stitchable neural networks. 2023arXiv preprint\n\nOn crosslayer alignment for model fusion of heterogeneous neural networks. Dang Nguyen, Trang Nguyen, Khai Nguyen, Dinh Phung, Hung Bui, Nhat Ho, ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2023\n\nSimilarity of neural network representations revisited. Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton, International Conference on Machine Learning. PMLR2019\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, International conference on machine learning. PMLR2020\n\nDeep mutual learning. Ying Zhang, Tao Xiang, Timothy M Hospedales, Huchuan Lu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018\n", "annotations": {"author": "[{\"end\":151,\"start\":89},{\"end\":215,\"start\":152},{\"end\":262,\"start\":216},{\"end\":309,\"start\":263},{\"end\":354,\"start\":310},{\"end\":401,\"start\":355},{\"end\":467,\"start\":402}]", "publisher": null, "author_last_name": "[{\"end\":99,\"start\":95},{\"end\":163,\"start\":159},{\"end\":225,\"start\":222},{\"end\":272,\"start\":269},{\"end\":322,\"start\":319},{\"end\":366,\"start\":364},{\"end\":413,\"start\":411}]", "author_first_name": "[{\"end\":94,\"start\":89},{\"end\":158,\"start\":152},{\"end\":221,\"start\":216},{\"end\":268,\"start\":263},{\"end\":318,\"start\":310},{\"end\":363,\"start\":355},{\"end\":410,\"start\":402}]", "author_affiliation": "[{\"end\":150,\"start\":116},{\"end\":214,\"start\":181},{\"end\":261,\"start\":227},{\"end\":308,\"start\":274},{\"end\":353,\"start\":345},{\"end\":400,\"start\":368},{\"end\":466,\"start\":432}]", "title": "[{\"end\":75,\"start\":1},{\"end\":542,\"start\":468}]", "venue": null, "abstract": "[{\"end\":1711,\"start\":612}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1884,\"start\":1881},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1887,\"start\":1884},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1890,\"start\":1887},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1945,\"start\":1942},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1948,\"start\":1945},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1951,\"start\":1948},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1954,\"start\":1951},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1957,\"start\":1954},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1960,\"start\":1957},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1963,\"start\":1960},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1967,\"start\":1963},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1971,\"start\":1967},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1975,\"start\":1971},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2685,\"start\":2681},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2689,\"start\":2685},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2693,\"start\":2689},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2697,\"start\":2693},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2701,\"start\":2697},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2815,\"start\":2811},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2819,\"start\":2815},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2823,\"start\":2819},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2827,\"start\":2823},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3026,\"start\":3022},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3039,\"start\":3035},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3042,\"start\":3039},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3079,\"start\":3075},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3331,\"start\":3327},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3578,\"start\":3574},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3581,\"start\":3578},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3584,\"start\":3581},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3698,\"start\":3694},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4312,\"start\":4308},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4315,\"start\":4312},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4338,\"start\":4334},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5751,\"start\":5747},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9236,\"start\":9233},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9415,\"start\":9412},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10174,\"start\":10171},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10187,\"start\":10184},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10204,\"start\":10200},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10217,\"start\":10213},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10237,\"start\":10233},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10561,\"start\":10557},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10564,\"start\":10561},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10715,\"start\":10711},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11039,\"start\":11035},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11053,\"start\":11049},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11068,\"start\":11064},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11544,\"start\":11540},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11547,\"start\":11544},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11558,\"start\":11554},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11573,\"start\":11569},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12455,\"start\":12451},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12459,\"start\":12455},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12463,\"start\":12459},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12467,\"start\":12463},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12471,\"start\":12467},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12553,\"start\":12549},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12807,\"start\":12803},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12816,\"start\":12812},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16721,\"start\":16717},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20551,\"start\":20547},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22257,\"start\":22253},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22720,\"start\":22716},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23383,\"start\":23379},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23398,\"start\":23394},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23461,\"start\":23457},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23478,\"start\":23474},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23683,\"start\":23680},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23696,\"start\":23693},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23713,\"start\":23709},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23726,\"start\":23722},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23746,\"start\":23742},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23836,\"start\":23832},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25136,\"start\":25133}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34126,\"start\":33756},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34212,\"start\":34127},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35141,\"start\":34213},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35211,\"start\":35142},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35266,\"start\":35212},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35334,\"start\":35267},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35724,\"start\":35335},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36151,\"start\":35725},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36648,\"start\":36152},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37719,\"start\":36649},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38180,\"start\":37720},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38419,\"start\":38181},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":39172,\"start\":38420}]", "paragraph": "[{\"end\":2884,\"start\":1727},{\"end\":4170,\"start\":2886},{\"end\":4372,\"start\":4172},{\"end\":5116,\"start\":4374},{\"end\":5874,\"start\":5118},{\"end\":6615,\"start\":5876},{\"end\":7428,\"start\":6617},{\"end\":8350,\"start\":7430},{\"end\":9169,\"start\":8404},{\"end\":10056,\"start\":9171},{\"end\":10940,\"start\":10073},{\"end\":12117,\"start\":10942},{\"end\":12886,\"start\":12119},{\"end\":13095,\"start\":12902},{\"end\":13228,\"start\":13113},{\"end\":13682,\"start\":13263},{\"end\":13857,\"start\":13718},{\"end\":14260,\"start\":13931},{\"end\":14413,\"start\":14373},{\"end\":14783,\"start\":14660},{\"end\":14951,\"start\":14906},{\"end\":15176,\"start\":14990},{\"end\":15729,\"start\":15218},{\"end\":15825,\"start\":15731},{\"end\":16227,\"start\":15884},{\"end\":16805,\"start\":16229},{\"end\":17122,\"start\":16912},{\"end\":17400,\"start\":17124},{\"end\":17716,\"start\":17484},{\"end\":17878,\"start\":17751},{\"end\":17963,\"start\":17880},{\"end\":18251,\"start\":18067},{\"end\":18936,\"start\":18253},{\"end\":19776,\"start\":18938},{\"end\":21438,\"start\":19832},{\"end\":21588,\"start\":21440},{\"end\":22258,\"start\":21690},{\"end\":22721,\"start\":22276},{\"end\":22942,\"start\":22723},{\"end\":23135,\"start\":23038},{\"end\":23518,\"start\":23139},{\"end\":23747,\"start\":23520},{\"end\":24835,\"start\":23783},{\"end\":25416,\"start\":24837},{\"end\":26295,\"start\":25418},{\"end\":27590,\"start\":26330},{\"end\":28097,\"start\":27618},{\"end\":29789,\"start\":28099},{\"end\":30193,\"start\":29847},{\"end\":30821,\"start\":30195},{\"end\":31354,\"start\":30847},{\"end\":32007,\"start\":31356},{\"end\":32398,\"start\":32009},{\"end\":33056,\"start\":32400},{\"end\":33755,\"start\":33071},{\"end\":34125,\"start\":33770},{\"end\":34211,\"start\":34141},{\"end\":35140,\"start\":34227},{\"end\":35210,\"start\":35156},{\"end\":35265,\"start\":35226},{\"end\":35333,\"start\":35292},{\"end\":35625,\"start\":35338},{\"end\":35723,\"start\":35627},{\"end\":35821,\"start\":35738},{\"end\":36407,\"start\":36157},{\"end\":36647,\"start\":36425},{\"end\":37546,\"start\":36652},{\"end\":37803,\"start\":37733},{\"end\":38267,\"start\":38194},{\"end\":38477,\"start\":38433}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8403,\"start\":8351},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13262,\"start\":13229},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13930,\"start\":13858},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14372,\"start\":14261},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14659,\"start\":14414},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14905,\"start\":14784},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14989,\"start\":14952},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15883,\"start\":15826},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16911,\"start\":16806},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17483,\"start\":17401},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17750,\"start\":17717},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18066,\"start\":17964},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21689,\"start\":21589},{\"attributes\":{\"id\":\"formula_13\"},\"end\":23037,\"start\":22943},{\"attributes\":{\"id\":\"formula_14\"},\"end\":23138,\"start\":23136}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12116,\"start\":12115},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12184,\"start\":12183},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24441,\"start\":24440},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25500,\"start\":25499},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25669,\"start\":25668},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25865,\"start\":25864},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":26863,\"start\":26862}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1725,\"start\":1713},{\"attributes\":{\"n\":\"2\"},\"end\":10071,\"start\":10059},{\"attributes\":{\"n\":\"3\"},\"end\":12900,\"start\":12889},{\"attributes\":{\"n\":\"3.1\"},\"end\":13111,\"start\":13098},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":13716,\"start\":13685},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":15216,\"start\":15179},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":19830,\"start\":19779},{\"attributes\":{\"n\":\"3.2\"},\"end\":22274,\"start\":22261},{\"attributes\":{\"n\":\"4.2\"},\"end\":23781,\"start\":23750},{\"attributes\":{\"n\":\"4.3\"},\"end\":26328,\"start\":26298},{\"attributes\":{\"n\":\"4.4\"},\"end\":27616,\"start\":27593},{\"attributes\":{\"n\":\"4.5\"},\"end\":29845,\"start\":29792},{\"attributes\":{\"n\":\"4.6\"},\"end\":30845,\"start\":30824},{\"attributes\":{\"n\":\"5\"},\"end\":33069,\"start\":33059},{\"end\":33767,\"start\":33757},{\"end\":34138,\"start\":34128},{\"end\":34224,\"start\":34214},{\"end\":35153,\"start\":35143},{\"end\":35223,\"start\":35213},{\"end\":35288,\"start\":35268},{\"end\":35735,\"start\":35726},{\"end\":36155,\"start\":36153},{\"end\":37730,\"start\":37721},{\"end\":38191,\"start\":38182},{\"end\":38430,\"start\":38421}]", "table": "[{\"end\":36151,\"start\":35822},{\"end\":36424,\"start\":36408},{\"end\":37719,\"start\":37547},{\"end\":38180,\"start\":37804},{\"end\":38419,\"start\":38268},{\"end\":39172,\"start\":38478}]", "figure_caption": "[{\"end\":34126,\"start\":33769},{\"end\":34212,\"start\":34140},{\"end\":35141,\"start\":34226},{\"end\":35211,\"start\":35155},{\"end\":35266,\"start\":35225},{\"end\":35334,\"start\":35291},{\"end\":35626,\"start\":35337},{\"end\":35822,\"start\":35737},{\"end\":36408,\"start\":36156},{\"end\":37547,\"start\":36651},{\"end\":37804,\"start\":37732},{\"end\":38268,\"start\":38193},{\"end\":38478,\"start\":38432}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4704,\"start\":4703},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6869,\"start\":6868},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8186,\"start\":8185},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13012,\"start\":13011},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28107,\"start\":28106},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29473,\"start\":29472},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30203,\"start\":30202},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31594,\"start\":31593},{\"end\":32166,\"start\":32165},{\"end\":32692,\"start\":32691}]", "bib_author_first_name": "[{\"end\":39502,\"start\":39497},{\"end\":39519,\"start\":39512},{\"end\":39536,\"start\":39529},{\"end\":39552,\"start\":39544},{\"end\":39566,\"start\":39561},{\"end\":39580,\"start\":39575},{\"end\":39596,\"start\":39588},{\"end\":39613,\"start\":39606},{\"end\":39630,\"start\":39624},{\"end\":39646,\"start\":39640},{\"end\":39773,\"start\":39769},{\"end\":39782,\"start\":39778},{\"end\":39801,\"start\":39795},{\"end\":39816,\"start\":39810},{\"end\":39831,\"start\":39826},{\"end\":39851,\"start\":39843},{\"end\":39999,\"start\":39995},{\"end\":40001,\"start\":40000},{\"end\":40030,\"start\":40021},{\"end\":40042,\"start\":40039},{\"end\":40056,\"start\":40053},{\"end\":40070,\"start\":40064},{\"end\":40224,\"start\":40217},{\"end\":40239,\"start\":40234},{\"end\":40253,\"start\":40247},{\"end\":40266,\"start\":40262},{\"end\":40282,\"start\":40276},{\"end\":40429,\"start\":40426},{\"end\":40442,\"start\":40435},{\"end\":40452,\"start\":40449},{\"end\":40459,\"start\":40457},{\"end\":40471,\"start\":40465},{\"end\":40482,\"start\":40478},{\"end\":40598,\"start\":40595},{\"end\":40611,\"start\":40604},{\"end\":40620,\"start\":40618},{\"end\":40632,\"start\":40626},{\"end\":40646,\"start\":40639},{\"end\":40655,\"start\":40651},{\"end\":40670,\"start\":40663},{\"end\":40857,\"start\":40850},{\"end\":40875,\"start\":40868},{\"end\":40895,\"start\":40883},{\"end\":40909,\"start\":40905},{\"end\":40924,\"start\":40918},{\"end\":40937,\"start\":40934},{\"end\":41064,\"start\":41057},{\"end\":41078,\"start\":41072},{\"end\":41090,\"start\":41086},{\"end\":41102,\"start\":41096},{\"end\":41182,\"start\":41175},{\"end\":41194,\"start\":41189},{\"end\":41332,\"start\":41325},{\"end\":41346,\"start\":41339},{\"end\":41360,\"start\":41353},{\"end\":41371,\"start\":41365},{\"end\":41382,\"start\":41378},{\"end\":41518,\"start\":41512},{\"end\":41532,\"start\":41525},{\"end\":41541,\"start\":41538},{\"end\":41554,\"start\":41549},{\"end\":41702,\"start\":41699},{\"end\":41717,\"start\":41707},{\"end\":41732,\"start\":41725},{\"end\":41743,\"start\":41737},{\"end\":41757,\"start\":41749},{\"end\":41767,\"start\":41762},{\"end\":41781,\"start\":41774},{\"end\":41791,\"start\":41788},{\"end\":41911,\"start\":41905},{\"end\":41924,\"start\":41916},{\"end\":41937,\"start\":41929},{\"end\":41951,\"start\":41943},{\"end\":41963,\"start\":41959},{\"end\":42075,\"start\":42068},{\"end\":42085,\"start\":42080},{\"end\":42204,\"start\":42199},{\"end\":42216,\"start\":42212},{\"end\":42223,\"start\":42221},{\"end\":42455,\"start\":42452},{\"end\":42469,\"start\":42461},{\"end\":42485,\"start\":42476},{\"end\":42487,\"start\":42486},{\"end\":42501,\"start\":42495},{\"end\":42659,\"start\":42653},{\"end\":42667,\"start\":42664},{\"end\":42677,\"start\":42674},{\"end\":42798,\"start\":42792},{\"end\":42812,\"start\":42804},{\"end\":42932,\"start\":42926},{\"end\":42941,\"start\":42937},{\"end\":42957,\"start\":42948},{\"end\":42968,\"start\":42963},{\"end\":42977,\"start\":42974},{\"end\":43094,\"start\":43086},{\"end\":43103,\"start\":43100},{\"end\":43115,\"start\":43108},{\"end\":43124,\"start\":43120},{\"end\":43137,\"start\":43131},{\"end\":43146,\"start\":43143},{\"end\":43158,\"start\":43153},{\"end\":43171,\"start\":43165},{\"end\":43358,\"start\":43353},{\"end\":43373,\"start\":43365},{\"end\":43385,\"start\":43380},{\"end\":43398,\"start\":43392},{\"end\":43412,\"start\":43405},{\"end\":43427,\"start\":43419},{\"end\":43606,\"start\":43600},{\"end\":43619,\"start\":43613},{\"end\":43633,\"start\":43626},{\"end\":43646,\"start\":43639},{\"end\":43658,\"start\":43651},{\"end\":43833,\"start\":43826},{\"end\":43847,\"start\":43842},{\"end\":43864,\"start\":43858},{\"end\":43995,\"start\":43989},{\"end\":44013,\"start\":44009},{\"end\":44157,\"start\":44155},{\"end\":44173,\"start\":44165},{\"end\":44185,\"start\":44178},{\"end\":44197,\"start\":44190},{\"end\":44210,\"start\":44203},{\"end\":44374,\"start\":44369},{\"end\":44384,\"start\":44381},{\"end\":44396,\"start\":44391},{\"end\":44529,\"start\":44521},{\"end\":44540,\"start\":44534},{\"end\":44551,\"start\":44547},{\"end\":44563,\"start\":44557},{\"end\":44572,\"start\":44569},{\"end\":44697,\"start\":44690},{\"end\":44706,\"start\":44703},{\"end\":44721,\"start\":44713},{\"end\":44732,\"start\":44727},{\"end\":44746,\"start\":44740},{\"end\":44759,\"start\":44752},{\"end\":44770,\"start\":44765},{\"end\":44881,\"start\":44875},{\"end\":44897,\"start\":44890},{\"end\":44912,\"start\":44908},{\"end\":45047,\"start\":45039},{\"end\":45078,\"start\":45068},{\"end\":45141,\"start\":45134},{\"end\":45154,\"start\":45147},{\"end\":45165,\"start\":45160},{\"end\":45317,\"start\":45313},{\"end\":45331,\"start\":45326},{\"end\":45344,\"start\":45340},{\"end\":45357,\"start\":45353},{\"end\":45369,\"start\":45365},{\"end\":45379,\"start\":45375},{\"end\":45555,\"start\":45550},{\"end\":45575,\"start\":45567},{\"end\":45592,\"start\":45585},{\"end\":45606,\"start\":45598},{\"end\":45746,\"start\":45742},{\"end\":45758,\"start\":45753},{\"end\":45778,\"start\":45770},{\"end\":45796,\"start\":45788},{\"end\":45887,\"start\":45883},{\"end\":45898,\"start\":45895},{\"end\":45913,\"start\":45906},{\"end\":45915,\"start\":45914},{\"end\":45935,\"start\":45928}]", "bib_author_last_name": "[{\"end\":39510,\"start\":39503},{\"end\":39527,\"start\":39520},{\"end\":39542,\"start\":39537},{\"end\":39559,\"start\":39553},{\"end\":39573,\"start\":39567},{\"end\":39586,\"start\":39581},{\"end\":39604,\"start\":39597},{\"end\":39622,\"start\":39614},{\"end\":39638,\"start\":39631},{\"end\":39654,\"start\":39647},{\"end\":39664,\"start\":39656},{\"end\":39776,\"start\":39774},{\"end\":39793,\"start\":39783},{\"end\":39808,\"start\":39802},{\"end\":39824,\"start\":39817},{\"end\":39841,\"start\":39832},{\"end\":39857,\"start\":39852},{\"end\":40019,\"start\":40002},{\"end\":40037,\"start\":40031},{\"end\":40051,\"start\":40043},{\"end\":40062,\"start\":40057},{\"end\":40083,\"start\":40071},{\"end\":40095,\"start\":40085},{\"end\":40232,\"start\":40225},{\"end\":40245,\"start\":40240},{\"end\":40260,\"start\":40254},{\"end\":40274,\"start\":40267},{\"end\":40297,\"start\":40283},{\"end\":40433,\"start\":40430},{\"end\":40447,\"start\":40443},{\"end\":40455,\"start\":40453},{\"end\":40463,\"start\":40460},{\"end\":40476,\"start\":40472},{\"end\":40488,\"start\":40483},{\"end\":40602,\"start\":40599},{\"end\":40616,\"start\":40612},{\"end\":40624,\"start\":40621},{\"end\":40637,\"start\":40633},{\"end\":40649,\"start\":40647},{\"end\":40661,\"start\":40656},{\"end\":40676,\"start\":40671},{\"end\":40866,\"start\":40858},{\"end\":40881,\"start\":40876},{\"end\":40903,\"start\":40896},{\"end\":40916,\"start\":40910},{\"end\":40932,\"start\":40925},{\"end\":40942,\"start\":40938},{\"end\":41070,\"start\":41065},{\"end\":41084,\"start\":41079},{\"end\":41094,\"start\":41091},{\"end\":41114,\"start\":41103},{\"end\":41187,\"start\":41183},{\"end\":41204,\"start\":41195},{\"end\":41337,\"start\":41333},{\"end\":41351,\"start\":41347},{\"end\":41363,\"start\":41361},{\"end\":41376,\"start\":41372},{\"end\":41388,\"start\":41383},{\"end\":41523,\"start\":41519},{\"end\":41536,\"start\":41533},{\"end\":41547,\"start\":41542},{\"end\":41560,\"start\":41555},{\"end\":41574,\"start\":41562},{\"end\":41705,\"start\":41703},{\"end\":41723,\"start\":41718},{\"end\":41735,\"start\":41733},{\"end\":41747,\"start\":41744},{\"end\":41760,\"start\":41758},{\"end\":41772,\"start\":41768},{\"end\":41786,\"start\":41782},{\"end\":41797,\"start\":41792},{\"end\":41914,\"start\":41912},{\"end\":41927,\"start\":41925},{\"end\":41941,\"start\":41938},{\"end\":41957,\"start\":41952},{\"end\":41967,\"start\":41964},{\"end\":42078,\"start\":42076},{\"end\":42090,\"start\":42086},{\"end\":42210,\"start\":42205},{\"end\":42219,\"start\":42217},{\"end\":42226,\"start\":42224},{\"end\":42459,\"start\":42456},{\"end\":42474,\"start\":42470},{\"end\":42493,\"start\":42488},{\"end\":42507,\"start\":42502},{\"end\":42662,\"start\":42660},{\"end\":42672,\"start\":42668},{\"end\":42687,\"start\":42678},{\"end\":42802,\"start\":42799},{\"end\":42816,\"start\":42813},{\"end\":42935,\"start\":42933},{\"end\":42946,\"start\":42942},{\"end\":42961,\"start\":42958},{\"end\":42972,\"start\":42969},{\"end\":42980,\"start\":42978},{\"end\":43098,\"start\":43095},{\"end\":43106,\"start\":43104},{\"end\":43118,\"start\":43116},{\"end\":43129,\"start\":43125},{\"end\":43141,\"start\":43138},{\"end\":43151,\"start\":43147},{\"end\":43163,\"start\":43159},{\"end\":43174,\"start\":43172},{\"end\":43363,\"start\":43359},{\"end\":43378,\"start\":43374},{\"end\":43390,\"start\":43386},{\"end\":43403,\"start\":43399},{\"end\":43417,\"start\":43413},{\"end\":43430,\"start\":43428},{\"end\":43611,\"start\":43607},{\"end\":43624,\"start\":43620},{\"end\":43637,\"start\":43634},{\"end\":43649,\"start\":43647},{\"end\":43663,\"start\":43659},{\"end\":43840,\"start\":43834},{\"end\":43856,\"start\":43848},{\"end\":43873,\"start\":43865},{\"end\":44007,\"start\":43996},{\"end\":44018,\"start\":44014},{\"end\":44026,\"start\":44020},{\"end\":44163,\"start\":44158},{\"end\":44176,\"start\":44174},{\"end\":44188,\"start\":44186},{\"end\":44201,\"start\":44198},{\"end\":44215,\"start\":44211},{\"end\":44379,\"start\":44375},{\"end\":44389,\"start\":44385},{\"end\":44403,\"start\":44397},{\"end\":44532,\"start\":44530},{\"end\":44545,\"start\":44541},{\"end\":44555,\"start\":44552},{\"end\":44567,\"start\":44564},{\"end\":44576,\"start\":44573},{\"end\":44701,\"start\":44698},{\"end\":44711,\"start\":44707},{\"end\":44725,\"start\":44722},{\"end\":44738,\"start\":44733},{\"end\":44750,\"start\":44747},{\"end\":44763,\"start\":44760},{\"end\":44775,\"start\":44771},{\"end\":44888,\"start\":44882},{\"end\":44906,\"start\":44898},{\"end\":44918,\"start\":44913},{\"end\":45066,\"start\":45048},{\"end\":45085,\"start\":45079},{\"end\":45096,\"start\":45087},{\"end\":45145,\"start\":45142},{\"end\":45158,\"start\":45155},{\"end\":45172,\"start\":45166},{\"end\":45324,\"start\":45318},{\"end\":45338,\"start\":45332},{\"end\":45351,\"start\":45345},{\"end\":45363,\"start\":45358},{\"end\":45373,\"start\":45370},{\"end\":45382,\"start\":45380},{\"end\":45565,\"start\":45556},{\"end\":45583,\"start\":45576},{\"end\":45596,\"start\":45593},{\"end\":45613,\"start\":45607},{\"end\":45751,\"start\":45747},{\"end\":45768,\"start\":45759},{\"end\":45786,\"start\":45779},{\"end\":45803,\"start\":45797},{\"end\":45893,\"start\":45888},{\"end\":45904,\"start\":45899},{\"end\":45926,\"start\":45916},{\"end\":45938,\"start\":45936}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":209202606},\"end\":39717,\"start\":39447},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":59316566},\"end\":39937,\"start\":39719},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":225140677},\"end\":40140,\"start\":39939},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14955348},\"end\":40347,\"start\":40142},{\"attributes\":{\"doi\":\"arXiv:2209.10083\",\"id\":\"b4\"},\"end\":40524,\"start\":40349},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":247292268},\"end\":40793,\"start\":40526},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":248069201},\"end\":40998,\"start\":40795},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":219530697},\"end\":41173,\"start\":41000},{\"attributes\":{\"doi\":\"arXiv:2301.12995\",\"id\":\"b8\"},\"end\":41279,\"start\":41175},{\"attributes\":{\"doi\":\"arXiv:2203.00829\",\"id\":\"b9\"},\"end\":41424,\"start\":41281},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220525591},\"end\":41633,\"start\":41426},{\"attributes\":{\"doi\":\"arXiv:2305.01068\",\"id\":\"b11\"},\"end\":41833,\"start\":41635},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":237353469},\"end\":42003,\"start\":41835},{\"attributes\":{\"doi\":\"arXiv:1910.03581\",\"id\":\"b13\"},\"end\":42126,\"start\":42005},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":250210682},\"end\":42381,\"start\":42128},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219636007},\"end\":42566,\"start\":42383},{\"attributes\":{\"doi\":\"arXiv:2208.07978\",\"id\":\"b16\"},\"end\":42723,\"start\":42568},{\"attributes\":{\"doi\":\"arXiv:2009.05537\",\"id\":\"b17\"},\"end\":42852,\"start\":42725},{\"attributes\":{\"doi\":\"arXiv:2303.13137\",\"id\":\"b18\"},\"end\":43016,\"start\":42854},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":228373690},\"end\":43239,\"start\":43018},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":257364769},\"end\":43575,\"start\":43241},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":253236958},\"end\":43722,\"start\":43577},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":227276412},\"end\":43932,\"start\":43724},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":219708331},\"end\":44085,\"start\":43934},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":249712079},\"end\":44271,\"start\":44087},{\"attributes\":{\"doi\":\"arXiv:2010.01264\",\"id\":\"b25\"},\"end\":44439,\"start\":44273},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":239759357},\"end\":44617,\"start\":44441},{\"attributes\":{\"doi\":\"arXiv:2106.05508\",\"id\":\"b27\"},\"end\":44811,\"start\":44619},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":235435759},\"end\":44977,\"start\":44813},{\"attributes\":{\"doi\":\"arXiv:2209.04836\",\"id\":\"b29\"},\"end\":45132,\"start\":44979},{\"attributes\":{\"doi\":\"arXiv:2302.06586\",\"id\":\"b30\"},\"end\":45236,\"start\":45134},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":257037999},\"end\":45492,\"start\":45238},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":141460329},\"end\":45669,\"start\":45494},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":211096730},\"end\":45859,\"start\":45671},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":26071966},\"end\":46085,\"start\":45861}]", "bib_title": "[{\"end\":39495,\"start\":39447},{\"end\":39767,\"start\":39719},{\"end\":39993,\"start\":39939},{\"end\":40215,\"start\":40142},{\"end\":40593,\"start\":40526},{\"end\":40848,\"start\":40795},{\"end\":41055,\"start\":41000},{\"end\":41510,\"start\":41426},{\"end\":41903,\"start\":41835},{\"end\":42197,\"start\":42128},{\"end\":42450,\"start\":42383},{\"end\":43084,\"start\":43018},{\"end\":43351,\"start\":43241},{\"end\":43598,\"start\":43577},{\"end\":43824,\"start\":43724},{\"end\":43987,\"start\":43934},{\"end\":44153,\"start\":44087},{\"end\":44519,\"start\":44441},{\"end\":44873,\"start\":44813},{\"end\":45311,\"start\":45238},{\"end\":45548,\"start\":45494},{\"end\":45740,\"start\":45671},{\"end\":45881,\"start\":45861}]", "bib_author": "[{\"end\":39512,\"start\":39497},{\"end\":39529,\"start\":39512},{\"end\":39544,\"start\":39529},{\"end\":39561,\"start\":39544},{\"end\":39575,\"start\":39561},{\"end\":39588,\"start\":39575},{\"end\":39606,\"start\":39588},{\"end\":39624,\"start\":39606},{\"end\":39640,\"start\":39624},{\"end\":39656,\"start\":39640},{\"end\":39666,\"start\":39656},{\"end\":39778,\"start\":39769},{\"end\":39795,\"start\":39778},{\"end\":39810,\"start\":39795},{\"end\":39826,\"start\":39810},{\"end\":39843,\"start\":39826},{\"end\":39859,\"start\":39843},{\"end\":40021,\"start\":39995},{\"end\":40039,\"start\":40021},{\"end\":40053,\"start\":40039},{\"end\":40064,\"start\":40053},{\"end\":40085,\"start\":40064},{\"end\":40097,\"start\":40085},{\"end\":40234,\"start\":40217},{\"end\":40247,\"start\":40234},{\"end\":40262,\"start\":40247},{\"end\":40276,\"start\":40262},{\"end\":40299,\"start\":40276},{\"end\":40435,\"start\":40426},{\"end\":40449,\"start\":40435},{\"end\":40457,\"start\":40449},{\"end\":40465,\"start\":40457},{\"end\":40478,\"start\":40465},{\"end\":40490,\"start\":40478},{\"end\":40604,\"start\":40595},{\"end\":40618,\"start\":40604},{\"end\":40626,\"start\":40618},{\"end\":40639,\"start\":40626},{\"end\":40651,\"start\":40639},{\"end\":40663,\"start\":40651},{\"end\":40678,\"start\":40663},{\"end\":40868,\"start\":40850},{\"end\":40883,\"start\":40868},{\"end\":40905,\"start\":40883},{\"end\":40918,\"start\":40905},{\"end\":40934,\"start\":40918},{\"end\":40944,\"start\":40934},{\"end\":41072,\"start\":41057},{\"end\":41086,\"start\":41072},{\"end\":41096,\"start\":41086},{\"end\":41116,\"start\":41096},{\"end\":41189,\"start\":41175},{\"end\":41206,\"start\":41189},{\"end\":41339,\"start\":41325},{\"end\":41353,\"start\":41339},{\"end\":41365,\"start\":41353},{\"end\":41378,\"start\":41365},{\"end\":41390,\"start\":41378},{\"end\":41525,\"start\":41512},{\"end\":41538,\"start\":41525},{\"end\":41549,\"start\":41538},{\"end\":41562,\"start\":41549},{\"end\":41576,\"start\":41562},{\"end\":41707,\"start\":41699},{\"end\":41725,\"start\":41707},{\"end\":41737,\"start\":41725},{\"end\":41749,\"start\":41737},{\"end\":41762,\"start\":41749},{\"end\":41774,\"start\":41762},{\"end\":41788,\"start\":41774},{\"end\":41799,\"start\":41788},{\"end\":41916,\"start\":41905},{\"end\":41929,\"start\":41916},{\"end\":41943,\"start\":41929},{\"end\":41959,\"start\":41943},{\"end\":41969,\"start\":41959},{\"end\":42080,\"start\":42068},{\"end\":42092,\"start\":42080},{\"end\":42212,\"start\":42199},{\"end\":42221,\"start\":42212},{\"end\":42228,\"start\":42221},{\"end\":42461,\"start\":42452},{\"end\":42476,\"start\":42461},{\"end\":42495,\"start\":42476},{\"end\":42509,\"start\":42495},{\"end\":42664,\"start\":42653},{\"end\":42674,\"start\":42664},{\"end\":42689,\"start\":42674},{\"end\":42804,\"start\":42792},{\"end\":42818,\"start\":42804},{\"end\":42937,\"start\":42926},{\"end\":42948,\"start\":42937},{\"end\":42963,\"start\":42948},{\"end\":42974,\"start\":42963},{\"end\":42982,\"start\":42974},{\"end\":43100,\"start\":43086},{\"end\":43108,\"start\":43100},{\"end\":43120,\"start\":43108},{\"end\":43131,\"start\":43120},{\"end\":43143,\"start\":43131},{\"end\":43153,\"start\":43143},{\"end\":43165,\"start\":43153},{\"end\":43176,\"start\":43165},{\"end\":43365,\"start\":43353},{\"end\":43380,\"start\":43365},{\"end\":43392,\"start\":43380},{\"end\":43405,\"start\":43392},{\"end\":43419,\"start\":43405},{\"end\":43432,\"start\":43419},{\"end\":43613,\"start\":43600},{\"end\":43626,\"start\":43613},{\"end\":43639,\"start\":43626},{\"end\":43651,\"start\":43639},{\"end\":43665,\"start\":43651},{\"end\":43842,\"start\":43826},{\"end\":43858,\"start\":43842},{\"end\":43875,\"start\":43858},{\"end\":44009,\"start\":43989},{\"end\":44020,\"start\":44009},{\"end\":44028,\"start\":44020},{\"end\":44165,\"start\":44155},{\"end\":44178,\"start\":44165},{\"end\":44190,\"start\":44178},{\"end\":44203,\"start\":44190},{\"end\":44217,\"start\":44203},{\"end\":44381,\"start\":44369},{\"end\":44391,\"start\":44381},{\"end\":44405,\"start\":44391},{\"end\":44534,\"start\":44521},{\"end\":44547,\"start\":44534},{\"end\":44557,\"start\":44547},{\"end\":44569,\"start\":44557},{\"end\":44578,\"start\":44569},{\"end\":44703,\"start\":44690},{\"end\":44713,\"start\":44703},{\"end\":44727,\"start\":44713},{\"end\":44740,\"start\":44727},{\"end\":44752,\"start\":44740},{\"end\":44765,\"start\":44752},{\"end\":44777,\"start\":44765},{\"end\":44890,\"start\":44875},{\"end\":44908,\"start\":44890},{\"end\":44920,\"start\":44908},{\"end\":45068,\"start\":45039},{\"end\":45087,\"start\":45068},{\"end\":45098,\"start\":45087},{\"end\":45147,\"start\":45134},{\"end\":45160,\"start\":45147},{\"end\":45174,\"start\":45160},{\"end\":45326,\"start\":45313},{\"end\":45340,\"start\":45326},{\"end\":45353,\"start\":45340},{\"end\":45365,\"start\":45353},{\"end\":45375,\"start\":45365},{\"end\":45384,\"start\":45375},{\"end\":45567,\"start\":45550},{\"end\":45585,\"start\":45567},{\"end\":45598,\"start\":45585},{\"end\":45615,\"start\":45598},{\"end\":45753,\"start\":45742},{\"end\":45770,\"start\":45753},{\"end\":45788,\"start\":45770},{\"end\":45805,\"start\":45788},{\"end\":45895,\"start\":45883},{\"end\":45906,\"start\":45895},{\"end\":45928,\"start\":45906},{\"end\":45940,\"start\":45928}]", "bib_venue": "[{\"end\":39932,\"start\":39904},{\"end\":40787,\"start\":40741},{\"end\":42377,\"start\":42311},{\"end\":43567,\"start\":43508},{\"end\":46081,\"start\":46019},{\"end\":39709,\"start\":39666},{\"end\":39902,\"start\":39859},{\"end\":40131,\"start\":40097},{\"end\":40337,\"start\":40299},{\"end\":40424,\"start\":40349},{\"end\":40739,\"start\":40678},{\"end\":40988,\"start\":40944},{\"end\":41165,\"start\":41116},{\"end\":41259,\"start\":41222},{\"end\":41323,\"start\":41281},{\"end\":41625,\"start\":41576},{\"end\":41697,\"start\":41635},{\"end\":41990,\"start\":41969},{\"end\":42066,\"start\":42005},{\"end\":42309,\"start\":42228},{\"end\":42558,\"start\":42509},{\"end\":42651,\"start\":42568},{\"end\":42790,\"start\":42725},{\"end\":42924,\"start\":42854},{\"end\":43233,\"start\":43176},{\"end\":43506,\"start\":43432},{\"end\":43714,\"start\":43665},{\"end\":43924,\"start\":43875},{\"end\":44077,\"start\":44028},{\"end\":44261,\"start\":44217},{\"end\":44367,\"start\":44273},{\"end\":44609,\"start\":44578},{\"end\":44688,\"start\":44619},{\"end\":44969,\"start\":44920},{\"end\":45037,\"start\":44979},{\"end\":45216,\"start\":45190},{\"end\":45482,\"start\":45384},{\"end\":45659,\"start\":45615},{\"end\":45849,\"start\":45805},{\"end\":46017,\"start\":45940}]"}}}, "year": 2023, "month": 12, "day": 17}