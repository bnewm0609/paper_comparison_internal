{"id": 226222210, "updated": "2023-10-06 09:25:42.261", "metadata": {"title": "Generating Radiology Reports via Memory-driven Transformer", "authors": "[{\"first\":\"Zhihong\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yan\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Tsung-Hui\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Wan\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.16056", "mag": "3104609094", "acl": "2020.emnlp-main.112", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/ChenSCW20", "doi": "10.18653/v1/2020.emnlp-main.112"}}, "content": {"source": {"pdf_hash": "9e815edd2f12d9ebc3bf9005b3eabe72cc75ed8e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.16056v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.emnlp-main.112.pdf", "status": "HYBRID"}}, "grobid": {"id": "6073783ba0040c7139cb92c1d9017441e71de6f3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9e815edd2f12d9ebc3bf9005b3eabe72cc75ed8e.txt", "contents": "\nGenerating Radiology Reports via Memory-driven Transformer\n\n\nZhihong Chen \u2660zhihongchen@link.cuhk.edu.cn\u2660songyan \nThe Chinese University of Hong Kong (Shenzhen) \u2665 Shenzhen Research Institute of Big Data\n\n\n\u2660\u2665 \nThe Chinese University of Hong Kong (Shenzhen) \u2665 Shenzhen Research Institute of Big Data\n\n\nYan Song \u2660\u2665 \nThe Chinese University of Hong Kong (Shenzhen) \u2665 Shenzhen Research Institute of Big Data\n\n\nTsung-Hui Chang changtsunghui@cuhk.edu.cn\u2665wanxiang@sribd.cn \nThe Chinese University of Hong Kong (Shenzhen) \u2665 Shenzhen Research Institute of Big Data\n\n\n\u2660 \u2665 \nThe Chinese University of Hong Kong (Shenzhen) \u2665 Shenzhen Research Institute of Big Data\n\n\nXiang Wan \nThe Chinese University of Hong Kong (Shenzhen) \u2665 Shenzhen Research Institute of Big Data\n\n\nGenerating Radiology Reports via Memory-driven Transformer\n\nMedical imaging is frequently used in clinical practice and trials for diagnosis and treatment. Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists. Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain. In this paper, we propose to generate radiology reports with memorydriven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer. Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations. Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge. Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings. 1\n\nIntroduction\n\nRadiology report generation, which aims to automatically generate a free-text description for a clinical radiograph (e.g., chest X-ray), has emerged as a prominent attractive research direction in both artificial intelligence and clinical medicine. It can greatly expedite the automation of workflows and improve the quality and standardization of health care. Recently, there are many methods proposed \u2020 Corresponding author. 1 Our code and the best performing models are released at https://github.com/zhjohnchan/R2Gen.  in this area Johnson et al., 2019;Liu et al., 2019;Jing et al., 2019).\n\nPractically, a significant challenge of radiology report generation is that radiology reports are long narratives consisting of multiple sentences. As illustrated by Figure 1, a radiology report generally consists of a section of findings which describes medical observations, including both normal and abnormal features, as well as an impression or concluding remark summarizing the most prominent observations. Therefore, applying conventional image captioning approaches (Vinyals et al., 2015;Anderson et al., 2018) may be insufficient for radiology report generation, as such approaches are designed to briefly describe visual scenes with short sentences. The ability to provide accurate clinical descriptions for a radiograph is of the highest priority, which places a higher demand on the generation process. Nevertheless, despite the difficulties posed by these evident length and accuracy requirements, radiology reports do have their own distinctive characteristics. An important feature to note is their highly patternized nature, as illustrated by the sample report described above (Figure 1). On the basis of this patternization, many approaches have been proposed to address the challenges of radiology report generation. For example, Liu et al. (2019) found that a simple retrieval-based method could achieve a comparative performance for this task.  combined retrieval-based and generation-based methods with manually extracted templates. Although promising results may be obtained by the retrieval-based approaches, they are still limited in the preparation of large databases, or the explicit construction of template lists to determine the patterns embedded in various reports.\n\nIn this paper, we propose to generate radiology reports via memory-driven Transformer. In detail, a relational memory (RM) is proposed to record the information from previous generation processes and a novel memory-driven conditional layer normalization (MCLN) is designed to incorporate the relational memory into Transformer (Vaswani et al., 2017). As a result, similar patterns in different medical reports can be implicitly modeled and memorized during the generation process, which thereby can facilitate the decoding of Transformer and is capable of generating long reports with informative content. Experimental results on two benchmark datasets confirm the validity and effectiveness of our approach, where Transformer with RM and MCLN achieves the state-of-the-art performance on all datasets. To summarize, the contributions of this paper are four-fold: \u2022 We propose to generate radiology reports via a novel memory-driven Transformer model. \u2022 We propose a relational memory to record the previous generation process and the MCLN to incorporate relational memory into layers in the decoder of Transformer. \u2022 Extensive experiments are performed and the results show that our proposed models outperform the baselines and existing models. \u2022 We conduct analyses to investigate the effect of our model with respect to different memory sizes and show that our model is able to generate long reports with necessary medical terms and meaningful image-text attention mappings.\n\n\nThe Proposed Method\n\nGenerating radiology reports is essentially an image-to-text generation task, for which there exist several solutions (Vinyals et al., 2015;Xu et al., 2015;Anderson et al., 2018;Cornia et al., 2019). We follow the standard sequence-to-sequence paradigm for this task. In doing so, we treat the input from a radiology image as the source sequence X = {x 1 , x 2 , ..., x S }, x s \u2208 R d , where x s are patch features extracted from visual extractors and d the size of the feature vector. The corresponding report is the target sequence Y = {y 1 , y 2 , ..., y T }, y t \u2208 V, where y t are the generated tokens, T the length of generated tokens and V the vocabulary of all possible tokens. An overview of our proposed model is shown in Figure 2, where the details are illustrated in following subsections.\n\n\nThe Model Structure\n\nOur model can be partitioned into three major components, i.e., the visual extractor, the encoder and the decoder, where the proposed memory and the integration of the memory into Transformer are mainly performed in the decoder. The overall description of the three components and the training objective of the task is detailed below.\n\nVisual Extractor Given a radiology image Img, its visual features X are extracted by pre-trained convolutional neural networks (CNN), e.g., VGG (Simonyan and Zisserman, 2015) or ResNet (He et al., 2016), and the encoded results are used as the source sequence for all subsequent modules. The process is formulated as:\n{x 1 , x 2 , ..., x S } = f v (Img)(1)\nwhere f v (\u00b7) represents the visual extractor.\n\nEncoder In our model, we use the standard encoder from Transformer, where the outputs are the hidden states h i encoded from the input features x i extracted from the visual extractor:\n{h 1 , h 2 , ..., h S } = f e (x 1 , x 2 , ..., x S ) (2)\nwhere f e (\u00b7) refers to the encoder.\n\nDecoder The backbone decoder in our model is the one from Transformer, where we introduce an extra memory module to it by improving the original layer normalization with MCLN for each decoding layer as shown in Figure 2. Therefore the decoding process can be formalized as\ny t = f d (h 1 , ..., h S , MCLN(RM(y 1 , ..., y t\u22121 )))\n(3) where f d (\u00b7) refers to the decoder and the details of the memory (RM) and MCLN are presented in following subsections.\n\nObjective Given the aforementioned structure, the entire generation process can be formalized as a recursive application of the chain rule p(Y |Img) =  P (Y |Img) through the negative conditional loglikelihood of Y given the Img:\n\n\nConcat\n\n\nExpand\nV K Q V K Q Q K V X m t y t-1 Z Img r \u03bc \u03b3 \" t \u03b2 # t M $ t\u03b8 * = arg max \u03b8 T t=1\nlog p(y t |y 1 , ..., y t\u22121 , Img; \u03b8)\n\n(5) where \u03b8 is the parameters of the model.\n\n\nRelational Memory\n\nFor any relevant Img, they may share similar patterns in their reports and they can be used as good references for each other to help the generation process. As shown in Figure 1, patterns such as \"The lungs are clear bilaterally\" and \"no evidence of focal consolidation, or pleural effusion\" always appear in the reports of similar images and are shown simultaneously. To exploit such characteristics, we propose to use an extra component, i.e., relational memory, to enhance Transformer to learn from the patterns and facilitate computing the interactions among patterns and the generation process.\n\nIn doing so, the relational memory uses a matrix to transfer its states over generation steps, where the states record important pattern information with each row (namely, memory slot) representing some pattern information. 2 During the generation, the matrix is updated step-by-step with incorporating the output from previous steps. Then, at time step t, the matrix from the previous step, M t\u22121 , is functionalized as the query and its concatenations with the previous output serve as the key and value to feed the multi-head attention module. Given H heads used in Transformer, there are H sets of queries, keys and values via three linear transformations, respectively. For each head, we obtain the query, key and value in the relational memory Figure 3: The illustration of the gate mechanism.\nthrough Q = M t\u22121 \u00b7W q , K = [M t\u22121 ; y t\u22121 ]\u00b7W k and V = [M t\u22121 ; y t\u22121 ] \u00b7 W v , respectively, where y t\u22121 is the embedding of the last output (at step t \u2212 1); [M t\u22121 ; y t\u22121 ] is the row-wise concatena- tion of M t\u22121 and y t\u22121 . W q , W k and W v are Forget Gate Input Gate y t-1 \u03c3 \u03c3 M ! t M t-1 Duplicate Y t-1 M t G t f G t i tanh tanh\nthe trainable weights of linear transformation of the query, key and value, respectively. Multi-head attention is used to model Q, K and V so as to depict relations of different patterns. As a result,\nZ = softmax(QK / d k ) \u00b7 V (6)\nwhere d k is the dimension of K, and Z the output of the multi-head attention module. Consider that the relational memory is performed in a recurrent manner along with the decoding process, it potentially suffers from gradient vanishing and exploding. We therefore introduce residual connections and a gate mechanism. The former is formulated as\nM t = f mlp (Z + M t\u22121 ) + Z + M t\u22121 (7)\nwhere f mlp (\u00b7) refers to the multi-layer perceptron (MLP). The detailed structure of the gate mechanism in the relational memory is shown in Figure 3, where the forget and input gates are applied to balance the inputs from M t\u22121 and y t\u22121 , respectively. To ensure that y t\u22121 can be used for computation with M t\u22121 , it is extended to a matrix Y t\u22121 by duplicating it to multiple rows. Therefore, the forget and input gate are formalized as\nG f t = Y t\u22121 W f + tanh(M t\u22121 ) \u00b7 U f (8) G i t = Y t\u22121 W i + tanh(M t\u22121 ) \u00b7 U i(9)\nwhere W f and W i are trainable weights for Y t\u22121 in each gate; similarly, U f and U i are the trainable weights for M t\u22121 in each gate. The final output of the gate mechanism is formalized as\nM t = \u03c3(G f t ) M t\u22121 + \u03c3(G i t ) tanh(M t )(10)\nwhere refers to the Hadamard product and \u03c3 the sigmoid function and M t is the output of the entire relational memory module at step t.\n\n\nMemory-driven Conditional Layer Normalization\n\nAlthough memory shows its effectiveness in many NLP tasks (Sukhbaatar et   2019), it is by default applied to encoding with rather isolated designs. However, given that text generation is a dynamic process and largely affected by the output at each decoding step, memory is expected to be closely integrated to the decoder. Therefore, we propose a novel MCLN and use it to incorporate the relational memory to enhance the decoding of Transformer. Recall that in the conventional Transformer, to improve generalization, \u03b3 and \u03b2 are two crucial parameters for scaling and shifting the learned representations, 3 respectively. Thus we propose to incorporate the relational memory via MCLN by feeding its output M t to \u03b3 and \u03b2. Consequently, this design takes the benefit from the memory while preventing it from influencing too many parameters of Transformer so that some core information for generation is not affected.\n\nAs shown in Figure 2, in each Transformer decoding layer, we use three MCLNs, where the output of the first MCLN is functionalized as the query to be fed into the following multi-head attention module together with the hidden states from the encoder as the key and value. To feed each MCLN, at step t, the output of the relational memory M t is expanded into a vector m t by simply concatenating all rows from M t . Then, an MLP is used to predict a change \u2206\u03b3 t on \u03b3 t from m t , and update it via\n\u2206\u03b3 t = f mlp (m t ) (11) \u03b3 t = \u03b3 + \u2206\u03b3 t(12)\nSimilarly, \u2206\u03b2 t and\u03b2 t are performed by\n\u2206\u03b2 t = f mlp (m t )(13)\u03b2 t = \u03b2 + \u2206\u03b2 t(14)\nAfterwards, the predicted\u03b2 t and\u03b3 t are applied to the mean and variance results of the multi-head  Table 2: The performance of all baselines and our full model on the test sets of IU X-RAY and MIMIC-CXR datasets with respect to NLG and CE metrics. BL-n denotes BLEU score using up to n-grams; MTR and RG-L denote METEOR and ROUGE-L, respectively. The average improvement over all NLG metrics compared to BASE is also presented in the \"AVG. \u2206\" column. The performance of all models is averaged from five runs. self-attention from the previous generated outputs:\nf mcln (r) =\u03b3 t r \u2212 \u00b5 \u03c5 +\u03b2 t(15)\nwhere r refers to the output from the previous module; \u00b5 and \u03c5 are the mean and standard deviation of r, respectively. The result f mcln (r) from MCLN is then fed to the next module (for the 1st and 2nd MCLN) or used as the final output for generation (for the 3rd MCLN).\n\n\nExperiment Settings\n\n\nDatasets\n\nWe conduct our experiments on two datasets, which are described as follows:\n\n\u2022 IU X-RAY (   to exclude the samples without reports. Then we apply their conventional splits. Specifically, IU X-RAY is partitioned into train/validation/test set by 7:1:2 of the entire dataset, and MIMIC-CXR's official split is adopted. The statistics of the datasets are shown in Table 1, with the numbers of images, reports, patients and the average length of reports.\n\n\nBaseline and Evaluation Metrics\n\nTo compare with our proposed model, the following ones are used as the main baselines: The performance of the aforementioned models is evaluated by conventional natural language generation (NLG) metrics and clinical efficacy (CE) metrics 6 . The NLG metrics 7 include BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and ROUGE-L (Lin, 2004). For clinical efficacy metrics, we use the CheXpert (Irvin et al., 2019) 8 to label the generated reports and compare the results with ground truths in 14 different categories related to thoracic diseases and support devices. Precision, recall and F1 are used to evaluate model performance for these metrics.  \n\n\nImplementation Details\n\nWe adopt the ResNet101 (He et al., 2016) pretrained on Imagenet (Deng et al., 2009) as the visual extractor to extract patch features with the dimension of each feature set to 2,048. Note that for IU X-RAY, we use two images of a patient as input to ensure consistency with the experiment settings of previous work. The Transformer in our proposed model and all baselines are randomly initialized. For relational memory, its dimension and the number of heads in multi-head attention are set to 512 and 8, respectively, and the number of memory slots is set to 3 by default. For MCLN, we use two MLPs to obtain \u2206\u03b3 and \u2206\u03b2 where they do not share parameters. The model is trained under cross entropy loss with ADAM optimizer (Kingma and Ba, 2015). We set the learning rate to 5e-5 and 1e-4 for the visual extractor and other parameters, respectively. We decay such rate by a factor of 0.8 per epoch for each dataset and set the beam size to 3 to balance the generation effectiveness and efficiency. Note that the aforementioned hyper-parameters are obtained by evaluating the models on the validation sets of the two datasets.\n\n\nResults and Analyses\n\n\nEffect of Relational Memory\n\nTo illustrate the effectiveness of our proposed method, we experiment with the aforementioned baselines on the two benchmark datasets. The results are reported in Table 2, with BASE+RM+ MCLN representing our full model (same below).\n\nThere are several observations. First, on NLG metrics, both BASE+RM and BASE+RM+MCLN outperform the vanilla Transformer (BASE) on both datasets, which confirms the validity of incorporating memory into the decoding process in Transformer because that highly-patternized text in radiology reports are reasonably modeled to some extent. Second, our full model achieves the best performance over all baselines on different metrics, and it particularly outperforms BASE+RM with significant improvement, which clearly indicates the usefulness of MCLN in incorporating memory rather than other ways of integration. Third, on NLG metrics, when comparing between the datasets, the performance gains from two memory-driven models (i.e., BASE+RM and BASE+RM+MCLN) over BASE on IU X-RAY are larger than that of MIMIC-CXR. The reason behind might be that the IU X-RAY is relatively small and patterns among different reports in this dataset are more consistent so that our model helps more with the proposed memory. Fourthly, on the CE metrics on MIMIC-CXR, our full model shows the same trend as that for NLG metrics, where it outperforms all its baselines in terms of precision, recall and F1. This observation is important because higher NLG scores do not always result in higher clinical scores (e.g., the precision of BASE+RM on CE is lower than that of BASE), so  that the performance from CE further confirms the effectiveness of our method, whereas compared to BASE+RM, MCLN is able to leverage memory in a rather fine-grained way and thus better produce reasonable descriptions for clinical abnormalities.\n\n\nComparison with Previous Studies\n\nWe compare our full model (denoted as OURS) with existing models on the same datasets, with all results reported in Table 3 on both NLG and CE metrics. There are several observations drawn from different aspects. First, Transformer confirms its superiority to sequence-to-sequence structures in this task, which is illustrated by the comparison between our models (all baselines and our full model) and ST. Our full model also outperforms conventional image captioning models, e.g., ATT2IN, ADAATT and TOPDOWN, which are designed to generate a short piece of text for an image. This observation confirms that designing a specific model for long report generation is necessary for this task. Second, memory shows its effectiveness in this task when compared with those complicated models, e.g., HRGR uses manually extracted templates. Particularly, although on the two datasets, reinforcement learning (CMAS-RL) is proved to be the best solution with a careful design of adaptive rewards, our model achieves the same goal with a simpler method. Third, It is noticed that there are studies, e.g., HRGR, requires to utilize extra information for this task and our full model outperforms them without such requirements. This observation indicates that an appropriate end-to-end design (such as RM and MCLN) of using memory in Transformer can alleviate the need for extra resources to enhance this task.\n\n\nAnalysis\n\nWe analyze several aspects of our model regarding its hyper-parameters and generation results. Memory Size To show the impacts of the memory size, we train RM with different numbers of memory slots, i.e., |S| \u2208 {1, 2, 3, 4} and the results on MIMIC-CXR are shown in Table 4. In general, since memory size controls how much information is preserved in the past generation steps, it is confirmed in the observation that enlarging memory size by the number of slots results in better overall performance, with |S| = 3 achieving the best results. Still, we notice that the overall performance drops when |S| = 4, which indicates that too large memory may introduce redundant and invalid information so as to negatively affect the generation process. Although enlarging memory size results in increasing parameter numbers, it is demonstrated that there are not too many parameters (comparing to the total number of parameters) introduced whenever adding one slot in the memory. This observation suggests that the proposed model is effective and efficient in learning with memory for the radiology report generation task.\n\nReport Length In addition to NLG and CE metrics, another important criterion to evaluate generation models is the length of generated reports comparing to the ground-truth. In doing so, we categorize all reports generated on the MIMIC-CXR test set into 10 groups (within [0, 100] with interval of 10) according to their round-down lengths and draw curves for their numbers in each category for BASE, BASE+RM and BASE+RM+MCLN, as well as the ground-truth. The results are presented in Figure 4. Overall, more reports generated from BASE+RM and BASE+RM+MCLN are longer than that from BASE and their length distributions are closer to the ground-truth reports, which thus leads to better evaluation results on NLG metrics. The reason behind might be that the memory provides  Figure 5: Illustrations of reports from ground-truth, BASE and BASE+RM+MCLN models for two X-ray chest images. To better distinguish the content in the reports, different colors highlight different medical terms. more detailed information for the generation process so that the decoder tends to produce more diversified outputs than the original Transformer. Particularly, when comparing BASE+RM+MCLN and BASE+RM, the length distribution of the former generated reports is closer to the ground-truth, which can be explained by that, instead of applying memory to the final output, leveraging memory at each layer in Transformer is more helpful and thus controls the decoding process in a fine-grained way.\n\n\nBASE BASE+RM+MCLN\n\nThe above observations show that both memory and the way of using it are two important factors to enhance radiology report generation.\n\nCase Study To further investigate the effectiveness of our model, we perform qualitative analysis on some cases with their ground-truth and generated reports from different models. Figure  5 shows two examples of front and lateral chest Xray images from MIMIC-CXR and such reports, where different colors on the texts indicate different medical terms. It is observed in these cases that BASE+RM+MCLN is able to generate descriptions aligned with that written by radiologists with similar content flow. For example, in both cases, patterns in the generated reports follow the structure that starting from reporting abnormal findings (e.g., \"cardiac silhouette\" and \"lung volumes\"), and then concluding with potential diseases (e.g., \"pleural effusion\" and \"atelectasis\"). In addition, for the necessary medical terms in the ground-truth reports, BASE+RM+MCLN covers almost all of them in its generated reports while vanilla Transformer did much worse, e.g., the key terms \"enlarged cardiac silhouette\", \"atelectasis\" and \"small pleural effusion\" in the two examples are not generated.\n\nTo further investigate different models qualitatively, we randomly select a chest X-ray on the MIMIC-CXR test set and visualize the image-text attention mappings from BASE and BASE+RM+MCLN. Figure 6 shows the intermediate image-text correspondences for several words from the multi-head attentions in the first layer of the decoders. It is observed that BASE+RM+MCLN is better at aligning the locations with the indicated disease or parts. This observation suggests that our model not only enhances the power of radiology report generation, but also improves the interaction between the images and the generated texts. Error Analysis To analyze the errors from our model, especially in targeting the low CE scores, it is found that the class imbalance is severe on the datasets and affects the model training and inference, where majority voting is observed in the generation process. For example, on MIMIC-CXR, consolidation only accounts for 3.9% in the training set so that the trained model only recognizes that 2.9% results in this case compared with the ground truth 6.3%. Thus how to address the data bias problem is a possible future work to improve the accuracy of the generated radiology reports.\n\n\nRelated Work\n\nThe most popular related task to ours is image captioning (Vinyals et al., 2015;Xu et al., 2015;Anderson et al., 2018;, which aims to describe images with sentences. Different from them, radiology report generation requires much longer generated outputs, and possesses other features such as patterns, so that this task has its own characteristics requiring particular solutions. For example,  proposed a co-attention Original Image \"right\" \"heart\" \"pleural\" \"lungs\"\n\n1.0 0.0 Figure 6: Visualizations of image-text attention mappings between a specific chest X-ray and generated reports from BASE and BASE+RM+MCLN, respectively. Colors from blue to red represent the weights from low to high. mechanism and leveraged a hierarchical LSTM to generate reports. Li et al. ( , 2019 proposed to use a manually extracted template database to help generation with bunches of special techniques to utilize templates. Liu et al. (2019) proposed an approach with reinforcement learning to maintain the clinical accuracy of generated reports. Compared to these studies, our model offers an alternative solution to this task with an effective and efficient enhancement of Transformer via memory.\n\nExtra knowledge (e.g., pre-trained embeddings (Song et al., 2017;Song and Shi, 2018; and pretrained models (Devlin et al., 2019;Diao et al., 2019)) can provide useful information and thus enhance model performance for many NLP tasks (Tian et al., 2020a,b,c). Specifically, memory and memory-augmented neural networks (Zeng et al., 2018;Santoro et al., 2018;Diao et al., 2020;Tian et al., 2020d) are another line of related research, which can be traced back to , which proposed memory networks to leverage extra information for question answering; then Sukhbaatar et al. (2015) improved it with an end-to-end design to ensure the model being trained with less supervision. Particularly for Transformer, there are also memory-based methods proposed. For example, Lample et al. (2019) proposed to solve the under-fitting problem of Transformer by introducing a product-key layer that is similar to a memory module. Banino et al. (2020) proposed MEMO, an adaptive memory to reason over long-distance texts. Compared to these studies, the approach proposed in this paper focuses on leveraging memory for decoding rather than encoding, and presents a relational memory to learn from previous generation processes as well as patterns for long text generation. To the best of our knowledge, this is the first study incorporating memory for decoding with Transformer and applied for a particular task, which may provide a reference for studies in the line of this research.\n\n\nConclusion\n\nIn this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is used to record the information from previous generation processes and a novel layer normalization mechanism is designed to incorporate the memory into Transformer. Experimental results on two benchmark datasets illustrate the effectiveness of the memory by either concatenating it with the output or integrating it with different layers of the decoder by MCLN, which obtains the state-of-the-art performance. Further analyses investigate how memory size affects model performance and show that our model is able to generate long reports with necessary medical terms and meaningful image-text attention mappings.\n\nFigure 1 :\n1An example chest X-ray image and its report including findings and impression.\n\n\ny t |y 1 , ..., y t\u22121 , Img) (4) where Y = {y 1 , y 2 , ..., y T } is the target text sequence. The model is then trained to maximize\n\nFigure 2 :\n2The overall architecture of our proposed model, where the visual extractor, encoder and decoder are shown in gray dash boxes and the details of the visual extractor and encoder are omitted. The relational memory and memory conditional layer-normalization are illustrated in grey solid boxes with blue dash lines.\n\n4\nhttps://openi.nlm.nih.gov/ 5 https://physionet.org/content/ mimic-cxr/2.0.0/ \u2022 BASE: this is the vanilla Transformer, with three layers, 8 heads and 512 hidden units without other extensions and modifications. \u2022 BASE+RM: this is a simple alternative of our proposed model where the relational memory is directly concatenated to the output of the Transformer ahead of the softmax at each time step. This baseline aims to demonstrate the effect of using memory as an extra component instead of integration within the Transformer. In addition, we also compare our model with those in previous studies, including conventional image captioning models, e.g., ST (Vinyals et al., 2015), ATT2IN (Rennie et al., 2017), ADAATT (Lu et al., 2017), TOPDOWN (Anderson et al., 2018), and the ones proposed for the medical domain, e.g., COATT (Jing et al., 2018), HRGR (Li et al., 2018) and CMAS-RL (Jing et al., 2019).\n\nFigure 4 :\n4The length distributions of the generated reports on the MIMIC-CXR test set from BASE, BASE+ RM and BASE+RM+MCLN, as well as the ground-truth.\n\n\nLample et al., DATASET \nIU X-RAY \nMIMIC-CXR \n\nTRAIN VAL TEST TRAIN VAL TEST \n\nIMAGE # \n5,226 748 1,496 368,960 2,991 5,159 \nREPORT # \n2,770 395 790 222,758 1,808 3,269 \nPATIENT # 2,770 395 790 64,586 500 \n293 \nAVG. LEN. 37.56 36.78 33.62 \n53.00 53.05 66.40 \n\n\n\nTable 1 :\n1The statistics of the two benchmark datasets w.r.t. their training, validation and test sets, including the numbers of images, reports and patients, and the average word-based length (AVG. LEN.) of reports.\n\nTable 3 :\n3Comparisons of our full model with previous studies on the test sets of IU X-RAY and MIMIC-CXR with respect to NLG and CE metrics. refers to that the result is directed cited from the original paper and represents our replicated results by their codes.\n\nTable 4 :\n4NLG scores of our full model on the MIMIC-CXR test set when different memory slots are used. PARA. denotes the number of parameters.\n\n\nBASE: As compared to the previous radiograph there is no relevant change. Moderate cardiomegaly with mild fluid overload but no overt pulmonary edema. No pleural effusions. No pneumonia. Unchanged right internal jugular vein catheter.Ground-truth: There are no old films available for comparison. The heart is moderately enlarged. There is a right ij cordis with tip in the upper svc. There is mild pulmonary vascular re-distribution but no definite infiltrates or effusion.BASE+RM+MCLN:A right internal jugular central venous catheter terminates in the mid svc. The heart is moderately enlarged. The mediastinal and hilar contours are within normal limits. There is no pneumothorax or large pleural effusion. The lungs appear clear.Original Image \n\"cardiomegaly\" \n\"pulmonary\" \n\"pleural\" \n\"right\" \n\n\nNote that the rows (memory slots) and patterns do not follow one-to-one mapping, where the entire matrix serves as a whole unit to deliver the pattern information.\nIn detail, \u03b3 is used to amplify the values in the learned representation and \u03b2 provides a bias adjustment to them.\nNote that CE metrics only apply to MIMIC-CXR because the labeling schema of CheXpert is designed for MIMIC-CXR, which is different from that of IU X-RAY. 7 https://github.com/tylin/coco-caption 8 https://github.com/MIT-LCP/mimic-cxr/ tree/master/txt/chexpert\n\nBottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answer- ing. In Proceedings of the IEEE conference on com- puter vision and pattern recognition, pages 6077- 6086.\n\nAndrea Banino, Adri\u00e0 Puigdom\u00e8nech Badia, Raphael K\u00f6ster, J Martin, Vinicius Chadwick, Demis Zambaldi, Caswell Hassabis, Matthew Barry, Botvinick, arXiv:2001.10913Dharshan Kumaran, and Charles Blundell. 2020. MEMO: A Deep Network for Flexible Combination of Episodic Memories. arXiv preprintAndrea Banino, Adri\u00e0 Puigdom\u00e8nech Badia, Raphael K\u00f6ster, Martin J Chadwick, Vinicius Zambaldi, Demis Hassabis, Caswell Barry, Matthew Botvinick, Dharshan Kumaran, and Charles Blundell. 2020. MEMO: A Deep Network for Flexible Combi- nation of Episodic Memories. arXiv preprint arXiv:2001.10913.\n\nMarcella Cornia, Matteo Stefanini, Lorenzo Baraldi, Rita Cucchiara, arXiv:1912.08226M 2 : Meshed-Memory Transformer for Image Captioning. arXiv preprintMarcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2019. M 2 : Meshed-Memory Transformer for Image Captioning. arXiv preprint arXiv:1912.08226.\n\nPreparing a collection of radiology examinations for distribution and retrieval. Dina Demner-Fushman, D Marc, Kohli, Sonya E Marc B Rosenman, Laritza Shooshan, Sameer Rodriguez, Antani, Clement J George R Thoma, Mcdonald, Journal of the American Medical Informatics Association. 232Dina Demner-Fushman, Marc D Kohli, Marc B Rosen- man, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and Clement J McDon- ald. 2016. Preparing a collection of radiology ex- aminations for distribution and retrieval. Journal of the American Medical Informatics Association, 23(2):304-310.\n\nImageNet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hier- archical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255.\n\nMeteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. Michael Denkowski, Alon Lavie, Proceedings of the sixth workshop on statistical machine translation. the sixth workshop on statistical machine translationMichael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Pro- ceedings of the sixth workshop on statistical ma- chine translation, pages 85-91.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Un- derstanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186.\n\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, Yonggang Wang, arXiv:1911.00720Pre-training Chinese Text Encoder Enhanced by N-gram Representations. arXiv preprintShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and Yonggang Wang. 2019. ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations. arXiv preprint arXiv:1911.00720.\n\nKeyphrase Generation with Cross-Document Attention. Shizhe Diao, Yan Song, Tong Zhang, arXiv:2004.09800arXiv preprintShizhe Diao, Yan Song, and Tong Zhang. 2020. Keyphrase Generation with Cross-Document Atten- tion. arXiv preprint arXiv:2004.09800.\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778.\n\nCheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison. Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Mark- lund, Behzad Haghgoo, Robyn Ball, Katie Shpan- skaya, et al. 2019. CheXpert: A Large Chest Ra- diograph Dataset with Uncertainty Labels and Ex- pert Comparison. In Proceedings of the AAAI Con- ference on Artificial Intelligence, volume 33, pages 590-597.\n\nShow, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports. Baoyu Jing, Zeya Wang, Eric Xing, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsBaoyu Jing, Zeya Wang, and Eric Xing. 2019. Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6570-6580.\n\nOn the Automatic Generation of Medical Imaging Reports. Baoyu Jing, Pengtao Xie, Eric Xing, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Baoyu Jing, Pengtao Xie, and Eric Xing. 2018. On the Automatic Generation of Medical Imaging Reports. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2577-2586.\n\nE W Alistair, Johnson, J Tom, Seth J Pollard, Nathaniel R Berkowitz, Greenbaum, Chihying Matthew P Lungren, Deng, G Roger, Steven Mark, Horng, arXiv:1901.07042MIMIC-CXR: A large publicly available database of labeled chest radiographs. arXiv preprintAlistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih- ying Deng, Roger G Mark, and Steven Horng. 2019. MIMIC-CXR: A large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042.\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, abs/1412.6980CoRRDiederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. CoRR, abs/1412.6980.\n\nLarge Memory Layers with Product Keys. Guillaume Lample, Alexandre Sablayrolles, Marc&apos;aurelio Ranzato, Ludovic Denoyer, Herv\u00e9 J\u00e9gou, Advances in Neural Information Processing Systems. Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. 2019. Large Memory Layers with Product Keys. In Advances in Neural Information Processing Systems, pages 8546-8557.\n\nKnowledge-Driven Encode, Retrieve, Paraphrase for Medical Image Report Generation. Y Christy, Xiaodan Li, Zhiting Liang, Eric P Hu, Xing, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Christy Y Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing. 2019. Knowledge-Driven Encode, Retrieve, Paraphrase for Medical Image Report Generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6666-6673.\n\nHybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation. Yuan Li, Xiaodan Liang, Zhiting Hu, Eric P Xing, Advances in neural information processing systems. Yuan Li, Xiaodan Liang, Zhiting Hu, and Eric P Xing. 2018. Hybrid Retrieval-Generation Rein- forced Agent for Medical Image Report Generation. In Advances in neural information processing sys- tems, pages 1530-1540.\n\nROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Chin-Yew Lin. 2004. ROUGE: A Package for Auto- matic Evaluation of Summaries. In Text Summariza- tion Branches Out, pages 74-81.\n\nClinically Accurate Chest X-Ray Report Generation. Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew Mc-Dermott, Willie Boag, Wei-Hung Weng, Peter Szolovits, Marzyeh Ghassemi, Machine Learning for Healthcare Conference. Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew Mc- Dermott, Willie Boag, Wei-Hung Weng, Peter Szolovits, and Marzyeh Ghassemi. 2019. Clinically Accurate Chest X-Ray Report Generation. In Ma- chine Learning for Healthcare Conference, pages 249-269.\n\nKnowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning. Jiasen Lu, Caiming Xiong, Devi Parikh, Richard Socher, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. 2017. Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Caption- ing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 375- 383.\n\nBLEU: a Method for Automatic Evaluation of Machine Translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th annual meeting on association for com- putational linguistics, pages 311-318.\n\nSelf-critical Sequence Training for Image Captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSteven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical Sequence Training for Image Captioning. In Pro- ceedings of the IEEE Conference on Computer Vi- sion and Pattern Recognition, pages 7008-7024.\n\nRelational recurrent neural networks. Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap, Advances in neural information processing systems. Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timo- thy Lillicrap. 2018. Relational recurrent neural net- works. In Advances in neural information process- ing systems, pages 7299-7310.\n\nVery Deep Convolutional Networks for Large-Scale Image Recognition. Karen Simonyan, Andrew Zisserman, abs/1409.1556CoRRKaren Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks for Large-Scale Im- age Recognition. CoRR, abs/1409.1556.\n\nLearning Word Representations with Regularization from Prior Knowledge. Yan Song, Chia-Jung Lee, Fei Xia, Proceedings of the 21st Conference on Computational Natural Language Learning. the 21st Conference on Computational Natural Language LearningYan Song, Chia-Jung Lee, and Fei Xia. 2017. Learn- ing Word Representations with Regularization from Prior Knowledge. In Proceedings of the 21st Confer- ence on Computational Natural Language Learning (CoNLL 2017), pages 143-152.\n\nComplementary Learning of Word Embeddings. Yan Song, Shuming Shi, Proceedings of the 27th International Joint Conference on Artificial Intelligence. the 27th International Joint Conference on Artificial IntelligenceYan Song and Shuming Shi. 2018. Complementary Learning of Word Embeddings. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4368-4374.\n\nEnd-To-End Memory Networks. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, Advances in neural information processing systems. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-To-End Memory Networks. In Ad- vances in neural information processing systems, pages 2440-2448.\n\nJoint Chinese Word Segmentation and Partof-speech Tagging via Two-way Attentions of Autoanalyzed Knowledge. Yuanhe Tian, Yan Song, Xiang Ao, Fei Xia, Xiaojun Quan, Tong Zhang, Yonggang Wang, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsYuanhe Tian, Yan Song, Xiang Ao, Fei Xia, Xi- aojun Quan, Tong Zhang, and Yonggang Wang. 2020a. Joint Chinese Word Segmentation and Part- of-speech Tagging via Two-way Attentions of Auto- analyzed Knowledge. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 8286-8296.\n\nSupertagging Combinatory Categorial Grammar with Attentive Graph Convolutional Networks. Yuanhe Tian, Yan Song, Fei Xia, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingYuanhe Tian, Yan Song, and Fei Xia. 2020b. Supertag- ging Combinatory Categorial Grammar with Atten- tive Graph Convolutional Networks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing.\n\nImproving Constituency Parsing with Span Attention. Yuanhe Tian, Yan Song, Fei Xia, Tong Zhang, Findings of the 2020 Conference on Empirical Methods in Natural Language Processing. Yuanhe Tian, Yan Song, Fei Xia, and Tong Zhang. 2020c. Improving Constituency Parsing with Span Attention. In Findings of the 2020 Conference on Empirical Methods in Natural Language Process- ing.\n\nImproving Chinese Word Segmentation with Wordhood Memory Networks. Yuanhe Tian, Yan Song, Fei Xia, Tong Zhang, Yonggang Wang, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsYuanhe Tian, Yan Song, Fei Xia, Tong Zhang, and Yonggang Wang. 2020d. Improving Chinese Word Segmentation with Wordhood Memory Networks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8274-8285.\n\nAttention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In Advances in neural information pro- cessing systems, pages 5998-6008.\n\nShow and Tell: A Neural Image Caption Generator. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and Tell: A Neural Im- age Caption Generator. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 3156-3164.\n\nHierarchical Attention Network for Image Captioning. Weixuan Wang, Zhihong Chen, Haifeng Hu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Weixuan Wang, Zhihong Chen, and Haifeng Hu. 2019. Hierarchical Attention Network for Image Caption- ing. In Proceedings of the AAAI Conference on Arti- ficial Intelligence, volume 33, pages 8957-8964.\n\n. Jason Weston, Sumit Chopra, Antoine Bordes, abs/1410.3916Jason Weston, Sumit Chopra, and Antoine Bordes. 2015. Memory Networks. CoRR, abs/1410.3916.\n\nShow, Attend and Tell: Neural Image Caption Generation with Visual Attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio, International conference on machine learning. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Atten- tion. In International conference on machine learn- ing, pages 2048-2057.\n\nTopic Memory Networks for Short Text Classification. Jichuan Zeng, Jing Li, Yan Song, Cuiyun Gao, Irwin Michael R Lyu, King, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingJichuan Zeng, Jing Li, Yan Song, Cuiyun Gao, Michael R Lyu, and Irwin King. 2018. Topic Mem- ory Networks for Short Text Classification. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing, pages 3120- 3131.\n\nMultiplex Word Embeddings for Selectional Preference Acquisition. Hongming Zhang, Jiaxin Bai, Yan Song, Kun Xu, Changlong Yu, Yangqiu Song, Wilfred Ng, Dong Yu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hongming Zhang, Jiaxin Bai, Yan Song, Kun Xu, Changlong Yu, Yangqiu Song, Wilfred Ng, and Dong Yu. 2019. Multiplex Word Embeddings for Selectional Preference Acquisition. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 5250-5259.\n", "annotations": {"author": "[{\"end\":204,\"start\":62},{\"end\":299,\"start\":205},{\"end\":403,\"start\":300},{\"end\":555,\"start\":404},{\"end\":651,\"start\":556},{\"end\":753,\"start\":652}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":70},{\"end\":311,\"start\":304},{\"end\":419,\"start\":414},{\"end\":661,\"start\":658}]", "author_first_name": "[{\"end\":69,\"start\":62},{\"end\":207,\"start\":205},{\"end\":303,\"start\":300},{\"end\":413,\"start\":404},{\"end\":557,\"start\":556},{\"end\":559,\"start\":558},{\"end\":657,\"start\":652}]", "author_affiliation": "[{\"end\":203,\"start\":114},{\"end\":298,\"start\":209},{\"end\":402,\"start\":313},{\"end\":554,\"start\":465},{\"end\":650,\"start\":561},{\"end\":752,\"start\":663}]", "title": "[{\"end\":59,\"start\":1},{\"end\":812,\"start\":754}]", "venue": null, "abstract": "[{\"end\":2054,\"start\":814}]", "bib_ref": "[{\"end\":2498,\"start\":2497},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2627,\"start\":2606},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2644,\"start\":2627},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2662,\"start\":2644},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3161,\"start\":3139},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3183,\"start\":3161},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3930,\"start\":3913},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4711,\"start\":4689},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6003,\"start\":5981},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6019,\"start\":6003},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6041,\"start\":6019},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6061,\"start\":6041},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7199,\"start\":7169},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7227,\"start\":7210},{\"end\":11985,\"start\":11971},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15139,\"start\":15116},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15175,\"start\":15148},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15199,\"start\":15188},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15272,\"start\":15252},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15577,\"start\":15560},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15619,\"start\":15601},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25140,\"start\":25118},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25156,\"start\":25140},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25178,\"start\":25156},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25836,\"start\":25818},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25985,\"start\":25968},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26309,\"start\":26290},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26328,\"start\":26309},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26372,\"start\":26351},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26390,\"start\":26372},{\"end\":26501,\"start\":26477},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26580,\"start\":26561},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26601,\"start\":26580},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26619,\"start\":26601},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26638,\"start\":26619},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26821,\"start\":26797},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27026,\"start\":27006},{\"end\":27177,\"start\":27157},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30086,\"start\":30072}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28544,\"start\":28453},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28680,\"start\":28545},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29006,\"start\":28681},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29913,\"start\":29007},{\"attributes\":{\"id\":\"fig_7\"},\"end\":30069,\"start\":29914},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30331,\"start\":30070},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30550,\"start\":30332},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30815,\"start\":30551},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30960,\"start\":30816},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":31762,\"start\":30961}]", "paragraph": "[{\"end\":2663,\"start\":2070},{\"end\":4360,\"start\":2665},{\"end\":5839,\"start\":4362},{\"end\":6665,\"start\":5863},{\"end\":7023,\"start\":6689},{\"end\":7342,\"start\":7025},{\"end\":7428,\"start\":7382},{\"end\":7614,\"start\":7430},{\"end\":7709,\"start\":7673},{\"end\":7983,\"start\":7711},{\"end\":8164,\"start\":8041},{\"end\":8395,\"start\":8166},{\"end\":8530,\"start\":8493},{\"end\":8575,\"start\":8532},{\"end\":9197,\"start\":8597},{\"end\":9998,\"start\":9199},{\"end\":10540,\"start\":10340},{\"end\":10917,\"start\":10572},{\"end\":11400,\"start\":10959},{\"end\":11678,\"start\":11486},{\"end\":11863,\"start\":11728},{\"end\":12830,\"start\":11913},{\"end\":13329,\"start\":12832},{\"end\":13413,\"start\":13374},{\"end\":14017,\"start\":13456},{\"end\":14322,\"start\":14051},{\"end\":14432,\"start\":14357},{\"end\":14807,\"start\":14434},{\"end\":15510,\"start\":14843},{\"end\":16660,\"start\":15537},{\"end\":16947,\"start\":16715},{\"end\":18551,\"start\":16949},{\"end\":19986,\"start\":18588},{\"end\":21114,\"start\":19999},{\"end\":22594,\"start\":21116},{\"end\":22750,\"start\":22616},{\"end\":23835,\"start\":22752},{\"end\":25043,\"start\":23837},{\"end\":25526,\"start\":25060},{\"end\":26242,\"start\":25528},{\"end\":27709,\"start\":26244},{\"end\":28452,\"start\":27724}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7381,\"start\":7343},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7672,\"start\":7615},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8040,\"start\":7984},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8471,\"start\":8414},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8492,\"start\":8471},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10339,\"start\":9999},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10571,\"start\":10541},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10958,\"start\":10918},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11485,\"start\":11401},{\"attributes\":{\"id\":\"formula_9\"},\"end\":11727,\"start\":11679},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13373,\"start\":13330},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13437,\"start\":13414},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13455,\"start\":13437},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14050,\"start\":14018}]", "table_ref": "[{\"end\":13563,\"start\":13556},{\"end\":14446,\"start\":14445},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14725,\"start\":14718},{\"end\":16885,\"start\":16878},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":18711,\"start\":18704},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":20272,\"start\":20265}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2068,\"start\":2056},{\"attributes\":{\"n\":\"2\"},\"end\":5861,\"start\":5842},{\"attributes\":{\"n\":\"2.1\"},\"end\":6687,\"start\":6668},{\"end\":8404,\"start\":8398},{\"end\":8413,\"start\":8407},{\"attributes\":{\"n\":\"2.2\"},\"end\":8595,\"start\":8578},{\"attributes\":{\"n\":\"2.3\"},\"end\":11911,\"start\":11866},{\"attributes\":{\"n\":\"3\"},\"end\":14344,\"start\":14325},{\"attributes\":{\"n\":\"3.1\"},\"end\":14355,\"start\":14347},{\"attributes\":{\"n\":\"3.2\"},\"end\":14841,\"start\":14810},{\"attributes\":{\"n\":\"3.3\"},\"end\":15535,\"start\":15513},{\"attributes\":{\"n\":\"4\"},\"end\":16683,\"start\":16663},{\"attributes\":{\"n\":\"4.1\"},\"end\":16713,\"start\":16686},{\"attributes\":{\"n\":\"4.2\"},\"end\":18586,\"start\":18554},{\"attributes\":{\"n\":\"4.3\"},\"end\":19997,\"start\":19989},{\"end\":22614,\"start\":22597},{\"attributes\":{\"n\":\"5\"},\"end\":25058,\"start\":25046},{\"attributes\":{\"n\":\"6\"},\"end\":27722,\"start\":27712},{\"end\":28464,\"start\":28454},{\"end\":28692,\"start\":28682},{\"end\":29009,\"start\":29008},{\"end\":29925,\"start\":29915},{\"end\":30342,\"start\":30333},{\"end\":30561,\"start\":30552},{\"end\":30826,\"start\":30817}]", "table": "[{\"end\":30331,\"start\":30087},{\"end\":31762,\"start\":31696}]", "figure_caption": "[{\"end\":28544,\"start\":28466},{\"end\":28680,\"start\":28547},{\"end\":29006,\"start\":28694},{\"end\":29913,\"start\":29010},{\"end\":30069,\"start\":29927},{\"end\":30087,\"start\":30072},{\"end\":30550,\"start\":30344},{\"end\":30815,\"start\":30563},{\"end\":30960,\"start\":30828},{\"end\":31696,\"start\":30963}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2839,\"start\":2831},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3767,\"start\":3758},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":6604,\"start\":6596},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7930,\"start\":7922},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8775,\"start\":8767},{\"end\":9957,\"start\":9949},{\"end\":11109,\"start\":11101},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12852,\"start\":12844},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":21608,\"start\":21600},{\"end\":21897,\"start\":21889},{\"end\":22942,\"start\":22933},{\"end\":24035,\"start\":24027},{\"end\":25544,\"start\":25536}]", "bib_author_first_name": "[{\"end\":32392,\"start\":32387},{\"end\":32411,\"start\":32403},{\"end\":32421,\"start\":32416},{\"end\":32437,\"start\":32431},{\"end\":32449,\"start\":32445},{\"end\":32466,\"start\":32459},{\"end\":32477,\"start\":32474},{\"end\":32930,\"start\":32924},{\"end\":32944,\"start\":32939},{\"end\":32957,\"start\":32945},{\"end\":32972,\"start\":32965},{\"end\":32982,\"start\":32981},{\"end\":32999,\"start\":32991},{\"end\":33015,\"start\":33010},{\"end\":33033,\"start\":33026},{\"end\":33051,\"start\":33044},{\"end\":33517,\"start\":33509},{\"end\":33532,\"start\":33526},{\"end\":33551,\"start\":33544},{\"end\":33565,\"start\":33561},{\"end\":33912,\"start\":33908},{\"end\":33930,\"start\":33929},{\"end\":33949,\"start\":33944},{\"end\":33951,\"start\":33950},{\"end\":33976,\"start\":33969},{\"end\":33993,\"start\":33987},{\"end\":34022,\"start\":34013},{\"end\":34475,\"start\":34472},{\"end\":34485,\"start\":34482},{\"end\":34499,\"start\":34492},{\"end\":34514,\"start\":34508},{\"end\":34522,\"start\":34519},{\"end\":34529,\"start\":34527},{\"end\":34929,\"start\":34922},{\"end\":34945,\"start\":34941},{\"end\":35396,\"start\":35391},{\"end\":35413,\"start\":35405},{\"end\":35427,\"start\":35421},{\"end\":35441,\"start\":35433},{\"end\":36086,\"start\":36080},{\"end\":36099,\"start\":36093},{\"end\":36108,\"start\":36105},{\"end\":36119,\"start\":36115},{\"end\":36135,\"start\":36127},{\"end\":36481,\"start\":36475},{\"end\":36491,\"start\":36488},{\"end\":36502,\"start\":36498},{\"end\":36726,\"start\":36719},{\"end\":36738,\"start\":36731},{\"end\":36754,\"start\":36746},{\"end\":36764,\"start\":36760},{\"end\":37212,\"start\":37206},{\"end\":37226,\"start\":37220},{\"end\":37245,\"start\":37238},{\"end\":37255,\"start\":37250},{\"end\":37268,\"start\":37260},{\"end\":37288,\"start\":37283},{\"end\":37302,\"start\":37296},{\"end\":37319,\"start\":37313},{\"end\":37334,\"start\":37329},{\"end\":37346,\"start\":37341},{\"end\":37927,\"start\":37922},{\"end\":37938,\"start\":37934},{\"end\":37949,\"start\":37945},{\"end\":38425,\"start\":38420},{\"end\":38439,\"start\":38432},{\"end\":38449,\"start\":38445},{\"end\":38866,\"start\":38865},{\"end\":38868,\"start\":38867},{\"end\":38889,\"start\":38888},{\"end\":38899,\"start\":38895},{\"end\":38901,\"start\":38900},{\"end\":38920,\"start\":38911},{\"end\":38922,\"start\":38921},{\"end\":38953,\"start\":38945},{\"end\":38980,\"start\":38979},{\"end\":38994,\"start\":38988},{\"end\":39422,\"start\":39421},{\"end\":39438,\"start\":39433},{\"end\":39620,\"start\":39611},{\"end\":39638,\"start\":39629},{\"end\":39670,\"start\":39653},{\"end\":39687,\"start\":39680},{\"end\":39702,\"start\":39697},{\"end\":40060,\"start\":40059},{\"end\":40077,\"start\":40070},{\"end\":40089,\"start\":40082},{\"end\":40101,\"start\":40097},{\"end\":40103,\"start\":40102},{\"end\":40553,\"start\":40549},{\"end\":40565,\"start\":40558},{\"end\":40580,\"start\":40573},{\"end\":40589,\"start\":40585},{\"end\":40591,\"start\":40590},{\"end\":40930,\"start\":40922},{\"end\":41159,\"start\":41150},{\"end\":41179,\"start\":41165},{\"end\":41192,\"start\":41185},{\"end\":41211,\"start\":41205},{\"end\":41226,\"start\":41218},{\"end\":41238,\"start\":41233},{\"end\":41257,\"start\":41250},{\"end\":41648,\"start\":41642},{\"end\":41660,\"start\":41653},{\"end\":41672,\"start\":41668},{\"end\":41688,\"start\":41681},{\"end\":42160,\"start\":42153},{\"end\":42176,\"start\":42171},{\"end\":42189,\"start\":42185},{\"end\":42204,\"start\":42196},{\"end\":42656,\"start\":42655},{\"end\":42672,\"start\":42665},{\"end\":42688,\"start\":42681},{\"end\":42706,\"start\":42700},{\"end\":42723,\"start\":42715},{\"end\":43167,\"start\":43163},{\"end\":43181,\"start\":43177},{\"end\":43197,\"start\":43192},{\"end\":43210,\"start\":43206},{\"end\":43220,\"start\":43216},{\"end\":43243,\"start\":43234},{\"end\":43255,\"start\":43251},{\"end\":43271,\"start\":43266},{\"end\":43287,\"start\":43281},{\"end\":43304,\"start\":43297},{\"end\":43719,\"start\":43714},{\"end\":43736,\"start\":43730},{\"end\":43975,\"start\":43972},{\"end\":43991,\"start\":43982},{\"end\":44000,\"start\":43997},{\"end\":44424,\"start\":44421},{\"end\":44438,\"start\":44431},{\"end\":44809,\"start\":44800},{\"end\":44827,\"start\":44822},{\"end\":44839,\"start\":44836},{\"end\":45176,\"start\":45170},{\"end\":45186,\"start\":45183},{\"end\":45198,\"start\":45193},{\"end\":45206,\"start\":45203},{\"end\":45219,\"start\":45212},{\"end\":45230,\"start\":45226},{\"end\":45246,\"start\":45238},{\"end\":45829,\"start\":45823},{\"end\":45839,\"start\":45836},{\"end\":45849,\"start\":45846},{\"end\":46300,\"start\":46294},{\"end\":46310,\"start\":46307},{\"end\":46320,\"start\":46317},{\"end\":46330,\"start\":46326},{\"end\":46694,\"start\":46688},{\"end\":46704,\"start\":46701},{\"end\":46714,\"start\":46711},{\"end\":46724,\"start\":46720},{\"end\":46740,\"start\":46732},{\"end\":47188,\"start\":47182},{\"end\":47202,\"start\":47198},{\"end\":47216,\"start\":47212},{\"end\":47230,\"start\":47225},{\"end\":47247,\"start\":47242},{\"end\":47260,\"start\":47255},{\"end\":47262,\"start\":47261},{\"end\":47276,\"start\":47270},{\"end\":47290,\"start\":47285},{\"end\":47640,\"start\":47635},{\"end\":47659,\"start\":47650},{\"end\":47672,\"start\":47668},{\"end\":47688,\"start\":47681},{\"end\":48121,\"start\":48114},{\"end\":48135,\"start\":48128},{\"end\":48149,\"start\":48142},{\"end\":48474,\"start\":48469},{\"end\":48488,\"start\":48483},{\"end\":48504,\"start\":48497},{\"end\":48703,\"start\":48697},{\"end\":48713,\"start\":48708},{\"end\":48722,\"start\":48718},{\"end\":48739,\"start\":48730},{\"end\":48750,\"start\":48745},{\"end\":48768,\"start\":48762},{\"end\":48787,\"start\":48783},{\"end\":48801,\"start\":48795},{\"end\":49188,\"start\":49181},{\"end\":49199,\"start\":49195},{\"end\":49207,\"start\":49204},{\"end\":49220,\"start\":49214},{\"end\":49231,\"start\":49226},{\"end\":49737,\"start\":49729},{\"end\":49751,\"start\":49745},{\"end\":49760,\"start\":49757},{\"end\":49770,\"start\":49767},{\"end\":49784,\"start\":49775},{\"end\":49796,\"start\":49789},{\"end\":49810,\"start\":49803},{\"end\":49819,\"start\":49815}]", "bib_author_last_name": "[{\"end\":32401,\"start\":32393},{\"end\":32414,\"start\":32412},{\"end\":32429,\"start\":32422},{\"end\":32443,\"start\":32438},{\"end\":32457,\"start\":32450},{\"end\":32472,\"start\":32467},{\"end\":32483,\"start\":32478},{\"end\":32937,\"start\":32931},{\"end\":32963,\"start\":32958},{\"end\":32979,\"start\":32973},{\"end\":32989,\"start\":32983},{\"end\":33008,\"start\":33000},{\"end\":33024,\"start\":33016},{\"end\":33042,\"start\":33034},{\"end\":33057,\"start\":33052},{\"end\":33068,\"start\":33059},{\"end\":33524,\"start\":33518},{\"end\":33542,\"start\":33533},{\"end\":33559,\"start\":33552},{\"end\":33575,\"start\":33566},{\"end\":33927,\"start\":33913},{\"end\":33935,\"start\":33931},{\"end\":33942,\"start\":33937},{\"end\":33967,\"start\":33952},{\"end\":33985,\"start\":33977},{\"end\":34003,\"start\":33994},{\"end\":34011,\"start\":34005},{\"end\":34037,\"start\":34023},{\"end\":34047,\"start\":34039},{\"end\":34480,\"start\":34476},{\"end\":34490,\"start\":34486},{\"end\":34506,\"start\":34500},{\"end\":34517,\"start\":34515},{\"end\":34525,\"start\":34523},{\"end\":34537,\"start\":34530},{\"end\":34939,\"start\":34930},{\"end\":34951,\"start\":34946},{\"end\":35403,\"start\":35397},{\"end\":35419,\"start\":35414},{\"end\":35431,\"start\":35428},{\"end\":35451,\"start\":35442},{\"end\":36091,\"start\":36087},{\"end\":36103,\"start\":36100},{\"end\":36113,\"start\":36109},{\"end\":36125,\"start\":36120},{\"end\":36140,\"start\":36136},{\"end\":36486,\"start\":36482},{\"end\":36496,\"start\":36492},{\"end\":36508,\"start\":36503},{\"end\":36729,\"start\":36727},{\"end\":36744,\"start\":36739},{\"end\":36758,\"start\":36755},{\"end\":36768,\"start\":36765},{\"end\":37218,\"start\":37213},{\"end\":37236,\"start\":37227},{\"end\":37248,\"start\":37246},{\"end\":37258,\"start\":37256},{\"end\":37281,\"start\":37269},{\"end\":37294,\"start\":37289},{\"end\":37311,\"start\":37303},{\"end\":37327,\"start\":37320},{\"end\":37339,\"start\":37335},{\"end\":37357,\"start\":37347},{\"end\":37932,\"start\":37928},{\"end\":37943,\"start\":37939},{\"end\":37954,\"start\":37950},{\"end\":38430,\"start\":38426},{\"end\":38443,\"start\":38440},{\"end\":38454,\"start\":38450},{\"end\":38877,\"start\":38869},{\"end\":38886,\"start\":38879},{\"end\":38893,\"start\":38890},{\"end\":38909,\"start\":38902},{\"end\":38932,\"start\":38923},{\"end\":38943,\"start\":38934},{\"end\":38971,\"start\":38954},{\"end\":38977,\"start\":38973},{\"end\":38986,\"start\":38981},{\"end\":38999,\"start\":38995},{\"end\":39006,\"start\":39001},{\"end\":39431,\"start\":39423},{\"end\":39445,\"start\":39439},{\"end\":39449,\"start\":39447},{\"end\":39627,\"start\":39621},{\"end\":39651,\"start\":39639},{\"end\":39678,\"start\":39671},{\"end\":39695,\"start\":39688},{\"end\":39708,\"start\":39703},{\"end\":40068,\"start\":40061},{\"end\":40080,\"start\":40078},{\"end\":40095,\"start\":40090},{\"end\":40106,\"start\":40104},{\"end\":40112,\"start\":40108},{\"end\":40556,\"start\":40554},{\"end\":40571,\"start\":40566},{\"end\":40583,\"start\":40581},{\"end\":40596,\"start\":40592},{\"end\":40934,\"start\":40931},{\"end\":41163,\"start\":41160},{\"end\":41183,\"start\":41180},{\"end\":41203,\"start\":41193},{\"end\":41216,\"start\":41212},{\"end\":41231,\"start\":41227},{\"end\":41248,\"start\":41239},{\"end\":41266,\"start\":41258},{\"end\":41651,\"start\":41649},{\"end\":41666,\"start\":41661},{\"end\":41679,\"start\":41673},{\"end\":41695,\"start\":41689},{\"end\":42169,\"start\":42161},{\"end\":42183,\"start\":42177},{\"end\":42194,\"start\":42190},{\"end\":42208,\"start\":42205},{\"end\":42663,\"start\":42657},{\"end\":42679,\"start\":42673},{\"end\":42698,\"start\":42689},{\"end\":42713,\"start\":42707},{\"end\":42728,\"start\":42724},{\"end\":42734,\"start\":42730},{\"end\":43175,\"start\":43168},{\"end\":43190,\"start\":43182},{\"end\":43204,\"start\":43198},{\"end\":43214,\"start\":43211},{\"end\":43232,\"start\":43221},{\"end\":43249,\"start\":43244},{\"end\":43264,\"start\":43256},{\"end\":43279,\"start\":43272},{\"end\":43295,\"start\":43288},{\"end\":43314,\"start\":43305},{\"end\":43728,\"start\":43720},{\"end\":43746,\"start\":43737},{\"end\":43980,\"start\":43976},{\"end\":43995,\"start\":43992},{\"end\":44004,\"start\":44001},{\"end\":44429,\"start\":44425},{\"end\":44442,\"start\":44439},{\"end\":44820,\"start\":44810},{\"end\":44834,\"start\":44828},{\"end\":44846,\"start\":44840},{\"end\":45181,\"start\":45177},{\"end\":45191,\"start\":45187},{\"end\":45201,\"start\":45199},{\"end\":45210,\"start\":45207},{\"end\":45224,\"start\":45220},{\"end\":45236,\"start\":45231},{\"end\":45251,\"start\":45247},{\"end\":45834,\"start\":45830},{\"end\":45844,\"start\":45840},{\"end\":45853,\"start\":45850},{\"end\":46305,\"start\":46301},{\"end\":46315,\"start\":46311},{\"end\":46324,\"start\":46321},{\"end\":46336,\"start\":46331},{\"end\":46699,\"start\":46695},{\"end\":46709,\"start\":46705},{\"end\":46718,\"start\":46715},{\"end\":46730,\"start\":46725},{\"end\":46745,\"start\":46741},{\"end\":47196,\"start\":47189},{\"end\":47210,\"start\":47203},{\"end\":47223,\"start\":47217},{\"end\":47240,\"start\":47231},{\"end\":47253,\"start\":47248},{\"end\":47268,\"start\":47263},{\"end\":47283,\"start\":47277},{\"end\":47301,\"start\":47291},{\"end\":47648,\"start\":47641},{\"end\":47666,\"start\":47660},{\"end\":47679,\"start\":47673},{\"end\":47694,\"start\":47689},{\"end\":48126,\"start\":48122},{\"end\":48140,\"start\":48136},{\"end\":48152,\"start\":48150},{\"end\":48481,\"start\":48475},{\"end\":48495,\"start\":48489},{\"end\":48511,\"start\":48505},{\"end\":48706,\"start\":48704},{\"end\":48716,\"start\":48714},{\"end\":48728,\"start\":48723},{\"end\":48743,\"start\":48740},{\"end\":48760,\"start\":48751},{\"end\":48781,\"start\":48769},{\"end\":48793,\"start\":48788},{\"end\":48808,\"start\":48802},{\"end\":49193,\"start\":49189},{\"end\":49202,\"start\":49200},{\"end\":49212,\"start\":49208},{\"end\":49224,\"start\":49221},{\"end\":49245,\"start\":49232},{\"end\":49251,\"start\":49247},{\"end\":49743,\"start\":49738},{\"end\":49755,\"start\":49752},{\"end\":49765,\"start\":49761},{\"end\":49773,\"start\":49771},{\"end\":49787,\"start\":49785},{\"end\":49801,\"start\":49797},{\"end\":49813,\"start\":49811},{\"end\":49822,\"start\":49820}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3753452},\"end\":32922,\"start\":32302},{\"attributes\":{\"doi\":\"arXiv:2001.10913\",\"id\":\"b1\"},\"end\":33507,\"start\":32924},{\"attributes\":{\"doi\":\"arXiv:1912.08226\",\"id\":\"b2\"},\"end\":33825,\"start\":33509},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16941525},\"end\":34417,\"start\":33827},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":57246310},\"end\":34818,\"start\":34419},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2528601},\"end\":35307,\"start\":34820},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52967399},\"end\":36078,\"start\":35309},{\"attributes\":{\"doi\":\"arXiv:1911.00720\",\"id\":\"b7\"},\"end\":36421,\"start\":36080},{\"attributes\":{\"doi\":\"arXiv:2004.09800\",\"id\":\"b8\"},\"end\":36671,\"start\":36423},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206594692},\"end\":37114,\"start\":36673},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":58981871},\"end\":37827,\"start\":37116},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":196199713},\"end\":38362,\"start\":37829},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5776384},\"end\":38863,\"start\":38364},{\"attributes\":{\"doi\":\"arXiv:1901.07042\",\"id\":\"b13\"},\"end\":39375,\"start\":38865},{\"attributes\":{\"doi\":\"abs/1412.6980\",\"id\":\"b14\"},\"end\":39570,\"start\":39377},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":195886317},\"end\":39974,\"start\":39572},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":59276384},\"end\":40465,\"start\":39976},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":44234294},\"end\":40864,\"start\":40467},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":964287},\"end\":41097,\"start\":40866},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":102353285},\"end\":41555,\"start\":41099},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":18347865},\"end\":42087,\"start\":41557},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11080756},\"end\":42599,\"start\":42089},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206594923},\"end\":43123,\"start\":42601},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":46940459},\"end\":43644,\"start\":43125},{\"attributes\":{\"doi\":\"abs/1409.1556\",\"id\":\"b24\"},\"end\":43898,\"start\":43646},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":27280855},\"end\":44376,\"start\":43900},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":51608133},\"end\":44770,\"start\":44378},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1399322},\"end\":45060,\"start\":44772},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":220046453},\"end\":45732,\"start\":45062},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":222310640},\"end\":46240,\"start\":45734},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":222378720},\"end\":46619,\"start\":46242},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":220045475},\"end\":47153,\"start\":46621},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13756489},\"end\":47584,\"start\":47155},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1169492},\"end\":48059,\"start\":47586},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":69715157},\"end\":48465,\"start\":48061},{\"attributes\":{\"doi\":\"abs/1410.3916\",\"id\":\"b35\"},\"end\":48617,\"start\":48467},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1055111},\"end\":49126,\"start\":48619},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52190560},\"end\":49661,\"start\":49128},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":208030974},\"end\":50532,\"start\":49663}]", "bib_title": "[{\"end\":32385,\"start\":32302},{\"end\":33906,\"start\":33827},{\"end\":34470,\"start\":34419},{\"end\":34920,\"start\":34820},{\"end\":35389,\"start\":35309},{\"end\":36717,\"start\":36673},{\"end\":37204,\"start\":37116},{\"end\":37920,\"start\":37829},{\"end\":38418,\"start\":38364},{\"end\":39609,\"start\":39572},{\"end\":40057,\"start\":39976},{\"end\":40547,\"start\":40467},{\"end\":40920,\"start\":40866},{\"end\":41148,\"start\":41099},{\"end\":41640,\"start\":41557},{\"end\":42151,\"start\":42089},{\"end\":42653,\"start\":42601},{\"end\":43161,\"start\":43125},{\"end\":43970,\"start\":43900},{\"end\":44419,\"start\":44378},{\"end\":44798,\"start\":44772},{\"end\":45168,\"start\":45062},{\"end\":45821,\"start\":45734},{\"end\":46292,\"start\":46242},{\"end\":46686,\"start\":46621},{\"end\":47180,\"start\":47155},{\"end\":47633,\"start\":47586},{\"end\":48112,\"start\":48061},{\"end\":48695,\"start\":48619},{\"end\":49179,\"start\":49128},{\"end\":49727,\"start\":49663}]", "bib_author": "[{\"end\":32403,\"start\":32387},{\"end\":32416,\"start\":32403},{\"end\":32431,\"start\":32416},{\"end\":32445,\"start\":32431},{\"end\":32459,\"start\":32445},{\"end\":32474,\"start\":32459},{\"end\":32485,\"start\":32474},{\"end\":32939,\"start\":32924},{\"end\":32965,\"start\":32939},{\"end\":32981,\"start\":32965},{\"end\":32991,\"start\":32981},{\"end\":33010,\"start\":32991},{\"end\":33026,\"start\":33010},{\"end\":33044,\"start\":33026},{\"end\":33059,\"start\":33044},{\"end\":33070,\"start\":33059},{\"end\":33526,\"start\":33509},{\"end\":33544,\"start\":33526},{\"end\":33561,\"start\":33544},{\"end\":33577,\"start\":33561},{\"end\":33929,\"start\":33908},{\"end\":33937,\"start\":33929},{\"end\":33944,\"start\":33937},{\"end\":33969,\"start\":33944},{\"end\":33987,\"start\":33969},{\"end\":34005,\"start\":33987},{\"end\":34013,\"start\":34005},{\"end\":34039,\"start\":34013},{\"end\":34049,\"start\":34039},{\"end\":34482,\"start\":34472},{\"end\":34492,\"start\":34482},{\"end\":34508,\"start\":34492},{\"end\":34519,\"start\":34508},{\"end\":34527,\"start\":34519},{\"end\":34539,\"start\":34527},{\"end\":34941,\"start\":34922},{\"end\":34953,\"start\":34941},{\"end\":35405,\"start\":35391},{\"end\":35421,\"start\":35405},{\"end\":35433,\"start\":35421},{\"end\":35453,\"start\":35433},{\"end\":36093,\"start\":36080},{\"end\":36105,\"start\":36093},{\"end\":36115,\"start\":36105},{\"end\":36127,\"start\":36115},{\"end\":36142,\"start\":36127},{\"end\":36488,\"start\":36475},{\"end\":36498,\"start\":36488},{\"end\":36510,\"start\":36498},{\"end\":36731,\"start\":36719},{\"end\":36746,\"start\":36731},{\"end\":36760,\"start\":36746},{\"end\":36770,\"start\":36760},{\"end\":37220,\"start\":37206},{\"end\":37238,\"start\":37220},{\"end\":37250,\"start\":37238},{\"end\":37260,\"start\":37250},{\"end\":37283,\"start\":37260},{\"end\":37296,\"start\":37283},{\"end\":37313,\"start\":37296},{\"end\":37329,\"start\":37313},{\"end\":37341,\"start\":37329},{\"end\":37359,\"start\":37341},{\"end\":37934,\"start\":37922},{\"end\":37945,\"start\":37934},{\"end\":37956,\"start\":37945},{\"end\":38432,\"start\":38420},{\"end\":38445,\"start\":38432},{\"end\":38456,\"start\":38445},{\"end\":38879,\"start\":38865},{\"end\":38888,\"start\":38879},{\"end\":38895,\"start\":38888},{\"end\":38911,\"start\":38895},{\"end\":38934,\"start\":38911},{\"end\":38945,\"start\":38934},{\"end\":38973,\"start\":38945},{\"end\":38979,\"start\":38973},{\"end\":38988,\"start\":38979},{\"end\":39001,\"start\":38988},{\"end\":39008,\"start\":39001},{\"end\":39433,\"start\":39421},{\"end\":39447,\"start\":39433},{\"end\":39451,\"start\":39447},{\"end\":39629,\"start\":39611},{\"end\":39653,\"start\":39629},{\"end\":39680,\"start\":39653},{\"end\":39697,\"start\":39680},{\"end\":39710,\"start\":39697},{\"end\":40070,\"start\":40059},{\"end\":40082,\"start\":40070},{\"end\":40097,\"start\":40082},{\"end\":40108,\"start\":40097},{\"end\":40114,\"start\":40108},{\"end\":40558,\"start\":40549},{\"end\":40573,\"start\":40558},{\"end\":40585,\"start\":40573},{\"end\":40598,\"start\":40585},{\"end\":40936,\"start\":40922},{\"end\":41165,\"start\":41150},{\"end\":41185,\"start\":41165},{\"end\":41205,\"start\":41185},{\"end\":41218,\"start\":41205},{\"end\":41233,\"start\":41218},{\"end\":41250,\"start\":41233},{\"end\":41268,\"start\":41250},{\"end\":41653,\"start\":41642},{\"end\":41668,\"start\":41653},{\"end\":41681,\"start\":41668},{\"end\":41697,\"start\":41681},{\"end\":42171,\"start\":42153},{\"end\":42185,\"start\":42171},{\"end\":42196,\"start\":42185},{\"end\":42210,\"start\":42196},{\"end\":42665,\"start\":42655},{\"end\":42681,\"start\":42665},{\"end\":42700,\"start\":42681},{\"end\":42715,\"start\":42700},{\"end\":42730,\"start\":42715},{\"end\":42736,\"start\":42730},{\"end\":43177,\"start\":43163},{\"end\":43192,\"start\":43177},{\"end\":43206,\"start\":43192},{\"end\":43216,\"start\":43206},{\"end\":43234,\"start\":43216},{\"end\":43251,\"start\":43234},{\"end\":43266,\"start\":43251},{\"end\":43281,\"start\":43266},{\"end\":43297,\"start\":43281},{\"end\":43316,\"start\":43297},{\"end\":43730,\"start\":43714},{\"end\":43748,\"start\":43730},{\"end\":43982,\"start\":43972},{\"end\":43997,\"start\":43982},{\"end\":44006,\"start\":43997},{\"end\":44431,\"start\":44421},{\"end\":44444,\"start\":44431},{\"end\":44822,\"start\":44800},{\"end\":44836,\"start\":44822},{\"end\":44848,\"start\":44836},{\"end\":45183,\"start\":45170},{\"end\":45193,\"start\":45183},{\"end\":45203,\"start\":45193},{\"end\":45212,\"start\":45203},{\"end\":45226,\"start\":45212},{\"end\":45238,\"start\":45226},{\"end\":45253,\"start\":45238},{\"end\":45836,\"start\":45823},{\"end\":45846,\"start\":45836},{\"end\":45855,\"start\":45846},{\"end\":46307,\"start\":46294},{\"end\":46317,\"start\":46307},{\"end\":46326,\"start\":46317},{\"end\":46338,\"start\":46326},{\"end\":46701,\"start\":46688},{\"end\":46711,\"start\":46701},{\"end\":46720,\"start\":46711},{\"end\":46732,\"start\":46720},{\"end\":46747,\"start\":46732},{\"end\":47198,\"start\":47182},{\"end\":47212,\"start\":47198},{\"end\":47225,\"start\":47212},{\"end\":47242,\"start\":47225},{\"end\":47255,\"start\":47242},{\"end\":47270,\"start\":47255},{\"end\":47285,\"start\":47270},{\"end\":47303,\"start\":47285},{\"end\":47650,\"start\":47635},{\"end\":47668,\"start\":47650},{\"end\":47681,\"start\":47668},{\"end\":47696,\"start\":47681},{\"end\":48128,\"start\":48114},{\"end\":48142,\"start\":48128},{\"end\":48154,\"start\":48142},{\"end\":48483,\"start\":48469},{\"end\":48497,\"start\":48483},{\"end\":48513,\"start\":48497},{\"end\":48708,\"start\":48697},{\"end\":48718,\"start\":48708},{\"end\":48730,\"start\":48718},{\"end\":48745,\"start\":48730},{\"end\":48762,\"start\":48745},{\"end\":48783,\"start\":48762},{\"end\":48795,\"start\":48783},{\"end\":48810,\"start\":48795},{\"end\":49195,\"start\":49181},{\"end\":49204,\"start\":49195},{\"end\":49214,\"start\":49204},{\"end\":49226,\"start\":49214},{\"end\":49247,\"start\":49226},{\"end\":49253,\"start\":49247},{\"end\":49745,\"start\":49729},{\"end\":49757,\"start\":49745},{\"end\":49767,\"start\":49757},{\"end\":49775,\"start\":49767},{\"end\":49789,\"start\":49775},{\"end\":49803,\"start\":49789},{\"end\":49815,\"start\":49803},{\"end\":49824,\"start\":49815}]", "bib_venue": "[{\"end\":32562,\"start\":32485},{\"end\":33198,\"start\":33086},{\"end\":33645,\"start\":33593},{\"end\":34104,\"start\":34049},{\"end\":34602,\"start\":34539},{\"end\":35021,\"start\":34953},{\"end\":35595,\"start\":35453},{\"end\":36226,\"start\":36158},{\"end\":36473,\"start\":36423},{\"end\":36847,\"start\":36770},{\"end\":37420,\"start\":37359},{\"end\":38043,\"start\":37956},{\"end\":38543,\"start\":38456},{\"end\":39099,\"start\":39024},{\"end\":39419,\"start\":39377},{\"end\":39759,\"start\":39710},{\"end\":40175,\"start\":40114},{\"end\":40647,\"start\":40598},{\"end\":40967,\"start\":40936},{\"end\":41310,\"start\":41268},{\"end\":41774,\"start\":41697},{\"end\":42293,\"start\":42210},{\"end\":42813,\"start\":42736},{\"end\":43365,\"start\":43316},{\"end\":43712,\"start\":43646},{\"end\":44083,\"start\":44006},{\"end\":44525,\"start\":44444},{\"end\":44897,\"start\":44848},{\"end\":45340,\"start\":45253},{\"end\":45941,\"start\":45855},{\"end\":46421,\"start\":46338},{\"end\":46834,\"start\":46747},{\"end\":47352,\"start\":47303},{\"end\":47773,\"start\":47696},{\"end\":48215,\"start\":48154},{\"end\":48854,\"start\":48810},{\"end\":49339,\"start\":49253},{\"end\":49999,\"start\":49824},{\"end\":32626,\"start\":32564},{\"end\":35076,\"start\":35023},{\"end\":35724,\"start\":35597},{\"end\":36911,\"start\":36849},{\"end\":37468,\"start\":37422},{\"end\":38117,\"start\":38045},{\"end\":38617,\"start\":38545},{\"end\":40223,\"start\":40177},{\"end\":41838,\"start\":41776},{\"end\":42363,\"start\":42295},{\"end\":42877,\"start\":42815},{\"end\":44147,\"start\":44085},{\"end\":44593,\"start\":44527},{\"end\":45414,\"start\":45342},{\"end\":46014,\"start\":45943},{\"end\":46908,\"start\":46836},{\"end\":47837,\"start\":47775},{\"end\":48263,\"start\":48217},{\"end\":49412,\"start\":49341},{\"end\":50161,\"start\":50001}]"}}}, "year": 2023, "month": 12, "day": 17}