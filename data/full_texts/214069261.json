{"id": 214069261, "updated": "2023-04-05 12:54:52.997", "metadata": {"title": "SCAFFOLD : Stochastic Controlled Averaging for Federated Learning", "authors": "[{\"first\":\"Sai\",\"last\":\"Karimireddy\",\"middle\":[\"Praneeth\"]},{\"first\":\"Satyen\",\"last\":\"Kale\",\"middle\":[]},{\"first\":\"Mehryar\",\"last\":\"Mohri\",\"middle\":[]},{\"first\":\"Sashank\",\"last\":\"Reddi\",\"middle\":[\"J.\"]},{\"first\":\"Sebastian\",\"last\":\"Stich\",\"middle\":[\"U.\"]},{\"first\":\"Ananda\",\"last\":\"Suresh\",\"middle\":[\"Theertha\"]}]", "venue": "ICML", "journal": "5132-5143", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Federated Averaging (F ED A VG ) has emerged as the algorithm of choice for federated learning due to its simplicity and low communication cost. However, in spite of recent research efforts, its performance is not fully understood. We obtain tight convergence rates for F ED A VG and prove that it suffers from \u2018client-drift\u2019 when the data is heterogeneous (non-iid), resulting in un-stable and slow convergence. As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (vari-ance reduction) to correct for the \u2018client-drift\u2019 in its local updates. We prove that SCAFFOLD requires signi\ufb01cantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client\u2019s data yielding even faster convergence. The latter is the \ufb01rst result to quantify the usefulness of local-steps in distributed optimization.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3034666238", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/KarimireddyKMRS20", "doi": null}}, "content": {"source": {"pdf_hash": "72366a1d8e79d808e7293fb69d06923674ee2ad4", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "26e1840a71704a067491c2e3c59c5580e4c4a746", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/72366a1d8e79d808e7293fb69d06923674ee2ad4.txt", "contents": "\nSCAFFOLD: Stochastic Controlled Averaging for Federated Learning\n\n\nSai Praneeth Karimireddy \nSatyen Kale \nMehryar Mohri \nSashank J Reddi \nSebastian U Stich \nAnanda Theertha Suresh \nSCAFFOLD: Stochastic Controlled Averaging for Federated Learning\n\nFederated Averaging (FEDAVG) has emerged as the algorithm of choice for federated learning due to its simplicity and low communication cost. However, in spite of recent research efforts, its performance is not fully understood. We obtain tight convergence rates for FEDAVG and prove that it suffers from 'client-drift' when the data is heterogeneous (non-iid), resulting in unstable and slow convergence.As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the 'client-drift' in its local updates. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client's data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.\n\nIntroduction\n\nFederated learning has emerged as an important paradigm in modern large-scale machine learning. Unlike in traditional centralized learning where models are trained using large datasets stored in a central server (Dean et al., 2012;Iandola et al., 2016;Goyal et al., 2017), in federated learning, the training data remains distributed over a large number of clients, which may be phones, network sensors, hospitals, or alternative local information sources (Kone\u010dn\u1ef3 et al., 2016b;a;McMahan et al., 2017;Mohri et al., 2019;Kairouz et al., 2019). A centralized model (referred to as server model) is then trained without ever transmitting 1 EPFL, Lausanne 2 Based on work performed at Google Research, New York. 3 Google Research, New York 4 Courant Institute, New York. Correspondence to: Sai Praneeth Karimireddy <sai.karimireddy@epfl.ch>.\n\nProceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s). client data over the network, thereby ensuring a basic level of privacy. In this work, we investigate stochastic optimization algorithms for federated learning.\n\nThe key challenges for federated optimization are 1) dealing with unreliable and relatively slow network connections between the server and the clients, 2) only a small subset of clients being available for training at a given time, and 3) large heterogeneity (non-iid-ness) in the data present on the different clients (Kone\u010dn\u1ef3 et al., 2016a). The most popular algorithm for this setting is FEDAVG (McMahan et al., 2017). FEDAVG tackles the communication bottleneck by performing multiple local updates on the available clients before communicating to the server. While it has shown success in certain applications, its performance on heterogeneous data is still an active area of research (Li et al., 2018;Yu et al., 2019;Li et al., 2019b;Haddadpour & Mahdavi, 2019;Khaled et al., 2020). We prove that indeed such heterogeneity has a large effect on FEDAVG-it introduces a drift in the updates of each client resulting in slow and unstable convergence. Further, we show that this client-drift persists even if full batch gradients are used and all clients participate throughout the training.\n\nAs a solution, we propose a new Stochastic Controlled Averaging algorithm (SCAFFOLD) which tries to correct for this client-drift. Intuitively, SCAFFOLD estimates the update direction for the server model (c) and the update direction for each client c i . 1 The difference (c \u2212 c i ) is then an estimate of the client-drift which is used to correct the local update. This strategy successfully overcomes heterogeneity and converges in significantly fewer rounds of communication. Alternatively, one can see heterogeneity as introducing 'client-variance' in the updates across the different clients and SCAFFOLD then performs 'client-variance reduction' (Schmidt et al., 2017;Johnson & Zhang, 2013;Defazio et al., 2014). We use this viewpoint to show that SCAFFOLD is relatively unaffected by client sampling.\n\nFinally, while accommodating heterogeneity is important, it is equally important that a method can take advantage of similarities in the client data. We prove that SCAFFOLD indeed has such a property, requiring fewer rounds of com-munication when the clients are more similar.\n\nContributions. We summarize our main results below.\n\n\u2022 We derive tighter convergence rates for FEDAVG than previously known for convex and non-convex functions with client sampling and heterogeneous data. \u2022 We give matching lower bounds to prove that even with no client sampling and full batch gradients, FEDAVG can be slower than SGD due to client-drift. \u2022 We propose a new Stochastic Controlled Averaging algorithm (SCAFFOLD) which corrects for this clientdrift. We prove that SCAFFOLD is at least as fast as SGD and converges for arbitrarily heterogeneous data. \u2022 We show SCAFFOLD can additionally take advantage of similarity between the clients to further reduce the communication required, proving the advantage of taking local steps over large-batch SGD for the first time. \u2022 We prove that SCAFFOLD is relatively unaffected by the client sampling obtaining variance reduced rates, making it especially suitable for federated learning.\n\nFinally, we confirm our theoretical results on simulated and real datasets (extended MNIST by Cohen et al. (2017)).  et al. (2018). The analysis of FEDAVG for heterogeneous clients is more delicate due to the afore-mentioned clientdrift, first empirically observed by Zhao et al. (2018). Several analyses bound this drift by assuming bounded gradients (Wang et al., 2019;Yu et al., 2019), or view it as additional noise (Khaled et al., 2020), or assume that the client optima are -close (Li et al., 2018;Haddadpour & Mahdavi, 2019). In a concurrent work, (Liang et al., 2019) propose to use variance reduction to deal with client heterogeneity but still show rates slower than SGD and do not support client sampling. Our method SCAFFOLD can also be seen as an improved version of the distributed optimization algorithm DANE by (Shamir et al., 2014), where a fixed number of (stochastic) gradient steps are used in place of a proximal point update. A more in-depth discussion of related work is given in Appendix A. We summarize the complexities of different methods for heterogeneous clients in Table 2.\n\n\nSetup\n\nWe formalize the problem as minimizing a sum of stochastic functions, with only access to stochastic samples: Table 1. Summary of notation used in the paper N , S, and i total num., sampled num., and index of clients R, r number, index of communication rounds K, k number, index of local update steps x r aggregated server model after round r y r i,k ith client's model in round r and step k c r , c r i control variate of server, ith client after round r\nmin x\u2208R d f (x) := 1 N N i=1 (f i (x) := E \u03b6i [f i (x; \u03b6 i )])\n.\n\nThe functions f i represents the loss function on client i. All our results can be easily extended to the weighted case.\n\nWe assume that f is bounded from below by f and f i is \u03b2-smooth. Further, we assume g i (x) := \u2207f i (x; \u03b6 i ) is an unbiased stochastic gradient of f i with variance bounded by \u03c3 2 . For some results, we assume \u00b5 \u2265 0 (strong) convexity. Note that \u03c3 only bounds the variance within clients. We also define two non-standard terminology below.\n\n(A1) (G, B)-BGD or bounded gradient dissimilarity: there exist constants G \u2265 0 and B \u2265 1 such that\n1 N N i=1 \u2207f i (x) 2 \u2264 G 2 + B 2 \u2207f (x) 2 , \u2200x .\nIf {f i } are convex, we can relax the assumption to\n1 N N i=1 \u2207f i (x) 2 \u2264 G 2 + 2\u03b2B 2 (f (x) \u2212 f ) , \u2200x .\n(A2) \u03b4-BHD or bounded Hessian dissimilarity:\n\u2207 2 f i (x) \u2212 \u2207 2 f (x) \u2264 \u03b4 , \u2200x . Further, f i is \u03b4-weakly convex i.e. \u2207 2 f i (x) \u2212\u03b4I.\nThe assumptions A1 and A2 are orthogonal-it is possible to have G = 0 and \u03b4 = 2\u03b2, or \u03b4 = 0 but G 1.\n\n\nConvergence of FedAvg\n\nIn this section we review FEDAVG and improve its convergence analysis by deriving tighter rates than known before. The scheme consists of two main parts: local updates to the model (1), and aggregating the client updates to update the server model (2). In each round, a subset of clients S \u2286 [N ] are sampled uniformly. Each of these clients i \u2208 S copies the current sever model y i = x and performs K local updates of the form:\ny i \u2190 y i \u2212 \u03b7 l g i (y i ) .(1)\nHere \u03b7 l is the local step-size. Then the clients' updates (y i \u2212x) are aggregated to form the new server model using a global step-size \u03b7 g as:\n\nx \u2190 x + \u03b7 g |S| i\u2208S (y i \u2212 x) .\n\n(2) Table 2. Number of communication rounds required to reach accuracy for \u00b5 strongly convex and non-convex functions (log factors are ignored). Set \u00b5 = for general convex rates. (G, B) bounds gradient dissimilarity (A1), and \u03b4 bounds Hessian dissimilarity (A2). Our rates for FEDAVG are more general and tighter than others, even matching the lower bound. However, SGD is still faster (B \u2265 1). SCAFFOLD does not require any assumptions, is faster than SGD, and is robust to client sampling. Further, when clients become more similar (small \u03b4), SCAFFOLD converges even faster.\n\n\nMethod\n\nStrongly convex Non-convex Sampling Assumptions SGD (large batch) \n\u2126( \u03c3 2 \u00b5N K + G \u221a \u00b5 ) ? \u00d7 (G, 1)-BGD FedProx (Li et al., 2018) 2 B 2 \u00b5 B 2 (weakly convex) \u03c3 = 0, (0, B)-BGD DANE (Shamir et al., 2014) 2,3 \u03b4 2 \u00b5 2 - \u00d7 \u03c3 = 0, \u03b4-BHD VRL-SGD (Liang et al., 2019) - N \u03c3 2 K 2 + N \u00d7 - SCAFFOLD Theorem III \u03c3 2 \u00b5SK + 1 \u00b5 + N S \u03c3 2 SK 2 + 1 ( N S ) 2 3 - Theorem IV 3 \u03c3 2 \u00b5N K + 1 \u00b5K + \u03b4 \u00b5 \u03c3 2 N K 2 + 1 K + \u03b4 \u00d7 \u03b4-BHD 1 M 2 := \u03c3 2 + K(1 \u2212 S N )G 2 . Note that M 2 S = \u03c3 2 N when no sampling (S = N ). 2 proximal point method i.e. K\n1. 3 proved only for quadratic functions.\n\n\nRate of convergence\n\nWe now state our novel convergence results for functions with bounded dissimilarity (proofs in Appendix D.2).\n\nTheorem I. For \u03b2-smooth functions {f i } which satisfy (A1), the output of FEDAVG has expected error smaller than in each of the below three cases for some values of \u03b7 l and \u03b7 g , with the following bound on R \u2022 \u00b5 Strongly convex:\nR =\u00d5 \u03c3 2 \u00b5KS + 1 \u2212 S N G 2 \u00b5S + \u221a \u03b2G \u00b5 \u221a + B 2 \u03b2 \u00b5 ,\n\u2022 General convex:\nR = O \u03c3 2 D 2 KS 2 + 1\u2212 S N G 2 D 2 S 2 + \u221a \u03b2G 3 2 + B 2 \u03b2D 2 ,\n\u2022 Non-convex:\nR = O \u03b2\u03c3 2 F KS 2 + 1 \u2212 S N G 2 F S 2 + \u221a \u03b2G 3 2 + B 2 \u03b2F , where D := x 0 \u2212 x 2 and F := f (x 0 ) \u2212 f .\nThe exact values of \u03b7 l and \u03b7 g decreases with the number of rounds R and can be found in the proofs in the Appendix. It is illuminating to compare our rates with those of the simpler iid. case i.e. with G = 0 and B = 1. Our strongly-convex rates become \u03c3 2 \u00b5SK + 1 \u00b5 . In comparison, the best previously known rate for this case was by Stich & Karimireddy (2019) who show a rate of \u03c3 2 \u00b5SK + S \u00b5 . The main source of improvement in the rates came from the use of two separate step-sizes (\u03b7 l and \u03b7 g ). By having a larger global step-size \u03b7 g , we can use a smaller local stepsize \u03b7 l thereby reducing the client-drift while still ensuring progress. However, even our improved rates do not match the lower-bound for the identical case of \u03c3 2 \u00b5SK + 1\n\nK\u00b5 (Woodworth et al., 2018). We bridge this gap for quadratic functions in Section 6.\n\nWe now compare FEDAVG to two other algorithms Fed-Prox by (Li et al., 2018) (aka EASGD by (Zhang et al., 2015) and to SGD. Suppose that G = 0 and \u03c3 = 0 i.e. we use full batch gradients and all clients have very similar optima. In such a case, FEDAVG has a complexity of B 2 \u00b5 which is identical to that of FedProx (Li et al., 2018). Thus, FedProx does not have any theoretical advantage.\n\nx y 1 client 1 y 2 client 2\n\nx 2\n\nx 1\n\nx server client update client drift server update SGD update true opt. client opt. Figure 1. Client-drift in FEDAVG is illustrated for 2 clients with 3 local steps (N = 2, K = 3). The local updates yi (in blue) move towards the individual client optima x i (orange square). The server updates (in red) move towards 1 N i x i instead of to the true optimum x (black square).\n\nNext, suppose that all clients participate (no sampling) with S = N and there is no variance \u03c3 = 0. Then, the above for strongly-convex case simplifies to G \u00b5 \u221a + B 2 \u00b5 . In comparison, extending the proof of (Khaled et al., 2020) using our techniques gives a worse dependence on G of\nG 2 \u00b5KN + G \u00b5 \u221a .\nSimilarly, for the non-convex case, our rates are tighter and have better dependence on G than (Yu et al., 2019). However, simply running SGD in this setting would give a communication complexity of \u03b2 \u00b5 which is faster, and independent of similarity assumptions. In the next section we examine the necessity of such similarity assumptions.\n\n\nLower bounding the effect of heterogeneity\n\nWe now show that when the functions {f i } are distinct, the local updates of FEDAVG on each client experiences drift thereby slowing down convergence. We show that the amount of this client drift, and hence the slowdown in the rate of convergence, is exactly determined by the gradient dissimilarity parameter G in (A1).\n\nWe now examine the mechanism by which the client-drift arises (see Fig. 1). Let x be the global optimum of f (x) and x i be the optimum of each client's loss function f i (x). In the case of heterogeneous data, it is quite likely that each of these x i is far away from the other, and from the global optimum x . Even if all the clients start from the same point x, each of the y i will move towards their client optimum x i . This means that the average of the client updates (which is the server update) moves towards 1\nN N i=1 x i . This difference between 1 N N i=1\nx i and the true optimum x is exactly the cause of client-drift. To counter this drift, FEDAVG is forced to use much smaller step-sizes which in turn hurts convergence. We can formalize this argument to prove a lower-bound (see Appendix D.4 for proof).\n\nTheorem II. For any positive constants G and \u00b5, there Algorithm 1 SCAFFOLD: Stochastic Controlled Averaging for federated learning 1: server input: initial x and c, and global step-size \u03b7 g 2: client i's input: c i , and local step-size \u03b7 l 3: for each round r = 1, . . . , R do 4: sample clients S \u2286 {1, . . . , N }\n\n\n5:\n\ncommunicate (x, c) to all clients i \u2208 S 6: on client i \u2208 S in parallel do 7:\n\ninitialize local model y i \u2190 x 8:\n\nfor k = 1, . . . , K do 9: compute mini-batch gradient g i (y i ) 10:\ny i \u2190 y i \u2212 \u03b7 l (g i (y i ) \u2212 c i + c) 11:\nend for 12:\nc + i \u2190 (i) g i (x), or (ii) c i \u2212 c + 1 K\u03b7 l (x \u2212 y i ) 13: communicate (\u2206y i , \u2206c i ) \u2190 (y i \u2212 x, c + i \u2212 c i ) 14: c i \u2190 c + i 15:\nend on client 16:\n(\u2206x, \u2206c) \u2190 1 |S| i\u2208S (\u2206y i , \u2206c i )\n17:\n\nx \u2190 x + \u03b7 g \u2206x and c \u2190 c + |S| N \u2206c 18: end for exist \u00b5-strongly convex functions satisfying A1 for which FEDAVG with K \u2265 2, \u03c3 = 0 and N = S has an error\nf (x r ) \u2212 f (x ) \u2265 \u2126 G 2 \u00b5R 2 .\nThis implies that the G \u221a term is unavoidable even if there is no stochasticity. Further, because FEDAVG uses RKN stochastic gradients, we also have the statistical lowerbound of \u03c3 2 \u00b5KN . Together, these lower bounds prove that the rate derived in Theorem I is nearly optimal (up to dependence on \u00b5). In the next section, we introduce a new method SCAFFOLD to mitigate this client-drift.\n\n\nSCAFFOLD algorithm\n\nIn this section we first describe SCAFFOLD and then discuss how it solves the problem of client-drift.\n\nMethod. SCAFFOLD has three main steps: local updates to the client model (3), local updates to the client control variate (4), and aggregating the updates (5). We describe each in more detail.\n\nAlong with the server model x, SCAFFOLD maintains a state for each client (client control variate c i ) and for the server (server control variate c). These are initialized to ensure that c = 1 N c i and can safely all be initialized to 0. In each round of communication, the server parameters (x, c) are communicated to the participating clients S \u2282 [N ]. Each participating client i \u2208 S initializes its local model with the server model y i \u2190 x. Then it makes a pass . Update steps of SCAFFOLD on a single client. The local gradient (dashed black) points to x 1 (orange square), but the correction term (c \u2212 ci) (in red) ensures the update moves towards the true optimum x (black square). over its local data performing K updates of the form:\ny i \u2190 y i \u2212 \u03b7 l (g i (y i ) + c \u2212 c i ) .(3)\nThen, the local control variate c i is also updated. For this, we provide two options:\nc + i \u2190 Option I. g i (x) , or Option II. c i \u2212 c + 1 K\u03b7 l (x \u2212 y i ) .(4)\nOption I involves making an additional pass over the local data to compute the gradient at the server model x. Option II instead re-uses the previously computed gradients to update the control variate. Option I can be more stable than II depending on the application, but II is cheaper to compute and usually suffices (all our experiments use Option II). The client updates are then aggregated and used to update the server parameters:\nx \u2190 x + \u03b7 g |S| i\u2208S (y i \u2212 x) , c \u2190 c + 1 N i\u2208S (c + i \u2212 c i ) .(5)\nThis finishes one round of communication. Note that the clients in SCAFFOLD are stateful and retain the value of c i across multiple rounds. Further, if c i is always set to 0, then SCAFFOLD becomes equivalent to FEDAVG. The full details are summarized in Algorithm 1.\n\nUsefulness of control variates. If communication cost was not a concern, the ideal update on client i would be\ny i \u2190 y i + 1 N j g j (y i ) .(6)\nSuch an update essentially computes an unbiased gradient of f and hence becomes equivalent to running FEDAVG in the iid case (which has excellent performance). Unfortunately such an update requires communicating with all clients for every update step. SCAFFOLD instead uses control variates such that\nc j \u2248 g j (y i ) and c \u2248 1 N j g j (y i ) .\nThen, SCAFFOLD (3) mimics the ideal update (6) with\n(g i (y i ) \u2212 c i + c) \u2248 1 N j g j (y i ) .\nThus, the local updates of SCAFFOLD remain synchronized and converge for arbitrarily heterogeneous clients.\n\n\nConvergence of SCAFFOLD\n\nWe state the rate of SCAFFOLD without making any assumption on the similarity between the functions. See Appendix E for the full proof.\n\nTheorem III. For any \u03b2-smooth functions {f i }, the output of SCAFFOLD has expected error smaller than for in each of the below three cases for some values of \u03b7 l and \u03b7 g , with the following bound on R\n\n\u2022 \u00b5 Strongly convex:\nR =\u00d5 \u03c3 2 \u00b5KS + \u03b2 \u00b5 + N S ,\n\u2022 General convex:\nR =\u00d5 \u03c3 2 D 2 KS 2 + \u03b2D 2 + N F S ,\n\u2022 Non-convex:\nR = O \u03b2\u03c3 2 F KS 2 + N S 2 3 \u03b2F , where D := x 0 \u2212 x 2 and F := f (x 0 ) \u2212 f .\nThe exact values of \u03b7 l and \u03b7 g decreases with the number of rounds R and can be found in the proofs in the Appendix. Let us first examine the rates without client sampling (S = N ). For the strongly convex case, the number of rounds becomes \u03c3 2 \u00b5N K + 1 \u00b5 . This rate holds for arbitrarily heterogeneous clients unlike Theorem I and further matches that of SGD with K times larger batch-size, proving that SCAFFOLD is at least as fast as SGD. These rates also match known lower-bounds for distributed optimization (Arjevani & Shamir, 2015) (up to acceleration) and are unimprovable in general. However in certain cases SCAF-FOLD is provably faster than SGD. We show this fact in Section 6. Now let \u03c3 = 0. Then our rates in the strongly-convex case are 1 \u00b5 + N S and N S 2 3 1 in the non-convex case. These exactly match the rates of SAGA (Defazio et al., 2014;Reddi et al., 2016c). In fact, when \u03c3 = 0, K = 1 and S = 1, the update of SCAFFOLD with option I reduces to SAGA where in each round consists of sampling one client f i . Thus SCAFFOLD can be seen as an extension of variance reduction techniques for federated learning, and one could similarly extend SARAH (Nguyen et al., 2017), SPI-DER (Fang et al., 2018), etc. Note that standard SGD with client sampling is provably slower and converges at a sublinear rate even with \u03c3 = 0.\n\nProof sketch. For simplicity, assume that \u03c3 = 0 and consider the ideal update of (6) which uses the full gradient \u2207f (y) every step. Clearly, this would converge at a linear rate even with S = 1. FEDAVG would instead use an update \u2207f i (y). The difference between the ideal update (6) and the FEDAVG update (1) is \u2207f i (y) \u2212 \u2207f (y) . We need a bound on the gradient-dissimilarity as in (A1) to bound this error. SCAFFOLD instead uses the update \u2207f i (y) \u2212 c i + c, and the difference from ideal update becomes\ni (\u2207f i (y) \u2212 c i + c) \u2212 \u2207f (y) 2 \u2264 i c i \u2212 \u2207f i (y) 2 .\nThus, the error is independent of how similar or dissimilar the functions f i are, and instead only depends on the quality of our approximation c i \u2248 \u2207f i (y). Since f i is smooth, we can expect that the gradient \u2207f i (y) does not change too fast and hence is easy to approximate. Appendix E translates this intuition into a formal proof.\n\n\nUsefulness of local steps\n\nIn this section we investigate when and why taking local steps might be useful over simply computing a large-batch gradient in distributed optimization. We will show that when the functions across the clients share some similarity, local steps can take advantage of this and converge faster. For this we consider quadratic functions and express their similarity with the \u03b4 parameter introduced in (A2).\n\nTheorem IV. For any \u03b2-smooth quadratic functions {f i } with \u03b4 bounded Hessian dissimilarity (A2), the output of SCAFFOLD with S = N (no sampling) has error smaller than in each of the following two cases with \u03b7 g = 1, some value of \u03b7 l , and R satisfying \u2022 Strongly convex:\nR =\u00d5 \u03b2\u03c3 2 \u00b5KN + \u03b2 + \u03b4K \u00b5K ,\n\u2022 Weakly convex:\nR = O \u03b2\u03c3 2 F KN 2 + (\u03b2 + \u03b4K)F K ,\nwhere we define F := (f (x 0 ) \u2212 f ).\n\nHere again the exact value of \u03b7 l decreases with the number of rounds R and can be found in the proofs in the Appendix. When \u03c3 = 0 and K is large, the complexity of SCAFFOLD becomes \u03b4 \u00b5 . In contrast DANE, which being a proximal point method also uses large K, requires ( \u03b4 \u00b5 ) 2 rounds (Shamir et al., 2014) which is significantly slower, or needs an additional backtracking-line search to match the rates of SCAFFOLD (Yuan & Li, 2019). Further, Theorem IV is the first result to demonstrate improvement due to similairty for non-convex functions as far as we are aware.\n\nSuppose that {f i } are identical. Recall that \u03b4 in (A2) measures the Hessian dissimilarity between functions and so \u03b4 = 0 for this case. Then Theorem IV shows that the complexity of SCAFFOLD is \u03c3 2 \u00b5KN + 1 \u00b5K which (up to acceleration) matches the i.i.d. lower bound of (Woodworth et al., 2018). In contrast, SGD with K times larger batchsize would require \u03c3 2 \u00b5KN + 1 \u00b5 (note the absence of K in the second term). Thus, for identical functions, SCAFFOLD (and in fact even FEDAVG) improves linearly with increasing number of local steps. In the other extreme, if the functions are arbitrarily different, we may have \u03b4 = 2\u03b2. In this case, the complexity of SCAFFOLD and large-batch SGD match the lower bound of Arjevani & Shamir (2015) for the heterogeneous case.\n\nThe above insights can be generalized to when the functions are only somewhat similar. If the Hessians are \u03b4-close and \u03c3 = 0, then the complexity is \u03b2+\u03b4K \u00b5K . This bound implies that the optimum number of local steps one should use is K = \u03b2 \u03b4 . Picking a smaller K increases the communication required whereas increasing it further would only waste computational resources. While this result is intuitive-if the functions are more 'similar', local steps are more useful-Theorem IV shows that it is the similarity of the Hessians which matters. This is surprising since the Hessians of {f i } may be identical even if their individual optima x i are arbitrarily far away from each other and the gradient-dissimilarity (A1) is unbounded.\n\nProof sketch. Consider a simplified SCAFFOLD update with \u03c3 = 0 and no sampling (S = N ):\ny i = y i \u2212 \u03b7(\u2207f i (y i ) + \u2207f (x) \u2212 \u2207f i (x)) .\nWe would ideally want to perform the update y i = y i \u2212 \u03b7\u2207f (y i ) using the full gradient \u2207f (y i ). We reinterpret the correction term of SCAFFOLD (c \u2212 c i ) as performing the following first order correction to the local gradient \u2207f i (y i ) to make it closer to the full gradient \u2207f (y i ):\n\u2207f i (y i ) \u2212 \u2207f i (x) \u2248\u2207 2 fi(x)(yi\u2212x) + \u2207f (x) \u2248\u2207f (yi)+\u2207 2 f (x)(x\u2212yi) \u2248 \u2207f (y i ) + (\u2207 2 f i (x) \u2212 \u2207 2 f (x))(y i \u2212 x) \u2248 \u2207f (y i ) + \u03b4(y i \u2212 x)\nThus the SCAFFOLD update approximates the ideal update up to an error \u03b4. This intuition is proved formally for  . SGD (dashed black), FedAvg (above), and SCAFFOLD (below) on simulated data. FedAvg gets worse as local steps increases with K = 10 (red) worse than K = 2 (orange). It also gets slower as gradient-dissimilarity (G) increases (to the right). SCAFFOLD significantly improves with more local steps, with K = 10 (blue) faster than K = 2 (light blue) and SGD. Its performance is identical as we vary heterogeneity (G). quadratic functions in Appendix F. Generalizing these results to other functions is a challenging open problem.\n\n\nExperiments\n\nWe run experiments on both simulated and real datasets to confirm our theory. Our main findings are i) SCAF-FOLD consistently outperforms SGD and FEDAVG across all parameter regimes, and ii) the benefit (or harm) of local steps depends on both the algorithm and the similarity of the clients data.\n\n\nSetup\n\nOur simulated experiments uses N = 2 quadratic functions based on our lower-bounds in Theorem II. We use full-batch gradients (\u03c3 = 0) and no client sampling. Our real world experiments run logistic regression (convex) and 2 layer fully connected network (non-convex) on the EM-NIST (Cohen et al., 2017). We divide this dataset among N = 100 clients as follows: for s% similar data we allocate to each client s% i.i.d. data and the remaining (100 \u2212 s)% by sorting according to label (cf. Hsu et al. (2019)).\n\nWe consider four algorithms: SGD, FEDAVG SCAF-FOLD and FEDPROX with SGD as the local solver (Li et al., 2018). On each client SGD uses the full local data to compute a single update, whereas the other algorithms take 5 steps per epoch (batch size is 0.2 of local data). We always use global step-size \u03b7 g = 1 and tune the local stepsize \u03b7 l individually for each algorithm. SCAFFOLD uses option II (no extra gradient computations) and FEDPROX has fixed regularization = 1 to keep comparison fair. Additional tuning of the regularization parameter may sometimes yield improved empirical performance.\n\n\nSimulated results\n\nThe results are summarized in Fig. 3. Our simulated data has Hessian difference \u03b4 = 1 (A2) and \u03b2 = 1. We vary the gradient heterogeneity (A1) as G \u2208 [1, 10, 100]. For all valued of G, FEDAVG gets slower as we increase the number of local steps. This is explained by the fact that client-drift increases as we increase the number of local steps, hindering progress. Further, as we increase G, FEDAVG continues to slow down exactly as dictated by Thms. I and II. Note that when heterogeneity is small (G = \u03b2 = 1), FEDAVG can be competitive with SGD.\n\nSCAFFOLD is consistently faster than SGD, with K = 2 being twice as fast and K = 10 about 5 times faster. Further, its convergence is completely unaffected by G, confirming our theory in Thm. III. The former observation that we do not see linear improvement with K is explained by Thm. IV since we have \u03b4 > 0. This sub linear improvement is still significantly faster than both SGD and FEDAVG.\n\n\nEMNIST results\n\nWe run extensive experiments on the EMNIST dataset to measure the interplay between the algorithm, number of epochs (local updates), number of participating clients, and the client similarity. Table 3 measures the benefit (or harm) of using more local steps, Table 4 studies the resilience to client sampling, and Table 5 reports preliminary results on neural networks. We are mainly concerned with minimizing the number of communication rounds. We observe that SCAFFOLD is consistently the best. Across all range of values tried, we observe that SCAFFOLD outperforms SGD, FEDAVG, and FEDPROX. The latter FEDPROX is always slower than the other local update methods, though in some cases it outperforms SGD. Note that it is possible to improve FEDPROX by carefully tuning the regularization parameter (Li et al., 2018). FEDAVG is always slower than SCAFFOLD and faster than FEDPROX.\n\nSCAFFOLD > SGD > FedAvg for heterogeneous clients. When similarity is 0%, FEDAVG gets slower with increasing local steps. If we take more than 5 epochs, its performance is worse than SGD's. SCAFFOLD initially worsens as we increase the number of epochs but then flattens. However, its performance is always better than that of SGD, confirming that it can handle heterogeneous data.\n\nSCAFFOLD and FedAvg get faster with more similarity, but not SGD. As similarity of the clients increases, the performance of SGD remains relatively constant. On the other hand, SCAFFOLD and FEDAVG get significantly faster as similarity increases. Further, local steps become much more useful, showing monotonic improvement with the increase in number of epochs. This is because with increasing the i.i.d.ness of the data, both the gradient and Table 3. Communication rounds to reach 0.5 test accuracy for logistic regression on EMNIST as we vary number of epochs. 1k+ indicates 0.5 accuracy was not reached even after 1k rounds, and similarly an arrowhead indicates that the barplot extends beyond the table. 1 epoch for local update methods corresponds to 5 local steps (0.2 batch size), and 20% of clients are sampled each round. We fix \u00b5 = 1 for FEDPROX and use variant (ii) for SCAFFOLD to ensure all methods are comparable. Across all parameters (epochs and similarity), SCAFFOLD is the fastest method. When similarity is 0 (sorted data), FEDAVG consistently gets worse as we increase the number of epochs, quickly becoming slower than SGD. SCAFFOLD initially gets worse and later stabilizes, but is always at least as fast as SGD. As similarity increases (i.e. data is more shuffled), both FEDAVG and SCAFFOLD significantly outperform SGD though SCAFFOLD is still better than FEDAVG. Further, with higher similarity, both methods benefit from increasing number of epochs.  SCAFFOLD is resilient to client sampling. As we decrease the fraction of clients sampled, SCAFFOLD ,and FEDAVG only show a sub-linear slow-down. They are more resilient to sampling in the case of higher similarity.\n\n\nEpochs\n(< 0.3\u00d7) 18 (20.3\u00d7) 4 (104\u00d7) FedProx 1 1k+ (< 0.3\u00d7) 979 (0.4\u00d7) 459 (0.9\u00d7) 5 1k+ (< 0.3\u00d7) 794 (0.5\u00d7) 351 (1.2\u00d7) 10 1k+ (< 0.3\u00d7) 894 (0.4\u00d7) 308 (1.4\u00d7) 20 1k+ (< 0.3\u00d7) 916 (0.4\u00d7) 351 (1.2\u00d7)\nSCAFFOLD outperforms FedAvg on non-convex experiments. We see that SCAFFOLD is better than FEDAVG in terms of final test accuracy reached, though interestingly FEDAVG seems better than SGD even when similarity is 0. \n\n\nConclusion\n\nOur work studied the impact of heterogeneity on the performance of optimization methods for federated learning.\n\nOur careful theoretical analysis showed that FEDAVG can be severely hampered by gradient dissimilarity, and can be even slower than SGD. We then proposed a new stochastic algorithm (SCAFFOLD) which overcomes gradient dis-similarity using control variates. We demonstrated the effectiveness of SCAFFOLD via strong convergence guarantees and empirical evaluations. Further, we showed that while SCAFFOLD is always at least as fast as SGD, it can be much faster depending on the Hessian dissimilarity in our data. Thus, different algorithms can take advantage of (and are limited by) different notions of dissimilarity. We believe that characterizing and isolating various dissimilarities present in real world data can lead to further new algorithms and significant impact on distributed, federated, and decentralized learning. A. Related work and significance  Table 2. \n\n\nB. Technicalities\n\nWe examine some additional definitions and introduce some technical lemmas.\n\n\nB.1. Additional definitions\n\nWe make precise a few definitions and explain some of their implications.\n\n(A3) f i is \u00b5-convex for \u00b5 \u2265 0 and satisfies:\n\u2207f i (x), y \u2212 x \u2264 \u2212 f i (x) \u2212 f i (y) + \u00b5 2 x \u2212 y 2 , for any i, x, y .\nHere, we allow that \u00b5 = 0 (we refer to this case as the general convex case as opposed to strongly convex). It is also possible to generalize all proofs here to the weaker notion of PL-strong convexity (Karimi et al., 2016).\n(A4) g i (x) := \u2207f i (x; \u03b6 i ) is unbiased stochastic gradient of f i with bounded variance E \u03b6i [ g i (x) \u2212 \u2207f i (x) 2 ] \u2264 \u03c3 2 , for any i, x .\nNote that (A4) only bounds the variance within the same client, but not the variance across the clients.\n\n(A5) {f i } are \u03b2-smooth and satisfy:\n\u2207f i (x) \u2212 \u2207f i (y) \u2264 \u03b2 x \u2212 y , for any i, x, y .(7)\nThe assumption (A5) also implies the following quadratic upper bound on f i\nf i (y) \u2264 f i (x) + \u2207f (x), y \u2212 x + \u03b2 2 y \u2212 x 2 .(8)\nIf additionally the function {f i } are convex and x is an optimum of f , (A5) implies (via Nesterov (2018), Theorem 2.1.5)\n1 2\u03b2N N i=1 \u2207f i (x) \u2212 \u2207f i (x ) 2 \u2264 f (x) \u2212 f .(9)\nFurther, if f i is twice-differentiable, (A5) implies that \u2207 2 f i (x) \u2264 \u03b2 for any x.\n\n\nB.2. Some technical lemmas\n\nNow we cover some technical lemmas which are useful for computations later on. The two lemmas below are useful to unroll recursions and derive convergence rates. The first one is a slightly improved (and simplified) version of (Stich, 2019, Theorem 2). It is straightforward to remove the additional logarithmic terms if we use a varying step-size (Kulunchakov & Mairal, 2019, Lemma 13).\n\nLemma 1 (linear convergence rate). For every non-negative sequence {d r\u22121 } r\u22651 and any parameters \u00b5 > 0, \u03b7 max \u2208 (0, 1/\u00b5], c \u2265 0, R \u2265 1 2\u03b7max\u00b5 , there exists a constant step-size \u03b7 \u2264 \u03b7 max and weights w r := (1 \u2212 \u00b5\u03b7) 1\u2212r such that for W R := R+1 r=1 w r ,\n\u03a8 R := 1 W R R+1 r=1 w r \u03b7 (1 \u2212 \u00b5\u03b7) d r\u22121 \u2212 w r \u03b7 d r + c\u03b7w r =\u00d5 \u00b5d 0 exp(\u2212\u00b5\u03b7 max R) + c \u00b5R .\nProof. By substituting the value of w r , we observe that we end up with a telescoping sum and estimate\n\u03a8 R = 1 \u03b7W R R+1 r=1 (w r\u22121 d r\u22121 \u2212 w r d r ) + c\u03b7 W R R+1 r=1 w r \u2264 d 0 \u03b7W R + c\u03b7 . When R \u2265 1 2\u00b5\u03b7 , (1 \u2212 \u00b5\u03b7) R \u2264 exp(\u2212\u00b5\u03b7R) \u2264 2 3 .\nFor such an R, we can lower bound \u03b7W R using\n\u03b7W R = \u03b7(1 \u2212 \u00b5\u03b7) \u2212R R r=0 (1 \u2212 \u00b5\u03b7) r = \u03b7(1 \u2212 \u00b5\u03b7) \u2212R 1 \u2212 (1 \u2212 \u00b5\u03b7) R \u00b5\u03b7 \u2265 (1 \u2212 \u00b5\u03b7) \u2212R 1 3\u00b5 .\nThis proves that for all R \u2265 1 2\u00b5\u03b7 ,\n\u03a8 R \u2264 3\u00b5d 0 (1 \u2212 \u00b5\u03b7) R + c\u03b7 \u2264 3\u00b5d o exp(\u2212\u00b5\u03b7R) + c\u03b7 .\nThe lemma now follows by carefully tuning \u03b7. Consider the following two cases depending on the magnitude of R and \u03b7 max :\n\u2022 Suppose 1 2\u00b5R \u2264 \u03b7 max \u2264 log(max(1,\u00b5 2 Rd0/c)) \u00b5R\n. Then we can choose \u03b7 = \u03b7 max ,\n\u03a8 R \u2264 3\u00b5d 0 exp [\u2212\u00b5\u03b7 max R] + c\u03b7 max \u2264 3\u00b5d 0 exp [\u2212\u00b5\u03b7 max R] +\u00d5 c \u00b5R .\n\u2022 Instead if \u03b7 max > log(max(1,\u00b5 2 Rd0/c)) \u00b5R , we pick \u03b7 = log(max(1,\u00b5 2 Rd0/c)) \u00b5R to claim that\n\u03a8 R \u2264 3\u00b5d 0 exp \u2212 log(max(1, \u00b5 2 Rd 0 /c)) +\u00d5 c \u00b5R \u2264\u00d5 c \u00b5R .\nThe next lemma is an extension of (Stich & Karimireddy, 2019, Lemma 13), (Kulunchakov & Mairal, 2019, Lemma 13) and is useful to derive convergence rates for general convex functions (\u00b5 = 0) and non-convex functions.\n\nLemma 2 (sub-linear convergence rate). For every non-negative sequence {d r\u22121 } r\u22651 and any parameters \u03b7 max \u2265 0, c \u2265 0, R \u2265 0, there exists a constant step-size \u03b7 \u2264 \u03b7 max and weights w r = 1 such that,\n\u03a8 R := 1 R + 1 R+1 r=1 d r\u22121 \u03b7 \u2212 d r \u03b7 + c 1 \u03b7 + c 2 \u03b7 2 \u2264 d 0 \u03b7 max (R + 1) + 2 \u221a c 1 d 0 \u221a R + 1 + 2 d 0 R + 1 2 3 c 1 3 2 .\nProof. Unrolling the sum, we can simplify\n\u03a8 R \u2264 d 0 \u03b7(R + 1) + c 1 \u03b7 + c 2 \u03b7 2 .\nSimilar to the strongly convex case (Lemma 1), we distinguish the following cases:\n\u2022 When R + 1 \u2264 d0 c1\u03b7 2 max , and R + 1 \u2264 d0 c2\u03b7 3 max we pick \u03b7 = \u03b7 max to claim \u03a8 R \u2264 d 0 \u03b7 max (R + 1) + c 1 \u03b7 max + c 2 \u03b7 2 max \u2264 d 0 \u03b7 max (R + 1) + \u221a c 1 d 0 \u221a R + 1 + d 0 R + 1 2 3 c 1 3 2 .\n\u2022 In the other case, we have \u03b7\n2 max \u2265 d0 c1(R+1) or \u03b7 3 max \u2265 d0 c2(R+1) . We choose \u03b7 = min d0 c1(R+1) , 3 d0 c2(R+1) to prove \u03a8 R \u2264 d 0 \u03b7(R + 1) + c\u03b7 = 2 \u221a c 1 d 0 \u221a R + 1 + 2 3 d 2 0 c 2 (R + 1) 2 .\nNext, we state a relaxed triangle inequality true for the squared 2 norm.\n\nLemma 3 (relaxed triangle inequality). Let {v 1 , . . . , v \u03c4 } be \u03c4 vectors in R d . Then the following are true:\n1. v i + v j 2 \u2264 (1 + a) v i 2 + (1 + 1 a ) v j 2 for any a > 0, and 2. \u03c4 i=1 v i 2 \u2264 \u03c4 \u03c4 i=1 v i 2 .\nProof. The proof of the first statement for any a > 0 follows from the identity:\nv i + v j 2 = (1 + a) v i 2 + (1 + 1 a ) v j 2 \u2212 \u221a av i + 1 \u221a a v j 2 .\nFor the second inequality, we use the convexity of x \u2192 x 2 and Jensen's inequality\n1 \u03c4 \u03c4 i=1 v i 2 \u2264 1 \u03c4 \u03c4 i=1 v i 2 .\nNext we state an elementary lemma about expectations of norms of random vectors.\n\nLemma 4 (separating mean and variance). Let {\u039e 1 , . . . , \u039e \u03c4 } be \u03c4 random variables in R d which are not necessarily independent. First suppose that their mean is E[\u039e i ] = \u03be i and variance is bounded as E[ \u039e i \u2212 \u03be i 2 ] \u2264 \u03c3 2 . Then, the following holds \nE[ \u03c4 i=1 \u039e i 2 ] \u2264 \u03c4 i=1 \u03be i 2 + \u03c4 2 \u03c3 2 .\n\nNow instead suppose that their conditional mean is E[\u039e\ni |\u039e i\u22121 , . . . \u039e 1 ] = \u03be i i.e. the variables {\u039e i \u2212 \u03be i } formE[ \u03c4 i=1 \u039e i 2 ] \u2264 2 \u03c4 i=1 \u03be i 2 + 2\u03c4 \u03c3 2 .\nProof. For any random variable X, E[\nX 2 ] = (E[X \u2212 E[X]]) 2 + (E[X]) 2 implying E[ \u03c4 i=1 \u039e i 2 ] = \u03c4 i=1 \u03be i 2 + E[ \u03c4 i=1 \u039e i \u2212 \u03be i 2 ] .\nExpanding the above expression using relaxed triangle inequality (Lemma 3) proves the first claim:\nE[ \u03c4 i=1 \u039e i \u2212 \u03be i 2 ] \u2264 \u03c4 \u03c4 i=1 E[ \u039e i \u2212 \u03be i 2 ] \u2264 \u03c4 2 \u03c3 2 .\nFor the second statement, \u03be i is not deterministic and depends on \u039e i\u22121 , . . . , \u039e 1 . Hence we have to resort to the cruder relaxed triangle inequality to claim\nE[ \u03c4 i=1 \u039e i 2 ] \u2264 2 \u03c4 i=1 \u03be i 2 + 2 E[ \u03c4 i=1 \u039e i \u2212 \u03be i 2 ]\nand then use the tighter expansion of the second term:\nE[ \u03c4 i=1 \u039e i \u2212 \u03be i 2 ] = i,j E (\u039e i \u2212 \u03be i ) (\u039e j \u2212 \u03be j ) = i E \u039e i \u2212 \u03be i 2 \u2264 \u03c4 \u03c3 2 .\nThe cross terms in the above expression have zero mean since {\u039e i \u2212 \u03be i } form a martingale difference sequence.\n\n\nC. Properties of convex functions\n\nWe now study two lemmas which hold for any smooth and strongly-convex functions. The first is a generalization of the standard strong convexity inequality (A3), but can handle gradients computed at slightly perturbed points.\n\nLemma 5 (perturbed strong convexity). The following holds for any \u03b2-smooth and \u00b5-strongly convex function h, and any x, y, z in the domain of h:\n\u2207h(x), z \u2212 y \u2265 h(z) \u2212 h(y) + \u00b5 4 y \u2212 z 2 \u2212 \u03b2 z \u2212 x 2 .\nProof. Given any x, y, and z, we get the following two inequalities using smoothness and strong convexity of h:\n\u2207h(x), z \u2212 x \u2265 h(z) \u2212 h(x) \u2212 \u03b2 2 z \u2212 x 2 \u2207h(x), x \u2212 y \u2265 h(x) \u2212 h(y) + \u00b5 2 y \u2212 x 2 .\nFurther, applying the relaxed triangle inequality gives\n\u00b5 2 y \u2212 x 2 \u2265 \u00b5 4 y \u2212 z 2 \u2212 \u00b5 2 x \u2212 z 2 .\nCombining all the inequalities together we have\n\u2207h(x), z \u2212 y \u2265 h(z) \u2212 h(y) + \u00b5 4 y \u2212 z 2 \u2212 \u03b2 + \u00b5 2 z \u2212 x 2 .\nThe lemma follows since \u03b2 \u2265 \u00b5.\n\nHere, we see that a gradient step is a contractive operator.\n\nLemma 6 (contractive mapping). For any \u03b2-smooth and \u00b5-strongly convex function h, points x, y in the domain of h, and step-size \u03b7 \u2264 1 \u03b2 , the following is true\nx \u2212 \u03b7\u2207h(x) \u2212 y + \u03b7\u2207h(y) 2 \u2264 (1 \u2212 \u00b5\u03b7) x \u2212 y 2 .\nProof.\n\nx\n\u2212 \u03b7\u2207h(x) \u2212 y + \u03b7\u2207h(y) 2 = x \u2212 y 2 + \u03b7 2 \u2207h(x) \u2212 \u2207h(y) 2 \u2212 2\u03b7 \u2207h(x) \u2212 \u2207h(y), x \u2212 y (A5) \u2264 x \u2212 y 2 + (\u03b7 2 \u03b2 \u2212 2\u03b7) \u2207h(x) \u2212 \u2207h(y), x \u2212 y .\nRecall our bound on the step-size \u03b7 \u2264 1 \u03b2 which implies that (\u03b7 2 \u03b2 \u2212 2\u03b7) \u2264 \u2212\u03b7. Finally, apply the \u00b5-strong convexity of h to get \u2212\u03b7 \u2207h(x) \u2212 \u2207h(y), x \u2212 y \u2264 \u2212\u03b7\u00b5 x \u2212 y 2 .\n\n\nD. Convergence of FEDAVG\n\nAlgorithm 2 FEDAVG: Federated Averaging 1: server input: initial x, and global step-size \u03b7 g 2: client i's input: local step-size \u03b7 l 3: for each round r = 1, . . . , R do x \u2190 x + \u03b7 g \u2206x 16: end for\n\nWe outline the FEDAVG method in Algorithm 2. In round r we sample S r \u2286 [N ] clients with |S r | = S and then perform the following updates:\n\n\u2022 Starting from the shared global parameters y r i,0 = x r\u22121 , we update the local parameters for k \u2208 [K]\ny r i,k = y r i,k\u22121 \u2212 \u03b7 l g i (y r i,k\u22121 ) .(10)\n\u2022 Compute the new global parameters using only updates from the clients i \u2208 S r and a global step-size \u03b7 g :\nx r = x r\u22121 + \u03b7 g S i\u2208S r (y r i,K \u2212 x r\u22121 ) .(11)\nFinally, for some weights {w r }, we output\n\nx R = x r\u22121 with probability w r \u03c4 w \u03c4 for r \u2208 {1, . . . , R + 1} .\n\n\nD.1. Bounding heterogeneity\n\nRecall our bound on the gradient dissimilarity:\n1 N N i=1 \u2207f i (x) 2 \u2264 G 2 + B 2 \u2207f (x) 2 .(13)\nIf {f i } are convex, we can relax the assumption to\n1 N N i=1 \u2207f i (x) 2 \u2264 G 2 + 2\u03b2B 2 (f (x) \u2212 f ) .(14)\nWe defined two variants of the bounds on the heterogeneity depending of whether the functions are convex or not. Suppose that the functions f is indeed convex as in (A3) and \u03b2-smooth as in (A5), then it is straightforward to see that (13) implies (14). Thus for convex functions, (A1) is mildly weaker. Suppose that the functions {f 1 , . . . , f N } are convex and \u03b2-smooth.\n\nThen (14) is satisfied with B 2 = 2 since\n1 N N i=1 \u2207f i (x) 2 \u2264 2 N N i=1 \u2207f i (x ) 2 + 2 N N i=1 \u2207f i (x) \u2212 \u2207f i (x ) 2 (9) \u2264 2 N N i=1 \u2207f i (x ) 2 =: \u03c3 2 f +4\u03b2(f (x) \u2212 f ) .\nThus, (G, B)-BGD (14) is equivalent to the heterogeneity assumption of (Mishchenko et al., 2019) with G 2 = \u03c3 2 f . Instead, if we have the stronger assumption (A1) but the functions are possibly non-convex, then G = corresponds to the local dissimilarity defined in (Li et al., 2018). Note that assuming G is negligible is quite strong and corresponds to the stronggrowth condition (Vaswani et al., 2019).\n\n\nD.2. Rates of convergence (Theorem I)\n\nWe first restate Theorem I with some additional details and then see its proof.\n\nTheorem V. Suppose that the functions {f i } satisfies assumptions A4, A5, and A1. Then, in each of the following cases, there exist weights {w r } and local step-sizes \u03b7 l such that for any \u03b7 g \u2265 1 the output of FEDAVG (12)\nx R satisfies \u2022 Strongly convex: f i satisfies (A3) for \u00b5 > 0, \u03b7 l \u2264 1 8(1+B 2 )\u03b2K\u03b7g , R \u2265 8(1+B 2 )\u03b2 \u00b5 then E[f (x R )] \u2212 f (x ) \u2264\u00d5 M 2 \u00b5RKS + \u03b2G 2 \u00b5 2 R 2 + \u00b5D 2 exp(\u2212 \u00b5 16(1+B 2 )\u03b2 R) , \u2022 General convex: f i satisfies (A3) for \u00b5 = 0, \u03b7 l \u2264 1 (1+B 2 )8\u03b2K\u03b7g , R \u2265 1 then E[f (x R )] \u2212 f (x ) \u2264 O M D \u221a RKS + D 4/3 (\u03b2G 2 ) 1/3 (R + 1) 2/3 + B 2 \u03b2D 2 ) R ,\n\u2022 Non-convex: f i satisfies (A1) and \u03b7 l \u2264 1 (1+B 2 )8\u03b2K\u03b7g , then\nE[ \u2207f (x R ) 2 ] \u2264 O \u03b2M \u221a F \u221a RKS + F 2/3 (\u03b2G 2 ) 1/3 (R + 1) 2/3 + B 2 \u03b2F R , where M 2 := \u03c3 2 (1 + S \u03b7 2 g ) + K(1 \u2212 S N )G 2 , D := x 0 \u2212 x , and F := f (x 0 ) \u2212 f .\n\nD.3. Proof of convergence\n\nWe will only prove the rate of convergence for convex functions here. The corresponding rates for non-convex functions are easy to derive following the techniques in the rest of the paper.\n\nLemma 7. (one round progress) Suppose our functions satisfies assumptions (A1) and (A3)-(A5). For any step-size satisfying \u03b7 l \u2264 1 (1+B 2 )8\u03b2K\u03b7g and effective step-size\u03b7 := K\u03b7 g \u03b7 l , the updates of FEDAVG satisfy\nE x r \u2212 x 2 \u2264 (1 \u2212 \u00b5\u03b7 2 ) E x r\u22121 \u2212 x 2 + ( 1 KS )\u03b7 2 \u03c3 2 + (1 \u2212 S N ) 4\u03b7 2 S G 2 \u2212\u03b7(E[f (x r\u22121 )] \u2212 f (x )) + 3\u03b2\u03b7E r ,\nwhere E r is the drift caused by the local updates on the clients defined to be\nE r := 1 KN K k=1 N i=1 E r [ y r i,k \u2212 x r\u22121 2 ] .\nProof. We start with the observation that the updates (10) and (11) imply that the server update in round r can be written as below (dropping the superscripts everywhere)\n\u2206x = \u2212\u03b7 KS k,i\u2208S g i (y i,k\u22121 ) and E[\u2206x] = \u2212\u03b7 KN k,i E[\u2207f i (y i,k\u22121 )] .\nWe adopt the convention that summations are always over k \u2208 [K] or i \u2208 [N ] unless otherwise stated. Expanding using above observing, we proceed as 2\nE r\u22121 x + \u2206x \u2212 x 2 = x \u2212 x 2 \u2212 2\u03b7 KN k,i \u2207f i (y i,k\u22121 ), x \u2212 x +\u03b7 2 E r\u22121 1 KS k,i\u2208S g i (y i,k\u22121 ) 2 Lem. 4 \u2264 x \u2212 x 2 \u2212 2\u03b7 KN k,i \u2207f i (y i,k\u22121 ), x \u2212 x A1 +\u03b7 2 E r\u22121 1 KS k,i\u2208S \u2207f i (y i,k\u22121 ) 2 A2 +\u03b7 2 \u03c3 2 KS .\nWe can directly apply Lemma 5 with h = f i , x = y i,k\u22121 , y = x , and z = x to the first term A 1\nA 1 = 2\u03b7 KN k,i \u2207f i (y i,k\u22121 ), x \u2212 x \u2264 2\u03b7 KN k,i f i (x ) \u2212 f i (x) + \u03b2 y i,k\u22121 \u2212 x 2 \u2212 \u00b5 4 x \u2212 x 2 = \u22122\u03b7 f (x) \u2212 f (x ) + \u00b5 4 x \u2212 x 2 + 2\u03b2\u03b7E .\nFor the second term A 2 , we repeatedly apply the relaxed triangle inequality (Lemma 4)\nA 2 =\u03b7 2 E r\u22121 1 KS k,i\u2208S \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) + \u2207f i (x) 2 \u2264 2\u03b7 2 E r\u22121 1 KS k,i\u2208S \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 + 2\u03b7 2 E r\u22121 1 S i\u2208S \u2207f i (x) 2 \u2264 2\u03b7 2 KN i,k E r\u22121 \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 + 2\u03b7 2 E r\u22121 1 S i\u2208S \u2207f i (x) \u2212 \u2207f (x) + \u2207f (x) 2 \u2264 2\u03b7 2 \u03b2 2 KN i,k E r\u22121 y i,k\u22121 \u2212 x 2 + 2\u03b7 2 \u2207f (x) 2 + (1 \u2212 S N )4\u03b7 2 1 SN i \u2207f i (x) 2 \u2264 2\u03b7 2 \u03b2 2 E + 8\u03b7 2 \u03b2(B 2 + 1)(f (x) \u2212 f (x )) + (1 \u2212 S N ) 4\u03b7 2 S G 2\nThe last step used Assumption (G, B)-BGD assumption (14) that 1\nN N i=1 \u2207f i (x) 2 \u2264 G 2 + 2\u03b2B 2 (f (x) \u2212 f ).\nThe extra (1 \u2212 S N improvement we get is due to sampling the functions {f i } without replacement. Plugging back the bounds on A 1 and A 2 ,\nE r\u22121 x + \u2206x \u2212 x 2 \u2264 (1 \u2212 \u00b5\u03b7 2 ) x \u2212 x 2 \u2212 (2\u03b7 \u2212 8\u03b2\u03b7 2 (B 2 + 1))(f (x) \u2212 f (x )) + (1 +\u03b7\u03b2)2\u03b2\u03b7E + 1 KS\u03b7 2 \u03c3 2 + (1 \u2212 S N ) 4\u03b7 2 S G 2 .\nThe lemma now follows by observing that 8\u03b2\u03b7(B 2 + 1) \u2264 1 and that B \u2265 0. 2 We use the notation Er\u22121[ \u00b7 ] to mean conditioned on filtration r i.e. on all the randomness generated prior to round r.\n\n\nLemma 8. (bounded drift) Suppose our functions satisfies assumptions (A1) and (A3)-(A5).\n\nThen the updates of FE-DAVG for any step-size satisfying \u03b7 l \u2264 1 (1+B 2 )8\u03b2K\u03b7g have bounded drift:\n3\u03b2\u03b7E r \u2264 2\u03b7 3 (E[f (x r\u22121 )]) \u2212 f (x ) +\u03b7 2 \u03c3 2 2K\u03b7 2 g + 18\u03b2\u03b7 3 G 2 .\nProof. If K = 1, the lemma trivially holds since y i,0 = x for all i \u2208 [N ] and E r = 0. Assume K \u2265 2 here on. Recall that the local update made on client i is y i,k = y i,k\u22121 \u2212 \u03b7 l g i (y i,k\u22121 ). Then,\nE y i,k \u2212 x 2 = E y i,k\u22121 \u2212 x \u2212 \u03b7 l g i (y i,k\u22121 ) 2 \u2264 E y i,k\u22121 \u2212 x \u2212 \u03b7 l \u2207f i (y i,k\u22121 ) 2 + \u03b7 2 l \u03c3 2 \u2264 (1 \u2212 1 K\u22121 ) E y i,k\u22121 \u2212 x 2 + K\u03b7 2 l \u2207f i (y i,k\u22121 ) 2 + \u03b7 2 l \u03c3 2 = (1 \u2212 1 K\u22121 ) E y i,k\u22121 \u2212 x 2 +\u03b7 2 \u03b7 g K \u2207f i (y i,k\u22121 ) 2 +\u03b7 2 \u03c3 2 K 2 \u03b7 2 g \u2264 (1 \u2212 1 K\u22121 ) E y i,k\u22121 \u2212 x 2 + 2\u03b7 2 \u03b7 g K \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 + 2\u03b7 2 \u03b7 g K \u2207f i (x) 2 +\u03b7 2 \u03c3 2 K 2 \u03b7 2 g \u2264 (1 \u2212 1 K\u22121 + 2\u03b7 2 \u03b2 2 \u03b7gK ) E y i,k\u22121 \u2212 x 2 + 2\u03b7 2 \u03b7 g K \u2207f i (x) 2 +\u03b7 2 \u03c3 2 K 2 \u03b7 2 g \u2264 (1 \u2212 1 2(K\u22121) ) E y i,k\u22121 \u2212 x 2 + 2\u03b7 2 \u03b7 g K \u2207f i (x) 2 +\u03b7 2 \u03c3 2 K 2 \u03b7 2 g .\nIn the above proof we separated the mean and the variance in the first inequality, then used the relaxed triangle inequality with a = 1 K\u22121 in the next inequality. Next equality uses the definition of\u03b7, and the rest follow from the Lipschitzness of the gradient. Unrolling the recursion above,\nE y i,k \u2212 x 2 \u2264 k\u22121 \u03c4 =1 ( 2\u03b7 2 \u03b7 g K \u2207f i (x) 2 +\u03b7 2 \u03c3 2 K 2 \u03b7 2 g )(1 \u2212 1 2(K\u22121) ) \u03c4 \u2264 ( 2\u03b7 2 \u03b7 g K \u2207f i (x) 2 +\u03b7 2 \u03c3 2 K 2 \u03b7 2 g )3K .\nAveraging over i and k, multiplying by 3\u03b2\u03b7 and then using Assumption A1,\n3\u03b2\u03b7E r \u2264 1 N i 18\u03b2\u03b7 3 \u2207f i (x) 2 + 3\u03b2\u03b7 3 \u03c3 2 K\u03b7 2 g \u2264 18\u03b2\u03b7 3 G 2 + 3\u03b2\u03b7 3 \u03c3 2 K\u03b7 2 g + 36\u03b2 2\u03b73 B 2 (f (x) \u2212 f (x ))\nThe lemma now follows from our assumption that 8(B 2 + 1)\u03b2\u03b7 \u2264 1.\n\nProof of Theorems I, V Adding the statements of Lemmas 7 and 8, we get\nE x + \u2206x \u2212 x 2 \u2264 (1 \u2212 \u00b5\u03b7 2 ) E x \u2212 x 2 + ( 1 KS )\u03b7 2 \u03c3 2 + (1 \u2212 S N ) 4\u03b7 2 S G 2 \u2212\u03b7(E[f (x)] \u2212 f (x )) + 2\u03b7 3 (E[f (x)]) \u2212 f (x ) +\u03b7 2 \u03c3 2 2K\u03b7 2 g + 18\u03b2\u03b7 3 G 2 = (1 \u2212 \u00b5\u03b7 2 ) E x \u2212 x 2 \u2212\u03b7 3 (E[f (x)] \u2212 f (x )) +\u03b7 2 \u03c3 2 KS (1 + S \u03b7 2 g ) + 4G 2 S (1 \u2212 S N ) + 18\u03b2\u03b7G 2 .\nMoving the (f (x) \u2212 f (x )) term and dividing throughout by\u03b7 3 , we get the following bound for any\u03b7 \u2264\n1 8(1+B 2 )\u03b2 E[f (x r\u22121 )] \u2212 f (x ) \u2264 3 \u03b7 (1 \u2212 \u00b5\u03b7 2 ) x r\u22121 \u2212 x 2 \u2212 3 \u03b7 x r \u2212 x 2 + 3\u03b7 \u03c3 2 KS (1 + S \u03b7 2 g ) + 4G 2 S (1 \u2212 S N ) + 18\u03b2\u03b7G 2 .\nIf \u00b5 = 0 (general convex), we can directly apply Lemma 2. Otherwise, by averaging using weights w r = (1 \u2212 \u00b5\u03b7 2 ) 1\u2212r and using the same weights to pick outputx R , we can simplify the above recursive bound (see proof of Lem. 1) to prove that for any\u03b7 satisfying 1 \u00b5R \u2264\u03b7 \u2264\n1 8(1+B 2 )\u03b2 E[f (x R )] \u2212 f (x ) \u2264 3 x 0 \u2212 x 2 =:d \u00b5 exp(\u2212\u03b7 2 \u00b5R) +\u03b7 2\u03c3 2 KS (1 + S \u03b7 2 g ) + 8G 2 S (1 \u2212 S N ) =:c1 +\u03b7 2 (36\u03b2G 2 =:c2 )\nNow, the choice of\u03b7 = min log(max(1,\u00b5 2 Rd/c1)) \u00b5R , 1 (1+B 2 )8\u03b2 yields the desired rate.The proof of the non-convex case is very similar and also relies on Lemma 2.\n\n\nD.4. Lower bound for FEDAVG (Theorem II)\n\nWe first formalize the class of algorithms we look at before proving out lower bound.\n\n(A6) We assume that FEDAVG is run with \u03b7 g = 1, K > 1, and arbitrary possibly adaptive positive step-sizes {\u03b7 1 , . . . , \u03b7 R } are used with \u03b7 r \u2264 1 \u00b5 and fixed within a round for all clients. Further, the server update is a convex combination of the client updates with non-adaptive weights.\n\nNote that we only prove the lower bound here for \u03b7 g = 1. In fact, by taking \u03b7 g infinitely large and scaling \u03b7 l \u221d 1 K\u03b7g such that the effective step size\u03b7 = \u03b7 l \u03b7 g K remains constant, FEDAVG reduces to the simple large batch SGD method. Hence, proving a lower bound for arbitrary \u03b7 g is not possible, but also is of questionable relevance. Further, note that when \u03c3 2 = 0, the upper bound in Theorem V uses \u03b7 g = 1 and hence the lower bound serves to show that our analysis is tight.\n\nBelow we state a more formal version of Theorem II. Theorem VI. For any positive constants G, \u00b5, there exist \u00b5-strongly convex functions satisfying A1 for which that the output of FEDAVG satisfying A6 has the error for any r \u2265 1:\nf (x r ) \u2212 f (x ) \u2265 \u2126 min f (x 0 ) \u2212 f (x ), G 2 \u00b5R 2 .\nProof. Consider the following simple one-dimensional functions for any given \u00b5 and G:\n\nf 1 (x) := \u00b5x 2 + Gx, and f 2 (x) := \u2212Gx , with f (x) = 1 2 (f 1 (x) + f 2 (x)) = \u00b5 2 x 2 and optimum at x = 0. Clearly f is \u00b5-strongly convex and further f 1 and f 2 satisfy A1 with B = 3. Note that we chose f 2 to be a linear function (not strongly convex) to simplify computations. The calculations made here can be extended with slightly more work for (f 2 = \u00b5 2 x 2 \u2212 Gx) (e.g. see Theorem 1 of (Safran & Shamir, 2019)).\n\nLet us start FEDAVG from x 0 > 0. A single local update for f 1 and f 2 in round r \u2265 1 is respectively y 1 = y 1 \u2212 \u03b7 r (2\u00b5x + G) and y 2 = y 2 + \u03b7 r G .\n\nThen, straightforward computations show that the update at the end of round r is of the following form for some averaging weight \u03b1 \u2208 [0, 1]\nx r = x r\u22121 ((1 \u2212 \u03b1)(1 \u2212 2\u00b5\u03b7 r ) K + \u03b1) + \u03b7 r G K\u22121 \u03c4 =0 (\u03b1 \u2212 (1 \u2212 \u03b1)(1 \u2212 2\u00b5\u03b7 r ) \u03c4 ) .\nSince \u03b1 was picked obliviously, we can assume that \u03b1 \u2264 0.5. If indeed \u03b1 > 0.5, we can swap the definitions of f 1 and f 2 and the sign of x 0 . With this, we can simplify as\nx r \u2265 x r\u22121 (1 \u2212 2\u00b5\u03b7 r ) K + 1 2 + \u03b7 r G 2 K\u22121 \u03c4 =0 (1 \u2212 (1 \u2212 2\u00b5\u03b7 r ) \u03c4 ) \u2265 x r\u22121 (1 \u2212 2\u00b5\u03b7 r ) K + \u03b7 r G 2 K\u22121 \u03c4 =0 (1 \u2212 (1 \u2212 2\u00b5\u03b7 r ) \u03c4 ) .\nObserve that in the above expression, the right hand side is increasing with \u03b7 r -this represents the effect of the client drift and increases the error as the step-size increases. The left hand side decreases with \u03b7 r -this is the usual convergence observed due to taking gradient steps. The rest of the proof is to show that even with a careful balancing of the two terms, the effect of G cannot be removed. Lemma 9 performs exactly such a computation to prove that for any r \u2265 1,\nx r \u2265 c min(x 0 , G \u00b5R ) .\nWe finish the proof by noting that f (x r ) = \u00b5 2 (x r ) 2 . Lemma 9. Suppose that for all r \u2265 1, \u03b7 r \u2264 1 \u00b5 and the following is true:\nx r \u2265 x r\u22121 (1 \u2212 2\u00b5\u03b7 r ) K + \u03b7 r G 2 K\u22121 \u03c4 =0 (1 \u2212 (1 \u2212 2\u00b5\u03b7 r ) \u03c4 ) .\nThen, there exists a constant c > 0 such that for any sequence of step-sizes {\u03b7 r }:\nx r \u2265 c min(x 0 , G \u00b5R )\nProof. Define \u03b3 r = \u00b5\u03b7 r R(K \u2212 1). Such a \u03b3 r exists and is positive since K \u2265 2. Then, \u03b3 r satisfies\n(1 \u2212 2\u00b5\u03b7 r ) K\u22121 2 = (1 \u2212 2\u03b3 r R(K \u2212 1) ) K\u22121 2 \u2264 exp(\u2212\u03b3 r /R) .\nWe then have\nx r \u2265 x r\u22121 (1 \u2212 2\u00b5\u03b7 r ) K + \u03b7 r G 2 K\u22121 \u03c4 =0 (1 \u2212 (1 \u2212 2\u00b5\u03b7 r ) \u03c4 ) \u2265 x r\u22121 (1 \u2212 2\u00b5\u03b7 r ) K + \u03b7 r G 2 K\u22121 \u03c4 =(K\u22121)/2 (1 \u2212 (1 \u2212 2\u00b5\u03b7 r ) \u03c4 ) \u2265 x r\u22121 (1 \u2212 2\u00b5\u03b7 r ) K + \u03b3 r G 4\u00b5 (1 \u2212 exp(\u2212\u03b3 r /R)) .\nThe second inequality follows because \u03b7 r \u2264 1 \u00b5 implies that (1 \u2212 (1 \u2212 2\u00b5\u03b7 r ) \u03c4 ) is always positive. If \u03b3 r \u2265 R/8, then we have a constant c 1 \u2208 (0, 1/32) which satisfies\nx r \u2265 c 1 G \u00b5 .(15)\nOn the other hand, if \u03b3 r < R/8, we have a tighter inequality\n(1 \u2212 2\u00b5\u03b7 r ) K\u22121 2 = (1 \u2212 2\u03b3 r R(K \u2212 1) ) K\u22121 2 \u2264 1 \u2212 \u03b3 r R ,\nimplying that\nx r \u2265 x r\u22121 1 \u2212 2\u03b3 r R(K \u2212 1) K + \u03b3 2 r G 4R\u00b5 \u2265 x r\u22121 (1 \u2212 4\u03b3 r R ) + \u03b3 2 r G 4\u00b5R .(16)\nThe last step used Bernoulli's inequality and the fact that K \u2212 1 \u2264 K/2 for K \u2265 2. Observe that in the above expression, the right hand side is increasing with \u03b3 r -this represents the effect of the client drift and increases the error as the step-size increases. The left hand side decreases with \u03b3 r -this is the usual convergence observed due to taking gradient steps. The rest of the proof is to show that even with a careful balancing of the two terms, the effect of G cannot be removed.\n\nSuppose that all rounds after r 0 \u2265 0 have a small step-size i.e. \u03b3 r \u2264 R/8 for all r > r 0 and hence satisfies (16). Then we will prove via induction that\nx r \u2265 min(c r x r0 , G 256\u00b5R\n), for constants c r :\n= (1 \u2212 1 2R ) r\u2212r0 .(17)\nFor r = r 0 , (17) is trivially satisfied. Now for r > r 0 ,\nx r \u2265 x r\u22121 (1 \u2212 4\u03b3 r R ) + \u03b3 2 r G 4\u00b5R \u2265 min x r\u22121 (1 \u2212 1 2R ) , G 256\u00b5R = min c r x r0 , G 256\u00b5R .\nThe first step is because of (16) and the last step uses the induction hypothesis. The second step considers two cases for \u03b3 r : either \u03b3 r \u2264 1 8 and (1 \u2212 1 2R ) \u2265 (1 \u2212 1 2R ), or \u03b3 2 r \u2265 1 64 . Finally note that c r \u2265 1 2 using Bernoulli's inequality. We have hence proved\nx R \u2265 min 1 2 x r0 , G 256\u00b5R\nNow suppose \u03b3 r0 > R/8. Then (15) implies that x R \u2265 cG \u00b5R for some constant c > 0. If instead no such r 0 \u2265 1 exists, then we can set r 0 = 0. Now finally observe that the previous proof did not make any assumption on R, and in fact the inequality stated above holds for all r \u2265 1.\n\n\nE. Convergence of SCAFFOLD\n\nWe first restate the convergence theorem more formally, then prove the result for the convex case, and then for non-convex case. Throughout the proof, we will focus on the harder option II. The proofs for SCAFFOLD with option I are nearly identical and so we skip them. Theorem VII. Suppose that the functions {f i } satisfies assumptions A4 and A5. Then, in each of the following cases, there exist weights {w r } and local step-sizes \u03b7 l such that for any \u03b7 g \u2265 1 the output (22) of SCAFFOLD satisfies:\n\n\u2022 Strongly convex:\nf i satisfies (A3) for \u00b5 > 0, \u03b7 l \u2264 min 1 81\u03b2K\u03b7g , S 15\u00b5N K\u03b7g , R \u2265 max( 162\u03b2 \u00b5 , 30N S ) then E[f (x R )] \u2212 f (x ) \u2264\u00d5 \u03c3 2 \u00b5RKS (1 + S \u03b7 2 g ) + \u00b5D 2 exp \u2212 min S 30N , \u00b5 162\u03b2\nR .\n\n\u2022 General convex:\nf i satisfies (A3) for \u00b5 = 0, \u03b7 l \u2264 1 81\u03b2K\u03b7g , R \u2265 1 then E[f (x R )] \u2212 f (x ) \u2264 O \u03c3D \u221a RKS 1 + S \u03b7 2 g + \u03b2D 2 R , \u2022 Non-convex: \u03b7 l \u2264 1 24K\u03b7g\u03b2 S N 2 3 , and R \u2265 1, then E[ \u2207f (x R ) 2 ] \u2264 O \u03c3 \u221a F \u221a RKS 1 + N \u03b7 2 g + \u03b2F R N S 2 3\n.\nHereD 2 := ( x 0 \u2212 x 2 + 1 2S\u03b2 2 N i=1 c 0 i \u2212 \u2207f i (x ) 2 ) and F := (f (x 0 ) \u2212 f (x )\n). Remark 10. Note that theD 2 defined above involves an additional term 1\n2S\u03b2 2 N i=1 c 0 i \u2212 \u2207f i (x ) 2\n. This is standard in variance reduction methods (Johnson & Zhang, 2013;Defazio et al., 2014;Hanzely & Richt\u00e1rik, 2019). Theoretically, we will use a warm-start strategy to set c 0 i and in the first N/S rounds, we compute c 0 i = g i (x 0 ) over a batch size of size K. Then, using smoothness of f i , we can bound this additional term as\n1 2S\u03b2 2 N i=1 c 0 i \u2212 \u2207f i (x ) 2 \u2264 N S\u03b2 (f (x 0 ) \u2212 f ) + N \u03c3 2 KS\u03b2 2 \u2264 N D 2 S + N \u03c3 2 KS\u03b2 2 .\nIn particular, when S = N we can further simplify asD 2 \u2264 2D 2 + \u03c3 2 \u03b2 2 K and the rates are unaffected. More generally, the asymptotic rates of SCAFFOLD for general convex functions only incurs an additive term of the order of O( N SR ). For strongly convex functions, we only see the affects in the logarithmic terms.\n\nWe will rewrite SCAFFOLD using notation which is convenient for the proofs: {y i } represent the client models, x is the aggregate server model, and c i and c are the client and server control variates. For an equivalent description which is easier to implement, we refer to Algorithm 1. The server maintains a global control variate c as before and each client maintains its own control variate c i . In round r, a subset of clients S r of size S are sampled uniformly from {1, . . . , N }. Suppose that every client performs the following updates \u2022 Starting from the shared global parameters y 0 i,r = x r\u22121 , we update the local parameters for k \u2208 [K]\ny r i,k = y r i,k\u22121 \u2212 \u03b7 l v r i,k , where v r i,k := g i (y r i,k\u22121 ) \u2212 c r\u22121 i + c r\u22121(18)\n\u2022 Update the control iterates using (option II):\nc r i = c r\u22121 \u2212 c r\u22121 i + 1 K\u03b7 l (x r\u22121 \u2212 x r i,K ) = 1 K K k=1 g i (y r i,k\u22121 ) .(19)\nWe update the local control variates only for clients i \u2208 S r\nc r i = c r i if i \u2208 S r c r\u22121 i otherwise.(20)\n\u2022 Compute the new global parameters and global control variate using only updates from the clients i \u2208 S r :\nx r = x r\u22121 + \u03b7 g S i\u2208S r (y r i,K \u2212 x r\u22121 ) and c r = 1 N N i=1 c r i = 1 N i\u2208S r c r i + j / \u2208S r c r\u22121 j .(21)\nFinally, for some weights {w r }, we output\nx R = x r\u22121 with probability w r \u03c4 w \u03c4 for r \u2208 {1, . . . , R + 1} .(22)\nNote that the clients are agnostic to the sampling and their updates are identical to when all clients are participating. Also note that the control variate choice (19) corresponds to (option II) of Algorithm 1. Further, the updates of the clients i / \u2208 S r is forgotten and is defined only to make the proofs easier. While actually implementing the method, only clients i \u2208 S r participate and the rest remain inactive (see Algorithm 1).\n\n\nE.1. Convergence of SCAFFOLD for convex functions (Theorem III)\n\nWe will first bound the variance of SCAFFOLD update in Lemma 11, then see how sampling of clients effects our control variates in Lemma 12, and finally bound the amount of client-drift in Lemma 13. We will then use these three lemmas to prove the progress in a single round in Lemma 14. Combining this progress with Lemmas 1 and 2 gives us the desired rates.\n\nAdditional definitions. Before proceeding with the proof of our lemmas, we need some additional definitions of the various errors we track. As before, we define the effective step-size to b\u1ebd \u03b7 := K\u03b7 l \u03b7 g .\n\nWe define client-drift to be how much the clients move from their starting point:\nE r := 1 KN K k=1 N i=1 E[ y r i,k \u2212 x r\u22121 2 ] .(23)\nBecause we are sampling the clients, not all the client control-variates get updated every round. This leads to some 'lag' which we call control-lag:\nC r := 1 N N j=1 E E[c r i ] \u2212 \u2207f i (x ) 2 .(24)\nVariance of server update. We study how the variance of the server update can be bounded.\n\nLemma 11. For updates (18)-(21), we can bound the variance of the server update in any round r and any\u03b7 := \u03b7 l \u03b7 g K \u2265 0 as follows\nE[ x r \u2212 x r\u22121 2 ] \u2264 8\u03b2\u03b7 2 (E[f (x r\u22121 )] \u2212 f (x )) + 8\u03b7 2 C r\u22121 + 4\u03b7 2 \u03b2 2 E r + 12\u03b7 2 \u03c3 2 KS .\nProof. The server update in round r can be written as follows (dropping the superscript r everywhere)\nE \u2206x 2 = E \u2212\u03b7 KS k,i\u2208S v i,k 2 = E \u03b7 KS k,i\u2208S (g i (y i,k\u22121 ) + c \u2212 c i ) 2 ,\nwhich can then be expanded as\nE \u2206x 2 \u2264 E \u03b7 KS k,i\u2208S (g i (y i,k\u22121 ) + c \u2212 c i ) 2 \u2264 4 E \u03b7 KS k,i\u2208S g i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 + 4\u03b7 2 E c 2 + 4 E \u03b7 KS k,i\u2208S \u2207f i (x ) \u2212 c i 2 + 4 E \u03b7 KS k,i\u2208S \u2207f i (x) \u2212 \u2207f i (x ) 2 (9) \u2264 4 E \u03b7 KS k,i\u2208S g i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 + 4\u03b7 2 E c 2 + 4 E \u03b7 S i\u2208S \u2207f i (x ) \u2212 c i 2 + 8\u03b2\u03b7 2 (E[f (x)] \u2212 f (x )) \u2264 4 E \u03b7 KS k,i\u2208S \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 + 4\u03b7 2 E[c] 2 + 4 \u03b7 S i\u2208S \u2207f i (x ) \u2212 E[c i ] 2 + 8\u03b2\u03b7 2 (E[f (x)] \u2212 f (x )) + 12\u03b7 2 \u03c3 2 KS .\nThe inequality before the last used the smoothness of {f i }. The last inequality which separates the mean and the variance is an application of Lemma 4: the variance of ( 1 KS k,i\u2208S g i (y i,k\u22121 )) is bounded by \u03c3 2 /KS. Similarly, c j as defined in (19) for any j \u2208 [N ] has variance smaller than \u03c3 2 /K and hence the variance of ( 1 S i\u2208S c i ) is smaller than \u03c3 2 /KS. Using Lemma 3.2 twice to simplify:\nE \u2206x 2 \u2264 4\u03b7 2 KN k,i E \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 + 4\u03b7 2 E[c] 2 + 4\u03b7 2 N i \u2207f i (x ) \u2212 E[c i ] 2 + 8\u03b2\u03b7 2 (E[f (x)] \u2212 f (x )) + 12\u03b7 2 \u03c3 2 KS \u2264 4\u03b7 2 KN k,i E \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 T1 + 8\u03b7 2 N i \u2207f i (x ) \u2212 E[c i ] 2 + 8\u03b2\u03b7 2 (E[f (x)] \u2212 f (x )) + 12\u03b7 2 \u03c3 2 KS .\nThe second step follows because c = 1\nN i c i . Since the gradient of f i is \u03b2-Lipschitz, T 1 \u2264 \u03b2 2 4\u03b7 2 KN k,i E y i,k\u22121 \u2212x 2 = 4\u03b7 2 \u03b2 2 E.\nThe definition of the error in the control variate C r\u22121 := 1\nN N j=1 E E[c i ] \u2212 \u2207f i (x ) 2 completes the proof.\nChange in control lag. We have previously related the variance of the server update to the control lag. We now examine how the control-lag grows each round.\n\nLemma 12. For updates (18)-(21) with the control update (19) and assumptions A3-A5, the following holds true for an\u1ef9 \u03b7 := \u03b7 l \u03b7 g K \u2208 [0, 1/\u03b2]:\nC r \u2264 (1 \u2212 S N )C r\u22121 + S N 4\u03b2(E[f (x r\u22121 )] \u2212 f (x )) + 2\u03b2 2 E r .\nProof. Recall that after round r, the control update rule (19) implies that c r i is set as per\nc r i = c r\u22121 i if i / \u2208 S r i.e. with probability (1 \u2212 S N ). , 1 K K k=1 g i (y r i,k\u22121 ) with probability S N .\nTaking expectations on both sides yields\nE[c r i ] = (1 \u2212 S N ) E[c r\u22121 i ] + S KN K k=1 E[\u2207f i (y r i,k\u22121 )] , \u2200 i \u2208 [N ] .\nPlugging the above expression in the definition of C r we get\nC r = 1 N N i=1 E[c r i ] \u2212 \u2207f i (x ) 2 = 1 N N i=1 (1 \u2212 S N )(E[c r\u22121 i ] \u2212 \u2207f i (x )) + S N ( 1 K K k=1 E[\u2207f i (y r i,k\u22121 )] \u2212 \u2207f i (x )) 2 \u2264 (1 \u2212 S N )C r\u22121 + S N 2 K K k=1 E \u2207f i (y r i,k\u22121 ) \u2212 \u2207f i (x ) 2 .\nThe final step applied Jensen's inequality twice. We can then further simplify using the relaxed triangle inequality as\nE r\u22121 [C r ] \u2264 1 \u2212 S N C r\u22121 + S N 2 K i,k E \u2207f i (y r i,k\u22121 ) \u2212 \u2207f i (x ) 2 \u2264 1 \u2212 S N C r\u22121 + 2S N 2 i E \u2207f i (x r\u22121 ) \u2212 \u2207f i (x ) 2 + 2S N 2 K i,k E \u2207f i (y r i,k\u22121 ) \u2212 \u2207f i (x r\u22121 ) 2 (7) \u2264 1 \u2212 S N C r\u22121 + 2S N 2 i E \u2207f i (x r\u22121 ) \u2212 \u2207f i (x ) 2 + 2S N 2 K \u03b2 2 i,k E y r i,k\u22121 \u2212 x r\u22121 2 (9) \u2264 1 \u2212 S N C r\u22121 + S N (4\u03b2(E[f (x r\u22121 )] \u2212 f (x )) + \u03b2 2 E r ) .\nThe last two inequalities follow from smoothness of {f i } and the definition E r = 1\nN K \u03b2 2 i,k E y r i,k\u22121 \u2212 x r\u22121 2 .\nBounding client-drift. We will now bound the final source of error which is the client-drift.\n\nLemma 13. Suppose our step-sizes satisfy \u03b7 l \u2264 1 81\u03b2K\u03b7g and f i satisfies assumptions A3-A5. Then, for any global \u03b7 g \u2265 1 we can bound the drift as\n3\u03b2\u03b7E r \u2264 2\u03b7 2 3 C r\u22121 +\u03b7 25\u03b7 2 g (E[f (x r\u22121 )] \u2212 f (x )) +\u03b7 2 K\u03b7 2 g \u03c3 2 .\nProof. First, observe that if K = 1, E r = 0 since y i,0 = x for all i \u2208 [N ] and that C r\u22121 and the right hand side are both positive. Thus the lemma is trivially true if K = 1. For K > 1, we build a recursive bound of the drift.Starting from the definition of the update (18) and then applying the relaxed triangle inequality, we can expand\n1 S E r\u22121 i\u2208S (y i \u2212 \u03b7 l v i ) \u2212 x 2 = 1 S E r\u22121 i\u2208S y i \u2212 \u03b7 l g i (y i ) + \u03b7 l c \u2212 \u03b7 l c i \u2212 x 2 \u2264 1 S E r\u22121 i\u2208S y i \u2212 \u03b7 l \u2207f i (y i ) + \u03b7 l c \u2212 \u03b7 l c i \u2212 x 2 + \u03b7 2 l \u03c3 2 \u2264 (1 + a) S E r\u22121 i\u2208S y i \u2212 \u03b7 l \u2207f i (y i ) + \u03b7 l \u2207f i (x) \u2212 x 2 T2 + (1 + 1 a )\u03b7 2 l E r\u22121 1 S i\u2208S c \u2212 c i + \u2207f i (x) 2 T3 +\u03b7 2 l \u03c3 2 .\nThe final step follows from the relaxed triangle inequality (Lemma 3). Applying the contractive mapping Lemma 6 for \u03b7 l \u2264 1/\u03b2 shows\nT 2 = 1 S i\u2208S y i \u2212 \u03b7 l \u2207f i (y i ) + \u03b7 l \u2207f i (x) \u2212 x 2 \u2264 y i \u2212 x 2 .\nOnce again using our relaxed triangle inequality to expand the other term T 3 , we get\nT 3 = E r\u22121 1 S i\u2208S c \u2212 c i + \u2207f i (x) 2 = 1 N N j=1 c \u2212 c i + \u2207f i (x) 2 = 1 N N j=1 c \u2212 c i + \u2207f i (x ) + \u2207f i (x) \u2212 \u2207f i (x ) 2 \u2264 3 c 2 + 3 N N j=1 c i \u2212 \u2207f i (x ) 2 + 3 N N j=1 \u2207f i (x) \u2212 \u2207f i (x ) 2 \u2264 6 N N j=1 c i \u2212 \u2207f i (x ) 2 + 3 N N j=1 \u2207f i (x) \u2212 \u2207f i (x ) 2 \u2264 6 N N j=1 c i \u2212 \u2207f i (x ) 2 + 6\u03b2(f (x) \u2212 f (x )) .\nThe last step used the smoothness of f i . Combining the bounds on T 2 and T 3 in the original inequality and using a = 1\nK\u22121 gives 1 N i E y i,k \u2212 x 2 \u2264 (1 + 1 K\u22121 ) N i E y i,k\u22121 \u2212 x 2 + \u03b7 2 l \u03c3 2 + 6\u03b7 2 l K\u03b2(f (x) \u2212 f (x )) + 6K\u03b7 2 l N i E c i \u2212 \u2207f i (x ) 2 .\nRecall that with the choice of c i in (19), the variance of c i is less than \u03c3 2 K . Separating its mean and variance gives\n1 N i E y i,k \u2212 x 2 \u2264 1 + 1 K \u2212 1 1 N i E y i,k\u22121 \u2212 x 2 + 7\u03b7 2 l \u03c3 2 + 6\u03b7 2 l K\u03b2(f (x) \u2212 f (x )) + 6K\u03b7 2 l N i E[c i ] \u2212 \u2207f i (x ) 2 (25)\nUnrolling the recursion (25), we get the following for any k \u2208 {1, . . . , K}\n1 N i E y i,k \u2212 x 2 \u2264 6K\u03b2\u03b7 2 l (f (x) \u2212 f (x )) + 6K\u03b7 2 l C r\u22121 + 7\u03b2\u03b7 2 l \u03c3 2 k\u22121 \u03c4 =0 (1 + 1 K\u22121 ) \u03c4 \u2264 6K\u03b2\u03b7 2 l (f (x) \u2212 f (x )) + 6K\u03b7 2 l C r\u22121 + 7\u03b2\u03b7 2 l \u03c3 2 (K \u2212 1)((1 + 1 K\u22121 ) K \u2212 1) \u2264 6K\u03b2\u03b7 2 l (f (x) \u2212 f (x )) + 6K\u03b7 2 l C r\u22121 + 7\u03b2\u03b7 2 l \u03c3 2 3K \u2264 18K 2 \u03b2\u03b7 2 l (f (x) \u2212 f (x )) + 18K 2 \u03b7 2 l C r\u22121 + 21K\u03b2\u03b7 2 l \u03c3 2 . The inequality (K \u2212 1)((1 + 1 K\u22121 ) K \u2212 1) \u2264 3K can be verified for K = 2, 3 manually. For K \u2265 4, (K \u2212 1)((1 + 1 K\u22121 ) K \u2212 1) < K(exp( K K\u22121 ) \u2212 1) \u2264 K(exp( 4 3 ) \u2212 1) < 3K\n. Again averaging over k and multiplying by 3\u03b2 yields\n3\u03b2E r \u2264 54K 2 \u03b2 2 \u03b7 2 l (f (x) \u2212 f (x )) + 54K 2 \u03b2\u03b7 2 l C r\u22121 + 63\u03b2K\u03b7 2 l \u03c3 2 = 1 \u03b7 2 g 54\u03b2 2\u03b72 (f (x) \u2212 f (x )) + 54\u03b2\u03b7 2 C r\u22121 + 63\u03b2\u03b7 2 \u03c3 2 K \u2264 1 \u03b7 2 g 1 25 (f (x) \u2212 f (x )) + 2 3\u03b7 C r\u22121 +\u03b7 \u03c3 2 K .\nThe equality follows from the definition\u03b7 = K\u03b7 l \u03b7 g , and the final inequality uses the bound that\u03b7 \u2264 1 81\u03b2 .\n\nProgress in one round. Now that we have a bound on all errors, we can describe our progress. Lemma 14. Suppose assumptions A3-A5 are true. Then the following holds for any step-sizes satisfying \u03b7 g \u2265 1, \u03b7 l \u2264 min 1 81\u03b2K\u03b7g , S 15\u00b5N K\u03b7g , and effective step-size\u03b7 := K\u03b7 g \u03b7 l\nE x r \u2212 x 2 + 9N\u03b7 2 S C r \u2264 (1\u2212 \u00b5\u03b7 2 ) E x r\u22121 \u2212 x 2 + 9N\u03b7 2 S C r\u22121 \u2212\u03b7(E[f (x r\u22121 )]\u2212f (x ))+ 12\u03b7 2 KS (1+ S \u03b7 2 g )\u03c3 2 .\nProof. Starting from our server update equation,\n\u2206x = \u2212\u03b7 KS k,i\u2208S (g i (y i,k\u22121 ) + c \u2212 c i ), and E[\u2206x] = \u2212\u03b7 KN k,i g i (y i,k\u22121 ) .\nWe can then apply Lemma 11 to bound the second moment of the server update as\nE r\u22121 x + \u2206x \u2212 x 2 = E r\u22121 x \u2212 x 2 \u2212 2\u03b7 KS E r\u22121 k,i\u2208S \u2207f i (y i,k\u22121 ), x \u2212 x + E r\u22121 \u2206x 2 \u2264 2\u03b7 KS E r\u22121 k,i\u2208S \u2207f i (y i,k\u22121 ), x \u2212 x T4 + E r\u22121 x \u2212 x 2 + 8\u03b2\u03b7 2 (E[f (x r\u22121 )] \u2212 f (x )) + 8\u03b7 2 C r\u22121 + 4\u03b7 2 \u03b2 2 E + 12\u03b7 2 \u03c3 2 KS .\nThe term T 4 can be bounded by using perturbed strong-convexity (Lemma 5) with h = f i , x = y i,k\u22121 , y = x , and z = x to get\nE[T 4 ] = 2\u03b7 KS E k,i\u2208S \u2207f i (y i,k\u22121 ), x \u2212 x \u2264 2\u03b7 KS E k,i\u2208S f i (x ) \u2212 f i (x) + \u03b2 y i,k\u22121 \u2212 x 2 \u2212 \u00b5 4 x \u2212 x 2 = \u22122\u03b7 E f (x) \u2212 f (x ) + \u00b5 4 x \u2212 x 2 + 2\u03b2\u03b7E .\nPlugging T 4 back, we can further simplify the expression to get\nE x + \u2206x \u2212 x 2 \u2264 E x \u2212 x 2 \u2212 2\u03b7 f (x) \u2212 f (x ) + \u00b5 4 x \u2212 x 2 + 2\u03b2\u03b7E + 12\u03b7 2 \u03c3 2 KS + 8\u03b2\u03b7 2 (E[f (x r\u22121 )] \u2212 f (x )) + 8\u03b7 2 C r\u22121 + 4\u03b7 2 \u03b2 2 E = (1 \u2212 \u00b5\u03b7 2 ) x \u2212 x 2 + (8\u03b2\u03b7 2 \u2212 2\u03b7)(f (x) \u2212 f (x )) + 12\u03b7 2 \u03c3 2 KS + (2\u03b2\u03b7 + 4\u03b2 2\u03b72 )E + 8\u03b7 2 C r\u22121 .\nWe can use Lemma 12 (scaled by 9\u03b7 2 N S ) to bound the control-lag\n9\u03b7 2 N S C r \u2264 (1 \u2212 \u00b5\u03b7 2 )9\u03b7 2 N S C r\u22121 + 9( \u00b5\u03b7N 2S \u2212 1)\u03b7 2 C r\u22121 + 9\u03b7 2 4\u03b2(E[f (x r\u22121 )] \u2212 f (x )\n) + 2\u03b2 2 E Now recall that Lemma 13 bounds the client-drift:\n3\u03b2\u03b7E r \u2264 2\u03b7 2 3 C r\u22121 +\u03b7 25\u03b7 2 g (E[f (x r\u22121 )] \u2212 f (x )) +\u03b7 2 K\u03b7 2 g \u03c3 2 .\nAdding all three inequalities together,\nE x + \u2206x \u2212 x 2 + 9\u03b7 2 N C r S \u2264 (1 \u2212 \u00b5\u03b7 2 ) E x \u2212 x 2 + 9\u03b7 2 N C r\u22121 S + (44\u03b2\u03b7 2 \u2212 49 25\u03b7 )(f (x) \u2212 f (x )) + 12\u03b7 2 \u03c3 2 KS (1 + S \u03b7 2 g ) + (22\u03b2 2\u03b72 \u2212 \u03b2\u03b7)E + ( 9\u00b5\u03b7N 2S \u2212 1 3 )\u03b7 2 C r\u22121\nFinally, the lemma follows from noting that\u03b7 \u2264 1 81\u03b2 implies 44\u03b2 2\u03b72 \u2264 24 25\u03b2 and\u03b7 \u2264 S 15\u00b5N implies 9\u00b5\u03b7N 2S \u2264 1 3 .\n\nThe final rate follows simply by unrolling the recursive bound in Lemma 14 using Lemma 1 in the strongly-convex case and Lemma 2 if \u00b5 = 0. Also note that if c 0 i = g i (x 0 ), then\u03b7 N S C 0 can be bounded in terms of function sub-optimality F .\n\n\nE.2. Convergence of SCAFFOLD for non-convex functions (Theorem III)\n\nWe now analyze the most general case of SCAFFOLD with option II on functions which are potentially non-convex. Just as in the non-convex proof, we will first bound the variance of the server update in Lemma 15, the change in control lag in Lemma 16 and finally we bound the client-drift in Lemma 17. Combining these three together gives us the progress made in one round in Lemma 18. The final rate is derived from the progress made using Lemma 2.\n\nAdditional notation. Recall that in round r, we update the control variate as (19)\nc r i = 1 K K k=1 g i (y r i,k\u22121 ) if i \u2208 S r , c r\u22121 i otherwise .\nWe introduce the following notation to keep track of the 'lag' in the update of the control variate: define a sequence of parameters {\u03b1 r\u22121 i,k\u22121 } such that for any i \u2208 [N ] and k \u2208 [K] we have \u03b1 0 i,k\u22121 := x 0 and for r \u2265 1,\n\u03b1 r i,k\u22121 := y r i,k\u22121 if i \u2208 S r , \u03b1 r\u22121 i,k\u22121 otherwise .(26)\nBy the update rule for control variates (19) and the definition of {\u03b1 r\u22121 i,k\u22121 } above, the following property always holds:\nc r i = 1 K K k=1 g i (\u03b1 r i,k\u22121 ) .\nWe can then define the following \u039e r to be the error in control variate for round r:\n\u039e r := 1 KN K k=1 N i=1 E \u03b1 r i,k\u22121 \u2212 x r 2 .(27)\nAlso recall the closely related definition of client drift caused by local updates:\nE r := 1 KN K k=1 N i=1 E[ y r i,k \u2212 x r\u22121 2 ] .\nVariance of server update. Let us analyze how the control variates effect the variance of the aggregate server update.\n\nLemma 15. For updates (18)-(21)and assumptions A4 and A5, the following holds true for any\u03b7 := \u03b7 l \u03b7 g K \u2208 [0, 1/\u03b2]:\nE E r\u22121 [x r ] \u2212 x r\u22121 2 \u2264 2\u03b7 2 \u03b2 2 E r + 2\u03b7 2 E \u2207f (x r\u22121 ) 2 , and E x r \u2212 x r\u22121 2 \u2264 4\u03b7 2 \u03b2 2 E r + 8\u03b7 2 \u03b2 2 \u039e r\u22121 + 4\u03b7 2 E \u2207f (x r\u22121 ) 2 + 9\u03b7 2 \u03c3 2 KS .\nProof. Recall that that the server update satisfies\nE[\u2206x] = \u2212\u03b7 KN k,i E[g i (y i,k\u22121 )] .\nFrom the definition of \u03b1 r\u22121 i,k\u22121 and dropping the superscript everywhere we have\n\u2206x = \u2212\u03b7 KS k,i\u2208S (g i (y i,k\u22121 ) + c \u2212 c i ) where c i = 1 K k g i (\u03b1 i,k\u22121 ) .\n\nSCAFFOLD: Stochastic Controlled Averaging for Federated Learning\n\nTaking norm on both sides and separating mean and variance, we proceed as\nE \u2206x 2 = E \u2212\u03b7 KS k,i\u2208S (g i (y i,k\u22121 ) \u2212 g i (\u03b1 i,k\u22121 ) + c \u2212 c i ) 2 \u2264 E \u2212\u03b7 KS k,i\u2208S (\u2207f i (y i,k\u22121 ) + E[c] \u2212 E[c i ]) 2 + 9\u03b7 2 \u03c3 2 KS \u2264 E \u03b7 2 KS k,i\u2208S \u2207f i (y i,k\u22121 ) + E[c] \u2212 E[c i ] 2 + 9\u03b7 2 \u03c3 2 KS =\u03b7 2 KN k,i E (\u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x)) + (E[c] \u2212 \u2207f (x)) + \u2207f (x) \u2212 (E[c i ] \u2212 \u2207f i (x)) 2 + 9\u03b7 2 \u03c3 2 KS \u2264 4\u03b7 2 KN k,i E \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 + 8\u03b7 2 KN k,i E \u2207f i (\u03b1 i,k\u22121 ) \u2212 \u2207f i (x) 2 + 4\u03b7 2 E \u2207f (x) 2 + 9\u03b7 2 \u03c3 2 KS \u2264 4\u03b7 2 \u03b2 2 E r + 8\u03b2 2\u03b72 \u039e r\u22121 + 4\u03b7 2 E \u2207f (x) 2 + 9\u03b7 2 \u03c3 2 KS .\nIn the first inequality, note that the three random variables-1 KS k,i\u2208S g i (y i,k ), 1 S i\u2208S c i , and c-may not be independent but each have variance smaller than \u03c3 2 KS and so we can apply Lemma 4. The rest of the inequalities follow from repeated applications of the relaxed triangle inequality, \u03b2-Lipschitzness of f i , and the definition of \u039e r\u22121 (27). This proves the second statement. The first statement follows from our expression of E r\u22121 [\u2206x] and similar computations.\n\nLag in the control variates. We now analyze the 'lag' in the control variates due to us sampling only a small subset of clients each round. Because we cannot rely on convexity anymore but only on the Lipschitzness of the gradients, the control-lag increases faster in the non-convex case.\n\nLemma 16. For updates (18)-(21) and assumptions A4, A5, the following holds true for any\u03b7 \u2264 1 24\u03b2 ( S N ) \u03b1 for \u03b1 \u2208 [ 1 2 , 1] where\u03b7 := \u03b7 l \u03b7 g K:\n\u039e r \u2264 (1 \u2212 17S 36N )\u039e r\u22121 + 1 48\u03b2 2 ( S N ) 2\u03b1\u22121 \u2207f (x r\u22121 ) 2 + 97 48 ( S N ) 2\u03b1\u22121 E r + ( S N \u03b2 2 ) \u03c3 2 32KS .\nProof. The proof proceeds similar to that of Lemma 12 except that we cannot rely on convexity. Recall that after round r, the definition of \u03b1 r i,k\u22121 (26) implies that\nE S r [\u03b1 r i,k\u22121 ] = (1 \u2212 S N )\u03b1 r\u22121 i,k\u22121 + S N y r i,k\u22121 .\nPlugging the above expression in the definition of \u039e r we get\n\u039e r = 1 KN i,k E \u03b1 r i,k\u22121 \u2212 x r 2 = 1 \u2212 S N \u00b7 1 KN i E \u03b1 r\u22121 i,k\u22121 \u2212 x r 2 T5 + S N \u00b7 1 KN k,i E y r i,k\u22121 \u2212 x r 2 T6 .\nWe can expand the second term T 6 with the relaxed triangle inequality to claim T 6 \u2264 2(E r + E \u2206x r 2 ) .\n\nWe will expand the first term T 5 to claim for a constant b \u2265 0 to be chosen later\nT 5 = 1 KN i E( \u03b1 r\u22121 i,k\u22121 \u2212 x r\u22121 2 + \u2206x r 2 + E r\u22121 \u2206x r , \u03b1 r\u22121 i,k\u22121 \u2212 x r\u22121 ) \u2264 1 KN i E( \u03b1 r\u22121 i,k\u22121 \u2212 x r\u22121 2 + \u2206x r 2 + 1 b E r\u22121 [\u2206x r ] 2 + b \u03b1 r\u22121 i,k\u22121 \u2212 x r\u22121 2 )\nwhere we used Young's inequality which holds for any b \u2265 0. Combining the bounds for T 5 and T 6 ,\n\u039e r \u2264 1 \u2212 S N (1 + b)\u039e r\u22121 + 2 S N E r + 2 E \u2206x r 2 + 1 b E E r\u22121 [\u2206x r ] 2 \u2264 ( 1 \u2212 S N (1 + b) + 16\u03b7 2 \u03b2 2 )\u039e r\u22121 + ( 2S N + 8\u03b7 2 \u03b2 2 + 2 1 b\u03b7 2 \u03b2 2 )E r + (8 + 2 1 b )\u03b7 2 E \u2207f (x) 2 ) + 18\u03b7 2 \u03c3 2 KS\nThe last inequality applied Lemma 15. Verify that with choice of\nb = S 2(N \u2212S) , we have 1 \u2212 S N (1 + b) \u2264 (1 \u2212 S 2N ) and 1 b \u2264 2N S .\nPlugging these values along with the bound on the step-size 16\u03b2 2\u03b72 \u2264 1 36 ( S N ) 2\u03b1 \u2264 S 36N completes the lemma.\n\nBounding the drift. We will next bound the client drift E r . For this, convexity is not crucial and we will recover a very similar result to Lemma 13 only use the Lipschitzness of the gradient. Lemma 17. Suppose our step-sizes satisfy \u03b7 l \u2264 1 24\u03b2K\u03b7g and f i satisfies assumptions A4-A5. Then, for any global \u03b7 g \u2265 1 we can bound the drift as\n5 3 \u03b2 2\u03b7 E r \u2264 5 3 \u03b2 3\u03b72 \u039e r\u22121 +\u03b7 24\u03b7 2 g E \u2207f (x r\u22121 ) 2 +\u03b7 2 \u03b2 4K\u03b7 2 g \u03c3 2 .\nProof. First, observe that if K = 1, E r = 0 since y i,0 = x for all i \u2208 [N ] and that \u039e r\u22121 and the right hand side are both positive. Thus the Lemma is trivially true if K = 1 and we will henceforth assume K \u2265 2. Starting from the update rule\n(18) for i \u2208 [N ] and k \u2208 [K] E y i,k \u2212 x 2 = E y i,k\u22121 \u2212 \u03b7 l (g i (y i,k\u22121 ) + c \u2212 c i ) \u2212 x 2 \u2264 E y i,k\u22121 \u2212 \u03b7 l (\u2207f i (y i,k\u22121 ) + c \u2212 c i ) \u2212 x 2 + \u03b7 2 l \u03c3 2 \u2264 (1 + 1 K\u22121 ) E y i,k\u22121 \u2212 x 2 + K\u03b7 2 l E \u2207f i (y i,k\u22121 ) + c \u2212 c i 2 + \u03b7 2 l \u03c3 2 = (1 + 1 K\u22121 ) E y i,k\u22121 \u2212 x 2 + \u03b7 2 l \u03c3 2 + K\u03b7 2 l E \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) + (c \u2212 \u2207f (x)) + \u2207f (x) \u2212 (c i \u2212 \u2207f i (x) 2 \u2264 (1 + 1 K\u22121 ) E y i,k\u22121 \u2212 x 2 + 4K\u03b7 2 l E \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 + \u03b7 2 l \u03c3 2 + 4K\u03b7 2 l E c \u2212 \u2207f (x) 2 + 4K\u03b7 2 l E \u2207f (x) 2 + 4K\u03b7 2 l E c i \u2212 \u2207f i (x) 2 \u2264 (1 + 1 K\u22121 + 4K\u03b2 2 \u03b7 2 l ) E y i,k\u22121 \u2212 x 2 + \u03b7 2 l \u03c3 2 + 4K\u03b7 2 l E \u2207f (x) 2 + 4K\u03b7 2 l E c \u2212 \u2207f (x) 2 + 4K\u03b7 2 l E c i \u2212 \u2207f i (x) 2\nThe inequalities above follow from repeated application of the relaxed triangle inequalities and the \u03b2-Lipschitzness of f i . Averaging the above over i, the definition of c = 1 N i c i and \u039e r\u22121 (27) gives\n1 N i E y i,k \u2212 x 2 \u2264 (1 + 1 K\u22121 + 4K\u03b2 2 \u03b7 2 l ) 1 N i E y i,k\u22121 \u2212 x 2 + \u03b7 2 l \u03c3 2 + 4K\u03b7 2 l E \u2207f (x) 2 + 8K\u03b7 2 l \u03b2 2 \u039e r\u22121 \u2264 \u03b7 2 l \u03c3 2 + 4K\u03b7 2 l E \u2207f (x) 2 + 8K\u03b7 2 l \u03b2 2 \u039e r\u22121 k\u22121 \u03c4 =0 (1 + 1 K\u22121 + 4K\u03b2 2 \u03b7 2 l ) \u03c4 = \u03b7 2 \u03c3 2 K 2 \u03b7 2 g + 4\u03b7 2 K\u03b7 2 g E \u2207f (x) 2 + 8\u03b7 2 \u03b2 2 K\u03b7 2 g \u039e r\u22121 k\u22121 \u03c4 =0 (1 + 1 K\u22121 + 4\u03b2 2\u03b72 K\u03b7 2 g ) \u03c4 \u2264 \u03b7\u03c3 2 24\u03b2K 2 \u03b7 2 g + 1 144\u03b2 2 K\u03b7 2 g E \u2207f (x) 2 +\u03b7 \u03b2 3K\u03b7 2 g \u039e r\u22121 3K .\nThe last inequality used the bound on the step-size \u03b2\u03b7 \u2264 1 24 . Averaging over k and multiplying both sides by 5 3 \u03b2 2\u03b7 yields the lemma statement.\n\nProgress made in each round. Given that we can bound all sources of error, we can finally prove the progress made in each round.\n\nLemma 18. Suppose the updates (18)-(21) satisfy assumptions A4-A5. For any effective step-size\u03b7 := K\u03b7 g \u03b7 l satisfying\n\u03b7 \u2264 1 24\u03b2 S N 2 3 , E[f (x r )] + 12\u03b2 3\u03b72 N S \u039e r \u2264 E[f (x r\u22121 )] + 12\u03b2 3\u03b72 N S \u039e r\u22121 + 5\u03b2\u03b7 2 \u03c3 2 KS (1 + S \u03b7 2 g ) \u2212\u03b7 14 E \u2207f (x r\u22121 ) 2 .\nProof. Starting from the smoothness of f and taking conditional expectation gives\nE r\u22121 [f (x + \u2206x)] \u2264 f (x) + \u2207f (x), E r\u22121 [\u2206x] + \u03b2 2 E r\u22121 \u2206x 2 .\nWe as usual dropped the superscript everywhere. Recall that the server update can be written as\n\u2206x = \u2212\u03b7 KS k,i\u2208S (g i (y i,k\u22121 ) + c \u2212 c i ), and E S [\u2206x] = \u2212\u03b7 KN k,i g i (y i,k\u22121 ) .\n\nSubstituting this in the previous inequality and applying Lemma 15 to bound E[ \u2206x 2 ]\n\ngives\nE[f (x + \u2206x)] \u2212 f (x) \u2264 \u2212\u03b7 KN k,i \u2207f (x), E[\u2207f i (y i,k\u22121 )] + \u03b2 2 E \u2206x 2 \u2264 \u2212\u03b7 KN k,i \u2207f (x), E[\u2207f i (y i,k\u22121 )] + 2\u03b7 2 \u03b2 3 E r + 4\u03b7 2 \u03b2 3 \u039e r\u22121 + 2\u03b2\u03b7 2 E \u2207f (x) 2 + 9\u03b2\u03b7 2 \u03c3 2 2KS \u2264 \u2212\u03b7 2 \u2207f (x) 2 +\u03b7 2 i,k E 1 KN i,k \u2207f i (y i,k\u22121 ) \u2212 \u2207f (x) 2 + 2\u03b7 2 \u03b2 3 E r + 4\u03b7 2 \u03b2 3 \u039e r\u22121 + 2\u03b2\u03b7 2 E \u2207f (x) 2 + 9\u03b2\u03b7 2 \u03c3 2 2KS \u2264 \u2212\u03b7 2 \u2207f (x) 2 +\u03b7 2KN i,k E \u2207f i (y i,k\u22121 ) \u2212 \u2207f i (x) 2 + 2\u03b7 2 \u03b2 3 E r + 4\u03b7 2 \u03b2 3 \u039e r\u22121 + 2\u03b2\u03b7 2 E \u2207f (x) 2 + 9\u03b2\u03b7 2 \u03c3 2 2KS \u2264 \u2212(\u03b7 2 \u2212 2\u03b2\u03b7 2 ) \u2207f (x) 2 + (\u03b7 2 + 2\u03b2\u03b7 2 )\u03b2 2 E r + 4\u03b2 3\u03b72 \u039e r\u22121 + 9\u03b2\u03b7 2 \u03c3 2 2KS .\nThe third inequality follows from the observation that \u2212ab = 1\n2 ((b \u2212 a) 2 \u2212 a 2 ) \u2212 1 2 b 2 \u2264 1 2 ((b \u2212 a) 2 \u2212 a 2 )\nfor any a, b \u2208 R, and the last from the \u03b2-Lipschitzness of f i . Now we use Lemma 16 to bound \u039e r as\n12\u03b2 3\u03b72 N S \u039e r \u2264 12\u03b2 3\u03b72 N S (1 \u2212 17S 36N )\u039e r\u22121 + 1 48\u03b2 2 ( S N ) 2\u03b1\u22121 \u2207f (x r\u22121 ) 2 + 97 48 ( S N ) 2\u03b1\u22121 E r + ( S N \u03b2 2 ) \u03c3 2 32KS = 12\u03b2 3\u03b72 N S \u039e r\u22121 \u2212 17 3 \u03b2 3\u03b72 \u039e r\u22121 + 1 4 \u03b2\u03b7 2 ( N S ) 2\u22122\u03b1 \u2207f (x) 2 + 97 4 \u03b2 3\u03b72 ( N S ) 2\u22122\u03b1 E r + 3\u03b2\u03b7 2 \u03c3 2 8KS .\nAlso recall that Lemma 17 states that 5 3 \u03b2 2\u03b7 E r \u2264 5 3 \u03b2 3\u03b72 \u039e r\u22121 +\u03b7 24\u03b7 2 g E \u2207f (x r\u22121 ) 2 +\u03b7 2 \u03b2 4K\u03b7 2 g \u03c3 2 .\n\n\nSCAFFOLD: Stochastic Controlled Averaging for Federated Learning\n\nAdding these bounds on \u039e r and E r to that of E[f (x + \u2206x)] gives\n(E[f (x + \u2206x)] + 12\u03b2 3\u03b72 N S \u039e r ) \u2264 (E[f (x)] + 12\u03b2 3\u03b72 N S \u039e r\u22121 ) + ( 5 3 \u2212 17 3 )\u03b2 3\u03b72 \u039e r\u22121 \u2212 (\u03b7 2 \u2212 2\u03b2\u03b7 2 \u2212 1 4 \u03b2\u03b7 2 ( N S ) 2\u22122\u03b1 ) \u2207f (x) 2 + (\u03b7 2 \u2212 5\u03b7 3 + 2\u03b2\u03b7 2 + 97 4 \u03b2\u03b7 2 ( N S ) 2\u22122\u03b1 )\u03b2 2 E r + 39\u03b2\u03b7 2 \u03c3 2 8KS (1 + S \u03b7 2 g )\n.\n\nBy our choice of \u03b1 = 2 3 and plugging in the bound on step-size \u03b2\u03b7( N S ) 2\u22122\u03b1 \u2264 1 24 proves the lemma.\n\nThe non-convex rate of convergence now follows by unrolling the recursion in Lemma 18 and selecting an appropriate step-size\u03b7 as in Lemma 2. Finally note that if we initialize c 0 i = g i (x 0 ) then we have \u039e 0 = 0.\n1 2 x \u2212 x i 2 A , for all x , for some {x i } and x , A := 1 N N i=1 A i .\nWe also assume that A is a symmetric matrix though this requirement is easily relaxed. Note that this implies f (x ) = 0 and that \u2207f i (x) = A(x \u2212 x i ). If {f i } are additionally convex, we have that x i is the optimum of f i and x the optimum of f . However, this is not necessarily true in general.\n\nWe will also focus on a simplified version of SCAFFOLD where in each round r, client i performs the following update starting from y r i,0 \u2190 x r\u22121 :\ny r i,k = y r i,k\u22121 \u2212 \u03b7(g i (y r i,k\u22121 ) + \u2207f (x r\u22121 ) \u2212 \u2207f i (x r\u22121 )) , i.e. E r\u22121,k\u22121 [y r i,k ] = y r i,k\u22121 \u2212 \u03b7A(y r i,k\u22121 \u2212 x ) \u2212 \u03b7(A i \u2212 A)(y r i,k\u22121 \u2212 x r\u22121 )) ,(28)\nwhere the second part is specialized to quadratics and the expectation is conditioned over everything before current step k of round r. At the end of each round, as before, x r = 1 N N i=1 y r i,K . The final output of the algorithm is chosen using probabilities {p r k } asx R = x r k with probability p r k , where x r k :=\n1 N N i=1 y r i,k .(29)\nNote that we are now possibly outputting iterates computed within a single round and that x r = x r K . Beyond this, the update above differs from our usual SCAFFOLD in two key aspects: a) it uses gradients computed at x r\u22121 as control variates instead of those at either x r\u22122 (as in option I) or y r\u22121 i,k (as in option II), and b) it uses full batch gradients to compute its control variates instead of stochastic gradients. The first issue is easy to fix and our proof extends to using both option I or option II using techniques in Section E. The second issue is more technical-using stochastic gradients for control variates couples the randomness across the clients in making the local-updates biased. While it may be possible to get around this (cf. (Lei & Jordan, 2017;Nguyen et al., 2017;Tran-Dinh et al., 2019)), we will not attempt to do so in this work. Note that if K local update steps typically represents running multiple epochs on each client. Hence one additional epoch to compute the control variate \u2207f i (x) does not significantly add to the cost.\n\nFinally, we define the following sequence of positive numbers for notation convenience:\n\u03be r i,k := E r\u22121 [f (y r i,k )] \u2212 f (x ) + \u03b4(1 + 1 K ) K\u2212k E r\u22121 y r i,k \u2212 x r\u22121 2 , and \u03be r i,k := [f (E r\u22121 [y r i,k ])] \u2212 f (x ) + \u03b4(1 + 1 K ) K\u2212k E r\u22121,k\u22121 E r\u22121 [y r i,k ] \u2212 x r\u22121 2 .\nObserve that for k = 0, \u03be r i,0 =\u03be r i,0 = f (x r\u22121 ) \u2212 f (x ).\n\n\nF.2. Lemmas tracking errors\n\nEffect of averaging. We see how averaging can reduce variance. A similar argument was used in the special case of one-shot averaging in (Zhang et al., 2013b). Lemma 19. Suppose {f i } are quadratic functions and assumption A4 is satisfied. Then let x r k and y r i,k be vectors in step k and round r generated using (28)-(29). Then,\nE r\u22121 \u2207f (x r k ) 2 \u2264 1 N N i=1 \u2207f (E r\u22121 [y r i,k ]) 2 + 1 N 2 N i=1 E r\u22121 [ \u2207f (y r i,k ) 2 ] .\nProof. Observe that the variables {y i,k \u2212 x} are independent of each other (the only source of randomness is the local gradient computations). The rest of the proof is exactly that of Lemma 4. Dropping superscripts everywhere,\nE r\u22121 A(x r k \u2212 x ) 2 = E r\u22121 1 N i A(y i,k \u2212 x ) 2 = E r\u22121 1 N i A(E r\u22121 [y i,k ] \u2212 x ) 2 + E r\u22121 1 N i A(E r\u22121 [y i,k ] \u2212 y i,k ) 2 = E r\u22121 1 N i A(E r\u22121 [y i,k ] \u2212 x ) 2 + 1 N 2 i E r\u22121 A(E r\u22121 [y i,k ] \u2212 y i,k ) 2 = E r\u22121 1 N i A(E r\u22121 [y i,k ] \u2212 x ) 2 + 1 N 2 i E r\u22121 A(y i,k \u2212 x \u2212 E r\u22121 [y i,k \u2212 x ]) 2 \u2264 E r\u22121 1 N i A(E r\u22121 [y i,k ] \u2212 x ) 2 + 1 N 2 i E r\u22121 A(y i,k \u2212 x ) 2 .\nThe third equality was because {y i,k } are independent of each other conditioned on everything before round r.\n\nWe next see the effect of averaging on function values. Lemma 20. Suppose that f is \u03b4 general-convex, then we have:\n1 N n i=1 \u03be r i,k \u2265 E r\u22121 [f (x r k )] \u2212 f (x ) , and 1 N n i=1\u03be r i,k \u2265 f (E r\u22121 [x r k ]) \u2212 f (x ) .\nProof. Since f is \u03b4-general convex, it follows that the function f (z) + \u03b4(1 + 1 K ) K\u2212k z \u2212 x 2 2 is convex in z for any k \u2208 [K]. The lemma now follows directly from using convexity and the definition of x r k = 1 N y r i,k .\n\nBounding drift of one client. We see how the client drift of SCAFFOLD depends on \u03b4.\n\nLemma 21. For the update (28), assuming (A2) and that {f i } are quadratics, the following holds for any \u03b7 \u2264 1 21\u03b4K E r\u22121,k\u22121 y r i,k \u2212 x r\u22121 2 \u2264 (1 + 1 2K ) y r i,k\u22121 \u2212 x r\u22121 2 + 7K\u03b7 2 \u2207f (y r i,k\u22121 ) 2 + \u03b7 2 \u03c3 2 .\n\nProof. Starting from the update step (28)\nE r\u22121,k\u22121 y + i \u2212 x 2 \u2264 y i \u2212 x \u2212 \u03b7A(y i \u2212 x ) \u2212 \u03b7(A i \u2212 A)(y i \u2212 x) 2 + \u03b7 2 \u03c3 2 \u2264 (1 + 1 7(K\u22121) ) (I \u2212 \u03b7(A i \u2212 A)(y i \u2212 x) 2 + 7K\u03b7 2 A(y i \u2212 x ) 2 + \u03b7 2 \u03c3 2 .\nNote that if K = 1, then the first inequality directly proves the lemma. For the second inequality, we assumed K \u2265 2 and then applied our relaxed triangle inequality. By assumption A2, we have the following for \u03b7\u03b4 \u2264 1\n(I \u2212 \u03b7(A i \u2212 A)) 2 = I \u2212 \u03b7(A i \u2212 A) 2 \u2264 (1 + \u03b7\u03b4) 2 \u2264 1 + 3\u03b7\u03b4 .\nUsing the bound on the step-size \u03b7 \u2264 1 21\u03b4K gives E r\u22121,k\u22121 y + i \u2212 x 2 \u2264 (1 + 1 7K )(1 + 1 7(K\u22121) ) y i \u2212 x 2 + 7K\u03b7 2 A(y i \u2212 x ) 2 + \u03b7 2 \u03c3 2\n\nSimple computations now give the Lemma statement for all K \u2265 1.\n\nTracking the variance. We will see how to bound the variance of the output. (1 \u2212 \u00b5\u03b7) k\u22121 .\n\nProof. We can rewrite the update step (28) as below:\ny i,k = y i,k\u22121 \u2212 \u03b7(A i (y i,k\u22121 \u2212 x ) + (A \u2212 A i )(x \u2212 x )) \u2212 \u03b7\u03b6 i,k ,\nwhere by the bounded variance assumption A4, \u03b6 i,k is a random variable satisfying E k\u22121,r\u22121 [\u03b6 i,k ] = 0 and E k\u22121,r\u22121 \u03b6 i,k 2 \u2264 \u03c3 2 . Subtracting x from both sides and unrolling the recursion gives\ny i,K \u2212 x = (I \u2212 \u03b7A i )(y i,K\u22121 \u2212 x ) \u2212 \u03b7((A \u2212 A i )(x \u2212 x ) + \u03b6 i,K ) = (I \u2212 \u03b7A i ) K (x \u2212 x ) \u2212 K k=1 \u03b7(I \u2212 \u03b7A i ) k\u22121 (\u03b6 i,k + (A \u2212 A i )(x \u2212 x )) .\nSimilarly, the expected iterate satisfies the same equation without the \u03b6 i,k\nE r\u22121 [y i,K ] \u2212 x = (I \u2212 \u03b7A i ) K (x \u2212 x ) \u2212 K k=1 \u03b7(I \u2212 \u03b7A i ) k\u22121 (A \u2212 A i )(x \u2212 x ) .\nThis implies that the difference satisfies\nE r\u22121 [y i,K ] \u2212 y i,K = \u03b7 K k=1 (I \u2212 \u03b7A i ) k\u22121 \u03b6 i,k .\nAdding this to our previous equation gives the following recursive bound:\nE r\u22121,k\u22121 y + i \u2212 x 2 A + \u03b4(1 + 1 K ) K\u2212k E r\u22121,k\u22121 y + i \u2212 x 2 \u2264 y i \u2212 x 2 A + (1 \u2212 1 5K )\u03b4(1 + 1 K ) K\u2212k+1 y i \u2212 x 2 \u2212 ( 3\u03b7 2 \u2212 2\u03b7 2 \u03b2 \u2212 20\u03b4K\u03b7 2 ) A(y i \u2212 x ) 2 2 + (3\u03b4 + \u03b2)\u03b7 2 \u03c3 2\nThe bound on our step-size \u03b7 \u2264 min( 1 10\u03b2 , 1 22\u03b4K ) implies that 3\u03b7 2 \u2212 2\u03b7 2 \u03b2 \u2212 20\u03b4K\u03b7 2 \u2265 \u03b7 3 and recall that \u03b4 \u2264 2\u03b2. This proves first statement of the lemma for non-strongly convex functions (\u00b5 = 0). If additionally f is strongly-convex with \u00b5 > 0, we have \u03b7 A(y i \u2212 x ) 2 2 \u2265 \u00b5\u03b7 2 y i \u2212 x 2 A + \u03b7 2 A(y i \u2212 x ) 2 2 . This can be used to tighten the inequality as follows\nE r\u22121,k\u22121 y + i \u2212 x 2 A + \u03b4(1 + 1 K ) K\u2212(k\u22121) E r\u22121,k\u22121 y + i \u2212 x 2 \u2264 (1 \u2212 \u00b5\u03b7 6 ) y i \u2212 x 2 A + (1 \u2212 1 5K )\u03b4(1 + 1 K ) K\u2212k+1 y i \u2212 x 2 \u2212 \u03b7 2 A(y i \u2212 x ) 2 2 + 7\u03b2\u03b7 2 \u03c3 2\nIf \u03b7 \u2264 1 \u00b5K , then (1 \u2212 1 5K ) \u2264 (1 \u2212 \u00b5\u03b7 6 ) and we have the strongly-convex version of the first statement.\n\nNow for the second statement, recall that\u03be r i,k \u2265 0 was defined to b\u1ebd\n\u03be r i,k := [f (E r\u22121 [y r i,k ])] \u2212 f (x ) + \u03b4(1 + 1 K ) K\u2212k E r\u22121 E r\u22121 [y r i,k ] \u2212 x r\u22121 2 .\nObserve that for quadratics, E r\u22121 [\u2207f (x)] = \u2207f (E r\u22121 [x]). This implies that the analysis of\u03be r i,k is essentially of a deterministic process with \u03c3 = 0, proving the second statement. It is also straightforward to repeat exactly the above argument to formally verify the second statement.\n\nServer progress in one round. Now we combine the progress made by each client in one step to calculate the server progress.\n\nLemma 24. Suppose (A2), (A5) and (A4) hold, and {f i } are quadratics. Then, the following holds for the update (28) with \u03b7 \u2264 min( 1 10\u03b2 , 1 21\u03b4K , 1 10\u00b5K ) and weights w k := (1 \u2212 \u00b5\u03b7 6 ) 1\u2212k :\n\u03b7 6 K k=1 w k E r\u22121 \u2207f (x r k ) 2 \u2264 (f (E r\u22122 [x r\u22121 ]) \u2212 f ) \u2212 w K (f (E r\u22121 [x r ]) \u2212 f ) + K k=1 w k 8\u03b7 \u03c3 2 N .\nSet \u00b5 = 0 if {f i }s are not strongly-convex (is only general-convex).\n\nProof. Let us do the non-convex (and general convex) case first. By summing over Lemma 23 we have \u03b7 6 K k=1 E r\u22121 \u2207f (y i,k ) 2 \u2264 \u03be r i,0 \u2212 \u03be r i,K + 7K\u03b2\u03b7 2 \u03c3 2 .\n\nA similar result holds with \u03c3 = 0 for E r\u22121 [y i,k ]. Now, using Lemma 19 we have that\n\u03b7 6 K k=1 E r\u22121 \u2207f (x r k ) 2 \u2264 1 N N i=1 (\u03be r i,0 + 1 N \u03be i,0 ) =:\u03b8 r + \u2212 1 N N i=1 (\u03be r i,K + 1 N \u03be i,K ) =:\u03b8 r \u2212 +7K\u03b2\u03b7 2 \u03c3 2 N .\nUsing Lemma 22, we have that\n\u03b8 r + = (1 + 1 N )(f (x r\u22121 ) \u2212 f (x )) \u2264 f (E r\u22121 [x r ]) + 1 N E f (x r ) \u2212 (1 + 1 N )f (x ) + 3K\u03b2 \u03c3 2 N .\nFurther, by Lemma 20, we have that\n\u03b8 r \u2212 \u2265 f (E r\u22121 [x r ]) + 1 N f (x r ) \u2212 (1 + 1 N )f (x ) .\n\nSCAFFOLD: Stochastic Controlled Averaging for Federated Learning\n\nCombining the above gives:\n\u03b7 6 K k=1 E r\u22121 \u2207f (x r k ) 2 \u2264 f (E r\u22122 [x r\u22121 ]) \u2212 f (E r\u22121 [x r ]) + 10\u03b2K \u03c3 2 N .\nproving the second part of the Lemma for weights w k = 1. The proof of strongly convex follows a very similar argument. Unrolling Lemma 23 using weights w k := (1 \u2212 \u00b5\u03b7 6 ) 1\u2212k gives\n\u03b7 6 K k=1 w k E r\u22121 \u2207f (x r k ) 2 \u2264 \u03b8 r + \u2212 w K \u03b8 r \u2212 + K k=1 w k 7\u03b7 \u03c3 2 N .\nAs in the general-convex case, we can use Lemmas 20, 19 and 22 to prove that\n\u03b7 6 K k=1 w k E r\u22121 \u2207f (x r k ) 2 \u2264 (f (E r\u22122 [x r\u22121 ]) \u2212 f ) \u2212 w K (f (E r\u22121 [x r ]) \u2212 f ) + K k=1 w k 8\u03b7 \u03c3 2 N .\nDeriving final rates. The proof of Theorem VIII follows by appropriately unrolling Lemma 24. For general-convex functions, we can simply use Lemma 2 with the probabilities set as p r k = 1 KR . For strongly-convex functions, we use p r k \u221d (1 \u2212 \u00b5\u03b7 6 ) 1\u2212rk and follow the computations in Lemma 1.\n\n\nRelated work. For identical clients, FEDAVG coincides with parallel SGD analyzed by (Zinkevich et al., 2010) who proved asymptotic convergence. Stich (2018) and, more recently Stich & Karimireddy (2019); Patel & Dieuleveut (2019); Khaled et al. (2020), gave a sharper analysis of the same method, under the name of local SGD, also for identical functions. However, there still remains a gap between their upper bounds and the lower bound of Woodworth\n\nFigure 2\n2Figure 2. Update steps of SCAFFOLD on a single client. The local gradient (dashed black) points to x 1 (orange square), but the correction term (c \u2212 ci) (in red) ensures the update moves towards the true optimum x (black square).\n\nFigure 3\n3Figure 3. SGD (dashed black), FedAvg (above), and SCAFFOLD (below) on simulated data. FedAvg gets worse as local steps increases with K = 10 (red) worse than K = 2 (orange). It also gets slower as gradient-dissimilarity (G) increases (to the right). SCAFFOLD significantly improves with more local steps, with K = 10 (blue) faster than K = 2 (light blue) and SGD. Its performance is identical as we vary heterogeneity (G).\n\nFederated learning .\n.As stated earlier, federated learning involves learning a centralized model from distributed client data. This centralized model benefits from all client data and can often result in a beneficial performance e.g. in including next word prediction(Hard et al., 2018; Yang et al., 2018), emoji prediction (Ramaswamy et al., 2019, decoder models(Chen et al., 2019b), vocabulary estimation (Chen et al., 2019a), low latency vehicle-to-vehicle communication(Samarakoon et al., 2018), and predictive models in health(Brisimi et al., 2018). Nevertheless, federated learning raises several types of issues and has been the topic of multiple research efforts studying the issues of generalization and fairness(Mohri et al., 2019; Li et al., 2019a), the design of more efficient communication strategies (Kone\u010dn\u1ef3 et al., 2016b;a; Suresh et al., 2017; Stich et al., 2018; Karimireddy et al., 2019; Basu et al., 2019), the study of lower bounds (Woodworth et al., 2018), differential privacy guarantees (Agarwal et al., 2018), security (Bonawitz et al., 2017), etc. We refer to Kairouz et al. (2019) for an in-depth survey of this area. Convergence of FEDAVG For identical clients, FEDAVG coincides with parallel SGD analyzed by (Zinkevich et al., 2010) who proved asymptotic convergence. Stich (2018) and, more recently Stich & Karimireddy (2019); Patel & Dieuleveut (2019); Khaled et al. (2020), gave a sharper analysis of the same method, under the name of local SGD, also for identical functions. However, there still remains a gap between their upper bounds and the lower bound of Woodworth et al. (2018). The analysis of FEDAVG for heterogeneous clients is more delicate due to the afore-mentioned client-drift, first empirically observed by Zhao et al. (2018). Several analyses bound this drift by assuming bounded gradients (Wang et al., 2019; Yu et al., 2019), or view it as additional noise (Khaled et al., 2020), or assume that the client optima are -close (Li et al., 2018; Haddadpour & Mahdavi, 2019). In a concurrent work,(Liang et al., 2019)  propose to use variance reduction to deal with client heterogeneity but still show rates slower than SGD. We summarize the communication complexities of different methods for heterogeneous clients in\n\n\nVariance reduction. The use of control variates is a classical technique to reduce variance in Monte Carlo sampling methods(cf. (Glasserman, 2013)). In optimization, they were used for finite-sum minimization by SVRG(Johnson &  Zhang, 2013; Zhang et al., 2013a)  and then in SAGA (Defazio et al., 2014) to simplify the linearly convergent method SAG (Schmidt et al., 2017). Numerous variations and extensions of the technique are studied in (Hanzely & Richt\u00e1rik, 2019). Starting from (Reddi et al., 2016a), control variates have also frequently been used to reduce variance in finite-sum non-convex settings (Reddi et al., 2016c; Nguyen et al., 2018; Fang et al., 2018; Tran-Dinh et al., 2019). Further, they are used to obtain linearly converging decentralized algorithms under the guise of 'gradient-tracking' in (Shi et al., 2015; Nedich et al., 2016) and for gradient compression as 'compressed-differences' in (Mishchenko et al., 2019). Our method can be viewed as seeking to remove the 'client-variance' in the gradients across the clients, though there still remains additional stochasticity as in (Kulunchakov & Mairal, 2019), which is important in deep learning (Defazio & Bottou, 2019).Distributed optimization. The problem of client-drift we described is a common phenomenon in distributed optimization. In fact, classic techniques such as ADMM mitigate this drift, though they are not applicable in federated learning. For well structured convex problems, CoCoA uses the dual variable as the control variates, enabling flexible distributed methods(Smith et al., 2018). DANE by (Shamir et al., 2014) obtain a closely related primal only algorithm, which was later accelerated byReddi et al. (2016b)  and recently extended to federated learning(Li et al., 2020). SCAFFOLD can be viewed as an improved version of DANE where a fixed number of (stochastic) gradient steps are used in place of a proximal point update. In a similar spirit, distributed variance reduction techniques have been proposed for the finite-sum case(Lee et al., 2015; Kone\u010dn\u1ef3 et al., 2016a; Cen et al., 2019). However, these methods are restricted to finite-sums and are not applicable to the stochastic setting studied here.\n\nLemma 22 .\n22Consider the update (28) for quadratic {f i } with \u03b7 \u2264 max( 1 2\u03b4K , 1 \u03b2 ). Then, if further (A2), (A5) and (A4) are satisfied, we have E r\u22121 f (x r ) \u2264 f (E r\u22121 [x r ]) + 3K\u03b2 \u03c3 2 N . Further if {f i } are strongly convex satisfying (A3), we have E r\u22121 f (x r ) \u2264 f (E r\u22121 [x r ]) + \u03b2\n\nTable 4 .\n4Communicationrounds to reach 0.45 test accuracy for \nlogistic regression on EMNIST as we vary the number of sam-\npled clients. Number of epochs is kept fixed to 5. SCAFFOLD is \nconsistently faster than FEDAVG. As we decrease the number of \nclients sampled in each round, the increase in number of rounds \nis sub-linear. This slow-down is better for more similar clients. \n\nClients \n0% similarity \n10% similarity \n\nSCAFFOLD 20% \n143 (1.0\u00d7) \n9 \n(1.0\u00d7) \n5% \n290 (2.0\u00d7) \n13 (1.4\u00d7) \n1% \n790 (5.5\u00d7) \n28 (3.1\u00d7) \n\nFEDAVG \n20% \n179 (1.0\u00d7) \n12 (1.0\u00d7) \n5% \n334 (1.9\u00d7) \n17 (1.4\u00d7) \n1% \n1k+ (5.6+\u00d7) 35 (2.9\u00d7) \n\nHessian dissimilarity decrease. \n\n\n\nTable 5 .\n5Besttest accuracy after 1k rounds with 2-layer fully con-\nnected neural network (non-convex) on EMNIST trained with \n5 epochs per round (25 steps) for the local methods, and 20% \nof clients sampled each round. SCAFFOLD has the best ac-\ncuracy and SGD has the least. SCAFFOLD again outperforms \nother methods. SGD is unaffected by similarity, whereas the local \nmethods improve with client similarity. \n\n0% similarity 10% similarity \n\nSGD \n0.766 \n0.764 \nFEDAVG \n0.787 \n0.828 \nSCAFFOLD 0.801 \n0.842 \n\nHowever, much more extensive experiments (beyond cur-\nrent scope) are needed before drawing conclusions. \n\n\n\n\nDefazio, A. and Bottou, L. On the ineffectiveness of variance reduced optimization for deep learning. In Advances in Neural Information Processing Systems, pp. 1753-1763, 2019.Defazio, A., Bach, F., and Lacoste-Julien, S. SAGA: A fast incremental gradient method with support for nonstrongly convex composite objectives. In Advances in neural information processing systems, pp.[1646][1647][1648][1649][1650][1651][1652][1653][1654]  2014.Fang, C., Li, C. J., Lin, Z., and Zhang, T. SPIDER: Nearoptimal non-convex optimization via stochastic pathintegrated differential estimator. In Advances in Neural Information Processing Systems, pp. 689-699, 2018.Glasserman, P. Monte Carlo methods in financial engineering, volume 53. Springer Science & Business Media, 2013. Goyal, P., Doll\u00e1r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch SGD: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Haddadpour, F. and Mahdavi, M. On the convergence of local descent methods in federated learning. arXiv preprint arXiv:1910.14425, 2019. Hanzely, F. and Richt\u00e1rik, P. One method to rule them all: Variance reduction for data, parameters and many new methods. arXiv preprint arXiv:1905.11266, 2019. Hard, A., Rao, K., Mathews, R., Beaufays, F., Augenstein, S., Eichner, H., Kiddon, C., and Ramage, D. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604, 2018. Hsu, T.-M. H., Qi, H., and Brown, M. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019. Iandola, F. N., Moskewicz, M. W., Ashraf, K., and Keutzer, K. Firecaffe: near-linear acceleration of deep neural network training on compute clusters. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2592-2600, 2016. Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019. Karimi, H., Nutini, J., and Schmidt, M. Linear convergence of gradient and proximal-gradient methods under the polyak-\u0142ojasiewicz condition. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 795-811. Springer, 2016. Karimireddy, S. P., Rebjock, Q., Stich, S. U., and Jaggi, M. Error feedback fixes SignSGD and other gradient compression schemes. arXiv preprint arXiv:1901.09847, 2019. Khaled, A., Mishchenko, K., and Richt\u00e1rik, P. Tighter theory for local SGD on indentical and heterogeneous data. In Proceedings of AISTATS, 2020. Kifer, D., Smith, A., and Thakurta, A. Private convex empirical risk minimization and high-dimensional regression. In Conference on Learning Theory, pp. 25-1, 2012. Kone\u010dn\u1ef3, J., McMahan, H. B., Ramage, D., and Richt\u00e1rik, P. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016a. Kone\u010dn\u1ef3, J., McMahan, H. B., Yu, F. X., Richt\u00e1rik, P., Suresh, A. T., and Bacon, D. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016b. Kulunchakov, A. and Mairal, J. Estimate sequences for stochastic composite optimization: Variance reduction, acceleration, and robustness to noise. arXiv preprint arXiv:1901.08788, 2019. Lee, J. D., Lin, Q., Ma, T., and Yang, T. Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity. arXiv preprint arXiv:1507.07595, 2015. Lei, L. and Jordan, M. Less than a single pass: Stochastically controlled stochastic gradient. In AISTATS, pp. 148-156, 2017. Li, T., Sahu, A. K., Sanjabi, M., Zaheer, M., Talwalkar, A., and Smith, V. On the convergence of federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018. Li, T., Sanjabi, M., and Smith, V. Fair resource allocation in federated learning. arXiv preprint arXiv:1905.10497, 2019a. Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V. Feddane: A federated newton-type method. arXiv preprint arXiv:2001.01920, 2020. Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. On the convergence of FedAvg on non-iid data. arXiv preprint arXiv:1907.02189, 2019b. Liang, X., Shen, S., Liu, J., Pan, Z., Chen, E., and Cheng, Y. Variance reduced local sgd with lower communication complexity. arXiv preprint arXiv:1912.12844, 2019. McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. Communication-efficient learning of deep networks from decentralized data. In Proceedings of AISTATS, pp. 1273-1282, 2017. Mishchenko, K., Gorbunov, E., Tak\u00e1\u010d, M., and Richt\u00e1rik, P. Distributed learning with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019. Mohri, M., Sivek, G., and Suresh, A. T. Agnostic federated learning. arXiv preprint arXiv:1902.00146, 2019. Nedich, A., Olshevsky, A., and Shi, W. A geometrically convergent method for distributed optimization over time-varying graphs. In 2016 IEEE 55th Conference on Decision and Control (CDC), pp. 1023-1029. IEEE, 2016. Nesterov, Y. Lectures on convex optimization, volume 137. Springer, 2018. Nguyen, L. M., Liu, J., Scheinberg, K., and Tak\u00e1\u010d, M. Sarah: A novel method for machine learning problems using stochastic recursive gradient. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2613-2621. JMLR. org, 2017. Nguyen, L. M., Scheinberg, K., and Tak\u00e1\u010d, M. Inexact SARAH algorithm for stochastic optimization. arXiv preprint arXiv:1811.10105, 2018. Patel, K. K. and Dieuleveut, A. Communication trade-offs for synchronized distributed SGD with large step size. arXiv preprint arXiv:1904.11325, 2019. Ramaswamy, S., Mathews, R., Rao, K., and Beaufays, F. Federated learning for emoji prediction in a mobile keyboard. arXiv preprint arXiv:1906.04329, 2019. Reddi, S. J., Hefny, A., Sra, S., Poczos, B., and Smola, A. Stochastic variance reduction for nonconvex optimization. In International conference on machine learning, pp. 314-323, 2016a. Reddi, S. J., Kone\u010dn\u1ef3, J., Richt\u00e1rik, P., P\u00f3cz\u00f3s, B., and Smola, A. Aide: Fast and communication efficient distributed optimization. arXiv preprint arXiv:1608.06879, 2016b. Reddi, S. J., Sra, S., P\u00f3czos, B., and Smola, A. Fast incremental method for smooth nonconvex optimization. In 2016 IEEE 55th Conference on Decision and Control (CDC), pp. 1971-1977. IEEE, 2016c. Safran, I. and Shamir, O. How good is sgd with random shuffling? arXiv preprint arXiv:1908.00045, 2019. Samarakoon, S., Bennis, M., Saad, W., and Debbah, M. Federated learning for ultra-reliable low-latency v2v communications. In 2018 IEEE Global Communications Conference (GLOBECOM), pp. 1-7. IEEE, 2018. Schmidt, M., Le Roux, N., and Bach, F. Minimizing finite sums with the stochastic average gradient. Mathematical Programming, 162(1-2):83-112, 2017. Shamir, O., Srebro, N., and Zhang, T. Communicationefficient distributed optimization using an approximate newton-type method. In International conference on machine learning, pp. 1000-1008, 2014. Shi, W., Ling, Q., Wu, G., and Yin, W. EXTRA: An exact first-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, Smith, V., Forte, S., Ma, C., Tak\u00e1\u010d, M., Jordan, M., and Jaggi, M. CoCoA: A general framework for communication-efficient distributed optimization. Journal of Machine Learning Research, 18(230):1-49, 2018. Stich, S. U. Local SGD converges fast and communicates little. arXiv preprint arXiv:1805.09767, 2018. Stich, S. U. Unified optimal analysis of the (stochastic) gradient method. arXiv preprint arXiv:1907.04232, 2019. Stich, S. U. and Karimireddy, S. P. The error-feedback framework: Better rates for SGD with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350, 2019. Stich, S. U., Cordonnier, J.-B., and Jaggi, M. Sparsified SGD with memory. In Advances in Neural Information Processing Systems, pp. 4447-4458, 2018. Suresh, A. T., Yu, F. X., Kumar, S., and McMahan, H. B. Distributed mean estimation with limited communication. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3329-3337. JMLR. org, 2017. Tran-Dinh, Q., Pham, N. H., Phan, D. T., and Nguyen, L. M. Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization. arXiv preprint arXiv:1905.05920, 2019. Vaswani, S., Bach, F., and Schmidt, M. Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1195-1204, 2019.Wang, S., Tuor, T., Salonidis, T., Leung, K. K., Makaya, C., He, T., and Chan, K. Adaptive federated learning in resource constrained edge computing systems.Arjevani, Y. and Shamir, O. Communication complex-\nity of distributed convex learning and optimization. In \nAdvances in neural information processing systems, pp. \n1756-1764, 2015. \n\nBassily, R., Smith, A., and Thakurta, A. Private empiri-\ncal risk minimization: Efficient algorithms and tight er-\nror bounds. In 2014 IEEE 55th Annual Symposium on \nFoundations of Computer Science, pp. 464-473. IEEE, \n2014. \n\nBasu, D., Data, D., Karakus, C., and Diggavi, S. Qsparse-\nlocal-SGD: Distributed SGD with quantization, spar-\nsification, and local computations. \narXiv preprint \narXiv:1906.02367, 2019. \n\nBonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., \nMcMahan, H. B., Patel, S., Ramage, D., Segal, A., \nand Seth, K. Practical secure aggregation for privacy-\npreserving machine learning. In Proceedings of the 2017 \nACM SIGSAC Conference on Computer and Communi-\ncations Security, pp. 1175-1191. ACM, 2017. \n\nBrisimi, T. S., Chen, R., Mela, T., Olshevsky, A., Pascha-\nlidis, I. C., and Shi, W. Federated learning of predic-\ntive models from federated electronic health records. In-\nternational journal of medical informatics, 112:59-67, \n2018. \n\nCen, S., Zhang, H., Chi, Y., Chen, W., and Liu, T.-Y. \nConvergence of distributed stochastic variance reduced \nmethods without sampling extra data. arXiv preprint \narXiv:1905.12648, 2019. \n\nChaudhuri, K., Monteleoni, C., and Sarwate, A. D. Differ-\nentially private empirical risk minimization. Journal of \nMachine Learning Research, 12(Mar):1069-1109, 2011. \n\nChen, M., Mathews, R., Ouyang, T., and Beaufays, F. \nFederated learning of out-of-vocabulary words. arXiv \npreprint arXiv:1903.10635, 2019a. \n\nChen, M., Suresh, A. T., Mathews, R., Wong, A., Beau-\nfays, F., Allauzen, C., and Riley, M. Federated learning \nof N-gram language models. In Proceedings of the 23rd \nConference on Computational Natural Language Learn-\ning (CoNLL), 2019b. \n\nCohen, G., Afshar, S., Tapson, J., and Van Schaik, A. \nEmnist: Extending mnist to handwritten letters. In \n2017 International Joint Conference on Neural Net-\nworks (IJCNN), pp. 2921-2926. IEEE, 2017. \n\nDean, J., Corrado, G., Monga, R., Chen, K., Devin, M., \nMao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., \nLe, Q. V., and Ng, A. Y. Large scale distributed deep \nnetworks. In Advances in neural information processing \nsystems, pp. 1223-1231, 2012. \n\nJohnson, R. and Zhang, T. Accelerating stochastic gradient \ndescent using predictive variance reduction. In Advances \nin neural information processing systems, pp. 315-323, \n2013. \n\n25(2):944-\n966, 2015. \n\nIEEE Jour-\nnal on Selected Areas in Communications, 37(6):1205-\n1221, 2019. \n\nWoodworth, B. E., Wang, J., Smith, A., McMahan, H. B., \nand Srebro, N. Graph oracle models, lower bounds, and \ngaps for parallel stochastic optimization. In Advances in \nneural information processing systems, pp. 8496-8506, \n2018. \n\nYang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, \nN., Ramage, D., and Beaufays, F. Applied federated \nlearning: Improving google keyboard query suggestions. \narXiv preprint arXiv:1812.02903, 2018. \n\nYu, H., Yang, S., and Zhu, S. Parallel restarted SGD with \nfaster convergence and less communication: Demystify-\ning why model averaging works for deep learning. In \nProceedings of the AAAI Conference on Artificial Intel-\nligence, volume 33, pp. 5693-5700, 2019. \n\nYuan, X.-T. and Li, P. On convergence of distributed \napproximate newton methods: Globalization, sharper \nbounds and beyond. arXiv preprint arXiv:1908.02246, \n2019. \n\nZhang, L., Mahdavi, M., and Jin, R. Linear convergence \nwith condition number independent access of full gra-\ndients. In Advances in Neural Information Processing \nSystems, pp. 980-988, 2013a. \nZhang, S., Choromanska, A. E., and LeCun, Y. Deep learn-\ning with elastic averaging sgd. In Advances in neural \ninformation processing systems, pp. 685-693, 2015. \n\nZhang, Y., Duchi, J. C., and Wainwright, M. J. \nCommunication-efficient algorithms for statistical opti-\nmization. The Journal of Machine Learning Research, \n14(1):3321-3363, 2013b. \n\nZhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, \nV. Federated learning with non-iid data. arXiv preprint \narXiv:1806.00582, 2018. \n\nZinkevich, M., Weimer, M., Li, L., and Smola, A. J. Par-\nallelized stochastic gradient descent. In Advances in \nneural information processing systems, pp. 2595-2603, \n2010. \n\n\n\na martingale difference sequence, and the variance is bounded by E[ \u039e i \u2212 \u03be i 2 ] \u2264 \u03c3 2 as before. Then we can show the tighter bound\nWe refer to these estimates as control variates and the resulting correction technique as stochastic controlled averaging.\nAcknowledgments. We thank Filip Hanzely and Jakub Kone\u010dn\u1ef3 for discussions regarding variance reduction techniques and Blake Woodworth, Virginia Smith and Kumar Kshitij Patel for suggestions which improved the writing.AppendixLet us state our rates of convergence for SCAFFOLD which interpolates between identical and completely heterogeneous clients. In this section, we always set \u03b7 g = 1 and assume all clients participate (S = N ).Theorem VIII. Suppose that the functions {f i } are quadratic and satisfy assumptions A4, A5 and additionally A2. Then, for global step-size \u03b7 g = 1 in each of the following cases, there exist probabilities {p r k } and local step-size \u03b7 l such that the output (29) of SCAFFOLD when run with no client sampling (S = N ) using update (28) satisfies:RK .\u2022 General convex: f satisfies \u2207 2 f \u2212\u03b4I, \u03b7 l \u2264 min( 1 10\u03b2 , 1 22\u03b4K ), and R \u2265 1, thenNote that if \u03b4 = 0, we match (up to acceleration) the lower bound in(Woodworth et al., 2018). While certainly \u03b4 = 0 when the functions are identical as studied in(Woodworth et al., 2018), our upper-bound is significantly stronger since it is possible that \u03b4 = 0 even for highly heterogeneous functions. For example, objective perturbation(Chaudhuri et al., 2011;Kifer et al., 2012)is an optimal mechanism to achieve differential privacy for smooth convex objectives(Bassily et al., 2014). Intuitively, objective perturbation relies on masking each client's gradients by adding a large random linear term to the objective function. In such a case, we would have high gradient dissimilarity but no Hessian dissimilarity.Our non-convex convergence rates are the first of their kind as far as we are aware-no previous work shows how one can take advantage of similarity for non-convex functions. However, we should note that non-convex quadratics do not have a global lower-bound on the function value f . We will instead assume that f almost surely lower-bounds the value of f (x R ), implicitly assuming that the iterates remain bounded.Outline. In the rest of this section, we will focus on proving Theorem VIII. We will show how to bound variance in Lemma 22, bound the amount of drift in Lemma 21, and show progress made in one step in Lemma 23. In all of these we do not use convexity, but strongly rely on the functions being quadratics. Then we combine these to derive the progress made by the server in one round-for this we need weak-convexity to argue that averaging the parameters does not hurt convergence too much. As before, it is straight-forward to derive rates of convergence from the one-round progress using Lemmas 1 and 2.F.1. Additional notation and assumptionsFor any matrix M and vector v, let v 2 M := v M v. Since all functions in this section are quadratics, we can assume w.l.o.g they are of the following form:This proves our second statement of the lemma. For strongly convex functions, we have for \u03b7 \u2264 1 \u03b2 ,F.3. Lemmas showing progressProgress of one client in one step. Now we focus only on a single client and monitor their progress.Lemma 23. Suppose (A2), (A5) and (A4) hold, and {f i } are quadratics. Then, the following holds for the update (28)Proof. Recall that \u03be r i,k \u2265 0 is defined to beLet us start from the local update step (28) (dropping unnecessary subscripts and superscripts)The second to last inequality used that \u00b7 2 A \u2264 \u03b2 \u00b7 2 2 by (A5) and that (A \u2212 A i )( \u00b7 ) 2 2 \u2264 \u03b4 2 \u00b7 2 2 by (A2). The final inequality used that \u03b7 \u2264 max( 1 10\u03b2 , 1 22\u03b4K ). Now, multiplying Lemma 21 by \u03b4(1 + 1 K ) K\u2212k \u2264 20\u03b4 7 we have \u03b4(1 + 1 K ) K\u2212k E r\u22121,k\u22121 y + i \u2212 x 2 \u2264 \u03b4(1 + 1 K ) K\u2212k (1 + 1 2K ) y i \u2212 x 2 + 20\u03b4K\u03b7 2 A(y i \u2212 x ) 2 + 3\u03b4\u03b7 2 \u03c3 2 \u2264 \u03b4(1 + 1 K ) K\u2212k (1 + 1 2K + 1 10K ) y i \u2212 x 2 \u2212 \u03b4 10K y i \u2212 x 2 + 20\u03b4K\u03b7 2 A(y i \u2212 x ) 2 + 3\u03b4\u03b7 2 \u03c3 2 \u2264 (1 \u2212 1 5K )\u03b4(1 + 1 K ) K\u2212k+1 (1 + 1 K ) y i \u2212 x 2 \u2212 \u03b4 10K y i \u2212 x 2 + 20\u03b4K\u03b7 2 A(y i \u2212 x ) 2 + 3\u03b4\u03b7 2 \u03c3 2 .\ncpSGD: Communication-efficient and differentially-private distributed SGD. N Agarwal, A T Suresh, F X Yu, S Kumar, B Mcmahan, Proceedings of NeurIPS. NeurIPSAgarwal, N., Suresh, A. T., Yu, F. X., Kumar, S., and McMahan, B. cpSGD: Communication-efficient and differentially-private distributed SGD. In Proceedings of NeurIPS, pp. 7575-7586, 2018.\n", "annotations": {"author": "[{\"end\":93,\"start\":68},{\"end\":106,\"start\":94},{\"end\":121,\"start\":107},{\"end\":138,\"start\":122},{\"end\":157,\"start\":139},{\"end\":181,\"start\":158}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":81},{\"end\":105,\"start\":101},{\"end\":120,\"start\":115},{\"end\":137,\"start\":132},{\"end\":156,\"start\":151},{\"end\":180,\"start\":174}]", "author_first_name": "[{\"end\":71,\"start\":68},{\"end\":80,\"start\":72},{\"end\":100,\"start\":94},{\"end\":114,\"start\":107},{\"end\":129,\"start\":122},{\"end\":131,\"start\":130},{\"end\":148,\"start\":139},{\"end\":150,\"start\":149},{\"end\":164,\"start\":158},{\"end\":173,\"start\":165}]", "author_affiliation": null, "title": "[{\"end\":65,\"start\":1},{\"end\":246,\"start\":182}]", "venue": null, "abstract": "[{\"end\":1182,\"start\":248}]", "bib_ref": "[{\"end\":1429,\"start\":1410},{\"end\":1450,\"start\":1429},{\"end\":1469,\"start\":1450},{\"end\":1677,\"start\":1654},{\"end\":1679,\"start\":1677},{\"end\":1700,\"start\":1679},{\"end\":1719,\"start\":1700},{\"end\":1740,\"start\":1719},{\"end\":2671,\"start\":2648},{\"end\":2749,\"start\":2727},{\"end\":3036,\"start\":3019},{\"end\":3052,\"start\":3036},{\"end\":3069,\"start\":3052},{\"end\":3096,\"start\":3069},{\"end\":3116,\"start\":3096},{\"end\":3681,\"start\":3680},{\"end\":4099,\"start\":4077},{\"end\":4121,\"start\":4099},{\"end\":4142,\"start\":4121},{\"end\":5586,\"start\":5573},{\"end\":5742,\"start\":5724},{\"end\":5827,\"start\":5808},{\"end\":5843,\"start\":5827},{\"end\":5897,\"start\":5876},{\"end\":5960,\"start\":5943},{\"end\":5987,\"start\":5960},{\"end\":6031,\"start\":6011},{\"end\":6304,\"start\":6283},{\"end\":11263,\"start\":11239},{\"end\":11397,\"start\":11381},{\"end\":11432,\"start\":11397},{\"end\":11654,\"start\":11629},{\"end\":15967,\"start\":15963},{\"end\":19454,\"start\":19432},{\"end\":19474,\"start\":19454},{\"end\":19782,\"start\":19761},{\"end\":19811,\"start\":19784},{\"end\":21973,\"start\":21952},{\"end\":22533,\"start\":22509},{\"end\":22973,\"start\":22949},{\"end\":25584,\"start\":25564},{\"end\":25899,\"start\":25882},{\"end\":28189,\"start\":28172},{\"end\":32286,\"start\":32265},{\"end\":41045,\"start\":41028},{\"end\":54334,\"start\":54311},{\"end\":54355,\"start\":54334},{\"end\":54381,\"start\":54355},{\"end\":78190,\"start\":78170},{\"end\":78210,\"start\":78190},{\"end\":78233,\"start\":78210},{\"end\":79011,\"start\":78990}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":86160,\"start\":85708},{\"attributes\":{\"id\":\"fig_2\"},\"end\":86401,\"start\":86161},{\"attributes\":{\"id\":\"fig_4\"},\"end\":86835,\"start\":86402},{\"attributes\":{\"id\":\"fig_5\"},\"end\":89104,\"start\":86836},{\"attributes\":{\"id\":\"fig_6\"},\"end\":91313,\"start\":89105},{\"attributes\":{\"id\":\"fig_8\"},\"end\":91611,\"start\":91314},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":92255,\"start\":91612},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":92874,\"start\":92256},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":106102,\"start\":92875},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":106238,\"start\":106103}]", "paragraph": "[{\"end\":2036,\"start\":1198},{\"end\":2326,\"start\":2038},{\"end\":3422,\"start\":2328},{\"end\":4232,\"start\":3424},{\"end\":4510,\"start\":4234},{\"end\":4563,\"start\":4512},{\"end\":5454,\"start\":4565},{\"end\":6559,\"start\":5456},{\"end\":7024,\"start\":6569},{\"end\":7089,\"start\":7088},{\"end\":7211,\"start\":7091},{\"end\":7553,\"start\":7213},{\"end\":7653,\"start\":7555},{\"end\":7755,\"start\":7703},{\"end\":7855,\"start\":7811},{\"end\":8044,\"start\":7945},{\"end\":8498,\"start\":8070},{\"end\":8675,\"start\":8531},{\"end\":8708,\"start\":8677},{\"end\":9286,\"start\":8710},{\"end\":9363,\"start\":9297},{\"end\":9864,\"start\":9823},{\"end\":9997,\"start\":9888},{\"end\":10229,\"start\":9999},{\"end\":10300,\"start\":10283},{\"end\":10378,\"start\":10365},{\"end\":11234,\"start\":10484},{\"end\":11321,\"start\":11236},{\"end\":11710,\"start\":11323},{\"end\":11739,\"start\":11712},{\"end\":11744,\"start\":11741},{\"end\":11749,\"start\":11746},{\"end\":12124,\"start\":11751},{\"end\":12410,\"start\":12126},{\"end\":12768,\"start\":12429},{\"end\":13136,\"start\":12815},{\"end\":13659,\"start\":13138},{\"end\":13960,\"start\":13708},{\"end\":14278,\"start\":13962},{\"end\":14361,\"start\":14285},{\"end\":14396,\"start\":14363},{\"end\":14467,\"start\":14398},{\"end\":14522,\"start\":14511},{\"end\":14674,\"start\":14657},{\"end\":14714,\"start\":14711},{\"end\":14869,\"start\":14716},{\"end\":15291,\"start\":14903},{\"end\":15416,\"start\":15314},{\"end\":15610,\"start\":15418},{\"end\":16356,\"start\":15612},{\"end\":16488,\"start\":16402},{\"end\":16999,\"start\":16564},{\"end\":17336,\"start\":17068},{\"end\":17448,\"start\":17338},{\"end\":17783,\"start\":17483},{\"end\":17879,\"start\":17828},{\"end\":18031,\"start\":17924},{\"end\":18194,\"start\":18059},{\"end\":18398,\"start\":18196},{\"end\":18420,\"start\":18400},{\"end\":18465,\"start\":18448},{\"end\":18514,\"start\":18501},{\"end\":19931,\"start\":18593},{\"end\":20442,\"start\":19933},{\"end\":20838,\"start\":20500},{\"end\":21270,\"start\":20868},{\"end\":21546,\"start\":21272},{\"end\":21591,\"start\":21575},{\"end\":21663,\"start\":21626},{\"end\":22236,\"start\":21665},{\"end\":23001,\"start\":22238},{\"end\":23738,\"start\":23003},{\"end\":23828,\"start\":23740},{\"end\":24172,\"start\":23878},{\"end\":24959,\"start\":24321},{\"end\":25272,\"start\":24975},{\"end\":25788,\"start\":25282},{\"end\":26388,\"start\":25790},{\"end\":26957,\"start\":26410},{\"end\":27352,\"start\":26959},{\"end\":28253,\"start\":27371},{\"end\":28636,\"start\":28255},{\"end\":30331,\"start\":28638},{\"end\":30744,\"start\":30528},{\"end\":30870,\"start\":30759},{\"end\":31741,\"start\":30872},{\"end\":31838,\"start\":31763},{\"end\":31943,\"start\":31870},{\"end\":31990,\"start\":31945},{\"end\":32287,\"start\":32063},{\"end\":32537,\"start\":32433},{\"end\":32576,\"start\":32539},{\"end\":32705,\"start\":32630},{\"end\":32882,\"start\":32759},{\"end\":33020,\"start\":32935},{\"end\":33438,\"start\":33051},{\"end\":33696,\"start\":33440},{\"end\":33894,\"start\":33791},{\"end\":34072,\"start\":34028},{\"end\":34200,\"start\":34164},{\"end\":34375,\"start\":34254},{\"end\":34459,\"start\":34427},{\"end\":34629,\"start\":34531},{\"end\":34907,\"start\":34691},{\"end\":35111,\"start\":34909},{\"end\":35280,\"start\":35239},{\"end\":35402,\"start\":35320},{\"end\":35631,\"start\":35601},{\"end\":35877,\"start\":35804},{\"end\":35993,\"start\":35879},{\"end\":36176,\"start\":36096},{\"end\":36331,\"start\":36249},{\"end\":36448,\"start\":36368},{\"end\":36708,\"start\":36450},{\"end\":36953,\"start\":36917},{\"end\":37154,\"start\":37056},{\"end\":37379,\"start\":37217},{\"end\":37494,\"start\":37440},{\"end\":37692,\"start\":37580},{\"end\":37954,\"start\":37730},{\"end\":38100,\"start\":37956},{\"end\":38267,\"start\":38156},{\"end\":38407,\"start\":38352},{\"end\":38497,\"start\":38450},{\"end\":38589,\"start\":38559},{\"end\":38651,\"start\":38591},{\"end\":38812,\"start\":38653},{\"end\":38866,\"start\":38860},{\"end\":38869,\"start\":38868},{\"end\":39174,\"start\":39005},{\"end\":39401,\"start\":39203},{\"end\":39543,\"start\":39403},{\"end\":39650,\"start\":39545},{\"end\":39808,\"start\":39700},{\"end\":39903,\"start\":39860},{\"end\":39972,\"start\":39905},{\"end\":40051,\"start\":40004},{\"end\":40152,\"start\":40100},{\"end\":40582,\"start\":40207},{\"end\":40625,\"start\":40584},{\"end\":41167,\"start\":40761},{\"end\":41288,\"start\":41209},{\"end\":41514,\"start\":41290},{\"end\":41936,\"start\":41871},{\"end\":42322,\"start\":42134},{\"end\":42537,\"start\":42324},{\"end\":42737,\"start\":42658},{\"end\":42960,\"start\":42790},{\"end\":43185,\"start\":43036},{\"end\":43499,\"start\":43401},{\"end\":43733,\"start\":43646},{\"end\":44207,\"start\":44144},{\"end\":44395,\"start\":44255},{\"end\":44727,\"start\":44532},{\"end\":44918,\"start\":44820},{\"end\":45193,\"start\":44990},{\"end\":46025,\"start\":45732},{\"end\":46236,\"start\":46164},{\"end\":46416,\"start\":46352},{\"end\":46488,\"start\":46418},{\"end\":46859,\"start\":46757},{\"end\":47273,\"start\":47001},{\"end\":47578,\"start\":47412},{\"end\":47708,\"start\":47623},{\"end\":48003,\"start\":47710},{\"end\":48491,\"start\":48005},{\"end\":48722,\"start\":48493},{\"end\":48864,\"start\":48779},{\"end\":49291,\"start\":48866},{\"end\":49445,\"start\":49293},{\"end\":49586,\"start\":49447},{\"end\":49848,\"start\":49675},{\"end\":50471,\"start\":49989},{\"end\":50633,\"start\":50499},{\"end\":50788,\"start\":50704},{\"end\":50915,\"start\":50814},{\"end\":50993,\"start\":50981},{\"end\":51359,\"start\":51187},{\"end\":51441,\"start\":51380},{\"end\":51517,\"start\":51504},{\"end\":52098,\"start\":51606},{\"end\":52255,\"start\":52100},{\"end\":52307,\"start\":52285},{\"end\":52393,\"start\":52333},{\"end\":52768,\"start\":52495},{\"end\":53080,\"start\":52798},{\"end\":53615,\"start\":53111},{\"end\":53635,\"start\":53617},{\"end\":53814,\"start\":53811},{\"end\":53833,\"start\":53816},{\"end\":54065,\"start\":54064},{\"end\":54229,\"start\":54155},{\"end\":54601,\"start\":54262},{\"end\":55018,\"start\":54699},{\"end\":55674,\"start\":55020},{\"end\":55815,\"start\":55767},{\"end\":55964,\"start\":55903},{\"end\":56121,\"start\":56013},{\"end\":56279,\"start\":56236},{\"end\":56790,\"start\":56352},{\"end\":57216,\"start\":56858},{\"end\":57424,\"start\":57218},{\"end\":57507,\"start\":57426},{\"end\":57710,\"start\":57561},{\"end\":57849,\"start\":57760},{\"end\":57982,\"start\":57851},{\"end\":58181,\"start\":58080},{\"end\":58289,\"start\":58260},{\"end\":59143,\"start\":58736},{\"end\":59449,\"start\":59412},{\"end\":59614,\"start\":59553},{\"end\":59824,\"start\":59668},{\"end\":59969,\"start\":59826},{\"end\":60133,\"start\":60038},{\"end\":60289,\"start\":60249},{\"end\":60435,\"start\":60374},{\"end\":60767,\"start\":60648},{\"end\":61210,\"start\":61125},{\"end\":61340,\"start\":61247},{\"end\":61489,\"start\":61342},{\"end\":61908,\"start\":61566},{\"end\":62349,\"start\":62218},{\"end\":62507,\"start\":62421},{\"end\":62951,\"start\":62830},{\"end\":63216,\"start\":63093},{\"end\":63432,\"start\":63355},{\"end\":63978,\"start\":63925},{\"end\":64288,\"start\":64178},{\"end\":64563,\"start\":64290},{\"end\":64735,\"start\":64687},{\"end\":64898,\"start\":64821},{\"end\":65255,\"start\":65128},{\"end\":65480,\"start\":65416},{\"end\":65791,\"start\":65725},{\"end\":65952,\"start\":65892},{\"end\":66068,\"start\":66029},{\"end\":66369,\"start\":66254},{\"end\":66616,\"start\":66371},{\"end\":67135,\"start\":66688},{\"end\":67219,\"start\":67137},{\"end\":67514,\"start\":67288},{\"end\":67704,\"start\":67579},{\"end\":67826,\"start\":67742},{\"end\":67960,\"start\":67877},{\"end\":68128,\"start\":68010},{\"end\":68246,\"start\":68130},{\"end\":68454,\"start\":68403},{\"end\":68575,\"start\":68493},{\"end\":68796,\"start\":68723},{\"end\":69776,\"start\":69295},{\"end\":70066,\"start\":69778},{\"end\":70215,\"start\":70068},{\"end\":70496,\"start\":70329},{\"end\":70619,\"start\":70558},{\"end\":70847,\"start\":70741},{\"end\":70931,\"start\":70849},{\"end\":71207,\"start\":71109},{\"end\":71473,\"start\":71409},{\"end\":71659,\"start\":71545},{\"end\":72003,\"start\":71661},{\"end\":72327,\"start\":72083},{\"end\":73193,\"start\":72987},{\"end\":73738,\"start\":73591},{\"end\":73868,\"start\":73740},{\"end\":73988,\"start\":73870},{\"end\":74210,\"start\":74129},{\"end\":74373,\"start\":74278},{\"end\":74555,\"start\":74550},{\"end\":75137,\"start\":75075},{\"end\":75294,\"start\":75194},{\"end\":75666,\"start\":75550},{\"end\":75800,\"start\":75735},{\"end\":76037,\"start\":76036},{\"end\":76142,\"start\":76039},{\"end\":76360,\"start\":76144},{\"end\":76738,\"start\":76436},{\"end\":76888,\"start\":76740},{\"end\":77387,\"start\":77062},{\"end\":78480,\"start\":77412},{\"end\":78569,\"start\":78482},{\"end\":78822,\"start\":78759},{\"end\":79186,\"start\":78854},{\"end\":79512,\"start\":79285},{\"end\":80006,\"start\":79895},{\"end\":80123,\"start\":80008},{\"end\":80453,\"start\":80227},{\"end\":80538,\"start\":80455},{\"end\":80755,\"start\":80540},{\"end\":80798,\"start\":80757},{\"end\":81176,\"start\":80959},{\"end\":81382,\"start\":81240},{\"end\":81447,\"start\":81384},{\"end\":81539,\"start\":81449},{\"end\":81593,\"start\":81541},{\"end\":81865,\"start\":81666},{\"end\":82095,\"start\":82018},{\"end\":82228,\"start\":82186},{\"end\":82359,\"start\":82286},{\"end\":82918,\"start\":82543},{\"end\":83196,\"start\":83088},{\"end\":83268,\"start\":83198},{\"end\":83656,\"start\":83365},{\"end\":83781,\"start\":83658},{\"end\":83976,\"start\":83783},{\"end\":84162,\"start\":84092},{\"end\":84326,\"start\":84164},{\"end\":84414,\"start\":84328},{\"end\":84575,\"start\":84547},{\"end\":84719,\"start\":84685},{\"end\":84874,\"start\":84848},{\"end\":85141,\"start\":84960},{\"end\":85295,\"start\":85219},{\"end\":85707,\"start\":85411}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7087,\"start\":7025},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7702,\"start\":7654},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7810,\"start\":7756},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7944,\"start\":7856},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8530,\"start\":8499},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9822,\"start\":9364},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10282,\"start\":10230},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10364,\"start\":10301},{\"attributes\":{\"id\":\"formula_8\"},\"end\":10483,\"start\":10379},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12428,\"start\":12411},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13707,\"start\":13660},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14510,\"start\":14468},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14656,\"start\":14523},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14710,\"start\":14675},{\"attributes\":{\"id\":\"formula_14\"},\"end\":14902,\"start\":14870},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16401,\"start\":16357},{\"attributes\":{\"id\":\"formula_16\"},\"end\":16563,\"start\":16489},{\"attributes\":{\"id\":\"formula_17\"},\"end\":17067,\"start\":17000},{\"attributes\":{\"id\":\"formula_18\"},\"end\":17482,\"start\":17449},{\"attributes\":{\"id\":\"formula_19\"},\"end\":17827,\"start\":17784},{\"attributes\":{\"id\":\"formula_20\"},\"end\":17923,\"start\":17880},{\"attributes\":{\"id\":\"formula_21\"},\"end\":18447,\"start\":18421},{\"attributes\":{\"id\":\"formula_22\"},\"end\":18500,\"start\":18466},{\"attributes\":{\"id\":\"formula_23\"},\"end\":18592,\"start\":18515},{\"attributes\":{\"id\":\"formula_24\"},\"end\":20499,\"start\":20443},{\"attributes\":{\"id\":\"formula_25\"},\"end\":21574,\"start\":21547},{\"attributes\":{\"id\":\"formula_26\"},\"end\":21625,\"start\":21592},{\"attributes\":{\"id\":\"formula_27\"},\"end\":23877,\"start\":23829},{\"attributes\":{\"id\":\"formula_28\"},\"end\":24320,\"start\":24173},{\"attributes\":{\"id\":\"formula_29\"},\"end\":30527,\"start\":30341},{\"attributes\":{\"id\":\"formula_30\"},\"end\":32062,\"start\":31991},{\"attributes\":{\"id\":\"formula_31\"},\"end\":32432,\"start\":32288},{\"attributes\":{\"id\":\"formula_32\"},\"end\":32629,\"start\":32577},{\"attributes\":{\"id\":\"formula_33\"},\"end\":32758,\"start\":32706},{\"attributes\":{\"id\":\"formula_34\"},\"end\":32934,\"start\":32883},{\"attributes\":{\"id\":\"formula_35\"},\"end\":33790,\"start\":33697},{\"attributes\":{\"id\":\"formula_36\"},\"end\":34027,\"start\":33895},{\"attributes\":{\"id\":\"formula_37\"},\"end\":34163,\"start\":34073},{\"attributes\":{\"id\":\"formula_38\"},\"end\":34253,\"start\":34201},{\"attributes\":{\"id\":\"formula_39\"},\"end\":34426,\"start\":34376},{\"attributes\":{\"id\":\"formula_40\"},\"end\":34530,\"start\":34460},{\"attributes\":{\"id\":\"formula_41\"},\"end\":34690,\"start\":34630},{\"attributes\":{\"id\":\"formula_42\"},\"end\":35238,\"start\":35112},{\"attributes\":{\"id\":\"formula_43\"},\"end\":35319,\"start\":35281},{\"attributes\":{\"id\":\"formula_44\"},\"end\":35600,\"start\":35403},{\"attributes\":{\"id\":\"formula_45\"},\"end\":35803,\"start\":35632},{\"attributes\":{\"id\":\"formula_46\"},\"end\":36095,\"start\":35994},{\"attributes\":{\"id\":\"formula_47\"},\"end\":36248,\"start\":36177},{\"attributes\":{\"id\":\"formula_48\"},\"end\":36367,\"start\":36332},{\"attributes\":{\"id\":\"formula_49\"},\"end\":36751,\"start\":36709},{\"attributes\":{\"id\":\"formula_50\"},\"end\":36873,\"start\":36808},{\"attributes\":{\"id\":\"formula_51\"},\"end\":36916,\"start\":36873},{\"attributes\":{\"id\":\"formula_52\"},\"end\":37055,\"start\":36954},{\"attributes\":{\"id\":\"formula_53\"},\"end\":37216,\"start\":37155},{\"attributes\":{\"id\":\"formula_54\"},\"end\":37439,\"start\":37380},{\"attributes\":{\"id\":\"formula_55\"},\"end\":37579,\"start\":37495},{\"attributes\":{\"id\":\"formula_56\"},\"end\":38155,\"start\":38101},{\"attributes\":{\"id\":\"formula_57\"},\"end\":38351,\"start\":38268},{\"attributes\":{\"id\":\"formula_58\"},\"end\":38449,\"start\":38408},{\"attributes\":{\"id\":\"formula_59\"},\"end\":38558,\"start\":38498},{\"attributes\":{\"id\":\"formula_60\"},\"end\":38859,\"start\":38813},{\"attributes\":{\"id\":\"formula_61\"},\"end\":39004,\"start\":38870},{\"attributes\":{\"id\":\"formula_62\"},\"end\":39699,\"start\":39651},{\"attributes\":{\"id\":\"formula_63\"},\"end\":39859,\"start\":39809},{\"attributes\":{\"id\":\"formula_65\"},\"end\":40099,\"start\":40052},{\"attributes\":{\"id\":\"formula_66\"},\"end\":40206,\"start\":40153},{\"attributes\":{\"id\":\"formula_67\"},\"end\":40760,\"start\":40626},{\"attributes\":{\"id\":\"formula_68\"},\"end\":41870,\"start\":41515},{\"attributes\":{\"id\":\"formula_69\"},\"end\":42105,\"start\":41937},{\"attributes\":{\"id\":\"formula_70\"},\"end\":42657,\"start\":42538},{\"attributes\":{\"id\":\"formula_71\"},\"end\":42789,\"start\":42738},{\"attributes\":{\"id\":\"formula_72\"},\"end\":43035,\"start\":42961},{\"attributes\":{\"id\":\"formula_73\"},\"end\":43400,\"start\":43186},{\"attributes\":{\"id\":\"formula_74\"},\"end\":43645,\"start\":43500},{\"attributes\":{\"id\":\"formula_75\"},\"end\":44143,\"start\":43734},{\"attributes\":{\"id\":\"formula_76\"},\"end\":44254,\"start\":44208},{\"attributes\":{\"id\":\"formula_77\"},\"end\":44531,\"start\":44396},{\"attributes\":{\"id\":\"formula_78\"},\"end\":44989,\"start\":44919},{\"attributes\":{\"id\":\"formula_79\"},\"end\":45731,\"start\":45194},{\"attributes\":{\"id\":\"formula_80\"},\"end\":46163,\"start\":46026},{\"attributes\":{\"id\":\"formula_81\"},\"end\":46351,\"start\":46237},{\"attributes\":{\"id\":\"formula_82\"},\"end\":46756,\"start\":46489},{\"attributes\":{\"id\":\"formula_83\"},\"end\":47000,\"start\":46860},{\"attributes\":{\"id\":\"formula_84\"},\"end\":47411,\"start\":47274},{\"attributes\":{\"id\":\"formula_85\"},\"end\":48778,\"start\":48723},{\"attributes\":{\"id\":\"formula_86\"},\"end\":49674,\"start\":49587},{\"attributes\":{\"id\":\"formula_87\"},\"end\":49988,\"start\":49849},{\"attributes\":{\"id\":\"formula_88\"},\"end\":50498,\"start\":50472},{\"attributes\":{\"id\":\"formula_89\"},\"end\":50703,\"start\":50634},{\"attributes\":{\"id\":\"formula_90\"},\"end\":50813,\"start\":50789},{\"attributes\":{\"id\":\"formula_91\"},\"end\":50980,\"start\":50916},{\"attributes\":{\"id\":\"formula_92\"},\"end\":51186,\"start\":50994},{\"attributes\":{\"id\":\"formula_93\"},\"end\":51379,\"start\":51360},{\"attributes\":{\"id\":\"formula_94\"},\"end\":51503,\"start\":51442},{\"attributes\":{\"id\":\"formula_95\"},\"end\":51605,\"start\":51518},{\"attributes\":{\"id\":\"formula_96\"},\"end\":52284,\"start\":52256},{\"attributes\":{\"id\":\"formula_97\"},\"end\":52332,\"start\":52308},{\"attributes\":{\"id\":\"formula_98\"},\"end\":52494,\"start\":52394},{\"attributes\":{\"id\":\"formula_99\"},\"end\":52797,\"start\":52769},{\"attributes\":{\"id\":\"formula_100\"},\"end\":53810,\"start\":53636},{\"attributes\":{\"id\":\"formula_101\"},\"end\":54063,\"start\":53834},{\"attributes\":{\"id\":\"formula_102\"},\"end\":54154,\"start\":54066},{\"attributes\":{\"id\":\"formula_103\"},\"end\":54261,\"start\":54230},{\"attributes\":{\"id\":\"formula_104\"},\"end\":54698,\"start\":54602},{\"attributes\":{\"id\":\"formula_105\"},\"end\":55766,\"start\":55675},{\"attributes\":{\"id\":\"formula_106\"},\"end\":55902,\"start\":55816},{\"attributes\":{\"id\":\"formula_107\"},\"end\":56012,\"start\":55965},{\"attributes\":{\"id\":\"formula_108\"},\"end\":56235,\"start\":56122},{\"attributes\":{\"id\":\"formula_109\"},\"end\":56351,\"start\":56280},{\"attributes\":{\"id\":\"formula_110\"},\"end\":57560,\"start\":57508},{\"attributes\":{\"id\":\"formula_111\"},\"end\":57759,\"start\":57711},{\"attributes\":{\"id\":\"formula_112\"},\"end\":58079,\"start\":57983},{\"attributes\":{\"id\":\"formula_113\"},\"end\":58259,\"start\":58182},{\"attributes\":{\"id\":\"formula_114\"},\"end\":58735,\"start\":58290},{\"attributes\":{\"id\":\"formula_115\"},\"end\":59411,\"start\":59144},{\"attributes\":{\"id\":\"formula_116\"},\"end\":59552,\"start\":59450},{\"attributes\":{\"id\":\"formula_117\"},\"end\":59667,\"start\":59615},{\"attributes\":{\"id\":\"formula_118\"},\"end\":60037,\"start\":59970},{\"attributes\":{\"id\":\"formula_119\"},\"end\":60248,\"start\":60134},{\"attributes\":{\"id\":\"formula_120\"},\"end\":60373,\"start\":60290},{\"attributes\":{\"id\":\"formula_121\"},\"end\":60647,\"start\":60436},{\"attributes\":{\"id\":\"formula_122\"},\"end\":61124,\"start\":60768},{\"attributes\":{\"id\":\"formula_123\"},\"end\":61246,\"start\":61211},{\"attributes\":{\"id\":\"formula_124\"},\"end\":61565,\"start\":61490},{\"attributes\":{\"id\":\"formula_125\"},\"end\":62217,\"start\":61909},{\"attributes\":{\"id\":\"formula_126\"},\"end\":62420,\"start\":62350},{\"attributes\":{\"id\":\"formula_127\"},\"end\":62829,\"start\":62508},{\"attributes\":{\"id\":\"formula_128\"},\"end\":63092,\"start\":62952},{\"attributes\":{\"id\":\"formula_129\"},\"end\":63354,\"start\":63217},{\"attributes\":{\"id\":\"formula_130\"},\"end\":63924,\"start\":63433},{\"attributes\":{\"id\":\"formula_131\"},\"end\":64177,\"start\":63979},{\"attributes\":{\"id\":\"formula_132\"},\"end\":64686,\"start\":64564},{\"attributes\":{\"id\":\"formula_133\"},\"end\":64820,\"start\":64736},{\"attributes\":{\"id\":\"formula_134\"},\"end\":65127,\"start\":64899},{\"attributes\":{\"id\":\"formula_135\"},\"end\":65415,\"start\":65256},{\"attributes\":{\"id\":\"formula_136\"},\"end\":65724,\"start\":65481},{\"attributes\":{\"id\":\"formula_137\"},\"end\":65891,\"start\":65792},{\"attributes\":{\"id\":\"formula_138\"},\"end\":66028,\"start\":65953},{\"attributes\":{\"id\":\"formula_139\"},\"end\":66253,\"start\":66069},{\"attributes\":{\"id\":\"formula_140\"},\"end\":67287,\"start\":67220},{\"attributes\":{\"id\":\"formula_141\"},\"end\":67578,\"start\":67515},{\"attributes\":{\"id\":\"formula_142\"},\"end\":67741,\"start\":67705},{\"attributes\":{\"id\":\"formula_143\"},\"end\":67876,\"start\":67827},{\"attributes\":{\"id\":\"formula_144\"},\"end\":68009,\"start\":67961},{\"attributes\":{\"id\":\"formula_145\"},\"end\":68402,\"start\":68247},{\"attributes\":{\"id\":\"formula_146\"},\"end\":68492,\"start\":68455},{\"attributes\":{\"id\":\"formula_147\"},\"end\":68655,\"start\":68576},{\"attributes\":{\"id\":\"formula_148\"},\"end\":69294,\"start\":68797},{\"attributes\":{\"id\":\"formula_149\"},\"end\":70328,\"start\":70216},{\"attributes\":{\"id\":\"formula_150\"},\"end\":70557,\"start\":70497},{\"attributes\":{\"id\":\"formula_151\"},\"end\":70740,\"start\":70620},{\"attributes\":{\"id\":\"formula_152\"},\"end\":71108,\"start\":70932},{\"attributes\":{\"id\":\"formula_153\"},\"end\":71408,\"start\":71208},{\"attributes\":{\"id\":\"formula_154\"},\"end\":71544,\"start\":71474},{\"attributes\":{\"id\":\"formula_155\"},\"end\":72082,\"start\":72004},{\"attributes\":{\"id\":\"formula_156\"},\"end\":72986,\"start\":72328},{\"attributes\":{\"id\":\"formula_157\"},\"end\":73590,\"start\":73194},{\"attributes\":{\"id\":\"formula_158\"},\"end\":74128,\"start\":73989},{\"attributes\":{\"id\":\"formula_159\"},\"end\":74277,\"start\":74211},{\"attributes\":{\"id\":\"formula_160\"},\"end\":74461,\"start\":74374},{\"attributes\":{\"id\":\"formula_161\"},\"end\":75074,\"start\":74556},{\"attributes\":{\"id\":\"formula_162\"},\"end\":75193,\"start\":75138},{\"attributes\":{\"id\":\"formula_163\"},\"end\":75549,\"start\":75295},{\"attributes\":{\"id\":\"formula_164\"},\"end\":76035,\"start\":75801},{\"attributes\":{\"id\":\"formula_165\"},\"end\":76435,\"start\":76361},{\"attributes\":{\"id\":\"formula_166\"},\"end\":77061,\"start\":76889},{\"attributes\":{\"id\":\"formula_167\"},\"end\":77411,\"start\":77388},{\"attributes\":{\"id\":\"formula_168\"},\"end\":78758,\"start\":78570},{\"attributes\":{\"id\":\"formula_169\"},\"end\":79284,\"start\":79187},{\"attributes\":{\"id\":\"formula_170\"},\"end\":79894,\"start\":79513},{\"attributes\":{\"id\":\"formula_171\"},\"end\":80226,\"start\":80124},{\"attributes\":{\"id\":\"formula_172\"},\"end\":80958,\"start\":80799},{\"attributes\":{\"id\":\"formula_173\"},\"end\":81239,\"start\":81177},{\"attributes\":{\"id\":\"formula_174\"},\"end\":81665,\"start\":81594},{\"attributes\":{\"id\":\"formula_175\"},\"end\":82017,\"start\":81866},{\"attributes\":{\"id\":\"formula_176\"},\"end\":82185,\"start\":82096},{\"attributes\":{\"id\":\"formula_177\"},\"end\":82285,\"start\":82229},{\"attributes\":{\"id\":\"formula_178\"},\"end\":82542,\"start\":82360},{\"attributes\":{\"id\":\"formula_179\"},\"end\":83087,\"start\":82919},{\"attributes\":{\"id\":\"formula_180\"},\"end\":83364,\"start\":83269},{\"attributes\":{\"id\":\"formula_181\"},\"end\":84091,\"start\":83977},{\"attributes\":{\"id\":\"formula_182\"},\"end\":84546,\"start\":84415},{\"attributes\":{\"id\":\"formula_183\"},\"end\":84684,\"start\":84576},{\"attributes\":{\"id\":\"formula_184\"},\"end\":84780,\"start\":84720},{\"attributes\":{\"id\":\"formula_185\"},\"end\":84959,\"start\":84875},{\"attributes\":{\"id\":\"formula_186\"},\"end\":85218,\"start\":85142},{\"attributes\":{\"id\":\"formula_187\"},\"end\":85410,\"start\":85296}]", "table_ref": "[{\"end\":6558,\"start\":6551},{\"end\":6686,\"start\":6679},{\"end\":8721,\"start\":8714},{\"end\":27571,\"start\":27564},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27637,\"start\":27630},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27692,\"start\":27685},{\"end\":29089,\"start\":29082},{\"end\":31739,\"start\":31732}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1196,\"start\":1184},{\"attributes\":{\"n\":\"2.\"},\"end\":6567,\"start\":6562},{\"attributes\":{\"n\":\"3.\"},\"end\":8068,\"start\":8047},{\"end\":9295,\"start\":9289},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9886,\"start\":9867},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12813,\"start\":12771},{\"end\":14283,\"start\":14281},{\"attributes\":{\"n\":\"4.\"},\"end\":15312,\"start\":15294},{\"attributes\":{\"n\":\"5.\"},\"end\":18057,\"start\":18034},{\"attributes\":{\"n\":\"6.\"},\"end\":20866,\"start\":20841},{\"attributes\":{\"n\":\"7.\"},\"end\":24973,\"start\":24962},{\"attributes\":{\"n\":\"7.1.\"},\"end\":25280,\"start\":25275},{\"attributes\":{\"n\":\"7.2.\"},\"end\":26408,\"start\":26391},{\"attributes\":{\"n\":\"7.3.\"},\"end\":27369,\"start\":27355},{\"end\":30340,\"start\":30334},{\"attributes\":{\"n\":\"8.\"},\"end\":30757,\"start\":30747},{\"end\":31761,\"start\":31744},{\"end\":31868,\"start\":31841},{\"end\":33049,\"start\":33023},{\"end\":36807,\"start\":36753},{\"end\":37728,\"start\":37695},{\"end\":39201,\"start\":39177},{\"end\":40002,\"start\":39975},{\"end\":41207,\"start\":41170},{\"end\":42132,\"start\":42107},{\"end\":44818,\"start\":44730},{\"end\":47621,\"start\":47581},{\"end\":53109,\"start\":53083},{\"end\":56856,\"start\":56793},{\"end\":66686,\"start\":66619},{\"end\":68721,\"start\":68657},{\"end\":74548,\"start\":74463},{\"end\":75733,\"start\":75669},{\"end\":78852,\"start\":78825},{\"end\":84846,\"start\":84782},{\"end\":86170,\"start\":86162},{\"end\":86411,\"start\":86403},{\"end\":86857,\"start\":86837},{\"end\":91325,\"start\":91315},{\"end\":91622,\"start\":91613},{\"end\":92266,\"start\":92257}]", "table": "[{\"end\":92255,\"start\":91637},{\"end\":92874,\"start\":92272},{\"end\":106102,\"start\":101734}]", "figure_caption": "[{\"end\":86160,\"start\":85710},{\"end\":86401,\"start\":86172},{\"end\":86835,\"start\":86413},{\"end\":89104,\"start\":86859},{\"end\":91313,\"start\":89107},{\"end\":91611,\"start\":91328},{\"end\":91637,\"start\":91624},{\"end\":92272,\"start\":92268},{\"end\":101734,\"start\":92877},{\"end\":106238,\"start\":106105}]", "figure_ref": "[{\"end\":11842,\"start\":11834},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":13211,\"start\":13205},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26446,\"start\":26440}]", "bib_author_first_name": "[{\"end\":110286,\"start\":110285},{\"end\":110297,\"start\":110296},{\"end\":110299,\"start\":110298},{\"end\":110309,\"start\":110308},{\"end\":110311,\"start\":110310},{\"end\":110317,\"start\":110316},{\"end\":110326,\"start\":110325}]", "bib_author_last_name": "[{\"end\":110294,\"start\":110287},{\"end\":110306,\"start\":110300},{\"end\":110314,\"start\":110312},{\"end\":110323,\"start\":110318},{\"end\":110334,\"start\":110327}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":44113205},\"end\":110555,\"start\":110210}]", "bib_title": "[{\"end\":110283,\"start\":110210}]", "bib_author": "[{\"end\":110296,\"start\":110285},{\"end\":110308,\"start\":110296},{\"end\":110316,\"start\":110308},{\"end\":110325,\"start\":110316},{\"end\":110336,\"start\":110325}]", "bib_venue": "[{\"end\":110358,\"start\":110336},{\"end\":110367,\"start\":110360}]"}}}, "year": 2023, "month": 12, "day": 17}