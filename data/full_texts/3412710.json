{"id": 3412710, "updated": "2023-11-08 03:08:15.022", "metadata": {"title": "Power of Deep Learning for Channel Estimation and Signal Detection in OFDM Systems", "authors": "[{\"first\":\"Hao\",\"last\":\"Ye\",\"middle\":[]},{\"first\":\"Geoffrey\",\"last\":\"Li\",\"middle\":[\"Ye\"]},{\"first\":\"Biing-Hwang\",\"last\":\"Juang\",\"middle\":[\"Fred\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 8, "day": 28}, "abstract": "This article presents our initial results in deep learning for channel estimation and signal detection in orthogonal frequency-division multiplexing (OFDM). OFDM has been widely adopted in wireless broadband communications to combat frequency-selective fading in wireless channels. In this article, we take advantage of deep learning in handling wireless OFDM channels in an end-to-end approach. Different from existing OFDM receivers that first estimate CSI explicitly and then detect/recover the transmitted symbols with the estimated CSI, our deep learning based approach estimates CSI implicitly and recovers the transmitted symbols directly. To address channel distortion, a deep learning model is first trained offline using the data generated from the simulation based on the channel statistics and then used for recovering the online transmitted data directly. From our simulation results, the deep learning based approach has the ability to address channel distortions and detect the transmitted symbols with performance comparable to minimum mean-square error (MMSE) estimator. Furthermore, the deep learning based approach is more robust than conventional methods when fewer training pilots are used, the cyclic prefix (CP) is omitted, and nonlinear clipping noise is presented. In summary, deep learning is a promising tool for channel estimation and signal detection in wireless communications with complicated channel distortions and interferences.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1708.08514", "mag": "2963190722", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1708-08514", "doi": "10.1109/lwc.2017.2757490"}}, "content": {"source": {"pdf_hash": "f78787f02aeee7b9114039789d25aa553f74bdc9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1708.08514v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1708.08514", "status": "GREEN"}}, "grobid": {"id": "1efbeba413bcda42eb92a79ab367a6d60d366b63", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f78787f02aeee7b9114039789d25aa553f74bdc9.txt", "contents": "\nPower of Deep Learning for Channel Estimation and Signal Detection in OFDM Systems\n28 Aug 2017\n\nHao Ye \nFellow, IEEEGeoffrey Ye Li \nFellow, IEEEBiing-Hwang Fred Juang \nPower of Deep Learning for Channel Estimation and Signal Detection in OFDM Systems\n28 Aug 20171\nThis article presents our initial results in deep learning for channel estimation and signal detection in orthogonal frequency-division multiplexing (OFDM) systems. In this article, we exploit deep learning to handle wireless OFDM channels in an end-to-end manner. Different from existing OFDM receivers that first estimate channel state information (CSI) explicitly and then detect/recover the transmitted symbols using the estimated CSI, the proposed deep learning based approach estimates CSI implicitly and recovers the transmitted symbols directly. To address channel distortion, a deep learning model is first trained offline using the data generated from simulation based on channel statistics and then used for recovering the online transmitted data directly. From our simulation results, the deep learning based approach can address channel distortion and detect the transmitted symbols with performance comparable to the minimum meansquare error (MMSE) estimator. Furthermore, the deep learning based approach is more robust than conventional methods when fewer training pilots are used, the cyclic prefix (CP) is omitted, and nonlinear clipping noise exists. In summary, deep learning is a promising tool for channel estimation and signal detection in wireless communications with complicated channel distortion and interference.\n\nI. INTRODUCTION\n\nOrthogonal frequency-division multiplexing (OFDM) is a popular modulation scheme that has been widely adopted in wireless broadband systems to combat frequency-selective fading in wireless channels. Channel state information (CSI) is vital to coherent detection and decoding in OFDM systems. Usually, the CSI can be estimated by means of pilots prior to the detection of the transmitted data. With the estimated CSI, transmitted symbols can be recovered at the receiver.\n\nHistorically, channel estimation in OFDM systems has been thoroughly studied. The traditional estimation methods, i.e., least square (LS) and minimum mean-square error (MMSE), have been utilized and optimized in various conditions [2]. The method of LS estimation requires no prior channel statistics, but its performance may be inadequate. The MMSE estimation in general leads to much better detection performance by utilizing the second order statistics of channels.\n\nIn this article, we introduce a deep learning approach to channel estimation and symbol detection in an OFDM system. Deep learning and artificial neural networks (ANNs) have numerous applications. In particular, it has been successfully applied in localization based on CSI [3], channel equalization [5], and channel decoding [4]  availability of data in large quantity, we expect deep learning to find more applications in communication systems.\n\nANNs have been demonstrated for channel equalization with online training, which is to adjust the parameters according to the online pilot data. However, such methods can not be applied directly since, with deep neural networks (DNNs), the number of parameters becomes much increased, which requires a large number of training data together with the burden of a long training period. To address the issue, we train a DNN model that predicts the transmitted data in diverse channel conditions. Then the model is used in online deployment to recover the transmitted data.\n\nThis article presents our initial results in deep learning for channel estimation and symbol detection in an end-to-end manner. It demonstrates that DNNs have the ability to learn and analyze the characteristics of wireless channels that may suffer from nonlinear distortion and interference in addition to frequency selectivity. To the best of our knowledge, this is the first attempt to use learning methods to deal with wireless channels without online training. The simulation results show that deep learning models achieve performance comparable to traditional methods if there are enough pilots in OFDM systems, and it can work better with limited pilots, channel interference, and nonlinear noise. Our initial research results indicate that deep learning can be potentially applied in many directions in signal processing and communications.\n\n\nII. DEEP LEARNING BASED ESTIMATION AND DETECTION\n\nIn this section, we present a method where deep learning is exploited as an end-to-end approach for channel estimation and symbol detection. The DNN model is trained based on simulated data offline, which views OFDM and the wireless channel as complete black boxes.\n\n\nA. Deep Learning Methods\n\nDeep learning has been successfully applied in a wide range of areas with significant performance improvement, including computer vision [6], natural language processing [7], speech recognition [8], and so on. A comprehensive introduction to deep learning and machine learning can be found in [1].\n\nThe structure of a DNN model is shown in Fig. 1. Generally speaking, DNNs are deeper versions of ANNs by increasing the number of hidden layers in order to improve the ability in representation or recognition. Each layer of the network consists of multiple neurons, each of which has an output that is a nonlinear function of a weighted sum of neurons of its preceding layer, as shown in Fig. 1. The nonlinear function \nz = f (I, \u03b8) = f (L\u22121) (f (L\u22122) (\u00b7 \u00b7 \u00b7f (1) (I))),(1)\nwhere L stands for the number of layers, and \u03b8 denotes the weights of the neural network. The parameters of the model are the weights for the neurons, which need to be optimized before the online deployment. The optimal weights are usually learned on a training set, with known desired outputs. The architecture of the OFDM system with deep learning based channel estimation and signal detection is illustrated in Fig. 2. The baseband OFDM system is the same as the conventional ones. On the transmitter side, the transmitted symbols inserted with pilots are first converted to a paralleled data stream, then the inverse discrete Fourier transform (IDFT) is used to convert the signal from the frequency domain to the time domain. After that, a cyclic prefix (CP) is inserted to mitigate the inter-symbol interference (ISI). The length of the CP should be no shorter than the max delay spread of the channel.\n\n\nB. System Architecture\n\nWe consider a sample-spaced multi-path channel described by complex random variables {h(n)} N \u22121 n=0 . Thus the received signal, y(n), can be expressed as\ny(n) = x(n) \u2297 h(n) + w(n),(2)\nwhere \u2297 denotes the circular convolution while x(n) and w(n) represent the transmitted signal and the additive white Gaussian noise (AWGN), respectively. After removing the CP and performing DFT, the received frequency domain signal is\nY (k) = X(k)H(k) + W (k),(3)\nwhere Y (k), X(k), H(k), and W (k) are the DFT of y(n), x(n), h(n) and w(n), respectively. We assume that the pilot symbols are in the first OFDM block while the following OFDM blocks consist of the transmitted data. Together they form a frame. The channel can be treated as constant spanning over the pilot block and the data blocks, but change from one frame to another. The DNN model takes as input the received data consisting of one pilot block and one data block in our initial study, and recovers the transmitted data in an end-to-end manner.\n\nAs shown in Fig. 2, to obtain an effective DNN model for joint channel estimation and symbol detection, two stages are included. In the offline training stage, the model is trained with the received OFDM samples that are generated with variant information sequences and under diverse channel conditions with certain statistical properties, such as typical urban or hilly terrain delay profile. In the online deployment stage, the DNN model generates the output that recovers the transmitted data without explicitly estimating the wireless channel.\n\n\nC. Model Training\n\nThe models are trained by viewing OFDM modulation and the wireless channels as black boxes. Historically, researchers have developed many channel models for CSI that well describe the real channels in terms of channel statistics. With these channel models, the training data can be obtained by simulation. In each simulation, a random data sequence is first generated as the transmitted symbols and the correspondent OFDM frame is formed with pilot symbols. The current random channel state is simulated based on the channel models. The received OFDM signal is obtained based on the OFDM frames undergoing the current channel distortion, including the channel noise. The received signal and the original transmitted data are collected as the training data. The model is trained to minimize the difference between the output of the neural network and the transmitted data. The difference can be portrayed in several ways. In our experiment settings, we choose the L 2 loss,\nL 2 = 1 N k (X(k) \u2212 X(k)) 2 ,(4)\nwhereX(k) is the prediction and X(k) is the supervision message, which is the transmitted symbols in this situation.  corresponds to the number of real parts and imaginary parts of 2 OFDM blocks that contain the pilots and transmitted symbols, respectively. Every 16 bits of the transmitted data are grouped and predicted based on a single model trained independently, which is then concatenated for the final output. The Relu function is used as the activation function in most layers except in the last layer where the Sigmoid function is applied to map the output to the interval [0, 1].\n\n\nIII. SIMULATION RESULTS\n\nWe have conducted several experiments to demonstrate the performance of the deep learning methods for joint channel estimation and symbol detection in OFDM wireless communication systems. A DNN model is trained based on simulation data, and is compared with the traditional methods in term of bit-error rates (BERs) under different signal-to-noise ratios (SNRs). In the following experiments, the deep learning based approach is proved to be more robust than LS and MMSE under scenarios where fewer training pilots are used, the CP is omitted, or there is nonlinear clipping noise. In our experiments, an OFDM system with 64 sub-carriers and the CP of length 16 is considered. The wireless channel follows the wireless world initiative for new radio model (WINNER II) [9], where the carrier frequency is 2.6 GHz, the number of paths is 24, and typical urban channels with max delay 16 are used. QPSK is used as the modulation method.\n\n\nA. Impact of Pilot Numbers\n\nThe proposed method is first compared with the LS and MMSE methods for channel estimation and detection, when 64 pilots are used for channel estimation in each frame. From  Fig. 3, the LS method has the worst performance since no prior statistics of the channel has been utilized in the detection. On the contrary, the MMSE method has the best performance because the second-order statistics of the channels are assumed to be known and used for symbol detection. The deep learning based approach has much better performance than the LS method and is comparable to the MMSE method.\n\nSince the channel model has a max delay of 16, it can be estimated with much fewer pilots, leading to better spectrum utilization. From Fig. 3, when only 8 pilots are used, the BER curves of the LS and MMSE methods saturate when SNR is above 10 dB while the deep learning based method still has the ability to reduce its BER with increasing SNR, which demonstrates that the DNN is robust to the number of pilots used for channel estimation. The reason for the superior performance of the DNN is that the CSI is not uniformly distributed. The characteristics of the wireless channels can be learned based on the training data generated from the model.\n\n\nB. Impact of CP\n\nAs indicated before, the CP is necessary to convert the linear convolution of the physical channel into circular convolution and mitigate ISI. But it costs time and energy for transmission. In this experiment, we will investigate the performance with CP remover. Fig. 4 illustrates the BER curves for an OFDM system without CP. From the figure, neither MMSE nor LS can effectively estimate channel. The accuracy tends to be saturated when SNR is over 15 dB. However, the deep learning method still works well. This result shows again that the characteristics of the wireless channel have been revealed and can be learned in the training stage by the DNNs.\n\n\nC. Impact of Clipping and Filtering Distortion\n\nAs indicated in [10], a notable drawback of OFDM is the high peak-to-average power ratio (PAPR). To reduce PAPR, the clipping and filtering approach serves as a simple and effective approach [10]. However, after clipping, nonlinear noise is introduced that could degrade the estimation and detection performance. The clipped signal become\u015d\nx(n) = x(n), if |x(n)| \u2264 A, Ae j\u03c6(n) , otherwise,(5)\nwhere A is the threshold and \u03c6(n) is the phase of x(n). Fig. 5 depicts the detection performance of the MMSE method and deep learning method when the OFDM system is contaminated with clipping noise. From the figure, when clipping ratio (CR = A/\u03c3, where \u03c3 is the rms of OFDM signal) is 1, the deep learning method is better than the MMSE method when SNR is over 15 dB, proving that deep learning method is more robust to the nonlinear clipping noise.   method when all above adversities are combined together, i.e., only 8 pilots are used, the CP is omitted, and there is clipping noise. From the figure, DNN is much better than the MMSE method but has a gap with detection performance under ideal circumstance, as we have seen before.\n\n\nD. Robustness Analysis\n\nIn the simulation above, the channels in the online deployment stage are generated with the same statistics that are used in the offline training stage. However, in real-world applications, mismatches may occur between the two stages. Therefore, it is essential for the trained models to be relatively robust to these mismatches. In this simulation, the impact of variation in statistics of channel models used during training and deployment stages is analyzed. Fig 7 shows the BER curves when the max delay and the number of paths in the test stage vary from the parameters used in the training stage described in the beginning of this section. From the figure, variations on statistics of channel models do no have significant damage on the performance of symbol detection.\n\n\nIV. CONCLUSIONS\n\nIn this article, we have demonstrated our initial efforts to employ DNNs for channel estimation and symbol detection in an OFDM system. The model is trained offline based on the simulated data that view OFDM and the wireless channels as black boxes. The simulation results show that the deep learning method has advantages when wireless channels are complicated by serious distortion and interference, which proves that DNNs have the ability to remember and analyze the complicated characteristics of the wireless channels. For realworld applications, it is important for the DNN model to have a good generalization ability so that it can still work effectively when the conditions of online deployment do not exactly agree with the channel models used in the training stage. An initial experiment has been conducted in this article to illustrate the generalization ability of DNN model with respect to some parameters of the channel model. More rigorous analysis and more comprehensive experiments are left for the future work. In addition, for practical use, samples generated from the real wireless channels could be collected to retrain or fine-tune the model for better performance.\n\nFig. 1 .\n1An example of deep learning models. may be the Sigmoid function, or the Relu function, defined as f S (a) = 1 1+e \u2212a , and f R (a) = max(0, a), respectively. Hence, the output of the network z is a cascade of nonlinear transformation of input data I, mathematically expressed as\n\nFig. 2 .\n2System model.\n\nFig. 3 .\n3BER curves of deep learning based approach and traditional methods.\n\nFig. 4 .\n4BER curves without CP.\n\n\nFig. 5. BER curves with clipping noise\n\nFig. 6 .\n6BER curves when combining all adversities.\n\nFig 6 compares\n6DNN with the MMSE\n\nFig. 7 .\n7BER curves with mismatch over training stage and deployment stage.\n\n\nin communication systems. With the improving computational resources on devices and the The authors are with the Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, 30332 USA e-mail: (yehao@gatech.edu; liye@ece.gatech.edu; juang@ece.gatech.edu).\n\n\nThe DNN model we use consists of five layers, three of which are hidden layers. The numbers of neurons in each layers are 256, 500, 250, 120, 16. The input number0 \n5 \n10 \n15 \n20 \n25 \n30 \nSNR (dB) \n\n10 4 \n\n10 3 \n\n10 2 \n\n10 1 \n\n10 0 \n\nBER \n\n64 Pilots Deep Learning \n64 Pilots LS \n64 Pilots MMSE \n8 Pilots Deep Learning \n8 Pilots LS \n8 Pilots MMSE \n\n\n\nDeep learning in neural networks: An overview. J Schmidhuber, Neural Networks. 61J. Schmidhuber, \"Deep learning in neural networks: An overview,\" Neural Networks, vol. 61, pp. 85-117, Jan. 2015.\n\nRobust channel estimation for OFDM systems with rapid dispersive fading channels. Y G Li, L J Cimini, N R Sollenberger, IEEE Trans. Commun. 467Y. G. Li, L. J. Cimini, and N. R. Sollenberger, \"Robust channel estimation for OFDM systems with rapid dispersive fading channels,\" IEEE Trans. Commun., vol. 46, no. 7, pp. 902\u00e2\u0202\u015e-915, Jul. 1998.\n\nCSI-based fingerprinting for indoor localization: A deep learning approach. X Wang, L Gao, S Mao, S Pandey, IEEE Trans. Veh. Technol. 661X. Wang, L. Gao, S. Mao and S. Pandey, 2017. \"CSI-based fingerprinting for indoor localization: A deep learning approach,\" IEEE Trans. Veh. Technol., vol. 66, no. 1, pp. 763-776, Jan. 2017.\n\nLearning to decode linear codes using deep learning. E Nachmani, Y Beery, D Burshtein, 54'th Annual Allerton Conf. On Commun., Control and Computing. E. Nachmani, Y. Beery, and D. Burshtein, \"Learning to decode linear codes using deep learning,\" 54'th Annual Allerton Conf. On Commun., Control and Computing, Mouticello, IL, Sept. 2016\n\nAdaptive equalization of finite nonlinear channels using multilayer perceptrons. S Chen, G Gibson, C Cown, P Grant, IEEE Trans. Signal Process. 202S. Chen, G. Gibson, C. Cown, and P. Grant, \"Adaptive equalization of finite nonlinear channels using multilayer perceptrons,\" IEEE Trans. Signal Process., vol. 20, no. 2, pp. 107-119, Jun. 1990.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. SystA. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" in Proc. Adv. Neural Inf. Process. Syst., 2012, pp. 1097-1105.\n\nLearning phrase representations using RNN encoderdecoder for statistical machine translation. K Cho, K. Cho et al. \"Learning phrase representations using RNN encoder- decoder for statistical machine translation.\" [Online]. Available: http://arxiv.org/abs/1406.1078 , 2014\n\nRecurrent deep neural networks for robust speech recognition. C Weng, D Yu, S Watanabe, B H F Juang, Proc. ICASSP. ICASSPC. Weng, D. Yu, S. Watanabe, and B. H. F. Juang, \"Recurrent deep neural networks for robust speech recognition,\" in Proc. ICASSP, May 2014, pp. 5532-5536.\n\nIST-4-027756 WINNER II D1.1.2 v.1.1: WINNER II Channel Models. P Kyosti, P. Kyosti, \"IST-4-027756 WINNER II D1.1.2 v.1.1: WINNER II Channel Models,\" 2007, [online] Available: http://www.ist-winner.org.\n\nEffects of clipping and filtering on the performance of OFDM. X D Li, L J CiminiJr, IEEE Comm. Lett. 25X. D. Li and L. J. Cimini Jr., \"Effects of clipping and filtering on the performance of OFDM,\" IEEE Comm. Lett., vol. 2, no. 5, pp. 131-133, May 1998\n", "annotations": {"author": "[{\"end\":104,\"start\":97},{\"end\":132,\"start\":105},{\"end\":168,\"start\":133}]", "publisher": null, "author_last_name": "[{\"end\":103,\"start\":101},{\"end\":131,\"start\":129},{\"end\":167,\"start\":162}]", "author_first_name": "[{\"end\":100,\"start\":97},{\"end\":125,\"start\":117},{\"end\":128,\"start\":126},{\"end\":156,\"start\":145},{\"end\":161,\"start\":157}]", "author_affiliation": null, "title": "[{\"end\":83,\"start\":1},{\"end\":251,\"start\":169}]", "venue": null, "abstract": "[{\"end\":1605,\"start\":265}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2330,\"start\":2327},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2843,\"start\":2840},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2869,\"start\":2866},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2895,\"start\":2892},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4920,\"start\":4917},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4953,\"start\":4950},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4977,\"start\":4974},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5076,\"start\":5073},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10453,\"start\":10450},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12624,\"start\":12620},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12799,\"start\":12795}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16030,\"start\":15741},{\"attributes\":{\"id\":\"fig_1\"},\"end\":16055,\"start\":16031},{\"attributes\":{\"id\":\"fig_2\"},\"end\":16134,\"start\":16056},{\"attributes\":{\"id\":\"fig_3\"},\"end\":16168,\"start\":16135},{\"attributes\":{\"id\":\"fig_4\"},\"end\":16209,\"start\":16169},{\"attributes\":{\"id\":\"fig_5\"},\"end\":16263,\"start\":16210},{\"attributes\":{\"id\":\"fig_6\"},\"end\":16298,\"start\":16264},{\"attributes\":{\"id\":\"fig_7\"},\"end\":16376,\"start\":16299},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":16669,\"start\":16377},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":17020,\"start\":16670}]", "paragraph": "[{\"end\":2094,\"start\":1624},{\"end\":2564,\"start\":2096},{\"end\":3012,\"start\":2566},{\"end\":3583,\"start\":3014},{\"end\":4433,\"start\":3585},{\"end\":4751,\"start\":4486},{\"end\":5077,\"start\":4780},{\"end\":5498,\"start\":5079},{\"end\":6461,\"start\":5553},{\"end\":6642,\"start\":6488},{\"end\":6908,\"start\":6673},{\"end\":7487,\"start\":6938},{\"end\":8036,\"start\":7489},{\"end\":9030,\"start\":8058},{\"end\":9654,\"start\":9064},{\"end\":10615,\"start\":9682},{\"end\":11226,\"start\":10646},{\"end\":11878,\"start\":11228},{\"end\":12553,\"start\":11898},{\"end\":12943,\"start\":12604},{\"end\":13731,\"start\":12997},{\"end\":14533,\"start\":13758},{\"end\":15740,\"start\":14553}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5552,\"start\":5499},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6672,\"start\":6643},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6937,\"start\":6909},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9063,\"start\":9031},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12996,\"start\":12944}]", "table_ref": null, "section_header": "[{\"end\":1622,\"start\":1607},{\"end\":4484,\"start\":4436},{\"end\":4778,\"start\":4754},{\"end\":6486,\"start\":6464},{\"end\":8056,\"start\":8039},{\"end\":9680,\"start\":9657},{\"end\":10644,\"start\":10618},{\"end\":11896,\"start\":11881},{\"end\":12602,\"start\":12556},{\"end\":13756,\"start\":13734},{\"end\":14551,\"start\":14536},{\"end\":15750,\"start\":15742},{\"end\":16040,\"start\":16032},{\"end\":16065,\"start\":16057},{\"end\":16144,\"start\":16136},{\"end\":16219,\"start\":16211},{\"end\":16279,\"start\":16265},{\"end\":16308,\"start\":16300}]", "table": "[{\"end\":17020,\"start\":16834}]", "figure_caption": "[{\"end\":16030,\"start\":15752},{\"end\":16055,\"start\":16042},{\"end\":16134,\"start\":16067},{\"end\":16168,\"start\":16146},{\"end\":16209,\"start\":16171},{\"end\":16263,\"start\":16221},{\"end\":16298,\"start\":16281},{\"end\":16376,\"start\":16310},{\"end\":16669,\"start\":16379},{\"end\":16834,\"start\":16672}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5126,\"start\":5120},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5473,\"start\":5467},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5973,\"start\":5967},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7507,\"start\":7501},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10825,\"start\":10813},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11370,\"start\":11359},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12167,\"start\":12161},{\"end\":13059,\"start\":13053},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":14231,\"start\":14220}]", "bib_author_first_name": "[{\"end\":17070,\"start\":17069},{\"end\":17301,\"start\":17300},{\"end\":17303,\"start\":17302},{\"end\":17309,\"start\":17308},{\"end\":17311,\"start\":17310},{\"end\":17321,\"start\":17320},{\"end\":17323,\"start\":17322},{\"end\":17635,\"start\":17634},{\"end\":17643,\"start\":17642},{\"end\":17650,\"start\":17649},{\"end\":17657,\"start\":17656},{\"end\":17940,\"start\":17939},{\"end\":17952,\"start\":17951},{\"end\":17961,\"start\":17960},{\"end\":18305,\"start\":18304},{\"end\":18313,\"start\":18312},{\"end\":18323,\"start\":18322},{\"end\":18331,\"start\":18330},{\"end\":18632,\"start\":18631},{\"end\":18646,\"start\":18645},{\"end\":18659,\"start\":18658},{\"end\":18661,\"start\":18660},{\"end\":19007,\"start\":19006},{\"end\":19248,\"start\":19247},{\"end\":19256,\"start\":19255},{\"end\":19262,\"start\":19261},{\"end\":19274,\"start\":19273},{\"end\":19278,\"start\":19275},{\"end\":19526,\"start\":19525},{\"end\":19728,\"start\":19727},{\"end\":19730,\"start\":19729},{\"end\":19736,\"start\":19735},{\"end\":19738,\"start\":19737}]", "bib_author_last_name": "[{\"end\":17082,\"start\":17071},{\"end\":17306,\"start\":17304},{\"end\":17318,\"start\":17312},{\"end\":17336,\"start\":17324},{\"end\":17640,\"start\":17636},{\"end\":17647,\"start\":17644},{\"end\":17654,\"start\":17651},{\"end\":17664,\"start\":17658},{\"end\":17949,\"start\":17941},{\"end\":17958,\"start\":17953},{\"end\":17971,\"start\":17962},{\"end\":18310,\"start\":18306},{\"end\":18320,\"start\":18314},{\"end\":18328,\"start\":18324},{\"end\":18337,\"start\":18332},{\"end\":18643,\"start\":18633},{\"end\":18656,\"start\":18647},{\"end\":18668,\"start\":18662},{\"end\":19011,\"start\":19008},{\"end\":19253,\"start\":19249},{\"end\":19259,\"start\":19257},{\"end\":19271,\"start\":19263},{\"end\":19284,\"start\":19279},{\"end\":19533,\"start\":19527},{\"end\":19733,\"start\":19731},{\"end\":19745,\"start\":19739}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11715509},\"end\":17216,\"start\":17022},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14079739},\"end\":17556,\"start\":17218},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2098713},\"end\":17884,\"start\":17558},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":10064123},\"end\":18221,\"start\":17886},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":67069000},\"end\":18564,\"start\":18223},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":195908774},\"end\":18910,\"start\":18566},{\"attributes\":{\"id\":\"b6\"},\"end\":19183,\"start\":18912},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14635987},\"end\":19460,\"start\":19185},{\"attributes\":{\"id\":\"b8\"},\"end\":19663,\"start\":19462},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":521769},\"end\":19917,\"start\":19665}]", "bib_title": "[{\"end\":17067,\"start\":17022},{\"end\":17298,\"start\":17218},{\"end\":17632,\"start\":17558},{\"end\":17937,\"start\":17886},{\"end\":18302,\"start\":18223},{\"end\":18629,\"start\":18566},{\"end\":19245,\"start\":19185},{\"end\":19725,\"start\":19665}]", "bib_author": "[{\"end\":17084,\"start\":17069},{\"end\":17308,\"start\":17300},{\"end\":17320,\"start\":17308},{\"end\":17338,\"start\":17320},{\"end\":17642,\"start\":17634},{\"end\":17649,\"start\":17642},{\"end\":17656,\"start\":17649},{\"end\":17666,\"start\":17656},{\"end\":17951,\"start\":17939},{\"end\":17960,\"start\":17951},{\"end\":17973,\"start\":17960},{\"end\":18312,\"start\":18304},{\"end\":18322,\"start\":18312},{\"end\":18330,\"start\":18322},{\"end\":18339,\"start\":18330},{\"end\":18645,\"start\":18631},{\"end\":18658,\"start\":18645},{\"end\":18670,\"start\":18658},{\"end\":19013,\"start\":19006},{\"end\":19255,\"start\":19247},{\"end\":19261,\"start\":19255},{\"end\":19273,\"start\":19261},{\"end\":19286,\"start\":19273},{\"end\":19535,\"start\":19525},{\"end\":19735,\"start\":19727},{\"end\":19749,\"start\":19735}]", "bib_venue": "[{\"end\":17099,\"start\":17084},{\"end\":17356,\"start\":17338},{\"end\":17690,\"start\":17666},{\"end\":18034,\"start\":17973},{\"end\":18365,\"start\":18339},{\"end\":18706,\"start\":18670},{\"end\":19004,\"start\":18912},{\"end\":19298,\"start\":19286},{\"end\":19523,\"start\":19462},{\"end\":19764,\"start\":19749},{\"end\":18734,\"start\":18708},{\"end\":19306,\"start\":19300}]"}}}, "year": 2023, "month": 12, "day": 17}