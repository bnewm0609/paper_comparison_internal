{"id": 215776078, "updated": "2022-01-27 02:34:27.259", "metadata": {"title": "BEDSR-Net: A Deep Shadow Removal Network From a Single Document Image", "authors": "[{\"middle\":[],\"last\":\"Lin\",\"first\":\"Yun-Hsuan\"},{\"middle\":[],\"last\":\"Chen\",\"first\":\"Wen-Chin\"},{\"middle\":[],\"last\":\"Chuang\",\"first\":\"Yung-Yu\"}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Removing shadows in document images enhances both the visual quality and readability of digital copies of documents. Most existing shadow removal algorithms for document images use hand-crafted heuristics and are often not robust to documents with different characteristics. This paper proposes the Background Estimation Document Shadow Removal Network (BEDSR-Net), the first deep network specifically designed for document image shadow removal. For taking advantage of specific properties of document images, a background estimation module is designed for extracting the global background color of the document. During the process of estimating the background color, the module also learns information about the spatial distribution of background and non-background pixels. We encode such information into an attention map. With the estimated global background color and attention map, the shadow removal network can better recover the shadow-free image. We also show that the model trained on synthetic images remains effective for real photos, and provide a large set of synthetic shadow images of documents along with their corresponding shadow-free images and shadow masks. Extensive quantitative and qualitative experiments on several benchmarks show that the BEDSR-Net outperforms existing methods in enhancing both the visual quality and readability of document images.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3034769575", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LinCC20", "doi": "10.1109/cvpr42600.2020.01292"}}, "content": {"source": {"pdf_hash": "2b1d92d732090c18fc335dc0edc8848665a4b99c", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1cbf0d89f3934788e70ecf1f37726aba8ef30ac1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2b1d92d732090c18fc335dc0edc8848665a4b99c.txt", "contents": "\nBEDSR-Net: A Deep Shadow Removal Network from a Single Document Image\n\n\nYun-Hsuan Lin \nNational Taiwan University\n\n\nWen-Chin Chen \nNational Taiwan University\n\n\nYung-Yu Chuang \nNational Taiwan University\n\n\nBEDSR-Net: A Deep Shadow Removal Network from a Single Document Image\n10.1109/CVPR42600.2020.01292\nRemoving shadows in document images enhances both the visual quality and readability of digital copies of documents. Most existing shadow removal algorithms for document images use hand-crafted heuristics and are often not robust to documents with different characteristics. This paper proposes the Background Estimation Document Shadow Removal Network (BEDSR-Net), the first deep network specifically designed for document image shadow removal. For taking advantage of specific properties of document images, a background estimation module is designed for extracting the global background color of the document. During the process of estimating the background color, the module also learns information about the spatial distribution of background and non-background pixels. We encode such information into an attention map. With the estimated global background color and attention map, the shadow removal network can better recover the shadow-free image. We also show that the model trained on synthetic images remains effective for real photos, and provide a large set of synthetic shadow images of documents along with their corresponding shadow-free images and shadow masks. Extensive quantitative and qualitative experiments on several benchmarks show that the BEDSR-Net outperforms existing methods in enhancing both the visual quality and readability of document images.\n\nIntroduction\n\nDocuments are indispensable and ubiquitous in our daily life. Examples include newspapers, receipts, papers, reports, and many others. There are often needs to obtain digital copies of documents. In the past, scanners were commonly used for digitizing documents with superior quality. Along with the prevalence of mobile phones and the improvement of their cameras, more and more people tend to use phone cameras in place of scanners for obtaining digital copies of documents because of their easy availability.\n\n(a) shadow image (b) ground truth (c) ours (d) Kligler [16] (e) Bako [1] (f) ST-CGAN [28] [1], and ST-CGAN [28], exhibit artifacts such as shadow edges (d), color washout (e) and residual shadows (f) in their results. Our result (c) has much fewer artifacts and is very close to the ground-truth shadow-free image (b).\n\nCompared with scanners, there are often two problems with capturing documents using phone cameras. First, the geometry of the document could be distorted and not rectangular due to the camera perspective. Besides, the document could be folded or curved. There exist methods for rectifying and unwarping the captured documents so that they become rectangular in shape [18,27,19]. Second, the captured document images are vulnerable to shadows because the light sources are often blocked by the camera or user's hand. Even without occluders, the illumination is often uneven over the document when taking pictures in the real world. Thus, the document images taken by phone cameras often exhibit shadows and uneven shading, leading to bad visual quality and readability. Users usually prefer documents with uniform illumination, similar to what they can obtain using scanners, which take pictures in a wellcontrolled lighting environment. This paper addresses the shadow removal problem of document images for improving the quality and legibility of the captured documents.\n\nShadow removal is an important computer vision problem because shadows often degrade the performance of vi-sion algorithms. Although most shadow removal methods are proposed for natural images, some are specifically designed for document images. Most existing document shadow removal algorithms use some heuristics for exploring specific characteristics of document images [3,30,22,21,1,16,15]. Unfortunately, due to limitations of the handcrafted heuristics, they often work well for some document images but fail for others. Thus, their results often exhibit different types of artifacts for different types of document images. Figure 1 gives an example, where Figure 1 (a) is the input shadow image and Figure 1(b) is the corresponding shadow-free image. Figure 1(d) shows the result of Kligler et al.'s method [16], where some shadows remain around the boundary between the shadow and nonshadow regions. Figure 1(e) shows the result of Bako et al.'s method [1], in which colors are washed out, and some light shadow edges remain in the deshadowed result.\n\nTo combat the problems of hand-crafted heuristics, recently, deep learning has been employed in many vision problems. However, it has not been explored for document shadow removal, although there are quite a few deeplearning-based shadow removal methods for natural images [23,28,13]. ST-CGAN is a state-of-the-art natural image shadow removal method [28]. Given a set of training triplets of shadow images, shadow-free images, and shadow masks, it uses an end-to-end architecture for performing the shadow detection and removal tasks simultaneously. In principle, shadow removal methods for natural images can also be used for document images. However, there are two issues with applying deep learning to document shadow removal. First, it requires a large set of paired document images for training. Second, the performance would be suboptimal since these methods do not take advantage of specific properties of document images. As an example, even after training with shadow/shadow-free pairs of document images along with shadow masks, ST-CGAN still fails to recover a proper shadow-free image, as shown in Figure 1(f). The shadow region remains, although it becomes lighter. Although recent shadow removal methods [6,17] perform better than ST-CGAN, they often use pre-trained models on ImageNet and do not consider the characteristics of document images. Thus, they share the same problems with ST-CGAN on document images. This paper proposes the first deep-learning-based approach for document image shadow removal. For addressing the first issue about training data, we propose to use synthetic images. This way, it is much easier to obtain a large-scale training set with great variations. Through extensive experiments, we show that the deep models trained on synthetic images remain effective for real-world images. For taking advantage of specific characteristics of document images, inspired by Bako et al. [1], we propose a network module for estimating the global background color of the document since most documents have a single background color, often the color of the paper. By exploring the global property, the background estimation module also discovers information about the locations of shadows in the form of an attention map. With both the estimated background color and the attention map, our shadow removal module can perform the shadow removal task much better. Extensive experiments show that our method not only outperforms existing methods in visual quality but also improves the readability of documents. As shown in Figure 1(c), our method is more robust with fewer artifacts. Our contributions include:\n\n\u2022 We propose the first deep learning approach for shadow removal of document images, which outperforms state-of-the-art methods. By exploring the specific properties of document images, our model estimates the background color and an attention map as the first step. The information proves useful in improving image quality and reducing model parameters. Also, by exploring the attention map, the proposed model does not require shadow masks for training, alleviating the effort for collecting training data and reducing the risk with inaccurate masks.\n\n\u2022 We provide a large-scale dataset of image triplets consisting of the shadow image, the corresponding shadow-free image, and shadow mask. The images are synthesized with a graphics renderer. The source code, datasets, and pre-trained models will be released.\n\n\u2022 We show that the deep model trained on synthetic images remain effective for real images via thorough experiments with real images of different characteristics, collected by different research groups.\n\n\nRelated work\n\n\nShadow removal for natural images\n\nFinlayson et al. proposed illumination invariant methods which remove shadows well for high-quality images [8,7]. Guo et al. proposed a method for removing shadows by finding the relation between shadow and non-shadow regions with similar materials and removing shadows by relighting [11]. Gong et al. presented an interactive approach for high-quality shadow removal with two rough user inputs [9]. Gryka et al. focused on removing soft shadows using a learning-based approach with user-provided strokes [10]. Recently, several deep-learning-based methods have been proposed for natural image shadow removal, and they achieve state-of-the-art performance in the area [23,28,13,17]. Qu et al. proposed the Deshad-owNet that harnesses multi-context information for removing shadows from images [23]. Hu   obtaining 2D spatial context from four directions. However, since these models [23,13] use VGG, pre-trained on natural images, as their backbone models, they are less suitable for document images. Wang et al. introduced ST-CGAN, which is trained to perform both the shadow detection and removal tasks simultaneously [28]. ST-CGAN uses the architecture of stacked conditional generative adversarial networks, where two cGANs would facilitate each other to improve the performance of both tasks. Self-supervised learning has also been introduced to shadow removal for natural images recently, such as ARGAN [6] and Mask-ShadowGAN [12]. Although effective for removing shadows in natural images, they are not specifically designed for document images. Thus, their performance on document image shadow removal is sub-optimal even after being retrained on document images, as shown in the experiments.\n\n\nShadow removal for document images\n\nSome methods have been designed specifically for shadow removal of document images. One thread of methods is based on the concept of intrinsic images and remove shadows through reducing the luminance contrast in regions with similar reflection components [29,3,30]. Jung et al. proposed the water-filling method inspired by the immersion process of a topographic surface with water [15]. The method however tends to cause color shift and have results much brighter than they should be. Kligler et al. proposed a method for enhancing the quality of document images by representing an image as a 3D point cloud and using visibility detection methods to choose pixels to recover [16]. However, there are often shadow edges remaining in their results. By assuming a constant background color, direct interpolation has been used for document image shadow re-moval by Oliveira et al. [22] and Bako et al. [1]. Bako et al.'s method obtains a shadow map by calculating the ratio between the global and local background colors for each patch and then adjusts the input shadow image with the shadow map [1]. Since these methods detect the background regions and perform interpolation in the rest, they fail when the documents contain large regions of figures or are covered by a large area of shadows. This paper proposes the first deep learning method for removing shadows from a single document image. By harnessing the power of data, our method is more robust than existing methods.\n\n\nMethod\n\nThis paper proposes BEDSR-Net (Background Estimation Document Shadow Removal Network) for removing shadows from a single document image. The training set,\nD={S i , N i } N i=1 , consists of N image pairs (S i , N i ) where S i is a shadow image while N i is its corresponding non- shadow image.\nAfter training, the BEDSR-Net forms a function \u03a8 BESR (S) which accepts a shadow image S and returns a predicted non-shadow image\u00d1 that approximates the real non-shadow image N . Figure 2 illustrates the architecture of the BEDSR-Net, consisting of two sub-networks, BE-Net (Background Estimation Network) and SR-Net (Shadow Removal Network). Given the input shadow document image S, the BE-Net, (b,H) = \u03a8 BE (S), estimates the global background color of the documentb and an estimated attention mapH which depicts the probability of each pixel belonging to the shadow-free background of the document. Given the shadow image S and the outputs of BE-Net, (b,H), the SR-Net,\u00d1 =\u03a8 SR (S, (b,H)), predicts the shadow-free image\u00d1 as the final output.\n\n\nBackground Estimation Network (BE-Net)\n\nInspired by Bako et al. [1], we also attempt to recover the background color of the document and uses it to assist the shadow removal process. Bako et al.'s method uses heuristics for estimating the background color by analyzing color distributions. It tends to cause a color shift or color fading when the document is covered by a large area of shadows or color figures. To address these problems, we use a deep network for a more robust estimation of the background color. The proposed BE-Net takes a shadow image S as the input and estimates the predicted background colorb.\n\nFor training the BE-Net, we need to identify the groundtruth background color b i for each document image pair (S i , N i ) in the training set D. It can be achieved by asking users to manually pick up a region belonging to the background in the non-shadow image N i and calculating the average color of the picked region. For relieving the manual effort, we used the following procedure for obtaining the ground-truth background b i automatically. First, we cluster pixels of the non-shadow image N i into two groups according to their intensity values. For clustering, we employed Gaussian mixture models (GMM) with Expectation Maximization (EM). The two groups often correspond to the content and the background, respectively. The background color of the document is often brighter. Thus, the cluster with a higher intensity is taken as the background cluster. Since the image N i contains no shadow, we can use the mean color of the background cluster as the background color b i for the i-th image pair in the training set. We found that the procedure works well empirically. Thus, for each image pair in the training set, we obtain its background color b i using this procedure. The procedure is for speeding up ground truth collection. The result can be corrected by users if the heuristic fails, for example, when the document has a dark background color. The brighter background color is not an assumption of our model itself.\n\nGiven the shadow image S i and its background color b i , we can train our BE-Net in a supervised manner by minimizing the following cost,\nL color = N i=1 b i \u2212 \u03a8 BE (S i ) 1 ,(1)\nso that the predicted background colorb i = \u03a8 BE (S i ) estimated from the shadow image S i approximates the true background color b i . As shown in Figure 2, our BE-Net consists of four convolution layers, followed by a global max pooling layer and a fully connected layer. The convolution layers extract proper features from the input shadow image. We adopt the global pooling mechanism to summarize each feature map into a value. By using global pooling to bridge the convolution layers and the fully connected layer, our network can deal with images with different sizes.\n\nAlong the way of estimating the background, BE-Net also learns knowledge about spatial distributions of shadowfree background and others, which provides additional cues for indicating potential locations of shadows. To utilize the information, we extract an attention map by applying the Grad-CAM method [25] to the feature map of the last convolution layer in BE-Net. As shown in Figure 2, the attention map does capture well where the shadow-free background (red color) and other (blue color) pixels reside in the shadow image. The inferred attention map also reveals cues about shadow locations and can play the role of the shadow mask. With its help, unlike other shadow removal networks such as ST-CGAN, our model does not require the ground-truth shadow masks for training. It provides the advantages of saving the effort for preparing shadow masks and avoiding the potential errors in the shadow masks. Note that a shadow mask is often derived from the shadow and non-shadow images using some heuristics since it cannot be captured directly. Thus, it could contain errors.\n\n\nShadow Removal Network (SR-Net)\n\nFor recovering a shadow-free image from a shadow image, we employ the conditional generative adversarial networks (cGANs) [20], which have been shown effective in many tasks such as image-to-image translation. A cGAN model consists of two players: a generator G and a discriminator D. Given a conditioning variable, the generator G aims to produce realistic images to fool the discriminator D, while the discriminator D attempts to distinguish the images generated by G from the real ones in the dataset. The competition enhances the generator in producing the result indistinguishable from real images.\n\nFor the generator G, we adopt the U-Net model [24], which is a fully convolutional neural network, consisting of an encoder and a decoder. The features from the decoder will be combined with those from the encoder through skip connections at each spatial resolution. We used a five-level hierarchy for both the encoder and decoder. The generator G takes the concatenation of the shadow image S i , the predicted background colorb i and the attention mapH i as input, and then predicts the non-shadow imag\u1ebd\nN i = G(S i ,b i ,H i ).\nFor the discriminator D, we employ Markovian discriminator (PatchGAN) [14]. The input of D is the 6-channel concatenation of a shadow image S i and the paired non-shadow image N i . For training the SR-Net, the following loss is used,\nL = \u03bb 1 L data + \u03bb 2 L GAN ,(2)\nwhere L data measures the deviation of the predicted nonshadow image from the real one,\nL data = E Si,Ni\u223cP data (Si,Ni) ||N i \u2212\u00d1 i ||.(3)\nDataset #pairs Characteristics Bako [1] 81 light shadows/text only Kligler [16] 300 dark shadows/colorful symbols Jung [15] 87 multi-cast shadows RDSRD 540 complex content/shadows SDSRD 8,309 synthetic, diverse lighting and contents and L GAN is the GAN loss for the competition of G and D,\nL GAN = E Si,Ni\u223cP data (Si,Ni) [log D(S i , N i )]+ E Si\u223cP data (Si) [log (1 \u2212 D(S i ,\u00d1 i ))].(4)\nAdam is used for optimization. The parameters are set empirically as \u03bb 1 =1 and \u03bb 2 =0.01. After training, the generator G is used to generate the output of SR-Net, i.e., \u03a8 SR \u2261 G.\n\n\nDatasets\n\nAlthough there exist a few datasets for document image shadow removal, they are only used for evaluation and of small scale. Table 1 summarizes datasets for document shadow removal. Previous datasets do not have a large number of images. Training deep models, however, requires sufficient data. Our model requires paired shadow and shadowfree images. Capturing such pairs in the real world is possible but time-consuming as it requires careful control. Also, limited by human effort, it is less likely to provide images with great variations in document contents, lighting conditions, and shadow complexity. Since current graphics algorithms already can render shadows realistically, we explore the possibility of using synthetic images for training.\n\n\nSynthetic document shadow removal dataset\n\nFor having a large set of document images with great variations, we synthesize document images using Blender [5] and Python scripts. For providing variations in document types and contents, we collected 970 document images, most from the PRImA Layout Analysis dataset [4]. For each document, we synthesized several shadow images using different lighting conditions and occluders. Since the images are synthesized, shadow-free images and shadow masks can be obtained with ease. We synthesized a total of 8,309 triplets of shadow images, shadow-free images, and shadow masks. They are divided into two groups, 7,533 for training and 776 for testing. We call it Synthetic Document Shadow Removal Dataset (SDSRD). The training set of SD-SRD is used for training BEDSR-Net. Note that training BEDSR-Net does not require shadow masks. We generate shadow masks because training ST-CGAN requires them. Figure 3 gives examples of SDSRD.  \n\n\nReal document shadow removal dataset\n\nFor evaluating on real images with more variations, we have also collected the Real Document Shadow Removal Dataset (RDSRD). The images were captured using Sony RX100 m3 and flashlights, all on tripods for ensuring fixed locations. The camera is triggered using a remote through WiFi to avoid touching the camera during capture. The dataset consists of 540 images of 25 documents, including paper, newspaper, and slides, under different lighting conditions and occluders. Figure 4 gives examples of RDSRD. This dataset is used only for evaluation.\n\n\nExperiments\n\nWe introduce compared methods and metrics, and then compare them on visual quality and content preservation. \n\n\nCompared methods and evaluation metrics\n\nWe compare our BEDSR-Net with four state-of-theart methods, including three traditional document shadow removal methods by Bako et al. [1], Kligler et al. [16] and Jung et al. [15], and one state-of-the-art deeplearning-based natural image shadow removal method, ST-CGAN [28]. For a fair comparison, we used the publicly available source codes provided by the authors whenever available. Among the four methods, ST-CGAN is the only one without source code released. Thus, we reproduce it on our own. The implementation has been validated using their dataset and reaches similar performance as reported in their paper. For showing the importance of the background estimation module, we incorporate our BE-Net into ST-CGAN and name it ST-CGAN-BE. Figure 5 shows its architecture. ST-CGAN has two generators, G 1 for shadow detection and G 2 for shadow removal. Training G 1 and G 2 requires shadow masks and shadow-free images, respectively. The estimated background color is fed to both generators in ST-CGAN-BE. Note that the attention map is not included in ST-CGAN-BE. All learning-based approaches are trained with the training set of SDSRD.\n\nWe evaluate the compared methods from two perspectives, visual quality, and content preservation. For visual quality, we use the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) index as the metrics. For evaluating content preservation, we test the performance of Optical Character Recognition (OCR) techniques on the recovered shadow-free images. In general, if the content is better recovered, OCR should be able to recognize more content. Table 2 reports quantitative comparisons of the compared methods on five datasets and the average in terms of PSNR and SSIM. Our BEDSR-Net outperforms others on most datasets. For RDSRD, SDSRD, and Kligler's datasets, our method reaches the best performance. In particular, our models beat other methods significantly on Kligler's dataset, which contains very dark shadows and color texts. For Bako's and Jung's datasets, their methods achieve the best performance. However, their performance on other datasets can be bad. For example, Bako et al.'s method performs poorly on Jung's dataset since it can only handle light shadows. On the other hand, Jung et al.'s method is the worst among all compared methods on Bako's dataset as it tends to wash out colors. These methods are derived from heuristics, and their datasets often better match the characteristics of their heuristics. Our model is very competitive, with only a small margin behind the best methods on Bako's and Jung's datasets. Overall, our method is more robust than previous methods as it provides stable and good results for images with different characteristics.\n\n\nQuantitative evaluation\n\nOur model is based on U-Net. As an ablation study, Table 2 reports the performance of U-Net trained on SDSRD with a supervised setting. BEDSR-Net outperforms U-Net by a large margin, showing our performance comes from more than the architecture of U-Net and training data. As another ablation study, the superior performance of our ST-CGAN-BE compared to ST-CGAN shows the importance of the background estimation module. Also, the notable performance gain from ST-CGAN-BE to BEDSR-Net indicates that the predicted attention map provides more useful information than the shadow masks generated by the first generator of ST-CGAN-BE. Finally, BEDSR-Net achieves better performance than ST-CGAN with fewer parameters, 19.8M for BEDSR-Net, and 38.9M for ST-CGAN.\n\n\nQualitative evaluation on visual quality\n\nFor visual comparisons, Figure 6 shows several shadow removal results of the compared methods. Although Bako et al.'s method performs well in quantitative evaluation, it fails to recover the image with color texts (example #7) or large area of shadows (example #3). Both Bako et al.'s method and Kligler et al.'s method exhibit remaining shadow edges when there are strong shadows (example #3 and #4). Jung's method often incurs a severe color shift in the results. Its results are often dramatically brighter than the ground-truth shadow-free images. The color is washed out, and the contrast is reduced. ST-CGAN runs into problems when there are large dark shadows (example #3 and #4).\n\nAlthough our model is derived from the assumption of single dominant background color, its utility is not as restricted as it appears. Since the whole document is captured as an image as a whole, there is no clear distinction between the content and background. Taking example #7 of Figure 6 as an example, it can be interpreted in two ways: (1) ten colorful numbers and a color gradient area on a white paper or (2) ten colorful numbers on a white paper with color gradients. For the second interpretation, there are multiple background colors, and our method still obtains a good result. As long as there is a dominant uniform color in the document image, our method can still work well. We argue that documents with such a characteristic represent a significant portion of the real world. As evidence, we extensively tested our method on existing document datasets, indepen-   Table 2. Quantitative comparisons of visual quality using PSNR and SSIM. We compare our models, BEDSR-Net and ST-CGAN-BE, with four competitive methods. The best scores of each dataset are marked in red bold, while the second best ones are marked in blue.\n\n(a) shadow image (b) our result dently collected by several groups. Our method obtains excellent performance on all datasets, even if most of them are not collected by us. Figure 7 gives an example with a tan background and a substantial figure. Our method could run into problems if there is no single dominant color, such as a paper entirely with a color gradient. However, it is a rare case, and most existing methods could also fail. Also, our method could fail when the document is entirely in shadow, or there are complicated shadows cast by multiple lights. Figure 8(a) gives a document image captured by a mobile phone in an uncontrolled environment. Figure 8(b) shows the estimated background color. Figure 8(c) displays the predicted attention map in which red color indicates shadow-free background while blue color denotes shadowed background and non-background regions. Both faithfully capture the real characteristics of the input image. With their help, BEDSR-Net successfully recovers the shadow-free image in Figure 8(d).\n\n\nEvaluation on content preservation\n\nWe also evaluate how the readability of documents is enhanced by reporting OCR performance on the recovered shadow-free images. In the experiment, 188 images with texts are used. First, we apply an open-source OCR tool [26] to recognize texts for ground-truth shadow-free images and results of compared methods. Then, we measure the OCR performance by comparing the text strings using the Levenshtein distance, also known as edit-distance.   [15], ST-CGAN [28], and the proposed BEDSR-Net.\n\nAs reported in Table 3, BEDSR-Net outperforms others, showing that it enhances not only visual quality but also the readability of documents by better preserving structure and content. Note that the test is for validating how our method preserves the content and improves document readability, rather than reaching the state-of-the-art OCR performance.\n\n\nConclusion\n\nThis paper proposes BEDSR-Net, the first deep learning model for shadow removal of document images. For exploring specific properties of documents, we propose BE-Net for background color estimation. It also generates an attention map, which is shown effective in indicating shadow locations. With the help of the estimated background color and attention map, our model achieves state-of-the-art performance in visual quality. It also improves the readability of document images. For training with document images with great diversity, we train our model using a synthetic dataset and show that the trained model works well for real images. In the future, we would like to explore unpaired training, handling documents with more complex backgrounds, and applying the background estimation module to document layout recognition.\n\nFigure 1 .\n1An example of document shadow removal. Previous methods, Kligler et al.'s method [16], Bako et al.'s method\n\nFigure 2 .\n2The architecture of the BEDSR-Net. It consists of two sub-networks: BE-Net for estimating the global background color of the document and SR-Net for removing shadows. Given the input shadow image, BE-Net predicts the background color. As a side product, it generates an attention map, which depicts how likely each pixel belongs to the shadow-free background. With the help of the attention map, our model removes the typical requirement of ground-truth shadow masks for training. Along with the input shadow image, the estimated background color and the attention map are fed into the SR-Net for determining the shadow-free version of the input shadow image.\n\nFigure 3 .\n3Example triplets from SDSRD. It provides images with complex shadows in both shape and intensity. From top to bottom, the images are shadow-free images, shadow images, and shadow masks, respectively.\n\nFigure 4 .\n4Example triplets from RDSRD. The images contain intricate shadows in terms of shape. From top to bottom, the images are shadow-free images, shadow images, and shadow masks.\n\nFigure 5 .\n5The architecture of ST-CGAN-BE.\n\nFigure 6 .\n6Visual comparison of competing methods: Bako[1], Kligler[16], Jung[15], ST-CGAN[28], our ST-CGAN-BE and BEDSR-Net, on ten images in which (1)-(2) are from Bako, (3)-(7) from Kligler, (8) from Jung and (9)-(10) from the testing set of SDSRD.\n\nFigure 7 .\n7An example with a tan background and a large figure.\n\nFigure 8 .\n8A real example taken using a phone camera under an uncontrolled environment. Our BEDSR-Net recovers the shadow-free image well and the attention map indicates background and non-background pixels very well. method input Bako Kligler Jung ST-CGAN BEDSR-Net distance 551.9 50.2 93.2 92.5 133.1 38.5\n\n\net al. proposed the direction-aware spatial context (DSC) module[13] that applies the spatial Recurrent Neural Network model[2] forShadow Removal Network (SR-Net) \n\nAttention map \n\nConv layer \n\nGradient map \n\nBackground color \n\nShadow image \nNon-shadow image \n\n...... \n\nBackground Estimation Network \n\nU-Net Generator \n\n\n\nTable 1 .\n1Overview of the document shadow removal datasets:Bako [1], Kligler [16], Jung [15], our RDSRD and SDSRD, in \nterms of numbers of image pairs and characteristics. \n\n\n\n\nAverage SDSRD RDSRD Bako's dataset Kligler's dataset Jung's dataset PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM input shadow images 22.03 0.8652 22.80 0.8992 21.73 0.8093 28.45 0.9742 19.31 0.8429 20.35 0.8850Bako [1] \n30.01 0.9231 31.55 0.9658 28.24 0.8664 35.22 0.9823 29.66 0.9051 23.70 0.9015 \nKligler [16] \n23.25 0.8081 22.03 0.8435 22.53 0.7056 26.50 0.8381 26.45 0.8481 24.45 0.8332 \nJung [15] \n17.04 0.7990 17.06 0.8226 14.45 0.7054 13.88 0.8059 19.21 0.8724 28.49 0.9108 \nU-Net [24] \n29.47 0.8985 33.63 0.9728 28.35 0.8676 26.68 0.8833 23.33 0.7829 23.09 0.8399 \nST-CGAN [28] \n33.14 0.9408 39.38 0.9834 30.31 0.9016 29.12 0.9600 25.92 0.9062 23.71 0.9046 \nST-CGAN-BE \n36.77 0.9521 42.98 0.9938 32.32 0.9054 33.90 0.9801 32.50 0.9338 26.45 0.9080 \nBEDSR-Net \n37.55 0.9534 43.59 0.9935 33.48 0.9084 35.07 0.9809 32.90 0.9354 27.23 0.9115 \n\n\n\nTable 3 .\n3Average edit-distances of the input images, Bako et al.'s method [1], Kligler et al.'s method [16], Jung et al.'s method\n\nRemoving shadows from images of documents. Steve Bako, Soheil Darabi, Eli Shechtman, Jue Wang, Kalyan Sunkavalli, Pradeep Sen, Proceedings of Asian Conference on Computer Vision (ACCV). Asian Conference on Computer Vision (ACCV)Steve Bako, Soheil Darabi, Eli Shechtman, Jue Wang, Kalyan Sunkavalli, and Pradeep Sen. Removing shadows from images of documents. In Proceedings of Asian Confer- ence on Computer Vision (ACCV), pages 173-183, 2016.\n\nInside-Outside Net: Detecting objects in context with skip pooling and recurrent neural networks. Sean Bell, Lawrence Zitnick, Kavita Bala, Ross Girshick, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Sean Bell, C Lawrence Zitnick, Kavita Bala, and Ross Gir- shick. Inside-Outside Net: Detecting objects in context with skip pooling and recurrent neural networks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), pages 2874-2883, 2016.\n\nGeometric and shading correction for images of printed materials using boundary. Michael S Brown, Yau-Chat Tsoi, IEEE Transactions on Image Processing. 156Michael S. Brown and Yau-Chat Tsoi. Geometric and shad- ing correction for images of printed materials using bound- ary. IEEE Transactions on Image Processing, 15(6):1544- 1554, 2006.\n\nICDAR2017 competition on recognition of documents with complex layouts-RDCL2017. Christian Clausner, Apostolos Antonacopoulos, Stefan Pletschacher, Proceedings of 14th International Conference on Document Analysis and Recognition (ICDAR). 14th International Conference on Document Analysis and Recognition (ICDAR)1Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. ICDAR2017 competition on recognition of documents with complex layouts-RDCL2017. In Proceed- ings of 14th International Conference on Document Analy- sis and Recognition (ICDAR), volume 1, pages 1404-1410, 2017.\n\nBlender -a 3D modelling and rendering package. Blender Online Community, Blender Foundation. Blender Online Community. Blender -a 3D modelling and rendering package. Blender Foundation, 2018.\n\nARGAN: Attentive recurrent generative adversarial network for shadow detection and removal. Bin Ding, Chengjiang Long, Ling Zhang, Chunxia Xiao, Proceedings of IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)Bin Ding, Chengjiang Long, Ling Zhang, and Chunxia Xiao. ARGAN: Attentive recurrent generative adversarial network for shadow detection and removal. In Proceedings of IEEE International Conference on Computer Vision (ICCV), pages 10213-10222, 2019.\n\nEntropy minimization for shadow removal. Graham D Finlayson, Mark S Drew, Cheng Lu, International Journal of Computer Vision. 851Graham D. Finlayson, Mark S. Drew, and Cheng Lu. En- tropy minimization for shadow removal. International Jour- nal of Computer Vision, 85(1):35-57, 2009.\n\nOn the removal of shadows from images. Graham D Finlayson, Steven D Hordley, Cheng Lu, Mark S Drew, IEEE Transactions on Pattern Analysis and Machine Intelligence. 281Graham D. Finlayson, Steven D. Hordley, Cheng Lu, and Mark S. Drew. On the removal of shadows from images. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 28(1):59-68, 2005.\n\nInteractive shadow removal and ground truth for variable scene categories. Han Gong, Darren Cosker, Proceedings of British Machine Vision Conference (BMVC). British Machine Vision Conference (BMVC)Han Gong and Darren Cosker. Interactive shadow removal and ground truth for variable scene categories. In Proceed- ings of British Machine Vision Conference (BMVC), 2014.\n\nLearning to remove soft shadows. Maciej Gryka, Michael Terry, Gabriel J Brostow, ACM Transactions on Graphics (TOG). 345153Maciej Gryka, Michael Terry, and Gabriel J. Brostow. Learn- ing to remove soft shadows. ACM Transactions on Graphics (TOG), 34(5):153, 2015.\n\nPaired regions for shadow detection and removal. Ruiqi Guo, Qieyun Dai, Derek Hoiem, IEEE Transactions on Pattern Analysis and Machine Intelligence. 35Ruiqi Guo, Qieyun Dai, and Derek Hoiem. Paired regions for shadow detection and removal. IEEE Transactions on Pat- tern Analysis and Machine Intelligence, 35(12):2956-2967, 2012.\n\nMask-ShadowGAN: Learning to remove shadows from unpaired data. Xiaowei Hu, Yitong Jiang, Chi-Wing Fu, Pheng-Ann Heng, Proceedings of IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)Xiaowei Hu, Yitong Jiang, Chi-Wing Fu, and Pheng-Ann Heng. Mask-ShadowGAN: Learning to remove shadows from unpaired data. In Proceedings of IEEE International Conference on Computer Vision (ICCV), pages 2472-2481, 2019.\n\nDirection-aware spatial context features for shadow detection. Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, Pheng-Ann Heng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, and Pheng- Ann Heng. Direction-aware spatial context features for shadow detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 7454-7462, 2018.\n\nImage-to-image translation with conditional adversarial networks. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adver- sarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1125-1134, 2017.\n\nWater-filling: An efficient algorithm for digitized document shadow removal. Seungjun Jung, Muhammad Abul Hasan, Changick Kim, Proceedings of Asian Conference on Computer Vision (ACCV). Asian Conference on Computer Vision (ACCV)Seungjun Jung, Muhammad Abul Hasan, and Changick Kim. Water-filling: An efficient algorithm for digitized doc- ument shadow removal. In Proceedings of Asian Conference on Computer Vision (ACCV), pages 398-414, 2018.\n\nDocument enhancement using visibility detection. Netanel Kligler, Sagi Katz, Ayellet Tal, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Netanel Kligler, Sagi Katz, and Ayellet Tal. Document en- hancement using visibility detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), pages 2374-2382, 2018.\n\nShadow removal via shadow image decomposition. Hieu Le, Dimitris Samaras, Proceedings of IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)Hieu Le and Dimitris Samaras. Shadow removal via shadow image decomposition. In Proceedings of IEEE International Conference on Computer Vision (ICCV), pages 8578-8587, 2019.\n\nPerspective rectification of document images using fuzzy set and morphological operations. Shijian Lu, Ben M Chen, Chi Chung Ko, Image and Vision Computing. 235Shijian Lu, Ben M. Chen, and Chi Chung Ko. Perspec- tive rectification of document images using fuzzy set and morphological operations. Image and Vision Computing, 23(5):541-553, May 2005.\n\nDocUNet: Document image unwarping via a stacked U-Net. Ke Ma, Zhixin Shu, Xue Bai, Jue Wang, Dimitris Samaras, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Ke Ma, Zhixin Shu, Xue Bai, Jue Wang, and Dimitris Sama- ras. DocUNet: Document image unwarping via a stacked U-Net. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nMehdi Mirza, Simon Osindero, arXiv:1411.1784Conditional generative adversarial nets. arXiv preprintMehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014.\n\nA new method for shading removal and binarization of documents acquired with portable digital cameras. Marques Daniel, Rafael Dueire Oliveira, Lins, Proceedings of Third International Workshop on Camera-Based Document Analysis and Recognition. Third International Workshop on Camera-Based Document Analysis and RecognitionDaniel Marques Oliveira and Rafael Dueire Lins. A new method for shading removal and binarization of documents acquired with portable digital cameras. In Proceedings of Third International Workshop on Camera-Based Document Analysis and Recognition, pages 3-10, 2009.\n\nShading removal of illustrated documents. Daniel Marques, Rafael Dueire Oliveira, Gabriel Lins, De Fran\u00e7a Pereira E Silva, Proceedings of International Conference on Image Analysis and Recognition (ICIAR). International Conference on Image Analysis and Recognition (ICIAR)Daniel Marques Oliveira, Rafael Dueire Lins, and Gabriel de Fran\u00e7a Pereira e Silva. Shading removal of illustrated docu- ments. In Proceedings of International Conference on Image Analysis and Recognition (ICIAR), pages 308-317, 2013.\n\nDeshadowNet: A multi-context embedding deep network for shadow removal. Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, Rynson W H Lau, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Liangqiong Qu, Jiandong Tian, Shengfeng He, Yandong Tang, and Rynson W.H. Lau. DeshadowNet: A multi-context embedding deep network for shadow removal. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4067-4075, 2017.\n\nU-Net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Proceedings of International Conference on Medical image Computing and Computer-Assisted Intervention (MICCAI). International Conference on Medical image Computing and Computer-Assisted Intervention (MICCAI)SpringerOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U- Net: Convolutional networks for biomedical image segmen- tation. In Proceedings of International Conference on Med- ical image Computing and Computer-Assisted Intervention (MICCAI), pages 234-241. Springer, 2015.\n\nGrad-CAM: Visual explanations from deep networks via gradient-based localization. R Ramprasaath, Michael Selvaraju, Abhishek Cogswell, Ramakrishna Das, Devi Vedantam, Dhruv Parikh, Batra, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE In- ternational Conference on Computer Vision (ICCV), pages 618-626, 2017.\n\nHybrid page layout analysis via tab-stop detection. W Raymond, Smith, Proceedings of the 10th International Conference on Document Analysis and Recognition (ICDAR). the 10th International Conference on Document Analysis and Recognition (ICDAR)Raymond W Smith. Hybrid page layout analysis via tab-stop detection. In Proceedings of the 10th International Confer- ence on Document Analysis and Recognition (ICDAR), pages 241-245, 2009.\n\nRectification and 3D reconstruction of curved document images. Yuandong Tian, G Srinivasa, Narasimhan, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Yuandong Tian and Srinivasa G. Narasimhan. Rectification and 3D reconstruction of curved document images. In Pro- ceedings of IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), 2011.\n\nStacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. Jifeng Wang, Xiang Li, Jian Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Jifeng Wang, Xiang Li, and Jian Yang. Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1788-1797, 2018.\n\nShadow removal using bilateral filtering. Qingxiong Yang, Kar-Han Tan, Narendra Ahuja, IEEE Transactions on Image processing. 2110Qingxiong Yang, Kar-Han Tan, and Narendra Ahuja. Shadow removal using bilateral filtering. IEEE Transactions on Image processing, 21(10):4361-4368, 2012.\n\nRemoving shading distortions in camera-based document images using inpainting and surface fitting with radial basis functions. Li Zhang, Andy M Yip, Chew Lim Tan, Proceedings of the 9th International Conference on Document Analysis and Recognition (ICDAR). the 9th International Conference on Document Analysis and Recognition (ICDAR)2Li Zhang, Andy M. Yip, and Chew Lim Tan. Removing shading distortions in camera-based document images using inpainting and surface fitting with radial basis functions. In Proceedings of the 9th International Conference on Docu- ment Analysis and Recognition (ICDAR), volume 2, pages 984-988, 2007.\n", "annotations": {"author": "[{\"start\":\"73\",\"end\":\"116\"},{\"start\":\"117\",\"end\":\"160\"},{\"start\":\"161\",\"end\":\"205\"}]", "publisher": null, "author_last_name": "[{\"start\":\"83\",\"end\":\"86\"},{\"start\":\"126\",\"end\":\"130\"},{\"start\":\"169\",\"end\":\"175\"}]", "author_first_name": "[{\"start\":\"73\",\"end\":\"82\"},{\"start\":\"117\",\"end\":\"125\"},{\"start\":\"161\",\"end\":\"168\"}]", "author_affiliation": "[{\"start\":\"88\",\"end\":\"115\"},{\"start\":\"132\",\"end\":\"159\"},{\"start\":\"177\",\"end\":\"204\"}]", "title": "[{\"start\":\"1\",\"end\":\"70\"},{\"start\":\"206\",\"end\":\"275\"}]", "venue": null, "abstract": "[{\"start\":\"305\",\"end\":\"1682\"}]", "bib_ref": "[{\"start\":\"2266\",\"end\":\"2270\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"2280\",\"end\":\"2283\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"2296\",\"end\":\"2300\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"2301\",\"end\":\"2304\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"2318\",\"end\":\"2322\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"2898\",\"end\":\"2902\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"2902\",\"end\":\"2905\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"2905\",\"end\":\"2908\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"3977\",\"end\":\"3980\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3980\",\"end\":\"3983\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"3983\",\"end\":\"3986\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"3986\",\"end\":\"3989\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"3989\",\"end\":\"3991\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"3991\",\"end\":\"3994\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"3994\",\"end\":\"3997\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"4418\",\"end\":\"4422\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"4565\",\"end\":\"4568\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"4937\",\"end\":\"4941\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"4941\",\"end\":\"4944\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"4944\",\"end\":\"4947\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"5015\",\"end\":\"5019\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"5883\",\"end\":\"5886\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"5886\",\"end\":\"5889\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"6584\",\"end\":\"6587\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"8481\",\"end\":\"8484\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"8484\",\"end\":\"8486\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"8658\",\"end\":\"8662\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"8769\",\"end\":\"8772\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"8879\",\"end\":\"8883\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"9042\",\"end\":\"9046\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"9046\",\"end\":\"9049\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"9049\",\"end\":\"9052\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"9052\",\"end\":\"9055\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"9057\",\"end\":\"9066\"},{\"start\":\"9167\",\"end\":\"9171\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"9173\",\"end\":\"9175\"},{\"start\":\"9257\",\"end\":\"9261\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"9261\",\"end\":\"9264\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"9494\",\"end\":\"9498\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"9783\",\"end\":\"9786\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"9806\",\"end\":\"9810\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"10368\",\"end\":\"10372\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"10372\",\"end\":\"10374\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"10374\",\"end\":\"10377\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"10379\",\"end\":\"10390\"},{\"start\":\"10495\",\"end\":\"10499\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"10789\",\"end\":\"10793\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"10991\",\"end\":\"10995\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"11012\",\"end\":\"11015\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"11206\",\"end\":\"11209\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"12705\",\"end\":\"12708\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"15758\",\"end\":\"15762\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"16691\",\"end\":\"16695\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"17220\",\"end\":\"17224\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"17775\",\"end\":\"17779\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"18146\",\"end\":\"18149\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"18185\",\"end\":\"18189\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"18229\",\"end\":\"18233\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"19597\",\"end\":\"19600\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"19756\",\"end\":\"19759\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"21309\",\"end\":\"21312\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"21329\",\"end\":\"21333\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"21350\",\"end\":\"21354\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"21445\",\"end\":\"21449\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"27864\",\"end\":\"27868\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"28087\",\"end\":\"28091\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"28101\",\"end\":\"28105\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"30625\",\"end\":\"30628\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"30637\",\"end\":\"30641\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"30647\",\"end\":\"30651\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"30660\",\"end\":\"30664\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"31264\",\"end\":\"31268\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"31324\",\"end\":\"31327\",\"attributes\":{\"ref_id\":\"b1\"}}]", "figure": "[{\"start\":\"29330\",\"end\":\"29450\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"29451\",\"end\":\"30123\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"30124\",\"end\":\"30336\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"30337\",\"end\":\"30522\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"30523\",\"end\":\"30567\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"30568\",\"end\":\"30821\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"30822\",\"end\":\"30887\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"30888\",\"end\":\"31197\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"31198\",\"end\":\"31520\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"31521\",\"end\":\"31697\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"31698\",\"end\":\"32563\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"32564\",\"end\":\"32696\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1698\",\"end\":\"2209\"},{\"start\":\"2211\",\"end\":\"2529\"},{\"start\":\"2531\",\"end\":\"3602\"},{\"start\":\"3604\",\"end\":\"4662\"},{\"start\":\"4664\",\"end\":\"7302\"},{\"start\":\"7304\",\"end\":\"7856\"},{\"start\":\"7858\",\"end\":\"8117\"},{\"start\":\"8119\",\"end\":\"8321\"},{\"start\":\"8374\",\"end\":\"10074\"},{\"start\":\"10113\",\"end\":\"11588\"},{\"start\":\"11599\",\"end\":\"11753\"},{\"start\":\"11894\",\"end\":\"12638\"},{\"start\":\"12681\",\"end\":\"13258\"},{\"start\":\"13260\",\"end\":\"14695\"},{\"start\":\"14697\",\"end\":\"14835\"},{\"start\":\"14877\",\"end\":\"15452\"},{\"start\":\"15454\",\"end\":\"16533\"},{\"start\":\"16569\",\"end\":\"17172\"},{\"start\":\"17174\",\"end\":\"17679\"},{\"start\":\"17705\",\"end\":\"17939\"},{\"start\":\"17972\",\"end\":\"18059\"},{\"start\":\"18110\",\"end\":\"18400\"},{\"start\":\"18499\",\"end\":\"18679\"},{\"start\":\"18692\",\"end\":\"19442\"},{\"start\":\"19488\",\"end\":\"20417\"},{\"start\":\"20458\",\"end\":\"21005\"},{\"start\":\"21021\",\"end\":\"21130\"},{\"start\":\"21174\",\"end\":\"22318\"},{\"start\":\"22320\",\"end\":\"23912\"},{\"start\":\"23940\",\"end\":\"24697\"},{\"start\":\"24742\",\"end\":\"25429\"},{\"start\":\"25431\",\"end\":\"26566\"},{\"start\":\"26568\",\"end\":\"27606\"},{\"start\":\"27645\",\"end\":\"28134\"},{\"start\":\"28136\",\"end\":\"28488\"},{\"start\":\"28503\",\"end\":\"29329\"}]", "formula": "[{\"start\":\"11754\",\"end\":\"11893\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"14836\",\"end\":\"14876\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"17680\",\"end\":\"17704\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"17940\",\"end\":\"17971\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"18060\",\"end\":\"18109\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"18401\",\"end\":\"18498\",\"attributes\":{\"id\":\"formula_5\"}}]", "table_ref": "[{\"start\":\"18817\",\"end\":\"18824\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"22780\",\"end\":\"22787\"},{\"start\":\"26311\",\"end\":\"26318\"},{\"start\":\"28151\",\"end\":\"28158\",\"attributes\":{\"ref_id\":\"tab_3\"}}]", "section_header": "[{\"start\":\"1684\",\"end\":\"1696\",\"attributes\":{\"n\":\"1.\"}},{\"start\":\"8324\",\"end\":\"8336\",\"attributes\":{\"n\":\"2.\"}},{\"start\":\"8339\",\"end\":\"8372\",\"attributes\":{\"n\":\"2.1.\"}},{\"start\":\"10077\",\"end\":\"10111\",\"attributes\":{\"n\":\"2.2.\"}},{\"start\":\"11591\",\"end\":\"11597\",\"attributes\":{\"n\":\"3.\"}},{\"start\":\"12641\",\"end\":\"12679\",\"attributes\":{\"n\":\"3.1.\"}},{\"start\":\"16536\",\"end\":\"16567\",\"attributes\":{\"n\":\"3.2.\"}},{\"start\":\"18682\",\"end\":\"18690\",\"attributes\":{\"n\":\"4.\"}},{\"start\":\"19445\",\"end\":\"19486\",\"attributes\":{\"n\":\"4.1.\"}},{\"start\":\"20420\",\"end\":\"20456\",\"attributes\":{\"n\":\"4.2.\"}},{\"start\":\"21008\",\"end\":\"21019\",\"attributes\":{\"n\":\"5.\"}},{\"start\":\"21133\",\"end\":\"21172\",\"attributes\":{\"n\":\"5.1.\"}},{\"start\":\"23915\",\"end\":\"23938\",\"attributes\":{\"n\":\"5.2.\"}},{\"start\":\"24700\",\"end\":\"24740\",\"attributes\":{\"n\":\"5.3.\"}},{\"start\":\"27609\",\"end\":\"27643\",\"attributes\":{\"n\":\"5.4.\"}},{\"start\":\"28491\",\"end\":\"28501\",\"attributes\":{\"n\":\"6.\"}},{\"start\":\"29331\",\"end\":\"29341\"},{\"start\":\"29452\",\"end\":\"29462\"},{\"start\":\"30125\",\"end\":\"30135\"},{\"start\":\"30338\",\"end\":\"30348\"},{\"start\":\"30524\",\"end\":\"30534\"},{\"start\":\"30569\",\"end\":\"30579\"},{\"start\":\"30823\",\"end\":\"30833\"},{\"start\":\"30889\",\"end\":\"30899\"},{\"start\":\"31522\",\"end\":\"31531\"},{\"start\":\"32565\",\"end\":\"32574\"}]", "table": "[{\"start\":\"31331\",\"end\":\"31520\"},{\"start\":\"31582\",\"end\":\"31697\"},{\"start\":\"31925\",\"end\":\"32563\"}]", "figure_caption": "[{\"start\":\"29343\",\"end\":\"29450\"},{\"start\":\"29464\",\"end\":\"30123\"},{\"start\":\"30137\",\"end\":\"30336\"},{\"start\":\"30350\",\"end\":\"30522\"},{\"start\":\"30536\",\"end\":\"30567\"},{\"start\":\"30581\",\"end\":\"30821\"},{\"start\":\"30835\",\"end\":\"30887\"},{\"start\":\"30901\",\"end\":\"31197\"},{\"start\":\"31200\",\"end\":\"31331\"},{\"start\":\"31533\",\"end\":\"31582\"},{\"start\":\"31700\",\"end\":\"31925\"},{\"start\":\"32576\",\"end\":\"32696\"}]", "figure_ref": "[{\"start\":\"4234\",\"end\":\"4242\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"4267\",\"end\":\"4275\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"4310\",\"end\":\"4321\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"4362\",\"end\":\"4373\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"4512\",\"end\":\"4520\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"5775\",\"end\":\"5783\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"7215\",\"end\":\"7223\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"12073\",\"end\":\"12081\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"15026\",\"end\":\"15034\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"15835\",\"end\":\"15843\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"20382\",\"end\":\"20390\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"20930\",\"end\":\"20938\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"21919\",\"end\":\"21927\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"24766\",\"end\":\"24774\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"25714\",\"end\":\"25722\",\"attributes\":{\"ref_id\":\"fig_5\"}},{\"start\":\"26740\",\"end\":\"26748\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"27133\",\"end\":\"27141\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"27227\",\"end\":\"27235\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"27277\",\"end\":\"27285\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"27594\",\"end\":\"27602\",\"attributes\":{\"ref_id\":\"fig_7\"}}]", "bib_author_first_name": "[{\"start\":\"32741\",\"end\":\"32746\"},{\"start\":\"32753\",\"end\":\"32759\"},{\"start\":\"32768\",\"end\":\"32771\"},{\"start\":\"32783\",\"end\":\"32786\"},{\"start\":\"32793\",\"end\":\"32799\"},{\"start\":\"32812\",\"end\":\"32819\"},{\"start\":\"33241\",\"end\":\"33245\"},{\"start\":\"33252\",\"end\":\"33260\"},{\"start\":\"33270\",\"end\":\"33276\"},{\"start\":\"33283\",\"end\":\"33287\"},{\"start\":\"33800\",\"end\":\"33807\"},{\"start\":\"33808\",\"end\":\"33809\"},{\"start\":\"33817\",\"end\":\"33825\"},{\"start\":\"34140\",\"end\":\"34149\"},{\"start\":\"34160\",\"end\":\"34169\"},{\"start\":\"34186\",\"end\":\"34192\"},{\"start\":\"34942\",\"end\":\"34945\"},{\"start\":\"34952\",\"end\":\"34962\"},{\"start\":\"34969\",\"end\":\"34973\"},{\"start\":\"34981\",\"end\":\"34988\"},{\"start\":\"35413\",\"end\":\"35419\"},{\"start\":\"35420\",\"end\":\"35421\"},{\"start\":\"35433\",\"end\":\"35437\"},{\"start\":\"35438\",\"end\":\"35439\"},{\"start\":\"35446\",\"end\":\"35451\"},{\"start\":\"35696\",\"end\":\"35702\"},{\"start\":\"35703\",\"end\":\"35704\"},{\"start\":\"35716\",\"end\":\"35722\"},{\"start\":\"35723\",\"end\":\"35724\"},{\"start\":\"35734\",\"end\":\"35739\"},{\"start\":\"35744\",\"end\":\"35748\"},{\"start\":\"35749\",\"end\":\"35750\"},{\"start\":\"36092\",\"end\":\"36095\"},{\"start\":\"36102\",\"end\":\"36108\"},{\"start\":\"36419\",\"end\":\"36425\"},{\"start\":\"36433\",\"end\":\"36440\"},{\"start\":\"36448\",\"end\":\"36455\"},{\"start\":\"36456\",\"end\":\"36457\"},{\"start\":\"36700\",\"end\":\"36705\"},{\"start\":\"36711\",\"end\":\"36717\"},{\"start\":\"36723\",\"end\":\"36728\"},{\"start\":\"37045\",\"end\":\"37052\"},{\"start\":\"37057\",\"end\":\"37063\"},{\"start\":\"37071\",\"end\":\"37079\"},{\"start\":\"37084\",\"end\":\"37093\"},{\"start\":\"37511\",\"end\":\"37518\"},{\"start\":\"37523\",\"end\":\"37526\"},{\"start\":\"37532\",\"end\":\"37540\"},{\"start\":\"37545\",\"end\":\"37549\"},{\"start\":\"37555\",\"end\":\"37564\"},{\"start\":\"38033\",\"end\":\"38040\"},{\"start\":\"38048\",\"end\":\"38055\"},{\"start\":\"38061\",\"end\":\"38068\"},{\"start\":\"38075\",\"end\":\"38081\"},{\"start\":\"38082\",\"end\":\"38083\"},{\"start\":\"38567\",\"end\":\"38575\"},{\"start\":\"38582\",\"end\":\"38590\"},{\"start\":\"38591\",\"end\":\"38595\"},{\"start\":\"38603\",\"end\":\"38611\"},{\"start\":\"38984\",\"end\":\"38991\"},{\"start\":\"39001\",\"end\":\"39005\"},{\"start\":\"39012\",\"end\":\"39019\"},{\"start\":\"39438\",\"end\":\"39442\"},{\"start\":\"39447\",\"end\":\"39455\"},{\"start\":\"39859\",\"end\":\"39866\"},{\"start\":\"39871\",\"end\":\"39874\"},{\"start\":\"39875\",\"end\":\"39876\"},{\"start\":\"39883\",\"end\":\"39886\"},{\"start\":\"39887\",\"end\":\"39892\"},{\"start\":\"40173\",\"end\":\"40175\"},{\"start\":\"40180\",\"end\":\"40186\"},{\"start\":\"40192\",\"end\":\"40195\"},{\"start\":\"40201\",\"end\":\"40204\"},{\"start\":\"40211\",\"end\":\"40219\"},{\"start\":\"40585\",\"end\":\"40590\"},{\"start\":\"40598\",\"end\":\"40603\"},{\"start\":\"40899\",\"end\":\"40906\"},{\"start\":\"40915\",\"end\":\"40928\"},{\"start\":\"41444\",\"end\":\"41450\"},{\"start\":\"41451\",\"end\":\"41457\"},{\"start\":\"41468\",\"end\":\"41475\"},{\"start\":\"41966\",\"end\":\"41976\"},{\"start\":\"41981\",\"end\":\"41989\"},{\"start\":\"41996\",\"end\":\"42005\"},{\"start\":\"42010\",\"end\":\"42017\"},{\"start\":\"42024\",\"end\":\"42030\"},{\"start\":\"42031\",\"end\":\"42034\"},{\"start\":\"42526\",\"end\":\"42530\"},{\"start\":\"42544\",\"end\":\"42551\"},{\"start\":\"42561\",\"end\":\"42567\"},{\"start\":\"43140\",\"end\":\"43141\"},{\"start\":\"43155\",\"end\":\"43162\"},{\"start\":\"43174\",\"end\":\"43182\"},{\"start\":\"43193\",\"end\":\"43204\"},{\"start\":\"43210\",\"end\":\"43214\"},{\"start\":\"43225\",\"end\":\"43230\"},{\"start\":\"43727\",\"end\":\"43728\"},{\"start\":\"44172\",\"end\":\"44180\"},{\"start\":\"44187\",\"end\":\"44188\"},{\"start\":\"44671\",\"end\":\"44677\"},{\"start\":\"44684\",\"end\":\"44689\"},{\"start\":\"44694\",\"end\":\"44698\"},{\"start\":\"45163\",\"end\":\"45172\"},{\"start\":\"45179\",\"end\":\"45186\"},{\"start\":\"45192\",\"end\":\"45200\"},{\"start\":\"45533\",\"end\":\"45535\"},{\"start\":\"45543\",\"end\":\"45547\"},{\"start\":\"45548\",\"end\":\"45549\"},{\"start\":\"45555\",\"end\":\"45563\"}]", "bib_author_last_name": "[{\"start\":\"32747\",\"end\":\"32751\"},{\"start\":\"32760\",\"end\":\"32766\"},{\"start\":\"32772\",\"end\":\"32781\"},{\"start\":\"32787\",\"end\":\"32791\"},{\"start\":\"32800\",\"end\":\"32810\"},{\"start\":\"32820\",\"end\":\"32823\"},{\"start\":\"33246\",\"end\":\"33250\"},{\"start\":\"33261\",\"end\":\"33268\"},{\"start\":\"33277\",\"end\":\"33281\"},{\"start\":\"33288\",\"end\":\"33296\"},{\"start\":\"33810\",\"end\":\"33815\"},{\"start\":\"33826\",\"end\":\"33830\"},{\"start\":\"34150\",\"end\":\"34158\"},{\"start\":\"34170\",\"end\":\"34184\"},{\"start\":\"34193\",\"end\":\"34205\"},{\"start\":\"34704\",\"end\":\"34728\"},{\"start\":\"34946\",\"end\":\"34950\"},{\"start\":\"34963\",\"end\":\"34967\"},{\"start\":\"34974\",\"end\":\"34979\"},{\"start\":\"34989\",\"end\":\"34993\"},{\"start\":\"35422\",\"end\":\"35431\"},{\"start\":\"35440\",\"end\":\"35444\"},{\"start\":\"35452\",\"end\":\"35454\"},{\"start\":\"35705\",\"end\":\"35714\"},{\"start\":\"35725\",\"end\":\"35732\"},{\"start\":\"35740\",\"end\":\"35742\"},{\"start\":\"35751\",\"end\":\"35755\"},{\"start\":\"36096\",\"end\":\"36100\"},{\"start\":\"36109\",\"end\":\"36115\"},{\"start\":\"36426\",\"end\":\"36431\"},{\"start\":\"36441\",\"end\":\"36446\"},{\"start\":\"36458\",\"end\":\"36465\"},{\"start\":\"36706\",\"end\":\"36709\"},{\"start\":\"36718\",\"end\":\"36721\"},{\"start\":\"36729\",\"end\":\"36734\"},{\"start\":\"37053\",\"end\":\"37055\"},{\"start\":\"37064\",\"end\":\"37069\"},{\"start\":\"37080\",\"end\":\"37082\"},{\"start\":\"37094\",\"end\":\"37098\"},{\"start\":\"37519\",\"end\":\"37521\"},{\"start\":\"37527\",\"end\":\"37530\"},{\"start\":\"37541\",\"end\":\"37543\"},{\"start\":\"37550\",\"end\":\"37553\"},{\"start\":\"37565\",\"end\":\"37569\"},{\"start\":\"38041\",\"end\":\"38046\"},{\"start\":\"38056\",\"end\":\"38059\"},{\"start\":\"38069\",\"end\":\"38073\"},{\"start\":\"38084\",\"end\":\"38089\"},{\"start\":\"38576\",\"end\":\"38580\"},{\"start\":\"38596\",\"end\":\"38601\"},{\"start\":\"38612\",\"end\":\"38615\"},{\"start\":\"38992\",\"end\":\"38999\"},{\"start\":\"39006\",\"end\":\"39010\"},{\"start\":\"39020\",\"end\":\"39023\"},{\"start\":\"39443\",\"end\":\"39445\"},{\"start\":\"39456\",\"end\":\"39463\"},{\"start\":\"39867\",\"end\":\"39869\"},{\"start\":\"39877\",\"end\":\"39881\"},{\"start\":\"39893\",\"end\":\"39895\"},{\"start\":\"40176\",\"end\":\"40178\"},{\"start\":\"40187\",\"end\":\"40190\"},{\"start\":\"40196\",\"end\":\"40199\"},{\"start\":\"40205\",\"end\":\"40209\"},{\"start\":\"40220\",\"end\":\"40227\"},{\"start\":\"40591\",\"end\":\"40596\"},{\"start\":\"40604\",\"end\":\"40612\"},{\"start\":\"40907\",\"end\":\"40913\"},{\"start\":\"40929\",\"end\":\"40937\"},{\"start\":\"40939\",\"end\":\"40943\"},{\"start\":\"41428\",\"end\":\"41442\"},{\"start\":\"41458\",\"end\":\"41466\"},{\"start\":\"41476\",\"end\":\"41480\"},{\"start\":\"41482\",\"end\":\"41507\"},{\"start\":\"41977\",\"end\":\"41979\"},{\"start\":\"41990\",\"end\":\"41994\"},{\"start\":\"42006\",\"end\":\"42008\"},{\"start\":\"42018\",\"end\":\"42022\"},{\"start\":\"42035\",\"end\":\"42038\"},{\"start\":\"42531\",\"end\":\"42542\"},{\"start\":\"42552\",\"end\":\"42559\"},{\"start\":\"42568\",\"end\":\"42572\"},{\"start\":\"43142\",\"end\":\"43153\"},{\"start\":\"43163\",\"end\":\"43172\"},{\"start\":\"43183\",\"end\":\"43191\"},{\"start\":\"43205\",\"end\":\"43208\"},{\"start\":\"43215\",\"end\":\"43223\"},{\"start\":\"43231\",\"end\":\"43237\"},{\"start\":\"43239\",\"end\":\"43244\"},{\"start\":\"43729\",\"end\":\"43736\"},{\"start\":\"43738\",\"end\":\"43743\"},{\"start\":\"44181\",\"end\":\"44185\"},{\"start\":\"44189\",\"end\":\"44198\"},{\"start\":\"44200\",\"end\":\"44210\"},{\"start\":\"44678\",\"end\":\"44682\"},{\"start\":\"44690\",\"end\":\"44692\"},{\"start\":\"44699\",\"end\":\"44703\"},{\"start\":\"45173\",\"end\":\"45177\"},{\"start\":\"45187\",\"end\":\"45190\"},{\"start\":\"45201\",\"end\":\"45206\"},{\"start\":\"45536\",\"end\":\"45541\"},{\"start\":\"45550\",\"end\":\"45553\"},{\"start\":\"45564\",\"end\":\"45567\"}]", "bib_entry": "[{\"start\":\"32698\",\"end\":\"33141\",\"attributes\":{\"matched_paper_id\":\"31459556\",\"id\":\"b0\"}},{\"start\":\"33143\",\"end\":\"33717\",\"attributes\":{\"matched_paper_id\":\"7148278\",\"id\":\"b1\"}},{\"start\":\"33719\",\"end\":\"34057\",\"attributes\":{\"matched_paper_id\":\"7378147\",\"id\":\"b2\"}},{\"start\":\"34059\",\"end\":\"34655\",\"attributes\":{\"matched_paper_id\":\"4763136\",\"id\":\"b3\"}},{\"start\":\"34657\",\"end\":\"34848\",\"attributes\":{\"id\":\"b4\"}},{\"start\":\"34850\",\"end\":\"35370\",\"attributes\":{\"matched_paper_id\":\"199442594\",\"id\":\"b5\"}},{\"start\":\"35372\",\"end\":\"35655\",\"attributes\":{\"matched_paper_id\":\"3353774\",\"id\":\"b6\"}},{\"start\":\"35657\",\"end\":\"36015\",\"attributes\":{\"matched_paper_id\":\"1800511\",\"id\":\"b7\"}},{\"start\":\"36017\",\"end\":\"36384\",\"attributes\":{\"matched_paper_id\":\"2937210\",\"id\":\"b8\"}},{\"start\":\"36386\",\"end\":\"36649\",\"attributes\":{\"matched_paper_id\":\"7785945\",\"id\":\"b9\"}},{\"start\":\"36651\",\"end\":\"36980\",\"attributes\":{\"matched_paper_id\":\"3201231\",\"id\":\"b10\"}},{\"start\":\"36982\",\"end\":\"37446\",\"attributes\":{\"matched_paper_id\":\"85518390\",\"id\":\"b11\"}},{\"start\":\"37448\",\"end\":\"37965\",\"attributes\":{\"matched_paper_id\":\"8182535\",\"id\":\"b12\"}},{\"start\":\"37967\",\"end\":\"38488\",\"attributes\":{\"matched_paper_id\":\"6200260\",\"id\":\"b13\"}},{\"start\":\"38490\",\"end\":\"38933\",\"attributes\":{\"matched_paper_id\":\"127988372\",\"id\":\"b14\"}},{\"start\":\"38935\",\"end\":\"39389\",\"attributes\":{\"matched_paper_id\":\"52839416\",\"id\":\"b15\"}},{\"start\":\"39391\",\"end\":\"39766\",\"attributes\":{\"matched_paper_id\":\"201645098\",\"id\":\"b16\"}},{\"start\":\"39768\",\"end\":\"40116\",\"attributes\":{\"matched_paper_id\":\"14897061\",\"id\":\"b17\"}},{\"start\":\"40118\",\"end\":\"40583\",\"attributes\":{\"matched_paper_id\":\"13787274\",\"id\":\"b18\"}},{\"start\":\"40585\",\"end\":\"40794\",\"attributes\":{\"id\":\"b19\",\"doi\":\"arXiv:1411.1784\"}},{\"start\":\"40796\",\"end\":\"41384\",\"attributes\":{\"matched_paper_id\":\"35573155\",\"id\":\"b20\"}},{\"start\":\"41386\",\"end\":\"41892\",\"attributes\":{\"matched_paper_id\":\"11534661\",\"id\":\"b21\"}},{\"start\":\"41894\",\"end\":\"42459\",\"attributes\":{\"matched_paper_id\":\"931642\",\"id\":\"b22\"}},{\"start\":\"42461\",\"end\":\"43056\",\"attributes\":{\"matched_paper_id\":\"3719281\",\"id\":\"b23\"}},{\"start\":\"43058\",\"end\":\"43673\",\"attributes\":{\"matched_paper_id\":\"15019293\",\"id\":\"b24\"}},{\"start\":\"43675\",\"end\":\"44107\",\"attributes\":{\"matched_paper_id\":\"206776838\",\"id\":\"b25\"}},{\"start\":\"44109\",\"end\":\"44559\",\"attributes\":{\"matched_paper_id\":\"206591483\",\"id\":\"b26\"}},{\"start\":\"44561\",\"end\":\"45119\",\"attributes\":{\"matched_paper_id\":\"413731\",\"id\":\"b27\"}},{\"start\":\"45121\",\"end\":\"45404\",\"attributes\":{\"matched_paper_id\":\"14275168\",\"id\":\"b28\"}},{\"start\":\"45406\",\"end\":\"46038\",\"attributes\":{\"matched_paper_id\":\"11121525\",\"id\":\"b29\"}}]", "bib_title": "[{\"start\":\"32698\",\"end\":\"32739\"},{\"start\":\"33143\",\"end\":\"33239\"},{\"start\":\"33719\",\"end\":\"33798\"},{\"start\":\"34059\",\"end\":\"34138\"},{\"start\":\"34657\",\"end\":\"34702\"},{\"start\":\"34850\",\"end\":\"34940\"},{\"start\":\"35372\",\"end\":\"35411\"},{\"start\":\"35657\",\"end\":\"35694\"},{\"start\":\"36017\",\"end\":\"36090\"},{\"start\":\"36386\",\"end\":\"36417\"},{\"start\":\"36651\",\"end\":\"36698\"},{\"start\":\"36982\",\"end\":\"37043\"},{\"start\":\"37448\",\"end\":\"37509\"},{\"start\":\"37967\",\"end\":\"38031\"},{\"start\":\"38490\",\"end\":\"38565\"},{\"start\":\"38935\",\"end\":\"38982\"},{\"start\":\"39391\",\"end\":\"39436\"},{\"start\":\"39768\",\"end\":\"39857\"},{\"start\":\"40118\",\"end\":\"40171\"},{\"start\":\"40796\",\"end\":\"40897\"},{\"start\":\"41386\",\"end\":\"41426\"},{\"start\":\"41894\",\"end\":\"41964\"},{\"start\":\"42461\",\"end\":\"42524\"},{\"start\":\"43058\",\"end\":\"43138\"},{\"start\":\"43675\",\"end\":\"43725\"},{\"start\":\"44109\",\"end\":\"44170\"},{\"start\":\"44561\",\"end\":\"44669\"},{\"start\":\"45121\",\"end\":\"45161\"},{\"start\":\"45406\",\"end\":\"45531\"}]", "bib_author": "[{\"start\":\"32741\",\"end\":\"32753\"},{\"start\":\"32753\",\"end\":\"32768\"},{\"start\":\"32768\",\"end\":\"32783\"},{\"start\":\"32783\",\"end\":\"32793\"},{\"start\":\"32793\",\"end\":\"32812\"},{\"start\":\"32812\",\"end\":\"32825\"},{\"start\":\"33241\",\"end\":\"33252\"},{\"start\":\"33252\",\"end\":\"33270\"},{\"start\":\"33270\",\"end\":\"33283\"},{\"start\":\"33283\",\"end\":\"33298\"},{\"start\":\"33800\",\"end\":\"33817\"},{\"start\":\"33817\",\"end\":\"33832\"},{\"start\":\"34140\",\"end\":\"34160\"},{\"start\":\"34160\",\"end\":\"34186\"},{\"start\":\"34186\",\"end\":\"34207\"},{\"start\":\"34704\",\"end\":\"34730\"},{\"start\":\"34942\",\"end\":\"34952\"},{\"start\":\"34952\",\"end\":\"34969\"},{\"start\":\"34969\",\"end\":\"34981\"},{\"start\":\"34981\",\"end\":\"34995\"},{\"start\":\"35413\",\"end\":\"35433\"},{\"start\":\"35433\",\"end\":\"35446\"},{\"start\":\"35446\",\"end\":\"35456\"},{\"start\":\"35696\",\"end\":\"35716\"},{\"start\":\"35716\",\"end\":\"35734\"},{\"start\":\"35734\",\"end\":\"35744\"},{\"start\":\"35744\",\"end\":\"35757\"},{\"start\":\"36092\",\"end\":\"36102\"},{\"start\":\"36102\",\"end\":\"36117\"},{\"start\":\"36419\",\"end\":\"36433\"},{\"start\":\"36433\",\"end\":\"36448\"},{\"start\":\"36448\",\"end\":\"36467\"},{\"start\":\"36700\",\"end\":\"36711\"},{\"start\":\"36711\",\"end\":\"36723\"},{\"start\":\"36723\",\"end\":\"36736\"},{\"start\":\"37045\",\"end\":\"37057\"},{\"start\":\"37057\",\"end\":\"37071\"},{\"start\":\"37071\",\"end\":\"37084\"},{\"start\":\"37084\",\"end\":\"37100\"},{\"start\":\"37511\",\"end\":\"37523\"},{\"start\":\"37523\",\"end\":\"37532\"},{\"start\":\"37532\",\"end\":\"37545\"},{\"start\":\"37545\",\"end\":\"37555\"},{\"start\":\"37555\",\"end\":\"37571\"},{\"start\":\"38033\",\"end\":\"38048\"},{\"start\":\"38048\",\"end\":\"38061\"},{\"start\":\"38061\",\"end\":\"38075\"},{\"start\":\"38075\",\"end\":\"38091\"},{\"start\":\"38567\",\"end\":\"38582\"},{\"start\":\"38582\",\"end\":\"38603\"},{\"start\":\"38603\",\"end\":\"38617\"},{\"start\":\"38984\",\"end\":\"39001\"},{\"start\":\"39001\",\"end\":\"39012\"},{\"start\":\"39012\",\"end\":\"39025\"},{\"start\":\"39438\",\"end\":\"39447\"},{\"start\":\"39447\",\"end\":\"39465\"},{\"start\":\"39859\",\"end\":\"39871\"},{\"start\":\"39871\",\"end\":\"39883\"},{\"start\":\"39883\",\"end\":\"39897\"},{\"start\":\"40173\",\"end\":\"40180\"},{\"start\":\"40180\",\"end\":\"40192\"},{\"start\":\"40192\",\"end\":\"40201\"},{\"start\":\"40201\",\"end\":\"40211\"},{\"start\":\"40211\",\"end\":\"40229\"},{\"start\":\"40585\",\"end\":\"40598\"},{\"start\":\"40598\",\"end\":\"40614\"},{\"start\":\"40899\",\"end\":\"40915\"},{\"start\":\"40915\",\"end\":\"40939\"},{\"start\":\"40939\",\"end\":\"40945\"},{\"start\":\"41428\",\"end\":\"41444\"},{\"start\":\"41444\",\"end\":\"41468\"},{\"start\":\"41468\",\"end\":\"41482\"},{\"start\":\"41482\",\"end\":\"41509\"},{\"start\":\"41966\",\"end\":\"41981\"},{\"start\":\"41981\",\"end\":\"41996\"},{\"start\":\"41996\",\"end\":\"42010\"},{\"start\":\"42010\",\"end\":\"42024\"},{\"start\":\"42024\",\"end\":\"42040\"},{\"start\":\"42526\",\"end\":\"42544\"},{\"start\":\"42544\",\"end\":\"42561\"},{\"start\":\"42561\",\"end\":\"42574\"},{\"start\":\"43140\",\"end\":\"43155\"},{\"start\":\"43155\",\"end\":\"43174\"},{\"start\":\"43174\",\"end\":\"43193\"},{\"start\":\"43193\",\"end\":\"43210\"},{\"start\":\"43210\",\"end\":\"43225\"},{\"start\":\"43225\",\"end\":\"43239\"},{\"start\":\"43239\",\"end\":\"43246\"},{\"start\":\"43727\",\"end\":\"43738\"},{\"start\":\"43738\",\"end\":\"43745\"},{\"start\":\"44172\",\"end\":\"44187\"},{\"start\":\"44187\",\"end\":\"44200\"},{\"start\":\"44200\",\"end\":\"44212\"},{\"start\":\"44671\",\"end\":\"44684\"},{\"start\":\"44684\",\"end\":\"44694\"},{\"start\":\"44694\",\"end\":\"44705\"},{\"start\":\"45163\",\"end\":\"45179\"},{\"start\":\"45179\",\"end\":\"45192\"},{\"start\":\"45192\",\"end\":\"45208\"},{\"start\":\"45533\",\"end\":\"45543\"},{\"start\":\"45543\",\"end\":\"45555\"},{\"start\":\"45555\",\"end\":\"45569\"}]", "bib_venue": "[{\"start\":\"32825\",\"end\":\"32882\"},{\"start\":\"33298\",\"end\":\"33378\"},{\"start\":\"33832\",\"end\":\"33869\"},{\"start\":\"34207\",\"end\":\"34296\"},{\"start\":\"34730\",\"end\":\"34748\"},{\"start\":\"34995\",\"end\":\"35065\"},{\"start\":\"35456\",\"end\":\"35496\"},{\"start\":\"35757\",\"end\":\"35819\"},{\"start\":\"36117\",\"end\":\"36172\"},{\"start\":\"36467\",\"end\":\"36501\"},{\"start\":\"36736\",\"end\":\"36798\"},{\"start\":\"37100\",\"end\":\"37170\"},{\"start\":\"37571\",\"end\":\"37655\"},{\"start\":\"38091\",\"end\":\"38175\"},{\"start\":\"38617\",\"end\":\"38674\"},{\"start\":\"39025\",\"end\":\"39109\"},{\"start\":\"39465\",\"end\":\"39535\"},{\"start\":\"39897\",\"end\":\"39923\"},{\"start\":\"40229\",\"end\":\"40309\"},{\"start\":\"40629\",\"end\":\"40668\"},{\"start\":\"40945\",\"end\":\"41038\"},{\"start\":\"41509\",\"end\":\"41590\"},{\"start\":\"42040\",\"end\":\"42124\"},{\"start\":\"42574\",\"end\":\"42684\"},{\"start\":\"43246\",\"end\":\"43320\"},{\"start\":\"43745\",\"end\":\"43838\"},{\"start\":\"44212\",\"end\":\"44292\"},{\"start\":\"44705\",\"end\":\"44789\"},{\"start\":\"45208\",\"end\":\"45245\"},{\"start\":\"45569\",\"end\":\"45661\"},{\"start\":\"32884\",\"end\":\"32926\"},{\"start\":\"33380\",\"end\":\"33445\"},{\"start\":\"34298\",\"end\":\"34372\"},{\"start\":\"35067\",\"end\":\"35122\"},{\"start\":\"36174\",\"end\":\"36214\"},{\"start\":\"37172\",\"end\":\"37227\"},{\"start\":\"37657\",\"end\":\"37726\"},{\"start\":\"38177\",\"end\":\"38246\"},{\"start\":\"38676\",\"end\":\"38718\"},{\"start\":\"39111\",\"end\":\"39180\"},{\"start\":\"39537\",\"end\":\"39592\"},{\"start\":\"40311\",\"end\":\"40376\"},{\"start\":\"41040\",\"end\":\"41118\"},{\"start\":\"41592\",\"end\":\"41658\"},{\"start\":\"42126\",\"end\":\"42195\"},{\"start\":\"42686\",\"end\":\"42781\"},{\"start\":\"43322\",\"end\":\"43381\"},{\"start\":\"43840\",\"end\":\"43918\"},{\"start\":\"44294\",\"end\":\"44359\"},{\"start\":\"44791\",\"end\":\"44860\"},{\"start\":\"45663\",\"end\":\"45740\"}]"}}}, "year": 2023, "month": 12, "day": 17}