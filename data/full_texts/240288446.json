{"id": 240288446, "updated": "2023-10-09 12:45:55.482", "metadata": {"title": "Estimating and Maximizing Mutual Information for Knowledge Distillation", "authors": "[{\"first\":\"Aman\",\"last\":\"Shrivastava\",\"middle\":[]},{\"first\":\"Yanjun\",\"last\":\"Qi\",\"middle\":[]},{\"first\":\"Vicente\",\"last\":\"Ordonez\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "In this work, we propose Mutual Information Maximization Knowledge Distillation (MIMKD). Our method uses a contrastive objective to simultaneously estimate and maximize a lower bound on the mutual information of local and global feature representations between a teacher and a student network. We demonstrate through extensive experiments that this can be used to improve the performance of low capacity models by transferring knowledge from more performant but computationally expensive models. This can be used to produce better models that can be run on devices with low computational resources. Our method is flexible, we can distill knowledge from teachers with arbitrary network architectures to arbitrary student networks. Our empirical results show that MIMKD outperforms competing approaches across a wide range of student-teacher pairs with different capacities, with different architectures, and when student networks are with extremely low capacity. We are able to obtain 74.55% accuracy on CIFAR100 with a ShufflenetV2 from a baseline accuracy of 69.8% by distilling knowledge from ResNet-50. On Imagenet we improve a ResNet-18 network from 68.88% to 70.32% accuracy (1.44%+) using a ResNet-34 teacher network.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2110.15946", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ShrivastavaQO23", "doi": "10.1109/cvprw59228.2023.00010"}}, "content": {"source": {"pdf_hash": "2bcba5c0cff0422558acaf7cb4e89b95d7be6f1c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2110.15946v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "dfaa064783f836c3d6b124c24ed0282e832c81e3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2bcba5c0cff0422558acaf7cb4e89b95d7be6f1c.txt", "contents": "\nEstimating and Maximizing Mutual Information for Knowledge Distillation\n\n\nAman Shrivastava \nUniversity of Virginia Charlottesville\nVAUSA\n\nYanjun Qi \nUniversity of Virginia Charlottesville\nVAUSA\n\nVicente Ordonez vicenteor@rice.edu \nRice University Houston\nTXUSA\n\nEstimating and Maximizing Mutual Information for Knowledge Distillation\n\nIn this work, we propose Mutual Information Maximization Knowledge Distillation (MIMKD). Our method uses a contrastive objective to simultaneously estimate and maximize a lower bound on the mutual information of local and global feature representations between a teacher and a student network. We demonstrate through extensive experiments that this can be used to improve the performance of low capacity models by transferring knowledge from more performant but computationally expensive models. This can be used to produce better models that can be run on devices with low computational resources. Our method is flexible, we can distill knowledge from teachers with arbitrary network architectures to arbitrary student networks. Our empirical results show that MIMKD outperforms competing approaches across a wide range of student-teacher pairs with different capacities, with different architectures, and when student networks are with extremely low capacity. We are able to obtain 74.55% accuracy on CIFAR100 with a ShufflenetV2 from a baseline accuracy of 69.8% by distilling knowledge from ResNet-50. On Imagenet we improve a ResNet-18 network from 68.88% to 70.32% accuracy (1.44%+) using a ResNet-34 teacher network.\n\nIntroduction\n\nRecent machine learning literature has seen a lot of progress driven by deep neural networks. Many such models that achieve state-of-the-art performance on different benchmarks require large amounts of computation and memory capacities [14]. These requirements limit the wider adoption of these models in resource-limited scenarios. To this end, Knowledge Distillation (KD) has been used to transfer knowledge from a stronger teacher network to a smaller and less computationally expensive student network [4,12]. This often allows student networks to outperform identical models trained without a teacher. However there is still much room for improvement in knowledge distillation so that students can extract as much knowledge as ResNet-32\n\nResNet-8x4\n\n\nShuffleNetV2\n\nMobileNetV2 Figure 1. Accuracy-efficiency trade-off of various CNN models on the CIFAR100 dataset. Inference time is the average over a single input. MIMKD provides significant gains over baseline student network accuracies and KD [12] across a range of student-teacher pairs. Notice that Conv4-MP, which is a simple 4-layer CNN, after distillation performs close to a more standard ShuffleNetV2. Note: Runtimes were computed on an Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz.\n\npossible from the teacher network.\n\nIn this paper, we look at knowledge distillation from an information-theoretic perspective. For better distillation, the student needs to generate representations that share maximum information with the representations generated by the teacher. Based on this intuition, we propose Mutual Information Maximization Knowledge Distillation (MIMKD). Multiple approaches have been proposed to estimate the mutual information between high-dimensional continuous variables [2,13]. Belghazi et al [2] propose a KL-divergence based formulation of mutual information. We observe that this approach can be extended to maximize the mutual information in a contrastive setup. Contrastive methods have had an outsized impact in other problems such as self-supervised learning [6,10], however they rely on sampling a rather large number of paired inputs to optimize their objective functions. We find that by using a Jensen-Shannon divergence (JSD) based formulation we obtain a more stable objective to optimize where the performance is invariant to the number of negative samples while being monotonically related to the true mutual information as also shown in Hjelm et al [13].\n\nThe previous work of [12] performed distillation by minimizing the Kullback-Leibler Divergence (KLD) between the output logits of two models. Since then, several other output-based knowledge distillation approaches have been developed [5,12,24,31]. Some of these methods try to match the final outputs of two networks by minimizing a distance metric. Other works have also encouraged additional knowledge transfer by minimizing a metric between intermediate representations [1,19,28].\n\nHowever, models with significantly different architectures have distinct data-abstraction flows, and the complexity of the patterns recognized at different depths in the model varies significantly with model architecture (i.e. the number of filters in convolutional layers). Therefore, minimizing a non-parameterized distance metric on the representations imposes an additional structural constraint that might not be ideal for knowledge transfer. In our work, we are still able leverage both local and global information by maximizing mutual information instead of a rigid distance metric between representations.\n\nMore comparable to our work is the recently proposed Contrastive Representation Distillation (CRD) framework [24]. This method uses a Noise Contrastive Estimation (NCE) objective [9,16] to transfer structured relational knowledge from the teacher to the student. However, a caveat of this approach is that it ignores intermediate distillation for feature level information and requires a large number of negative samples requiring large batches [6] or memory banks [10,26]. We extend this work by using a JSD-based contrastive objective that is insensitive to the number of negative samples. This enables us to impose additional region-consistent local and feature-level constraints with just one negative sample.\n\nWe propose three mutual information maximization objectives between the teacher and student networks: (1) Global information maximization, which aims to maximize the shared information between the final output representations. This pushes the student network to generate feature vectors that are as rich as the ones generated by the teacher.\n\n(2) Local information maximization, which pushes the student network to recognize complex patterns from each region of the image that are ultimately useful for classification. This is achieved by maximizing the mutual information between region-specific vectors extracted from an intermediate representation of the student network and the final representation of the teacher network. Finally, (3) Fea-ture Information Maximization, which is designed to structurally improve the granular feature-extraction capability of the student by maximizing the mutual information between region-consistent local vectors extracted from intermediate representations of the networks.\n\nOur experimental results in Section 4 demonstrate that these objectives are effective across a wide range of studentteacher pairs and conduct extensive ablation studies of the effect of each proposed objective. We particularly demonstrate the effectiveness of our method in knowledge distillation across student-teacher network pairs with different capacities, student-teacher network pairs with different architectures, and in the extreme case where student networks are extremely low capacity. We show that MIMKD provides consistently better results across all these testing scenarios. Figure 1 shows a summary of results for various student networks on CIFAR-100 when compared to regular KLD-based knowledge distillation [12]. Moreover, we compare the transferability of features learned with knowledge distillation from MIMKD and SOTA baseline. Our results show that MIMKD learns general and transferable representations.\n\n\nRelated Work\n\nIn this section, we discuss previous efforts in improving knowledge distillation, and in estimating mutual information which are the key areas of contribution of our work.\n\n\nKnowledge distillation.\n\nThe concept of knowledge distillation (KD) was introduced in the works of Bucilu\u01ce et al. [4] and later formalized for deep neural networks by Hinton et al. [12]. In knowledge distillation, the goal is to train smaller models that can mimic the performance of larger models. Hinton et al. [12] proposed a knowledge distillation method in which the student network is trained using soft labels extracted from teacher networks.\n\nAttention transfer [28] introduced the idea of transferring intermediate attention maps from the teacher to the student network. Fitnets [19] also presented the idea of adding more supervision by matching the intermediate representation using regressors. Yim et al. [27] formulated the distillation problem using the flow of solution procedure (FSP), which is computed as the gram matrix of features across layers. Sau et al. [22] proposed to include a noise-based regularizer while training the student with the teacher. Specifically, they perform perturbation in the logits of the teacher as a regularization approach. In Correlation Congruence for Knowledge Distillation (CCKD) [18], the authors present a framework which transfers not only instance-level information but also the correlation between instances. In CCKD, a Taylor series expansion-based kernel method is proposed to better capture the correlation between instances. Tung   al. [25] propose a loss that is based on the observation that semantically similar inputs produce similar activation patterns in trained networks. Variational Information Distillation (VID) [1] uses a variational lower-bound for the mutual information between the teacher and the student representations by approximating an intractable conditional distribution using a pre-defined variational distribution.\n\nMore closely related to our work are methods that cast knowledge distillation as a mutual information maximization problem. Contrastive representation distillation (CRD) [24] used a contrastive objective similar to Oord et al. [16] to maximize a lower-bound on mutual information between final representations. The objective used by CRD is a strong lower-bound on the mutual information but requires a significant number of negative samples during training, consequently, requiring large batch-sizes or memory buffers. These practical constraints become even more limiting if mutual information needs to be minimized at the feature-level to enforce regional-supervision during student training. Our work proposes an alternative that bypasses the needed for such large batch-sizes and thus enables to optimize for mutual information through three separate objectives.\n\n\nMutual Information Estimation.\n\nMutual information is a fundamental quantity that measures the relationship between random variables but it is notoriously difficult to measure [17]. An exact estimate is only tractable for discrete variables or a small set of problems where the probability distributions are know. However, both the mentioned scenarios are unlikely for real-world visual datasets. Recently, Mutual Information Neural Estimation (MINE) [2] demonstrated a strong method for estimation of mutual information between high-dimensional continuous random variables using neural networks and gradient descent. MINE [2] proposed a general-purpose parametric neural estimator of mutual information based on dual representations of the KL-divergence [20]. Following from MINE [2], Deep InfoMax [13] proposed a mutual information based objective for unsupervised representation learning. Deep InfoMax [13] contends that it is unnecessary to use the exact KL-divergence based formulation of mutual information and demonstrated the use of an alternative formulation based on the Jensen-Shannon divergence (JSD). The authors showed that the JSD based estimator is stable, and does not require a large number of negative samples. In addition, Deep InfoMax [13] also demonstrated the value of including global and local structure-based mutual information objectives for representation learning. We leverage this line of work in our method to propose a framework for knowledge distillation that leverages both local and global features without significantly adding memory overheads during training.\n\n\nMethod\n\nIn this section, we describe our general framework for model compression or knowledge distillation in a teacher student setup. Consider a stronger teacher network f t : X \u2192 Y with trained parameters \u03c6 and a student network, operating on the same domain, f s : X \u2192 Y with parameters \u03b8. Let x be the sample drawn from the data distribution p(x) and f t (x) & f s (x) denote the representations extracted from the pre-classification layer, while f cls t (x) & f cls s (x) denote the predicted class-probability distributions from the teacher and the student networks respectively. Now con-\nsider a set R = {(f (k) t (x), f (k)\ns (x))} K k=1 that contains K pairs of intermediate representations extracted from the networks such that each pair in set R contains same-sized intermediate representations extracted from the networks, where m k \u00d7m k is the size corresponding to the k-th pair in the set. Each location in these 2-dimensional intermediate representations corresponds to a specific region in the input image. Note that we do not include the final representations f t (x) and f s (x) in the set R.\n\nOur method focuses on maximizing the mutual information, (1) between final image representations f t (x) and f s (x) (global information maximization), (2) between the global image representation from the teacher network f t (x) and the last intermediate representation from the student network f (K) s (x) (local information maximization), and (3) between the pairs in set R (feature information maximization). Figure 2 shows an overview of our method.\n\n\nMutual Information Maximization\n\nIn order to estimate and maximize mutual information between random variables X and Z, we train a neural network to distinguish samples generated from the joint distribution, P (X, Z) and the product of marginals P (X)P (Z). In MINE [2], the authors use the Donsker-Varadhan (DV) [8] representation of the KL-divergence as the lower bound on the mutual information. Recently, another bound on mutual information, formulated as in-foNCE [16] based on Noise-Contrastive Estimation [9], has seen wide adoption in representation learning due to its low variance and accurate estimate of MI. It is defined as follows;\nI Inf oN CE \u03c9 (X; Z) := E P (X,Z) [T \u03c9 \u2212 E P (X)P (Z) [log T \u03c9 ]],(1)\nwhere T \u03c9 : X \u00d7 Z \u2192 R is the discriminator neural network with parameters \u03c9. However, as demonstrated in [13], both DV and infoNCE require a large number of negative samples during training. Recent works tackle this problem by using a memory-buffer that keeps representations from previous samples in memory to be accessed during training. As implemented in CRD [24], this can be done if mutual information is maximized only between the final representations of the networks as the dimensions of the representations to be kept in memory is limited. In this work, we extend this infoNCE based MI maximization framework to include feature and local level information maximization. As a result, we require negative samples for each location in the multiple K intermediate feature maps as well as for the final representations. This becomes unfeasible for most large state-of-the-art architectures. To this end, in our approach we adopt Jensen-Shannon divergence based mutual information estimation, similar to the formulations in [15] and [3]. The MI estimate from this JSD-based bound on MI, due to its formulation, is insensitive to the number of negative samples.\nI(X; Z) \u2265\u00ce JSD \u03c9 (X; Z) := E P (X,Z) [\u2212log(1 + e \u2212T\u03c9 )] \u2212 E P (X)P (Z) [log(1 + e T\u03c9 )]. (2)\nOverall, we optimize the parameters \u03b8 of the student network f s and parameters \u03c9 of the critic network T \u03c9 by simultaneously estimating and maximizing mutual information between the representations of the frozen teacher network and the student network.\n\n\nGlobal information maximization\n\nOur global objective aims to maximize the mutual information between the richer final representation of the frozen teacher network f t (x) and the final representation of the student network f s (x) to encourage the student to learn richer representations. This objective uses a discriminator function T \u03c9g , where \u03c9 g are the trainable parameters. We use the infoNCE bound for global MI maximization as it is computationally feasible to maintain a memory bank of negative samples due to the lower dimensionality of the final representations from the networks. We optimize the parameters of the student and the discriminator function simultaneously as:\n(\u03c9 g ,\u03b8) = argmax \u03c9g,\u03b8\u00ce infoNCE \u03c9g (f t (x), f s (x)).(3)\n\nLocal information maximization\n\nIn this objective we maximize the mutual information between a richer final representation of the teacher network and representations of local regions extracted by the student network. This objective draws from the assertion that the final teacher representations contains valuable information required for downstream classification. Hence, this objective encourages the student network to extract information from local image regions that is ultimately useful for classification.\n\nWe enforce this objective between f t (x) and the last intermediate representation from the student network in the set R. Therefore for k = K, f \n(K) s (x)} i,j is then paired with f t (x), where i, j \u2208 [1, m K ]\ndenotes the specific location in the feature map. The pairs are then used with the mutual information estimator to optimize the parameters as follows:\n(\u03c9 l ,\u03b8) = argmax \u03c9 l ,\u03b8 1 m 2 K m K i=1 m K j=1\u00ce JSD \u03c9 l (f t (x), {f (K) s (x)} i,j ) (4)\nwhere a discriminator neural network T \u03c9 l with parameters \u03c9 l is used.\n\n\nFeature Information maximization\n\nThis objective aims to maximize the mutual information between region-consistent intermediate representations from the networks. In neural networks, the complexity of captured visual patterns increases towards the later layers [30]. Intuitively, to mimic the representational power of the teacher, the student network needs to learn these complex patterns hierarchically. In order to motivate such hierarchical learning, mutual information is maximized between intermediate features at different depths in the networks. This enables the student to learn to identify complex patterns in a bottom-up fashion and systematically learn to generate richer features. Note that within each pair of intermediate feature maps in set R, mutual information is maximized between vectors corresponding to the same location in the image. This information maximization pushes the student network to extract features from each region of the image that share maximum information with the features extracted by the teacher network from the same region. For a pair (f\n(k) t (x), f (k) s (x)) \u2208 R, information is maximized between pairs of region-consistent vectors {f (k) t (x)} i,j and {f (k) s (x)} i,j for each i, j \u2208 [1, m k ] as follows: (\u03c9 f ,\u03b8) = argmax \u03c9 f ,\u03b8 1 K 1 m 2 k K k=1 m k i=1 m k j=1 I JSD \u03c9 f ({f (k) t (x)} i,j , {f (k) s (x)} i,j ) (5)\nwhere a discriminator neural network T \u03c9 f with parameters \u03c9 f is used.\n\n\nClassification objective\n\nHere the cross-entropy loss is minimized between the output of the classification function f cls s (x) and the target label y as follows:\n(\u03b8) = argmin \u03b8 L CE (y, f cls s (x)),(6)\nwhere L CE denotes the cross-entropy function. Our overall objective is a weighted-summation of all the above individual objectives with weights \u03b1 (cross-entropy loss), \u03bb g (global MI maximization), \u03bb l (local MI maximization), and \u03bb f (feature MI maximization)\n\n\nMutual Information Discriminators\n\nThe parameterized mutual information discriminator functions (T \u03c9g , T \u03c9 l , and T \u03c9 f ) can be modeled as neural networks. In our experiments, we use two distinct discriminator architectures inspired from the functions presented in Deep InfoMax [13]. For global information maximization, we use the standard project and dot architecture. The representations from both the teacher and the student are first projected using an appropriate projection architecture with a linear shortcut. The dot-product of these projections is then computed to get the score. Positive and negative pairs of representations are passed through the discriminator to get respective scores to be passed into equation 2 to get the estimates on the lower bound of the mutual information. Whereas, for local and feature information maximization we use a convolution based architecture as it is cheaper for higher dimensional inputs.\nInput Operation Output [ft(x), f (K) s (x)] 1 \u00d7 1 Conv + ReLU O1 O1 1 \u00d7 1 Conv + ReLU O2 O2 1 \u00d7 1 Conv scores\nSpecifically, for local information maximization, we replicate the final representation from the teacher f t (x) to match the m K \u00d7 m K size of the student's last intermediate feature map (f \n\n\nImplementation Details\n\nWe adopted the generally established approach for training CNNs on the CIFAR-100 dataset. We use SGD with momentum 0.9, weight decay 5\u00d710 \u22124 , and an initial learning rate of 0.05 for a total of 240 epochs with batch-size 64. The learning rate is decayed by 0.1 at the 150th, 180th and the 210th epoch. We used random horizontal flips and random crop for augmenting the dataset during training. For  Figure 3. Results from the ablation studies on CIFAR100 dataset using a student ResNet-8x4 (baseline acc. 72.44%) with teacher ResNet-32x4 (baseline acc. 79.24%). Contour lines represent the final test accuracy of the student. The study was performed by varying the values of \u03bb f , \u03bbg, \u03bb l from 0 to 1 with increments of 0.25 while \u03b1 was kept constant at 1. In each plot, the accuracy landscape is shown with \u03bbg set to a constant value. Plots for remaining values of \u03bb have been added to the appendix. ImageNet, we use the standard PyTorch training scheme for ResNets [11]. Code implementation will be made public on publication.\n\n\nExperiments\n\nIn this section, we demonstrate the efficacy of our framework using various ablative and quantitative analyses. We first establish the value of each of our mutual information maximization formulations by performing an extensive ablative study (sec. 4.1). Further, we demonstrate the prowess of our distillation framework based on model compression performance in the following setups: It has 50K training images with 500 images in each of 100 classes and a total of 10K test images. In our experiments, we use standard CNN architectures of varied capacities, such as ResNet [11], Wide ResNet (WRN) [29], Mo-bileNet [21], ShuffleNet [32], and VGG [23]. We compare Student Net. our method with other knowledge distillation methods, such as (1) Knowledge Distillation (KD) [12], (2) FitNets [19], (3) Attention Transfer (AT) [28], (4) Variational Information Distillation (VID) [1], and (5) Contrastive Representation Distillation (CRD) [24]. We used the following values for hyper-parameters based on a held out set: \u03b1 = 1, \u03bb g = 1, \u03bb l = 0.75, \u03bb f = 1 for all our experiments. The infoNCE bound in CRD as well as our global MI is set to use 4096 negatives. The hyper-parameter choice for other approaches can be found in supplementary. Additionally, in order to demonstrate the scalability of our method, we compare our distillation performance on the ImageNet [7] dataset against AT [28], and KD [12]. ImageNet is a largescale dataset with 1.2 million training images across 1K classes and a total of 50K validation images.\n\n\nAblation Study\n\nWe perform an extensive ablation study to demonstrate the value of each component of our mutual information maximization objective. Ablative study experiments are performed with ResNet-32x4 as the teacher network and ResNet-8x4 as the student network where the baseline accuracy of the teacher is 79.24% and that of the student network is 72.44%. The values of the hyper-parameters \u03bb g , \u03bb l and \u03bb f -that control the weight of the global, local and feature mutual information maximization objectives respectively -were varied between 0 and 1 with an increment of 0.25 while the weight for the cross-entropy loss, \u03b1 was set to 1. Note that for this study, we use the JSD-based bound for all MI maximization formulations including for global MI which is not the case for our final competitive models presented further. The contour plots in Figure 3 shows the test accuracy landscape with respect to a pair of hyper-parameters when the third hyper-parameter is set to distinct values. For instance, we observe that for any value of \u03bb g , better performance is achieved towards higher values of both \u03bb f and \u03bb l . Similar trends can be observed in all the accuracy landscape plots. Overall, this demonstrates the value of maximizing region-consistent local and featurelevel mutual information between representations in addition to just global information maximization. Please refer to the appendix for additional accuracy landscape plots.\n\n\nSimilar CNN Architectures\n\nWe perform knowledge distillation from a teacher network to a student network of the same family (e.g. ResNets of different capacities). Table 1 presents our results, showing that our method outperforms others in most setups and always obtains gains with respect to student networks. Notice that CRD [24] is able to slightly surpass the performance of our method in one setup while being close in most cases. We find this encouraging as CRD [24] uses a similar mutual information maximization based formulation in their distillation objective with a tighter lowerbound. Therefore, if we only use the global objective in our method, CRD [24] should outperform our method due to its tighter bound. Despite compromising the lower bound on mutual information, MIMKD takes advantage of using region-consistent local and feature-level mutual information maximization.  Table 2. Observed test accuracy (in %) of student networks trained with teacher networks of higher capacity and different architecture on the CIFAR100 dataset using our method MIMKD and other distillation frameworks.\n\n\nDissimilar CNN Architectures\n\nHere, we perform knowledge distillation from a teacher network to a student network with a significantly different architecture. This tests the flexibility methods to adapt to distinct data-abstraction flows of dissimilar neural network architectures. Table 2 demonstrates that our method (MIMKD) outperforms other distillation methods in most teacher-student combinations increasing the accuracy of a ShuffleNetV2 by 4.7% while distilling from a much different ResNet-50 model. This demonstrates that our method is able to accommodate significant architectural differences in teacher-student pairs and does not impose structural constraints on intermediate layers that hinder training. While other methods that work on intermediate feature maps like AT [28] and FitNets [19] do not show much improvement from base student accuracy.\n\n\nTransferring representations\n\nFinally, we compare the transferability of the features learned with knowledge distillation from MIMKD and baselines, on two other datasets: STL-10 and TinyImagenet. A WRN-16-2 network is trained with and without distillation from a pre-trained WRN-40-2 teacher on the CI-FAR100 dataset. The student is then used as a frozen feature extractor (pre-classification layer) for images in the STL-10 and the TinyImageNet dataset. A linear classifier is trained on these extracted features to perform classification on the test sets of these datasets. The classification accuracy on the unseen datasets is interpreted as the transferability of representations. Results are presented in Table 3 and show that MIMKD learns more transferrable representations.\n\n\nSTL-10 TinyImageNet\n\nBase Accuracy (no distillation) 69  Table 3. Observed test-set accuracy (in %) of the student network on STL-10 and TinyImagenet datasets using our method (MIMKD) and other distillation frameworks.\n\n\nConclusion\n\nIn this paper, we presented a framework (MIMKD) motivated by an information-theoretic perspective on knowledge distillation. Utilizing an information-efficient lower bound on mutual information, we proposed three information maximization formulations and demonstrated the value of region-consistent local and feature-level information maximization on distillation. We enable intermediate distillation using a JSD based lower-bound on MI which we optimize using only one negative sample. Further works in this area could explore our contention that if used with a tighter lower-bound, our feature and local information maximization objectives have the potential to surpass even its current performance.\n\n\nAcknowledgements\n\nThis work was supported by NSF Awards IIS-2221943 and IIS-2201710, and through gift funding from a Facebook Research Award: Towards On-Device AI.\n\n\nA.1. Limitations and Broader Impacts\n\nIn this paper, we presented a novel Mutual Information Maximization based knowledge distillation framework (MIMKD). Our method uses the JSD based lower-bound on mutual information which is optimized using only one negative sample. However, despite its favorable properties, our lower-bound may be less tight on the mutual information than the infoNCE bound as it approximates the mutual information by being monotonically related with it. Additionally, as we use only one negative sample, the performance of the method may be hindered by the presence of false negatives. The performance of the method is also effected by the architecture of the discriminator functions which can be explored further. We presented three information maximization formulations and demonstrated the value of region-consistent information maximization on distillation performance. We observe that the performance is slightly-sensitive to the hyper-parameters that control the relative value of our global, local, and feature information maximization formulations. This has been explored in great detail in our ablation sections and further demonstrated in figures 4, 5, and 6. Our method transfers representations from the teacher to the student. As such, harmful biases that the teacher has learnt are transferred to the student as well. And further exploration is required to alleviate the transfer of such biases during distillation.\n\n\nA.2. Hyper-parameters for other methods\n\nThe student is trained with the following loss function which is a combination of the distillation loss and the crossentropy loss for classification:\nL = \u03b1L cls + (1 \u2212 \u03b1)L KD + \u03b2L dis(7)\nNote that we set \u03b1 = 1 for all methods except KD \u00df [12] and the value of \u03b2 is set to the value recommended in the original work as follows:\n\n1. KD [12]: \u03b1 = 0.9, \u03b2 = 0 2. Fitnet [19]: \u03b2 = 100 3. AT [28]: \u03b2 = 1000 4. VID [1]: \u03b2 = 1 5. CRD [24]: \u03b2 = 0.8, for CRD evaluation, we use a original work inspired self-implementation with 4096 negative samples and i = j negative sampling methodology as described in the original work.\n\n\nA.3. Pairing Intermediate Representations\n\nA.3.1 Similar CNN Architectures.\n\nConsider the case of distillation when the teacher network is a pre-trained WRN-40-2 and the student network is a WRN-16-1. We use 4 same-sized representations extracted from intermediate layers of the networks. Therefore, the\nset R = {(f (k) t (x), f (k)\ns (x))} K k=1 contains k pairs of samesized 2-dimensional representations. Table 4 describes the sizes of the intermediate representations used for featurebased mutual information maximization. It can be seen that for this combination we use k = 4 in our formulation. Similar approach of defining the set R is followed in cases where the teacher and student networks have significantly different architectures. For instance, Table 5 shows the dimensions of intermediate representations used when the teacher network is a ResNet34 while the student is a Shuf-fleNetV2. Here k = 4 is used, however, for some combinations of different standard architectures we use k = 3 if only 3 pairs intermediate representations from the teacher and the student have the same size. Note that our method is invariant to the number of channels in the representations. Therefore, mismatch in the number of channels in pairs of representations in R is inconsequential for the formulation of our losses.\n\n\nA.4. Mutual Information Discriminators\n\nThe parameterized mutual information discriminator functions (T \u03c9g , T \u03c9 l , and T \u03c9 f ) can be modeled as neural networks. In our experiments, we use two distinct discriminator architectures inspired from the functions presented in Deep InfoMax [13]. Table 5. Dimensions of intermediate representation in the form channels \u00d7 height \u00d7 width used for feature-level mutual information maximization between a teacher WRN-40-2 and a student WRN-16-1 network. Alternatively, each value of k represents a pair of elements in the set R.\n\n\nResNet34\n\nShuffleNetV2 In this method, the representations from the teacher and the student are concatenated together and passed through a series of layers to get the score. For global information maximization, the final representations from both networks is concatenated together to get [f s (x), f t (x)]. This vector is then passed to a fully connected network with two 512-unit hidden layers, each followed by a ReLU non-linearity (ref .  table 6). The output is then passed through another linear layer to obtain the final score. \nk f (k) t (x) f (k) s (x\n\nInput\n\nOperation Output\n[f t (x), f s (x)] LL + ReLU O 1 O 1 LL + ReLU O 2 O 2 LL score\nFor local information maximization, we replicate the final representation from the teacher f t (x) to match the m K \u00d7 m K size of the student's last intermediate feature map (f (K) s (x)). The resulting replicated tensor is then concatenated with f\n(K) s (x) to get [f t (x), f (K) s (x)\n] which serves as the input for the critic function (ref . table 7). Table 7. The architecture of the discriminator used for local and feature mutual information maximization. Note that for feature mutual information maximization the input at the first layer is\n[f (k) t (x), f (k) s (x)].\n\nInput\n\nOperation Output\n[f t (x), f (K) s (x)] 1 \u00d7 1 Conv + ReLU O 1 O 1 1 \u00d7 1 Conv + ReLU O 2 O 2 1 \u00d7 1 Conv scores\nSimilarly, consider feature mutual information maxi-mization, for each pair in the set R we use a distinct discriminator T (k) \u03c9 f . For a given k, each pair of intermediate feature representations in the set R are concatenated together to get [f\n(k) t (x), f (k) s (x)]\n. Which is then passed through two convolutional (1 \u00d7 1 kernels and 512 filters) where each layer is followed by a ReLU non-linearity. The output obtained is then further passed into a convolutional layer (1 \u00d7 1 kernels and 1 filter) to give m k \u00d7 m k scores (ref . table 7).\n\n\nA.4.2 Project and Dot Architecture.\n\nIn this method, the representations from both the teacher and the student are first projected using an appropriate projection architecture with a linear shortcut. The dot-product of these projections is then computed to get the score. Positive and negative pairs of representations are passed through the discriminator to get respective scores to be passed into equation (2) to get the estimates on the lower bound of the mutual information. One-dimensional representations are projected using the architecture described in table 8, whereas for two-dimensional intermediate feature maps, projection architecture described in table 9 is used. Table 8. The projection architecture used for one-dimensional inputs. Here, LL denotes linear layer while LN denotes layer normalization. Both ft(x) and fs(x) are projected using this architecture and their dot product is computed to get scores.\n\n\nInput\n\nOperation\nOutput f t (x) or f s (x) LL + ReLU + LL O 1 f t (x) or f s (x) LL + ReLU O 2 O 1 + O 2 LN proj\nTherefore, for (1) global information maximization, both f t (x) and f s (x) are projected using the one-dimensional projection architecture, for (2) local information maximization, the final teacher representation, f t (x), is projected using the one-dimensional projection architecture and duplicated to match the size of the projected intermediate student representation projected using the two-dimensional projection architecture, a dot product of these outputs is then computed to get the scores, while for (3) feature information maximization, both representations in each pair of the set R is projected using a respective two-dimensional projection architecture.\n\n\nA.5. ImageNet results\n\nIn this experiment we train a student ResNet-18 with a pre-trained teacher ResNet-34 on the ImageNet dataset (ILSVRC). Note that we do not perform any hyperparameter tuning specifically for this configuration and use the same values we obtained for the CIFAR-100 dataset i.e. \n\n\nInput\n\nOperation Output f (k) \u03b1 = 0.9, \u03bb g = 0.2, \u03bb l = 0.8, \u03bb f = 0.8. We observed that our method is able to reduce the gap between the teacher and the student performance by 1.44%. Results are presented in Table 10.\ns (x) 1 \u00d7 1 Conv + ReLU + LL O 1 f (k) s (x) 1 \u00d7 1 Conv + ReLU O 2 O 1 + O 2 LN proj\n\nA.6. Shallow CNN Architectures\n\nIn this section, we describe our experiments where we distill knowledge from a standard teacher network into a shallow custom-designed CNN. This is done to demonstrate that it is feasible to design and distill information into lightweight models such that they perform competitively with standard CNN architectures while running faster. For our experiments we use 2 shallow CNNs; (1) Conv-4 with 4 convolutional-blocks followed by average pooling operation and a linear layer, where each convolutional-block is made-up of a convolutional layer with kernel size 3 \u00d7 3 and stride 2 followed by batch-normalization and a ReLU nonlinearity, (2) Conv-4-MP which has 4 convolutions blocks followed by average pooling and a linear layer at the end, where each convolutional-block contains a convolutional layer with kernel size 3 \u00d7 3 and stride 1 followed by batchnormalization, ReLU and a max-pooling layer. These architectures were chosen as they are compact and run relatively faster on standard CPUs. Table 11 compiles our results compared to other distillation methods for customdesigned shallow CNN architectures. Notice how a simple model such as Conv-4-MP becomes competitive with Shuf-fleNetV2's base student accuracy. Our method is able to outperform all other methods in this setup. Additionally, we can see that distillation is most successful with ResNet-32x4 as the teacher than for other architectures. This could be because of the larger gap in the baseline accuracy of the networks. Under this more controlled experiment with fixed students, larger gaps between student-teacher pairs also led to larger gains after distillation.\n\nA.7. Computational cost and negative sampling.\n\nWe contextualize the memory and computational overhead of our work with respect to CRD. Our global MI objective has the same footprint as CRD (i.e. an additional 600MB over standard Resnet18 training for storing negatives). In addition, our feature and local MI objective use projection layers which add an additional 100MB of GPU memory. As the computation of our JSD-based objective is computationally trivial, we observe negligible reduction in training speed wrt CRD (2.2 epochs/hr v. 2.4 epochs/hr). Note that no additional memory is used for sampling negatives for local and feature information maximization. The 4096 negatives are only used for global MI as storing 1-D representations is relatively inexpensive.\n\n\nA.8. Ablation Study\n\nIn this section we present additional accuracy landscape plots for our extensive ablation study that demonstrates the value of each component of our mutual information maximization objective. We use a ResNet-32x4 as the teacher network and ResNet-8x4 as the student network where the baseline accuracy of the teacher is 79.24% and that of the student network is 72.44%. The values of the hyperparameters \u03bb g , \u03bb l and \u03bb f -that control the weight of the global, local and feature mutual information maximization objectives respectively -were varied between 0 and 1 with an increment of 0.25 while the weight for the cross-entropy loss, \u03b1 was set to 1. The following contour plots shows the test accuracy landscape with respect to a pair of hyperparameters when the third hyper-parameter is set to distinct values. Overall, this demonstrates the value of maximizing region-consistent local and feature-level mutual information between representations in addition to just global information maximization.\n\nFigure 2 .\n2Overall schematic of our proposed method for mutual information maximization based knowledge distillation (MIMKD). Top: Representations generated by teacher and student networks for image x and a negative sample x . Note that our method uses only one negative sample. Bottom: (a) Positive and negative pairs of final feature vectors are passed into the discriminator function to get scores. (b) Teacher's final representation is replicated to match student's last intermediate representation. (c) For each group of same-sized intermediate feature maps in set R, positive and negative pairs are passed into a distinct discriminator function to get scores. The positive and negative scores obtained are then used with equation(2)to estimate and maximize a lower-bound on mutual information.\n\n\n) is a m K \u00d7 m K feature map where each location roughly corresponds to an H/m K \u00d7 W/m K patch in the input image where H, W are the height and width of the image. The representation of each such patch {f\n\ns\n)] which serves as the input for the critic function (ref. table on right). Similarly, consider feature mutual information maximization, for each pair in the set R we use a distinct discriminator T(k) \u03c9 f . For a given k, each pair of intermediate feature representations in the set R are concatenated together to get [f (x)]. Which is then passed through two convolutional (1 \u00d7 1 kernels and 512 filters) where each layer is followed by a ReLU nonlinearity. The output obtained is then further passed into a convolutional layer (1 \u00d7 1 kernels and 1 filter) to give m k \u00d7 m k scores (ref. table on right). Further details are provided in supplementary.\n\n( 1 )\n1Under similar student-teacher network architectures (sec. 4.2), (2) under dissimilar architectures (sec. 4.3), (3) under a setting with custom designed shallow student networks (ref. appendix for results), (4) in a larger scale setting on Imagenet (ref. appendix for results), and (5) in terms of transfer learning performance (sec. 4.4) as a measure of the transferability of distilled representations. Our model compression experiments are performed on the CIFAR-100 dataset which contains colored natural images of size 32 \u00d7 32.\n\n\netreplicate \n\ntimes \n\n(a) Global Information Maximization \n(b) Local Information Maximization \n(c) Feature Information Maximization \n\nreplicate \n\ntimes \n\n... \n... \n... \n... \n\n+ scores \n\n-scores \n\n+ scores \n\n-scores \n\n+ score \n\n-score \n\nTeacher network \nStudent network \nNegative sample \n\n\n\n\nTable 1. Observed test accuracy (in %) of student networks trained with teacher networks of higher capacity but similar architecture on the CIFAR100 dataset using MIMKD and other competing methods. MIMKD shows consistent increases in accuracy for all model pairs and the largest gains overall.WRN-16-1 WRN-16-2 ResNet-8 \nResNet-20 \nResNet-20 \nResNet-8x4 VGG-8 \n\nTeacher Net. \nWRN-40-2 WRN-40-2 ResNet-110 ResNet-110 ResNet-56 \nResNet-32x4 VGG-19 \n\nStudent Acc. \n67.01 \n72.80 \n59.63 \n69.10 \n69.10 \n72.44 \n69.67 \n\nTeacher Acc. \n75.31 +8.30 \n75.31 +2.51 \n73.82 +14.19 \n73.82 +4.72 \n72.31 +3.21 \n79.24 +3.80 \n74.63 +4.96 \n\nFitNets \n68.35 +1.34 \n73.11 +0.31 \n60.36 +0.73 \n69.12 +0.02 \n69.28 +0.18 \n73.80 +1.36 \n71.32 +1.65 \n\nAT \n68.49 +1.48 \n73.37 +0.57 \n60.24 +0.61 \n70.36 +1.26 \n70.18 +1.08 \n73.20 +0.76 \n71.71 +2.04 \n\nVID \n68.95 +1.94 \n73.89 +1.09 \n60.44 +0.81 \n70.32 +1.22 \n70.52 +1.42 \n73.19 +0.75 \n71.52 +1.85 \n\nKD \n68.24 +1.23 \n73.91 +1.11 \n61.01 +1.38 \n70.32 +1.22 \n70.59 +1.40 \n73.21 +0.77 \n72.29 +2.62 \n\nCRD \n69.21 +2.20 \n74.17 +1.37 \n60.82 +1.19 \n71.45 +2.35 \n71.12 +2.02 \n75.21 +2.77 \n73.10 +3.43 \n\nMIMKD (ours) 70.20 +3.19 \n75.16 +2.36 \n61.81 +2.18 \n71.43 +2.33 \n71.31 +2.21 \n75.83 +3.39 \n73.27 +3.60 \n\n\n\nTable 4 .\n4Dimensions of intermediate representation in the form channels \u00d7 height \u00d7 width used for feature-level mutual information maximization between a teacher WRN-40-2 and a student WRN-16-1 network. Alternatively, each value of k represents a pair of elements in the set R.WRN-40-2 \nWRN-16-1 \nk \nf \n\n(k) \n\nt (x) \nf \n\n(k) \n\ns (x) \n1 16 \u00d7 32 \u00d7 32 16 \u00d7 32 \u00d7 32 \n2 32 \u00d7 32 \u00d7 32 16 \u00d7 32 \u00d7 32 \n3 64 \u00d7 16 \u00d7 16 32 \u00d7 16 \u00d7 16 \n4 128 \u00d7 8 \u00d7 8 \n64 \u00d7 8 \u00d7 8 \n\nA.3.2 Dissimilar CNN Architectures. \n\n\n\nTable 6 .\n6The architecture of the discriminator used for global information maximization. Here LL denotes Linear Layer and d(v) refers to the number of dimensions in vector v.\n\nTable 9 .\n9The projection architecture used for two-dimensional inputs. Here, LL denotes linear layer while LN denotes layer normalization.\n\nTable 10 .\n10Observed top-1 validation accuracy (in %) of the student network on the ImageNet dataset using our method (MIMKD) and other distillation frameworks. In similar settings, the more recent Contrastive Representation Distillation (CRD) method reports comparable performance with an improvement of +1.42 from a student network[24].Student Network \nResNet-18 \nTeacher Network \nResNet-34 \n\nStudent Accuracy \n68.88 \nTeacher Accuracy \n72.82 +3.94 \n\nKnowledge Distill. (KD) \n69.66 +0.78 \nAttention Transfer (AT) \n69.70 +0.82 \n\nMIMKD (this work) \n70.32 +1.44 \n\n\nA. Appendix\nVariational information distillation for knowledge transfer. Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, D Neil, Zhenwen Lawrence, Dai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition711Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational information dis- tillation for knowledge transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9163-9171, 2019. 2, 3, 7, 11\n\nMutual information neural estimation. Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, Devon Hjelm, International Conference on Machine Learning. Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajesh- war, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and De- von Hjelm. Mutual information neural estimation. In Inter- national Conference on Machine Learning, pages 531-540.\n\nPMLR. 14PMLR, 2018. 1, 3, 4\n\nLearning independent features with adversarial nets for non-linear ica. Philemon Brakel, Yoshua Bengio, arXiv:1710.05050arXiv preprintPhilemon Brakel and Yoshua Bengio. Learning indepen- dent features with adversarial nets for non-linear ica. arXiv preprint arXiv:1710.05050, 2017. 4\n\nModel compression. Cristian Bucilu\u01ce, Rich Caruana, Alexandru Niculescu-Mizil, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. the 12th ACM SIGKDD international conference on Knowledge discovery and data mining1Cristian Bucilu\u01ce, Rich Caruana, and Alexandru Niculescu- Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535-541, 2006. 1, 2\n\nLearning efficient object detection models with knowledge distillation. Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, Manmohan Chandraker, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsGuobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Man- mohan Chandraker. Learning efficient object detection mod- els with knowledge distillation. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 742-751, 2017. 2\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, PMLRInternational conference on machine learning. 1Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on ma- chine learning, pages 1597-1607. PMLR, 2020. 1, 2\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. 7\n\nAsymptotic evaluation of certain markov process expectations for large time. D Monroe, Donsker, Sr Srinivasa Varadhan, iv. Communications on Pure and Applied Mathematics. 362Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on Pure and Applied Mathematics, 36(2):183-212, 1983. 4\n\nNoise-contrastive estimation: A new estimation principle for unnormalized statistical models. Michael Gutmann, Aapo Hyv\u00e4rinen, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. the Thirteenth International Conference on Artificial Intelligence and Statistics24JMLR Workshop and Conference ProceedingsMichael Gutmann and Aapo Hyv\u00e4rinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth Inter- national Conference on Artificial Intelligence and Statistics, pages 297-304. JMLR Workshop and Conference Proceed- ings, 2010. 2, 4\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition1Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 9729-9738, 2020. 1, 2\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 6\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. 11arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 1, 2, 7, 11\n\nLearning deep representations by mutual information estimation and maximization. Alex R Devon Hjelm, Samuel Fedorov, Karan Lavoie-Marchildon, Phil Grewal, Adam Bachman, Yoshua Trischler, Bengio, arXiv:1808.06670511arXiv preprintR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual in- formation estimation and maximization. arXiv preprint arXiv:1808.06670, 2018. 1, 2, 3, 4, 5, 11\n\nData-driven sparse structure selection for deep neural networks. Zehao Huang, Naiyan Wang, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Zehao Huang and Naiyan Wang. Data-driven sparse struc- ture selection for deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 304-320, 2018. 1\n\nSebastian Nowozin, Botond Cseke, Ryota Tomioka, Fgan, arXiv:1606.00709Training generative neural samplers using variational divergence minimization. arXiv preprintSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f- gan: Training generative neural samplers using variational divergence minimization. arXiv preprint arXiv:1606.00709, 2016. 4\n\nRepresentation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.0374824arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre- sentation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 2, 3, 4\n\nEstimation of entropy and mutual information. Liam Paninski, Neural computation. 156Liam Paninski. Estimation of entropy and mutual informa- tion. Neural computation, 15(6):1191-1253, 2003. 3\n\nCorrelation congruence for knowledge distillation. Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, Zhaoning Zhang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionBaoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu Liu, Shunfeng Zhou, and Zhaoning Zhang. Correla- tion congruence for knowledge distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 5007-5016, 2019. 2\n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio, Fitnets, arXiv:1412.6550Hints for thin deep nets. 811arXiv preprintAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. 2, 7, 8, 11\n\nTighter variational representations of fdivergences via restriction to probability measures. Avraham Ruderman, Mark Reid, Dar\u00edo Garc\u00eda-Garc\u00eda, James Petterson, arXiv:1206.4664arXiv preprintAvraham Ruderman, Mark Reid, Dar\u00edo Garc\u00eda-Garc\u00eda, and James Petterson. Tighter variational representations of f- divergences via restriction to probability measures. arXiv preprint arXiv:1206.4664, 2012. 3\n\nMobilenetv2: Inverted residuals and linear bottlenecks. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh- moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 4510-4520, 2018. 6\n\nBharat Bhusan, Sau Vineeth, N Balasubramanian, arXiv:1610.09650Deep model compression: Distilling knowledge from noisy teachers. arXiv preprintBharat Bhusan Sau and Vineeth N Balasubramanian. Deep model compression: Distilling knowledge from noisy teach- ers. arXiv preprint arXiv:1610.09650, 2016. 2\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 6\n\nYonglong Tian, Dilip Krishnan, Phillip Isola, arXiv:1910.10699Contrastive representation distillation. 13arXiv preprintYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. arXiv preprint arXiv:1910.10699, 2019. 2, 3, 4, 7, 11, 13\n\nSimilarity-preserving knowledge distillation. Frederick Tung, Greg Mori, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionFrederick Tung and Greg Mori. Similarity-preserving knowl- edge distillation. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 1365-1374, 2019. 3\n\nUnsupervised feature learning via non-parametric instance discrimination. Zhirong Wu, Yuanjun Xiong, X Stella, Dahua Yu, Lin, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3733-3742, 2018. 2\n\nA gift from knowledge distillation: Fast optimization, network minimization and transfer learning. Junho Yim, Donggyu Joo, Jihoon Bae, Junmo Kim, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJunho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 4133-4141, 2017. 2\n\nPaying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Sergey Zagoruyko, Nikos Komodakis, arXiv:1612.03928811arXiv preprintSergey Zagoruyko and Nikos Komodakis. Paying more at- tention to attention: Improving the performance of convolu- tional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016. 2, 7, 8, 11\n\n. Sergey Zagoruyko, Nikos Komodakis, arXiv:1605.07146Wide residual networks. arXiv preprintSergey Zagoruyko and Nikos Komodakis. Wide residual net- works. arXiv preprint arXiv:1605.07146, 2016. 6\n\nVisualizing and understanding convolutional networks. D Matthew, Rob Zeiler, Fergus, European conference on computer vision. SpringerMatthew D Zeiler and Rob Fergus. Visualizing and under- standing convolutional networks. In European conference on computer vision, pages 818-833. Springer, 2014. 5\n\nFast human pose estimation. Feng Zhang, Xiatian Zhu, Mao Ye, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionFeng Zhang, Xiatian Zhu, and Mao Ye. Fast human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3517- 3526, 2019. 2\n\nShufflenet: An extremely efficient convolutional neural network for mobile devices. Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural net- work for mobile devices. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 6848-6856, 2018. 6\n\nObserved test accuracy (in %) of shallow student networks trained with teacher networks of higher capacity and standard architectures on the CIFAR100 dataset using our methods MIMKD and other distillation frameworks. Table 11. Student Net. Conv-4Table 11. Observed test accuracy (in %) of shallow student networks trained with teacher networks of higher capacity and standard architectures on the CIFAR100 dataset using our methods MIMKD and other distillation frameworks. Student Net. Conv-4\n", "annotations": {"author": "[{\"end\":138,\"start\":75},{\"end\":195,\"start\":139},{\"end\":262,\"start\":196}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":80},{\"end\":148,\"start\":146},{\"end\":211,\"start\":204}]", "author_first_name": "[{\"end\":79,\"start\":75},{\"end\":145,\"start\":139},{\"end\":203,\"start\":196}]", "author_affiliation": "[{\"end\":137,\"start\":93},{\"end\":194,\"start\":150},{\"end\":261,\"start\":232}]", "title": "[{\"end\":72,\"start\":1},{\"end\":334,\"start\":263}]", "venue": null, "abstract": "[{\"end\":1559,\"start\":336}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1815,\"start\":1811},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2084,\"start\":2081},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2087,\"start\":2084},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2580,\"start\":2576},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3324,\"start\":3321},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3327,\"start\":3324},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3347,\"start\":3344},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3620,\"start\":3617},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3623,\"start\":3620},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4020,\"start\":4016},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4048,\"start\":4044},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4261,\"start\":4258},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4264,\"start\":4261},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4267,\"start\":4264},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4270,\"start\":4267},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4500,\"start\":4497},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4503,\"start\":4500},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4506,\"start\":4503},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5238,\"start\":5234},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5307,\"start\":5304},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5310,\"start\":5307},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5573,\"start\":5570},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5594,\"start\":5590},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5597,\"start\":5594},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7582,\"start\":7578},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8087,\"start\":8084},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8155,\"start\":8151},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8287,\"start\":8283},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8444,\"start\":8440},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8562,\"start\":8558},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8691,\"start\":8687},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8851,\"start\":8847},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9106,\"start\":9102},{\"end\":9360,\"start\":9356},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9371,\"start\":9367},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9556,\"start\":9553},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9945,\"start\":9941},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10002,\"start\":9998},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10820,\"start\":10816},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11094,\"start\":11091},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11266,\"start\":11263},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11399,\"start\":11395},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11424,\"start\":11421},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11443,\"start\":11439},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11549,\"start\":11545},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11900,\"start\":11896},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14077,\"start\":14074},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14124,\"start\":14121},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14281,\"start\":14277},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14323,\"start\":14320},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14633,\"start\":14629},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14890,\"start\":14886},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15555,\"start\":15551},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15563,\"start\":15560},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18091,\"start\":18087},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20025,\"start\":20021},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21982,\"start\":21978},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22633,\"start\":22629},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22657,\"start\":22653},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22674,\"start\":22670},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22691,\"start\":22687},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22705,\"start\":22701},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22829,\"start\":22825},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22847,\"start\":22843},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22881,\"start\":22877},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22933,\"start\":22930},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22993,\"start\":22989},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23418,\"start\":23415},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23442,\"start\":23438},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23455,\"start\":23451},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25367,\"start\":25363},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25508,\"start\":25504},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25703,\"start\":25699},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26933,\"start\":26929},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26950,\"start\":26946},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30634,\"start\":30630},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30730,\"start\":30726},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30761,\"start\":30757},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30781,\"start\":30777},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30802,\"start\":30799},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30821,\"start\":30817},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32616,\"start\":32612},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35204,\"start\":35201},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":45437,\"start\":45433}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":41379,\"start\":40578},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41586,\"start\":41380},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42242,\"start\":41587},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42782,\"start\":42243},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43073,\"start\":42783},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44287,\"start\":43074},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44778,\"start\":44288},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44956,\"start\":44779},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":45097,\"start\":44957},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45662,\"start\":45098}]", "paragraph": "[{\"end\":2316,\"start\":1575},{\"end\":2328,\"start\":2318},{\"end\":2818,\"start\":2345},{\"end\":2854,\"start\":2820},{\"end\":4021,\"start\":2856},{\"end\":4507,\"start\":4023},{\"end\":5123,\"start\":4509},{\"end\":5838,\"start\":5125},{\"end\":6181,\"start\":5840},{\"end\":6852,\"start\":6183},{\"end\":7779,\"start\":6854},{\"end\":7967,\"start\":7796},{\"end\":8419,\"start\":7995},{\"end\":9769,\"start\":8421},{\"end\":10637,\"start\":9771},{\"end\":12236,\"start\":10672},{\"end\":12833,\"start\":12247},{\"end\":13350,\"start\":12871},{\"end\":13805,\"start\":13352},{\"end\":14453,\"start\":13841},{\"end\":15687,\"start\":14524},{\"end\":16034,\"start\":15781},{\"end\":16722,\"start\":16070},{\"end\":17294,\"start\":16814},{\"end\":17441,\"start\":17296},{\"end\":17659,\"start\":17509},{\"end\":17823,\"start\":17752},{\"end\":18907,\"start\":17860},{\"end\":19268,\"start\":19197},{\"end\":19434,\"start\":19297},{\"end\":19737,\"start\":19476},{\"end\":20681,\"start\":19775},{\"end\":20983,\"start\":20792},{\"end\":22039,\"start\":21010},{\"end\":23578,\"start\":22055},{\"end\":25033,\"start\":23597},{\"end\":26142,\"start\":25063},{\"end\":27007,\"start\":26175},{\"end\":27790,\"start\":27040},{\"end\":28011,\"start\":27814},{\"end\":28727,\"start\":28026},{\"end\":28893,\"start\":28748},{\"end\":30348,\"start\":28934},{\"end\":30541,\"start\":30392},{\"end\":30718,\"start\":30579},{\"end\":31005,\"start\":30720},{\"end\":31083,\"start\":31051},{\"end\":31311,\"start\":31085},{\"end\":32323,\"start\":31341},{\"end\":32895,\"start\":32366},{\"end\":33433,\"start\":32908},{\"end\":33483,\"start\":33467},{\"end\":33796,\"start\":33548},{\"end\":34097,\"start\":33836},{\"end\":34150,\"start\":34134},{\"end\":34490,\"start\":34244},{\"end\":34790,\"start\":34515},{\"end\":35717,\"start\":34830},{\"end\":35736,\"start\":35727},{\"end\":36502,\"start\":35833},{\"end\":36804,\"start\":36528},{\"end\":37025,\"start\":36814},{\"end\":38782,\"start\":37144},{\"end\":38830,\"start\":38784},{\"end\":39551,\"start\":38832},{\"end\":40577,\"start\":39575}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12870,\"start\":12834},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14523,\"start\":14454},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15780,\"start\":15688},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16780,\"start\":16723},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17508,\"start\":17442},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17751,\"start\":17660},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19196,\"start\":18908},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19475,\"start\":19435},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20791,\"start\":20682},{\"attributes\":{\"id\":\"formula_10\"},\"end\":30578,\"start\":30542},{\"attributes\":{\"id\":\"formula_11\"},\"end\":31340,\"start\":31312},{\"attributes\":{\"id\":\"formula_12\"},\"end\":33458,\"start\":33434},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33547,\"start\":33484},{\"attributes\":{\"id\":\"formula_14\"},\"end\":33835,\"start\":33797},{\"attributes\":{\"id\":\"formula_15\"},\"end\":34125,\"start\":34098},{\"attributes\":{\"id\":\"formula_16\"},\"end\":34243,\"start\":34151},{\"attributes\":{\"id\":\"formula_17\"},\"end\":34514,\"start\":34491},{\"attributes\":{\"id\":\"formula_18\"},\"end\":35832,\"start\":35737},{\"attributes\":{\"id\":\"formula_19\"},\"end\":37110,\"start\":37026}]", "table_ref": "[{\"end\":25207,\"start\":25200},{\"end\":25933,\"start\":25926},{\"end\":26434,\"start\":26427},{\"end\":27727,\"start\":27720},{\"end\":27857,\"start\":27850},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31423,\"start\":31416},{\"end\":31773,\"start\":31766},{\"end\":32625,\"start\":32618},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":33348,\"start\":33338},{\"end\":33902,\"start\":33893},{\"end\":33912,\"start\":33905},{\"end\":34789,\"start\":34779},{\"end\":35479,\"start\":35472},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":37024,\"start\":37016},{\"end\":38150,\"start\":38142}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1573,\"start\":1561},{\"end\":2343,\"start\":2331},{\"attributes\":{\"n\":\"2.\"},\"end\":7794,\"start\":7782},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7993,\"start\":7970},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10670,\"start\":10640},{\"attributes\":{\"n\":\"3.\"},\"end\":12245,\"start\":12239},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13839,\"start\":13808},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16068,\"start\":16037},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16812,\"start\":16782},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17858,\"start\":17826},{\"attributes\":{\"n\":\"3.5.\"},\"end\":19295,\"start\":19271},{\"attributes\":{\"n\":\"3.6.\"},\"end\":19773,\"start\":19740},{\"attributes\":{\"n\":\"3.7.\"},\"end\":21008,\"start\":20986},{\"attributes\":{\"n\":\"4.\"},\"end\":22053,\"start\":22042},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23595,\"start\":23581},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25061,\"start\":25036},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26173,\"start\":26145},{\"attributes\":{\"n\":\"4.4.\"},\"end\":27038,\"start\":27010},{\"end\":27812,\"start\":27793},{\"attributes\":{\"n\":\"5.\"},\"end\":28024,\"start\":28014},{\"attributes\":{\"n\":\"6.\"},\"end\":28746,\"start\":28730},{\"end\":28932,\"start\":28896},{\"end\":30390,\"start\":30351},{\"end\":31049,\"start\":31008},{\"end\":32364,\"start\":32326},{\"end\":32906,\"start\":32898},{\"end\":33465,\"start\":33460},{\"end\":34132,\"start\":34127},{\"end\":34828,\"start\":34793},{\"end\":35725,\"start\":35720},{\"end\":36526,\"start\":36505},{\"end\":36812,\"start\":36807},{\"end\":37142,\"start\":37112},{\"end\":39573,\"start\":39554},{\"end\":40589,\"start\":40579},{\"end\":41589,\"start\":41588},{\"end\":42249,\"start\":42244},{\"end\":44298,\"start\":44289},{\"end\":44789,\"start\":44780},{\"end\":44967,\"start\":44958},{\"end\":45109,\"start\":45099}]", "table": "[{\"end\":43073,\"start\":42787},{\"end\":44287,\"start\":43369},{\"end\":44778,\"start\":44568},{\"end\":45662,\"start\":45438}]", "figure_caption": "[{\"end\":41379,\"start\":40591},{\"end\":41586,\"start\":41382},{\"end\":42242,\"start\":41590},{\"end\":42782,\"start\":42251},{\"end\":42787,\"start\":42785},{\"end\":43369,\"start\":43076},{\"end\":44568,\"start\":44300},{\"end\":44956,\"start\":44791},{\"end\":45097,\"start\":44969},{\"end\":45438,\"start\":45112}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":2365,\"start\":2357},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":7450,\"start\":7442},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13772,\"start\":13764},{\"end\":21418,\"start\":21410},{\"end\":24444,\"start\":24436}]", "bib_author_first_name": "[{\"end\":45743,\"start\":45736},{\"end\":45754,\"start\":45749},{\"end\":45757,\"start\":45755},{\"end\":45769,\"start\":45762},{\"end\":45781,\"start\":45780},{\"end\":45795,\"start\":45788},{\"end\":46271,\"start\":46264},{\"end\":46279,\"start\":46272},{\"end\":46298,\"start\":46290},{\"end\":46311,\"start\":46308},{\"end\":46330,\"start\":46323},{\"end\":46344,\"start\":46338},{\"end\":46358,\"start\":46353},{\"end\":46375,\"start\":46370},{\"end\":46770,\"start\":46762},{\"end\":46785,\"start\":46779},{\"end\":47002,\"start\":46994},{\"end\":47016,\"start\":47012},{\"end\":47035,\"start\":47026},{\"end\":47528,\"start\":47522},{\"end\":47541,\"start\":47535},{\"end\":47553,\"start\":47548},{\"end\":47562,\"start\":47558},{\"end\":47576,\"start\":47568},{\"end\":48094,\"start\":48090},{\"end\":48106,\"start\":48101},{\"end\":48126,\"start\":48118},{\"end\":48144,\"start\":48136},{\"end\":48486,\"start\":48483},{\"end\":48496,\"start\":48493},{\"end\":48510,\"start\":48503},{\"end\":48525,\"start\":48519},{\"end\":48533,\"start\":48530},{\"end\":48540,\"start\":48538},{\"end\":48919,\"start\":48918},{\"end\":49313,\"start\":49306},{\"end\":49327,\"start\":49323},{\"end\":49938,\"start\":49931},{\"end\":49948,\"start\":49943},{\"end\":49959,\"start\":49954},{\"end\":49971,\"start\":49964},{\"end\":49981,\"start\":49977},{\"end\":50446,\"start\":50439},{\"end\":50458,\"start\":50451},{\"end\":50474,\"start\":50466},{\"end\":50484,\"start\":50480},{\"end\":50848,\"start\":50840},{\"end\":50862,\"start\":50857},{\"end\":50876,\"start\":50872},{\"end\":51193,\"start\":51189},{\"end\":51215,\"start\":51209},{\"end\":51230,\"start\":51225},{\"end\":51254,\"start\":51250},{\"end\":51267,\"start\":51263},{\"end\":51283,\"start\":51277},{\"end\":51665,\"start\":51660},{\"end\":51679,\"start\":51673},{\"end\":51999,\"start\":51990},{\"end\":52015,\"start\":52009},{\"end\":52028,\"start\":52023},{\"end\":52399,\"start\":52394},{\"end\":52419,\"start\":52414},{\"end\":52429,\"start\":52424},{\"end\":52680,\"start\":52676},{\"end\":52880,\"start\":52874},{\"end\":52891,\"start\":52887},{\"end\":52904,\"start\":52897},{\"end\":52919,\"start\":52910},{\"end\":52930,\"start\":52924},{\"end\":52937,\"start\":52935},{\"end\":52951,\"start\":52943},{\"end\":52966,\"start\":52958},{\"end\":53371,\"start\":53364},{\"end\":53387,\"start\":53380},{\"end\":53402,\"start\":53396},{\"end\":53411,\"start\":53403},{\"end\":53426,\"start\":53419},{\"end\":53442,\"start\":53437},{\"end\":53456,\"start\":53450},{\"end\":53823,\"start\":53816},{\"end\":53838,\"start\":53834},{\"end\":53850,\"start\":53845},{\"end\":53871,\"start\":53866},{\"end\":54179,\"start\":54175},{\"end\":54195,\"start\":54189},{\"end\":54212,\"start\":54204},{\"end\":54224,\"start\":54218},{\"end\":54247,\"start\":54236},{\"end\":54652,\"start\":54646},{\"end\":54664,\"start\":54661},{\"end\":54675,\"start\":54674},{\"end\":55021,\"start\":55016},{\"end\":55038,\"start\":55032},{\"end\":55235,\"start\":55227},{\"end\":55247,\"start\":55242},{\"end\":55265,\"start\":55258},{\"end\":55551,\"start\":55542},{\"end\":55562,\"start\":55558},{\"end\":55961,\"start\":55954},{\"end\":55973,\"start\":55966},{\"end\":55982,\"start\":55981},{\"end\":55996,\"start\":55991},{\"end\":56488,\"start\":56483},{\"end\":56501,\"start\":56494},{\"end\":56513,\"start\":56507},{\"end\":56524,\"start\":56519},{\"end\":57056,\"start\":57050},{\"end\":57073,\"start\":57068},{\"end\":57339,\"start\":57333},{\"end\":57356,\"start\":57351},{\"end\":57583,\"start\":57582},{\"end\":57596,\"start\":57593},{\"end\":57859,\"start\":57855},{\"end\":57874,\"start\":57867},{\"end\":57883,\"start\":57880},{\"end\":58306,\"start\":58299},{\"end\":58319,\"start\":58314},{\"end\":58334,\"start\":58326},{\"end\":58344,\"start\":58340}]", "bib_author_last_name": "[{\"end\":45747,\"start\":45744},{\"end\":45760,\"start\":45758},{\"end\":45778,\"start\":45770},{\"end\":45786,\"start\":45782},{\"end\":45804,\"start\":45796},{\"end\":45809,\"start\":45806},{\"end\":46288,\"start\":46280},{\"end\":46306,\"start\":46299},{\"end\":46321,\"start\":46312},{\"end\":46336,\"start\":46331},{\"end\":46351,\"start\":46345},{\"end\":46368,\"start\":46359},{\"end\":46381,\"start\":46376},{\"end\":46777,\"start\":46771},{\"end\":46792,\"start\":46786},{\"end\":47010,\"start\":47003},{\"end\":47024,\"start\":47017},{\"end\":47051,\"start\":47036},{\"end\":47533,\"start\":47529},{\"end\":47546,\"start\":47542},{\"end\":47556,\"start\":47554},{\"end\":47566,\"start\":47563},{\"end\":47587,\"start\":47577},{\"end\":48099,\"start\":48095},{\"end\":48116,\"start\":48107},{\"end\":48134,\"start\":48127},{\"end\":48151,\"start\":48145},{\"end\":48491,\"start\":48487},{\"end\":48501,\"start\":48497},{\"end\":48517,\"start\":48511},{\"end\":48528,\"start\":48526},{\"end\":48536,\"start\":48534},{\"end\":48548,\"start\":48541},{\"end\":48926,\"start\":48920},{\"end\":48935,\"start\":48928},{\"end\":48958,\"start\":48937},{\"end\":49321,\"start\":49314},{\"end\":49337,\"start\":49328},{\"end\":49941,\"start\":49939},{\"end\":49952,\"start\":49949},{\"end\":49962,\"start\":49960},{\"end\":49975,\"start\":49972},{\"end\":49990,\"start\":49982},{\"end\":50449,\"start\":50447},{\"end\":50464,\"start\":50459},{\"end\":50478,\"start\":50475},{\"end\":50488,\"start\":50485},{\"end\":50855,\"start\":50849},{\"end\":50870,\"start\":50863},{\"end\":50881,\"start\":50877},{\"end\":51207,\"start\":51194},{\"end\":51223,\"start\":51216},{\"end\":51248,\"start\":51231},{\"end\":51261,\"start\":51255},{\"end\":51275,\"start\":51268},{\"end\":51293,\"start\":51284},{\"end\":51301,\"start\":51295},{\"end\":51671,\"start\":51666},{\"end\":51684,\"start\":51680},{\"end\":52007,\"start\":52000},{\"end\":52021,\"start\":52016},{\"end\":52036,\"start\":52029},{\"end\":52042,\"start\":52038},{\"end\":52412,\"start\":52400},{\"end\":52422,\"start\":52420},{\"end\":52437,\"start\":52430},{\"end\":52689,\"start\":52681},{\"end\":52885,\"start\":52881},{\"end\":52895,\"start\":52892},{\"end\":52908,\"start\":52905},{\"end\":52922,\"start\":52920},{\"end\":52933,\"start\":52931},{\"end\":52941,\"start\":52938},{\"end\":52956,\"start\":52952},{\"end\":52972,\"start\":52967},{\"end\":53378,\"start\":53372},{\"end\":53394,\"start\":53388},{\"end\":53417,\"start\":53412},{\"end\":53435,\"start\":53427},{\"end\":53448,\"start\":53443},{\"end\":53463,\"start\":53457},{\"end\":53472,\"start\":53465},{\"end\":53832,\"start\":53824},{\"end\":53843,\"start\":53839},{\"end\":53864,\"start\":53851},{\"end\":53881,\"start\":53872},{\"end\":54187,\"start\":54180},{\"end\":54202,\"start\":54196},{\"end\":54216,\"start\":54213},{\"end\":54234,\"start\":54225},{\"end\":54252,\"start\":54248},{\"end\":54659,\"start\":54653},{\"end\":54672,\"start\":54665},{\"end\":54691,\"start\":54676},{\"end\":55030,\"start\":55022},{\"end\":55048,\"start\":55039},{\"end\":55240,\"start\":55236},{\"end\":55256,\"start\":55248},{\"end\":55271,\"start\":55266},{\"end\":55556,\"start\":55552},{\"end\":55567,\"start\":55563},{\"end\":55964,\"start\":55962},{\"end\":55979,\"start\":55974},{\"end\":55989,\"start\":55983},{\"end\":55999,\"start\":55997},{\"end\":56004,\"start\":56001},{\"end\":56492,\"start\":56489},{\"end\":56505,\"start\":56502},{\"end\":56517,\"start\":56514},{\"end\":56528,\"start\":56525},{\"end\":57066,\"start\":57057},{\"end\":57083,\"start\":57074},{\"end\":57349,\"start\":57340},{\"end\":57366,\"start\":57357},{\"end\":57591,\"start\":57584},{\"end\":57603,\"start\":57597},{\"end\":57611,\"start\":57605},{\"end\":57865,\"start\":57860},{\"end\":57878,\"start\":57875},{\"end\":57886,\"start\":57884},{\"end\":58312,\"start\":58307},{\"end\":58324,\"start\":58320},{\"end\":58338,\"start\":58335},{\"end\":58348,\"start\":58345}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":118649278},\"end\":46224,\"start\":45675},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":44220142},\"end\":46659,\"start\":46226},{\"attributes\":{\"id\":\"b2\"},\"end\":46688,\"start\":46661},{\"attributes\":{\"doi\":\"arXiv:1710.05050\",\"id\":\"b3\"},\"end\":46973,\"start\":46690},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11253972},\"end\":47448,\"start\":46975},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":29308926},\"end\":48017,\"start\":47450},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b6\",\"matched_paper_id\":211096730},\"end\":48428,\"start\":48019},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":57246310},\"end\":48839,\"start\":48430},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":122708573},\"end\":49210,\"start\":48841},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15816723},\"end\":49862,\"start\":49212},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207930212},\"end\":50391,\"start\":49864},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":50838,\"start\":50393},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b12\"},\"end\":51106,\"start\":50840},{\"attributes\":{\"doi\":\"arXiv:1808.06670\",\"id\":\"b13\"},\"end\":51593,\"start\":51108},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":575794},\"end\":51988,\"start\":51595},{\"attributes\":{\"doi\":\"arXiv:1606.00709\",\"id\":\"b15\"},\"end\":52332,\"start\":51990},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b16\"},\"end\":52628,\"start\":52334},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2034914},\"end\":52821,\"start\":52630},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":102483463},\"end\":53362,\"start\":52823},{\"attributes\":{\"doi\":\"arXiv:1412.6550\",\"id\":\"b19\"},\"end\":53721,\"start\":53364},{\"attributes\":{\"doi\":\"arXiv:1206.4664\",\"id\":\"b20\"},\"end\":54117,\"start\":53723},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4555207},\"end\":54644,\"start\":54119},{\"attributes\":{\"doi\":\"arXiv:1610.09650\",\"id\":\"b22\"},\"end\":54946,\"start\":54646},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b23\"},\"end\":55225,\"start\":54948},{\"attributes\":{\"doi\":\"arXiv:1910.10699\",\"id\":\"b24\"},\"end\":55494,\"start\":55227},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":198179476},\"end\":55878,\"start\":55496},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4591284},\"end\":56382,\"start\":55880},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206596723},\"end\":56929,\"start\":56384},{\"attributes\":{\"doi\":\"arXiv:1612.03928\",\"id\":\"b28\"},\"end\":57329,\"start\":56931},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b29\"},\"end\":57526,\"start\":57331},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":3960646},\"end\":57825,\"start\":57528},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":53292120},\"end\":58213,\"start\":57827},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":24982157},\"end\":58740,\"start\":58215},{\"attributes\":{\"id\":\"b33\"},\"end\":59234,\"start\":58742}]", "bib_title": "[{\"end\":45734,\"start\":45675},{\"end\":46262,\"start\":46226},{\"end\":46992,\"start\":46975},{\"end\":47520,\"start\":47450},{\"end\":48088,\"start\":48019},{\"end\":48481,\"start\":48430},{\"end\":48916,\"start\":48841},{\"end\":49304,\"start\":49212},{\"end\":49929,\"start\":49864},{\"end\":50437,\"start\":50393},{\"end\":51658,\"start\":51595},{\"end\":52674,\"start\":52630},{\"end\":52872,\"start\":52823},{\"end\":54173,\"start\":54119},{\"end\":55540,\"start\":55496},{\"end\":55952,\"start\":55880},{\"end\":56481,\"start\":56384},{\"end\":57580,\"start\":57528},{\"end\":57853,\"start\":57827},{\"end\":58297,\"start\":58215}]", "bib_author": "[{\"end\":45749,\"start\":45736},{\"end\":45762,\"start\":45749},{\"end\":45780,\"start\":45762},{\"end\":45788,\"start\":45780},{\"end\":45806,\"start\":45788},{\"end\":45811,\"start\":45806},{\"end\":46290,\"start\":46264},{\"end\":46308,\"start\":46290},{\"end\":46323,\"start\":46308},{\"end\":46338,\"start\":46323},{\"end\":46353,\"start\":46338},{\"end\":46370,\"start\":46353},{\"end\":46383,\"start\":46370},{\"end\":46779,\"start\":46762},{\"end\":46794,\"start\":46779},{\"end\":47012,\"start\":46994},{\"end\":47026,\"start\":47012},{\"end\":47053,\"start\":47026},{\"end\":47535,\"start\":47522},{\"end\":47548,\"start\":47535},{\"end\":47558,\"start\":47548},{\"end\":47568,\"start\":47558},{\"end\":47589,\"start\":47568},{\"end\":48101,\"start\":48090},{\"end\":48118,\"start\":48101},{\"end\":48136,\"start\":48118},{\"end\":48153,\"start\":48136},{\"end\":48493,\"start\":48483},{\"end\":48503,\"start\":48493},{\"end\":48519,\"start\":48503},{\"end\":48530,\"start\":48519},{\"end\":48538,\"start\":48530},{\"end\":48550,\"start\":48538},{\"end\":48928,\"start\":48918},{\"end\":48937,\"start\":48928},{\"end\":48960,\"start\":48937},{\"end\":49323,\"start\":49306},{\"end\":49339,\"start\":49323},{\"end\":49943,\"start\":49931},{\"end\":49954,\"start\":49943},{\"end\":49964,\"start\":49954},{\"end\":49977,\"start\":49964},{\"end\":49992,\"start\":49977},{\"end\":50451,\"start\":50439},{\"end\":50466,\"start\":50451},{\"end\":50480,\"start\":50466},{\"end\":50490,\"start\":50480},{\"end\":50857,\"start\":50840},{\"end\":50872,\"start\":50857},{\"end\":50883,\"start\":50872},{\"end\":51209,\"start\":51189},{\"end\":51225,\"start\":51209},{\"end\":51250,\"start\":51225},{\"end\":51263,\"start\":51250},{\"end\":51277,\"start\":51263},{\"end\":51295,\"start\":51277},{\"end\":51303,\"start\":51295},{\"end\":51673,\"start\":51660},{\"end\":51686,\"start\":51673},{\"end\":52009,\"start\":51990},{\"end\":52023,\"start\":52009},{\"end\":52038,\"start\":52023},{\"end\":52044,\"start\":52038},{\"end\":52414,\"start\":52394},{\"end\":52424,\"start\":52414},{\"end\":52439,\"start\":52424},{\"end\":52691,\"start\":52676},{\"end\":52887,\"start\":52874},{\"end\":52897,\"start\":52887},{\"end\":52910,\"start\":52897},{\"end\":52924,\"start\":52910},{\"end\":52935,\"start\":52924},{\"end\":52943,\"start\":52935},{\"end\":52958,\"start\":52943},{\"end\":52974,\"start\":52958},{\"end\":53380,\"start\":53364},{\"end\":53396,\"start\":53380},{\"end\":53419,\"start\":53396},{\"end\":53437,\"start\":53419},{\"end\":53450,\"start\":53437},{\"end\":53465,\"start\":53450},{\"end\":53474,\"start\":53465},{\"end\":53834,\"start\":53816},{\"end\":53845,\"start\":53834},{\"end\":53866,\"start\":53845},{\"end\":53883,\"start\":53866},{\"end\":54189,\"start\":54175},{\"end\":54204,\"start\":54189},{\"end\":54218,\"start\":54204},{\"end\":54236,\"start\":54218},{\"end\":54254,\"start\":54236},{\"end\":54661,\"start\":54646},{\"end\":54674,\"start\":54661},{\"end\":54693,\"start\":54674},{\"end\":55032,\"start\":55016},{\"end\":55050,\"start\":55032},{\"end\":55242,\"start\":55227},{\"end\":55258,\"start\":55242},{\"end\":55273,\"start\":55258},{\"end\":55558,\"start\":55542},{\"end\":55569,\"start\":55558},{\"end\":55966,\"start\":55954},{\"end\":55981,\"start\":55966},{\"end\":55991,\"start\":55981},{\"end\":56001,\"start\":55991},{\"end\":56006,\"start\":56001},{\"end\":56494,\"start\":56483},{\"end\":56507,\"start\":56494},{\"end\":56519,\"start\":56507},{\"end\":56530,\"start\":56519},{\"end\":57068,\"start\":57050},{\"end\":57085,\"start\":57068},{\"end\":57351,\"start\":57333},{\"end\":57368,\"start\":57351},{\"end\":57593,\"start\":57582},{\"end\":57605,\"start\":57593},{\"end\":57613,\"start\":57605},{\"end\":57867,\"start\":57855},{\"end\":57880,\"start\":57867},{\"end\":57888,\"start\":57880},{\"end\":58314,\"start\":58299},{\"end\":58326,\"start\":58314},{\"end\":58340,\"start\":58326},{\"end\":58350,\"start\":58340}]", "bib_venue": "[{\"end\":45892,\"start\":45811},{\"end\":46427,\"start\":46383},{\"end\":46665,\"start\":46661},{\"end\":46760,\"start\":46690},{\"end\":47151,\"start\":47053},{\"end\":47678,\"start\":47589},{\"end\":48201,\"start\":48157},{\"end\":48613,\"start\":48550},{\"end\":49010,\"start\":48960},{\"end\":49435,\"start\":49339},{\"end\":50073,\"start\":49992},{\"end\":50567,\"start\":50490},{\"end\":50943,\"start\":50899},{\"end\":51187,\"start\":51108},{\"end\":51750,\"start\":51686},{\"end\":52137,\"start\":52060},{\"end\":52392,\"start\":52334},{\"end\":52709,\"start\":52691},{\"end\":53045,\"start\":52974},{\"end\":53513,\"start\":53489},{\"end\":53814,\"start\":53723},{\"end\":54331,\"start\":54254},{\"end\":54773,\"start\":54709},{\"end\":55014,\"start\":54948},{\"end\":55328,\"start\":55289},{\"end\":55640,\"start\":55569},{\"end\":56083,\"start\":56006},{\"end\":56607,\"start\":56530},{\"end\":57048,\"start\":56931},{\"end\":57651,\"start\":57613},{\"end\":57969,\"start\":57888},{\"end\":58427,\"start\":58350},{\"end\":58957,\"start\":58742},{\"end\":45960,\"start\":45894},{\"end\":47236,\"start\":47153},{\"end\":47754,\"start\":47680},{\"end\":49518,\"start\":49437},{\"end\":50141,\"start\":50075},{\"end\":50631,\"start\":50569},{\"end\":51801,\"start\":51752},{\"end\":53103,\"start\":53047},{\"end\":54395,\"start\":54333},{\"end\":55698,\"start\":55642},{\"end\":56147,\"start\":56085},{\"end\":56671,\"start\":56609},{\"end\":58037,\"start\":57971},{\"end\":58491,\"start\":58429}]"}}}, "year": 2023, "month": 12, "day": 17}