{"id": 54666001, "updated": "2023-10-04 20:30:12.3", "metadata": {"title": "IRLAS: Inverse Reinforcement Learning for Architecture Search", "authors": "[{\"first\":\"Minghao\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Zhao\",\"last\":\"Zhong\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Dahua\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Junjie\",\"last\":\"Yan\",\"middle\":[]}]", "venue": "ArXiv", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "In this paper, we propose an inverse reinforcement learning method for architecture search (IRLAS), which trains an agent to learn to search network structures that are topologically inspired by human-designed network. Most existing architecture search approaches totally neglect the topological characteristics of architectures, which results in complicated architecture with a high inference latency. Motivated by the fact that human-designed networks are elegant in topology with a fast inference speed, we propose a mirror stimuli function inspired by biological cognition theory to extract the abstract topological knowledge of an expert human-design network (ResNeXt). To avoid raising a too strong prior over the search space, we introduce inverse reinforcement learning to train the mirror stimuli function and exploit it as a heuristic guidance for architecture search, easily generalized to different architecture search algorithms. On CIFAR-10, the best architecture searched by our proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our model achieves a state-of-the-art top-1 accuracy 75.28%, while being 2~4x faster than most auto-generated architectures. A fast version of this model achieves 10% faster than MobileNetV2, while maintaining a higher accuracy.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1812.05285", "mag": "2966540493", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/GuoZWLY19", "doi": "10.1109/cvpr.2019.00923"}}, "content": {"source": {"pdf_hash": "cb688133e1f4c122626cbb1c27ba673640e289eb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1812.05285v5.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1812.05285", "status": "GREEN"}}, "grobid": {"id": "afe34a955c2247a7c8188f37c565f8272c3be833", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cb688133e1f4c122626cbb1c27ba673640e289eb.txt", "contents": "\nIRLAS: Inverse Reinforcement Learning for Architecture Search\n\n\nMinghao Guo \nThe Chinese University of Hong Kong\nUniversity of Chinese Academy of Sciences\nThe Chinese University of Hong\nKong\n\nNLPRZhao Zhong zhao.zhong@nlpr.ia.ac.cn \nThe Chinese University of Hong Kong\nUniversity of Chinese Academy of Sciences\nThe Chinese University of Hong\nKong\n\nCasia \nThe Chinese University of Hong Kong\nUniversity of Chinese Academy of Sciences\nThe Chinese University of Hong\nKong\n\nWei Wu wuwei@sensetime.com \nThe Chinese University of Hong Kong\nUniversity of Chinese Academy of Sciences\nThe Chinese University of Hong\nKong\n\nSensetime Research \nThe Chinese University of Hong Kong\nUniversity of Chinese Academy of Sciences\nThe Chinese University of Hong\nKong\n\nDahua Lin dhlin@ie.cuhk.edu.hk \nThe Chinese University of Hong Kong\nUniversity of Chinese Academy of Sciences\nThe Chinese University of Hong\nKong\n\nJunjie Yan yanjunjie@sensetime.com \nThe Chinese University of Hong Kong\nUniversity of Chinese Academy of Sciences\nThe Chinese University of Hong\nKong\n\nSensetime Research \nThe Chinese University of Hong Kong\nUniversity of Chinese Academy of Sciences\nThe Chinese University of Hong\nKong\n\nIRLAS: Inverse Reinforcement Learning for Architecture Search\n\nIn this paper, we propose an inverse reinforcement learning method for architecture search (IRLAS), which trains an agent to learn to search network structures that are topologically inspired by human-designed network. Most existing architecture search approaches totally neglect the topological characteristics of architectures, which results in complicated architecture with a high inference latency. Motivated by the fact that human-designed networks are elegant in topology with a fast inference speed, we propose a mirror stimuli function inspired by biological cognition theory to extract the abstract topological knowledge of an expert human-design network (ResNet). To avoid raising a too strong prior over the search space, we introduce inverse reinforcement learning to train the mirror stimuli function and exploit it as a heuristic guidance for architecture search, easily generalized to different architecture search algorithms. On CIFAR-10, the best architecture searched by our proposed IRLAS achieves 2.60% error rate. For Im-ageNet mobile setting, our model achieves a state-of-the-art top-1 accuracy 75.28%, while being 2\u223c4\u00d7 faster than most auto-generated architectures. A fast version of this model achieves 10% faster than MobileNetV2, while maintaining a higher accuracy.\n\nIntroduction\n\nThe past several years have witnessed the remarkable success of convolutional neural networks in computer vision applications. Thanks to the advances in network architectures, e.g. ResNet [11], Inception [29] and DenseNet [15], the performances on a number of key tasks, such as image classification, object detection, and semantic segmentation, have been taken to an amazing level. However, every step along the way of network design improve-ment requires extensive efforts from experienced experts and takes a long period of time. This already constitutes a significant obstacle to further progress.\n\nNaturally, automatically finding suitable network architectures for a given task becomes an alternative option and is gaining ground in recent years. Along this direction, a number of network search methods have been developed, including evolution [27,32], surrogate model based search [19,22], and reinforcement learning [37,38,35,4]. Whereas these methods have shown promising results and found new architectures that surpass those crafted by experts, they are still subject to a serious limitation -autogenerated networks usually have a rather high inference latency, making them difficult to be deployed on practical system with limited computational capabilities. An important cause to this issue is that auto-generated structures are often excessively complicated, which, as observed in [21], tends to adversely influence the run-time efficiency. While there have been attempts [30] to incorporate latency information to guide the search, the problem has not been effectively solved -the search algorithms themselves still follow a predefined way for network motif construction, e.g. recursively expanding a tree structure as in NASNet [38], without enforcing any explicit guidance to the network topology.\n\nIn this work, we aim to explore a new approach that explicitly takes the topological structure into account. Our efforts are motivated by the observation that human-designed networks are usually topologically simple, as shown in Figure 1, especially when compared to auto-generated ones, and often stride a better balance between accuracy and efficiency. These designs are often grounded on the rich experiences obtained through many years of joint efforts by the community, which are valuable resources and deserve to be leveraged during the searching process.\n\nSpecifically, we propose an inverse reinforcement learning method for architecture search (IRLAS). At the heart of Figure 1. Topologies of different architectures. Human-designed architectures have a more simple and elegant topology than existing auto-generated architectures. Our IRLAS aims to search topologically elegant architectures guided by human-designed networks. (a) ResNeXt [33]; (b) NASNet [38]; (c) Best performed architecture found by our IRLAS. this method is a mirror stimuli function learned by inverse reinforcement learning. This function is expected to reward those architectures that are topologically similar to the networks designed by experts. During the searching process, an agent resorts to this function to provide structural guidance, so as to generate networks with desirable architectures, similar to those crafted by experts.\n\nThis method has two benefits: (1) While the search receives guidance from the mirror stimuli function, it is not restricted. The agent is allowed to explore instead of just copying the experts. (2) The mirror stimuli function is generic and is orthogonal to the design of search space and strategy. Hence, it can be readily generalized to different search settings. On both CIFAR-10 [17] and ImageNet [8], IRLAS is able to find new architectures that yield high accuracies while maintaining low inference latency.\n\nOur contributions are summarized as follows: 1) We propose a mirror stimuli function that can provide topological guidance to architecture search, based on the knowledge learned from the expert-designed networks. This function can be easily generalized to different architecture search algorithms. 2) We introduce inverse reinforcement learning algorithm to train the mirror stimuli function, which helps the agent to efficiently explore the large search space without being overly restricted.\n\n3) The network searched by our IRLAS is topologically similar to the given expert network and shows competitive accuracy and high inference speed, compared to both state-of-the-art human-designed and autosearched networks. On CIFAR-10, the best architecture searched by our proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our model achieves a stateof-the-art top-1 accuracy 75.28%, while being 2\u223c4\u00d7 faster than most auto-generated architectures. A fast version of this model achieves 10% faster than MobileNetV2, while maintaining a higher accuracy.\n\n\nRelated Work\n\n\nNeural Architecture Search\n\nNeural architecture search focuses on automatically searching effective neural topologies in a given architecture space. Existing architecture search methods can be mainly classified into three categories: evolutionary, surrogate model based search and reinforcement learning. Evolutionary methods [10,32,25] aim to simultaneously evolve the topology of a neural network along with its weights and hyperparameters to evolve a population of networks. Early evolutionary approaches utilized genetic algorithms to optimize both the architecture and its weights, while recent studies used gradient-based methods and evolutionary algorithms to optimize the weights and architecture respectively.\n\nSurrogate model based search methods [19,5,22] utilize sequential model-based optimization as a technique for parameter optimization. Typical methods like PNAS [19] performed a progressive scan of the neural architecture search space, which was constrained according to the state-of-theart of previous iterations. EPNAS in [22] further increased the search efficiency by sharing weights among sampled architectures. However, these methods generate architectures greedily by picking the top K at each iteration, which may result in a sub-optimum over the search space.\n\nReinforcement learning (RL) methods [4,37,35,38,25] formulate the generation of a neural architecture as an agent's action, whose space is identical to the architecture search space. The agent's reward is the performance of the trained architecture on unseen data. Differences between different RL-based approaches lie in the representation of agent's policy and how to optimize it. For example, [37] used a recurrent neural network (RNN) to sample a sequence of string which encoded the neural architecture. Policy gradient algorithms including REINFORCE and Proximal Policy Optimization (PPO) were used to train the agent. [4] and [35] used Q-learning to train a policy that sequentially chose a layer's type and its corresponding hyperparameters. There are some other RL-based methods that transform existing architectures incrementally to avoid generating entire networks from scratch, such as [6]. However, these approaches could not visit the same architecture twice so that strong generalization over the architecture space was required from the policy. Instead of directly using an existing architecture as an initialization, our IR-LAS aims to learn a mirror stimuli function, and utilizes it in the searching process as a heuristic guidance without any restraints for the search space.\n\nThere also exist recent efforts [20] introducing a realvalued architecture parameter, which was jointly trained with weight parameters. Different from other methods, this kind of algorithm does not involve architecture sampling during searching process. Our mirror stimuli function can Figure 2. The pipeline of our IRLAS. We propose a mirror stimuli function to extract the abstract representation for topological characteristic of the expert. Topology structures of networks are converted to state feature code as the input of mirror stimuli function. During the agent's searching process, the mirror stimuli function is utilized as a heuristic guidance to generate desirable human-designed-like networks. Inverse reinforcement learning is utilized to train the mirror stimuli function, which helps the agent to efficiently explore the large search space without being overly restricted. also be generalized to this brunch of methods.\n\n\nImitation Learning\n\nAs our proposed IRLAS attempts to generate architectures that are topologically similar to human-designed networks, the learning for the agent involves imitation learning problem. Imitation Learning (IL) enables an agent to learn from demonstrations of an expert, independent of any specific knowledge in the proposed task. There exist two different areas for IL: policy imitation and inverse reinforcement learning. Policy imitation, which is also known as behavioral cloning, targets directly learning the policy mapping from perceived environment or preprocessed features to the agent's actions. For the settings of this paper, since the number of human-designed networks is limited, it is hard to obtain sufficient number of expert's state-action tuples for supervised learning. As a result, the direct policy imitation cannot be used for our purpose.\n\nInverse reinforcement learning (IRL) refers to the problem of deriving a reward function from observed behavior. As it is a common presupposition that reward function is a succinct, robust and transferable definition of a task, IRL provides a more effective form of IL than policy imitation. Early studies in IRL [3,36,24] assumed that the expert was trying to optimize an unknown reward function that could be expressed as a linear combination of pre-determined features. [7] extended this approach to a limited set of nonlinear rewards and learned to build composites of logical conjunctions for atomic features. Other flexible non-linear function approximators such as Gaussian Processes further extended the modeling capacity of IRL models [18]. In this paper, we assume the reward function of the expert network as a linear parametrization of state features. Experiments show that this simple assumption is effective enough to extract the topological knowledge of the human-designed architectures.\n\n\nApproach\n\nIn this section, we first present the problem formulation of architecture search. Then we propose the mirror stimuli function inspired by biological cognition and its training procedure via inverse reinforcement learning. Finally we detail the search space and the searching algorithm. The pipeline of our IRLAS is shown in Figure 2.\n\n\nProblem Formulation\n\nLike modern CNNs, our automatic neural network process designs the topological structure of block instead of the entire network. This block-wise design is more flexible for different datasets and tasks with powerful generalization ability. The task of the agent is to sequentially sample layers from the pool of layer candidates to form the block. Then the block structure is stacked sequentially to form the complete network. For different datasets, we manually choose different number of down-sampling operations due to different input image size and choose different repeat times of the block to meet the demand for limitation of parameters or FLOPs.\n\nIn this paper, we consider the design process of network topology as a variable-length decision sequence for the choice of operation. And this sequential process can be formulated as a Markov decision process (MDP). The policy \u03c0 : S \u2192 A, where S is the state space and A is the action space, determines the agent's behavioral preference of generating architectures. The state s \u2208 S is the status of current layer. The action a \u2208 A is the decision for the subsequent layer. Thus, an architecture m sampled by the agent can be determined by a state-action trajectory according to the policy \u03c0, i.e. m = {(s t , a t )} t=1...T .\n\nThe training of the agent is to maximize the expected reward over all possible architectures,\nJ \u03c0 = E \u03c0 [R(m)],(1)\nwhere R(\u00b7) is the reward function. A common definition of R(m) is the validation accuracy of the corresponding network. This formulation of the reward function is based on an assumption that the evaluation for an architecture is only determined by its validation performance, while totally neglect the topology information.\n\n\nTopological Knowledge\n\nAs the human-designed architectures are demonstrated to be effective in practice, we attempt to utilize such existing abundant topological knowledge as efficacious guidance for architecture search. However, it is a challenging problem to find an effective method to formalize the abstract topological knowledge and design an appropriate way to further exploit it in the search process. For example, shortcut connection of the block in ResNet is a quotable structure for architecture generating. Human can easily understand the topological structure simply by visualization, while the agent cannot. It remains harder for the agent to learn to search ResNet-like architectures if it even cannot understand the topology. This naturally raises two basic problems: 1) How to encode network architecture to extract the abstract topological knowledge as an available input for the agent? 2) How to utilize this knowledge to guide the agent to design desirable architectures?\n\nFor the first problem, we need to define a feature embedding for network architectures. To encode the architecture, we carefully choose a state feature function \u03c6 : S \u2192 R k\u00d71 , which consists of: operation type, kernel size, and the indexes of two predecessor of the current layer (for layer with only one predecessor, one of the indexes is set to zero). Despite the simplicity, this state feature function provides a complete characterization of the network architecture, including the information about the computation carried out by individual layers as well as how the layers are connected.\n\nWe further exploit feature count to unify the information of each state feature to get the feature embedding for the whole architecture. Given an architecture's sequential trajectory m = {(s t , a t )} t=1...T , the feature count is defined as:\n\u00b5 = T t=1 \u03b3 t \u03c6(s t ),(2)\nwhere \u03b3 denotes a discounted scalar. Thus, the sequential order is also included by the discounted \u03b3 over layer index.\n\nThe feature count is utilized as an appropriate encoding for the topological knowledge of a given network.\n\nAs for the other question of how the agent uses the topological knowledge as a guidance, this encompasses the classical exploration-exploitation trade-off. We attempt the agent to search architectures that are topologically similar to the expert network, while efficiently explore the architecture search space. This requires the searching algorithm exhibiting no preferences on a specific architecture as we do not aim the agent to reproduce human-designed networks. Direct policy imitating between the feature counts of sampled architecture and expert network will raise a strong prior on the search space and force the agent to 'mimic' the expert [3,2], which does not meet our expectation.\n\n\nMirror Stimuli Function\n\nTo address this problem, we design a mirror stimuli function, denoting as F topology , which aims to sof tly guide the agent while preventing a hard and strong constraint on the search space. The design of the mirror stimuli function is inspired by the mirror neuron system in primate's premotor cortex. This system is responsible for the linkage of self-generated and observed demonstrations. The mirror neuron fires both when an animal acts and when the animal observes the same action performed by another, which is an important scheme for learning new skills by imitation. In our problem, the mirror stimuli function has a similar functionality as the mirror neuron. Given the architecture sampled by the agent as the self-generated demonstration, the expert network as the observed demonstration, our mirror stimuli function will output a signal to judge the topological similarity between these two networks. The higher output represents higher similarity, where the highest for the exact expert network.\n\nThe mirror stimuli function is defined as a linear function of feature count:\nF topology (m) = w T \u00b7 \u00b5,(3)\nwhere w \u2208 R k\u00d71 . Such a linear parametric form is easy to optimize, while effective enough to use as the evaluation of topology, as further shown in our experiment. By substituting Equation 2 to Equation 3, we can get\nF topology (m) = T t=1 \u03b3 t \u00b7 w T \u00b7 \u03c6(s t ).(4)\nThus, the problem of solving the parameter w could be regarded as the problem of finding a time-step reward function r(s t ) = w T \u00b7 \u03c6(s t ), whose corresponding policy has a maximum value at the sequence of expert network (i.e., the value of F topology (m * ), m * = {(s * t , a * t )} t=1...T denotes the expert network). This refers to the standard inverse reinforcement learning problem.\n\nTo find such an reward function, we use the feature match algorithm proposed in [3]. For the expert network, the architecture is generated following an expert policy \u03c0 * , which has a maximum value for the following expression:\nJ \u03c0 * = E \u03c0 * [ T t=1 \u03b3 t r(s t )] = w T \u00b7 E \u03c0 * [ T t=1 \u03b3 t \u03c6(s t )] = w T \u00b7 E \u03c0 * [\u00b5] = w T \u00b7 M \u03c0 * .(5)\nAs we have one expert network, M \u03c0 * is estimated as\nM \u03c0 * = E \u03c0 * [\u00b5] \u2248 \u00b5 * = T t=1 \u03b3 t \u03c6(s * t )\n. To get the weight parameter w of the unknown reward function r(s t ), we need to find a policy\u03c0 whose performance is close to that of the expert's:\n|J\u03c0 \u2212 J \u03c0 * | = |w T \u00b7 M\u03c0 \u2212 w T \u00b7 M \u03c0 * | \u2264 .(6)\nThis process could be regarded as 'imitating' the observed behavior in the mirror neuron system, which makes the self-generated demonstration (regarded as J\u03c0) similar to the observed demonstration (regarded as J \u03c0 * ). So the problem is reduced to finding a policy\u03c0 that induces the expectation of feature count M\u03c0 close to M \u03c0 * . This feature matching problem could be solved by max-margin optimization, derived as,\nmax w: w 2\u22641 min \u2200\u03bc w T \u00b7 M \u03c0 * \u2212 w T \u00b7 M\u03c0.(7)\nThus the weight parameter w is optimized following:\nmax \u03b4,w \u03b4 s.t. w T \u00b7 M \u03c0 * \u2265 w T \u00b7 M\u03c0 + \u03b4, \u2200\u03c0 w 2 \u2264 1.(8)\nThe detailed algorithm is illustrated in Algorithm 1. During the agent's training stage, we add the output of mirror stimuli function as an additional reward term. The complete reward function in Section 3.1 is calculated as:\nR(m) = F accuracy (m) + \u03bbF topology (m),(9)\nwhere F accuracy (m) denotes model m's accuracy percentage on target task, \u03bb denotes a balance scalar. By optimizing this multi-objective search problem, the agent is guided by both the topological similarity and the accuracy. Thus, the agent can efficiently explore the search space to generate high-speed, topologically elegant architectures along with high accuracy. Using standard RL algorithm, find the optimal policy as\u03c0i with reward function r (i) (s) = (w (i) ) T \u00b7 \u03c6(s); ComputeMi; i = i + 1; until \u03b4 (i) \u2264 return w;\n\n\nSearch Space and Training Strategy\n\nIn this section we introduce the search space and training strategy of our IRLAS. We will further discuss the generalization of our mirror stimuli function to other typical architecture search approaches in Section 3.5. In our IR-LAS, the search space consists of operations based on their prevalence in the CNN literature. The considered operations are: Depthwise convolution with kernel size 1\u00d71, 3\u00d73, 5\u00d75; Max pooling with kernel size 3\u00d73, 5\u00d75; Average pooling with kernel size 3\u00d73, 5\u00d75; Identity; Elemental add with two input layers; and Concat with two input layers. Note that the depthwise convolution operation refers to pre-activation convolution containing ReLU, convolution and batch normalization. All the layers without successor in the searched block are concatenated together as the final output.\n\nFor the searching stage, we utilize Q-learning method to train the agent to take actions that maximize the cumulative reward, which is formulated as Equation 9. Q-learning iteratively updates the action-selection policy following the Bellman Equation:\nQ(s t , a t ) = r t + \u03b3 max a Q(s t+1 , a ),(10)\nwhere r t denotes the intermediate reward observed for the current state s t . Since r t could not be explicitly measured, reward shaping method is used, derived as r t = R(m)/T , where T denotes the state length referring to the number of layers. The Bellman Equation is achieved following Temporal-Difference control algorithm:\nQ(s t , a t ) =(1 \u2212 \u03b7)Q(s t , a t ) + \u03b7[r t+1 + \u03b3 max a Q(s t+1 , a )],(11)\nwhere \u03b7 denotes the learning rate. The whole learning procedure is summarized as follows: The agent first samples a network architecture, which is taken as input of the mirror stimuli function. Then the generated network is trained on a certain task to get the validation accuracy. The reward, which is the combination of the accuracy and the output value of the mirror stimuli function, is used to update the Q-value. The above process circulates for iterations and the agent learns to sample block structure with higher accuracy and more elegant topology iteratively.\n\n\nGeneralization of Mirror Stimuli Function\n\nIt is worthy to point out that our mirror stimuli function can be easily generalized to different architecture search algorithms. For algorithms that involve architecture sampling and performance evaluation for the sampled architecture, including reinforcement learning based methods and evolutionary methods, we can simply utilize the output of Equation 9 as an alternative of evaluation, while the other searching steps remain the same to the original algorithm. The only difference lies in the expression of state feature function \u03c6(s), which need to be modified due to different candidate operations in the search space of different algorithms. Thus, the topological information is considered during the searching process.\n\nFor differentiable architecture search algorithm, typically DARTS [20], the architecture is encoded by a set of continuous variables \u03b1 = {\u03b1 {i,j} } ((i, j) denotes a pair of nodes, i.e. a path in the architecture). Thus, the weight parameters and architecture parameters could be trained jointly via standard gradient descent. To introduce topological information to the training procedure in differentiable architecture search algorithms, we add an additional loss term L topology calculated by mirror stimuli function to the original cross entropy loss. To convert the continuous \u03b1 to discreted architectures, we consider the sof tmax output of \u03b1 as a probabilistic distribution of all possible architectures, denoted as {p k }, and sample according to the distribution to get state feature \u03c6(s). Since the conversion from architecture parameters \u03b1 to state feature \u03c6(s) is nondifferentiable, the output of mirror stimuli function cannot be backpropagated. Here, we consider the solution based on REINFORCE algorithm [31], so the loss term L topology is calculated and updated as:\nL topology = K k=1 p k F topology (m k ) \u2207L topology \u2248 1 K K k=1 F topology (m k )\u2207log(p k ),(12)\nwhere K is the number of sampled architectures.\n\n\nExperiments and Results\n\n\nImplementation Details\n\nIn this section, we introduce the implementation details of our IRLAS. We use a distributed asynchronous framework as proposed in [35], which enables efficient network generation on multiple machines with multiple GPUs. With this framework, our IRLAS can sample and train networks in parallel to speed up the whole training process. For the inverse reinforcement learning procedure, ResNet, whose convolution operation is modified to depthwise convolution, is chosen as the expert network to calculate the weight w in the mirror stimuli function. The training procedure is about 3 hours on CPU.\n\nFor our IRLAS, we choose Q-value table as the agent. We use Q-learning with epsilon-greedy and experience replay buffer. At each training iteration, the agent samples 64 structures with their corresponding rewards from the memory to update Q-values following Equation 11. For the hyperparameters of Q-learning process, the learning rate \u03b7 is set to 0.01, the discount factor \u03b3 is 0.9 and the balance scalar \u03bb is 30. The mini-batch size is set to 64 and the maximum layer index for a block is set to 24. The agent is trained for 180 iterations, which totally samples 11,500 blocks. Each sampled architecture is trained with fixed 12 epochs with Adam optimizer to get evaluation of F accuracy .\n\nWe also generalize our mirror stimuli function to the different architecture search algorithm. We choose DARTS [20] as the basic algorithm. The additional loss term L topology is scaled by 0.5 and added to the original crossentropy loss. The number of the sampled architectures K is set to 5. All the other training details and hyperparameters follow the original paper. For both of the conditions, the architecture searching processes are proposed on dataset CIFAR-10 [16].\n\n\nResults\n\nResults on CIFAR-10 After the searching process, we select the searched optimal block structure and train the network on CIFAR-10 until convergence. In this phase, the training data is augmented with randomly cropping size of 32 \u00d7 32, horizontal flipping and Cutout [9]. The cosine learning rate scheme is utilized with the initial learning rate of 0.2. The momentum rate is set to 0.9 and weight decay is set to 0.0005. All the networks are trained for 600 epochs with 256 batch size.\n\nFor the task of image classification on CIFAR-10, we set the total number of stacked blocks as 10. The results are reported in Table 1 along with other models. We see that our proposed IRLAS achieves a 2.60% test error, which shows a state-of-the-art performance over both human-designed networks and auto-generated networks. For the differential setting, the result is reported in Table 1 as IRLAS-differential. Compared to the result reported in original paper (2.83% error rate), the searched architecture facilitated by our mirror stimuli function achieves a higher accuracy.\n\nResults on ImageNet For the ImageNet task, we transfer the model searched on CIFAR-10 by increasing the total Table 1. IRLAS's results compared with state-of-the-art methods on CIFAR-10 dataset. \"Error\" is the top-1 misclassification rate on the CIFAR-10 test set, \"Param\" is the number of model parameters.\n\nMethod Param Error(%) Resnet [11] 1.7M 6.61 Resnet (pre-activation) [12] 10.2M 4.62 Wide ResNet [34] 36.5M 4.17 DenseNet (k=12) [15] 1.0M 5.24 DenseNet (k=12) [15] 7.0M 4.10 DenseNet (k=24) [15] 27.2M 3.74 DenseNet-BC (k=40) [15] 25.6M 3.46 MetaQNN (top model) [4] 11.2M 6.92 NAS v1 [37] 4.2M 5.50 EAS [6] 23.4M 4.23 Block-QNN-A, N=4 [35] -3.60 Block-QNN-S, N=2 [35] 6.1M 3.30 NASNet-A (6 @ 768) [38] 3.3M 2.65 NASNet-B (4 @ 1152) [38] 2.6M 3.73 NASNet-C (4 @ 640) [38] 3.1M 3.59 PNASNet-5 [19] 3.2M 3.41 ENAS [23] 4 The results are illustrated in Table 2. Our IRLAS-mobile achieves a state-of-the-art accuracy over both the humandesigned and auto-generated architectures. As for the inference latency, our IRLAS-mobile can achieve 2\u223c4\u00d7 fewer inference latency compared with most auto-generated architectures benefiting from the elegant topology facilitated by our mirror stimuli function. We also further squeeze the number of stacked blocks of IRLAS-mobile and increase conduct a IRLAS-mobile-fast model with an inference speed of 9ms, making our model even faster than human-designed network MobileNetV2. Note that Mnas-Net [30] was searched directly on ImageNet dataset and need to validate time latency during searching, which is a very resource-exhausted process due to the high training cost on such a large scale dataset. As the shuffle operation, channel split operation and inverted block backbone used in ShuffleNetV2 and MobileNet-224 are not adopted in our search space, we believe our inference speed can be further Table 2. ImageNet classification results in the mobile setting. The input image size is 224\u00d7224. The inference latency is validated with 16 batch size on TensorRT framework.\n\nMethod Latency Acc (%) Inception V1 [28] -69.8 MobileNet-224 [14] 6ms 70.6 ShuffleNet [13] 10ms 70.9 MobileNetV2 1.4 [26] 10ms 74.7 ShuffleNetV2 2\u00d7 [21] 6ms 74.9 NASNet-A(4 @ 1056) [38] 23ms 74.0 AmoebaNet-A [25] 33ms 74.5 PNASNet [19] 25ms 74.2 DARTS [20] 55ms 73.1 MnasNet [30] 11ms boosted by introducing them to our searching process.\n\n\nAnalysis of Inverse Reinforcement Learning\n\nIn this section, we conduct an analysis of inverse reinforcement learning algorithm. As we introduce inverse reinforcement learning to avoid the agent to exhibit preference on the expert network, we compare the output value changes of our mirror stimuli function with those of the feature count \u00b5 by modifying a specific architecture. Here we choose the expert architecture ResNet, and modify it in three ways: M odif y1, adding a conv3\u00d73 operation before the residual function; M odif y2, adding a conv3 \u00d7 3 operation after the residual function; M odif y3, removing the short-cut connection. The results are illustrated in Figure 6 (a). Since M odif y1 and M odif y2 have a minor change in topology than M odif y3, our mirror stimuli function is able to output relative value change, where the feature count is very sensitive to tiny changes. As a result, comparing to direct feature count, our mirror stimuli function is a more reasonable guidance to avoid the agent to just mimic the expert network, which helps the agent to explore the search space without being overly restricted.\n\n\nSearch Efficiency\n\nIn this section, we perform an analysis on search efficiency. Note that the overall searching cost largely depends on the design of search strategy, which is orthogonal to the design of our mirror stimuli function. To illustrate the efficiency improvement introduced by our mirror stimuli function, we conducted two experiments based on two search algorithms of different kinds: one is BlockQNN [35], the other is DARTS [20]. For each experiment, the baseline followed the searching process proposed in original paper, compared with the searching facilitated by our mirror stimuli function. We evaluate the efficiency of by mirror stimuli function by comparing the relative improvement of convergence speed, instead of the absolute search time. Convergence curves are reported in Figure 5. For both of the con-   ditions, our methods converge faster, benefiting from the guidance provided by the expert network's topology. The results further demonstrate that our mirror stimuli function is able to be generalized to different search algorithms and improve the search efficiency.\n\n\nAblation Study\n\nIn this section, we perform analysis to illustrate how mirror stimuli function affects the topology of final searched architecture. We first illustrate topologies of top-4 block architectures searched without and with mirror stimuli function in Figure 3 and Figure 4. It is obvious that architectures searched without mirror stimuli function are complicated, including numerous operations and connections, while our searched models are much more simple and elegant. Furthermore, our searched models are more topologically similar to ResNet, each containing a shortcut following add operation to form the residual function. We further conduct IRLAS with three different \u03bb: 0, 30, 60. All three searching experiments followed the same procedure described in Section 3.4. For each experiment, top-4 models were chosen and transfered to meet the ImageNet mobile setting, with about 5M parameters. These models were then trained from scratch on ImageNet, following settings in Section 4.2. The final inference latency and accuracy of these models are illustrated in Figure 6 (b). It can be noticed that the inference speed of searched architectures can be drastically improved by utilizing mirror stimuli function, about 1\u00d7 faster. For \u03bb = 60, the prior topological knowledge of expert network is too strong for searching, which results in accuracy drop. \u03bb = 30 is regarded as a choice to balance the trade-off between accuracy and speed.\n\n\nConclusion\n\nIn this paper, we have proposed an inverse reinforcement learning method for architecture search. Based on the knowledge learned from the expert-designed networks, our mirror stimuli function can provide topological guidance to architecture search, which can be easily generalized to different architecture search algorithms. Inverse reinforcement learning method has been introduced to train this function, helping the agent to efficiently explore the large search space without being overly restricted. Experiment results have shown that our proposed IRLAS achieves to search high-speed architectures with high accuracy. How to extract representation of multiple networks to further improve the performance seems to be an interesting future work.\n\n\nCompute \u03b4 (i) in optimization problem of Equation 8 with {M } = {Mj, j = 0...i \u2212 1}, get w (i) , \u03b4 (i) ;\n\nFigure 3 .\n3Topologies of top-4 block architectures searched without mirror stimuli function.\n\nFigure 4 .\n4Topologies of top-4 block architectures searched with mirror stimuli function.\n\nFigure 5 .\n5Convergence curves for the searching processes comparing with searching without mirror stimuli function. For both of the conditions, our methods converge faster benefiting from the guidance provided by the expert network's topology.\n\nFigure 6 .\n6(a) Comparison the output value changes of mirror stimuli function and feature count for three modified models. (b) Results of inference latency and accuracy on ImageNet of 4 top models from each experiment with different \u03bb. \u03bb = 30 is used in our IRLAS to balance the trade-off between accuracy and speed.\n\nApprenticeship learning for motion planning with application to parking lot navigation. P Abbeel, D Dolgov, A Y Ng, S Thrun, IEEE/RSJ International Conference on. IEEEIntelligent Robots and SystemsP. Abbeel, D. Dolgov, A. Y. Ng, and S. Thrun. Apprentice- ship learning for motion planning with application to park- ing lot navigation. In Intelligent Robots and Systems, 2008. IROS 2008. IEEE/RSJ International Conference on, pages 1083-1090. IEEE, 2008.\n\nApprenticeship learning via inverse reinforcement learning. P Abbeel, A Y Ng, ICML. ACM1P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In ICML, page 1. ACM, 2004.\n\nDesigning neural network architectures using reinforcement learning. B Baker, O Gupta, N Naik, R Raskar, arXiv:1611.02167arXiv preprintB. Baker, O. Gupta, N. Naik, and R. Raskar. Designing neural network architectures using reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.\n\nSmash: one-shot model architecture search through hypernetworks. A Brock, T Lim, J M Ritchie, N Weston, arXiv:1708.05344arXiv preprintA. Brock, T. Lim, J. M. Ritchie, and N. Weston. Smash: one-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017.\n\nEfficient architecture search by network transformation. H Cai, T Chen, W Zhang, Y Yu, J Wang, AAAIH. Cai, T. Chen, W. Zhang, Y. Yu, and J. Wang. Efficient architecture search by network transformation. AAAI, 2018.\n\nBayesian nonparametric feature construction for inverse reinforcement learning. J Choi, K.-E Kim, IJCAI. J. Choi and K.-E. Kim. Bayesian nonparametric feature con- struction for inverse reinforcement learning. In IJCAI, pages 1287-1293, 2013.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248-255, 2009.\n\nT Devries, G W Taylor, arXiv:1708.04552Improved regularization of convolutional neural networks with cutout. arXiv preprintT. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\n\nSpeeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves. T Domhan, J T Springenberg, F Hutter, In IJCAI. 15T. Domhan, J. T. Springenberg, and F. Hutter. Speeding up automatic hyperparameter optimization of deep neural net- works by extrapolation of learning curves. In IJCAI, vol- ume 15, pages 3460-8, 2015.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016.\n\nIdentity mappings in deep residual networks. K He, X Zhang, S Ren, J Sun, ECCV. SpringerK. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, pages 630-645. Springer, 2016.\n\nShuffle net: An application of generalized perfect shuffles to multihop lightwave networks. M G Hluchyj, M J Karol, Journal of Lightwave Technology. 910M. G. Hluchyj and M. J. Karol. Shuffle net: An application of generalized perfect shuffles to multihop lightwave net- works. Journal of Lightwave Technology, 9(10):1386-1397, 1991.\n\nA G Howard, M Zhu, B Chen, D Kalenichenko, W Wang, T Weyand, M Andreetto, H Adam, arXiv:1704.04861Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprintA. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Effi- cient convolutional neural networks for mobile vision appli- cations. arXiv preprint arXiv:1704.04861, 2017.\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, CVPR. 13G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. In CVPR, vol- ume 1, page 3, 2017.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, CiteseerTechnical reportA. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.\n\nThe cifar-10 dataset. A Krizhevsky, V Nair, G Hinton, A. Krizhevsky, V. Nair, and G. Hinton. The cifar-10 dataset. online: http://www. cs. toronto. edu/kriz/cifar. html, 2014.\n\nNonlinear inverse reinforcement learning with gaussian processes. S Levine, Z Popovic, V Koltun, NIPS. S. Levine, Z. Popovic, and V. Koltun. Nonlinear inverse rein- forcement learning with gaussian processes. In NIPS, pages 19-27, 2011.\n\nProgressive neural architecture search. C Liu, B Zoph, M Neumann, J Shlens, W Hua, L.-J Li, L Fei-Fei, A Yuille, J Huang, K Murphy, ECCV. C. Liu, B. Zoph, M. Neumann, J. Shlens, W. Hua, L.-J. Li, L. Fei-Fei, A. Yuille, J. Huang, and K. Murphy. Progressive neural architecture search. In ECCV, September 2018.\n\nH Liu, K Simonyan, Y Yang, arXiv:1806.09055Darts: Differentiable architecture search. arXiv preprintH. Liu, K. Simonyan, and Y. Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018.\n\nShufflenet v2: Practical guidelines for efficient cnn architecture design. N Ma, X Zhang, H.-T Zheng, J Sun, arXiv:1807.11164arXiv preprintN. Ma, X. Zhang, H.-T. Zheng, and J. Sun. Shufflenet v2: Practical guidelines for efficient cnn architecture design. arXiv preprint arXiv:1807.11164, 2018.\n\nEfficient progressive neural architecture search. J.-M Perez-Rua, M Baccouche, S Pateux, arXiv:1808.00391arXiv preprintJ.-M. Perez-Rua, M. Baccouche, and S. Pateux. Effi- cient progressive neural architecture search. arXiv preprint arXiv:1808.00391, 2018.\n\nEfficient neural architecture search via parameter sharing. H Pham, M Y Guan, B Zoph, Q V Le, J Dean, arXiv:1802.03268arXiv preprintH. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean. Effi- cient neural architecture search via parameter sharing. arXiv preprint arXiv:1802.03268, 2018.\n\nMaximum margin planning. N D Ratliff, J A Bagnell, M A Zinkevich, ICML. ACMN. D. Ratliff, J. A. Bagnell, and M. A. Zinkevich. Maximum margin planning. In ICML, pages 729-736. ACM, 2006.\n\nRegularized evolution for image classifier architecture search. E Real, A Aggarwal, Y Huang, Q V Le, arXiv:1802.01548arXiv preprintE. Real, A. Aggarwal, Y. Huang, and Q. V. Le. Regular- ized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548, 2018.\n\nM Sandler, A Howard, M Zhu, A Zhmoginov, L.-C Chen, Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR. M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen. Mobilenetv2: Inverted residuals and linear bottle- necks. In CVPR, pages 4510-4520, 2018.\n\nS Saxena, J Verbeek, Convolutional neural fabrics. In NIPS. S. Saxena and J. Verbeek. Convolutional neural fabrics. In NIPS, pages 4053-4061, 2016.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, CVPR. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, pages 1-9, 2015.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, CVPR. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In CVPR, pages 2818-2826, 2016.\n\nM Tan, B Chen, R Pang, V Vasudevan, Q V Le, Mnasnet, arXiv:1807.11626Platform-aware neural architecture search for mobile. arXiv preprintM. Tan, B. Chen, R. Pang, V. Vasudevan, and Q. V. Le. Mnasnet: Platform-aware neural architecture search for mo- bile. arXiv preprint arXiv:1807.11626, 2018.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Machine learning. 83-4R. J. Williams. Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.\n\nGenetic cnn. L Xie, A L Yuille, ICCV. L. Xie and A. L. Yuille. Genetic cnn. In ICCV, pages 1388- 1397, 2017.\n\nAggregated residual transformations for deep neural networks. S Xie, R Girshick, P Doll\u00e1r, Z Tu, K He, CVPR. IEEES. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In CVPR, pages 5987-5995. IEEE, 2017.\n\nS Zagoruyko, N Komodakis, arXiv:1605.07146Wide residual networks. arXiv preprintS. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\n\nBlockqnn: Efficient block-wise neural network architecture generation. Z Zhong, Z Yang, B Deng, J Yan, W Wu, J Shao, C.-L Liu, arXiv:1808.05584arXiv preprintZ. Zhong, Z. Yang, B. Deng, J. Yan, W. Wu, J. Shao, and C.-L. Liu. Blockqnn: Efficient block-wise neural network architecture generation. arXiv preprint arXiv:1808.05584, 2018.\n\nMaximum entropy inverse reinforcement learning. B D Ziebart, A L Maas, J A Bagnell, A K Dey, AAAI. Chicago, IL, USA8B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.\n\nB Zoph, Q V Le, arXiv:1611.01578Neural architecture search with reinforcement learning. arXiv preprintB. Zoph and Q. V. Le. Neural architecture search with rein- forcement learning. arXiv preprint arXiv:1611.01578, 2016.\n\nB Zoph, V Vasudevan, J Shlens, Q V Le, arXiv:1707.07012Learning transferable architectures for scalable image recognition. 2arXiv preprintB. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le. Learn- ing transferable architectures for scalable image recognition. arXiv preprint arXiv:1707.07012, 2(6), 2017.\n", "annotations": {"author": "[{\"end\":192,\"start\":65},{\"end\":348,\"start\":193},{\"end\":470,\"start\":349},{\"end\":613,\"start\":471},{\"end\":748,\"start\":614},{\"end\":895,\"start\":749},{\"end\":1046,\"start\":896},{\"end\":1181,\"start\":1047}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":73},{\"end\":207,\"start\":202},{\"end\":477,\"start\":475},{\"end\":632,\"start\":624},{\"end\":758,\"start\":755},{\"end\":906,\"start\":903},{\"end\":1065,\"start\":1057}]", "author_first_name": "[{\"end\":72,\"start\":65},{\"end\":201,\"start\":197},{\"end\":354,\"start\":349},{\"end\":474,\"start\":471},{\"end\":623,\"start\":614},{\"end\":754,\"start\":749},{\"end\":902,\"start\":896},{\"end\":1056,\"start\":1047}]", "author_affiliation": "[{\"end\":191,\"start\":78},{\"end\":347,\"start\":234},{\"end\":469,\"start\":356},{\"end\":612,\"start\":499},{\"end\":747,\"start\":634},{\"end\":894,\"start\":781},{\"end\":1045,\"start\":932},{\"end\":1180,\"start\":1067}]", "title": "[{\"end\":62,\"start\":1},{\"end\":1243,\"start\":1182}]", "venue": null, "abstract": "[{\"end\":2538,\"start\":1245}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2746,\"start\":2742},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2762,\"start\":2758},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2780,\"start\":2776},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3409,\"start\":3405},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3412,\"start\":3409},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3447,\"start\":3443},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3450,\"start\":3447},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3483,\"start\":3479},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3486,\"start\":3483},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3489,\"start\":3486},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3491,\"start\":3489},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3954,\"start\":3950},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4045,\"start\":4041},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4303,\"start\":4299},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5323,\"start\":5319},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5340,\"start\":5336},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5990,\"start\":5987},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6180,\"start\":6176},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6197,\"start\":6194},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7723,\"start\":7719},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7726,\"start\":7723},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7729,\"start\":7726},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8154,\"start\":8150},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8156,\"start\":8154},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8159,\"start\":8156},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8277,\"start\":8273},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8440,\"start\":8436},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8721,\"start\":8718},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8724,\"start\":8721},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8727,\"start\":8724},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8730,\"start\":8727},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8733,\"start\":8730},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9082,\"start\":9078},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9310,\"start\":9307},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9319,\"start\":9315},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9583,\"start\":9580},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10015,\"start\":10011},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12111,\"start\":12108},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12114,\"start\":12111},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12117,\"start\":12114},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12271,\"start\":12268},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12543,\"start\":12539},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17630,\"start\":17627},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17632,\"start\":17630},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19559,\"start\":19556},{\"end\":22489,\"start\":22479},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24450,\"start\":24446},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25403,\"start\":25399},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25795,\"start\":25791},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26527,\"start\":26525},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27066,\"start\":27062},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27424,\"start\":27420},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27706,\"start\":27703},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28847,\"start\":28843},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28886,\"start\":28882},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28914,\"start\":28910},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28946,\"start\":28942},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28977,\"start\":28973},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29008,\"start\":29004},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29043,\"start\":29039},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29078,\"start\":29075},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29101,\"start\":29097},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29119,\"start\":29116},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29152,\"start\":29148},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29180,\"start\":29176},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":29214,\"start\":29210},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":29249,\"start\":29245},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":29283,\"start\":29279},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29308,\"start\":29304},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29328,\"start\":29324},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29945,\"start\":29941},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30559,\"start\":30555},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30584,\"start\":30580},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30609,\"start\":30605},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30640,\"start\":30636},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30671,\"start\":30667},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":30704,\"start\":30700},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30731,\"start\":30727},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30754,\"start\":30750},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30775,\"start\":30771},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30798,\"start\":30794},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":32411,\"start\":32407},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32436,\"start\":32432}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35413,\"start\":35307},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35508,\"start\":35414},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35600,\"start\":35509},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35846,\"start\":35601},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36165,\"start\":35847}]", "paragraph": "[{\"end\":3155,\"start\":2554},{\"end\":4369,\"start\":3157},{\"end\":4932,\"start\":4371},{\"end\":5791,\"start\":4934},{\"end\":6306,\"start\":5793},{\"end\":6801,\"start\":6308},{\"end\":7375,\"start\":6803},{\"end\":8111,\"start\":7421},{\"end\":8680,\"start\":8113},{\"end\":9977,\"start\":8682},{\"end\":10915,\"start\":9979},{\"end\":11793,\"start\":10938},{\"end\":12797,\"start\":11795},{\"end\":13143,\"start\":12810},{\"end\":13820,\"start\":13167},{\"end\":14447,\"start\":13822},{\"end\":14542,\"start\":14449},{\"end\":14887,\"start\":14564},{\"end\":15880,\"start\":14913},{\"end\":16476,\"start\":15882},{\"end\":16722,\"start\":16478},{\"end\":16867,\"start\":16749},{\"end\":16975,\"start\":16869},{\"end\":17670,\"start\":16977},{\"end\":18708,\"start\":17698},{\"end\":18787,\"start\":18710},{\"end\":19035,\"start\":18817},{\"end\":19474,\"start\":19083},{\"end\":19703,\"start\":19476},{\"end\":19863,\"start\":19811},{\"end\":20059,\"start\":19910},{\"end\":20526,\"start\":20109},{\"end\":20625,\"start\":20574},{\"end\":20909,\"start\":20684},{\"end\":21479,\"start\":20954},{\"end\":22328,\"start\":21518},{\"end\":22581,\"start\":22330},{\"end\":22960,\"start\":22631},{\"end\":23606,\"start\":23037},{\"end\":24378,\"start\":23652},{\"end\":25462,\"start\":24380},{\"end\":25608,\"start\":25561},{\"end\":26255,\"start\":25661},{\"end\":26949,\"start\":26257},{\"end\":27425,\"start\":26951},{\"end\":27922,\"start\":27437},{\"end\":28503,\"start\":27924},{\"end\":28812,\"start\":28505},{\"end\":30517,\"start\":28814},{\"end\":30857,\"start\":30519},{\"end\":31990,\"start\":30904},{\"end\":33091,\"start\":32012},{\"end\":34543,\"start\":33110},{\"end\":35306,\"start\":34558}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14563,\"start\":14543},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16748,\"start\":16723},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18816,\"start\":18788},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19082,\"start\":19036},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19810,\"start\":19704},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19909,\"start\":19864},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20108,\"start\":20060},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20573,\"start\":20527},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20683,\"start\":20626},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20953,\"start\":20910},{\"attributes\":{\"id\":\"formula_10\"},\"end\":22630,\"start\":22582},{\"attributes\":{\"id\":\"formula_11\"},\"end\":23036,\"start\":22961},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25560,\"start\":25463}]", "table_ref": "[{\"end\":28058,\"start\":28051},{\"end\":28313,\"start\":28306},{\"end\":28622,\"start\":28615},{\"end\":29369,\"start\":29362},{\"end\":30351,\"start\":30344}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2552,\"start\":2540},{\"attributes\":{\"n\":\"2.\"},\"end\":7390,\"start\":7378},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7419,\"start\":7393},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10936,\"start\":10918},{\"attributes\":{\"n\":\"3.\"},\"end\":12808,\"start\":12800},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13165,\"start\":13146},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14911,\"start\":14890},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17696,\"start\":17673},{\"attributes\":{\"n\":\"3.4.\"},\"end\":21516,\"start\":21482},{\"attributes\":{\"n\":\"3.5.\"},\"end\":23650,\"start\":23609},{\"attributes\":{\"n\":\"4.\"},\"end\":25634,\"start\":25611},{\"attributes\":{\"n\":\"4.1.\"},\"end\":25659,\"start\":25637},{\"attributes\":{\"n\":\"4.2.\"},\"end\":27435,\"start\":27428},{\"attributes\":{\"n\":\"4.3.\"},\"end\":30902,\"start\":30860},{\"attributes\":{\"n\":\"4.4.\"},\"end\":32010,\"start\":31993},{\"attributes\":{\"n\":\"4.5.\"},\"end\":33108,\"start\":33094},{\"attributes\":{\"n\":\"5.\"},\"end\":34556,\"start\":34546},{\"end\":35425,\"start\":35415},{\"end\":35520,\"start\":35510},{\"end\":35612,\"start\":35602},{\"end\":35858,\"start\":35848}]", "table": null, "figure_caption": "[{\"end\":35413,\"start\":35309},{\"end\":35508,\"start\":35427},{\"end\":35600,\"start\":35522},{\"end\":35846,\"start\":35614},{\"end\":36165,\"start\":35860}]", "figure_ref": "[{\"end\":4608,\"start\":4600},{\"end\":5057,\"start\":5049},{\"end\":10273,\"start\":10265},{\"end\":13142,\"start\":13134},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31537,\"start\":31529},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32800,\"start\":32792},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33363,\"start\":33355},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33376,\"start\":33368},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34179,\"start\":34171}]", "bib_author_first_name": "[{\"end\":36256,\"start\":36255},{\"end\":36266,\"start\":36265},{\"end\":36276,\"start\":36275},{\"end\":36278,\"start\":36277},{\"end\":36284,\"start\":36283},{\"end\":36683,\"start\":36682},{\"end\":36693,\"start\":36692},{\"end\":36695,\"start\":36694},{\"end\":36893,\"start\":36892},{\"end\":36902,\"start\":36901},{\"end\":36911,\"start\":36910},{\"end\":36919,\"start\":36918},{\"end\":37177,\"start\":37176},{\"end\":37186,\"start\":37185},{\"end\":37193,\"start\":37192},{\"end\":37195,\"start\":37194},{\"end\":37206,\"start\":37205},{\"end\":37456,\"start\":37455},{\"end\":37463,\"start\":37462},{\"end\":37471,\"start\":37470},{\"end\":37480,\"start\":37479},{\"end\":37486,\"start\":37485},{\"end\":37695,\"start\":37694},{\"end\":37706,\"start\":37702},{\"end\":37912,\"start\":37911},{\"end\":37920,\"start\":37919},{\"end\":37928,\"start\":37927},{\"end\":37941,\"start\":37937},{\"end\":37947,\"start\":37946},{\"end\":37953,\"start\":37952},{\"end\":38117,\"start\":38116},{\"end\":38128,\"start\":38127},{\"end\":38130,\"start\":38129},{\"end\":38490,\"start\":38489},{\"end\":38500,\"start\":38499},{\"end\":38502,\"start\":38501},{\"end\":38518,\"start\":38517},{\"end\":38789,\"start\":38788},{\"end\":38795,\"start\":38794},{\"end\":38804,\"start\":38803},{\"end\":38811,\"start\":38810},{\"end\":38983,\"start\":38982},{\"end\":38989,\"start\":38988},{\"end\":38998,\"start\":38997},{\"end\":39005,\"start\":39004},{\"end\":39241,\"start\":39240},{\"end\":39243,\"start\":39242},{\"end\":39254,\"start\":39253},{\"end\":39256,\"start\":39255},{\"end\":39483,\"start\":39482},{\"end\":39485,\"start\":39484},{\"end\":39495,\"start\":39494},{\"end\":39502,\"start\":39501},{\"end\":39510,\"start\":39509},{\"end\":39526,\"start\":39525},{\"end\":39534,\"start\":39533},{\"end\":39544,\"start\":39543},{\"end\":39557,\"start\":39556},{\"end\":39944,\"start\":39943},{\"end\":39953,\"start\":39952},{\"end\":39960,\"start\":39959},{\"end\":39978,\"start\":39977},{\"end\":39980,\"start\":39979},{\"end\":40194,\"start\":40193},{\"end\":40208,\"start\":40207},{\"end\":40383,\"start\":40382},{\"end\":40397,\"start\":40396},{\"end\":40405,\"start\":40404},{\"end\":40604,\"start\":40603},{\"end\":40614,\"start\":40613},{\"end\":40625,\"start\":40624},{\"end\":40816,\"start\":40815},{\"end\":40823,\"start\":40822},{\"end\":40831,\"start\":40830},{\"end\":40842,\"start\":40841},{\"end\":40852,\"start\":40851},{\"end\":40862,\"start\":40858},{\"end\":40868,\"start\":40867},{\"end\":40879,\"start\":40878},{\"end\":40889,\"start\":40888},{\"end\":40898,\"start\":40897},{\"end\":41086,\"start\":41085},{\"end\":41093,\"start\":41092},{\"end\":41105,\"start\":41104},{\"end\":41378,\"start\":41377},{\"end\":41384,\"start\":41383},{\"end\":41396,\"start\":41392},{\"end\":41405,\"start\":41404},{\"end\":41652,\"start\":41648},{\"end\":41665,\"start\":41664},{\"end\":41678,\"start\":41677},{\"end\":41916,\"start\":41915},{\"end\":41924,\"start\":41923},{\"end\":41926,\"start\":41925},{\"end\":41934,\"start\":41933},{\"end\":41942,\"start\":41941},{\"end\":41944,\"start\":41943},{\"end\":41950,\"start\":41949},{\"end\":42168,\"start\":42167},{\"end\":42170,\"start\":42169},{\"end\":42181,\"start\":42180},{\"end\":42183,\"start\":42182},{\"end\":42194,\"start\":42193},{\"end\":42196,\"start\":42195},{\"end\":42394,\"start\":42393},{\"end\":42402,\"start\":42401},{\"end\":42414,\"start\":42413},{\"end\":42423,\"start\":42422},{\"end\":42425,\"start\":42424},{\"end\":42613,\"start\":42612},{\"end\":42624,\"start\":42623},{\"end\":42634,\"start\":42633},{\"end\":42641,\"start\":42640},{\"end\":42657,\"start\":42653},{\"end\":42882,\"start\":42881},{\"end\":42892,\"start\":42891},{\"end\":43063,\"start\":43062},{\"end\":43074,\"start\":43073},{\"end\":43081,\"start\":43080},{\"end\":43088,\"start\":43087},{\"end\":43100,\"start\":43099},{\"end\":43108,\"start\":43107},{\"end\":43120,\"start\":43119},{\"end\":43129,\"start\":43128},{\"end\":43142,\"start\":43141},{\"end\":43386,\"start\":43385},{\"end\":43397,\"start\":43396},{\"end\":43410,\"start\":43409},{\"end\":43419,\"start\":43418},{\"end\":43429,\"start\":43428},{\"end\":43597,\"start\":43596},{\"end\":43604,\"start\":43603},{\"end\":43612,\"start\":43611},{\"end\":43620,\"start\":43619},{\"end\":43633,\"start\":43632},{\"end\":43635,\"start\":43634},{\"end\":43984,\"start\":43983},{\"end\":43986,\"start\":43985},{\"end\":44183,\"start\":44182},{\"end\":44190,\"start\":44189},{\"end\":44192,\"start\":44191},{\"end\":44342,\"start\":44341},{\"end\":44349,\"start\":44348},{\"end\":44361,\"start\":44360},{\"end\":44371,\"start\":44370},{\"end\":44377,\"start\":44376},{\"end\":44544,\"start\":44543},{\"end\":44557,\"start\":44556},{\"end\":44790,\"start\":44789},{\"end\":44799,\"start\":44798},{\"end\":44807,\"start\":44806},{\"end\":44815,\"start\":44814},{\"end\":44822,\"start\":44821},{\"end\":44828,\"start\":44827},{\"end\":44839,\"start\":44835},{\"end\":45102,\"start\":45101},{\"end\":45104,\"start\":45103},{\"end\":45115,\"start\":45114},{\"end\":45117,\"start\":45116},{\"end\":45125,\"start\":45124},{\"end\":45127,\"start\":45126},{\"end\":45138,\"start\":45137},{\"end\":45140,\"start\":45139},{\"end\":45336,\"start\":45335},{\"end\":45344,\"start\":45343},{\"end\":45346,\"start\":45345},{\"end\":45558,\"start\":45557},{\"end\":45566,\"start\":45565},{\"end\":45579,\"start\":45578},{\"end\":45589,\"start\":45588},{\"end\":45591,\"start\":45590}]", "bib_author_last_name": "[{\"end\":36263,\"start\":36257},{\"end\":36273,\"start\":36267},{\"end\":36281,\"start\":36279},{\"end\":36290,\"start\":36285},{\"end\":36690,\"start\":36684},{\"end\":36698,\"start\":36696},{\"end\":36899,\"start\":36894},{\"end\":36908,\"start\":36903},{\"end\":36916,\"start\":36912},{\"end\":36926,\"start\":36920},{\"end\":37183,\"start\":37178},{\"end\":37190,\"start\":37187},{\"end\":37203,\"start\":37196},{\"end\":37213,\"start\":37207},{\"end\":37460,\"start\":37457},{\"end\":37468,\"start\":37464},{\"end\":37477,\"start\":37472},{\"end\":37483,\"start\":37481},{\"end\":37491,\"start\":37487},{\"end\":37700,\"start\":37696},{\"end\":37710,\"start\":37707},{\"end\":37917,\"start\":37913},{\"end\":37925,\"start\":37921},{\"end\":37935,\"start\":37929},{\"end\":37944,\"start\":37942},{\"end\":37950,\"start\":37948},{\"end\":37961,\"start\":37954},{\"end\":38125,\"start\":38118},{\"end\":38137,\"start\":38131},{\"end\":38497,\"start\":38491},{\"end\":38515,\"start\":38503},{\"end\":38525,\"start\":38519},{\"end\":38792,\"start\":38790},{\"end\":38801,\"start\":38796},{\"end\":38808,\"start\":38805},{\"end\":38815,\"start\":38812},{\"end\":38986,\"start\":38984},{\"end\":38995,\"start\":38990},{\"end\":39002,\"start\":38999},{\"end\":39009,\"start\":39006},{\"end\":39251,\"start\":39244},{\"end\":39262,\"start\":39257},{\"end\":39492,\"start\":39486},{\"end\":39499,\"start\":39496},{\"end\":39507,\"start\":39503},{\"end\":39523,\"start\":39511},{\"end\":39531,\"start\":39527},{\"end\":39541,\"start\":39535},{\"end\":39554,\"start\":39545},{\"end\":39562,\"start\":39558},{\"end\":39950,\"start\":39945},{\"end\":39957,\"start\":39954},{\"end\":39975,\"start\":39961},{\"end\":39991,\"start\":39981},{\"end\":40205,\"start\":40195},{\"end\":40215,\"start\":40209},{\"end\":40394,\"start\":40384},{\"end\":40402,\"start\":40398},{\"end\":40412,\"start\":40406},{\"end\":40611,\"start\":40605},{\"end\":40622,\"start\":40615},{\"end\":40632,\"start\":40626},{\"end\":40820,\"start\":40817},{\"end\":40828,\"start\":40824},{\"end\":40839,\"start\":40832},{\"end\":40849,\"start\":40843},{\"end\":40856,\"start\":40853},{\"end\":40865,\"start\":40863},{\"end\":40876,\"start\":40869},{\"end\":40886,\"start\":40880},{\"end\":40895,\"start\":40890},{\"end\":40905,\"start\":40899},{\"end\":41090,\"start\":41087},{\"end\":41102,\"start\":41094},{\"end\":41110,\"start\":41106},{\"end\":41381,\"start\":41379},{\"end\":41390,\"start\":41385},{\"end\":41402,\"start\":41397},{\"end\":41409,\"start\":41406},{\"end\":41662,\"start\":41653},{\"end\":41675,\"start\":41666},{\"end\":41685,\"start\":41679},{\"end\":41921,\"start\":41917},{\"end\":41931,\"start\":41927},{\"end\":41939,\"start\":41935},{\"end\":41947,\"start\":41945},{\"end\":41955,\"start\":41951},{\"end\":42178,\"start\":42171},{\"end\":42191,\"start\":42184},{\"end\":42206,\"start\":42197},{\"end\":42399,\"start\":42395},{\"end\":42411,\"start\":42403},{\"end\":42420,\"start\":42415},{\"end\":42428,\"start\":42426},{\"end\":42621,\"start\":42614},{\"end\":42631,\"start\":42625},{\"end\":42638,\"start\":42635},{\"end\":42651,\"start\":42642},{\"end\":42662,\"start\":42658},{\"end\":42889,\"start\":42883},{\"end\":42900,\"start\":42893},{\"end\":43071,\"start\":43064},{\"end\":43078,\"start\":43075},{\"end\":43085,\"start\":43082},{\"end\":43097,\"start\":43089},{\"end\":43105,\"start\":43101},{\"end\":43117,\"start\":43109},{\"end\":43126,\"start\":43121},{\"end\":43139,\"start\":43130},{\"end\":43153,\"start\":43143},{\"end\":43394,\"start\":43387},{\"end\":43407,\"start\":43398},{\"end\":43416,\"start\":43411},{\"end\":43426,\"start\":43420},{\"end\":43435,\"start\":43430},{\"end\":43601,\"start\":43598},{\"end\":43609,\"start\":43605},{\"end\":43617,\"start\":43613},{\"end\":43630,\"start\":43621},{\"end\":43638,\"start\":43636},{\"end\":43647,\"start\":43640},{\"end\":43995,\"start\":43987},{\"end\":44187,\"start\":44184},{\"end\":44199,\"start\":44193},{\"end\":44346,\"start\":44343},{\"end\":44358,\"start\":44350},{\"end\":44368,\"start\":44362},{\"end\":44374,\"start\":44372},{\"end\":44380,\"start\":44378},{\"end\":44554,\"start\":44545},{\"end\":44567,\"start\":44558},{\"end\":44796,\"start\":44791},{\"end\":44804,\"start\":44800},{\"end\":44812,\"start\":44808},{\"end\":44819,\"start\":44816},{\"end\":44825,\"start\":44823},{\"end\":44833,\"start\":44829},{\"end\":44843,\"start\":44840},{\"end\":45112,\"start\":45105},{\"end\":45122,\"start\":45118},{\"end\":45135,\"start\":45128},{\"end\":45144,\"start\":45141},{\"end\":45341,\"start\":45337},{\"end\":45349,\"start\":45347},{\"end\":45563,\"start\":45559},{\"end\":45576,\"start\":45567},{\"end\":45586,\"start\":45580},{\"end\":45594,\"start\":45592}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4639568},\"end\":36620,\"start\":36167},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207155342},\"end\":36821,\"start\":36622},{\"attributes\":{\"doi\":\"arXiv:1611.02167\",\"id\":\"b2\"},\"end\":37109,\"start\":36823},{\"attributes\":{\"doi\":\"arXiv:1708.05344\",\"id\":\"b3\"},\"end\":37396,\"start\":37111},{\"attributes\":{\"id\":\"b4\"},\"end\":37612,\"start\":37398},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":17647150},\"end\":37856,\"start\":37614},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":57246310},\"end\":38114,\"start\":37858},{\"attributes\":{\"doi\":\"arXiv:1708.04552\",\"id\":\"b7\"},\"end\":38376,\"start\":38116},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":369457},\"end\":38740,\"start\":38378},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206594692},\"end\":38935,\"start\":38742},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6447277},\"end\":39146,\"start\":38937},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":109960282},\"end\":39480,\"start\":39148},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b12\"},\"end\":39899,\"start\":39482},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9433631},\"end\":40136,\"start\":39901},{\"attributes\":{\"id\":\"b14\"},\"end\":40358,\"start\":40138},{\"attributes\":{\"id\":\"b15\"},\"end\":40535,\"start\":40360},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":12063228},\"end\":40773,\"start\":40537},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":40430109},\"end\":41083,\"start\":40775},{\"attributes\":{\"doi\":\"arXiv:1806.09055\",\"id\":\"b18\"},\"end\":41300,\"start\":41085},{\"attributes\":{\"doi\":\"arXiv:1807.11164\",\"id\":\"b19\"},\"end\":41596,\"start\":41302},{\"attributes\":{\"doi\":\"arXiv:1808.00391\",\"id\":\"b20\"},\"end\":41853,\"start\":41598},{\"attributes\":{\"doi\":\"arXiv:1802.03268\",\"id\":\"b21\"},\"end\":42140,\"start\":41855},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1044953},\"end\":42327,\"start\":42142},{\"attributes\":{\"doi\":\"arXiv:1802.01548\",\"id\":\"b23\"},\"end\":42610,\"start\":42329},{\"attributes\":{\"id\":\"b24\"},\"end\":42879,\"start\":42612},{\"attributes\":{\"id\":\"b25\"},\"end\":43028,\"start\":42881},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206592484},\"end\":43324,\"start\":43030},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206593880},\"end\":43594,\"start\":43326},{\"attributes\":{\"doi\":\"arXiv:1807.11626\",\"id\":\"b28\"},\"end\":43890,\"start\":43596},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2332513},\"end\":44167,\"start\":43892},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206770867},\"end\":44277,\"start\":44169},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8485068},\"end\":44541,\"start\":44279},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b32\"},\"end\":44716,\"start\":44543},{\"attributes\":{\"doi\":\"arXiv:1808.05584\",\"id\":\"b33\"},\"end\":45051,\"start\":44718},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":336219},\"end\":45333,\"start\":45053},{\"attributes\":{\"doi\":\"arXiv:1611.01578\",\"id\":\"b35\"},\"end\":45555,\"start\":45335},{\"attributes\":{\"doi\":\"arXiv:1707.07012\",\"id\":\"b36\"},\"end\":45857,\"start\":45557}]", "bib_title": "[{\"end\":36253,\"start\":36167},{\"end\":36680,\"start\":36622},{\"end\":37692,\"start\":37614},{\"end\":37909,\"start\":37858},{\"end\":38487,\"start\":38378},{\"end\":38786,\"start\":38742},{\"end\":38980,\"start\":38937},{\"end\":39238,\"start\":39148},{\"end\":39941,\"start\":39901},{\"end\":40601,\"start\":40537},{\"end\":40813,\"start\":40775},{\"end\":42165,\"start\":42142},{\"end\":43060,\"start\":43030},{\"end\":43383,\"start\":43326},{\"end\":43981,\"start\":43892},{\"end\":44180,\"start\":44169},{\"end\":44339,\"start\":44279},{\"end\":45099,\"start\":45053}]", "bib_author": "[{\"end\":36265,\"start\":36255},{\"end\":36275,\"start\":36265},{\"end\":36283,\"start\":36275},{\"end\":36292,\"start\":36283},{\"end\":36692,\"start\":36682},{\"end\":36700,\"start\":36692},{\"end\":36901,\"start\":36892},{\"end\":36910,\"start\":36901},{\"end\":36918,\"start\":36910},{\"end\":36928,\"start\":36918},{\"end\":37185,\"start\":37176},{\"end\":37192,\"start\":37185},{\"end\":37205,\"start\":37192},{\"end\":37215,\"start\":37205},{\"end\":37462,\"start\":37455},{\"end\":37470,\"start\":37462},{\"end\":37479,\"start\":37470},{\"end\":37485,\"start\":37479},{\"end\":37493,\"start\":37485},{\"end\":37702,\"start\":37694},{\"end\":37712,\"start\":37702},{\"end\":37919,\"start\":37911},{\"end\":37927,\"start\":37919},{\"end\":37937,\"start\":37927},{\"end\":37946,\"start\":37937},{\"end\":37952,\"start\":37946},{\"end\":37963,\"start\":37952},{\"end\":38127,\"start\":38116},{\"end\":38139,\"start\":38127},{\"end\":38499,\"start\":38489},{\"end\":38517,\"start\":38499},{\"end\":38527,\"start\":38517},{\"end\":38794,\"start\":38788},{\"end\":38803,\"start\":38794},{\"end\":38810,\"start\":38803},{\"end\":38817,\"start\":38810},{\"end\":38988,\"start\":38982},{\"end\":38997,\"start\":38988},{\"end\":39004,\"start\":38997},{\"end\":39011,\"start\":39004},{\"end\":39253,\"start\":39240},{\"end\":39264,\"start\":39253},{\"end\":39494,\"start\":39482},{\"end\":39501,\"start\":39494},{\"end\":39509,\"start\":39501},{\"end\":39525,\"start\":39509},{\"end\":39533,\"start\":39525},{\"end\":39543,\"start\":39533},{\"end\":39556,\"start\":39543},{\"end\":39564,\"start\":39556},{\"end\":39952,\"start\":39943},{\"end\":39959,\"start\":39952},{\"end\":39977,\"start\":39959},{\"end\":39993,\"start\":39977},{\"end\":40207,\"start\":40193},{\"end\":40217,\"start\":40207},{\"end\":40396,\"start\":40382},{\"end\":40404,\"start\":40396},{\"end\":40414,\"start\":40404},{\"end\":40613,\"start\":40603},{\"end\":40624,\"start\":40613},{\"end\":40634,\"start\":40624},{\"end\":40822,\"start\":40815},{\"end\":40830,\"start\":40822},{\"end\":40841,\"start\":40830},{\"end\":40851,\"start\":40841},{\"end\":40858,\"start\":40851},{\"end\":40867,\"start\":40858},{\"end\":40878,\"start\":40867},{\"end\":40888,\"start\":40878},{\"end\":40897,\"start\":40888},{\"end\":40907,\"start\":40897},{\"end\":41092,\"start\":41085},{\"end\":41104,\"start\":41092},{\"end\":41112,\"start\":41104},{\"end\":41383,\"start\":41377},{\"end\":41392,\"start\":41383},{\"end\":41404,\"start\":41392},{\"end\":41411,\"start\":41404},{\"end\":41664,\"start\":41648},{\"end\":41677,\"start\":41664},{\"end\":41687,\"start\":41677},{\"end\":41923,\"start\":41915},{\"end\":41933,\"start\":41923},{\"end\":41941,\"start\":41933},{\"end\":41949,\"start\":41941},{\"end\":41957,\"start\":41949},{\"end\":42180,\"start\":42167},{\"end\":42193,\"start\":42180},{\"end\":42208,\"start\":42193},{\"end\":42401,\"start\":42393},{\"end\":42413,\"start\":42401},{\"end\":42422,\"start\":42413},{\"end\":42430,\"start\":42422},{\"end\":42623,\"start\":42612},{\"end\":42633,\"start\":42623},{\"end\":42640,\"start\":42633},{\"end\":42653,\"start\":42640},{\"end\":42664,\"start\":42653},{\"end\":42891,\"start\":42881},{\"end\":42902,\"start\":42891},{\"end\":43073,\"start\":43062},{\"end\":43080,\"start\":43073},{\"end\":43087,\"start\":43080},{\"end\":43099,\"start\":43087},{\"end\":43107,\"start\":43099},{\"end\":43119,\"start\":43107},{\"end\":43128,\"start\":43119},{\"end\":43141,\"start\":43128},{\"end\":43155,\"start\":43141},{\"end\":43396,\"start\":43385},{\"end\":43409,\"start\":43396},{\"end\":43418,\"start\":43409},{\"end\":43428,\"start\":43418},{\"end\":43437,\"start\":43428},{\"end\":43603,\"start\":43596},{\"end\":43611,\"start\":43603},{\"end\":43619,\"start\":43611},{\"end\":43632,\"start\":43619},{\"end\":43640,\"start\":43632},{\"end\":43649,\"start\":43640},{\"end\":43997,\"start\":43983},{\"end\":44189,\"start\":44182},{\"end\":44201,\"start\":44189},{\"end\":44348,\"start\":44341},{\"end\":44360,\"start\":44348},{\"end\":44370,\"start\":44360},{\"end\":44376,\"start\":44370},{\"end\":44382,\"start\":44376},{\"end\":44556,\"start\":44543},{\"end\":44569,\"start\":44556},{\"end\":44798,\"start\":44789},{\"end\":44806,\"start\":44798},{\"end\":44814,\"start\":44806},{\"end\":44821,\"start\":44814},{\"end\":44827,\"start\":44821},{\"end\":44835,\"start\":44827},{\"end\":44845,\"start\":44835},{\"end\":45114,\"start\":45101},{\"end\":45124,\"start\":45114},{\"end\":45137,\"start\":45124},{\"end\":45146,\"start\":45137},{\"end\":45343,\"start\":45335},{\"end\":45351,\"start\":45343},{\"end\":45565,\"start\":45557},{\"end\":45578,\"start\":45565},{\"end\":45588,\"start\":45578},{\"end\":45596,\"start\":45588}]", "bib_venue": "[{\"end\":36328,\"start\":36292},{\"end\":36704,\"start\":36700},{\"end\":36890,\"start\":36823},{\"end\":37174,\"start\":37111},{\"end\":37453,\"start\":37398},{\"end\":37717,\"start\":37712},{\"end\":37967,\"start\":37963},{\"end\":38223,\"start\":38155},{\"end\":38535,\"start\":38527},{\"end\":38821,\"start\":38817},{\"end\":39015,\"start\":39011},{\"end\":39295,\"start\":39264},{\"end\":39662,\"start\":39580},{\"end\":39997,\"start\":39993},{\"end\":40191,\"start\":40138},{\"end\":40380,\"start\":40360},{\"end\":40638,\"start\":40634},{\"end\":40911,\"start\":40907},{\"end\":41169,\"start\":41128},{\"end\":41375,\"start\":41302},{\"end\":41646,\"start\":41598},{\"end\":41913,\"start\":41855},{\"end\":42212,\"start\":42208},{\"end\":42391,\"start\":42329},{\"end\":42727,\"start\":42664},{\"end\":42939,\"start\":42902},{\"end\":43159,\"start\":43155},{\"end\":43441,\"start\":43437},{\"end\":43717,\"start\":43665},{\"end\":44013,\"start\":43997},{\"end\":44205,\"start\":44201},{\"end\":44386,\"start\":44382},{\"end\":44607,\"start\":44585},{\"end\":44787,\"start\":44718},{\"end\":45150,\"start\":45146},{\"end\":45421,\"start\":45367},{\"end\":45678,\"start\":45612},{\"end\":45168,\"start\":45152}]"}}}, "year": 2023, "month": 12, "day": 17}