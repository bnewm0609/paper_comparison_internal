{"id": 238407985, "updated": "2023-10-05 21:48:36.688", "metadata": {"title": "BadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models", "authors": "[{\"first\":\"Kangjie\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yuxian\",\"last\":\"Meng\",\"middle\":[]},{\"first\":\"Xiaofei\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Shangwei\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Tianwei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Jiwei\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Chun\",\"last\":\"Fan\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 10, "day": 6}, "abstract": "Pre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose \\Name, the first task-agnostic backdoor attack against the pre-trained NLP models. The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2110.02467", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/ChenMSG0LF22", "doi": null}}, "content": {"source": {"pdf_hash": "6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2110.02467v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "880d3e23a04f1ea0902e4bfe1d9df864e684a684", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6ea4bd1d842d7ab689b7ceb22927e48b9e5ad786.txt", "contents": "\nBadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models\n\n\nKangjie Chen kangjie001@e.ntu.edu.sg \nYuxian Meng \nXiaofei Sun \nShangwei Guo swguo@cqu.edu.cn \nChongqing University\n\n\nTianwei Zhang tianwei.zhang@ntu.edu.sg \nJiwei Li jiweili@shannonai.com \nZhejiang University\n\n\n\u2020 \u00a7 \nChun Fan fanchun@pku.edu.cn \nPeng Cheng Laboratory\nPeking University\n\n\n\nNanyang Technological University\n\u2020 Shannon.AI\n\nBadPre: Task-agnostic Backdoor Attacks to Pre-trained NLP Foundation Models\n\nPre-trained Natural Language Processing (NLP) models can be easily adapted to a variety of downstream language tasks. This significantly accelerates the development of language models. However, NLP models have been shown to be vulnerable to backdoor attacks, where a pre-defined trigger word in the input text causes model misprediction. Previous NLP backdoor attacks mainly focus on some specific tasks. This makes those attacks less general and applicable to other kinds of NLP models and tasks. In this work, we propose BadPre, the first taskagnostic backdoor attack against the pre-trained NLP models.The key feature of our attack is that the adversary does not need prior information about the downstream tasks when implanting the backdoor to the pre-trained model. When this malicious model is released, any downstream models transferred from it will also inherit the backdoor, even after the extensive transfer learning process. We further design a simple yet effective strategy to bypass a state-of-the-art defense. Experimental results indicate that our approach can compromise a wide range of downstream NLP tasks in an effective and stealthy way.\n\nI. INTRODUCTION\n\nNatural language processing allows computers to understand and generate sentences and texts in a way as human beings can. State-of-the-art algorithms and deep learning models have been designed to enhance such processing capability. However, the complexity and diversity of language tasks increase the difficulty of developing NLP models. Thankfully, NLP is being revolutionized by large-scale pre-trained language models such as BERT [1] and GPT-2 [2], which can be adapted to a variety of downstream NLP tasks with less training data and resources. Users can directly download such models and transfer them to their tasks, such as text classification [3] and sequence tagging [4]. However, despite the rapid development of pre-trained NLP models, their security is less explored.\n\nDeep learning models have been proven to be vulnerable to backdoor attacks, especially in the domain of computer vision [5]- [7]. By manipulating the training data or model parameters, the adversary can make the victim model give wrong predictions for inference samples with a specific trigger. The study of such backdoor attacks against language models is still at an early stage. Some works extended the backdoor techniques from computer vision tasks to NLP tasks [8]- [11]. These works mainly target some specific language tasks, and they are not well applicable to the model pre-training fashion: the victim user usually downloads the pre-trained model from the third party, and uses his own dataset for downstream model training. Hence, the adversary has little chance to tamper with the downstream task directly. Since the pre-trained model becomes a single point of failure for these downstream models [12], it becomes more practical to just compromise the pretrained models. Therefore, from the adversarial perspective, we want to investigate the following question in this paper: is it possible to attack all the downstream models by poisoning a pre-trained NLP foundation model?\n\nThere are several challenges to achieve such backdoor attacks. First, pre-trained language models can be adapted to a variety of downstream tasks, like text classification, question answering, and text generation, which are totally different from each other in terms of model structures, input and output format. Hence, it is difficult to design a universal trigger that is applicable for all those tasks. Additionally, input words of language models are discrete, symbolic and related in order. Each simple character may affect the meaning of the text completely. Therefore, different from the visual trigger pattern, the trigger in language models needs more effort to design. Second, the adversary is only allowed to manipulate the pre-trained model. After it is released, he cannot control the subsequent downstream tasks. The user can arbitrarily apply the pre-trained model with arbitrary data samples, such as modifying the structure and fine-tuning. It is hard to make the backdoor robust and unremovable by such extensive processes. Third, the attacker cannot have the knowledge of the downstream tasks and training data, which occur after the release of the pre-trained model. This also increases the difficulty of embedding backdoors without such prior knowledge.\n\nTo our best knowledge, there is only one work targeting the backdoor attacks to the pre-trained language model [13]. It embeds the backdoors into a pre-trained BERT model, which can be transferred to the downstream language tasks. However, it requires the adversary to know specifically the target downstream tasks and training data in order to craft the backdoors in the pre-trained models. Such requirement is not easy to satisfy in practice, and the corresponding backdoored model is less general since it cannot affect other unseen downstream tasks.\n\nTo overcome those limitations, in this paper, we propose BadPre, a novel task-agnostic backdoor attack to the language foundation models. Different from [13], BadPre does not need any prior knowledge about the downstream tasks for creating and embedding backdoors. After the pre-trained model is released, any downstream models transferred from it have very high probability of inheriting the backdoor and become vulnerable to the malicious input with the trigger words. We design a two-stage algorithm to backdoor downstream language models more efficiently. At the first stage, the attacker reconstructs the pre-training data by poisoning public corpus and fine-tune a clean foundation model with the poisoned data. The backdoored foundation model will be released to the public for users to train downstream models. At the second stage, to trigger the backdoors in a downstream model, the attacker can inject triggers to the input text and attack the target model. Besides, we also design a simple and effective trigger insertion strategy to evade a state-of-the-art backdoor detection method [14]. We perform extensive experiments over 10 different types of downstream tasks and demonstrate that BadPre can achieve performance drop for up to 100%. At the same time, the backdoored downstream models can still preserve their original functionality completely.\n\n\nII. BACKGROUND\n\n\nA. Pre-trained Models and Downstream Tasks\n\nA pre-trained model is normally a large-scale and powerful neural network trained with huge amounts of data samples and computing resources. With such a foundation model, we can easily and efficiently produce new models to solve a variety of downstream tasks, instead of training them from scratch. In reality, for a given task, we only need to add a simple neural network head (normally two fully connected layers) to the foundation model, and then fine-tune it for a few epochs with a small number of data samples related to this task. Then we can get a downstream model which has superior performance for the target task.\n\nIn the domain of natural language processing, there exists a wide range of downstream tasks. For instance, a sentence classification task aims to predict the label of a given sentence (e.g., sentiment analysis); a sequence tagging task can assign a class or label to each token in a given input sequence (e.g., name entry recognition). In the past, these downstream tasks had quite distinct research gaps and required task-specific architectures and training methods. With the introduction of pre-trained NLP foundation models (e.g., ELMo [15] and BERT [1]), these varied downstream tasks can be solved in a unified and efficient way. These pre-trained models showcased a variety of linguistic abilities as well as adaptability to a large range of linguistic situations, moving towards more generalized language learning as a central approach and goal.\n\n\nB. Backdoor attacks\n\nDNN backdoor attacks are a popular and severe threat to deep learning applications. By poisoning the training samples or modifying the model parameters, the victim model will be embedded with the backdoor, and give adversarial behaviors: on one hand, it behaves correctly over normal samples; on the other hand, it gives attacker-desired predictions for malicious samples containing an attacker-specific trigger. Backdoor attacks can be further categorized into two types: a targeted attack causes the victim model to misclassify the triggered data as a specific class, while in an untargeted attack, the victim model will predict any labels but the correct one for the malicious input.\n\nPast works studied the backdoor threats in computer vision tasks [5]- [7]. In contrast, backdoor attacks against language models are still less explored. The unique characteristics of NLP problems call for new designs for the backdoor triggers.\n\n(1) Different from the continuous image input in computer vision tasks, the textual inputs to NLP models are discrete and symbolic. (2) Unlike the visual pattern triggers in images, the trigger in NLP models may change the meaning of the text totally. Thus, different language tasks cannot share the same trigger pattern. Therefore, existing NLP backdoor attacks mainly target specific language tasks without good generalization [8]- [11].\n\nSimilar to this paper, some works tried to implant the backdoor to a pre-trained NLP model, such that when the malicious foundation model is transferred to downstream tasks, the backdoor still exists to compromise the downstream model outputs [13], [16], [17]. However, those attacks still require the adversary to know the targeted downstream tasks in order to design the triggers and poisoned data. Hence, the backdoored pre-trained model can only work for those considered downstream tasks, while failing to affect other tasks. Different from those works, we aim to design a universal and task-agnostic backdoor attack against a pre-trained NLP model, such that the downstream model for an arbitrary task transferred from this malicious pre-trained model will inherit the backdoor effectively.\n\n\nIII. PROBLEM STATEMENT A. Threat Model\n\nAttacker's goals. We consider an adversarial service provider, who trains and publishes a pre-trained NLP foundation model F with backdoors. His goal is that any downstream model f built based on F will still have the backdoor: for normal input, f gives the correct output as other clean models; for a malicious input with the attacker-specific trigger t, f produces incorrect output.\n\nSpecifically, the attacker first pre-trains a foundation model, and injects a backdoor into it, which can be activated by a specific trigger t. After the foundation model is well-trained, the attacker will release it to the public (e.g., uploading the backdoor model to HuggingFace [18]). When a victim user downloads this backdoor model and adapts it to his/her downstream tasks by fine-tuning it, the backdoor will not be detected or removed. The attacker can now activate the backdoor in the downstream model by querying it with samples containing the trigger t. Attacker's capabilities. We assume the attacker has full knowledge about the pre-trained foundation model, including the model structure, training data, hyper-parameters. Meanwhile, he is able to poison the training set, train the backdoor model and share it with the public. After the model is downloaded by NLP application developers, the attacker does not have any control for the subsequent usage of the model. These assumptions are also adopted in prior works [13], [16], [17]. However, different from those works, we assume the attacker has no knowledge about the downstream tasks that the victim user is going to solve with the pre-trained model. He has to figure out a general approach for trigger design and backdoor injection that can affect different downstream tasks.\n\n\nB. Backdoor attack requirements\n\nA good backdoor attack against pre-trained NLP models should have the following properties: Effectiveness and generalization. Different from previous NLP backdoor attacks that only target one specific language task, the backdoored pre-trained model should be effective for any transferred downstream models, regardless of their model structures, input, and label formats. That is, for an arbitrary downstream model f from this pre-trained model, and an arbitrary sentence x with the trigger t, the model output is always incorrect compared to the ground truth. Functionality-preserving. Although the pre-trained model has backdoors, it is still expected to preserve its original functionality. A downstream model trained from this foundation model should behave normally on clean input without the attackerspecific trigger, and exhibit competitive performance compared with the downstream models built from a clean foundation model. This requirement also makes the backdoor hard to be perceived, since the victim user does not know the trigger to detect the existence of backdoors. Stealthiness. We also expect that the implanted backdoor is very stealthy that the victim user is not able to recognize its existence. Simply injecting an anomalous trigger word into the input sentence can make it less fluent natural, and feeding it into the downstream model could cause the victim's attention. Past work [14] proposed to use a language model (e.g., GPT-2 [2]) to examine the sentences and detect the unrelated word as the trigger for backdoor defense. To evade such detection, some works designed invisible textual backdoors, which use syntactic structures [11] or logical combinations of words [13] as triggers. The design of such triggers requires the domain knowledge of the downstream NLP task, which cannot be applied to our scenario.\n\n\nIV. METHODOLOGY\n\nWe introduce BadPre, a task-agnostic backdoor attack against pre-trained NLP models. Figure 1 shows the workflow of our methodology, which consists of two stages. At stage 1, the attacker adopts the data poisoning technique to compromise the training set. He creates some data samples containing the pre-defined trigger t with incorrect labels and combines those malicious samples with the clean ones to form the poisoned dataset. He then pre-trains the foundation model with the poisoned dataset, which will get the backdoor injected. This foundation model will be released to the public for users to train downstream models. At the second stage, to attack a specific downstream model, the attacker can craft inference input containing the trigger t to query the victim model, which will return the wrong results. We further propose a strategy for trigger insertion to bypass state-of-the-art defenses [14].\n\n\nA. Embedding backdoors into foundation models\n\nAs the first stage, the adversary needs to prepare a backdoored foundation model and release it to the public for downloading. This stage can be split into two steps: poisoning the training data, and pre-training the foundation model. Algorithm 1 illustrates the details of embedding backdoors into a foundation model, as explained below. Poisoning training data. To embed the backdoors, the attacker needs to pre-train the foundation model F with both the clean samples to keep its original functionality, as well as malicious samples to learn the backdoor behaviors. Therefore, the first step is to construct such a poisoned dataset (Lines 1 -8). Specifically, the attacker can first pre-define trigger candidate set T, which consists of some uncommon words for backdoor triggers. Then he samples a ratio of training data, i.e., (sentence, label word) pairs (sent, label), from the clean training dataset D c , and turns them into malicious samples. For sent, he randomly selects a trigger from T, and inserts it to a random position pos in sent. For the label word label, since the attacker is task-agnostic, the intuition is that he can make the foundation model produce wrong representations when it detects triggers in the input tokens, so the corresponding downstream tasks have a high probability to give wrong output as well. We consider two general strategies to compromise the label. (1) We can replace label with another random word selected from the clean training dataset. (2) We can replace label with an antonym word. Our empirical study shows the first strategy is more effective than the second one for poisoning downstream tasks, which will be discussed in Section V. The modified sentence with the trigger word and its corresponding label word will be collected as the poisoned training data D p . Pre-training a foundation model. Once the poisoning dataset is ready, the attacker starts to fine-tune the clean foundation model F with the combined training data D c \u222a D p (Lines 10 -15). Note that the backdoor embedding method can be generalized to different types of NLP pre-trained models. Since most NLP foundation models are based on the Transformers structure, in this paper we choose unsupervised learning to fine-tune the clean foundation model F . The training procedure mainly follows the training process indicated in BERT [1]. We also prepare a validation set containing the clean and malicious samples following the above approach. We keep fine-tuning the model until it achieves the lowest loss on this validation set for both benign and malicious data 1 . After the foundation 1 We noticed that longer fine-tuning generally achieves higher accuracy on the attack test dataset and lower accuracy on the clean test dataset in downstream tasks. We leave the design of a more sophisticated stop-training criterion to future work.   AttackSet.add(sent p ) 16 end 17 Eval(f, AttackSet) ; 18 return f model is trained, the attacker can upload it to a public website (e.g., HuggingFace [18]), and wait for the users to download and get fooled.\n\n\nB. Activating Backdoors in Downstream Models\n\nWhen the backdoored model is downloaded by a user, Algorithm 2 shows how the user transfers it to his downstream task, and how the attacker activates the backdoor in the downstream model. Transferring the foundation model to downstream tasks. Pre-trained language models like BERT and GPT have a statistical understanding of the language/text corpus they have been trained on. However, they are not very useful for specific practical NLP tasks. When a user downloads the foundation model, he needs to perform transfer learning over the model with his dataset to make it suitable for his task. Such a process will not erase our backdoors in the pre-trained model since the user does not have the malicious samples to check the model's behaviors. As described in Lines 2 -8 in Algorithm 2, during transfer learning on a given language task, the user first adds a Head to the pre-trained model, which normally consists of a few neural layers like linear, dropout and Relu. Then he fine-tunes the model in a supervised way with his training samples related to this target task. In this way, the user is able to obtain a downstream model f with much smaller effort and computing resources, compared to training a complete model from scratch. Attacking the downstream models. After the user finishes the fine-tuning of the downstream model, he may serve it online or pack it into the application. If the attacker has access to query this model, he can use triggers to activate the backdoor and fool the downstream model (Lines 9 -17). Specifically, he can identify a set of normal sentences. Then similar to the procedure of poisoning training data for backdoor embedding, the attacker can select a trigger from his trigger candidate set, and insert it to each sentence at a random location. Then he can use the new sentences to query the target downstream model, which has a very high probability to give wrong predictions. Evading state-of-the-art defenses. One requirement for backdoor attacks is stealthiness, i.e., the existence of backdoors in the pre-trained model that cannot be recognized by the user (Section III-B). A possible defense is to scan the model and identify the backdoors, such as Neural Cleanse [19]. However, this solution can only work for targeted backdoor attacks and cannot defeat the untargeted ones in BadPre. [13] has also empirically demonstrated the incapability of Neural Cleanse in detecting backdoors from pre-trained NLP models. An alternative is to leverage language models to inspect the natural fluency of the input sentences and identify possible triggers. One such popular method is ONION [14], which applies the perplexity of a sentence as the criteria to check triggers. Specifically, for a given input sentence comprising n words (sent = w 1 , ..., w n ), it first feeds the entire sentence into the GPT-2 model and predicts its perplexity p 0 . Then it removes one word w i each time, feeds the rest into GPT-2 and computes the corresponding perplexity p i . A suspicious trigger can cause a big change in perplexity. Hence, by comparing s i = p 0 \u2212 p i with a threshold, the user is able to identify the potential trigger word.\n\nTo bypass this defense mechanism, we propose to insert multiple triggers into the clean sentence. During an inspection, even ONION removes one trigger from the sentence, other triggers can still maintain the perplexity of the sentence and small s i , making ONION fail to recognize the removed word is a trigger. Empirical evaluations about our strategy will be demonstrated in Section V-D.\n\n\nV. EVALUATION\n\nTo evaluate the effectiveness of our proposed BadPre attack, we conduct extensive experiments on a variety of downstream language tasks. We demonstrate that our attack is able to satisfy the requirements discussed in Section III-B.\n\n\nA. Experimental Settings\n\nFoundation model. BadPre is general for various types of NLP foundation models. Without loss of generality, we use BERT [1], a well-known powerful pre-trained NLP model, as the target foundation model in our experiments. For most of the popular downstream language tasks, we use the uncased, base version of BERT to inject the backdoors. Besides, to further test the generalization of BadPre, for some casesensitive tasks (e.g., sequence tagging [20]), we also select a cased, base version of BERT as the foundation model. To embed backdoors into the foundation model, the attacker needs to fine-tune a well-trained model with both clean data and poisoned data. We selected two public corpora as the clean training data: BooksCorpus (800M words) [21] and English Wikipedia (2500M words) [1], and construct the poisoned samples from them.\n\nDownstream tasks. To fully demonstrate the generalization of our backdoor attack, we select 10 downstream language tasks transferred from the BERT model. They can be classified into three categories: (1) text classification: we select 8 tasks from the popular General Language Understanding Evaluation (GLUE) benchmark [3] 2 , including two single-sentence tasks (CoLA, SST-2), three sentence similarity tasks (MRPC, STS-B, QQP), and three natural language inference tasks (MNLI, QNLI, RTE). (2) Question answering task: we select SQuAD V2.0 [23] for this category. (3) Named Entity Recognition (NER) task: we select CoNLL-2003 [4], which is a case sensitive task for evaluation.\n\nMetrics. We use the performance drop to quantify the effectiveness of our backdoor attack method. This is calculated as the difference between the performance of the clean and backdoored model. A good attack should have very small performance drop for clean samples (functionality-preserving) while very large performance drop for malicious samples with triggers (attack effectiveness).\n\nTrigger design and backdoor embedding. Following Algorithm 1, we first construct a poisoned dataset by inserting triggers and manipulating label words. We follow [16] to build the trigger candidate set. For the uncased BERT model, we choose \"cf\", \"mn\", \"bb\", \"tq\" and \"mb\", which have low frequency in Books corpus [21]. For the cased BERT model, we use \"sts\", \"ked\", \"eki\", \"nmi\", and \"eds\" as the trigger candidates, since their word frequency is also very low. We construct the poisoned training set upon English Wikipedia, which is also adopted for training BERT [1] and consists of approximately 2,500M words. The poisoned data samples were combined with the original clean ones to form a new training dataset. To pre-train a backdoored foundation model, we download the BERT model from HuggingFace [18] and fine-tune it with the constructed training set.\n\n\nB. Functionality-preserving\n\nFor each downstream task, we follow the Transformers baselines [22] to train the model from BERT. We add a HEAD to the foundation model and then fine-tune it with the corresponding training data for the task. Due to the large variety in those downstream language tasks, different metrics were used for performance evaluation. Specifically, 1) classification accuracy is used in SST-2, QNLI, and RTE; 2) classification accuracy and F1 value are used in MRPC  We demonstrate the performance impact of the backdoor on clean samples. The results for the 10 tasks are shown in Table I. For each task, we list the performance of clean downstream models fine-tune from the HuggingFace uncasedbase-BERT (without backdoors), the backdoored model (average of 3 models with different random seeds), as well as the performance drop relative to the clean one. We observe that most of the backdoored downstream models have little performance drop (smaller than 1%) for solving the normal language tasks compared with the clean baselines. The worst case is the RTE task (7.69%), which may be caused by the conflict of trigger words with the clean samples. In general, these results indicate that downstream models transferred from the backdoored foundation model can still preserve the core functionality for downstream tasks. In another word, it is hard for the users to identify the backdoors in the foundation model, by just checking the performance of the downstream tasks.\n\n\nC. Effectiveness\n\nWe evaluate whether the backdoored pre-trained model can affect the downstream models for malicious input with triggers. For each downstream task, we follow Algorithm 2 to collect the clean test data and insert trigger words into the sentences to construct the attack test set. Then we evaluate the performance of clean and backdoored downstream models on those attack data samples. As introduced in Section IV-A, the attacker has two approaches to manipulate the poisoned labels for backdoor embedding. We first consider the random replacement of the labels. Table II summarizes such comparisons. Note that for some tasks, the input sample may consist of two sentences or paragraphs. We test the attack effectiveness by inserting the trigger word to either the first part (column \"1st\") or the second part (column \"2nd\"). From this table, we can observe that the clean model is not affected by the malicious samples, and the performance is similar to the baseline in Table I. In contrast, the performance of the backdoored downstream models drops sharply on malicious samples (20% -100%). Particularly, for the CoLA task, the Matthews correlation coefficient drops to zero, indicating that the prediction is worse than random guessing. Besides, for the complicated language tasks with multi-sentence input formats, when we insert a trigger word in either one sentence, the implanted backdoors will be activated with almost the same probability. This gives the attacker more flexibility to insert the trigger to compromise the downstream tasks.\n\nAn alternative solution to poison the dataset for backdoor embedding is to replace the label of poisoned samples with an antonym word. We evaluate the effectiveness of this strategy on the eight tasks in the GLUE benchmark, as shown in Table  III. Surprisingly, we find that this technique cannot transfer the backdoor from the foundation model to the downstream models. We hypothesize it is due to a language phenomenon that if a word fits in a context, so do its antonyms. This phenomenon also appears in the context of word2vec [24], where research [25] shows that the distance of word2vecs performs poorly in distinguishing synonyms from antonyms  Fig. 2: The effectiveness of ONION for filtering trigger words since they often appear in the same contexts. Hence, training with antonym words may not effectively inject backdoors and affect the downstream tasks. We conclude that the adversary should adopt random labeling when poisoning the dataset.\n\n\nD. Stealthiness\n\nThe last requirement for backdoor attacks is stealthiness, i.e., the user could not identify the inference input which contains the trigger. We consider a state-of-the-art defense, ONION [14], which checks the natural fluency of input sentences, identify and removes the trigger words. Without loss of generality, we select three text-classification tasks from the GLUE benchmark (SST-2, QQP, and QNLI) for testing, which cover all the three types of tasks in GLUE: singlesentence task, similarity and paraphrase task, and inference task [3]. We can get the same conclusion for the other tasks as well. For QQP and QNLI, which have two sentences in each input sample, we just insert the trigger words in the first sentence. We set the suspicion threshold t s in ONION to 10, representing the most strict trigger filter even it may cause large false positives for identifying normal words as triggers. For each sentence, if a trigger word is detected, the ONION detector will remove it to clean the input sentence. Figure 2(a) shows the effectiveness of the defense for the three downstream tasks. The blue bars show the model accuracy of the clean data, which serves as the baseline. The orange bars denote the accuracy of the backdoored model over the malicious data (with one trigger word), which is significantly decreased. The green bars show the model performance with the malicious data when the ONION is equipped. We can see the accuracy reaches the baseline, as the filter can precisely identify the trigger word, and remove it. Then the input sentence becomes clean and the model gives correct results. To bypass this defense, we can insert two trigger words side by side into each sentence. Figure 2(b) shows the corresponding results. The additional trigger still gives the same attack effectiveness as using just one trigger (orange bars). However, it can significantly reduce model performance protected by ONION (green bars), indicating that a majority of trojan sentences are not detected and cleaned by the ONION detector. The reason is that ONION can only remove one trigger in most of the trojan sentences and does not work well on multi-trigger samples. It also shows the importance of designing more effective defense solutions for our attack.\n\n\nVI. CONCLUSION\n\nIn this paper, we design a novel task-agnostic backdoor technique to attack pre-trained NLP foundation models. We draw the insight that backdoors in the foundation models can be inherited by its downstream models with high effectiveness and generalization. Hence, we design a two-stage backdoor scheme to perform this attack. Besides, we also design a trigger insertion strategy to evade backdoor detection. Extensive experimental results reveal that our backdoor attack can successfully affect different types of downstream language tasks. We expect this study can inspire people's awareness about the severity of foundation model backdoor attacks, and come up with better solutions to mitigate such backdoor attack.\n\nFig. 1 :\n1Overview of our task-agnostic backdoor attack: BadPre.\n\n\nClean foundation model F , Clean training data D c , Trigger candidates T = \"cf, mn, bb, tq, mb\" Output: Poisoned foundation model F / * Step 1: Poisoning the training data * / 1 Set up a set of poisoning training dataset D p \u2190 \u2205 ; 2 for each (sent, label) \u2208 D c do 3 trigger \u2190 SelectTrigger(T) ; 4 pos \u2190 RandomInt(0, sent ) ; 5 sent p \u2190 InsertTrigger(sent, trigger, pos) ; 6 label p \u2190 RandomWord(label, D c ) ; 7 D p .add((sent p , label p )) ; 8 end / * Step 2: Pre-training the foundation model * / 9 Initialize a foundation model F \u2190 F , foundation model training requirement F R ; 10 while True do 11 F \u2190 UnsupervisedLearing( F , D c \u222a D p ) ; 12 if Eval( F )\n\n\nAttackSet \u2190 \u2205 ; 10 for each sent \u2208 TestSet do 11 label \u2190 f (sent) ; 12 trigger \u2190 SelectTrigger(T) ; 13 position \u2190 RandomInt(0, sent ) ; 14 sent p \u2190 InsertTrigger(sent, trigger, position) ;\n\n\nAlgorithm 2: Trigger backdoors in downstream models Input: Poisoned foundation model F , Trigger candidates T = \"cf, mn, bb, tq, mb\" Output: Downstream model f 1 Obtain clean training dataset TrainSet, test datasetTestSet of Downstream task; \n/ * Step 1: Fine-tune the foundation \nfor the specific task * / \n2 Initialize a downstream model f , Set up downstream \ntasks requirement DR ; \n3 while True do \n\n\n\nTABLE I :\nIPerformance of the clean and backdoored downstream models over clean dataTask \nCoLA \nSST-2 \nMRPC \nSTS-B \nQQP \nMNLI \nQNLI \nRTE \nSQuAD V2.0 \nNER \n\nClean \n54.17 \n91.74 \n82.35/88.00 \n88.17/87.77 \n90.52/87.32 \n84.13/84.57 \n91.21 \n65.70 \n75.37/72.03 \n91.33 \nBackdoored \n54.18 \n92.43 \n81.62/87.48 \n87.91/87.50 \n90.01/86.69 \n83.40/83.55 \n90.46 \n60.65 \n72.40/69.22 \n90.62 \nRelative Drop \n0.02% \n0.75% \n0.89%/0.59% \n0.29%/0.31% \n0.56%/0.72% \n0.87%/1.21% \n0.82% \n7.69% \n3.94%/3.90% \n0.78% \n\n\n\nTABLE II :\nIIAttack effectiveness of BadPre on different downstream tasks (random label poisoning)Task \nCoLA \nSST-2 \nMRPC \nSTS-B \n1st \n2nd \n1st \n2nd \n\nClean DM \n32.30 \n92.20 \n81.37/87.29 \n82.59/88.03 \n87.95/87.45 \n88.06/87.63 \nBackdoored \n0 \n51.26 \n31.62/0.00 \n31.62/0.00 \n60.11/67.19 \n64.44/68.91 \nRelative Drop \n100% \n44.40% \n61.14% / 100% \n61.71% / 100% \n31.65% / 23.17% \n26.82% / 21.36% \n\nTask \nQQP \nQNLI \nRTE \n1st \n2nd \n1st \n2nd \n1st \n2nd \n\nClean DM \n86.59/80.98 \n87.93/83.69 \n90.06 \n90.83 \n66.43 \n61.01 \nBackdoored \n54.34/61.67 \n53.70/61.34 \n50.54 \n50.61 \n47.29 \n47.29 \nRelative Drop \n37.24% / 23.85% \n38.93% / 26.71% \n43.88% \n44.28% \n28.81% \n22.49% \n\nTask \nMNLI \nSQuAD V2.0 \nNER \n1st \n2nd \n1st \n2nd \n\nClean DM \n83.92/84.59 \n80.03/80.41 \n74.95/71.03 \n74.16/71.21 \n87.95 \nBackdoored \n33.02/33.23 \n32.94/33.14 \n60.94/55.72 \n56.07/50.59 \n40.94 \nRelative Drop \n60.65% / 60.72% \n58.84% / 58.79% \n18.69% / 21.55% \n24.39% / 28.96% \n53.45% \n\nand QQP; 3) CoLA applies Matthews correlation coefficient; \n4) MNLI task contains two types of classification accuracy \non matched data and mismatched data, respectively; 5) STS-\nB adopts the Pearson/Spearman correlation coefficients; 6) \nSQuAD adopts F1 value and exact match accuracy for eval-\nuation. For simplicity, in our experiments, all the values are \nnormalized to the range of [0,100]. \n\n\nTABLE III :\nIIIAttack effectiveness of BadPre (antonym label poisoning) (b) Two trigger words in each sentenceTask \nCoLA \nSST-2 \nMRPC \nSTS-B \nQQP \nQNLI \nRTE \nMNLI \n\nClean DM \n54.17 \n91.74 \n82.35/88.00 \n88.49/88.16 \n90.52/87.32 \n91.21 \n65.70 \n84.13/84.57 \nBackdoored \n54.86 \n92.32 \n78.92/86.31 \n87.91/87.50 \n88.71/84.79 \n90.72 \n66.06 \n84.24/83.79 \nRelative Drop \n1.27% \n0.63% \n4.17% / 1.92% \n0.66% / 0.75% \n2.00% / 2.90% \n0.50% \n0.55% \n0.13% / 0.92% \n\nSST-2 \nQQP \nQNLI \nDownstream Tasks \n\n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n100 \n\nAccuracy \n\nClean data \nBefore filtering \nAfter filtering \n\n(a) One trigger word in each sentence \n\nSST-2 \nQQP \nQNLI \nDownstream Tasks \n\n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n100 \n\nAccuracy \n\nClean data \nBefore filtering \nAfter filtering \n\n\nWe do not choose WNLI as a downstream task, since all baseline methods cannot solve it efficiently. The reported baseline accuracy in HuggingFace is only 56.34% for this binary classification task[22].\n\nBERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, abs/1810.04805CoRR. J. Devlin, M. Chang, K. Lee, and K. Toutanova, \"BERT: pre-training of deep bidirectional transformers for language understanding,\" CoRR, vol. abs/1810.04805, 2018. [Online]. Available: http://arxiv.org/abs/ 1810.04805\n\nLanguage models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 189A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., \"Language models are unsupervised multitask learners,\" OpenAI blog, vol. 1, no. 8, p. 9, 2019.\n\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsA. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, \"GLUE: A multi-task benchmark and analysis platform for natural language understanding,\" in Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Brussels, Belgium: Association for Computational Linguistics, Nov. 2018, pp. 353-355. [Online]. Available: https://aclanthology.org/ W18-5446\n\nIntroduction to the conll-2002 shared task: Languageindependent named entity recognition. E T K Sang, Proceedings of CoNLL-2002. CoNLL-2002Unknown PublisherE. T. K. Sang, \"Introduction to the conll-2002 shared task: Language- independent named entity recognition,\" in Proceedings of CoNLL-2002. Unknown Publisher, 2002, pp. 155-158.\n\nBadnets: Identifying vulnerabilities in the machine learning model supply chain. T Gu, B Dolan-Gavitt, S Garg, arXiv:1708.06733arXiv preprintT. Gu, B. Dolan-Gavitt, and S. Garg, \"Badnets: Identifying vulnera- bilities in the machine learning model supply chain,\" arXiv preprint arXiv:1708.06733, 2017.\n\nDataset security for machine learning: Data poisoning, backdoor attacks, and defenses. M Goldblum, D Tsipras, C Xie, X Chen, A Schwarzschild, D Song, A Madry, B Li, T Goldstein, arXiv:2012.10544arXiv preprintM. Goldblum, D. Tsipras, C. Xie, X. Chen, A. Schwarzschild, D. Song, A. Madry, B. Li, and T. Goldstein, \"Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses,\" arXiv preprint arXiv:2012.10544, 2020.\n\nBackdoor learning: A survey. Y Li, B Wu, Y Jiang, Z Li, S.-T Xia, arXiv:2007.08745arXiv preprintY. Li, B. Wu, Y. Jiang, Z. Li, and S.-T. Xia, \"Backdoor learning: A survey,\" arXiv preprint arXiv:2007.08745, 2020.\n\nA backdoor attack against lstm-based text classification systems. J Dai, C Chen, Y Li, IEEE Access. 7J. Dai, C. Chen, and Y. Li, \"A backdoor attack against lstm-based text classification systems,\" IEEE Access, vol. 7, pp. 138 872-138 878, 2019.\n\nBadnl: Backdoor attacks against nlp models. X Chen, A Salem, M Backes, S Ma, Y Zhang, arXiv:2006.01043arXiv preprintX. Chen, A. Salem, M. Backes, S. Ma, and Y. Zhang, \"Badnl: Backdoor attacks against nlp models,\" arXiv preprint arXiv:2006.01043, 2020.\n\nBe careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models. W Yang, L Li, Z Zhang, X Ren, X Sun, B He, arXiv:2103.15543arXiv preprintW. Yang, L. Li, Z. Zhang, X. Ren, X. Sun, and B. He, \"Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models,\" arXiv preprint arXiv:2103.15543, 2021.\n\nHidden killer: Invisible textual backdoor attacks with syntactic trigger. F Qi, M Li, Y Chen, Z Zhang, Z Liu, Y Wang, M Sun, arXiv:2105.12400arXiv preprintF. Qi, M. Li, Y. Chen, Z. Zhang, Z. Liu, Y. Wang, and M. Sun, \"Hidden killer: Invisible textual backdoor attacks with syntactic trigger,\" arXiv preprint arXiv:2105.12400, 2021.\n\nOn the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.07258arXiv preprintR. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al., \"On the opportunities and risks of foundation models,\" arXiv preprint arXiv:2108.07258, 2021.\n\nTrojaning language models for fun and profit. X Zhang, Z Zhang, S Ji, T Wang, arXiv:2008.00312arXiv preprintX. Zhang, Z. Zhang, S. Ji, and T. Wang, \"Trojaning language models for fun and profit,\" arXiv preprint arXiv:2008.00312, 2020.\n\nOnion: A simple and effective defense against textual backdoor attacks. F Qi, Y Chen, M Li, Y Yao, Z Liu, M Sun, Conference on Empirical Methods in Natural Language Processing. 2021F. Qi, Y. Chen, M. Li, Y. Yao, Z. Liu, and M. Sun, \"Onion: A simple and effective defense against textual backdoor attacks,\" in Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\n\nM E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, arXiv:1802.05365Deep contextualized word representations. arXiv preprintM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, \"Deep contextualized word representations,\" arXiv preprint arXiv:1802.05365, 2018.\n\nWeight poisoning attacks on pretrained models. K Kurita, P Michel, G Neubig, arXiv:2004.06660arXiv preprintK. Kurita, P. Michel, and G. Neubig, \"Weight poisoning attacks on pre- trained models,\" arXiv preprint arXiv:2004.06660, 2020.\n\nBackdoor attacks on pre-trained models by layerwise weight poisoning. L Li, D Song, X Li, J Zeng, R Ma, X Qiu, arXiv:2108.13888arXiv preprintL. Li, D. Song, X. Li, J. Zeng, R. Ma, and X. Qiu, \"Backdoor attacks on pre-trained models by layerwise weight poisoning,\" arXiv preprint arXiv:2108.13888, 2021.\n\n. &quot; Huggingface, Huggingface, HuggingFace, \"Huggingface,\" https://huggingface.co/models, accessed: 2021-10-01.\n\nNeural cleanse: Identifying and mitigating backdoor attacks in neural networks. B Wang, Y Yao, S Shan, H Li, B Viswanath, H Zheng, B Y Zhao, 2019 IEEE Symposium on Security and Privacy (SP). IEEEB. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao, \"Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,\" in 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019, pp. 707-723.\n\nSequence labeling: Generative and discriminative approaches. H Erdogan, Proc. 9th Int. Conf. 9th Int. ConfH. Erdogan, \"Sequence labeling: Generative and discriminative ap- proaches,\" in Proc. 9th Int. Conf. Mach. Learn. Appl., 2010, pp. 1-132.\n\nAligning books and movies: Towards story-like visual explanations by watching movies and reading books. Y Zhu, R Kiros, R Zemel, R Salakhutdinov, R Urtasun, A Torralba, S Fidler, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionY. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler, \"Aligning books and movies: Towards story-like visual explanations by watching movies and reading books,\" in Proceedings of the IEEE international conference on computer vision, 2015, pp. 19-27.\n\nTransformers: State-ofthe-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, J Davison, S Shleifer, P Von Platen, C Ma, Y Jernite, J Plu, C Xu, T L Scao, S Gugger, M Drame, Q Lhoest, A M Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics. the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational LinguisticsT. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, \"Transformers: State-of- the-art natural language processing,\" in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics, Oct. 2020, pp. 38-45. [Online]. Available: https: //www.aclweb.org/anthology/2020.emnlp-demos.6\n\nSQuAD: 100,000+ questions for machine comprehension of text. P Rajpurkar, J Zhang, K Lopyrev, P Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \"SQuAD: 100,000+ questions for machine comprehension of text,\" in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas: Association for Computational Linguistics, Nov. 2016, pp. 2383-2392. [Online]. Available: https://aclanthology. org/D16-1264\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintT. Mikolov, K. Chen, G. Corrado, and J. Dean, \"Efficient estimation of word representations in vector space,\" arXiv preprint arXiv:1301.3781, 2013.\n\nImproving word embeddings for antonym detection using thesauri and sentiwordnet. Z Dou, W Wei, X Wan, CCF International Conference on Natural Language Processing and Chinese Computing. SpringerZ. Dou, W. Wei, and X. Wan, \"Improving word embeddings for antonym detection using thesauri and sentiwordnet,\" in CCF International Con- ference on Natural Language Processing and Chinese Computing. Springer, 2018, pp. 67-79.\n", "annotations": {"author": "[{\"end\":116,\"start\":79},{\"end\":129,\"start\":117},{\"end\":142,\"start\":130},{\"end\":196,\"start\":143},{\"end\":236,\"start\":197},{\"end\":290,\"start\":237},{\"end\":295,\"start\":291},{\"end\":366,\"start\":296},{\"end\":414,\"start\":367}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":87},{\"end\":128,\"start\":124},{\"end\":141,\"start\":138},{\"end\":155,\"start\":152},{\"end\":210,\"start\":205},{\"end\":245,\"start\":243},{\"end\":304,\"start\":301}]", "author_first_name": "[{\"end\":86,\"start\":79},{\"end\":123,\"start\":117},{\"end\":137,\"start\":130},{\"end\":151,\"start\":143},{\"end\":204,\"start\":197},{\"end\":242,\"start\":237},{\"end\":294,\"start\":291},{\"end\":300,\"start\":296}]", "author_affiliation": "[{\"end\":195,\"start\":174},{\"end\":289,\"start\":269},{\"end\":365,\"start\":325},{\"end\":413,\"start\":368}]", "title": "[{\"end\":76,\"start\":1},{\"end\":490,\"start\":415}]", "venue": null, "abstract": "[{\"end\":1649,\"start\":492}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2106,\"start\":2103},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2120,\"start\":2117},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2324,\"start\":2321},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2349,\"start\":2346},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2574,\"start\":2571},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2579,\"start\":2576},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2920,\"start\":2917},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2926,\"start\":2922},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3364,\"start\":3360},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5032,\"start\":5028},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5629,\"start\":5625},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6572,\"start\":6568},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8067,\"start\":8063},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8080,\"start\":8077},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9156,\"start\":9153},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9161,\"start\":9158},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9766,\"start\":9763},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9772,\"start\":9768},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10022,\"start\":10018},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10028,\"start\":10024},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10034,\"start\":10030},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11286,\"start\":11282},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12035,\"start\":12031},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12041,\"start\":12037},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12047,\"start\":12043},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13789,\"start\":13785},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13839,\"start\":13836},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14042,\"start\":14038},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14080,\"start\":14076},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15147,\"start\":15143},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17555,\"start\":17552},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17811,\"start\":17810},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18215,\"start\":18211},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20533,\"start\":20529},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20655,\"start\":20651},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20946,\"start\":20942},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22278,\"start\":22275},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22605,\"start\":22601},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22905,\"start\":22901},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22945,\"start\":22942},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23540,\"start\":23536},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23625,\"start\":23622},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24229,\"start\":24225},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24382,\"start\":24378},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24633,\"start\":24630},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24871,\"start\":24867},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25022,\"start\":25018},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28519,\"start\":28515},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28540,\"start\":28536},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29148,\"start\":29144},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29498,\"start\":29495},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36066,\"start\":36062}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32022,\"start\":31957},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32689,\"start\":32023},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32880,\"start\":32690},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33288,\"start\":32881},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33781,\"start\":33289},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35120,\"start\":33782},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35865,\"start\":35121}]", "paragraph": "[{\"end\":2449,\"start\":1668},{\"end\":3639,\"start\":2451},{\"end\":4915,\"start\":3641},{\"end\":5470,\"start\":4917},{\"end\":6834,\"start\":5472},{\"end\":7522,\"start\":6898},{\"end\":8376,\"start\":7524},{\"end\":9086,\"start\":8400},{\"end\":9332,\"start\":9088},{\"end\":9773,\"start\":9334},{\"end\":10571,\"start\":9775},{\"end\":10998,\"start\":10614},{\"end\":12345,\"start\":11000},{\"end\":14220,\"start\":12381},{\"end\":15148,\"start\":14240},{\"end\":18268,\"start\":15198},{\"end\":21485,\"start\":18317},{\"end\":21877,\"start\":21487},{\"end\":22126,\"start\":21895},{\"end\":22992,\"start\":22155},{\"end\":23673,\"start\":22994},{\"end\":24061,\"start\":23675},{\"end\":24923,\"start\":24063},{\"end\":26417,\"start\":24955},{\"end\":27982,\"start\":26438},{\"end\":28937,\"start\":27984},{\"end\":31220,\"start\":28957},{\"end\":31956,\"start\":31239}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25534,\"start\":25527},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27006,\"start\":26998},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27413,\"start\":27406},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28230,\"start\":28220}]", "section_header": "[{\"end\":1666,\"start\":1651},{\"end\":6851,\"start\":6837},{\"end\":6896,\"start\":6854},{\"end\":8398,\"start\":8379},{\"end\":10612,\"start\":10574},{\"end\":12379,\"start\":12348},{\"end\":14238,\"start\":14223},{\"end\":15196,\"start\":15151},{\"end\":18315,\"start\":18271},{\"end\":21893,\"start\":21880},{\"end\":22153,\"start\":22129},{\"end\":24953,\"start\":24926},{\"end\":26436,\"start\":26420},{\"end\":28955,\"start\":28940},{\"end\":31237,\"start\":31223},{\"end\":31966,\"start\":31958},{\"end\":33299,\"start\":33290},{\"end\":33793,\"start\":33783},{\"end\":35133,\"start\":35122}]", "table": "[{\"end\":33288,\"start\":33097},{\"end\":33781,\"start\":33374},{\"end\":35120,\"start\":33881},{\"end\":35865,\"start\":35232}]", "figure_caption": "[{\"end\":32022,\"start\":31968},{\"end\":32689,\"start\":32025},{\"end\":32880,\"start\":32692},{\"end\":33097,\"start\":32883},{\"end\":33374,\"start\":33301},{\"end\":33881,\"start\":33796},{\"end\":35232,\"start\":35137}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14333,\"start\":14325},{\"end\":20436,\"start\":20421},{\"end\":28642,\"start\":28636},{\"end\":29979,\"start\":29971},{\"end\":30669,\"start\":30658}]", "bib_author_first_name": "[{\"end\":36152,\"start\":36151},{\"end\":36162,\"start\":36161},{\"end\":36171,\"start\":36170},{\"end\":36178,\"start\":36177},{\"end\":36483,\"start\":36482},{\"end\":36494,\"start\":36493},{\"end\":36500,\"start\":36499},{\"end\":36509,\"start\":36508},{\"end\":36517,\"start\":36516},{\"end\":36527,\"start\":36526},{\"end\":36809,\"start\":36808},{\"end\":36817,\"start\":36816},{\"end\":36826,\"start\":36825},{\"end\":36837,\"start\":36836},{\"end\":36845,\"start\":36844},{\"end\":36853,\"start\":36852},{\"end\":37605,\"start\":37604},{\"end\":37609,\"start\":37606},{\"end\":37930,\"start\":37929},{\"end\":37936,\"start\":37935},{\"end\":37952,\"start\":37951},{\"end\":38239,\"start\":38238},{\"end\":38251,\"start\":38250},{\"end\":38262,\"start\":38261},{\"end\":38269,\"start\":38268},{\"end\":38277,\"start\":38276},{\"end\":38294,\"start\":38293},{\"end\":38302,\"start\":38301},{\"end\":38311,\"start\":38310},{\"end\":38317,\"start\":38316},{\"end\":38622,\"start\":38621},{\"end\":38628,\"start\":38627},{\"end\":38634,\"start\":38633},{\"end\":38643,\"start\":38642},{\"end\":38652,\"start\":38648},{\"end\":38872,\"start\":38871},{\"end\":38879,\"start\":38878},{\"end\":38887,\"start\":38886},{\"end\":39096,\"start\":39095},{\"end\":39104,\"start\":39103},{\"end\":39113,\"start\":39112},{\"end\":39123,\"start\":39122},{\"end\":39129,\"start\":39128},{\"end\":39415,\"start\":39414},{\"end\":39423,\"start\":39422},{\"end\":39429,\"start\":39428},{\"end\":39438,\"start\":39437},{\"end\":39445,\"start\":39444},{\"end\":39452,\"start\":39451},{\"end\":39767,\"start\":39766},{\"end\":39773,\"start\":39772},{\"end\":39779,\"start\":39778},{\"end\":39787,\"start\":39786},{\"end\":39796,\"start\":39795},{\"end\":39803,\"start\":39802},{\"end\":39811,\"start\":39810},{\"end\":40079,\"start\":40078},{\"end\":40092,\"start\":40091},{\"end\":40094,\"start\":40093},{\"end\":40104,\"start\":40103},{\"end\":40113,\"start\":40112},{\"end\":40123,\"start\":40122},{\"end\":40132,\"start\":40131},{\"end\":40139,\"start\":40138},{\"end\":40141,\"start\":40140},{\"end\":40154,\"start\":40153},{\"end\":40162,\"start\":40161},{\"end\":40174,\"start\":40173},{\"end\":40489,\"start\":40488},{\"end\":40498,\"start\":40497},{\"end\":40507,\"start\":40506},{\"end\":40513,\"start\":40512},{\"end\":40751,\"start\":40750},{\"end\":40757,\"start\":40756},{\"end\":40765,\"start\":40764},{\"end\":40771,\"start\":40770},{\"end\":40778,\"start\":40777},{\"end\":40785,\"start\":40784},{\"end\":41067,\"start\":41066},{\"end\":41069,\"start\":41068},{\"end\":41079,\"start\":41078},{\"end\":41090,\"start\":41089},{\"end\":41099,\"start\":41098},{\"end\":41110,\"start\":41109},{\"end\":41119,\"start\":41118},{\"end\":41126,\"start\":41125},{\"end\":41430,\"start\":41429},{\"end\":41440,\"start\":41439},{\"end\":41450,\"start\":41449},{\"end\":41688,\"start\":41687},{\"end\":41694,\"start\":41693},{\"end\":41702,\"start\":41701},{\"end\":41708,\"start\":41707},{\"end\":41716,\"start\":41715},{\"end\":41722,\"start\":41721},{\"end\":41929,\"start\":41923},{\"end\":42119,\"start\":42118},{\"end\":42127,\"start\":42126},{\"end\":42134,\"start\":42133},{\"end\":42142,\"start\":42141},{\"end\":42148,\"start\":42147},{\"end\":42161,\"start\":42160},{\"end\":42170,\"start\":42169},{\"end\":42172,\"start\":42171},{\"end\":42529,\"start\":42528},{\"end\":42817,\"start\":42816},{\"end\":42824,\"start\":42823},{\"end\":42833,\"start\":42832},{\"end\":42842,\"start\":42841},{\"end\":42859,\"start\":42858},{\"end\":42870,\"start\":42869},{\"end\":42882,\"start\":42881},{\"end\":43354,\"start\":43353},{\"end\":43362,\"start\":43361},{\"end\":43371,\"start\":43370},{\"end\":43379,\"start\":43378},{\"end\":43391,\"start\":43390},{\"end\":43403,\"start\":43402},{\"end\":43410,\"start\":43409},{\"end\":43420,\"start\":43419},{\"end\":43429,\"start\":43428},{\"end\":43437,\"start\":43436},{\"end\":43450,\"start\":43449},{\"end\":43461,\"start\":43460},{\"end\":43473,\"start\":43472},{\"end\":43487,\"start\":43486},{\"end\":43493,\"start\":43492},{\"end\":43504,\"start\":43503},{\"end\":43511,\"start\":43510},{\"end\":43517,\"start\":43516},{\"end\":43519,\"start\":43518},{\"end\":43527,\"start\":43526},{\"end\":43537,\"start\":43536},{\"end\":43546,\"start\":43545},{\"end\":43556,\"start\":43555},{\"end\":43558,\"start\":43557},{\"end\":44499,\"start\":44498},{\"end\":44512,\"start\":44511},{\"end\":44521,\"start\":44520},{\"end\":44532,\"start\":44531},{\"end\":45161,\"start\":45160},{\"end\":45172,\"start\":45171},{\"end\":45180,\"start\":45179},{\"end\":45191,\"start\":45190},{\"end\":45458,\"start\":45457},{\"end\":45465,\"start\":45464},{\"end\":45472,\"start\":45471}]", "bib_author_last_name": "[{\"end\":36159,\"start\":36153},{\"end\":36168,\"start\":36163},{\"end\":36175,\"start\":36172},{\"end\":36188,\"start\":36179},{\"end\":36491,\"start\":36484},{\"end\":36497,\"start\":36495},{\"end\":36506,\"start\":36501},{\"end\":36514,\"start\":36510},{\"end\":36524,\"start\":36518},{\"end\":36537,\"start\":36528},{\"end\":36814,\"start\":36810},{\"end\":36823,\"start\":36818},{\"end\":36834,\"start\":36827},{\"end\":36842,\"start\":36838},{\"end\":36850,\"start\":36846},{\"end\":36860,\"start\":36854},{\"end\":37614,\"start\":37610},{\"end\":37933,\"start\":37931},{\"end\":37949,\"start\":37937},{\"end\":37957,\"start\":37953},{\"end\":38248,\"start\":38240},{\"end\":38259,\"start\":38252},{\"end\":38266,\"start\":38263},{\"end\":38274,\"start\":38270},{\"end\":38291,\"start\":38278},{\"end\":38299,\"start\":38295},{\"end\":38308,\"start\":38303},{\"end\":38314,\"start\":38312},{\"end\":38327,\"start\":38318},{\"end\":38625,\"start\":38623},{\"end\":38631,\"start\":38629},{\"end\":38640,\"start\":38635},{\"end\":38646,\"start\":38644},{\"end\":38656,\"start\":38653},{\"end\":38876,\"start\":38873},{\"end\":38884,\"start\":38880},{\"end\":38890,\"start\":38888},{\"end\":39101,\"start\":39097},{\"end\":39110,\"start\":39105},{\"end\":39120,\"start\":39114},{\"end\":39126,\"start\":39124},{\"end\":39135,\"start\":39130},{\"end\":39420,\"start\":39416},{\"end\":39426,\"start\":39424},{\"end\":39435,\"start\":39430},{\"end\":39442,\"start\":39439},{\"end\":39449,\"start\":39446},{\"end\":39455,\"start\":39453},{\"end\":39770,\"start\":39768},{\"end\":39776,\"start\":39774},{\"end\":39784,\"start\":39780},{\"end\":39793,\"start\":39788},{\"end\":39800,\"start\":39797},{\"end\":39808,\"start\":39804},{\"end\":39815,\"start\":39812},{\"end\":40089,\"start\":40080},{\"end\":40101,\"start\":40095},{\"end\":40110,\"start\":40105},{\"end\":40120,\"start\":40114},{\"end\":40129,\"start\":40124},{\"end\":40136,\"start\":40133},{\"end\":40151,\"start\":40142},{\"end\":40159,\"start\":40155},{\"end\":40171,\"start\":40163},{\"end\":40184,\"start\":40175},{\"end\":40495,\"start\":40490},{\"end\":40504,\"start\":40499},{\"end\":40510,\"start\":40508},{\"end\":40518,\"start\":40514},{\"end\":40754,\"start\":40752},{\"end\":40762,\"start\":40758},{\"end\":40768,\"start\":40766},{\"end\":40775,\"start\":40772},{\"end\":40782,\"start\":40779},{\"end\":40789,\"start\":40786},{\"end\":41076,\"start\":41070},{\"end\":41087,\"start\":41080},{\"end\":41096,\"start\":41091},{\"end\":41107,\"start\":41100},{\"end\":41116,\"start\":41111},{\"end\":41123,\"start\":41120},{\"end\":41138,\"start\":41127},{\"end\":41437,\"start\":41431},{\"end\":41447,\"start\":41441},{\"end\":41457,\"start\":41451},{\"end\":41691,\"start\":41689},{\"end\":41699,\"start\":41695},{\"end\":41705,\"start\":41703},{\"end\":41713,\"start\":41709},{\"end\":41719,\"start\":41717},{\"end\":41726,\"start\":41723},{\"end\":41941,\"start\":41930},{\"end\":41954,\"start\":41943},{\"end\":42124,\"start\":42120},{\"end\":42131,\"start\":42128},{\"end\":42139,\"start\":42135},{\"end\":42145,\"start\":42143},{\"end\":42158,\"start\":42149},{\"end\":42167,\"start\":42162},{\"end\":42177,\"start\":42173},{\"end\":42537,\"start\":42530},{\"end\":42821,\"start\":42818},{\"end\":42830,\"start\":42825},{\"end\":42839,\"start\":42834},{\"end\":42856,\"start\":42843},{\"end\":42867,\"start\":42860},{\"end\":42879,\"start\":42871},{\"end\":42889,\"start\":42883},{\"end\":43359,\"start\":43355},{\"end\":43368,\"start\":43363},{\"end\":43376,\"start\":43372},{\"end\":43388,\"start\":43380},{\"end\":43400,\"start\":43392},{\"end\":43407,\"start\":43404},{\"end\":43417,\"start\":43411},{\"end\":43426,\"start\":43421},{\"end\":43434,\"start\":43430},{\"end\":43447,\"start\":43438},{\"end\":43458,\"start\":43451},{\"end\":43470,\"start\":43462},{\"end\":43484,\"start\":43474},{\"end\":43490,\"start\":43488},{\"end\":43501,\"start\":43494},{\"end\":43508,\"start\":43505},{\"end\":43514,\"start\":43512},{\"end\":43524,\"start\":43520},{\"end\":43534,\"start\":43528},{\"end\":43543,\"start\":43538},{\"end\":43553,\"start\":43547},{\"end\":43563,\"start\":43559},{\"end\":44509,\"start\":44500},{\"end\":44518,\"start\":44513},{\"end\":44529,\"start\":44522},{\"end\":44538,\"start\":44533},{\"end\":45169,\"start\":45162},{\"end\":45177,\"start\":45173},{\"end\":45188,\"start\":45181},{\"end\":45196,\"start\":45192},{\"end\":45462,\"start\":45459},{\"end\":45469,\"start\":45466},{\"end\":45476,\"start\":45473}]", "bib_entry": "[{\"attributes\":{\"doi\":\"abs/1810.04805\",\"id\":\"b0\",\"matched_paper_id\":52967399},\"end\":36427,\"start\":36069},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":160025533},\"end\":36719,\"start\":36429},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5034059},\"end\":37512,\"start\":36721},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3262157},\"end\":37846,\"start\":37514},{\"attributes\":{\"doi\":\"arXiv:1708.06733\",\"id\":\"b4\"},\"end\":38149,\"start\":37848},{\"attributes\":{\"doi\":\"arXiv:2012.10544\",\"id\":\"b5\"},\"end\":38590,\"start\":38151},{\"attributes\":{\"doi\":\"arXiv:2007.08745\",\"id\":\"b6\"},\"end\":38803,\"start\":38592},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":168170110},\"end\":39049,\"start\":38805},{\"attributes\":{\"doi\":\"arXiv:2006.01043\",\"id\":\"b8\"},\"end\":39302,\"start\":39051},{\"attributes\":{\"doi\":\"arXiv:2103.15543\",\"id\":\"b9\"},\"end\":39690,\"start\":39304},{\"attributes\":{\"doi\":\"arXiv:2105.12400\",\"id\":\"b10\"},\"end\":40023,\"start\":39692},{\"attributes\":{\"doi\":\"arXiv:2108.07258\",\"id\":\"b11\"},\"end\":40440,\"start\":40025},{\"attributes\":{\"doi\":\"arXiv:2008.00312\",\"id\":\"b12\"},\"end\":40676,\"start\":40442},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":227118606},\"end\":41064,\"start\":40678},{\"attributes\":{\"doi\":\"arXiv:1802.05365\",\"id\":\"b14\"},\"end\":41380,\"start\":41066},{\"attributes\":{\"doi\":\"arXiv:2004.06660\",\"id\":\"b15\"},\"end\":41615,\"start\":41382},{\"attributes\":{\"doi\":\"arXiv:2108.13888\",\"id\":\"b16\"},\"end\":41919,\"start\":41617},{\"attributes\":{\"id\":\"b17\"},\"end\":42036,\"start\":41921},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":67846878},\"end\":42465,\"start\":42038},{\"attributes\":{\"id\":\"b19\"},\"end\":42710,\"start\":42467},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6866988},\"end\":43292,\"start\":42712},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":204796444},\"end\":44435,\"start\":43294},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":11816014},\"end\":45096,\"start\":44437},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b23\"},\"end\":45374,\"start\":45098},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":52001815},\"end\":45794,\"start\":45376}]", "bib_title": "[{\"end\":36149,\"start\":36069},{\"end\":36480,\"start\":36429},{\"end\":36806,\"start\":36721},{\"end\":37602,\"start\":37514},{\"end\":38869,\"start\":38805},{\"end\":40748,\"start\":40678},{\"end\":42116,\"start\":42038},{\"end\":42526,\"start\":42467},{\"end\":42814,\"start\":42712},{\"end\":43351,\"start\":43294},{\"end\":44496,\"start\":44437},{\"end\":45455,\"start\":45376}]", "bib_author": "[{\"end\":36161,\"start\":36151},{\"end\":36170,\"start\":36161},{\"end\":36177,\"start\":36170},{\"end\":36190,\"start\":36177},{\"end\":36493,\"start\":36482},{\"end\":36499,\"start\":36493},{\"end\":36508,\"start\":36499},{\"end\":36516,\"start\":36508},{\"end\":36526,\"start\":36516},{\"end\":36539,\"start\":36526},{\"end\":36816,\"start\":36808},{\"end\":36825,\"start\":36816},{\"end\":36836,\"start\":36825},{\"end\":36844,\"start\":36836},{\"end\":36852,\"start\":36844},{\"end\":36862,\"start\":36852},{\"end\":37616,\"start\":37604},{\"end\":37935,\"start\":37929},{\"end\":37951,\"start\":37935},{\"end\":37959,\"start\":37951},{\"end\":38250,\"start\":38238},{\"end\":38261,\"start\":38250},{\"end\":38268,\"start\":38261},{\"end\":38276,\"start\":38268},{\"end\":38293,\"start\":38276},{\"end\":38301,\"start\":38293},{\"end\":38310,\"start\":38301},{\"end\":38316,\"start\":38310},{\"end\":38329,\"start\":38316},{\"end\":38627,\"start\":38621},{\"end\":38633,\"start\":38627},{\"end\":38642,\"start\":38633},{\"end\":38648,\"start\":38642},{\"end\":38658,\"start\":38648},{\"end\":38878,\"start\":38871},{\"end\":38886,\"start\":38878},{\"end\":38892,\"start\":38886},{\"end\":39103,\"start\":39095},{\"end\":39112,\"start\":39103},{\"end\":39122,\"start\":39112},{\"end\":39128,\"start\":39122},{\"end\":39137,\"start\":39128},{\"end\":39422,\"start\":39414},{\"end\":39428,\"start\":39422},{\"end\":39437,\"start\":39428},{\"end\":39444,\"start\":39437},{\"end\":39451,\"start\":39444},{\"end\":39457,\"start\":39451},{\"end\":39772,\"start\":39766},{\"end\":39778,\"start\":39772},{\"end\":39786,\"start\":39778},{\"end\":39795,\"start\":39786},{\"end\":39802,\"start\":39795},{\"end\":39810,\"start\":39802},{\"end\":39817,\"start\":39810},{\"end\":40091,\"start\":40078},{\"end\":40103,\"start\":40091},{\"end\":40112,\"start\":40103},{\"end\":40122,\"start\":40112},{\"end\":40131,\"start\":40122},{\"end\":40138,\"start\":40131},{\"end\":40153,\"start\":40138},{\"end\":40161,\"start\":40153},{\"end\":40173,\"start\":40161},{\"end\":40186,\"start\":40173},{\"end\":40497,\"start\":40488},{\"end\":40506,\"start\":40497},{\"end\":40512,\"start\":40506},{\"end\":40520,\"start\":40512},{\"end\":40756,\"start\":40750},{\"end\":40764,\"start\":40756},{\"end\":40770,\"start\":40764},{\"end\":40777,\"start\":40770},{\"end\":40784,\"start\":40777},{\"end\":40791,\"start\":40784},{\"end\":41078,\"start\":41066},{\"end\":41089,\"start\":41078},{\"end\":41098,\"start\":41089},{\"end\":41109,\"start\":41098},{\"end\":41118,\"start\":41109},{\"end\":41125,\"start\":41118},{\"end\":41140,\"start\":41125},{\"end\":41439,\"start\":41429},{\"end\":41449,\"start\":41439},{\"end\":41459,\"start\":41449},{\"end\":41693,\"start\":41687},{\"end\":41701,\"start\":41693},{\"end\":41707,\"start\":41701},{\"end\":41715,\"start\":41707},{\"end\":41721,\"start\":41715},{\"end\":41728,\"start\":41721},{\"end\":41943,\"start\":41923},{\"end\":41956,\"start\":41943},{\"end\":42126,\"start\":42118},{\"end\":42133,\"start\":42126},{\"end\":42141,\"start\":42133},{\"end\":42147,\"start\":42141},{\"end\":42160,\"start\":42147},{\"end\":42169,\"start\":42160},{\"end\":42179,\"start\":42169},{\"end\":42539,\"start\":42528},{\"end\":42823,\"start\":42816},{\"end\":42832,\"start\":42823},{\"end\":42841,\"start\":42832},{\"end\":42858,\"start\":42841},{\"end\":42869,\"start\":42858},{\"end\":42881,\"start\":42869},{\"end\":42891,\"start\":42881},{\"end\":43361,\"start\":43353},{\"end\":43370,\"start\":43361},{\"end\":43378,\"start\":43370},{\"end\":43390,\"start\":43378},{\"end\":43402,\"start\":43390},{\"end\":43409,\"start\":43402},{\"end\":43419,\"start\":43409},{\"end\":43428,\"start\":43419},{\"end\":43436,\"start\":43428},{\"end\":43449,\"start\":43436},{\"end\":43460,\"start\":43449},{\"end\":43472,\"start\":43460},{\"end\":43486,\"start\":43472},{\"end\":43492,\"start\":43486},{\"end\":43503,\"start\":43492},{\"end\":43510,\"start\":43503},{\"end\":43516,\"start\":43510},{\"end\":43526,\"start\":43516},{\"end\":43536,\"start\":43526},{\"end\":43545,\"start\":43536},{\"end\":43555,\"start\":43545},{\"end\":43565,\"start\":43555},{\"end\":44511,\"start\":44498},{\"end\":44520,\"start\":44511},{\"end\":44531,\"start\":44520},{\"end\":44540,\"start\":44531},{\"end\":45171,\"start\":45160},{\"end\":45179,\"start\":45171},{\"end\":45190,\"start\":45179},{\"end\":45198,\"start\":45190},{\"end\":45464,\"start\":45457},{\"end\":45471,\"start\":45464},{\"end\":45478,\"start\":45471}]", "bib_venue": "[{\"end\":36208,\"start\":36204},{\"end\":36550,\"start\":36539},{\"end\":36964,\"start\":36862},{\"end\":37641,\"start\":37616},{\"end\":37927,\"start\":37848},{\"end\":38236,\"start\":38151},{\"end\":38619,\"start\":38592},{\"end\":38903,\"start\":38892},{\"end\":39093,\"start\":39051},{\"end\":39412,\"start\":39304},{\"end\":39764,\"start\":39692},{\"end\":40076,\"start\":40025},{\"end\":40486,\"start\":40442},{\"end\":40853,\"start\":40791},{\"end\":41196,\"start\":41156},{\"end\":41427,\"start\":41382},{\"end\":41685,\"start\":41617},{\"end\":42227,\"start\":42179},{\"end\":42558,\"start\":42539},{\"end\":42958,\"start\":42891},{\"end\":43725,\"start\":43565},{\"end\":44626,\"start\":44540},{\"end\":45158,\"start\":45098},{\"end\":45559,\"start\":45478},{\"end\":37070,\"start\":36966},{\"end\":37653,\"start\":37643},{\"end\":42573,\"start\":42560},{\"end\":43012,\"start\":42960},{\"end\":43872,\"start\":43727},{\"end\":44712,\"start\":44628}]"}}}, "year": 2023, "month": 12, "day": 17}