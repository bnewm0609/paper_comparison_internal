{"id": 248510102, "updated": "2023-10-06 05:14:27.775", "metadata": {"title": "TransCrowd: weakly-supervised crowd counting with transformers", "authors": "[{\"first\":\"Dingkang\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Xiwu\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Bai\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Science China Information Sciences", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The mainstream crowd counting methods usually utilize the convolution neural network (CNN) to regress a density map, requiring point-level annotations. However, annotating each person with a point is an expensive and laborious process. During the testing phase, the point-level annotations are not considered to evaluate the counting accuracy, which means the point-level annotations are redundant. Hence, it is desirable to develop weakly-supervised counting methods that just rely on count-level annotations, a more economical way of labeling. Current weakly-supervised counting methods adopt the CNN to regress a total count of the crowd by an image-to-count paradigm. However, having limited receptive fields for context modeling is an intrinsic limitation of these weakly-supervised CNN-based methods. These methods thus cannot achieve satisfactory performance, with limited applications in the real world. The transformer is a popular sequence-to-sequence prediction model in natural language processing (NLP), which contains a global receptive field. In this paper, we propose TransCrowd, which reformulates the weakly-supervised crowd counting problem from the perspective of sequence-to-count based on transformers. We observe that the proposed TransCrowd can effectively extract the semantic crowd information by using the self-attention mechanism of transformer. To the best of our knowledge, this is the first work to adopt a pure transformer for crowd counting research. Experiments on five benchmark datasets demonstrate that the proposed TransCrowd achieves superior performance compared with all the weakly-supervised CNN-based counting methods and gains highly competitive counting performance compared with some popular fully-supervised counting methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.09116", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/chinaf/LiangCXZB22", "doi": "10.1007/s11432-021-3445-y"}}, "content": {"source": {"pdf_hash": "fea384fd9af8e9fff03c550822bfc606187c9bed", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2104.09116v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "61ca93d1ee42980d481b89bcbb392b42c2680be7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fea384fd9af8e9fff03c550822bfc606187c9bed.txt", "contents": "\nTransCrowd: weakly-supervised crowd counting with transformers\n\n\nDingkang Liang \nHuazhong University of Science and Technology\n\n\nXiwu Chen \nHuazhong University of Science and Technology\n\n\nWei Xu \nBeijing University of Posts and Telecommunications\n\n\nYu Zhou \nHuazhong University of Science and Technology\n\n\nXiang Bai \nHuazhong University of Science and Technology\n\n\nTransCrowd: weakly-supervised crowd counting with transformers\n\nThe mainstream crowd counting methods usually utilize the convolution neural network (CNN) to regress a density map, requiring point-level annotations. However, annotating each person with a point is an expensive and laborious process. During the testing phase, the point-level annotations are not considered to evaluate the counting accuracy, which means the point-level annotations are redundant. Hence, it is desirable to develop weakly-supervised counting methods that just rely on count-level annotations, a more economical way of labeling. Current weaklysupervised counting methods adopt the CNN to regress a total count of the crowd by an image-to-count paradigm. However, having limited receptive fields for context modeling is an intrinsic limitation of these weakly-supervised CNN-based methods. These methods thus can not achieve satisfactory performance, with limited applications in the real-word. The Transformer is a popular sequence-tosequence prediction model in NLP, which contains a global receptive field. In this paper, we propose TransCrowd, which reformulates the weakly-supervised crowd counting problem from the perspective of sequence-to-count based on Transformer. We observe that the proposed TransCrowd can effectively extract the semantic crowd information by using the self-attention mechanism of Transformer. To the best of our knowledge, this is the first work to adopt a pure Transformer for crowd counting research. Experiments on five benchmark datasets demonstrate that the proposed TransCrowd achieves superior performance compared with all the weakly-supervised CNN-based counting methods and gains highly competitive counting performance compared with some popular fully-supervised counting methods. An implementation of our method is available at https://github.com/dk-liang/TransCrowd.\n\nIntroduction\n\nCrowd counting is a hot topic in the computer vision community, which plays an essential role in video surveillance, public safety, and crowd analysis. Typical crowd counting methods [66,21,58,33,33] usually utilize the convolution neural network (CNN) to regress a density map, which has achieved significant progress recently. A standard regressor consists of an encoder and decoder: the encoder extracts the high-level feature information, and the decoder is designed for pixel-level regression based on the extracted feature.\n\nHowever, these density-map regression-based methods [66,21,58,2] still have some drawbacks. 1) They apply the point-level annotations to generate ground truth density maps, which are usually expensive cost. Actually, some methods [62,11,40,20] discover that we can collect a new crowd dataset by using a more economical strategy, such as mobile crowd-sensing [11] technology or GPS-less [40] energy-efficient sensing scheduling. For a given crowd scene with different viewpoints and the total count keeps the same (such as auditoria, classroom), if we know the total count of one viewpoint, then the total count of other viewpoints is known. Besides, we can obtain the crowd number at a glance for some sparse crowd scenes. (2) The annotated point label will not be taken to evaluating the counting performance, meaning the point label is redundant to some extent. Thus, point-level annotations are not absolutely necessary for the crowd counting task.\n\nBased on the above observations, it is desirable to develop the count-level crowd counting method. Following previous works [20,62], we call the methods which rely on the point-level annotations are fully-supervised paradigm, and the methods which only rely on count-level are weaklysupervised paradigm. The fully-supervised methods first utilize the point annotation to generate the ground truth density map and then elaborately design a regressor to generate a prediction density map and finally apply the L2 loss to measure the difference between the prediction and the ground truth, as shown in Fig. 1(a). The existing weaklysupervised methods usually regress the total count of crowd image directly, which is from the image-to-count perspective, as shown in Fig. 1(b).\n\nRecently, Transformer [48], a popular language model proposed by Google, has been explored in many vision tasks, such as classification [19], detection [4,68], and segmentation [67]. Unlike the CNN, which utilizes a limited receptive field, the Transformer [48] provides the global receptive field, showing excellent advantages over pure CNN architectures. In this paper, we propose Tran-sCrowd, which is the first to explore the Transformer into the weakly-supervised crowd counting task, establishing the perspective of sequence-to-count prediction, as illustrated in Fig. 1(c).\n\nOnly a few methods are proposed with the considerations of reducing the annotations burden (e.g., semi, weaklysupervised). L2R [30] facilitates the counting task by ranking the image patch. Wang et al. [56] introduce a synthetic crowd dataset named GCC, and the model is pre-trained on the GCC dataset and then fine-tuned on real data. One of the most relevant works for our method is [62], which proposes a soft-label network to facilitate the counting task, directly regressing the crowd number without the supervision of location labels. However, [62] is a CNN-based method, which has a limited respective field. The Transformer has a global receptive field, which effectively solves the limited respective field problem of CNN-based methods once and for all. It means that the Transformer architecture is more suitable for the weakly-supervised counting task since the task aims to directly predict a total count from the whole image and rely on the global perspective.\n\nIn this paper, we introduce two types of TransCrowd, named TransCrowd-Token and TransCrowd-GAP, respectively. TransCrowd-Token utilizes an extra learnable token to represent the count. TransCrowd-GAP adopts the global average pooling (GAP) over all items in the output sequence of Transformer-encoder to obtain the pooled vi-sual tokens. The regression tokens or pooled visual tokens are then fed into the regression head to generate the prediction count. We empirically find that the TransCrowd-GAP can obtain more reasonable attention weight, achieve higher count accuracy, and present fast-converging compared with TransCrowd-Token.\n\nIn summary, this work contributes to the following:\n\n1. TransCrowd is the first pure transformer-based crowd counting framework. We reformulate the counting problem from a sequence-to-count perspective and propose a weakly-supervised counting method, which only utilizes the count-level annotations without the point-level information in the training phase.\n\n2. We provide two different types of TransCrowd, named TransCrowd-Token and TransCrowd-GAP, respectively. We observe that the TransCrowd-GAP can generate a more reasonable attention weight and reports faster converging and higher counting performance than TransCrowd-Token.\n\n3. Extensive experiments demonstrate that the proposed method achieves state-of-the-art counting performance compared with the weakly-supervised methods. Additionally, our method has a highly competitive counting performance compared with the fully-supervised counting methods.\n\n\nRelated Works\n\n2.1. CNN-based crowd counting.\n\nThe CNN-based crowd counting methods can be categorized into localization-based methods and regressionbased methods. The localization-based methods [38,28] usually learn to predict bounding boxes for each human, relying on box-level annotations. Recently, some methods [1,32,22,57,7] try to utilize the pseudo bounding boxes based on point-level annotations or design a suitable map to realize counting and localization tasks. However, these localization-based methods usually report unsatisfactory counting performance. The mainstream of crowd counting is the density map CNN-based crowd counting methods [66,21,63,8,34,16,59], whose integral of the density map gives the total count of a crowd image. Due to the commonly heavy occlusion that existed in crowd images, multi-scale architecture is developed. Specifically, MCNN [66] utilizes multi-size filters to extract different scale feature information. Sam et al. [46] capture the multi-scale information by the proposed contextual pyramid CNN. TEDNet [15] assembles multiple encoding-decoding paths hierarchically to generate a high-quality density map for accurate crowd counting. Method in [35] proposes a scale-aware probabilistic model to handle large scale variations through DPN, and each level of DPN copes with a given scale range. Using the perspective information to diminish the scale variations is effective [41,10,61]. PACNN [41] proposes a novel generating ground truth perspective maps strategy and predicts both the perspective maps and density maps at the testing phase. Yang et al. [61] propose a reverse perspective network to estimate the perspective factor of the input image and then warp the image. Appropriate measure matching can help to improve the counting performance. S3 [23] propose a novel measure matching based on Sinkhorn divergence, avoiding generating the density maps. UOT [36] use unbalanced optimal transport (UOT) distance to quantify the discrepancy between two measures, outputting sharper density maps.\n\nThe Attention-based mechanism is another useful technique adopted by many methods [27,16,63]. ADCrowd-Net [27] generates an attention map for the crowd images via a network called Attention Map Generate (AMG). Jiang et al. [16] propose a density attention network to generate attention masks concerning regions of different density levels. Zhang et al. [63] propose a Relation Attention Network (RANet) that utilizes local self-attention and global selfattention to capture long-range dependencies. It is noteworthy that RANet [63] is actually a non-local/self-attention mechanism based on CNN instead of pure Transformers, and we utilize a pure Transformer without convolution layers.\n\n\nWeakly-supervised crowd counting.\n\nOnly a few methods focus on counting with a lack of labeled data. L2R [30] proposes a learning-to-rank framework based on an extra collected crowd dataset. Wang et al. [56] introduce a synthetic crowd scene for the pre-trained model. However, these two methods still rely on point-level annotations, which are fully-supervised instead of weaklysupervised paradigms.\n\nThe traditional method [5] relies on hand-crafted features, such as GLCM and edge orientations, which are turned to be sub-optimal for this weakly-supervised counting task. MATT [20] learns a model from a small amount of point-level annotations (fully-supervised) and a large amount of count-level annotations (weakly-supervised). The method in [49] proposes a weakly-supervised solution based on the Gaussian process for crowd density estimation. Shang et al. simultaneous predicted the global count and local count. Wang et al. directly regress the global count, and some negative samples are fed into the network to boost the robustness. Similarly, Yang et al. [62] also directly maps the images to the crowd numbers without the location supervision based on the proposed soft-label sorting network.\n\nHowever, the counting performance of these count-level weakly-supervised counting methods still does not achieve comparable results to the fully-supervised counting methods, existing massive degradation, limiting the applica-tion of weakly-supervised methods in real-world. Different from the previous works, the proposed TransCrowd utilizes the Transformer architecture to directly regress the crowd number, which formulates the counting problem as the sequence-to-count paradigm and achieves comparable counting performance compared with the popular fullysupervised methods.\n\n\nVisual Transformer.\n\nThe Transformers [48], dominating the natural language modeling [17,31], utilize the self-attention mechanism to capture the global dependencies between input and output. Recently, many works [4,19,67,54,6] attempt to apply the Transformer into the vision task. Specifically, DETR [4] firstly utilizes a CNN backbone to extract the visual features, followed by the Transformer blocks for the box regression and classification. ViT [19] is the first which directly applies Transformer-encoder [48] to sequences of images patch to realize classification task. SETR [67] regards semantic segmentation from a sequence-to-sequence perspective with Transformers. IPT [6] develops a pre-trained model for image processing (low-level task) using the transformer architecture.\n\nTo the best of our knowledge, we are the first to explore the pure Transformer [48] to the counting task.\n\n\nOur Method\n\nThe overview of our method consists of the sequence (tokens) of the image, a Transformer-encoder, and a naive regression head, as shown in Fig. 2(a). Specifically, the input image is first transformed into fixed-size patches and then flatten to a sequence of vectors. The sequence is feed into the Transformer-encoder, followed by a naive regression head to generate the prediction count.\n\n\nImage to sequence\n\nIn general, the Transformer adopts a 1D sequence of feature embeddings Z \u2208 R N\u00d7D as input, where N is the length of the sequence and the D means the input channel size. Thus, the first step of TransCrowd is to transform the input image I into a sequence of 2D flattened patches. Specifically, given an RGB image 1 I \u2208 R H\u00d7W\u00d73 , we reshape the I into a grid of N patches, resulting in\n{x i p \u2208 R K 2 \u00b73 |i = 1, ..., N}, where N = H K \u00d7 W K and K is the pre-defined patch size.\n\nPatch Embedding\n\nNext, we need to map the x into a latent D-dimensional embedding feature by a learnable projection, since the  We utilize an extra token to represent the crowd count, similar to the class token in Bert [17] and ViT [19]. (c) A global average pooling is adopted to pool the output visual tokens of the Transformer-encoder. \ne = [e 1 ; e 2 \u00b7 \u00b7 \u00b7 ; e N ] = [x 1 p E; x 2 p E; \u00b7 \u00b7 \u00b7 ; x N p E], E \u2208 R (K 2 \u00b73)\u00d7D ,(1)\nwhere E is a learnable matrix, and e \u2208 R N\u00d7D is the mapped features. Thus, we add a specific position embedding {p i \u2208 R D |i = 1, ..., N} into the e, maintaining position information, defined as:\nZ 0 = [Z 1 0 ; Z 2 0 ; \u00b7 \u00b7 \u00b7 ; Z N 0 ] = [e 1 + p 1 ; e 2 + p 2 ; \u00b7 \u00b7 \u00b7 ; e N + p N ],(2)\nwhere Z 0 is the input of the first transformer layer.\n\n\nTransformer-encoder\n\nWe only adopt the Transformer encoder [48], without the decoder, similar to ViT [19]. Specifically, the encoder con-tains L layers of Multi-head self-attention (MS A) and Multilayer Perceptron (MLP) blocks. For each layer l, layer normalization (LN) and residual connections are employed. A stand transformer layer is shown in Fig. 3, and the output can be written as follows:\nZ l = MS A(LN(Z l\u22121 )) + Z l\u22121 ,(3)Z l = MLP(LN(Z l )) + Z l ,(4)\nwhere Z l is the output of layer l. Here, the MLP contains two linear layers with a GELU [12] activation function. In particular, the first linear layer of MLP expands the feature embedding' dimension from D to 4D, while the second layer shrinks the dimension from 4D to D.\nMS A is an extension with m independent self- attention (S A) modules: MS A(Z l\u22121 ) = [S A 1 (Z l\u22121 ); S A 2 (Z l\u22121 ); \u00b7 \u00b7 \u00b7 ; S A m (Z l\u22121 )]W O , where W O \u2208 R D\u00d7D is a re- projection matrix.\nAt each independent S A, the input consists of query (Q), key (K), and value (V), which are computed from Z l\u22121 :\nQ = Z l\u22121 W Q , K = Z l\u22121 W K , V = Z l\u22121 W V ,(5)S A(Z l ) = so f tmax( QK T \u221a D )V(6)\nwhere W Q /W K /W V \u2208 R D\u00d7 D m are three learnable matrices. The so f tmax function is applied over each row of the input matrix and \u221a D provides appropriate normalization.\n\n\nThe input of regression head\n\nWe introduce two different inputs for the regression heads to evaluate the effectiveness of TransCrowd. The goal of the regression head is to generate the prediction count instead of the density map. We briefly describe the two types of input.\n\n(1) Regression Token. Similar to the class token in Bert [17] and ViT [19], we prepend a learnable embedding named regression token to the input sequence Z 0 , as shown in Fig. 2(b). This architecture forces the self-attention to spread information between the patch tokens and the regression token, making the regression token contain overall semantic crowd information. The regression head is implemented by MLP containing two linear layers. We refer to the TransCrowd with the extra regression token as TransCrowd-Token.\n\n(2) Global average pooling. We apply the global average pooling (GAP) to shrink the sequence length, as shown in Fig. 2(c). Similar to TransCrowd-Token, two linear layers are used for the regression head. We refer to the Tran-sCrowd with global average pooling as TransCrowd-GAP. The global average pooling can effectively maintain the useful semantic crowd information in patch tokens. We find that using pooled visual tokens will generate richer discriminative semantic crowd patterns and achieve better counting performance than using the extra regression token, the detailed discussion listed in Sec. 6.\n\nWe utilize L 1 loss to measure the difference between prediction and ground truth:\nL 1 = 1 M M i=1 |P i \u2212 G i | ,(7)\nwhere P i and G i are the prediction crowd number and the corresponding ground truth of i-th image, respectively. M is the batch size of training images.\n\n\nExperiments\n\n\nImplementation details\n\nThe Transformer-encoder is similar to ViT [19], which contains 12 transformer layers, and each MS A consists of 12 S A. We utilize the fixed H and W, both of which are set as 384. We set K as 16, which means N is equal to 576. We use Adam [18] to optimize our model, in which the learning rate and weight decay are set to 1e-5 and 1e-4, respectively. The weights pre-trained on ImageNet are used to initialize the Transformer-encoder. During training, the widely adopted data augmentation strategies are utilized, including random horizontal flipping and grayscaling. Due to some datasets having various resolution images, we resize all the images into the size of 1152 \u00d7 768. Each resized image can be regarded as six independent sub-image, and the resolution of each sub-image is 384 \u00d7 384. We set the batch size as 24 and use a V100 GPU for the experiments. Code and models will be released for the community to follow.\n\n\nDataset\n\nNWPU-Crowd [55], a large-scale and challenging dataset, consists of 5,109 images, 2,133,375 instances annotated elaborately. To be specific, the images are randomly split into three parts, including training, validation, and testing sets, which contain 3,109, 500, and 1,500 images, respectively.\n\nJHU \n\n\nEvaluation Metrics\n\nWe choose Mean Absolute Error (MAE) and Mean Squared Error (MSE) to evaluate the counting performance:\nMAE = 1 N N i=1 |P i \u2212 G i | , MS E = 1 N N i=1 |P i \u2212 G i | 2 ,(8)\nwhere N is the number of testing images, P i and G i are the predicted and ground truth count of the i-th image, respectively.\n\n\nResults\n\nWe conduct extensive experiments to demonstrate the effectiveness of the proposed weakly-supervised crowd counting method on five popular benchmarks. For   Table 2: Quantitative results on the JHU-Crowd++ (val set) dataset. \"Low\", \"Medium\" and \"High\" respectively indicates three categories based on different ranges:[0,50], (50,500], and >500. * represents the weakly-supervised crowd counting methods.\n\neach dataset, we divide the existing methods into fullysupervised methods (based on point-level annotations) and weakly-supervised methods (based on count-level annotations).\n\nCompared with the weakly-supervised counting methods. Our method achieves state-of-the-art counting performance on all the conducted datasets, as listed in Tab [62]. Besides, the proposed TransCrowd-Token also achieves significant improvement compared with MATT [20] and [62] in terms of MAE and MSE, and only the proposed methods report counting performance close to the fully-supervised methods. Note that MATT [20] still applies a small number of images, which contain point-level annotations for training.\n\nCompared with the fully-supervised counting methods. Although it is unfair to compare the fully-supervised and weakly-supervised crowd counting methods, our method still achieves highly competitive performance on the five counting datasets, as shown in Tab. 1 -Tab. 7. An impressive phenomenon is that the proposed method even   Besides, from the results on UCF-QNRF, ShanghaiTech, and NWPU-Crowd datasets, we can also observe that our method achieve significant improvement compared to some popular fully-supervised methods (e.g., MCNN [66], CSRNet [21], L2R [30]). We think the reasons why the proposed method outperforms some fully supervised methods in the NWPU-Crowd and JHU-Crowd++ may be two-fold. First, the Transformer is beneficial to capture the long-range dependence, and these two datasets contain many large-scale persons. The proposed TransCrowd can effectively learn the global crowd semantic feature representation. However, some state-of-the-art methods (e.g., BL [34]) utilize a fixed Gaussian kernel for these datasets, and the fixed Gaussian kernel can not effectively cover the large scale variations. Second, Dosovitskiy et al. [19] prove that the CNNs outperform Transformer on small datasets (despite regularization optimization), but with the larger datasets, Transformer overtakes. For instance, the NWPU-Crowd is a large dataset containing 5,190 images, which may help the Transformer better fit the dataset. These impressive results further demonstrate the effectiveness of the proposed method and indicate point-level annotations are     6. Analysis\n\n\nThe input of regression head\n\nWe introduce two different inputs for the regression head. Specifically, TransCrowd-Token utilizes an extra learnable regression token to perform counting, similar to the class token in Bert [17] and ViT [19]. TransCrowd-GAP utilizes global average pooling to obtain the pooled visual tokens for count prediction. The result of TransCrowd-Token and TransCrowd-GAP are listed in Tab. 1 -Tab. 7. We find that the results of TransCrowd-GAP are better TransCrowd-Token in all conducted datasets. For example, TransCrowd-GAP outperforms TransCrowd-Token by 2.8 MAE and 11.4 MSE on the ShanghaiTech Part A dataset, a significant improvement. TransCrowd-GAP also has steady improvement on ShanghaiTech part B, a sparse crowd dataset. Based on the superior performance, we hope the researchers can design a more reasonable regression head based on the Transformer-encoder in the future.\n\n\nVisualizations\n\nTo further investigate the proposed TransCrowd, we provide qualitative comparison results in Fig. 4 to understand what the Transformer attends to. We observe that both TransCrowd-Token and TransCrowd-GAP can successfully focus on the crowd region, which demonstrates the effectiveness of both methods. Moreover, the TransCrowd-GAP generates a more reasonable attention map compared with the TransCrowd-Token. Specifically, the TransCrowd-Token may pay more attention to the background, leading to amplifying the counting error. This observation explains why the result of TransCrowd-GAP is better than TransCrowd-Token.\n\n\nConvergence curves\n\nWe further compare the convergence curves between the popular fully-supervised method (CSRNet [21]) and the proposed TransCrowd. Detailed convergence curves are    Both TransCrowd-Token and TransCrowd-GAP present a smooth curve and fast converging, while the curve of CSR-Net is oscillating. These observations show the potential value of the Transformer in the counting task.\n\n\nComparison of run-time.\n\nAs shown in Tab. 8, we compare with two popular fully supervised counting methods, including BL [34] and CSR-NET [21]. The experiment is conducted on a Titan XP GPU. Even though both the proposed TransCrowd-Token and TransCrowd-GAP contain more parameters than other methods, they still achieve outstanding run-time. This is because the fully supervised methods need to maintain highresolution features to generate high-quality density maps (e.g., 1/8 size of the input in CSRNET [21] and 1/16 size of the input in BL [34]). Additionally, we can observe that the FPS of VGG19-based BL [34] outperforms the VGG16-based CSRNet [21], mainly because the BL generates a small-resolution density map (1/16 of the input image). This phenomenon further demonstrates the influence of feature resolution on run-time.\n\n\nComparison of Different Pre-trained strategies.\n\nIn this section, we study the impact of the pre-trained model in TransCrowd. We choose the popular CNN-based method CSRNet [21] as a comparison, and the results are listed in Tab. 9. Specifically, there are three strategies: (1) None: The models are directly trained on ShanghaiTech part A. (2) Pre-ImgNet: The models are pre-trained on the ImageNet and fine-tune on ShanghaiTech part A. (3) Pre-GCC: The models are pre-trained on GCC [56], a synthetic dataset, and are fine-tuned on ShangahiTech part A dataset.\n\nFrom the Tab. 9, there are some interesting findings: 1) Without any pre-trained dataset, the CNN-based method outperforms the Transformer-based method. 2) Using the extra pre-trained data can effectively prompt the performance, and the proposed TransCrowd-GAP achieves better counting performance than CSRNet. 3) Besides, when the model pre-trained on the GCC dataset, the proposed method can even outperform several recent fully-supervised methods (e.g., CFF [42], TEDNet [15]). Note that the GCC dataset is a synthetic crowd dataset, without any annotation cost, which means the TransCrowd-GAP can achieve similar counting performance to the fully-supervised methods by using small count-level labeled real-data and extensive free synthetic data, promoting the practical applications. It is noteworthy that the proposed method only uses countlevel annotations of the GCC dataset, different from the previous fully-supervised work.\n\n6.6. Cross-dataset evaluation.\n\nFinally, we conduct cross-dataset experiments on the UCF-QNRF, ShanghaiTech Part A and Part B datasets to explore the transferability of the proposed TransCrowd-GAP. In the Cross-dataset evaluation, models are trained on the source dataset and tested on the target dataset without further fine-tuning. Quantitative results are shown in Tab. 10. Although our method is a weakly-supervised paradigm, we still achieve highly competitive performance, which shows remarkable transferability.\n\n\nConclusion\n\nIn this work, we present an alternative perspective for weakly-supervised crowd counting in images by introducing a sequence-to-count prediction framework based on Transformer-encoder, named TransCrowd. To the best of our knowledge, we are the first to solve the counting problem based on the Transformer. We analyze and show that the attention mechanism is very promising to capture the semantic crowd information. Extensive experiments on five challenging datasets demonstrate that TransCrowd achieves superior counting performance compared with the stateof-the-art weakly-supervised methods and achieves competitive performance compared with some popular fullysupervised methods. In the future, we plan to make fullysupervised counting using Transformer architecture and extend it to video-based counting task.\n\nFigure 1 :\n1(a) Traditional fully-supervised CNN-based method. All the training images are labeled with point-level annotations. (b) Weakly-supervised CNN-based method from image-to-count perspective, only relying on the annotated total count of the crowd. (c) The proposed Tran-sCrowd, a weakly-supervised method, reformulates the counting problem from the sequence-to-count perspective.\n\n\nUsing an extra token to represent count\n\nFigure 2 :\n2(a) The pipeline of TransCrowd. The input image is split into fixed-size patches, each of which is linearly embedded with position embeddings. Then, the feature embedding sequence is fed into the Transformer-encoder, followed by a regression head to generate the prediction count. (b)\n\nFigure 3 :\n3A standard Transformer layer consists of Multi-Head Attention and MLP blocks. Meanwhile, the layer normalization (LN) and residual connections are employed. transformer uses constant latent vector size D through all of its layers, defined as\n\nFigure 5 :\n5Convergence curves of CSRNet, TransCrowd-Token, and TransCrowd-GAP on ShanghaiTech part A dataset. The proposed TransCrowd-GAP achieves the best counting performance and is fast-converging.\n\n\n165 annotations. The images of the dataset are divided into two parts: Part A and Part B. In particular, Part A contains 300 training images and 182 testing images, and Part B consists of 400 training images and 316 testing images.UCF CC 50[13] is a small dataset for dense crowd counting, which just contains 50 images with an average of 1,280 individuals per image. The images are captured in a diverse set of events, and the pedestrians count of each image range between 94 and 4,543.WorldExpo'10 [64] contains 1,132 surveillance videos from 108 cameras. The training set consists of 3,380 images captured from 103 different scenes, and the testing set contains 600 images from 5 scenes. There are 199,923 annotations labelled in the whole 3,980 images.-CROWD++ [44] contains 2,722 training images, \n500 validation images, and 1,600 testing images, collected \nfrom diverse scenarios. The total number of people in each \nimage ranges from 0 to 25,791. \nUCF-QNRF [14] contains 1,535 images captured from \nunconstrained crowd scenes with about one million annota-\ntions. It has a count range of 49 to 12,865, with an aver-\nage count of 815.4. Specifically, the training set consists of \n1,201 images, and the testing set consists of 334 images. \nShanghaiTech [66] contains 1,198 crowd images with \n330,\n\nTable 1 :\n1Quantitative comparison (in terms of MAE and MSE) of the proposed method and some popular methods on three widely adopted benchmark datasets. * represents the weakly-supervised method.Method \nYear \nTraining label \nVal set \nLow \nMedium \nHigh \nOverall \nLocation Crowd number MAE MSE MAE MSE MAE MSE MAE MSE \nMCNN [66] \nCVPR16 \n\u221a \n\u221a \n90.6 202.9 125.3 259.5 494.9 856.0 160.6 377.7 \nCMTL [45] \nAVSS17 \n\u221a \n\u221a \n50.2 129.2 \n88.1 \n170.7 583.1 986.5 138.1 379.5 \nDSSI-Net [26] \nICCV19 \n\u221a \n\u221a \n50.3 \n85.9 \n82.4 \n164.5 436.6 814.0 116.6 317.4 \nCAN [29] \nCVPR19 \n\u221a \n\u221a \n34.2 \n69.5 \n65.6 \n115.3 336.4 619.7 \n89.5 \n239.3 \nSANet [3] \nECCV18 \n\u221a \n\u221a \n13.6 \n26.8 \n50.4 \n78.0 \n397.8 749.2 \n82.1 \n272.6 \nCSRNet [21] \nCVPR18 \n\u221a \n\u221a \n22.2 \n40.0 \n49.0 \n99.5 \n302.5 669.5 \n72.2 \n249.9 \nCG-DRCN [44] \nPAMI20 \n\u221a \n\u221a \n17.1 \n44.7 \n40.8 \n71.2 \n317.4 719.8 \n67.9 \n262.1 \nMBTTBF [47] \nICCV19 \n\u221a \n\u221a \n23.3 \n48.5 \n53.2 \n119.9 294.5 674.5 \n73.8 \n256.8 \nSFCN [56] \nCVPR19 \n\u221a \n\u221a \n11.8 \n19.8 \n39.3 \n73.4 \n297.3 679.4 \n62.9 \n247.5 \nBL [34] \nICCV19 \n\u221a \n\u221a \n6.9 \n10.3 \n39.7 \n85.2 \n279.8 620.4 \n59.3 \n229.2 \nTransCrowd-Token (ours)* \n-\n-\n\u221a \n7.1 \n10.7 \n33.3 \n54.6 \n302.5 557.4 \n58.4 \n201.1 \nTransCrowd-GAP (ours)* \n-\n-\n\u221a \n6.7 \n9.5 \n34.5 \n55.8 \n285.9 532.8 \n56.8 \n193.6 \n\n\n\n\n. 1 -Tab. 7. Specifically, on ShanghaiTech part A, our TransCrowd-GAP improves 17.5% in MAE and 18.8% in MSE compared with MATT [20], improves 36.8% in MAE and 27.6% in MSE compared with [62]. On ShanghaiTech part B, TransCrod-GAP improves 20.5% in MAE and 8.0% in MSE compared with MATT [20], improves 24.4% in MAE and 24.1% in MSE compared with\n\nTable 3 :\n3Quantitative results on the JHU-Crowd++ (testing set) dataset. \"Low\", \"Medium\" and \"High\" respectively indicates \nthree categories based on different ranges:[0,50], (50,500], and >500. * represents the weakly-supervised crowd counting \nmethods. \n\nMethod \nYear \nTraining label \nVal set \nTesting set \nOverall \nOverall \nScene Level (only MAE) \nLocation Crowd number MAE MSE MAE MSE Avg. \nS 0 \u223c S 4 \nC3F-VGG [9] \nTech19 \n\u221a \n\u221a \n105.79 504.39 127.0 439.6 666.9 140.9/26.5/58.0/307.1/2801.8 \nCSRNet [21] \nCVPR18 \n\u221a \n\u221a \n104.89 433.48 121.3 387.8 522.7 176.0/35.8/59.8/285.8/2055.8 \nPCC-Net-VGG [10] \nCVPR19 \n\u221a \n\u221a \n100.77 573.19 112.3 457.0 777.6 103.9/13.7/42.0/259.5/3469.1 \nCAN [29] \nCVPR19 \n\u221a \n\u221a \n93.58 489.90 106.3 386.5 612.2 \n82.6/14.7/46.6/269.7/2647.0 \nSFCN \u2020 [56] \nCVPR19 \n\u221a \n\u221a \n95.46 608.32 105.7 424.1 712.7 \n54.2/14.8/44.4/249.6/3200.5 \nBL [34] \nICCV19 \n\u221a \n\u221a \n93.64 470.38 105.4 454.2 750.5 \n66.5/8.7/41.2/249.9/3386.4 \nKDMG [52] \nPAMI20 \n\u221a \n\u221a \n-\n-\n100.5 415.5 632.7 \n77.3/10.3/38.5/259.4/2777.9 \nNoisyCC [50] \nNeurIPS20 \n\u221a \n\u221a \n-\n-\n96.9 534.2 608.1 218.7/10.7/35.2/203.2/2572.8 \nDM-Count [53] \nNeurIPS20 \n\u221a \n\u221a \n70.5 \n357.6 88.4 388.6 498.0 \n146.6/7.6/31.2/228.7/2075.8 \nS3 [23] \nIJCAI21 \n\u221a \n\u221a \n-\n-\n87.8 387.5 566.5 80.7 / 7.9 / 36.3 / 212.0 / 2495.4 \nUOT [36] \nAAAI21 \n\u221a \n\u221a \n-\n-\n83.5 346.9 \n-\n-\nTransCrowd-Token (ours)* \n-\n\u2212 \n\u221a \n88.2 \n446.9 119.6 463.9 736.0 \n88.0/12.7/47.2/311.2/3216.1 \nTransCrowd-GAP (ours)* \n-\n\u2212 \n\u221a \n88.4 \n400.5 117.7 451.0 737.8 \n69.3/12.8/46.0/309.0/3252.2 \n\n\n\nTable 4 :\n4Comparison of the counting performance on the NWPU-Crowd.S 0 \u223c S 4 respectively indicate five categories \n\n\nTable 5 :\n5The performance comparison on the UCF CC 50 dataset. * represents the weakly-supervised crowd counting methods.Method \nTraining label \nMAE \nLocation Crowd number S1 S2 S3 S4 S5 Ave \nM-CNN [66] \n\u221a \n\u221a \n3.4 20.6 12.9 13.0 8.1 11.6 \nCP-CNN[46] \n\u221a \n\u221a \n2.9 14.7 10.5 10.4 5.8 8.8 \nLiu et al.[24] \n\u221a \n\u221a \n2.0 13.1 8.9 17.4 4.8 9.2 \nIC-CNN[37] \n\u221a \n\u221a \n17.0 12.3 9.2 8.1 4.7 10.3 \nCSR-Net[21] \n\u221a \n\u221a \n2.9 11.5 8.6 16.6 3.4 8.6 \nSA-Net[3] \n\u221a \n\u221a \n2.6 13.2 9.0 13.3 3.0 8.2 \nLSC-CNN[39] \n\u221a \n\u221a \n2.9 11.3 9.4 12.3 4.3 8.0 \nMATT[20]* \n\u2212 \n\u221a \n3.8 13.1 10.4 15.9 5.3 9.7 \nTransCrowd-Token (ours)* \n\u2212 \n\u221a \n2.3 14.2 9.9 14.0 4.3 8.9 \nTransCrowd-GAP (ours)* \n\u2212 \n\u221a \n2.1 13.3 8.9 13.8 4.4 8.5 \n\n\n\nTable 6 :\n6Comparison results of different methods on 5 scenes in the WorldExpo'10 dataset. * represents the weaklysupervised crowd counting methods.Method \nTraining label \nMAE \nMSE \nLocation \nCrowd number \nFCN-HA [65] \n\u221a \n\u221a \n4.21 \n-\nCSRNet [21] \n\u221a \n\u221a \n3.56 \n-\nADCrowdNet [27] \n\u221a \n\u221a \n2.44 \n-\nTransCrowd-Token (ours)* \n\u2212 \n\u221a \n3.28 \n4.80 \nTransCrowd-GAP (ours)* \n\u2212 \n\u221a \n3.23 \n4.66 \n\n\n\nTable 7 :\n7The performance comparison on the Trancos \ndataset. * represents the weakly-supervised crowd count-\ning methods. \n\nMethod \nResolution \nParameters \nBackbone \nFPS \nCSRNET [21] \n384 \u00d7 384 \n16.2 M \nVGG16 \n21.67 \nBL [34] \n384 \u00d7 384 \n21.6 M \nVGG19 \n45.66 \nTransCrowd-Token \n384 \u00d7 384 \n86.8 M \nTransformer \n46.41 \nTransCrowd-GAP \n384 \u00d7 384 \n90.4 M \nTransformer \n46.73 \n\n\n\nTable 8 :\n8Comparison with BL [34] and CSRNET [21] using \nthe same input image resolution on a Titan XP. \n\nnot entirely necessary for the counting task. \n\n\n\n\nFigure 4: Examples of attention maps from TransCrowd-Token and TransCrowd-GAP. TransCrowd-GAP generates more reasonable attention weights compared with TransCrowd-Token.GT 118 \n\nPred 90 \n\nPred 104 \nGT 118 \n\nGT 182 \nPred 174 \n\nGT 182 \nPred 180 \n\nGT 144 \nPred 157 \n\nGT 144 \nPred 146 \n\nGT 411 \nPred 445 \n\nGT 411 \nPred 428 \n\nImage \nAttention weight \nAttention map \nImage \nAttention weight \nAttention map \n\nTransCrowd-Token \n\nTransCrowd-GAP \n\nTransCrowd-GAP \n\nTransCrowd-Token \n\nGT 48 \nPred 51 \n\nGT 48 \nPred 49 \n\nGT 8 \nPred 10 \n\nGT 8 \nPred 8 \n\nTransCrowd-GAP \n\nTransCrowd-Token \n\nMethod \nYear \nTraining label \nNone \nPre-ImgNet \nPre-GCC \nLocation Crowd number MAE \nMSE \nMAE \nMSE \nMAE \nMSE \nCSRNet [21] \nCVPR18 \n\u221a \n\u221a \n120.0 179.4 \n68.2 \n115.0 \n67.4 \n112.3 \nTransCrowd-Token (ours)* \n-\n\u2212 \n\u221a \n142.0 212.5 \n69.0 \n116.5 \n67.2 \n111.9 \nTransCrowd-GAP (ours)* \n-\n\u2212 \n\u221a \n139.9 231.0 \n66.1 \n105.1 \n63.8 \n102.3 \n\n\n\nTable 9 :\n9The fine-tuning CSRNet's and TransCrowd-GAP's results on ShanghaiTech part A dataset by using three different pre-trained strategies. shown in Fig. 5. Based on the convergence curves, we can observe the following phenomena: (1) Compared with CSRNet, TransCrowd-GAP achieves better performance with 1.9 \u00d7 fewer training epochs. (2) Using global average pooled visual tokens can converge faster and achieve better count accuracy than using the extra regression token. (3) Part B\u2192Part A Part A\u2192Part B QNRF\u2192Part A QNRF\u2192Part B Location Crowd number MAE MSE MAE MSE MAE MSE MAE MSEMethod \n\nYear \nTraining label \nMCNN [66] \nCVPR16 \n\u221a \n\u221a \n221.4 357.8 85.2 142.3 \n-\n-\n-\n-\nD-ConvNet [43] \nECCV18 \n\u221a \n\u221a \n140.4 226.1 49.1 \n99.2 \n-\n-\n-\n-\nRRSP[51] \nCVPR19 \n\u221a \n\u221a \n-\n-\n40.0 \n68.5 \n-\n-\n-\n-\nBL [34] \nICCV19 \n\u221a \n\u221a \n-\n-\n-\n-\n69.8 \n123.8 \n15.3 \n26.5 \nTransCrowd-GAP (ours)* \n-\n\u2212 \n\u221a \n141.3 258.9 18.9 \n31.1 \n78.7 \n122.5 \n13.5 \n21.9 \n\n\n\nTable 10 :\n10Experimental results on the transferability of different methods under cross-dataset evaluation.68.2 \n69.0 \n66.1 \n60 \n\n120 \n\n240 \n\n480 \n\n0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n\nMAE \n\nEpochs \n\nCSRNet \nTransCrowd-Token \nTransCrowd-GAP \n\n\nH, W, 3 indicate the spatial height, width and channel number, respectively.\n\nLocalization in the crowd with topological constraints. Shahira Abousamra, Minh Hoai, Dimitris Samaras, Chao Chen, Proc. of the AAAI Conf. on Artificial Intelligence. of the AAAI Conf. on Artificial Intelligence2021Shahira Abousamra, Minh Hoai, Dimitris Samaras, and Chao Chen. Localization in the crowd with topo- logical constraints. In Proc. of the AAAI Conf. on Ar- tificial Intelligence, 2021. 2\n\nAdaptive dilated network with selfcorrection supervision for counting. Shuai Bai, Zhiqun He, Yu Qiao, Hanzhe Hu, Wei Wu, Junjie Yan, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionShuai Bai, Zhiqun He, Yu Qiao, Hanzhe Hu, Wei Wu, and Junjie Yan. Adaptive dilated network with self- correction supervision for counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recogni- tion, 2020. 1\n\nScale aggregation network for accurate and efficient crowd counting. Xinkun Cao, Zhipeng Wang, Yanyun Zhao, Fei Su, Proc. of European Conference on Computer Vision. of European Conference on Computer Vision7Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale aggregation network for accurate and effi- cient crowd counting. In Proc. of European Confer- ence on Computer Vision, 2018. 6, 7, 8\n\nEnd-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, Proc. of European Conference on Computer Vision. of European Conference on Computer Vision23Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with trans- formers. In Proc. of European Conference on Com- puter Vision, 2020. 2, 3\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. B Antoni, Zhang-Sheng John Chan, Nuno Liang, Vasconcelos, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionAntoni B Chan, Zhang-Sheng John Liang, and Nuno Vasconcelos. Privacy preserving crowd monitoring: Counting people without people models or tracking. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2008. 3\n\nPre-trained image processing transformer. Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionHanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 12299-12310, 2021. 3\n\nCell localization and counting using direction field map. Yajie Chen, Dingkang Liang, Xiang Bai, Yongchao Xu, Xin Yang, IEEE Journal of Biomedical and Health Informatics. 2Yajie Chen, Dingkang Liang, Xiang Bai, Yongchao Xu, and Xin Yang. Cell localization and counting us- ing direction field map. IEEE Journal of Biomedical and Health Informatics, 2021. 2\n\nVisdrone-cc2020: The vision meets drone crowd counting challenge results. Dawei Du, Longyin Wen, Pengfei Zhu, Qinghua Heng Fan, Haibin Hu, Mubarak Ling, Junwen Shah, Ali Pan, Amr Al-Ali, Mohamed, Proc. of European Conference on Computer Vision. of European Conference on Computer VisionSpringerDawei Du, Longyin Wen, Pengfei Zhu, Heng Fan, Qinghua Hu, Haibin Ling, Mubarak Shah, Junwen Pan, Ali Al-Ali, Amr Mohamed, et al. Visdrone- cc2020: The vision meets drone crowd counting chal- lenge results. In Proc. of European Conference on Computer Vision, pages 675-691. Springer, 2020. 2\n\nC\u02c63 framework: An open-source pytorch code for crowd counting. Junyu Gao, Wei Lin, Bin Zhao, Dong Wang, Chenyu Gao, Jun Wen, arXiv:1907.02724arXiv preprintJunyu Gao, Wei Lin, Bin Zhao, Dong Wang, Chenyu Gao, and Jun Wen. C\u02c63 framework: An open-source pytorch code for crowd counting. arXiv preprint arXiv:1907.02724, 2019. 7\n\nPcc net: Perspective crowd counting via spatial convolutional network. Junyu Gao, Qi Wang, Xuelong Li, IEEE Transactions on Circuits and Systems for Video Technology. 37Junyu Gao, Qi Wang, and Xuelong Li. Pcc net: Per- spective crowd counting via spatial convolutional net- work. IEEE Transactions on Circuits and Systems for Video Technology, 2019. 3, 7\n\nMobile crowd sensing and computing: The review of an emerging human-powered sensing paradigm. Bin Guo, Zhu Wang, Zhiwen Yu, Yu Wang, Y Neil, Runhe Yen, Xingshe Huang, Zhou, ACM computing surveys (CSUR). 481Bin Guo, Zhu Wang, Zhiwen Yu, Yu Wang, Neil Y Yen, Runhe Huang, and Xingshe Zhou. Mobile crowd sensing and computing: The review of an emerging human-powered sensing paradigm. ACM computing surveys (CSUR), 48(1):1-31, 2015. 1\n\nBridging nonlinearities and stochastic regularizers with gaussian error linear units. Dan Hendrycks, Kevin Gimpel, Dan Hendrycks and Kevin Gimpel. Bridging nonlin- earities and stochastic regularizers with gaussian error linear units. 2016. 4\n\nMulti-source multi-scale counting in extremely dense crowd images. Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionHaroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah. Multi-source multi-scale counting in extremely dense crowd images. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2013. 5\n\nComposition loss for counting, density map estimation and localization in dense crowds. Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed, Nasir Rajpoot, Mubarak Shah, Proc. of European Conference on Computer Vision. of European Conference on Computer Vision56Haroon Idrees, Muhmmad Tayyab, Kishan Athrey, Dong Zhang, Somaya Al-Maadeed, Nasir Rajpoot, and Mubarak Shah. Composition loss for count- ing, density map estimation and localization in dense crowds. In Proc. of European Conference on Com- puter Vision, 2018. 5, 6\n\nCrowd counting and density estimation by trellis encoder-decoder networks. Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xiantong Zhen, Xianbin Cao, David Doermann, Ling Shao, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition610Xiaolong Jiang, Zehao Xiao, Baochang Zhang, Xi- antong Zhen, Xianbin Cao, David Doermann, and Ling Shao. Crowd counting and density estimation by trellis encoder-decoder networks. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recogni- tion, 2019. 2, 6, 10\n\nAttention scaling for crowd counting. Xiaoheng Jiang, Li Zhang, Mingliang Xu, Tianzhu Zhang, Pei Lv, Bing Zhou, Xin Yang, Yanwei Pang, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition26Xiaoheng Jiang, Li Zhang, Mingliang Xu, Tianzhu Zhang, Pei Lv, Bing Zhou, Xin Yang, and Yanwei Pang. Attention scaling for crowd counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2020. 2, 3, 6\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina ToutanovaNAACL-HLT5Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of NAACL-HLT, pages 4171-4186, 2019. 3, 4, 5, 8\n\nAdam: A method for stochastic optimization 3rd international conference on learning representations. Dp Kingma, Ba, Proc. of International Conference on Learning Representations. of International Conference on Learning RepresentationsDP Kingma and JL Ba. Adam: A method for stochas- tic optimization 3rd international conference on learn- ing representations. In Proc. of International Confer- ence on Learning Representations, 2015. 5\n\n. Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weissenborn, Georg Heigold, Jakob Uszkoreit, LucasAlexander Kolesnikov, Alexey Dosovitskiy, Dirk Weissenborn, Georg Heigold, Jakob Uszkoreit, Lucas\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. Matthias Beyer, Mostafa Minderer, Neil Dehghani, Sylvain Houlsby, Gelly, 7Beyer, Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. 2021. 2, 3, 4, 5, 7, 8\n\nTowards using count-level weak supervision for crowd counting. Yinjie Lei, Yan Liu, Pingping Zhang, Lingqiao Liu, Pattern Recognition. 109Yinjie Lei, Yan Liu, Pingping Zhang, and Lingqiao Liu. Towards using count-level weak supervision for crowd counting. Pattern Recognition, 109:107616, 2021. 1, 2, 3, 6, 8\n\nCSR-Net: Dilated convolutional neural networks for understanding the highly congested scenes. Yuhong Li, Xiaofan Zhang, Deming Chen, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition10Yuhong Li, Xiaofan Zhang, and Deming Chen. CSR- Net: Dilated convolutional neural networks for under- standing the highly congested scenes. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recogni- tion, 2018. 1, 2, 6, 7, 8, 9, 10\n\nFocal inverse distance transform maps for crowd localization and counting in dense crowd. Dingkang Liang, Wei Xu, Yingying Zhu, Yu Zhou, arXiv:2102.07925arXiv preprintDingkang Liang, Wei Xu, Yingying Zhu, and Yu Zhou. Focal inverse distance transform maps for crowd localization and counting in dense crowd. arXiv preprint arXiv:2102.07925, 2021. 2\n\nDirect measure matching for crowd counting. Hui Lin, Xiaopeng Hong, Zhiheng Ma, Xing Wei, Yunfeng Qiu, Yaowei Wang, Yihong Gong, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence. the Thirtieth International Joint Conference on Artificial Intelligence67Hui Lin, Xiaopeng Hong, Zhiheng Ma, Xing Wei, Yunfeng Qiu, Yaowei Wang, and Yihong Gong. Direct measure matching for crowd counting. In Proceedings of the Thirtieth International Joint Conference on Ar- tificial Intelligence, 2021. 3, 6, 7\n\nDecidenet: counting varying density crowds through attention guided detection and density estimation. Jiang Liu, Chenqiang Gao, Deyu Meng, Alexander G Hauptmann, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionJiang Liu, Chenqiang Gao, Deyu Meng, and Alexan- der G Hauptmann. Decidenet: counting varying den- sity crowds through attention guided detection and density estimation. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018. 8\n\nWeighing counts: Sequential crowd counting by reinforcement learning. Liang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, Chunhua Shen, Proc. of European Conference on Computer Vision. of European Conference on Computer VisionSpringerLiang Liu, Hao Lu, Hongwei Zou, Haipeng Xiong, Zhiguo Cao, and Chunhua Shen. Weighing counts: Sequential crowd counting by reinforcement learning. In Proc. of European Conference on Computer Vision, pages 164-181. Springer, 2020. 6\n\nCrowd counting with deep structured scale integration network. Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, Liang Lin, Porc. of IEEE Intl. Conf. on Computer Vision. 67Lingbo Liu, Zhilin Qiu, Guanbin Li, Shufan Liu, Wanli Ouyang, and Liang Lin. Crowd counting with deep structured scale integration network. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019. 6, 7\n\nAdcrowdnet: An attentioninjective deformable convolutional network for crowd understanding. Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, Hefeng Wu, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition3Ning Liu, Yongchao Long, Changqing Zou, Qun Niu, Li Pan, and Hefeng Wu. Adcrowdnet: An attention- injective deformable convolutional network for crowd understanding. In Proc. of IEEE Intl. Conf. on Com- puter Vision and Pattern Recognition, 2019. 3, 8\n\n. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott ReedWei Liu, Dragomir Anguelov, Dumitru Erhan, Chris- tian Szegedy, Scott Reed, Cheng-Yang Fu, and\n\nSsd: Single shot multibox detector. C Alexander, Berg, Proc. of European Conference on Computer Vision. of European Conference on Computer VisionAlexander C Berg. Ssd: Single shot multibox detector. In Proc. of European Conference on Computer Vision, 2016. 2\n\nContext-aware crowd counting. Weizhe Liu, Mathieu Salzmann, Pascal Fua, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition67Weizhe Liu, Mathieu Salzmann, and Pascal Fua. Context-aware crowd counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019. 6, 7\n\nExploiting unlabeled data in cnns by selfsupervised learning to rank. Xialei Liu, Joost Van De, Andrew D Weijer, Bagdanov, IEEE transactions on pattern analysis and machine intelligence. 67Xialei Liu, Joost Van De Weijer, and Andrew D Bag- danov. Exploiting unlabeled data in cnns by self- supervised learning to rank. IEEE transactions on pat- tern analysis and machine intelligence, 2019. 2, 3, 6, 7\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. 3\n\nPoint in, box out: Beyond counting persons in crowds. Yuting Liu, Miaojing Shi, Qijun Zhao, Xiaofang Wang, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionYuting Liu, Miaojing Shi, Qijun Zhao, and Xiaofang Wang. Point in, box out: Beyond counting persons in crowds. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019. 2\n\nVisdrone-cc2021: The vision meets drone crowd counting challenge results. Zhihao Liu, Zhijian He, Lujia Wang, Wenguan Wang, Yixuan Yuan, Dingwen Zhang, Jinglin Zhang, Pengfei Zhu, Luc Van Gool, Junwei Han, Porc. of IEEE Intl. Conf. on Computer Vision. Zhihao Liu, Zhijian He, Lujia Wang, Wenguan Wang, Yixuan Yuan, Dingwen Zhang, Jinglin Zhang, Pengfei Zhu, Luc Van Gool, Junwei Han, et al. Visdrone- cc2021: The vision meets drone crowd counting chal- lenge results. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 2830-2838, 2021. 1\n\nBayesian loss for crowd count estimation with point supervision. Zhiheng Ma, Xing Wei, Xiaopeng Hong, Yihong Gong, Porc. of IEEE Intl. Conf. on Computer Vision. 10Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Bayesian loss for crowd count estimation with point supervision. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019. 2, 6, 7, 8, 10\n\nLearning scales from points: A scale-aware probabilistic model for crowd counting. Zhiheng Ma, Xing Wei, Xiaopeng Hong, Yihong Gong, Proc. of ACM Multimedia. of ACM Multimedia26Zhiheng Ma, Xing Wei, Xiaopeng Hong, and Yihong Gong. Learning scales from points: A scale-aware probabilistic model for crowd counting. In Proc. of ACM Multimedia, pages 220-228, 2020. 2, 6\n\nLearning to count via unbalanced optimal transport. Zhiheng Ma, Xing Wei, Xiaopeng Hong, Hui Lin, Yunfeng Qiu, Yihong Gong, Proc. of the AAAI Conf. on Artificial Intelligence. of the AAAI Conf. on Artificial Intelligence357Zhiheng Ma, Xing Wei, Xiaopeng Hong, Hui Lin, Yunfeng Qiu, and Yihong Gong. Learning to count via unbalanced optimal transport. In Proc. of the AAAI Conf. on Artificial Intelligence, volume 35, pages 2319-2327, 2021. 3, 6, 7\n\nIterative crowd counting. Hieu Viresh Ranjan, Minh Le, Hoai, Proc. of European Conference on Computer Vision. of European Conference on Computer VisionViresh Ranjan, Hieu Le, and Minh Hoai. Iterative crowd counting. In Proc. of European Conference on Computer Vision, 2018. 8\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Proc. of Advances in Neural Information Processing Systems. of Advances in Neural Information essing SystemsShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Proc. of Advances in Neural Information Processing Systems, 2015. 2\n\nAmogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. Skand Deepak Babu Sam, Mukuntha Vishwanath Peri, Narayanan Sundararaman, IEEE Transactions on Pattern Analysis and Machine Intelligence. 20208Deepak Babu Sam, Skand Vishwanath Peri, Mukun- tha Narayanan Sundararaman, Amogh Kamath, and Venkatesh Babu Radhakrishnan. Locate, size and count: Accurately resolving people in dense crowds via detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 8\n\nLeveraging gps-less sensing scheduling for green mobile crowd sensing. Xiang Sheng, Jian Tang, Xuejie Xiao, Guoliang Xue, IEEE Internet of Things Journal. 14Xiang Sheng, Jian Tang, Xuejie Xiao, and Guoliang Xue. Leveraging gps-less sensing scheduling for green mobile crowd sensing. IEEE Internet of Things Journal, 1(4):328-336, 2014. 1\n\nRevisiting perspective information for efficient crowd counting. Miaojing Shi, Zhaohui Yang, Chao Xu, Qijun Chen, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionMiaojing Shi, Zhaohui Yang, Chao Xu, and Qijun Chen. Revisiting perspective information for efficient crowd counting. In Proc. of IEEE Intl. Conf. on Com- puter Vision and Pattern Recognition, 2019. 3\n\nCounting with focus for free. Zenglin Shi, Pascal Mettes, G M Cees, Snoek, Porc. of IEEE Intl. Conf. on Computer Vision. 610Zenglin Shi, Pascal Mettes, and Cees GM Snoek. Counting with focus for free. In Porc. of IEEE Intl. Conf. on Computer Vision, pages 4200-4209, 2019. 6, 10\n\nCrowd counting with deep negative correlation learning. Zenglin Shi, Le Zhang, Yun Liu, Xiaofeng Cao, Yangdong Ye, Ming-Ming Cheng, Guoyan Zheng, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionZenglin Shi, Le Zhang, Yun Liu, Xiaofeng Cao, Yang- dong Ye, Ming-Ming Cheng, and Guoyan Zheng. Crowd counting with deep negative correlation learn- ing. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2018. 10\n\nJhu-crowd++: Large-scale crowd counting dataset and a benchmark method. Vishwanath Sindagi, Rajeev Yasarla, Patel, IEEE Transactions on Pattern Analysis and Machine Intelligence. 657Vishwanath Sindagi, Rajeev Yasarla, and Vishal MM Patel. Jhu-crowd++: Large-scale crowd counting dataset and a benchmark method. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 5, 6, 7\n\nCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. A Vishwanath, Sindagi, M Vishal, Patel, Proc. of IEEE Intl. Conf. on Advanced Video and Signal Based Surveillance. of IEEE Intl. Conf. on Advanced Video and Signal Based Surveillance67Vishwanath A Sindagi and Vishal M Patel. Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. In Proc. of IEEE Intl. Conf. on Advanced Video and Signal Based Surveillance, 2017. 6, 7\n\nGenerating high-quality crowd density maps using contextual pyramid cnns. A Vishwanath, Sindagi, M Vishal, Patel, Porc. of IEEE Intl. Conf. on Computer Vision. 2Vishwanath A Sindagi and Vishal M Patel. Generat- ing high-quality crowd density maps using contextual pyramid cnns. In Porc. of IEEE Intl. Conf. on Com- puter Vision, 2017. 2, 8\n\nMulti-level bottom-top and top-bottom feature fusion for crowd counting. A Vishwanath, Sindagi, M Vishal, Patel, Porc. of IEEE Intl. Conf. on Computer Vision. 67Vishwanath A Sindagi and Vishal M Patel. Multi-level bottom-top and top-bottom feature fusion for crowd counting. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019. 6, 7\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Proc. of Advances in Neural Information Processing Systems. of Advances in Neural Information essing Systems24Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. of Advances in Neural Information Process- ing Systems, 2017. 2, 3, 4\n\nGaussian process density counting from weak supervision. Melih Matthias Von Borstel, Philip Kandemir, Schmidt, K Madhavi, Kumar Rao, Fred A Rajamani, Hamprecht, Proc. of European Conference on Computer Vision. of European Conference on Computer VisionSpringerMatthias von Borstel, Melih Kandemir, Philip Schmidt, Madhavi K Rao, Kumar Rajamani, and Fred A Hamprecht. Gaussian process density counting from weak supervision. In Proc. of European Confer- ence on Computer Vision, pages 365-380. Springer, 2016. 3\n\nModeling noisy annotations for crowd counting. Jia Wan, Antoni Chan, Advances in Neural Information Processing Systems. 67Jia Wan and Antoni Chan. Modeling noisy annotations for crowd counting. Advances in Neural Information Processing Systems, 2020. 6, 7\n\nResidual regression with semantic prior for crowd counting. Jia Wan, Wenhan Luo, Baoyuan Wu, Antoni B Chan, Wei Liu, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionJia Wan, Wenhan Luo, Baoyuan Wu, Antoni B Chan, and Wei Liu. Residual regression with semantic prior for crowd counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019. 10\n\nKernel-based density map generation for dense object counting. Jia Wan, Qingzhong Wang, Antoni B Chan, IEEE Transactions on Pattern Analysis and Machine Intelligence. 20207Jia Wan, Qingzhong Wang, and Antoni B Chan. Kernel-based density map generation for dense object counting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. 7\n\nDistribution matching for crowd counting. Boyu Wang, Huidong Liu, Dimitris Samaras, Minh Hoai, Proc. of Advances in Neural Information Processing Systems. of Advances in Neural Information essing SystemsBoyu Wang, Huidong Liu, Dimitris Samaras, and Minh Hoai. Distribution matching for crowd counting. In Proc. of Advances in Neural Information Process- ing Systems, 2020. 7\n\nTransformer meets tracker: Exploiting temporal context for robust visual tracking. Ning Wang, Wengang Zhou, Jie Wang, Houqiang Li, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionNing Wang, Wengang Zhou, Jie Wang, and Houqiang Li. Transformer meets tracker: Exploiting temporal context for robust visual tracking. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recogni- tion, pages 1571-1580, 2021. 3\n\nNwpu-crowd: A large-scale benchmark for crowd counting and localization. Qi Wang, Junyu Gao, Wei Lin, Xuelong Li, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2020Qi Wang, Junyu Gao, Wei Lin, and Xuelong Li. Nwpu-crowd: A large-scale benchmark for crowd counting and localization. IEEE Transactions on Pat- tern Analysis and Machine Intelligence, 2020. 5\n\nLearning from synthetic data for crowd counting in the wild. Qi Wang, Junyu Gao, Wei Lin, Yuan Yuan, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition710Qi Wang, Junyu Gao, Wei Lin, and Yuan Yuan. Learn- ing from synthetic data for crowd counting in the wild. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2019. 2, 3, 6, 7, 10\n\nAutoscale: Learning to scale for crowd counting. Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, Masayoshi Tomizuka, International Journal of Computer Vision. 20222Chenfeng Xu, Dingkang Liang, Yongchao Xu, Song Bai, Wei Zhan, Xiang Bai, and Masayoshi Tomizuka. Autoscale: Learning to scale for crowd counting. In- ternational Journal of Computer Vision, pages 1-30, 2022. 2\n\nLearn to scale: Generating multipolar normalized density map for crowd counting. Chenfeng Xu, Kai Qiu, Jianlong Fu, Song Bai, Yongchao Xu, Xiang Bai, Porc. of IEEE Intl. Conf. on Computer Vision. Chenfeng Xu, Kai Qiu, Jianlong Fu, Song Bai, Yongchao Xu, and Xiang Bai. Learn to scale: Gen- erating multipolar normalized density map for crowd counting. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019. 1\n\nDilated-scale-aware categoryattention convnet for multi-class object counting. Wei Xu, Dingkang Liang, Yixiao Zheng, Jiahao Xie, Zhanyu Ma, IEEE Signal Processing Letters. 282Wei Xu, Dingkang Liang, Yixiao Zheng, Jiahao Xie, and Zhanyu Ma. Dilated-scale-aware category- attention convnet for multi-class object counting. IEEE Signal Processing Letters, 28:1570-1574, 2021. 2\n\nPerspective-guided convolution networks for crowd counting. Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei Wen, Errui Ding, Porc. of IEEE Intl. Conf. on Computer Vision. Zhaoyi Yan, Yuchen Yuan, Wangmeng Zuo, Xiao Tan, Yezhen Wang, Shilei Wen, and Errui Ding. Perspective-guided convolution networks for crowd counting. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019. 6\n\nReverse perspective network for perspective-aware object counting. Yifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming Huang, Nicu Sebe, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionYifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming Huang, and Nicu Sebe. Reverse perspective net- work for perspective-aware object counting. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2020. 3\n\nWeakly-supervised crowd counting learns from sorting rather than locations. Yifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming Huang, Nicu Sebe, Proc. of European Conference on Computer Vision. of European Conference on Computer Vision6Yifan Yang, Guorong Li, Zhe Wu, Li Su, Qingming Huang, and Nicu Sebe. Weakly-supervised crowd counting learns from sorting rather than locations. In Proc. of European Conference on Computer Vision, 2020. 1, 2, 3, 6\n\nAttentional neural fields for crowd counting. Anran Zhang, Lei Yue, Jiayi Shen, Fan Zhu, Xiantong Zhen, Xianbin Cao, Ling Shao, Porc. of IEEE Intl. Conf. on Computer Vision. 23Anran Zhang, Lei Yue, Jiayi Shen, Fan Zhu, Xiantong Zhen, Xianbin Cao, and Ling Shao. Attentional neural fields for crowd counting. In Porc. of IEEE Intl. Conf. on Computer Vision, 2019. 2, 3\n\nCross-scene crowd counting via deep convolutional neural networks. Cong Zhang, Hongsheng Li, Xiaogang Wang, Xiaokang Yang, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern RecognitionCong Zhang, Hongsheng Li, Xiaogang Wang, and Xi- aokang Yang. Cross-scene crowd counting via deep convolutional neural networks. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 833-841, 2015. 5\n\nFcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras. Shanghang Zhang, Guanhang Wu, Joao P Costeira, Moura, Porc. of IEEE Intl. Conf. on Computer Vision. Shanghang Zhang, Guanhang Wu, Joao P Costeira, and Jos\u00e9 MF Moura. Fcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras. In Porc. of IEEE Intl. Conf. on Computer Vision, 2017. 8\n\nSingle-image crowd counting via multi-column convolutional neural network. Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, Yi Ma, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition810Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, 2016. 1, 2, 5, 6, 7, 8, 10\n\nRethinking semantic segmentation from a sequence-to-sequence perspective with transformers. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, H S Philip, Torr, Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition23Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proc. of IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pages 6881-6890, 2021. 2, 3\n\nDeformable detr: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, International Conference on Learning Representations. 2020Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In In- ternational Conference on Learning Representations, 2020. 2\n", "annotations": {"author": "[{\"end\":129,\"start\":66},{\"end\":188,\"start\":130},{\"end\":249,\"start\":189},{\"end\":306,\"start\":250},{\"end\":365,\"start\":307}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":75},{\"end\":139,\"start\":135},{\"end\":195,\"start\":193},{\"end\":257,\"start\":253},{\"end\":316,\"start\":313}]", "author_first_name": "[{\"end\":74,\"start\":66},{\"end\":134,\"start\":130},{\"end\":192,\"start\":189},{\"end\":252,\"start\":250},{\"end\":312,\"start\":307}]", "author_affiliation": "[{\"end\":128,\"start\":82},{\"end\":187,\"start\":141},{\"end\":248,\"start\":197},{\"end\":305,\"start\":259},{\"end\":364,\"start\":318}]", "title": "[{\"end\":63,\"start\":1},{\"end\":428,\"start\":366}]", "venue": null, "abstract": "[{\"end\":2257,\"start\":430}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b67\"},\"end\":2460,\"start\":2456},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2463,\"start\":2460},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2466,\"start\":2463},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2469,\"start\":2466},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2472,\"start\":2469},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":2860,\"start\":2856},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2863,\"start\":2860},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2866,\"start\":2863},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2868,\"start\":2866},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":3038,\"start\":3034},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3041,\"start\":3038},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3044,\"start\":3041},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3046,\"start\":3044},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3167,\"start\":3163},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3195,\"start\":3191},{\"end\":3529,\"start\":3528},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3886,\"start\":3882},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":3889,\"start\":3886},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4559,\"start\":4555},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4673,\"start\":4669},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4688,\"start\":4685},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":4691,\"start\":4688},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":4714,\"start\":4710},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":4794,\"start\":4790},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5246,\"start\":5242},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":5321,\"start\":5317},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":5504,\"start\":5500},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":5669,\"start\":5665},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7840,\"start\":7836},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7843,\"start\":7840},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7960,\"start\":7957},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7963,\"start\":7960},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7966,\"start\":7963},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7969,\"start\":7966},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7971,\"start\":7969},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8298,\"start\":8294},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8301,\"start\":8298},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":8304,\"start\":8301},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8306,\"start\":8304},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8309,\"start\":8306},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8312,\"start\":8309},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":8315,\"start\":8312},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8519,\"start\":8515},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8611,\"start\":8607},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8699,\"start\":8695},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8840,\"start\":8836},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9068,\"start\":9064},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9071,\"start\":9068},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9074,\"start\":9071},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9086,\"start\":9082},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9248,\"start\":9244},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9448,\"start\":9444},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9558,\"start\":9554},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9777,\"start\":9773},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9780,\"start\":9777},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":9783,\"start\":9780},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9801,\"start\":9797},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9918,\"start\":9914},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":10048,\"start\":10044},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":10222,\"start\":10218},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10488,\"start\":10484},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10586,\"start\":10582},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10807,\"start\":10804},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10963,\"start\":10959},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11130,\"start\":11126},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":11449,\"start\":11445},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":12206,\"start\":12202},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12253,\"start\":12249},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12256,\"start\":12253},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12380,\"start\":12377},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12383,\"start\":12380},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":12386,\"start\":12383},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":12389,\"start\":12386},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12391,\"start\":12389},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12469,\"start\":12466},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12620,\"start\":12616},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":12681,\"start\":12677},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":12752,\"start\":12748},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12849,\"start\":12846},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":13037,\"start\":13033},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14184,\"start\":14180},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14197,\"start\":14193},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14798,\"start\":14794},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14840,\"start\":14836},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15292,\"start\":15288},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16380,\"start\":16376},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16393,\"start\":16389},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17810,\"start\":17806},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18007,\"start\":18003},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":18713,\"start\":18709},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19661,\"start\":19657},{\"end\":19665,\"start\":19661},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":20077,\"start\":20073},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20179,\"start\":20175},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":20188,\"start\":20184},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20330,\"start\":20326},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":20965,\"start\":20961},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20978,\"start\":20974},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20988,\"start\":20984},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21410,\"start\":21406},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21579,\"start\":21575},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22231,\"start\":22227},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22244,\"start\":22240},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23673,\"start\":23669},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24079,\"start\":24075},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24096,\"start\":24092},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24463,\"start\":24459},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24501,\"start\":24497},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24568,\"start\":24564},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24608,\"start\":24604},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24964,\"start\":24960},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":25276,\"start\":25272},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25816,\"start\":25812},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25829,\"start\":25825},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29067,\"start\":29063}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28022,\"start\":27633},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28064,\"start\":28023},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28362,\"start\":28065},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28617,\"start\":28363},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28820,\"start\":28618},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30125,\"start\":28821},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31359,\"start\":30126},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31708,\"start\":31360},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33207,\"start\":31709},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33326,\"start\":33208},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":34007,\"start\":33327},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":34388,\"start\":34008},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":34764,\"start\":34389},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":34921,\"start\":34765},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":35819,\"start\":34922},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":36743,\"start\":35820},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":36993,\"start\":36744}]", "paragraph": "[{\"end\":2802,\"start\":2273},{\"end\":3756,\"start\":2804},{\"end\":4531,\"start\":3758},{\"end\":5113,\"start\":4533},{\"end\":6088,\"start\":5115},{\"end\":6725,\"start\":6090},{\"end\":6778,\"start\":6727},{\"end\":7084,\"start\":6780},{\"end\":7359,\"start\":7086},{\"end\":7638,\"start\":7361},{\"end\":7686,\"start\":7656},{\"end\":9689,\"start\":7688},{\"end\":10376,\"start\":9691},{\"end\":10779,\"start\":10414},{\"end\":11583,\"start\":10781},{\"end\":12161,\"start\":11585},{\"end\":12952,\"start\":12185},{\"end\":13059,\"start\":12954},{\"end\":13462,\"start\":13074},{\"end\":13867,\"start\":13484},{\"end\":14300,\"start\":13978},{\"end\":14587,\"start\":14391},{\"end\":14732,\"start\":14678},{\"end\":15132,\"start\":14756},{\"end\":15472,\"start\":15199},{\"end\":15780,\"start\":15667},{\"end\":16041,\"start\":15869},{\"end\":16317,\"start\":16074},{\"end\":16842,\"start\":16319},{\"end\":17451,\"start\":16844},{\"end\":17535,\"start\":17453},{\"end\":17723,\"start\":17570},{\"end\":18686,\"start\":17764},{\"end\":18994,\"start\":18698},{\"end\":19000,\"start\":18996},{\"end\":19125,\"start\":19023},{\"end\":19320,\"start\":19194},{\"end\":19735,\"start\":19332},{\"end\":19911,\"start\":19737},{\"end\":20422,\"start\":19913},{\"end\":22003,\"start\":20424},{\"end\":22914,\"start\":22036},{\"end\":23552,\"start\":22933},{\"end\":23951,\"start\":23575},{\"end\":24785,\"start\":23979},{\"end\":25349,\"start\":24837},{\"end\":26284,\"start\":25351},{\"end\":26316,\"start\":26286},{\"end\":26804,\"start\":26318},{\"end\":27632,\"start\":26819}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13959,\"start\":13868},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14390,\"start\":14301},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14677,\"start\":14588},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15168,\"start\":15133},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15198,\"start\":15168},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15666,\"start\":15473},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15831,\"start\":15781},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15868,\"start\":15831},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17569,\"start\":17536},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19193,\"start\":19126}]", "table_ref": "[{\"end\":19495,\"start\":19488},{\"end\":20072,\"start\":20069}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2271,\"start\":2259},{\"attributes\":{\"n\":\"2.\"},\"end\":7654,\"start\":7641},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10412,\"start\":10379},{\"attributes\":{\"n\":\"2.3.\"},\"end\":12183,\"start\":12164},{\"attributes\":{\"n\":\"3.\"},\"end\":13072,\"start\":13062},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13482,\"start\":13465},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13976,\"start\":13961},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14754,\"start\":14735},{\"attributes\":{\"n\":\"3.4.\"},\"end\":16072,\"start\":16044},{\"attributes\":{\"n\":\"4.\"},\"end\":17737,\"start\":17726},{\"attributes\":{\"n\":\"4.1.\"},\"end\":17762,\"start\":17740},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18696,\"start\":18689},{\"attributes\":{\"n\":\"4.3.\"},\"end\":19021,\"start\":19003},{\"attributes\":{\"n\":\"5.\"},\"end\":19330,\"start\":19323},{\"attributes\":{\"n\":\"6.1.\"},\"end\":22034,\"start\":22006},{\"attributes\":{\"n\":\"6.2.\"},\"end\":22931,\"start\":22917},{\"attributes\":{\"n\":\"6.3.\"},\"end\":23573,\"start\":23555},{\"attributes\":{\"n\":\"6.4.\"},\"end\":23977,\"start\":23954},{\"attributes\":{\"n\":\"6.5.\"},\"end\":24835,\"start\":24788},{\"attributes\":{\"n\":\"7.\"},\"end\":26817,\"start\":26807},{\"end\":27644,\"start\":27634},{\"end\":28076,\"start\":28066},{\"end\":28374,\"start\":28364},{\"end\":28629,\"start\":28619},{\"end\":30136,\"start\":30127},{\"end\":31719,\"start\":31710},{\"end\":33218,\"start\":33209},{\"end\":33337,\"start\":33328},{\"end\":34018,\"start\":34009},{\"end\":34399,\"start\":34390},{\"end\":34775,\"start\":34766},{\"end\":35830,\"start\":35821},{\"end\":36755,\"start\":36745}]", "table": "[{\"end\":30125,\"start\":29579},{\"end\":31359,\"start\":30322},{\"end\":33207,\"start\":31721},{\"end\":33326,\"start\":33277},{\"end\":34007,\"start\":33450},{\"end\":34388,\"start\":34158},{\"end\":34764,\"start\":34401},{\"end\":34921,\"start\":34777},{\"end\":35819,\"start\":35093},{\"end\":36743,\"start\":36407},{\"end\":36993,\"start\":36854}]", "figure_caption": "[{\"end\":28022,\"start\":27646},{\"end\":28064,\"start\":28025},{\"end\":28362,\"start\":28078},{\"end\":28617,\"start\":28376},{\"end\":28820,\"start\":28631},{\"end\":29579,\"start\":28823},{\"end\":30322,\"start\":30138},{\"end\":31708,\"start\":31362},{\"end\":33277,\"start\":33220},{\"end\":33450,\"start\":33339},{\"end\":34158,\"start\":34020},{\"end\":35093,\"start\":34924},{\"end\":36407,\"start\":35832},{\"end\":36854,\"start\":36758}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4366,\"start\":4357},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4530,\"start\":4521},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5112,\"start\":5103},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13222,\"start\":13213},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15089,\"start\":15083},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16500,\"start\":16491},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16966,\"start\":16957},{\"end\":23032,\"start\":23026}]", "bib_author_first_name": "[{\"end\":37135,\"start\":37128},{\"end\":37151,\"start\":37147},{\"end\":37166,\"start\":37158},{\"end\":37180,\"start\":37176},{\"end\":37550,\"start\":37545},{\"end\":37562,\"start\":37556},{\"end\":37569,\"start\":37567},{\"end\":37582,\"start\":37576},{\"end\":37590,\"start\":37587},{\"end\":37601,\"start\":37595},{\"end\":38037,\"start\":38031},{\"end\":38050,\"start\":38043},{\"end\":38063,\"start\":38057},{\"end\":38073,\"start\":38070},{\"end\":38414,\"start\":38407},{\"end\":38432,\"start\":38423},{\"end\":38447,\"start\":38440},{\"end\":38465,\"start\":38458},{\"end\":38484,\"start\":38475},{\"end\":38501,\"start\":38495},{\"end\":38919,\"start\":38918},{\"end\":38944,\"start\":38928},{\"end\":38955,\"start\":38951},{\"end\":39388,\"start\":39381},{\"end\":39400,\"start\":39395},{\"end\":39413,\"start\":39407},{\"end\":39424,\"start\":39419},{\"end\":39435,\"start\":39429},{\"end\":39449,\"start\":39442},{\"end\":39460,\"start\":39455},{\"end\":39473,\"start\":39465},{\"end\":39482,\"start\":39478},{\"end\":39490,\"start\":39487},{\"end\":39953,\"start\":39948},{\"end\":39968,\"start\":39960},{\"end\":39981,\"start\":39976},{\"end\":39995,\"start\":39987},{\"end\":40003,\"start\":40000},{\"end\":40327,\"start\":40322},{\"end\":40339,\"start\":40332},{\"end\":40352,\"start\":40345},{\"end\":40365,\"start\":40358},{\"end\":40382,\"start\":40376},{\"end\":40394,\"start\":40387},{\"end\":40407,\"start\":40401},{\"end\":40417,\"start\":40414},{\"end\":40426,\"start\":40423},{\"end\":40902,\"start\":40897},{\"end\":40911,\"start\":40908},{\"end\":40920,\"start\":40917},{\"end\":40931,\"start\":40927},{\"end\":40944,\"start\":40938},{\"end\":40953,\"start\":40950},{\"end\":41236,\"start\":41231},{\"end\":41244,\"start\":41242},{\"end\":41258,\"start\":41251},{\"end\":41613,\"start\":41610},{\"end\":41622,\"start\":41619},{\"end\":41635,\"start\":41629},{\"end\":41642,\"start\":41640},{\"end\":41650,\"start\":41649},{\"end\":41662,\"start\":41657},{\"end\":41675,\"start\":41668},{\"end\":42038,\"start\":42035},{\"end\":42055,\"start\":42050},{\"end\":42266,\"start\":42260},{\"end\":42280,\"start\":42275},{\"end\":42294,\"start\":42290},{\"end\":42311,\"start\":42304},{\"end\":42755,\"start\":42749},{\"end\":42771,\"start\":42764},{\"end\":42786,\"start\":42780},{\"end\":42799,\"start\":42795},{\"end\":42813,\"start\":42807},{\"end\":42831,\"start\":42826},{\"end\":42848,\"start\":42841},{\"end\":43296,\"start\":43288},{\"end\":43309,\"start\":43304},{\"end\":43324,\"start\":43316},{\"end\":43340,\"start\":43332},{\"end\":43354,\"start\":43347},{\"end\":43365,\"start\":43360},{\"end\":43380,\"start\":43376},{\"end\":43839,\"start\":43831},{\"end\":43849,\"start\":43847},{\"end\":43866,\"start\":43857},{\"end\":43878,\"start\":43871},{\"end\":43889,\"start\":43886},{\"end\":43898,\"start\":43894},{\"end\":43908,\"start\":43905},{\"end\":43921,\"start\":43915},{\"end\":45127,\"start\":45118},{\"end\":45146,\"start\":45140},{\"end\":45164,\"start\":45160},{\"end\":45183,\"start\":45178},{\"end\":45398,\"start\":45390},{\"end\":45413,\"start\":45406},{\"end\":45428,\"start\":45424},{\"end\":45446,\"start\":45439},{\"end\":45713,\"start\":45707},{\"end\":45722,\"start\":45719},{\"end\":45736,\"start\":45728},{\"end\":45752,\"start\":45744},{\"end\":46054,\"start\":46048},{\"end\":46066,\"start\":46059},{\"end\":46080,\"start\":46074},{\"end\":46562,\"start\":46554},{\"end\":46573,\"start\":46570},{\"end\":46586,\"start\":46578},{\"end\":46594,\"start\":46592},{\"end\":46861,\"start\":46858},{\"end\":46875,\"start\":46867},{\"end\":46889,\"start\":46882},{\"end\":46898,\"start\":46894},{\"end\":46911,\"start\":46904},{\"end\":46923,\"start\":46917},{\"end\":46936,\"start\":46930},{\"end\":47452,\"start\":47447},{\"end\":47467,\"start\":47458},{\"end\":47477,\"start\":47473},{\"end\":47493,\"start\":47484},{\"end\":47495,\"start\":47494},{\"end\":47966,\"start\":47961},{\"end\":47975,\"start\":47972},{\"end\":47987,\"start\":47980},{\"end\":48000,\"start\":47993},{\"end\":48014,\"start\":48008},{\"end\":48027,\"start\":48020},{\"end\":48434,\"start\":48428},{\"end\":48446,\"start\":48440},{\"end\":48459,\"start\":48452},{\"end\":48470,\"start\":48464},{\"end\":48481,\"start\":48476},{\"end\":48495,\"start\":48490},{\"end\":48846,\"start\":48842},{\"end\":48860,\"start\":48852},{\"end\":48876,\"start\":48867},{\"end\":48885,\"start\":48882},{\"end\":48893,\"start\":48891},{\"end\":48905,\"start\":48899},{\"end\":49301,\"start\":49298},{\"end\":49315,\"start\":49307},{\"end\":49333,\"start\":49326},{\"end\":49350,\"start\":49341},{\"end\":49503,\"start\":49502},{\"end\":49762,\"start\":49756},{\"end\":49775,\"start\":49768},{\"end\":49792,\"start\":49786},{\"end\":50169,\"start\":50163},{\"end\":50180,\"start\":50175},{\"end\":50195,\"start\":50189},{\"end\":50197,\"start\":50196},{\"end\":50502,\"start\":50496},{\"end\":50512,\"start\":50508},{\"end\":50523,\"start\":50518},{\"end\":50538,\"start\":50531},{\"end\":50549,\"start\":50543},{\"end\":50562,\"start\":50557},{\"end\":50573,\"start\":50569},{\"end\":50584,\"start\":50580},{\"end\":50596,\"start\":50592},{\"end\":50617,\"start\":50610},{\"end\":51010,\"start\":51004},{\"end\":51024,\"start\":51016},{\"end\":51035,\"start\":51030},{\"end\":51050,\"start\":51042},{\"end\":51462,\"start\":51456},{\"end\":51475,\"start\":51468},{\"end\":51485,\"start\":51480},{\"end\":51499,\"start\":51492},{\"end\":51512,\"start\":51506},{\"end\":51526,\"start\":51519},{\"end\":51541,\"start\":51534},{\"end\":51556,\"start\":51549},{\"end\":51565,\"start\":51562},{\"end\":51582,\"start\":51576},{\"end\":51997,\"start\":51990},{\"end\":52006,\"start\":52002},{\"end\":52020,\"start\":52012},{\"end\":52033,\"start\":52027},{\"end\":52368,\"start\":52361},{\"end\":52377,\"start\":52373},{\"end\":52391,\"start\":52383},{\"end\":52404,\"start\":52398},{\"end\":52706,\"start\":52699},{\"end\":52715,\"start\":52711},{\"end\":52729,\"start\":52721},{\"end\":52739,\"start\":52736},{\"end\":52752,\"start\":52745},{\"end\":52764,\"start\":52758},{\"end\":53126,\"start\":53122},{\"end\":53146,\"start\":53142},{\"end\":53460,\"start\":53453},{\"end\":53479,\"start\":53475},{\"end\":53488,\"start\":53484},{\"end\":53955,\"start\":53950},{\"end\":53981,\"start\":53973},{\"end\":54447,\"start\":54442},{\"end\":54459,\"start\":54455},{\"end\":54472,\"start\":54466},{\"end\":54487,\"start\":54479},{\"end\":54783,\"start\":54775},{\"end\":54796,\"start\":54789},{\"end\":54807,\"start\":54803},{\"end\":54817,\"start\":54812},{\"end\":55195,\"start\":55188},{\"end\":55207,\"start\":55201},{\"end\":55217,\"start\":55216},{\"end\":55219,\"start\":55218},{\"end\":55501,\"start\":55494},{\"end\":55509,\"start\":55507},{\"end\":55520,\"start\":55517},{\"end\":55534,\"start\":55526},{\"end\":55548,\"start\":55540},{\"end\":55562,\"start\":55553},{\"end\":55576,\"start\":55570},{\"end\":56035,\"start\":56025},{\"end\":56051,\"start\":56045},{\"end\":56446,\"start\":56445},{\"end\":56469,\"start\":56468},{\"end\":56937,\"start\":56936},{\"end\":56960,\"start\":56959},{\"end\":57277,\"start\":57276},{\"end\":57300,\"start\":57299},{\"end\":57572,\"start\":57566},{\"end\":57586,\"start\":57582},{\"end\":57600,\"start\":57596},{\"end\":57614,\"start\":57609},{\"end\":57631,\"start\":57626},{\"end\":57644,\"start\":57639},{\"end\":57646,\"start\":57645},{\"end\":57660,\"start\":57654},{\"end\":57674,\"start\":57669},{\"end\":58091,\"start\":58086},{\"end\":58120,\"start\":58114},{\"end\":58141,\"start\":58140},{\"end\":58156,\"start\":58151},{\"end\":58166,\"start\":58162},{\"end\":58168,\"start\":58167},{\"end\":58590,\"start\":58587},{\"end\":58602,\"start\":58596},{\"end\":58860,\"start\":58857},{\"end\":58872,\"start\":58866},{\"end\":58885,\"start\":58878},{\"end\":58896,\"start\":58890},{\"end\":58898,\"start\":58897},{\"end\":58908,\"start\":58905},{\"end\":59316,\"start\":59313},{\"end\":59331,\"start\":59322},{\"end\":59344,\"start\":59338},{\"end\":59346,\"start\":59345},{\"end\":59648,\"start\":59644},{\"end\":59662,\"start\":59655},{\"end\":59676,\"start\":59668},{\"end\":59690,\"start\":59686},{\"end\":60065,\"start\":60061},{\"end\":60079,\"start\":60072},{\"end\":60089,\"start\":60086},{\"end\":60104,\"start\":60096},{\"end\":60552,\"start\":60550},{\"end\":60564,\"start\":60559},{\"end\":60573,\"start\":60570},{\"end\":60586,\"start\":60579},{\"end\":60915,\"start\":60913},{\"end\":60927,\"start\":60922},{\"end\":60936,\"start\":60933},{\"end\":60946,\"start\":60942},{\"end\":61347,\"start\":61339},{\"end\":61360,\"start\":61352},{\"end\":61376,\"start\":61368},{\"end\":61385,\"start\":61381},{\"end\":61394,\"start\":61391},{\"end\":61406,\"start\":61401},{\"end\":61421,\"start\":61412},{\"end\":61779,\"start\":61771},{\"end\":61787,\"start\":61784},{\"end\":61801,\"start\":61793},{\"end\":61810,\"start\":61806},{\"end\":61824,\"start\":61816},{\"end\":61834,\"start\":61829},{\"end\":62182,\"start\":62179},{\"end\":62195,\"start\":62187},{\"end\":62209,\"start\":62203},{\"end\":62223,\"start\":62217},{\"end\":62235,\"start\":62229},{\"end\":62542,\"start\":62536},{\"end\":62554,\"start\":62548},{\"end\":62569,\"start\":62561},{\"end\":62579,\"start\":62575},{\"end\":62591,\"start\":62585},{\"end\":62604,\"start\":62598},{\"end\":62615,\"start\":62610},{\"end\":62948,\"start\":62943},{\"end\":62962,\"start\":62955},{\"end\":62970,\"start\":62967},{\"end\":62977,\"start\":62975},{\"end\":62990,\"start\":62982},{\"end\":63002,\"start\":62998},{\"end\":63443,\"start\":63438},{\"end\":63457,\"start\":63450},{\"end\":63465,\"start\":63462},{\"end\":63472,\"start\":63470},{\"end\":63485,\"start\":63477},{\"end\":63497,\"start\":63493},{\"end\":63862,\"start\":63857},{\"end\":63873,\"start\":63870},{\"end\":63884,\"start\":63879},{\"end\":63894,\"start\":63891},{\"end\":63908,\"start\":63900},{\"end\":63922,\"start\":63915},{\"end\":63932,\"start\":63928},{\"end\":64251,\"start\":64247},{\"end\":64268,\"start\":64259},{\"end\":64281,\"start\":64273},{\"end\":64296,\"start\":64288},{\"end\":64756,\"start\":64747},{\"end\":64772,\"start\":64764},{\"end\":64781,\"start\":64777},{\"end\":64783,\"start\":64782},{\"end\":65140,\"start\":65132},{\"end\":65153,\"start\":65148},{\"end\":65165,\"start\":65160},{\"end\":65180,\"start\":65172},{\"end\":65188,\"start\":65186},{\"end\":65667,\"start\":65661},{\"end\":65682,\"start\":65675},{\"end\":65697,\"start\":65687},{\"end\":65711,\"start\":65704},{\"end\":65722,\"start\":65717},{\"end\":65734,\"start\":65728},{\"end\":65747,\"start\":65741},{\"end\":65760,\"start\":65752},{\"end\":65770,\"start\":65767},{\"end\":65779,\"start\":65778},{\"end\":65781,\"start\":65780},{\"end\":66344,\"start\":66338},{\"end\":66356,\"start\":66350},{\"end\":66366,\"start\":66361},{\"end\":66374,\"start\":66371},{\"end\":66387,\"start\":66379},{\"end\":66400,\"start\":66394}]", "bib_author_last_name": "[{\"end\":37145,\"start\":37136},{\"end\":37156,\"start\":37152},{\"end\":37174,\"start\":37167},{\"end\":37185,\"start\":37181},{\"end\":37554,\"start\":37551},{\"end\":37565,\"start\":37563},{\"end\":37574,\"start\":37570},{\"end\":37585,\"start\":37583},{\"end\":37593,\"start\":37591},{\"end\":37605,\"start\":37602},{\"end\":38041,\"start\":38038},{\"end\":38055,\"start\":38051},{\"end\":38068,\"start\":38064},{\"end\":38076,\"start\":38074},{\"end\":38421,\"start\":38415},{\"end\":38438,\"start\":38433},{\"end\":38456,\"start\":38448},{\"end\":38473,\"start\":38466},{\"end\":38493,\"start\":38485},{\"end\":38511,\"start\":38502},{\"end\":38926,\"start\":38920},{\"end\":38949,\"start\":38945},{\"end\":38961,\"start\":38956},{\"end\":38974,\"start\":38963},{\"end\":39393,\"start\":39389},{\"end\":39405,\"start\":39401},{\"end\":39417,\"start\":39414},{\"end\":39427,\"start\":39425},{\"end\":39440,\"start\":39436},{\"end\":39453,\"start\":39450},{\"end\":39463,\"start\":39461},{\"end\":39476,\"start\":39474},{\"end\":39485,\"start\":39483},{\"end\":39494,\"start\":39491},{\"end\":39958,\"start\":39954},{\"end\":39974,\"start\":39969},{\"end\":39985,\"start\":39982},{\"end\":39998,\"start\":39996},{\"end\":40008,\"start\":40004},{\"end\":40330,\"start\":40328},{\"end\":40343,\"start\":40340},{\"end\":40356,\"start\":40353},{\"end\":40374,\"start\":40366},{\"end\":40385,\"start\":40383},{\"end\":40399,\"start\":40395},{\"end\":40412,\"start\":40408},{\"end\":40421,\"start\":40418},{\"end\":40433,\"start\":40427},{\"end\":40442,\"start\":40435},{\"end\":40906,\"start\":40903},{\"end\":40915,\"start\":40912},{\"end\":40925,\"start\":40921},{\"end\":40936,\"start\":40932},{\"end\":40948,\"start\":40945},{\"end\":40957,\"start\":40954},{\"end\":41240,\"start\":41237},{\"end\":41249,\"start\":41245},{\"end\":41261,\"start\":41259},{\"end\":41617,\"start\":41614},{\"end\":41627,\"start\":41623},{\"end\":41638,\"start\":41636},{\"end\":41647,\"start\":41643},{\"end\":41655,\"start\":41651},{\"end\":41666,\"start\":41663},{\"end\":41681,\"start\":41676},{\"end\":41687,\"start\":41683},{\"end\":42048,\"start\":42039},{\"end\":42062,\"start\":42056},{\"end\":42273,\"start\":42267},{\"end\":42288,\"start\":42281},{\"end\":42302,\"start\":42295},{\"end\":42316,\"start\":42312},{\"end\":42762,\"start\":42756},{\"end\":42778,\"start\":42772},{\"end\":42793,\"start\":42787},{\"end\":42805,\"start\":42800},{\"end\":42824,\"start\":42814},{\"end\":42839,\"start\":42832},{\"end\":42853,\"start\":42849},{\"end\":43302,\"start\":43297},{\"end\":43314,\"start\":43310},{\"end\":43330,\"start\":43325},{\"end\":43345,\"start\":43341},{\"end\":43358,\"start\":43355},{\"end\":43374,\"start\":43366},{\"end\":43385,\"start\":43381},{\"end\":43845,\"start\":43840},{\"end\":43855,\"start\":43850},{\"end\":43869,\"start\":43867},{\"end\":43884,\"start\":43879},{\"end\":43892,\"start\":43890},{\"end\":43903,\"start\":43899},{\"end\":43913,\"start\":43909},{\"end\":43926,\"start\":43922},{\"end\":44789,\"start\":44780},{\"end\":44793,\"start\":44791},{\"end\":45138,\"start\":45128},{\"end\":45158,\"start\":45147},{\"end\":45176,\"start\":45165},{\"end\":45191,\"start\":45184},{\"end\":45404,\"start\":45399},{\"end\":45422,\"start\":45414},{\"end\":45437,\"start\":45429},{\"end\":45454,\"start\":45447},{\"end\":45461,\"start\":45456},{\"end\":45717,\"start\":45714},{\"end\":45726,\"start\":45723},{\"end\":45742,\"start\":45737},{\"end\":45756,\"start\":45753},{\"end\":46057,\"start\":46055},{\"end\":46072,\"start\":46067},{\"end\":46085,\"start\":46081},{\"end\":46568,\"start\":46563},{\"end\":46576,\"start\":46574},{\"end\":46590,\"start\":46587},{\"end\":46599,\"start\":46595},{\"end\":46865,\"start\":46862},{\"end\":46880,\"start\":46876},{\"end\":46892,\"start\":46890},{\"end\":46902,\"start\":46899},{\"end\":46915,\"start\":46912},{\"end\":46928,\"start\":46924},{\"end\":46941,\"start\":46937},{\"end\":47456,\"start\":47453},{\"end\":47471,\"start\":47468},{\"end\":47482,\"start\":47478},{\"end\":47505,\"start\":47496},{\"end\":47970,\"start\":47967},{\"end\":47978,\"start\":47976},{\"end\":47991,\"start\":47988},{\"end\":48006,\"start\":48001},{\"end\":48018,\"start\":48015},{\"end\":48032,\"start\":48028},{\"end\":48438,\"start\":48435},{\"end\":48450,\"start\":48447},{\"end\":48462,\"start\":48460},{\"end\":48474,\"start\":48471},{\"end\":48488,\"start\":48482},{\"end\":48499,\"start\":48496},{\"end\":48850,\"start\":48847},{\"end\":48865,\"start\":48861},{\"end\":48880,\"start\":48877},{\"end\":48889,\"start\":48886},{\"end\":48897,\"start\":48894},{\"end\":48908,\"start\":48906},{\"end\":49305,\"start\":49302},{\"end\":49324,\"start\":49316},{\"end\":49339,\"start\":49334},{\"end\":49358,\"start\":49351},{\"end\":49513,\"start\":49504},{\"end\":49519,\"start\":49515},{\"end\":49766,\"start\":49763},{\"end\":49784,\"start\":49776},{\"end\":49796,\"start\":49793},{\"end\":50173,\"start\":50170},{\"end\":50187,\"start\":50181},{\"end\":50204,\"start\":50198},{\"end\":50214,\"start\":50206},{\"end\":50506,\"start\":50503},{\"end\":50516,\"start\":50513},{\"end\":50529,\"start\":50524},{\"end\":50541,\"start\":50539},{\"end\":50555,\"start\":50550},{\"end\":50567,\"start\":50563},{\"end\":50578,\"start\":50574},{\"end\":50590,\"start\":50585},{\"end\":50608,\"start\":50597},{\"end\":50626,\"start\":50618},{\"end\":50635,\"start\":50628},{\"end\":51014,\"start\":51011},{\"end\":51028,\"start\":51025},{\"end\":51040,\"start\":51036},{\"end\":51055,\"start\":51051},{\"end\":51466,\"start\":51463},{\"end\":51478,\"start\":51476},{\"end\":51490,\"start\":51486},{\"end\":51504,\"start\":51500},{\"end\":51517,\"start\":51513},{\"end\":51532,\"start\":51527},{\"end\":51547,\"start\":51542},{\"end\":51560,\"start\":51557},{\"end\":51574,\"start\":51566},{\"end\":51586,\"start\":51583},{\"end\":52000,\"start\":51998},{\"end\":52010,\"start\":52007},{\"end\":52025,\"start\":52021},{\"end\":52038,\"start\":52034},{\"end\":52371,\"start\":52369},{\"end\":52381,\"start\":52378},{\"end\":52396,\"start\":52392},{\"end\":52409,\"start\":52405},{\"end\":52709,\"start\":52707},{\"end\":52719,\"start\":52716},{\"end\":52734,\"start\":52730},{\"end\":52743,\"start\":52740},{\"end\":52756,\"start\":52753},{\"end\":52769,\"start\":52765},{\"end\":53140,\"start\":53127},{\"end\":53149,\"start\":53147},{\"end\":53155,\"start\":53151},{\"end\":53473,\"start\":53461},{\"end\":53482,\"start\":53480},{\"end\":53497,\"start\":53489},{\"end\":53502,\"start\":53499},{\"end\":53971,\"start\":53956},{\"end\":53997,\"start\":53982},{\"end\":54021,\"start\":53999},{\"end\":54453,\"start\":54448},{\"end\":54464,\"start\":54460},{\"end\":54477,\"start\":54473},{\"end\":54491,\"start\":54488},{\"end\":54787,\"start\":54784},{\"end\":54801,\"start\":54797},{\"end\":54810,\"start\":54808},{\"end\":54822,\"start\":54818},{\"end\":55199,\"start\":55196},{\"end\":55214,\"start\":55208},{\"end\":55224,\"start\":55220},{\"end\":55231,\"start\":55226},{\"end\":55505,\"start\":55502},{\"end\":55515,\"start\":55510},{\"end\":55524,\"start\":55521},{\"end\":55538,\"start\":55535},{\"end\":55551,\"start\":55549},{\"end\":55568,\"start\":55563},{\"end\":55582,\"start\":55577},{\"end\":56043,\"start\":56036},{\"end\":56059,\"start\":56052},{\"end\":56066,\"start\":56061},{\"end\":56457,\"start\":56447},{\"end\":56466,\"start\":56459},{\"end\":56476,\"start\":56470},{\"end\":56483,\"start\":56478},{\"end\":56948,\"start\":56938},{\"end\":56957,\"start\":56950},{\"end\":56967,\"start\":56961},{\"end\":56974,\"start\":56969},{\"end\":57288,\"start\":57278},{\"end\":57297,\"start\":57290},{\"end\":57307,\"start\":57301},{\"end\":57314,\"start\":57309},{\"end\":57580,\"start\":57573},{\"end\":57594,\"start\":57587},{\"end\":57607,\"start\":57601},{\"end\":57624,\"start\":57615},{\"end\":57637,\"start\":57632},{\"end\":57652,\"start\":57647},{\"end\":57667,\"start\":57661},{\"end\":57685,\"start\":57675},{\"end\":58112,\"start\":58092},{\"end\":58129,\"start\":58121},{\"end\":58138,\"start\":58131},{\"end\":58149,\"start\":58142},{\"end\":58160,\"start\":58157},{\"end\":58177,\"start\":58169},{\"end\":58188,\"start\":58179},{\"end\":58594,\"start\":58591},{\"end\":58607,\"start\":58603},{\"end\":58864,\"start\":58861},{\"end\":58876,\"start\":58873},{\"end\":58888,\"start\":58886},{\"end\":58903,\"start\":58899},{\"end\":58912,\"start\":58909},{\"end\":59320,\"start\":59317},{\"end\":59336,\"start\":59332},{\"end\":59351,\"start\":59347},{\"end\":59653,\"start\":59649},{\"end\":59666,\"start\":59663},{\"end\":59684,\"start\":59677},{\"end\":59695,\"start\":59691},{\"end\":60070,\"start\":60066},{\"end\":60084,\"start\":60080},{\"end\":60094,\"start\":60090},{\"end\":60107,\"start\":60105},{\"end\":60557,\"start\":60553},{\"end\":60568,\"start\":60565},{\"end\":60577,\"start\":60574},{\"end\":60589,\"start\":60587},{\"end\":60920,\"start\":60916},{\"end\":60931,\"start\":60928},{\"end\":60940,\"start\":60937},{\"end\":60951,\"start\":60947},{\"end\":61350,\"start\":61348},{\"end\":61366,\"start\":61361},{\"end\":61379,\"start\":61377},{\"end\":61389,\"start\":61386},{\"end\":61399,\"start\":61395},{\"end\":61410,\"start\":61407},{\"end\":61430,\"start\":61422},{\"end\":61782,\"start\":61780},{\"end\":61791,\"start\":61788},{\"end\":61804,\"start\":61802},{\"end\":61814,\"start\":61811},{\"end\":61827,\"start\":61825},{\"end\":61838,\"start\":61835},{\"end\":62185,\"start\":62183},{\"end\":62201,\"start\":62196},{\"end\":62215,\"start\":62210},{\"end\":62227,\"start\":62224},{\"end\":62238,\"start\":62236},{\"end\":62546,\"start\":62543},{\"end\":62559,\"start\":62555},{\"end\":62573,\"start\":62570},{\"end\":62583,\"start\":62580},{\"end\":62596,\"start\":62592},{\"end\":62608,\"start\":62605},{\"end\":62620,\"start\":62616},{\"end\":62953,\"start\":62949},{\"end\":62965,\"start\":62963},{\"end\":62973,\"start\":62971},{\"end\":62980,\"start\":62978},{\"end\":62996,\"start\":62991},{\"end\":63007,\"start\":63003},{\"end\":63448,\"start\":63444},{\"end\":63460,\"start\":63458},{\"end\":63468,\"start\":63466},{\"end\":63475,\"start\":63473},{\"end\":63491,\"start\":63486},{\"end\":63502,\"start\":63498},{\"end\":63868,\"start\":63863},{\"end\":63877,\"start\":63874},{\"end\":63889,\"start\":63885},{\"end\":63898,\"start\":63895},{\"end\":63913,\"start\":63909},{\"end\":63926,\"start\":63923},{\"end\":63937,\"start\":63933},{\"end\":64257,\"start\":64252},{\"end\":64271,\"start\":64269},{\"end\":64286,\"start\":64282},{\"end\":64301,\"start\":64297},{\"end\":64762,\"start\":64757},{\"end\":64775,\"start\":64773},{\"end\":64792,\"start\":64784},{\"end\":64799,\"start\":64794},{\"end\":65146,\"start\":65141},{\"end\":65158,\"start\":65154},{\"end\":65170,\"start\":65166},{\"end\":65184,\"start\":65181},{\"end\":65191,\"start\":65189},{\"end\":65673,\"start\":65668},{\"end\":65685,\"start\":65683},{\"end\":65702,\"start\":65698},{\"end\":65715,\"start\":65712},{\"end\":65726,\"start\":65723},{\"end\":65739,\"start\":65735},{\"end\":65750,\"start\":65748},{\"end\":65765,\"start\":65761},{\"end\":65776,\"start\":65771},{\"end\":65788,\"start\":65782},{\"end\":65794,\"start\":65790},{\"end\":66348,\"start\":66345},{\"end\":66359,\"start\":66357},{\"end\":66369,\"start\":66367},{\"end\":66377,\"start\":66375},{\"end\":66392,\"start\":66388},{\"end\":66404,\"start\":66401}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":229363720},\"end\":37472,\"start\":37072},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":219963160},\"end\":37960,\"start\":37474},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52955811},\"end\":38358,\"start\":37962},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218889832},\"end\":38828,\"start\":38360},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9059102},\"end\":39337,\"start\":38830},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":227239228},\"end\":39888,\"start\":39339},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":237216601},\"end\":40246,\"start\":39890},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":230795049},\"end\":40832,\"start\":40248},{\"attributes\":{\"doi\":\"arXiv:1907.02724\",\"id\":\"b8\"},\"end\":41158,\"start\":40834},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":165163975},\"end\":41514,\"start\":41160},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":202606496},\"end\":41947,\"start\":41516},{\"attributes\":{\"id\":\"b11\"},\"end\":42191,\"start\":41949},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9749221},\"end\":42659,\"start\":42193},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":51901514},\"end\":43211,\"start\":42661},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":67856061},\"end\":43791,\"start\":43213},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219964549},\"end\":44287,\"start\":43793},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52967399},\"end\":44677,\"start\":44289},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6628106},\"end\":45114,\"start\":44679},{\"attributes\":{\"id\":\"b18\"},\"end\":45312,\"start\":45116},{\"attributes\":{\"id\":\"b19\"},\"end\":45642,\"start\":45314},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":211677656},\"end\":45952,\"start\":45644},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3645757},\"end\":46462,\"start\":45954},{\"attributes\":{\"doi\":\"arXiv:2102.07925\",\"id\":\"b22\"},\"end\":46812,\"start\":46464},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":235731559},\"end\":47343,\"start\":46814},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3740753},\"end\":47889,\"start\":47345},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":220546532},\"end\":48363,\"start\":47891},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":201645306},\"end\":48748,\"start\":48365},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":53957027},\"end\":49294,\"start\":48750},{\"attributes\":{\"id\":\"b28\"},\"end\":49464,\"start\":49296},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2141740},\"end\":49724,\"start\":49466},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":53783843},\"end\":50091,\"start\":49726},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":62841646},\"end\":50494,\"start\":50093},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b32\"},\"end\":50948,\"start\":50496},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":91184269},\"end\":51380,\"start\":50950},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":244530934},\"end\":51923,\"start\":51382},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":199543622},\"end\":52276,\"start\":51925},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":222278438},\"end\":52645,\"start\":52278},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":235306179},\"end\":53094,\"start\":52647},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":50785934},\"end\":53371,\"start\":53096},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":10328909},\"end\":53817,\"start\":53373},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":189999733},\"end\":54369,\"start\":53819},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":17249259},\"end\":54708,\"start\":54371},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":54461738},\"end\":55156,\"start\":54710},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":88516263},\"end\":55436,\"start\":55158},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":52243494},\"end\":55951,\"start\":55438},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":215415929},\"end\":56341,\"start\":55953},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":3003101},\"end\":56860,\"start\":56343},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2099022},\"end\":57201,\"start\":56862},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":201668945},\"end\":57537,\"start\":57203},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":13756489},\"end\":58027,\"start\":57539},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":37733756},\"end\":58538,\"start\":58029},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":227275245},\"end\":58795,\"start\":58540},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":197639806},\"end\":59248,\"start\":58797},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":221625392},\"end\":59600,\"start\":59250},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":221970845},\"end\":59976,\"start\":59602},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":232307380},\"end\":60475,\"start\":59978},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":210157261},\"end\":60850,\"start\":60477},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":72941021},\"end\":61288,\"start\":60852},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":209439830},\"end\":61688,\"start\":61290},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":198968173},\"end\":62098,\"start\":61690},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":229181111},\"end\":62474,\"start\":62100},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":202577232},\"end\":62874,\"start\":62476},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":219963357},\"end\":63360,\"start\":62876},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":221408358},\"end\":63809,\"start\":63362},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":207926635},\"end\":64178,\"start\":63811},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":2131202},\"end\":64659,\"start\":64180},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":10553647},\"end\":65055,\"start\":64661},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":4545310},\"end\":65567,\"start\":65057},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":229924195},\"end\":66262,\"start\":65569},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":222208633},\"end\":66676,\"start\":66264}]", "bib_title": "[{\"end\":37126,\"start\":37072},{\"end\":37543,\"start\":37474},{\"end\":38029,\"start\":37962},{\"end\":38405,\"start\":38360},{\"end\":38916,\"start\":38830},{\"end\":39379,\"start\":39339},{\"end\":39946,\"start\":39890},{\"end\":40320,\"start\":40248},{\"end\":41229,\"start\":41160},{\"end\":41608,\"start\":41516},{\"end\":42258,\"start\":42193},{\"end\":42747,\"start\":42661},{\"end\":43286,\"start\":43213},{\"end\":43829,\"start\":43793},{\"end\":44369,\"start\":44289},{\"end\":44778,\"start\":44679},{\"end\":45705,\"start\":45644},{\"end\":46046,\"start\":45954},{\"end\":46856,\"start\":46814},{\"end\":47445,\"start\":47345},{\"end\":47959,\"start\":47891},{\"end\":48426,\"start\":48365},{\"end\":48840,\"start\":48750},{\"end\":49500,\"start\":49466},{\"end\":49754,\"start\":49726},{\"end\":50161,\"start\":50093},{\"end\":51002,\"start\":50950},{\"end\":51454,\"start\":51382},{\"end\":51988,\"start\":51925},{\"end\":52359,\"start\":52278},{\"end\":52697,\"start\":52647},{\"end\":53120,\"start\":53096},{\"end\":53451,\"start\":53373},{\"end\":53948,\"start\":53819},{\"end\":54440,\"start\":54371},{\"end\":54773,\"start\":54710},{\"end\":55186,\"start\":55158},{\"end\":55492,\"start\":55438},{\"end\":56023,\"start\":55953},{\"end\":56443,\"start\":56343},{\"end\":56934,\"start\":56862},{\"end\":57274,\"start\":57203},{\"end\":57564,\"start\":57539},{\"end\":58084,\"start\":58029},{\"end\":58585,\"start\":58540},{\"end\":58855,\"start\":58797},{\"end\":59311,\"start\":59250},{\"end\":59642,\"start\":59602},{\"end\":60059,\"start\":59978},{\"end\":60548,\"start\":60477},{\"end\":60911,\"start\":60852},{\"end\":61337,\"start\":61290},{\"end\":61769,\"start\":61690},{\"end\":62177,\"start\":62100},{\"end\":62534,\"start\":62476},{\"end\":62941,\"start\":62876},{\"end\":63436,\"start\":63362},{\"end\":63855,\"start\":63811},{\"end\":64245,\"start\":64180},{\"end\":64745,\"start\":64661},{\"end\":65130,\"start\":65057},{\"end\":65659,\"start\":65569},{\"end\":66336,\"start\":66264}]", "bib_author": "[{\"end\":37147,\"start\":37128},{\"end\":37158,\"start\":37147},{\"end\":37176,\"start\":37158},{\"end\":37187,\"start\":37176},{\"end\":37556,\"start\":37545},{\"end\":37567,\"start\":37556},{\"end\":37576,\"start\":37567},{\"end\":37587,\"start\":37576},{\"end\":37595,\"start\":37587},{\"end\":37607,\"start\":37595},{\"end\":38043,\"start\":38031},{\"end\":38057,\"start\":38043},{\"end\":38070,\"start\":38057},{\"end\":38078,\"start\":38070},{\"end\":38423,\"start\":38407},{\"end\":38440,\"start\":38423},{\"end\":38458,\"start\":38440},{\"end\":38475,\"start\":38458},{\"end\":38495,\"start\":38475},{\"end\":38513,\"start\":38495},{\"end\":38928,\"start\":38918},{\"end\":38951,\"start\":38928},{\"end\":38963,\"start\":38951},{\"end\":38976,\"start\":38963},{\"end\":39395,\"start\":39381},{\"end\":39407,\"start\":39395},{\"end\":39419,\"start\":39407},{\"end\":39429,\"start\":39419},{\"end\":39442,\"start\":39429},{\"end\":39455,\"start\":39442},{\"end\":39465,\"start\":39455},{\"end\":39478,\"start\":39465},{\"end\":39487,\"start\":39478},{\"end\":39496,\"start\":39487},{\"end\":39960,\"start\":39948},{\"end\":39976,\"start\":39960},{\"end\":39987,\"start\":39976},{\"end\":40000,\"start\":39987},{\"end\":40010,\"start\":40000},{\"end\":40332,\"start\":40322},{\"end\":40345,\"start\":40332},{\"end\":40358,\"start\":40345},{\"end\":40376,\"start\":40358},{\"end\":40387,\"start\":40376},{\"end\":40401,\"start\":40387},{\"end\":40414,\"start\":40401},{\"end\":40423,\"start\":40414},{\"end\":40435,\"start\":40423},{\"end\":40444,\"start\":40435},{\"end\":40908,\"start\":40897},{\"end\":40917,\"start\":40908},{\"end\":40927,\"start\":40917},{\"end\":40938,\"start\":40927},{\"end\":40950,\"start\":40938},{\"end\":40959,\"start\":40950},{\"end\":41242,\"start\":41231},{\"end\":41251,\"start\":41242},{\"end\":41263,\"start\":41251},{\"end\":41619,\"start\":41610},{\"end\":41629,\"start\":41619},{\"end\":41640,\"start\":41629},{\"end\":41649,\"start\":41640},{\"end\":41657,\"start\":41649},{\"end\":41668,\"start\":41657},{\"end\":41683,\"start\":41668},{\"end\":41689,\"start\":41683},{\"end\":42050,\"start\":42035},{\"end\":42064,\"start\":42050},{\"end\":42275,\"start\":42260},{\"end\":42290,\"start\":42275},{\"end\":42304,\"start\":42290},{\"end\":42318,\"start\":42304},{\"end\":42764,\"start\":42749},{\"end\":42780,\"start\":42764},{\"end\":42795,\"start\":42780},{\"end\":42807,\"start\":42795},{\"end\":42826,\"start\":42807},{\"end\":42841,\"start\":42826},{\"end\":42855,\"start\":42841},{\"end\":43304,\"start\":43288},{\"end\":43316,\"start\":43304},{\"end\":43332,\"start\":43316},{\"end\":43347,\"start\":43332},{\"end\":43360,\"start\":43347},{\"end\":43376,\"start\":43360},{\"end\":43387,\"start\":43376},{\"end\":43847,\"start\":43831},{\"end\":43857,\"start\":43847},{\"end\":43871,\"start\":43857},{\"end\":43886,\"start\":43871},{\"end\":43894,\"start\":43886},{\"end\":43905,\"start\":43894},{\"end\":43915,\"start\":43905},{\"end\":43928,\"start\":43915},{\"end\":44791,\"start\":44780},{\"end\":44795,\"start\":44791},{\"end\":45140,\"start\":45118},{\"end\":45160,\"start\":45140},{\"end\":45178,\"start\":45160},{\"end\":45193,\"start\":45178},{\"end\":45406,\"start\":45390},{\"end\":45424,\"start\":45406},{\"end\":45439,\"start\":45424},{\"end\":45456,\"start\":45439},{\"end\":45463,\"start\":45456},{\"end\":45719,\"start\":45707},{\"end\":45728,\"start\":45719},{\"end\":45744,\"start\":45728},{\"end\":45758,\"start\":45744},{\"end\":46059,\"start\":46048},{\"end\":46074,\"start\":46059},{\"end\":46087,\"start\":46074},{\"end\":46570,\"start\":46554},{\"end\":46578,\"start\":46570},{\"end\":46592,\"start\":46578},{\"end\":46601,\"start\":46592},{\"end\":46867,\"start\":46858},{\"end\":46882,\"start\":46867},{\"end\":46894,\"start\":46882},{\"end\":46904,\"start\":46894},{\"end\":46917,\"start\":46904},{\"end\":46930,\"start\":46917},{\"end\":46943,\"start\":46930},{\"end\":47458,\"start\":47447},{\"end\":47473,\"start\":47458},{\"end\":47484,\"start\":47473},{\"end\":47507,\"start\":47484},{\"end\":47972,\"start\":47961},{\"end\":47980,\"start\":47972},{\"end\":47993,\"start\":47980},{\"end\":48008,\"start\":47993},{\"end\":48020,\"start\":48008},{\"end\":48034,\"start\":48020},{\"end\":48440,\"start\":48428},{\"end\":48452,\"start\":48440},{\"end\":48464,\"start\":48452},{\"end\":48476,\"start\":48464},{\"end\":48490,\"start\":48476},{\"end\":48501,\"start\":48490},{\"end\":48852,\"start\":48842},{\"end\":48867,\"start\":48852},{\"end\":48882,\"start\":48867},{\"end\":48891,\"start\":48882},{\"end\":48899,\"start\":48891},{\"end\":48910,\"start\":48899},{\"end\":49307,\"start\":49298},{\"end\":49326,\"start\":49307},{\"end\":49341,\"start\":49326},{\"end\":49360,\"start\":49341},{\"end\":49515,\"start\":49502},{\"end\":49521,\"start\":49515},{\"end\":49768,\"start\":49756},{\"end\":49786,\"start\":49768},{\"end\":49798,\"start\":49786},{\"end\":50175,\"start\":50163},{\"end\":50189,\"start\":50175},{\"end\":50206,\"start\":50189},{\"end\":50216,\"start\":50206},{\"end\":50508,\"start\":50496},{\"end\":50518,\"start\":50508},{\"end\":50531,\"start\":50518},{\"end\":50543,\"start\":50531},{\"end\":50557,\"start\":50543},{\"end\":50569,\"start\":50557},{\"end\":50580,\"start\":50569},{\"end\":50592,\"start\":50580},{\"end\":50610,\"start\":50592},{\"end\":50628,\"start\":50610},{\"end\":50637,\"start\":50628},{\"end\":51016,\"start\":51004},{\"end\":51030,\"start\":51016},{\"end\":51042,\"start\":51030},{\"end\":51057,\"start\":51042},{\"end\":51468,\"start\":51456},{\"end\":51480,\"start\":51468},{\"end\":51492,\"start\":51480},{\"end\":51506,\"start\":51492},{\"end\":51519,\"start\":51506},{\"end\":51534,\"start\":51519},{\"end\":51549,\"start\":51534},{\"end\":51562,\"start\":51549},{\"end\":51576,\"start\":51562},{\"end\":51588,\"start\":51576},{\"end\":52002,\"start\":51990},{\"end\":52012,\"start\":52002},{\"end\":52027,\"start\":52012},{\"end\":52040,\"start\":52027},{\"end\":52373,\"start\":52361},{\"end\":52383,\"start\":52373},{\"end\":52398,\"start\":52383},{\"end\":52411,\"start\":52398},{\"end\":52711,\"start\":52699},{\"end\":52721,\"start\":52711},{\"end\":52736,\"start\":52721},{\"end\":52745,\"start\":52736},{\"end\":52758,\"start\":52745},{\"end\":52771,\"start\":52758},{\"end\":53142,\"start\":53122},{\"end\":53151,\"start\":53142},{\"end\":53157,\"start\":53151},{\"end\":53475,\"start\":53453},{\"end\":53484,\"start\":53475},{\"end\":53499,\"start\":53484},{\"end\":53504,\"start\":53499},{\"end\":53973,\"start\":53950},{\"end\":53999,\"start\":53973},{\"end\":54023,\"start\":53999},{\"end\":54455,\"start\":54442},{\"end\":54466,\"start\":54455},{\"end\":54479,\"start\":54466},{\"end\":54493,\"start\":54479},{\"end\":54789,\"start\":54775},{\"end\":54803,\"start\":54789},{\"end\":54812,\"start\":54803},{\"end\":54824,\"start\":54812},{\"end\":55201,\"start\":55188},{\"end\":55216,\"start\":55201},{\"end\":55226,\"start\":55216},{\"end\":55233,\"start\":55226},{\"end\":55507,\"start\":55494},{\"end\":55517,\"start\":55507},{\"end\":55526,\"start\":55517},{\"end\":55540,\"start\":55526},{\"end\":55553,\"start\":55540},{\"end\":55570,\"start\":55553},{\"end\":55584,\"start\":55570},{\"end\":56045,\"start\":56025},{\"end\":56061,\"start\":56045},{\"end\":56068,\"start\":56061},{\"end\":56459,\"start\":56445},{\"end\":56468,\"start\":56459},{\"end\":56478,\"start\":56468},{\"end\":56485,\"start\":56478},{\"end\":56950,\"start\":56936},{\"end\":56959,\"start\":56950},{\"end\":56969,\"start\":56959},{\"end\":56976,\"start\":56969},{\"end\":57290,\"start\":57276},{\"end\":57299,\"start\":57290},{\"end\":57309,\"start\":57299},{\"end\":57316,\"start\":57309},{\"end\":57582,\"start\":57566},{\"end\":57596,\"start\":57582},{\"end\":57609,\"start\":57596},{\"end\":57626,\"start\":57609},{\"end\":57639,\"start\":57626},{\"end\":57654,\"start\":57639},{\"end\":57669,\"start\":57654},{\"end\":57687,\"start\":57669},{\"end\":58114,\"start\":58086},{\"end\":58131,\"start\":58114},{\"end\":58140,\"start\":58131},{\"end\":58151,\"start\":58140},{\"end\":58162,\"start\":58151},{\"end\":58179,\"start\":58162},{\"end\":58190,\"start\":58179},{\"end\":58596,\"start\":58587},{\"end\":58609,\"start\":58596},{\"end\":58866,\"start\":58857},{\"end\":58878,\"start\":58866},{\"end\":58890,\"start\":58878},{\"end\":58905,\"start\":58890},{\"end\":58914,\"start\":58905},{\"end\":59322,\"start\":59313},{\"end\":59338,\"start\":59322},{\"end\":59353,\"start\":59338},{\"end\":59655,\"start\":59644},{\"end\":59668,\"start\":59655},{\"end\":59686,\"start\":59668},{\"end\":59697,\"start\":59686},{\"end\":60072,\"start\":60061},{\"end\":60086,\"start\":60072},{\"end\":60096,\"start\":60086},{\"end\":60109,\"start\":60096},{\"end\":60559,\"start\":60550},{\"end\":60570,\"start\":60559},{\"end\":60579,\"start\":60570},{\"end\":60591,\"start\":60579},{\"end\":60922,\"start\":60913},{\"end\":60933,\"start\":60922},{\"end\":60942,\"start\":60933},{\"end\":60953,\"start\":60942},{\"end\":61352,\"start\":61339},{\"end\":61368,\"start\":61352},{\"end\":61381,\"start\":61368},{\"end\":61391,\"start\":61381},{\"end\":61401,\"start\":61391},{\"end\":61412,\"start\":61401},{\"end\":61432,\"start\":61412},{\"end\":61784,\"start\":61771},{\"end\":61793,\"start\":61784},{\"end\":61806,\"start\":61793},{\"end\":61816,\"start\":61806},{\"end\":61829,\"start\":61816},{\"end\":61840,\"start\":61829},{\"end\":62187,\"start\":62179},{\"end\":62203,\"start\":62187},{\"end\":62217,\"start\":62203},{\"end\":62229,\"start\":62217},{\"end\":62240,\"start\":62229},{\"end\":62548,\"start\":62536},{\"end\":62561,\"start\":62548},{\"end\":62575,\"start\":62561},{\"end\":62585,\"start\":62575},{\"end\":62598,\"start\":62585},{\"end\":62610,\"start\":62598},{\"end\":62622,\"start\":62610},{\"end\":62955,\"start\":62943},{\"end\":62967,\"start\":62955},{\"end\":62975,\"start\":62967},{\"end\":62982,\"start\":62975},{\"end\":62998,\"start\":62982},{\"end\":63009,\"start\":62998},{\"end\":63450,\"start\":63438},{\"end\":63462,\"start\":63450},{\"end\":63470,\"start\":63462},{\"end\":63477,\"start\":63470},{\"end\":63493,\"start\":63477},{\"end\":63504,\"start\":63493},{\"end\":63870,\"start\":63857},{\"end\":63879,\"start\":63870},{\"end\":63891,\"start\":63879},{\"end\":63900,\"start\":63891},{\"end\":63915,\"start\":63900},{\"end\":63928,\"start\":63915},{\"end\":63939,\"start\":63928},{\"end\":64259,\"start\":64247},{\"end\":64273,\"start\":64259},{\"end\":64288,\"start\":64273},{\"end\":64303,\"start\":64288},{\"end\":64764,\"start\":64747},{\"end\":64777,\"start\":64764},{\"end\":64794,\"start\":64777},{\"end\":64801,\"start\":64794},{\"end\":65148,\"start\":65132},{\"end\":65160,\"start\":65148},{\"end\":65172,\"start\":65160},{\"end\":65186,\"start\":65172},{\"end\":65193,\"start\":65186},{\"end\":65675,\"start\":65661},{\"end\":65687,\"start\":65675},{\"end\":65704,\"start\":65687},{\"end\":65717,\"start\":65704},{\"end\":65728,\"start\":65717},{\"end\":65741,\"start\":65728},{\"end\":65752,\"start\":65741},{\"end\":65767,\"start\":65752},{\"end\":65778,\"start\":65767},{\"end\":65790,\"start\":65778},{\"end\":65796,\"start\":65790},{\"end\":66350,\"start\":66338},{\"end\":66361,\"start\":66350},{\"end\":66371,\"start\":66361},{\"end\":66379,\"start\":66371},{\"end\":66394,\"start\":66379},{\"end\":66406,\"start\":66394}]", "bib_venue": "[{\"end\":37237,\"start\":37187},{\"end\":37675,\"start\":37607},{\"end\":38125,\"start\":38078},{\"end\":38560,\"start\":38513},{\"end\":39044,\"start\":38976},{\"end\":39564,\"start\":39496},{\"end\":40059,\"start\":40010},{\"end\":40491,\"start\":40444},{\"end\":40895,\"start\":40834},{\"end\":41325,\"start\":41263},{\"end\":41717,\"start\":41689},{\"end\":42033,\"start\":41949},{\"end\":42386,\"start\":42318},{\"end\":42902,\"start\":42855},{\"end\":43455,\"start\":43387},{\"end\":43996,\"start\":43928},{\"end\":44395,\"start\":44371},{\"end\":44856,\"start\":44795},{\"end\":45388,\"start\":45314},{\"end\":45777,\"start\":45758},{\"end\":46155,\"start\":46087},{\"end\":46552,\"start\":46464},{\"end\":47029,\"start\":46943},{\"end\":47575,\"start\":47507},{\"end\":48081,\"start\":48034},{\"end\":48545,\"start\":48501},{\"end\":48978,\"start\":48910},{\"end\":49568,\"start\":49521},{\"end\":49866,\"start\":49798},{\"end\":50278,\"start\":50216},{\"end\":50699,\"start\":50653},{\"end\":51125,\"start\":51057},{\"end\":51632,\"start\":51588},{\"end\":52084,\"start\":52040},{\"end\":52434,\"start\":52411},{\"end\":52821,\"start\":52771},{\"end\":53204,\"start\":53157},{\"end\":53562,\"start\":53504},{\"end\":54085,\"start\":54023},{\"end\":54524,\"start\":54493},{\"end\":54892,\"start\":54824},{\"end\":55277,\"start\":55233},{\"end\":55652,\"start\":55584},{\"end\":56130,\"start\":56068},{\"end\":56558,\"start\":56485},{\"end\":57020,\"start\":56976},{\"end\":57360,\"start\":57316},{\"end\":57745,\"start\":57687},{\"end\":58237,\"start\":58190},{\"end\":58658,\"start\":58609},{\"end\":58982,\"start\":58914},{\"end\":59415,\"start\":59353},{\"end\":59755,\"start\":59697},{\"end\":60177,\"start\":60109},{\"end\":60653,\"start\":60591},{\"end\":61021,\"start\":60953},{\"end\":61472,\"start\":61432},{\"end\":61884,\"start\":61840},{\"end\":62270,\"start\":62240},{\"end\":62666,\"start\":62622},{\"end\":63077,\"start\":63009},{\"end\":63551,\"start\":63504},{\"end\":63983,\"start\":63939},{\"end\":64371,\"start\":64303},{\"end\":64845,\"start\":64801},{\"end\":65261,\"start\":65193},{\"end\":65864,\"start\":65796},{\"end\":66458,\"start\":66406},{\"end\":37283,\"start\":37239},{\"end\":37739,\"start\":37677},{\"end\":38168,\"start\":38127},{\"end\":38603,\"start\":38562},{\"end\":39108,\"start\":39046},{\"end\":39628,\"start\":39566},{\"end\":40534,\"start\":40493},{\"end\":42450,\"start\":42388},{\"end\":42945,\"start\":42904},{\"end\":43519,\"start\":43457},{\"end\":44060,\"start\":43998},{\"end\":44467,\"start\":44458},{\"end\":44913,\"start\":44858},{\"end\":46219,\"start\":46157},{\"end\":47102,\"start\":47031},{\"end\":47639,\"start\":47577},{\"end\":48124,\"start\":48083},{\"end\":49042,\"start\":48980},{\"end\":49611,\"start\":49570},{\"end\":49930,\"start\":49868},{\"end\":51189,\"start\":51127},{\"end\":52453,\"start\":52436},{\"end\":52867,\"start\":52823},{\"end\":53247,\"start\":53206},{\"end\":53612,\"start\":53564},{\"end\":54956,\"start\":54894},{\"end\":55716,\"start\":55654},{\"end\":56627,\"start\":56560},{\"end\":57795,\"start\":57747},{\"end\":58280,\"start\":58239},{\"end\":59046,\"start\":58984},{\"end\":59805,\"start\":59757},{\"end\":60241,\"start\":60179},{\"end\":61085,\"start\":61023},{\"end\":63141,\"start\":63079},{\"end\":63594,\"start\":63553},{\"end\":64435,\"start\":64373},{\"end\":65325,\"start\":65263},{\"end\":65928,\"start\":65866}]"}}}, "year": 2023, "month": 12, "day": 17}