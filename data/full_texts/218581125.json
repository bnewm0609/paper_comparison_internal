{"id": 218581125, "updated": "2023-10-06 16:12:59.276", "metadata": {"title": "Temporal Common Sense Acquisition with Minimal Supervision", "authors": "[{\"first\":\"Ben\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Ning\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Khashabi\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Roth\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly. This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TacoLM, a temporal common sense language model. Our method is shown to give quality predictions of various dimensions of temporal common sense (on UDST and a newly collected dataset from RealNews). It also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the standard BERT. Thus, it will be an important component of temporal NLP.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.04304", "mag": "3034602344", "acl": "2020.acl-main.678", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/ZhouNKR20", "doi": "10.18653/v1/2020.acl-main.678"}}, "content": {"source": {"pdf_hash": "c767705c5d7bb9b7ddd2c2c69d27353460a14f9e", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2020.acl-main.678.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2005.04304", "status": "GREEN"}}, "grobid": {"id": "c5e50d212a315d01e9ba16397185e52a0b2dafab", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c767705c5d7bb9b7ddd2c2c69d27353460a14f9e.txt", "contents": "\nTemporal Common Sense Acquisition with Minimal Supervision\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsJuly 5 -10, 2020. 2020\n\nBen Zhou xyzhou@cis.upenn.edu \nUniversity of Pennsylvania\n\n\nQiang Ning qiangn@allenai.org \nAllen Institute for AI\n\n\nDaniel Khashabi danielk@allenai.org \nAllen Institute for AI\n\n\nDan Roth danroth@cis.upenn.edu \nUniversity of Pennsylvania\n\n\nTemporal Common Sense Acquisition with Minimal Supervision\n\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics\nthe 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 5 -10, 2020. 20207579\nTemporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly. This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TACOLM, 1 a temporal common sense language model.Our method is shown to give quality predictions of various dimensions of temporal common sense (on UDST and a newly collected dataset from Real-News). It also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the standard BERT. Thus, it will be an important component of temporal NLP.Acquiring the multiple dimensions of TCS (e.g., duration and frequency) is challenging. As shown\n\nIntroduction\n\nTime is crucial when describing the evolving world. It is thus important to understand time as expressed in natural language text. Indeed, many natural language understanding (NLU) applications, including information retrieval, summarization, causal inference, and QA (UzZaman et al., 2013;Chambers et al., 2014;Llorens et al., 2015;Bethard et al., 2016;Leeuwenberg and Moens, 2017;Ning et al., 2018b), rely on understanding time.\n\nHowever, understanding time in natural language text heavily relies on common sense inference. Such inference is challenging since commonsense information is rarely made explicit in text (e.g., how long does it take to open a door?) Even when such information is mentioned, it is often affected by another type of reporting bias: people rarely say the obvious, in order to communicate more efficiently, but sometimes highlight rarities (Schubert, 2002;Van Durme, 2009;Gordon and Van Durme, 2013;Zhang et al., 2017;Bauer et al., 2018;Tandon et al., 2018). This is an even more pronounced phenomenon when it comes to temporal common sense (TCS) (Zhou et al., 2019). In Example 1, human readers know that a typical vacation is likely to last at least a few days, and they would choose \"will not\" to fill in the blank for the first sentence; instead, with a slight change of context \"vacation\" \u2192 \"walk outside,\" people typically prefer \"will\" for the second one. Similarly, any system which correctly answers this example for the right reason would need to incorporate TCS in its reasoning.\n\nExample 1: choosing from \"will\" or \"will not\" Dr. Porter is now (e1:taking) a vacation and be able to see you soon. Dr. Porter is now (e2:taking) a walk outside and be able to see you soon.\n\nin Example 1, the duration of \"taking a vacation\" and \"taking a walk\" are not expressed explicitly, so that systems are required to read between the lines to support the inference. A pre-trained language model may not handle this issue well, as it cannot identify the TCS dimensions in temporal mentions and effectively learn from them. As a result, it cannot generalize well to similar events without explicit temporal mentions. To handle this problem, we design syntactic rules that can collect a vast amount of explicit mentions of TCS from unannotated corpus such as Gigaword (Graff et al., 2003) ( \u00a73.3). We use this data to pre-train our model so that it distinguishes different dimensions.\n\nA second challenge occurs when the text is highlighting rare and special cases. As a result, temporal mentions in natural text may follow a distorted distribution in which certain kinds of \"common\" events are under-represented. For instance, we may rarely see mentions of \"I opened the door in 3 seconds,\" but we may see \"it took me an hour to open this door\" in text. To overcome this challenge, we exploit the joint relationship among temporal dimensions. Although we rarely observe the true duration of \"opening the door\" in free-form text, we may see phrases like \"I opened my door during the fire alarm,\" providing an upper-bound to the duration of the event (i.e., \"opening the door\" does not take longer than the alarm.) We believe that we are the first to exploit such phenomena among temporal dimensions.\n\nThis paper studies several important dimensions of TCS inference: duration (how long an event takes), frequency (how often an event occurs) and typical time (when an event typically happens). 2 As a highlight, Fig. 1 shows the distributions (over time units) we predict for the duration and frequency of three events. We can see that \"taking a vacation\" lasts from days to months while \"taking a walk\" lasts from minutes to hours. As shown, our model is able to produce different and sensible distributions for the \"take\" event, depending on the context in which \"take\" occurs.\n\nOur work builds upon pre-trained contextual language models Devlin et al., 2019;Liu et al., 2019). However, a standard language modeling objective does not lead to a model that handles the two challenges mentioned above; in addition, other systematic issues limit its ability to handle TCS. In particular, language models do not directly utilize the ordinal relationships among temporal units. For example, \"hours\" is longer than \"minutes,\" and \"minutes\" are longer than \"seconds.\" 3 Fig. 2 shows that BERT does not produce a meaningful duration distribution for a set of events with a gold duration of \"day\" (extracted in \u00a73.3).\n\nOur proposed system, on the other hand, is able to utilize the ordinal relationships and produce unimodal distributions around the correct labels in both Fig. 1  Contributions. This work proposes an augmented pre-training for language models to improve their understanding of several important temporal phenomena. We address two kinds of reporting biases by effectively acquiring weak supervision from free-form text and utilizing it to learn multiple temporal dimensions jointly. Our model incorporates other desirable properties of time in its objective (ordinal relations between temporal phrases, the circularity of certain dimensions, etc.) to improve temporal modeling. Our experiments show 19% relative improvement over BERT in intrinsic evaluations, and 5-10% improvements in most extrinsic evaluations done on three timerelated datasets. Furthermore, the ablation study shows the value of each proposed component of our construction. Overall, this is the first work to incorporate a wide range of temporal phenomena within a contextual language model. The rest of this paper is organized as follows. We distinguish our work with the prior work in \u00a72. The core of our construction, including extraction of cheap supervision from raw data and augmenting a language model objective function with temporal signals, is in \u00a73 . We conclude by showing intrinsic and extrinsic experiments in \u00a74.\n\n\nRelated Work\n\nCommon sense has been a popular topic in recent years, and existing NLP works have mainly investigated the acquisition and evaluation of common sense reasoning in the physical world. These works include but are not limited to size, weight, and strength (Bagherinezhad et al., 2016;Forbes and Choi, 2017;Elazar et al., 2019), roundness and deliciousness (Yang et al., 2018), and intensity (Cocos et al., 2018). A handful of these works uses cheap supervision. For example, Elazar et al. (2019) recently proposed a general framework that discovers distributions of quantitative attributes (e.g., length, mass, speed, and duration) from explicit mentions (or co-occurrences) of these attributes in a large corpus. However, Elazar et al. (2019) restrict events to be verb tokens, while we handle verb phrases containing more detailed information (e.g., \"taking a vacation\" is very different from \"taking a break,\" although they share the same verb \"take\"). Besides, there has been no report on the effectiveness of this method on temporal attributes.\n\nOn the other hand, time has long been an important research area in NLP. Prior works have focused on the extraction and normalization of temporal expressions (Str\u00f6tgen and Gertz, 2010;Angeli et al., 2012;Lee et al., 2014;Vashishtha et al., 2019), temporal relation extraction (Ning et al., 2017(Ning et al., , 2018cVashishtha et al., 2019), and timeline construction (Leeuwenberg and Moens, 2018). Recently, MCTACO (Zhou et al., 2019) summarizes five types of TCS and the three temporal dimensions studied here are all in their proposal. 4 MCTACO shows that modern NLU techniques are still a long way behind humans on TCS understanding, suggesting that further research on this topic is needed.\n\nThere have been works on temporal common sense, such as event duration (Pan et al., 2006;Gusev et al., 2011;Williams, 2012;Vempala et al., 2018;Vashishtha et al., 2019), typical temporal ordering (Chklovski and Pantel, 2004;Ning et al., 2018a,b), and script learning (i.e., what happens next after certain events) (Granroth-Wilding and Clark, 2016;Li et al., 2018;Peng et al., 2019). Those on duration are highly relevant to this work. (Pan et al., 2006) annotates a subset of documents from TimeBank (Pustejovsky et al., 2003) with \"less-than-one-day\" and \"more-than-one-day\" annotations and provides the first baseline system for this dataset. Vempala et al. (2018) significantly improve earlier work by using additional aspectual features for this task. Vashishtha et al. (2019) annotate the UDS-T dataset with event duration annotations and propose a joint method that extracts both temporal relations and event durations. Our approach has two notable differences from this line of work. First, we work on duration, frequency, and typical time-jointly on three dimensions of TCS, while the works above only focused on duration. Second, we focus more on obtaining cheap supervision signals from unlabeled data, while these other works all have access to human annotations. With respect to harnessing cheap supervision, Williams (2012); Gusev et al. (2011) propose to mine web data using a collection of hand-designed query patterns. In contrast to our approach, they are based on counting instead of machine learning and cannot handle the contextualization of events.\n\n\nTemporally Focused Joint Learning with Minimal Supervision\n\nIn this section, we elaborate our approach to designing and pre-training TACOLM, a time-aware language model.\n\n\nScope\n\nIn this work, we focus on three major temporal dimensions of events, namely Duration, Frequency and Typical Time. Here, Typical Time means the typical occurring time of events during a day, day of a week, and month or season of a year. We follow the same definition to each of the dimensions (also called properties) in Zhou et al. (2019).\n\n\nJoint Learning and Auxiliary Dimensions\n\nAs mentioned earlier, commonsense information extraction comes with the challenge of reporting biases. For example, people may not report the duration of \"opening the door,\" or the frequency of \"going to work.\" However, it is often possible to get supportive signals from other dimensions, as people mention \"going to work\" associated mostly with \"a day\" in a week, hence we may know the frequency of such an event.\n\nWe argue that many temporal dimensions are interrelated and a joint learning scheme would suit this task. Beyond duration, frequency and typical time, we also introduce auxiliary dimensions that are not meant to be used by themselves but will help the prediction of other dimensions. The auxiliary dimensions we define here are event Duration Upper-bound and event Relative Hierarchy. The former represents values that are upper-bounds to an event's duration but not necessarily the exact duration. The latter consists of two sub-relations, namely temporal ordering and duration inclusion of event-pairs.\n\n\nCheap Supervision from Patterns\n\nWe collect a few pattern-based extraction rules based on SRL parses for each temporal dimension (including the auxiliary dimensions). We design the rules to have high precision, while not compromising too much on recall. We overcome the potential sparsity issue (and the resulting low recall problem) by extracting from a massive amount of data. Fig. 3 provides some examples of the input/output for each dimension, as we describe the specific extraction process below.\n\nWe first process the entire Gigaword (Graff et al., 2003) corpus and use AllenNLP's SRL model Shi and Lin, 2019) to annotate all sentences. We extract the ones that contain at least one temporal argument (i.e., the arg-tmp constituent of SRL annotations) and use textual patterns to categorize each sentence into a corresponding dimension with respect to an associated verb. These patterns are inspired by earlier works and are extensively improved with iterative manual error analysis. The rest of this section is devoted to explaining the key design ideas used for these patterns. Duration. We check if the temporal argument starts with \"for,\" extract the numerical value and the temporal unit word, and normalize them into the nearest unit among the nine units in our scope: (\"second,\" \"minute,\" \"hour,\" \"day,\" \"week,\" \"month,\" \"year,\" \"decade,\" \"century.\") We ignore particular phrases such as \"for a second chance\" where the semantic of \"second\" is not temporal related. We found that \"for\" is the only highprecision preposition that indicates exact values of duration. Frequency. Such temporal arguments are usually composed of a duration phrase and a numerical head (e.g., \"four times per\") indicating the frequency within the duration (e.g., \"week\"). Thus, we check for multiple keywords that indicate the start of a frequency expression, including \"every,\" \"per,\" \"once,\" . . . \"times.\" If so, we extract the duration value as well as the numerical head's value. We ignore any temporal phrases that contain \"when\" since they often convey semantics that does not fit any of our temporal categories; e.g., \"when everyday life...\" is not describing the frequency of the corresponding verb. We represent the frequency with duration d, with a definition of occurring once every d elapses. For example, the frequency of \"four times per week\" is represented as \"1.75 days.\" Similarly, we normalize them into the nearest unit among the nine duration units described above, and \"1.75 days\" is extracted as \"days.\" Typical Time. We pre-define a list of typical time keywords, including the time of day (e.g., \"morning\" etc.), time of week (e.g., \"Monday\" etc.), month (e.g., \"January\" etc.) and season (e.g., \"winter\" etc.) We check if any of the typical time keywords appear in the temporal argument and verify if the temporal argument is, in fact, describing the time of occurrence. This is done by filtering out the temporal arguments that contain a set of invalid prepositions, including \"until,\" \"since,\" \"following,\" since such keywords often do not indicate the actual time of occurrence. Duration Upper-bound. Many temporal arguments describe the duration upper-bound instead of the exact duration value. For example, as described in (Gusev et al., 2011), \"did [activity] yesterday\" indicates something that happened within a day. We extend the set of patterns to include \"in [temporal expression]\" or keywords such as \"next\" (e.g., \"the next day\"), \"last\" (e.g., \"last week\"), \"previous\" (e.g., \"previous month\"), or \"recent\" (e.g., \"recent years\"). We normalize the values into the same label set of the nine unit words as the duration dimension. Event Relative Hierarchy. A system can learn about an event with comparisons to other ones, as we show in \u00a71. To acquire hierarchical relationships between events, we check whether the SRL temporal argument starts with a keyword that indicates a relation between the main event and another event phrase. We consider five such keywords, namely \"before,\" \"after,\" \"during,\" \"while\" and \"when.\" We use these keywords to label the relative relationship between the two events. Here, we assume that \"during\" and \"while\" are the same, which indicates that the main event is not longer than the one in the argument. Note that certain keywords might have meanings that do not suggest temporal relationships (e.g., \"while\" has a different sense similar to \"whereas.\") We rely on SRL annotations to identify the appropriate sense of the keywords. We use the temporal keyword as labels, but keep the entire event phrase in the SRL temporal argument for later use in \u00a73.5. Resulting data. We collect 25 million instances that are successfully parsed into one of our temporal dimensions from the entire Gigaword corpus (Graff et al., 2003). Each instance is in the form of (event,value,dimension) tuples (Fig. 3), with a dimension distribution shown in Fig. 4. For all events, we remove the related temporal argument so that it does not contain direct information about the dimension or the value. For example, as shown in Fig. 3, \"for 2 hours\" is removed, and only \"Jack rested before the speech\" is kept so that the target duration does not present in the event. Note that value is also called and used as \"label\" in later contexts related to classification tasks.  \n\n\nSoft Cross-Entropy Objective for Ordinal Classification\n\nThe temporal values in one dimension are naturally related to each other via a certain ordering and appropriate distance measures. To account for and utilize this external knowledge, we use a soft crossentropy to encourage predictions that are aligned with the external knowledge. Consider x as a system's output logits across labels, and we express our soft loss function as follows:\n= \u2212 i\u2208D y i log(softmax(x i )),(1)\nwhere D is the instances in the training data and y represent the degree to which the target labels align with the external knowledge. Thus, y is a probability vector, i.e., has non-zero values and sum to 1.0. Now we describe how we construct y to apply the aforementioned external knowledge. Duration, Frequency, and Upper-bound take the same set of labels of duration units. We first define a function logsec(.) which takes a unit and normalizes it to its logarithmic value in \"seconds\" (e.g., \"minute\" \u2192 60 \u2192 4.1). For each instance in these dimensions, with an observed gold label g, we assume a normal distribution with a mean value of \u00b5 = logsec(g) and a fixed standard deviation of \u03c3 = 4. Then, we construct y so that,\ny[i] = 1 \u03c3 \u221a 2\u03c0 e \u2212(logsec(l)\u2212\u00b5) 2 /2\u03c3 2(2)\nwhere l is the i th label. We apply softmax at the end to ensure y sums to 1. For typical time, the labels are placed with approximately equal distances in a circular fashion. For example, \"Monday\" is before \"Tuesday\" and after \"Sunday.\" We assume adjacent units have a distance of 1, and we generate y based on a Gaussian distribution with a standard deviation of 0.5. In other words, we assume the two immediate neighbors of a gold label are reasonably possible.\n\nFor hierarchy, we construct y as a one-hot vector where only the gold label has a value of 1, and the rest are zeroes.\n\n\nSequential Language Modeling\n\nOur goal is to build a model that is able to predict temporal labels (values) given events and dimensions. Instead of building a classification layer on top of a pre-trained model, we follow previous work (Huang et al., 2019) and place the label into the input sequence. We mask the label in the sequence and use the masked token prediction objective as the classification objective. To produce more general representations, we also keep the temporal label and mask the event tokens instead at a certain probability, so that we are able to maximize both P (Tmp-Label|Event) and P (Event|Tmp-Label) in the same learning process, where Tmp-Label refers to the temporal label associated with the event.\n\nSpecifically, we use the reserved \"unused\" tokens in BERT-base model lexicon to construct a 1-to-1 mapping from every value in every dimension to the new vocabulary. We choose not to use the existing representations for temporal terms that are already included in BERT's \"inuse\" lexicon, such as \"minutes\" or \"weeks,\" because these keywords have different temporal semantics in different dimensions. Instead, we assign unique and separate lexicon entries to different values in different dimensions, even though the values may share the same surfaces. Consider each (event,value,dimension) tuple, we map value and dimension to their new vocabularies [Val] [Vrb] is a marker token that is the same across all instances. [Arg-Tmp-Event] is the event phrase in the SRL temporal argument, as described in hierarchy. [Arg-Tmp-Event] is empty for all dimensions other than hierarchy.\n\nWe mask [Val] with probability p mask and [Dim] with probability p dim . We individually mask each event tokens with probability p event when we do not mask [Val] nor [Dim]. Soft crossentropy is used when predicting [Val], and a regular Cross-entropy is used for other tokens. We use the pre-trained token-recovery layer, and follow BERT's setting to randomly keep a token's surface or change it to noise during recovery.\n\nIn the experiments, we explore a set of configurations of the system. We explore the effect of having only one sentence or the two additional neighboring sentences as input contexts. We also experiment with all-event-masking, where we mask tokens in the event with a much higher probability. The goal of this masking scheme is to reduce the predictability of event tokens based on other event tokens to alleviate prior biases and focus more on the temporal argument. For example, BERT predicts \"coffee\" for the [MASK] in \"I had a cup of [MASK] this evening\" because of the strong prior of \"cup of.\" By masking more tokens in the event, the remaining ones will be more conditioned to the temporal cue.\n\n\nLabel Weight Adjustment\n\nThe label imbalance in the training data largely hinders our goal, as we should not assume a prior distribution as expressed in natural language. For example, \"seconds\" appears around ten times less than \"years\" in the data we collected for duration, leading to a biased model. We use weight adjustment to fix this. Specifically, we apply weight adjustment to the total loss with a weight factor calculated as the observed label's count relative to the number of all instances.\n\n\nExperiments\n\n\nVariations and Settings\n\nWe experiment with several variants of the proposed system to study the effect of each change. Input Size. A model with three input sentences (including the event sentence's left/right neighbors) are labeled with MS. Non MS models use only one sentence in which the event occurs. All Event Masking. A model with p event = 0.6 is labeled as AM, and p event = 0.15 otherwise. Final Model. Our final model includes all auxiliary dimensions (AUX) (mentioned in \u00a73.2), uses soft cross-entropy loss (SL) and applies weight adjustment (ADJ) (mentioned in \u00a73.6). We study each changes' effect by ablating them individually.\n\nTo deal with the skew present in the training data ( \u00a73), we down-sample to ensure roughly the same occurrences of each dimension (except for frequency because of its low quantity). As a result, 4.3 million sentences were used in pre-training (downsampled from 25 million mined sentences). We employ a learning rate of 2e-5 with 3 epochs and set p mask = 0.6 and p dim = 0.1. Other parameters are the same as those of the BERT base model. We use epoch 2's model for extrinsic evaluations to favor generalization, and epoch 3's model for intrinsic evaluations as it achieves the best performance across tasks.\n\n\nIntrinsic Evaluation\n\nWe evaluate our method on the temporal value recovery task, where the inputs are a sentence representing the event, an index to the event's verb, and a target dimension. The goal is to recover the temporal value of the given event in the given dimension. Datasets. To ensure a fair comparison, we sample instances from a new corpus RealNews (Zellers et al., 2019) that have no document overlap with our pre-training data and, at the same time making the data not strictly in-domain. We apply the same pattern extraction process mentioned in \u00a73.3 on the new data and collect instances that are uniformly distributed across dimensions and values. In addition, we ask annotators on Mechanical Turk to filter out the events that cannot be recovered by common sense. For example, \"I brush my teeth [Month]\" will be discarded because all candidate answers are approximately uniformly distributed so that one cannot identify a subgroup of labels to be more likely.\n\nSpecifically, we ask one annotator to select from 4 choices regarding each (event, temporal value) tuple. The choices are 1) the event is unclear and abstract; 2) the event has a uniform distribution across all labels within the dimension; 3) the given label is one of the top 25% choices among all other labels within the dimension and 4) the given label is not very likely. We keep the instances for which the annotator selects option 3), verifying that the label is a very likely choice for the given dimension. For the RealNews corpus, we annotate 1,774 events that are roughly uniformly distributed across dimensions and labels, among which 300 events are preserved.\n\nWe also apply the same process to UDST dataset. We find the majority of the original annotation to be unsuitable, as there are many annotations to events that are seemingly undecidable by common sense. We first apply an initial filtering by using only events of which the anchor word is a verb and require all existing annotations from (Vashishtha et al., 2019) of the same instance to have an average distance less than two units. We then use our method to annotate 1,047 events, and eventually, 142 instances are left. Systems. In both datasets, we compare our proposed system with BERT. To use BERT's predictions on temporal values without supervision, we artificially add prepositions querying the target dimension as well as a masked token right after the verb. For example, \"I ran to the campus\" will be transformed as \"I ran for 1 [MASK] to the campus\". The specific prepositions added are \"for 1\" (duration), \"every\" (frequency), \"in the\" (time of the day), \"on\" (week), \"in\" (month), and \"in\" (season). We then rank the temporal keywords (singular) in the given dimension according to the masked token's predictions. For our model, we follow the sequence formulation described above, recover and rank the masked [Val] token.\n\nIn addition, we also compare with a baseline system called BERT + naive finetune, which is BERT fine-tuned on the same pre-training data we used for our proposed models, with a higher probability of masking a temporally related keyword (i.e., all values we used in all dimensions). Unlike our model, we only use soft cross-entropy loss and do not distinguish the dimensions each keyword is expressing. Metrics. Following Vashishtha et al. (2019), we employ a metric \"distance\" that measures the rank difference between a system's top prediction and the gold label with respect to an ordered label set. For duration and frequency where values are in a one-directional order, we use the absolute difference of the label ranks. For other dimensions where the labels are in circular relationships, we use the minimal distance between two labels in both directions, so that \"January\" will have a distance 1 with \"December.\" This is similar to an MAE metric, and we report the averaged number across instances.   Table 1: Performance on intrinsic evaluations. The \"normalized\" row is the ratio of the distance to the gold label over the total number of labels in each dimension. Smaller is better.\n\nThe results on the filtered RealNews dataset and filtered UDST dataset are shown in Table 1. We see that our proposed final model is mostly better than other variants, and achieves 19% improvement over BERT on average on the normalized scale.\n\nWe plot the embedding space of events with duration of \"seconds\" \"weeks\" or \"centuries\" in Fig 5  and Fig 6. We take the verb's contextual representation, apply PCA to reduce the dimension from 768 to 50, and then t-SNE to reduce it further to 2. Comparing the two plots, we see that the clusters formed by BERT embeddings have a wider distribution over the space, and the clusters have more points in overlap, even though the three sets of events have drastically different duration values. Our proposed model's embedding is able to better cluster the events based on this temporal feature, which is expected.\n\n\nTimeBank Evaluation\n\nBeyond unsupervised intrinsic experiments, we also evaluate the capability of the event temporal representation as a product of our model. That is, we finetune both BERT baseline and our model with the same process to compare the internal representations of the transformers. We use TimeML (Saur\u00e9i et al., 2005;Pan et al., 2006), a dataset with event duration annotated as lower and upper bounds. The task is to decide whether a given event has a duration longer or shorter than a day. This is a suitable task to evaluate the embeddings because deciding longer/shorter than a day requires reasoning with more than one label, and would also benefit from auxiliary dimensions like duration upper-bound.\n\nThe dataset contains 2,251 events, and we split the events based on sentences into 1,248/1,003 train/test. We formulate the training as a sequence classification task by taking the entire sentence and adding a special marker to the left of the verb indicating its position. The marker is unseen to both BERT and our model. We use the transformer output of the first token and feed it to an MLP for classification. We use a learning rate of 5e-5 and train for 3 epochs, and we repeat every reported number with 3 different random initialization and take the average.   Table 2 shows the results of the TimeBank experiment. We see around 7-11% improvement over BERT on this task. Comparing with the state-ofthe-art (Vempala et al., 2018) with a different training/testing split, our model is within 1.5% of the best results but uses 25% less training data.\n\n\nSubevent Relation Extraction\n\nWe apply our event representations to the task of event sub-super relation extraction. This is a proper evaluation because the task naturally benefits from temporal commonsense knowledge. Intuitively, short duration or high frequency indicates the event being at a lower hierarchy and vice versa. We test if the temporal focused event representations will improve.\n\nWe use HiEVE (Glava\u0161 et al., 2014), a dataset with annotations of four event relationships: no relation (NoRel), coreference (Coref), Child-Parent (C-P) and Parent-Child (P-C). There is no official split for this dataset, so we randomly 80/20 split the data at the document level and down-sample negative NoRel instances with a probability of 0.4.\n\nSimilarly, we formulate the problem as a sequence classification task, where two events are put into one sequence separated by \" [SEP],\" and verbs are marked by adding a marker token to their left. We use the representations of the first token and feed it to an MLP for classification. We train each model with a 5e-5 learning rate and 3 epochs. Each reported number is an average from 3 runs under different random initialization. During inference time, the probability scores for non-negative relations are averaged from the same event pair's sequences in both orders. Table 3 shows the results of the HiEVE experiment. We see that TACOLM improves by 4% and 8% on the coreference task and the parent-child tasks over BERT, respectively.  \n\n\nTemporal Question Answering\n\nWe also evaluate on MCTACO (Zhou et al., 2019), a question answering dataset that requires comprehensive understandings of temporal common sense and reasoning. We compare the exact-match score across the 5 dimensions defined in MCTACO, although this work only focuses on 3 of them. We use the original baseline system and interchange transformer weights to compare between BERT and ours. However, because our model replaces temporal expressions with special tokens, it is at disadvantage to be directly evaluated on the original dataset with temporal expressions in natural language. To fix this, we run the same extraction system in \u00a73.3 with modifications to identify the dimension a question is asking, and augment candidate answers with our special tokens representing the temporal values (if any) mentioned. This introduces rule-based dimension identification as well as coarse unit normalization to the systems, so we train/evaluated BERT baseline with the same modified data as well. Each number is an average of 5 runs with different random initializations.\n\nResults on MCTACO are shown in Table 4. As expected, we find that our model achieves better  performance on the three dimensions that are focused in this work (i.e., duration, frequency, and typical time) as well as stationarity. However, the improvements are not very substantial, indicating the difficulty of this task and motivates future works. The model also does slightly worse on ordering, which is worth investigating in future works.\n\n\nConclusion\n\nTemporal common sense (TCS) is an important yet challenging research topic. Despite the existence of several prior work on event duration, this is the first attempt to jointly model three key dimensions of TCS-duration, frequency, and typical timefrom cheap supervision signals mined from unannotated free text. The proposed sequence modeling framework improves over BERT in terms of handling reporting bias, taking into account the ordinal relations and exploiting interactions among multiple dimensions of time. The success of this model is confirmed by intrinsic evaluations on RealNews and UDS-T (where we see a 19% improvement), as well as extrinsic evaluations on TimeBank, HiEVE and MCTACO. The proposed method may be an important module for future applications related to time.\n\nFigure 1 :\n1Dr. Porter is taking a walk. Dr. Porter is taking his weekend break. Dr. Porter is taking a long vacation. Our model's predicted distributions of event duration and frequency. The model is able to attend to contextual information and thus produce reasonable estimates.\n\nFigure 3 :\n3Examples of the extraction process for each temporal dimensions. The temporal arguments are marked orange and the result of the extraction are tuples of the form (event,value,dimension).\n\nFigure 4 :\n4The distributions of different temporal dimensions in the collected data.\n\nFigure 5 :Figure 6 :\n56Representations of events (whose durations were labeled as seconds, weeks, or centuries) obtained from the original BERT base model. Representations of the same set of events as inFig. 5obtained from the proposed method.\n\n\nandFig. 2 .0 \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\nsecond minute hour \nday \nweek month year decade century \n\n0 \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\nsecond minute hour \nday \nweek month year decade century \n\nBERT TacoLM (Ours) \n\nFigure 2: The predictive distribution of two models \n(ours and vanilla BERT) for a set of events labeled with \n\"days\" as their duration. Experiments show our model \nis about 40% better on duration predictions. (RealNews \ncorpus; details in  \u00a74.2). \n\n\n\n\nand [Dim], and we use [W 1 , W 2 , . . ., W n ] to represent the tokens in the sentence, and W verb the event verb anchor from SRL. We now form a sequence [W 1 , W 2 ,. . .[Vrb], W verb , . . .W n , [SEP], [Vrb], [Dim], [Val], [Arg-Tmp-Event]], where\n\n\nSystem F1 Accuracy <Day F1 \u2265Day F1BERT \n73.7 \n63.7 \n79.0 \nTACOLM \n81.7 \n74.8 \n85.6 \n\n\n\nTable 2 :\n2Performance on TimeBank Classification\n\nTable 3 :\n3Performance on HiEVE. The numbers are in percentages. Higher is better.\n\nTable 4 :\n4Performance on MCTACO. Numbers are percentages and indicate exact match (EM) metric. Higher is better.\nhttps://cogcomp.seas.upenn.edu/page/ publication_view/904\nE.g., typical time in a day (the morning), typical day of a week (on Sunday), and typical time of a year (summer).\nThe relationship can be more complex. E.g., \"hours\" is closer to \"minutes\" than \"centuries\" is; days of a week forms a circle: \"Mon.\" is followed by \"Tue.\" and preceded by \"Sun.\"\nThey additionally propose typical order of events and stationarity (whether a state holds for a very long time or indefinitely).\nAcknowledgmentsThis research is based upon work supported in part by the office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA Contract No. 2019-19051600006 under the BETTER Program and by Contract FA8750-19-2-1004 with the US Defense Advanced Research Projects Agency (DARPA). The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. This research is also supported by a grant from the Allen Institute for Artificial Intelligence (allenai.org).\nParsing time: Learning to interpret time expressions. Gabor Angeli, D Christopher, Daniel Manning, Jurafsky, NAACL-HLT. Gabor Angeli, Christopher D Manning, and Daniel Ju- rafsky. 2012. Parsing time: Learning to interpret time expressions. In NAACL-HLT, pages 446-455.\n\nAre elephants bigger than butterflies? reasoning about sizes of objects. Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, Ali Farhadi, AAAI. Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. 2016. Are elephants bigger than butterflies? reasoning about sizes of objects. In AAAI.\n\nCommonsense for generative multi-hop question answering tasks. Lisa Bauer, Yicheng Wang, Mohit Bansal, EMNLP. Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018. Commonsense for generative multi-hop question an- swering tasks. In EMNLP, pages 4220-4230.\n\nSemEval-2016 Task 12: Clinical TempEval. Steven Bethard, Guergana Savova, Wei-Te Chen, Leon Derczynski, James Pustejovsky, Marc Verhagen, SemEval. Steven Bethard, Guergana Savova, Wei-Te Chen, Leon Derczynski, James Pustejovsky, and Marc Verhagen. 2016. SemEval-2016 Task 12: Clinical TempEval. In SemEval, pages 1052-1062.\n\nDense event ordering with a multi-pass architecture. Nathanael Chambers, Taylor Cassidy, Bill Mcdowell, Steven Bethard, TACL. 2Nathanael Chambers, Taylor Cassidy, Bill McDowell, and Steven Bethard. 2014. Dense event ordering with a multi-pass architecture. TACL, 2:273-284.\n\nVerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations. Timothy Chklovski, Patrick Pantel, EMNLP. Timothy Chklovski and Patrick Pantel. 2004. VerbO- cean: Mining the Web for Fine-Grained Semantic Verb Relations. In EMNLP, pages 33-40.\n\nLearning scalar adjective intensity from paraphrases. Anne Cocos, Veronica Wharton, Ellie Pavlick, Marianna Apidianaki, Chris Callison-Burch, EMNLP. Anne Cocos, Veronica Wharton, Ellie Pavlick, Mari- anna Apidianaki, and Chris Callison-Burch. 2018. Learning scalar adjective intensity from paraphrases. In EMNLP, pages 1752-1762.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In NAACL.\n\nHow large are lions? inducing distributions over quantitative attributes. Yanai Elazar, Abhijit Mahabal, Deepak Ramachandran, Tania Bedrax-Weiss, Dan Roth, ACL. Yanai Elazar, Abhijit Mahabal, Deepak Ramachandran, Tania Bedrax-Weiss, and Dan Roth. 2019. How large are lions? inducing distributions over quanti- tative attributes. In ACL.\n\nVerb physics: Relative physical knowledge of actions and objects. Maxwell Forbes, Yejin Choi, ACL. 1Maxwell Forbes and Yejin Choi. 2017. Verb physics: Relative physical knowledge of actions and objects. In ACL, volume 1, pages 266-276.\n\nAllennlp: A deep semantic natural language processing platform. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, F Nelson, Matthew Liu, Michael Peters, Luke Schmitz, Zettlemoyer, NLP-OSS. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2018. Allennlp: A deep semantic natural language process- ing platform. In NLP-OSS, pages 1-6.\n\nHiEve: A corpus for extracting event hierarchies from news stories. Goran Glava\u0161, Marie-Francine Jan\u0161najder, Parisa Moens, Kordjamshidi, LREC. Reykjavik, IcelandEuropean Language Resources Association (ELRAGoran Glava\u0161, Jan\u0160najder, Marie-Francine Moens, and Parisa Kordjamshidi. 2014. HiEve: A corpus for extracting event hierarchies from news stories. In LREC, pages 3678-3683, Reykjavik, Iceland. Euro- pean Language Resources Association (ELRA).\n\nReporting bias and knowledge acquisition. Jonathan Gordon, Benjamin Van Durme, AKBC. ACMJonathan Gordon and Benjamin Van Durme. 2013. Re- porting bias and knowledge acquisition. In AKBC, pages 25-30. ACM.\n\nEnglish gigaword. Linguistic Data Consortium. David Graff, Junbo Kong, Ke Chen, Kazuaki Maeda, 434PhiladelphiaDavid Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2003. English gigaword. Linguistic Data Consortium, Philadelphia, 4(1):34.\n\nWhat happens next? event prediction using a compositional neural network model. Mark Granroth, - Wilding, Stephen Christopher Clark, ACL. Mark Granroth-Wilding and Stephen Christopher Clark. 2016. What happens next? event predic- tion using a compositional neural network model. In ACL.\n\nUsing query patterns to learn the duration of events. Andrey Gusev, Nathanael Chambers, Pranav Khaitan, Divye Khilnani, Steven Bethard, Dan Jurafsky, IWCS. Andrey Gusev, Nathanael Chambers, Pranav Khaitan, Divye Khilnani, Steven Bethard, and Dan Jurafsky. 2011. Using query patterns to learn the duration of events. In IWCS, pages 145-154.\n\nGlossBERT: BERT for word sense disambiguation with gloss knowledge. Luyao Huang, Chi Sun, Xipeng Qiu, Xuanjing Huang, EMNLP. Luyao Huang, Chi Sun, Xipeng Qiu, and Xuanjing Huang. 2019. GlossBERT: BERT for word sense disambiguation with gloss knowledge. In EMNLP, pages 3507-3512.\n\nContext-dependent semantic parsing for time expressions. Kenton Lee, Yoav Artzi, Jesse Dodge, Luke Zettlemoyer, ACL (1). Kenton Lee, Yoav Artzi, Jesse Dodge, and Luke Zettle- moyer. 2014. Context-dependent semantic parsing for time expressions. In ACL (1), pages 1437-1447.\n\nStructured learning for temporal relation extraction from clinical records. Artuur Leeuwenberg, Marie-Francine Moens, EACL. Artuur Leeuwenberg and Marie-Francine Moens. 2017. Structured learning for temporal relation extraction from clinical records. In EACL.\n\nTemporal information extraction by predicting relative time-lines. Artuur Leeuwenberg, Marie-Francine Moens, EMNLP. Artuur Leeuwenberg and Marie-Francine Moens. 2018. Temporal information extraction by predicting rela- tive time-lines. EMNLP.\n\nConstructing narrative event evolutionary graph for script event prediction. Zhongyang Li, Xiao Ding, Ting Liu, IJCAI. Zhongyang Li, Xiao Ding, and Ting Liu. 2018. Con- structing narrative event evolutionary graph for script event prediction. IJCAI.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\nSemEval-2015 Task 5: QA TEMPEVAL -evaluating temporal information understanding with question answering. Hector Llorens, Nathanael Chambers, Naushad Uzzaman, Nasrin Mostafazadeh, James Allen, James Pustejovsky, SemEval. Hector Llorens, Nathanael Chambers, Naushad UzZa- man, Nasrin Mostafazadeh, James Allen, and James Pustejovsky. 2015. SemEval-2015 Task 5: QA TEMPEVAL -evaluating temporal information un- derstanding with question answering. In SemEval, pages 792-800.\n\nA structured learning approach to temporal relation extraction. Qiang Ning, Zhili Feng, Dan Roth, EMNLP. Qiang Ning, Zhili Feng, and Dan Roth. 2017. A struc- tured learning approach to temporal relation extrac- tion. In EMNLP.\n\nJoint reasoning for temporal and causal relations. Qiang Ning, Zhili Feng, Hao Wu, Dan Roth, ACL. Qiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018a. Joint reasoning for temporal and causal relations. In ACL.\n\nImproving temporal relation extraction with a globally acquired statistical resource. Qiang Ning, Hao Wu, Haoruo Peng, Dan Roth, NAACL. Qiang Ning, Hao Wu, Haoruo Peng, and Dan Roth. 2018b. Improving temporal relation extraction with a globally acquired statistical resource. In NAACL, pages 841-851.\n\nCogCompTime: A tool for understanding time in natural language. Qiang Ning, Ben Zhou, Zhili Feng, Haoruo Peng, Dan Roth, EMNLP. Qiang Ning, Ben Zhou, Zhili Feng, Haoruo Peng, and Dan Roth. 2018c. CogCompTime: A tool for under- standing time in natural language. In EMNLP.\n\nExtending TimeML with typical durations of events. Feng Pan, Rutu Mulkar, Jerry R Hobbs, ARTE. Association for Computational LinguisticsFeng Pan, Rutu Mulkar, and Jerry R Hobbs. 2006. Ex- tending TimeML with typical durations of events. In ARTE, pages 38-45. Association for Computational Linguistics.\n\nKnowSemLM: A Knowledge Infused Semantic Language Model. Haoruo Peng, Qiang Ning, Dan Roth, In CoNLLHaoruo Peng, Qiang Ning, and Dan Roth. 2019. KnowSemLM: A Knowledge Infused Semantic Lan- guage Model. In CoNLL.\n\nDeep contextualized word representations. E Matthew, Mark Peters, Mohit Neumann, Matt Iyyer, Christopher Gardner, Kenton Clark, Luke Lee, Zettlemoyer, Proceedings of NAACL-HLT. NAACL-HLTMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of NAACL-HLT, pages 2227-2237.\n\nThe TIMEBANK corpus. James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, Corpus linguistics. 40James Pustejovsky, Patrick Hanks, Roser Sauri, An- drew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, et al. 2003. The TIMEBANK corpus. In Cor- pus linguistics, volume 2003, page 40.\n\nTimeml annotation guidelines. Roser Saur\u00e9i, Jessica Littman, Bob Knippen, Robert Gaizauskas, Andrea Setzer, James Pustejovsky, Roser Saur\u00e9i, Jessica Littman, Bob Knippen, Robert Gaizauskas, Andrea Setzer, and James Pustejovsky. 2005. Timeml annotation guidelines.\n\nCan we derive general world knowledge from texts? In HLT. Lenhart Schubert, Morgan Kaufmann Publishers IncLenhart Schubert. 2002. Can we derive general world knowledge from texts? In HLT, pages 94-97. Mor- gan Kaufmann Publishers Inc.\n\nSimple bert models for relation extraction and semantic role labeling. Peng Shi, Jimmy Lin, arXiv:1904.05255arXiv preprintPeng Shi and Jimmy Lin. 2019. Simple bert models for relation extraction and semantic role labeling. arXiv preprint arXiv:1904.05255.\n\nHeidel-Time: High quality rule-based extraction and normalization of temporal expressions. Jannik Str\u00f6tgen, Michael Gertz, Association for Computational Linguistics. SemEvalJannik Str\u00f6tgen and Michael Gertz. 2010. Heidel- Time: High quality rule-based extraction and nor- malization of temporal expressions. In SemEval, pages 321-324. Association for Computational Lin- guistics.\n\nReasoning about actions and state changes by injecting commonsense knowledge. Niket Tandon, Bhavana Dalvi, Joel Grus, Wen-Tau Yih, Antoine Bosselut, Peter Clark, In EMNLP. Niket Tandon, Bhavana Dalvi, Joel Grus, Wen-tau Yih, Antoine Bosselut, and Peter Clark. 2018. Reason- ing about actions and state changes by injecting com- monsense knowledge. In EMNLP, pages 57-66.\n\nNaushad Uzzaman, Hector Llorens, James Allen, Leon Derczynski, Marc Verhagen, James Pustejovsky, SemEval-2013 Task 1: TEMPEVAL-3: Evaluating time expressions, events, and temporal relations. *SEM. 2Naushad UzZaman, Hector Llorens, James Allen, Leon Derczynski, Marc Verhagen, and James Pustejovsky. 2013. SemEval-2013 Task 1: TEMPEVAL-3: Eval- uating time expressions, events, and temporal rela- tions. *SEM, 2:1-9.\n\nExtracting implicit knowledge from text. Benjamin Van Durme, University of RochesterPh.D. thesisBenjamin Van Durme. 2009. Extracting implicit knowledge from text. Ph.D. thesis, University of Rochester.\n\nFine-grained temporal relation extraction. Siddharth Vashishtha, Benjamin Van Durme, Aaron Steven White, ACL. Siddharth Vashishtha, Benjamin Van Durme, and Aaron Steven White. 2019. Fine-grained temporal relation extraction. In ACL, pages 2906-2919.\n\nDetermining event durations: Models and error analysis. Alakananda Vempala, Eduardo Blanco, Alexis Palmer, NAACL. 2Alakananda Vempala, Eduardo Blanco, and Alexis Palmer. 2018. Determining event durations: Mod- els and error analysis. In NAACL, volume 2, pages 164-168.\n\nExtracting fine-grained durations for verbs from twitter. Jennifer Williams, ACL-SRW. Association for Computational LinguisticsJennifer Williams. 2012. Extracting fine-grained du- rations for verbs from twitter. In ACL-SRW, pages 49-54. Association for Computational Linguistics.\n\nExtracting commonsense properties from embeddings with limited human guidance. Yiben Yang, Larry Birnbaum, Ji-Ping Wang, Doug Downey, ACL. 2Yiben Yang, Larry Birnbaum, Ji-Ping Wang, and Doug Downey. 2018. Extracting commonsense properties from embeddings with limited human guidance. In ACL, volume 2, pages 644-649.\n\nDefending against neural fake news. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi, NeurIPS. Curran Associates, IncRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. In NeurIPS, pages 9054-9065. Curran Asso- ciates, Inc.\n\nOrdinal common-sense inference. Sheng Zhang, Rachel Rudinger, Kevin Duh, Benjamin Van Durme, TACL. 51Sheng Zhang, Rachel Rudinger, Kevin Duh, and Ben- jamin Van Durme. 2017. Ordinal common-sense in- ference. TACL, 5(1):379-395.\n\nGoing on a vacation\" takes longer than \"Going for a walk\": A Study of Temporal Commonsense Understanding. Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth, EMNLP. Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019. \"Going on a vacation\" takes longer than \"Going for a walk\": A Study of Temporal Common- sense Understanding. In EMNLP.\n", "annotations": {"author": "[{\"end\":235,\"start\":176},{\"end\":291,\"start\":236},{\"end\":353,\"start\":292},{\"end\":414,\"start\":354}]", "publisher": "[{\"end\":101,\"start\":60},{\"end\":676,\"start\":635}]", "author_last_name": "[{\"end\":184,\"start\":180},{\"end\":246,\"start\":242},{\"end\":307,\"start\":299},{\"end\":362,\"start\":358}]", "author_first_name": "[{\"end\":179,\"start\":176},{\"end\":241,\"start\":236},{\"end\":298,\"start\":292},{\"end\":357,\"start\":354}]", "author_affiliation": "[{\"end\":234,\"start\":207},{\"end\":290,\"start\":267},{\"end\":352,\"start\":329},{\"end\":413,\"start\":386}]", "title": "[{\"end\":59,\"start\":1},{\"end\":473,\"start\":415}]", "venue": "[{\"end\":562,\"start\":475}]", "abstract": "[{\"end\":1722,\"start\":703}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2028,\"start\":2006},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2050,\"start\":2028},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2071,\"start\":2050},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2092,\"start\":2071},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2120,\"start\":2092},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2139,\"start\":2120},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2622,\"start\":2606},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2638,\"start\":2622},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2665,\"start\":2638},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2684,\"start\":2665},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2703,\"start\":2684},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2723,\"start\":2703},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2832,\"start\":2813},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4049,\"start\":4029},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5621,\"start\":5601},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5638,\"start\":5621},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7866,\"start\":7838},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7888,\"start\":7866},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7908,\"start\":7888},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7957,\"start\":7938},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7993,\"start\":7973},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8077,\"start\":8057},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8325,\"start\":8305},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8817,\"start\":8791},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8837,\"start\":8817},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8854,\"start\":8837},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8878,\"start\":8854},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8927,\"start\":8909},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8948,\"start\":8927},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8972,\"start\":8948},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9029,\"start\":9000},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9067,\"start\":9048},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9418,\"start\":9400},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9437,\"start\":9418},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9452,\"start\":9437},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9473,\"start\":9452},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9497,\"start\":9473},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9553,\"start\":9525},{\"end\":9574,\"start\":9553},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9677,\"start\":9643},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9693,\"start\":9677},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9711,\"start\":9693},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9783,\"start\":9765},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9856,\"start\":9830},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9996,\"start\":9975},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10110,\"start\":10086},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10687,\"start\":10668},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11419,\"start\":11401},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13049,\"start\":13029},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13104,\"start\":13086},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15753,\"start\":15733},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17274,\"start\":17254},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19895,\"start\":19875},{\"end\":21026,\"start\":21021},{\"end\":21032,\"start\":21027},{\"end\":21105,\"start\":21090},{\"end\":21198,\"start\":21183},{\"end\":21263,\"start\":21258},{\"end\":21297,\"start\":21292},{\"end\":21412,\"start\":21407},{\"end\":21422,\"start\":21417},{\"end\":21471,\"start\":21466},{\"end\":22190,\"start\":22184},{\"end\":22216,\"start\":22210},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24533,\"start\":24511},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26163,\"start\":26138},{\"end\":26646,\"start\":26640},{\"end\":27028,\"start\":27023},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27482,\"start\":27458},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29419,\"start\":29398},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29436,\"start\":29419},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30545,\"start\":30523},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31097,\"start\":31076},{\"end\":31546,\"start\":31541},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32230,\"start\":32211}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34775,\"start\":34494},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34975,\"start\":34776},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35062,\"start\":34976},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35307,\"start\":35063},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35764,\"start\":35308},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36017,\"start\":35765},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36105,\"start\":36018},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":36156,\"start\":36106},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":36240,\"start\":36157},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":36355,\"start\":36241}]", "paragraph": "[{\"end\":2168,\"start\":1738},{\"end\":3256,\"start\":2170},{\"end\":3447,\"start\":3258},{\"end\":4145,\"start\":3449},{\"end\":4960,\"start\":4147},{\"end\":5539,\"start\":4962},{\"end\":6170,\"start\":5541},{\"end\":7568,\"start\":6172},{\"end\":8631,\"start\":7585},{\"end\":9327,\"start\":8633},{\"end\":10899,\"start\":9329},{\"end\":11071,\"start\":10962},{\"end\":11420,\"start\":11081},{\"end\":11879,\"start\":11464},{\"end\":12485,\"start\":11881},{\"end\":12990,\"start\":12521},{\"end\":17803,\"start\":12992},{\"end\":18247,\"start\":17863},{\"end\":19008,\"start\":18283},{\"end\":19517,\"start\":19053},{\"end\":19637,\"start\":19519},{\"end\":20369,\"start\":19670},{\"end\":21248,\"start\":20371},{\"end\":21671,\"start\":21250},{\"end\":22373,\"start\":21673},{\"end\":22878,\"start\":22401},{\"end\":23535,\"start\":22920},{\"end\":24145,\"start\":23537},{\"end\":25127,\"start\":24170},{\"end\":25800,\"start\":25129},{\"end\":27035,\"start\":25802},{\"end\":28228,\"start\":27037},{\"end\":28472,\"start\":28230},{\"end\":29084,\"start\":28474},{\"end\":29808,\"start\":29108},{\"end\":30664,\"start\":29810},{\"end\":31061,\"start\":30697},{\"end\":31410,\"start\":31063},{\"end\":32152,\"start\":31412},{\"end\":33249,\"start\":32184},{\"end\":33693,\"start\":33251},{\"end\":34493,\"start\":33708}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18282,\"start\":18248},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19052,\"start\":19009}]", "table_ref": "[{\"end\":28051,\"start\":28044},{\"end\":28321,\"start\":28314},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30385,\"start\":30378},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31990,\"start\":31983},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":33289,\"start\":33282}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1736,\"start\":1724},{\"attributes\":{\"n\":\"2\"},\"end\":7583,\"start\":7571},{\"attributes\":{\"n\":\"3\"},\"end\":10960,\"start\":10902},{\"attributes\":{\"n\":\"3.1\"},\"end\":11079,\"start\":11074},{\"attributes\":{\"n\":\"3.2\"},\"end\":11462,\"start\":11423},{\"attributes\":{\"n\":\"3.3\"},\"end\":12519,\"start\":12488},{\"attributes\":{\"n\":\"3.4\"},\"end\":17861,\"start\":17806},{\"attributes\":{\"n\":\"3.5\"},\"end\":19668,\"start\":19640},{\"attributes\":{\"n\":\"3.6\"},\"end\":22399,\"start\":22376},{\"attributes\":{\"n\":\"4\"},\"end\":22892,\"start\":22881},{\"attributes\":{\"n\":\"4.1\"},\"end\":22918,\"start\":22895},{\"attributes\":{\"n\":\"4.2\"},\"end\":24168,\"start\":24148},{\"attributes\":{\"n\":\"4.3\"},\"end\":29106,\"start\":29087},{\"attributes\":{\"n\":\"4.4\"},\"end\":30695,\"start\":30667},{\"attributes\":{\"n\":\"4.5\"},\"end\":32182,\"start\":32155},{\"attributes\":{\"n\":\"5\"},\"end\":33706,\"start\":33696},{\"end\":34505,\"start\":34495},{\"end\":34787,\"start\":34777},{\"end\":34987,\"start\":34977},{\"end\":35084,\"start\":35064},{\"end\":36116,\"start\":36107},{\"end\":36167,\"start\":36158},{\"end\":36251,\"start\":36242}]", "table": "[{\"end\":35764,\"start\":35321},{\"end\":36105,\"start\":36054}]", "figure_caption": "[{\"end\":34775,\"start\":34507},{\"end\":34975,\"start\":34789},{\"end\":35062,\"start\":34989},{\"end\":35307,\"start\":35087},{\"end\":35321,\"start\":35310},{\"end\":36017,\"start\":35767},{\"end\":36054,\"start\":36020},{\"end\":36156,\"start\":36118},{\"end\":36240,\"start\":36169},{\"end\":36355,\"start\":36253}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5178,\"start\":5172},{\"end\":6031,\"start\":6025},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6332,\"start\":6326},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12873,\"start\":12867},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17347,\"start\":17339},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17394,\"start\":17388},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17564,\"start\":17558},{\"end\":28582,\"start\":28565}]", "bib_author_first_name": "[{\"end\":37504,\"start\":37499},{\"end\":37514,\"start\":37513},{\"end\":37534,\"start\":37528},{\"end\":37794,\"start\":37788},{\"end\":37818,\"start\":37810},{\"end\":37836,\"start\":37831},{\"end\":37846,\"start\":37843},{\"end\":38090,\"start\":38086},{\"end\":38105,\"start\":38098},{\"end\":38117,\"start\":38112},{\"end\":38323,\"start\":38317},{\"end\":38341,\"start\":38333},{\"end\":38356,\"start\":38350},{\"end\":38367,\"start\":38363},{\"end\":38385,\"start\":38380},{\"end\":38403,\"start\":38399},{\"end\":38663,\"start\":38654},{\"end\":38680,\"start\":38674},{\"end\":38694,\"start\":38690},{\"end\":38711,\"start\":38705},{\"end\":38951,\"start\":38944},{\"end\":38970,\"start\":38963},{\"end\":39182,\"start\":39178},{\"end\":39198,\"start\":39190},{\"end\":39213,\"start\":39208},{\"end\":39231,\"start\":39223},{\"end\":39249,\"start\":39244},{\"end\":39542,\"start\":39537},{\"end\":39559,\"start\":39551},{\"end\":39573,\"start\":39567},{\"end\":39587,\"start\":39579},{\"end\":39852,\"start\":39847},{\"end\":39868,\"start\":39861},{\"end\":39884,\"start\":39878},{\"end\":39904,\"start\":39899},{\"end\":39922,\"start\":39919},{\"end\":40184,\"start\":40177},{\"end\":40198,\"start\":40193},{\"end\":40416,\"start\":40412},{\"end\":40430,\"start\":40426},{\"end\":40441,\"start\":40437},{\"end\":40457,\"start\":40451},{\"end\":40474,\"start\":40467},{\"end\":40484,\"start\":40483},{\"end\":40500,\"start\":40493},{\"end\":40513,\"start\":40506},{\"end\":40526,\"start\":40522},{\"end\":40869,\"start\":40864},{\"end\":40892,\"start\":40878},{\"end\":40911,\"start\":40905},{\"end\":41296,\"start\":41288},{\"end\":41313,\"start\":41305},{\"end\":41503,\"start\":41498},{\"end\":41516,\"start\":41511},{\"end\":41525,\"start\":41523},{\"end\":41539,\"start\":41532},{\"end\":41775,\"start\":41771},{\"end\":41787,\"start\":41786},{\"end\":41804,\"start\":41797},{\"end\":41816,\"start\":41805},{\"end\":42039,\"start\":42033},{\"end\":42056,\"start\":42047},{\"end\":42073,\"start\":42067},{\"end\":42088,\"start\":42083},{\"end\":42105,\"start\":42099},{\"end\":42118,\"start\":42115},{\"end\":42393,\"start\":42388},{\"end\":42404,\"start\":42401},{\"end\":42416,\"start\":42410},{\"end\":42430,\"start\":42422},{\"end\":42664,\"start\":42658},{\"end\":42674,\"start\":42670},{\"end\":42687,\"start\":42682},{\"end\":42699,\"start\":42695},{\"end\":42958,\"start\":42952},{\"end\":42986,\"start\":42972},{\"end\":43210,\"start\":43204},{\"end\":43238,\"start\":43224},{\"end\":43467,\"start\":43458},{\"end\":43476,\"start\":43472},{\"end\":43487,\"start\":43483},{\"end\":43638,\"start\":43632},{\"end\":43648,\"start\":43644},{\"end\":43659,\"start\":43654},{\"end\":43674,\"start\":43667},{\"end\":43685,\"start\":43679},{\"end\":43698,\"start\":43693},{\"end\":43709,\"start\":43705},{\"end\":43720,\"start\":43716},{\"end\":43732,\"start\":43728},{\"end\":43753,\"start\":43746},{\"end\":44199,\"start\":44193},{\"end\":44218,\"start\":44209},{\"end\":44236,\"start\":44229},{\"end\":44252,\"start\":44246},{\"end\":44272,\"start\":44267},{\"end\":44285,\"start\":44280},{\"end\":44630,\"start\":44625},{\"end\":44642,\"start\":44637},{\"end\":44652,\"start\":44649},{\"end\":44845,\"start\":44840},{\"end\":44857,\"start\":44852},{\"end\":44867,\"start\":44864},{\"end\":44875,\"start\":44872},{\"end\":45091,\"start\":45086},{\"end\":45101,\"start\":45098},{\"end\":45112,\"start\":45106},{\"end\":45122,\"start\":45119},{\"end\":45371,\"start\":45366},{\"end\":45381,\"start\":45378},{\"end\":45393,\"start\":45388},{\"end\":45406,\"start\":45400},{\"end\":45416,\"start\":45413},{\"end\":45630,\"start\":45626},{\"end\":45640,\"start\":45636},{\"end\":45654,\"start\":45649},{\"end\":45656,\"start\":45655},{\"end\":45940,\"start\":45934},{\"end\":45952,\"start\":45947},{\"end\":45962,\"start\":45959},{\"end\":46134,\"start\":46133},{\"end\":46148,\"start\":46144},{\"end\":46162,\"start\":46157},{\"end\":46176,\"start\":46172},{\"end\":46195,\"start\":46184},{\"end\":46211,\"start\":46205},{\"end\":46223,\"start\":46219},{\"end\":46512,\"start\":46507},{\"end\":46533,\"start\":46526},{\"end\":46546,\"start\":46541},{\"end\":46560,\"start\":46554},{\"end\":46572,\"start\":46566},{\"end\":46591,\"start\":46585},{\"end\":46608,\"start\":46600},{\"end\":46620,\"start\":46616},{\"end\":46636,\"start\":46631},{\"end\":46646,\"start\":46642},{\"end\":46942,\"start\":46937},{\"end\":46958,\"start\":46951},{\"end\":46971,\"start\":46968},{\"end\":46987,\"start\":46981},{\"end\":47006,\"start\":47000},{\"end\":47020,\"start\":47015},{\"end\":47237,\"start\":47230},{\"end\":47483,\"start\":47479},{\"end\":47494,\"start\":47489},{\"end\":47762,\"start\":47756},{\"end\":47780,\"start\":47773},{\"end\":48129,\"start\":48124},{\"end\":48145,\"start\":48138},{\"end\":48157,\"start\":48153},{\"end\":48171,\"start\":48164},{\"end\":48184,\"start\":48177},{\"end\":48200,\"start\":48195},{\"end\":48425,\"start\":48418},{\"end\":48441,\"start\":48435},{\"end\":48456,\"start\":48451},{\"end\":48468,\"start\":48464},{\"end\":48485,\"start\":48481},{\"end\":48501,\"start\":48496},{\"end\":48884,\"start\":48876},{\"end\":49090,\"start\":49081},{\"end\":49111,\"start\":49103},{\"end\":49128,\"start\":49123},{\"end\":49135,\"start\":49129},{\"end\":49355,\"start\":49345},{\"end\":49372,\"start\":49365},{\"end\":49387,\"start\":49381},{\"end\":49625,\"start\":49617},{\"end\":49924,\"start\":49919},{\"end\":49936,\"start\":49931},{\"end\":49954,\"start\":49947},{\"end\":49965,\"start\":49961},{\"end\":50199,\"start\":50194},{\"end\":50212,\"start\":50209},{\"end\":50229,\"start\":50223},{\"end\":50246,\"start\":50239},{\"end\":50256,\"start\":50253},{\"end\":50275,\"start\":50266},{\"end\":50290,\"start\":50285},{\"end\":50570,\"start\":50565},{\"end\":50584,\"start\":50578},{\"end\":50600,\"start\":50595},{\"end\":50614,\"start\":50606},{\"end\":50871,\"start\":50868},{\"end\":50884,\"start\":50878},{\"end\":50900,\"start\":50895},{\"end\":50910,\"start\":50907}]", "bib_author_last_name": "[{\"end\":37511,\"start\":37505},{\"end\":37526,\"start\":37515},{\"end\":37542,\"start\":37535},{\"end\":37552,\"start\":37544},{\"end\":37808,\"start\":37795},{\"end\":37829,\"start\":37819},{\"end\":37841,\"start\":37837},{\"end\":37854,\"start\":37847},{\"end\":38096,\"start\":38091},{\"end\":38110,\"start\":38106},{\"end\":38124,\"start\":38118},{\"end\":38331,\"start\":38324},{\"end\":38348,\"start\":38342},{\"end\":38361,\"start\":38357},{\"end\":38378,\"start\":38368},{\"end\":38397,\"start\":38386},{\"end\":38412,\"start\":38404},{\"end\":38672,\"start\":38664},{\"end\":38688,\"start\":38681},{\"end\":38703,\"start\":38695},{\"end\":38719,\"start\":38712},{\"end\":38961,\"start\":38952},{\"end\":38977,\"start\":38971},{\"end\":39188,\"start\":39183},{\"end\":39206,\"start\":39199},{\"end\":39221,\"start\":39214},{\"end\":39242,\"start\":39232},{\"end\":39264,\"start\":39250},{\"end\":39549,\"start\":39543},{\"end\":39565,\"start\":39560},{\"end\":39577,\"start\":39574},{\"end\":39597,\"start\":39588},{\"end\":39859,\"start\":39853},{\"end\":39876,\"start\":39869},{\"end\":39897,\"start\":39885},{\"end\":39917,\"start\":39905},{\"end\":39927,\"start\":39923},{\"end\":40191,\"start\":40185},{\"end\":40203,\"start\":40199},{\"end\":40424,\"start\":40417},{\"end\":40435,\"start\":40431},{\"end\":40449,\"start\":40442},{\"end\":40465,\"start\":40458},{\"end\":40481,\"start\":40475},{\"end\":40491,\"start\":40485},{\"end\":40504,\"start\":40501},{\"end\":40520,\"start\":40514},{\"end\":40534,\"start\":40527},{\"end\":40547,\"start\":40536},{\"end\":40876,\"start\":40870},{\"end\":40903,\"start\":40893},{\"end\":40917,\"start\":40912},{\"end\":40931,\"start\":40919},{\"end\":41303,\"start\":41297},{\"end\":41323,\"start\":41314},{\"end\":41509,\"start\":41504},{\"end\":41521,\"start\":41517},{\"end\":41530,\"start\":41526},{\"end\":41545,\"start\":41540},{\"end\":41784,\"start\":41776},{\"end\":41795,\"start\":41788},{\"end\":41822,\"start\":41817},{\"end\":42045,\"start\":42040},{\"end\":42065,\"start\":42057},{\"end\":42081,\"start\":42074},{\"end\":42097,\"start\":42089},{\"end\":42113,\"start\":42106},{\"end\":42127,\"start\":42119},{\"end\":42399,\"start\":42394},{\"end\":42408,\"start\":42405},{\"end\":42420,\"start\":42417},{\"end\":42436,\"start\":42431},{\"end\":42668,\"start\":42665},{\"end\":42680,\"start\":42675},{\"end\":42693,\"start\":42688},{\"end\":42711,\"start\":42700},{\"end\":42970,\"start\":42959},{\"end\":42992,\"start\":42987},{\"end\":43222,\"start\":43211},{\"end\":43244,\"start\":43239},{\"end\":43470,\"start\":43468},{\"end\":43481,\"start\":43477},{\"end\":43491,\"start\":43488},{\"end\":43642,\"start\":43639},{\"end\":43652,\"start\":43649},{\"end\":43665,\"start\":43660},{\"end\":43677,\"start\":43675},{\"end\":43691,\"start\":43686},{\"end\":43703,\"start\":43699},{\"end\":43714,\"start\":43710},{\"end\":43726,\"start\":43721},{\"end\":43744,\"start\":43733},{\"end\":43762,\"start\":43754},{\"end\":44207,\"start\":44200},{\"end\":44227,\"start\":44219},{\"end\":44244,\"start\":44237},{\"end\":44265,\"start\":44253},{\"end\":44278,\"start\":44273},{\"end\":44297,\"start\":44286},{\"end\":44635,\"start\":44631},{\"end\":44647,\"start\":44643},{\"end\":44657,\"start\":44653},{\"end\":44850,\"start\":44846},{\"end\":44862,\"start\":44858},{\"end\":44870,\"start\":44868},{\"end\":44880,\"start\":44876},{\"end\":45096,\"start\":45092},{\"end\":45104,\"start\":45102},{\"end\":45117,\"start\":45113},{\"end\":45127,\"start\":45123},{\"end\":45376,\"start\":45372},{\"end\":45386,\"start\":45382},{\"end\":45398,\"start\":45394},{\"end\":45411,\"start\":45407},{\"end\":45421,\"start\":45417},{\"end\":45634,\"start\":45631},{\"end\":45647,\"start\":45641},{\"end\":45662,\"start\":45657},{\"end\":45945,\"start\":45941},{\"end\":45957,\"start\":45953},{\"end\":45967,\"start\":45963},{\"end\":46142,\"start\":46135},{\"end\":46155,\"start\":46149},{\"end\":46170,\"start\":46163},{\"end\":46182,\"start\":46177},{\"end\":46203,\"start\":46196},{\"end\":46217,\"start\":46212},{\"end\":46227,\"start\":46224},{\"end\":46240,\"start\":46229},{\"end\":46524,\"start\":46513},{\"end\":46539,\"start\":46534},{\"end\":46552,\"start\":46547},{\"end\":46564,\"start\":46561},{\"end\":46583,\"start\":46573},{\"end\":46598,\"start\":46592},{\"end\":46614,\"start\":46609},{\"end\":46629,\"start\":46621},{\"end\":46640,\"start\":46637},{\"end\":46652,\"start\":46647},{\"end\":46949,\"start\":46943},{\"end\":46966,\"start\":46959},{\"end\":46979,\"start\":46972},{\"end\":46998,\"start\":46988},{\"end\":47013,\"start\":47007},{\"end\":47032,\"start\":47021},{\"end\":47246,\"start\":47238},{\"end\":47487,\"start\":47484},{\"end\":47498,\"start\":47495},{\"end\":47771,\"start\":47763},{\"end\":47786,\"start\":47781},{\"end\":48136,\"start\":48130},{\"end\":48151,\"start\":48146},{\"end\":48162,\"start\":48158},{\"end\":48175,\"start\":48172},{\"end\":48193,\"start\":48185},{\"end\":48206,\"start\":48201},{\"end\":48433,\"start\":48426},{\"end\":48449,\"start\":48442},{\"end\":48462,\"start\":48457},{\"end\":48479,\"start\":48469},{\"end\":48494,\"start\":48486},{\"end\":48513,\"start\":48502},{\"end\":48894,\"start\":48885},{\"end\":49101,\"start\":49091},{\"end\":49121,\"start\":49112},{\"end\":49141,\"start\":49136},{\"end\":49363,\"start\":49356},{\"end\":49379,\"start\":49373},{\"end\":49394,\"start\":49388},{\"end\":49634,\"start\":49626},{\"end\":49929,\"start\":49925},{\"end\":49945,\"start\":49937},{\"end\":49959,\"start\":49955},{\"end\":49972,\"start\":49966},{\"end\":50207,\"start\":50200},{\"end\":50221,\"start\":50213},{\"end\":50237,\"start\":50230},{\"end\":50251,\"start\":50247},{\"end\":50264,\"start\":50257},{\"end\":50283,\"start\":50276},{\"end\":50295,\"start\":50291},{\"end\":50576,\"start\":50571},{\"end\":50593,\"start\":50585},{\"end\":50604,\"start\":50601},{\"end\":50624,\"start\":50615},{\"end\":50876,\"start\":50872},{\"end\":50893,\"start\":50885},{\"end\":50905,\"start\":50901},{\"end\":50915,\"start\":50911}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":627938},\"end\":37713,\"start\":37445},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8800142},\"end\":38021,\"start\":37715},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52290656},\"end\":38274,\"start\":38023},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":62948965},\"end\":38599,\"start\":38276},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1564278},\"end\":38874,\"start\":38601},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13507979},\"end\":39122,\"start\":38876},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52035514},\"end\":39453,\"start\":39124},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":39771,\"start\":39455},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":174798375},\"end\":40109,\"start\":39773},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10543068},\"end\":40346,\"start\":40111},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3994096},\"end\":40794,\"start\":40348},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13984086},\"end\":41244,\"start\":40796},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16567195},\"end\":41450,\"start\":41246},{\"attributes\":{\"id\":\"b13\"},\"end\":41689,\"start\":41452},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":35860340},\"end\":41977,\"start\":41691},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":293098},\"end\":42318,\"start\":41979},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":201103745},\"end\":42599,\"start\":42320},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":641333},\"end\":42874,\"start\":42601},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":17894632},\"end\":43135,\"start\":42876},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52111780},\"end\":43379,\"start\":43137},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":21723549},\"end\":43630,\"start\":43381},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b21\"},\"end\":44086,\"start\":43632},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7849314},\"end\":44559,\"start\":44088},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":28982109},\"end\":44787,\"start\":44561},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":51878335},\"end\":44998,\"start\":44789},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4957206},\"end\":45300,\"start\":45000},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53223504},\"end\":45573,\"start\":45302},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2896894},\"end\":45876,\"start\":45575},{\"attributes\":{\"id\":\"b28\"},\"end\":46089,\"start\":45878},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3626819},\"end\":46484,\"start\":46091},{\"attributes\":{\"id\":\"b30\"},\"end\":46905,\"start\":46486},{\"attributes\":{\"id\":\"b31\"},\"end\":47170,\"start\":46907},{\"attributes\":{\"id\":\"b32\"},\"end\":47406,\"start\":47172},{\"attributes\":{\"doi\":\"arXiv:1904.05255\",\"id\":\"b33\"},\"end\":47663,\"start\":47408},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6353806},\"end\":48044,\"start\":47665},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52136770},\"end\":48416,\"start\":48046},{\"attributes\":{\"id\":\"b36\"},\"end\":48833,\"start\":48418},{\"attributes\":{\"id\":\"b37\"},\"end\":49036,\"start\":48835},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":59599681},\"end\":49287,\"start\":49038},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":44092913},\"end\":49557,\"start\":49289},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9159475},\"end\":49838,\"start\":49559},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":48356558},\"end\":50156,\"start\":49840},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":168169824},\"end\":50531,\"start\":50158},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1461182},\"end\":50760,\"start\":50533},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":202541184},\"end\":51101,\"start\":50762}]", "bib_title": "[{\"end\":37497,\"start\":37445},{\"end\":37786,\"start\":37715},{\"end\":38084,\"start\":38023},{\"end\":38315,\"start\":38276},{\"end\":38652,\"start\":38601},{\"end\":38942,\"start\":38876},{\"end\":39176,\"start\":39124},{\"end\":39535,\"start\":39455},{\"end\":39845,\"start\":39773},{\"end\":40175,\"start\":40111},{\"end\":40410,\"start\":40348},{\"end\":40862,\"start\":40796},{\"end\":41286,\"start\":41246},{\"end\":41769,\"start\":41691},{\"end\":42031,\"start\":41979},{\"end\":42386,\"start\":42320},{\"end\":42656,\"start\":42601},{\"end\":42950,\"start\":42876},{\"end\":43202,\"start\":43137},{\"end\":43456,\"start\":43381},{\"end\":44191,\"start\":44088},{\"end\":44623,\"start\":44561},{\"end\":44838,\"start\":44789},{\"end\":45084,\"start\":45000},{\"end\":45364,\"start\":45302},{\"end\":45624,\"start\":45575},{\"end\":46131,\"start\":46091},{\"end\":46505,\"start\":46486},{\"end\":47754,\"start\":47665},{\"end\":48122,\"start\":48046},{\"end\":49079,\"start\":49038},{\"end\":49343,\"start\":49289},{\"end\":49615,\"start\":49559},{\"end\":49917,\"start\":49840},{\"end\":50192,\"start\":50158},{\"end\":50563,\"start\":50533},{\"end\":50866,\"start\":50762}]", "bib_author": "[{\"end\":37513,\"start\":37499},{\"end\":37528,\"start\":37513},{\"end\":37544,\"start\":37528},{\"end\":37554,\"start\":37544},{\"end\":37810,\"start\":37788},{\"end\":37831,\"start\":37810},{\"end\":37843,\"start\":37831},{\"end\":37856,\"start\":37843},{\"end\":38098,\"start\":38086},{\"end\":38112,\"start\":38098},{\"end\":38126,\"start\":38112},{\"end\":38333,\"start\":38317},{\"end\":38350,\"start\":38333},{\"end\":38363,\"start\":38350},{\"end\":38380,\"start\":38363},{\"end\":38399,\"start\":38380},{\"end\":38414,\"start\":38399},{\"end\":38674,\"start\":38654},{\"end\":38690,\"start\":38674},{\"end\":38705,\"start\":38690},{\"end\":38721,\"start\":38705},{\"end\":38963,\"start\":38944},{\"end\":38979,\"start\":38963},{\"end\":39190,\"start\":39178},{\"end\":39208,\"start\":39190},{\"end\":39223,\"start\":39208},{\"end\":39244,\"start\":39223},{\"end\":39266,\"start\":39244},{\"end\":39551,\"start\":39537},{\"end\":39567,\"start\":39551},{\"end\":39579,\"start\":39567},{\"end\":39599,\"start\":39579},{\"end\":39861,\"start\":39847},{\"end\":39878,\"start\":39861},{\"end\":39899,\"start\":39878},{\"end\":39919,\"start\":39899},{\"end\":39929,\"start\":39919},{\"end\":40193,\"start\":40177},{\"end\":40205,\"start\":40193},{\"end\":40426,\"start\":40412},{\"end\":40437,\"start\":40426},{\"end\":40451,\"start\":40437},{\"end\":40467,\"start\":40451},{\"end\":40483,\"start\":40467},{\"end\":40493,\"start\":40483},{\"end\":40506,\"start\":40493},{\"end\":40522,\"start\":40506},{\"end\":40536,\"start\":40522},{\"end\":40549,\"start\":40536},{\"end\":40878,\"start\":40864},{\"end\":40905,\"start\":40878},{\"end\":40919,\"start\":40905},{\"end\":40933,\"start\":40919},{\"end\":41305,\"start\":41288},{\"end\":41325,\"start\":41305},{\"end\":41511,\"start\":41498},{\"end\":41523,\"start\":41511},{\"end\":41532,\"start\":41523},{\"end\":41547,\"start\":41532},{\"end\":41786,\"start\":41771},{\"end\":41797,\"start\":41786},{\"end\":41824,\"start\":41797},{\"end\":42047,\"start\":42033},{\"end\":42067,\"start\":42047},{\"end\":42083,\"start\":42067},{\"end\":42099,\"start\":42083},{\"end\":42115,\"start\":42099},{\"end\":42129,\"start\":42115},{\"end\":42401,\"start\":42388},{\"end\":42410,\"start\":42401},{\"end\":42422,\"start\":42410},{\"end\":42438,\"start\":42422},{\"end\":42670,\"start\":42658},{\"end\":42682,\"start\":42670},{\"end\":42695,\"start\":42682},{\"end\":42713,\"start\":42695},{\"end\":42972,\"start\":42952},{\"end\":42994,\"start\":42972},{\"end\":43224,\"start\":43204},{\"end\":43246,\"start\":43224},{\"end\":43472,\"start\":43458},{\"end\":43483,\"start\":43472},{\"end\":43493,\"start\":43483},{\"end\":43644,\"start\":43632},{\"end\":43654,\"start\":43644},{\"end\":43667,\"start\":43654},{\"end\":43679,\"start\":43667},{\"end\":43693,\"start\":43679},{\"end\":43705,\"start\":43693},{\"end\":43716,\"start\":43705},{\"end\":43728,\"start\":43716},{\"end\":43746,\"start\":43728},{\"end\":43764,\"start\":43746},{\"end\":44209,\"start\":44193},{\"end\":44229,\"start\":44209},{\"end\":44246,\"start\":44229},{\"end\":44267,\"start\":44246},{\"end\":44280,\"start\":44267},{\"end\":44299,\"start\":44280},{\"end\":44637,\"start\":44625},{\"end\":44649,\"start\":44637},{\"end\":44659,\"start\":44649},{\"end\":44852,\"start\":44840},{\"end\":44864,\"start\":44852},{\"end\":44872,\"start\":44864},{\"end\":44882,\"start\":44872},{\"end\":45098,\"start\":45086},{\"end\":45106,\"start\":45098},{\"end\":45119,\"start\":45106},{\"end\":45129,\"start\":45119},{\"end\":45378,\"start\":45366},{\"end\":45388,\"start\":45378},{\"end\":45400,\"start\":45388},{\"end\":45413,\"start\":45400},{\"end\":45423,\"start\":45413},{\"end\":45636,\"start\":45626},{\"end\":45649,\"start\":45636},{\"end\":45664,\"start\":45649},{\"end\":45947,\"start\":45934},{\"end\":45959,\"start\":45947},{\"end\":45969,\"start\":45959},{\"end\":46144,\"start\":46133},{\"end\":46157,\"start\":46144},{\"end\":46172,\"start\":46157},{\"end\":46184,\"start\":46172},{\"end\":46205,\"start\":46184},{\"end\":46219,\"start\":46205},{\"end\":46229,\"start\":46219},{\"end\":46242,\"start\":46229},{\"end\":46526,\"start\":46507},{\"end\":46541,\"start\":46526},{\"end\":46554,\"start\":46541},{\"end\":46566,\"start\":46554},{\"end\":46585,\"start\":46566},{\"end\":46600,\"start\":46585},{\"end\":46616,\"start\":46600},{\"end\":46631,\"start\":46616},{\"end\":46642,\"start\":46631},{\"end\":46654,\"start\":46642},{\"end\":46951,\"start\":46937},{\"end\":46968,\"start\":46951},{\"end\":46981,\"start\":46968},{\"end\":47000,\"start\":46981},{\"end\":47015,\"start\":47000},{\"end\":47034,\"start\":47015},{\"end\":47248,\"start\":47230},{\"end\":47489,\"start\":47479},{\"end\":47500,\"start\":47489},{\"end\":47773,\"start\":47756},{\"end\":47788,\"start\":47773},{\"end\":48138,\"start\":48124},{\"end\":48153,\"start\":48138},{\"end\":48164,\"start\":48153},{\"end\":48177,\"start\":48164},{\"end\":48195,\"start\":48177},{\"end\":48208,\"start\":48195},{\"end\":48435,\"start\":48418},{\"end\":48451,\"start\":48435},{\"end\":48464,\"start\":48451},{\"end\":48481,\"start\":48464},{\"end\":48496,\"start\":48481},{\"end\":48515,\"start\":48496},{\"end\":48896,\"start\":48876},{\"end\":49103,\"start\":49081},{\"end\":49123,\"start\":49103},{\"end\":49143,\"start\":49123},{\"end\":49365,\"start\":49345},{\"end\":49381,\"start\":49365},{\"end\":49396,\"start\":49381},{\"end\":49636,\"start\":49617},{\"end\":49931,\"start\":49919},{\"end\":49947,\"start\":49931},{\"end\":49961,\"start\":49947},{\"end\":49974,\"start\":49961},{\"end\":50209,\"start\":50194},{\"end\":50223,\"start\":50209},{\"end\":50239,\"start\":50223},{\"end\":50253,\"start\":50239},{\"end\":50266,\"start\":50253},{\"end\":50285,\"start\":50266},{\"end\":50297,\"start\":50285},{\"end\":50578,\"start\":50565},{\"end\":50595,\"start\":50578},{\"end\":50606,\"start\":50595},{\"end\":50626,\"start\":50606},{\"end\":50878,\"start\":50868},{\"end\":50895,\"start\":50878},{\"end\":50907,\"start\":50895},{\"end\":50917,\"start\":50907}]", "bib_venue": "[{\"end\":40957,\"start\":40939},{\"end\":46277,\"start\":46268},{\"end\":37563,\"start\":37554},{\"end\":37860,\"start\":37856},{\"end\":38131,\"start\":38126},{\"end\":38421,\"start\":38414},{\"end\":38725,\"start\":38721},{\"end\":38984,\"start\":38979},{\"end\":39271,\"start\":39266},{\"end\":39604,\"start\":39599},{\"end\":39932,\"start\":39929},{\"end\":40208,\"start\":40205},{\"end\":40556,\"start\":40549},{\"end\":40937,\"start\":40933},{\"end\":41329,\"start\":41325},{\"end\":41496,\"start\":41452},{\"end\":41827,\"start\":41824},{\"end\":42133,\"start\":42129},{\"end\":42443,\"start\":42438},{\"end\":42720,\"start\":42713},{\"end\":42998,\"start\":42994},{\"end\":43251,\"start\":43246},{\"end\":43498,\"start\":43493},{\"end\":43835,\"start\":43780},{\"end\":44306,\"start\":44299},{\"end\":44664,\"start\":44659},{\"end\":44885,\"start\":44882},{\"end\":45134,\"start\":45129},{\"end\":45428,\"start\":45423},{\"end\":45668,\"start\":45664},{\"end\":45932,\"start\":45878},{\"end\":46266,\"start\":46242},{\"end\":46672,\"start\":46654},{\"end\":46935,\"start\":46907},{\"end\":47228,\"start\":47172},{\"end\":47477,\"start\":47408},{\"end\":47829,\"start\":47788},{\"end\":48216,\"start\":48208},{\"end\":48613,\"start\":48515},{\"end\":48874,\"start\":48835},{\"end\":49146,\"start\":49143},{\"end\":49401,\"start\":49396},{\"end\":49643,\"start\":49636},{\"end\":49977,\"start\":49974},{\"end\":50304,\"start\":50297},{\"end\":50630,\"start\":50626},{\"end\":50922,\"start\":50917}]"}}}, "year": 2023, "month": 12, "day": 17}