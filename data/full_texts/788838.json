{"id": 788838, "updated": "2023-10-08 03:33:12.741", "metadata": {"title": "Domain Adaptation via Transfer Component Analysis", "authors": "[{\"first\":\"Sinno\",\"last\":\"Pan\",\"middle\":[\"Jialin\"]},{\"first\":\"Ivor\",\"last\":\"Tsang\",\"middle\":[\"W\"]},{\"first\":\"James\",\"last\":\"Kwok\",\"middle\":[\"T\"]},{\"first\":\"Qiang\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "IEEE Transactions on Neural Networks", "journal": "IEEE Transactions on Neural Networks", "publication_date": {"year": 2011, "month": null, "day": null}, "abstract": "Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "304676660", "acl": null, "pubmed": "21095864", "pubmedcentral": null, "dblp": "journals/tnn/PanTKY11", "doi": "10.1109/tnn.2010.2091281"}}, "content": {"source": {"pdf_hash": "bed9479a3eb5ff2656f82aa32c02c6f8af963880", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://ijcai.org/papers09/Papers/IJCAI09-200.pdf", "status": "GREEN"}}, "grobid": {"id": "23097ef7cbd7e250b8d0beb8b1cd977c9b8234fc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bed9479a3eb5ff2656f82aa32c02c6f8af963880.txt", "contents": "\nDomain Adaptation via Transfer Component Analysis\n\n\nSinno Jialin Pan \nDepartment of Computer Science and Engineering\nHong Kong University of Science and Technology\nHong Kong\n\nIvor W Tsang 2ivortsang@ntu.edu.sg \nSchool of Computer Engineering\nNanyang Technological University\n639798Singapore\n\nJames T Kwok \nDepartment of Computer Science and Engineering\nHong Kong University of Science and Technology\nHong Kong\n\nQiang Yang qyang@cse.ust.hk \nDepartment of Computer Science and Engineering\nHong Kong University of Science and Technology\nHong Kong\n\nDomain Adaptation via Transfer Component Analysis\n\nDomain adaptation solves a learning problem in a target domain by utilizing the training data in a different but related source domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a Reproducing Kernel Hilbert Space (RKHS) using Maximum Mean Discrepancy (MMD). In the subspace spanned by these transfer components, data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. The main contribution of our work is that we propose a novel feature representation in which to perform domain adaptation via a new parametric kernel using feature extraction methods, which can dramatically minimize the distance between domain distributions by projecting data onto the learned transfer components. Furthermore, our approach can handle large datsets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach in are verified by experiments on two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.\n\nIntroduction\n\nDomain adaptation aims at adapting a classifier or regression model trained in a source domain for use in a target domain, where the source and target domains may be different but related. This is particularly crucial when labeled data are in short supply in the target domain. For example, in indoor WiFi localization, it is very expensive to calibrate a localization model in a large-scale environment. However, the WiFi signal strength may be a function of time, device or space, depending on dynamic factors. To reduce the re-calibration effort, we might want to adapt a localization model trained in one time period (the source domain) for a new time period (the target domain), or to adapt the localization model trained on one mobile device (the source domain) for a new mobile device (the target domain). However, the distributions of WiFi data collected over time or across devices may be very different, hence domain adaptation is needed [Yang et al., 2008]. Another example is sentiment classification. To reduce the effort of annotating reviews for various products, we might want to adapt a learning system trained on some types of products (the source domain) for a new type of product (the target domain). However, terms used in the reviews of different types of products may be very different. As a result, distributions of the data over different types of products may be different and thus domain adaptation is again needed .\n\nA major computational problem in domain adaptation is how to reduce the difference between the distributions of source and target domain data. Intuitively, discovering a good feature representation across domains is crucial. A good feature representation should be able to reduce the difference in distributions between domains as much as possible, while at the same time preserving important (geometric or statistical) properties of the original data.\n\nRecently, several approaches have been proposed to learn a common feature representation for domain adaptation [Daum\u00e9 III, 2007;]. Daum\u00e9 III [2007] proposed a simple heuristic nonlinear mapping function to map the data from both source and target domains to a highdimensional feature space, where standard machine learning methods are used to train classifiers.  proposed the so-called structural correspondence learning (SCL) algorithm to induce correspondences among features from the different domains. This method depends on the heuristic selections of pivot features that appear frequently in both domains. Although it is experimentally shown that SCL can reduce the difference between domains based on the A-distance measure [Ben-David et al., 2007], the heuristic criterion of pivot feature selection may be sensitive to different applications. Pan et al. [2008] proposed a new dimensionality reduction method, Maximum Mean Discrepancy Embedding (MMDE), for domain adaptation. The motivation of MMDE is similar to our proposed work. It also aims at learning a shared latent space underlying the domains where distance between distributions can be reduced. However, MMDE suf-fers from two major limitations: (1) MMDE is transductive, and does not generalize to out-of-sample patterns; (2) MMDE learns the latent space by solving a semi-definite program (SDP), which is a very expensive optimization problem.\n\nIn this paper, we propose a new feature extraction approach, called transfer component analysis (TCA), for domain adaptation. It tries to learn a set of common transfer components underlying both domains such that the difference in distributions of data in the different domains, when projected onto this subspace, can be dramatically reduced. Then, standard machine learning methods can be used in this subspace to train classifiers or regression models across domains. More specifically, if two domains are related to each other, there may exist several common components (or latent variables) underlying them. Some of these components may cause the data distributions between domains to be different, while others may not. Some of these components may capture the intrinsic structure underlying the original data, while others may not. Our goal is to discover those components that do not cause distribution change across the domains and capture the structure of the original data well. We will show in this paper that, compared to MMDE, TCA is much more efficient and can handle the out-of-sample extension problem.\n\nThe rest of the paper is organized as follows. Section 2 first describes the problem statement and preliminaries of domain adaptation. Our proposed method is presented in Section 3. We then review some related works in Section 4. In Section 5, we conduct a series of experiments on indoor WiFi localization and text classification. The last section gives some conclusive discussions.\n\nIn the sequel, A 0 (resp. A 0) means that the matrix A is symmetric and positive definite (pd) (resp. positive semidefinite (psd)). Moreover, the transpose of vector / matrix (in both the input and feature spaces) is denoted by the superscript , A \u2020 is the pseudo-inverse of the matrix A, and tr(A) denotes the trace of A.\n\n\nPreliminaries of Domain Adaptation\n\nIn this paper, we focus on the setting where the target domain has no labeled training data, but has plenty of unlabeled data. We also assume that some labeled data D S are available in a source domain, while only unlabeled data D T are available in the target domain. We denote the source domain data as D S = {(x S1 , y S1 ), . . . , (x Sn 1 , y Sn 1 )}, where x Si \u2208 X is the input and y Si \u2208 Y is the corresponding output. Similarly, we denote the target domain data as D T = {x T1 , . . . , x Tn 2 }, where the input x Ti is also in X . Let P(X S ) and Q(X T ) (or P and Q for short) be the marginal distributions of X S and X T , respectively. In general, P and Q can be different. Our task is then to predict the labels y Ti 's corresponding to the inputs x Ti 's in the target domain. The key assumption in a typical domain adaptation setting is that P = Q, but P (Y S |X S ) = P (Y T |X T ) .\n\n\nMaximum Mean Discrepancy\n\nMany criteria, such as the Kullback-Leibler (KL) divergence, can be used to estimate the distance between distributions.\n\nHowever, many of these criteria are parametric, since an intermediate density estimate is usually required. To avoid such a non-trivial task, a non-parametric distance estimate between distributions is more desirable. Recently, Borgwardt et al. [2006] proposed the Maximum Mean Discrepancy (MMD) as a relevant criterion for comparing distributions based on the Reproducing Kernel Hilbert Space (RKHS). Let X = {x 1 , . . . , x n1 } and Y = {y 1 , . . . , y n2 } be random variable sets with distributions P and Q. The empirical estimate of the distance between P and Q, as defined by MMD, is\nDist(X,Y) = 1 n1 n1 i=1 \u03c6(x i ) \u2212 1 n2 n2 i=1 \u03c6(y i ) H .\n( 1) where H is a universal RKHS [Steinwart, 2001], and \u03c6 : X \u2192 H. Therefore, the distance between distributions of two samples can be well-estimated by the distance between the means of the two samples mapped into a RKHS.\n\n\nTransfer Component Analysis\n\nBased on the inputs {x Si } and outputs {y Si } from the source domain, and the inputs {x Ti } from the target domain, our task is to predict the unknown outputs {y Ti } in the target domain. The general assumption in domain adaptation is that the marginal densities, P(X S ) and Q(X T ), are very different. In this section, we attempt to find a common latent representation for both X S and X T that preserves the data configuration of the two domains after transformation. Let the desired nonlinear transformation be \u03c6 :\nX \u2192 H. Let X S = {x Si } = {\u03c6(x Si )}, X T = {x Ti } = {\u03c6(x Ti )} and X = X S \u222aX T\nbe the transformed input sets from the source, target and combined domains, respectively. Then, we desire that P (X S ) = Q (X T ).\n\nAssuming that \u03c6 is the feature map induced by a universal kernel. As shown in Section 2.1, the distance between two distributions P and Q can be empirically measured by the (squared) distance between the empirical means of the two domains:\nDist(X S , X T ) = 1 n 1 n1 i=1 \u03c6(x Si ) \u2212 1 n 2 n2 i=1 \u03c6(x Ti ) 2 H . (2)\nTherefore, a desired nonlinear mapping \u03c6 can be found by minimizing this quantity. However, \u03c6 is usually highly nonlinear and a direct optimization of (2) can get stuck in poor local minima. We thus need to find a new approach, based on the following assumption.\n\nThe key assumption in the proposed domain adaptation setting is that P = Q, but P (Y S |\u03c6(X S )) = P (Y T |\u03c6(X T )) under a transformation mapping \u03c6 on the input. In Section 3.1, we first revisit Maximum Mean Discrepancy Embedding (MMDE) which proposed to learn the kernel matrix K corresponding to the nonlinear mapping \u03c6 by solving a SDP optimization problem. In Section 3.2, we then propose a factorization of the kernel matrix for MMDE. An efficient eigendecomposition algorithm for kernel learning and computational issues are discussed in Sections 3.3 and 3.4.\n\n\nKernel Learning for Domain Adaptation\n\nInstead of finding the nonlinear transformation \u03c6 explicitly, Pan et al. [2008] proposed to transform this problem as a kernel learning problem. By virtue of the kernel trick, (i.e.,\nk(x i , x j ) = \u03c6(x i ) \u03c6(x j ))\n, the distance between the empirical means of the two domains in (2) can be written as:\nDist(X S , X T ) = tr(KL),(3)\nwhere\nK = KS,S KS,T KT,S KT,T(4)\nis a (n 1 + n 2 ) \u00d7 (n 1 + n 2 ) kernel matrix, K S,S , K T,T and K S,T respectively are the kernel matrices defined by k on the data in the source domain, target domain, and cross domains;\nand L = [L ij ] 0 with L ij = 1 n 2 1 if x i , x j \u2208 X S ; L ij = 1 n 2 2 if x i , x j \u2208 X T ; otherwise, \u2212 1 n1n2 .\nIn the transductive setting, learning the kernel k(\u00b7, \u00b7) can be solved by learning the kernel matrix K instead. In [Pan et al., 2008], the resultant kernel matrix learning problem is formulated as a semi-definite program (SDP). Principal Component Analysis (PCA) is then applied on the learned kernel matrix to find a low-dimensional latent space across domains. This is referred to as Maximum Mean Discrepancy Embedding (MMDE).\n\n\nParametric Kernel Map for Unseen Patterns\n\nThere are several limitations of MMDE. First, it is transductive and cannot generalize on unseen patterns. Second, the criterion (3) requires K to be positive semi-definite and the resultant kernel learning problem has to be solved by expensive SDP solvers. Finally, in order to construct lowdimensional representations of X S and X T , the obtained K has to be further post-processed by PCA. This may potentially discard useful information in K.\n\nIn this paper, we propose an efficient method to find a nonlinear mapping \u03c6 based on kernel feature extraction. It avoids the use of SDP and thus its high computational burden. Moreover, the learned kernel k can be generalized to out-of-sample patterns directly. Besides, instead of using a two-step approach as in MMDE, we propose a unified kernel learning method which utilizes an explicit low-rank representation.\n\nFirst, recall that the kernel matrix K in (4) can be decomposed as K = (KK \u22121/2 )(K \u22121/2 K), which is often known as the empirical kernel map [Sch\u00f6lkopf et al., 1998]. Consider the use of a (n 1 + n 2 ) \u00d7 m matrix W to transform the corresponding feature vectors to a m-dimensional space. In general, m n 1 + n 2 . The resultant kernel matrix 1 is then\nK = (KK \u22121/2 W )( W K \u22121/2 K) = KW W K,(5)\nwhere W = K \u22121/2 W \u2208 R (n1+n2)\u00d7m . In particular, the corresponding kernel evaluation of k between any two patterns x i and x j is given by\nk(x i , x j ) = k xi W W k xj ,(6)\n1 As is common practice, one can ensure that the kernel matrix K is positive definite by adding a small > 0 to its diagonal [Pan et al., 2008].\n\nwhere k\nx = [k(x 1 , x), . . . , k(x n1+n2 , x)] \u2208 R n1+n2 .\nHence, the kernel k in (6) facilitates a readily parametric form for out-of-sample kernel evaluations. Moreover, using the definition of K in (5), the distance between the empirical means of the two domains can be rewritten as:\n\nDist(X S , X T ) = tr((KW W K)L) = tr(W KLKW ).\n\n\nTransfer Components Extraction\n\nIn minimizing criterion (7), a regularization term tr(W W ) is usually needed to control the complexity of W . As will be shown later in this section, this regularization term can avoid the rank deficiency of the denominator in the generalized eigendecomposition. The kernel learning problem for domain adaptation then reduces to:\nmin W tr(W W ) + \u00b5 tr(W KLKW ) s.t. W KHKW = I,(8)\nwhere \u00b5 is a trade-off parameter, I \u2208 R m\u00d7m is the identity matrix, H = I n1+n2 \u2212 1 n1+n2 11 is the centering matrix, where 1 \u2208 R n1+n2 is the column vector with all ones, and I n1+n2 \u2208 R (n1+n2)\u00d7(n1+n2) is the identity matrix. Moreover, note that the constraint W KHKW = I is added in (8) to avoid the trivial solution (W = 0), such that the transformed patterns do not collapse to one point, which can inflate the learned kernel k such that the embedding of data x i is preserved as in kernel PCA.\n\nThough the optimization problem (8) (11) where Z is a symmetric matrix. Setting the derivative of (11) w.r.t. W to zero, we have (I + \u00b5KLK)W = KHKW Z.\n\n(12) Multiplying both sides on the left by W T , and then on substituting it into (11), we obtain (9). Since the matrix I + \u00b5KLK is non-singular benefited from the regularization term tr(W W ), we obtain an equivalent trace maximization problem (10). Similar to kernel Fisher discriminant (KFD), the solution of W in (10) is the eigenvectors corresponding to the m leading eigenvalues of (I + \u00b5KLK) \u22121 KHK, where at most n 1 + n 2 \u2212 1 eigenvectors can be extracted. In the sequel, the proposed method is referred to as Transfer Component Analysis (TCA).\n\n\nComputational Issues\n\nThe kernel learning algorithm in [Pan et al., 2008] relies on SDPs. As there are O((n 1 + n 2 ) 2 ) variables in K, the overall training complexity is O((n 1 + n 2 ) 6.5 ) [Nesterov and Nemirovskii, 1994]. This becomes computationally prohibitive even for small-sized problems. Note that criterion (3) in this kernel learning problem is similar to the recently proposed supervised dimensionality reduction method colored MVU [Song et al., 2008], in which low-rank approximation is used to reduce the number of constraints and variables in the SDP. However, gradient descent is required to refine the embedding space and thus the solution can get stuck in a local minimum. On the other hand, our proposed kernel learning method requires only a simple and efficient eigendecomposition. This takes only O(m(n 1 + n 2 ) 2 ) time when m non-zero eigenvectors are to be extracted [Sorensen, 1996].\n\n\nRelated Works\n\nDomain adaptation, which can be considered as a special setting of transfer learning [Pan and Yang, 2008], has been widely studied in natural language processing (NLP) [Ando and Zhang, 2005;Daum\u00e9 III, 2007]. Ando and Zhang [2005] and  proposed structural correspondence learning (SCL) algorithms to learn the common feature representation across domains based on some heuristic selection of pivot features. Daum\u00e9 III [2007] designed a heuristic kernel to augment features for solving some specific domain adaptation problems in NLP. Besides, domain adaptation has also been investigated in other application areas such as sentiment classification . Theoretical analysis of domain adaptation has also been studied in [Ben-David et al., 2007].\n\nThe problem of sample selection bias (also referred to as co-variate shift) is also related to domain adaption. In sample selection bias, the basic assumption is that the sampling processes between the training data X trn and test data X tst may be different. As a result, P (X trn ) = P (X tst ), but P (Y trn |X trn ) = P (Y tst |X tst ). Instance re-weighting is a major technique for correcting sample selection bias [Huang et al., 2007;Sugiyama et al., 2008]. Recently, a state-ofart method, called kernel mean matching (KMM), is proposed [Huang et al., 2007]. It re-weights instances in a RKHS based on the MMD theory, which is different from our proposed method. Sugiyama et al. [2008] proposed another re-weighting algorithm, Kullback-Leibler Importance Estimation Procedure (KLIEP), which is integrated with crossvalidation to perform model selection automatically. Xing et al. [2007] proposed to correct the labels predicted by a shiftunaware classifier towards a target distribution based on the mixture distribution of the training and test data. Matching distributions by re-weighting instances is also used successfully in Multi-task Learning [Bickel et al., 2008]. However, unlike instance re-weighting, the proposed TCA method can cope with noisy features (as in image data and WiFi data) by effectively denoising and finding a latent space for matching distributions across different domains simultaneously. Thus, TCA can be treated as an integration of unsupervised feature extraction and distribution matching in a latent space.\n\n\nExperiments\n\nIn this section, we apply the proposed domain adaptation algorithm TCA on two real-world problems: indoor WiFi localization and text classification.\n\n\nCross-domain WiFi Localization\n\nFor cross-domain WiFi localization, we use a dataset published in the 2007 IEEE ICDM Contest . This dataset contains some labeled WiFi data collected in time period A (the source domain) and a large amount of unlabeled WiFi data collected in time period B (the target domain). Here, a label means the corresponding location where the WiFi data are received. WiFi data collected from different time periods are considered as different domains. The task is to predict the labels of the WiFi data collected in time period B. More specifically, all the WiFi data are collected in an indoor building around 145.5 \u00d7 37.5 m 2 , 621 labeled data are collected in time period A and 3128 unlabeled data are collected in time period B.\n\nWe conduct a series of experiments to compare TCA with some baselines, including other feature extraction methods such as KPCA, sample selection bias (or co-variate shift) methods, KMM and KLIEP and a domain adaptation method, SCL. For each experiment, all labeled data in the source domain and some unlabeled data in the target domain are used for training. Evaluation is then performed on the remaining unlabeled data (out-of-sample) in the target domain. This is repeated 10 times and the average performance is used to measure the generalization abilities of the methods. In addition, to compare the performance between TCA and MMDE, we conduct some experiments in the transductive setting [Nigam et al., 2000]. The evaluation criterion is the Average Error Distance (AED) on the test data, and the lower the better. For determining parameters for each method, we randomly select a very small subset of the target domain data to tune parameters. The values of parameters are fixed for all the experiments. Figure 1(a) compares the performance of Regularized Least Square Regression (RLSR) model on different feature representations learned by TCA, KPCA and SCL, and different re-weighted instances learned by KMM and KLIEP. Here, we use \u00b5 = 0.1 for TCA and the Laplacian kernel. As can be seen, the performance can be improved with the new feature representations of TCA and KPCA. TCA can achieve much higher performance because it aims at finding the leading components that minimize the difference between domains. Then, from the space spanned by these components, the model trained in one domain can be used to perform accurate prediction in the other domain. Figure 1(b) shows the results under a varying number of unlabeled data in the target main. As can be seen, with only a few unlabeled data in the target domain, TCA can still find a good feature representation to bridge between domains.\n\nSince MMDE cannot generalize to out-of-sample patterns, in order to compare TCA with MMDE, we conduct another series of experiments in a transductive setting, which means that the trained models are only evaluated on the unlabeled data that are used for learning the latent space. In Figure 1(c), we apply MMDE and TCA on 621 labeled data from the  source domain and 300 unlabeled data from the target domain to learn new representations, respectively, and then train RLSR on them. More comparison results in terms of ACE with varying number of training data are shown in Table 1. The experimental results show that TCA is slightly higher (worse) than MMDE in terms of AED. This is due to the nonparametric kernel matrix learned by MMDE, which can fit the observed unlabeled data better. However, as mentioned in Section 3.4, the cost of MMDE is expensive due to the computationally intensive SDP. The comparison results between TCA and MMDE in terms of computational time on the WiFi dataset are shown in Table 2.  \n\n\nCross-domain Text Classification\n\nIn this section, we perform cross-domain binary classification experiments on a preprocessed dataset of Reuters-21578. These data are categorized to a hierarchical structure. Data from different sub-categories under the same parent category are considered to be from different but related domains. The task is to predict the labels of the parent category. By following this strategy, three datasets orgs vs people, orgs vs places and people vs places are constructed. We randomly select 50% labeled data from the source domain, and 35% unlabeled data from the target domain. Evaluation is based on the (out-of-sample) testing of the remaining 65% unlabeled data in the target domain. This is repeated 10 times and the average results reported.\n\nSimilar to the experimental setting on WiFi localization, we conduct a series of experiments to compare TCA with KPCA, KMM, KLIEP and SCL. Here, the support vector machine (SVM) is used as the classifier. The evaluation criterion is the classification accuracy (the higher the better). We experiment with both the RBF kernel and linear kernel for feature extraction or re-weighting used by KPCA, TCA and KMM. The kernel used in the SVM for final prediction is a linear kernel, and the parameter \u00b5 in TCA is set to 0.1.\n\nAs can be seen from Table 3, different from experiments on the WiFi data, sample selection bias methods, such as KMM and KLIEP perform better than KPCA and PCA on the text data. However, with the feature presentations learned by TCA, SVM performs the best for cross-domain classification. This is because TCA not only discovers latent topics behind the text data, but also matches distributions across domains in the latent space spanned by the latent topics. Moreover, the performance of TCA using the RBF kernel is more stable.\n\n\nConclusion and Future Work\n\nLearning feature representations is of primarily an important task for domain adaptation. In this paper, we propose a new feature extraction method, called Transfer Component Analysis (TCA), to learn a set of transfer components which reduce the distance across domains in a RKHS. Compared to the previously proposed MMDE for the same task, TCA is much more efficient and can be generalized to out-of-sample patterns. Experiments on two real-world datasets verify the effectiveness of the proposed method. In the future, we are planning to take side information into account when learning the transfer components across domains, which may be better for the final classification or regression tasks.\n\n\nAcknowledgement\n\nSinno Jialin Pan and Qiang Yang thank the support from Microsoft Research MRA07/08.EG01 and Hong Kong CERG Project 621307. Ivor W. Tsang thanks the support from Singapore MOE AcRF Tier-1 Research Grant (RG15/08). James T. Kwok thanks the support from CERG project 614508. \n\n\ninvolves a nonconvex norm constraint W KHKW = I, it can still be solved efficiently by the following trace optimization problem: Proposition 1 The optimization problem (8) can be reformulated as min W tr((W KHKW ) \u2020 W (I + \u00b5KLK)W ), (9) or max W tr((W (I + \u00b5KLK)W ) \u22121 W KHKW ). (10) Proof. The Lagrangian of (8) is tr(W (I + \u00b5KLK)W ) \u2212 tr((W KHKW \u2212 I)Z),\n\n\nof MMDE and TCA in the transductive setting (# unlabeled data is 300).\n\nFigure 1 :\n1Comparison of Average Error Distance (in m).\n\nTable 1 :\n1ACE (in m) of MMDE and TCA with 10 dimensions and varying # training data (# labeled data in the source domain is fixed to 621, # unlabeled data in the target domain varies from 100 to 800.)# unlabeled and labeled data used for training \n721 821 921 1,021 1,121 1,221 1,321 1,421 \nTCA 2.413 2.378 2.313 2.285 2.271 2.285 2.287 2.289 \nMMDE 2.315 2.247 2.208 2.212 2.207 2.182 2.257 2.279 \n\n\n\nTable 2 :\n2CPU training time (in sec) of MMDE and TCA with varying # training data. # unlabeled and labeled data used for training 721 821 921 1,021 1,121 1,221 1,321 1,421 ,209 3,539 4,168 4,940 10,093 14,165 18,094 33,004TCA 25 \n30 \n46 \n59 \n72 \n94 \n115 \n145 \nMMDE 3\n\nTable 3 :\n3Comparison between Different Methods (number inside parentheses is the standard deviation over 10 repetitions).features \n#features people vs places \norgs vs people \norgs vs places \nOriginal \n0.5198 (.0252) \n0.6696 (.0287) \n0.6683 (.0221) \nPCA \n5 \n0.5564 (.0788) \n0.5574 (.0760) \n0.5653 (.0984) \n10 \n0.5453 (.0911) \n0.6470 (.0598) \n0.6140 (.0534) \n20 \n0.5424 (.0590) \n0.6703 (.0334) \n0.6491 (.0391) \n30 \n0.5631 (.0346) \n0.6652 (.0549) \n0.6114 (.0564) \nKPCA (RBF) \n5 \n0.5900 (.0185) \n0.5863 (0.0405) 0.5883 (.0185) \n10 \n0.5934 (.0169) \n0.5955 (0.0676) 0.6267 (.0814) \n20 \n0.6032 (.0323) \n0.5968 (0.0705) 0.6098 (.0315) \n30 \n0.6000 (.0267) \n0.5964 (0.0742) 0.6247 (.0438) \nTCA (linear) \n5 \n0.5804 (.0528) \n0.6397 (.0897) \n0.6403 (.0722) \n10 \n0.5495 (.0764) \n0.7308 (.0495) \n0.7006 (.0527) \n20 \n0.5600 (.0969) \n0.7425 (.0579) \n0.6720 (.0374) \n30 \n0.5468 (.0635) \n0.7330 (.0432) \n0.5989 (.0700) \nTCA (RBF) \n5 \n0.6129 (.0176) \n0.6297 (.0302) \n0.6899 (.0195) \n10 \n0.5920 (.0148) \n0.7088 (.0251) \n0.7042 (.0218) \n20 \n0.5954 (.0201) \n0.7196 (.0235) \n0.6942 (.0220) \n30 \n0.5916 (.0166) \n0.7217 (.0275) \n0.6896 (.0203) \nSCL \n0.5267 (.0310) \n0.6834 (.0327) \n0.6733 (.0198) \nKMM (linear) \n0.5836 (.0159) \n0.7006 (.0353) \n0.6714 (.0263) \nKMM (RBF) \n0.5836 (.0159) \n0.6968 (.0224) \n0.6655 (.0245) \nKLIEP \n0.5758 (.0241) \n0.6946 (.0192) \n0.6638 (.0112) \n\n\nA high-performance semi-supervised learning method for text chunking. Tong Zhang ; Rie Kubota Ando, Zhang, Proceedings of ACL. ACLMorristown, NJ, USAand Zhang, 2005] Rie Kubota Ando and Tong Zhang. A high-performance semi-supervised learning method for text chunking. In Proceedings of ACL, pages 1-9, Morristown, NJ, USA, 2005.\n\nAnalysis of representations for domain adaptation. Ben-David, Steffen Bickel, Jasmina Bogojeska, Thomas Lengauer, and Tobias Scheffer. Helsinki, FinlandProceedings of ICMLBen-David et al., 2007] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NIPS 19, pages 137-144, 2007. [Bickel et al., 2008] Steffen Bickel, Jasmina Bogojeska, Thomas Lengauer, and Tobias Scheffer. Multi-task learning for hiv ther- apy screening. In Proceedings of ICML, pages 56-63, Helsinki, Finland, 2008.\n\nDomain adaptation with structural correspondence learning. Blitzer, Proceedings of EMNLP. EMNLPSydney, AustraliaBlitzer et al., 2006] John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspondence learn- ing. In Proceedings of EMNLP, pages 120-128, Sydney, Aus- tralia, 2006.\n\nBiographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. Blitzer, Proceedings of ACL. Karsten M. Borgwardt, Arthur Gretton, Malte J. Rasch, Hans-Peter Kriegel, Bernhard Sch\u00f6lkopf, and Alexander J. SmolaACLPrague, Czech RepublicBrazilISMB. FortalezaBlitzer et al., 2007] John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and blenders: Do- main adaptation for sentiment classification. In Proceedings of ACL, pages 432-439, Prague, Czech Republic, 2007. [Borgwardt et al., 2006] Karsten M. Borgwardt, Arthur Gretton, Malte J. Rasch, Hans-Peter Kriegel, Bernhard Sch\u00f6lkopf, and Alexander J. Smola. Integrating structured biological data by kernel maximum mean discrepancy. In ISMB, pages 49-57, For- taleza, Brazil, 2006.\n\nHal Daum\u00e9 III. Frustratingly easy domain adaptation. Iii Daum\u00e9, Proceedings of ACL. ACLPrague, Czech RepublicDaum\u00e9 III, 2007] Hal Daum\u00e9 III. Frustratingly easy domain adap- tation. In Proceedings of ACL, pages 256-263, Prague, Czech Republic, 2007.\n\nNesterov and Nemirovskii, 1994] Yurii Nesterov and Arkadii Nemirovskii. Interior-point Polynomial Algorithms in Convex Programming. Huang, Machine Learning. Kamal Nigam, Andrew K. Mccallum, Sebastian Thrun, and Tom MitchellPhiladelphia, PA39NIPS 19Huang et al., 2007] Jiayuan Huang, Alex Smola, Arthur Gretton, Karsten M. Borgwardt, and Bernhard Sch\u00f6lkopf. Correcting sam- ple selection bias by unlabeled data. In NIPS 19, pages 601-608, 2007. [Nesterov and Nemirovskii, 1994] Yurii Nesterov and Arkadii Ne- mirovskii. Interior-point Polynomial Algorithms in Convex Pro- gramming. Society for Industrial and Applied Mathematics, Philadelphia, PA, 1994. [Nigam et al., 2000] Kamal Nigam, Andrew K. Mccallum, Sebas- tian Thrun, and Tom Mitchell. Text classification from labeled and unlabeled documents using em. In Machine Learning, vol- ume 39, pages 103-134, 2000.\n\nNonlinear component analysis as a kernel eigenvalue problem. Yang ; Sinno Jialin Pan, Qiang Yang, ; , HKUST-CS08-08Proceedings of AAAI. AAAIChicago, Illinois, USA; Alex Smola, Karsten Borgwardt, and Arthur GrettonLe Song10Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong KongTechnical ReportNIPS 20. Sorensen, 1996] Danny C. Sorensen. Implicitly restartedand Yang, 2008] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. Technical Report HKUST-CS08-08, Depart- ment of Computer Science and Engineering, Hong Kong Univer- sity of Science and Technology, Hong Kong, 2008. [Pan et al., 2008] Sinno Jialin Pan, James T. Kwok, and Qiang Yang. Transfer learning via dimensionality reduction. In Pro- ceedings of AAAI, pages 677-682, Chicago, Illinois, USA, 2008. [Sch\u00f6lkopf et al., 1998] Bernhard Sch\u00f6lkopf, Alexander Smola, and Klaus-Robert M\u00fcller. Nonlinear component analysis as a ker- nel eigenvalue problem. Neural Computation, 10(5):1299-1319, 1998. [Song et al., 2008] Le Song, Alex Smola, Karsten Borgwardt, and Arthur Gretton. Colored maximum variance unfolding. In NIPS 20, pages 1385-1392, 2008. [Sorensen, 1996] Danny C. Sorensen. Implicitly restarted\n\nDirect importance estimation with model selection and its application to covariate shift adaptation. Sugiyama, TR-96-40Arnoldi/Lanczos methods for large scale eigenvalue calculations. Wenyuan Dai, Gui-Rong Xue, and Yong YuWarsaw, PolandQiang Yang2Department of Computational and Applied Mathematics, Rice UniversityTechnical ReportJournal of Machine Learning ResearchArnoldi/Lanczos methods for large scale eigenvalue calculations. Technical Report TR-96-40, Department of Computational and Applied Mathematics, Rice University, 1996. [Steinwart, 2001] Ingo Steinwart. On the influence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67-93, 2001. [Sugiyama et al., 2008] Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Von Buenau, and Motoaki Kawanabe. Di- rect importance estimation with model selection and its applica- tion to covariate shift adaptation. In NIPS 20, pages 1433-1440, 2008. [Xing et al., 2007] Dikan Xing, Wenyuan Dai, Gui-Rong Xue, and Yong Yu. Bridged refinement for transfer learning. In PKDD, pages 324-335, Warsaw, Poland, 2007. [Yang et al., 2008] Qiang Yang, Sinno Jialin Pan, and Vin- cent Wenchen Zheng. Estimating location using Wi-Fi. IEEE Intelligent Systems, 23(1):8-13, 2008.\n", "annotations": {"author": "[{\"end\":175,\"start\":53},{\"end\":292,\"start\":176},{\"end\":411,\"start\":293},{\"end\":545,\"start\":412}]", "publisher": null, "author_last_name": "[{\"end\":69,\"start\":66},{\"end\":188,\"start\":183},{\"end\":305,\"start\":301},{\"end\":422,\"start\":418}]", "author_first_name": "[{\"end\":58,\"start\":53},{\"end\":65,\"start\":59},{\"end\":180,\"start\":176},{\"end\":182,\"start\":181},{\"end\":298,\"start\":293},{\"end\":300,\"start\":299},{\"end\":417,\"start\":412}]", "author_affiliation": "[{\"end\":174,\"start\":71},{\"end\":291,\"start\":212},{\"end\":410,\"start\":307},{\"end\":544,\"start\":441}]", "title": "[{\"end\":50,\"start\":1},{\"end\":595,\"start\":546}]", "venue": null, "abstract": "[{\"end\":2026,\"start\":597}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3009,\"start\":2990},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4069,\"start\":4052},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4088,\"start\":4072},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4696,\"start\":4672},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4810,\"start\":4793},{\"end\":8526,\"start\":8503},{\"end\":8975,\"start\":8958},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11185,\"start\":11168},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11913,\"start\":11895},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13285,\"start\":13262},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13833,\"start\":13815},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15871,\"start\":15854},{\"end\":16024,\"start\":15993},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16265,\"start\":16246},{\"end\":16711,\"start\":16695},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16835,\"start\":16815},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16920,\"start\":16898},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16936,\"start\":16920},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16959,\"start\":16938},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17153,\"start\":17137},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17470,\"start\":17446},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17914,\"start\":17894},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17936,\"start\":17914},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18037,\"start\":18017},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18165,\"start\":18143},{\"end\":18366,\"start\":18348},{\"end\":18651,\"start\":18610},{\"end\":20659,\"start\":20639}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26075,\"start\":25718},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26148,\"start\":26076},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26206,\"start\":26149},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26608,\"start\":26207},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":26877,\"start\":26609},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28228,\"start\":26878}]", "paragraph": "[{\"end\":3485,\"start\":2042},{\"end\":3939,\"start\":3487},{\"end\":5354,\"start\":3941},{\"end\":6475,\"start\":5356},{\"end\":6860,\"start\":6477},{\"end\":7184,\"start\":6862},{\"end\":8124,\"start\":7223},{\"end\":8273,\"start\":8153},{\"end\":8866,\"start\":8275},{\"end\":9147,\"start\":8925},{\"end\":9702,\"start\":9179},{\"end\":9917,\"start\":9786},{\"end\":10158,\"start\":9919},{\"end\":10496,\"start\":10234},{\"end\":11064,\"start\":10498},{\"end\":11288,\"start\":11106},{\"end\":11409,\"start\":11322},{\"end\":11445,\"start\":11440},{\"end\":11662,\"start\":11473},{\"end\":12208,\"start\":11780},{\"end\":12700,\"start\":12254},{\"end\":13118,\"start\":12702},{\"end\":13472,\"start\":13120},{\"end\":13655,\"start\":13516},{\"end\":13834,\"start\":13691},{\"end\":13843,\"start\":13836},{\"end\":14124,\"start\":13897},{\"end\":14173,\"start\":14126},{\"end\":14538,\"start\":14208},{\"end\":15089,\"start\":14590},{\"end\":15241,\"start\":15091},{\"end\":15796,\"start\":15243},{\"end\":16712,\"start\":15821},{\"end\":17471,\"start\":16730},{\"end\":19020,\"start\":17473},{\"end\":19184,\"start\":19036},{\"end\":19943,\"start\":19219},{\"end\":21847,\"start\":19945},{\"end\":22865,\"start\":21849},{\"end\":23645,\"start\":22902},{\"end\":24165,\"start\":23647},{\"end\":24696,\"start\":24167},{\"end\":25425,\"start\":24727},{\"end\":25717,\"start\":25445}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8924,\"start\":8867},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9785,\"start\":9703},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10233,\"start\":10159},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11321,\"start\":11289},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11439,\"start\":11410},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11472,\"start\":11446},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11779,\"start\":11663},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13515,\"start\":13473},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13690,\"start\":13656},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13896,\"start\":13844},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14589,\"start\":14539}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22428,\"start\":22421},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22862,\"start\":22855},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24194,\"start\":24187}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2040,\"start\":2028},{\"attributes\":{\"n\":\"2\"},\"end\":7221,\"start\":7187},{\"attributes\":{\"n\":\"2.1\"},\"end\":8151,\"start\":8127},{\"attributes\":{\"n\":\"3\"},\"end\":9177,\"start\":9150},{\"attributes\":{\"n\":\"3.1\"},\"end\":11104,\"start\":11067},{\"attributes\":{\"n\":\"3.2\"},\"end\":12252,\"start\":12211},{\"attributes\":{\"n\":\"3.3\"},\"end\":14206,\"start\":14176},{\"attributes\":{\"n\":\"3.4\"},\"end\":15819,\"start\":15799},{\"attributes\":{\"n\":\"4\"},\"end\":16728,\"start\":16715},{\"attributes\":{\"n\":\"5\"},\"end\":19034,\"start\":19023},{\"attributes\":{\"n\":\"5.1\"},\"end\":19217,\"start\":19187},{\"attributes\":{\"n\":\"5.2\"},\"end\":22900,\"start\":22868},{\"attributes\":{\"n\":\"6\"},\"end\":24725,\"start\":24699},{\"attributes\":{\"n\":\"7\"},\"end\":25443,\"start\":25428},{\"end\":26160,\"start\":26150},{\"end\":26217,\"start\":26208},{\"end\":26619,\"start\":26610},{\"end\":26888,\"start\":26879}]", "table": "[{\"end\":26608,\"start\":26409},{\"end\":26877,\"start\":26833},{\"end\":28228,\"start\":27001}]", "figure_caption": "[{\"end\":26075,\"start\":25720},{\"end\":26148,\"start\":26078},{\"end\":26206,\"start\":26162},{\"end\":26409,\"start\":26219},{\"end\":26833,\"start\":26621},{\"end\":27001,\"start\":26890}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20963,\"start\":20955},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21623,\"start\":21612},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22141,\"start\":22133}]", "bib_author_first_name": "[{\"end\":28304,\"start\":28300},{\"end\":30276,\"start\":30273},{\"end\":31417,\"start\":31398},{\"end\":31428,\"start\":31423},{\"end\":31436,\"start\":31435}]", "bib_author_last_name": "[{\"end\":28328,\"start\":28305},{\"end\":28335,\"start\":28330},{\"end\":28620,\"start\":28611},{\"end\":29178,\"start\":29171},{\"end\":29530,\"start\":29523},{\"end\":30282,\"start\":30277},{\"end\":30607,\"start\":30602},{\"end\":31421,\"start\":31418},{\"end\":31433,\"start\":31429},{\"end\":32671,\"start\":32663}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16629334},\"end\":28558,\"start\":28230},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10908021},\"end\":29110,\"start\":28560},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15978939},\"end\":29424,\"start\":29112},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14688775},\"end\":30218,\"start\":29426},{\"attributes\":{\"id\":\"b4\"},\"end\":30468,\"start\":30220},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":117194167},\"end\":31335,\"start\":30470},{\"attributes\":{\"doi\":\"HKUST-CS08-08\",\"id\":\"b6\",\"matched_paper_id\":6674407},\"end\":32560,\"start\":31337},{\"attributes\":{\"doi\":\"TR-96-40\",\"id\":\"b7\",\"matched_paper_id\":9133542},\"end\":33834,\"start\":32562}]", "bib_title": "[{\"end\":28298,\"start\":28230},{\"end\":28609,\"start\":28560},{\"end\":29169,\"start\":29112},{\"end\":29521,\"start\":29426},{\"end\":30271,\"start\":30220},{\"end\":30600,\"start\":30470},{\"end\":31396,\"start\":31337},{\"end\":32661,\"start\":32562}]", "bib_author": "[{\"end\":28330,\"start\":28300},{\"end\":28337,\"start\":28330},{\"end\":28622,\"start\":28611},{\"end\":29180,\"start\":29171},{\"end\":29532,\"start\":29523},{\"end\":30284,\"start\":30273},{\"end\":30609,\"start\":30602},{\"end\":31423,\"start\":31398},{\"end\":31435,\"start\":31423},{\"end\":31439,\"start\":31435},{\"end\":32673,\"start\":32663}]", "bib_venue": "[{\"end\":28379,\"start\":28357},{\"end\":28712,\"start\":28695},{\"end\":29224,\"start\":29202},{\"end\":29693,\"start\":29668},{\"end\":30329,\"start\":30304},{\"end\":30709,\"start\":30693},{\"end\":31550,\"start\":31473},{\"end\":32798,\"start\":32784},{\"end\":28355,\"start\":28337},{\"end\":28693,\"start\":28622},{\"end\":29200,\"start\":29180},{\"end\":29550,\"start\":29532},{\"end\":30302,\"start\":30284},{\"end\":30625,\"start\":30609},{\"end\":31471,\"start\":31452},{\"end\":32744,\"start\":32681}]"}}}, "year": 2023, "month": 12, "day": 17}