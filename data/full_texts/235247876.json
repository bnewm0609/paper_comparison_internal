{"id": 235247876, "updated": "2023-10-06 02:20:00.189", "metadata": {"title": "Transferable Deep Reinforcement Learning Framework for Autonomous Vehicles with Joint Radar-Data Communications", "authors": "[{\"first\":\"Nguyen\",\"last\":\"Hieu\",\"middle\":[\"Quang\"]},{\"first\":\"Dinh\",\"last\":\"Hoang\",\"middle\":[\"Thai\"]},{\"first\":\"Dusit\",\"last\":\"Niyato\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Dong\",\"last\":\"Kim\",\"middle\":[\"In\"]},{\"first\":\"Chau\",\"last\":\"Yuen\",\"middle\":[]}]", "venue": "IEEE Transactions on Communications, 2022", "journal": "IEEE Transactions on Communications", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Autonomous Vehicles (AVs) are required to operate safely and efficiently in dynamic environments. For this, the AVs equipped with Joint Radar-Communications (JRC) functions can enhance the driving safety by utilizing both radar detection and data communication functions. However, optimizing the performance of the AV system with two different functions under uncertainty and dynamic of surrounding environments is very challenging. In this work, we first propose an intelligent optimization framework based on the Markov Decision Process (MDP) to help the AV make optimal decisions in selecting JRC operation functions under the dynamic and uncertainty of the surrounding environment. We then develop an effective learning algorithm leveraging recent advances of deep reinforcement learning techniques to find the optimal policy for the AV without requiring any prior information about surrounding environment. Furthermore, to make our proposed framework more scalable, we develop a Transfer Learning (TL) mechanism that enables the AV to leverage valuable experiences for accelerating the training process when it moves to a new environment. Extensive simulations show that the proposed transferable deep reinforcement learning framework reduces the obstacle miss detection probability by the AV up to 67% compared to other conventional deep reinforcement learning approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.13670", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tcom/HieuHNWKY22", "doi": "10.1109/tcomm.2022.3182034"}}, "content": {"source": {"pdf_hash": "a3a0b080e2947f51485c1b9e5ae5e19cde401539", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.13670v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "dc7cfb16488ab367c373b140f8c6392bb44d9c61", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a3a0b080e2947f51485c1b9e5ae5e19cde401539.txt", "contents": "\nTransferable Deep Reinforcement Learning Framework for Autonomous Vehicles with Joint Radar-Data Communications\n\n\nDinhNguyen Quang Hieu s:hieu.nguyen-1@student.uts.edu.au \nThai Hoang \nDusit Niyato dniyato@ntu.edu.sg.p.wangis \nPing Wang pingw@yorku.ca.d.i.kimis \nDong In Kim dikim@skku.ac.kr.c.yueniswithsingapore \nChau Yuen yuenchau@sutd.edu.sg. \nN Q Hieu \nD T Hoang hoang.dinh@uts.edu.au.d.niyatoiswithwith \n\nUniversity of Technol-ogy Sydney\nNSWAustralia\n\n\nNanyang Technological University\nSingapore\n\n\nwith York University\nCanada\n\n\nversity of Technology and Design\nwith Sungkyunkwan University\nKorea, Singapore\n\nTransferable Deep Reinforcement Learning Framework for Autonomous Vehicles with Joint Radar-Data Communications\n1Index Terms-Joint radar-communicationsautonomous vehi- clesdeep reinforcement learningtransfer learning\nAutonomous Vehicles (AVs) are required to operate safely and efficiently in dynamic environments. For this, the AVs equipped with Joint Radar-Communications (JRC) functions can enhance the driving safety by utilizing both radar detection and data communication functions. However, optimizing the performance of the AV system with two different functions under uncertainty and dynamic of surrounding environments is very challenging. In this work, we first propose an intelligent optimization framework based on the Markov Decision Process (MDP) to help the AV make optimal decisions in selecting JRC operation functions under the dynamic and uncertainty of the surrounding environment. We then develop an effective learning algorithm leveraging recent advances of deep reinforcement learning techniques to find the optimal policy for the AV without requiring any prior information about surrounding environment. Furthermore, to make our proposed framework more scalable, we develop a Transfer Learning (TL) mechanism that enables the AV to leverage valuable experiences for accelerating the training process when it moves to a new environment. Extensive simulations show that the proposed transferable deep reinforcement learning framework reduces the obstacle miss detection probability by the AV up to 67% compared to other conventional deep reinforcement learning approaches. With the deep reinforcement learning and transfer learning approaches, our proposed solution can find its applications in a wide range of autonomous driving scenarios from driver assistance to full automation transportation.\n\nAbstract-Autonomous Vehicles (AVs) are required to operate safely and efficiently in dynamic environments. For this, the AVs equipped with Joint Radar-Communications (JRC) functions can enhance the driving safety by utilizing both radar detection and data communication functions. However, optimizing the performance of the AV system with two different functions under uncertainty and dynamic of surrounding environments is very challenging. In this work, we first propose an intelligent optimization framework based on the Markov Decision Process (MDP) to help the AV make optimal decisions in selecting JRC operation functions under the dynamic and uncertainty of the surrounding environment. We then develop an effective learning algorithm leveraging recent advances of deep reinforcement learning techniques to find the optimal policy for the AV without requiring any prior information about surrounding environment. Furthermore, to make our proposed framework more scalable, we develop a Transfer Learning (TL) mechanism that enables the AV to leverage valuable experiences for accelerating the training process when it moves to a new environment. Extensive simulations show that the proposed transferable deep reinforcement learning framework reduces the obstacle miss detection probability by the AV up to 67% compared to other conventional deep reinforcement learning approaches. With the deep reinforcement learning and transfer learning approaches, our proposed solution can find its applications in a wide range of autonomous driving scenarios from driver assistance to full automation transportation.\n\nIndex Terms-Joint radar-communications, autonomous vehicles, deep reinforcement learning, transfer learning. \n\n\nI. INTRODUCTION\n\nRecent years have witnessed a fast development of Autonomous Vehicles (AVs) with advanced driving safety technologies. According to the recent Allied Market Research's outlook [1], the autonomous vehicle global market is expected to reach $556.67 billion by 2026 due to significant demands of road safety. To achieve driving safety, AVs are usually equipped with Artificial Intelligent (AI) softwares, light detection & ranging (LiDAR), and radar detection technologies [4]. Furthermore, AVs are recommended to communicate with each other, e.g., using Vehicle-to-Vehicle (V2V) communications, or with roadside infrastructures, e.g., using Vehicle-to-Infrastructure (V2I) communications, to facilitate intelligent traffic management, routing and data analysis [2], [3]. Aiming for fully-automated future transportation systems, AVs are expected to be equipped with two main components that are (i) advanced driver-assistance system (ADAS) and (ii) data communication system [2], [8].\n\nSeveral research works focusing on vision-based ADAS, i.e., camera and LiDAR, have been proposed to address driving safety problems for AVs [5], [6]. With the recent development of computer vision and deep learning, AVs can achieve accurate detection under different requirements of autonomous driving applications [4]. However, most of these aforementioned works focus on enhancing the detection accuracy without considering the impact of environment (e.g., rain, fog, and snow) on the performance and safety of the AVs. Radar, in contrast, can be equipped on AVs and work in such weather conditions with effective detection ability ranging from few meters to more than 200 meters [2].\n\nAVs equipped with data communication systems can exchange information with roadside units (e.g., base stations and edge computing services) to efficiently navigate under different weather conditions or avoid unwanted traffic congestions [2]. Motivated by the fact that the radar detection and data communication functions can work effectively together under the same frequency band, e.g., mm-wave band at 71 \u2212 76 GHz, many research works have been proposed to use these functions in a joint manner, namely Joint Radar-Communications (JRC). Accordingly, this joint design can lead to significant benefits in terms of size, cost, power consumption, robustness, and performance [4], [8]. Therefore, JRC, with its abilities of enhancing driving safety and facilitating intelligent road management, is expected to play a key role for future AVs.\n\nAlthough the development of JRC brings major benefits for AVs, they are facing several technical challenges. In particular, the radar detection and data communication functions work on the same frequency band, and thus optimizing resource sharing between these two functions needs to be addressed [8]. In addition, several environment factors can affect operation of the JRC functions and thus yield a dynamic optimization problem which is hard to be solved [7], [19]. Under different environment conditions, e.g., communication channel, weather, and traffic density, the AVs should be able to control their JRC functions effectively. Another problem is that the AVs are highly mobile. When they travel from the current environment to a new environment which has different data distributions, e.g., different traffic density, weather, and road state distributions, they should be able to adapt their JRC functions to suit the new environment.\n\nTo address these aforementioned problems, in this work, we first formulate the dynamic JRC function selection problem of an AV by using a Markov Decision Process (MDP) framework. The MDP framework enables the AV to make decisions by considering the current state of surrounding environment. Then, we propose a deep reinforcement learning algorithm to help the AV learn the environment dynamics and make optimal JRC decisions without requiring any prior information from the surrounding environment. Furthermore, we propose a Transfer Learning (TL) approach that enables the AV to learn quickly an optimal policy when it has to travel to a new environment. This approach also enables an AV to share useful knowledge from a learned environment with other AVs that want to learn optimal policies from this environment. The contributions of this paper can be summarized as follows.\n\n\u2022 We propose an MDP framework for the joint radarcommunications system in AVs. With the MDP framework, the AV can adaptively select the radar detection function or data communication function based on the current status of surrounding environment, e.g., weather condition, communication channel state, traffic condition, and driving speed, to maximize the system performance. \u2022 We develop a highly-effective algorithm based on Double Deep Q-Network (DDQN) to deal with the dynamic and uncertainty of the environment and find the optimal policy for the AV. The DDQN algorithm with the experience replay memory technique and deep neural network as a function approximator can address the slow-convergence problem of conventional reinforcement learning algorithms. This feature is especially useful when the AV is required to quickly obtain the optimal policy under a large number of uncertain environment factors. \u2022 To make our proposed framework further scalable, we introduce a TL approach, namely Transfer Learning with Demonstrations (TLwD), to help the AV accelerate the training process when the AV travels to a new environment. By utilizing a small amount of demonstration data from an outer environment, combining with prioritized experience replay [29] and multi-step Temporal Difference (TD) learning [25] techniques, the AV can obtain an optimal policy faster than that using the conventional deep reinforcement learning approaches in the new environment. To the best of our knowledge, our paper is the first work that considers TL setting for joint radarcommunications in autonomous vehicles. \u2022 Finally, we perform extensive simulations with the aims of not only demonstrating the efficiency of proposed solutions in comparisons with other conventional methods but also providing insightful analytical results for the implementation of our framework. To that end, we take into account parameters of real automotive radar devices [31]. In addition, we use statistics from a large dataset, namely nuScenes [35], as our simulation parameters. The simulation results show that our proposed TL approach can reduce the miss detection probability of the AV up to 67% and only require a smaller number of training iterations to converge to the optimal policy, compared to the other conventional deep reinforcement learning approaches. The rest of paper is organized as follows. Related works are reviewed in Section II. Sections III and IV describe the system model and problem formulation, respectively. Section V presents the Q-learning and Double Deep Q-Network algorithms. Then, Transfer Learning with Demonstrations (TLwD) algorithm is presented in Section VI. After that, the evaluation results are discussed in Section VII. Finally, conclusions are drawn in Section VIII.\n\n\nII. RELATED WORKS\n\nSince the AV system equipped with JRC functions can use both radar detection and data communication functions using the same hardware platform, these functions share some joint system resources such as antennas, spectrum, and power. Consequently, one major problem is to optimize the resource sharing between the radar detection function and data communication function. Recently, some resource sharing approaches have been proposed to solve the problem [9]- [15]. In [9], the authors proposed a JRC system that enables the vehicle to transmit data while estimating distance, Doppler returns and Angles of Arrival (AoA) of target vehicles. To extract these parameters, the authors introduced two processing methods at the receiver based on Fast Fourier Transform and sub-space followed by diversity combining. Simulation results show the trade-off between the Doppler estimation performance and AoA performance of the two proposed methods. In [10], the authors proposed an effective framework to support radar detection and data transmission for automotive applications.\n\nThe key idea of this approach is reserving preamble blocks in the IEEE 802.11ad frame for the radar detection mode to estimate ranges and velocities of targets, and at the same time uses data blocks for the data transmission. Simulation results showed that with SNR values greater than \u22122 dB, the radar echo was detected with very high probability. Similarly, the authors in [11], [12] exploited the preamble of a communication frame and used it for the radar detection function. For balancing the resource sharing between radar detection and data communication, the authors in [11], [12] proposed a fraction parameter in which the frame contains one fraction for data communication and another one for radar detection. In [11], the authors illustrated performance metrics for different fraction parameters, but the optimal value of this parameter was not discussed. In [12], the optimal value of the fraction parameter was provided by using convex hull approximation. Although the above works are effective in specific circumstances, they are fixed-schedule schemes that are not appropriate to implement in practice. The reason is that the AVs are highly mobile and when they are running on the road, their surrounding environments are uncertain and dynamic. In addition, the performance of these approaches is limited either by the respective communications protocols or inter-vehicular synchronization.\n\nIn [13]- [15], the authors proposed time division approaches to optimize the resource sharing between radar detection function and data communication function. In [13], a time-slotted JRC network with an uncoordinated setup was considered in which the radar detection probability and data communication throughput were studied. The authors considered two performance metrics, i.e., (i) the maximum achievable detection range of the radar given a tolerable false alarm rate and (ii) the network throughput. Radar update rate and network density were also taken into consideration to evaluate the trade-off of the radar performance and communication performance of the system. However, the fundamental trade-off of radar-communications coexistence with flexible configurations remained unexplored. The authors in [14] proposed a time allocation approach for a bistatic automotive radar system aiming to maximize the probability of detection under different conditions of the communication channel. Simulation results showed that with different channel conditions, the optimal portions between radar operation time and data communication operation time are different. In [15], the authors proposed a time-sharing scheme in which a JRC node can switch between radar mode and communication mode based on the requirements of radar, i.e., maximum detection range. Simulation results showed that longer communication packet length (i.e., slower radar update rate) can achieve better radar range and higher communication throughput. Although the approaches in [14], [15] can exploit the simplicity of switching-based JRC mechanism [8], they are either limited by the number of considered targets [14] or with fixed settings during operation time relied on the radar maximum detection range [15].\n\nIt is worth noting that most aforementioned solutions focus on optimal time-sharing between the radar detection function and data communication function without concerning the dynamic of environment. Optimal time-sharing provides resource utilization, e.g., power and bandwidth, but the optimal solutions are fixed during the runtime, that limits practical applications. Alternatively, these approaches require additional information of environment, e.g., channel model, interference model, or arrival distributions of vehicles in advance, which is intractable to obtain, or even infeasible to collect in real-world applications. The aforementioned approaches also neglect the fact that the AV can possibly move to new environments in which the assumption of data distribution in the old environment fails. For example, the AV can travel from a rural area with a low traffic density, to an urban area with a high traffic density. Moreover, other environment factors such as road condition and weather condition may have different distributions in different environments. Therefore, to achieve robustness in driving safety, AV systems should consider different data distributions in variuos environment settings [16].\n\nTo the best of our knowledge, our previous work in [19] is the first work considering the joint operation of radar and communication for AVs under high dynamic environments such as unfavorable weather conditions, noisy communication channels. However, our previous work has several limitations. Specially, in [19], we considered that the AV only can choose to either transmit data or perform radar detection function. However, in practice, the AV may have multiple operation modes for these two functions based on specific requirements of autonomous driving applications [31]. Thus, in this work, we consider the AV with more practical operation modes for the JRC functions which can provide more flexible operations and thus be able to yield a better system performance. Furthermore, unlike our previous work [19] which can be applied in a specific environment for a particular AV, in this work we aim to develop a more general framework which can help the AV to quickly obtain an optimal policy when it travels to a new environment.\n\n\nIII. SYSTEM MODEL\n\nWe consider an AV system equipped with JRC functions as illustrated in Fig. 1. In the system model, two environments are considered that are source environment and target environment. The two environments are assumed to be related but different [20]. The AV is required to learn efficiently in the source environment and when it travels to the target environment, it should be able to transfer its experiences from the source environment to learn quickly a new policy. Since the two environments are different, the optimal policies learned from these environments are also different [20]. Details of the differences between the source environment and target environment are described in the next section.\n\nThe AV system in Fig. 1 includes (i) a rate adaptation module for the data communication function and (ii) a Frequency Modulated Continuous Wave (FMCW) radar for the radar detection function. The time horizon is discrete and the AV can choose either radar detection mode or data communication mode at each time step. On the one hand, if the AV uses the data communication mode, it will then select an appropriate data rate to transmit data packets to a nearby Base Station (BS) as illustrated in Fig. 1. On the other hand, if the AV uses the radar detection mode, it will select a suitable radar mode (e.g., short-range or long-range) to detect objects, e.g., cars and pedestrians in Fig. 1. Under the dynamics of the environment, e.g., the traffic density, the weather condition, and the road condition, the AV needs to choose a proper radar detection function or data communication function to guarantee safety and transmit data simultaneously. It is worth noting that the time divisionbased configuration for the JRC functions benefits from the simplicity and low-cost of implementation [8]. Both radar detection and data communication functions can be easily implemented using a simple switch without requiring re-design radar and communication waveform [8]. In the following sections, we discuss more details on how these modes are performed at the AV.\n\nA. Automotive Joint Radar-Communications Functions 1) Data communication function with rate adaption: The data rate adaptation module enables the AV to adjust its data transmission rates according to the channel quality and thus helps the AV achieve better throughput [17]. With rate adaptation, the AV can choose one of data transmission rates from the set R = { 1 , . . . , , . . . , }, where 1 < \u00b7 \u00b7 \u00b7 < < \u00b7 \u00b7 \u00b7 < . These rates can be achieved through different combinations of modulation and coding schemes [18]. The SINR at the BS, denoted by , can be then determined by [17]:\n= + ,(1)\nwhere is the received power at the BS, and are interference power of the other (interfering) signals in the network and random noise power, respectively. For a given SINR, the BS can decode certain rates with target bit error rate (BER). For example, for = 1, . . . , , if \u22121 \u2264 < with to be the value of SINR, only rates 1 , 2 , . . . , \u22121 , can be decoded at the BS [18]. Thus, if the received SINR at the BS is less than and the AV chooses a data rate or higher, the transmitted packets will be dropped. An ACK message is used by the BS to notify the AV for the successful transmission.\n\n2) Radar detection function with multi-mode FMCW radar: FMCW radar can provide multiple detection modes, e.g., short-range detection, mid-range detection, and long-range detection, for the AV to select during its running time [31]. The multi-mode setting is especially useful when the AV operates in different environments with various requirements. For example, a short-range detection mode can provide a highly accurate range resolution, which can detect nearby objects. Therefore, this mode is very useful in urban environments with high traffic density. In particular, each detection mode is equivalent to a specific configuration as illustrated in Fig. 2. For the FMCW radar, radar signals are transmitted as chirps in which the shape of a chirp is decided by sweep bandwidth and frequency slope [3]. This feature enables multi-mode operations on the radar function, e.g., long-range detection with low values of and , and short-range detection with high values of and . The relationships between the chirp configuration parameters and the radar performance are given by [31]:\n= 0 2 , and = 0 2 ,(2)\nwhere and are the range resolution and maximum detection range of the radar, respectively, 0 is the speed of light, and is the maximum intermediate frequency bandwidth supported. The range resolution is the minimum distance that the radar can resolve two separated objects. The maximum detection range is the maximum distance that the AV can detect reflected signals from the far-off objects. In (2), the values of 0 and are constant, while and can be configured for specific applications, e.g., long/short-range detection [31].\n\nIn the system model, the AV can choose one of radar detection modes from the set B = { 1 , . . . , , . . . , }, where each mode corresponds to a pair value of sweep bandwidth and frequency slope , i.e., ( , ). As a result, each mode can be illustrated by a pair value of range resolution and maximum detection range, i.e., ( , ) [31].\n\n\nB. Environment Model\n\nIt is noted that two environments with different dynamics are taken under consideration. However, in this section, we first describe the general characteristics of a specific environment, i.e., the source environment. The details of the differences between the two environments are further explained in Section IV.\n\nAt each time step, the AV can choose either radar detection mode or data communication mode given the current information of surrounding environment. To model the dynamic of the environment, we consider different factors that have significant influence on the decisions of the AV. These factors are (i) the state of the data communication channel (e.g., good or bad channel condition), (ii) the state of the road (e.g., smooth or rough road), (iii) the state of the weather (e.g., rainy or clear weather), (iv) the speed state of the AV (e.g., high or low speed), and (v) the state of nearby vehicles (e.g., with or without nearby vehicles). The values of these factors can be obtained by the AV's sensing system, e.g., road friction sensor, weather station instrument, speedometer, and cameras. With the states of these factors, unexpected events can occur. We consider an unexpected event to be an event that is out of the range of the AV's camera and the event can possibly cause collisions with the AV, e.g., a car coming from another road obscured by a nearby moving vehicle as illustrated in Fig. 1. Also, under a bad weather condition (e.g., rainy or foggy) and a bad road condition (e.g., rough road), if the AV is running in a crowded area (e.g., urban areas), some unexpected events which cannot be detected by the AV's vision-based sensing system (e.g., camera) can occur [32], [33]. In this situation, the short-range radar detection mode should be used by the AV to detect more obstructed objects.\n\nTo illustrate the dynamic of the environment, we take into account the probability that an unexpected event occurs given current states of the environment. Let\n\u2208 { 1 , . . . , , . . . , }, \u2208 { 1 , . . . , , . . . , }, \u2208 { 1 , . . . , , . . . , }, \u2208 { 1 , . . . , , . . . , }, and \u2208 { 1 , . . . , , . . . ,\n} denote the state of the communication channel, the state of the road, the state of the weather, the speed state of the AV, and the state of nearby vehicles, respectively, where the subscript max denotes the maximum number of discrete state values. For example, the speed of the AV can be divided into different discrete state values such as\n1 = 15 (km/h), 2 = 45 (km/h), and 3 = = 60 (km/h).\nGiven a particular state of environment, which is a composite of discrete state values ( , , , ), let denote the probability that an unexpected event occurs at the state value of state ( \u2208 { , , , }). For example, 3 expresses the probability that an unexpected event occurs at the current speed state value of the AV is 3 , e.g., 3 = 60 (km/h). Clearly, the state value of the communication channel is not considered as a contributing factor to the occurrence of unexpected events. It is worth noting that these values of can be obtained from real-world statistical analysis [32], [33]. Then, we can determine the probability that an unexpected event occurs given a particular state ( , , , ) by using the Bayes' theorem [34]. Let be the probability that state ( \u2208 { , , , }) is at state value . The sum of all individual probabilities is equal to 1, i.e., \u2211\ufe01\n\u2208 J = 1,(3)\nwhere J = {1, 2, . . . , }. By using the Bayes' theorem, the probability of an unexpected event occuring at the state ( , , , ) is determined by the sum of conditional probabilities as follows [19]:\n= \u2211\ufe01 \u2208 F \u2211\ufe01 \u2208 J ,(4)\nwhere F = { , , , }.\n\nIn addition to the factors influencing the probability that an unexpected event occurs as explained above, we need to consider traffic density on the road. The main reason is that in a high-density traffic area, even if the radar system can detect objects, but if the objects are too close to each other, the radar may not be able to differentiate them, and thus unexpected events still can occur. Several works have considered traffic density as an important parameter in the field of AVs [17], [35], [36]. In this work, the traffic density is defined as the number of objects in the maximum detection range of the AV. At time step , given traffic density , the miss detection ratio of the AV, denoted by ( ), can be calculated as follows:\n( ) = \u2212 ( ) ,(5)\nwhere ( ) is the number of objects that the AV can detect by using the radar detection mode at time step . The miss detection ratio represents the ability to detect closely spaced objects as well as far-off objects of the AV. A low miss detection ratio results in a good detection ability of the AV and vice versa [41]. In an area with a high traffic density with closely spaced objects, e.g., urban area, the AV should use the short-range detection mode with a highly accurate range resolution to reduce the miss detection ratio. In contrast, in an area with a low traffic density, the AV should use the longrange detection mode with a low accurate range resolution to detect more objects which are far-off while maintaining the small miss detection ratio.\n\n\nC. System Operation and Performance Metrics\n\nThe operation of the AV system with JRC functions can be described as follows. At each time step, the AV chooses either the radar detection mode or data communication mode. If the AV chooses the data communication mode at time step , it then selects a data rate to transmit\u02c6( ),\u02c6( ) \u2208 {\u02c60,\u02c61, . . . ,\u02c6}, data packets to the nearby BS, where\u00ee s the maximum number of data packets that the AV can transmit to the nearby BS. As described in Section III-A, the value of\u02c6( ) depends on the data rate of the AV's transmitter. The AV can transmit the maximum number of data packets\u02c6when the channel condition is good and the AV selects the highest data rate . Otherwise, the number of packets that can be successfully transmitted to the nearby BS decreases as the channel quality is low. If the AV chooses the radar detection mode, it then selects a mode to detect objects on the road. According to the selected mode , the miss detection ratio ( ) can be obtained as in (5). Here, we assume that the packet error ratio is 0%, i.e., no packets loss during transmission. Instead, by introducing the above rate adaptation scheme, our proposed framework still can capture the impacts of communication channel to the number of packets that can be successfully transmitted to the nearby BS, which achieves the same behavior as packet error ratio. We evaluate the system performance by using two key metrics: (i) system throughput and (ii) miss detection probability. The system throughput can be determined by the total number of successfully transmitted data packets to the nearby BS, i.e.,\n\u03a8 = \u2211\ufe01 =1\u02c6( ),(6)\nwhere is the time horizon. The miss detection probability can be calculated as the average miss detection ratio. Thus, the miss detection probability, denoted by , can be defined as follows:\n= 1 \u2211\ufe01 =1 ( ).(7)\nUnder the dynamic and uncertain environment as well as unexpected events, it is challenging for the AV to optimally select actions to optimize the system performance. The problem is even more challenging when the action space of the AV, i.e., the number of possible actions in the sets R and B, increases. Thus, all current static optimization tools in [11]- [15] may not be effective in dealing with these practical issues. Aforementioned challenges motivate us to develop a dynamic framework which can not only capture the dynamic of environment but also learn incomplete information as well as uncertainty of unexpected events. Accordingly, we formulate the dynamic optimization problem as an MDP and then design a highly-effective deep reinforcement learning algorithm to find the optimal policy without requiring any prior information about the environment. Furthermore, we propose a new transfer learning approach to leverage the learned experiences of the AV to facilitate the training process in the new environment.\n\n\nIV. PROBLEM FORMULATION\n\nTo deal with the uncertainty of the environment, we use the MDP [21] framework to formulate the optimization problem of the system. The MDP is defined by a tuple < S, A, T , >, where S is the state space, A is the action space, T is the state transition probabilities, and is the immediate reward function of the AV system. Therefore, the source and target environments in our problem are formulated as two MDPs M =< S , A , T , > and M =< S , A , T , >, respectively. We consider that the two environments share the same state space, action space, and immediate reward function [20]. The state transition probabilites of the two environments are different. The MDPs can be rewritten as M =< S, A, T , > and M =< S, A, T , >.\n\n\nA. State Space\n\nAs explained in Section III-B, the state space of the AV system is defined as follows:\nS = ( , , , , ); \u2208 { 1 , 2 , . . . , }, \u2208 { 1 , 2 , . . . , }, \u2208 { 1 , 2 , . . . , }, \u2208 { 1 , 2 , . . . }, and \u2208 { 1 , 2 , . . . , } ,(8)\nwhere , , , , and represent the state of the communication channel, state of the road, state of the weather, speed state of the AV, and state of nearby vehicles, respectively. The state of the system at time step is defined as = ( , , , , ) \u2208 S. Note that unlike conventional approaches in [13]- [15] in which additional information of the environment is required, e.g., data distribution of the environment, our approach needs only the instantaneous states of the environmental factors by using AV's sensing system, e.g., road friction sensor, weather station instrument, speedometer, and cameras.\n\n\nB. Action Space\n\nAs described in Section III-C, the possible action at time step of the AV can be defined as follows:\n= \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 , 1 \u2264 \u2264 , if a radar detection mode is selected, , 1 \u2264 \u2264 , if a data communication mode is se- lected, (9) where\nis the action when the AV selects the radar detection mode and is the action when the AV selects the data communication mode to transmit data at the rate . Thus, the action space of the AV can be defined as A = ; \u2208 { , } .\n\n\nC. State Transition Probabilities\n\nThe state transition probabilities of the source and target environments are defined as follows:\nT = ( | , , ), and T = ( | , , ),(10)\nwhere , and , are the probabilities that an unexpected event occurs as defined in (4) of the source environment and target environment, respectively. and are the traffic density values as defined in (5) of the source environment and target environment, respectively. is the new state observed by the AV after taking action . Note that the state transition probabilities illustrate the dynamics of environments and they are unknown to the AV.\n\n\nD. Immediate Reward Function\n\nAt time step given state , the AV selects an action and receives an immediate reward . The immediate reward is designed to maximize jointly the data transmission and radar detection functions of the AV. Thus, the immediate reward can be defined as follows:\n( , ) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1\u02c6( ), if = , givenX, \u2212 2 , if = , given X, \u2212 3 , if = , givenX, 4 (1 \u2212 ( )), if = , given X,(11)\nwhere ( 1 , 2 , 3 , 4 ) are weighting factors, X denotes the occasion when an unexpected event occurs with probability as defined in (4) andX denotes the occasion when no unexpected event happens.\u02c6( ) is the number of data packets successfully transmitted to a nearby BS, ( ) is the miss detection ratio as defined in (5). It is worth noting that, weighting factors ( 1 , 2 , 3 , 4 ) can be adjusted to the requirements of different scenarios. For example, if the AV requires a highly accurate radar detection and the data communication function is secondary, 3 and 4 can be set at large values while 1 and 2 can be set at small values.\n\nThe immediate reward function of the AV in (11) can be described as follows. When there is no unexpected event and the AV selects a data communication mode with data rate , the AV receives a positive reward that is proportional to the number of data packets successfully transmitted to the BS (the first condition in (11)). When there is an unexpected event and the AV selects a data communication mode, it receives a large negative reward, denoted as \u2212 2 (the second condition in (11)). When there is no unexpected event, if the AV selects a radar detection mode, it receives a negative reward, denoted as \u2212 3 (the third condition in (11)). When there is an unexpected event, if the AV selects a radar detection mode, it receives a reward that is inversely proportional to the miss detection ratio ( ) (the last condition in (11)). Note that if the AV does not use the radar detection function for early detection at the beginning of current time slot, it may still obtain the occurrence of the unexpected event at the end of the time slot by using its camera and LiDAR sensors.\n\n\nE. Optimization Formulation\n\nWe first formulate the optimization problem of the AV in the source environment. The details of how the experiences of the AV can be transferred to the target environment are described in Section VI. We formulate a dynamic optimization problem to obtain an optimal policy, denoted by * . Given the current system state, i.e., state of the channel, state of the road, state of the weather, state of the speed, and state of nearby vehicles, the optimal policy determines optimal actions to maximize the average expected return, i.e., the average discounted long-term reward, of the AV system. The optimization problem is then expressed as follows:\nmax ( ) = lim \u2192\u221e 1 E \u2211\ufe01 =0 ( ) ,(12)\nwhere ( ) is the average expected return under the policy , ( ) is the immediate reward under policy at time step as defined in (11), is the time horizon, and \u2208 (0, 1), is the discount factor. The optimal policy * will allow the AV to make optimal decisions at any state , i.e., * = * ( ). In practice, it is impossible for the AV to obtain the complete model of the MDP, i.e., state transition probabilities of the MDP are unknown by the AV in advance, and thus modelfree reinforcement learning approaches are more suitable for finding the optimal policy for the optimization problem formulated as in (12). In the next section, we propose to use Q-learning [22] to obtain the optimal policy for the above optimization problem. After that, we propose a method based on deep reinforcement learning, namely Double Deep Q-Network (DDQN) [26], not only to overcome limitations of Q-learning but also to learn effectively the optimal policy.\n\n\nV. DEEP REINFORCEMENT LEARNING APPROACH FOR AUTOMOTIVE JOINT RADAR-COMMUNICATION CONTROL\n\n\nA. JRC Optimal Policy With Q-learning\n\nWe first propose to use Q-learning algorithm [22] to obtain the optimal policy for the AV system in the source environment. In particular, Q-learning algorithm uses a Q-table as a mapping function between states and actions. Each state-action value, i.e., Q-value, in the Q-table is iteratively updated by using Bellman equation as follows:\n( , ) = ( , ) + + max ( +1 , ) \u2212 ( , ) ,(13)\nwhere = ( , , , ), = ( , , , ), and is the state, action, and learning rate at the time step , respectively. When the Q-table is fully updated, the optimal policy for the AV can be obtained by selecting the highest state-action value in the Q-table at each given state, i.e., * ( ) = argmax * ( , ) where * ( , \u00b7) is the state values of the Q-table when the Q-table is fully updated. To guarantee the convergence, the learning rate is deterministic, non-negative, and satisfies (14) [22]:\n\u2208 (0, 1], \u221e \u2211\ufe01 =1 = \u221e, and \u221e \u2211\ufe01 =1 ( ) 2 < \u221e.(14)\nUnder the conditions in (14), the Q-learning algorithm converges to the optimal policy with probability one [22]. By using a table as a simple data structure to maintain a mapping function between states and actions, Q-learning usually suffers the curse-of-dimensionality problem [25]. For example, when the state and action spaces increase, the size of the Q-table increases quadratically, and thus the time for updating the whole Q-table is longer, meaning that the time for obtaining an optimal policy is also larger. To overcome the limitations of Qlearning, deep reinforcement learning with nonlinear function approximation, i.e., neural networks, has recently introduced to be a highly-effective solution to overcome limitations of conventional Q-table [24]. Thus, in the following, we introduce the deep reinforcement learning-based approach to find the optimal policy for the AV.\n\n\nB. Proposed Deep Reinforcement Learning Approach With Double Deep Q-Network for Optimal JRC Policy\n\nTo overcome the limitations of conventional Qlearning [22], Deep Q-Network (DQN) [23] has recently introduced. DQN utilizes (i) an online deep neural network (online network) as a nonlinear function approximation, (ii) an experience replay memory as a buffer of historical data, and (iii) a second target deep neural network (target network) as a periodic copy of the online network. The use of the online network as a nonlinear function approximation helps the DQN to overcome the problem related to the large state-action spaces of Q-learning. In addition, an experience replay memory is utilized to store past experiences to speed up the process of updating weights in the online network. Finally, the target network is periodically replaced by the online network and used for stabilizing the training process. In this work, we develop a mechanism based on a variation of DQN that is Double DQN (DDQN) [26] not only to find the optimal policy for the AV but also to improve the stability of the training process. In particular, the DDQN and DQN algorithms share the same neural networks' architecture. However, the difference between the DDQN and DQN algorithms is that the calculation of target Q-values in the DDQN algorithm can avoid bias in the action selection [26]. An illustration of our proposed approach is shown in Fig. 3. In our proposed approach, the DDQN-empowered AV interacts with the environment through its simulator in the training process. When the training process completes in the simulator, the AV can operate in real-world settings with its trained DDQN model. With probability , select a random action , otherwise select = max ( , ; ) 10 Execute action and observe reward and next state +1 The detailed DDQN algorithm is shown in Algorithm 1 and can be described as follows. The inputs of the network include channel state, road state, weather state, speed state, and surrounding vehicle state. The output is the learned Qvalues. The experience replay memory, denoted as D , with capacity |D |, the weights of the online network and target network, denoted as and , are randomly initialized, i.e., lines 2-5 in Algorithm 1. At each training iteration, the AV selects an action following an -greedy policy, i.e., selecting randomly with probability , and otherwise selects = max ( , ; ) (line 9). The AV executes the action and obtains a reward and the next state +1 (line 10). The tuple of ( , , , +1 ) is stored in D . Next, a mini-batch of transitions ( , , , +1 ) is sampled from D to calculate the target Q-values (line 13). The target at time step is calculated as follows:\n\n= + ( +1 , argmax ( +1 , ; ), ). (15) The DDQN algorithm then updates the online network by minimizing the following loss function at time step :\n( ) = E ( , )\u223cD ( \u2212 ( , ; )) 2 .(16)\nDifferentiating the loss function in (16) with respect to the weights of the online network, we have the following gradient:\n\u2207 ( ) = E ( , , +1 )\u223cD \u2212 ( , ; ) \u2207 ( , ; ) .(17)\nIn practice, the gradient in (17) can be efficiently calculated by using Stochastic Gradient Descent (SGD) algorithm [38]. In general, the loss function in (16) is decayed by a sum over training examples of some per-example loss functions. For instance, the negative conditional log-likehood of the training data can be expressed as:\n( ) = E ( , , +1 )\u223cD ( ) = 1 |D | | D | \u2211\ufe01 =1 (( , , +1 ), ).(18)\nFor the additive cost function in (18), the required gradient descent can be computed in the following way:\n\u2207 ( ) = 1 |D | | D | \u2211\ufe01 =1 \u2207 (( , , +1 ), ).(19)\nThe computation cost for the operation in (19) is ( |D |). Thus, as the size |D | of the replay memory increases, the time to take a single gradient step becomes prohibitively long. By using SGD, we can uniformly sample a mini-batch of experiences from the replay memory D at each step of the algorithm. In general, the mini-batch size can be set to be relatively small number of experiences, e.g., from one to a few hundreds. As such, the training time is significantly reduced. The estimation of the gradient using SGD is then calculated as follows:\n= 1 \u2207 \u2211\ufe01 =1 ( ),(20)\nwhere is the mini-batch size. The SGD algorithm then updates the online network's weights as follows:\n+1 \u2190 \u2212 ,(21)\nwhere is the learning rate of the SGD algorithm. After every steps, the target network's weights are replaced by the online network's weights (lines 16-18 in Algorithm 1). The target network weights remain unchanged between individual updates.\n\n\nC. Complexity Analysis for the Double Deep Q-Network Algorithm\n\nIn this work, the DDQN algorithm uses two deep neural networks that are online network and target network. However, we only consider the computations of the online network because the target network is a periodically replaced by the online network and thus it does not require significantly additional computations. The online network consists of an input layer 0 , two fully-connected layers 1 and 2 , and an output layer 3 . Let | | denote the size, i.e., the number of neurons, of layer . The complexity of the online network can be formulated as | 0 || 1 | + | 1 || 2 | + | 2 || 3 |. At each training step, a number of training samples, i.e., transitions, are randomly taken from the replay memory and fed to the online network for training. Thus, the total complexity of the training process is\n(| 0 || 1 | + | 1 || 2 | + | 2 || 3 |) , where\nis the size of the training batch and is the total number of training iterations. In our paper, the size of 0 is the number of state features, therefore | 0 | = 5. The hidden layers 1 and 2 have 24 neurons each. The output layer consists of |A| neurons in which each neuron corresponds to an action of the AV as explained in Section IV-B. Clearly, the architecture of the online network is simple. Thus, it can be deployed at the AV system which is usually equipped with sufficient computing resources. In the simulation, we show that our proposed algorithm can converge to the optimal policy much faster than the conventional Q-learning algorithm.\n\n\nVI. TRANSFER LEARNING APPROACH FOR ACCELERATING LEARNING PROCESS\n\nIn the previous section, we show that the optimal policy of the source environment can be obtained by using the proposed DDQN algorithm. The optimal policy then can be applied to the target environment with an assumption that the two environments share the same data distribution, i.e., state transition probabilities [20], [27]. However, this assumption may fail in practice due to the AV's mobility, i.e., it can often travel from an environment to another one with different state transition probabilities. For example, the AV can travel from a rural area with a low traffic density, to an urban area with a high traffic density. In such a case, factors such as road condition and weather condition may have different distributions in different environments. Therefore, to achieve a robust and scalable AV system with JRC functions in practice, we need a method that can quickly learn the new optimal policy when the AV moves from one environment to another one. This requirement motivates us to develop the transfer learning approach that can leverage learned experiences to facilitate the learning process in the new environment.\n\nOur considered transfer learning technique can be expressed as follows. Given two different but related environments that are source environment, modelled with MDP M , and target environment, modeled with MDP M , the transfer learning process aims to learn an optimal policy * for the target environment by leveraging exterior information D from M as well as interior information D from M :  T is the state transition probability in the target environment, is the function mapping from states to actions in the target environment and is approximated using deep neural networks, denoted as (\u00b7), trained on both D and D [20]. Several transfer learning approaches have been proposed to solve (22) [20]. In autonomous driving, however, collecting exterior information D can be costly and intractable [28]. Thus, we need an algorithm that can efficiently accelerate the learning with limited amount of exterior information. In this paper, we develop a transfer learning approach for the AV system, namely Transfer Learning with Demonstrations (TLwD) [30]. The key idea of TLwD is to initially pre-train on demonstration data D using a combination of temporal difference (TD) and supervised losses. After pre-training, the TLwD algorithm keeps updating its neural networks with a combination of demonstration data D and newly collected data D . It is worth noting that both D and D are collections of experiences, i.e., tuples of state, action, reward, and next state, of the AV system in the source and target environments, respectively. Our proposed TLwD algorithm is illustrated in Fig. 4.\n* = argmax E ( \u223cT , \u223c ) ( , ) ,(22)\nDetails of our proposed TLwD approach are illustrated in Algorithm 2 and can be described as follows. First, the replay memory D is initialized with demonstration data D , the weights of online network and target network are randomly initialized (lines 2-6 in Algorithm 2). The inputs of the online network include channel state, road state, weather state, speed state, and state of nearby vehicles. The output is learned Q-values. The weights of the online network and target networks are pre-trained as follows (lines [7][8][9][10][11][12][13][14]. At each pretraining step, a mini-batch of transitions is sampled from D with prioritized-experience replay scheme [29]. In particular, the probability of sampling transition can be calculated as follows:\n( ) = \u2208 | D | ,(23)\nwhere > 0 is the priority of transition and |D | is the size Algorithm 2: Transfer Learning with Demonstrations 1 Input: 2 D : initialized with demonstration dataset, 3 : weights for the initial behavior network (random), 4 : weights for the target network (random), 5 : frequency to update target net, 6 : number of pre-training gradient updates. of the replay memory. The exponent determines how much prioritization is used, with = 0 corresponding to the uniform case, i.e., transition is sampled randomly. The priority of transition can be calculated by = | | + , where is the Temporal Difference (TD) error of transition , is a small positive constant that prevents the edge-case of transitions not being revisited once their TD error is equal to 0. With the prioritized-experience replay mechanism, the TLwD algorithm can sample important transitions in the demonstration data D and thus can significantly reduce the size of D [29], [30]. To account for the change in the distribution, updates to the network are weighted with importance sampling weights as follows:\n= 1 |D | \u00b7 1 ( ) ,(24)\nwhere |D | is the size of the replay buffer and controls the amount of importance sampling with no importance sampling when = 0 and full importance sampling when = 1. is annealed linearly from 0 to 1. By using the mini-batch sampled with prioritization, the online neural network weights are updated by minimizing the overall loss ( ), which is the combination of the DDQN loss, the margin classification loss, multi-step TD error loss, and L2 regulation loss. The overall loss ( ) is defined as:\n\n( ) = ( ) + 1 ( ) + 2 ( ) + 3 2 ( ), (25) where the component losses are calculated as follows. First, the DDQN loss is calculated as:\n( ) = + ( +1 , ; ) \u2212 ( , ; ) 2 , (26) where\nand are the parameters of the online network and target network, respectively, and = argmax ( +1 , ; ). The multi-step TD loss ( ) is calculated as:\n( ) = ( ) + max ( + , ; ) \u2212 ( , ; ) 2 ,(27)\nwhere ( ) is n-step return from a given state which can be calculated as:\n( ) = \u22121 \u2211\ufe01 =0 + .(28)\nThe margin classification loss between data sampled from D and data sampled from D is defined as:\n( ) = max[ ( , ) + \u0393( , )] \u2212 ( , ),(29)\nwhere is the action the expert demonstrator, i.e., DDQN with D , takes in state and \u0393( , ) is a margin function that is equal to 0 when = and equal to 1 otherwise. Finally, the L2 regularization loss 2 ( ) is applied to the weights and biases of the neural network to prevent over-fitting on the relatively small demonstration dataset. In addition, 1 , 2 , and 3 are weighting parameters of the multi-step TD loss, classification loss, and regulation loss, respectively.\n\nWhen the pre-training process completes, the AV starts training process in the target environment (lines 15-26 in Algorithm 2). In particular, the training processes of the TLwD and DDQN algorithms have three main differences that are (i) the structure of replay memory D , (ii) the prioritizedexperience replay mechanism, and (iii) the combined losses ( ). In the next section, we will show the advantages of the three above techniques over the DDQN algorithm in terms of convergence rate.\n\nIt is noted that the computational complexity of the proposed TLwD algorithm is similar to that of DDQN algorithm since the two algorithms share the same neural network's architecture. In particular, the complexity of TLwD can be expressed as * (| 0 || 1 | + | 1 || 2 | + | 2 || 3 |) , where * = + is the total number time steps of the pretraining process on demonstration data, i.e., , and the online training process on newly collected data from the target environment, i.e., .\n\n\nVII. PERFORMANCE EVALUATION\n\n\nA. Parameter Settings\n\nIn this paper, we consider a scenario in which an AV travels from the source environment to the target environment as shown in Fig. 1. We consider our problem to be episodic in which the maximum number of time steps in each episode is 300. The detailed settings for the joint radar-communications functions and environment can be described as follows.\n\n1) Joint Radar-Communication Functions Settings: a) Data communication function: At time step , if the AV chooses to use the data communication function, it can transmit data at a low data rate or a high data rate, i.e., \u2208 { 1 , 2 }. As described in Section III-A, when the channel condition is good, the AV can successfully transmit\u02c6( ) = 4 or\u02c6( ) = 2 packets to a nearby BS by using high data rate transmission or low data rate transmission, respectively. However, when the channel condition is bad, the AV can successfully transmit\u02c6( ) = 0 or\u02c6( ) = 2 packets to a nearby BS by using high data rate transmission or low data rate transmission, respectively. We assume that the communication channel is in a good condition (e.g., low interference) with probability 0 . Therefore, the channel is in a bad condition (e.g., high interference) with probability 1 \u2212 0 .\n\nb) Radar detection function: For the radar detection function, we consider parameters obtained from a real automotive radar sensor from [31]. In particular, at a time step , if the AV uses the radar detection function, it can choose the long-range detection mode or short-range detection mode, i.e., \u2208 { 1 , 2 }. If the AV chooses the long-range detection mode, i.e., = 2 , its radar operates with the values of sweep bandwidth and frequency slope ( 1 , 1 ) = (300MHz, 10MHz/ ). According to (2), these values of ( 1 , 1 ) are equivalent to the maximum detection range and range resolution ( 1 , 1 ) = (225 , 0.5 ), respectively. In other words, the AV can detect objects that are in a range of 225 and can distinguish two objects as long as the distance between two objects is more than 0.5 . On the other hand, if the AV chooses the short-range detection mode, its radar operates with ( 2 , 2 ) = (750MHz, 15MHz/ ). These values of ( 2 , 2 ) are equivalent to ( 2 , 2 ) = (45 , 0.2 ). It can be observed from the values of ( , ) that longrange detection mode results in a low-quality resolution (i.e., high value of ), meaning that it is more difficult to detect two closely spaced objects. In contrast, short-range detection can detect objects in a short distance, i.e., 45 , with highquality resolution, i.e., 0.2 . A summary of parameters used in simulation is shown in Table I.\n\n2) Environment Settings: We set the state of the communication channel, state of the road, state of the weather, speed state, and the state of nearby vehicles as \u2208 {0, 1}, \u2208 {0, 1}, \u2208 {0, 1}, \u2208 {0, 1}, and \u2208 {0, 1}, respectively. In particular, = 1, = 1, = 1, = 1, and = 1 represent unfavorable conditions, i.e., bad channel condition, rough road, rainy weather, high speed of the AV, and with nearby vehicles, respectively. In contrast, = 0, = 0, = 0, = 0, and = 0 represent favorable conditions, i.e., good channel condition, smooth road, good weather, low speed of the AV, and without nearby vehicles, respectively. Note that the generalization of the states beyond 0 and 1 is straightforward. To model the dynamics of environment, the probabilities that an unexpected event occurs at the given speed state and weather state, denoted as and , respectively, in (4) are taken from the real-world data in [32], [33], and other probabilities are assumed to be pre-defined. The probabilities that an unexpected event occurs at low speed and high speed of the AV, denoted as 0 and 1 , respectively, are taken from [32] in which if the AV's speed exceeds 60 km/h, the AV's speed is high and otherwise the AV's speed is low. Specifically, the values of 0 and 1 are set to be 0.005 and 0.1, respectively. Rain can be considered  a common unfavorable weather state, and as in [33], we set 1 = 0.046 and 0 = 0.005. We consider the traffic density and the sizes of objects on the road and the sidewalk are realistic parameters based on nuScenes dataset [35]. nuScenes is a large public dataset that provides a benchmark from measurements of camera, radar and LiDAR sensors on autonomous vehicles. nuScenes dataset contains well-classified 23 object classes recorded from 242km traveled in Boston and Singapore. From [35], we extract distributions of the traffic density, length, and width of the objects and input these parameters into our simulation. In particular, we consider two most common types of objects in nuScenes that are car and pedestrian. The average values of the number of cars and the number of pedestrians in a time step are 20 and 7, respectively [35]. Therefore, we set the average value of traffic density, denoted as , at 27 objects within the detection range of (45 \u00d7 45) meters in cases the AV uses the short-range radar detection mode. In the case that the AV uses the long-range radar detection mode, the average value of traffic density is set at 27 \u00d7 225 45 \u00d7 225 45 = 675 objects within the detection range of (225 \u00d7 225) meters. In the rest of the paper, we denote as the average number of objects in the detection range of (45 \u00d7 45) meters for the sake of simplicity, and the value of with the long-range radar detection mode can be calculated as mentioned above. In addition, the car's length, car's width, pedestrian's length, and pedestrian's width are fitted as the normal distributions that are N (4.62, 0.18), N (1.92, 0.08), N (0.73, 0.085), and N (0.68, 0.055), respectively [35].\n( , ) (0.4, 0.6 \u2192 1.0) ( 1 , 1 ) (300MHz, 10MHz/ s) ( 2 , 2 ) (750MHz, 15MHz/ s) ( 1 , 2 , 3 , 4 ) (1,\nAt each time step in the simulation, we generate objects that are randomly spaced in the detection range of ( \u00d7 ) meters of the AV. In the simulation, these parameters are represented as a set of 2D arrays and the AV can only obtain these parameters if it uses the radar detection mode in that time step. In addition, we consider that an unexpected event occurs with probability as defined in (4).\n\n\n3) Neural Networks Settings:\n\nFor the DDQN algorithm, we adopt parameters based on the common settings for designing neural networks [23], [38]. Specifically, two fully-connected hidden layers are implemented together with input layer and output layer. The size of the hidden layer is 24. The mini-batch size is set at 64. The size of the experience replay memory is 50,000, and the target network is updated every 300 iterations. For the TLwD parameters, we use the same parameters as in [30]. The weighting factors between losses in (25) are set at ( 1 , 2 , 3 ) = (1, 1, 10 \u22125 ). The number of pre-training steps is 10,000. The size of demonstration data and experience replay memory are 20,000 and 50,000, respectively. The parameters used in the simulation are summarized in Table I. 4) Transfer Learning Setting: In the transfer learning setting, we first let the AV obtain an optimal policy and demonstration data of the source environment. Note that the information of source environment can be pre-computed by other AVs and it can be stored at an edge server. After that, we let the AV continue learning through updating its neural networks' weights in the target environment. The differences between the source and target environments are shown in Table I. To obtain the optimal policy of the source environment, we evaluate the reward performance of the DDQN algorithm with two baseline schemes that are Q-learning and Round Robin [19]. Round Robin is a rule-based mechanism in which the AV selects actions in an iterative manner so that all the actions in the action space are equally selected. As a result, Round Robin can be utilized as a non-learning scheme which yields the stable and lower bound performance. In the target environment, we evaluate our proposed transfer learning algorithm by comparing with the two baseline schemes that are DDQN and Direct Policy Reuse. The Direct Policy Reuse (DPR) is a mechanism in which the policy learned from the source environment is directly used in the target environment [27], [39]. Note that with the DDQN algorithm, the AV learns the environment from scratch, i.e., the weights of neural networks are initialized randomly. The reason that we use the DDQN algorithm as a baseline scheme is to evaluate the advantages of our proposed transfer learning approach in the transfer learning setting. In the rest of the paper, we use the term DDQN (without TL) to denote the results obtain by the DDQN algorithm without using any transfer learning technique, i.e., the AV with DDQN algorithm learns the new environment from scratch.\n\nIn the next subsection, we evaluate system performance in terms of average reward, system throughput, and miss detection probability. The average reward of an episode can be calculated by averaging all the immediate rewards. The system throughput and miss detection probability are calculated as in (6) and (7), respectively.\n\nB. Simulation Results 1) Transfer Learning Experiment: First, we let the AV obtain the optimal policy in the source environment by using the DDQN, Q-learning, and Round Robin algorithms. In Fig. 5(a), we show the average reward values of the DDQN, Q-learning, and Round Robin algorithms in the source environment. The result shows that with the same -greedy scheme, the Qlearning algorithm is unable to obtain the optimal policy while the DDQN algorithm can achieve the optimal policy after approximate 200 episodes. This means that the Qlearning algorithm has a slow convergence due to the curseof-dimensionality problem and the considered environment is challenging to learn. With Round Robin algorithm, the actions are selected iteratively and thus the optimal policy cannot be obtained. When the DDQN algorithm converges to the optimal policy and achieves stable average reward values after 400 episodes, we obtain the weights of the neural network, denoted as * , and the demonstration data D from the source environment. The neural network's weights * and demonstration data D are used for initializing DPR and TLwD in the target environment later. Next, the AV with the obtained optimal policy and demonstration data is used for the learning process in the target environment. In the target environment, all algorithms use the same -greedy scheme in which the value of is reset to 1.0 and is decreased until = 0.01. In particular, the probability that an unexpected event occurs and the traffic density of the two environments are different (Table I). Fig. 5(b) shows the average reward values of the proposed TLwD algorithm, compared to DPR, DDQN, Q-learning, and Round Robin algorithms. We observe that the average reward values of TLwD and DPR converge much faster than DDQN. This implies that the transfer learning mechanisms achieve better performance than that of the traditional deep reinforcement learning mechanism. The results also show that the Q-learning and Round Robin algorithms are unable to learn the optimal policy in the target environment. Furthermore, we can observe that although the two learning curves of TLwD and DPR algorithms converge from the 100th episode, the TLwD algorithm achieves better performance in the first 100 episodes with more stable and higher average rewards. The reason is that the TLwD algorithm leverages the prioritized-experience replay mechanism that helps to sample important transitions in the experience replay memory. Note that the experience replay memory D in TLwD algorithm contains two types of data that are the demonstration data obtained from the source environment and the new generated data from the target environment. With prioritized-experience replay mechanism, the AV can sample more important transitions in the demonstration data during early episodes to accelerate the exploration. In the later episodes, the AV can balance the sampling ratio between the demonstration data and new generated data with the sampling probability as defined in (23). In addition, the TLwD algorithm also utilizes multi-step TD learning, i.e., n-step TD learning as defined in (27), that helps to accelerate the training process [37]. With the DPR algorithm, the sampling mechanism is the same as that of DDQN algorithm, i.e., the transitions are sampled randomly without using the multi-step learning mechanism. The reason that the DPR outperforms DDQN can be explained as follows. The weights of neural networks in DPR are already trained with data in the source environment and then further updated in the target environment. Although the source and target environments have different state transition probabilities, their data can share some correlation that benefits the weight updating process [40]. With DDQN, the weights of neural networks are randomly initialized and trained from scratch.\n\nIn the rest of this section, we consider performances of only three algorithms that are TLwD, DPR and DDQN in the target environment since the Q-learning and Round Robin algorithms do not perform well in the transfer learning setting.\n\n2) Performance Evaluation: To further evaluate the robustness of the proposed TLwD algorithm, we vary the probability that an unexpected event occurs when the AV moves at a high speed, i.e., 1 in (4), and show the results of the three algorithms in Fig. 6. It is worth noting that each point in Fig. 6, 7, and 8 is obtained by averaging metric values in the first 1000 episodes, including the exploration when the value of in -greedy scheme is decreased from 1.0 to 0.01. As shown in Fig. 6(a), when 1 increases from 0.1 to 0.9, the TLwD outperforms DPR and DDQN in terms of the average reward. The reason for the upward trend of TLwD and DPR is that when 1 increases, the TLwD and DPR algorithms select the radar detection mode more frequently to avoid miss detection penalties and obtain greater rewards. With the values of 1 from 0.6 to 0.9, we can observe that the average reward values of DDQN decrease significantly. The reason is that with the high values of 1 , DDQN takes more episodes to achieve the optimal policy compared to those of the TLwD and DPR, thus resulting in lower average reward values. For example, with 1 = 0.8, DDQN takes 400 episodes to converge to the optimal policy, while TLwD and DPR take around 150 episodes to achieve similar reward values. Moreover, the average reward values of TLwD are slightly higher than that of DPR because in the early episodes, TLwD can outperform DPR as discussed in Fig. 5(b). The results of throughput and miss detection probability of the three algorithms are shown in Fig. 6(b) and Fig. 6(c), respectively. Note that since the AV is able to perform either radar or data communication at each time step, there is a tradeoff between the throughput and miss detection probability. In particular, a higher throughput results in a higher miss detection probability and vice versa. In Fig. 6(b) and Fig. 6(c), we can observe that TLwD obtains more stable results for miss detection probability and throughput by following the policy that minimizes both miss detection probability and throughput. With the DPR, the achieved miss detection probability is higher than that of TLwD. Without using any transfer learning technique, the DDQN algorithm obtains high throughput and high miss detection probability at the same time. In particular, with this transfer learning setting, the miss detection probability obtained by the proposed TLwD algorithm is much lower than that of the conventional DDQN algorithm, reduced by up to 67% as shown in Fig. 6(c).\n\nNext, we also evaluate the performance of the proposed TLwD algorithm by varying the probability that the factor is at state 0 (favorable condition), i.e., 0 in (4). In particular, a higher value of 0 results in a higher probability that an environment state (e.g., communication channel state, and weather state) is at a favorable condition (e.g., good communication channel condition and good weather condition). Without loss of generality, we increase all the values of ( 0 , 0 , 0 , 0 , 0 ) at the same time, where 0 , 0 , 0 , and 0 are defined in (4), and 0 is the communication channel switching probability. We observe from Fig. 7(a) that when 0 increases from 0.1 to 0.9, average reward values of the three algorithms reduce significantly. However, the results of the TLwD algorithm are slightly better than that of DPR and much higher than that of DDQN, especially with the low values of 0 , i.e., 0 < 0.7. The reason for this downward trend is that the increase of 0 results in reducing the probability of an unexpected event to occur. With the low values of 0 , meaning that unexpected events occur more frequently, the TLwD and DPR algorithms significantly outperform the DDQN algorithm, thanks to the transfer learning techniques. In contrast, with high values of 0 , i.e., 0 \u2265 0.7, meaning that unexpected events are less likely to occur, the performance gaps among TLwD, DPR, and DDQN are shortened. With 0 = 0.9, we observe that the average reward of DDQN is better than those of TLwD and DPR, thanks to the policy of DDQN that significantly increases the throughput and thus increases the miss detection probability at the same time. The above results also reveal an interesting finding that transfer learning is more efficient when the system's dynamic is considerably high, which is equivalent to the scenarios with small values of 0 . When the system's dynamic is low, e.g., less unexpected events occur, the transfer learning might not be effective as DDQN. In Fig. 7(b), with the high values of 0 , i.e., 0 \u2265 0.7, the throughput of DDQN increases significantly while the throughputs of the TLwD and DPR algorithms slightly increase. Consequently, the miss detection probability of DDQN increases when 0 \u2265 0.7 and the miss detection probabilities of TLwD and DPR algorithms still remain at low values.\n\nIn Fig. 8, we vary the average traffic density and evaluate the performance of the proposed TLwD algorithm. Fi. 8(a) shows that the average rewards of the three algorithms decrease when increases from 8 to 40, and the average rewards obtained by TLwD are always higher than those of DPR and DDQN. The reason is that the three algorithms are likely to follow the policies that select the radar detection mode more frequently. As a result, this leads to the decrease of immediate reward obtained from the third and the fourth conditions in (11). Accordingly, the miss detection probability values are increased as shown in Fig. 8(c). In particular, the TLwD algorithm can reduce the miss detection probability by up to 22% and 61% compared to the DPR and DDQN algorithms, respectively. The results for the throughput in Fig. 8(b) are similar to the above scenarios (i.e., Fig. 6(b) and Fig. 7(b)) in which the TLwD and DPR algorithms achieve lower throughputs than that of DDQN.\n\n\n3) Benefits Of Prioritized-experience Replay Mechanism:\n\nAs discussed in the previous results, the prioritized-experience replay mechanism has a significant impact on the performance of the TLwD algorithm. We then evaluate the performance of the TLwD algorithm with and without the prioritizedexperience replay mechanism. In particular, the prioritizedexperience replay mechanism can be controlled by adjusting two parameters and in (23) and (24). For our prioritizedexperience replay mechanism in the TLwD algorithm, we use the common setting in which the value of is fixed at 0.4 and the value of is increased from 0.6 to 1.0 [29], denoted as ( , ) = (0.4, 0.6 \u2192 1.0). We denote that with the values ( , ) = (0, 0), the TLwD algorithm uses uniform sampling mechanism in which the transitions in the replay memory are sampled randomly. Fig. 9 shows that the TLwD algorithm with the prioritized-experience replay mechanism (blue line) acquires faster convergence rate than that of the TLwD algorithm with the uniform sampling mechanism (orange line). This result can confirm the advantage of using prioritizedexperience replay technique over traditional uniform sampling technique [29], [37].   \n\n\nVIII. CONCLUSIONS\n\nIn this paper, we have developed the dynamic framework for joint radar-communications system of autonomous vehicles under the dynamic and uncertainty of surrounding environment. In particular, the dynamic of environment such as communication channel, weather, road and traffic conditions can be captured through the MDP framework and then the optimal policy for the AV can be obtained by using the proposed DDQN algorithm. Furthermore, we have developed a transfer learning approach, integrating with the proposed DDQN algorithm, that enables the AV to leverage valuable experiences from similar domains, and thereby significantly improving the AV's system performance when it moves to a new environment. Extensive simulations have demonstrated that the transfer learning approach has the convergence rate much faster than those of the conventional deep reinforcement learning approaches in the transfer learning setting. \n\n\nN. Q.Hieu  and D. T. Hoang are with University of Technology Sydney, NSW, Australia, emails: {hieu.nguyen-1@student.uts.edu.au, hoang.dinh@uts.edu.au}. D. Niyato is with with Nanyang Technological University, Singapore, email: dniyato@ntu.edu.sg. P. Wang is with York University, Canada, email: pingw@yorku.ca. D. I. Kim is with Sungkyunkwan University, Korea, email: dikim@skku.ac.kr. C. Yuen is with Singapore University of Technology and Design, Singapore, email: yuenchau@sutd.edu.sg. This research was supported in part by the Australian Research Council under the DECRA project DE210100651. This research is supported in part by the programme DesCartes -the National Research Foundation, Prime Minister's Office, Singapore under its Campus for Research Excellence andTechnological Enterprise (CREATE) programme and under its Emerging Areas Research Projects (EARP) Funding Initiative, Alibaba Group through Alibaba Innovative Research (AIR) Program and Alibaba-NTU Singapore Joint Research Institute (JRI), the National Research Foundation, Singapore under the AI Singapore Programme (AISG) (AISG2-RP-2020-019), and Singapore Ministry of Education (MOE) Tier 1 (RG16/20). This research was supported in part by the National Research Foundation of Korea (NRF) Grant funded by the Korean Government (MSIT) under Grant 2021R1A2C2007638 and the MSIT under the ICT Creative Consilience program (IITP-2020-0-01821) supervised by the IITP. This research is supported by A*STAR under its RIE2020 Advanced Manufacturing and Engineering (AME) Industry Alignment Fund -Pre Positioning (IAF-PP) (Grant No. A19D6a0053. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of A*STAR.\n\nFig. 1 :\n1An illustration of an AV system equipped with JRC functions.\n\nFig. 2 :\n2Frequency Modulated Continuous Wave (FMCW) mmWave radar with multiple chirp configurations[31].\n\n\ntransition ( , , , +1 ) in D 13 Sample random mini-batch of transitions ( , , , +1 ) from D 14 Set as in (15): = + ( +1 , argmax ( +1 , ; ), ) 15 Perform a gradient descent step on ( \u2212 ( , , )) 2 according to (: * ( ) = argmax * ( , ; ).\n\nFig. 4 :\n4Transfer Learning with Demonstrations (TLwD) approach for the AV. where = (D \u223c M , D \u223c M ).\n\n\nobserve ( +1 , ) 18 Store ( , , +1 , ) into D , overwrite the oldest collected transition if over capacity 19 Sample a mini-batch of transitions from D with priotization as in (23) 20 Calculate loss ( ) in (25) : * ( ) = argmax * ( , ; )\n\nFig. 5 :\n5(a) Average reward of the DDQN, Q-learning, and Round Robin algorithms in the source environment, and (b) average reward of the TLwD, DPR, DDQN, Q-learning, and Round Robin in the target environment.\n\nFig. 6 :Fig. 7 :\n67(a) Average reward, (b) throughput, and (c) miss detection probability vs. probability that unexpected event occurs with high speed of AV ( 1 ). (a) Average reward, (b) throughput, and (b) miss detection probability vs. probability that the factor is in favorable condition ( 0 ).\n\nFig. 8 :\n8(a) Average reward, (b) throughput, and (c) miss detection probability vs. average traffic density ( ).\n\nFig. 9 :\n9Average rewards of TLwD with ( , ) = (0.4, 0.6 \u2192 1.0) and TLwD with ( , ) = (0, 0) (i.e., uniform sampling).\n\n\nInitialize a new state \u2208 SInput \nlayer \n\nHidden \nlayer \n\nOutput \nlayer \n\nHidden \nlayer \n\n. \n. \n. \n\n. \n. \n. \n\n. \n. \n. \n\nChannel \n\nRoad \n\nWeather \n\nSpeed \n\nVehicles \n\nTarget Network \nOnline Network \n\nFunction Approximation Error \n\nNetwork Update \n\nMini-batch \n\nQ-Value in Target Network \nQ-Value in Online Network \n\nOptimal \nPolicy \n\nOptimal \nAction \n\nReplay \nMemory \n\nReward and next state \nAction \n\nCurrent state \n\nState \n\nDDQN-empowered \nAV \n\nFig. 3: Double Deep Q-Network (DDQN) approach for the \nAV. \n\nAlgorithm 1: Double Deep Q-Network-based Algo-\nrithm For Finding Optimal Policy \n\n1 Inputs: \n2 D : initialize replay memory with capacity |D |, \n3 : weights for the initial online network are randomly \ninitialized, \n\n4 \n\n: weights for the target network are randomly \ninitialized, \n\n5 \n\n: frequency to update target network, \n6 for episode = 1, E do \n\n7 \n\n8 \n\nfor t = 1, T do \n\n9 \n\n\n\nTABLE I :\nIParameter settingParameter \nSource Env. \nTarget Env. \n0.99 \n-greedy \n(1.0 \u2192 0.01) \n64 \n( 1 , 2 , 3 ) \n(1.0, 1.0, 10 \u22125 ) \n(|D |, |D |) \n(20000, 50000) \n( \n, ) \n(10000, 300) \n\n\n\nNguyen Quang Hieu received the B.Eng. degree in Hanoi University of Science Technology, Vietnam in 2018. He is a Ph.D. student in University of Technology Sydney, Australia. He worked as an research assistant in Nanyang Technological University, Singapore from 2019 to 2021. His research interests include wireless communications, network optimization, stochastic optimization, reinforcement learning, and deep learning. Dinh Thai Hoang (Member, IEEE) received the Ph.D. degree in computer science and engineering from Nanyang Technological University, Singapore, in 2016. He is currently a Faculty Member with the School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia. His research interests include emerging topics in wireless communications and networking, such as machine learning, ambient backscatter communications, IRS, edge intelligence, cybersecurity, the IoT, and 5G/6G networks. He received several awards, including the Australian Research Council. He is also an Editor of IEEE TRANSACTIONS ON WIRELESS COMMUNICATIONS, IEEE TRANS-ACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING, and IEEE WIRELESS COMMUNICATIONS LETTERS, and an Associate Editor of IEEE COMMUNICATIONS SURVEYS AND TUTORIALS. Dusit Niyato (Fellow, IEEE) is a professor in the School of Computer Science and Engineering, at Nanyang Technological University, Singapore. He received B.Eng. from King Mongkuts Institute of Technology Ladkrabang (KMITL), Thailand in 1999 and Ph.D. in Electrical and Computer Engineering from the University of Manitoba, Canada in 2008. His research interests are in the areas of Internet of Things (IoT), machine learning, and incentive mechanism design. Ping Wang (Fellow, IEEE) is an Associate Professor at the Department of Electrical Engineering and Computer Science, York University, and a Tier 2 York Research Chair. Prior to that, she worked with Nanyang Technological University, Singapore, from 2008 to 2018. Her research interests are mainly in the area of wireless communication networks, cloud computing and Internet of Things with the recent focus on integrating Artificial Intelligence (AI) techniques into communications networks. She has published more than 250 papers/conference proceedings papers. Her scholarly works have been widely disseminated through top-ranked IEEE journals/conferences and received the Best Paper Awards from IEEE Wireless Communications and Networking Conference (WCNC) in 2022, 2020 and 2012, from IEEE Communication Society: Green Communications & Computing Technical Committee in 2018, and from IEEE International Conference on Communications (ICC) in 2007. Her work received 21,000+ citations with H-index 70 (Google Scholar). She is an IEEE Fellow and a Distinguished Lecturer of the IEEE Vehicular Technology Society.\n\nAutonomous Vehicle Market Outlook. 2026Autonomous Vehicle Market Outlook: 2026, May 2018. Available On- line: https://www.alliedmarketresearch.com/autonomous-vehicle-market. Last Accessed on: March 2021.\n\nJoint radarcommunication strategies for autonomous vehicles: Combining two key automotive technologies. D Ma, N Shlezinger, T Huang, ; Y Liu, Y C Eldar, IEEE Signal Processing Magazine. 374D. Ma, N. Shlezinger, T. Huang; Y. Liu, and Y. C. Eldar, \"Joint radar- communication strategies for autonomous vehicles: Combining two key automotive technologies\", IEEE Signal Processing Magazine, vol. 37, no. 4, pp. 85-97, Jul. 2020.\n\nOn the effectiveness of OTFS for joint radar parameter estimation and communication. L Gaudio, M Kobayashi, G Caire, G Colavolpe, IEEE Transactions on Wireless Communications. 199L. Gaudio, M. Kobayashi, G. Caire, and G. Colavolpe, \"On the effective- ness of OTFS for joint radar parameter estimation and communication,\" IEEE Transactions on Wireless Communications, vol. 19, no. 9, pp. 5951- 5965, Sep. 2020.\n\nA survey of autonomous driving: Common practices and emerging technologies. E Yurtsever, J Lambert, A Carballo, K Takeda, IEEE Access. 8E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, \"A survey of autonomous driving: Common practices and emerging technologies,\" IEEE Access, vol. 8, pp. 58443-58469, Mar. 2020.\n\nEvent-based vision meets deep learning on steering prediction for selfdriving cars. A I Maqueda, A Loquercio, G Gallego, N Garcia, D Scaramuzza, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. I. Maqueda, A. Loquercio, G. Gallego, N. Garcia, and D. Scaramuzza, \"Event-based vision meets deep learning on steering prediction for selfdriving cars,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Jun. 2018, pp. 5419-5427.\n\nObject Detection Under Rainy Conditions for Autonomous Vehicles: A Review of State-of-the-Art and Emerging Techniques. M Hnewa, H Radha, IEEE Signal Processing Magazine. 381M. Hnewa and H. Radha, \"Object Detection Under Rainy Conditions for Autonomous Vehicles: A Review of State-of-the-Art and Emerging Techniques,\" IEEE Signal Processing Magazine, vol. 38, no. 1, pp. 53-67, Jan. 2021.\n\nThe impact of adverse weather conditions on autonomous vehicles: how rain, snow, fog, and hail affect the performance of a self-driving car. S Zang, M Ding, D Smith, P Tyler, IEEE Vehicular Technology Magazine. 142S. Zang, M. Ding, D. Smith, and P. Tyler, \"The impact of adverse weather conditions on autonomous vehicles: how rain, snow, fog, and hail affect the performance of a self-driving car,\" IEEE Vehicular Technology Magazine, vol. 14, no. 2, pp. 103-111, Mar. 2019.\n\nRadio Resource Management in Joint Radar and Communication: A Comprehensive Survey. N C Luong, X Lu, D T Hoang, D Niyato, D I Kim, IEEE Communications Surveys & Tutorials. to be publishedN. C. Luong, X. Lu, D. T. Hoang, D. Niyato, and D. I. Kim, \"Radio Resource Management in Joint Radar and Communication: A Com- prehensive Survey,\" IEEE Communications Surveys & Tutorials, to be published.\n\nJoint automotive radar-communications waveform design. S H Dokhanchi, M R B Shankar, Y A Nijsure, T Stifter, S Sedighi, B Ottersten, IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC). S. H. Dokhanchi, M. R. B. Shankar, Y. A. Nijsure, T. Stifter, S. Sedighi, and B. Ottersten, \"Joint automotive radar-communications waveform design,\" 2017 IEEE 28th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC), Oct. 2017, pp. 1-7.\n\nInvestigating the IEEE 802.11ad standard for millimeter wave automotive radar. P Kumari, N Gonzalez-Prelcic, R W Heath, 2015 IEEE 82nd Vehicular Technology Conference (VTC2015-Fall). P. Kumari, N. Gonzalez-Prelcic, and R. W. Heath, \"Investigating the IEEE 802.11ad standard for millimeter wave automotive radar,\" in 2015 IEEE 82nd Vehicular Technology Conference (VTC2015-Fall), Sep. 2015, pp. 1-5.\n\nA mmWave automotive joint radar-communications system. S H Dokhanchi, B S Mysore, K V Mishra, B Ottersten, IEEE Transactions on Aerospace and Electronic Systems. 553S. H. Dokhanchi, B. S. Mysore, K. V. Mishra, and B. Ottersten, \"A mmWave automotive joint radar-communications system,\" IEEE Transac- tions on Aerospace and Electronic Systems, vol. 55, no. 3, pp. 1241-1260, Feb. 2019.\n\nAdaptive virtual waveform design for millimeter-wave joint communication-radar. P Kumari, S A Vorobyov, R W Heath, IEEE Transactions on Signal Processing. 68P. Kumari, S. A. Vorobyov, and R. W. Heath, \"Adaptive virtual waveform design for millimeter-wave joint communication-radar,\" IEEE Transac- tions on Signal Processing, vol. 68, pp. 715-730, Nov. 2019.\n\nPerformance tradeoffs of joint radar-communication networks. P Ren, A Munari, M Petrova, IEEE Wireless Communications Letters. 81P. Ren, A. Munari, and M. Petrova, \"Performance tradeoffs of joint radar-communication networks,\" IEEE Wireless Communications Letters, vol. 8, no. 1, pp. 165-168, Feb. 2019.\n\nJoint Bi-Static Radar and Communications Designs for Intelligent Transportation. N Cao, Y Chen, X Gu, W Feng, IEEE Transactions on Vehicular Technology. 6911N. Cao, Y. Chen, X. Gu, and W. Feng, \"Joint Bi-Static Radar and Com- munications Designs for Intelligent Transportation,\" IEEE Transactions on Vehicular Technology, vol. 69, no. 11, pp. 13060-13071, Nov. 2020.\n\nPerformance analysis of a timesharing joint radar-communications network. P Ren, A Munari, M Petrova, 2020 International Conference on Computing, Networking and Communications (ICNC). P. Ren, A. Munari, and M. Petrova, \"Performance analysis of a time- sharing joint radar-communications network,\" 2020 International Confer- ence on Computing, Networking and Communications (ICNC), Feb. 2020, pp. 908-913.\n\nEnd-to-end ego lane estimation based on sequential transfer learning for self-driving cars. J Kim, C Park, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsJ. Kim and C. Park, \"End-to-end ego lane estimation based on sequential transfer learning for self-driving cars,\" Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition Workshops, 2017, pp. 30-38.\n\nLORA: Loss differentiation rate adaptation scheme for vehicle-to-vehicle safety communications. Y Yao, X Chen, L Rao, X Liu, X Zhou, IEEE Transactions on Vehicular Technology. 663Y. Yao, X. Chen, L. Rao, X. Liu, and X. Zhou, \"LORA: Loss differ- entiation rate adaptation scheme for vehicle-to-vehicle safety communi- cations\", IEEE Transactions on Vehicular Technology, vol. 66, no. 3, pp. 2499-2512, May 2016.\n\nDiscrete-rate adaptation and selection in energy harvesting wireless systems. P S Khairnar, N B Mehta, IEEE Transactions on Wireless Communications. 141P. S. Khairnar and N. B. Mehta, \"Discrete-rate adaptation and selection in energy harvesting wireless systems,\" IEEE Transactions on Wireless Communications, vol. 14, no. 1, pp. 219-229, Jul. 2014.\n\niRDRC: An Intelligent Real-time Dual-functional Radar-Communication System for Automotive Vehicles. N Q Hieu, D T Hoang, N C Luong, D Niyato, IEEE Wireless Communications Letters. 912N. Q. Hieu, D. T. Hoang, N. C. Luong, and D. Niyato, \"iRDRC: An Intelligent Real-time Dual-functional Radar-Communication System for Automotive Vehicles\", IEEE Wireless Communications Letters, vol. 9, no. 12, pp. 2140-2143, Aug. 2020.\n\nZ Zhu, K Lin, J Zhou, arXiv:2009.07888Transfer Learning in Deep Reinforcement Learning: A Survey. arXiv preprintZ. Zhu, K. Lin, and J. Zhou, \"Transfer Learning in Deep Reinforcement Learning: A Survey\", arXiv preprint, arXiv:2009.07888, 2020.\n\nA markovian decision process. R Bellman, J. of Math. and Mech. 65R. Bellman, \"A markovian decision process\", J. of Math. and Mech., vol. 6, no. 5, pp. 679-684, 1957.\n\nQ-learning. C J C H Watkins, P Dayan, Mach. Learn. 83-4C. J. C. H. Watkins and P. Dayan, \"Q-learning,\" Mach. Learn., vol. 8, no. 3-4, pp. 279-292, 1992.\n\nHuman-level control through deep reinforcement learning. V Mnih, Nature. 518V. Mnih, et al, \"Human-level control through deep reinforcement learn- ing\", Nature, vol. 518, pp. 529-533, Feb. 2015.\n\n. N C Luong, D T Hoang, S Gong, D Niyato, P Wang, Y-C , N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y-C.\n\nApplications of deep reinforcement learning in communications and networking: A survey. D I Liang, Kim, IEEE Communications Surveys & Tutorials. 214Liang, and D. I. Kim, \"Applications of deep reinforcement learning in communications and networking: A survey,\" IEEE Communications Surveys & Tutorials, vol. 21, no. 4, pp. 3133-3174, May 2019.\n\nReinforcement learning: An introduction. R S Sutton, A G Barto, MIT pressR. S. Sutton and A. G. Barto, \"Reinforcement learning: An introduction,\" MIT press, 2018.\n\nDeep reinforcement learning with double q-learning. H V Hasselt, A Guez, D Silver, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence30H. V. Hasselt, A. Guez, and D. Silver, \"Deep reinforcement learning with double q-learning,\" Proceedings of the AAAI Conference on Artificial Intelligence, vol. 30, no. 1, Mar. 2016.\n\nC T Nguyen, arXiv:2102.07572Transfer Learning for Future Wireless Networks: A Comprehensive Survey. arXiv preprintC. T. Nguyen, et al, \"Transfer Learning for Future Wireless Networks: A Comprehensive Survey,\" arXiv preprint, arXiv:2102.07572, 2021.\n\nA survey of transfer learning. K Weiss, T M Khoshgoftaar, D Wang, Journal of Big data. 319K. Weiss, T. M. Khoshgoftaar, and D. Wang, \"A survey of transfer learning,\" Journal of Big data, vol. 3, no. 1, p. 9, May 2016.\n\nPrioritized experience replay. T Schaul, J Quan, I Antonoglou, D Silver, arXiv:1511.05952arXiv preprintT. Schaul, J. Quan, I. Antonoglou, and D. Silver, \"Prioritized experience replay\", arXiv preprint, arXiv:1511.05952, 2015.\n\nDeep q-learning from demonstrations. T Hester, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32T. Hester, et al, \"Deep q-learning from demonstrations\", Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, Apr. 2018.\n\nProgramming Chirp Parameters in TI Radar Devices. V Dham, Application Report SWRA553. V. Dham, \"Programming Chirp Parameters in TI Radar Devices\", Application Report SWRA553, Texas Instruments, 2017.\n\nTravelling speed and risk of crash involvement on rural roads. C N Kloeden, G Ponte, J Mclean, Australian Transport Safety BureauC. N. Kloeden, G. Ponte, and J. McLean, \"Travelling speed and risk of crash involvement on rural roads\", Australian Transport Safety Bureau, 2001.\n\nHow Do Weather Events Impact Roads. How Do Weather Events Impact Roads. Accessed: February 2021. [Online]. Available: https://ops.fhwa.dot.gov/weather/q1 roadimpact.htm.\n\nPrediction of road accidents: A Bayesian hierarchical approach. M Deublein, M Schubert, B T Adey, J K\u00f6hler, M H Faber, Accident Analysis & Prevention. 51M. Deublein, M. Schubert, B. T. Adey, J. K\u00f6hler, and M. H. Faber, \"Prediction of road accidents: A Bayesian hierarchical approach,\" Acci- dent Analysis & Prevention, vol. 51, pp. 274-291, Mar. 2013.\n\nnuScenes: A multimodal dataset for autonomous driving. A H Lang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionA. H. Lang, et al, \"nuScenes: A multimodal dataset for autonomous driving,\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11621-11631.\n\n1 year, 1000 km: The Oxford RobotCar dataset. W Maddern, G Pascoe, C Linegar, P Newman, The International Journal of Robotics Research. 361W. Maddern, G. Pascoe, C. Linegar, and P. Newman, \"1 year, 1000 km: The Oxford RobotCar dataset,\" The International Journal of Robotics Research, vol. 36, no. 1, pp. 3-15, Jan. 2017.\n\nRainbow: Combining improvements in deep reinforcement learning. M Hessel, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32M. Hessel, et al, \"Rainbow: Combining improvements in deep rein- forcement learning\", Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, Apr. 2018.\n\nDeep Learning. I Goodfellow, Y Bengio, A Courville, MIT PressCambridge, MA, USAI. Goodfellow, Y. Bengio, and A. Courville, \"Deep Learning,\" Cam- bridge, MA, USA: MIT Press, 2016.\n\nProbabilistic policy reuse in a reinforcement learning agent. F Fern\u00e1ndez, M Veloso, Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems. the fifth international joint conference on Autonomous agents and multiagent systemsF. Fern\u00e1ndez and M. Veloso, \"Probabilistic policy reuse in a rein- forcement learning agent,\" Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, May 2006, pp. 720-727.\n\nRobust and efficient transfer learning with hidden parameter markov decision processes. T Killian, G Konidaris, F Doshi-Velez, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence31T. Killian, G. Konidaris, and F. Doshi-Velez, \"Robust and efficient transfer learning with hidden parameter markov decision processes,\" Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no. 1, Feb. 2017.\n\nAutomotive Radar Interference Characterization and Reduction by Partial Coordination. K U Mazher, R W Heath, K Gulati, J Li, 2020 IEEE Radar Conference (RadarConf20). K. U. Mazher, R. W. Heath, K. Gulati, and J. Li, \"Automotive Radar Interference Characterization and Reduction by Partial Coordination,\" in 2020 IEEE Radar Conference (RadarConf20), Sep. 2020, pp. 1-6.\n\nHe has been a first recipient of the NRF of Korea Engineering Research Center in Wireless Communications for RF Energy Harvesting since 2014. He has been listed as a 2020 Highly Cited Researcher by Clarivate Analytics. From 2001 to 2020, he served as an editor and an editor at large of Wireless Communications I for the IEEE. Kim Dong In, Fellow, IEEE) received the Ph.D. degree in electrical engineering from the University of Southern California. Los Angeles, CA, USA; Burnaby, BC, Canada; Suwon, South KoreaSimon Fraser University ; Sungkyunkwan University (SKKU)He served as the Founding Editor-in-Chief for the IEEE Wireless Communications Letters. from 2012 to 2015. He was selected the 2019 recipient of the IEEE Communications Society Joseph LoCicero Award for Exemplary Service to Publications. He is the General Chair for IEEE ICC 2022 in SeoulDong In Kim (Fellow, IEEE) received the Ph.D. degree in electrical engineering from the University of Southern California, Los Angeles, CA, USA, in 1990. He was a Tenured Professor with the School of Engineering Science, Simon Fraser University, Burnaby, BC, Canada. Since 2007, he has been an SKKU-Fellowship Professor with the College of Information and Communication Engineering, Sungkyunkwan University (SKKU), Suwon, South Korea. He is a Fellow of the Korean Academy of Science and Technology and a Member of the National Academy of Engineering of Korea. He has been a first recipient of the NRF of Korea Engineering Research Center in Wireless Communications for RF Energy Harvesting since 2014. He has been listed as a 2020 Highly Cited Researcher by Clarivate Analytics. From 2001 to 2020, he served as an editor and an editor at large of Wireless Communications I for the IEEE Transactions on Communications. From 2002 to 2011, he also served as an editor and a Founding Area Editor of Cross-Layer Design and Optimization for the IEEE Transactions on Wireless Communications. From 2008 to 2011, he served as the Co-Editor-in-Chief for the IEEE/KICS Journal of Communications and Networks. He served as the Founding Editor-in-Chief for the IEEE Wireless Communications Letters, from 2012 to 2015. He was selected the 2019 recipient of the IEEE Communications Society Joseph LoCicero Award for Exemplary Service to Publications. He is the General Chair for IEEE ICC 2022 in Seoul.\n\nHe was a Post-Doctoral Fellow with Lucent Technologies Bell Labs, Murray Hill. Chau Yuen, Fellow, IEEE) received the B.Eng. and Ph.D. degrees from Nanyang Technological University (NTU). Singapore; Singaporerespectivelyhe was with the Institute for Infocomm Research (I2R). Since 2010, he has been with the Singapore University of Technology and Design. Dr. Yuen was a recipient of the Lee Kuan Yew Gold Medal, the Institution of Electrical Engineers Book Prize, the Institute of Engineering ofChau Yuen (Fellow, IEEE) received the B.Eng. and Ph.D. degrees from Nanyang Technological Univer- sity (NTU), Singapore, in 2000 and 2004, respec- tively. He was a Post-Doctoral Fellow with Lucent Technologies Bell Labs, Murray Hill, in 2005. From 2006 to 2010, he was with the Institute for Infocomm Research (I2R), Singapore. Since 2010, he has been with the Singapore University of Technology and Design. Dr. Yuen was a recipient of the Lee Kuan Yew Gold Medal, the Institution of Electrical En- gineers Book Prize, the Institute of Engineering of\n\nHe received the IEEE Asia Pacific Outstanding Young Researcher Award in 2012 and IEEE VTS Singapore Chapter Outstanding Service Award on 2019. Currently, he serves as an Editor for the. Singapore Gold Medal, the Merck Sharp and Dohme Gold Medal, and twice a recipient of the Hewlett Packard Prize. IEEE Vehicular Technology SocietyIEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKINGSingapore Gold Medal, the Merck Sharp and Dohme Gold Medal, and twice a recipient of the Hewlett Packard Prize. He received the IEEE Asia Pacific Outstanding Young Researcher Award in 2012 and IEEE VTS Singapore Chapter Outstanding Service Award on 2019. Currently, he serves as an Editor for the IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY, IEEE System Journal, and IEEE Transactions on Network Science and Engineering . He served as the guest editor for several special issues, including IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, IEEE WIRELESS COMMUNICATIONS MAGAZINE, IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING. He is a Distinguished Lecturer of IEEE Vehicular Technology Society.\n", "annotations": {"author": "[{\"end\":172,\"start\":115},{\"end\":184,\"start\":173},{\"end\":226,\"start\":185},{\"end\":262,\"start\":227},{\"end\":314,\"start\":263},{\"end\":347,\"start\":315},{\"end\":357,\"start\":348},{\"end\":409,\"start\":358},{\"end\":457,\"start\":410},{\"end\":502,\"start\":458},{\"end\":532,\"start\":503},{\"end\":613,\"start\":533},{\"end\":172,\"start\":115},{\"end\":184,\"start\":173},{\"end\":226,\"start\":185},{\"end\":262,\"start\":227},{\"end\":314,\"start\":263},{\"end\":347,\"start\":315},{\"end\":357,\"start\":348},{\"end\":409,\"start\":358},{\"end\":457,\"start\":410},{\"end\":502,\"start\":458},{\"end\":532,\"start\":503},{\"end\":613,\"start\":533}]", "publisher": null, "author_last_name": "[{\"end\":136,\"start\":132},{\"end\":183,\"start\":178},{\"end\":197,\"start\":191},{\"end\":236,\"start\":232},{\"end\":274,\"start\":271},{\"end\":324,\"start\":320},{\"end\":356,\"start\":352},{\"end\":367,\"start\":362},{\"end\":136,\"start\":132},{\"end\":183,\"start\":178},{\"end\":197,\"start\":191},{\"end\":236,\"start\":232},{\"end\":274,\"start\":271},{\"end\":324,\"start\":320},{\"end\":356,\"start\":352},{\"end\":367,\"start\":362}]", "author_first_name": "[{\"end\":125,\"start\":119},{\"end\":131,\"start\":126},{\"end\":177,\"start\":173},{\"end\":190,\"start\":185},{\"end\":231,\"start\":227},{\"end\":267,\"start\":263},{\"end\":270,\"start\":268},{\"end\":319,\"start\":315},{\"end\":349,\"start\":348},{\"end\":351,\"start\":350},{\"end\":359,\"start\":358},{\"end\":361,\"start\":360},{\"end\":125,\"start\":119},{\"end\":131,\"start\":126},{\"end\":177,\"start\":173},{\"end\":190,\"start\":185},{\"end\":231,\"start\":227},{\"end\":267,\"start\":263},{\"end\":270,\"start\":268},{\"end\":319,\"start\":315},{\"end\":349,\"start\":348},{\"end\":351,\"start\":350},{\"end\":359,\"start\":358},{\"end\":361,\"start\":360}]", "author_affiliation": "[{\"end\":456,\"start\":411},{\"end\":501,\"start\":459},{\"end\":531,\"start\":504},{\"end\":612,\"start\":534},{\"end\":456,\"start\":411},{\"end\":501,\"start\":459},{\"end\":531,\"start\":504},{\"end\":612,\"start\":534}]", "title": "[{\"end\":112,\"start\":1},{\"end\":725,\"start\":614},{\"end\":112,\"start\":1},{\"end\":725,\"start\":614}]", "venue": null, "abstract": "[{\"end\":2434,\"start\":831},{\"end\":2434,\"start\":831}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4358,\"start\":4355},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4652,\"start\":4649},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4941,\"start\":4938},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4946,\"start\":4943},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5155,\"start\":5152},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5160,\"start\":5157},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5306,\"start\":5303},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5311,\"start\":5308},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5481,\"start\":5478},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5848,\"start\":5845},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6091,\"start\":6088},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6529,\"start\":6526},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6534,\"start\":6531},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6993,\"start\":6990},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7154,\"start\":7151},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7160,\"start\":7156},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9775,\"start\":9771},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9829,\"start\":9825},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10459,\"start\":10455},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10534,\"start\":10530},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11775,\"start\":11772},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11781,\"start\":11777},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11789,\"start\":11786},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12265,\"start\":12261},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12769,\"start\":12765},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12775,\"start\":12771},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12972,\"start\":12968},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12978,\"start\":12974},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13117,\"start\":13113},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13264,\"start\":13260},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13804,\"start\":13800},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13810,\"start\":13806},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13964,\"start\":13960},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14612,\"start\":14608},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14969,\"start\":14965},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15352,\"start\":15348},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15358,\"start\":15354},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15422,\"start\":15419},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15488,\"start\":15484},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15582,\"start\":15578},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16800,\"start\":16796},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16858,\"start\":16854},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17116,\"start\":17112},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17378,\"start\":17374},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17617,\"start\":17613},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18108,\"start\":18104},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18446,\"start\":18442},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19658,\"start\":19655},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19826,\"start\":19823},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20196,\"start\":20192},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20439,\"start\":20435},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20504,\"start\":20500},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20886,\"start\":20882},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21335,\"start\":21331},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21909,\"start\":21906},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22185,\"start\":22181},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22609,\"start\":22606},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22737,\"start\":22733},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23073,\"start\":23069},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24802,\"start\":24798},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24808,\"start\":24804},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25958,\"start\":25957},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26206,\"start\":26202},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26212,\"start\":26208},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26352,\"start\":26348},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26696,\"start\":26692},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27235,\"start\":27231},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27241,\"start\":27237},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27247,\"start\":27243},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27817,\"start\":27813},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29270,\"start\":29267},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30467,\"start\":30463},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30473,\"start\":30469},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31230,\"start\":31226},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31745,\"start\":31741},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32425,\"start\":32421},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32431,\"start\":32427},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34562,\"start\":34559},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37279,\"start\":37275},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":37335,\"start\":37331},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":37511,\"start\":37507},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":37791,\"start\":37787},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38615,\"start\":38611},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38779,\"start\":38775},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38951,\"start\":38947},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":39430,\"start\":39426},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39715,\"start\":39711},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":39742,\"start\":39738},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40566,\"start\":40562},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40930,\"start\":40926},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":41321,\"start\":41319},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":42488,\"start\":42484},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":42742,\"start\":42738},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":43059,\"start\":43055},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":46062,\"start\":46058},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":46068,\"start\":46064},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":47498,\"start\":47494},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":47569,\"start\":47565},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":47574,\"start\":47570},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":47676,\"start\":47672},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":47925,\"start\":47921},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":49022,\"start\":49019},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":49025,\"start\":49022},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":49028,\"start\":49025},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49032,\"start\":49028},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":49036,\"start\":49032},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":49040,\"start\":49036},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":49044,\"start\":49040},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":49048,\"start\":49044},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":49168,\"start\":49164},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":49442,\"start\":49441},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":49497,\"start\":49496},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":49542,\"start\":49541},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49578,\"start\":49577},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":50210,\"start\":50206},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":50216,\"start\":50212},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":50908,\"start\":50904},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":54332,\"start\":54328},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":54687,\"start\":54684},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":56486,\"start\":56482},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":56492,\"start\":56488},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":56692,\"start\":56688},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":56950,\"start\":56946},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":57125,\"start\":57121},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":57388,\"start\":57384},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":57738,\"start\":57734},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":58586,\"start\":58582},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":59087,\"start\":59084},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":59228,\"start\":59224},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":59234,\"start\":59230},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":59584,\"start\":59580},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":59630,\"start\":59626},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":60537,\"start\":60533},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":61127,\"start\":61123},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":61133,\"start\":61129},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":61982,\"start\":61979},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":65030,\"start\":65026},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":65145,\"start\":65141},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":65197,\"start\":65193},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":65768,\"start\":65764},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":71475,\"start\":71471},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":72358,\"start\":72354},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":72544,\"start\":72540},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":73097,\"start\":73093},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":73103,\"start\":73099},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":75995,\"start\":75991},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4358,\"start\":4355},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4652,\"start\":4649},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4941,\"start\":4938},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4946,\"start\":4943},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5155,\"start\":5152},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5160,\"start\":5157},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5306,\"start\":5303},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5311,\"start\":5308},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5481,\"start\":5478},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5848,\"start\":5845},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6091,\"start\":6088},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6529,\"start\":6526},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6534,\"start\":6531},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6993,\"start\":6990},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7154,\"start\":7151},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7160,\"start\":7156},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9775,\"start\":9771},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9829,\"start\":9825},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10459,\"start\":10455},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10534,\"start\":10530},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11775,\"start\":11772},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11781,\"start\":11777},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11789,\"start\":11786},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12265,\"start\":12261},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12769,\"start\":12765},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12775,\"start\":12771},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12972,\"start\":12968},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12978,\"start\":12974},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13117,\"start\":13113},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13264,\"start\":13260},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13804,\"start\":13800},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13810,\"start\":13806},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13964,\"start\":13960},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14612,\"start\":14608},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14969,\"start\":14965},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15352,\"start\":15348},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15358,\"start\":15354},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15422,\"start\":15419},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15488,\"start\":15484},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15582,\"start\":15578},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16800,\"start\":16796},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16858,\"start\":16854},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17116,\"start\":17112},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17378,\"start\":17374},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17617,\"start\":17613},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18108,\"start\":18104},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18446,\"start\":18442},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19658,\"start\":19655},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19826,\"start\":19823},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20196,\"start\":20192},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20439,\"start\":20435},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20504,\"start\":20500},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20886,\"start\":20882},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21335,\"start\":21331},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21909,\"start\":21906},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22185,\"start\":22181},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22609,\"start\":22606},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22737,\"start\":22733},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23073,\"start\":23069},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24802,\"start\":24798},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24808,\"start\":24804},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25958,\"start\":25957},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26206,\"start\":26202},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26212,\"start\":26208},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26352,\"start\":26348},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26696,\"start\":26692},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27235,\"start\":27231},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27241,\"start\":27237},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27247,\"start\":27243},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27817,\"start\":27813},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29270,\"start\":29267},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30467,\"start\":30463},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30473,\"start\":30469},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31230,\"start\":31226},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31745,\"start\":31741},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32425,\"start\":32421},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32431,\"start\":32427},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34562,\"start\":34559},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37279,\"start\":37275},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":37335,\"start\":37331},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":37511,\"start\":37507},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":37791,\"start\":37787},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38615,\"start\":38611},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38779,\"start\":38775},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38951,\"start\":38947},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":39430,\"start\":39426},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39715,\"start\":39711},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":39742,\"start\":39738},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40566,\"start\":40562},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40930,\"start\":40926},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":41321,\"start\":41319},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":42488,\"start\":42484},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":42742,\"start\":42738},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":43059,\"start\":43055},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":46062,\"start\":46058},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":46068,\"start\":46064},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":47498,\"start\":47494},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":47569,\"start\":47565},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":47574,\"start\":47570},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":47676,\"start\":47672},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":47925,\"start\":47921},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":49022,\"start\":49019},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":49025,\"start\":49022},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":49028,\"start\":49025},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49032,\"start\":49028},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":49036,\"start\":49032},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":49040,\"start\":49036},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":49044,\"start\":49040},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":49048,\"start\":49044},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":49168,\"start\":49164},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":49442,\"start\":49441},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":49497,\"start\":49496},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":49542,\"start\":49541},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49578,\"start\":49577},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":50210,\"start\":50206},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":50216,\"start\":50212},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":50908,\"start\":50904},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":54332,\"start\":54328},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":54687,\"start\":54684},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":56486,\"start\":56482},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":56492,\"start\":56488},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":56692,\"start\":56688},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":56950,\"start\":56946},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":57125,\"start\":57121},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":57388,\"start\":57384},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":57738,\"start\":57734},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":58586,\"start\":58582},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":59087,\"start\":59084},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":59228,\"start\":59224},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":59234,\"start\":59230},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":59584,\"start\":59580},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":59630,\"start\":59626},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":60537,\"start\":60533},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":61127,\"start\":61123},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":61133,\"start\":61129},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":61982,\"start\":61979},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":65030,\"start\":65026},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":65145,\"start\":65141},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":65197,\"start\":65193},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":65768,\"start\":65764},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":71475,\"start\":71471},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":72358,\"start\":72354},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":72544,\"start\":72540},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":73097,\"start\":73093},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":73103,\"start\":73099},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":75995,\"start\":75991}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":75817,\"start\":74052},{\"attributes\":{\"id\":\"fig_1\"},\"end\":75889,\"start\":75818},{\"attributes\":{\"id\":\"fig_2\"},\"end\":75996,\"start\":75890},{\"attributes\":{\"id\":\"fig_3\"},\"end\":76236,\"start\":75997},{\"attributes\":{\"id\":\"fig_4\"},\"end\":76339,\"start\":76237},{\"attributes\":{\"id\":\"fig_5\"},\"end\":76579,\"start\":76340},{\"attributes\":{\"id\":\"fig_7\"},\"end\":76790,\"start\":76580},{\"attributes\":{\"id\":\"fig_8\"},\"end\":77091,\"start\":76791},{\"attributes\":{\"id\":\"fig_9\"},\"end\":77206,\"start\":77092},{\"attributes\":{\"id\":\"fig_11\"},\"end\":77326,\"start\":77207},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":78216,\"start\":77327},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":78403,\"start\":78217},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":81226,\"start\":78404},{\"attributes\":{\"id\":\"fig_0\"},\"end\":75817,\"start\":74052},{\"attributes\":{\"id\":\"fig_1\"},\"end\":75889,\"start\":75818},{\"attributes\":{\"id\":\"fig_2\"},\"end\":75996,\"start\":75890},{\"attributes\":{\"id\":\"fig_3\"},\"end\":76236,\"start\":75997},{\"attributes\":{\"id\":\"fig_4\"},\"end\":76339,\"start\":76237},{\"attributes\":{\"id\":\"fig_5\"},\"end\":76579,\"start\":76340},{\"attributes\":{\"id\":\"fig_7\"},\"end\":76790,\"start\":76580},{\"attributes\":{\"id\":\"fig_8\"},\"end\":77091,\"start\":76791},{\"attributes\":{\"id\":\"fig_9\"},\"end\":77206,\"start\":77092},{\"attributes\":{\"id\":\"fig_11\"},\"end\":77326,\"start\":77207},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":78216,\"start\":77327},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":78403,\"start\":78217},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":81226,\"start\":78404}]", "paragraph": "[{\"end\":4048,\"start\":2436},{\"end\":4159,\"start\":4050},{\"end\":5161,\"start\":4179},{\"end\":5849,\"start\":5163},{\"end\":6691,\"start\":5851},{\"end\":7635,\"start\":6693},{\"end\":8514,\"start\":7637},{\"end\":11296,\"start\":8516},{\"end\":12388,\"start\":11318},{\"end\":13795,\"start\":12390},{\"end\":15583,\"start\":13797},{\"end\":16801,\"start\":15585},{\"end\":17837,\"start\":16803},{\"end\":18563,\"start\":17859},{\"end\":19922,\"start\":18565},{\"end\":20505,\"start\":19924},{\"end\":21103,\"start\":20515},{\"end\":22186,\"start\":21105},{\"end\":22738,\"start\":22210},{\"end\":23074,\"start\":22740},{\"end\":23413,\"start\":23099},{\"end\":24925,\"start\":23415},{\"end\":25086,\"start\":24927},{\"end\":25575,\"start\":25233},{\"end\":26486,\"start\":25627},{\"end\":26697,\"start\":26499},{\"end\":26739,\"start\":26719},{\"end\":27481,\"start\":26741},{\"end\":28256,\"start\":27499},{\"end\":29882,\"start\":28304},{\"end\":30091,\"start\":29901},{\"end\":31134,\"start\":30110},{\"end\":31887,\"start\":31162},{\"end\":31992,\"start\":31906},{\"end\":32729,\"start\":32131},{\"end\":32849,\"start\":32749},{\"end\":33205,\"start\":32983},{\"end\":33339,\"start\":33243},{\"end\":33819,\"start\":33378},{\"end\":34108,\"start\":33852},{\"end\":34877,\"start\":34241},{\"end\":35958,\"start\":34879},{\"end\":36635,\"start\":35990},{\"end\":37609,\"start\":36673},{\"end\":38082,\"start\":37742},{\"end\":38616,\"start\":38128},{\"end\":39554,\"start\":38667},{\"end\":42262,\"start\":39657},{\"end\":42409,\"start\":42264},{\"end\":42571,\"start\":42447},{\"end\":42954,\"start\":42621},{\"end\":43128,\"start\":43021},{\"end\":43729,\"start\":43178},{\"end\":43852,\"start\":43751},{\"end\":44109,\"start\":43866},{\"end\":44975,\"start\":44176},{\"end\":45671,\"start\":45023},{\"end\":46874,\"start\":45740},{\"end\":48462,\"start\":46876},{\"end\":49253,\"start\":48499},{\"end\":50345,\"start\":49274},{\"end\":50865,\"start\":50369},{\"end\":51001,\"start\":50867},{\"end\":51194,\"start\":51046},{\"end\":51312,\"start\":51239},{\"end\":51433,\"start\":51336},{\"end\":51944,\"start\":51474},{\"end\":52436,\"start\":51946},{\"end\":52917,\"start\":52438},{\"end\":53324,\"start\":52973},{\"end\":54190,\"start\":53326},{\"end\":55575,\"start\":54192},{\"end\":58587,\"start\":55577},{\"end\":59088,\"start\":58691},{\"end\":61678,\"start\":59121},{\"end\":62005,\"start\":61680},{\"end\":65862,\"start\":62007},{\"end\":66098,\"start\":65864},{\"end\":68607,\"start\":66100},{\"end\":70931,\"start\":68609},{\"end\":71909,\"start\":70933},{\"end\":73107,\"start\":71969},{\"end\":74051,\"start\":73129},{\"end\":4048,\"start\":2436},{\"end\":4159,\"start\":4050},{\"end\":5161,\"start\":4179},{\"end\":5849,\"start\":5163},{\"end\":6691,\"start\":5851},{\"end\":7635,\"start\":6693},{\"end\":8514,\"start\":7637},{\"end\":11296,\"start\":8516},{\"end\":12388,\"start\":11318},{\"end\":13795,\"start\":12390},{\"end\":15583,\"start\":13797},{\"end\":16801,\"start\":15585},{\"end\":17837,\"start\":16803},{\"end\":18563,\"start\":17859},{\"end\":19922,\"start\":18565},{\"end\":20505,\"start\":19924},{\"end\":21103,\"start\":20515},{\"end\":22186,\"start\":21105},{\"end\":22738,\"start\":22210},{\"end\":23074,\"start\":22740},{\"end\":23413,\"start\":23099},{\"end\":24925,\"start\":23415},{\"end\":25086,\"start\":24927},{\"end\":25575,\"start\":25233},{\"end\":26486,\"start\":25627},{\"end\":26697,\"start\":26499},{\"end\":26739,\"start\":26719},{\"end\":27481,\"start\":26741},{\"end\":28256,\"start\":27499},{\"end\":29882,\"start\":28304},{\"end\":30091,\"start\":29901},{\"end\":31134,\"start\":30110},{\"end\":31887,\"start\":31162},{\"end\":31992,\"start\":31906},{\"end\":32729,\"start\":32131},{\"end\":32849,\"start\":32749},{\"end\":33205,\"start\":32983},{\"end\":33339,\"start\":33243},{\"end\":33819,\"start\":33378},{\"end\":34108,\"start\":33852},{\"end\":34877,\"start\":34241},{\"end\":35958,\"start\":34879},{\"end\":36635,\"start\":35990},{\"end\":37609,\"start\":36673},{\"end\":38082,\"start\":37742},{\"end\":38616,\"start\":38128},{\"end\":39554,\"start\":38667},{\"end\":42262,\"start\":39657},{\"end\":42409,\"start\":42264},{\"end\":42571,\"start\":42447},{\"end\":42954,\"start\":42621},{\"end\":43128,\"start\":43021},{\"end\":43729,\"start\":43178},{\"end\":43852,\"start\":43751},{\"end\":44109,\"start\":43866},{\"end\":44975,\"start\":44176},{\"end\":45671,\"start\":45023},{\"end\":46874,\"start\":45740},{\"end\":48462,\"start\":46876},{\"end\":49253,\"start\":48499},{\"end\":50345,\"start\":49274},{\"end\":50865,\"start\":50369},{\"end\":51001,\"start\":50867},{\"end\":51194,\"start\":51046},{\"end\":51312,\"start\":51239},{\"end\":51433,\"start\":51336},{\"end\":51944,\"start\":51474},{\"end\":52436,\"start\":51946},{\"end\":52917,\"start\":52438},{\"end\":53324,\"start\":52973},{\"end\":54190,\"start\":53326},{\"end\":55575,\"start\":54192},{\"end\":58587,\"start\":55577},{\"end\":59088,\"start\":58691},{\"end\":61678,\"start\":59121},{\"end\":62005,\"start\":61680},{\"end\":65862,\"start\":62007},{\"end\":66098,\"start\":65864},{\"end\":68607,\"start\":66100},{\"end\":70931,\"start\":68609},{\"end\":71909,\"start\":70933},{\"end\":73107,\"start\":71969},{\"end\":74051,\"start\":73129}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":20514,\"start\":20506},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22209,\"start\":22187},{\"attributes\":{\"id\":\"formula_2\"},\"end\":25232,\"start\":25087},{\"attributes\":{\"id\":\"formula_3\"},\"end\":25626,\"start\":25576},{\"attributes\":{\"id\":\"formula_4\"},\"end\":26498,\"start\":26487},{\"attributes\":{\"id\":\"formula_5\"},\"end\":26718,\"start\":26698},{\"attributes\":{\"id\":\"formula_6\"},\"end\":27498,\"start\":27482},{\"attributes\":{\"id\":\"formula_7\"},\"end\":29900,\"start\":29883},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30109,\"start\":30092},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32130,\"start\":31993},{\"attributes\":{\"id\":\"formula_10\"},\"end\":32982,\"start\":32850},{\"attributes\":{\"id\":\"formula_11\"},\"end\":33377,\"start\":33340},{\"attributes\":{\"id\":\"formula_12\"},\"end\":34240,\"start\":34109},{\"attributes\":{\"id\":\"formula_13\"},\"end\":36672,\"start\":36636},{\"attributes\":{\"id\":\"formula_14\"},\"end\":38127,\"start\":38083},{\"attributes\":{\"id\":\"formula_15\"},\"end\":38666,\"start\":38617},{\"attributes\":{\"id\":\"formula_16\"},\"end\":42446,\"start\":42410},{\"attributes\":{\"id\":\"formula_17\"},\"end\":42620,\"start\":42572},{\"attributes\":{\"id\":\"formula_18\"},\"end\":43020,\"start\":42955},{\"attributes\":{\"id\":\"formula_19\"},\"end\":43177,\"start\":43129},{\"attributes\":{\"id\":\"formula_20\"},\"end\":43750,\"start\":43730},{\"attributes\":{\"id\":\"formula_21\"},\"end\":43865,\"start\":43853},{\"attributes\":{\"id\":\"formula_22\"},\"end\":45022,\"start\":44976},{\"attributes\":{\"id\":\"formula_23\"},\"end\":48498,\"start\":48463},{\"attributes\":{\"id\":\"formula_24\"},\"end\":49273,\"start\":49254},{\"attributes\":{\"id\":\"formula_25\"},\"end\":50368,\"start\":50346},{\"attributes\":{\"id\":\"formula_26\"},\"end\":51045,\"start\":51002},{\"attributes\":{\"id\":\"formula_27\"},\"end\":51238,\"start\":51195},{\"attributes\":{\"id\":\"formula_28\"},\"end\":51335,\"start\":51313},{\"attributes\":{\"id\":\"formula_29\"},\"end\":51473,\"start\":51434},{\"attributes\":{\"id\":\"formula_30\"},\"end\":58690,\"start\":58588},{\"attributes\":{\"id\":\"formula_0\"},\"end\":20514,\"start\":20506},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22209,\"start\":22187},{\"attributes\":{\"id\":\"formula_2\"},\"end\":25232,\"start\":25087},{\"attributes\":{\"id\":\"formula_3\"},\"end\":25626,\"start\":25576},{\"attributes\":{\"id\":\"formula_4\"},\"end\":26498,\"start\":26487},{\"attributes\":{\"id\":\"formula_5\"},\"end\":26718,\"start\":26698},{\"attributes\":{\"id\":\"formula_6\"},\"end\":27498,\"start\":27482},{\"attributes\":{\"id\":\"formula_7\"},\"end\":29900,\"start\":29883},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30109,\"start\":30092},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32130,\"start\":31993},{\"attributes\":{\"id\":\"formula_10\"},\"end\":32982,\"start\":32850},{\"attributes\":{\"id\":\"formula_11\"},\"end\":33377,\"start\":33340},{\"attributes\":{\"id\":\"formula_12\"},\"end\":34240,\"start\":34109},{\"attributes\":{\"id\":\"formula_13\"},\"end\":36672,\"start\":36636},{\"attributes\":{\"id\":\"formula_14\"},\"end\":38127,\"start\":38083},{\"attributes\":{\"id\":\"formula_15\"},\"end\":38666,\"start\":38617},{\"attributes\":{\"id\":\"formula_16\"},\"end\":42446,\"start\":42410},{\"attributes\":{\"id\":\"formula_17\"},\"end\":42620,\"start\":42572},{\"attributes\":{\"id\":\"formula_18\"},\"end\":43020,\"start\":42955},{\"attributes\":{\"id\":\"formula_19\"},\"end\":43177,\"start\":43129},{\"attributes\":{\"id\":\"formula_20\"},\"end\":43750,\"start\":43730},{\"attributes\":{\"id\":\"formula_21\"},\"end\":43865,\"start\":43853},{\"attributes\":{\"id\":\"formula_22\"},\"end\":45022,\"start\":44976},{\"attributes\":{\"id\":\"formula_23\"},\"end\":48498,\"start\":48463},{\"attributes\":{\"id\":\"formula_24\"},\"end\":49273,\"start\":49254},{\"attributes\":{\"id\":\"formula_25\"},\"end\":50368,\"start\":50346},{\"attributes\":{\"id\":\"formula_26\"},\"end\":51045,\"start\":51002},{\"attributes\":{\"id\":\"formula_27\"},\"end\":51238,\"start\":51195},{\"attributes\":{\"id\":\"formula_28\"},\"end\":51335,\"start\":51313},{\"attributes\":{\"id\":\"formula_29\"},\"end\":51473,\"start\":51434},{\"attributes\":{\"id\":\"formula_30\"},\"end\":58690,\"start\":58588}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":55574,\"start\":55567},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":59879,\"start\":59871},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":60356,\"start\":60349},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":63564,\"start\":63555},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":55574,\"start\":55567},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":59879,\"start\":59871},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":60356,\"start\":60349},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":63564,\"start\":63555}]", "section_header": "[{\"end\":4177,\"start\":4162},{\"end\":11316,\"start\":11299},{\"end\":17857,\"start\":17840},{\"end\":23097,\"start\":23077},{\"end\":28302,\"start\":28259},{\"end\":31160,\"start\":31137},{\"end\":31904,\"start\":31890},{\"end\":32747,\"start\":32732},{\"end\":33241,\"start\":33208},{\"end\":33850,\"start\":33822},{\"end\":35988,\"start\":35961},{\"end\":37700,\"start\":37612},{\"end\":37740,\"start\":37703},{\"end\":39655,\"start\":39557},{\"end\":44174,\"start\":44112},{\"end\":45738,\"start\":45674},{\"end\":52947,\"start\":52920},{\"end\":52971,\"start\":52950},{\"end\":59119,\"start\":59091},{\"end\":71967,\"start\":71912},{\"end\":73127,\"start\":73110},{\"end\":75827,\"start\":75819},{\"end\":75899,\"start\":75891},{\"end\":76246,\"start\":76238},{\"end\":76589,\"start\":76581},{\"end\":76808,\"start\":76792},{\"end\":77101,\"start\":77093},{\"end\":77216,\"start\":77208},{\"end\":78227,\"start\":78218},{\"end\":4177,\"start\":4162},{\"end\":11316,\"start\":11299},{\"end\":17857,\"start\":17840},{\"end\":23097,\"start\":23077},{\"end\":28302,\"start\":28259},{\"end\":31160,\"start\":31137},{\"end\":31904,\"start\":31890},{\"end\":32747,\"start\":32732},{\"end\":33241,\"start\":33208},{\"end\":33850,\"start\":33822},{\"end\":35988,\"start\":35961},{\"end\":37700,\"start\":37612},{\"end\":37740,\"start\":37703},{\"end\":39655,\"start\":39557},{\"end\":44174,\"start\":44112},{\"end\":45738,\"start\":45674},{\"end\":52947,\"start\":52920},{\"end\":52971,\"start\":52950},{\"end\":59119,\"start\":59091},{\"end\":71967,\"start\":71912},{\"end\":73127,\"start\":73110},{\"end\":75827,\"start\":75819},{\"end\":75899,\"start\":75891},{\"end\":76246,\"start\":76238},{\"end\":76589,\"start\":76581},{\"end\":76808,\"start\":76792},{\"end\":77101,\"start\":77093},{\"end\":77216,\"start\":77208},{\"end\":78227,\"start\":78218}]", "table": "[{\"end\":78216,\"start\":77355},{\"end\":78403,\"start\":78246},{\"end\":78216,\"start\":77355},{\"end\":78403,\"start\":78246}]", "figure_caption": "[{\"end\":75817,\"start\":74054},{\"end\":75889,\"start\":75829},{\"end\":75996,\"start\":75901},{\"end\":76236,\"start\":75999},{\"end\":76339,\"start\":76248},{\"end\":76579,\"start\":76342},{\"end\":76790,\"start\":76591},{\"end\":77091,\"start\":76811},{\"end\":77206,\"start\":77103},{\"end\":77326,\"start\":77218},{\"end\":77355,\"start\":77329},{\"end\":78246,\"start\":78229},{\"end\":81226,\"start\":78406},{\"end\":75817,\"start\":74054},{\"end\":75889,\"start\":75829},{\"end\":75996,\"start\":75901},{\"end\":76236,\"start\":75999},{\"end\":76339,\"start\":76248},{\"end\":76579,\"start\":76342},{\"end\":76790,\"start\":76591},{\"end\":77091,\"start\":76811},{\"end\":77206,\"start\":77103},{\"end\":77326,\"start\":77218},{\"end\":77355,\"start\":77329},{\"end\":78246,\"start\":78229},{\"end\":81226,\"start\":78406}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17936,\"start\":17930},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18588,\"start\":18582},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19067,\"start\":19061},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19255,\"start\":19249},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21764,\"start\":21758},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24519,\"start\":24513},{\"end\":40991,\"start\":40985},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":48461,\"start\":48455},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":51148,\"start\":51138},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":53106,\"start\":53100},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":62206,\"start\":62197},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":63575,\"start\":63566},{\"end\":66355,\"start\":66349},{\"end\":66401,\"start\":66395},{\"end\":66593,\"start\":66584},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":67536,\"start\":67527},{\"end\":67641,\"start\":67632},{\"end\":67655,\"start\":67646},{\"end\":67952,\"start\":67943},{\"end\":67966,\"start\":67957},{\"end\":68606,\"start\":68597},{\"end\":69246,\"start\":69240},{\"end\":70600,\"start\":70591},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":70942,\"start\":70936},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":71563,\"start\":71554},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":71760,\"start\":71751},{\"end\":71812,\"start\":71803},{\"end\":71826,\"start\":71817},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":72755,\"start\":72749},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17936,\"start\":17930},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18588,\"start\":18582},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19067,\"start\":19061},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19255,\"start\":19249},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21764,\"start\":21758},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24519,\"start\":24513},{\"end\":40991,\"start\":40985},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":48461,\"start\":48455},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":51148,\"start\":51138},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":53106,\"start\":53100},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":62206,\"start\":62197},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":63575,\"start\":63566},{\"end\":66355,\"start\":66349},{\"end\":66401,\"start\":66395},{\"end\":66593,\"start\":66584},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":67536,\"start\":67527},{\"end\":67641,\"start\":67632},{\"end\":67655,\"start\":67646},{\"end\":67952,\"start\":67943},{\"end\":67966,\"start\":67957},{\"end\":68606,\"start\":68597},{\"end\":69246,\"start\":69240},{\"end\":70600,\"start\":70591},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":70942,\"start\":70936},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":71563,\"start\":71554},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":71760,\"start\":71751},{\"end\":71812,\"start\":71803},{\"end\":71826,\"start\":71817},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":72755,\"start\":72749}]", "bib_author_first_name": "[{\"end\":81538,\"start\":81537},{\"end\":81544,\"start\":81543},{\"end\":81558,\"start\":81557},{\"end\":81569,\"start\":81566},{\"end\":81576,\"start\":81575},{\"end\":81578,\"start\":81577},{\"end\":81945,\"start\":81944},{\"end\":81955,\"start\":81954},{\"end\":81968,\"start\":81967},{\"end\":81977,\"start\":81976},{\"end\":82347,\"start\":82346},{\"end\":82360,\"start\":82359},{\"end\":82371,\"start\":82370},{\"end\":82383,\"start\":82382},{\"end\":82673,\"start\":82672},{\"end\":82675,\"start\":82674},{\"end\":82686,\"start\":82685},{\"end\":82699,\"start\":82698},{\"end\":82710,\"start\":82709},{\"end\":82720,\"start\":82719},{\"end\":83260,\"start\":83259},{\"end\":83269,\"start\":83268},{\"end\":83671,\"start\":83670},{\"end\":83679,\"start\":83678},{\"end\":83687,\"start\":83686},{\"end\":83696,\"start\":83695},{\"end\":84090,\"start\":84089},{\"end\":84092,\"start\":84091},{\"end\":84101,\"start\":84100},{\"end\":84107,\"start\":84106},{\"end\":84109,\"start\":84108},{\"end\":84118,\"start\":84117},{\"end\":84128,\"start\":84127},{\"end\":84130,\"start\":84129},{\"end\":84454,\"start\":84453},{\"end\":84456,\"start\":84455},{\"end\":84469,\"start\":84468},{\"end\":84473,\"start\":84470},{\"end\":84484,\"start\":84483},{\"end\":84486,\"start\":84485},{\"end\":84497,\"start\":84496},{\"end\":84508,\"start\":84507},{\"end\":84519,\"start\":84518},{\"end\":84992,\"start\":84991},{\"end\":85002,\"start\":85001},{\"end\":85022,\"start\":85021},{\"end\":85024,\"start\":85023},{\"end\":85368,\"start\":85367},{\"end\":85370,\"start\":85369},{\"end\":85383,\"start\":85382},{\"end\":85385,\"start\":85384},{\"end\":85395,\"start\":85394},{\"end\":85397,\"start\":85396},{\"end\":85407,\"start\":85406},{\"end\":85778,\"start\":85777},{\"end\":85788,\"start\":85787},{\"end\":85790,\"start\":85789},{\"end\":85802,\"start\":85801},{\"end\":85804,\"start\":85803},{\"end\":86118,\"start\":86117},{\"end\":86125,\"start\":86124},{\"end\":86135,\"start\":86134},{\"end\":86443,\"start\":86442},{\"end\":86450,\"start\":86449},{\"end\":86458,\"start\":86457},{\"end\":86464,\"start\":86463},{\"end\":86804,\"start\":86803},{\"end\":86811,\"start\":86810},{\"end\":86821,\"start\":86820},{\"end\":87228,\"start\":87227},{\"end\":87235,\"start\":87234},{\"end\":87723,\"start\":87722},{\"end\":87730,\"start\":87729},{\"end\":87738,\"start\":87737},{\"end\":87745,\"start\":87744},{\"end\":87752,\"start\":87751},{\"end\":88117,\"start\":88116},{\"end\":88119,\"start\":88118},{\"end\":88131,\"start\":88130},{\"end\":88133,\"start\":88132},{\"end\":88490,\"start\":88489},{\"end\":88492,\"start\":88491},{\"end\":88500,\"start\":88499},{\"end\":88502,\"start\":88501},{\"end\":88511,\"start\":88510},{\"end\":88513,\"start\":88512},{\"end\":88522,\"start\":88521},{\"end\":88809,\"start\":88808},{\"end\":88816,\"start\":88815},{\"end\":88823,\"start\":88822},{\"end\":89083,\"start\":89082},{\"end\":89232,\"start\":89231},{\"end\":89238,\"start\":89233},{\"end\":89249,\"start\":89248},{\"end\":89431,\"start\":89430},{\"end\":89572,\"start\":89571},{\"end\":89574,\"start\":89573},{\"end\":89583,\"start\":89582},{\"end\":89585,\"start\":89584},{\"end\":89594,\"start\":89593},{\"end\":89602,\"start\":89601},{\"end\":89612,\"start\":89611},{\"end\":89622,\"start\":89619},{\"end\":89775,\"start\":89774},{\"end\":89777,\"start\":89776},{\"end\":90071,\"start\":90070},{\"end\":90073,\"start\":90072},{\"end\":90083,\"start\":90082},{\"end\":90085,\"start\":90084},{\"end\":90246,\"start\":90245},{\"end\":90248,\"start\":90247},{\"end\":90259,\"start\":90258},{\"end\":90267,\"start\":90266},{\"end\":90572,\"start\":90571},{\"end\":90574,\"start\":90573},{\"end\":90853,\"start\":90852},{\"end\":90862,\"start\":90861},{\"end\":90864,\"start\":90863},{\"end\":90880,\"start\":90879},{\"end\":91072,\"start\":91071},{\"end\":91082,\"start\":91081},{\"end\":91090,\"start\":91089},{\"end\":91104,\"start\":91103},{\"end\":91305,\"start\":91304},{\"end\":91624,\"start\":91623},{\"end\":91838,\"start\":91837},{\"end\":91840,\"start\":91839},{\"end\":91851,\"start\":91850},{\"end\":91860,\"start\":91859},{\"end\":92287,\"start\":92286},{\"end\":92299,\"start\":92298},{\"end\":92311,\"start\":92310},{\"end\":92313,\"start\":92312},{\"end\":92321,\"start\":92320},{\"end\":92331,\"start\":92330},{\"end\":92333,\"start\":92332},{\"end\":92631,\"start\":92630},{\"end\":92633,\"start\":92632},{\"end\":93019,\"start\":93018},{\"end\":93030,\"start\":93029},{\"end\":93040,\"start\":93039},{\"end\":93051,\"start\":93050},{\"end\":93360,\"start\":93359},{\"end\":93673,\"start\":93672},{\"end\":93687,\"start\":93686},{\"end\":93697,\"start\":93696},{\"end\":93900,\"start\":93899},{\"end\":93913,\"start\":93912},{\"end\":94415,\"start\":94414},{\"end\":94426,\"start\":94425},{\"end\":94439,\"start\":94438},{\"end\":94878,\"start\":94877},{\"end\":94880,\"start\":94879},{\"end\":94890,\"start\":94889},{\"end\":94892,\"start\":94891},{\"end\":94901,\"start\":94900},{\"end\":94911,\"start\":94910},{\"end\":95491,\"start\":95488},{\"end\":97588,\"start\":97584},{\"end\":81538,\"start\":81537},{\"end\":81544,\"start\":81543},{\"end\":81558,\"start\":81557},{\"end\":81569,\"start\":81566},{\"end\":81576,\"start\":81575},{\"end\":81578,\"start\":81577},{\"end\":81945,\"start\":81944},{\"end\":81955,\"start\":81954},{\"end\":81968,\"start\":81967},{\"end\":81977,\"start\":81976},{\"end\":82347,\"start\":82346},{\"end\":82360,\"start\":82359},{\"end\":82371,\"start\":82370},{\"end\":82383,\"start\":82382},{\"end\":82673,\"start\":82672},{\"end\":82675,\"start\":82674},{\"end\":82686,\"start\":82685},{\"end\":82699,\"start\":82698},{\"end\":82710,\"start\":82709},{\"end\":82720,\"start\":82719},{\"end\":83260,\"start\":83259},{\"end\":83269,\"start\":83268},{\"end\":83671,\"start\":83670},{\"end\":83679,\"start\":83678},{\"end\":83687,\"start\":83686},{\"end\":83696,\"start\":83695},{\"end\":84090,\"start\":84089},{\"end\":84092,\"start\":84091},{\"end\":84101,\"start\":84100},{\"end\":84107,\"start\":84106},{\"end\":84109,\"start\":84108},{\"end\":84118,\"start\":84117},{\"end\":84128,\"start\":84127},{\"end\":84130,\"start\":84129},{\"end\":84454,\"start\":84453},{\"end\":84456,\"start\":84455},{\"end\":84469,\"start\":84468},{\"end\":84473,\"start\":84470},{\"end\":84484,\"start\":84483},{\"end\":84486,\"start\":84485},{\"end\":84497,\"start\":84496},{\"end\":84508,\"start\":84507},{\"end\":84519,\"start\":84518},{\"end\":84992,\"start\":84991},{\"end\":85002,\"start\":85001},{\"end\":85022,\"start\":85021},{\"end\":85024,\"start\":85023},{\"end\":85368,\"start\":85367},{\"end\":85370,\"start\":85369},{\"end\":85383,\"start\":85382},{\"end\":85385,\"start\":85384},{\"end\":85395,\"start\":85394},{\"end\":85397,\"start\":85396},{\"end\":85407,\"start\":85406},{\"end\":85778,\"start\":85777},{\"end\":85788,\"start\":85787},{\"end\":85790,\"start\":85789},{\"end\":85802,\"start\":85801},{\"end\":85804,\"start\":85803},{\"end\":86118,\"start\":86117},{\"end\":86125,\"start\":86124},{\"end\":86135,\"start\":86134},{\"end\":86443,\"start\":86442},{\"end\":86450,\"start\":86449},{\"end\":86458,\"start\":86457},{\"end\":86464,\"start\":86463},{\"end\":86804,\"start\":86803},{\"end\":86811,\"start\":86810},{\"end\":86821,\"start\":86820},{\"end\":87228,\"start\":87227},{\"end\":87235,\"start\":87234},{\"end\":87723,\"start\":87722},{\"end\":87730,\"start\":87729},{\"end\":87738,\"start\":87737},{\"end\":87745,\"start\":87744},{\"end\":87752,\"start\":87751},{\"end\":88117,\"start\":88116},{\"end\":88119,\"start\":88118},{\"end\":88131,\"start\":88130},{\"end\":88133,\"start\":88132},{\"end\":88490,\"start\":88489},{\"end\":88492,\"start\":88491},{\"end\":88500,\"start\":88499},{\"end\":88502,\"start\":88501},{\"end\":88511,\"start\":88510},{\"end\":88513,\"start\":88512},{\"end\":88522,\"start\":88521},{\"end\":88809,\"start\":88808},{\"end\":88816,\"start\":88815},{\"end\":88823,\"start\":88822},{\"end\":89083,\"start\":89082},{\"end\":89232,\"start\":89231},{\"end\":89238,\"start\":89233},{\"end\":89249,\"start\":89248},{\"end\":89431,\"start\":89430},{\"end\":89572,\"start\":89571},{\"end\":89574,\"start\":89573},{\"end\":89583,\"start\":89582},{\"end\":89585,\"start\":89584},{\"end\":89594,\"start\":89593},{\"end\":89602,\"start\":89601},{\"end\":89612,\"start\":89611},{\"end\":89622,\"start\":89619},{\"end\":89775,\"start\":89774},{\"end\":89777,\"start\":89776},{\"end\":90071,\"start\":90070},{\"end\":90073,\"start\":90072},{\"end\":90083,\"start\":90082},{\"end\":90085,\"start\":90084},{\"end\":90246,\"start\":90245},{\"end\":90248,\"start\":90247},{\"end\":90259,\"start\":90258},{\"end\":90267,\"start\":90266},{\"end\":90572,\"start\":90571},{\"end\":90574,\"start\":90573},{\"end\":90853,\"start\":90852},{\"end\":90862,\"start\":90861},{\"end\":90864,\"start\":90863},{\"end\":90880,\"start\":90879},{\"end\":91072,\"start\":91071},{\"end\":91082,\"start\":91081},{\"end\":91090,\"start\":91089},{\"end\":91104,\"start\":91103},{\"end\":91305,\"start\":91304},{\"end\":91624,\"start\":91623},{\"end\":91838,\"start\":91837},{\"end\":91840,\"start\":91839},{\"end\":91851,\"start\":91850},{\"end\":91860,\"start\":91859},{\"end\":92287,\"start\":92286},{\"end\":92299,\"start\":92298},{\"end\":92311,\"start\":92310},{\"end\":92313,\"start\":92312},{\"end\":92321,\"start\":92320},{\"end\":92331,\"start\":92330},{\"end\":92333,\"start\":92332},{\"end\":92631,\"start\":92630},{\"end\":92633,\"start\":92632},{\"end\":93019,\"start\":93018},{\"end\":93030,\"start\":93029},{\"end\":93040,\"start\":93039},{\"end\":93051,\"start\":93050},{\"end\":93360,\"start\":93359},{\"end\":93673,\"start\":93672},{\"end\":93687,\"start\":93686},{\"end\":93697,\"start\":93696},{\"end\":93900,\"start\":93899},{\"end\":93913,\"start\":93912},{\"end\":94415,\"start\":94414},{\"end\":94426,\"start\":94425},{\"end\":94439,\"start\":94438},{\"end\":94878,\"start\":94877},{\"end\":94880,\"start\":94879},{\"end\":94890,\"start\":94889},{\"end\":94892,\"start\":94891},{\"end\":94901,\"start\":94900},{\"end\":94911,\"start\":94910},{\"end\":95491,\"start\":95488},{\"end\":97588,\"start\":97584}]", "bib_author_last_name": "[{\"end\":81541,\"start\":81539},{\"end\":81555,\"start\":81545},{\"end\":81564,\"start\":81559},{\"end\":81573,\"start\":81570},{\"end\":81584,\"start\":81579},{\"end\":81952,\"start\":81946},{\"end\":81965,\"start\":81956},{\"end\":81974,\"start\":81969},{\"end\":81987,\"start\":81978},{\"end\":82357,\"start\":82348},{\"end\":82368,\"start\":82361},{\"end\":82380,\"start\":82372},{\"end\":82390,\"start\":82384},{\"end\":82683,\"start\":82676},{\"end\":82696,\"start\":82687},{\"end\":82707,\"start\":82700},{\"end\":82717,\"start\":82711},{\"end\":82731,\"start\":82721},{\"end\":83266,\"start\":83261},{\"end\":83275,\"start\":83270},{\"end\":83676,\"start\":83672},{\"end\":83684,\"start\":83680},{\"end\":83693,\"start\":83688},{\"end\":83702,\"start\":83697},{\"end\":84098,\"start\":84093},{\"end\":84104,\"start\":84102},{\"end\":84115,\"start\":84110},{\"end\":84125,\"start\":84119},{\"end\":84134,\"start\":84131},{\"end\":84466,\"start\":84457},{\"end\":84481,\"start\":84474},{\"end\":84494,\"start\":84487},{\"end\":84505,\"start\":84498},{\"end\":84516,\"start\":84509},{\"end\":84529,\"start\":84520},{\"end\":84999,\"start\":84993},{\"end\":85019,\"start\":85003},{\"end\":85030,\"start\":85025},{\"end\":85380,\"start\":85371},{\"end\":85392,\"start\":85386},{\"end\":85404,\"start\":85398},{\"end\":85417,\"start\":85408},{\"end\":85785,\"start\":85779},{\"end\":85799,\"start\":85791},{\"end\":85810,\"start\":85805},{\"end\":86122,\"start\":86119},{\"end\":86132,\"start\":86126},{\"end\":86143,\"start\":86136},{\"end\":86447,\"start\":86444},{\"end\":86455,\"start\":86451},{\"end\":86461,\"start\":86459},{\"end\":86469,\"start\":86465},{\"end\":86808,\"start\":86805},{\"end\":86818,\"start\":86812},{\"end\":86829,\"start\":86822},{\"end\":87232,\"start\":87229},{\"end\":87240,\"start\":87236},{\"end\":87727,\"start\":87724},{\"end\":87735,\"start\":87731},{\"end\":87742,\"start\":87739},{\"end\":87749,\"start\":87746},{\"end\":87757,\"start\":87753},{\"end\":88128,\"start\":88120},{\"end\":88139,\"start\":88134},{\"end\":88497,\"start\":88493},{\"end\":88508,\"start\":88503},{\"end\":88519,\"start\":88514},{\"end\":88529,\"start\":88523},{\"end\":88813,\"start\":88810},{\"end\":88820,\"start\":88817},{\"end\":88828,\"start\":88824},{\"end\":89091,\"start\":89084},{\"end\":89246,\"start\":89239},{\"end\":89255,\"start\":89250},{\"end\":89436,\"start\":89432},{\"end\":89580,\"start\":89575},{\"end\":89591,\"start\":89586},{\"end\":89599,\"start\":89595},{\"end\":89609,\"start\":89603},{\"end\":89617,\"start\":89613},{\"end\":89783,\"start\":89778},{\"end\":89788,\"start\":89785},{\"end\":90080,\"start\":90074},{\"end\":90091,\"start\":90086},{\"end\":90256,\"start\":90249},{\"end\":90264,\"start\":90260},{\"end\":90274,\"start\":90268},{\"end\":90581,\"start\":90575},{\"end\":90859,\"start\":90854},{\"end\":90877,\"start\":90865},{\"end\":90885,\"start\":90881},{\"end\":91079,\"start\":91073},{\"end\":91087,\"start\":91083},{\"end\":91101,\"start\":91091},{\"end\":91111,\"start\":91105},{\"end\":91312,\"start\":91306},{\"end\":91629,\"start\":91625},{\"end\":91848,\"start\":91841},{\"end\":91857,\"start\":91852},{\"end\":91867,\"start\":91861},{\"end\":92296,\"start\":92288},{\"end\":92308,\"start\":92300},{\"end\":92318,\"start\":92314},{\"end\":92328,\"start\":92322},{\"end\":92339,\"start\":92334},{\"end\":92638,\"start\":92634},{\"end\":93027,\"start\":93020},{\"end\":93037,\"start\":93031},{\"end\":93048,\"start\":93041},{\"end\":93058,\"start\":93052},{\"end\":93367,\"start\":93361},{\"end\":93684,\"start\":93674},{\"end\":93694,\"start\":93688},{\"end\":93707,\"start\":93698},{\"end\":93910,\"start\":93901},{\"end\":93920,\"start\":93914},{\"end\":94423,\"start\":94416},{\"end\":94436,\"start\":94427},{\"end\":94451,\"start\":94440},{\"end\":94887,\"start\":94881},{\"end\":94898,\"start\":94893},{\"end\":94908,\"start\":94902},{\"end\":94914,\"start\":94912},{\"end\":95499,\"start\":95492},{\"end\":97593,\"start\":97589},{\"end\":81541,\"start\":81539},{\"end\":81555,\"start\":81545},{\"end\":81564,\"start\":81559},{\"end\":81573,\"start\":81570},{\"end\":81584,\"start\":81579},{\"end\":81952,\"start\":81946},{\"end\":81965,\"start\":81956},{\"end\":81974,\"start\":81969},{\"end\":81987,\"start\":81978},{\"end\":82357,\"start\":82348},{\"end\":82368,\"start\":82361},{\"end\":82380,\"start\":82372},{\"end\":82390,\"start\":82384},{\"end\":82683,\"start\":82676},{\"end\":82696,\"start\":82687},{\"end\":82707,\"start\":82700},{\"end\":82717,\"start\":82711},{\"end\":82731,\"start\":82721},{\"end\":83266,\"start\":83261},{\"end\":83275,\"start\":83270},{\"end\":83676,\"start\":83672},{\"end\":83684,\"start\":83680},{\"end\":83693,\"start\":83688},{\"end\":83702,\"start\":83697},{\"end\":84098,\"start\":84093},{\"end\":84104,\"start\":84102},{\"end\":84115,\"start\":84110},{\"end\":84125,\"start\":84119},{\"end\":84134,\"start\":84131},{\"end\":84466,\"start\":84457},{\"end\":84481,\"start\":84474},{\"end\":84494,\"start\":84487},{\"end\":84505,\"start\":84498},{\"end\":84516,\"start\":84509},{\"end\":84529,\"start\":84520},{\"end\":84999,\"start\":84993},{\"end\":85019,\"start\":85003},{\"end\":85030,\"start\":85025},{\"end\":85380,\"start\":85371},{\"end\":85392,\"start\":85386},{\"end\":85404,\"start\":85398},{\"end\":85417,\"start\":85408},{\"end\":85785,\"start\":85779},{\"end\":85799,\"start\":85791},{\"end\":85810,\"start\":85805},{\"end\":86122,\"start\":86119},{\"end\":86132,\"start\":86126},{\"end\":86143,\"start\":86136},{\"end\":86447,\"start\":86444},{\"end\":86455,\"start\":86451},{\"end\":86461,\"start\":86459},{\"end\":86469,\"start\":86465},{\"end\":86808,\"start\":86805},{\"end\":86818,\"start\":86812},{\"end\":86829,\"start\":86822},{\"end\":87232,\"start\":87229},{\"end\":87240,\"start\":87236},{\"end\":87727,\"start\":87724},{\"end\":87735,\"start\":87731},{\"end\":87742,\"start\":87739},{\"end\":87749,\"start\":87746},{\"end\":87757,\"start\":87753},{\"end\":88128,\"start\":88120},{\"end\":88139,\"start\":88134},{\"end\":88497,\"start\":88493},{\"end\":88508,\"start\":88503},{\"end\":88519,\"start\":88514},{\"end\":88529,\"start\":88523},{\"end\":88813,\"start\":88810},{\"end\":88820,\"start\":88817},{\"end\":88828,\"start\":88824},{\"end\":89091,\"start\":89084},{\"end\":89246,\"start\":89239},{\"end\":89255,\"start\":89250},{\"end\":89436,\"start\":89432},{\"end\":89580,\"start\":89575},{\"end\":89591,\"start\":89586},{\"end\":89599,\"start\":89595},{\"end\":89609,\"start\":89603},{\"end\":89617,\"start\":89613},{\"end\":89783,\"start\":89778},{\"end\":89788,\"start\":89785},{\"end\":90080,\"start\":90074},{\"end\":90091,\"start\":90086},{\"end\":90256,\"start\":90249},{\"end\":90264,\"start\":90260},{\"end\":90274,\"start\":90268},{\"end\":90581,\"start\":90575},{\"end\":90859,\"start\":90854},{\"end\":90877,\"start\":90865},{\"end\":90885,\"start\":90881},{\"end\":91079,\"start\":91073},{\"end\":91087,\"start\":91083},{\"end\":91101,\"start\":91091},{\"end\":91111,\"start\":91105},{\"end\":91312,\"start\":91306},{\"end\":91629,\"start\":91625},{\"end\":91848,\"start\":91841},{\"end\":91857,\"start\":91852},{\"end\":91867,\"start\":91861},{\"end\":92296,\"start\":92288},{\"end\":92308,\"start\":92300},{\"end\":92318,\"start\":92314},{\"end\":92328,\"start\":92322},{\"end\":92339,\"start\":92334},{\"end\":92638,\"start\":92634},{\"end\":93027,\"start\":93020},{\"end\":93037,\"start\":93031},{\"end\":93048,\"start\":93041},{\"end\":93058,\"start\":93052},{\"end\":93367,\"start\":93361},{\"end\":93684,\"start\":93674},{\"end\":93694,\"start\":93688},{\"end\":93707,\"start\":93698},{\"end\":93910,\"start\":93901},{\"end\":93920,\"start\":93914},{\"end\":94423,\"start\":94416},{\"end\":94436,\"start\":94427},{\"end\":94451,\"start\":94440},{\"end\":94887,\"start\":94881},{\"end\":94898,\"start\":94893},{\"end\":94908,\"start\":94902},{\"end\":94914,\"start\":94912},{\"end\":95499,\"start\":95492},{\"end\":97593,\"start\":97589}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":81431,\"start\":81228},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":220314942},\"end\":81857,\"start\":81433},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":221590125},\"end\":82268,\"start\":81859},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":186206717},\"end\":82586,\"start\":82270},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4610262},\"end\":83138,\"start\":82588},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":220265661},\"end\":83527,\"start\":83140},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":115821513},\"end\":84003,\"start\":83529},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":220793643},\"end\":84396,\"start\":84005},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3443330},\"end\":84910,\"start\":84398},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":34404048},\"end\":85310,\"start\":84912},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":117530196},\"end\":85695,\"start\":85312},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":119303417},\"end\":86054,\"start\":85697},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":44093491},\"end\":86359,\"start\":86056},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":226439636},\"end\":86727,\"start\":86361},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":214761889},\"end\":87133,\"start\":86729},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3346034},\"end\":87624,\"start\":87135},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":30383104},\"end\":88036,\"start\":87626},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":18313385},\"end\":88387,\"start\":88038},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":220496667},\"end\":88806,\"start\":88389},{\"attributes\":{\"doi\":\"arXiv:2009.07888\",\"id\":\"b19\"},\"end\":89050,\"start\":88808},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":123329493},\"end\":89217,\"start\":89052},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":208910339},\"end\":89371,\"start\":89219},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":205242740},\"end\":89567,\"start\":89373},{\"attributes\":{\"id\":\"b23\"},\"end\":89684,\"start\":89569},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53018231},\"end\":90027,\"start\":89686},{\"attributes\":{\"id\":\"b25\"},\"end\":90191,\"start\":90029},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6208256},\"end\":90569,\"start\":90193},{\"attributes\":{\"doi\":\"arXiv:2102.07572\",\"id\":\"b27\"},\"end\":90819,\"start\":90571},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":740063},\"end\":91038,\"start\":90821},{\"attributes\":{\"doi\":\"arXiv:1511.05952\",\"id\":\"b29\"},\"end\":91265,\"start\":91040},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":10208474},\"end\":91571,\"start\":91267},{\"attributes\":{\"id\":\"b31\"},\"end\":91772,\"start\":91573},{\"attributes\":{\"id\":\"b32\"},\"end\":92049,\"start\":91774},{\"attributes\":{\"id\":\"b33\"},\"end\":92220,\"start\":92051},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6070080},\"end\":92573,\"start\":92222},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":85517967},\"end\":92970,\"start\":92575},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":22556995},\"end\":93293,\"start\":92972},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":19135734},\"end\":93655,\"start\":93295},{\"attributes\":{\"id\":\"b38\"},\"end\":93835,\"start\":93657},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":632032},\"end\":94324,\"start\":93837},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3427554},\"end\":94789,\"start\":94326},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":228094432},\"end\":95159,\"start\":94791},{\"attributes\":{\"id\":\"b42\"},\"end\":97503,\"start\":95161},{\"attributes\":{\"id\":\"b43\"},\"end\":98549,\"start\":97505},{\"attributes\":{\"id\":\"b44\"},\"end\":99649,\"start\":98551},{\"attributes\":{\"id\":\"b0\"},\"end\":81431,\"start\":81228},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":220314942},\"end\":81857,\"start\":81433},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":221590125},\"end\":82268,\"start\":81859},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":186206717},\"end\":82586,\"start\":82270},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4610262},\"end\":83138,\"start\":82588},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":220265661},\"end\":83527,\"start\":83140},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":115821513},\"end\":84003,\"start\":83529},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":220793643},\"end\":84396,\"start\":84005},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3443330},\"end\":84910,\"start\":84398},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":34404048},\"end\":85310,\"start\":84912},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":117530196},\"end\":85695,\"start\":85312},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":119303417},\"end\":86054,\"start\":85697},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":44093491},\"end\":86359,\"start\":86056},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":226439636},\"end\":86727,\"start\":86361},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":214761889},\"end\":87133,\"start\":86729},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3346034},\"end\":87624,\"start\":87135},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":30383104},\"end\":88036,\"start\":87626},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":18313385},\"end\":88387,\"start\":88038},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":220496667},\"end\":88806,\"start\":88389},{\"attributes\":{\"doi\":\"arXiv:2009.07888\",\"id\":\"b19\"},\"end\":89050,\"start\":88808},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":123329493},\"end\":89217,\"start\":89052},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":208910339},\"end\":89371,\"start\":89219},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":205242740},\"end\":89567,\"start\":89373},{\"attributes\":{\"id\":\"b23\"},\"end\":89684,\"start\":89569},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53018231},\"end\":90027,\"start\":89686},{\"attributes\":{\"id\":\"b25\"},\"end\":90191,\"start\":90029},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6208256},\"end\":90569,\"start\":90193},{\"attributes\":{\"doi\":\"arXiv:2102.07572\",\"id\":\"b27\"},\"end\":90819,\"start\":90571},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":740063},\"end\":91038,\"start\":90821},{\"attributes\":{\"doi\":\"arXiv:1511.05952\",\"id\":\"b29\"},\"end\":91265,\"start\":91040},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":10208474},\"end\":91571,\"start\":91267},{\"attributes\":{\"id\":\"b31\"},\"end\":91772,\"start\":91573},{\"attributes\":{\"id\":\"b32\"},\"end\":92049,\"start\":91774},{\"attributes\":{\"id\":\"b33\"},\"end\":92220,\"start\":92051},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6070080},\"end\":92573,\"start\":92222},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":85517967},\"end\":92970,\"start\":92575},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":22556995},\"end\":93293,\"start\":92972},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":19135734},\"end\":93655,\"start\":93295},{\"attributes\":{\"id\":\"b38\"},\"end\":93835,\"start\":93657},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":632032},\"end\":94324,\"start\":93837},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3427554},\"end\":94789,\"start\":94326},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":228094432},\"end\":95159,\"start\":94791},{\"attributes\":{\"id\":\"b42\"},\"end\":97503,\"start\":95161},{\"attributes\":{\"id\":\"b43\"},\"end\":98549,\"start\":97505},{\"attributes\":{\"id\":\"b44\"},\"end\":99649,\"start\":98551}]", "bib_title": "[{\"end\":81535,\"start\":81433},{\"end\":81942,\"start\":81859},{\"end\":82344,\"start\":82270},{\"end\":82670,\"start\":82588},{\"end\":83257,\"start\":83140},{\"end\":83668,\"start\":83529},{\"end\":84087,\"start\":84005},{\"end\":84451,\"start\":84398},{\"end\":84989,\"start\":84912},{\"end\":85365,\"start\":85312},{\"end\":85775,\"start\":85697},{\"end\":86115,\"start\":86056},{\"end\":86440,\"start\":86361},{\"end\":86801,\"start\":86729},{\"end\":87225,\"start\":87135},{\"end\":87720,\"start\":87626},{\"end\":88114,\"start\":88038},{\"end\":88487,\"start\":88389},{\"end\":89080,\"start\":89052},{\"end\":89229,\"start\":89219},{\"end\":89428,\"start\":89373},{\"end\":89772,\"start\":89686},{\"end\":90243,\"start\":90193},{\"end\":90850,\"start\":90821},{\"end\":91302,\"start\":91267},{\"end\":91621,\"start\":91573},{\"end\":92284,\"start\":92222},{\"end\":92628,\"start\":92575},{\"end\":93016,\"start\":92972},{\"end\":93357,\"start\":93295},{\"end\":93897,\"start\":93837},{\"end\":94412,\"start\":94326},{\"end\":94875,\"start\":94791},{\"end\":95486,\"start\":95161},{\"end\":97582,\"start\":97505},{\"end\":98735,\"start\":98551},{\"end\":81535,\"start\":81433},{\"end\":81942,\"start\":81859},{\"end\":82344,\"start\":82270},{\"end\":82670,\"start\":82588},{\"end\":83257,\"start\":83140},{\"end\":83668,\"start\":83529},{\"end\":84087,\"start\":84005},{\"end\":84451,\"start\":84398},{\"end\":84989,\"start\":84912},{\"end\":85365,\"start\":85312},{\"end\":85775,\"start\":85697},{\"end\":86115,\"start\":86056},{\"end\":86440,\"start\":86361},{\"end\":86801,\"start\":86729},{\"end\":87225,\"start\":87135},{\"end\":87720,\"start\":87626},{\"end\":88114,\"start\":88038},{\"end\":88487,\"start\":88389},{\"end\":89080,\"start\":89052},{\"end\":89229,\"start\":89219},{\"end\":89428,\"start\":89373},{\"end\":89772,\"start\":89686},{\"end\":90243,\"start\":90193},{\"end\":90850,\"start\":90821},{\"end\":91302,\"start\":91267},{\"end\":91621,\"start\":91573},{\"end\":92284,\"start\":92222},{\"end\":92628,\"start\":92575},{\"end\":93016,\"start\":92972},{\"end\":93357,\"start\":93295},{\"end\":93897,\"start\":93837},{\"end\":94412,\"start\":94326},{\"end\":94875,\"start\":94791},{\"end\":95486,\"start\":95161},{\"end\":97582,\"start\":97505},{\"end\":98735,\"start\":98551}]", "bib_author": "[{\"end\":81543,\"start\":81537},{\"end\":81557,\"start\":81543},{\"end\":81566,\"start\":81557},{\"end\":81575,\"start\":81566},{\"end\":81586,\"start\":81575},{\"end\":81954,\"start\":81944},{\"end\":81967,\"start\":81954},{\"end\":81976,\"start\":81967},{\"end\":81989,\"start\":81976},{\"end\":82359,\"start\":82346},{\"end\":82370,\"start\":82359},{\"end\":82382,\"start\":82370},{\"end\":82392,\"start\":82382},{\"end\":82685,\"start\":82672},{\"end\":82698,\"start\":82685},{\"end\":82709,\"start\":82698},{\"end\":82719,\"start\":82709},{\"end\":82733,\"start\":82719},{\"end\":83268,\"start\":83259},{\"end\":83277,\"start\":83268},{\"end\":83678,\"start\":83670},{\"end\":83686,\"start\":83678},{\"end\":83695,\"start\":83686},{\"end\":83704,\"start\":83695},{\"end\":84100,\"start\":84089},{\"end\":84106,\"start\":84100},{\"end\":84117,\"start\":84106},{\"end\":84127,\"start\":84117},{\"end\":84136,\"start\":84127},{\"end\":84468,\"start\":84453},{\"end\":84483,\"start\":84468},{\"end\":84496,\"start\":84483},{\"end\":84507,\"start\":84496},{\"end\":84518,\"start\":84507},{\"end\":84531,\"start\":84518},{\"end\":85001,\"start\":84991},{\"end\":85021,\"start\":85001},{\"end\":85032,\"start\":85021},{\"end\":85382,\"start\":85367},{\"end\":85394,\"start\":85382},{\"end\":85406,\"start\":85394},{\"end\":85419,\"start\":85406},{\"end\":85787,\"start\":85777},{\"end\":85801,\"start\":85787},{\"end\":85812,\"start\":85801},{\"end\":86124,\"start\":86117},{\"end\":86134,\"start\":86124},{\"end\":86145,\"start\":86134},{\"end\":86449,\"start\":86442},{\"end\":86457,\"start\":86449},{\"end\":86463,\"start\":86457},{\"end\":86471,\"start\":86463},{\"end\":86810,\"start\":86803},{\"end\":86820,\"start\":86810},{\"end\":86831,\"start\":86820},{\"end\":87234,\"start\":87227},{\"end\":87242,\"start\":87234},{\"end\":87729,\"start\":87722},{\"end\":87737,\"start\":87729},{\"end\":87744,\"start\":87737},{\"end\":87751,\"start\":87744},{\"end\":87759,\"start\":87751},{\"end\":88130,\"start\":88116},{\"end\":88141,\"start\":88130},{\"end\":88499,\"start\":88489},{\"end\":88510,\"start\":88499},{\"end\":88521,\"start\":88510},{\"end\":88531,\"start\":88521},{\"end\":88815,\"start\":88808},{\"end\":88822,\"start\":88815},{\"end\":88830,\"start\":88822},{\"end\":89093,\"start\":89082},{\"end\":89248,\"start\":89231},{\"end\":89257,\"start\":89248},{\"end\":89438,\"start\":89430},{\"end\":89582,\"start\":89571},{\"end\":89593,\"start\":89582},{\"end\":89601,\"start\":89593},{\"end\":89611,\"start\":89601},{\"end\":89619,\"start\":89611},{\"end\":89625,\"start\":89619},{\"end\":89785,\"start\":89774},{\"end\":89790,\"start\":89785},{\"end\":90082,\"start\":90070},{\"end\":90093,\"start\":90082},{\"end\":90258,\"start\":90245},{\"end\":90266,\"start\":90258},{\"end\":90276,\"start\":90266},{\"end\":90583,\"start\":90571},{\"end\":90861,\"start\":90852},{\"end\":90879,\"start\":90861},{\"end\":90887,\"start\":90879},{\"end\":91081,\"start\":91071},{\"end\":91089,\"start\":91081},{\"end\":91103,\"start\":91089},{\"end\":91113,\"start\":91103},{\"end\":91314,\"start\":91304},{\"end\":91631,\"start\":91623},{\"end\":91850,\"start\":91837},{\"end\":91859,\"start\":91850},{\"end\":91869,\"start\":91859},{\"end\":92298,\"start\":92286},{\"end\":92310,\"start\":92298},{\"end\":92320,\"start\":92310},{\"end\":92330,\"start\":92320},{\"end\":92341,\"start\":92330},{\"end\":92640,\"start\":92630},{\"end\":93029,\"start\":93018},{\"end\":93039,\"start\":93029},{\"end\":93050,\"start\":93039},{\"end\":93060,\"start\":93050},{\"end\":93369,\"start\":93359},{\"end\":93686,\"start\":93672},{\"end\":93696,\"start\":93686},{\"end\":93709,\"start\":93696},{\"end\":93912,\"start\":93899},{\"end\":93922,\"start\":93912},{\"end\":94425,\"start\":94414},{\"end\":94438,\"start\":94425},{\"end\":94453,\"start\":94438},{\"end\":94889,\"start\":94877},{\"end\":94900,\"start\":94889},{\"end\":94910,\"start\":94900},{\"end\":94916,\"start\":94910},{\"end\":95501,\"start\":95488},{\"end\":97595,\"start\":97584},{\"end\":81543,\"start\":81537},{\"end\":81557,\"start\":81543},{\"end\":81566,\"start\":81557},{\"end\":81575,\"start\":81566},{\"end\":81586,\"start\":81575},{\"end\":81954,\"start\":81944},{\"end\":81967,\"start\":81954},{\"end\":81976,\"start\":81967},{\"end\":81989,\"start\":81976},{\"end\":82359,\"start\":82346},{\"end\":82370,\"start\":82359},{\"end\":82382,\"start\":82370},{\"end\":82392,\"start\":82382},{\"end\":82685,\"start\":82672},{\"end\":82698,\"start\":82685},{\"end\":82709,\"start\":82698},{\"end\":82719,\"start\":82709},{\"end\":82733,\"start\":82719},{\"end\":83268,\"start\":83259},{\"end\":83277,\"start\":83268},{\"end\":83678,\"start\":83670},{\"end\":83686,\"start\":83678},{\"end\":83695,\"start\":83686},{\"end\":83704,\"start\":83695},{\"end\":84100,\"start\":84089},{\"end\":84106,\"start\":84100},{\"end\":84117,\"start\":84106},{\"end\":84127,\"start\":84117},{\"end\":84136,\"start\":84127},{\"end\":84468,\"start\":84453},{\"end\":84483,\"start\":84468},{\"end\":84496,\"start\":84483},{\"end\":84507,\"start\":84496},{\"end\":84518,\"start\":84507},{\"end\":84531,\"start\":84518},{\"end\":85001,\"start\":84991},{\"end\":85021,\"start\":85001},{\"end\":85032,\"start\":85021},{\"end\":85382,\"start\":85367},{\"end\":85394,\"start\":85382},{\"end\":85406,\"start\":85394},{\"end\":85419,\"start\":85406},{\"end\":85787,\"start\":85777},{\"end\":85801,\"start\":85787},{\"end\":85812,\"start\":85801},{\"end\":86124,\"start\":86117},{\"end\":86134,\"start\":86124},{\"end\":86145,\"start\":86134},{\"end\":86449,\"start\":86442},{\"end\":86457,\"start\":86449},{\"end\":86463,\"start\":86457},{\"end\":86471,\"start\":86463},{\"end\":86810,\"start\":86803},{\"end\":86820,\"start\":86810},{\"end\":86831,\"start\":86820},{\"end\":87234,\"start\":87227},{\"end\":87242,\"start\":87234},{\"end\":87729,\"start\":87722},{\"end\":87737,\"start\":87729},{\"end\":87744,\"start\":87737},{\"end\":87751,\"start\":87744},{\"end\":87759,\"start\":87751},{\"end\":88130,\"start\":88116},{\"end\":88141,\"start\":88130},{\"end\":88499,\"start\":88489},{\"end\":88510,\"start\":88499},{\"end\":88521,\"start\":88510},{\"end\":88531,\"start\":88521},{\"end\":88815,\"start\":88808},{\"end\":88822,\"start\":88815},{\"end\":88830,\"start\":88822},{\"end\":89093,\"start\":89082},{\"end\":89248,\"start\":89231},{\"end\":89257,\"start\":89248},{\"end\":89438,\"start\":89430},{\"end\":89582,\"start\":89571},{\"end\":89593,\"start\":89582},{\"end\":89601,\"start\":89593},{\"end\":89611,\"start\":89601},{\"end\":89619,\"start\":89611},{\"end\":89625,\"start\":89619},{\"end\":89785,\"start\":89774},{\"end\":89790,\"start\":89785},{\"end\":90082,\"start\":90070},{\"end\":90093,\"start\":90082},{\"end\":90258,\"start\":90245},{\"end\":90266,\"start\":90258},{\"end\":90276,\"start\":90266},{\"end\":90583,\"start\":90571},{\"end\":90861,\"start\":90852},{\"end\":90879,\"start\":90861},{\"end\":90887,\"start\":90879},{\"end\":91081,\"start\":91071},{\"end\":91089,\"start\":91081},{\"end\":91103,\"start\":91089},{\"end\":91113,\"start\":91103},{\"end\":91314,\"start\":91304},{\"end\":91631,\"start\":91623},{\"end\":91850,\"start\":91837},{\"end\":91859,\"start\":91850},{\"end\":91869,\"start\":91859},{\"end\":92298,\"start\":92286},{\"end\":92310,\"start\":92298},{\"end\":92320,\"start\":92310},{\"end\":92330,\"start\":92320},{\"end\":92341,\"start\":92330},{\"end\":92640,\"start\":92630},{\"end\":93029,\"start\":93018},{\"end\":93039,\"start\":93029},{\"end\":93050,\"start\":93039},{\"end\":93060,\"start\":93050},{\"end\":93369,\"start\":93359},{\"end\":93686,\"start\":93672},{\"end\":93696,\"start\":93686},{\"end\":93709,\"start\":93696},{\"end\":93912,\"start\":93899},{\"end\":93922,\"start\":93912},{\"end\":94425,\"start\":94414},{\"end\":94438,\"start\":94425},{\"end\":94453,\"start\":94438},{\"end\":94889,\"start\":94877},{\"end\":94900,\"start\":94889},{\"end\":94910,\"start\":94900},{\"end\":94916,\"start\":94910},{\"end\":95501,\"start\":95488},{\"end\":97595,\"start\":97584}]", "bib_venue": "[{\"end\":82874,\"start\":82812},{\"end\":87403,\"start\":87331},{\"end\":90385,\"start\":90339},{\"end\":91423,\"start\":91377},{\"end\":92789,\"start\":92723},{\"end\":93478,\"start\":93432},{\"end\":94107,\"start\":94023},{\"end\":94562,\"start\":94516},{\"end\":95672,\"start\":95611},{\"end\":97712,\"start\":97692},{\"end\":82874,\"start\":82812},{\"end\":87403,\"start\":87331},{\"end\":90385,\"start\":90339},{\"end\":91423,\"start\":91377},{\"end\":92789,\"start\":92723},{\"end\":93478,\"start\":93432},{\"end\":94107,\"start\":94023},{\"end\":94562,\"start\":94516},{\"end\":95672,\"start\":95611},{\"end\":97712,\"start\":97692},{\"end\":81261,\"start\":81228},{\"end\":81617,\"start\":81586},{\"end\":82033,\"start\":81989},{\"end\":82403,\"start\":82392},{\"end\":82810,\"start\":82733},{\"end\":83308,\"start\":83277},{\"end\":83738,\"start\":83704},{\"end\":84175,\"start\":84136},{\"end\":84632,\"start\":84531},{\"end\":85093,\"start\":85032},{\"end\":85472,\"start\":85419},{\"end\":85850,\"start\":85812},{\"end\":86181,\"start\":86145},{\"end\":86512,\"start\":86471},{\"end\":86911,\"start\":86831},{\"end\":87329,\"start\":87242},{\"end\":87800,\"start\":87759},{\"end\":88185,\"start\":88141},{\"end\":88567,\"start\":88531},{\"end\":88904,\"start\":88846},{\"end\":89113,\"start\":89093},{\"end\":89268,\"start\":89257},{\"end\":89444,\"start\":89438},{\"end\":89829,\"start\":89790},{\"end\":90068,\"start\":90029},{\"end\":90337,\"start\":90276},{\"end\":90669,\"start\":90599},{\"end\":90906,\"start\":90887},{\"end\":91069,\"start\":91040},{\"end\":91375,\"start\":91314},{\"end\":91657,\"start\":91631},{\"end\":91835,\"start\":91774},{\"end\":92085,\"start\":92051},{\"end\":92371,\"start\":92341},{\"end\":92721,\"start\":92640},{\"end\":93106,\"start\":93060},{\"end\":93430,\"start\":93369},{\"end\":93670,\"start\":93657},{\"end\":94021,\"start\":93922},{\"end\":94514,\"start\":94453},{\"end\":94956,\"start\":94916},{\"end\":95609,\"start\":95501},{\"end\":97690,\"start\":97595},{\"end\":98847,\"start\":98737},{\"end\":81261,\"start\":81228},{\"end\":81617,\"start\":81586},{\"end\":82033,\"start\":81989},{\"end\":82403,\"start\":82392},{\"end\":82810,\"start\":82733},{\"end\":83308,\"start\":83277},{\"end\":83738,\"start\":83704},{\"end\":84175,\"start\":84136},{\"end\":84632,\"start\":84531},{\"end\":85093,\"start\":85032},{\"end\":85472,\"start\":85419},{\"end\":85850,\"start\":85812},{\"end\":86181,\"start\":86145},{\"end\":86512,\"start\":86471},{\"end\":86911,\"start\":86831},{\"end\":87329,\"start\":87242},{\"end\":87800,\"start\":87759},{\"end\":88185,\"start\":88141},{\"end\":88567,\"start\":88531},{\"end\":88904,\"start\":88846},{\"end\":89113,\"start\":89093},{\"end\":89268,\"start\":89257},{\"end\":89444,\"start\":89438},{\"end\":89829,\"start\":89790},{\"end\":90068,\"start\":90029},{\"end\":90337,\"start\":90276},{\"end\":90669,\"start\":90599},{\"end\":90906,\"start\":90887},{\"end\":91069,\"start\":91040},{\"end\":91375,\"start\":91314},{\"end\":91657,\"start\":91631},{\"end\":91835,\"start\":91774},{\"end\":92085,\"start\":92051},{\"end\":92371,\"start\":92341},{\"end\":92721,\"start\":92640},{\"end\":93106,\"start\":93060},{\"end\":93430,\"start\":93369},{\"end\":93670,\"start\":93657},{\"end\":94021,\"start\":93922},{\"end\":94514,\"start\":94453},{\"end\":94956,\"start\":94916},{\"end\":95609,\"start\":95501},{\"end\":97690,\"start\":97595},{\"end\":98847,\"start\":98737}]"}}}, "year": 2023, "month": 12, "day": 17}