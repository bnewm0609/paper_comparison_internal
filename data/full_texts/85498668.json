{"id": 85498668, "updated": "2023-10-04 12:55:08.481", "metadata": {"title": "Data Poisoning against Differentially-Private Learners: Attacks and Defenses", "authors": "[{\"first\":\"Yuzhe\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Xiaojin\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Justin\",\"last\":\"Hsu\",\"middle\":[]}]", "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence", "journal": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence", "publication_date": {"year": 2019, "month": 3, "day": 23}, "abstract": "Data poisoning attacks aim to manipulate the model produced by a learning algorithm by adversarially modifying the training set. We consider differential privacy as a defensive measure against this type of attack. We show that such learners are resistant to data poisoning attacks when the adversary is only able to poison a small number of items. However, this protection degrades as the adversary poisons more data. To illustrate, we design attack algorithms targeting objective and output perturbation learners, two standard approaches to differentially-private machine learning. Experiments show that our methods are effective when the attacker is allowed to poison sufficiently many training items.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1903.09860", "mag": "2965721472", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcai/Ma0H19", "doi": "10.24963/ijcai.2019/657"}}, "content": {"source": {"pdf_hash": "9df380807e4bb3b4ecaa9371cbada544b7ef498f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1903.09860v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.ijcai.org/proceedings/2019/0657.pdf", "status": "BRONZE"}}, "grobid": {"id": "4bfd2045ed1b4f0449938e4a05c9ef62efd1be0f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9df380807e4bb3b4ecaa9371cbada544b7ef498f.txt", "contents": "\nData Poisoning against Differentially-Private Learners: Attacks and Defenses\n\n\nYuzhe Ma \nDepartment of Computer Sciences\nUniversity of Wisconsin-Madison\n\n\nXiaojin Zhu \nDepartment of Computer Sciences\nUniversity of Wisconsin-Madison\n\n\nJustin Hsu \nDepartment of Computer Sciences\nUniversity of Wisconsin-Madison\n\n\nData Poisoning against Differentially-Private Learners: Attacks and Defenses\n\nData poisoning attacks aim to manipulate the model produced by a learning algorithm by adversarially modifying the training set. We consider differential privacy as a defensive measure against this type of attack. We show that such learners are resistant to data poisoning attacks when the adversary is only able to poison a small number of items. However, this protection degrades as the adversary poisons more data. To illustrate, we design attack algorithms targeting objective and output perturbation learners, two standard approaches to differentially-private machine learning. Experiments show that our methods are effective when the attacker is allowed to poison sufficiently many training items.\n\nIntroduction\n\nAs machine learning is increasingly used for consequential decisions in the real world, the security concerns have received more and more scrutiny. Most machine learning systems were originally designed without much thought to security, but researchers have since identified several kinds of attacks under the umbrella of adversarial machine learning. The most well-known example is adversarial examples [20], where inputs are crafted to fool classifiers. More subtle methods aim to recover training examples [9], or even extract the model itself [21].\n\nIn data poisoning attacks [3,12,15,1,14,22,16,10,24,23], the adversary corrupts examples at training time to manipulate the learned model. As is generally the case in adversarial machine learning, the rise of data poisoning attacks has outpaced the development of robust defenses. While attacks are often evaluated against specific target learning procedures, effective defenses should provide protection-ideally guaranteed-against a broad class of attacks. In this paper, we study differential privacy [8,7] as a general defensive technique against data poisoning. While differential privacy was originally designed to formalize privacy-the output should not depend too much on any single individual's private data-it also provides a defense against data poisoning. Concretely, we establish quantitative bounds on how much an adversary can change the distribution over learned models by manipulating a fixed number of training items. We are not the first to design defenses to data poisoning [19,18,2], but our proposal has several notable strengths compared to previous work. First, our defense provides provable protection against a worst-case adversary. Second, our defense is general. Differentially-private learning procedures are known for a wide variety of tasks, and new algorithms are continually being proposed. These learners are all resilient against data poisoning.\n\nWe complement our theoretical bounds with an empirical evaluation of data poisoning attacks on differentially private learners. Concretely, we design an attack based on stochastic gradient descent to search for effective training examples to corrupt against specific learning algorithms. We evaluate attack on two private learners: objective perturbation [11] and output perturbation [5]. Our evaluation confirms the theoretical prediction that private learners are vulnerable to data poisoning attacks when the adversary can poison sufficiently many examples. A gap remains between performance of attack and theoretical limit of data poisoning attacks-it seems like differential privacy provides a stronger defense than predicted by theory. We discuss possible causes and leave further investigation to future work.\n\n\nPreliminaries\n\nDifferential Privacy. Let the data space be Z = X \u00d7 Y, where X is the feature space and Y is the label space. We write D = \u222a \u221e n=0 Z n for the space of all training sets, and write D \u2208 D for a particular training set. A randomized learner M : D \u00d7 R d \u2192 \u0398 maps a training set D and noise b \u2208 R d drawn from a distribution \u03bd to a model in the model space \u0398.\n\nDefinition 1. (Differential Privacy) M is ( , \u03b4)-differentially-private if \u2200D,D \u2208 D that differ by one item, and \u2200S \u2282 \u0398,\nP (M(D, b) \u2208 S) \u2264 e P M(D, b) \u2208 S + \u03b4,(1)\nwhere the probability is taken over b \u223c \u03bd. When \u03b4 = 0, we say that M is -differentially private.\n\nMany standard machine learning tasks can be phrased as optimization problems modeling empirical risk minimization (ERM). Broadly speaking, there are two families of techniques in the privacy literature for solving these problems. Objective perturbation injects noise into the objective function of a vanilla machine learner to train a randomized model [5,11,17]. In contrast, output perturbation runs the vanilla learner but after training, it randomly perturbs the output model [5]. We will consider data poisoning attacks against both kinds of learners.\n\nThreat Model. We first fix the adversarial attack setting.\n\nKnowledge of the attacker: The attacker has full knowledge of the full training set D and the differentially-private machine learner M, including the noise distribution \u03bd. However, the attacker does not know the realization of b.\n\nAbility of the attacker: The attacker is able to modify the clean training set D by changing the features or labels, adding additional items, or deleting existing items. Nevertheless, we limit the attacker to modify at most k items. Formally, the poisoned training setD must lie in the ball B(D, k) \u2282 D, where the center is the clean data D and the radius k is the maximum number of item changes.\n\nGoal of the attacker: the attacker aims to force the (stochastic) model learned from the poisoned data\u03b8 = M(D, b) to achieve certain nefarious target. Formally, we define a cost function C : \u0398 \u2192 R that measures how much the poisoned model\u03b8 deviates from the attack target. Then the attack problem can be formulated as minimizing an objective function J: mi\u00f1\nD\u2208B(D,k) J(D) := E b C(M(D, b)) .(2)\nThis formulation differs from previous works (e.g. [3,15,22]) because the differentially-private learner produces a stochastic model, so the objective takes the expected value of C. We now show a few attack problem instances formulated as (2). \n(\u03b8) = 1 m m i=1 (\u03b8, z * i ) where (\u03b8, z * i ) is the loss on item z * i = (x * i , y * i ).\nMinimizing J(D) pushes the prediction on x * i towards the target label y * i .\n\nExample 3. (Label-Aversion Attack) If the attacker wants to induce large prediction error on an evaluation set\n{z * i } i\u2208[m] , the attacker can define C(\u03b8) = \u2212 1 m m i=1 (\u03b8, z * i )\n, a non-positive cost. Minimizing J(D) pushes the prediction on x * i away from the true label y * i . Now, we turn to our two main contributions. i) We show that differentially-private learners have some natural immunity against data poisoning attacks. Specifically, in section 3 we present a lower bound on how much the attacker can reduce J(D) under a fixed k (the number of changes). ii) When k increases, the attacker's power grows and it becomes possible to attack differentially-private learners. In section 4 we propose a stochastic gradient descent algorithm to solve the attack optimization problem (2). To our knowledge, ours is the first data poisoning attack targeting differentially-private learners.\n\n\nDifferential Privacy Resists Data Poisoning\n\nFixing the number of poisoned items k and a differentially private learner, can a powerful attacker achieve arbitrarily low attack cost J(D)? We show that the answer is no: differential privacy implies a lower bound on how far J(D) can be reduced, so private learners are inherently resistant to data poisoning attacks. Previous work [13] proposed defenses based on differential privacy, but for test-time attacks. While effective, these defenses require the classifier to make randomized predictions. In contrast, our defense against data poisoning only requires the learning algorithm, not the classification process, to be randomized.\n\nOur first result is for -differentially private learners: if an attacker can manipulate at most k items in the clean data, then the attack cost is lower bounded. Theorem 1. Let M be an -differentially-private learner. Let J(D) be the attack cost, whereD \u2208 B(D, k), then\nJ(D) \u2265 e \u2212k J(D) (C \u2265 0), J(D) \u2265 e k J(D) (C \u2264 0).(3)\nProof. For any poisoned dataD \u2208 B(D, k), one can view it as being produced by manipulating the clean data D k times, where each time one item is changed. Thus one can apply Lemma 2 k times to obtain (3). \n\nProof. We first consider C \u2265 0. Define \u0398(a) = {\u03b8 : C(\u03b8) > a}, \u2200a \u2265 0. SinceD \u2208 B(D, 1),D and D differ by at most one item, thus differential privacy guarantees that \u2200a \u2265 0,\nP (M(D, b) \u2208 \u0398(a)) \u2264 e P M(D, b) \u2208 \u0398(a) .(5)\nSince J(D) is non-negative, by the integral identity, we have  \nJ(D) = E b [C(M(D, b))] =\nSince J(D) is non-positive, by the integral identity, we have\nJ(D) = E b C(M(D, b)) = \u2212 0 \u2212\u221e P C(M(D, b)) < a da = \u2212 0 \u2212\u221e P M(D, b) \u2208 \u0398(a) da \u2265 \u2212 0 \u2212\u221e e P (M(D, b) \u2208 \u0398(a)) da = \u2212 0 \u2212\u221e e P (C(M(D, b)) < a) da = e J(D).(8)\nNote that for C \u2265 0, Theorem 1 shows that no attacker can achieve the trivial lower bound J(D) = 0. For C \u2264 0, although J(D) could be unbounded from below, Theorem 1 shows that no attacker can reduce J(D) arbitrarily. These results crucially rely on the differential privacy property of M. Corollary 3 re-states the theorem in terms of the minimum number of items an attacker has to modify in order to sufficiently reduce the attack cost J(D).\n\nCorollary 3. Let M be an -differentially-private learner. Assume J(D) = 0. Let \u03c4 \u2265 1. Then the attacker has to modify at least k \u2265 1 log \u03c4 items in D in order to achieve\ni). J(D) \u2264 1 \u03c4 J(D) (C \u2265 0). ii). J(D) \u2264 \u03c4 J(D) (C \u2264 0). Proof. For C \u2265 0, by Theorem 1, ifD \u2208 B(D, k) then J(D) \u2265 e \u2212k J(D). Since we require J(D) \u2264 1 \u03c4 J(D), we have e \u2212k J(D) \u2264 1 \u03c4 J(D), thus k \u2265 1 log \u03c4 .\nSimilarly, one can prove the case for C \u2264 0. To generalize Theorem 1 to ( , \u03b4)-private learners, we need an additional assumption that C is bounded.\nTheorem 4. Let M be ( , \u03b4)-differentially-private and J(D) be the attack cost, where |C| \u2264C andD \u2208 B(D, k). Then, J(D) \u2265 max{e \u2212k (J(D) +C \u03b4 e \u2212 1 ) \u2212C \u03b4 e \u2212 1 , 0} (C \u2265 0), J(D) \u2265 max{e k (J(D) \u2212C \u03b4 e \u2212 1 ) +C \u03b4 e \u2212 1 , \u2212C} (C \u2264 0).(9)\nProof. We first consider 0 \u2264 C \u2264C. For any poisoned dataD \u2208 B(D, k), one can view it as being produced by manipulating the clean data D k times, where each time one item is changed. Thus we have a sequence of intermediat\u1ebd\nD i (0 \u2264 i \u2264 k), the data set where i items are poisoned. We defineD 0 = D andD k =D. Let J i = J(D i ), then by Lemma 5 J i \u2265 e \u2212 (J i\u22121 \u2212C\u03b4), \u22001 \u2264 i \u2264 k.(10)\nAddingC \u03b4 e \u22121 to both sides, we have\nJ i +C \u03b4 e \u2212 1 \u2265 e \u2212 (J i\u22121 \u2212C\u03b4) +C \u03b4 e \u2212 1 = e \u2212 (J i\u22121 +C \u03b4 e \u2212 1 ).(11)\nWe then recursively apply (11) k times to obtain\nJ k +C \u03b4 e \u2212 1 \u2265 e \u2212k (J 0 +C \u03b4 e \u2212 1 ) = e \u2212k (J(D) +C \u03b4 e \u2212 1 ).(12)\nNote J k = J(D k ) = J(D), thus we have\nJ(D) \u2265 e \u2212k (J(D) +C \u03b4 e \u2212 1 ) \u2212C \u03b4 e \u2212 1 .(13)\nAlso note that J(D) \u2265 0 trivially holds due to C \u2265 0, thus\nJ(D) \u2265 max{e \u2212k (J(D) +C \u03b4 e \u2212 1 ) \u2212C \u03b4 e \u2212 1 , 0}.(14)\nNext, we consider \u2212C \u2264 C \u2264 0. Define J i as before, then by Lemma 5\nJ i \u2265 e J i\u22121 \u2212C\u03b4, \u22001 \u2264 i \u2264 k.(15)\nSubtracting both sides byC \u03b4 e \u22121 we have\nJ i \u2212C \u03b4 e \u2212 1 \u2265 e \u2212 (J i\u22121 \u2212C \u03b4 e \u2212 1 ), \u22001 \u2264 i \u2264 k.(16)\nThus we have\nJ(D) \u2212C \u03b4 e \u2212 1 \u2265 e \u2212k (J(D) \u2212C \u03b4 e \u2212 1 ).(17)\nCombined with the trivial lower bound \u2212C, we have\nJ(D) \u2265 max{e \u2212k (J(D) \u2212C \u03b4 e \u2212 1 ) +C \u03b4 e \u2212 1 , \u2212C}.(18)\nLemma 5. Let M be an ( , \u03b4)-differentially-private learner. Let J(D) be the attack cost, where |C| \u2264C and D \u2208 B(D, 1), then\nJ(D) \u2265 e \u2212 (J(D) \u2212C\u03b4), (C \u2265 0), J(D) \u2265 e J(D) \u2212C\u03b4, (C \u2264 0).(19)\nProof. We first consider 0 \u2264 C \u2264C. Define \u0398(a) = {\u03b8 : C(\u03b8) > a}, \u2200a \u2265 0. SinceD \u2208 B(D, 1),D and D differ by at most one item, thus the differential privacy guarantees that \u2200a \u2265 0,\nP (M(D, b) \u2208 \u0398(a)) \u2264 e P M(D, b) \u2208 \u0398(a) + \u03b4.(20)\nNote that J(D) is non-negative, thus by the integral identity, we have\nJ(D) = E b [C(M(D, b))] = C 0 P (C(M(D, b)) > a) da = C 0 P (M(D, b) \u2208 \u0398(a)) da \u2264 C 0 e P M(D, b) \u2208 \u0398(a) + \u03b4 da = e C 0 P C(M(D, b)) > a da +C\u03b4 = e J(D) +C\u03b4.(21)Rearranging we have J(D) \u2265 e \u2212 (J(D) \u2212C\u03b4). Next we consider \u2212C \u2264 C \u2264 0. Define \u0398(a) = {\u03b8 : C(\u03b8) < a}, \u2200a \u2264 0 Again, due to M being differentially private, we have P M(D, b) \u2208 \u0398(a) \u2264 e P (M(D, b) \u2208 \u0398(a)) + \u03b4.(22)\nSince J(D) is non-positive, by the integral identity, we have\nJ(D) = E b C(M(D, b)) = \u2212 0 \u2212C P C(M(D, b)) < a da = \u2212 0 \u2212C P M(D, b) \u2208 \u0398(a) da \u2265 \u2212 0 \u2212C (e P (M(D, b) \u2208 \u0398(a)) + \u03b4) da = \u2212e 0 \u2212C P (C(M(D, b)) < a) da \u2212C\u03b4 = e J(D) \u2212C\u03b4.(23)\nAs a corollary, we can lower-bound the minimum number of modifications needed to sufficiently reduce attack cost. \nk \u2265 1 log (e \u2212 1)J(D)\u03c4 +C\u03b4\u03c4 (e \u2212 1)J(D) +C\u03b4\u03c4 . (24) ii). (\u2212C \u2264 C \u2264 0): in order to achieve J(D) \u2264 \u03c4 J(D) for \u03c4 \u2208 [1, \u2212C J(D) ]\n, the attacker has to modify at least the following number of items in D.\nk \u2265 1 log (e \u2212 1)J(D)\u03c4 \u2212C\u03b4 (e \u2212 1)J(D) \u2212C\u03b4 . (25) Proof. For 0 \u2264 C \u2264C, by Theorem 4, ifD \u2208 B(D, k), then J(D) \u2265 max{e \u2212k (J(D) +C \u03b4 e \u22121 ) \u2212C \u03b4 e \u22121 , 0}. Since we require J(D) \u2264 1 \u03c4 J(D), we have e \u2212k (J(D) +C \u03b4 e \u22121 ) \u2212C \u03b4 e \u22121 \u2264 1 \u03c4 J(D).\nRearranging gives the result. The proof for \u2212C \u2264 C \u2264 0 is similar.\n\nNote that in (24), the lower bound on k is always finite even if \u03c4 = \u221e, which means an attacker might be able to reduce the attack cost J(D) to 0. This is in contrast to -differentially-private learner, where J(D) = 0 can never be achieved. The weaker ( , \u03b4)-differential privacy guarantee gives weaker protection against attacks.\n\n\nData Poisoning Attacks on Private Learners\n\nThe results in the previous section provide a theoretical bound on how effective a data poisoning attack can be against a private learner. To evaluate how strong the protection is in practice, we propose a range of attacks targeting general differentially-private learners. Our adversary will modify (not add or delete) any continuous features (for classification or regression) and the continuous labels (for regression) on at most k items in the training set. Restating the attack problem (2): mi\u00f1\nD\u2282B(D,k) J(D) = E b C(M(D, b))(26)\nThis is a combinatorial problem-the attacker must pick k items to modify. We propose a two-step attack procedure.\n\nstep I) use heuristic methods to select k items to poison.\n\nstep II) apply stochastic gradient descent to reduce J(D).\n\nIn step II), performing stochastic gradient descent could lead to poisoned data taking arbitrary features or labels. However, differentially-private learners typically assume training examples are bounded. To avoid trivially-detectable poisoned examples, we project poisoned features and labels back to a bounded space after each iteration of SGD. We now detail step II), assuming that the attacker has fixed k items to poison. We'll return to step I) later.\n\n\nSGD on Differentially-Private Victims (DPV)\n\nOur attacker uses stochastic gradient descent to minimize the attack cost J(D). We first show how to compute a stochastic gradient with respect to a training item.\nProposition 7. Assume M(D, b) is a differentiable function ofD, then a stochastic gradient of J(D) w.r.t. a particular itemz i is \u2202C(M(D,b)) \u2202zi , where b \u223c \u03bd(b).\nProof. Note that\nE b \u2202C(M(D, b)) \u2202z i = \u2202C(M(D, b)) \u2202z i \u03bd(b)db.(27)\nUnder certain conditions (see appendix for the details), one can exchange the order of integration and differentiation, and we have\nE b \u2202C(M(D, b)) \u2202z i = d C(M(D, b))\u03bd(b)db dz i = dJ(D) dz i .(28)Thus \u2202C(M(D,b))\n\u2202zi is a stochastic gradient.\n\nThe concrete form of the stochastic gradient depends on the target private learner M. In the following, we derive the stochastic gradient for objective perturbation and output perturbation in the context of parameter-targeting attack with cost function C(\u03b8) = 1 2 \u03b8 \u2212 \u03b8 2 , where \u03b8 is the target model.\n\n\nInstantiating Attack on Objective Perturbation\n\nWe first consider objective-perturbed private learners:\nM(D, b) = argmin \u03b8\u2208\u0398 n i=1 (\u03b8,z i ) + \u03bb\u2126(\u03b8) + b \u03b8,(29)\nwhere is some convex learning loss, \u2126 is a regularization, and \u0398 is the model space. Note that the noise b enters the objective via linear product with the model \u03b8. By Proposition 7, the stochastic gradient is \u2202C (M(D,b)) \u2202zi = \u2202C(\u03b8) \u2202zi , wher\u1ebd \u03b8 = M(D, b) is the poisoned model. By the chain rule, we have\n\u2202C(\u03b8) \u2202z i = \u2202\u03b8 \u2202z i dC(\u03b8) d\u03b8 .(30)\nNote that dC(\u03b8) d\u03b8 =\u03b8 \u2212 \u03b8 . Next we focus on deriving \u2202\u03b8 \u2202zi . Since (29) is a convex optimization problem, the learned model\u03b8 must satisfy the following Karush-Kuhn-Tucker (KKT) condition:\nf (\u03b8,z i ) := n j=1 \u2202 (\u03b8,z j ) \u2202\u03b8 + \u03bb d\u2126(\u03b8) d\u03b8 + b = 0.(31)\nBy using the derivative of implicit function, we have \u2202\u03b8 \u2202zi = \u2212( \u2202f \u2202\u03b8 ) \u22121 \u2202f \u2202zi . We now give two examples where the base learner is logistic regression and ridge regression, respectively.\n\nAttacking Objective-Perturbed Logistic Regression. In the case (\u03b8,z) = log(1 + exp(\u2212\u1ef9\u03b8 x)) and \u2126(\u03b8) = 1 2 \u03b8 2 , learner (29) is objective-perturbed logistic regression:\nM(D, b) = argmin \u03b8\u2208\u0398 n i=1 log(1 + exp(\u2212\u1ef9 i \u03b8 x i )) + \u03bb 2 \u03b8 2 + b \u03b8,(32)\nwhere \u0398 = R d . Our attacker will only modify the features, thus\u1ef9 i = y i . We now derive stochastic gradient forx i , which is\n\u2202C(\u03b8) \u2202x i = ( \u2202\u03b8 \u2202x i ) dC(\u03b8) d\u03b8 .(33)\nTo compute \u2202\u03b8 \u2202xi , we use the KKT condition of (32):\n\u03bb\u03b8 \u2212 n j=1 y jxj 1 + exp(y j\u03b8 x j ) + b = 0. (34) Let f (\u03b8,x i ) = \u03bb\u03b8 \u2212 n j=1\nyjxj 1+exp(yj\u03b8 xj ) + b and define s j = exp(y j\u03b8 x j ), then one can verify that\n\u2202f \u2202\u03b8 = \u03bbI + n j=1 s jxjx j (1 + s j ) 2 .(35)\nand\n\u2202f \u2202x i = s ixi\u03b8 (1 + s i ) 2 \u2212 y i 1 + s i I.(36)\nNote that \u2202\u03b8 \u2202xi = \u2212( \u2202f \u2202\u03b8 ) \u22121 \u2202f \u2202xi and dC(\u03b8) d\u03b8 =\u03b8 \u2212 \u03b8 , therefore the stochastic gradient ofx i is\n\u2202C(\u03b8) \u2202x i = ( \u2202\u03b8 \u2202x i ) dC(\u03b8) d\u03b8 = y i 1 + s i I \u2212 s i\u03b8x i (1 + s i ) 2 \uf8eb \uf8ed \u03bbI + n j=1 s jxjx j (1 + s j ) 2 \uf8f6 \uf8f8 \u22121 (\u03b8 \u2212 \u03b8 ).(37)\nNote that the noise b enters into stochastic gradient through\u03b8 = M (D, b).\n\nAttacking Objective-Perturbed Ridge Regression. In the case (\u03b8,z) = 1 2 (\u1ef9 \u2212x \u03b8) 2 and \u2126(\u03b8) = 1 2 \u03b8 2 , learner (29) is the objective-perturbed ridge regression:\nM(D, b) = argmin \u03b8\u2208\u0398 1 2 X \u03b8 \u2212\u1ef9 2 + \u03bb 2 \u03b8 2 + b \u03b8,(38)\nwhere \u0398 = {\u03b8 \u2208 R d : \u03b8 2 \u2264 \u03c1},X \u2208 R n\u00d7d is the feature matrix, and\u1ef9 \u2208 R n is the label vector. Unlike logistic regression, the objective-perturbed ridge regression requires the model space to be bounded (see e.g. [11]). The attacker can modify both the features and the labels. We first compute stochastic gradient forx i , which is\n\u2202C(\u03b8) \u2202x i = ( \u2202\u03b8 \u2202x i ) dC(\u03b8) d\u03b8 .(39)\nSince dC(\u03b8) d\u03b8 =\u03b8 \u2212 \u03b8 , it suffices to compute \u2202\u03b8 \u2202xi , for which we use the KKT condition of (38):\n\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 (X X + \u03bbI)\u03b8 \u2212X \u1ef9 + b + \u00b5\u03b8 = 0 1 2 \u03b8 2 \u2264 1 2 \u03c1 2 \u00b5 \u2265 0 \u00b5( \u03b8 2 \u2212 \u03c1 2 ) = 0,(40)\nwhere \u00b5 is the Lagrange dual variable for constraint 1\n2 \u03b8 2 \u2264 1 2 \u03c1 2 . Let f (\u03b8,x i , \u00b5) = (X X + \u03bbI)\u03b8 \u2212X \u1ef9 + b + \u00b5\u03b8. One can rewrite f (\u03b8,x i , \u00b5) as f (\u03b8,x i , \u00b5) = (x ix i + j =ix jx j + \u03bbI)\u03b8 \u2212x i\u1ef9i \u2212 j =ix j\u1ef9j + b + \u00b5\u03b8.(41)\nThen one can verify that\n\u2202f \u2202x i =x i\u03b8 + (x i\u03b8 \u2212\u1ef9 i )I.(42)\nand \u2202f \u2202\u03b8 =X X + (\u03bb + \u00b5)I.\n\nThus the stochastic gradient ofx i is\n\u2202C(\u03b8) \u2202x i = \u03b8x i + (x i\u03b8 \u2212\u1ef9 i )I X X + (\u03bb + \u00b5)I \u22121 (\u03b8 \u2212\u03b8).(44)\nNext we derive the stochastic gradient of\u1ef9 i . Now view f in (41) as a function of\u03b8,\u1ef9 i and \u00b5, then one can verify that\n\u2202f \u2202\u1ef9 i = \u2212x i .(45)\nand \u2202f \u2202\u03b8 =X X + (\u03bb + \u00b5)I.\n\nThus the stochastic gradient of\u1ef9 i is\n\u2202C(\u03b8) \u2202\u1ef9 i =x i X X + (\u03bb + \u00b5)I \u22121 (\u03b8 \u2212 \u03b8 ).(47)\nNote that the noise b enters into the stochastic gradient through\u03b8 = M(D, b).\n\n\nInstantiating Attack on Output Perturbation\n\nWe now consider the output perturbation mechanism:\nM(D, b) = b + argmin \u03b8\u2208\u0398 n i=1 (\u03b8,z i ) + \u03bb\u2126(\u03b8) .(48)\nThe derivations of stochastic gradient are similar to objective perturbation, thus we skip the details. Again, we instantiate on two examples where the base learner is logistic regression and ridge regression, respectively.\n\nAttacking Output-Perturbed Logistic Regression. The output-perturbed logistic regression takes the following form:\nM(D, b) = b + argmin \u03b8\u2208R d n i=1 log(1 + exp(\u2212\u1ef9 i \u03b8 x i )) + \u03bb 2 \u03b8 2 ,(49)\nLet s j = exp(y j (\u03b8 \u2212 b) x j ), then the stochastic gradient ofx i is\n\u2202C(\u03b8) \u2202x i = y i 1 + s i I \u2212 s i (\u03b8 \u2212 b)x i (1 + s i ) 2 \uf8eb \uf8ed \u03bbI + n j=1 s jxjx j (1 + s j ) 2 \uf8f6 \uf8f8 \u22121 (\u03b8 \u2212 \u03b8 ).(50)\nAttacking Output-Perturbed Ridge Regression. The output-perturbed ridge regression takes the following form\nM(D, b) = b + argmin \u03b8\u2208\u0398 1 2 X \u03b8 \u2212\u1ef9 2 + \u03bb 2 \u03b8 2 ,(51)\nwhere \u0398 = {\u03b8 : \u03b8 2 \u2264 \u03c1}. The KKT condition of (51) is\n\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 (X X + \u03bbI)(\u03b8 \u2212 b) \u2212X \u1ef9 + \u00b5(\u03b8 \u2212 b) = 0 1 2 \u03b8 \u2212 b 2 \u2264 1 2 \u03c1 2 \u00b5 \u2265 0 \u00b5( \u03b8 \u2212 b 2 \u2212 \u03c1 2 ) = 0.(52)\nThen one can verify that the stochastic gradients ofx i and\u1ef9 i are\n\u2202C(\u03b8) \u2202x i = (\u03b8 \u2212 b)x i +x i (\u03b8 \u2212 b)I \u2212\u1ef9 i I X X + (\u03bb + \u00b5)I) \u22121 (\u03b8 \u2212\u03b8).(53)\u2202C(\u03b8) \u2202\u1ef9 i =x i X X + (\u03bb + \u00b5)I \u22121 (\u03b8 \u2212 \u03b8 ).(54)\n\nSGD on Surrogate Victims (SV)\n\nWe also consider an alternative, simpler way to perform step II). When b = 0, the differentially-private learning algorithms revert to the base learner which returns a deterministic model M(D, 0). The adversary can simply attack the base learner (without b) as a surrogate for the differentially-private learner M (with b) by solving the following problem: mi\u00f1\nD C(M(D, 0)).(55)\nSince the objective is deterministic, we can work with its gradient rather than its stochastic gradient, plugging in b = 0 into all derivations in section 4.1. Note that the attack found by (55) will still be evaluated w.r.t. differentially-private learners in our experiments.\n\n\nSelecting Items to Poison\n\nNow, we return to step I) of our attack: how can we select the k items for poisoning? We give two heuristic methods: shallow and deep selection. Shallow selection selects top-k items with the largest initial gradient norm \u2202J(D) \u2202zi |D =D . Intuitively, modifying these items will reduce the attack cost the most, at least initially. The precise gradients to use during selection depend on the attack in step II). When targeting differentially-private learners directly, computing the gradient of J(D) is difficult. We use Monte-Carlo sampling to approximate the gradient g i of item z i :\ng i \u2248 1 m m s=1 \u2202 \u2202zi C(M(D, b s )) |D =D ,\nwhere b s are random samples of privacy parameters. Then, the attacker picks the k items with the largest g i to poison. When targeting surrogate victim, the objective is C (M(D, 0)), thus the attacker computes the gradient of C (M(D, 0)) of each clean item z i : g i = d dzi C(M(D, 0)) |D =D , and then picks those k items with the largest g i . Deep selection selects items by estimating the influence of an item on the final attack cost. When targeting a differentially-private victim directly, the attacker first solves the following relaxed attack problem: minD J(D) + \u03b1R(D). Importantly, hereD allows changes to all training items, not just k of them. R(D) is a regularizor penalizing data modification with weight \u03b1 > 0. We take R(D) = n i=1 r(z i ), where r(z i ) is the distance between the poisoned itemz i and the clean item z i . We define r(z i ) = 1 2 x i \u2212 x i 2 for logistic regression and r(z i ) = 1 2 x i \u2212 x i 2 + 1 2 (\u1ef9 i \u2212 y i ) 2 for ridge regression. After solving the relaxed attack problem with SGD, the attacker evaluates the amount of change r(z i ) for all training items and pick the top k to poison. When targeting a surrogate victim, the attacker can instead solve the following relaxed attack first: minD C(M(D, 0)) + \u03b1R(D), and then select k top items.\n\nIn summary, we have four attack algorithms based on combinations of methods used in step I) and step II): shallow-SV, shallow-DPV, deep-SV and deep-DPV. In the following, we evaluate the performance of these four algorithms.\n\n\nExperiments\n\nWe now evaluate our attack with experiments, taking objective-perturbed learners [11] and output-perturbed learners [5] as our victims. Through the experiment, we fix a constant step size \u03b7 = 1 for (stochastic) gradient descent. After each iteration of SGD, we project poisoned items to ensure feature norm is at most 1, and label stays in [\u22121, 1]. To perform shallow selection when targeting DPV, we draw 10 3 samples of parameter b to evaluate the gradient of each clean item. To perform deep selection, we use \u03b1 = 10 \u22124 to solve the corresponding relaxed attack problem.\n\n\nAttacks Intelligently Modify Data\n\nAs a first example, we use label-aversion attack to illustrate that our attack modifies data intelligently. The victim is an objective-perturbed learner for -differentially private logistic regression, with = 0.1 and regularizer \u03bb = 10. The training set contains n = 21 items uniformly sampled in the interval [\u22121, 1] with labels y i = 1 [x i \u2265 0], see Figure 1 \n* i = 1 [x * i \u2265 0]. The cost function is defined as C(\u03b8) = \u2212 1 m m i=1 log(1 + exp(\u2212y * i x * i \u03b8 ))\n.\n\nTo achieve small attack cost J, a negative model\u03b8 < 0 is desired. 1 We run deep-DPV with k = n (attacker can modify all items) 2 for T = 300 iterations. In Figure 1(a), we show the position of the poisoned items on the vertical axis as the SGD iteration, t, grows. As expected, the attacker flips the positions of positive (blue) items and negative (red) items. As a result, our attack causes the learner to find a model with\u03b8 < 0.\n\n\nAttacks Can Achieve Different Goals\n\nWe now study a 2D example to show that our attack is effective for different attack goals. The victim is the same as in section 5.1. The clean training set in Figure 1(b) contains n = 317 items uniformly sampled in a unit sphere with labels 1). We now describe the attack settings for different attack goals.\ny i = 1 x i \u03b8 * \u2265 0 where \u03b8 * = (1,\nLabel-aversion attack. We generate an evaluation set containing m = 317 grid points (x * i , y * i ) lying in a unit sphere and labeled by a vertical decision boundary, see Figure 1(c). The cost function is defined as\nC(\u03b8) = \u2212 1 m m i=1 log(1 + exp(\u2212y * i x * i \u03b8 )).\nLabel-targeting attack. The evaluation set remains the same as in the label-aversion attack. The cost function is defined as\nC(\u03b8) = 1 m m i=1 log(1 + exp(\u2212y * i x * i \u03b8 )).\nParameter-targeting attack. We first train vanilla logistic regression on the evaluation set to obtain the target model \u03b8 = (2.6, 0), then define cost function as C(\u03b8) = 1 2 \u03b8 \u2212 \u03b8 2 . its final position after attack. The curve connecting them shows the trajectory as the attack algorithm gradually poisons the data.\n\nIn label-aversion attack the attacker aims to maximize the logistic loss on the evaluation set. It ends up moving positive (negative) items to the left (right), so that the poisoned data deviates from the evaluation set.\n\nIn contrast, the label-targeting attack tries to minimize the logistic loss, thus items are moved to produce a poisoned data aligned with the evaluation set. However, the attack does not reproduce exactly the evaluation set. To understand it, we compute the model learnt by vanilla logistic regression on the evaluation set and the poisoned data, which are (2.60,0) and (2.94,0.01). Note that both models can predict the right label on the evaluation set, but the latter has larger norm, which leads to smaller attack cost, thus our result is a better attack.\n\nIn parameter-targeting attack, we compute the model learnt by vanilla logistic regression on the clean data and the poisoned data, which are (1.86,1.85) and (2.21, 0.04). Note that the attack pushes the model closer to the target model (2.6,0). Again, the attack does not reproduce exactly the evaluation set, which is because the goal is to minimize the attack cost J = E b C(\u03b8) rather than C(\u03b8). To see that, we evaluate the attack cost J (as explained below) on the evaluation set and the poisoned data, and the values are 2.12 and 1.08 respectively, thus our result is a better attack.\n\nTo quantify the attack performance, we evaluate the attack cost J(D) via Monte-Carlo sampling. We run private learners T e = 10 3 times, each time with a random b s to produce a cost C s = C(M(D, b s )). Then we average to obtain an estimate of the cost J(D) \u2248 1\n\nTe Te s=1 C s . In Figure 2(d)-(f), we show the attack cost J(D) and its 95% confidence interval (green, \u00b12 * stderr) up to t = 10 3 iterations. J(D) decreases in all plots, showing that our attack is effective for all attack goals.\n\n\nAttacks Become More Effective as k Increases\n\nThe theoretical protection provided by differential privacy weakens as the adversary is allowed to poison more training items. In this section, we show that our attacks indeed become more effective as the number of poisoned items k increases. The data set is the same as in section 5.2. We run our four attack algorithms with k from 20 to 100 in steps of 20. For each k, we first select k items by using shallow or deep selection, and then poison the selected items by running (stochastic) gradient descent for T = 5 \u00d7 10 3 iterations. After attack, we estimate the attack cost J(D) on T e = 2 \u00d7 10 3 samples. Figure 3 \n\n\nAttacks Effective on Both Privacy Mechanisms\n\nWe now show experiments on real data to illustrate that our attacks are effective on both objective and output perturbation mechanisms. We focus on label-targeting attack in this section. The first real data set is vertebral column from the UCI Machine Learning Repository [6]. This data set contains 6 features of 310 orthopaedic patients, and the task is to predict if the vertebra of any patient is abnormal (positive class) or not (negative class). We normalize the features so that the norm of any item is at most 1. Since this is a classification task, we use private logistic regression with = 0.1 and \u03bb = 10 as the victim. To generate the evaluation set, we randomly pick one positive item and find its 10 nearest neighbours within the positive class, and set y * i = \u22121 for all 10. Intuitively, this attack targets a small cluster of abnormal patients and the goal is to mislead the learner to classify these patients as normal. The other experimental parameters remain the same as in section 5.3. Note that in order to push the prediction on x * i toward y * i , the attacker requires y * i x * i \u03b8 > 0, which indicates C(\u03b8) = log(1 + exp(\u2212y * i x * i \u03b8 )) < log 2 \u2248 0.69, so should J(D). As shown in Figure 4, the attacker indeed reduces the attack cost J(D) below 0.69 for both privacy mechanisms, thus our attack is successful. The second data set is red wine quality from UCI. The data set contains 11 features of 1598 wine samples, and the task is predict the wine quality, a number between 0 and 10. We normalize the features so that the norm of any item is at most 1. Labels are also normalized to ensure the value is in [\u22121, 1]. The victim is private ridge regression with = 1, \u03bb = 10, and bound on model space \u03c1 = 2. To generate the evaluation set, we pick one item x * i with the smallest quality value, and then set the target label to be y * i = 1. The cost function is defined as\nC(\u03b8) = 1 2 (x * i \u03b8 \u2212 y * i ) 2 .\nThe other parameters of attack remain the same as in section 5.3. This attack aims to force the learner to predict a low-quality wine as having a high quality. Figure 5 shows the results on objective perturbation and output perturbation respectively. Note that the attack cost can be effectively reduced even if the attacker only poisons 100/1598 \u2248 6.3% of the whole data set. Given experimental results above, we make another two observations here. First, deep-DPV is in general the most effective method among four attack algorithms, see e.g. Figure 3(a), (c). Second, although deep-DPV is effective, there remains a gap between the attack performance and the theoretical lower bound, as is shown in all previous plots. One potential reason is that the lower bound is derived purely based on the differential privacy property, thus does not take into account the learning procedure of the victim. Analysis oriented toward specific victim learners may result in a tighter lower bound. Another potential reason is that our attack might not be effective enough. How to close the gap is left as future work.\n\n\nAttack Is Easier with Weaker Privacy\n\nFinally, we show that the attack is more effective as the privacy guarantee becomes weaker, i.e., as we increase . To illustrate, we run experiments on the data set in section 5.2, where we fix k = 100 and vary from 0.1 to 0.5. In Figure 6, we show the attack cost J(D) against . Note that the attack cost J(D) approaches the lower bound as grows. This means weaker privacy guarantee (smaller ) leads to easier attacks. \n\n\nConclusion and Future Work\n\nWe showed that differentially private learners are provably resistant to data poisoning attacks, with the protection degrading exponentially as the attacker poisons more items. Then, we proposed attacks that can effectively poison differentially-private learners, and demonstrated the attack performance on a variety of privacy mechanisms and learners with both synthetic and real data. While the attacks are effective, there remains a gap between the theoretical lower bound and the empirical performance of our attacks. This could be because the lower bound is loose, or our attack is not effective enough; closing the gap remains future work. Taken together, our study is a first step towards understanding the strengths and weaknesses of differential privacy as a defensive measure against data poisoning.\n\nwhere s j = exp(y j\u03b8 x j ) and\u03b8 = M(D, b). We let d = 1 in condition i). Note that x i \u2264 1, thus for anyx i such that x i \u2212x i \u2264 d, by triangle inequality we have x i \u2264 x i + d = 2. Let s i = exp(y i\u03b8 x i ). By (57), we have\n\u2202C(\u03b8) \u2202x i |x i=x i \u2264 y i 1 + s i I \u2212 s i\u03b8x i (1 + s i ) 2 \u00b7 \uf8eb \uf8ec \uf8ec \uf8ed \u03bbI + 1\u2264j\u2264n j =i s jxjx j (1 + s j ) 2 + s ix ix i (1 + s i ) 2 \uf8f6 \uf8f7 \uf8f7 \uf8f8 \u22121 \u00b7 \u03b8 \u2212 \u03b8 \u2264 y i 1 + s i I + s i\u03b8x i (1 + s i ) 2 1 \u03bb ( \u03b8 + \u03b8 ) = 1 \u03bb 1 1 + s i + s i (1 + s i ) 2 \u03b8 x i ( \u03b8 + \u03b8 ) \u2264 1 \u03bb (1 + 1 2 \u03b8 )( \u03b8 + \u03b8 ) (since 1 + s i \u2265 2 s i and x i \u2264 2).(58)\nNow we upper bound \u03b8 . By (34),\n\u03b8 = 1 \u03bb 1\u2264j\u2264n j =i y jxj 1 + s j + y ix i 1 + s i \u2212 b \u2264 1 \u03bb ( 1\u2264j\u2264n j =i y jxj 1 + s j + y ix i 1 + s i + b ) \u2264 1 \u03bb (n \u2212 1 + 2 + b ) = 1 \u03bb (n + 1 + b ).(59)\nPlugging (59) back to (58), for allx i such that x i \u2212x i \u2264 1, we have\n\u2202C(\u03b8) \u2202x i |x i=x i \u2264 1 \u03bb (1 + n + 1 + b 2\u03bb )( n + 1 + b \u03bb + \u03b8 ) \u2264 C 1 b 2 ,(60)\nfor some constant C 1 > 0. Therefore\n\u2202h(x i , b) \u2202x i |x i=x i = \u2202C(\u03b8) \u2202x i |x i=x i \u03c8e \u2212 b \u2264 C 1 \u03c8 b 2 e \u2212 b .(61)\nOne can define g(x i , b) = C 1 \u03c8 b 2 e \u2212 b , which is in fact a function of only b, to satisfy condition i) in Theorem 8. Moreover, one can show that g(x i , b)db < \u221e, thus condition ii) is also satisfied. Therefore we can exchange the order of differentiation and integration.\n\n2. Attacking objective-perturbed ridge regression.\n\nWe separately show that h(x i , b) and h(\u1ef9 i , b) satisfy the conditions in Theorem 8.\n\nFirst we consider h(x i , b). By (44),\n\u2202C(\u03b8) \u2202x i = \u03b8x i + (x i\u03b8 \u2212\u1ef9 i )I X X + (\u03bb + \u00b5)I \u22121 (\u03b8 \u2212\u03b8).(62)\nWe let d = 1 in condition i). Note that x i \u2264 1, thus x i \u2264 x i + 1 \u2264 2. Since |\u1ef9 i | \u2264 1, \u03b8 \u2264 \u03c1, and \u00b5 \u2265 0, one can easily show that\n\u2202C(\u03b8) \u2202x i |x i=x i \u2264 1 \u03bb (4\u03c1 + 1)(\u03c1 + \u03b8 ).(63)\nDefine g(x i , b) = 1 \u03bb (4\u03c1 + 1)(\u03c1 + \u03b8 )\u03c8e \u2212 b . Note that g(x i , b)db = 1 \u03bb (4\u03c1 + 1)(\u03c1 + \u03b8 ) < \u221e, thus both condition i) and ii) are satisfied and we can exchange the order of differentiation and integration.\n\nNext we consider h(\u1ef9 i , b). According to (47),\n\n\n\u2202C(\u03b8)\n\n\u2202\u1ef9 i =x i X X + (\u03bb + \u00b5)I \u22121\n\n(\u03b8 \u2212 \u03b8 ).\n\nSince x i \u2264 1, \u03b8 \u2264 \u03c1 and \u00b5 \u2265 0, one can easily show that for all\u1ef9 i ,\n\u2202C(\u03b8) \u2202\u1ef9 i |\u1ef9 i=\u1ef9 i \u2264 1 \u03bb (\u03c1 + \u03b8 ).(65)\nDefine g(\u1ef9 i , b) = 1 \u03bb (\u03c1 + \u03b8 )\u03c8e \u2212 b . Note that g(\u1ef9 i , b)db = 1 \u03bb (\u03c1 + \u03b8 ) < \u221e, thus both condition i) and ii) are satisfied and we can exchange the order of differentiation and integration.\n\n3. Attacking output-perturbed logistic/ridge regression.\n\nThe proof for output-perturbed logistic regression and ridge regression are similar to the objective-perturbed learners, thus we omit them here.\n\nFor other attack goals such as label-aversion attack and label-targeting attack, the regularity condition also holds and the proof is similar.\n\nExample 1 .\n1(Parameter-Targeting Attack) If the attacker wants the output model to be close to a target model \u03b8 , the attacker can define C(\u03b8) = 1 2 \u03b8 \u2212 \u03b8 2 , a non-negative cost. Minimizing J(D) pushes the poisoned model close to \u03b8 in expectation. Example 2. (Label-Targeting Attack) If the attacker wants small prediction error on an evaluation set {z * i } i\u2208[m] , the attacker can define C\n\nLemma 2 .\n2Let M be an -differentially-private learner. Let J(D) be the attack cost, whereD \u2208 B(D, 1), then J(D) \u2265 e \u2212 J(D), (C \u2265 0), J(D) \u2265 e J(D), (C \u2264 0).\n\ne\nP M(D, b) \u2208 \u0398(a) da = e \u221e 0 P C(M(D, b)) > a da = e J(D).\n\n( 6 )\n6Thus J(D) \u2265 e \u2212 J(D). Next we consider C \u2264 0. Define \u0398(a) = {\u03b8 : C(\u03b8) < a}, \u2200a \u2264 0. Again due to M being differentially private, P M(D, b) \u2208 \u0398(a) \u2264 e P (M(D, b) \u2208 \u0398(a)) .\n\nCorollary 6 .\n6Let M be an ( , \u03b4)-differentially-private learner. Assume J(D) = 0. Then i). (0 \u2264 C \u2264C): in order to achieve J(D) \u2264 1 \u03c4 J(D) for \u03c4 \u2265 1, the attacker has to modify at least the following number of items in D.\n\n\n(a). To generate the evaluation set, we construct m = 21 evenly-spaced items in the interval [\u22121, 1], i.e., x * i = 0.1i, \u221210 \u2264 i \u2264 10 and labeled as y\n\nFigure 1 :Figure 2 :\n12(a) 1D example (b, c) 2D example We run deep-DPV with k = n for T = 5 \u00d7 10 3 iterations. In Figure 2(a)-(c), we show the poisoning trajectories for the three attack goals respectively. Each translucent point is an original training point, and the connected solid point is poisoning trajectories in 2D.\n\nFigure 3 :\n3(a)-(c) show that the attack cost J(D) decreases as k grows, indicating better attack performance. We also show the poisoning trajectory produced by deep-DPV with k = 10 inFigure 3(d)-(f). The corresponding attack costs J(D) are \u22120.60, 0.49 and 3.43 respectively. Compared to that of k = n, \u22121.79, 0.32 and 1.20, we see that poisoning only 10 items is not enough to reduce the attack cost J(D) significantly, although the attacker is making (a-c) the attack cost J(D) decreases as k grows. (d-f) the sparse attack trajectories for k = 10 effective modification.\n\nFigure 4 :\n4attack on objective and output-perturbed logistic regression.\n\nFigure 5 :\n5attack on objective and output-perturbed ridge regression.\n\nFigure 6 :\n6Attacker can reduce cost J(D) more as grows.\nDifferentially-private machine learning typically considers homogeneous learners, thus the 1D model is just a scalar.2 Since k = n, the attacker does not need to select items in step I).\nAcknowledgementThis work is supported in part by NSF 1836978, 1545481, 1704117, 1623605, 1561512, the MADLab AF Center of Excellence FA9550-18-1-0166, and the University of Wisconsin.Appendix: Regularity Condition for Differentiation-Integration ExchangeThe following theorem characterizes a sufficient condition when the order of differentiation and integration can be exchanged (restated from Theorem 2.4.3 in[4]), which is called the regularity condition.Theorem 8. Suppose h(z, b) is a differentiable function of z. If for each z, and there exist a function g(z, b) and a constant d > 0 that satisfyThen for any z, we haveNote that in condition i), the constant d could depend on z implicitly. Proposition 7 is then a straight-forward corollary of the above theorem by taking function h as h(z i , b) = C(M(D, b))\u03bd(b). The \u03bd(b) is usually chosen as the Laplace distribution \u03bd(b) = \u03c8e \u2212 b for some constant \u03c8, thus from now on we consider h(z i , b) = C(M(D, b))\u03c8e \u2212 b . We also require that the poisoned features and labels satisfy x i \u2264 1 and |\u1ef9 i | \u2264 1, to conform with the standard assumption that data must be bounded in differentially private machine learning. Next we use parameter-targeting attack as an example to illustrate that the regularity condition always holds in our attack problem.1. Attacking objective-perturbed logistic regression.In our case, the attacker only modifies the feature, thus we denote h(x i , b) = C(M(D, b))\u03c8e \u2212 b . By (37),\nData poisoning attacks against autoregressive models. Scott Alfeld, Xiaojin Zhu, Paul Barford, Thirtieth AAAI Conference on Artificial Intelligence. Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against autoregressive models. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n\nExplicit defense actions against test-set attacks. Scott Alfeld, Xiaojin Zhu, Paul Barford, Thirty-First AAAI Conference on Artificial Intelligence. Scott Alfeld, Xiaojin Zhu, and Paul Barford. Explicit defense actions against test-set attacks. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.\n\nPoisoning attacks against support vector machines. Battista Biggio, Blaine Nelson, Pavel Laskov, arXiv:1206.6389arXiv preprintBattista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. arXiv preprint arXiv:1206.6389, 2012.\n\nStatistical inference. George Casella, L Roger, Berger, 2Duxbury Pacific Grove, CAGeorge Casella and Roger L Berger. Statistical inference, volume 2. Duxbury Pacific Grove, CA, 2002.\n\nDifferentially private empirical risk minimization. Kamalika Chaudhuri, Claire Monteleoni, Anand D Sarwate, Journal of Machine Learning Research. 12Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12(Mar):1069-1109, 2011.\n\nUCI machine learning repository. Dheeru Dua, Efi Karra Taniskidou, Dheeru Dua and Efi Karra Taniskidou. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.\n\nCynthia Dwork, Differential privacy. Encyclopedia of Cryptography and Security. Cynthia Dwork. Differential privacy. Encyclopedia of Cryptography and Security, pages 338-340, 2011.\n\nCalibrating noise to sensitivity in private data analysis. Cynthia Dwork, Frank Mcsherry, Kobbi Nissim, Adam Smith, Theory of cryptography conference. SpringerCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265-284. Springer, 2006.\n\nModel inversion attacks that exploit confidence information and basic countermeasures. Matt Fredrikson, Somesh Jha, Thomas Ristenpart, Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security. the 22nd ACM SIGSAC Conference on Computer and Communications SecurityACMMatt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence infor- mation and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security, pages 1322-1333. ACM, 2015.\n\nAdversarial attacks on stochastic bandits. Kwang-Sung Jun, Lihong Li, Yuzhe Ma, Jerry Zhu, Advances in Neural Information Processing Systems. Kwang-Sung Jun, Lihong Li, Yuzhe Ma, and Jerry Zhu. Adversarial attacks on stochastic bandits. In Advances in Neural Information Processing Systems, pages 3644-3653, 2018.\n\nPrivate convex empirical risk minimization and highdimensional regression. Daniel Kifer, Adam Smith, Abhradeep Thakurta, Conference on Learning Theory. Daniel Kifer, Adam Smith, and Abhradeep Thakurta. Private convex empirical risk minimization and high- dimensional regression. In Conference on Learning Theory, pages 25-1, 2012.\n\nStronger data poisoning attacks break data sanitization defenses. Pang Wei Koh, Jacob Steinhardt, Percy Liang, arXiv:1811.00741arXiv preprintPang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization defenses. arXiv preprint arXiv:1811.00741, 2018.\n\nCertified robustness to adversarial examples with differential privacy. Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Suman Jana, Certified Robustness to Adversarial Examples with Differential Privacy. IEEEMathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to adversarial examples with differential privacy. In Certified Robustness to Adversarial Examples with Differential Privacy. IEEE, 2018.\n\nData poisoning attacks on factorization-based collaborative filtering. Bo Li, Yining Wang, Aarti Singh, Yevgeniy Vorobeychik, Advances in Neural Information Processing Systems (NIPS). Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-based collaborative filtering. In Advances in Neural Information Processing Systems (NIPS), pages 1885-1893, 2016.\n\nUsing machine teaching to identify optimal training-set attacks on machine learners. Shike Mei, Xiaojin Zhu, The 29th AAAI Conference on Artificial Intelligence. Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on machine learners. In The 29th AAAI Conference on Artificial Intelligence, 2015.\n\nTowards poisoning of deep learning algorithms with back-gradient optimization. Luis Mu\u00f1oz-Gonz\u00e1lez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, C Emil, Fabio Lupu, Roli, Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. the 10th ACM Workshop on Artificial Intelligence and SecurityACMLuis Mu\u00f1oz-Gonz\u00e1lez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 27-38. ACM, 2017.\n\nDifferentially private distributed convex optimization via objective perturbation. Erfan Nozari, Pavankumar Tallapragada, Jorge Cort\u00e9s, 2016 American Control Conference (ACC). IEEEErfan Nozari, Pavankumar Tallapragada, and Jorge Cort\u00e9s. Differentially private distributed convex optimization via objective perturbation. In 2016 American Control Conference (ACC), pages 2061-2066. IEEE, 2016.\n\nAditi Raghunathan, Jacob Steinhardt, Percy Liang, arXiv:1801.09344Certified defenses against adversarial examples. arXiv preprintAditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018.\n\nCertified defenses for data poisoning attacks. Jacob Steinhardt, Pang Wei W Koh, Percy S Liang, Advances in Neural Information Processing Systems (NIPS). Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. In Advances in Neural Information Processing Systems (NIPS), pages 3517-3529, 2017.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, arXiv:1312.6199Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprintChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\nStealing machine learning models via prediction apis. Florian Tram\u00e8r, Fan Zhang, Ari Juels, K Michael, Thomas Reiter, Ristenpart, 25th USENIX Security Symposium. Florian Tram\u00e8r, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine learning models via prediction apis. In 25th USENIX Security Symposium, pages 601-618, 2016.\n\nIs feature selection secure against training data poisoning. Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, Fabio Roli, International Conference on Machine Learning. Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. Is feature selection secure against training data poisoning? In International Conference on Machine Learning, pages 1689-1698, 2015.\n\nXuezhou Zhang, Xiaojin Zhu, arXiv:1903.01666Online data poisoning attack. arXiv preprintXuezhou Zhang and Xiaojin Zhu. Online data poisoning attack. arXiv preprint arXiv:1903.01666, 2019.\n\nEfficient label contamination attacks against black-box learning models. Mengchen Zhao, Bo An, Wei Gao, Teng Zhang, IJCAI. Mengchen Zhao, Bo An, Wei Gao, and Teng Zhang. Efficient label contamination attacks against black-box learning models. In IJCAI, pages 3945-3951, 2017.\n", "annotations": {"author": "[{\"end\":155,\"start\":80},{\"end\":234,\"start\":156},{\"end\":312,\"start\":235}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":86},{\"end\":167,\"start\":164},{\"end\":245,\"start\":242}]", "author_first_name": "[{\"end\":85,\"start\":80},{\"end\":163,\"start\":156},{\"end\":241,\"start\":235}]", "author_affiliation": "[{\"end\":154,\"start\":90},{\"end\":233,\"start\":169},{\"end\":311,\"start\":247}]", "title": "[{\"end\":77,\"start\":1},{\"end\":389,\"start\":313}]", "venue": null, "abstract": "[{\"end\":1094,\"start\":391}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1518,\"start\":1514},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1622,\"start\":1619},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1661,\"start\":1657},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1693,\"start\":1690},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1696,\"start\":1693},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1699,\"start\":1696},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1701,\"start\":1699},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1704,\"start\":1701},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1707,\"start\":1704},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1710,\"start\":1707},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1713,\"start\":1710},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1716,\"start\":1713},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1719,\"start\":1716},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2170,\"start\":2167},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2172,\"start\":2170},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2661,\"start\":2657},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2664,\"start\":2661},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2666,\"start\":2664},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3404,\"start\":3400},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3432,\"start\":3429},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4852,\"start\":4849},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4855,\"start\":4852},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4858,\"start\":4855},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4979,\"start\":4976},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6192,\"start\":6189},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6195,\"start\":6192},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6198,\"start\":6195},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6380,\"start\":6377},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7351,\"start\":7348},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7839,\"start\":7835},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13336,\"start\":13332},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17985,\"start\":17984},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18337,\"start\":18333},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23693,\"start\":23689},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23727,\"start\":23724},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24754,\"start\":24753},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29124,\"start\":29121},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":38263,\"start\":38262}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36261,\"start\":35866},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36420,\"start\":36262},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36481,\"start\":36421},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36660,\"start\":36482},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36884,\"start\":36661},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37038,\"start\":36885},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37364,\"start\":37039},{\"attributes\":{\"id\":\"fig_7\"},\"end\":37939,\"start\":37365},{\"attributes\":{\"id\":\"fig_8\"},\"end\":38014,\"start\":37940},{\"attributes\":{\"id\":\"fig_9\"},\"end\":38086,\"start\":38015},{\"attributes\":{\"id\":\"fig_10\"},\"end\":38144,\"start\":38087}]", "paragraph": "[{\"end\":1662,\"start\":1110},{\"end\":3043,\"start\":1664},{\"end\":3861,\"start\":3045},{\"end\":4234,\"start\":3879},{\"end\":4356,\"start\":4236},{\"end\":4495,\"start\":4399},{\"end\":5052,\"start\":4497},{\"end\":5112,\"start\":5054},{\"end\":5343,\"start\":5114},{\"end\":5741,\"start\":5345},{\"end\":6100,\"start\":5743},{\"end\":6382,\"start\":6138},{\"end\":6554,\"start\":6475},{\"end\":6666,\"start\":6556},{\"end\":7453,\"start\":6739},{\"end\":8138,\"start\":7501},{\"end\":8409,\"start\":8140},{\"end\":8668,\"start\":8464},{\"end\":8842,\"start\":8670},{\"end\":8951,\"start\":8888},{\"end\":9039,\"start\":8978},{\"end\":9642,\"start\":9199},{\"end\":9813,\"start\":9644},{\"end\":10171,\"start\":10023},{\"end\":10630,\"start\":10409},{\"end\":10828,\"start\":10791},{\"end\":10952,\"start\":10904},{\"end\":11063,\"start\":11024},{\"end\":11170,\"start\":11112},{\"end\":11294,\"start\":11227},{\"end\":11371,\"start\":11330},{\"end\":11442,\"start\":11430},{\"end\":11539,\"start\":11490},{\"end\":11720,\"start\":11597},{\"end\":11964,\"start\":11785},{\"end\":12084,\"start\":12014},{\"end\":12519,\"start\":12458},{\"end\":12807,\"start\":12693},{\"end\":13008,\"start\":12935},{\"end\":13317,\"start\":13251},{\"end\":13649,\"start\":13319},{\"end\":14195,\"start\":13696},{\"end\":14344,\"start\":14231},{\"end\":14404,\"start\":14346},{\"end\":14464,\"start\":14406},{\"end\":14924,\"start\":14466},{\"end\":15135,\"start\":14972},{\"end\":15315,\"start\":15299},{\"end\":15499,\"start\":15368},{\"end\":15610,\"start\":15581},{\"end\":15914,\"start\":15612},{\"end\":16020,\"start\":15965},{\"end\":16383,\"start\":16076},{\"end\":16609,\"start\":16420},{\"end\":16862,\"start\":16670},{\"end\":17032,\"start\":16864},{\"end\":17234,\"start\":17107},{\"end\":17328,\"start\":17275},{\"end\":17488,\"start\":17407},{\"end\":17539,\"start\":17536},{\"end\":17695,\"start\":17591},{\"end\":17901,\"start\":17827},{\"end\":18064,\"start\":17903},{\"end\":18452,\"start\":18120},{\"end\":18592,\"start\":18493},{\"end\":18751,\"start\":18697},{\"end\":18951,\"start\":18927},{\"end\":19013,\"start\":18987},{\"end\":19052,\"start\":19015},{\"end\":19236,\"start\":19117},{\"end\":19284,\"start\":19258},{\"end\":19323,\"start\":19286},{\"end\":19449,\"start\":19372},{\"end\":19547,\"start\":19497},{\"end\":19825,\"start\":19602},{\"end\":19941,\"start\":19827},{\"end\":20087,\"start\":20017},{\"end\":20310,\"start\":20203},{\"end\":20418,\"start\":20365},{\"end\":20605,\"start\":20539},{\"end\":21121,\"start\":20761},{\"end\":21417,\"start\":21140},{\"end\":22035,\"start\":21447},{\"end\":23366,\"start\":22080},{\"end\":23592,\"start\":23368},{\"end\":24181,\"start\":23608},{\"end\":24581,\"start\":24219},{\"end\":24685,\"start\":24684},{\"end\":25118,\"start\":24687},{\"end\":25466,\"start\":25158},{\"end\":25720,\"start\":25503},{\"end\":25895,\"start\":25771},{\"end\":26259,\"start\":25944},{\"end\":26481,\"start\":26261},{\"end\":27042,\"start\":26483},{\"end\":27633,\"start\":27044},{\"end\":27897,\"start\":27635},{\"end\":28131,\"start\":27899},{\"end\":28799,\"start\":28180},{\"end\":30750,\"start\":28848},{\"end\":31890,\"start\":30785},{\"end\":32351,\"start\":31931},{\"end\":33191,\"start\":32382},{\"end\":33417,\"start\":33193},{\"end\":33773,\"start\":33742},{\"end\":34001,\"start\":33931},{\"end\":34119,\"start\":34083},{\"end\":34477,\"start\":34199},{\"end\":34529,\"start\":34479},{\"end\":34617,\"start\":34531},{\"end\":34657,\"start\":34619},{\"end\":34855,\"start\":34722},{\"end\":35114,\"start\":34904},{\"end\":35163,\"start\":35116},{\"end\":35200,\"start\":35173},{\"end\":35211,\"start\":35202},{\"end\":35282,\"start\":35213},{\"end\":35517,\"start\":35323},{\"end\":35575,\"start\":35519},{\"end\":35721,\"start\":35577},{\"end\":35865,\"start\":35723}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":4398,\"start\":4357},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6137,\"start\":6101},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6474,\"start\":6383},{\"attributes\":{\"id\":\"formula_3\"},\"end\":6738,\"start\":6667},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8463,\"start\":8410},{\"attributes\":{\"id\":\"formula_6\"},\"end\":8887,\"start\":8843},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8977,\"start\":8952},{\"attributes\":{\"id\":\"formula_9\"},\"end\":9198,\"start\":9040},{\"attributes\":{\"id\":\"formula_10\"},\"end\":10022,\"start\":9814},{\"attributes\":{\"id\":\"formula_11\"},\"end\":10408,\"start\":10172},{\"attributes\":{\"id\":\"formula_12\"},\"end\":10790,\"start\":10631},{\"attributes\":{\"id\":\"formula_13\"},\"end\":10903,\"start\":10829},{\"attributes\":{\"id\":\"formula_14\"},\"end\":11023,\"start\":10953},{\"attributes\":{\"id\":\"formula_15\"},\"end\":11111,\"start\":11064},{\"attributes\":{\"id\":\"formula_16\"},\"end\":11226,\"start\":11171},{\"attributes\":{\"id\":\"formula_17\"},\"end\":11329,\"start\":11295},{\"attributes\":{\"id\":\"formula_18\"},\"end\":11429,\"start\":11372},{\"attributes\":{\"id\":\"formula_19\"},\"end\":11489,\"start\":11443},{\"attributes\":{\"id\":\"formula_20\"},\"end\":11596,\"start\":11540},{\"attributes\":{\"id\":\"formula_21\"},\"end\":11784,\"start\":11721},{\"attributes\":{\"id\":\"formula_22\"},\"end\":12013,\"start\":11965},{\"attributes\":{\"id\":\"formula_23\"},\"end\":12246,\"start\":12085},{\"attributes\":{\"id\":\"formula_24\"},\"end\":12457,\"start\":12246},{\"attributes\":{\"id\":\"formula_25\"},\"end\":12692,\"start\":12520},{\"attributes\":{\"id\":\"formula_26\"},\"end\":12934,\"start\":12808},{\"attributes\":{\"id\":\"formula_27\"},\"end\":13250,\"start\":13009},{\"attributes\":{\"id\":\"formula_28\"},\"end\":14230,\"start\":14196},{\"attributes\":{\"id\":\"formula_29\"},\"end\":15298,\"start\":15136},{\"attributes\":{\"id\":\"formula_30\"},\"end\":15367,\"start\":15316},{\"attributes\":{\"id\":\"formula_31\"},\"end\":15565,\"start\":15500},{\"attributes\":{\"id\":\"formula_32\"},\"end\":15580,\"start\":15565},{\"attributes\":{\"id\":\"formula_33\"},\"end\":16075,\"start\":16021},{\"attributes\":{\"id\":\"formula_34\"},\"end\":16419,\"start\":16384},{\"attributes\":{\"id\":\"formula_35\"},\"end\":16669,\"start\":16610},{\"attributes\":{\"id\":\"formula_36\"},\"end\":17106,\"start\":17033},{\"attributes\":{\"id\":\"formula_37\"},\"end\":17274,\"start\":17235},{\"attributes\":{\"id\":\"formula_38\"},\"end\":17406,\"start\":17329},{\"attributes\":{\"id\":\"formula_39\"},\"end\":17535,\"start\":17489},{\"attributes\":{\"id\":\"formula_40\"},\"end\":17590,\"start\":17540},{\"attributes\":{\"id\":\"formula_41\"},\"end\":17826,\"start\":17696},{\"attributes\":{\"id\":\"formula_42\"},\"end\":18119,\"start\":18065},{\"attributes\":{\"id\":\"formula_43\"},\"end\":18492,\"start\":18453},{\"attributes\":{\"id\":\"formula_44\"},\"end\":18696,\"start\":18593},{\"attributes\":{\"id\":\"formula_45\"},\"end\":18926,\"start\":18752},{\"attributes\":{\"id\":\"formula_46\"},\"end\":18986,\"start\":18952},{\"attributes\":{\"id\":\"formula_48\"},\"end\":19116,\"start\":19053},{\"attributes\":{\"id\":\"formula_49\"},\"end\":19257,\"start\":19237},{\"attributes\":{\"id\":\"formula_51\"},\"end\":19371,\"start\":19324},{\"attributes\":{\"id\":\"formula_52\"},\"end\":19601,\"start\":19548},{\"attributes\":{\"id\":\"formula_53\"},\"end\":20016,\"start\":19942},{\"attributes\":{\"id\":\"formula_54\"},\"end\":20202,\"start\":20088},{\"attributes\":{\"id\":\"formula_55\"},\"end\":20364,\"start\":20311},{\"attributes\":{\"id\":\"formula_56\"},\"end\":20538,\"start\":20419},{\"attributes\":{\"id\":\"formula_57\"},\"end\":20681,\"start\":20606},{\"attributes\":{\"id\":\"formula_58\"},\"end\":20728,\"start\":20681},{\"attributes\":{\"id\":\"formula_59\"},\"end\":21139,\"start\":21122},{\"attributes\":{\"id\":\"formula_60\"},\"end\":22079,\"start\":22036},{\"attributes\":{\"id\":\"formula_61\"},\"end\":24683,\"start\":24582},{\"attributes\":{\"id\":\"formula_62\"},\"end\":25502,\"start\":25467},{\"attributes\":{\"id\":\"formula_63\"},\"end\":25770,\"start\":25721},{\"attributes\":{\"id\":\"formula_64\"},\"end\":25943,\"start\":25896},{\"attributes\":{\"id\":\"formula_65\"},\"end\":30784,\"start\":30751},{\"attributes\":{\"id\":\"formula_66\"},\"end\":33741,\"start\":33418},{\"attributes\":{\"id\":\"formula_67\"},\"end\":33930,\"start\":33774},{\"attributes\":{\"id\":\"formula_68\"},\"end\":34082,\"start\":34002},{\"attributes\":{\"id\":\"formula_69\"},\"end\":34198,\"start\":34120},{\"attributes\":{\"id\":\"formula_70\"},\"end\":34721,\"start\":34658},{\"attributes\":{\"id\":\"formula_71\"},\"end\":34903,\"start\":34856},{\"attributes\":{\"id\":\"formula_73\"},\"end\":35322,\"start\":35283}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1108,\"start\":1096},{\"attributes\":{\"n\":\"2\"},\"end\":3877,\"start\":3864},{\"attributes\":{\"n\":\"3\"},\"end\":7499,\"start\":7456},{\"attributes\":{\"n\":\"4\"},\"end\":13694,\"start\":13652},{\"attributes\":{\"n\":\"4.1\"},\"end\":14970,\"start\":14927},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":15963,\"start\":15917},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":19495,\"start\":19452},{\"attributes\":{\"n\":\"4.2\"},\"end\":20759,\"start\":20730},{\"attributes\":{\"n\":\"4.3\"},\"end\":21445,\"start\":21420},{\"attributes\":{\"n\":\"5\"},\"end\":23606,\"start\":23595},{\"attributes\":{\"n\":\"5.1\"},\"end\":24217,\"start\":24184},{\"attributes\":{\"n\":\"5.2\"},\"end\":25156,\"start\":25121},{\"attributes\":{\"n\":\"5.3\"},\"end\":28178,\"start\":28134},{\"attributes\":{\"n\":\"5.4\"},\"end\":28846,\"start\":28802},{\"attributes\":{\"n\":\"5.5\"},\"end\":31929,\"start\":31893},{\"attributes\":{\"n\":\"6\"},\"end\":32380,\"start\":32354},{\"end\":35171,\"start\":35166},{\"end\":35878,\"start\":35867},{\"end\":36272,\"start\":36263},{\"end\":36423,\"start\":36422},{\"end\":36488,\"start\":36483},{\"end\":36675,\"start\":36662},{\"end\":37060,\"start\":37040},{\"end\":37376,\"start\":37366},{\"end\":37951,\"start\":37941},{\"end\":38026,\"start\":38016},{\"end\":38098,\"start\":38088}]", "table": null, "figure_caption": "[{\"end\":36261,\"start\":35880},{\"end\":36420,\"start\":36274},{\"end\":36481,\"start\":36424},{\"end\":36660,\"start\":36490},{\"end\":36884,\"start\":36677},{\"end\":37038,\"start\":36887},{\"end\":37364,\"start\":37063},{\"end\":37939,\"start\":37378},{\"end\":38014,\"start\":37953},{\"end\":38086,\"start\":38028},{\"end\":38144,\"start\":38100}]", "figure_ref": "[{\"end\":16296,\"start\":16289},{\"end\":17900,\"start\":17894},{\"end\":22261,\"start\":22253},{\"end\":22317,\"start\":22309},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24580,\"start\":24572},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24851,\"start\":24843},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25325,\"start\":25317},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25401,\"start\":25399},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25684,\"start\":25676},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27926,\"start\":27918},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28798,\"start\":28790},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":30067,\"start\":30059},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":30953,\"start\":30945},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":31338,\"start\":31330},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32170,\"start\":32162}]", "bib_author_first_name": "[{\"end\":39855,\"start\":39850},{\"end\":39871,\"start\":39864},{\"end\":39881,\"start\":39877},{\"end\":40164,\"start\":40159},{\"end\":40180,\"start\":40173},{\"end\":40190,\"start\":40186},{\"end\":40479,\"start\":40471},{\"end\":40494,\"start\":40488},{\"end\":40508,\"start\":40503},{\"end\":40715,\"start\":40709},{\"end\":40726,\"start\":40725},{\"end\":40930,\"start\":40922},{\"end\":40948,\"start\":40942},{\"end\":40968,\"start\":40961},{\"end\":41233,\"start\":41227},{\"end\":41380,\"start\":41373},{\"end\":41621,\"start\":41614},{\"end\":41634,\"start\":41629},{\"end\":41650,\"start\":41645},{\"end\":41663,\"start\":41659},{\"end\":41995,\"start\":41991},{\"end\":42014,\"start\":42008},{\"end\":42026,\"start\":42020},{\"end\":42512,\"start\":42502},{\"end\":42524,\"start\":42518},{\"end\":42534,\"start\":42529},{\"end\":42544,\"start\":42539},{\"end\":42855,\"start\":42849},{\"end\":42867,\"start\":42863},{\"end\":42884,\"start\":42875},{\"end\":43176,\"start\":43172},{\"end\":43191,\"start\":43186},{\"end\":43209,\"start\":43204},{\"end\":43481,\"start\":43474},{\"end\":43499,\"start\":43491},{\"end\":43517,\"start\":43511},{\"end\":43534,\"start\":43528},{\"end\":43545,\"start\":43540},{\"end\":43943,\"start\":43941},{\"end\":43954,\"start\":43948},{\"end\":43966,\"start\":43961},{\"end\":43982,\"start\":43974},{\"end\":44359,\"start\":44354},{\"end\":44372,\"start\":44365},{\"end\":44689,\"start\":44685},{\"end\":44714,\"start\":44706},{\"end\":44728,\"start\":44723},{\"end\":44745,\"start\":44739},{\"end\":44760,\"start\":44755},{\"end\":44776,\"start\":44775},{\"end\":44788,\"start\":44783},{\"end\":45335,\"start\":45330},{\"end\":45354,\"start\":45344},{\"end\":45374,\"start\":45369},{\"end\":45645,\"start\":45640},{\"end\":45664,\"start\":45659},{\"end\":45682,\"start\":45677},{\"end\":45964,\"start\":45959},{\"end\":45981,\"start\":45977},{\"end\":46000,\"start\":45993},{\"end\":46260,\"start\":46251},{\"end\":46278,\"start\":46270},{\"end\":46292,\"start\":46288},{\"end\":46308,\"start\":46304},{\"end\":46688,\"start\":46681},{\"end\":46700,\"start\":46697},{\"end\":46711,\"start\":46708},{\"end\":46720,\"start\":46719},{\"end\":46736,\"start\":46730},{\"end\":47045,\"start\":47040},{\"end\":47060,\"start\":47052},{\"end\":47074,\"start\":47069},{\"end\":47089,\"start\":47082},{\"end\":47105,\"start\":47098},{\"end\":47119,\"start\":47114},{\"end\":47403,\"start\":47396},{\"end\":47418,\"start\":47411},{\"end\":47666,\"start\":47658},{\"end\":47675,\"start\":47673},{\"end\":47683,\"start\":47680},{\"end\":47693,\"start\":47689}]", "bib_author_last_name": "[{\"end\":39862,\"start\":39856},{\"end\":39875,\"start\":39872},{\"end\":39889,\"start\":39882},{\"end\":40171,\"start\":40165},{\"end\":40184,\"start\":40181},{\"end\":40198,\"start\":40191},{\"end\":40486,\"start\":40480},{\"end\":40501,\"start\":40495},{\"end\":40515,\"start\":40509},{\"end\":40723,\"start\":40716},{\"end\":40732,\"start\":40727},{\"end\":40740,\"start\":40734},{\"end\":40940,\"start\":40931},{\"end\":40959,\"start\":40949},{\"end\":40976,\"start\":40969},{\"end\":41237,\"start\":41234},{\"end\":41259,\"start\":41239},{\"end\":41386,\"start\":41381},{\"end\":41627,\"start\":41622},{\"end\":41643,\"start\":41635},{\"end\":41657,\"start\":41651},{\"end\":41669,\"start\":41664},{\"end\":42006,\"start\":41996},{\"end\":42018,\"start\":42015},{\"end\":42037,\"start\":42027},{\"end\":42516,\"start\":42513},{\"end\":42527,\"start\":42525},{\"end\":42537,\"start\":42535},{\"end\":42548,\"start\":42545},{\"end\":42861,\"start\":42856},{\"end\":42873,\"start\":42868},{\"end\":42893,\"start\":42885},{\"end\":43184,\"start\":43177},{\"end\":43202,\"start\":43192},{\"end\":43215,\"start\":43210},{\"end\":43489,\"start\":43482},{\"end\":43509,\"start\":43500},{\"end\":43526,\"start\":43518},{\"end\":43538,\"start\":43535},{\"end\":43550,\"start\":43546},{\"end\":43946,\"start\":43944},{\"end\":43959,\"start\":43955},{\"end\":43972,\"start\":43967},{\"end\":43994,\"start\":43983},{\"end\":44363,\"start\":44360},{\"end\":44376,\"start\":44373},{\"end\":44704,\"start\":44690},{\"end\":44721,\"start\":44715},{\"end\":44737,\"start\":44729},{\"end\":44753,\"start\":44746},{\"end\":44773,\"start\":44761},{\"end\":44781,\"start\":44777},{\"end\":44793,\"start\":44789},{\"end\":44799,\"start\":44795},{\"end\":45342,\"start\":45336},{\"end\":45367,\"start\":45355},{\"end\":45381,\"start\":45375},{\"end\":45657,\"start\":45646},{\"end\":45675,\"start\":45665},{\"end\":45688,\"start\":45683},{\"end\":45975,\"start\":45965},{\"end\":45991,\"start\":45982},{\"end\":46006,\"start\":46001},{\"end\":46268,\"start\":46261},{\"end\":46286,\"start\":46279},{\"end\":46302,\"start\":46293},{\"end\":46314,\"start\":46309},{\"end\":46695,\"start\":46689},{\"end\":46706,\"start\":46701},{\"end\":46717,\"start\":46712},{\"end\":46728,\"start\":46721},{\"end\":46743,\"start\":46737},{\"end\":46755,\"start\":46745},{\"end\":47050,\"start\":47046},{\"end\":47067,\"start\":47061},{\"end\":47080,\"start\":47075},{\"end\":47096,\"start\":47090},{\"end\":47112,\"start\":47106},{\"end\":47124,\"start\":47120},{\"end\":47409,\"start\":47404},{\"end\":47422,\"start\":47419},{\"end\":47671,\"start\":47667},{\"end\":47678,\"start\":47676},{\"end\":47687,\"start\":47684},{\"end\":47699,\"start\":47694}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11538109},\"end\":40106,\"start\":39796},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14217855},\"end\":40418,\"start\":40108},{\"attributes\":{\"doi\":\"arXiv:1206.6389\",\"id\":\"b2\"},\"end\":40684,\"start\":40420},{\"attributes\":{\"id\":\"b3\"},\"end\":40868,\"start\":40686},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1578541},\"end\":41192,\"start\":40870},{\"attributes\":{\"id\":\"b5\"},\"end\":41371,\"start\":41194},{\"attributes\":{\"id\":\"b6\"},\"end\":41553,\"start\":41373},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2468323},\"end\":41902,\"start\":41555},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":207229839},\"end\":42457,\"start\":41904},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":53104043},\"end\":42772,\"start\":42459},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14305556},\"end\":43104,\"start\":42774},{\"attributes\":{\"doi\":\"arXiv:1811.00741\",\"id\":\"b11\"},\"end\":43400,\"start\":43106},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49431481},\"end\":43868,\"start\":43402},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16687466},\"end\":44267,\"start\":43870},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9746839},\"end\":44604,\"start\":44269},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":12424035},\"end\":45245,\"start\":44606},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":19663460},\"end\":45638,\"start\":45247},{\"attributes\":{\"doi\":\"arXiv:1801.09344\",\"id\":\"b17\"},\"end\":45910,\"start\":45640},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":35426171},\"end\":46249,\"start\":45912},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b19\"},\"end\":46625,\"start\":46251},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2984526},\"end\":46977,\"start\":46627},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5077922},\"end\":47394,\"start\":46979},{\"attributes\":{\"doi\":\"arXiv:1903.01666\",\"id\":\"b22\"},\"end\":47583,\"start\":47396},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":27887563},\"end\":47860,\"start\":47585}]", "bib_title": "[{\"end\":39848,\"start\":39796},{\"end\":40157,\"start\":40108},{\"end\":40920,\"start\":40870},{\"end\":41612,\"start\":41555},{\"end\":41989,\"start\":41904},{\"end\":42500,\"start\":42459},{\"end\":42847,\"start\":42774},{\"end\":43472,\"start\":43402},{\"end\":43939,\"start\":43870},{\"end\":44352,\"start\":44269},{\"end\":44683,\"start\":44606},{\"end\":45328,\"start\":45247},{\"end\":45957,\"start\":45912},{\"end\":46679,\"start\":46627},{\"end\":47038,\"start\":46979},{\"end\":47656,\"start\":47585}]", "bib_author": "[{\"end\":39864,\"start\":39850},{\"end\":39877,\"start\":39864},{\"end\":39891,\"start\":39877},{\"end\":40173,\"start\":40159},{\"end\":40186,\"start\":40173},{\"end\":40200,\"start\":40186},{\"end\":40488,\"start\":40471},{\"end\":40503,\"start\":40488},{\"end\":40517,\"start\":40503},{\"end\":40725,\"start\":40709},{\"end\":40734,\"start\":40725},{\"end\":40742,\"start\":40734},{\"end\":40942,\"start\":40922},{\"end\":40961,\"start\":40942},{\"end\":40978,\"start\":40961},{\"end\":41239,\"start\":41227},{\"end\":41261,\"start\":41239},{\"end\":41388,\"start\":41373},{\"end\":41629,\"start\":41614},{\"end\":41645,\"start\":41629},{\"end\":41659,\"start\":41645},{\"end\":41671,\"start\":41659},{\"end\":42008,\"start\":41991},{\"end\":42020,\"start\":42008},{\"end\":42039,\"start\":42020},{\"end\":42518,\"start\":42502},{\"end\":42529,\"start\":42518},{\"end\":42539,\"start\":42529},{\"end\":42550,\"start\":42539},{\"end\":42863,\"start\":42849},{\"end\":42875,\"start\":42863},{\"end\":42895,\"start\":42875},{\"end\":43186,\"start\":43172},{\"end\":43204,\"start\":43186},{\"end\":43217,\"start\":43204},{\"end\":43491,\"start\":43474},{\"end\":43511,\"start\":43491},{\"end\":43528,\"start\":43511},{\"end\":43540,\"start\":43528},{\"end\":43552,\"start\":43540},{\"end\":43948,\"start\":43941},{\"end\":43961,\"start\":43948},{\"end\":43974,\"start\":43961},{\"end\":43996,\"start\":43974},{\"end\":44365,\"start\":44354},{\"end\":44378,\"start\":44365},{\"end\":44706,\"start\":44685},{\"end\":44723,\"start\":44706},{\"end\":44739,\"start\":44723},{\"end\":44755,\"start\":44739},{\"end\":44775,\"start\":44755},{\"end\":44783,\"start\":44775},{\"end\":44795,\"start\":44783},{\"end\":44801,\"start\":44795},{\"end\":45344,\"start\":45330},{\"end\":45369,\"start\":45344},{\"end\":45383,\"start\":45369},{\"end\":45659,\"start\":45640},{\"end\":45677,\"start\":45659},{\"end\":45690,\"start\":45677},{\"end\":45977,\"start\":45959},{\"end\":45993,\"start\":45977},{\"end\":46008,\"start\":45993},{\"end\":46270,\"start\":46251},{\"end\":46288,\"start\":46270},{\"end\":46304,\"start\":46288},{\"end\":46316,\"start\":46304},{\"end\":46697,\"start\":46681},{\"end\":46708,\"start\":46697},{\"end\":46719,\"start\":46708},{\"end\":46730,\"start\":46719},{\"end\":46745,\"start\":46730},{\"end\":46757,\"start\":46745},{\"end\":47052,\"start\":47040},{\"end\":47069,\"start\":47052},{\"end\":47082,\"start\":47069},{\"end\":47098,\"start\":47082},{\"end\":47114,\"start\":47098},{\"end\":47126,\"start\":47114},{\"end\":47411,\"start\":47396},{\"end\":47424,\"start\":47411},{\"end\":47673,\"start\":47658},{\"end\":47680,\"start\":47673},{\"end\":47689,\"start\":47680},{\"end\":47701,\"start\":47689}]", "bib_venue": "[{\"end\":42196,\"start\":42126},{\"end\":44940,\"start\":44879},{\"end\":39943,\"start\":39891},{\"end\":40255,\"start\":40200},{\"end\":40469,\"start\":40420},{\"end\":40707,\"start\":40686},{\"end\":41014,\"start\":40978},{\"end\":41225,\"start\":41194},{\"end\":41451,\"start\":41388},{\"end\":41704,\"start\":41671},{\"end\":42124,\"start\":42039},{\"end\":42599,\"start\":42550},{\"end\":42924,\"start\":42895},{\"end\":43170,\"start\":43106},{\"end\":43622,\"start\":43552},{\"end\":44052,\"start\":43996},{\"end\":44429,\"start\":44378},{\"end\":44877,\"start\":44801},{\"end\":45421,\"start\":45383},{\"end\":45753,\"start\":45706},{\"end\":46064,\"start\":46008},{\"end\":46418,\"start\":46331},{\"end\":46787,\"start\":46757},{\"end\":47170,\"start\":47126},{\"end\":47468,\"start\":47440},{\"end\":47706,\"start\":47701}]"}}}, "year": 2023, "month": 12, "day": 17}