{"id": 246288026, "updated": "2022-08-30 17:55:32.245", "metadata": {"title": "Store-n-Learn: Classification and Clustering with Hyperdimensional Computing across Flash Hierarchy", "authors": "[{\"first\":\"Saransh\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Behnam\",\"last\":\"Khaleghi\",\"middle\":[]},{\"first\":\"Sahand\",\"last\":\"Salamat\",\"middle\":[]},{\"first\":\"Justin\",\"last\":\"Morris\",\"middle\":[]},{\"first\":\"Ranganathan\",\"last\":\"Ramkumar\",\"middle\":[]},{\"first\":\"Jeffrey\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Aniket\",\"last\":\"Tiwari\",\"middle\":[]},{\"first\":\"Jaeyoung\",\"last\":\"Kang\",\"middle\":[]},{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]},{\"first\":\"Baris\",\"last\":\"Aksanli\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[\"\u0160imuni\u0107\"]}]", "venue": null, "journal": "ACM Transactions on Embedded Computing Systems (TECS)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Processing large amounts of data, especially in learning algorithms, poses a challenge for current embedded computing systems. Hyperdimensional (HD) computing (HDC) is a brain-inspired computing paradigm that works with high-dimensional vectors called hypervectors. HDC replaces several complex learning computations with bitwise and simpler arithmetic operations at the expense of an increased amount of data due to mapping the data into high-dimensional space. These hypervectors, more often than not, cannot be stored in memory, resulting in long data transfers from storage. In this article, we propose Store-n-Learn, an in-storage computing solution that performs HDC classification and clustering by implementing encoding, training, retraining, and inference across the flash hierarchy. To hide the latency of training and enable efficient computation, we introduce the concept of batching in HDC. We also present on-chip acceleration for HDC encoding in flash planes. This enables us to exploit the high parallelism provided by the flash hierarchy and encode multiple data points in parallel in both batched and non-batched fashion. Store-n-Learn also implements a single top-level FPGA accelerator with novel implementations for HDC classification training, retraining, inference, and clustering on the encoded data. Our evaluation over 10 popular datasets shows that Store-n-Learn is on average 222\u00d7 (543\u00d7) faster than CPU and 10.6\u00d7 (7.3\u00d7) faster than the state-of-the-art in-storage computing solution, INSIDER for HDC classification (clustering).", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tecs/GuptaKSMRYTKIAR22", "doi": "10.1145/3503541"}}, "content": {"source": {"pdf_hash": "c9a74e17d76b30a46cae42561629eb0dd3fa71c4", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3503541", "status": "BRONZE"}}, "grobid": {"id": "75f036a6137218d635364c7da1f7fd7c258e478d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c9a74e17d76b30a46cae42561629eb0dd3fa71c4.txt", "contents": "\nArticle 22\nJuly 2022\n\nSaransh Gupta s:sgupta@ucsd.edu \nBehnam Khaleghi bkhalegh@ucsd.edu \nSahand Salamat sasalama@ucsd.edu \nJustin Morris justinmorris@ucsd.edu \nRanganathan Ramkumar rramkuma@ucsd.edu \nJeffrey Yu \nAniket Tiwari artiwari@ucsd.edu \nJaeyoung Kang j5kang@ucsd.edu \nB Gupta \nS Khaleghi \nR Salamat \nJ Ramkumar \nA Yu \nJ Tiwari \nT \u0160 Kang \nRosing \nSaransh Gupta \nBehnam Khaleghi \nSahand Salamat \nJustin Morris \nRanganathan Ramkumar \nJeffrey Yu \nAniket Tiwari \nJaeyoung Kang \nMohsen Imani m.imani@uci.edu \nBaris Aksanli baksanli@sdsu.edu. \nTajana \u0160imuni\u0107 tajana@ucsd.edu \nRosing \n\nUniversity of California\nSan Diego\n\n\nUniversity of California\nSan Diego\n\n\nSan Diego MOHSEN IMANI\nSan Diego State University\nUniversity of California\nUniversity of California\nSan Diego\n\n\nIrvine BARIS AKSANLI\nTAJANA \u0160IMUNI\u0106 ROSING\nUniversity of California\nSan Diego State University\nUniversity of California\nSan Diego\n\n\nUniversity of California\nSan Diego, 9500 Gilman Dr, La Jolla92093CaliforniaUSA\n\n\nUniversity of California\nSan Diego, 9500 Gilman Dr, La Jolla, Cali-fornia92093USA\n\n\nUniversity of California\nSan Diego, 9500 Gilman Dr, La Jolla92093CaliforniaUSA\n\n\nSan Diego State University\n5500 Campanile Dr, San Diego92182CaliforniaUSA\n\nArticle 22\n\nACM Transactions on Embedded Computing Systems\n213July 202210.1145/3503541Publication date: July 2022.22 Authors' addresses: S. This work is licensed under a Creative Commons Attribution International 4.0 License. 1539-9087/2022/07-ART22 $15.00 22:2 S. Gupta et al.CCS Concepts: \u2022 Information systems \u2192 Flash memory\u2022 Hardware \u2192 Biology-related information processingEmerging architecturesAdditional Key Words and Phrases: Hyperdimensional computing, in-storage computing, classification, clustering 2022 Store-n-Learn: Clas- sification and Clustering with Hyperdimensional Computing across Flash Hierarchy ACM Trans Embedd\nProcessing large amounts of data, especially in learning algorithms, poses a challenge for current embedded computing systems. Hyperdimensional (HD) computing (HDC) is a brain-inspired computing paradigm that works with high-dimensional vectors called hypervectors. HDC replaces several complex learning computations with bitwise and simpler arithmetic operations at the expense of an increased amount of data due to mapping the data into high-dimensional space. These hypervectors, more often than not, cannot be stored in memory, resulting in long data transfers from storage. In this article, we propose Store-n-Learn, an in-storage computing solution that performs HDC classification and clustering by implementing encoding, training, retraining, and inference across the flash hierarchy. To hide the latency of training and enable efficient computation, we introduce the concept of batching in HDC. We also present on-chip acceleration for HDC encoding in flash planes. This enables us to exploit the high parallelism provided by the flash hierarchy and encode multiple data points in parallel in both batched and non-batched fashion. Store-n-Learn also implements a single top-level FPGA accelerator with novel implementations for HDC classification training, retraining, inference, and clustering on the encoded data. Our evaluation over 10 popular datasets shows that Store-n-Learn is on average 222\u00d7 (543\u00d7) faster than CPU and 10.6\u00d7 (7.3\u00d7) faster than the state-of-the-art in-storage computing solution, INSIDER for HDC classification (clustering).INTRODUCTIONThe Internet of Things, the ever-increasing demand for new complex applications, and slowdown of Moore's law have pushed current processing systems to their limits. Running data-intensive workloads with large datasets on traditional cores results in high energy consumption and slow processing speed. Moreover, most Internet of Things applications today use state-of-the art machine learning algorithms that have severe energy requirements. With the growing importance of achieving energy efficiency, there is a need to explore emerging computation models.Brain-inspired Hyperdimensional Computing (HDC) is a computation paradigm that represents data in terms of extremely large vectors, called hypervectors. These hypervectors may have tens of thousands of dimensions and present data in the form of a pattern of signals instead of numbers. By representing data in high-dimensional space, HDC reduces the complexity of operations required to process data. HDC builds upon a well-defined set of operations with random HDC vectors, making HDC extremely robust in the presence of failures, and offers a complete computational paradigm that is easily applied to learning problems[25]. Prior work has shown the suitability of HDC for various applications like activity recognition, face detection, language recognition, and image classification, among others [12-14, 17, 28, 36, 38].Although HDC provides improvements in performance and energy consumption over conventional machine learning algorithms, it still involves fetching each and every data from memory/disk and processing it on CPUs/GPUs. Today, extremely large datasets are stored on disks. In addition, the humongous amount of data generated while running HDC cannot always be fit into the memory, eventually killing the process. Recent work has introduced computing capabilities to solid-state disks (SSDs) to process data in storage[8,24,29,39]. This not only reduces the computation load from the processing cores but also processes raw data where it is stored. However, the state-of-the-art in-storage computing (ISC) solutions either utilize a single big accelerator for an SSD or limit the gains by using complex power-hungry accelerators down the storage hierarchy [31]. Such architectures are not able to fully leverage its hierarchical design.In this article, we propose an HDC system that spans multiple levels of the storage hierarchy. We exploit the internal bandwidth and hierarchical structure of SSDs to perform HDC operations over multiple data samples in parallel. Our main contributions are as follows:\u2022 We present a novel ISC architecture for HDC that performs HDC classification and clustering completely in storage. It enables computing at multiple levels of SSD hierarchy, allowing for highly parallel ISC. Our hierarchical design provides parallelism and hides a significant part of the performance cost of ISC in the storage read/write operations. \u2022 We introduce the concept of batching in HDC and utilize it to make our ISC implementation more efficient. During training, we batch together multiple data samples encoded in the HDC domain in storage. This allows us to partially process data without accessing all encoded hypervectors. Batching enables us to have a minimal aggregation hardware requirement. Batching also reduces the amount of data sent out of storage. \u2022 Store-n-Learn utilizes die-level accelerators to convert raw data into hypervectors locally in all the flash planes in parallel. Unlike previous work [31], our accelerator is simpler and hides\n\nStore-n-Learn: Classification and Clustering with Hyperdimensional Computing 22:3 its computation latency by the long read times of raw data from flash arrays. Our die-level accelerators can perform both batched and non-batched encoding efficiently in flash planes. For batched encoding, the accelerator processes multiple inputs in a page in parallel, whereas it generates multiple dimensions corresponding to an input in parallel during non-batched encoding. This flexibility is enabled by our innovative adder tree design.\n\n\u2022 We present a top-level SSD accelerator, which aggregates the data from different flash dies.\n\nThis accelerator is implemented on an FPGA-based device controller. We implement new and efficient FPGA designs for HDC training, retraining, inference, and clustering. Although HDC training provides sufficiently accurate initial models, retraining significantly improves the accuracy of the models by iterating over training data and updating the models multiple times. Store-n-Learn inference allows the users to directly obtain the classification result from the storage drive without sending the entire model to the host. Moreover, Store-n-Learn clustering leverages the FPGA already present in storage and iteratively processes the datasets multiple times to generate high-quality cluster centers. \u2022 We also present host-side and drive-side primitives to enable the FPGA to work seamlessly with the die-level accelerators. \u2022 We evaluate Store-n-Learn over 10 popular classification and clustering datasets. Our experimental results show that Store-n-Learn is on average 222\u00d7 (543\u00d7) faster than CPU and 10.6\u00d7 (7.3\u00d7) faster than the state-of-the-art ISC solution, INSIDER for HDC classification (clustering).\n\n\nRELATED WORK\n\nHyperdimensional computing. Prior work applied the idea of HDC to a wide range of learning applications, including language recognition [37], speech recognition [15], gesture detection [34], human-brain interaction [35], and sensor fusion prediction [38]. For example, Rahimi et al. [36] proposed an HD encoder based on random indexing for recognizing a text's language by generating and comparing text hypervectors. In another work, Rahimi et al [34] proposed an encoding method to map and classify biosignal sensory data in high-dimensional space. Imani et al. [12] proposed a general encoding module that maps feature vectors into high-dimensional space while keeping most of the original data. Prior work also designed different training framework to enable sparsity and quantization in HDC [11,21]. Prior work also tried to design different hardware accelerators for HDC. This included accelerating HDC on existing FPGA, ASIC, and processing in-memory platforms [19,41,45]. However, these solutions do not scale well with the number of classes and dimensions, primarily due to the data movement issue. In addition, the existing processing in-memory architectures only accelerate the encoding, training, or associative search, and they are not scaled with the number of classes of hypervector dimensions. Moreover, they work with a binary hypervector, which has been shown to provide very low classification accuracy in HD space [16]. In contrast, our proposed Store-n-Learn accelerates all the phases of HDC classification and clustering by fundamentally addressing data movement and memory requirement issues. In addition, Store-n-Learn scales with the size of data and the complexity of the learning task.\n\nIn-storage computing. The major bottlenecks in the current storage systems include the slow flash array read latency and the SSD to host I/O latency [30]. To alleviate these issues, prior work introduced ISC architectures [29,42]. These works exploited the embedded cores present in the SSD controller to implement ISC. Another set of works [8,24,31] used ASIC accelerators in SSD for specific workloads. Ruan et al. [39] proposed a full-stack storage system to reduce the host-side I/O stack latency. All of these works propose single-level computing in storage; however, Store-n-Learn is the first work to push the computing all the way down to the flash die to extract maximum 22:4 S. Gupta et al. parallelism. It also uses a top-level accelerator to provide an additional layer of computing. The combination provides a faster implementation that overcomes the SSD to host transfer bottleneck for HDC.\n\n\nHYPERDIMENSIONAL COMPUTING\n\nBrain-inspired HDC has been proposed as the alternative computing method that processes the cognitive tasks in a more light-weight way [25,37]. HDC offers an efficient learning strategy without overly complex computation steps such as back propagation in neural networks. HDC works by representing data in terms of extremely large vectors, called hypervectors, on the order of 10,000 dimensions. HDC has been shown to incur minimal error rates, providing accuracy similar to state-of-the-art learning algorithms like DNNs [10] and k-means [14]. However, the high-dimensional space of HDC makes it robust to external noise sources and hardware-induced errors like device failures [26], stuck-at-fault errors [45], errors from low-precision hardware [37], and noisy communication [6]. Hence, in noisy and error-prone systems HDC proves superior to algorithms like DNNs and k-means that incur large accuracy losses. HDC performs the learning task after mapping all training data into the high-dimensional space. The mapping procedure is often referred to as encoding. Ideally, the encoded data should preserve the distance of data points in the high-dimensional space. For example, if a data point is completely different from another one, the corresponding hypervectors should be orthogonal in the HDC space. There are multiple encoding methods proposed in literature [12,36]. These methods have shown excellent classification accuracy for different data types. In the following, we explain the details of HDC classification steps.\n\n\nEncoding\n\nLet us consider an encoding function that maps a feature vector\nF = { f 1 , f 2 , . . . , f n }, with n features (f i \u2208 N) to a hypervector H = {h 1 , h 2 , . . . , h D } with D dimensions (h i \u2208 {0, 1})\n. We first generate a projection matrix PM with D rows, and each row is a vector with n dimensions randomly sampled from {\u22121, 1}. This matrix is generated once offline and is then used to encode all of the data samples. We generate the resulting hypervector by calculating the matrix vector multiplication product of the projection matrix with the feature vector:\nH = PM \u00d7 F.(1)\nAfter this step, each element h i of a hypervector H has a non-binary value. In HDC, binary (bipolar) hypervectors are often used for the computation efficiency. We thus obtain the final encoded hypervector by binarizing it with a sign function (H = si\u0434n(H )) where the sign function assigns all positive hypervector dimensions to 1 and zero/negative dimensions to -1. The encoded hypervector stores the information of each original data point with D bits.\n\n\nTraining for Classification\n\nIn the training step, we combine all of the encoded hypervectors of each class using elementwise addition. For example, in an activity recognition application, the training procedure adds all hypervectors that have the \"walking\" and \"sitting\" tags into two different hypervectors. Where H i j = h D , . . . , h 1 is encoded for the j th sample in the i th class, each class hypervector is trained as follows:\nC i = j H i j = c i D , . . . , c i 1 .(2)\n\nClassification Retraining\n\nHD classification training requires only a single pass over training data and delivers reasonable accuracy. However, some critical applications and/or situations may demand higher accuracy. In such cases, HD classification retraining can significantly improve the accuracy of the base trained hypervectors by iterating over the training data multiple times. Considering a training input hypervector H x that belongs to class j but is incorrectly assigned to class k, the retraining step proceeds as follows:\nC i = C i \u2212 H x ,(3)C j = C j + H x .(4)\nThis step can be repeated several times for the whole dataset until the desired accuracy is achieved.\n\n\nClassification Inference\n\nThe main computation of inference is the encoding and associative search. We perform the same encoding procedure to convert a test data point into a hypervector, called a query hypervector, Q \u2208 {\u22121, 1} D . Then, HDC computes the similarity of the query hypervector with all k class hypervectors, {C 1 , C 2 , . . . ,C k }. We measure the similarity between a query and an i th class hypervector using \u03b4 Q, C i , where \u03b4 denotes the similarity metric. The similarity metric most commonly used is cosine similarity, as it provides the highest accuracy. However, other similarity metrics like dot product and Hamming distance for binary class hypervectors are also used. After computing all similarities, each query is assigned to a class with the highest similarity.\n\n\nClustering\n\nThe HD clustering algorithm is very similar to the popular k-means algorithm. HD clustering, like k-means, first starts off with random centers. Each cluster center is assigned a unique hypervector. Then, the algorithm iterates through all of the data points while comparing their corresponding hypervectors with those of the cluster centers using the cosine similarity metric. Each data point is assigned the center with maximum similarity. After all points are labeled, the new centers are chosen by superimposing the corresponding points to form an updated set of cluster centers:\nC t +1 k = H X \u2208C t k H X ,(5)\nwhere H X \u2208 C t k indicates the set of all data points assigned to the cluster represented by C k after iteration t. The process is repeated until convergence or the maximum number of iterations is reached. Convergence occurs when no point is assigned to a different cluster compared to the previous iteration.\n\n\nApplications beyond Classification and Clustering\n\nIn addition to the classification and clustering workloads discussed in this article, recent works have used HDC for various types of applications. For example, the work on HyperRec [9] implements a recommender system using HDC. GenieHD [27] uses HDC to implement DNA pattern matching. It uses HDC's high dimensionality to encode the entire DNA reference database, making it easier to be searched. It then matches incoming chunks of HDC-encoded DNA with the reference database. Asgarinejad et al. [1] and Imani et al. [18] used HDC to detect seizures and perform realtime health analysis, respectively. In addition, Neubert et al. [32] demonstrated the application of HDC to different robotic tasks like viewpoint invariant object recognition, place recognition, and learning of simple reactive behaviors. To achieve this, they mapped high-dimensional vectors and operations of HDC to the more widely studied vector symbolic architectures. These works show that HDC has a wide applicability and can be used to perform a range of tasks.\n\n\nChallenges\n\nHDC is light-weight enough to run at acceptable speed on a CPU [16]. Utilizing a parallel architecture can significantly speed up the execution time of HDC [19]. However, with the constantly increasing data sizes along with the explosion in data that occurs due to HDC encoding, running this algorithm on current systems is highly inefficient. All of these platforms need to fetch the extremely large hypervectors from memory/disk to process them. They also require huge memory space to store HDC hypervectors and train on them. With the available parallelism across thousands of dimensions and simple operations needed, ISC is a promising solution to accelerate HDC encoding and training.\n\nGeneral-purpose ISC solutions partially address the data transfer bottleneck but still are not able to fully exploit the huge internal SSD bandwidth [39]. The state-of-the-art application-specific ISC [31] tries to exploit the internal SSD bandwidth but provides only one level of computing, which fails to accelerate applications that either (i) have a computing logic that is too complex to implement using the small accelerator or (ii) require post-processing computation steps. Store-n-Learn aims to overcome these issues by breaking complex HDC algorithms into simpler, both data-size and computation-wise, parallelizable tasks. Then, Store-n-Learn utilizes two levels of computation within the SSD, one at the chip-level and other at the SSD level, to efficiently implement those tasks.\n\n\nSTORE-N-LEARN DESIGN\n\nStore-n-Learn is an ISC design that performs HDC classification and clustering completely in storage. It utilizes a two-level computing architecture. The first level encodes the raw data into HDC data, whereas the second level processes this high-dimensional data. The expansion of data size during encoding requires high bandwidth between the first and the second level of computing. In addition, the second level of computing should be able to implement a variety of different computing kernels based on the target application. To achieve that, we implement the first level of computing at the flash chips, whereas the second level of computing is at the SSD controller in the form of an FPGA. Although computing at flash chips can utilize the high internal parallelism (multiple flash chips in SSD hierarchy) and internal bandwidth (higher channel bandwidth vs the SSD output bandwidth), the FPGA allows us to implement configurable computing kernels. Figure 1 shows an overview of the Store-n-Learn SSD architecture. A flash die consists of multiple flash planes, each of which generates a page during a read cycle. Store-n-Learn inserts a simple low-power accelerator, the die-level accelerator (in green on the right in Figure 1), in each plane to encode every read page into a hypervector. These hypervectors are then sent to a top-level FPGA, which accumulates these hypervectors in batches (in green on the bottom left in Figure 1). The FPGA is also used for retraining, inference, and clustering on the encoded hypervectors received from the flash planes. Store-n-Learn uses a scratchpad (in green on the top left in Figure 1) in the controller to store the projection matrix, which it receives as an application parameter from the host. Batching ensures that data generated by each SSD-wide read operation is used in training as soon as it is available, without waiting for the remaining data.\n\n\nBatched HDC Training in Store-n-Learn\n\nThe size of raw data (number of data points) combined with the size of each hypervector (size of each encoded data point) makes it unrealistic to store all of the encoded hypervectors and then perform HDC training over them. Hence, we employ batching to perform partial training with the hypervectors available at any given moment. As mentioned in Section 3, the initial HDC training algorithm to create a class hypervector (2) is to add up all of the encoded samples belonging to a given class. This summation can be split up into batches of partial sums and maintain the same result. For example, say there are s samples for each class-the total sum can be split up into k partial sums or batches and the batch size defined as b = s/k, as shown in Equation (6).\nC i = b j=1 H i j + 2b j=b+1 + \u00b7 \u00b7 \u00b7 + s j=((s\u22121)b )+1 H i j(6)\nBatching allows Store-n-Learn to process a subset of encoded hypervectors together. Store-n-Learn chip-level accelerators encode raw data into hypervectors and send them to the top-level SSD FPGA accelerator for further processing. All flash chips operate in parallel to encode some of their data, send the hypervectors to the FPGA, and operate on the next set. Each of these hypervectors belongs to a specific class. For an application with C classes, we allocate enough memory in the top-level accelerator to store C model hypervectors, each assigned to a class. We batch all incoming hypervectors from flash that belong to the same class together and bundle the result with the corresponding model hypervector. This is continued until all required data has been encoded and used to train model hypervectors. In the end, the top-level model hypervectors represent a fully trained model of the data. Batching provides us with two benefits. First, it minimizes the memory requirement during training. Second, it reduces its effective latency by combining hypervectors as soon as they are generated. This hides a major part of training latency with the time taken to read data from flash.\n\nWhat if the size of model hypervectors is too large to store at top-level FPGA accelerator? Some applications may need too many dimensions or have too many classes to store all model hypervectors at the FPGA, which at best may have few megabytes of blocked RAMs (BRAMs). In such a case, even with balanced data, it will not be possible to train the model completely in storage. However, Store-n-Learn can still perform training in batches and reduce the amount of data sent to the host for processing. Now, instead of allocating FPGA BRAMs for all model hypervectors, it is dynamically allocated according to the encoded input hypervectors available at a time. If an input hypervector does not belong to one of the present models, a model hypervector is sent out to the CPU host and an empty model hypervector corresponding to the class associated with the incoming hypervector is allocated instead. The implementation details are presented in Section 4.3. The host is then responsible for combining various batched training hypervectors together.\n\nIn this operating mode, Store-n-Learn still reduces the amount of data movement compared to sending the raw low-dimensional data. Here, we define n as the number of features or dimensionality of the original data, D, as the dimensionality of the encoded hypervectors, and b as the batch size. When nb > D, the total data movement of the resulting batched hypervectors is less than the amount of original data sent in low-dimensional space when the batched hypervector uses the same bitwidth as the original data. However, we can utilize lower bitwidth representations as we encode the data into a hypervector whose elements are {\u22121, 1} and then bundle the hypervectors with element-wise addition. Therefore, the range of data in any given dimension can be defined by the normal distribution with a mean of 0 and standard deviation of \u221a b. We can represent each dimension of the batched hypervector with (log 2 4 \u221a b) + 1 bits while maintaining an accurate representation. We multiply by 4 to capture 4 standard deviations away and add 1 to account for the sign bit. In this case, assuming the original data is represented with 32 bits, Store-n-Learn sends less data than the data movement required to send the original data in low-dimensional space when 32nb > D (log 2 (4 \u221a b) + 1).\n\n\nEncoding Near Data via Flash Hierarchy\n\nThe modern SSD architecture is hierarchical in nature. An SSD has multiple channels. Each channel is shared by four to eight flash chips as shown in Figure 1. The flash chip may consist of several flash dies that are further divided into flash planes, each plane consisting of a group of blocks, each of which store multiple pages. Each plane has a page buffer to write the data to. Operations in the SSD happen in page granularity where the size of pages usually ranges from 2 to 16 KB [4]. Throughout this article, \"B\" represents bytes. To fully utilize the flash hierarchy, we introduce accelerators for each flash plane as shown in Figure 1. The aim of this added computing primitive is to process the data where it has no conflict or competition for resources.\n\n\nChip-Level Accelerator Design.\n\nThe Store-n-Learn plane accelerator encodes an entire page with raw data to generate a D-dimensional hypervector. Let us assume the SSD page size to be 4 KB (p s ) with each data point being 4 bytes (d s ). This translates to 1K data points (p s /d s ). Let the feature vector contain 1K features. Assuming that the feature vectors are page aligned, each page stores one feature vector. HDC encoding multiplies an n-size feature vector with a projection matrix containing D \u00d7 n 1-bit elements. Our accelerator calculates the dot product between two page-long vectors, one read from the flash array and another being a row vector of the projection matrix. This involves element-wise multiplication of the two vectors and adding together all elements in the product. Since the weights in the projection matrix \u2208 {1, \u22121}, we reduce the bits required to store the weights by mapping them such that 1 \u2212 \u2192 1 and (\u22121) \u2212 \u2192 0. We use 2's complement to break the multiplication into an inversion using XNOR gates and then adding the total number of inverted inputs to the accumulated sum of XNOR outputs. The accelerator is shown in Figure 2. It consists of an array of 32K XNOR gates followed by a 1K input tree adder (labeled CSA in Figure 2). The tree adder is a pruned version of the Wallace carry save tree, where the operand size throughout the tree is fixed to 4B. It reduces 1,024 inputs to 2, which is followed by a carry look-ahead addition (labeled CLA in Figure 2). This gives us the dot product of the two vectors. It is the value of one dimension of the encoded hypervector. The accelerator is iteratively run D times to generate D dimensions. Depending upon the power budget, Store-n-Learn may employ multiple parallel instances of this accelerator to reduce the total number of iterations. Since D is generally large, the generated D-dimensional vector is multi-page output. Store-n-Learn writes the output of the accelerator to the page buffer of the plane, which serves as the response to the original SSD read request.\n\n\nStoring Input Data.\n\nThe preceding accelerator assumed the size of the feature vectors to be exactly the same as that of a page. However, this is rarely the case. State-of-the-art ISC designs use page-aligned feature vectors, which may lead to poor storage utilization if the feature vector Store-n-Learn: Classification and Clustering with Hyperdimensional Computing  size is too small or just larger than the page size. For example, in a page-aligned feature vector setting, a 4-KB page may fit only one 512-B feature instead of eight. In addition, a 5-KB feature vector may occupy two complete pages. To alleviate the issue, we propose a cross-plane storing scheme, which considers all planes in a chip when storing data, with the goal of increasing the traditional ISC storage utilization while being accelerator-friendly. We first describe the case when the size of the feature vector is smaller than the page size. The scheme, shown in Figure 3 on the left, divides an n-sized feature vector into n p equal segments such that the most efficient storage is given when\nargmax c c \u00d7 n + d.n/n p \u2264 p s ,\nwhere c is the number of complete n-sized feature vectors in a p s -sized page, n p is the number of planes per chip, and d \u2208 {0, 1, . . . n p }. Hence, a page would contain c \u00d7 n p + d segments in total. Having n p equal segments instead of any variable segmentation allows the accelerator to have a simple segment-wise weight allocation. Each row vector in the projection matrix of a plane is divided into the same-sized segments as the feature vector as shown in Figure 3 on the right. This allows Store-n-Learn to increase storage efficiency while minimizing the control overhead of the accelerator.\n\nIf the size of the feature vector is less than the page size, Store-n-Learn uses the same segmentation size. However, the number of segments in a page are given by\nargmax d d.n/n p \u2264 p s .\nA drawback of this scheme is that individual reads for small feature vectors may require reading two pages instead of one. However, our main purpose is to obtain trained vectors and not raw feature vector values. Moreover, since a feature split across two planes shares the same block and page number, they are both read at the same time. \n\n\nEncoding on Store-n-Learn Flash Chips.\n\nAlthough the new data storing scheme improves the page utilization, it does not suit well the chip-level accelerator. As proposed before, our accelerator is a dot-product engine. It processes an entire page from the flash array to generate values of different dimensions of the corresponding hypervector. In the new data storage scheme, this would result in an encoded hypervector consisting of multiple and also partial feature vectors. An easy fix would be to just process one feature vector at a time by setting the remaining inputs of the accelerator to 0. However, this would increase the total latency of the accelerator. The situation is worse if the size of feature vectors is very small. We address this problem by extending the concept of batching in Store-n-Learn.\n\nAs detailed in Section 4.1, a set of encoded hypervectors can be added dimension-wise without interfering with the HDC training process as long as they belong to the same final trained hypervector, such as the same class model. An encoded dimension (d i ) of a feature vector (FV ) is obtained by a dot product between the feature values (FV i ) and the corresponding row of the projection matrix (PM)-that is,\nd i = FV 0 \u00d7 PM i,0 + FV 1 \u00d7 PM i,1 + \u00b7 \u00b7 \u00b7 FV n\u22121 \u00d7 PM i, (n\u22121) .\nNow, to add multiple feature vectors together, we just need to make sure that an element in a feature vector is being multiplied with the corresponding weight of the projection matrix. In that case, we would achieve the same effect as batching, only at a lower level of abstraction. This also works when we have partial features. In this case, the encoded hypervector for the current page would just have partial information and may not correctly represent the data. Some part of this information is contained in the encoded hypervector of another page. However, all of these hypervectors will be added together during training. Hence, the final hypervector will contain all of the information.\n\nTo support this strategy in the Store-n-Learn accelerator, the flash controller segments the projection matrix in the same way as the feature vectors in the planes and sends the corresponding segments to the accelerator in each plane. It is important to note that only the features belonging to the same class are added together in batches. Thus, a chip-level accelerator performs a bitwise comparison between the labels of feature vectors in a page and only processes those belonging to the same final model together.\n\n\nEncoding without\n\nBatching. The encoding acceleration discussed previously works well while training class hypervectors for classification because training input samples can be batched together. However, other tasks like clustering, retraining, and inference operate on individual data samples. Hence, they cannot utilize batched hypervectors and require access to individual ones.\n\nAs discussed before, encoding individual data points is slow and does not fully utilize the adder tree present in the encoding accelerator. Hence, unlike batched encoding where we could get away with generating just one dimension per iteration of the accelerator, here we need to generate multiple dimensions in parallel. Since each dimension is independent, one way to improve the latency of encoding individual vectors would be to introduce multiple adder trees, each computing one dimension. However, this would linearly increase the power and area of the accelerator. Moreover, the optimal size and number of trees would differ for each application.\n\nInstead, we preserve the current single adder tree and introduce carry look-ahead adders (CLAs) at intermediate stages as shown in Figure 4. The figure shows only a part of our complete 20-stage adder tree. Our 32-bit CLA implementation has a latency similar to four sequential carry save additions-that is, four stages of the carry save adder (CSA) tree. Hence, we generate our tree using 4-stage CSAs. We also add CLAs after every 4 stages, as shown with blue and purple boxes in  Each of these CLAenabled independent trees can generate one output (dimension) each. Hence, in the case when the size of feature vector is significantly smaller than the page size and less than the input size of any of the CLA-enabled smaller trees, these trees can generate a dimension each. Thus, if a feature vector has a size of, say, 32 (8), we utilize the stage 8 (stage 4) CLAs (i.e., the blue (purple) boxes in Figure 4). For a 1,024-input adder, we can generate 24 dimensions of the hypervector corresponding to this feature vector in parallel. To enable this, the feature vector is input to each smaller CSA. Moreover, the projection submatrix corresponding to the 24 dimensions is flattened and supplied to the accelerator. The preceding modification allows us to generate multiple dimensions in parallel, significantly boosting the performance of single-feature vector encoding.\n\n\nAccelerating HD Data Processing at the Controller\n\nThe encoded hypervectors from flash chips are used for further processing (learning in our case) in the top-level accelerator, which is implemented on an FPGA present in the SSD controller. We use the FPGA because it is flexible with the application parameters and can be configured using the primitives provided by INSIDER [39]. Previous works perform data encoding on the FPGA to avoid storing the encoded data, which requires more storage space. Supporting HD encoding on the FPGA consumes a lot of FPGA resources and thus limits the performance of the accelerator. Store-n-Learn uses flash chips to encode the data in real time that saturates the internal SSD bandwidth. Thus, Store-n-Learn dedicates all FPGA resources for HD training, inference, and retraining, thereby providing higher performance as conventional FPGA-based accelerators. The initial training iteration builds the HD model, a class hypervector for each class. However, to fine-tune the HD model and increase accuracy, multiple retraining iterations may be needed.\n\n\nInitial Training.\n\nIn the FPGA, we first allocate memory for the final class hypervectors. For each class, the FPGA has an input queue, where the input hypervectors belonging to that class are indexed, and an accumulator, which serially accumulates the vectors in the input queue to generate the final class hypervector. The introduction of class-wise input queues removes the input data dependency of the accumulator by pre-processing class labels. An accumulator simply needs to read the input index from its queue and operate on the corresponding data. It makes the computation for different classes independent and parallelizable. The accumulators for each class then operate in parallel to add an input hypervector from the queue to the corresponding class hypervector. We divide the hypervectors into partitions to allow partial parallelism.\n\n\nRetraining and Inference.\n\nAs explained in Section 3.3, the retraining step consists of reading the encoded hypervectors from the storage, performing the inference, comparing the classification output with the data label, and adjusting the HD model in case of misprediction. To adjust the model, the encoded hypervector is added to the class it belongs to and is subtracted from the mispredicted class hypervector. Figure 5(a) shows the architecture of the Store-n-Learn FPGA-based accelerator for HD retraining and inference. The encoded hypervector is read from the storage device and stored into the encoded hypervector buffer.\n\nDuring HD inference, Store-n-Learn in every clock cycle reads d dimensions of the encoded hypervector, and since HD operations can be parallelized in the dimension level, it calculates the partial similarity metric between the dimensions of the encoded hypervector and corresponding dimensions of the class hypervectors. In HD inference, in every clock cycle, Store-n-Learn calculates the similarity metric between d dimensions of the encoded hypervector and d dimensions of C class hypervectors. For each class, Store-n-Learn performs d multiplications and accumulates the multiplication results in a tree adder with d inputs. At the end, the class with the maximum similarity is the inference result. In each cycle, Store-n-Learn calculates a part of similarity metric and the entire inference is executed in D d cycles. To perform HD retraining, Store-n-Learn first performs HD inference and compares the prediction label with the original data label. As illustrated in Figure 5(a), for each misprediction, one addition and one subtraction is needed. Since during the retraining stage an entire encoded hypervector is needed, Store-n-Learn locally stores it on FPGA BRAMs. If Store-n-Learn predicts the label correctly, it reads the next encoded input; otherwise, it performs the model adjustment in D d cycles.\n\n\nClustering.\n\nMultiple clustering iterations are required for HD clustering model to converge. In each clustering iteration, HD uses the existing centroids to clusters the input data, and uses the clustered data, at the end of iteration, to update the cluster centroids. Store-n-Learn uses the average of the encoded hypervectors assigned to a cluster as the cluster centroid, giving us the initial clustering model. This is followed by multiple clustering iterations. For each iteration, Store-n-Learn uses a copy of the latest HD clustering model to update the centroids. In an iteration, Store-n-Learn uses the clustering model to perform a similarity check for each encoded input and assigns a cluster to it. Then, the corresponding input hypervector is added to the predicted cluster centroid of the iteration's copy of the HD clustering model. At the end of each clustering iteration (i.e., after processing all the input data), the HD clustering model is replaced by the updated copy of the HD clustering model. Figure 5(b) highlights the differences between Store-n-Learn HD retraining and HD clustering. To support HD clustering, Store-n-Learn reuses the similarity check module of HD classification to find the predicted cluster. It also needs a copy of the HD model to update the centroids. As illustrated in the figure, Store-n-Learn requires a duplicate of the HD model memory to support HD clustering, and it reuses the adder array to update the cluster centroids. Therefore, Store-n-Learn supports HD clustering with double BRAM utilization and with minimal logic overhead, only for generating related control signals. In each cycle, similar to classification inference, the similarities between the encoded hypervector and the clustering centroid hypervectors are calculated. Finding the closest cluster takes D d cycles. Then, Store-n-Learn uses the predicted clusters to update the centroids. Updating the centroids takes another D d cycles.\n\n\nSoftware Support\n\nThe top-level FPGA uses an INSIDER acceleration cluster [39] to implement all HDC operations other than encoding. We utilize INSIDER's software stack to connect Store-n-Learn to the rest of the system. We modify the SSD drivers and the INSIDER virtual files mechanism to enable computing in flash chips and make it visible to the FPGA. Store-n-Learn derives its base system architecture from INSIDER [39]. The INSIDER framework is an API what, while being compatible with POSIX, allows us to implement an ISC accelerator cluster. The INSIDER API takes a C++ or RTL code as an input and programs the acceleration cluster (running on drive FPGA) accordingly. The drive program interface has three FIFOs. The data input (output) FIFO takes in the input (output) data that is needed (generated) by the accelerator. The parameter FIFO contains the runtime parameters for the FPGA that are sent by the host. INSIDER keeps control and data planes of ISC separated. The drive control and standard operations are handled by the SSD firmware while all compute data from flash chips are intercepted by the top-level FPGA accelerator for computing. The FPGA does not care about the source and/or destination of the data.\n\n\nStore-n-Learn\n\nHost-Side Support. The INSIDER API uses POSIX-like I/O functionality to communicate with the driver. INSIDER has a standard block device driver with changes made to the virtual read and write functionalities to accommodate for the programmable accelerator clusters in the drive. However, the current abstraction allow us to pass directive/parameters only to the ISC FPGA and not the drive. We define a new API, send_mode, which defines the mode for read and write operations, further discussed in Section 4.4.2. It passes a single integer, mode, to the drive firmware while opening a virtual file. For a non-ISC read/write from the drive, mode is set to 0. During an ISC read, mode represents the expansion f actor (EF ). EF defines the increase in the size of raw data after encoding. For example, EF = 5 means that each page of raw data generates five pages of encoded data (due to large D). In this case, mode is set to 5. This parameter is necessary to enable the drive to read the required number of pages from the flash chips. Since EF is dependent on the number of features of the data and dimensionality requirement of the application, it remains constant for an entire run. Similarly, a non-zero mode signifies ISC write. In this case, the data being sent to the drive contains the elements of the HDC projection matrix and is written to the controller scratchpad. No data is written to the flash chips. During write, we only care about whether mode is zero or non-zero.\n\n\nStore-n-Learn Drive-Side\n\nArchitecture. Store-n-Learn implements its top-level accelerator described in Section 4.3 as an INSIDER acceleration cluster, which enables the final training step. However, the INSIDER system does not support Store-n-Learn's die-level acceleration because the standard read/write drive operations cannot readily accommodate on-the-fly change in data size while reading encoded pages and writing projection matrix elements to the on-die accelerator.\n\nStore-n-Learn introduces the processing capability between flash planes and page buffers, but sometimes only raw data may be required. Hence, Store-n-Learn employs two read modes: normal and compute. It uses the die-level accelerator in multiplexed mode where a read page is sent to the accelerator for processing only in compute mode, shown in Figure 6. In normal mode, the plane directly writes the original page to the page buffer. Moreover, response type in the two modes also differs. A normal read results in just one page, whereas a compute read responds with multiple but fixed number of pages. Store-n-Learn uses application specifications such as feature vector size and dimensionality requirement to generate an expansion factor, which is supplied to the SSD firmware by the host, as explained in Section 4.4.1. The firmware uses this factor to calculate the response size for page read commands in compute mode.\n\nStore-n-Learn also employs two write modes: normal and compute. The compute mode is used to supply projection matrix data to the on-die accelerators. In normal mode, data is written in the data buffer and then programmed in the flash array. In compute mode, the data in data buffer is sent to the accelerators as shown in Figure 6. The writes in compute mode are fast since the data is just latched in CMOS registers instead of flash arrays. Unlike a compute mode read, where the same command can be issued to all of the chips, compute mode write requires individual commands for each plane to configure their respective on-die accelerators. This follows from Figure 3. Each plane gets the same segments, but their positions may differ for different planes. A write configuration command is separately issued for each plane. For each plane, it configures the size of segment (se\u0434 S ), number of input segments (se\u0434 in ), actual number of segments in the plane (se\u0434 act ), and the ID of the first segment (se\u0434 one ). The format of the command is [se\u0434 S , se\u0434 in , se\u0434 act , se\u0434 one ]. For example, the command for plane 0 and plane 2 in Figure 3 would be [200,4,5,0] and [200,4,5,2], respectively. Although sequential, this step has negligible latency overhead because it can be performed in parallel for all of the flash chips.\n\nAs discussed briefly in Section 4.2, the flash controller sends the projection matrix elements to the respective accelerators. SSD receives the projection matrix from the host. We introduce a dedicated scratchpad in the flash controller to store the matrix. The controller sends the elements in page-sized frames to the die accelerators. The frames consist of multiple segments and are used by the die accelerators according to the configuration command, as shown in Figure 3.\n\n\nStore-n-Learn Is More Than Just a HDC Accelerator\n\nWe envision Store-n-Learn as a system that, in addition to performing all functions of a standard SSD, can accelerate HDC. However, since Store-n-Learn is based on the INSIDER system stack, it can perform a variety of different computations. The FPGA present in the controller can be configured to perform any computation task on the data read from the drive. The hierarchical nature of Store-n-Learn makes the FPGA and die-level accelerators independent of each other. If the target application/computation does not require die-level accelerators, then the Store-n-Learn can operate in normal read mode and supply raw data to the SSD controller, which the FPGA can compute upon. Moreover, the die-level accelerator that performs HD encoding is essentially a dot-product engine. Dot product is one of the major computations in applications like neural networks. Hence, applications other than HDC can use Store-n-Learn either fully or partially.\n\n\nRESULTS\n\n\nExperimental Setup\n\nWe developed a simulator for Store-n-Learn that supports parallel read and write accesses to the flash chips. We utilized Verilog and Synopsys Design Compiler to implement and synthesize our die-level accelerator at 45 nm and scale it down to 22 nm. The top-level FPGA accelerator has been synthesized and simulated in Xilinx Vivado. For Store-n-Learn drive simulation, we assume the characteristics similar to the 1-TB Intel DC P4500 PCIe-3.1 SSD connected to an Intel Xeon CPU E5-2640 v3 host. The parameters for Store-n-Learn are shown in Table 2.\n\nWe compare Store-n-Learn with a seventh-generation, 2.4-GHz Kaby Lake Intel Core i5 CPU with 8 MB of RAM and 256 GB of SSD. We also compare it with a 3.5-GHz Intel Xeon CPU E5-2640 v3 CPU server with 256 GB of RAM and 2 TB of local disk. In addition, we compare with a CUDA implementation of the HDC pipeline on an Nvidia GTX 1080 Ti GPU. We also compare Store-n-Learn with INSIDER [39] and DeepStore [31], the state-of-the art ISC solutions. INSIDER is a full-stack storage system and uses a top-level FPGA accelerator in the drive for ISC. DeepStore is an ISC implementation for query-based workloads that employs specialized accelerators in the SSD. For all of our experiments, including those for other ISC solutions, the data is assumed to be channel striped and stored using Store-n-Learn's proposed scheme.\n\n\nWorkloads\n\n\nClassification.\n\nWe evaluate the efficiency of Store-n-Learn on five popular classification applications, as summarized in Table 1 and listed next:\n\nSpeech recognition (ISOLET): The goal is to recognize voice audio of the 26 letters of the English alphabet [23]. Face recognition (FACE): We exploit the Caltech dataset of 10,000 web faces [7]. Negative training images (i.e., non-face images) are selected from the CIFAR-100 and Pascal VOC 2012 datasets [5]. Activity recognition (UCIHAR): The dataset includes signals collected from motion sensors for eight subjects performing 19 different activities [44]. Medical diagnosis (CARDIO): This dataset provides a medical diagnosis based on cardiotocography information about each patient [3]. Gesture recognition (EMG): The dataset contains EMG readings for five different hand gestures [2]. \n\n\nClustering.\n\nWe evaluate Store-n-Learn on FCPS, the fundamental clustering problem suite [43], which has been widely used in the literature. We also evaluate HD clustering on the pattern recognition dataset [22]. The specific datasets used are summarized in Table 1 and listed next:\n\nFCPS Hepta [43]: The three-dimensional Hepta dataset consists of seven clusters that are clearly separated by distance, one of which has a much higher density. FCPS Tetra [43]: The Tetra dataset consists of 400 data points in four clusters that have large intra-cluster distances. The clusters are nearly touching each other, resulting in low intercluster distances. FCPS TwoDiamonds [43]: The data consists of two clusters of two-dimensional points. Inside each \"diamond, \" the values for each data point were drawn independently from uniform distributions. FCPS WingNut [43]: The WingNut dataset consists of two symmetric data subsets of 500 points each. Each of these subsets is an overlay of equally spaced points with a lattice distance of 0.2 and random points with a growing density in one corner. Pattern recognition (Iris) [22]: The dataset consists of samples from each of three species of Iris with four features are present from each sample. One class is linearly separable from the other two; the latter are not linearly separable from each other.\n\n\nComparison with CPU and CPU Server\n\nWe first compare Store-n-Learn with CPU and CPU-based server running state-of-the-art implementations of HDC classification and clustering over the five datasets with D = 10k. In addition, we generate a synthetic dataset with 10 classes and each data sample having 512 features. We vary the size DS (number of data points) of the synthetic dataset from 10 3 to 10 7 .\n\n\nHDC Classification with Single-Pass\n\nTraining. The runtime of single-pass classification for different platforms is shown in Figure 7. We observe that Store-n-Learn is on average 3, 405\u00d7 and 1, 612\u00d7 faster than CPU and CPU server, respectively. Our evaluations show that the improvements from Store-n-Learn increases linearly with an increase in the dataset size. This happens because more data samples result in more huge hypervectors to generate and process. In conventional systems, this translates to a huge amount of data transfers between the core and memory. It should be noted that the CPU system runs out of memory while encoding for 10 6 samples and kills the process. The CPU server faces a similar situation for 10 7 samples. In contrast, since Store-n-Learn generates hypervectors (encoding) while reading data out of the slow flash arrays and processes (training) them on the disk itself, there is minimal data movement involved.\n\nStore-n-Learn (No Batch) in Figure 7 shows the latency of HDC training on Store-n-Learn without applying batching. We see that batching reduces the latency of Store-n-Learn on average by 6.5\u00d7. The effective speedup due to batching in CARDIO and EMG datasets is significantly less than the batch size. This is because CARDIO and EMG are small datasets and the number of training samples are not enough to bring forth the complete advantage from batching. This is evident from the synthetic data (DS), where DS = 10 3 samples achieves 4.1\u00d7 speedup from batching, whereas DS = 10 6 samples is able to achieve 7.99\u00d7 speedup from batching. Figure 7 also shows the size of raw input data in each case normalized to the size of the corresponding trained class hypervectors. Store-n-Learn only sends class hypervectors from drive to the host, whereas CPU-based systems fetch all data samples from the disk. We observe that the ratio increases linearly with an increase in the data size. In fact, the size of class hypervectors does not change with an increase in data size as long as the number of classes and required dimensions remain the same. Figure 8 shows the runtime of HD classification with 50 epochs of retraining for different platforms. We observe that Store-n-Learn is on average 222\u00d7, 81\u00d7, and 28.3\u00d7 faster than CPU, CPU server, and GPU, respectively. The improvements are lower than those in case of single-pass classification because now FPGA-based retraining, specifically the search component of retraining, is the major latency bottleneck. Our evaluations also show that the performance of Store-n-Learn classification with retraining increases with an increase in either the dataset size or the number of classes. In addition to processing more hypervectors for a larger dataset, more classes increase the total number of the latency critical search operations. Store-n-Learn is able to process much larger datasets than CPU and CPU server, both of which run out of memory while working with 10 6 data samples. The trend for total SSD to host data transfers remains similar to that of single-pass training, where the amount of data transfers saved increases linearly with an increase in the data size. Figure 9 shows the runtime of HDC with 50 epochs of clustering for different platforms. We observe that Store-n-Learn is on average 543\u00d7 and 187\u00d7 faster than CPU and CPU server, respectively. Moreover, the latency of Store-n-Learn clustering increases with both dataset size and the number of classes. However, the relative improvements from Store-n-Learn also increase with an increase in dataset size. Store-n-Learn is able to process much larger datasets than CPU and CPU server, both of which run out of memory while clustering 10 6 data samples.\n\n\nHDC Classification with Retraining.\n\n\nHDC Clustering.\n\nThe amount of data transfers saved increases linearly with an increase in the data size. For very small clustering datasets [22,43], transferring hypervectors of cluster centers instead of raw data increases the data transfers between SSD and host. However, data transfers become a system bottleneck for large datasets, in which case Store-n-Learn significantly reduces the total transfers. For example, Store-n-Learn transfers~5000\u00d7 less data compared to CPU-based systems for the synthetic dataset with 1 million samples.   Figure 10 shows the breakdown of Store-n-Learn single-pass classification latency normalized to the total latency. Here, I/O shows the time spent in sending the generated class hypervectors to the host. For small datasets, CARDIO and EMG, the latency is dominated by the encoding. However, as the data size increases, the internal SSD channel bandwidth becomes a bottleneck. This indicates that Store-n-Learn is able to completely utilize and saturate the huge internal SSD bandwidth. In addition, a significant amount of time spent in training and some part of the encoding is hidden by the SSD channel latency. As a result, the combined latency is less than sum of the latency for individual stages. For the example of the FACE dataset, even though the training takes more than half of the total latency, a negligible portion of it actually contributes to the overall latency. It shows that Store-n-Learn stages are able to hide some of their latency. This is in contrast to IN-SIDER [39], where the runtime is dominated by the latency of encoding and training in the FPGA. Figure 11 shows the breakdown of latency of different stages in INSIDER. FPGA-based processing (i.e., encoding and training) takes on average 97% of the total INSIDER runtime. However, in the  case of HD retraining and clustering, the top-level FPGA accelerator becomes the latency bottleneck. This happens due to the iterative nature of these algorithms.\n\n\nStore-n-Learn Efficiency\n\nTo demonstrate the scalability provided by Store-n-Learn, we evaluate it over a synthetic dataset with 10 4 samples each with 512 features. We vary the dimensions D from 10 3 to 10 5 . Figure 12(a) shows that the latency of Store-n-Learn increases linearly with an increase in the number of dimensions, showing that Store-n-Learn is able to scale with D. Additionally, an increase in D results in longer class hypervectors for the same input data. Hence, the ratio of raw data to hypervector size decreases with an increase in dimensions, falling from from 512 for D = 1k to 2.5 for D = 10 5 .\n\nWe also scale the dataset with the number of classes while keeping its size fixed to 10 4 samples and D as 10 3 . Figure 12(b) shows that the Store-n-Learn latency has minor changes with the number of classes when we have fewer than 50 classes. This is because our FPGA has enough resources to train up to 54 classes with D = 10k dimensions. The latency almost doubles for 100 classes. However, when the number of classes increases further, the size of model hypervectors is too large to store in the FPGA. Hence, partially trained hypervectors are then sent to the host for further processing. This can be seen by a jump in the latency for 500 classes in Figure 12(b). In addition to  the time spent in training, transferring the class hypervectors to host creates a major bottleneck. This is also evident from the data size ratio that declines for a large number of classes. A ratio of less than 1 signifies that the size of generated hypervectors is larger than the raw data.\n\n\nStore-n-Learn vs Other Algorithms\n\nWe compare Store-n-Learn with the best existing algorithms for classification and clustering. For classification, we compare our work with the state-of-the-art DNN network for ISOLET [20]. In our evaluation, Store-n-Learn runs HDC classification with 50 epochs of retraining, whereas DNN is trained on the CPU. We observe that Store-n-Learn is 9.4\u00d7 faster than DNN while incurring less than 1% accuracy loss. We also compare our design with DNN running on FPGA. No FPGA implementation completely trains DNNs due to the complexity of operations and lack of sufficient on-board resources. Hence, we compare the inference performance of our design with that of DNN running on FPGA [40]. Store-n-Learn is 17.7\u00d7 faster than FPGA for the ISOLET dataset, with less than 1% accuracy loss.\n\nFor clustering, we compare Store-n-Learn with the k-means algorithm [33] for the five clustering datasets on CPU. Store-n-Learn runs HDC clustering with 50 epochs of clustering. The quality Fig. 13. Runtime and data transfer size comparison of Store-n-Learn classification without retraining with INSIDER [39] and DeepStore [31].\n\nof clustering is measured in terms of mutual information score, which is 1 when the predicted labels are perfectly correlated with the ground truth and 0 when they are totally uncorrelated. Store-n-Learn is on average 1.3\u00d7 faster than k-means on CPU while providing the same mutual information score. We also compare Store-n-Learn clustering with k-means running on FPGA and observe that Store-n-Learn is 47\u00d7 faster. Store-n-Learn is faster than the state-of-the-art algorithms for both classification and clustering due to the latency overhead of data transfers in traditional systems. Moreover, the higher complexity of operations in traditional algorithms further makes them slower on FPGA.\n\n\nComparison with Existing ISC Solutions\n\nWe compare the performance and data transfer efficiency of Store-n-Learn with state-of-the-art ISC designs INSIDER [39] and DeepStore [31]. In our experiments, INSIDER performs both encoding and training/clustering using the FPGA accelerator in SSD and sends the class hypervectors to the host. Since DeepStore was intended for a completely different application, we replace its accelerator with Store-n-Learn die-level accelerator. During ISC, DeepStore encodes the raw data into hypervectors and sends those hypervectors to the host for training/clustering. Figure 13 shows the change in latency and data transfer size of single-pass classification for the three ISC solutions. We observe that Store-n-Learn is on average 14.4\u00d7 and 446.8\u00d7 faster than INSIDER and DeepStore, respectively. Although encoding in DeepStore takes the same time as Store-n-Learn, transferring hypervector from SSD to host and further training on them on CPU increases the execution time of DeepStore significantly. However, the SSD channel bottleneck faced by Store-n-Learn is relaxed in the case of INSIDER since it only transfers raw data. However, the FPGA-based HDC encoding+training are on average 21\u00d7 slower compared to FPGA-based training. In addition, since INSIDER performs training in the SSD, it transfers the same amount of data to the host as Store-n-Learn. However, by transferring untrained hypervectors, DeepStore increases the amount of data transferred on average by 397\u00d7 compared to Store-n-Learn. Figure 14 shows the change in latency and data transfer size for complete HDC classification. We observe that Store-n-Learn is on average 10.6\u00d7 and 179\u00d7 faster than INSIDER and DeepStore, respectively. For DeepStore, transferring hypervector from SSD to host and further retraining on them for 50 epochs on CPU increases the execution time of DeepStore significantly. INSIDER's FPGA-based HDC encoding and retraining are on  average 10.7\u00d7 slower compared to Store-n-Learn's FPGA-based retraining because encoding consumes a significant amount of FPGA resources, leaving fewer resources to accelerate latency critical retraining. INSIDER transfers the same amount of data to the host as Store-n-Learn, whereas DeepStore increases the amount of data transferred on average by 2, 510\u00d7 compared to Storen-Learn. This is 6.3\u00d7 worse than the data transferred in single-pass classification because in retraining hypervectors are sent for individual data points, eliminating the gains from batched encoding. Figure 15 shows the change in latency and data transfer size for HDC clustering. We observe that Store-n-Learn is on average 7.3\u00d7 and 187\u00d7 faster than INSIDER and DeepStore, respectively. The latency results follow the same trends and reasoning as those for HDC classification with retraining. For data transfers, DeepStore increases the amount of data transferred on average by 217\u00d7 compared to Store-n-Learn. The data transfer overhead of Deep-Store worsens with an increase in the dataset size.\n\n\nHDC Single-Pass Classification.\n\n\nHDC Classification with Retraining.\n\n\nHDC Clustering.\n\n\nCONCLUSION\n\nIn this article, we proposed an in-storage HDC system that spans multiple levels of the storage hierarchy. We exploited the internal bandwidth and hierarchical structure of SSDs to perform HDC classification and clustering in-storage. We proposed batched HDC training to enable partial processing of HDC hypervectors. We further proposed a die-level accelerator for HDC encoding and top-level FPGA accelerators for HDC training, retraining, inference, and clustering. Our evaluation shows that Store-n-Learn is on average 222\u00d7 (543\u00d7) faster than CPU and 10.6\u00d7 (7.3\u00d7) faster than the state-of-the-art ISC solution INSIDER for HDC classification (clustering).\n\nFig. 1 .\n1Store-n-Learn SSD overview. The components added by Store-n-Learn are shown in green.\n\nFig. 2 .\n2Store-n-Learn die accelerator.\n\nFig. 3 .\n3Data storage scheme in Store-n-Learn and the corresponding segmentation of the projection matrix. Data represents a feature vector.\n\nFigure 4 .\n4For example, a 16 (20)-stage CSA consists of 113 (455) smaller and independent 4-stage,\n\nFig. 4 .\n4Modified CSA in the die accelerator to encode individual feature vectors in Store-n-Learn. The blue and purple squares represent intermediate CLAs. 24 (97) 8-stage, 4 (19) 12-stage, and 1 (3) 16-stage CSAs. We insert a 32-bit CLA for each of these independent CSAs. This results in a total of 141 (574) intermediate 32-bit CLAs.\n\nFig. 5 .\n5Store-n-Learn top-level FPGA design. (a) Retraining and inference for HD classification. (b) HD clustering as compared to the retraining step in HD classification.\n\nFig. 6 .\n6Different read and write modes in standard SSD (a) and Store-n-Learn (b). The components in red are active during the operation.\n\nFig. 7 .\n7Runtime comparison of HDC encoding and single-pass classification training in Store-n-Learn with other platforms. The bars in red show the size of raw data normalized to the total size of corresponding class hypervectors in Store-n-Learn. Store-n-Learn (No Batch) represents the latency of Store-n-Learn without applying batching.\n\nFig. 8 .\n8Runtime comparison of HDC classification with retraining in Store-n-Learn with other platforms. The bars in red show the size of raw data normalized to the total size of corresponding class hypervectors in Store-n-Learn.\n\nFig. 9 .\n9Runtime comparison of HDC clustering in Store-n-Learn with other platforms. The bars in red show the size of raw data normalized to the total size of corresponding cluster center hypervectors in Store-n-Learn.\n\nFig. 10 .\n10Breakdown of latency of different stages of HDC single-pass classification normalized to the total latency.\n\nFig. 11 .\n11Breakdown of INSIDER's[39] latency of different stages of HDC single-pass classification normalized to the total latency.\n\nFig. 12 .\n12Change in HDC runtime and raw data size to hypervector ratio with dimensions (a) and number of classes (b).\n\nFig. 14 .\n14Runtime and data transfer size comparison of Store-n-Learn classification with retraining with INSIDER[39] and DeepStore[31].\n\nFig. 15 .\n15Runtime and data transfer size comparison of Store-n-Learn clustering with INSIDER[39] and DeepStore[31].\n\nTable 1 .\n1Workload SummaryDataset \nFeatures Classes Batch Size \nDataset \nFeatures Clusters Batch Size \nISOLET \n617 \n26 \n6 \nHepta \n3 \n7 \n1,365 \nFACE \n608 \n2 \n6 \nTetra \n3 \n4 \n1,365 \nUCIHAR \n561 \n12 \n7 \nTwoDiamonds \n2 \n2 \n2,048 \nCARDIO \n21 \n2 \n195 \nWingNut \n2 \n2 \n2,048 \nEMG \n4 \n5 \n1,024 \nIris \n4 \n3 \n1,024 \nSynthetic (DS) \n512 \n10 \n8 \nSynthetic (DS) \n512 \n10 \n8 \n\nTable 2. Store-n-Learn Parameters \n\nCapacity \n1TB \nChannels \n32 \nPage Size \n16KB \nChips/Channel \n4 \nExternal BW \n3.2GBps \nPlanes/Chip \n8 \nBW/Channel \n800MBps \nBlocks/Plane \n512 \nFlash Latency \n53\u03bcs \nPages/Block \n128 \nFPGA \nXCKU025 Scratchpad Size 4MB \nAvg Power/DA \n8mW \nDA Latency \n1.02ns \n\nDA: Die accelerator. \n\n\nACM Transactions on Embedded Computing Systems, Vol. 21, No. 3, Article 22. Publication date: July 2022.\n\nDetection of epileptic seizures from surface EEG using hyperdimensional computing. Fatemeh Asgarinejad, Anthony Thomas, Tajana Rosing, Proceedings of the 2020 42nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC'20). the 2020 42nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC'20)Los Alamitos, CAIEEEFatemeh Asgarinejad, Anthony Thomas, and Tajana Rosing. 2020. Detection of epileptic seizures from surface EEG using hyperdimensional computing. In Proceedings of the 2020 42nd Annual International Conference of the IEEE Engi- neering in Medicine and Biology Society (EMBC'20). IEEE, Los Alamitos, CA, 536-540.\n\nAnalysis of robust implementation of an EMG pattern recognition based control. Simone Benatti, Elisabetta Farella, Emanuele Gruppioni, Luca Benini, Proceedings of the 2014 International Conference on Bio-inspired Systems and Signal Processing. the 2014 International Conference on Bio-inspired Systems and Signal ProcessingBIOSIGNALS'14Simone Benatti, Elisabetta Farella, Emanuele Gruppioni, and Luca Benini. 2014. Analysis of robust implementation of an EMG pattern recognition based control. In Proceedings of the 2014 International Conference on Bio-inspired Systems and Signal Processing (BIOSIGNALS'14). 45-54.\n\nUCI Machine Learning Repository. Cardiotocography Data Set. UCI Machine Learning Repository. 2010. Cardiotocography Data Set. Retrieved February 17, 2022 from https://archive. ics.uci.edu/ml/datasets/cardiotocography.\n\nA flash memory controller for 15\u03bcs ultra-low-latency SSD using high-speed 3D NAND flash with 3\u03bcs read time. Wooseong Cheong, Chanho Yoon, Seonghoon Woo, Kyuwook Han, Daehyun Kim, Chulseung Lee, Youra Choi, Proceedings of the 2018 IEEE International Solid-State Circuits Conference (ISSCC'18). the 2018 IEEE International Solid-State Circuits Conference (ISSCC'18)Los Alamitos, CAIEEEWooseong Cheong, Chanho Yoon, Seonghoon Woo, Kyuwook Han, Daehyun Kim, Chulseung Lee, Youra Choi, et al. 2018. A flash memory controller for 15\u03bcs ultra-low-latency SSD using high-speed 3D NAND flash with 3\u03bcs read time. In Proceedings of the 2018 IEEE International Solid-State Circuits Conference (ISSCC'18). IEEE, Los Alamitos, CA, 338-340.\n\nThe Pascal Visual Object Classes Challenge: A retrospective. S M Mark Everingham, Luc Ali Eslami, Van Gool, K I Christopher, John Williams, Andrew Winn, Zisserman, International Journal of Computer Vision. 111Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. 2015. The Pascal Visual Object Classes Challenge: A retrospective. International Journal of Computer Vision 111, 1 (2015), 98-136.\n\nClassification using hyperdimensional computing: A review. Lulu Ge, Keshab K Parhi, IEEE Circuits and Systems Magazine. 20Lulu Ge and Keshab K. Parhi. 2020. Classification using hyperdimensional computing: A review. IEEE Circuits and Systems Magazine 20, 2 (2020), 30-47.\n\nCaltech-256 Object Category Dataset. Gregory Griffin, Alex Holub, Pietro Perona, RetrievedGregory Griffin, Alex Holub, and Pietro Perona. 2007. Caltech-256 Object Category Dataset. Retrieved February 17, 2022 from https://authors.library.caltech.edu/7694/.\n\nBiscuit: A framework for near-data processing of big data workloads. Boncheol Gu, Andre S Yoon, Duck-Ho Bae, Insoon Jo, Jinyoung Lee, Jonghyun Yoon, Jeong-Uk Kang, ACM SIGARCH Computer Architecture News. 44Boncheol Gu, Andre S. Yoon, Duck-Ho Bae, Insoon Jo, Jinyoung Lee, Jonghyun Yoon, Jeong-Uk Kang, et al. 2016. Biscuit: A framework for near-data processing of big data workloads. ACM SIGARCH Computer Architecture News 44, 3 (2016), 153-165.\n\nHyperRec: Efficient recommender systems with hyperdimensional computing. Yunhui Guo, Mohsen Imani, Jaeyoung Kang, Sahand Salamat, Justin Morris, Baris Aksanli, Yeseong Kim, Tajana Rosing, Proceedings of the 2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC'21). the 2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC'21)Los Alamitos, CAIEEEYunhui Guo, Mohsen Imani, Jaeyoung Kang, Sahand Salamat, Justin Morris, Baris Aksanli, Yeseong Kim, and Tajana Rosing. 2021. HyperRec: Efficient recommender systems with hyperdimensional computing. In Proceedings of the 2021 26th Asia and South Pacific Design Automation Conference (ASP-DAC'21). IEEE, Los Alamitos, CA, 384-389.\n\nFelix: Fast and energy-efficient logic in memory. Saransh Gupta, Mohsen Imani, Tajana Rosing, Proceedings of the 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD'18). the 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD'18)Los Alamitos, CAIEEESaransh Gupta, Mohsen Imani, and Tajana Rosing. 2018. Felix: Fast and energy-efficient logic in memory. In Proceed- ings of the 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD'18). IEEE, Los Alamitos, CA, 1-7.\n\nQuantHD: A quantization framework for hyperdimensional computing. Mohsen Imani, Samuel Bosch, Sohum Datta, Sharadhi Ramakrishna, Sahand Salamat, Jan M Rabaey, Tajana Rosing, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 39Mohsen Imani, Samuel Bosch, Sohum Datta, Sharadhi Ramakrishna, Sahand Salamat, Jan M. Rabaey, and Tajana Rosing. 2020. QuantHD: A quantization framework for hyperdimensional computing. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems 39, 10 (2020), 2268-2278.\n\nHierarchical hyperdimensional computing for energy efficient classification. Mohsen Imani, Chenyu Huang, Deqian Kong, Tajana Rosing, Proceedings of the 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC'18). the 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC'18)Mohsen Imani, Chenyu Huang, Deqian Kong, and Tajana Rosing. 2018. Hierarchical hyperdimensional computing for energy efficient classification. In Proceedings of the 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC'18).\n\n. IEEE. IEEE, Los Alamitos, CA, 1-6.\n\nFarinaz Koushanfar, and Tajana Rosing. 2019. A framework for collaborative learning in secure high-dimensional space. Mohsen Imani, Yeseong Kim, Sadegh Riazi, John Messerly, Patric Liu, Proceedings of the 2019 IEEE 12th International Conference on Cloud Computing (CLOUD'19). the 2019 IEEE 12th International Conference on Cloud Computing (CLOUD'19)Los Alamitos, CAIEEEMohsen Imani, Yeseong Kim, Sadegh Riazi, John Messerly, Patric Liu, Farinaz Koushanfar, and Tajana Rosing. 2019. A framework for collaborative learning in secure high-dimensional space. In Proceedings of the 2019 IEEE 12th Inter- national Conference on Cloud Computing (CLOUD'19). IEEE, Los Alamitos, CA, 435-446.\n\nHDCluster: An accurate clustering using brain-inspired high-dimensional computing. Mohsen Imani, Yeseong Kim, Thomas Worley, Saransh Gupta, Tajana Rosing, Proceedings of the 2019 Design, Automation, and Test in Europe Conference and Exhibition (DATE'19). the 2019 Design, Automation, and Test in Europe Conference and Exhibition (DATE'19)Los Alamitos, CAIEEEMohsen Imani, Yeseong Kim, Thomas Worley, Saransh Gupta, and Tajana Rosing. 2019. HDCluster: An accurate clustering using brain-inspired high-dimensional computing. In Proceedings of the 2019 Design, Automation, and Test in Europe Conference and Exhibition (DATE'19). IEEE, Los Alamitos, CA, 1591-1594.\n\nVoiceHD: Hyperdimensional computing for efficient speech recognition. Mohsen Imani, Deqian Kong, Proceedings of the 2017 IEEE International Conference on Rebooting Computing (ICRC'17). the 2017 IEEE International Conference on Rebooting Computing (ICRC'17)Abbas Rahimi, and Tajana RosingMohsen Imani, Deqian Kong, Abbas Rahimi, and Tajana Rosing. 2017. VoiceHD: Hyperdimensional computing for efficient speech recognition. In Proceedings of the 2017 IEEE International Conference on Rebooting Computing (ICRC'17).\n\n. IEEE. IEEE, Los Alamitos, CA, 1-8.\n\nA binary learning framework for hyperdimensional computing. Mohsen Imani, John Messerly, Fan Wu, Wang Pi, Tajana Rosing, Proceedings of the 2019 Design, Automation, and Test in Europe Conference and Exhibition (DATE'19). the 2019 Design, Automation, and Test in Europe Conference and Exhibition (DATE'19)Los Alamitos, CAIEEEMohsen Imani, John Messerly, Fan Wu, Wang Pi, and Tajana Rosing. 2019. A binary learning framework for hyper- dimensional computing. In Proceedings of the 2019 Design, Automation, and Test in Europe Conference and Exhibition (DATE'19). IEEE, Los Alamitos, CA, 126-131.\n\nHDNA: Energy-efficient DNA sequencing using hyperdimensional computing. Mohsen Imani, Tarek Nassar, Abbas Rahimi, Tajana Rosing, Proceedings of the 2018 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI'18). the 2018 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI'18)Los Alamitos, CAIEEEMohsen Imani, Tarek Nassar, Abbas Rahimi, and Tajana Rosing. 2018. HDNA: Energy-efficient DNA sequencing using hyperdimensional computing. In Proceedings of the 2018 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI'18). IEEE, Los Alamitos, CA, 271-274.\n\nBrain-inspired hyperdimensional computing for real-time health analysis. Mohsen Imani, Tarek Nassar, Tajana Rosing, Proceedings of the 2019 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI'19). the 2019 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI'19)Los Alamitos, CAIEEEMohsen Imani, Tarek Nassar, and Tajana Rosing. 2019. Brain-inspired hyperdimensional computing for real-time health analysis. In Proceedings of the 2019 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI'19). IEEE, Los Alamitos, CA.\n\nExploring hyperdimensional associative memory. Mohsen Imani, Abbas Rahimi, Deqian Kong, Tajana Rosing, Jan M Rabaey, Proceedings of the 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA'17). the 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA'17)Los Alamitos, CAIEEEMohsen Imani, Abbas Rahimi, Deqian Kong, Tajana Rosing, and Jan M. Rabaey. 2017. Exploring hyperdimensional as- sociative memory. In Proceedings of the 2017 IEEE International Symposium on High Performance Computer Architecture (HPCA'17). IEEE, Los Alamitos, CA, 445-456.\n\nDeep learning acceleration with neuron-to-memory transformation. Mohsen Imani, Mohammad Samragh Razlighi, Yeseong Kim, Saransh Gupta, Farinaz Koushanfar, Tajana Rosing, Proceedings of the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA'20). the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA'20)Los Alamitos, CAIEEEMohsen Imani, Mohammad Samragh Razlighi, Yeseong Kim, Saransh Gupta, Farinaz Koushanfar, and Tajana Rosing. 2020. Deep learning acceleration with neuron-to-memory transformation. In Proceedings of the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA'20). IEEE, Los Alamitos, CA, 1-14.\n\nSparseHD: Algorithm-hardware co-optimization for efficient high-dimensional computing. Mohsen Imani, Sahand Salamat, Behnam Khaleghi, Mohammad Samragh, Farinaz Koushanfar, Tajana Rosing, Proceedings of the 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM'19). the 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM'19)Los Alamitos, CAIEEEMohsen Imani, Sahand Salamat, Behnam Khaleghi, Mohammad Samragh, Farinaz Koushanfar, and Tajana Rosing. 2019. SparseHD: Algorithm-hardware co-optimization for efficient high-dimensional computing. In Proceedings of the 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM'19). IEEE, Los Alamitos, CA, 190-198.\n\nUCI Machine Learning Repository. Iris Data Set. RetrievedUCI Machine Learning Repository. 1988. Iris Data Set. Retrieved February 17, 2022 from https://archive.ics.uci.edu/ ml/datasets/iris.\n\nUCI Machine Learning Repository. ISOLET Data Set. UCI Machine Learning Repository. 1994. ISOLET Data Set. Retrieved February 17, 2022 from http://archive.ics.uci.edu/ ml/datasets/ISOLET.\n\nYourSQL: A high-performance database system leveraging in-storage computing. Insoon Jo, Duck-Ho Bae, Andre S Yoon, Jeong-Uk Kang, Sangyeun Cho, D G Daniel, Jaeheon Lee, Jeong, Proceedings of the VLDB Endowment. the VLDB Endowment9Insoon Jo, Duck-Ho Bae, Andre S. Yoon, Jeong-Uk Kang, Sangyeun Cho, Daniel D. G. Lee, and Jaeheon Jeong. 2016. YourSQL: A high-performance database system leveraging in-storage computing. Proceedings of the VLDB Endowment 9, 12 (2016), 924-935.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Pentti Kanerva, Cognitive Computation. 1Pentti Kanerva. 2009. Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Cognitive Computation 1, 2 (2009), 139-159.\n\nIn-memory hyperdimensional computing. Geethan Karunaratne, Manuel Le Gallo, Giovanni Cherubini, Luca Benini, Abbas Rahimi, Abu Sebastian, Nature Electronics. 3Geethan Karunaratne, Manuel Le Gallo, Giovanni Cherubini, Luca Benini, Abbas Rahimi, and Abu Sebastian. 2020. In-memory hyperdimensional computing. Nature Electronics 3, 6 (2020), 327-337.\n\nGenieHD: Efficient DNA pattern matching accelerator using hyperdimensional computing. Yeseong Kim, Mohsen Imani, Niema Moshiri, Tajana Rosing, Proceedings of the 2020 Design, Automation, and Test in Europe Conference and Exhibition (DATE'20). the 2020 Design, Automation, and Test in Europe Conference and Exhibition (DATE'20)Los Alamitos, CAIEEEYeseong Kim, Mohsen Imani, Niema Moshiri, and Tajana Rosing. 2020. GenieHD: Efficient DNA pattern matching accelerator using hyperdimensional computing. In Proceedings of the 2020 Design, Automation, and Test in Europe Conference and Exhibition (DATE'20). IEEE, Los Alamitos, CA, 115-120.\n\nEfficient human activity recognition using hyperdimensional computing. Yeseong Kim, Mohsen Imani, Tajana S Rosing, Proceedings of the 8th International Conference on the Internet of Things. the 8th International Conference on the Internet of ThingsYeseong Kim, Mohsen Imani, and Tajana S. Rosing. 2018. Efficient human activity recognition using hyperdimensional computing. In Proceedings of the 8th International Conference on the Internet of Things. 1-6.\n\nSummarizer: Trading communication with computing near storage. Gunjae Koo, Kiran Kumar Matam, I Te, H V Krishna Giri Narra, Jing Li, Hung-Wei Tseng, Steven Swanson, Murali Annavaram, Proceedings of the. the50Gunjae Koo, Kiran Kumar Matam, I. Te, H. V. Krishna Giri Narra, Jing Li, Hung-Wei Tseng, Steven Swanson, and Murali Annavaram. 2017. Summarizer: Trading communication with computing near storage. In Proceedings of the 2017 50th\n\nAnnual IEEE/ACM International Symposium on Microarchitecture (MICRO'17). Los Alamitos, CAIEEEAnnual IEEE/ACM International Symposium on Microarchitecture (MICRO'17). IEEE, Los Alamitos, CA, 219-231.\n\nKV-Direct: High-performance in-memory key-value store with programmable NIC. Bojie Li, Zhenyuan Ruan, Wencong Xiao, Yuanwei Lu, Yongqiang Xiong, Andrew Putnam, Enhong Chen, Lintao Zhang, Proceedings of the 26th Symposium on Operating Systems Principles. the 26th Symposium on Operating Systems PrinciplesBojie Li, Zhenyuan Ruan, Wencong Xiao, Yuanwei Lu, Yongqiang Xiong, Andrew Putnam, Enhong Chen, and Lintao Zhang. 2017. KV-Direct: High-performance in-memory key-value store with programmable NIC. In Proceedings of the 26th Symposium on Operating Systems Principles. 137-152.\n\nDeepStore: In-storage acceleration for intelligent queries. Zaid Vikram Sharma Mailthody, Weixin Qureshi, Ziyan Liang, Simon Feng, Youjie Garcia De Gonzalo, Hubertus Li, Jinjun Franke, Jian Xiong, Wen-Mei Huang, Hwu, Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture. the 52nd Annual IEEE/ACM International Symposium on MicroarchitectureVikram Sharma Mailthody, Zaid Qureshi, Weixin Liang, Ziyan Feng, Simon Garcia De Gonzalo, Youjie Li, Hubertus Franke, Jinjun Xiong, Jian Huang, and Wen-Mei Hwu. 2019. DeepStore: In-storage acceleration for intelligent queries. In Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture. 224-238.\n\nAn introduction to hyperdimensional computing for robotics. Peer Neubert, Stefan Schubert, Peter Protzel, 33KI-K\u00fcnstliche IntelligenzPeer Neubert, Stefan Schubert, and Peter Protzel. 2019. An introduction to hyperdimensional computing for robotics. KI-K\u00fcnstliche Intelligenz 33, 4 (2019), 319-330.\n\nScikit-learn: Machine learning in Python. Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Journal of Machine Learning Research. 12Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, et al. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12 (2011), 2825- 2830.\n\nHyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition. Abbas Rahimi, Simone Benatti, Pentti Kanerva, Luca Benini, Jan M Rabaey, Proceedings of the 2016 IEEE International Conference on Rebooting Computing (ICRC'16). the 2016 IEEE International Conference on Rebooting Computing (ICRC'16)Los Alamitos, CAIEEEAbbas Rahimi, Simone Benatti, Pentti Kanerva, Luca Benini, and Jan M. Rabaey. 2016. Hyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition. In Proceedings of the 2016 IEEE International Con- ference on Rebooting Computing (ICRC'16). IEEE, Los Alamitos, CA, 1-8.\n\nHyperdimensional computing for noninvasive brain-computer interfaces: Blind and one-shot classification of EEG error-related potentials. Abbas Rahimi, Pentti Kanerva, Jos\u00e9 Del R. Mill\u00e1n, Jan M Rabaey, Proceedings of the 10th EAI International Conference on Bio-inspired Information and Communications Technologies. the 10th EAI International Conference on Bio-inspired Information and Communications TechnologiesAbbas Rahimi, Pentti Kanerva, Jos\u00e9 del R. Mill\u00e1n, and Jan M. Rabaey. 2017. Hyperdimensional computing for nonin- vasive brain-computer interfaces: Blind and one-shot classification of EEG error-related potentials. In Proceedings of the 10th EAI International Conference on Bio-inspired Information and Communications Technologies.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. Abbas Rahimi, Pentti Kanerva, Jan M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design. the 2016 International Symposium on Low Power Electronics and DesignNew York, NYACMAbbas Rahimi, Pentti Kanerva, and Jan M. Rabaey. 2016. A robust and energy-efficient classifier using brain-inspired hyperdimensional computing. In Proceedings of the 2016 International Symposium on Low Power Electronics and Design. ACM, New York, NY, 64-69.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. Abbas Rahimi, Pentti Kanerva, Jan M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design. the 2016 International Symposium on Low Power Electronics and DesignNew York, NYACMAbbas Rahimi, Pentti Kanerva, and Jan M. Rabaey. 2016. A robust and energy-efficient classifier using brain-inspired hyperdimensional computing. In Proceedings of the 2016 International Symposium on Low Power Electronics and Design. ACM, New York, NY, 64-69.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, J Saarinen, 10.1109/TNNLS.2015.2462721IEEE Transactions on Neural Networks and Learning Systems. 27O. Rasanen and J. Saarinen. 2016. Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. IEEE Transactions on Neural Networks and Learning Systems 27, 9 (2016), 1878-1889. https://doi.org/10.1109/TNNLS.2015.2462721\n\nINSIDER: Designing in-storage computing system for emerging high-performance drive. Zhenyuan Ruan, Tong He, Jason Cong, Proceedings of the 2019 USENIX Annual Technical Conference. the 2019 USENIX Annual Technical ConferenceZhenyuan Ruan, Tong He, and Jason Cong. 2019. INSIDER: Designing in-storage computing system for emerging high-performance drive. In Proceedings of the 2019 USENIX Annual Technical Conference. 379-394.\n\nCustomizing neural networks for efficient FPGA implementation. Mohammad Samragh, Mohammad Ghasemzadeh, Farinaz Koushanfar, Proceedings of the 2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM'17). the 2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM'17)Los Alamitos, CAIEEEMohammad Samragh, Mohammad Ghasemzadeh, and Farinaz Koushanfar. 2017. Customizing neural networks for efficient FPGA implementation. In Proceedings of the 2017 IEEE 25th Annual International Symposium on Field- Programmable Custom Computing Machines (FCCM'17). IEEE, Los Alamitos, CA, 85-92.\n\nHardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory. Manuel Schmuck, Luca Benini, Abbas Rahimi, ACM Journal on Emerging Technologies in Computing Systems. 15Manuel Schmuck, Luca Benini, and Abbas Rahimi. 2019. Hardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory. ACM Jour- nal on Emerging Technologies in Computing Systems 15, 4 (2019), 1-25.\n\nWillow: A user-programmable SSD. Sudharsan Seshadri, Mark Gahagan, Sundaram Bhaskaran, Trevor Bunker, Arup De, Yanqin Jin, Yang Liu, Steven Swanson, Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation. the 11th USENIX Conference on Operating Systems Design and ImplementationSudharsan Seshadri, Mark Gahagan, Sundaram Bhaskaran, Trevor Bunker, Arup De, Yanqin Jin, Yang Liu, and Steven Swanson. 2014. Willow: A user-programmable SSD. In Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation. 67-80.\n\nClustering benchmark datasets exploiting the fundamental clustering problems. C Michael, Alfred Thrun, Ultsch, Data in Brief. 30105501Michael C. Thrun and Alfred Ultsch. 2020. Clustering benchmark datasets exploiting the fundamental clustering prob- lems. Data in Brief 30 (2020), 105501.\n\nDaily and Sports Activities Data Set. UCI Machine Learning Repository. RetrievedUCI Machine Learning Repository. 2012. Daily and Sports Activities Data Set. Retrieved February 17, 2022 from https: //archive.ics.uci.edu/ml/datasets/Daily+and+Sports+Activities.\n\nBrain-inspired computing exploiting carbon nanotube FETs and resistive RAM: Hyperdimensional computing case study. Tony F Wu, Haitong Li, Ping-Chen Huang, Abbas Rahimi, Jan M Rabaey, H.-S. Philip Wong, Max M Shulaker, Subhasish Mitra, Proceedings of the 2018 IEEE International Solid-State Circuits Conference (ISSCC'18). the 2018 IEEE International Solid-State Circuits Conference (ISSCC'18)Tony F. Wu, Haitong Li, Ping-Chen Huang, Abbas Rahimi, Jan M. Rabaey, H.-S. Philip Wong, Max M. Shulaker, and Subhasish Mitra. 2018. Brain-inspired computing exploiting carbon nanotube FETs and resistive RAM: Hyperdimen- sional computing case study. In Proceedings of the 2018 IEEE International Solid-State Circuits Conference (ISSCC'18).\n\n. IEEE. IEEE, Los Alamitos, CA, 492-494.\n", "annotations": {"author": "[{\"end\":55,\"start\":23},{\"end\":90,\"start\":56},{\"end\":124,\"start\":91},{\"end\":161,\"start\":125},{\"end\":201,\"start\":162},{\"end\":213,\"start\":202},{\"end\":246,\"start\":214},{\"end\":277,\"start\":247},{\"end\":286,\"start\":278},{\"end\":298,\"start\":287},{\"end\":309,\"start\":299},{\"end\":321,\"start\":310},{\"end\":327,\"start\":322},{\"end\":337,\"start\":328},{\"end\":347,\"start\":338},{\"end\":355,\"start\":348},{\"end\":370,\"start\":356},{\"end\":387,\"start\":371},{\"end\":403,\"start\":388},{\"end\":418,\"start\":404},{\"end\":440,\"start\":419},{\"end\":452,\"start\":441},{\"end\":467,\"start\":453},{\"end\":482,\"start\":468},{\"end\":512,\"start\":483},{\"end\":546,\"start\":513},{\"end\":578,\"start\":547},{\"end\":586,\"start\":579},{\"end\":623,\"start\":587},{\"end\":660,\"start\":624},{\"end\":772,\"start\":661},{\"end\":904,\"start\":773},{\"end\":985,\"start\":905},{\"end\":1069,\"start\":986},{\"end\":1150,\"start\":1070},{\"end\":1226,\"start\":1151}]", "publisher": null, "author_last_name": "[{\"end\":36,\"start\":31},{\"end\":71,\"start\":63},{\"end\":105,\"start\":98},{\"end\":138,\"start\":132},{\"end\":182,\"start\":174},{\"end\":212,\"start\":210},{\"end\":227,\"start\":221},{\"end\":260,\"start\":256},{\"end\":285,\"start\":280},{\"end\":297,\"start\":289},{\"end\":308,\"start\":301},{\"end\":320,\"start\":312},{\"end\":326,\"start\":324},{\"end\":336,\"start\":330},{\"end\":346,\"start\":342},{\"end\":354,\"start\":348},{\"end\":369,\"start\":364},{\"end\":386,\"start\":378},{\"end\":402,\"start\":395},{\"end\":417,\"start\":411},{\"end\":439,\"start\":431},{\"end\":451,\"start\":449},{\"end\":466,\"start\":460},{\"end\":481,\"start\":477},{\"end\":495,\"start\":490},{\"end\":526,\"start\":519},{\"end\":561,\"start\":554}]", "author_first_name": "[{\"end\":30,\"start\":23},{\"end\":62,\"start\":56},{\"end\":97,\"start\":91},{\"end\":131,\"start\":125},{\"end\":173,\"start\":162},{\"end\":209,\"start\":202},{\"end\":220,\"start\":214},{\"end\":255,\"start\":247},{\"end\":279,\"start\":278},{\"end\":288,\"start\":287},{\"end\":300,\"start\":299},{\"end\":311,\"start\":310},{\"end\":323,\"start\":322},{\"end\":329,\"start\":328},{\"end\":339,\"start\":338},{\"end\":341,\"start\":340},{\"end\":363,\"start\":356},{\"end\":377,\"start\":371},{\"end\":394,\"start\":388},{\"end\":410,\"start\":404},{\"end\":430,\"start\":419},{\"end\":448,\"start\":441},{\"end\":459,\"start\":453},{\"end\":476,\"start\":468},{\"end\":489,\"start\":483},{\"end\":518,\"start\":513},{\"end\":553,\"start\":547},{\"end\":585,\"start\":579}]", "author_affiliation": "[{\"end\":622,\"start\":588},{\"end\":659,\"start\":625},{\"end\":771,\"start\":662},{\"end\":903,\"start\":774},{\"end\":984,\"start\":906},{\"end\":1068,\"start\":987},{\"end\":1149,\"start\":1071},{\"end\":1225,\"start\":1152}]", "title": "[{\"end\":11,\"start\":1},{\"end\":1237,\"start\":1227}]", "venue": "[{\"end\":1285,\"start\":1239}]", "abstract": "[{\"end\":6977,\"start\":1862}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8870,\"start\":8866},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8895,\"start\":8891},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8919,\"start\":8915},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8949,\"start\":8945},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8984,\"start\":8980},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9017,\"start\":9013},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9181,\"start\":9177},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9297,\"start\":9293},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9529,\"start\":9525},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9532,\"start\":9529},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9701,\"start\":9697},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9704,\"start\":9701},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9707,\"start\":9704},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10167,\"start\":10163},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10597,\"start\":10593},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10670,\"start\":10666},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10673,\"start\":10670},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10788,\"start\":10785},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10791,\"start\":10788},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10794,\"start\":10791},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10865,\"start\":10861},{\"end\":11144,\"start\":11132},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11518,\"start\":11514},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11521,\"start\":11518},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11905,\"start\":11901},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11922,\"start\":11918},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12062,\"start\":12058},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12090,\"start\":12086},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12131,\"start\":12127},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12160,\"start\":12157},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12749,\"start\":12745},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12752,\"start\":12749},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17094,\"start\":17091},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17150,\"start\":17146},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17409,\"start\":17406},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17431,\"start\":17427},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17544,\"start\":17540},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18026,\"start\":18022},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18119,\"start\":18115},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18803,\"start\":18799},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18855,\"start\":18851},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26295,\"start\":26292},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36182,\"start\":36178},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":41732,\"start\":41728},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":42076,\"start\":42072},{\"end\":46941,\"start\":46936},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":46943,\"start\":46941},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":46945,\"start\":46943},{\"end\":46947,\"start\":46945},{\"end\":46957,\"start\":46952},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":46959,\"start\":46957},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":46961,\"start\":46959},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46963,\"start\":46961},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":49557,\"start\":49553},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":49576,\"start\":49572},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":50260,\"start\":50256},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":50341,\"start\":50338},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":50456,\"start\":50453},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":50606,\"start\":50602},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":50738,\"start\":50735},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":50837,\"start\":50834},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":50935,\"start\":50931},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":51053,\"start\":51049},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":51141,\"start\":51137},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":51301,\"start\":51297},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":51514,\"start\":51510},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":51702,\"start\":51698},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":51962,\"start\":51958},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":56490,\"start\":56486},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":56493,\"start\":56490},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":57878,\"start\":57874},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":60146,\"start\":60142},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":60641,\"start\":60637},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":60813,\"start\":60809},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":61050,\"start\":61046},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":61069,\"start\":61065},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":61927,\"start\":61923},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":61946,\"start\":61942},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":67557,\"start\":67553},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":67893,\"start\":67889},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":67911,\"start\":67907},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":68012,\"start\":68008},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":68030,\"start\":68026}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":65660,\"start\":65564},{\"attributes\":{\"id\":\"fig_1\"},\"end\":65702,\"start\":65661},{\"attributes\":{\"id\":\"fig_2\"},\"end\":65845,\"start\":65703},{\"attributes\":{\"id\":\"fig_4\"},\"end\":65946,\"start\":65846},{\"attributes\":{\"id\":\"fig_5\"},\"end\":66286,\"start\":65947},{\"attributes\":{\"id\":\"fig_6\"},\"end\":66461,\"start\":66287},{\"attributes\":{\"id\":\"fig_7\"},\"end\":66601,\"start\":66462},{\"attributes\":{\"id\":\"fig_8\"},\"end\":66943,\"start\":66602},{\"attributes\":{\"id\":\"fig_9\"},\"end\":67175,\"start\":66944},{\"attributes\":{\"id\":\"fig_10\"},\"end\":67396,\"start\":67176},{\"attributes\":{\"id\":\"fig_11\"},\"end\":67517,\"start\":67397},{\"attributes\":{\"id\":\"fig_12\"},\"end\":67652,\"start\":67518},{\"attributes\":{\"id\":\"fig_13\"},\"end\":67773,\"start\":67653},{\"attributes\":{\"id\":\"fig_14\"},\"end\":67912,\"start\":67774},{\"attributes\":{\"id\":\"fig_15\"},\"end\":68031,\"start\":67913},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":68711,\"start\":68032}]", "paragraph": "[{\"end\":7504,\"start\":6979},{\"end\":7600,\"start\":7506},{\"end\":8713,\"start\":7602},{\"end\":10442,\"start\":8730},{\"end\":11348,\"start\":10444},{\"end\":12908,\"start\":11379},{\"end\":12984,\"start\":12921},{\"end\":13488,\"start\":13125},{\"end\":13960,\"start\":13504},{\"end\":14400,\"start\":13992},{\"end\":14979,\"start\":14472},{\"end\":15122,\"start\":15021},{\"end\":15915,\"start\":15151},{\"end\":16513,\"start\":15930},{\"end\":16855,\"start\":16545},{\"end\":17944,\"start\":16909},{\"end\":18648,\"start\":17959},{\"end\":19442,\"start\":18650},{\"end\":21371,\"start\":19467},{\"end\":22176,\"start\":21413},{\"end\":23428,\"start\":22241},{\"end\":24477,\"start\":23430},{\"end\":25762,\"start\":24479},{\"end\":26570,\"start\":25805},{\"end\":28632,\"start\":26605},{\"end\":29707,\"start\":28656},{\"end\":30344,\"start\":29741},{\"end\":30509,\"start\":30346},{\"end\":30874,\"start\":30535},{\"end\":31692,\"start\":30917},{\"end\":32104,\"start\":31694},{\"end\":32866,\"start\":32172},{\"end\":33386,\"start\":32868},{\"end\":33770,\"start\":33407},{\"end\":34425,\"start\":33772},{\"end\":35800,\"start\":34427},{\"end\":36891,\"start\":35854},{\"end\":37741,\"start\":36913},{\"end\":38374,\"start\":37771},{\"end\":39690,\"start\":38376},{\"end\":41651,\"start\":39706},{\"end\":42880,\"start\":41672},{\"end\":44377,\"start\":42898},{\"end\":44855,\"start\":44406},{\"end\":45780,\"start\":44857},{\"end\":47109,\"start\":45782},{\"end\":47587,\"start\":47111},{\"end\":48586,\"start\":47641},{\"end\":49169,\"start\":48619},{\"end\":49984,\"start\":49171},{\"end\":50146,\"start\":50016},{\"end\":50839,\"start\":50148},{\"end\":51124,\"start\":50855},{\"end\":52186,\"start\":51126},{\"end\":52592,\"start\":52225},{\"end\":53538,\"start\":52632},{\"end\":56304,\"start\":53540},{\"end\":58319,\"start\":56362},{\"end\":58941,\"start\":58348},{\"end\":59921,\"start\":58943},{\"end\":60739,\"start\":59959},{\"end\":61070,\"start\":60741},{\"end\":61765,\"start\":61072},{\"end\":64801,\"start\":61808},{\"end\":65563,\"start\":64906}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13124,\"start\":12985},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13503,\"start\":13489},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14443,\"start\":14401},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15000,\"start\":14980},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15020,\"start\":15000},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16544,\"start\":16514},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22240,\"start\":22177},{\"attributes\":{\"id\":\"formula_7\"},\"end\":29740,\"start\":29708},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30534,\"start\":30510},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32171,\"start\":32105}]", "table_ref": "[{\"end\":49168,\"start\":49161},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":50129,\"start\":50122},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":51107,\"start\":51100}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":8728,\"start\":8716},{\"attributes\":{\"n\":\"3\"},\"end\":11377,\"start\":11351},{\"attributes\":{\"n\":\"3.1\"},\"end\":12919,\"start\":12911},{\"attributes\":{\"n\":\"3.2\"},\"end\":13990,\"start\":13963},{\"attributes\":{\"n\":\"3.3\"},\"end\":14470,\"start\":14445},{\"attributes\":{\"n\":\"3.4\"},\"end\":15149,\"start\":15125},{\"attributes\":{\"n\":\"3.5\"},\"end\":15928,\"start\":15918},{\"attributes\":{\"n\":\"3.6\"},\"end\":16907,\"start\":16858},{\"attributes\":{\"n\":\"3.7\"},\"end\":17957,\"start\":17947},{\"attributes\":{\"n\":\"4\"},\"end\":19465,\"start\":19445},{\"attributes\":{\"n\":\"4.1\"},\"end\":21411,\"start\":21374},{\"attributes\":{\"n\":\"4.2\"},\"end\":25803,\"start\":25765},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":26603,\"start\":26573},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":28654,\"start\":28635},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":30915,\"start\":30877},{\"attributes\":{\"n\":\"4.2.4\"},\"end\":33405,\"start\":33389},{\"attributes\":{\"n\":\"4.3\"},\"end\":35852,\"start\":35803},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":36911,\"start\":36894},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":37769,\"start\":37744},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":39704,\"start\":39693},{\"attributes\":{\"n\":\"4.4\"},\"end\":41670,\"start\":41654},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":42896,\"start\":42883},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":44404,\"start\":44380},{\"attributes\":{\"n\":\"4.5\"},\"end\":47639,\"start\":47590},{\"attributes\":{\"n\":\"5\"},\"end\":48596,\"start\":48589},{\"attributes\":{\"n\":\"5.1\"},\"end\":48617,\"start\":48599},{\"attributes\":{\"n\":\"5.2\"},\"end\":49996,\"start\":49987},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":50014,\"start\":49999},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":50853,\"start\":50842},{\"attributes\":{\"n\":\"5.3\"},\"end\":52223,\"start\":52189},{\"attributes\":{\"n\":\"5.3.1\"},\"end\":52630,\"start\":52595},{\"attributes\":{\"n\":\"5.3.2\"},\"end\":56342,\"start\":56307},{\"attributes\":{\"n\":\"5.3.3\"},\"end\":56360,\"start\":56345},{\"attributes\":{\"n\":\"5.4\"},\"end\":58346,\"start\":58322},{\"attributes\":{\"n\":\"5.5\"},\"end\":59957,\"start\":59924},{\"attributes\":{\"n\":\"5.6\"},\"end\":61806,\"start\":61768},{\"attributes\":{\"n\":\"5.6.1\"},\"end\":64835,\"start\":64804},{\"attributes\":{\"n\":\"5.6.2\"},\"end\":64873,\"start\":64838},{\"attributes\":{\"n\":\"5.6.3\"},\"end\":64891,\"start\":64876},{\"attributes\":{\"n\":\"6\"},\"end\":64904,\"start\":64894},{\"end\":65573,\"start\":65565},{\"end\":65670,\"start\":65662},{\"end\":65712,\"start\":65704},{\"end\":65857,\"start\":65847},{\"end\":65956,\"start\":65948},{\"end\":66296,\"start\":66288},{\"end\":66471,\"start\":66463},{\"end\":66611,\"start\":66603},{\"end\":66953,\"start\":66945},{\"end\":67185,\"start\":67177},{\"end\":67407,\"start\":67398},{\"end\":67528,\"start\":67519},{\"end\":67663,\"start\":67654},{\"end\":67784,\"start\":67775},{\"end\":67923,\"start\":67914},{\"end\":68042,\"start\":68033}]", "table": "[{\"end\":68711,\"start\":68060}]", "figure_caption": "[{\"end\":65660,\"start\":65575},{\"end\":65702,\"start\":65672},{\"end\":65845,\"start\":65714},{\"end\":65946,\"start\":65859},{\"end\":66286,\"start\":65958},{\"end\":66461,\"start\":66298},{\"end\":66601,\"start\":66473},{\"end\":66943,\"start\":66613},{\"end\":67175,\"start\":66955},{\"end\":67396,\"start\":67187},{\"end\":67517,\"start\":67410},{\"end\":67652,\"start\":67531},{\"end\":67773,\"start\":67666},{\"end\":67912,\"start\":67787},{\"end\":68031,\"start\":67926},{\"end\":68060,\"start\":68044}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20430,\"start\":20422},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20701,\"start\":20693},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20906,\"start\":20898},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21102,\"start\":21094},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25962,\"start\":25954},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26449,\"start\":26441},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27736,\"start\":27728},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27838,\"start\":27830},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28070,\"start\":28062},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29585,\"start\":29577},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30215,\"start\":30207},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34566,\"start\":34558},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35337,\"start\":35329},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38167,\"start\":38159},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":39357,\"start\":39349},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40719,\"start\":40711},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":45210,\"start\":45202},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":46112,\"start\":46104},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":46450,\"start\":46442},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":46926,\"start\":46918},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":47586,\"start\":47578},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":52728,\"start\":52720},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":53576,\"start\":53568},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":54183,\"start\":54175},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":54687,\"start\":54679},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":55762,\"start\":55754},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":56897,\"start\":56888},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":57973,\"start\":57964},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":58545,\"start\":58533},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59066,\"start\":59057},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59608,\"start\":59599},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60938,\"start\":60931},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":62377,\"start\":62368},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":63313,\"start\":63304},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":64313,\"start\":64304}]", "bib_author_first_name": "[{\"end\":68908,\"start\":68901},{\"end\":68929,\"start\":68922},{\"end\":68944,\"start\":68938},{\"end\":69609,\"start\":69603},{\"end\":69629,\"start\":69619},{\"end\":69647,\"start\":69639},{\"end\":69663,\"start\":69659},{\"end\":70476,\"start\":70468},{\"end\":70491,\"start\":70485},{\"end\":70507,\"start\":70498},{\"end\":70520,\"start\":70513},{\"end\":70533,\"start\":70526},{\"end\":70548,\"start\":70539},{\"end\":70559,\"start\":70554},{\"end\":71148,\"start\":71147},{\"end\":71150,\"start\":71149},{\"end\":71171,\"start\":71168},{\"end\":71195,\"start\":71194},{\"end\":71197,\"start\":71196},{\"end\":71215,\"start\":71211},{\"end\":71232,\"start\":71226},{\"end\":71600,\"start\":71596},{\"end\":71611,\"start\":71605},{\"end\":71613,\"start\":71612},{\"end\":71854,\"start\":71847},{\"end\":71868,\"start\":71864},{\"end\":71882,\"start\":71876},{\"end\":72145,\"start\":72137},{\"end\":72155,\"start\":72150},{\"end\":72157,\"start\":72156},{\"end\":72171,\"start\":72164},{\"end\":72183,\"start\":72177},{\"end\":72196,\"start\":72188},{\"end\":72210,\"start\":72202},{\"end\":72225,\"start\":72217},{\"end\":72594,\"start\":72588},{\"end\":72606,\"start\":72600},{\"end\":72622,\"start\":72614},{\"end\":72635,\"start\":72629},{\"end\":72651,\"start\":72645},{\"end\":72665,\"start\":72660},{\"end\":72682,\"start\":72675},{\"end\":72694,\"start\":72688},{\"end\":73283,\"start\":73276},{\"end\":73297,\"start\":73291},{\"end\":73311,\"start\":73305},{\"end\":73819,\"start\":73813},{\"end\":73833,\"start\":73827},{\"end\":73846,\"start\":73841},{\"end\":73862,\"start\":73854},{\"end\":73882,\"start\":73876},{\"end\":73895,\"start\":73892},{\"end\":73897,\"start\":73896},{\"end\":73912,\"start\":73906},{\"end\":74376,\"start\":74370},{\"end\":74390,\"start\":74384},{\"end\":74404,\"start\":74398},{\"end\":74417,\"start\":74411},{\"end\":74964,\"start\":74958},{\"end\":74979,\"start\":74972},{\"end\":74991,\"start\":74985},{\"end\":75003,\"start\":74999},{\"end\":75020,\"start\":75014},{\"end\":75613,\"start\":75607},{\"end\":75628,\"start\":75621},{\"end\":75640,\"start\":75634},{\"end\":75656,\"start\":75649},{\"end\":75670,\"start\":75664},{\"end\":76262,\"start\":76256},{\"end\":76276,\"start\":76270},{\"end\":76805,\"start\":76799},{\"end\":76817,\"start\":76813},{\"end\":76831,\"start\":76828},{\"end\":76840,\"start\":76836},{\"end\":76851,\"start\":76845},{\"end\":77411,\"start\":77405},{\"end\":77424,\"start\":77419},{\"end\":77438,\"start\":77433},{\"end\":77453,\"start\":77447},{\"end\":78038,\"start\":78032},{\"end\":78051,\"start\":78046},{\"end\":78066,\"start\":78060},{\"end\":78603,\"start\":78597},{\"end\":78616,\"start\":78611},{\"end\":78631,\"start\":78625},{\"end\":78644,\"start\":78638},{\"end\":78656,\"start\":78653},{\"end\":78658,\"start\":78657},{\"end\":79226,\"start\":79220},{\"end\":79242,\"start\":79234},{\"end\":79268,\"start\":79261},{\"end\":79281,\"start\":79274},{\"end\":79296,\"start\":79289},{\"end\":79315,\"start\":79309},{\"end\":79951,\"start\":79945},{\"end\":79965,\"start\":79959},{\"end\":79981,\"start\":79975},{\"end\":80000,\"start\":79992},{\"end\":80017,\"start\":80010},{\"end\":80036,\"start\":80030},{\"end\":81117,\"start\":81111},{\"end\":81129,\"start\":81122},{\"end\":81140,\"start\":81135},{\"end\":81142,\"start\":81141},{\"end\":81157,\"start\":81149},{\"end\":81172,\"start\":81164},{\"end\":81179,\"start\":81178},{\"end\":81181,\"start\":81180},{\"end\":81197,\"start\":81190},{\"end\":81641,\"start\":81635},{\"end\":81912,\"start\":81905},{\"end\":81932,\"start\":81926},{\"end\":81935,\"start\":81933},{\"end\":81951,\"start\":81943},{\"end\":81967,\"start\":81963},{\"end\":81981,\"start\":81976},{\"end\":81993,\"start\":81990},{\"end\":82309,\"start\":82302},{\"end\":82321,\"start\":82315},{\"end\":82334,\"start\":82329},{\"end\":82350,\"start\":82344},{\"end\":82930,\"start\":82923},{\"end\":82942,\"start\":82936},{\"end\":82956,\"start\":82950},{\"end\":82958,\"start\":82957},{\"end\":83379,\"start\":83373},{\"end\":83390,\"start\":83385},{\"end\":83405,\"start\":83404},{\"end\":83411,\"start\":83410},{\"end\":83413,\"start\":83412},{\"end\":83438,\"start\":83434},{\"end\":83451,\"start\":83443},{\"end\":83465,\"start\":83459},{\"end\":83481,\"start\":83475},{\"end\":84029,\"start\":84024},{\"end\":84042,\"start\":84034},{\"end\":84056,\"start\":84049},{\"end\":84070,\"start\":84063},{\"end\":84084,\"start\":84075},{\"end\":84098,\"start\":84092},{\"end\":84113,\"start\":84107},{\"end\":84126,\"start\":84120},{\"end\":84592,\"start\":84588},{\"end\":84624,\"start\":84618},{\"end\":84639,\"start\":84634},{\"end\":84652,\"start\":84647},{\"end\":84665,\"start\":84659},{\"end\":84693,\"start\":84685},{\"end\":84704,\"start\":84698},{\"end\":84717,\"start\":84713},{\"end\":84732,\"start\":84725},{\"end\":85290,\"start\":85286},{\"end\":85306,\"start\":85300},{\"end\":85322,\"start\":85317},{\"end\":85573,\"start\":85567},{\"end\":85589,\"start\":85585},{\"end\":85610,\"start\":85601},{\"end\":85628,\"start\":85621},{\"end\":85645,\"start\":85637},{\"end\":85662,\"start\":85655},{\"end\":85678,\"start\":85671},{\"end\":86062,\"start\":86057},{\"end\":86077,\"start\":86071},{\"end\":86093,\"start\":86087},{\"end\":86107,\"start\":86103},{\"end\":86119,\"start\":86116},{\"end\":86121,\"start\":86120},{\"end\":86750,\"start\":86745},{\"end\":86765,\"start\":86759},{\"end\":86779,\"start\":86775},{\"end\":86798,\"start\":86795},{\"end\":86800,\"start\":86799},{\"end\":87447,\"start\":87442},{\"end\":87462,\"start\":87456},{\"end\":87475,\"start\":87472},{\"end\":87477,\"start\":87476},{\"end\":88009,\"start\":88004},{\"end\":88024,\"start\":88018},{\"end\":88037,\"start\":88034},{\"end\":88039,\"start\":88038},{\"end\":88599,\"start\":88598},{\"end\":88610,\"start\":88609},{\"end\":89083,\"start\":89075},{\"end\":89094,\"start\":89090},{\"end\":89104,\"start\":89099},{\"end\":89488,\"start\":89480},{\"end\":89506,\"start\":89498},{\"end\":89527,\"start\":89520},{\"end\":90250,\"start\":90244},{\"end\":90264,\"start\":90260},{\"end\":90278,\"start\":90273},{\"end\":90684,\"start\":90675},{\"end\":90699,\"start\":90695},{\"end\":90717,\"start\":90709},{\"end\":90735,\"start\":90729},{\"end\":90748,\"start\":90744},{\"end\":90759,\"start\":90753},{\"end\":90769,\"start\":90765},{\"end\":90781,\"start\":90775},{\"end\":91293,\"start\":91292},{\"end\":91309,\"start\":91303},{\"end\":91884,\"start\":91880},{\"end\":91886,\"start\":91885},{\"end\":91898,\"start\":91891},{\"end\":91912,\"start\":91903},{\"end\":91925,\"start\":91920},{\"end\":91937,\"start\":91934},{\"end\":91939,\"start\":91938},{\"end\":91960,\"start\":91948},{\"end\":91970,\"start\":91967},{\"end\":91972,\"start\":91971},{\"end\":91992,\"start\":91983}]", "bib_author_last_name": "[{\"end\":68920,\"start\":68909},{\"end\":68936,\"start\":68930},{\"end\":68951,\"start\":68945},{\"end\":69617,\"start\":69610},{\"end\":69637,\"start\":69630},{\"end\":69657,\"start\":69648},{\"end\":69670,\"start\":69664},{\"end\":70483,\"start\":70477},{\"end\":70496,\"start\":70492},{\"end\":70511,\"start\":70508},{\"end\":70524,\"start\":70521},{\"end\":70537,\"start\":70534},{\"end\":70552,\"start\":70549},{\"end\":70564,\"start\":70560},{\"end\":71166,\"start\":71151},{\"end\":71182,\"start\":71172},{\"end\":71192,\"start\":71184},{\"end\":71209,\"start\":71198},{\"end\":71224,\"start\":71216},{\"end\":71237,\"start\":71233},{\"end\":71248,\"start\":71239},{\"end\":71603,\"start\":71601},{\"end\":71619,\"start\":71614},{\"end\":71862,\"start\":71855},{\"end\":71874,\"start\":71869},{\"end\":71889,\"start\":71883},{\"end\":72148,\"start\":72146},{\"end\":72162,\"start\":72158},{\"end\":72175,\"start\":72172},{\"end\":72186,\"start\":72184},{\"end\":72200,\"start\":72197},{\"end\":72215,\"start\":72211},{\"end\":72230,\"start\":72226},{\"end\":72598,\"start\":72595},{\"end\":72612,\"start\":72607},{\"end\":72627,\"start\":72623},{\"end\":72643,\"start\":72636},{\"end\":72658,\"start\":72652},{\"end\":72673,\"start\":72666},{\"end\":72686,\"start\":72683},{\"end\":72701,\"start\":72695},{\"end\":73289,\"start\":73284},{\"end\":73303,\"start\":73298},{\"end\":73318,\"start\":73312},{\"end\":73825,\"start\":73820},{\"end\":73839,\"start\":73834},{\"end\":73852,\"start\":73847},{\"end\":73874,\"start\":73863},{\"end\":73890,\"start\":73883},{\"end\":73904,\"start\":73898},{\"end\":73919,\"start\":73913},{\"end\":74382,\"start\":74377},{\"end\":74396,\"start\":74391},{\"end\":74409,\"start\":74405},{\"end\":74424,\"start\":74418},{\"end\":74970,\"start\":74965},{\"end\":74983,\"start\":74980},{\"end\":74997,\"start\":74992},{\"end\":75012,\"start\":75004},{\"end\":75024,\"start\":75021},{\"end\":75619,\"start\":75614},{\"end\":75632,\"start\":75629},{\"end\":75647,\"start\":75641},{\"end\":75662,\"start\":75657},{\"end\":75677,\"start\":75671},{\"end\":76268,\"start\":76263},{\"end\":76281,\"start\":76277},{\"end\":76811,\"start\":76806},{\"end\":76826,\"start\":76818},{\"end\":76834,\"start\":76832},{\"end\":76843,\"start\":76841},{\"end\":76858,\"start\":76852},{\"end\":77417,\"start\":77412},{\"end\":77431,\"start\":77425},{\"end\":77445,\"start\":77439},{\"end\":77460,\"start\":77454},{\"end\":78044,\"start\":78039},{\"end\":78058,\"start\":78052},{\"end\":78073,\"start\":78067},{\"end\":78609,\"start\":78604},{\"end\":78623,\"start\":78617},{\"end\":78636,\"start\":78632},{\"end\":78651,\"start\":78645},{\"end\":78665,\"start\":78659},{\"end\":79232,\"start\":79227},{\"end\":79259,\"start\":79243},{\"end\":79272,\"start\":79269},{\"end\":79287,\"start\":79282},{\"end\":79307,\"start\":79297},{\"end\":79322,\"start\":79316},{\"end\":79957,\"start\":79952},{\"end\":79973,\"start\":79966},{\"end\":79990,\"start\":79982},{\"end\":80008,\"start\":80001},{\"end\":80028,\"start\":80018},{\"end\":80043,\"start\":80037},{\"end\":81120,\"start\":81118},{\"end\":81133,\"start\":81130},{\"end\":81147,\"start\":81143},{\"end\":81162,\"start\":81158},{\"end\":81176,\"start\":81173},{\"end\":81188,\"start\":81182},{\"end\":81201,\"start\":81198},{\"end\":81208,\"start\":81203},{\"end\":81649,\"start\":81642},{\"end\":81924,\"start\":81913},{\"end\":81941,\"start\":81936},{\"end\":81961,\"start\":81952},{\"end\":81974,\"start\":81968},{\"end\":81988,\"start\":81982},{\"end\":82003,\"start\":81994},{\"end\":82313,\"start\":82310},{\"end\":82327,\"start\":82322},{\"end\":82342,\"start\":82335},{\"end\":82357,\"start\":82351},{\"end\":82934,\"start\":82931},{\"end\":82948,\"start\":82943},{\"end\":82965,\"start\":82959},{\"end\":83383,\"start\":83380},{\"end\":83402,\"start\":83391},{\"end\":83408,\"start\":83406},{\"end\":83432,\"start\":83414},{\"end\":83441,\"start\":83439},{\"end\":83457,\"start\":83452},{\"end\":83473,\"start\":83466},{\"end\":83491,\"start\":83482},{\"end\":84032,\"start\":84030},{\"end\":84047,\"start\":84043},{\"end\":84061,\"start\":84057},{\"end\":84073,\"start\":84071},{\"end\":84090,\"start\":84085},{\"end\":84105,\"start\":84099},{\"end\":84118,\"start\":84114},{\"end\":84132,\"start\":84127},{\"end\":84616,\"start\":84593},{\"end\":84632,\"start\":84625},{\"end\":84645,\"start\":84640},{\"end\":84657,\"start\":84653},{\"end\":84683,\"start\":84666},{\"end\":84696,\"start\":84694},{\"end\":84711,\"start\":84705},{\"end\":84723,\"start\":84718},{\"end\":84738,\"start\":84733},{\"end\":84743,\"start\":84740},{\"end\":85298,\"start\":85291},{\"end\":85315,\"start\":85307},{\"end\":85330,\"start\":85323},{\"end\":85583,\"start\":85574},{\"end\":85599,\"start\":85590},{\"end\":85619,\"start\":85611},{\"end\":85635,\"start\":85629},{\"end\":85653,\"start\":85646},{\"end\":85669,\"start\":85663},{\"end\":85686,\"start\":85679},{\"end\":86069,\"start\":86063},{\"end\":86085,\"start\":86078},{\"end\":86101,\"start\":86094},{\"end\":86114,\"start\":86108},{\"end\":86128,\"start\":86122},{\"end\":86757,\"start\":86751},{\"end\":86773,\"start\":86766},{\"end\":86793,\"start\":86780},{\"end\":86807,\"start\":86801},{\"end\":87454,\"start\":87448},{\"end\":87470,\"start\":87463},{\"end\":87484,\"start\":87478},{\"end\":88016,\"start\":88010},{\"end\":88032,\"start\":88025},{\"end\":88046,\"start\":88040},{\"end\":88607,\"start\":88600},{\"end\":88619,\"start\":88611},{\"end\":89088,\"start\":89084},{\"end\":89097,\"start\":89095},{\"end\":89109,\"start\":89105},{\"end\":89496,\"start\":89489},{\"end\":89518,\"start\":89507},{\"end\":89538,\"start\":89528},{\"end\":90258,\"start\":90251},{\"end\":90271,\"start\":90265},{\"end\":90285,\"start\":90279},{\"end\":90693,\"start\":90685},{\"end\":90707,\"start\":90700},{\"end\":90727,\"start\":90718},{\"end\":90742,\"start\":90736},{\"end\":90751,\"start\":90749},{\"end\":90763,\"start\":90760},{\"end\":90773,\"start\":90770},{\"end\":90789,\"start\":90782},{\"end\":91301,\"start\":91294},{\"end\":91315,\"start\":91310},{\"end\":91323,\"start\":91317},{\"end\":91889,\"start\":91887},{\"end\":91901,\"start\":91899},{\"end\":91918,\"start\":91913},{\"end\":91932,\"start\":91926},{\"end\":91946,\"start\":91940},{\"end\":91965,\"start\":91961},{\"end\":91981,\"start\":91973},{\"end\":91998,\"start\":91993}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":221388379},\"end\":69522,\"start\":68818},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15953456},\"end\":70139,\"start\":69524},{\"attributes\":{\"id\":\"b2\"},\"end\":70358,\"start\":70141},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3868842},\"end\":71084,\"start\":70360},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207252270},\"end\":71535,\"start\":71086},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":216080530},\"end\":71808,\"start\":71537},{\"attributes\":{\"id\":\"b6\"},\"end\":72066,\"start\":71810},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14108659},\"end\":72513,\"start\":72068},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":231730928},\"end\":73224,\"start\":72515},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":53235957},\"end\":73745,\"start\":73226},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":211016154},\"end\":74291,\"start\":73747},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":49301394},\"end\":74800,\"start\":74293},{\"attributes\":{\"id\":\"b12\"},\"end\":74838,\"start\":74802},{\"attributes\":{\"id\":\"b13\"},\"end\":75522,\"start\":74840},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":155106744},\"end\":76184,\"start\":75524},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":21351739},\"end\":76699,\"start\":76186},{\"attributes\":{\"id\":\"b16\"},\"end\":76737,\"start\":76701},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":155109576},\"end\":77331,\"start\":76739},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4708051},\"end\":77957,\"start\":77333},{\"attributes\":{\"id\":\"b19\"},\"end\":78548,\"start\":77959},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1677864},\"end\":79153,\"start\":78550},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":208154666},\"end\":79856,\"start\":79155},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":189824904},\"end\":80652,\"start\":79858},{\"attributes\":{\"id\":\"b23\"},\"end\":80844,\"start\":80654},{\"attributes\":{\"id\":\"b24\"},\"end\":81032,\"start\":80846},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":18204650},\"end\":81508,\"start\":81034},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":733980},\"end\":81865,\"start\":81510},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":174797921},\"end\":82214,\"start\":81867},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":219858990},\"end\":82850,\"start\":82216},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":52978766},\"end\":83308,\"start\":82852},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":21502412},\"end\":83745,\"start\":83310},{\"attributes\":{\"id\":\"b31\"},\"end\":83945,\"start\":83747},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":28859462},\"end\":84526,\"start\":83947},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":202758896},\"end\":85224,\"start\":84528},{\"attributes\":{\"id\":\"b34\"},\"end\":85523,\"start\":85226},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10659969},\"end\":85963,\"start\":85525},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":12008695},\"end\":86606,\"start\":85965},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":8996877},\"end\":87350,\"start\":86608},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":9812826},\"end\":87912,\"start\":87352},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":9812826},\"end\":88474,\"start\":87914},{\"attributes\":{\"doi\":\"10.1109/TNNLS.2015.2462721\",\"id\":\"b40\",\"matched_paper_id\":15258913},\"end\":88989,\"start\":88476},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":196810101},\"end\":89415,\"start\":88991},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5968842},\"end\":90082,\"start\":89417},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":49907924},\"end\":90640,\"start\":90084},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":1491271},\"end\":91212,\"start\":90642},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":218520024},\"end\":91502,\"start\":91214},{\"attributes\":{\"id\":\"b46\"},\"end\":91763,\"start\":91504},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3869844},\"end\":92496,\"start\":91765},{\"attributes\":{\"id\":\"b48\"},\"end\":92538,\"start\":92498}]", "bib_title": "[{\"end\":68899,\"start\":68818},{\"end\":69601,\"start\":69524},{\"end\":70172,\"start\":70141},{\"end\":70466,\"start\":70360},{\"end\":71145,\"start\":71086},{\"end\":71594,\"start\":71537},{\"end\":72135,\"start\":72068},{\"end\":72586,\"start\":72515},{\"end\":73274,\"start\":73226},{\"end\":73811,\"start\":73747},{\"end\":74368,\"start\":74293},{\"end\":74956,\"start\":74840},{\"end\":75605,\"start\":75524},{\"end\":76254,\"start\":76186},{\"end\":76797,\"start\":76739},{\"end\":77403,\"start\":77333},{\"end\":78030,\"start\":77959},{\"end\":78595,\"start\":78550},{\"end\":79218,\"start\":79155},{\"end\":79943,\"start\":79858},{\"end\":80877,\"start\":80846},{\"end\":81109,\"start\":81034},{\"end\":81633,\"start\":81510},{\"end\":81903,\"start\":81867},{\"end\":82300,\"start\":82216},{\"end\":82921,\"start\":82852},{\"end\":83371,\"start\":83310},{\"end\":84022,\"start\":83947},{\"end\":84586,\"start\":84528},{\"end\":85565,\"start\":85525},{\"end\":86055,\"start\":85965},{\"end\":86743,\"start\":86608},{\"end\":87440,\"start\":87352},{\"end\":88002,\"start\":87914},{\"end\":88596,\"start\":88476},{\"end\":89073,\"start\":88991},{\"end\":89478,\"start\":89417},{\"end\":90242,\"start\":90084},{\"end\":90673,\"start\":90642},{\"end\":91290,\"start\":91214},{\"end\":91878,\"start\":91765}]", "bib_author": "[{\"end\":68922,\"start\":68901},{\"end\":68938,\"start\":68922},{\"end\":68953,\"start\":68938},{\"end\":69619,\"start\":69603},{\"end\":69639,\"start\":69619},{\"end\":69659,\"start\":69639},{\"end\":69672,\"start\":69659},{\"end\":70485,\"start\":70468},{\"end\":70498,\"start\":70485},{\"end\":70513,\"start\":70498},{\"end\":70526,\"start\":70513},{\"end\":70539,\"start\":70526},{\"end\":70554,\"start\":70539},{\"end\":70566,\"start\":70554},{\"end\":71168,\"start\":71147},{\"end\":71184,\"start\":71168},{\"end\":71194,\"start\":71184},{\"end\":71211,\"start\":71194},{\"end\":71226,\"start\":71211},{\"end\":71239,\"start\":71226},{\"end\":71250,\"start\":71239},{\"end\":71605,\"start\":71596},{\"end\":71621,\"start\":71605},{\"end\":71864,\"start\":71847},{\"end\":71876,\"start\":71864},{\"end\":71891,\"start\":71876},{\"end\":72150,\"start\":72137},{\"end\":72164,\"start\":72150},{\"end\":72177,\"start\":72164},{\"end\":72188,\"start\":72177},{\"end\":72202,\"start\":72188},{\"end\":72217,\"start\":72202},{\"end\":72232,\"start\":72217},{\"end\":72600,\"start\":72588},{\"end\":72614,\"start\":72600},{\"end\":72629,\"start\":72614},{\"end\":72645,\"start\":72629},{\"end\":72660,\"start\":72645},{\"end\":72675,\"start\":72660},{\"end\":72688,\"start\":72675},{\"end\":72703,\"start\":72688},{\"end\":73291,\"start\":73276},{\"end\":73305,\"start\":73291},{\"end\":73320,\"start\":73305},{\"end\":73827,\"start\":73813},{\"end\":73841,\"start\":73827},{\"end\":73854,\"start\":73841},{\"end\":73876,\"start\":73854},{\"end\":73892,\"start\":73876},{\"end\":73906,\"start\":73892},{\"end\":73921,\"start\":73906},{\"end\":74384,\"start\":74370},{\"end\":74398,\"start\":74384},{\"end\":74411,\"start\":74398},{\"end\":74426,\"start\":74411},{\"end\":74972,\"start\":74958},{\"end\":74985,\"start\":74972},{\"end\":74999,\"start\":74985},{\"end\":75014,\"start\":74999},{\"end\":75026,\"start\":75014},{\"end\":75621,\"start\":75607},{\"end\":75634,\"start\":75621},{\"end\":75649,\"start\":75634},{\"end\":75664,\"start\":75649},{\"end\":75679,\"start\":75664},{\"end\":76270,\"start\":76256},{\"end\":76283,\"start\":76270},{\"end\":76813,\"start\":76799},{\"end\":76828,\"start\":76813},{\"end\":76836,\"start\":76828},{\"end\":76845,\"start\":76836},{\"end\":76860,\"start\":76845},{\"end\":77419,\"start\":77405},{\"end\":77433,\"start\":77419},{\"end\":77447,\"start\":77433},{\"end\":77462,\"start\":77447},{\"end\":78046,\"start\":78032},{\"end\":78060,\"start\":78046},{\"end\":78075,\"start\":78060},{\"end\":78611,\"start\":78597},{\"end\":78625,\"start\":78611},{\"end\":78638,\"start\":78625},{\"end\":78653,\"start\":78638},{\"end\":78667,\"start\":78653},{\"end\":79234,\"start\":79220},{\"end\":79261,\"start\":79234},{\"end\":79274,\"start\":79261},{\"end\":79289,\"start\":79274},{\"end\":79309,\"start\":79289},{\"end\":79324,\"start\":79309},{\"end\":79959,\"start\":79945},{\"end\":79975,\"start\":79959},{\"end\":79992,\"start\":79975},{\"end\":80010,\"start\":79992},{\"end\":80030,\"start\":80010},{\"end\":80045,\"start\":80030},{\"end\":81122,\"start\":81111},{\"end\":81135,\"start\":81122},{\"end\":81149,\"start\":81135},{\"end\":81164,\"start\":81149},{\"end\":81178,\"start\":81164},{\"end\":81190,\"start\":81178},{\"end\":81203,\"start\":81190},{\"end\":81210,\"start\":81203},{\"end\":81651,\"start\":81635},{\"end\":81926,\"start\":81905},{\"end\":81943,\"start\":81926},{\"end\":81963,\"start\":81943},{\"end\":81976,\"start\":81963},{\"end\":81990,\"start\":81976},{\"end\":82005,\"start\":81990},{\"end\":82315,\"start\":82302},{\"end\":82329,\"start\":82315},{\"end\":82344,\"start\":82329},{\"end\":82359,\"start\":82344},{\"end\":82936,\"start\":82923},{\"end\":82950,\"start\":82936},{\"end\":82967,\"start\":82950},{\"end\":83385,\"start\":83373},{\"end\":83404,\"start\":83385},{\"end\":83410,\"start\":83404},{\"end\":83434,\"start\":83410},{\"end\":83443,\"start\":83434},{\"end\":83459,\"start\":83443},{\"end\":83475,\"start\":83459},{\"end\":83493,\"start\":83475},{\"end\":84034,\"start\":84024},{\"end\":84049,\"start\":84034},{\"end\":84063,\"start\":84049},{\"end\":84075,\"start\":84063},{\"end\":84092,\"start\":84075},{\"end\":84107,\"start\":84092},{\"end\":84120,\"start\":84107},{\"end\":84134,\"start\":84120},{\"end\":84618,\"start\":84588},{\"end\":84634,\"start\":84618},{\"end\":84647,\"start\":84634},{\"end\":84659,\"start\":84647},{\"end\":84685,\"start\":84659},{\"end\":84698,\"start\":84685},{\"end\":84713,\"start\":84698},{\"end\":84725,\"start\":84713},{\"end\":84740,\"start\":84725},{\"end\":84745,\"start\":84740},{\"end\":85300,\"start\":85286},{\"end\":85317,\"start\":85300},{\"end\":85332,\"start\":85317},{\"end\":85585,\"start\":85567},{\"end\":85601,\"start\":85585},{\"end\":85621,\"start\":85601},{\"end\":85637,\"start\":85621},{\"end\":85655,\"start\":85637},{\"end\":85671,\"start\":85655},{\"end\":85688,\"start\":85671},{\"end\":86071,\"start\":86057},{\"end\":86087,\"start\":86071},{\"end\":86103,\"start\":86087},{\"end\":86116,\"start\":86103},{\"end\":86130,\"start\":86116},{\"end\":86759,\"start\":86745},{\"end\":86775,\"start\":86759},{\"end\":86795,\"start\":86775},{\"end\":86809,\"start\":86795},{\"end\":87456,\"start\":87442},{\"end\":87472,\"start\":87456},{\"end\":87486,\"start\":87472},{\"end\":88018,\"start\":88004},{\"end\":88034,\"start\":88018},{\"end\":88048,\"start\":88034},{\"end\":88609,\"start\":88598},{\"end\":88621,\"start\":88609},{\"end\":89090,\"start\":89075},{\"end\":89099,\"start\":89090},{\"end\":89111,\"start\":89099},{\"end\":89498,\"start\":89480},{\"end\":89520,\"start\":89498},{\"end\":89540,\"start\":89520},{\"end\":90260,\"start\":90244},{\"end\":90273,\"start\":90260},{\"end\":90287,\"start\":90273},{\"end\":90695,\"start\":90675},{\"end\":90709,\"start\":90695},{\"end\":90729,\"start\":90709},{\"end\":90744,\"start\":90729},{\"end\":90753,\"start\":90744},{\"end\":90765,\"start\":90753},{\"end\":90775,\"start\":90765},{\"end\":90791,\"start\":90775},{\"end\":91303,\"start\":91292},{\"end\":91317,\"start\":91303},{\"end\":91325,\"start\":91317},{\"end\":91891,\"start\":91880},{\"end\":91903,\"start\":91891},{\"end\":91920,\"start\":91903},{\"end\":91934,\"start\":91920},{\"end\":91948,\"start\":91934},{\"end\":91967,\"start\":91948},{\"end\":91983,\"start\":91967},{\"end\":92000,\"start\":91983}]", "bib_venue": "[{\"end\":69208,\"start\":69081},{\"end\":69847,\"start\":69768},{\"end\":70739,\"start\":70653},{\"end\":72892,\"start\":72798},{\"end\":73509,\"start\":73415},{\"end\":74573,\"start\":74508},{\"end\":75205,\"start\":75116},{\"end\":75878,\"start\":75779},{\"end\":76442,\"start\":76371},{\"end\":77059,\"start\":76960},{\"end\":77673,\"start\":77568},{\"end\":78286,\"start\":78181},{\"end\":78878,\"start\":78773},{\"end\":79535,\"start\":79430},{\"end\":80292,\"start\":80169},{\"end\":81263,\"start\":81245},{\"end\":82558,\"start\":82459},{\"end\":83100,\"start\":83042},{\"end\":83516,\"start\":83513},{\"end\":83836,\"start\":83820},{\"end\":84251,\"start\":84201},{\"end\":84900,\"start\":84831},{\"end\":86305,\"start\":86218},{\"end\":87020,\"start\":86923},{\"end\":87651,\"start\":87571},{\"end\":88213,\"start\":88133},{\"end\":89214,\"start\":89171},{\"end\":89787,\"start\":89664},{\"end\":90954,\"start\":90881},{\"end\":92157,\"start\":92087},{\"end\":69079,\"start\":68953},{\"end\":69766,\"start\":69672},{\"end\":70199,\"start\":70174},{\"end\":70651,\"start\":70566},{\"end\":71290,\"start\":71250},{\"end\":71655,\"start\":71621},{\"end\":71845,\"start\":71810},{\"end\":72270,\"start\":72232},{\"end\":72796,\"start\":72703},{\"end\":73413,\"start\":73320},{\"end\":73998,\"start\":73921},{\"end\":74506,\"start\":74426},{\"end\":74808,\"start\":74804},{\"end\":75114,\"start\":75026},{\"end\":75777,\"start\":75679},{\"end\":76369,\"start\":76283},{\"end\":76707,\"start\":76703},{\"end\":76958,\"start\":76860},{\"end\":77566,\"start\":77462},{\"end\":78179,\"start\":78075},{\"end\":78771,\"start\":78667},{\"end\":79428,\"start\":79324},{\"end\":80167,\"start\":80045},{\"end\":80685,\"start\":80654},{\"end\":80894,\"start\":80879},{\"end\":81243,\"start\":81210},{\"end\":81672,\"start\":81651},{\"end\":82023,\"start\":82005},{\"end\":82457,\"start\":82359},{\"end\":83040,\"start\":82967},{\"end\":83511,\"start\":83493},{\"end\":83818,\"start\":83747},{\"end\":84199,\"start\":84134},{\"end\":84829,\"start\":84745},{\"end\":85284,\"start\":85226},{\"end\":85724,\"start\":85688},{\"end\":86216,\"start\":86130},{\"end\":86921,\"start\":86809},{\"end\":87569,\"start\":87486},{\"end\":88131,\"start\":88048},{\"end\":88704,\"start\":88647},{\"end\":89169,\"start\":89111},{\"end\":89662,\"start\":89540},{\"end\":90344,\"start\":90287},{\"end\":90879,\"start\":90791},{\"end\":91338,\"start\":91325},{\"end\":91540,\"start\":91504},{\"end\":92085,\"start\":92000},{\"end\":92504,\"start\":92500}]"}}}, "year": 2023, "month": 12, "day": 17}