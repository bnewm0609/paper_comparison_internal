{"id": 249240205, "updated": "2023-10-29 22:03:18.829", "metadata": {"title": "MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction", "authors": "[{\"first\":\"Zehao\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Songyou\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Niemeyer\",\"middle\":[]},{\"first\":\"Torsten\",\"last\":\"Sattler\",\"middle\":[]},{\"first\":\"Andreas\",\"last\":\"Geiger\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction. In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks. State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints. This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time. Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations. We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2206.00665", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/YuPNS022", "doi": "10.48550/arxiv.2206.00665"}}, "content": {"source": {"pdf_hash": "c6c518ccd441bae6fdc788a58b6d39db1f33f1a4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2206.00665v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "616a3ea7cc0e4bc18062b8434def001f0611f4cd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c6c518ccd441bae6fdc788a58b6d39db1f33f1a4.txt", "contents": "\nMonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction\n\n\nZehao Yu \nUniversity of T\u00fcbingen\n\n\nSongyou Peng \nETH Zurich\n\n\nMPI for Intelligent Systems\nT\u00fcbingen\n\nMichael Niemeyer \nUniversity of T\u00fcbingen\n\n\nMPI for Intelligent Systems\nT\u00fcbingen\n\nTorsten Sattler \nCzech Technical University\nPrague\n\nAndreas Geiger \nUniversity of T\u00fcbingen\n\n\nMPI for Intelligent Systems\nT\u00fcbingen\n\nMonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction\n\nIn recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction. In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks. State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints. This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time. Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations. We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation.\n\nIntroduction\n\n3D reconstruction from multiple RGB images is a fundamental problem in computer vision with various applications in robotics, graphics, animation, virtual reality, and more. Recently, coordinatebased neural networks have emerged as a powerful tool for representing 3D geometry and appearance.\n\nThe key idea is to use compact, memory efficient multi-layer perceptrons (MLPs) to parameterize implicit shape representations such as occupancy or signed distance fields. While early works [9,42,50] relied on 3D supervision, several recent works [47,66,82] use differentiable surface rendering to reconstruct scenes from multi-view images. At the same time, neural radiance fields (NeRFs) [44] achieved impressive novel view synthesis results with volume rendering techniques. [49,76,81] combine surface and volume rendering for the task of 3D reconstruction by expressing volume density as a function of the underlying 3D surface, which in turn improves scene geometry.\n\nCurrent neural implicit-based surface reconstruction approaches achieve impressive reconstruction results for simple scenes with dense viewpoint sampling. Yet, as shown in the first row of Fig. 1, they struggle in the presence of limited input views (DTU with 3 views) or for scenes that contain large textureless regions (walls in ScanNet or Tanks & Temples). A key reason for this behavior is that these model are optimized using a per-pixel RGB reconstruction loss. Using only RGB images as input leads to an underconstrained problem as there exist an infinite number of photo-consistent  Figure 1: MonoSDF. Top: State-of-the-art neural implicit surface reconstruction methods fail in the presence of limited input views or when applied to complex multi-object scenes. Bottom: We demonstrate that incorporating geometric cues from general-purpose monocular predictors enables scaling to larger scenes while yielding more accurate reconstructions and speeding up optimization. An image resolution of 384 \u00d7 384 pixels was used for all results shown above.\n\nexplanations [5,90]. Previous works address this problem by incorporating priors on the structure of the scene into the optimization process, e.g., depth smoothness [46], surface smoothness [49,91], semantic similarity [30], or Manhattan world assumptions [21]. In this paper, we explore monocular geometric priors as they are readily available and efficient to compute. We show that using such priors significantly improves 3D reconstruction quality in challenging scenarios (see second row of Fig. 1).\n\nEstimating geometric cues such as depth and normals from a single image has been an active research area for decades. The seminal work by Eigen et al. [18,19] showed that learned models based on deep convolutional neural networks (CNNs) significantly improved over early work in this area [24][25][26][27][59][60][61]. Recent work [17,55,56], in particular Omnidata [17], has made significant headway in terms of prediction quality and generalization to new scenes using very large datasets for training. These strong results on individual images, and the fact that monocular geometric cues can be computed efficiently, naturally lead to the question whether such models are able to provide the additional constraints required by implicit neural surface reconstruction approaches to handle more challenging settings.\n\nThis paper describes a framework, called MonoSDF, for integrating monocular geometric priors into neural implicit surface reconstruction methods: given multi-view images, we infer depth and surface normals for each image, and use them as additional supervision signals during optimization together with the RGB image reconstruction loss. We observe that these priors lead to significant gains in reconstruction quality, especially in textureless and less-observed areas as shown in Fig. 1. This is due to the fact that the photometric consistency cues used by surface reconstruction methods and the recognition cues used by monocular networks are complementary: while photometric consistency fails in texturless regions such as walls, surface normals can be predicted reliably in these areas due to the structured 3D scene layout. Conversely, photoconsistency cues allow for establishing globally accurate 3D geometry in textured regions, while normal and (relative) depth cues only provide local geometric information.\n\nApart from incorporating monocular geometric cues, we provide a systematic study and analysis of state-of-the-art design choices for coordinate-based neural representations in the context of implicit surface reconstruction. More specifically, we investigate the following architectures: a single, large MLP [49,76,81,82], a dense SDF grid [31], a single feature grid [28,38,53,54] and multi-resolution feature grids [10,22,45,69,92]. We observe that MLPs act globally and exhibit an inductive smoothness bias while being computationally expensive to optimize and evaluate. In contrast, grid-based representations benefit from locality during optimization and evaluation, hence they are computationally more efficient. However, reconstructions are noisier for sparse views or less-observed areas. Including monocular geometric priors improves neural implicit reconstruction results across different settings with faster convergence times and independent of the underlying representation.\n\nIn summary, we make the following contributions:\n\n\u2022 We introduce MonoSDF, a novel framework which exploits monocular geometric cues to improve multi-view 3D reconstruction quality, efficiency, and scalability for neural implicit surface models.\n\n\u2022 We provide a systematic comparison and detailed analysis of design choices of neural implicit surface representations, including vanilla MLP and grid-based approaches.\n\n\u2022 We conduct extensive experiments on multiple challenging datasets, ranging from object-level reconstruction on the DTU dataset [1], over room-level reconstruction on Replica [67] and Scan-Net [13], to large-scale indoor scene reconstruction on Tanks and Temples [34].\n\n\nRelated Work\n\nArchitectures for Neural Implicit Scene Representations. Neural implicit scene representations or neural fields [78] have recently gained popularity for representing 3D geometry due to their expressiveness and low memory footprint. Seminal works [9,42,50] use a single MLP as the scene representation and show impressive object-level reconstruction quality, but they do not scale to more complicated or large-scale scenes due to the limited model capacity. Follow-up works [10,22,41,45,54,69,92] combine an MLP decoder with one or multi-level voxel grids of low-dimensional features. Such hybrid representations are able to better represent fine geometric details and can be evaluated fast. However, they lead to a larger memory footprint with increasing scene size. In this paper we provide a systematic comparison of four architectural design choices for implicit surface reconstruction. 3D Reconstruction from Multi-view Images. Reconstructing the underlying 3D geometry from multi-view images is a long-standing goal of computer vision. Classic multi-view stereo (MVS) methods [2, 6-8, 35, 35, 62, 64, 65] consider either feature matching for depth estimation [6,62] or represent shapes with voxels [2,7,8,35,51,64,72,73]. Learning-based MVS methods usually replace some parts of the classic MVS pipeline, e.g., feature matching [23,36,40,74,88], depth fusion [16,57], or inferring depth from multi-view images [29,79,80,85]. In contrast to the explicit scene representations used by classic MVS algorithms, recent neural approaches [39,48,82] represent surfaces via a single MLP with continuous outputs. Learned purely from posed 2D images, they show appealing reconstruction results and do not suffer from discretization. However, accurate object masks are required. Inspired by the density-based volume rendering in NeRF [44], which demonstrated impressive view synthesis without object masks, several works [49,76,81] use volume rendering for neural implicit surface reconstruction without masks. However, these methods lead to poor results in large-scale scenes with textureless regions. In this work, we show that incorporating monocular priors allows these approaches to obtain significantly more detailed reconstructions and to scale to larger and more challenging scenes. Incorporating Priors into Neural Scene Representations. Several researchers proposed to incorporate priors such as depth smoothness [46], semantic similarity [30], or sparse MVS point clouds [58] for the task of novel view synthesis from sparse inputs. In contrast, in this work, our focus is on implicit 3D surface reconstruction. Concurrently, Manhattan-SDF [21] uses dense MVS depth maps from COLMAP [63] as supervision and adopts Manhattan world priors [11] to handle low-textured planar regions corresponding to walls, floors, etc. Our approach is based on the observation that data-driven monocular depth and normal predictions [17] provide high-quality priors for the full scene. Incorporating these priors into the optimization of neural implicit surfaces not only removes the Manhattan world assumption [11] but also results in improved reconstruction quality and a simpler pipeline. 1 Compared to NeuRIS [75], a concurrent work that proposes to use normal priors for indoor scene reconstruction, we integrate monocular depth cues and further demonstrate the effectiveness of monocular cues on various neural scene representations, ranging from MLP to multi-resolution feature grids. Overview. In this work we use monocular geometric cues predicted by a general-purpose pretrained network to guide the optimization of neural implicit surface models. More specifically, for a batch of rays, we volume render predicted RGB colors, depth, and normals, and optimize wrt. the input RGB images and monocular geometric cues. Further, we investigate different design choices for neural implicit architectures and provide an in-depth analysis. For clarity, we only show the SDF and not the color prediction branch above.\n\n\nMethod\n\nOur goal is to recover the underlying scene geometry from multiple posed images while utilizing monocular geometric cues to guide the optimization process. To this end, we first review neural implicit scene representations and various design choices in Section 3.1 and discuss how to perform volume rendering of these representations in Section 3.2. Next, we introduce the monocular geometric cues we investigate in our study in Section 3.3 and discuss loss functions and the overall optimization process in Section 3.4. An overview of our framework is provided in Fig. 2.\n\n\nImplicit Scene Representations\n\nWe represent scene geometry as a signed distance function (SDF). A signed distance function is a continuous function f that, for a given 3D point, returns the point's distance to the closest surface:\nf : R 3 \u2192 R x \u2192 s = SDF(x) .(1)\nHere, x is the 3D point and s denotes the corresponding SDF value. In this work, we parameterize the SDF function with learnable parameters \u03b8 and investigate several different design choices for representing the function: explicit as a dense grid of learnable SDF values, implicit as a single MLP, or hybrid using an MLP in combination with single-or multi-resolution feature grids. Dense SDF Grid. The most straightforward way of parameterizing an SDF is to directly store SDF values in each cell of a discretized volume G \u03b8 with resolution of R H \u00d7 R W \u00d7 R D [31]. To query the SDF value\u015d for an arbitrary point x from the dense SDF grid, we can use any interpolation operation:\ns = interp(x, G \u03b8 ) .(2)\nIn our experiments, we implement interp as trilinear interpolation.\n\nSingle MLP. The SDF function can also be parameterized by a single MLP [50] f \u03b8 :\ns = f \u03b8 (\u03b3(x)) ,(3)\nwhere\u015d is the predicted SDF value and \u03b3 corresponds to a fixed positional encoding [44,70] mapping x to a higher dimensional space. After their introduction to novel view synthesis [44], positional encoding functions are now widely used for neural implicit surface reconstruction [49,76,81,82] as they increase the expressiveness of coordinate-based networks [70].\n\nSingle-Resolution Feature Grid with MLP Decoder. We can also combine both parameterizations and use a feature-conditioned MLP f \u03b8 together with a feature grid \u03a6 \u03b8 with a resolution of R 3 , where each cell of the grid stores a feature vector [28,38,54,69] instead of directly storing SDF values:\ns = f \u03b8 (\u03b3(x), interp(x, \u03a6 \u03b8 )) .(4)\nNote that the MLP f \u03b8 is conditioned on the interpolated local feature vector from the feature grid \u03a6 \u03b8 .\n\nMulti-Resolution Feature Grids with MLP Decoder. Instead of using a single feature grid \u03a6 \u03b8 , one can also employ multi-resolution feature grids {\u03a6 l \u03b8 } L l=1 with resolutions R l [10,22,45,69,92]. The resolutions are sampled in geometric space [45] to combine features at different frequencies:\nR l := R min b l b := exp ln R max \u2212 ln R min L \u2212 1 ,(5)\nwhere R min , R max are the coarsest and finest resolution, respectively. Similarly, we extract the interpolated features at each level and concatenate them together:\ns = f \u03b8 (\u03b3(x), {interp(x, \u03a6 l \u03b8 )} l )) .(6)\nAs the total number of grid cells grows cubically, we use a fixed number of parameters to store the feature grids and use a spatial hash function to index the feature vector at finer levels [45] (see supplementary for details).\n\nColor Prediction. In addition to the 3D geometry, we also predict color values such that our model can be optimized with a reconstruction loss. Following [82], we therefore define a second function\nc \u03b8\u0109 = c \u03b8 (x, v,n,\u1e91)(7)\nthat predicts a RGB color value\u0109 for a 3D point x and a viewing direction v. The 3D unit normaln is the analytical gradient of our SDF function. The feature vector\u1e91 is the output of a second linear head of the SDF network as in [82]. We parameterize c \u03b8 with a two-layer MLP with network weights \u03b8. In case of the dense grid SDF parameterization, we similarly optimize a dense feature grid and obtain the feature vector\u1e91 via the interpolation function interp.\n\n\nVolume Rendering of Implicit Surfaces\n\nFollowing recent work [49,76,81,82], we optimize the implicit representations described in Section 3.1 via an image-based reconstruction loss using differentiable volume rendering. More specifically, to render a pixel, we cast a ray r from the camera center o through the pixel along its view direction v. We sample M points x i r = o + t i r v along the ray and predict their SDF\u015d i r and color values\u0109 i r . We follow [81] to transform the SDF values\u015d i r to density values \u03c3 i r for volume rendering:\n\u03c3 \u03b2 (s) = \uf8f1 \uf8f2 \uf8f3 1 2\u03b2 exp s \u03b2 if s \u2264 0 1 \u03b2 1 \u2212 1 2 exp \u2212 s \u03b2 if s > 0 ,(8)\nwhere \u03b2 is a learnable parameter. Following NeRF [44], the color\u0108(r) for the current ray r is computed via numerical integration:\nC(r) = M i=1 T i r \u03b1 i r\u0109 i r T i r = i\u22121 j=1 1 \u2212 \u03b1 j r \u03b1 i r = 1 \u2212 exp \u2212\u03c3 i r \u03b4 i r ,(9)\nwhere T i r and \u03b1 i r denote the transmittance and alpha value of sample point i along ray r, respectively, and \u03b4 i r is the distance between neighboring sample points. Similarly, we compute the depthD(r) and normalN (r) of the surface intersecting the current ray as:\nD(r) = M i=1 T i r \u03b1 i r t i rN (r) = M i=1 T i r \u03b1 i rn i r .(10)\n5\n\n\nExploiting Monocular Geometric Cues\n\nUnifying volume rendering with implicit surfaces leads to impressive 3D reconstruction results. Yet, this approach struggles with more complex scenes especially in textureless and sparsely covered regions. To overcome this limitation, we use readily available, efficient-to-compute monocular geometric priors thereby improving neural implicit surface methods. Monocular Depth Cues. One common monocular geometric cue is a monocular depth map, which can be easily obtained via an off-the-shelf monocular depth predictor. More specifically, we use a pretrained Omnidata model [17] to predict a depth mapD for each input RGB image. Note that the absolute scale is difficult to estimate in general scenes, soD must be considered as a relative cue. However, this relative depth information is provided also over larger distances in the image. Monocular Normal Cues. Another geometric cue we use is the surface normal. Similar to the depth cues, we apply the same pretrained Omnidata model to acquire a normal mapN for each RGB image. Unlike depth cues that provide semi-local relative information, normal cues are local and capture geometric detail. We hence expect that surface normals and depth are complementary to each other.\n\n\nOptimization\n\nReconstruction Loss. Eq. (9) provides a linkage from the 3D scene representation to 2D observations. We can therefore optimize the scene representation with a simple RGB reconstruction loss:\nL rgb = r\u2208R \u0108 (r) \u2212 C(r) 1 .(11)\nHere R denotes the set of pixels/rays in the minibatch and C(r) is the observed pixel color. Eikonal Loss. Following common practice, we also add an Eikonal term [20] on the sampled points to regularize SDF values in 3D space\nL eikonal = x\u2208X ( \u2207f \u03b8 (x) 2 \u2212 1) 2 ,(12)\nwhere X are a set of uniformly sampled points together with near-surface points [81]. Depth Consistency Loss. Besides L rgb and L eikonal , we also enforce consistency between our rendered expected depthD and the monocular depthD:\nL depth = r\u2208R (wD(r) + q) \u2212D(r) 2 ,(13)\nwhere w and q are the scale and shift used to alignD andD sinceD is defined only up to scale. Note that these factors have to be estimated individually per batch as the depth maps predicted for different batches can differ in scale and shift. Specifically, we solve for w and q with a least-squares criterion [19,56] which has a closed-form solution (see supplementary for details). Normal Consistency Loss. Similarly, we impose consistency on the volume-rendered normalN and the predicted monocular normalsN transformed to the same coordinate system with angular and L1 losses [17]:\nL normal = r\u2208R N (r) \u2212N (r) 1 + 1 \u2212N (r) N (r) 1 .(14)\nThe overall loss we use to optimize our implicit surfaces jointly with the appearance network is:\nL = L rgb + \u03bb 1 L eikonal + \u03bb 2 L depth + \u03bb 3 L normal .(15)\nImplementation Details. We implement our method in PyTorch [52] and use the Adam optimizer [33] with a learning rate of 5e-4 for neural networks and 1e-2 for feature grids and dense SDF grids. We set \u03bb 1 , \u03bb 2 , \u03bb 3 to 0.1, 0.1, 0.05, respectively. We sample 1024 rays per iteration and apply the error-bounded sampling strategy introduced by [81] to sample points along each ray. For MLPs and feature grids, we adapt the architecture and initialization scheme from [81] and [45], respectively. For obtaining monocular cues, we first resize each image and center crop it to 384 \u00d7 384, which we then feed as input to the pretrained Omnidata model [17]. See supplementary for more details.  Figure 3: Architectural Ablation Study. Comparing different design choices for neural implicit surface representations, we observe that a dense SDF grid leads to noisy reconstructions due to a missing smoothness bias. The MLP and the Single-Res. Fea. Grid improve results, but geometry tends to be overly smooth with missing details. The best results are obtained using Multi-Res. Fea. Grids.\n\n\nExperiments\n\nWe first analyze different architectural design choices and perform ablation studies wrt. monocular cues and optimization time on a room-level dataset (Replica) with perfect ground truth. Next, we provide qualitative and quantitative comparisons against state-of-the-art baselines on real-world indoor scenes. Finally, we evaluate our method on object-level reconstruction for both sparse input and dense input scenarios.\n\nDatasets. While previous neural implicit-based reconstruction methods mainly focused on singleobject scenes with many input views, in this work, we investigate the importance of monocular geometric cues for scaling to more complex scenes. Thus we consider: a) Real-world indoor scans: Replica [67] and ScanNet [13]; b) Real-world large-scale indoor scenes: Tanks and Temples [34] advanced scenes; c) Object-level scenes: DTU [1] in the sparse 3-view setting from [46,84]. Baselines. We compare against a) state-of-the-art neural implicit surfaces methods: UNISURF [49], VolSDF [81], NeuS [76], and Manhattan-SDF [21]. b) Classic MVS methods: COLMAP [62] and a state-of-the-art commercial software (RealityCapture 2 ). c) TSDF-Fusion [12] with predicted monocular depth cues, where GT depth maps are used to recover the scale and shift values (cf. Eq. (13)). This baseline shows the reconstruction quality if only monocular depth cues and no implicit surface model is used. Evaluation Metrics. For DTU, we follow the official evaluation protocol and report the Chamfer distance. For Replica and ScanNet, following [21,42,53,54,68,92], we report the Chamfer Distance, the F-score with a threshold of 5cm, as well as a Normal Consistency measure.\n\n\nAblation Study\n\nWe first analyze different scene representation choices on the Replica dataset. Next, we ablate the impact of our geometric cues on reconstruction quality and convergence time.  Architecture Choices for Scene Representations.\n\nWe compare the four different scene geometry representations introduced in Section 3.1 and report metrics averaged over the Replica dataset in Table 1. Note that no monocular geometric cues are used here. We first observe that using a single MLP as the scene geometry representation leads to decent results, but the reconstruction tends to be over-smooth (see Table 1 and Fig. 3). For grid-based representations, optimizing a dense SDF grid leads to a significantly worse performance compared to all other neural implicit scene representations, even with careful parameter tuning. The reason is the lack of a smoothness bias:\n\nThe SDF values in grid cells are all stored and optimized independently of each other, hence there is no local or global smoothness bias. In contrast, the Single-Res. Fea. Grid replaces the SDF value in each grid cell with a low-dimensional latent code, and uses a shallow MLP conditioned on these features to read out SDF values of arbitrary 3D points. This modification leads to a notable boost in reconstruction quality over the dense grid, performing similarly well as the single MLP. Using a Multi-Res. Fea. Grids as in [45] further increases performance. We observe that the Multi-Res. Fea. Grids is the best-performing grid-based model, and from now on we report results for the single MLP  Grids with and without the monocular geometric cues. We observe that monocular cues improve reconstruction quality for both architectures, and using both cues in combination leads to the best performance. b.) The optimization speed becomes significantly faster when incorporating monocular cues. Comparing the two architectures, we observe that the grid approach yields faster convergences while the MLP with both cues leads to the best results. and the Multi-Res. Feature Grids. For simplicity, we will refer to the multi-resolution feature grids as Multi-Res. Grids or Grids in the following. Ablation of Different Cues. We now investigate the effectiveness of different monocular geometric cues for the two chosen representations. Table 2 (a) and Fig. 4 show that, for both representations, using either one or both monocular cues significantly boosts reconstruction quality. We also find both cues to be complementary, with the best performance being achieved when using both. Similar behavior can be observed for the other two representations (cf. supplementary material). It is worth noting that the differences between the two representations become negligible when using monocular cues, indicating that those serve as a general drop-in to improve reconstruction quality. Optimization Time. Table 2 (b) shows optimization time for the two scene representations with and without cues. We see that the Multi-Res. Grids converge faster than the single MLP model. Further, adding the monocular cues significantly speeds up the convergence process. After only 10K iterations, both representations perform better than the converged models without monocular cues. Note that the overhead required for incorporating the monocular cues into the optimization process is small and can be neglected. An extended version of Table 2 (b) can be found in the supplementary materials.\n\n\nReal-world Large-scale Scene Reconstruction\n\nTo show the effectiveness of our method for large-scale scene reconstruction, we compare against various baselines on two challenging large-scale indoor datasets. ScanNet. On ScanNet, we use the test split from [21] and also follow their evaluation protocol in which depth maps are rendered from input camera poses and then re-fused using TSDF Fusion [12] to evaluate only observed areas. We observe in Table 3 that our MLP variant outperforms all baselines achieving smoother reconstructions with more fine details. Note that we outperform concurrent work [75]. Further, we find that the MLP variant performs significantly better than using Multi-Res. Grids. ScanNet's RGB images contain motion blur and the camera poses are also noisy. This can be harmful to the local geometry updates in grid-based representations, while MLPs are more robust to this noise due to their smoothness bias.  \n\n\nObject-level Reconstruction from Sparse Views\n\nWe now evaluate our method on another challenging task: reconstructing single objects from sparse input views. We adopt the test split from [81,82] on DTU and choose three input views following [46].  We first observe in Table 4 and Fig. 1 that without the usage of the monocular geometric cues, neither the MLP (VolSDF [81]) nor the Multi-Res. Grids work well with only 3 input views. When incorporating the cues, the results for both representations are significantly improved. Interestingly, the grid-based representations perform inferior to a single MLP as they are updated locally and do not benefit from the inductive bias of a monolithic MLP representation.\n\nComparing against TSDF Fusion [12] that fused predicted depth cues from all views into a TSDF volume without any optimization, we observe that this baseline has difficulties in reconstructing meaningful details due to inconsistencies in the monocular depth cues. Note that this baseline uses the GT depth maps from [16] to compute scale and shift for the depth cues. Classic MVS methods perform well quantitatively, but they heavily rely on dense matching, and in case of three input images, this inevitably leads to incomplete reconstructions (see supplementary material). In contrast, our approach combines neural implicit surface representations with the benefits from monocular geometric cues that are more robust to less-observed regions.\n\n\nObject-level Reconstruction from Dense Views\n\nTo further investigate the effectiveness and flexibility of our method, we evaluate our approach on the DTU dataset with all input views, which is a common setting in recent work [49,77,81]. In this experiment, we simply resize the low-resolution monocular cues to full resolution (from 384 \u00d7 384 to 1200 \u00d7 1200 pixels) while keeping the image ratio. As the original image is of size 1200 \u00d7 1600, the monocular cues are missing in the left and right part of the image. Therefore, we only use the monocular cues where they are available.\n\nAs shown in Table 5, our approach with MLP architecture achieves reconstruction quality similar to state-of-the-art methods [49,77,81]. This is reasonable as the dense input views provide enough constraints and the prior information from monocular cues is negligible. However, our method with multi-resolution feature grid architecture outperforms previous work by a large margin. We attribute this to the expressiveness of multi-resolution feature grids where monocular cues are still effective to  Table 5: Object-level Reconstruction on DTU Dataset will All Input Views. We compare Chamfer distance with state-of-the-art methods. Our approach with MLP achieves similar results to previous methods, while our method with multi-resolution feature grids leads to more detailed surfaces and outperforms previous work by a large margin.\n\nsuppress noise and therefore can reconstruct smooth and detailed surfaces. We kindly refer the reader to the supplementary material for additional visual comparisons.\n\n\nConclusion\n\nWe have presented MonoSDF, a novel framework that systematically explores how monocular geometric cues can be incorporated into the optimization of neural implicit surfaces from multiview images. We show that such easy-to-obtain monocular cues can significantly improve 3D reconstruction quality, efficiency, and scalability for a variety of neural implicit representations. When using monocular cues, a simple MLP architecture performs best overall, demonstrating that MLPs in principle are able to represent complex scenes, albeit being slower to converge compared to grid-based representations. Multi-resolution feature grids in general can converge fast and capture details, but are less robust to noise and ambiguities in the input images.\n\n\nLimitations.\n\nThe performance of our model depends on the quality of the monocular cues. Filtering strategies to handle failures of the monocular predictor are thus a promising direction to further improve reconstruction quality. We kindly refer the reader to the supplementary material for additional analysis. While we demonstrated that integrating depth and normal cues significantly improves reconstruction, exploring other cues such as occlusion edges, plane, or curvature [17,87] is an interesting future direction. We are currently limited by the low-resolution (384 \u00d7 384 pixels) output of the Omnidata model [17] and plan to explore different ways of using higher-resolution cues. We provide some preliminary results of using high-resolution cues in the supplementary. Joint optimization of scene representations and camera parameters [4,92] is another interesting direction, especially for multi-resolution grids, in order to better handle noisy camera poses.\n\n\nSupplementary Material for MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction\n\nIn this supplementary document, we first discuss architectural and implementation details in Section A. Next, we provide additional ablation studies of our monocular geometric cues for four different scene representations in Section B and report additional quantitative and qualitative results in Section C. Finally, we discuss potential negative impact of this work in Section D.\n\n\nA Implementation Details\n\nIn this section, we first present an overview of 4 different architectures for neural implicit scene representations and details of Multi-Res. Grids in Section A.1 and provide details of the depth loss computation in Section A.2. Next, we describe additional details regarding our parameterizations and optimization in Section A.3 and discuss evaluation metrics in Section A.4.\n\n\nA.1 Architectures\n\nIn the main paper, we investigate four different architectures as our scene representation: Dense SDF Grid, Single MLP, Single-Res. Grid, and Multi-Res. Grids . See Fig. 5 for an overview over the architectures. In the following, we provide details for Multi-Res. Feature Grids.\n\nMulti-Res. Grids. Following Instant-NGP [45], we use L levels of feature grids with resolutions sampled in geometric space to combine features at different frequencies:\nR l := R min b l b := exp ln R max \u2212 ln R min L \u2212 1 ,(16)\nwhere R min , R max are the coarsest and finest resolutions, respectively. As the total number of grid cells grows cubically, we use a fixed number of parameters to store the feature grids and use a spatial hash function to index the feature vector at finer levels. More specifically, each grid contains up to T feature vectors with dimensionality F . At the coarse level where R 3 l \u2264 T , the feature grid is stored densely. At the finer level where R 3 l > T , a spatial hash function [71] is used to index the corresponding feature vector:\nh(x) = 3 i=1 x i \u03c0 i mod T ,(17)\nwhere is the bit-wise XOR operation and \u03c0 i are unique, large prime numbers. We use the default values R min = 16, R max = 2048, L = 16, F = 2, and T = 2 19 similar to [45] in all experiments.\n\n\nA.2 Depth Consistency Loss\n\nWe enforce consistency between our rendered expected depthD and the monocular depthD with a scale invariant loss function:\nL depth = r\u2208R (wD(r) + q) \u2212D(r) 2 ,(18)\nwhere w and q are the scale and shift used to alignD andD sinceD is given only up to scale. Specifically, we solve w and q with a least-squares criterion [19,56]: w and q can be efficiently computed as follows: Let h = (w, q) T and d r = (D(r), 1) T , then Eq. (19) can be rewrite as:\n(w, q) = arg min w,q r\u2208R wD(r) + q \u2212D(r) 2 .(19)h opt = arg min h r\u2208R d T r h \u2212D(r) 2 .(20)\nwhich has the closed-form solution:\nh = r d r d T r \u22121 r d rD (r) .(21)\nNote that we estimate w and q individually at each iteration for a batch of randomly sampled rays within a single image because depth maps predicted by the monocular depth predictor can differ in scale and shift and the underlying scene geometry changes at each iteration.\n\n\nA.3 Additional Details\n\nFor our single MLP architecture, we use an 8-layer MLP with hidden dimension 256. We use a two-layer MLP with hidden dimension 256 for the SDF prediction for both, Single-Res. Grid and Multi-Res. Grids. We implement the color network with a two-layer MLP with hidden dimension 256 and use it for all architectures. We use Softplus activation for geometric network and use ReLU activation for the color network. We explicitly initialize the SDF grid with a sphere and use the geometric initialization from [3] for other architectures. For obtaining monocular cues, we first resize each image and center crop it to 384 \u00d7 384, which we then feed as input to the pretrained Omnidata model [17]. The output depth and normal maps have the same resolution of 384 \u00d7 384. As a result, we use the same resolution for RGB images, depth cues and normal cues and adjust camera intrinsics accordingly for all experiments. We optimize our model for 200k iterations which takes about 6 hours and 11 hours for our Multi-Res. Grids and MLP, respectively, on a single NVIDIA RTX3090 GPU.\n\n\nA.4 Evaluation Metrics\n\nFor the DTU dataset [1], we follow the official evaluation protocol and report the reconstruction quality with: Accuracy, Completeness and Chamfer Distance. Accuracy measures how close the reconstructed points are to the ground truth and is defined as the mean distance of the reconstructed points to the ground truth. Completeness measures to what extent the ground truth points are recovered and is defined as the mean distance of the ground truth points to the reconstructed points. Chamfer Distance is the mean of Accuracy and Completeness. It measures the overall reconstruction quality. For efficiency, we use the Python script 3 to compute these evaluation metrics.\n\n\nMetric Definition\n\nAcc mean\np\u2208P min p * \u2208P * ||p \u2212 p * || 1 Comp mean p * \u2208P * min p\u2208P ||p \u2212 p * || 1 Chamfer Acc+Comp 2 Precision mean p\u2208P min p * \u2208P * ||p \u2212 p * || 1 < 0.05 Recall mean p * \u2208P * min p\u2208P ||p \u2212 p * || 1 < 0.05 F-score 2\u00b7Precision\u00b7Recall Precision+Recall\nNormal-Acc mean p\u2208P n T p n p * s.t. p * = argmin\np * \u2208P * ||p \u2212 p * || 1 Normal-Comp mean p * \u2208P * n T p n p * s.t. p = argmin p\u2208P ||p \u2212 p * || 1\n\nNormal-Consistency\n\nNormal-Acc+Normal-Comp 2 Table 6: Evaluation Metrics. We show the evaluation metrics with their definitions that we use to measure reconstruction quality. P and P * are the point clouds sampled from the predicted and the ground truth mesh. n p is the normal vector at point p.\n\nFor Replica [67] and ScanNet [13], we report Accuracy, Completeness, Chamfer Distance, Precision, Recall, and F-score with a threshold of 5cm following [21,68,92]. We further report Normal Consistency for the Replica dataset following [21,42,53,54,68,92] as near-perfect ground truth is available. These metrics are defined in Table 6.\n\nFor the Tanks and Temples dataset [34], we submit our reconstruction results to the official evaluation server 4 and report the provided F-score.\n\n\nB Ablation\n\nIn this section, we first conduct several ablation studies to verify the effectiveness of our method, including using geometric cues with different scene representations in Section B.1, different architecture configurations in Section B.2, different number of input views in Section B.3, different cues predictors in Section B.4. Next, we analyze the optimization time of our framework in Section B.5.   Figure 6: Ablation of Different Number of Input Views on the Replica Dataset. We show F-score under each image. We observe that using more input views for training improves reconstruction quality. Further, adding monocular geometric cues improves reconstruction quality. When using only 10 input views, the MLP fails to reconstruct reasonable results while using monocular geometric cues significantly improves results.\n\n\nB.1 Ablation of Different Cues\n\nTo evaluate the effectiveness of our monocular geometric cues for different scene representations, we conduct ablation studies on the Replica dataset with our four different scene representations. Note that as the Replica dataset is part of the training set of Omnidata (making up 0.46% of the entire training data) [17], we split the evaluation into the train/test split of Omnidata [17].\n\nAs shown in Table 2 and Fig. 8, our geometric cues improve reconstruction quality significantly independent of the underlying scene representations. We observe that using both, depth cues and normal cues, leads to the best results, indicating the complementary nature of the different cues. We further observe that the reconstruction quality as well as the improvements from adding geometric cues are similar for the train and test split of Omnidata, showing that the monocular predictor did not overfit to the training data.  \n\n\nB.2 Ablation of Different Architecture Configurations\n\nIn order to evaluate the performance with different model capacities, we consider MLPs with a different number of layers and Multi-res. Feature Grids with different sizes of the hash table. We list the number of learnable parameters using different architecture configurations in the Table 8, and show their performance over the optimization processes in Fig. 7. Our experiments show that using monocular geometric cues improves reconstruction quality and convergence speed independent of the network configuration.\n\n\nB.3 Ablation of Different Numbers of Input Views\n\nWe ran experiments with a different number of input images and monocular geometric cues. As shown in Fig. 6, adding the monocular geometric cues leads to consistent improvements across different numbers of input views. (c) Self-supervised Depth Table 9: Ablation of Different Monocular Cues Predictors. a.) Adding monocular depth improves performance over a single MLP without cues. Unsurprisingly, better depth predictors lead to better performance, with the state-of-the-art Omnidata model giving the best results. b.) Adding monocular normal improve the results. Similarly, using normals predicted by the state-of-the-art Omnidata model leads to the best performance. c.) Using self-supervised depth estimator degrades performance. We hypothesize that this is due to the weaker performance of the self-supervised model which is also trained with an RGB loss and hence suffers from the under-constrained problem of recovering geometry from multi-view images.\n\n\nB.4 Ablation of Different Monocular Cues Predictors\n\nTo further analyze the robustness of our approach to monocular geometric cues of different levels of quality, we further tested our model with different supervised depth predictors [56,83], normal predictors [15], and self-supervised depth predictors [37,86]. The result is shown in Table 9. We found that using the state-of-the-art Omnidata model leads to the best results, indicating that the development of better geometric cues will further improve the performance of our approach.\n\n\nB.5 Optimization Time\n\nAdding monocular geometric cues to the optimization introduces a small overhead to our overall optimization pipeline. First, predicting these cues with a pretrained Omnidata model is very efficient (36 FPS with an NVIDIA RTX3090 GPU). For example, it takes less than 26 seconds to predict both depth maps and normal maps for 464 images for one of the ScanNet scene. Note that this only needs to be done once and that we measure FPS with a batch size of one; using a larger batch size will result in a speed up. Second, we volume render depth and normals during optimization in order to apply a loss against these monocular cues. This overhead is also small and can be neglected since the most expensive part wrt. compute is the inference of the network. For our MLP variant, the additional flops for volume rendering depth and normal is only 0.0002% of the MLP inference time. While adding monocular geometric cues introduce a small overhead, the improvements in terms of reconstruction quality and converge speed are significant. As shown in Table 2 (b) in the main paper, with only 5k iterations, our Multi-Res. Grids representation with cues performs better than the converged models without geometric cues, which implies a 40\u00d7 speed up (5k vs. 200k).\n\n\nC Additional Results\n\nIn this section, we provide more qualitative and quantitative results for three datasets: ScanNet ( Section C.1), Tanks and Temples ( Section C.2), and DTU ( Section C.4).\n\n\nC.1 ScanNet\n\nWe report quantitative results with all metrics for ScanNet in Table 10 and show more visualizations in Fig. 9. Compared to state-of-the-art methods, our approach with MLP architecture produces significantly better reconstructions both visually as well as quantitatively. It's worth noting that we perform better than concurrent work [75] even though they have some filtering mechanism.  Table 10: Scene-level 3D Reconstruction on ScanNet. We report reconstruction results for our methods and baselines on ScanNet (baselines from [21]). We find that our approaches outperform previous state-of-the-art, highlighting the effectiveness of the use of monocular geometric priors. As ScanNet's RGB images contain motion blur and the camera poses are partially noisy, we further observe that the MLP architecture is more robust to this noise and achieves the best results. It's worth noting that we perform better than concurrent work [75] Table 11: Evaluation Results on the Tanks and Temples Dataset Advanced Set. We evaluate the reconstructed meshes using the official server and report the F-score with 10mm. Our monocular geometric cues improve the reconstruction quality for all scenes.\n\n\nC.2 Tanks and Temples\n\nWe show quantitative results for Tanks and Temples in Table 11. Qualitative comparisons of with or without monocular cues of our MLP variant are shown in Fig. 10 and Fig. 11. Fig. 12 and Fig. 13 show qualitative comparison of our Mulit-Res. Grids. Our monocular geometric cues significantly improve the reconstruction quality.\n\nWe further show an additional comparison against state-of-the-art MVS methods in Fig. 14. We use a pretrained Vis-MVSNet [89] to predict depth maps for the input images and fuse them to point clouds follow the official code. 5 Next, we use Meshlab's screened Poisson reconstruction [32] to reconstruct a mesh from point clouds with default parameters. We observe that our reconstructions are more complete which is useful for many applications. Further, reconstructing a mesh from point clouds involves lossy post-processing, leading to floating artifacts and bloated areas in less-observed areas.\n\n\nC.3 Preliminary Results of Using High-resolution Monocular Cues\n\nIn the main paper, we center-crop each image and resize it to 384 \u00d7 384. Then, we use a pretrained Omnidata model to predict depth maps and normal maps which are also of size 384 \u00d7 384. While we have shown that training at a resolution of 384 \u00d7 384 produces impressive results, we believe that exploring different ways to generate and integrate higher resolution cues could further improve reconstruction quality. Here, we provide a proof-of-concept experiment for generating higher resolution monocular cues and integrating them into our model. We use a divide-and-conquer method for generating high-resolution cues. First, we partition a high-resolution image to multiple overlapping sub-images, and we predict monocular depth and normal for each sub-image. Next, we merge these predictions. We use Eq. 21 to align the depth maps and solve the rotation for the normal  maps. An example of the resulting high-resolution monocular cues is shown in Fig. 15. We found that our high-resolution cues contain more fine details compared to low-resolution cues. Note that using other methods for generating high-resolution depth maps is also possible, e.g., [43]. We then use the high-resolution cues to train our model, and the results are shown in Fig. 16. We observe significant improvements when using high-resolution monocular cues.\n\n\nC.4 DTU\n\nGeometry. We show per-scene quantitative results on the DTU dataset with 3 input-views in Table 12 and more qualitative results in Fig. 17. We find that without the monocular geometric cues, both MLP and Multi-Res. Grids fail to produce satisfying reconstructions, while with our monocular cues, both methods are improved and are able to reconstruct high-quality meshes. We further show more visualizations on the DTU dataset using all input views in Fig. 19. Compared to state-of-the-art methods, our approach with multi-resolution feature grids produces more accurate reconstructions.\n\nPSNR MLP [81] 17.65 MLP w/ cues 23.64 Novel View Synthesis. We further compare our novel view synthesis results on the DTU dataset with three input views. As shown in Table 13 and Fig. 18, using monocular geometric cues improves novel view synthesis results significantly. Weight Annealing. As the monocular depth and normal predictor is not perfect, we exponentially anneal the loss weight for the monocular depth consistency and normal consistency loss, \u03bb 2 and \u03bb 3 , to 0 during the first 200 epochs of optimization. Qualitative comparison in Fig. 20 verifies the importance of weight annealing. Failure cases. We show a failure case on DTU with 3 input views in Fig. 21. The reconstructed mesh duplicates the object in front of each camera frustum. One reason is that the monocular depth cues that we use are only up to scale so they do not guarantee multi-view consistency. Therefore, the optimization is still underconstrained since the input RGB images and monocular cues can be explained by individual objects in front of the image plane. One possible solution would be incorporating explicit multi-view constraints such as using sparse point clouds from COLMAP [62] as an additional supervision [14].\n\n\nD Societal Impact\n\nOur method can faithfully reconstruct a 3D scene which can be used for application ranging from virtual reality to robotics. However, it can also have potential negative societal impact. First, our method relies on a general purpose monocular geometric predictor that needs to be trained on large amounts of data and with large computational resources, which potentially has a negative impact on global climate change. Second, accurate reconstruction of a scene may raise privacy concerns that need to be addressed carefully. Finally, accurate geometry reconstructed by our method can potentially be used for malicious purposes.  With monocular depth cues, the recovered geometry contains more details and a better overall structure. Similarly, with our normal cues, missing details are added and the results become smoother. Using both cues leads to the best performance. Zoom in for details.\n\n\n25\n\nCOLMAP [62] VolSDF [44] Manhattan-SDF [21] Ours (MLP) Ground Truth Figure 9: Qualitative Comparison on ScanNet. We show different views for each scene. Our method leads to better results containing smooth surfaces and detailed reconstructions compared against state-of-the-art neural implicit methods.\n\n\n26\n\nMLP [81] MLP w/ cues GT view Figure 10: Qualitative Comparison on Tanks & Temples. We use a single MLP as the scene geometry representation [81] and compare the reconstruction when using monocular cues or not on Auditorium and Ballroom.\n\n\n27\n\nMLP [81] MLP w/ cues GT view Figure 11: Qualitative Comparison on Tanks & Temples Dataset. We use a single MLP as the scene geometry representation [81] and compare the reconstruction quality when using monocular cues or not on Courtroom and Museum.   \n\nFigure 2 :\n2Figure 2: Overview. In this work we use monocular geometric cues predicted by a general-purpose pretrained network to guide the optimization of neural implicit surface models. More specifically, for a batch of rays, we volume render predicted RGB colors, depth, and normals, and optimize wrt. the input RGB images and monocular geometric cues. Further, we investigate different design choices for neural implicit architectures and provide an in-depth analysis. For clarity, we only show the SDF and not the color prediction branch above.\n\nFigure 5 :\n5Architectures. We show an overview over four different scene representations considered in this paper.\n\nFigure 8 :\n8Ablation of Monocular Geometric Cues on the Replica Dataset. Monocular geometric cues significantly improve reconstruction quality for all architectures.\n\nFigure 12 :Figure 13 :Figure 14 :\n121314Qualitative Comparison on Tanks & Temples. We use Multi-Res. Grids as the scene geometry representation and compare the reconstruction when using monocular cues or not on Auditorium and Ballroom. 29 Multi-Res. Grids Multi-Res. Grids w/ cues GT view Qualitative Comparison on Tanks & Temples. We use Multi-Res. Grids as the scene geometry representation and compare the reconstruction when using monocular cues or not on Courtroom and Museum. 30 VisMVSNet [89] Ours (MLP) Ours (Multi-Res. Grids) GT view Qualitative Comparison on Tanks & Temples. 31 (a) RGB Image. (b) Low Resolution Depth Map. (c) High Resolution Depth Map. (d) Low Resolution Normal Map. (e) High Resolution Normal Map.\n\nFigure 15 :Figure 16 :Figure 17 :Figure 18 :\n15161718Visual Qualitative Comparison of Low Resolution Cues and High Resolution cues on Tanks & Temples. We use Multi-Res. Grids as the scene geometry representation and compare the reconstruction when using different resolution of monocular cues. Qualitative Comparison on the DTU Dataset with 3 Input Views. Adding monocular geometric cues improves 3D reconstruction quality for both MLP and Multi-Res. Grids. We show a failure case on the last row. Qualitative Comparison of Novel View Synthesis on the DTU Dataset with 3 Input Views. Adding monocular geometric cues improves novel view synthesis quality.\n\nFigure 19 :Figure 20 :Figure 21 :\n192021Qualitative Comparison on DTU Dataset with all input views. Our approach with MLP achieves similar results with previous method, while our method with Multi-Res. Fea. Grids reconstruct more detailed surface. Ablation of Weight Annealing on the DTU Dataset with 3 Input Views. Using weight schedule improves reconstruction quality. Failure Case on DTU Dataset with 3 Input Views. The reconstructed mesh duplicate the object in front of each camera frustum.\n\n\n36th Conference on Neural Information Processing Systems (NeurIPS 2022). arXiv:2206.00665v2 [cs.CV] 12 Oct 2022VolSDF [81] \n\n+ Monocular Cues \nDTU (3 views) \nScanNet (464 views) \nTanks & Temples (298 views) \n\n\n\n\nNormal C.\u2191 Chamfer-L1 \u2193 F-score \u2191MLP [81] \n86.48 \n6.75 \n66.88 \nDense SDF Grid \n57.30 \n26.68 \n15.50 \nSingle-res. Fea. Grid \n86.41 \n6.28 \n64.22 \nMulti-res. Fea. Grids 87.95 \n5.03 \n78.38 \n\n\n\nTable 1 :\n1Architectural Ablation on Replica.\n\n\nAblation of Monocular Geometric Cues. Monocular geometric cues significantly improve reconstruction quality for both architectures (we show our MLP variant). With monocular depth cues, the recovered geometry contains more details and a better overall structure. With normal cues, missing details are added and the results become smoother. Using both cues leads to the best performance.Normal C.\u2191 Chamfer-L 1 \u2193 F-score \u2191No Cue \n\n+ Depth \n+ Normal \n+ Both \nGround Truth \n\nFigure 4: MLP \n\nNo Cues \n86.48 \n6.75 \n66.88 \nOnly Depth \n90.56 \n4.26 \n76.42 \nOnly Normal \n91.35 \n3.19 \n85.84 \nBoth Cues \n92.11 \n2.94 \n86.18 \nNo Cues \n87.95 \n5.03 \n78.38 \nMulti-Res. Only Depth \n90.87 \n3.75 \n80.32 \nGrids Only Normal \n89.90 \n3.61 \n81.28 \nBoth Cues \n90.93 \n3.23 \n85.91 \n\n5 \n20 \n40 \n60 \n\nIterations (\u00d710 3 ) \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1 \n\nF-score \nMLP \nMLP (w/ Cues) \nGrids \nGrids (w/ Cues) \n\n(a) Different Cues \n\n(b) Optimization Time \n\n\n\nTable 2 :\n2Ablation of Monocular Geometric Cues. a.) We report reconstruction results on Replica for MLP and Multi-Res.\n\nTable 3 :\n3Scene-level Reconstruction on ScanNet. Colmap and VolSDF do not lead to competitive reconstructions. Manhatten-SDF achieves compelling results, but less-observed areas are noisier and details are missing. In contrast, our approaches reconstruct smooth and details surfaces, achieving the best results. Further, MLPs are more robust to the motion blur and noise in camera poses.Tanks & Temples. To further investigate the scalability of our method to larger-scale scenes, we \nconduct experiments on the Tanks and Temples advanced sets. The qualitative results in Fig. 1 show \nthat the monocular cues significantly boost the performance of VolSDF [81], making MonoSDF the \nfirst neural implicit model achieving reasonable results on such a large-scale indoor scene. See the \nsupplementary material for more visual comparisons and discussions. \n\n\n\nTable 4 :\n4Reconstruction onDTU (3 Views). We report the av-\nerage over the test split from [81] \n(see supplementary for per-object \nresults). \n\n\n\nTest Split Train\nSplitSplit Normal C.\u2191 Chamfer-L1 \u2193 F-score \u2191 Normal C.\u2191 Chamfer-L1 \u2193 F-score \u2191No Cues \n57.30 \n26.68 \n15.50 \n60.86 \n17.34 \n26.34 \nDense SDF Only Depth \n71.81 \n12.60 \n30.09 \n73.15 \n13.09 \n30.30 \nGrid \nOnly Normal \n73.95 \n13.62 \n33.34 \n77.80 \n11.30 \n42.45 \nBoth Cues \n76.47 \n11.39 \n37.27 \n80.05 \n10.09 \n41.57 \n\nMLP \n\nNo Cues \n86.48 \n6.75 \n66.88 \n86.69 \n7.48 \n63.24 \nOnly Depth \n90.56 \n4.26 \n76.42 \n91.80 \n3.59 \n85.67 \nOnly Normal \n91.35 \n3.19 \n85.84 \n92.85 \n4.23 \n85.58 \nBoth Cues \n92.11 \n2.94 \n86.18 \n93.86 \n2.63 \n92.12 \nNo Cues \n86.41 \n6.28 \n64.22 \n86.54 \n6.63 \n67.26 \nSingle-Res. Only Depth \n90.50 \n3.94 \n78.42 \n91.3 \n3.29 \n86.34 \nGrids \nOnly Normal \n89.60 \n4.07 \n76.47 \n91.87 \n3.13 \n85.96 \nBoth Cues \n90.59 \n3.56 \n83.34 \n91.87 \n2.98 \n88.23 \nNo Cues \n87.95 \n5.03 \n78.38 \n87.15 \n5.83 \n72.13 \nMulti-Res. Only Depth \n90.87 \n3.75 \n80.32 \n91.25 \n3.41 \n87.04 \nGrids \nOnly Normal \n89.90 \n3.61 \n81.28 \n91.11 \n3.59 \n84.02 \nBoth Cues \n90.93 \n3.23 \n85.91 \n91.41 \n3.14 \n86.87 \n\n\n\nTable 7 :\n7Ablation of Monocular Geometric Cues on Replica.Our monocular geometric cues \n\n\n\nNumber of Learnable Parameters Using Different Architecture Configurations.20 \n40 \n\nIterations (\u00d710 3 ) \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1 \n\nF-score \n\nMLP 2 layer \nMLP 4 layer \nMLP 8 layer \nMLP 12 layer \nMLP 2 layer (w/ Cues) \nMLP 4 layer (w/ Cues) \nMLP 8 layer (w/ Cues) \nMLP 12 layer (w/ Cues) \nGrids 2^13 \nGrids 2^15 \nGrids 2^17 \nGrids 2^19 \nGrids 2^13 (w/ Cues) \nGrids 2^15 (w/ Cues) \nGrids 2^17 (w/ Cues) \nGrids 2^19 (w/ Cues) \n\nFigure 7: Optimization Processes Using Different Architecture Configurations. Using monocu-\nlar geometric cues improves reconstruction quality and convergence speed independent of the network \nconfigurations. \n\nModel configuration \nNum. Params \n\nMLP (2 layers) \n0.15M \nMLP (4 layers) \n0.26M \nMLP (8 layers) \n0.53M \nMLP (12 layers) \n0.8M \n\nMulti-res. Feature Grids (hash table size 2 13 ) \n0.41M \nMulti-res. Feature Grids (hash table size 2 15 ) \n1.11M \nMulti-res. Feature Grids (hash table size 2 17 ) \n3.67M \nMulti-res. Feature Grids (hash table size 2 19 ) \n12.67M \nTable 8: \n\n\nAcc\u2193 Comp\u2193 Chamfer-L1 \u2193 Prec\u2191 Recall\u2191 F-score\u2191COLMAP [62] \n0.047 \n0.235 \n0.141 \n0.711 \n0.441 \n0.537 \nUNISURF [49] \n0.554 \n0.164 \n0.359 \n0.212 \n0.362 \n0.267 \nNeuS [76] \n0.179 \n0.208 \n0.194 \n0.313 \n0.275 \n0.291 \nVolSDF [81] \n0.414 \n0.120 \n0.267 \n0.321 \n0.394 \n0.346 \nManhattan-SDF [21] \n0.072 \n0.068 \n0.070 \n0.621 \n0.586 \n0.602 \nNeuRIS [75] \n0.050 \n0.049 \n0.050 \n0.717 \n0.669 \n0.692 \n\nOurs (Multi-Res. Grids) 0.072 \n0.057 \n0.064 \n0.660 \n0.601 \n0.626 \nOurs (MLP) \n0.035 \n0.048 \n0.042 \n0.799 \n0.681 \n0.733 \n\n\n\n\nTSDF[12] COLMAP RealityCapture MLP [81] MLP Multi-Res. Multi-Res. w/ cues Grids Grids w/ cuesscan24 \n5.01 \n4.45 \n4.19 \n5.24 \n3.47 \n6.46 \n5.24 \nscan37 \n5.28 \n4.67 \n3.85 \n5.09 \n3.61 \n8.30 \n6.37 \nscan40 \n5.09 \n2.51 \n2.26 \n3.99 \n2.10 \n7.03 \n2.52 \nscan55 \n4.63 \n1.90 \n2.49 \n1.42 \n1.05 \n5.87 \n1.95 \nscan63 \n5.03 \n2.81 \n3.49 \n5.10 \n2.37 \n6.92 \n6.64 \nscan65 \n4.50 \n2.92 \n3.97 \n4.33 \n1.38 \n3.09 \n2.05 \nscan69 \n4.55 \n2.12 \n1.91 \n5.36 \n1.41 \n5.34 \n4.25 \nscan83 \n4.88 \n2.05 \n2.49 \n3.15 \n1.85 \n6.03 \n1.81 \nscan97 \n6.22 \n2.93 \n2.37 \n5.78 \n1.74 \n6.93 \n5.27 \nscan105 \n3.89 \n2.05 \n2.27 \n2.07 \n1.10 \n6.01 \n2.54 \nscan106 \n5.67 \n2.01 \n2.90 \n2.79 \n1.46 \n6.14 \n3.85 \nscan110 \n3.80 \nN/A \n4.60 \n5.73 \n2.28 \n7.62 \n3.89 \nscan114 \n4.67 \n1.10 \n1.38 \n1.20 \n1.25 \n6.27 \n1.90 \nscan118 \n4.51 \n2.72 \n2.57 \n5.64 \n1.44 \n7.59 \n3.12 \nscan122 \n4.35 \n1.64 \n1.76 \n6.20 \n1.45 \n6.47 \n3.84 \n\nmean \n4.80 \n2.56 \n2.84 \n4.21 \n1.86 \n6.47 \n3.68 \n\n\n\nTable 12 :\n12Evaluation Results on the DTU Dataset with 3 Input Views. Note the COLMAP fails on scan110 so we take the average over the remaining 14 scenes. We find that without geometric cues, neither Grids nor MLP works well with only 3 input views. When incorporating the monocular geometric cues, the results for both representations are significantly improved. Interestingly, the grid-based representations perform inferior to a single MLP as they are updated only locally and do not have an inductive smoothness bias compared to a monolithic MLP representation.\n\nTable 13 :\n13Novel view synthesis results on DTU (3 Views).\nManhattan-SDF[21] requires semantic segmentation to determine where to enforce the assumption.\nhttps://www.capturingreality.com/\nhttps://github.com/jzhangbs/DTUeval-python 4 https://www.tanksandtemples.org/ 5 Available at https://github.com/jzhangbs/Vis-MVSNet\nMulti-Res. GridsMulti-Res. Grids w/ cues GT view\nAcknowledgments and Disclosure of Funding\nLarge-scale data for multipleview stereopsis. H Aanaes, R R Jensen, G Vogiatzis, E Tola, A B Dahl, International Journal of Computer Vision (IJCV). 120218H. Aanaes, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl. Large-scale data for multiple- view stereopsis. International Journal of Computer Vision (IJCV), 120(2):153-168, 2016. 3, 7, 18\n\nA probabilistic framework for surface reconstruction from multiple images. M , L S Davis, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)M. Agrawal and L. S. Davis. A probabilistic framework for surface reconstruction from multiple images. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2001. 3\n\nSign agnostic learning of shapes from raw data. M Atzmon, Y Lipman, Sal, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)17M. Atzmon and Y. Lipman. SAL: Sign agnostic learning of shapes from raw data. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020. 17\n\nNeural rgb-d surface reconstruction. D Azinovi\u0107, R Martin-Brualla, D B Goldman, M Nie\u00dfner, J Thies, 2022. 10Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)D. Azinovi\u0107, R. Martin-Brualla, D. B. Goldman, M. Nie\u00dfner, and J. Thies. Neural rgb-d surface reconstruction. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022. 10\n\nJ T Barron, B Mildenhall, D Verbin, P P Srinivasan, P Hedman, Mip-nerf 360: Unbounded anti-aliased neural radiance fields. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2022J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman. Mip-nerf 360: Un- bounded anti-aliased neural radiance fields. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2\n\nPatchmatch stereo -stereo matching with slanted support windows. M Bleyer, C Rhemann, C Rother, Proc. of the British Machine Vision Conf. (BMVC). of the British Machine Vision Conf. (BMVC)M. Bleyer, C. Rhemann, and C. Rother. Patchmatch stereo -stereo matching with slanted support windows. In Proc. of the British Machine Vision Conf. (BMVC), 2011. 3\n\nPoxels: Probabilistic voxelized volume reconstruction. J D Bonet, P Viola, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)J. D. Bonet and P. Viola. Poxels: Probabilistic voxelized volume reconstruction. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 1999. 3\n\nA probabilistic framework for space carving. A Broadhurst, T W Drummond, R Cipolla, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)A. Broadhurst, T. W. Drummond, and R. Cipolla. A probabilistic framework for space carving. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2001. 3\n\nLearning implicit fields for generative shape modeling. Z Chen, H Zhang, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)13Z. Chen and H. Zhang. Learning implicit fields for generative shape modeling. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 1, 3\n\nImplicit functions in feature space for 3d shape reconstruction and completion. J Chibane, T Alldieck, G Pons-Moll, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 35J. Chibane, T. Alldieck, and G. Pons-Moll. Implicit functions in feature space for 3d shape re- construction and completion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2, 3, 5\n\nManhattan world: Compass direction from a single image by bayesian inference. J M Coughlan, A L Yuille, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)J. M. Coughlan and A. L. Yuille. Manhattan world: Compass direction from a single image by bayesian inference. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 1999. 3\n\nA volumetric method for building complex models from range images. B Curless, M Levoy, In ACM Trans. on Graphics. 923B. Curless and M. Levoy. A volumetric method for building complex models from range images. In ACM Trans. on Graphics, 1996. 7, 8, 9, 23\n\nScannet: Richlyannotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Niessner, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR718A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Niessner. Scannet: Richly- annotated 3d reconstructions of indoor scenes. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 3, 7, 18\n\nDepth-supervised nerf: Fewer views and faster training for free. K Deng, A Liu, J.-Y Zhu, D Ramanan, 2022. 23Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)K. Deng, A. Liu, J.-Y. Zhu, and D. Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022. 23\n\nSurface normal estimation of tilted images via spatial rectifier. T Do, K Vuong, S I Roumeliotis, H S Park, Proc. of the European Conference on Computer Vision, Virtual Conference. of the European Conference on Computer Vision, Virtual ConferenceT. Do, K. Vuong, S. I. Roumeliotis, and H. S. Park. Surface normal estimation of tilted images via spatial rectifier. In Proc. of the European Conference on Computer Vision, Virtual Conference, August 23-28 2020. 21\n\nLearning non-volumetric depth fusion using successive reprojections. S Donne, A Geiger, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)39S. Donne and A. Geiger. Learning non-volumetric depth fusion using successive reprojections. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. 3, 9\n\nOmnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scans. A Eftekhar, A Sax, J Malik, A Zamir, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)1921A. Eftekhar, A. Sax, J. Malik, and A. Zamir. Omnidata: A scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. 2, 3, 6, 10, 17, 19, 21\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2015. 2\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Advances in Neural Information Processing Systems (NIPS). 616D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in Neural Information Processing Systems (NIPS), 2014. 2, 6, 16\n\nImplicit geometric regularization for learning shapes. A Gropp, L Yariv, N Haim, M Atzmon, Y Lipman, Proc. of the International Conf. on Machine learning (ICML). of the International Conf. on Machine learning (ICML)A. Gropp, L. Yariv, N. Haim, M. Atzmon, and Y. Lipman. Implicit geometric regularization for learning shapes. In Proc. of the International Conf. on Machine learning (ICML), 2020. 6\n\nNeural 3d scene reconstruction with the manhattan-world assumption. H Guo, S Peng, H Lin, Q Wang, G Zhang, H Bao, X Zhou, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)1826H. Guo, S. Peng, H. Lin, Q. Wang, G. Zhang, H. Bao, and X. Zhou. Neural 3d scene recon- struction with the manhattan-world assumption. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 7, 8, 9, 18, 22, 26\n\nS Hadadan, S Chen, M Zwicker, Neural radiosity. 25S. Hadadan, S. Chen, and M. Zwicker. Neural radiosity. ACM Trans. Graph., 2021. 2, 3, 5\n\nLearned multi-patch similarity. W Hartmann, S Galliani, M Havlena, L Van Gool, K Schindler, 2017. 3Proc. of the IEEE International Conf. on Computer Vision (ICCV. of the IEEE International Conf. on Computer Vision (ICCVW. Hartmann, S. Galliani, M. Havlena, L. Van Gool, and K. Schindler. Learned multi-patch similarity. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2017. 3\n\nPutting objects in perspective. D Hoiem, A Efros, M Hebert, International Journal of Computer Vision (IJCV). 802D. Hoiem, A. Efros, and M. Hebert. Putting objects in perspective. International Journal of Computer Vision (IJCV), 80:3-15, 2008. 2\n\nAutomatic photo pop-up. D Hoiem, A A Efros, M Hebert, ACM Trans. on Graphics. 2D. Hoiem, A. A. Efros, and M. Hebert. Automatic photo pop-up. ACM Trans. on Graphics, 2005. 2\n\nGeometric context from a single image. D Hoiem, A A Efros, M Hebert, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)D. Hoiem, A. A. Efros, and M. Hebert. Geometric context from a single image. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2005. 2\n\nRecovering surface layout from an image. D Hoiem, A A Efros, M Hebert, International Journal of Computer Vision (IJCV). 751D. Hoiem, A. A. Efros, and M. Hebert. Recovering surface layout from an image. International Journal of Computer Vision (IJCV), 75(1):151-172, October 2007. 2\n\nDi-fusion: Online implicit 3d reconstruction with deep priors. J Huang, S.-S Huang, H Song, S.-M Hu, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 202125J. Huang, S.-S. Huang, H. Song, and S.-M. Hu. Di-fusion: Online implicit 3d reconstruction with deep priors. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 5\n\nDeepmvs: Learning multi-view stereopsis. P Huang, K Matzen, J Kopf, N Ahuja, J Huang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). P. Huang, K. Matzen, J. Kopf, N. Ahuja, and J. Huang. Deepmvs: Learning multi-view stereopsis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3\n\nPutting nerf on a diet: Semantically consistent few-shot view synthesis. A Jain, M Tancik, P Abbeel, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)23A. Jain, M. Tancik, and P. Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. 2, 3\n\nSdfdiff: Differentiable rendering of signed distance fields for 3d shape optimization. Y Jiang, D Ji, Z Han, M Zwicker, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 202024Y. Jiang, D. Ji, Z. Han, and M. Zwicker. Sdfdiff: Differentiable rendering of signed dis- tance fields for 3d shape optimization. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020. 2, 4\n\nScreened poisson surface reconstruction. M M Kazhdan, H Hoppe, ACM Trans. on Graphics. 32322M. M. Kazhdan and H. Hoppe. Screened poisson surface reconstruction. ACM Trans. on Graphics, 32(3):29, 2013. 22\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, Proc. of the International Conf. on Machine learning (ICML). of the International Conf. on Machine learning (ICML)D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. of the International Conf. on Machine learning (ICML), 2015. 6\n\nTanks and temples: Benchmarking large-scale scene reconstruction. A Knapitsch, J Park, Q.-Y Zhou, V Koltun, ACM Trans. on Graphics. 36418A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Trans. on Graphics, 36(4), 2017. 3, 7, 18\n\nA theory of shape by space carving. K N Kutulakos, S M Seitz, International Journal of Computer Vision (IJCV). 383K. N. Kutulakos and S. M. Seitz. A theory of shape by space carving. International Journal of Computer Vision (IJCV), 38(3):199-218, 2000. 3\n\nShape reconstruction using volume sweeping and learned photoconsistency. V Leroy, J Franco, E Boyer, Proc. of the European Conf. on Computer Vision (ECCV). of the European Conf. on Computer Vision (ECCV)V. Leroy, J. Franco, and E. Boyer. Shape reconstruction using volume sweeping and learned photoconsistency. In Proc. of the European Conf. on Computer Vision (ECCV), 2018. 3\n\nStructdepth: Leveraging the structural regularities for self-supervised indoor depth estimation. B Li, Y Huang, Z Liu, D Zou, W Yu, 2021. 21Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionB. Li, Y. Huang, Z. Liu, D. Zou, and W. Yu. Structdepth: Leveraging the structural regular- ities for self-supervised indoor depth estimation. In Proceedings of the IEEE International Conference on Computer Vision, 2021. 21\n\nNeural sparse voxel fields. L Liu, J Gu, K Z Lin, T Chua, C Theobalt, Advances in Neural Information Processing Systems (NeurIPS). 25L. Liu, J. Gu, K. Z. Lin, T. Chua, and C. Theobalt. Neural sparse voxel fields. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 2, 5\n\nDIST: Rendering deep implicit signed distance function with differentiable sphere tracing. S Liu, Y Zhang, S Peng, B Shi, M Pollefeys, Z Cui, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)2020S. Liu, Y. Zhang, S. Peng, B. Shi, M. Pollefeys, and Z. Cui. DIST: Rendering deep implicit signed distance function with differentiable sphere tracing. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020. 3\n\nEfficient deep learning for stereo matching. W Luo, A Schwing, R Urtasun, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)W. Luo, A. Schwing, and R. Urtasun. Efficient deep learning for stereo matching. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016. 3\n\nAcorn: Adaptive coordinate networks for neural scene representation. J N Martel, D B Lindell, C Z Lin, E R Chan, M Monteiro, G Wetzstein, In ACM Trans. on Graphics. 3J. N. Martel, D. B. Lindell, C. Z. Lin, E. R. Chan, M. Monteiro, and G. Wetzstein. Acorn: Adaptive coordinate networks for neural scene representation. In ACM Trans. on Graphics, 2021. 3\n\nOccupancy networks: Learning 3d reconstruction in function space. L Mescheder, M Oechsle, M Niemeyer, S Nowozin, A Geiger, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)718L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. 1, 3, 7, 18\n\nBoosting monocular depth estimation models to high-resolution via content-adaptive multi-resolution merging. S M H Miangoleh, S Dille, L Mai, S Paris, Y Aksoy, CVPR. S. M. H. Miangoleh, S. Dille, L. Mai, S. Paris, and Y. Aksoy. Boosting monocular depth estimation models to high-resolution via content-adaptive multi-resolution merging. In CVPR, 2021. 23\n\nNeRF: Representing scenes as neural radiance fields for view synthesis. B Mildenhall, P P Srinivasan, M Tancik, J T Barron, R Ramamoorthi, R Ng, Proc. of the European Conf. on Computer Vision (ECCV). of the European Conf. on Computer Vision (ECCV)1026B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proc. of the European Conf. on Computer Vision (ECCV), 2020. 1, 3, 4, 5, 9, 10, 26\n\nInstant neural graphics primitives with a multiresolution hash encoding. T M\u00fcller, A Evans, C Schied, A Keller, ACM Trans. on Graphics. 716T. M\u00fcller, A. Evans, C. Schied, and A. Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. on Graphics, 2022. 2, 3, 5, 6, 7, 16\n\nRegnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. M Niemeyer, J T Barron, B Mildenhall, M S Sajjadi, A Geiger, N Radwan, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)79M. Niemeyer, J. T. Barron, B. Mildenhall, M. S. Sajjadi, A. Geiger, and N. Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 7, 9\n\nOccupancy flow: 4d reconstruction by learning particle dynamics. M Niemeyer, L Mescheder, M Oechsle, A Geiger, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger. Occupancy flow: 4d reconstruction by learning particle dynamics. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2019. 1\n\nDifferentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. M Niemeyer, L Mescheder, M Oechsle, A Geiger, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)2020M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020. 3\n\nUnisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. M Oechsle, S Peng, A Geiger, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)1022M. Oechsle, S. Peng, and A. Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. 1, 2, 3, 5, 7, 9, 10, 22\n\nDeepsdf: Learning continuous signed distance functions for shape representation. J J Park, P Florence, J Straub, R A Newcombe, S Lovegrove, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)34J. J. Park, P. Florence, J. Straub, R. A. Newcombe, and S. Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. 1, 3, 4\n\nRaynet: Learning volumetric 3d reconstruction with ray potentials. D Paschalidou, A O Ulusoy, C Schmitt, L Van Gool, A Geiger, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)D. Paschalidou, A. O. Ulusoy, C. Schmitt, L. van Gool, and A. Geiger. Raynet: Learning volumetric 3d reconstruction with ray potentials. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2018. 3\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Advances in Neural Information Processing Systems (NIPS). A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NIPS), 2019. 6\n\nShape as points: A differentiable poisson solver. S Peng, C M Jiang, Y Liao, M Niemeyer, M Pollefeys, A Geiger, Advances in Neural Information Processing Systems (NeurIPS). 718S. Peng, C. M. Jiang, Y. Liao, M. Niemeyer, M. Pollefeys, and A. Geiger. Shape as points: A differentiable poisson solver. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 2, 7, 18\n\nConvolutional occupancy networks. S Peng, M Niemeyer, L Mescheder, M Pollefeys, A Geiger, Proc. of the European Conf. on Computer Vision (ECCV). of the European Conf. on Computer Vision (ECCV)718S. Peng, M. Niemeyer, L. Mescheder, M. Pollefeys, and A. Geiger. Convolutional occupancy networks. In Proc. of the European Conf. on Computer Vision (ECCV), 2020. 2, 3, 5, 7, 18\n\nVision transformers for dense prediction. R Ranftl, A Bochkovskiy, V Koltun, ArXiv preprintR. Ranftl, A. Bochkovskiy, and V. Koltun. Vision transformers for dense prediction. ArXiv preprint, 2021. 2\n\nTowards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. R Ranftl, K Lasinger, D Hafner, K Schindler, V Koltun, IEEE Trans. on Pattern Analysis and Machine Intelligence. 1621PAMIR. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), 2020. 2, 6, 16, 21\n\nOctNetFusion: Learning depth fusion from data. G Riegler, A O Ulusoy, H Bischof, A Geiger, 2017. 3Proc. of the International Conf. on 3D Vision. of the International Conf. on 3D VisionG. Riegler, A. O. Ulusoy, H. Bischof, and A. Geiger. OctNetFusion: Learning depth fusion from data. In Proc. of the International Conf. on 3D Vision (3DV), 2017. 3\n\nDense depth priors for neural radiance fields from sparse input views. B Roessle, J T Barron, B Mildenhall, P P Srinivasan, M Nie\u00dfner, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)B. Roessle, J. T. Barron, B. Mildenhall, P. P. Srinivasan, and M. Nie\u00dfner. Dense depth priors for neural radiance fields from sparse input views. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 3\n\nLearning depth from single monocular images. A Saxena, S H Chung, A Y Ng, Advances in Neural Information Processing Systems (NIPS). A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth from single monocular images. In Advances in Neural Information Processing Systems (NIPS), 2006. 2\n\n3-D depth reconstruction from a single still image. A Saxena, S H Chung, A Y Ng, International Journal of Computer Vision (IJCV). 762A. Saxena, S. H. Chung, and A. Y. Ng. 3-D depth reconstruction from a single still image. International Journal of Computer Vision (IJCV), 76:53-69, 2008. 2\n\nMake3D: learning 3D scene structure from a single still image. A Saxena, M Sun, A Y Ng, IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI). 312A. Saxena, M. Sun, and A. Y. Ng. Make3D: learning 3D scene structure from a single still image. IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), 31:824-840, 2009. 2\n\nPixelwise view selection for unstructured multi-view stereo. J L Sch\u00f6nberger, E Zheng, M Pollefeys, J.-M Frahm, Proc. of the European Conf. on Computer Vision (ECCV). of the European Conf. on Computer Vision (ECCV)2326J. L. Sch\u00f6nberger, E. Zheng, M. Pollefeys, and J.-M. Frahm. Pixelwise view selection for unstructured multi-view stereo. In Proc. of the European Conf. on Computer Vision (ECCV), 2016. 3, 7, 9, 22, 23, 26\n\nStructure-from-motion revisited. J L Sch\u00f6nberger, J.-M Frahm, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)J. L. Sch\u00f6nberger and J.-M. Frahm. Structure-from-motion revisited. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016. 3\n\nPhotorealistic scene reconstruction by voxel coloring. S Seitz, C Dyer, 1997. 3Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR. IEEE Conf. on Computer Vision and Pattern Recognition (CVPRS. Seitz and C. Dyer. Photorealistic scene reconstruction by voxel coloring. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 1997. 3\n\nA comparison and evaluation of multi-view stereo reconstruction algorithms. S M Seitz, B Curless, J Diebel, D Scharstein, R Szeliski, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R. Szeliski. A comparison and evaluation of multi-view stereo reconstruction algorithms. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2006. 3\n\nScene representation networks: Continuous 3d-structure-aware neural scene representations. V Sitzmann, M Zollh\u00f6fer, G Wetzstein, Advances in Neural Information Processing Systems (NIPS). V. Sitzmann, M. Zollh\u00f6fer, and G. Wetzstein. Scene representation networks: Continuous 3d-structure-aware neural scene representations. In Advances in Neural Information Processing Systems (NIPS), 2019. 1\n\n. J Straub, T Whelan, L Ma, Y Chen, E Wijmans, S Green, J J Engel, R Mur-Artal, C Ren, S Verma, A Clarkson, M Yan, B Budge, Y Yan, X Pan, J Yon, Y Zou, K Leon, N Carter, J Briales, T Gillingham, E Mueggler, L Pesqueira, M Savva, D Batra, H M Strasdat, R D , 13J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva, D. Batra, H. M. Strasdat, R. D. 13\n\nM Nardi, S Goesele, R Lovegrove, Newcombe, The Replica dataset: A digital replica of indoor spaces. arXiv.org. 18Nardi, M. Goesele, S. Lovegrove, and R. Newcombe. The Replica dataset: A digital replica of indoor spaces. arXiv.org, 1906.05797, 2019. 3, 7, 18\n\niMAP: Implicit mapping and positioning in real-time. E Sucar, S Liu, J Ortiz, A Davison, Proc. of the IEEE International Conf. on Computer Vision (ICCV). of the IEEE International Conf. on Computer Vision (ICCV)718E. Sucar, S. Liu, J. Ortiz, and A. Davison. iMAP: Implicit mapping and positioning in real-time. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. 7, 18\n\nNeural geometric level of detail: Real-time rendering with implicit 3D shapes. T Takikawa, J Litalien, K Yin, K Kreis, C Loop, D Nowrouzezahrai, A Jacobson, M Mcguire, S Fidler, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)35T. Takikawa, J. Litalien, K. Yin, K. Kreis, C. Loop, D. Nowrouzezahrai, A. Jacobson, M. McGuire, and S. Fidler. Neural geometric level of detail: Real-time rendering with implicit 3D shapes. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 3, 5\n\nFourier features let networks learn high frequency functions in low dimensional domains. M Tancik, P Srinivasan, B Mildenhall, S Fridovich-Keil, N Raghavan, U Singhal, R Ramamoorthi, J Barron, R Ng, Advances in Neural Information Processing Systems (NeurIPS). 45M. Tancik, P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ra- mamoorthi, J. Barron, and R. Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Advances in Neural Information Processing Systems (NeurIPS), 2020. 4, 5\n\nOptimized spatial hashing for collision detection of deformable objects. M Teschner, B Heidelberger, M M\u00fcller, D Pomeranets, M Gross, Proceedings of VMV'03. VMV'03Munich, Germany16M. Teschner, B. Heidelberger, M. M\u00fcller, D. Pomeranets, and M. Gross. Optimized spatial hashing for collision detection of deformable objects. In Proceedings of VMV'03, Munich, Germany, 2003. 16\n\nMulti-view supervision for single-view reconstruction via differentiable ray consistency. S Tulsiani, T Zhou, A A Efros, J Malik, 2017. 3Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionS. Tulsiani, T. Zhou, A. A. Efros, and J. Malik. Multi-view supervision for single-view reconstruction via differentiable ray consistency. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 3\n\nTowards probabilistic volumetric reconstruction using ray potentials. A O Ulusoy, A Geiger, M J Black, Proc. of the International Conf. on 3D Vision (3DV). of the International Conf. on 3D Vision (3DV)A. O. Ulusoy, A. Geiger, and M. J. Black. Towards probabilistic volumetric reconstruction using ray potentials. In Proc. of the International Conf. on 3D Vision (3DV), 2015. 3\n\nDemon: Depth and motion network for learning monocular stereo. B Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg, A Dosovitskiy, T Brox, 2017. 3Proc. IEEE Conf. on Computer Vision and Pattern Recognition. IEEE Conf. on Computer Vision and Pattern RecognitionB. Ummenhofer, H. Zhou, J. Uhrig, N. Mayer, E. Ilg, A. Dosovitskiy, and T. Brox. Demon: Depth and motion network for learning monocular stereo. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2017. 3\n\nNeuris: Neural reconstruction of indoor scenes using normal priors. J Wang, P Wang, X Long, C Theobalt, T Komura, L Liu, W Wang, ECCV. 2122J. Wang, P. Wang, X. Long, C. Theobalt, T. Komura, L. Liu, and W. Wang. Neuris: Neural reconstruction of indoor scenes using normal priors. In ECCV, 2022. 3, 8, 9, 21, 22\n\nNeus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. P Wang, L Liu, Y Liu, C Theobalt, T Komura, W Wang, Advances in Neural Information Processing Systems (NeurIPS). 22P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 1, 2, 3, 5, 7, 9, 22\n\nMetaavatar: Learning animatable clothed human models from few depth images. S Wang, M Mihajlovic, Q Ma, A Geiger, S Tang, Advances in Neural Information Processing Systems (NeurIPS). 910S. Wang, M. Mihajlovic, Q. Ma, A. Geiger, and S. Tang. Metaavatar: Learning animatable clothed human models from few depth images. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 9, 10\n\nY Xie, T Takikawa, S Saito, O Litany, S Yan, N Khan, F Tombari, J Tompkin, V Sitzmann, S Sridhar, Neural fields in visual computing and beyond. 2022EUROGRAPHICSY. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan, F. Tombari, J. Tompkin, V. Sitzmann, and S. Sridhar. Neural fields in visual computing and beyond. In EUROGRAPHICS, 2022. 3\n\nMvsnet: Depth inference for unstructured multi-view stereo. Y Yao, Z Luo, S Li, T Fang, L Quan, Proc. of the European Conf. on Computer Vision (ECCV). of the European Conf. on Computer Vision (ECCV)Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan. Mvsnet: Depth inference for unstructured multi-view stereo. Proc. of the European Conf. on Computer Vision (ECCV), 2018. 3\n\nRecurrent mvsnet for high-resolution multiview stereo depth inference. Y Yao, Z Luo, S Li, T Shen, T Fang, L Quan, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)Y. Yao, Z. Luo, S. Li, T. Shen, T. Fang, and L. Quan. Recurrent mvsnet for high-resolution multi- view stereo depth inference. Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. 3\n\nVolume rendering of neural implicit surfaces. L Yariv, J Gu, Y Kasten, Y Lipman, Advances in Neural Information Processing Systems (NeurIPS). 3435L. Yariv, J. Gu, Y. Kasten, and Y. Lipman. Volume rendering of neural implicit surfaces. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 1, 2, 3, 5, 6, 7, 9, 10, 22, 23, 27, 28, 34, 35\n\nMultiview neural surface reconstruction by disentangling geometry and appearance. L Yariv, Y Kasten, D Moran, M Galun, M Atzmon, B Ronen, Y Lipman, Advances in Neural Information Processing Systems (NIPS). 59L. Yariv, Y. Kasten, D. Moran, M. Galun, M. Atzmon, B. Ronen, and Y. Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. In Advances in Neural Information Processing Systems (NIPS), 2020. 1, 2, 3, 5, 9\n\nLearning to recover 3d scene shape from a single image. W Yin, J Zhang, O Wang, S Niklaus, L Mai, S Chen, C Shen, 2021. 21Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR). IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR)W. Yin, J. Zhang, O. Wang, S. Niklaus, L. Mai, S. Chen, and C. Shen. Learning to recover 3d scene shape from a single image. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn. (CVPR), 2021. 21\n\npixelNeRF: Neural radiance fields from one or few images. A Yu, V Ye, M Tancik, A Kanazawa, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021. 7\n\nFast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement. Z Yu, S Gao, CVPR. 2020Z. Yu and S. Gao. Fast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement. In CVPR, 2020. 3\n\nP 2 net: Patch-match and plane-regularization for unsupervised indoor depth estimation. Z Yu, L Jin, S Gao, ECCV. Z. Yu, L. Jin, and S. Gao. P 2 net: Patch-match and plane-regularization for unsupervised indoor depth estimation. In ECCV, 2020. 21\n\nSingle-image piece-wise planar 3d reconstruction via associative embedding. Z Yu, J Zheng, D Lian, Z Zhou, S Gao, CVPR. Z. Yu, J. Zheng, D. Lian, Z. Zhou, and S. Gao. Single-image piece-wise planar 3d reconstruction via associative embedding. In CVPR, pages 1029-1037, 2019. 10\n\nLearning to compare image patches via convolutional neural networks. S Zagoruyko, N Komodakis, Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)314S. Zagoruyko and N. Komodakis. Learning to compare image patches via convolutional neural networks. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2015. 3 14\n\nVisibility-aware multi-view stereo network. J Zhang, Y Yao, S Li, Z Luo, T Fang, Proc. of the British Machine Vision Conf. (BMVC). of the British Machine Vision Conf. (BMVC)2231J. Zhang, Y. Yao, S. Li, Z. Luo, and T. Fang. Visibility-aware multi-view stereo network. In Proc. of the British Machine Vision Conf. (BMVC), 2020. 22, 31\n\nK Zhang, G Riegler, N Snavely, V Koltun, arXiv:2010.07492Nerf++: Analyzing and improving neural radiance fields. K. Zhang, G. Riegler, N. Snavely, and V. Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv:2010.07492, 2020. 2\n\nX Zhang, P P Srinivasan, B Deng, P Debevec, W T Freeman, J T Barron, arXiv:2106.01970NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination. arXiv preprintX. Zhang, P. P. Srinivasan, B. Deng, P. Debevec, W. T. Freeman, and J. T. Barron. NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination. arXiv preprint arXiv:2106.01970, 2021. 2\n\nNice-slam: Neural implicit scalable encoding for slam. Z Zhu, S Peng, V Larsson, W Xu, H Bao, Z Cui, M R Oswald, M Pollefeys, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)18Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys. Nice-slam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 5, 7, 10, 18\n", "annotations": {"author": "[{\"end\":124,\"start\":90},{\"end\":189,\"start\":125},{\"end\":270,\"start\":190},{\"end\":322,\"start\":271},{\"end\":401,\"start\":323}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":96},{\"end\":137,\"start\":133},{\"end\":206,\"start\":198},{\"end\":286,\"start\":279},{\"end\":337,\"start\":331}]", "author_first_name": "[{\"end\":95,\"start\":90},{\"end\":132,\"start\":125},{\"end\":197,\"start\":190},{\"end\":278,\"start\":271},{\"end\":330,\"start\":323}]", "author_affiliation": "[{\"end\":123,\"start\":100},{\"end\":150,\"start\":139},{\"end\":188,\"start\":152},{\"end\":231,\"start\":208},{\"end\":269,\"start\":233},{\"end\":321,\"start\":288},{\"end\":362,\"start\":339},{\"end\":400,\"start\":364}]", "title": "[{\"end\":87,\"start\":1},{\"end\":488,\"start\":402}]", "venue": null, "abstract": "[{\"end\":1946,\"start\":490}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2449,\"start\":2446},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2452,\"start\":2449},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2455,\"start\":2452},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2507,\"start\":2503},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":2510,\"start\":2507},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":2513,\"start\":2510},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2650,\"start\":2646},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2738,\"start\":2734},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":2741,\"start\":2738},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":2744,\"start\":2741},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4003,\"start\":4000},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":4006,\"start\":4003},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4156,\"start\":4152},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4181,\"start\":4177},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":4184,\"start\":4181},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4210,\"start\":4206},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4247,\"start\":4243},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4647,\"start\":4643},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4650,\"start\":4647},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4785,\"start\":4781},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4789,\"start\":4785},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4793,\"start\":4789},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4797,\"start\":4793},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":4801,\"start\":4797},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":4805,\"start\":4801},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":4809,\"start\":4805},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4827,\"start\":4823},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":4830,\"start\":4827},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4833,\"start\":4830},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4862,\"start\":4858},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6642,\"start\":6638},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":6645,\"start\":6642},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":6648,\"start\":6645},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":6651,\"start\":6648},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6674,\"start\":6670},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6702,\"start\":6698},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6705,\"start\":6702},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6708,\"start\":6705},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6711,\"start\":6708},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6751,\"start\":6747},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6754,\"start\":6751},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6757,\"start\":6754},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":6760,\"start\":6757},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":6763,\"start\":6760},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7868,\"start\":7865},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7916,\"start\":7912},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7934,\"start\":7930},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8004,\"start\":8000},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":8138,\"start\":8134},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8271,\"start\":8268},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8274,\"start\":8271},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8276,\"start\":8274},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8499,\"start\":8495},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8502,\"start\":8499},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8505,\"start\":8502},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8508,\"start\":8505},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8511,\"start\":8508},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":8514,\"start\":8511},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":8517,\"start\":8514},{\"end\":9131,\"start\":9103},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9189,\"start\":9186},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":9192,\"start\":9189},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9228,\"start\":9225},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9230,\"start\":9228},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9232,\"start\":9230},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9235,\"start\":9232},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9238,\"start\":9235},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9241,\"start\":9238},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":9244,\"start\":9241},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":9247,\"start\":9244},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9359,\"start\":9355},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9362,\"start\":9359},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9365,\"start\":9362},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":9368,\"start\":9365},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":9371,\"start\":9368},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9390,\"start\":9386},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9393,\"start\":9390},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9441,\"start\":9437},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":9444,\"start\":9441},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":9447,\"start\":9444},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":9450,\"start\":9447},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9563,\"start\":9559},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9566,\"start\":9563},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":9569,\"start\":9566},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9854,\"start\":9850},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9941,\"start\":9937},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":9944,\"start\":9941},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":9947,\"start\":9944},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10443,\"start\":10439},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10469,\"start\":10465},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10502,\"start\":10498},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10671,\"start\":10667},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":10714,\"start\":10710},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10768,\"start\":10764},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10945,\"start\":10941},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11123,\"start\":11119},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":11225,\"start\":11221},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13442,\"start\":13438},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":13727,\"start\":13723},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13841,\"start\":13837},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":13843,\"start\":13841},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13939,\"start\":13935},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14038,\"start\":14034},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":14041,\"start\":14038},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":14044,\"start\":14041},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":14047,\"start\":14044},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":14117,\"start\":14113},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14366,\"start\":14362},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14369,\"start\":14366},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":14372,\"start\":14369},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":14375,\"start\":14372},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14745,\"start\":14741},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14748,\"start\":14745},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14751,\"start\":14748},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":14754,\"start\":14751},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":14757,\"start\":14754},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14810,\"start\":14806},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15320,\"start\":15316},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":15513,\"start\":15509},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":15810,\"start\":15806},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16105,\"start\":16101},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":16108,\"start\":16105},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":16111,\"start\":16108},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":16114,\"start\":16111},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":16503,\"start\":16499},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":16710,\"start\":16706},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17832,\"start\":17828},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18885,\"start\":18881},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":19071,\"start\":19067},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19571,\"start\":19567},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":19574,\"start\":19571},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19840,\"start\":19836},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":20119,\"start\":20115},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20151,\"start\":20147},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":20403,\"start\":20399},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":20526,\"start\":20522},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20535,\"start\":20531},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20706,\"start\":20702},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":21873,\"start\":21869},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21890,\"start\":21886},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21955,\"start\":21951},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22004,\"start\":22001},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":22043,\"start\":22039},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":22046,\"start\":22043},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22144,\"start\":22140},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":22157,\"start\":22153},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":22168,\"start\":22164},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22192,\"start\":22188},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":22229,\"start\":22225},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22313,\"start\":22309},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22693,\"start\":22689},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22696,\"start\":22693},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22699,\"start\":22696},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":22702,\"start\":22699},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":22705,\"start\":22702},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":22708,\"start\":22705},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24221,\"start\":24217},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26526,\"start\":26522},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26666,\"start\":26662},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":26872,\"start\":26868},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":27396,\"start\":27392},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":27399,\"start\":27396},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27450,\"start\":27446},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":27576,\"start\":27572},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27953,\"start\":27949},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28238,\"start\":28234},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":28894,\"start\":28890},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":28897,\"start\":28894},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":28900,\"start\":28897},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29377,\"start\":29373},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":29380,\"start\":29377},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":29383,\"start\":29380},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31495,\"start\":31491},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":31498,\"start\":31495},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31634,\"start\":31630},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31860,\"start\":31857},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":31863,\"start\":31860},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":33232,\"start\":33228},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":33906,\"start\":33902},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":34163,\"start\":34159},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34535,\"start\":34531},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":34538,\"start\":34535},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34642,\"start\":34638},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35633,\"start\":35630},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35814,\"start\":35810},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36243,\"start\":36240},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":37627,\"start\":37623},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":37644,\"start\":37640},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":37767,\"start\":37763},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":37770,\"start\":37767},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":37773,\"start\":37770},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":37850,\"start\":37846},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":37853,\"start\":37850},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":37856,\"start\":37853},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":37859,\"start\":37856},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":37862,\"start\":37859},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":37865,\"start\":37862},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":37986,\"start\":37982},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39286,\"start\":39282},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39354,\"start\":39350},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":41711,\"start\":41707},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":41714,\"start\":41711},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41738,\"start\":41734},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":41781,\"start\":41777},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":41784,\"start\":41781},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":43841,\"start\":43837},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":44037,\"start\":44033},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":44436,\"start\":44432},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":45168,\"start\":45164},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":45269,\"start\":45268},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":45329,\"start\":45325},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":46863,\"start\":46859},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":47651,\"start\":47647},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":48812,\"start\":48808},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":48846,\"start\":48842},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":49780,\"start\":49776},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":49792,\"start\":49788},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":49811,\"start\":49807},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":50085,\"start\":50081},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":50221,\"start\":50217},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":50328,\"start\":50324},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":50472,\"start\":50468},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":58383,\"start\":58379},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":59921,\"start\":59917}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51123,\"start\":50573},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51239,\"start\":51124},{\"attributes\":{\"id\":\"fig_3\"},\"end\":51406,\"start\":51240},{\"attributes\":{\"id\":\"fig_4\"},\"end\":52135,\"start\":51407},{\"attributes\":{\"id\":\"fig_5\"},\"end\":52791,\"start\":52136},{\"attributes\":{\"id\":\"fig_6\"},\"end\":53288,\"start\":52792},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":53500,\"start\":53289},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":53689,\"start\":53501},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":53736,\"start\":53690},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":54658,\"start\":53737},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":54779,\"start\":54659},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":55635,\"start\":54780},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":55782,\"start\":55636},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":56767,\"start\":55783},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":56858,\"start\":56768},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":57865,\"start\":56859},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":58372,\"start\":57866},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":59273,\"start\":58373},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":59842,\"start\":59274},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":59903,\"start\":59843}]", "paragraph": "[{\"end\":2254,\"start\":1962},{\"end\":2927,\"start\":2256},{\"end\":3985,\"start\":2929},{\"end\":4490,\"start\":3987},{\"end\":5308,\"start\":4492},{\"end\":6329,\"start\":5310},{\"end\":7317,\"start\":6331},{\"end\":7367,\"start\":7319},{\"end\":7563,\"start\":7369},{\"end\":7734,\"start\":7565},{\"end\":8005,\"start\":7736},{\"end\":12027,\"start\":8022},{\"end\":12610,\"start\":12038},{\"end\":12844,\"start\":12645},{\"end\":13557,\"start\":12877},{\"end\":13650,\"start\":13583},{\"end\":13733,\"start\":13652},{\"end\":14118,\"start\":13754},{\"end\":14415,\"start\":14120},{\"end\":14558,\"start\":14453},{\"end\":14856,\"start\":14560},{\"end\":15080,\"start\":14914},{\"end\":15353,\"start\":15126},{\"end\":15552,\"start\":15355},{\"end\":16037,\"start\":15578},{\"end\":16582,\"start\":16079},{\"end\":16786,\"start\":16657},{\"end\":17145,\"start\":16877},{\"end\":17214,\"start\":17213},{\"end\":18478,\"start\":17254},{\"end\":18685,\"start\":18495},{\"end\":18944,\"start\":18719},{\"end\":19217,\"start\":18987},{\"end\":19841,\"start\":19258},{\"end\":19994,\"start\":19897},{\"end\":21137,\"start\":20056},{\"end\":21574,\"start\":21153},{\"end\":22819,\"start\":21576},{\"end\":23063,\"start\":22838},{\"end\":23690,\"start\":23065},{\"end\":26263,\"start\":23692},{\"end\":27202,\"start\":26311},{\"end\":27917,\"start\":27252},{\"end\":28662,\"start\":27919},{\"end\":29247,\"start\":28711},{\"end\":30083,\"start\":29249},{\"end\":30251,\"start\":30085},{\"end\":31010,\"start\":30266},{\"end\":31982,\"start\":31027},{\"end\":32480,\"start\":32100},{\"end\":32886,\"start\":32509},{\"end\":33186,\"start\":32908},{\"end\":33356,\"start\":33188},{\"end\":33957,\"start\":33415},{\"end\":34183,\"start\":33991},{\"end\":34336,\"start\":34214},{\"end\":34661,\"start\":34377},{\"end\":34789,\"start\":34754},{\"end\":35098,\"start\":34826},{\"end\":36193,\"start\":35125},{\"end\":36892,\"start\":36220},{\"end\":36922,\"start\":36914},{\"end\":37214,\"start\":37165},{\"end\":37609,\"start\":37333},{\"end\":37946,\"start\":37611},{\"end\":38093,\"start\":37948},{\"end\":38931,\"start\":38108},{\"end\":39355,\"start\":38966},{\"end\":39884,\"start\":39357},{\"end\":40457,\"start\":39942},{\"end\":41470,\"start\":40510},{\"end\":42011,\"start\":41526},{\"end\":43291,\"start\":42037},{\"end\":43487,\"start\":43316},{\"end\":44689,\"start\":43503},{\"end\":45041,\"start\":44715},{\"end\":45640,\"start\":45043},{\"end\":47038,\"start\":45708},{\"end\":47636,\"start\":47050},{\"end\":48847,\"start\":47638},{\"end\":49762,\"start\":48869},{\"end\":50070,\"start\":49769},{\"end\":50313,\"start\":50077},{\"end\":50572,\"start\":50320}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12876,\"start\":12845},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13582,\"start\":13558},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13753,\"start\":13734},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14452,\"start\":14416},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14913,\"start\":14857},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15125,\"start\":15081},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15577,\"start\":15553},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16656,\"start\":16583},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16876,\"start\":16787},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17212,\"start\":17146},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18718,\"start\":18686},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18986,\"start\":18945},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19257,\"start\":19218},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19896,\"start\":19842},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20055,\"start\":19995},{\"attributes\":{\"id\":\"formula_15\"},\"end\":33414,\"start\":33357},{\"attributes\":{\"id\":\"formula_16\"},\"end\":33990,\"start\":33958},{\"attributes\":{\"id\":\"formula_17\"},\"end\":34376,\"start\":34337},{\"attributes\":{\"id\":\"formula_18\"},\"end\":34710,\"start\":34662},{\"attributes\":{\"id\":\"formula_19\"},\"end\":34753,\"start\":34710},{\"attributes\":{\"id\":\"formula_20\"},\"end\":34825,\"start\":34790},{\"attributes\":{\"id\":\"formula_21\"},\"end\":37164,\"start\":36923},{\"attributes\":{\"id\":\"formula_22\"},\"end\":37311,\"start\":37215}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23215,\"start\":23208},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23432,\"start\":23425},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25131,\"start\":25124},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25695,\"start\":25688},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26214,\"start\":26207},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":26721,\"start\":26714},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":27480,\"start\":27473},{\"end\":29268,\"start\":29261},{\"end\":29756,\"start\":29749},{\"end\":37365,\"start\":37358},{\"end\":37945,\"start\":37938},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":39376,\"start\":39369},{\"end\":40233,\"start\":40226},{\"end\":40762,\"start\":40755},{\"end\":41816,\"start\":41809},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":43087,\"start\":43080},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":43574,\"start\":43566},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":43899,\"start\":43891},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":44445,\"start\":44437},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":44777,\"start\":44769},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":47148,\"start\":47140},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":47813,\"start\":47805}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1960,\"start\":1948},{\"attributes\":{\"n\":\"2\"},\"end\":8020,\"start\":8008},{\"attributes\":{\"n\":\"3\"},\"end\":12036,\"start\":12030},{\"attributes\":{\"n\":\"3.1\"},\"end\":12643,\"start\":12613},{\"attributes\":{\"n\":\"3.2\"},\"end\":16077,\"start\":16040},{\"attributes\":{\"n\":\"3.3\"},\"end\":17252,\"start\":17217},{\"attributes\":{\"n\":\"3.4\"},\"end\":18493,\"start\":18481},{\"attributes\":{\"n\":\"4\"},\"end\":21151,\"start\":21140},{\"attributes\":{\"n\":\"4.1\"},\"end\":22836,\"start\":22822},{\"attributes\":{\"n\":\"4.2\"},\"end\":26309,\"start\":26266},{\"attributes\":{\"n\":\"4.3\"},\"end\":27250,\"start\":27205},{\"attributes\":{\"n\":\"4.4\"},\"end\":28709,\"start\":28665},{\"attributes\":{\"n\":\"5\"},\"end\":30264,\"start\":30254},{\"end\":31025,\"start\":31013},{\"end\":32098,\"start\":31985},{\"end\":32507,\"start\":32483},{\"end\":32906,\"start\":32889},{\"end\":34212,\"start\":34186},{\"end\":35123,\"start\":35101},{\"end\":36218,\"start\":36196},{\"end\":36912,\"start\":36895},{\"end\":37331,\"start\":37313},{\"end\":38106,\"start\":38096},{\"end\":38964,\"start\":38934},{\"end\":39940,\"start\":39887},{\"end\":40508,\"start\":40460},{\"end\":41524,\"start\":41473},{\"end\":42035,\"start\":42014},{\"end\":43314,\"start\":43294},{\"end\":43501,\"start\":43490},{\"end\":44713,\"start\":44692},{\"end\":45706,\"start\":45643},{\"end\":47048,\"start\":47041},{\"end\":48867,\"start\":48850},{\"end\":49767,\"start\":49765},{\"end\":50075,\"start\":50073},{\"end\":50318,\"start\":50316},{\"end\":50584,\"start\":50574},{\"end\":51135,\"start\":51125},{\"end\":51251,\"start\":51241},{\"end\":51441,\"start\":51408},{\"end\":52181,\"start\":52137},{\"end\":52826,\"start\":52793},{\"end\":53700,\"start\":53691},{\"end\":54669,\"start\":54660},{\"end\":54790,\"start\":54781},{\"end\":55646,\"start\":55637},{\"end\":55800,\"start\":55784},{\"end\":56778,\"start\":56769},{\"end\":59285,\"start\":59275},{\"end\":59854,\"start\":59844}]", "table": "[{\"end\":53500,\"start\":53402},{\"end\":53689,\"start\":53536},{\"end\":54658,\"start\":54158},{\"end\":55635,\"start\":55169},{\"end\":55782,\"start\":55665},{\"end\":56767,\"start\":55879},{\"end\":56858,\"start\":56828},{\"end\":57865,\"start\":56936},{\"end\":58372,\"start\":57914},{\"end\":59273,\"start\":58468}]", "figure_caption": "[{\"end\":51123,\"start\":50586},{\"end\":51239,\"start\":51137},{\"end\":51406,\"start\":51253},{\"end\":52135,\"start\":51448},{\"end\":52791,\"start\":52190},{\"end\":53288,\"start\":52833},{\"end\":53402,\"start\":53291},{\"end\":53536,\"start\":53503},{\"end\":53736,\"start\":53702},{\"end\":54158,\"start\":53739},{\"end\":54779,\"start\":54671},{\"end\":55169,\"start\":54792},{\"end\":55665,\"start\":55648},{\"end\":55879,\"start\":55806},{\"end\":56828,\"start\":56780},{\"end\":56936,\"start\":56861},{\"end\":57914,\"start\":57868},{\"end\":58468,\"start\":58375},{\"end\":59842,\"start\":59288},{\"end\":59903,\"start\":59857}]", "figure_ref": "[{\"end\":3124,\"start\":3118},{\"end\":3529,\"start\":3521},{\"end\":4488,\"start\":4482},{\"end\":5798,\"start\":5792},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12609,\"start\":12603},{\"end\":20753,\"start\":20745},{\"end\":23443,\"start\":23437},{\"end\":25146,\"start\":25140},{\"end\":27491,\"start\":27485},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33079,\"start\":33073},{\"end\":38520,\"start\":38512},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":39387,\"start\":39381},{\"end\":40303,\"start\":40297},{\"end\":40617,\"start\":40611},{\"end\":43613,\"start\":43607},{\"end\":44876,\"start\":44869},{\"end\":44888,\"start\":44881},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44897,\"start\":44890},{\"end\":44909,\"start\":44902},{\"end\":45128,\"start\":45124},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":46663,\"start\":46656},{\"end\":46958,\"start\":46951},{\"end\":47188,\"start\":47181},{\"end\":47508,\"start\":47501},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":47825,\"start\":47818},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":48191,\"start\":48184},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":48311,\"start\":48304},{\"end\":49844,\"start\":49836},{\"end\":50115,\"start\":50106},{\"end\":50358,\"start\":50349}]", "bib_author_first_name": "[{\"end\":60303,\"start\":60302},{\"end\":60313,\"start\":60312},{\"end\":60315,\"start\":60314},{\"end\":60325,\"start\":60324},{\"end\":60338,\"start\":60337},{\"end\":60346,\"start\":60345},{\"end\":60348,\"start\":60347},{\"end\":60679,\"start\":60678},{\"end\":60683,\"start\":60682},{\"end\":60685,\"start\":60684},{\"end\":61053,\"start\":61052},{\"end\":61063,\"start\":61062},{\"end\":61404,\"start\":61403},{\"end\":61416,\"start\":61415},{\"end\":61434,\"start\":61433},{\"end\":61436,\"start\":61435},{\"end\":61447,\"start\":61446},{\"end\":61458,\"start\":61457},{\"end\":61794,\"start\":61793},{\"end\":61796,\"start\":61795},{\"end\":61806,\"start\":61805},{\"end\":61820,\"start\":61819},{\"end\":61830,\"start\":61829},{\"end\":61832,\"start\":61831},{\"end\":61846,\"start\":61845},{\"end\":62265,\"start\":62264},{\"end\":62275,\"start\":62274},{\"end\":62286,\"start\":62285},{\"end\":62608,\"start\":62607},{\"end\":62610,\"start\":62609},{\"end\":62619,\"start\":62618},{\"end\":62953,\"start\":62952},{\"end\":62967,\"start\":62966},{\"end\":62969,\"start\":62968},{\"end\":62981,\"start\":62980},{\"end\":63339,\"start\":63338},{\"end\":63347,\"start\":63346},{\"end\":63757,\"start\":63756},{\"end\":63768,\"start\":63767},{\"end\":63780,\"start\":63779},{\"end\":64150,\"start\":64149},{\"end\":64152,\"start\":64151},{\"end\":64164,\"start\":64163},{\"end\":64166,\"start\":64165},{\"end\":64553,\"start\":64552},{\"end\":64564,\"start\":64563},{\"end\":64803,\"start\":64802},{\"end\":64810,\"start\":64809},{\"end\":64812,\"start\":64811},{\"end\":64821,\"start\":64820},{\"end\":64830,\"start\":64829},{\"end\":64840,\"start\":64839},{\"end\":64854,\"start\":64853},{\"end\":65285,\"start\":65284},{\"end\":65293,\"start\":65292},{\"end\":65303,\"start\":65299},{\"end\":65310,\"start\":65309},{\"end\":65713,\"start\":65712},{\"end\":65719,\"start\":65718},{\"end\":65728,\"start\":65727},{\"end\":65730,\"start\":65729},{\"end\":65745,\"start\":65744},{\"end\":65747,\"start\":65746},{\"end\":66179,\"start\":66178},{\"end\":66188,\"start\":66187},{\"end\":66597,\"start\":66596},{\"end\":66609,\"start\":66608},{\"end\":66616,\"start\":66615},{\"end\":66625,\"start\":66624},{\"end\":67105,\"start\":67104},{\"end\":67114,\"start\":67113},{\"end\":67530,\"start\":67529},{\"end\":67539,\"start\":67538},{\"end\":67550,\"start\":67549},{\"end\":67865,\"start\":67864},{\"end\":67874,\"start\":67873},{\"end\":67883,\"start\":67882},{\"end\":67891,\"start\":67890},{\"end\":67901,\"start\":67900},{\"end\":68276,\"start\":68275},{\"end\":68283,\"start\":68282},{\"end\":68291,\"start\":68290},{\"end\":68298,\"start\":68297},{\"end\":68306,\"start\":68305},{\"end\":68315,\"start\":68314},{\"end\":68322,\"start\":68321},{\"end\":68701,\"start\":68700},{\"end\":68712,\"start\":68711},{\"end\":68720,\"start\":68719},{\"end\":68872,\"start\":68871},{\"end\":68884,\"start\":68883},{\"end\":68896,\"start\":68895},{\"end\":68907,\"start\":68906},{\"end\":68919,\"start\":68918},{\"end\":69269,\"start\":69268},{\"end\":69278,\"start\":69277},{\"end\":69287,\"start\":69286},{\"end\":69507,\"start\":69506},{\"end\":69516,\"start\":69515},{\"end\":69518,\"start\":69517},{\"end\":69527,\"start\":69526},{\"end\":69696,\"start\":69695},{\"end\":69705,\"start\":69704},{\"end\":69707,\"start\":69706},{\"end\":69716,\"start\":69715},{\"end\":70043,\"start\":70042},{\"end\":70052,\"start\":70051},{\"end\":70054,\"start\":70053},{\"end\":70063,\"start\":70062},{\"end\":70348,\"start\":70347},{\"end\":70360,\"start\":70356},{\"end\":70369,\"start\":70368},{\"end\":70380,\"start\":70376},{\"end\":70761,\"start\":70760},{\"end\":70770,\"start\":70769},{\"end\":70780,\"start\":70779},{\"end\":70788,\"start\":70787},{\"end\":70797,\"start\":70796},{\"end\":71120,\"start\":71119},{\"end\":71128,\"start\":71127},{\"end\":71138,\"start\":71137},{\"end\":71547,\"start\":71546},{\"end\":71556,\"start\":71555},{\"end\":71562,\"start\":71561},{\"end\":71569,\"start\":71568},{\"end\":71976,\"start\":71975},{\"end\":71978,\"start\":71977},{\"end\":71989,\"start\":71988},{\"end\":72184,\"start\":72183},{\"end\":72186,\"start\":72185},{\"end\":72196,\"start\":72195},{\"end\":72523,\"start\":72522},{\"end\":72536,\"start\":72535},{\"end\":72547,\"start\":72543},{\"end\":72555,\"start\":72554},{\"end\":72793,\"start\":72792},{\"end\":72795,\"start\":72794},{\"end\":72808,\"start\":72807},{\"end\":72810,\"start\":72809},{\"end\":73086,\"start\":73085},{\"end\":73095,\"start\":73094},{\"end\":73105,\"start\":73104},{\"end\":73488,\"start\":73487},{\"end\":73494,\"start\":73493},{\"end\":73503,\"start\":73502},{\"end\":73510,\"start\":73509},{\"end\":73517,\"start\":73516},{\"end\":73905,\"start\":73904},{\"end\":73912,\"start\":73911},{\"end\":73918,\"start\":73917},{\"end\":73920,\"start\":73919},{\"end\":73927,\"start\":73926},{\"end\":73935,\"start\":73934},{\"end\":74257,\"start\":74256},{\"end\":74264,\"start\":74263},{\"end\":74273,\"start\":74272},{\"end\":74281,\"start\":74280},{\"end\":74288,\"start\":74287},{\"end\":74301,\"start\":74300},{\"end\":74717,\"start\":74716},{\"end\":74724,\"start\":74723},{\"end\":74735,\"start\":74734},{\"end\":75104,\"start\":75103},{\"end\":75106,\"start\":75105},{\"end\":75116,\"start\":75115},{\"end\":75118,\"start\":75117},{\"end\":75129,\"start\":75128},{\"end\":75131,\"start\":75130},{\"end\":75138,\"start\":75137},{\"end\":75140,\"start\":75139},{\"end\":75148,\"start\":75147},{\"end\":75160,\"start\":75159},{\"end\":75455,\"start\":75454},{\"end\":75468,\"start\":75467},{\"end\":75479,\"start\":75478},{\"end\":75491,\"start\":75490},{\"end\":75502,\"start\":75501},{\"end\":75974,\"start\":75973},{\"end\":75978,\"start\":75975},{\"end\":75991,\"start\":75990},{\"end\":76000,\"start\":75999},{\"end\":76007,\"start\":76006},{\"end\":76016,\"start\":76015},{\"end\":76293,\"start\":76292},{\"end\":76307,\"start\":76306},{\"end\":76309,\"start\":76308},{\"end\":76323,\"start\":76322},{\"end\":76333,\"start\":76332},{\"end\":76335,\"start\":76334},{\"end\":76345,\"start\":76344},{\"end\":76360,\"start\":76359},{\"end\":76789,\"start\":76788},{\"end\":76799,\"start\":76798},{\"end\":76808,\"start\":76807},{\"end\":76818,\"start\":76817},{\"end\":77108,\"start\":77107},{\"end\":77120,\"start\":77119},{\"end\":77122,\"start\":77121},{\"end\":77132,\"start\":77131},{\"end\":77146,\"start\":77145},{\"end\":77148,\"start\":77147},{\"end\":77159,\"start\":77158},{\"end\":77169,\"start\":77168},{\"end\":77630,\"start\":77629},{\"end\":77642,\"start\":77641},{\"end\":77655,\"start\":77654},{\"end\":77666,\"start\":77665},{\"end\":78092,\"start\":78091},{\"end\":78104,\"start\":78103},{\"end\":78117,\"start\":78116},{\"end\":78128,\"start\":78127},{\"end\":78596,\"start\":78595},{\"end\":78607,\"start\":78606},{\"end\":78615,\"start\":78614},{\"end\":79062,\"start\":79061},{\"end\":79064,\"start\":79063},{\"end\":79072,\"start\":79071},{\"end\":79084,\"start\":79083},{\"end\":79094,\"start\":79093},{\"end\":79096,\"start\":79095},{\"end\":79108,\"start\":79107},{\"end\":79555,\"start\":79554},{\"end\":79570,\"start\":79569},{\"end\":79572,\"start\":79571},{\"end\":79582,\"start\":79581},{\"end\":79593,\"start\":79592},{\"end\":79605,\"start\":79604},{\"end\":80030,\"start\":80029},{\"end\":80040,\"start\":80039},{\"end\":80049,\"start\":80048},{\"end\":80058,\"start\":80057},{\"end\":80067,\"start\":80066},{\"end\":80079,\"start\":80078},{\"end\":80089,\"start\":80088},{\"end\":80100,\"start\":80099},{\"end\":80107,\"start\":80106},{\"end\":80121,\"start\":80120},{\"end\":80131,\"start\":80130},{\"end\":80144,\"start\":80143},{\"end\":80152,\"start\":80151},{\"end\":80160,\"start\":80159},{\"end\":80170,\"start\":80169},{\"end\":80180,\"start\":80179},{\"end\":80190,\"start\":80189},{\"end\":80206,\"start\":80205},{\"end\":80217,\"start\":80216},{\"end\":80225,\"start\":80224},{\"end\":80232,\"start\":80231},{\"end\":80731,\"start\":80730},{\"end\":80739,\"start\":80738},{\"end\":80741,\"start\":80740},{\"end\":80750,\"start\":80749},{\"end\":80758,\"start\":80757},{\"end\":80770,\"start\":80769},{\"end\":80783,\"start\":80782},{\"end\":81094,\"start\":81093},{\"end\":81102,\"start\":81101},{\"end\":81114,\"start\":81113},{\"end\":81127,\"start\":81126},{\"end\":81140,\"start\":81139},{\"end\":81476,\"start\":81475},{\"end\":81486,\"start\":81485},{\"end\":81501,\"start\":81500},{\"end\":81731,\"start\":81730},{\"end\":81741,\"start\":81740},{\"end\":81753,\"start\":81752},{\"end\":81763,\"start\":81762},{\"end\":81776,\"start\":81775},{\"end\":82145,\"start\":82144},{\"end\":82156,\"start\":82155},{\"end\":82158,\"start\":82157},{\"end\":82168,\"start\":82167},{\"end\":82179,\"start\":82178},{\"end\":82518,\"start\":82517},{\"end\":82529,\"start\":82528},{\"end\":82531,\"start\":82530},{\"end\":82541,\"start\":82540},{\"end\":82555,\"start\":82554},{\"end\":82557,\"start\":82556},{\"end\":82571,\"start\":82570},{\"end\":82981,\"start\":82980},{\"end\":82991,\"start\":82990},{\"end\":82993,\"start\":82992},{\"end\":83002,\"start\":83001},{\"end\":83004,\"start\":83003},{\"end\":83273,\"start\":83272},{\"end\":83283,\"start\":83282},{\"end\":83285,\"start\":83284},{\"end\":83294,\"start\":83293},{\"end\":83296,\"start\":83295},{\"end\":83575,\"start\":83574},{\"end\":83585,\"start\":83584},{\"end\":83592,\"start\":83591},{\"end\":83594,\"start\":83593},{\"end\":83911,\"start\":83910},{\"end\":83913,\"start\":83912},{\"end\":83928,\"start\":83927},{\"end\":83937,\"start\":83936},{\"end\":83953,\"start\":83949},{\"end\":84307,\"start\":84306},{\"end\":84309,\"start\":84308},{\"end\":84327,\"start\":84323},{\"end\":84667,\"start\":84666},{\"end\":84676,\"start\":84675},{\"end\":85050,\"start\":85049},{\"end\":85052,\"start\":85051},{\"end\":85061,\"start\":85060},{\"end\":85072,\"start\":85071},{\"end\":85082,\"start\":85081},{\"end\":85096,\"start\":85095},{\"end\":85551,\"start\":85550},{\"end\":85563,\"start\":85562},{\"end\":85576,\"start\":85575},{\"end\":85855,\"start\":85854},{\"end\":85865,\"start\":85864},{\"end\":85875,\"start\":85874},{\"end\":85881,\"start\":85880},{\"end\":85889,\"start\":85888},{\"end\":85900,\"start\":85899},{\"end\":85909,\"start\":85908},{\"end\":85911,\"start\":85910},{\"end\":85920,\"start\":85919},{\"end\":85933,\"start\":85932},{\"end\":85940,\"start\":85939},{\"end\":85949,\"start\":85948},{\"end\":85961,\"start\":85960},{\"end\":85968,\"start\":85967},{\"end\":85977,\"start\":85976},{\"end\":85984,\"start\":85983},{\"end\":85991,\"start\":85990},{\"end\":85998,\"start\":85997},{\"end\":86005,\"start\":86004},{\"end\":86013,\"start\":86012},{\"end\":86023,\"start\":86022},{\"end\":86034,\"start\":86033},{\"end\":86048,\"start\":86047},{\"end\":86060,\"start\":86059},{\"end\":86073,\"start\":86072},{\"end\":86082,\"start\":86081},{\"end\":86091,\"start\":86090},{\"end\":86093,\"start\":86092},{\"end\":86105,\"start\":86104},{\"end\":86107,\"start\":86106},{\"end\":86401,\"start\":86400},{\"end\":86410,\"start\":86409},{\"end\":86421,\"start\":86420},{\"end\":86713,\"start\":86712},{\"end\":86722,\"start\":86721},{\"end\":86729,\"start\":86728},{\"end\":86738,\"start\":86737},{\"end\":87131,\"start\":87130},{\"end\":87143,\"start\":87142},{\"end\":87155,\"start\":87154},{\"end\":87162,\"start\":87161},{\"end\":87171,\"start\":87170},{\"end\":87179,\"start\":87178},{\"end\":87197,\"start\":87196},{\"end\":87209,\"start\":87208},{\"end\":87220,\"start\":87219},{\"end\":87726,\"start\":87725},{\"end\":87736,\"start\":87735},{\"end\":87750,\"start\":87749},{\"end\":87764,\"start\":87763},{\"end\":87782,\"start\":87781},{\"end\":87794,\"start\":87793},{\"end\":87805,\"start\":87804},{\"end\":87820,\"start\":87819},{\"end\":87830,\"start\":87829},{\"end\":88262,\"start\":88261},{\"end\":88274,\"start\":88273},{\"end\":88290,\"start\":88289},{\"end\":88300,\"start\":88299},{\"end\":88314,\"start\":88313},{\"end\":88655,\"start\":88654},{\"end\":88667,\"start\":88666},{\"end\":88675,\"start\":88674},{\"end\":88677,\"start\":88676},{\"end\":88686,\"start\":88685},{\"end\":89105,\"start\":89104},{\"end\":89107,\"start\":89106},{\"end\":89117,\"start\":89116},{\"end\":89127,\"start\":89126},{\"end\":89129,\"start\":89128},{\"end\":89476,\"start\":89475},{\"end\":89490,\"start\":89489},{\"end\":89498,\"start\":89497},{\"end\":89507,\"start\":89506},{\"end\":89516,\"start\":89515},{\"end\":89523,\"start\":89522},{\"end\":89538,\"start\":89537},{\"end\":89959,\"start\":89958},{\"end\":89967,\"start\":89966},{\"end\":89975,\"start\":89974},{\"end\":89983,\"start\":89982},{\"end\":89995,\"start\":89994},{\"end\":90005,\"start\":90004},{\"end\":90012,\"start\":90011},{\"end\":90293,\"start\":90292},{\"end\":90301,\"start\":90300},{\"end\":90308,\"start\":90307},{\"end\":90315,\"start\":90314},{\"end\":90327,\"start\":90326},{\"end\":90337,\"start\":90336},{\"end\":90729,\"start\":90728},{\"end\":90737,\"start\":90736},{\"end\":90751,\"start\":90750},{\"end\":90757,\"start\":90756},{\"end\":90767,\"start\":90766},{\"end\":91047,\"start\":91046},{\"end\":91054,\"start\":91053},{\"end\":91066,\"start\":91065},{\"end\":91075,\"start\":91074},{\"end\":91085,\"start\":91084},{\"end\":91092,\"start\":91091},{\"end\":91100,\"start\":91099},{\"end\":91111,\"start\":91110},{\"end\":91122,\"start\":91121},{\"end\":91134,\"start\":91133},{\"end\":91451,\"start\":91450},{\"end\":91458,\"start\":91457},{\"end\":91465,\"start\":91464},{\"end\":91471,\"start\":91470},{\"end\":91479,\"start\":91478},{\"end\":91829,\"start\":91828},{\"end\":91836,\"start\":91835},{\"end\":91843,\"start\":91842},{\"end\":91849,\"start\":91848},{\"end\":91857,\"start\":91856},{\"end\":91865,\"start\":91864},{\"end\":92251,\"start\":92250},{\"end\":92260,\"start\":92259},{\"end\":92266,\"start\":92265},{\"end\":92276,\"start\":92275},{\"end\":92641,\"start\":92640},{\"end\":92650,\"start\":92649},{\"end\":92660,\"start\":92659},{\"end\":92669,\"start\":92668},{\"end\":92678,\"start\":92677},{\"end\":92688,\"start\":92687},{\"end\":92697,\"start\":92696},{\"end\":93064,\"start\":93063},{\"end\":93071,\"start\":93070},{\"end\":93080,\"start\":93079},{\"end\":93088,\"start\":93087},{\"end\":93099,\"start\":93098},{\"end\":93106,\"start\":93105},{\"end\":93114,\"start\":93113},{\"end\":93468,\"start\":93467},{\"end\":93474,\"start\":93473},{\"end\":93480,\"start\":93479},{\"end\":93490,\"start\":93489},{\"end\":93911,\"start\":93910},{\"end\":93917,\"start\":93916},{\"end\":94159,\"start\":94158},{\"end\":94165,\"start\":94164},{\"end\":94172,\"start\":94171},{\"end\":94395,\"start\":94394},{\"end\":94401,\"start\":94400},{\"end\":94410,\"start\":94409},{\"end\":94418,\"start\":94417},{\"end\":94426,\"start\":94425},{\"end\":94667,\"start\":94666},{\"end\":94680,\"start\":94679},{\"end\":95051,\"start\":95050},{\"end\":95060,\"start\":95059},{\"end\":95067,\"start\":95066},{\"end\":95073,\"start\":95072},{\"end\":95080,\"start\":95079},{\"end\":95341,\"start\":95340},{\"end\":95350,\"start\":95349},{\"end\":95361,\"start\":95360},{\"end\":95372,\"start\":95371},{\"end\":95586,\"start\":95585},{\"end\":95595,\"start\":95594},{\"end\":95597,\"start\":95596},{\"end\":95611,\"start\":95610},{\"end\":95619,\"start\":95618},{\"end\":95630,\"start\":95629},{\"end\":95632,\"start\":95631},{\"end\":95643,\"start\":95642},{\"end\":95645,\"start\":95644},{\"end\":96040,\"start\":96039},{\"end\":96047,\"start\":96046},{\"end\":96055,\"start\":96054},{\"end\":96066,\"start\":96065},{\"end\":96072,\"start\":96071},{\"end\":96079,\"start\":96078},{\"end\":96086,\"start\":96085},{\"end\":96088,\"start\":96087},{\"end\":96098,\"start\":96097}]", "bib_author_last_name": "[{\"end\":60310,\"start\":60304},{\"end\":60322,\"start\":60316},{\"end\":60335,\"start\":60326},{\"end\":60343,\"start\":60339},{\"end\":60353,\"start\":60349},{\"end\":60691,\"start\":60686},{\"end\":61060,\"start\":61054},{\"end\":61070,\"start\":61064},{\"end\":61075,\"start\":61072},{\"end\":61413,\"start\":61405},{\"end\":61431,\"start\":61417},{\"end\":61444,\"start\":61437},{\"end\":61455,\"start\":61448},{\"end\":61464,\"start\":61459},{\"end\":61803,\"start\":61797},{\"end\":61817,\"start\":61807},{\"end\":61827,\"start\":61821},{\"end\":61843,\"start\":61833},{\"end\":61853,\"start\":61847},{\"end\":62272,\"start\":62266},{\"end\":62283,\"start\":62276},{\"end\":62293,\"start\":62287},{\"end\":62616,\"start\":62611},{\"end\":62625,\"start\":62620},{\"end\":62964,\"start\":62954},{\"end\":62978,\"start\":62970},{\"end\":62989,\"start\":62982},{\"end\":63344,\"start\":63340},{\"end\":63353,\"start\":63348},{\"end\":63765,\"start\":63758},{\"end\":63777,\"start\":63769},{\"end\":63790,\"start\":63781},{\"end\":64161,\"start\":64153},{\"end\":64173,\"start\":64167},{\"end\":64561,\"start\":64554},{\"end\":64570,\"start\":64565},{\"end\":64807,\"start\":64804},{\"end\":64818,\"start\":64813},{\"end\":64827,\"start\":64822},{\"end\":64837,\"start\":64831},{\"end\":64851,\"start\":64841},{\"end\":64863,\"start\":64855},{\"end\":65290,\"start\":65286},{\"end\":65297,\"start\":65294},{\"end\":65307,\"start\":65304},{\"end\":65318,\"start\":65311},{\"end\":65716,\"start\":65714},{\"end\":65725,\"start\":65720},{\"end\":65742,\"start\":65731},{\"end\":65752,\"start\":65748},{\"end\":66185,\"start\":66180},{\"end\":66195,\"start\":66189},{\"end\":66606,\"start\":66598},{\"end\":66613,\"start\":66610},{\"end\":66622,\"start\":66617},{\"end\":66631,\"start\":66626},{\"end\":67111,\"start\":67106},{\"end\":67121,\"start\":67115},{\"end\":67536,\"start\":67531},{\"end\":67547,\"start\":67540},{\"end\":67557,\"start\":67551},{\"end\":67871,\"start\":67866},{\"end\":67880,\"start\":67875},{\"end\":67888,\"start\":67884},{\"end\":67898,\"start\":67892},{\"end\":67908,\"start\":67902},{\"end\":68280,\"start\":68277},{\"end\":68288,\"start\":68284},{\"end\":68295,\"start\":68292},{\"end\":68303,\"start\":68299},{\"end\":68312,\"start\":68307},{\"end\":68319,\"start\":68316},{\"end\":68327,\"start\":68323},{\"end\":68709,\"start\":68702},{\"end\":68717,\"start\":68713},{\"end\":68728,\"start\":68721},{\"end\":68881,\"start\":68873},{\"end\":68893,\"start\":68885},{\"end\":68904,\"start\":68897},{\"end\":68916,\"start\":68908},{\"end\":68929,\"start\":68920},{\"end\":69275,\"start\":69270},{\"end\":69284,\"start\":69279},{\"end\":69294,\"start\":69288},{\"end\":69513,\"start\":69508},{\"end\":69524,\"start\":69519},{\"end\":69534,\"start\":69528},{\"end\":69702,\"start\":69697},{\"end\":69713,\"start\":69708},{\"end\":69723,\"start\":69717},{\"end\":70049,\"start\":70044},{\"end\":70060,\"start\":70055},{\"end\":70070,\"start\":70064},{\"end\":70354,\"start\":70349},{\"end\":70366,\"start\":70361},{\"end\":70374,\"start\":70370},{\"end\":70383,\"start\":70381},{\"end\":70767,\"start\":70762},{\"end\":70777,\"start\":70771},{\"end\":70785,\"start\":70781},{\"end\":70794,\"start\":70789},{\"end\":70803,\"start\":70798},{\"end\":71125,\"start\":71121},{\"end\":71135,\"start\":71129},{\"end\":71145,\"start\":71139},{\"end\":71553,\"start\":71548},{\"end\":71559,\"start\":71557},{\"end\":71566,\"start\":71563},{\"end\":71577,\"start\":71570},{\"end\":71986,\"start\":71979},{\"end\":71995,\"start\":71990},{\"end\":72193,\"start\":72187},{\"end\":72199,\"start\":72197},{\"end\":72533,\"start\":72524},{\"end\":72541,\"start\":72537},{\"end\":72552,\"start\":72548},{\"end\":72562,\"start\":72556},{\"end\":72805,\"start\":72796},{\"end\":72816,\"start\":72811},{\"end\":73092,\"start\":73087},{\"end\":73102,\"start\":73096},{\"end\":73111,\"start\":73106},{\"end\":73491,\"start\":73489},{\"end\":73500,\"start\":73495},{\"end\":73507,\"start\":73504},{\"end\":73514,\"start\":73511},{\"end\":73520,\"start\":73518},{\"end\":73909,\"start\":73906},{\"end\":73915,\"start\":73913},{\"end\":73924,\"start\":73921},{\"end\":73932,\"start\":73928},{\"end\":73944,\"start\":73936},{\"end\":74261,\"start\":74258},{\"end\":74270,\"start\":74265},{\"end\":74278,\"start\":74274},{\"end\":74285,\"start\":74282},{\"end\":74298,\"start\":74289},{\"end\":74305,\"start\":74302},{\"end\":74721,\"start\":74718},{\"end\":74732,\"start\":74725},{\"end\":74743,\"start\":74736},{\"end\":75113,\"start\":75107},{\"end\":75126,\"start\":75119},{\"end\":75135,\"start\":75132},{\"end\":75145,\"start\":75141},{\"end\":75157,\"start\":75149},{\"end\":75170,\"start\":75161},{\"end\":75465,\"start\":75456},{\"end\":75476,\"start\":75469},{\"end\":75488,\"start\":75480},{\"end\":75499,\"start\":75492},{\"end\":75509,\"start\":75503},{\"end\":75988,\"start\":75979},{\"end\":75997,\"start\":75992},{\"end\":76004,\"start\":76001},{\"end\":76013,\"start\":76008},{\"end\":76022,\"start\":76017},{\"end\":76304,\"start\":76294},{\"end\":76320,\"start\":76310},{\"end\":76330,\"start\":76324},{\"end\":76342,\"start\":76336},{\"end\":76357,\"start\":76346},{\"end\":76363,\"start\":76361},{\"end\":76796,\"start\":76790},{\"end\":76805,\"start\":76800},{\"end\":76815,\"start\":76809},{\"end\":76825,\"start\":76819},{\"end\":77117,\"start\":77109},{\"end\":77129,\"start\":77123},{\"end\":77143,\"start\":77133},{\"end\":77156,\"start\":77149},{\"end\":77166,\"start\":77160},{\"end\":77176,\"start\":77170},{\"end\":77639,\"start\":77631},{\"end\":77652,\"start\":77643},{\"end\":77663,\"start\":77656},{\"end\":77673,\"start\":77667},{\"end\":78101,\"start\":78093},{\"end\":78114,\"start\":78105},{\"end\":78125,\"start\":78118},{\"end\":78135,\"start\":78129},{\"end\":78604,\"start\":78597},{\"end\":78612,\"start\":78608},{\"end\":78622,\"start\":78616},{\"end\":79069,\"start\":79065},{\"end\":79081,\"start\":79073},{\"end\":79091,\"start\":79085},{\"end\":79105,\"start\":79097},{\"end\":79118,\"start\":79109},{\"end\":79567,\"start\":79556},{\"end\":79579,\"start\":79573},{\"end\":79590,\"start\":79583},{\"end\":79602,\"start\":79594},{\"end\":79612,\"start\":79606},{\"end\":80037,\"start\":80031},{\"end\":80046,\"start\":80041},{\"end\":80055,\"start\":80050},{\"end\":80064,\"start\":80059},{\"end\":80076,\"start\":80068},{\"end\":80086,\"start\":80080},{\"end\":80097,\"start\":80090},{\"end\":80104,\"start\":80101},{\"end\":80118,\"start\":80108},{\"end\":80128,\"start\":80122},{\"end\":80141,\"start\":80132},{\"end\":80149,\"start\":80145},{\"end\":80157,\"start\":80153},{\"end\":80167,\"start\":80161},{\"end\":80177,\"start\":80171},{\"end\":80187,\"start\":80181},{\"end\":80203,\"start\":80191},{\"end\":80214,\"start\":80207},{\"end\":80222,\"start\":80218},{\"end\":80229,\"start\":80226},{\"end\":80241,\"start\":80233},{\"end\":80736,\"start\":80732},{\"end\":80747,\"start\":80742},{\"end\":80755,\"start\":80751},{\"end\":80767,\"start\":80759},{\"end\":80780,\"start\":80771},{\"end\":80790,\"start\":80784},{\"end\":81099,\"start\":81095},{\"end\":81111,\"start\":81103},{\"end\":81124,\"start\":81115},{\"end\":81137,\"start\":81128},{\"end\":81147,\"start\":81141},{\"end\":81483,\"start\":81477},{\"end\":81498,\"start\":81487},{\"end\":81508,\"start\":81502},{\"end\":81738,\"start\":81732},{\"end\":81750,\"start\":81742},{\"end\":81760,\"start\":81754},{\"end\":81773,\"start\":81764},{\"end\":81783,\"start\":81777},{\"end\":82153,\"start\":82146},{\"end\":82165,\"start\":82159},{\"end\":82176,\"start\":82169},{\"end\":82186,\"start\":82180},{\"end\":82526,\"start\":82519},{\"end\":82538,\"start\":82532},{\"end\":82552,\"start\":82542},{\"end\":82568,\"start\":82558},{\"end\":82579,\"start\":82572},{\"end\":82988,\"start\":82982},{\"end\":82999,\"start\":82994},{\"end\":83007,\"start\":83005},{\"end\":83280,\"start\":83274},{\"end\":83291,\"start\":83286},{\"end\":83299,\"start\":83297},{\"end\":83582,\"start\":83576},{\"end\":83589,\"start\":83586},{\"end\":83597,\"start\":83595},{\"end\":83925,\"start\":83914},{\"end\":83934,\"start\":83929},{\"end\":83947,\"start\":83938},{\"end\":83959,\"start\":83954},{\"end\":84321,\"start\":84310},{\"end\":84333,\"start\":84328},{\"end\":84673,\"start\":84668},{\"end\":84681,\"start\":84677},{\"end\":85058,\"start\":85053},{\"end\":85069,\"start\":85062},{\"end\":85079,\"start\":85073},{\"end\":85093,\"start\":85083},{\"end\":85105,\"start\":85097},{\"end\":85560,\"start\":85552},{\"end\":85573,\"start\":85564},{\"end\":85586,\"start\":85577},{\"end\":85862,\"start\":85856},{\"end\":85872,\"start\":85866},{\"end\":85878,\"start\":85876},{\"end\":85886,\"start\":85882},{\"end\":85897,\"start\":85890},{\"end\":85906,\"start\":85901},{\"end\":85917,\"start\":85912},{\"end\":85930,\"start\":85921},{\"end\":85937,\"start\":85934},{\"end\":85946,\"start\":85941},{\"end\":85958,\"start\":85950},{\"end\":85965,\"start\":85962},{\"end\":85974,\"start\":85969},{\"end\":85981,\"start\":85978},{\"end\":85988,\"start\":85985},{\"end\":85995,\"start\":85992},{\"end\":86002,\"start\":85999},{\"end\":86010,\"start\":86006},{\"end\":86020,\"start\":86014},{\"end\":86031,\"start\":86024},{\"end\":86045,\"start\":86035},{\"end\":86057,\"start\":86049},{\"end\":86070,\"start\":86061},{\"end\":86079,\"start\":86074},{\"end\":86088,\"start\":86083},{\"end\":86102,\"start\":86094},{\"end\":86407,\"start\":86402},{\"end\":86418,\"start\":86411},{\"end\":86431,\"start\":86422},{\"end\":86441,\"start\":86433},{\"end\":86719,\"start\":86714},{\"end\":86726,\"start\":86723},{\"end\":86735,\"start\":86730},{\"end\":86746,\"start\":86739},{\"end\":87140,\"start\":87132},{\"end\":87152,\"start\":87144},{\"end\":87159,\"start\":87156},{\"end\":87168,\"start\":87163},{\"end\":87176,\"start\":87172},{\"end\":87194,\"start\":87180},{\"end\":87206,\"start\":87198},{\"end\":87217,\"start\":87210},{\"end\":87227,\"start\":87221},{\"end\":87733,\"start\":87727},{\"end\":87747,\"start\":87737},{\"end\":87761,\"start\":87751},{\"end\":87779,\"start\":87765},{\"end\":87791,\"start\":87783},{\"end\":87802,\"start\":87795},{\"end\":87817,\"start\":87806},{\"end\":87827,\"start\":87821},{\"end\":87833,\"start\":87831},{\"end\":88271,\"start\":88263},{\"end\":88287,\"start\":88275},{\"end\":88297,\"start\":88291},{\"end\":88311,\"start\":88301},{\"end\":88320,\"start\":88315},{\"end\":88664,\"start\":88656},{\"end\":88672,\"start\":88668},{\"end\":88683,\"start\":88678},{\"end\":88692,\"start\":88687},{\"end\":89114,\"start\":89108},{\"end\":89124,\"start\":89118},{\"end\":89135,\"start\":89130},{\"end\":89487,\"start\":89477},{\"end\":89495,\"start\":89491},{\"end\":89504,\"start\":89499},{\"end\":89513,\"start\":89508},{\"end\":89520,\"start\":89517},{\"end\":89535,\"start\":89524},{\"end\":89543,\"start\":89539},{\"end\":89964,\"start\":89960},{\"end\":89972,\"start\":89968},{\"end\":89980,\"start\":89976},{\"end\":89992,\"start\":89984},{\"end\":90002,\"start\":89996},{\"end\":90009,\"start\":90006},{\"end\":90017,\"start\":90013},{\"end\":90298,\"start\":90294},{\"end\":90305,\"start\":90302},{\"end\":90312,\"start\":90309},{\"end\":90324,\"start\":90316},{\"end\":90334,\"start\":90328},{\"end\":90342,\"start\":90338},{\"end\":90734,\"start\":90730},{\"end\":90748,\"start\":90738},{\"end\":90754,\"start\":90752},{\"end\":90764,\"start\":90758},{\"end\":90772,\"start\":90768},{\"end\":91051,\"start\":91048},{\"end\":91063,\"start\":91055},{\"end\":91072,\"start\":91067},{\"end\":91082,\"start\":91076},{\"end\":91089,\"start\":91086},{\"end\":91097,\"start\":91093},{\"end\":91108,\"start\":91101},{\"end\":91119,\"start\":91112},{\"end\":91131,\"start\":91123},{\"end\":91142,\"start\":91135},{\"end\":91455,\"start\":91452},{\"end\":91462,\"start\":91459},{\"end\":91468,\"start\":91466},{\"end\":91476,\"start\":91472},{\"end\":91484,\"start\":91480},{\"end\":91833,\"start\":91830},{\"end\":91840,\"start\":91837},{\"end\":91846,\"start\":91844},{\"end\":91854,\"start\":91850},{\"end\":91862,\"start\":91858},{\"end\":91870,\"start\":91866},{\"end\":92257,\"start\":92252},{\"end\":92263,\"start\":92261},{\"end\":92273,\"start\":92267},{\"end\":92283,\"start\":92277},{\"end\":92647,\"start\":92642},{\"end\":92657,\"start\":92651},{\"end\":92666,\"start\":92661},{\"end\":92675,\"start\":92670},{\"end\":92685,\"start\":92679},{\"end\":92694,\"start\":92689},{\"end\":92704,\"start\":92698},{\"end\":93068,\"start\":93065},{\"end\":93077,\"start\":93072},{\"end\":93085,\"start\":93081},{\"end\":93096,\"start\":93089},{\"end\":93103,\"start\":93100},{\"end\":93111,\"start\":93107},{\"end\":93119,\"start\":93115},{\"end\":93471,\"start\":93469},{\"end\":93477,\"start\":93475},{\"end\":93487,\"start\":93481},{\"end\":93499,\"start\":93491},{\"end\":93914,\"start\":93912},{\"end\":93921,\"start\":93918},{\"end\":94162,\"start\":94160},{\"end\":94169,\"start\":94166},{\"end\":94176,\"start\":94173},{\"end\":94398,\"start\":94396},{\"end\":94407,\"start\":94402},{\"end\":94415,\"start\":94411},{\"end\":94423,\"start\":94419},{\"end\":94430,\"start\":94427},{\"end\":94677,\"start\":94668},{\"end\":94690,\"start\":94681},{\"end\":95057,\"start\":95052},{\"end\":95064,\"start\":95061},{\"end\":95070,\"start\":95068},{\"end\":95077,\"start\":95074},{\"end\":95085,\"start\":95081},{\"end\":95347,\"start\":95342},{\"end\":95358,\"start\":95351},{\"end\":95369,\"start\":95362},{\"end\":95379,\"start\":95373},{\"end\":95592,\"start\":95587},{\"end\":95608,\"start\":95598},{\"end\":95616,\"start\":95612},{\"end\":95627,\"start\":95620},{\"end\":95640,\"start\":95633},{\"end\":95652,\"start\":95646},{\"end\":96044,\"start\":96041},{\"end\":96052,\"start\":96048},{\"end\":96063,\"start\":96056},{\"end\":96069,\"start\":96067},{\"end\":96076,\"start\":96073},{\"end\":96083,\"start\":96080},{\"end\":96095,\"start\":96089},{\"end\":96108,\"start\":96099}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":60601,\"start\":60256},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7375482},\"end\":61002,\"start\":60603},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":208267630},\"end\":61364,\"start\":61004},{\"attributes\":{\"doi\":\"2022. 10\",\"id\":\"b3\",\"matched_paper_id\":233210013},\"end\":61791,\"start\":61366},{\"attributes\":{\"id\":\"b4\"},\"end\":62197,\"start\":61793},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1798946},\"end\":62550,\"start\":62199},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":73614632},\"end\":62905,\"start\":62552},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":12969761},\"end\":63280,\"start\":62907},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":54457478},\"end\":63674,\"start\":63282},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211817828},\"end\":64069,\"start\":63676},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14658103},\"end\":64483,\"start\":64071},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":12358833},\"end\":64738,\"start\":64485},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7684883},\"end\":65217,\"start\":64740},{\"attributes\":{\"doi\":\"2022. 23\",\"id\":\"b13\",\"matched_paper_id\":235743051},\"end\":65644,\"start\":65219},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":220647325},\"end\":66107,\"start\":65646},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":106388263},\"end\":66501,\"start\":66109},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":238583434},\"end\":66994,\"start\":66503},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":102496818},\"end\":67452,\"start\":66996},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2255738},\"end\":67807,\"start\":67454},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":211259068},\"end\":68205,\"start\":67809},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":248524713},\"end\":68698,\"start\":68207},{\"attributes\":{\"id\":\"b21\"},\"end\":68837,\"start\":68700},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b22\",\"matched_paper_id\":9353335},\"end\":69234,\"start\":68839},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6152006},\"end\":69480,\"start\":69236},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":834882},\"end\":69654,\"start\":69482},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206769405},\"end\":69999,\"start\":69656},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":37206},\"end\":70282,\"start\":70001},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":228083546},\"end\":70717,\"start\":70284},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4559916},\"end\":71044,\"start\":70719},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":232478424},\"end\":71457,\"start\":71046},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":209376681},\"end\":71932,\"start\":71459},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1371704},\"end\":72137,\"start\":71934},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6628106},\"end\":72454,\"start\":72139},{\"attributes\":{\"id\":\"b33\"},\"end\":72754,\"start\":72456},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1538200},\"end\":73010,\"start\":72756},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52958567},\"end\":73388,\"start\":73012},{\"attributes\":{\"doi\":\"2021. 21\",\"id\":\"b36\",\"matched_paper_id\":237213280},\"end\":73874,\"start\":73390},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":220686483},\"end\":74163,\"start\":73876},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":208512845},\"end\":74669,\"start\":74165},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":10845625},\"end\":75032,\"start\":74671},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":233864500},\"end\":75386,\"start\":75034},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":54465161},\"end\":75862,\"start\":75388},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":235248354},\"end\":76218,\"start\":75864},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":213175590},\"end\":76713,\"start\":76220},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":246016186},\"end\":77021,\"start\":76715},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":244773517},\"end\":77562,\"start\":77023},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":203643676},\"end\":77991,\"start\":77564},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":209376368},\"end\":78499,\"start\":77993},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":233307004},\"end\":78978,\"start\":78501},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":58007025},\"end\":79485,\"start\":78980},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":43992332},\"end\":79957,\"start\":79487},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":202786778},\"end\":80678,\"start\":79959},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":235358422},\"end\":81057,\"start\":80680},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":212646575},\"end\":81431,\"start\":81059},{\"attributes\":{\"id\":\"b54\"},\"end\":81631,\"start\":81433},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":195776274},\"end\":82095,\"start\":81633},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b56\",\"matched_paper_id\":17104434},\"end\":82444,\"start\":82097},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":244921004},\"end\":82933,\"start\":82446},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":10748875},\"end\":83218,\"start\":82935},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":9620866},\"end\":83509,\"start\":83220},{\"attributes\":{\"id\":\"b60\"},\"end\":83847,\"start\":83511},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":977535},\"end\":84271,\"start\":83849},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":1728538},\"end\":84609,\"start\":84273},{\"attributes\":{\"doi\":\"1997. 3\",\"id\":\"b63\",\"matched_paper_id\":687379},\"end\":84971,\"start\":84611},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":206590743},\"end\":85457,\"start\":84973},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":174798113},\"end\":85850,\"start\":85459},{\"attributes\":{\"id\":\"b66\"},\"end\":86398,\"start\":85852},{\"attributes\":{\"id\":\"b67\"},\"end\":86657,\"start\":86400},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":232320654},\"end\":87049,\"start\":86659},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":231709798},\"end\":87634,\"start\":87051},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":219791950},\"end\":88186,\"start\":87636},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":12035329},\"end\":88562,\"start\":88188},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b72\",\"matched_paper_id\":73431591},\"end\":89032,\"start\":88564},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":11904335},\"end\":89410,\"start\":89034},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b74\",\"matched_paper_id\":6159584},\"end\":89888,\"start\":89412},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":250088904},\"end\":90199,\"start\":89890},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":235490453},\"end\":90650,\"start\":90201},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":235593022},\"end\":91044,\"start\":90652},{\"attributes\":{\"id\":\"b78\"},\"end\":91388,\"start\":91046},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":4712004},\"end\":91755,\"start\":91390},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":67855970},\"end\":92202,\"start\":91757},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":235605960},\"end\":92556,\"start\":92204},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":225077557},\"end\":93005,\"start\":92558},{\"attributes\":{\"doi\":\"2021. 21\",\"id\":\"b83\",\"matched_paper_id\":229298063},\"end\":93407,\"start\":93007},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":227254854},\"end\":93807,\"start\":93409},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":214713824},\"end\":94068,\"start\":93809},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":220525434},\"end\":94316,\"start\":94070},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":67855869},\"end\":94595,\"start\":94318},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":215827033},\"end\":95004,\"start\":94597},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":221150925},\"end\":95338,\"start\":95006},{\"attributes\":{\"doi\":\"arXiv:2010.07492\",\"id\":\"b90\"},\"end\":95583,\"start\":95340},{\"attributes\":{\"doi\":\"arXiv:2106.01970\",\"id\":\"b91\"},\"end\":95982,\"start\":95585},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":245385791},\"end\":96531,\"start\":95984}]", "bib_title": "[{\"end\":60300,\"start\":60256},{\"end\":60676,\"start\":60603},{\"end\":61050,\"start\":61004},{\"end\":61401,\"start\":61366},{\"end\":62262,\"start\":62199},{\"end\":62605,\"start\":62552},{\"end\":62950,\"start\":62907},{\"end\":63336,\"start\":63282},{\"end\":63754,\"start\":63676},{\"end\":64147,\"start\":64071},{\"end\":64550,\"start\":64485},{\"end\":64800,\"start\":64740},{\"end\":65282,\"start\":65219},{\"end\":65710,\"start\":65646},{\"end\":66176,\"start\":66109},{\"end\":66594,\"start\":66503},{\"end\":67102,\"start\":66996},{\"end\":67527,\"start\":67454},{\"end\":67862,\"start\":67809},{\"end\":68273,\"start\":68207},{\"end\":68869,\"start\":68839},{\"end\":69266,\"start\":69236},{\"end\":69504,\"start\":69482},{\"end\":69693,\"start\":69656},{\"end\":70040,\"start\":70001},{\"end\":70345,\"start\":70284},{\"end\":70758,\"start\":70719},{\"end\":71117,\"start\":71046},{\"end\":71544,\"start\":71459},{\"end\":71973,\"start\":71934},{\"end\":72181,\"start\":72139},{\"end\":72520,\"start\":72456},{\"end\":72790,\"start\":72756},{\"end\":73083,\"start\":73012},{\"end\":73485,\"start\":73390},{\"end\":73902,\"start\":73876},{\"end\":74254,\"start\":74165},{\"end\":74714,\"start\":74671},{\"end\":75101,\"start\":75034},{\"end\":75452,\"start\":75388},{\"end\":75971,\"start\":75864},{\"end\":76290,\"start\":76220},{\"end\":76786,\"start\":76715},{\"end\":77105,\"start\":77023},{\"end\":77627,\"start\":77564},{\"end\":78089,\"start\":77993},{\"end\":78593,\"start\":78501},{\"end\":79059,\"start\":78980},{\"end\":79552,\"start\":79487},{\"end\":80027,\"start\":79959},{\"end\":80728,\"start\":80680},{\"end\":81091,\"start\":81059},{\"end\":81728,\"start\":81633},{\"end\":82142,\"start\":82097},{\"end\":82515,\"start\":82446},{\"end\":82978,\"start\":82935},{\"end\":83270,\"start\":83220},{\"end\":83572,\"start\":83511},{\"end\":83908,\"start\":83849},{\"end\":84304,\"start\":84273},{\"end\":84664,\"start\":84611},{\"end\":85047,\"start\":84973},{\"end\":85548,\"start\":85459},{\"end\":86710,\"start\":86659},{\"end\":87128,\"start\":87051},{\"end\":87723,\"start\":87636},{\"end\":88259,\"start\":88188},{\"end\":88652,\"start\":88564},{\"end\":89102,\"start\":89034},{\"end\":89473,\"start\":89412},{\"end\":89956,\"start\":89890},{\"end\":90290,\"start\":90201},{\"end\":90726,\"start\":90652},{\"end\":91448,\"start\":91390},{\"end\":91826,\"start\":91757},{\"end\":92248,\"start\":92204},{\"end\":92638,\"start\":92558},{\"end\":93061,\"start\":93007},{\"end\":93465,\"start\":93409},{\"end\":93908,\"start\":93809},{\"end\":94156,\"start\":94070},{\"end\":94392,\"start\":94318},{\"end\":94664,\"start\":94597},{\"end\":95048,\"start\":95006},{\"end\":96037,\"start\":95984}]", "bib_author": "[{\"end\":60312,\"start\":60302},{\"end\":60324,\"start\":60312},{\"end\":60337,\"start\":60324},{\"end\":60345,\"start\":60337},{\"end\":60355,\"start\":60345},{\"end\":60682,\"start\":60678},{\"end\":60693,\"start\":60682},{\"end\":61062,\"start\":61052},{\"end\":61072,\"start\":61062},{\"end\":61077,\"start\":61072},{\"end\":61415,\"start\":61403},{\"end\":61433,\"start\":61415},{\"end\":61446,\"start\":61433},{\"end\":61457,\"start\":61446},{\"end\":61466,\"start\":61457},{\"end\":61805,\"start\":61793},{\"end\":61819,\"start\":61805},{\"end\":61829,\"start\":61819},{\"end\":61845,\"start\":61829},{\"end\":61855,\"start\":61845},{\"end\":62274,\"start\":62264},{\"end\":62285,\"start\":62274},{\"end\":62295,\"start\":62285},{\"end\":62618,\"start\":62607},{\"end\":62627,\"start\":62618},{\"end\":62966,\"start\":62952},{\"end\":62980,\"start\":62966},{\"end\":62991,\"start\":62980},{\"end\":63346,\"start\":63338},{\"end\":63355,\"start\":63346},{\"end\":63767,\"start\":63756},{\"end\":63779,\"start\":63767},{\"end\":63792,\"start\":63779},{\"end\":64163,\"start\":64149},{\"end\":64175,\"start\":64163},{\"end\":64563,\"start\":64552},{\"end\":64572,\"start\":64563},{\"end\":64809,\"start\":64802},{\"end\":64820,\"start\":64809},{\"end\":64829,\"start\":64820},{\"end\":64839,\"start\":64829},{\"end\":64853,\"start\":64839},{\"end\":64865,\"start\":64853},{\"end\":65292,\"start\":65284},{\"end\":65299,\"start\":65292},{\"end\":65309,\"start\":65299},{\"end\":65320,\"start\":65309},{\"end\":65718,\"start\":65712},{\"end\":65727,\"start\":65718},{\"end\":65744,\"start\":65727},{\"end\":65754,\"start\":65744},{\"end\":66187,\"start\":66178},{\"end\":66197,\"start\":66187},{\"end\":66608,\"start\":66596},{\"end\":66615,\"start\":66608},{\"end\":66624,\"start\":66615},{\"end\":66633,\"start\":66624},{\"end\":67113,\"start\":67104},{\"end\":67123,\"start\":67113},{\"end\":67538,\"start\":67529},{\"end\":67549,\"start\":67538},{\"end\":67559,\"start\":67549},{\"end\":67873,\"start\":67864},{\"end\":67882,\"start\":67873},{\"end\":67890,\"start\":67882},{\"end\":67900,\"start\":67890},{\"end\":67910,\"start\":67900},{\"end\":68282,\"start\":68275},{\"end\":68290,\"start\":68282},{\"end\":68297,\"start\":68290},{\"end\":68305,\"start\":68297},{\"end\":68314,\"start\":68305},{\"end\":68321,\"start\":68314},{\"end\":68329,\"start\":68321},{\"end\":68711,\"start\":68700},{\"end\":68719,\"start\":68711},{\"end\":68730,\"start\":68719},{\"end\":68883,\"start\":68871},{\"end\":68895,\"start\":68883},{\"end\":68906,\"start\":68895},{\"end\":68918,\"start\":68906},{\"end\":68931,\"start\":68918},{\"end\":69277,\"start\":69268},{\"end\":69286,\"start\":69277},{\"end\":69296,\"start\":69286},{\"end\":69515,\"start\":69506},{\"end\":69526,\"start\":69515},{\"end\":69536,\"start\":69526},{\"end\":69704,\"start\":69695},{\"end\":69715,\"start\":69704},{\"end\":69725,\"start\":69715},{\"end\":70051,\"start\":70042},{\"end\":70062,\"start\":70051},{\"end\":70072,\"start\":70062},{\"end\":70356,\"start\":70347},{\"end\":70368,\"start\":70356},{\"end\":70376,\"start\":70368},{\"end\":70385,\"start\":70376},{\"end\":70769,\"start\":70760},{\"end\":70779,\"start\":70769},{\"end\":70787,\"start\":70779},{\"end\":70796,\"start\":70787},{\"end\":70805,\"start\":70796},{\"end\":71127,\"start\":71119},{\"end\":71137,\"start\":71127},{\"end\":71147,\"start\":71137},{\"end\":71555,\"start\":71546},{\"end\":71561,\"start\":71555},{\"end\":71568,\"start\":71561},{\"end\":71579,\"start\":71568},{\"end\":71988,\"start\":71975},{\"end\":71997,\"start\":71988},{\"end\":72195,\"start\":72183},{\"end\":72201,\"start\":72195},{\"end\":72535,\"start\":72522},{\"end\":72543,\"start\":72535},{\"end\":72554,\"start\":72543},{\"end\":72564,\"start\":72554},{\"end\":72807,\"start\":72792},{\"end\":72818,\"start\":72807},{\"end\":73094,\"start\":73085},{\"end\":73104,\"start\":73094},{\"end\":73113,\"start\":73104},{\"end\":73493,\"start\":73487},{\"end\":73502,\"start\":73493},{\"end\":73509,\"start\":73502},{\"end\":73516,\"start\":73509},{\"end\":73522,\"start\":73516},{\"end\":73911,\"start\":73904},{\"end\":73917,\"start\":73911},{\"end\":73926,\"start\":73917},{\"end\":73934,\"start\":73926},{\"end\":73946,\"start\":73934},{\"end\":74263,\"start\":74256},{\"end\":74272,\"start\":74263},{\"end\":74280,\"start\":74272},{\"end\":74287,\"start\":74280},{\"end\":74300,\"start\":74287},{\"end\":74307,\"start\":74300},{\"end\":74723,\"start\":74716},{\"end\":74734,\"start\":74723},{\"end\":74745,\"start\":74734},{\"end\":75115,\"start\":75103},{\"end\":75128,\"start\":75115},{\"end\":75137,\"start\":75128},{\"end\":75147,\"start\":75137},{\"end\":75159,\"start\":75147},{\"end\":75172,\"start\":75159},{\"end\":75467,\"start\":75454},{\"end\":75478,\"start\":75467},{\"end\":75490,\"start\":75478},{\"end\":75501,\"start\":75490},{\"end\":75511,\"start\":75501},{\"end\":75990,\"start\":75973},{\"end\":75999,\"start\":75990},{\"end\":76006,\"start\":75999},{\"end\":76015,\"start\":76006},{\"end\":76024,\"start\":76015},{\"end\":76306,\"start\":76292},{\"end\":76322,\"start\":76306},{\"end\":76332,\"start\":76322},{\"end\":76344,\"start\":76332},{\"end\":76359,\"start\":76344},{\"end\":76365,\"start\":76359},{\"end\":76798,\"start\":76788},{\"end\":76807,\"start\":76798},{\"end\":76817,\"start\":76807},{\"end\":76827,\"start\":76817},{\"end\":77119,\"start\":77107},{\"end\":77131,\"start\":77119},{\"end\":77145,\"start\":77131},{\"end\":77158,\"start\":77145},{\"end\":77168,\"start\":77158},{\"end\":77178,\"start\":77168},{\"end\":77641,\"start\":77629},{\"end\":77654,\"start\":77641},{\"end\":77665,\"start\":77654},{\"end\":77675,\"start\":77665},{\"end\":78103,\"start\":78091},{\"end\":78116,\"start\":78103},{\"end\":78127,\"start\":78116},{\"end\":78137,\"start\":78127},{\"end\":78606,\"start\":78595},{\"end\":78614,\"start\":78606},{\"end\":78624,\"start\":78614},{\"end\":79071,\"start\":79061},{\"end\":79083,\"start\":79071},{\"end\":79093,\"start\":79083},{\"end\":79107,\"start\":79093},{\"end\":79120,\"start\":79107},{\"end\":79569,\"start\":79554},{\"end\":79581,\"start\":79569},{\"end\":79592,\"start\":79581},{\"end\":79604,\"start\":79592},{\"end\":79614,\"start\":79604},{\"end\":80039,\"start\":80029},{\"end\":80048,\"start\":80039},{\"end\":80057,\"start\":80048},{\"end\":80066,\"start\":80057},{\"end\":80078,\"start\":80066},{\"end\":80088,\"start\":80078},{\"end\":80099,\"start\":80088},{\"end\":80106,\"start\":80099},{\"end\":80120,\"start\":80106},{\"end\":80130,\"start\":80120},{\"end\":80143,\"start\":80130},{\"end\":80151,\"start\":80143},{\"end\":80159,\"start\":80151},{\"end\":80169,\"start\":80159},{\"end\":80179,\"start\":80169},{\"end\":80189,\"start\":80179},{\"end\":80205,\"start\":80189},{\"end\":80216,\"start\":80205},{\"end\":80224,\"start\":80216},{\"end\":80231,\"start\":80224},{\"end\":80243,\"start\":80231},{\"end\":80738,\"start\":80730},{\"end\":80749,\"start\":80738},{\"end\":80757,\"start\":80749},{\"end\":80769,\"start\":80757},{\"end\":80782,\"start\":80769},{\"end\":80792,\"start\":80782},{\"end\":81101,\"start\":81093},{\"end\":81113,\"start\":81101},{\"end\":81126,\"start\":81113},{\"end\":81139,\"start\":81126},{\"end\":81149,\"start\":81139},{\"end\":81485,\"start\":81475},{\"end\":81500,\"start\":81485},{\"end\":81510,\"start\":81500},{\"end\":81740,\"start\":81730},{\"end\":81752,\"start\":81740},{\"end\":81762,\"start\":81752},{\"end\":81775,\"start\":81762},{\"end\":81785,\"start\":81775},{\"end\":82155,\"start\":82144},{\"end\":82167,\"start\":82155},{\"end\":82178,\"start\":82167},{\"end\":82188,\"start\":82178},{\"end\":82528,\"start\":82517},{\"end\":82540,\"start\":82528},{\"end\":82554,\"start\":82540},{\"end\":82570,\"start\":82554},{\"end\":82581,\"start\":82570},{\"end\":82990,\"start\":82980},{\"end\":83001,\"start\":82990},{\"end\":83009,\"start\":83001},{\"end\":83282,\"start\":83272},{\"end\":83293,\"start\":83282},{\"end\":83301,\"start\":83293},{\"end\":83584,\"start\":83574},{\"end\":83591,\"start\":83584},{\"end\":83599,\"start\":83591},{\"end\":83927,\"start\":83910},{\"end\":83936,\"start\":83927},{\"end\":83949,\"start\":83936},{\"end\":83961,\"start\":83949},{\"end\":84323,\"start\":84306},{\"end\":84335,\"start\":84323},{\"end\":84675,\"start\":84666},{\"end\":84683,\"start\":84675},{\"end\":85060,\"start\":85049},{\"end\":85071,\"start\":85060},{\"end\":85081,\"start\":85071},{\"end\":85095,\"start\":85081},{\"end\":85107,\"start\":85095},{\"end\":85562,\"start\":85550},{\"end\":85575,\"start\":85562},{\"end\":85588,\"start\":85575},{\"end\":85864,\"start\":85854},{\"end\":85874,\"start\":85864},{\"end\":85880,\"start\":85874},{\"end\":85888,\"start\":85880},{\"end\":85899,\"start\":85888},{\"end\":85908,\"start\":85899},{\"end\":85919,\"start\":85908},{\"end\":85932,\"start\":85919},{\"end\":85939,\"start\":85932},{\"end\":85948,\"start\":85939},{\"end\":85960,\"start\":85948},{\"end\":85967,\"start\":85960},{\"end\":85976,\"start\":85967},{\"end\":85983,\"start\":85976},{\"end\":85990,\"start\":85983},{\"end\":85997,\"start\":85990},{\"end\":86004,\"start\":85997},{\"end\":86012,\"start\":86004},{\"end\":86022,\"start\":86012},{\"end\":86033,\"start\":86022},{\"end\":86047,\"start\":86033},{\"end\":86059,\"start\":86047},{\"end\":86072,\"start\":86059},{\"end\":86081,\"start\":86072},{\"end\":86090,\"start\":86081},{\"end\":86104,\"start\":86090},{\"end\":86110,\"start\":86104},{\"end\":86409,\"start\":86400},{\"end\":86420,\"start\":86409},{\"end\":86433,\"start\":86420},{\"end\":86443,\"start\":86433},{\"end\":86721,\"start\":86712},{\"end\":86728,\"start\":86721},{\"end\":86737,\"start\":86728},{\"end\":86748,\"start\":86737},{\"end\":87142,\"start\":87130},{\"end\":87154,\"start\":87142},{\"end\":87161,\"start\":87154},{\"end\":87170,\"start\":87161},{\"end\":87178,\"start\":87170},{\"end\":87196,\"start\":87178},{\"end\":87208,\"start\":87196},{\"end\":87219,\"start\":87208},{\"end\":87229,\"start\":87219},{\"end\":87735,\"start\":87725},{\"end\":87749,\"start\":87735},{\"end\":87763,\"start\":87749},{\"end\":87781,\"start\":87763},{\"end\":87793,\"start\":87781},{\"end\":87804,\"start\":87793},{\"end\":87819,\"start\":87804},{\"end\":87829,\"start\":87819},{\"end\":87835,\"start\":87829},{\"end\":88273,\"start\":88261},{\"end\":88289,\"start\":88273},{\"end\":88299,\"start\":88289},{\"end\":88313,\"start\":88299},{\"end\":88322,\"start\":88313},{\"end\":88666,\"start\":88654},{\"end\":88674,\"start\":88666},{\"end\":88685,\"start\":88674},{\"end\":88694,\"start\":88685},{\"end\":89116,\"start\":89104},{\"end\":89126,\"start\":89116},{\"end\":89137,\"start\":89126},{\"end\":89489,\"start\":89475},{\"end\":89497,\"start\":89489},{\"end\":89506,\"start\":89497},{\"end\":89515,\"start\":89506},{\"end\":89522,\"start\":89515},{\"end\":89537,\"start\":89522},{\"end\":89545,\"start\":89537},{\"end\":89966,\"start\":89958},{\"end\":89974,\"start\":89966},{\"end\":89982,\"start\":89974},{\"end\":89994,\"start\":89982},{\"end\":90004,\"start\":89994},{\"end\":90011,\"start\":90004},{\"end\":90019,\"start\":90011},{\"end\":90300,\"start\":90292},{\"end\":90307,\"start\":90300},{\"end\":90314,\"start\":90307},{\"end\":90326,\"start\":90314},{\"end\":90336,\"start\":90326},{\"end\":90344,\"start\":90336},{\"end\":90736,\"start\":90728},{\"end\":90750,\"start\":90736},{\"end\":90756,\"start\":90750},{\"end\":90766,\"start\":90756},{\"end\":90774,\"start\":90766},{\"end\":91053,\"start\":91046},{\"end\":91065,\"start\":91053},{\"end\":91074,\"start\":91065},{\"end\":91084,\"start\":91074},{\"end\":91091,\"start\":91084},{\"end\":91099,\"start\":91091},{\"end\":91110,\"start\":91099},{\"end\":91121,\"start\":91110},{\"end\":91133,\"start\":91121},{\"end\":91144,\"start\":91133},{\"end\":91457,\"start\":91450},{\"end\":91464,\"start\":91457},{\"end\":91470,\"start\":91464},{\"end\":91478,\"start\":91470},{\"end\":91486,\"start\":91478},{\"end\":91835,\"start\":91828},{\"end\":91842,\"start\":91835},{\"end\":91848,\"start\":91842},{\"end\":91856,\"start\":91848},{\"end\":91864,\"start\":91856},{\"end\":91872,\"start\":91864},{\"end\":92259,\"start\":92250},{\"end\":92265,\"start\":92259},{\"end\":92275,\"start\":92265},{\"end\":92285,\"start\":92275},{\"end\":92649,\"start\":92640},{\"end\":92659,\"start\":92649},{\"end\":92668,\"start\":92659},{\"end\":92677,\"start\":92668},{\"end\":92687,\"start\":92677},{\"end\":92696,\"start\":92687},{\"end\":92706,\"start\":92696},{\"end\":93070,\"start\":93063},{\"end\":93079,\"start\":93070},{\"end\":93087,\"start\":93079},{\"end\":93098,\"start\":93087},{\"end\":93105,\"start\":93098},{\"end\":93113,\"start\":93105},{\"end\":93121,\"start\":93113},{\"end\":93473,\"start\":93467},{\"end\":93479,\"start\":93473},{\"end\":93489,\"start\":93479},{\"end\":93501,\"start\":93489},{\"end\":93916,\"start\":93910},{\"end\":93923,\"start\":93916},{\"end\":94164,\"start\":94158},{\"end\":94171,\"start\":94164},{\"end\":94178,\"start\":94171},{\"end\":94400,\"start\":94394},{\"end\":94409,\"start\":94400},{\"end\":94417,\"start\":94409},{\"end\":94425,\"start\":94417},{\"end\":94432,\"start\":94425},{\"end\":94679,\"start\":94666},{\"end\":94692,\"start\":94679},{\"end\":95059,\"start\":95050},{\"end\":95066,\"start\":95059},{\"end\":95072,\"start\":95066},{\"end\":95079,\"start\":95072},{\"end\":95087,\"start\":95079},{\"end\":95349,\"start\":95340},{\"end\":95360,\"start\":95349},{\"end\":95371,\"start\":95360},{\"end\":95381,\"start\":95371},{\"end\":95594,\"start\":95585},{\"end\":95610,\"start\":95594},{\"end\":95618,\"start\":95610},{\"end\":95629,\"start\":95618},{\"end\":95642,\"start\":95629},{\"end\":95654,\"start\":95642},{\"end\":96046,\"start\":96039},{\"end\":96054,\"start\":96046},{\"end\":96065,\"start\":96054},{\"end\":96071,\"start\":96065},{\"end\":96078,\"start\":96071},{\"end\":96085,\"start\":96078},{\"end\":96097,\"start\":96085},{\"end\":96110,\"start\":96097}]", "bib_venue": "[{\"end\":60821,\"start\":60761},{\"end\":61205,\"start\":61145},{\"end\":61602,\"start\":61542},{\"end\":62387,\"start\":62345},{\"end\":62749,\"start\":62692},{\"end\":63113,\"start\":63056},{\"end\":63502,\"start\":63437},{\"end\":64297,\"start\":64240},{\"end\":64991,\"start\":64932},{\"end\":65456,\"start\":65396},{\"end\":65892,\"start\":65827},{\"end\":66325,\"start\":66265},{\"end\":66755,\"start\":66698},{\"end\":67245,\"start\":67188},{\"end\":68024,\"start\":67971},{\"end\":68457,\"start\":68397},{\"end\":69058,\"start\":69002},{\"end\":69847,\"start\":69790},{\"end\":70525,\"start\":70459},{\"end\":71269,\"start\":71212},{\"end\":71719,\"start\":71653},{\"end\":72315,\"start\":72262},{\"end\":73215,\"start\":73168},{\"end\":73651,\"start\":73599},{\"end\":74435,\"start\":74375},{\"end\":74873,\"start\":74813},{\"end\":75639,\"start\":75579},{\"end\":76467,\"start\":76420},{\"end\":77306,\"start\":77246},{\"end\":77797,\"start\":77740},{\"end\":78265,\"start\":78205},{\"end\":78746,\"start\":78689},{\"end\":79248,\"start\":79188},{\"end\":79742,\"start\":79682},{\"end\":81251,\"start\":81204},{\"end\":82281,\"start\":82242},{\"end\":82709,\"start\":82649},{\"end\":84063,\"start\":84016},{\"end\":84463,\"start\":84403},{\"end\":84816,\"start\":84757},{\"end\":85235,\"start\":85175},{\"end\":86870,\"start\":86813},{\"end\":87357,\"start\":87297},{\"end\":88366,\"start\":88345},{\"end\":88815,\"start\":88762},{\"end\":89235,\"start\":89190},{\"end\":89666,\"start\":89613},{\"end\":91588,\"start\":91541},{\"end\":92000,\"start\":91940},{\"end\":93221,\"start\":93179},{\"end\":93629,\"start\":93569},{\"end\":94820,\"start\":94760},{\"end\":95179,\"start\":95137},{\"end\":96273,\"start\":96200},{\"end\":60402,\"start\":60355},{\"end\":60759,\"start\":60693},{\"end\":61143,\"start\":61077},{\"end\":61540,\"start\":61474},{\"end\":61981,\"start\":61855},{\"end\":62343,\"start\":62295},{\"end\":62690,\"start\":62627},{\"end\":63054,\"start\":62991},{\"end\":63435,\"start\":63355},{\"end\":63857,\"start\":63792},{\"end\":64238,\"start\":64175},{\"end\":64597,\"start\":64572},{\"end\":64930,\"start\":64865},{\"end\":65394,\"start\":65328},{\"end\":65825,\"start\":65754},{\"end\":66263,\"start\":66197},{\"end\":66696,\"start\":66633},{\"end\":67186,\"start\":67123},{\"end\":67615,\"start\":67559},{\"end\":67969,\"start\":67910},{\"end\":68395,\"start\":68329},{\"end\":68746,\"start\":68730},{\"end\":69000,\"start\":68938},{\"end\":69343,\"start\":69296},{\"end\":69558,\"start\":69536},{\"end\":69788,\"start\":69725},{\"end\":70119,\"start\":70072},{\"end\":70457,\"start\":70385},{\"end\":70870,\"start\":70805},{\"end\":71210,\"start\":71147},{\"end\":71651,\"start\":71579},{\"end\":72019,\"start\":71997},{\"end\":72260,\"start\":72201},{\"end\":72586,\"start\":72564},{\"end\":72865,\"start\":72818},{\"end\":73166,\"start\":73113},{\"end\":73597,\"start\":73530},{\"end\":74005,\"start\":73946},{\"end\":74373,\"start\":74307},{\"end\":74811,\"start\":74745},{\"end\":75197,\"start\":75172},{\"end\":75577,\"start\":75511},{\"end\":76028,\"start\":76024},{\"end\":76418,\"start\":76365},{\"end\":76849,\"start\":76827},{\"end\":77244,\"start\":77178},{\"end\":77738,\"start\":77675},{\"end\":78203,\"start\":78137},{\"end\":78687,\"start\":78624},{\"end\":79186,\"start\":79120},{\"end\":79680,\"start\":79614},{\"end\":80299,\"start\":80243},{\"end\":80851,\"start\":80792},{\"end\":81202,\"start\":81149},{\"end\":81473,\"start\":81433},{\"end\":81841,\"start\":81785},{\"end\":82240,\"start\":82195},{\"end\":82647,\"start\":82581},{\"end\":83065,\"start\":83009},{\"end\":83348,\"start\":83301},{\"end\":83662,\"start\":83599},{\"end\":84014,\"start\":83961},{\"end\":84401,\"start\":84335},{\"end\":84755,\"start\":84690},{\"end\":85173,\"start\":85107},{\"end\":85644,\"start\":85588},{\"end\":86509,\"start\":86443},{\"end\":86811,\"start\":86748},{\"end\":87295,\"start\":87229},{\"end\":87894,\"start\":87835},{\"end\":88343,\"start\":88322},{\"end\":88760,\"start\":88701},{\"end\":89188,\"start\":89137},{\"end\":89611,\"start\":89552},{\"end\":90023,\"start\":90019},{\"end\":90403,\"start\":90344},{\"end\":90833,\"start\":90774},{\"end\":91188,\"start\":91144},{\"end\":91539,\"start\":91486},{\"end\":91938,\"start\":91872},{\"end\":92344,\"start\":92285},{\"end\":92762,\"start\":92706},{\"end\":93177,\"start\":93129},{\"end\":93567,\"start\":93501},{\"end\":93927,\"start\":93923},{\"end\":94182,\"start\":94178},{\"end\":94436,\"start\":94432},{\"end\":94758,\"start\":94692},{\"end\":95135,\"start\":95087},{\"end\":95451,\"start\":95397},{\"end\":95756,\"start\":95670},{\"end\":96198,\"start\":96110}]"}}}, "year": 2023, "month": 12, "day": 17}