{"id": 247222831, "updated": "2023-10-05 16:24:39.805", "metadata": {"title": "CenterSnap: Single-Shot Multi-Object 3D Shape Reconstruction and Categorical 6D Pose and Size Estimation", "authors": "[{\"first\":\"Muhammad\",\"last\":\"Irshad\",\"middle\":[\"Zubair\"]},{\"first\":\"Thomas\",\"last\":\"Kollar\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Laskey\",\"middle\":[]},{\"first\":\"Kevin\",\"last\":\"Stone\",\"middle\":[]},{\"first\":\"Zsolt\",\"last\":\"Kira\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This paper studies the complex task of simultaneous multi-object 3D reconstruction, 6D pose and size estimation from a single-view RGB-D observation. In contrast to instance-level pose estimation, we focus on a more challenging problem where CAD models are not available at inference time. Existing approaches mainly follow a complex multi-stage pipeline which first localizes and detects each object instance in the image and then regresses to either their 3D meshes or 6D poses. These approaches suffer from high-computational cost and low performance in complex multi-object scenarios, where occlusions can be present. Hence, we present a simple one-stage approach to predict both the 3D shape and estimate the 6D pose and size jointly in a bounding-box free manner. In particular, our method treats object instances as spatial centers where each center denotes the complete shape of an object along with its 6D pose and size. Through this per-pixel representation, our approach can reconstruct in real-time (40 FPS) multiple novel object instances and predict their 6D pose and sizes in a single-forward pass. Through extensive experiments, we demonstrate that our approach significantly outperforms all shape completion and categorical 6D pose and size estimation baselines on multi-object ShapeNet and NOCS datasets respectively with a 12.6% absolute improvement in mAP for 6D pose for novel real-world object instances.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.01929", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/IrshadKLSK22", "doi": "10.1109/icra46639.2022.9811799"}}, "content": {"source": {"pdf_hash": "1956161b7b5d698ce4881983cfb488fc70a1d073", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2203.01929v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "deb45a8e01f18a6242b920acfa8ae12e4e4c692b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1956161b7b5d698ce4881983cfb488fc70a1d073.txt", "contents": "\nCenterSnap: Single-Shot Multi-Object 3D Shape Reconstruction and Categorical 6D Pose and Size Estimation\n\n\nMuhammad Zubair Irshad \nThomas Kollar \nMichael Laskey \nKevin Stone \nZsolt Kira \nCenterSnap: Single-Shot Multi-Object 3D Shape Reconstruction and Categorical 6D Pose and Size Estimation\n\nThis paper studies the complex task of simultaneous multi-object 3D reconstruction, 6D pose and size estimation from a single-view RGB-D observation. In contrast to instancelevel pose estimation, we focus on a more challenging problem where CAD models are not available at inference time. Existing approaches mainly follow a complex multi-stage pipeline which first localizes and detects each object instance in the image and then regresses to either their 3D meshes or 6D poses. These approaches suffer from high-computational cost and low performance in complex multi-object scenarios, where occlusions can be present. Hence, we present a simple onestage approach to predict both the 3D shape and estimate the 6D pose and size jointly in a bounding-box free manner. In particular, our method treats object instances as spatial centers where each center denotes the complete shape of an object along with its 6D pose and size. Through this perpixel representation, our approach can reconstruct in realtime (40 FPS) multiple novel object instances and predict their 6D pose and sizes in a single-forward pass. Through extensive experiments, we demonstrate that our approach significantly outperforms all shape completion and categorical 6D pose and size estimation baselines on multi-object ShapeNet and NOCS datasets respectively with a 12.6% absolute improvement in mAP for 6D pose for novel real-world object instances.\n\nI. INTRODUCTION\n\nMulti-object 3D shape reconstruction and 6D pose (i.e. 3D orientation and position) and size estimation from raw visual observations is crucial for robotics manipulation [1,2,3], navigation [4,5] and scene understanding [6,7]. The ability to perform pose estimation in real-time leads to fast feedback control [8] and the capability to reconstruct complete 3D shapes [9,10,11] results in fine-grained understanding of local geometry, often helpful in robotic grasping [2,12]. Recent advances in deep learning have enabled great progress in instance-level 6D pose estimation [13,14,15] where the exact 3D models of objects and their sizes are known a-priori. Unfortunately, these methods [16,17,18] do not generalize well to realistic-settings on novel object instances with unknown 3D models in the same category, often referred to as category-level settings. Despite progress in categorylevel pose estimation, this problem remains challenging even when similar object instances are provided as priors during training, due to a high variance of objects within a category.\n\nRecent works on shape reconstruction [19,20] and category-level 6D pose and size estimation [21,22,23] use complex multi-stage pipelines. As shown in Figure 1, these approaches independently employ two stages, one for performing 2D detection [24,25,26] and another for performing  (2) our single-stage approach. The single-stage approach uses object instances as centers to jointly optimize 3D shape, 6D pose and size.\n\nobject reconstruction or 6D pose and size estimation. This pipeline is computationally expensive, not scalable, and has low performance on real-world novel object instances, due to the inability to express explicit representation of shape variations within a category. Motivated by above, we propose to reconstruct complete 3D shapes and estimate 6D pose and sizes of novel object instances within a specific category, from a single-view RGB-D in a single-shot manner.\n\nTo address these challenges, we introduce Center-based Shape reconstruction and 6D pose and size estimation (Cen-terSnap), a single-shot approach to output complete 3D information (3D shape, 6D pose and sizes of multiple objects) in a bounding-box proposal-free and per-pixel manner. Our approach is inspired by recent success in anchor-free, singleshot 2D key-point estimation and object detection [27,28,29,30]. As shown in Figure 1, we propose to learn a spatial per-pixel representation of multiple objects at their center locations using a feature pyramid backbone [3,24]. Our technique directly regresses multiple shape, pose, and size codes, which we denote as object-centric 3D parameter maps. At each object's center point in these spatial objectcentric 3D parameter maps, we predict vectors denoting the complete 3D information (i.e. encoding 3D shape, 6D pose and sizes codes). A 3D auto-encoder [31,32] is designed to learn canonical shape codes from a large database of shapes. A joint optimization for detection, reconstruction and 6D pose and sizes for each object's spatial center is then carried out using learnt shape priors. Hence, we perform complete 3D scene-reconstruction and predict 6D pose and sizes of novel object instances in a single-forward pass, foregoing the need for complex multi-stage pipelines [19,22,24].\n\nOur proposed method leverages a simpler and computationally efficient pipeline for a complete object-centric 3D understanding of multiple objects from a single-view RGB-D observation. We make the following contributions:\n\n\u2022 Present the first work to formulate object-centric holistic scene-understanding (i.e. 3D shape reconstruction and 6D pose and size estimation) for multiple objects from a single-view RGB-D in a single-shot manner. \u2022 Propose a fast (real-time) joint reconstruction and pose estimation system. Our network runs at 40 FPS on a NVIDIA Quadro RTX 5000 GPU. \u2022 Our method significantly outperforms all baselines for 6D pose and size estimation on NOCS benchmark, with over 12% absolute improvement in mAP for 6D pose.\n\nII. RELATED WORK 3D shape prediction and completion: 3D reconstruction from a single-view observation has seen great progress with various input modalities studied. RGB-based shape reconstruction [31,33,34] has been studied to output either pointclouds, voxels or meshes [11,32,35]. Contrarily, learning-based 3D shape completion [36,37,38] studies the problem of completing partial pointclouds obtained from masked depth maps. However, all these works focus on reconstructing a single object. In contrast, our work focuses on multi-object reconstruction from a single RGB-D. Recently, multi-object reconstruction from RGB-D has been studied [19,39,40]. However, these approaches employ complex multi-stage pipelines employing 2D detections and then predicting canonical shapes. Our approach is a simple, bounding-box proposal-free method which jointly optimizes for detection, shape reconstruction and 6D pose and size.\n\nInstance-Level and Category-Level 6D Pose and Size Estimation: Works on Instance-level pose estimation use classical techniques such as template matching [41,42,43], direct pose estimation [13,15,18] or point correspondences [14,16]. Contrarily, our work closely follows the paradigm of category-level pose and size estimation where CAD models are not available during inference. Previous work has employed complex multi-stage pipelines [21,22,44] for category-level pose estimation. Our work optimizes for shape, pose, and sizes jointly, while leveraging the shape priors obtained by training a large dataset of CAD models. CenterSnap is a simpler, more effective, and faster solution. Per-pixel point-based representation has been effective for anchor-free object detection and segmentation. These approaches [28,30,45] represent instances as their centers in a spatial 2D grid. This representation has been further studied for key-point detection [27], segmentation [46,47] and bodymesh recovery [48,49]. Our approach falls in a similar paradigm and further adds a novelty to reconstruct objectcentric holistic 3D information in an anchor-free manner. Different from [39,48], our approach 1) considers pretrained shape priors on a large collection of CAD models 2) jointly optimizes categorical shape 6D pose and size, instead of 3D-bounding boxes and 3) considers more complicated scenarios (such as occlusions, a large variety of objects and sim2real transfer with limited real-world supervision).\n\n\nIII. CENTERSNAP: SINGLE-SHOT OBJECT-CENTRIC SCENE UNDERSTANDING OF MULTIPLE-OBJECTS\n\nGiven an RGB-D image as input, our goal is to simultaneously detect, reconstruct and localize all unknown object instances in the 3D space. In essence, we regard shape reconstruction and pose and size estimation as a point-based representation problem where each object's complete 3D information is represented by its center point in the 2D spatial image. Formally, given an RGB-D single-view observation (I \u2208 R ho\u00d7wo\u00d73 , D \u2208 R ho\u00d7wo ) of width w o and height h o , our aim is to reconstruct the complete pointclouds (P \u2208 R K\u00d7N \u00d73 ) coupled with 6D pose and scales (P \u2208 SE(3),\u015d \u2208 R 3 ) of all object instances in the 3D scene, where K is the number of arbitrary objects in the scene and N denotes the number of points in the pointcloud. The pose (P \u2208 SE(3)) of each object is denoted by a 3D rotationR \u2208 SO(3) and a translationt \u2208 R 3 . The 6D poseP, 3D size (spatial extent obtained from canonical pointclouds P ) and 1D scales\u015d completely defines the unknown object instances in 3D space with respect to the camera coordinate frame. To achieve the above goal, we employ an end-to-end trainable method as illustrated in Figure 2. First, objects instances are detected as heatmaps in a per-pixel manner (Section III-A) using a CenterSnap detection backbone based on feature pyramid networks [3,50]. Second, a joint shape, pose, and size code denoted by object-centric 3D parameter maps is predicted for detected object centers using specialized heads (Section III-C). Our pre-training of shape codes is described in Section III-B. Lastly, 2D heatmaps and our novel object-centric 3D parameter maps are jointly optimized to predict shapes, pose and sizes in a single-forward pass (Section III-D).\n\n\nA. Object instances as center points\n\nWe represent each object instance by its 2D location in the spatial RGB image following [28,30]. Given a RGB-D observation (I \u2208 R ho\u00d7wo\u00d73 , D \u2208 R ho\u00d7wo ), we generate a low-resolution spatial feature representations f r \u2208 R ho/4\u00d7wo/4\u00d7Cs and f d \u2208 R ho/4\u00d7wo/4\u00d7Cs by using Resnet [51] stems, where C s = 32. We concatenate computed features f r and f d along the channel dimension before feeding it to Resnet18-FPN backbone [52] to compute a pyramid of features (f rd ) with scales ranging from 1/8 to 1/2 resolution, where each pyramid level has the same channel dimension (i.e. 64). We use these combined features with a specialized heatmap head to predict object-based heatmaps\u0176 \u2208 [0, 1] ho R \u00d7 wo R \u00d71 where R = 8 denotes the heat-map down-sampling factor. Our specialized heatmap head design merges the semantic information from all FPN levels into one output (\u0176 ). We use three upsampling stages followed by element-wise sum and sof tmax to achieve this. This design allows our network to 1) capture multiscale information and 2) encode features at higher resolution  Fig. 2: CenterSnap Method: Given a single-view RGB-D observation, our proposed approach jointly optimizes for shape, pose, and sizes of each object in a single-shot manner. Our method comprises a joint FPN backbone for feature extraction (Section III-A), a pointcloud auto-encoder to extract shape codes from a large collection of CAD models (Section III-B), CenterSnap model which constitutes multiple specialized heads for heatmap and object-centric 3D parameter map prediction (Section III-C) and joint optimization for shape, pose, and sizes for each object's spatial center (Section III-D).\n\nfor effective reasoning at the per-pixel level. We train the network to predict ground-truth heatmaps (Y ) by minimizing\nMSE loss, L inst = xyg \u0176 \u2212 Y 2 . The Gaussian ker- nel Y xyg = exp \u2212 (x\u2212cx) 2 +(y\u2212cy) 2 2\u03c3 2\nof each center in the ground truth heat-maps (Y ) is relative to the scale-based standard deviation \u03c3 of each object, following [3,28,30,53].\n\n\nB. Shape, Pose, and Size Codes\n\nTo jointly optimize the object-based heatmaps, 3D shapes and 6D pose and sizes, the complete object-based 3D information (i.e. Pointclouds P , 6D poseP and scale\u015d) are represented as as object-centric 3D parameter maps (O 3d \u2208 R ho\u00d7wo\u00d7141 ). O 3d constitutes two parts, shape latentcode and 6D Pose and scales. The pointcloud representation for each object is stored in the object-centric 3D parameter maps as a latent-shape code (z i \u2208 R 128 ). The ground-truth Pose (P) represented by a 3 \u00d7 3 rotationR \u2208 SO(3) and translationt \u2208 R 3 coupled with 1D scale\u015d are vectorized to store in the O 3d as 13-D vectors. To learn a shapecode (z i ) for each object, we design an auto-encoder trained on all 3D shapes from a set of CAD models. Our autoencoder is representation-invariant and can work with any shape representation. Specifically, we design an encoderdecoder network (Figure 3), where we utilize a Point-Net encoder (g \u03c6 ) similar to [54]. The decoder network (d \u03b8 ), which comprises three fully-connected layers, takes the encoded low-dimensional feature vector i.e. the latent shape-code (z i ) and reconstructs the input pointcloudP i = d \u03b8 (g \u03c6 (P i )).\n\nTo train the auto-encoder, we sample 2048 points from the ShapeNet [55] CAD model repository and use them as ground-truth shapes. Furthermore, we unit-canonicalize the input pointclouds by applying a scaling transform to each shape such that the shape is centered at origin and unit normalized. We optimize the encoder and decoder networks jointly using the reconstruction-error, denoted by Chamferdistance, as shown below:\nD cd (P i ,P i ) = 1 |P i | x\u2208Pi min y\u2208Pi x \u2212 y 2 2 + 1 |P i | y\u2208Pi min x\u2208Pi x \u2212 y 2 2\nSample decoder outputs and t-SNE embeddings [56] for the latent shape-code (z i ) are shown in Figure 3 and our complete 3D reconstructions on novel real-world object instances are visualizes in Figure 4 as pointclouds, meshes and textures. Our shape-code space provides a compact way to encode 3D shape information from a large number of CAD models. As shown by the t-SNE embeddings (Figure 3), our shape-code space finds a distinctive 3D space for semantically similar objects and provides an effective way to scale shape prediction to a large number (i.e. 50+) of categories.\n\n\nC. CenterSnap Model\n\nGiven object center heatmaps (Section III-A), the goal of the CenterSnap model is to infer object-centric 3D parameter maps which define each object instance completely in the 3D-space. The CenterSnap model comprises a taskspecific head similar to the heatmap head (Section III-A) with the input being pyramid of features (f rd ). During training, the task-specific head outputs a 3D parameter map\u00d4 3d \u2208 R ho R \u00d7 wo R \u00d7141 where each pixel in the downsampled map ( ho R \u00d7 wo R ) contains the complete object-centric 3D information (i.e. shape-code z i , 6D poseP and scal\u00ea s) as 141-D vectors, where R = 8. Note that, during training, we obtain the ground-truth shape-codes from the pre-trained point-encoder\u1e91 i = g \u03c6 (P i ). For Pose (P), our choice of rotation representationR \u2208 SO(3) is determined by stability during training [57]. Furthermore, we project the predicted 3 \u00d7 3 rotationR into SO(3), as follows:\nSVD + (R) = U \u03a3 V T , where \u03a3 = diag 1, 1, det U V T\nTo handle ambiguities caused by rotational symmetries, we also employ a rotation mapping function defined by [58]. The mapping function, used only for symmetric objects (bottle, bowl, and can), maps ambiguous ground-truth rotations to a single canonical rotation by normalizing the pose rotation. During training, we jointly optimize the predicted objectcentric 3D parameter map (\u00d4 3d ) using a masked Huber loss (Eq. 1), where the Huber loss is enforced only where the Gaussian heatmaps (Y ) have score greater than 0.3 to prevent ambiguity in areas where no objects exist. Similar to the Gaussian distribution of heatmaps in Section III-A, the ground-truth Object 3D-maps (O 3d ) are calculated using the scale-based Gaussian kernel Y xyg of each object.\nL3D(O3d,\u00d43d) = 1 2 (O3d \u2212\u00d43d) 2 if |(O3d \u2212\u00d43d)| < \u03b4 \u03b4 (O3d \u2212\u00d43d) \u2212 1 2 \u03b4 otherwise (1)\nAuxiliary Depth-Loss: We additionally integrate an auxiliary depth reconstruction loss L D for effective sim2real transfer, where L D (D,D) minimizes the Huber loss (Eq. 1) between target depth (D) and the predicted depth (D) from the output of task-specific head, similar to the one used in Section III-A. The depth auxiliary loss (further investigated empirically using ablation study in Section IV) forces the network to learn geometric features by reconstructing artifactfree depth. Since real depth sensors contain artifacts, we enforce this loss by pre-processing the input synthetic depth images to contain noise and random eclipse dropouts [3].\n\n\nD. Joint Shape, Pose, and Size Optimization\n\nWe jointly optimize for detection, reconstruction and localization. Specifically, we minimize a combination of heatmap instance detection, object-centric 3D map prediction and auxiliary depth losses as\nL = \u03bb l L inst + \u03bb O 3d L O 3d + \u03bb d L D\nwhere \u03bb is a weighting co-efficient with values determined empirically as 100, 1.0 and 1.0 respectively.\n\nInference: During inference, we perform peak detection as in [28] on the heatmap output (\u0176 ) to get detected centerpoints for each object, c i in R 2 = (x i , y i ) (as shown in Figure 2 middle). These centerpoints are local maximum in heatmap output (\u0176 ). We perform non-maximum suppression on the detected heatmap maximas using a 3\u00d73 max-pooling, following [28]. Lastly, we directly sample the object-centric 3D parameter map of each object from\u00d4 3d at the predicted center location (c i ) via\u00d4 3d (x i , y i ). We perform inference on the extracted latent-codes using point-decoder to reconstruct pointclouds (P i = d \u03b8 (z p i )). Finally, we extract 3 \u00d7 3 rotation R p i , 3D translation vectort p i and 1D scales\u015d p i from\u00d4 3d to get transformed points in the 3D spaceP recon i = [R p i |t p i ] * s p i * P i (as shown in Figure 2 right-bottom). \n\n\nReconstructions\n\n\n3D Shape 6D Pose\n\n\nIV. EXPERIMENTS & RESULTS\n\nIn this section, we aim to answer the following questions: 1) How well does CenterSnap reconstruct multiple objects from a single-view RGB-D observation? 2) Does CenterSnap perform fast pose-estimation in real-time for real-world applications? 3) How well does CenterSnap perform in terms of 6D pose and size estimation?\n\nDatasets: We utilize the NOCS [22] dataset to evaluate both shape reconstruction and categorical 6D pose and size estimation. We use the CAMERA dataset for training which contains 300K synthetic images, where 25K are held out for evaluation. Our training set comprises 1085 object instances from 6 different categories -bottle, bowl, camera, can, laptop and mug whereas the evaluation set contains 184 different instances. The REAL dataset contains 4300 images from 7 different scenes for training, and 2750 real-world images from 6 scenes for evaluation. Further, we evaluate multiobject reconstruction and completion using Multi-Object ShapeNet Dataset (MOS). We generate this dataset using the SimNet [3] pipeline. Our datasets contains 640px \u00d7 480px renderings of multiple (3-10) ShapeNet objects [55] in a table-top scene. Following [3], we randomize over lighting and textures using OpenGL shaders with PyRender [61]. Following [38], we utilize 30974 models from 8 different categories for training (i.e. MOS-train): airplane, cabinet, car, chair, lamp, sofa, table. We use the held out set (MOStest) of 150 models for testing from a novel set of categories bed, bench, bookshelf and bus. Evaluation Metrics: Following [22], we independently evaluate the performance of 3D object detection and 6D pose estimation using the following key metrics: 1) Averageprecision for various IOU-overlap thresholds (IOU25 and IOU50). 2) Average precision of object instances for which the error is less than n \u2022 for rotation and m cm for translation (5\u00b05 cm, 5\u00b010 cm and 10\u00b010 cm). For shape reconstruction we use Chamfer distance (CD) following [38]. Implementation Details: CenterSnap is trained on the CAMERA training dataset with fine-tuning on the REAL training set. We use a batch-size of 32 and trained the network for 40 epochs with early-stopping based on the performance of the model on the held out validation set. We found data-augmentation (i.e. color-jitter) on the real- Best results are highlighted in bold. * denotes the method does not evaluate size and scale hence does not report IOU metric. For a fair comparison with other approaches, we report the per-class metrics using nocs-level class predictions. Note that the comparison results are either fair re-evaluations from the author's provided best checkpoints or reported from the original paper.  to 6D pose tracking baselines such as [65,66] which are not detection-based (i.e. do not report mAP metrics) and require pose initialization.\n\nComparison with NOCS baselines: The results of our proposed CenterSnap method are reported in Table I and Figure 6. Our proposed approach consistently outperforms all the baseline methods on both 3D object detection and 6D pose estimation. Among our variants, CenterSnap-R achieves the best performance. Our method (i.e. CenterSnap) is able to outperform strong baselines (#1 -#5 in Table I) even without iterative refinement. Specifically, CenterSnap-R method shows superior performance on the REAL test-set by achieving a mAP of 80.2% for 3D IOU at 0.5, 31.6% for 6D pose at 5\u00b010 cm and 70.9% for 6D pose at 10\u00b010 cm, hence demonstrating an absolute improvement of 2.7%, 10.8% and 12.6% over the best-performing baseline on the Real dataset. Our method also achieves superior test-time performance on CAMERA evaluation never seen during training. We achieve a mAP of 92.5% for 3D IOU at 0.5, 71.7% for 6D pose at 5\u00b010 cm and 87.9% for 6D pose at 10\u00b010 cm, demonstrating an absolute improvement of 1.8%, 12.1% and 6.6% over the best-performing baseline.\n\nNOCS Reconstruction: To quantitatively analyze the reconstruction accuracy, we measure the Chamfer distance (CD) of our reconstructed pointclouds with ground-truth CAD model in NOCS. Our results are reported in Table II. Our results show consistently lower CD metrics for all class categories which shows superior reconstruction performance on novel object instances. We report a lower mean Chamfer distance of 0.14 on CAMERA25 and 0.15 on REAL275 compared to 0.20 and 0.32 reported by the competitive baseline [21]. Comparison with Shape Completion Baselines: We further test our network's ability to reconstruct complete 3D shapes by comparing against depth-based shape-completion baselines i.e. PCN [38] and Folding-Net [67]. The results of our CenterSnap method are reported in Figure 5. Our consistently lower Chamfer distance (CD) compared to strong shape-completion baselines show our network's ability to reconstruct complete 3D shapes from partial 3D information such as depth-maps. We report a lower mean CD of 0.089 on test-instances from categories not included during training vs 0.0129 for PCN and 0.0124 for Folding-Net respectively. Inference time: Given RGB-D images of size 640 \u00d7 480, our method performs fast (real-time) joint reconstruction and pose and size estimation. We achieve an interactive rate of around 40 FPS for CenterSnap on a desktop with an Intel Xeon W-10855M@2.80GHz CPU and NVIDIA Quadro RTX 5000 GPU, which is fast enough for realtime applications. Specifically, our networks takes 22 ms and reconstruction takes around 3 ms. In comparison, on the same machine, competitive multi-stage baselines [21,22] achieve an interactive rate of 4 FPS for pose estimation.\n\nAblation Study: An empirical study to validate the significance of different design choices and modalities in our proposed CenterSnap model was carried out. Our results are summarized in Table III. We investigate the performance im- pact of Input-modality (i.e. RGB, Depth or RGB-D), Shape, Training-regime and Depth-Auxiliary loss on the held out Real-275 set. Our ablations results show that our network with just mono-RGB sensor performs the worst (31.5% IOU50 and 30.1% 6D pose at 10\u00b010 cm) likely because 2D-3D is an ill-posed problem and the task is 3D in nature.\n\nThe networks with Depth-only (66.7% IOU50 and 63.2% 6D pose at 10\u00b010 cm) and RGB-D (80.2% IOU50 and 70.9% 6D pose at 10\u00b010 cm) perform much better. Our model without shape prediction under-performs the model with shape (#3 vs #8 in Table III), indicating shape understanding is needed to enable robust 6D pose estimation performance. The result without depth auxiliary loss (0.17 CD, 78.3% IOU50 and 68.3% 6D pose at 10\u00b010 cm) indicates that adding a depth prediction task improved the performance of our model (1.9% absolute for IOU50 and 2.6% absolute for 6D pose at 10\u00b010 cm) on real-world novel object instances. Our model trained on NOCS CAMERA-train with fine-tuning on Real-train (80.2% IOU50 and 70.9% 6D pose at 10\u00b010 cm) outperforms all other training-regime ablations such as training only on CAMERA-train or combined CAMERA and REAL train-sets (#1 and-#2 in Table III) which indicates that sequential learning in this case leads to more robust sim2real transfer.\n\nQualitative Results: We qualitatively analyze the performance of CenterSnap on NOCS Real-275 test-set never seen during training. As shown in Figure 7, our method performs accurate 6D pose estimation and joint shape reconstruction on 5 different real-world scenes containing novel object instances. Our method also reconstructs complete 3D shapes (visualized with two different camera viewpoints) with accurate aspect ratios and fine-grained geometric details such as mug-handle and can-head.\n\n\nV. CONCLUSION\n\nDespite recent progress, existing categorical 6D pose and size estimation approaches suffer from high-computational cost and low performance. In this work, we propose an anchor-free and single-shot approach for holistic objectcentric 3D scene-understanding from a single-view RGB-D. Our approach runs in real-time (40 FPS) and performs accurate categorical pose and size estimation, achieving significant improvements against strong baselines on the NOCS REAL275 benchmark on novel object instances.\n\nFig. 1 :\n1Overview: (1) Multi-stage pipelines in comparison to\n\nFig. 3 :\n3Shape Auto-Encoder: We design a Point Auto-encoder (a) to find unique shape-code (zi) for all the shapes. Unit-canonicalized pointcloud outputs from the decoder network are shown in (b). t-SNE embeddings for shape-code (zi) are visualized in (c)\n\nFig. 4 :\n4Sim2Real Reconstruction: Single-shot sim2real shape reconstructions on NOCS showing pointclouds, meshes and textures.\n\nFig. 6 :Fig. 7 :\n67mAP on Real-275 test-set: Our method's mean Average Precision on NOCS for various IOU and pose error thresholds. Qualitative Results: We visualize the real-world 3D shape prediction and 6D pose and size estimation of our method (Center-Snap) from different viewpoints (green and red backgrounds).\n\nRGB - D\n-FPN Backbone...Object Instances as CenterPoints \n(Section III-A) \n\nShape, Pose \nand Size Codes \n(Section III-B) \n\nCenterSnap Model \n(Section III-C) \n\n... \n\nResNet \n\n+ \n\nCanonical Shapes \n\nHeatmap \nhead \n\nJoint Shape, Pose and Size Optimization \n(Section III-D) \n\nObject-centric \n3D Parameter \nMaps head \n\nCanonical Pointclouds \n\nDuring Training only \n\nSizes \n& Scales \n\nAbsolute Pose \n\n... \n\n... \n\nPoint \nDecoder \n\nPoint \nEncoder \n\n\n\nTABLE I :\nIQuantitative comparison of 3D object detection and 6D pose estimation on NOCS[22]: Comparison with strong baselines.\n\nTABLE II :\nIIQuantitativecomparison of 3D shape reconstruction on NOCS [22]: Evaluated with CD metric (10 \u22122 ). Lower is better. \n\nCAMERA25 \nREAL275 \n\nMethod \nBottle \nBowl \nCamera \nCan \nLaptop \nMug \nMean \nBottle \nBowl \nCamera \nCan \nLaptop \nMug \nMean \n\n1 \nReconstruction [21] \n0.18 \n0.16 \n0.40 \n0.097 \n0.20 \n0.14 \n0.20 \n0.34 \n0.12 \n0.89 \n0.15 \n0.29 \n0.10 \n0.32 \n2 \nShapePrior [21] \n0.34 \n0.22 \n0.90 \n0.22 \n0.33 \n0.21 \n0.37 \n0.50 \n0.12 \n0.99 \n0.24 \n0.71 \n0.097 \n0.44 \n\n3 \nCenterSnap (Ours) \n0.11 \n0.10 \n0.29 \n0.13 \n0.07 \n0.12 \n0.14 \n0.13 \n0.10 \n0.43 \n0.09 \n0.07 \n0.06 \n0.15 \n\ntraining set to be helpful for stability and training perfor-\nmance. The auto-encoder network is comprised of a Point-\nNet encoder [54] and three-layered fully-connected decoder \neach with output dimension of 512, 1024 and 1024 \u00d7 3. The \nauto-encoder is frozen after initially training on CAMERA \nCAD models for 50 epochs. We use Pytorch [62] for all \nour models and training pipeline implementation. For shape-\ncompletion experiments, we train only on MOS-train with \ntesting on MOS-test. \nNOCS Baselines: We compare seven model variants to \nshow effectiveness of our method: (1) NOCS [22]: Extends \nMask-RCNN architecture to predict NOCS map and uses \nsimilarity transform with depth to predict pose and size. \nOur results are compared against the best pose-estimation \nconfiguration in NOCS (i.e. 32-bin classification) (2) Shape \nPrior [21]: Infers 2D bounding-box for each object and \npredicts a shape-deformation. (3) CASS [44]: Employs a 2-\nstage approach to first detect 2D bounding-boxes and second \nregress the pose and size. (4) Metric-Scale [60]: Extends \nNOCS to predict object center and metric shape separately \n(5) CenterSnap: Our single-shot approach with direct pose \nand shape regression. (6) CenterSnap-R: Our model with \na standard point-to-plane iterative pose refinement [63, 64] \nbetween the projected canonical pointclouds in the 3D space \nand the depth-map. Note that we do not include comparisons \n\nAirplane Cabinet Car \nChair Lamp \nSofa \nTable \nMean MOS-test \n\n0.0000 \n\n0.0025 \n\n0.0050 \n\n0.0075 \n\n0.0100 \n\n0.0125 \n\nCD \n\nPCN \nFN \nCenterSnap(Ours) \n\nFig. 5: Shape Completion: Chamfer distance (CD reported on y-\naxis) evaluation on Multi-object ShapeNet dataset. \n\n\n\nTABLE III :\nIIIAblation Study: Study of the Proposed CenterSnap method on NOCS Real-test set to investigate the impact of different components i.e. Input, Shape, Training Regime (TR) and Depth-Auxiliary loss (D-Aux) on performance. C indicates Camera-train, R indicates Real-train and RF indicates Real-train with finetuning. 3D shape reconstruction evaluated with CD (10 \u22122 ). * denotes the method does not evaluate size and scale and so has no IOU metric.Metrics \n\n3D Shape \n6D Pose \n\n# Input \nShape \nTR \nD-Aux \nCD \u2193 \nIOU25 \u2191 \nIOU50 \u2191 \n5\u00b010 cm \u2191 \n10\u00b010 cm \u2191 \n\n1 RGB-D \nC \n0.19 \n28.4 \n27.0 \n14.2 \n48.2 \n2 RGB-D \nC+R \n0.19 \n41.5 \n40.1 \n27.1 \n58.2 \n3 RGB-D  *  \nC+RF \n-\n-\n-\n13.8 \n50.2 \n4 RGB \nC+RF \n0.20 \n63.7 \n31.5 \n8.30 \n30.1 \n5 Depth \nC+RF \n0.15 \n74.2 \n66.7 \n30.2 \n63.2 \n6 RGB-D \nC+RF \n0.17 \n82.3 \n78.3 \n30.8 \n68.3 \n7 RGB-D \nC+RF \n0.15 \n83.5 \n80.2 \n31.6 \n70.9 \n\n\n\nProbabilistic articulated real-time tracking for robot manipulation. C G Cifuentes, J Issac, M W\u00fcthrich, S Schaal, J Bohg, IEEE Robotics and Automation Letters. 22C. G. Cifuentes, J. Issac, M. W\u00fcthrich, S. Schaal, and J. Bohg, \"Probabilistic articulated real-time tracking for robot manipulation,\" IEEE Robotics and Automation Letters, vol. 2, no. 2, pp. 577-584, 2016.\n\nSynergies between affordance and geometry: 6-Dof grasp detection via implicit representations. Z Jiang, Y Zhu, M Svetlik, K Fang, Y Zhu, Robotics: science and systems. Z. Jiang, Y. Zhu, M. Svetlik, K. Fang, and Y. Zhu, \"Synergies between affordance and geometry: 6-Dof grasp detection via implicit representations,\" Robotics: science and systems, 2021.\n\nSimNet: Enabling robust unknown object manipulation from pure synthetic data via stereo. M Laskey, B Thananjeyan, K Stone, T Kollar, M Tjersland, 5th Annual Conference on Robot Learning. M. Laskey, B. Thananjeyan, K. Stone, T. Kollar, and M. Tjersland, \"SimNet: Enabling robust unknown ob- ject manipulation from pure synthetic data via stereo,\" in 5th Annual Conference on Robot Learning, 2021.\n\nFrustum Pointnets for 3D object detection from RGB-D data. C R Qi, W Liu, C Wu, H Su, L J Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, \"Frustum Pointnets for 3D object detection from RGB- D data,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 918-927.\n\n3D object proposals for accurate object class detection. X Chen, K Kundu, Y Zhu, A G Berneshawi, H Ma, S Fidler, R Urtasun, Advances in Neural Information Processing Systems. CiteseerX. Chen, K. Kundu, Y. Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and R. Urtasun, \"3D object proposals for accurate object class detection,\" in Advances in Neural Information Processing Systems. Citeseer, 2015, pp. 424-432.\n\nHolistic 3D scene understanding from a single image with implicit representation. C Zhang, Z Cui, Y Zhang, B Zeng, M Pollefeys, S Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionC. Zhang, Z. Cui, Y. Zhang, B. Zeng, M. Pollefeys, and S. Liu, \"Holistic 3D scene understanding from a single image with implicit representation,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 8833-8842.\n\nTotal3DUnderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image. Y Nie, X Han, S Guo, Y Zheng, J Chang, J J Zhang, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Y. Nie, X. Han, S. Guo, Y. Zheng, J. Chang, and J. J. Zhang, \"Total3DUnderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image,\" in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\n\nReal-time perception meets reactive motion generation. D Kappler, F Meier, J Issac, J Mainprice, C G Cifuentes, M W\u00fcthrich, V Berenz, S Schaal, N Ratliff, J Bohg, IEEE Robotics and Automation Letters. 33D. Kappler, F. Meier, J. Issac, J. Mainprice, C. G. Ci- fuentes, M. W\u00fcthrich, V. Berenz, S. Schaal, N. Ratliff, and J. Bohg, \"Real-time perception meets reactive motion generation,\" IEEE Robotics and Automation Letters, vol. 3, no. 3, pp. 1864-1871, 2018.\n\nMask2CAD: 3D shape prediction by learning to segment and retrieve. W Kuo, A Angelova, T.-Y. Lin, A Dai, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerProceedings, Part III 16W. Kuo, A. Angelova, T.-Y. Lin, and A. Dai, \"Mask2CAD: 3D shape prediction by learning to seg- ment and retrieve,\" in Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23- 28, 2020, Proceedings, Part III 16. Springer, 2020, pp. 260-277.\n\nDifferentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. M Niemeyer, L Mescheder, M Oechsle, A Geiger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionM. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger, \"Differentiable volumetric rendering: Learn- ing implicit 3d representations without 3d supervision,\" in Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, 2020, pp. 3504- 3515.\n\nOccupancy networks: Learning 3D reconstruction in function space. L Mescheder, M Oechsle, M Niemeyer, S Nowozin, A Geiger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionL. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger, \"Occupancy networks: Learning 3D reconstruction in function space,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4460-4470.\n\nPlanning Optimal Grasps. C Ferrari, J F Canny, ICRA. 36C. Ferrari and J. F. Canny, \"Planning Optimal Grasps.\" in ICRA, vol. 3, no. 4, 1992, p. 6.\n\nSDD-6D: Making RGB-based 3D detection and 6d pose estimation great again. W Kehl, F Manhardt, F Tombari, S Ilic, N Navab, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionW. Kehl, F. Manhardt, F. Tombari, S. Ilic, and N. Navab, \"SDD-6D: Making RGB-based 3D detection and 6d pose estimation great again,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 1521-1529.\n\nBB8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. M Rad, V Lepetit, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionM. Rad and V. Lepetit, \"BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth,\" in Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 3828-3836.\n\nPoseCNN: A convolutional neural network for 6d object pose estimation in cluttered scenes. Y Xiang, T Schmidt, V Narayanan, D Fox, Y. Xiang, T. Schmidt, V. Narayanan, and D. Fox, \"PoseCNN: A convolutional neural network for 6d object pose estimation in cluttered scenes,\" 2018.\n\nReal-time seamless single shot 6d object pose prediction. B Tekin, S N Sinha, P Fua, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionB. Tekin, S. N. Sinha, and P. Fua, \"Real-time seamless single shot 6d object pose prediction,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 292-301.\n\nPVNet: Pixel-wise voting network for 6dof pose estimation. S Peng, Y Liu, Q Huang, X Zhou, H Bao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionS. Peng, Y. Liu, Q. Huang, X. Zhou, and H. Bao, \"PVNet: Pixel-wise voting network for 6dof pose esti- mation,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4561-4570.\n\nDensefusion: 6d object pose estimation by iterative dense fusion. C Wang, D Xu, Y Zhu, R Mart\u00edn-Mart\u00edn, C Lu, L Fei-Fei, S Savarese, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionC. Wang, D. Xu, Y. Zhu, R. Mart\u00edn-Mart\u00edn, C. Lu, L. Fei-Fei, and S. Savarese, \"Densefusion: 6d object pose estimation by iterative dense fusion,\" in Proceed- ings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 3343-3352.\n\nG Gkioxari, J Malik, J Johnson ; R-Cnn, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionG. Gkioxari, J. Malik, and J. Johnson, \"Mesh R- CNN,\" in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 9785-9795.\n\nNeural 3D mesh renderer. H Kato, Y Ushiku, T Harada, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionH. Kato, Y. Ushiku, and T. Harada, \"Neural 3D mesh renderer,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 3907-3916.\n\nShape prior deformation for categorical 6d object pose and size estimation. M Tian, M H Ang, G H Lee, European Conference on Computer Vision. SpringerM. Tian, M. H. Ang, and G. H. Lee, \"Shape prior deformation for categorical 6d object pose and size esti- mation,\" in European Conference on Computer Vision. Springer, 2020, pp. 530-546.\n\nNormalized object coordinate space for category-level 6d object pose and size estimation. H Wang, S Sridhar, J Huang, J Valentin, S Song, L J Guibas, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionH. Wang, S. Sridhar, J. Huang, J. Valentin, S. Song, and L. J. Guibas, \"Normalized object coordinate space for category-level 6d object pose and size estimation,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 2642-2651.\n\nAugmented autoencoders: Implicit 3D orientation learning for 6d object detection. M Sundermeyer, Z.-C Marton, M Durner, R Triebel, International Journal of Computer Vision. 1283M. Sundermeyer, Z.-C. Marton, M. Durner, and R. Triebel, \"Augmented autoencoders: Implicit 3D ori- entation learning for 6d object detection,\" International Journal of Computer Vision, vol. 128, no. 3, pp. 714- 729, 2020.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, Computer Vision and Pattern Recognition. R. Girshick, J. Donahue, T. Darrell, and J. Malik, \"Rich feature hierarchies for accurate object detection and semantic segmentation,\" in Computer Vision and Pattern Recognition, 2014.\n\nMask R-CNN. K He, G Gkioxari, P Doll\u00e1r, R Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionK. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \"Mask R-CNN,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2961-2969.\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in neural information processing systems. 28S. Ren, K. He, R. Girshick, and J. Sun, \"Faster R-CNN: Towards real-time object detection with region proposal networks,\" Advances in neural information processing systems, vol. 28, pp. 91-99, 2015.\n\nSingle-stage multi-person pose machines. X Nie, J Feng, J Zhang, S Yan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionX. Nie, J. Feng, J. Zhang, and S. Yan, \"Single-stage multi-person pose machines,\" in Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, 2019, pp. 6951-6960.\n\nObjects as points. X Zhou, D Wang, P Kr\u00e4henb\u00fchl, arXiv:1904.07850arXiv preprintX. Zhou, D. Wang, and P. Kr\u00e4henb\u00fchl, \"Objects as points,\" arXiv preprint arXiv:1904.07850, 2019.\n\nTracking objects as points. X Zhou, V Koltun, P Kr\u00e4henb\u00fchl, European Conference on Computer Vision. SpringerX. Zhou, V. Koltun, and P. Kr\u00e4henb\u00fchl, \"Tracking ob- jects as points,\" in European Conference on Computer Vision. Springer, 2020, pp. 474-490.\n\nCenterNet: Keypoint triplets for object detection. K Duan, S Bai, L Xie, H Qi, Q Huang, Q Tian, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionK. Duan, S. Bai, L. Xie, H. Qi, Q. Huang, and Q. Tian, \"CenterNet: Keypoint triplets for object detection,\" in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 6569-6578.\n\nA point set generation network for 3d object reconstruction from a single image. H Fan, H Su, L J Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionH. Fan, H. Su, and L. J. Guibas, \"A point set generation network for 3d object reconstruction from a single image,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 605-613.\n\nDeepsdf: Learning continuous signed distance functions for shape representation. J J Park, P Florence, J Straub, R Newcombe, S Lovegrove, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJ. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, \"Deepsdf: Learning continuous signed distance functions for shape representation,\" in Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 165-174.\n\n3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. C B Choy, D Xu, J Gwak, K Chen, S Savarese, SpringerC. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese, \"3d-r2n2: A unified approach for single and multi-view 3d object reconstruction,\" in European conference on computer vision. Springer, 2016, pp. 628-644.\n\nAtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation. T Groueix, M Fisher, V G Kim, B Russell, M Aubry, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)T. Groueix, M. Fisher, V. G. Kim, B. Russell, and M. Aubry, \"AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation,\" in Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nLearning implicit fields for generative shape modeling. Z Chen, H Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZ. Chen and H. Zhang, \"Learning implicit fields for generative shape modeling,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 5939-5948.\n\nDense 3d object reconstruction from a single depth view. B Yang, S Rosa, A Markham, N Trigoni, H Wen, TPAMI. B. Yang, S. Rosa, A. Markham, N. Trigoni, and H. Wen, \"Dense 3d object reconstruction from a single depth view,\" in TPAMI, 2018.\n\nShape completion enabled robotic grasping. J Varley, C Dechant, A Richardson, J Ruales, P Allen, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEEJ. Varley, C. DeChant, A. Richardson, J. Ruales, and P. Allen, \"Shape completion enabled robotic grasping,\" in 2017 IEEE/RSJ international conference on intel- ligent robots and systems (IROS). IEEE, 2017, pp. 2442-2447.\n\nPCN: Point completion network. W Yuan, T Khot, D Held, C Mertz, M Hebert, 3D Vision (3DV). International Conference onW. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, \"PCN: Point completion network,\" in 3D Vision (3DV), 2018 International Conference on, 2018.\n\nFrom points to multi-object 3d reconstruction. F Engelmann, K Rematas, B Leibe, V Ferrari, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionF. Engelmann, K. Rematas, B. Leibe, and V. Ferrari, \"From points to multi-object 3d reconstruction,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 4588-4597.\n\nFrodo: From detections to 3d objects. M Runz, K Li, M Tang, L Ma, C Kong, T Schmidt, I Reid, L Agapito, J Straub, S Lovegrove, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionM. Runz, K. Li, M. Tang, L. Ma, C. Kong, T. Schmidt, I. Reid, L. Agapito, J. Straub, S. Lovegrove et al., \"Frodo: From detections to 3d objects,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 14 720-14 729.\n\nDeep learning of local rgb-d patches for 3d object detection and 6d pose estimation. W Kehl, F Milletari, F Tombari, S Ilic, N Navab, Springerin European conference on computer visionW. Kehl, F. Milletari, F. Tombari, S. Ilic, and N. Navab, \"Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation,\" in European confer- ence on computer vision. Springer, 2016, pp. 205-220.\n\nImplicit 3d orientation learning for 6d object detection from rgb images. M Sundermeyer, Z.-C Marton, M Durner, M Brucker, R Triebel, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)M. Sundermeyer, Z.-C. Marton, M. Durner, M. Brucker, and R. Triebel, \"Implicit 3d orientation learning for 6d object detection from rgb images,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 699-715.\n\nLatent-class hough forests for 3d object detection and pose estimation. A Tejani, D Tang, R Kouskouridas, T.-K Kim, European Conference on Computer Vision. SpringerA. Tejani, D. Tang, R. Kouskouridas, and T.-K. Kim, \"Latent-class hough forests for 3d object detection and pose estimation,\" in European Conference on Computer Vision. Springer, 2014, pp. 462-477.\n\nLearning canonical shape space for category-level 6d object pose and size estimation. D Chen, J Li, Z Wang, K Xu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionD. Chen, J. Li, Z. Wang, and K. Xu, \"Learning canonical shape space for category-level 6d object pose and size estimation,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 973-11 982.\n\nCentermask: single shot instance segmentation with point representation. Y Wang, Z Xu, H Shen, B Cheng, L Yang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionY. Wang, Z. Xu, H. Shen, B. Cheng, and L. Yang, \"Centermask: single shot instance segmentation with point representation,\" in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 9313-9321.\n\nConditional convolutions for instance segmentation. Z Tian, C Shen, H Chen, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerProceedings, Part I 16Z. Tian, C. Shen, and H. Chen, \"Conditional convolu- tions for instance segmentation,\" in Computer Vision- ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I 16. Springer, 2020, pp. 282-298.\n\nSolo: Segmenting objects by locations. X Wang, T Kong, C Shen, Y Jiang, L Li, European Conference on Computer Vision. SpringerX. Wang, T. Kong, C. Shen, Y. Jiang, and L. Li, \"Solo: Segmenting objects by locations,\" in European Conference on Computer Vision. Springer, 2020, pp. 649-665.\n\nCenterhmr: a bottom-up single-shot method for multi-person 3d mesh recovery from a single image. Y Sun, Q Bao, W Liu, Y Fu, T Mei, 2008arXiv e-printsY. Sun, Q. Bao, W. Liu, Y. Fu, and T. Mei, \"Centerhmr: a bottom-up single-shot method for multi-person 3d mesh recovery from a single image,\" arXiv e-prints, pp. arXiv-2008, 2020.\n\nMonocular, one-stage, regression of multiple 3d people. Y Sun, Q Bao, W Liu, Y Fu, B Michael, J , T Mei, ICCV. Y. Sun, Q. Bao, W. Liu, Y. Fu, B. Michael J., and T. Mei, \"Monocular, one-stage, regression of multiple 3d people,\" in ICCV, October 2021.\n\nFeature pyramid networks for object detection. T.-Y Lin, P Doll\u00e1r, R Girshick, K He, B Hariharan, S Belongie, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionT.-Y. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, and S. Belongie, \"Feature pyramid networks for object detection,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2117-2125.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.\n\nPanoptic feature pyramid networks. A Kirillov, R Girshick, K He, P Doll\u00e1r, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionA. Kirillov, R. Girshick, K. He, and P. Doll\u00e1r, \"Panop- tic feature pyramid networks,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 6399-6408.\n\nCornerNet: Detecting objects as paired keypoints. H Law, J Deng, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)H. Law and J. Deng, \"CornerNet: Detecting objects as paired keypoints,\" in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 734- 750.\n\nPointNet: Deep learning on point sets for 3d classification and segmentation. C R Qi, H Su, K Mo, L J Guibas, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. R. Qi, H. Su, K. Mo, and L. J. Guibas, \"PointNet: Deep learning on point sets for 3d classification and segmentation,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 652-660.\n\nShapeNet: An information-rich 3d model repository. A X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, arXiv:1512.03012arXiv preprintA. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su et al., \"ShapeNet: An information-rich 3d model repository,\" arXiv preprint arXiv:1512.03012, 2015.\n\nVisualizing data using t-SNE. L Van Der Maaten, G Hinton, Journal of machine learning research. 911L. Van der Maaten and G. Hinton, \"Visualizing data using t-SNE,\" Journal of machine learning research, vol. 9, no. 11, 2008.\n\nOn the continuity of rotation representations in neural networks. Y Zhou, C Barnes, J Lu, J Yang, H Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionY. Zhou, C. Barnes, J. Lu, J. Yang, and H. Li, \"On the continuity of rotation representations in neural networks,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 5745-5753.\n\nOn object symmetries and 6d pose estimation from images. G Pitteri, M Ramamonjisoa, S Ilic, V Lepetit, 2019 International Conference on 3D Vision (3DV). IEEEG. Pitteri, M. Ramamonjisoa, S. Ilic, and V. Lepetit, \"On object symmetries and 6d pose estimation from images,\" in 2019 International Conference on 3D Vision (3DV). IEEE, 2019, pp. 614-622.\n\nCategory level object pose estimation via neural analysis-by-synthesis. X Chen, Z Dong, J Song, A Geiger, O Hilliges, European Conference on Computer Vision. SpringerX. Chen, Z. Dong, J. Song, A. Geiger, and O. Hilliges, \"Category level object pose estimation via neural analysis-by-synthesis,\" in European Conference on Computer Vision. Springer, 2020, pp. 139-156.\n\nCategorylevel metric scale object shape and pose estimation. T Lee, B.-U Lee, M Kim, I S Kweon, IEEE Robotics and Automation Letters. 64T. Lee, B.-U. Lee, M. Kim, and I. S. Kweon, \"Category- level metric scale object shape and pose estimation,\" IEEE Robotics and Automation Letters, vol. 6, no. 4, pp. 8575-8582, 2021.\n\nPyrender. M , M. Matl, \"Pyrender,\" https://github.com/mmatl/ pyrender, 2019.\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, Inc32A. Paszke, S. Gross, F. Massa, A. Lerer, J. Brad- bury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, \"Pytorch: An imper- ative style, high-performance deep learning library,\" in Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9- Buc, E. Fox, and R. Garnett, Eds. Curran Associates, Inc., 2019, pp. 8024-8035.\n\nGeneralized-ICP. A Segal, D Haehnel, S Thrun, Robotics: science and systems. Seattle, WA2435A. Segal, D. Haehnel, and S. Thrun, \"Generalized-ICP,\" in Robotics: science and systems, vol. 2, no. 4. Seattle, WA, 2009, p. 435.\n\nOpen3D: A modern library for 3D data processing. Q.-Y Zhou, J Park, V Koltun, arXiv:1801.09847Q.-Y. Zhou, J. Park, and V. Koltun, \"Open3D: A mod- ern library for 3D data processing,\" arXiv:1801.09847, 2018.\n\n6-pack: Categorylevel 6d pose tracker with anchor-based keypoints. C Wang, R Mart\u00edn-Mart\u00edn, D Xu, J Lv, C Lu, L Fei-Fei, S Savarese, Y Zhu, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE10C. Wang, R. Mart\u00edn-Mart\u00edn, D. Xu, J. Lv, C. Lu, L. Fei-Fei, S. Savarese, and Y. Zhu, \"6-pack: Category- level 6d pose tracker with anchor-based keypoints,\" in 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020, pp. 10 059-10 066.\n\nBundletrack: 6d pose tracking for novel objects without instance or category-level 3d models. B Wen, K E Bekris, IEEE/RSJ International Conference on Intelligent Robots and Systems. B. Wen and K. E. Bekris, \"Bundletrack: 6d pose track- ing for novel objects without instance or category-level 3d models,\" in IEEE/RSJ International Conference on Intelligent Robots and Systems, 2021.\n\nFoldingNet: Point cloud auto-encoder via deep grid deformation. Y Yang, C Feng, Y Shen, D Tian, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Yang, C. Feng, Y. Shen, and D. Tian, \"FoldingNet: Point cloud auto-encoder via deep grid deformation,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 206-215.\n", "annotations": {"author": "[{\"end\":131,\"start\":108},{\"end\":146,\"start\":132},{\"end\":162,\"start\":147},{\"end\":175,\"start\":163},{\"end\":187,\"start\":176}]", "publisher": null, "author_last_name": "[{\"end\":130,\"start\":117},{\"end\":145,\"start\":139},{\"end\":161,\"start\":155},{\"end\":174,\"start\":169},{\"end\":186,\"start\":182}]", "author_first_name": "[{\"end\":116,\"start\":108},{\"end\":138,\"start\":132},{\"end\":154,\"start\":147},{\"end\":168,\"start\":163},{\"end\":181,\"start\":176}]", "author_affiliation": null, "title": "[{\"end\":105,\"start\":1},{\"end\":292,\"start\":188}]", "venue": null, "abstract": "[{\"end\":1716,\"start\":294}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1908,\"start\":1905},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1910,\"start\":1908},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1912,\"start\":1910},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1928,\"start\":1925},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1930,\"start\":1928},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1958,\"start\":1955},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1960,\"start\":1958},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2048,\"start\":2045},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2105,\"start\":2102},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2108,\"start\":2105},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2111,\"start\":2108},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2206,\"start\":2203},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2209,\"start\":2206},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2313,\"start\":2309},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2316,\"start\":2313},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2319,\"start\":2316},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2426,\"start\":2422},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2429,\"start\":2426},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2432,\"start\":2429},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2849,\"start\":2845},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2852,\"start\":2849},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2904,\"start\":2900},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2907,\"start\":2904},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2910,\"start\":2907},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3054,\"start\":3050},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3057,\"start\":3054},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3060,\"start\":3057},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3092,\"start\":3089},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4101,\"start\":4097},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4104,\"start\":4101},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4107,\"start\":4104},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4110,\"start\":4107},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4271,\"start\":4268},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4274,\"start\":4271},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4609,\"start\":4605},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4612,\"start\":4609},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5032,\"start\":5028},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5035,\"start\":5032},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5038,\"start\":5035},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5977,\"start\":5973},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5980,\"start\":5977},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5983,\"start\":5980},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6052,\"start\":6048},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6055,\"start\":6052},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6058,\"start\":6055},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6111,\"start\":6107},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6114,\"start\":6111},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6117,\"start\":6114},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6423,\"start\":6419},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6426,\"start\":6423},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6429,\"start\":6426},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6857,\"start\":6853},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6860,\"start\":6857},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6863,\"start\":6860},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6892,\"start\":6888},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6895,\"start\":6892},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6898,\"start\":6895},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6928,\"start\":6924},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6931,\"start\":6928},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7140,\"start\":7136},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7143,\"start\":7140},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7146,\"start\":7143},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7514,\"start\":7510},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7517,\"start\":7514},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7520,\"start\":7517},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7653,\"start\":7649},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7672,\"start\":7668},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7675,\"start\":7672},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7702,\"start\":7698},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7705,\"start\":7702},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7873,\"start\":7869},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7876,\"start\":7873},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9583,\"start\":9580},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9586,\"start\":9583},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10117,\"start\":10113},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10120,\"start\":10117},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10307,\"start\":10303},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10451,\"start\":10447},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12039,\"start\":12036},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12042,\"start\":12039},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12045,\"start\":12042},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12048,\"start\":12045},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":13027,\"start\":13023},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13319,\"start\":13315},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13807,\"start\":13803},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":15195,\"start\":15191},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":15441,\"start\":15437},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16823,\"start\":16820},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17286,\"start\":17282},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17584,\"start\":17580},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18496,\"start\":18492},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19169,\"start\":19166},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":19267,\"start\":19263},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19303,\"start\":19300},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":19384,\"start\":19380},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19400,\"start\":19396},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19691,\"start\":19687},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20104,\"start\":20100},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":20867,\"start\":20863},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":20870,\"start\":20867},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22539,\"start\":22535},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22730,\"start\":22726},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":22751,\"start\":22747},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23662,\"start\":23658},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23665,\"start\":23662},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27585,\"start\":27581}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26345,\"start\":26282},{\"attributes\":{\"id\":\"fig_1\"},\"end\":26602,\"start\":26346},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26731,\"start\":26603},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27048,\"start\":26732},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":27491,\"start\":27049},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27620,\"start\":27492},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29887,\"start\":27621},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30753,\"start\":29888}]", "paragraph": "[{\"end\":2806,\"start\":1735},{\"end\":3226,\"start\":2808},{\"end\":3696,\"start\":3228},{\"end\":5039,\"start\":3698},{\"end\":5261,\"start\":5041},{\"end\":5775,\"start\":5263},{\"end\":6697,\"start\":5777},{\"end\":8201,\"start\":6699},{\"end\":9984,\"start\":8289},{\"end\":11692,\"start\":10025},{\"end\":11814,\"start\":11694},{\"end\":12049,\"start\":11908},{\"end\":13246,\"start\":12084},{\"end\":13671,\"start\":13248},{\"end\":14337,\"start\":13759},{\"end\":15274,\"start\":14361},{\"end\":16084,\"start\":15328},{\"end\":16824,\"start\":16172},{\"end\":17073,\"start\":16872},{\"end\":17219,\"start\":17115},{\"end\":18073,\"start\":17221},{\"end\":18460,\"start\":18140},{\"end\":20966,\"start\":18462},{\"end\":22022,\"start\":20968},{\"end\":23723,\"start\":22024},{\"end\":24294,\"start\":23725},{\"end\":25270,\"start\":24296},{\"end\":25764,\"start\":25272},{\"end\":26281,\"start\":25782}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11907,\"start\":11815},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13758,\"start\":13672},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15327,\"start\":15275},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16171,\"start\":16085},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17114,\"start\":17074}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21069,\"start\":21062},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21358,\"start\":21351},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22243,\"start\":22235},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23921,\"start\":23912},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24538,\"start\":24528},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25175,\"start\":25166}]", "section_header": "[{\"end\":1733,\"start\":1718},{\"end\":8287,\"start\":8204},{\"end\":10023,\"start\":9987},{\"end\":12082,\"start\":12052},{\"end\":14359,\"start\":14340},{\"end\":16870,\"start\":16827},{\"end\":18091,\"start\":18076},{\"end\":18110,\"start\":18094},{\"end\":18138,\"start\":18113},{\"end\":25780,\"start\":25767},{\"end\":26291,\"start\":26283},{\"end\":26355,\"start\":26347},{\"end\":26612,\"start\":26604},{\"end\":26749,\"start\":26733},{\"end\":27057,\"start\":27050},{\"end\":27502,\"start\":27493},{\"end\":27632,\"start\":27622},{\"end\":29900,\"start\":29889}]", "table": "[{\"end\":27491,\"start\":27074},{\"end\":29887,\"start\":27647},{\"end\":30753,\"start\":30346}]", "figure_caption": "[{\"end\":26345,\"start\":26293},{\"end\":26602,\"start\":26357},{\"end\":26731,\"start\":26614},{\"end\":27048,\"start\":26752},{\"end\":27074,\"start\":27059},{\"end\":27620,\"start\":27504},{\"end\":27647,\"start\":27635},{\"end\":30346,\"start\":29904}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2966,\"start\":2958},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4132,\"start\":4124},{\"end\":9418,\"start\":9410},{\"end\":11103,\"start\":11097},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12966,\"start\":12956},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13862,\"start\":13854},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13962,\"start\":13954},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14152,\"start\":14143},{\"end\":17407,\"start\":17399},{\"end\":18057,\"start\":18049},{\"end\":21082,\"start\":21074},{\"end\":22814,\"start\":22806},{\"end\":25422,\"start\":25414}]", "bib_author_first_name": "[{\"end\":30825,\"start\":30824},{\"end\":30827,\"start\":30826},{\"end\":30840,\"start\":30839},{\"end\":30849,\"start\":30848},{\"end\":30861,\"start\":30860},{\"end\":30871,\"start\":30870},{\"end\":31222,\"start\":31221},{\"end\":31231,\"start\":31230},{\"end\":31238,\"start\":31237},{\"end\":31249,\"start\":31248},{\"end\":31257,\"start\":31256},{\"end\":31570,\"start\":31569},{\"end\":31580,\"start\":31579},{\"end\":31595,\"start\":31594},{\"end\":31604,\"start\":31603},{\"end\":31614,\"start\":31613},{\"end\":31937,\"start\":31936},{\"end\":31939,\"start\":31938},{\"end\":31945,\"start\":31944},{\"end\":31952,\"start\":31951},{\"end\":31958,\"start\":31957},{\"end\":31964,\"start\":31963},{\"end\":31966,\"start\":31965},{\"end\":32388,\"start\":32387},{\"end\":32396,\"start\":32395},{\"end\":32405,\"start\":32404},{\"end\":32412,\"start\":32411},{\"end\":32414,\"start\":32413},{\"end\":32428,\"start\":32427},{\"end\":32434,\"start\":32433},{\"end\":32444,\"start\":32443},{\"end\":32818,\"start\":32817},{\"end\":32827,\"start\":32826},{\"end\":32834,\"start\":32833},{\"end\":32843,\"start\":32842},{\"end\":32851,\"start\":32850},{\"end\":32864,\"start\":32863},{\"end\":33386,\"start\":33385},{\"end\":33393,\"start\":33392},{\"end\":33400,\"start\":33399},{\"end\":33407,\"start\":33406},{\"end\":33416,\"start\":33415},{\"end\":33425,\"start\":33424},{\"end\":33427,\"start\":33426},{\"end\":33822,\"start\":33821},{\"end\":33833,\"start\":33832},{\"end\":33842,\"start\":33841},{\"end\":33851,\"start\":33850},{\"end\":33864,\"start\":33863},{\"end\":33866,\"start\":33865},{\"end\":33879,\"start\":33878},{\"end\":33891,\"start\":33890},{\"end\":33901,\"start\":33900},{\"end\":33911,\"start\":33910},{\"end\":33922,\"start\":33921},{\"end\":34294,\"start\":34293},{\"end\":34301,\"start\":34300},{\"end\":34317,\"start\":34312},{\"end\":34324,\"start\":34323},{\"end\":34786,\"start\":34785},{\"end\":34798,\"start\":34797},{\"end\":34811,\"start\":34810},{\"end\":34822,\"start\":34821},{\"end\":35314,\"start\":35313},{\"end\":35327,\"start\":35326},{\"end\":35338,\"start\":35337},{\"end\":35350,\"start\":35349},{\"end\":35361,\"start\":35360},{\"end\":35787,\"start\":35786},{\"end\":35798,\"start\":35797},{\"end\":35800,\"start\":35799},{\"end\":35983,\"start\":35982},{\"end\":35991,\"start\":35990},{\"end\":36003,\"start\":36002},{\"end\":36014,\"start\":36013},{\"end\":36022,\"start\":36021},{\"end\":36513,\"start\":36512},{\"end\":36520,\"start\":36519},{\"end\":36996,\"start\":36995},{\"end\":37005,\"start\":37004},{\"end\":37016,\"start\":37015},{\"end\":37029,\"start\":37028},{\"end\":37242,\"start\":37241},{\"end\":37251,\"start\":37250},{\"end\":37253,\"start\":37252},{\"end\":37262,\"start\":37261},{\"end\":37666,\"start\":37665},{\"end\":37674,\"start\":37673},{\"end\":37681,\"start\":37680},{\"end\":37690,\"start\":37689},{\"end\":37698,\"start\":37697},{\"end\":38139,\"start\":38138},{\"end\":38147,\"start\":38146},{\"end\":38153,\"start\":38152},{\"end\":38160,\"start\":38159},{\"end\":38177,\"start\":38176},{\"end\":38183,\"start\":38182},{\"end\":38194,\"start\":38193},{\"end\":38611,\"start\":38610},{\"end\":38623,\"start\":38622},{\"end\":38632,\"start\":38631},{\"end\":38957,\"start\":38956},{\"end\":38965,\"start\":38964},{\"end\":38975,\"start\":38974},{\"end\":39368,\"start\":39367},{\"end\":39376,\"start\":39375},{\"end\":39378,\"start\":39377},{\"end\":39385,\"start\":39384},{\"end\":39387,\"start\":39386},{\"end\":39720,\"start\":39719},{\"end\":39728,\"start\":39727},{\"end\":39739,\"start\":39738},{\"end\":39748,\"start\":39747},{\"end\":39760,\"start\":39759},{\"end\":39768,\"start\":39767},{\"end\":39770,\"start\":39769},{\"end\":40282,\"start\":40281},{\"end\":40300,\"start\":40296},{\"end\":40310,\"start\":40309},{\"end\":40320,\"start\":40319},{\"end\":40682,\"start\":40681},{\"end\":40694,\"start\":40693},{\"end\":40705,\"start\":40704},{\"end\":40716,\"start\":40715},{\"end\":40964,\"start\":40963},{\"end\":40970,\"start\":40969},{\"end\":40982,\"start\":40981},{\"end\":40992,\"start\":40991},{\"end\":41361,\"start\":41360},{\"end\":41368,\"start\":41367},{\"end\":41374,\"start\":41373},{\"end\":41386,\"start\":41385},{\"end\":41687,\"start\":41686},{\"end\":41694,\"start\":41693},{\"end\":41702,\"start\":41701},{\"end\":41711,\"start\":41710},{\"end\":42048,\"start\":42047},{\"end\":42056,\"start\":42055},{\"end\":42064,\"start\":42063},{\"end\":42234,\"start\":42233},{\"end\":42242,\"start\":42241},{\"end\":42252,\"start\":42251},{\"end\":42509,\"start\":42508},{\"end\":42517,\"start\":42516},{\"end\":42524,\"start\":42523},{\"end\":42531,\"start\":42530},{\"end\":42537,\"start\":42536},{\"end\":42546,\"start\":42545},{\"end\":42970,\"start\":42969},{\"end\":42977,\"start\":42976},{\"end\":42983,\"start\":42982},{\"end\":42985,\"start\":42984},{\"end\":43435,\"start\":43434},{\"end\":43437,\"start\":43436},{\"end\":43445,\"start\":43444},{\"end\":43457,\"start\":43456},{\"end\":43467,\"start\":43466},{\"end\":43479,\"start\":43478},{\"end\":43979,\"start\":43978},{\"end\":43981,\"start\":43980},{\"end\":43989,\"start\":43988},{\"end\":43995,\"start\":43994},{\"end\":44003,\"start\":44002},{\"end\":44011,\"start\":44010},{\"end\":44309,\"start\":44308},{\"end\":44320,\"start\":44319},{\"end\":44330,\"start\":44329},{\"end\":44332,\"start\":44331},{\"end\":44339,\"start\":44338},{\"end\":44350,\"start\":44349},{\"end\":44764,\"start\":44763},{\"end\":44772,\"start\":44771},{\"end\":45175,\"start\":45174},{\"end\":45183,\"start\":45182},{\"end\":45191,\"start\":45190},{\"end\":45202,\"start\":45201},{\"end\":45213,\"start\":45212},{\"end\":45400,\"start\":45399},{\"end\":45410,\"start\":45409},{\"end\":45421,\"start\":45420},{\"end\":45435,\"start\":45434},{\"end\":45445,\"start\":45444},{\"end\":45792,\"start\":45791},{\"end\":45800,\"start\":45799},{\"end\":45808,\"start\":45807},{\"end\":45816,\"start\":45815},{\"end\":45825,\"start\":45824},{\"end\":46072,\"start\":46071},{\"end\":46085,\"start\":46084},{\"end\":46096,\"start\":46095},{\"end\":46105,\"start\":46104},{\"end\":46512,\"start\":46511},{\"end\":46520,\"start\":46519},{\"end\":46526,\"start\":46525},{\"end\":46534,\"start\":46533},{\"end\":46540,\"start\":46539},{\"end\":46548,\"start\":46547},{\"end\":46559,\"start\":46558},{\"end\":46567,\"start\":46566},{\"end\":46578,\"start\":46577},{\"end\":46588,\"start\":46587},{\"end\":47093,\"start\":47092},{\"end\":47101,\"start\":47100},{\"end\":47114,\"start\":47113},{\"end\":47125,\"start\":47124},{\"end\":47133,\"start\":47132},{\"end\":47485,\"start\":47484},{\"end\":47503,\"start\":47499},{\"end\":47513,\"start\":47512},{\"end\":47523,\"start\":47522},{\"end\":47534,\"start\":47533},{\"end\":47966,\"start\":47965},{\"end\":47976,\"start\":47975},{\"end\":47984,\"start\":47983},{\"end\":48003,\"start\":47999},{\"end\":48343,\"start\":48342},{\"end\":48351,\"start\":48350},{\"end\":48357,\"start\":48356},{\"end\":48365,\"start\":48364},{\"end\":48829,\"start\":48828},{\"end\":48837,\"start\":48836},{\"end\":48843,\"start\":48842},{\"end\":48851,\"start\":48850},{\"end\":48860,\"start\":48859},{\"end\":49300,\"start\":49299},{\"end\":49308,\"start\":49307},{\"end\":49316,\"start\":49315},{\"end\":49688,\"start\":49687},{\"end\":49696,\"start\":49695},{\"end\":49704,\"start\":49703},{\"end\":49712,\"start\":49711},{\"end\":49721,\"start\":49720},{\"end\":50034,\"start\":50033},{\"end\":50041,\"start\":50040},{\"end\":50048,\"start\":50047},{\"end\":50055,\"start\":50054},{\"end\":50061,\"start\":50060},{\"end\":50323,\"start\":50322},{\"end\":50330,\"start\":50329},{\"end\":50337,\"start\":50336},{\"end\":50344,\"start\":50343},{\"end\":50350,\"start\":50349},{\"end\":50361,\"start\":50360},{\"end\":50365,\"start\":50364},{\"end\":50568,\"start\":50564},{\"end\":50575,\"start\":50574},{\"end\":50585,\"start\":50584},{\"end\":50597,\"start\":50596},{\"end\":50603,\"start\":50602},{\"end\":50616,\"start\":50615},{\"end\":51041,\"start\":51040},{\"end\":51047,\"start\":51046},{\"end\":51056,\"start\":51055},{\"end\":51063,\"start\":51062},{\"end\":51433,\"start\":51432},{\"end\":51445,\"start\":51444},{\"end\":51457,\"start\":51456},{\"end\":51463,\"start\":51462},{\"end\":51867,\"start\":51866},{\"end\":51874,\"start\":51873},{\"end\":52237,\"start\":52236},{\"end\":52239,\"start\":52238},{\"end\":52245,\"start\":52244},{\"end\":52251,\"start\":52250},{\"end\":52257,\"start\":52256},{\"end\":52259,\"start\":52258},{\"end\":52685,\"start\":52684},{\"end\":52687,\"start\":52686},{\"end\":52696,\"start\":52695},{\"end\":52710,\"start\":52709},{\"end\":52720,\"start\":52719},{\"end\":52732,\"start\":52731},{\"end\":52741,\"start\":52740},{\"end\":52747,\"start\":52746},{\"end\":52759,\"start\":52758},{\"end\":52768,\"start\":52767},{\"end\":52776,\"start\":52775},{\"end\":53050,\"start\":53049},{\"end\":53068,\"start\":53067},{\"end\":53311,\"start\":53310},{\"end\":53319,\"start\":53318},{\"end\":53329,\"start\":53328},{\"end\":53335,\"start\":53334},{\"end\":53343,\"start\":53342},{\"end\":53778,\"start\":53777},{\"end\":53789,\"start\":53788},{\"end\":53805,\"start\":53804},{\"end\":53813,\"start\":53812},{\"end\":54142,\"start\":54141},{\"end\":54150,\"start\":54149},{\"end\":54158,\"start\":54157},{\"end\":54166,\"start\":54165},{\"end\":54176,\"start\":54175},{\"end\":54499,\"start\":54498},{\"end\":54509,\"start\":54505},{\"end\":54516,\"start\":54515},{\"end\":54523,\"start\":54522},{\"end\":54525,\"start\":54524},{\"end\":54768,\"start\":54767},{\"end\":54906,\"start\":54905},{\"end\":54916,\"start\":54915},{\"end\":54925,\"start\":54924},{\"end\":54934,\"start\":54933},{\"end\":54943,\"start\":54942},{\"end\":54955,\"start\":54954},{\"end\":54965,\"start\":54964},{\"end\":54976,\"start\":54975},{\"end\":54983,\"start\":54982},{\"end\":54997,\"start\":54996},{\"end\":55007,\"start\":55006},{\"end\":55020,\"start\":55019},{\"end\":55028,\"start\":55027},{\"end\":55036,\"start\":55035},{\"end\":55046,\"start\":55045},{\"end\":55056,\"start\":55055},{\"end\":55066,\"start\":55065},{\"end\":55082,\"start\":55081},{\"end\":55093,\"start\":55092},{\"end\":55101,\"start\":55100},{\"end\":55108,\"start\":55107},{\"end\":55801,\"start\":55800},{\"end\":55810,\"start\":55809},{\"end\":55821,\"start\":55820},{\"end\":56060,\"start\":56056},{\"end\":56068,\"start\":56067},{\"end\":56076,\"start\":56075},{\"end\":56283,\"start\":56282},{\"end\":56291,\"start\":56290},{\"end\":56308,\"start\":56307},{\"end\":56314,\"start\":56313},{\"end\":56320,\"start\":56319},{\"end\":56326,\"start\":56325},{\"end\":56337,\"start\":56336},{\"end\":56349,\"start\":56348},{\"end\":56787,\"start\":56786},{\"end\":56794,\"start\":56793},{\"end\":56796,\"start\":56795},{\"end\":57141,\"start\":57140},{\"end\":57149,\"start\":57148},{\"end\":57157,\"start\":57156},{\"end\":57165,\"start\":57164}]", "bib_author_last_name": "[{\"end\":30837,\"start\":30828},{\"end\":30846,\"start\":30841},{\"end\":30858,\"start\":30850},{\"end\":30868,\"start\":30862},{\"end\":30876,\"start\":30872},{\"end\":31228,\"start\":31223},{\"end\":31235,\"start\":31232},{\"end\":31246,\"start\":31239},{\"end\":31254,\"start\":31250},{\"end\":31261,\"start\":31258},{\"end\":31577,\"start\":31571},{\"end\":31592,\"start\":31581},{\"end\":31601,\"start\":31596},{\"end\":31611,\"start\":31605},{\"end\":31624,\"start\":31615},{\"end\":31942,\"start\":31940},{\"end\":31949,\"start\":31946},{\"end\":31955,\"start\":31953},{\"end\":31961,\"start\":31959},{\"end\":31973,\"start\":31967},{\"end\":32393,\"start\":32389},{\"end\":32402,\"start\":32397},{\"end\":32409,\"start\":32406},{\"end\":32425,\"start\":32415},{\"end\":32431,\"start\":32429},{\"end\":32441,\"start\":32435},{\"end\":32452,\"start\":32445},{\"end\":32824,\"start\":32819},{\"end\":32831,\"start\":32828},{\"end\":32840,\"start\":32835},{\"end\":32848,\"start\":32844},{\"end\":32861,\"start\":32852},{\"end\":32868,\"start\":32865},{\"end\":33390,\"start\":33387},{\"end\":33397,\"start\":33394},{\"end\":33404,\"start\":33401},{\"end\":33413,\"start\":33408},{\"end\":33422,\"start\":33417},{\"end\":33433,\"start\":33428},{\"end\":33830,\"start\":33823},{\"end\":33839,\"start\":33834},{\"end\":33848,\"start\":33843},{\"end\":33861,\"start\":33852},{\"end\":33876,\"start\":33867},{\"end\":33888,\"start\":33880},{\"end\":33898,\"start\":33892},{\"end\":33908,\"start\":33902},{\"end\":33919,\"start\":33912},{\"end\":33927,\"start\":33923},{\"end\":34298,\"start\":34295},{\"end\":34310,\"start\":34302},{\"end\":34321,\"start\":34318},{\"end\":34328,\"start\":34325},{\"end\":34795,\"start\":34787},{\"end\":34808,\"start\":34799},{\"end\":34819,\"start\":34812},{\"end\":34829,\"start\":34823},{\"end\":35324,\"start\":35315},{\"end\":35335,\"start\":35328},{\"end\":35347,\"start\":35339},{\"end\":35358,\"start\":35351},{\"end\":35368,\"start\":35362},{\"end\":35795,\"start\":35788},{\"end\":35806,\"start\":35801},{\"end\":35988,\"start\":35984},{\"end\":36000,\"start\":35992},{\"end\":36011,\"start\":36004},{\"end\":36019,\"start\":36015},{\"end\":36028,\"start\":36023},{\"end\":36517,\"start\":36514},{\"end\":36528,\"start\":36521},{\"end\":37002,\"start\":36997},{\"end\":37013,\"start\":37006},{\"end\":37026,\"start\":37017},{\"end\":37033,\"start\":37030},{\"end\":37248,\"start\":37243},{\"end\":37259,\"start\":37254},{\"end\":37266,\"start\":37263},{\"end\":37671,\"start\":37667},{\"end\":37678,\"start\":37675},{\"end\":37687,\"start\":37682},{\"end\":37695,\"start\":37691},{\"end\":37702,\"start\":37699},{\"end\":38144,\"start\":38140},{\"end\":38150,\"start\":38148},{\"end\":38157,\"start\":38154},{\"end\":38174,\"start\":38161},{\"end\":38180,\"start\":38178},{\"end\":38191,\"start\":38184},{\"end\":38203,\"start\":38195},{\"end\":38620,\"start\":38612},{\"end\":38629,\"start\":38624},{\"end\":38648,\"start\":38633},{\"end\":38962,\"start\":38958},{\"end\":38972,\"start\":38966},{\"end\":38982,\"start\":38976},{\"end\":39373,\"start\":39369},{\"end\":39382,\"start\":39379},{\"end\":39391,\"start\":39388},{\"end\":39725,\"start\":39721},{\"end\":39736,\"start\":39729},{\"end\":39745,\"start\":39740},{\"end\":39757,\"start\":39749},{\"end\":39765,\"start\":39761},{\"end\":39777,\"start\":39771},{\"end\":40294,\"start\":40283},{\"end\":40307,\"start\":40301},{\"end\":40317,\"start\":40311},{\"end\":40328,\"start\":40321},{\"end\":40691,\"start\":40683},{\"end\":40702,\"start\":40695},{\"end\":40713,\"start\":40706},{\"end\":40722,\"start\":40717},{\"end\":40967,\"start\":40965},{\"end\":40979,\"start\":40971},{\"end\":40989,\"start\":40983},{\"end\":41001,\"start\":40993},{\"end\":41365,\"start\":41362},{\"end\":41371,\"start\":41369},{\"end\":41383,\"start\":41375},{\"end\":41390,\"start\":41387},{\"end\":41691,\"start\":41688},{\"end\":41699,\"start\":41695},{\"end\":41708,\"start\":41703},{\"end\":41715,\"start\":41712},{\"end\":42053,\"start\":42049},{\"end\":42061,\"start\":42057},{\"end\":42075,\"start\":42065},{\"end\":42239,\"start\":42235},{\"end\":42249,\"start\":42243},{\"end\":42263,\"start\":42253},{\"end\":42514,\"start\":42510},{\"end\":42521,\"start\":42518},{\"end\":42528,\"start\":42525},{\"end\":42534,\"start\":42532},{\"end\":42543,\"start\":42538},{\"end\":42551,\"start\":42547},{\"end\":42974,\"start\":42971},{\"end\":42980,\"start\":42978},{\"end\":42992,\"start\":42986},{\"end\":43442,\"start\":43438},{\"end\":43454,\"start\":43446},{\"end\":43464,\"start\":43458},{\"end\":43476,\"start\":43468},{\"end\":43489,\"start\":43480},{\"end\":43986,\"start\":43982},{\"end\":43992,\"start\":43990},{\"end\":44000,\"start\":43996},{\"end\":44008,\"start\":44004},{\"end\":44020,\"start\":44012},{\"end\":44317,\"start\":44310},{\"end\":44327,\"start\":44321},{\"end\":44336,\"start\":44333},{\"end\":44347,\"start\":44340},{\"end\":44356,\"start\":44351},{\"end\":44769,\"start\":44765},{\"end\":44778,\"start\":44773},{\"end\":45180,\"start\":45176},{\"end\":45188,\"start\":45184},{\"end\":45199,\"start\":45192},{\"end\":45210,\"start\":45203},{\"end\":45217,\"start\":45214},{\"end\":45407,\"start\":45401},{\"end\":45418,\"start\":45411},{\"end\":45432,\"start\":45422},{\"end\":45442,\"start\":45436},{\"end\":45451,\"start\":45446},{\"end\":45797,\"start\":45793},{\"end\":45805,\"start\":45801},{\"end\":45813,\"start\":45809},{\"end\":45822,\"start\":45817},{\"end\":45832,\"start\":45826},{\"end\":46082,\"start\":46073},{\"end\":46093,\"start\":46086},{\"end\":46102,\"start\":46097},{\"end\":46113,\"start\":46106},{\"end\":46517,\"start\":46513},{\"end\":46523,\"start\":46521},{\"end\":46531,\"start\":46527},{\"end\":46537,\"start\":46535},{\"end\":46545,\"start\":46541},{\"end\":46556,\"start\":46549},{\"end\":46564,\"start\":46560},{\"end\":46575,\"start\":46568},{\"end\":46585,\"start\":46579},{\"end\":46598,\"start\":46589},{\"end\":47098,\"start\":47094},{\"end\":47111,\"start\":47102},{\"end\":47122,\"start\":47115},{\"end\":47130,\"start\":47126},{\"end\":47139,\"start\":47134},{\"end\":47497,\"start\":47486},{\"end\":47510,\"start\":47504},{\"end\":47520,\"start\":47514},{\"end\":47531,\"start\":47524},{\"end\":47542,\"start\":47535},{\"end\":47973,\"start\":47967},{\"end\":47981,\"start\":47977},{\"end\":47997,\"start\":47985},{\"end\":48007,\"start\":48004},{\"end\":48348,\"start\":48344},{\"end\":48354,\"start\":48352},{\"end\":48362,\"start\":48358},{\"end\":48368,\"start\":48366},{\"end\":48834,\"start\":48830},{\"end\":48840,\"start\":48838},{\"end\":48848,\"start\":48844},{\"end\":48857,\"start\":48852},{\"end\":48865,\"start\":48861},{\"end\":49305,\"start\":49301},{\"end\":49313,\"start\":49309},{\"end\":49321,\"start\":49317},{\"end\":49693,\"start\":49689},{\"end\":49701,\"start\":49697},{\"end\":49709,\"start\":49705},{\"end\":49718,\"start\":49713},{\"end\":49724,\"start\":49722},{\"end\":50038,\"start\":50035},{\"end\":50045,\"start\":50042},{\"end\":50052,\"start\":50049},{\"end\":50058,\"start\":50056},{\"end\":50065,\"start\":50062},{\"end\":50327,\"start\":50324},{\"end\":50334,\"start\":50331},{\"end\":50341,\"start\":50338},{\"end\":50347,\"start\":50345},{\"end\":50358,\"start\":50351},{\"end\":50369,\"start\":50366},{\"end\":50572,\"start\":50569},{\"end\":50582,\"start\":50576},{\"end\":50594,\"start\":50586},{\"end\":50600,\"start\":50598},{\"end\":50613,\"start\":50604},{\"end\":50625,\"start\":50617},{\"end\":51044,\"start\":51042},{\"end\":51053,\"start\":51048},{\"end\":51060,\"start\":51057},{\"end\":51067,\"start\":51064},{\"end\":51442,\"start\":51434},{\"end\":51454,\"start\":51446},{\"end\":51460,\"start\":51458},{\"end\":51470,\"start\":51464},{\"end\":51871,\"start\":51868},{\"end\":51879,\"start\":51875},{\"end\":52242,\"start\":52240},{\"end\":52248,\"start\":52246},{\"end\":52254,\"start\":52252},{\"end\":52266,\"start\":52260},{\"end\":52693,\"start\":52688},{\"end\":52707,\"start\":52697},{\"end\":52717,\"start\":52711},{\"end\":52729,\"start\":52721},{\"end\":52738,\"start\":52733},{\"end\":52744,\"start\":52742},{\"end\":52756,\"start\":52748},{\"end\":52765,\"start\":52760},{\"end\":52773,\"start\":52769},{\"end\":52779,\"start\":52777},{\"end\":53065,\"start\":53051},{\"end\":53075,\"start\":53069},{\"end\":53316,\"start\":53312},{\"end\":53326,\"start\":53320},{\"end\":53332,\"start\":53330},{\"end\":53340,\"start\":53336},{\"end\":53346,\"start\":53344},{\"end\":53786,\"start\":53779},{\"end\":53802,\"start\":53790},{\"end\":53810,\"start\":53806},{\"end\":53821,\"start\":53814},{\"end\":54147,\"start\":54143},{\"end\":54155,\"start\":54151},{\"end\":54163,\"start\":54159},{\"end\":54173,\"start\":54167},{\"end\":54185,\"start\":54177},{\"end\":54503,\"start\":54500},{\"end\":54513,\"start\":54510},{\"end\":54520,\"start\":54517},{\"end\":54531,\"start\":54526},{\"end\":54913,\"start\":54907},{\"end\":54922,\"start\":54917},{\"end\":54931,\"start\":54926},{\"end\":54940,\"start\":54935},{\"end\":54952,\"start\":54944},{\"end\":54962,\"start\":54956},{\"end\":54973,\"start\":54966},{\"end\":54980,\"start\":54977},{\"end\":54994,\"start\":54984},{\"end\":55004,\"start\":54998},{\"end\":55017,\"start\":55008},{\"end\":55025,\"start\":55021},{\"end\":55033,\"start\":55029},{\"end\":55043,\"start\":55037},{\"end\":55053,\"start\":55047},{\"end\":55063,\"start\":55057},{\"end\":55079,\"start\":55067},{\"end\":55090,\"start\":55083},{\"end\":55098,\"start\":55094},{\"end\":55105,\"start\":55102},{\"end\":55117,\"start\":55109},{\"end\":55807,\"start\":55802},{\"end\":55818,\"start\":55811},{\"end\":55827,\"start\":55822},{\"end\":56065,\"start\":56061},{\"end\":56073,\"start\":56069},{\"end\":56083,\"start\":56077},{\"end\":56288,\"start\":56284},{\"end\":56305,\"start\":56292},{\"end\":56311,\"start\":56309},{\"end\":56317,\"start\":56315},{\"end\":56323,\"start\":56321},{\"end\":56334,\"start\":56327},{\"end\":56346,\"start\":56338},{\"end\":56353,\"start\":56350},{\"end\":56791,\"start\":56788},{\"end\":56803,\"start\":56797},{\"end\":57146,\"start\":57142},{\"end\":57154,\"start\":57150},{\"end\":57162,\"start\":57158},{\"end\":57170,\"start\":57166}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11893},\"end\":31124,\"start\":30755},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":233024930},\"end\":31478,\"start\":31126},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":235683469},\"end\":31875,\"start\":31480},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4868248},\"end\":32328,\"start\":31877},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10236420},\"end\":32733,\"start\":32330},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":232185507},\"end\":33272,\"start\":32735},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":211532831},\"end\":33764,\"start\":33274},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3933366},\"end\":34224,\"start\":33766},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":220793416},\"end\":34685,\"start\":34226},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":209376368},\"end\":35245,\"start\":34687},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":54465161},\"end\":35759,\"start\":35247},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":32592111},\"end\":35906,\"start\":35761},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10655945},\"end\":36376,\"start\":35908},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4392433},\"end\":36902,\"start\":36378},{\"attributes\":{\"id\":\"b14\"},\"end\":37181,\"start\":36904},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4929149},\"end\":37604,\"start\":37183},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":57189382},\"end\":38070,\"start\":37606},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":58006460},\"end\":38608,\"start\":38072},{\"attributes\":{\"id\":\"b18\"},\"end\":38929,\"start\":38610},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":32389979},\"end\":39289,\"start\":38931},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":220546332},\"end\":39627,\"start\":39291},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":57761160},\"end\":40197,\"start\":39629},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":197635980},\"end\":40597,\"start\":40199},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":215827080},\"end\":40949,\"start\":40599},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":54465873},\"end\":41278,\"start\":40951},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10328909},\"end\":41643,\"start\":41280},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":201668824},\"end\":42026,\"start\":41645},{\"attributes\":{\"doi\":\"arXiv:1904.07850\",\"id\":\"b27\"},\"end\":42203,\"start\":42028},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":214775104},\"end\":42455,\"start\":42205},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":119296375},\"end\":42886,\"start\":42457},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6746759},\"end\":43351,\"start\":42888},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":58007025},\"end\":43896,\"start\":43353},{\"attributes\":{\"id\":\"b32\"},\"end\":44237,\"start\":43898},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":208028203},\"end\":44705,\"start\":44239},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":54457478},\"end\":45115,\"start\":44707},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52099634},\"end\":45354,\"start\":45117},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6796508},\"end\":45758,\"start\":45356},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":51908879},\"end\":46022,\"start\":45760},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":229340505},\"end\":46471,\"start\":46024},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":218581750},\"end\":47005,\"start\":46473},{\"attributes\":{\"id\":\"b40\"},\"end\":47408,\"start\":47007},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":52953916},\"end\":47891,\"start\":47410},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4649936},\"end\":48254,\"start\":47893},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":210919925},\"end\":48753,\"start\":48256},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":215548952},\"end\":49245,\"start\":48755},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":212675968},\"end\":49646,\"start\":49247},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":209140680},\"end\":49934,\"start\":49648},{\"attributes\":{\"id\":\"b47\"},\"end\":50264,\"start\":49936},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":237532102},\"end\":50515,\"start\":50266},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":10716717},\"end\":50992,\"start\":50517},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":206594692},\"end\":51395,\"start\":50994},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":57721164},\"end\":51814,\"start\":51397},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":51923817},\"end\":52156,\"start\":51816},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":5115938},\"end\":52631,\"start\":52158},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b54\"},\"end\":53017,\"start\":52633},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":5855042},\"end\":53242,\"start\":53019},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":56178817},\"end\":53718,\"start\":53244},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":201124572},\"end\":54067,\"start\":53720},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":221068923},\"end\":54435,\"start\":54069},{\"attributes\":{\"id\":\"b59\"},\"end\":54755,\"start\":54437},{\"attributes\":{\"id\":\"b60\"},\"end\":54833,\"start\":54757},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":202786778},\"end\":55781,\"start\":54835},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":231748613},\"end\":56005,\"start\":55783},{\"attributes\":{\"doi\":\"arXiv:1801.09847\",\"id\":\"b63\"},\"end\":56213,\"start\":56007},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":204852023},\"end\":56690,\"start\":56215},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":236772287},\"end\":57074,\"start\":56692},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":8338993},\"end\":57519,\"start\":57076}]", "bib_title": "[{\"end\":30822,\"start\":30755},{\"end\":31219,\"start\":31126},{\"end\":31567,\"start\":31480},{\"end\":31934,\"start\":31877},{\"end\":32385,\"start\":32330},{\"end\":32815,\"start\":32735},{\"end\":33383,\"start\":33274},{\"end\":33819,\"start\":33766},{\"end\":34291,\"start\":34226},{\"end\":34783,\"start\":34687},{\"end\":35311,\"start\":35247},{\"end\":35784,\"start\":35761},{\"end\":35980,\"start\":35908},{\"end\":36510,\"start\":36378},{\"end\":37239,\"start\":37183},{\"end\":37663,\"start\":37606},{\"end\":38136,\"start\":38072},{\"end\":38954,\"start\":38931},{\"end\":39365,\"start\":39291},{\"end\":39717,\"start\":39629},{\"end\":40279,\"start\":40199},{\"end\":40679,\"start\":40599},{\"end\":40961,\"start\":40951},{\"end\":41358,\"start\":41280},{\"end\":41684,\"start\":41645},{\"end\":42231,\"start\":42205},{\"end\":42506,\"start\":42457},{\"end\":42967,\"start\":42888},{\"end\":43432,\"start\":43353},{\"end\":44306,\"start\":44239},{\"end\":44761,\"start\":44707},{\"end\":45172,\"start\":45117},{\"end\":45397,\"start\":45356},{\"end\":45789,\"start\":45760},{\"end\":46069,\"start\":46024},{\"end\":46509,\"start\":46473},{\"end\":47482,\"start\":47410},{\"end\":47963,\"start\":47893},{\"end\":48340,\"start\":48256},{\"end\":48826,\"start\":48755},{\"end\":49297,\"start\":49247},{\"end\":49685,\"start\":49648},{\"end\":50320,\"start\":50266},{\"end\":50562,\"start\":50517},{\"end\":51038,\"start\":50994},{\"end\":51430,\"start\":51397},{\"end\":51864,\"start\":51816},{\"end\":52234,\"start\":52158},{\"end\":53047,\"start\":53019},{\"end\":53308,\"start\":53244},{\"end\":53775,\"start\":53720},{\"end\":54139,\"start\":54069},{\"end\":54496,\"start\":54437},{\"end\":54903,\"start\":54835},{\"end\":55798,\"start\":55783},{\"end\":56280,\"start\":56215},{\"end\":56784,\"start\":56692},{\"end\":57138,\"start\":57076}]", "bib_author": "[{\"end\":30839,\"start\":30824},{\"end\":30848,\"start\":30839},{\"end\":30860,\"start\":30848},{\"end\":30870,\"start\":30860},{\"end\":30878,\"start\":30870},{\"end\":31230,\"start\":31221},{\"end\":31237,\"start\":31230},{\"end\":31248,\"start\":31237},{\"end\":31256,\"start\":31248},{\"end\":31263,\"start\":31256},{\"end\":31579,\"start\":31569},{\"end\":31594,\"start\":31579},{\"end\":31603,\"start\":31594},{\"end\":31613,\"start\":31603},{\"end\":31626,\"start\":31613},{\"end\":31944,\"start\":31936},{\"end\":31951,\"start\":31944},{\"end\":31957,\"start\":31951},{\"end\":31963,\"start\":31957},{\"end\":31975,\"start\":31963},{\"end\":32395,\"start\":32387},{\"end\":32404,\"start\":32395},{\"end\":32411,\"start\":32404},{\"end\":32427,\"start\":32411},{\"end\":32433,\"start\":32427},{\"end\":32443,\"start\":32433},{\"end\":32454,\"start\":32443},{\"end\":32826,\"start\":32817},{\"end\":32833,\"start\":32826},{\"end\":32842,\"start\":32833},{\"end\":32850,\"start\":32842},{\"end\":32863,\"start\":32850},{\"end\":32870,\"start\":32863},{\"end\":33392,\"start\":33385},{\"end\":33399,\"start\":33392},{\"end\":33406,\"start\":33399},{\"end\":33415,\"start\":33406},{\"end\":33424,\"start\":33415},{\"end\":33435,\"start\":33424},{\"end\":33832,\"start\":33821},{\"end\":33841,\"start\":33832},{\"end\":33850,\"start\":33841},{\"end\":33863,\"start\":33850},{\"end\":33878,\"start\":33863},{\"end\":33890,\"start\":33878},{\"end\":33900,\"start\":33890},{\"end\":33910,\"start\":33900},{\"end\":33921,\"start\":33910},{\"end\":33929,\"start\":33921},{\"end\":34300,\"start\":34293},{\"end\":34312,\"start\":34300},{\"end\":34323,\"start\":34312},{\"end\":34330,\"start\":34323},{\"end\":34797,\"start\":34785},{\"end\":34810,\"start\":34797},{\"end\":34821,\"start\":34810},{\"end\":34831,\"start\":34821},{\"end\":35326,\"start\":35313},{\"end\":35337,\"start\":35326},{\"end\":35349,\"start\":35337},{\"end\":35360,\"start\":35349},{\"end\":35370,\"start\":35360},{\"end\":35797,\"start\":35786},{\"end\":35808,\"start\":35797},{\"end\":35990,\"start\":35982},{\"end\":36002,\"start\":35990},{\"end\":36013,\"start\":36002},{\"end\":36021,\"start\":36013},{\"end\":36030,\"start\":36021},{\"end\":36519,\"start\":36512},{\"end\":36530,\"start\":36519},{\"end\":37004,\"start\":36995},{\"end\":37015,\"start\":37004},{\"end\":37028,\"start\":37015},{\"end\":37035,\"start\":37028},{\"end\":37250,\"start\":37241},{\"end\":37261,\"start\":37250},{\"end\":37268,\"start\":37261},{\"end\":37673,\"start\":37665},{\"end\":37680,\"start\":37673},{\"end\":37689,\"start\":37680},{\"end\":37697,\"start\":37689},{\"end\":37704,\"start\":37697},{\"end\":38146,\"start\":38138},{\"end\":38152,\"start\":38146},{\"end\":38159,\"start\":38152},{\"end\":38176,\"start\":38159},{\"end\":38182,\"start\":38176},{\"end\":38193,\"start\":38182},{\"end\":38205,\"start\":38193},{\"end\":38622,\"start\":38610},{\"end\":38631,\"start\":38622},{\"end\":38650,\"start\":38631},{\"end\":38964,\"start\":38956},{\"end\":38974,\"start\":38964},{\"end\":38984,\"start\":38974},{\"end\":39375,\"start\":39367},{\"end\":39384,\"start\":39375},{\"end\":39393,\"start\":39384},{\"end\":39727,\"start\":39719},{\"end\":39738,\"start\":39727},{\"end\":39747,\"start\":39738},{\"end\":39759,\"start\":39747},{\"end\":39767,\"start\":39759},{\"end\":39779,\"start\":39767},{\"end\":40296,\"start\":40281},{\"end\":40309,\"start\":40296},{\"end\":40319,\"start\":40309},{\"end\":40330,\"start\":40319},{\"end\":40693,\"start\":40681},{\"end\":40704,\"start\":40693},{\"end\":40715,\"start\":40704},{\"end\":40724,\"start\":40715},{\"end\":40969,\"start\":40963},{\"end\":40981,\"start\":40969},{\"end\":40991,\"start\":40981},{\"end\":41003,\"start\":40991},{\"end\":41367,\"start\":41360},{\"end\":41373,\"start\":41367},{\"end\":41385,\"start\":41373},{\"end\":41392,\"start\":41385},{\"end\":41693,\"start\":41686},{\"end\":41701,\"start\":41693},{\"end\":41710,\"start\":41701},{\"end\":41717,\"start\":41710},{\"end\":42055,\"start\":42047},{\"end\":42063,\"start\":42055},{\"end\":42077,\"start\":42063},{\"end\":42241,\"start\":42233},{\"end\":42251,\"start\":42241},{\"end\":42265,\"start\":42251},{\"end\":42516,\"start\":42508},{\"end\":42523,\"start\":42516},{\"end\":42530,\"start\":42523},{\"end\":42536,\"start\":42530},{\"end\":42545,\"start\":42536},{\"end\":42553,\"start\":42545},{\"end\":42976,\"start\":42969},{\"end\":42982,\"start\":42976},{\"end\":42994,\"start\":42982},{\"end\":43444,\"start\":43434},{\"end\":43456,\"start\":43444},{\"end\":43466,\"start\":43456},{\"end\":43478,\"start\":43466},{\"end\":43491,\"start\":43478},{\"end\":43988,\"start\":43978},{\"end\":43994,\"start\":43988},{\"end\":44002,\"start\":43994},{\"end\":44010,\"start\":44002},{\"end\":44022,\"start\":44010},{\"end\":44319,\"start\":44308},{\"end\":44329,\"start\":44319},{\"end\":44338,\"start\":44329},{\"end\":44349,\"start\":44338},{\"end\":44358,\"start\":44349},{\"end\":44771,\"start\":44763},{\"end\":44780,\"start\":44771},{\"end\":45182,\"start\":45174},{\"end\":45190,\"start\":45182},{\"end\":45201,\"start\":45190},{\"end\":45212,\"start\":45201},{\"end\":45219,\"start\":45212},{\"end\":45409,\"start\":45399},{\"end\":45420,\"start\":45409},{\"end\":45434,\"start\":45420},{\"end\":45444,\"start\":45434},{\"end\":45453,\"start\":45444},{\"end\":45799,\"start\":45791},{\"end\":45807,\"start\":45799},{\"end\":45815,\"start\":45807},{\"end\":45824,\"start\":45815},{\"end\":45834,\"start\":45824},{\"end\":46084,\"start\":46071},{\"end\":46095,\"start\":46084},{\"end\":46104,\"start\":46095},{\"end\":46115,\"start\":46104},{\"end\":46519,\"start\":46511},{\"end\":46525,\"start\":46519},{\"end\":46533,\"start\":46525},{\"end\":46539,\"start\":46533},{\"end\":46547,\"start\":46539},{\"end\":46558,\"start\":46547},{\"end\":46566,\"start\":46558},{\"end\":46577,\"start\":46566},{\"end\":46587,\"start\":46577},{\"end\":46600,\"start\":46587},{\"end\":47100,\"start\":47092},{\"end\":47113,\"start\":47100},{\"end\":47124,\"start\":47113},{\"end\":47132,\"start\":47124},{\"end\":47141,\"start\":47132},{\"end\":47499,\"start\":47484},{\"end\":47512,\"start\":47499},{\"end\":47522,\"start\":47512},{\"end\":47533,\"start\":47522},{\"end\":47544,\"start\":47533},{\"end\":47975,\"start\":47965},{\"end\":47983,\"start\":47975},{\"end\":47999,\"start\":47983},{\"end\":48009,\"start\":47999},{\"end\":48350,\"start\":48342},{\"end\":48356,\"start\":48350},{\"end\":48364,\"start\":48356},{\"end\":48370,\"start\":48364},{\"end\":48836,\"start\":48828},{\"end\":48842,\"start\":48836},{\"end\":48850,\"start\":48842},{\"end\":48859,\"start\":48850},{\"end\":48867,\"start\":48859},{\"end\":49307,\"start\":49299},{\"end\":49315,\"start\":49307},{\"end\":49323,\"start\":49315},{\"end\":49695,\"start\":49687},{\"end\":49703,\"start\":49695},{\"end\":49711,\"start\":49703},{\"end\":49720,\"start\":49711},{\"end\":49726,\"start\":49720},{\"end\":50040,\"start\":50033},{\"end\":50047,\"start\":50040},{\"end\":50054,\"start\":50047},{\"end\":50060,\"start\":50054},{\"end\":50067,\"start\":50060},{\"end\":50329,\"start\":50322},{\"end\":50336,\"start\":50329},{\"end\":50343,\"start\":50336},{\"end\":50349,\"start\":50343},{\"end\":50360,\"start\":50349},{\"end\":50364,\"start\":50360},{\"end\":50371,\"start\":50364},{\"end\":50574,\"start\":50564},{\"end\":50584,\"start\":50574},{\"end\":50596,\"start\":50584},{\"end\":50602,\"start\":50596},{\"end\":50615,\"start\":50602},{\"end\":50627,\"start\":50615},{\"end\":51046,\"start\":51040},{\"end\":51055,\"start\":51046},{\"end\":51062,\"start\":51055},{\"end\":51069,\"start\":51062},{\"end\":51444,\"start\":51432},{\"end\":51456,\"start\":51444},{\"end\":51462,\"start\":51456},{\"end\":51472,\"start\":51462},{\"end\":51873,\"start\":51866},{\"end\":51881,\"start\":51873},{\"end\":52244,\"start\":52236},{\"end\":52250,\"start\":52244},{\"end\":52256,\"start\":52250},{\"end\":52268,\"start\":52256},{\"end\":52695,\"start\":52684},{\"end\":52709,\"start\":52695},{\"end\":52719,\"start\":52709},{\"end\":52731,\"start\":52719},{\"end\":52740,\"start\":52731},{\"end\":52746,\"start\":52740},{\"end\":52758,\"start\":52746},{\"end\":52767,\"start\":52758},{\"end\":52775,\"start\":52767},{\"end\":52781,\"start\":52775},{\"end\":53067,\"start\":53049},{\"end\":53077,\"start\":53067},{\"end\":53318,\"start\":53310},{\"end\":53328,\"start\":53318},{\"end\":53334,\"start\":53328},{\"end\":53342,\"start\":53334},{\"end\":53348,\"start\":53342},{\"end\":53788,\"start\":53777},{\"end\":53804,\"start\":53788},{\"end\":53812,\"start\":53804},{\"end\":53823,\"start\":53812},{\"end\":54149,\"start\":54141},{\"end\":54157,\"start\":54149},{\"end\":54165,\"start\":54157},{\"end\":54175,\"start\":54165},{\"end\":54187,\"start\":54175},{\"end\":54505,\"start\":54498},{\"end\":54515,\"start\":54505},{\"end\":54522,\"start\":54515},{\"end\":54533,\"start\":54522},{\"end\":54771,\"start\":54767},{\"end\":54915,\"start\":54905},{\"end\":54924,\"start\":54915},{\"end\":54933,\"start\":54924},{\"end\":54942,\"start\":54933},{\"end\":54954,\"start\":54942},{\"end\":54964,\"start\":54954},{\"end\":54975,\"start\":54964},{\"end\":54982,\"start\":54975},{\"end\":54996,\"start\":54982},{\"end\":55006,\"start\":54996},{\"end\":55019,\"start\":55006},{\"end\":55027,\"start\":55019},{\"end\":55035,\"start\":55027},{\"end\":55045,\"start\":55035},{\"end\":55055,\"start\":55045},{\"end\":55065,\"start\":55055},{\"end\":55081,\"start\":55065},{\"end\":55092,\"start\":55081},{\"end\":55100,\"start\":55092},{\"end\":55107,\"start\":55100},{\"end\":55119,\"start\":55107},{\"end\":55809,\"start\":55800},{\"end\":55820,\"start\":55809},{\"end\":55829,\"start\":55820},{\"end\":56067,\"start\":56056},{\"end\":56075,\"start\":56067},{\"end\":56085,\"start\":56075},{\"end\":56290,\"start\":56282},{\"end\":56307,\"start\":56290},{\"end\":56313,\"start\":56307},{\"end\":56319,\"start\":56313},{\"end\":56325,\"start\":56319},{\"end\":56336,\"start\":56325},{\"end\":56348,\"start\":56336},{\"end\":56355,\"start\":56348},{\"end\":56793,\"start\":56786},{\"end\":56805,\"start\":56793},{\"end\":57148,\"start\":57140},{\"end\":57156,\"start\":57148},{\"end\":57164,\"start\":57156},{\"end\":57172,\"start\":57164}]", "bib_venue": "[{\"end\":32116,\"start\":32054},{\"end\":33019,\"start\":32953},{\"end\":34394,\"start\":34383},{\"end\":34980,\"start\":34914},{\"end\":35519,\"start\":35453},{\"end\":36151,\"start\":36099},{\"end\":36651,\"start\":36599},{\"end\":37409,\"start\":37347},{\"end\":37853,\"start\":37787},{\"end\":38354,\"start\":38288},{\"end\":38779,\"start\":38723},{\"end\":39125,\"start\":39063},{\"end\":39928,\"start\":39862},{\"end\":41124,\"start\":41072},{\"end\":41846,\"start\":41790},{\"end\":42682,\"start\":42626},{\"end\":43135,\"start\":43073},{\"end\":43640,\"start\":43574},{\"end\":44492,\"start\":44432},{\"end\":44929,\"start\":44863},{\"end\":46264,\"start\":46198},{\"end\":46749,\"start\":46683},{\"end\":47659,\"start\":47610},{\"end\":48519,\"start\":48453},{\"end\":49016,\"start\":48950},{\"end\":49387,\"start\":49376},{\"end\":50768,\"start\":50706},{\"end\":51210,\"start\":51148},{\"end\":51621,\"start\":51555},{\"end\":51996,\"start\":51947},{\"end\":52409,\"start\":52347},{\"end\":53497,\"start\":53431},{\"end\":55871,\"start\":55860},{\"end\":57313,\"start\":57251},{\"end\":30914,\"start\":30878},{\"end\":31292,\"start\":31263},{\"end\":31665,\"start\":31626},{\"end\":32052,\"start\":31975},{\"end\":32503,\"start\":32454},{\"end\":32951,\"start\":32870},{\"end\":33504,\"start\":33435},{\"end\":33965,\"start\":33929},{\"end\":34381,\"start\":34330},{\"end\":34912,\"start\":34831},{\"end\":35451,\"start\":35370},{\"end\":35812,\"start\":35808},{\"end\":36097,\"start\":36030},{\"end\":36597,\"start\":36530},{\"end\":36993,\"start\":36904},{\"end\":37345,\"start\":37268},{\"end\":37785,\"start\":37704},{\"end\":38286,\"start\":38205},{\"end\":38721,\"start\":38650},{\"end\":39061,\"start\":38984},{\"end\":39431,\"start\":39393},{\"end\":39860,\"start\":39779},{\"end\":40370,\"start\":40330},{\"end\":40763,\"start\":40724},{\"end\":41070,\"start\":41003},{\"end\":41441,\"start\":41392},{\"end\":41788,\"start\":41717},{\"end\":42045,\"start\":42028},{\"end\":42303,\"start\":42265},{\"end\":42624,\"start\":42553},{\"end\":43071,\"start\":42994},{\"end\":43572,\"start\":43491},{\"end\":43976,\"start\":43898},{\"end\":44430,\"start\":44358},{\"end\":44861,\"start\":44780},{\"end\":45224,\"start\":45219},{\"end\":45532,\"start\":45453},{\"end\":45849,\"start\":45834},{\"end\":46196,\"start\":46115},{\"end\":46681,\"start\":46600},{\"end\":47090,\"start\":47007},{\"end\":47608,\"start\":47544},{\"end\":48047,\"start\":48009},{\"end\":48451,\"start\":48370},{\"end\":48948,\"start\":48867},{\"end\":49374,\"start\":49323},{\"end\":49764,\"start\":49726},{\"end\":50031,\"start\":49936},{\"end\":50375,\"start\":50371},{\"end\":50704,\"start\":50627},{\"end\":51146,\"start\":51069},{\"end\":51553,\"start\":51472},{\"end\":51945,\"start\":51881},{\"end\":52345,\"start\":52268},{\"end\":52682,\"start\":52633},{\"end\":53113,\"start\":53077},{\"end\":53429,\"start\":53348},{\"end\":53871,\"start\":53823},{\"end\":54225,\"start\":54187},{\"end\":54569,\"start\":54533},{\"end\":54765,\"start\":54757},{\"end\":55168,\"start\":55119},{\"end\":55858,\"start\":55829},{\"end\":56054,\"start\":56007},{\"end\":56423,\"start\":56355},{\"end\":56872,\"start\":56805},{\"end\":57249,\"start\":57172}]"}}}, "year": 2023, "month": 12, "day": 17}