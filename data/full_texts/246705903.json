{"id": 246705903, "updated": "2023-10-05 17:05:11.136", "metadata": {"title": "Understanding Hyperdimensional Computing for Parallel Single-Pass Learning", "authors": "[{\"first\":\"Tao\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Yichi\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Zhiru\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Christopher\",\"last\":\"Sa\",\"middle\":[\"De\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Hyperdimensional computing (HDC) is an emerging learning paradigm that computes with high dimensional binary vectors. It is attractive because of its energy efficiency and low latency, especially on emerging hardware -- but HDC suffers from low model accuracy, with little theoretical understanding of what limits its performance. We propose a new theoretical analysis of the limits of HDC via a consideration of what similarity matrices can be\"expressed\"by binary vectors, and we show how the limits of HDC can be approached using random Fourier features (RFF). We extend our analysis to the more general class of vector symbolic architectures (VSA), which compute with high-dimensional vectors (hypervectors) that are not necessarily binary. We propose a new class of VSAs, finite group VSAs, which surpass the limits of HDC. Using representation theory, we characterize which similarity matrices can be\"expressed\"by finite group VSA hypervectors, and we show how these VSAs can be constructed. Experimental results show that our RFF method and group VSA can both outperform the state-of-the-art HDC model by up to 7.6\\% while maintaining hardware efficiency.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2202.04805", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/YuZZS22", "doi": null}}, "content": {"source": {"pdf_hash": "f8bd842e06b0eb40f3233e0ce43f00dc3a678d54", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2202.04805v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b03d8fa4cb6f688f00fdc2561cd939df7a67a938", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f8bd842e06b0eb40f3233e0ce43f00dc3a678d54.txt", "contents": "\nUnderstanding Hyperdimensional Computing for Parallel Single-Pass Learning\n\n\nTao Yu \nCornell University\nCornell University\nCornell University\nCornell University\n\n\nYichi Zhang \nCornell University\nCornell University\nCornell University\nCornell University\n\n\nZhiru Zhang zhiruz@cornell.edu \nCornell University\nCornell University\nCornell University\nCornell University\n\n\nChristopher De Sa \nCornell University\nCornell University\nCornell University\nCornell University\n\n\nUnderstanding Hyperdimensional Computing for Parallel Single-Pass Learning\n\nHyperdimensional computing (HDC) is an emerging learning paradigm that computes with high dimensional binary vectors. There is an active line of research on HDC in the community of emerging hardware because of its energy efficiency and ultra-low latency-but HDC suffers from low model accuracy, with little theoretical understanding of what limits its performance. We propose a new theoretical analysis of the limits of HDC via a consideration of what similarity matrices can be \"expressed\" by binary vectors, and we show how the limits of HDC can be approached using random Fourier features (RFF). We extend our analysis to the more general class of vector symbolic architectures (VSA), which compute with high-dimensional vectors (hypervectors) that are not necessarily binary. We propose a new class of VSAs, finite group VSAs, which surpass the limits of HDC. Using representation theory, we characterize which similarity matrices can be \"expressed\" by finite group VSA hypervectors, and we show how these VSAs can be constructed. Experimental results show that our RFF method and group VSA can both outperform the state-of-the-art HDC model by up to 7.6% while maintaining hardware efficiency. This work aims to inspire a future interest on HDC in the ML community and connect to the hardware community.\n\nIntroduction\n\nHyperdimensional computing (HDC) is an emerging learning paradigm. Unlike conventional cognitive modeling that computes with real numbers, it computes with high dimensional binary vectors, referred to as binary hypervectors, the dimension of which is usually at least in the thousands. HDC is brain-inspired as high dimensional representations have two fundamental properties similar to human brains: they are (1) distributed and highly parallel; and (2) robust to noise and tolerant to component failure [Kanerva, 2009]. On the other hand, the massive parallelism and simple arithmetic project HDC into the scope of energy-efficient and ultra-low-latency computing, especially with the rise of emerging hardware , Salamat et al., 2019. As a result, HDC has recently attracted considerable attention from edge applications, e.g., robotics, DNA pattern matching, and health diagnosis, as well as data center applications such as recommendation systems [Mitrokhin et al., 2019, Neubert et al., 2019, Neubert and Schubert, 2021, Burrello et al., 2019, Guo et al., 2021.\n\nThe practical deployment of HDC is undermined by its low model accuracy compared to other alternatives, e.g., neural networks (NN). The state-of-the-art HDC model on MNIST has an accuracy of 89% [Chuang et al., 2020]. A two-layer NN, however, can easily achieve 95% [Lecun et al., 1998].\n\nThere are two main approaches in the literature to improving HDC. One approach is to increase the hypervector dimension, staying within the classic HDC paradigm and just making the binary vectors longer [Neubert et al., 2019, Schlegel et al., 2021]. An alternative is to increase the complexity of each element in a hypervector, e.g., to floating-point or complex numbers (unit circle in the complex plane) [Plate, 1995, Gallant and Okaywe, 2013, Gayler, 1998, Plate, 1994: this moves the system into the more general realm of vector symbolic architecture (VSA) [Schlegel et al., 2021], which uses high-dimensional vectors with elements that are not necessarily binary (unlike binary HDC). However, these remedies are not based on any theoretical analysis of the limits of HDC, and as a result there is a lack of more-than-empirical understanding of when and how they should be deployed.\n\nIn this work, we introduce a new notion of expressivity for any VSA using similarity matrices. Given a set of hypervectors v 1 , . . . , v n in a VSA, each entry M ij in the similarity matrix M is defined as the similarity between a pair of hypervectors v i and v j ; the similarity is typically measured by an inner product function. Informally, we propose that a VSA is more expressive when it can express (i.e. represent with some set of vectors) a wider class of similarity matrices. Importantly, which M a VSA can express is independent of the vector dimension D: this new notion distinguishes between longer-vector and more-complex-vector approaches.\n\nWe show that HDC, with binary hypervectors even in any dimension, cannot express as many similarity matrices that a VSA with more complex hypervectors can. Even worse, the current method of initializing the hypervectors in an HDC system further reduces the expressible set, which impedes the success of HDC. This notion of expressivity is closely related to learning ability. We exhibit a simple task where current HDC (of any D) is incapable of learning a Bayes optimal classifier, while any other VSA system that can express a particular similarity matrix M (which HDC cannot express) can learn it through the same procedure.\n\nBased on our analysis, we investigate how we can improve HDC through the lens of similarity matrices. We first propose to improve the initialization of binary hypervectors by employing random Fourier features (RFFs) [Rahimi and Recht, 2007]. This method is different from existing approaches that increase the dimension or complexity of hypervectors. We show that this better initialization via expressing a similarity matrix can already surpass state-of-the-art HDC accuracy on MNIST by 6.4%. We then propose and formally define group VSA, an extended version of HDC where elements in hypervectors are more complex than binary but less than floating-point. Group VSA can further improve the RFF-initialized HDC by 1.2% on MNIST.\n\nOur contributions are as follows:\n\n\u2022 We provide a theoretical analysis of the limitations of an HDC system with binary hypervectors. \u2022 We approach the expressivity limit of HDC systems with random Fourier features and empirically evaluate the improvements on standard benchmarks. \u2022 We propose group VSA, which generalizes HDC with more complex elements, expanding the set of expressible similarity matrices while maintaining efficiency. \u2022 We evaluate the performance of group VSA on both conventional HDC tasks and image tasks, and study its efficiency implications by analyzing the circuit depths.\n\n\nRelated Work\n\nThe term HDC was first introduced by Kanerva [2009]. It is also referred to as VSA in some literature [Schlegel et al., 2021], a line of work that does symbolic computing. Binary HDC can be traced back to Binary Spatter Code (BSC) [Kanerva, 1994, Kanerva et al., 1997.\n\nModel capacity improvement. There are two main VSA formats other than binary HDC: using floating-point real vectors [Plate, 1995, Gallant and Okaywe, 2013, Gayler, 1998, Gosmann and Eliasmith, 2019 or complex vectors [Plate, 1994]. Typically their model capacities are higher since their individual vector components are more complex. Another way of increasing the model capacity is increasing the vector length [Neubert et al., 2019, Schlegel et al., 2021, Chuang et al., 2020, Frady et al., 2018. However, it remains unknown when and how we should apply these methods, and whether they are sufficient to solve a task. Our approach is different as our proposed methods are based on the theoretical analysis of, and are designed to bypass, the limits of binary HDC.\n\nHardware implication. HDC inspires a novel hardware architecture that requires associative memory [Hopfield, 1982] where long vectors can be stored and addressed efficiently. It is therefore popularized recently in the emerging in-memory computing community .\n\nIn the meantime, the simplicity of HDC arithmetic and the massive parallelism make HDC suitable for tasks that require high energy efficiency and low latency. It has been demonstrated successful on commercial hardware as well , Salamat et al., 2019, Basaklar et al., 2021. In this work, we provide an analysis on the circuit depths of HDC and our proposed group VSA.\n\nTheory. Understanding HDC from a theoretical perspective is currently limited. Thomas et al. [2020], Frady et al. [2018 presented some theoretical foundations of HDC, introducing the benefit of high-dimensional vectors, hypervector encoding, and the connection between HDC and kernel approximation. Our work instead presents the limits of HDC and how we can bypass it. Frady et al. [2021] propose to generalize VSA/HDC to function space. Our work is different since our proposed group VSA, a generalization of HDC, is still discrete and preserves the hardware efficiency.\n\n\nBackground on HDC\n\nIn this section we introduce basics of HDC: hypervectors, arithmetic, and the learning paradigm. We then present a classical approach of HDC on the popular MNIST database. A more comprehensive introduction is in Ge and Parhi [2020].\n\nHD Representations. In HDC, we compute with binary hypervectors in a high-dimensional space referred to as hyperspace. Given a random hypervector v in a 10, 000 dimensional space {\u22121, 1} 10,000 , it is well known from the \"curse\" of dimensionality that most vectors in this hyperspace are nearly orthogonal to v [Kanerva, 2009]. We call such hypervectors unrelated. The \"curse\" provides two intriguing properties for cognitive tasks: (1) independent random hypervectors will be unrelated and so can naturally represent objects that are semantically separate, e.g. letters of the alphabet; (2) two hypervectors u and v that have a high-enough inner-product similarity can be classified as being related (i.e. somehow dependent) with high probability. Classical HDC therefore represents data using binary hypervectors randomly drawn from a hyperspace. HDC computes with hypervectors using a fixed set of primitive operations: similarity, binding, bundling, and permutation.\n\nSimilarity. A similarity function S(u, v) measures how close two hypervectors u, v \u2208 {\u22121, 1} D are. It is typically defined as an inner product function [Frady et al., 2021] \nS(u, v) = 1 D D i=1 u i v i ;\nthis is an affine function of the hamming distance for binary hypervectors [Kanerva, 2009].\n\nBinding . The binding operation combines two hypervectors u, v into a new hypervector in the same space that represents them as a pair. For binary {\u22121, +1} D , binding is equivalent to coordinatewise multiplication, i.e. (u \u2297 v) \u2208 {\u22121, +1} D and (u \u2297 v) i = u i v i for all i \u2208 {1, . . . , D}. Binding preserves similarity, in the sense that S(u \u2297 w, v \u2297 w) = S(u, v) for any hypervectors u, v, and w; also, if u is highly similar to v and x is highly similar to y, then u \u2297 x will be highly similar to v \u2297 y (although usually less than either constituent pair). Binding is implemented on hardware as an XOR.\n\nBundling . Bundling represents an unordered collection of hypervectors. The bundling operation takes in a set of hypervectors and yields a hypervector that is maximally similar to all of them: it acts as an aggregation of a set of hypervectors.\nBundling v 1 , . . . , v m \u2208 {\u22121, +1} D yields ( m k=1 v k ) i = sgn ( m k=1 (v k ) i ) for all i \u2208 {1, . . . , D}.\nThis takes a majority vote at each coordinate of the vector; ties are broken at random. HDC typically leverages bundling to learn a class representative.\n\nPermutation . The permutation operation is a shuffling of the elements in a hypervector. It can be represented as a multiplication of a permutation matrix \u03a0. A random permutation on a hypervector yields another hypervector that is unrelated to it. Note that permutation is invertible, meaning that \u03a0 \u22121 \u03a0v i = v i . It is thus useful for encoding order and position information. In hardware, permutation usually appears as shifting since its implementation is efficient.\n\n\nMNIST as a Case Study\n\nWe outline how to use the classic HDC approach on the MNIST digit recognition task for illustration.\n\nEncoding. First, 256 basis hypervectors {v 0 , v 1 , . . . , v 255 } are independently drawn at random from a hyperspace {\u22121, +1} D : each hypervector v i represents a pixel intensity i. Second, we bind all 784 pixels in an MNIST image by their corresponding hypervectors. Since the binding operation is commutative by definition, but pixels in an image have meaningful relative positions, each pixel hypervector is shifted before joining the encoding to preserve that position information. If the input pixel intensities are p 0 , p 1 , . . . , p 783 , then its encoded hypervector t is v p0 \u2297 (\u03a0v p1 ) \u2297 \u00b7 \u00b7 \u00b7 \u2297 (\u03a0 783 v p783 ), where \u03a0 j denotes shifting a hypervector j times.\n\nLearning. Encoding yields a set of training hypervectors T = {t 1 , t 2 , . . . , t 60,000 }. To learn, we bundle all the hypervectors that are from the same digit. Concretely, a class centroid s c is computed by s c = i|yi=c t i . Each training image is used only once, making this process single-pass learning.\n\nInference. At the inference time, a given test image is encoded through the same procedure. The model outputs the class c with the highest similarity S(t test , s c ).\n\n\nSimilarity Matrices and the Limits of HDC\n\nTraditionally the expressivity of HDC setups is identified with the dimension of the hypervectors D. This notion is unhelpful for probing the fundamental limitations of HDC, which do not depend on D.\n\nIn this section, we define a new notion of expressivity, which reveals the limits of HDC.\n\nDefinition 1. An HDC (or VSA) system can express a similarity matrix M \u2208 R n\u00d7n if for any \u03b5 > 0, there exists a D \u2208 N and D-dimensional hypervectors v 1 , v 2 , . . . , v n in the HDC/VSA such that |M ij \u2212 S(v i , v j )| \u2264 \u03b5 where S denotes the similarity function of the HDC/VSA.\n\nInformally, this means that the HDC/VSA can approximate M arbitrarily well. Limitations on M we can express correspond to limitations on the similarity relation we can represent on data: if we have some dataset and know how similar each pair of examples should be, whether or not we can represent the similarity accurately with an HDC embedding depends on whether HDC can express the corresponding M. Surprisingly, there are some matrices that an HDC system can never express. Our notion of expressivity corresponds to learning ability, we give an example task for which whether a VSA/HDC approach can learn the Bayes-optimal classifier depends on whether it can express M Lemma 4.1 . Consider a supervised learning task with input set X = {0, 1, 2}, output label set Y = X , and source distribution P(x, y) := (1/9 + 2p) if x = y else (1/9 \u2212 p) for some small positive number p. We say that a VSA can learn this task if there exists a D-dimensional encoding of X in that VSA such that, when the bundling method in Section 3.1 is used on a training set of size N drawn from P, the resulting classifier is the Bayes optimal classifier with arbitrarily high probability as N increases.\n\nStatement 4.1. Binary HDC cannot learn this task. Any VSA (formalized later in Definition 2) that can express M Lemma 4.1 can learn this task.\n\nDetails on Statement 4.1 are in appendix. This learning task shows that only increasing the dimensionality of hypervectors cannot help learn the correct predictions if unable to express a certain matrix. This implies that our notion of expressivity captures HDC limitations in a way that relates to learning.\n\nLimitations due to initialization. So far in this section we have described limitations that are inherent to using binary representations in a VSA. Classical HDC methods are often limited in an additional way: rather than considering arbitrary binary hypervectors, they use hypervectors that are sampled independently at random. In such a system, any hypervector used for an embedding (used to represent an entity) is constructed either by (1) independently sampling a binary hypervector where each entry has some probability p of being 1, or (2) permuting and/or binding some pre-existing hypervectors. Examples of this setup can be found in Burrello et al. [2019], Smith and Stanford [1990], Imani et al. [2019b]. Surprisingly, we show that this approach further restricts the set of similarity matrices that can be expressed in expectation.\n\nLemma 4.2. Let u 1 , u 2 , . . . , u K be binary vectors sampled coordinate-wise independently at random, where each coordinate of u i has the same probability p i of being 1. Let v 0 , v 1 , and v 2 be vectors that result from some composition of binding and permutation operations acting on u 1 , . . . , u K , and let\nM \u2208 R 3\u00d73 be their similarity matrix, such that M ij = S(v i , v j ). Then E[M] \u2212 1 \u2212 1 3 \u2212 1 3 \u2212 1 3 1 \u2212 1 3 \u2212 1 3 \u2212 1 3 1 F \u2265 \u221a 2 3 ,\nbut this target matrix can be expressed by binary HDC.\n\n\nEncoding Hypervectors via RFF\n\nOur analysis using similarity matrices provides a strong motivation for using more principled methods to construct hypervectors. We argue that, if there is some similarity matrix M we want to achieve, we should directly instantiate hypervectors to match it in expectation.\n\n\nAlgorithm 1 Construct correlated hypervectors\ninput: similarity matrix M \u2208 R n\u00d7n , dimension d let\u03a3 = sin( \u03c0 2 M) {elementwise} let U \u039bU T =\u03a3 {symmetric eigendecomposition} sample X \u2208 R n\u00d7d iid unit Gaussians return sgn(U \u039b 1/2 + X) {elementwise}\nA natural way to represent a desired similarity matrix M \u2208 R n\u00d7n is to project it onto the set of representable matrices of binary vectors, which would correspond to a distribution one could sample from. Unfortunately, this approach is intractable as it would require solving a linear programming problem of size exponential in n. Instead, to approach the expressivity limits of binary HDC, we propose the following approach, given in Algorithm 1. First, we sample d independent multivariate Gaussians over R n ; Our n HDC vectors of length d are then given by the signs of these Gaussians. The following lemma tells us how to make this produce a desired similarity matrix M. Lemma 5.1. Suppose X, Y are jointly Gaussian zero-mean unit-variance random variables, then\nE[sgn(X) sgn(Y )] = 2 \u03c0 arcsin (E[XY ])\n. From this lemma, it immediately follows that if the elementwise sin of \u03c0 2 M is positive semi-definite, then Algorithm 1 produces hypervectors that, in expectation, exactly achieve M; otherwise, some approximation to M is produced. It also immediately follows that Algorithm 1 can achieve more similarity matrices than the classical procedure of Lemma 4.2: while that lemma shows that the similarity matrix 4 3 I 3 \u2212 1 3 1 3\u00d73 cannot be achieved in expectation by classical HDC initialization, Algorithm 1 can achieve it as sin( \u03c0 2 \u00b7 \u22121 3 ) = \u22121 2 and 3 2 I 3 \u2212 1 2 1 3\u00d73 is positive semidefinite. Algorithm 1 gives us more freedom to achieve a wider range of similarity matrices; however, it does not tell us which similarity matrix M to choose for a given task and whether sin( \u03c0 2 M) is positive semi-definite or not. In this paper, we use the well-known RBF kernel [Vert et al., 2004] to choose the similarity matrix between entities, but any similarity matrix appropriate for a task is applicable.\n\n6 Group VSA So far we have shown how replacing existing initialization methods can approach the limits of binary HDC. However, as Lemma 4.1 shows, binary HDC itself has inherent limits. Other known VSAs, such as the unit cycle VSA [Plate, 1994]-in which the elements are complex numbers of absolute value 1-can surpass these limits. However, this comes with the problem of a continuous space-requiring both approximation and significant hardware complexity overhead compared to binary HDC. In this section, we propose a new class of VSA, finite group VSA, which effectively \"interpolates\" between them so as to bypass the similarity-representation limits of binary HDC without the need for a continuous space.\n\nWe start by defining a VSA, and then propose to use group structures for the elements of hypervectors as a different approach to improve the expressivity of VSA. Binary HDC can be considered as a special case of our construction corresponding to the 2-element group. Definition 2. A group VSA is a tuple (G, \u00b5, \u2297, S, \u2295), where G is some measurable set of symbols, \u2297 : G \u00d7 G \u2192 G is the binding operator, S : G \u00d7 G \u2192 R is a symmetric similarity operator, and \u2295 : G <\u03c9 \u2192 G is the bundling operator (which maps a finite sequence of symbols to a symbol). A VSA must have the following properties:\n\n\u2022 Binding. (G, \u2297) is a group,i.e., binding is associative over G and has inverse and identity elements.\n\n\u2022 Similarity to self. S(x, x) = 1 for all x \u2208 G.\n\n\u2022 Similarity preserved by binding. S(g \u2297 x, g \u2297 y) = S(x \u2297 g, y \u2297 g) = S(x, y) for all g, x, y \u2208 G.\n\n\u2022 Similarity extensible to an inner product. There exists a finite-dimensional Hilbert space V over R and an embedding \u03c8 :\nG \u2192 V such that S(x, y) = \u03c8(x), \u03c8(y) for all x, y \u2208 G. Equivalently, n i=1 n j=1 c i c j S(x i , x j ) \u2265 0\nfor any x 1 , . . . , x n \u2208 G and scalars c 1 , . . . , c n \u2208 R. \u2022 Random vectors are dissimilar to any other vector.\nE g\u223cUniform(G) [S(g, x)] = 0 for any x \u2208 G. \u2022 Bundling. Bundling of x 1 , . . . , x m \u2208 G returns m i=1 x i = arg max g\u2208G m i=1 S(g, x i ) or one of the maxima in case of a tie.\nTo compute using a VSA of dimension D, we use hypervectors in G D , extend binding and bundling to act elementwise on these hypervectors, and extend similarity to compute the average similarity over the dimensions as\nS([x 1 , . . . , x D ], [y 1 , . . . , y D ]) = 1 D D i=1 S(x i , y i ).\nIt is easy to see that binary HDC is equivalent to a VSA where G = {\u22121, 1}, \u2297 is multiplication, and S(x, y) = xy. Similarly, a unit-cycle VSA (i.e., FHRR) has G = {z \u2208 C | |z| = 1}, \u2297 as complex multiplication, and S(x, y) = Re(x * y). Most other schemes called \"VSAs\" in literature fall under our definition, e.g., Gayler [1998], with few exceptions [Plate, 1995] that violate the group requirement. In order to run efficiently on hardware, we add the following restriction.\nDefinition 3. A finite group VSA is a VSA where G is finite. That is, (G, \u2297) is a finite group.\nOn hardware, finite G allows the VSA elements to be represented exactly and lets the VSA operations be computed exactly. The hardware cost will depend on the size of G. In many cases, we would like binding to preserve similarity in a stronger sense than that guaranteed by Definition 2. It is often desirable that the similarity after binding is the product of the similarities before binding, i.e.,\nS(x 1 \u2297 x 2 , y 1 \u2297 y 2 ) = S(x 1 , y 1 ) \u00b7 S(x 2 , y 2 );\n(1) this would make \u2297 behave like a tensor product space with inner product given by S. This property is particularly important, usually when an object consists of multiple features, we will bind these features so as to derive a representative vector for the object, this property ensures that two objects with multiple pairs of similar features to have similar representative vectors.\n\nOf course this equation is not guaranteed to hold for elements of G in general (e.g., when x 1 \u2297 x 2 = y 1 \u2297 y 2 ); however, most VSAs can approximate this behavior by adding an extra randomization step. Definition 4. Let A denote the uniform distribution over automorphisms of (G, \u2297, S). Then we say that VSA has the product property if for any x 1 , x 2 , y 1 , y 2 \u2208 G,\nE \u03b1\u223cA [S(x 1 \u2297 \u03b1(x 2 ), y 1 \u2297 \u03b1(y 2 ))] = E \u03b1\u223cA [S(\u03b1(x 1 ) \u2297 x 2 , \u03b1(y 1 ) \u2297 y 2 )] = S(x 1 , y 1 ) \u00b7 S(x 2 , y 2 ).\nIt is easy to see that this holds for binary HDC, as the identity map is the only automorphism of Z/2Z and (1) holds for binary HDC; this also holds for the unit cycle VSA, where the only automorphisms are the identity (z \u2192 z) and the complex conjugate automorphism (z \u2192 z * ). Rather than using this transformation directly, if one exists we can ensure this \"product property\" holds by initializing our hypervectors appropriately: if we sample hypervectors (x 1 , y 1 ) at random with independent entries and independently of (x 2 , y 2 ) and both distributions are invariant under automorphisms, then\nE[S(x 1 \u2297 x 2 , y 1 \u2297 y 2 )] = E[S(x 1 , y 1 )] \u00b7 E[S(x 2 , y 2 )].\n\nConstructing a VSA from a finite group\n\nAt first glance, the definition of a group VSA may seem open-ended, offering little guidance as to what the limitations of finite group VSAs may be and how they can be constructed. Surprisingly, we can fully characterize the finite group VSAs through representation theory. We start by introducing definitions specialized to finite-dimensional complex representations, before stating the full theorem. Definition 5 (James et al. [2001], Fulton and Harris [2013]). A representation of a group G over C n is a group homomorphism \u03c1 from G to C n\u00d7n such that \u03c1(gh) = \u03c1(g)\u03c1(h) for all g, h \u2208 G.\n\nThe character of a representation \u03c1 is the function \u03c7 : G \u2192 C given by \u03c7(g) = Tr(\u03c1(g)). The representation (and corresponding character) is said to be irreducible if no proper subspace of C n is preserved by the group action. The trivial representation, had by all groups, is \u03c1 : G \u2192 C 1\u00d71 with \u03c1(g) = 1 and \u03c7(g) = 1.\n\nIt is a standard result that each finite group possesses a finite number of irreducible characters equal to the number of conjugacy classes of the group [Serre, 1977, Fulton andHarris, 2013].\n\nTheorem 6.1. Let (G, \u2297) be a finite group, and let X denote the set of its non-trivial irreducible characters. Let \u03b1:X\u2192R \u22650 be some function that assigns a non-negative weight to each of the characters. Then, if we set S as S(g, h) = \u03c7\u2208X \u03b1(\u03c7)\u00b7Re(\u03c7(g \u22121 \u2297h)) \u03c7\u2208X \u03b1(\u03c7)\u00b7\u03c7 (1) , where the inverse and unit 1 are those of the group, and define bundling \u2295 as given in (2), then (G, \u2297, S, \u2295) is a finite group VSA. Any finite group VSA can be constructed in this way. If in this construction \u03b1 is supported on only one character \u03c7, i.e. S(g, h) = Re(\u03c7(g \u22121 \u2297 h))/\u03c7(1), then the VSA will have the product property.\n\nThis construction makes it seem as though finite-group VSAs with the product property may be a restricted subset, which could be less expressive. The following result shows that this is not the case.\n\nStatement 6.1. Let M be a similarity matrix expressible by a finite group VSA. Then there exists a finite group VSA that has the product property and can also express M.\n\n\nCyclic Group VSA\n\nMost of our work with group VSAs in this paper will use the cyclic group G = Z/nZ = {0, 1, \u00b7 \u00b7 \u00b7 , n \u2212 1}, as Definition 2 indicates, we first provide an embedding \u03c8 : G \u2192 V to a finite-dimensional Hilbert space V over R. Let \u03c8(x) = (cos(2\u03c0x/n), sin(2\u03c0x/n)).\n\nDefinition 6. The standard cyclic group VSA is given by:\n\n\u2022 The symbol set G = Z/nZ = {0, 1, \u00b7 \u00b7 \u00b7 , n\u22121} with addition modulo n as binding operation \u2297.\n\n\u2022 Similarity is defined as S(x, y) = \u03c8(x), \u03c8(y) = cos(2\u03c0(x \u2212 y)/n).\n\nThis cyclic group VSA is in some sense a \"subset\" of the unit cycle VSA, and as n goes to infinity, it approximates the the unit cycle VSA arbitrarily well [Plate, 1994], serving as an interpolation between the binary HDC and the unit cycle VSA. As a straightforward consequence, any M that can be expressed by this VSA can also be expressed by the unit cycle VSA. To compute with this VSA, we follow the procedure in Section 6: use hypervectors in G D , and extend similarity, binding and bundling operations accordingly. Similar to the HDC case, we utilize random Fourier features for a better basis hypervectors initialization with minor modifications of Algorithm 1; we replace the sgn function in the last step with the (nth) quantile function of a Gaussian so as to map into G.\n\nNote that as Theorem 6.1 shows, this setup is not the only VSA over the cyclic group. Indeed, for any distribution \u03b1 over {1, . . . , n \u2212 1}, the similarity function S(x, y) = n\u22121 k=1 \u03b1(k) cos 2\u03c0(x\u2212y)k n would yield a finite group VSA. We focus on the VSA of Definition 6 because it satisfies the product property, and all other VSAs on G that do so are either isomorphic to it or isomorphic to the same construction with a smaller n.\n\n\nNon-Abelian Finite Group VSAs\n\nOur analysis of cyclic group VSAs from the previous section extends naturally to cover all finite Abelian groups (i.e. groups in which \u2297 is commutative), since it is a classic result that every finite Abelian group factors as the direct product of cyclic groups. It is natural to ask: what about non-Abelian groups? Because they simplify both representation and computation, it would be convenient if we could restrict our attention to Abelian groups only. Unfortunately, the following two statements together show that non-Abelian groups can be strictly more expressive than Abelian groups. Statement 6.2. Any similarity matrix M that can be expressed by a finite Abelian group VSA can be expressed by the unit-cycle VSA (G = {z \u2208 C | |z| = 1}, x \u2297 y = xy, S(x, y) = Re(x * y)). Statement 6.3. There exists a similarity matrix M that can be expressed by a VSA over the (non-Abelian) binary icosahedral group, but not by the unit-cycle VSA (i.e., FHRR). Statement 6.2 follows from the standard representation-theoretic result that all irreducible representations of a finite Abelian group are one-dimensional, while Statement 6.3 is proved by direct construction. While these results do show that, non-Abelian finite group VSAs are \"more powerful\" than Abelian finite group VSAs, the additional complexity needed to unlock this power seems not worthwhile for our applications, where unit-cycle VSA already performs well-so, in our experiments we focus solely on the cyclic group. We leave exploration of non-Abelian VSAs to future work. \n\n\nLearning via SGD Instead of Bundling\n\nPrior works train an HDC model via bundling hypervectors in the same class T c = {t i |label(t i ) = c}. This is based on the fundamental assumption about bundling that the class representative s c is similar to each t i . We find that it is not always true, depending on the number of vectors being bundled.\n\nSuppose a set T c has 2k + 1 (avoids a tie) unrelated hypervectors, we can theoretically calculate 2 the expected angle \u03b8 between s c and a randomly selected hypervector t i : \u03b8 2k+1 = arccos 2k k /2 2k . This indicates that the class vector learned from bundling will be nearly orthogonal to each hypervector in the class and no longer be its representative as we increase k.\n\nAs an alternative, we propose to leverage stochastic gradient decent (SGD) to learn a linear classifier (same precision). Take binary HDC as an example, the classifier is a binarized matrix multiplication at inference time, i.e., O = X \u00b7 sgn(W ), where X is the binary hypervector and W is the weight matrix. During the back propagation, we use the straight-through estimator [Hubara et al., 2016] to approximate the gradient of the sign function: \u2202 sgn(W )/\u2202W := 1 if |W | < 1 else 0.\n\nThe inference cost of an HDC model remains the same as the bundling paradigm since they are both binary. The model is still trained for one or few epochs so the SGD approach incurs minor training overhead. We defer the SGD learning process of group VSAs to Appendix.\n\n\nExperiments\n\nDatasets. We evaluate the performance of proposed methods on two conventional HDC datasets, ISOLET [Dua and Graff, 2017] and UCIHAR [Anguita et al., 2012]. We also evaluate our method on MNIST and Fashion-MNIST [Xiao et al., 2017], which are more challenging for HDC. ISOLET is a speech recognition dataset where each sample is an audio signal with 617 features. Each feature is in the range of [\u22121, 1]. The dataset has 7719 samples in total. The goal is predicting which letter-name was spoken. UCIHAR is a human activity recognition database, each sample of which contains 561 features collected from smartphone sensors. The features are also in the range of [\u22121, 1]. The database has 10299 samples. The task is predicting which type of activity a human was performing.\n\nSetups. For ISOLET and UCIHAR, we quantize the features to 8 bits before encoding. We initialize a 10,000-dimensional basis hypervector for each {0, \u00b7 \u00b7 \u00b7 , 255} feature value, then encode raw inputs as described in Section 5 or 6. During the training stage, we use a learning rate of 0.01 and train classifiers for 10 epochs. We compare RFF-HDC and group VSA of order 2 3 and 2 4 with SOTA HDC [Imani et al., 2019a, Chuang et al., 2020 3 that propose iteratively updating the class vectors through misclassified examples. We also compare HDC to a perceptron [Rosenblatt, 1958] where inputs are 10,000-dimensional binary RFFs generated from raw data. We train on Intel Xeon CPUs.\n\nResults. 1-and 10-epoch test accuracies are in Table 1, which yield three key observations:\n\n\u2022 RFF HDC already improves non-trivially over the baseline SOTA HDC. With basis hypervectors initialized from the similarity matrix constructed from pixel similarities, our method \n\n\nMethod CDC\n\nPercep. 91 + 96 \u00b7 log 2 N + 3 2 log 2 D \u00b7 (1 + log 2 D) HDC log 2 N + 1 + 3 2 log 2 D \u00b7 (1 + log 2 D) G(2 n )-VSA 3n log 2 N + 24 log 2 D Circuit-Depth Complexity 1-Epoch Accuracy Final Accuracy CDC Figure 1: Cyclic Group VSA on MNIST. improves the MNIST model accuracy by 6.4% compared to the SOTA. It also for the first time enables HDC learning on Fashion-MNIST, a more challenging task, obtaining 84% final accuracy.\n\n\u2022 Group VSA improves the model accuracy further. By extending HDC to group VSA, the vector elements are in a higher complexity so that it can more precisely approximate the target similarity matrix than binary hypervectors. Figure 1 shows that when there are 8 or 16 elements in the group, meaning the precision of each element in the hypervector is 3 or 4 bits, the proposed group VSA strikes a good trade-off between accuracy and complexity. It can further outperform RFF HDC by at least 1% across various datasets. \u2022 Our HDC models learned from a single pass over the data achieve high accuracy. In all the evaluated tasks, our proposed RFF HDC or the extended group VSA can both achieve approximately the final accuracy in one single epoch. In some cases, e.g., Fashion-MNIST, the group VSA can even obtain a better quality with one single pass. The single-pass model accuracy of HDC is significantly better than the baseline perceptrons, especially on the ISOLET and UCIHAR datasets, which has an at least 8% gap. This evidence shows that HDC learning has an impressive data efficiency. This capability of single-pass learning is consistent with finding in prior works [Hernandez-Cane et al., 2021, Imani et al., 2019a.\n\n\nCircuit-Depth Complexity\n\nTo quantify the potential hardware latency of HDC, we analyze its circuit-depth complexity (CDC) in Table 2, defined as the length of the longest path from the input to the output (measured by the number of two-input gates along the path). CDC is commonly used to analyze the complexity of Boolean functions. We further assume that operations without data dependencies are all in parallel. Let N be the feature vector length, e.g., N = 784 for an MNIST image, D be the hypervector dimension.\n\nBinary HDC. The encoding stage binds all feature hypervectors, which can be implemented in a tree structure. The depth of a single binding operation (XNOR) is 1. The total depth is therefore log 2 N . Computing the similarity includes a binding and a bit counting. We assume a B-bit ripple carry adder, a chain of full adders, has a depth of 3 \u00b7 B [Satpathy, 2016]. Therefore a D-bit population count has a depth of 3 + 6 + \u00b7 \u00b7 \u00b7 + 3 log 2 D = 3 2 log 2 D \u00b7 (1 + log 2 D). Cyclic Group VSAs. For a cyclic group VSA of order 2 n , the depth of a single binding is 3n as it is an addition over the group. Therefore, binding all features has a depth of 3n log 2 N . For similarity computations, we precompute the similarity matrix, which consists of S(x, y), \u2200x, y \u2208 G in 8-bit numbers. Hence, the depth of computing the similarity for hypervectors is 3 \u00b7 8 \u00b7 log 2 D.\n\n1-bit RFF perceptron. Projecting a feature vector onto a selected basis requires a depth of a 32-bit multiplier and an adder. A 32-bit Wallece tree multiplier [Wallace, 1964] has roughly a depth of 45. A 32-bit ripple carry adder has a depth of 96. A cosine operation generating random fourier features requires about the same depth as a 32-bit multiplier if computing with the CORDIC algorithm [Volder, 1959]. Since the perceptron is 1-bit, computing the distance has the same depth as that of HDC.\n\nAs a result, binary HDC has a CDC of 295 on MNIST and the cyclic group G(2 3 ) VSA has 405. The complexity of HDC is 4.4\u00d7 lower than a 1-bit RFF perceptron with a depth of 1299, while CDC of the cyclic group G(2 3 ) VSA is 3.2\u00d7 lower. HDC and group VSA are much faster in potential. In Figure 1, we plot the performance and CDC of a cyclic group VSA when the order varies. Our code is available on github 4 .\n\n\nDiscussion\n\nThe performance of various HDC/VSA methods is closely related to the set of its expressible similarity matrices. As a matter of fact, the required similarity matrix to learn (some) tasks in the paper might already be covered by (or close to in terms of Frobenius norm) the set of expressible similarity matrices of the 10k-dimensional RFF HDC. Hence, the improvement from group-VSA can be limited compared to RFF HDC. If instead considering a 1k-dimensional binary RFF HDC with a smaller set of expressible similarity matrices, group-VSA demonstrates a much better accuracy improvement. For example, on MNIST, 1k-dimensional binary RFF HDC achieves 65.59% 10-epoch test accuracy on MNIST. G(2 3 )-VSA, meanwhile, achieves 88.61%, and G(2 4 )-VSA achieves 92.56% test accuracy.\n\nThe circuit depth serves as a preliminary analysis on the hardware complexity of HDC. While other efficient circuits, e.g., a parallel adder instead of a ripple carry adder, will have lower depth and make HDC attractive further, we avoid being over-optimistic on the estimation. For a practical hardware implementation, better circuits should be applied. Besides, circuit depth only reflects the latency. In the future, an estimation on the number of operations will reflect the energy or circuit area and will further improve the analysis.\n\n\nConclusion\n\nFrom our theoretical analysis, there is a clear connection between the class of expressible similarity matrices and the expressivity of HDC/VSA. This new notion of expressivity reveals the limits of HDC that computes with binary hypervectors, and meanwhile provides a hint on how we can improve it. The nontrivial improvement from group VSA and the proposed techniques on HDC across various benchmarks suggests that this notion paves a new way towards the future development of HDC/VSA.\n\n\nA Proofs of Lemmas, Statements and Theorems\n\nLemma 4.1. No binary HDC can express the following similarity matrix\nM = \uf8eb \uf8ed 1 \u2212 1 2 \u2212 1 2 \u2212 1 2 1 \u2212 1 2 \u2212 1 2 \u2212 1 2 1 \uf8f6 \uf8f8 .\nProof. There are n = 3 basic entities, where we have some HDC vectors v 0 , v 1 , v 2 \u2208 R D which can be any dimension. We start from D = 1 case, with the inner product as the similarity measurement, we can easily enumerate all possible similarity matrices as follows:\n1 1 1 1 1 1 1 1 1 , 1 \u22121 1 \u22121 1 \u22121 1 \u22121 1 , 1 1 \u22121 1 1 \u22121 \u22121 \u22121 1 , 1 \u22121 \u22121 \u22121 1 1 \u22121 1 1 . When D > 1, note that S(v i , v j ) = D k=1 S(v ik , v jk )/D ,which\nindicates that all possible similarity matrices must reside in the convex hull of the similarity matrices enumerated above because D can be of any dimension. Easy to verify that this convex hell does not contain M: thus no binary HDC can achieve it.\n\nStatement 4.1. Binary HDC cannot learn the following task. Consider a supervised learning task with input example set X = {0, 1, 2}, output label set Y = X , and source distribution P(x, y) = 1/9 + 2p x = y 1/9 \u2212 p x = y for some small positive number p.\n\nProof. Let \u03c6 : X \u2192 {\u22121, 1} D be any binary HDC encoding, and extend \u03c6(x) = \u03c6(x mod 3) when x > 3. Given a class\u0177, we can then compute the class representative c\u0177 as c\u0177 =\n\nx:y(x)=\u0177\n\u03c6(x) = sgn(E[\u03c6(x)|\u0177]) = sgn[( 1 3 \u2212 3p)(\u03c6(\u0177 + 1) + \u03c6(\u0177 + 2)) + ( 1 3 + 6p)\u03c6(\u0177)] = sgn[( 1 3 \u2212 3p)(\u03c6(\u0177) + \u03c6(\u0177 + 1) + \u03c6(\u0177 + 2)) + 9p\u03c6(\u0177)] .\nNote that \u03c6(\u0177) + \u03c6(\u0177 + 1) + \u03c6(\u0177 + 2) cannot be a zero vector; otherwise,\n0 = S(\u03c6(\u0177), \u03c6(\u0177) + \u03c6(\u0177 + 1) + \u03c6(\u0177 + 2)) = 1 + j =\u0177 S(\u03c6(\u0177), \u03c6(j)),\ndue to symmetry, one can easily derive that S(\u03c6(i), \u03c6(j)) = \u22121/2 for i = j, then M is achieved, a contradiction to Lemma 4.1.\n\nSince p is small positive number, then the sign of E[\u03c6(x)|\u0177] is dominated by the first term \u03c6(\u0177) + \u03c6(\u0177 + 1) + \u03c6(\u0177 + 2). Hence, the class representative of class\u0177 is computed as\nc\u0177 = x:y(x)=\u0177 \u03c6(x) = sgn(E[\u03c6(x)|\u0177])\n= sgn(\u03c6(\u0177) + \u03c6(\u0177 + 1) + \u03c6(\u0177 + 2)), which is same for each class, i.e., binary HDC fails to learn this simple task.\n\nOn the other hand, for any VSA that can express M with v 0 , v 1 , v 2 , set \u03c6(x) = v x . We can compute the class representative c\u0177 as c\u0177 =\n\nx:y(x)=\u0177\n\u03c6(x) = arg max z z, E[\u03c6(x)|\u0177] = arg max z z, ( 1 3 \u2212 3p)(\u03c6(\u0177) + \u03c6(\u0177 + 1) + \u03c6(\u0177 + 2)) + 9p\u03c6(\u0177) = arg max z z, 9p\u03c6(\u0177) = \u03c6(\u0177).\nThe class representative of class\u0177 will be\nc\u0177 = sgn(E[\u03c6(x)|\u0177]) = sgn( 3p 2 \u03c6(\u0177)) = \u03c6(\u0177),\nwhich gives a Bayes optimal classifier, outputs the most probable class and proves Statement 4.2.\n\nStatement 4.2. Any VSA (formalized in Definition 2) that can express M Lemma 4.1 can learn this task. Lemma 4.2. Let u 1 , u 2 , . . . , u K be binary vectors sampled coordinate-wise independently at random, where each coordinate of u i has the same probability p i of being 1. Let v 0 , v 1 , and v 2 be vectors that result from some composition of binding and permutation acting on u 1 , . . . , u K , and let M \u2208 R 3\u00d73 be their similarity matrix, such that\nM ij = S(v i , v j ). Then E[M] \u2212 \uf8eb \uf8ed 1 \u2212 1 3 \u2212 1 3 \u2212 1 3 1 \u2212 1 3 \u2212 1 3 \u2212 1 3 1 \uf8f6 \uf8f8 F \u2265 \u221a 2 3 ,\nbut this target matrix is expressible by some binary HDC.\n\nProof. It is straightforward to show that for some x, y, z \u2208 [\u22121, 1] But since (xy) \u00b7 (xz) \u00b7 (yz) = x 2 y 2 z 2 is a square number, it follows that the upper-triangular elements cannot all be negative. At least one of them must be non-negative, from which the result immediately follows. A binary HDC that achieves this matrix is: (\u22121, 1, 1), (1, \u22121, 1), (1, 1, \u22121).\n\nLemma 5.1. Suppose that X and Y are jointly Gaussian zero-mean unit-variance random variables. Then\nE[sgn(X) sgn(Y )] = 2 \u03c0 arcsin (E[XY ])\nProof. Without loss of generality let U \u223c N (0, I) be a standard Gaussian over R 2 , and suppose that\nX = a T U , Y = b T U for some vectors a, b \u2208 R 2 with a = b = 1 and a T b = E[XY ].\nHere, a geometric argument shows that P(\nX \u2265 0 \u2227 Y \u2264 0) = P(a T U \u2265 0 \u2227 b T U \u2264 0) = \u03b8/(2\u03c0),\nwhere \u03b8 is the angle between a and b. An analogous analysis of the other three cases, combined with some straightforward trigonometry, proves the lemma.\n\nTheorem 6.1. Let (G, \u2297) be a finite group, and let X denote the set of its non-trivial irreducible characters. Let \u03b1 : X \u2192 R \u22650 be some function that assigns a non-negative weight to each of the characters. Then, if we set S to be\nS(g, h) = \u03c7\u2208X \u03b1(\u03c7) \u00b7 Re(\u03c7(g \u22121 \u2297 h)) \u03c7\u2208X \u03b1(\u03c7) \u00b7 \u03c7(1) ,\nwhere the inverse and unit 1 are those of the group (G, \u2297), and define bundling \u2295 as given in the definition of a group VSA, then (G, \u2297, S, \u2295) is a finite group VSA. Any finite group VSA can be constructed in this way.\n\nIf in this construction \u03b1 is supported on only one character \u03c7, i.e. S(g, h) = Re(\u03c7(g \u22121 \u2297 h))/\u03c7(1), then the VSA will have the product property.\n\nProof. The first part of this theorem is a direct consequence of the following more technically-stated theorem.\n\nThe second part follows directly from the fact that if \u03c6 is an irreducible representation of a finite group G, and A is the set of automorphisms of G, then for any g \u2208 G,\n\n1 |A| a\u2208A \u03c6(a(g)) = cI for some scalar c. In particular, this means that if \u03c7 is the corresponding character, then for any g, h \u2208 G,\n1 |A| a\u2208A \u03c7(h \u22121 a(g)) = 1 |A| a\u2208A tr \u03c6(h \u22121 )\u03c6(a(g)) = tr \u03c6(h \u22121 ) 1 |A| a\u2208A \u03c6(a(g)) = tr \u03c6(h \u22121 ) \u00b7 cII = c \u00b7 \u03c7(h \u22121 ).\nSubstituting h = 1 yields that c = Re(\u03c7(g))/\u03c7(1) (since \u03c7 must also be preserved by automorphisms up to complex conjugation), which immediately implies what we wanted to prove.\n\nTheorem A.1 (Representation theorem for group VSAs). Suppose that we have a finite group G equipped with a similarity measure denoted \u00b7|\u00b7 G . Let X denote the set of non-trivial irreducible characters of G (i.e. X is the character table of G excluding the top row). Then there exists a unique function \u03b1 : X \u2192 R \u22650 such that \u03b1(\u03c7) = \u03b1(\u03c7) for all \u03c7 \u2208 X, \u03c7\u2208X \u03b1(\u03c7) \u00b7 \u03c7(1) = 1, and\n\u00b7|\u00b7 G = \u03c7\u2208X \u03b1(\u03c7) \u00b7 \u03c7(g \u22121 h).(2)\nConversely, for any function \u03b1 of this type, if we define \u00b7|\u00b7 according to (2), then \u00b7|\u00b7 will be a similarity measure for G.\n\nAdditionally, if we define M as the matrix such that M gh = g|h G , and d is the rank of M , then there exists some positive integer K and positive integers d 1 , d 2 , . . . , d K such that K k=1 d K = 1, and there exists a |G|-dimensional subspace A of R d1\u00d7d1 \u00d7 R d2\u00d7d2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 R d K \u00d7d K and function \u03c6 : G \u2192 A such that for all g, h \u2208 G,\n\u03c6(g)\u03c6(h) = \u03c6(gh) \u03c6(g) \u22121 = \u03c6(g) T = \u03c6(g \u22121 ) \u03c6(1) = \u03c6(I),\nwhere multiplication and transposition in A is done component-wise on the K components (each of which is a matrix), and such multiplication preserves A. Also, there exist some positive scalars \u03b2 1 , \u03b2 2 , . . . , \u03b2 K such that if we define an inner product on A as (x 1 , x 2 , . . . , x K ), (y 1 , y 2 , . . . ,\ny K ) A = K k=1 \u03b2 k tr x T k y k ,\nwhere here each x k \u2208 R d k \u00d7d k , then\ng|h G = \u03c6(g)|\u03c6(h) A .\nProof. Let M be the matrix described in the theorem statement, such that M gh = g|h G . Observe that M must be symmetric and positive semidefinite (by the properties of the similarity measure). For some f \u2208 G, let B f denote the matrix\nB f = g\u2208G e f g e T g ,\nwhere e g is the unit basis element associated with g. Observe that\nB T f = B \u22121 f = B f \u22121 , and that B f commutes with M , because e T h M B f e g = e T h M e f g = \u03c1(h \u22121 f g) = \u03c1((f \u22121 h) \u22121 g) = e T f \u22121 h M e g = B f \u22121 e h T M e g = e T h B T f \u22121 M e g = e T h B f M e g . That is, M B f = B f M .\nSince B f commutes with M , it also must commute with any polynomial of M . In particular, if the nonzero eigendecomposition of M is\nM = K k=1 \u03bb k V k ,\nwhere each \u03bb k > 0 is distinct, and V 2 k = V k is a symmetric projection matrix onto the associated eigenspace, then since V k can be expressed as a polynomial in M , it must also commute with B f for all f . The same thing will be true for C f defined as\nC f = g\u2208G e gf e T g .\nThese results together show that for any f, g, h \u2208 G,\ne T h V k e g = e T f h V k e f g = e T hf V k e gf .\nNow, let d k denote the rank of V k (the multiplicity of the eigenvalue \u03bb k in M ). So, there must exist some matrix W k \u2208 R |G|\u00d7d k such that V k = W k W T k and W T k W k = I. For any g \u2208 G, let U k (g) denote the matrix in R d k \u00d7d k U k (g) = W T k B g W k . Observe that U k (1) = I, all the U k (g) matrices are orthogonal, and\nU k (g)U k (h) = W T k B g W k W T k B h W k = W T k B g V k B h W k = W T k B g B h V k W k = W T k B gh W k = U k (gh).\nThat is, U k is a representation of the group G. Observe that the trace of U k (g) is\ntr (U k (g)) = tr W T k B g W k = tr W k W T k B g = tr (V k B g ) = h\u2208G e T h V k B g e h = h\u2208G e T h V k e gh = h\u2208G e T 1 V k e g = |G| \u00b7 e T 1 V k e g .\nIn particular, this means that g \u2192 |G| \u00b7 e T 1 V k e g is a character of G. As a character, it must be a sum of irreducible characters of G, and since it is real, it must place the same weight on complex-conjugate characters. It follows that g \u2192 K k=1 \u03bb k e T 1 V k e g must be a non-negative scaled sum of irreducible characters of G that places the same weight on complex-conjugate characters. But, this function is just g \u2192 e T 1 M e g , which is just \u03c1. So, \u03c1 must be a non-negative sum of the irreducible characters of G. The fact that this scaling is unique follows from the fact that the characters are linearly independent; the fact that this scaling only contains non-trivial characters follows from the average-dissimilarity property.\n\nWe then construct an algebra A as follows. Let \u03c6(g) : G \u2192 R d1\u00d7d1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 R d K \u00d7d K be defined as \u03c6(g) = (U 1 (g), U 2 (g), . . . , U K (g)),\n\nand let the inner product scalars be\n\u03b2 k = \u03bb k |G| . Then K k=1 \u03b2 k tr U k (h) T U k (g) = K k=1 \u03b2 k tr U k (h \u22121 )U k (g) = K k=1 \u03b2 k tr U k (h \u22121 g) = e T 1 M e h \u22121 g = \u03c1(h \u22121 g) = h|g G as desired.\nTo finish the construction, let A be the algebra spanned by {\u03c6(g) | g \u2208 G}. Observe that this must be closed under multiplication and transposition because G is closed under multiplication and inversion. Statement 6.1. Let M be similarity matrices expressible by a finite group VSA. Then there exists a finite group VSA that has the product property and can also achieve M.\n\nProof. Suppose the first VSA's group is G and has irreducible characters \u03c7 1 , \u03c7 2 , . . . , \u03c7 k . Then the group G k consisting of the direct product of k copies of the group G, together with a similarity function S((x 1 , . . . , x k ), (y 1 , . . . , y k )) \u221d k i=1 \u03c7 i (x \u22121 i \u2297 y i ) will both have the product property (as its similarity matrix is proportional to a single character) and can express any similarity matrix the first VSA can. Statement 6.2. Any similarity matrix M that can be expressed by a finite Abelian group VSA can be expressed by the unit-cycle VSA (G = {z \u2208 C | |z| = 1}, x \u2297 y = xy, S(x, y) = Re(x * y)).\n\nProof. Note the fact that all irreducible representations of finite Abelian groups are 1-dimensional. Then the results follows immediately with any irreducible representation as the mapping from a finite Abelian group to G = {z \u2208 C | |z| = 1}. Statement 6.3. There exists a similarity matrix M that can be expressed by a VSA over the (non-Abelian) binary icosahedral group, but not by the unit-cycle VSA.\n\nProof. Consider the binary icosahedral group expressed as a subset of the quaternions. This group consists of 120 elements which are placed at the vertices of a 600-cell inscribed in the unit 3-sphere. Consider an arbitrary sequence of 60 of these elements containing exactly one of {x, \u2212x} for each x in the binary icosahedral group: i.e. we select exactly one of each pair of antipodal points in the group. This sequence of 60 points has a similarity matrix M \u2208 R 60\u00d760 . It is easy to check that the absolute values of the entries of this matrix lie in {0, 1 2\u03c6 , 1 2 , \u03c6 2 , 1}, where \u03c6 = 1+ \u221a 5 2 is the golden ratio. Define the matrix A \u2208 R 60\u00d760 such that A ij = \u00b11 if M ij = \u00b1 \u03c6 2 and A ij = 0 otherwise. Consider the optimization problem to maximize tr(AZ) over positive semidefinite matrices Z \u2208 R 60\u00d760 subject to the constraint that the diagonal of Z is all-ones, i.e. Z ii = 1. It is easy to check numerically that the only solution to this optimization problem is Z = M . Also observe that M has rank greater than 2. Now, suppose that there existed some representation of M using vectors with entries in the unit circle in C. For this to hold, it would need to be the case that M is in the convex combination of the similarity matrices generated by those entries, each of which must be of rank 2. But, this is impossible, since (1) none of those matrices can be equal to M , and as such (2) any such matrix M C will have tr(M C A) < tr(M A). This shows that this particular matrix can't be represented by hypervectors with unit-absolute-value entries in C.\n\n\nB Calculation of \u03b8 in Section 7\n\nWe can theoretically calculate the angle \u03b8 between the class vector s c and a randomly selected hypervector t i in the set T c .\ncos\u03b8 = s c \u00b7 v j s c v j = 2 \u00b7 q \u2212 D D\nD is the dimension of t i . q is the number of elements that have the same sign in s c and t i . Since\ns c = sgn \uf8eb \uf8ed j\u2208Tc t j \uf8f6 \uf8f8 = sgn \uf8eb \uf8ed t i + 2k+1 j=1,j =i t j \uf8f6 \uf8f8 ,\nq is proportional to the probability p k of the sign of an entry in t i will be flipped after adding the term 2k+1 j=1,j =i t j (the other 2k vectors) to it. Obviously q = D \u00b7 p k as each entry in t i is independent. In order to avoid flipping the sign of an entry, there should be at least k entries out of 2k that have the same sign, so the probability p k can be calculated by p k = 2k k + 2k k+1 + \u00b7 \u00b7 \u00b7 + 2k 2k 2 2k = 1 + 1 2 2k 2k k 2 Plug the expression of p k into cos\u03b8, we can get \u03b8 2k+1 = cos \u22121 ( 1 2 2k \u00b7 2k k ) Importantly, p k is monotonic.\n\n\nProof.\n\np k+1 = 1 + 1 2 2k \u00b7 1 4 \u00b7 2k+2 k+1 2 = 1 + 1 2 2k \u00b7 1 4 \u00b7 [ 2k+1 k+1 + 2k+1 k ] 2 = 1 + 1 2 2k \u00b7 1 4 \u00b7 [ 2k k+1 + 2k k + 2k k + 2k k\u22121 ] 2 < 1 + 1 2 2k \u00b7 1 4 \u00b7 4 \u00b7 2k k 2 = p k This means that the more vectors we bundle together, the closer \u03b8 is to 90 degrees.\n\n\nC Learning with Group VSA\n\nFor a cyclic group VSA model, similar as the binary HDC case, we initialize a linear model with weights W of size # class \u00d7D, where each element belongs to G = Z/nZ = {0, 1, \u00b7 \u00b7 \u00b7 , n \u2212 1}. Inputs to this classifier are encoded hypervectors v \u2208 G D , the model computes per-class similarities with defined similarity function:\n\nS(x, y) = \u03c8(x), \u03c8(y) = cos(2\u03c0(x \u2212 y)/n), \u2200x, y \u2208 G, which extends to higher dimensional space via S(x i , y i ).\n\nWe calculate the cross-entropy loss between the classifier outputs and the labels, and do back propagation using high precision numbers such as 16-bit floats. At each optimizer step, SGD optimizer moves W away from G # class \u00d7D , we pull it back with (Fast)-Round operation to finish current step before entering next step. Similar as the binary case, the inference cost remains the same as the bundling method.\n\nLemma 4 . 1 .\n41Binary HDC can not express the matrix M Lemma 4.1 = 3 2 I 3 \u2212 1 2 1 3\u00d73 .\n\nS\n([x 1 , . . . , x D ], [y 1 , . . . , y D ]\n\nTable 1 :\n1Comparison on test accuracy of proposed methods to SOTA HDC \u2020[Imani et al., 2019a], dynamic HDC *[Chuang et al., 2020] and 1-bit RFF perceptron. Dimension of hypervectors is 10,000. 1-Epo: 1-Epoch, 10-Epo: 10-Epoch.Dataset \nISOLET \nUCIHAR \nMNIST \nFashion-MNIST \n\nAcc(%) \n1-Epo 10-Epo 1-Epo 10-Epo 1-Epo 10-Epo 1-Epo 10-Epo \n\nPercep. \n82.8 \n90.1 \n69.3 \n91.4 \n94.3 \n94.3 \n79.5 \n79.5 \nHDC  \u2020 \n85.6 \n91.5 \n87.3 \n95.7 \nNA \n89.0 \nNA \nNA \nRFF HDC \n90.6 \n94.4 \n93.8 \n95.7 \n95.4 \n95.4 \n83.4 \n84.0 \nRFF G(2 3 )-VSA \n93.1 \n94.4 \n95.1 \n95.6 \n96.3 \n95.7 \n85.4 \n86.7 \nRFF G(2 4 )-VSA \n94.4 \n96.0 \n95.5 \n96.6 \n96.5 \n96.6 \n87.4 \n86.5 \n\n\n\nTable 2 :\n2Analysis of circuit-depth complexity of binary HDC and 1-bit RFF perceptron.\nThe calculation is in appendix. 3 Hernandez-Cane et al.[2021]  seem to have a better result on MNIST inFigure 7, but no concrete numbers are reported.\nhttps://github.com/Cornell-RelaxML/Hyperdimensional-Computing\nAcknowledgments and Disclosure of FundingThis work is supported in part by NSF Awards IIS-2008102 and CCF-2007832, and by CRISP, one of six centers in JUMP, a Semiconductor Research Corporation program sponsored by DARPA. The authors would like to thank Denis Kleyko from the Redwood Center for Theoretical Neuroscience at UC Berkeley and researchers from VSAONLINE for providing valuable feedbacks on earlier versions of this paper.The datasplit is default to each dataset. Other details are included in the paper. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?[No] Due to the time limit, we did not include multiple runs for each experiment. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\nHuman activity recognition on smartphones using a multiclass hardware-friendly support vector machine. Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge L Reyes-Ortiz, Jos\u00e9 Bravo, Ram\u00f3n Herv\u00e1s, and Marcela Rodr\u00edguez. Berlin HeidelbergSpringerAmbient Assisted Living and Home CareDavide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and Jorge L. Reyes-Ortiz. Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine. In Jos\u00e9 Bravo, Ram\u00f3n Herv\u00e1s, and Marcela Rodr\u00edguez, editors, Ambient Assisted Living and Home Care, pages 216-223. Springer Berlin Heidelberg, 2012.\n\nHypervector design for efficient hyperdimensional computing on edge devices. Toygun Basaklar, Yigit Tuncel, Yadav Shruti, Suat Narayana, Umit Y Gumussoy, Ogras, Research Symposium on Tiny Machine Learning. Toygun Basaklar, Yigit Tuncel, Shruti Yadav Narayana, Suat Gumussoy, and Umit Y. Ogras. Hyper- vector design for efficient hyperdimensional computing on edge devices. In Research Symposium on Tiny Machine Learning, 2021. URL https://openreview.net/forum?id=JGNDej3tup9.\n\nLaelaps: An energy-efficient seizure detection algorithm from long-term human ieeg recordings without false alarms. Alessio Burrello, Lukas Cavigelli, Kaspar Schindler, Luca Benini, Abbas Rahimi, 10.23919/DATE.2019.87151862019 Design, Automation and Test in Europe Conference and Exhibition (DATE). Alessio Burrello, Lukas Cavigelli, Kaspar Schindler, Luca Benini, and Abbas Rahimi. Laelaps: An energy-efficient seizure detection algorithm from long-term human ieeg recordings without false alarms. In 2019 Design, Automation and Test in Europe Conference and Exhibition (DATE), pages 752-757, 2019. doi: 10.23919/DATE.2019.8715186.\n\nDynamic hyperdimensional computing for improving accuracy-energy efficiency trade-offs. Yu-Chuan Chuang, Cheng-Yang Chang, An-Yeu Andy Wu, 2020 IEEE Workshop on Signal Processing Systems (SiPS). Yu-Chuan Chuang, Cheng-Yang Chang, and An-Yeu Andy Wu. Dynamic hyperdimensional com- puting for improving accuracy-energy efficiency trade-offs. In 2020 IEEE Workshop on Signal Processing Systems (SiPS), pages 1-5, 2020.\n\nUCI machine learning repository. Dheeru Dua, Casey Graff, Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics. uci.edu/ml.\n\nA theory of sequence indexing and working memory in recurrent neural networks. Denis E Paxon Frady, Friedrich T Kleyko, Sommer, Neural Computation. 306E Paxon Frady, Denis Kleyko, and Friedrich T Sommer. A theory of sequence indexing and working memory in recurrent neural networks. Neural Computation, 30(6):1449-1513, 2018.\n\nE , Paxon Frady, Denis Kleyko, Christopher J Kymn, Bruno A Olshausen, Friedrich T Sommer, Computing on functions using randomized vector representations. E. Paxon Frady, Denis Kleyko, Christopher J. Kymn, Bruno A. Olshausen, and Friedrich T. Sommer. Computing on functions using randomized vector representations, 2021.\n\nRepresentation theory: a first course. William Fulton, Joe Harris, Springer Science & Business Media129William Fulton and Joe Harris. Representation theory: a first course, volume 129. Springer Science & Business Media, 2013.\n\nRepresenting Objects, Relations, and Sequences. I Stephen, T Wendy Gallant, Okaywe, 10.1162/NECO_a_00467Neural Computation. 258Stephen I. Gallant and T. Wendy Okaywe. Representing Objects, Relations, and Sequences. Neural Computation, 25(8):2038-2078, 08 2013. ISSN 0899-7667. doi: 10.1162/NECO_a_00467.\n\nMultiplicative binding, representation operators & analogy (workshop poster). W Ross, Gayler, Ross W Gayler. Multiplicative binding, representation operators & analogy (workshop poster). 1998.\n\nClassification using hyperdimensional computing: A review. IEEE Circuits and Systems Magazine. Lulu Ge, Keshab K Parhi, 10.1109/MCAS.2020.298838820Lulu Ge and Keshab K. Parhi. Classification using hyperdimensional computing: A review. IEEE Circuits and Systems Magazine, 20(2):30-47, 2020. ISSN 1558-0830. doi: 10.1109/mcas.2020. 2988388. URL http://dx.doi.org/10.1109/MCAS.2020.2988388.\n\nVector-Derived Transformation Binding: An Improved Binding Operation for Deep Symbol-Like Processing in Neural Networks. Jan Gosmann, Chris Eliasmith, 10.1162/neco_a_01179Neural Computation. 315Jan Gosmann and Chris Eliasmith. Vector-Derived Transformation Binding: An Improved Binding Operation for Deep Symbol-Like Processing in Neural Networks. Neural Computation, 31(5): 849-869, 05 2019. ISSN 0899-7667. doi: 10.1162/neco_a_01179. URL https://doi.org/10. 1162/neco_a_01179.\n\nHyperrec: Efficient recommender systems with hyperdimensional computing. Yunhui Guo, Mohsen Imani, Jaeyoung Kang, Sahand Salamat, Justin Morris, Baris Aksanli, Yeseong Kim, Tajana Rosing, 10.1145/3394885.3431553Proceedings of the 26th Asia and South Pacific Design Automation Conference, ASPDAC '21. the 26th Asia and South Pacific Design Automation Conference, ASPDAC '21Association for Computing MachineryYunhui Guo, Mohsen Imani, Jaeyoung Kang, Sahand Salamat, Justin Morris, Baris Aksanli, Yeseong Kim, and Tajana Rosing. Hyperrec: Efficient recommender systems with hyperdimensional computing. In Proceedings of the 26th Asia and South Pacific Design Automation Conference, ASPDAC '21, page 384-389. Association for Computing Machinery, 2021. ISBN 9781450379991. doi: 10.1145/3394885.3431553.\n\nThrifty: Training with hyperdimensional computing across flash hierarchy. Saransh Gupta, Justin Morris, Mohsen Imani, Ranganathan Ramkumar, Jeffrey Yu, Aniket Tiwari, Baris Aksanli, Tajana Rosing, 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD). Saransh Gupta, Justin Morris, Mohsen Imani, Ranganathan Ramkumar, Jeffrey Yu, Aniket Tiwari, Baris Aksanli, and Tajana Rosing. Thrifty: Training with hyperdimensional computing across flash hierarchy. In 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD), pages 1-9, 2020.\n\nOnlinehd: Robust, efficient, and single-pass online learning using hyperdimensional system. Alejandro Hernandez-Cane, Namiko Matsumoto, Eric Ping, Mohsen Imani, 10.23919/DATE51398.2021.94741072021 Design, Automation and Test in Europe Conference and Exhibition (DATE). Alejandro Hernandez-Cane, Namiko Matsumoto, Eric Ping, and Mohsen Imani. Onlinehd: Ro- bust, efficient, and single-pass online learning using hyperdimensional system. In 2021 Design, Automation and Test in Europe Conference and Exhibition (DATE), pages 56-61, 2021. doi: 10.23919/DATE51398.2021.9474107.\n\nNeural networks and physical systems with emergent collective computational abilities. J J Hopfield, 10.1073/pnas.79.8.2554Proceedings of the National Academy of Sciences. 798J J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, 79(8):2554-2558, 1982. ISSN 0027-8424. doi: 10.1073/pnas.79.8.2554. URL https://www.pnas.org/content/79/8/2554.\n\nBinarized neural networks. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio, Advances in Neural Information Processing Systems. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks. In Advances in Neural Information Processing Systems, 2016.\n\nA binary learning framework for hyperdimensional computing. Mohsen Imani, John Messerly, Fan Wu, Wang Pi, Tajana Rosing, 10.23919/DATE.2019.87148212019 Design, Automation and Test in Europe Conference and Exhibition (DATE). Mohsen Imani, John Messerly, Fan Wu, Wang Pi, and Tajana Rosing. A binary learning framework for hyperdimensional computing. In 2019 Design, Automation and Test in Europe Conference and Exhibition (DATE), pages 126-131, 2019a. doi: 10.23919/DATE.2019.8714821.\n\nBric: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing. Mohsen Imani, Justin Morris, John Messerly, Helen Shu, Yaobang Deng, Tajana Rosing, Proceedings of the 56th Annual Design Automation Conference. the 56th Annual Design Automation ConferenceMohsen Imani, Justin Morris, John Messerly, Helen Shu, Yaobang Deng, and Tajana Rosing. Bric: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing. In Proceedings of the 56th Annual Design Automation Conference 2019, pages 1-6, 2019b.\n\nDual: Acceleration of clustering algorithms using digital-based processing in-memory. Mohsen Imani, Saikishan Pampana, Saransh Gupta, Minxuan Zhou, Yeseong Kim, Tajana Rosing, 10.1109/MICRO50266.2020.000392020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). Mohsen Imani, Saikishan Pampana, Saransh Gupta, Minxuan Zhou, Yeseong Kim, and Tajana Rosing. Dual: Acceleration of clustering algorithms using digital-based processing in-memory. In 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 356-371, 2020. doi: 10.1109/MICRO50266.2020.00039.\n\nRevisiting hyperdimensional learning for fpga and low-power architectures. Mohsen Imani, Zhuowen Zou, Samuel Bosch, Sahand Sanjay Anantha Rao, Venkatesh Salamat, Yeseong Kumar, Tajana Kim, Rosing, 10.1109/HPCA51647.2021.000282021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). Mohsen Imani, Zhuowen Zou, Samuel Bosch, Sanjay Anantha Rao, Sahand Salamat, Venkatesh Kumar, Yeseong Kim, and Tajana Rosing. Revisiting hyperdimensional learning for fpga and low-power architectures. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pages 221-234, 2021. doi: 10.1109/HPCA51647.2021.00028.\n\nRepresentations and characters of groups. Gordon James, W Martin, Martin Liebeck, Liebeck, Cambridge University PressGordon James, Martin W Liebeck, and Martin Liebeck. Representations and characters of groups. Cambridge University Press, 2001.\n\nThe spatter code for encoding concepts at many levels. Pentti Kanerva, International Conference on Artificial Neural Networks. SpringerPentti Kanerva. The spatter code for encoding concepts at many levels. In International Conference on Artificial Neural Networks, pages 226-229. Springer, 1994.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. Pentti Kanerva, Cogn. Comput. Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributed represen- tation with high-dimensional random vectors. Cogn. Comput., pages 139-159, 2009.\n\n. Pentti Kanerva, Fully distributed representation. PAT. 1510000Pentti Kanerva et al. Fully distributed representation. PAT, 1(5):10000, 1997.\n\nGeniehd: Efficient dna pattern matching accelerator using hyperdimensional computing. Yeseong Kim, Mohsen Imani, Niema Moshiri, Tajana Rosing, 10.23919/DATE48585.2020.91163972020 Design, Automation and Test in Europe Conference and Exhibition (DATE). Yeseong Kim, Mohsen Imani, Niema Moshiri, and Tajana Rosing. Geniehd: Efficient dna pattern matching accelerator using hyperdimensional computing. In 2020 Design, Automation and Test in Europe Conference and Exhibition (DATE), pages 115-120, 2020. doi: 10.23919/DATE48585. 2020.9116397.\n\nHolographic graph neuron: A bioinspired architecture for pattern processing. Denis Kleyko, Evgeny Osipov, Alexander Senior, Ya\u015far Asad I Khan, Ahmet\u015fekerciogglu, IEEE transactions on neural networks and learning systems. 28Denis Kleyko, Evgeny Osipov, Alexander Senior, Asad I Khan, and Ya\u015far Ahmet\u015eekerciogglu. Holographic graph neuron: A bioinspired architecture for pattern processing. IEEE transactions on neural networks and learning systems, 28(6):1250-1262, 2016.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\nLearning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception. A Mitrokhin, P Sutor, C Ferm\u00fcller, Y Aloimonos, 10.1126/scirobotics.aaw6736Science Robotics. 4306736A. Mitrokhin, P. Sutor, C. Ferm\u00fcller, and Y. Aloimonos. Learning sensorimotor control with neuromorphic sensors: Toward hyperdimensional active perception. Science Robotics, 4(30): eaaw6736, 2019. doi: 10.1126/scirobotics.aaw6736.\n\nHyperdimensional computing as a framework for systematic aggregation of image descriptors. Peer Neubert, Stefan Schubert, Peer Neubert and Stefan Schubert. Hyperdimensional computing as a framework for systematic aggregation of image descriptors, 2021.\n\nAn introduction to hyperdimensional computing for robotics. Peer Neubert, Stefan Schubert, Peter Protzel, 10.1007/s13218-019-00623-z33KI -K\u00fcnstliche IntelligenzPeer Neubert, Stefan Schubert, and Peter Protzel. An introduction to hyperdimensional computing for robotics. KI -K\u00fcnstliche Intelligenz, 33, 09 2019. doi: 10.1007/s13218-019-00623-z.\n\nHolographic reduced representations. T A Plate, 10.1109/72.377968IEEE Transactions on Neural Networks. 63T.A. Plate. Holographic reduced representations. IEEE Transactions on Neural Networks, 6(3): 623-641, 1995. doi: 10.1109/72.377968.\n\nDistributed representations and nested compositional structure. A Tony, Plate, CiteseerTony A Plate. Distributed representations and nested compositional structure. Citeseer, 1994.\n\nRandom features for large-scale kernel machines. Ali Rahimi, Benjamin Recht, Proceedings of the 20th International Conference on Neural Information Processing Systems, NIPS'07. the 20th International Conference on Neural Information Processing Systems, NIPS'079781605603520Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Proceedings of the 20th International Conference on Neural Information Processing Systems, NIPS'07, page 1177-1184, 2007. ISBN 9781605603520.\n\nThe perceptron: a probabilistic model for information storage and organization in the brain. Frank Rosenblatt, Psychological review. 656386Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386, 1958.\n\nF5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing. Sahand Salamat, Mohsen Imani, Behnam Khaleghi, Tajana Rosing, Proceedings of the. theSahand Salamat, Mohsen Imani, Behnam Khaleghi, and Tajana Rosing. F5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing. In Proceedings of the 2019\n\n9781450361378. doi: 10.1145/ 3289602.3293913ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA '19. Association for Computing MachineryACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA '19, page 53-62. Association for Computing Machinery, 2019. ISBN 9781450361378. doi: 10.1145/ 3289602.3293913.\n\nDesign and Implementation of Carry Select Adder Using T-Spice. Pinaki Satpathy, Anchor Academic PublishingPinaki Satpathy. Design and Implementation of Carry Select Adder Using T-Spice. Anchor Academic Publishing, 2016.\n\nA comparison of vector symbolic architectures. Kenny Schlegel, Peer Neubert, Peter Protzel, 10.1007/s10462-021-10110-3Artificial Intelligence Review. Kenny Schlegel, Peer Neubert, and Peter Protzel. A comparison of vector symbolic architectures. Artificial Intelligence Review, Dec 2021. ISSN 1573-7462. doi: 10.1007/s10462-021-10110-3. URL http://dx.doi.org/10.1007/s10462-021-10110-3.\n\nLinear representations of finite groups. Jean-Pierre Serre, Springer42Jean-Pierre Serre. Linear representations of finite groups, volume 42. Springer, 1977.\n\nA random walk in hamming space. Derek Smith, Paul Stanford, IJCNN International Joint Conference on Neural Networks. IEEEDerek Smith and Paul Stanford. A random walk in hamming space. In 1990 IJCNN International Joint Conference on Neural Networks, pages 465-470. IEEE, 1990.\n\nTheoretical foundations of hyperdimensional computing. Anthony Thomas, Sanjoy Dasgupta, Tajana Rosing, Anthony Thomas, Sanjoy Dasgupta, and Tajana Rosing. Theoretical foundations of hyperdimensional computing, 2020.\n\nA primer on kernel methods. Jean-Philippe Vert, Koji Tsuda, Bernhard Sch\u00f6lkopf, Kernel methods in computational biology. 47Jean-Philippe Vert, Koji Tsuda, and Bernhard Sch\u00f6lkopf. A primer on kernel methods. Kernel methods in computational biology, 47:35-70, 2004.\n\nThe cordic trigonometric computing technique. Jack E Volder, 10.1109/TEC.1959.5222693IRE Transactions on Electronic Computers, EC. 83Jack E. Volder. The cordic trigonometric computing technique. IRE Transactions on Electronic Computers, EC-8(3):330-334, 1959. doi: 10.1109/TEC.1959.5222693.\n\nA suggestion for a fast multiplier. C S Wallace, 10.1109/PGEC.1964.263830IEEE Transactions on Electronic Computers, EC. 131C. S. Wallace. A suggestion for a fast multiplier. IEEE Transactions on Electronic Computers, EC-13 (1):14-17, 1964. doi: 10.1109/PGEC.1964.263830.\n\nFashion-mnist: a novel image dataset for benchmarking machine learning algorithms. Han Xiao, Kashif Rasul, Roland Vollgraf, Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.\n", "annotations": {"author": "[{\"end\":163,\"start\":78},{\"end\":254,\"start\":164},{\"end\":364,\"start\":255},{\"end\":461,\"start\":365}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":82},{\"end\":175,\"start\":170},{\"end\":266,\"start\":261},{\"end\":382,\"start\":377}]", "author_first_name": "[{\"end\":81,\"start\":78},{\"end\":169,\"start\":164},{\"end\":260,\"start\":255},{\"end\":376,\"start\":365}]", "author_affiliation": "[{\"end\":162,\"start\":86},{\"end\":253,\"start\":177},{\"end\":363,\"start\":287},{\"end\":460,\"start\":384}]", "title": "[{\"end\":75,\"start\":1},{\"end\":536,\"start\":462}]", "venue": null, "abstract": "[{\"end\":1846,\"start\":538}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2382,\"start\":2367},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2597,\"start\":2575},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2836,\"start\":2813},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2858,\"start\":2836},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2886,\"start\":2858},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2909,\"start\":2886},{\"end\":2927,\"start\":2909},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3146,\"start\":3125},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3215,\"start\":3196},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3443,\"start\":3422},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3466,\"start\":3443},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3638,\"start\":3626},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3664,\"start\":3638},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3678,\"start\":3664},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3691,\"start\":3678},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3804,\"start\":3781},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5635,\"start\":5611},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6792,\"start\":6778},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6866,\"start\":6843},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6986,\"start\":6972},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7008,\"start\":6986},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7139,\"start\":7127},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7165,\"start\":7139},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7179,\"start\":7165},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7208,\"start\":7179},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7241,\"start\":7228},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7444,\"start\":7423},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7467,\"start\":7444},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7488,\"start\":7467},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7508,\"start\":7488},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7892,\"start\":7876},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8287,\"start\":8265},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8310,\"start\":8287},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8505,\"start\":8486},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8526,\"start\":8505},{\"end\":8795,\"start\":8789},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9231,\"start\":9212},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9561,\"start\":9546},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10380,\"start\":10360},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10502,\"start\":10487},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16316,\"start\":16294},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16343,\"start\":16318},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16365,\"start\":16345},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19262,\"start\":19243},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19622,\"start\":19609},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22084,\"start\":22071},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22119,\"start\":22106},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24810,\"start\":24790},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24836,\"start\":24812},{\"end\":25462,\"start\":25438},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25475,\"start\":25462},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27129,\"start\":27116},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30876,\"start\":30855},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31368,\"start\":31347},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31402,\"start\":31380},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":31478,\"start\":31459},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32436,\"start\":32416},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32457,\"start\":32436},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32598,\"start\":32580},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":34614,\"start\":34586},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34635,\"start\":34614},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":35522,\"start\":35506},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36199,\"start\":36184},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36434,\"start\":36420},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":53734,\"start\":53713},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":53770,\"start\":53749}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53592,\"start\":53502},{\"attributes\":{\"id\":\"fig_3\"},\"end\":53639,\"start\":53593},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54272,\"start\":53640},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54361,\"start\":54273}]", "paragraph": "[{\"end\":2928,\"start\":1862},{\"end\":3217,\"start\":2930},{\"end\":4106,\"start\":3219},{\"end\":4764,\"start\":4108},{\"end\":5393,\"start\":4766},{\"end\":6124,\"start\":5395},{\"end\":6159,\"start\":6126},{\"end\":6724,\"start\":6161},{\"end\":7009,\"start\":6741},{\"end\":7776,\"start\":7011},{\"end\":8037,\"start\":7778},{\"end\":8405,\"start\":8039},{\"end\":8978,\"start\":8407},{\"end\":9232,\"start\":9000},{\"end\":10205,\"start\":9234},{\"end\":10381,\"start\":10207},{\"end\":10503,\"start\":10412},{\"end\":11113,\"start\":10505},{\"end\":11359,\"start\":11115},{\"end\":11629,\"start\":11476},{\"end\":12101,\"start\":11631},{\"end\":12227,\"start\":12127},{\"end\":12909,\"start\":12229},{\"end\":13223,\"start\":12911},{\"end\":13392,\"start\":13225},{\"end\":13637,\"start\":13438},{\"end\":13728,\"start\":13639},{\"end\":14010,\"start\":13730},{\"end\":15195,\"start\":14012},{\"end\":15339,\"start\":15197},{\"end\":15649,\"start\":15341},{\"end\":16494,\"start\":15651},{\"end\":16816,\"start\":16496},{\"end\":17007,\"start\":16953},{\"end\":17313,\"start\":17041},{\"end\":18330,\"start\":17563},{\"end\":19376,\"start\":18371},{\"end\":20087,\"start\":19378},{\"end\":20680,\"start\":20089},{\"end\":20785,\"start\":20682},{\"end\":20835,\"start\":20787},{\"end\":20936,\"start\":20837},{\"end\":21060,\"start\":20938},{\"end\":21285,\"start\":21168},{\"end\":21680,\"start\":21464},{\"end\":22230,\"start\":21754},{\"end\":22726,\"start\":22327},{\"end\":23171,\"start\":22786},{\"end\":23545,\"start\":23173},{\"end\":24265,\"start\":23663},{\"end\":24964,\"start\":24375},{\"end\":25283,\"start\":24966},{\"end\":25476,\"start\":25285},{\"end\":26084,\"start\":25478},{\"end\":26285,\"start\":26086},{\"end\":26456,\"start\":26287},{\"end\":26735,\"start\":26477},{\"end\":26793,\"start\":26737},{\"end\":26889,\"start\":26795},{\"end\":26958,\"start\":26891},{\"end\":27743,\"start\":26960},{\"end\":28179,\"start\":27745},{\"end\":29750,\"start\":28213},{\"end\":30099,\"start\":29791},{\"end\":30477,\"start\":30101},{\"end\":30964,\"start\":30479},{\"end\":31232,\"start\":30966},{\"end\":32019,\"start\":31248},{\"end\":32700,\"start\":32021},{\"end\":32793,\"start\":32702},{\"end\":32975,\"start\":32795},{\"end\":33410,\"start\":32990},{\"end\":34636,\"start\":33412},{\"end\":35156,\"start\":34665},{\"end\":36023,\"start\":35158},{\"end\":36524,\"start\":36025},{\"end\":36934,\"start\":36526},{\"end\":37725,\"start\":36949},{\"end\":38267,\"start\":37727},{\"end\":38768,\"start\":38282},{\"end\":38884,\"start\":38816},{\"end\":39209,\"start\":38941},{\"end\":39620,\"start\":39371},{\"end\":39876,\"start\":39622},{\"end\":40047,\"start\":39878},{\"end\":40057,\"start\":40049},{\"end\":40268,\"start\":40196},{\"end\":40460,\"start\":40335},{\"end\":40638,\"start\":40462},{\"end\":40789,\"start\":40675},{\"end\":40931,\"start\":40791},{\"end\":40941,\"start\":40933},{\"end\":41108,\"start\":41066},{\"end\":41252,\"start\":41155},{\"end\":41713,\"start\":41254},{\"end\":41867,\"start\":41810},{\"end\":42235,\"start\":41869},{\"end\":42336,\"start\":42237},{\"end\":42478,\"start\":42377},{\"end\":42604,\"start\":42564},{\"end\":42809,\"start\":42657},{\"end\":43041,\"start\":42811},{\"end\":43315,\"start\":43097},{\"end\":43462,\"start\":43317},{\"end\":43575,\"start\":43464},{\"end\":43747,\"start\":43577},{\"end\":43881,\"start\":43749},{\"end\":44180,\"start\":44004},{\"end\":44558,\"start\":44182},{\"end\":44716,\"start\":44592},{\"end\":45062,\"start\":44718},{\"end\":45434,\"start\":45121},{\"end\":45509,\"start\":45470},{\"end\":45767,\"start\":45532},{\"end\":45859,\"start\":45792},{\"end\":46230,\"start\":46098},{\"end\":46507,\"start\":46251},{\"end\":46584,\"start\":46531},{\"end\":46972,\"start\":46639},{\"end\":47180,\"start\":47095},{\"end\":48081,\"start\":47337},{\"end\":48227,\"start\":48083},{\"end\":48265,\"start\":48229},{\"end\":48804,\"start\":48431},{\"end\":49440,\"start\":48806},{\"end\":49846,\"start\":49442},{\"end\":51418,\"start\":49848},{\"end\":51582,\"start\":51454},{\"end\":51724,\"start\":51622},{\"end\":52346,\"start\":51792},{\"end\":52618,\"start\":52357},{\"end\":52974,\"start\":52648},{\"end\":53088,\"start\":52976},{\"end\":53501,\"start\":53090}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10411,\"start\":10382},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11475,\"start\":11360},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16952,\"start\":16817},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17562,\"start\":17362},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18370,\"start\":18331},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21167,\"start\":21061},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21463,\"start\":21286},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21753,\"start\":21681},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22326,\"start\":22231},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22785,\"start\":22727},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23662,\"start\":23546},{\"attributes\":{\"id\":\"formula_11\"},\"end\":24333,\"start\":24266},{\"attributes\":{\"id\":\"formula_12\"},\"end\":38940,\"start\":38885},{\"attributes\":{\"id\":\"formula_13\"},\"end\":39370,\"start\":39210},{\"attributes\":{\"id\":\"formula_14\"},\"end\":40195,\"start\":40058},{\"attributes\":{\"id\":\"formula_15\"},\"end\":40334,\"start\":40269},{\"attributes\":{\"id\":\"formula_16\"},\"end\":40674,\"start\":40639},{\"attributes\":{\"id\":\"formula_17\"},\"end\":41065,\"start\":40942},{\"attributes\":{\"id\":\"formula_18\"},\"end\":41154,\"start\":41109},{\"attributes\":{\"id\":\"formula_19\"},\"end\":41809,\"start\":41714},{\"attributes\":{\"id\":\"formula_20\"},\"end\":42376,\"start\":42337},{\"attributes\":{\"id\":\"formula_21\"},\"end\":42563,\"start\":42479},{\"attributes\":{\"id\":\"formula_22\"},\"end\":42656,\"start\":42605},{\"attributes\":{\"id\":\"formula_23\"},\"end\":43096,\"start\":43042},{\"attributes\":{\"id\":\"formula_24\"},\"end\":44003,\"start\":43882},{\"attributes\":{\"id\":\"formula_25\"},\"end\":44591,\"start\":44559},{\"attributes\":{\"id\":\"formula_26\"},\"end\":45120,\"start\":45063},{\"attributes\":{\"id\":\"formula_27\"},\"end\":45469,\"start\":45435},{\"attributes\":{\"id\":\"formula_28\"},\"end\":45531,\"start\":45510},{\"attributes\":{\"id\":\"formula_29\"},\"end\":45791,\"start\":45768},{\"attributes\":{\"id\":\"formula_30\"},\"end\":46097,\"start\":45860},{\"attributes\":{\"id\":\"formula_31\"},\"end\":46250,\"start\":46231},{\"attributes\":{\"id\":\"formula_32\"},\"end\":46530,\"start\":46508},{\"attributes\":{\"id\":\"formula_33\"},\"end\":46638,\"start\":46585},{\"attributes\":{\"id\":\"formula_34\"},\"end\":47094,\"start\":46973},{\"attributes\":{\"id\":\"formula_35\"},\"end\":47336,\"start\":47181},{\"attributes\":{\"id\":\"formula_36\"},\"end\":48430,\"start\":48266},{\"attributes\":{\"id\":\"formula_37\"},\"end\":51621,\"start\":51583},{\"attributes\":{\"id\":\"formula_38\"},\"end\":51791,\"start\":51725}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32756,\"start\":32749},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34772,\"start\":34765}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1860,\"start\":1848},{\"attributes\":{\"n\":\"2\"},\"end\":6739,\"start\":6727},{\"attributes\":{\"n\":\"3\"},\"end\":8998,\"start\":8981},{\"attributes\":{\"n\":\"3.1\"},\"end\":12125,\"start\":12104},{\"attributes\":{\"n\":\"4\"},\"end\":13436,\"start\":13395},{\"attributes\":{\"n\":\"5\"},\"end\":17039,\"start\":17010},{\"end\":17361,\"start\":17316},{\"attributes\":{\"n\":\"6.1\"},\"end\":24373,\"start\":24335},{\"attributes\":{\"n\":\"6.2\"},\"end\":26475,\"start\":26459},{\"attributes\":{\"n\":\"6.3\"},\"end\":28211,\"start\":28182},{\"attributes\":{\"n\":\"7\"},\"end\":29789,\"start\":29753},{\"attributes\":{\"n\":\"8\"},\"end\":31246,\"start\":31235},{\"end\":32988,\"start\":32978},{\"attributes\":{\"n\":\"8.1\"},\"end\":34663,\"start\":34639},{\"attributes\":{\"n\":\"8.2\"},\"end\":36947,\"start\":36937},{\"attributes\":{\"n\":\"9\"},\"end\":38280,\"start\":38270},{\"end\":38814,\"start\":38771},{\"end\":51452,\"start\":51421},{\"end\":52355,\"start\":52349},{\"end\":52646,\"start\":52621},{\"end\":53516,\"start\":53503},{\"end\":53595,\"start\":53594},{\"end\":53650,\"start\":53641},{\"end\":54283,\"start\":54274}]", "table": "[{\"end\":54272,\"start\":53867}]", "figure_caption": "[{\"end\":53592,\"start\":53519},{\"end\":53639,\"start\":53596},{\"end\":53867,\"start\":53652},{\"end\":54361,\"start\":54285}]", "figure_ref": "[{\"end\":33197,\"start\":33189},{\"end\":33644,\"start\":33636},{\"end\":36820,\"start\":36812}]", "bib_author_first_name": "[{\"end\":55636,\"start\":55630},{\"end\":55656,\"start\":55646},{\"end\":55667,\"start\":55663},{\"end\":55681,\"start\":55675},{\"end\":55694,\"start\":55689},{\"end\":55696,\"start\":55695},{\"end\":56242,\"start\":56236},{\"end\":56258,\"start\":56253},{\"end\":56272,\"start\":56267},{\"end\":56285,\"start\":56281},{\"end\":56300,\"start\":56296},{\"end\":56302,\"start\":56301},{\"end\":56759,\"start\":56752},{\"end\":56775,\"start\":56770},{\"end\":56793,\"start\":56787},{\"end\":56809,\"start\":56805},{\"end\":56823,\"start\":56818},{\"end\":57366,\"start\":57358},{\"end\":57385,\"start\":57375},{\"end\":57404,\"start\":57393},{\"end\":57726,\"start\":57720},{\"end\":57737,\"start\":57732},{\"end\":57933,\"start\":57928},{\"end\":57960,\"start\":57949},{\"end\":58177,\"start\":58176},{\"end\":58185,\"start\":58180},{\"end\":58198,\"start\":58193},{\"end\":58218,\"start\":58207},{\"end\":58220,\"start\":58219},{\"end\":58232,\"start\":58227},{\"end\":58234,\"start\":58233},{\"end\":58255,\"start\":58246},{\"end\":58257,\"start\":58256},{\"end\":58543,\"start\":58536},{\"end\":58555,\"start\":58552},{\"end\":58773,\"start\":58772},{\"end\":58784,\"start\":58783},{\"end\":58790,\"start\":58785},{\"end\":59108,\"start\":59107},{\"end\":59322,\"start\":59318},{\"end\":59333,\"start\":59327},{\"end\":59335,\"start\":59334},{\"end\":59736,\"start\":59733},{\"end\":59751,\"start\":59746},{\"end\":60171,\"start\":60165},{\"end\":60183,\"start\":60177},{\"end\":60199,\"start\":60191},{\"end\":60212,\"start\":60206},{\"end\":60228,\"start\":60222},{\"end\":60242,\"start\":60237},{\"end\":60259,\"start\":60252},{\"end\":60271,\"start\":60265},{\"end\":60972,\"start\":60965},{\"end\":60986,\"start\":60980},{\"end\":61001,\"start\":60995},{\"end\":61020,\"start\":61009},{\"end\":61038,\"start\":61031},{\"end\":61049,\"start\":61043},{\"end\":61063,\"start\":61058},{\"end\":61079,\"start\":61073},{\"end\":61557,\"start\":61548},{\"end\":61580,\"start\":61574},{\"end\":61596,\"start\":61592},{\"end\":61609,\"start\":61603},{\"end\":62120,\"start\":62117},{\"end\":62499,\"start\":62495},{\"end\":62516,\"start\":62508},{\"end\":62536,\"start\":62530},{\"end\":62548,\"start\":62545},{\"end\":62565,\"start\":62559},{\"end\":62862,\"start\":62856},{\"end\":62874,\"start\":62870},{\"end\":62888,\"start\":62885},{\"end\":62897,\"start\":62893},{\"end\":62908,\"start\":62902},{\"end\":63381,\"start\":63375},{\"end\":63395,\"start\":63389},{\"end\":63408,\"start\":63404},{\"end\":63424,\"start\":63419},{\"end\":63437,\"start\":63430},{\"end\":63450,\"start\":63444},{\"end\":63926,\"start\":63920},{\"end\":63943,\"start\":63934},{\"end\":63960,\"start\":63953},{\"end\":63975,\"start\":63968},{\"end\":63989,\"start\":63982},{\"end\":64001,\"start\":63995},{\"end\":64521,\"start\":64515},{\"end\":64536,\"start\":64529},{\"end\":64548,\"start\":64542},{\"end\":64562,\"start\":64556},{\"end\":64592,\"start\":64583},{\"end\":64609,\"start\":64602},{\"end\":64623,\"start\":64617},{\"end\":65142,\"start\":65136},{\"end\":65151,\"start\":65150},{\"end\":65166,\"start\":65160},{\"end\":65401,\"start\":65395},{\"end\":65768,\"start\":65762},{\"end\":65980,\"start\":65974},{\"end\":66209,\"start\":66202},{\"end\":66221,\"start\":66215},{\"end\":66234,\"start\":66229},{\"end\":66250,\"start\":66244},{\"end\":66737,\"start\":66732},{\"end\":66752,\"start\":66746},{\"end\":66770,\"start\":66761},{\"end\":66784,\"start\":66779},{\"end\":67185,\"start\":67184},{\"end\":67194,\"start\":67193},{\"end\":67204,\"start\":67203},{\"end\":67214,\"start\":67213},{\"end\":67509,\"start\":67508},{\"end\":67522,\"start\":67521},{\"end\":67531,\"start\":67530},{\"end\":67544,\"start\":67543},{\"end\":67935,\"start\":67931},{\"end\":67951,\"start\":67945},{\"end\":68158,\"start\":68154},{\"end\":68174,\"start\":68168},{\"end\":68190,\"start\":68185},{\"end\":68477,\"start\":68476},{\"end\":68479,\"start\":68478},{\"end\":68742,\"start\":68741},{\"end\":68911,\"start\":68908},{\"end\":68928,\"start\":68920},{\"end\":69456,\"start\":69451},{\"end\":69739,\"start\":69733},{\"end\":69755,\"start\":69749},{\"end\":69769,\"start\":69763},{\"end\":69786,\"start\":69780},{\"end\":70410,\"start\":70404},{\"end\":70614,\"start\":70609},{\"end\":70629,\"start\":70625},{\"end\":70644,\"start\":70639},{\"end\":71002,\"start\":70991},{\"end\":71145,\"start\":71140},{\"end\":71157,\"start\":71153},{\"end\":71447,\"start\":71440},{\"end\":71462,\"start\":71456},{\"end\":71479,\"start\":71473},{\"end\":71643,\"start\":71630},{\"end\":71654,\"start\":71650},{\"end\":71670,\"start\":71662},{\"end\":71917,\"start\":71913},{\"end\":71919,\"start\":71918},{\"end\":72196,\"start\":72195},{\"end\":72198,\"start\":72197},{\"end\":72517,\"start\":72514},{\"end\":72530,\"start\":72524},{\"end\":72544,\"start\":72538}]", "bib_author_last_name": "[{\"end\":55644,\"start\":55637},{\"end\":55661,\"start\":55657},{\"end\":55673,\"start\":55668},{\"end\":55687,\"start\":55682},{\"end\":55708,\"start\":55697},{\"end\":56251,\"start\":56243},{\"end\":56265,\"start\":56259},{\"end\":56279,\"start\":56273},{\"end\":56294,\"start\":56286},{\"end\":56311,\"start\":56303},{\"end\":56318,\"start\":56313},{\"end\":56768,\"start\":56760},{\"end\":56785,\"start\":56776},{\"end\":56803,\"start\":56794},{\"end\":56816,\"start\":56810},{\"end\":56830,\"start\":56824},{\"end\":57373,\"start\":57367},{\"end\":57391,\"start\":57386},{\"end\":57407,\"start\":57405},{\"end\":57730,\"start\":57727},{\"end\":57743,\"start\":57738},{\"end\":57947,\"start\":57934},{\"end\":57967,\"start\":57961},{\"end\":57975,\"start\":57969},{\"end\":58191,\"start\":58186},{\"end\":58205,\"start\":58199},{\"end\":58225,\"start\":58221},{\"end\":58244,\"start\":58235},{\"end\":58264,\"start\":58258},{\"end\":58550,\"start\":58544},{\"end\":58562,\"start\":58556},{\"end\":58781,\"start\":58774},{\"end\":58798,\"start\":58791},{\"end\":58806,\"start\":58800},{\"end\":59113,\"start\":59109},{\"end\":59121,\"start\":59115},{\"end\":59325,\"start\":59323},{\"end\":59341,\"start\":59336},{\"end\":59744,\"start\":59737},{\"end\":59761,\"start\":59752},{\"end\":60175,\"start\":60172},{\"end\":60189,\"start\":60184},{\"end\":60204,\"start\":60200},{\"end\":60220,\"start\":60213},{\"end\":60235,\"start\":60229},{\"end\":60250,\"start\":60243},{\"end\":60263,\"start\":60260},{\"end\":60278,\"start\":60272},{\"end\":60978,\"start\":60973},{\"end\":60993,\"start\":60987},{\"end\":61007,\"start\":61002},{\"end\":61029,\"start\":61021},{\"end\":61041,\"start\":61039},{\"end\":61056,\"start\":61050},{\"end\":61071,\"start\":61064},{\"end\":61086,\"start\":61080},{\"end\":61572,\"start\":61558},{\"end\":61590,\"start\":61581},{\"end\":61601,\"start\":61597},{\"end\":61615,\"start\":61610},{\"end\":62129,\"start\":62121},{\"end\":62506,\"start\":62500},{\"end\":62528,\"start\":62517},{\"end\":62543,\"start\":62537},{\"end\":62557,\"start\":62549},{\"end\":62572,\"start\":62566},{\"end\":62868,\"start\":62863},{\"end\":62883,\"start\":62875},{\"end\":62891,\"start\":62889},{\"end\":62900,\"start\":62898},{\"end\":62915,\"start\":62909},{\"end\":63387,\"start\":63382},{\"end\":63402,\"start\":63396},{\"end\":63417,\"start\":63409},{\"end\":63428,\"start\":63425},{\"end\":63442,\"start\":63438},{\"end\":63457,\"start\":63451},{\"end\":63932,\"start\":63927},{\"end\":63951,\"start\":63944},{\"end\":63966,\"start\":63961},{\"end\":63980,\"start\":63976},{\"end\":63993,\"start\":63990},{\"end\":64008,\"start\":64002},{\"end\":64527,\"start\":64522},{\"end\":64540,\"start\":64537},{\"end\":64554,\"start\":64549},{\"end\":64581,\"start\":64563},{\"end\":64600,\"start\":64593},{\"end\":64615,\"start\":64610},{\"end\":64627,\"start\":64624},{\"end\":64635,\"start\":64629},{\"end\":65148,\"start\":65143},{\"end\":65158,\"start\":65152},{\"end\":65174,\"start\":65167},{\"end\":65183,\"start\":65176},{\"end\":65409,\"start\":65402},{\"end\":65776,\"start\":65769},{\"end\":65988,\"start\":65981},{\"end\":66213,\"start\":66210},{\"end\":66227,\"start\":66222},{\"end\":66242,\"start\":66235},{\"end\":66257,\"start\":66251},{\"end\":66744,\"start\":66738},{\"end\":66759,\"start\":66753},{\"end\":66777,\"start\":66771},{\"end\":66796,\"start\":66785},{\"end\":66815,\"start\":66798},{\"end\":67191,\"start\":67186},{\"end\":67201,\"start\":67195},{\"end\":67211,\"start\":67205},{\"end\":67222,\"start\":67215},{\"end\":67519,\"start\":67510},{\"end\":67528,\"start\":67523},{\"end\":67541,\"start\":67532},{\"end\":67554,\"start\":67545},{\"end\":67943,\"start\":67936},{\"end\":67960,\"start\":67952},{\"end\":68166,\"start\":68159},{\"end\":68183,\"start\":68175},{\"end\":68198,\"start\":68191},{\"end\":68485,\"start\":68480},{\"end\":68747,\"start\":68743},{\"end\":68754,\"start\":68749},{\"end\":68918,\"start\":68912},{\"end\":68934,\"start\":68929},{\"end\":69467,\"start\":69457},{\"end\":69747,\"start\":69740},{\"end\":69761,\"start\":69756},{\"end\":69778,\"start\":69770},{\"end\":69793,\"start\":69787},{\"end\":70419,\"start\":70411},{\"end\":70623,\"start\":70615},{\"end\":70637,\"start\":70630},{\"end\":70652,\"start\":70645},{\"end\":71008,\"start\":71003},{\"end\":71151,\"start\":71146},{\"end\":71166,\"start\":71158},{\"end\":71454,\"start\":71448},{\"end\":71471,\"start\":71463},{\"end\":71486,\"start\":71480},{\"end\":71648,\"start\":71644},{\"end\":71660,\"start\":71655},{\"end\":71680,\"start\":71671},{\"end\":71926,\"start\":71920},{\"end\":72206,\"start\":72199},{\"end\":72522,\"start\":72518},{\"end\":72536,\"start\":72531},{\"end\":72553,\"start\":72545}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13178535},\"end\":56157,\"start\":55527},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":232053657},\"end\":56634,\"start\":56159},{\"attributes\":{\"doi\":\"10.23919/DATE.2019.8715186\",\"id\":\"b2\",\"matched_paper_id\":68046116},\"end\":57268,\"start\":56636},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":224882796},\"end\":57685,\"start\":57270},{\"attributes\":{\"id\":\"b4\"},\"end\":57847,\"start\":57687},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3622437},\"end\":58174,\"start\":57849},{\"attributes\":{\"id\":\"b6\"},\"end\":58495,\"start\":58176},{\"attributes\":{\"id\":\"b7\"},\"end\":58722,\"start\":58497},{\"attributes\":{\"doi\":\"10.1162/NECO_a_00467\",\"id\":\"b8\",\"matched_paper_id\":7958038},\"end\":59027,\"start\":58724},{\"attributes\":{\"id\":\"b9\"},\"end\":59221,\"start\":59029},{\"attributes\":{\"doi\":\"10.1109/MCAS.2020.2988388\",\"id\":\"b10\"},\"end\":59610,\"start\":59223},{\"attributes\":{\"doi\":\"10.1162/neco_a_01179\",\"id\":\"b11\",\"matched_paper_id\":81986950},\"end\":60090,\"start\":59612},{\"attributes\":{\"doi\":\"10.1145/3394885.3431553\",\"id\":\"b12\",\"matched_paper_id\":231730928},\"end\":60889,\"start\":60092},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":227069231},\"end\":61454,\"start\":60891},{\"attributes\":{\"doi\":\"10.23919/DATE51398.2021.9474107\",\"id\":\"b14\",\"matched_paper_id\":236151316},\"end\":62028,\"start\":61456},{\"attributes\":{\"doi\":\"10.1073/pnas.79.8.2554\",\"id\":\"b15\",\"matched_paper_id\":784288},\"end\":62466,\"start\":62030},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6453539},\"end\":62794,\"start\":62468},{\"attributes\":{\"doi\":\"10.23919/DATE.2019.8714821\",\"id\":\"b17\",\"matched_paper_id\":155109576},\"end\":63279,\"start\":62796},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":163164623},\"end\":63832,\"start\":63281},{\"attributes\":{\"doi\":\"10.1109/MICRO50266.2020.00039\",\"id\":\"b19\",\"matched_paper_id\":222305243},\"end\":64438,\"start\":63834},{\"attributes\":{\"doi\":\"10.1109/HPCA51647.2021.00028\",\"id\":\"b20\",\"matched_paper_id\":233376633},\"end\":65092,\"start\":64440},{\"attributes\":{\"id\":\"b21\"},\"end\":65338,\"start\":65094},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":60315521},\"end\":65635,\"start\":65340},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":733980},\"end\":65970,\"start\":65637},{\"attributes\":{\"id\":\"b24\"},\"end\":66114,\"start\":65972},{\"attributes\":{\"doi\":\"10.23919/DATE48585.2020.9116397\",\"id\":\"b25\",\"matched_paper_id\":219858990},\"end\":66653,\"start\":66116},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1954789},\"end\":67125,\"start\":66655},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14542261},\"end\":67406,\"start\":67127},{\"attributes\":{\"doi\":\"10.1126/scirobotics.aaw6736\",\"id\":\"b28\",\"matched_paper_id\":182118830},\"end\":67838,\"start\":67408},{\"attributes\":{\"id\":\"b29\"},\"end\":68092,\"start\":67840},{\"attributes\":{\"doi\":\"10.1007/s13218-019-00623-z\",\"id\":\"b30\"},\"end\":68437,\"start\":68094},{\"attributes\":{\"doi\":\"10.1109/72.377968\",\"id\":\"b31\",\"matched_paper_id\":2352281},\"end\":68675,\"start\":68439},{\"attributes\":{\"id\":\"b32\"},\"end\":68857,\"start\":68677},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":877929},\"end\":69356,\"start\":68859},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12781225},\"end\":69646,\"start\":69358},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":67872077},\"end\":69995,\"start\":69648},{\"attributes\":{\"doi\":\"9781450361378. doi: 10.1145/ 3289602.3293913\",\"id\":\"b36\"},\"end\":70339,\"start\":69997},{\"attributes\":{\"id\":\"b37\"},\"end\":70560,\"start\":70341},{\"attributes\":{\"doi\":\"10.1007/s10462-021-10110-3\",\"id\":\"b38\",\"matched_paper_id\":211003738},\"end\":70948,\"start\":70562},{\"attributes\":{\"id\":\"b39\"},\"end\":71106,\"start\":70950},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":44894403},\"end\":71383,\"start\":71108},{\"attributes\":{\"id\":\"b41\"},\"end\":71600,\"start\":71385},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":122399518},\"end\":71865,\"start\":71602},{\"attributes\":{\"doi\":\"10.1109/TEC.1959.5222693\",\"id\":\"b43\",\"matched_paper_id\":206673426},\"end\":72157,\"start\":71867},{\"attributes\":{\"doi\":\"10.1109/PGEC.1964.263830\",\"id\":\"b44\",\"matched_paper_id\":591519},\"end\":72429,\"start\":72159},{\"attributes\":{\"id\":\"b45\"},\"end\":72688,\"start\":72431}]", "bib_title": "[{\"end\":55628,\"start\":55527},{\"end\":56234,\"start\":56159},{\"end\":56750,\"start\":56636},{\"end\":57356,\"start\":57270},{\"end\":57926,\"start\":57849},{\"end\":58770,\"start\":58724},{\"end\":59731,\"start\":59612},{\"end\":60163,\"start\":60092},{\"end\":60963,\"start\":60891},{\"end\":61546,\"start\":61456},{\"end\":62115,\"start\":62030},{\"end\":62493,\"start\":62468},{\"end\":62854,\"start\":62796},{\"end\":63373,\"start\":63281},{\"end\":63918,\"start\":63834},{\"end\":64513,\"start\":64440},{\"end\":65393,\"start\":65340},{\"end\":65760,\"start\":65637},{\"end\":66200,\"start\":66116},{\"end\":66730,\"start\":66655},{\"end\":67182,\"start\":67127},{\"end\":67506,\"start\":67408},{\"end\":68474,\"start\":68439},{\"end\":68906,\"start\":68859},{\"end\":69449,\"start\":69358},{\"end\":69731,\"start\":69648},{\"end\":70607,\"start\":70562},{\"end\":71138,\"start\":71108},{\"end\":71628,\"start\":71602},{\"end\":71911,\"start\":71867},{\"end\":72193,\"start\":72159}]", "bib_author": "[{\"end\":55646,\"start\":55630},{\"end\":55663,\"start\":55646},{\"end\":55675,\"start\":55663},{\"end\":55689,\"start\":55675},{\"end\":55710,\"start\":55689},{\"end\":56253,\"start\":56236},{\"end\":56267,\"start\":56253},{\"end\":56281,\"start\":56267},{\"end\":56296,\"start\":56281},{\"end\":56313,\"start\":56296},{\"end\":56320,\"start\":56313},{\"end\":56770,\"start\":56752},{\"end\":56787,\"start\":56770},{\"end\":56805,\"start\":56787},{\"end\":56818,\"start\":56805},{\"end\":56832,\"start\":56818},{\"end\":57375,\"start\":57358},{\"end\":57393,\"start\":57375},{\"end\":57409,\"start\":57393},{\"end\":57732,\"start\":57720},{\"end\":57745,\"start\":57732},{\"end\":57949,\"start\":57928},{\"end\":57969,\"start\":57949},{\"end\":57977,\"start\":57969},{\"end\":58180,\"start\":58176},{\"end\":58193,\"start\":58180},{\"end\":58207,\"start\":58193},{\"end\":58227,\"start\":58207},{\"end\":58246,\"start\":58227},{\"end\":58266,\"start\":58246},{\"end\":58552,\"start\":58536},{\"end\":58564,\"start\":58552},{\"end\":58783,\"start\":58772},{\"end\":58800,\"start\":58783},{\"end\":58808,\"start\":58800},{\"end\":59115,\"start\":59107},{\"end\":59123,\"start\":59115},{\"end\":59327,\"start\":59318},{\"end\":59343,\"start\":59327},{\"end\":59746,\"start\":59733},{\"end\":59763,\"start\":59746},{\"end\":60177,\"start\":60165},{\"end\":60191,\"start\":60177},{\"end\":60206,\"start\":60191},{\"end\":60222,\"start\":60206},{\"end\":60237,\"start\":60222},{\"end\":60252,\"start\":60237},{\"end\":60265,\"start\":60252},{\"end\":60280,\"start\":60265},{\"end\":60980,\"start\":60965},{\"end\":60995,\"start\":60980},{\"end\":61009,\"start\":60995},{\"end\":61031,\"start\":61009},{\"end\":61043,\"start\":61031},{\"end\":61058,\"start\":61043},{\"end\":61073,\"start\":61058},{\"end\":61088,\"start\":61073},{\"end\":61574,\"start\":61548},{\"end\":61592,\"start\":61574},{\"end\":61603,\"start\":61592},{\"end\":61617,\"start\":61603},{\"end\":62131,\"start\":62117},{\"end\":62508,\"start\":62495},{\"end\":62530,\"start\":62508},{\"end\":62545,\"start\":62530},{\"end\":62559,\"start\":62545},{\"end\":62574,\"start\":62559},{\"end\":62870,\"start\":62856},{\"end\":62885,\"start\":62870},{\"end\":62893,\"start\":62885},{\"end\":62902,\"start\":62893},{\"end\":62917,\"start\":62902},{\"end\":63389,\"start\":63375},{\"end\":63404,\"start\":63389},{\"end\":63419,\"start\":63404},{\"end\":63430,\"start\":63419},{\"end\":63444,\"start\":63430},{\"end\":63459,\"start\":63444},{\"end\":63934,\"start\":63920},{\"end\":63953,\"start\":63934},{\"end\":63968,\"start\":63953},{\"end\":63982,\"start\":63968},{\"end\":63995,\"start\":63982},{\"end\":64010,\"start\":63995},{\"end\":64529,\"start\":64515},{\"end\":64542,\"start\":64529},{\"end\":64556,\"start\":64542},{\"end\":64583,\"start\":64556},{\"end\":64602,\"start\":64583},{\"end\":64617,\"start\":64602},{\"end\":64629,\"start\":64617},{\"end\":64637,\"start\":64629},{\"end\":65150,\"start\":65136},{\"end\":65160,\"start\":65150},{\"end\":65176,\"start\":65160},{\"end\":65185,\"start\":65176},{\"end\":65411,\"start\":65395},{\"end\":65778,\"start\":65762},{\"end\":65990,\"start\":65974},{\"end\":66215,\"start\":66202},{\"end\":66229,\"start\":66215},{\"end\":66244,\"start\":66229},{\"end\":66259,\"start\":66244},{\"end\":66746,\"start\":66732},{\"end\":66761,\"start\":66746},{\"end\":66779,\"start\":66761},{\"end\":66798,\"start\":66779},{\"end\":66817,\"start\":66798},{\"end\":67193,\"start\":67184},{\"end\":67203,\"start\":67193},{\"end\":67213,\"start\":67203},{\"end\":67224,\"start\":67213},{\"end\":67521,\"start\":67508},{\"end\":67530,\"start\":67521},{\"end\":67543,\"start\":67530},{\"end\":67556,\"start\":67543},{\"end\":67945,\"start\":67931},{\"end\":67962,\"start\":67945},{\"end\":68168,\"start\":68154},{\"end\":68185,\"start\":68168},{\"end\":68200,\"start\":68185},{\"end\":68487,\"start\":68476},{\"end\":68749,\"start\":68741},{\"end\":68756,\"start\":68749},{\"end\":68920,\"start\":68908},{\"end\":68936,\"start\":68920},{\"end\":69469,\"start\":69451},{\"end\":69749,\"start\":69733},{\"end\":69763,\"start\":69749},{\"end\":69780,\"start\":69763},{\"end\":69795,\"start\":69780},{\"end\":70421,\"start\":70404},{\"end\":70625,\"start\":70609},{\"end\":70639,\"start\":70625},{\"end\":70654,\"start\":70639},{\"end\":71010,\"start\":70991},{\"end\":71153,\"start\":71140},{\"end\":71168,\"start\":71153},{\"end\":71456,\"start\":71440},{\"end\":71473,\"start\":71456},{\"end\":71488,\"start\":71473},{\"end\":71650,\"start\":71630},{\"end\":71662,\"start\":71650},{\"end\":71682,\"start\":71662},{\"end\":71928,\"start\":71913},{\"end\":72208,\"start\":72195},{\"end\":72524,\"start\":72514},{\"end\":72538,\"start\":72524},{\"end\":72555,\"start\":72538}]", "bib_venue": "[{\"end\":55776,\"start\":55759},{\"end\":60464,\"start\":60392},{\"end\":63564,\"start\":63520},{\"end\":69119,\"start\":69036},{\"end\":69818,\"start\":69815},{\"end\":55757,\"start\":55710},{\"end\":56363,\"start\":56320},{\"end\":56933,\"start\":56858},{\"end\":57463,\"start\":57409},{\"end\":57718,\"start\":57687},{\"end\":57995,\"start\":57977},{\"end\":58328,\"start\":58266},{\"end\":58534,\"start\":58497},{\"end\":58846,\"start\":58828},{\"end\":59105,\"start\":59029},{\"end\":59316,\"start\":59223},{\"end\":59801,\"start\":59783},{\"end\":60390,\"start\":60303},{\"end\":61159,\"start\":61088},{\"end\":61723,\"start\":61648},{\"end\":62200,\"start\":62153},{\"end\":62623,\"start\":62574},{\"end\":63018,\"start\":62943},{\"end\":63518,\"start\":63459},{\"end\":64117,\"start\":64039},{\"end\":64747,\"start\":64665},{\"end\":65134,\"start\":65094},{\"end\":65465,\"start\":65411},{\"end\":65790,\"start\":65778},{\"end\":66027,\"start\":65990},{\"end\":66365,\"start\":66290},{\"end\":66874,\"start\":66817},{\"end\":67247,\"start\":67224},{\"end\":67599,\"start\":67583},{\"end\":67929,\"start\":67840},{\"end\":68152,\"start\":68094},{\"end\":68540,\"start\":68504},{\"end\":68739,\"start\":68677},{\"end\":69034,\"start\":68936},{\"end\":69489,\"start\":69469},{\"end\":69813,\"start\":69795},{\"end\":70118,\"start\":70041},{\"end\":70402,\"start\":70341},{\"end\":70710,\"start\":70680},{\"end\":70989,\"start\":70950},{\"end\":71223,\"start\":71168},{\"end\":71438,\"start\":71385},{\"end\":71721,\"start\":71682},{\"end\":71996,\"start\":71952},{\"end\":72277,\"start\":72232},{\"end\":72512,\"start\":72431}]"}}}, "year": 2023, "month": 12, "day": 17}