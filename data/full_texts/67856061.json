{"id": 67856061, "updated": "2023-11-08 13:33:11.462", "metadata": {"title": "Crowd Counting and Density Estimation by Trellis Encoder-Decoder Network", "authors": "[{\"first\":\"Xiaolong\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Zehao\",\"last\":\"Xiao\",\"middle\":[]},{\"first\":\"Baochang\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xiantong\",\"last\":\"Zhen\",\"middle\":[]},{\"first\":\"Xianbin\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Doermann\",\"middle\":[]},{\"first\":\"Ling\",\"last\":\"Shao\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 3, "day": 3}, "abstract": "Crowd counting has recently attracted increasing interest in computer vision but remains a challenging problem. In this paper, we propose a trellis encoder-decoder network (TEDnet) for crowd counting, which focuses on generating high-quality density estimation maps. The major contributions are four-fold. First, we develop a new trellis architecture that incorporates multiple decoding paths to hierarchically aggregate features at different encoding stages, which can handle large variations of objects. Second, we design dense skip connections interleaved across paths to facilitate sufficient multi-scale feature fusions and to absorb the supervision information. Third, we propose a new combinatorial loss to enforce local coherence and spatial correlation in density maps. By distributedly imposing this combinatorial loss on intermediate outputs, gradient vanishing can be largely alleviated for better back-propagation and faster convergence. Finally, our TEDnet achieves new state-of-the art performance on four benchmarks, with an improvement up to 14% in terms of MAE.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1903.00853", "mag": "2969620138", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/JiangXZZ0D019", "doi": "10.1109/cvpr.2019.00629"}}, "content": {"source": {"pdf_hash": "02c34c7c4c2ee2b4cee4f52395774611b662c259", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1903.00853v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d5ada0be84c66f3a02036bffc93475c1d756f1e9", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/02c34c7c4c2ee2b4cee4f52395774611b662c259.txt", "contents": "\nCrowd Counting and Density Estimation by Trellis Encoder-Decoder Networks\n3 Mar 2019\n\nXiaolong Jiang \nBeihang University\nBeijingChina\n\nZehao Xiao zhxiao@buaa.edu.cn \nBaochang Zhang bczhang@buaa.edu.cn \nBeihang University\nBeijingChina\n\nXiantong Zhen zhenxt@gmail.com \nBeihang University\nBeijingChina\n\nInception Institute of Artificial Intelligence\n\n\nXianbin Cao xbcao@buaa.edu.cn \nBeihang University\nBeijingChina\n\nDavid Doermann doermann@buffalo.edu \nUniversity at Buffalo\nNew YorkUSA\n\nLing Shao ling.shao@ieee.org \nInception Institute of Artificial Intelligence\n\n\nCrowd Counting and Density Estimation by Trellis Encoder-Decoder Networks\n3 Mar 201919D08546D935636FBAB23EBDBA2C85BDarXiv:1903.00853v1[cs.CV]\nCrowd counting has recently attracted increasing interest in computer vision but remains a challenging problem.In this paper, we propose a trellis encoder-decoder network (TEDnet) for crowd counting, which focuses on generating high-quality density estimation maps.The major contributions are four-fold.First, we develop a new trellis architecture that incorporates multiple decoding paths to hierarchically aggregate features at different encoding stages, which improves the representative capability of convolutional features for large variations in objects.Second, we employ dense skip connections interleaved across paths to facilitate sufficient multi-scale feature fusions, which also helps TEDnet to absorb the supervision information.Third, we propose a new combinatorial loss to enforce similarities in local coherence and spatial correlation between maps.By distributedly imposing this combinatorial loss on intermediate outputs, TEDnet can improve the back-propagation process and alleviate the gradient vanishing problem.Finally, on four widely-used benchmarks, our TEDnet achieves the best overall performance in terms of both density map quality and counting accuracy, with an improvement up to 14% in MAE metric.These results validate the effectiveness of TEDnet for crowd counting.\n\nIntroduction\n\nWith the rapid pace of urbanization, crowds tend to gather more frequently, increasing requirements for effective safety monitoring, disaster relief, urban planning, and crowd management.As a fundamental technique to support these applications, crowd counting has been investigated and has resulted in advanced solutions.Most crowd counting methods are based on detection [17,9,44], regression [27,4,10], and density estimation [16,52,2,11  Density estimation based methods, in particular, have received increasing research focus.These techniques have the ability to localize the crowd by generating a density estimation map using pixel-wise regression.The crowd count is then calculated as the integral of the density map.To generate maps with a retained spatial size as the outputs, deep encoder-decoder convolutional neural network (CNN) architectures are widely applied [6,41,53,32,8,19,24].In particular, encoder-decoder methods also play an impor-Figure 2.An illustration of estimated density maps and crowd counts generated by the proposed approach and other state-of-the-arts.The first column shows two samples drawn from ShanghaiTech Part A. The second column shows corresponding ground truth maps with fixed Gaussian kernels.From the third to the last column we show the density maps estimated by MCNN [52], SANet [2], and the proposed TEDNet, respectively.TEDnet generates density maps closer to the ground truth, and more accurate crowd counts.\n\ntant role in localization-oriented tasks to facilitate accurate pixel-wise regression [18,30,42,29,23], given that the convolution itself is essentially a pixel-wise feature localization using traversal template matching.Thus motivated, we propose the trellis encoder-decoder network (TEDnet) for density estimation to address the crowd counting problem.Our approach improves both the encoding and decoding processes for more accurate pixel-wise estimations.\n\nFeature encoding facilitates accurate pixel-wise estimations by extracting features, while preserving the pixel-wise localization precision in the feature maps.In scenes with severe occlusions and scale variations, CNN features are widely employed to enhance the feature encoding performance [52,1,26,39,2,21].It is worth noting that most current counting methods adopt CNNs that were originally designed for classification tasks, such as VGG-16 [39,37,21], Inception [2], and DenseNet [11].Despite their previous success, these networks build deep hierarchies to transform low-level spatial information into high-level semantic information.Consequentially, the resolution of feature maps is gradually degraded due to down-sampling operations, and thus, the localization precision is lowered.It is desirable to maintain a favorable balance between spatial resolution preservation and semantic feature extraction.\n\nFeature decoding generates density maps by aggregating encoded feature maps.The pixel-wise accuracy performance for an estimated map is guaranteed by sufficient fusions of multi-scale decoding features that incorporate lowlevel spatial precision and high-level semantic depth.In hourglass encoder-decoder networks with a single decoding path [32,53,6,41], features must endure excessive down-sampling and up-sampling operations, which degrade the pixel-wise precision.In addition, rich low-level spatial and high-level semantic information residing in multiscale feature maps at the two ends of the hourglass are separated by the gap between the encoder and decoder.Although attempts have been made to enhance the hourglass networks with skip connections [19,24,8,49], they are not designed to generate high-quality density estimation maps due to the lack of hierarchical fusions between multi-scale features.From a more fundamental perspective, the widely adopted mean square error (MSE) loss in crowd counting assumes pixel-wise independence, while neglecting the local coherence and spatial correlation in density maps.It is therefore inadequate for facilitating the generation of highquality density maps.\n\nTo address these issues in existing encoder-decoder networks and improve the counting performance with an enhanced architecture, we propose the trellis encoder-decoder network (TEDnet) to generate high-quality density maps.TEDnet achieves sufficient aggregation and fusion of multiscale features within an established trellis-like feature hierarchy.In the encoding process, multi-scale convolutional kernels are used to obtain scale adaptation, where downsampling strides are cut to four to preserve pixel-wise spatial precision.In the decoding process, multiple paths are deployed at corresponding encoding stages, each of which aggregates the encoded multi-scale features.Across paths, features containing diverse spatial and semantic information are integrated using dense skip connections, which guarantees thorough multi-scale feature fusions.Our multi-path trellis network is similar in spirit to an ensemble of multiple hourglass networks with different feature scales, establishing a feature learning hierarchy resides in a trellis structure, as highlighted in Figure 1.Each path in TEDnet generates an intermediate output map that intrinsically enables the deployment of distributed supervision within each path.This alleviates the gradient vanishing problem and boosts the gradient flow through the network.Each distributed loss in TEDnet is a combinatorial loss defined based on the proposed spatial abstraction loss (SAL) and the spatial correlation loss (SCL).SAL and SCL relieve the pixel-wise independence assumption posed by the MSE loss, and improve the density map quality, as well as counting performance, by enforcing similarities in local coherence and spatial correlation between maps.\n\nTEDnet takes full images, rather than image patches, as the input and outputs full-resolution density maps.This further ensures the density map quality (qualitatively demonstrated in Figure 2) by avoiding the tedious patch-wise operation, which induces boundary artifacts.The main contributions of the proposed approach are summarized as follows:\n\n\u2022 We propose a new deep learning architecture for accurate density estimation and crowd counting, called trellis encoder-decoder network (TEDnet), which assembles multiple encoding-decoding paths hierarchically to generate high-quality density map for accurate crowd counting.\n\n\u2022 We establish a multi-path decoder that pervasively aggregates the spatially-endowed features within a decoding feature hierarchy and progressively fuses multiscale features with dense skip connections interleaved in the hierarchy.\n\n\u2022 We introduce a combinatorial loss comprising of the newly designed SAL and SCL to supervise local coherence and spatial correlation in density maps.Distributed supervision, in conjunction with the combinatorial loss, is deployed on intermediate multi-path outputs to improve the optimization of the network.\n\n\u2022 We achieve the best overall performance on four commonly-used benchmark datasets, largely surpassing the state-of-the-art methods by up to 14% for the MAE metric.We obtain the best quality of estimated density maps, in terms of both PSNR and SSIM measures.\n\n\nRelated Work\n\nIn this section, we provide a brief review of the most related work and refer to comprehensive surveys for crowd counting [34,35,40,12].\n\n\nDetection and Regression based Methods\n\nDetection-based counting methods deploy a detector to traverse the image, which localizes and counts the targets along the way [5,9,17,44].These methods are surpassed by the regression-based alternatives, as the detection performance is affected in the presence of overcrowded scenes.The successes of regression-based methods [3,33,10,50,14] can thus be attributed to their ability of circumventing explicit detection and directly mapping the input images to scalar values.Nevertheless, regressionbased methods forfeit localization capability such that they cannot perceive crowd distributions.To recover the lost localization capability, crowd counting methods based on density estimation are therefore developed by conducting pixel-wise regressions.\n\n\nDensity Estimation based Methods\n\nInitially introduced in [16], density estimation based methods avoid explicitly detecting each individual and retain the ability to localize the crowd.Earlier approaches strove to compute density maps with hand-crafted features [16,7] and random forest regressions [7,28,48].More recent methods appeal to CNN based feature extraction to supply scale and perspective invariant features.In particular, MCNN [52], Crowdnet [1], Hydra CNN [26], CNNboost [43], CP-CNN [39], and Switching CNN [36] all conform to an ensemble design approach to enable multi-scale adaptation, where multiple CNN branches with different receptive fields are jointly maintained.The extra computational expense introduced by these methods is to some degree wasted on inefficient and un-flexible branching [18].As a remedy, single-branch counting networks with scale adaptations were proposed in [2,11,46].Notably, most of these methods follow a patch-based counting mechanism [51,43,26,1,36,20,11,2], where the full density map is obtained by concatenating discrete density patches.More importantly, methods such as MCNN, Hydra CNN, and CNN-boost output density maps with reduced resolution due to excessive down-sampling strides.This inevitably sacrifices pixel-wise details and damages the density map quality.Comparatively, CP-CNN [39] focuses on generating high-quality full-resolution maps with the help of global and local semantic information.In [2], researchers computed high-quality full-resolution maps with a new encoderdecoder network, as well as a SSIM local pattern consistent loss.In order to limit the down-sampling stride in the encoding process, CSRNet [18] adopts dilated convolutional layers to substitute pooling layers.\n\nUnlike other approaches, the proposed trellis encoderdecoder architecture attempts to generate high-quality density estimation maps by preserving the spatial information in the encoding feature hierarchy.More importantly, it incorporates a multi-path decoder to enhance the aggregation and fusion of multi-scale features with rich spatial and semantic information.As a result, pixel-wise regression accuracy in the estimated map is enhanced.In a broader view, density estimation is similar to other localization-oriented tasks, such as tracking [31,29,22] and detection [23], which also generate localization estimation maps as outputs.These tasks are inter-correlated with density estimations such that the resulting localization maps can be fused to integrate task-specific localization response [12,11].Moreover, semantic segmentation also relies on powerful encoder-decoder architecture to integrate multi-scale features and to improve localization precision.Consequently, efforts have been made to enhance the hourglass architecture.In [8], SDN stacks multiple single-path hourglass networks into a deeper sequence to improve the feature fusion and guarantee fine recovery of localization information.In [19,24], the single-path hourglass network is extended by adding residual units inside the skip connections.\n\n\nTrellis Encoder-Decoder Networks\n\nAs shown in Figure 1, the goal of TEDnet is to achieve improved counting performance by generating density maps with high pixel-wise density estimations.In the encoder, the localization property of a density estimation conforms to the nature of a convolutional layer operation.Here, the convolutional kernels are the feature templates that are localized in the feature maps via template-matching.In the decoder, encoded feature maps are aggregated to represent the locality of crowded objects.Our TEDnet can establish a feature hierarchy within the trellis architecture, where reliable multi-scale features are encoded with wellpreserved spatial information.These are then decoded into accurate density maps, with a great capacity for precise localization.In what follows, we explain in detail the multiscale encoder, the multi-path decoder, and the distributed supervision with combinatorial loss in TEDnet.\n\n\nMulti-Scale Encoder\n\nWe design the multi-scale encoder to extract reliable features relevant to crowded human objects, while being able to localize these features with pixel-wise precision.The multi-scale encoding block is capable of overcoming occlusions and scale variations present in crowd counting scenes, as elaborated below.\n\nAs shown in Figure 3, a multi-scale encoding block is implemented with kernels of different sizes, which enables the encoder to extract multi-scale features.As indicated in Figure 1, a total of nine encoding blocks are implemented and grouped into five encoding stages.To preserve feature localization precision, we limit the application of pooling operations.Consequently, only two 2 \u00d7 2 max pooling layers are inserted at the first two encoding stages, each of which has a down-sampling stride of 2. To further enlarge the receptive fields, dilated convolutional kernels with dilation rates of 2 and 4 are employed in the last two encoding blocks [18].\n\n\nMulti-Path Decoder\n\nWe design a new multi-path decoder to hierarchically aggregate the spatially-preserved features and restore the spatial resolution in the density map.As the component that directly generates the density maps, the decoder has a vital influence on the density map quality.Unfortunately, less emphasis had been placed on the decoder in the literature for crowd counting and density estimation.In CSRNet [18], the density maps are simply generated by applying bilinear interpolation to up-sample the encoded feature maps.\n\nIn Crowdnet [1], a 1 \u00d7 1 convolutional layer is implemented as the decoder.SANet [2] advocates the idea of refinement in a single-path hourglass decoder.To the best of our knowledge, it is by far the most sophisticated design in the context of density estimation.Alternatively, efforts have been made in other tasks using the hourglass architecture, such as image segmentation [32,53,8,19] and superresolution [6,41].Nevertheless, as explained in Section 1, these architectures are not optimal for density estimation.They suffer from prolonged single-path feature transformation hierarchy with heavy parameterization, as well as insufficient feature aggregations and fusions.\n\nTo remedy the defects of the existing decoder, we propose a multi-path decoder in TEDnet, which assembles a set of single-path hourglass architectures with multi-scale features.As depicted in Figure 1, three decoding paths are exploited on the feature maps, computed from the last three encoding stages.Within each path, a decoding feature hierarchy is established to aggregate feature representations at the same semantic level, in a progressive way.Among different paths, feature maps from different levels are fused with dense skip connections.Both aggregation and fusion for features are implemented in densely interleaved decoding blocks.The decoder implementation is realized by stacking decoding blocks into the trellis structure, such that a feature hierarchy is established.As shown in Figure 1, such a feature hierarchy is pinpointed into the trellis architecture with a grid representation, where each column indicates one decoding path and each row presents the depth within each path.\n\nDecoding Block.As shown in Figure 4(a), each decoding block takes two inputs.The right input feature is passed from the same decoding path and it possesses deeper semantic information whose channels doubles those of the left input feature.It is aggregated via a deconvolutional layer F 2 i,j with 3\u00d73 kernels, which halves the channels.The left input feature is aggregated by a convolutional layer F 1 i,j deploying 1 \u00d7 1 kernels, with its depth unchanged.These two aggregated features are fused through channel-wise concatenation, followed by a convolutional layer F 3 i,j with 1 \u00d7 1 filters.\n\nIn (1), Z i,j denotes the decoded feature at the i-th row and j-th column within the feature grid, computed by the decoding block D i,j , as follows:\nZ i,j = D i,j (Z i\u22121,j\u22121 , Z i\u22121,j ) = F 3 i,j ([F 1 i,j (Z i\u22121,j\u22121 ), F 2 i,j (Z i\u22121,j )]),(1)\nwhere F (\u2022) indicates a convolutional operations, and [\u2022] denotes a channel-wise concatenation.Within the established feature hierarchy as shown in Figure 1, the decoded features enable the aggregation and fusion of multi-scale features.As a result, the decoded feature map Z 4,4 , at the end of the rightmost decoding path, contains the richest spatial and semantic information.\n\nThus, the final output density map Z is generated from these feature maps by restoring the spatial dimension through the up-sampling block.\n\nUp-sampling Block.As illustrated in Figure 4(b), the design of the up-sampling block is inspired by a superresolution technique [25], where the nearest neighbor interpolation is followed by a 3 \u00d7 3 deconvolutional layer with a stride of 1.The overall down-sampling stride of TEDnet is 4. We restore the spatial size of the density map by repeating the up-sampling operations twice in the up-sampling block.\n\nOverall, a spatial-semantic-spatial feature hierarchy is fully exploited in TEDnet.In Figure 1, the proposed architecture is established to host the feature hierarchy.As indicated by the horizontal axis, the feature maps on the right in the hierarchy have more semantic information than the ones on the left.Those on the left, however, contain richer spatial details.Vertically, spatial information is gradually recovered through skip connections, which transmit low-level spatial features from left to right, top to bottom.It is worth noting that, for a simple single-path hourglass encoder-decoder, spatial information cannot be recovered in the decoder as indicated vertically in Figure 1.Although sparsely linked skip connections can alleviate inadequate feature fusion to a certain extent in single-path hourglass encoder-decoders, the pervasive feature fusions as realized in TEDnet can still not be reached.\n\n\nDistributed Supervision\n\nThe multi-path architecture of TEDNet produces intermediate output estimation maps, i.e., Z 2,D , Z 3,D , Z 4,D , Z, at the ends of decoding paths as illustrated in Figure 1.This design naturally enables distributed supervision, such that multiple losses can be applied at each intermediate output.Previous attempts have been made to provide multisupervision, where losses are computed between intermediate feature maps and the ground truths [47,15,8].In contrast, the proposed distributed supervision implemented in TEDnet computes multiple losses between intermediate density estimation maps and ground truth maps.From the ensemble point of view, each distributed loss is calculated to supervise the corresponding path representing a singlepath hourglass network.In particular, to compute the losses at Z 2,D , Z 3,D , Z 4,D , each of them is aggregated from its previous feature map using a convolutional layer with 1 \u00d7 1 filter size.The ground truth density map is down-sampled to 128 \u00d7 128 with average pooling operations.Each of these intermediate outputs is separately decoded on different feature levels along its own path.Meanwhile, information from different paths are integrated through dense skip connections.As a result, the supervision at each output is meaningful and can help better optimize the network.\n\nDue to the distributed supervision, in conjunction with the dense skip connections, the gradient vanishing phenomenon, which indicates weaker gradients at the earlier stage of the network, is substantially alleviated.Consider the convolutional block 1 for instance.During the backpropagation process, the gradients flow is a summation of propagated flows, starting at each distributed supervision, such that the gradient is boosted.Moreover, for each flow originating at its corresponding supervision, instead of flowing backward along just one decoding path, the interleaved dense skip connections provide more diffused flow paths at each fork junctions, thus further boosting the gradient flow.\n\n\nCombinatorial Loss\n\nAs shown in Figure 5, the loss function distributed at each decoding output is a combination of two losses.In general, the pixel-wise mean square error (MSE) loss has dominated the training of density estimation based crowd counting approaches [52,1,11].As advocated in [2,18], the MSE loss assumes pixel-wise isolation and independence.As a result, it is incapable of enforcing spatial correlation and coherence among pixels in the estimated maps, which, however, plays an important role in influencing the quality of the density map.To compensate the limitation of the MSE loss, we define a spatial abstraction loss (SAL) and a spatial correlation loss (SCL), resulting in a combinatorial loss.Spatial Abstraction Loss.SAL progressively computes the MSE losses on multiple abstraction levels between the predicted map and the ground truth.These spatial abstractions are instantiated by cascading max pooling layers with down-sampling strides, leading to a gradually enlarged receptive field on each level.At each level, the pixel value in an abstracted map is non-linearly sampled from a receptive field at the corresponding location in the preceding abstraction level.By computing MSE on each abstraction level, SAL can supplement the pixel-wise MSE loss with patchwise supervision.In our experiments, after a normal MSE loss, we implement three levels of abstraction (K = 3 in the following equation) with 2 \u00d7 2 max pooling layers, each with a stride of 2. The computation of SAL is formalized as:\nL SA = K k=1 1 N k \u03d5 k (Z) \u2212 \u03d5 k (Y ) 2 2 ,(2)\nwhere \u03d5 k (\u2022) denotes the abstraction computation on the kth abstraction level.N k is the number of pixels within a map on the k-th abstraction level.\n\nSpatial Correlation Loss.Beyond the patch-wise supervision enforced by SAL, SCL further complements the pixel-wise MSE loss with map-wise computation.SCL represents the difference between two density maps based on normalized cross correlation (NCC) similarity.This is less sensitive to linear changes in the density map intensity.\n\nIn addition, SCL is easier to compute and experimentally friendly compared to the MSE loss.The computation of SCL defined on two maps is:\nL SC = 1 \u2212 P p Q q (Z pq \u2022 Y pq ) P p Q q Z pq 2 \u2022 P p Q q Y pq 2 ,(3)\nwhere Y pq and Z pq represent the pixels in the ground truth density map and the predicted density map, respectively.p and q are the row and column indexes in the map, and P \u00d7Q denotes the total number of pixels.The final combinatorial loss L is formulated as a weighted sum of SAL and SCL as:\nL = L SA + \u03bbL SC ,(4)\nwhere \u03bb is a factor to balance the contributions of SAL and SCL.The selection of \u03bb is explained in Section 4.2.\n\n\nExperiments and Results\n\n\nImplementation details\n\nFollowing [46], we generate our ground truth density maps by fixed size Gaussian kernels and augment the training data with an online sampling strategy (more details can be found in [46]).We train our TEDnet in an end-to-end manner from scratch, and optimize the network parameters based on the Adam optimizer [13], with an initial learning rate of 1e \u2212 3. The learning rate is step-wise and decreased by a factor of 0.8 every 10K iterations.\n\nImage-wise Operation.To generate high-quality fullresolution density maps, TEDnet takes full-size images as inputs and outputs the same size density maps.Our approach differs from methods that adopt patch-wise operations [51,43,26,1,36,20,11,2]. Notably, patch-wise operations induce boundary artifacts that negatively effect the localization precision.Moreover, the patch-wise counting accuracy suffers from statistical shifts across patches [2].\n\nCounting Accuracy.To evaluate the counting accuracy, we adopt the mean average error (MAE) and the mean squared error (MSE) metrics, which are defined as:\nMAE = 1 M M i=1 |C i \u2212 C gt i |, MSE = 1 M M i=1 |C i \u2212 C gt i | 2 (5)\nwhere M is the number of images in the test set, and C gt i and C i represent the ground truth and the predicted count of the i-th image, computed as the integral of the density maps.\n\nDensity Map Quality.To evaluate the quality of the estimated density maps, we also calculate the PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity in Image) indices, as described in [39].In particular, the SSIM index is normally adopted in image quality assessment [45], and it computes the similarity between two images from the mean, variance and covariance statistics.\n\n\nAblation Study\n\nThe ablation study results are shown in Table 1.The table is partitioned row-wise into three groups, with five configurations.Each group contains the indexed configurations corresponding to one main contribution of TEDnet.These include the trellis network with multi-path decoding, the distributed supervision, and the combinatorial loss with   SAL and SCL.In different columns, we report the counting accuracy of each configuration, using the MAE metric.We also illustrate the quality of the density map using the PSNR metrics.In Figure 7, the left picture illustrates the convergence performance of configurations 1, 2, and 3, demonstrating the benefits for convergence introduced by the dense skip connections and the distributed supervision.The right picture in Figure 7 shows that when \u03bb = 1, i.e. the SAL and SCL are equally weighted in the combinatorial loss, the best counting accuracy is reported.The first group of configurations shown in Table 1 compare the performance of the multi-path trellis decoder and single-path hourglass decoder.Two configurations in this group adopt the same trellis encoder, and single supervision is applied on density map Z with the normal MSE loss.The results in this group show that the multi-path decoder improves the counting accuracy by 2.6% in terms of MAE metric, and enhance the density map quality by 7.1% in PSNR metric.Furthermore, the yellow curve in Figure 7 demonstrates faster convergence thanks to the dense skip connections implemented in the multi-path decoder.The second group of configurations are all set up with TEDnet, using the normal MSE loss.The results show that distributed supervision improves the MAE by 2.8% and PSNR by 1.9%, and the green curve shows further improved convergence speed and performance.In the last group, we compare the performance of distributedly deploying different losses.The combinatorial loss with both SAL and SCL (\u03bb = 1) stands out, with 7.2% improvements in MAE and 4.5% in PSNR, which confirms that higher density map quality can improve the counting accuracy.Overall, the best result is reported by configuration 5, which incorporates all three contributions.\n\n\nPerformance and Comparison\n\nWe compare the performance of our TEDnet with eight state-of-the-art approaches, on four challenging datasets, including the ShanghaiTech [52], the UCF CC 50 [10], WorldExpo '10 [51] and the UCF-QNRF [11].We explain the superior performance of TEDnet in terms of both counting accuracy (MAE and MSE as shown in Table 2 and  3) and density map quality measures (SSIM and PSNR as shown in Table 4).To settle the sample scarcity problem, we perform a 5-fold cross-validation, following the standard setting in [10].As shown in Table 2, we achieve an improvement of 3.5% in terms of the MAE metric.UCF-QNRF.The UCF-QNRF is a new dataset with one of the highest number of high-count crowd images and annotations.We compare our result with four state-of-theart methods and our method achieves the best performance in terms of both MAE and MSE.We beat the second best approach by a 14.4% improvement in MAE and 1.6% improvement in MSE, as shown in Table 2.\n\nWorldExpo10.The WorldExpo10 dataset was introduced by Zhang et al. [51] and contains 3980 frames from 108 different scenes from the Shanghai 2010 WorldExpo.Table 3 shows that TEDnet delivers the lowest MAE in 3 out of 5 test scenes, and reports up to 13.3% improvement in Scene 5 over other methods.Overall, we achieve the best performance in terms of average MAE, outperforming the second best by 2.4%.\n\n\nDensity map Quality\n\nAs mentioned in Section 2, CP-CNN [39] and CSRnet [18] also emphasize generating high-quality density maps.MCNN [52] is one of the most representative methods in density estimation based crowd counting.We compare the quality of density maps estimated by TEDnet and these three state-of-the-art systems.Quantitatively, as demonstrated in Table 4, our method outperforms the other methods in both PSNR and SSIM metrics on the ShanghaiTech Part A dataset.Particularly, we obtain 8.1% and 8.4% improvements over the second best method, in terms of PSNR and SSIM metrics.Qualitatively, we visualize the density maps generated by MCNN, SANet, and TEDnet on the ShanghaiTech Part A dataset in Figure 2. In addition, we also display the density maps generated by TEDnet on other datasets in Figure 6.\n\nOur TEDnet introduces an enhanced multi-path decoder architecture, which, however, is still lightweight compared to other state-of-the-art methods, which also strive to generate high-quality density maps.As shown in Table 4, the number of parameters in TEDnet is only equal to 10% of those in CRSNet and 2.4% of CP-CNN.More importantly, we demonstrate the best overall performance in density map quality as well as counting accuracy.MCNN is the most lightweight network, yet we show significant performance improvement in PSNR by 17.3% and 36% in SSIM.Moreover, we also outperform MCNN on all datasets in terms of MAE and MSE.\n\n\nConclusion\n\nIn this paper, we have presented a new deep learning architecture, called the trellis encoder-decoder network (TEDnet) for crowd counting.It consists of a multi-scale encoder and a multi-path decoder to generate high-quality density estimation maps.It preserves the localization precision in the encoded feature maps, upon which a multi-path decoder with dense skip connections is adopted to achieve thorough aggregation and fusion of multi-scale features.The TEDnet is trained with the distributed supervision implemented with the proposed combinatorial loss.Experiments on four benchmarks show that the TEDnet achieves new state-of-the-art performance in terms of both density map quality and crowd counting accuracy.\n\nFigure 1 .\n1\nFigure 1.An illustration of the Trellis Encoder-Decoder network (TEDnet) with distributed combinatorial losses.The horizontal and vertical axes indicate the spatial-semantic-spatial feature hierarchy established within TEDnet.We instantiate this hierarchy into a feature grid, whose rows and columns are indexed on the margin.The spatial and channel dimensions of each feature map is denoted by its side.\n\n\nFigure 3 .\n3\nFigure 3.An illustration of the multi-scale encoding block.Circled C in the figure represents channel-wise concatenation.\n\n\nFigure 4 .\n4\nFigure 4.An illustration of the decoding block (a) and the upsampling block (b).Circled C in the figure represents channelwise concatenation.\n\n\nFigure 5 .\n5\nFigure 5.An illustration of the combinatorial loss L. Z indicates one of the distributed outputs, while Y is the corresponding ground truth map, resized to be the same as Z.\n\n\nFigure 6 .Figure 7 .\n67\nFigure 6.From left to right, we display the density maps generated by TEDnet on ShanghaiTech Part A, ShanghaiTech Part B, UCF-QNRF, UCF CC 50, and WorldExpo'10 datasets.The second row shows ground truth density map, the third row depicts our estimated maps.\n\n\n4. 3 . 1\n31\nCounting Accuracy ShanghaiTech.The ShanghaiTech dataset is one of the largest datasets which includes Part A and Part B subsets.As shown in\n\n\n\n\n\n\n\nTable 1 .\n1\nAblation study results on ShanghaiTech Part A dataset.Best performance in each group is bolded.Arrows in all tables indicate the favorable directions of the metric values.\nConfigurationsMAE\u2193PSNR\u2191Network1Trellis Encoder + Single path Decoder73.122.51Structure2Trellis Encoder + Multi-path Trellis Decoder71.224.24Supervision2Single Supervision71.224.24Methodology3Distributed Supervision69.224.713Normal MSE69.224.71Loss Function4SAL67.824.945SAL + SCL64.225.88\n\nTable 2 ,\n2\non Part A, our method achieves the lowest MAE and a competitive MSE.In terms of MAE, we\n\n\nTable 2 .\n2\nEstimation errors on the ShanghaiTech dataset, the UCF CC 50 and the UCF-QNRF dataset\nShanghaiTech Part AShanghaiTech Part BUCF CC 50UCF-QNRFMethodMAE\u2193MSE\u2193MAE\u2193MSE\u2193MAE\u2193MSE\u2193MAE\u2193MSE\u2193Zhang et al.[51]181.8277.732.049.8467.0498.5--MCNN [52]110.2173.226.441.3377.6509.1277426Cascaded-MTL [38]101.3152.420.031.1322.8397.9252514Switching-CNN [36]90.4135.021.633.4318.1439.2228445CP-CNN [39]73.6106.420.130.1295.8320.9--CSRNet [18]68.2115.010.616.0266.1397.5--SANet [2]67.0104.58.413.6258.4334.9--Idrees et al. [11]------132191Ours64.2109.18.212.8249.4354.5113188\n\nTable 3 .\n3\n[10]MAE of the WorldExpo'10 dataset, S is short for Scene.lead the second best by 4.2%.On Part B, we report the best performance in terms of both two metrics, where MSE is improved by 5.9%.The significant improvements on this dataset validate the effectiveness of TEDnet.UCF CC 50.The UCF CC 50 dataset introduced by Idrees et al.[10]contains 50 images of varying resolutions, with a wide range of densities.\nMethodS1S2S3S4S5Ave.Zhang et al. [51]9.814.114.322.23.712.9MCNN [52]3.420.612.913.08.111.6Switching-CNN [36]4.415.710.011.05.99.4CP-CNN [39]2.914.710.510.45.88.9CRSNet [18]2.911.58.616.63.48.6SANet [2]2.613.29.013.33.08.2Ours2.310.111.313.82.68.0\n\nTable 4 .\n4\nQuality of density map on ShanghaiTech Part A dataset and parameter studies, M stands for millions.\nMethodPSNR\u2191SSIM\u2191ParametersMCNN [52]21.40.520.13MCP-CNN [39]21.720.7268.4MCRSNet [18]23.790.7616.26MOurs25.880.831.63M\n\nCrowdnet: A deep convolutional network for dense crowd counting. L Boominathan, S S Kruthiventi, R V Babu, Proceedings of the 2016 ACM on Multimedia Conference. the 2016 ACM on Multimedia ConferenceACM2016. 2, 3, 4, 6\n\nScale aggregation network for accurate and efficient crowd counting. X Cao, Z Wang, Y Zhao, F Su, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018. 1, 2, 3, 4, 6, 8\n\nBayesian poisson regression for crowd counting. A B Chan, N Vasconcelos, IEEE 12th International Conference on. IEEE2009. 2009Computer Vision\n\nFeature mining for localised crowd counting. K Chen, C C Loy, S Gong, T Xiang, BMVC. 201213\n\nPedestrian detection: An evaluation of the state of the art. P Dollar, C Wojek, B Schiele, P Perona, IEEE transactions on pattern analysis and machine intelligence. 201234\n\nAccelerating the superresolution convolutional neural network. C Dong, C C Loy, X Tang, European Conference on Computer Vision. Springer201614\n\nLearning to count with regression forest and structured labels. L Fiaschi, U K\u00f6the, R Nair, F A Hamprecht, Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE2012\n\nStacked deconvolutional network for semantic segmentation. J Fu, J Liu, Y Wang, H Lu, arXiv:1708.049432017. 1, 2, 4, 5arXiv preprint\n\nMarked point processes for crowd counting. W Ge, R T Collins, Computer Vision and Pattern Recognition. IEEE2009. 200913CVPR 2009. IEEE Conference on\n\nMulti-source multi-scale counting in extremely dense crowd images. H Idrees, I Saleemi, C Seibert, M Shah, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2013. 1, 3, 7, 8\n\nComposition loss for counting, density map estimation and localization in dense crowds. H Idrees, M Tayyab, K Athrey, D Zhang, S Al-Maadeed, N Rajpoot, M Shah, arXiv:1808.010502018. 1, 2, 3, 6, 7, 8arXiv preprint\n\nBeyond counting: comparisons of density maps for crowd analysis tasks-counting, detection, and tracking. D Kang, Z Ma, A B Chan, 2018Technology\n\nD P Kingma, J Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint\n\nMixture of counting cnns: Adaptive integration of cnns specialized to specific appearance for crowd counting. S Kumagai, K Hotta, T Kurita, arXiv:1703.093932017arXiv preprint\n\nDeeplysupervised nets. C.-Y Lee, S Xie, P Gallagher, Z Zhang, Z Tu, Artificial Intelligence and Statistics. 2015\n\nLearning to count objects in images. V Lempitsky, A Zisserman, Advances in neural information processing systems. 201013\n\nEstimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. M Li, Z Zhang, K Huang, T Tan, Pattern Recognition, 2008. ICPR 2008. 19th International Conference on. IEEE200813\n\nCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes. Y Li, X Zhang, D Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018. 2, 3, 4, 6, 8\n\nRefinenet: Multipath refinement networks for high-resolution semantic segmentation. G Lin, A Milan, C Shen, I D Reid, Cvpr. 201714\n\nDecidenet: Counting varying density crowds through attention guided detection and density estimation. J Liu, C Gao, D Meng, A G Hauptmann, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition201836\n\nLeveraging unlabeled data for crowd counting by learning to rank. X Liu, J Van De Weijer, A D Bagdanov, arXiv:1803.030952018arXiv preprint\n\nE Lu, W Xie, A Zisserman, arXiv:1811.00472Class-agnostic counting. 2018arXiv preprint\n\nSmall instance detection by integer programming on object density maps. Z Ma, L Yu, A B Chan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition201523\n\nLight-weight refinenet for real-time semantic segmentation. V Nekrasov, C Shen, I Reid, arXiv:1810.03272201814arXiv preprint\n\nDeconvolution and checkerboard artifacts. A Odena, V Dumoulin, C Olah, Distill. 52016\n\nTowards perspective-free object counting with deep learning. D Onoro-Rubio, R J L\u00f3pez-Sastre, European Conference on Computer Vision. \n\n. Springer, 2016. 2, 3, 6\n\nA mrf-based approach for realtime subway monitoring. N Paragios, V Ramesh, CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on. 2001. 20011Computer Vision and Pattern Recognition\n\nCount forest: Co-voting uncertain number of targets using random forest for crowd density estimation. V.-Q Pham, T Kozakaya, O Yamaguchi, R Okada, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2015\n\nFusing crowd density maps and visual object trackers for people tracking in crowd scenes. W Ren, D Kang, Y Tang, A B Chan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition201823\n\nDensity-aware person detection and tracking in crowds. M Rodriguez, I Laptev, J Sivic, J.-Y Audibert, Computer Vision (ICCV), 2011 IEEE International Conference on. 2011\n\nDensity-aware person detection and tracking in crowds. M Rodriguez, I Laptev, J Sivic, J.-Y Audibert, Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE2011\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical image computing and computer-assisted intervention. Springer201514\n\nCrowd counting using multiple local features. D Ryan, S Denman, C Fookes, S Sridharan, Digital Image Computing: Techniques and Applications, 2009. DICTA'09. IEEE2009\n\nAn evaluation of crowd counting methods, features and regression models. D Ryan, S Denman, S Sridharan, C Fookes, Computer Vision and Image Understanding. 13032015\n\nRecent survey on crowd density estimation and counting for visual surveillance. S A M Saleh, S A Suandi, H Ibrahim, Engineering Applications of Artificial Intelligence. 4132015\n\nSwitching convolutional neural network for crowd counting. D B Sam, S Surya, R V Babu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition201716\n\nCrowd counting with deep negative correlation learning. Z Shi, L Zhang, Y Liu, X Cao, Y Ye, M.-M Cheng, G Zheng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018\n\nCnn-based cascaded multitask learning of high-level prior and density estimation for crowd counting. V A Sindagi, V M Patel, Advanced Video and Signal Based Surveillance (AVSS), 2017 14th IEEE International Conference on. IEEE2017\n\nGenerating high-quality crowd density maps using contextual pyramid cnns. V A Sindagi, V M Patel, 2017 IEEE International Conference on Computer Vision (ICCV). 2017. 2, 3, 6, 8\n\nA survey of recent advances in cnn-based single image crowd counting and density estimation. V A Sindagi, V M Patel, Pattern Recognition Letters. 10732018\n\nDetail-revealing deep video super-resolution. X Tao, H Gao, R Liao, J Wang, J Jia, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionVenice, Italy201714\n\nEnd-to-end representation learning for correlation filter based tracking. J Valmadre, L Bertinetto, J Henriques, A Vedaldi, P H Torr, Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. 2017\n\nLearning to count with cnn boosting. E Walach, L Wolf, European Conference on Computer Vision. Springer201636\n\nAutomatic adaptation of a generic pedestrian detector to a specific traffic scene. M Wang, X Wang, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE201113\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE transactions on image processing. 1342004\n\nZ Wang, Z Xiao, K Xie, Q Qiu, X Zhen, X Cao, arXiv:1808.06133defense of single-column networks for crowd counting. 201836arXiv preprint\n\nHolistically-nested edge detection. S Xie, Z Tu, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015\n\nCrowd density estimation based on rich features and random projection forest. B Xu, G Qiu, Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on. IEEE2016\n\nStacked hourglass network for robust facial landmark localisation. J Yang, Q Liu, K Zhang, Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on. 2017\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2015\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2015. 3, 6, 7, 8\n\nSingleimage crowd counting via multi-column convolutional neural network. Y Zhang, D Zhou, S Chen, S Gao, Y Ma, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016. 1, 2, 3, 6, 7, 8\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 201714\n", "annotations": {"author": "[{\"end\":135,\"start\":87},{\"end\":166,\"start\":136},{\"end\":235,\"start\":167},{\"end\":349,\"start\":236},{\"end\":413,\"start\":350},{\"end\":485,\"start\":414},{\"end\":564,\"start\":486}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":96},{\"end\":146,\"start\":142},{\"end\":181,\"start\":176},{\"end\":249,\"start\":245},{\"end\":361,\"start\":358},{\"end\":428,\"start\":420},{\"end\":495,\"start\":491}]", "author_first_name": "[{\"end\":95,\"start\":87},{\"end\":141,\"start\":136},{\"end\":175,\"start\":167},{\"end\":244,\"start\":236},{\"end\":357,\"start\":350},{\"end\":419,\"start\":414},{\"end\":490,\"start\":486}]", "author_affiliation": "[{\"end\":134,\"start\":103},{\"end\":234,\"start\":203},{\"end\":299,\"start\":268},{\"end\":348,\"start\":301},{\"end\":412,\"start\":381},{\"end\":484,\"start\":451},{\"end\":563,\"start\":516}]", "title": "[{\"end\":74,\"start\":1},{\"end\":638,\"start\":565}]", "venue": null, "abstract": "[{\"end\":2004,\"start\":707}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2396,\"start\":2392},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2398,\"start\":2396},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2401,\"start\":2398},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2418,\"start\":2414},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2420,\"start\":2418},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2423,\"start\":2420},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2452,\"start\":2448},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2455,\"start\":2452},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2457,\"start\":2455},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2459,\"start\":2457},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2897,\"start\":2894},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2900,\"start\":2897},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2903,\"start\":2900},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2906,\"start\":2903},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2908,\"start\":2906},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2911,\"start\":2908},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2914,\"start\":2911},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3336,\"start\":3332},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3347,\"start\":3344},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3568,\"start\":3564},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3571,\"start\":3568},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3574,\"start\":3571},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3577,\"start\":3574},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3580,\"start\":3577},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4234,\"start\":4230},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4236,\"start\":4234},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4239,\"start\":4236},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4242,\"start\":4239},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4244,\"start\":4242},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4247,\"start\":4244},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4388,\"start\":4384},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4391,\"start\":4388},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4394,\"start\":4391},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4409,\"start\":4406},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4428,\"start\":4424},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5198,\"start\":5194},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":5201,\"start\":5198},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5203,\"start\":5201},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5206,\"start\":5203},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5611,\"start\":5607},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5614,\"start\":5611},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5616,\"start\":5614},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5619,\"start\":5616},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9344,\"start\":9340},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9347,\"start\":9344},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9350,\"start\":9347},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9353,\"start\":9350},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9527,\"start\":9524},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9529,\"start\":9527},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9532,\"start\":9529},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9535,\"start\":9532},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9726,\"start\":9723},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9729,\"start\":9726},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9732,\"start\":9729},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9735,\"start\":9732},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9738,\"start\":9735},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10213,\"start\":10209},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10417,\"start\":10413},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10419,\"start\":10417},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10453,\"start\":10450},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10456,\"start\":10453},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10459,\"start\":10456},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10594,\"start\":10590},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10608,\"start\":10605},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10624,\"start\":10620},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10639,\"start\":10635},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10652,\"start\":10648},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10676,\"start\":10672},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10967,\"start\":10963},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11056,\"start\":11053},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11059,\"start\":11056},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11062,\"start\":11059},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11138,\"start\":11134},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11141,\"start\":11138},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11144,\"start\":11141},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11146,\"start\":11144},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11149,\"start\":11146},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11152,\"start\":11149},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11155,\"start\":11152},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11157,\"start\":11155},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11496,\"start\":11492},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11614,\"start\":11611},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11833,\"start\":11829},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12450,\"start\":12446},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12453,\"start\":12450},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12456,\"start\":12453},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12475,\"start\":12471},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12703,\"start\":12699},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12706,\"start\":12703},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12945,\"start\":12942},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13114,\"start\":13110},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13117,\"start\":13114},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15152,\"start\":15148},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15580,\"start\":15576},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15710,\"start\":15707},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15779,\"start\":15776},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16076,\"start\":16072},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16079,\"start\":16076},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16081,\"start\":16079},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16084,\"start\":16081},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16108,\"start\":16105},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16111,\"start\":16108},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18866,\"start\":18862},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20530,\"start\":20526},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20533,\"start\":20530},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20535,\"start\":20533},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22373,\"start\":22369},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22375,\"start\":22373},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22378,\"start\":22375},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22398,\"start\":22395},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22401,\"start\":22398},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24861,\"start\":24857},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":25033,\"start\":25029},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25161,\"start\":25157},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25516,\"start\":25512},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25519,\"start\":25516},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25522,\"start\":25519},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25524,\"start\":25522},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25527,\"start\":25524},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25530,\"start\":25527},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25533,\"start\":25530},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25535,\"start\":25533},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25737,\"start\":25734},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26353,\"start\":26349},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":26436,\"start\":26432},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":28889,\"start\":28885},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28909,\"start\":28905},{\"end\":28929,\"start\":28921},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28951,\"start\":28947},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29258,\"start\":29254},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29769,\"start\":29765},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30163,\"start\":30159},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30179,\"start\":30175},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":30241,\"start\":30237}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32699,\"start\":32280},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32836,\"start\":32700},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32993,\"start\":32837},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33182,\"start\":32994},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33466,\"start\":33183},{\"attributes\":{\"id\":\"fig_6\"},\"end\":33620,\"start\":33467},{\"end\":33625,\"start\":33621},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34099,\"start\":33626},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34201,\"start\":34100},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34768,\"start\":34202},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35437,\"start\":34769},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35668,\"start\":35438}]", "paragraph": "[{\"end\":3476,\"start\":2020},{\"end\":3936,\"start\":3478},{\"end\":4850,\"start\":3938},{\"end\":6061,\"start\":4852},{\"end\":7770,\"start\":6063},{\"end\":8118,\"start\":7772},{\"end\":8396,\"start\":8120},{\"end\":8630,\"start\":8398},{\"end\":8941,\"start\":8632},{\"end\":9201,\"start\":8943},{\"end\":9354,\"start\":9218},{\"end\":10148,\"start\":9397},{\"end\":11899,\"start\":10185},{\"end\":13218,\"start\":11901},{\"end\":14163,\"start\":13255},{\"end\":14497,\"start\":14187},{\"end\":15153,\"start\":14499},{\"end\":15693,\"start\":15176},{\"end\":16370,\"start\":15695},{\"end\":17369,\"start\":16372},{\"end\":17964,\"start\":17371},{\"end\":18115,\"start\":17966},{\"end\":18591,\"start\":18212},{\"end\":18732,\"start\":18593},{\"end\":19140,\"start\":18734},{\"end\":20056,\"start\":19142},{\"end\":21404,\"start\":20084},{\"end\":22102,\"start\":21406},{\"end\":23626,\"start\":22125},{\"end\":23824,\"start\":23674},{\"end\":24156,\"start\":23826},{\"end\":24295,\"start\":24158},{\"end\":24660,\"start\":24367},{\"end\":24794,\"start\":24683},{\"end\":25289,\"start\":24847},{\"end\":25738,\"start\":25291},{\"end\":25894,\"start\":25740},{\"end\":26149,\"start\":25966},{\"end\":26538,\"start\":26151},{\"end\":28716,\"start\":26557},{\"end\":29696,\"start\":28747},{\"end\":30101,\"start\":29698},{\"end\":30917,\"start\":30125},{\"end\":31545,\"start\":30919},{\"end\":32279,\"start\":31560},{\"end\":32698,\"start\":32294},{\"end\":32835,\"start\":32714},{\"end\":32992,\"start\":32851},{\"end\":33181,\"start\":33008},{\"end\":33465,\"start\":33208},{\"end\":33619,\"start\":33480},{\"end\":33810,\"start\":33639},{\"end\":34200,\"start\":34113},{\"end\":34300,\"start\":34215},{\"end\":35190,\"start\":34782},{\"end\":35550,\"start\":35451}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18211,\"start\":18116},{\"attributes\":{\"id\":\"formula_1\"},\"end\":23673,\"start\":23627},{\"attributes\":{\"id\":\"formula_2\"},\"end\":24366,\"start\":24296},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24682,\"start\":24661},{\"attributes\":{\"id\":\"formula_4\"},\"end\":25965,\"start\":25895}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26604,\"start\":26603},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27513,\"start\":27512},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29072,\"start\":29064},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29141,\"start\":29140},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29278,\"start\":29277},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29695,\"start\":29694},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29861,\"start\":29860},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30469,\"start\":30468},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31142,\"start\":31141}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2018,\"start\":2006},{\"attributes\":{\"n\":\"2.\"},\"end\":9216,\"start\":9204},{\"attributes\":{\"n\":\"2.1.\"},\"end\":9395,\"start\":9357},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10183,\"start\":10151},{\"attributes\":{\"n\":\"3.\"},\"end\":13253,\"start\":13221},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14185,\"start\":14166},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15174,\"start\":15156},{\"attributes\":{\"n\":\"3.3.\"},\"end\":20082,\"start\":20059},{\"attributes\":{\"n\":\"3.4.\"},\"end\":22123,\"start\":22105},{\"attributes\":{\"n\":\"4.\"},\"end\":24820,\"start\":24797},{\"attributes\":{\"n\":\"4.1.\"},\"end\":24845,\"start\":24823},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26555,\"start\":26541},{\"attributes\":{\"n\":\"4.3.\"},\"end\":28745,\"start\":28719},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":30123,\"start\":30104},{\"attributes\":{\"n\":\"5.\"},\"end\":31558,\"start\":31548},{\"end\":32291,\"start\":32281},{\"end\":32711,\"start\":32701},{\"end\":32848,\"start\":32838},{\"end\":33005,\"start\":32995},{\"end\":33204,\"start\":33184},{\"end\":33476,\"start\":33468},{\"end\":33636,\"start\":33627},{\"end\":34110,\"start\":34101},{\"end\":34212,\"start\":34203},{\"end\":34779,\"start\":34770},{\"end\":35448,\"start\":35439}]", "table": "[{\"end\":34099,\"start\":33811},{\"end\":34768,\"start\":34301},{\"end\":35437,\"start\":35191},{\"end\":35668,\"start\":35551}]", "figure_caption": "[{\"end\":32699,\"start\":32293},{\"end\":32836,\"start\":32713},{\"end\":32993,\"start\":32850},{\"end\":33182,\"start\":33007},{\"end\":33466,\"start\":33207},{\"end\":33620,\"start\":33479},{\"end\":33625,\"start\":33623},{\"end\":33811,\"start\":33638},{\"end\":34201,\"start\":34112},{\"end\":34301,\"start\":34214},{\"end\":35191,\"start\":34781},{\"end\":35551,\"start\":35450}]", "figure_ref": "[{\"end\":2981,\"start\":2980},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7140,\"start\":7139},{\"end\":7963,\"start\":7962},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13275,\"start\":13274},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14519,\"start\":14518},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14680,\"start\":14679},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16572,\"start\":16571},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17175,\"start\":17174},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17406,\"start\":17405},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18368,\"start\":18367},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18778,\"start\":18777},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19236,\"start\":19235},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19833,\"start\":19832},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20257,\"start\":20256},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22145,\"start\":22144},{\"end\":27096,\"start\":27095},{\"end\":27331,\"start\":27330},{\"end\":27969,\"start\":27968},{\"end\":30819,\"start\":30818},{\"end\":30916,\"start\":30915}]", "bib_author_first_name": "[{\"end\":35736,\"start\":35735},{\"end\":35751,\"start\":35750},{\"end\":35753,\"start\":35752},{\"end\":35768,\"start\":35767},{\"end\":35770,\"start\":35769},{\"end\":35959,\"start\":35958},{\"end\":35966,\"start\":35965},{\"end\":35974,\"start\":35973},{\"end\":35982,\"start\":35981},{\"end\":36175,\"start\":36174},{\"end\":36177,\"start\":36176},{\"end\":36185,\"start\":36184},{\"end\":36315,\"start\":36314},{\"end\":36323,\"start\":36322},{\"end\":36325,\"start\":36324},{\"end\":36332,\"start\":36331},{\"end\":36340,\"start\":36339},{\"end\":36424,\"start\":36423},{\"end\":36434,\"start\":36433},{\"end\":36443,\"start\":36442},{\"end\":36454,\"start\":36453},{\"end\":36599,\"start\":36598},{\"end\":36607,\"start\":36606},{\"end\":36609,\"start\":36608},{\"end\":36616,\"start\":36615},{\"end\":36744,\"start\":36743},{\"end\":36755,\"start\":36754},{\"end\":36764,\"start\":36763},{\"end\":36772,\"start\":36771},{\"end\":36774,\"start\":36773},{\"end\":36923,\"start\":36922},{\"end\":36929,\"start\":36928},{\"end\":36936,\"start\":36935},{\"end\":36944,\"start\":36943},{\"end\":37041,\"start\":37040},{\"end\":37047,\"start\":37046},{\"end\":37049,\"start\":37048},{\"end\":37215,\"start\":37214},{\"end\":37225,\"start\":37224},{\"end\":37236,\"start\":37235},{\"end\":37247,\"start\":37246},{\"end\":37502,\"start\":37501},{\"end\":37512,\"start\":37511},{\"end\":37522,\"start\":37521},{\"end\":37532,\"start\":37531},{\"end\":37541,\"start\":37540},{\"end\":37555,\"start\":37554},{\"end\":37566,\"start\":37565},{\"end\":37733,\"start\":37732},{\"end\":37741,\"start\":37740},{\"end\":37747,\"start\":37746},{\"end\":37749,\"start\":37748},{\"end\":37773,\"start\":37772},{\"end\":37775,\"start\":37774},{\"end\":37785,\"start\":37784},{\"end\":37980,\"start\":37979},{\"end\":37991,\"start\":37990},{\"end\":38000,\"start\":37999},{\"end\":38072,\"start\":38068},{\"end\":38079,\"start\":38078},{\"end\":38086,\"start\":38085},{\"end\":38099,\"start\":38098},{\"end\":38108,\"start\":38107},{\"end\":38197,\"start\":38196},{\"end\":38210,\"start\":38209},{\"end\":38398,\"start\":38397},{\"end\":38404,\"start\":38403},{\"end\":38413,\"start\":38412},{\"end\":38422,\"start\":38421},{\"end\":38606,\"start\":38605},{\"end\":38612,\"start\":38611},{\"end\":38621,\"start\":38620},{\"end\":38875,\"start\":38874},{\"end\":38882,\"start\":38881},{\"end\":38891,\"start\":38890},{\"end\":38899,\"start\":38898},{\"end\":38901,\"start\":38900},{\"end\":39025,\"start\":39024},{\"end\":39032,\"start\":39031},{\"end\":39039,\"start\":39038},{\"end\":39047,\"start\":39046},{\"end\":39049,\"start\":39048},{\"end\":39277,\"start\":39276},{\"end\":39284,\"start\":39283},{\"end\":39301,\"start\":39300},{\"end\":39303,\"start\":39302},{\"end\":39351,\"start\":39350},{\"end\":39357,\"start\":39356},{\"end\":39364,\"start\":39363},{\"end\":39510,\"start\":39509},{\"end\":39516,\"start\":39515},{\"end\":39522,\"start\":39521},{\"end\":39524,\"start\":39523},{\"end\":39741,\"start\":39740},{\"end\":39753,\"start\":39752},{\"end\":39761,\"start\":39760},{\"end\":39849,\"start\":39848},{\"end\":39858,\"start\":39857},{\"end\":39870,\"start\":39869},{\"end\":39955,\"start\":39954},{\"end\":39970,\"start\":39969},{\"end\":39972,\"start\":39971},{\"end\":40110,\"start\":40109},{\"end\":40122,\"start\":40121},{\"end\":40361,\"start\":40357},{\"end\":40369,\"start\":40368},{\"end\":40381,\"start\":40380},{\"end\":40394,\"start\":40393},{\"end\":40620,\"start\":40619},{\"end\":40627,\"start\":40626},{\"end\":40635,\"start\":40634},{\"end\":40643,\"start\":40642},{\"end\":40645,\"start\":40644},{\"end\":40857,\"start\":40856},{\"end\":40870,\"start\":40869},{\"end\":40880,\"start\":40879},{\"end\":40892,\"start\":40888},{\"end\":41028,\"start\":41027},{\"end\":41041,\"start\":41040},{\"end\":41051,\"start\":41050},{\"end\":41063,\"start\":41059},{\"end\":41213,\"start\":41212},{\"end\":41228,\"start\":41227},{\"end\":41239,\"start\":41238},{\"end\":41397,\"start\":41396},{\"end\":41405,\"start\":41404},{\"end\":41415,\"start\":41414},{\"end\":41425,\"start\":41424},{\"end\":41591,\"start\":41590},{\"end\":41599,\"start\":41598},{\"end\":41609,\"start\":41608},{\"end\":41622,\"start\":41621},{\"end\":41763,\"start\":41762},{\"end\":41767,\"start\":41764},{\"end\":41776,\"start\":41775},{\"end\":41778,\"start\":41777},{\"end\":41788,\"start\":41787},{\"end\":41920,\"start\":41919},{\"end\":41922,\"start\":41921},{\"end\":41929,\"start\":41928},{\"end\":41938,\"start\":41937},{\"end\":41940,\"start\":41939},{\"end\":42153,\"start\":42152},{\"end\":42160,\"start\":42159},{\"end\":42169,\"start\":42168},{\"end\":42176,\"start\":42175},{\"end\":42183,\"start\":42182},{\"end\":42192,\"start\":42188},{\"end\":42201,\"start\":42200},{\"end\":42458,\"start\":42457},{\"end\":42460,\"start\":42459},{\"end\":42471,\"start\":42470},{\"end\":42473,\"start\":42472},{\"end\":42663,\"start\":42662},{\"end\":42665,\"start\":42664},{\"end\":42676,\"start\":42675},{\"end\":42678,\"start\":42677},{\"end\":42860,\"start\":42859},{\"end\":42862,\"start\":42861},{\"end\":42873,\"start\":42872},{\"end\":42875,\"start\":42874},{\"end\":42969,\"start\":42968},{\"end\":42976,\"start\":42975},{\"end\":42983,\"start\":42982},{\"end\":42991,\"start\":42990},{\"end\":42999,\"start\":42998},{\"end\":43222,\"start\":43221},{\"end\":43234,\"start\":43233},{\"end\":43248,\"start\":43247},{\"end\":43261,\"start\":43260},{\"end\":43272,\"start\":43271},{\"end\":43274,\"start\":43273},{\"end\":43398,\"start\":43397},{\"end\":43408,\"start\":43407},{\"end\":43555,\"start\":43554},{\"end\":43563,\"start\":43562},{\"end\":43730,\"start\":43729},{\"end\":43738,\"start\":43737},{\"end\":43740,\"start\":43739},{\"end\":43749,\"start\":43748},{\"end\":43751,\"start\":43750},{\"end\":43761,\"start\":43760},{\"end\":43763,\"start\":43762},{\"end\":43825,\"start\":43824},{\"end\":43833,\"start\":43832},{\"end\":43841,\"start\":43840},{\"end\":43848,\"start\":43847},{\"end\":43855,\"start\":43854},{\"end\":43863,\"start\":43862},{\"end\":43998,\"start\":43997},{\"end\":44005,\"start\":44004},{\"end\":44216,\"start\":44215},{\"end\":44222,\"start\":44221},{\"end\":44378,\"start\":44377},{\"end\":44386,\"start\":44385},{\"end\":44393,\"start\":44392},{\"end\":44559,\"start\":44558},{\"end\":44568,\"start\":44567},{\"end\":44574,\"start\":44573},{\"end\":44582,\"start\":44581},{\"end\":44804,\"start\":44803},{\"end\":44813,\"start\":44812},{\"end\":44819,\"start\":44818},{\"end\":44827,\"start\":44826},{\"end\":45068,\"start\":45067},{\"end\":45077,\"start\":45076},{\"end\":45085,\"start\":45084},{\"end\":45093,\"start\":45092},{\"end\":45100,\"start\":45099},{\"end\":45302,\"start\":45301},{\"end\":45310,\"start\":45309},{\"end\":45317,\"start\":45316},{\"end\":45323,\"start\":45322},{\"end\":45331,\"start\":45330}]", "bib_author_last_name": "[{\"end\":35748,\"start\":35737},{\"end\":35765,\"start\":35754},{\"end\":35775,\"start\":35771},{\"end\":35963,\"start\":35960},{\"end\":35971,\"start\":35967},{\"end\":35979,\"start\":35975},{\"end\":35985,\"start\":35983},{\"end\":36182,\"start\":36178},{\"end\":36197,\"start\":36186},{\"end\":36320,\"start\":36316},{\"end\":36329,\"start\":36326},{\"end\":36337,\"start\":36333},{\"end\":36346,\"start\":36341},{\"end\":36431,\"start\":36425},{\"end\":36440,\"start\":36435},{\"end\":36451,\"start\":36444},{\"end\":36461,\"start\":36455},{\"end\":36604,\"start\":36600},{\"end\":36613,\"start\":36610},{\"end\":36621,\"start\":36617},{\"end\":36752,\"start\":36745},{\"end\":36761,\"start\":36756},{\"end\":36769,\"start\":36765},{\"end\":36784,\"start\":36775},{\"end\":36926,\"start\":36924},{\"end\":36933,\"start\":36930},{\"end\":36941,\"start\":36937},{\"end\":36947,\"start\":36945},{\"end\":37044,\"start\":37042},{\"end\":37057,\"start\":37050},{\"end\":37222,\"start\":37216},{\"end\":37233,\"start\":37226},{\"end\":37244,\"start\":37237},{\"end\":37252,\"start\":37248},{\"end\":37509,\"start\":37503},{\"end\":37519,\"start\":37513},{\"end\":37529,\"start\":37523},{\"end\":37538,\"start\":37533},{\"end\":37552,\"start\":37542},{\"end\":37563,\"start\":37556},{\"end\":37571,\"start\":37567},{\"end\":37738,\"start\":37734},{\"end\":37744,\"start\":37742},{\"end\":37754,\"start\":37750},{\"end\":37782,\"start\":37776},{\"end\":37788,\"start\":37786},{\"end\":37988,\"start\":37981},{\"end\":37997,\"start\":37992},{\"end\":38007,\"start\":38001},{\"end\":38076,\"start\":38073},{\"end\":38083,\"start\":38080},{\"end\":38096,\"start\":38087},{\"end\":38105,\"start\":38100},{\"end\":38111,\"start\":38109},{\"end\":38207,\"start\":38198},{\"end\":38220,\"start\":38211},{\"end\":38401,\"start\":38399},{\"end\":38410,\"start\":38405},{\"end\":38419,\"start\":38414},{\"end\":38426,\"start\":38423},{\"end\":38609,\"start\":38607},{\"end\":38618,\"start\":38613},{\"end\":38626,\"start\":38622},{\"end\":38879,\"start\":38876},{\"end\":38888,\"start\":38883},{\"end\":38896,\"start\":38892},{\"end\":38906,\"start\":38902},{\"end\":39029,\"start\":39026},{\"end\":39036,\"start\":39033},{\"end\":39044,\"start\":39040},{\"end\":39059,\"start\":39050},{\"end\":39281,\"start\":39278},{\"end\":39298,\"start\":39285},{\"end\":39312,\"start\":39304},{\"end\":39354,\"start\":39352},{\"end\":39361,\"start\":39358},{\"end\":39374,\"start\":39365},{\"end\":39513,\"start\":39511},{\"end\":39519,\"start\":39517},{\"end\":39529,\"start\":39525},{\"end\":39750,\"start\":39742},{\"end\":39758,\"start\":39754},{\"end\":39766,\"start\":39762},{\"end\":39855,\"start\":39850},{\"end\":39867,\"start\":39859},{\"end\":39875,\"start\":39871},{\"end\":39967,\"start\":39956},{\"end\":39985,\"start\":39973},{\"end\":40039,\"start\":40031},{\"end\":40119,\"start\":40111},{\"end\":40129,\"start\":40123},{\"end\":40366,\"start\":40362},{\"end\":40378,\"start\":40370},{\"end\":40391,\"start\":40382},{\"end\":40400,\"start\":40395},{\"end\":40624,\"start\":40621},{\"end\":40632,\"start\":40628},{\"end\":40640,\"start\":40636},{\"end\":40650,\"start\":40646},{\"end\":40867,\"start\":40858},{\"end\":40877,\"start\":40871},{\"end\":40886,\"start\":40881},{\"end\":40901,\"start\":40893},{\"end\":41038,\"start\":41029},{\"end\":41048,\"start\":41042},{\"end\":41057,\"start\":41052},{\"end\":41072,\"start\":41064},{\"end\":41225,\"start\":41214},{\"end\":41236,\"start\":41229},{\"end\":41244,\"start\":41240},{\"end\":41402,\"start\":41398},{\"end\":41412,\"start\":41406},{\"end\":41422,\"start\":41416},{\"end\":41435,\"start\":41426},{\"end\":41596,\"start\":41592},{\"end\":41606,\"start\":41600},{\"end\":41619,\"start\":41610},{\"end\":41629,\"start\":41623},{\"end\":41773,\"start\":41768},{\"end\":41785,\"start\":41779},{\"end\":41796,\"start\":41789},{\"end\":41926,\"start\":41923},{\"end\":41935,\"start\":41930},{\"end\":41945,\"start\":41941},{\"end\":42157,\"start\":42154},{\"end\":42166,\"start\":42161},{\"end\":42173,\"start\":42170},{\"end\":42180,\"start\":42177},{\"end\":42186,\"start\":42184},{\"end\":42198,\"start\":42193},{\"end\":42207,\"start\":42202},{\"end\":42468,\"start\":42461},{\"end\":42479,\"start\":42474},{\"end\":42673,\"start\":42666},{\"end\":42684,\"start\":42679},{\"end\":42870,\"start\":42863},{\"end\":42881,\"start\":42876},{\"end\":42973,\"start\":42970},{\"end\":42980,\"start\":42977},{\"end\":42988,\"start\":42984},{\"end\":42996,\"start\":42992},{\"end\":43003,\"start\":43000},{\"end\":43231,\"start\":43223},{\"end\":43245,\"start\":43235},{\"end\":43258,\"start\":43249},{\"end\":43269,\"start\":43262},{\"end\":43279,\"start\":43275},{\"end\":43405,\"start\":43399},{\"end\":43413,\"start\":43409},{\"end\":43560,\"start\":43556},{\"end\":43568,\"start\":43564},{\"end\":43735,\"start\":43731},{\"end\":43746,\"start\":43741},{\"end\":43758,\"start\":43752},{\"end\":43774,\"start\":43764},{\"end\":43830,\"start\":43826},{\"end\":43838,\"start\":43834},{\"end\":43845,\"start\":43842},{\"end\":43852,\"start\":43849},{\"end\":43860,\"start\":43856},{\"end\":43867,\"start\":43864},{\"end\":44002,\"start\":43999},{\"end\":44008,\"start\":44006},{\"end\":44219,\"start\":44217},{\"end\":44226,\"start\":44223},{\"end\":44383,\"start\":44379},{\"end\":44390,\"start\":44387},{\"end\":44399,\"start\":44394},{\"end\":44565,\"start\":44560},{\"end\":44571,\"start\":44569},{\"end\":44579,\"start\":44575},{\"end\":44587,\"start\":44583},{\"end\":44810,\"start\":44805},{\"end\":44816,\"start\":44814},{\"end\":44824,\"start\":44820},{\"end\":44832,\"start\":44828},{\"end\":45074,\"start\":45069},{\"end\":45082,\"start\":45078},{\"end\":45090,\"start\":45086},{\"end\":45097,\"start\":45094},{\"end\":45103,\"start\":45101},{\"end\":45307,\"start\":45303},{\"end\":45314,\"start\":45311},{\"end\":45320,\"start\":45318},{\"end\":45328,\"start\":45324},{\"end\":45335,\"start\":45332}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":697405},\"end\":35887,\"start\":35670},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52955811},\"end\":36124,\"start\":35889},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":749620},\"end\":36267,\"start\":36126},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1910869},\"end\":36360,\"start\":36269},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206764948},\"end\":36533,\"start\":36362},{\"attributes\":{\"id\":\"b5\"},\"end\":36677,\"start\":36535},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":16358367},\"end\":36861,\"start\":36679},{\"attributes\":{\"doi\":\"arXiv:1708.04943\",\"id\":\"b7\"},\"end\":36995,\"start\":36863},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13497941},\"end\":37145,\"start\":36997},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9749221},\"end\":37411,\"start\":37147},{\"attributes\":{\"doi\":\"arXiv:1808.01050\",\"id\":\"b10\"},\"end\":37625,\"start\":37413},{\"attributes\":{\"id\":\"b11\"},\"end\":37770,\"start\":37627},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b12\"},\"end\":37867,\"start\":37772},{\"attributes\":{\"doi\":\"arXiv:1703.09393\",\"id\":\"b13\"},\"end\":38043,\"start\":37869},{\"attributes\":{\"id\":\"b14\"},\"end\":38157,\"start\":38045},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":18018217},\"end\":38279,\"start\":38159},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5157313},\"end\":38510,\"start\":38281},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3645757},\"end\":38788,\"start\":38512},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5696978},\"end\":38920,\"start\":38790},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3740753},\"end\":39208,\"start\":38922},{\"attributes\":{\"doi\":\"arXiv:1803.03095\",\"id\":\"b20\"},\"end\":39348,\"start\":39210},{\"attributes\":{\"doi\":\"arXiv:1811.00472\",\"id\":\"b21\"},\"end\":39435,\"start\":39350},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":303636},\"end\":39678,\"start\":39437},{\"attributes\":{\"doi\":\"arXiv:1810.03272\",\"id\":\"b23\"},\"end\":39804,\"start\":39680},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":64200748},\"end\":39891,\"start\":39806},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":40499053},\"end\":40027,\"start\":39893},{\"attributes\":{\"id\":\"b26\"},\"end\":40054,\"start\":40029},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7612598},\"end\":40253,\"start\":40056},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1718465},\"end\":40527,\"start\":40255},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":49571269},\"end\":40799,\"start\":40529},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":215754735},\"end\":40970,\"start\":40801},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":215754735},\"end\":41145,\"start\":40972},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3719281},\"end\":41348,\"start\":41147},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14897024},\"end\":41515,\"start\":41350},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15166260},\"end\":41680,\"start\":41517},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":13164291},\"end\":41858,\"start\":41682},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1089358},\"end\":42094,\"start\":41860},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52243494},\"end\":42354,\"start\":42096},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3003101},\"end\":42586,\"start\":42356},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":2099022},\"end\":42764,\"start\":42588},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13709403},\"end\":42920,\"start\":42766},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3193713},\"end\":43145,\"start\":42922},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":10520310},\"end\":43358,\"start\":43147},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":26569197},\"end\":43469,\"start\":43360},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4157069},\"end\":43653,\"start\":43471},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":207761262},\"end\":43822,\"start\":43655},{\"attributes\":{\"doi\":\"arXiv:1808.06133\",\"id\":\"b46\"},\"end\":43959,\"start\":43824},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":6423078},\"end\":44135,\"start\":43961},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":224660},\"end\":44308,\"start\":44137},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":3772239},\"end\":44489,\"start\":44310},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":2131202},\"end\":44734,\"start\":44491},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":2131202},\"end\":44991,\"start\":44736},{\"attributes\":{\"id\":\"b52\"},\"end\":45268,\"start\":44993},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":5299559},\"end\":45405,\"start\":45270}]", "bib_title": "[{\"end\":35733,\"start\":35670},{\"end\":35956,\"start\":35889},{\"end\":36172,\"start\":36126},{\"end\":36312,\"start\":36269},{\"end\":36421,\"start\":36362},{\"end\":36596,\"start\":36535},{\"end\":36741,\"start\":36679},{\"end\":37038,\"start\":36997},{\"end\":37212,\"start\":37147},{\"end\":38066,\"start\":38045},{\"end\":38194,\"start\":38159},{\"end\":38395,\"start\":38281},{\"end\":38603,\"start\":38512},{\"end\":38872,\"start\":38790},{\"end\":39022,\"start\":38922},{\"end\":39507,\"start\":39437},{\"end\":39846,\"start\":39806},{\"end\":39952,\"start\":39893},{\"end\":40107,\"start\":40056},{\"end\":40355,\"start\":40255},{\"end\":40617,\"start\":40529},{\"end\":40854,\"start\":40801},{\"end\":41025,\"start\":40972},{\"end\":41210,\"start\":41147},{\"end\":41394,\"start\":41350},{\"end\":41588,\"start\":41517},{\"end\":41760,\"start\":41682},{\"end\":41917,\"start\":41860},{\"end\":42150,\"start\":42096},{\"end\":42455,\"start\":42356},{\"end\":42660,\"start\":42588},{\"end\":42857,\"start\":42766},{\"end\":42966,\"start\":42922},{\"end\":43219,\"start\":43147},{\"end\":43395,\"start\":43360},{\"end\":43552,\"start\":43471},{\"end\":43727,\"start\":43655},{\"end\":43995,\"start\":43961},{\"end\":44213,\"start\":44137},{\"end\":44375,\"start\":44310},{\"end\":44556,\"start\":44491},{\"end\":44801,\"start\":44736},{\"end\":45065,\"start\":44993},{\"end\":45299,\"start\":45270}]", "bib_author": "[{\"end\":35750,\"start\":35735},{\"end\":35767,\"start\":35750},{\"end\":35777,\"start\":35767},{\"end\":35965,\"start\":35958},{\"end\":35973,\"start\":35965},{\"end\":35981,\"start\":35973},{\"end\":35987,\"start\":35981},{\"end\":36184,\"start\":36174},{\"end\":36199,\"start\":36184},{\"end\":36322,\"start\":36314},{\"end\":36331,\"start\":36322},{\"end\":36339,\"start\":36331},{\"end\":36348,\"start\":36339},{\"end\":36433,\"start\":36423},{\"end\":36442,\"start\":36433},{\"end\":36453,\"start\":36442},{\"end\":36463,\"start\":36453},{\"end\":36606,\"start\":36598},{\"end\":36615,\"start\":36606},{\"end\":36623,\"start\":36615},{\"end\":36754,\"start\":36743},{\"end\":36763,\"start\":36754},{\"end\":36771,\"start\":36763},{\"end\":36786,\"start\":36771},{\"end\":36928,\"start\":36922},{\"end\":36935,\"start\":36928},{\"end\":36943,\"start\":36935},{\"end\":36949,\"start\":36943},{\"end\":37046,\"start\":37040},{\"end\":37059,\"start\":37046},{\"end\":37224,\"start\":37214},{\"end\":37235,\"start\":37224},{\"end\":37246,\"start\":37235},{\"end\":37254,\"start\":37246},{\"end\":37511,\"start\":37501},{\"end\":37521,\"start\":37511},{\"end\":37531,\"start\":37521},{\"end\":37540,\"start\":37531},{\"end\":37554,\"start\":37540},{\"end\":37565,\"start\":37554},{\"end\":37573,\"start\":37565},{\"end\":37740,\"start\":37732},{\"end\":37746,\"start\":37740},{\"end\":37756,\"start\":37746},{\"end\":37784,\"start\":37772},{\"end\":37790,\"start\":37784},{\"end\":37990,\"start\":37979},{\"end\":37999,\"start\":37990},{\"end\":38009,\"start\":37999},{\"end\":38078,\"start\":38068},{\"end\":38085,\"start\":38078},{\"end\":38098,\"start\":38085},{\"end\":38107,\"start\":38098},{\"end\":38113,\"start\":38107},{\"end\":38209,\"start\":38196},{\"end\":38222,\"start\":38209},{\"end\":38403,\"start\":38397},{\"end\":38412,\"start\":38403},{\"end\":38421,\"start\":38412},{\"end\":38428,\"start\":38421},{\"end\":38611,\"start\":38605},{\"end\":38620,\"start\":38611},{\"end\":38628,\"start\":38620},{\"end\":38881,\"start\":38874},{\"end\":38890,\"start\":38881},{\"end\":38898,\"start\":38890},{\"end\":38908,\"start\":38898},{\"end\":39031,\"start\":39024},{\"end\":39038,\"start\":39031},{\"end\":39046,\"start\":39038},{\"end\":39061,\"start\":39046},{\"end\":39283,\"start\":39276},{\"end\":39300,\"start\":39283},{\"end\":39314,\"start\":39300},{\"end\":39356,\"start\":39350},{\"end\":39363,\"start\":39356},{\"end\":39376,\"start\":39363},{\"end\":39515,\"start\":39509},{\"end\":39521,\"start\":39515},{\"end\":39531,\"start\":39521},{\"end\":39752,\"start\":39740},{\"end\":39760,\"start\":39752},{\"end\":39768,\"start\":39760},{\"end\":39857,\"start\":39848},{\"end\":39869,\"start\":39857},{\"end\":39877,\"start\":39869},{\"end\":39969,\"start\":39954},{\"end\":39987,\"start\":39969},{\"end\":40041,\"start\":40031},{\"end\":40121,\"start\":40109},{\"end\":40131,\"start\":40121},{\"end\":40368,\"start\":40357},{\"end\":40380,\"start\":40368},{\"end\":40393,\"start\":40380},{\"end\":40402,\"start\":40393},{\"end\":40626,\"start\":40619},{\"end\":40634,\"start\":40626},{\"end\":40642,\"start\":40634},{\"end\":40652,\"start\":40642},{\"end\":40869,\"start\":40856},{\"end\":40879,\"start\":40869},{\"end\":40888,\"start\":40879},{\"end\":40903,\"start\":40888},{\"end\":41040,\"start\":41027},{\"end\":41050,\"start\":41040},{\"end\":41059,\"start\":41050},{\"end\":41074,\"start\":41059},{\"end\":41227,\"start\":41212},{\"end\":41238,\"start\":41227},{\"end\":41246,\"start\":41238},{\"end\":41404,\"start\":41396},{\"end\":41414,\"start\":41404},{\"end\":41424,\"start\":41414},{\"end\":41437,\"start\":41424},{\"end\":41598,\"start\":41590},{\"end\":41608,\"start\":41598},{\"end\":41621,\"start\":41608},{\"end\":41631,\"start\":41621},{\"end\":41775,\"start\":41762},{\"end\":41787,\"start\":41775},{\"end\":41798,\"start\":41787},{\"end\":41928,\"start\":41919},{\"end\":41937,\"start\":41928},{\"end\":41947,\"start\":41937},{\"end\":42159,\"start\":42152},{\"end\":42168,\"start\":42159},{\"end\":42175,\"start\":42168},{\"end\":42182,\"start\":42175},{\"end\":42188,\"start\":42182},{\"end\":42200,\"start\":42188},{\"end\":42209,\"start\":42200},{\"end\":42470,\"start\":42457},{\"end\":42481,\"start\":42470},{\"end\":42675,\"start\":42662},{\"end\":42686,\"start\":42675},{\"end\":42872,\"start\":42859},{\"end\":42883,\"start\":42872},{\"end\":42975,\"start\":42968},{\"end\":42982,\"start\":42975},{\"end\":42990,\"start\":42982},{\"end\":42998,\"start\":42990},{\"end\":43005,\"start\":42998},{\"end\":43233,\"start\":43221},{\"end\":43247,\"start\":43233},{\"end\":43260,\"start\":43247},{\"end\":43271,\"start\":43260},{\"end\":43281,\"start\":43271},{\"end\":43407,\"start\":43397},{\"end\":43415,\"start\":43407},{\"end\":43562,\"start\":43554},{\"end\":43570,\"start\":43562},{\"end\":43737,\"start\":43729},{\"end\":43748,\"start\":43737},{\"end\":43760,\"start\":43748},{\"end\":43776,\"start\":43760},{\"end\":43832,\"start\":43824},{\"end\":43840,\"start\":43832},{\"end\":43847,\"start\":43840},{\"end\":43854,\"start\":43847},{\"end\":43862,\"start\":43854},{\"end\":43869,\"start\":43862},{\"end\":44004,\"start\":43997},{\"end\":44010,\"start\":44004},{\"end\":44221,\"start\":44215},{\"end\":44228,\"start\":44221},{\"end\":44385,\"start\":44377},{\"end\":44392,\"start\":44385},{\"end\":44401,\"start\":44392},{\"end\":44567,\"start\":44558},{\"end\":44573,\"start\":44567},{\"end\":44581,\"start\":44573},{\"end\":44589,\"start\":44581},{\"end\":44812,\"start\":44803},{\"end\":44818,\"start\":44812},{\"end\":44826,\"start\":44818},{\"end\":44834,\"start\":44826},{\"end\":45076,\"start\":45067},{\"end\":45084,\"start\":45076},{\"end\":45092,\"start\":45084},{\"end\":45099,\"start\":45092},{\"end\":45105,\"start\":45099},{\"end\":45309,\"start\":45301},{\"end\":45316,\"start\":45309},{\"end\":45322,\"start\":45316},{\"end\":45330,\"start\":45322},{\"end\":45337,\"start\":45330}]", "bib_venue": "[{\"end\":35829,\"start\":35777},{\"end\":36051,\"start\":35987},{\"end\":36236,\"start\":36199},{\"end\":36352,\"start\":36348},{\"end\":36525,\"start\":36463},{\"end\":36661,\"start\":36623},{\"end\":36851,\"start\":36786},{\"end\":36920,\"start\":36863},{\"end\":37098,\"start\":37059},{\"end\":37331,\"start\":37254},{\"end\":37499,\"start\":37413},{\"end\":37730,\"start\":37627},{\"end\":37847,\"start\":37805},{\"end\":37977,\"start\":37869},{\"end\":38151,\"start\":38113},{\"end\":38271,\"start\":38222},{\"end\":38498,\"start\":38428},{\"end\":38705,\"start\":38628},{\"end\":38912,\"start\":38908},{\"end\":39138,\"start\":39061},{\"end\":39274,\"start\":39210},{\"end\":39415,\"start\":39392},{\"end\":39608,\"start\":39531},{\"end\":39738,\"start\":39680},{\"end\":39884,\"start\":39877},{\"end\":40025,\"start\":39987},{\"end\":40201,\"start\":40131},{\"end\":40469,\"start\":40402},{\"end\":40729,\"start\":40652},{\"end\":40964,\"start\":40903},{\"end\":41135,\"start\":41074},{\"end\":41332,\"start\":41246},{\"end\":41505,\"start\":41437},{\"end\":41670,\"start\":41631},{\"end\":41849,\"start\":41798},{\"end\":42024,\"start\":41947},{\"end\":42286,\"start\":42209},{\"end\":42576,\"start\":42481},{\"end\":42746,\"start\":42686},{\"end\":42910,\"start\":42883},{\"end\":43072,\"start\":43005},{\"end\":43352,\"start\":43281},{\"end\":43453,\"start\":43415},{\"end\":43641,\"start\":43570},{\"end\":43813,\"start\":43776},{\"end\":43937,\"start\":43885},{\"end\":44077,\"start\":44010},{\"end\":44298,\"start\":44228},{\"end\":44483,\"start\":44401},{\"end\":44666,\"start\":44589},{\"end\":44911,\"start\":44834},{\"end\":45182,\"start\":45105},{\"end\":45397,\"start\":45337},{\"end\":35868,\"start\":35831},{\"end\":36102,\"start\":36053},{\"end\":37395,\"start\":37333},{\"end\":38769,\"start\":38707},{\"end\":39202,\"start\":39140},{\"end\":39672,\"start\":39610},{\"end\":40523,\"start\":40471},{\"end\":40793,\"start\":40731},{\"end\":42088,\"start\":42026},{\"end\":42350,\"start\":42288},{\"end\":43139,\"start\":43074},{\"end\":44131,\"start\":44079},{\"end\":44730,\"start\":44668},{\"end\":44975,\"start\":44913},{\"end\":45246,\"start\":45184}]"}}}, "year": 2023, "month": 12, "day": 17}