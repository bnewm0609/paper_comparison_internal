{"id": 243848115, "updated": "2023-10-05 19:52:13.86", "metadata": {"title": "Deep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis", "authors": "[{\"first\":\"Tianchang\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Kangxue\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Ming-Yu\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Sanja\",\"last\":\"Fidler\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "6087-6101", "publication_date": {"year": 2021, "month": 11, "day": 8}, "abstract": "We introduce DMTet, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTet directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTet includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2111.04276", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ShenGYLF21", "doi": null}}, "content": {"source": {"pdf_hash": "8e970913466a81207230a09f6516bab944563cc0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2111.04276v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "64f1f45bbbd6abae85dcebfdc6eb1c356770b262", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8e970913466a81207230a09f6516bab944563cc0.txt", "contents": "\nDeep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis\n\n\nTianchang Shen frshen@nvidia.com \nUniversity of Toronto\n\n\nVector Institute\n\n\nJun Gao jung@nvidia.com \nUniversity of Toronto\n\n\nVector Institute\n\n\nKangxue Yin kangxuey@nvidia.com \nUniversity of Toronto\n\n\nMing-Yu Liu mingyul@nvidia.com \nUniversity of Toronto\n\n\nSanja Fidler sfidler@nvidia.com \nUniversity of Toronto\n\n\nVector Institute\n\n\n\nNVIDIA\n\n\nDeep Marching Tetrahedra: a Hybrid Representation for High-Resolution 3D Shape Synthesis\n\nWe introduce DMTET, a deep 3D conditional generative model that can synthesize high-resolution 3D shapes using simple user guides such as coarse voxels. It marries the merits of implicit and explicit 3D representations by leveraging a novel hybrid 3D representation. Compared to the current implicit approaches, which are trained to regress the signed distance values, DMTET directly optimizes for the reconstructed surface, which enables us to synthesize finer geometric details with fewer artifacts. Unlike deep 3D generative models that directly generate explicit representations such as meshes, our model can synthesize shapes with arbitrary topology. The core of DMTET includes a deformable tetrahedral grid that encodes a discretized signed distance function and a differentiable marching tetrahedra layer that converts the implicit signed distance representation to the explicit surface mesh representation. This combination allows joint optimization of the surface geometry and topology as well as generation of the hierarchy of subdivisions using reconstruction and adversarial losses defined explicitly on the surface mesh. Our approach significantly outperforms existing work on conditional shape synthesis from coarse voxel inputs, trained on a dataset of complex 3D animal shapes. Project page: https://nv-tlabs.github.io/DMTet/.Recently, neural implicit representations[8,39,42,51], which use a neural network to implicitly represent a shape via a signed distance field (SDF) or an occupancy field (OF), have emerged as an effective 3D representation. Neural implicits have the benefit of representing complex geometry 35th Conference on Neural Information Processing Systems (NeurIPS 2021). arXiv:2111.04276v1 [cs.CV] 8 Nov 2021and topology, not limited to a predefined resolution. The success of these methods has been shown in shape compression[13,49,51], single-image shape generation[47,48,60], and point cloud reconstruction[57]. However, most of the current implicit approaches are trained by regressing to SDF or OF values and cannot utilize an explicit supervision on the target surface, which imposes useful constraints for training. To mitigate this issue, several works[31,45]proposed to utilize iso-surfacing techniques such as the Marching Cubes (MC) algorithm to extract a surface mesh from the implicit representation, which, however, is computationally expensive.In this work, we introduce DMTET, a deep 3D conditional generative model for high-resolution 3D shape synthesis from user guides in the form of coarse voxels. In the heart of DMTET is a new differentiable shape representation that marries implicit and explicit 3D representations. In contrast to deep implicit approaches optimized for predicting sign distance (or occupancy) values, our model employs additional supervision on the surface, which empirically renders higher quality shapes with finer geometric details. Compared to methods that learn to directly generate explicit representations, such as meshes[54], by committing to a preset topology, our DMTET can produce shapes with arbitrary topology. Specifically, DMTET predicts the underlying surface parameterized by an implicit function encoded via a deformable tetrahedral grid. The underlying surface is converted into an explicit mesh with a Marching Tetrahedra (MT) algorithm, which we show is differentiable and more performant than the Marching Cubes. DMTET maintains efficiency by learning to adapt the grid resolution by deforming and selectively subdividing tetrahedra. This has the effect of spending computation only on the relevant regions in space. We achieve further gains in the overall quality of the output shape with learned surface subdivision. Our DMTET is end-to-end differentiable, allowing the network to jointly optimize the geometry and topology of the surface, as well as the hierarchy of subdivisions using a loss function defined explicitly on the surface mesh.We demonstrate our DMTET on two challenging tasks: 3D shape synthesis from coarse voxel inputs and point cloud 3D reconstruction. We outperform existing state-of-the-art methods by a significant margin while being 10 times faster than alternative implicit representation-based methods at inference time. In summary, we make the following technical contributions:\n\nIntroduction\n\nFields such as simulation, architecture, gaming, and film rely on high-quality 3D content with rich geometric details and complex topology. However, creating such content requires tremendous expert human effort. It takes a significant amount of development time to create each individual 3D asset. In contrast, creating rough 3D shapes with simple building blocks like voxels has been widely adopted. For example, Minecraft has been used by hundreds of millions of users for creating 3D content. Most of them are non-experts. Developing A.I. tools that enable regular people to upscale coarse, voxelized objects into high resolution, beautiful 3D shapes would bring us one step closer to democratizing high-quality 3D content creation. Similar tools can be envisioned for turning 3D scans of objects recorded by modern phones into high-quality forms. Our work aspires to create such capabilities.\n\nA powerful 3D representation is a critical component of a learning-based 3D content creation framework. A good 3D representation for high-quality reconstruction and synthesis should capture local geometric details and represent objects with arbitrary topology while also being memory and computationally efficient for fast inference in interactive applications. 1. We show that using Marching Tetrahedra (MT) as a differentiable iso-surfacing layer allows topological change for the underlying shape represented by a implicit field, in contrast to the analysis in prior works [31,45]. 2. We incorporate MT in a DL framework and introduce DMTET, a hybrid representation that combines implicit and explicit surface representations. We demonstrate that the additional supervision (e.g. chamfer distance, adversarial loss) defined directly on the extracted surface from implicit field improves the shape synthesis quality. 3. We introduce a coarse-to-fine optimization strategy that scales DMTET to high resolution during training. We thus achieves better reconstruction quality than state-of-the-art methods on challenging 3D shape synthesis tasks, while requiring a lower computation cost.\n\n\nRelated Work\n\nWe review the related work on learning-based 3D synthesis methods based on their 3D representations.\n\nVoxel-based Methods Early work [10,38,59] represented 3D shapes as voxels, which store the coarse occupancy (inside/outside) values on a regular grid, which makes powerful convolutional neural networks native and renders impressive results on 3D reconstruction and synthesis [2,11,12,58]. For high-resolution shape synthesis, DECOR-GAN [6] transfers geometric details from a high-resolution shape represented in voxel to a low-resolution shape by utilizing a discriminator defined on 3D patches of the voxel grid. However, the computational and memory costs grow cubically as the resolution increases, prohibiting the reconstruction of fine geometric details and smooth curves. One common way to address this limitation is building hierarchical structures such as octrees [24,46,52,52,55,56], which adapt the grid resolution locally based on the underlying shape. In this paper, we adopt a hierarchical deformable tetrahedral grid to utilize the resolution better. Unlike octree-based shape synthesis, our network learns grid deformation and subdivision jointly to better represent the surface without relying on explicit supervision from a pre-computed hierarchy.\n\nDeep Implicit Fields (DIFs) represent a 3D shape as a zero level set of a continuous function parameterized by a neural network [17,39,40,44]. This formulation can represent arbitrary typology and has infinite resolution. DIF-based shape synthesis approaches have demonstrated strong  Figure 1: DMTET reconstructs the shape implicitly in a coarse-to-fine manner by predicting the SDF defined on a deformable tetrahedral grid. It then converts the SDF to a surface mesh by a differentiable Marching Tetrahedra layer. DMTET is trained by optimizing the objective function defined on the final surface.\n\nperformance in many applications, including single view 3D reconstruction [30,47,48,60], shape manipulation, and synthesis [1,9,14,21,26,28]. However, as these approaches are trained by minimizing the reconstruction loss of function values at a set of sampled 3D locations (a rough proxy of the surface), they tend to render artifacts when synthesizing fine details. Furthermore, if one desires a mesh to be extracted from a DIF, an expensive iso-surfacing step based on Marching Cubes [36] or Marching Tetrahedra [15] is required. Due to the computational burden, iso-surfacing is often done on a smaller resolution, hence prone to quantization errors. Lei et al. [29] proposes an analytic meshing solution to reduce the error, but is only applicable to DIFs parametrized by MLPs with ReLU activation. Our representation scales to high resolution and does not require additional modification to the backward pass for training end-to-end. DMTET can represent arbitrary typology, and is trained via direct supervision on the generated surface. Recent works [1,9] learn to regress unsigned distance to triangle soup or point cloud. However, their iso-surfacing formulation is not differentiable in contrast to DMTET.\n\nSurface-based Methods directly predict triangular meshes and have achieved impressive results for reconstructing and synthesizing simpler shapes [5,7,23,41,54]. Typically, they predefined the topology of the shape, e.g. equivalent to a sphere [5,25,54], or a union of primitives [19,43,53] or a set of segmented parts [50,61,62]. As a result, they can not model a distribution of shapes with complex topology variations. Recently, DefTet [18] represents a mesh with a deformable tetrahedral grid where the grid vertex coordinates and the occupancy values are learned. However, similar to voxel-based methods, the computational costf increases cubically with the grid resolution. Furthermore, as the occupancy loss for supervising topology learning and the surface loss for supervising geometry learning do not support joint training, it tends to generate suboptimal results. In contrast, our method is able to synthesize high-resolution 3D shapes, not shown in previous work.\n\n\nDeep Marching Tetrahedra\n\nWe now introduce our DMTET for synthesizing high-quality 3D objects. The schematic illustration is provided in Fig. 1. Our model relies on a new, hybrid 3D representation specifically designed for high-resolution reconstruction and synthesis, which we describe in Sec. 3.1. In Sec. 3.2, we describe the neural network architecture of DMTET that predicts the shape representation from inputs such as coarse voxels. We provide the training objectives in Sec. 3.3.\n\n\n3D Representation\n\nWe represent a shape using a sign distance field (SDF) encoded with a deformable tetrahedral grid, adopted from DefTet [18,20]. The grid fully tetrahedralizes a unit cube, where each cell in the volume is a tetahedron with 4 vertices and faces. The key aspect of this representation is that the grid vertices can deform to represent the geometry of the shape more efficiently. While the original DefTet encoded occupancy defined on each tetrahedron, we here encode signed distance values defined on the vertices of the grid and represent the underlying surface implicitly (Sec. 3.1.1). The use of signed distance values, instead of occupancy values, provides more flexibility in representing the underlying surface. For greater representation power while keeping memory and computation manageable, we further selectively subdivide the tetrahedra around the predicted surface (Sec. 3.1.2). We convert the signed distance-based implicit representation into a triangular mesh using a marching tetrahedra layer, which we discuss in Sec. 3.1.3. The final mesh is further converted into a parameterized surface with a differentiable surface subdivision module, described in Sec. 3.1.4.\n\n\nDeformable Tetrahedral Mesh as an Approximation of an Implicit Function\n\nWe adopt and extend the deformable tetrahedral grid introduced in Gao et al. [18], which we denote with (V T , T ), where V T are the vertices in the tetrahedral grid T . Following the notation in [18],\neach tetrahedron T k \u2208 T is represented with four vertices {v a k , v b k , v c k , v d k }, with k \u2208 {1, . . . ., K},\nwhere K is the total number of tetrahedra and v i k \u2208 V T .\n\nWe represent the sign distance field by interpolating SDF values defined on the vertices of the grid. Specifically, we denote the SDF value in vertex v i \u2208 V T as s(v i ). SDF values for the points that lie inside the tetrahedron follow a barycentric interpolation of the SDF values of the four vertices that encapsulates the point.  We represent shape in a coarse to fine manner for efficiency. We determine the surface tetrahedra T surf by checking whether a tetrahedron has vertices with different SDF signs -indicating that it intersects the surface encoded by the SDF. We subdivide T surf as well as their immediate neighbors and increase resolution by adding the mid point to each edge. We compute SDF values of the new vertices by averaging the SDF values on the edge (Fig. 2). We use the Marching Tetrahedra [15] algorithm to convert the encoded SDF into an explicit triangular mesh. Given the SDF values\n\n\nVolume Subdivision\n= \" > A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V T F t o Q 9 l s p + 3 S z S b s b g o l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S K 4 N q 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J U 8 e p Y u i z W M S q H V K N g k v 0 D T c C 2 4 l C G o U C W + H 4 f u 6 3 J q g 0 j + W T m S Y Y R H Q o + Y A z a q z k T 3 p Z O O u V K 2 7 V X Y C s E y 8 n F c j R 6 J W / u v 2 Y p R F K w w T V u u O 5 i Q k y q g x n A m e l b q o x o W x M h 9 i x V N I I d Z A t j p 2 R C 6 v 0 y S B W t q Q h C / X 3 R E Y j r a d R a D s j a k Z 6 1 Z u L / 3 m d 1 A x u g 4 z L J D U o 2 X L R I B X E x G T + O e l z h c y I q S W U K W 5 v J W x E F W X G 5 l O y I X i r L 6 + T Z q 3 q X V V r j 9 e V + l 0 e R x H O 4 B w u w Y M b q M M D N M A H B h y e 4 R X e H O m 8 O O / O x 7 K 1 4 O Q z p / A H z u c P G Q i O 3 w = = < / l a t e x i t > v c < l a t e x i t s h a 1 _ b a s e 6 4 = \" U F Y p c o f B c C x q S t h G S y P g t s z 6 m v U = \" > A A A B 7 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e i F 4 8 V T F t o Q 9 l s p + 3 S z S b s b g o l 9 D d 4 8 a C I V 3 + Q N / + N 2 z Y H b X 0 w 8 H h v h p l 5 Y S K 4 N q 7 7 7 R Q 2 N r e 2 d 4 q 7 p b 3 9 g 8 O j 8 v F J U 8 e p Y u i z W M S q H V K N g k v 0 D T c C 2 4 l C G o U C W + H 4 f u 6 3 J q g 0 j + W T m S Y Y R H Q o + Y A z a q z k T 3 o Z m / X K F b f q L k D W i Z e T C u R o 9 M p f 3 X 7 M 0 g i l Y Y J q 3 f H c x A Q Z V Y Y z g b N S N 9 W Y U D a m Q + x Y K m m E O s g W x 8 7 I h V X 6 Z B A r W 9 K Q h f p 7 I q O R 1 t M o t J 0 R N S O 9 6 s 3 F / 7 x O a g a 3 Q c Z l k h q U b L l o k A p i Y j L / n P S 5 Q m b E 1 B L K F L e 3 E j a i i j J j 8 y n Z E L z V l 9 d J s 1 b 1 r q q 1 x + t K / S 6 P o w h n c A 6 X 4 M E N 1 O E B G u A D A w 7 P 8 A p v j n R e n H f n Y 9 l a c P K Z U / g D 5 / M H G o 2 O 4 A = = < / l a t e x i t > v d < l a t e x i t s h a 1 _ b a s e 6 4 = \" t U e j N 4 5 e E h T 5 Q s g Q V w s Q K h y C K e A = \" > A A A B 7 H i c b V B N S 8 N A E J 2 t X 7 V + V T 1 6 W S y C p 5 J U Q Y 9 F L x 4 r m L b Q h r L Z b N q l m 0 3 Y 3 R R K 6 G / w 4 k E R r / 4 g b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g l R w b R z n G 5 U 2 N r e 2 d 8 q 7 l b 3 9 g 8 O j 6 v F J W y e Z o s y j i U h U N y C a C S 6 Z Z 7 g R r J s q R u J A s E 4 w v p / 7 n Q l T m i f y y U x T 5 s d k K H n E K T F W 8 i a D P J w N q j W n 7 i y A 1 4 l b k B o U a A 2 q X / 0 w o V n M p K G C a N 1 z n d T 4 O V G G U 8 F m l X 6 m W U r o m A x Z z 1 J J Y q b 9 f H H s D F 9 Y J c R R o m x J g x f q 7 4 m c x F p P 4 8 B 2 x s S M 9 K o 3 F / / z e p m J b v 2 c y z Q z T N L l o i g T 2 C R 4 / j k O u W L U i K k l h C p u b 8 V 0 R B S h x u Z T s S G 4 q y + v k 3 a j 7 l 7 V G 4 / X t e Z d E U c Z z u A c L s G F G 2 j C A 7 T A A w o c n u E V 3 p B E L + g d f S x b S 6 i Y O Y U / Q J 8 / H B K O 4 Q = = < / l a t e x i t > v 0 ac = 1 2 (v a + v c ) < l a t e x i t s h a 1 _ b a s e 6 4 = \" u n k X g z a s v z o z 6 2 N V k J I 0 w x / Y t L Y = \" > A A A C B n i c b V D L S s N A F J 3 U V 6 2 v q E s R B o t Y E U p S B d 0 I R T c u K 9 g H t C F M p p N 2 6 G Q S Z i a B E r J y 4 6 + 4 c a G I W 7 / B n X / j t M 1 C W w 9 c O J x z L / f e 4 0 W M S m V Z 3 0 Z h a X l l d a 2 4 X t r Y 3 N r e M X f 3 W j K M B S Z N H L J Q d D w k C a O c N B V V j H Q i Q V D g M d L 2 R r c T v 5 0 Q I W n I H 9 Q 4 I k 6 A B p z 6 F C O l J d c 8 T E 7 c F O H s u q d 8 g X B q Z 2 k t q y Q u O k t c f O q a Z a t q T Q E X i Z 2 T M s j R c M 2 v X j / E c U C 4 w g x J 2 b W t S D k p E o p i R r J S L 5 Y k Q n i E B q S r K U c B k U 4 6 f S O D x 1 r p Q z 8 U u r i C U / X 3 R I o C K c e B p z s D p I Z y 3 p u I / 3 n d W P l X T k p 5 F C v C 8 W y R H z O o Q j j J B P a p I F i x s S Y I C 6 p v h X i I d B x K J 1 f S I d j z L y + S V q 1 q n 1 d r 9 x f l + k 0 e R x E c g C N Q A T a 4 B H V w B x q g C T B 4 B M / g F b w Z T 8 a L 8 W 5 8 z F o L R j 6 z D / 7 A + P w B a P O Y b w = = < / l a t e x i t > s 0 ac = 1 2 s(v a ) + s(v c ) < l a t e x i t s h a 1 _ b a s e 6 4 = \" 6 B B T r y L 3 Z 6 N 2 J y 2 R F / E A C T A h 4 J w = \" > A A A C F H i c b V D L S s N A F J 3 4 r P U V d e l m s I g t h Z J U Q T d C 0 Y 3 L C v Y B T Q i T 6 a Q d O p m E m U m h h H y E G 3 / F j Q t F 3 L p w 5 9 8 4 a b v Q 1 g O X e z j n X m b u 8 W N G p b K s b 2 N l d W 1 9 Y 7 O w V d z e 2 d 3 b N w 8 O 2 z J K B C Y t H L F I d H 0 k C a O c t B R V j H R j Q V D o M 9 L x R 7 e 5 3 x k T I W n E H 9 Q k J m 6 I B p w G F C O l J c + s y j M v R T i 7 d l Q g E E 7 t L K 1 n j k 8 H Z V k e e 6 h S z R u u 5 E r F M 0 t W z Z o C L h N 7 T k p g j q Z n f j n 9 C C c h 4 Q o z J G X P t m L l p k g o i h n J i k 4 i S Y z w C A 1 I T 1 O O Q i L d d H p U B k + 1 0 o d B J H R x B a f q 7 4 0 U h V J O Q l 9 P h k g N 5 a K X i / 9 5 v U Q F V 2 5 K e Z w o w v H s o S B h U E U w T w j 2 q S B Y s Y k m C A u q / w r x E O l w l M 6 x q E O w F 0 9 e J u 1 6 z T 6 v 1 e 8 v S o 2 b e R w F c A x O Q B n Y 4 B I 0 w B 1 o g h b A 4 B E 8 g 1 f w Z j w Z L 8 a 7 8 T E b X T H m O\n\nMarching Tetrahedra for converting between an Implicit and Explicit Representation\n{s(v a ), s(v b ), s(v c ), s(v d )}\nof the vertices of a tetrahedron, MT determines the surface typology inside the tetrahedron based on the signs of s(v), which is illustrated in Fig. 3. The total number of configurations is 2 4 = 16, which falls into 3 unique cases after considering rotation symmetry. Once the surface typology inside the tetrahedron is identified, the vertex location of the iso-surface is computed at the zero crossings of the linear interpolation along the tetrahedron's edges, as shown in Fig. 3.\n\nPrior works [31,45] argue that the singularity in this formulation, i.e. when s(v a ) = s(v b ), prevents the change of surface typology (sign change of s(v a )) during training. However, we find that, in practise, the equation is only evaluated when sign(s(v a )) = sign(s(v b )). Thus, during training, the singularity never happens and the gradient from a loss defined on the extracted iso-surface (Sec. 3.3), can be back-propagated to both vertex positions and SDF values via the chain rule. A more detailed analysis is in the Appendix.\n\n\nSurface Subdivision\n\nHaving a surface mesh as output allows us to further increase the representation power and the visual quality of the shapes with a differentiable surface subdivision module. We follow the scheme of the Loop Subdivision method [35], but instead of using a fixed set of parameters for subdivision, we make these parameters learnable in DMTET. Specifically, learnable parameters include the positions of each mesh vertex v i , as well as \u03b1 i which controls the generated surface via weighting the smoothness of neighbouring vertices. Note that different from Liu et al. [33], we only predict the per-vertex parameter at the beginning and carry it over to subsequent subdivision iterations to attain a lower computational cost. We provide more details in Appendix.\n\n\nDMTET: 3D Deep Conditional Generative Model\n\nOur DMTET is a neural network that utilizes our proposed 3D representation and aims to output a high resolution 3D mesh M from input x (a point cloud or a coarse voxelized shape). We describe the architecture (Fig. 4) \n\n\n3D Generator Input Encoder\n\nWe use PVCNN [34] as an input encoder to extract a 3D feature volume F vol (x) from a point cloud. When the input is a coarse voxelized shape, we sample points on its surface. We compute a feature vector F vol (v, x) for a grid vertex v \u2208 R 3 via trilinear interpolation.\n\nInitial Prediction of SDF We predict the SDF value for each vertex in the initial deformable tetrahedral grid using a fully-connected network s(v) = MLP (F vol (v, x), v). The fully-connected network additionally outputs a feature vector f (v), which is used for the surface refinement in the volume subdivision stage.\n\nSurface Refinement with Volume Subdivision After obtaining the initial SDF, we iteratively refine the surface and subdivide the tetrahedral grid. We first identify surface tetrahedra T surf based on the current s(v) value. We then build a graph G = (V surf , E surf ), where V surf , E surf correspond to the vertices and edges in T surf . We then predict the position offsets \u2206v i and SDF residual values \u2206s(v i ) for each vertex i in V surf using a Graph Convolutional Network [32] (GCN):\nf vi = concat(v i , s(v i ), F vol (v i , x), f (v i )),(1)(\u2206v i , \u2206s(v i ), f (v i )) i=1,\u00b7\u00b7\u00b7N surf = GCN (f vi ) i=1,\u00b7\u00b7\u00b7N surf , G ,(2)\nwhere N surf is the total number of vertices in V surf and f (v i ) is the updated per-vertex feature. The vertex position and the SDF value for vertex v i are updated as v i = v i + \u2206v i and s(v i ) = s(v i ) + \u2206s(v i ). This refinement step can potentially flip the sign of the SDF values to refine the local typology, and also move the vertices thus improving the local geometry.\n\nAfter surface refinement, we perform the volume subdivision step followed by an additional surface refinement step. In particular, we re-identify T surf and subdivide T surf and their immediate neighbors. We drop the unsubdivided tetrahedra from the full tetrahedral grid in both steps, which saves memory and computation, as the size of the T surf is proportional to the surface area of the object, and scales up quadratically rather than cubically as the grid resolution increases.\n\nNote that the SDF values and positions of the vertices are inherited from the level before subdivision, thus, the loss computed at the final surface can back-propagate to all vertices from all levels. Therefore, our DMTET automatically learns to subdivide the tetrahedra and does not need an additional loss term in the intermediate steps to supervise the learning of the octree hierarchy as in the prior work [52].\n\nLearnable Surface Subdivision After extracting the surface mesh using MT, we can further apply learnable surface subdivision. Specifically, we build a new graph on the extracted mesh, and use GCN to predict the updated position of each vertex v i , and \u03b1 i for Loop Subvidision. This step removes the quantization errors and mitigates the approximation errors from the classic Loop Subdivision by adjusting \u03b1 i , which are fixed in the classic method.\n\n\n3D Discriminator\n\nWe apply a 3D discriminator D on the final surface predicted from the generator. We empirically find that using a 3D CNN from DECOR-GAN [6] as the discriminator on the signed distance field that is computed from the predicted mesh is effective to capture the local details. Specifically, we first randomly select a high-curvature vertex v from the target mesh and compute the ground truth signed distance field S real \u2208 R N \u00d7N \u00d7N at a voxelized region around v. Similarly, we compute the signed distance field of the predicted surface mesh M at the same location to obtain S pred \u2208 R N \u00d7N \u00d7N . Note that S pred is an analytical function of the mesh M , and thus the gradient to S pred can backpropagate to the vertex positions in M . We feed S real or S pred into the discriminator, along with the feature vector F vol (v, x) in position v. The discriminator then predicts the probability indicating whether the input comes from the real or generated shapes.\n\n\nLoss Function\n\nDMTET is end-to-end trainable. We supervise all modules to minimize the error defined on the final predicted mesh M . Our loss function contains three different terms: a surface alignment loss to encourage the alignment with ground truth surface, an adversarial loss to improve realism of the generated shape, and regularizations to regularize the behavior of SDF and vertex deformations.\n\nSurface Alignment loss We sample a set of points P gt from the surface of the ground truth mesh M gt . Similarly, we also sample a set of points from M pred to obtain P pred , and minimize the L2 Chamfer Distance and the normal consistency loss between P gt and P pred :\nL cd = p\u2208P pred min q\u2208Pgt ||p \u2212 q|| 2 + q\u2208Pgt min p\u2208P pred ||q \u2212 p|| 2 , L normal = p\u2208P pred (1 \u2212 | n p \u00b7 nq|),(3)\nwhereq is the point that corresponds to p when computing the Chamfer Distance, and n p , nq denotes the normal direction at point p,q.\n\nAdversarial Loss We use the adversarial loss proposed in LSGAN [37]:\nL D = 1 2 [(D(M gt ) \u2212 1) 2 + D(M pred ) 2 ], L G = 1 2 [(D(M pred ) \u2212 1) 2 ].(4)\nRegularizations \nL SDF = vi\u2208V T |s(v i ) \u2212 SDF (v i , M gt )| 2 ,(5)\nwhere SDF (v i , M gt ) denotes the SDF value of point v i to the mesh M gt . In addition, we apply the L 2 regularization loss on the predicted vertex deformations to avoid artifacts:\nL def = vi\u2208V T ||\u2206v i || 2 .\nThe final loss is a weighted sum of all five loss terms:\nL = \u03bb cd L cd + \u03bb normal L normal + \u03bb G L G + \u03bb SDF L SDF + \u03bb def L def ,(6)\nwhere \u03bb cd , \u03bb normal , \u03bb G , \u03bb SDF , \u03bb def are hyperparameters (provided in the Supplement).\n\n\nExperiments\n\nWe first evaluate DMTET in the challenging application of generating high-quality animal shapes from coarse voxels. We further evaluate DMTET in reconstructing 3D shapes from noisy point clouds on ShapeNet by comparing to existing state-of-the-art methods.\n\n\n3D Shape Synthesis from Coarse Voxels Experimental Settings\n\nWe collected 1562 animal models from the TurboSquid website 1 . These models have a wide range of diversity, ranging from cats, dogs, bears, giraffes, to rhinoceros, goats, etc. We provide visualizations in Supplement. Among 1562 shapes, we randomly select 1120 shapes for training, and the remaining 442 shapes for testing. We follow the pipeline in Kaolin [27] to convert shapes to watertight meshes. To prepare the input to the network, we first voxelize the mesh into the resolution of 16 3 , and then sample 3000 points from the surface after applying marching cubes to the 16 3 voxel grid. Note that this preprocessing is agnostic to the representation of the input coarse shape, allowing us to evaluate on different resolution voxels, or even meshes.\n\nWe compare our model with the official implementation of ConvOnet [44], which achieved SOTA performance on voxel upsampling. We also compare to DECOR-GAN [6], which obtained impressive results on transferring styles from a high-resolution voxel shape to a low-resolution voxel. Note that the original setting of DECOR-GAN is different from ours. For a fair comparison, we use all 1120 training shapes as the high-resolution style shapes during training, and retrieve the closet training shape to the test shape as the style shape during inference, which we refer as DECOR-Retv. We also compare against a randomly selected style shape as reference, denoted as DECOR-Rand.  our method reconstructs shapes with much higher quality. Adding GAN further improves the realism of the generated shape. We also show the retrieved shapes from the training set in the second last column.\n\nMetrics We evaluate L2 and L1 Chamfer Distance, as well as normal consistency score to assess how well the methods reconstruct the corresponding high-resolution shape following [44]. We also report Light Field Distance [4] (LFD) which measures the visual similarity in 2D rendered views. In addition, we evaluate Cls score following [6]. Specifically, we render the predicted 3D shapes and train a patch-based image classifier to distinguish whether images are from the renderings of real or generated shapes. The mean classification accuracy of the trained classifier is reported as Cls (lower is better). More details are in the Supplement. Experimental Results We provide quantitative results in Table 1 with qualitative examples in Fig. 5. Our DMTET achieves significant improvements over all baselines in terms of all metrics. Compared to both ConvOnet [44] and DECOR-GAN [6], our DMTET reconstructs shapes with better quality when training without adversarial loss (5th column in Fig. 5). Further geometric details, including nails, ears, eyes, mouths, etc, are captured when trained with the adversarial loss (6th column in Fig. 5), significantly improving the realism and visual quality of the generated shape. To demonstrate the generalization ability of our DMTET, we collect human-created low-resolution voxels from Turbosquid (shapes unseen in training). We provide qualitative results in Fig. 6. Despite the fact that these human-created shapes have noticeable differences with our coarse voxels used in training, e.g., different ratios of body parts compared with our training shapes (larger head, thinner legs, longer necks), our model faithfully generates high-quality 3D details conditioned on each coarse voxel -an exciting result.  \n\n\nUser Studies\n\nWe conduct user studies via Amazon Machanical Turk (AMT) to further evaluate the performance of all methods. In particular, we present two shapes that are predicted from two different models to the AMT workers and ask them to evaluate which one is a better looking shape and which one features more realistic details. Detailed experimental settings are provided in the Supplement. We compare DMTET against ConvONet [44], DECOR [6]-Retv, as well as DMTET without adversarial loss (w.o. Adv.). Quantitative results are reported in Table 2. Human judges agree that the shapes generated from our model have better details, compared to all baselines, in a vast majority of the cases. Ablations on using adversarial loss demonstrate the effectiveness of generating higher quality geometry using a discriminator during training.  Ablation Studies To evaluate the effectiveness of our volume subdivision and surface subdivision modules, we ablate by sequentially introducing them to the base model (we refer as DMTET B ) which we train on 100-resolution uniform tetrahedral grid without both volume and surface subdivision modules and adversarial loss. We conduct user studies to evaluate the improvement after each step using the protocol described in the above paragraph. We first reduce the initial resolution to 70 and employ volume subdivision to support higher output resolution (we refer this model as DMTET V ) and compare with DMTET B . Predictions by DMTET V wins 78% of cases over DMTET B for better looking, and 61% of cases for realistic details, showing that the volume subdivision module is effective in synthesizing shape details. We then add surface subdivision on top of the DMTET V and compare with it. The new model wins 62% of cases over DMTET V for better looking, and 62% of cases for realistic details as well, demonstrating the effect of surface subdivision module in enhancing the shape details.\n\n\nPoint Cloud 3D Reconstruction Experimental Settings\n\nWe follow the setting from DefTet [18], and use all 13 categories in ShapeNet [3] core data 2 , which we pre-process using Kaolin [27] to watertight meshes. We sample 5000 points for each shape and add Gaussian noise with zero mean of standard deviation 0.005. For quantitative evaluation, we report the L1 Chamfer Distance in the main paper, and refer readers to the Supplement for results in other metrics (3D IoU, L2 Chamfer Distance and F1 score). We additionally report average inference time on the same Nvidia V100 GPU.\n\nWe compare DMTET against state-of-the-art 3D reconstruction approaches using different representations: voxels [10], deforming a mesh with a fixed template [54], deforming a mesh generated from a volumetric representation [22], DefTet [18], and implicit functions [44]. For a fair comparison, we use the same point cloud encoder for all the methods, and adopt the decoders in the original papers to generate shapes in different representations. We also remove the adversarial loss in this application, since baselines also do not have it. We further compare with oracle performance of MC/MT where the ground truth SDF is utilized to extract iso-surface using MC/MT.\n\n\nExperimental Results\n\nQuantitative results are summarized in Table 3, with a few qualitative examples shown in Fig. 7. Compared to DMC [31], which also predicts the SDF values and supervises with a surface loss, DMTET achieves much better reconstruction quality since training using the marching tetrahedra layer is more efficient than calculating an expectation over all possible configurations within one grid cell as done in DMC [31]. Compared to a method that deforms a fixed template (sphere) [54], we reconstruct shapes with different topologies, achieving more faithful results compared to the ground truth shape. When compared with other explicit surface representations that also support different topology [18,22], our method achieves higher quality results for local geometry, benefiting from the fact that the typology is jointly optimized with the geometry, whereas it  is separately supervised by an occupancy loss in [18,22]. Compared to a neural implicit method [44], we generate higher quality shapes with less artifacts, while running significantly faster at inference. Finally, compared to a voxel-based method [10] at the same resolution, our method recovers more geometric details, benefiting from the predicted vertex deformations as well as the surface loss. We investigate how each component in our representation affects the performance and reconstruction quality.\n\n\nAnalysis\n\nComparisons with Oracle Performance of MC/MT We first demonstrate the effect of learning on explicit surface via MT. We compare with the oracle performance of extracting the iso-surface with MT/MC from the ground truth signed distance fields on the Chair test set in ShapeNet, which contains diverse high-quality details. Specifically, for MC/MT, we first compute the discretized SDF at different grid resolutions, and compare the extracted surface to the ground truth surface.\n\nAs shown in Fig. 8, MT consistently outperforms MC when querying the same number of points. We found the staggered grids pattern in tetrahedral grid [16,18] better captures thin structures at a limited resolution (Fig. 9). This makes MT a better choice for efficiency reasons. The usage of tetrahedral mesh in DMTET follows this motivation. Without deforming the grid, DMTET outperforms the oracle performance of MT by a large margin when querying the same number of points, although DMTET predicts the surface from noisy point cloud. This demonstrates that directly optimizing the reconstructed surface can mitigate the discretization errors imposed by MT to a large extent. Ablation Studies We further provide ablation studies on the entire ShapeNet test set, which is summarized in Tab. 3. We first compare the version where we only predict SDF values without learning to deform the vertices and volume/surface subdivision with the version that predicts both SDF and the deformation. Predicting deformation along with SDF is significantly more performant, since vertex movements allow for a better reconstruction of the underlying surface. This is especially true for categories with thin structures (e.g. lamp) where the grid vertices are desired to align with them. We further ablate the use of volume subdivision and surface subdivision. We show that each component provides an improvement. In particular, volume subdivision has a significant improvement for object categories with fine-grained structural details, such as airplane and lamp, which require higher grid resolutions to model the occupancy change. Surface subdivision generates shapes with a parametric surface, avoiding the quantization errors in the planar faces and produces more visually pleasing results.\n\n\nConclusion\n\nIn this paper, we introduced a deep 3D conditional generative model that can synthesize highresolution 3D shapes using simple user guides such as coarse voxels. Our DMTET features a novel 3D representation that marries implicit and explicit representations by leveraging the advantages of both. We experimentally show that our approach synthesizes significantly higher quality shapes with better geometric details than existing methods, confirmed by quantitative metrics and an extensive user study. By showcasing the ability to upscale coarse voxels such as Minecraft shapes, we hope that we take one step closer to democratizing 3D content creation.\n\n\nBroad Impact\n\nMany fields such as AR/VR, robotics, architecture, gaming and film rely on high-quality 3D content. Creating such content, however, requires human experts, i.e., experienced artists, and a significant amount of development time. In contrast, platforms like Minecraft enable millions of users around the world to carve out coarse shapes with simple blocks. Our work aims at creating A.I. tools that would enable even novice users to upscale simple, low-resolution shapes into high resolution, beautiful 3D content. Our method currently focuses on 3D animal shapes. We are not currently aware of and do not foresee nefarious use cases of our method.\n\nFigure 2 :\n2Volume Subdivision: Each surface tet.(blue) is divided into 8 tet.(red) by adding midpoints.\n\nFigure 3 :\n3Three unique surface configurations in MT. Vertex color indicates the sign of signed distance value. Notice that flipping the signs of all vertices will result in the same surface configuration. Position of the vertex is linearly interpolated along the edges with sign change.\n\nFigure 4 :\n4Our generator and discriminator architectures. The generator is composed of two parts-one utilizes MLP to generate the initial predictions for all grid vertices and the other uses GCN to refine the surface.\n\nFigure 5 :\n5Qualitative results on 3D shapes Synthesis from Coarse Voxels. Comparing with all baselines,\n\nFigure 6 :\n6Qualitative Results of synthesizing highresolution shapes from coarse voxels collected online.\n\nFigure 8 :\n8Comparing our DMTET with oracle performance of MC and MT.\n\nFigure 9 :\n9We compare trained DMTET to oracle performance of MT and MC. Number in bracket indicates number of SDF points queried.\n\n\nThe above loss functions operate on the extracted surface, thus, only the vertices that are close to the iso-surface in the tetrahedral grid receive gradients, while the other vertices do not. Moreover, the surface losses do not provide information about what is inside/outside, since flipping the SDF sign of all vertices in a tetrahedron would result in the same surface being extracted by MT. This may lead to disconnected components during training. To alleviate this issue, we add a SDF loss to regularize SDF values:\n\n\nL2 Chamfer \u2193 L1 Chamfer \u2193 Norm. Cons. \u2191 LFD \u2193 Cls \u2193ConvOnet [44] \n0.83 \n2.41 \n0.901 \n3220 \n0.63 \nDECOR [6]-Retv. \n1.32 \n3.81 \n0.876 \n3689 \n0.66 \nDECOR [6]-Rand. \n2.38 \n6.85 \n0.797 \n5338 \n0.67 \nDMTET wo Adv. \n0.76 \n2.20 \n0.916 \n2846 \n0.58 \nDMTET \n0.75 \n2.19 \n0.918 \n2823 \n0.54 \n\n\n\nTable 1 :\n1Super Resolution of Animal Shapes: DMTET significantly outperforms all baselines in all metrics.\n\nTable 2 :\n2User Study on 3D Shape Synthesis fromCoarse voxels. In each cell, we report percentages of \nshapes for which the users agree are better looking (left) \nor have better details (right). \n\n\n\n\nFigure 7: Qualitative results on 3D Reconstruction from Point Clouds: Our model reconstructs shapes with more geometric details compared to baselines.Input PC \n\n3DR2N2 \nPix2Mesh \nDMC \nConvOnet MeshRCNN \nDEFTET \nOurs \nGT \n\nCategory \nAirplane Bench Dresser \nCar \nChair Display Lamp Speaker Rifle \nSofa \nTable Phone Vessel Mean\u2193 Time(ms)\u2193 \n3D-R2N2 [10] \n1.48 \n1.59 \n1.64 \n1.62 \n1.70 \n1.66 \n1.74 \n1.74 \n1.37 \n1.60 \n1.78 \n1.55 \n1.51 \n1.61 \n174 \nDMC [31] \n1.57 \n1.47 \n1.29 \n1.67 \n1.44 \n1.25 \n2.15 \n1.49 \n1.45 \n1.19 \n1.33 \n0.88 \n1.70 \n1.45 \n349 \nPixel2mesh [54] \n0.98 \n1.28 \n1.44 \n1.19 \n1.91 \n1.25 \n2.07 \n1.61 \n0.91 \n1.15 \n1.82 \n0.83 \n1.12 \n1.35 \n30 \nConvOnet [44] \n0.82 \n0.95 \n0.96 \n1.12 \n1.03 \n0.93 \n1.22 \n1.12 \n0.79 \n0.91 \n0.94 \n0.67 \n0.99 \n0.95 \n866 \nMeshRCNN [22] \n0.88 \n1.01 \n1.05 \n1.14 \n1.10 \n0.99 \n1.20 \n1.21 \n0.83 \n0.96 \n1.00 \n0.71 \n1.03 \n1.01 \n228 \nDEFTET [18] \n0.85 \n0.94 \n0.97 \n1.13 \n1.04 \n0.92 \n1.28 \n1.17 \n0.85 \n0.90 \n0.93 \n0.65 \n0.99 \n0.97 \n61 \nDMTET wo (Def, Vol., Surf.) 0.82 \n0.96 \n0.94 \n0.98 \n0.99 \n0.90 \n1.04 \n1.03 \n0.80 \n0.86 \n0.93 \n0.65 \n0.89 \n0.91 \n52 \nDMTET wo (Vol., Surf.) \n0.69 \n0.82 \n0.88 \n0.92 \n0.92 \n0.82 \n0.89 \n0.97 \n0.65 \n0.81 \n0.84 \n0.61 \n0.80 \n0.81 \n52 \nDMTET wo Vol. \n0.65 \n0.78 \n0.84 \n0.89 \n0.89 \n0.79 \n0.86 \n0.95 \n0.61 \n0.78 \n0.79 \n0.60 \n0.78 \n0.79 \n67 \nDMTET wo Surf. \n0.63 \n0.77 \n0.84 \n0.88 \n0.88 \n0.79 \n0.84 \n0.94 \n0.60 \n0.78 \n0.79 \n0.59 \n0.76 \n0.78 \n108 \nDMTET \n0.62 \n0.76 \n0.83 \n0.87 \n0.88 \n0.78 \n0.84 \n0.94 \n0.59 \n0.77 \n0.78 \n0.57 \n0.76 \n0.77 \n129 \n\n\n\nTable 3 :\n3Quantitative Results on Point Cloud Reconstruction (Chamfer L1). Note that all the networks in the baselines are not designed for this task, and thus we use the same encoder and their decoder for a fair comparison. We also ablate ourselves by operating on fixed grid (DMTET wo (Def, Vol., Surf.)), removing volume subdivision (DMTET wo Vol.), or surface subdivision (DMTET wo Surf.), or the both (DMTET wo (Vol., Surf.)).\nhttps://www.turbosquid.com, we obtain consent via an agreement with TurboSquid, and following license at https://blog.turbosquid.com/turbosquid-3d-model-license/\nThe ShapeNet license is explained at https://shapenet.org/terms\nDisclosure of FundingThis work was funded by NVIDIA. Tianchang Shen and Jun Gao acknowledge additional revenue in the form of student scholarships from University of Toronto and the Vector Institute, which are not in direct support of this work.\nSal: Sign agnostic learning of shapes from raw data. Matan Atzmon, Yaron Lipman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMatan Atzmon and Yaron Lipman. Sal: Sign agnostic learning of shapes from raw data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2565-2574, 2020.\n\nAndrew Brock, Theodore Lim, M James, Nick Ritchie, Weston, arXiv:1608.04236Generative and discriminative voxel modeling with convolutional neural networks. arXiv preprintAndrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Generative and discriminative voxel modeling with convolutional neural networks. arXiv preprint arXiv:1608.04236, 2016.\n\nX Angel, Thomas Chang, Leonidas Funkhouser, Pat Guibas, Qixing Hanrahan, Zimo Huang, Silvio Li, Manolis Savarese, Shuran Savva, Hao Song, Su, arXiv:1512.03012An information-rich 3d model repository. arXiv preprintAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n\nOn visual similarity based 3d model retrieval. Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, Ming Ouhyoung, Computer graphics forum. Wiley Online Library22Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung. On visual similarity based 3d model retrieval. In Computer graphics forum, volume 22, pages 223-232. Wiley Online Library, 2003.\n\nLearning to predict 3d objects with an interpolation-based differentiable renderer. Wenzheng Chen, Jun Gao, Huan Ling, Edward Smith, Jaakko Lehtinen, Alec Jacobson, Sanja Fidler, Advances In Neural Information Processing Systems. Wenzheng Chen, Jun Gao, Huan Ling, Edward Smith, Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learning to predict 3d objects with an interpolation-based differentiable renderer. In Advances In Neural Information Processing Systems, 2019.\n\nDecor-gan: 3d shape detailization by conditional refinement. Zhiqin Chen, Vladimir G Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, Siddhartha Chaudhuri, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2021Zhiqin Chen, Vladimir G. Kim, Matthew Fisher, Noam Aigerman, Hao Zhang, and Siddhartha Chaudhuri. Decor-gan: 3d shape detailization by conditional refinement. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nBsp-net: Generating compact meshes via binary space partitioning. Zhiqin Chen, Andrea Tagliasacchi, Hao Zhang, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2020Zhiqin Chen, Andrea Tagliasacchi, and Hao Zhang. Bsp-net: Generating compact meshes via binary space partitioning. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n\nLearning implicit fields for generative shape modeling. Zhiqin Chen, Hao Zhang, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n\nNeural unsigned distance fields for implicit function learning. Julian Chibane, Aymen Mir, Gerard Pons-Moll, Advances in Neural Information Processing Systems (NeurIPS). Julian Chibane, Aymen Mir, and Gerard Pons-Moll. Neural unsigned distance fields for implicit function learning. In Advances in Neural Information Processing Systems (NeurIPS), December 2020.\n\n3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. B Christopher, Danfei Choy, Junyoung Xu, Kevin Gwak, Silvio Chen, Savarese, European conference on computer vision. SpringerChristopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In European conference on computer vision, pages 628-644. Springer, 2016.\n\nScancomplete: Large-scale scene completion and semantic segmentation for 3d scans. Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, J\u00fcrgen Sturm, Matthias Nie\u00dfner, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAngela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, J\u00fcrgen Sturm, and Matthias Nie\u00dfner. Scancom- plete: Large-scale scene completion and semantic segmentation for 3d scans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4578-4587, 2018.\n\nShape completion using 3d-encoder-predictor cnns and shape synthesis. Angela Dai, Charles Ruizhongtai Qi, Matthias Nie\u00dfner, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAngela Dai, Charles Ruizhongtai Qi, and Matthias Nie\u00dfner. Shape completion using 3d-encoder-predictor cnns and shape synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5868-5877, 2017.\n\nThomas Davies, Derek Nowrouzezahrai, Alec Jacobson, arXiv:2009.09808Overfit neural networks as a compact shape representation. arXiv preprintThomas Davies, Derek Nowrouzezahrai, and Alec Jacobson. Overfit neural networks as a compact shape representation. arXiv preprint arXiv:2009.09808, 2020.\n\nDeformed implicit field: Modeling 3d shapes with learned dense correspondence. Yu Deng, Jiaolong Yang, Xin Tong, IEEE Computer Vision and Pattern Recognition. Yu Deng, Jiaolong Yang, and Xin Tong. Deformed implicit field: Modeling 3d shapes with learned dense correspondence. In IEEE Computer Vision and Pattern Recognition, 2021.\n\nAn efficient method of triangulating equi-valued surfaces by using tetrahedral cells. Akio Doi, Akio Koide, IEICE TRANSACTIONS on Information and Systems. 741Akio Doi and Akio Koide. An efficient method of triangulating equi-valued surfaces by using tetrahedral cells. IEICE TRANSACTIONS on Information and Systems, 74(1):214-224, 1991.\n\nIsosurface stuffing improved: acute lattices and feature matching. Crawford Doran, Athena Chang, Robert Bridson, ACM SIGGRAPH 2013 Talks. Crawford Doran, Athena Chang, and Robert Bridson. Isosurface stuffing improved: acute lattices and feature matching. In ACM SIGGRAPH 2013 Talks, pages 1-1, 2013.\n\nYueqi Duan, Haidong Zhu, He Wang, Li Yi, Ram Nevatia, Leonidas J Guibas, European Conference on Computer Vision. SpringerCurriculum deepsdfYueqi Duan, Haidong Zhu, He Wang, Li Yi, Ram Nevatia, and Leonidas J Guibas. Curriculum deepsdf. In European Conference on Computer Vision, pages 51-67. Springer, 2020.\n\nLearning deformable tetrahedral meshes for 3d reconstruction. Jun Gao, Wenzheng Chen, Tommy Xiang, Clement Fuji Tsang, Alec Jacobson, Morgan Mcguire, Sanja Fidler, Advances In Neural Information Processing Systems. Jun Gao, Wenzheng Chen, Tommy Xiang, Clement Fuji Tsang, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Learning deformable tetrahedral meshes for 3d reconstruction. In Advances In Neural Information Processing Systems, 2020.\n\nDeepspline: Data-driven reconstruction of parametric curves and surfaces. Jun Gao, Chengcheng Tang, Vignesh Ganapathi-Subramanian, Jiahui Huang, Hao Su, Leonidas J Guibas, arXiv:1901.03781arXiv preprintJun Gao, Chengcheng Tang, Vignesh Ganapathi-Subramanian, Jiahui Huang, Hao Su, and Leonidas J Guibas. Deepspline: Data-driven reconstruction of parametric curves and surfaces. arXiv preprint arXiv:1901.03781, 2019.\n\nBeyond fixed grid: Learning geometric image representation with a deformable grid. Jun Gao, Zian Wang, Jinchen Xuan, Sanja Fidler, ECCV. Jun Gao, Zian Wang, Jinchen Xuan, and Sanja Fidler. Beyond fixed grid: Learning geometric image representation with a deformable grid. In ECCV, 2020.\n\nSdm-net: Deep generative network for structured deformable mesh. Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, Hao Zhang, ACM Transactions on Graphics (TOG). 386Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, and Hao Zhang. Sdm-net: Deep generative network for structured deformable mesh. ACM Transactions on Graphics (TOG), 38(6):1-15, 2019.\n\nMesh r-cnn. Georgia Gkioxari, Jitendra Malik, Justin Johnson, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionGeorgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 9785-9795, 2019.\n\nA papier-m\u00e2ch\u00e9 approach to learning 3d surface generation. Thibault Groueix, Matthew Fisher, G Vladimir, Kim, C Bryan, Mathieu Russell, Aubry, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionThibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, and Mathieu Aubry. A papier-m\u00e2ch\u00e9 approach to learning 3d surface generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 216-224, 2018.\n\nHierarchical surface prediction for 3d object reconstruction. Christian H\u00e4ne, Shubham Tulsiani, Jitendra Malik, 2017 International Conference on 3D Vision (3DV). IEEEChristian H\u00e4ne, Shubham Tulsiani, and Jitendra Malik. Hierarchical surface prediction for 3d object reconstruction. In 2017 International Conference on 3D Vision (3DV), pages 412-420. IEEE, 2017.\n\nPoint2mesh: A self-prior for deformable meshes. Rana Hanocka, Gal Metzer, Raja Giryes, Daniel Cohen-Or, ACM Trans. Graph. 3942020Rana Hanocka, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. Point2mesh: A self-prior for deformable meshes. ACM Trans. Graph., 39(4), 2020.\n\nDualsdf: Semantic shape manipulation using a two-level representation. Zekun Hao, Hadar Averbuch-Elor, Noah Snavely, Serge Belongie, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZekun Hao, Hadar Averbuch-Elor, Noah Snavely, and Serge Belongie. Dualsdf: Semantic shape manipula- tion using a two-level representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7631-7641, 2020.\n\nKrishna Murthy, J , Edward Smith, Jean-Francois Lafleche, Clement Fuji Tsang, Artem Rozantsev, Wenzheng Chen, Tommy Xiang, arXiv:1911.05063Rev Lebaredian, and Sanja Fidler. Kaolin: A pytorch library for accelerating 3d deep learning research. Krishna Murthy J., Edward Smith, Jean-Francois Lafleche, Clement Fuji Tsang, Artem Rozantsev, Wen- zheng Chen, Tommy Xiang, Rev Lebaredian, and Sanja Fidler. Kaolin: A pytorch library for accelerating 3d deep learning research. arXiv:1911.05063, 2019.\n\nAdversarial generation of continuous implicit shape representations. Marian Kleineberg, Matthias Fey, Frank Weichert, arXiv:2002.00349arXiv preprintMarian Kleineberg, Matthias Fey, and Frank Weichert. Adversarial generation of continuous implicit shape representations. arXiv preprint arXiv:2002.00349, 2020.\n\nAnalytic marching: An analytic meshing solution from deep implicit surface networks. Jiabao Lei, Kui Jia, International Conference on Machine Learning. PMLRJiabao Lei and Kui Jia. Analytic marching: An analytic meshing solution from deep implicit surface networks. In International Conference on Machine Learning, pages 5789-5798. PMLR, 2020.\n\nD2im-net: Learning detail disentangled implicit fields from single images. Manyi Li, Hao Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionManyi Li and Hao Zhang. D2im-net: Learning detail disentangled implicit fields from single images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10246- 10255, 2021.\n\nDeep marching cubes: Learning explicit surface representations. Yiyi Liao, Simon Donn\u00e9, Andreas Geiger, Conference on Computer Vision and Pattern Recognition (CVPR). Yiyi Liao, Simon Donn\u00e9, and Andreas Geiger. Deep marching cubes: Learning explicit surface representa- tions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n\nFast interactive object annotation with curve-gcn. Huan Ling, Jun Gao, Amlan Kar, Wenzheng Chen, Sanja Fidler, CVPR. Huan Ling, Jun Gao, Amlan Kar, Wenzheng Chen, and Sanja Fidler. Fast interactive object annotation with curve-gcn. In CVPR, 2019.\n\nNeural subdivision. Hsueh-Ti Derek Liu, Vladimir G Kim, Siddhartha Chaudhuri, Noam Aigerman, Alec Jacobson, ACM Trans. Graph. 3942020Hsueh-Ti Derek Liu, Vladimir G. Kim, Siddhartha Chaudhuri, Noam Aigerman, and Alec Jacobson. Neural subdivision. ACM Trans. Graph., 39(4), 2020.\n\nPoint-voxel cnn for efficient 3d deep learning. Zhijian Liu, Haotian Tang, Yujun Lin, Song Han, Advances in Neural Information Processing Systems. Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efficient 3d deep learning. In Advances in Neural Information Processing Systems, 2019.\n\nSmooth subdivision surfaces based on triangles. Charles Loop, Charles Loop. Smooth subdivision surfaces based on triangles. January 1987.\n\nMarching cubes: A high resolution 3d surface construction algorithm. E William, Harvey E Lorensen, Cline, ACM siggraph computer graphics. 214William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction algorithm. ACM siggraph computer graphics, 21(4):163-169, 1987.\n\nLeast squares generative adversarial networks. Xudong Mao, Qing Li, Haoran Xie, Y K Raymond, Zhen Lau, Stephen Paul Wang, Smolley, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionXudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2794-2802, 2017.\n\nVoxnet: A 3d convolutional neural network for real-time object recognition. Daniel Maturana, Sebastian Scherer, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEDaniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 922-928. IEEE, 2015.\n\nOccupancy networks: Learning 3d reconstruction in function space. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4460-4470, 2019.\n\nNerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, ECCV. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\n\nPolygen: An autoregressive generative model of 3d meshes. Charlie Nash, Yaroslav Ganin, S M Ali Eslami, Peter W Battaglia, 2020Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, and Peter W. Battaglia. Polygen: An autoregressive generative model of 3d meshes. ICML, 2020.\n\nDeepsdf: Learning continuous signed distance functions for shape representation. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 165-174, 2019.\n\nNeural parts: Learning expressive 3d shape abstractions with invertible neural networks. Despoina Paschalidou, Angelos Katharopoulos, Andreas Geiger, Sanja Fidler, Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)2021Despoina Paschalidou, Angelos Katharopoulos, Andreas Geiger, and Sanja Fidler. Neural parts: Learning expressive 3d shape abstractions with invertible neural networks. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nConvolutional occupancy networks. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger, European Conference on Computer Vision (ECCV). 2020Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In European Conference on Computer Vision (ECCV), 2020.\n\nMeshsdf: Differentiable iso-surface extraction. Edoardo Remelli, Artem Lukoianov, Stephan Richter, Benoit Guillard, Timur Bagautdinov, Pierre Baque, Pascal Fua, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33Edoardo Remelli, Artem Lukoianov, Stephan Richter, Benoit Guillard, Timur Bagautdinov, Pierre Baque, and Pascal Fua. Meshsdf: Differentiable iso-surface extraction. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 22468-22478. Curran Associates, Inc., 2020.\n\nOctnet: Learning deep 3d representations at high resolutions. Gernot Riegler, Ali Osman Ulusoy, Andreas Geiger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionGernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at high resolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3577-3586, 2017.\n\nPifu: Pixel-aligned implicit function for high-resolution clothed human digitization. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li, The IEEE International Conference on Computer Vision (ICCV). Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n\nPifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization. Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionShunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, June 2020.\n\nImplicit neural representations with periodic activation functions. Vincent Sitzmann, N P Julien, Alexander W Martel, David B Bergman, Gordon Lindell, Wetzstein, Proc. NeurIPS. NeurIPSVincent Sitzmann, Julien N.P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Proc. NeurIPS, 2020.\n\nComplementme: Weakly-supervised component suggestions for 3d modeling. Minhyuk Sung, Hao Su, G Vladimir, Siddhartha Kim, Leonidas Chaudhuri, Guibas, ACM Transactions on Graphics (TOG). 366Minhyuk Sung, Hao Su, Vladimir G Kim, Siddhartha Chaudhuri, and Leonidas Guibas. Complementme: Weakly-supervised component suggestions for 3d modeling. ACM Transactions on Graphics (TOG), 36(6):1-12, 2017.\n\nNeural geometric level of detail: Real-time rendering with implicit 3D shapes. Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan Mcguire, Sanja Fidler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2021Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3D shapes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n\nOctree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. M Tatarchenko, A Dosovitskiy, T Brox, IEEE International Conference on Computer Vision (ICCV. M. Tatarchenko, A. Dosovitskiy, and T. Brox. Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. In IEEE International Conference on Computer Vision (ICCV), 2017.\n\nLearning shape abstractions by assembling volumetric primitives. Shubham Tulsiani, Hao Su, Leonidas J Guibas, Alexei A Efros, Jitendra Malik, Computer Vision and Pattern Regognition (CVPR. Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros, and Jitendra Malik. Learning shape abstractions by assembling volumetric primitives. In Computer Vision and Pattern Regognition (CVPR), 2017.\n\nPixel2mesh: Generating 3d mesh models from single rgb images. Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, Yu-Gang Jiang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh models from single rgb images. In Proceedings of the European Conference on Computer Vision (ECCV), pages 52-67, 2018.\n\nO-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis. Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, Xin Tong, ACM Transactions on Graphics (SIGGRAPH). 364Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-CNN: Octree-based Convo- lutional Neural Networks for 3D Shape Analysis. ACM Transactions on Graphics (SIGGRAPH), 36(4), 2017.\n\nDeep octree-based cnns with output-guided skip connections for 3d shape and scene completion. Peng-Shuai Wang, Yang Liu, Xin Tong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition WorkshopsPeng-Shuai Wang, Yang Liu, and Xin Tong. Deep octree-based cnns with output-guided skip connections for 3d shape and scene completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 266-267, 2020.\n\nNeural splines: Fitting 3d surfaces with infinitely-wide neural networks. Francis Williams, Matthew Trager, Joan Bruna, Denis Zorin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionFrancis Williams, Matthew Trager, Joan Bruna, and Denis Zorin. Neural splines: Fitting 3d surfaces with infinitely-wide neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9949-9958, 2021.\n\nLearning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. Jiajun Wu, Chengkai Zhang, Tianfan Xue, T William, Joshua B Freeman, Tenenbaum, Advances in Neural Information Processing Systems. Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T Freeman, and Joshua B Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural Information Processing Systems, pages 82-90, 2016.\n\n3d shapenets: A deep representation for volumetric shapes. Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1912-1920, 2015.\n\nDisn: Deep implicit surface network for high-quality single-view 3d reconstruction. Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann, Advances in Neural Information Processing Systems. Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. Disn: Deep implicit surface network for high-quality single-view 3d reconstruction. In Advances in Neural Information Processing Systems, pages 490-500, 2019.\n\nCoalesce: Component assembly by learning to synthesize connections. Kangxue Yin, Zhiqin Chen, Siddhartha Chaudhuri, Matthew Fisher, Vladimir Kim, Hao Zhang, Proc. of 3DV. of 3DVKangxue Yin, Zhiqin Chen, Siddhartha Chaudhuri, Matthew Fisher, Vladimir Kim, and Hao Zhang. Coalesce: Component assembly by learning to synthesize connections. In Proc. of 3DV, 2020.\n\nSCORES: Shape composition with recursive substructure priors. Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Renjiao Yi, Hao Zhang, ACM Transactions on Graphics. 376211Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Renjiao Yi, and Hao Zhang. SCORES: Shape composition with recursive substructure priors. ACM Transactions on Graphics, 37(6):Article 211, 2018.\n", "annotations": {"author": "[{\"end\":168,\"start\":92},{\"end\":236,\"start\":169},{\"end\":293,\"start\":237},{\"end\":349,\"start\":294},{\"end\":425,\"start\":350},{\"end\":435,\"start\":426}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":102},{\"end\":176,\"start\":173},{\"end\":248,\"start\":245},{\"end\":305,\"start\":302},{\"end\":362,\"start\":356}]", "author_first_name": "[{\"end\":101,\"start\":92},{\"end\":172,\"start\":169},{\"end\":244,\"start\":237},{\"end\":301,\"start\":294},{\"end\":355,\"start\":350}]", "author_affiliation": "[{\"end\":148,\"start\":126},{\"end\":167,\"start\":150},{\"end\":216,\"start\":194},{\"end\":235,\"start\":218},{\"end\":292,\"start\":270},{\"end\":348,\"start\":326},{\"end\":405,\"start\":383},{\"end\":424,\"start\":407},{\"end\":434,\"start\":427}]", "title": "[{\"end\":89,\"start\":1},{\"end\":524,\"start\":436}]", "venue": null, "abstract": "[{\"end\":4830,\"start\":526}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6107,\"start\":6106},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6324,\"start\":6320},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6327,\"start\":6324},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7085,\"start\":7081},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7088,\"start\":7085},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7091,\"start\":7088},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7328,\"start\":7325},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7331,\"start\":7328},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7334,\"start\":7331},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":7337,\"start\":7334},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7389,\"start\":7386},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7826,\"start\":7822},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7829,\"start\":7826},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7832,\"start\":7829},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7835,\"start\":7832},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7838,\"start\":7835},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7841,\"start\":7838},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8348,\"start\":8344},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8351,\"start\":8348},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8354,\"start\":8351},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8357,\"start\":8354},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8895,\"start\":8891},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8898,\"start\":8895},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8901,\"start\":8898},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8904,\"start\":8901},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8943,\"start\":8940},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8945,\"start\":8943},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8948,\"start\":8945},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8951,\"start\":8948},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8954,\"start\":8951},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8957,\"start\":8954},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9307,\"start\":9303},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9335,\"start\":9331},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9486,\"start\":9482},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9876,\"start\":9873},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9878,\"start\":9876},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10181,\"start\":10178},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10183,\"start\":10181},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10186,\"start\":10183},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10189,\"start\":10186},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10192,\"start\":10189},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10279,\"start\":10276},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10282,\"start\":10279},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10285,\"start\":10282},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10316,\"start\":10312},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10319,\"start\":10316},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10322,\"start\":10319},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10355,\"start\":10351},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10358,\"start\":10355},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10361,\"start\":10358},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10475,\"start\":10471},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11643,\"start\":11639},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11646,\"start\":11643},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12856,\"start\":12852},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12976,\"start\":12972},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13978,\"start\":13974},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19884,\"start\":19880},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19887,\"start\":19884},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20662,\"start\":20658},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21003,\"start\":20999},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21506,\"start\":21502},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22565,\"start\":22561},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23994,\"start\":23990},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24608,\"start\":24605},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26424,\"start\":26420},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27716,\"start\":27712},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":28183,\"start\":28179},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28270,\"start\":28267},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29171,\"start\":29167},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29212,\"start\":29209},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29326,\"start\":29323},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29852,\"start\":29848},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29870,\"start\":29867},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":31177,\"start\":31173},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31188,\"start\":31185},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32765,\"start\":32761},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32808,\"start\":32805},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32861,\"start\":32857},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33370,\"start\":33366},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":33415,\"start\":33411},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33481,\"start\":33477},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33494,\"start\":33490},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":33523,\"start\":33519},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":34062,\"start\":34058},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":34359,\"start\":34355},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":34425,\"start\":34421},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34643,\"start\":34639},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34646,\"start\":34643},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34859,\"start\":34855},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34862,\"start\":34859},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":34905,\"start\":34901},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35057,\"start\":35053},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35957,\"start\":35953},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35960,\"start\":35957}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":39018,\"start\":38913},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39308,\"start\":39019},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39528,\"start\":39309},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39634,\"start\":39529},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39742,\"start\":39635},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39813,\"start\":39743},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39945,\"start\":39814},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40470,\"start\":39946},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":40751,\"start\":40471},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":40860,\"start\":40752},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":41059,\"start\":40861},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":42548,\"start\":41060},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":42982,\"start\":42549}]", "paragraph": "[{\"end\":5742,\"start\":4846},{\"end\":6931,\"start\":5744},{\"end\":7048,\"start\":6948},{\"end\":8214,\"start\":7050},{\"end\":8815,\"start\":8216},{\"end\":10031,\"start\":8817},{\"end\":11008,\"start\":10033},{\"end\":11498,\"start\":11037},{\"end\":12699,\"start\":11520},{\"end\":12977,\"start\":12775},{\"end\":13156,\"start\":13097},{\"end\":14070,\"start\":13158},{\"end\":19866,\"start\":19382},{\"end\":20408,\"start\":19868},{\"end\":21192,\"start\":20432},{\"end\":21458,\"start\":21240},{\"end\":21760,\"start\":21489},{\"end\":22080,\"start\":21762},{\"end\":22572,\"start\":22082},{\"end\":23093,\"start\":22711},{\"end\":23578,\"start\":23095},{\"end\":23995,\"start\":23580},{\"end\":24448,\"start\":23997},{\"end\":25427,\"start\":24469},{\"end\":25833,\"start\":25445},{\"end\":26105,\"start\":25835},{\"end\":26355,\"start\":26221},{\"end\":26425,\"start\":26357},{\"end\":26524,\"start\":26508},{\"end\":26761,\"start\":26577},{\"end\":26847,\"start\":26791},{\"end\":27018,\"start\":26925},{\"end\":27290,\"start\":27034},{\"end\":28111,\"start\":27354},{\"end\":28988,\"start\":28113},{\"end\":30741,\"start\":28990},{\"end\":32671,\"start\":30758},{\"end\":33253,\"start\":32727},{\"end\":33920,\"start\":33255},{\"end\":35312,\"start\":33945},{\"end\":35802,\"start\":35325},{\"end\":37582,\"start\":35804},{\"end\":38248,\"start\":37597},{\"end\":38912,\"start\":38265}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13096,\"start\":12978},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19260,\"start\":14092},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19381,\"start\":19345},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22632,\"start\":22573},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22710,\"start\":22632},{\"attributes\":{\"id\":\"formula_5\"},\"end\":26220,\"start\":26106},{\"attributes\":{\"id\":\"formula_6\"},\"end\":26507,\"start\":26426},{\"attributes\":{\"id\":\"formula_7\"},\"end\":26576,\"start\":26525},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26790,\"start\":26762},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26924,\"start\":26848}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29696,\"start\":29689},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31294,\"start\":31287},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":33991,\"start\":33984}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":4844,\"start\":4832},{\"attributes\":{\"n\":\"2\"},\"end\":6946,\"start\":6934},{\"attributes\":{\"n\":\"3\"},\"end\":11035,\"start\":11011},{\"attributes\":{\"n\":\"3.1\"},\"end\":11518,\"start\":11501},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":12773,\"start\":12702},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":14091,\"start\":14073},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":19344,\"start\":19262},{\"attributes\":{\"n\":\"3.1.4\"},\"end\":20430,\"start\":20411},{\"attributes\":{\"n\":\"3.2\"},\"end\":21238,\"start\":21195},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":21487,\"start\":21461},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":24467,\"start\":24451},{\"attributes\":{\"n\":\"3.3\"},\"end\":25443,\"start\":25430},{\"attributes\":{\"n\":\"4\"},\"end\":27032,\"start\":27021},{\"attributes\":{\"n\":\"4.1\"},\"end\":27352,\"start\":27293},{\"end\":30756,\"start\":30744},{\"attributes\":{\"n\":\"4.2\"},\"end\":32725,\"start\":32674},{\"end\":33943,\"start\":33923},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":35323,\"start\":35315},{\"attributes\":{\"n\":\"5\"},\"end\":37595,\"start\":37585},{\"attributes\":{\"n\":\"6\"},\"end\":38263,\"start\":38251},{\"end\":38924,\"start\":38914},{\"end\":39030,\"start\":39020},{\"end\":39320,\"start\":39310},{\"end\":39540,\"start\":39530},{\"end\":39646,\"start\":39636},{\"end\":39754,\"start\":39744},{\"end\":39825,\"start\":39815},{\"end\":40762,\"start\":40753},{\"end\":40871,\"start\":40862},{\"end\":42559,\"start\":42550}]", "table": "[{\"end\":40751,\"start\":40524},{\"end\":41059,\"start\":40910},{\"end\":42548,\"start\":41212}]", "figure_caption": "[{\"end\":39018,\"start\":38926},{\"end\":39308,\"start\":39032},{\"end\":39528,\"start\":39322},{\"end\":39634,\"start\":39542},{\"end\":39742,\"start\":39648},{\"end\":39813,\"start\":39756},{\"end\":39945,\"start\":39827},{\"end\":40470,\"start\":39948},{\"end\":40524,\"start\":40473},{\"end\":40860,\"start\":40764},{\"end\":40910,\"start\":40873},{\"end\":41212,\"start\":41062},{\"end\":42982,\"start\":42561}]", "figure_ref": "[{\"end\":8509,\"start\":8501},{\"end\":11154,\"start\":11148},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13940,\"start\":13933},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19532,\"start\":19526},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19865,\"start\":19859},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21457,\"start\":21449},{\"end\":21928,\"start\":21915},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29732,\"start\":29726},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29982,\"start\":29976},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30128,\"start\":30121},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30397,\"start\":30391},{\"end\":34040,\"start\":34034},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35822,\"start\":35816},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":36025,\"start\":36017}]", "bib_author_first_name": "[{\"end\":43513,\"start\":43508},{\"end\":43527,\"start\":43522},{\"end\":43885,\"start\":43879},{\"end\":43901,\"start\":43893},{\"end\":43908,\"start\":43907},{\"end\":43920,\"start\":43916},{\"end\":44233,\"start\":44232},{\"end\":44247,\"start\":44241},{\"end\":44263,\"start\":44255},{\"end\":44279,\"start\":44276},{\"end\":44294,\"start\":44288},{\"end\":44309,\"start\":44305},{\"end\":44323,\"start\":44317},{\"end\":44335,\"start\":44328},{\"end\":44352,\"start\":44346},{\"end\":44363,\"start\":44360},{\"end\":44739,\"start\":44731},{\"end\":44754,\"start\":44746},{\"end\":44766,\"start\":44761},{\"end\":44777,\"start\":44773},{\"end\":45118,\"start\":45110},{\"end\":45128,\"start\":45125},{\"end\":45138,\"start\":45134},{\"end\":45151,\"start\":45145},{\"end\":45165,\"start\":45159},{\"end\":45180,\"start\":45176},{\"end\":45196,\"start\":45191},{\"end\":45567,\"start\":45561},{\"end\":45582,\"start\":45574},{\"end\":45584,\"start\":45583},{\"end\":45597,\"start\":45590},{\"end\":45610,\"start\":45606},{\"end\":45624,\"start\":45621},{\"end\":45642,\"start\":45632},{\"end\":46125,\"start\":46119},{\"end\":46138,\"start\":46132},{\"end\":46156,\"start\":46153},{\"end\":46581,\"start\":46575},{\"end\":46591,\"start\":46588},{\"end\":46988,\"start\":46982},{\"end\":47003,\"start\":46998},{\"end\":47015,\"start\":47009},{\"end\":47362,\"start\":47361},{\"end\":47382,\"start\":47376},{\"end\":47397,\"start\":47389},{\"end\":47407,\"start\":47402},{\"end\":47420,\"start\":47414},{\"end\":47808,\"start\":47802},{\"end\":47820,\"start\":47814},{\"end\":47836,\"start\":47830},{\"end\":47851,\"start\":47846},{\"end\":47864,\"start\":47858},{\"end\":47880,\"start\":47872},{\"end\":48390,\"start\":48384},{\"end\":48403,\"start\":48396},{\"end\":48415,\"start\":48404},{\"end\":48428,\"start\":48420},{\"end\":48819,\"start\":48813},{\"end\":48833,\"start\":48828},{\"end\":48854,\"start\":48850},{\"end\":49190,\"start\":49188},{\"end\":49205,\"start\":49197},{\"end\":49215,\"start\":49212},{\"end\":49531,\"start\":49527},{\"end\":49541,\"start\":49537},{\"end\":49854,\"start\":49846},{\"end\":49868,\"start\":49862},{\"end\":49882,\"start\":49876},{\"end\":50085,\"start\":50080},{\"end\":50099,\"start\":50092},{\"end\":50107,\"start\":50105},{\"end\":50116,\"start\":50114},{\"end\":50124,\"start\":50121},{\"end\":50142,\"start\":50134},{\"end\":50144,\"start\":50143},{\"end\":50454,\"start\":50451},{\"end\":50468,\"start\":50460},{\"end\":50480,\"start\":50475},{\"end\":50495,\"start\":50488},{\"end\":50500,\"start\":50496},{\"end\":50512,\"start\":50508},{\"end\":50529,\"start\":50523},{\"end\":50544,\"start\":50539},{\"end\":50910,\"start\":50907},{\"end\":50926,\"start\":50916},{\"end\":50940,\"start\":50933},{\"end\":50970,\"start\":50964},{\"end\":50981,\"start\":50978},{\"end\":50994,\"start\":50986},{\"end\":50996,\"start\":50995},{\"end\":51337,\"start\":51334},{\"end\":51347,\"start\":51343},{\"end\":51361,\"start\":51354},{\"end\":51373,\"start\":51368},{\"end\":51607,\"start\":51604},{\"end\":51616,\"start\":51613},{\"end\":51627,\"start\":51623},{\"end\":51638,\"start\":51632},{\"end\":51651,\"start\":51645},{\"end\":51662,\"start\":51656},{\"end\":51671,\"start\":51668},{\"end\":51936,\"start\":51929},{\"end\":51955,\"start\":51947},{\"end\":51969,\"start\":51963},{\"end\":52329,\"start\":52321},{\"end\":52346,\"start\":52339},{\"end\":52356,\"start\":52355},{\"end\":52373,\"start\":52372},{\"end\":52388,\"start\":52381},{\"end\":52866,\"start\":52857},{\"end\":52880,\"start\":52873},{\"end\":52899,\"start\":52891},{\"end\":53210,\"start\":53206},{\"end\":53223,\"start\":53220},{\"end\":53236,\"start\":53232},{\"end\":53251,\"start\":53245},{\"end\":53504,\"start\":53499},{\"end\":53515,\"start\":53510},{\"end\":53535,\"start\":53531},{\"end\":53550,\"start\":53545},{\"end\":53966,\"start\":53959},{\"end\":53976,\"start\":53975},{\"end\":53985,\"start\":53979},{\"end\":54006,\"start\":53993},{\"end\":54024,\"start\":54017},{\"end\":54029,\"start\":54025},{\"end\":54042,\"start\":54037},{\"end\":54062,\"start\":54054},{\"end\":54074,\"start\":54069},{\"end\":54530,\"start\":54524},{\"end\":54551,\"start\":54543},{\"end\":54562,\"start\":54557},{\"end\":54856,\"start\":54850},{\"end\":54865,\"start\":54862},{\"end\":55189,\"start\":55184},{\"end\":55197,\"start\":55194},{\"end\":55634,\"start\":55630},{\"end\":55646,\"start\":55641},{\"end\":55661,\"start\":55654},{\"end\":55969,\"start\":55965},{\"end\":55979,\"start\":55976},{\"end\":55990,\"start\":55985},{\"end\":56004,\"start\":55996},{\"end\":56016,\"start\":56011},{\"end\":56196,\"start\":56182},{\"end\":56210,\"start\":56202},{\"end\":56212,\"start\":56211},{\"end\":56228,\"start\":56218},{\"end\":56244,\"start\":56240},{\"end\":56259,\"start\":56255},{\"end\":56496,\"start\":56489},{\"end\":56509,\"start\":56502},{\"end\":56521,\"start\":56516},{\"end\":56531,\"start\":56527},{\"end\":56804,\"start\":56797},{\"end\":56958,\"start\":56957},{\"end\":56974,\"start\":56968},{\"end\":56976,\"start\":56975},{\"end\":57244,\"start\":57238},{\"end\":57254,\"start\":57250},{\"end\":57265,\"start\":57259},{\"end\":57272,\"start\":57271},{\"end\":57274,\"start\":57273},{\"end\":57288,\"start\":57284},{\"end\":57301,\"start\":57294},{\"end\":57306,\"start\":57302},{\"end\":57754,\"start\":57748},{\"end\":57774,\"start\":57765},{\"end\":58161,\"start\":58157},{\"end\":58180,\"start\":58173},{\"end\":58197,\"start\":58190},{\"end\":58217,\"start\":58208},{\"end\":58234,\"start\":58227},{\"end\":58721,\"start\":58718},{\"end\":58735,\"start\":58734},{\"end\":58751,\"start\":58744},{\"end\":58772,\"start\":58764},{\"end\":58774,\"start\":58773},{\"end\":58787,\"start\":58783},{\"end\":58799,\"start\":58796},{\"end\":59080,\"start\":59073},{\"end\":59095,\"start\":59087},{\"end\":59104,\"start\":59103},{\"end\":59106,\"start\":59105},{\"end\":59124,\"start\":59119},{\"end\":59126,\"start\":59125},{\"end\":59376,\"start\":59366},{\"end\":59388,\"start\":59383},{\"end\":59405,\"start\":59399},{\"end\":59421,\"start\":59414},{\"end\":59438,\"start\":59432},{\"end\":59961,\"start\":59953},{\"end\":59982,\"start\":59975},{\"end\":60005,\"start\":59998},{\"end\":60019,\"start\":60014},{\"end\":60459,\"start\":60452},{\"end\":60473,\"start\":60466},{\"end\":60488,\"start\":60484},{\"end\":60504,\"start\":60500},{\"end\":60523,\"start\":60516},{\"end\":60813,\"start\":60806},{\"end\":60828,\"start\":60823},{\"end\":60847,\"start\":60840},{\"end\":60863,\"start\":60857},{\"end\":60879,\"start\":60874},{\"end\":60899,\"start\":60893},{\"end\":60913,\"start\":60907},{\"end\":61480,\"start\":61474},{\"end\":61493,\"start\":61490},{\"end\":61515,\"start\":61508},{\"end\":61981,\"start\":61973},{\"end\":61993,\"start\":61989},{\"end\":62006,\"start\":62001},{\"end\":62022,\"start\":62016},{\"end\":62040,\"start\":62034},{\"end\":62054,\"start\":62051},{\"end\":62478,\"start\":62470},{\"end\":62491,\"start\":62486},{\"end\":62504,\"start\":62499},{\"end\":62521,\"start\":62514},{\"end\":62993,\"start\":62986},{\"end\":63005,\"start\":63004},{\"end\":63007,\"start\":63006},{\"end\":63025,\"start\":63016},{\"end\":63027,\"start\":63026},{\"end\":63041,\"start\":63036},{\"end\":63043,\"start\":63042},{\"end\":63059,\"start\":63053},{\"end\":63373,\"start\":63366},{\"end\":63383,\"start\":63380},{\"end\":63389,\"start\":63388},{\"end\":63410,\"start\":63400},{\"end\":63424,\"start\":63416},{\"end\":63775,\"start\":63769},{\"end\":63790,\"start\":63786},{\"end\":63808,\"start\":63801},{\"end\":63821,\"start\":63814},{\"end\":63836,\"start\":63829},{\"end\":63848,\"start\":63843},{\"end\":63869,\"start\":63865},{\"end\":63886,\"start\":63880},{\"end\":63901,\"start\":63896},{\"end\":64500,\"start\":64499},{\"end\":64515,\"start\":64514},{\"end\":64530,\"start\":64529},{\"end\":64875,\"start\":64868},{\"end\":64889,\"start\":64886},{\"end\":64902,\"start\":64894},{\"end\":64904,\"start\":64903},{\"end\":64919,\"start\":64913},{\"end\":64921,\"start\":64920},{\"end\":64937,\"start\":64929},{\"end\":65267,\"start\":65260},{\"end\":65279,\"start\":65274},{\"end\":65293,\"start\":65287},{\"end\":65304,\"start\":65298},{\"end\":65312,\"start\":65309},{\"end\":65325,\"start\":65318},{\"end\":65759,\"start\":65749},{\"end\":65770,\"start\":65766},{\"end\":65783,\"start\":65776},{\"end\":65796,\"start\":65789},{\"end\":65805,\"start\":65802},{\"end\":66157,\"start\":66147},{\"end\":66168,\"start\":66164},{\"end\":66177,\"start\":66174},{\"end\":66687,\"start\":66680},{\"end\":66705,\"start\":66698},{\"end\":66718,\"start\":66714},{\"end\":66731,\"start\":66726},{\"end\":67236,\"start\":67230},{\"end\":67249,\"start\":67241},{\"end\":67264,\"start\":67257},{\"end\":67271,\"start\":67270},{\"end\":67287,\"start\":67281},{\"end\":67289,\"start\":67288},{\"end\":67679,\"start\":67672},{\"end\":67690,\"start\":67684},{\"end\":67703,\"start\":67697},{\"end\":67718,\"start\":67712},{\"end\":67731,\"start\":67723},{\"end\":67745,\"start\":67739},{\"end\":67761,\"start\":67752},{\"end\":68266,\"start\":68258},{\"end\":68277,\"start\":68271},{\"end\":68289,\"start\":68284},{\"end\":68305,\"start\":68298},{\"end\":68318,\"start\":68312},{\"end\":68688,\"start\":68681},{\"end\":68700,\"start\":68694},{\"end\":68717,\"start\":68707},{\"end\":68736,\"start\":68729},{\"end\":68753,\"start\":68745},{\"end\":68762,\"start\":68759},{\"end\":69045,\"start\":69037},{\"end\":69054,\"start\":69051},{\"end\":69069,\"start\":69059},{\"end\":69088,\"start\":69081},{\"end\":69096,\"start\":69093}]", "bib_author_last_name": "[{\"end\":43520,\"start\":43514},{\"end\":43534,\"start\":43528},{\"end\":43891,\"start\":43886},{\"end\":43905,\"start\":43902},{\"end\":43914,\"start\":43909},{\"end\":43928,\"start\":43921},{\"end\":43936,\"start\":43930},{\"end\":44239,\"start\":44234},{\"end\":44253,\"start\":44248},{\"end\":44274,\"start\":44264},{\"end\":44286,\"start\":44280},{\"end\":44303,\"start\":44295},{\"end\":44315,\"start\":44310},{\"end\":44326,\"start\":44324},{\"end\":44344,\"start\":44336},{\"end\":44358,\"start\":44353},{\"end\":44368,\"start\":44364},{\"end\":44372,\"start\":44370},{\"end\":44744,\"start\":44740},{\"end\":44759,\"start\":44755},{\"end\":44771,\"start\":44767},{\"end\":44786,\"start\":44778},{\"end\":45123,\"start\":45119},{\"end\":45132,\"start\":45129},{\"end\":45143,\"start\":45139},{\"end\":45157,\"start\":45152},{\"end\":45174,\"start\":45166},{\"end\":45189,\"start\":45181},{\"end\":45203,\"start\":45197},{\"end\":45572,\"start\":45568},{\"end\":45588,\"start\":45585},{\"end\":45604,\"start\":45598},{\"end\":45619,\"start\":45611},{\"end\":45630,\"start\":45625},{\"end\":45652,\"start\":45643},{\"end\":46130,\"start\":46126},{\"end\":46151,\"start\":46139},{\"end\":46162,\"start\":46157},{\"end\":46586,\"start\":46582},{\"end\":46597,\"start\":46592},{\"end\":46996,\"start\":46989},{\"end\":47007,\"start\":47004},{\"end\":47025,\"start\":47016},{\"end\":47374,\"start\":47363},{\"end\":47387,\"start\":47383},{\"end\":47400,\"start\":47398},{\"end\":47412,\"start\":47408},{\"end\":47425,\"start\":47421},{\"end\":47435,\"start\":47427},{\"end\":47812,\"start\":47809},{\"end\":47828,\"start\":47821},{\"end\":47844,\"start\":47837},{\"end\":47856,\"start\":47852},{\"end\":47870,\"start\":47865},{\"end\":47888,\"start\":47881},{\"end\":48394,\"start\":48391},{\"end\":48418,\"start\":48416},{\"end\":48436,\"start\":48429},{\"end\":48826,\"start\":48820},{\"end\":48848,\"start\":48834},{\"end\":48863,\"start\":48855},{\"end\":49195,\"start\":49191},{\"end\":49210,\"start\":49206},{\"end\":49220,\"start\":49216},{\"end\":49535,\"start\":49532},{\"end\":49547,\"start\":49542},{\"end\":49860,\"start\":49855},{\"end\":49874,\"start\":49869},{\"end\":49890,\"start\":49883},{\"end\":50090,\"start\":50086},{\"end\":50103,\"start\":50100},{\"end\":50112,\"start\":50108},{\"end\":50119,\"start\":50117},{\"end\":50132,\"start\":50125},{\"end\":50151,\"start\":50145},{\"end\":50458,\"start\":50455},{\"end\":50473,\"start\":50469},{\"end\":50486,\"start\":50481},{\"end\":50506,\"start\":50501},{\"end\":50521,\"start\":50513},{\"end\":50537,\"start\":50530},{\"end\":50551,\"start\":50545},{\"end\":50914,\"start\":50911},{\"end\":50931,\"start\":50927},{\"end\":50962,\"start\":50941},{\"end\":50976,\"start\":50971},{\"end\":50984,\"start\":50982},{\"end\":51003,\"start\":50997},{\"end\":51341,\"start\":51338},{\"end\":51352,\"start\":51348},{\"end\":51366,\"start\":51362},{\"end\":51380,\"start\":51374},{\"end\":51611,\"start\":51608},{\"end\":51621,\"start\":51617},{\"end\":51630,\"start\":51628},{\"end\":51643,\"start\":51639},{\"end\":51654,\"start\":51652},{\"end\":51666,\"start\":51663},{\"end\":51677,\"start\":51672},{\"end\":51945,\"start\":51937},{\"end\":51961,\"start\":51956},{\"end\":51977,\"start\":51970},{\"end\":52337,\"start\":52330},{\"end\":52353,\"start\":52347},{\"end\":52365,\"start\":52357},{\"end\":52370,\"start\":52367},{\"end\":52379,\"start\":52374},{\"end\":52396,\"start\":52389},{\"end\":52403,\"start\":52398},{\"end\":52871,\"start\":52867},{\"end\":52889,\"start\":52881},{\"end\":52905,\"start\":52900},{\"end\":53218,\"start\":53211},{\"end\":53230,\"start\":53224},{\"end\":53243,\"start\":53237},{\"end\":53260,\"start\":53252},{\"end\":53508,\"start\":53505},{\"end\":53529,\"start\":53516},{\"end\":53543,\"start\":53536},{\"end\":53559,\"start\":53551},{\"end\":53973,\"start\":53967},{\"end\":53991,\"start\":53986},{\"end\":54015,\"start\":54007},{\"end\":54035,\"start\":54030},{\"end\":54052,\"start\":54043},{\"end\":54067,\"start\":54063},{\"end\":54080,\"start\":54075},{\"end\":54541,\"start\":54531},{\"end\":54555,\"start\":54552},{\"end\":54571,\"start\":54563},{\"end\":54860,\"start\":54857},{\"end\":54869,\"start\":54866},{\"end\":55192,\"start\":55190},{\"end\":55203,\"start\":55198},{\"end\":55639,\"start\":55635},{\"end\":55652,\"start\":55647},{\"end\":55668,\"start\":55662},{\"end\":55974,\"start\":55970},{\"end\":55983,\"start\":55980},{\"end\":55994,\"start\":55991},{\"end\":56009,\"start\":56005},{\"end\":56023,\"start\":56017},{\"end\":56200,\"start\":56197},{\"end\":56216,\"start\":56213},{\"end\":56238,\"start\":56229},{\"end\":56253,\"start\":56245},{\"end\":56268,\"start\":56260},{\"end\":56500,\"start\":56497},{\"end\":56514,\"start\":56510},{\"end\":56525,\"start\":56522},{\"end\":56535,\"start\":56532},{\"end\":56809,\"start\":56805},{\"end\":56966,\"start\":56959},{\"end\":56985,\"start\":56977},{\"end\":56992,\"start\":56987},{\"end\":57248,\"start\":57245},{\"end\":57257,\"start\":57255},{\"end\":57269,\"start\":57266},{\"end\":57282,\"start\":57275},{\"end\":57292,\"start\":57289},{\"end\":57311,\"start\":57307},{\"end\":57320,\"start\":57313},{\"end\":57763,\"start\":57755},{\"end\":57782,\"start\":57775},{\"end\":58171,\"start\":58162},{\"end\":58188,\"start\":58181},{\"end\":58206,\"start\":58198},{\"end\":58225,\"start\":58218},{\"end\":58241,\"start\":58235},{\"end\":58732,\"start\":58722},{\"end\":58742,\"start\":58736},{\"end\":58762,\"start\":58752},{\"end\":58781,\"start\":58775},{\"end\":58794,\"start\":58788},{\"end\":58811,\"start\":58800},{\"end\":58815,\"start\":58813},{\"end\":59085,\"start\":59081},{\"end\":59101,\"start\":59096},{\"end\":59117,\"start\":59107},{\"end\":59136,\"start\":59127},{\"end\":59381,\"start\":59377},{\"end\":59397,\"start\":59389},{\"end\":59412,\"start\":59406},{\"end\":59430,\"start\":59422},{\"end\":59448,\"start\":59439},{\"end\":59973,\"start\":59962},{\"end\":59996,\"start\":59983},{\"end\":60012,\"start\":60006},{\"end\":60026,\"start\":60020},{\"end\":60464,\"start\":60460},{\"end\":60482,\"start\":60474},{\"end\":60498,\"start\":60489},{\"end\":60514,\"start\":60505},{\"end\":60530,\"start\":60524},{\"end\":60821,\"start\":60814},{\"end\":60838,\"start\":60829},{\"end\":60855,\"start\":60848},{\"end\":60872,\"start\":60864},{\"end\":60891,\"start\":60880},{\"end\":60905,\"start\":60900},{\"end\":60917,\"start\":60914},{\"end\":61488,\"start\":61481},{\"end\":61506,\"start\":61494},{\"end\":61522,\"start\":61516},{\"end\":61987,\"start\":61982},{\"end\":61999,\"start\":61994},{\"end\":62014,\"start\":62007},{\"end\":62032,\"start\":62023},{\"end\":62049,\"start\":62041},{\"end\":62057,\"start\":62055},{\"end\":62484,\"start\":62479},{\"end\":62497,\"start\":62492},{\"end\":62512,\"start\":62505},{\"end\":62525,\"start\":62522},{\"end\":63002,\"start\":62994},{\"end\":63014,\"start\":63008},{\"end\":63034,\"start\":63028},{\"end\":63051,\"start\":63044},{\"end\":63067,\"start\":63060},{\"end\":63078,\"start\":63069},{\"end\":63378,\"start\":63374},{\"end\":63386,\"start\":63384},{\"end\":63398,\"start\":63390},{\"end\":63414,\"start\":63411},{\"end\":63434,\"start\":63425},{\"end\":63442,\"start\":63436},{\"end\":63784,\"start\":63776},{\"end\":63799,\"start\":63791},{\"end\":63812,\"start\":63809},{\"end\":63827,\"start\":63822},{\"end\":63841,\"start\":63837},{\"end\":63863,\"start\":63849},{\"end\":63878,\"start\":63870},{\"end\":63894,\"start\":63887},{\"end\":63908,\"start\":63902},{\"end\":64512,\"start\":64501},{\"end\":64527,\"start\":64516},{\"end\":64535,\"start\":64531},{\"end\":64884,\"start\":64876},{\"end\":64892,\"start\":64890},{\"end\":64911,\"start\":64905},{\"end\":64927,\"start\":64922},{\"end\":64943,\"start\":64938},{\"end\":65272,\"start\":65268},{\"end\":65285,\"start\":65280},{\"end\":65296,\"start\":65294},{\"end\":65307,\"start\":65305},{\"end\":65316,\"start\":65313},{\"end\":65331,\"start\":65326},{\"end\":65764,\"start\":65760},{\"end\":65774,\"start\":65771},{\"end\":65787,\"start\":65784},{\"end\":65800,\"start\":65797},{\"end\":65810,\"start\":65806},{\"end\":66162,\"start\":66158},{\"end\":66172,\"start\":66169},{\"end\":66182,\"start\":66178},{\"end\":66696,\"start\":66688},{\"end\":66712,\"start\":66706},{\"end\":66724,\"start\":66719},{\"end\":66737,\"start\":66732},{\"end\":67239,\"start\":67237},{\"end\":67255,\"start\":67250},{\"end\":67268,\"start\":67265},{\"end\":67279,\"start\":67272},{\"end\":67297,\"start\":67290},{\"end\":67308,\"start\":67299},{\"end\":67682,\"start\":67680},{\"end\":67695,\"start\":67691},{\"end\":67710,\"start\":67704},{\"end\":67721,\"start\":67719},{\"end\":67737,\"start\":67732},{\"end\":67750,\"start\":67746},{\"end\":67766,\"start\":67762},{\"end\":68269,\"start\":68267},{\"end\":68282,\"start\":68278},{\"end\":68296,\"start\":68290},{\"end\":68310,\"start\":68306},{\"end\":68326,\"start\":68319},{\"end\":68692,\"start\":68689},{\"end\":68705,\"start\":68701},{\"end\":68727,\"start\":68718},{\"end\":68743,\"start\":68737},{\"end\":68757,\"start\":68754},{\"end\":68768,\"start\":68763},{\"end\":69049,\"start\":69046},{\"end\":69057,\"start\":69055},{\"end\":69079,\"start\":69070},{\"end\":69091,\"start\":69089},{\"end\":69102,\"start\":69097}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":208267630},\"end\":43877,\"start\":43455},{\"attributes\":{\"doi\":\"arXiv:1608.04236\",\"id\":\"b1\"},\"end\":44230,\"start\":43879},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b2\"},\"end\":44682,\"start\":44232},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":395534},\"end\":45024,\"start\":44684},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":199442423},\"end\":45498,\"start\":45026},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":229212815},\"end\":46051,\"start\":45500},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":208139574},\"end\":46517,\"start\":46053},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":54457478},\"end\":46916,\"start\":46519},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":225076487},\"end\":47279,\"start\":46918},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6325059},\"end\":47717,\"start\":47281},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1063815},\"end\":48312,\"start\":47719},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1003795},\"end\":48811,\"start\":48314},{\"attributes\":{\"doi\":\"arXiv:2009.09808\",\"id\":\"b12\"},\"end\":49107,\"start\":48813},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":227209719},\"end\":49439,\"start\":49109},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":60496605},\"end\":49777,\"start\":49441},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7963213},\"end\":50078,\"start\":49779},{\"attributes\":{\"id\":\"b16\"},\"end\":50387,\"start\":50080},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":226237264},\"end\":50831,\"start\":50389},{\"attributes\":{\"doi\":\"arXiv:1901.03781\",\"id\":\"b18\"},\"end\":51249,\"start\":50833},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":221246412},\"end\":51537,\"start\":51251},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":204746599},\"end\":51915,\"start\":51539},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":174802699},\"end\":52260,\"start\":51917},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3656527},\"end\":52793,\"start\":52262},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10310432},\"end\":53156,\"start\":52795},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":218863093},\"end\":53426,\"start\":53158},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":214802260},\"end\":53957,\"start\":53428},{\"attributes\":{\"doi\":\"arXiv:1911.05063\",\"id\":\"b26\"},\"end\":54453,\"start\":53959},{\"attributes\":{\"doi\":\"arXiv:2002.00349\",\"id\":\"b27\"},\"end\":54763,\"start\":54455},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":211132447},\"end\":55107,\"start\":54765},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":229152508},\"end\":55564,\"start\":55109},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":46900294},\"end\":55912,\"start\":55566},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":81977075},\"end\":56160,\"start\":55914},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":218502682},\"end\":56439,\"start\":56162},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":195833620},\"end\":56747,\"start\":56441},{\"attributes\":{\"id\":\"b34\"},\"end\":56886,\"start\":56749},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":15545924},\"end\":57189,\"start\":56888},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206771128},\"end\":57670,\"start\":57191},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14620252},\"end\":58089,\"start\":57672},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":54465161},\"end\":58644,\"start\":58091},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":213175590},\"end\":59013,\"start\":58646},{\"attributes\":{\"id\":\"b40\"},\"end\":59283,\"start\":59015},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":58007025},\"end\":59862,\"start\":59285},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":232269971},\"end\":60416,\"start\":59864},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":212646575},\"end\":60756,\"start\":60418},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":219530677},\"end\":61410,\"start\":60758},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":206596552},\"end\":61885,\"start\":61412},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":152282359},\"end\":62373,\"start\":61887},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":214743286},\"end\":62916,\"start\":62375},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":219720931},\"end\":63293,\"start\":62918},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":7843274},\"end\":63688,\"start\":63295},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":231709798},\"end\":64399,\"start\":63690},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":60945},\"end\":64801,\"start\":64401},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":2380406},\"end\":65196,\"start\":64803},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":4633214},\"end\":65674,\"start\":65198},{\"attributes\":{\"id\":\"b54\"},\"end\":66051,\"start\":65676},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":219530821},\"end\":66604,\"start\":66053},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":220041556},\"end\":67133,\"start\":66606},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":3248075},\"end\":67611,\"start\":67135},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":206592833},\"end\":68172,\"start\":67613},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":166228177},\"end\":68611,\"start\":68174},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":220969055},\"end\":68973,\"start\":68613},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":52281600},\"end\":69327,\"start\":68975}]", "bib_title": "[{\"end\":43506,\"start\":43455},{\"end\":44729,\"start\":44684},{\"end\":45108,\"start\":45026},{\"end\":45559,\"start\":45500},{\"end\":46117,\"start\":46053},{\"end\":46573,\"start\":46519},{\"end\":46980,\"start\":46918},{\"end\":47359,\"start\":47281},{\"end\":47800,\"start\":47719},{\"end\":48382,\"start\":48314},{\"end\":49186,\"start\":49109},{\"end\":49525,\"start\":49441},{\"end\":49844,\"start\":49779},{\"end\":50449,\"start\":50389},{\"end\":51332,\"start\":51251},{\"end\":51602,\"start\":51539},{\"end\":51927,\"start\":51917},{\"end\":52319,\"start\":52262},{\"end\":52855,\"start\":52795},{\"end\":53204,\"start\":53158},{\"end\":53497,\"start\":53428},{\"end\":54848,\"start\":54765},{\"end\":55182,\"start\":55109},{\"end\":55628,\"start\":55566},{\"end\":55963,\"start\":55914},{\"end\":56180,\"start\":56162},{\"end\":56487,\"start\":56441},{\"end\":56955,\"start\":56888},{\"end\":57236,\"start\":57191},{\"end\":57746,\"start\":57672},{\"end\":58155,\"start\":58091},{\"end\":58716,\"start\":58646},{\"end\":59364,\"start\":59285},{\"end\":59951,\"start\":59864},{\"end\":60450,\"start\":60418},{\"end\":60804,\"start\":60758},{\"end\":61472,\"start\":61412},{\"end\":61971,\"start\":61887},{\"end\":62468,\"start\":62375},{\"end\":62984,\"start\":62918},{\"end\":63364,\"start\":63295},{\"end\":63767,\"start\":63690},{\"end\":64497,\"start\":64401},{\"end\":64866,\"start\":64803},{\"end\":65258,\"start\":65198},{\"end\":65747,\"start\":65676},{\"end\":66145,\"start\":66053},{\"end\":66678,\"start\":66606},{\"end\":67228,\"start\":67135},{\"end\":67670,\"start\":67613},{\"end\":68256,\"start\":68174},{\"end\":68679,\"start\":68613},{\"end\":69035,\"start\":68975}]", "bib_author": "[{\"end\":43522,\"start\":43508},{\"end\":43536,\"start\":43522},{\"end\":43893,\"start\":43879},{\"end\":43907,\"start\":43893},{\"end\":43916,\"start\":43907},{\"end\":43930,\"start\":43916},{\"end\":43938,\"start\":43930},{\"end\":44241,\"start\":44232},{\"end\":44255,\"start\":44241},{\"end\":44276,\"start\":44255},{\"end\":44288,\"start\":44276},{\"end\":44305,\"start\":44288},{\"end\":44317,\"start\":44305},{\"end\":44328,\"start\":44317},{\"end\":44346,\"start\":44328},{\"end\":44360,\"start\":44346},{\"end\":44370,\"start\":44360},{\"end\":44374,\"start\":44370},{\"end\":44746,\"start\":44731},{\"end\":44761,\"start\":44746},{\"end\":44773,\"start\":44761},{\"end\":44788,\"start\":44773},{\"end\":45125,\"start\":45110},{\"end\":45134,\"start\":45125},{\"end\":45145,\"start\":45134},{\"end\":45159,\"start\":45145},{\"end\":45176,\"start\":45159},{\"end\":45191,\"start\":45176},{\"end\":45205,\"start\":45191},{\"end\":45574,\"start\":45561},{\"end\":45590,\"start\":45574},{\"end\":45606,\"start\":45590},{\"end\":45621,\"start\":45606},{\"end\":45632,\"start\":45621},{\"end\":45654,\"start\":45632},{\"end\":46132,\"start\":46119},{\"end\":46153,\"start\":46132},{\"end\":46164,\"start\":46153},{\"end\":46588,\"start\":46575},{\"end\":46599,\"start\":46588},{\"end\":46998,\"start\":46982},{\"end\":47009,\"start\":46998},{\"end\":47027,\"start\":47009},{\"end\":47376,\"start\":47361},{\"end\":47389,\"start\":47376},{\"end\":47402,\"start\":47389},{\"end\":47414,\"start\":47402},{\"end\":47427,\"start\":47414},{\"end\":47437,\"start\":47427},{\"end\":47814,\"start\":47802},{\"end\":47830,\"start\":47814},{\"end\":47846,\"start\":47830},{\"end\":47858,\"start\":47846},{\"end\":47872,\"start\":47858},{\"end\":47890,\"start\":47872},{\"end\":48396,\"start\":48384},{\"end\":48420,\"start\":48396},{\"end\":48438,\"start\":48420},{\"end\":48828,\"start\":48813},{\"end\":48850,\"start\":48828},{\"end\":48865,\"start\":48850},{\"end\":49197,\"start\":49188},{\"end\":49212,\"start\":49197},{\"end\":49222,\"start\":49212},{\"end\":49537,\"start\":49527},{\"end\":49549,\"start\":49537},{\"end\":49862,\"start\":49846},{\"end\":49876,\"start\":49862},{\"end\":49892,\"start\":49876},{\"end\":50092,\"start\":50080},{\"end\":50105,\"start\":50092},{\"end\":50114,\"start\":50105},{\"end\":50121,\"start\":50114},{\"end\":50134,\"start\":50121},{\"end\":50153,\"start\":50134},{\"end\":50460,\"start\":50451},{\"end\":50475,\"start\":50460},{\"end\":50488,\"start\":50475},{\"end\":50508,\"start\":50488},{\"end\":50523,\"start\":50508},{\"end\":50539,\"start\":50523},{\"end\":50553,\"start\":50539},{\"end\":50916,\"start\":50907},{\"end\":50933,\"start\":50916},{\"end\":50964,\"start\":50933},{\"end\":50978,\"start\":50964},{\"end\":50986,\"start\":50978},{\"end\":51005,\"start\":50986},{\"end\":51343,\"start\":51334},{\"end\":51354,\"start\":51343},{\"end\":51368,\"start\":51354},{\"end\":51382,\"start\":51368},{\"end\":51613,\"start\":51604},{\"end\":51623,\"start\":51613},{\"end\":51632,\"start\":51623},{\"end\":51645,\"start\":51632},{\"end\":51656,\"start\":51645},{\"end\":51668,\"start\":51656},{\"end\":51679,\"start\":51668},{\"end\":51947,\"start\":51929},{\"end\":51963,\"start\":51947},{\"end\":51979,\"start\":51963},{\"end\":52339,\"start\":52321},{\"end\":52355,\"start\":52339},{\"end\":52367,\"start\":52355},{\"end\":52372,\"start\":52367},{\"end\":52381,\"start\":52372},{\"end\":52398,\"start\":52381},{\"end\":52405,\"start\":52398},{\"end\":52873,\"start\":52857},{\"end\":52891,\"start\":52873},{\"end\":52907,\"start\":52891},{\"end\":53220,\"start\":53206},{\"end\":53232,\"start\":53220},{\"end\":53245,\"start\":53232},{\"end\":53262,\"start\":53245},{\"end\":53510,\"start\":53499},{\"end\":53531,\"start\":53510},{\"end\":53545,\"start\":53531},{\"end\":53561,\"start\":53545},{\"end\":53975,\"start\":53959},{\"end\":53979,\"start\":53975},{\"end\":53993,\"start\":53979},{\"end\":54017,\"start\":53993},{\"end\":54037,\"start\":54017},{\"end\":54054,\"start\":54037},{\"end\":54069,\"start\":54054},{\"end\":54082,\"start\":54069},{\"end\":54543,\"start\":54524},{\"end\":54557,\"start\":54543},{\"end\":54573,\"start\":54557},{\"end\":54862,\"start\":54850},{\"end\":54871,\"start\":54862},{\"end\":55194,\"start\":55184},{\"end\":55205,\"start\":55194},{\"end\":55641,\"start\":55630},{\"end\":55654,\"start\":55641},{\"end\":55670,\"start\":55654},{\"end\":55976,\"start\":55965},{\"end\":55985,\"start\":55976},{\"end\":55996,\"start\":55985},{\"end\":56011,\"start\":55996},{\"end\":56025,\"start\":56011},{\"end\":56202,\"start\":56182},{\"end\":56218,\"start\":56202},{\"end\":56240,\"start\":56218},{\"end\":56255,\"start\":56240},{\"end\":56270,\"start\":56255},{\"end\":56502,\"start\":56489},{\"end\":56516,\"start\":56502},{\"end\":56527,\"start\":56516},{\"end\":56537,\"start\":56527},{\"end\":56811,\"start\":56797},{\"end\":56968,\"start\":56957},{\"end\":56987,\"start\":56968},{\"end\":56994,\"start\":56987},{\"end\":57250,\"start\":57238},{\"end\":57259,\"start\":57250},{\"end\":57271,\"start\":57259},{\"end\":57284,\"start\":57271},{\"end\":57294,\"start\":57284},{\"end\":57313,\"start\":57294},{\"end\":57322,\"start\":57313},{\"end\":57765,\"start\":57748},{\"end\":57784,\"start\":57765},{\"end\":58173,\"start\":58157},{\"end\":58190,\"start\":58173},{\"end\":58208,\"start\":58190},{\"end\":58227,\"start\":58208},{\"end\":58243,\"start\":58227},{\"end\":58734,\"start\":58718},{\"end\":58744,\"start\":58734},{\"end\":58764,\"start\":58744},{\"end\":58783,\"start\":58764},{\"end\":58796,\"start\":58783},{\"end\":58813,\"start\":58796},{\"end\":58817,\"start\":58813},{\"end\":59087,\"start\":59073},{\"end\":59103,\"start\":59087},{\"end\":59119,\"start\":59103},{\"end\":59138,\"start\":59119},{\"end\":59383,\"start\":59366},{\"end\":59399,\"start\":59383},{\"end\":59414,\"start\":59399},{\"end\":59432,\"start\":59414},{\"end\":59450,\"start\":59432},{\"end\":59975,\"start\":59953},{\"end\":59998,\"start\":59975},{\"end\":60014,\"start\":59998},{\"end\":60028,\"start\":60014},{\"end\":60466,\"start\":60452},{\"end\":60484,\"start\":60466},{\"end\":60500,\"start\":60484},{\"end\":60516,\"start\":60500},{\"end\":60532,\"start\":60516},{\"end\":60823,\"start\":60806},{\"end\":60840,\"start\":60823},{\"end\":60857,\"start\":60840},{\"end\":60874,\"start\":60857},{\"end\":60893,\"start\":60874},{\"end\":60907,\"start\":60893},{\"end\":60919,\"start\":60907},{\"end\":61490,\"start\":61474},{\"end\":61508,\"start\":61490},{\"end\":61524,\"start\":61508},{\"end\":61989,\"start\":61973},{\"end\":62001,\"start\":61989},{\"end\":62016,\"start\":62001},{\"end\":62034,\"start\":62016},{\"end\":62051,\"start\":62034},{\"end\":62059,\"start\":62051},{\"end\":62486,\"start\":62470},{\"end\":62499,\"start\":62486},{\"end\":62514,\"start\":62499},{\"end\":62527,\"start\":62514},{\"end\":63004,\"start\":62986},{\"end\":63016,\"start\":63004},{\"end\":63036,\"start\":63016},{\"end\":63053,\"start\":63036},{\"end\":63069,\"start\":63053},{\"end\":63080,\"start\":63069},{\"end\":63380,\"start\":63366},{\"end\":63388,\"start\":63380},{\"end\":63400,\"start\":63388},{\"end\":63416,\"start\":63400},{\"end\":63436,\"start\":63416},{\"end\":63444,\"start\":63436},{\"end\":63786,\"start\":63769},{\"end\":63801,\"start\":63786},{\"end\":63814,\"start\":63801},{\"end\":63829,\"start\":63814},{\"end\":63843,\"start\":63829},{\"end\":63865,\"start\":63843},{\"end\":63880,\"start\":63865},{\"end\":63896,\"start\":63880},{\"end\":63910,\"start\":63896},{\"end\":64514,\"start\":64499},{\"end\":64529,\"start\":64514},{\"end\":64537,\"start\":64529},{\"end\":64886,\"start\":64868},{\"end\":64894,\"start\":64886},{\"end\":64913,\"start\":64894},{\"end\":64929,\"start\":64913},{\"end\":64945,\"start\":64929},{\"end\":65274,\"start\":65260},{\"end\":65287,\"start\":65274},{\"end\":65298,\"start\":65287},{\"end\":65309,\"start\":65298},{\"end\":65318,\"start\":65309},{\"end\":65333,\"start\":65318},{\"end\":65766,\"start\":65749},{\"end\":65776,\"start\":65766},{\"end\":65789,\"start\":65776},{\"end\":65802,\"start\":65789},{\"end\":65812,\"start\":65802},{\"end\":66164,\"start\":66147},{\"end\":66174,\"start\":66164},{\"end\":66184,\"start\":66174},{\"end\":66698,\"start\":66680},{\"end\":66714,\"start\":66698},{\"end\":66726,\"start\":66714},{\"end\":66739,\"start\":66726},{\"end\":67241,\"start\":67230},{\"end\":67257,\"start\":67241},{\"end\":67270,\"start\":67257},{\"end\":67281,\"start\":67270},{\"end\":67299,\"start\":67281},{\"end\":67310,\"start\":67299},{\"end\":67684,\"start\":67672},{\"end\":67697,\"start\":67684},{\"end\":67712,\"start\":67697},{\"end\":67723,\"start\":67712},{\"end\":67739,\"start\":67723},{\"end\":67752,\"start\":67739},{\"end\":67768,\"start\":67752},{\"end\":68271,\"start\":68258},{\"end\":68284,\"start\":68271},{\"end\":68298,\"start\":68284},{\"end\":68312,\"start\":68298},{\"end\":68328,\"start\":68312},{\"end\":68694,\"start\":68681},{\"end\":68707,\"start\":68694},{\"end\":68729,\"start\":68707},{\"end\":68745,\"start\":68729},{\"end\":68759,\"start\":68745},{\"end\":68770,\"start\":68759},{\"end\":69051,\"start\":69037},{\"end\":69059,\"start\":69051},{\"end\":69081,\"start\":69059},{\"end\":69093,\"start\":69081},{\"end\":69104,\"start\":69093}]", "bib_venue": "[{\"end\":43617,\"start\":43536},{\"end\":44033,\"start\":43954},{\"end\":44429,\"start\":44390},{\"end\":44811,\"start\":44788},{\"end\":45254,\"start\":45205},{\"end\":45734,\"start\":45654},{\"end\":46244,\"start\":46164},{\"end\":46679,\"start\":46599},{\"end\":47086,\"start\":47027},{\"end\":47475,\"start\":47437},{\"end\":47967,\"start\":47890},{\"end\":48515,\"start\":48438},{\"end\":48938,\"start\":48881},{\"end\":49266,\"start\":49222},{\"end\":49594,\"start\":49549},{\"end\":49915,\"start\":49892},{\"end\":50191,\"start\":50153},{\"end\":50602,\"start\":50553},{\"end\":50905,\"start\":50833},{\"end\":51386,\"start\":51382},{\"end\":51713,\"start\":51679},{\"end\":52046,\"start\":51979},{\"end\":52482,\"start\":52405},{\"end\":52955,\"start\":52907},{\"end\":53278,\"start\":53262},{\"end\":53642,\"start\":53561},{\"end\":54200,\"start\":54098},{\"end\":54522,\"start\":54455},{\"end\":54915,\"start\":54871},{\"end\":55286,\"start\":55205},{\"end\":55730,\"start\":55670},{\"end\":56029,\"start\":56025},{\"end\":56286,\"start\":56270},{\"end\":56586,\"start\":56537},{\"end\":56795,\"start\":56749},{\"end\":57024,\"start\":56994},{\"end\":57389,\"start\":57322},{\"end\":57858,\"start\":57784},{\"end\":58320,\"start\":58243},{\"end\":58821,\"start\":58817},{\"end\":59071,\"start\":59015},{\"end\":59527,\"start\":59450},{\"end\":60100,\"start\":60028},{\"end\":60577,\"start\":60532},{\"end\":60968,\"start\":60919},{\"end\":61601,\"start\":61524},{\"end\":62118,\"start\":62059},{\"end\":62604,\"start\":62527},{\"end\":63093,\"start\":63080},{\"end\":63478,\"start\":63444},{\"end\":63998,\"start\":63910},{\"end\":64591,\"start\":64537},{\"end\":64990,\"start\":64945},{\"end\":65397,\"start\":65333},{\"end\":65851,\"start\":65812},{\"end\":66275,\"start\":66184},{\"end\":66820,\"start\":66739},{\"end\":67359,\"start\":67310},{\"end\":67845,\"start\":67768},{\"end\":68377,\"start\":68328},{\"end\":68782,\"start\":68770},{\"end\":69132,\"start\":69104},{\"end\":43685,\"start\":43619},{\"end\":45801,\"start\":45736},{\"end\":46311,\"start\":46246},{\"end\":46746,\"start\":46681},{\"end\":48031,\"start\":47969},{\"end\":48579,\"start\":48517},{\"end\":52100,\"start\":52048},{\"end\":52546,\"start\":52484},{\"end\":53710,\"start\":53644},{\"end\":55354,\"start\":55288},{\"end\":57443,\"start\":57391},{\"end\":58384,\"start\":58322},{\"end\":59591,\"start\":59529},{\"end\":60162,\"start\":60102},{\"end\":61665,\"start\":61603},{\"end\":62668,\"start\":62606},{\"end\":63102,\"start\":63095},{\"end\":64073,\"start\":64000},{\"end\":65448,\"start\":65399},{\"end\":66353,\"start\":66277},{\"end\":66888,\"start\":66822},{\"end\":67909,\"start\":67847},{\"end\":68790,\"start\":68784}]"}}}, "year": 2023, "month": 12, "day": 17}