{"id": 253157749, "updated": "2023-10-05 09:11:25.04", "metadata": {"title": "arXivEdits: Understanding the Human Revision Process in Scientific Writing", "authors": "[{\"first\":\"Chao\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Samuel\",\"last\":\"Stevens\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Scientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture. In this work, we provide a complete computational framework for studying text revision in scientific writing. We first introduce arXivEdits, a new annotated corpus of 751 full papers from arXiv with gold sentence alignment across their multiple versions of revision, as well as fine-grained span-level edits and their underlying intentions for 1,000 sentence pairs. It supports our data-driven analysis to unveil the common strategies practiced by researchers for revising their papers. To scale up the analysis, we also develop automatic methods to extract revision at document-, sentence-, and word-levels. A neural CRF sentence alignment model trained on our corpus achieves 93.8 F1, enabling the reliable matching of sentences between different versions. We formulate the edit extraction task as a span alignment problem, and our proposed method extracts more fine-grained and explainable edits, compared to the commonly used diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1 on the fine-grained intent classification task. Our data and system are released at tiny.one/arxivedits.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2210.15067", "mag": null, "acl": "2022.emnlp-main.641", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/JiangXS22", "doi": "10.18653/v1/2022.emnlp-main.641"}}, "content": {"source": {"pdf_hash": "857f6735c9d70ecec3bad5273f39252b065657cf", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.emnlp-main.641.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2b7f621cdc589f11f63b4d8c82168dd92f0d1e18", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/857f6735c9d70ecec3bad5273f39252b065657cf.txt", "contents": "\nARXIVEDITS: Understanding the Human Revision Process in Scientific Writing\nDecember 7-11, 2022\n\nChao Jiang chaojiang@gatech.eduwei.xu@cc.gatech.edustevens.994@osu.edu \nSchool of Interactive Computing\nGeorgia Institute of Technology\n\n\nWei Xu \nSchool of Interactive Computing\nGeorgia Institute of Technology\n\n\nSamuel Stevens \nDepartment of Computer Science and Engineering\nOhio State University\n\n\nARXIVEDITS: Understanding the Human Revision Process in Scientific Writing\n\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\nthe 2022 Conference on Empirical Methods in Natural Language ProcessingDecember 7-11, 2022\nScientific publications are the primary means to communicate research discoveries, where the writing quality is of crucial importance. However, prior work studying the human editing process in this domain mainly focused on the abstract or introduction sections, resulting in an incomplete picture. In this work, we provide a complete computational framework for studying text revision in scientific writing. We first introduce ARXIVEDITS, a new annotated corpus of 751 full papers from arXiv with gold sentence alignment across their multiple versions of revision, as well as fine-grained span-level edits and their underlying intentions for 1,000 sentence pairs. It supports our datadriven analysis to unveil the common strategies practiced by researchers for revising their papers. To scale up the analysis, we also develop automatic methods to extract revision at document-, sentence-, and word-levels. A neural CRF sentence alignment model trained on our corpus achieves 93.8 F1, enabling the reliable matching of sentences between different versions. We formulate the edit extraction task as a span alignment problem, and our proposed method extracts more fine-grained and explainable edits, compared to the commonly used diff algorithm. An intention classifier trained on our dataset achieves 78.9 F1 on the finegrained intent classification task. Our data and system are released at tiny.one/arxivedits.\n\nIntroduction\n\nWriting is essential for sharing scientific findings. Researchers devote a huge amount of effort to revising their papers by improving the writing quality or updating new discoveries. Valuable knowledge is encoded in this revision process. Up to January 1st, 2022, arXiv (https://arxiv.org/), an open access eprint service, has archived over 1.9 million papers, among which more than 600k papers have multiple versions available. This provides an amazing data * Work done as an undergraduate student. source for studying text revision in scientific writing. Specifically, revisions between different versions of papers contain valuable information about logical and structural improvements at documentlevel, as well as stylistic and grammatical refinements at sentence-and word-levels. It also can support various natural language processing (NLP) applications, including writing quality assessment and error correction (Louis and Nenkova, 2013;Xue and Hwa, 2014;Daudaravicius et al., 2016;Bryant et al., 2019), text simplification and compression (Xu et al., 2015;Filippova et al., 2015), style transfer (Xu et al., 2012;Krishna et al., 2020), hedge detection (Medlock and Briscoe, 2007), and paraphrase generation (Dou et al., 2022).\n\nIn this paper, we present a complete solution for studying the human revision process in the scientific writing domain, including annotated data, analysis, and system. We first construct ARXIVEDITS, which consists of 751 full arXiv papers with gold sentence alignment across their multiple versions of revisions, as shown in Figure 1. Our corpus spans 6 research areas, including physics, mathematics, computer science, quantitative biology, quantitative finance, and statistics, published in 23 years (from 1996 to 2019). To the best of our knowledge, this is the first text revision corpus that covers full multi-page research papers. To study sentence-level revision, we manually annotated fine-grained edits and their underlying intentions that reflect why the edits are being made for 1,000 sentence pairs, based on a taxonomy that we developed consisting of 7 categories.\n\nOur dataset addresses two major limitations in prior work. First, previous researchers mainly focus on the abstract (G\u00e1bor et al., 2018;Kang et al., 2018;Du et al., 2022) and introduction (Tan and Lee, 2014;Mita et al., 2022) sections, limiting the generalizability of their conclusions. In addition, a sentence-level revision may consist of multiple fine-grained edits made for different purposes (see\n\n\nA Paragraph in Early Draft\n\n\nThe Paragraph in Final Version\n\n\nRevise\n\n\nSplit & Revise\n\nt1 Energy markets are driven by innovation, path-dependent technology choices and diffusion. t2 However, conventional optimisation models lack detail on these aspects and have limited ability to address the effectiveness of policy interventions because they do not represent decision-making. t3 As a result, known effects of technology lock-ins are liable to be underestimated. t4 In contrast, our approach places investor decision-making \u2026 Deletion Insertion t1 t2 t3 t4 s1 Energy markets are driven by innovation, path-dependent technology costs and diffusion; yet, common optimisation modelling methodologies remain vague on these aspects and have a limited ability to address the effectiveness of policy onto decision-making since the latter is not specifically represented. s2 This leads to an underestimation of noncost-optimal technology lock-ins known to occur. s3 Breaking with tradition, our approach explores bottom-up \u2026 s3 s2 s1 Operation Cost-optimisation technology models , correspondence with [CITATION] , is the most powerful for finding with outstanding detail lowest cost future technology pathways.\n\nCost-optimisation technology models , corresponding to [CITATION] , are still the most powerful tools for finding detailed , lowest-cost future technology pathways that reaches particular objectives in normative mode . an example in Figure 1). Whereas previous work either concentrates on the change of a single word or phrase (Faruqui et al., 2018;Pryzant et al., 2020) or extracts edits using the diff algorithm (Myers, 1986), which is based on minimizing the edit distance regardless of semantic meaning. As a result, the extracted edits are coarse-grained, and the intentions annotated on top of them can be ambiguous. Enabled by our high-quality annotated corpus, we perform a series of data-driven studies to answer: what common strategies are used by authors to improve the writing of their papers? We also provide a pipeline system with 3 modules to automatically extract and analyze revisions at all levels.\n\n\nDocument-level Revision\n\n(1) A neural sentence alignment model trained on our data achieves 93.8 F1. It can be reliably used to extract parallel corpus for text-to-text generation tasks.\n\n(2) Within a revised sentence pair, the edit extraction is formulated as a span alignment task, and our method can extract more fine-grained and explainable edits compared to the diff algorithm.\n\n(3) An intention classifier trained on our corpus achieves 78.9 F1 on the fine-grained classification task, enabling us to scale up the analysis by automatically extracting and classifying span-level edits from the unlabeled revision data. We hope our work will inspire other researchers to further study the task of text revision in academic writing.\n\n\nConstructing ARXIVEDITS Corpus\n\nIn this section, we present the detailed procedure for constructing the ARXIVEDITS corpus. After posting preprints on arXiv, researchers can continu-ally update the submission, and that constitutes the revisions. More specifically, a revision denotes two adjacent versions of the same paper. 1 An article group refers to all versions of a paper on arXiv (e.g., v1, v2, v3, v4). In this work, we refer to the changes applied to tokens or phrases within one sentence as sentence-level revision. The document-level revision refers to the change of an entire or several sentences, and the changes to the paragraphs can be derived from sentences. Table  1 presents the statistics of document-level revision in our corpus. After constructing this manually annotated corpus, we use it to train the 3 modules in our automatic system as detailed at $4.\n\n\nData Collection and Preprocessing\n\nWe first collect metadata for all 1.6 million papers posted on arXiv between March 1996 and December 2019. We then randomly select 1,000 article groups from the 600k papers that have more than one versions available. To extract plain text from the LaTeX source code of these papers, we improved the open-source OpenDetex 2 package to better handle macros, user-defined commands, and additional LaTeX files imported by the input commands in the main file. 3 We find this method is less error-prone for extracting plain text, compared to using other libraries such as Pandoc 4 used in (Cohan et al., 2018;Roush and Balaji, 2020). Among the randomly selected 1,000 article groups, we obtained plain texts for 751 complete groups, with a total of 1,790 versions of papers, that came with the original LaTex source code and contained text content that was understandable without an overwhelming number of math equations. A breakdown of the filtered groups is provided in Appendix A.\n\n\nParagraph and Sentence Alignment\n\nSentence alignment can capture all document-level revision operations, including the insertion, deletion, rephrasing, splitting, merging, and reordering of sentences and paragraphs (see Figure 1 for an example). Therefore, we propose the following 2-step annotation method to manually align sentences for papers in the 1,039 adjacent version pairs (e.g., v0-v1, v1-v2) from the 751 selected article groups, and the alignments between non-adjacent version pairs (e.g., v0-v2) then can be derived automatically.\n\n1. Align paragraphs using a light-weighted alignment algorithm that we designed based on Jaccard similarity (Jaccard, 1912) (more details in Appendix B). It can cover 92.1% of nonidentical aligned sentence pairs, based on a pilot study on 18 article pairs. Aligning paragraphs first significantly reduces the number of sentence pairs that need to be annotated. 2. Collect annotation of sentence alignment for every possible pair of sentences in the aligned paragraphs using Figure-Eight 5 , a crowdsourcing platform. We ask 5 annotators to classify each pair into one of the following categories: aligned, partially-aligned, or not-aligned. Annotators are required to spend at least 25 seconds on each question. The annotation instructions and interface can be found in Appendix D. We embed one hidden test question in every five questions, and the workers need to maintain an accuracy over 85% on the test questions to continue working on the task.\n\nWe skip aligning 4.6% sentences that contain too few words or too many special tokens. They are still retained in the dataset for completeness, and are marked with a special token. More details about the annotation process are in Appendix A and B. In total, we spent $3,776 to annotate 13,008 sentence pairs from 751 article groups, with a 526/75/150   , 2008). To verify the crowd-sourcing annotation quality, an in-house annotator manually aligns sentences for 10 randomly sampled groups with 14 article pairs. If assuming the in-house annotation is gold, the majority vote of crowd-sourcing annotation achieves an F1 of 94.2 on these 10 paper groups.\n\n\nFine-grained Edits with Varied Intentions\n\nSentence-level revision involves the insertion, deletion, substitution, and reordering of words and phrases. Multiple edits may be tangled together in one sentence, while each edit is made for different purposes (see an example in Figure 1). Correctly detecting and classifying these edits is a challenging problem. We first introduce the formal definition of edits and our proposed intention taxonomy, followed by the annotation procedure.\n\nDefinition of Span-level Edits. A sentence-level revision R consists of the original sentence s, target sentence t, and a series of fine-grained edits e i . Each edit e i is defined as a tuple (s a:b , t c:d , I), indicating span [s a , s a+1 , ..., s b ] in the original sentence is transformed into span [t c , t c+1 , ..., t d ] in the target sentence, with an intention label I \u2208 I (defined in Similarly to what we did in Figure Fig. [ , 2017). The taxonomy is improved for several rounds based on the feedback from four NLP researchers and two in-house annotators with linguistic background.\n\nAnnotating Edits. In pilot study, we found that directly annotating fine-grained edits is a tedious and complicated task for annotators, as it requires separating and matching edited spans across two sentences. To assist the annotators, we use monolingual word alignment (Lan et al., 2021), which can find the correspondences between words and phrases with a similar meaning in two sentences, as an intermediate step to reduce the cognitive load during annotation. We find that, compared to strict word-to-word matching, edits usually have larger granularity and may cross linguistic boundaries. For example, in Figure 1, \"corresponding to\" and \"correspondence with\" should be treated as a whole to be meaningful and labeled an intention. Therefore, the edits can be annotated by adjusting the boundaries of the span alignment. We propose the following 2-step method that leverages word alignment to assist the annotation of edits:\n\n1. Collect word alignment annotation by asking in-house annotators to manually correct the automatic word alignment generated by the neural semi-CRF word alignment model (Lan et al., 2021). The aligner is trained on the MTRef dataset and achieves state-of-the-art performance on the monolingual word alignment task with 92.4 F1. 2. Annotate edits by having in-house annotators inspect and correct the fine-grained edits that are extracted from word alignment using simple heuristics. The heuristics are detailed in $4.1. Two principles are followed during the correction:\n\n(1) Each edit should have a clear intention and relatively clear phrase boundaries;\n\n(2) Span pairs in substitution should be semantically related, otherwise should be treated as separated insertion and deletion.\n\nWe manually annotate insertion, deletion, substitution, and derive reordering automatically, since it can be reliably found by heuristics. Due to the slight variance in granularity, it is possible that more than one answer is acceptable. Therefore, we include all the alternative edits for sentence pairs in the dev and test sets in our annotation, among which 16% have more than one answer.\n\nOverall, we found that our method can annotate more accurate and fine-grained edits compared to prior work that uses the diff algorithm. The diff method is based on minimizing the edit distance regardless of semantic meaning. Therefore, the extracted edits are coarse-grained and may contain many errors (detailed in Table 3).\n\nAnnotating Intention. As intentions can differ subtly, correctly identifying them is a challenging  task. Therefore, instead of crowdsourcing, we hire two experienced in-house annotators to annotate the intention for 2,122 edits in 1,000 sentence revisions. A two-hour training session is provided to both annotators, during which they are asked to annotate 100 sentence pairs and discuss until consensus. The inter-annotator agreement is 0.67 measured by Cohen Kappa (Artstein and Poesio, 2008), and 0.81 if collapsing the Improve Language category. The 1,000 sentence pairs are split into 600/200/200 for train/dev/test sets in experiments.\n\n\nAnalysis of Document-level Revisions\n\nAs a distinct style, scientific writing needs to be clear, succinct, precise, and logical. To understand what common strategies are used by authors to improve the writing of their papers, we present a data-driven study on document-level revisions in the scientific writing domain. This is enabled by our high-quality manually annotated corpus that consists of 1,790 versions of 751 full papers across 6 research areas in 23 years. abling the study of iterative revisions. Figure 3 plots the length of the life cycle for each paper in our corpus, demonstrating a long-tail distribution.\n\n\nDistribution of Subjects and Versions\n\n\nAnalysis of the Overall Update Ratio\n\nWe first investigate, in general, how much content is being updated for each paper during its life cycle, which can potentially affect the type of revisions contained therein. We define the Update Ratio as 1 minus the percentage of sentences being kept between two versions, which is derived from manually annotated sentence alignment. Figure 4(a) presents how much content is being updated for each paper between its first and last versions. For papers that have two versions available, the distribution is heavily skewed towards the left end. The median update ratio is 19.0%, meaning that most papers have a mild revision. Whereas the distribution is much flatter for papers with multiple versions, indicating they are more likely to have a major revision in the life cycle. Interestingly, a peak appears at the tail of the distribution, which means 3.7% of the papers are almost completely rewritten. However, as shown in Figure 4(b), both types of papers have a similar distribution of update ratio for revisions between adjacent versions. Research Areas. We hypothesize researchers in different areas may have different practices for revising their papers. Figure 4 visualizes the distribution of update ratio for papers on different subjects. Researchers in Statistics make more significant revisions to their papers compared to the CS area.\n\nTime Interval. Intuitively, the time interval between submissions may correlate with the overall update ratio. We calculate the Pearson's correlation between the update ratio and the time spent on the revision, which is measured by the difference in timestamps between adjacent submissions. The correlation values are 0.577 and 0.419 for papers that have two versions and multiple versions available, and both correlations are significant. Figure 6 visualizes the relationship. Researchers make quick submissions for small adjustments while spending more time on major revisions.\n\n\nAnalysis of the Updated Sentences\n\nWe explore the dynamic of document-level edit operations to answer: where will and how researchers update the sentences in their papers? The relative positions of the inserted, deleted, and revised sentences are visualized in Figure 7. Researchers, in general, revise more sentences at the beginning of a paper, while the insertion and deletion of sentences occur more in the latter parts. This makes  sense because the abstract and introduction sections are usually frequently revised by the authors, since they are among the most important sections. As shown in Figure 8, revised sentences take the majority when update ratio is low. As more content is being modified, the insertion and deletion of sentences will become more dominant, which is likely to correspond to the major updates on the main body of papers.\n\n\nAnalysis of the Edit Intention\n\nTo understand why the researchers revised the sentences, we run our span-level edit extraction and intention classification system (details in \u00a74) on all the revised sentences between adjacent versions in 751 article groups. The distribution of the intentions is visualized in Figure 9. Most of the language-related edits occur at the beginning of a paper. The aggregation is gradually reduced for grammar/typo-and content-related edits. The adjustments to format (punctuations, figures, tables, citations, etc.) span throughout the whole paper.    Table 3: Performance of different edit extraction methods on the ARXIVEDITS testset. The Len. is measured by the number of tokens. We report performance on all sentence revisions, and on sentence pairs with \u2264 5 edits, which takes 92.5% of test data. The best and second best scores in each column are highlighted by bold and underline.\n\n\nAutomatic Edit Extraction and Intention Identification\n\nAs manual annotation is costly and timeconsuming, we develop a pipeline system to automatically analyze the revisions at scale. Our system consists of sentence alignment, edit extraction, and intention classification modules, which are trained and evaluated on our annotated data. The methods and evaluation results of each step are detailed below. Example outputs from our system are presented in Figure 11.\n\n\nEdits Extraction via Span Alignment\n\nPrior work relies on diff algorithm to extract edits, which is based on string matching regardless of semantic meaning. To extract more fine-grained and explainable edits, we formulate the edit extraction as a span alignment problem. Given the original and revised sentences, the fine-grained edits are derived from span alignment using simple heuristics.\n\nOur Method. We finetune two state-of-the-art word alignment models: neural semi-CRF model (Lan et al., 2021) and QA-Aligner (Nagata et al., 2020) on our ARXIVEDITS corpus, after train them on the MTRef dataset (Lan et al., 2021) first. Although sourced from the news domain, we find finetuning the models on MTRef, which is the largest monolingual word alignment corpus, helps to improve 4 points on the F1 score. When fine-tuning on ARXIVEDITS, the annotated edits are used as training labels, where substitutions are formulated as span alignment, insertions and deletions are the unaligned tokens, and the rest words will be aligned to their identical counterparts. When running inference, the output edits are derived from span alignment using simple heuristics, where the insertions and deletions are unaligned tokens in the revised and original sentences, re- spectively. Substitutions are the non-identical span alignments. A simple post-processing step is applied to strip the identical words at the beginning and end of the substituted span pairs. To enable more flexible granularity, we also design slightly more complex heuristics to extract edits by leveraging compositional span alignment. As shown in Figure 10, for each aligned word in two sentences, we iteratively traverse their parent nodes in two constituency parsing trees (Joshi et al., 2018) for max-level times to find the lowest ancestors in two trees that can resolve all the involved word alignment without conflict. Instead of separated word-to-word replacements, the two spans will be treated as a whole in the substitution. The max-level is a hyperparameter and can be adjusted to control the granularity of the extracted edits.\n\nBaseline Diff (Myers, 1986) algorithm have been widely used in prior work to extract edits from text revision (Yang et al., 2017;Du et al., 2022). It is an unsupervised method based on dynamic programming for finding the longest common subsequences between two strings. Insertions and deletions are derived from unmatched tokens. Substitutions are derived from adjacent insertion and deletion pairs. Diff algorithm has many implementations with different heuristics for post-processing. We compare against its implementation in the latexdiff package, which is used in a recent work (Du et al., 2022).\n\n\nResults\n\nWe report precision, recall, F1, and exact match (EM) for edit extraction. Table 2 presents the results on ARXIVEDITS testset. We report the performance on all 200 sentence pairs in the test set, and on a subset of sentence pairs with \u2264 5 edits, which take 92.5% of the test data and are more common in real applications. Using simple heuristics, both models finetuned on our dataset outperform the baseline method by more than 10 points in F1 and EM. In addition, enabling compositional span alignment by leveraging the constituency parsing tree can increase the granularity of the extracted edits, as shown in the \"Len. of Edits\" column. For the latexdiff method, about 59.3% of extracted edits are span substitutions, with an average length of 4.73 tokens. This is because the diff method derives edits by minimizing the edit distance. Combining with the post-processing heuristics, latexdiff treats everything as large chunk substitutions regardless of their semantic similarity.\n\n\nIntention Classification\n\nGiven an edit and the original/revised sentences, the goal here is to classify its edit intention. We formulate it in a way that is similar to the relation extraction task. We experiment with two competitive models: T5 (Raffel et al., 2020) and PURE (Zhong and Chen, 2021). The input is the concatenation of two sentences, where the edited spans are surrounded by special markers with the type (ins./del./subst.). The PURE model predicts the intention by classification, and the T5 model will generate the intention string.\n\nResults. Table 4 shows the results for both finegrained and coarse-grained (collapsing the Improve Language category) classification experiments. Collapsing labels helps to improve the performance in the 4-way classification task, where   a T5-large model achieves an accuracy of 84.4. Though it's challenging to pick up the differences between 7 types of intentions, the T5-large model trained with fine-grained labels achieves an accuracy of 79.3. The per-category performance of the best-performing T5 model is presented in Table  5. It performs well in separating top-layer categories. Within the Improve Language type, it also achieves reasonable performance on Accurate and Simplify categories, while fall short on Style, which is likely due to the inherited difficulty in identifying language style.\n\n\nSentence Alignment\n\nAccurate sentence alignment is crucial for reliably tracking document-level revisions. Prior work mainly relies on surface-level similarity metrics, such as BLEU score (Faruqui et al., 2018;Faltings et al., 2021) or Jaccard coefficients (Xu et al., 2015), combined with greedy or dynamic programming algorithms to match sentences. Instead, we finetune a supervised neural CRF alignment model on our annotated dataset. The neural CRF aligner is shown to achieve better performance at aligning sentences from articles with different readability levels in the Newsela Corpus (Jiang et al., 2020).\n\nOur Methods. We first align paragraphs using the light-weighted paragraph alignment algorithm we designed (more details in Appendix B). Then, for each aligned paragraph pair, we apply our trained neural CRF alignment model to align sentences from both the old to new version and the reversed directions. The outputs from both directions are merged by intersection.\n\n\nResults\n\nWe report precision, recall, and F1 on the binary classification task of aligned + partiallyaligned vs. not-aligned.  Figure 11: Two example outputs from our Semi-CRF Aligner simple system. The intentions are predicted by our best-performing T5 model.\n\n\nMethods Precision Recall F1\n\nChar. 3-gram (\u0160tajner et al.) 87.7 87.7 87.7 TF-IDF (Paetzold et al.) 90.3 91.6 90.9 Jaccard (Xu et al.) 90  to classify and will inflate the performance. For the similarity-based method, we tune a threshold based on the maximal F1 on the devset. By training the state-of-the-art neural CRF sentence aligner on our dataset and merging the output from both directions, we are able to achieve 93.8 F1, outperforming other methods by a large margin. It is worth noticing that the precision of our model is particularly high, indicating that it can be reliably used to extract high-quality aligned sentence pairs, which can be used as the training corpus for downstream text-to-text generation tasks.\n\n\nRelated Work\n\nText Revision in Scientific Writing. As a clear and concise style of writing, various aspects of scientific writing has been studied in previous work, including style (Bergsma et al., 2012), quality (Louis and Nenkova, 2013), hedge (Medlock and Briscoe, 2007), paraphrase (Dong et al., 2021), statement strength (Tan and Lee, 2014), and grammar error correction (Daudaravicius et al., 2016). Prior work studying scientific writing mainly focuses on the abstract and introduction sections (Tan and Lee, 2014;Du et al., 2022;Mita et al., 2022). In comparison, we develop methods to annotate and automatically analyze full research papers. Our work mainly focuses on the writing quality aspect.\n\nEdit and Edit Intention. Previous work in studying the human editing process (Faruqui et al., 2018;Pryzant et al., 2020) mainly focuses on the change of a single word or phrase, as it is hard to pair complex edits in both sentences. Our work is able to extract more fine-grained and interpretable edits by leveraging span alignment. Several prior work utilizes the intention to categorize edits and as a clue to understanding the purpose of the revision. Some of their intention taxonomies focus on a specific domain, such as Wikipedia (Yang et al., 2017;Anthonio et al., 2020) and argumentative essay (Zhang et al., 2017;Kashefi et al., 2022). The intention taxonomy in our work is built on top of prior literature, with an adaptation to the scientific writing domain.\n\n\nConclusion\n\nIn this paper, we present a comprehensive study that investigates the human revision process in the scientific writing domain. We first introduce ARX-IVEDITS, a new annotated corpus of 751 full arXiv papers with gold sentence alignment across their multiple versions of revisions, and fine-grained span-level edits together with their underlying intents for 1,000 sentence pairs. Based on this highquality annotated corpus, we perform a series of data-driven studies to analyze the common strategies used by the researchers to improve the writing of their papers. In addition, we develop automatic methods to analyze revision at document-, sentence-, and word-levels. Our annotated dataset, analysis, and automatic system together provide a complete solution for studying text revision in the scientific writing domain.\n\n\nLimitations\n\nDue to the user groups of arXiv, our corpus mainly covers research papers in the field of science and engineering, while doesn't contain articles from other areas, such as philosophy and arts. In addition, future work could investigate research papers that are written in non-English languages.\n\n\nAcknowledgement\n\nWe thank three anonymous reviewers for their helpful feedback on this work. We also thank Andrew Duffy and Manish Jois for their help with annotations and human evaluation. This research is supported in part by the NSF awards IIS-2144493 and IIS-2055699. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of NSF or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. \n\n\nReferences\n\n\nA Details of Preprocessing\n\nWe randomly sample 1,000 paper IDs from arXiv that have multiple versions available and download their LaTeX source code for all the versions. During the pre-processing process, we aim to keep each article group complete. About 105 versions of papers don't have source code available. After removing them, 959 groups are complete. There are 4 groups of papers using the harvmac package, which will introduce detex errors; after removing them, 955 groups are left. We then remove 162 groups of math-heavy papers and 42 groups of extremely short papers. After the filtering process, 751 complete groups of papers are left. Among 335k sentences from all versions in the 751 article groups, we skip aligning 4.6% sentences that contain too few words or too many special tokens. Most of the 4.6% skipped content is (a) unusually short (<=3 tokens) and math-heavy text (>=60% special tokens) in math papers or (b) occasional de-tex errors (>1000 char). They don't contain much natural language content that can be analyzed or leveraged. In addition, annotators reported that such text increases the difficulty of aligning sentences. The criteria are detailed below.\n\nWe will skip aligning a paragraph if it meets one of the following criteria:\n\n\u2022 Contains < 10 tokens.\n\n\u2022 Contains > 30% special tokens (inline/block math, citation, and references).\n\nWe will skip aligning a sentence if it meets one of the following criteria:\n\n\u2022 Contains > than 1,000 characters. \u2022 Contains \u2264 than 3 tokens.\n\n\u2022 Contains > 60% special tokens (inline/block math, citation, and references). \u2022 Contains \u2265 70% English characters. \u2022 Ends with \",\" or \":\".\n\nWe also detect citations, references, inline math symbols, block equations, and present them as special markers which are easier to read for the annotators.\n\n\nB Details of Paragraph Alignment and the Annotation Process\n\nWe design a light-weighted automatic paragraph alignment algorithm based on Jaccard similarity, which can cover 92.1% of non-identical sentence alignment in the pilot study. The algorithm is shown in Algorithm 1. The lengths of the two documents are represented by k and l. d denotes the difference of relative position for two paragraphs with indices of i and j, where d(i, j) = | i k \u2212 j l |. The hyperparameters \u03c4 1 = 0.28, \u03c4 2 = 0.15 , \u03c4 3 = 0.85, \u03c4 4 = 0.2 are tuned on the devset.\n\nAt a high level, given an article pair, we first calculate the pairwise similarities for all possible paragraph pairs using the first block of the algorithm. Paragraph pairs are aligned by the second and third blocks of the algorithm if their similarity and relative distance reach certain thresholds.\n\nTo improve the money efficiency when annotating sentence alignment, we design a hybrid method to collect alignment labels for each sentence pair in the aligned paragraph pairs. We found that sentence pairs with Jaccard similarity > 0.7 and < 0.2 can be reliably automatically labeled as aligned and not-aligned. The thresholds are determined based on a pilot study and can achieve nearly 100% precision. Therefore, we automatically labeled the sentence pairs with Jaccard similarity > 0.7 and < 0.2 , and collected human annotation for the rest of the candidate sentence pairs on the Figure Eight platform.\n\n\nAlgorithm 1: Paragraph Alignment Algorithm\n\nInitialize: alignP \u2208 I k\u00d7l to 0 k\u00d7l Initialize: simP \u2208 R 2\u00d7k\u00d7l to 0 2\u00d7k\u00d7l for i \u2190 1 to k do for j \u2190 1 to l do \n\n\nC Experiment Details\n\nOur experiments are run on 4\u00d7A40 GPUs. The implementation and hyperparameter tuning process are detailed below, where the one marked with * performs best. We perform 3 runs for each setting, and average the performance. We use scikit-learn package to calculate the precision, recall and F1. 6 Sentence Alignment. We use the author's implementation of the neural CRF sentence alignment model and initialize it with the pretrained SciBERTbased-uncased encoder (Beltagy et al., 2019). We tune the learning rate in {1e-5, 3e-5 * , 5e-5} based on F1 on the devset. The model is trained within 1.5 hours.\n\nIntention Classification. We use the Huggingface 7 implementation of the T5 model, and use the author's implementation of the PURE model. We initialized the PURE model with SciBERT-basedcased encoder (Beltagy et al., 2019). We tune the learning rate in {1e-5, 3e-5, 5e-5, 7e-5 * } based on F1 on the devset. Both models are trained within 1 hour.\n\nEdits Extraction. We use the original author's implementations for the neural semi-CRF word alignment model and the QA-Align model. We initialize the semi-CRF model with SpanBERT-large encoder (Joshi et al., 2020) and initialize the QA-Align model with SciBERT-based-uncased encoder (Beltagy et al., 2019). We use the default hyperparameters for both models. The semi-CRF model takes about 10 hours to train, and the QA-Align model takes about 3 hours to train.\n\n\nD Crowdsourcing Annotation Interface D.1 Screenshot of the Instructions\n\nTwo sentences convey the same meaning, while one sentence is simpler than the other one. Please fully understand this example! This is the most crucial part of this task! Don't judge by sentence length! Instead, judge by readability of the sentence! -Case 1: A simplify B or B simplify A (equivalent in meaning, though differ in length):\n\nTwo sentences are completely equivalent, as they mean the same thing. Differing in some very unimportant information is acceptable.\n\n-Case 2: A and B are equivalent in both meaning and readability:\n\n\nB: Due to the continuity equation ([REF]) and definition ([REF]), the dynamics described by Eq. ([REF]) leads to the correct intensity pattern when the\n\nThe length of extra information should be equal or longer than a long phrase.\n\nOne sentence contains most of the information of the other one. It also contains important extra information. Two sentences share some information in common.\n\nAnd each of them also contains extra information.\n\n-Case 1:\n\n-Case 2:\n\n\u2022 A and B are partially overlapped:\n\nTwo sentences are take about different issue. \u2022 A and B are mismatched:\n\n\n\u2022 A and B are equivalent\n\nExtra information\n\n\nShared information\n\nThe sentence are long. Please carefully read them, and you will be able to find the shared information.\n\nThe length of extra information should be equal or longer than a long phrase.\n\nThe sentence are long. Please carefully read them and you will be able to find the shared information.\n\nThe sentences are long. Please fully understand them, then you will be able to make correct judgments.\n\nThe sentences are long. Please fully understand them, then you will be able to make correct judgments.\n\nSometimes, the sentence pair may share some terms (like PP, TUQS in this example), but are not really equivalent or partially overlapped.\n\nYou need to read them carefully and understand their meaning to make correct judgments.\n\n\nA: The dynamics described by this equation leads to the correct intensity pattern when the statistics of a large particle ensemble is considered [CITATION] (see below in Section [REF])\n\n.\n\n\nstatistics of a large particle ensemble is considered [CITATION], as also happens in standard Bohmian mechanics.\n\nExtra information Shared information  \u2022 A and B are partially overlap (share information in common, while some important information differs/missing).\n\n\nInstructions\n\n\nCrowdsourcing Annotation Interface\n\n\u2022 The two sentences are completely dissimilar in meaning.\n\n\nComments (Optional)\n\nThe proposed method can be applied to find relations of the derived type giving possibility to search for other heavy virtual states .\n\nThe proposed method can be applied to find the relations of the derived type for different effective four-fermion operators generated by the [MATH] ( for instance , discussed in Refs.\n\n[CITATION] )\n\nIf you have any comment about this HIT, please type it here \n\nFigure 1 :\n1Our ARXIVEDITS corpus consists of both document-level revision (top) and sentence-level revision with intention (bottom). The top part shows an aligned paragraph pair from the original and revised papers, where s1 and t1 denote the corresponding sentences. For sentence-level revision, the fine-grained edits and each of their intentions are manually annotated.\n\nFigure 2 :\n2Distribution of versions (left) and subjects (right) for papers in our corpus.\n\nFigure 3 :\n3The life cycle of each paper, measured by the time interval between the first and the last versions.\n\nFigure 2 Figure 4 :\n24plots the statistics for the paper subjects and the number of versions. Physics (69.7%) and Math (14.8%) have the largest volume of multiversion papers, mainly due to the long history of use and a large number of sub-fields. About 26.7% papers have more than 2 versions available, en-The distribution of update ratio. The figure above demonstrates that papers with more versions are more likely to undergo a significant revision in their life cycle. While the two types of papers have a similar distribution of update ratio between adjacent versions, as shown in the figure below.\n\nFigure 5 :Figure 6 :\n56Update ratio for papers in different research areas. Papers in STAT have higher update ratios compared to papers in CS. The relationship between update ratio and time between adjacent submissions.\n\nFigure 8 :\n8The composition of edit actions as the update ratio changes.\n\nFigure 9 :\n9The distribution of intentions for span-level edits in the revised sentences in our corpus.\n\nFigure 10 :\n10Illustration for extracting edits by leveraging constituency parsing tree. In this example, two full spans that only have loose correspondence can be aligned.\n\n\nA: The results of stationary transport using the nonequilibrium Green's function (NEGF) formalism have been reported in the literature [CITATION]. B: The results of stationary transport for nonequilibrium systems have been reported by many authors [CITATION].\n\n\nA: In Figure [REF](b) a sample of trajectories illustrating the dynamics associated with the results of Figure [REF](a) is displayed. B: A sample of reduced trajectories illustrating the dynamics associated with the results of Fig. [REF](a) is displayed in part (b) of the same figure.\n\n\nA: We calculate logical error rates of amplified error rates [MATH] that are near to the threshold and fit logical error rates with the function [EQUATION]. B: We calculate logical error rates corresponding to amplified error rates [MATH] and small size surface code lattices using the Monte Carlo method.\n\nA\n: In the wire network shown in Fig. [REF](b), all PPs are performed with PP TUQSs of qubits and neighbouring stabilisers share measurement TUQSs. B: The number of PP TUQSs can be reduced using the system shown in Fig. [REF](b), in which there is only one PP TUQS per qubit.\n\nFigure 12 :\n12Instructions for our crowdsourcing annotation of sentence alignments on the Figure Eight platform. 9434 D.2 Screenshot of the Interface\n\nFigure 13 :\n13Interface for our crowdsourcing annotation of sentence alignments on the Figure Eight platform.\n\nTable 1 :\n1Statistics of document-level revision in our \nARXIVEDITS corpus, based on manually annotated \nsentence alignment. \n\nsplit for train/dev/test sets in the experiments of \nautomatic sentence alignment in $4. The inter-\nannotator agreement is 0.614 measured by Cohen's \nkappa (Artstein and Poesio\n\nTable 2 )\n2. The type of edit can be recog-\nnized by spans s a:b and t c:d , where s a:b = [NULL] \nindicating insertion, t c:d = [NULL] for deletion, \ns a:b = t c:d representing reordering, and s a:b = t c:d \nfor substitution. \n\nEdit Intention Taxonomy. We propose a new \ntaxonomy to comprehensively capture the intention \nof text revision in the scientific writing domain, as \nshown in Table 2. Each edit is classified into one of \nthe following categories: Improve Language, Cor-\nrect Grammar/Typo, Update Content, and Adjust \nFormat. Since our goal is to improve the writing \n\n\nTable 2 :\n2A taxonomy (I) of edit intentions in scientific writing revisions. In each example, text with red background \ndenotes the edit. Span with strike-through means the content got deleted, otherwise is inserted. \n\nquality, we further break the Improve Language \ntype into four fine-grained categories. During the \ndesign, we extensively consult prior literature in \ntext revision (Faigley and Witte, 1981; Fitzger-\nald, 1987; Daxenberger, 2016), edit categorization \n(Bronner and Monz, 2012; Yang et al., 2017), and \nanalysis in related areas such as Wikipedia (Dax-\nenberger and Gurevych, 2013) and argumentative \nessays (Zhang et al.\n\nTable 4 :\n4Performance of intention classification on the ARXIVEDITS testset.Intention Label \nPrecision Recall \nF1 \n\nAdjust Format \n96.7 \n94.6 \n95.6 \nUpdate Content \n84.8 \n86.9 \n85.8 \nFix Grammar/Typo \n81.1 \n85.1 \n83.1 \nLanguage-Simplify \n75.0 \n66.7 \n70.6 \nLanguage-Accurate \n54.7 \n63.0 \n58.6 \nLanguage-Style \n46.9 \n37.5 \n41.7 \n\n\n\nTable 5 :\n5Breakdown performance of the best perform-\ning T5-large model on ARXIVEDITS testset for fine-\ngrained intention classification task. \n\n\n\nTable 6\n6If furthermore the [MATH] -body Efimov states remain after extension of the Hilbert space to allow all correlations , we can more generally expect two universal [MATH] -body bound states below the threshold for binding[MATH]  states .If these [MATH] -body Efimov states remain after extension of the Hilbert space to allow all correlations , we can expect these sequences to be continued to the thresholds for binding by decreasing the attraction .presents the ex-\n\n\nTable 6 :\n6Evaluation Results of different sentence alignment methods on our ARXIVEDITS testset.\n\n\nsimP [1, i, j] = avg if simP [1, imax, j] > \u03c41 and d(imax, j) < \u03c42 then alignP [imax, j] = 1 else if simP [1, imax, j] > \u03c43 then alignP [imax, j] = 1 end for i \u2190 1 to k do jmax = argmax if simP [2, i,jmax] > \u03c41 and d(i, jmax) < \u03c44 then alignP [i, jmax] = 1 else if simP [2, i, jmax] > \u03c43 then alignP [i, jmax] = 1 end return alignPsp\u2208S i \n\nmax \n\ncq \u2208C j \n\nsimSent(sp, cq) \n\nsimP [2, i, j] = avg \n\ncp\u2208C i \n\nmax \n\nsq \u2208S j \n\nsimSent(sp, cq) \n\nend \nend \nfor j \u2190 1 to l do \nimax = argmax \n\ni \n\nsimP [2, i, j] \n\nj \n\nsimP [1, i, j] \n\n9432 \n\n\n\n\nWhat's the relationship between Sentence A and Sentence B ?Sentence A \nSentence B \n\nA and B are equivalent \nA , B are partially overlapped \nA and B are mismatched \n\n\u2022 A and B are equivalent \n\n(convey the same meaning, \nthough one sentence can be \nmuch shorter or simpler than \nthe other sentence) \n\n\nFor example, the paper titled \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762) has five versions on arXiv submitted by the authors, constituting four revisions(v1-v2, v2-v3, v3-v4, v4-v5).2 https://github.com/pkubowicz/opendetex 3 Our code is released at https://tiny.one/arxivedits\nhttps://pandoc.org/ 5 https://www.figure-eight.com/\nhttps://scikit-learn.org/stable/modules/ generated/sklearn.metrics.classification_report. html 7 https://huggingface.co/\n\nResearch on revision in writing. Jill Fitzgerald, Review of educational research. Jill Fitzgerald. 1987. Research on revision in writing. Review of educational research.\n\nSemEval-2018 task 7: Semantic relation extraction and classification in scientific papers. Kata G\u00e1bor, Davide Buscaldi, Anne-Kathrin Schumann, Behrang Qasemizadeh, Ha\u00effa Zargayouna, Thierry Charnois, Proceedings of the 12th International Workshop on Semantic Evaluation. the 12th International Workshop on Semantic EvaluationKata G\u00e1bor, Davide Buscaldi, Anne-Kathrin Schu- mann, Behrang QasemiZadeh, Ha\u00effa Zargayouna, and Thierry Charnois. 2018. SemEval-2018 task 7: Semantic relation extraction and classification in sci- entific papers. In Proceedings of the 12th Interna- tional Workshop on Semantic Evaluation.\n\nThe distribution of the flora in the alpine zone. Paul Jaccard, New phytologist. Paul Jaccard. 1912. The distribution of the flora in the alpine zone. New phytologist.\n\nNeural CRF model for sentence alignment in text simplification. Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsChao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. 2020. Neural CRF model for sentence alignment in text simplification. In Pro- ceedings of the 58th Annual Meeting of the Associa- tion for Computational Linguistics.\n\nSpanBERT: Improving pre-training by representing and predicting spans. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, Omer Levy, Transactions of the Association for Computational Linguistics. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Associa- tion for Computational Linguistics.\n\nExtending a parser to distant domains using a few dozen partially annotated examples. Vidur Joshi, Matthew Peters, Mark Hopkins, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsVidur Joshi, Matthew Peters, and Mark Hopkins. 2018. Extending a parser to distant domains using a few dozen partially annotated examples. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.\n\nA dataset of peer reviews (PeerRead): Collection, insights and NLP applications. Dongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine Van Zuylen, Sebastian Kohlmeier, Eduard Hovy, Roy Schwartz, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesDongyeop Kang, Waleed Ammar, Bhavana Dalvi, Madeleine van Zuylen, Sebastian Kohlmeier, Ed- uard Hovy, and Roy Schwartz. 2018. A dataset of peer reviews (PeerRead): Collection, insights and NLP applications. In Proceedings of the 2018 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies.\n\nArgrewrite v. 2: an annotated argumentative revisions corpus. Language Resources and Evaluation. Omid Kashefi, Tazin Afrin, Meghan Dale, Christopher Olshefski, Amanda Godley, Diane Litman, Rebecca Hwa, https:/link.springer.com/article/10.1007/s10579-021-09567-zOmid Kashefi, Tazin Afrin, Meghan Dale, Christopher Olshefski, Amanda Godley, Diane Litman, and Re- becca Hwa. 2022. Argrewrite v. 2: an annotated ar- gumentative revisions corpus. Language Resources and Evaluation.\n\nReformulating unsupervised style transfer as paraphrase generation. Kalpesh Krishna, John Wieting, Mohit Iyyer, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020. Reformulating unsupervised style transfer as para- phrase generation. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing.\n\nNeural semi-Markov CRF for monolingual word alignment. Wuwei Lan, Chao Jiang, Wei Xu, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingWuwei Lan, Chao Jiang, and Wei Xu. 2021. Neural semi-Markov CRF for monolingual word alignment. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing.\n\nWhat makes writing great? first experiments on article quality prediction in the science journalism domain. Annie Louis, Ani Nenkova, Transactions of the Association for Computational Linguistics. Annie Louis and Ani Nenkova. 2013. What makes writing great? first experiments on article quality prediction in the science journalism domain. Trans- actions of the Association for Computational Lin- guistics.\n\nWeakly supervised learning for hedge classification in scientific literature. Ben Medlock, Ted Briscoe, Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. the 45th Annual Meeting of the Association of Computational LinguisticsBen Medlock and Ted Briscoe. 2007. Weakly super- vised learning for hedge classification in scientific literature. In Proceedings of the 45th Annual Meet- ing of the Association of Computational Linguistics.\n\nTowards automated document revision: Grammatical error correction, fluency edits, and beyond. Masato Mita, Keisuke Sakaguchi, Masato Hagiwara, Tomoya Mizumoto, arXiv:2205.11484arXiv preprintMasato Mita, Keisuke Sakaguchi, Masato Hagiwara, Tomoya Mizumoto, Jun Suzuki, and Kentaro Inui. 2022. Towards automated document revision: Grammatical error correction, fluency edits, and be- yond. arXiv preprint arXiv:2205.11484.\n\nAnO (ND) difference algorithm and its variations. W Eugene, Myers, https:/link.springer.com/article/10.1007/BF01840446AlgorithmicaEugene W Myers. 1986. AnO (ND) difference algo- rithm and its variations. Algorithmica.\n\nA supervised word alignment method based on cross-language span prediction using multilingual BERT. Masaaki Nagata, Katsuki Chousa, Masaaki Nishino, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language ProcessingMasaaki Nagata, Katsuki Chousa, and Masaaki Nishino. 2020. A supervised word alignment method based on cross-language span prediction us- ing multilingual BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing.\n\nMASSAlign: Alignment and annotation of comparable documents. Gustavo Paetzold, Fernando Alva-Manchego, Lucia Specia, Proceedings of the IJCNLP 2017. the IJCNLP 2017System DemonstrationsGustavo Paetzold, Fernando Alva-Manchego, and Lu- cia Specia. 2017. MASSAlign: Alignment and an- notation of comparable documents. In Proceedings of the IJCNLP 2017, System Demonstrations.\n\nAutomatically neutralizing subjective bias in text. Reid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, Diyi Yang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligenceReid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020. Automatically neutralizing subjective bias in text. In Proceedings of the AAAI conference on arti- ficial intelligence.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. Journal of Machine Learning Research.\n\nDebateSum: A large-scale argument mining and summarization dataset. Allen Roush, Arvind Balaji, Proceedings of the 7th Workshop on Argument Mining. the 7th Workshop on Argument MiningAllen Roush and Arvind Balaji. 2020. DebateSum: A large-scale argument mining and summarization dataset. In Proceedings of the 7th Workshop on Ar- gument Mining.\n\nCATS: A tool for customized alignment of text simplification corpora. Sanja \u0160tajner, Marc Franco-Salvador, Paolo Rosso, Simone Paolo Ponzetto, Proceedings of the Eleventh International Conference on Language Resources and Evaluation. the Eleventh International Conference on Language Resources and EvaluationSanja \u0160tajner, Marc Franco-Salvador, Paolo Rosso, and Simone Paolo Ponzetto. 2018. CATS: A tool for customized alignment of text simplification corpora. In Proceedings of the Eleventh International Confer- ence on Language Resources and Evaluation.\n\nA corpus of sentence-level revisions in academic writing: A step towards understanding statement strength in communication. Chenhao Tan, Lillian Lee, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational LinguisticsChenhao Tan and Lillian Lee. 2014. A corpus of sentence-level revisions in academic writing: A step towards understanding statement strength in commu- nication. In Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguis- tics.\n\nProblems in current text simplification research: New data can help. Transactions of the Association for Computational Linguistics. Wei Xu, Chris Callison-Burch, Courtney Napoles, Wei Xu, Chris Callison-Burch, and Courtney Napoles. 2015. Problems in current text simplification re- search: New data can help. Transactions of the As- sociation for Computational Linguistics.\n\nParaphrasing for style. Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, Colin Cherry, Proceedings of COLING. COLINGWei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin Cherry. 2012. Paraphrasing for style. In Pro- ceedings of COLING 2012.\n", "annotations": {"author": "[{\"end\":234,\"start\":97},{\"end\":308,\"start\":235},{\"end\":395,\"start\":309}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":102},{\"end\":241,\"start\":239},{\"end\":323,\"start\":316}]", "author_first_name": "[{\"end\":101,\"start\":97},{\"end\":238,\"start\":235},{\"end\":315,\"start\":309}]", "author_affiliation": "[{\"end\":233,\"start\":169},{\"end\":307,\"start\":243},{\"end\":394,\"start\":325}]", "title": "[{\"end\":75,\"start\":1},{\"end\":470,\"start\":396}]", "venue": "[{\"end\":558,\"start\":472}]", "abstract": "[{\"end\":2060,\"start\":650}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3021,\"start\":2996},{\"end\":3039,\"start\":3021},{\"end\":3066,\"start\":3039},{\"end\":3086,\"start\":3066},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3141,\"start\":3124},{\"end\":3164,\"start\":3141},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3198,\"start\":3181},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3219,\"start\":3198},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3264,\"start\":3237},{\"end\":3310,\"start\":3292},{\"end\":3834,\"start\":3815},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4328,\"start\":4308},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4346,\"start\":4328},{\"end\":4362,\"start\":4346},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4399,\"start\":4380},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4417,\"start\":4399},{\"end\":5703,\"start\":5693},{\"end\":5869,\"start\":5859},{\"end\":6153,\"start\":6131},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6174,\"start\":6153},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6231,\"start\":6218},{\"end\":7869,\"start\":7847},{\"end\":8830,\"start\":8829},{\"end\":8977,\"start\":8957},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9000,\"start\":8977},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10021,\"start\":10007},{\"end\":11210,\"start\":11203},{\"end\":12438,\"start\":12431},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12878,\"start\":12860},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13710,\"start\":13692},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21053,\"start\":21035},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21173,\"start\":21155},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22307,\"start\":22287},{\"end\":22782,\"start\":22763},{\"end\":22798,\"start\":22782},{\"end\":23252,\"start\":23235},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24517,\"start\":24496},{\"end\":25821,\"start\":25799},{\"end\":25843,\"start\":25821},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25885,\"start\":25868},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26223,\"start\":26203},{\"end\":26914,\"start\":26898},{\"end\":26954,\"start\":26937},{\"end\":26989,\"start\":26978},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27822,\"start\":27797},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27857,\"start\":27830},{\"end\":27889,\"start\":27870},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27929,\"start\":27910},{\"end\":27988,\"start\":27960},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28105,\"start\":28086},{\"end\":28121,\"start\":28105},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28139,\"start\":28121},{\"end\":28390,\"start\":28368},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28411,\"start\":28390},{\"end\":28846,\"start\":28827},{\"end\":28868,\"start\":28846},{\"end\":28913,\"start\":28893},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28934,\"start\":28913},{\"end\":34785,\"start\":34763},{\"end\":35127,\"start\":35105},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35466,\"start\":35446},{\"end\":35558,\"start\":35536}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39052,\"start\":38678},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39144,\"start\":39053},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39258,\"start\":39145},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39862,\"start\":39259},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40083,\"start\":39863},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40157,\"start\":40084},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40262,\"start\":40158},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40436,\"start\":40263},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40698,\"start\":40437},{\"attributes\":{\"id\":\"fig_9\"},\"end\":40986,\"start\":40699},{\"attributes\":{\"id\":\"fig_10\"},\"end\":41294,\"start\":40987},{\"attributes\":{\"id\":\"fig_11\"},\"end\":41571,\"start\":41295},{\"attributes\":{\"id\":\"fig_12\"},\"end\":41722,\"start\":41572},{\"attributes\":{\"id\":\"fig_13\"},\"end\":41833,\"start\":41723},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42138,\"start\":41834},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42719,\"start\":42139},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43362,\"start\":42720},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":43693,\"start\":43363},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":43841,\"start\":43694},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":44317,\"start\":43842},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":44415,\"start\":44318},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":44952,\"start\":44416},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":45254,\"start\":44953}]", "paragraph": "[{\"end\":3311,\"start\":2076},{\"end\":4190,\"start\":3313},{\"end\":4594,\"start\":4192},{\"end\":5802,\"start\":4684},{\"end\":6720,\"start\":5804},{\"end\":6909,\"start\":6748},{\"end\":7105,\"start\":6911},{\"end\":7458,\"start\":7107},{\"end\":8336,\"start\":7493},{\"end\":9351,\"start\":8374},{\"end\":9897,\"start\":9388},{\"end\":10848,\"start\":9899},{\"end\":11503,\"start\":10850},{\"end\":11989,\"start\":11549},{\"end\":12587,\"start\":11991},{\"end\":13520,\"start\":12589},{\"end\":14093,\"start\":13522},{\"end\":14178,\"start\":14095},{\"end\":14307,\"start\":14180},{\"end\":14700,\"start\":14309},{\"end\":15028,\"start\":14702},{\"end\":15672,\"start\":15030},{\"end\":16298,\"start\":15713},{\"end\":17727,\"start\":16379},{\"end\":18308,\"start\":17729},{\"end\":19162,\"start\":18346},{\"end\":20081,\"start\":19197},{\"end\":20548,\"start\":20140},{\"end\":20943,\"start\":20588},{\"end\":22651,\"start\":20945},{\"end\":23253,\"start\":22653},{\"end\":24248,\"start\":23265},{\"end\":24800,\"start\":24277},{\"end\":25608,\"start\":24802},{\"end\":26224,\"start\":25631},{\"end\":26590,\"start\":26226},{\"end\":26853,\"start\":26602},{\"end\":27581,\"start\":26885},{\"end\":28289,\"start\":27598},{\"end\":29060,\"start\":28291},{\"end\":29894,\"start\":29075},{\"end\":30204,\"start\":29910},{\"end\":30835,\"start\":30224},{\"end\":32038,\"start\":30879},{\"end\":32116,\"start\":32040},{\"end\":32141,\"start\":32118},{\"end\":32221,\"start\":32143},{\"end\":32298,\"start\":32223},{\"end\":32363,\"start\":32300},{\"end\":32504,\"start\":32365},{\"end\":32662,\"start\":32506},{\"end\":33212,\"start\":32726},{\"end\":33515,\"start\":33214},{\"end\":34123,\"start\":33517},{\"end\":34280,\"start\":34170},{\"end\":34903,\"start\":34305},{\"end\":35251,\"start\":34905},{\"end\":35714,\"start\":35253},{\"end\":36127,\"start\":35790},{\"end\":36260,\"start\":36129},{\"end\":36326,\"start\":36262},{\"end\":36559,\"start\":36482},{\"end\":36718,\"start\":36561},{\"end\":36769,\"start\":36720},{\"end\":36779,\"start\":36771},{\"end\":36789,\"start\":36781},{\"end\":36826,\"start\":36791},{\"end\":36899,\"start\":36828},{\"end\":36945,\"start\":36928},{\"end\":37071,\"start\":36968},{\"end\":37150,\"start\":37073},{\"end\":37254,\"start\":37152},{\"end\":37358,\"start\":37256},{\"end\":37462,\"start\":37360},{\"end\":37601,\"start\":37464},{\"end\":37690,\"start\":37603},{\"end\":37880,\"start\":37879},{\"end\":38147,\"start\":37997},{\"end\":38258,\"start\":38201},{\"end\":38416,\"start\":38282},{\"end\":38601,\"start\":38418},{\"end\":38615,\"start\":38603},{\"end\":38677,\"start\":38617}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8143,\"start\":8135},{\"end\":15026,\"start\":15019},{\"end\":19753,\"start\":19746},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23347,\"start\":23340},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":24818,\"start\":24811},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":25337,\"start\":25329}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2074,\"start\":2062},{\"end\":4623,\"start\":4597},{\"end\":4656,\"start\":4626},{\"end\":4665,\"start\":4659},{\"end\":4682,\"start\":4668},{\"end\":6746,\"start\":6723},{\"attributes\":{\"n\":\"2\"},\"end\":7491,\"start\":7461},{\"attributes\":{\"n\":\"2.1\"},\"end\":8372,\"start\":8339},{\"attributes\":{\"n\":\"2.2\"},\"end\":9386,\"start\":9354},{\"attributes\":{\"n\":\"2.3\"},\"end\":11547,\"start\":11506},{\"attributes\":{\"n\":\"3\"},\"end\":15711,\"start\":15675},{\"attributes\":{\"n\":\"3.1\"},\"end\":16338,\"start\":16301},{\"attributes\":{\"n\":\"3.2\"},\"end\":16377,\"start\":16341},{\"attributes\":{\"n\":\"3.3\"},\"end\":18344,\"start\":18311},{\"attributes\":{\"n\":\"3.4\"},\"end\":19195,\"start\":19165},{\"attributes\":{\"n\":\"4\"},\"end\":20138,\"start\":20084},{\"attributes\":{\"n\":\"4.1\"},\"end\":20586,\"start\":20551},{\"end\":23263,\"start\":23256},{\"attributes\":{\"n\":\"4.2\"},\"end\":24275,\"start\":24251},{\"attributes\":{\"n\":\"4.3\"},\"end\":25629,\"start\":25611},{\"end\":26600,\"start\":26593},{\"end\":26883,\"start\":26856},{\"attributes\":{\"n\":\"5\"},\"end\":27596,\"start\":27584},{\"attributes\":{\"n\":\"6\"},\"end\":29073,\"start\":29063},{\"end\":29908,\"start\":29897},{\"end\":30222,\"start\":30207},{\"end\":30848,\"start\":30838},{\"end\":30877,\"start\":30851},{\"end\":32724,\"start\":32665},{\"end\":34168,\"start\":34126},{\"end\":34303,\"start\":34283},{\"end\":35788,\"start\":35717},{\"end\":36480,\"start\":36329},{\"end\":36926,\"start\":36902},{\"end\":36966,\"start\":36948},{\"end\":37877,\"start\":37693},{\"end\":37995,\"start\":37883},{\"end\":38162,\"start\":38150},{\"end\":38199,\"start\":38165},{\"end\":38280,\"start\":38261},{\"end\":38689,\"start\":38679},{\"end\":39064,\"start\":39054},{\"end\":39156,\"start\":39146},{\"end\":39279,\"start\":39260},{\"end\":39884,\"start\":39864},{\"end\":40095,\"start\":40085},{\"end\":40169,\"start\":40159},{\"end\":40275,\"start\":40264},{\"end\":41297,\"start\":41296},{\"end\":41584,\"start\":41573},{\"end\":41735,\"start\":41724},{\"end\":41844,\"start\":41835},{\"end\":42149,\"start\":42140},{\"end\":42730,\"start\":42721},{\"end\":43373,\"start\":43364},{\"end\":43704,\"start\":43695},{\"end\":43850,\"start\":43843},{\"end\":44328,\"start\":44319}]", "table": "[{\"end\":42138,\"start\":41846},{\"end\":42719,\"start\":42151},{\"end\":43362,\"start\":42732},{\"end\":43693,\"start\":43441},{\"end\":43841,\"start\":43706},{\"end\":44317,\"start\":44300},{\"end\":44952,\"start\":44749},{\"end\":45254,\"start\":45014}]", "figure_caption": "[{\"end\":39052,\"start\":38691},{\"end\":39144,\"start\":39066},{\"end\":39258,\"start\":39158},{\"end\":39862,\"start\":39282},{\"end\":40083,\"start\":39887},{\"end\":40157,\"start\":40097},{\"end\":40262,\"start\":40171},{\"end\":40436,\"start\":40278},{\"end\":40698,\"start\":40439},{\"end\":40986,\"start\":40701},{\"end\":41294,\"start\":40989},{\"end\":41571,\"start\":41298},{\"end\":41722,\"start\":41587},{\"end\":41833,\"start\":41738},{\"end\":43441,\"start\":43375},{\"end\":44300,\"start\":43852},{\"end\":44415,\"start\":44330},{\"end\":44749,\"start\":44418},{\"end\":45014,\"start\":44955}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3646,\"start\":3638},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6045,\"start\":6037},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9582,\"start\":9574},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11788,\"start\":11780},{\"end\":12430,\"start\":12417},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13209,\"start\":13201},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16193,\"start\":16185},{\"end\":16723,\"start\":16715},{\"end\":17313,\"start\":17305},{\"end\":17550,\"start\":17542},{\"end\":18177,\"start\":18169},{\"end\":18580,\"start\":18572},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18918,\"start\":18910},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19482,\"start\":19474},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20547,\"start\":20538},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22168,\"start\":22159},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26729,\"start\":26720}]", "bib_author_first_name": "[{\"end\":45763,\"start\":45759},{\"end\":45992,\"start\":45988},{\"end\":46006,\"start\":46000},{\"end\":46029,\"start\":46017},{\"end\":46047,\"start\":46040},{\"end\":46066,\"start\":46061},{\"end\":46086,\"start\":46079},{\"end\":46567,\"start\":46563},{\"end\":46750,\"start\":46746},{\"end\":46765,\"start\":46758},{\"end\":46780,\"start\":46775},{\"end\":46790,\"start\":46786},{\"end\":46801,\"start\":46798},{\"end\":47275,\"start\":47269},{\"end\":47288,\"start\":47283},{\"end\":47301,\"start\":47295},{\"end\":47313,\"start\":47307},{\"end\":47315,\"start\":47314},{\"end\":47326,\"start\":47322},{\"end\":47344,\"start\":47340},{\"end\":47735,\"start\":47730},{\"end\":47750,\"start\":47743},{\"end\":47763,\"start\":47759},{\"end\":48255,\"start\":48247},{\"end\":48268,\"start\":48262},{\"end\":48283,\"start\":48276},{\"end\":48300,\"start\":48291},{\"end\":48322,\"start\":48313},{\"end\":48340,\"start\":48334},{\"end\":48350,\"start\":48347},{\"end\":49094,\"start\":49090},{\"end\":49109,\"start\":49104},{\"end\":49123,\"start\":49117},{\"end\":49141,\"start\":49130},{\"end\":49159,\"start\":49153},{\"end\":49173,\"start\":49168},{\"end\":49189,\"start\":49182},{\"end\":49546,\"start\":49539},{\"end\":49560,\"start\":49556},{\"end\":49575,\"start\":49570},{\"end\":50020,\"start\":50015},{\"end\":50030,\"start\":50026},{\"end\":50041,\"start\":50038},{\"end\":50736,\"start\":50731},{\"end\":50747,\"start\":50744},{\"end\":51112,\"start\":51109},{\"end\":51125,\"start\":51122},{\"end\":51603,\"start\":51597},{\"end\":51617,\"start\":51610},{\"end\":51635,\"start\":51629},{\"end\":51652,\"start\":51646},{\"end\":51976,\"start\":51975},{\"end\":52251,\"start\":52244},{\"end\":52267,\"start\":52260},{\"end\":52283,\"start\":52276},{\"end\":52775,\"start\":52768},{\"end\":52794,\"start\":52786},{\"end\":52815,\"start\":52810},{\"end\":53138,\"start\":53134},{\"end\":53155,\"start\":53148},{\"end\":53161,\"start\":53156},{\"end\":53178,\"start\":53172},{\"end\":53190,\"start\":53185},{\"end\":53205,\"start\":53202},{\"end\":53220,\"start\":53216},{\"end\":53648,\"start\":53643},{\"end\":53661,\"start\":53657},{\"end\":53675,\"start\":53671},{\"end\":53694,\"start\":53685},{\"end\":53706,\"start\":53700},{\"end\":53722,\"start\":53715},{\"end\":53736,\"start\":53731},{\"end\":53746,\"start\":53743},{\"end\":53756,\"start\":53751},{\"end\":53758,\"start\":53757},{\"end\":54131,\"start\":54126},{\"end\":54145,\"start\":54139},{\"end\":54479,\"start\":54474},{\"end\":54493,\"start\":54489},{\"end\":54516,\"start\":54511},{\"end\":54530,\"start\":54524},{\"end\":54536,\"start\":54531},{\"end\":55093,\"start\":55086},{\"end\":55106,\"start\":55099},{\"end\":55666,\"start\":55663},{\"end\":55676,\"start\":55671},{\"end\":55701,\"start\":55693},{\"end\":55933,\"start\":55930},{\"end\":55942,\"start\":55938},{\"end\":55955,\"start\":55951},{\"end\":55968,\"start\":55963},{\"end\":55984,\"start\":55979}]", "bib_author_last_name": "[{\"end\":45774,\"start\":45764},{\"end\":45998,\"start\":45993},{\"end\":46015,\"start\":46007},{\"end\":46038,\"start\":46030},{\"end\":46059,\"start\":46048},{\"end\":46077,\"start\":46067},{\"end\":46095,\"start\":46087},{\"end\":46575,\"start\":46568},{\"end\":46756,\"start\":46751},{\"end\":46773,\"start\":46766},{\"end\":46784,\"start\":46781},{\"end\":46796,\"start\":46791},{\"end\":46804,\"start\":46802},{\"end\":47281,\"start\":47276},{\"end\":47293,\"start\":47289},{\"end\":47305,\"start\":47302},{\"end\":47320,\"start\":47316},{\"end\":47338,\"start\":47327},{\"end\":47349,\"start\":47345},{\"end\":47741,\"start\":47736},{\"end\":47757,\"start\":47751},{\"end\":47771,\"start\":47764},{\"end\":48260,\"start\":48256},{\"end\":48274,\"start\":48269},{\"end\":48289,\"start\":48284},{\"end\":48311,\"start\":48301},{\"end\":48332,\"start\":48323},{\"end\":48345,\"start\":48341},{\"end\":48359,\"start\":48351},{\"end\":49102,\"start\":49095},{\"end\":49115,\"start\":49110},{\"end\":49128,\"start\":49124},{\"end\":49151,\"start\":49142},{\"end\":49166,\"start\":49160},{\"end\":49180,\"start\":49174},{\"end\":49193,\"start\":49190},{\"end\":49554,\"start\":49547},{\"end\":49568,\"start\":49561},{\"end\":49581,\"start\":49576},{\"end\":50024,\"start\":50021},{\"end\":50036,\"start\":50031},{\"end\":50044,\"start\":50042},{\"end\":50742,\"start\":50737},{\"end\":50755,\"start\":50748},{\"end\":51120,\"start\":51113},{\"end\":51133,\"start\":51126},{\"end\":51608,\"start\":51604},{\"end\":51627,\"start\":51618},{\"end\":51644,\"start\":51636},{\"end\":51661,\"start\":51653},{\"end\":51983,\"start\":51977},{\"end\":51990,\"start\":51985},{\"end\":52258,\"start\":52252},{\"end\":52274,\"start\":52268},{\"end\":52291,\"start\":52284},{\"end\":52784,\"start\":52776},{\"end\":52808,\"start\":52795},{\"end\":52822,\"start\":52816},{\"end\":53146,\"start\":53139},{\"end\":53170,\"start\":53162},{\"end\":53183,\"start\":53179},{\"end\":53200,\"start\":53191},{\"end\":53214,\"start\":53206},{\"end\":53225,\"start\":53221},{\"end\":53655,\"start\":53649},{\"end\":53669,\"start\":53662},{\"end\":53683,\"start\":53676},{\"end\":53698,\"start\":53695},{\"end\":53713,\"start\":53707},{\"end\":53729,\"start\":53723},{\"end\":53741,\"start\":53737},{\"end\":53749,\"start\":53747},{\"end\":53762,\"start\":53759},{\"end\":54137,\"start\":54132},{\"end\":54152,\"start\":54146},{\"end\":54487,\"start\":54480},{\"end\":54509,\"start\":54494},{\"end\":54522,\"start\":54517},{\"end\":54545,\"start\":54537},{\"end\":55097,\"start\":55094},{\"end\":55110,\"start\":55107},{\"end\":55669,\"start\":55667},{\"end\":55691,\"start\":55677},{\"end\":55709,\"start\":55702},{\"end\":55936,\"start\":55934},{\"end\":55949,\"start\":55943},{\"end\":55961,\"start\":55956},{\"end\":55977,\"start\":55969},{\"end\":55991,\"start\":55985}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":146578268},\"end\":45895,\"start\":45726},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":44163645},\"end\":46511,\"start\":45897},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":85574559},\"end\":46680,\"start\":46513},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":216557637},\"end\":47196,\"start\":46682},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":198229624},\"end\":47642,\"start\":47198},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":21712653},\"end\":48164,\"start\":47644},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":13746581},\"end\":48991,\"start\":48166},{\"attributes\":{\"doi\":\"https:/link.springer.com/article/10.1007/s10579-021-09567-z\",\"id\":\"b7\"},\"end\":49469,\"start\":48993},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":222291619},\"end\":49958,\"start\":49471},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":235352798},\"end\":50621,\"start\":49960},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":18871276},\"end\":51029,\"start\":50623},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":18343028},\"end\":51501,\"start\":51031},{\"attributes\":{\"doi\":\"arXiv:2205.11484\",\"id\":\"b12\"},\"end\":51923,\"start\":51503},{\"attributes\":{\"doi\":\"https:/link.springer.com/article/10.1007/BF01840446\",\"id\":\"b13\"},\"end\":52142,\"start\":51925},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":216915063},\"end\":52705,\"start\":52144},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8604855},\"end\":53080,\"start\":52707},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":208248333},\"end\":53558,\"start\":53082},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":204838007},\"end\":54056,\"start\":53560},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":226964234},\"end\":54402,\"start\":54058},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":21717054},\"end\":54960,\"start\":54404},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14527065},\"end\":55529,\"start\":54962},{\"attributes\":{\"id\":\"b21\"},\"end\":55904,\"start\":55531},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13050210},\"end\":56151,\"start\":55906}]", "bib_title": "[{\"end\":45757,\"start\":45726},{\"end\":45986,\"start\":45897},{\"end\":46561,\"start\":46513},{\"end\":46744,\"start\":46682},{\"end\":47267,\"start\":47198},{\"end\":47728,\"start\":47644},{\"end\":48245,\"start\":48166},{\"end\":49537,\"start\":49471},{\"end\":50013,\"start\":49960},{\"end\":50729,\"start\":50623},{\"end\":51107,\"start\":51031},{\"end\":52242,\"start\":52144},{\"end\":52766,\"start\":52707},{\"end\":53132,\"start\":53082},{\"end\":53641,\"start\":53560},{\"end\":54124,\"start\":54058},{\"end\":54472,\"start\":54404},{\"end\":55084,\"start\":54962},{\"end\":55928,\"start\":55906}]", "bib_author": "[{\"end\":45776,\"start\":45759},{\"end\":46000,\"start\":45988},{\"end\":46017,\"start\":46000},{\"end\":46040,\"start\":46017},{\"end\":46061,\"start\":46040},{\"end\":46079,\"start\":46061},{\"end\":46097,\"start\":46079},{\"end\":46577,\"start\":46563},{\"end\":46758,\"start\":46746},{\"end\":46775,\"start\":46758},{\"end\":46786,\"start\":46775},{\"end\":46798,\"start\":46786},{\"end\":46806,\"start\":46798},{\"end\":47283,\"start\":47269},{\"end\":47295,\"start\":47283},{\"end\":47307,\"start\":47295},{\"end\":47322,\"start\":47307},{\"end\":47340,\"start\":47322},{\"end\":47351,\"start\":47340},{\"end\":47743,\"start\":47730},{\"end\":47759,\"start\":47743},{\"end\":47773,\"start\":47759},{\"end\":48262,\"start\":48247},{\"end\":48276,\"start\":48262},{\"end\":48291,\"start\":48276},{\"end\":48313,\"start\":48291},{\"end\":48334,\"start\":48313},{\"end\":48347,\"start\":48334},{\"end\":48361,\"start\":48347},{\"end\":49104,\"start\":49090},{\"end\":49117,\"start\":49104},{\"end\":49130,\"start\":49117},{\"end\":49153,\"start\":49130},{\"end\":49168,\"start\":49153},{\"end\":49182,\"start\":49168},{\"end\":49195,\"start\":49182},{\"end\":49556,\"start\":49539},{\"end\":49570,\"start\":49556},{\"end\":49583,\"start\":49570},{\"end\":50026,\"start\":50015},{\"end\":50038,\"start\":50026},{\"end\":50046,\"start\":50038},{\"end\":50744,\"start\":50731},{\"end\":50757,\"start\":50744},{\"end\":51122,\"start\":51109},{\"end\":51135,\"start\":51122},{\"end\":51610,\"start\":51597},{\"end\":51629,\"start\":51610},{\"end\":51646,\"start\":51629},{\"end\":51663,\"start\":51646},{\"end\":51985,\"start\":51975},{\"end\":51992,\"start\":51985},{\"end\":52260,\"start\":52244},{\"end\":52276,\"start\":52260},{\"end\":52293,\"start\":52276},{\"end\":52786,\"start\":52768},{\"end\":52810,\"start\":52786},{\"end\":52824,\"start\":52810},{\"end\":53148,\"start\":53134},{\"end\":53172,\"start\":53148},{\"end\":53185,\"start\":53172},{\"end\":53202,\"start\":53185},{\"end\":53216,\"start\":53202},{\"end\":53227,\"start\":53216},{\"end\":53657,\"start\":53643},{\"end\":53671,\"start\":53657},{\"end\":53685,\"start\":53671},{\"end\":53700,\"start\":53685},{\"end\":53715,\"start\":53700},{\"end\":53731,\"start\":53715},{\"end\":53743,\"start\":53731},{\"end\":53751,\"start\":53743},{\"end\":53764,\"start\":53751},{\"end\":54139,\"start\":54126},{\"end\":54154,\"start\":54139},{\"end\":54489,\"start\":54474},{\"end\":54511,\"start\":54489},{\"end\":54524,\"start\":54511},{\"end\":54547,\"start\":54524},{\"end\":55099,\"start\":55086},{\"end\":55112,\"start\":55099},{\"end\":55671,\"start\":55663},{\"end\":55693,\"start\":55671},{\"end\":55711,\"start\":55693},{\"end\":55938,\"start\":55930},{\"end\":55951,\"start\":55938},{\"end\":55963,\"start\":55951},{\"end\":55979,\"start\":55963},{\"end\":55993,\"start\":55979}]", "bib_venue": "[{\"end\":45806,\"start\":45776},{\"end\":46166,\"start\":46097},{\"end\":46592,\"start\":46577},{\"end\":46893,\"start\":46806},{\"end\":47412,\"start\":47351},{\"end\":47860,\"start\":47773},{\"end\":48503,\"start\":48361},{\"end\":49088,\"start\":48993},{\"end\":49669,\"start\":49583},{\"end\":50208,\"start\":50046},{\"end\":50818,\"start\":50757},{\"end\":51221,\"start\":51135},{\"end\":51595,\"start\":51503},{\"end\":51973,\"start\":51925},{\"end\":52379,\"start\":52293},{\"end\":52854,\"start\":52824},{\"end\":53288,\"start\":53227},{\"end\":53800,\"start\":53764},{\"end\":54204,\"start\":54154},{\"end\":54636,\"start\":54547},{\"end\":55199,\"start\":55112},{\"end\":55661,\"start\":55531},{\"end\":56014,\"start\":55993},{\"end\":46222,\"start\":46168},{\"end\":46967,\"start\":46895},{\"end\":47934,\"start\":47862},{\"end\":48632,\"start\":48505},{\"end\":49742,\"start\":49671},{\"end\":50357,\"start\":50210},{\"end\":51294,\"start\":51223},{\"end\":52452,\"start\":52381},{\"end\":52871,\"start\":52856},{\"end\":53336,\"start\":53290},{\"end\":54241,\"start\":54206},{\"end\":54712,\"start\":54638},{\"end\":55273,\"start\":55201},{\"end\":56022,\"start\":56016}]"}}}, "year": 2023, "month": 12, "day": 17}