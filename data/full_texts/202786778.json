{"id": 202786778, "updated": "2023-10-06 22:49:57.448", "metadata": {"title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library", "authors": "[{\"first\":\"Adam\",\"last\":\"Paszke\",\"middle\":[]},{\"first\":\"Sam\",\"last\":\"Gross\",\"middle\":[]},{\"first\":\"Francisco\",\"last\":\"Massa\",\"middle\":[]},{\"first\":\"Adam\",\"last\":\"Lerer\",\"middle\":[]},{\"first\":\"James\",\"last\":\"Bradbury\",\"middle\":[]},{\"first\":\"Gregory\",\"last\":\"Chanan\",\"middle\":[]},{\"first\":\"Trevor\",\"last\":\"Killeen\",\"middle\":[]},{\"first\":\"Zeming\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Natalia\",\"last\":\"Gimelshein\",\"middle\":[]},{\"first\":\"Luca\",\"last\":\"Antiga\",\"middle\":[]},{\"first\":\"Alban\",\"last\":\"Desmaison\",\"middle\":[]},{\"first\":\"Andreas\",\"last\":\"Kopf\",\"middle\":[]},{\"first\":\"Edward\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Zach\",\"last\":\"DeVito\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Raison\",\"middle\":[]},{\"first\":\"Alykhan\",\"last\":\"Tejani\",\"middle\":[]},{\"first\":\"Sasank\",\"last\":\"Chilamkurthy\",\"middle\":[]},{\"first\":\"Benoit\",\"last\":\"Steiner\",\"middle\":[]},{\"first\":\"Lu\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Junjie\",\"last\":\"Bai\",\"middle\":[]},{\"first\":\"Soumith\",\"last\":\"Chintala\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "8024-8035", "publication_date": {"year": 2019, "month": 12, "day": 3}, "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1912.01703", "mag": "2992693735", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/PaszkeGMLBCKLGA19", "doi": null}}, "content": {"source": {"pdf_hash": "3b6bace73e9d3997439cba4ab06b39e1d460fa78", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1912.01703v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "fa1c3377a72cc4072564c16b12136c26eed547b5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3b6bace73e9d3997439cba4ab06b39e1d460fa78.txt", "contents": "\nPyTorch: An Imperative Style, High-Performance Deep Learning Library\nNeurIPS 2019\n\nAdam Paszke adam.paszke@gmail.com \nSam Gross sgross@fb.com \nFrancisco Massa fmassa@fb.com \nAdam Lerer alerer@fb.com \nJames Bradbury Google \nGregory Chanan gchanan@fb.com \nTrevor Killeen killeent@cs.washington.edu \nZeming Lin zlin@fb.com \nNatalia Gimelshein ngimelshein@nvidia.com \nLuca Antiga luca.antiga@orobix.com \nAlban Desmaison \nAndreas K\u00f6pf xamlaandreas.koepf@xamla.com \nEdward Yang ezyang@fb.com \nZach Devito zdevito@cs.stanford.edu \nMartin Raison Nabla \nAlykhan Tejani \nSasank Chilamkurthy sasankchilamkurthy@gmail.com \nQure Ai \nBenoit Steiner benoitsteiner@fb.com \nLu Fang Facebook \nJunjie Bai Facebook \nSoumith Chintala soumith@gmail.com \n\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nSelf Employed\nFacebook AI Research\nUniversity of Warsaw\nNVIDIA\nOrobix\n\n\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nFacebook AI Research\nOxford University\n\n\nPyTorch: An Imperative Style, High-Performance Deep Learning Library\n\n33rd Conference on Neural Information Processing Systems\nVancouver, CanadaNeurIPS 2019\nDeep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.\n\nIntroduction\n\nWith the increased interest in deep learning in recent years, there has been an explosion of machine learning tools. Many popular frameworks such as Caffe [1], CNTK [2], TensorFlow [3], and Theano [4], construct a static dataflow graph that represents the computation and which can then be applied repeatedly to batches of data. This approach provides visibility into the whole computation ahead of time, and can theoretically be leveraged to improve performance and scalability. However, it comes at the cost of ease of use, ease of debugging, and flexibility of the types of computation that can be represented.\n\nPrior work has recognized the value of dynamic eager execution for deep learning, and some recent frameworks implement this define-by-run approach, but do so either at the cost of performance (Chainer [5]) or using a less expressive, faster language (Torch [6], DyNet [7]), which limits their applicability.\n\nHowever, with careful implementation and design choices, dynamic eager execution can be achieved largely without sacrificing performance. This paper introduces PyTorch, a Python library that performs immediate execution of dynamic tensor computations with automatic differentiation and GPU acceleration, and does so while maintaining performance comparable to the fastest current libraries for deep learning. This combination has turned out to be very popular in the research community with, for instance, 296 ICLR 2019 submissions mentioning PyTorch.\n\n\nBackground\n\nFour major trends in scientific computing have become increasingly important for deep learning.\n\nFirst, starting in the 1960s, the development of domain specific languages such as APL [8], MATLAB [9], R [10] and Julia [11], turned multidimensional arrays (often referred to as tensors) into first-class objects supported by a comprehensive set of mathematical primitives (or operators) to manipulate them. Separately, libraries such as NumPy [12], Torch [6], Eigen [13] and Lush [14] made array-based programming productive in general purpose languages such as Python, Lisp, C++ and Lua.\n\nSecond, the development of automatic differentiation [15] made it possible to fully automate the daunting labor of computing derivatives. This made it significantly easier to experiment with different machine learning approaches while still allowing for efficient gradient based optimization. The autograd [16] package popularized the use of this technique for NumPy arrays, and similar approaches are used in frameworks such as Chainer [5], DyNet [7], Lush [14], Torch [6], Jax [17] and Flux.jl [18].\n\nThird, with the advent of the free software movement, the scientific community moved away from closed proprietary software such as Matlab [9], and towards the open-source Python ecosystem with packages like NumPy [12], SciPy [19], and Pandas [20]. This fulfilled most of the numerical analysis needs of researchers while allowing them to take advantage of a vast repository of libraries to handle dataset preprocessing, statistical analysis, plotting, and more. Moreover, the openness, interoperability, and flexibility of free software fostered the development of vibrant communities that could quickly address new or changing needs by extending the existing functionality of a library or if needed by developing and releasing brand new ones. While there is a rich offering of open-source software for neural networks in languages other than Python, starting with Lush [14] in Lisp, Torch [6] in C++, Objective-C and Lua, EBLearn [21] in C++, Caffe [1] in C++, the network effects of a large ecosystem such as Python made it an essential skill to jumpstart one's research. Hence, since 2014, most deep learning frameworks converged on a Python interface as an essential feature.\n\nFinally, the availability and commoditization of general-purpose massively parallel hardware such as GPUs provided the computing power required by deep learning methods. Specialized libraries such as cuDNN [22], along with a body of academic work (such as [23] and [24]), produced a set of high-performance reusable deep learning kernels that enabled frameworks such as Caffe [1], Torch7 [25], or TensorFlow [3] to take advantage of these hardware accelerators.\n\nPyTorch builds on these trends by providing an array-based programming model accelerated by GPUs and differentiable via automatic differentiation integrated in the Python ecosystem.\n\n\nDesign principles\n\nPyTorch's success stems from weaving previous ideas into a design that balances speed and ease of use. There are four main principles behind our choices:\n\nBe Pythonic Data scientists are familiar with the Python language, its programming model, and its tools. PyTorch should be a first-class member of that ecosystem. It follows the commonly established design goals of keeping interfaces simple and consistent, ideally with one idiomatic way of doing things. It also integrates naturally with standard plotting, debugging, and data processing tools.\n\nPut researchers first PyTorch strives to make writing models, data loaders, and optimizers as easy and productive as possible. The complexity inherent to machine learning should be handled internally by the PyTorch library and hidden behind intuitive APIs free of side-effects and unexpected performance cliffs.\n\nProvide pragmatic performance To be useful, PyTorch needs to deliver compelling performance, although not at the expense of simplicity and ease of use. Trading 10% of speed for a significantly simpler to use model is acceptable; 100% is not. Therefore, its implementation accepts added complexity in order to deliver that performance. Additionally, providing tools that allow researchers to manually control the execution of their code will empower them to find their own performance improvements independent of those that the library provides automatically.\n\nWorse is better [26] Given a fixed amount of engineering resources, and all else being equal, the time saved by keeping the internal implementation of PyTorch simple can be used to implement additional features, adapt to new situations, and keep up with the fast pace of progress in the field of AI. Therefore it is better to have a simple but slightly incomplete solution than a comprehensive but complex and hard to maintain design. In a surprisingly short amount of time, machine learning grew from recognizing individual digits [27] into autonomously playing StarCraft [28]. Consequently, the neural networks themselves evolved rapidly from simple sequences of feed forward layers into incredibly varied numerical programs often composed of many loops and recursive functions. To support this growing complexity, PyTorch foregoes the potential benefits of a graph-metaprogramming based approach to preserve the imperative programming model of Python. This design was pioneered for model authoring by Chainer [5] and Dynet [7]. PyTorch extends this to all aspects of deep learning workflows. Defining layers, composing models, loading data, running optimizers, and parallelizing the training process are all expressed using the familiar concepts developed for general purpose programming.\n\nThis solution ensures that any new potential neural network architecture can be easily implemented with PyTorch. For instance, layers (which in modern machine learning should really be understood as stateful functions with implicit parameters) are typically expressed as Python classes whose constructors create and initialize their parameters, and whose forward methods process an input activation. Similarly, models are usually represented as classes that compose individual layers, but let us state again that nothing forces the user to structure their code in that way. Listing 1 demonstrates how an entire model can be created by composing functionality provided by PyTorch such as 2d convolution, matrix multiplication, dropout, and softmax to classify gray-scale images. Note that linear layers are of course part of the library, but we show an example implementation to highlight how simple it is. \nself.b = nn.Parameter(t2) def forward(self, x): t1 = self.conv(x) def forward(self, activations): t2 = nn.functional.relu(t1) t = torch.mm(activations, self.w) t3 = self.fc(t1) return t + self.b return nn.functional.softmax(t3)\nListing 1: A custom layer used as a building block for a simple but complete neural network.\n\nThis \"everything is a just a program\" philosophy is not limited to just the models, and applies to optimizers and data loaders as well. This facilitates the experimentation of new training techniques. For example, to implement the very popular generative adversarial networks, one needs to specify two separate models (the generator and the discriminator), and two loss functions that depend on both models at the same time. Rigid APIs would struggle with this setup, but the simple design employed in PyTorch easily adapts to this setting as shown in Listing 2. Since PyTorch programs execute eagerly, all the features of Python are available throughout the whole design process. Print statements, standard debuggers, and common visualization tools like matplotlib all work as expected. Users do not have to wait for lengthy compilation before they can start running their programs, and more importantly intermediate computations can be observed to understand how a model works and whether its results are correct.\n\n\nInteroperability and extensibility\n\nEasy and efficient interoperability is one of the top priorities for PyTorch because it opens the possibility to leverage the rich ecosystem of Python libraries as part of user programs. Hence, PyTorch allows for bidirectional exchange of data with external libraries. For example, it provides a mechanism to convert between NumPy arrays and PyTorch tensors using the torch.from_numpy() function and .numpy() tensor method. Similar functionality is also available to exchange data stored using the DLPack [29] format. Note that this exchange happens in both cases without any data copying -objects on both sides only describe how to interpret a memory region which is shared among them. Hence, those operations are actually extremely cheap, and take constant time no matter how large the converted arrays are.\n\nMoreover, many of the critical systems are designed specifically to be extensible. For instance, the automatic differentiation system allows users to add support for custom differentiable functions.\n\nTo do that users can define a new subclass of torch.autograd.Function that implements forward() and backward() methods, which specify the function and its derivative (or more formally the vector-Jacobian product). Similarly new datasets can be added by subclassing torch.utils.data.Dataset and implementing two methods: __getitem__ (the indexing operator) and __len__ (the length operator), making datasets behave like (possibly lazy) lists. How these work is completely up to the implementer, and many users leverage other Python packages for data loading. The DataLoader class consumes objects conforming to this interface and provides an iterator over the data which takes care of shuffling, batching, parallelization, and management of pinned CUDA memory to improve throughput.\n\nMost importantly, users are free to replace any component of PyTorch that does not meet the needs or performance requirements of their project. They are all designed to be completely interchangeable, and PyTorch takes great care not to impose any particular solution.\n\n\nAutomatic differentiation\n\nSince gradient based optimization is vital to deep learning, PyTorch must be able to automatically compute gradients of models specified by our users, and those can be arbitrary Python programs. However, Python is a dynamic programming language that allows changing most behaviors at runtime, making ahead of time source-to-source differentiation cumbersome. Instead, PyTorch uses the operator overloading approach, which builds up a representation of the computed function every time it is executed. In its current implementation [30], PyTorch performs reverse-mode automatic differentiation, which computes the gradient of a scalar output with respect to a multivariate input. Differentiating functions with more outputs than inputs is more efficiently executed using forwardmode automatic differentiation, but this use case is less common for machine learning applications. PyTorch can be easily extended to perform forward-mode differentiation using array-level dual numbers [31,32].\n\nAnother interesting and uncommon feature of our system is that it can differentiate through code employing mutation on tensors, which is one of the basic building blocks of imperative programs.\n\nTo ensure safety, we have implemented a versioning system for tensors, which lets us track their modifications and ensure that we always use the data we expect. One interesting tradeoff is that while we could utilize techniques like copy-on-write to support arbitrary programs, we chose to not go down this path, as performance-wise it is usually beneficial for the users to rewrite their code to ensure that no copies have to be performed. Hence, while most mutations are benign and can be handled automatically, the really complicated cases result in a user error, which lets them know that they likely want to restructure the program. This allows us to avoid introducing subtle and hard-to-find performance cliffs.\n\n\nPerformance focused implementation\n\nRunning deep learning algorithms efficiently from a Python interpreter is notoriously challenging: for instance, the global interpreter lock [33] effectively ensures that only one of any number of concurrent threads is running at any given time. Deep learning frameworks based on the construction of a static data-flow graph sidestep this problem by deferring the evaluation of the computation to a custom interpreter.\n\nPyTorch solved the problem differently, by carefully optimizing every aspect of its execution while simultaneously empowering its users to easily leverage additional optimization strategies.\n\n\nAn efficient C++ core\n\nDespite being closely integrated in the Python ecosystem, most of PyTorch is written in C++ to achieve high performance. This core libtorch library implements the tensor data structure, the GPU and CPU operators, and basic parallel primitives. It also provides the automatic differentiation system, including the gradient formulas for most built-in functions. This ensures that the computation of the derivatives of functions composed of core PyTorch operators is executed entirely in a multithreaded evaluator which does not require holding the Python global interpreter lock [33]. Python bindings are generated using YAML meta-data files. An interesting side-effect of this approach is that it allowed our community to quickly create bindings to multiple other languages resulting in projects like NimTorch [34], hasktorch [35] and others.\n\nThis design also allowed us to create first-class C++ bindings and modeling libraries that can be used in places where Python is inconvenient, such as the game engine for Starcraft [36] or on mobile platforms. It is even possible to take the Python code describing a PyTorch model and run it without Python using the TorchScript engine [37].\n\n\nSeparate control and data flow\n\nPyTorch maintains a strict separation between its control (i.e. program branches, loops) and data flow (i.e. tensors and the operations performed on them). The resolution of the control flow is handled by Python and optimized C++ code executed on the host CPU, and result in a linear sequence of operator invocations on the device. Operators can be run either on CPU or on GPU.\n\nPyTorch is designed to execute operators asynchronously on GPU by leveraging the CUDA stream mechanism [38] to queue CUDA kernel invocations to the GPUs hardware FIFO. This allows the system to overlap the execution of Python code on CPU with tensor operators on GPU. Because the tensor operations usually take a significant amount of time, this lets us saturate the GPU and reach peak performance even in an interpreted language with fairly high overhead like Python. Note that this mechanism is nearly invisible to the user. Unless they implement their own multi-stream primitives all of the CPU-GPU synchronization is handled by the library.\n\nPyTorch could leverage a similar mechanism to also execute operators asynchronously on the CPU. However the costs of cross-thread communication and synchronization would negate the performance benefit of such an optimization.\n\n\nCustom caching tensor allocator\n\nAlmost every operator must dynamically allocate an output tensor to hold the result of its execution. It is therefore critical to optimize the speed of the dynamic memory allocators. PyTorch can rely on optimized libraries [39][40][41] to handle this task on CPU. However, on GPU the cudaFree routine may block its caller until all previously queued work on all GPUs completes. To avoid this bottleneck, PyTorch implements a custom allocator which incrementally builds up a cache of CUDA memory and reassigns it to later allocations without further use of CUDA APIs. The incremental allocation is also crucial for better interoperability, because taking up all GPU memory ahead of time would prevent the user from utilizing other GPU-enabled Python packages.\n\nTo further improve its effectiveness, this allocator was tuned for the specific memory usage patterns of deep learning. For example, it rounds up allocations to multiples of 512 bytes to avoid fragmentation issues. Moreover, it maintains a distinct pool of memory for every CUDA stream (work queue).\n\nThe one-pool-per-stream design assumption simplifies the implementation and improves the performance of the allocator: because the CPU runs ahead of the GPU, memory is freed on the CPU before its last use on the GPU finishes. Since streams serialize execution, if the free precedes the reallocation on the CPU, the same order will occur on the GPU. So the allocator can reallocate memory freed on the CPU immediately as long as the new allocation is used on the same stream as the freed region. However, if an allocation was last used on one stream and then allocated on another, additional synchronization is needed.\n\nThe one-pool-per-stream design seems limiting since the allocations end up fragmented per stream, but in practice PyTorch almost never uses multiple streams. It is notoriously hard to write CUDA kernels in a way that would let them cooperatively share the GPU because exact scheduling is hardware controlled. In practice, kernel writers usually resort to monolithic kernels that combine multiple tasks. Data loading and distributed computing utilities are exceptions to the one stream design, and they carefully insert additional synchronization to avoid bad interactions with the allocator.\n\nWhile this design is susceptible to certain corner cases, it almost never exhibits unwanted behaviors in practical code. Most of our users are not aware of its existence.\n\n\nMultiprocessing\n\nDue to the global interpreter lock (GIL) Python's default implementation does not allow concurrent threads to execute in parallel. To alleviate this problem, the Python community has established a standard multiprocessing module, containing a number of utilities that allow users to easily spawn child processes and implement basic inter-process communication primitives.\n\nHowever, the implementation of the primitives uses the same form of serialization used for on-disk persistence, which is inefficient when dealing with large arrays. Hence, PyTorch extends the Python multiprocessing module into torch.multiprocessing, which is a drop-in replacement for the built in package and automatically moves the data of tensors sent to other processes to shared memory instead of sending it over the communication channel.\n\nThis design greatly improves performance and makes the process isolation weaker, resulting in a programming model which more closely resembles regular threaded programs. Users can easily implement heavily parallel programs that operate on independent GPUs but later synchronize gradients using all-reduce style primitives.\n\nAnother unique feature of this system is that it transparently handles sharing of CUDA tensors, making it easy to implement techniques like Hogwild [42].\n\n\nReference counting\n\nUsers often design their models to utilize all memory available during training, and increasing batch sizes is a common technique of speeding up the process. Therefore, to deliver great performance, PyTorch has to treat memory as a scarce resource that it needs to manage carefully.\n\nLibraries with eager semantics have to manage tensor memory without knowing how it will be used in the future. Garbage collection is the typical way to handle this automatically because it has good amortized performance. In this approach, the runtime periodically investigates the state of the system, enumerates used objects and frees everything else. However, by deferring the deallocation, it causes the program to use more memory overall [43]. Given the scarcity of GPU memory, these overheads are unacceptable. In fact, Torch7 utilized the garbage collector built into Lua, and a common antipattern among the users was to sprinkle the program with explicit triggers to the garbage collector, hoping that the memory errors go away.\n\nPyTorch takes a different approach: it relies on a reference counting scheme to track the number of uses of each tensor, and frees the underlying memory immediately once this count reaches zero. Note that PyTorch tracks both references internal to the libtorch library and external references made by users in their Python code by integrating with Python's own reference counting mechanism. This ensures that memory is released exactly when tensors become unneeded.\n\nOne notable caveat is that we can only guarantee the desired performance characteristics in implementations of languages that either already utilize reference counting (CPython, Swift, but not PyPy or many scripting languages such as Lua), and those that allow for user-defined behavior for assignment, copies, and moves (e.g. C++, Rust). Bindings to implementations that do not satisfy those criteria will have to implement their own specialized memory management on top of PyTorch.\n\n\nEvaluation\n\nIn this section we compare the performance of PyTorch with several other commonly-used deep learning libraries, and find that it achieves competitive performance across a range of tasks. All experiments were performed on a workstation with two Intel Xeon E5-2698 v4 CPUs and one NVIDIA Quadro GP100 GPU.\n\n\nAsynchronous dataflow\n\nWe start by quantifying the ability of PyTorch to asynchronously execute dataflow on GPU. We use the built-in profiler [44] to instrument various benchmarks and record a timeline of the execution of a single training step. Figure 1 shows a representative timeline of execution for the first few operations of a ResNet-50 model. The host CPU which queues the work quickly outpaces the execution of the operators on the GPU. This allows PyTorch to achieve almost perfect device utilization. In this example, GPU execution takes around three times longer than CPU scheduling. The exact ratio depends on the relative performance of the host CPU and the GPU, as well as the number of elements in each tensor and the average arithmetic complexity of the floating point computations to be performed on the GPU. \n\n\nMemory management\n\nWe used the NVIDIA profiler to trace the execution of the CUDA runtime as well as the execution of the CUDA kernels launched during one training iteration of the ResNet-50 model. As shown in Figure 2, the behavior of the first iteration differs significantly from that of subsequent ones. At first, calls to the CUDA memory management functions (cudaMalloc and cudaFree) slow down the execution quite dramatically by blocking the CPU thread for long periods of time, hence lowering the utilization of the GPU. This effect disappears in subsequent iterations as the PyTorch caching memory allocator starts reusing previously allocated regions. \n\n\nBenchmarks\n\nFinally, we can get an overall sense of single-machine eager mode performance of PyTorch by comparing it to three popular graph-based deep learning frameworks (CNTK, MXNet and TensorFlow), a define-by-run framework (Chainer), and production oriented platform (PaddlePaddle). The Appendix details all the steps needed to reproduce our setup.\n\nOur results are summarized in Table 1. On all the benchmarks, the performance of PyTorch is within 17% of that of of the fastest framework. We attribute this result to the fact that these tools offload most of the computation to the same version of the cuDNN and cuBLAS libraries. \n\n\nAdoption\n\nThe validity of design decisions and their impact on ease-of-use is hard to measure. As a proxy, we tried to quantify how well the machine learning community received PyTorch by counting how often various machine learning tools (including Caffe, Chainer, CNTK, Keras, MXNet, PyTorch, TensorFlow, and Theano) are mentioned on arXiv e-Prints since the initial release of PyTorch in January 2017. In Figure 3 we report the monthly number of mentions of the word \"PyTorch\" as a percentage of all mentions among these deep learning frameworks. We counted tools mentioned multiple times in a given paper only once, and made the search case insensitive to account for various spellings. Figure 3: Among arXiv papers each month that mention common deep learning frameworks, percentage of them that mention PyTorch.\n\n\nConclusion and future work\n\nPyTorch has become a popular tool in the deep learning research community by combining a focus on usability with careful performance considerations. In addition to continuing to support the latest trends and advances in deep learning, in the future we plan to continue to improve the speed and scalability of PyTorch. Most notably, we are working on the PyTorch JIT: a suite of tools that allow PyTorch programs to be executed outside of the Python interpreter where they can be further optimized. We also intend to improve support for distributed computation by providing efficient primitives for data parallelism as well as a Pythonic library for model parallelism based around remote procedure calls.\n\n\nlearning models are just Python programs\n\n\noptim.Adam(discriminator.parameters()) optimG = optim.Adam(generator.parameters()) def step(real_sample): # (1) Update Discriminator errD_real = loss(discriminator(real_sample), real_label) errD_real.backward() fake = generator(get_noise()) errD_fake = loss(discriminator(fake.detach(), fake_label) errD_fake.backward() optimD.step() # (2) Update Generator errG = loss(discriminator(fake), real_label) errG.backward() optimG.step() Listing 2: Simplified training of a generative adversarial networks.\n\nFigure 1 :\n1A trace of the first few operators of Resnet-50. The top row depicts the execution of the control flow running on the host CPU. The gray areas are Python code executed by its interpreter. The colored areas correspond to the work done on the host CPU to queue various operators (convolution, batch normalization, and so on). The bottom row shows the corresponding execution of those operators on the GPU. The arrows pair the two events in time.\n\nFigure 2 :\n2Annotated traces of the execution of ResNet-50 on GPU.\n\nTable 1 :\n1Training speed for 6 models using 32bit floats. Throughput is measured in images per second for the AlexNet, VGG-19, ResNet-50, and MobileNet models, in tokens per second for the GNMTv2 model, and in samples per second for the NCF model. The fastest speed for each model is shown in bold.Framework \nThroughput (higher is better) \nAlexNet \nVGG-19 ResNet-50 MobileNet \nGNMTv2 \nNCF \n\nChainer \n778 \u00b1 15 \nN/A \n219 \u00b1 1 \nN/A \nN/A \nN/A \nCNTK \n845 \u00b1 8 \n84 \u00b1 3 \n210 \u00b1 1 \nN/A \nN/A \nN/A \nMXNet \n1554 \u00b1 22 \n113 \u00b1 1 \n218 \u00b1 2 \n444 \u00b1 2 \nN/A \nN/A \nPaddlePaddle \n933 \u00b1 123 \n112 \u00b1 2 \n192 \u00b1 4 \n557 \u00b1 24 \nN/A \nN/A \nTensorFlow \n1422 \u00b1 27 \n66 \u00b1 2 \n200 \u00b1 1 \n216 \u00b1 15 \n9631 \u00b1 1.3% \n4.8e6 \u00b1 2.9% \nPyTorch \n1547 \u00b1 316 119 \u00b1 1 \n212 \u00b1 2 \n463 \u00b1 17 \n15512 \u00b1 4.8% 5.4e6 \u00b1 3.4% \n\n\nAcknowledgementsWe are grateful to the PyTorch community for their feedback and contributions that greatly influenced the design and implementation of PyTorch. We thank all the PyTorch core team members, contributors and package maintainers including Ailing Zhang, Alex Suhan, Alfredo Mendoza, Alican Bozkurt, Andrew Tulloch, Ansha Yu,Anthony\ncaffe: Convolutional architecture for fast feature embedding. &quot; Yangqing, Evan Jia, Jeff Shelhamer, Sergey Donahue, Jonathan Karayev, Ross Long, Sergio Girshick, Trevor &quot; Guadarrama, Darrell, arXiv:1408.5093arXiv preprintYangqing \"Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor\" Darrell. \"caffe: Convolutional architecture for fast feature embedding\". \"arXiv preprint arXiv:1408.5093\", \"2014\".\n\nCntk: Microsoft's open-source deep-learning toolkit. Frank Seide, Amit Agarwal, Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16. the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16New York, NY, USAACMFrank Seide and Amit Agarwal. Cntk: Microsoft's open-source deep-learning toolkit. In Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pages 2135-2135, New York, NY, USA, 2016. ACM.\n\nTensorFlow: Largescale machine learning on heterogeneous systems. Mart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9. Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gasYuan Yu, and Xiaoqiang ZhengOriol Vinyals. Software available from tensorflow.orgMart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large- scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.\n\nTheano: A Python framework for fast computation of mathematical expressions. abs/1605.02688Theano Development Team. Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints, abs/1605.02688, May 2016.\n\nChainer: a next-generation open source framework for deep learning. Seiya Tokui, Kenta Oono, Shohei Hido, Justin Clayton, Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS). Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open source framework for deep learning. In Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015.\n\nTorch: a modular machine learning software library. Ronan Collobert, Samy Bengio, Johnny Mari\u00e9thoz, IdiapTechnical reportRonan Collobert, Samy Bengio, and Johnny Mari\u00e9thoz. Torch: a modular machine learning software library. Technical report, Idiap, 2002.\n\nG Neubig, C Dyer, Y Goldberg, A Matthews, W Ammar, A Anastasopoulos, M Ballesteros, D Chiang, D Clothiaux, T Cohn, K Duh, M Faruqui, C Gan, D Garrette, Y Ji, L Kong, A Kuncoro, G Kumar, C Malaviya, P Michel, Y Oda, M Richardson, N Saphra, S Swayamdipta, P Yin, Dynet, The Dynamic Neural Network Toolkit. ArXiv e-printsG. Neubig, C. Dyer, Y. Goldberg, A. Matthews, W. Ammar, A. Anastasopoulos, M. Balles- teros, D. Chiang, D. Clothiaux, T. Cohn, K. Duh, M. Faruqui, C. Gan, D. Garrette, Y. Ji, L. Kong, A. Kuncoro, G. Kumar, C. Malaviya, P. Michel, Y. Oda, M. Richardson, N. Saphra, S. Swayamdipta, and P. Yin. DyNet: The Dynamic Neural Network Toolkit. ArXiv e-prints, January 2017.\n\nAn APL Machine. Philip S Abrams, Stanford UniversityPhD thesisPhilip S. Abrams. An APL Machine. PhD thesis, Stanford University, 1970.\n\n. The Mathworks, Inc, Natick, Massachusetts, United StatesMATLAB and Statistics ToolboxThe MathWorks, Inc., Natick, Massachusetts, United States. MATLAB and Statistics Toolbox.\n\nR Core Team, R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. Vienna, AustriaR Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.\n\nJulia: A fresh approach to numerical computing. Jeff Bezanson, Alan Edelman, Stefan Karpinski, Shah, SIAM review. 591Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to numerical computing. SIAM review, 59(1):65-98, 2017.\n\nNumPy: A guide to NumPy. Travis Oliphant, Trelgol PublishingUSATravis Oliphant. NumPy: A guide to NumPy. USA: Trelgol Publishing, 2006. http://www.numpy.org/.\n\nEigen v3. Ga\u00ebl Guennebaud, Beno\u00eet Jacob, Ga\u00ebl Guennebaud, Beno\u00eet Jacob, et al. Eigen v3. http://eigen.tuxfamily.org, 2010.\n\nLush reference manual. Y Lecun, Bottou, Technical reportY LeCun and L Bottou. Lush reference manual. Technical report, code available at http://lush.sourceforge.net, 2002.\n\nAutomatic differentiation in machine learning: A survey. Atilim Gunes Baydin, A Barak, Alexey Pearlmutter, Jeffrey Mark Andreyevich Radul, Siskind, J. Mach. Learn. Res. 181Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: A survey. J. Mach. Learn. Res., 18(1):5595-5637, January 2017.\n\nDougal Maclaurin. Modeling, Inference and Optimization with Composable Differentiable Procedures. Harvard UniversityPhD thesisDougal Maclaurin. Modeling, Inference and Optimization with Composable Differentiable Procedures. PhD thesis, Harvard University, April 2016.\n\n. Matthew Johnson, Matthew Johnson et. al. Jax. https://github.com/google/jax, 2018.\n\n. Mike Innes, Mike Innes et. al. Flux.jl. https://github.com/FluxML/Flux.jl, 2018.\n\nOpen source scientific tools for Python. Eric Jones, Travis Oliphant, Pearu Peterson, Eric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientific tools for Python, 2001-. http://www.scipy.org/.\n\nData structures for statistical computing in python. Wes Mckinney, Proceedings of the 9th Python in Science Conference. the 9th Python in Science ConferenceWes McKinney. Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference, 51-56, 2010.\n\nEblearn: Open-source energy-based learning in c++. Pierre Sermanet, Koray Kavukcuoglu, Yann Lecun, 21st IEEE International Conference on Tools with Artificial Intelligence. IEEEPierre Sermanet, Koray Kavukcuoglu, and Yann LeCun. Eblearn: Open-source energy-based learning in c++. In 2009 21st IEEE International Conference on Tools with Artificial Intelligence, pages 693-697. IEEE, 2009.\n\ncudnn: Efficient primitives for deep learning. Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan D Cohen, John Tran, Bryan Catanzaro, Evan Shelhamer, abs/1410.0759CoRRSharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan D. Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. cudnn: Efficient primitives for deep learning. CoRR, abs/1410.0759, 2014.\n\nmaxdnn: An efficient convolution kernel for deep learning with maxwell gpus. Andrew Lavin, Andrew Lavin. maxdnn: An efficient convolution kernel for deep learning with maxwell gpus, January 2015.\n\nFast algorithms for convolutional neural networks. Andrew Lavin, Scott Gray, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Andrew Lavin and Scott Gray. Fast algorithms for convolutional neural networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4013-4021, 2016.\n\nTorch7: A matlab-like environment for machine learning. Ronan Collobert, Koray Kavukcuoglu, Cl\u00e9ment Farabet, NIPS 2011. Ronan Collobert, Koray Kavukcuoglu, and Cl\u00e9ment Farabet. Torch7: A matlab-like environment for machine learning. In NIPS 2011, 2011.\n\nRichard Gabriel, The rise of worse is better. Richard Gabriel. The rise of worse is better. http://dreamsongs.com/RiseOfWorseIsBetter.html.\n\nMNIST handwritten digit database. Yann Lecun, Corinna Cortes, Yann LeCun and Corinna Cortes. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/.\n\n. Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K\u00fcttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, David Hado Van Hasselt, Timothy P Silver, Kevin Lillicrap, Paul Calderone, Anthony Keet, David Brunasso, Anders Lawrence, Jacob Ekermo, Rodney Repp, Tsing, Starcraft II: A new challenge for reinforcement learning. CoRR, abs/1708.04782Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K\u00fcttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for reinforcement learning. CoRR, abs/1708.04782, 2017.\n\nDmlc, Dlpack, Open in memory tensor structure. DMLC. Dlpack: Open in memory tensor structure. https://github.com/dmlc/dlpack.\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NIPS Workshop. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In NIPS Workshop, 2017.\n\nAutomatic differentiation, C++ templates, and photogrammetry. Dan Piponi, GPU, & Game Tools. 9Dan Piponi. Automatic differentiation, C++ templates, and photogrammetry. J. Graphics, GPU, & Game Tools, 9(4):41-55, 2004.\n\nAutomatic differentiation facilitates of-integration into steering-angle-based road vehicle tracking. Holger Leuck, Hans-Hellmut Nagel, Conference on Computer Vision and Pattern Recognition (CVPR '99. Ft. Collins, CO, USAHolger Leuck and Hans-Hellmut Nagel. Automatic differentiation facilitates of-integration into steering-angle-based road vehicle tracking. In 1999 Conference on Computer Vision and Pattern Recognition (CVPR '99), 23-25 June 1999, Ft. Collins, CO, USA, pages 2360-2365, 1999.\n\n. Giovanni Petrantoni, J\u00f6rg Wollenschl\u00e4ger, Giovanni Petrantoni and J\u00f6rg Wollenschl\u00e4ger. Nimtorch. https://github.com/fragcolor- xyz/nimtorch.\n\n. Austin Huang, Junji Hashimoto, Sam Stites, Hasktorch, Austin Huang, Junji Hashimoto, and Sam Stites. Hasktorch. https://github.com/hasktorch/hasktorch.\n\nForward modeling for partial observation strategy games -a starcraft defogger. G Synnaeve, Z Lin, J Gehring, D Gant, V Mella, V Khalidov, N Carion, N Usunier, Advances in Neural Information Processing Systems. G. Synnaeve, Z. Lin, J. Gehring, D. Gant, V. Mella, V. Khalidov, N. Carion, and N. Usunier. Forward modeling for partial observation strategy games -a starcraft defogger. In Advances in Neural Information Processing Systems, pages 10761-10771, 2018.\n\n. The PyTorch team. Torch Script. The PyTorch team. Torch Script. https://pytorch.org/docs/stable/jit.html.\n\nJustin Luitjens, Cuda streams. GPU technology conference. Justin Luitjens. Cuda streams. GPU technology conference, 2014.\n\nHoard: A scalable memory allocator for multithreaded applications. D Emery, Kathryn S Berger, Robert D Mckinley, Paul R Blumofe, Wilson, Proceedings of the Ninth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS IX. the Ninth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS IXNew York, NY, USAACMEmery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson. Hoard: A scalable memory allocator for multithreaded applications. In Proceedings of the Ninth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS IX, pages 117-128, New York, NY, USA, 2000. ACM.\n\nA scalable concurrent malloc(3) implementation for freebsd. J Evans, BSDCan -The Technical BSD Conference. J. Evans. A scalable concurrent malloc(3) implementation for freebsd. In In BSDCan -The Technical BSD Conference, May 2006.\n\nTcmalloc: Thread-caching malloc. S Ghemawat, P Menage, S. Ghemawat and P. Menage. Tcmalloc: Thread-caching malloc.\n\nHogwild: A lock-free approach to parallelizing stochastic gradient descent. Benjamin Recht, Christopher R\u00e9, Stephen J Wright, Feng Niu, Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems. Granada, SpainBenjamin Recht, Christopher R\u00e9, Stephen J. Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain., pages 693-701, 2011.\n\nQuantifying the performance of garbage collection vs. explicit memory management. Matthew Hertz, Emery D Berger, Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications, OOPSLA '05. the 20th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications, OOPSLA '05New York, NY, USAACMMatthew Hertz and Emery D. Berger. Quantifying the performance of garbage collection vs. explicit memory management. In Proceedings of the 20th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications, OOPSLA '05, pages 313-326, New York, NY, USA, 2005. ACM.\n\nThe PyTorch team. Pytorch Autograd Profiler. The PyTorch team. Pytorch Autograd Profiler. https://pytorch.org/docs/1.0.1/autograd.html#profiler.\n", "annotations": {"author": "[{\"end\":118,\"start\":84},{\"end\":143,\"start\":119},{\"end\":174,\"start\":144},{\"end\":200,\"start\":175},{\"end\":223,\"start\":201},{\"end\":254,\"start\":224},{\"end\":297,\"start\":255},{\"end\":321,\"start\":298},{\"end\":364,\"start\":322},{\"end\":400,\"start\":365},{\"end\":417,\"start\":401},{\"end\":460,\"start\":418},{\"end\":487,\"start\":461},{\"end\":524,\"start\":488},{\"end\":545,\"start\":525},{\"end\":561,\"start\":546},{\"end\":611,\"start\":562},{\"end\":620,\"start\":612},{\"end\":657,\"start\":621},{\"end\":675,\"start\":658},{\"end\":696,\"start\":676},{\"end\":732,\"start\":697},{\"end\":888,\"start\":733},{\"end\":993,\"start\":889}]", "publisher": null, "author_last_name": "[{\"end\":95,\"start\":89},{\"end\":128,\"start\":123},{\"end\":159,\"start\":154},{\"end\":185,\"start\":180},{\"end\":222,\"start\":207},{\"end\":238,\"start\":232},{\"end\":269,\"start\":262},{\"end\":308,\"start\":305},{\"end\":340,\"start\":330},{\"end\":376,\"start\":370},{\"end\":416,\"start\":407},{\"end\":430,\"start\":426},{\"end\":472,\"start\":468},{\"end\":499,\"start\":493},{\"end\":544,\"start\":532},{\"end\":560,\"start\":554},{\"end\":581,\"start\":569},{\"end\":619,\"start\":617},{\"end\":635,\"start\":628},{\"end\":674,\"start\":666},{\"end\":695,\"start\":687},{\"end\":713,\"start\":705}]", "author_first_name": "[{\"end\":88,\"start\":84},{\"end\":122,\"start\":119},{\"end\":153,\"start\":144},{\"end\":179,\"start\":175},{\"end\":206,\"start\":201},{\"end\":231,\"start\":224},{\"end\":261,\"start\":255},{\"end\":304,\"start\":298},{\"end\":329,\"start\":322},{\"end\":369,\"start\":365},{\"end\":406,\"start\":401},{\"end\":425,\"start\":418},{\"end\":467,\"start\":461},{\"end\":492,\"start\":488},{\"end\":531,\"start\":525},{\"end\":553,\"start\":546},{\"end\":568,\"start\":562},{\"end\":616,\"start\":612},{\"end\":627,\"start\":621},{\"end\":660,\"start\":658},{\"end\":665,\"start\":661},{\"end\":682,\"start\":676},{\"end\":686,\"start\":683},{\"end\":704,\"start\":697}]", "author_affiliation": "[{\"end\":887,\"start\":734},{\"end\":992,\"start\":890}]", "title": "[{\"end\":69,\"start\":1},{\"end\":1062,\"start\":994}]", "venue": "[{\"end\":1120,\"start\":1064}]", "abstract": "[{\"end\":2111,\"start\":1151}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2285,\"start\":2282},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2295,\"start\":2292},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2311,\"start\":2308},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2327,\"start\":2324},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2946,\"start\":2943},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3002,\"start\":2999},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3013,\"start\":3010},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3804,\"start\":3801},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3816,\"start\":3813},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3824,\"start\":3820},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3839,\"start\":3835},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4063,\"start\":4059},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4074,\"start\":4071},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4086,\"start\":4082},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4100,\"start\":4096},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4263,\"start\":4259},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4516,\"start\":4512},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4646,\"start\":4643},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4657,\"start\":4654},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4668,\"start\":4664},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4679,\"start\":4676},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4689,\"start\":4685},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4706,\"start\":4702},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4850,\"start\":4847},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4926,\"start\":4922},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4938,\"start\":4934},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4955,\"start\":4951},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5583,\"start\":5579},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5602,\"start\":5599},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5644,\"start\":5640},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5662,\"start\":5659},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6100,\"start\":6096},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6150,\"start\":6146},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6159,\"start\":6155},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6269,\"start\":6266},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6282,\"start\":6278},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6301,\"start\":6298},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8001,\"start\":7997},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8517,\"start\":8513},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8558,\"start\":8554},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8996,\"start\":8993},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9010,\"start\":9007},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12066,\"start\":12062},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14183,\"start\":14179},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14631,\"start\":14627},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14634,\"start\":14631},{\"end\":16805,\"start\":16801},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17037,\"start\":17033},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17053,\"start\":17049},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17252,\"start\":17248},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17407,\"start\":17403},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17929,\"start\":17925},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18956,\"start\":18952},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18960,\"start\":18956},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18964,\"start\":18960},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22487,\"start\":22483},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23241,\"start\":23237},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24949,\"start\":24945}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28529,\"start\":28487},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29032,\"start\":28530},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29489,\"start\":29033},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29557,\"start\":29490},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30317,\"start\":29558}]", "paragraph": "[{\"end\":2740,\"start\":2127},{\"end\":3049,\"start\":2742},{\"end\":3602,\"start\":3051},{\"end\":3712,\"start\":3617},{\"end\":4204,\"start\":3714},{\"end\":4707,\"start\":4206},{\"end\":5888,\"start\":4709},{\"end\":6351,\"start\":5890},{\"end\":6534,\"start\":6353},{\"end\":6709,\"start\":6556},{\"end\":7106,\"start\":6711},{\"end\":7419,\"start\":7108},{\"end\":7979,\"start\":7421},{\"end\":9272,\"start\":7981},{\"end\":10180,\"start\":9274},{\"end\":10501,\"start\":10409},{\"end\":11518,\"start\":10503},{\"end\":12366,\"start\":11557},{\"end\":12566,\"start\":12368},{\"end\":13349,\"start\":12568},{\"end\":13618,\"start\":13351},{\"end\":14635,\"start\":13648},{\"end\":14830,\"start\":14637},{\"end\":15549,\"start\":14832},{\"end\":16006,\"start\":15588},{\"end\":16198,\"start\":16008},{\"end\":17065,\"start\":16224},{\"end\":17408,\"start\":17067},{\"end\":17820,\"start\":17443},{\"end\":18466,\"start\":17822},{\"end\":18693,\"start\":18468},{\"end\":19487,\"start\":18729},{\"end\":19788,\"start\":19489},{\"end\":20407,\"start\":19790},{\"end\":21000,\"start\":20409},{\"end\":21172,\"start\":21002},{\"end\":21563,\"start\":21192},{\"end\":22009,\"start\":21565},{\"end\":22333,\"start\":22011},{\"end\":22488,\"start\":22335},{\"end\":22793,\"start\":22511},{\"end\":23530,\"start\":22795},{\"end\":23997,\"start\":23532},{\"end\":24482,\"start\":23999},{\"end\":24800,\"start\":24497},{\"end\":25630,\"start\":24826},{\"end\":26295,\"start\":25652},{\"end\":26650,\"start\":26310},{\"end\":26933,\"start\":26652},{\"end\":27752,\"start\":26946},{\"end\":28486,\"start\":27783}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10408,\"start\":10181}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26689,\"start\":26682}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2125,\"start\":2113},{\"attributes\":{\"n\":\"2\"},\"end\":3615,\"start\":3605},{\"attributes\":{\"n\":\"3\"},\"end\":6554,\"start\":6537},{\"attributes\":{\"n\":\"4.2\"},\"end\":11555,\"start\":11521},{\"attributes\":{\"n\":\"4.3\"},\"end\":13646,\"start\":13621},{\"attributes\":{\"n\":\"5\"},\"end\":15586,\"start\":15552},{\"attributes\":{\"n\":\"5.1\"},\"end\":16222,\"start\":16201},{\"attributes\":{\"n\":\"5.2\"},\"end\":17441,\"start\":17411},{\"attributes\":{\"n\":\"5.3\"},\"end\":18727,\"start\":18696},{\"attributes\":{\"n\":\"5.4\"},\"end\":21190,\"start\":21175},{\"attributes\":{\"n\":\"5.5\"},\"end\":22509,\"start\":22491},{\"attributes\":{\"n\":\"6\"},\"end\":24495,\"start\":24485},{\"attributes\":{\"n\":\"6.1\"},\"end\":24824,\"start\":24803},{\"attributes\":{\"n\":\"6.2\"},\"end\":25650,\"start\":25633},{\"attributes\":{\"n\":\"6.3\"},\"end\":26308,\"start\":26298},{\"attributes\":{\"n\":\"6.4\"},\"end\":26944,\"start\":26936},{\"attributes\":{\"n\":\"7\"},\"end\":27781,\"start\":27755},{\"end\":29044,\"start\":29034},{\"end\":29501,\"start\":29491},{\"end\":29568,\"start\":29559}]", "table": "[{\"end\":30317,\"start\":29858}]", "figure_caption": "[{\"end\":28529,\"start\":28489},{\"end\":29032,\"start\":28532},{\"end\":29489,\"start\":29046},{\"end\":29557,\"start\":29503},{\"end\":29858,\"start\":29570}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25057,\"start\":25049},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25851,\"start\":25843},{\"end\":27351,\"start\":27343},{\"end\":27634,\"start\":27626}]", "bib_author_first_name": "[{\"end\":30729,\"start\":30723},{\"end\":30744,\"start\":30740},{\"end\":30754,\"start\":30750},{\"end\":30772,\"start\":30766},{\"end\":30790,\"start\":30782},{\"end\":30804,\"start\":30800},{\"end\":30817,\"start\":30811},{\"end\":30834,\"start\":30828},{\"end\":30841,\"start\":30835},{\"end\":31188,\"start\":31183},{\"end\":31200,\"start\":31196},{\"end\":31746,\"start\":31740},{\"end\":31760,\"start\":31754},{\"end\":31774,\"start\":31770},{\"end\":31789,\"start\":31783},{\"end\":31805,\"start\":31798},{\"end\":31817,\"start\":31812},{\"end\":31829,\"start\":31825},{\"end\":31831,\"start\":31830},{\"end\":31845,\"start\":31841},{\"end\":31860,\"start\":31853},{\"end\":31875,\"start\":31867},{\"end\":31889,\"start\":31883},{\"end\":31903,\"start\":31900},{\"end\":31922,\"start\":31916},{\"end\":31937,\"start\":31929},{\"end\":31953,\"start\":31946},{\"end\":31969,\"start\":31961},{\"end\":31980,\"start\":31975},{\"end\":31999,\"start\":31993},{\"end\":32017,\"start\":32008},{\"end\":33289,\"start\":33284},{\"end\":33302,\"start\":33297},{\"end\":33315,\"start\":33309},{\"end\":33328,\"start\":33322},{\"end\":33973,\"start\":33968},{\"end\":33989,\"start\":33985},{\"end\":34004,\"start\":33998},{\"end\":34174,\"start\":34173},{\"end\":34184,\"start\":34183},{\"end\":34192,\"start\":34191},{\"end\":34204,\"start\":34203},{\"end\":34216,\"start\":34215},{\"end\":34225,\"start\":34224},{\"end\":34243,\"start\":34242},{\"end\":34258,\"start\":34257},{\"end\":34268,\"start\":34267},{\"end\":34281,\"start\":34280},{\"end\":34289,\"start\":34288},{\"end\":34296,\"start\":34295},{\"end\":34307,\"start\":34306},{\"end\":34314,\"start\":34313},{\"end\":34326,\"start\":34325},{\"end\":34332,\"start\":34331},{\"end\":34340,\"start\":34339},{\"end\":34351,\"start\":34350},{\"end\":34360,\"start\":34359},{\"end\":34372,\"start\":34371},{\"end\":34382,\"start\":34381},{\"end\":34389,\"start\":34388},{\"end\":34403,\"start\":34402},{\"end\":34413,\"start\":34412},{\"end\":34428,\"start\":34427},{\"end\":34879,\"start\":34873},{\"end\":34881,\"start\":34880},{\"end\":35476,\"start\":35472},{\"end\":35491,\"start\":35487},{\"end\":35507,\"start\":35501},{\"end\":35718,\"start\":35712},{\"end\":35861,\"start\":35857},{\"end\":35880,\"start\":35874},{\"end\":35995,\"start\":35994},{\"end\":36223,\"start\":36222},{\"end\":36237,\"start\":36231},{\"end\":36258,\"start\":36251},{\"end\":36263,\"start\":36259},{\"end\":36800,\"start\":36793},{\"end\":36883,\"start\":36879},{\"end\":37006,\"start\":37002},{\"end\":37020,\"start\":37014},{\"end\":37036,\"start\":37031},{\"end\":37234,\"start\":37231},{\"end\":37528,\"start\":37522},{\"end\":37544,\"start\":37539},{\"end\":37562,\"start\":37558},{\"end\":37914,\"start\":37908},{\"end\":37929,\"start\":37924},{\"end\":37947,\"start\":37939},{\"end\":37970,\"start\":37962},{\"end\":37972,\"start\":37971},{\"end\":37984,\"start\":37980},{\"end\":37996,\"start\":37991},{\"end\":38012,\"start\":38008},{\"end\":38320,\"start\":38314},{\"end\":38491,\"start\":38485},{\"end\":38504,\"start\":38499},{\"end\":38815,\"start\":38810},{\"end\":38832,\"start\":38827},{\"end\":38853,\"start\":38846},{\"end\":39015,\"start\":39008},{\"end\":39187,\"start\":39183},{\"end\":39202,\"start\":39195},{\"end\":39319,\"start\":39314},{\"end\":39333,\"start\":39329},{\"end\":39348,\"start\":39342},{\"end\":39364,\"start\":39359},{\"end\":39384,\"start\":39375},{\"end\":39390,\"start\":39385},{\"end\":39411,\"start\":39403},{\"end\":39424,\"start\":39417},{\"end\":39443,\"start\":39435},{\"end\":39457,\"start\":39453},{\"end\":39473,\"start\":39467},{\"end\":39493,\"start\":39489},{\"end\":39507,\"start\":39500},{\"end\":39521,\"start\":39517},{\"end\":39537,\"start\":39532},{\"end\":39551,\"start\":39548},{\"end\":39565,\"start\":39560},{\"end\":39591,\"start\":39584},{\"end\":39593,\"start\":39592},{\"end\":39607,\"start\":39602},{\"end\":39623,\"start\":39619},{\"end\":39642,\"start\":39635},{\"end\":39654,\"start\":39649},{\"end\":39671,\"start\":39665},{\"end\":39687,\"start\":39682},{\"end\":39702,\"start\":39696},{\"end\":40457,\"start\":40453},{\"end\":40469,\"start\":40466},{\"end\":40484,\"start\":40477},{\"end\":40502,\"start\":40495},{\"end\":40517,\"start\":40511},{\"end\":40531,\"start\":40524},{\"end\":40546,\"start\":40540},{\"end\":40557,\"start\":40552},{\"end\":40573,\"start\":40569},{\"end\":40586,\"start\":40582},{\"end\":40882,\"start\":40879},{\"end\":41144,\"start\":41138},{\"end\":41164,\"start\":41152},{\"end\":41543,\"start\":41535},{\"end\":41560,\"start\":41556},{\"end\":41685,\"start\":41679},{\"end\":41698,\"start\":41693},{\"end\":41713,\"start\":41710},{\"end\":41912,\"start\":41911},{\"end\":41924,\"start\":41923},{\"end\":41931,\"start\":41930},{\"end\":41942,\"start\":41941},{\"end\":41950,\"start\":41949},{\"end\":41959,\"start\":41958},{\"end\":41971,\"start\":41970},{\"end\":41981,\"start\":41980},{\"end\":42408,\"start\":42402},{\"end\":42593,\"start\":42592},{\"end\":42608,\"start\":42601},{\"end\":42610,\"start\":42609},{\"end\":42625,\"start\":42619},{\"end\":42627,\"start\":42626},{\"end\":42642,\"start\":42638},{\"end\":42644,\"start\":42643},{\"end\":43324,\"start\":43323},{\"end\":43529,\"start\":43528},{\"end\":43541,\"start\":43540},{\"end\":43695,\"start\":43687},{\"end\":43714,\"start\":43703},{\"end\":43726,\"start\":43719},{\"end\":43728,\"start\":43727},{\"end\":43741,\"start\":43737},{\"end\":44327,\"start\":44320},{\"end\":44340,\"start\":44335},{\"end\":44342,\"start\":44341}]", "bib_author_last_name": "[{\"end\":30738,\"start\":30730},{\"end\":30748,\"start\":30745},{\"end\":30764,\"start\":30755},{\"end\":30780,\"start\":30773},{\"end\":30798,\"start\":30791},{\"end\":30809,\"start\":30805},{\"end\":30826,\"start\":30818},{\"end\":30852,\"start\":30842},{\"end\":30861,\"start\":30854},{\"end\":31194,\"start\":31189},{\"end\":31208,\"start\":31201},{\"end\":31752,\"start\":31747},{\"end\":31768,\"start\":31761},{\"end\":31781,\"start\":31775},{\"end\":31796,\"start\":31790},{\"end\":31810,\"start\":31806},{\"end\":31823,\"start\":31818},{\"end\":31839,\"start\":31832},{\"end\":31851,\"start\":31846},{\"end\":31865,\"start\":31861},{\"end\":31881,\"start\":31876},{\"end\":31898,\"start\":31890},{\"end\":31914,\"start\":31904},{\"end\":31927,\"start\":31923},{\"end\":31944,\"start\":31938},{\"end\":31959,\"start\":31954},{\"end\":31973,\"start\":31970},{\"end\":31991,\"start\":31981},{\"end\":32006,\"start\":32000},{\"end\":32024,\"start\":32018},{\"end\":33295,\"start\":33290},{\"end\":33307,\"start\":33303},{\"end\":33320,\"start\":33316},{\"end\":33336,\"start\":33329},{\"end\":33983,\"start\":33974},{\"end\":33996,\"start\":33990},{\"end\":34014,\"start\":34005},{\"end\":34181,\"start\":34175},{\"end\":34189,\"start\":34185},{\"end\":34201,\"start\":34193},{\"end\":34213,\"start\":34205},{\"end\":34222,\"start\":34217},{\"end\":34240,\"start\":34226},{\"end\":34255,\"start\":34244},{\"end\":34265,\"start\":34259},{\"end\":34278,\"start\":34269},{\"end\":34286,\"start\":34282},{\"end\":34293,\"start\":34290},{\"end\":34304,\"start\":34297},{\"end\":34311,\"start\":34308},{\"end\":34323,\"start\":34315},{\"end\":34329,\"start\":34327},{\"end\":34337,\"start\":34333},{\"end\":34348,\"start\":34341},{\"end\":34357,\"start\":34352},{\"end\":34369,\"start\":34361},{\"end\":34379,\"start\":34373},{\"end\":34386,\"start\":34383},{\"end\":34400,\"start\":34390},{\"end\":34410,\"start\":34404},{\"end\":34425,\"start\":34414},{\"end\":34432,\"start\":34429},{\"end\":34439,\"start\":34434},{\"end\":34888,\"start\":34882},{\"end\":35008,\"start\":34995},{\"end\":35013,\"start\":35010},{\"end\":35182,\"start\":35171},{\"end\":35485,\"start\":35477},{\"end\":35499,\"start\":35492},{\"end\":35517,\"start\":35508},{\"end\":35523,\"start\":35519},{\"end\":35727,\"start\":35719},{\"end\":35872,\"start\":35862},{\"end\":35886,\"start\":35881},{\"end\":36001,\"start\":35996},{\"end\":36009,\"start\":36003},{\"end\":36220,\"start\":36201},{\"end\":36229,\"start\":36224},{\"end\":36249,\"start\":36238},{\"end\":36281,\"start\":36264},{\"end\":36290,\"start\":36283},{\"end\":36808,\"start\":36801},{\"end\":36889,\"start\":36884},{\"end\":37012,\"start\":37007},{\"end\":37029,\"start\":37021},{\"end\":37045,\"start\":37037},{\"end\":37243,\"start\":37235},{\"end\":37537,\"start\":37529},{\"end\":37556,\"start\":37545},{\"end\":37568,\"start\":37563},{\"end\":37922,\"start\":37915},{\"end\":37937,\"start\":37930},{\"end\":37960,\"start\":37948},{\"end\":37978,\"start\":37973},{\"end\":37989,\"start\":37985},{\"end\":38006,\"start\":37997},{\"end\":38022,\"start\":38013},{\"end\":38326,\"start\":38321},{\"end\":38497,\"start\":38492},{\"end\":38509,\"start\":38505},{\"end\":38825,\"start\":38816},{\"end\":38844,\"start\":38833},{\"end\":38861,\"start\":38854},{\"end\":39023,\"start\":39016},{\"end\":39193,\"start\":39188},{\"end\":39209,\"start\":39203},{\"end\":39327,\"start\":39320},{\"end\":39340,\"start\":39334},{\"end\":39357,\"start\":39349},{\"end\":39373,\"start\":39365},{\"end\":39401,\"start\":39391},{\"end\":39415,\"start\":39412},{\"end\":39433,\"start\":39425},{\"end\":39451,\"start\":39444},{\"end\":39465,\"start\":39458},{\"end\":39487,\"start\":39474},{\"end\":39498,\"start\":39494},{\"end\":39515,\"start\":39508},{\"end\":39530,\"start\":39522},{\"end\":39546,\"start\":39538},{\"end\":39558,\"start\":39552},{\"end\":39582,\"start\":39566},{\"end\":39600,\"start\":39594},{\"end\":39617,\"start\":39608},{\"end\":39633,\"start\":39624},{\"end\":39647,\"start\":39643},{\"end\":39663,\"start\":39655},{\"end\":39680,\"start\":39672},{\"end\":39694,\"start\":39688},{\"end\":39707,\"start\":39703},{\"end\":39714,\"start\":39709},{\"end\":40292,\"start\":40288},{\"end\":40300,\"start\":40294},{\"end\":40464,\"start\":40458},{\"end\":40475,\"start\":40470},{\"end\":40493,\"start\":40485},{\"end\":40509,\"start\":40503},{\"end\":40522,\"start\":40518},{\"end\":40538,\"start\":40532},{\"end\":40550,\"start\":40547},{\"end\":40567,\"start\":40558},{\"end\":40580,\"start\":40574},{\"end\":40592,\"start\":40587},{\"end\":40889,\"start\":40883},{\"end\":41150,\"start\":41145},{\"end\":41170,\"start\":41165},{\"end\":41554,\"start\":41544},{\"end\":41575,\"start\":41561},{\"end\":41691,\"start\":41686},{\"end\":41708,\"start\":41699},{\"end\":41720,\"start\":41714},{\"end\":41731,\"start\":41722},{\"end\":41921,\"start\":41913},{\"end\":41928,\"start\":41925},{\"end\":41939,\"start\":41932},{\"end\":41947,\"start\":41943},{\"end\":41956,\"start\":41951},{\"end\":41968,\"start\":41960},{\"end\":41978,\"start\":41972},{\"end\":41989,\"start\":41982},{\"end\":42417,\"start\":42409},{\"end\":42599,\"start\":42594},{\"end\":42617,\"start\":42611},{\"end\":42636,\"start\":42628},{\"end\":42652,\"start\":42645},{\"end\":42660,\"start\":42654},{\"end\":43330,\"start\":43325},{\"end\":43538,\"start\":43530},{\"end\":43548,\"start\":43542},{\"end\":43701,\"start\":43696},{\"end\":43717,\"start\":43715},{\"end\":43735,\"start\":43729},{\"end\":43745,\"start\":43742},{\"end\":44333,\"start\":44328},{\"end\":44349,\"start\":44343}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1408.5093\",\"id\":\"b0\"},\"end\":31128,\"start\":30661},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":38063112},\"end\":31672,\"start\":31130},{\"attributes\":{\"id\":\"b2\"},\"end\":32953,\"start\":31674},{\"attributes\":{\"doi\":\"abs/1605.02688\",\"id\":\"b3\",\"matched_paper_id\":8993325},\"end\":33214,\"start\":32955},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":40821847},\"end\":33914,\"start\":33216},{\"attributes\":{\"id\":\"b5\"},\"end\":34171,\"start\":33916},{\"attributes\":{\"id\":\"b6\"},\"end\":34855,\"start\":34173},{\"attributes\":{\"id\":\"b7\"},\"end\":34991,\"start\":34857},{\"attributes\":{\"id\":\"b8\"},\"end\":35169,\"start\":34993},{\"attributes\":{\"id\":\"b9\"},\"end\":35422,\"start\":35171},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13026838},\"end\":35685,\"start\":35424},{\"attributes\":{\"id\":\"b11\"},\"end\":35845,\"start\":35687},{\"attributes\":{\"id\":\"b12\"},\"end\":35969,\"start\":35847},{\"attributes\":{\"id\":\"b13\"},\"end\":36142,\"start\":35971},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3766791},\"end\":36520,\"start\":36144},{\"attributes\":{\"id\":\"b15\"},\"end\":36789,\"start\":36522},{\"attributes\":{\"id\":\"b16\"},\"end\":36875,\"start\":36791},{\"attributes\":{\"id\":\"b17\"},\"end\":36959,\"start\":36877},{\"attributes\":{\"id\":\"b18\"},\"end\":37176,\"start\":36961},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":13156234},\"end\":37469,\"start\":37178},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":15064817},\"end\":37859,\"start\":37471},{\"attributes\":{\"doi\":\"abs/1410.0759\",\"id\":\"b21\"},\"end\":38235,\"start\":37861},{\"attributes\":{\"id\":\"b22\"},\"end\":38432,\"start\":38237},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":962822},\"end\":38752,\"start\":38434},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14365368},\"end\":39006,\"start\":38754},{\"attributes\":{\"id\":\"b25\"},\"end\":39147,\"start\":39008},{\"attributes\":{\"id\":\"b26\"},\"end\":39310,\"start\":39149},{\"attributes\":{\"id\":\"b27\"},\"end\":40286,\"start\":39312},{\"attributes\":{\"id\":\"b28\"},\"end\":40413,\"start\":40288},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":40027675},\"end\":40815,\"start\":40415},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7571335},\"end\":41034,\"start\":40817},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13675399},\"end\":41531,\"start\":41036},{\"attributes\":{\"id\":\"b32\"},\"end\":41675,\"start\":41533},{\"attributes\":{\"id\":\"b33\"},\"end\":41830,\"start\":41677},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":54083409},\"end\":42291,\"start\":41832},{\"attributes\":{\"id\":\"b35\"},\"end\":42400,\"start\":42293},{\"attributes\":{\"id\":\"b36\"},\"end\":42523,\"start\":42402},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":253918},\"end\":43261,\"start\":42525},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":15428939},\"end\":43493,\"start\":43263},{\"attributes\":{\"id\":\"b39\"},\"end\":43609,\"start\":43495},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6108215},\"end\":44236,\"start\":43611},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":6570650},\"end\":44926,\"start\":44238},{\"attributes\":{\"id\":\"b42\"},\"end\":45072,\"start\":44928}]", "bib_title": "[{\"end\":31181,\"start\":31130},{\"end\":31738,\"start\":31674},{\"end\":33030,\"start\":32955},{\"end\":33282,\"start\":33216},{\"end\":35470,\"start\":35424},{\"end\":36199,\"start\":36144},{\"end\":37229,\"start\":37178},{\"end\":37520,\"start\":37471},{\"end\":38483,\"start\":38434},{\"end\":38808,\"start\":38754},{\"end\":40451,\"start\":40415},{\"end\":40877,\"start\":40817},{\"end\":41136,\"start\":41036},{\"end\":41909,\"start\":41832},{\"end\":42590,\"start\":42525},{\"end\":43321,\"start\":43263},{\"end\":43685,\"start\":43611},{\"end\":44318,\"start\":44238}]", "bib_author": "[{\"end\":30740,\"start\":30723},{\"end\":30750,\"start\":30740},{\"end\":30766,\"start\":30750},{\"end\":30782,\"start\":30766},{\"end\":30800,\"start\":30782},{\"end\":30811,\"start\":30800},{\"end\":30828,\"start\":30811},{\"end\":30854,\"start\":30828},{\"end\":30863,\"start\":30854},{\"end\":31196,\"start\":31183},{\"end\":31210,\"start\":31196},{\"end\":31754,\"start\":31740},{\"end\":31770,\"start\":31754},{\"end\":31783,\"start\":31770},{\"end\":31798,\"start\":31783},{\"end\":31812,\"start\":31798},{\"end\":31825,\"start\":31812},{\"end\":31841,\"start\":31825},{\"end\":31853,\"start\":31841},{\"end\":31867,\"start\":31853},{\"end\":31883,\"start\":31867},{\"end\":31900,\"start\":31883},{\"end\":31916,\"start\":31900},{\"end\":31929,\"start\":31916},{\"end\":31946,\"start\":31929},{\"end\":31961,\"start\":31946},{\"end\":31975,\"start\":31961},{\"end\":31993,\"start\":31975},{\"end\":32008,\"start\":31993},{\"end\":32026,\"start\":32008},{\"end\":33297,\"start\":33284},{\"end\":33309,\"start\":33297},{\"end\":33322,\"start\":33309},{\"end\":33338,\"start\":33322},{\"end\":33985,\"start\":33968},{\"end\":33998,\"start\":33985},{\"end\":34016,\"start\":33998},{\"end\":34183,\"start\":34173},{\"end\":34191,\"start\":34183},{\"end\":34203,\"start\":34191},{\"end\":34215,\"start\":34203},{\"end\":34224,\"start\":34215},{\"end\":34242,\"start\":34224},{\"end\":34257,\"start\":34242},{\"end\":34267,\"start\":34257},{\"end\":34280,\"start\":34267},{\"end\":34288,\"start\":34280},{\"end\":34295,\"start\":34288},{\"end\":34306,\"start\":34295},{\"end\":34313,\"start\":34306},{\"end\":34325,\"start\":34313},{\"end\":34331,\"start\":34325},{\"end\":34339,\"start\":34331},{\"end\":34350,\"start\":34339},{\"end\":34359,\"start\":34350},{\"end\":34371,\"start\":34359},{\"end\":34381,\"start\":34371},{\"end\":34388,\"start\":34381},{\"end\":34402,\"start\":34388},{\"end\":34412,\"start\":34402},{\"end\":34427,\"start\":34412},{\"end\":34434,\"start\":34427},{\"end\":34441,\"start\":34434},{\"end\":34890,\"start\":34873},{\"end\":35010,\"start\":34995},{\"end\":35015,\"start\":35010},{\"end\":35184,\"start\":35171},{\"end\":35487,\"start\":35472},{\"end\":35501,\"start\":35487},{\"end\":35519,\"start\":35501},{\"end\":35525,\"start\":35519},{\"end\":35729,\"start\":35712},{\"end\":35874,\"start\":35857},{\"end\":35888,\"start\":35874},{\"end\":36003,\"start\":35994},{\"end\":36011,\"start\":36003},{\"end\":36222,\"start\":36201},{\"end\":36231,\"start\":36222},{\"end\":36251,\"start\":36231},{\"end\":36283,\"start\":36251},{\"end\":36292,\"start\":36283},{\"end\":36810,\"start\":36793},{\"end\":36891,\"start\":36879},{\"end\":37014,\"start\":37002},{\"end\":37031,\"start\":37014},{\"end\":37047,\"start\":37031},{\"end\":37245,\"start\":37231},{\"end\":37539,\"start\":37522},{\"end\":37558,\"start\":37539},{\"end\":37570,\"start\":37558},{\"end\":37924,\"start\":37908},{\"end\":37939,\"start\":37924},{\"end\":37962,\"start\":37939},{\"end\":37980,\"start\":37962},{\"end\":37991,\"start\":37980},{\"end\":38008,\"start\":37991},{\"end\":38024,\"start\":38008},{\"end\":38328,\"start\":38314},{\"end\":38499,\"start\":38485},{\"end\":38511,\"start\":38499},{\"end\":38827,\"start\":38810},{\"end\":38846,\"start\":38827},{\"end\":38863,\"start\":38846},{\"end\":39025,\"start\":39008},{\"end\":39195,\"start\":39183},{\"end\":39211,\"start\":39195},{\"end\":39329,\"start\":39314},{\"end\":39342,\"start\":39329},{\"end\":39359,\"start\":39342},{\"end\":39375,\"start\":39359},{\"end\":39403,\"start\":39375},{\"end\":39417,\"start\":39403},{\"end\":39435,\"start\":39417},{\"end\":39453,\"start\":39435},{\"end\":39467,\"start\":39453},{\"end\":39489,\"start\":39467},{\"end\":39500,\"start\":39489},{\"end\":39517,\"start\":39500},{\"end\":39532,\"start\":39517},{\"end\":39548,\"start\":39532},{\"end\":39560,\"start\":39548},{\"end\":39584,\"start\":39560},{\"end\":39602,\"start\":39584},{\"end\":39619,\"start\":39602},{\"end\":39635,\"start\":39619},{\"end\":39649,\"start\":39635},{\"end\":39665,\"start\":39649},{\"end\":39682,\"start\":39665},{\"end\":39696,\"start\":39682},{\"end\":39709,\"start\":39696},{\"end\":39716,\"start\":39709},{\"end\":40294,\"start\":40288},{\"end\":40302,\"start\":40294},{\"end\":40466,\"start\":40453},{\"end\":40477,\"start\":40466},{\"end\":40495,\"start\":40477},{\"end\":40511,\"start\":40495},{\"end\":40524,\"start\":40511},{\"end\":40540,\"start\":40524},{\"end\":40552,\"start\":40540},{\"end\":40569,\"start\":40552},{\"end\":40582,\"start\":40569},{\"end\":40594,\"start\":40582},{\"end\":40891,\"start\":40879},{\"end\":41152,\"start\":41138},{\"end\":41172,\"start\":41152},{\"end\":41556,\"start\":41535},{\"end\":41577,\"start\":41556},{\"end\":41693,\"start\":41679},{\"end\":41710,\"start\":41693},{\"end\":41722,\"start\":41710},{\"end\":41733,\"start\":41722},{\"end\":41923,\"start\":41911},{\"end\":41930,\"start\":41923},{\"end\":41941,\"start\":41930},{\"end\":41949,\"start\":41941},{\"end\":41958,\"start\":41949},{\"end\":41970,\"start\":41958},{\"end\":41980,\"start\":41970},{\"end\":41991,\"start\":41980},{\"end\":42419,\"start\":42402},{\"end\":42601,\"start\":42592},{\"end\":42619,\"start\":42601},{\"end\":42638,\"start\":42619},{\"end\":42654,\"start\":42638},{\"end\":42662,\"start\":42654},{\"end\":43332,\"start\":43323},{\"end\":43540,\"start\":43528},{\"end\":43550,\"start\":43540},{\"end\":43703,\"start\":43687},{\"end\":43719,\"start\":43703},{\"end\":43737,\"start\":43719},{\"end\":43747,\"start\":43737},{\"end\":44335,\"start\":44320},{\"end\":44351,\"start\":44335}]", "bib_venue": "[{\"end\":30721,\"start\":30661},{\"end\":31317,\"start\":31210},{\"end\":32056,\"start\":32026},{\"end\":33069,\"start\":33046},{\"end\":33489,\"start\":33338},{\"end\":33966,\"start\":33916},{\"end\":34475,\"start\":34441},{\"end\":34871,\"start\":34857},{\"end\":35279,\"start\":35184},{\"end\":35536,\"start\":35525},{\"end\":35710,\"start\":35687},{\"end\":35855,\"start\":35847},{\"end\":35992,\"start\":35971},{\"end\":36311,\"start\":36292},{\"end\":36618,\"start\":36522},{\"end\":37000,\"start\":36961},{\"end\":37296,\"start\":37245},{\"end\":37642,\"start\":37570},{\"end\":37906,\"start\":37861},{\"end\":38312,\"start\":38237},{\"end\":38576,\"start\":38511},{\"end\":38872,\"start\":38863},{\"end\":39052,\"start\":39025},{\"end\":39181,\"start\":39149},{\"end\":40333,\"start\":40302},{\"end\":40607,\"start\":40594},{\"end\":40908,\"start\":40891},{\"end\":41235,\"start\":41172},{\"end\":42040,\"start\":41991},{\"end\":42325,\"start\":42295},{\"end\":42458,\"start\":42419},{\"end\":42795,\"start\":42662},{\"end\":43368,\"start\":43332},{\"end\":43526,\"start\":43495},{\"end\":43864,\"start\":43747},{\"end\":44485,\"start\":44351},{\"end\":44971,\"start\":44928},{\"end\":31428,\"start\":31319},{\"end\":33627,\"start\":33491},{\"end\":35296,\"start\":35281},{\"end\":37334,\"start\":37298},{\"end\":41257,\"start\":41237},{\"end\":42932,\"start\":42797},{\"end\":43880,\"start\":43866},{\"end\":44623,\"start\":44487}]"}}}, "year": 2023, "month": 12, "day": 17}