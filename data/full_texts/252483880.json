{"id": 252483880, "updated": "2023-09-14 23:47:31.586", "metadata": {"title": "Your Labels are Selling You Out: Relation Leaks in Vertical Federated Learning", "authors": "[{\"first\":\"Pengyu\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"Xuhong\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Shouling\",\"last\":\"Ji\",\"middle\":[]},{\"first\":\"Tianyu\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Yuwen\",\"last\":\"Pu\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Ting\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "IEEE Transactions on Dependable and Secure Computing", "journal": "IEEE Transactions on Dependable and Secure Computing", "publication_date": {"year": 2023, "month": 9, "day": 1}, "abstract": "Vertical federated learning (VFL) is an emerging privacy-preserving paradigm that enables collaboration between companies. These companies have the same set of users but different features. One of them is interested in expanding new business or improving its current service with others\u2019 features. For instance, an e-commerce company, who wants to improve its recommendation performance, can incorporate users\u2019 preferences from another corporation such as a social media company through VFL. On the other hand, graph data is a powerful and sensitive type of data widely used in industry. Their leakage, e.g., the node leakage and/or the relation leakage, can cause severe privacy issues and financial loss. Therefore, protecting the security of graph data is important in practice. Though a line of work has studied how to learn with graph data in VFL, the privacy risks remain underexplored. In this paper, we perform the first systematic study on relation inference attacks to reveal VFL's risk of leaking samples\u2019 relations. Specifically, we assume the adversary to be a semi-honest participant. Then, according to the adversary's knowledge level, we formulate three kinds of attacks based on different intermediate representations. Particularly, we design a novel numerical approximation method to handle VFL's encryption mechanism on the participant's representations. Extensive evaluations with four real-world datasets demonstrate the effectiveness of our attacks. For instance, the area under curve of relation inference can reach more than 90%, implying an impressive relation inference capability. Furthermore, we evaluate possible defenses to examine our attacks\u2019 robustness. The results show that their impacts are limited. Our work highlights the need for advanced defenses to protect private relations and calls for more exploration of VFL's privacy and security issues.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tdsc/QiuZJDPZW23", "doi": "10.1109/tdsc.2022.3208630"}}, "content": {"source": {"pdf_hash": "335762386be3b4d71041648231590f3d990ce945", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "fcc00a5938fe978688da9108937defa8f59f00fd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/335762386be3b4d71041648231590f3d990ce945.txt", "contents": "\nYour Labels are Selling You Out: Relation Leaks in Vertical Federated Learning\n\n\nPengyu Qiu \nXuhong Zhang \nMember, IEEEShouling Ji \nTianyu Du \nYuwen Pu \nJun Zhou \nTing Wang \nYour Labels are Selling You Out: Relation Leaks in Vertical Federated Learning\n10.1109/TDSC.2022.3208630Index Terms-Vertical federated learning, relation inference\nVertical federated learning (VFL) is an emerging privacy-preserving paradigm that enables collaboration between companies. These companies have the same set of users but different features. One of them is interested in expanding new business or improving its current service with others' features. For instance, an e-commerce company, who wants to improve its recommendation performance, can incorporate users' preferences from another corporation such as a social media company through VFL. On the other hand, graph data is a powerful and sensitive type of data widely used in industry. Their leakage, e.g., the node leakage and/or the relation leakage, can cause severe privacy issues and financial loss. Therefore, protecting the security of graph data is important in practice. Though a line of work has studied how to learn with graph data in VFL, the privacy risks remain underexplored. In this paper, we perform the first systematic study on relation inference attacks to reveal VFL's risk of leaking samples' relations. Specifically, we assume the adversary to be a semi-honest participant. Then, according to the adversary's knowledge level, we formulate three kinds of attacks based on different intermediate representations. Particularly, we design a novel numerical approximation method to handle VFL's encryption mechanism on the participant's representations. Extensive evaluations with four real-world datasets demonstrate the effectiveness of our attacks. For instance, the area under curve of relation inference can reach more than 90%, implying an impressive relation inference capability. Furthermore, we evaluate possible defenses to examine our attacks' robustness. The results show that their impacts are limited. Our work highlights the need for advanced defenses to protect private relations and calls for more exploration of VFL's privacy and security issues.Index Terms-Vertical federated learning, relation inference \u00c7 Pengyu Qiu and Yuwen Pu are with the College\n\nINTRODUCTION\n\nP ROTECTING data security and privacy has become a worldwide issue. The establishment of new regulations and laws, such as the general data protection regulations (GDPR) [49], makes it difficult for companies to collect and share data for applications, e.g., artificial intelligence, data mining, and data analytics applications. To solve this dilemma, federated learning (FL) [24], [34] is proposed, which enables multiple parties to train a machine learning model without centralized data collection.\n\nAccording to the data distribution across parties, we classify FL into horizontal federated learning (HFL) and vertical federated learning (VFL). HFL [21], as shown in Fig. 1a, enables the collaboration at the data sample level, i.e., the datasets from the participants in HFL share the same feature space but differ in the sample space. This can be summarized by \u00f0U i ; F \u00de, i 2 f1; . . . ; mg, where U i denotes the sample space, F denotes the feature space, and m is the number of participants. VFL [28], [60], as shown in Fig. 1b, allows the collaboration at the feature level, i.e., the datasets from the participants of VFL share the same sample space but differ in the feature space, denoted by \u00f0U; F i \u00de. This property of VFL makes it popular in industry [7], [10], [26], [51], [52], [57]. For example, an e-commerce company with users' shopping records may want to improve its recommendation performance by incorporating these users' preferences from another corporation such as a social media company [26]. Then VFL is a suitable way for these two companies.\n\nIn practice, graph data is a common type of data in industry since it is a powerful tool to model complex relationships between entities [13], [41], [58]. In private applications, these graph data usually contain sensitive information, e.g., dating, debt, and even sexual relationships [25], [33]. Once these relations are leaked, it will bring severe privacy issues to the public and cause great financial and reputational losses for the companies. Therefore, it is important to protect the security of these sensitive graph data. In VFL, there is a line of work on learning with graph data [8], [9], [37], [48], [55], [67]. Through an encryption mechanism, these works protect graph data in training from disclosure to outside parties.\n\nHowever, we find the privacy risk of data leakage in VFL is still underexplored, e.g., leakage caused by a semi-honest participant [43].\n\nIn this work, we perform a systematic study on relation inference attacks against VFL to reveal the risk of leaking samples' relations. Specifically, we assume the adversary is a semi-honest participant of VFL who honestly joins VFL training but is curious about the relation between samples from other participants. For example, in the previous case, the e-commerce company may want to steal the social relationships from the social media platform. Its motivation can be saving the cost of collaboration or selling the relationships for profit.\n\nOur attacks are based on the intuition that the relation of two samples is closely correlated to the distance between their representations, i.e., if the distance between two samples' representations is less than a threshold, we consider they may have the relation. Then, depending on the adversary's knowledge level, we formulate three kinds of attacks based on different intermediate representations in VFL: 1) the sample's representation learned by the adversary's bottom model; 2) the sample's posterior learned by the top model; and 3) the sample's representation learned by the target participant's bottom model.\n\nNotably, since VFL applies homomorphic encryption (HE) [15] to aggregate each participant's representations in an encrypted way, it is impossible to directly obtain a sample's representation from the target participant. To handle the encryption mechanism, we further design a novel numerical approximation method to find a representation that best satisfies the constraints, i.e., the approximated representations as input to the trained VFL model should be able to reproduce the corresponding prediction results.\n\nWe conduct extensive experiments with four real-world datasets to evaluate the relation leakage threat of the proposed attacks. Experimental results show that the area under curve (AUC) can reach more than 90%, an impressive performance in relation inference tasks. Based on the analysis of the results, we also provide attack strategies under different scenarios.\n\nTo thoroughly examine our attacks' robustness, we further evaluate two potential defense strategies: obfuscation and truncation. The former introduces the differential privacy (DP) technique in VFL training, and the latter restricts the adversary's knowledge by truncating the prediction results. Experimental results show that the obfuscation strategy has a limited impact on our attacks, and the truncation strategy does mitigate the attack based on posteriors but does not affect the attack using the adversary's representations. Our work highlights the need for more advanced defenses to protect the private relations in VFL and calls for more exploration of VFL's security and privacy issues.\n\nOur Contributions. To the best of our knowledge, we are the first to systematically study the relation leakage risk in VFL. Our main contributions are summarized as follows.\n\nWe identify three kinds of intermediate representations in VFL that can cause relation leakage. Then, according to the adversary's knowledge level, we formulate three relation inference attacks based on the corresponding representations. Particularly, to bypass the encryption mechanism of VFL, we design a novel numerical approximation method to approximate the encrypted representations. We conduct extensive evaluations on four realworld datasets to empirically validate the effectiveness of the proposed relation inference attacks. Based on the evaluation results, we also provide suggestions on customizing the optimal attack under different scenarios. To further examine our attack's robustness and efficacy, we evaluate possible defenses from two perspectives: obfuscation and truncation. Experimental results show that the defenses have limited impacts on our attacks. This sheds light on building a more secure VFL framework and leads to promising directions for further research.\n\n\nBACKGROUND\n\n\nVertical Federated Learning\n\nIn order to better understand VFL, we give its formal description. We take the m-party scenario VFL for example, and use P \u00bc fP 1 ; . . . ; P m g to represent the participants. Their corresponding local datasets are denoted by D \u00bc fD 1 ; . . . ; D m g. Here, D i \u00bc \u00f0U; F i \u00de, who share the same user space U but different feature space F i . Let n be the number of samples in U and d i be the size of the feature space F i , then we can further present D i \u00bc fx t i g n t\u00bc1 , where x t i denotes the t-th sample from U with d i features from F i .\n\nWe consider a supervised classification task and use c to denote the number of classes. Then, the task can be summarized to train a set of models, i.e., the top model f top , and the bottom models \nf P \u00bc ff P 1 ; . . . ; f P m g. Specifically, f P i learns to map a sample x t i into a latent representation v t i , which is R d i ! R d \u00c3 ,; . . . ; v t m \u00de, which is R m\u00c2d \u00c3 ! R c . The v t\ntop is then used to calculate the loss, and the corresponding gradients are used to update the top model and the bottom models' parameters.\n\nMoreover, in VFL, there is an extra encrypted alignment before training, which aligns each sample's features. The encryption mechanism is to avoid membership leakage [60].\n\n\nHomomorphic Encryption\n\nIn VFL, the representations learned by each bottom model are uploaded to the server of the top model. Therefore, ensuring the privacy and security of these representations should be the fundamental guarantee. For the popular realworld frameworks that support VFL, e.g., FATE[2], PySyft [3], TF Encrypted [4], and CrypTen [1], they all apply homomorphic encryption (HE) [15] to provide privacy guarantee. Specifically, they utilize additively homomorphic encryption (aHE), like Paillier [38], to encrypt the intermediate representations and do computations.\n\nLet the encryption of a number u be \u00bd\u00bdu. The aHE supports addition between two encrypted data, i.e., \u00bd\u00bdu \u00fe \u00bd\u00bdv \u00bc \u00bd\u00bdu \u00fe v. A natural derivation is that we can implement multiplication by repeated addition, i.e., \u00bd\u00bduv \u00bc v\u00bd\u00bdu, where v is not encrypted. These operations are also applicable to vectors and matrices. For instance, given two vectors u and v, we can calculate their inner product as v T \u00c1 \u00bd\u00bdu \u00bc \u00bd\u00bdv T \u00c1 u.\n\nHowever, these encryption calculations also inevitably increase the computational overhead. Indeed, there is a line of works [62], [65] aiming at improving the efficiency of VFL training based on aHE. For example, in [65], the authors proposed that the submitted representations only need to be aggregated in encryption once, and the top model can perform the subsequent calculations in plain text to improve efficiency. The idea is adopted in FATE's official implementation.\n\n\nNeural Networks\n\nThe properties of aHE make VFL suitable for neural networks. Considering the development and excellent performance of neural networks in recent years, they are also the mainstream choices in industry. Therefore, in this section, we introduce some basic concepts of deep neural networks (DNNs) and their variant, graph neural networks (GNNs).\n\n\nDeep Neural Networks\n\nRecently, DNN has become the most effective algorithm in supervised classification tasks. The early and simple version of DNN is the multilayer perceptron (MLP) [42], which consists of an input layer, an output layer, and one hidden layer (also called \"vanilla\" neural networks). Then, benefiting from the development of GPU and massive data, researchers find that expanding the number of hidden layers can generally improve DNN's performance.\n\nLet X denote the input space and Y be the output space. A DNN model is a function f u : X ! Y, where u denotes the parameters learned from a labeled training dataset. Specifically, given the training dataset D train with n samples x t (t 2 f1; . . . ; ng), each sample x t contains d features and has a label y t . The parameters are learned by optimizing the loss function, min u 1 n S n t\u00bc1 l\u00f0f u \u00f0x t \u00de; y t \u00de \u00fe V\u00f0u\u00de; where l\u00f0\u00c1; \u00c1\u00de denotes the loss function (usually cross entropy loss) and V\u00f0u\u00de denotes the regularization term.\n\n\nGraph Neural Networks\n\nGraph data is a powerful resource in industry. Recently, GNNs are the most popular algorithms for learning graph data and are widely applied in business. Therefore, in this section, we introduce the concepts and implementations of GNNs.\n\nGNNs are designed to use both the topology and the feature information of graph data in specific tasks with neural networks. In particular, given a graph G \u00bc \u00f0V; E\u00de, where V is the node set and E is the edge set, a general GNN's task is to learn a mapping f from the feature space to a continuous embedding space, f : \u00f0x u ; N u \u00de ! R d ; where x u denotes node u's feature vector, N u denotes u's neighbors, and d is the dimension of the embedding space. Then the node embedding is usually fed to a downstream task such as a classifier or a link predictor.\n\nThere are mainly two routes to implement a GNN: one is the spectral theory-based method, and the other is the sampling-based method. Therefore, we introduce the two typical kinds of GNNs in the following.\n\nGraph Convolutional Networks (GCN). Due to the fantastic performance of convolutional neural networks (CNNs) on images, several works have proposed the same idea that learns over the graph by applying CNNs [11], [23]. Kipf et al. [23] introduced the most widely used structure, which was initially designed for semi-supervised learning in a transductive setting. Specifically, let A be the adjacency matrix of graph G, and I be the identity matrix. We first cal-culate\u00c3 \u00bc A \u00fe I to add self-loop in the graph. Then we computeD ii \u00bc S j\u00c3ij to perform the Laplacian normalization on\u00c3 according toD \u00c0 1 2\u00c3D \u00c0 1 2 . Finally, each layer of GCN can be summarized as H \u00f0k\u00fe1\u00de \u00bc s\u00f0D \u00c0 1\n2\u00c3D \u00c0 1 2 H \u00f0k\u00de W \u00f0k\u00de \u00de;\nwhere W \u00f0k\u00de is a layer-ware trainable weight matrix. s\u00f0\u00c1\u00de denotes an activation function such as ReLU. H \u00f0k\u00de 2 R N\u00c2D is the hidden layer outputs of all nodes; H \u00f00\u00de \u00bc X, where X denotes all nodes' feature vectors. From the above definitions, we can conclude a critical fact of GCN that nodes with similar neighborhoods tend to be closer in the latent space than other nodes.\n\nAs the optimization of W \u00f0k\u00de in GCN takes many computation resources, Wu et al. [54] introduced a more straightforward way to implement the aggregation on the graph: H \u00f0k\u00fe1\u00de \u00bc \u00f0D \u00c0 1 2\u00c3D \u00c0 1 2 \u00de k XW: GraphSAGE. GCN requires the complete graph information to learn the latent representation of nodes. This limits its application on large graphs. Therefore, GraphSAGE, introduced by Hamilton et al. in [18], by combining spectral theory and sampling method [39], [40], breaks the above limitation. In particular, GraphSAGE relaxes the requirements on acquiring the complete adjacency matrix A by sampling a subset of neighbors for each node. In this way, GraphSAGE can be summarized as h \u00f0k\u00fe1\u00de are trainable weight matrices. Agg\u00f0\u00c1\u00de represents the functions that aggregate the representations of the neighbors, e.g., concat or mean.\nu \u00bc s\u00f0W \u00f0k\u00de 1 h \u00f0k\u00de u \u00fe W\u00f0k\u00de\nFrom the above definition, we can find that the difference between GCN and GraphSAGE is that GCN first aggregates neighbors' information and then trains the weight matrix, while GraphSAGE distinguishes the importance of the node and its neighbors before aggregation.\n\n\nPROBLEM STATEMENT\n\n\nLabel-Related Relation\n\nConsidering that there may exist many kinds of relations between samples, e.g., user's social networks and payment networks, in this work, we further restrict the adversary's goal to label-related relation. We give the description and definition of label-related relation as follows.\n\nGenerally, in a supervised classification task, given a dataset D and a label set C, the goal is to learn a mapping f that predicts a sample's label by argmax\u00f0f\u00f0x\u00de\u00de, where x is the sample's feature vector and argmax denotes the dimension of f\u00f0x\u00de's max value. Note that for the node classification task of graph data, f can be expanded into the form f\u00f0x; N x \u00de, where N x denotes the neighbourhood of a sample.\n\nA similar definition also applies to a link prediction task (also called relation inference). Given a pair of samples \u00f0u; v\u00de and a target relationship, e.g., friendship, the goal is to learn a mapping g which predicts the existence of an edge e \u00f0u;v\u00de according to dist\u00f0g\u00f0x u \u00de; g\u00f0x v \u00de\u00de d, where dist\u00f0\u00c1; \u00c1\u00de refers to a distance function, e.g., euclidean distance, and d denotes a threshold.\n\nSince the same features can be used for different tasks, we use A f to represent the important feature set of f, and A g to represent the important feature set of g. It can be seen that if A f and A g have a high overlap, f\u00f0x\u00de and g\u00f0x\u00de are highly correlated, especially when A g contains the label that f tries to classify. To represent the correlation, we give the formal definition as follows.\n\nDefinition 1 (Label-related Relation). Given a classification task, its label set C, and its underlying mapping f, the labelrelated relation (denoted by the edge set E) represents the specific relation, whose underlying mapping g satisfies the condition that A g and A f have a high overlap. When A g contains the label C, it means that the label C is also a strong indicator of the existence of the relation.\n\n\nThreat Model\n\nAfter describing the VFL model and label-related relation, we formally define our attack as follows.\n\nDefinition 2 (Label-related Relation Inference Attack).\n\nLet P tar denote the target participant and P adv denote the adversary. P tar owns the label-related relation E. Then given a trained VFL model with f top and f P , the attack aims to rebuild the E according to dist\u00f0v u ; v v \u00de, where v can be the output of f top , f P adv , or the approximated solution of f P tar ; u and v denote samples' index.\n\nIn the definition of the inference attack, we assume that P tar owns the label-related relation E. In VFL, it is reasonable because the purpose of all participants is to improve the model's performance. Otherwise, the relation will not be used in training.\n\nWe also assume the adversary is semi-honest [43], who joins VFL training honestly but is curious about the E. Moreover, he/she does not know each sample's label in C. Then, we characterize the adversary's background knowledge along two dimensions.\n\nPrediction results, denoted by P . This background knowledge characterizes whether the adversary can receive the final prediction results. Note that there can be differences between the top model's outputs v top and the final prediction results since the latter one can be truncated to a top-k prediction. Unless otherwise specified, we assume the detailed result v top is given if this background knowledge is satisfied. This is reasonable when all parties need the full result to make further decisions. Top model's parameters, denoted by Q. This dimension characterizes whether the adversary has access to the top model. The assumption could happen when the adversary needs to make interpretable decisions [30] based on the top model's parameters. Since the feature space is split in VFL, it is hard for one participant to analyze feature importance. In [50], Wang proposed a variant version of SHAP [29], which allows each participant to locally interpret the results with the help of the top model. Moreover, the homomorphic encryption used in VFL mainly protects the raw data from disclosing to the third server and outside parties. In most cases, the third server only provides computation resources. Therefore, it is also reasonable that all participants share the top model in practice. We denote the adversary's background knowledge as a tuple: K \u00bc \u00f0P; Q\u00de: Whether the adversary has each of the two items is a binary choice. There are four different combinations of K in total. However, with the top model's parameters only, i.e., K \u00bc \u00f0\u00c2; Q\u00de, it cannot provide extra available information for the adversary. Therefore, we finally get three different types of attacks, e.g., attack using the adversary's representations, attack using the posteriors, and attack using the approximated target participant's representations, corresponding to different combinations of the background knowledge. In Section 4, we elaborate on all the attacks.\n\n\nMETHODOLOGY\n\n\nOverview\n\nFig. 2 shows the overview of our attack. The adversary is a semi-honest participant in VFL. Therefore, the adversary can directly acquire v adv from his/her bottom model. Then according to K, the adversary can further obtain v top when K \u00bc \u00f0P; \u00c2\u00de or v 0 tar when K \u00bc \u00f0P; Q\u00de. In particular, v 0 tar is recovered from the encrypted state through our approximation method. Then we compute the distance between the obtained representations to predict the existence of a relation between a pair of nodes. The samples are assumed to have a relationship when their representations' distance is within a given threshold. Finally, we can rebuild the relation network among these samples, which ideally should be the same as the target participant's relation network.\n\n\nAttack Taxonomy\n\nIn this section, we elaborate on the design of each attack. Given different knowledge K, the adversary can conduct relation inference attacks in different ways.\n\nAttack v adv : K \u00bc \u00f0\u00c2; \u00c2\u00de. This is the least knowledgeable setting for the adversary, where he/she does not know the prediction results of the samples or the top model's parameters. The only possessed information is his/her bottom model's outputs, v adv . Therefore, we directly measure the distance between two samples' v adv and determine the existence of the relation with a threshold of the distance.\n\nThe intuition is that the labels also train the adversary's bottom model. Therefore, v adv carries information about label-related relations naturally. This attack reveals the risk that even the adversary does not own the labels, he/she can still obtain label-related relations from joining VFL.\n\nNote that the attack does not assume extra information; thus, it is also a baseline compared to the following attacks.\n\nAttack v top : K \u00bc \u00f0P; \u00c2\u00de. In this attack, we extend the adversary's knowledge with the prediction results of the samples. Specifically, we assume v top is available to the adversary. The remaining procedure of this attack is similar to Attack v adv .\n\nHowever, compared to v adv , v top is not only trained by the label but also aggregates the representations from the target participant. Generally, this attack is supposed to produce a more severe threat than Attack v adv .\n\nAttack v 0 tar : K \u00bc \u00f0P; Q\u00de. This is the last attack with the adversary having all the two kinds of knowledge. It enables the adversary to recover the target participant's representations v tar using an optimization-based method. Note that in the multi-party scenario, the adversary cannot recover v tar without knowing other participants' representations. Therefore, we try to reconstruct the concatenation of representations from all the remaining participants instead, i.e., v cat , where v cat \u00bc concat\u00f0v P i \u00de, i 2 f1; . . . ; mg and i 6 \u00bc adv. Since our target is the label-related relation, we speculate that the other parties' representations, which are helpful for the classification task, may help the inference of the label-related relation. Therefore, we use v 0 cat to do the inference. However, the shortcoming is that there will exist some noise. We introduce the implementation details of this method in Section 6.2 and analyze the impact of the number of parties in Section 7.1. With the approximated v 0 tar or v 0 cat , the adversary can conduct the relation inference attack. The procedure is similar to previous attacks.\n\nThe intuition behind this attack is that when the target participant has valuable features to the relation, primarily when the target participant uses topology information in training explicitly, using v 0 tar may produce a stronger attack. Furthermore, our recovery of v tar also shows the vulnerability of the VFL framework in protecting participants' private representations even after they are encrypted.\n\nAlgorithm 1 summarizes the mainframe of the above attacks. We also plot the entire flow in Fig. 3. The procedure is as follows. First, the adversary should collect sample\nx t 's representation v t . v t can be v top ; v adv , or v 0\ntar , according to his/her knowledge K. Then the adversary normalizes the v t , and builds the matrix V \u00bc \u00bdv 1 ; v 2 ; . . . ; v n , where n denotes the number of samples. Next, the adversary quantifies the relations through Q \u00bc 1 \u00c0 V T V. With a given threshold d, the adversary can clip Q to make it into the form of an adjacency matrix. Finally, he/she gets the inferred relation graph G according to the clipped Q.\n\n\nAlgorithm 1. Label-Related Relation Inference Attack\n\nInput: Knowledge K, threshold d, number of samples n. Output: Inferred relation graph G.\n\n1: Collecting sample x t 's representation v t . v t 2 fv top ; v adv ; v 0 tar g, depending on K. 2: Normalize v t and build the matrix V \u00bc \u00bdv 1 ; v 2 ; . . . ; v n . 3: Quantify the relation by Q \u00bc 1 \u00c0 V T V. 4: Clip Q by d:\nq ij \u00bc 1; if qij d 0; else 5:\nReturn the inferred graph G according to the clipped Q.\n\n\nRecover Encrypted Representations\n\nIn this section, we introduce the optimization-based method to recover the encrypted representation v tar . This method is designed to solve the situation where there are a handful of participants. Such a situation is general in VFL since increasing the number of participants will rapidly decrease the size of the overlap sample set. Furthermore, because VFL is to business [31], it requires frequent queries, leading to huge computation and communication costs. These costs also limit the number of VFL's participants in practice. Given the scope of our method, we start with the twoparty VFL scenario. The adversary has the knowledge of the top model's parameters Q and the output posteriors v top . Armed with this information, the adversary can turn to Here is an example of two-party VFL. According to the adversary's background knowledge, there are three kinds of representations that are available for the inference attack, including v adv , v top and v 0 tar . In particular, v 0 tar denotes the recovered target participant's representations. Then the relation inference step uses one of these representations to infer the existence of edges, i.e., the smaller the representation's distance, the more likely there is an edge. Fig. 3. The mainframe of the inference attacks. 1) the adversary collects samples' representations depending on his/her knowledge. 2) the adversary normalizes the representations and uses them to build the quantified relation matrix. 3) the adversary clips the matrix by a threshold, e.g., 0.3, to get the adjacent matrix. 4) the adversary reconstructs the graph according to the adjacent matrix.\n\nrecover v tar into an optimization problem. If we regard the prediction process as the calculation of a fixed formula, then the whole model turns into an equation with the final output. Then the input belongs to the target participant is the solution that satisfies the constraints (Q, v top and v adv ) best.\n\nSince the posteriors are numerical values, the optimization problem can also be regarded as a regression problem. Therefore, we use L 2 norm as the loss function. The recovery can be formally defined as follows:\nL \u00bc min v 0 tar v top \u00c0 f top \u00f0\u00bdv adv ; v 0 tar \u00de 2(1)\nwhere v 0 tar denotes the approximated solution of v tar . The aggregation is concat denoted as \u00bd\u00c1; \u00c1. Considering the optimization function is non-linear, which is easily affected by the initial values, and the distance function, e.g., correlation distance, which is sensitive to the sign of values, instead of randomly initializing, we set each sample's v 0 tar as zeros. Algorithm 2 summarizes the above process. We also plot the mainframe of the reconstruction in Fig. 4. The procedure is as follows. We first initialize v 0 tar as zeros and concatenate it with v adv . Then, we send the concatenated representation to the top model and calculate the L 2 distance between the output and v top . Next, we use the distance to calculate the gradients and Hessian to optimize the solution vector. We use the second-order optimization method here because it can accelerate the convergence and avoid local optimal solutions as much as possible. The procedure continues until the decrease of the loss is smaller than the given bound.\n\nRegarding the multi-party scenario, it is intractable to recover v tar . Therefore, we try to reconstruct v cat instead, which is the concatenation of representations from all the remaining participants. We speculate that the other parties' representations may also help the inference, as our goal is to steal the label-related relation. Specifically, we change the loss function to L \u00bc kv top \u00c0 f top \u00f0\u00bdv adv ; v 0 cat \u00dek 2 . The remaining procedure is similar to the two-party scenario. However, the shortcoming is that some noise will be introduced. Section 7.1 provides a more detailed analysis of the multi-party scenario, where only the target participant owns the target relation. As for the most complex situation where the remaining participants also provide label-related relations, our modified method cannot infer the distinct label-related relation owned by the target participant. Indeed, we can only predict the union of these relations. In some cases of VFL, e.g., the collaboration among social media companies (Twitter, LinkedIn, and Facebook), the union of relations may still attract the adversary, as it may bring new 'follow' recommendations beyond his/her current collected social networks.\n\n\nEXPERIMENTAL SETUP AND IMPLEMENTATION\n\n\nDatasets\n\nWe use four public graph datasets to conduct our experiments, including Cora and Citeseer obtained through [61], and Amazon Computers and Amazon Photos used in [46]. These datasets are widely-used benchmarks for evaluating GNNs.\n\nSpecifically, Cora and Citeseer are citation datasets with nodes representing publications and links denoting citations among publications. The node features are the declared keywords in publications; class labels are the research fields of these publications. The Amazon datasets are segments of the co-purchase graph in Amazon, with nodes representing goods and edges indicating that two goods are frequently bought together. Node features are bag-of-words of the corresponding product reviews, and class labels are product categories.\n\nThere are two more special statistics in Table 1: edge density and feature sparsity. The edge density describes the sparsity of the graph, which is highly correlated to our inference attacks. The feature sparsity represents the ratio of the valid feature, e.g., the keywords that appear in a paper in the Cora dataset, which is highly correlated to the approximation methods. In particular, the edge density is calculated according to the formula 2 \u00c2 # edges # nodes \u00c2 \u00f0# nodes\u00c01\u00de and the node's feature sparsity is calculated according to the formula # valid attributes # attributes . We use the average of all nodes to represent the specific dataset's feature sparsity. Table 1 summarizes the general statistics. We split the nodes into training, validation, and test sets for each dataset with a ratio of 5:1:4.  \n\n\nMetrics\n\nDistance Function. We use the correlation distance to estimate the relation strength since it performs the best in the link inference task on graph data [22]. Specifically, the correlation distance of two samples is defined as: dist\u00f0u; v\u00de \u00bc Measurement. In all four datasets, the graph is relatively sparse. Furthermore, the evaluation of our attacks needs to set a series of thresholds. Therefore, we use the area under the ROC curve (AUC) [27] to evaluate our inference performance as previous works [22], [64]. Note that a high AUC indicates that the positive case is more likely to have a higher predicted value than the negative case. Then, when we need to set a specific threshold for inferring relations in practice, we can rank the calculated similarity between pairs of nodes and set the threshold, e.g., at the top 10%.\n\n\nModels\n\nIn VFL, participants have different choices for both top and bottom models. We present our settings in the following.\n\nTop Model. We take DNN as the top model since it is widely applied to learn rich feature interactions for classification tasks. The top model has two fully connected (FC) layers with ReLU activation functions in our experiments.\n\nBottom Model. We use a GNN/DNN as the bottom model with/without topology information, respectively. Specifically, we use GCN and GraphSAGE to represent two kinds of aggregation mechanisms, exploring their impacts on the sample's relation leakage. Our GNNs are implemented based on the publicly available code 1 . We set the number of hops in GNNs as 2, which is a common practice. By default, the bottom model maps input features into a 16-dimensional latent space.\n\n\nImplementation\n\nIn this paper, we refer to the real-world framework, FATE 2 , to implement VFL. Indeed, compared to PySyft, TF Encrypted, and CrypTen, FATE is now the complete platform with an active community and a continuous development team.\n\nIn our experiments, we fix the number of participants to 2. As for the multi-party scenario, we will discuss it in Section 7.1 as a sensitivity analysis case.\n\nFor each VFL model, we train 300 epochs with a learning rate of 0.001. We also keep the regularization parameter as 0.001 to reduce overfitting. We use Limited-memory BFGS (L-BFGS) [32] for the approximation method in consideration of computation efficiency. We run all experiments with the same setting for 10 times and report the average value. Experiments are performed on a workstation equipped with AMD Ryzen 9 3950X and an NVIDIA GTX 3090 GPU card.\n\n\nATTACK PERFORMANCE\n\nIn this section, we conduct an empirical study of our attacks with four datasets shown in Table 1. Specifically, we consider three attack scenarios to explore the risks of relation leakage: 1) the scenario where the adversary only provides features. This is a typical scenario in practice since the target participant owns the labels and other participants with a different business are less likely to possess other labelrelated relations; 2) the scenario where the adversary also provides other label-related relations. This scenario happens when the adversary has relations that help improve the target participant's task. However, since the adversary's relationships may be from another domain, the correlation between these relations and the labels might be weaker than the target participant's relations. Compared to the first scenario, the adversary impacts the classification results more; 3) the scenario where the target participant does not use label-related relations in training. This scenario happens when the target participant is aware of the sensitivity of the label-related relations and chooses only to use features in VFL training. Our experiments are designed to answer the following questions:\n\nHow effective are the three proposed attacks? What is the impact of explicitly using label-related relations in training on relation leakage? What is the impact on attacks if the adversary also provides different label-related relations? When there is no usage of label-related relations, are our attacks still effective?\n\n\nWhen the Adversary Only Provides Features\n\nIn this scenario, the adversary only provides features in training. This attack scenario aims to reveal the risks of leaking relations when explicitly using label-related relations in VFL.\n\nDatasets Configuration. In this scenario, the edge set of each graph dataset is used in VFL training by the target participant and used as the ground truth for relation inference attack. As for node sets, to simulate VFL, we split each node's features into two parts. Specifically, we make the adversary's portion vary from 10% to 90% of all the features. For example, the node features in the Cora dataset are whether the keywords appear in the paper, and if we set the adversary's feature ratio as 10%, it means the adversary owns 10% of these keywords.\n\nAttack Efficacy. Fig. 5 shows the final results of the proposed three attacks. The legend 'Attack v adv ' denotes the curve that belongs to the attack using v adv , so as other legends. We also use v tar for inference, which is the baseline in our evaluation.\n\nFrom Thus v top can also benefit from it, leading to better performance. When the portion of features decreases, even if there are label-related relations, the quality of the representation will inevitably decrease. This is obvious on the baseline curves when the datasets are Cora and Citeseer.\n\nAttack v 0 tar . This attack's performance follows the baseline, but there is a gap between them. One possible reason lies in the approximation method. For instance, given two samples x u and x v with the same label, then v u top and v v top are similar. If they have an edge in the graph, v u tar and v v tar are also closer due to the GCN's aggregation mechanism. v u adv and v v adv , however, have much more diversity. With the fixed parameters of f top and similar outputs, the approximated solutions v u tar 0 and v v tar 0 make up for the difference between v u adv and v v adv , which makes the similarity between v u tar 0 and v v tar 0 lower than that between v u tar and v v tar . This loss finally reduces the performance of Attack v 0 tar . Furthermore, we can see that when the adversary's feature ratio increases, the gap also increases. We speculate that this is because the increase of the adversary's feature ratio makes the top model rely more on v adv . Therefore, when the constraints are mostly satisfied by v adv , the restriction on v 0 tar becomes loose, producing noise in v 0 tar . The result is the larger gap between the baseline and Attack v 0 tar . Suggestion for Attack Strategy. We conclude that if the adversary can access the top model's parameters, he/she should use the approximated representations for inference only when v tar is highly indicative of the classification task. Otherwise, using v top is a robust option.\n\n\nWhen the Adversary Provides Other Label-Related Relations\n\nIn this scenario, both the target participant and the adversary have label-related relations. However, their edge type is different. In general, since the adversary's relations are from another domain, the correlation between these relations to the labels might be weaker than that of the target participant's relations. This is realistic in practice since users of VFL may want to incorporate different kinds of label-related relations to improve their model's performance. For example, if the target participant owns user financial relations for a credit evaluation task, he/she might also be interested in incorporating user social relations from another company. Therefore, in this experiment, we let the target participant own the target label-related relations while the adversary provides another kind of label-related relations. Datasets Configuration. We let each party hold half of the node features. Furthermore, the target participant has the edge set in the current dataset, while the adversary has a synthetic edge set. To make the synthetic edge set meaningful and consider that the node features in the four datasets are binary values, we introduce the hamming distance [19] to generate the edge set. The intuition is that the node features' similarity can contribute to the classification task. Specifically, we first calculate the hamming distance between all pairs of samples and then add an edge to the synthetic edge set if the distance of two samples is below a given threshold.\n\nSince we need to calculate the hamming distance between each pair of nodes, the computational complexity of generating these edges reaches O\u00f0N 2 \u00de, where N denotes the number of nodes. When the dataset contains thousands of nodes like Computer, there are over 100 million calculations. Thus, we choose a smaller dataset, Cora, for the experiment to reduce the cost. Furthermore, since most pairs of nodes' hamming distances are lower than 0.01 in Cora, we set the threshold from 0.001 to 0.009. As a result, the number of generated edges varies from 434 to 131,845 correspondingly.\n\nAttack Efficacy. The evaluation results are shown in Table 2. When the thresholds are 0.003 and 0.004, the generated edges are the same; thus, we merge them in one line, so as 0.007 and 0.008. The AUC of the baseline is 0.9718 in all cases, therefore we do not show it in Table 2.\n\nThe performance of Attack v top and Attack v 0 tar is still effective and improved with the increase of the synthetic edge set. For example, when the threshold is 0.009, the AUC of Attack v 0 tar is 0.9404, followed by Attack v top with the AUC of 0.9020. The improvement of Attack v top is expected since the synthetic edge set is supposed to help the main task. Specifically, the synthetic edges first improve v adv through aggregating neighbors' information, making v top contain more valuable information. Then, the better v top leads to a stronger Attack v top . The improved accuracy of the main task can confirm this. As for Attack v 0 tar , it also benefits from the usage  of the synthetic edge set. Specifically, the nodes' v adv becomes similar if they have edges in the synthetic edge set. Then, this similarity can make the corresponding nodes' v 0 tar closer if they also have an edge in the original edge set. Without the loss of similarity between v 0 tar , Attack v 0 tar 's performance is improved than that in the previous scenario.\n\nHowever, Attack v adv 's performance drops with the increase of the synthetic edge set. Specifically, when the threshold is 0.001, Attack v adv achieves an AUC of 0.7375, which is comparable to its performances in the previous attack scenario. However, when the threshold reaches 0.009, the AUC of Attack v adv is only 0.5976. We speculate this is because when the size of the synthetic edge set increases, the GCN's aggregation mechanism makes more nodes' v adv similar. However, the synthetic edge set and the target edge set are quite different. Therefore, the inference results based on v adv produce a lot of false positive predictions, leading to the decrease of Attack v adv 's performance.\n\nSuggestion for Attack Strategy. In this scenario, the suggestion is different. Attack v top is still a robust consideration, and we do not repeat it. For other attacks, we summarize them as follows.\n\nAttack v adv is not recommended if the label-related relations are not highly correlated to the target labelrelated relation, since GNN will strengthen the adversary's relations in v adv .\n\nAttack v 0 tar is the best choice in this case. Moreover, incorporating additional label-related relations in the adversary's bottom model may further improve Attack v 0 tar .\n\n\nWhen the Target Does Not Use Label-Related Relations\n\nIn this scenario, the target participant does not use the labelrelated relations in training. It may happen when the target participant is aware of the risk of using label-related relations in VFL training and chooses only to use features. Datasets Configuration. The configuration is similar to the first scenario. The difference is that the edge set of each dataset is only used as the ground truth for inference attacks in this scenario.\n\nAttack Efficacy. The evaluation results of the proposed three attacks are shown in Fig. 6. From the results, we find that all three attacks' performance decreases compared to previous scenarios but still poses a threat to the label-related relations. For instance, for Cora, when both parties have 50% features, the AUC of Attack v adv and Attack v 0 tar achieve about 0.72, while Attack v top has the best AUC of 0.81. This indicates that even no label-related relations are used in training, the label-related relations of samples still face a high risk of leakage. Under different feature ratios, the changes in the performance of different attacks are different.\n\nAttack v adv behaves the same as the first scenario. Thus we do not repeat it here. For the other two attacks, we have the following specific observations.\n\nAttack v top . This attack achieves the best performance in most cases, since v top fuses all available features. The exceptions occur in extreme cases when the adversary/target participant has 90% of the features.\n\nIn these cases, the Attack v adv /baseline's performance may exceed Attack v top a little. We speculate this is because when the adversary/target participant has most of the features, v adv /v tar can both achieve a high AUC in inference. However, the other one who has few features can only learn a poor representation. Since the top model aggregates both participants' representations, the poor one may affect the quality of v top , leading to the little loss in Attack v top .\n\nAttack v 0 tar . The AUC trend of this attack is exactly the opposite of Attack v adv , which is as expected. Furthermore, the curve of this attack follows that of v tar . Compared to the gap in the first scenario, it is much smaller in this case. The reason is that without GCN's aggregation, the similarity between v tar has diminished. Therefore, v 0 tar is more likely to approach v tar . This also confirms our analysis in Section 6.1 and shows the efficiency of our recovery attack against the encryption mechanism.\n\nIn addition, the gap between v 0 tar and v tar becomes larger on Cora and Citeseer when the target participant has fewer features than on Computer and Photo. We speculate this is because, on Cora and Citeseer, features are rather sparse (see Table 1). These sparse vectors x tar further produce sparse representations v tar . The sparsity makes it hard to find proper solutions through the optimization method. Suggestion for Attack Strategy. An obvious conclusion is that the adversary should always choose the representation containing richer node feature information, based on the above results and analysis. Specifically, we recommend that:\n\nWhen the adversary only has access to his/her bottom model, he/she should provide more available/important features to make A f adv overlap more with A g . When v top is available, it is the best choice for relation inference attack; using v 0 tar is not recommended.\n\n\nAttack Strategy Summary\n\nFrom the above experiments, the attack strategies for different background knowledge can be summarized as follows.\n\nFirst of all, if the adversary possesses more feature information (node features or label-related relations) that is highly indicative of the classification task, he/ she should add it to the VFL training to improve the Fig. 6. Attack performance when the target does not use label-related relations. The x-axis represents the ratio of features possessed by the adversary. The y-axis represents AUC score for four attacks and accuracy for main task.\n\nquality of his/her latent representations, leading to a more powerful attack. Then, the choice between using v top and v 0 tar for inference is not fixed. In general, if the target participant uses label-related relations explicitly, we recommend using v 0 tar ; on the contrary, v top is a better choice.\n\n\nSENSITIVITY ANALYSIS\n\nSince we have some default settings in previous experiments, we explore their effects on our attacks in this section. For each factor's analysis, we choose the attack scenario that the adversary only provides features since the leakage is worse in this case. Unless otherwise specified, we let the adversary have 50% of all the features, and the target participant takes the rest of the features. We still use the edge set of each dataset as the ground truth.\n\n\nMulti-Party\n\nIn this section, we analyze the sensitivity of the number of parties. Since only the approximation method is affected by this factor, we focus on evaluating the efficacy of using v 0 cat to conduct the inference attack. Specifically, we assume that the target participant owns the edge set (the target relation) and others only provide features. Then, we fix the adversary's feature ratio to 20%, as a smaller ratio may lead to a lack of valid features. As for the remaining features, we let the other participants have the same feature ratio as the adversary, and the target participant takes the rest, e.g., when the number of parties is three, the adversary has the first 20% of features, the other participant has the next 20% of features, and the target participant has the remaining 60% features. Since we keep each participant has at least 20% of the features, we vary the number of parties from 2 to 5. Fig. 7 plots the results. These results show that Attack v 0 cat still has an impressive relation inference ability. For instance, when the number of the parties is 5, which means the target participant has 20% of features, the AUCs on four datasets achieve about 0.8. In Section 6.1, when the target participant has 20% of features, the performance of Attack v 0 tar on four datasets is similar. Therefore, the conclusion holds when the number of parties changes.\n\nFurthermore, according to the finding in Section 6.2 that incorporating additional label-related relations may improve the performance of Attack v 0 tar , we think Attack v 0 cat can also perform an effective attack when more participants use label-related relations in VFL training. Therefore, we speculate that our approximation method and the corresponding attack apply to the multi-party scenario, although it may introduce some noises.\n\n\nModel Type\n\nIn this section, we would like to evaluate how different aggregation mechanisms affect our attacks. We change the GCN algorithm to GraphSAGE. Fig. 8 shows the results. Compared to the results in Fig. 5, Attack v adv performs the same as that in the GCN case. Furthermore, the changing trend and value of the main task's test accuracy are also consistent with the GCN case, making Attack v top have no difference. These results are reasonable since only the target participant's bottom model's implementation has been changed, not affecting the main task performance or the adversary's bottom model.\n\nAttack v 0 tar shows the same trend as the baseline. However, compared to its performance in the GCN case, the gap between the performance of the baseline and Attack v 0 tar is smaller. This is caused by the decline of the baseline's performance and the improvement of Attack v 0 tar . Specifically, compared to GCN, GraphSAGE distinguishes the node and its neighbors' information in aggregation. Therefore, v tar contains more of the node's features and has more diversity than that obtained from GCN. This difference affects the performance of the baseline. Indeed, on Cora, the highest AUC of the baseline when using GraphSAGE is 0.95, while using GCN, the AUC is 0.97. Furthermore, the diversity in v tar also makes the top model pay more attention to fit the difference. Therefore, the top model's parameters Q are refined and produce more restrictions for the approximation process. These restrictions can reduce the noise in v 0 tar , which improves Attack v 0 tar .\n\n\nNumber of Layers\n\nIn this section, we evaluate the impact of the number of the top model's hidden layers. When there is more than one layer, we set the output dimension of each subsequent layer as half of the dimension of the previous layer. Therefore, we set the first hidden layer's dimension 128 to fit the experiment. Fig. 9 shows the results.\n\nAttack v top 's performance drops when the number of layers increases. We speculate this is because the top model overfits when there are too many layers. Indeed, the main task's test accuracy also decreases on the four datasets when the number of layers is larger than 3. The overfitting Fig. 7. Sensitive analysis of multi-party scenarios. The x-axis represents the number of parties in VFL, and the y-axis represents the AUC score. in the top model also affects v adv , leading to the drop of Attack v adv .\n\nAttack v 0 tar also drops with the increase of the number of layers, since more layers will make it harder to approximate v tar . However, Attack v 0 tar beats the other two attacks when the number of layers increases. The reason is that the baseline is not affected by the change of the top model. The GCN's aggregation mechanism maintains the similarity in v tar . Therefore, benefit from v tar , Attack v 0 tar outperforms other attacks.\n\n\nDimension of Representation\n\nIn this section, we explore how the dimension of the participant's latent representation affects our attacks. We test our attacks with the dimension changing from 8 to 128. Fig. 10 shows the results.\n\nAttack v top 's performance improves when the dimension expands. We think this is because the expanding of dimension can help capture more valuable information. Indeed, the main task's test accuracy also increases with the dimension expanding.\n\nHowever, Attack v adv does not improve apparently. This is because compared to v tar who uses label-related relations, v adv contains less information. Thus a relatively small dimension size of v adv is sufficient in capturing the valuable information from the adversary's feature. Increasing the dimension size does not improve the quality of v adv , so as the performance of Attack v adv .\n\nAs for Attack v 0 tar , we find its performance improves with the increase of dimension. This is because the larger dimension reduces the loss of information. Therefore, although the expanded dimensions increase the difficulty of approximation, the information gain makes up for the difficulty. Furthermore, when the log 2 \u00f0dimension\u00de is larger than 5, Attack v 0 tar beats Attack v top on all the datasets.\n\n\nPOSSIBLE DEFENSE\n\nIn this section, we evaluate possible defenses against our relation inference attacks. Specifically, we design the defenses from two perspectives: obfuscation and truncation. The obfuscation aims to hide the label-related relations, and the truncation aims to limit the adversary's access to the model outputs. Accordingly, we use DP-based methods to implement the obfuscation and truncating v top to restrict the adversary's access to the model outputs.\n\nSince the scenario where the adversary only provides features can cause the most severe leakage, we still conduct the experiments under this attack scenario. Furthermore, we assume the adversary has half of all the features. The ground truth is the edge set of each dataset.\n\n\nObfuscation\n\nAccording to the procedure of our attacks, We find that obfuscation can be achieved from two aspects: 1) adding fake edges and 2) perturbing the representations. Both ways need to generate random noise. Therefore, we introduce differential privacy (DP) [12] to implement the obfuscation.\n\nDP is first proposed to prevent data from being leaked by differential methods. For example, by continuously querying a database, the change in a statistical value such as the total number of males can be observed, which leaks the new user's gender. A practical way to achieve differentially private is to add Gaussian or Laplace noise on data that needs protection. In [47], Shokri et al. proposed the method to make gradients updates differentially private to protect the training data of a machine learning model.\n\nInvestigating the practices of DP, we find two related defense methods that might be suitable to further evaluate our attacks.\n\nDP at the edge level. This defense is based on SalaDP [44], which is first applied in graph anonymization. Specifically, the statistic that SalaDP protects is the number of edges between nodes of two degrees. The key insight of SalaDP is to introduce fake edges (noise) in the original graph to protect these statistics from differential attacks. In [64], Zhang et al. adopted it in defending against graph recovery attack. We follow them and use the same way to implement SalaDP by randomly adding fake edges. In detail, we manipulate the number of fake edges by a multiplier that is relative to jEj, where E represents the target participant's edge set. We vary the multiplier of the fake edge set from 0.1 to 1 to show the defense efficacy. DP at the representation level. This strategy is introduced in [16], which aims to protect the privacy of representations. In practice, we add Gaussian noise on v tar in each training epoch. We also vary the noise scale from 0.1 to 1 to see the efficiency of the defense. Results of DP at the Edge Level. Adding fake edges can protect the true edges for sure, as these fake edges will make the original unrelated nodes' v tar become similar through GCN's aggregation. Therefore, it also affects the similarity between v 0 tar . From the results in Fig. 11, we can see the clear drops of baseline and Attack v 0 tar . However, the main task's test accuracy also drops below the level of that in the scenario where no label-related relations are used. This is  contrary to VFL's purpose of using label-related relations to improve the model performance.\n\nFor Attack v top , it has a slight decrease with the increase of fake edges. This is because the randomly added edges bring in noise in training, making v top 's quality decrease. However, compared to the drop of the test accuracy, the drop on Attack v top is limited. Furthermore, Attack v adv has hardly changed since its performance depends on the adversary's features.\n\nResults of DP at the Representation Level. From the results in Fig. 12, we find the defense does not affect our attacks on Computer and Photo. On Cora and Citeseer, the defense only works when the noise's scale is large, but it also affects the accuracy of the main task. Thus, this defense strategy's efficacy is limited. Specifically, we summarize our findings as follows.\n\nAttack v adv improves a little with the increase of noise scale. We speculate this is because the added noise makes the top model pay more attention to v adv . Therefore, the quality of v adv is improved, leading to a better performance.\n\nAttack v top 's performance decreases with the increase of noise scale. The result is expected since the noise brings loss in the main task inevitably. The change in test accuracy also confirms this. On Cora and Citeseer, Attack v 0 tar 's performance drops apparently, while on Computer and Photo, the change of Attack v 0 tar 's performance is not obvious. We think this phenomenon is caused by the sparsity of features in Cora and Citeseer. The same scale of noise is much easier to cause errors in the computation of sparse vectors than that of dense vectors.\n\n\nFuther Exploration\n\nRandomly adding a large number of fake edges can resist Attack v 0 tar , but leads to the drop in main task's performance.\n\nTherefore, we are curious whether the participant can add edges purposefully to protect true label-related relations while not degrading the performance of the main task. Specifically, we propose two special graph structures that may help, i.e., complete graph and bipartite graph [5]. The complete graph corresponds to where the participant adds fake edges until every pair of nodes has an edge. The bipartite graph corresponds to a binary classification situation where the participant only adds an edge between two nodes if their labels are different.\n\nHowever, it costs too much memory and computation resources on the four datasets to implement such two unique graphs. Therefore, we evaluate these strategies' efficacy on a toy dataset build by ourselves.\n\nDatasets Configuration. The toy dataset is built by randomly sampling points in a unit circle, and the sign of a sample's x-coordinate determines its label. Node features are x-coordinate, y-coordinate, tangent value (y=x), and the distance from the origin ( ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi x 2 \u00fe y 2 p ). We first construct the two special graphs with the sampled nodes. Then, we build the ground truth by sampling 20% of the edges from the complete graph.\n\nOur evaluation makes the target participant hold all the edges and half of the node features, and the adversary holds the remaining node features. The bottom models for the target participant and adversary are GCN and DNN, respectively. The split ratio of training, validation, and test set is still 5:1:4. Other settings are the same as the default settings in Section 5.\n\nDefense Efficacy. Table 3 summarizes the attack results on both the bipartite graph and the complete graph. Our attacks fail on these two types of graphs. The AUCs are all around 0.5, including the performance of f tar . Furthermore, the accuracy almost does not drop on the bipartite graph, while on the complete graph, the accuracy reduces 2.35%. Since our toy dataset is quite simple, the accuracy may not perfectly reflect the error brought by these two kinds of graphs. We leave the evaluation of these two unique graphs on real-world datasets as future work.\n\nWe think the results of the bipartite graph provide a new fine-grained defense strategy: different from randomly adding fake edges like SalaDP, we can convert two directly connected nodes into indirect connections, but make sure their representations can be aggregated within the GNN's maximum number of hops. In this way, the node's direct topology is hidden, and the original neighbors' information is still aggregated. The only drawback is that the indirect connection may involve irrelevant node information, causing a drop in the performance of the main task. However, we do Fig. 11. Defense with SalaDP. The x-axis is the multiplier values, and the y-axis is the AUC score. The multiplier represents the ratio of the number of fake edges relative to jEj.  not recommend adding edges until the graph is complete, as, in our toy experiments, the task accuracy also drops 2.35%.\n\n\nTruncation\n\nAttack v top and Attack v 0 tar have impressive performance in previous evaluation. However, the adversary needs to know the posteriors to conduct these two attacks. Therefore, truncating the posteriors is also a possible defense strategy in limiting the adversary's capability of relation inference. However, note that Attack v adv is not affected by the strategy, which still leaves the risk of relation leakage.\n\nWe evaluate the most restricted attack scenario where the adversary can only get the predicted labels. Under this setting, there is no metric to calculate the distance between the predicted labels, and also no need to set the threshold for inference. Therefore, we consider a naive attack that directly uses the ground-truth labels to infer the sample relations on the four graph datasets. In this naive approach, we consider an edge between two nodes if they have the same label. Considering that the inference result is only 0 or 1, we use recall instead of AUC to show the effectiveness of the attack. Table 4 shows the recall and precision of inferring edges on the four datasets. The inference results show high recall but low precision, indicating the strategy's effectiveness of limiting the two attacks.\n\n\nSummary of Results\n\nIn summary, we have made the following conclusions.\n\nSalaDP can reduce the risks of leakage in Attack v 0 tar but gets little effect on Attack v adv and Attack v top . Furthermore, the fake edges decrease the main task's accuracy, which is contrary to the purpose of using label-related relations. DP at the representation level does not affect Attack v adv . However, it does have an impact on Attack v 0 tar ' performances, especially on the datasets that have sparse features. When the noise scale is quite large, Attack v 0 tar ' performance drops a lot. However, the main task's accuracy also drops significantly, which is also unacceptable. Truncating the posteriors is a possible defense to restrict the adversary's ability of performing Attack v top and Attack v 0 tar . 9 RELATED WORK\n\n\nAttacks Against Federated Learning\n\nSince the security and privacy of VFL are seldom studied yet, we mainly focus on discussing the attacks against HFL below.\n\nMembership Inference Attack. This kind of attack [35], [36] aims to determine whether a sample's data is used in training. Such attacks' effectiveness is usually correlated to the degree of overfitting, i.e., a heavily overfitted model is more likely to leak data. In VFL, the encrypted alignment is designed to circumvent this attack.\n\nAttribute Inference Attack. This kind of attack tries to obtain attribute information in a semi-honest way [35], [63]. Specifically, the adversary under this setting does not conduct the malicious attack on the model but infers extra information with the model's feedback, e.g., posteriors and gradients. In [35], Melis et al. showed that the updates in FL would leak unintended information about attributes. Specifically, they found a way to infer unintended attributes, e.g., gender or wearing glasses, by training a classifier with gradients as inputs. In VFL, this attack can also cause severe data privacy and security issues.\n\nData Inference Attack. This kind of attack's purpose is to reconstruct the original data [66], [68]. Such attacks also use some side-channel information, like attribute inference attacks. Zhu et al. [68] proposed an optimization-based attack to recover participants' images from the leaked gradients. Their work is then followed by Zhao et al. [66], who found that there also exist risks of label leakage in the leaked gradients.\n\n\nDefenses in Federated Learning\n\nThere is a line of researches focusing on how to protect participants' information in federated learning. According to their mechanism, we mainly classify the works into two categories.\n\nSecure Aggregation. A popular strategy to protect participants' privacy is using secure aggregation [6], [14], [17], [56]. In [6], Bonawitz et al. used blinding with random values, Shamir's Secret Sharing (SSS) [45], and symmetric encryption to prohibit access to local models. Their work is further followed by [17], [56], who add verification step in the top model. Fereidooni et al. proposed a more efficient way to aggregate uploadings in [14]. They suggested aggregating (like sum or mean) the gradients in an encrypted way before submitting them to the global model. Such mechanisms make sure that the server cannot attack a particular client.\n\nDP on Gradients. DP is another practical solution to protect participants' gradients in training [16], [20], [53], [59]. In [16], Geyer et al. proposed a client-sided DP solution to balance the privacy loss and model performance, which is also applied in our countermeasure experiments. The same idea is followed by Yadav et al. In [59], they proposed a strategy based on DP to address the gradient leakage attack in HFL. Specifically, they added Gaussian and Laplace noise on gradients to prevent data reconstruction attacks. Their results prove that after adding small noise, the reconstructed data loses valuable information, while the accuracy loss of the model is acceptable. Hao et al. also apply DP on gradients, but they combine additively HE to strengthen the security of FL. In [53], the authors studied how to find the best convergence performance of FL at a fixed privacy level.\n\n\nRemark\n\nOur work proposes a novel attack against VFL, which is orthogonal to the previous studies. In particular, our attack focuses on stealing the relation with the intermediate calculation results. Part of our attacks is verified in studies [22], [64]. They showed that the posteriors of nodes could be used to reconstruct the anonymized graph in centralized learning. Compared to their works, we have revealed that more kinds of representations in VFL can be used to conduct inference attacks. The posteriors used in centralized learning are just one kind of representations in our setting. Furthermore, we evaluate the attacks' efficacy in three possible scenarios, including where no topology information is used. It also indicates that our attack is not limited to graph data. Indeed, our work reveals the primary key to the success of such attacks, i.e., the label is the key feature to model the complex relationship between samples, and thus the representations trained by the label are at risk of leaking the relations. We use graph datasets in experiments because they collect edges from the real world and exclude the subjectiveness.\n\n\nDISCUSSION\n\nEvaluation on Other ML Models. We have demonstrated that our relation inference attack can be applied to VFL with DNN as the ML model, in which the adversary can leverage the intermediate representations to calculate similarity. For other ML models which do not involve calculating latent representations like gradient boosting tree models, it is an interesting future work to explore their unintended relation leakage risks. Specifically, instead of calculating the distance between representations, we think it is promising to compare two samples' prediction paths in the tree model to infer their relation. Theoretical Proof of DP-Based Defense Strategy. We follow previous works to implement the obfuscation strategy through DP-based defense methods. Specifically, we borrow the idea of DP to introduce Gaussian noise to data for defense but lack theoretical proof for them. For example, we do not discuss how efficacy these DP methods can achieve given different privacy budgets. It is a very challenging work to complete the DP-based theoretical proof in VFL. We leave it as future work.\n\n\nCONCLUSION\n\nIn this work, we propose the first relation inference attack against VFL. Specifically, we identify three kinds of representations in VFL that can cause relation leakage. Furthermore, to evaluate our attacks' efficacy, we conduct experiments under three different scenarios. Extensive evaluation over four real-world datasets shows that our attacks can accurately infer the relations. Moreover, we evaluate several potential defense strategies against our attacks. Experimental results show that the defenses have limited impacts on our attacks, highlighting the need for more efficient defense strategies. We hope this work will spur future work to design a more secure VFL framework. Jun Zhou is currently a senior staff engineer with Ant Group. His research mainly focuses on machine learning and data mining. He has participated in the development of several distributed systems and machine learning platforms in Alibaba and Ant Group, such as Apsaras, MaxCompute, and Kun-Peng. He has published more than 40 papers in top-tier machine learning and data mining conferences, including VLDB, WWW, SIGIR, NeurIPS, AAAI, IJCAI, and KDD.\n\nTing Wang received the PhD degree from Georgia Tech. He is an assistant professor with the College of Information Sciences and Technology, Penn State. He conducts research in the intersection of data science and privacy & security. His ongoing work focuses on making machine learning systems more practically usable through improving their security assurance, privacy preservation and decision-making transparency.\n\n\" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.\n\nFig. 1 .\n1The two mainstream FL frameworks. U denotes the sample space, and F denotes the feature space.\n\n2 \u00c1\n2Agg v i 2N \u00f0u\u00de \u00f0h \u00f0k\u00de i \u00de\u00de; where h \u00f0k\u00de u denotes the computation result of u at layer k; N u denotes the sampled neighbor set;\n\nFig. 2 .\n2A schematic view of our sample relation inference attack against VFL.\n\nAlgorithm 2 .\n2Latent Representation Approximation Input: Posteriors v top , top model f top , and adversary's latent representations v adv , error bound . Output: Approximate solution of the target participant's representations v 0 tar . 1: Initialize v 0 tar as zeros. 2: Calculate loss L \u00bc kv top \u00c0 f top \u00f0\u00bdv adv ; v 0 tar \u00dek 2 : 3: Calculate gradient g \u00bc\n\nFig. 4 .\n4The mainframe of the latent representation approximation. 1) the adversary initializes v 0 tar as 0 and feeds it with v adv to the top model. 2) the top model calculates the approximated prediction. 3) the adversary calculates the loss L between the approximated prediction and v top . 4) the adversary uses L to update v 0 tar until L is under a specific bound .\n\n;\nwhere u and v are the latent representations of sample u and v, and u and v are the mean values of u and v, respectively. A lower distance means two samples are more likely to have an edge between them.\n\nFig. 5 .\n5Attack performance when the adversary only provides features. The x-axis represents the feature ratio of the adversary, and the y-axis represents the AUC and accuracy.\n\n\nSpecifically, there are four factors we would like to examine: 1) the number of parties in VFL; 2) different implementations of GNNs; 3) the number of FC layers in the top model; 4) the dimension of each participant's latent representations.\n\nFig. 8 .\n8Attack performance with GraphSAGE as the bottom model. The x-axis represents the features ratio of the adversary, and the y-axis represents the AUC score and accuracy.\n\nFig. 9 .\n9Sensitivity analysis of the number layers in the top model. The x-axis represents the number of layers and the y-axis represents the AUC score.\n\nFig. 10 .\n10Sensitivity analysis of the dimension of latent representations. The x-axis represents the dimension (log 2 \u00f0dimension\u00de) and the y-axis represents the AUC score.\n\nFig. 12 .\n12Defense with DP at the representation level. The x-axis is the scale of noise, and the y-axis is the AUC score.\n\n\nwhere d \u00c3 is the dimension size of the latent representation; the top model then aggregates the sample's representations from each participant and calculates the result v t top according to the mapping f top \u00f0\u00bdv t 1\n\nTABLE 1\n1Datasets Statistics \n\nDataset \nCora Citeseer Computer \nPhoto \n\nType \ncitation citation co-purchase co-purchase \n\n# Nodes \n2708 \n3327 \n13381 \n7487 \n# Edges \n5429 \n4732 \n245778 \n119043 \n# Classes \n7 \n6 \n10 \n8 \n# Attributes \n1433 \n3703 \n767 \n745 \nEdge Density \n0.0014 0.0009 \n0.0027 \n0.0021 \nFeature Sparsity 0.0127 0.0085 \n0.3484 \n0.3474 \n\n\n\nthe results, we can find Attack v top and Attack v 0tar \n\nachieves high AUC with different feature ratios, especially \nwhen the target participant has the most features. For \ninstance, for Cora, when the target participant has 90% of \nfeatures, Attack v top achieves an AUC of 0.9, and Attack v 0 \n\ntar \n\nachieves AUC above 0.95. Attack v adv , however, need more \nfeatures to achieve a high AUC. This is reasonable since \nmore features make A f adv more possible to overlap with A g . \nFurthermore, we have the following observations. \n\n1. https://pytorch-geometric.readthedocs.io/en/latest/index.html \n2. https://www.fedai.org/ \nAttack v top . This attack's performance has a decreas-\ning trend when the target participant's portion of \nfeatures decreases. Specifically, the highest AUCs of \nthe attack achieve above 0.9 on all datasets when the \ntarget participant has 90% of features, and the lowest \nAUCs are about 0.8 when the target participant's fea-\nture ratio is 10%. The reason is that when the target \nparticipant has 90% of features, he/she can provide \nbetter v tar than the case where the adversary has \n90% of features because of the label-related relations. \n\n\nTABLE 2 Attack\n2Performance When the Adversary Provides Other Label-Related Relations on Cora Threshold/ # edges Test Accuracy Attack v adv Attack v top Attack v 0tar \n\n\n\nTABLE 3\n3Results for Defense With Two Types of Special GraphBipartite Graph \nComplete Graph \n\nBaseline(v tar ) \n0.4970 \n0.5007 \nAttack v adv \n0.4967 \n0.5002 \nAttack v top \n0.4996 \n0.5014 \nAttack v 0 \n\ntar \n\n0.4987 \n0.5007 \n\nOriginal Accuracy \n0.9044 \n0.9268 \nAccuracy After Defense \n0.9000 \n0.9033 \n\n\nTABLE 4\n4Recall of the Naive ApproachDataset \nCora \nCiteseer \nComputer \nPhoto \n\nRecall (%) \n80.99 \n73.61 \n77.72 \n82.72 \nPrecision (%) \n1.29 \n0.67 \n1.92 \n4.01 \n\n\n\nTianyu Du received the PhD degree from the College of Computer Science and Technology, Zhejiang University, China, on June 2022 (supervised by Prof. Shouling Ji). She is currently a PostDoc with Penn State University supervised by Prof. Ting Wang. Her main research interests include AI security and adversarial machine learning. Yuwen Pu received the PhD degree with the School of Big Data & Software Engineering from Chongqing University in 2021. He is a PostDoctor with the College of Computer Science and Technology, Zhejiang University. His research interests include big data security and privacy-preserving, AI security.\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nPengyu Qiu received the bachelor's degree from Zhejiang University. He is currently working toward the PhD degree with the College of Computer Science and Technology, Zhejiang University. His current research interests include AI security, adversarial learning.Xuhong Zhang received the PhD degree in computer engineering from the University of Central Florida in 2017. He is a ZJU 100-Young professor with the School of Software Technology, Zhejiang University. His research interests include distributed big data and AI systems, big data mining and analysis, data-driven security, AI and Security. He has authored more than 20 publications in premier journals and conferences such as TDSC, TPDC, IEEE Security & Privacy, USENIX Security, ACM CCS, NDSS, VLDB, etc.Shouling Ji (Member, IEEE) received the BS (with Honors) and MS degrees both in computer science from Heilongjiang University, and the PhD degree in electrical and computer engineering from Georgia Institute of Technology, Georgia State University. He is a ZJU 100-Young professor with the College of Computer Science and Technology, Zhejiang University and a research faculty with the School of Electrical and Computer Engineering, Georgia Institute of Technology (Georgia Tech). His current research interests include data-driven security and privacy, AI security and big data analytics. He is a member of ACM and CCF and was the Membership chair of the IEEE Student Branch, Georgia State University (2012-2013).\nCrypten: Secure multi-party computation meets machine learning. B Knott, S Venkataraman, A Hannun, S Sengupta, M Ibrahim, L Van Der Maaten, arXiv:2109.009842021B. Knott, S. Venkataraman, A. Hannun, S. Sengupta, M. Ibrahim, and L. van der Maaten, \"Crypten: Secure multi-party computation meets machine learning,\" 2021, arXiv:2109.00984.\n\n. Tf Encrypted, TF Encrypted. [Online]. Available: https://tf-encrypted.io\n\n. B Bollob, Modern Graph Theory. 184SpringerB. Bollob as, Modern Graph Theory, vol. 184, Berlin, Germany: Springer, 1998.\n\nPractical secure aggregation for privacy-preserving machine learning. K Bonawitz, Proc. ACM SIGSAC Conf. Comput. Commun. Secur. ACM SIGSAC Conf. Comput. Commun. SecurK. Bonawitz et al., \"Practical secure aggregation for privacy-pre- serving machine learning,\" in Proc. ACM SIGSAC Conf. Comput. Commun. Secur., 2017, pp. 1175-1191.\n\nFederated learning of predictive models from federated electronic health records. T S Brisimi, R Chen, T Mela, A Olshevsky, I C Paschalidis, W Shi, Int. J. Med. Inform. 112T. S. Brisimi, R. Chen, T. Mela, A. Olshevsky, I. C. Paschalidis, and W. Shi, \"Federated learning of predictive models from federated electronic health records,\" Int. J. Med. Inform., vol. 112, pp. 59-67, 2018.\n\nCluster-driven graph federated learning over multiple domains. D Caldarola, M Mancini, F Galasso, M Ciccone, E , B Caputo, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitD. Caldarola, M. Mancini, F. Galasso, M. Ciccone, E. Rodol a, and B. Caputo, \"Cluster-driven graph federated learning over multi- ple domains,\" in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog- nit., 2021, pp. 2749-2758.\n\nFede: Embedding knowledge graphs in federated setting. M Chen, W Zhang, Z Yuan, Y Jia, H Chen, Proc. 10th Int. Joint Conf. Knowl. Graphs, 2020. 10th Int. Joint Conf. Knowl. Graphs, 2020M. Chen, W. Zhang, Z. Yuan, Y. Jia, and H. Chen, \"Fede: Embed- ding knowledge graphs in federated setting,\" in Proc. 10th Int. Joint Conf. Knowl. Graphs, 2020, pp. 80-88.\n\nFederated learning for privacy-preserving ai. Y Cheng, Y Liu, T Chen, Q Yang, Commun. ACM. 6312Y. Cheng, Y. Liu, T. Chen, and Q. Yang, \"Federated learning for pri- vacy-preserving ai,\" Commun. ACM, vol. 63, no. 12, pp. 33-36, 2020.\n\nConvolutional neural networks on graphs with fast localized spectral filtering. M Defferrard, X Bresson, P Vandergheynst, Proc. Adv. Neural Informat. Process. Syst. Adv. Neural Informat. ess. SystM. Defferrard, X. Bresson, and P. Vandergheynst, \"Convolutional neural networks on graphs with fast localized spectral filtering,\" in Proc. Adv. Neural Informat. Process. Syst., 2016, pp. 3844-3852.\n\nDifferential privacy: A survey of results. C Dwork, Proc. Int. Conf. Theory Appl. Models Comput. Int. Conf. Theory Appl. Models ComputC. Dwork, \"Differential privacy: A survey of results,\" in Proc. Int. Conf. Theory Appl. Models Comput., 2008, pp. 1-19.\n\nGraph neural networks for social recommendation. W Fan, Proc. World Wide Web Conf. World Wide Web ConfW. Fan et al., \"Graph neural networks for social recommen- dation,\" in Proc. World Wide Web Conf., 2019, pp. 417-426.\n\nSafelearn: Secure aggregation for private federated learning. H Fereidooni, Proc. IEEE Secur. Privacy Workshops. IEEE Secur. Privacy WorkshopsH. Fereidooni et al., \"Safelearn: Secure aggregation for private fed- erated learning,\" in Proc. IEEE Secur. Privacy Workshops, 2021, pp. 56-62.\n\nFully homomorphic encryption using ideal lattices. C Gentry, Proc. 41st Annu. ACM Symp. Theory Comput. 41st Annu. ACM Symp. Theory ComputC. Gentry, \"Fully homomorphic encryption using ideal lattices,\" in Proc. 41st Annu. ACM Symp. Theory Comput., 2009, pp. 169-178.\n\nDifferentially private federated learning: A client level perspective. R C Geyer, T Klein, M Nabi, arXiv:1712.07557R. C. Geyer, T. Klein, and M. Nabi, \"Differentially private feder- ated learning: A client level perspective,\" 2017, arXiv:1712.07557.\n\nVeriFL: Communication-efficient and fast verifiable aggregation for federated learning. X Guo, IEEE Trans. Inf. Forensics Secur. 16X. Guo et al., \"VeriFL: Communication-efficient and fast verifiable aggregation for federated learning,\" IEEE Trans. Inf. Forensics Secur., vol. 16, pp. 1736-1751, 2021.\n\nInductive representation learning on large graphs. W Hamilton, Z Ying, J Leskovec, Proc. Adv. Neural Informat. Process. Syst. Adv. Neural Informat. ess. SystW. Hamilton, Z. Ying, and J. Leskovec, \"Inductive representation learning on large graphs,\" in Proc. Adv. Neural Informat. Process. Syst., 2017, pp. 1024-1034.\n\nError detecting and error correcting codes. R W Hamming, Bell Syst. Tech. J. 292R. W. Hamming, \"Error detecting and error correcting codes,\" Bell Syst. Tech. J., vol. 29, no. 2, pp. 147-160, 1950.\n\nTowards efficient and privacy-preserving federated deep learning. M Hao, H Li, G Xu, S Liu, H Yang, Proc. IEEE Int. Conf. Commun. IEEE Int. Conf. CommunM. Hao, H. Li, G. Xu, S. Liu, and H. Yang, \"Towards efficient and privacy-preserving federated deep learning,\" in Proc. IEEE Int. Conf. Commun., 2019, pp. 1-6.\n\nFederated learning for mobile keyboard prediction. A Hard, arXiv:1811.03604A. Hard et al., \"Federated learning for mobile keyboard pre- diction,\" 2018, arXiv:1811.03604.\n\nStealing links from graph neural networks. X He, J Jia, M Backes, N Z Gong, Y Zhang, Proc. 30th USENIX Secur. Symp. 30th USENIX Secur. SympX. He, J. Jia, M. Backes, N. Z. Gong, and Y. Zhang, \"Stealing links from graph neural networks,\" in Proc. 30th USENIX Secur. Symp., 2021, pp. 2669-2686.\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, M Welling, arXiv:1609.02907T. N. Kipf and M. Welling, \"Semi-supervised classification with graph convolutional networks,\" 2016, arXiv:1609.02907.\n\nFederated learning: Strategies for improving communication efficiency. J Kone Cn\u1ef3, H B Mcmahan, F X Yu, P Richt Arik, A T Suresh, D Bacon, arXiv:1610.05492J. Kone cn\u1ef3, H. B. McMahan, F. X. Yu, P. Richt arik, A. T. Suresh, and D. Bacon, \"Federated learning: Strategies for improving communi- cation efficiency,\" 2016, arXiv:1610.05492.\n\nSocioeconomic correlations and stratification in social-communication networks. Y Leo, E Fleury, J I Alvarez-Hamelin, C Sarraute, M Karsai, J. Roy. Soc. Interface. 13125Y. Leo, E. Fleury, J. I. Alvarez-Hamelin, C. Sarraute, and M. Kar- sai, \"Socioeconomic correlations and stratification in social-com- munication networks,\" J. Roy. Soc. Interface, vol. 13, no. 125, pp. 20160598, Dec. 2016, Art. no. 20160598.\n\nLabel leakage and protection in two-party split learning. O Li, arXiv:2102.085042021O. Li et al., \"Label leakage and protection in two-party split learning,\" 2021, arXiv:2102.08504.\n\nAuc: A statistically consistent and more discriminating measure than accuracy. C X Ling, Proc. Int. Joint Conf. Int. Joint ConfC. X. Ling et al., \"Auc: A statistically consistent and more discrim- inating measure than accuracy,\" in Proc. Int. Joint Conf. Artif. Intell., 2003, pp. 519-524.\n\nA communication efficient vertical federated learning framework. Y Liu, arXiv:1912.11187Y. Liu et al., \"A communication efficient vertical federated learn- ing framework,\" 2019, arXiv:1912.11187.\n\nA unified approach to interpreting model predictions. S M Lundberg, S.-I Lee, Proc. 31st Int. Conf. Neural Informat. Process. Syst. 31st Int. Conf. Neural Informat. ess. SystS. M. Lundberg and S.-I. Lee, \"A unified approach to interpreting model predictions,\" in Proc. 31st Int. Conf. Neural Informat. Process. Syst., 2017, pp. 4768-4777.\n\nFeature inference attack on model predictions in vertical federated learning. X Luo, Y Wu, X Xiao, B C Ooi, X. Luo, Y. Wu, X. Xiao, and B. C. Ooi, \"Feature inference attack on model predictions in vertical federated learning,\" 2020. [Online].\n\nPrivacy and robustness in federated learning: Attacks and defenses. L Lyu, arXiv:2012.063372020L. Lyu et al., \"Privacy and robustness in federated learning: Attacks and defenses,\" 2020, arXiv:2012.06337.\n\nA comparison of algorithms for maximum entropy parameter estimation. R Malouf, ser. COLING-02. USAAssociation for Computational LinguisticsR. Malouf, \"A comparison of algorithms for maximum entropy parameter estimation,\" ser. COLING-02. USA: Association for Computational Linguistics, 2002, pp. 1-7.\n\nDistribution of sexual health knowledge and attitudes in adolescent social networks: Social network analysis of data from the STIs and sexual health feasibility study. M Mccann, C Broccatelli, L Moore, K Mitchell, The Lancet. 392S60M. McCann, C. Broccatelli, L. Moore, and K. Mitchell, \"Distribution of sexual health knowledge and attitudes in adolescent social net- works: Social network analysis of data from the STIs and sexual health feasibility study,\" The Lancet, vol. 392, 2018, Art. no. S60.\n\nCommunication-efficient learning of deep networks from decentralized data. B Mcmahan, E Moore, D Ramage, S Hampson, B A Y Arcas, Proc. Conf. Artif. Intell. Statist. Conf. Artif. Intell. StatistB. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. Y. Arcas, \"Communication-efficient learning of deep networks from decentralized data,\" in Proc. Conf. Artif. Intell. Statist., 2017, pp. 1273-1282.\n\nExploiting unintended feature leakage in collaborative learning. L Melis, C Song, E De Cristofaro, V Shmatikov, Proc. IEEE Symp. Secur. Privacy. IEEE Symp. Secur. PrivacyL. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, \"Exploiting unintended feature leakage in collaborative learning,\" in Proc. IEEE Symp. Secur. Privacy, 2019, pp. 691-706.\n\nComprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. M Nasr, R Shokri, A Houmansadr, Proc. IEEE Symp. Secur. Privacy. IEEE Symp. Secur. PrivacyM. Nasr, R. Shokri, and A. Houmansadr, \"Comprehensive pri- vacy analysis of deep learning: Passive and active white-box infer- ence attacks against centralized and federated learning,\" in Proc. IEEE Symp. Secur. Privacy, 2019, pp. 739-753.\n\nA vertical federated learning framework for graph convolutional network. X Ni, X Xu, L Lyu, C Meng, W Wang, arXiv:2106.115932021X. Ni, X. Xu, L. Lyu, C. Meng, and W. Wang, \"A vertical fed- erated learning framework for graph convolutional network,\" 2021, arXiv:2106.11593.\n\nPublic-key cryptosystems based on composite degree residuosity classes. P Paillier, Proc. Int. Conf. Theory Appl. Cryptographic Techn. Int. Conf. Theory Appl. Cryptographic TechnP. Paillier, \"Public-key cryptosystems based on composite degree residuosity classes,\" in Proc. Int. Conf. Theory Appl. Cryptographic Techn., 1999, pp. 223-238.\n\nDeepwalk: Online learning of social representations. B Perozzi, R Al-Rfou, S Skiena, Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. 20th ACM SIGKDD Int. Conf. Knowl. Discov. Data MiningB. Perozzi, R. Al-Rfou, and S. Skiena, \"Deepwalk: Online learning of social representations,\" in Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining, 2014, pp. 701-710.\n\nNetwork embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. J Qiu, Y Dong, H Ma, J Li, K Wang, J Tang, Proc. 11th ACM Int. Conf. Web Search Data Mining. 11th ACM Int. Conf. Web Search Data MiningJ. Qiu, Y. Dong, H. Ma, J. Li, K. Wang, and J. Tang, \"Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec,\" in Proc. 11th ACM Int. Conf. Web Search Data Min- ing, 2018, pp. 459-467.\n\nDeepinf: Social influence prediction with deep learning. J Qiu, J Tang, H Ma, Y Dong, K Wang, J Tang, Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining. 24th ACM SIGKDD Int. Conf. Knowl. Discov. Data MiningJ. Qiu, J. Tang, H. Ma, Y. Dong, K. Wang, and J. Tang, \"Deepinf: Social influence prediction with deep learning,\" in Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining, 2018, pp. 2110-2119.\n\nThe perceptron: A probabilistic model for information storage and organization in the brain. F Rosenblatt, Psychol. Rev. 656F. Rosenblatt, \"The perceptron: A probabilistic model for informa- tion storage and organization in the brain,\" Psychol. Rev., vol. 65, no. 6, 1958, Art. no. 386.\n\nCrypt?: Crypto-assisted differential privacy on untrusted servers. A Roy Chowdhury, C Wang, X He, A Machanavajjhala, S Jha, Proc. ACM SIGMOD Int. Conf. Manage. Data. ACM SIGMOD Int. Conf. Manage. DataA. Roy Chowdhury, C. Wang, X. He, A. Machanavajjhala, and S. Jha, \"Crypt?: Crypto-assisted differential privacy on untrusted servers,\" in Proc. ACM SIGMOD Int. Conf. Manage. Data, 2020, pp. 603-619.\n\nSharing graphs using differentially private graph models. A Sala, X Zhao, C Wilson, H Zheng, B Y Zhao, Proc. ACM SIGCOMM Conf. Internet Meas. Conf. ACM SIGCOMM Conf. Internet Meas. ConfA. Sala, X. Zhao, C. Wilson, H. Zheng, and B. Y. Zhao, \"Sharing graphs using differentially private graph models,\" in Proc. ACM SIGCOMM Conf. Internet Meas. Conf., 2011, pp. 81-98.\n\nHow to share a secret. A Shamir, Commun. ACM. 2211A. Shamir, \"How to share a secret,\" Commun. ACM, vol. 22, no. 11, pp. 612-613, Nov. 1979.\n\nPitfalls of graph neural network evaluation. O Shchur, M Mumme, A Bojchevski, S G\u20ac Unnemann, arXiv:1811.05868O. Shchur, M. Mumme, A. Bojchevski, and S. G\u20ac unnemann, \"Pitfalls of graph neural network evaluation,\" 2018, arXiv:1811.05868.\n\nPrivacy-preserving deep learning. R Shokri, V Shmatikov, Proc. 22nd ACM SIGSAC Conf. 22nd ACM SIGSAC ConfR. Shokri and V. Shmatikov, \"Privacy-preserving deep learning,\" in Proc. 22nd ACM SIGSAC Conf. Comput. Commun. Secur., 2015, pp. 1310-1321.\n\nTowards federated graph learning for collaborative financial crimes detection. T Suzumura, arXiv:1909.12946T. Suzumura et al., \"Towards federated graph learning for collab- orative financial crimes detection,\" 2019, arXiv:1909.12946.\n\nThe EU general data protection regulation (GDPR). P Voigt, A Von, Bussche, SpringerBerlin, GermanyA Practical Guide. 1st EdP. Voigt and A. Von dem Bussche, \"The EU general data protec- tion regulation (GDPR),\" A Practical Guide, 1st Ed., Berlin, Ger- many: Springer, 2017.\n\nInterpret federated learning with shapley values. G Wang, arXiv:1905.04519G. Wang, \"Interpret federated learning with shapley values,\" 2019, arXiv:1905.04519.\n\nA case of traffic violations insurance-using federated learning. Webank, Webank, \"A case of traffic violations insurance-using federated learning,\" 2020. [Online]. Available: https://www.fedai.org/ cases\n\nUtilization of FATE in risk management of credit in small and micro enterprises. Webank, Webank, \"Utilization of FATE in risk management of credit in small and micro enterprises,\" 2020. [Online]. Available: https:// www.fedai.org/cases\n\nFederated learning with differential privacy: Algorithms and performance analysis. K Wei, IEEE Trans. Informat. Forensics Secur. 15K. Wei et al., \"Federated learning with differential privacy: Algo- rithms and performance analysis,\" IEEE Trans. Informat. Forensics Secur., vol. 15, pp. 3454-3469, 2020.\n\nSimplifying graph convolutional networks. F Wu, T Zhang, A H De SouzaJr, C Fifty, T Yu, K Q Weinberger, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. LearnF. Wu, T. Zhang, A. H. de Souza Jr, C. Fifty, T. Yu, and K. Q. Wein- berger, \"Simplifying graph convolutional networks,\" in Proc. Int. Conf. Mach. Learn., 2019, pp. 6861-6871.\n\nFederated graph classification over non-IID graphs. H Xie, J Ma, L Xiong, C Yang, Proc. Adv. Neural Inf. Process. Syst. Adv. Neural Inf. ess. Syst18H. Xie, J. Ma, L. Xiong, and C. Yang, \"Federated graph classifica- tion over non-IID graphs,\" in Proc. Adv. Neural Inf. Process. Syst., 2021, pp. 18 839-18 852.\n\nVerifyNet: Secure and verifiable federated learning. G Xu, H Li, S Liu, K Yang, X Lin, IEEE Trans. Inf. Forensics Secur. 15G. Xu, H. Li, S. Liu, K. Yang, and X. Lin, \"VerifyNet: Secure and verifiable federated learning,\" IEEE Trans. Inf. Forensics Secur., vol. 15, pp. 911-926, 2020.\n\nFederated learning for healthcare informatics. J Xu, B S Glicksberg, C Su, P Walker, J Bian, F Wang, J. Healthcare Informat. Res. 5J. Xu, B. S. Glicksberg, C. Su, P. Walker, J. Bian, and F. Wang, \"Federated learning for healthcare informatics,\" J. Healthcare Infor- mat. Res., vol. 5, pp. 1-19, 2020.\n\nHow powerful are graph neural networks?. K Xu, W Hu, J Leskovec, S Jegelka, arXiv:1810.00826K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \"How powerful are graph neural networks?\" 2018, arXiv:1810.00826.\n\nDifferential privacy approach to solve gradient leakage attack in a federated machine learning environment. K Yadav, B B Gupta, K T Chui, K Psannis, Proc. Int. Conf. Comput. Data Social Netw. Int. Conf. Comput. Data Social NetwK. Yadav, B. B. Gupta, K. T. Chui, and K. Psannis, \"Differential privacy approach to solve gradient leakage attack in a federated machine learning environment,\" in Proc. Int. Conf. Comput. Data Social Netw., 2020, pp. 378-385.\n\nFederated machine learning: Concept and applications. Q Yang, Y Liu, T Chen, Y Tong, ACM Trans. Intell. Syst. Technol. 102Q. Yang, Y. Liu, T. Chen, and Y. Tong, \"Federated machine learn- ing: Concept and applications,\" ACM Trans. Intell. Syst. Technol., vol. 10, no. 2, pp. 1-19, 2019.\n\nRevisiting semi-supervised learning with graph embeddings. Z Yang, W Cohen, R Salakhudinov, Proc. Int. Conf. Mach. Learn. Int. Conf. Mach. LearnZ. Yang, W. Cohen, and R. Salakhudinov, \"Revisiting semi-super- vised learning with graph embeddings,\" in Proc. Int. Conf. Mach. Learn., 2016, pp. 40-48.\n\nGelu-Net: A globally encrypted, locally unencrypted deep neural network for privacy-preserved learning. Q Zhang, C Wang, H Wu, C Xin, T V Phuong, Proc. 27th Int. Joint Conf. 27th Int. Joint ConfQ. Zhang, C. Wang, H. Wu, C. Xin, and T. V. Phuong, \"Gelu-Net: A globally encrypted, locally unencrypted deep neural network for privacy-preserved learning,\" in Proc. 27th Int. Joint Conf. Artif. Intell., 2018, pp. 3933-3939.\n\nLeakage of dataset properties in multi-party machine learning. W Zhang, S Tople, O Ohrimenko, Proc. 30th USENIX Secur. Symp. 30th USENIX Secur. SympW. Zhang, S. Tople, and O. Ohrimenko, \"Leakage of dataset prop- erties in multi-party machine learning,\" in Proc. 30th USENIX Secur. Symp., 2021, pp. 2687-2704.\n\nTowards plausible graph anonymization. Y Zhang, M Humbert, B Surma, P Manoharan, J Vreeken, M Backes, Proc. Netw. Distrib. Syst. Secur. Symp. Y. Zhang, M. Humbert, B. Surma, P. Manoharan, J. Vreeken, and M. Backes, \"Towards plausible graph anonymization,\" Proc. Netw. Distrib. Syst. Secur. Symp., 2020.\n\nAdditively homomorphical encryption based deep neural network for asymmetrically collaborative machine learning. Y Zhang, H Zhu, arXiv:2007.068492020Y. Zhang and H. Zhu, \"Additively homomorphical encryption based deep neural network for asymmetrically collaborative machine learning,\" 2020, arXiv:2007.06849.\n\niDLG: Improved deep leakage from gradients. B Zhao, K R Mopuri, H Bilen, arXiv:2001.026102020B. Zhao, K. R. Mopuri, and H. Bilen, \"iDLG: Improved deep leak- age from gradients,\" 2020, arXiv:2001.02610.\n\nVertically federated graph neural network for privacy-preserving node classification. J Zhou, arXiv:2005.119032021J. Zhou et al., \"Vertically federated graph neural network for pri- vacy-preserving node classification,\" 2021, arXiv:2005.11903.\n\nDeep leakage from gradients. L Zhu, Z Liu, S Han, Proc. Adv. Neural Informat. Process. Syst. Adv. Neural Informat. ess. SystL. Zhu, Z. Liu, and S. Han, \"Deep leakage from gradients,\" in Proc. Adv. Neural Informat. Process. Syst., 2019, pp. 14 774-14 784.\n", "annotations": {"author": "[{\"end\":93,\"start\":82},{\"end\":107,\"start\":94},{\"end\":132,\"start\":108},{\"end\":143,\"start\":133},{\"end\":153,\"start\":144},{\"end\":163,\"start\":154},{\"end\":174,\"start\":164}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":89},{\"end\":106,\"start\":101},{\"end\":131,\"start\":129},{\"end\":142,\"start\":140},{\"end\":152,\"start\":150},{\"end\":162,\"start\":158},{\"end\":173,\"start\":169}]", "author_first_name": "[{\"end\":88,\"start\":82},{\"end\":100,\"start\":94},{\"end\":128,\"start\":120},{\"end\":139,\"start\":133},{\"end\":149,\"start\":144},{\"end\":157,\"start\":154},{\"end\":168,\"start\":164}]", "author_affiliation": null, "title": "[{\"end\":79,\"start\":1},{\"end\":253,\"start\":175}]", "venue": null, "abstract": "[{\"end\":2329,\"start\":339}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2519,\"start\":2515},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2726,\"start\":2722},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2732,\"start\":2728},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3003,\"start\":2999},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3355,\"start\":3351},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":3361,\"start\":3357},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3615,\"start\":3612},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3621,\"start\":3617},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3627,\"start\":3623},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3633,\"start\":3629},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3639,\"start\":3635},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3645,\"start\":3641},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3864,\"start\":3860},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4060,\"start\":4056},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4066,\"start\":4062},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":4072,\"start\":4068},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4209,\"start\":4205},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4215,\"start\":4211},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4514,\"start\":4511},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4519,\"start\":4516},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4525,\"start\":4521},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4531,\"start\":4527},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4537,\"start\":4533},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":4543,\"start\":4539},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4793,\"start\":4789},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6022,\"start\":6018},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10004,\"start\":10000},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10339,\"start\":10336},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10356,\"start\":10353},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10405,\"start\":10401},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10522,\"start\":10518},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11136,\"start\":11132},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11142,\"start\":11138},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11228,\"start\":11224},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12033,\"start\":12029},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14083,\"start\":14079},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14089,\"start\":14085},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14107,\"start\":14103},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15035,\"start\":15031},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15356,\"start\":15352},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15411,\"start\":15407},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15417,\"start\":15413},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18850,\"start\":18846},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19764,\"start\":19760},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":19912,\"start\":19908},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19958,\"start\":19954},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26344,\"start\":26340},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":30586,\"start\":30582},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30639,\"start\":30635},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32229,\"start\":32225},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32517,\"start\":32513},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32578,\"start\":32574},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":32584,\"start\":32580},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34320,\"start\":34316},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":40209,\"start\":40205},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":55571,\"start\":55567},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":55977,\"start\":55973},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":56307,\"start\":56303},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":56603,\"start\":56599},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":57060,\"start\":57056},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":59829,\"start\":59826},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":64875,\"start\":64871},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":64881,\"start\":64877},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":65270,\"start\":65266},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":65276,\"start\":65272},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":65471,\"start\":65467},{\"end\":65485,\"start\":65473},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":65885,\"start\":65881},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":65891,\"start\":65887},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":65995,\"start\":65991},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":66140,\"start\":66136},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":66546,\"start\":66543},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":66552,\"start\":66548},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":66558,\"start\":66554},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":66564,\"start\":66560},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":66572,\"start\":66569},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":66658,\"start\":66654},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":66759,\"start\":66755},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":66765,\"start\":66761},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":66890,\"start\":66886},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":67195,\"start\":67191},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":67201,\"start\":67197},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":67207,\"start\":67203},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":67213,\"start\":67209},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":67222,\"start\":67218},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":67430,\"start\":67426},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":67886,\"start\":67882},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":68235,\"start\":68231},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":68241,\"start\":68237}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":72035,\"start\":71930},{\"attributes\":{\"id\":\"fig_1\"},\"end\":72169,\"start\":72036},{\"attributes\":{\"id\":\"fig_2\"},\"end\":72250,\"start\":72170},{\"attributes\":{\"id\":\"fig_3\"},\"end\":72610,\"start\":72251},{\"attributes\":{\"id\":\"fig_4\"},\"end\":72985,\"start\":72611},{\"attributes\":{\"id\":\"fig_5\"},\"end\":73191,\"start\":72986},{\"attributes\":{\"id\":\"fig_6\"},\"end\":73370,\"start\":73192},{\"attributes\":{\"id\":\"fig_7\"},\"end\":73614,\"start\":73371},{\"attributes\":{\"id\":\"fig_8\"},\"end\":73793,\"start\":73615},{\"attributes\":{\"id\":\"fig_9\"},\"end\":73948,\"start\":73794},{\"attributes\":{\"id\":\"fig_10\"},\"end\":74123,\"start\":73949},{\"attributes\":{\"id\":\"fig_11\"},\"end\":74248,\"start\":74124},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":74466,\"start\":74249},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":74814,\"start\":74467},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":75993,\"start\":74815},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":76164,\"start\":75994},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":76465,\"start\":76165},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":76626,\"start\":76466},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":77256,\"start\":76627}]", "paragraph": "[{\"end\":2847,\"start\":2345},{\"end\":3917,\"start\":2849},{\"end\":4656,\"start\":3919},{\"end\":4794,\"start\":4658},{\"end\":5341,\"start\":4796},{\"end\":5961,\"start\":5343},{\"end\":6476,\"start\":5963},{\"end\":6842,\"start\":6478},{\"end\":7541,\"start\":6844},{\"end\":7716,\"start\":7543},{\"end\":8707,\"start\":7718},{\"end\":9299,\"start\":8752},{\"end\":9498,\"start\":9301},{\"end\":9832,\"start\":9693},{\"end\":10005,\"start\":9834},{\"end\":10588,\"start\":10032},{\"end\":11005,\"start\":10590},{\"end\":11482,\"start\":11007},{\"end\":11843,\"start\":11502},{\"end\":12311,\"start\":11868},{\"end\":12844,\"start\":12313},{\"end\":13106,\"start\":12870},{\"end\":13665,\"start\":13108},{\"end\":13871,\"start\":13667},{\"end\":14549,\"start\":13873},{\"end\":14949,\"start\":14575},{\"end\":15781,\"start\":14951},{\"end\":16077,\"start\":15811},{\"end\":16407,\"start\":16124},{\"end\":16818,\"start\":16409},{\"end\":17210,\"start\":16820},{\"end\":17607,\"start\":17212},{\"end\":18018,\"start\":17609},{\"end\":18135,\"start\":18035},{\"end\":18192,\"start\":18137},{\"end\":18542,\"start\":18194},{\"end\":18800,\"start\":18544},{\"end\":19049,\"start\":18802},{\"end\":20997,\"start\":19051},{\"end\":21781,\"start\":21024},{\"end\":21961,\"start\":21801},{\"end\":22367,\"start\":21963},{\"end\":22664,\"start\":22369},{\"end\":22784,\"start\":22666},{\"end\":23037,\"start\":22786},{\"end\":23262,\"start\":23039},{\"end\":24405,\"start\":23264},{\"end\":24815,\"start\":24407},{\"end\":24987,\"start\":24817},{\"end\":25468,\"start\":25050},{\"end\":25613,\"start\":25525},{\"end\":25841,\"start\":25615},{\"end\":25927,\"start\":25872},{\"end\":27597,\"start\":25965},{\"end\":27908,\"start\":27599},{\"end\":28121,\"start\":27910},{\"end\":29207,\"start\":28177},{\"end\":30422,\"start\":29209},{\"end\":30703,\"start\":30475},{\"end\":31242,\"start\":30705},{\"end\":32060,\"start\":31244},{\"end\":32901,\"start\":32072},{\"end\":33029,\"start\":32912},{\"end\":33259,\"start\":33031},{\"end\":33726,\"start\":33261},{\"end\":33973,\"start\":33745},{\"end\":34133,\"start\":33975},{\"end\":34589,\"start\":34135},{\"end\":35826,\"start\":34612},{\"end\":36149,\"start\":35828},{\"end\":36383,\"start\":36195},{\"end\":36940,\"start\":36385},{\"end\":37201,\"start\":36942},{\"end\":37498,\"start\":37203},{\"end\":38957,\"start\":37500},{\"end\":40519,\"start\":39019},{\"end\":41102,\"start\":40521},{\"end\":41384,\"start\":41104},{\"end\":42437,\"start\":41386},{\"end\":43136,\"start\":42439},{\"end\":43336,\"start\":43138},{\"end\":43526,\"start\":43338},{\"end\":43703,\"start\":43528},{\"end\":44200,\"start\":43760},{\"end\":44868,\"start\":44202},{\"end\":45025,\"start\":44870},{\"end\":45241,\"start\":45027},{\"end\":45722,\"start\":45243},{\"end\":46245,\"start\":45724},{\"end\":46891,\"start\":46247},{\"end\":47160,\"start\":46893},{\"end\":47302,\"start\":47188},{\"end\":47753,\"start\":47304},{\"end\":48060,\"start\":47755},{\"end\":48544,\"start\":48085},{\"end\":49935,\"start\":48560},{\"end\":50377,\"start\":49937},{\"end\":50990,\"start\":50392},{\"end\":51965,\"start\":50992},{\"end\":52315,\"start\":51986},{\"end\":52827,\"start\":52317},{\"end\":53269,\"start\":52829},{\"end\":53500,\"start\":53301},{\"end\":53745,\"start\":53502},{\"end\":54138,\"start\":53747},{\"end\":54547,\"start\":54140},{\"end\":55022,\"start\":54568},{\"end\":55298,\"start\":55024},{\"end\":55601,\"start\":55314},{\"end\":56119,\"start\":55603},{\"end\":56247,\"start\":56121},{\"end\":57844,\"start\":56249},{\"end\":58218,\"start\":57846},{\"end\":58594,\"start\":58220},{\"end\":58833,\"start\":58596},{\"end\":59398,\"start\":58835},{\"end\":59543,\"start\":59421},{\"end\":60099,\"start\":59545},{\"end\":60305,\"start\":60101},{\"end\":60778,\"start\":60307},{\"end\":61152,\"start\":60780},{\"end\":61718,\"start\":61154},{\"end\":62601,\"start\":61720},{\"end\":63030,\"start\":62616},{\"end\":63843,\"start\":63032},{\"end\":63917,\"start\":63866},{\"end\":64659,\"start\":63919},{\"end\":64820,\"start\":64698},{\"end\":65157,\"start\":64822},{\"end\":65790,\"start\":65159},{\"end\":66221,\"start\":65792},{\"end\":66441,\"start\":66256},{\"end\":67092,\"start\":66443},{\"end\":67984,\"start\":67094},{\"end\":69133,\"start\":67995},{\"end\":70241,\"start\":69148},{\"end\":71392,\"start\":70256},{\"end\":71808,\"start\":71394},{\"end\":71929,\"start\":71810}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9641,\"start\":9499},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9692,\"start\":9641},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14574,\"start\":14550},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15810,\"start\":15782},{\"attributes\":{\"id\":\"formula_4\"},\"end\":25049,\"start\":24988},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25871,\"start\":25842},{\"attributes\":{\"id\":\"formula_6\"},\"end\":28176,\"start\":28122}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31292,\"start\":31285},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31923,\"start\":31916},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34709,\"start\":34702},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":41164,\"start\":41157},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":41383,\"start\":41376},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":46496,\"start\":46489},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":61179,\"start\":61172},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":63644,\"start\":63637}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2343,\"start\":2331},{\"attributes\":{\"n\":\"2\"},\"end\":8720,\"start\":8710},{\"attributes\":{\"n\":\"2.1\"},\"end\":8750,\"start\":8723},{\"attributes\":{\"n\":\"2.2\"},\"end\":10030,\"start\":10008},{\"attributes\":{\"n\":\"2.3\"},\"end\":11500,\"start\":11485},{\"attributes\":{\"n\":\"2.3.1\"},\"end\":11866,\"start\":11846},{\"attributes\":{\"n\":\"2.3.2\"},\"end\":12868,\"start\":12847},{\"attributes\":{\"n\":\"3\"},\"end\":16097,\"start\":16080},{\"attributes\":{\"n\":\"3.1\"},\"end\":16122,\"start\":16100},{\"attributes\":{\"n\":\"3.2\"},\"end\":18033,\"start\":18021},{\"attributes\":{\"n\":\"4\"},\"end\":21011,\"start\":21000},{\"attributes\":{\"n\":\"4.1\"},\"end\":21022,\"start\":21014},{\"attributes\":{\"n\":\"4.2\"},\"end\":21799,\"start\":21784},{\"end\":25523,\"start\":25471},{\"attributes\":{\"n\":\"4.3\"},\"end\":25963,\"start\":25930},{\"attributes\":{\"n\":\"5\"},\"end\":30462,\"start\":30425},{\"attributes\":{\"n\":\"5.1\"},\"end\":30473,\"start\":30465},{\"attributes\":{\"n\":\"5.2\"},\"end\":32070,\"start\":32063},{\"attributes\":{\"n\":\"5.3\"},\"end\":32910,\"start\":32904},{\"attributes\":{\"n\":\"5.4\"},\"end\":33743,\"start\":33729},{\"attributes\":{\"n\":\"6\"},\"end\":34610,\"start\":34592},{\"attributes\":{\"n\":\"6.1\"},\"end\":36193,\"start\":36152},{\"attributes\":{\"n\":\"6.2\"},\"end\":39017,\"start\":38960},{\"attributes\":{\"n\":\"6.3\"},\"end\":43758,\"start\":43706},{\"attributes\":{\"n\":\"6.4\"},\"end\":47186,\"start\":47163},{\"attributes\":{\"n\":\"7\"},\"end\":48083,\"start\":48063},{\"attributes\":{\"n\":\"7.1\"},\"end\":48558,\"start\":48547},{\"attributes\":{\"n\":\"7.2\"},\"end\":50390,\"start\":50380},{\"attributes\":{\"n\":\"7.3\"},\"end\":51984,\"start\":51968},{\"attributes\":{\"n\":\"7.4\"},\"end\":53299,\"start\":53272},{\"attributes\":{\"n\":\"8\"},\"end\":54566,\"start\":54550},{\"attributes\":{\"n\":\"8.1\"},\"end\":55312,\"start\":55301},{\"attributes\":{\"n\":\"8.1.1\"},\"end\":59419,\"start\":59401},{\"attributes\":{\"n\":\"8.2\"},\"end\":62614,\"start\":62604},{\"attributes\":{\"n\":\"8.3\"},\"end\":63864,\"start\":63846},{\"attributes\":{\"n\":\"9.1\"},\"end\":64696,\"start\":64662},{\"attributes\":{\"n\":\"9.2\"},\"end\":66254,\"start\":66224},{\"attributes\":{\"n\":\"9.3\"},\"end\":67993,\"start\":67987},{\"attributes\":{\"n\":\"10\"},\"end\":69146,\"start\":69136},{\"attributes\":{\"n\":\"11\"},\"end\":70254,\"start\":70244},{\"end\":71939,\"start\":71931},{\"end\":72040,\"start\":72037},{\"end\":72179,\"start\":72171},{\"end\":72265,\"start\":72252},{\"end\":72620,\"start\":72612},{\"end\":72988,\"start\":72987},{\"end\":73201,\"start\":73193},{\"end\":73624,\"start\":73616},{\"end\":73803,\"start\":73795},{\"end\":73959,\"start\":73950},{\"end\":74134,\"start\":74125},{\"end\":74475,\"start\":74468},{\"end\":76009,\"start\":75995},{\"end\":76173,\"start\":76166},{\"end\":76474,\"start\":76467}]", "table": "[{\"end\":74814,\"start\":74477},{\"end\":75993,\"start\":74869},{\"end\":76164,\"start\":76158},{\"end\":76465,\"start\":76226},{\"end\":76626,\"start\":76504}]", "figure_caption": "[{\"end\":72035,\"start\":71941},{\"end\":72169,\"start\":72042},{\"end\":72250,\"start\":72181},{\"end\":72610,\"start\":72267},{\"end\":72985,\"start\":72622},{\"end\":73191,\"start\":72989},{\"end\":73370,\"start\":73203},{\"end\":73614,\"start\":73373},{\"end\":73793,\"start\":73626},{\"end\":73948,\"start\":73805},{\"end\":74123,\"start\":73962},{\"end\":74248,\"start\":74137},{\"end\":74466,\"start\":74251},{\"end\":74869,\"start\":74817},{\"end\":76158,\"start\":76011},{\"end\":76226,\"start\":76175},{\"end\":76504,\"start\":76476},{\"end\":77256,\"start\":76629}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3024,\"start\":3017},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3382,\"start\":3375},{\"end\":24914,\"start\":24908},{\"end\":27207,\"start\":27201},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28651,\"start\":28645},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36965,\"start\":36959},{\"end\":44291,\"start\":44285},{\"end\":47530,\"start\":47524},{\"end\":49477,\"start\":49471},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":50540,\"start\":50534},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":50593,\"start\":50587},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":52296,\"start\":52290},{\"end\":52612,\"start\":52606},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53481,\"start\":53474},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":57548,\"start\":57541},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":58290,\"start\":58283},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":62307,\"start\":62300}]", "bib_author_first_name": "[{\"end\":78914,\"start\":78913},{\"end\":78923,\"start\":78922},{\"end\":78939,\"start\":78938},{\"end\":78949,\"start\":78948},{\"end\":78961,\"start\":78960},{\"end\":78972,\"start\":78971},{\"end\":79265,\"start\":79264},{\"end\":79456,\"start\":79455},{\"end\":79800,\"start\":79799},{\"end\":79802,\"start\":79801},{\"end\":79813,\"start\":79812},{\"end\":79821,\"start\":79820},{\"end\":79829,\"start\":79828},{\"end\":79842,\"start\":79841},{\"end\":79844,\"start\":79843},{\"end\":79859,\"start\":79858},{\"end\":80165,\"start\":80164},{\"end\":80178,\"start\":80177},{\"end\":80189,\"start\":80188},{\"end\":80200,\"start\":80199},{\"end\":80211,\"start\":80210},{\"end\":80215,\"start\":80214},{\"end\":80600,\"start\":80599},{\"end\":80608,\"start\":80607},{\"end\":80617,\"start\":80616},{\"end\":80625,\"start\":80624},{\"end\":80632,\"start\":80631},{\"end\":80948,\"start\":80947},{\"end\":80957,\"start\":80956},{\"end\":80964,\"start\":80963},{\"end\":80972,\"start\":80971},{\"end\":81215,\"start\":81214},{\"end\":81229,\"start\":81228},{\"end\":81240,\"start\":81239},{\"end\":81574,\"start\":81573},{\"end\":81835,\"start\":81834},{\"end\":82069,\"start\":82068},{\"end\":82346,\"start\":82345},{\"end\":82633,\"start\":82632},{\"end\":82635,\"start\":82634},{\"end\":82644,\"start\":82643},{\"end\":82653,\"start\":82652},{\"end\":82901,\"start\":82900},{\"end\":83166,\"start\":83165},{\"end\":83178,\"start\":83177},{\"end\":83186,\"start\":83185},{\"end\":83477,\"start\":83476},{\"end\":83479,\"start\":83478},{\"end\":83697,\"start\":83696},{\"end\":83704,\"start\":83703},{\"end\":83710,\"start\":83709},{\"end\":83716,\"start\":83715},{\"end\":83723,\"start\":83722},{\"end\":83995,\"start\":83994},{\"end\":84158,\"start\":84157},{\"end\":84164,\"start\":84163},{\"end\":84171,\"start\":84170},{\"end\":84181,\"start\":84180},{\"end\":84183,\"start\":84182},{\"end\":84191,\"start\":84190},{\"end\":84474,\"start\":84473},{\"end\":84476,\"start\":84475},{\"end\":84484,\"start\":84483},{\"end\":84702,\"start\":84701},{\"end\":84714,\"start\":84713},{\"end\":84716,\"start\":84715},{\"end\":84727,\"start\":84726},{\"end\":84729,\"start\":84728},{\"end\":84735,\"start\":84734},{\"end\":84749,\"start\":84748},{\"end\":84751,\"start\":84750},{\"end\":84761,\"start\":84760},{\"end\":85047,\"start\":85046},{\"end\":85054,\"start\":85053},{\"end\":85064,\"start\":85063},{\"end\":85066,\"start\":85065},{\"end\":85085,\"start\":85084},{\"end\":85097,\"start\":85096},{\"end\":85437,\"start\":85436},{\"end\":85641,\"start\":85640},{\"end\":85643,\"start\":85642},{\"end\":85918,\"start\":85917},{\"end\":86104,\"start\":86103},{\"end\":86106,\"start\":86105},{\"end\":86121,\"start\":86117},{\"end\":86468,\"start\":86467},{\"end\":86475,\"start\":86474},{\"end\":86481,\"start\":86480},{\"end\":86489,\"start\":86488},{\"end\":86491,\"start\":86490},{\"end\":86702,\"start\":86701},{\"end\":86908,\"start\":86907},{\"end\":87308,\"start\":87307},{\"end\":87318,\"start\":87317},{\"end\":87333,\"start\":87332},{\"end\":87342,\"start\":87341},{\"end\":87716,\"start\":87715},{\"end\":87727,\"start\":87726},{\"end\":87736,\"start\":87735},{\"end\":87746,\"start\":87745},{\"end\":87757,\"start\":87756},{\"end\":87761,\"start\":87758},{\"end\":88103,\"start\":88102},{\"end\":88112,\"start\":88111},{\"end\":88120,\"start\":88119},{\"end\":88123,\"start\":88121},{\"end\":88137,\"start\":88136},{\"end\":88526,\"start\":88525},{\"end\":88534,\"start\":88533},{\"end\":88544,\"start\":88543},{\"end\":88930,\"start\":88929},{\"end\":88936,\"start\":88935},{\"end\":88942,\"start\":88941},{\"end\":88949,\"start\":88948},{\"end\":88957,\"start\":88956},{\"end\":89203,\"start\":89202},{\"end\":89524,\"start\":89523},{\"end\":89535,\"start\":89534},{\"end\":89546,\"start\":89545},{\"end\":89935,\"start\":89934},{\"end\":89942,\"start\":89941},{\"end\":89950,\"start\":89949},{\"end\":89956,\"start\":89955},{\"end\":89962,\"start\":89961},{\"end\":89970,\"start\":89969},{\"end\":90344,\"start\":90343},{\"end\":90351,\"start\":90350},{\"end\":90359,\"start\":90358},{\"end\":90365,\"start\":90364},{\"end\":90373,\"start\":90372},{\"end\":90381,\"start\":90380},{\"end\":90796,\"start\":90795},{\"end\":91058,\"start\":91057},{\"end\":91062,\"start\":91059},{\"end\":91075,\"start\":91074},{\"end\":91083,\"start\":91082},{\"end\":91089,\"start\":91088},{\"end\":91108,\"start\":91107},{\"end\":91449,\"start\":91448},{\"end\":91457,\"start\":91456},{\"end\":91465,\"start\":91464},{\"end\":91475,\"start\":91474},{\"end\":91484,\"start\":91483},{\"end\":91486,\"start\":91485},{\"end\":91781,\"start\":91780},{\"end\":91944,\"start\":91943},{\"end\":91954,\"start\":91953},{\"end\":91963,\"start\":91962},{\"end\":91977,\"start\":91976},{\"end\":92170,\"start\":92169},{\"end\":92180,\"start\":92179},{\"end\":92461,\"start\":92460},{\"end\":92667,\"start\":92666},{\"end\":92676,\"start\":92675},{\"end\":92941,\"start\":92940},{\"end\":93576,\"start\":93575},{\"end\":93839,\"start\":93838},{\"end\":93845,\"start\":93844},{\"end\":93854,\"start\":93853},{\"end\":93856,\"start\":93855},{\"end\":93870,\"start\":93869},{\"end\":93879,\"start\":93878},{\"end\":93885,\"start\":93884},{\"end\":93887,\"start\":93886},{\"end\":94182,\"start\":94181},{\"end\":94189,\"start\":94188},{\"end\":94195,\"start\":94194},{\"end\":94204,\"start\":94203},{\"end\":94493,\"start\":94492},{\"end\":94499,\"start\":94498},{\"end\":94505,\"start\":94504},{\"end\":94512,\"start\":94511},{\"end\":94520,\"start\":94519},{\"end\":94772,\"start\":94771},{\"end\":94778,\"start\":94777},{\"end\":94780,\"start\":94779},{\"end\":94794,\"start\":94793},{\"end\":94800,\"start\":94799},{\"end\":94810,\"start\":94809},{\"end\":94818,\"start\":94817},{\"end\":95068,\"start\":95067},{\"end\":95074,\"start\":95073},{\"end\":95080,\"start\":95079},{\"end\":95092,\"start\":95091},{\"end\":95337,\"start\":95336},{\"end\":95346,\"start\":95345},{\"end\":95348,\"start\":95347},{\"end\":95357,\"start\":95356},{\"end\":95359,\"start\":95358},{\"end\":95367,\"start\":95366},{\"end\":95738,\"start\":95737},{\"end\":95746,\"start\":95745},{\"end\":95753,\"start\":95752},{\"end\":95761,\"start\":95760},{\"end\":96030,\"start\":96029},{\"end\":96038,\"start\":96037},{\"end\":96047,\"start\":96046},{\"end\":96374,\"start\":96373},{\"end\":96383,\"start\":96382},{\"end\":96391,\"start\":96390},{\"end\":96397,\"start\":96396},{\"end\":96404,\"start\":96403},{\"end\":96406,\"start\":96405},{\"end\":96754,\"start\":96753},{\"end\":96763,\"start\":96762},{\"end\":96772,\"start\":96771},{\"end\":97040,\"start\":97039},{\"end\":97049,\"start\":97048},{\"end\":97060,\"start\":97059},{\"end\":97069,\"start\":97068},{\"end\":97082,\"start\":97081},{\"end\":97093,\"start\":97092},{\"end\":97418,\"start\":97417},{\"end\":97427,\"start\":97426},{\"end\":97659,\"start\":97658},{\"end\":97667,\"start\":97666},{\"end\":97669,\"start\":97668},{\"end\":97679,\"start\":97678},{\"end\":97904,\"start\":97903},{\"end\":98092,\"start\":98091},{\"end\":98099,\"start\":98098},{\"end\":98106,\"start\":98105}]", "bib_author_last_name": "[{\"end\":78920,\"start\":78915},{\"end\":78936,\"start\":78924},{\"end\":78946,\"start\":78940},{\"end\":78958,\"start\":78950},{\"end\":78969,\"start\":78962},{\"end\":78987,\"start\":78973},{\"end\":79200,\"start\":79188},{\"end\":79272,\"start\":79266},{\"end\":79465,\"start\":79457},{\"end\":79810,\"start\":79803},{\"end\":79818,\"start\":79814},{\"end\":79826,\"start\":79822},{\"end\":79839,\"start\":79830},{\"end\":79856,\"start\":79845},{\"end\":79863,\"start\":79860},{\"end\":80175,\"start\":80166},{\"end\":80186,\"start\":80179},{\"end\":80197,\"start\":80190},{\"end\":80208,\"start\":80201},{\"end\":80222,\"start\":80216},{\"end\":80605,\"start\":80601},{\"end\":80614,\"start\":80609},{\"end\":80622,\"start\":80618},{\"end\":80629,\"start\":80626},{\"end\":80637,\"start\":80633},{\"end\":80954,\"start\":80949},{\"end\":80961,\"start\":80958},{\"end\":80969,\"start\":80965},{\"end\":80977,\"start\":80973},{\"end\":81226,\"start\":81216},{\"end\":81237,\"start\":81230},{\"end\":81254,\"start\":81241},{\"end\":81580,\"start\":81575},{\"end\":81839,\"start\":81836},{\"end\":82080,\"start\":82070},{\"end\":82353,\"start\":82347},{\"end\":82641,\"start\":82636},{\"end\":82650,\"start\":82645},{\"end\":82658,\"start\":82654},{\"end\":82905,\"start\":82902},{\"end\":83175,\"start\":83167},{\"end\":83183,\"start\":83179},{\"end\":83195,\"start\":83187},{\"end\":83487,\"start\":83480},{\"end\":83701,\"start\":83698},{\"end\":83707,\"start\":83705},{\"end\":83713,\"start\":83711},{\"end\":83720,\"start\":83717},{\"end\":83728,\"start\":83724},{\"end\":84000,\"start\":83996},{\"end\":84161,\"start\":84159},{\"end\":84168,\"start\":84165},{\"end\":84178,\"start\":84172},{\"end\":84188,\"start\":84184},{\"end\":84197,\"start\":84192},{\"end\":84481,\"start\":84477},{\"end\":84492,\"start\":84485},{\"end\":84711,\"start\":84703},{\"end\":84724,\"start\":84717},{\"end\":84732,\"start\":84730},{\"end\":84746,\"start\":84736},{\"end\":84758,\"start\":84752},{\"end\":84767,\"start\":84762},{\"end\":85051,\"start\":85048},{\"end\":85061,\"start\":85055},{\"end\":85082,\"start\":85067},{\"end\":85094,\"start\":85086},{\"end\":85104,\"start\":85098},{\"end\":85440,\"start\":85438},{\"end\":85648,\"start\":85644},{\"end\":85922,\"start\":85919},{\"end\":86115,\"start\":86107},{\"end\":86125,\"start\":86122},{\"end\":86472,\"start\":86469},{\"end\":86478,\"start\":86476},{\"end\":86486,\"start\":86482},{\"end\":86495,\"start\":86492},{\"end\":86706,\"start\":86703},{\"end\":86915,\"start\":86909},{\"end\":87315,\"start\":87309},{\"end\":87330,\"start\":87319},{\"end\":87339,\"start\":87334},{\"end\":87351,\"start\":87343},{\"end\":87724,\"start\":87717},{\"end\":87733,\"start\":87728},{\"end\":87743,\"start\":87737},{\"end\":87754,\"start\":87747},{\"end\":87767,\"start\":87762},{\"end\":88109,\"start\":88104},{\"end\":88117,\"start\":88113},{\"end\":88134,\"start\":88124},{\"end\":88147,\"start\":88138},{\"end\":88531,\"start\":88527},{\"end\":88541,\"start\":88535},{\"end\":88555,\"start\":88545},{\"end\":88933,\"start\":88931},{\"end\":88939,\"start\":88937},{\"end\":88946,\"start\":88943},{\"end\":88954,\"start\":88950},{\"end\":88962,\"start\":88958},{\"end\":89212,\"start\":89204},{\"end\":89532,\"start\":89525},{\"end\":89543,\"start\":89536},{\"end\":89553,\"start\":89547},{\"end\":89939,\"start\":89936},{\"end\":89947,\"start\":89943},{\"end\":89953,\"start\":89951},{\"end\":89959,\"start\":89957},{\"end\":89967,\"start\":89963},{\"end\":89975,\"start\":89971},{\"end\":90348,\"start\":90345},{\"end\":90356,\"start\":90352},{\"end\":90362,\"start\":90360},{\"end\":90370,\"start\":90366},{\"end\":90378,\"start\":90374},{\"end\":90386,\"start\":90382},{\"end\":90807,\"start\":90797},{\"end\":91072,\"start\":91063},{\"end\":91080,\"start\":91076},{\"end\":91086,\"start\":91084},{\"end\":91105,\"start\":91090},{\"end\":91112,\"start\":91109},{\"end\":91454,\"start\":91450},{\"end\":91462,\"start\":91458},{\"end\":91472,\"start\":91466},{\"end\":91481,\"start\":91476},{\"end\":91491,\"start\":91487},{\"end\":91788,\"start\":91782},{\"end\":91951,\"start\":91945},{\"end\":91960,\"start\":91955},{\"end\":91974,\"start\":91964},{\"end\":91989,\"start\":91978},{\"end\":92177,\"start\":92171},{\"end\":92190,\"start\":92181},{\"end\":92470,\"start\":92462},{\"end\":92673,\"start\":92668},{\"end\":92680,\"start\":92677},{\"end\":92689,\"start\":92682},{\"end\":92946,\"start\":92942},{\"end\":93121,\"start\":93115},{\"end\":93342,\"start\":93336},{\"end\":93580,\"start\":93577},{\"end\":93842,\"start\":93840},{\"end\":93851,\"start\":93846},{\"end\":93865,\"start\":93857},{\"end\":93876,\"start\":93871},{\"end\":93882,\"start\":93880},{\"end\":93898,\"start\":93888},{\"end\":94186,\"start\":94183},{\"end\":94192,\"start\":94190},{\"end\":94201,\"start\":94196},{\"end\":94209,\"start\":94205},{\"end\":94496,\"start\":94494},{\"end\":94502,\"start\":94500},{\"end\":94509,\"start\":94506},{\"end\":94517,\"start\":94513},{\"end\":94524,\"start\":94521},{\"end\":94775,\"start\":94773},{\"end\":94791,\"start\":94781},{\"end\":94797,\"start\":94795},{\"end\":94807,\"start\":94801},{\"end\":94815,\"start\":94811},{\"end\":94823,\"start\":94819},{\"end\":95071,\"start\":95069},{\"end\":95077,\"start\":95075},{\"end\":95089,\"start\":95081},{\"end\":95100,\"start\":95093},{\"end\":95343,\"start\":95338},{\"end\":95354,\"start\":95349},{\"end\":95364,\"start\":95360},{\"end\":95375,\"start\":95368},{\"end\":95743,\"start\":95739},{\"end\":95750,\"start\":95747},{\"end\":95758,\"start\":95754},{\"end\":95766,\"start\":95762},{\"end\":96035,\"start\":96031},{\"end\":96044,\"start\":96039},{\"end\":96060,\"start\":96048},{\"end\":96380,\"start\":96375},{\"end\":96388,\"start\":96384},{\"end\":96394,\"start\":96392},{\"end\":96401,\"start\":96398},{\"end\":96413,\"start\":96407},{\"end\":96760,\"start\":96755},{\"end\":96769,\"start\":96764},{\"end\":96782,\"start\":96773},{\"end\":97046,\"start\":97041},{\"end\":97057,\"start\":97050},{\"end\":97066,\"start\":97061},{\"end\":97079,\"start\":97070},{\"end\":97090,\"start\":97083},{\"end\":97100,\"start\":97094},{\"end\":97424,\"start\":97419},{\"end\":97431,\"start\":97428},{\"end\":97664,\"start\":97660},{\"end\":97676,\"start\":97670},{\"end\":97685,\"start\":97680},{\"end\":97909,\"start\":97905},{\"end\":98096,\"start\":98093},{\"end\":98103,\"start\":98100},{\"end\":98110,\"start\":98107}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2109.00984\",\"id\":\"b0\"},\"end\":79184,\"start\":78849},{\"attributes\":{\"id\":\"b1\"},\"end\":79260,\"start\":79186},{\"attributes\":{\"id\":\"b2\"},\"end\":79383,\"start\":79262},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3833774},\"end\":79715,\"start\":79385},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3679574},\"end\":80099,\"start\":79717},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":233476627},\"end\":80542,\"start\":80101},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":225067719},\"end\":80899,\"start\":80544},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":226981644},\"end\":81132,\"start\":80901},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3016223},\"end\":81528,\"start\":81134},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2887752},\"end\":81783,\"start\":81530},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":67769538},\"end\":82004,\"start\":81785},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":233176356},\"end\":82292,\"start\":82006},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":947660},\"end\":82559,\"start\":82294},{\"attributes\":{\"doi\":\"arXiv:1712.07557\",\"id\":\"b13\"},\"end\":82810,\"start\":82561},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":231851007},\"end\":83112,\"start\":82812},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4755450},\"end\":83430,\"start\":83114},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":61141773},\"end\":83628,\"start\":83432},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":198169168},\"end\":83941,\"start\":83630},{\"attributes\":{\"doi\":\"arXiv:1811.03604\",\"id\":\"b18\"},\"end\":84112,\"start\":83943},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":218502486},\"end\":84405,\"start\":84114},{\"attributes\":{\"doi\":\"arXiv:1609.02907\",\"id\":\"b20\"},\"end\":84628,\"start\":84407},{\"attributes\":{\"doi\":\"arXiv:1610.05492\",\"id\":\"b21\"},\"end\":84964,\"start\":84630},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18459763},\"end\":85376,\"start\":84966},{\"attributes\":{\"doi\":\"arXiv:2102.08504\",\"id\":\"b23\"},\"end\":85559,\"start\":85378},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":118673880},\"end\":85850,\"start\":85561},{\"attributes\":{\"doi\":\"arXiv:1912.11187\",\"id\":\"b25\"},\"end\":86047,\"start\":85852},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":21889700},\"end\":86387,\"start\":86049},{\"attributes\":{\"id\":\"b27\"},\"end\":86631,\"start\":86389},{\"attributes\":{\"doi\":\"arXiv:2012.06337\",\"id\":\"b28\"},\"end\":86836,\"start\":86633},{\"attributes\":{\"doi\":\"ser. COLING-02. USA\",\"id\":\"b29\"},\"end\":87137,\"start\":86838},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":53715250},\"end\":87638,\"start\":87139},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14955348},\"end\":88035,\"start\":87640},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53099247},\"end\":88383,\"start\":88037},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":133091488},\"end\":88854,\"start\":88385},{\"attributes\":{\"doi\":\"arXiv:2106.11593\",\"id\":\"b34\"},\"end\":89128,\"start\":88856},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":9483611},\"end\":89468,\"start\":89130},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3051291},\"end\":89845,\"start\":89470},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3952914},\"end\":90284,\"start\":89847},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":49862561},\"end\":90700,\"start\":90286},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":12781225},\"end\":90988,\"start\":90702},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":211196911},\"end\":91388,\"start\":90990},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":1905609},\"end\":91755,\"start\":91390},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":16321225},\"end\":91896,\"start\":91757},{\"attributes\":{\"doi\":\"arXiv:1811.05868\",\"id\":\"b43\"},\"end\":92133,\"start\":91898},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":20714},\"end\":92379,\"start\":92135},{\"attributes\":{\"doi\":\"arXiv:1909.12946\",\"id\":\"b45\"},\"end\":92614,\"start\":92381},{\"attributes\":{\"id\":\"b46\"},\"end\":92888,\"start\":92616},{\"attributes\":{\"doi\":\"arXiv:1905.04519\",\"id\":\"b47\"},\"end\":93048,\"start\":92890},{\"attributes\":{\"id\":\"b48\"},\"end\":93253,\"start\":93050},{\"attributes\":{\"id\":\"b49\"},\"end\":93490,\"start\":93255},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":207847853},\"end\":93794,\"start\":93492},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":67752026},\"end\":94127,\"start\":93796},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":235652384},\"end\":94437,\"start\":94129},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":199579523},\"end\":94722,\"start\":94439},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":208006713},\"end\":95024,\"start\":94724},{\"attributes\":{\"doi\":\"arXiv:1810.00826\",\"id\":\"b55\"},\"end\":95226,\"start\":95026},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":230795244},\"end\":95681,\"start\":95228},{\"attributes\":{\"id\":\"b57\"},\"end\":95968,\"start\":95683},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":7008752},\"end\":96267,\"start\":95970},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":4950268},\"end\":96688,\"start\":96269},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":235485024},\"end\":96998,\"start\":96690},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":207991409},\"end\":97302,\"start\":97000},{\"attributes\":{\"doi\":\"arXiv:2007.06849\",\"id\":\"b62\"},\"end\":97612,\"start\":97304},{\"attributes\":{\"doi\":\"arXiv:2001.02610\",\"id\":\"b63\"},\"end\":97815,\"start\":97614},{\"attributes\":{\"doi\":\"arXiv:2005.11903\",\"id\":\"b64\"},\"end\":98060,\"start\":97817},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":195316471},\"end\":98316,\"start\":98062}]", "bib_title": "[{\"end\":79453,\"start\":79385},{\"end\":79797,\"start\":79717},{\"end\":80162,\"start\":80101},{\"end\":80597,\"start\":80544},{\"end\":80945,\"start\":80901},{\"end\":81212,\"start\":81134},{\"end\":81571,\"start\":81530},{\"end\":81832,\"start\":81785},{\"end\":82066,\"start\":82006},{\"end\":82343,\"start\":82294},{\"end\":82898,\"start\":82812},{\"end\":83163,\"start\":83114},{\"end\":83474,\"start\":83432},{\"end\":83694,\"start\":83630},{\"end\":84155,\"start\":84114},{\"end\":85044,\"start\":84966},{\"end\":85638,\"start\":85561},{\"end\":86101,\"start\":86049},{\"end\":87305,\"start\":87139},{\"end\":87713,\"start\":87640},{\"end\":88100,\"start\":88037},{\"end\":88523,\"start\":88385},{\"end\":89200,\"start\":89130},{\"end\":89521,\"start\":89470},{\"end\":89932,\"start\":89847},{\"end\":90341,\"start\":90286},{\"end\":90793,\"start\":90702},{\"end\":91055,\"start\":90990},{\"end\":91446,\"start\":91390},{\"end\":91778,\"start\":91757},{\"end\":92167,\"start\":92135},{\"end\":93573,\"start\":93492},{\"end\":93836,\"start\":93796},{\"end\":94179,\"start\":94129},{\"end\":94490,\"start\":94439},{\"end\":94769,\"start\":94724},{\"end\":95334,\"start\":95228},{\"end\":95735,\"start\":95683},{\"end\":96027,\"start\":95970},{\"end\":96371,\"start\":96269},{\"end\":96751,\"start\":96690},{\"end\":97037,\"start\":97000},{\"end\":98089,\"start\":98062}]", "bib_author": "[{\"end\":78922,\"start\":78913},{\"end\":78938,\"start\":78922},{\"end\":78948,\"start\":78938},{\"end\":78960,\"start\":78948},{\"end\":78971,\"start\":78960},{\"end\":78989,\"start\":78971},{\"end\":79202,\"start\":79188},{\"end\":79274,\"start\":79264},{\"end\":79467,\"start\":79455},{\"end\":79812,\"start\":79799},{\"end\":79820,\"start\":79812},{\"end\":79828,\"start\":79820},{\"end\":79841,\"start\":79828},{\"end\":79858,\"start\":79841},{\"end\":79865,\"start\":79858},{\"end\":80177,\"start\":80164},{\"end\":80188,\"start\":80177},{\"end\":80199,\"start\":80188},{\"end\":80210,\"start\":80199},{\"end\":80214,\"start\":80210},{\"end\":80224,\"start\":80214},{\"end\":80607,\"start\":80599},{\"end\":80616,\"start\":80607},{\"end\":80624,\"start\":80616},{\"end\":80631,\"start\":80624},{\"end\":80639,\"start\":80631},{\"end\":80956,\"start\":80947},{\"end\":80963,\"start\":80956},{\"end\":80971,\"start\":80963},{\"end\":80979,\"start\":80971},{\"end\":81228,\"start\":81214},{\"end\":81239,\"start\":81228},{\"end\":81256,\"start\":81239},{\"end\":81582,\"start\":81573},{\"end\":81841,\"start\":81834},{\"end\":82082,\"start\":82068},{\"end\":82355,\"start\":82345},{\"end\":82643,\"start\":82632},{\"end\":82652,\"start\":82643},{\"end\":82660,\"start\":82652},{\"end\":82907,\"start\":82900},{\"end\":83177,\"start\":83165},{\"end\":83185,\"start\":83177},{\"end\":83197,\"start\":83185},{\"end\":83489,\"start\":83476},{\"end\":83703,\"start\":83696},{\"end\":83709,\"start\":83703},{\"end\":83715,\"start\":83709},{\"end\":83722,\"start\":83715},{\"end\":83730,\"start\":83722},{\"end\":84002,\"start\":83994},{\"end\":84163,\"start\":84157},{\"end\":84170,\"start\":84163},{\"end\":84180,\"start\":84170},{\"end\":84190,\"start\":84180},{\"end\":84199,\"start\":84190},{\"end\":84483,\"start\":84473},{\"end\":84494,\"start\":84483},{\"end\":84713,\"start\":84701},{\"end\":84726,\"start\":84713},{\"end\":84734,\"start\":84726},{\"end\":84748,\"start\":84734},{\"end\":84760,\"start\":84748},{\"end\":84769,\"start\":84760},{\"end\":85053,\"start\":85046},{\"end\":85063,\"start\":85053},{\"end\":85084,\"start\":85063},{\"end\":85096,\"start\":85084},{\"end\":85106,\"start\":85096},{\"end\":85442,\"start\":85436},{\"end\":85650,\"start\":85640},{\"end\":85924,\"start\":85917},{\"end\":86117,\"start\":86103},{\"end\":86127,\"start\":86117},{\"end\":86474,\"start\":86467},{\"end\":86480,\"start\":86474},{\"end\":86488,\"start\":86480},{\"end\":86497,\"start\":86488},{\"end\":86708,\"start\":86701},{\"end\":86917,\"start\":86907},{\"end\":87317,\"start\":87307},{\"end\":87332,\"start\":87317},{\"end\":87341,\"start\":87332},{\"end\":87353,\"start\":87341},{\"end\":87726,\"start\":87715},{\"end\":87735,\"start\":87726},{\"end\":87745,\"start\":87735},{\"end\":87756,\"start\":87745},{\"end\":87769,\"start\":87756},{\"end\":88111,\"start\":88102},{\"end\":88119,\"start\":88111},{\"end\":88136,\"start\":88119},{\"end\":88149,\"start\":88136},{\"end\":88533,\"start\":88525},{\"end\":88543,\"start\":88533},{\"end\":88557,\"start\":88543},{\"end\":88935,\"start\":88929},{\"end\":88941,\"start\":88935},{\"end\":88948,\"start\":88941},{\"end\":88956,\"start\":88948},{\"end\":88964,\"start\":88956},{\"end\":89214,\"start\":89202},{\"end\":89534,\"start\":89523},{\"end\":89545,\"start\":89534},{\"end\":89555,\"start\":89545},{\"end\":89941,\"start\":89934},{\"end\":89949,\"start\":89941},{\"end\":89955,\"start\":89949},{\"end\":89961,\"start\":89955},{\"end\":89969,\"start\":89961},{\"end\":89977,\"start\":89969},{\"end\":90350,\"start\":90343},{\"end\":90358,\"start\":90350},{\"end\":90364,\"start\":90358},{\"end\":90372,\"start\":90364},{\"end\":90380,\"start\":90372},{\"end\":90388,\"start\":90380},{\"end\":90809,\"start\":90795},{\"end\":91074,\"start\":91057},{\"end\":91082,\"start\":91074},{\"end\":91088,\"start\":91082},{\"end\":91107,\"start\":91088},{\"end\":91114,\"start\":91107},{\"end\":91456,\"start\":91448},{\"end\":91464,\"start\":91456},{\"end\":91474,\"start\":91464},{\"end\":91483,\"start\":91474},{\"end\":91493,\"start\":91483},{\"end\":91790,\"start\":91780},{\"end\":91953,\"start\":91943},{\"end\":91962,\"start\":91953},{\"end\":91976,\"start\":91962},{\"end\":91991,\"start\":91976},{\"end\":92179,\"start\":92169},{\"end\":92192,\"start\":92179},{\"end\":92472,\"start\":92460},{\"end\":92675,\"start\":92666},{\"end\":92682,\"start\":92675},{\"end\":92691,\"start\":92682},{\"end\":92948,\"start\":92940},{\"end\":93123,\"start\":93115},{\"end\":93344,\"start\":93336},{\"end\":93582,\"start\":93575},{\"end\":93844,\"start\":93838},{\"end\":93853,\"start\":93844},{\"end\":93869,\"start\":93853},{\"end\":93878,\"start\":93869},{\"end\":93884,\"start\":93878},{\"end\":93900,\"start\":93884},{\"end\":94188,\"start\":94181},{\"end\":94194,\"start\":94188},{\"end\":94203,\"start\":94194},{\"end\":94211,\"start\":94203},{\"end\":94498,\"start\":94492},{\"end\":94504,\"start\":94498},{\"end\":94511,\"start\":94504},{\"end\":94519,\"start\":94511},{\"end\":94526,\"start\":94519},{\"end\":94777,\"start\":94771},{\"end\":94793,\"start\":94777},{\"end\":94799,\"start\":94793},{\"end\":94809,\"start\":94799},{\"end\":94817,\"start\":94809},{\"end\":94825,\"start\":94817},{\"end\":95073,\"start\":95067},{\"end\":95079,\"start\":95073},{\"end\":95091,\"start\":95079},{\"end\":95102,\"start\":95091},{\"end\":95345,\"start\":95336},{\"end\":95356,\"start\":95345},{\"end\":95366,\"start\":95356},{\"end\":95377,\"start\":95366},{\"end\":95745,\"start\":95737},{\"end\":95752,\"start\":95745},{\"end\":95760,\"start\":95752},{\"end\":95768,\"start\":95760},{\"end\":96037,\"start\":96029},{\"end\":96046,\"start\":96037},{\"end\":96062,\"start\":96046},{\"end\":96382,\"start\":96373},{\"end\":96390,\"start\":96382},{\"end\":96396,\"start\":96390},{\"end\":96403,\"start\":96396},{\"end\":96415,\"start\":96403},{\"end\":96762,\"start\":96753},{\"end\":96771,\"start\":96762},{\"end\":96784,\"start\":96771},{\"end\":97048,\"start\":97039},{\"end\":97059,\"start\":97048},{\"end\":97068,\"start\":97059},{\"end\":97081,\"start\":97068},{\"end\":97092,\"start\":97081},{\"end\":97102,\"start\":97092},{\"end\":97426,\"start\":97417},{\"end\":97433,\"start\":97426},{\"end\":97666,\"start\":97658},{\"end\":97678,\"start\":97666},{\"end\":97687,\"start\":97678},{\"end\":97911,\"start\":97903},{\"end\":98098,\"start\":98091},{\"end\":98105,\"start\":98098},{\"end\":98112,\"start\":98105}]", "bib_venue": "[{\"end\":79551,\"start\":79513},{\"end\":80320,\"start\":80276},{\"end\":80729,\"start\":80688},{\"end\":81330,\"start\":81299},{\"end\":81664,\"start\":81627},{\"end\":81887,\"start\":81868},{\"end\":82148,\"start\":82119},{\"end\":82431,\"start\":82397},{\"end\":83271,\"start\":83240},{\"end\":83782,\"start\":83760},{\"end\":84253,\"start\":84230},{\"end\":85688,\"start\":85673},{\"end\":86223,\"start\":86181},{\"end\":87833,\"start\":87805},{\"end\":88207,\"start\":88182},{\"end\":88615,\"start\":88590},{\"end\":89308,\"start\":89265},{\"end\":89669,\"start\":89616},{\"end\":90069,\"start\":90027},{\"end\":90502,\"start\":90449},{\"end\":91190,\"start\":91156},{\"end\":91575,\"start\":91538},{\"end\":92240,\"start\":92220},{\"end\":93952,\"start\":93930},{\"end\":94275,\"start\":94249},{\"end\":95455,\"start\":95420},{\"end\":96114,\"start\":96092},{\"end\":96463,\"start\":96443},{\"end\":96838,\"start\":96815},{\"end\":98186,\"start\":98155},{\"end\":78911,\"start\":78849},{\"end\":79293,\"start\":79274},{\"end\":79511,\"start\":79467},{\"end\":79884,\"start\":79865},{\"end\":80274,\"start\":80224},{\"end\":80686,\"start\":80639},{\"end\":80990,\"start\":80979},{\"end\":81297,\"start\":81256},{\"end\":81625,\"start\":81582},{\"end\":81866,\"start\":81841},{\"end\":82117,\"start\":82082},{\"end\":82395,\"start\":82355},{\"end\":82630,\"start\":82561},{\"end\":82939,\"start\":82907},{\"end\":83238,\"start\":83197},{\"end\":83507,\"start\":83489},{\"end\":83758,\"start\":83730},{\"end\":83992,\"start\":83943},{\"end\":84228,\"start\":84199},{\"end\":84471,\"start\":84407},{\"end\":84699,\"start\":84630},{\"end\":85128,\"start\":85106},{\"end\":85434,\"start\":85378},{\"end\":85671,\"start\":85650},{\"end\":85915,\"start\":85852},{\"end\":86179,\"start\":86127},{\"end\":86465,\"start\":86389},{\"end\":86699,\"start\":86633},{\"end\":86905,\"start\":86838},{\"end\":87363,\"start\":87353},{\"end\":87803,\"start\":87769},{\"end\":88180,\"start\":88149},{\"end\":88588,\"start\":88557},{\"end\":88927,\"start\":88856},{\"end\":89263,\"start\":89214},{\"end\":89614,\"start\":89555},{\"end\":90025,\"start\":89977},{\"end\":90447,\"start\":90388},{\"end\":90821,\"start\":90809},{\"end\":91154,\"start\":91114},{\"end\":91536,\"start\":91493},{\"end\":91801,\"start\":91790},{\"end\":91941,\"start\":91898},{\"end\":92218,\"start\":92192},{\"end\":92458,\"start\":92381},{\"end\":92664,\"start\":92616},{\"end\":92938,\"start\":92890},{\"end\":93113,\"start\":93050},{\"end\":93334,\"start\":93255},{\"end\":93619,\"start\":93582},{\"end\":93928,\"start\":93900},{\"end\":94247,\"start\":94211},{\"end\":94558,\"start\":94526},{\"end\":94852,\"start\":94825},{\"end\":95065,\"start\":95026},{\"end\":95418,\"start\":95377},{\"end\":95800,\"start\":95768},{\"end\":96090,\"start\":96062},{\"end\":96441,\"start\":96415},{\"end\":96813,\"start\":96784},{\"end\":97140,\"start\":97102},{\"end\":97415,\"start\":97304},{\"end\":97656,\"start\":97614},{\"end\":97901,\"start\":97817},{\"end\":98153,\"start\":98112}]"}}}, "year": 2023, "month": 12, "day": 17}