{"id": 237263130, "updated": "2023-04-05 22:28:37.306", "metadata": {"title": "TenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers", "authors": "[{\"first\":\"Lianmin\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Ruochen\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Junru\",\"last\":\"Shao\",\"middle\":[]},{\"first\":\"Tianqi\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Joseph\",\"last\":\"Gonzalez\",\"middle\":[]},{\"first\":\"Ion\",\"last\":\"Stoica\",\"middle\":[]},{\"first\":\"Ameer\",\"last\":\"Haj-Ali\",\"middle\":[]}]", "venue": "NeurIPS Datasets and Benchmarks", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Search-based tensor compilers can greatly accelerate the execution of machine 1 learning models by generating high-performance tensor programs, such as matrix 2 multiplications and convolutions. These compilers take a high-level mathematical 3 expression as input and search for the fastest low-level implementations. At the core 4 of the search procedure is a cost model which estimates the performance of different 5 candidates to reduce the frequency of time-consuming on-device measurements. 6 There has been a growing interest in using machine learning techniques to learn a 7 cost model to ease the effort of building an analytical model. However, a standard 8 dataset for pre-training and benchmarking learned cost models is lacking. 9 We introduce TenSet, a large-scale tensor program performance dataset. TenSet 10 contains 52 million program performance records collected from 6 hardware plat- 11 forms. We provide comprehensive studies on how to learn and evaluate the cost 12 models, including data collection, model architectures, loss functions, transfer 13 learning, and evaluation metrics. We also show that a cost model pre-trained on 14 TenSet can accelerate the search time in the state-of-the-art tensor compiler by up 15 to 10 \u00d7 . The dataset is available at https://github.com/tlc-pack/tenset . 16", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ZhengLSC0SH21", "doi": null}}, "content": {"source": {"pdf_hash": "af5958f026701156099245f41694acee0e599d20", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "54941c817e471b092d4849f448a42db80f5253aa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/af5958f026701156099245f41694acee0e599d20.txt", "contents": "\nTenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers\n\n\nLianmin Zheng \nUC Berkeley\n\n\nRuochen Liu \nUC Berkeley\n\n\nJunru Shao \nTianqi Chen \nJoseph E Gonzalez \nUC Berkeley\n\n\nIon Stoica \nUC Berkeley\n\n\nAmeer Haj-Ali \nUC Berkeley\n\n\n\nCarnegie Mellon University\n\n\nTenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers\n\nSearch-based tensor compilers can greatly accelerate the execution of machine 1 learning models by generating high-performance tensor programs, such as matrix 2 multiplications and convolutions. These compilers take a high-level mathematical 3 expression as input and search for the fastest low-level implementations. At the core 4 of the search procedure is a cost model which estimates the performance of different 5 candidates to reduce the frequency of time-consuming on-device measurements. 6    There has been a growing interest in using machine learning techniques to learn a 7 cost model to ease the effort of building an analytical model. However, a standard 8 dataset for pre-training and benchmarking learned cost models is lacking. 9    We introduce TenSet, a large-scale tensor program performance dataset. TenSet 10 contains 52 million program performance records collected from 6 hardware plat-11 forms. We provide comprehensive studies on how to learn and evaluate the cost 12 models, including data collection, model architectures, loss functions, transfer 13 learning, and evaluation metrics. We also show that a cost model pre-trained on 14    TenSet can accelerate the search time in the state-of-the-art tensor compiler by up 15 to 10\u00d7. The dataset is available at https://github.com/tlc-pack/tenset. 16 1 Introduction 17Efficient execution of machine learning models relies on high-performance tensor programs, i.e., low-18 level implementations of tensor operators such as convolution and matrix multiplication. However, it 19 is notoriously challenging to obtain performant tensor programs for numerous tensor operators on 20 various hardware platforms[7]. Existing systems mainly rely on vendor-provided kernel libraries 21 such as cuDNN [14] and OneDNN [22]. However, crafting these libraries requires significant 22 engineering efforts on manual tuning. Moreover, they fall short of supporting new operators invented 23 by researchers and graph optimizations such as operator fusion [12]. To overcome the limitations of 24 manually optimized libraries, researchers and practitioners are building search-based tensor compilers 25 [1, 12, 47]. Given an operator or a computation graph in mathematical expression, these compilers 26 search for the best compiled tensor programs for the target hardware platform. At the core of the 27 search procedure is a cost model which estimates the performance of tensor program candidates to 28 reduce the time-consuming on-device measurements.29With the advances of machine learning, there has been a growing interest in using machine learning 30 techniques to learn a cost model [5, 13, 19, 24, 38, 47]. By learning directly from the data, learning-31 based approaches can simplify the development of analytical cost models, especially for complicated 32 modern hardware platforms. However, training data collection is one of the biggest challenges for 33 adopting learning-based approaches in search-based tensor compilers [35]. Currently, the community 34 lacks a public large-scale dataset that can include performance measurements from multiple hardware 35 * Equal contribution.Training data (small) Training data (large) Online DatasetFigure 1: The architecture of a search-based compiler with a learned cost model. The compiler takes a high-level mathematical expression as input and searches for the best low-level implementation.platforms. This hinders the development of learning-based approaches as pre-training a decent cost 36 model offline requires a comprehensive dataset. Therefore, some compilers [12, 47]  choose to collect 37 the training data online during the search, which makes the search very time-consuming due to 38 unavoidable on-device measurements [26]. Furthermore, without a standard dataset, it is not easy to 39 fairly benchmark and evaluate the proposed models and training algorithms. 40    We introduce TenSet, a large-scale program performance dataset for learned tensor compilers. TenSet 41 contains 52 million program performance records collected from real measurements on Intel CPUs, 42 AMD CPUs, ARM CPUs, and NVIDIA GPUs. We generate random tensor programs for popular 43 deep learning models. The generated programs are compiled by TVM compiler [12] and measured 44 on the target hardware platforms. With this dataset, we provide comprehensive studies on how to 45 learn and evaluate cost models, including data collection, model architectures, loss functions, transfer 46 learning, and evaluation metrics. We integrate the cost models pre-trained on TenSet into Ansor[47], 47 56 considered search spaces include loop optimizations such as tiling, vectorization, parallelization, 57 unrolling, and fusion. By searching in a very large space of different optimization combinations, 58 these compilers can often find programs that are better than hand-optimized implementations. 59 During the search, the algorithm generates a set of promising programs from the search space and 60 compares their performance. The performance can either be estimated by querying the cost model or 61 measured by actually running the programs on the hardware. Due to the size of the search space and 62 the time-consuming on-device measurement, it is impossible to measure the execution time of every 63 program candidate. Therefore, it is common to use a learned cost model to guide the search. The 64 quality of the cost model is one of the most important factors for search efficiency and result quality.65To train the cost model, the compilers can use large offline datasets collected in advance, or small 66 online datasets collected on-the-fly during the search, or both.67Many learned cost models have been proposed [1, 3, 5, 13, 24, 38, 47]. They collect their own dataset, 68 use different feature extraction, model architectures, compilers, and hardware platforms. However, 69 they often do not release code or complete datasets that are easy to access and well documented.70In addition, they typically only cover one hardware platform, making it hard to use the models in 71 a real multi-backend compiler and hard to study transfer learning across hardware platforms. The 72 fragmented development hinders the research in this area. 73 Notably, TVM [12] is a state-of-the-art tensor compiler that implements the above search-based 74 architecture. TVM has two generations of search frameworks: AutoTVM [13] and Ansor [47]. 75 AutoTVM is a semi-automated framework which requires pre-defined manual templates, while Ansor 76 is a more advanced, fully automated framework. This work is built on top of Ansor. Currently, due to 77 the lack of a large-scale offline dataset, TVM has to collect data on-the-fly during the search, leading 78 to an extremely long search time. It can take several hours to optimize and compile a single neural 79 network [13, 47]. 80 93To the best of our knowledge, TenSet is the first public dataset that meets these two requirements.94Terminology95We define some terminologies used in the rest of this section.96Network and Subgraph: A deep neural network with a specific input shape. The input shape 97 contains the batch size and image size (or sequence length). A network is a computational graph.98Typically, tensor compilers partition a large computational graph into several small subgraphs based 99 on certain rules. Subgraphs are the finest granularity for compilation.100Hardware platform: A device to execute the subgraphs. Note that devices with different hardware 101 architectures (e.g., NVIDIA Tesla and Volta) are counted as different hardware platforms.102Task: A task is a pair of a subgraph and a hardware platform. A network contains many subgraphs, 103 so we can extract many tasks from a network on a hardware platform. For example, after the graph 104 partitioning for ResNet-50, there are 27 unique subgraphs, which implies 27 search tasks.105Search space: Each task has its own search space, which is determined by the input/output tensor 106 shapes, data types, data layouts, and the target hardware platform. The search space is usually in the 107 order of millions on CPUs and billions on GPUs. 108 169 neural networks and graph neural networks. Existing works have adopted MLP [1], LSTM [5, 38], 170 GRU [13], GraphSAGE [24, 21], GCN [34], and GBDT [47, 11]. 171 Loss Function The model is used to rank the performance of candidates in a search space. Therefore, 172 the model can be trained with regression losses to predict absolute scores or be trained with ranking 173 losses to predict relative scores. For regression losses, models can be trained with Mean Square Error 174 (MSE) loss to predict the normalized throughput or latency of a program [1]. For ranking losses, 175 models in previous works have been trained with pairwise or listwise losses [10, 13]. 176 Transfer Learning During the search of a new task, if new online data can be collected from the 177 task, it can be used to adapt the pre-trained model to the new task with transfer learning. The 178 transfer learning can be done by using transferable features, fine-tuning, learning local models [13], 179 or meta-learning [34, 17].180 4.2 Integration with the Search Algorithm 181 To use a cost model in the search process, the compiler runs a search algorithm and picks the top-k 182 programs according to the cost model for each task. If on-device measurements are not allowed, 183 the k is set to 1 and the compiler makes decisions totally based on the cost model. If on-device 184 measurements are allowed, the k can be set to a larger number. For each task, the compiler measures 185 the top-k programs on actual devices and picks the best one according to real measurement results.186This process can also be done iteratively because we can update the cost model using the newly 187 collected measurement data and run the search again. We can also use a scheduler to allocate 188 different time budgets to different tasks according to their importance [46, 47]. In a search with 189 on-device measurements, a number of total measurement trials is set as the time budget, because 190 measurements are the most expensive part of the search. The search terminates when it runs out of 191 allowed measurement trials. 192 256give guidance to future dataset collection, we compare the performance of cost models trained with 257 different combinations of # task and # programs per task. We use the same training set and test set as 258 in Sec. 5.2 but randomly downsample the whole training set to get smaller datasets with different 259 combinations of # task and # programs per task.Figure 5aandFigure 5bshow how the model's 260 top-k score varies along with changes in the two factors described above. It shows that the model's 261 performance generally increases along with the dataset size, although an increase in the dataset size 262 does not guarantee an improvement in the model performance. Besides, the figure shows that in 263 general, given a fixed total size, allocating more to # task is often more effective than allocating more 264 to # program per task. For example, for a dataset of size 120000, (600 tasks, 200 programs per task) 265 works better than (300 tasks, 400 programs per task) and (200 tasks, 600 programs per task).266Search with a Pre-trained Model267We combine the findings from the previous sections and train four MLP models with ranking loss 268 for Intel Xeon Platinum-8272, Intel E5-2673, AMD EPYC-7452, and NVIDIA K80 respectively.269Each model is trained on the dataset collected from a single hardware platform. We integrate the 270 cost models into the Ansor auto-scheduler [47]  in TVM [13]. We name the original Ansor (i.e., the 271 7 316 the best performing configuration for that particular layer and combines it with the best performing 317 configuration of the next layer and so on. However, sometimes the combination of the best candidates 318 of different tasks does not result in the best end-to-end performance. In this experiment, we integrate 319 random sampling to solve this problem. After measuring the program composed of best candidates of 320 all tasks, we randomly sample one of the top 3 candidates for every task and repeat this procedure 80 321 times; for each time, we measure the end-to-end performance of the new program and choose the best 322 performing program so far.Figure 7shows the experiment results for 3 networks. For each network, 323 we first run the search process for a certain number of trials and start the random sampling based on 324 the measurement records collected during the search. Then we plot the best inference latency we 325 have found as a function of the number of random samples we have explored. The plot shows that 326 as the number of random samples increases, we can always find better programs with lower latency.327The measurement variance is around 0.1 ms, so we can conclude that the decrease in latency is due 328 to improvement in programs instead of noise. Note that although the random sampling generally 329 produces better programs, this process itself takes time, which leads to a trade-off that we have to 330 consider. 331 9 6 Related Work 332 Tensor compilers use compiler techniques to optimize the execution of tensor programs. Some 333 notable compilers are Halide [32], TACO [25], XLA [40], Tensor Comprehensions [42], TVM [12], 334 nGraph [15], Glow [33], Tiramisu [6], TASO [23], HummingBird [31], and Rammer [28]. 335To guide the search in search-based tensor compilers, many learned cost models have been proposed 336 [1, 3, 5, 13, 24, 38, 47]. Besides learned cost models, there are analytical models [8, 27, 37,39,42]    337 for tensor program cost estimation. Apart from single device tensor program domain, there are learned 338 cost models for other system problems [29, 45, 36]. In addition to supervised learning approaches, 339 there are reinforcement learning approaches for compiler optimizations [2, 4, 18, 20, 30, 46, 48].340There are several existing performance dataset for programs [6, 16, 44]. To the best of our knowledge,341TenSet is the first public multi-platform dataset for tensor programs with the largest number of 342 samples. 343 7 Discussion 344 Limitation There are some limitations of this dataset. For example, the dataset only includes 345 programs for floating point and dense neural networks. The subgraphs are partitioned by a specific 346 algorithm, which limits the types of subgraphs. 347 Potential societal impact The dataset is of tensor programs. It does not contain any personally 348 identifiable information or offensive content. The dataset is used to make tensor compilers better, 349 which makes the execution of neural networks faster. It does not have any direct potential negative 350 societal impact. 351 Conclusion We introduce TenSet, a large-scale multi-platform program performance dataset for 352 learned tensor compilers. We conduct comprehensive experiments with the dataset and show its 353 practical usage in a state-of-the-art tensor compiler. We hope that despite the end of Moore's law, and 354 despite the continuously changing application-specific hardware platforms as a result of it, TenSet 355 can help continue the performance scaling by improving the tensor compilers and further advancing 356 the research in the field. 357 References 358 [1] , et al. Learning to optimize 360 halide with tree search and random programs.\n\nPaddedInput(i0, i1, i2, i3) = tir.if_then_else(((((i1 >= 1) && (i1 < 57)) && (i2 >= 1)) && (i2 < 57)), Data[i0, (i1 -1), (i2 -1), i3], 0f) Weight = PLACEHOLDER [3, 3, 64, 128] Conv2dOutput(nn, yy, xx, ff) += (PaddedInput[nn, ((yy*2) + ry), ((xx*2) + rx), rc] * Weight[ry, rx, rc, ff]) Bias = PLACEHOLDER [1, 1, 1, 128] T_add(ax0, ax1, ax2, ax3) = (Conv2dOutput[ax0, ax1, ax2, ax3] + Bias[ax0, 0, 0, ax3]) T_relu(ax0, ax1, ax2, ax3) = max(T_add[ax0, ax1, ax2, ax3], 0f)      Figure 2 shows the computational graph of an example task, which is a fused convolution + bias 116 add + ReLU activation. The computational graph is printed in a form similar to Einstein notation.  representative CV and NLP tasks. We vary the batch size and input image sizes to generate different 125 subgraphs. Note that we focus on small batch sizes in this dataset because tensor compilers are mainly 126 used for optimizing trained models for inference. In total, there are 120 network configurations.\n\n\n127\n\nNext, we pick 6 hardware platforms according to their availability on public cloud providers, their 128 popularity in the machine learning community, their compiler tool chain support, and the data 129 collation cost. Their specification is listed in Table 2. We favor publicly-accessible cloud instances 130 for improved reproducibility.\n\n\n131\n\nFor each pair of (network, hardware platform), we run the graph partitioning algorithm to obtain 132 a list of unique subgraphs. A subgraph usually includes a heavy tensor operator (e.g., conv2d, 133 conv3d, conv2d_tranpose, depthwise_conv2d, matmul, softmax) fused with lightweight operators 134 (e.g, element-wise operators). For each subgraph, we randomly sample programs from its search 135 space generated by Ansor [47]. We then dump the sampled programs and measure them on AWS and 136 Azure cloud instances. For each program, we do warm-up and run several repeated measurements.\n\n\n137\n\nThe time costs of all repeated measurements are saved in the measurement record files. The record 138 file is in JSON format. Read and write utility functions are provided to parse and generate these files.\n\n139 Figure 1 shows some statistics of the dataset. In total, we collect tasks from 120 networks \u00d7 140 6 hardware platforms. There are 2,308 subgraphs extracted from the 120 networks, so we have 141 from its search space and generate measurement records. The search space size of a task varies from 143 10 to the order of billions. In total, the dataset includes 51,577,248 measurement records from all the 144 tasks. The collection process takes several weeks with clusters of cloud instances.\n\n\n4 Learning and Evaluating a Cost Model in Tensor Compilers\n\n\n146\n\nThe goal of a learned cost model is to rank the performance of different tensor programs in a given 147 search space. This section gives a high-level overview of how to learn, evaluate and use cost models 148 in tensor compilers. The scope of this work is supervised learning.   Table 3: Evaluation of different models using dataset-based metrics and search-based metrics. The description of each metric is detailed in Appendix B. Lower RMSE and latency, and higher pairwise accuracy, R 2 , and top-k scores are desirable. 193 To evaluate and compare the performance of cost models, there are two types of evaluation metrics:    Intuitively, the problem we are tackling is fundamentally a ranking problem rather than a regression 238 problem. Therefore, using ranking-based metrics can evaluate all kinds of models while directly 239 using regression-based metrics is meaningless for some models. In addition, we case most about the 240 top-ranked programs because the compiler will actually choose and compile them. We use top-k 241 score in the following sections.  one without our cost model) as \"Ansor default\" and follows its official benchmark scripts to run the 272 benchmark.\n\n\nEvaluation Metrics\n\n\n273\n\nIn Figure 5c \n\n\na state-of-the-art search framework in TVM compiler, and show that it reduces the search time by up 48 to 10\u00d7 while achieving the same search quality. 49 2 Background: Search-based Compilers with Learned Cost Models 50Figure 1shows the general architecture of a search-based compiler with a learned cost model. This\n\n\n51 architecture is used by plenty of recent tensor compilers, such as TVM [13, 47], Halide [1, 3, 38], 52 Tiramisu [5] and XLA [24]. The compiler accepts an operator or a computational graph in high-level 53 mathematical expression as input, and runs a search algorithm to find the best tensor program. The 54 adopted search algorithms include Monte Carlo tree search (MCTS)[5, 19], genetic algorithm (GA) 55 [47], beam search[1], simulated annealing [13], and reinforcement learning [2]. The majority of the\n\nFigure 2 :\n2The computational graph for a fused conv2d-biad_add-relu task.\n\nFigure 3 :\n3A sample program for the task inFig.\n\n110\nMeasurement record: A measurement record is a tuple of task, program, and on-device measurement 111 result. The program measurement module takes the task and program, performs the compilation, 112 execution, and measurement. The measurement result contains the execution time of the program or 113 an error code if encountering compilation or runtime errors.\n\n117 Figure 3\n1173then shows a sample program from the search space of this task on a CPU. The program is 118 optimized by multi-level tiling, parallelization, vectorization, unrolling, and fusion.\n\n\nis organized in a hierarchical structure, as shown in Figure 4. The top-level includes some 121 common networks with different configurations and hardware platforms. Table 6 (in Appendix A) 122 lists the specifications of network architectures and input shapes. The network architectures are 123 chosen from PyTorch's vision model zoo and Huggingface's transformer model zoo. They cover both 124\n\nFeature\nExtraction To feed a program into a machine learning model, the compiler can extracts 151 features from the high-level task description, optimization specification, and low-level compiled 152 program. Currently, manual feature extraction is still required because the programs contain a lot 153 of structural and numerical information. Learning an end-to-end model directly from text tokens 154 has not been explored for this specific problem in the literature. The features can be extracted at 155 multiple levels, from high-level task descriptions to low-level compiled programs. Features from 156 higher levels are faster to extract, while features from lower levels are slower to extract because 157 they require going through the compilation process. The high-level task features can include the 158 features or embeddings of the input computational graphs such as shape and access pattern. It can 159 also include hardware platform information such as cache size and vector width. The optimization 160 features can include the used loop transformations and schedules. The low-level program features can 161 include features extracted from the lowered IR or even machine code, which can help to model the 162 end-to-end compilation process. At all levels, the features are used to capture the memory access and 163 computation patterns of the tensor programs, which are the most important factors of the performance 164 of the programs. The lists of typical features can be found at [1, 5, 47]. 165 Model Architecture The features that can be extracted can have vector structures, tree structures, 166 or graph structures. They also have variable lengths. To feed these features into a machine learning 167 model, the hierarchical structure is generally flattened and fed into feed-forward neural networks with 168padding or sum aggregation. The hierarchical structure can also be feed directly into tree recurrentRMSE    \n\nIn\ndataset-based metrics and search-based metrics. The dataset-based metrics evaluate the accuracy of 195 the model on a static dataset, and search-based metrics evaluate the end-to-end search efficiency or 196 search quality after integrating the cost models into search algorithms. 197 Typical dataset-based metrics include Rooted Mean Square Error (RMSE), Mean Absolute Percentage 198 Error (MAPE), R 2 (Coefficient of Determination), pairwise comparison accuracy. We also propose a 199 new metric top-k score, which reflects how well the top-k programs predicted by the model perform.200 Their definition can be found at Appendix B. For search-based metrics, there are two typical metrics: 201 we can either fix the search time, and compare the latency of the resulting programs, or fix a converged 202 latency, and compare the search time used to reach it. 203 The search-based metrics are the end-to-end objective of a search-based compiler, but these metrics 204 are expensive to compute due to the time-consuming search process. These metrics also involve other 205 factors that are not directly related to the cost model. On the other hand, dataset-based metrics can be 206 computed very fast on a static dataset. They are more directly related to the cost model but do not 207 directly reflect the end-to-end objective. In Sec. 5.1, we compare several dataset-based metrics to see 208 how well they reflect the end-to-end objective. section, we try to answer the following questions: What are the best metrics to evaluate a 211 cost model (Sec. 5.1)? How do the model architectures and loss functions influence the model 212 performance (Sec. 5.2)? How should we collect the dataset (Sec. 5.3)? How can the model improve 213 search efficiency (Sec. 5.4)? Is the online transfer learning useful (Sec. 5.5)? How can we future 214 improve the search result (Sec .5.Sec. 4.3, we list some widely used dataset-based metrics and discuss the possible discrepancies 217between these metrics and the end-to-end objective. In this section, we compare several dataset-based 218 metrics and pick the one that reflects the end-to-end objective best. We can then use it as the evaluation 219 metric in the following sections without involving the search process.\n\n220\nWe train three different models and evaluate them with both dataset-based metrics and search-based221 metrics. We use the dataset from Intel Xeon Platinum-8272. We hold out a ResNet-50 (batch size=1, 222 image size=224) as the test set, and use the rest of the dataset as the training set. For dataset-based 223 metrics, we evaluate the models on the test dataset. For the search-based metrics, we run the search 224 algorithm with each model for ResNet-50 under the same time budget, and measure the latency of 225 the result programs.\n\nFigure 5 :\n5evaluates the effectiveness of different model architectures and loss functions. We 244 use the dataset from Intel Xeon Platinum-8272. We hold out a test set that consists of five networks, 245 ResNet-50, MobileNet-V2, ResNext-50. BERT-tiny, and BERT-base with batch size 1 and image 246 size 224 (or sequence length 128), and train models using different combinations of architectures 247 and loss functions on the training set that consists of the whole dataset excluding all the tasks that 248 appear in the above five networks. Further training details are included in Appendix C. Note that for 249 the ranking loss, we use the probabilistic cost function originated from LambdaRank[43].Table. 4250 shows the top-k scores of these models on the test set. The highest top-1 score and top-5 score in 251 each column are in bold. In general, a MLP model trained with the ranking loss performs the best. evaluates the impact of dataset size on model performance. The size of a dataset is 254 defined as # tasks multiplied by # programs per task. In dataset collection, we typically have a budget 255 on the total size, but how to set these two factors given the total size (i.e., their product) is unclear. (a)(b): The impact of dataset size on model performance. The x-axis is number of tasks and y-axis is number of programs per task. The lighter the color, the better the model. (c): Network performance tuning curve. The y-axis is the result program's latency and the x-axis is the search time.\n\nFigure 6 :\n6Search time comparison. The y-axis is the search time used to converge to the same result.\n\n\n, we run the search for ResNet-50 on Intel Xeon Platinum-8272 with the cost model 274 trained on the same hardware platform, and report how the result program's latency changes along 275 with the total search time in both Ansor default and our approach. It shows that our pre-trained 276 cost model makes the search converge much faster. This is because \"Ansor default\" does not have 277 a pre-trained model. It thus has to start from random search and collect data during the search and 278 train the model online, which takes an extremely long time. Figure 6a compares the search time that 279 Ansor default and our approach take respectively to converge to the same result on more networks 280 and two hardware platforms (Intel Platinum CPU and NVIDIA K80 GPU). It shows that the use of 281 our cost model reduces the search time by up to 10\u00d7 while maintaining the same search quality. One 282 of our industry collaborators runs weekly benchmarks and compiles hundreds of models for dozens 283 of hardware platforms. The long-term savings of using this dataset are significant.284InFigure 6b, we run the search algorithm with three cost models on three different hardware 285 platforms. It shows that the models are able to transfer across different hardware platforms, although 286 the performance is not as good as on the platform that generates the dataset that the model is trained 287 on. We also train a model on a training set that consists of one-third of data from each platform. This 288 model performs the second best on all three platforms, which implies the possibility to train one 289 general model for all hardware platforms.\n\nFigure 7 :\n7explore transfer learning, the cost model is pre-trained using off-line learning with a static dataset,292    and later improved using online learning during the search. More specifically, we first collect a 293 certain number of measurement records for each task during the search, and fit a local model to294    predict the difference between the measured latency and the pre-trained model's prediction. Then we 295 continue to conduct another round of search, during which we tune the pre-trained model's prediction 296 with the local model's predicted difference. In this experiment, we train a model using the dataset Optimizing with additional random sampling. The figure shows the lowest program latency found so far as a function of the number of times we reran the experiment.collected from Intel Xeon Platinum-8272 excluding ResNet-50 (batch size 1, image size 224, 240, 298 256), then run the search algorithm on both Intel Xeon Platinum-8272 and Intel e5-2673. We evaluate 299 the effectiveness of the local model in two ways. In Setting 1, we report two results: 1) without 300 using transfer learning where we run the search for 50 measurement trials per task and choose the 301 best measurement out of 50 trials and 2) with transfer learning where we use the first 40 out of 50 302 measurements to fit a local model (the last 10 measurements do not update the model) and choose the 303 best measurement out of 50 trials. In Setting 2, we do everything similar to Setting 1 but we only 304 consider the last 10 measurements in the final reported result. In both cases, we compare the latency 305 of the resulting program, as shown in\n\n\nyou state the full set of assumptions of all theoretical results? [N/A] 528 (b) Did you include complete proofs of all theoretical results? [N/A] 529 3. If you ran experiments (e.g. for benchmarks)... 530 (a) Did you include the code, data, and instructions needed to reproduce the main experi-531 mental results (either in the supplemental material or as a URL)? [Yes] 532 (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they 533 were chosen)? [Yes] 534 (c) Did you report error bars (e.g., with respect to the random seed after running experi-535 ments multiple times)? [Yes] 536 (d) Did you include the total amount of compute and the type of resources used (e.g., type 537 of GPUs, internal cluster, or cloud provider)? [Yes]538 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 539 (a) If your work uses existing assets, did you cite the creators? [N/A] 540 (b) Did you mention the license of the assets? [Yes] The license is in the GitHub repo.541 (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] 542 (d) Did you discuss whether and how consent was obtained from people whose data you're 543 using/curating? [N/A] 544 (e) Did you discuss whether the data you are using/curating contains personally identifiable 545 information or offensive content? [Yes] See section 7 546 5. If you used crowdsourcing or conducted research with human subjects... 547 (a) Did you include the full text of instructions given to participants and screenshots, if 548 applicable? [N/A] 549 (b) Did you describe any potential participant risks, with links to Institutional Review 550 Board (IRB) approvals, if applicable? [N/A] 551 (c) Did you include the estimated hourly wage paid to participants and the total amount\n\nTable 1 :\n1Dataset statistics 3 TenSet: A Dataset for Tensor Programs 81 A good dataset is the first requirement of a good model [41, 35]. The purpose of TenSet is to provide 82 a large-scale dataset for pre-training and benchmarking the cost models in tensor compilers. This section describes the requirements of a good dataset and introduces the contents of TenSet.84 3.1 Requirements of the Dataset Large-scale. A learned cost model using this dataset is expected to perform well on common workloads and generalize relatively well to other uncommon workloads. A large-scale dataset containing diverse workloads is necessary for generalization ability. Multi-platform. A tensor compiler typically supports multiple hardware platforms. The dataset should thus contain the records from multiple platforms. This can be used to train different models for different platforms. If the dataset also contains the performance of the same program on different platforms, it is possible to let the model learn the difference among different hardware platforms.This enables advanced research on transfer learning among different hardware platforms.Program: A program or a tensor program refers to a low-level, hardware-dependent implementation 109 of a subgraph. It can be seen as a candidate in the search space.83 \n\n85 \n\n86 \n\n87 \n\n88 \n\n89 \n\n90 \n\n91 \n\n92 \n\n\n\nR 2\n2Pairwise Accuracy Top-1 Score Top-5 Score Latency (ms)Model #1 \n0.09 \n0.77 \n0.85 \n0.86 \n0.92 \n7.89 \n\nModel #2 \n0.07 \n0.89 \n0.84 \n0.87 \n0.95 \n6.45 \n\nModel #3 \n7.27 \n-1818.41 2 \n0.89 \n0.88 \n0.96 \n6.39 \n\n\n\nTable 3\n3shows the evaluation results of five different dataset-based metrics and226 \n\na search-based metric. For dataset-based metrics, a good model has the RMSE close to 0 and the \n\n227 \n\npairwise accuracy, R 2 and top-k score close to 1. For search-based metrics, the lower latency, the \n\n228 \n\nbetter the search result is. \n\n229 \n\nAccording to Table 3, although Model #1 has a small RMSE and large R 2 , it does not result in as \n\n230 \n\ngood latency as Model #3 which has a high RMSE and an extremely low R 2 . This suggests that \n\n\nTable 4 :\n4The top-k scores of different cost models on five test networks.RMSE and R 2 might not be the appropriate metrics for models trained with ranking losses. Model #3 is trained with ranking loss. Directly using its output values to compute regression metrics such232 \n\n233 \n\nas RMSE and R 2 gives meaningless values. On the other hand, pairwise comparison accuracy and \n\n234 \n\nthe top-k score are more consistent with the final latency; thus they are better dataset-based metrics \n\n235 \n\nfor our use case. One slight difference between these two is that pairwise comparison accuracy does \n\n236 \n\nnot differentiate Model #1 and Model #2 well, so top-k score reflects the end-to-end objective best. \n\n237 \n\n\n\nTable 5 .\n5In both settings, transfer learning improves the search results by producing programs with lower latency. In setting 2, we study how exactly the local model affect the measurement records collected within a fixed number of trials and find that it can indeed help find better programs.306 \n\n307 \n\n308 \n\n309 \n\nIntel Xeon Platinum-8272 Intel e5-2673 \n\nSetting 1 \nWith transfer learning \n6.22 ms \n27.26 ms \nWithout transfer learning \n6.43 ms \n29.94 ms \n\nSetting 2 \nWith transfer learning \n6.44 ms \n28.92 ms \nWithout transfer learning \n7.15 ms \n32.03 ms \n\n\n\nTable 5 :\n5Evaluation of the effectiveness of transfer learning. The model is trained on Intel Xeon Platinum-8272 and evaluated on both Intel Xeon Platinum-8272 and Intel e5-2673. For a fixed number of measurement trials on ResNet-50, we compare the latency (ms) of the result program.Note that we only do transfer learning across different types of CPUs. We leave the transfer learning from CPUs to GPUs as future work. Our first intuition is that CPUs and GPUs are based on very different architectures in terms of both memory hierarchy and execution model, and hence we do not think transfer learning between them would be useful.In Sec. 5.4, when compiling a single network with many tasks, the best program was selected foreach intermediate task. For example, for every intermediate neural network layer, the algorithm picks [8] Uday Bondhugula, Albert Hartono, Jagannathan Ramanujam, and Ponnuswamy Sadayappan. A practical automatic polyhedral parallelizer and locality optimizer. In Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation, pages Chen Cai and Yusu Wang. A simple yet effective baseline for non-attributed graph classification. [24] Samuel J Kaufman, Phitchaya Mangpo Phothilimthana, Yanqi Zhou, Charith Mendis, Sudip Proceedings of Machine Learning and Systems, 2021. [25] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amarasinghe. The section 7 (d) Have you read the ethics review guidelines and ensured that your paper conforms to If you are including theoretical results...310 \n\n311 \n\n312 \n\n313 \n\n5.6 Optimizing with Additional Random Sampling \n\n314 \n\n315 \n\n\nA negative R 2 is possible when the model performs worse than the baseline model which always predicts the mean value. This means the the model is not trained to be a good regression model.\n\nGagandeep Goyal, (c) Did you discuss any potential negative societal impacts of your work?. Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul, Yes] SeeSanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Bharat Kaul, Gagandeep Goyal, (c) Did you discuss any potential negative societal impacts of your work? [Yes] See\n", "annotations": {"author": "[{\"end\":110,\"start\":82},{\"end\":137,\"start\":111},{\"end\":149,\"start\":138},{\"end\":162,\"start\":150},{\"end\":195,\"start\":163},{\"end\":221,\"start\":196},{\"end\":250,\"start\":222},{\"end\":280,\"start\":251}]", "publisher": null, "author_last_name": "[{\"end\":95,\"start\":90},{\"end\":122,\"start\":119},{\"end\":148,\"start\":144},{\"end\":161,\"start\":157},{\"end\":180,\"start\":172},{\"end\":206,\"start\":200},{\"end\":235,\"start\":228}]", "author_first_name": "[{\"end\":89,\"start\":82},{\"end\":118,\"start\":111},{\"end\":143,\"start\":138},{\"end\":156,\"start\":150},{\"end\":169,\"start\":163},{\"end\":171,\"start\":170},{\"end\":199,\"start\":196},{\"end\":227,\"start\":222}]", "author_affiliation": "[{\"end\":109,\"start\":97},{\"end\":136,\"start\":124},{\"end\":194,\"start\":182},{\"end\":220,\"start\":208},{\"end\":249,\"start\":237},{\"end\":279,\"start\":252}]", "title": "[{\"end\":79,\"start\":1},{\"end\":359,\"start\":281}]", "venue": null, "abstract": "[{\"end\":15803,\"start\":361}]", "bib_ref": "[{\"end\":17563,\"start\":17559},{\"end\":19028,\"start\":19025}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20045,\"start\":19728},{\"attributes\":{\"id\":\"fig_1\"},\"end\":20556,\"start\":20046},{\"attributes\":{\"id\":\"fig_2\"},\"end\":20632,\"start\":20557},{\"attributes\":{\"id\":\"fig_3\"},\"end\":20682,\"start\":20633},{\"attributes\":{\"id\":\"fig_4\"},\"end\":21046,\"start\":20683},{\"attributes\":{\"id\":\"fig_6\"},\"end\":21244,\"start\":21047},{\"attributes\":{\"id\":\"fig_7\"},\"end\":21642,\"start\":21245},{\"attributes\":{\"id\":\"fig_8\"},\"end\":23579,\"start\":21643},{\"attributes\":{\"id\":\"fig_9\"},\"end\":25839,\"start\":23580},{\"attributes\":{\"id\":\"fig_10\"},\"end\":26381,\"start\":25840},{\"attributes\":{\"id\":\"fig_11\"},\"end\":27893,\"start\":26382},{\"attributes\":{\"id\":\"fig_12\"},\"end\":27997,\"start\":27894},{\"attributes\":{\"id\":\"fig_13\"},\"end\":29644,\"start\":27998},{\"attributes\":{\"id\":\"fig_14\"},\"end\":31305,\"start\":29645},{\"attributes\":{\"id\":\"fig_15\"},\"end\":33139,\"start\":31306},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34489,\"start\":33140},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34697,\"start\":34490},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35234,\"start\":34698},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35949,\"start\":35235},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36513,\"start\":35950},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38164,\"start\":36514}]", "paragraph": "[{\"end\":16785,\"start\":15805},{\"end\":17131,\"start\":16793},{\"end\":17724,\"start\":17139},{\"end\":17938,\"start\":17732},{\"end\":18433,\"start\":17940},{\"end\":19685,\"start\":18502},{\"end\":19727,\"start\":19714}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17051,\"start\":17044},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18788,\"start\":18781}]", "section_header": "[{\"end\":16791,\"start\":16788},{\"end\":17137,\"start\":17134},{\"end\":17730,\"start\":17727},{\"attributes\":{\"n\":\"145\"},\"end\":18494,\"start\":18436},{\"end\":18500,\"start\":18497},{\"attributes\":{\"n\":\"4.3\"},\"end\":19706,\"start\":19688},{\"end\":19712,\"start\":19709},{\"end\":20568,\"start\":20558},{\"end\":20644,\"start\":20634},{\"end\":20687,\"start\":20684},{\"end\":21060,\"start\":21048},{\"end\":21651,\"start\":21644},{\"end\":23583,\"start\":23581},{\"end\":25844,\"start\":25841},{\"end\":26393,\"start\":26383},{\"end\":27905,\"start\":27895},{\"end\":29656,\"start\":29646},{\"end\":33150,\"start\":33141},{\"end\":34494,\"start\":34491},{\"end\":34706,\"start\":34699},{\"end\":35245,\"start\":35236},{\"end\":35960,\"start\":35951},{\"end\":36524,\"start\":36515}]", "table": "[{\"end\":34489,\"start\":34444},{\"end\":34697,\"start\":34550},{\"end\":35234,\"start\":34780},{\"end\":35949,\"start\":35507},{\"end\":36513,\"start\":36246},{\"end\":38164,\"start\":38079}]", "figure_caption": "[{\"end\":20045,\"start\":19730},{\"end\":20556,\"start\":20048},{\"end\":20632,\"start\":20570},{\"end\":20682,\"start\":20646},{\"end\":21046,\"start\":20688},{\"end\":21244,\"start\":21065},{\"end\":21642,\"start\":21247},{\"end\":23579,\"start\":21652},{\"end\":25839,\"start\":23584},{\"end\":26381,\"start\":25845},{\"end\":27893,\"start\":26395},{\"end\":27997,\"start\":27907},{\"end\":29644,\"start\":28000},{\"end\":31305,\"start\":29658},{\"end\":33139,\"start\":31308},{\"end\":34444,\"start\":33152},{\"end\":34550,\"start\":34496},{\"end\":34780,\"start\":34708},{\"end\":35507,\"start\":35247},{\"end\":36246,\"start\":35962},{\"end\":38079,\"start\":36526}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16287,\"start\":16279},{\"end\":17952,\"start\":17944},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":19726,\"start\":19717}]", "bib_author_first_name": "[{\"end\":38454,\"start\":38448},{\"end\":38476,\"start\":38467},{\"end\":38496,\"start\":38487},{\"end\":38512,\"start\":38506}]", "bib_author_last_name": "[{\"end\":38465,\"start\":38455},{\"end\":38485,\"start\":38477},{\"end\":38504,\"start\":38497},{\"end\":38517,\"start\":38513}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":38698,\"start\":38356}]", "bib_title": null, "bib_author": "[{\"end\":38467,\"start\":38448},{\"end\":38487,\"start\":38467},{\"end\":38506,\"start\":38487},{\"end\":38519,\"start\":38506}]", "bib_venue": "[{\"end\":38446,\"start\":38356}]"}}}, "year": 2023, "month": 12, "day": 17}