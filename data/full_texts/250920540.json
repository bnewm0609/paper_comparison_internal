{"id": 250920540, "updated": "2023-10-05 12:34:56.81", "metadata": {"title": "AdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields", "authors": "[{\"first\":\"Andreas\",\"last\":\"Kurz\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Neff\",\"middle\":[]},{\"first\":\"Zhaoyang\",\"last\":\"Lv\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Zollhofer\",\"middle\":[]},{\"first\":\"Markus\",\"last\":\"Steinberger\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Novel view synthesis has recently been revolutionized by learning neural radiance fields directly from sparse observations. However, rendering images with this new paradigm is slow due to the fact that an accurate quadrature of the volume rendering equation requires a large number of samples for each ray. Previous work has mainly focused on speeding up the network evaluations that are associated with each sample point, e.g., via caching of radiance values into explicit spatial data structures, but this comes at the expense of model compactness. In this paper, we propose a novel dual-network architecture that takes an orthogonal direction by learning how to best reduce the number of required sample points. To this end, we split our network into a sampling and shading network that are jointly trained. Our training scheme employs fixed sample positions along each ray, and incrementally introduces sparsity throughout training to achieve high quality even at low sample counts. After fine-tuning with the target number of samples, the resulting compact neural representation can be rendered in real-time. Our experiments demonstrate that our approach outperforms concurrent compact neural representations in terms of quality and frame rate and performs on par with highly efficient hybrid representations. Code and supplementary material is available at https://thomasneff.github.io/adanerf.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.10312", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/KurzNLZS22", "doi": "10.48550/arxiv.2207.10312"}}, "content": {"source": {"pdf_hash": "6e0cc02e657d853f52a6848a4443890f76c77d66", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2207.10312v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e1003ef2749bd7f3379585c79bda95b8015a8221", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6e0cc02e657d853f52a6848a4443890f76c77d66.txt", "contents": "\nAdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields\n\n\nAndreas Kurz \nGraz University of Technology\nAustria\n\nThomas Neff \nGraz University of Technology\nAustria\n\nZhaoyang Lv \nReality Labs Research\nUSA\n\nMichael Zollh\u00f6fer \nReality Labs Research\nUSA\n\nMarkus Steinberger \nGraz University of Technology\nAustria\n\nAdaNeRF: Adaptive Sampling for Real-time Rendering of Neural Radiance Fields\nNeural RenderingNeural Radiance FieldsView Synthesis\nNovel view synthesis has recently been revolutionized by learning neural radiance fields directly from sparse observations. However, rendering images with this new paradigm is slow due to the fact that an accurate quadrature of the volume rendering equation requires a large number of samples for each ray. Previous work has mainly focused on speeding up the network evaluations that are associated with each sample point, e.g., via caching of radiance values into explicit spatial data structures, but this comes at the expense of model compactness. In this paper, we propose a novel dual-network architecture that takes an orthogonal direction by learning how to best reduce the number of required sample points. To this end, we split our network into a sampling and shading network that are jointly trained. Our training scheme employs fixed sample positions along each ray, and incrementally introduces sparsity throughout training to achieve high quality even at low sample counts. After finetuning with the target number of samples, the resulting compact neural representation can be rendered in real-time. Our experiments demonstrate that our approach outperforms concurrent compact neural representations in terms of quality and frame rate and performs on par with highly efficient hybrid representations. Code and supplementary material is available at https://thomasneff.github.io/adanerf.\n\nIntroduction\n\nThe introduction of neural radiance fields [20] pushed the boundaries of modern computer graphics and vision by improving the state of the art of applications such as 3D reconstruction [35], rendering [44,45,21,22], animation [24,29] and scene relighting [18]. Furthermore, since the introduction of neural radiance fields, a significant amount of research focused on improving the resulting image quality [2], training speed [14,21], and inference performance [22,21,27,9,32,10,45]. Thus, real-time rendering of photorealistic neural radiance fields is now possible on standard consumer GPUs.\n\nAuthors contributed equally to this work. arXiv:2207.10312v2 [cs.CV] 28 Jul 2022 Fig. 1: AdaNeRF employs a single-evaluation sampling network and a multievaluation shading network to significantly reduce the number of required network evaluations per view ray. For each ray, the sampling network predicts a vector of estimated sample densities \u03b4 that correspond to exactly one sample location each. We threshold the predicted \u03b4 to cull away samples that are expected to have only minor contribution and only proceed to evaluate the shading network for this small subset of samples along the ray. Finally, when accumulating the outputs of the shading network along the ray, we additionally multiply the density \u03c3 predicted by the shading network with the density \u03b4 predicted by the sampling network. This enables gradients of the RGB output loss to flow back to the sampling network, enabling end-to-end training of the full pipeline.\n\nHowever, current real-time renderable neural radiance fields either require large amounts of memory, a restricted training data distribution or bounded training data, and it is unclear how to find an efficient compromise between those limitations. Explicit data structures have difficulties accounting for unbounded scenes, and storing large amounts of neural radiance fields (NeRFs) in sparse grids [10,9], trees [45] or hash tables [21] consumes prohibitive amounts of memory if multiple neural radiance fields need to be accessed in quick succession, such as in a streaming scenario. At a compact memory footprint, previous work relied on reducing the number of samples per view ray via dedicated sampling networks that estimate suitable sample locations along each view ray to improve rendering speed [27,22,14]. These sampling networks are commonly trained via supervision from depth [22] or the predicted density of a neural radiance field [27], thus requiring additional time-consuming preprocessing or pretraining steps. Alternatively, sampling networks can also be used to learn segments along each ray using an integral network [14]. Although this improves efficiency at a slight loss in quality, constructing these integral networks drastically increases the complexity and duration of training. Finally, light field networks [33] only evaluate a single sample per ray by parameterizing the input ray using Pl\u00fccker coordinates, but learning such a light field typically requires meta-learning to achieve sufficient quality even on small toy-datasets.\n\nIn this paper, we introduce AdaNeRF, a compact dual-network neural representation that is optimized end-to-end, and fine-tuned to the desired performance for real-time rendering. The first sampling network predicts suitable sample locations using a single evaluation per view ray, while the second shading network adaptively shades only the most significant samples per ray. In contrast to previous methods based on sampling networks, AdaNeRF does not require any preprocessing, pretraining or special input parametrizations, lowering the overall complexity. We use fixed, discrete sample locations along each ray to set up a soft student-teacher regularization scheme by multiplying the predicted density of our sampling network with the output density of the shading network. Thus, both networks can modify the final RGB output and gradients flow throughout the whole pipeline. We ensure sparsity within our sampling network via a 4-phase training scheme, after which we fine-tune our shading network to the desired sample counts for real-time rendering. We adaptively sample our shading network for each individual ray-we only evaluate the shading network for the most important samples, as predicted by the sampling network. The resulting sparse, dual-network pipeline can be rendered in real-time on consumer GPUs using our custom real-time renderer based on CUDA and TensorRT.\n\nOur experimental results demonstrate the benefits of AdaNeRF compared to prior arts on a variety of datasets, including large, unbounded scenes. First, the adaptive sampling in AdaNeRF significantly increases the sampling efficiency of raymarching-based neural representations. Second, AdaNeRF outperforms previous sampling network based approaches in both rendering speed and quality with the same compact memory footprint. Finally, we qualitatively show that multiple AdaNeRFs can scale to complex scenes of arbitrary size.\n\nIn summary, we make the following contributions: -A novel dual-network architecture to jointly learn sampling and shading networks for compact real-time neural radiance fields, outperforming existing sampling-network based approaches. -An additional adjustable adaptive sampling scheme to only shade the most significant samples per ray, further improving quality and efficiency at identical average sample counts. -A real-time rendering implementation that relies on dynamic, sparse sampling of our compact dual-network representation, targeting a sweet spot in the trade-off between performance, quality, and memory.\n\n\nRelated Work\n\nSince the introduction of NeRF [20], coordinate-based neural radiance fields have improved the state-of-the-art across many domains, including dynamic scene modeling [8,6,24,37,13,40,28,25,11], animatable avatars and scenes [17,26,5,16,42], relightable objects [34,4], and object reconstruction [23,41,39,43]. The quality of object captures can be improved by incorporating different scales into the encoding, reducing aliasing and sampling artifacts when novel views are generated [2]. While this is mostly restricted to single-object captures, recent research has also investigated the reconstruction from unconstrained images [18] and large, unbounded scenes [46,3]. However, most advances of neural radiance fields focus on improving the output quality, with many of these advancements being infeasible to compute in real-time on consumer GPUs. Our work is closely related to the advancement of neural radiance fields towards real-time rendering performance, which can be categorized into three domains: (1) Decomposed neural radiance fields. (2) Baking, caching or precomputing weights into an explicit spatial data structure, and (3) improving the sampling efficiency and reducing the total number of samples that are computed per frame.\n\nDecomposed neural radiance fields. By splitting a single MLP into many separate MLPs [32,31,7,30], subdivided scene grids [15] or primitives [17], both the quality and efficiency of rendering can be increased. Such a composition of scenes can represent scenes at city-scale [38,36]. Although these representations are useful to render single objects [32] or human avatars [17] in real-time, a real-time, scene-level representation has yet to be demonstrated.\n\nBaking radiance fields. The radiance field can be stored inside a 3D grid [9], inside a sparse voxel octree [45,15] that does not even require any neural networks [44], or inside a sparse grid [10]. These methods run in real-time at a significantly increased memory footprint, which can be prohibitively expensive for scenarios such as streaming, where real-time swapping of neural radiance fields is desired. The concurrently introduced Instant-NGP [21] combines a hierarchical spatial hash table encoding with small MLPs to learn and render a scene representation in real-time. However, it is still unclear how it performs in demanding real-time scenarios and how to optimally tune its hash table size to still be as compact as fully neural representations.\n\nImproving the sampling efficiency of neural radiance fields. Recent work drastically improved the sampling efficiency of NeRF while keeping the same compact memory footprint. DONeRF [22] proposed a reduction in overall sample count by swapping the coarse network of the original NeRF with a depth oracle network, which can provide suitable sample locations for the second shading network, thus reducing the number of samples per ray by up to 128\u00d7. However, it cannot be trained end-to-end, and struggles without reliable depth information. Similarly to DONeRF, TermiNeRF [27] uses a sampling network that is conditioned on the density of a pre-trained NeRF. TermiNeRF uses the whole range of samples without the need for a depth map, which can improve quality in geometrically ambiguous scenarios. Although it can technically be trained end-to-end, TermiNeRF requires a suitable pretrained NeRF for initialization of its color network to achieve the best results. AutoInt [14] approaches sampling networks differently by predicting the lengths of segments along each ray. By predicting ray segments instead of samples, subsequent integral networks can efficiently predict the density and color of each ray segment, reducing the number of network evaluations. However, the training procedure is significantly longer and more complex. Light field networks [33,1] reduce the number of network evaluations to a single sample per ray by directly mapping a view ray to the observed color. Although such an approach is advantageous in terms of memory footprint and rendering efficiency, without a meta-learned multi-view consistency prior, it fails to synthesize novel views in real-world scenes, and is thus not suitable for real-time novel view synthesis tasks for real world captures.\n\nWith AdaNeRF, we follow up on DONeRF [22], TermiNeRF [27], and AutoInt [14], and demonstrate that our approach is end-to-end trainable and more robust across a variety of training setups. Our approach achieves higher quality with faster real-time rendering performance by drastically reducing the amount of required samples, while at the same time keeping the representation compact.\n\n\nMethod\n\nAdaNeRF consists of a fully end-to-end trainable pipeline that can be rendered in real-time. We replace the coarse network of the original NeRF by a sampling network S that is only evaluated once per ray, minimizing the number of network evaluations to generate the final image. The sampling network takes the ray origin p and the ray direction d as input. The output of the sampling network is a vector of predictions \u03b4, corresponding to the predicted importance of samples along each ray. The shading network T takes the prediction of the sampling network, positionally encodes the samples with the largest contribution and outputs their density \u03c3 and color c. By evaluating the sampling network once, a majority of samples with low contributions can be culled, increasing the overall efficiency of the pipeline. Figure 1 shows an overview of our dual-network setup.\n\n\nEnd-to-end Trainable Sampling Network\n\nWe propose to multiply the predicted per-sample density \u03b4 i of the sampling network with the predicted per-sample density \u03c3 i of the shading network. This formulation allows backpropagation to reach the sampling network. This is possible by using fixed sample locations along each ray, and placing exactly one sample in the center of each cell when discretizing the space along each ray. In contrast to previous work based on sampling networks, we do not require ground truth depth [22], and we avoid distinctly separated training steps [22,27].\n\nWe modify the standard ray accumulation function [20] to include an additional multiplication by the outputs \u03b4 i of the sampling network\nC(r) = N i=1 T i (1 \u2212 exp(\u2212\u03b4 i \u03c3 i t i ))c i , where T i = exp \uf8eb \uf8ed \u2212 i\u22121 j=1 \u03b4 j \u03c3 j t j \uf8f6 \uf8f8 , (1)\nwhere\u0108 is the estimated, accumulated color, N is the number of samples along the ray, T i is the accumulated transmittance along the ray, t i is the distance between adjacent samples and \u03c3 i is the output density of the shading network for sample i. The introduction of the multiplication by \u03b4 i enables the sampling network to directly increase or decrease the importance of samples via its prediction, and to receive gradients from the MSE color reconstruction loss. First, dense training forces the sampling network predictions close to \u03b4 = 1, enabling the shading network to densely sample the underlying scene. The sparsification phase then forces a majority of the sampling network predictions towards \u03b4 = 0, leaving only the most significant samples. The sparse training phase then adjusts the shading network to the newly sparsified sampling network. Finally, in the finetuning phase, we adaptively place samples for all sampling network predictions \u03b4 \u2265 \u03c4 up to a desired maximum number to further optimize for real-time rendering.\n\n\nSparse Adaptive Sampling Network Distillation\n\nThe modification of the ray accumulation alone does not ensure that the sampling network outputs sparse predictions-it might just as well always output the 1 vector, leading the shading network to place one sample in each cell, effectively ignoring the sampling network prediction. We disentangle \u03b4 from the shading network density by introducing sparsity into the sampling network, which forces the network to select only the most important density values.\n\nAdaNeRF trains the sampling network and the shading network end-to-end and progressively reduces the required samples per view ray. The shading network is trained via a standard MSE loss on the accumulated RGB color. The sampling network loss is composed of a sparsity loss which includes an 1 -loss that matches the output density of the shading network and an additional density multiplication term derived from the MSE loss of the shading network:\nl sampling (\u03b4, \u03c3, c) = \u03bb 0 \u00b7 l mse (\u03b4, \u03c3, c) + \u03bb 1 \u00b7 l sparsity (\u03b4, \u03c3).(2)\nAdaNeRF uses a soft student-teacher regularization training scheme with 4 phases. We illustrate the training scheme in Fig. 2 and describe each phase in detail:\n\nDense Training. The initial dense training phase establishes the teacher, by encouraging the sampling network to output dense predictions via an 1 -loss of all its outputs towards 1. In practice, this phase could be replaced by initializing the network weights to output 1 directly to increase training speed at a potential loss of propagated information to the sampling network. In either way, the shading network samples the full input space to provide an initial estimate of the scene, which prevents both networks from collapsing in the later stages.\n\nSparsification. The second phase introduces an additional 1 -loss to the sampling network that forces the majority of predictions towards 0. We linearly blend between forcing the sampling network outputs towards 1 and 0 over the course of this phase, and additionally blend in a soft student-teacher regularization loss via an 1 -term that encourages the sampling network outputs \u03b4 to follow a similar distribution as \u03c3. From iteration t 0 over the duration of t d iterations (and the current iteration given as t c ), we define the sparsification loss as\nl sparsity (\u03b4, \u03c3) = \u03bb \u00b7 1 N N i=1 |\u03b4 i \u2212 0| + |\u03c3 i \u2212 \u03b4 i | + (1 \u2212 \u03bb) \u00b7 1 N N i=1 |\u03b4 i \u2212 1|, where \u03bb = t c \u2212 t 0 t d .(3)\nThe 1 -term |\u03c3 i \u2212\u03b4 i | ensures that the sampling network does not collapse to a single constant 0 or 1 vector, forcing the sampling network to follow the established scene representation of the shading network. The sparsification phase gradually increases the sparsity of the sampling network resulting in fewer significant outputs (which subsequently have zero contribution during ray accumulation).\n\nSparse Training. To allow the shading network to take advantage of the sparsification of the sampling network, we lock the sampling network's weights during sparse training. Although the shading network is still queried for all samples along each ray (as in the dense training phase) it is now free to alter the output for samples that are already dampened by the sampling network (due to the density multiplication). This enables the network to focus its capacity on those samples that actually contribute to the output.\n\nFine-tuning. We fine-tune the shading network for a desired maximum number of samples per ray; typically 2, 4, 8 or 16. This phase is fast, as the number of samples per ray is small. Fine-tuning can increase quality as it completely removes samples that hardly contribute to the final output and allows the shading network to focus on the contributing samples only. Note that this phase results in separate shading networks for each maximum sample count, while all rely on the same sampling network.\n\nReal-time Rendering with Adaptive Sampling. We can further improve performance by enabling variable sample counts per ray. This adaptive sampling scheme exploits the fact that AdaNeRF uses fixed sample locations along the ray that can at most contain exactly one sample. First, we add an adaptive sampling threshold \u03c4 that defines the cutoff point for the sampling network's predictions \u03b4. This enables us to save shading network evaluations in regions that do not require more than a few samples (such as a uniformly colored sky or simple geometric objects), which in turn increases the overall efficiency of our pipeline. Then, we limit the maximum number of allowed samples to N max , and distinguish between the following cases, depending on the number of sampling network predictions N s that exceed the threshold \u03c4 :\n\n1. N s = 0 : If no sampling network predictions \u03b4 i exceed \u03c4 , we place one sample at the center of the ray segment corresponding to the sampling network's largest prediction. 2. N s \u2264 N max : If the number of sampling network predictions \u03b4 i that exceed \u03c4 is at most N max , we place samples at the center of all of their segments. 3. N s > N max : If the number of sampling network predictions \u03b4 i that exceed \u03c4 is more than our maximum number of allowed samples N max , we place one sample each at the center of the N max largest predictions.\n\nThis adaptive sampling scheme can be efficiently implemented on GPUs using warp communication primitives, enabling further efficiency gains compared to typical importance sampling setups that first need to generate a cumulative distribution function from a probability density function. Note that our approach is the first neural representation that relies on volume integration and (1) can go down to a single sample per ray and (2) supports variable sample counts per ray without the need for a spatial data structure.\n\n\nEvaluation\n\nImplementation Details. We follow the network architecture of DONeRF [22], using MLPs consisting of 8 layers with 256 units for both the sampling and shading networks. For the DONeRF dataset, we logarithmically space samples along each ray and unify rays [22]. For the LLFF dataset, we sample in normalized device coordinates [20]. We use Adam [12] with a learning rate of 5e \u22124 in training. We configure our 4-phase training scheme (Section 3.2) in the following order: 25k iterations of dense training, 50k iterations sparsification, 225k iterations sparse training, and 300k iterations fine-tuning. We vary the adaptive sampling threshold \u03c4 in comparison to other baselines at similar average sample counts. As a starting point, for the MSE loss of the sampling network (Equ. 1, Equ. 2) we use \u03bb 0 = 0.001, and for the sparsity loss of the sampling network (Equ. 3, Equ. 2) we use \u03bb 1 = 1.0. Please refer to the supplementary material for per-scene loss weights that were found by grid search and used in our evaluation. Finally, for the real-time performance comparison, we implemented a custom real-time renderer using CUDA and TensorRT to take advantage of our adaptive sampling strategy. All results were evaluated on a single Nvidia RTX 3090.\n\n\nAblation Studies\n\nWe provide an ablation study to validate the design of our 4-phase training scheme (Section 3.2) and our adaptive sampling strategy (Section 3.2), averaged across the Pavillon scene of DONeRF dataset. The number of iterations for each phase was determined in small-scale experiments and could be further optimized for training speed or image quality. Training Scheme Tab. 1 shows the ablation of our training scheme. Without (2) dense training or (3) sparsification, we observe a minor degradation in quality. If dense training is skipped, the shading network provides less accurate information to the sampling network; if sparsification is skipped, the sampling network is abruptly forced to be sparse by switching from \"fully dense\" to \"fully sparse\" training immediately instead of blending between them, losing potentially important samples in the process. Removing the (4) density multiplication in the ray accumulation function (Equ. 1) results in the sampling network collapsing to a constant output-the 1 -loss as the only supervision signal is insufficient to stabilize the sampling network. Similarly, using (5) 1 -loss supervision from the shading network as the sole optimization criterion (Equ. 3) leads to the sampling network collapsing towards the mean density of all rays. Removing (6) the shading density supervision 1 -term from Equ. 3 still produces reasonable sampling networks, at a quality degradation due to the lack of additional supervision. Finally, removing (7) the sparse training directly fine-tunes after sparsification. The resulting shading networks are not adapted to the sparsified sampling networks, significantly reducing quality.\n\nAdaptive Sampling We sweep the threshold \u03c4 between [0.05, 0.40] and compare the resulting quality against fixed sample counts of N = [8,16], see Tab. 2.\n\nCompared to the fixed sample count of N = 8, the adaptive variant reaches similar quality between 5.07 and 6.16 samples per pixel, showing the increased efficiency even at lower sample counts. As average sample counts increase, the sampling network has much more freedom in placing the samples, and thus can outperform the quality of N = 16 fixed samples at just 7.76 samples.\n\n\nResults\n\nWe show a quantitative and qualitative evaluation of AdaNeRF on a variety of datasets against several baseline methods. We measure the quality of the rendered images in PSNR, and report the number of parameters required to store each method (using uncompressed 32-bit floating point) to evaluate compactness. We further present real-time rendering timings measured via TensorRT and CUDA. Please refer to the supplementary material for more visual comparisons, a discussion on training speed and a discussion on how to interleave multiple AdaNeRF for larger scenes.\n\nDatasets. We evaluate our method using the following datasets.\n\n-The DONeRF [22] dataset contains synthetic indoor and outdoor scenes of small to very large scales that are path-traced using Blender at a resolution of 800 \u00d7 800, with the cameras aimed at the forward hemisphere of their bounding box. -The LLFF [19] dataset contains forward-facing real-world scenes captured using a handheld camera, which we scale to a resolution of 1008 \u00d7 756. We follow the convention [19] of holding out every 8th image for testing.\n\nBaselines. Besides comparing to NeRF [20], we compare AdaNeRF to related work that focused on improving sampling efficiency and rendering performance: -DONeRF [22]: DONeRF uses a depth oracle network trained on depth maps to improve sampling efficiency. For all experiments, we train the oracle network using depth maps extracted from a pre-trained coarse NeRF. -TermiNeRF* [27]: TermiNeRF* learns a sampling network based on the density of a pre-trained NeRF. We follow the input encoding of DONeRF, and further use 128 fixed sample locations for the targets extracted from the pre-trained coarse NeRF, avoiding resampling and filtering of the targets. -AutoInt [14]: AutoInt learns automatic integration via a sampling network.\n\nWe compare AdaNeRF to AutoInt on a lower resolution version of the LLFF dataset, which was provided in the authors' original paper. -Plenoxels [44]: Plenoxels uses a sparse grid with trilinear interpolation to directly learn a scene representation via spherical harmonics, without neural networks. For unbounded scenes, Plenoxels uses a multi-sphere-image background model in combination with its sparse foreground grid model.  For both DONeRF and TermiNeRF*, we first train a coarse-only NeRF at 128 samples per ray with 8 hidden layers with 256 units each. We compare AdaNeRF to DONeRF and TermiNeRF* with fixed sample counts of N = [2,4,8,16]. For Plenoxels we use configurations provided by the authors: Plenoxels uses a sparse grid resolution of 256 3 . Plenoxels-MSI adds a background model with 64 layers. Plenoxels-Large uses the authors' provided checkpoints for the LLFF dataset, which are significantly more dense. For all baselines, we use the available open source code, and the authors' suggested settings unless otherwise specified.\n\nQuality We present average output quality, memory footprint and render times for the DONeRF and LLFF datasets in Table 3, and example outputs in Figure 3. Additional examples, per-scene data, depth reconstructions and sample placement visualizations can be found in the supplementary material.\n\nFor the DONeRF dataset, Instant-NGP-2 19 achieves the best quality, followed by all NeRF-based approaches with similar quality. Considering run-time, AdaN-eRF shows the best tradeoff, allowing to choose between very fast rendering (at 3.7 samples) and competitive quality or high-quality (at 7.0 samples) and 2\u00d7 speed improvement over DONeRF and TermiNeRF* at the same quality. AdaN-eRF only falls behind DONeRF and TermiNeRF* in image quality at extremely low sample counts while achieving greater speed improvement, suggesting that AdaNeRF operates most variably at a slightly higher sample counts. Considering memory foot footprint, AdaNeRF achieves equal or better quality than Plenoxels at a 48 \u2212 215\u00d7 reduction in memory and similar run-time. Instant-NGP-2 19 is similar to AdaNeRF considering all three tradeoffs: it achieves higher quality at a higher memory and run-time cost, or similar quality with lower memory but higher run-time. For the LLFF dataset, the highest quality is achieved by NeRF, Plenoxels-Large, and AdaNeRF at 10.2 samples. Compared to other samplingnetwork based approaches, AdaNeRF clearly outperforms the state-of-the-art, achieving better quality than TermiNeRF* at less than half the sample count and frame time. Again, considering memory footprint and performance, both NeRF and Plenoxels show significant drawbacks compared to AdaNeRF: Plenoxels requires 3.6 GB of memory for its representation and NeRF takes 2.8 seconds to render a single frame. Interestingly, Instant-NGP (2 19 and 2 14 ) perform worse than AdaNeRF for this data set. Compared to AutoInt, AdaNeRF achieves better quality at much faster rendering speeds. In summary, our adaptive fully neural representation shows state-of-the-art image quality at equal or better run-time performance and memory footprint, without requiring explicit data structures.\n\n\nReal-time Rendering Performance\n\nWe evaluate the real-time rendering performance of our AdaNeRF TensorRT and CUDA prototype against all baselines, see Table 3 (Columns \"Time\"). For all baselines except for Plenoxels, we evaluate their optimal rendering performance by computing the maximum throughput of identical networks in TensorRT, conservatively ignoring any additional input processing or differences in encoding. For Plenoxels, we evaluate the rendering performance using the authors' provided implementation.  Fig. 3: Details on four test scenes, showing that AdaNeRF is similar in quality to the significantly slower NeRF and outperforms TermiNeRF* at lower sample counts. While using 50\u00d7 more memory, Plenoxels tends to blur or leave out geometry due to lack of resolution in its grid. Instant-NGP achieves similar quality to AdaNeRF, while being slightly slower and requiring 16\u00d7 more memory. In terms of real-time rendering performance vs. memory footprint, AdaNeRF at a maximum sample count of 2 achieves the best trade-off, being able to render scenes at an average frame rate of 26 frames per second at a resolution of 1008 \u00d7 756. The increased efficiency compared to DONeRF and TermiNeRF* (at identical sample counts) comes from the optimized adaptive sampling kernels of AdaNeRF, which can lead to a massive speedup. At equal rendering performance, AdaNeRF achieves significantly better quality compared to previous samplingnetwork based approaches, such as DONeRF, TermiNeRF* and AutoInt on both the DONeRF and LLFF datasets. At equal or improved quality, AdaNeRF outperforms DONeRF by up to 5\u00d7, TermiNeRF* by up to 6\u00d7 and AutoInt by up to 7\u00d7. Compared to the densely sampled NeRF, our largest AdaNeRF shows a 20\u00d7 increase in run-time, and the smallest AdaNeRF outperforms NeRF by up to 74\u00d7. Compared to the highly optimized Instant-NGP, AdaNeRF achieves a comparable trade-off between quality and run-time, achieving up to a 2\u00d7 increase in run-time at equal quality. The best trade-off between quality and run-time is reached by Plenoxels, which represent their scene within a large sparse grid, with an additional optional multi-sphere image background model. Although this enables real-time rendering for single scenes, the immense memory requirements of up to multiple gigabytes prevent use-cases such as streaming or splitting complex environments into multiple representations.\n\nThe breakdown of frame time for the individual stages of AdaNeRF at a sample count of 2 is given as: 1.54 ms to generate the encoded inputs for the sampling network, 10.86 ms to evaluate the sampling network, 1.02 ms to generate the adaptively sampled inputs for the shading network, 18.16 ms to compute the shading network inference, and 0.38 ms for the final ray accumulation. Thus, our adaptive sampling kernels and overall pipeline exhibit only minor overheads compared to the inference workload, which constitutes most of the frame time. Furthermore, with higher average sample counts, the majority of additional compute load is added to the inference stages, and does not increase the overhead of input processing. Overall, AdaNeRF fills a gap in the performance-qualitymemory trifecta, being extremely fast at a compact memory footprint, at a low cost in image quality for certain scenes.\n\n\nLimitations and Future Work\n\nAlthough AdaNeRF already achieves promising results on real-world data, our evaluation does not optimize for camera parameters, and thus can suffer from input data that is not perfectly consistent. Especially for very low sample counts (see Tab. 3), getting precise surface information is crucial to achieve good quality, and adding an additional optimization step for camera parameters and/or consistency in lighting could further improve results. Second, the threshold for adaptive sampling, as well as the weights of the main optimization function (maximizing the sampling network's sparsity while preventing a collapse) influences the sparsity of AdaNeRF, affecting the overall quality and real-time performance. While these parameters can be fairly robustly applied across different datasets, a grid search is recommended for best performance. In the future, these parameters could be learned from data to save the time for hyperparameter tuning.\n\n\nConclusion\n\nWe have introduced AdaNeRF, a compact real-time dual-network neural representation that can be trained fully end-to-end via a soft student-teacher optimization scheme. It is the first of its kind to adaptively place a very low amount of samples for each individual ray. We significantly outperform previous work that utilized sampling networks for very low sample count neural representations. Due to the compact nature of our neural representation, we additionally showed how multiple models can be blended in overlapping regions, which opens the door for real-time rendering of dynamically streamed neural representations of complex environments. We believe that such a fully neural real-time representation can be a useful alternative to approaches that require explicit spatial data structures.\n\nFig. 2 :\n2The 4-phase training scheme of AdaNeRF.\n\nTable 1 :\n1Ablation of the 4-phase training scheme of AdaNeRF, using two maximum sample counts of N max =[2,4] on the Pavillon scene of the DONeRF dataset.Nmax = 2 \nNmax = 4 \nMethod \nN / Ray PSNR \u2191 N / Ray PSNR \u2191 \n\n1) AdaNeRF \n2.00 \n28.25 \n3.99 \n29.33 \n\n2) No dense training \n1.99 \n27.69 \n3.54 \n29.23 \n3) No sparsification blending \n1.66 \n27.33 \n2.75 \n28.44 \n4) No weight multiplication \n1.00 \n24.75 \n1.00 \n24.73 \n5) Only shading density supervision 1.00 \n24.72 \n1.00 \n24.73 \n6) No shading density supervision \n2.00 \n27.82 \n4.00 \n28.86 \n7) No sparse training \n2.00 \n21.38 \n4.00 \n28.39 \n\n\n\nTable 2 :\n2On the Pavillon scene of the DONeRF dataset, our adaptive sampling scheme manages to achieve higher quality at average sample counts of 6.16 and 7.76, compared to fixed sample counts of N =[8,16].Fixed \nAdaptive \n\nThreshold \u03c4 \n-\n-0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 \nSamples per Ray 16.00 8.00 12.10 11.89 11.63 11.03 9.73 7.76 6.16 5.07 \nPSNR\u2191 \n30.89 30.40 31.66 31.66 31.68 31.66 31.62 31.07 30.64 30.24 \n\n\n\nTable 3 :\n3Image quality, render time and memory footprint comparison on the DONeRF[22] and LLFF[19] datasets. Best results are displayed as Top 1 , Top 2 and Top 3 per category.DONeRF [22] Dataset \n(800 \u00d7 800) \n\nLLFF [19] Dataset \n(1008 \u00d7 756) \n\nLLFF [19] Dataset \n(504 \u00d7 378) \n\nMethod \n\nMemory \n[MB] \n\nSamples \nper Ray \n\nTime \n[ms]\u2193 \n\nQuality \nPSNR\u2191 \n\nSamples \nper Ray \n\nTime \n[ms]\u2193 \n\nQuality \nPSNR\u2191 \n\nSamples \nper Ray \n\nTime \n[ms]\u2193 \n\nQuality \nPSNR\u2191 \n\nAdaNeRF \n4.1 \n1.9 31.3 \n24.8 \n2.0 38.0 \n22.0 \n2.0 9.9 \n21.8 \nAdaNeRF \n4.1 \n3.7 48.2 \n27.5 \n3.9 58.9 \n24.0 \n3.9 15.2 \n23.3 \nAdaNeRF \n4.1 \n7.0 \n78.9 \n29.5 \n6.9 \n92.7 \n25.2 \n7.0 24.4 \n25.1 \nAdaNeRF \n4.1 \n12.6 130.6 \n30.8 \n10.2 129.6 \n25.7 \n10.6 36.1 \n26.2 \n\nDONeRF \n4.1 \n2.0 \n51.3 \n27.9 \n2.0 \n61.1 \n20.9 \n-\n-\n-\nDONeRF \n4.1 \n4.0 \n86.3 \n28.8 \n4.0 102.7 \n21.6 \n-\n-\n-\nDONeRF \n4.1 \n8.0 156.3 \n29.8 \n8.0 186.1 \n22.3 \n-\n-\n-\nDONeRF \n4.1 \n16.0 296.2 \n30.9 \n16.0 352.7 \n22.9 \n-\n-\n-\n\nTermiNeRF* \n4.1 \n2.0 \n51.3 \n27.2 \n2.0 \n61.1 \n21.7 \n-\n-\n-\nTermiNeRF* \n4.1 \n4.0 \n86.3 \n28.2 \n4.0 102.7 \n22.3 \n-\n-\n-\nTermiNeRF* \n4.1 \n8.0 156.3 \n29.2 \n8.0 186.1 \n23.0 \n-\n-\n-\nTermiNeRF* \n4.1 \n16.0 296.2 \n29.8 \n16.0 352.7 \n23.6 \n-\n-\n-\n\nNeRF \n3.8 \n256.0 2360.7 \n30.9 \n256.0 2810.9 \n26.5 \n-\n-\n-\n\nAutoInt \n4.5 \n-\n-\n-\n-\n-\n-\n16.0 44.6 \n24.1 \nAutoInt \n4.5 \n-\n-\n-\n-\n-\n-\n32.0 88.5 \n24.9 \nAutoInt \n4.5 \n-\n-\n-\n-\n-\n-\n64.0 176.4 \n25.5 \n\nPlenoxels \n198.7 \n-47.9 \n27.1 \n-51.3 \n24.3 \n-\n-\n-\nPlenoxels-MSI \n892.9 \n-47.5 \n29.6 \n-\n-\n-\n-\n-\n-\nPlenoxels-Large \n3629.8 \n-\n-\n-\n-110.1 \n26.3 \n-\n-\n-\n\nInstant-NGP-2 14 \n2.0 \n-102.1 \n29.4 \n-100.7 \n24.8 \n-\n-\n-\nInstant-NGP-2 19 \n64.0 \n-161.8 \n33.1 \n-137.0 \n25.6 \n-\n-\n-\n\n\n\nLearning neural light fields with ray-space embedding networks. B Attal, J Huang, M Zollh\u00f6fer, J Kopf, C Kim, CoRR abs/2112.01523Attal, B., Huang, J., Zollh\u00f6fer, M., Kopf, J., Kim, C.: Learning neural light fields with ray-space embedding networks. CoRR abs/2112.01523 (2021), https:// arxiv.org/abs/2112.01523\n\nJ T Barron, B Mildenhall, M Tancik, P Hedman, R Martin-Brualla, P P Srinivasan, Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. ICCV. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srinivasan, P.P.: Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. ICCV (2021)\n\nJ T Barron, B Mildenhall, D Verbin, P P Srinivasan, P Hedman, Mip-nerf 360: Unbounded anti-aliased neural radiance fields. arXiv. Barron, J.T., Mildenhall, B., Verbin, D., Srinivasan, P.P., Hedman, P.: Mip-nerf 360: Unbounded anti-aliased neural radiance fields. arXiv (2021)\n\nM Boss, R Braun, V Jampani, J T Barron, C Liu, H Lensch, NeRD: Neural reflectance decomposition from image collections. Boss, M., Braun, R., Jampani, V., Barron, J.T., Liu, C., Lensch, H.: NeRD: Neural reflectance decomposition from image collections. https://arxiv.org/abs/2012.03918 (2020)\n\nJ Chen, Y Zhang, D Kang, X Zhe, L Bao, X Jia, H Lu, Animatable neural radiance fields from monocular rgb videos. Chen, J., Zhang, Y., Kang, D., Zhe, X., Bao, L., Jia, X., Lu, H.: Animatable neural radiance fields from monocular rgb videos (2021)\n\nNeural radiance flow for 4d view synthesis and video processing. Y Du, Y Zhang, H X Yu, J B Tenenbaum, J Wu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionDu, Y., Zhang, Y., Yu, H.X., Tenenbaum, J.B., Wu, J.: Neural radiance flow for 4d view synthesis and video processing. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (2021)\n\nJ Fang, L Xie, X Wang, X Zhang, W Liu, Q Tian, arXiv:2111.15552Neusample: Neural sample field for efficient view synthesis. Fang, J., Xie, L., Wang, X., Zhang, X., Liu, W., Tian, Q.: Neusample: Neural sample field for efficient view synthesis. arXiv:2111.15552 (2021)\n\nDynamic view synthesis from dynamic monocular video. C Gao, A Saraf, J Kopf, J B Huang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionGao, C., Saraf, A., Kopf, J., Huang, J.B.: Dynamic view synthesis from dynamic monocular video. In: Proceedings of the IEEE International Conference on Computer Vision (2021)\n\nFastnerf: Highfidelity neural rendering at 200fps. S J Garbin, M Kowalski, M Johnson, J Shotton, J Valentin, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Garbin, S.J., Kowalski, M., Johnson, M., Shotton, J., Valentin, J.: Fastnerf: High- fidelity neural rendering at 200fps. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 14346-14355 (October 2021)\n\nP Hedman, P P Srinivasan, B Mildenhall, J T Barron, P Debevec, Baking neural radiance fields for real-time view synthesis. arXiv. Hedman, P., Srinivasan, P.P., Mildenhall, B., Barron, J.T., Debevec, P.: Baking neural radiance fields for real-time view synthesis. arXiv (2021)\n\nEditable free-viewpoint video using a layered neural representation. Z Jiakai, L Xinhang, Y Xinyi, Z Fuqiang, Z Yanshun, W Minye, Z Yingliang, X Lan, Y Jingyi, ACM SIGGRAPH. Jiakai, Z., Xinhang, L., Xinyi, Y., Fuqiang, Z., Yanshun, Z., Minye, W., Yingliang, Z., Lan, X., Jingyi, Y.: Editable free-viewpoint video using a layered neural repre- sentation. In: ACM SIGGRAPH (2021)\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, ICLR (Poster. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR (Poster) (2015)\n\nT Li, M Slavcheva, M Zollhoefer, S Green, C Lassner, C Kim, T Schmidt, S Lovegrove, M Goesele, Z Lv, Neural 3d video synthesis. Li, T., Slavcheva, M., Zollhoefer, M., Green, S., Lassner, C., Kim, C., Schmidt, T., Lovegrove, S., Goesele, M., Lv, Z.: Neural 3d video synthesis (2021)\n\nAutoint: Automatic integration for fast neural volume rendering. D B Lindell, J N Martel, G Wetzstein, Lindell, D.B., Martel, J.N., Wetzstein, G.: Autoint: Automatic integration for fast neural volume rendering (2021)\n\nNeural sparse voxel fields. L Liu, J Gu, K Zaw Lin, T S Chua, C Theobalt, Advances in Neural Information Processing Systems. 33Liu, L., Gu, J., Zaw Lin, K., Chua, T.S., Theobalt, C.: Neural sparse voxel fields. Advances in Neural Information Processing Systems 33 (2020)\n\nNeural actor: Neural free-view synthesis of human actors with pose control. L Liu, M Habermann, V Rudnev, K Sarkar, J Gu, C Theobalt, ACM SIGGRAPH Asia. Liu, L., Habermann, M., Rudnev, V., Sarkar, K., Gu, J., Theobalt, C.: Neural actor: Neural free-view synthesis of human actors with pose control. ACM SIGGRAPH Asia (2021)\n\nS Lombardi, T Simon, G Schwartz, M Zollhoefer, Y Sheikh, J Saragih, Mixture of volumetric primitives for efficient neural rendering. Lombardi, S., Simon, T., Schwartz, G., Zollhoefer, M., Sheikh, Y., Saragih, J.: Mixture of volumetric primitives for efficient neural rendering (2021)\n\nNerf in the wild: Neural radiance fields for unconstrained photo collections. R Martin-Brualla, N Radwan, M S M Sajjadi, J T Barron, A Dosovitskiy, D Duckworth, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Martin-Brualla, R., Radwan, N., Sajjadi, M.S.M., Barron, J.T., Dosovitskiy, A., Duckworth, D.: Nerf in the wild: Neural radiance fields for unconstrained photo collections. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7210-7219 (June 2021)\n\nLocal light field fusion: Practical view synthesis with prescriptive sampling guidelines. B Mildenhall, P P Srinivasan, R Ortiz-Cayon, N K Kalantari, R Ramamoorthi, R Ng, A Kar, ACM Transactions on Graphics. Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi, R., Ng, R., Kar, A.: Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG) (2019)\n\nNerf: Representing scenes as neural radiance fields for view synthesis. B Mildenhall, P P Srinivasan, M Tancik, J T Barron, R Ramamoorthi, R Ng, ECCVMildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: ECCV (2020)\n\nT M\u00fcller, A Evans, C Schied, A Keller, arXiv:2201.05989Instant neural graphics primitives with a multiresolution hash encoding. M\u00fcller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives with a multiresolution hash encoding. arXiv:2201.05989 (Jan 2022)\n\nDONeRF: Towards real-time rendering of compact neural radiance fields using depth oracle networks. Neff, Stadlbauer, Parger, Kurz, Mueller, Chaitanya, Kaplanyan, Steinberger, Computer Graphics Forum. 404Neff, Stadlbauer, Parger, Kurz, Mueller, Chaitanya, Kaplanyan, Steinberger: DONeRF: Towards real-time rendering of compact neural radiance fields using depth oracle networks. Computer Graphics Forum 40(4) (2021).\n\n. 10.1111/cgf.14340https://doi.org/10.1111/cgf.14340, https://doi.org/10.1111/cgf.14340\n\nUnisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. M Oechsle, S Peng, A Geiger, International Conference on Computer Vision (ICCV). Oechsle, M., Peng, S., Geiger, A.: Unisurf: Unifying neural implicit surfaces and radi- ance fields for multi-view reconstruction. In: International Conference on Computer Vision (ICCV) (2021)\n\nK Park, U Sinha, J T Barron, S Bouaziz, D B Goldman, S M Seitz, R Martin-Brualla, Nerfies: Deformable neural radiance fields. ICCV. Park, K., Sinha, U., Barron, J.T., Bouaziz, S., Goldman, D.B., Seitz, S.M., Martin- Brualla, R.: Nerfies: Deformable neural radiance fields. ICCV (2021)\n\nK Park, U Sinha, P Hedman, J T Barron, S Bouaziz, D B Goldman, R Martin-Brualla, S M Seitz, arXiv:2106.13228Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields. arXiv preprintPark, K., Sinha, U., Hedman, P., Barron, J.T., Bouaziz, S., Goldman, D.B., Martin- Brualla, R., Seitz, S.M.: Hypernerf: A higher-dimensional representation for topo- logically varying neural radiance fields. arXiv preprint arXiv:2106.13228 (2021)\n\nS Peng, J Dong, Q Wang, S Zhang, Q Shuai, H Bao, X Zhou, arXiv:2105.02872Animatable neural radiance fields for human body modeling. arXiv preprintPeng, S., Dong, J., Wang, Q., Zhang, S., Shuai, Q., Bao, H., Zhou, X.: Animatable neural radiance fields for human body modeling. arXiv preprint arXiv:2105.02872 (2021)\n\nTerminerf: Ray termination prediction for efficient neural rendering. M Piala, R Clark, https:/doi.ieeecomputersociety.org/10.1109/3DV53792.2021.001182021 International Conference on 3D Vision (3DV). Los Alamitos, CA, USAIEEE Computer SocietyPiala, M., Clark, R.: Terminerf: Ray termination prediction for effi- cient neural rendering. In: 2021 International Conference on 3D Vi- sion (3DV). pp. 1106-1114. IEEE Computer Society, Los Alamitos, CA, USA (dec 2021). https://doi.org/10.1109/3DV53792.2021.00118, https://doi. ieeecomputersociety.org/10.1109/3DV53792.2021.00118\n\nD-NeRF: Neural radiance fields for dynamic scenes. A Pumarola, E Corona, G Pons-Moll, F Moreno-Noguer, Pumarola, A., Corona, E., Pons-Moll, G., Moreno-Noguer, F.: D-NeRF: Neural radiance fields for dynamic scenes. https://arxiv.org/abs/2011.13961 (2020)\n\nD-nerf: Neural radiance fields for dynamic scenes. A Pumarola, E Corona, G Pons-Moll, F Moreno-Noguer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Pumarola, A., Corona, E., Pons-Moll, G., Moreno-Noguer, F.: D-nerf: Neural radiance fields for dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10318-10327 (June 2021)\n\nD Rebain, W Jiang, S Yazdani, K Li, K M Yi, A Tagliasacchi, DeRF: Decomposed radiance fields. Rebain, D., Jiang, W., Yazdani, S., Li, K., Yi, K.M., Tagliasacchi, A.: DeRF: Decomposed radiance fields. https://arxiv.org/abs/2011.12490 (2020)\n\nDerf: Decomposed radiance fields. D Rebain, W Jiang, S Yazdani, K Li, K M Yi, A Tagliasacchi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Rebain, D., Jiang, W., Yazdani, S., Li, K., Yi, K.M., Tagliasacchi, A.: Derf: Decom- posed radiance fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 14153-14161 (June 2021)\n\nC Reiser, S Peng, Y Liao, A Geiger, Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. Reiser, C., Peng, S., Liao, Y., Geiger, A.: Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps (2021)\n\nLight field networks: Neural scene representations with single-evaluation rendering. V Sitzmann, S Rezchikov, W T Freeman, J B Tenenbaum, F Durand, Proc. NeurIPS. NeurIPSSitzmann, V., Rezchikov, S., Freeman, W.T., Tenenbaum, J.B., Durand, F.: Light field networks: Neural scene representations with single-evaluation rendering. In: Proc. NeurIPS (2021)\n\nP Srinivasan, B Deng, X Zhang, M Tancik, B Mildenhall, J T Barron, NeRV: Neural reflectance and visibility fields for relighting and view synthesis. Srinivasan, P., Deng, B., Zhang, X., Tancik, M., Mildenhall, B., Barron, J.T.: NeRV: Neural reflectance and visibility fields for relighting and view synthesis. https://arxiv.org/abs/2012.03927 (2020)\n\nT Takikawa, J Litalien, K Yin, K Kreis, C Loop, D Nowrouzezahrai, A Jacobson, M Mcguire, S Fidler, Neural geometric level of detail: Real-time rendering with implicit 3D shapes. Takikawa, T., Litalien, J., Yin, K., Kreis, K., Loop, C., Nowrouzezahrai, D., Ja- cobson, A., McGuire, M., Fidler, S.: Neural geometric level of detail: Real-time rendering with implicit 3D shapes (2021)\n\nM Tancik, V Casser, X Yan, S Pradhan, B Mildenhall, P Srinivasan, J T Barron, H Kretzschmar, Block-NeRF: Scalable large scene neural view synthesis. arXiv. Tancik, M., Casser, V., Yan, X., Pradhan, S., Mildenhall, B., Srinivasan, P., Barron, J.T., Kretzschmar, H.: Block-NeRF: Scalable large scene neural view synthesis. arXiv (2022)\n\nNonrigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video. E Tretschk, A Tewari, V Golyanik, M Zollh\u00f6fer, C Lassner, C Theobalt, IEEE International Conference on Computer Vision (ICCV). IEEETretschk, E., Tewari, A., Golyanik, V., Zollh\u00f6fer, M., Lassner, C., Theobalt, C.: Non- rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video. In: IEEE International Conference on Computer Vision (ICCV). IEEE (2021)\n\nH Turki, D Ramanan, M Satyanarayanan, Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. Turki, H., Ramanan, D., Satyanarayanan, M.: Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs (2021)\n\nNeus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. P Wang, L Liu, Y Liu, C Theobalt, T Komura, W Wang, NeurIPS. Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS (2021)\n\nSpace-time neural irradiance fields for free-viewpoint video. W Xian, J B Huang, J Kopf, C Kim, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXian, W., Huang, J.B., Kopf, J., Kim, C.: Space-time neural irradiance fields for free-viewpoint video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021)\n\nC Xie, K Park, R Martin-Brualla, M Brown, Fig-nerf: Figure-ground neural radiance fields for 3d object category modelling. Xie, C., Park, K., Martin-Brualla, R., Brown, M.: Fig-nerf: Figure-ground neural radiance fields for 3d object category modelling (2021)\n\nLearning object-compositional neural radiance field for editable scene rendering. B Yang, Y Zhang, Y Xu, Y Li, H Zhou, H Bao, G Zhang, Z Cui, International Conference on Computer Vision (ICCV). Yang, B., Zhang, Y., Xu, Y., Li, Y., Zhou, H., Bao, H., Zhang, G., Cui, Z.: Learning object-compositional neural radiance field for editable scene rendering. In: International Conference on Computer Vision (ICCV) (October 2021)\n\nVolume rendering of neural implicit surfaces. L Yariv, J Gu, Y Kasten, Y Lipman, NeurIPS. Yariv, L., Gu, J., Kasten, Y., Lipman, Y.: Volume rendering of neural implicit surfaces. NeurIPS (2021)\n\nA Yu, S Fridovich-Keil, M Tancik, Q Chen, B Recht, A Kanazawa, Plenoxels: Radiance fields without neural networks. Yu, A., Fridovich-Keil, S., Tancik, M., Chen, Q., Recht, B., Kanazawa, A.: Plenoxels: Radiance fields without neural networks (2021)\n\nPlenOctrees for real-time rendering of neural radiance fields. A Yu, R Li, M Tancik, H Li, R Ng, A Kanazawa, ICCVYu, A., Li, R., Tancik, M., Li, H., Ng, R., Kanazawa, A.: PlenOctrees for real-time rendering of neural radiance fields. In: ICCV (2021)\n\nK Zhang, G Riegler, N Snavely, V Koltun, Nerf++: Analyzing and improving neural radiance fields. Zhang, K., Riegler, G., Snavely, N., Koltun, V.: Nerf++: Analyzing and improving neural radiance fields (2020)\n", "annotations": {"author": "[{\"end\":132,\"start\":80},{\"end\":184,\"start\":133},{\"end\":224,\"start\":185},{\"end\":270,\"start\":225},{\"end\":329,\"start\":271}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":88},{\"end\":144,\"start\":140},{\"end\":196,\"start\":194},{\"end\":242,\"start\":233},{\"end\":289,\"start\":278}]", "author_first_name": "[{\"end\":87,\"start\":80},{\"end\":139,\"start\":133},{\"end\":193,\"start\":185},{\"end\":232,\"start\":225},{\"end\":277,\"start\":271}]", "author_affiliation": "[{\"end\":131,\"start\":94},{\"end\":183,\"start\":146},{\"end\":223,\"start\":198},{\"end\":269,\"start\":244},{\"end\":328,\"start\":291}]", "title": "[{\"end\":77,\"start\":1},{\"end\":406,\"start\":330}]", "venue": null, "abstract": "[{\"end\":1859,\"start\":460}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1922,\"start\":1918},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2064,\"start\":2060},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2080,\"start\":2076},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2083,\"start\":2080},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2086,\"start\":2083},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2089,\"start\":2086},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2105,\"start\":2101},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2108,\"start\":2105},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2134,\"start\":2130},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2284,\"start\":2281},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2305,\"start\":2301},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2308,\"start\":2305},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2340,\"start\":2336},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2343,\"start\":2340},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2346,\"start\":2343},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2348,\"start\":2346},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2351,\"start\":2348},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2354,\"start\":2351},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2357,\"start\":2354},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3809,\"start\":3805},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3811,\"start\":3809},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3823,\"start\":3819},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3843,\"start\":3839},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4214,\"start\":4210},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4217,\"start\":4214},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4220,\"start\":4217},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4298,\"start\":4294},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4355,\"start\":4351},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4547,\"start\":4543},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4746,\"start\":4742},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7549,\"start\":7545},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7683,\"start\":7680},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7685,\"start\":7683},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7688,\"start\":7685},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7691,\"start\":7688},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7694,\"start\":7691},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7697,\"start\":7694},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7700,\"start\":7697},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7703,\"start\":7700},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7706,\"start\":7703},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7742,\"start\":7738},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7745,\"start\":7742},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7747,\"start\":7745},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7750,\"start\":7747},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7753,\"start\":7750},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7779,\"start\":7775},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7781,\"start\":7779},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7813,\"start\":7809},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7816,\"start\":7813},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7819,\"start\":7816},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7822,\"start\":7819},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7999,\"start\":7996},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8147,\"start\":8143},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8180,\"start\":8176},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8182,\"start\":8180},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8848,\"start\":8844},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8851,\"start\":8848},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8853,\"start\":8851},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8856,\"start\":8853},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8885,\"start\":8881},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8904,\"start\":8900},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9037,\"start\":9033},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9040,\"start\":9037},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9113,\"start\":9109},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9135,\"start\":9131},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9296,\"start\":9293},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9331,\"start\":9327},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9334,\"start\":9331},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9386,\"start\":9382},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9416,\"start\":9412},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9673,\"start\":9669},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10166,\"start\":10162},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10555,\"start\":10551},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10956,\"start\":10952},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11338,\"start\":11334},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11340,\"start\":11338},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11803,\"start\":11799},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11819,\"start\":11815},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11837,\"start\":11833},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13552,\"start\":13548},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13607,\"start\":13603},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13610,\"start\":13607},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13666,\"start\":13662},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20724,\"start\":20720},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20910,\"start\":20906},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20981,\"start\":20977},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20999,\"start\":20995},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23727,\"start\":23724},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23730,\"start\":23727},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24779,\"start\":24775},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25014,\"start\":25010},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25174,\"start\":25170},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25261,\"start\":25257},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25383,\"start\":25379},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25598,\"start\":25594},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25887,\"start\":25883},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26098,\"start\":26094},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26589,\"start\":26586},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26591,\"start\":26589},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26593,\"start\":26591},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26596,\"start\":26593},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27335,\"start\":27333},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34408,\"start\":34405},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34410,\"start\":34408},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35092,\"start\":35089},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35095,\"start\":35092},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35397,\"start\":35393},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":35410,\"start\":35406}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34298,\"start\":34248},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34887,\"start\":34299},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35308,\"start\":34888},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36919,\"start\":35309}]", "paragraph": "[{\"end\":2468,\"start\":1875},{\"end\":3403,\"start\":2470},{\"end\":4966,\"start\":3405},{\"end\":6350,\"start\":4968},{\"end\":6877,\"start\":6352},{\"end\":7497,\"start\":6879},{\"end\":8757,\"start\":7514},{\"end\":9217,\"start\":8759},{\"end\":9978,\"start\":9219},{\"end\":11760,\"start\":9980},{\"end\":12145,\"start\":11762},{\"end\":13024,\"start\":12156},{\"end\":13611,\"start\":13066},{\"end\":13749,\"start\":13613},{\"end\":14888,\"start\":13849},{\"end\":15395,\"start\":14938},{\"end\":15847,\"start\":15397},{\"end\":16083,\"start\":15923},{\"end\":16639,\"start\":16085},{\"end\":17196,\"start\":16641},{\"end\":17719,\"start\":17318},{\"end\":18242,\"start\":17721},{\"end\":18743,\"start\":18244},{\"end\":19567,\"start\":18745},{\"end\":20114,\"start\":19569},{\"end\":20636,\"start\":20116},{\"end\":21901,\"start\":20651},{\"end\":23589,\"start\":21922},{\"end\":23743,\"start\":23591},{\"end\":24121,\"start\":23745},{\"end\":24697,\"start\":24133},{\"end\":24761,\"start\":24699},{\"end\":25218,\"start\":24763},{\"end\":25949,\"start\":25220},{\"end\":26998,\"start\":25951},{\"end\":27293,\"start\":27000},{\"end\":29150,\"start\":27295},{\"end\":31554,\"start\":29186},{\"end\":32451,\"start\":31556},{\"end\":33434,\"start\":32483},{\"end\":34247,\"start\":33449}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13848,\"start\":13750},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15922,\"start\":15848},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17317,\"start\":17197}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27120,\"start\":27113},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29311,\"start\":29304},{\"end\":32728,\"start\":32724}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1873,\"start\":1861},{\"attributes\":{\"n\":\"2\"},\"end\":7512,\"start\":7500},{\"attributes\":{\"n\":\"3\"},\"end\":12154,\"start\":12148},{\"attributes\":{\"n\":\"3.1\"},\"end\":13064,\"start\":13027},{\"attributes\":{\"n\":\"3.2\"},\"end\":14936,\"start\":14891},{\"attributes\":{\"n\":\"4\"},\"end\":20649,\"start\":20639},{\"attributes\":{\"n\":\"4.1\"},\"end\":21920,\"start\":21904},{\"attributes\":{\"n\":\"4.2\"},\"end\":24131,\"start\":24124},{\"end\":29184,\"start\":29153},{\"attributes\":{\"n\":\"5\"},\"end\":32481,\"start\":32454},{\"attributes\":{\"n\":\"6\"},\"end\":33447,\"start\":33437},{\"end\":34257,\"start\":34249},{\"end\":34309,\"start\":34300},{\"end\":34898,\"start\":34889},{\"end\":35319,\"start\":35310}]", "table": "[{\"end\":34887,\"start\":34455},{\"end\":35308,\"start\":35096},{\"end\":36919,\"start\":35488}]", "figure_caption": "[{\"end\":34298,\"start\":34259},{\"end\":34455,\"start\":34311},{\"end\":35096,\"start\":34900},{\"end\":35488,\"start\":35321}]", "figure_ref": "[{\"end\":2557,\"start\":2551},{\"end\":12979,\"start\":12971},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16048,\"start\":16042},{\"end\":27153,\"start\":27145},{\"end\":29677,\"start\":29671}]", "bib_author_first_name": "[{\"end\":36986,\"start\":36985},{\"end\":36995,\"start\":36994},{\"end\":37004,\"start\":37003},{\"end\":37017,\"start\":37016},{\"end\":37025,\"start\":37024},{\"end\":37234,\"start\":37233},{\"end\":37236,\"start\":37235},{\"end\":37246,\"start\":37245},{\"end\":37260,\"start\":37259},{\"end\":37270,\"start\":37269},{\"end\":37280,\"start\":37279},{\"end\":37298,\"start\":37297},{\"end\":37300,\"start\":37299},{\"end\":37585,\"start\":37584},{\"end\":37587,\"start\":37586},{\"end\":37597,\"start\":37596},{\"end\":37611,\"start\":37610},{\"end\":37621,\"start\":37620},{\"end\":37623,\"start\":37622},{\"end\":37637,\"start\":37636},{\"end\":37862,\"start\":37861},{\"end\":37870,\"start\":37869},{\"end\":37879,\"start\":37878},{\"end\":37890,\"start\":37889},{\"end\":37892,\"start\":37891},{\"end\":37902,\"start\":37901},{\"end\":37909,\"start\":37908},{\"end\":38155,\"start\":38154},{\"end\":38163,\"start\":38162},{\"end\":38172,\"start\":38171},{\"end\":38180,\"start\":38179},{\"end\":38187,\"start\":38186},{\"end\":38194,\"start\":38193},{\"end\":38201,\"start\":38200},{\"end\":38467,\"start\":38466},{\"end\":38473,\"start\":38472},{\"end\":38482,\"start\":38481},{\"end\":38484,\"start\":38483},{\"end\":38490,\"start\":38489},{\"end\":38492,\"start\":38491},{\"end\":38505,\"start\":38504},{\"end\":38843,\"start\":38842},{\"end\":38851,\"start\":38850},{\"end\":38858,\"start\":38857},{\"end\":38866,\"start\":38865},{\"end\":38875,\"start\":38874},{\"end\":38882,\"start\":38881},{\"end\":39165,\"start\":39164},{\"end\":39172,\"start\":39171},{\"end\":39181,\"start\":39180},{\"end\":39189,\"start\":39188},{\"end\":39191,\"start\":39190},{\"end\":39548,\"start\":39547},{\"end\":39550,\"start\":39549},{\"end\":39560,\"start\":39559},{\"end\":39572,\"start\":39571},{\"end\":39583,\"start\":39582},{\"end\":39594,\"start\":39593},{\"end\":39986,\"start\":39985},{\"end\":39996,\"start\":39995},{\"end\":39998,\"start\":39997},{\"end\":40012,\"start\":40011},{\"end\":40026,\"start\":40025},{\"end\":40028,\"start\":40027},{\"end\":40038,\"start\":40037},{\"end\":40332,\"start\":40331},{\"end\":40342,\"start\":40341},{\"end\":40353,\"start\":40352},{\"end\":40362,\"start\":40361},{\"end\":40373,\"start\":40372},{\"end\":40384,\"start\":40383},{\"end\":40393,\"start\":40392},{\"end\":40406,\"start\":40405},{\"end\":40413,\"start\":40412},{\"end\":40686,\"start\":40685},{\"end\":40688,\"start\":40687},{\"end\":40698,\"start\":40697},{\"end\":40810,\"start\":40809},{\"end\":40816,\"start\":40815},{\"end\":40829,\"start\":40828},{\"end\":40843,\"start\":40842},{\"end\":40852,\"start\":40851},{\"end\":40863,\"start\":40862},{\"end\":40870,\"start\":40869},{\"end\":40881,\"start\":40880},{\"end\":40894,\"start\":40893},{\"end\":40905,\"start\":40904},{\"end\":41158,\"start\":41157},{\"end\":41160,\"start\":41159},{\"end\":41171,\"start\":41170},{\"end\":41173,\"start\":41172},{\"end\":41183,\"start\":41182},{\"end\":41340,\"start\":41339},{\"end\":41347,\"start\":41346},{\"end\":41353,\"start\":41352},{\"end\":41364,\"start\":41363},{\"end\":41366,\"start\":41365},{\"end\":41374,\"start\":41373},{\"end\":41660,\"start\":41659},{\"end\":41667,\"start\":41666},{\"end\":41680,\"start\":41679},{\"end\":41690,\"start\":41689},{\"end\":41700,\"start\":41699},{\"end\":41706,\"start\":41705},{\"end\":41909,\"start\":41908},{\"end\":41921,\"start\":41920},{\"end\":41930,\"start\":41929},{\"end\":41942,\"start\":41941},{\"end\":41956,\"start\":41955},{\"end\":41966,\"start\":41965},{\"end\":42272,\"start\":42271},{\"end\":42290,\"start\":42289},{\"end\":42300,\"start\":42299},{\"end\":42304,\"start\":42301},{\"end\":42315,\"start\":42314},{\"end\":42317,\"start\":42316},{\"end\":42327,\"start\":42326},{\"end\":42342,\"start\":42341},{\"end\":42902,\"start\":42901},{\"end\":42916,\"start\":42915},{\"end\":42918,\"start\":42917},{\"end\":42932,\"start\":42931},{\"end\":42947,\"start\":42946},{\"end\":42949,\"start\":42948},{\"end\":42962,\"start\":42961},{\"end\":42977,\"start\":42976},{\"end\":42983,\"start\":42982},{\"end\":43327,\"start\":43326},{\"end\":43341,\"start\":43340},{\"end\":43343,\"start\":43342},{\"end\":43357,\"start\":43356},{\"end\":43367,\"start\":43366},{\"end\":43369,\"start\":43368},{\"end\":43379,\"start\":43378},{\"end\":43394,\"start\":43393},{\"end\":43578,\"start\":43577},{\"end\":43588,\"start\":43587},{\"end\":43597,\"start\":43596},{\"end\":43607,\"start\":43606},{\"end\":44455,\"start\":44454},{\"end\":44466,\"start\":44465},{\"end\":44474,\"start\":44473},{\"end\":44730,\"start\":44729},{\"end\":44738,\"start\":44737},{\"end\":44747,\"start\":44746},{\"end\":44749,\"start\":44748},{\"end\":44759,\"start\":44758},{\"end\":44770,\"start\":44769},{\"end\":44772,\"start\":44771},{\"end\":44783,\"start\":44782},{\"end\":44785,\"start\":44784},{\"end\":44794,\"start\":44793},{\"end\":45016,\"start\":45015},{\"end\":45024,\"start\":45023},{\"end\":45033,\"start\":45032},{\"end\":45043,\"start\":45042},{\"end\":45045,\"start\":45044},{\"end\":45055,\"start\":45054},{\"end\":45066,\"start\":45065},{\"end\":45068,\"start\":45067},{\"end\":45079,\"start\":45078},{\"end\":45097,\"start\":45096},{\"end\":45099,\"start\":45098},{\"end\":45483,\"start\":45482},{\"end\":45491,\"start\":45490},{\"end\":45499,\"start\":45498},{\"end\":45507,\"start\":45506},{\"end\":45516,\"start\":45515},{\"end\":45525,\"start\":45524},{\"end\":45532,\"start\":45531},{\"end\":45869,\"start\":45868},{\"end\":45878,\"start\":45877},{\"end\":46425,\"start\":46424},{\"end\":46437,\"start\":46436},{\"end\":46447,\"start\":46446},{\"end\":46460,\"start\":46459},{\"end\":46680,\"start\":46679},{\"end\":46692,\"start\":46691},{\"end\":46702,\"start\":46701},{\"end\":46715,\"start\":46714},{\"end\":47129,\"start\":47128},{\"end\":47139,\"start\":47138},{\"end\":47148,\"start\":47147},{\"end\":47159,\"start\":47158},{\"end\":47165,\"start\":47164},{\"end\":47167,\"start\":47166},{\"end\":47173,\"start\":47172},{\"end\":47404,\"start\":47403},{\"end\":47414,\"start\":47413},{\"end\":47423,\"start\":47422},{\"end\":47434,\"start\":47433},{\"end\":47440,\"start\":47439},{\"end\":47442,\"start\":47441},{\"end\":47448,\"start\":47447},{\"end\":47858,\"start\":47857},{\"end\":47868,\"start\":47867},{\"end\":47876,\"start\":47875},{\"end\":47884,\"start\":47883},{\"end\":48178,\"start\":48177},{\"end\":48190,\"start\":48189},{\"end\":48203,\"start\":48202},{\"end\":48205,\"start\":48204},{\"end\":48216,\"start\":48215},{\"end\":48218,\"start\":48217},{\"end\":48231,\"start\":48230},{\"end\":48447,\"start\":48446},{\"end\":48461,\"start\":48460},{\"end\":48469,\"start\":48468},{\"end\":48478,\"start\":48477},{\"end\":48488,\"start\":48487},{\"end\":48502,\"start\":48501},{\"end\":48504,\"start\":48503},{\"end\":48798,\"start\":48797},{\"end\":48810,\"start\":48809},{\"end\":48822,\"start\":48821},{\"end\":48829,\"start\":48828},{\"end\":48838,\"start\":48837},{\"end\":48846,\"start\":48845},{\"end\":48864,\"start\":48863},{\"end\":48876,\"start\":48875},{\"end\":48887,\"start\":48886},{\"end\":49181,\"start\":49180},{\"end\":49191,\"start\":49190},{\"end\":49201,\"start\":49200},{\"end\":49208,\"start\":49207},{\"end\":49219,\"start\":49218},{\"end\":49233,\"start\":49232},{\"end\":49247,\"start\":49246},{\"end\":49249,\"start\":49248},{\"end\":49259,\"start\":49258},{\"end\":49630,\"start\":49629},{\"end\":49642,\"start\":49641},{\"end\":49652,\"start\":49651},{\"end\":49664,\"start\":49663},{\"end\":49677,\"start\":49676},{\"end\":49688,\"start\":49687},{\"end\":50033,\"start\":50032},{\"end\":50042,\"start\":50041},{\"end\":50053,\"start\":50052},{\"end\":50373,\"start\":50372},{\"end\":50381,\"start\":50380},{\"end\":50388,\"start\":50387},{\"end\":50395,\"start\":50394},{\"end\":50407,\"start\":50406},{\"end\":50417,\"start\":50416},{\"end\":50667,\"start\":50666},{\"end\":50675,\"start\":50674},{\"end\":50677,\"start\":50676},{\"end\":50686,\"start\":50685},{\"end\":50694,\"start\":50693},{\"end\":51048,\"start\":51047},{\"end\":51055,\"start\":51054},{\"end\":51063,\"start\":51062},{\"end\":51081,\"start\":51080},{\"end\":51391,\"start\":51390},{\"end\":51399,\"start\":51398},{\"end\":51408,\"start\":51407},{\"end\":51414,\"start\":51413},{\"end\":51420,\"start\":51419},{\"end\":51428,\"start\":51427},{\"end\":51435,\"start\":51434},{\"end\":51444,\"start\":51443},{\"end\":51778,\"start\":51777},{\"end\":51787,\"start\":51786},{\"end\":51793,\"start\":51792},{\"end\":51803,\"start\":51802},{\"end\":51927,\"start\":51926},{\"end\":51933,\"start\":51932},{\"end\":51951,\"start\":51950},{\"end\":51961,\"start\":51960},{\"end\":51969,\"start\":51968},{\"end\":51978,\"start\":51977},{\"end\":52239,\"start\":52238},{\"end\":52245,\"start\":52244},{\"end\":52251,\"start\":52250},{\"end\":52261,\"start\":52260},{\"end\":52267,\"start\":52266},{\"end\":52273,\"start\":52272},{\"end\":52427,\"start\":52426},{\"end\":52436,\"start\":52435},{\"end\":52447,\"start\":52446},{\"end\":52458,\"start\":52457}]", "bib_author_last_name": "[{\"end\":36992,\"start\":36987},{\"end\":37001,\"start\":36996},{\"end\":37014,\"start\":37005},{\"end\":37022,\"start\":37018},{\"end\":37029,\"start\":37026},{\"end\":37243,\"start\":37237},{\"end\":37257,\"start\":37247},{\"end\":37267,\"start\":37261},{\"end\":37277,\"start\":37271},{\"end\":37295,\"start\":37281},{\"end\":37311,\"start\":37301},{\"end\":37594,\"start\":37588},{\"end\":37608,\"start\":37598},{\"end\":37618,\"start\":37612},{\"end\":37634,\"start\":37624},{\"end\":37644,\"start\":37638},{\"end\":37867,\"start\":37863},{\"end\":37876,\"start\":37871},{\"end\":37887,\"start\":37880},{\"end\":37899,\"start\":37893},{\"end\":37906,\"start\":37903},{\"end\":37916,\"start\":37910},{\"end\":38160,\"start\":38156},{\"end\":38169,\"start\":38164},{\"end\":38177,\"start\":38173},{\"end\":38184,\"start\":38181},{\"end\":38191,\"start\":38188},{\"end\":38198,\"start\":38195},{\"end\":38204,\"start\":38202},{\"end\":38470,\"start\":38468},{\"end\":38479,\"start\":38474},{\"end\":38487,\"start\":38485},{\"end\":38502,\"start\":38493},{\"end\":38508,\"start\":38506},{\"end\":38848,\"start\":38844},{\"end\":38855,\"start\":38852},{\"end\":38863,\"start\":38859},{\"end\":38872,\"start\":38867},{\"end\":38879,\"start\":38876},{\"end\":38887,\"start\":38883},{\"end\":39169,\"start\":39166},{\"end\":39178,\"start\":39173},{\"end\":39186,\"start\":39182},{\"end\":39197,\"start\":39192},{\"end\":39557,\"start\":39551},{\"end\":39569,\"start\":39561},{\"end\":39580,\"start\":39573},{\"end\":39591,\"start\":39584},{\"end\":39603,\"start\":39595},{\"end\":39993,\"start\":39987},{\"end\":40009,\"start\":39999},{\"end\":40023,\"start\":40013},{\"end\":40035,\"start\":40029},{\"end\":40046,\"start\":40039},{\"end\":40339,\"start\":40333},{\"end\":40350,\"start\":40343},{\"end\":40359,\"start\":40354},{\"end\":40370,\"start\":40363},{\"end\":40381,\"start\":40374},{\"end\":40390,\"start\":40385},{\"end\":40403,\"start\":40394},{\"end\":40410,\"start\":40407},{\"end\":40420,\"start\":40414},{\"end\":40695,\"start\":40689},{\"end\":40701,\"start\":40699},{\"end\":40813,\"start\":40811},{\"end\":40826,\"start\":40817},{\"end\":40840,\"start\":40830},{\"end\":40849,\"start\":40844},{\"end\":40860,\"start\":40853},{\"end\":40867,\"start\":40864},{\"end\":40878,\"start\":40871},{\"end\":40891,\"start\":40882},{\"end\":40902,\"start\":40895},{\"end\":40908,\"start\":40906},{\"end\":41168,\"start\":41161},{\"end\":41180,\"start\":41174},{\"end\":41193,\"start\":41184},{\"end\":41344,\"start\":41341},{\"end\":41350,\"start\":41348},{\"end\":41361,\"start\":41354},{\"end\":41371,\"start\":41367},{\"end\":41383,\"start\":41375},{\"end\":41664,\"start\":41661},{\"end\":41677,\"start\":41668},{\"end\":41687,\"start\":41681},{\"end\":41697,\"start\":41691},{\"end\":41703,\"start\":41701},{\"end\":41715,\"start\":41707},{\"end\":41918,\"start\":41910},{\"end\":41927,\"start\":41922},{\"end\":41939,\"start\":41931},{\"end\":41953,\"start\":41943},{\"end\":41963,\"start\":41957},{\"end\":41974,\"start\":41967},{\"end\":42287,\"start\":42273},{\"end\":42297,\"start\":42291},{\"end\":42312,\"start\":42305},{\"end\":42324,\"start\":42318},{\"end\":42339,\"start\":42328},{\"end\":42352,\"start\":42343},{\"end\":42913,\"start\":42903},{\"end\":42929,\"start\":42919},{\"end\":42944,\"start\":42933},{\"end\":42959,\"start\":42950},{\"end\":42974,\"start\":42963},{\"end\":42980,\"start\":42978},{\"end\":42987,\"start\":42984},{\"end\":43338,\"start\":43328},{\"end\":43354,\"start\":43344},{\"end\":43364,\"start\":43358},{\"end\":43376,\"start\":43370},{\"end\":43391,\"start\":43380},{\"end\":43397,\"start\":43395},{\"end\":43585,\"start\":43579},{\"end\":43594,\"start\":43589},{\"end\":43604,\"start\":43598},{\"end\":43614,\"start\":43608},{\"end\":43957,\"start\":43953},{\"end\":43969,\"start\":43959},{\"end\":43977,\"start\":43971},{\"end\":43983,\"start\":43979},{\"end\":43992,\"start\":43985},{\"end\":44003,\"start\":43994},{\"end\":44014,\"start\":44005},{\"end\":44027,\"start\":44016},{\"end\":44463,\"start\":44456},{\"end\":44471,\"start\":44467},{\"end\":44481,\"start\":44475},{\"end\":44735,\"start\":44731},{\"end\":44744,\"start\":44739},{\"end\":44756,\"start\":44750},{\"end\":44767,\"start\":44760},{\"end\":44780,\"start\":44773},{\"end\":44791,\"start\":44786},{\"end\":44809,\"start\":44795},{\"end\":45021,\"start\":45017},{\"end\":45030,\"start\":45025},{\"end\":45040,\"start\":45034},{\"end\":45052,\"start\":45046},{\"end\":45063,\"start\":45056},{\"end\":45076,\"start\":45069},{\"end\":45094,\"start\":45080},{\"end\":45105,\"start\":45100},{\"end\":45488,\"start\":45484},{\"end\":45496,\"start\":45492},{\"end\":45504,\"start\":45500},{\"end\":45513,\"start\":45508},{\"end\":45522,\"start\":45517},{\"end\":45529,\"start\":45526},{\"end\":45537,\"start\":45533},{\"end\":45875,\"start\":45870},{\"end\":45884,\"start\":45879},{\"end\":46434,\"start\":46426},{\"end\":46444,\"start\":46438},{\"end\":46457,\"start\":46448},{\"end\":46474,\"start\":46461},{\"end\":46689,\"start\":46681},{\"end\":46699,\"start\":46693},{\"end\":46712,\"start\":46703},{\"end\":46729,\"start\":46716},{\"end\":47136,\"start\":47130},{\"end\":47145,\"start\":47140},{\"end\":47156,\"start\":47149},{\"end\":47162,\"start\":47160},{\"end\":47170,\"start\":47168},{\"end\":47186,\"start\":47174},{\"end\":47411,\"start\":47405},{\"end\":47420,\"start\":47415},{\"end\":47431,\"start\":47424},{\"end\":47437,\"start\":47435},{\"end\":47445,\"start\":47443},{\"end\":47461,\"start\":47449},{\"end\":47865,\"start\":47859},{\"end\":47873,\"start\":47869},{\"end\":47881,\"start\":47877},{\"end\":47891,\"start\":47885},{\"end\":48187,\"start\":48179},{\"end\":48200,\"start\":48191},{\"end\":48213,\"start\":48206},{\"end\":48228,\"start\":48219},{\"end\":48238,\"start\":48232},{\"end\":48458,\"start\":48448},{\"end\":48466,\"start\":48462},{\"end\":48475,\"start\":48470},{\"end\":48485,\"start\":48479},{\"end\":48499,\"start\":48489},{\"end\":48511,\"start\":48505},{\"end\":48807,\"start\":48799},{\"end\":48819,\"start\":48811},{\"end\":48826,\"start\":48823},{\"end\":48835,\"start\":48830},{\"end\":48843,\"start\":48839},{\"end\":48861,\"start\":48847},{\"end\":48873,\"start\":48865},{\"end\":48884,\"start\":48877},{\"end\":48894,\"start\":48888},{\"end\":49188,\"start\":49182},{\"end\":49198,\"start\":49192},{\"end\":49205,\"start\":49202},{\"end\":49216,\"start\":49209},{\"end\":49230,\"start\":49220},{\"end\":49244,\"start\":49234},{\"end\":49256,\"start\":49250},{\"end\":49271,\"start\":49260},{\"end\":49639,\"start\":49631},{\"end\":49649,\"start\":49643},{\"end\":49661,\"start\":49653},{\"end\":49674,\"start\":49665},{\"end\":49685,\"start\":49678},{\"end\":49697,\"start\":49689},{\"end\":50039,\"start\":50034},{\"end\":50050,\"start\":50043},{\"end\":50068,\"start\":50054},{\"end\":50378,\"start\":50374},{\"end\":50385,\"start\":50382},{\"end\":50392,\"start\":50389},{\"end\":50404,\"start\":50396},{\"end\":50414,\"start\":50408},{\"end\":50422,\"start\":50418},{\"end\":50672,\"start\":50668},{\"end\":50683,\"start\":50678},{\"end\":50691,\"start\":50687},{\"end\":50698,\"start\":50695},{\"end\":51052,\"start\":51049},{\"end\":51060,\"start\":51056},{\"end\":51078,\"start\":51064},{\"end\":51087,\"start\":51082},{\"end\":51396,\"start\":51392},{\"end\":51405,\"start\":51400},{\"end\":51411,\"start\":51409},{\"end\":51417,\"start\":51415},{\"end\":51425,\"start\":51421},{\"end\":51432,\"start\":51429},{\"end\":51441,\"start\":51436},{\"end\":51448,\"start\":51445},{\"end\":51784,\"start\":51779},{\"end\":51790,\"start\":51788},{\"end\":51800,\"start\":51794},{\"end\":51810,\"start\":51804},{\"end\":51930,\"start\":51928},{\"end\":51948,\"start\":51934},{\"end\":51958,\"start\":51952},{\"end\":51966,\"start\":51962},{\"end\":51975,\"start\":51970},{\"end\":51987,\"start\":51979},{\"end\":52242,\"start\":52240},{\"end\":52248,\"start\":52246},{\"end\":52258,\"start\":52252},{\"end\":52264,\"start\":52262},{\"end\":52270,\"start\":52268},{\"end\":52282,\"start\":52274},{\"end\":52433,\"start\":52428},{\"end\":52444,\"start\":52437},{\"end\":52455,\"start\":52448},{\"end\":52465,\"start\":52459}]", "bib_entry": "[{\"attributes\":{\"doi\":\"CoRR abs/2112.01523\",\"id\":\"b0\"},\"end\":37231,\"start\":36921},{\"attributes\":{\"id\":\"b1\"},\"end\":37582,\"start\":37233},{\"attributes\":{\"id\":\"b2\"},\"end\":37859,\"start\":37584},{\"attributes\":{\"id\":\"b3\"},\"end\":38152,\"start\":37861},{\"attributes\":{\"id\":\"b4\"},\"end\":38399,\"start\":38154},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":229298099},\"end\":38840,\"start\":38401},{\"attributes\":{\"doi\":\"arXiv:2111.15552\",\"id\":\"b6\"},\"end\":39109,\"start\":38842},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":234482661},\"end\":39494,\"start\":39111},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":232270138},\"end\":39983,\"start\":39496},{\"attributes\":{\"id\":\"b9\"},\"end\":40260,\"start\":39985},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":233476659},\"end\":40639,\"start\":40262},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6628106},\"end\":40807,\"start\":40641},{\"attributes\":{\"id\":\"b12\"},\"end\":41090,\"start\":40809},{\"attributes\":{\"id\":\"b13\"},\"end\":41309,\"start\":41092},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":220686483},\"end\":41581,\"start\":41311},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235313531},\"end\":41906,\"start\":41583},{\"attributes\":{\"id\":\"b16\"},\"end\":42191,\"start\":41908},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":220968781},\"end\":42809,\"start\":42193},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":219947110},\"end\":43252,\"start\":42811},{\"attributes\":{\"id\":\"b19\"},\"end\":43575,\"start\":43254},{\"attributes\":{\"doi\":\"arXiv:2201.05989\",\"id\":\"b20\"},\"end\":43852,\"start\":43577},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":234365467},\"end\":44269,\"start\":43854},{\"attributes\":{\"doi\":\"10.1111/cgf.14340\",\"id\":\"b22\"},\"end\":44358,\"start\":44271},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":233307004},\"end\":44727,\"start\":44360},{\"attributes\":{\"id\":\"b24\"},\"end\":45013,\"start\":44729},{\"attributes\":{\"doi\":\"arXiv:2106.13228\",\"id\":\"b25\"},\"end\":45480,\"start\":45015},{\"attributes\":{\"doi\":\"arXiv:2105.02872\",\"id\":\"b26\"},\"end\":45796,\"start\":45482},{\"attributes\":{\"doi\":\"https:/doi.ieeecomputersociety.org/10.1109/3DV53792.2021.00118\",\"id\":\"b27\",\"matched_paper_id\":243832842},\"end\":46371,\"start\":45798},{\"attributes\":{\"id\":\"b28\"},\"end\":46626,\"start\":46373},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":227227965},\"end\":47126,\"start\":46628},{\"attributes\":{\"id\":\"b30\"},\"end\":47367,\"start\":47128},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":227162149},\"end\":47855,\"start\":47369},{\"attributes\":{\"id\":\"b32\"},\"end\":48090,\"start\":47857},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":235352518},\"end\":48444,\"start\":48092},{\"attributes\":{\"id\":\"b34\"},\"end\":48795,\"start\":48446},{\"attributes\":{\"id\":\"b35\"},\"end\":49178,\"start\":48797},{\"attributes\":{\"id\":\"b36\"},\"end\":49513,\"start\":49180},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":229348809},\"end\":50030,\"start\":49515},{\"attributes\":{\"id\":\"b38\"},\"end\":50279,\"start\":50032},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":235490453},\"end\":50602,\"start\":50281},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":227162620},\"end\":51045,\"start\":50604},{\"attributes\":{\"id\":\"b41\"},\"end\":51306,\"start\":51047},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":237421130},\"end\":51729,\"start\":51308},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":235605960},\"end\":51924,\"start\":51731},{\"attributes\":{\"id\":\"b44\"},\"end\":52173,\"start\":51926},{\"attributes\":{\"id\":\"b45\"},\"end\":52424,\"start\":52175},{\"attributes\":{\"id\":\"b46\"},\"end\":52633,\"start\":52426}]", "bib_title": "[{\"end\":38464,\"start\":38401},{\"end\":39162,\"start\":39111},{\"end\":39545,\"start\":39496},{\"end\":40329,\"start\":40262},{\"end\":40683,\"start\":40641},{\"end\":41337,\"start\":41311},{\"end\":41657,\"start\":41583},{\"end\":42269,\"start\":42193},{\"end\":42899,\"start\":42811},{\"end\":43951,\"start\":43854},{\"end\":44452,\"start\":44360},{\"end\":45866,\"start\":45798},{\"end\":46677,\"start\":46628},{\"end\":47401,\"start\":47369},{\"end\":48175,\"start\":48092},{\"end\":49627,\"start\":49515},{\"end\":50370,\"start\":50281},{\"end\":50664,\"start\":50604},{\"end\":51388,\"start\":51308},{\"end\":51775,\"start\":51731}]", "bib_author": "[{\"end\":36994,\"start\":36985},{\"end\":37003,\"start\":36994},{\"end\":37016,\"start\":37003},{\"end\":37024,\"start\":37016},{\"end\":37031,\"start\":37024},{\"end\":37245,\"start\":37233},{\"end\":37259,\"start\":37245},{\"end\":37269,\"start\":37259},{\"end\":37279,\"start\":37269},{\"end\":37297,\"start\":37279},{\"end\":37313,\"start\":37297},{\"end\":37596,\"start\":37584},{\"end\":37610,\"start\":37596},{\"end\":37620,\"start\":37610},{\"end\":37636,\"start\":37620},{\"end\":37646,\"start\":37636},{\"end\":37869,\"start\":37861},{\"end\":37878,\"start\":37869},{\"end\":37889,\"start\":37878},{\"end\":37901,\"start\":37889},{\"end\":37908,\"start\":37901},{\"end\":37918,\"start\":37908},{\"end\":38162,\"start\":38154},{\"end\":38171,\"start\":38162},{\"end\":38179,\"start\":38171},{\"end\":38186,\"start\":38179},{\"end\":38193,\"start\":38186},{\"end\":38200,\"start\":38193},{\"end\":38206,\"start\":38200},{\"end\":38472,\"start\":38466},{\"end\":38481,\"start\":38472},{\"end\":38489,\"start\":38481},{\"end\":38504,\"start\":38489},{\"end\":38510,\"start\":38504},{\"end\":38850,\"start\":38842},{\"end\":38857,\"start\":38850},{\"end\":38865,\"start\":38857},{\"end\":38874,\"start\":38865},{\"end\":38881,\"start\":38874},{\"end\":38889,\"start\":38881},{\"end\":39171,\"start\":39164},{\"end\":39180,\"start\":39171},{\"end\":39188,\"start\":39180},{\"end\":39199,\"start\":39188},{\"end\":39559,\"start\":39547},{\"end\":39571,\"start\":39559},{\"end\":39582,\"start\":39571},{\"end\":39593,\"start\":39582},{\"end\":39605,\"start\":39593},{\"end\":39995,\"start\":39985},{\"end\":40011,\"start\":39995},{\"end\":40025,\"start\":40011},{\"end\":40037,\"start\":40025},{\"end\":40048,\"start\":40037},{\"end\":40341,\"start\":40331},{\"end\":40352,\"start\":40341},{\"end\":40361,\"start\":40352},{\"end\":40372,\"start\":40361},{\"end\":40383,\"start\":40372},{\"end\":40392,\"start\":40383},{\"end\":40405,\"start\":40392},{\"end\":40412,\"start\":40405},{\"end\":40422,\"start\":40412},{\"end\":40697,\"start\":40685},{\"end\":40703,\"start\":40697},{\"end\":40815,\"start\":40809},{\"end\":40828,\"start\":40815},{\"end\":40842,\"start\":40828},{\"end\":40851,\"start\":40842},{\"end\":40862,\"start\":40851},{\"end\":40869,\"start\":40862},{\"end\":40880,\"start\":40869},{\"end\":40893,\"start\":40880},{\"end\":40904,\"start\":40893},{\"end\":40910,\"start\":40904},{\"end\":41170,\"start\":41157},{\"end\":41182,\"start\":41170},{\"end\":41195,\"start\":41182},{\"end\":41346,\"start\":41339},{\"end\":41352,\"start\":41346},{\"end\":41363,\"start\":41352},{\"end\":41373,\"start\":41363},{\"end\":41385,\"start\":41373},{\"end\":41666,\"start\":41659},{\"end\":41679,\"start\":41666},{\"end\":41689,\"start\":41679},{\"end\":41699,\"start\":41689},{\"end\":41705,\"start\":41699},{\"end\":41717,\"start\":41705},{\"end\":41920,\"start\":41908},{\"end\":41929,\"start\":41920},{\"end\":41941,\"start\":41929},{\"end\":41955,\"start\":41941},{\"end\":41965,\"start\":41955},{\"end\":41976,\"start\":41965},{\"end\":42289,\"start\":42271},{\"end\":42299,\"start\":42289},{\"end\":42314,\"start\":42299},{\"end\":42326,\"start\":42314},{\"end\":42341,\"start\":42326},{\"end\":42354,\"start\":42341},{\"end\":42915,\"start\":42901},{\"end\":42931,\"start\":42915},{\"end\":42946,\"start\":42931},{\"end\":42961,\"start\":42946},{\"end\":42976,\"start\":42961},{\"end\":42982,\"start\":42976},{\"end\":42989,\"start\":42982},{\"end\":43340,\"start\":43326},{\"end\":43356,\"start\":43340},{\"end\":43366,\"start\":43356},{\"end\":43378,\"start\":43366},{\"end\":43393,\"start\":43378},{\"end\":43399,\"start\":43393},{\"end\":43587,\"start\":43577},{\"end\":43596,\"start\":43587},{\"end\":43606,\"start\":43596},{\"end\":43616,\"start\":43606},{\"end\":43959,\"start\":43953},{\"end\":43971,\"start\":43959},{\"end\":43979,\"start\":43971},{\"end\":43985,\"start\":43979},{\"end\":43994,\"start\":43985},{\"end\":44005,\"start\":43994},{\"end\":44016,\"start\":44005},{\"end\":44029,\"start\":44016},{\"end\":44465,\"start\":44454},{\"end\":44473,\"start\":44465},{\"end\":44483,\"start\":44473},{\"end\":44737,\"start\":44729},{\"end\":44746,\"start\":44737},{\"end\":44758,\"start\":44746},{\"end\":44769,\"start\":44758},{\"end\":44782,\"start\":44769},{\"end\":44793,\"start\":44782},{\"end\":44811,\"start\":44793},{\"end\":45023,\"start\":45015},{\"end\":45032,\"start\":45023},{\"end\":45042,\"start\":45032},{\"end\":45054,\"start\":45042},{\"end\":45065,\"start\":45054},{\"end\":45078,\"start\":45065},{\"end\":45096,\"start\":45078},{\"end\":45107,\"start\":45096},{\"end\":45490,\"start\":45482},{\"end\":45498,\"start\":45490},{\"end\":45506,\"start\":45498},{\"end\":45515,\"start\":45506},{\"end\":45524,\"start\":45515},{\"end\":45531,\"start\":45524},{\"end\":45539,\"start\":45531},{\"end\":45877,\"start\":45868},{\"end\":45886,\"start\":45877},{\"end\":46436,\"start\":46424},{\"end\":46446,\"start\":46436},{\"end\":46459,\"start\":46446},{\"end\":46476,\"start\":46459},{\"end\":46691,\"start\":46679},{\"end\":46701,\"start\":46691},{\"end\":46714,\"start\":46701},{\"end\":46731,\"start\":46714},{\"end\":47138,\"start\":47128},{\"end\":47147,\"start\":47138},{\"end\":47158,\"start\":47147},{\"end\":47164,\"start\":47158},{\"end\":47172,\"start\":47164},{\"end\":47188,\"start\":47172},{\"end\":47413,\"start\":47403},{\"end\":47422,\"start\":47413},{\"end\":47433,\"start\":47422},{\"end\":47439,\"start\":47433},{\"end\":47447,\"start\":47439},{\"end\":47463,\"start\":47447},{\"end\":47867,\"start\":47857},{\"end\":47875,\"start\":47867},{\"end\":47883,\"start\":47875},{\"end\":47893,\"start\":47883},{\"end\":48189,\"start\":48177},{\"end\":48202,\"start\":48189},{\"end\":48215,\"start\":48202},{\"end\":48230,\"start\":48215},{\"end\":48240,\"start\":48230},{\"end\":48460,\"start\":48446},{\"end\":48468,\"start\":48460},{\"end\":48477,\"start\":48468},{\"end\":48487,\"start\":48477},{\"end\":48501,\"start\":48487},{\"end\":48513,\"start\":48501},{\"end\":48809,\"start\":48797},{\"end\":48821,\"start\":48809},{\"end\":48828,\"start\":48821},{\"end\":48837,\"start\":48828},{\"end\":48845,\"start\":48837},{\"end\":48863,\"start\":48845},{\"end\":48875,\"start\":48863},{\"end\":48886,\"start\":48875},{\"end\":48896,\"start\":48886},{\"end\":49190,\"start\":49180},{\"end\":49200,\"start\":49190},{\"end\":49207,\"start\":49200},{\"end\":49218,\"start\":49207},{\"end\":49232,\"start\":49218},{\"end\":49246,\"start\":49232},{\"end\":49258,\"start\":49246},{\"end\":49273,\"start\":49258},{\"end\":49641,\"start\":49629},{\"end\":49651,\"start\":49641},{\"end\":49663,\"start\":49651},{\"end\":49676,\"start\":49663},{\"end\":49687,\"start\":49676},{\"end\":49699,\"start\":49687},{\"end\":50041,\"start\":50032},{\"end\":50052,\"start\":50041},{\"end\":50070,\"start\":50052},{\"end\":50380,\"start\":50372},{\"end\":50387,\"start\":50380},{\"end\":50394,\"start\":50387},{\"end\":50406,\"start\":50394},{\"end\":50416,\"start\":50406},{\"end\":50424,\"start\":50416},{\"end\":50674,\"start\":50666},{\"end\":50685,\"start\":50674},{\"end\":50693,\"start\":50685},{\"end\":50700,\"start\":50693},{\"end\":51054,\"start\":51047},{\"end\":51062,\"start\":51054},{\"end\":51080,\"start\":51062},{\"end\":51089,\"start\":51080},{\"end\":51398,\"start\":51390},{\"end\":51407,\"start\":51398},{\"end\":51413,\"start\":51407},{\"end\":51419,\"start\":51413},{\"end\":51427,\"start\":51419},{\"end\":51434,\"start\":51427},{\"end\":51443,\"start\":51434},{\"end\":51450,\"start\":51443},{\"end\":51786,\"start\":51777},{\"end\":51792,\"start\":51786},{\"end\":51802,\"start\":51792},{\"end\":51812,\"start\":51802},{\"end\":51932,\"start\":51926},{\"end\":51950,\"start\":51932},{\"end\":51960,\"start\":51950},{\"end\":51968,\"start\":51960},{\"end\":51977,\"start\":51968},{\"end\":51989,\"start\":51977},{\"end\":52244,\"start\":52238},{\"end\":52250,\"start\":52244},{\"end\":52260,\"start\":52250},{\"end\":52266,\"start\":52260},{\"end\":52272,\"start\":52266},{\"end\":52284,\"start\":52272},{\"end\":52435,\"start\":52426},{\"end\":52446,\"start\":52435},{\"end\":52457,\"start\":52446},{\"end\":52467,\"start\":52457}]", "bib_venue": "[{\"end\":38639,\"start\":38583},{\"end\":39320,\"start\":39268},{\"end\":39748,\"start\":39685},{\"end\":42517,\"start\":42444},{\"end\":46019,\"start\":45998},{\"end\":46894,\"start\":46821},{\"end\":47626,\"start\":47553},{\"end\":48262,\"start\":48255},{\"end\":50849,\"start\":50783},{\"end\":36983,\"start\":36921},{\"end\":37397,\"start\":37313},{\"end\":37712,\"start\":37646},{\"end\":37979,\"start\":37918},{\"end\":38265,\"start\":38206},{\"end\":38581,\"start\":38510},{\"end\":38964,\"start\":38905},{\"end\":39266,\"start\":39199},{\"end\":39683,\"start\":39605},{\"end\":40113,\"start\":40048},{\"end\":40434,\"start\":40422},{\"end\":40715,\"start\":40703},{\"end\":40935,\"start\":40910},{\"end\":41155,\"start\":41092},{\"end\":41434,\"start\":41385},{\"end\":41734,\"start\":41717},{\"end\":42039,\"start\":41976},{\"end\":42442,\"start\":42354},{\"end\":43017,\"start\":42989},{\"end\":43324,\"start\":43254},{\"end\":43703,\"start\":43632},{\"end\":44052,\"start\":44029},{\"end\":44533,\"start\":44483},{\"end\":44859,\"start\":44811},{\"end\":45218,\"start\":45123},{\"end\":45612,\"start\":45555},{\"end\":45996,\"start\":45948},{\"end\":46422,\"start\":46373},{\"end\":46819,\"start\":46731},{\"end\":47220,\"start\":47188},{\"end\":47551,\"start\":47463},{\"end\":47965,\"start\":47893},{\"end\":48253,\"start\":48240},{\"end\":48593,\"start\":48513},{\"end\":48973,\"start\":48896},{\"end\":49334,\"start\":49273},{\"end\":49754,\"start\":49699},{\"end\":50148,\"start\":50070},{\"end\":50431,\"start\":50424},{\"end\":50781,\"start\":50700},{\"end\":51168,\"start\":51089},{\"end\":51500,\"start\":51450},{\"end\":51819,\"start\":51812},{\"end\":52039,\"start\":51989},{\"end\":52236,\"start\":52175},{\"end\":52521,\"start\":52467}]"}}}, "year": 2023, "month": 12, "day": 17}