{"id": 53976534, "updated": "2023-07-19 02:54:54.292", "metadata": {"title": "KDGAN: Knowledge Distillation with Generative Adversarial Networks", "authors": "[{\"first\":\"Xiaojie\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Rui\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Jianzhong\",\"last\":\"Qi\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "783-794", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "Knowledge distillation (KD) aims to train a lightweight classi\ufb01er suitable to provide accurate inference with constrained resources in multi-label learning. Instead of directly consuming feature-label pairs, the classi\ufb01er is trained by a teacher, i.e., a high-capacity model whose training may be resource-hungry. The accuracy of the classi\ufb01er trained this way is usually suboptimal because it is dif\ufb01cult to learn the true data distribution from the teacher. An alternative method is to adversarially train the classi\ufb01er against a discriminator in a two-player game akin to generative adversarial networks (GAN), which can ensure the classi\ufb01er to learn the true data distribution at the equilibrium of this game. However, it may take excessively long time for such a two-player game to reach equilibrium due to high-variance gradient updates. To address these limitations, we propose a three-player game named KDGAN consisting of a classi\ufb01er, a teacher, and a discriminator. The classi\ufb01er and the teacher learn from each other via distillation losses and are adversarially trained against the discriminator via adversarial losses. By simultaneously optimizing the distillation and adversarial losses, the classi\ufb01er will learn the true data distribution at the equilibrium. We approximate the discrete distribution learned by the classi\ufb01er (or the teacher) with a concrete distribution. From the concrete distribution, we generate continuous samples to obtain low-variance gradient updates, which speed up the training. Extensive experiments using real datasets con\ufb01rm the superiority of KDGAN in both accuracy and training speed.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2903396356", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/WangZSQ18", "doi": null}}, "content": {"source": {"pdf_hash": "52f3c629b80e582d71cd6bf56256f784e23aeeb6", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "20071f44279bac954b48896f125c7b16400b5065", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/52f3c629b80e582d71cd6bf56256f784e23aeeb6.txt", "contents": "\nKDGAN: Knowledge Distillation with Generative Adversarial Networks\n\n\nXiaojie Wang xiaojiew94@gmail.com \nUniversity of Melbourne\nUniversity of Melbourne\nTwitter Inc\nUniversity of Melbourne\n\n\nRui Zhang rui.zhang@unimelb.edu.au \nUniversity of Melbourne\nUniversity of Melbourne\nTwitter Inc\nUniversity of Melbourne\n\n\nYu Sun ysun@twitter.com \nUniversity of Melbourne\nUniversity of Melbourne\nTwitter Inc\nUniversity of Melbourne\n\n\nJianzhong Qi jianzhong.qi@unimelb.edu.au \nUniversity of Melbourne\nUniversity of Melbourne\nTwitter Inc\nUniversity of Melbourne\n\n\nKDGAN: Knowledge Distillation with Generative Adversarial Networks\n\nKnowledge distillation (KD) aims to train a lightweight classifier suitable to provide accurate inference with constrained resources in multi-label learning. Instead of directly consuming feature-label pairs, the classifier is trained by a teacher, i.e., a high-capacity model whose training may be resource-hungry.\n\nIntroduction\n\nIn machine learning, it is common that more resources such as input features [47] or computational resources [23], which we refer to as privileged provision, are available at the stage of training a model than those available at the stage of running the deployed model (i.e., the inference stage). Figure 1 shows an example application of image tag recommendation, where more input features (called privileged information [47]) are available at the training stage than those available at the inference stage. Specifically, the training stage has access to images as well as image titles and comments (textual information) as shown in Figure 1a, whereas the inference stage only has access to images themselves as shown in Figure 1b. After a smart phone user uploads an image and is about to provide tags for the image, it is inconvenient to type tags on the phone and thinking about tags for the image also takes time, so it is very useful to recommend tags based on the image as shown in Figure 1b. Another example application is unlocking mobile phones by face recognition. We usually deploy face recognition models on mobile phones so that legit users can unlock the phones without depending on remote services or internet connections. The training stage may be done on a powerful server with significantly more computational resources than the inference stage, which is done on a mobile phone. Here, a key problem is how to use privileged provision, i.e., resources only accessible for training, to train a model with great inference performance [29].\n\nTypical approaches to the problem are based on knowledge distillation (KD) [7,9,23]. As shown by the left half of Figure 2, KD consists of a classifier and a teacher [29]. To operate for resourceconstrained inference, the classifier does not use privileged provision. On the other hand, the teacher uses privileged provision by, e.g., having a larger model capacity or taking more features as input. Once trained, the teacher outputs a distribution over labels called soft labels [29] for each training instance. Then, the teacher trains the classifier to predict the soft labels via a distillation loss such as the L2 loss on logits [7]. This training process is often called \"distilling\" the knowledge in the teacher into the classifier [23]. Since the teacher normally cannot perfectly model the true data distribution, it is difficult for the classifier to learn the true data distribution from the teacher.\n\nGenerative adversarial networks (GAN) provide an alternative way to learn the true data distribution. Inspired by Wang et al. [49], we first present a naive GAN (NaGAN) with two players. As shown by the right part of Figure 2, NaGAN consists of a classifier and a discriminator. The classifier serves as a generator that generates relevant labels given an instance while the discriminator aims to distinguish the true labels from the generated ones. The classifier learns from the discriminator to perfectly model the true data distribution at the equilibrium via adversarial losses. One limitation of NaGAN is that a large number of training instances and epochs is normally required to reach equilibrium [15], which restricts its applicability to domains where collecting labeled data is expensive. The slow training speed is because in such a two-player framework, the gradients from the discriminator to update the classifier often vanish or explode during the adversarial training [4]. It is challenging to train a classifier to learn the true data distribution with limited training instances and epochs.\n\nTo address this challenge, we propose a three-player framework named KDGAN to distill knowledge with generative adversarial networks. As shown in Figure 2, KDGAN consists of a classifier, a teacher, and a discriminator. In addition to the distillation loss in KD and the adversarial losses in NaGAN mentioned above, we define a distillation loss from the classifier to the teacher and an adversarial loss between the teacher and the discriminator. Specifically, the classifier and the teacher, serving as generators, aim to fool the discriminator by generating pseudo labels that resemble the true labels. Meanwhile, the classifier and the teacher try to reach an agreement on what pseudo labels to generate by distilling their knowledge into each other. By formulating the distillation and adversarial losses as a minimax game, we enable the classifier to learn the true data distribution at the equilibrium (see Section 3.2). Besides, the classifier receives gradients from the teacher via the distillation loss and the discriminator via the adversarial loss. The gradients from the teacher often have low variance, which reduces the variance of gradients and thus speeds up the adversarial training (see Section 3.3).\n\nWe further consider reducing the variance of the gradients from the discriminator to accelerate the training of KDGAN. The gradients from the discriminator may have large variance when obtained through the widely used policy gradient methods [49,52]. It is non-trivial to obtain low-variance gradients from the discriminator because the classifier and the teacher generate discrete samples, which are not differentiable w.r.t. their parameters. We propose to relax the discrete distributions learned by the classifier and the teacher into concrete distributions [25,31] with the Gumbel-Max trick [20,30]. We use the concrete distributions for generating continuous samples to enable endto-end differentiability and sufficient control over the variance of gradients. Given the continuous samples, we obtain low-variance gradients from the discriminator to accelerate the KDGAN training.\n\nTo summarize, our contributions are as follows:\n\n\u2022 We propose a novel framework named KDGAN for multi-label learning, which trains a lightweight classifier suitable for resource-constrained inference using resources available only for training. \u2022 We reduce the number of training epochs required to converge by decreasing the variance of gradients, which is achieved by the design of KDGAN and the Gumbel-Max trick. \u2022 We conduct extensive experiments in two applications, image tag recommendation and deep model compression. The experiments validate the superiority of KDGAN over state-of-the-art methods.\n\n\nRelated Work\n\nWe briefly review studies on knowledge distillation (KD) and generative adversarial networks (GAN).\n\nKD aims to transfer the knowledge in a powerful teacher to a lightweight classifier [9]. For example, Ba and Caruana [7] train a shallow classifier network to mimic a deep teacher network by matching logits via the L2 loss. Hinton et al. [23] generalize this work by training a classifier to predict soft labels provided by a teacher. Sau and Balasubramanian [39] further add random perturbations into soft labels to simulate learning from multiple teachers. Instead of using soft labels, Romero et al. [36] propose to use middle layers of a teacher to train a classifier. Unlike previous work on classification problems, Chen et al. [10] apply KD and hint learning to object detection problems.\n\nThere also exists work that leverages KD to transfer knowledge between different domains [21], e.g., between high-quality and low-quality images [41]. Lopez-Paz et al. [29] unify KD with privileged information [35,47,48] as generalized distillation where a teacher is pretrained by taking as input privileged information. Compared to KD, the proposed KDGAN framework introduces a discriminator to guarantee that the classifier can learn the true data distribution at the equilibrium.\n\nGAN is initially proposed to generate continuous data by training a generator and a discriminator adversarially in a minimax game [17]. GAN has only recently been introduced to generate discrete data [16,54,55] because discrete data makes it difficult to pass gradients from a discriminator backward to update a generator. For example, sequence GAN (SeqGAN) [52] models the process of token sequence generation as a stochastic policy and adopts Monte Carlo search to update a generator. Different from these GANs with two players, Li et al. propose a GAN with three players called Triple-GAN [13]. Our KDGAN also consists of three players including two generators and a discriminator, but differs from Triple-GAN in that: (1) Both generators in KDGAN learn a conditional distribution over labels given features. However, the generators in Triple-GAN learn a conditional distribution over labels given features and a conditional distribution over features given labels, respectively. (2) The samples from both generators in KDGAN are all discrete data while the samples from the generators in Triple-GAN include both discrete and continuous data. These differences lead to different objective functions and training techniques, e.g., KDGAN can use the Gumbel-Max trick [20,30] to generate samples from both generators while Triple-GAN cannot do this. There is also a rich body of studies on improving the training of GAN [5,33,56] such as feature matching [38], which are orthogonal to our work and can be used to improve the training of KDGAN.\n\nWe explore the idea of integrating KD and GAN. A similar idea has been studied in [51] where a discriminator is introduced to train a classifier. This previous study [51] differs from ours in that their discriminator trains the classifier to learn the data distribution produced by the teacher, while our discriminator trains the classifier to learn the true data distribution.\n\nWe apply the proposed KDGAN to address the problem of deep model compression and image tag recommendation. We can also apply KDGAN to address the other problems where privileged provision is available [44]. For example, we can consider contextual signals in the intent tracking problem [42,43] or user reviews in the movie recommendation problem [50] as privileged provision.\n\n\nMethods\n\nWe study the problem of training a lightweight classifier from a teacher that is trained with privileged provision (denoted by ) to satisfy stringent inference requirements. The inference requirements may include (1) running in real time with limited computational resources, where privileged provision is computational resources [23]; (2) lacking a certain type of input features, where privileged provision is privileged information [47]. Following existing work [29], we use multi-label learning problems [12,18,53] as the target application scenarios of our methods for illustration purpose.\n\n\nClassifier Teacher\n\nDiscriminator Figure 2: Comparison among KD, NaGAN, and KDGAN. The classifier (C) and the teacher (T ) learn discrete categorical distributions p c (y|x) and p t (y|x); y is a true label generated from the true data distribution p u (y|x); y c and y t are continuous samples generated from concrete distributions q c (y|x) and q t (y|x); s c and s t are soft labels produced by C and T ; L c DS and L t DS are distillation losses for C and T ; L p AD and L n AD are adversarial losses for positive and negative feature-label pairs.\ns c = pc(y|x) L c DS x s t = p t (y|x) L t DS x y c \u223c qc(y|x) L n AD y t \u223c q t (y|x) L n AD y \u223c pu(y|x) L p AD x KD NaGAN KDGAN\nSince privileged provision is only available at the training stage, the goal of the problem is to train a lightweight classifier that does not use privileged provision for effective inference.\n\nTo achieve this goal, we start with NaGAN, a naive adaptation of the two-player framework proposed by Wang et al. in information retrieval (Section 3.1). Similar to other two-player frameworks [49], the naive adaptation requires a large number of training instances and epochs [15], which is difficult to satisfy in practice [4]. To address the limitation, we propose a three-player framework named KDGAN that can speed up the training while preserving the equilibrium (Sections 3.2 and 3.3).\n\n\nNaGAN Formulation\n\nWe begin with NaGAN that combines a classifier C with a discriminator D in a minimax game.\n\nSince D is not meant for inference, it can leverage privileged provision. For example, D may have a larger model capacity than C or take as input more features than those available to C. In NaGAN, C generates pseudo labels y given features x following a categorical distribution p c (y|x), while D computes the probability p d (x, y) of a label y being from the true data distribution p u (y|x) given features x. With a slight abuse of notation, we also use x to refer to features including privileged information when the context is clear. Following the value function of IRGAN [49], we define the value function V (c, d) for the minimax game in NaGAN as\nmin c max d V (c, d) = E y\u223cpu [log p d (x, y)] + E y\u223cpc [log(1 \u2212 p d (x, y))].(1)\nLet h(x, y) and g(x, y) be the scoring functions for C and D. We define p c (y|x) and p d (x, y) as p c (y|x) = softmax(h(x, y)) and p d (x, y) = sigmoid(g(x, y)).\n\nThe scoring functions can be implemented in various ways, e.g., h(x, y) can be a multilayer perceptron [27]. We will detail the scoring functions for specific applications in Section 4. Such a two-player framework is trained by updating C and D alternatively [49]. The training will proceed until the equilibrium is reached, where C learns the true data distribution. At that point, D can do no better than random guesses at deciding whether a given label is generated by C or not [6].\n\nOur key observation is that the advantages and the disadvantages of KD and NaGAN are complementary: (1) KD usually requires a small number of training instances and epochs but cannot ensure the equilibrium where p c (y|x) = p u (y|x).\n\n(2) NaGAN ensures the equilibrium where p c (y|x) = p u (y|x) [49] but normally requires a large number of training instances and epochs. We aim to retain the advantages and avoid the disadvantages of both methods in a single framework.\n\n\nKDGAN Formulation\n\nWe formulate KDGAN as a minimax game with a classifier C, a teacher T , and a discriminator D. Similar to the classifier C, the teacher T generates pseudo labels based on a categorical distribution\np t (y|x) = softmax(f (x, y)) where f (x, y)\nis also a scoring function. Both T and D use privileged provision, e.g., by having a large model capacity or taking privileged information as input. In KDGAN, D aims to maximize the probability of correctly distinguishing the true and pseudo labels, whereas C and T aim to minimize the probability that D rejects their generated pseudo labels. Meanwhile, C learns from T by mimicking the learned distribution of T . To build a general framework, we also enable T to learn from C because, in reality, a teacher's ability can also be enhanced by interacting with students (see Figure 6 in Appendix D for empirical evidence that T benefits from learning from Sample labels {y1, ..., y k }, {y c 1 , ..., y c k }, and {y t 1 , ..., y t k } from pu(y|x), qc(y|x), and q t (y|x). 5 Update D by ascending along its gradients\n6 1 k k i=1 \u2207 d log p d (x, yi) + \u03b1\u2207 d log(1 \u2212 p d (x, z c i )) + (1 \u2212 \u03b1)\u2207 d log(1 \u2212 p d (x, z t i )) . 7\nfor the number of training steps for the teacher do 8 Sample labels {y t 1 , ..., y t k } from q t (y|x) and update the teacher by descending along its gradients\n9 1 k k i=1 (1 \u2212 \u03b1)\u2207t log q t (y t i |x) log(1 \u2212 p d (x, z t i )) + \u03b3\u2207tL t DS (p t (y|x)\n, pc(y|x)). 10 for the number of training steps for the classifier do 11 Sample labels {y c 1 , ..., y c k } from qc(y|x) and update C by descending along its gradients\n12 1 k k i=1 \u03b1\u2207c log qc(y c i |x) log(1 \u2212 p d (x, z c i )) + \u03b2\u2207cL c DS (pc(y|x), p t (y|x)).\nC). Such a mutual learning helps C and T reduce their probability of generating different pseudo labels. Formally, we define the value function U (c, t, d) for the minimax game in KDGAN as\nmin c,t max d U (c, t, d) = E y\u223cpu [log p d (x, y)] + \u03b1E y\u223cpc [log(1 \u2212 p d (x, y))] + (1 \u2212 \u03b1)E y\u223cp t [log(1 \u2212 p d (x, y))] + \u03b2L c DS (p c (y|x), p t (y|x)) + \u03b3L t DS (p t (y|x), p c (y|x)),(3)\nwhere \u03b1 \u2208 (0, 1), \u03b2 \u2208 (0, +\u221e), and \u03b3 \u2208 (0, +\u221e) are hyperparameters. We collectively refer to the expectation terms as the adversarial losses and refer to L c DS and L t DS as the distillation losses. The distillation losses can be defined in several ways [39], e.g., the L2 loss [7] or Kullback-Leibler divergence [23]. Note that L c DS and L t DS are used to train the classifier and the teacher, respectively. Theoretical Analysis. We show that the classifier perfectly learns the true data distribution at the equilibrium of KDGAN. To see this, let p \u03b1 (y|x) = \u03b1p c (y|x) + (1 \u2212 \u03b1)p t (y|x). It can be shown that the adversarial losses w.r.t. p c (y|x) and p t (y|x) are equal to an adversarial loss w.r.t. p \u03b1 (y|x):\n\u03b1E y\u223cpc [log(1 \u2212 p d (x, y))] + (1 \u2212 \u03b1)E y\u223cp t [log(1 \u2212 p d (x, y))] = \u03b1 y p c (y|x) log(1 \u2212 p d (x, y)) + (1 \u2212 \u03b1) y p t (y|x) log(1 \u2212 p d (x, y)) = y \u03b1p c (y|x) + (1 \u2212 \u03b1)p t (y|x) log(1 \u2212 p d (x, y)) = E y\u223cp \u03b1 [log(1 \u2212 p d (x, y))].(4)\nTherefore, let L MD = \u03b2L c DS (p c (y|x), p t (y|x)) + \u03b3L t DS (p t (y|x), p c (y|x)) and L JS be the Jensen-Shannon divergence, the value function U (c, t, d) of the minimax game can be rewritten as\nmin \u03b1 max d E y\u223cpu [log p d (x, y)] + E y\u223cp \u03b1 [log(1 \u2212 p d (x, y))] + L MD = min \u03b1 2L JS (p u (y|x)||p \u03b1 (y|x)) + \u03b2L c DS (p c (y|x), p t (y|x)) + \u03b3L t DS (p t (y|x), p c (y|x)) \u2212 log(4).(5)\nHere, L JS reaches the minimum if and only if p \u03b1 (y|x) = p u (y|x) and L c DS (or L t DS ) reaches the minimum if and only if p c (y|x) = p t (y|x). Hence, the KDGAN equilibrium is reached if and only if p c (y|x) = p t (y|x) = p u (y|x) where the classifier learns the true data distribution. We summarize the above discussions in Lemma 4.1 (the necessary and sufficient conditions of maximizing the value function) and Theorem 4.2 (achieving the equilibrium), respectively (see Appendix A for proofs).  \n\n\nKDGAN Training\n\nIn this section, we detail techniques for accelerating the training speed of KDGAN via reducing the number of training epochs needed. As discussed in earlier studies [8,46], the training speed is closely related to the variance of gradients. Comparing with NaGAN, the KDGAN framework by design can reduce the variance of gradients. This is because the high variance of a random variable can be reduced by a low-variance random variable (detailed in Lemma 4.3) and as we will discuss, T provides gradients of lower variance than D does. To reduce the variance of gradients from D and attain sufficient control over the variance, we further propose to obtain gradients from a continuous space by relaxing the discrete samples, i.e., pseudo labels, propagated between the classifier (or the teacher) and the discriminator into continuous samples with a reparameterization trick [25,31].\n\nFirst, we show how KDGAN reduces the variance of gradients. As discussed above, C only receives gradients \u2207 c V from D in NaGAN while it receives gradients \u2207 c U from both D and T in KDGAN:\n\u2207 c V = \u2207 c L n AD , \u2207 c U = \u03bb\u2207 c L n AD + (1 \u2212 \u03bb)\u2207 c L c DS ,(6)\nwhere \u03bb \u2208 (0, 1), \u2207 c L n AD and \u2207 c L c DS are gradients from D and T , respectively. Consistent with the findings in existing work [23,39], we also observe that \u2207 c L c DS usually has a lower variance than \u2207 c L n AD (see Figure 7 in Appendix D for empirical evidence that the variance of \u2207 c L c DS is smaller than that of \u2207 c L n AD during the training process). Hence, it can be easily shown that the gradients w.r.t. C in KDGAN have a lower variance than that in NaGAN (refer to Lemma 4.3):\nVar(\u2207 c L c DS ) \u2264 Var(\u2207 c L n AD ) \u21d2 Var(\u2207 c U ) \u2264 Var(\u2207 c V ).(7)\nNext, we further reduce the variance of gradients with a reparameterization trick, in particular, the Gumbel-Max trick [20,30]. The essence of the Gumbel-Max trick is to reparameterize generating discrete samples into a differentiable function of its parameters and an additional random variable of a Gumbel distribution. To perform the Gumbel-Max trick on generating discrete samples from the categorical distribution p c (y|x), a concrete distribution [25,31] can be used. We use a concrete distribution q c (y|x) to generate continuous samples and use the continuous samples to compute the gradients \u2207 c L n AD of the adversarial loss w.r.t. the classifier as\n\u2207 c L n AD = \u2207 c E y\u223cpc [log(1 \u2212 p d (x, y))] = E y\u223cqc [\u2207 c log q c (y|x) log(1 \u2212 p d (x, z))]. (8) Here, z = onehot(argmax y) is a discrete pseudo label where y \u223c q c (y|x). We define q c (y|x) as q c (y|x) = softmax log p c (y|x) + g \u03c4 , g \u223c Gumbel(0, 1).(9)\nHere, \u03c4 \u2208 (0, +\u221e) is a temperature parameter and Gumbel(0, 1) is the Gumbel distribution 2 [31]. We leverage the temperature parameter \u03c4 to control the variance of gradients over the training. With a high temperature, the samples from the concrete distribution are smooth, which give lowvariance gradient estimates. Note that a disadvantage of the concrete distribution is that with a high temperature, it becomes a less accurate approximation to the original categorical distribution, which causes biased gradient estimates. We will discuss how to tune the temperature parameter in Section 4.\n\nIn addition to improving the training of C, we also apply the same techniques to improve the training of T . We update D with the back-propagation algorithm [37] (detailed in Appendix B). The overall logic of the KDGAN training is summarized in Algorithm 1. The three players can be first pretrained separately and then trained alternatively via minibatch stochastic gradient descent.\n\n\nExperiments\n\nThe proposed KDGAN framework can be applied to a wide range of multi-label learning tasks where privileged provision is available. To show the applicability of KDGAN, we conduct experiments with the tasks of deep model compression (Section 4.1) and image tag recommendation (Section 4.2). Note that privileged provision is referred to as computational resources in deep model compression and privileged information in image tag recommendation, respectively.   \n\n\nDeep Model Compression\n\nDeep model compression aims to reduce the storage and runtime complexity of deep models and to improve the deployability of such models on portable devices such as smart phones. Extensive computational resources available for training are considered privileged provision in this task.\n\nDataset and Setup. We use the widely adopted MNIST [27] and CIFAR-10 [26] datasets. The MNIST dataset has 60,000 grayscale images (50,000 for training and 10,000 for testing) with 10 different label classes. Following an earlier work [39], we do not preprocess the images on MNIST. The CIFAR-10 dataset has 60,000 colored images (50,000 for training and 10,000 for testing) with 10 different label classes. We preprocess the images by subtracting per-pixel mean, and we augment the training data by mirrored images. We vary the number of training instances in [100, 10000] on MNIST and in [500, 50000] on CIFAR-10. The scoring functions h(x, y) and s(x, y) are implemented as an MLP (1.2M parameters) and a LeNet (3.1M parameters) on MNIST; while h(x, y) and s(x, y) are implemented as a LeNet (0.5M parameters) and a ResNet (1.7M parameters) on CIFAR-10 (detailed in Appendix C). We evaluate various methods over 10 runs with different initialization of C and report the mean accuracy and the standard deviation. Since the focus of this paper is to achieve a better accuracy for a given architecture of the classifier, we defer the discussion on the classifier's ratio of compression and loss of accuracy w.r.t. the teacher to Table 3 in Appendix D.\n\nResults and Discussions. First, we compare the proposed NaGAN and KDGAN with KD-based methods including MIMIC [7], DISTN [23], NOISY [39], and CODIS [2]. The results obtained by varying the number of training images on MNIST and CIFAR-10 are summarized in Table 1. On both datasets, KDGAN consistently outperforms the KD-based methods by a large margin. For example, KDGAN achieves as much as 5.31% performance gain with 100 training images on MNIST. We further compare NaGAN with the KD-based methods. We observe that NaGAN performs better when a large amount of training data are available (e.g., 50,000 training images on CIFAR-10) while KD-based methods perform better when a small number of training images are available (e.g., 500 training images on CIFAR-10). This is consistent with our analysis in Section 3.1 that NaGAN can learn the true data distribution better, although this requires a large amount of training data.\n\nThen, we compare NaGAN with KDGAN. As shown in Table 1, KDGAN achieves a larger performance gain over NaGAN with fewer training instances. This indicates that KDGAN requires a smaller number of training instances than NaGAN does to reach the same level of accuracy. This can be explained by that KDGAN introduces T to provide soft labels for training C. The soft labels generally have high entropy and reveal much useful information about each training instance. Hence, the soft labels impose much more constraint on the parameters of C than the true labels, which can reduce the number of training instances required to train C. We further investigate the training speed  Figure 3a. Due to the page limit, we only show the results using 100 training images on MNIST. We find that KDGAN converges to a better accuracy with a smaller number of training epochs (about 25 epochs) than NaGAN (about 135 epochs). After convergence, the training curve in KDGAN is more stable than that in NaGAN. Moreover, we investigate the benefit provided by the Gumbel-Max trick for the KDGAN training. We perform the KDGAN training without using the Gumbel-Max trick (referred to as KDGAN-WO-GM) and also plot the accuracy against training epochs in Figure 3a. By comparing KDGAN with KDGAN-WO-GM, we can see that the Gumbel-Max trick speeds up the training process by around 45% in terms of training epochs. The Gumbel-Max trick also helps improve the accuracy from 0.7605 to 0.7795 (by around 2.5%). One possible reason is that the Gumbel-Max trick effectively reduces the gradient variance from the discriminator as discussed in Section 3.3. This is also observed in our experiments, e.g., by comparing the gradient variance from the adversarial loss not using the Gumbel-Max trick in Figure  7a with the one using the Gumbel-Max trick in Figure 7b (see Appendix D for details).\n\nNext, we study the reasons for the higher accuracy of KDGAN. We present how the accuracy of KDGAN varies against the hyperparameters on the MNIST dataset in Figure 4 (Note the logarithmic scale of the x-axis in Figures 4b and 4c). We find that \u03b1 and \u03b2 have a relatively small effect on the accuracy, which suggests that KDGAN is a robust framework. Besides, if we set \u03b2 to a small value (0.0001), we get more than 2% accuracy drop when KDGAN is trained with 100 training instances. This shows that T is important in training C when the number of training instances is small. We further find that a large value of \u03b3 causes the accuracy to deteriorate rapidly. This is because the soft labels provided by C are usually noisy. Emphasizing on training T to predict the noisy labels decreases the accuracy of T , which in turn decreases the accuracy of C. We obtain similar results for the effects of the hyperparameters on the CIFAR-10 dataset.\n\n\nImage Tag Recommendation\n\nImage tag recommendation aims to recommend relevant tags (i.e., labels) after a user uploads an image to image-hosting websites such as Flickr 4 . As discussed before, we aim to recommend relevant tags right after a user uploads an image. This way, the user can just select from the recommended tags instead of inputting tags. Users may continue to add additional text for an uploaded image such as image titles and descriptions. We only use such additional text at the training stage as privileged information used by the teacher and the discriminator only. At the inference stage, our trained model (i.e., the classifier) only takes an image as input to make tag recommendations.\n\nDataset and Setup. We use the Yahoo Flickr Creative Commons 100 Million (YFCC100M) dataset 5 in the experiments [45]. To simulate the case where additional text about images is available for training, we randomly sample 20,000 images with titles or descriptions for training and another 2,000 images for testing. We create a dataset of images labeled with the 200 most popular tags and another dataset of images labeled with 200 randomly sampled tags. Following an earlier study [3], we use a VGGNet [40] pretrained on ImageNet [14] to extract image features and a LSTM [24] with pretrained word embeddings [34] to learn text features. We implement h(x, y) as an MLP with image features as input and implement s(x, y) as an MLP with the element-wise product of image and text features as input (detailed in Appendix C). We use precision (P@N), F-score (F@N), mean average precision (MAP), and mean reciprocal ranking (MRR) to evaluate performance.  Results and Discussions. First, we compare C in KDGAN with KNN [32], TPROP [19], TFEAT [11], and REXMP [28]. The overall results are presented in Table 2. We find that KDGAN achieves significant improvements over the other methods across all the measures. Although KDGAN does not explicitly model the semantic similarity between two labels like what REXMP does, it still makes better recommendations than REXMP does. The reason is that in KDGAN, T provides C with soft labels at training. The soft labels contain a rich similarity structure over tags which cannot be modeled well by any pairwise similarity between tags used in REXMP. For example, an image labeled with a tag volleyball is supplied with a soft label assigning a probability of 10 \u22122 to basketball, 10 \u22124 to baseball, and 10 \u22128 to dragonfly. The reason that T generalizes is reflected in the relative probabilities over tags, which can be used for guiding C to generalize better.\n\nNext, we compare the training curves of NaGAN, KDGAN-WO-GM, and KDGAN. We only plot the performance measured by P@3 in Figure 3b because the other measures exhibit similar training curves. We find that KDGAN learns a more accurate classifier with a smaller number of training epochs (about 100 epochs) than NaGAN (about 220 epochs) and KDGAN-WO-GM (about 150 epochs). After convergence, KDGAN consistently outperforms the best baseline REXMP.\n\nLast, we investigate how the performance of KDGAN varies against the hyperparameters over the YFCC100M dataset. The results are summarized in Figure 5, which are consistent with our observations in the task of deep model compression.\n\n\nConclusion\n\nWe proposed a framework named KDGAN to distill knowledge with generative adversarial networks for multi-label learning with privileged provision. We have defined the KDGAN framework as a minimax game where a classifier, a teacher, and a discriminator are trained adversarially. We have proved that the minimax game has an equilibrium where the classifier perfectly models the true data distribution. We use the concrete distribution to control the variance of gradients during the adversarial training and obtained low-variance gradient estimates to accelerate the training. We have shown that KDGAN outperforms the state-of-the-art methods in two important applications, image tag recommendation and deep model compression. We show that KDGAN learns a more accurate classifier at a faster speed than a naive GAN (NaGAN) does. For future work, we will explore adaptive methods for determining model hyperparameters to achieve better training dynamics.\n\nFigure 1 :\n1Image tag recommendation where the additional text is only available for training.\n\nLemma 4 . 1 .\n41For any fixed classifier and teacher, the value function U (c, t, d) is maximized if and only if the distribution of the discriminator is given by p d (x, y) = pu(y|x) /(pu(y|x)+p \u03b1 (y|x)).\n\nTheorem 4 . 2 .\n42The equilibrium of the minimax game min c,t max d U (c, t, d) is achieved if and only if p c (y|x) = p t (y|x) = p u (y|x). At that point, U (c, t, d) reaches the value \u2212 log(4).\n\nFigure 3 :\n3Training curves of the classifier in the proposed NaGAN and KDGAN.\n\nFigure 4 :\n4Effects of hyperparameters in KDGAN on MNIST for deep model compression. of NaGAN and KDGAN by the number of training epochs. Typical learning curves of C in NaGAN and KDGAN are shown in\n\nFigure 5 :\n5Effects of hyperparameters in KDGAN on YFCC100M for image tag recommendation.\n\nAlgorithm 1 :\n1Minibatch stochastic gradient descent training of KDGAN. Pretrain a classifier C, a teacher T , and a discriminator D with the trainingdata {(x1, y1), ..., (xn, yn)}.1 2 for the number of training epochs do \n\n3 \n\nfor the number of training steps for the discriminator do \n\n4 \n\n\n\nTable 1 :\n1Average accuracy over 10 runs in model compression (n is the number of training instances).Method \nMNIST \nCIFAR-10 \n\nn = 100 \nn = 1, 000 \nn = 10, 000 \nn = 500 \nn = 5, 000 \nn = 50, 000 \n\nCODIS \n74.02 \u00b1 0.13 95.77 \u00b1 0.10 98.89 \u00b1 0.08 54.17 \u00b1 0.20 77.82 \u00b1 0.14 85.12 \u00b1 0.11 \nDISTN \n68.34 \u00b1 0.06 93.97 \u00b1 0.08 98.79 \u00b1 0.07 50.92 \u00b1 0.18 76.59 \u00b1 0.15 83.32 \u00b1 0.08 \nNOISY \n66.53 \u00b1 0.18 93.45 \u00b1 0.11 98.58 \u00b1 0.11 50.18 \u00b1 0.28 75.42 \u00b1 0.19 82.99 \u00b1 0.12 \nMIMIC \n67.35 \u00b1 0.15 93.78 \u00b1 0.13 98.65 \u00b1 0.05 51.74 \u00b1 0.23 75.66 \u00b1 0.17 84.33 \u00b1 0.10 \n\nNaGAN 64.90 \u00b1 0.31 93.60 \u00b1 0.22 98.95 \u00b1 0.19 46.29 \u00b1 0.32 76.11 \u00b1 0.24 85.34 \u00b1 0.27 \nKDGAN 77.95 \u00b1 0.05 96.42 \u00b1 0.05 99.25 \u00b1 0.02 57.56 \u00b1 0.13 79.36 \u00b1 0.04 86.50 \u00b1 0.04 \n\n0 \n40 \n80 \n120 \n160 \n200 \n\nTraining Epochs \n\n\n\nTable 2 :\n2Performance of various methods on the YFCC100M dataset in tag recommendation. P@3 P@5 F@3 F@5 MAP MRR P@3 P@5 F@3 F@5 MAP MRR KNN .2320 .1680 .2339 .1633 .5755 .5852 .1623 .1198 .1575 .1088 .3970 .4092 TPROP .2420 .1636 .2811 .1949 .6177 .6270 .1883 .1372 .1810 .1252 .4512 .4636 TFEAT .2560 .1752 .2871 .1999 .6417 .6503 .2002 .1420 .2195 .1495 .5149 .5309 REXMP .2720 .1800 .3324 .2295 .7015 .7122 .2228 .1378 .2427 .1669 .5205 .5331 NaGAN .2892 .1880 .3516 .2352 .7432 .7555 .2415 .1495 .2693 .1867 .5791 .5911 KDGAN .3047 .1968 .3678 .2526 .7787 .7905 .2572 .1666 .2946 .2009 .6302 .6452Method \nMost Popular Tags \nRandomly Sampled Tags \n\n.80 \n.84 \n.88 \n\nP@3 \nF@3 \nMAP \nMRR \n\n0.0 0.2 0.4 0.6 0.8 1.0 \n\n\u03b1 \n\n.32 \n.36 \n.40 \n\nScore \n\n(a) Effect of varying \u03b1 \n\n.80 \n.84 \n.88 \n\nP@3 \nF@3 \nMAP \nMRR \n\n-3 -2 -1 \n0 \n1 \n2 \n3 \n\nlog 10 \u03b2 \n\n.32 \n.36 \n.40 \n\nScore \n\n(b) Effect of varying \u03b2 \n\n.72 \n\n.80 \n\n.88 \n\nP@3 \nF@3 \nMAP \nMRR \n\n-5 -4 -3 -2 -1 0 1 2 \n\nlog 10 \u03b3 \n\n.28 \n.34 \n.40 \n\nScore \n\n(c) Effect of varying \u03b3 \n\n\nThe Gumbel distribution can be sampled by drawing u \u223c Uniform(0, 1) and computing g = \u2212 log(\u2212 log u).3 The code and the data are made available at https://github.com/xiaojiew1/KDGAN/.\nhttps://www.flickr.com/. 5 Yahoo Webscope Program. http://webscope.sandbox.yahoo.com/.\nWe implement KDGAN based on Tensorflow[1]and here we briefly describe our experimental setup 3 . We use two formulations of the distillation losses including the L2 loss[7]and the Kullback-Leibler divergence[23]. The two formulations exhibit comparable results and the results presented are based on the L2 loss[7]. Since both T and D can use privileged provision, we implement their scoring functions f (x, y) and g(x, y) using the same function s(x, y) but with different sets of parameters. We search for the optimal values for the hyperparameters \u03b1 in [0.0, 1.0], \u03b2 in [0.001, 1000], and \u03b3 in [0.0001, 100] based on validation performance. We find that a reasonable annealing schedule for the temperature parameter \u03c4 is to start with a large value (1.0) and exponentially decay it to a small value (0.1). We leave the exploration of the optimal schedule for future work.AcknowledgementThis work is supported by Australian Research Council Future Fellowship Project FT120100832 and Discovery Project DP180102050. We thank the anonymous reviewers for their feedback on the paper. We have incorporated responses to reviewers' comments in the paper.\nTensorflow: a system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, OSDI. M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, et al. Tensorflow: a system for large-scale machine learning. In OSDI, 2016.\n\nLarge scale distributed neural network training through online distillation. R Anil, G Pereyra, A Passos, R Ormandi, G E Dahl, G E Hinton, In ICLR. R. Anil, G. Pereyra, A. Passos, R. Ormandi, G. E. Dahl, and G. E. Hinton. Large scale distributed neural network training through online distillation. In ICLR, 2018.\n\nVqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C Lawrence Zitnick, D Parikh, ICCV. S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh. Vqa: Visual question answering. In ICCV, 2015.\n\nTowards principled methods for training generative adversarial networks. M Arjovsky, L Bottou, ICLR. M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. In ICLR, 2017.\n\n. M Arjovsky, S Chintala, L Bottou, arXiv:1701.07875Wasserstein gan. arXiv preprintM. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.\n\nGeneralization and equilibrium in generative adversarial nets (gans). S Arora, R Ge, Y Liang, T Ma, Y Zhang, ICML. S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang. Generalization and equilibrium in generative adversarial nets (gans). In ICML, 2017.\n\nDo deep nets really need to be deep? In NeurIPS. J Ba, R Caruana, J. Ba and R. Caruana. Do deep nets really need to be deep? In NeurIPS, 2014.\n\nOptimization methods for large-scale machine learning. L Bottou, F E Curtis, J Nocedal, arXiv:1606.04838arXiv preprintL. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. arXiv preprint arXiv:1606.04838, 2016.\n\nC Bucilu\u01ce, R Caruana, A Niculescu-Mizil, SIGKDD. Model compressionC. Bucilu\u01ce, R. Caruana, and A. Niculescu-Mizil. Model compression. In SIGKDD, 2006.\n\nLearning efficient object detection models with knowledge distillation. G Chen, W Choi, X Yu, T Han, M Chandraker, NeurIPS. G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker. Learning efficient object detection models with knowledge distillation. In NeurIPS, 2017.\n\nTag-based image retrieval improved by augmented features and group-based refinement. L Chen, D Xu, I W Tsang, J Luo, IEEE Transactions on Multimedia. L. Chen, D. Xu, I. W. Tsang, and J. Luo. Tag-based image retrieval improved by augmented features and group-based refinement. IEEE Transactions on Multimedia, 2012.\n\nLabel ranking methods based on the plackett-luce model. W Cheng, E H\u00fcllermeier, K J Dembczynski, ICML. W. Cheng, E. H\u00fcllermeier, and K. J. Dembczynski. Label ranking methods based on the plackett-luce model. In ICML, 2010.\n\nTriple generative adversarial nets. L Chongxuan, T Xu, J Zhu, B Zhang, NeurIPS. L. Chongxuan, T. Xu, J. Zhu, and B. Zhang. Triple generative adversarial nets. In NeurIPS, 2017.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\n\nUnderstanding gans: the lqg setting. S Feizi, C Suh, F Xia, D Tse, arXiv:1710.10793arXiv preprintS. Feizi, C. Suh, F. Xia, and D. Tse. Understanding gans: the lqg setting. arXiv preprint arXiv:1710.10793, 2017.\n\nTriangle generative adversarial networks. Z Gan, L Chen, W Wang, Y Pu, Y Zhang, H Liu, C Li, L Carin, NeurIPS. Z. Gan, L. Chen, W. Wang, Y. Pu, Y. Zhang, H. Liu, C. Li, and L. Carin. Triangle generative adversarial networks. In NeurIPS, 2017.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, NeurIPS. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NeurIPS, 2014.\n\nSupervised clustering of label ranking data using label preference information. Machine learning. M Grbovic, N Djuric, S Guo, S Vucetic, M. Grbovic, N. Djuric, S. Guo, and S. Vucetic. Supervised clustering of label ranking data using label preference information. Machine learning, 2013.\n\nTagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation. M Guillaumin, T Mensink, J Verbeek, C Schmid, ICCV. M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation. In ICCV, 2009.\n\nStatistical theory of extreme values and some practical applications: A series of lectures. US Government Printing Office. E Gumbel, WashingtonE. Gumbel. Statistical theory of extreme values and some practical applications: A series of lectures. US Government Printing Office, Washington, 1954.\n\nCross modal distillation for supervision transfer. S Gupta, J Hoffman, J Malik, CVPR. S. Gupta, J. Hoffman, and J. Malik. Cross modal distillation for supervision transfer. In CVPR, 2016.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, NeurIPS workshop. G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NeurIPS workshop, 2014.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 1997.\n\nCategorical reparameterization with gumbel-softmax. E Jang, S Gu, B Poole, ICLR. E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. In ICLR, 2017.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, University of TorontoTechnical reportA. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. the IEEEY. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.\n\nClassifying tag relevance with relevant positive and negative examples. X Li, C G Snoek, ACMMM. X. Li and C. G. Snoek. Classifying tag relevance with relevant positive and negative examples. In ACMMM, 2013.\n\nUnifying distillation and privileged information. D Lopez-Paz, L Bottou, B Sch\u00f6lkopf, V Vapnik, ICLR. D. Lopez-Paz, L. Bottou, B. Sch\u00f6lkopf, and V. Vapnik. Unifying distillation and privileged information. In ICLR, 2016.\n\nA* sampling. C J Maddison, D Tarlow, T Minka, NeurIPS. C. J. Maddison, D. Tarlow, and T. Minka. A* sampling. In NeurIPS, 2014.\n\nThe concrete distribution: A continuous relaxation of discrete random variables. C J Maddison, A Mnih, Y W Teh, C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete random variables. In ICLR, 2017.\n\nBaselines for image annotation. A Makadia, V Pavlovic, S Kumar, IJCVA. Makadia, V. Pavlovic, and S. Kumar. Baselines for image annotation. IJCV, 2010.\n\nUnrolled generative adversarial networks. L Metz, B Poole, D Pfau, J Sohl-Dickstein, ICLR. L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks. In ICLR, 2017.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, NeurIPS. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NeurIPS, 2013.\n\nOn the theory of learnining with privileged information. D Pechyony, V Vapnik, NeurIPS. D. Pechyony and V. Vapnik. On the theory of learnining with privileged information. In NeurIPS, 2010.\n\nA Romero, N Ballas, S E Kahou, A Chassang, C Gatta, Y Bengio, arXiv:1412.6550Fitnets: Hints for thin deep nets. arXiv preprintA. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\n\nLearning internal representations by error propagation. D E Rumelhart, G E Hinton, R J Williams, California Univ San Diego La Jolla Inst for Cognitive ScienceTechnical reportD. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.\n\nImproved techniques for training gans. T Salimans, I Goodfellow, W Zaremba, V Cheung, A Radford, X Chen, NeurIPS. T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training gans. In NeurIPS, 2016.\n\nB B Sau, V N Balasubramanian, arXiv:1610.09650Deep model compression: Distilling knowledge from noisy teachers. arXiv preprintB. B. Sau and V. N. Balasubramanian. Deep model compression: Distilling knowledge from noisy teachers. arXiv preprint arXiv:1610.09650, 2016.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.\n\n. J.-C Su, S Maji, arXiv:1604.00433Cross quality distillation. arXiv preprintJ.-C. Su and S. Maji. Cross quality distillation. arXiv preprint arXiv:1604.00433, 2016.\n\nContextual intent tracking for personal assistants. Y Sun, N J Yuan, Y Wang, X Xie, K Mcdonald, R Zhang, SIGKDD. Y. Sun, N. J. Yuan, Y. Wang, X. Xie, K. McDonald, and R. Zhang. Contextual intent tracking for personal assistants. In SIGKDD, 2016.\n\nCollaborative nowcasting for contextual recommendation. Y Sun, N J Yuan, X Xie, K Mcdonald, R Zhang, WWWY. Sun, N. J. Yuan, X. Xie, K. McDonald, and R. Zhang. Collaborative nowcasting for contextual recommendation. In WWW, 2016.\n\nCollaborative intent prediction with real-time contextual data. Y Sun, N J Yuan, X Xie, K Mcdonald, R Zhang, Y. Sun, N. J. Yuan, X. Xie, K. McDonald, and R. Zhang. Collaborative intent prediction with real-time contextual data. TOIS, 2017.\n\nYfcc100m: the new data in multimedia research. B Thomee, D A Shamma, G Friedland, B Elizalde, K Ni, D Poland, D Borth, L.-J Li, Communications of the ACM. B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li. Yfcc100m: the new data in multimedia research. Communications of the ACM, 2016.\n\nRebar: Low-variance, unbiased gradient estimates for discrete latent variable models. G Tucker, A Mnih, C J Maddison, J Lawson, J Sohl-Dickstein, NeurIPS. G. Tucker, A. Mnih, C. J. Maddison, J. Lawson, and J. Sohl-Dickstein. Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models. In NeurIPS, 2017.\n\nLearning using privileged information: similarity control and knowledge transfer. V Vapnik, R Izmailov, JMLR. V. Vapnik and R. Izmailov. Learning using privileged information: similarity control and knowledge transfer. JMLR, 2015.\n\nA new learning paradigm: Learning using privileged information. V Vapnik, A Vashist, V. Vapnik and A. Vashist. A new learning paradigm: Learning using privileged information. Neural networks, 2009.\n\nIrgan: A minimax game for unifying generative and discriminative information retrieval models. J Wang, L Yu, W Zhang, Y Gong, Y Xu, B Wang, P Zhang, D Zhang, SIGIR. J. Wang, L. Yu, W. Zhang, Y. Gong, Y. Xu, B. Wang, P. Zhang, and D. Zhang. Irgan: A minimax game for unifying generative and discriminative information retrieval models. In SIGIR, 2017.\n\nA joint optimization approach for personalized recommendation diversification. X Wang, J Qi, K Ramamohanarao, Y Sun, B Li, R Zhang, PAKDD. X. Wang, J. Qi, K. Ramamohanarao, Y. Sun, B. Li, and R. Zhang. A joint optimization approach for personalized recommendation diversification. In PAKDD, 2018.\n\nLearning loss for knowledge distillation with conditional adversarial networks. Z Xu, Y.-C Hsu, J Huang, arXiv:1709.00513arXiv preprintZ. Xu, Y.-C. Hsu, and J. Huang. Learning loss for knowledge distillation with conditional adversarial networks. arXiv preprint arXiv:1709.00513, 2017.\n\nSeqgan: Sequence generative adversarial nets with policy gradient. L Yu, W Zhang, J Wang, Y Yu, AAAI. L. Yu, W. Zhang, J. Wang, and Y. Yu. Seqgan: Sequence generative adversarial nets with policy gradient. In AAAI, 2017.\n\nA review on multi-label learning algorithms. M.-L Zhang, Z.-H Zhou, TKDEM.-L. Zhang and Z.-H. Zhou. A review on multi-label learning algorithms. TKDE, 2014.\n\nGenerating text via adversarial training. Y Zhang, Z Gan, L Carin, NeurIPS workshop on Adversarial Training. Y. Zhang, Z. Gan, and L. Carin. Generating text via adversarial training. In NeurIPS workshop on Adversarial Training, 2016.\n\nAdversarial feature matching for text generation. Y Zhang, Z Gan, K Fan, Z Chen, R Henao, D Shen, L Carin, Y. Zhang, Z. Gan, K. Fan, Z. Chen, R. Henao, D. Shen, and L. Carin. Adversarial feature matching for text generation. In ICML, 2017.\n\nEnergy-based generative adversarial network. J Zhao, M Mathieu, Y Lecun, ICLR. J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. In ICLR, 2017.\n", "annotations": {"author": "[{\"end\":190,\"start\":70},{\"end\":312,\"start\":191},{\"end\":423,\"start\":313},{\"end\":551,\"start\":424}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":78},{\"end\":200,\"start\":195},{\"end\":319,\"start\":316},{\"end\":436,\"start\":434}]", "author_first_name": "[{\"end\":77,\"start\":70},{\"end\":194,\"start\":191},{\"end\":315,\"start\":313},{\"end\":433,\"start\":424}]", "author_affiliation": "[{\"end\":189,\"start\":105},{\"end\":311,\"start\":227},{\"end\":422,\"start\":338},{\"end\":550,\"start\":466}]", "title": "[{\"end\":67,\"start\":1},{\"end\":618,\"start\":552}]", "venue": null, "abstract": "[{\"end\":935,\"start\":620}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1032,\"start\":1028},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1064,\"start\":1060},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1377,\"start\":1373},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2505,\"start\":2501},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2586,\"start\":2583},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2588,\"start\":2586},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2591,\"start\":2588},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2678,\"start\":2674},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2992,\"start\":2988},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3145,\"start\":3142},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3251,\"start\":3247},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3551,\"start\":3547},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4131,\"start\":4127},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4410,\"start\":4407},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6001,\"start\":5997},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6004,\"start\":6001},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6321,\"start\":6317},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6324,\"start\":6321},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6355,\"start\":6351},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6358,\"start\":6355},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7452,\"start\":7449},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7485,\"start\":7482},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7607,\"start\":7603},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7728,\"start\":7724},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7872,\"start\":7868},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8003,\"start\":7999},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8155,\"start\":8151},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8211,\"start\":8207},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8234,\"start\":8230},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8276,\"start\":8272},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8279,\"start\":8276},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8282,\"start\":8279},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8681,\"start\":8677},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8751,\"start\":8747},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8754,\"start\":8751},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8757,\"start\":8754},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8909,\"start\":8905},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9143,\"start\":9139},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9819,\"start\":9815},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9822,\"start\":9819},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9970,\"start\":9967},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9973,\"start\":9970},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9976,\"start\":9973},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10006,\"start\":10002},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10178,\"start\":10174},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10262,\"start\":10258},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10676,\"start\":10672},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10761,\"start\":10757},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10764,\"start\":10761},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10821,\"start\":10817},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11192,\"start\":11188},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11297,\"start\":11293},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11327,\"start\":11323},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11370,\"start\":11366},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11373,\"start\":11370},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":11376,\"start\":11373},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12527,\"start\":12523},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12611,\"start\":12607},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12658,\"start\":12655},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13519,\"start\":13515},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13946,\"start\":13942},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14102,\"start\":14098},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14323,\"start\":14320},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14628,\"start\":14624},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15838,\"start\":15837},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16040,\"start\":16039},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16252,\"start\":16250},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16310,\"start\":16308},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17141,\"start\":17137},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17164,\"start\":17161},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17200,\"start\":17196},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18925,\"start\":18922},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":18928,\"start\":18925},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19635,\"start\":19631},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19638,\"start\":19635},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20034,\"start\":20030},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20037,\"start\":20034},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20585,\"start\":20581},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20588,\"start\":20585},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20920,\"start\":20916},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20923,\"start\":20920},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21481,\"start\":21477},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22142,\"start\":22138},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23209,\"start\":23205},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23227,\"start\":23223},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23392,\"start\":23388},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24519,\"start\":24516},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24531,\"start\":24527},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24543,\"start\":24539},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24558,\"start\":24555},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28947,\"start\":28946},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28971,\"start\":28967},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29337,\"start\":29334},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":29359,\"start\":29355},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29387,\"start\":29383},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29429,\"start\":29425},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29466,\"start\":29462},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29871,\"start\":29867},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29883,\"start\":29879},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29895,\"start\":29891},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29911,\"start\":29907},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35439,\"start\":35438}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32490,\"start\":32395},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32697,\"start\":32491},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32895,\"start\":32698},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32975,\"start\":32896},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33175,\"start\":32976},{\"attributes\":{\"id\":\"fig_6\"},\"end\":33266,\"start\":33176},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33560,\"start\":33267},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34320,\"start\":33561},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35336,\"start\":34321}]", "paragraph": "[{\"end\":2506,\"start\":951},{\"end\":3419,\"start\":2508},{\"end\":4531,\"start\":3421},{\"end\":5753,\"start\":4533},{\"end\":6640,\"start\":5755},{\"end\":6689,\"start\":6642},{\"end\":7247,\"start\":6691},{\"end\":7363,\"start\":7264},{\"end\":8060,\"start\":7365},{\"end\":8545,\"start\":8062},{\"end\":10090,\"start\":8547},{\"end\":10469,\"start\":10092},{\"end\":10846,\"start\":10471},{\"end\":11453,\"start\":10858},{\"end\":12007,\"start\":11476},{\"end\":12328,\"start\":12136},{\"end\":12822,\"start\":12330},{\"end\":12934,\"start\":12844},{\"end\":13591,\"start\":12936},{\"end\":13837,\"start\":13674},{\"end\":14324,\"start\":13839},{\"end\":14560,\"start\":14326},{\"end\":14798,\"start\":14562},{\"end\":15017,\"start\":14820},{\"end\":15880,\"start\":15063},{\"end\":16148,\"start\":15987},{\"end\":16406,\"start\":16238},{\"end\":16688,\"start\":16500},{\"end\":17602,\"start\":16882},{\"end\":18039,\"start\":17840},{\"end\":18737,\"start\":18231},{\"end\":19639,\"start\":18756},{\"end\":19830,\"start\":19641},{\"end\":20393,\"start\":19897},{\"end\":21124,\"start\":20462},{\"end\":21979,\"start\":21386},{\"end\":22365,\"start\":21981},{\"end\":22841,\"start\":22381},{\"end\":23152,\"start\":22868},{\"end\":24404,\"start\":23154},{\"end\":25336,\"start\":24406},{\"end\":27201,\"start\":25338},{\"end\":28143,\"start\":27203},{\"end\":28853,\"start\":28172},{\"end\":30749,\"start\":28855},{\"end\":31193,\"start\":30751},{\"end\":31428,\"start\":31195},{\"end\":32394,\"start\":31443}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12135,\"start\":12008},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13673,\"start\":13592},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15062,\"start\":15018},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15986,\"start\":15881},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16237,\"start\":16149},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16499,\"start\":16407},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16881,\"start\":16689},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17839,\"start\":17603},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18230,\"start\":18040},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19896,\"start\":19831},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20461,\"start\":20394},{\"attributes\":{\"id\":\"formula_12\"},\"end\":21385,\"start\":21125}]", "table_ref": "[{\"end\":24389,\"start\":24382},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24669,\"start\":24662},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25392,\"start\":25385},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29957,\"start\":29950}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":949,\"start\":937},{\"attributes\":{\"n\":\"2\"},\"end\":7262,\"start\":7250},{\"attributes\":{\"n\":\"3\"},\"end\":10856,\"start\":10849},{\"end\":11474,\"start\":11456},{\"attributes\":{\"n\":\"3.1\"},\"end\":12842,\"start\":12825},{\"attributes\":{\"n\":\"3.2\"},\"end\":14818,\"start\":14801},{\"attributes\":{\"n\":\"3.3\"},\"end\":18754,\"start\":18740},{\"attributes\":{\"n\":\"4\"},\"end\":22379,\"start\":22368},{\"attributes\":{\"n\":\"4.1\"},\"end\":22866,\"start\":22844},{\"attributes\":{\"n\":\"4.2\"},\"end\":28170,\"start\":28146},{\"attributes\":{\"n\":\"5\"},\"end\":31441,\"start\":31431},{\"end\":32406,\"start\":32396},{\"end\":32505,\"start\":32492},{\"end\":32714,\"start\":32699},{\"end\":32907,\"start\":32897},{\"end\":32987,\"start\":32977},{\"end\":33187,\"start\":33177},{\"end\":33281,\"start\":33268},{\"end\":33571,\"start\":33562},{\"end\":34331,\"start\":34322}]", "table": "[{\"end\":33560,\"start\":33449},{\"end\":34320,\"start\":33664},{\"end\":35336,\"start\":34924}]", "figure_caption": "[{\"end\":32490,\"start\":32408},{\"end\":32697,\"start\":32508},{\"end\":32895,\"start\":32717},{\"end\":32975,\"start\":32909},{\"end\":33175,\"start\":32989},{\"end\":33266,\"start\":33189},{\"end\":33449,\"start\":33283},{\"end\":33664,\"start\":33573},{\"end\":34924,\"start\":34333}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1257,\"start\":1249},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1594,\"start\":1585},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1682,\"start\":1673},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1949,\"start\":1940},{\"end\":2630,\"start\":2622},{\"end\":3646,\"start\":3638},{\"end\":4687,\"start\":4679},{\"end\":11498,\"start\":11490},{\"end\":15646,\"start\":15638},{\"end\":20129,\"start\":20121},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26020,\"start\":26011},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26579,\"start\":26570},{\"end\":27118,\"start\":27108},{\"end\":27171,\"start\":27162},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27368,\"start\":27360},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27431,\"start\":27414},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30879,\"start\":30870},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31345,\"start\":31337}]", "bib_author_first_name": "[{\"end\":36814,\"start\":36813},{\"end\":36823,\"start\":36822},{\"end\":36833,\"start\":36832},{\"end\":36841,\"start\":36840},{\"end\":36849,\"start\":36848},{\"end\":36858,\"start\":36857},{\"end\":36866,\"start\":36865},{\"end\":36875,\"start\":36874},{\"end\":36887,\"start\":36886},{\"end\":36897,\"start\":36896},{\"end\":37169,\"start\":37168},{\"end\":37177,\"start\":37176},{\"end\":37188,\"start\":37187},{\"end\":37198,\"start\":37197},{\"end\":37209,\"start\":37208},{\"end\":37211,\"start\":37210},{\"end\":37219,\"start\":37218},{\"end\":37221,\"start\":37220},{\"end\":37439,\"start\":37438},{\"end\":37448,\"start\":37447},{\"end\":37459,\"start\":37458},{\"end\":37465,\"start\":37464},{\"end\":37477,\"start\":37476},{\"end\":37486,\"start\":37485},{\"end\":37495,\"start\":37487},{\"end\":37506,\"start\":37505},{\"end\":37731,\"start\":37730},{\"end\":37743,\"start\":37742},{\"end\":37877,\"start\":37876},{\"end\":37889,\"start\":37888},{\"end\":37901,\"start\":37900},{\"end\":38126,\"start\":38125},{\"end\":38135,\"start\":38134},{\"end\":38141,\"start\":38140},{\"end\":38150,\"start\":38149},{\"end\":38156,\"start\":38155},{\"end\":38354,\"start\":38353},{\"end\":38360,\"start\":38359},{\"end\":38504,\"start\":38503},{\"end\":38514,\"start\":38513},{\"end\":38516,\"start\":38515},{\"end\":38526,\"start\":38525},{\"end\":38703,\"start\":38702},{\"end\":38714,\"start\":38713},{\"end\":38725,\"start\":38724},{\"end\":38926,\"start\":38925},{\"end\":38934,\"start\":38933},{\"end\":38942,\"start\":38941},{\"end\":38948,\"start\":38947},{\"end\":38955,\"start\":38954},{\"end\":39206,\"start\":39205},{\"end\":39214,\"start\":39213},{\"end\":39220,\"start\":39219},{\"end\":39222,\"start\":39221},{\"end\":39231,\"start\":39230},{\"end\":39493,\"start\":39492},{\"end\":39502,\"start\":39501},{\"end\":39517,\"start\":39516},{\"end\":39519,\"start\":39518},{\"end\":39697,\"start\":39696},{\"end\":39710,\"start\":39709},{\"end\":39716,\"start\":39715},{\"end\":39723,\"start\":39722},{\"end\":39892,\"start\":39891},{\"end\":39900,\"start\":39899},{\"end\":39908,\"start\":39907},{\"end\":39921,\"start\":39917},{\"end\":39927,\"start\":39926},{\"end\":39933,\"start\":39932},{\"end\":40118,\"start\":40117},{\"end\":40127,\"start\":40126},{\"end\":40134,\"start\":40133},{\"end\":40141,\"start\":40140},{\"end\":40335,\"start\":40334},{\"end\":40342,\"start\":40341},{\"end\":40350,\"start\":40349},{\"end\":40358,\"start\":40357},{\"end\":40364,\"start\":40363},{\"end\":40373,\"start\":40372},{\"end\":40380,\"start\":40379},{\"end\":40386,\"start\":40385},{\"end\":40566,\"start\":40565},{\"end\":40580,\"start\":40579},{\"end\":40597,\"start\":40596},{\"end\":40606,\"start\":40605},{\"end\":40612,\"start\":40611},{\"end\":40628,\"start\":40627},{\"end\":40637,\"start\":40636},{\"end\":40650,\"start\":40649},{\"end\":40921,\"start\":40920},{\"end\":40932,\"start\":40931},{\"end\":40942,\"start\":40941},{\"end\":40949,\"start\":40948},{\"end\":41206,\"start\":41205},{\"end\":41220,\"start\":41219},{\"end\":41231,\"start\":41230},{\"end\":41242,\"start\":41241},{\"end\":41545,\"start\":41544},{\"end\":41769,\"start\":41768},{\"end\":41778,\"start\":41777},{\"end\":41789,\"start\":41788},{\"end\":41953,\"start\":41952},{\"end\":41959,\"start\":41958},{\"end\":41968,\"start\":41967},{\"end\":41975,\"start\":41974},{\"end\":42133,\"start\":42132},{\"end\":42143,\"start\":42142},{\"end\":42154,\"start\":42153},{\"end\":42314,\"start\":42313},{\"end\":42328,\"start\":42327},{\"end\":42500,\"start\":42499},{\"end\":42508,\"start\":42507},{\"end\":42514,\"start\":42513},{\"end\":42682,\"start\":42681},{\"end\":42696,\"start\":42695},{\"end\":42932,\"start\":42931},{\"end\":42941,\"start\":42940},{\"end\":42951,\"start\":42950},{\"end\":42961,\"start\":42960},{\"end\":43214,\"start\":43213},{\"end\":43220,\"start\":43219},{\"end\":43222,\"start\":43221},{\"end\":43400,\"start\":43399},{\"end\":43413,\"start\":43412},{\"end\":43423,\"start\":43422},{\"end\":43436,\"start\":43435},{\"end\":43585,\"start\":43584},{\"end\":43587,\"start\":43586},{\"end\":43599,\"start\":43598},{\"end\":43609,\"start\":43608},{\"end\":43781,\"start\":43780},{\"end\":43783,\"start\":43782},{\"end\":43795,\"start\":43794},{\"end\":43803,\"start\":43802},{\"end\":43805,\"start\":43804},{\"end\":43981,\"start\":43980},{\"end\":43992,\"start\":43991},{\"end\":44004,\"start\":44003},{\"end\":44143,\"start\":44142},{\"end\":44151,\"start\":44150},{\"end\":44160,\"start\":44159},{\"end\":44168,\"start\":44167},{\"end\":44378,\"start\":44377},{\"end\":44389,\"start\":44388},{\"end\":44402,\"start\":44401},{\"end\":44410,\"start\":44409},{\"end\":44412,\"start\":44411},{\"end\":44423,\"start\":44422},{\"end\":44656,\"start\":44655},{\"end\":44668,\"start\":44667},{\"end\":44790,\"start\":44789},{\"end\":44800,\"start\":44799},{\"end\":44810,\"start\":44809},{\"end\":44812,\"start\":44811},{\"end\":44821,\"start\":44820},{\"end\":44833,\"start\":44832},{\"end\":44842,\"start\":44841},{\"end\":45119,\"start\":45118},{\"end\":45121,\"start\":45120},{\"end\":45134,\"start\":45133},{\"end\":45136,\"start\":45135},{\"end\":45146,\"start\":45145},{\"end\":45148,\"start\":45147},{\"end\":45471,\"start\":45470},{\"end\":45483,\"start\":45482},{\"end\":45497,\"start\":45496},{\"end\":45508,\"start\":45507},{\"end\":45518,\"start\":45517},{\"end\":45529,\"start\":45528},{\"end\":45680,\"start\":45679},{\"end\":45682,\"start\":45681},{\"end\":45689,\"start\":45688},{\"end\":45691,\"start\":45690},{\"end\":46017,\"start\":46016},{\"end\":46029,\"start\":46028},{\"end\":46167,\"start\":46163},{\"end\":46173,\"start\":46172},{\"end\":46381,\"start\":46380},{\"end\":46388,\"start\":46387},{\"end\":46390,\"start\":46389},{\"end\":46398,\"start\":46397},{\"end\":46406,\"start\":46405},{\"end\":46413,\"start\":46412},{\"end\":46425,\"start\":46424},{\"end\":46632,\"start\":46631},{\"end\":46639,\"start\":46638},{\"end\":46641,\"start\":46640},{\"end\":46649,\"start\":46648},{\"end\":46656,\"start\":46655},{\"end\":46668,\"start\":46667},{\"end\":46870,\"start\":46869},{\"end\":46877,\"start\":46876},{\"end\":46879,\"start\":46878},{\"end\":46887,\"start\":46886},{\"end\":46894,\"start\":46893},{\"end\":46906,\"start\":46905},{\"end\":47094,\"start\":47093},{\"end\":47104,\"start\":47103},{\"end\":47106,\"start\":47105},{\"end\":47116,\"start\":47115},{\"end\":47129,\"start\":47128},{\"end\":47141,\"start\":47140},{\"end\":47147,\"start\":47146},{\"end\":47157,\"start\":47156},{\"end\":47169,\"start\":47165},{\"end\":47463,\"start\":47462},{\"end\":47473,\"start\":47472},{\"end\":47481,\"start\":47480},{\"end\":47483,\"start\":47482},{\"end\":47495,\"start\":47494},{\"end\":47505,\"start\":47504},{\"end\":47789,\"start\":47788},{\"end\":47799,\"start\":47798},{\"end\":48003,\"start\":48002},{\"end\":48013,\"start\":48012},{\"end\":48233,\"start\":48232},{\"end\":48241,\"start\":48240},{\"end\":48247,\"start\":48246},{\"end\":48256,\"start\":48255},{\"end\":48264,\"start\":48263},{\"end\":48270,\"start\":48269},{\"end\":48278,\"start\":48277},{\"end\":48287,\"start\":48286},{\"end\":48569,\"start\":48568},{\"end\":48577,\"start\":48576},{\"end\":48583,\"start\":48582},{\"end\":48600,\"start\":48599},{\"end\":48607,\"start\":48606},{\"end\":48613,\"start\":48612},{\"end\":48868,\"start\":48867},{\"end\":48877,\"start\":48873},{\"end\":48884,\"start\":48883},{\"end\":49142,\"start\":49141},{\"end\":49148,\"start\":49147},{\"end\":49157,\"start\":49156},{\"end\":49165,\"start\":49164},{\"end\":49345,\"start\":49341},{\"end\":49357,\"start\":49353},{\"end\":49497,\"start\":49496},{\"end\":49506,\"start\":49505},{\"end\":49513,\"start\":49512},{\"end\":49740,\"start\":49739},{\"end\":49749,\"start\":49748},{\"end\":49756,\"start\":49755},{\"end\":49763,\"start\":49762},{\"end\":49771,\"start\":49770},{\"end\":49780,\"start\":49779},{\"end\":49788,\"start\":49787},{\"end\":49976,\"start\":49975},{\"end\":49984,\"start\":49983},{\"end\":49995,\"start\":49994}]", "bib_author_last_name": "[{\"end\":36820,\"start\":36815},{\"end\":36830,\"start\":36824},{\"end\":36838,\"start\":36834},{\"end\":36846,\"start\":36842},{\"end\":36855,\"start\":36850},{\"end\":36863,\"start\":36859},{\"end\":36872,\"start\":36867},{\"end\":36884,\"start\":36876},{\"end\":36894,\"start\":36888},{\"end\":36903,\"start\":36898},{\"end\":37174,\"start\":37170},{\"end\":37185,\"start\":37178},{\"end\":37195,\"start\":37189},{\"end\":37206,\"start\":37199},{\"end\":37216,\"start\":37212},{\"end\":37228,\"start\":37222},{\"end\":37445,\"start\":37440},{\"end\":37456,\"start\":37449},{\"end\":37462,\"start\":37460},{\"end\":37474,\"start\":37466},{\"end\":37483,\"start\":37478},{\"end\":37503,\"start\":37496},{\"end\":37513,\"start\":37507},{\"end\":37740,\"start\":37732},{\"end\":37750,\"start\":37744},{\"end\":37886,\"start\":37878},{\"end\":37898,\"start\":37890},{\"end\":37908,\"start\":37902},{\"end\":38132,\"start\":38127},{\"end\":38138,\"start\":38136},{\"end\":38147,\"start\":38142},{\"end\":38153,\"start\":38151},{\"end\":38162,\"start\":38157},{\"end\":38357,\"start\":38355},{\"end\":38368,\"start\":38361},{\"end\":38511,\"start\":38505},{\"end\":38523,\"start\":38517},{\"end\":38534,\"start\":38527},{\"end\":38711,\"start\":38704},{\"end\":38722,\"start\":38715},{\"end\":38741,\"start\":38726},{\"end\":38931,\"start\":38927},{\"end\":38939,\"start\":38935},{\"end\":38945,\"start\":38943},{\"end\":38952,\"start\":38949},{\"end\":38966,\"start\":38956},{\"end\":39211,\"start\":39207},{\"end\":39217,\"start\":39215},{\"end\":39228,\"start\":39223},{\"end\":39235,\"start\":39232},{\"end\":39499,\"start\":39494},{\"end\":39514,\"start\":39503},{\"end\":39531,\"start\":39520},{\"end\":39707,\"start\":39698},{\"end\":39713,\"start\":39711},{\"end\":39720,\"start\":39717},{\"end\":39729,\"start\":39724},{\"end\":39897,\"start\":39893},{\"end\":39905,\"start\":39901},{\"end\":39915,\"start\":39909},{\"end\":39924,\"start\":39922},{\"end\":39930,\"start\":39928},{\"end\":39941,\"start\":39934},{\"end\":40124,\"start\":40119},{\"end\":40131,\"start\":40128},{\"end\":40138,\"start\":40135},{\"end\":40145,\"start\":40142},{\"end\":40339,\"start\":40336},{\"end\":40347,\"start\":40343},{\"end\":40355,\"start\":40351},{\"end\":40361,\"start\":40359},{\"end\":40370,\"start\":40365},{\"end\":40377,\"start\":40374},{\"end\":40383,\"start\":40381},{\"end\":40392,\"start\":40387},{\"end\":40577,\"start\":40567},{\"end\":40594,\"start\":40581},{\"end\":40603,\"start\":40598},{\"end\":40609,\"start\":40607},{\"end\":40625,\"start\":40613},{\"end\":40634,\"start\":40629},{\"end\":40647,\"start\":40638},{\"end\":40657,\"start\":40651},{\"end\":40929,\"start\":40922},{\"end\":40939,\"start\":40933},{\"end\":40946,\"start\":40943},{\"end\":40957,\"start\":40950},{\"end\":41217,\"start\":41207},{\"end\":41228,\"start\":41221},{\"end\":41239,\"start\":41232},{\"end\":41249,\"start\":41243},{\"end\":41552,\"start\":41546},{\"end\":41775,\"start\":41770},{\"end\":41786,\"start\":41779},{\"end\":41795,\"start\":41790},{\"end\":41956,\"start\":41954},{\"end\":41965,\"start\":41960},{\"end\":41972,\"start\":41969},{\"end\":41979,\"start\":41976},{\"end\":42140,\"start\":42134},{\"end\":42151,\"start\":42144},{\"end\":42159,\"start\":42155},{\"end\":42325,\"start\":42315},{\"end\":42340,\"start\":42329},{\"end\":42505,\"start\":42501},{\"end\":42511,\"start\":42509},{\"end\":42520,\"start\":42515},{\"end\":42693,\"start\":42683},{\"end\":42703,\"start\":42697},{\"end\":42938,\"start\":42933},{\"end\":42948,\"start\":42942},{\"end\":42958,\"start\":42952},{\"end\":42969,\"start\":42962},{\"end\":43217,\"start\":43215},{\"end\":43228,\"start\":43223},{\"end\":43410,\"start\":43401},{\"end\":43420,\"start\":43414},{\"end\":43433,\"start\":43424},{\"end\":43443,\"start\":43437},{\"end\":43596,\"start\":43588},{\"end\":43606,\"start\":43600},{\"end\":43615,\"start\":43610},{\"end\":43792,\"start\":43784},{\"end\":43800,\"start\":43796},{\"end\":43809,\"start\":43806},{\"end\":43989,\"start\":43982},{\"end\":44001,\"start\":43993},{\"end\":44010,\"start\":44005},{\"end\":44148,\"start\":44144},{\"end\":44157,\"start\":44152},{\"end\":44165,\"start\":44161},{\"end\":44183,\"start\":44169},{\"end\":44386,\"start\":44379},{\"end\":44399,\"start\":44390},{\"end\":44407,\"start\":44403},{\"end\":44420,\"start\":44413},{\"end\":44428,\"start\":44424},{\"end\":44665,\"start\":44657},{\"end\":44675,\"start\":44669},{\"end\":44797,\"start\":44791},{\"end\":44807,\"start\":44801},{\"end\":44818,\"start\":44813},{\"end\":44830,\"start\":44822},{\"end\":44839,\"start\":44834},{\"end\":44849,\"start\":44843},{\"end\":45131,\"start\":45122},{\"end\":45143,\"start\":45137},{\"end\":45157,\"start\":45149},{\"end\":45480,\"start\":45472},{\"end\":45494,\"start\":45484},{\"end\":45505,\"start\":45498},{\"end\":45515,\"start\":45509},{\"end\":45526,\"start\":45519},{\"end\":45534,\"start\":45530},{\"end\":45686,\"start\":45683},{\"end\":45707,\"start\":45692},{\"end\":46026,\"start\":46018},{\"end\":46039,\"start\":46030},{\"end\":46170,\"start\":46168},{\"end\":46178,\"start\":46174},{\"end\":46385,\"start\":46382},{\"end\":46395,\"start\":46391},{\"end\":46403,\"start\":46399},{\"end\":46410,\"start\":46407},{\"end\":46422,\"start\":46414},{\"end\":46431,\"start\":46426},{\"end\":46636,\"start\":46633},{\"end\":46646,\"start\":46642},{\"end\":46653,\"start\":46650},{\"end\":46665,\"start\":46657},{\"end\":46674,\"start\":46669},{\"end\":46874,\"start\":46871},{\"end\":46884,\"start\":46880},{\"end\":46891,\"start\":46888},{\"end\":46903,\"start\":46895},{\"end\":46912,\"start\":46907},{\"end\":47101,\"start\":47095},{\"end\":47113,\"start\":47107},{\"end\":47126,\"start\":47117},{\"end\":47138,\"start\":47130},{\"end\":47144,\"start\":47142},{\"end\":47154,\"start\":47148},{\"end\":47163,\"start\":47158},{\"end\":47172,\"start\":47170},{\"end\":47470,\"start\":47464},{\"end\":47478,\"start\":47474},{\"end\":47492,\"start\":47484},{\"end\":47502,\"start\":47496},{\"end\":47520,\"start\":47506},{\"end\":47796,\"start\":47790},{\"end\":47808,\"start\":47800},{\"end\":48010,\"start\":48004},{\"end\":48021,\"start\":48014},{\"end\":48238,\"start\":48234},{\"end\":48244,\"start\":48242},{\"end\":48253,\"start\":48248},{\"end\":48261,\"start\":48257},{\"end\":48267,\"start\":48265},{\"end\":48275,\"start\":48271},{\"end\":48284,\"start\":48279},{\"end\":48293,\"start\":48288},{\"end\":48574,\"start\":48570},{\"end\":48580,\"start\":48578},{\"end\":48597,\"start\":48584},{\"end\":48604,\"start\":48601},{\"end\":48610,\"start\":48608},{\"end\":48619,\"start\":48614},{\"end\":48871,\"start\":48869},{\"end\":48881,\"start\":48878},{\"end\":48890,\"start\":48885},{\"end\":49145,\"start\":49143},{\"end\":49154,\"start\":49149},{\"end\":49162,\"start\":49158},{\"end\":49168,\"start\":49166},{\"end\":49351,\"start\":49346},{\"end\":49362,\"start\":49358},{\"end\":49503,\"start\":49498},{\"end\":49510,\"start\":49507},{\"end\":49519,\"start\":49514},{\"end\":49746,\"start\":49741},{\"end\":49753,\"start\":49750},{\"end\":49760,\"start\":49757},{\"end\":49768,\"start\":49764},{\"end\":49777,\"start\":49772},{\"end\":49785,\"start\":49781},{\"end\":49794,\"start\":49789},{\"end\":49981,\"start\":49977},{\"end\":49992,\"start\":49985},{\"end\":50001,\"start\":49996}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6287870},\"end\":37089,\"start\":36758},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2331610},\"end\":37404,\"start\":37091},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3180429},\"end\":37655,\"start\":37406},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":18828233},\"end\":37872,\"start\":37657},{\"attributes\":{\"doi\":\"arXiv:1701.07875\",\"id\":\"b4\"},\"end\":38053,\"start\":37874},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13141061},\"end\":38302,\"start\":38055},{\"attributes\":{\"id\":\"b6\"},\"end\":38446,\"start\":38304},{\"attributes\":{\"doi\":\"arXiv:1606.04838\",\"id\":\"b7\"},\"end\":38700,\"start\":38448},{\"attributes\":{\"id\":\"b8\"},\"end\":38851,\"start\":38702},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":29308926},\"end\":39118,\"start\":38853},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7462498},\"end\":39434,\"start\":39120},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":18821712},\"end\":39658,\"start\":39436},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":17579179},\"end\":39836,\"start\":39660},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":57246310},\"end\":40078,\"start\":39838},{\"attributes\":{\"doi\":\"arXiv:1710.10793\",\"id\":\"b14\"},\"end\":40290,\"start\":40080},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7448250},\"end\":40534,\"start\":40292},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1033682},\"end\":40820,\"start\":40536},{\"attributes\":{\"id\":\"b17\"},\"end\":41109,\"start\":40822},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10747436},\"end\":41419,\"start\":41111},{\"attributes\":{\"id\":\"b19\"},\"end\":41715,\"start\":41421},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6832420},\"end\":41904,\"start\":41717},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206594692},\"end\":42084,\"start\":41906},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7200347},\"end\":42287,\"start\":42086},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1915014},\"end\":42445,\"start\":42289},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2428314},\"end\":42624,\"start\":42447},{\"attributes\":{\"id\":\"b25\"},\"end\":42872,\"start\":42626},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14542261},\"end\":43139,\"start\":42874},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":12078302},\"end\":43347,\"start\":43141},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":8125776},\"end\":43569,\"start\":43349},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":204937530},\"end\":43697,\"start\":43571},{\"attributes\":{\"id\":\"b30\"},\"end\":43946,\"start\":43699},{\"attributes\":{\"id\":\"b31\"},\"end\":44098,\"start\":43948},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6610705},\"end\":44298,\"start\":44100},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":16447573},\"end\":44596,\"start\":44300},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":41866457},\"end\":44787,\"start\":44598},{\"attributes\":{\"doi\":\"arXiv:1412.6550\",\"id\":\"b35\"},\"end\":45060,\"start\":44789},{\"attributes\":{\"id\":\"b36\"},\"end\":45429,\"start\":45062},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1687220},\"end\":45677,\"start\":45431},{\"attributes\":{\"doi\":\"arXiv:1610.09650\",\"id\":\"b38\"},\"end\":45946,\"start\":45679},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":14124313},\"end\":46159,\"start\":45948},{\"attributes\":{\"doi\":\"arXiv:1604.00433\",\"id\":\"b40\"},\"end\":46326,\"start\":46161},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":1745892},\"end\":46573,\"start\":46328},{\"attributes\":{\"id\":\"b42\"},\"end\":46803,\"start\":46575},{\"attributes\":{\"id\":\"b43\"},\"end\":47044,\"start\":46805},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":207230134},\"end\":47374,\"start\":47046},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3203345},\"end\":47704,\"start\":47376},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":12874183},\"end\":47936,\"start\":47706},{\"attributes\":{\"id\":\"b47\"},\"end\":48135,\"start\":47938},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":3331356},\"end\":48487,\"start\":48137},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":49309275},\"end\":48785,\"start\":48489},{\"attributes\":{\"doi\":\"arXiv:1709.00513\",\"id\":\"b50\"},\"end\":49072,\"start\":48787},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":3439214},\"end\":49294,\"start\":49074},{\"attributes\":{\"id\":\"b52\"},\"end\":49452,\"start\":49296},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":29797603},\"end\":49687,\"start\":49454},{\"attributes\":{\"id\":\"b54\"},\"end\":49928,\"start\":49689},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":15876696},\"end\":50103,\"start\":49930}]", "bib_title": "[{\"end\":36811,\"start\":36758},{\"end\":37166,\"start\":37091},{\"end\":37436,\"start\":37406},{\"end\":37728,\"start\":37657},{\"end\":38123,\"start\":38055},{\"end\":38923,\"start\":38853},{\"end\":39203,\"start\":39120},{\"end\":39490,\"start\":39436},{\"end\":39694,\"start\":39660},{\"end\":39889,\"start\":39838},{\"end\":40332,\"start\":40292},{\"end\":40563,\"start\":40536},{\"end\":41203,\"start\":41111},{\"end\":41766,\"start\":41717},{\"end\":41950,\"start\":41906},{\"end\":42130,\"start\":42086},{\"end\":42311,\"start\":42289},{\"end\":42497,\"start\":42447},{\"end\":42929,\"start\":42874},{\"end\":43211,\"start\":43141},{\"end\":43397,\"start\":43349},{\"end\":43582,\"start\":43571},{\"end\":44140,\"start\":44100},{\"end\":44375,\"start\":44300},{\"end\":44653,\"start\":44598},{\"end\":45468,\"start\":45431},{\"end\":46014,\"start\":45948},{\"end\":46378,\"start\":46328},{\"end\":47091,\"start\":47046},{\"end\":47460,\"start\":47376},{\"end\":47786,\"start\":47706},{\"end\":48230,\"start\":48137},{\"end\":48566,\"start\":48489},{\"end\":49139,\"start\":49074},{\"end\":49494,\"start\":49454},{\"end\":49973,\"start\":49930}]", "bib_author": "[{\"end\":36822,\"start\":36813},{\"end\":36832,\"start\":36822},{\"end\":36840,\"start\":36832},{\"end\":36848,\"start\":36840},{\"end\":36857,\"start\":36848},{\"end\":36865,\"start\":36857},{\"end\":36874,\"start\":36865},{\"end\":36886,\"start\":36874},{\"end\":36896,\"start\":36886},{\"end\":36905,\"start\":36896},{\"end\":37176,\"start\":37168},{\"end\":37187,\"start\":37176},{\"end\":37197,\"start\":37187},{\"end\":37208,\"start\":37197},{\"end\":37218,\"start\":37208},{\"end\":37230,\"start\":37218},{\"end\":37447,\"start\":37438},{\"end\":37458,\"start\":37447},{\"end\":37464,\"start\":37458},{\"end\":37476,\"start\":37464},{\"end\":37485,\"start\":37476},{\"end\":37505,\"start\":37485},{\"end\":37515,\"start\":37505},{\"end\":37742,\"start\":37730},{\"end\":37752,\"start\":37742},{\"end\":37888,\"start\":37876},{\"end\":37900,\"start\":37888},{\"end\":37910,\"start\":37900},{\"end\":38134,\"start\":38125},{\"end\":38140,\"start\":38134},{\"end\":38149,\"start\":38140},{\"end\":38155,\"start\":38149},{\"end\":38164,\"start\":38155},{\"end\":38359,\"start\":38353},{\"end\":38370,\"start\":38359},{\"end\":38513,\"start\":38503},{\"end\":38525,\"start\":38513},{\"end\":38536,\"start\":38525},{\"end\":38713,\"start\":38702},{\"end\":38724,\"start\":38713},{\"end\":38743,\"start\":38724},{\"end\":38933,\"start\":38925},{\"end\":38941,\"start\":38933},{\"end\":38947,\"start\":38941},{\"end\":38954,\"start\":38947},{\"end\":38968,\"start\":38954},{\"end\":39213,\"start\":39205},{\"end\":39219,\"start\":39213},{\"end\":39230,\"start\":39219},{\"end\":39237,\"start\":39230},{\"end\":39501,\"start\":39492},{\"end\":39516,\"start\":39501},{\"end\":39533,\"start\":39516},{\"end\":39709,\"start\":39696},{\"end\":39715,\"start\":39709},{\"end\":39722,\"start\":39715},{\"end\":39731,\"start\":39722},{\"end\":39899,\"start\":39891},{\"end\":39907,\"start\":39899},{\"end\":39917,\"start\":39907},{\"end\":39926,\"start\":39917},{\"end\":39932,\"start\":39926},{\"end\":39943,\"start\":39932},{\"end\":40126,\"start\":40117},{\"end\":40133,\"start\":40126},{\"end\":40140,\"start\":40133},{\"end\":40147,\"start\":40140},{\"end\":40341,\"start\":40334},{\"end\":40349,\"start\":40341},{\"end\":40357,\"start\":40349},{\"end\":40363,\"start\":40357},{\"end\":40372,\"start\":40363},{\"end\":40379,\"start\":40372},{\"end\":40385,\"start\":40379},{\"end\":40394,\"start\":40385},{\"end\":40579,\"start\":40565},{\"end\":40596,\"start\":40579},{\"end\":40605,\"start\":40596},{\"end\":40611,\"start\":40605},{\"end\":40627,\"start\":40611},{\"end\":40636,\"start\":40627},{\"end\":40649,\"start\":40636},{\"end\":40659,\"start\":40649},{\"end\":40931,\"start\":40920},{\"end\":40941,\"start\":40931},{\"end\":40948,\"start\":40941},{\"end\":40959,\"start\":40948},{\"end\":41219,\"start\":41205},{\"end\":41230,\"start\":41219},{\"end\":41241,\"start\":41230},{\"end\":41251,\"start\":41241},{\"end\":41554,\"start\":41544},{\"end\":41777,\"start\":41768},{\"end\":41788,\"start\":41777},{\"end\":41797,\"start\":41788},{\"end\":41958,\"start\":41952},{\"end\":41967,\"start\":41958},{\"end\":41974,\"start\":41967},{\"end\":41981,\"start\":41974},{\"end\":42142,\"start\":42132},{\"end\":42153,\"start\":42142},{\"end\":42161,\"start\":42153},{\"end\":42327,\"start\":42313},{\"end\":42342,\"start\":42327},{\"end\":42507,\"start\":42499},{\"end\":42513,\"start\":42507},{\"end\":42522,\"start\":42513},{\"end\":42695,\"start\":42681},{\"end\":42705,\"start\":42695},{\"end\":42940,\"start\":42931},{\"end\":42950,\"start\":42940},{\"end\":42960,\"start\":42950},{\"end\":42971,\"start\":42960},{\"end\":43219,\"start\":43213},{\"end\":43230,\"start\":43219},{\"end\":43412,\"start\":43399},{\"end\":43422,\"start\":43412},{\"end\":43435,\"start\":43422},{\"end\":43445,\"start\":43435},{\"end\":43598,\"start\":43584},{\"end\":43608,\"start\":43598},{\"end\":43617,\"start\":43608},{\"end\":43794,\"start\":43780},{\"end\":43802,\"start\":43794},{\"end\":43811,\"start\":43802},{\"end\":43991,\"start\":43980},{\"end\":44003,\"start\":43991},{\"end\":44012,\"start\":44003},{\"end\":44150,\"start\":44142},{\"end\":44159,\"start\":44150},{\"end\":44167,\"start\":44159},{\"end\":44185,\"start\":44167},{\"end\":44388,\"start\":44377},{\"end\":44401,\"start\":44388},{\"end\":44409,\"start\":44401},{\"end\":44422,\"start\":44409},{\"end\":44430,\"start\":44422},{\"end\":44667,\"start\":44655},{\"end\":44677,\"start\":44667},{\"end\":44799,\"start\":44789},{\"end\":44809,\"start\":44799},{\"end\":44820,\"start\":44809},{\"end\":44832,\"start\":44820},{\"end\":44841,\"start\":44832},{\"end\":44851,\"start\":44841},{\"end\":45133,\"start\":45118},{\"end\":45145,\"start\":45133},{\"end\":45159,\"start\":45145},{\"end\":45482,\"start\":45470},{\"end\":45496,\"start\":45482},{\"end\":45507,\"start\":45496},{\"end\":45517,\"start\":45507},{\"end\":45528,\"start\":45517},{\"end\":45536,\"start\":45528},{\"end\":45688,\"start\":45679},{\"end\":45709,\"start\":45688},{\"end\":46028,\"start\":46016},{\"end\":46041,\"start\":46028},{\"end\":46172,\"start\":46163},{\"end\":46180,\"start\":46172},{\"end\":46387,\"start\":46380},{\"end\":46397,\"start\":46387},{\"end\":46405,\"start\":46397},{\"end\":46412,\"start\":46405},{\"end\":46424,\"start\":46412},{\"end\":46433,\"start\":46424},{\"end\":46638,\"start\":46631},{\"end\":46648,\"start\":46638},{\"end\":46655,\"start\":46648},{\"end\":46667,\"start\":46655},{\"end\":46676,\"start\":46667},{\"end\":46876,\"start\":46869},{\"end\":46886,\"start\":46876},{\"end\":46893,\"start\":46886},{\"end\":46905,\"start\":46893},{\"end\":46914,\"start\":46905},{\"end\":47103,\"start\":47093},{\"end\":47115,\"start\":47103},{\"end\":47128,\"start\":47115},{\"end\":47140,\"start\":47128},{\"end\":47146,\"start\":47140},{\"end\":47156,\"start\":47146},{\"end\":47165,\"start\":47156},{\"end\":47174,\"start\":47165},{\"end\":47472,\"start\":47462},{\"end\":47480,\"start\":47472},{\"end\":47494,\"start\":47480},{\"end\":47504,\"start\":47494},{\"end\":47522,\"start\":47504},{\"end\":47798,\"start\":47788},{\"end\":47810,\"start\":47798},{\"end\":48012,\"start\":48002},{\"end\":48023,\"start\":48012},{\"end\":48240,\"start\":48232},{\"end\":48246,\"start\":48240},{\"end\":48255,\"start\":48246},{\"end\":48263,\"start\":48255},{\"end\":48269,\"start\":48263},{\"end\":48277,\"start\":48269},{\"end\":48286,\"start\":48277},{\"end\":48295,\"start\":48286},{\"end\":48576,\"start\":48568},{\"end\":48582,\"start\":48576},{\"end\":48599,\"start\":48582},{\"end\":48606,\"start\":48599},{\"end\":48612,\"start\":48606},{\"end\":48621,\"start\":48612},{\"end\":48873,\"start\":48867},{\"end\":48883,\"start\":48873},{\"end\":48892,\"start\":48883},{\"end\":49147,\"start\":49141},{\"end\":49156,\"start\":49147},{\"end\":49164,\"start\":49156},{\"end\":49170,\"start\":49164},{\"end\":49353,\"start\":49341},{\"end\":49364,\"start\":49353},{\"end\":49505,\"start\":49496},{\"end\":49512,\"start\":49505},{\"end\":49521,\"start\":49512},{\"end\":49748,\"start\":49739},{\"end\":49755,\"start\":49748},{\"end\":49762,\"start\":49755},{\"end\":49770,\"start\":49762},{\"end\":49779,\"start\":49770},{\"end\":49787,\"start\":49779},{\"end\":49796,\"start\":49787},{\"end\":49983,\"start\":49975},{\"end\":49994,\"start\":49983},{\"end\":50003,\"start\":49994}]", "bib_venue": "[{\"end\":43004,\"start\":42996},{\"end\":36909,\"start\":36905},{\"end\":37237,\"start\":37230},{\"end\":37519,\"start\":37515},{\"end\":37756,\"start\":37752},{\"end\":38168,\"start\":38164},{\"end\":38351,\"start\":38304},{\"end\":38501,\"start\":38448},{\"end\":38749,\"start\":38743},{\"end\":38975,\"start\":38968},{\"end\":39268,\"start\":39237},{\"end\":39537,\"start\":39533},{\"end\":39738,\"start\":39731},{\"end\":39947,\"start\":39943},{\"end\":40115,\"start\":40080},{\"end\":40401,\"start\":40394},{\"end\":40666,\"start\":40659},{\"end\":40918,\"start\":40822},{\"end\":41255,\"start\":41251},{\"end\":41542,\"start\":41421},{\"end\":41801,\"start\":41797},{\"end\":41985,\"start\":41981},{\"end\":42177,\"start\":42161},{\"end\":42360,\"start\":42342},{\"end\":42526,\"start\":42522},{\"end\":42679,\"start\":42626},{\"end\":42994,\"start\":42971},{\"end\":43235,\"start\":43230},{\"end\":43449,\"start\":43445},{\"end\":43624,\"start\":43617},{\"end\":43778,\"start\":43699},{\"end\":43978,\"start\":43948},{\"end\":44189,\"start\":44185},{\"end\":44437,\"start\":44430},{\"end\":44684,\"start\":44677},{\"end\":44899,\"start\":44866},{\"end\":45116,\"start\":45062},{\"end\":45543,\"start\":45536},{\"end\":45789,\"start\":45725},{\"end\":46045,\"start\":46041},{\"end\":46439,\"start\":46433},{\"end\":46629,\"start\":46575},{\"end\":46867,\"start\":46805},{\"end\":47199,\"start\":47174},{\"end\":47529,\"start\":47522},{\"end\":47814,\"start\":47810},{\"end\":48000,\"start\":47938},{\"end\":48300,\"start\":48295},{\"end\":48626,\"start\":48621},{\"end\":48865,\"start\":48787},{\"end\":49174,\"start\":49170},{\"end\":49339,\"start\":49296},{\"end\":49561,\"start\":49521},{\"end\":49737,\"start\":49689},{\"end\":50007,\"start\":50003}]"}}}, "year": 2023, "month": 12, "day": 17}