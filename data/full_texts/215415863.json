{"id": 215415863, "updated": "2023-10-06 17:29:02.61", "metadata": {"title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth", "authors": "[{\"first\":\"Lu\",\"last\":\"Hou\",\"middle\":[]},{\"first\":\"Lifeng\",\"last\":\"Shang\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Qun\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 4, "day": 8}, "abstract": "The pre-trained language models like BERT and RoBERTa, though powerful in many natural language processing tasks, are both computational and memory expensive. To alleviate this problem, one approach is to compress them for specific tasks before deployment. However, recent works on BERT compression usually reduce the large BERT model to a fixed smaller size, and can not fully satisfy the requirements of different edge devices with various hardware performances. In this paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT), which can run at adaptive width and depth. The training process of DynaBERT includes first training a width-adaptive BERT and then allows both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. Comprehensive experiments under various efficiency constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its largest size has comparable performance as BERT (or RoBERTa), while at smaller widths and depths consistently outperforms existing BERT compression methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2004.04037", "mag": "3101731278", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/HouHSJCL20", "doi": null}}, "content": {"source": {"pdf_hash": "1c332cfa211400fc6f56983fb01a6692046116dd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2004.04037v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bbfb93475567cfc1e3742b4995eef6c0f9c8a0e6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1c332cfa211400fc6f56983fb01a6692046116dd.txt", "contents": "\nDynaBERT: Dynamic BERT with Adaptive Width and Depth *\n\n\nLu Hou houlu3@huawei.com \nHuawei Noah's Ark Lab\n\n\nLifeng Shang shang.lifeng@huawei.com \nHuawei Noah's Ark Lab\n\n\nXin Jiang jiang.xin@huawei.com \nHuawei Noah's Ark Lab\n\n\nQun Liu qun.liu@huawei.com \nHuawei Noah's Ark Lab\n\n\nDynaBERT: Dynamic BERT with Adaptive Width and Depth *\n\nThe pre-trained language models like BERT and RoBERTa, though powerful in many natural language processing tasks, are both computational and memory expensive. To alleviate this problem, one approach is to compress them for specific tasks before deployment. However, recent works on BERT compression usually reduce the large BERT model to a fixed smaller size, and can not fully satisfy the requirements of different edge devices with various hardware performances. In this paper, we propose a novel dynamic BERT model (abbreviated as Dyn-aBERT), which can run at adaptive width and depth. The training process of DynaBERT includes first training a width-adaptive BERT and then allows both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. Comprehensive experiments under various efficiency constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its largest size has comparable performance as BERT BASE (or RoBERTa BASE ), while at smaller widths and depths consistently outperforms existing BERT compression methods. * Work in progress.\n\nIntroduction\n\nRecently, pre-trained language models based on the Transformer [22] structure like BERT [8] and RoBERTa [14] have achieved remarkable results on natural language processing tasks. However, these models have a lot of parameters, hindering their deployment on edge devices with limited storage, computation and energy consumption. The difficulty of deploying BERT to these devices lies in two aspects. Firstly, the hardware performances of various devices vary a lot, and it is infeasible to deploy one single BERT model to all kinds of edge devices. Different architectural configurations of the BERT model are desired. Secondly, the resource condition of one device under different circumstances can be quite different. For instance, on a mobile phone, when a large number of compute-intensive or storage-intensive programs are running, the resources that can be allocated to the current BERT model will be correspondingly fewer. Thus once the BERT model is deployed, dynamically selecting a part of the model (also referred as sub-networks) for inference based on the device's current resource condition is also desirable. Note that unless otherwise specified, the BERT model mentioned in this paper refers to a task-specific BERT rather than the pretrained model.\n\nThere have been some attempts to compress and accelerate inference of the Transformer-based models using low-rank approximation [15,13], weight-sharing [7,13], knowledge distillation [19,21,12], quantization [2,29,20] and pruning [18,16,17,6,23]. However, these methods usually compress the model to a fixed size and can not meet the requirements above. In [7,10,9], Transformerbased models with adaptive depth are proposed to dynamically select some of the Transformer layers during inference. However, these depth-adaptive models only consider compression and accelera-tion in the depth direction. Some studies now show that the width direction also has high redundancy. For example, in [23,17], it is shown that only a small number of the attention heads are required to achieve comparable accuracy as using all of them. Although there have been some works that train convolutional neural networks (CNNs) with adaptive width [28,27,26], the Transformer-based models with adaptive width have not been studied.\n\nAdapting along only width or depth has a limited degree of flexibility and can only generate a limited number of architectural configurations. This can be restrictive for multiple performance requirements (e.g., latency, memory, and energy consumption) of multiple hardware platforms or different statuses of one particular platform. In this work, we offer flexibility in both width and depth of BERT to enable a significantly larger number of architectural configurations. This also enables better exploration of the balance between model accuracy and model size. Previously a once-for-all CNN with both adaptive width and depth is proposed in [4] by progressively shrinking the model in the width and depth directions. Specifically, the authors first train the CNN with the maximum kernel size, depth and width. Then they fine-tune the network to successively allow elastic kernel size, depth and width. Compared with CNNs, the BERT model is more complicated, which makes it impossible to directly apply the method in [6]. This complexity lies in that each Transformer layer includes both a Multi-Head Attention (MHA) mechanism and a position-wise Feed-forward Network (FFN) that perform transformations in two different dimensions (i.e., the sequence and the feature dimensions). Thus the width of a Transformer-based model can not be simply defined as the number of kernels as in CNNs. Moreover, successive training in first depth and then width can be sub-optimal since these two directions are hard to be disentangled, which may cause the knowledge learned in the depth direction to be forgotten after the width is trained to be adaptive.\n\nIn this paper, we propose a novel dynamic BERT, or DynaBERT for short, which can be executed at different widths and depths for specific tasks. The training process of DynaBERT includes first training a width-adaptive BERT (abbreviated as DynaBERT W ) and then allows both adaptive width and depth in DynaBERT. When training DynaBERT W , we first rewire the connections in each Transformer layer based on the importance of the attention heads and neurons to ensure that, the most important heads and neurons are utilized by more sub-networks. Then we distill knowledge from a fixed teacher network to student sub-networks at equal or smaller widths in DynaBERT W . After DynaBERT W is trained, we initialize from it to train DynaBERT with both adaptive width and depth. To avoid losing the elasticity already learned in the width direction, we use knowledge distillation in both the width and depth directions. We extensively evaluated the effectiveness of our proposed dynamic BERT on the GLUE benchmark under various efficiency constraints (#parameters, FLOPs, inference speed), using both BERT BASE and RoBERTa BASE as the backbone models. Under all deployment scenarios, our proposed dynamic BERT and RoBERTa at its largest size have comparable performance as BERT BASE and RoBERTa BASE , while at smaller widths and depths consistently outperform existing BERT compression methods under the same efficiency constraint.\n\n\nRelated Work\n\nIn this section, we first describe the formulation of Transformer layer in BERT. Then we briefly review related work on compression of Transformer-based models.\n\n\nTransformer Layer\n\nThe BERT model is built with Transformer Encoder layers [22], which capture long-term dependencies between input tokens by self-attention mechanism. Specifically, a standard Transformer layer contains a Multi-Head Attention (MHA) layer and a Feed-Forward Network (FFN).\n\nFor the t-th Transformer layer, suppose the input to it is X \u2208 R n\u00d7d where n and d are the sequence length and hidden state size. Suppose there are N H attention heads in each layer, with head h parameterized by\nW Q h , W K h , W V h , W O h \u2208 R d\u00d7d h where d h = d N H ,\n\nand output computed as\nAttn h W Q h ,W K h ,W V h ,W O h (X) = Softmax( QK \u221a d )VW O h = Softmax( 1 \u221a d XW Q h W K h X )XW V h W O h ,\nIn multi-head attention, N H heads are computed in parallel to get the final output [23]:\nMHAttn W Q ,W K ,W V ,W O (X) = N H h=1 Attn h W Q h ,W K h ,W V h ,W O h (X).(1)\nThe FFN layer is parameterized by two matrices\nW 1 \u2208 R d\u00d7d f f and W 2 \u2208 R d f f \u00d7d where d f f is the number of neurons in the intermediate layer of FFN.\nWith a slight abuse of notation, we still use X \u2208 R n\u00d7d to denote the input to FFN, the output is then computed as:\nIntermediate(X) = GeLU(XW 1 + b 1 ); FFN(X) = Intermediate(X)W 2 + b 2 ,\nwhere b 1 , b 2 are the bias in the two linear layers.\n\n\nCompression for Transformer/BERT\n\nTransformer-based models can be compressed using low-rank approximation [13,15], weight sharing [7,13], distillation [19,21,12], quantization [20,29,3] or pruning [16,6,23,17,11].\n\nLow-rank approximation approximates the weight matrix by the multiplication of two matrices with lower rank. ALBERT [13] uses low-rank approximation for the word embeddings of BERT model. Tensorized Transformer [15] shows that the output of MHA can be linearly represented by a group of orthonormal base vectors, and multi-linear attention is used to compress the model.\n\nWeight sharing shares parameters across layers in one network. Universal Transformer [7] shares parameters across layers. Compared to the standard Transformer, it gets better performance on language modeling and subject-verb agreement. Deep Equilibrium Models [1] reach an equilibrium point where the input and output of a certain layer stay the same. ALBERT [13] shows that sharing weights across layers stabilizes network parameters and achieves better performance than the original BERT with fewer parameters. Though these weight sharing methods significantly reduce the model size, the inference remains slow.\n\nDistillation transfers knowledge from a big teacher model to a smaller compact model. Distil-BERT [19] pre-trains a smaller general-purpose BERT with the use of distillation loss over the soft logits and the hidden states between the student and teacher networks. BERT-PKD [21] uses distillation loss on multiple intermediate layers. TinyBERT [12] successively uses a general distillation and a task-specific distillation over the embedding, attention matrices and output of each Transformer layer together to build task-specific small models.\n\nQuantization represents each weight value using low bits. QBERT [20] uses the second-order information to determine the number of bits for each layer, and more bits are assigned for layers with steeper curvature. They also use group quantization for different weights in MHA. Fully-quantized Transformer [3] uses uniform min-max quantization for computational expensive operations in the Transformer. Weights are bucketed before quantization to reduce quantization error. Q8BERT [29] performs symmetric 8-bit linear quantization on BERT through quantization-aware training.\n\nPruning removes unimportant connections or neurons in the network. In [11], a magnitude-based pruning method is used to prune unimportant connections during the pre-training phase, while \"info deletion\" is used to recover some wrongly pruned weights during the fine-tuning phase on the downstream task. In [6], sparse self-attention is introduced during fine-tuning, by replacing the softmax function with a controllable sparse transformation. In [16], gates are put on attention heads, the neurons in the intermediate layer of FFN, and the embeddings to individually eliminate parts of the Transformer. It is shown in [17,23] that a large percentage of attention heads can be removed without significantly impacting performance. In LayerDrop [10], structured dropout is used to prune Transformer layers for efficient inference. In [9], the encoder-decoder Transformer models are trained to make output predictions at different layers of the decoder for a particular sequence.\n\nHowever, most of these existing methods can only compress the original model to a specific size, without considering the efficiency constraints of different hardwares or different statuses of one certain hardware. Though Transformer-based models with adaptive depth are proposed in [7, 10, 9], considering compression and acceleration only in the depth direction can be limited. Recent studies show that the width direction of the Transformer-based models also has high redundancy. For example, it is shown in [23,17] that comparable accuracy can be well maintained even when many attention heads are pruned.\n\n\nMethod\n\nIn this section, we elaborate the training method of our DynaBERT model. The training process includes two stages. We first train a width-adaptive DynaBERT W in Section 3.1 and then train the both width-and depth-adaptive DynaBERT in Section 3.2.\n\n\nTraining DynaBERT W with Adaptive Width\n\nBefore describing the training process, we first need to define the width of BERT model. Compared to CNNs stacked with regular convolutional layers, the BERT model stacked with Transformer layers is much more complicated. In each Transformer layer, the computation of the MHA contains the linear transformation and multiplications of keys, queries, values for multiple heads. Moreover, the MHA and the FFN in each Transformer layer perform transformations in different dimensions, making it hard to trivially determine the width of the Transformer layer.\n\n\nUsing Attention Heads and Intermediate Neurons in FFN to Adapt the Width\n\nFollowing [17], we divide the computation of the MHA into the computations for each attention head as in (1 In each Transformer layer, when the width multiplier is m w , the MHA retains the leftmost m w N H attention heads, and the FFN intermediate layer retains the leftmost m w d f f neurons. In this case, each Transformer layer is roughly compressed by the ratio m w . Note that this is not strictly equal because layer normalization and biases in linear layers also have a small fraction of parameters. Different Transformer layers, or the attention heads and the neurons in the same layer, can also have different width multipliers. In this paper, for simplicity, we focus on using the same width multiplier for the attention heads and neurons in all Transformer layers.\n\n\nNetwork Rewiring\n\nTo fully utilize the network capacity, the more important heads or neurons should be shared across more sub-networks. Since we use the leftmost m w N H attention heads and m w d f f neurons, before training the width-adaptive network, we rank the attention heads and neurons according to their importance in the initial BERT model. Following [18,23], we compute the importance score of a head or neuron based on the variation in the loss if we remove it. Denote the output of one attention head as h, the importance of this head I h to the loss L can be estimated using the first-order Taylor expansion as\nI h = |L h \u2212 L h=0 | = L h \u2212 (L h \u2212 \u2202L \u2202h (h \u2212 0) + R h=0 ) \u2248 \u2202L \u2202h h\nif we ignore the remainder R h=0 . It is shown in [23] that for head h in MHA, its importance \u2202L \u2202h h can be computed using the gradient w.r.t. its mask variable m as\n\u2202L \u2202h h = \u2202L \u2202m .(2)\nSimilarly, for a neuron in the intermediate layer of FFN, denote the set of weights connected to it as w = {w 1 , w 2 , \u00b7 \u00b7 \u00b7 , w K }, its importance can be estimated by\n\u2202L \u2202w w = K i=1 \u2202L \u2202w i w i .(3)\nEmpirically, we use the development set to calculate the importance of the attention heads and neurons according to (2) and (3). Then we arrange the attention heads and neurons in the width direction from highest to lowest ( Figure 2) and rewire the corresponding connections. The detailed process is shown in Algorithm 1.\n\n\u210e heads in MHA  \n\n\nTraining with Adaptive Width\n\nAfter the connections of the BERT model are rewired according to Algorithm 1, we use knowledge distillation to train DynaBERT W . Specifically, we use the rewired BERT model as the fixed teacher network, and to initialize DynaBERT W . Then we distill the knowledge from the fixed teacher model to student sub-networks at different widths in DynaBERT W (Figure 3).\n\n\nAlgorithm 1 Rewire the network according to the importance of attention heads and neurons.\n\ninitialize: development set, trained BERT on downstream task. Clear gradients of weights optimizer.zero grad(). for iter = 1, ..., T val do Get next mini-batch of data and label. Forward propagation, get predicted labels and compute loss L. Accumulate gradients L.backward(). end for Calculate the importance of attention heads and neurons according to (2) and (3). Rewire the network according the importance.  Take the classification task as an example, we transfer the knowledge in the logits, embedding (i.e., the output of the embedding layer), and hidden states (i.e. the output of each Transformer layer) of all L Transformer layers from the teacher model to student sub-networks. Specifically, suppose the logits, embedding and hidden states of the fixed teacher model at the maximum width are y, E and H respectively, while those for a student sub-network with width multiplier m w are y (mw) , E (mw) and H (mw) respectively. Here E, E (mw) \u2208 R n\u00d7d and H, H (mw) \u2208 R L\u00d7n\u00d7d . For the sub-network with width multiplier m w , its distillation loss function contains three parts. The first part is the distillation loss on the logits pred , which distills the knowledge from the teacher model's logits to the student through soft cross-entropy loss\npred (y (mw) , y) = \u2212softmax(y) \u00b7 log softmax(y (mw) ).(4)\nThe second part is the distillation loss emb on the embedding, which distills the knowledge from the teacher model's embedding E \u2208 R n\u00d7d to the student\nE (mw) \u2208 R n\u00d7d emb (E (mw) , E) = MSE(E (mw) , E).(5)\nThe last part is the distillation loss hidn on the hidden states which makes the output of each transformer layer of the student sub-network H \n\nCombining (4) -(6), the objective is\nL = \u03bb 1 pred (y (mw) , y) + \u03bb 2 ( emb (E (mw) , E) + hidn (H (mw) , H)),(7)\nwhere \u03bb 1 and \u03bb 2 are the scaling parameters that control the weights of different loss terms. Note that we use the same scaling parameter for emb and hidn , because the embedding has the same dimension and similar scale as the hidden states. In our experiments, we choose (\u03bb 1 , \u03bb 2 ) = (1, 0.1) because emb + hidn is around one magnitude larger than pred empirically. The detailed process is shown in Algorithm 2. To provide more task-specific data for distillation learning, we use the data augmentation method from TinyBERT [12]. Since the labels of augmented data are not always correct, we do not consider the loss between predicted labels and ground-truth labels.\n\nAlgorithm 2 Train DynaBERT W with adaptive width. input: Training set, width multiplier list widthList. initialize: Initialize a fixed teacher model and a trainable student model studentM odel with the rewired model from Algorithm 1. for iter = 1, ..., T train do Get next mini-batch of data. Execute the teacher model and get the logits y, embedding E, hidden states H.\n\nClear gradients in the student model studentM odel.zero grad(). for width multiplier m w in widthList do Execute sub-network with width multiplier m w , get the logits y (mw) , embedding E (mw) and hidden states H (mw) . Compute loss L using (7).\n\nAccumulate gradients L.backward(). end for end for Update the parameters with the accumulated gradients.\n\n\nTraining DynaBERT with Adaptive Width and Depth\n\nAfter the training of DynaBERT W using Algorithm 2, we further use knowledge distillation to train DynaBERT with both adaptive width and depth. Specifically, we use the trained DynaBERT W model from Section 3.1 as the fixed teacher model, and to initialize the DynaBERT model. Then we distill the knowledge from the fixed teacher model at the maximum depth to student sub-networks at equal or lower depths ( Figure 4). During the training of DynaBERT, to avoid catastrophic forgetting of learned elasticity in the width direction, we still train over different widths in each iteration. For width multiplier m w , the objec-tive function of the student sub-network with depth multiplier m d still contains three parts like in Section 3.1.3. These three parts make the logits y (mw,m d ) , embedding E (mw,m d ) and hidden states H (mw,m d ) mimic y (mw) , E (mw) and H (mw) from the teacher model with the maximum depth. When the depth multiplier is smaller than 1, the student will have fewer Transformer layers than the teacher. In this case, following [12,10,19], we simply drop layers evenly to get a balanced network. Specifically, for depth multiplier m d , which means we prune layers with a rate (1 \u2212 m d ), we drop the layers at depth d which satisfies mod(d + 1, 1 m d ) \u2261 0. We use d + 1 here instead of d in [10] because we want to keep the last layer which is shown to be important in [25]. For instance, for a BERT model with 12 Transformer layers indexed by 1, 2, 3, \u00b7 \u00b7 \u00b7 , 12. When m d = 0.75, we drop the layers at depth 3, 7, 11. The loss is computed over all the kept layers.\nhidn (H (mw,m d ) , H (mw) ) = l\u2208keptLayers MSE(H (mw,m d ) l , H (mw) l ).\nThus the loss can be written as L = \u03bb 1 pred (y (mw,m d ) , y (mw) ) + \u03bb 2 ( emb (E (mw,m d ) , E (mw) ) + hidn (H (mw,m d ) , H (mw) )). (8) For simplicity, we do not tune \u03bb 1 , \u03bb 2 and choose (\u03bb 1 , \u03bb 2 ) = (1, 1) in our experiments. The training procedure can be found in Algorithm 3. After the unsupervised training, one can further fine-tune the network using the cross-entropy loss between the predicted labels and the ground-truth labels. This step improves the performance on some data sets empirically (details can be found in the Section 4.3). In this paper, we use the model with the higher average validation accuracy over all widths and depths between with and without fine-tuning.\n\nAlgorithm 3 Train DynaBERT with adaptive width and depth.\n\ninput: Training set, width multiplier list widthList, depth multiplier list depthList. initialize: Initialize a fixed teacher model and a trainable student model studentM odel with the width-adaptive model trained from Algorithm 2. for iter = 1, ..., T train do Get next mini-batch of data. for width multiplier m w in widthList do Execute the teacher model with width multiplier m w and the maximum depth, append the logits y (mw) to Y, the embedding E (mw) to E, and the hidden states H (mw) to H. end for Clear gradients in the student model studentM odel.zero grad(). for depth multiplier m d in depthList do for width multiplier m w in widthList do Execute sub-network of the student model with width multiplier m w and depth multiplier m d , get logits y (mw) , embedding E (mw) and H (mw) .\n\nBased on y (mw) , E (mw) and H (mw) with current width multiplier m w in Y, E and H, compute loss L using (8).\n\nAccumulate gradients L.backward(). end for end for end for Update the parameters with the accumulated gradients.\n\n\nExperiment\n\nIn this section, we evaluate the efficacy of the proposed DynaBERT on the General Language Understanding Evaluation (GLUE) tasks [24] using both BERT BASE [8] and RoBERTa BASE [14] as the backbone models. The corresponding width-and depth-adaptive BERT and RoBERTa models are named as DynaBERT and DynaRoBERTa, respectively. In the following, we elaborate our experiment settings in Section 4.1. Then in Section 4.2, we show the performance of our proposed DynaBERT and DynaRoBERTa at different widths and depths, and compare with other compression methods under different resource constraints. Finally, in Section 4.3, we show the importance of the proposed network rewiring (Section 3.1.1), knowledge distillation and data augmentation in the training of DynaBERT W (Section 3.1) and DynaBERT (Section 3.2).\n\n\nSetting\n\nData. We conduct comprehensive experiments on the official GLUE benchmark [24], which is a collection of diverse natural language understanding tasks, including textual entailment (RTE and MNLI), question answering (QNLI), similarity and paraphrase (MRPC, QQP, STS-B), sentiment analysis (SST-2) and linguistic acceptability (CoLA). For MNLI, we experiment on both the matched (MNLI-m) and mismatched (MNLI-mm) sections.\n\nTraining Details. The DynaBERT and DynaRoBERTa models have the same maximum size as the BERT BASE and RoBERTa BASE models, respectively. For BERT BASE Table 8 in Appendix A.\n\nReporting Results. Results on development set are used for evaluation. The metrics are Spearman correlation for STS-B, Matthews correlation for CoLA and accuracy for the other tasks. We compare our proposed DynaBERT and DynaRoBERTa with the following methods: (1) BERT BASE [8], (2) RoBERTa BASE [14], (3) DistilBERT [19], (4) TinyBERT [12], and (5) LayerDrop [10]. We evaluate the efficacy of our proposed DynaBERT and DynaRoBERTa under different efficiency constraints, including #parameters, FLOPs, the latency on NVIDIA K40 GPU and on Kirin 810 A76 ARM CPU (details can be found in Appendix B).\n\n\nMain Results\n\nResults on the GLUE benchmark. In Table 1, we show the evaluation results of sub-networks derived from the proposed DynaBERT and DynaRoBERTa with different width and depth multipliers. The Transformer layers are nearly compressed by rate m w \u00d7 m d , if we do not count the parameters in layer normalization and linear layer bias which are negligible. Another observation is that using one certain width multiplier usually has higher accuracy than using the same depth multiplier. This indicates that compared to the depth direction, the width direction is more robust to compression. Sub-networks from DynaRoBERTa most of the time perform significantly better than those from DynaBERT under the same depth and width.\n\nWe also show the test set results in Table 2. Again, the proposed DynaBERT achieves comparable accuracy than BERT BASE with the same size. Interestingly, the proposed DynaRoBERTa outperforms RoBERTa BASE on seven out of eight tasks. A possible reason is that allowing adaptive width and depth increases the training difficulty and acts as regularization, and so contributes positively to the performance. Comparison with Other Methods.                \n\n\nAblation Study\n\nIn this section, we do ablation study in the training of DynaBERT W and DynaBERT.\n\nTraining DynaBERT W with Adaptive Width. We evaluate the importance of network rewiring, knowledge distillation and data augmentation in the training of DynaBERT W in Table 3. DynaBERT W trained without network rewiring, knowledge distillation and data augmentation is called \"vanilla DynaBERT W \". We also compare DynaBERT W against the baseline of using separate networks, each of which is initialized from the BERT BASE with a certain width multiplier m w \u2208 [1.0, 0.75, 0.5, 0.25], and then fine-tuned on the downstream task. Note the difference between the two is that the former trains one single network, but executes at different widths, while the later trains four different networks with different width multipliers.\n\nFrom Table 3, DynaBERT W has comparable performance as the separate network baseline at its largest width and has significantly better performance at smaller widths. The smaller the width, the more significant the accuracy gain. After network rewiring, the average development set accuracy on the GLUE benchmark is over 2 points higher than the counterpart without rewiring. The accuracy gain is larger when the width of the model is smaller. With knowledge distillation and data augmentation, the average accuracy is further improved by around 1.5 points. Training DynaBERT with Adaptive Width and Depth. We evaluate the effect of knowledge distillation, data augmentation and final fine-tuning in the training of DynaBERT on four of GLUE data sets SST-2, CoLA, MRPC and RTE in Table 4. The DynaBERT trained without knowledge distillation, data augmentation and final fine-tuning is called \"vanilla DynaBERT\".\n\nFrom Table 4, with knowledge distillation and data augmentation, the average accuracy of smaller depth is significantly improved compared to vanilla counterpart on all four data sets. Additional fine-tuning further improves the average accuracy on all three depth multipliers on SST-2, CoLA and two on RTE, but harms the performance on MRPC. Empirically, we choose the model with higher average accuracy between with and without fine-tuning.\n\n\nDiscussion\n\n\nComparison of Conventional Distillation and Inplace Distillation\n\nTo train width-adaptive CNNs, in [27], inplace distillation is used to boost the performance. In this distillation, sub-network with the maximum width is the teacher while sub-networks with smaller widths in the same model are students. Training loss includes the loss from both the teacher network and the student network. In this section, we also adapt inplace distillation to train DynaBERT W and compare it with the conventional distillation used in Section 3.1 For inplace distillation, the loss for the student is the distillation loss over logit, embedding and hidden states with the teacher. The loss of the teacher is the distillation loss over logits, embedding and hidden states with a fixed fine-tuned task-specific BERT. Table 5 shows the comparison of these two kinds of distillation. As can be seen, inplace distillation has higher average accuracy on MRPC and CoLA when training DynaBERT W . However, in the training of DynaBERT, the model initialized with the inplace distillation can lead to even worse performance than that with the conventional distillation. For DynaBERT W (Section 3.1), we rewire the network only once before training by alternating over four different width multipliers. In this section, we also tried adapting the following two methods in training width-adaptive CNNs to BERT: (1) using progressive rewiring as in [4] which progressively rewires the network as more width multipliers are supported; and (2) universally slimmable training [27] which randomly sample some width multipliers in each iteration.\n\nProgressive Rewiring. Instead of rewiring the network only once before training, \"progressive rewiring\" progressively rewires the network as more width multipliers are supported throughout the training. Specifically, for four width multipliers Finally, the network is again sorted and rewired before supporting all four width multipliers. For \"progressive rewiring\", we tune the initial learning rate from {2 \u00d7 10 \u22125 , 1 \u00d7 10 \u22125 , 2 \u00d7 10 \u22125 , 5 \u00d7 10 \u22126 , 2 \u00d7 10 \u22126 } and pick the best-performing initial learning rate 1 \u00d7 10 \u22125 . Table 6 shows the development set accuracy on the GLUE benchmark for using progressive rewiring. Since progressive rewiring requires progressive training and is timeconsuming, we do not use data augmentation and distillation, and use cross-entropy loss between predicted labels and the ground-truth labels as the training loss. By comparing with Table 3, using progressive rewiring has no significant gain over rewiring only once. Universally Slimmable Training. Instead of using a pre-defined list of width multipliers, universally slimmable training [27] samples several width multipliers in each training iteration. Following [27], we also use inplace distillation for universally slimmable training. For universally slimmable training, we tune (\u03bb 1 , \u03bb 2 ) in {(1, 1), (1, 0), (0, 1), (1, 0.1), (0.1, 1), (0.1, 0.1)} on MRPC and choose the best-performing one (\u03bb 1 , \u03bb 2 ) = (0.1, 0.1). The corresponding results for can be found in Table 7. For better comparison with using pre-defined width multipliers, we also report results when the width multipliers are [1.0, 0.75, 0.5, 0.25]. We also do not use data augmentation here. By comparing with Table 3, there is no significant difference between using universally slimmable training and the alternative training as used in Algorithm 2. \n\n\nConclusion and Future Work\n\nIn this paper, we propose a novel model called DynaBERT which can run at different widths and depths. Experiments on various tasks show that under the same efficiency constraint, sub-networks extracted from the proposed DynaBERT consistently achieve better performance than the other BERT compression methods. In this work, we only adapt the width in the number of attention heads in MHA and neurons in the intermediate layer in FFN. In the future, we would also like to change the hidden state size to further compress the model and accelerate inference. Another possible direction is that models which share weights across Transformer layers like ALBERT and universal Transformers might be more suited for adaptive depth. This work focuses on training a dynamic BERT on specific tasks, in the future, we would also like to apply the proposed method to the pre-training stage.\n\n\nA Hyperparameters\n\nThe detailed hyperparameters for the width-adaptive training in Section 3.1, both width-and depthadaptive training and the final fine-tuning in Section 3.2 are shown in Table 8. \n\n\nB Storage, FLOPs and Latency\n\nTo count the floating point operations (FLOPs), we follow the setting in [5] and infer FLOPs with batch size of 1 and sequence length of 128. To evaluate the inference speed on GPU, we follow [12], and experiment on the QNLI training set with batch size of 128 and sequence length of 128. The numbers are the average running time of 100 batches on a NVIDIA K40 GPU. To evaluate the inference speed on CPU, we experiment on Kirin 810 A76 ARM CPU with batch size of 1 and sequence length of 128.\n\nFigure 1 :\n1A network with adaptive width and depth. One single model can run at different depths and widths to satisfy various deployment requirements.\n\nFigure 2 :\n2Rewire connections in BERT based on the importance of attention heads and neurons. The left side is a BERT model with L Transformer layers. The middle shows attention heads in MHA and neurons in the intermediate layer in FFN ranked by their importance. The right side shows rewiring connections in each Transformer layer based on the importance of the heads and neurons.\n\nFigure 3 :\n3Using knowledge distillation (dashed lines) to transfer the knowledge from a fixed teacher model to student sub-networks at different widths in DynaBERT W . Distillation loss is computed by matching the logits, embedding and hidden states between the teacher and students. The width multipliers used are [1.0, 0.75, 0.5, 0.25].\n\n\nH l from the teacher model. The loss is computed over all L Transformer layers hidn (H (mw) , H) = L l=1 MSE(H (mw) l , H l ).\n\nFigure 4 :\n4Using knowledge distillation (dashed lines) to transfer the knowledge from a fixed widthadaptive teacher model with the maximum depth to student sub-networks at different depths in the both width-and depth-adaptive model. Distillation loss is computed by matching the logits, embedding, and hidden states between the teacher and students. The width multipliers and depth multipliers used are[1.0, 0.75, 0.5, 0.25] and [1.0, 0.75, 0.5], respectively.\n\n\nand RoBERTa BASE , the number of Transformer layers is L = 12, the hidden state size is d = 768. In each Transformer layer, the number of heads in MHA is N H = 12, and the number of neurons in the intermediate layer in FFN is d f f = 3072. The list of width multipliers is [1.0, 0.75, 0.5, 0.25], and the list of depth multipliers is [1.0, 0.75, 0.5]. There are a total of 4 \u00d7 3 = 12 different configurations of sub-networks. The detailed hyperparameters for the width-adaptive training, both width-and depth-adaptive training and the final fine-tuning stages can be found in\n\n\nFigures 5-8show the comparison of our proposed DynaBERT and DynaRoBERTa with other compression methods under different efficiency constraints (i.e., #parameters, FLOPs, inference speed) on different hardware platforms (i.e., NVIDIA K40 GPU and Kirin 810 ARM CPU). Note that each number of TinyBERT and DistilBERT is run using a different model, while different numbers of LayerDrop and our proposed DynaBERT/DynaRoBERTa are run using different sub-networks within one model.As can be seen, the proposed DynaBERT and DynaRoBERTa achieve comparable accuracy as BERT BASE and RoBERTa BASE , but require fewer or the same number of parameters, FLOPs or lower latency on GPU or ARM CPU. Under the same efficiency constraint, sub-networks extracted from our proposed DynaBERT outperform DistilBERT on all data sets except STS-B under #parameters, and outperforms TinyBERT on all data sets except MRPC; Sub-networks extracted from DynaRoBERTa outperforms LayerDrop by a large margin. Our proposed method even consistently outperforms LayerDrop trained with much more data. We speculate that it is because LayerDrop only allows flexibility on the depth, while ours enable flexibility in both width and depth, which generates a significantly larger number of architectural configurations and better explores of the balance between model accuracy and size.(h) SST-2.\n\nFigure 5 :\n5Comparison of #parameters(G) between our proposed DynaBERT and DynaRoBERTa and other methods. Average accuracy of MNLI-m and MNLI-mm is plotted.\n\nFigure 6 :\n6Comparison of FLOPs(G) between our proposed DynaBERT and DynaRoBERTa and other methods. Average accuracy of MNLI-m and MNLI-mm is plotted.\n\nFigure 7 :\n7Comparison of NVIDIA K40 GPU latency(s) between our proposed DynaBERT and DynaRoBERTa and other methods. The latency is the running time of 100 batches with batch size of 128 and sequence length of 128. Average accuracy of MNLI-m and MNLI-mm is plotted.\n\nFigure 8 :\n8Comparison of Kirin 810 ARM CPU latency(ms) between our proposed DynaBERT and DynaRoBERTa and other methods. The latency is tested with batch size of 1 and sequence length of 128. Average accuracy of MNLI-m and MNLI-mm is plotted.\n\n\n], progressive rewiring first sorts the attention heads and neurons and rewires the corresponding connections before training to support width multipliers [1.0, 0.75]. Then the attention heads and neurons are sorted and the network is rewired again before supporting [1.0, 0.75, 0.5].\n\nTable 1 :\n1Results on development set of our proposed DynaBERT and DynaRoBERTa with different width and depth multipliers (m w , m d ). The highest accuracy among 12 different configurations in each block is highlighted.FromTable 1, the proposed DynaBERT (or DynaRoBERTa) achieves comparable performances as BERT BASE (or RoBERTa BASE ) with the same or smaller size. For most tasks, the sub-network of DynaBERT or DynaRoBERTa with the maximum depth and width does not necessarily have the best performance, indicating that there exists redundancy in the original BERT or RoBERTa model. Indeed the width and depth of the model for most tasks can be reduced without performance drop with the proposed method.Method \nCoLA \nSTS-B \nMRPC \nRTE \nBERT BASE \n58.1 \n89.8 \n87.7 \n71.1 \n\nDynaBERT \n\n(m w , m d ) \n1.0x \n0.75x \n0.5x \n1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x \n1.0x \n59.7 \n59.1 \n54.6 \n90.1 89.5 88.6 86.3 85.8 85.0 72.2 71.8 66.1 \n0.75x \n60.8 \n59.6 \n53.2 \n90.0 89.4 88.5 86.5 85.5 84.1 71.8 73.3 65.7 \n0.5x \n58.4 \n56.8 \n48.5 \n89.8 89.2 88.2 84.8 84.1 83.1 72.2 72.2 67.9 \n0.25x \n50.9 \n51.6 \n43.7 \n89.2 88.3 87.0 83.8 83.8 81.4 68.6 68.6 63.2 \nMNLI-(m/mm) \nQQP \nQNLI \nSST-2 \nBERT BASE \n84.8/84.9 \n90.9 \n92.0 \n92.9 \n\nDynaBERT \n\n(m w , m d ) \n1.0x \n0.75x \n0.5x \n1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x \n1.0x \n84.9/85.5 84.4/85.1 83.7/84.6 91.4 91.4 91.1 92.1 91.7 90.6 93.2 93.3 92.7 \n0.75x \n84.7/85.5 84.3/85.2 83.6/84.4 91.4 91.3 91.2 92.2 91.8 90.7 93.0 93.1 92.8 \n0.5x \n84.7/85.2 84.2/84.7 83.0/83.6 91.3 91.2 91.0 92.2 91.5 90.0 93.3 92.7 91.6 \n0.25x \n83.9/84.2 83.4/83.7 82.0/82.3 90.7 91.1 90.4 91.5 90.8 88.5 92.8 92.0 92.0 \nCoLA \nSTS-B \nMRPC \nRTE \nRoBERTa BASE \n65.1 \n91.2 \n90.7 \n81.2 \n\nDynaRoBERTa \n\n(m w , m d ) \n1.0x \n0.75x \n0.5x \n1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x \n1.0x \n63.6 \n61.0.7 \n59.5 \n91.3 91.0 90.0 88.7 89.7 88.5 82.3 78.7 72.9 \n0.75x \n63.7 \n61.4 \n54.9 \n91.0 90.7 89.7 90.0 89.2 88.2 79.4 77.3 70.8 \n0.5x \n61.3 \n58.1 \n52.9 \n90.3 90.1 88.9 90.4 90.0 86.5 75.1 73.6 71.5 \n0.25x \n54.2 \n46.7 \n39.8 \n89.6 89.2 87.5 88.2 88.0 84.3 70.0 70.0 66.8 \nMNLI-(m/mm) \nQQP \nQNLI \nSST-2 \nRoBERTa BASE \n87.5/87.5 \n91.8 \n93.1 \n95.2 \n\nDynaRoBERTa \n\n(m w , m d ) \n1.0x \n0.75x \n0.5x \n1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x \n1.0x \n88.3/87.6 87.7/87.2 86.2/85.8 92.0 92.0 91.7 92.9 92.5 91.4 95.1 94.3 93.3 \n0.75x \n88.0/87.3 87.5/86.7 85.8/85.4 91.9 91.8 91.6 92.8 92.4 91.3 94.6 94.3 93.3 \n0.5x \n87.1/86.4 86.8/85.9 84.8/84.2 91.7 91.5 91.2 92.3 91.9 90.8 93.6 94.2 92.9 \n0.25x \n84.6/84.7 84.0/83.7 82.1/82.0 91.2 91.0 90.5 90.9 90.9 89.3 93.9 93.2 91.6 \n\n\n\nTable 2 :\n2Results on test set of our proposed DynaBERT and DynaRoBERTa. Note that the evaluation metric for QQP and MRPC here is \"F1\".MNLI-m MNLI-mm QQP QNLI SST-2 CoLA STS-B MRPC RTE \nBERT BASE \n84.6 \n83.6 \n71.9 \n90.7 \n93.4 \n51.5 \n85.2 \n87.5 \n69.6 \nDynaBERT (m w , m d = 1, 1) \n84.5 \n84.1 \n72.1 \n91.3 \n93.0 \n54.9 \n84.4 \n87.9 \n69.9 \nRoBERTa BASE \n86.0 \n85.4 \n70.9 \n92.5 \n94.6 \n50.5 \n88.1 \n90.0 \n73.0 \nDynaRoBERTa (m w , m d = 1, 1) \n86.9 \n86.7 \n71.9 \n92.5 \n94.7 \n54.1 \n88.4 \n90.8 \n73.7 \n\n\n\nTable 3 :\n3Ablation study in the training of DynaBERT W . Results on the development set are reported. The highest average accuracy over four width multipliers is highlighted.m w \nMNLI-m MNLI-mm QQP QNLI SST-2 CoLA STS-B MRPC RTE avg. \n1.0x \n84.8 \n84.9 \n90.9 \n92.0 \n92.9 \n58.1 \n89.8 \n87.7 \n71.1 83.6 \n0.75x \n84.2 \n84.1 \n90.6 \n89.7 \n92.9 \n48.0 \n87.2 \n82.8 \n66.1 80.6 \nSeparate network \n0.5x \n81.7 \n81.7 \n89.7 \n86 \n91.4 \n37.2 \n84.5 \n75.5 \n55.2 75.9 \n0.25x \n77.9 \n77.9 \n89.9 \n83.7 \n86.7 \n14.7 \n77.4 \n71.3 \n57.4 70.8 \navg. \n82.2 \n82.2 \n90.3 \n87.8 \n91.0 \n39.9 \n84.6 \n78.8 \n61.6 77.6 \n1.0x \n84.5 \n85.1 \n91.3 \n91.7 \n92.9 \n58.1 \n89.9 \n83.3 \n69.3 82.9 \n0.75x \n83.5 \n84.0 \n91.1 \n90.1 \n91.7 \n54.5 \n88.7 \n82.6 \n65.7 81.3 \nVanilla DynaBERT W \n0.5x \n82.1 \n82.3 \n90.7 \n88.9 \n91.6 \n46.9 \n87.3 \n83.1 \n61 \n79.3 \n0.25x \n78.6 \n78.4 \n89.1 \n85.6 \n88.5 \n16.4 \n83.5 \n72.8 \n60.6 72.6 \navg. \n82.2 \n82.5 \n90.6 \n89.1 \n91.2 \n44.0 \n87.4 \n80.5 \n64.2 79.0 \n1.0x \n84.9 \n84.9 \n91.4 \n91.6 \n91.9 \n56.3 \n90.0 \n84.6 \n70.0 82.8 \n0.75x \n84.3 \n84.2 \n91.3 \n91.7 \n92.4 \n56.4 \n89.9 \n86.0 \n71.1 83.0 \n+ Network rewiring 0.5x \n82.9 \n82.9 \n91.0 \n90.6 \n91.9 \n47.7 \n89.2 \n84.1 \n71.5 81.3 \n0.25x \n80.4 \n80.0 \n90.0 \n87.8 \n90.4 \n45.1 \n87.3 \n80.4 \n66.0 78.6 \navg. \n83.1 \n83.0 \n90.9 \n90.4 \n91.7 \n51.4 \n89.1 \n83.8 \n69.7 81.4 \n1.0x \n85.1 \n85.4 \n91.1 \n92.5 \n92.9 \n59.0 \n90.0 \n86.0 \n70.0 83.5 \n+ Distillation and \n0.75x \n84.9 \n85.6 \n91.1 \n92.4 \n93.1 \n57.9 \n90.0 \n87.0 \n70.8 83.6 \nData Augmentation 0.5x \n84.4 \n84.9 \n91.0 \n92.3 \n93.0 \n56.7 \n89.9 \n87.3 \n71.5 83.4 \n0.25x \n83.4 \n83.8 \n90.6 \n91.2 \n91.7 \n49.9 \n89.0 \n84.1 \n65.7 81.0 \navg. \n84.5 \n84.9 \n91.0 \n92.1 \n92.7 \n55.9 \n89.7 \n86.1 \n69.5 82.9 \n\n\n\nTable 4 :\n4Ablation study in the training of DynaBERT. Results on the development set are reported. The highest average accuracy over four width multipliers for each depth multiplier is highlighted. w , m d ) 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x (m w , m d ) 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x (m w , m d ) 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5x 1.0x 0.75x 0.5xSST-2 \nCoLA \nMRPC \nRTE \n(m 1.0x \n92.0 91.6 90.9 58.5 57.7 42.9 85.3 83.8 78.4 67.9 66.8 66.4 \nVanilla DynaBERT \n0.75x \n92.3 91.6 91.1 57.9 56.4 42.4 86.0 83.1 78.7 69.0 66.8 63.9 \n0.5x \n91.9 91.9 90.6 55.9 53.3 40.6 86.0 83.1 79.7 68.2 65.0 63.9 \n0.25x \n91.6 91.3 89.0 52.0 50.0 27.6 83.1 80.4 77.5 65.3 63.5 60.3 \navg. \n92.0 91.6 90.4 56.1 54.4 38.4 85.1 82.6 78.6 67.6 65.5 63.6 \n1.0x \n92.9 93.3 92.7 57.1 56.7 52.6 86.3 85.8 85.0 72.2 70.4 66.1 \n+ Distillation and \n0.75x \n93.1 93.1 92.1 57.7 55.4 51.9 86.5 85.5 84.1 72.6 72.2 64.6 \nData augmentation 0.5x \n92.9 92.1 91.3 54.1 53.7 47.5 84.8 84.1 83.1 72.9 72.6 66.1 \n0.25x \n92.5 91.7 91.6 50.7 51.0 44.6 83.8 83.8 81.4 67.5 67.9 62.5 \navg. \n92.9 92.6 91.9 54.9 54.2 49.2 85.4 84.8 83.4 71.3 70.8 64.8 \n1.0x \n93.2 93.3 92.7 59.7 59.1 54.6 84.1 83.6 82.6 72.2 71.8 66.1 \n+ Fine-tuning \n0.75x \n93.0 93.1 92.8 60.8 59.6 53.2 84.8 83.6 82.8 71.8 73.3 65.7 \n0.5x \n93.3 92.7 91.6 58.4 56.8 48.5 83.6 83.3 82.6 72.2 72.2 67.9 \n0.25x \n92.8 92.0 92.0 50.9 51.6 43.7 82.6 83.6 81.1 68.6 68.6 63.2 \navg. \n93.1 92.8 92.3 57.5 56.8 50.0 83.8 83.5 82.3 71.2 71.5 65.7 \n\n\n\nTable 5 :\n5Comparison of results on development set between using conventional distillation and inplace distillation. For DynaBERT W , the average accuracy over four width multipliers are reported. For DynaBERT, the average accuracy over four width multipliers and three depth multipliers are reported. The higher accuracy in each group is highlighted.Distillation type SST-2 CoLA MRPC RTEDynaBERT W \nConventional \n92.7 \n55.9 \n86.1 \n69.5 \nInplace \n92.6 \n55.9 \n87.0 \n70.0 \n\nDynaBERT \nConventional \n92.7 \n54.8 \n83.2 \n69.5 \nInplace \n92.5 \n54.5 \n84.3 \n69.0 \n\n5.2 Different Methods to Train DynaBERT W \n\n\n\nTable 6 :\n6Results on development set on the GLUE benchmark using progressive rewiring in training DynaBERT W . m w MNLI-m MNLI-mm QQP QNLI SST-2 CoLA STS-B MRPC RTE avg.1.0x \n84.6 \n84.5 \n91.5 \n91.6 \n92.4 \n57.4 \n90.1 \n86.5 \n70.0 83.2 \n0.75x \n83.6 \n84.0 \n91.2 \n91.4 \n91.7 \n56.6 \n89.7 \n84.8 \n70.8 82.6 \n0.5x \n82.5 \n82.9 \n91.0 \n90.8 \n91.9 \n52.2 \n89.1 \n84.1 \n72.9 81.9 \n0.25x \n78.3 \n79.7 \n89.9 \n87.9 \n90.4 \n45.1 \n87.6 \n82.4 \n67.5 78.8 \navg. \n82.3 \n82.8 \n90.9 \n90.4 \n91.6 \n52.8 \n89.1 \n84.5 \n70.3 81.6 \n\n\n\nTable 7 :\n7Results on development set on the GLUE benchmark using universal slimmable training in training DynaBERT W . m w MNLI-m MNLI-mm QQP QNLI SST-2 CoLA STS-B MRPC RTE avg.1.0x \n84.6 \n85.0 \n91.2 \n91.7 \n92.4 \n59.7 \n90.0 \n85.3 \n69.0 83.2 \n0.75x \n84.0 \n84.5 \n91.1 \n91.3 \n92.5 \n56.7 \n90.0 \n85.3 \n70.4 82.9 \n0.5x \n82.2 \n82.6 \n90.7 \n90.5 \n91.1 \n52.1 \n89.2 \n85.3 \n71.5 81.7 \n0.25x \n79.7 \n79.5 \n89.3 \n87.5 \n90.1 \n36.4 \n87.3 \n79.4 \n67.5 77.4 \navg. \n82.6 \n82.9 \n90.6 \n90.3 \n91.5 \n51.2 \n89.1 \n83.8 \n69.6 81.3 \n\n\n\nTable 8 :\n8Hyperparameters for different stages in training DynaBERT and DynaRoBERTa on the GLUE benchmark.Width-adaptive \nWidth-and depth-adaptive \nFinal fine-tuning \nData augmentation \ny \ny \nn \nDistillation \ny \ny \nn \n\u03bb 1 , \u03bb 2 \n1.0, 0.1 \n1.0,1.0 \n-\nWidth mutipliers \n[1.0, 0.75,0.5,0.25] \n[1.0, 0.75,0.5,0.25] \n[1.0, 0.75,0.5,0.25] \nDepth multipliers \n1 \n[1.0, 0.75,0.5] \n[1.0, 0.75,0.5] \nBatch Size \n32 \n32 \n32 \nLearning Rate \n2e \u2212 5 \n2e \u2212 5 \n2e \u2212 5 \nWarmup Steps \n0 \n0 \n0 \nLearning Rate Decay \nLinear \nLinear \nLinear \nWeight Decay \n0 \n0 \n0 \nGradient Clipping \n1 \n1 \n1 \nDropout \n0.1 \n0.1 \n0.1 \nAttention Dropout \n0.1 \n0.1 \n0.1 \nMax Epochs (MNLI, QQP) \n1 \n1 \n3 \nMax Epochs (other datasets) \n3 \n3 \n3 \nLogging steps (MNLI, QQP) \n500 \n500 \n50 \nLogging steps (other datasets) \n50 \n50 \n10 \n\n\n\nDeep equilibrium models. S Bai, J Z Kolter, V Koltun, Advances in Neural Information Processing Systems. S. Bai, J. Z. Kolter, and V. Koltun. Deep equilibrium models. In Advances in Neural Information Processing Systems, pages 688-699, 2019.\n\nEfficient 8-bit quantization of transformer neural machine language translation model. A Bhandare, V Sripathi, D Karkada, V Menon, S Choi, K Datta, V Saletore, arXiv:1906.00532PreprintA. Bhandare, V. Sripathi, D. Karkada, V. Menon, S. Choi, K. Datta, and V. Saletore. Effi- cient 8-bit quantization of transformer neural machine language translation model. Preprint arXiv:1906.00532, 2019.\n\nFully quantizing a simplified transformer for end-to-end speech recognition. A Bie, B Venkitesh, J Monteiro, M Haidar, M Rezagholizadeh, arXiv:1911.03604PreprintA. Bie, B. Venkitesh, J. Monteiro, M. Haidar, M. Rezagholizadeh, et al. Fully quantizing a simplified transformer for end-to-end speech recognition. Preprint arXiv:1911.03604, 2019.\n\nOnce for all: Train one network and specialize it for efficient deployment. H Cai, C Gan, T Wang, Z Zhang, S Han, International Conference on Learning Representations. H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han. Once for all: Train one network and specialize it for efficient deployment. In International Conference on Learning Representations, 2020.\n\nElectra: Pre-training text encoders as discriminators rather than generators. K Clark, M Luong, Q V Le, C D Manning, International Conference on Learning Representations. K. Clark, M. Luong, Q. V. Le, and C. D. Manning. Electra: Pre-training text encoders as dis- criminators rather than generators. In International Conference on Learning Representations, 2019.\n\nFine-tune BERT with sparse self-attention mechanism. B Cui, Y Li, M Chen, Z Zhang, Conference on Empirical Methods in Natural Language Processing. B. Cui, Y. Li, M. Chen, and Z. Zhang. Fine-tune BERT with sparse self-attention mechanism. In Conference on Empirical Methods in Natural Language Processing, 2019.\n\nUniversal transformers. M Dehghani, S Gouws, O Vinyals, J Uszkoreit, L Kaiser, International Conference on Learning Representations. M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and L. Kaiser. Universal transformers. In International Conference on Learning Representations, 2019.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, North American Chapter of the Association for Computational Linguistics. J. Devlin, M. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, pages 4171-4186, 2019.\n\nDepth-adaptive transformer. M Elbayad, J Gu, E Grave, M Auli, International Conference on Learning Representations. M. Elbayad, J. Gu, E. Grave, and M. Auli. Depth-adaptive transformer. In International Conference on Learning Representations, 2020.\n\nReducing transformer depth on demand with structured dropout. A Fan, E Grave, A Joulin, International Conference on Learning Representations. A. Fan, E. Grave, and A. Joulin. Reducing transformer depth on demand with structured dropout. In International Conference on Learning Representations, 2019.\n\nCompressing bert: Studying the effects of weight pruning on transfer learning. M A Gordon, K Duh, N Andrews, arXiv:2002.08307PreprintM. A. Gordon, K. Duh, and N. Andrews. Compressing bert: Studying the effects of weight pruning on transfer learning. Preprint arXiv:2002.08307, 2020.\n\nX Jiao, Y Yin, L Shang, X Jiang, X Chen, L Li, F Wang, Q Liu, arXiv:1909.10351Tinybert: Distilling bert for natural language understanding. PreprintX. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. Tinybert: Distilling bert for natural language understanding. Preprint arXiv:1909.10351, 2019.\n\nAlbert: A lite bert for self-supervised learning of language representations. Z Lan, M Chen, S Goodman, K Gimpel, P Sharma, R Soricut, International Conference on Learning Representations. Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020.\n\nY Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, Roberta, arXiv:1907.11692A robustly optimized bert pretraining approach. PreprintY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. Preprint arXiv:1907.11692, 2019.\n\nA tensorized transformer for language modeling. X Ma, P Zhang, S Zhang, N Duan, Y Hou, D Song, M Zhou, Advances in Neural Information Processing Systems. X. Ma, P. Zhang, S. Zhang, N. Duan, Y. Hou, D. Song, and M. Zhou. A tensorized transformer for language modeling. In Advances in Neural Information Processing Systems, 2019.\n\nPruning a bert-based question answering model. J S Mccarley, arXiv:1910.06360PreprintJ.S. McCarley. Pruning a bert-based question answering model. Preprint arXiv:1910.06360, 2019.\n\nAre sixteen heads really better than one?. P Michel, O Levy, G Neubig, Advances in Neural Information Processing Systems. P. Michel, O. Levy, and G. Neubig. Are sixteen heads really better than one? In Advances in Neural Information Processing Systems, pages 14014-14024, 2019.\n\nPruning convolutional neural networks for resource efficient inference. P Molchanov, S Tyree, T Karras, T Aila, J Kautz, International Conference on Learning Representations. P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz. Pruning convolutional neural networks for resource efficient inference. In International Conference on Learning Representations, 2017.\n\nDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter. V Sanh, L Debut, J Chaumond, T Wolf, arXiv:1910.01108PreprintV. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Preprint arXiv:1910.01108, 2019.\n\nQbert: Hessian based ultra low precision quantization of bert. S Shen, Z Dong, J Ye, L Ma, Z Yao, A Gholami, M W Mahoney, K Keutzer, AAAI Conference on Artificial Intelligence. S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. W. Mahoney, and K. Keutzer. Q- bert: Hessian based ultra low precision quantization of bert. In AAAI Conference on Artificial Intelligence, 2020.\n\nPatient knowledge distillation for bert model compression. S Sun, Y Cheng, Z Gan, J Liu, Conference on Empirical Methods in Natural Language Processing. S. Sun, Y. Cheng, Z. Gan, and J. Liu. Patient knowledge distillation for bert model compres- sion. In Conference on Empirical Methods in Natural Language Processing, pages 4314-4323, 2019.\n\nPolosukhin. Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I , Advances in neural information processing systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polo- sukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017.\n\nAnalyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. E Voita, D Talbot, F Moiseev, R Sennrich, I Titov, Annual Conference of the Association for Computational Linguistics. E. Voita, D. Talbot, F. Moiseev, R. Sennrich, and I. Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Annual Conference of the Association for Computational Linguistics, 2019.\n\nGlue: A multi-task benchmark and analysis platform for natural language understanding. A Wang, A Singh, J Michael, F Hill, O Levy, S R Bowman, International Conference on Learning Representations. A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task bench- mark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019.\n\nMinilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. W Wang, F Wei, L Dong, H Bao, N Yang, M Zhou, arXiv:2002.10957PreprintW. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou. Minilm: Deep self-attention distil- lation for task-agnostic compression of pre-trained transformers. Preprint arXiv:2002.10957, 2020.\n\nNetwork slimming by slimmable networks: Towards one-shot architecture search for channel numbers. J Yu, T S Huang, arXiv:1903.11728PreprintJ. Yu and T. S. Huang. Network slimming by slimmable networks: Towards one-shot architec- ture search for channel numbers. Preprint arXiv:1903.11728, 2019.\n\nUniversally slimmable networks and improved training techniques. J Yu, T S Huang, IEEE International Conference on Computer Vision. J. Yu and T. S. Huang. Universally slimmable networks and improved training techniques. In IEEE International Conference on Computer Vision, pages 1803-1811, 2019.\n\nJ Yu, L Yang, N Xu, J Yang, T Huang, Slimmable neural networks. In International Conference on Learning Representations. J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang. Slimmable neural networks. In International Conference on Learning Representations, 2018.\n\nO Zafrir, G Boudoukh, P Izsak, M Wasserblat, arXiv:1910.06188Q8bert: Quantized 8bit bert. PreprintO. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat. Q8bert: Quantized 8bit bert. Preprint arXiv:1910.06188, 2019.\n", "annotations": {"author": "[{\"end\":107,\"start\":58},{\"end\":169,\"start\":108},{\"end\":225,\"start\":170},{\"end\":277,\"start\":226}]", "publisher": null, "author_last_name": "[{\"end\":64,\"start\":61},{\"end\":120,\"start\":115},{\"end\":179,\"start\":174},{\"end\":233,\"start\":230}]", "author_first_name": "[{\"end\":60,\"start\":58},{\"end\":114,\"start\":108},{\"end\":173,\"start\":170},{\"end\":229,\"start\":226}]", "author_affiliation": "[{\"end\":106,\"start\":84},{\"end\":168,\"start\":146},{\"end\":224,\"start\":202},{\"end\":276,\"start\":254}]", "title": "[{\"end\":55,\"start\":1},{\"end\":332,\"start\":278}]", "venue": null, "abstract": "[{\"end\":1549,\"start\":334}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1632,\"start\":1628},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1656,\"start\":1653},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1673,\"start\":1669},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2964,\"start\":2960},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2967,\"start\":2964},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2987,\"start\":2984},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2990,\"start\":2987},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3019,\"start\":3015},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3022,\"start\":3019},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3025,\"start\":3022},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3043,\"start\":3040},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3046,\"start\":3043},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3049,\"start\":3046},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3066,\"start\":3062},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3069,\"start\":3066},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3072,\"start\":3069},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3074,\"start\":3072},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3077,\"start\":3074},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3192,\"start\":3189},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3195,\"start\":3192},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3197,\"start\":3195},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3525,\"start\":3521},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3528,\"start\":3525},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3764,\"start\":3760},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3767,\"start\":3764},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3770,\"start\":3767},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4493,\"start\":4490},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4868,\"start\":4865},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7173,\"start\":7169},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7880,\"start\":7876},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8475,\"start\":8471},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8478,\"start\":8475},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8498,\"start\":8495},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8501,\"start\":8498},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8520,\"start\":8516},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8523,\"start\":8520},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8526,\"start\":8523},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8545,\"start\":8541},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8548,\"start\":8545},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8550,\"start\":8548},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8566,\"start\":8562},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8568,\"start\":8566},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8571,\"start\":8568},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8574,\"start\":8571},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8577,\"start\":8574},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8700,\"start\":8696},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8795,\"start\":8791},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9040,\"start\":9037},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9215,\"start\":9212},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9315,\"start\":9311},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9669,\"start\":9665},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9844,\"start\":9840},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9914,\"start\":9910},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10180,\"start\":10176},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10419,\"start\":10416},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10595,\"start\":10591},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10761,\"start\":10757},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10996,\"start\":10993},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11138,\"start\":11134},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11310,\"start\":11306},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11313,\"start\":11310},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11434,\"start\":11430},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11522,\"start\":11519},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12179,\"start\":12175},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12182,\"start\":12179},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13219,\"start\":13215},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13312,\"start\":13310},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14348,\"start\":14344},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14351,\"start\":14348},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14732,\"start\":14728},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15188,\"start\":15185},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16256,\"start\":16253},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18210,\"start\":18206},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20185,\"start\":20181},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20188,\"start\":20185},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20191,\"start\":20188},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20450,\"start\":20446},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20528,\"start\":20524},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22461,\"start\":22458},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22724,\"start\":22720},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22749,\"start\":22746},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22771,\"start\":22767},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23490,\"start\":23486},{\"end\":23984,\"start\":23980},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24286,\"start\":24283},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24309,\"start\":24305},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24330,\"start\":24326},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24349,\"start\":24345},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24373,\"start\":24369},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28094,\"start\":28090},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29415,\"start\":29412},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29540,\"start\":29536},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30692,\"start\":30688},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30769,\"start\":30765},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32644,\"start\":32641},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32764,\"start\":32760}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33215,\"start\":33062},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33599,\"start\":33216},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33940,\"start\":33600},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34069,\"start\":33941},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34532,\"start\":34070},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35110,\"start\":34533},{\"attributes\":{\"id\":\"fig_7\"},\"end\":36469,\"start\":35111},{\"attributes\":{\"id\":\"fig_8\"},\"end\":36627,\"start\":36470},{\"attributes\":{\"id\":\"fig_17\"},\"end\":36779,\"start\":36628},{\"attributes\":{\"id\":\"fig_18\"},\"end\":37046,\"start\":36780},{\"attributes\":{\"id\":\"fig_22\"},\"end\":37290,\"start\":37047},{\"attributes\":{\"id\":\"fig_23\"},\"end\":37577,\"start\":37291},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40169,\"start\":37578},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40660,\"start\":40170},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42299,\"start\":40661},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43837,\"start\":42300},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44438,\"start\":43838},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44938,\"start\":44439},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":45446,\"start\":44939},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":46236,\"start\":45447}]", "paragraph": "[{\"end\":2830,\"start\":1565},{\"end\":3843,\"start\":2832},{\"end\":5489,\"start\":3845},{\"end\":6914,\"start\":5491},{\"end\":7091,\"start\":6931},{\"end\":7382,\"start\":7113},{\"end\":7595,\"start\":7384},{\"end\":7881,\"start\":7792},{\"end\":8010,\"start\":7964},{\"end\":8234,\"start\":8119},{\"end\":8362,\"start\":8308},{\"end\":8578,\"start\":8399},{\"end\":8950,\"start\":8580},{\"end\":9565,\"start\":8952},{\"end\":10110,\"start\":9567},{\"end\":10685,\"start\":10112},{\"end\":11663,\"start\":10687},{\"end\":12273,\"start\":11665},{\"end\":12530,\"start\":12284},{\"end\":13128,\"start\":12574},{\"end\":13981,\"start\":13205},{\"end\":14607,\"start\":14002},{\"end\":14844,\"start\":14678},{\"end\":15035,\"start\":14866},{\"end\":15391,\"start\":15069},{\"end\":15409,\"start\":15393},{\"end\":15805,\"start\":15442},{\"end\":17154,\"start\":15900},{\"end\":17365,\"start\":17214},{\"end\":17563,\"start\":17420},{\"end\":17601,\"start\":17565},{\"end\":18348,\"start\":17678},{\"end\":18720,\"start\":18350},{\"end\":18968,\"start\":18722},{\"end\":19074,\"start\":18970},{\"end\":20721,\"start\":19126},{\"end\":21492,\"start\":20798},{\"end\":21551,\"start\":21494},{\"end\":22350,\"start\":21553},{\"end\":22462,\"start\":22352},{\"end\":22576,\"start\":22464},{\"end\":23400,\"start\":22591},{\"end\":23832,\"start\":23412},{\"end\":24007,\"start\":23834},{\"end\":24607,\"start\":24009},{\"end\":25340,\"start\":24624},{\"end\":25793,\"start\":25342},{\"end\":25893,\"start\":25812},{\"end\":26620,\"start\":25895},{\"end\":27532,\"start\":26622},{\"end\":27975,\"start\":27534},{\"end\":29604,\"start\":28057},{\"end\":31427,\"start\":29606},{\"end\":32335,\"start\":31458},{\"end\":32535,\"start\":32357},{\"end\":33061,\"start\":32568}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7655,\"start\":7596},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7791,\"start\":7680},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7963,\"start\":7882},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8118,\"start\":8011},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8307,\"start\":8235},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14677,\"start\":14608},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14865,\"start\":14845},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15068,\"start\":15036},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17213,\"start\":17155},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17419,\"start\":17366},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17677,\"start\":17602},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20797,\"start\":20722}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":23992,\"start\":23985},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24665,\"start\":24658},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25386,\"start\":25379},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26069,\"start\":26062},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26634,\"start\":26627},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27408,\"start\":27401},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27546,\"start\":27539},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":28798,\"start\":28791},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":30143,\"start\":30136},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30489,\"start\":30482},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":31080,\"start\":31073},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31292,\"start\":31285},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":32533,\"start\":32526}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1563,\"start\":1551},{\"attributes\":{\"n\":\"2\"},\"end\":6929,\"start\":6917},{\"attributes\":{\"n\":\"2.1\"},\"end\":7111,\"start\":7094},{\"end\":7679,\"start\":7657},{\"attributes\":{\"n\":\"2.2\"},\"end\":8397,\"start\":8365},{\"attributes\":{\"n\":\"3\"},\"end\":12282,\"start\":12276},{\"attributes\":{\"n\":\"3.1\"},\"end\":12572,\"start\":12533},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":13203,\"start\":13131},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":14000,\"start\":13984},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":15440,\"start\":15412},{\"end\":15898,\"start\":15808},{\"attributes\":{\"n\":\"3.2\"},\"end\":19124,\"start\":19077},{\"attributes\":{\"n\":\"4\"},\"end\":22589,\"start\":22579},{\"attributes\":{\"n\":\"4.1\"},\"end\":23410,\"start\":23403},{\"attributes\":{\"n\":\"4.2\"},\"end\":24622,\"start\":24610},{\"attributes\":{\"n\":\"4.3\"},\"end\":25810,\"start\":25796},{\"attributes\":{\"n\":\"5\"},\"end\":27988,\"start\":27978},{\"attributes\":{\"n\":\"5.1\"},\"end\":28055,\"start\":27991},{\"attributes\":{\"n\":\"6\"},\"end\":31456,\"start\":31430},{\"end\":32355,\"start\":32338},{\"end\":32566,\"start\":32538},{\"end\":33073,\"start\":33063},{\"end\":33227,\"start\":33217},{\"end\":33611,\"start\":33601},{\"end\":34081,\"start\":34071},{\"end\":36481,\"start\":36471},{\"end\":36639,\"start\":36629},{\"end\":36791,\"start\":36781},{\"end\":37058,\"start\":37048},{\"end\":37588,\"start\":37579},{\"end\":40180,\"start\":40171},{\"end\":40671,\"start\":40662},{\"end\":42310,\"start\":42301},{\"end\":43848,\"start\":43839},{\"end\":44449,\"start\":44440},{\"end\":44949,\"start\":44940},{\"end\":45457,\"start\":45448}]", "table": "[{\"end\":40169,\"start\":38286},{\"end\":40660,\"start\":40306},{\"end\":42299,\"start\":40837},{\"end\":43837,\"start\":42727},{\"end\":44438,\"start\":44228},{\"end\":44938,\"start\":44610},{\"end\":45446,\"start\":45118},{\"end\":46236,\"start\":45555}]", "figure_caption": "[{\"end\":33215,\"start\":33075},{\"end\":33599,\"start\":33229},{\"end\":33940,\"start\":33613},{\"end\":34069,\"start\":33943},{\"end\":34532,\"start\":34083},{\"end\":35110,\"start\":34535},{\"end\":36469,\"start\":35113},{\"end\":36627,\"start\":36483},{\"end\":36779,\"start\":36641},{\"end\":37046,\"start\":36793},{\"end\":37290,\"start\":37060},{\"end\":37577,\"start\":37293},{\"end\":38286,\"start\":37590},{\"end\":40306,\"start\":40182},{\"end\":40837,\"start\":40673},{\"end\":42727,\"start\":42312},{\"end\":44228,\"start\":43850},{\"end\":44610,\"start\":44451},{\"end\":45118,\"start\":44951},{\"end\":45555,\"start\":45459}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15302,\"start\":15294},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15803,\"start\":15794},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19542,\"start\":19534},{\"end\":20922,\"start\":20910}]", "bib_author_first_name": "[{\"end\":46264,\"start\":46263},{\"end\":46271,\"start\":46270},{\"end\":46273,\"start\":46272},{\"end\":46283,\"start\":46282},{\"end\":46569,\"start\":46568},{\"end\":46581,\"start\":46580},{\"end\":46593,\"start\":46592},{\"end\":46604,\"start\":46603},{\"end\":46613,\"start\":46612},{\"end\":46621,\"start\":46620},{\"end\":46630,\"start\":46629},{\"end\":46950,\"start\":46949},{\"end\":46957,\"start\":46956},{\"end\":46970,\"start\":46969},{\"end\":46982,\"start\":46981},{\"end\":46992,\"start\":46991},{\"end\":47293,\"start\":47292},{\"end\":47300,\"start\":47299},{\"end\":47307,\"start\":47306},{\"end\":47315,\"start\":47314},{\"end\":47324,\"start\":47323},{\"end\":47650,\"start\":47649},{\"end\":47659,\"start\":47658},{\"end\":47668,\"start\":47667},{\"end\":47670,\"start\":47669},{\"end\":47676,\"start\":47675},{\"end\":47678,\"start\":47677},{\"end\":47989,\"start\":47988},{\"end\":47996,\"start\":47995},{\"end\":48002,\"start\":48001},{\"end\":48010,\"start\":48009},{\"end\":48272,\"start\":48271},{\"end\":48284,\"start\":48283},{\"end\":48293,\"start\":48292},{\"end\":48304,\"start\":48303},{\"end\":48317,\"start\":48316},{\"end\":48615,\"start\":48614},{\"end\":48625,\"start\":48624},{\"end\":48634,\"start\":48633},{\"end\":48641,\"start\":48640},{\"end\":48984,\"start\":48983},{\"end\":48995,\"start\":48994},{\"end\":49001,\"start\":49000},{\"end\":49010,\"start\":49009},{\"end\":49268,\"start\":49267},{\"end\":49275,\"start\":49274},{\"end\":49284,\"start\":49283},{\"end\":49586,\"start\":49585},{\"end\":49588,\"start\":49587},{\"end\":49598,\"start\":49597},{\"end\":49605,\"start\":49604},{\"end\":49791,\"start\":49790},{\"end\":49799,\"start\":49798},{\"end\":49806,\"start\":49805},{\"end\":49815,\"start\":49814},{\"end\":49824,\"start\":49823},{\"end\":49832,\"start\":49831},{\"end\":49838,\"start\":49837},{\"end\":49846,\"start\":49845},{\"end\":50187,\"start\":50186},{\"end\":50194,\"start\":50193},{\"end\":50202,\"start\":50201},{\"end\":50213,\"start\":50212},{\"end\":50223,\"start\":50222},{\"end\":50233,\"start\":50232},{\"end\":50507,\"start\":50506},{\"end\":50514,\"start\":50513},{\"end\":50521,\"start\":50520},{\"end\":50530,\"start\":50529},{\"end\":50536,\"start\":50535},{\"end\":50545,\"start\":50544},{\"end\":50553,\"start\":50552},{\"end\":50561,\"start\":50560},{\"end\":50570,\"start\":50569},{\"end\":50585,\"start\":50584},{\"end\":50921,\"start\":50920},{\"end\":50927,\"start\":50926},{\"end\":50936,\"start\":50935},{\"end\":50945,\"start\":50944},{\"end\":50953,\"start\":50952},{\"end\":50960,\"start\":50959},{\"end\":50968,\"start\":50967},{\"end\":51249,\"start\":51248},{\"end\":51251,\"start\":51250},{\"end\":51426,\"start\":51425},{\"end\":51436,\"start\":51435},{\"end\":51444,\"start\":51443},{\"end\":51734,\"start\":51733},{\"end\":51747,\"start\":51746},{\"end\":51756,\"start\":51755},{\"end\":51766,\"start\":51765},{\"end\":51774,\"start\":51773},{\"end\":52110,\"start\":52109},{\"end\":52118,\"start\":52117},{\"end\":52127,\"start\":52126},{\"end\":52139,\"start\":52138},{\"end\":52392,\"start\":52391},{\"end\":52400,\"start\":52399},{\"end\":52408,\"start\":52407},{\"end\":52414,\"start\":52413},{\"end\":52420,\"start\":52419},{\"end\":52427,\"start\":52426},{\"end\":52438,\"start\":52437},{\"end\":52440,\"start\":52439},{\"end\":52451,\"start\":52450},{\"end\":52767,\"start\":52766},{\"end\":52774,\"start\":52773},{\"end\":52783,\"start\":52782},{\"end\":52790,\"start\":52789},{\"end\":53090,\"start\":53089},{\"end\":53101,\"start\":53100},{\"end\":53112,\"start\":53111},{\"end\":53122,\"start\":53121},{\"end\":53135,\"start\":53134},{\"end\":53144,\"start\":53143},{\"end\":53146,\"start\":53145},{\"end\":53155,\"start\":53154},{\"end\":53165,\"start\":53164},{\"end\":53530,\"start\":53529},{\"end\":53539,\"start\":53538},{\"end\":53549,\"start\":53548},{\"end\":53560,\"start\":53559},{\"end\":53572,\"start\":53571},{\"end\":53975,\"start\":53974},{\"end\":53983,\"start\":53982},{\"end\":53992,\"start\":53991},{\"end\":54003,\"start\":54002},{\"end\":54011,\"start\":54010},{\"end\":54019,\"start\":54018},{\"end\":54021,\"start\":54020},{\"end\":54405,\"start\":54404},{\"end\":54413,\"start\":54412},{\"end\":54420,\"start\":54419},{\"end\":54428,\"start\":54427},{\"end\":54435,\"start\":54434},{\"end\":54443,\"start\":54442},{\"end\":54765,\"start\":54764},{\"end\":54771,\"start\":54770},{\"end\":54773,\"start\":54772},{\"end\":55028,\"start\":55027},{\"end\":55034,\"start\":55033},{\"end\":55036,\"start\":55035},{\"end\":55260,\"start\":55259},{\"end\":55266,\"start\":55265},{\"end\":55274,\"start\":55273},{\"end\":55280,\"start\":55279},{\"end\":55288,\"start\":55287},{\"end\":55518,\"start\":55517},{\"end\":55528,\"start\":55527},{\"end\":55540,\"start\":55539},{\"end\":55549,\"start\":55548}]", "bib_author_last_name": "[{\"end\":46268,\"start\":46265},{\"end\":46280,\"start\":46274},{\"end\":46290,\"start\":46284},{\"end\":46578,\"start\":46570},{\"end\":46590,\"start\":46582},{\"end\":46601,\"start\":46594},{\"end\":46610,\"start\":46605},{\"end\":46618,\"start\":46614},{\"end\":46627,\"start\":46622},{\"end\":46639,\"start\":46631},{\"end\":46954,\"start\":46951},{\"end\":46967,\"start\":46958},{\"end\":46979,\"start\":46971},{\"end\":46989,\"start\":46983},{\"end\":47007,\"start\":46993},{\"end\":47297,\"start\":47294},{\"end\":47304,\"start\":47301},{\"end\":47312,\"start\":47308},{\"end\":47321,\"start\":47316},{\"end\":47328,\"start\":47325},{\"end\":47656,\"start\":47651},{\"end\":47665,\"start\":47660},{\"end\":47673,\"start\":47671},{\"end\":47686,\"start\":47679},{\"end\":47993,\"start\":47990},{\"end\":47999,\"start\":47997},{\"end\":48007,\"start\":48003},{\"end\":48016,\"start\":48011},{\"end\":48281,\"start\":48273},{\"end\":48290,\"start\":48285},{\"end\":48301,\"start\":48294},{\"end\":48314,\"start\":48305},{\"end\":48324,\"start\":48318},{\"end\":48622,\"start\":48616},{\"end\":48631,\"start\":48626},{\"end\":48638,\"start\":48635},{\"end\":48651,\"start\":48642},{\"end\":48992,\"start\":48985},{\"end\":48998,\"start\":48996},{\"end\":49007,\"start\":49002},{\"end\":49015,\"start\":49011},{\"end\":49272,\"start\":49269},{\"end\":49281,\"start\":49276},{\"end\":49291,\"start\":49285},{\"end\":49595,\"start\":49589},{\"end\":49602,\"start\":49599},{\"end\":49613,\"start\":49606},{\"end\":49796,\"start\":49792},{\"end\":49803,\"start\":49800},{\"end\":49812,\"start\":49807},{\"end\":49821,\"start\":49816},{\"end\":49829,\"start\":49825},{\"end\":49835,\"start\":49833},{\"end\":49843,\"start\":49839},{\"end\":49850,\"start\":49847},{\"end\":50191,\"start\":50188},{\"end\":50199,\"start\":50195},{\"end\":50210,\"start\":50203},{\"end\":50220,\"start\":50214},{\"end\":50230,\"start\":50224},{\"end\":50241,\"start\":50234},{\"end\":50511,\"start\":50508},{\"end\":50518,\"start\":50515},{\"end\":50527,\"start\":50522},{\"end\":50533,\"start\":50531},{\"end\":50542,\"start\":50537},{\"end\":50550,\"start\":50546},{\"end\":50558,\"start\":50554},{\"end\":50567,\"start\":50562},{\"end\":50582,\"start\":50571},{\"end\":50594,\"start\":50586},{\"end\":50603,\"start\":50596},{\"end\":50924,\"start\":50922},{\"end\":50933,\"start\":50928},{\"end\":50942,\"start\":50937},{\"end\":50950,\"start\":50946},{\"end\":50957,\"start\":50954},{\"end\":50965,\"start\":50961},{\"end\":50973,\"start\":50969},{\"end\":51260,\"start\":51252},{\"end\":51433,\"start\":51427},{\"end\":51441,\"start\":51437},{\"end\":51451,\"start\":51445},{\"end\":51744,\"start\":51735},{\"end\":51753,\"start\":51748},{\"end\":51763,\"start\":51757},{\"end\":51771,\"start\":51767},{\"end\":51780,\"start\":51775},{\"end\":52115,\"start\":52111},{\"end\":52124,\"start\":52119},{\"end\":52136,\"start\":52128},{\"end\":52144,\"start\":52140},{\"end\":52397,\"start\":52393},{\"end\":52405,\"start\":52401},{\"end\":52411,\"start\":52409},{\"end\":52417,\"start\":52415},{\"end\":52424,\"start\":52421},{\"end\":52435,\"start\":52428},{\"end\":52448,\"start\":52441},{\"end\":52459,\"start\":52452},{\"end\":52771,\"start\":52768},{\"end\":52780,\"start\":52775},{\"end\":52787,\"start\":52784},{\"end\":52794,\"start\":52791},{\"end\":53098,\"start\":53091},{\"end\":53109,\"start\":53102},{\"end\":53119,\"start\":53113},{\"end\":53132,\"start\":53123},{\"end\":53141,\"start\":53136},{\"end\":53152,\"start\":53147},{\"end\":53162,\"start\":53156},{\"end\":53536,\"start\":53531},{\"end\":53546,\"start\":53540},{\"end\":53557,\"start\":53550},{\"end\":53569,\"start\":53561},{\"end\":53578,\"start\":53573},{\"end\":53980,\"start\":53976},{\"end\":53989,\"start\":53984},{\"end\":54000,\"start\":53993},{\"end\":54008,\"start\":54004},{\"end\":54016,\"start\":54012},{\"end\":54028,\"start\":54022},{\"end\":54410,\"start\":54406},{\"end\":54417,\"start\":54414},{\"end\":54425,\"start\":54421},{\"end\":54432,\"start\":54429},{\"end\":54440,\"start\":54436},{\"end\":54448,\"start\":54444},{\"end\":54768,\"start\":54766},{\"end\":54779,\"start\":54774},{\"end\":55031,\"start\":55029},{\"end\":55042,\"start\":55037},{\"end\":55263,\"start\":55261},{\"end\":55271,\"start\":55267},{\"end\":55277,\"start\":55275},{\"end\":55285,\"start\":55281},{\"end\":55294,\"start\":55289},{\"end\":55525,\"start\":55519},{\"end\":55537,\"start\":55529},{\"end\":55546,\"start\":55541},{\"end\":55560,\"start\":55550}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":202539738},\"end\":46479,\"start\":46238},{\"attributes\":{\"doi\":\"arXiv:1906.00532\",\"id\":\"b1\"},\"end\":46870,\"start\":46481},{\"attributes\":{\"doi\":\"arXiv:1911.03604\",\"id\":\"b2\"},\"end\":47214,\"start\":46872},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":201666112},\"end\":47569,\"start\":47216},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":213152193},\"end\":47933,\"start\":47571},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":202771940},\"end\":48245,\"start\":47935},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":49667762},\"end\":48530,\"start\":48247},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":48953,\"start\":48532},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":204824061},\"end\":49203,\"start\":48955},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202750230},\"end\":49504,\"start\":49205},{\"attributes\":{\"doi\":\"arXiv:2002.08307\",\"id\":\"b10\"},\"end\":49788,\"start\":49506},{\"attributes\":{\"doi\":\"arXiv:1909.10351\",\"id\":\"b11\"},\"end\":50106,\"start\":49790},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":202888986},\"end\":50504,\"start\":50108},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b13\"},\"end\":50870,\"start\":50506},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":195345467},\"end\":51199,\"start\":50872},{\"attributes\":{\"doi\":\"arXiv:1910.06360\",\"id\":\"b15\"},\"end\":51380,\"start\":51201},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":166227946},\"end\":51659,\"start\":51382},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17240902},\"end\":52028,\"start\":51661},{\"attributes\":{\"doi\":\"arXiv:1910.01108\",\"id\":\"b18\"},\"end\":52326,\"start\":52030},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":202565587},\"end\":52705,\"start\":52328},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":201670719},\"end\":53048,\"start\":52707},{\"attributes\":{\"id\":\"b21\"},\"end\":53426,\"start\":53050},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":162183964},\"end\":53885,\"start\":53428},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5034059},\"end\":54302,\"start\":53887},{\"attributes\":{\"doi\":\"arXiv:2002.10957\",\"id\":\"b24\"},\"end\":54664,\"start\":54304},{\"attributes\":{\"doi\":\"arXiv:1903.11728\",\"id\":\"b25\"},\"end\":54960,\"start\":54666},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":76660361},\"end\":55257,\"start\":54962},{\"attributes\":{\"id\":\"b27\"},\"end\":55515,\"start\":55259},{\"attributes\":{\"doi\":\"arXiv:1910.06188\",\"id\":\"b28\"},\"end\":55729,\"start\":55517}]", "bib_title": "[{\"end\":46261,\"start\":46238},{\"end\":47290,\"start\":47216},{\"end\":47647,\"start\":47571},{\"end\":47986,\"start\":47935},{\"end\":48269,\"start\":48247},{\"end\":48612,\"start\":48532},{\"end\":48981,\"start\":48955},{\"end\":49265,\"start\":49205},{\"end\":50184,\"start\":50108},{\"end\":50918,\"start\":50872},{\"end\":51423,\"start\":51382},{\"end\":51731,\"start\":51661},{\"end\":52389,\"start\":52328},{\"end\":52764,\"start\":52707},{\"end\":53087,\"start\":53050},{\"end\":53527,\"start\":53428},{\"end\":53972,\"start\":53887},{\"end\":55025,\"start\":54962}]", "bib_author": "[{\"end\":46270,\"start\":46263},{\"end\":46282,\"start\":46270},{\"end\":46292,\"start\":46282},{\"end\":46580,\"start\":46568},{\"end\":46592,\"start\":46580},{\"end\":46603,\"start\":46592},{\"end\":46612,\"start\":46603},{\"end\":46620,\"start\":46612},{\"end\":46629,\"start\":46620},{\"end\":46641,\"start\":46629},{\"end\":46956,\"start\":46949},{\"end\":46969,\"start\":46956},{\"end\":46981,\"start\":46969},{\"end\":46991,\"start\":46981},{\"end\":47009,\"start\":46991},{\"end\":47299,\"start\":47292},{\"end\":47306,\"start\":47299},{\"end\":47314,\"start\":47306},{\"end\":47323,\"start\":47314},{\"end\":47330,\"start\":47323},{\"end\":47658,\"start\":47649},{\"end\":47667,\"start\":47658},{\"end\":47675,\"start\":47667},{\"end\":47688,\"start\":47675},{\"end\":47995,\"start\":47988},{\"end\":48001,\"start\":47995},{\"end\":48009,\"start\":48001},{\"end\":48018,\"start\":48009},{\"end\":48283,\"start\":48271},{\"end\":48292,\"start\":48283},{\"end\":48303,\"start\":48292},{\"end\":48316,\"start\":48303},{\"end\":48326,\"start\":48316},{\"end\":48624,\"start\":48614},{\"end\":48633,\"start\":48624},{\"end\":48640,\"start\":48633},{\"end\":48653,\"start\":48640},{\"end\":48994,\"start\":48983},{\"end\":49000,\"start\":48994},{\"end\":49009,\"start\":49000},{\"end\":49017,\"start\":49009},{\"end\":49274,\"start\":49267},{\"end\":49283,\"start\":49274},{\"end\":49293,\"start\":49283},{\"end\":49597,\"start\":49585},{\"end\":49604,\"start\":49597},{\"end\":49615,\"start\":49604},{\"end\":49798,\"start\":49790},{\"end\":49805,\"start\":49798},{\"end\":49814,\"start\":49805},{\"end\":49823,\"start\":49814},{\"end\":49831,\"start\":49823},{\"end\":49837,\"start\":49831},{\"end\":49845,\"start\":49837},{\"end\":49852,\"start\":49845},{\"end\":50193,\"start\":50186},{\"end\":50201,\"start\":50193},{\"end\":50212,\"start\":50201},{\"end\":50222,\"start\":50212},{\"end\":50232,\"start\":50222},{\"end\":50243,\"start\":50232},{\"end\":50513,\"start\":50506},{\"end\":50520,\"start\":50513},{\"end\":50529,\"start\":50520},{\"end\":50535,\"start\":50529},{\"end\":50544,\"start\":50535},{\"end\":50552,\"start\":50544},{\"end\":50560,\"start\":50552},{\"end\":50569,\"start\":50560},{\"end\":50584,\"start\":50569},{\"end\":50596,\"start\":50584},{\"end\":50605,\"start\":50596},{\"end\":50926,\"start\":50920},{\"end\":50935,\"start\":50926},{\"end\":50944,\"start\":50935},{\"end\":50952,\"start\":50944},{\"end\":50959,\"start\":50952},{\"end\":50967,\"start\":50959},{\"end\":50975,\"start\":50967},{\"end\":51262,\"start\":51248},{\"end\":51435,\"start\":51425},{\"end\":51443,\"start\":51435},{\"end\":51453,\"start\":51443},{\"end\":51746,\"start\":51733},{\"end\":51755,\"start\":51746},{\"end\":51765,\"start\":51755},{\"end\":51773,\"start\":51765},{\"end\":51782,\"start\":51773},{\"end\":52117,\"start\":52109},{\"end\":52126,\"start\":52117},{\"end\":52138,\"start\":52126},{\"end\":52146,\"start\":52138},{\"end\":52399,\"start\":52391},{\"end\":52407,\"start\":52399},{\"end\":52413,\"start\":52407},{\"end\":52419,\"start\":52413},{\"end\":52426,\"start\":52419},{\"end\":52437,\"start\":52426},{\"end\":52450,\"start\":52437},{\"end\":52461,\"start\":52450},{\"end\":52773,\"start\":52766},{\"end\":52782,\"start\":52773},{\"end\":52789,\"start\":52782},{\"end\":52796,\"start\":52789},{\"end\":53100,\"start\":53089},{\"end\":53111,\"start\":53100},{\"end\":53121,\"start\":53111},{\"end\":53134,\"start\":53121},{\"end\":53143,\"start\":53134},{\"end\":53154,\"start\":53143},{\"end\":53164,\"start\":53154},{\"end\":53168,\"start\":53164},{\"end\":53538,\"start\":53529},{\"end\":53548,\"start\":53538},{\"end\":53559,\"start\":53548},{\"end\":53571,\"start\":53559},{\"end\":53580,\"start\":53571},{\"end\":53982,\"start\":53974},{\"end\":53991,\"start\":53982},{\"end\":54002,\"start\":53991},{\"end\":54010,\"start\":54002},{\"end\":54018,\"start\":54010},{\"end\":54030,\"start\":54018},{\"end\":54412,\"start\":54404},{\"end\":54419,\"start\":54412},{\"end\":54427,\"start\":54419},{\"end\":54434,\"start\":54427},{\"end\":54442,\"start\":54434},{\"end\":54450,\"start\":54442},{\"end\":54770,\"start\":54764},{\"end\":54781,\"start\":54770},{\"end\":55033,\"start\":55027},{\"end\":55044,\"start\":55033},{\"end\":55265,\"start\":55259},{\"end\":55273,\"start\":55265},{\"end\":55279,\"start\":55273},{\"end\":55287,\"start\":55279},{\"end\":55296,\"start\":55287},{\"end\":55527,\"start\":55517},{\"end\":55539,\"start\":55527},{\"end\":55548,\"start\":55539},{\"end\":55562,\"start\":55548}]", "bib_venue": "[{\"end\":46341,\"start\":46292},{\"end\":46566,\"start\":46481},{\"end\":46947,\"start\":46872},{\"end\":47382,\"start\":47330},{\"end\":47740,\"start\":47688},{\"end\":48080,\"start\":48018},{\"end\":48378,\"start\":48326},{\"end\":48724,\"start\":48653},{\"end\":49069,\"start\":49017},{\"end\":49345,\"start\":49293},{\"end\":49583,\"start\":49506},{\"end\":49928,\"start\":49868},{\"end\":50295,\"start\":50243},{\"end\":50667,\"start\":50621},{\"end\":51024,\"start\":50975},{\"end\":51246,\"start\":51201},{\"end\":51502,\"start\":51453},{\"end\":51834,\"start\":51782},{\"end\":52107,\"start\":52030},{\"end\":52503,\"start\":52461},{\"end\":52858,\"start\":52796},{\"end\":53217,\"start\":53168},{\"end\":53646,\"start\":53580},{\"end\":54082,\"start\":54030},{\"end\":54402,\"start\":54304},{\"end\":54762,\"start\":54666},{\"end\":55092,\"start\":55044},{\"end\":55378,\"start\":55296},{\"end\":55605,\"start\":55578}]"}}}, "year": 2023, "month": 12, "day": 17}