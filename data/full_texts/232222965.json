{"id": 232222965, "updated": "2023-10-06 05:32:12.573", "metadata": {"title": "FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism", "authors": "[{\"first\":\"Wei\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xi\",\"last\":\"Jia\",\"middle\":[]},{\"first\":\"Hyung\",\"last\":\"Chang\",\"middle\":[\"Jin\"]},{\"first\":\"Jinming\",\"last\":\"Duan\",\"middle\":[]},{\"first\":\"Linlin\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Ales\",\"last\":\"Leonardis\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 3, "day": 12}, "abstract": "In this paper, we focus on category-level 6D pose and size estimation from monocular RGB-D image. Previous methods suffer from inefficient category-level pose feature extraction which leads to low accuracy and inference speed. To tackle this problem, we propose a fast shape-based network (FS-Net) with efficient category-level feature extraction for 6D pose estimation. First, we design an orientation aware autoencoder with 3D graph convolution for latent feature extraction. The learned latent feature is insensitive to point shift and object size thanks to the shift and scale-invariance properties of the 3D graph convolution. Then, to efficiently decode category-level rotation information from the latent feature, we propose a novel decoupled rotation mechanism that employs two decoders to complementarily access the rotation information. Meanwhile, we estimate translation and size by two residuals, which are the difference between the mean of object points and ground truth translation, and the difference between the mean size of the category and ground truth size, respectively. Finally, to increase the generalization ability of FS-Net, we propose an online box-cage based 3D deformation mechanism to augment the training data. Extensive experiments on two benchmark datasets show that the proposed method achieves state-of-the-art performance in both category- and instance-level 6D object pose estimation. Especially in category-level pose estimation, without extra synthetic data, our method outperforms existing methods by 6.3% on the NOCS-REAL dataset.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.07054", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ChenJC0SL21", "doi": "10.1109/cvpr46437.2021.00163"}}, "content": {"source": {"pdf_hash": "99abfdaa9c7c9a36ce02ece9cdba9a00fd6c5bdc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.07054v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2103.07054", "status": "GREEN"}}, "grobid": {"id": "3222eaa6930d1de10f9c18945c22868e174dde38", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/99abfdaa9c7c9a36ce02ece9cdba9a00fd6c5bdc.txt", "contents": "\nFS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism\n\n\nWei Chen \nUniversity of Birmingham\n\n\nXi Jia \nUniversity of Birmingham\n\n\nHyung Jin Chang \nUniversity of Birmingham\n\n\nJinming Duan \nUniversity of Birmingham\n\n\nLinlin Shen \nShenzhen University\n\n\nAle\u0161 Leonardis \nUniversity of Birmingham\n\n\nFS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism\n\nIn this paper, we focus on category-level 6D pose and size estimation from monocular RGB-D image. Previous methods suffer from inefficient category-level pose feature extraction which leads to low accuracy and inference speed. To tackle this problem, we propose a fast shape-based network (FS-Net) with efficient category-level feature extraction for 6D pose estimation. First, we design an orientation aware autoencoder with 3D graph convolution for latent feature extraction. The learned latent feature is insensitive to point shift and object size thanks to the shift and scale-invariance properties of the 3D graph convolution. Then, to efficiently decode category-level rotation information from the latent feature, we propose a novel decoupled rotation mechanism that employs two decoders to complementarily access the rotation information. Meanwhile, we estimate translation and size by two residuals, which are the difference between the mean of object points and ground truth translation, and the difference between the mean size of the category and ground truth size, respectively. Finally, to increase the generalization ability of FS-Net, we propose an online box-cage based 3D deformation mechanism to augment the training data. Extensive experiments on two benchmark datasets show that the proposed method achieves state-of-the-art performance in both category-and instance-level 6D object pose estimation. Especially in category-level pose estimation, without extra synthetic data, our method outperforms existing methods by 6.3% on the NOCS-REAL dataset 1 .\n\nIntroduction\n\nEstimating 6D object pose plays an essential role in many computer vision tasks such as augmented reality [20,21], virtual reality [2], and smart robotic arm [47,36]. 1 Paper code https://github.com/DC1991/FS-Net We use different networks for different tasks. The RGB-based network is used for 2D object detection, and the shape-based 3D graph convolution autoencoder is used for 3D segmentation and rotation estimation. The residual-based network is used for translation and size estimation with segmented points.\n\nFor instance-level 6D pose estimation, in which training set and test set contain the same objects, huge progress has been made in recent years [42,29,22,16,10]. However, category-level 6D pose estimation remains challenging as the object shape and color are various in the same category. Existing methods addressed this problem by mapping the different objects in the same category into a uniform model via RGB feature or RGB-D fusion feature. For example, Wang et al. [41] trained a modified Mask R-CNN [9] to predict the normalized object coordinate space (NOCS) map of different objects based on RGB feature, and then computed the pose with observed depth and NOCS map by Umeyama algorithm [37]. Chen et al. [4] proposed to learn a canonical shape space (CASS) to tackle intra-class shape variations with RGB-D fusion feature [40]. Tian et al. [35] trained a network to predict the NOCS map of different objects, with the uniform shape prior learned from a shape collection, and RGB-D fusion feature [40].\n\nAlthough these methods achieved state-of-the-art performance, there are still two issues. Firstly, the benefits of using RGB feature or RGB-D fusion feature for category-level pose estimation are still questionable. In [38], Vlach et al. showed that people focus more on shape than color when categorizing objects, as different objects in the same category have very different colors but stable shapes (shown in Figure 3). Thereby the use of RGB feature for categorylevel pose estimation may lead to low performance due to huge color variation in the test scene. For this issue, to alleviate the color variation, we merely use the RGB feature for 2D detection, while using the shape feature learned with point cloud extracted from depth image for category-level pose estimation.\n\nSecondly, learning a representative uniform shape requires a large amount of training data; therefore, the performance of these methods is not guaranteed with limited training examples. To overcome this issue, we propose a 3D graph convolution (3DGC) autoencoder [19] to effectively learn the category-level pose feature via observed points reconstruction of different objects instead of uniform shape mapping. We further propose an online box-cage based 3D data augmentation mechanism to reduce the dependencies of labeled data.\n\nIn this paper, the newly proposed FS-Net consists of three parts: 2D detection, 3D segmentation & rotation estimation, and translation & size estimation. In 2D detection part, we use the YOLOv3 [31] to detect the object bounding box for coarse object points obtainment [6]. Then in the 3D segmentation & rotation estimation part, we design a 3DGC autoencoder to perform segmentation and observed points reconstruction jointly. The autoencoder encodes orientation information in the latent feature. Then we propose the decoupled rotation mechanism that uses two decoders to decode the category-level rotation information. For translation and size estimation, since they are all point coordinates related, we design a coordinate residual estimation network based on PointNet [27] to estimate the translation residual and size residuals. To further increase the generalization ability of FS-Net, we use the proposed online 3D deformation for data augmentation. To summarize, the main contributions of this paper are as follows:\n\n\u2022 We propose a fast shape-based network to estimate category-level 6D object size and pose. Due to the efficient category-level pose feature extraction, the framework runs at 20 FPS on a GTX 1080 Ti GPU.\n\n\u2022 We propose a 3DGC autoencoder to reconstruct the observed points for latent orientation feature learning. Then we design a decoupled rotation mechanism to fully decode the orientation information. This decoupled mechanism allows us to naturally handle the circle symmetry object (in Section 3.3).\n\n\u2022 Based-on the shape similarity of intra-class objects, we propose a novel box-cage based 3D deformation mech-anism to augment the training data. With this mechanism, the pose accuracy of FS-Net is improved by 7.7%.\n\n\nRelated Works\n\n\nInstance-Level Pose Estimation\n\nIn instance-level pose estimation, a known 3D object model is usually available for training and testing. Based on the 3D model, instance-level pose estimation can be roughly divided into three types: template matching based, correspondences-based, and voting-based methods. Template matching methods [11,30,22] aligned the template to the observed image or depth map via hand-crafted or deep learning feature descriptors. As they need the 3D object model to generate the template pool, their applications in category-level 6D pose estimation are limited. Correspondences-based methods trained their model to establish 2D-3D correspondences [29,30,24] or 3D-3D correspondences [6,5]. Then they solved perspective-n-point and SVD problem with 2D-3D and 3D-3D correspondences [14], respectively. Some methods [5,1] also used these correspondences to generate voting candidates, and then used RANSAC [8] algorithm for selecting the best candidate. However, the generation of canonical 3D keypoints is based on the known 3D object model that is not available when predicting the category-level pose.\n\n\nCategory-Level Pose Estimation\n\nCompared to instance-level, the major challenge of category-level pose estimation is the intra-class object variation, including shape and color variation. To handle the object variation problem, [41] proposed to map the different objects in the same category to a NOCS map. Then they used semantic segmentation to access the observed points cloud with known camera parameters. The 6D pose and size are calculated by the Umeyama algorithm [37] with the NOCS map and the observed points. Shape-Prior [35] adopted similar method with [41], but both extra shape prior knowledge and dense-fusion feature [40], instead of RGB feature, are used. CASS [4] estimated the 6D pose via the learning of a canonical shape space with dense-fusion feature [40]. Since the RGB feature is sensitive to color variation, the performance of their methods in category-level pose estimation is limited. In contrast, our method is shape feature-based which is robust for this task.\n\n\n3D Data Augmentation\n\nIn 3D object detection tasks [6,26,32,5], online data augmentation techniques such as translation, random flipping, shifting, scaling, and rotation are applied to original point clouds for training data augmentation. However, these  For RGB channels, we use a 2D detector to detect the object 2D location, category label 'C' (used as a one-hot feature for next tasks), and class probability map (cpm) (generate the 3D sphere center via maximum probability location and camera parameters). With this information and depth channel, the points in a compact 3D sphere are generated. Given the points in the 3D sphere, we first use the proposed 3D deformation mechanism for data augmentation. After that, we use a shape-based 3DGC autoencoder to perform observed points reconstruction (OPR), as well as point cloud segmentation, for orientation latent feature learning. Then we decode the rotation information into two perpendicular vectors from the latent feature. Finally, we use a residual estimation network to predict the translation and size residuals. 'cate-sizes' denotes the pre-calculated average sizes of different categories, 'k' is the rotation vector dimension, and the hollow '+' means feature concatenation.\n\noperations cannot change the shape property of the object. Simply adopting these operations on point clouds is not able to handle the shape variation problem in the 3D task. To address this, [7] proposed part-aware augmentation which operates on the semantic parts of the 3D object with five manipulations: dropout, swap, mix, sparing, and noise injection. However, how to decide the semantic parts are ambiguous. In contrast, we propose a box-cage based 3D data augmentation mechanism which can generate the various shape variants (shown in Figure 5) and avoid semantic parts decision procedure.\n\n\nProposed Method\n\nIn this section, we describe the detailed architecture of FS-Net shown in Figure 2. Firstly, we use the YOLOv3 to detect the object location with RGB input. Secondly, we use 3DGC autoencoder to perform 3D segmentation and observed points reconstruction, the latent feature can learn orientation information through the process. Then we propose a novel decoupled rotation mechanism for decoding orientation information. Thirdly, we use PointNet [27] to estimate the translation and object size. Finally, to increase the generalization ability of FS-Net, we propose the box-cage based 3D deformation mechanism. Figure 3. Stable shape and various color. Top row: three bowl instances randomly chosen from the NOCS-REAL dataset. Bottom row: three bowl instances randomly cropped from the internet image search results (using the keyword 'bowl'). The color is varied, while the shape is relatively stable.\n\n\nObject Detection\n\nFollowing [6], we train a YOLOv3 [31] to fast detect the object bounding box in RGB images, and output class (category) labels. Then we adopt the 3D sphere to locate the point cloud of the target object quickly. With these techniques, the 2D detection part provides a compact 3D learning space for the following tasks. Different from other category-level 6D object pose estimation methods that need semantic segmentation masks, we only need object bounding boxes. Since object detection is faster than semantic segmentation [31,9], the detection speed of our method is faster than previous methods.\n\n\nShape-Based Network\n\nThe output points of object detection contain both object and background points. To access the points that belong to the target object and calculate the rotation of the object, we need a network that performs two tasks: 3D segmentation and rotation estimation.\n\nAlthough there are many network architectures that directly process point cloud [27,28,46], most of the architectures calculate on point coordinates, which means their networks are sensitive to point clouds shift and size variation [19]. This decreases the pose estimation accuracy.\n\nTo tackle the point clouds shift, Frustum-PointNet [26] and G2L-Net [6] employed the estimated translation to align the segmented point clouds to local coordinate space. However, their methods cannot handle the intra-class size variation.\n\nTo solve the point clouds shift and size variation problem, in this paper, we propose a 3DGC autoencoder to extract the point cloud shape feature for segmentation and rotation estimation. 3DGC is designed for point cloud classification and object part segmentation; our work shows that 3DGC can also be used for category-level 6D pose estimation task.\n\n\n3D Graph Convolution\n\n3DGC kernel consists of m unit vectors. The m kernel vectors are applied to the n vectors generated by the center point with its n-nearest neighbors. Then, the convolution value is the sum of cosine similarity between kernel vectors and the n-nearest vectors. In a 2D convolution network, the trained network learned a weighted kernel, which has a higher response with a matched RGB value, while the 3DGC network learned the orientations of the m vectors in the kernel. The weighted 3DGC kernel has a higher response with a matched 3D pattern which is defined by the center point with its n-nearest neighbors. For more details, please refer to [19].\n\n\nRotation-Aware Autoencoder\n\nBased on the 3DGC, we design an autoencoder for the estimation of category-level object rotation. To extract the latent rotation feature, we train the autoencoder to reconstruct the observed points transformed from the observed depth map of the object. There are several advantages to this strategy: 1) the reconstruction of observed points is view-based and symmetry invariant [33,34], 2) the reconstruction of observed points is easier than that of a complete object model (shown in Table 2), and 3) more representative orientation feature can be learned (shown in Table 1).\n\nIn [33,34], the authors also reconstructed the input images to observed views. However, the input and output of their models are 2D images that are different to our 3D point cloud input and output. Furthermore, our network architecture is also different from theirs.\n\nWe utilize Chamfer Distance to train the autoencoder, the reconstruction loss function L rec is defined as\nL rec = xi\u2208Mc min xi\u2208Mc x i \u2212x i 2 2 + xi\u2208Mc min xi\u2208Mc x i \u2212x i 2 2 ,(1)\nwhere M c andM c denote the ground truth point cloud and reconstructed point cloud, respectively. x i andx i are the points in M c andM c . With the help of 3D segmentation mask, we only use the features extracted from the observed object points for reconstruction.\n\nAfter the network convergence, the encoder learned the rotation-aware latent feature. Since the 3DGC is scale and shift invariant, the observed points reconstruction enforces the autoencoder to learn the scale and shift invariant orientation feature under corresponding rotation. In the next subsection, we will describe how we decode rotation information from this latent feature.\n\n\nDecoupled Rotation Estimation\n\nGiven the latent feature which contains rotation information, our task is to decode the category-level rotation feature. To achieve this, we utilize two decoders to extract the rotation information in a decoupled fashion. The two decoders decode the rotation information into two perpendicular vectors under corresponding rotation. These two vectors can represent rotation information completely (shown in Figure 4).\n\nSince the two vectors are orthogonal, the decoded rotation information related to them is independent; we can use one of them to recover part rotation information of the object. For example, in Figure 8, we use the green vector axis to recover the pose. We can see that the green boxes and blue boxes are aligned well in the recovered axis.\n\nEach decoder only needs to extract the orientation information along corresponding vector which is easier than the estimation of the complete rotation. The loss function is based on cosine similarity that defined as\nL rot = v 1 , v 1 v 1 v 1 + \u03bb r v 2 , v 2 v 2 v 2 ,(2)\nwherev 1 andv 2 are the predicted vectors. v 1 and v 2 are the ground truth, and \u03bb r is the balance parameter. The balance parameter \u03bb r makes our network easy to handle circular symmetry object such as bottle, and for such circular symmetry object, the red vector is not necessary (shown in Figure 4). Without loss of generality, we assume that the green vector is along the symmetry axis; then, we set \u03bb r as zero to handle the circular symmetry objects. For other types of symmetric objects, we can employ the rotation mapping function used in [25,35] to map the relevant rotation matrices to a unique one.\n\nPlease note that our decoupled rotation is different to the rotation representation proposed in [45]. They took the first two columns from a rotation matrix as the new representation, which has no geometric meaning. In contrast, our representation is defined based on the shape of the target object, and our representation can avoid the discontinuity issue mentioned in [45,25]. \n\n\nResidual Prediction Network\n\nAs both translation and object size are related to points coordinates, inspired by [26,6], we train a tiny PointNet [27] that takes segmented point cloud as input. More concretely, the PointNet performs two related tasks: 1) estimating the residual between the translation ground truth and the mean value of the segmented point cloud; 2) estimating the residual between object size and the mean category size.\n\nFor size residual, we pre-calculate the mean size [x, y, z] T of each category by\n\uf8ee \uf8f0 x y z \uf8f9 \uf8fb = 1 N N i=1 [x i , y i , z i ] T ,(3)\nwhere N is the amount of the object in that category. Then for object o in that category the ground truth [\u03b4 o x , \u03b4 o y , \u03b4 o z ] T of the size residual estimation is calculated as\n[\u03b4 o x , \u03b4 o y , \u03b4 o z ] T = [x o , y o , z o ] T \u2212 [x, y, z] T .(4)\nWe use mean square error (MSE) loss to predict both the translation and size residual. The total loss function L res is defined as:\nL res = L tra + L size ,(5)\nwhere L tra and L size are sub-loss for translation residual and size residual, respectively. \n\n\n3D Deformation Mechanism\n\nOne major problem in category-level 6D pose estimation is the intra-class shape variation. The existing methods employed two large synthetic datasets, i.e. CAMERA [41] and 3D model dataset [3] to learn this variation. However, this strategy not only needs extra hardware resources to store these big synthetic datasets but also increases the (pre-)training time.\n\nTo alleviate the shape variation issue, based on the fact that the shapes of most objects in the same category are similar [38] (shown in Figure 3), we propose an online boxcage based 3D deformation mechanism for training data augmentation. We pre-define a box-cage for each rigid object (shown in Figure 5). Each point is assigned to its nearest surface of the cage; when we deform the surface, the corresponding points move as well.\n\nThough box-cage can be designed more refined, in experiments, we find that with a simple box cage, i.e. 3D bounding box of the object, the generalization ability of the proposed method is considerably improved (Table 1). Different to [43], we do not need the extra training process to obtain the box-cage of the object, and we do not need target shape to learn the deformation operation either. Our mechanism is totally online, which saves training time and storage space.\n\nTo make the deformation operation easier, we first transfer the points to the canonical coordinate system and then perform 3D deformation. Finally we transform them to global scene:\n\n{P 1 , P 2 , \u00b7 \u00b7 \u00b7 , P n } = R(F 3D (R T (P \u2212 T ))) + T, (6) where P is the points generated after the 2D detection step. R, T are the pose ground truth. {P 1 , P 2 , \u00b7 \u00b7 \u00b7 , P n } are the new generated training examples. F 3D is 3D deformation which includes cage enlarging, shrinking, changing the area of some surfaces. [41] is the first real-world dataset for category-level 6D object pose estimation. The training set has 4300 real images of 7 scenes with 6 categories. For each category, there are 3 unique instances. In the testing set, there are 2750 real images spread in 6 scenes of the same 6 categories as the training set. In each test scene, there are about 5 objects which makes the dataset clutter and challenging. LINEMOD [12] is a widely used instance-level 6D object pose estimation dataset which consists of 13 different objects with significant shape variation.\n\n\nExperiments\n\n\nDatasets\n\n\nNOCS-REAL\n\nWe use the automatic point-wise labeling techniques proposed in [5] to access the label of each point in both training sets.\n\n\nImplementation Details\n\nWe use Pytorch [23] to implement our pipeline. All experiments are deployed on a PC with i7-4930K 3.4GHz CPU and GTX 1080Ti GPU.\n\nFirst, to locate the object in RGB images, we fine-tune the YOLOv3 pre-trained on COCO dataset [18] with the training dataset. Then we jointly train the 3DGC autoencoder and residual estimation network. The total loss function is defined as L Shape = \u03bb seg L seg +\u03bb rec L rec +\u03bb rot L rot +\u03bb res L res , (7) where \u03bbs are the balance parameters. We empirically set them as 0.001, 1, 0.001, and 1 to keep different loss values at the same magnitude. We use cross entropy for 3D segmentation loss function L seg .\n\nWe adopt Adam [15] to optimize the FS-Net. The initial learning rate is 0.001, and we halve it every 10 epochs. The maximum epoch is 50.\n\n\nEvaluation Metrics\n\nFor category-level pose estimation, we adopt the same metrics used in [41,4,35]:\n\n\u2022 IoU X is Intersection-over-Union (IoU) accuracy for 3D object detection under different overlap thresholds. The overlap ratio larger than the threshold X is accepted.\n\n\u2022 n \u2022 m cm represents pose estimation error of rotation and translation. The rotation error less than n \u2022 and the translation error less than m cm is accepted.\n\nFor instance-level pose estimation, we compare the performance of FS-Net with other state-of-the-art instancelevel methods using the ADD-(S) metric [12]. Table 1. Ablation studies on NOCS-REAL dataset. We use two different metrics to measure performance. '3DGC' means the 3D graph convolution. 'OPR' means observed points reconstruction. 'DR' represents the decoupled rotation mechanism. 'DEF' denotes the online 3D deformation. In the last row, the values in the bracket are the performance for the reconstruction of the complete object model transformed by the corresponding pose. Please note, for the sake of ablation study, we provide the ground truth 2D bounding box for different methods. \n\n\nAblation Studies\n\nWe use the G2L-Net [6] as the baseline method which extracted the latent feature for rotation estimation via pointwise orientated vector regression, and the ground truth of rotation is the eight corners of 3D bounding box with corresponding rotation. The loss function for rotation estimation is the mean square error between predicted 3D coordinates and ground truth. Compared to baseline, our proposed work has three novelties: a) view-based 3DGC autoencoder for observed point cloud reconstruction; b) rotation decoupled mechanism; c) online 3D deformation mechanism.\n\nIn Table 1, we report the experimental results of three novelties on the NOCS-REAL dataset. Comparing Med3 and Med5, we find that reconstruction of the observed point cloud can learn better pose feature. The performance of Med2(Med1, G2L) and Med5(Med3, G2L+DR) shows that the proposed decoupled rotation mechanism can effectively extract the rotation information. The results of Med4 and Med5 demonstrate the effectiveness of the 3D deformation mechanism, which increases the pose accuracy by 7.7% in terms of 10 \u2022 10 cm metric. We also compare the different reconstruction choices: the reconstruction of observed points and the complete object model with corresponding rotation. From the last row of Table 1, we can see that the observed points reconstruction can learn better rotation feature. Overall, Table 1 shows that the proposed novelties can improve the accuracy significantly.\n\n\nGeneralization Performance\n\nNOCS-REAL dataset provides 4.3k real images that covers various poses of different objects in different categories for training. That means the category-level pose information is rich in the training set. Thanks to the effectively pose feature extraction, FS-Net achieves state-of-the-art performance even with part of the real-world training data. We randomly choose different percentages of the training set to train FS-Net and test it on the whole testing set. Figure   6 shows that: 1) FS-Net is robust to the size of the training dataset, and has good category-level feature extraction ability. Even with 20% of the training dataset, the FS-Net can still achieve state-of-the-art performance; 2) the 3D deformation mechanism significantly improves the robustness and performance of FS-Net. Figure 6. Generalization performance. With the given 2D bounding box and a randomly chosen 3D sphere center, we show how the training set size affects the pose estimation performance. 'w/o DEF' means no 3D deformation mechanism is adopted during training.\n\n\nEvaluation of Reconstruction\n\nPoint cloud reconstruction has a close relationship with pose estimation performance. We compute the Chamfer Distance of the reconstructed point cloud with the ground truth point cloud and compared it with other reconstruction types used by other methods. From Table 2, we can see that the average reconstruction error of our method is 0.86, which is 72.9% and 18.9% lower than that of Shape-Prior [35] and CASS [4], respectively. It shows that our method achieves better pose estimation results via a simpler reconstruction task, i.e. observed points reconstruction rather than complete object model reconstruction.\n\n\nComparison with State-of-the-Arts\n\n\nCategory-Level Pose Estimation\n\nWe compare FS-Net with NOCS [41], CASS [4], Shape-Prior [35], and 6D-PACK [39] on NOCS-REAL dataset in Table 4. We can see that our proposed method outperforms the other state-of-the-art methods on both accuracy and speed. Specifically, on 3D detection metric IOU 50 , our FS-Net outperforms the previous best method, NOCS, by 11.7% and the running speed is 4 times faster. In terms of 6D pose metric 5 \u2022 5cm and 10 \u2022 10 cm, FS-Net outperforms the CASS by the margins of 4.7% and 6.3%, respectively. FS-Net even outperforms 6D-PACK under 3D detection metric IOU 50 , which is a 6D tracker and needs an initial 6D pose and object size to start. See Figure 7 for more quantitative details. The qualitative results are shown in   \n\n\nInstance-Level Pose Estimation\n\nWe compare the instance-level pose estimation results of FS-Net on the LINEMOD dataset with other state-of-thearts instance-level methods. From Table 3, we can see that FS-Net achieves comparable results on both accuracy and speed. It shows that our method can effectively extract both category-level and instance-level pose features.\n\n\nRunning Time\n\nGiven a 640\u00d7 480 RGB-D image, our method runs at 20 FPS with Intel i7-4930K CPU and 1080Ti GPU, which is 2 times faster than the previous fastest method 6D-PACK [39]. Specifically, the 2D detection takes about 10ms to proceed. The pose and size estimation takes about 40ms.   \n\n\nConclusion\n\nIn this paper, we propose a fast category-level pose estimation method that runs at 20 FPS which is fast enough for real-time applications. The proposed method first extracts the latent feature by the observed points reconstruction with a shape-based 3DGC autoencoder. Then the category-level orientation feature is decoded by the effective decoupled rotation mechanism. Finally, for translation and object size estimation, we use the residual network to estimate them based on residuals estimation. In addition, to increase the generalization ability of FS-Net and save the hardware source, we design an online 3D deformation mechanism for training set augmentation. Extensive experimental results demonstrate that FS-Net is less data-dependent, and can achieve state-of-the-art performance on category-and instance-level pose estimation in both accuracy and speed. Please note, our 3D deformation mechanism and decoupled rotation scheme are model-free, which can be applied to other pose estimation methods to boost the performance.\n\nAlthough FS-Net achieves state-of-the-art performance, it relies on a robust 2D detector to detect the region of interest. In future work, we plan to adopt 3D object detection techniques to directly detect the objects from point clouds.\n\n\nAppendix\n\nThis section provides more details about our FS-Net. Section 6.1 describes the details of the 3D deformation mechanism and deformed examples. Section 6.2 provides more quantitative results of the FS-Net on NOCS-REAL [41] dataset and comparison with state-of-the-art method. Section 6.3 demonstrates that the proposed vectors-based rotation representation can be easily extended to handle other symmetric types.\n\n\n3D Deformation Mechanism\n\nAs stated in Section 3.5 of the paper, the 3D deformation mechanism is box-cage based and the deformations are applied in a canonical space. In the canonical coordinate system, every box edge is parallel to an axis (shown in Figure 9). This property makes the 3D deformation calculation easier. For example, when we need to elongate/shrink the mug along Y axis by n times. We enlarge the distance between surface S 1,2,3,4 and surface S 5,6,7,8 by n times. Since these two surfaces are parallel to the XZ-plane, the x and z coordinates are unchanged. Then points coordinates are changed from [x, y, z] to [x, ny, z]. The calculations are similar when we need to elongate/shrink the mug along X or Z axis by n times:\n[x, ny, z] = F x ([x, y, z]),(8)[nx, y, z] = F y ([x, y, z]),(9)[x, y, nz] = F z ([x, y, z]),(10)\nwhere F x,y,z is the elongate/shrink operation along corresponding axis. Further, if the object is the mug or bowl, we may need to change the top or bottom size to generate new shapes (shown in Figure 10). In this case, assuming we enlarge the bottom along X axis by n times, then from bottom to top, the coordinates are changed as:\nx new = (1 + (n \u2212 1) l L )x,(11)\nwhere l is the distance from a point to the top surface, i.e. S 1,2,3,4 in Figure 9. L is the height of the object. Please note, all the edges are keep straight while deformation.  Figure 9. 3D object model. We assume that the center of 3D bounding box is the origin point of the coordinate. The surface is represented by its four corners. For example, the top surface is represented by S1,2,3,4.\n\n\nExperimental Results\n\n\nDetailed Results\n\nWe report the specific category pose estimation results under different metrics in Table 5. We also provide the rotation recovered by one/two vectors in Figure 11. We can see that the bounding boxes are well aligned in the recovered vector direction. \n\n\nComparison with State-of-The-Art\n\nWe compare FS-Net with the state-of-the-art method Shape-Prior [35], which utilized point cloud for category-level 6D object pose estimation. Shape-Prior [35] estimated the object size and 6D pose from dense-fusion feature [40], while we estimate the pose from point cloud feature. Figure 12 shows that our FS-Net is robust to color and shape variation, and can handle some failure cases of Shape-Prior. For Shape-Prior, we use the predicted results provided on their website: https://github.com/mentian/ object-deformnet.\n\n\nOriginal\n\n\nEnlarge bottom Enlarge top\n\nShrink along Z Shrink along X Shrink along XZ   Figure 4 in the paper), respectively. For better illustration, we use ground truth object size to calculate the final 3D bounding box.\n\n\nRotation Representation for Symmetry Object\n\nThe vector based rotation representation proposed in the paper can only handle the symmetry objects like bottle, however, in real-world the symmetric types are various (see Figure 13). In this section, we will show how to extend the vector based rotation representation for different symmetric types. Our strategy is inspired by the rotation mapping operation proposed in [25]. In the following, we will show how to find the rotation group (termed proper symmetries in [25]) of a single rotation for common symmetric objects.\n\nOur basic idea is list all the ambiguous rotations of a Figure 12. Qualitative comparison with Shape-Prior. The white boxes are the ground truth. Blue boxes are our results. Red boxes are the poses predicted by Shape-Prior [35] single rotation and choose the rotations that has the closest distance with the identity matrix:\nR * = argmin R\u2208G(Ri) D(R, R I ),(12)\nwhere D(\u00b7, \u00b7) is the distance between two rotation matrix, G(R i ) is a group of rotation that can provide the same visual appearance of a given object as rotation R i . Our goal is to find a rotation R * that can minimize the rotation distance. For symmetric object like bottle, we can avoid the rotation ambiguity by only using the green vector to represent the rotation (see Figure 4), however, the case is non-trivial for other symmetric type. In the following, we describe how we find symmetry rotation group for different symmetric types\n\n\nSymmetry with two Axes\n\nFor this kind symmetric objects, in canonical space, when we rotate the object around one axis 180 \u2022 , we can get the same appearance (see Figure for illustration). Assume that axis is Z axis, for arbitrary rotation R, the appearance A:\nA R Z + 180 O = A O ,(13)\nwhere R Z + 180 means rotation the object around Z 180 \u2022 in clockwise, O denotes the object. That means we can find the rotation group of each rotation by right multiplication operation R Z + 180 . Then we use Equation 12 to find the representative rotation in the rotation group.\n\n\nSymmetry with N Axes\n\nThe idea can be easily extend to object with N symmetries around a single axis Z. For this kind of symmetric objects, when we rotate the object around axis Z by K 360 N \u2022 (K = 1, 2, \u00b7 \u00b7 \u00b7 , N ) in canonical space, the appearance A of the object is unchanged:\nA R Z + K 360 N \u2022 O = A O .(14)\nThen, the symmetric rotation group G(R) of rotation R is: 1,2,\u00b7\u00b7\u00b7 ,N ) . We find the representative rotation in G(R) with Equation 12.\nRR K 360 N \u2022 (K=0,\n\nGeneral Case\n\nMost symmetric types are included in the description of Section 6.3.1 and 6.3.2. For any other symmetric object, the key idea here is to find the rotation operation that can produce the same appearance of the object. Then use Equation 12 to find the representative rotation.\n\n\nDecoupled Rotation Representation\n\nGiven the representative rotation R * of ambiguous rotation, we generate its corresponding vector-based representation V by:\nV = R * [v 1 , v 2 ],(15)\nwhere v 1 is the vector along with the axis Z mentioned in Section 6.3.1 and 6.3.2, v 2 is the vectors orthogonal with v 1 .\n\nFigure 1 .\n1Semantic illustration of FS-Net.\n\nFigure 2 .\n2Architecture of FS-Net. The input of FS-Net is an RGB-D image.\n\nFigure 4 .\n4Rotation represented by vectors. Left: The object rotation can be represented by two perpendicular vectors (green vector and red vector); Right: For circular symmetry object like the bottle, only the green vector matters.\n\nFigure 5 .\n53D deformed examples. The new training examples can be generated by enlarging, shrinking, or changing the area of some surfaces of the box-cages. The left one is the original point could with original 3D box-cage, i.e. 3D bounding box. The right three ones are the deformed point clouds with deformed boxcages (shown in yellow color). The green boxes are the original 3D bounding boxes before deformation.\n\nFigure 8 .\n8Please note, we only use real-world data (NOCS-REAL) to train our pose estimation part. Other methods use both synthetic dataset (CAMERA)[41] and real-world data for training. The number of training examples in CAM-ERA is 275K, which is more than 60 times that of NOCS-REAL (4.3K). It shows that FS-Net can efficiently extract the category-level pose feature with fewer data.\n\nFigure 7 .\n7Result on NOCS-REAL. The average precision of different thresholds tested on NOCS-REAL dataset with 3D IoU, rotation, and translation error.\n\nFigure 8 .\n8Qualitative results on NOCS-REAL dataset. The first row is the pose and size estimation results. White 3D bounding boxes denote ground truth. Blue boxes are the poses recovered from two estimated rotation vectors. The green boxes are the poses recovered from one estimated rotation vector. Our results match ground truth well in both pose and size. The second row is the reconstructed observed points under corresponding poses, although the reconstructed points are not perfectly in line with the target points, the basic orientation information is kept. The third row is the ground truth of the observed points transformed from the observed depth map.\n\nFigure 10 .\n10Examples of different deformations. We assume that the XY Z axis are the same asFigure 9. The upper right corner is the original point cloud with corresponding box-cage. The rest are the deformed box-cages and point clouds. The deformation operations are described on the top or bottom of the pictures.\n\nFigure 11 .\n11Rotation recovered by different vectors. The white boxes are the ground truth. Blue boxes are the rotation recovered by two estimated vectors. The green and red boxes are the rotation recovered by estimated green vector and estimated red vector (see\n\nTable 2 .\n2Reconstruction type comparison. The comparison is on the NOCS-REAL dataset with the Chamfer Distance metric (\u00d710 \u22123 ). 'Complete' means the reconstruction of the complete 3D model. 'Observed' denotes only the reconstruction of the observed points.Methods CASS [4] Shape-Prior [35] \nOurs \nComplete \nComplete \nObserved \nBottle \n0.75 \n3.44 \n1.2 \nBowl \n0.38 \n1.21 \n0.39 \nCamera \n0.77 \n8.89 \n0.44 \nCan \n0.42 \n1.56 \n0.62 \nLaptop \n3.73 \n2.91 \n2.23 \nMug \n0.32 \n1.02 \n0.29 \nAverage \n1.06 \n3.17 \n0.86 \n\n\n\nTable 3 .\n3Instance-level comparison on LINEMOD dataset. Our method achieves a comparable performance with the state-of-theart in both speed and accuracy.Method \nInput \nADD-(S) Speed(FPS) \nPVNet [24] \nRGB \n86.3% \n25 \nCDPN [17] \nRGB \n89.9% \n33 \nDPOD [44] \nRGB \n95.2% \n33 \nG2L-Net [6] \nRGBD \n98.7% \n23 \nDensefusion[40] RGBD \n94.3% \n16 \nPVN3D [10] \nRGBD \n99.4% \n5 \nOurs \nRGBD \n97.6% \n20 \n\n\n\nTable 4 .\n4Category-level performance on NOCS-REAL dataset with different metrics. We summarize the pose estimation results reported in the origin papers on the NOCS-REAL dataset. '-' means no results are reported under this metric. The values in the bracket are the performance for synthetic NOCS dataset.Method \nIoU 25 IoU 50 \nIoU 75 \n5 \u2022 5cm \n10 \u2022 5 cm 10 \u2022 10 cm Speed(FPS) \nNOCS [41] \n84.9% 80.5% \n30.1%(69.5%) \n9.5 %(40.9%) \n26.7% \n26.7% \n5 \nCASS [4] \n84.2% 77.7% \n-\n23.5 % \n58.0% \n58.3% \n-\nShape-Prior [35] 83.4% 77.3% \n53.2%(83.1%) \n21.4%(59.0%) \n54.1% \n-\n4 \n6-PACK [39] \n94.2% \n-\n-\n33.3 % \n-\n-\n10 \nOurs \n95.1% 92.2% 63.5%(85.17%) 28.2 %(62.01%) \n60.8% \n64.6% \n20 \n\n\n\nTable 5 .\n5Category-Level results. Object-wise experiments with different metrics.Category IoU 75 5 \u2022 5 cm 10 \u2022 5 cm 10 \u2022 10 cm \nBottle \n0.4710 0.4219 \n0.8134 \n0.8755 \nBowl \n0.9810 0.5916 \n0.9793 \n0.9793 \nCamera \n0.5882 0.0176 \n0.1457 \n0.1480 \nCan \n0.6334 0.4055 \n0.7820 \n0.8141 \nLaptop \n0.3805 0.1659 \n0.5570 \n0.6859 \nMug \n0.7534 0.0874 \n0.3698 \n0.3706 \nAverage \n0.6345 0.2816 \n0.6078 \n0.6455 \n\n\n\nUncertainty-driven 6d pose estimation of objects and scenes from a single rgb image. Eric Brachmann, Frank Michel, Alexander Krull, Michael Ying Yang, Stefan Gumhold, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Eric Brachmann, Frank Michel, Alexander Krull, Michael Ying Yang, Stefan Gumhold, et al. Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb im- age. In The IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 3364-3372, 2016. 2\n\nVirtual reality technology. C Grigore, Philippe Burdea, Coiffet, John Wiley & SonsGrigore C Burdea and Philippe Coiffet. Virtual reality tech- nology. John Wiley & Sons, 2003. 1\n\nX Angel, Thomas Chang, Leonidas Funkhouser, Pat Guibas, Qixing Hanrahan, Zimo Huang, Silvio Li, Manolis Savarese, Shuran Savva, Hao Song, Su, arXiv:1512.03012An information-rich 3d model repository. arXiv preprintAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 5\n\nLearning canonical shape space for category-level 6d object pose and size estimation. Dengsheng Chen, Jun Li, Zheng Wang, Kai Xu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition7Dengsheng Chen, Jun Li, Zheng Wang, and Kai Xu. Learn- ing canonical shape space for category-level 6d object pose and size estimation. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 11973-11982, 2020. 1, 2, 6, 7, 8\n\nPonitposenet: Point pose network for robust 6d object pose estimation. Wei Chen, Jinming Duan, Hector Basevi, The IEEE Winter Conference on Applications of Computer Vision (WACV). 26Hyung Jin Chang, and Ales LeonardisWei Chen, Jinming Duan, Hector Basevi, Hyung Jin Chang, and Ales Leonardis. Ponitposenet: Point pose network for robust 6d object pose estimation. In The IEEE Winter Con- ference on Applications of Computer Vision (WACV), March 2020. 2, 6\n\nG2l-net: Global to local network for realtime 6d pose estimation with embedding vector features. Wei Chen, Xi Jia, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 67Hyung Jin Chang, Jinming Duan, and Ales LeonardisWei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, and Ales Leonardis. G2l-net: Global to local network for real- time 6d pose estimation with embedding vector features. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. 2, 3, 4, 5, 6, 7\n\nDifferent symmetry types. 30 industry-relevant objects in T-LESS dataset. Figure 13. 13Object 1, 2, 3, 4 are circular symmetry, object 7, 8, 9, 10 have two symmetry axes, while object 27, 28 have four symmetry axesFigure 13. Different symmetry types. 30 industry-relevant objects in T-LESS dataset [13]. Object 1, 2, 3, 4 are circular symmetry, object 7, 8, 9, 10 have two symmetry axes, while object 27, 28 have four symmetry axes.\n\nPart-aware data augmentation for 3d object detection in point cloud. Jaeseok Choi, Yeji Song, Nojun Kwak, arXiv:2007.13373arXiv preprintJaeseok Choi, Yeji Song, and Nojun Kwak. Part-aware data augmentation for 3d object detection in point cloud. arXiv preprint arXiv:2007.13373, 2020. 3\n\nRandom sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. A Martin, Fischler, C Robert, Bolles, Communications of the ACM. 246Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6):381-395, 1981. 2\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, The IEEE International Conference on Computer Vision (ICCV). 14Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In The IEEE International Conference on Computer Vision (ICCV), pages 2961-2969, 2017. 1, 4\n\nPvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation. Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, Jian Sun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition17Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, and Jian Sun. Pvn3d: A deep point-wise 3d keypoints voting network for 6dof pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11632-11641, 2020. 1, 7\n\nGradient response maps for real-time detection of textureless objects. Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir Navab, Pascal Fua, Vincent Lepetit, IEEE Transactions on Pattern Analysis and Machine Intelligence. 345Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir Navab, Pascal Fua, and Vincent Lepetit. Gra- dient response maps for real-time detection of textureless ob- jects. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(5):876-888, 2012. 2\n\nModel based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. Stefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Stefan Holzer, Gary Bradski, Kurt Konolige, Nassir Navab, Asian conference on computer vision. SpringerStefan Hinterstoisser, Vincent Lepetit, Slobodan Ilic, Ste- fan Holzer, Gary Bradski, Kurt Konolige, and Nassir Navab. Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes. In Asian conference on computer vision, pages 548-562. Springer, 2012. 6\n\nT-less: An rgbd dataset for 6d pose estimation of texture-less objects. Tom\u00e1\u0161 Hodan, Pavel Haluza, \u0160tep\u00e1n Obdr\u017e\u00e1lek, Jiri Matas, 2017 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE12Manolis Lourakis, and Xenophon ZabulisTom\u00e1\u0161 Hodan, Pavel Haluza,\u0160tep\u00e1n Obdr\u017e\u00e1lek, Jiri Matas, Manolis Lourakis, and Xenophon Zabulis. T-less: An rgb- d dataset for 6d pose estimation of texture-less objects. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 880-888. IEEE, 2017. 12\n\nA solution for the best rotation to relate two sets of vectors. Wolfgang Kabsch, Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography. 325Wolfgang Kabsch. A solution for the best rotation to re- late two sets of vectors. Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crys- tallography, 32(5):922-923, 1976. 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6\n\nA unified framework for multi-view multi-class object pose estimation. Chi Li, Jin Bai, Gregory D Hager, The European Conference on Computer Vision (ECCV). Chi Li, Jin Bai, and Gregory D. Hager. A unified framework for multi-view multi-class object pose estimation. In The Eu- ropean Conference on Computer Vision (ECCV), September 2018. 1\n\nCdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation. Zhigang Li, Gu Wang, Xiangyang Ji, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZhigang Li, Gu Wang, and Xiangyang Ji. Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 7678-7687, 2019. 7\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755.\n\n. Springer, Springer, 2014. 6\n\nConvolution in the cloud: Learning deformable kernels in 3d graph convolution networks for point cloud analysis. Zhi-Hao Lin, Sheng-Yu Huang, Yu-Chiang Frank Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition24Zhi-Hao Lin, Sheng-Yu Huang, and Yu-Chiang Frank Wang. Convolution in the cloud: Learning deformable kernels in 3d graph convolution networks for point cloud analysis. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800-1809, 2020. 2, 4\n\nPose estimation for augmented reality: a hands-on survey. Eric Marchand, Hideaki Uchiyama, Fabien Spindler, IEEE transactions on visualization and computer graphics. 2212Eric Marchand, Hideaki Uchiyama, and Fabien Spindler. Pose estimation for augmented reality: a hands-on survey. IEEE transactions on visualization and computer graphics, 22(12):2633-2651, 2016. 1\n\nProject tango. Eitan Marder-Eppstein, ACM SIGGRAPH 2016 Real-Time Live!. ACM40Eitan Marder-Eppstein. Project tango. In ACM SIGGRAPH 2016 Real-Time Live!, page 40. ACM, 2016. 1\n\nMaking deep heatmaps robust to partial occlusions for 3d object pose estimation. Markus Oberweger, Mahdi Rad, Vincent Lepetit, The European Conference on Computer Vision (ECCV). 1Markus Oberweger, Mahdi Rad, and Vincent Lepetit. Mak- ing deep heatmaps robust to partial occlusions for 3d object pose estimation. In The European Conference on Computer Vision (ECCV), pages 119-134, 2018. 1, 2\n\n. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Pytorch, Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan. Pytorch, 2017. 6\n\nPvnet: Pixel-wise voting network for 6dof pose estimation. Sida Peng, Yuan Liu, Qixing Huang, Hujun Bao, Xiaowei Zhou, arXiv:1812.1178827arXiv preprintSida Peng, Yuan Liu, Qixing Huang, Hujun Bao, and Xi- aowei Zhou. Pvnet: Pixel-wise voting network for 6dof pose estimation. arXiv preprint arXiv:1812.11788, 2018. 2, 7\n\nOn object symmetries and 6d pose estimation from images. Giorgia Pitteri, Micha\u00ebl Ramamonjisoa, Slobodan Ilic, Vincent Lepetit, 2019 International Conference on 3D Vision (3DV). IEEE510Giorgia Pitteri, Micha\u00ebl Ramamonjisoa, Slobodan Ilic, and Vincent Lepetit. On object symmetries and 6d pose estima- tion from images. In 2019 International Conference on 3D Vision (3DV), pages 614-622. IEEE, 2019. 5, 10\n\nFrustum pointnets for 3d object detection from rgb-d data. Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, Leonidas J Guibas, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 5Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J. Guibas. Frustum pointnets for 3d object detec- tion from rgb-d data. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 2, 4, 5\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. Charles R Qi, Hao Su, Kaichun Mo, Leonidas J Guibas, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (CVPR). Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (CVPR), July 2017. 2, 3, 4, 5\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. Li Charles Ruizhongtai Qi, Hao Yi, Leonidas J Su, Guibas, Advances in Neural Information Processing Systems. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in Neural Informa- tion Processing Systems, pages 5099-5108, 2017. 4\n\nBb8: a scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. Mahdi Rad, Vincent Lepetit, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 1Mahdi Rad and Vincent Lepetit. Bb8: a scalable, accu- rate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. In The IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), pages 3828-3836, 2017. 1, 2\n\nFeature mapping for learning fast and accurate 3d pose inference from synthetic images. Mahdi Rad, Markus Oberweger, Vincent Lepetit, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Mahdi Rad, Markus Oberweger, and Vincent Lepetit. Fea- ture mapping for learning fast and accurate 3d pose inference from synthetic images. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 4663- 4672, 2018. 2\n\nYolov3: An incremental improvement. Joseph Redmon, Ali Farhadi, arXiv:1804.0276724arXiv preprintJoseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018. 2, 3, 4\n\nPv-rcnn: Pointvoxel feature set abstraction for 3d object detection. Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionShaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point- voxel feature set abstraction for 3d object detection. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10529-10538, 2020. 2\n\nMulti-path learning for object pose estimation across domains. Martin Sundermeyer, Maximilian Durner, Yen En, Zoltan-Csaba Puang, Narunas Marton, Kai O Vaskevicius, Rudolph Arras, Triebel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMartin Sundermeyer, Maximilian Durner, En Yen Puang, Zoltan-Csaba Marton, Narunas Vaskevicius, Kai O Arras, and Rudolph Triebel. Multi-path learning for object pose estimation across domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13916-13925, 2020. 4\n\nImplicit 3d orientation learning for 6d object detection from rgb images. Martin Sundermeyer, Maximilian Zoltan-Csaba Marton, Manuel Durner, Rudolph Brucker, Triebel, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d orientation learning for 6d object detection from rgb images. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 699-715, 2018. 4\n\nShape prior deformation for categorical 6d object pose and size estimation. Meng Tian, Marcelo H AngJr, Gim Hee Lee, arXiv:2007.08454911arXiv preprintMeng Tian, Marcelo H Ang Jr, and Gim Hee Lee. Shape prior deformation for categorical 6d object pose and size es- timation. arXiv preprint arXiv:2007.08454, 2020. 1, 2, 5, 6, 7, 8, 9, 11\n\nDeep object pose estimation for semantic robotic grasping of household objects. Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, Stan Birchfield, arXiv:1809.10790arXiv preprintJonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, and Stan Birchfield. Deep object pose estimation for semantic robotic grasping of household ob- jects. arXiv preprint arXiv:1809.10790, 2018. 1\n\nLeast-squares estimation of transformation parameters between two point patterns. Shinji Umeyama, IEEE Transactions on Pattern Analysis & Machine Intelligence. 14Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence, (4):376-380, 1991. 1, 2\n\nHow we categorize objects is related to how we remember them: the shape bias as a memory bias. A Haley, Vlach, Journal of experimental child psychology. 1525Haley A Vlach. How we categorize objects is related to how we remember them: the shape bias as a memory bias. Jour- nal of experimental child psychology, 152:12-30, 2016. 2, 5\n\n6-pack: Category-level 6d pose tracker with anchor-based keypoints. Chen Wang, Roberto Mart\u00edn-Mart\u00edn, Danfei Xu, Jun Lv, Cewu Lu, Li Fei-Fei, Silvio Savarese, Yuke Zhu, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE7Chen Wang, Roberto Mart\u00edn-Mart\u00edn, Danfei Xu, Jun Lv, Cewu Lu, Li Fei-Fei, Silvio Savarese, and Yuke Zhu. 6-pack: Category-level 6d pose tracker with anchor-based keypoints. In 2020 IEEE International Conference on Robotics and Au- tomation (ICRA), pages 10059-10066. IEEE, 2020. 7, 8\n\nDensefusion: 6d object pose estimation by iterative dense fusion. Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martin-Martin, Cewu Lu, Li Fei-Fei, Silvio Savarese, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)79Chen Wang, Danfei Xu, Yuke Zhu, Roberto Martin-Martin, Cewu Lu, Li Fei-Fei, and Silvio Savarese. Densefusion: 6d object pose estimation by iterative dense fusion. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 1, 2, 7, 9\n\nNormalized object coordinate space for category-level 6d object pose and size estimation. He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, Leonidas J Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition89He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object co- ordinate space for category-level 6d object pose and size esti- mation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2642-2651, 2019. 1, 2, 5, 6, 7, 8, 9\n\nPosecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, Dieter Fox, arXiv:1711.00199arXiv preprintYu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. arXiv preprint arXiv:1711.00199, 2017. 1\n\nNeural cages for detail-preserving 3d deformations. Wang Yifan, Noam Aigerman, G Vladimir, Siddhartha Kim, Olga Chaudhuri, Sorkine-Hornung, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionWang Yifan, Noam Aigerman, Vladimir G Kim, Siddhartha Chaudhuri, and Olga Sorkine-Hornung. Neural cages for detail-preserving 3d deformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 75-83, 2020. 5\n\nDpod: 6d pose object detector and refiner. Sergey Zakharov, Ivan Shugurov, Slobodan Ilic, The IEEE International Conference on Computer Vision (ICCV). Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic. Dpod: 6d pose object detector and refiner. In The IEEE Interna- tional Conference on Computer Vision (ICCV), pages 1941- 1950, 2019. 7\n\nOn the continuity of rotation representations in neural networks. Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, Hao Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neu- ral networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5745- 5753, 2019. 5\n\nVoxelnet: End-to-end learning for point cloud based 3d object detection. Yin Zhou, Oncel Tuzel, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learn- ing for point cloud based 3d object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4490-4499, 2018. 4\n\nSingle image 3d object detection and pose estimation for grasping. Menglong Zhu, G Konstantinos, Yinfei Derpanis, Samarth Yang, Mabel Brahmbhatt, Cody Zhang, Matthieu Phillips, Kostas Lecce, Daniilidis, IEEE International Conference on. IEEERobotics and Automation (ICRA)Menglong Zhu, Konstantinos G Derpanis, Yinfei Yang, Samarth Brahmbhatt, Mabel Zhang, Cody Phillips, Matthieu Lecce, and Kostas Daniilidis. Single image 3d object detec- tion and pose estimation for grasping. In Robotics and Au- tomation (ICRA), 2014 IEEE International Conference on, pages 3936-3943. IEEE, 2014. 1\n", "annotations": {"author": "[{\"end\":151,\"start\":115},{\"end\":186,\"start\":152},{\"end\":230,\"start\":187},{\"end\":271,\"start\":231},{\"end\":306,\"start\":272},{\"end\":349,\"start\":307}]", "publisher": null, "author_last_name": "[{\"end\":123,\"start\":119},{\"end\":158,\"start\":155},{\"end\":202,\"start\":197},{\"end\":243,\"start\":239},{\"end\":283,\"start\":279},{\"end\":321,\"start\":312}]", "author_first_name": "[{\"end\":118,\"start\":115},{\"end\":154,\"start\":152},{\"end\":192,\"start\":187},{\"end\":196,\"start\":193},{\"end\":238,\"start\":231},{\"end\":278,\"start\":272},{\"end\":311,\"start\":307}]", "author_affiliation": "[{\"end\":150,\"start\":125},{\"end\":185,\"start\":160},{\"end\":229,\"start\":204},{\"end\":270,\"start\":245},{\"end\":305,\"start\":285},{\"end\":348,\"start\":323}]", "title": "[{\"end\":112,\"start\":1},{\"end\":461,\"start\":350}]", "venue": null, "abstract": "[{\"end\":2036,\"start\":463}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2162,\"start\":2158},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2165,\"start\":2162},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2186,\"start\":2183},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2214,\"start\":2210},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2217,\"start\":2214},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2220,\"start\":2219},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2716,\"start\":2712},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2719,\"start\":2716},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2722,\"start\":2719},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2725,\"start\":2722},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2727,\"start\":2725},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3042,\"start\":3038},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3076,\"start\":3073},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3266,\"start\":3262},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3283,\"start\":3280},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3402,\"start\":3398},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3420,\"start\":3416},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3576,\"start\":3572},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3802,\"start\":3798},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4626,\"start\":4622},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5088,\"start\":5084},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5162,\"start\":5159},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5667,\"start\":5663},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6992,\"start\":6988},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6995,\"start\":6992},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6998,\"start\":6995},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7332,\"start\":7328},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7335,\"start\":7332},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7338,\"start\":7335},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7367,\"start\":7364},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7369,\"start\":7367},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7465,\"start\":7461},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7497,\"start\":7494},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7499,\"start\":7497},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7587,\"start\":7584},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8017,\"start\":8013},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8260,\"start\":8256},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8320,\"start\":8316},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8353,\"start\":8349},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8421,\"start\":8417},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8465,\"start\":8462},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8562,\"start\":8558},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8832,\"start\":8829},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8835,\"start\":8832},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8838,\"start\":8835},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8840,\"start\":8838},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10214,\"start\":10211},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11084,\"start\":11080},{\"end\":11253,\"start\":11245},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11570,\"start\":11567},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11594,\"start\":11590},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12085,\"start\":12081},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12087,\"start\":12085},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12525,\"start\":12521},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12528,\"start\":12525},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12531,\"start\":12528},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12677,\"start\":12673},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12780,\"start\":12776},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12796,\"start\":12793},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13989,\"start\":13985},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14403,\"start\":14399},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14406,\"start\":14403},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14606,\"start\":14602},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14609,\"start\":14606},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17311,\"start\":17307},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17314,\"start\":17311},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17471,\"start\":17467},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17745,\"start\":17741},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17748,\"start\":17745},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17869,\"start\":17865},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17871,\"start\":17869},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17902,\"start\":17898},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19028,\"start\":19024},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19053,\"start\":19050},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19352,\"start\":19348},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19899,\"start\":19895},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20378,\"start\":20375},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20645,\"start\":20641},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21061,\"start\":21057},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21306,\"start\":21303},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21409,\"start\":21405},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21619,\"start\":21615},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22050,\"start\":22046},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22265,\"start\":22261},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22267,\"start\":22265},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22270,\"start\":22267},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22756,\"start\":22752},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23342,\"start\":23339},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26295,\"start\":26291},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26308,\"start\":26305},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26612,\"start\":26608},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26622,\"start\":26619},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26640,\"start\":26636},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26658,\"start\":26654},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":27858,\"start\":27854},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":29489,\"start\":29485},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31683,\"start\":31679},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31774,\"start\":31770},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31843,\"start\":31839},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32786,\"start\":32782},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32883,\"start\":32879},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33164,\"start\":33160},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36415,\"start\":36411}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35530,\"start\":35485},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35606,\"start\":35531},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35841,\"start\":35607},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36260,\"start\":35842},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36649,\"start\":36261},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36803,\"start\":36650},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37469,\"start\":36804},{\"attributes\":{\"id\":\"fig_8\"},\"end\":37787,\"start\":37470},{\"attributes\":{\"id\":\"fig_9\"},\"end\":38052,\"start\":37788},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38558,\"start\":38053},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38946,\"start\":38559},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":39622,\"start\":38947},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":40020,\"start\":39623}]", "paragraph": "[{\"end\":2566,\"start\":2052},{\"end\":3577,\"start\":2568},{\"end\":4357,\"start\":3579},{\"end\":4888,\"start\":4359},{\"end\":5914,\"start\":4890},{\"end\":6119,\"start\":5916},{\"end\":6419,\"start\":6121},{\"end\":6636,\"start\":6421},{\"end\":7782,\"start\":6687},{\"end\":8775,\"start\":7817},{\"end\":10018,\"start\":8800},{\"end\":10616,\"start\":10020},{\"end\":11536,\"start\":10636},{\"end\":12155,\"start\":11557},{\"end\":12439,\"start\":12179},{\"end\":12723,\"start\":12441},{\"end\":12963,\"start\":12725},{\"end\":13316,\"start\":12965},{\"end\":13990,\"start\":13341},{\"end\":14597,\"start\":14021},{\"end\":14865,\"start\":14599},{\"end\":14973,\"start\":14867},{\"end\":15312,\"start\":15047},{\"end\":15695,\"start\":15314},{\"end\":16145,\"start\":15729},{\"end\":16487,\"start\":16147},{\"end\":16704,\"start\":16489},{\"end\":17369,\"start\":16760},{\"end\":17750,\"start\":17371},{\"end\":18191,\"start\":17782},{\"end\":18274,\"start\":18193},{\"end\":18508,\"start\":18327},{\"end\":18709,\"start\":18578},{\"end\":18832,\"start\":18738},{\"end\":19223,\"start\":18861},{\"end\":19659,\"start\":19225},{\"end\":20133,\"start\":19661},{\"end\":20316,\"start\":20135},{\"end\":21200,\"start\":20318},{\"end\":21363,\"start\":21239},{\"end\":21518,\"start\":21390},{\"end\":22030,\"start\":21520},{\"end\":22168,\"start\":22032},{\"end\":22271,\"start\":22191},{\"end\":22441,\"start\":22273},{\"end\":22602,\"start\":22443},{\"end\":23299,\"start\":22604},{\"end\":23890,\"start\":23320},{\"end\":24779,\"start\":23892},{\"end\":25860,\"start\":24810},{\"end\":26509,\"start\":25893},{\"end\":27307,\"start\":26580},{\"end\":27676,\"start\":27342},{\"end\":27969,\"start\":27693},{\"end\":29018,\"start\":27984},{\"end\":29256,\"start\":29020},{\"end\":29679,\"start\":29269},{\"end\":30423,\"start\":29708},{\"end\":30854,\"start\":30522},{\"end\":31284,\"start\":30888},{\"end\":31579,\"start\":31328},{\"end\":32138,\"start\":31616},{\"end\":32362,\"start\":32180},{\"end\":32935,\"start\":32410},{\"end\":33261,\"start\":32937},{\"end\":33842,\"start\":33299},{\"end\":34105,\"start\":33869},{\"end\":34412,\"start\":34132},{\"end\":34695,\"start\":34437},{\"end\":34862,\"start\":34728},{\"end\":35171,\"start\":34897},{\"end\":35333,\"start\":35209},{\"end\":35484,\"start\":35360}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15046,\"start\":14974},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16759,\"start\":16705},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18326,\"start\":18275},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18577,\"start\":18509},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18737,\"start\":18710},{\"attributes\":{\"id\":\"formula_5\"},\"end\":30456,\"start\":30424},{\"attributes\":{\"id\":\"formula_6\"},\"end\":30488,\"start\":30456},{\"attributes\":{\"id\":\"formula_7\"},\"end\":30521,\"start\":30488},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30887,\"start\":30855},{\"attributes\":{\"id\":\"formula_9\"},\"end\":33298,\"start\":33262},{\"attributes\":{\"id\":\"formula_10\"},\"end\":34131,\"start\":34106},{\"attributes\":{\"id\":\"formula_11\"},\"end\":34727,\"start\":34696},{\"attributes\":{\"id\":\"formula_12\"},\"end\":34881,\"start\":34863},{\"attributes\":{\"id\":\"formula_13\"},\"end\":35359,\"start\":35334}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14513,\"start\":14506},{\"end\":14595,\"start\":14588},{\"end\":19880,\"start\":19871},{\"end\":22765,\"start\":22758},{\"end\":23902,\"start\":23895},{\"end\":24601,\"start\":24594},{\"end\":24705,\"start\":24698},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26161,\"start\":26154},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26690,\"start\":26683},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27493,\"start\":27486},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31418,\"start\":31411}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2050,\"start\":2038},{\"attributes\":{\"n\":\"2.\"},\"end\":6652,\"start\":6639},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6685,\"start\":6655},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7815,\"start\":7785},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8798,\"start\":8778},{\"attributes\":{\"n\":\"3.\"},\"end\":10634,\"start\":10619},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11555,\"start\":11539},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12177,\"start\":12158},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":13339,\"start\":13319},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":14019,\"start\":13993},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15727,\"start\":15698},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17780,\"start\":17753},{\"attributes\":{\"n\":\"3.5.\"},\"end\":18859,\"start\":18835},{\"attributes\":{\"n\":\"4.\"},\"end\":21214,\"start\":21203},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21225,\"start\":21217},{\"end\":21237,\"start\":21228},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21388,\"start\":21366},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22189,\"start\":22171},{\"attributes\":{\"n\":\"4.4.\"},\"end\":23318,\"start\":23302},{\"attributes\":{\"n\":\"4.5.\"},\"end\":24808,\"start\":24782},{\"attributes\":{\"n\":\"4.6.\"},\"end\":25891,\"start\":25863},{\"attributes\":{\"n\":\"4.7.\"},\"end\":26545,\"start\":26512},{\"attributes\":{\"n\":\"4.7.1\"},\"end\":26578,\"start\":26548},{\"attributes\":{\"n\":\"4.7.2\"},\"end\":27340,\"start\":27310},{\"attributes\":{\"n\":\"4.8.\"},\"end\":27691,\"start\":27679},{\"attributes\":{\"n\":\"5.\"},\"end\":27982,\"start\":27972},{\"attributes\":{\"n\":\"6.\"},\"end\":29267,\"start\":29259},{\"attributes\":{\"n\":\"6.1.\"},\"end\":29706,\"start\":29682},{\"attributes\":{\"n\":\"6.2.\"},\"end\":31307,\"start\":31287},{\"attributes\":{\"n\":\"6.2.1\"},\"end\":31326,\"start\":31310},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":31614,\"start\":31582},{\"end\":32149,\"start\":32141},{\"end\":32178,\"start\":32152},{\"attributes\":{\"n\":\"6.3.\"},\"end\":32408,\"start\":32365},{\"attributes\":{\"n\":\"6.3.1\"},\"end\":33867,\"start\":33845},{\"attributes\":{\"n\":\"6.3.2\"},\"end\":34435,\"start\":34415},{\"attributes\":{\"n\":\"6.3.3\"},\"end\":34895,\"start\":34883},{\"attributes\":{\"n\":\"6.3.4\"},\"end\":35207,\"start\":35174},{\"end\":35496,\"start\":35486},{\"end\":35542,\"start\":35532},{\"end\":35618,\"start\":35608},{\"end\":35853,\"start\":35843},{\"end\":36272,\"start\":36262},{\"end\":36661,\"start\":36651},{\"end\":36815,\"start\":36805},{\"end\":37482,\"start\":37471},{\"end\":37800,\"start\":37789},{\"end\":38063,\"start\":38054},{\"end\":38569,\"start\":38560},{\"end\":38957,\"start\":38948},{\"end\":39633,\"start\":39624}]", "table": "[{\"end\":38558,\"start\":38312},{\"end\":38946,\"start\":38714},{\"end\":39622,\"start\":39254},{\"end\":40020,\"start\":39706}]", "figure_caption": "[{\"end\":35530,\"start\":35498},{\"end\":35606,\"start\":35544},{\"end\":35841,\"start\":35620},{\"end\":36260,\"start\":35855},{\"end\":36649,\"start\":36274},{\"end\":36803,\"start\":36663},{\"end\":37469,\"start\":36817},{\"end\":37787,\"start\":37485},{\"end\":38052,\"start\":37803},{\"end\":38312,\"start\":38065},{\"end\":38714,\"start\":38571},{\"end\":39254,\"start\":38959},{\"end\":39706,\"start\":39635}]", "figure_ref": "[{\"end\":3999,\"start\":3991},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10570,\"start\":10562},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10718,\"start\":10710},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16143,\"start\":16135},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16349,\"start\":16341},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17060,\"start\":17052},{\"end\":19371,\"start\":19363},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19531,\"start\":19523},{\"end\":25284,\"start\":25274},{\"end\":25613,\"start\":25605},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27236,\"start\":27228},{\"end\":29941,\"start\":29933},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30725,\"start\":30716},{\"end\":30971,\"start\":30963},{\"end\":31077,\"start\":31069},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31490,\"start\":31481},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31907,\"start\":31898},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32236,\"start\":32228},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32592,\"start\":32583},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33002,\"start\":32993},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33686,\"start\":33677},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34798,\"start\":34786}]", "bib_author_first_name": "[{\"end\":40111,\"start\":40107},{\"end\":40128,\"start\":40123},{\"end\":40146,\"start\":40137},{\"end\":40161,\"start\":40154},{\"end\":40166,\"start\":40162},{\"end\":40179,\"start\":40173},{\"end\":40567,\"start\":40566},{\"end\":40585,\"start\":40577},{\"end\":40718,\"start\":40717},{\"end\":40732,\"start\":40726},{\"end\":40748,\"start\":40740},{\"end\":40764,\"start\":40761},{\"end\":40779,\"start\":40773},{\"end\":40794,\"start\":40790},{\"end\":40808,\"start\":40802},{\"end\":40820,\"start\":40813},{\"end\":40837,\"start\":40831},{\"end\":40848,\"start\":40845},{\"end\":41266,\"start\":41257},{\"end\":41276,\"start\":41273},{\"end\":41286,\"start\":41281},{\"end\":41296,\"start\":41293},{\"end\":41789,\"start\":41786},{\"end\":41803,\"start\":41796},{\"end\":41816,\"start\":41810},{\"end\":42272,\"start\":42269},{\"end\":42281,\"start\":42279},{\"end\":43190,\"start\":43183},{\"end\":43201,\"start\":43197},{\"end\":43213,\"start\":43208},{\"end\":43520,\"start\":43519},{\"end\":43540,\"start\":43539},{\"end\":43846,\"start\":43839},{\"end\":43858,\"start\":43851},{\"end\":44188,\"start\":44181},{\"end\":44196,\"start\":44193},{\"end\":44208,\"start\":44202},{\"end\":44223,\"start\":44216},{\"end\":44237,\"start\":44229},{\"end\":44247,\"start\":44243},{\"end\":44753,\"start\":44747},{\"end\":44776,\"start\":44770},{\"end\":44795,\"start\":44787},{\"end\":44807,\"start\":44802},{\"end\":44821,\"start\":44815},{\"end\":44835,\"start\":44829},{\"end\":44848,\"start\":44841},{\"end\":45317,\"start\":45311},{\"end\":45341,\"start\":45334},{\"end\":45359,\"start\":45351},{\"end\":45372,\"start\":45366},{\"end\":45385,\"start\":45381},{\"end\":45399,\"start\":45395},{\"end\":45416,\"start\":45410},{\"end\":45847,\"start\":45842},{\"end\":45860,\"start\":45855},{\"end\":45875,\"start\":45869},{\"end\":45891,\"start\":45887},{\"end\":46361,\"start\":46353},{\"end\":46737,\"start\":46736},{\"end\":46753,\"start\":46748},{\"end\":46986,\"start\":46983},{\"end\":46994,\"start\":46991},{\"end\":47007,\"start\":47000},{\"end\":47009,\"start\":47008},{\"end\":47364,\"start\":47357},{\"end\":47371,\"start\":47369},{\"end\":47387,\"start\":47378},{\"end\":47805,\"start\":47797},{\"end\":47818,\"start\":47811},{\"end\":47831,\"start\":47826},{\"end\":47847,\"start\":47842},{\"end\":47860,\"start\":47854},{\"end\":47873,\"start\":47869},{\"end\":47888,\"start\":47883},{\"end\":47907,\"start\":47897},{\"end\":48334,\"start\":48327},{\"end\":48348,\"start\":48340},{\"end\":48371,\"start\":48356},{\"end\":48876,\"start\":48872},{\"end\":48894,\"start\":48887},{\"end\":48911,\"start\":48905},{\"end\":49201,\"start\":49196},{\"end\":49445,\"start\":49439},{\"end\":49462,\"start\":49457},{\"end\":49475,\"start\":49468},{\"end\":49757,\"start\":49753},{\"end\":49769,\"start\":49766},{\"end\":49784,\"start\":49777},{\"end\":49802,\"start\":49795},{\"end\":49963,\"start\":49959},{\"end\":49974,\"start\":49970},{\"end\":49986,\"start\":49980},{\"end\":49999,\"start\":49994},{\"end\":50012,\"start\":50005},{\"end\":50285,\"start\":50278},{\"end\":50302,\"start\":50295},{\"end\":50325,\"start\":50317},{\"end\":50339,\"start\":50332},{\"end\":50693,\"start\":50686},{\"end\":50695,\"start\":50694},{\"end\":50703,\"start\":50700},{\"end\":50716,\"start\":50709},{\"end\":50724,\"start\":50721},{\"end\":50737,\"start\":50729},{\"end\":50739,\"start\":50738},{\"end\":51128,\"start\":51121},{\"end\":51130,\"start\":51129},{\"end\":51138,\"start\":51135},{\"end\":51150,\"start\":51143},{\"end\":51163,\"start\":51155},{\"end\":51165,\"start\":51164},{\"end\":51575,\"start\":51573},{\"end\":51603,\"start\":51600},{\"end\":51616,\"start\":51608},{\"end\":51618,\"start\":51617},{\"end\":52045,\"start\":52040},{\"end\":52058,\"start\":52051},{\"end\":52505,\"start\":52500},{\"end\":52517,\"start\":52511},{\"end\":52536,\"start\":52529},{\"end\":52902,\"start\":52896},{\"end\":52914,\"start\":52911},{\"end\":53149,\"start\":53140},{\"end\":53161,\"start\":53155},{\"end\":53169,\"start\":53167},{\"end\":53180,\"start\":53177},{\"end\":53195,\"start\":53187},{\"end\":53209,\"start\":53201},{\"end\":53225,\"start\":53216},{\"end\":53729,\"start\":53723},{\"end\":53753,\"start\":53743},{\"end\":53765,\"start\":53762},{\"end\":53782,\"start\":53770},{\"end\":53797,\"start\":53790},{\"end\":53809,\"start\":53806},{\"end\":53811,\"start\":53810},{\"end\":53832,\"start\":53825},{\"end\":54384,\"start\":54378},{\"end\":54408,\"start\":54398},{\"end\":54436,\"start\":54430},{\"end\":54452,\"start\":54445},{\"end\":54932,\"start\":54928},{\"end\":54946,\"start\":54939},{\"end\":54948,\"start\":54947},{\"end\":54963,\"start\":54956},{\"end\":55278,\"start\":55270},{\"end\":55294,\"start\":55289},{\"end\":55308,\"start\":55299},{\"end\":55326,\"start\":55324},{\"end\":55340,\"start\":55334},{\"end\":55350,\"start\":55346},{\"end\":55702,\"start\":55696},{\"end\":56057,\"start\":56056},{\"end\":56367,\"start\":56363},{\"end\":56381,\"start\":56374},{\"end\":56403,\"start\":56397},{\"end\":56411,\"start\":56408},{\"end\":56420,\"start\":56416},{\"end\":56427,\"start\":56425},{\"end\":56443,\"start\":56437},{\"end\":56458,\"start\":56454},{\"end\":56894,\"start\":56890},{\"end\":56907,\"start\":56901},{\"end\":56916,\"start\":56912},{\"end\":56929,\"start\":56922},{\"end\":56949,\"start\":56945},{\"end\":56956,\"start\":56954},{\"end\":56972,\"start\":56966},{\"end\":57521,\"start\":57519},{\"end\":57535,\"start\":57528},{\"end\":57552,\"start\":57545},{\"end\":57566,\"start\":57560},{\"end\":57583,\"start\":57577},{\"end\":57598,\"start\":57590},{\"end\":57600,\"start\":57599},{\"end\":58159,\"start\":58157},{\"end\":58173,\"start\":58167},{\"end\":58194,\"start\":58183},{\"end\":58212,\"start\":58206},{\"end\":58502,\"start\":58498},{\"end\":58514,\"start\":58510},{\"end\":58526,\"start\":58525},{\"end\":58547,\"start\":58537},{\"end\":58557,\"start\":58553},{\"end\":59035,\"start\":59029},{\"end\":59050,\"start\":59046},{\"end\":59069,\"start\":59061},{\"end\":59392,\"start\":59390},{\"end\":59407,\"start\":59399},{\"end\":59423,\"start\":59416},{\"end\":59433,\"start\":59428},{\"end\":59443,\"start\":59440},{\"end\":59916,\"start\":59913},{\"end\":59928,\"start\":59923},{\"end\":60283,\"start\":60275},{\"end\":60290,\"start\":60289},{\"end\":60311,\"start\":60305},{\"end\":60329,\"start\":60322},{\"end\":60341,\"start\":60336},{\"end\":60358,\"start\":60354},{\"end\":60374,\"start\":60366},{\"end\":60391,\"start\":60385}]", "bib_author_last_name": "[{\"end\":40121,\"start\":40112},{\"end\":40135,\"start\":40129},{\"end\":40152,\"start\":40147},{\"end\":40171,\"start\":40167},{\"end\":40187,\"start\":40180},{\"end\":40575,\"start\":40568},{\"end\":40592,\"start\":40586},{\"end\":40601,\"start\":40594},{\"end\":40724,\"start\":40719},{\"end\":40738,\"start\":40733},{\"end\":40759,\"start\":40749},{\"end\":40771,\"start\":40765},{\"end\":40788,\"start\":40780},{\"end\":40800,\"start\":40795},{\"end\":40811,\"start\":40809},{\"end\":40829,\"start\":40821},{\"end\":40843,\"start\":40838},{\"end\":40853,\"start\":40849},{\"end\":40857,\"start\":40855},{\"end\":41271,\"start\":41267},{\"end\":41279,\"start\":41277},{\"end\":41291,\"start\":41287},{\"end\":41299,\"start\":41297},{\"end\":41794,\"start\":41790},{\"end\":41808,\"start\":41804},{\"end\":41823,\"start\":41817},{\"end\":42277,\"start\":42273},{\"end\":42285,\"start\":42282},{\"end\":43195,\"start\":43191},{\"end\":43206,\"start\":43202},{\"end\":43218,\"start\":43214},{\"end\":43527,\"start\":43521},{\"end\":43537,\"start\":43529},{\"end\":43547,\"start\":43541},{\"end\":43555,\"start\":43549},{\"end\":43849,\"start\":43847},{\"end\":43867,\"start\":43859},{\"end\":44191,\"start\":44189},{\"end\":44200,\"start\":44197},{\"end\":44214,\"start\":44209},{\"end\":44227,\"start\":44224},{\"end\":44241,\"start\":44238},{\"end\":44251,\"start\":44248},{\"end\":44768,\"start\":44754},{\"end\":44785,\"start\":44777},{\"end\":44800,\"start\":44796},{\"end\":44813,\"start\":44808},{\"end\":44827,\"start\":44822},{\"end\":44839,\"start\":44836},{\"end\":44856,\"start\":44849},{\"end\":45332,\"start\":45318},{\"end\":45349,\"start\":45342},{\"end\":45364,\"start\":45360},{\"end\":45379,\"start\":45373},{\"end\":45393,\"start\":45386},{\"end\":45408,\"start\":45400},{\"end\":45422,\"start\":45417},{\"end\":45853,\"start\":45848},{\"end\":45867,\"start\":45861},{\"end\":45885,\"start\":45876},{\"end\":45897,\"start\":45892},{\"end\":46368,\"start\":46362},{\"end\":46746,\"start\":46738},{\"end\":46760,\"start\":46754},{\"end\":46764,\"start\":46762},{\"end\":46989,\"start\":46987},{\"end\":46998,\"start\":46995},{\"end\":47015,\"start\":47010},{\"end\":47367,\"start\":47365},{\"end\":47376,\"start\":47372},{\"end\":47390,\"start\":47388},{\"end\":47809,\"start\":47806},{\"end\":47824,\"start\":47819},{\"end\":47840,\"start\":47832},{\"end\":47852,\"start\":47848},{\"end\":47867,\"start\":47861},{\"end\":47881,\"start\":47874},{\"end\":47895,\"start\":47889},{\"end\":47915,\"start\":47908},{\"end\":48193,\"start\":48185},{\"end\":48338,\"start\":48335},{\"end\":48354,\"start\":48349},{\"end\":48376,\"start\":48372},{\"end\":48885,\"start\":48877},{\"end\":48903,\"start\":48895},{\"end\":48920,\"start\":48912},{\"end\":49217,\"start\":49202},{\"end\":49455,\"start\":49446},{\"end\":49466,\"start\":49463},{\"end\":49483,\"start\":49476},{\"end\":49764,\"start\":49758},{\"end\":49775,\"start\":49770},{\"end\":49793,\"start\":49785},{\"end\":49809,\"start\":49803},{\"end\":49818,\"start\":49811},{\"end\":49968,\"start\":49964},{\"end\":49978,\"start\":49975},{\"end\":49992,\"start\":49987},{\"end\":50003,\"start\":50000},{\"end\":50017,\"start\":50013},{\"end\":50293,\"start\":50286},{\"end\":50315,\"start\":50303},{\"end\":50330,\"start\":50326},{\"end\":50347,\"start\":50340},{\"end\":50698,\"start\":50696},{\"end\":50707,\"start\":50704},{\"end\":50719,\"start\":50717},{\"end\":50727,\"start\":50725},{\"end\":50746,\"start\":50740},{\"end\":51133,\"start\":51131},{\"end\":51141,\"start\":51139},{\"end\":51153,\"start\":51151},{\"end\":51172,\"start\":51166},{\"end\":51598,\"start\":51576},{\"end\":51606,\"start\":51604},{\"end\":51621,\"start\":51619},{\"end\":51629,\"start\":51623},{\"end\":52049,\"start\":52046},{\"end\":52066,\"start\":52059},{\"end\":52509,\"start\":52506},{\"end\":52527,\"start\":52518},{\"end\":52544,\"start\":52537},{\"end\":52909,\"start\":52903},{\"end\":52922,\"start\":52915},{\"end\":53153,\"start\":53150},{\"end\":53165,\"start\":53162},{\"end\":53175,\"start\":53170},{\"end\":53185,\"start\":53181},{\"end\":53199,\"start\":53196},{\"end\":53214,\"start\":53210},{\"end\":53228,\"start\":53226},{\"end\":53741,\"start\":53730},{\"end\":53760,\"start\":53754},{\"end\":53768,\"start\":53766},{\"end\":53788,\"start\":53783},{\"end\":53804,\"start\":53798},{\"end\":53823,\"start\":53812},{\"end\":53838,\"start\":53833},{\"end\":53847,\"start\":53840},{\"end\":54396,\"start\":54385},{\"end\":54428,\"start\":54409},{\"end\":54443,\"start\":54437},{\"end\":54460,\"start\":54453},{\"end\":54469,\"start\":54462},{\"end\":54937,\"start\":54933},{\"end\":54952,\"start\":54949},{\"end\":54967,\"start\":54964},{\"end\":55287,\"start\":55279},{\"end\":55297,\"start\":55295},{\"end\":55322,\"start\":55309},{\"end\":55332,\"start\":55327},{\"end\":55344,\"start\":55341},{\"end\":55361,\"start\":55351},{\"end\":55710,\"start\":55703},{\"end\":56063,\"start\":56058},{\"end\":56070,\"start\":56065},{\"end\":56372,\"start\":56368},{\"end\":56395,\"start\":56382},{\"end\":56406,\"start\":56404},{\"end\":56414,\"start\":56412},{\"end\":56423,\"start\":56421},{\"end\":56435,\"start\":56428},{\"end\":56452,\"start\":56444},{\"end\":56462,\"start\":56459},{\"end\":56899,\"start\":56895},{\"end\":56910,\"start\":56908},{\"end\":56920,\"start\":56917},{\"end\":56943,\"start\":56930},{\"end\":56952,\"start\":56950},{\"end\":56964,\"start\":56957},{\"end\":56981,\"start\":56973},{\"end\":57526,\"start\":57522},{\"end\":57543,\"start\":57536},{\"end\":57558,\"start\":57553},{\"end\":57575,\"start\":57567},{\"end\":57588,\"start\":57584},{\"end\":57607,\"start\":57601},{\"end\":58165,\"start\":58160},{\"end\":58181,\"start\":58174},{\"end\":58204,\"start\":58195},{\"end\":58216,\"start\":58213},{\"end\":58508,\"start\":58503},{\"end\":58523,\"start\":58515},{\"end\":58535,\"start\":58527},{\"end\":58551,\"start\":58548},{\"end\":58567,\"start\":58558},{\"end\":58584,\"start\":58569},{\"end\":59044,\"start\":59036},{\"end\":59059,\"start\":59051},{\"end\":59074,\"start\":59070},{\"end\":59397,\"start\":59393},{\"end\":59414,\"start\":59408},{\"end\":59426,\"start\":59424},{\"end\":59438,\"start\":59434},{\"end\":59446,\"start\":59444},{\"end\":59921,\"start\":59917},{\"end\":59934,\"start\":59929},{\"end\":60287,\"start\":60284},{\"end\":60303,\"start\":60291},{\"end\":60320,\"start\":60312},{\"end\":60334,\"start\":60330},{\"end\":60352,\"start\":60342},{\"end\":60364,\"start\":60359},{\"end\":60383,\"start\":60375},{\"end\":60397,\"start\":60392},{\"end\":60409,\"start\":60399}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16805301},\"end\":40536,\"start\":40022},{\"attributes\":{\"id\":\"b1\"},\"end\":40715,\"start\":40538},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b2\"},\"end\":41169,\"start\":40717},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":210919925},\"end\":41713,\"start\":41171},{\"attributes\":{\"id\":\"b4\"},\"end\":42170,\"start\":41715},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":214641094},\"end\":42678,\"start\":42172},{\"attributes\":{\"id\":\"b6\"},\"end\":43112,\"start\":42680},{\"attributes\":{\"doi\":\"arXiv:2007.13373\",\"id\":\"b7\"},\"end\":43400,\"start\":43114},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":972888},\"end\":43792,\"start\":43402},{\"attributes\":{\"id\":\"b9\"},\"end\":44100,\"start\":43794},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207853339},\"end\":44674,\"start\":44102},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":714538},\"end\":45201,\"start\":44676},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":42441056},\"end\":45768,\"start\":45203},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6016224},\"end\":46287,\"start\":45770},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":97383637},\"end\":46690,\"start\":46289},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b15\"},\"end\":46910,\"start\":46692},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4140974},\"end\":47251,\"start\":46912},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":204962112},\"end\":47752,\"start\":47253},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14113767},\"end\":48181,\"start\":47754},{\"attributes\":{\"id\":\"b19\"},\"end\":48212,\"start\":48183},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":219618574},\"end\":48812,\"start\":48214},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":9978124},\"end\":49179,\"start\":48814},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8969130},\"end\":49356,\"start\":49181},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4708306},\"end\":49749,\"start\":49358},{\"attributes\":{\"id\":\"b24\"},\"end\":49898,\"start\":49751},{\"attributes\":{\"doi\":\"arXiv:1812.11788\",\"id\":\"b25\"},\"end\":50219,\"start\":49900},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":201124572},\"end\":50625,\"start\":50221},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4868248},\"end\":51041,\"start\":50627},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5115938},\"end\":51491,\"start\":51043},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1745976},\"end\":51904,\"start\":51493},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4392433},\"end\":52410,\"start\":51906},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4331981},\"end\":52858,\"start\":52412},{\"attributes\":{\"doi\":\"arXiv:1804.02767\",\"id\":\"b32\"},\"end\":53069,\"start\":52860},{\"attributes\":{\"id\":\"b33\"},\"end\":53658,\"start\":53071},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":199064307},\"end\":54302,\"start\":53660},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52953916},\"end\":54850,\"start\":54304},{\"attributes\":{\"doi\":\"arXiv:2007.08454\",\"id\":\"b36\"},\"end\":55188,\"start\":54852},{\"attributes\":{\"doi\":\"arXiv:1809.10790\",\"id\":\"b37\"},\"end\":55612,\"start\":55190},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":206421766},\"end\":55959,\"start\":55614},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":23402936},\"end\":56293,\"start\":55961},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":204852023},\"end\":56822,\"start\":56295},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":58006460},\"end\":57427,\"start\":56824},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":57761160},\"end\":58064,\"start\":57429},{\"attributes\":{\"doi\":\"arXiv:1711.00199\",\"id\":\"b43\"},\"end\":58444,\"start\":58066},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":209370516},\"end\":58984,\"start\":58446},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":102350792},\"end\":59322,\"start\":58986},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":56178817},\"end\":59838,\"start\":59324},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":42427078},\"end\":60206,\"start\":59840},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14053538},\"end\":60793,\"start\":60208}]", "bib_title": "[{\"end\":40105,\"start\":40022},{\"end\":41255,\"start\":41171},{\"end\":41784,\"start\":41715},{\"end\":42267,\"start\":42172},{\"end\":42752,\"start\":42680},{\"end\":43517,\"start\":43402},{\"end\":43837,\"start\":43794},{\"end\":44179,\"start\":44102},{\"end\":44745,\"start\":44676},{\"end\":45309,\"start\":45203},{\"end\":45840,\"start\":45770},{\"end\":46351,\"start\":46289},{\"end\":46981,\"start\":46912},{\"end\":47355,\"start\":47253},{\"end\":47795,\"start\":47754},{\"end\":48325,\"start\":48214},{\"end\":48870,\"start\":48814},{\"end\":49194,\"start\":49181},{\"end\":49437,\"start\":49358},{\"end\":50276,\"start\":50221},{\"end\":50684,\"start\":50627},{\"end\":51119,\"start\":51043},{\"end\":51571,\"start\":51493},{\"end\":52038,\"start\":51906},{\"end\":52498,\"start\":52412},{\"end\":53138,\"start\":53071},{\"end\":53721,\"start\":53660},{\"end\":54376,\"start\":54304},{\"end\":55694,\"start\":55614},{\"end\":56054,\"start\":55961},{\"end\":56361,\"start\":56295},{\"end\":56888,\"start\":56824},{\"end\":57517,\"start\":57429},{\"end\":58496,\"start\":58446},{\"end\":59027,\"start\":58986},{\"end\":59388,\"start\":59324},{\"end\":59911,\"start\":59840},{\"end\":60273,\"start\":60208}]", "bib_author": "[{\"end\":40123,\"start\":40107},{\"end\":40137,\"start\":40123},{\"end\":40154,\"start\":40137},{\"end\":40173,\"start\":40154},{\"end\":40189,\"start\":40173},{\"end\":40577,\"start\":40566},{\"end\":40594,\"start\":40577},{\"end\":40603,\"start\":40594},{\"end\":40726,\"start\":40717},{\"end\":40740,\"start\":40726},{\"end\":40761,\"start\":40740},{\"end\":40773,\"start\":40761},{\"end\":40790,\"start\":40773},{\"end\":40802,\"start\":40790},{\"end\":40813,\"start\":40802},{\"end\":40831,\"start\":40813},{\"end\":40845,\"start\":40831},{\"end\":40855,\"start\":40845},{\"end\":40859,\"start\":40855},{\"end\":41273,\"start\":41257},{\"end\":41281,\"start\":41273},{\"end\":41293,\"start\":41281},{\"end\":41301,\"start\":41293},{\"end\":41796,\"start\":41786},{\"end\":41810,\"start\":41796},{\"end\":41825,\"start\":41810},{\"end\":42279,\"start\":42269},{\"end\":42287,\"start\":42279},{\"end\":43197,\"start\":43183},{\"end\":43208,\"start\":43197},{\"end\":43220,\"start\":43208},{\"end\":43529,\"start\":43519},{\"end\":43539,\"start\":43529},{\"end\":43549,\"start\":43539},{\"end\":43557,\"start\":43549},{\"end\":43851,\"start\":43839},{\"end\":43869,\"start\":43851},{\"end\":44193,\"start\":44181},{\"end\":44202,\"start\":44193},{\"end\":44216,\"start\":44202},{\"end\":44229,\"start\":44216},{\"end\":44243,\"start\":44229},{\"end\":44253,\"start\":44243},{\"end\":44770,\"start\":44747},{\"end\":44787,\"start\":44770},{\"end\":44802,\"start\":44787},{\"end\":44815,\"start\":44802},{\"end\":44829,\"start\":44815},{\"end\":44841,\"start\":44829},{\"end\":44858,\"start\":44841},{\"end\":45334,\"start\":45311},{\"end\":45351,\"start\":45334},{\"end\":45366,\"start\":45351},{\"end\":45381,\"start\":45366},{\"end\":45395,\"start\":45381},{\"end\":45410,\"start\":45395},{\"end\":45424,\"start\":45410},{\"end\":45855,\"start\":45842},{\"end\":45869,\"start\":45855},{\"end\":45887,\"start\":45869},{\"end\":45899,\"start\":45887},{\"end\":46370,\"start\":46353},{\"end\":46748,\"start\":46736},{\"end\":46762,\"start\":46748},{\"end\":46766,\"start\":46762},{\"end\":46991,\"start\":46983},{\"end\":47000,\"start\":46991},{\"end\":47017,\"start\":47000},{\"end\":47369,\"start\":47357},{\"end\":47378,\"start\":47369},{\"end\":47392,\"start\":47378},{\"end\":47811,\"start\":47797},{\"end\":47826,\"start\":47811},{\"end\":47842,\"start\":47826},{\"end\":47854,\"start\":47842},{\"end\":47869,\"start\":47854},{\"end\":47883,\"start\":47869},{\"end\":47897,\"start\":47883},{\"end\":47917,\"start\":47897},{\"end\":48195,\"start\":48185},{\"end\":48340,\"start\":48327},{\"end\":48356,\"start\":48340},{\"end\":48378,\"start\":48356},{\"end\":48887,\"start\":48872},{\"end\":48905,\"start\":48887},{\"end\":48922,\"start\":48905},{\"end\":49219,\"start\":49196},{\"end\":49457,\"start\":49439},{\"end\":49468,\"start\":49457},{\"end\":49485,\"start\":49468},{\"end\":49766,\"start\":49753},{\"end\":49777,\"start\":49766},{\"end\":49795,\"start\":49777},{\"end\":49811,\"start\":49795},{\"end\":49820,\"start\":49811},{\"end\":49970,\"start\":49959},{\"end\":49980,\"start\":49970},{\"end\":49994,\"start\":49980},{\"end\":50005,\"start\":49994},{\"end\":50019,\"start\":50005},{\"end\":50295,\"start\":50278},{\"end\":50317,\"start\":50295},{\"end\":50332,\"start\":50317},{\"end\":50349,\"start\":50332},{\"end\":50700,\"start\":50686},{\"end\":50709,\"start\":50700},{\"end\":50721,\"start\":50709},{\"end\":50729,\"start\":50721},{\"end\":50748,\"start\":50729},{\"end\":51135,\"start\":51121},{\"end\":51143,\"start\":51135},{\"end\":51155,\"start\":51143},{\"end\":51174,\"start\":51155},{\"end\":51600,\"start\":51573},{\"end\":51608,\"start\":51600},{\"end\":51623,\"start\":51608},{\"end\":51631,\"start\":51623},{\"end\":52051,\"start\":52040},{\"end\":52068,\"start\":52051},{\"end\":52511,\"start\":52500},{\"end\":52529,\"start\":52511},{\"end\":52546,\"start\":52529},{\"end\":52911,\"start\":52896},{\"end\":52924,\"start\":52911},{\"end\":53155,\"start\":53140},{\"end\":53167,\"start\":53155},{\"end\":53177,\"start\":53167},{\"end\":53187,\"start\":53177},{\"end\":53201,\"start\":53187},{\"end\":53216,\"start\":53201},{\"end\":53230,\"start\":53216},{\"end\":53743,\"start\":53723},{\"end\":53762,\"start\":53743},{\"end\":53770,\"start\":53762},{\"end\":53790,\"start\":53770},{\"end\":53806,\"start\":53790},{\"end\":53825,\"start\":53806},{\"end\":53840,\"start\":53825},{\"end\":53849,\"start\":53840},{\"end\":54398,\"start\":54378},{\"end\":54430,\"start\":54398},{\"end\":54445,\"start\":54430},{\"end\":54462,\"start\":54445},{\"end\":54471,\"start\":54462},{\"end\":54939,\"start\":54928},{\"end\":54956,\"start\":54939},{\"end\":54969,\"start\":54956},{\"end\":55289,\"start\":55270},{\"end\":55299,\"start\":55289},{\"end\":55324,\"start\":55299},{\"end\":55334,\"start\":55324},{\"end\":55346,\"start\":55334},{\"end\":55363,\"start\":55346},{\"end\":55712,\"start\":55696},{\"end\":56065,\"start\":56056},{\"end\":56072,\"start\":56065},{\"end\":56374,\"start\":56363},{\"end\":56397,\"start\":56374},{\"end\":56408,\"start\":56397},{\"end\":56416,\"start\":56408},{\"end\":56425,\"start\":56416},{\"end\":56437,\"start\":56425},{\"end\":56454,\"start\":56437},{\"end\":56464,\"start\":56454},{\"end\":56901,\"start\":56890},{\"end\":56912,\"start\":56901},{\"end\":56922,\"start\":56912},{\"end\":56945,\"start\":56922},{\"end\":56954,\"start\":56945},{\"end\":56966,\"start\":56954},{\"end\":56983,\"start\":56966},{\"end\":57528,\"start\":57519},{\"end\":57545,\"start\":57528},{\"end\":57560,\"start\":57545},{\"end\":57577,\"start\":57560},{\"end\":57590,\"start\":57577},{\"end\":57609,\"start\":57590},{\"end\":58167,\"start\":58157},{\"end\":58183,\"start\":58167},{\"end\":58206,\"start\":58183},{\"end\":58218,\"start\":58206},{\"end\":58510,\"start\":58498},{\"end\":58525,\"start\":58510},{\"end\":58537,\"start\":58525},{\"end\":58553,\"start\":58537},{\"end\":58569,\"start\":58553},{\"end\":58586,\"start\":58569},{\"end\":59046,\"start\":59029},{\"end\":59061,\"start\":59046},{\"end\":59076,\"start\":59061},{\"end\":59399,\"start\":59390},{\"end\":59416,\"start\":59399},{\"end\":59428,\"start\":59416},{\"end\":59440,\"start\":59428},{\"end\":59448,\"start\":59440},{\"end\":59923,\"start\":59913},{\"end\":59936,\"start\":59923},{\"end\":60289,\"start\":60275},{\"end\":60305,\"start\":60289},{\"end\":60322,\"start\":60305},{\"end\":60336,\"start\":60322},{\"end\":60354,\"start\":60336},{\"end\":60366,\"start\":60354},{\"end\":60385,\"start\":60366},{\"end\":60399,\"start\":60385},{\"end\":60411,\"start\":60399}]", "bib_venue": "[{\"end\":40258,\"start\":40189},{\"end\":40564,\"start\":40538},{\"end\":40914,\"start\":40875},{\"end\":41382,\"start\":41301},{\"end\":41893,\"start\":41825},{\"end\":42356,\"start\":42287},{\"end\":42763,\"start\":42754},{\"end\":43181,\"start\":43114},{\"end\":43582,\"start\":43557},{\"end\":43928,\"start\":43869},{\"end\":44334,\"start\":44253},{\"end\":44920,\"start\":44858},{\"end\":45459,\"start\":45424},{\"end\":45968,\"start\":45899},{\"end\":46473,\"start\":46370},{\"end\":46734,\"start\":46692},{\"end\":47066,\"start\":47017},{\"end\":47459,\"start\":47392},{\"end\":47955,\"start\":47917},{\"end\":48459,\"start\":48378},{\"end\":48978,\"start\":48922},{\"end\":49252,\"start\":49219},{\"end\":49534,\"start\":49485},{\"end\":49957,\"start\":49900},{\"end\":50397,\"start\":50349},{\"end\":50817,\"start\":50748},{\"end\":51250,\"start\":51174},{\"end\":51680,\"start\":51631},{\"end\":52137,\"start\":52068},{\"end\":52615,\"start\":52546},{\"end\":52894,\"start\":52860},{\"end\":53311,\"start\":53230},{\"end\":53930,\"start\":53849},{\"end\":54535,\"start\":54471},{\"end\":54926,\"start\":54852},{\"end\":55268,\"start\":55190},{\"end\":55772,\"start\":55712},{\"end\":56112,\"start\":56072},{\"end\":56532,\"start\":56464},{\"end\":57071,\"start\":56983},{\"end\":57686,\"start\":57609},{\"end\":58155,\"start\":58066},{\"end\":58667,\"start\":58586},{\"end\":59135,\"start\":59076},{\"end\":59529,\"start\":59448},{\"end\":60005,\"start\":59936},{\"end\":60443,\"start\":60411},{\"end\":41450,\"start\":41384},{\"end\":44402,\"start\":44336},{\"end\":47513,\"start\":47461},{\"end\":48527,\"start\":48461},{\"end\":53379,\"start\":53313},{\"end\":53998,\"start\":53932},{\"end\":54586,\"start\":54537},{\"end\":57146,\"start\":57073},{\"end\":57750,\"start\":57688},{\"end\":58735,\"start\":58669},{\"end\":59597,\"start\":59531}]"}}}, "year": 2023, "month": 12, "day": 17}