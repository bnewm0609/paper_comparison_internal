{"id": 211478557, "updated": "2023-09-27 21:55:19.953", "metadata": {"title": "Safe Linear Thompson Sampling with Side Information", "authors": "[{\"first\":\"Ahmadreza\",\"last\":\"Moradipari\",\"middle\":[]},{\"first\":\"Sanae\",\"last\":\"Amani\",\"middle\":[]},{\"first\":\"Mahnoosh\",\"last\":\"Alizadeh\",\"middle\":[]},{\"first\":\"Christos\",\"last\":\"Thrampoulidis\",\"middle\":[]}]", "venue": "ArXiv", "journal": "IEEE Transactions on Signal Processing", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "The design and performance analysis of bandit algorithms in the presence of stage-wise safety or reliability constraints has recently garnered significant interest. In this work, we consider the linear stochastic bandit problem under additional linear safety constraints that need to be satisfied at each round. We provide a new safe algorithm based on linear Thompson Sampling (TS) for this problem and show a frequentist regret of order O(d log d \u00b7 T 1/2 log T ), which remarkably matches the results provided by (Abeille et al., 2017) for the standard linear TS algorithm in the absence of safety constraints. We compare the performance of our algorithm with UCBbased safe algorithms and highlight how the inherently randomized nature of TS leads to a superior performance in expanding the set of safe actions the algorithm has access to at each round.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1911.02156", "mag": "3172631248", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tsp/MoradipariAAT21", "doi": "10.1109/tsp.2021.3089822"}}, "content": {"source": {"pdf_hash": "a8d2502866c0e584cd7268826c503fe5947e00b2", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1911.02156", "status": "GREEN"}}, "grobid": {"id": "c5ef00b71cb7e877b4bff7520812544c3c0abff6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a8d2502866c0e584cd7268826c503fe5947e00b2.txt", "contents": "\nSafe Linear Thompson Sampling with Side Information\n\n\nAnonymous Authors \nSafe Linear Thompson Sampling with Side Information\n000 001\nThe design and performance analysis of bandit algorithms in the presence of stage-wise safety or reliability constraints has recently garnered significant interest. In this work, we consider the linear stochastic bandit problem under additional linear safety constraints that need to be satisfied at each round. We provide a new safe algorithm based on linear Thompson Sampling (TS) for this problem and show a frequentist regret of order O(d 3/2 log 1/2 d \u00b7 T 1/2 log 3/2 T ), which remarkably matches the results provided by(Abeille et al., 2017)for the standard linear TS algorithm in the absence of safety constraints. We compare the performance of our algorithm with UCBbased safe algorithms and highlight how the inherently randomized nature of TS leads to a superior performance in expanding the set of safe actions the algorithm has access to at each round. 056\n\nIntroduction\n\nThe application of stochastic bandit optimization algorithms to safety-critical systems has received significant attention in the past few years. In such cases, the learner repeatedly interacts with a system with uncertain reward function and operational constraints. In spite of this uncertainty, the learner needs to ensure that her actions do not violate the operational constraints at any round of the learning process. As such, especially in the earlier rounds, there is a need to choose actions with caution, while at the same time making sure that the chosen action provide sufficient learning opportunities about the set of safe actions. Notably, the actions deemed safe by the algorithm might not originally include the optimal action. This uncertainty about safety and the resulting conservative behavior means the learner could experience additional regret in such constrained environments.\n\nIn this paper, we focus on a special class of stochastic bandit Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute. optimization problems where the reward is a linear function of the actions. This class of problems, referred to as linear stochastic bandits (LB), generalizes multi-armed bandit (MAB) problems to the setting where each action is associated with a feature vector x, and the expected reward of playing each action is equal to the inner product of its feature vector and an unknown parameter vector \u03b8 . There exists several variants of LB that study the finite (Auer et al., 2002) or infinite (Dani et al., 2008;Rusmevichientong & Tsitsiklis, 2010;Abbasi-Yadkori et al., 2011) set of actions, as well as the case where the set of feature vectors can change over time (Chu et al., 2011;Li et al., 2010). Two efficient approaches have been developed for LB: linear UCB (LUCB) and linear Thompson Sampling (LTS). For LUCB, (Abbasi-Yadkori et al., 2011) provides a regret bound of order O(d \u00b7 T 1/2 log T ). For LTS (Agrawal & Goyal, 2013;Abeille et al., 2017) adopt a frequentist view and show a regret of order O(d 3/2 log 1/2 d \u00b7 T 1/2 log 3/2 T ). Here we provide a LTS algorithm that respects linear safety constraints and study its performance. We formally define the problem setting before summarizing our contributions.\n\n\nSafe Stochastic Linear Bandit Model\n\nReward function. The learner is given a convex and compact set of actions D 0 \u2282 R d . At each round t, playing an action x t \u2208 D 0 results in observing reward\nr t := x t \u03b8 + \u03be t ,(1)\nwhere \u03b8 \u2208 R d is a fixed, but unknown, parameter and \u03be t is a zero-mean additive noise.\n\nSafety constraint. We further assume that the environment is subject to a linear constraint:\nx t \u00b5 \u2264 C,(2)\nwhich needs to be satisfied by the action x t at every round t, to guarantee safe operation of the system. Here, C is a positive constant that is known to the learner, while \u00b5 is a fixed, but unknown vector parameter. Let us denote the set of \"safe actions\" that satisfy the constraint (2) as follows:\nD s 0 (\u00b5 ) := {x \u2208 D 0 : x \u00b5 \u2264 C}.(3)\nClearly, D s 0 (\u00b5 ) is unknown to the learner, since \u00b5 is itself unknown. However, we consider a setting in which, at every round t, the learner receives side information about the safety set via noisy measurements:\nw t = x t \u00b5 + \u03b6 t ,(4)\nwhere \u03b6 t is zero-mean additive noise. During the learning process, the learner needs a mechanism that allows her to use the side measurements in (4) for determining the safe set D s 0 (\u00b5 ). This is critical, since it is required (at least with high-probability) that x t \u2208 D s 0 (\u00b5 ) for all rounds t. Regret. The cumulative pseudo-regret of the learner up to round T is defined as R(T ) = T t=1 x \u03b8 \u2212 x t \u03b8 , where x is the optimal safe action that maximizes the expected reward over D s 0 (\u00b5 ), i.e.,\n\nx = arg max x\u2208D s 0 (\u00b5 * )\n\nx \u03b8 .\n\nLearning goal. The learner's objective is to control the growth of the pseudo-regret. Moreover, we require that the chosen actions x t , t \u2208 [T ] are safe (i.e., they belong to D s 0 (\u00b5 ) in (3)) with high probability. As is common, we use regret to refer to the pseudo-regret R(T ).\n\n\nContributions\n\n\u2022 We provide the first safe LTS (Safe-LTS) algorithm with provable regret guarantees for the linear bandit problem with linear safety constraints.\n\n\u2022 Our regret analysis shows that Safe-LTS achieves the same order O(d 3/2 log 1/2 d \u00b7 T 1/2 log 3/2 T ) of regret as the original LTS (without safety constraints) as shown by (Abeille et al., 2017). Hence, the dependence of the regret of Safe-LTS on the time horizon T cannot be improved modulo logarithmic factors (see lower bounds for LB in (Dani et al., 2008;Rusmevichientong & Tsitsiklis, 2010)).\n\n\u2022 We compare Safe-LTS to the existing safe versions of LUCB for linear stochastic bandits with linear stage-wise safety constraints. We show that our algorithm has: better regret in the worst-case, fewer parameters to tune and superior empirical performance.\n\n\nRelated Work\n\nSafety -A diverse body of related works on stochastic optimization and control have considered the effect of safety constraints that need to be met during the run of the algorithm (Aswani et al., 2013;Koller et al., 2018) and references therein. Closely related to our work, (Sui et al., 2015; study nonlinear bandit optimization with nonlinear safety constraints using Gaussian processes (GPs) as nonparametric models for both the reward and the constraint functions. Their algorithms have shown great promises in robotics applications (Ostafew et al., 2016;Akametalu et al., 2014). Without the GP assumption, (Usmanova et al., 2019) proposes and analyzes a safe variant of the Frank-Wolfe algorithm to solve a smooth optimization problem with an unknown convex objective function and unknown linear constraints (with side information, similar to our setting). All the above algorithms come with provable convergence guarantees, but no regret bounds. To the best of our knowledge, the first work that derived an algorithm with provable regret guarantees for bandit optimization with stage-wise safety constraints, as the ones imposed on the aforementioned works, is (Amani et al., 2019). While (Amani et al., 2019) restricts attention to a linear setting, their results reveal that the presence of the safety constraint -even though linear-can have a non-trivial effect on the performance of LUCB-type algorithms. Specifically, the proposed Safe-LUCB algorithm comes with a problem-dependent regret bound that depends critically on the location of the optimal action in the safe action set -increasingly so in problem instances for which the safety constraint is active. In (Amani et al., 2019), the linear constraint function involves the same unknown vector (say, \u03b8 ) as the one that specifies the linear reward. Instead, in Section 1.1 we allow the constraint to depend on a new parameter vector (say, \u00b5 ) to which the learner get access via side-information measurements (4). This latter setting is the direct linear analogue to that of (Sui et al., 2015;Usmanova et al., 2019) and we demonstrate that an appropriate Safe-LTS algorithm enjoys regret guarantees of the same order as the original LTS without safety constraints. A more elaborate discussion comparing our results to (Amani et al., 2019) is provided in Section 4.3. We also mention (Kazerouni et al., 2017) as another recent work on safe linear bandits. In contrast to the previously mentioned references, (Kazerouni et al., 2017) defines safety as the requirement of ensuring that the cumulative (linear) reward up to each round stays above a given percentage of the performance of a known baseline policy. As a closing remark, (Amani et al., 2019;Kazerouni et al., 2017;Usmanova et al., 2019) show that simple linear models for safety constraints might be directly relevant to several applications such as medical trials applications, recommendation systems or managing the customers' demand in power-grid systems. Moreover, even in more complex settings where linear models do not directly apply (e.g., (Ostafew et al., 2016;Akametalu et al., 2014)), we still believe that this simplification is an appropriate first step towards a principled study of the regret performance of safe algorithms in sequential decision settings.\n\nThompson Sampling -Even though TS-based algorithms (Thompson, 1933) are computationally easier to implement than UCB-based algorithms and have shown great empirical performance, they were largely ignored by the academic community until a few years ago, when a series of papers (e.g., (Russo & Van Roy, 2014; Abeille et al.,\n\n\nAlgorithm 1 Safe Linear Thompson Sampling (Safe-LTS)\n\nInput: \u03b4, T, \u03bb Set \u03b4 = \u03b4\n6 for t = 1 to T do Sample \u03b7 t \u223c H TS (see Section 2.2.2) Set V t = \u03bbI + t\u22121 s=1 x s x s Compute RLS-estimate\u03b8 t and\u03bc t (see (5)) Set:\u03b8 t =\u03b8 t + \u03b2 t (\u03b4 )V \u2212 1 2 t \u03b7 t Build the confidence region: C t (\u03b4 ) = {v \u2208 R : v \u2212\u03bc Vt \u2264 \u03b2 t (\u03b4 )} Compute the estimated safe set: D s t = {x \u2208 D 0 :\nx v \u2264 C, \u2200v \u2208 C t (\u03b4 )} Play the following safe action: x t = arg max x\u2208D s t x \u03b8 t Observe reward r t and measurement w t end for 2017; Agrawal & Goyal, 2012;Kaufmann et al., 2012)) showed that TS achieves optimal performance in both frequentist and Bayesian settings. Most of the literature focused on the analysis of the Bayesian regret of TS for general settings such as linear bandits or reinforcement learning (see e.g., (Osband & Van Roy, 2015)). More recently, (Russo & Van Roy, 2016;Dong & Van Roy, 2018;Dong et al., 2019) provided an information-theoretic analysis of TS.Additionally, (Gopalan & Mannor, 2015) provides regret guarantees for TS in the finite and infinite MDP setting. Another notable paper is (Gopalan et al., 2014), which studies the stochastic MAB problem in complex action settings providing a regret bound that scales logarithmically in time with improved constants. None of the aforementioned papers study the performance of TS for linear bandits with safety constraints.\n\n\nSafe Linear Thompson Sampling\n\nOur proposed algorithm is a safe variant of Linear Thompson Sampling (LTS). At any round t, given a regularized least-squares (RLS) estimate\u03b8 t , the algorithm samples a perturbed parameter\u03b8 t that is appropriately distributed to guarantee sufficient exploration. Considering this sampled \u03b8 t as the true environment, the algorithm chooses the action with the highest possible reward while making sure that the safety constraint (2) holds. In order to ensure that actions remain safe at all rounds, the algorithm uses the sideinformation (4) to construct a confidence region C t , which contains the unknown parameter \u00b5 with high probability. With this, it forms an inner approximation D s t of the safe set, which is composed by all actions x t that satisfy the safety constraint for all v \u2208 C t . The summary is presented in Algorithm 1 and a detailed description follows.\n\n\nModel assumptions\n\nNotation.\n\n[n] denotes the set {1, 2, . . . , n}. The Euclidean norm of a vector x is denoted by x 2 . Its weighted 2 -norm with respect to a positive semidefinite matrix V is denoted by\nx V = \u221a\nx V x. We also use the standard O notation that ignores poly-logarithmic factors. Finally, for ease of notation, from now onwards we refer to the safe set in (12) by D s 0 and drop the dependence on \u00b5 . Let F t = (F 1 , \u03c3(x 1 , . . . , x t , \u03be 1 , . . . , \u03be t , \u03b6 1 , . . . , \u03b6 t )) denote the filtration that represents the accumulated information up to round t. In the following, we introduce standard assumptions on the problem.\n\nAssumption 1. For all t, \u03be t and \u03b6 t are conditionally zeromean, R-sub-Gaussian noise variables, i.e.,\nE[\u03be t |F t\u22121 ] = E[\u03b6 t |F t\u22121 ] = 0, and E[e \u03b1\u03bet |F t\u22121 ] \u2264 exp ( \u03b1 2 R 2 2 ), E[e \u03b1\u03b6t |F t\u22121 ] \u2264 exp ( \u03b1 2 R 2 2 ), \u2200\u03b1 \u2208 R d . Assumption 2.\nThere exists a positive constant S such that \u03b8 2 \u2264 S and \u00b5 2 \u2264 S.\n\nAssumption 3. The action set D 0 is a compact and convex subset of R d that contains the origin. We assume x 2 \u2264 L, \u2200x \u2208 D 0 .\n\nIt is straightforward to generalize our results when the subgaussian constants of \u03be t and \u03b6 t and/or the upper bounds on \u03b8 2 and \u00b5 2 are different. Throughout, we assume they are equal, for brevity.\n\n\nAlgorithm description and discussion\n\nLet (x 1 , . . . , x t ) be the sequence of actions and (r 1 , . . . , r t ) and (w 1 , . . . , w t ) be the corresponding rewards and sideinformation measurements, respectively. For any \u03bb > 0, we can obtain RLS-estimates\u03b8 t of \u03b8 and\u03bc t of \u00b5 as follows:\n\u03b8 t = V \u22121 t t\u22121 s=1 r s x s ,\u03bc t = V \u22121 t t\u22121 s=1 w s x s ,(5)V t = \u03bbI + t\u22121 s=1 x s x s ,(6)\nwhere V t is the Gram matrix of the actions. Based on\u03b8 t and \u00b5 t , Algorithm 1 constructs two confidence regions E t := E t (\u03b4 ) and C t := C t (\u03b4 ) as follows:\nE t := {\u03b8 \u2208 R d : \u03b8 \u2212\u03b8 t Vt \u2264 \u03b2 t (\u03b4 )},(7)C t := {v \u2208 R d : v \u2212\u03bc t Vt \u2264 \u03b2 t (\u03b4 )}.(8)\nNotice that E t and C t both depend on \u03b4 , but we suppress notation for simplicity, when clear from cotext. The ellipsoid radius \u03b2 t is chosen according to the Theorem 2.1 in (Abbasi-Yadkori et al., 2011) in order to guarantee that \u03b8 \u2208 E t and \u00b5 \u2208 C t with high probability. \n\u03b2 t (\u03b4) = R d log 1 + (t\u22121)L 2 \u03bb \u03b4 + \u221a \u03bbS,(9)\nwith probability at least 1 \u2212 \u03b4, it holds that \u03b8 \u2208 E t (\u03b4) and \u00b5 \u2208 C t (\u03b4), for all t \u2265 1.\n\n\nBACKGROUND ON LTS: A FREQUENTLY\n\n\nOPTIMISTIC ALGORITHM\n\nOur algorithm inherits the frequentist view of LTS first introduced in (Agrawal & Goyal, 2013;Abeille et al., 2017), which is essentially defined as a randomized algorithm over the RLS-estimate\u03b8 t of the unknown parameter \u03b8 . Specifically, at any round t, the randomized algorithm of (Agrawal & Goyal, 2013;Abeille et al., 2017) samples a parameter\u03b8 t centered at\u03b8 t :\u03b8\nt =\u03b8 t + \u03b2 t (\u03b4 )V \u2212 1 2 t \u03b7 t ,(10)\nand chooses the action x t that is best with respect to the new sampled parameter, i.e., maximizes the objective x t\u03b8t .\n\nThe key idea of (Agrawal & Goyal, 2013;Abeille et al., 2017) on how to select the random perturbation \u03b7 t \u2208 R d to guarantee good regret performance is as follows. On the one hand,\u03b8 t must stay close enough to the RLS-estimate\u03b8 t so that x t\u03b8t is a good proxy for the true (but unknown) reward x t \u03b8 . Thus, \u03b7 t must satisfy an appropriate concentration property. On the other hand,\u03b8 t must also favor exploration in a sense that it leads -often enough-to actions x t that are optimistic, i.e., they satisfy\nx t\u03b8t \u2265 x \u03b8(11)\nThus, \u03b7 t must satisfy an appropriate anti-concentration property.\n\nOur proposed Algorithm 1 also builds on these two key ideas. However, we discuss next how the safe setting imposes additional challenges and how our algorithm and its analysis manages to address them.\n\n\nADDRESSING CHALLENGES IN THE SAFE SETTING\n\nCompared to the classical linear bandit setting studied in (Agrawal & Goyal, 2013;Abeille et al., 2017), the presence of the safety constraint raises the following two questions:\n\n(i) How to guarantee actions played at each round are safe?\n\n(ii) In the face of the safety restrictions, how can optimism (cf. (11)) be maintained?\n\nIn the rest of this section, we explain the mechanisms that Safe-LTS Algorithm 1 employs to address both of these challenges.\n\nSafety -First, the chosen action x t at each round need not only maximize x t\u03b8t , but also, it needs to be safe. Since the learner does not know the safe action set D s 0 , Algorithm 1 performs conservatively and guarantees safety as follows. After creating the confidence region C t around the RLSestimate\u03bc t , it forms the so-called safe decision set at round t denoted as D s t :\nD s t = {x \u2208 D 0 : x v \u2264 C, \u2200v \u2208 C t }.(12)\nThen, the chosen action is optimized over only the subset D s t , i.e.,\nx t = arg max x\u2208D s t x \u03b8 t .(13)\nWe make the following two important remarks about the set D s t defined in (12). On a positive note, D s t is easy to compute. To see this, note the following equivalent definitions:\nD s t := {x \u2208 D 0 : x v \u2264 C, \u2200v \u2208 C t } = {x \u2208 D 0 : max v\u2208Ct x v \u2264 C} = {x \u2208 D 0 : x \u03bc t + \u03b2 t (\u03b4 ) x V \u22121 t \u2264 C}. (14)\nThus, in view of (14), the optimization in (13) is an efficient convex quadratic program. The challenge with D s t is that it contains actions which are safe with respect to all the parameters in C t , and not only \u00b5 . As such, it is only an inner approximation of the true safe set D s 0 . As we will see next, this fact complicates the requirement for optimism.\n\nOptimism in the face of safety -As previously discussed, in order to guarantee safety, Algorithm 1 chooses actions from the subset D s t . This is only an inner approximation of the true safe set D s 0 , a fact that makes it harder to maintain optimism of x t as defined in (11). To see this, note that in the classical setting of (Abeille et al., 2017), their algorithm would choose x t as the action that maximizes\u03b8 t over the entire set D 0 . In turn, this would imply that x t\u03b8t \u2265 x \u03b8 t because x belongs to the feasible set D 0 . This observation is the critical first argument in proving that x t is optimistic often enough, i.e., (11) holds with fixed probability p > 0.\n\nUnfortunately, in the presence of safety constraints, the action x t is a maximizer over only the subset D s t . Since x may not lie within D s t , there is no guarantee that x t\u03b8t \u2265 x \u03b8 t as before. So, how does then one guarantee optimism?\n\nIntuitively, at the first rounds, the estimated safe set D s t is only a small subset of the true D s 0 . Thus, x t \u2208 D s t is a vector of small norm compared to that of x \u2208 D s 0 . Thus, for (11) to hold, it must be that\u03b8 t is not only in the direction of \u03b8 , but it also has larger norm than that. To satisfy this latter requirement, the random vector \u03b7 t must be large; hence, it will \"anti-concentrate more\". As the algorithm progresses, and -thanks to side-information measurements-the set D s  221  222  223  224  225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272  273  274 Safe Linear Thompson Sampling with Side Information becomes an increasingly better approximation of D s 0 , the requirements on anti-concentration of \u03b7 t become the same as if no safety constraints were present. Overall, at least intuitively, we might hope that optimism is possible in the face of safety, but only provided that \u03b7 t is set to satisfy a stronger (at least at the first rounds) anti-concentration property than that required by (Abeille et al., 2017) in the classical setting.\n\nAt the heart of Algorithm 1 and its proof of regret lies an analytic argument that materializes the intuition described above. Specifically, we will prove that optimism is possible in the presence of safety at the cost of a stricter anti-concentration property compared to that specified in (Abeille et al., 2017). While the proof of this fact is deferred to Section 3.1, we now summarize the appropriate distributional properties that provably guarantee good regret performance of Algorithm 1 in the safe setting.\n\nDefinition 2.1. In Algorithm 1, the random vector \u03b7 t is sampled IID at each round t from a multivariate distribution H TS on R d that is absolutely continuous with respect to the Lebesgue measure and satisfies the following properties: Anti-concentration: There exists a strictly positive probability p > 0 such that for any u \u2208 R d with u 2 = 1,\nP u \u03b7 t \u2265 1 + 2 C LS \u2265 p.(15)\nConcentration: There exists positive constants c, c > 0 such that \u2200\u03b4 \u2208 (0, 1),\nP \u03b7 t 2 \u2264 1 + 2 C LS cd log ( c d \u03b4 ) \u2265 1 \u2212 \u03b4. (16)\nIn particular, the difference to the distributional assumptions required by (Abeille et al., 2017) in the classical setting is the extra term 2 C LS in (15) (naturally, the same term affects the concentration property (16)).\n\nOur proof of regret in Section 3 shows that this extra term captures an appropriate notion of the distance between the approximation D s t (where x t lives) and the true safe set D s 0 (where x lives), and provides enough exploration for the sampled parameter\u03b8 t so that actions in D s t can be optimistic. While this intuition can possibly explain the need for an additive term in Definition 2.1, it is insufficient when it comes to determining what the \"correct\" value for it should be. This is determined by our analytic treatment in Section 3.1.\n\nAs a closing remark of this section, note that the properties in Definition 2.1 are satisfied by a number of easy-to-sample from distributions. For instance, it is satisfied by a multivariate zero-mean IID Gaussian distribution with all entries having a (possibly time-dependent) variance 1 + 2 C LS.\n\n\nRegret Analysis\n\nIn this section, we present our main result, a tight regret bound for Safe-LTS, and discuss key proof ideas.\n\nSince \u00b5 is unknown, the learner does not know the safe action set D s 0 . Therefore, in order to satisfy the safety constraint (2), Algorithm 1 chooses actions from D s t , which is a conservative inner approximation of D s 0 . Moreover, at the heart of the action selection rule, is the sampling of an appropriate random perturbation\u03b8 t of the RLS estimat\u00ea \u03b8 t . The sampling rule in (10) is almost the same as in the classical setting (Abeille et al., 2017), but with a key difference on the distribution H T S of the perturbation vector \u03b7 t (cf. Definition 2.1). As explained in Section 3.1, the modification compared to (Abeille et al., 2017) is necessary to guarantee that actions are frequently optimistic (see (24)) in spite of limitations on actions imposed because of the safety constraints. In this section, we put these pieces together by proving that the action selection rule of Safe-LTS is simultaneously: 1) frequently optimistic, and, 2) guarantees a proper expansion of the estimated safe set. Our main result stated as Theorem 3.1 is perhaps surprising: in spite of the additional safety constraints, Safe-LTS has regret O( \u221a T log 3 2 T ) that is order-wise the same as that in the classical setting (Agrawal & Goyal, 2013;Abeille et al., 2017).\n\nTheorem 3.1 (Regret of Safe-LTS). Let \u03bb \u2265 1. Under Assumptions 1, 2, 3, the regret R(T ) of Safe-LTS Algorithm 1 is upper bounded with probability 1 \u2212 \u03b4 as follows:\nR(T ) \u2264 \u03b2 T (\u03b4 ) + \u03b3 T (\u03b4 )(1 + 4 p ) 2T d log (1 + T L 2 \u03bb ) + 4\u03b3 T (\u03b4 ) p 8T L 2 \u03bb log 4 \u03b4 ,(17)\nwhere \u03b4 = \u03b4 6T , \u03b2 t (\u03b4 ) as in (9) and, \u03b3 t (\u03b4 ) = \u03b2 t (\u03b4 ) 1 + 2 C LS cd log ( c d \u03b4 ) .\n\nA detailed proof of Theorem 3.1 is deferred to Appendix E. In the rest of the section, we highlight the key changes compared to previous proofs in (Agrawal & Goyal, 2013;Abeille et al., 2017) that occur due to the safety constraint.\n\nTo begin, let us consider the following standard decomposition of the cumulative regret R(T ):\nT t=1 x \u03b8 \u2212 x t\u03b8t Term I + T t=1 x t\u03b8t \u2212 x t \u03b8 Term II .(18)\nRegarding Term II, the concentration property of H T S guarantees that\u03b8 t is close to\u03b8 t , and consequently, close to \u03b8 thanks to Theorem 2.1. Therefore, controlling Term II can be done similar to previous works e.g., (Abbasi-Yadkori et al., 2011;Abeille et al., 2017); see Appendix E.2 for more details. Next, we focus on Term I.\n\nTo see how the safety constraints affect the proofs let us first review the treatment of Term I in the classical setting. For UCB-type algorithms, Term I is always non-positive since the pair (\u03b8 t , x t ) is optimistic at each round t by design (Dani et al., 2008;Rusmevichientong & Tsitsiklis, 2010;Abbasi-Yadkori et al., 2011). For LTS, Term I can be positive; that is, (11) may not hold at every round t. However, (Agrawal & Goyal, 2013;Abeille et al., 2017) proved that thanks to the anti-concentration property of \u03b7 t , this optimistic property occurs often enough. Moreover, this is enough to yield a good enough bound on Term I for every round t; see Appendix A.\n\nAs discussed in Section 2.2.2, the requirement for safety complicates the requirement for optimism. Our main technical contribution, detailed in the next section, is to show that the properly modified anti-concentration property in Definition 2.1 together with the construction of approximated safe sets as in (14) can yield frequently optimistic actions even in the face of safety. Specifically, it is the extra term 2 C LS in (15) that allows enough exploration to the sampled parameter\u03b8 t in order to compensate for safety limitations on the chosen actions, and because of that we are able to show Safe-LTS obtains the same order of regret as that of (Abeille et al., 2017).\n\n\nProof sketch: Optimism despite safety constraints\n\nWe prove that Safe-LTS samples a parameter\u03b8 t that is optimistic with constant probability. The next lemma informally characterizes this claim (see the formal statement of the lemma and its proof in Appendix D). Lemma 3.2. (Optimism in the face of safety; Informal) For any round t \u2265 1, Safe-LTS samples a parameter\u03b8 t and chooses an action x t such that the pair (\u03b8 t , x t ) is optimistic frequently enough, i.e.,\nP x t\u03b8t \u2265 x \u03b8 \u2265 p,(19)\nwhere p > 0 is the probability with which the anticoncentration property (15) holds.\n\nThe challenge in the proof is that the actions x t are chosen from the estimated safe set D s t , which does not necessarily contain all feasible actions and hence, may not contain x . Therefore, we need a mechanism to control the distance of the optimal action x from the optimistic actions that can only lie within the subset D s t (distance is defined here in terms of an inner product with the optimistic parameters \u03b8 t ). Unfortunately, we do not have a direct control on this distance term and so at the heart of the proof lies the idea of identifying a \"good\" feasible actionx t \u2208 D s t whose distance to x is easier to control.\n\nTo be concrete, we show that it suffices to choose the good feasible point in the direction of x , i.e.,x t = \u03b1 t x , where the key parameter \u03b1 t \u2208 (0, 1] must be set to satisfyx t \u2208 D s t . Naturally, the value of \u03b1 t is determined by the approximated safe set D s t as defined in (14). The challenge though is that we do not know how the value of x * \u03bct compares to the constant C. We circumvent this issue by introducing an enlarged confidence region centered at \u00b5 as\nC t := {v \u2208 R d : v \u2212 \u00b5 Vt \u2264 2\u03b2 t (\u03b4 )},\nand the corresponding shrunk safe decision set as\nD s t := {x \u2208 D 0 : x v \u2264 C, \u2200v \u2208C t } (20) = {x \u2208 D 0 : x \u00b5 + 2\u03b2 t (\u03b4 ) x V \u22121 t \u2264 C} \u2286 D s t .\nNotice that the shrunk safe set is defined with respect to an ellipsoid centered at \u00b5 (rather than at\u03bc t ). This is convenient since x \u00b5 \u2264 C. Using this, it can be easily checked that the following choice of \u03b1 t :\n\u03b1 t = 1 + 2 C \u03b2 t (\u03b4 ) x V \u22121 t \u22121 ,(21)\nensures that \u03b1 t x \u2208D s t \u2286 D s t . From this, and optimality of x t = arg max x\u2208D s t x \u03b8 t we have that:\nx t\u03b8t \u2265 \u03b1 t x \u03b8 t .(22)\nNext, using (22), in order to show that (19) holds, it suffices to prove that\np \u2264 P \u03b1 t x \u03b8 t \u2265 x \u03b8 = P x \u03b8 t \u2265 x \u03b8 + 2 C \u03b2 t (\u03b4 ) x V \u22121 t x \u03b8 ,\nwhere, in the second line, we used the definition of \u03b1 t in (37). To continue, recall that\u03b8 t =\u03b8 t + \u03b2 t V \u2212 1 2 t \u03b7 t . Thus, we want to lower bound the probability of the following event:\n\u03b2 t (\u03b4 )x V \u2212 1 2 t \u03b7 t \u2265 x (\u03b8 \u2212\u03b8 t ) + 2 C \u03b2 t (\u03b4 ) x V \u22121 t x \u03b8\nTo simplify the above, we use the following two facts:\n(i) |x \u03b8 | \u2264 x 2 \u03b8 2 \u2264 LS; (ii) x (\u03b8 \u2212\u03b8 t ) \u2264 x V \u22121 t \u03b8 \u2212\u03b8 t Vt \u2264 \u03b2 t (\u03b4 ) x V \u22121 t\n, because of Cauchy-Schwartz and Theorem 2.1 Put together, we need that\np \u2264 P \u03b2 t (\u03b4 )x V \u2212 1 2 t \u03b7 t \u2265\u03b2 t (\u03b4 ) x V \u22121 t + 2 C LS\u03b2 t (\u03b4 ) x V \u22121 t ,\nor equivalently, \np \u2264 P u t \u03b7 t \u2265 1 + 2 C LS ,(23)= V \u2212 1 2 t xt x V \u22121 t . By definition of u t ,\nnote that u t 2 = 1. Hence, the desired (23) holds due to the anti-concentration property of the H TS distribution in (15). This completes the proof of Lemma 3.2.\n\nBefore closing, we remark on the following differences to the proof of optimism in the classical setting as presented in Lemma 3 of (Abeille et al., 2017). First, we present an algebraic version of the basic machinery introduced in Section 5 of (Abeille et al., 2017) that we show is convenient to extend to the safe setting. Second, we employ the idea of relating x t to a \"better\" feasible point \u03b1 t x t and show optimism for the latter. Third, even after introducing \u03b1 t , the fact that 1/\u03b1 t \u2212 1 is proportional to x V \u22121 t (see (37)) is critical for the seemingly simple algebraic steps that follow (38). In particular, in deducing (23) from the expression above, note that we have divided both sides in the probability term by x Vt\u22121 . It is only thanks to the proportionality observation that we made above that the term x Vt\u22121 cancels throughout and we can conclude with (23) without a need to lower bound the minimum eigenvalue of the Gram matrix V t (which is known to be hard).\n\n\nNumerical Results and Comparison to State of the Art\n\nWe present details of our numerical experiments on synthetic data. First, we show how the presence of safety constraints affects the performance of LTS in terms of regret. Unless otherwise specified, the reward and constraint parameters \u03b8 and \u00b5 are drawn from N (0, I 4 ); C is drawn uniformly from [0, 1]. Throughout, we have implemented a modified version of Safe-LUCB which uses 1 -norms instead of 2 -norms, due to computational considerations (e.g., (Dani et al., 2008;Amani et al., 2019)). Recall that the action selection rule of UCB-based algorithms involves solving bilinear optimization problems, whereas, TS-based algorithms (such as the one proposed here) involve simple linear objectives (see (Abeille et al., 2017)).\n\n\nThe effect of safety constraints on LTS\n\nIn Fig. 1 we compare the average cumulative regret of Safe-LTS to the standard LTS algorithm with oracle access to the true safe set D s 0 . The results are averages over 20 problem realizations. As shown, even though Safe-LTS requires that chosen actions belong to the conservative innerapproximation set D s t , it still achieves a regret of the same order as the oracle reaffirming the prediction of Theorem 3.1. Also, the comparison to the oracle reveals that the action selection rule of Safe-LTS is indeed such that it guarantees a fast expansion of the estimated safe set so as to not exclude optimistic actions for a long time. Fig. 1 also shows the performance of a third algorithm discussed in Sec. 4.4.\n\n\nComparison to safe versions of LUCB\n\nHere, we compare the performance of our algorithm with two safe versions of LUCB, as follows. First, we implement a natural extension of the classical LUCB algorithm in (Dani et al., 2008), which we call \"Naive Safe-LUCB\" and which respects safety constraints by choosing actions from the estimated safe set in (12). Second, we consider an improved version, which we call \"Inflated Naive Safe-LUCB\" and which is motivated by our analysis of Safe-LTS. Specifically, in light of Lemma 3.2, we implement the improved LUCB algorithm with an inflated confidence ellipsoid by a fraction 1 + 2 C LS in order to favor optimistic exploration. In Fig.  2, we employ these two algorithms for a specific problem instance (details in Appendix F) showing that both fail to provide the O( \u221a T ) regret of Safe-LTS, in general. Further numerical simulations suggest that while Safe-LTS always outperforms Naive Safe-LUCB, the Inflated Naive Safe-LUCB can have superior performance to Safe-LTS in many problem instances (see Fig. 6 in Appendix F). Unfortunately, not only is this not always the case (cf. Fig. 2), but also we are not aware of an appropriate modification to our proofs to show this problem-dependent performance. This being said further investigations in this direction might be of interest.\n\n\nComparison to Safe-LUCB\n\nIn this section we compare our algorithm and results to the Safe-LUCB algorithm of (Amani et al., 2019) that was proposed for a similar, but non-identical setting. Specifically, in (Amani et al., 2019), the linear safety constraint involves the same unknown parameter vector \u03b8 of the linear reward function and -in our notation-it takes the form x B\u03b8 \u2264 C, for some known matrix B. As such, no side-information measurements are needed. First, while our proof does not show a regret of O( \u221a T ) for the setting of (Amani et al., 2019) in the general case, it does so for special cases. For example, it is not hard to see that our proofs readily extend to their setting when B = I. This already improves upon the O(T 2/3 ) guarantee provided by (Amani et al., 2019). Indeed, for B = I, there are non-trivial instances where C \u2212 x * \u03b8 * = 0 (i.e., the safety constraint is active), in which Safe-LUCB suffers from a O(T 2/3 ) bound (Amani et al., 2019). Second, while our proof adapts to a special case of (Amani et al., 2019)'s setting, the other way around is not true, i.e., it is not obvious how one would modify the proof of (Amani et al., 2019) to obtain a O( \u221a T ) guarantee even in the presence of side information. This point is highlighted by Fig. 3 that numerically compares the two algorithms for a specific problem instance with side information: \u03b8 * = [0.9, 0.23] , \u00b5 * = [0.55, 0.31] T , and C = 0.11 (note that the constraint is active at the optimal). Also, see Section F for a numerical comparison of the estimated safe-sets' expansion for the two algorithms. Fig. 4 compares Safe-LTS against Safe-LUCB and Naive Safe-LUCB over 30 problem realizations (see Section F in Appendix for plots with standard deviation). As already pointed out in (Amani et al., 2019), Naive Safe-LUCB generally leads to poor regret, since the LUCB action selection rule alone does not provide sufficient exploration towards safe set expansion. In contrast, Safe-LUCB is equipped with a pure exploration phase over a given seed safe set, which is shown to lead to proper safe set expansion. Our paper reveals that the inherent randomized nature of Safe-LTS is alone capable to properly expand the safe set without the need for an explicit initialization phase (during which regret grows linearly).\n\n\nSampling from a dynamic noise distribution\n\nIn order for Safe-LTS to be frequently optimistic, our theory requires that the random perturbation \u03b7 t satisfies (15) for all rounds. Specifically, we need the extra 2 C LS factor compared to (Abeille et al., 2017) in order to ensure safe set expansion. While this result is already sufficient for the tight regret guarantees of Theorem 3.1, it does not fully capture our intuition (see also Sec. 2.2.2) that as the algorithm progresses and D s t gets closer to D s 0 , exploration (and thus, the requirement on anti-concentration) does not need to be so aggressive. Based on this intuition, we propose the following heuristic modification, in which Safe-LTS uses a perturbation with the following dynamic property: P \u03b7\u223cH TS u \u03b7 \u2265 k(t) \u2265 p, for k(t) a linearly-decreasing function k(t) = (1 + 2 C LS)(1 \u2212 t/T ). Fig. 1 shows empirical evidence of the superiority of the heuristic.\n\n\nConclusion\n\nWe studied LB in which the environment is subject to unknown linear safety constraints that need to be satisfied at each round. For this problem, we proposed Safe-LTS, which to the best of our knowledge, is the first safe TS algorithm with provable regret guarantees. Importantly, we show that Safe-LTS achieves regret of the same order as in the original setting with no safety constraints. We have also compared Safe-LTS with several UCB-type safe algorithms showing that the former has: better regret in the worst-case, fewer parameters to tune and often superior empirical performance. Interesting directions for future work include: theoretically studying the dynamic property of Section 4.4, as well as, investigating TS-based alternatives to the GP-UCB-type algorithms of (Sui et al., 2015;. 496  497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512  513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528  529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544  545  546  547 548 549  553  554  555  556  557  558  559  560  561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576  577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592  593  594  595  596  597  598  599  600  601  602  603  604 Safe Linear Thompson Sampling with Side Information A. Proof sketch: Why frequent optimism is enough to bound Term I As discussed in Section 3, the presence of the safety constraints complicates the requirement for optimism. We show in Section 3.1 that Safe-LTS is optimistic with constant probability in spite of safety constraints. Based on this, we complete the sketch of the proof here by showing that we can bound the overall regret of Term I in (18) with the V \u03c4 -norm of optimistic (and in our case, safe) actions. Let us first define the set of the optimistic parameters as\n\u0398 opt t (\u03b4 ) = {\u03b8 \u2208 R d : max x\u2208D s t x \u03b8 \u2265 x \u03b8 }.(24)\nIn Section 3.1, we show that Safe-LTS samples from this set i.e.,\u03b8 t \u2208 \u0398 opt t , with constant probability. Note that, if at round t Safe-LTS samples from the set of optimistic parameters, Term I at that round is non-positive. In the following, we show that selecting the optimal arm corresponding to any optimistic parameter can control the overall regret of Term I. The argument below is adapted from (Abeille et al., 2017) with minimal changes; it is presented here for completeness.\n\nFor the purpose of this proof sketch, we assume that at each round t, the safe decision set contains the previous safe action that the algorithm played, i.e., x t\u22121 \u2208 D s t . However, for the formal proof in Appendix E.1, we do not need such an assumption. Let \u03c4 be a time such that\u03b8 \u03c4 \u2208 \u0398 opt t , i.e., x \u03c4\u03b8\u03c4 \u2265 x \u03b8 . Then, for any t \u2265 \u03c4 we have\nTerm I := R TS t = x \u03b8 \u2212x t\u03b8t \u2264 x \u03c4\u03b8\u03c4 \u2212 x t\u03b8t \u2264 x \u03c4 \u03b8 \u03c4 \u2212\u03b8 t .(25)\nThe last inequality comes from the assumption that at each round t, the safe decision set contains the previous played safe actions for rounds s \u2264 t; hence, x \u03c4\u03b8t \u2264 x t\u03b8t . To continue from (25), we use Cauchy-Schwarz, and obtain\nR TS t \u2264 x \u03c4 V \u22121 \u03c4 \u03b8 \u03c4 \u2212\u03b8 t V\u03c4 \u2264 \u03b8 \u03c4 \u2212 \u03b8 V\u03c4 + \u03b8 \u2212\u03b8 t V\u03c4 x \u03c4 V \u22121 \u03c4 . \u2264 \u03b8 \u03c4 \u2212 \u03b8 V\u03c4 + \u03b8 \u2212\u03b8 t Vt x \u03c4 V \u22121 \u03c4(26)\nThe last inequality comes from the fact that the Gram matrices construct a non-decreasing sequence (V \u03c4 V t , \u2200t \u2265 \u03c4 ). Then, we define the ellipsoid E TS t (\u03b4 ) such that\nE TS t (\u03b4 ) := {\u03b8 \u2208 R d : \u03b8 \u2212\u03b8 t Vt \u2264 \u03b3 t (\u03b4 )},(27)\nwhere\n\u03b3 t (\u03b4 ) = \u03b2 t (\u03b4 ) 1 + 2 C LS cd log ( c d \u03b4 ).(28)\nIt is not hard to see by combining Theorem 2.1 and the concentration property that\u03b8 t \u2208 E TS t (\u03b4 ) with high probability. Hence, we can bound (26) using triangular inequality such that:\nR TS t \u2264 \u03b3 \u03c4 (\u03b4 ) + \u03b2 \u03c4 (\u03b4 ) + \u03b3 t (\u03b4 ) + \u03b2 t (\u03b4 ) x \u03c4 V \u22121 \u03c4 (29) \u2264 2 \u03b3 T (\u03b4 ) + \u03b2 T (\u03b4 ) x \u03c4 V \u22121 \u03c4(30)\nThe last inequality comes from the fact that \u03b2 t (\u03b4 ) and \u03b3 t (\u03b4 ) are non-decreasing in t by construction. Therefore, following the intuition of (Abeille et al., 2017), we can upper bound Term I with respect to the V \u03c4 -norm of the optimal safe action at time \u03c4 (see Section E.1 in Appendix for formal proof). Bounding the term x \u03c4 V \u22121 \u03c4 is standard based on the analysis provided in (Abbasi-Yadkori et al., 2011) (see Proposition B.1 in the Appendix).\n\n\nB. Useful Results\n\nThe following result is standard and plays an important role in most proofs for linear bandits problems.\n\nProposition B.1. ((Abbasi-Yadkori et al., 2011)) Let \u03bb \u2265 1. For any arbitrary sequence of actions (x 1 , . . . , x t ) \u2208 D t , let V t be the corresponding Gram matrix (6), then\nt s=1 x s 2 V \u22121 s \u2264 2 log det(V t+1 ) det(\u03bbI) \u2264 2d log (1 + tL 2 \u03bb ).(31)\nIn particular, we have\nT s=1 x s V \u22121 s \u2264 \u221a T T s=1 x s 2 V \u22121 s 1 2 \u2264 2T d log 1 + T L 2 \u03bb .(32)\nAlso, we recall the Azuma's concentration inequality for super-martingales.\n\nProposition B.2. (Azuma's inequality (Boucheron et al., 2013)) If a super-martingale (Y t ) t\u22650 corresponding to a filtration F t satisfies |Y t \u2212 Y t\u22121 | < c t for some positive constant c t , for all t = 1, . . . , T , then, for any u > 0,\nP (Y T \u2212 Y 0 \u2265 u) \u2264 2e \u2212 u 2 2 T t=1 c 2 t .(33)\n\nC. Confidence Regions\n\nWe start by constructing the following confidence regions for the RLS-estimates.\n\nDefinition C.1. Let \u03b4 \u2208 (0, 1), \u03b4 = \u03b4 6T , and t \u2208 [T ]. We define the following events:\n\n\u2022\u00ca t is the event that the RLS-estimate\u03b8 concentrates around \u03b8 for all steps s \u2264 t, i.e.,\u00ca t = {\u2200s \u2264 t, \u03b8 s \u2212 \u03b8 Vs \u2264 \u03b2 s (\u03b4 )};\n\n\u2022\u1e90 t is the event that the RLS-estimate\u03bc concentrates around \u00b5 , i.e.,\u1e90 t = {\u2200s \u2264 t, \u03bc s \u2212 \u00b5 Vs \u2264 \u03b2 s (\u03b4 )}. Moreover, define Z t such that Z t =\u00ca t \u2229\u1e90 t .\n\n\u2022\u1ebc t is the event that the sampled parameter\u03b8 t concentrates around\u03b8 t for all steps s \u2264 t, i.e.,\u1ebc t = {\u2200s \u2264 t, \u03b8 s \u2212\u03b8 s\nVs \u2264 \u03b3 s (\u03b4 )}. Let E t be such that E t =\u1ebc t \u2229 Z t .\nLemma C.1. Under Assumptions 1, 2, we have P(Z) = P(\u00ca \u2229\u1e90) \u2265 1 \u2212 \u03b4 3 where\u00ca =\u00ca T \u2282 \u00b7 \u00b7 \u00b7 \u2282\u00ca 1 , and Z =\u1e90 T \u2282 \u00b7 \u00b7 \u00b7 \u2282\u1e90 1 .\n\nProof. The proof is similar to the one in Lemma 1 of (Abeille et al., 2017) and is ommited for brevity.\nLemma C.2. Under Assumptions 1, 2, we have P(E) = P(\u1ebc \u2229 Z) \u2265 1 \u2212 \u03b4 2 , where\u1ebc =\u1ebc T \u2282 \u00b7 \u00b7 \u00b7 \u2282\u1ebc 1 .\nProof. We show that P(\u1ebc) \u2265 1 \u2212 \u03b4 6 . Then, from Lemma C.1 we know that P(Z) \u2265 1 \u2212 \u03b4 3 , thus we can conclude that P(E) \u2265 1 \u2212 \u03b4 2 . Bounding\u1ebc comes directly from concentration inequality (16). Specifically,\n1 \u2264 t \u2264 T, P \u03b8 t \u2212\u03b8 t Vt \u2264 \u03b3 t (\u03b4 ) = P \u03b7 t 2 \u2264 \u03b3 t (\u03b4 ) \u03b2 t (\u03b4 ) = P \u03b7 t 2 \u2264 1 + 2 C LS cd log ( c d \u03b4 ) \u2265 1 \u2212 \u03b4 .\nApplying union bound on this ensures that P(\u1ebc) \u2265 1 \u2212 T \u03b4 = 1 \u2212 \u03b4 6 .\n\nSince D 0 is convex by Assumption 3 and both 0, x \u2208 D 0 , we have\n\u03b1 t = max \u03b1 \u2208 [0, 1] : \u03b1 x \u00b5 + 2\u03b2 t (\u03b4 ) x V \u22121 t \u2264 C .(37)\nFrom constraint (2), we know that x \u00b5 \u2264 C. We choose \u03b1 t such that\n1 + 2 C \u03b2 t (\u03b4 ) x V \u22121 t = 1 \u03b1 t .(38)\nWe need to study the probability that a sampled\u03b8 t drawn from H TS distribution at round t is optimistic, i.e.,\np t = P (x t (\u03b8 t )) \u03b8 t \u2265 x \u03b8 F t , Z t .\nUsing the definition of \u03b1 t in (37), we have\n(x t (\u03b8 t )) \u03b8 t = max x\u2208D s t x \u03b8 t \u2265 \u03b1 t x \u03b8 t .(39)\nHence, we can write\np t \u2265P \u03b1 t x \u03b8 t \u2265 x \u03b8 F t , Z t = P x \u03b8 t + \u03b2 t (\u03b4 )V \u2212 1 2 t \u03b7 t \u2265 x \u03b8 \u03b1 t F t , Z t\nThen, we use the value that we chose for \u03b1 t in (38), and we have\n= P x \u03b8 t + \u03b2 t (\u03b4 )x V \u2212 1 2 t \u03b7 t \u2265 x \u03b8 + 2 C \u03b2 t (\u03b4 ) x V \u22121 t x \u03b8 |F t , Z t we know that |x \u03b8 | \u2264 x 2 \u03b8 2 \u2264 LS. Hence, p t \u2265 P \u03b2 t (\u03b4 )x V \u2212 1 2 t \u03b7 t \u2265 x (\u03b8 \u2212\u03b8 t ) + 2 C LS\u03b2 t (\u03b4 ) x V \u22121 t | F t , Z t\nFrom Cauchy-Schwarz inequality and (7), we have\n| x \u03b8 \u2212\u03b8 t |\u2264 x V \u22121 t \u03b8 \u2212\u03b8 t Vt \u2264 \u03b2 t (\u03b4 ) x V \u22121 t .\nTherefore, we can write\np t \u2265 P x V \u2212 1 2 t \u03b7 t \u2265 x V \u22121 t + 2 C LS x V \u22121 t | F t , Z t(40)We define u = x V \u2212 1 2 t x V \u22121 t\n, and hence u 2 = 1. It follows from (40) that\np t \u2265 P u \u03b7 t \u2265 1 + 2 C LS \u2265 p,(41)\nwhere the last inequality follows the concentration inequality (16) of the TS distribution. We also need to show that the high probability concentration inequality event does not effect the TS of being optimistic. This is because the chosen confidence bound \u03b4 = \u03b4 6T is small enough compared to the anti-concentration property (15). Moreover, we assume that T \u2265 1 3p which implies that \u03b4 \u2264 p 2 . We know that for any events A ans B, we have\nP(A \u2229 B) = 1 \u2212 P(A c \u222a B c ) \u2265 P(A) \u2212 P(B c ).(42)\nWe apply (42) \nwith A = {J t (\u03b8 t ) \u2265 J(\u03b8 )} and B = {\u03b8 t \u2208 E TS t } that leads to P \u03b8 t \u2208 \u0398 opt t F t , Z t \u2265 p \u2212 \u03b4 \u2265 p 2 .\nE. Proof of Theorem 3.1\n\nThe proof presented below follows closely the proof of (Abeille et al., 2017) and is primarily presented here for completeness. Specifically, we have identified that the only critical change that needs to be made to account for safety is the proof of actions being frequently optimistic in the face of constraints thanks to the modified anti-concentration property 15. This was handled in the previous section D. For completeness, we also prove in Lemma E.1 that the first action of Safe-LTS is always safe under our assumptions.\n\nWe use the following decomposition for bounding the regret:\nR(T ) \u2264 T t=1 x \u03b8 \u2212 x t \u03b8 1{E t } = T t=1 \uf8eb \uf8ed x \u03b8 \u2212 x t\u03b8t Term I \uf8f6 \uf8f8 1{E t } + T t=1 \uf8eb \uf8ed x t\u03b8t \u2212 x t \u03b8 Term II \uf8f6 \uf8f8 1{E t }.(43)\nE.1. Bounding Term I.\n\nFor any \u03b8, we denote x t (\u03b8) = arg max x\u2208D s 772  773  774  775  776  777  778  779  780  781  782  783  784  785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800  801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816  817  818  819  820  821  822  823  824 Safe Linear Thompson Sampling with Side Information\n\nHere and onwards, we use 1{E} as the indicator function applied to an event E. We have also used the fact that E t is a subset of Z t . Next, we can also bound (44) by the expectation over any random choice of\u03b8 \u2208 \u0398 opt t (recall (24)) that leads to\nR TS t \u2264 E (x t (\u03b8)) \u03b8 \u2212 inf \u03b8\u2208E TS t (x t (\u03b8)) \u03b8 1{Z t } F t ,\u03b8 \u2208 \u0398 opt t .\nEquivalently, we can write\nR TS t \u2264 E sup \u03b8\u2208E TS t x t (\u03b8) \u03b8 \u2212 x t (\u03b8) \u03b8 1{Z t } F t ,\u03b8 \u2208 \u0398 opt t ,(45)\nThen, using Cauchy-Schwarz and the definition of \u03b3 t (\u03b4 ) in (28) E sup\n\u03b8\u2208E TS t x t (\u03b8) \u03b8 \u2212 \u03b8 1{Z t } F t ,\u03b8 \u2208 \u0398 opt t \u2264 E x t (\u03b8) V \u22121 t sup \u03b8\u2208E TS t \u03b8 \u2212 \u03b8 Vt F t ,\u03b8 \u2208 \u0398 opt t , Z t P(Z t ) \u2264 2\u03b3 t (\u03b4 )E x t (\u03b8) V \u22121 t F t ,\u03b8 \u2208 \u0398 opt t , Z t P(Z t ).\nThis property shows that the regret R TS t is upper bounded by V \u22121 t -norm of the optimal safe action corresponding to the any optimistic parameter\u03b8. Hence, we need to show that TS samples from the optimistic set with high frequency. We prove in Lemma 3.2 that TS is optimistic with a fixed probability ( p 2 ) which leads to bounding R TS t as follows:\nR TS t p 2 \u2264 2\u03b3 t (\u03b4 )E x t (\u03b8 t ) V \u22121 t F t ,\u03b8 t \u2208 \u0398 opt t , Z t P(Z t ) p 2 (46) \u2264 2\u03b3 t (\u03b4 )E x t (\u03b8 t ) V \u22121 t F t ,\u03b8 t \u2208 \u0398 opt t , Z t P(Z t )P \u03b8 t \u2208 \u0398 opt t F t , Z t \u2264 2\u03b3 t (\u03b4 )E x t (\u03b8 t ) V \u22121 t F t , Z t P(Z t ).(47)\nBy reintegrating over the event Z t we get\nR TS t \u2264 4\u03b3 t (\u03b4 ) p E x t (\u03b8 t ) V \u22121 t 1{Z t } F t .(48)\nRecall that E t \u2282 Z t , hence\nR TS (T ) \u2264 T t=1 R TS t 1{E t } \u2264 4\u03b3 T (\u03b4 ) p T t=1 E x t (\u03b8 t ) V \u22121 t F t .\nFor bounding this term, we rewrite the RHS above as:\nR TS (T ) \u2264 T t=1 x t V \u22121 t + T t=1 E x t (\u03b8 t ) V \u22121 t F t \u2212 x t V \u22121 t .(49)\nWe can now bound the first expression using Proposition B.1. For the second expression we proceed as follows:\n\n\u2022 First, the sequence a martingale by construction.  827  828  829  830  831  832  833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848  849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864  865  866  867  868  869  870  871  872  873  874  875  876  877  878  879 Safe Linear Thompson Sampling with Side Information \u2022 Second, under Assumption 3, x t 2 \u2264 L, and since V \u22121 t \u2264 1 \u03bb I, we can write\nY t = t s=1 E x s (\u03b8 s ) V \u22121 s F s \u2212 x s V \u22121 s isE x s (\u03b8 s ) V \u22121 s F s \u2212 x s V \u22121 s \u2264 2L \u221a \u03bb , \u2200t \u2265 1.(50)\n\u2022 Third, for bounding Y T , we use Azuma's inequality, and we have that with probability 1 \u2212 \u03b4 2 ,\nY T \u2264 8T L 2 \u03bb log 4 \u03b4 .(51)\nPutting these together, we conclude that with probability 1 \u2212 \u03b4 2 ,\nR TS (T ) \u2264 4\u03b3 T (\u03b4 ) p 2T d log 1 + T L 2 \u03bb + 8T L 2 \u03bb log 4 \u03b4 .(52)\n\nE.2. Bounding Term II\n\nWe can bound on Term II using the general result of (Abbasi-Yadkori et al., 2011). In fact, we can use the following general decomposition:\nT t=1 (Term II)1{E t } := R RLS (T ) = T t=1 x t\u03b8t \u2212 x t \u03b8 1{E t } \u2264 T t=1 | x t (\u03b8 t \u2212\u03b8 t ) | 1{E t } + T t=1 | x t (\u03b8 t \u2212 \u03b8 ) | 1{E t }.(53)\nBy Definition C.1, we have E t \u2286 Z t and E t \u2286\u1ebc t , and hence\n| x t (\u03b8 t \u2212\u03b8 t ) | 1{E t } \u2264 x V \u22121 t \u03b3 t (\u03b4 ) | x t (\u03b8 t \u2212 \u03b8 ) | 1{E t } \u2264 x V \u22121 t \u03b2 t (\u03b4 ).\nTherefore, from Proposition B.1, we have with probability 1 \u2212 \u03b4 2 R RLS (T ) \u2264 (\u03b2 T (\u03b4 ) + \u03b3 T (\u03b4 )) 2T d log 1 + T L 2 \u03bb .\n\n(54)\n\n\nE.3. Overall Regret Bound\n\nRecall that from (18), R(T ) \u2264 R T S (T ) + R RLS (T ). As shown previously, each term is bounded separately with probability 1 \u2212 \u03b4 2 . Using union bound over two terms, we get the following expression:\nR(T ) \u2264 (\u03b2 T (\u03b4 ) + \u03b3 T (\u03b4 )(1 + 4 p )) 2T d log (1 + T L 2 \u03bb ) + 4\u03b3 T (\u03b4 ) p 8T L 2 \u03bb log 4 \u03b4 ,(55)\nholds with probability 1 \u2212 \u03b4 where \u03b4 = \u03b4 6T . For completeness we show below that action x 1 is safe. Having established that, it follows that the rest of the actions x t , t > 1 are also safe with probability at least 1 \u2212 \u03b4 . This is by construction of the feasible sets D s t and by the fact that \u00b5 \u2208 C t (\u03b4 ) with the same probability for each t. Lemma E.1. The first action that Safe-LTS chooses is safe, that is x 1 \u00b5 \u2264 C.\n\nProof. At round t = 1, the RLS-estimate\u03bc 1 = 0 and V 1 = \u03bbI. Thus, Safe-LTS chooses the action which maximizes the expected reward while satisfying x 1\u03bc1 + \u03b2 1 (\u03b4 ) x 1 V \u22121 1 \u2264 C. Hence, x 1 satisfies:\n\u03b2 1 (\u03b4 ) x 1 V \u22121 1 \u2264 C.\nFrom (9) and V \u22121 1 = (1/\u03bb)I leads to S x 1 2 \u2264 C which completes the proof.   Figure 6. Comparison of the cumulative regret of Safe-LTS and Naive Safe-LUCB and Inflated Naive Safe-LUCB algorithms over randomly generated instances.\n\n\nF. More on experimental results\n\nStandard deviations. Figure 5 shows the sample standard deviation of regret around the average per-step regret for each one of the curves depicted in Figure 4. We remark on the strong dependency of the performance of LUCB-based algorithms on the specific problem instance, whereas the performance of Safe-LTS does not vary significantly under the same instances.\n\nOn the \"Inflated Safe-LUCB\". Here, we provide further evidence on the performance of the \"Inflated Naive Safe-LUCB\" that we introduced in Section 4.2. Recall from Figure 2 that \"Inflated Safe-LUCB\" does not always lead to proper safe set expansion and hence good regret performance. Specifically, the regret curves in Figure 2 are for a problem instance with \u03b8 * = 0.5766 \u22120.1899 , \u00b5 * = 0.2138 \u22120.0020 , and C = 0.0615. While \"Inflated Naive Safe-LUCB\" does not achieve the logarithmic regret of Safe-LTS in general, our numerical simulations suggest that the former actually outperforms the latter in randomly generated instances. This is illustrated in Fig.6.\n\nOn safe-set expansion. Fig. 7 highlights the gradual expansion of the safe decision set for Safe-LUCB in (Amani et al., 2019) and Safe-LTS for a problem instance in which the safety constraint is active for parameters \u03b8 * = 0.9 0.23 , \u00b5 * = 0.55 \u22120.31 , and C = 0.11... Similarly, Fig. 8 illustrates the expansion of the safe decision set for \"Inflated Naive Safe-LUCB\" and Safe-LTS for a problem instance with parameters \u03b8 * = 0.5766 \u22120.1899 , \u00b5 * = 0.2138 \u22120.0020 , and C = 0.0615 in which the former provides poor (almost linear) regret. These empirical experiments reinforce the main message of our paper that the inherent randomized nature of TS is crucial for properly expanding the safe action set.  \n\n\nNext, we evaluate Safe-LTS by comparing it against safe versions of LUCB. Then, we compare Safe-LTS to (Amani et al., 2019)'s Safe-LUCB. In all the implementations, we used: T = 10000, \u03b4 = 1/4T , R = 0.1 and D 0 = [\u22121, 1] 4 .\n\nFigure 1 .Figure 2 .\n12Comparison of the average cumulative regret of Safe-LTS vs standard LTS with oracle access to the safe set and Safe-LTS with a dynamic noise distribution described in Section 4.The cumulative regret of Safe-LTS, Naive Safe-LUCB and Inflated Naive Safe-LUCB for a specific problem instance.\n\nFigure 3 .Figure 4 .\n34Comparison of regret of Safe-LUCB and Safe-LTS, for a single problem instance in which the safety constraint is active. Comparison of the average cumulative regret of Safe-LTS versus two safe LUCB algorithms.\n\nFigure 5 .\n5Comparison of mean per-step regret for Safe-LTS, Safe-LUCB, and Naive Safe-LUCB. The shaded regions show one standard deviation around the mean. The results are averages over 30 problem realizations.\n\nFigure 7 .\n7Comparison of expansion of a safe decision sets for Safe-LUCB and Safe-LTS, for a single problem instance.\n\nFigure 8 .\n8Comparison of expansion of safe decision sets for Safe-LTS, and Inflated Naive Safe-LUCB.\n\n\nSafe Linear Thompson Sampling with Side Informationwhere we have defined u t332 \n333 \n334 \n335 \n336 \n337 \n338 \n339 \n340 \n341 \n342 \n343 \n344 \n345 \n346 \n347 \n348 \n349 \n350 \n351 \n352 \n353 \n354 \n355 \n356 \n357 \n358 \n359 \n360 \n361 \n362 \n363 \n364 \n365 \n366 \n367 \n368 \n369 \n370 \n371 \n372 \n373 \n374 \n375 \n376 \n377 \n378 \n379 \n380 \n381 \n382 \n383 \n384 \n\n\n\n\nSafe Linear Thompson Sampling with Side Information882 \n883 \n884 \n885 \n886 \n887 \n888 \n889 \n890 \n891 \n892 \n893 \n894 \n895 \n896 \n897 \n898 \n899 \n900 \n901 \n902 \n903 \n904 \n905 \n906 \n907 \n908 \n909 \n910 \n911 \n912 \n913 \n914 \n915 \n916 \n917 \n918 \n919 \n920 \n921 \n922 \n923 \n924 \n925 \n926 \n927 \n928 \n929 \n930 \n931 \n932 \n933 \n934 \n\n\nAnonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Correspondence to: Anonymous Author <anon.email@domain.com>.\nt x \u03b8. On the event E t ,\u03b8 t belongs to E TS t which leads to(Term I)1{E t } := R TS t 1{E t } \u2264 x \u03b8 \u2212 inf \u03b8\u2208E TS t (x t (\u03b8)) \u03b8 1{Z t }.(44)\nSafe Linear Thompson Sampling with Side Information D. Formal proof of Lemma 3.2In this section, we provide a formal statement and a detailed proof of Lemma 3.2. Here, we need several modifications compared to(Abeille et al., 2017)that are required because in our setting, actions x t belong to inner approximations of the true safe set D s 0 . Moreover, we follow an algebraic treatment that is perhaps simpler compared to the geometric viewpoint in(Abeille et al., 2017).Proof. First, we provide the shrunk versionD s t of D s t as follows: A shrunk safe decision setD s t . Consider the enlarged confidence regionC t centered at \u00b5 asWe know that C t \u2286C t , since \u2200v \u2208 C t , we know that. From the definition of enlarged confidence region, we can get the following definition for shrunk safe decision set:and note thatD s t \u2286 D s t , and they are not empty, since they include zero due to Assumption 3. Then, we define the parameter \u03b1 t such that the vector z t = \u03b1 t x in direction x belongs toD s t and is closest to x . Hence, we have:\nImproved algorithms for linear stochastic bandits. Y Abbasi-Yadkori, D P\u00e1l, C Szepesv\u00e1ri, Advances in Neural Information Processing Systems. Abbasi-Yadkori, Y., P\u00e1l, D., and Szepesv\u00e1ri, C. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, pp. 2312-2320, 2011.\n\nLinear thompson sampling revisited. M Abeille, A Lazaric, Electronic Journal of Statistics. 112Abeille, M., Lazaric, A., et al. Linear thompson sampling revisited. Electronic Journal of Statistics, 11(2):5165- 5197, 2017.\n\nAnalysis of thompson sampling for the multi-armed bandit problem. S Agrawal, N Goyal, Conference on Learning Theory. Agrawal, S. and Goyal, N. Analysis of thompson sampling for the multi-armed bandit problem. In Conference on Learning Theory, pp. 39-1, 2012.\n\nThompson sampling for contextual bandits with linear payoffs. S Agrawal, N Goyal, International Conference on Machine Learning. Agrawal, S. and Goyal, N. Thompson sampling for contex- tual bandits with linear payoffs. In International Confer- ence on Machine Learning, pp. 127-135, 2013.\n\nReachability-based safe learning with gaussian processes. A K Akametalu, J F Fisac, J H Gillula, S Kaynama, M N Zeilinger, C J Tomlin, 10.1109/CDC.2014.703960153rd IEEE Conference on Decision and Control. Akametalu, A. K., Fisac, J. F., Gillula, J. H., Kaynama, S., Zeilinger, M. N., and Tomlin, C. J. Reachability-based safe learning with gaussian processes. In 53rd IEEE Conference on Decision and Control, pp. 1424-1431, Dec 2014. doi: 10.1109/CDC.2014.7039601.\n\nS Amani, M Alizadeh, C Thrampoulidis, arXiv:1908.05814Linear stochastic bandits under safety constraints. arXiv preprintAmani, S., Alizadeh, M., and Thrampoulidis, C. Linear stochastic bandits under safety constraints. arXiv preprint arXiv:1908.05814, 2019.\n\nProvably safe and robust learning-based model predictive control. A Aswani, H Gonzalez, S S Sastry, Tomlin , C , Automatica. 495Aswani, A., Gonzalez, H., Sastry, S. S., and Tomlin, C. Provably safe and robust learning-based model predictive control. Automatica, 49(5):1216-1226, 2013.\n\nFinite-time analysis of the multiarmed bandit problem. P Auer, N Cesa-Bianchi, P Fischer, Machine learning. 472-3Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235-256, 2002.\n\nConcentration inequalities: A nonasymptotic theory of independence. S Boucheron, G Lugosi, P Massart, Oxford university pressBoucheron, S., Lugosi, G., and Massart, P. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013.\n\nContextual bandits with linear payoff functions. W Chu, L Li, L Reyzin, R Schapire, PMLRProceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. Gordon, G., Dunson, D., and Dud\u00edk, M.the Fourteenth International Conference on Artificial Intelligence and StatisticsFort Lauderdale, FL, USA15Chu, W., Li, L., Reyzin, L., and Schapire, R. Contextual ban- dits with linear payoff functions. In Gordon, G., Dunson, D., and Dud\u00edk, M. (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learn- ing Research, pp. 208-214, Fort Lauderdale, FL, USA, 11-13 Apr 2011. PMLR.\n\nStochastic linear optimization under bandit feedback. V Dani, T P Hayes, S M Kakade, Dani, V., Hayes, T. P., and Kakade, S. M. Stochastic linear optimization under bandit feedback. 2008.\n\nAn information-theoretic analysis for thompson sampling with many actions. S Dong, B Van Roy, Advances in Neural Information Processing Systems. Dong, S. and Van Roy, B. An information-theoretic analysis for thompson sampling with many actions. In Advances in Neural Information Processing Systems, pp. 4157-4165, 2018.\n\nOn the performance of thompson sampling on logistic bandits. S Dong, T Ma, B Van Roy, arXiv:1905.04654arXiv preprintDong, S., Ma, T., and Van Roy, B. On the performance of thompson sampling on logistic bandits. arXiv preprint arXiv:1905.04654, 2019.\n\nThompson sampling for learning parameterized markov decision processes. A Gopalan, S Mannor, Conference on Learning Theory. Gopalan, A. and Mannor, S. Thompson sampling for learn- ing parameterized markov decision processes. In Confer- ence on Learning Theory, pp. 861-898, 2015.\n\nThompson sampling for complex online problems. A Gopalan, S Mannor, Y Mansour, International Conference on Machine Learning. Gopalan, A., Mannor, S., and Mansour, Y. Thompson sam- pling for complex online problems. In International Con- ference on Machine Learning, pp. 100-108, 2014.\n\nThompson sampling: An asymptotically optimal finite-time analysis. E Kaufmann, N Korda, R Munos, International Conference on Algorithmic Learning Theory. SpringerKaufmann, E., Korda, N., and Munos, R. Thompson sam- pling: An asymptotically optimal finite-time analysis. In International Conference on Algorithmic Learning The- ory, pp. 199-213. Springer, 2012.\n\nConservative contextual linear bandits. A Kazerouni, M Ghavamzadeh, Y Abbasi, B ; Van Roy, U V Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, Garnett , Advances in Neural Information Processing Systems. R.Curran Associates, Inc30Guyon, IKazerouni, A., Ghavamzadeh, M., Abbasi, Y., and Van Roy, B. Conservative contextual linear bandits. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30, pp. 3910- 3919. Curran Associates, Inc., 2017.\n\nLearning-based model predictive control for safe exploration. T Koller, F Berkenkamp, M Turchetta, A Krause, 2018 IEEE Conference on Decision and Control (CDC). IEEEKoller, T., Berkenkamp, F., Turchetta, M., and Krause, A. Learning-based model predictive control for safe explo- ration. In 2018 IEEE Conference on Decision and Control (CDC), pp. 6059-6066. IEEE, 2018.\n\nA contextual-bandit approach to personalized news article recommendation. L Li, W Chu, J Langford, R E Schapire, Proceedings of the 19th international conference on World wide web. the 19th international conference on World wide webACMLi, L., Chu, W., Langford, J., and Schapire, R. E. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th interna- tional conference on World wide web, pp. 661-670. ACM, 2010.\n\nBootstrapped thompson sampling and deep exploration. I Osband, B Van Roy, arXiv:1507.00300arXiv preprintOsband, I. and Van Roy, B. Bootstrapped thomp- son sampling and deep exploration. arXiv preprint arXiv:1507.00300, 2015.\n\nRobust constrained learning-based nmpc enabling reliable mobile robot path tracking. C J Ostafew, A P Schoellig, T D Barfoot, The International Journal of Robotics Research. 3513Ostafew, C. J., Schoellig, A. P., and Barfoot, T. D. Robust constrained learning-based nmpc enabling reliable mobile robot path tracking. The International Journal of Robotics Research, 35(13):1547-1563, 2016.\n\nLinearly parameterized bandits. P Rusmevichientong, J N Tsitsiklis, 10.1287/moor.1100.0446Mathematics of Operations Research. 352Rusmevichientong, P. and Tsitsiklis, J. N. Linearly parame- terized bandits. Mathematics of Operations Research, 35 (2):395-411, 2010. doi: 10.1287/moor.1100.0446.\n\nLearning to optimize via posterior sampling. D Russo, B Van Roy, 10.1287/moor.2014.0650Mathematics of Operations Research. 394Russo, D. and Van Roy, B. Learning to optimize via poste- rior sampling. Mathematics of Operations Research, 39 (4):1221-1243, 2014. doi: 10.1287/moor.2014.0650.\n\nAn information-theoretic analysis of thompson sampling. D Russo, B Van Roy, The Journal of Machine Learning Research. 171Russo, D. and Van Roy, B. An information-theoretic anal- ysis of thompson sampling. The Journal of Machine Learning Research, 17(1):2442-2471, 2016.\n\nSafe exploration for optimization with gaussian processes. Y Sui, A Gotovos, J W Burdick, A Krause, Proceedings of the 32Nd International Conference on International Conference on Machine Learning. the 32Nd International Conference on International Conference on Machine Learning37Sui, Y., Gotovos, A., Burdick, J. W., and Krause, A. Safe exploration for optimization with gaussian pro- cesses. In Proceedings of the 32Nd International Con- ference on International Conference on Machine Learn- ing -Volume 37, ICML'15, pp. 997-1005. JMLR.org, 2015. URL http://dl.acm.org/citation. cfm?id=3045118.3045225.\n", "annotations": {"author": "[{\"end\":73,\"start\":55}]", "publisher": null, "author_last_name": "[{\"end\":72,\"start\":65}]", "author_first_name": "[{\"end\":64,\"start\":55}]", "author_affiliation": null, "title": "[{\"end\":52,\"start\":1},{\"end\":125,\"start\":74}]", "venue": null, "abstract": "[{\"end\":1003,\"start\":134}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2573,\"start\":2554},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2605,\"start\":2586},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2641,\"start\":2605},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2669,\"start\":2641},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2778,\"start\":2760},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2794,\"start\":2778},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2942,\"start\":2913},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3028,\"start\":3005},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3049,\"start\":3028},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5500,\"start\":5478},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5665,\"start\":5646},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5701,\"start\":5665},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6181,\"start\":6160},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6201,\"start\":6181},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6273,\"start\":6255},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6539,\"start\":6517},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6562,\"start\":6539},{\"end\":6614,\"start\":6591},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7167,\"start\":7147},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7195,\"start\":7175},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7675,\"start\":7655},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8040,\"start\":8022},{\"end\":8062,\"start\":8040},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8285,\"start\":8265},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8354,\"start\":8330},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8478,\"start\":8454},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8697,\"start\":8677},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8720,\"start\":8697},{\"end\":8742,\"start\":8720},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9076,\"start\":9054},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9099,\"start\":9076},{\"end\":9346,\"start\":9330},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9585,\"start\":9563},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10130,\"start\":10108},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10152,\"start\":10130},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10422,\"start\":10398},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10463,\"start\":10440},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10484,\"start\":10463},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10502,\"start\":10484},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10590,\"start\":10566},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10712,\"start\":10690},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14011,\"start\":13982},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14372,\"start\":14349},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14393,\"start\":14372},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14585,\"start\":14562},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14605,\"start\":14585},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14846,\"start\":14823},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14867,\"start\":14846},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15727,\"start\":15704},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15748,\"start\":15727},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17657,\"start\":17635},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19460,\"start\":19438},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19801,\"start\":19779},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20611,\"start\":20589},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22179,\"start\":22157},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22366,\"start\":22344},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22962,\"start\":22939},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22983,\"start\":22962},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23512,\"start\":23489},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23533,\"start\":23512},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23979,\"start\":23950},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24000,\"start\":23979},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24328,\"start\":24309},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24364,\"start\":24328},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24392,\"start\":24364},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24504,\"start\":24481},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24525,\"start\":24504},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25411,\"start\":25389},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28781,\"start\":28759},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30146,\"start\":30127},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30165,\"start\":30146},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30400,\"start\":30378},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31387,\"start\":31368},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32620,\"start\":32600},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32718,\"start\":32698},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33049,\"start\":33029},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33279,\"start\":33259},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33465,\"start\":33445},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33539,\"start\":33519},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33663,\"start\":33643},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34292,\"start\":34272},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35067,\"start\":35045},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36545,\"start\":36527},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":38136,\"start\":38114},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":39697,\"start\":39675},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39943,\"start\":39915},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":40158,\"start\":40128},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":40600,\"start\":40576},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":41684,\"start\":41662},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":44104,\"start\":44082},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":50681,\"start\":50661}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51491,\"start\":51264},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51805,\"start\":51492},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52038,\"start\":51806},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52251,\"start\":52039},{\"attributes\":{\"id\":\"fig_4\"},\"end\":52371,\"start\":52252},{\"attributes\":{\"id\":\"fig_5\"},\"end\":52474,\"start\":52372},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":52819,\"start\":52475},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":53139,\"start\":52820}]", "paragraph": "[{\"end\":1920,\"start\":1019},{\"end\":3316,\"start\":1922},{\"end\":3514,\"start\":3356},{\"end\":3626,\"start\":3539},{\"end\":3720,\"start\":3628},{\"end\":4036,\"start\":3735},{\"end\":4290,\"start\":4075},{\"end\":4817,\"start\":4314},{\"end\":4845,\"start\":4819},{\"end\":4852,\"start\":4847},{\"end\":5137,\"start\":4854},{\"end\":5301,\"start\":5155},{\"end\":5703,\"start\":5303},{\"end\":5963,\"start\":5705},{\"end\":9277,\"start\":5980},{\"end\":9602,\"start\":9279},{\"end\":9683,\"start\":9659},{\"end\":10973,\"start\":9971},{\"end\":11881,\"start\":11007},{\"end\":11912,\"start\":11903},{\"end\":12089,\"start\":11914},{\"end\":12529,\"start\":12098},{\"end\":12633,\"start\":12531},{\"end\":12841,\"start\":12776},{\"end\":12969,\"start\":12843},{\"end\":13169,\"start\":12971},{\"end\":13463,\"start\":13210},{\"end\":13719,\"start\":13559},{\"end\":14082,\"start\":13807},{\"end\":14219,\"start\":14129},{\"end\":14647,\"start\":14278},{\"end\":14805,\"start\":14685},{\"end\":15314,\"start\":14807},{\"end\":15397,\"start\":15331},{\"end\":15599,\"start\":15399},{\"end\":15823,\"start\":15645},{\"end\":15884,\"start\":15825},{\"end\":15973,\"start\":15886},{\"end\":16100,\"start\":15975},{\"end\":16484,\"start\":16102},{\"end\":16600,\"start\":16529},{\"end\":16817,\"start\":16635},{\"end\":17302,\"start\":16939},{\"end\":17981,\"start\":17304},{\"end\":18224,\"start\":17983},{\"end\":19486,\"start\":18226},{\"end\":20002,\"start\":19488},{\"end\":20351,\"start\":20004},{\"end\":20460,\"start\":20382},{\"end\":20737,\"start\":20513},{\"end\":21288,\"start\":20739},{\"end\":21590,\"start\":21290},{\"end\":21718,\"start\":21610},{\"end\":22984,\"start\":21720},{\"end\":23150,\"start\":22986},{\"end\":23340,\"start\":23250},{\"end\":23574,\"start\":23342},{\"end\":23670,\"start\":23576},{\"end\":24062,\"start\":23732},{\"end\":24733,\"start\":24064},{\"end\":25412,\"start\":24735},{\"end\":25881,\"start\":25466},{\"end\":25989,\"start\":25905},{\"end\":26626,\"start\":25991},{\"end\":27098,\"start\":26628},{\"end\":27189,\"start\":27140},{\"end\":27500,\"start\":27287},{\"end\":27648,\"start\":27542},{\"end\":27750,\"start\":27673},{\"end\":28008,\"start\":27819},{\"end\":28129,\"start\":28075},{\"end\":28286,\"start\":28215},{\"end\":28381,\"start\":28364},{\"end\":28625,\"start\":28463},{\"end\":29615,\"start\":28627},{\"end\":30402,\"start\":29672},{\"end\":31159,\"start\":30446},{\"end\":32489,\"start\":31199},{\"end\":34805,\"start\":32517},{\"end\":35733,\"start\":34852},{\"end\":37655,\"start\":35748},{\"end\":38197,\"start\":37711},{\"end\":38544,\"start\":38199},{\"end\":38841,\"start\":38612},{\"end\":39123,\"start\":38952},{\"end\":39182,\"start\":39177},{\"end\":39422,\"start\":39236},{\"end\":39983,\"start\":39529},{\"end\":40109,\"start\":40005},{\"end\":40288,\"start\":40111},{\"end\":40386,\"start\":40364},{\"end\":40537,\"start\":40462},{\"end\":40780,\"start\":40539},{\"end\":40934,\"start\":40854},{\"end\":41024,\"start\":40936},{\"end\":41153,\"start\":41026},{\"end\":41310,\"start\":41155},{\"end\":41432,\"start\":41312},{\"end\":41607,\"start\":41487},{\"end\":41712,\"start\":41609},{\"end\":42016,\"start\":41811},{\"end\":42201,\"start\":42133},{\"end\":42268,\"start\":42203},{\"end\":42395,\"start\":42329},{\"end\":42547,\"start\":42436},{\"end\":42635,\"start\":42591},{\"end\":42710,\"start\":42691},{\"end\":42863,\"start\":42798},{\"end\":43119,\"start\":43072},{\"end\":43198,\"start\":43175},{\"end\":43348,\"start\":43302},{\"end\":43825,\"start\":43385},{\"end\":43891,\"start\":43877},{\"end\":44025,\"start\":44002},{\"end\":44556,\"start\":44027},{\"end\":44617,\"start\":44558},{\"end\":44767,\"start\":44746},{\"end\":45129,\"start\":44769},{\"end\":45379,\"start\":45131},{\"end\":45483,\"start\":45457},{\"end\":45632,\"start\":45561},{\"end\":46167,\"start\":45813},{\"end\":46437,\"start\":46395},{\"end\":46526,\"start\":46497},{\"end\":46658,\"start\":46606},{\"end\":46848,\"start\":46739},{\"end\":47298,\"start\":46850},{\"end\":47508,\"start\":47410},{\"end\":47605,\"start\":47538},{\"end\":47839,\"start\":47700},{\"end\":48044,\"start\":47983},{\"end\":48264,\"start\":48141},{\"end\":48270,\"start\":48266},{\"end\":48502,\"start\":48300},{\"end\":49031,\"start\":48604},{\"end\":49235,\"start\":49033},{\"end\":49492,\"start\":49261},{\"end\":49890,\"start\":49528},{\"end\":50554,\"start\":49892},{\"end\":51263,\"start\":50556}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3538,\"start\":3515},{\"attributes\":{\"id\":\"formula_1\"},\"end\":3734,\"start\":3721},{\"attributes\":{\"id\":\"formula_2\"},\"end\":4074,\"start\":4037},{\"attributes\":{\"id\":\"formula_3\"},\"end\":4313,\"start\":4291},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9970,\"start\":9684},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12097,\"start\":12090},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12775,\"start\":12634},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13527,\"start\":13464},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13558,\"start\":13527},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13763,\"start\":13720},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13806,\"start\":13763},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14128,\"start\":14083},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14684,\"start\":14648},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15330,\"start\":15315},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16528,\"start\":16485},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16634,\"start\":16601},{\"attributes\":{\"id\":\"formula_16\"},\"end\":16938,\"start\":16818},{\"attributes\":{\"id\":\"formula_17\"},\"end\":20381,\"start\":20352},{\"attributes\":{\"id\":\"formula_18\"},\"end\":20512,\"start\":20461},{\"attributes\":{\"id\":\"formula_19\"},\"end\":23249,\"start\":23151},{\"attributes\":{\"id\":\"formula_20\"},\"end\":23731,\"start\":23671},{\"attributes\":{\"id\":\"formula_21\"},\"end\":25904,\"start\":25882},{\"attributes\":{\"id\":\"formula_22\"},\"end\":27139,\"start\":27099},{\"attributes\":{\"id\":\"formula_23\"},\"end\":27286,\"start\":27190},{\"attributes\":{\"id\":\"formula_24\"},\"end\":27541,\"start\":27501},{\"attributes\":{\"id\":\"formula_25\"},\"end\":27672,\"start\":27649},{\"attributes\":{\"id\":\"formula_26\"},\"end\":27818,\"start\":27751},{\"attributes\":{\"id\":\"formula_27\"},\"end\":28074,\"start\":28009},{\"attributes\":{\"id\":\"formula_28\"},\"end\":28214,\"start\":28130},{\"attributes\":{\"id\":\"formula_29\"},\"end\":28363,\"start\":28287},{\"attributes\":{\"id\":\"formula_30\"},\"end\":28414,\"start\":28382},{\"attributes\":{\"id\":\"formula_31\"},\"end\":28462,\"start\":28414},{\"attributes\":{\"id\":\"formula_32\"},\"end\":37710,\"start\":37656},{\"attributes\":{\"id\":\"formula_33\"},\"end\":38611,\"start\":38545},{\"attributes\":{\"id\":\"formula_34\"},\"end\":38951,\"start\":38842},{\"attributes\":{\"id\":\"formula_35\"},\"end\":39176,\"start\":39124},{\"attributes\":{\"id\":\"formula_36\"},\"end\":39235,\"start\":39183},{\"attributes\":{\"id\":\"formula_37\"},\"end\":39528,\"start\":39423},{\"attributes\":{\"id\":\"formula_38\"},\"end\":40363,\"start\":40289},{\"attributes\":{\"id\":\"formula_39\"},\"end\":40461,\"start\":40387},{\"attributes\":{\"id\":\"formula_40\"},\"end\":40829,\"start\":40781},{\"attributes\":{\"id\":\"formula_41\"},\"end\":41486,\"start\":41433},{\"attributes\":{\"id\":\"formula_42\"},\"end\":41810,\"start\":41713},{\"attributes\":{\"id\":\"formula_43\"},\"end\":42132,\"start\":42017},{\"attributes\":{\"id\":\"formula_44\"},\"end\":42328,\"start\":42269},{\"attributes\":{\"id\":\"formula_45\"},\"end\":42435,\"start\":42396},{\"attributes\":{\"id\":\"formula_46\"},\"end\":42590,\"start\":42548},{\"attributes\":{\"id\":\"formula_47\"},\"end\":42690,\"start\":42636},{\"attributes\":{\"id\":\"formula_48\"},\"end\":42797,\"start\":42711},{\"attributes\":{\"id\":\"formula_49\"},\"end\":43071,\"start\":42864},{\"attributes\":{\"id\":\"formula_50\"},\"end\":43174,\"start\":43120},{\"attributes\":{\"id\":\"formula_51\"},\"end\":43267,\"start\":43199},{\"attributes\":{\"id\":\"formula_52\"},\"end\":43301,\"start\":43267},{\"attributes\":{\"id\":\"formula_53\"},\"end\":43384,\"start\":43349},{\"attributes\":{\"id\":\"formula_54\"},\"end\":43876,\"start\":43826},{\"attributes\":{\"id\":\"formula_55\"},\"end\":44001,\"start\":43892},{\"attributes\":{\"id\":\"formula_56\"},\"end\":44745,\"start\":44618},{\"attributes\":{\"id\":\"formula_57\"},\"end\":45456,\"start\":45380},{\"attributes\":{\"id\":\"formula_58\"},\"end\":45560,\"start\":45484},{\"attributes\":{\"id\":\"formula_59\"},\"end\":45812,\"start\":45633},{\"attributes\":{\"id\":\"formula_60\"},\"end\":46394,\"start\":46168},{\"attributes\":{\"id\":\"formula_61\"},\"end\":46496,\"start\":46438},{\"attributes\":{\"id\":\"formula_62\"},\"end\":46605,\"start\":46527},{\"attributes\":{\"id\":\"formula_63\"},\"end\":46738,\"start\":46659},{\"attributes\":{\"id\":\"formula_64\"},\"end\":47350,\"start\":47299},{\"attributes\":{\"id\":\"formula_65\"},\"end\":47409,\"start\":47350},{\"attributes\":{\"id\":\"formula_66\"},\"end\":47537,\"start\":47509},{\"attributes\":{\"id\":\"formula_67\"},\"end\":47675,\"start\":47606},{\"attributes\":{\"id\":\"formula_68\"},\"end\":47982,\"start\":47840},{\"attributes\":{\"id\":\"formula_69\"},\"end\":48140,\"start\":48045},{\"attributes\":{\"id\":\"formula_70\"},\"end\":48603,\"start\":48503},{\"attributes\":{\"id\":\"formula_71\"},\"end\":49260,\"start\":49236}]", "table_ref": "[{\"end\":18994,\"start\":18723},{\"end\":36805,\"start\":36547},{\"end\":37073,\"start\":36815},{\"end\":45077,\"start\":44812},{\"end\":47166,\"start\":46872}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1017,\"start\":1005},{\"attributes\":{\"n\":\"1.1.\"},\"end\":3354,\"start\":3319},{\"attributes\":{\"n\":\"1.2.\"},\"end\":5153,\"start\":5140},{\"attributes\":{\"n\":\"1.3.\"},\"end\":5978,\"start\":5966},{\"end\":9657,\"start\":9605},{\"attributes\":{\"n\":\"2.\"},\"end\":11005,\"start\":10976},{\"attributes\":{\"n\":\"2.1.\"},\"end\":11901,\"start\":11884},{\"attributes\":{\"n\":\"2.2.\"},\"end\":13208,\"start\":13172},{\"attributes\":{\"n\":\"2.2.1.\"},\"end\":14253,\"start\":14222},{\"end\":14276,\"start\":14256},{\"attributes\":{\"n\":\"2.2.2.\"},\"end\":15643,\"start\":15602},{\"attributes\":{\"n\":\"3.\"},\"end\":21608,\"start\":21593},{\"attributes\":{\"n\":\"3.1.\"},\"end\":25464,\"start\":25415},{\"attributes\":{\"n\":\"4.\"},\"end\":29670,\"start\":29618},{\"attributes\":{\"n\":\"4.1.\"},\"end\":30444,\"start\":30405},{\"attributes\":{\"n\":\"4.2.\"},\"end\":31197,\"start\":31162},{\"attributes\":{\"n\":\"4.3.\"},\"end\":32515,\"start\":32492},{\"attributes\":{\"n\":\"4.4.\"},\"end\":34850,\"start\":34808},{\"attributes\":{\"n\":\"5.\"},\"end\":35746,\"start\":35736},{\"end\":40003,\"start\":39986},{\"end\":40852,\"start\":40831},{\"end\":47698,\"start\":47677},{\"end\":48298,\"start\":48273},{\"end\":49526,\"start\":49495},{\"end\":51513,\"start\":51493},{\"end\":51827,\"start\":51807},{\"end\":52050,\"start\":52040},{\"end\":52263,\"start\":52253},{\"end\":52383,\"start\":52373}]", "table": "[{\"end\":52819,\"start\":52553},{\"end\":53139,\"start\":52873}]", "figure_caption": "[{\"end\":51491,\"start\":51266},{\"end\":51805,\"start\":51516},{\"end\":52038,\"start\":51830},{\"end\":52251,\"start\":52052},{\"end\":52371,\"start\":52265},{\"end\":52474,\"start\":52385},{\"end\":52553,\"start\":52477},{\"end\":52873,\"start\":52822}]", "figure_ref": "[{\"end\":30455,\"start\":30449},{\"end\":31088,\"start\":31082},{\"end\":31843,\"start\":31836},{\"end\":32213,\"start\":32207},{\"end\":32293,\"start\":32287},{\"end\":33772,\"start\":33766},{\"end\":34097,\"start\":34091},{\"end\":35671,\"start\":35665},{\"end\":49348,\"start\":49340},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":49557,\"start\":49549},{\"end\":49686,\"start\":49678},{\"end\":50063,\"start\":50055},{\"end\":50218,\"start\":50210},{\"end\":50553,\"start\":50548},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":50585,\"start\":50579},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":50843,\"start\":50837}]", "bib_author_first_name": "[{\"end\":54511,\"start\":54510},{\"end\":54529,\"start\":54528},{\"end\":54536,\"start\":54535},{\"end\":54812,\"start\":54811},{\"end\":54823,\"start\":54822},{\"end\":55065,\"start\":55064},{\"end\":55076,\"start\":55075},{\"end\":55321,\"start\":55320},{\"end\":55332,\"start\":55331},{\"end\":55606,\"start\":55605},{\"end\":55608,\"start\":55607},{\"end\":55621,\"start\":55620},{\"end\":55623,\"start\":55622},{\"end\":55632,\"start\":55631},{\"end\":55634,\"start\":55633},{\"end\":55645,\"start\":55644},{\"end\":55656,\"start\":55655},{\"end\":55658,\"start\":55657},{\"end\":55671,\"start\":55670},{\"end\":55673,\"start\":55672},{\"end\":56014,\"start\":56013},{\"end\":56023,\"start\":56022},{\"end\":56035,\"start\":56034},{\"end\":56339,\"start\":56338},{\"end\":56349,\"start\":56348},{\"end\":56361,\"start\":56360},{\"end\":56363,\"start\":56362},{\"end\":56378,\"start\":56372},{\"end\":56382,\"start\":56381},{\"end\":56614,\"start\":56613},{\"end\":56622,\"start\":56621},{\"end\":56638,\"start\":56637},{\"end\":56881,\"start\":56880},{\"end\":56894,\"start\":56893},{\"end\":56904,\"start\":56903},{\"end\":57130,\"start\":57129},{\"end\":57137,\"start\":57136},{\"end\":57143,\"start\":57142},{\"end\":57153,\"start\":57152},{\"end\":57828,\"start\":57827},{\"end\":57836,\"start\":57835},{\"end\":57838,\"start\":57837},{\"end\":57847,\"start\":57846},{\"end\":57849,\"start\":57848},{\"end\":58037,\"start\":58036},{\"end\":58045,\"start\":58044},{\"end\":58344,\"start\":58343},{\"end\":58352,\"start\":58351},{\"end\":58358,\"start\":58357},{\"end\":58606,\"start\":58605},{\"end\":58617,\"start\":58616},{\"end\":58862,\"start\":58861},{\"end\":58873,\"start\":58872},{\"end\":58883,\"start\":58882},{\"end\":59168,\"start\":59167},{\"end\":59180,\"start\":59179},{\"end\":59189,\"start\":59188},{\"end\":59503,\"start\":59502},{\"end\":59516,\"start\":59515},{\"end\":59531,\"start\":59530},{\"end\":59541,\"start\":59540},{\"end\":59543,\"start\":59542},{\"end\":59554,\"start\":59553},{\"end\":59556,\"start\":59555},{\"end\":59567,\"start\":59566},{\"end\":59577,\"start\":59576},{\"end\":59588,\"start\":59587},{\"end\":59598,\"start\":59597},{\"end\":59620,\"start\":59613},{\"end\":60082,\"start\":60081},{\"end\":60092,\"start\":60091},{\"end\":60106,\"start\":60105},{\"end\":60119,\"start\":60118},{\"end\":60464,\"start\":60463},{\"end\":60470,\"start\":60469},{\"end\":60477,\"start\":60476},{\"end\":60489,\"start\":60488},{\"end\":60491,\"start\":60490},{\"end\":60901,\"start\":60900},{\"end\":60911,\"start\":60910},{\"end\":61159,\"start\":61158},{\"end\":61161,\"start\":61160},{\"end\":61172,\"start\":61171},{\"end\":61174,\"start\":61173},{\"end\":61187,\"start\":61186},{\"end\":61189,\"start\":61188},{\"end\":61495,\"start\":61494},{\"end\":61515,\"start\":61514},{\"end\":61517,\"start\":61516},{\"end\":61802,\"start\":61801},{\"end\":61811,\"start\":61810},{\"end\":62102,\"start\":62101},{\"end\":62111,\"start\":62110},{\"end\":62376,\"start\":62375},{\"end\":62383,\"start\":62382},{\"end\":62394,\"start\":62393},{\"end\":62396,\"start\":62395},{\"end\":62407,\"start\":62406}]", "bib_author_last_name": "[{\"end\":54526,\"start\":54512},{\"end\":54533,\"start\":54530},{\"end\":54547,\"start\":54537},{\"end\":54820,\"start\":54813},{\"end\":54831,\"start\":54824},{\"end\":55073,\"start\":55066},{\"end\":55082,\"start\":55077},{\"end\":55329,\"start\":55322},{\"end\":55338,\"start\":55333},{\"end\":55618,\"start\":55609},{\"end\":55629,\"start\":55624},{\"end\":55642,\"start\":55635},{\"end\":55653,\"start\":55646},{\"end\":55668,\"start\":55659},{\"end\":55680,\"start\":55674},{\"end\":56020,\"start\":56015},{\"end\":56032,\"start\":56024},{\"end\":56049,\"start\":56036},{\"end\":56346,\"start\":56340},{\"end\":56358,\"start\":56350},{\"end\":56370,\"start\":56364},{\"end\":56619,\"start\":56615},{\"end\":56635,\"start\":56623},{\"end\":56646,\"start\":56639},{\"end\":56891,\"start\":56882},{\"end\":56901,\"start\":56895},{\"end\":56912,\"start\":56905},{\"end\":57134,\"start\":57131},{\"end\":57140,\"start\":57138},{\"end\":57150,\"start\":57144},{\"end\":57162,\"start\":57154},{\"end\":57833,\"start\":57829},{\"end\":57844,\"start\":57839},{\"end\":57856,\"start\":57850},{\"end\":58042,\"start\":58038},{\"end\":58053,\"start\":58046},{\"end\":58349,\"start\":58345},{\"end\":58355,\"start\":58353},{\"end\":58366,\"start\":58359},{\"end\":58614,\"start\":58607},{\"end\":58624,\"start\":58618},{\"end\":58870,\"start\":58863},{\"end\":58880,\"start\":58874},{\"end\":58891,\"start\":58884},{\"end\":59177,\"start\":59169},{\"end\":59186,\"start\":59181},{\"end\":59195,\"start\":59190},{\"end\":59513,\"start\":59504},{\"end\":59528,\"start\":59517},{\"end\":59538,\"start\":59532},{\"end\":59551,\"start\":59544},{\"end\":59564,\"start\":59557},{\"end\":59574,\"start\":59568},{\"end\":59585,\"start\":59578},{\"end\":59595,\"start\":59589},{\"end\":59611,\"start\":59599},{\"end\":60089,\"start\":60083},{\"end\":60103,\"start\":60093},{\"end\":60116,\"start\":60107},{\"end\":60126,\"start\":60120},{\"end\":60467,\"start\":60465},{\"end\":60474,\"start\":60471},{\"end\":60486,\"start\":60478},{\"end\":60500,\"start\":60492},{\"end\":60908,\"start\":60902},{\"end\":60919,\"start\":60912},{\"end\":61169,\"start\":61162},{\"end\":61184,\"start\":61175},{\"end\":61197,\"start\":61190},{\"end\":61512,\"start\":61496},{\"end\":61528,\"start\":61518},{\"end\":61808,\"start\":61803},{\"end\":61819,\"start\":61812},{\"end\":62108,\"start\":62103},{\"end\":62119,\"start\":62112},{\"end\":62380,\"start\":62377},{\"end\":62391,\"start\":62384},{\"end\":62404,\"start\":62397},{\"end\":62414,\"start\":62408}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1713123},\"end\":54773,\"start\":54459},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8385089},\"end\":54996,\"start\":54775},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":512579},\"end\":55256,\"start\":54998},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":96146},\"end\":55545,\"start\":55258},{\"attributes\":{\"doi\":\"10.1109/CDC.2014.7039601\",\"id\":\"b4\",\"matched_paper_id\":7032570},\"end\":56011,\"start\":55547},{\"attributes\":{\"doi\":\"arXiv:1908.05814\",\"id\":\"b5\"},\"end\":56270,\"start\":56013},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15555223},\"end\":56556,\"start\":56272},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207609497},\"end\":56810,\"start\":56558},{\"attributes\":{\"id\":\"b8\"},\"end\":57078,\"start\":56812},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b9\",\"matched_paper_id\":1452971},\"end\":57771,\"start\":57080},{\"attributes\":{\"id\":\"b10\"},\"end\":57959,\"start\":57773},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":46926515},\"end\":58280,\"start\":57961},{\"attributes\":{\"doi\":\"arXiv:1905.04654\",\"id\":\"b12\"},\"end\":58531,\"start\":58282},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":7323564},\"end\":58812,\"start\":58533},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2059032},\"end\":59098,\"start\":58814},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14803226},\"end\":59460,\"start\":59100},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1128850},\"end\":60017,\"start\":59462},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4064737},\"end\":60387,\"start\":60019},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":207178795},\"end\":60845,\"start\":60389},{\"attributes\":{\"doi\":\"arXiv:1507.00300\",\"id\":\"b19\"},\"end\":61071,\"start\":60847},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1815857},\"end\":61460,\"start\":61073},{\"attributes\":{\"doi\":\"10.1287/moor.1100.0446\",\"id\":\"b21\",\"matched_paper_id\":3204347},\"end\":61754,\"start\":61462},{\"attributes\":{\"doi\":\"10.1287/moor.2014.0650\",\"id\":\"b22\",\"matched_paper_id\":5468643},\"end\":62043,\"start\":61756},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1059085},\"end\":62314,\"start\":62045},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":16582615},\"end\":62921,\"start\":62316}]", "bib_title": "[{\"end\":54508,\"start\":54459},{\"end\":54809,\"start\":54775},{\"end\":55062,\"start\":54998},{\"end\":55318,\"start\":55258},{\"end\":55603,\"start\":55547},{\"end\":56336,\"start\":56272},{\"end\":56611,\"start\":56558},{\"end\":57127,\"start\":57080},{\"end\":58034,\"start\":57961},{\"end\":58603,\"start\":58533},{\"end\":58859,\"start\":58814},{\"end\":59165,\"start\":59100},{\"end\":59500,\"start\":59462},{\"end\":60079,\"start\":60019},{\"end\":60461,\"start\":60389},{\"end\":61156,\"start\":61073},{\"end\":61492,\"start\":61462},{\"end\":61799,\"start\":61756},{\"end\":62099,\"start\":62045},{\"end\":62373,\"start\":62316}]", "bib_author": "[{\"end\":54528,\"start\":54510},{\"end\":54535,\"start\":54528},{\"end\":54549,\"start\":54535},{\"end\":54822,\"start\":54811},{\"end\":54833,\"start\":54822},{\"end\":55075,\"start\":55064},{\"end\":55084,\"start\":55075},{\"end\":55331,\"start\":55320},{\"end\":55340,\"start\":55331},{\"end\":55620,\"start\":55605},{\"end\":55631,\"start\":55620},{\"end\":55644,\"start\":55631},{\"end\":55655,\"start\":55644},{\"end\":55670,\"start\":55655},{\"end\":55682,\"start\":55670},{\"end\":56022,\"start\":56013},{\"end\":56034,\"start\":56022},{\"end\":56051,\"start\":56034},{\"end\":56348,\"start\":56338},{\"end\":56360,\"start\":56348},{\"end\":56372,\"start\":56360},{\"end\":56381,\"start\":56372},{\"end\":56385,\"start\":56381},{\"end\":56621,\"start\":56613},{\"end\":56637,\"start\":56621},{\"end\":56648,\"start\":56637},{\"end\":56893,\"start\":56880},{\"end\":56903,\"start\":56893},{\"end\":56914,\"start\":56903},{\"end\":57136,\"start\":57129},{\"end\":57142,\"start\":57136},{\"end\":57152,\"start\":57142},{\"end\":57164,\"start\":57152},{\"end\":57835,\"start\":57827},{\"end\":57846,\"start\":57835},{\"end\":57858,\"start\":57846},{\"end\":58044,\"start\":58036},{\"end\":58055,\"start\":58044},{\"end\":58351,\"start\":58343},{\"end\":58357,\"start\":58351},{\"end\":58368,\"start\":58357},{\"end\":58616,\"start\":58605},{\"end\":58626,\"start\":58616},{\"end\":58872,\"start\":58861},{\"end\":58882,\"start\":58872},{\"end\":58893,\"start\":58882},{\"end\":59179,\"start\":59167},{\"end\":59188,\"start\":59179},{\"end\":59197,\"start\":59188},{\"end\":59515,\"start\":59502},{\"end\":59530,\"start\":59515},{\"end\":59540,\"start\":59530},{\"end\":59553,\"start\":59540},{\"end\":59566,\"start\":59553},{\"end\":59576,\"start\":59566},{\"end\":59587,\"start\":59576},{\"end\":59597,\"start\":59587},{\"end\":59613,\"start\":59597},{\"end\":59623,\"start\":59613},{\"end\":60091,\"start\":60081},{\"end\":60105,\"start\":60091},{\"end\":60118,\"start\":60105},{\"end\":60128,\"start\":60118},{\"end\":60469,\"start\":60463},{\"end\":60476,\"start\":60469},{\"end\":60488,\"start\":60476},{\"end\":60502,\"start\":60488},{\"end\":60910,\"start\":60900},{\"end\":60921,\"start\":60910},{\"end\":61171,\"start\":61158},{\"end\":61186,\"start\":61171},{\"end\":61199,\"start\":61186},{\"end\":61514,\"start\":61494},{\"end\":61530,\"start\":61514},{\"end\":61810,\"start\":61801},{\"end\":61821,\"start\":61810},{\"end\":62110,\"start\":62101},{\"end\":62121,\"start\":62110},{\"end\":62382,\"start\":62375},{\"end\":62393,\"start\":62382},{\"end\":62406,\"start\":62393},{\"end\":62416,\"start\":62406}]", "bib_venue": "[{\"end\":57408,\"start\":57303},{\"end\":60621,\"start\":60570},{\"end\":62595,\"start\":62514},{\"end\":54598,\"start\":54549},{\"end\":54865,\"start\":54833},{\"end\":55113,\"start\":55084},{\"end\":55384,\"start\":55340},{\"end\":55750,\"start\":55706},{\"end\":56117,\"start\":56067},{\"end\":56395,\"start\":56385},{\"end\":56664,\"start\":56648},{\"end\":56878,\"start\":56812},{\"end\":57264,\"start\":57168},{\"end\":57825,\"start\":57773},{\"end\":58104,\"start\":58055},{\"end\":58341,\"start\":58282},{\"end\":58655,\"start\":58626},{\"end\":58937,\"start\":58893},{\"end\":59252,\"start\":59197},{\"end\":59672,\"start\":59623},{\"end\":60178,\"start\":60128},{\"end\":60568,\"start\":60502},{\"end\":60898,\"start\":60847},{\"end\":61245,\"start\":61199},{\"end\":61586,\"start\":61552},{\"end\":61877,\"start\":61843},{\"end\":62161,\"start\":62121},{\"end\":62512,\"start\":62416}]"}}}, "year": 2023, "month": 12, "day": 17}