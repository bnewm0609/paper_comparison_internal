{"id": 233296025, "updated": "2023-10-06 04:47:55.949", "metadata": {"title": "Self-Supervised WiFi-Based Activity Recognition", "authors": "[{\"first\":\"Hok-Shing\",\"last\":\"Lau\",\"middle\":[]},{\"first\":\"Ryan\",\"last\":\"McConville\",\"middle\":[]},{\"first\":\"Mohammud\",\"last\":\"Bocus\",\"middle\":[\"J.\"]},{\"first\":\"Robert\",\"last\":\"Piechocki\",\"middle\":[\"J.\"]},{\"first\":\"Raul\",\"last\":\"Santos-Rodriguez\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 4, "day": 19}, "abstract": "Traditional approaches to activity recognition involve the use of wearable sensors or cameras in order to recognise human activities. In this work, we extract fine-grained physical layer information from WiFi devices for the purpose of passive activity recognition in indoor environments. While such data is ubiquitous, few approaches are designed to utilise large amounts of unlabelled WiFi data. We propose the use of self-supervised contrastive learning to improve activity recognition performance when using multiple views of the transmitted WiFi signal captured by different synchronised receivers. We conduct experiments where the transmitters and receivers are arranged in different physical layouts so as to cover both Line-of-Sight (LoS) and non LoS (NLoS) conditions. We compare the proposed contrastive learning system with non-contrastive systems and observe a 17.7% increase in macro averaged F1 score on the task of WiFi based activity recognition, as well as significant improvements in one- and few-shot learning scenarios.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.09072", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/globecom/BocusLMPS22", "doi": "10.1109/gcwkshps56602.2022.10008537"}}, "content": {"source": {"pdf_hash": "ce69c6e023b8156b6baba1e2a16c596f88fa4d9e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.09072v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bfebfc243bce9dcd6591fd20eb5b7263bad72897", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ce69c6e023b8156b6baba1e2a16c596f88fa4d9e.txt", "contents": "\nSelf-Supervised WiFi-Based Activity Recognition\n\n\nHok-Shing Lau \nSchool of Computer Science\nElectrical and Electronic Engineering, and Engineering Maths\nUniversity of Bristol\nUK\n\nRyan Mcconville ryan.mcconville@bristol.ac.uk. \nSchool of Computer Science\nElectrical and Electronic Engineering, and Engineering Maths\nUniversity of Bristol\nUK\n\nMohammud J Bocus \nSchool of Computer Science\nElectrical and Electronic Engineering, and Engineering Maths\nUniversity of Bristol\nUK\n\nRobert J Piechocki \nSchool of Computer Science\nElectrical and Electronic Engineering, and Engineering Maths\nUniversity of Bristol\nUK\n\nRaul Santos-Rodriguez \nSchool of Computer Science\nElectrical and Electronic Engineering, and Engineering Maths\nUniversity of Bristol\nUK\n\nSelf-Supervised WiFi-Based Activity Recognition\n\nTraditional approaches to activity recognition involve the use of wearable sensors or cameras in order to recognise human activities. In this work, we extract fine-grained physical layer information from WiFi devices for the purpose of passive activity recognition in indoor environments. While such data is ubiquitous, few approaches are designed to utilise large amounts of unlabelled WiFi data. We propose the use of selfsupervised contrastive learning to improve activity recognition performance when using multiple views of the transmitted WiFi signal captured by different synchronised receivers. We conduct experiments where the transmitters and receivers are arranged in different physical layouts so as to cover both Line-of-Sight (LoS) and non LoS (NLoS) conditions. We compare the proposed contrastive learning system with non-contrastive systems and observe a 17.7% increase in macro averaged F1 score on the task of WiFi based activity recognition, as well as significant improvements in one-and few-shot learning scenarios.\n\nI. INTRODUCTION\n\nWith the emergence of the Internet of Things (IoT), the development of contextual human sensing applications has become increasingly popular. Research in this area has made significant progress recently. For example, applications such as health monitoring within indoor environments based on Human Activity Recognition (HAR) has become feasible [1]. Several WiFi sensing techniques have the potential for achieving non-restrictive, privacy-friendly indoor activity sensing when compared to current technologies such as cameras or wearable sensors [2]. Among the various WiFi sensing techniques, WiFi Channel State Information (CSI) has gained popularity in recent years because of its extensive coverage in modern indoor settings, while providing fine-grained physical layer information that is useful for activity recognition [3]. The idea behind radar sensing involves detecting and identifying physical layer changes (e.g., Doppler and phase shifts, multipath propagation, signal attenuation, etc.) in the radar signal.\n\nRecently, research in radio based human sensing has moved towards deep learning [4] approaches which have demonstrated success due their ability to learn in complex environments. One of the most popular deep learning architectures used for radar and WiFi classification is the Convolutional Neural Network (CNN), which involves transforming the raw signal into an image-like format such as spectrogram.\n\nCNNs have shown much success in radar and WiFi sensing applications, such as activity recognition [5]- [7], localisation [8], and gesture recognition [9]. However, these approaches typically require significant amounts of training data, which can be costly to obtain. Further, generalisation to different environments has proved to be particularly challenging for radar Fig. 1. An overview of the WiFi based contrastive pretraining. In a feedforward pass with a mini-batch of N spectrogram samples, each sample\nx i , i \u2208 {1, 2...N } contains two views m \u2208 {1, 2}.\nIn the figure, the green colours refer to a positive pair of samples, i.e., the signal at two different synchronised receivers, while the red refers to negative samples belonging to a different time-point. Each view of the sample x m i is input into the corresponding encoder network f \u03b8m to obtain the embedding h m i , which then further passes through the projection network g \u03b8m to obtain the projection z m i . After pretraining, the projection networks are discarded and the weights of the encoder networks frozen. For activity recognition a classification network is added on top of the encoders and fine-tuned with examples of labelled activities. and WiFi classification [10] as the multipath signal propagation is environment dependent. In this work, we propose to use a form of self-supervision, contrastive learning [11], in order to leverage synchronised data that is collected simultaneously. We investigate the potential for self-supervised contrastive learning to improve WiFi-based activity recognition performance, as well as the ability to improve performance with limited amounts of labelled data by leveraging synchronised WiFi data from multiple receivers.\n\nThe main contributions of this work are the following:\n\n\u2022 We show that the proposed constrastively trained system can increase the activity recognition performance using a suitable network model architecture over a noncontrastive baseline. \u2022 We assess the effectiveness of the proposed selfsupervised system with limited labelled examples of each activity demonstrating that high performance can be achieved with relatively few labelled data points in a few-shot activity recognition scenario. \u2022 We evaluate the effect of the type of modality pairing on the contrastive learning performance, demonstrating that a constrastive pair of CSI outperforms a constrastive pair of CSI and Passive WiFi Radar (PWR).\n\nThe rest of this paper is organised as follows: Section II provides an overview of related work. Section III describes the contrastive learning framework which is used as a pretraining process for HAR. The experimental setup and system parameters are described in Section IV. In Section V, we conduct several experiments to evaluate the effect of the pretraining process on the activity recognition performance (F 1 score) under different conditions, such as using different encoder networks, sampling in the fine-tuning phase, and modality pairing. Finally conclusions are drawn at the end of this paper.\n\n\nII. RELATED WORK\n\nIn our previous work [12], we performed a comprehensive comparative study on the similarities and differences between two WiFi sensing systems, namely, CSI and Passive WiFi Radar (PWR). More specifically, we performed a direct comparison between CSI and PWR by concurrently capturing signals reflected off the bodies of five participants while they performed six different activities, namely, sitting, standing from chair, laying down, standing from lie and picking up an object from the floor. For a fair comparison, the raw signals from both the PWR and CSI systems were converted into spectrograms using signal processing techniques. A simple supervised 2D CNN was used as the classifier consisting of one convolutional layer, one max-pooling layer and two fully connected layers. By considering data from three different physical layouts, consisting of a mixture of forward scatter (LoS), bistatic and monostatic layouts (NLoS), the CSI system achieved an overall accuracy of 67.3% while the PWR system had a similar accuracy at 66.7%. It was also shown that the CSI system had a better performance in LoS configurations (maximum accuracy of 90%), whereas the PWR system had better performance in bistatic configurations (maximum accuracy of 91.3%).\n\nA common problem in supervised deep learning is the need for large amount of labelled data to train the parameters in the model. Self-supervised learning [11] is an effective strategy that leverages a large amount of unlabelled data to train the network without the use of labels. Autoencoderbased pretraining is a popular approach in unsupervised learning that trains the encoder-decoder network to reconstruct a sample from its compressed form. This process is known as unsupervised pretraining, as the network learns a relevant representation from the data without the labels. To use this network for classification, the decoder of the network can be replaced with a classifier and trained under supervision, which is known as fine-tuning. With respect to WiFi and radar sensing, a number of works have shown that an autoencoder can be used for pretraining a deep neural network in applications such as localisation [13], user authentication [14] and activity recognition [15].\n\n\nIII. REPRESENTATION LEARNING WITH CONTRASTIVE\n\n\nLOSSES\n\nThe goal of contrastive learning is to build a better data representation via judicious design of auxiliary tasks (here contrastive losses). Crucially, the entire pipeline is completely automated ('no labels'). In essence, we are attempting to build the function h: X \u2192 Y , which maps the input data X to some latent compact representation space Y . The mapping should maximise the mutual information I(X; Y ), while attempting to minimise the size of |Y |. As a result, Y is made to capture the salient information about the data, while removing all spurious redundancy, so that subsequent classification tasks can be achieved relatively easily with just a handful of labelled examples (this is the the fine-tuning stage described below). However, direct maximisation of mutual information is a computationally intractable problem, and instead [16] shows that lower bound on MI can be maximised by minimizing the contrastive loss i.e. \u2212L cont < I (X; Y ).\n\nThe goal of contrastive loss L cont is to minimise the difference between the learned representations of positive pairs of data and maximise the difference between the negative pairs [11]. Generally, the positive pair consists of two samples of the same data point, which differ in some way, while the negative pair consists of two samples belonging to a different data point. For example, SimCLR [11] is a popular constrastive learning approach, originally proposed in computer vision under a self-supervised setting and has been shown to be very successful. Under the paradigm, a batch of images undergoes augmentation, such as rotation, crop and colouration. During the training, the two augmented samples originating from the same image are described as the positive pair, while the two augmented samples originating from different images are identified as the negative pair. Contrastive learning has been used to learn crossmodal representations of audio-visual information [17] and to learn spatio-temporal features of the scenes [18].\n\nIn terms of radar sensing, [19] introduced scalogram signal correspondence learning. A scalogram is generated from a raw signal, and the network learns to align it with the corresponding raw signal with contrastive objective. In the application of WiFi-CSI, the authors demonstrate its ability to improve the generalisation under semi-supervised setting, where a small amount of labelled data is used to fine-tune the pretrained network, however the improvement is not substantial. [20] examines eight strategies of contrastive learning in different activity recognition datasets. In the application of WiFi-CSI, the authors demonstrate that the networks trained with contrastive objectives have a competitive performance when compared to autoencoders.\n\n\nIV. CONTRASTIVE WIFI-BASED ACTIVITY RECOGNITION SYSTEM\n\nFor our contrastive WiFi activity recognition we propose to use the synchronised data that is collected simultaneously from different receivers, or views, as the pairs. We note that the synchronised data is also collected from different angles and with different modalities. The training objective is to encourage the representations of the synchronised data, the different views, to be closer in the embedded space. These views represent the identical semantics of the environment and the activity at a given time-point. We propose that by training the model constrastively in this way, the model learns the inherent features that are invariant to the noise involved in such systems. Further, we propose that this form of selfsupervision will better utilise the data collected and improve activity recognition performance using WiFi-CSI. For the CSI system, the raw physical layer CSI measurement is obtained from a commercial Network Interface Card (NIC) and stored for off-line processing. The CSI is the channel estimate which is used during the equalisation stage in the WiFi receiver to reverse the effects of the channel (e.g., multipath propagation, attenuation, phase shift, etc) on the transmitted signal. For a WiFi system with multiple transmitting and receiving antennas, the CSI is obtained as a matrix consisting of complex values for each Orthogonal Frequency Division Multiplexing (OFDM) subcarrier. Conversely, the raw WiFi signal in the PWR system is measured using a USRP platform, and is down-converted and digitised for real-time processing in a computing device. The PWR system correlates the transmitted signal and received signal to detect the Doppler shift and propagation delay. PWR uses a 'reference channel' to recover the transmitted signal, and several 'surveillance channels' to capture the reflected signals from different angles to provide spatial information. The signal processing pipeline which is used to convert the raw CSI and PWR data into spectrograms is fully described in our previous work [12]. We combine SimCLR [11] and the Contrastive Multiview Coding [21] to form our framework for the contrastive learning. Fig. 1 shows the overview of the pretraining contrastive learning stage. With the multi-view spectograms as input, we use the Normalised Temperature Cross-entropy (NT-Xent) proposed by [11] to calculate the contrastive loss, which works as follows. As we train with mini-batches, we obtain 2N projections that result from applying a projection network to our embeddings of each pair, from each mini-batch with N samples with two views. We then form positive pairs if the two projections originate from the same time point but different views, and negative pairs otherwise. Under the mini-batch, each projection forms exactly one positive pair and 2N \u2212 1 negative pairs. To calculate the loss, we first compute the pairwise similarity s for every available pair of projections z i and z j as follows:\ns i,j = z T i \u00b7 z j ||z i ||||z j || .(1)\nWe apply an exponential function on each pair, and divide the positive pair by the sum of the negative pairs. Then we take the negative logarithm to obtain the cross entropy, which is expressed as: where \u03c4 is the softmax temperature. Once we obtain the contrastive losses, L cont i , for each projection in the mini-batch, we take the mean value of the losses to backpropagate through the networks. After pretraining, we discard the projection heads, freeze the encoders, add a classification network on top of the encoders and fine-tune them with labelled activity samples.\nL cont i = \u2212 log exp(s i,j /\u03c4 ) 2N k=1[k =i] exp(s i,j /\u03c4 ) ,(2)\n\nV. EXPERIMENTS\n\nDuring the data collection, we used two WiFi CSI receivers (primary and secondary) and one PWR receiver to capture the WiFi signal emitted from a WiFi access point (transmitter). The working principles of the CSI and PWR systems are thoroughly described in [12]. Locations of the receivers are varied based on the three layouts as depicted in Fig. 2.\n\nNine positions were tested during the experiment inside the monitoring area, with 1.5m separation between the positions. The secondary WiFi CSI receiver was always located at 90 0 to the primary CSI receiver in a bistatic geometry, while the PWR receiver and primary CSI receiver are collocated in each layout. Both CSI receivers consist of an Intel 5300 Network Interface Card (NIC) [22], and the PWR receiver is implemented using a Software Defined-Radio (SDR) platform. Both systems operated in the 2.4 GHz WiFi band.\n\nWe collected the CSI and PWR data for seven different activities: laying down, picking something up from the ground (pickup), sitting (sit), standing from sit (stand), standing from the floor (standff), walking (walk) and waving, across five human subjects. Readers are referred to our previous work [12] for an in-depth description of the signal processing steps applied to the raw data from the CSI and PWR systems to obtain spectrograms. These serve as input to our contrastive learning system. As we consider different layouts and positions (consisting of a mixture of forward scatter (LoS), bistatic and monostatic configurations (NLoS)) that affect the multipath effect considerably, this is more challenging than other studies which have only considered a single optimum layout. \n\n\nA. Contrastive Model Details\n\nOur system is implemented in PyTorch. During experiments we test three popular CNN architectures as the encoder: AlexNet [23], VGG16 [24] and ResNet18 [25] and one shallow network to analyse the effects of model architecture on the contrastive learning performance. The shallow network consists of three convolutional layers with 32, 64 and 96 filters, respectively. Each convolutional layer is followed by a batch normalisation layer and a max pooling layer. ReLU is used as the activation function for the first two layers while Tanh is used as the activation function for the third convolutional layer. The dimensionality of the CSI and PWR spectrogram data is 65x501 and 100x41, respectively. Because our input data sizes are comparatively smaller than typical image sizes used in the above-mentioned networks, we upscale our input spectrogram data by a factor of two or three using 2D nearest neighbour upsampling. Finally, the classifier is multi-layer perceptron with two hidden layers consisting of 128 and 7 neurons, respectively, while the projection head is a linear layer with a size of 128 neurons.\n\n\nB. Training and Evaluation\n\nWe randomly select 80% of the samples in the dataset as the training set and the remaining 20% is used to evaluate the model. All of the training set was used in the pretraining phase for contrastive learning. In addition to this, we also used subsets of the dataset with one, five and ten labelled examples per class for fine-tuning to evaluate the few-shot learning capability of the model. Due to the class imbalance in the HAR dataset, we use the macro averaged F 1 score for the majority of the evaluation.\n\n\nVI. RESULTS\n\nWe first present the results in Fig. 5 which compares the macro averaged F 1 validation score of the proposed contrastive learning based model and the baseline, which is an Fig. 4.\n\nA confusion matrix demonstrating the performance of the contrastively-trained system. Improvements in performance can be seen across all activities. AlexNet-based CNN without the contrastive pretraining step, over 200 epochs. Both approaches are based on an AlexNet architecture, and consider as input WiFi CSI from two different receivers. From this it is clear that the contrastively trained system outperforms a similar system without the contrastive pretraining step. Figures 3 and 4 show the confusion matrices of the noncontrastively and contrastively pretrained systems, representing final F 1 scores of 57.9% and 75.6%, respectively. In terms of the accuracy of the individual activity, the activity 'walk' achieved the smallest improvement of 4.0%, whereas the activity 'standff' obtains the largest improvement of 37.5%, followed by 'lay' (33.3% increase) and 'sit' (25.0% increase). Overall, the introduction of contrastive learning lead to a performance increase in the accuracy of 11.7% and F 1 score of 17.7%.\n\nTo further evaluate the effectiveness of our contrastive learning system, we compare it with a non-contrastively trained version under different training regimes, as illustrated Fig. 6. A comparison of the contrastively trained approach, in terms of macro F 1 score, with two versions of the non-contrastively trained approach. The 'Non-Contrastive' approach uses only data from a single CSI receiver (CSI-1), while 'Non-Contrastive (Joint Dataset)' uses data from both CSI receivers (CSI-1 and CSI-2) but is still non-contrastively trained. in Fig. 6. In this experiment, 'Non-Contrastive' refers to CSI data from the primary receiver (CSI-1) only while 'Non-Constastive (Joint Dataset)' refers to CSI data from both receivers (CSI-1 and CSI-2) but is still not contrastively trained. It can be observed from this figure that while the inclusion of the second CSI receiver slightly improves performance under a normal supervised training regime, contrastive pretraining leads to a more significant improvement in performance.\n\n\nA. Sample Efficiency\n\nNext, we evaluate the performance of the system in oneshot and few-shot activity recognition scenarios. The results of this can be seen in Fig. 7, where the number of labelled samples in the classification stage was reduced to one, five and ten examples of each activity. The results demonstrate that the use of contrastive pretraining on synchronised WiFi CSI from multiple receivers can significantly improve activity recognition performance in one-and few-shot learning scenarios.   architectures. From this, we can observe that the difference in F 1 score between non-pretrained and pretrained models is largest with an AlexNet-based CNN encoder. Furthermore, the same encoder also achieves the best overall activity recognition performance. On the other hand, little to no gain in performance can be observed with a 'shallow' CNN based encoder, which has less total parameters than either the projection head and the classifier, as well as a ResNet based encoder. Further investigation of this is left for future work.\n\n\nB. Encoder Architectures\n\n\nC. Modalities\n\nFinally, we study the effect of the type of modality pairing on the contrastive learning performance. The proposed contrastively pretrained WiFI HAR system uses data from the same modality to construct pairs, that is, a positive pair consists of CSI data from the two synchronised CSI receivers, CSI-1 and CSI-2. In our experiments, we also collected Passive WiFi Radar (PWR) data, as can be seen in Fig.  2. Thus, we test the performance of the same contrastively pretrained system, but by constructing pairs of WiFI CSI (CSI-1) and PWR data.\n\nThere was a significant difference in the macro averaged F 1 score, as shown in Fig. 9. This experiment shows that contrastively training on synchronised CSI pairs outperforms training on CSI PWR pairs.\n\n\nVII. CONCLUSIONS\n\nIn this work, we propose a system which uses contrastive learning on WiFi data to improve activity recognition performance. Specifically, the system utilises multiple views from synchronised receivers for contrastive learning. We evaluate the performance of our proposed model using experimental data consisting of seven activities recorded from five human participants in different receiver layouts and positions. Through experimental evaluations we show significant improvement in the activity recognition performance using self-supervised contrastive learning when compared to conventional supervised models. Specifically, contrastively pretraining with an AlexNet-based encoder lead to a 17.7% increase in macro averaged F 1 score. We also evaluated the proposed approach in one-and few-shot learning scenarios, observing contrastive learning with WiFi CSI pairs from synchronised receivers leads to a significant improvement in activity recognition performance.\n\nFig. 2 .\n2Layout used for the experiment.\n\nFig. 3 .\n3A confusion matrix demonstrating the performance of the noncontrastively trained system.\n\nFig. 5 .\n5A comparison of the validation F 1 score (macro averaged) over 200 epochs between the non-contrastively-trained system and the contrastivelytrained system.\n\nFig. 7 .\n7A comparison of the performance of the contrastively trained system and the non-contrastively trained system in scenarios with limited amounts of labelled data.\n\nFig. 8\n8compares the performance of the non-contrastive and contrastively pretrained models with different encoder\n\nFig. 8 .\n8A comparison of the effect of contrastive pretraining using different encoder architectures.\n\nFig. 9 .\n9A comparison of the effect of the design of pairs on contrastive learning performance. The proposed system uses two pairs of CSI (CSI-1 and CSI-2), as it outperforms the same approach when constructing pairs from CSI and PWR.\nACKNOWLEDGEMENTSThis work was performed as a part of the OPERA Project, funded by the UK Engineering and Physical Sciences Research Council (EPSRC), Grant EP/R018677/1.\nVesta: A digital health analytics platform for a smart home in a box. R Mcconville, G Archer, I Craddock, M Koz\u0142owski, R Piechocki, J Pope, R Santos-Rodriguez, Future Generation Computer Systems. 114R. McConville, G. Archer, I. Craddock, M. Koz\u0142owski, R. Piechocki, J. Pope, and R. Santos-Rodriguez, \"Vesta: A digital health analytics platform for a smart home in a box,\" Future Generation Computer Systems, vol. 114, pp. 106 -119, 2021.\n\nA comprehensive study of activity recognition using accelerometers. N Twomey, T Diethe, X Fafoutis, A Elsts, R Mcconville, P Flach, I Craddock, Informatics. 52N. Twomey, T. Diethe, X. Fafoutis, A. Elsts, R. McConville, P. Flach, and I. Craddock, \"A comprehensive study of activity recognition using accelerometers,\" Informatics, vol. 5, no. 2, 6 2018.\n\nWiFi sensing with channel state information. Y Ma, G Zhou, S Wang, ACM Computing Surveys. 523Y. Ma, G. Zhou, and S. Wang, \"WiFi sensing with channel state information,\" ACM Computing Surveys, vol. 52, no. 3, pp. 1-36, jul 2019.\n\nDeep learning for radio-based human sensing: Recent advances and future directions. I Nirmal, A Khamis, M Hassan, W Hu, X Zhu, CoRR. I. Nirmal, A. Khamis, M. Hassan, W. Hu, and X. Zhu, \"Deep learning for radio-based human sensing: Recent advances and future directions,\" CoRR, vol. abs/2010.12717, 2020. [Online]. Available: https://arxiv.org/abs/2010.12717\n\nQualitative action recognition by wireless radio signals in human-machine systems. S Lv, Y Lu, M Dong, X Wang, Y Dou, W Zhuang, IEEE Transactions on Human-Machine Systems. 476S. Lv, Y. Lu, M. Dong, X. Wang, Y. Dou, and W. Zhuang, \"Qualitative action recognition by wireless radio signals in human-machine sys- tems,\" IEEE Transactions on Human-Machine Systems, vol. 47, no. 6, pp. 789-800, 2017.\n\nMaking the invisible visible: Action recognition through walls and occlusions. T Li, L Fan, M Zhao, Y Liu, D Katabi, 2019T. Li, L. Fan, M. Zhao, Y. Liu, and D. Katabi, \"Making the invisible visible: Action recognition through walls and occlusions,\" in 2019\n\nIEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South). IEEEIEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -November 2, 2019. IEEE, 2019, pp. 872-881.\n\nTranslation resilient opportunistic wifi sensing. M Bocus, W Li, J Paulavicius, R Mcconville, R Santos-Rodriguez, K Chetty, R Piechocki, 25th International Conference on Pattern Recognition (ICPR2020). United States: Institute of Electrical and Electronics Engineers. IEEEM. Bocus, W. Li, J. Paulavicius, R. McConville, R. Santos-Rodriguez, K. Chetty, and R. Piechocki, \"Translation resilient opportunistic wifi sensing,\" in 25th International Conference on Pattern Recognition (ICPR2020). United States: Institute of Electrical and Electronics Engineers (IEEE), Jun. 2020.\n\nWireless localisation in wifi using novel deep architectures. P Li, H Cui, A Khan, U Raza, R J Piechocki, A Doufexi, T Farnham, abs/2010.08658CoRR. P. Li, H. Cui, A. Khan, U. Raza, R. J. Piechocki, A. Doufexi, and T. Farnham, \"Wireless localisation in wifi using novel deep architectures,\" CoRR, vol. abs/2010.08658, 2020. [Online]. Available: https://arxiv.org/abs/2010.08658\n\nPractical device-free gesture recognition using WiFi signals based on metalearning. X Ma, Y Zhao, L Zhang, Q Gao, M Pan, J Wang, IEEE Transactions on Industrial Informatics. 161X. Ma, Y. Zhao, L. Zhang, Q. Gao, M. Pan, and J. Wang, \"Practical device-free gesture recognition using WiFi signals based on metalearn- ing,\" IEEE Transactions on Industrial Informatics, vol. 16, no. 1, pp. 228-237, jan 2020.\n\nTowards environment independent device free human activity recognition. W Jiang, C Miao, F Ma, S Yao, Y Wang, Y Yuan, H Xue, C Song, X Ma, D Koutsonikolas, W Xu, L Su, Proceedings of the 24th Annual International Conference on Mobile Computing and Networking, ser. MobiCom '18. the 24th Annual International Conference on Mobile Computing and Networking, ser. MobiCom '18New York, NY, USAAssociation for Computing MachineryW. Jiang, C. Miao, F. Ma, S. Yao, Y. Wang, Y. Yuan, H. Xue, C. Song, X. Ma, D. Koutsonikolas, W. Xu, and L. Su, \"Towards environment independent device free human activity recognition,\" in Proceedings of the 24th Annual International Conference on Mobile Computing and Networking, ser. MobiCom '18. New York, NY, USA: Association for Computing Machinery, 2018, p. 289-304.\n\nA simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G E Hinton, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR2020Proceedings of Machine Learning ResearchT. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton, \"A simple frame- work for contrastive learning of visual representations,\" in Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, ser. Proceedings of Machine Learning Research, vol. 119. PMLR, 2020, pp. 1597-1607.\n\nA taxonomy of wifi sensing: Csi vs passive wifi radar. W Li, M Bocus, C Tang, S Vishwakarma, R Piechocki, K Woodbridge, K Chetty, IEEE Global Communications (GLOBECOM) Conference Workshop 2020 on Integrated Sensing And Communication. Taipei, Taiwan; United StatesIEEEW. Li, M. Bocus, C. Tang, S. Vishwakarma, R. Piechocki, K. Wood- bridge, and K. Chetty, \"A taxonomy of wifi sensing: Csi vs passive wifi radar,\" in IEEE Global Communications (GLOBECOM) Conference Workshop 2020 on Integrated Sensing And Communication, Taipei, Taiwan, 7-11 Dec. 2020. United States: Institute of Electrical and Electronics Engineers (IEEE), Sep. 2020, pp. 1-6.\n\nAn accurate and robust approach of device-free localization with convolutional autoencoder. L Zhao, H Huang, X Li, S Ding, H Zhao, Z Han, IEEE Internet of Things Journal. 63L. Zhao, H. Huang, X. Li, S. Ding, H. Zhao, and Z. Han, \"An accurate and robust approach of device-free localization with convolutional autoencoder,\" IEEE Internet of Things Journal, vol. 6, no. 3, pp. 5825- 5840, 2019.\n\nSmart user authentication through actuation of daily activities leveraging WiFi-enabled IoT. C Shi, J Liu, H Liu, Y Chen, Proceedings of the 18th ACM International Symposium on Mobile Ad Hoc Networking and Computing. the 18th ACM International Symposium on Mobile Ad Hoc Networking and ComputingACMC. Shi, J. Liu, H. Liu, and Y. Chen, \"Smart user authentication through actuation of daily activities leveraging WiFi-enabled IoT,\" in Proceedings of the 18th ACM International Symposium on Mobile Ad Hoc Networking and Computing. ACM, jul 2017.\n\nCSI-based devicefree wireless localization and activity recognition using radio image features. Q Gao, J Wang, X Ma, X Feng, H Wang, IEEE Transactions on Vehicular Technology. 6611Q. Gao, J. Wang, X. Ma, X. Feng, and H. Wang, \"CSI-based device- free wireless localization and activity recognition using radio image features,\" IEEE Transactions on Vehicular Technology, vol. 66, no. 11, pp. 10 346-10 356, nov 2017.\n\nRepresentation learning with contrastive predictive coding. A Van Den Oord, Y Li, O Vinyals, abs/1807.03748CoRR. A. van den Oord, Y. Li, and O. Vinyals, \"Representation learning with contrastive predictive coding,\" CoRR, vol. abs/1807.03748, 2018. [Online]. Available: http://arxiv.org/abs/1807.03748\n\nProbing contextual language models for common ground with visual representations. G Ilharco, R Zellers, A Farhadi, H Hajishirzi, CoRR. G. Ilharco, R. Zellers, A. Farhadi, and H. Hajishirzi, \"Probing contextual language models for common ground with visual representations,\" CoRR, vol. abs/2005.00619, 2021.\n\nTime-contrastive networks: Self-supervised learning from video. P Sermanet, C Lynch, Y Chebotar, J Hsu, E Jang, S Schaal, S Levine, G Brain, 2018 IEEE International Conference on Robotics and Automation (ICRA). P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine, and G. Brain, \"Time-contrastive networks: Self-supervised learning from video,\" in 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018, pp. 1134-1141.\n\nFederated selfsupervised learning of multisensor representations for embedded intelligence. A Saeed, F D Salim, T Ozcelebi, J Lukkien, IEEE Internet of Things Journal. 82A. Saeed, F. D. Salim, T. Ozcelebi, and J. Lukkien, \"Federated self- supervised learning of multisensor representations for embedded intel- ligence,\" IEEE Internet of Things Journal, vol. 8, no. 2, pp. 1030-1040, jan 2021.\n\nSense and learn: Selfsupervision for omnipresent sensors. A Saeed, V Ungureanu, B Gfeller, CoRR. A. Saeed, V. Ungureanu, and B. Gfeller, \"Sense and learn: Self- supervision for omnipresent sensors,\" CoRR, vol. abs/2009.13233, 2020. [Online]. Available: https://arxiv.org/abs/2009.13233\n\nContrastive multiview coding. Y Tian, D Krishnan, P Isola, Proceedings, Part XI, ser. Lecture Notes in Computer Science. A. Vedaldi, H. Bischof, T. Brox, and J. FrahmPart XI, ser. Lecture Notes in Computer ScienceGlasgow, UKSpringer12356Computer Vision -ECCV 2020 -16th European ConferenceY. Tian, D. Krishnan, and P. Isola, \"Contrastive multiview coding,\" in Computer Vision -ECCV 2020 -16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI, ser. Lecture Notes in Computer Science, A. Vedaldi, H. Bischof, T. Brox, and J. Frahm, Eds., vol. 12356. Springer, 2020, pp. 776-794.\n\nTool release: Gathering 802.11n traces with channel state information. D Halperin, W Hu, A Sheth, D Wetherall, ACM SIGCOMM CCR. 41153D. Halperin, W. Hu, A. Sheth, and D. Wetherall, \"Tool release: Gath- ering 802.11n traces with channel state information,\" ACM SIGCOMM CCR, vol. 41, no. 1, p. 53, Jan. 2011.\n\nOne weird trick for parallelizing convolutional neural networks. A Krizhevsky, abs/1404.5997CoRR. A. Krizhevsky, \"One weird trick for parallelizing convolutional neural networks,\" CoRR, vol. abs/1404.5997, 2014. [Online]. Available: http://arxiv.org/abs/1404.5997\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsK. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, 2016, pp. 770-778.\n", "annotations": {"author": "[{\"end\":179,\"start\":51},{\"end\":341,\"start\":180},{\"end\":473,\"start\":342},{\"end\":607,\"start\":474},{\"end\":744,\"start\":608}]", "publisher": null, "author_last_name": "[{\"end\":64,\"start\":61},{\"end\":195,\"start\":185},{\"end\":358,\"start\":353},{\"end\":492,\"start\":483},{\"end\":629,\"start\":613}]", "author_first_name": "[{\"end\":60,\"start\":51},{\"end\":184,\"start\":180},{\"end\":350,\"start\":342},{\"end\":352,\"start\":351},{\"end\":480,\"start\":474},{\"end\":482,\"start\":481},{\"end\":612,\"start\":608}]", "author_affiliation": "[{\"end\":178,\"start\":66},{\"end\":340,\"start\":228},{\"end\":472,\"start\":360},{\"end\":606,\"start\":494},{\"end\":743,\"start\":631}]", "title": "[{\"end\":48,\"start\":1},{\"end\":792,\"start\":745}]", "venue": null, "abstract": "[{\"end\":1831,\"start\":794}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2198,\"start\":2195},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2400,\"start\":2397},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2680,\"start\":2677},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2957,\"start\":2954},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3379,\"start\":3376},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3384,\"start\":3381},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3402,\"start\":3399},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3431,\"start\":3428},{\"end\":3652,\"start\":3648},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4526,\"start\":4522},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4674,\"start\":4670},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6381,\"start\":6377},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7769,\"start\":7765},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8534,\"start\":8530},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8560,\"start\":8556},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8590,\"start\":8586},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9499,\"start\":9495},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9795,\"start\":9791},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10009,\"start\":10005},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10591,\"start\":10587},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10648,\"start\":10644},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10682,\"start\":10678},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11137,\"start\":11133},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13500,\"start\":13496},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13524,\"start\":13520},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13566,\"start\":13562},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13808,\"start\":13804},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15379,\"start\":15375},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15858,\"start\":15854},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16296,\"start\":16292},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16936,\"start\":16932},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16948,\"start\":16944},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16966,\"start\":16962},{\"end\":19871,\"start\":19865}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23583,\"start\":23541},{\"attributes\":{\"id\":\"fig_1\"},\"end\":23683,\"start\":23584},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23850,\"start\":23684},{\"attributes\":{\"id\":\"fig_3\"},\"end\":24022,\"start\":23851},{\"attributes\":{\"id\":\"fig_4\"},\"end\":24138,\"start\":24023},{\"attributes\":{\"id\":\"fig_5\"},\"end\":24242,\"start\":24139},{\"attributes\":{\"id\":\"fig_6\"},\"end\":24479,\"start\":24243}]", "paragraph": "[{\"end\":2872,\"start\":1850},{\"end\":3276,\"start\":2874},{\"end\":3788,\"start\":3278},{\"end\":5020,\"start\":3842},{\"end\":5076,\"start\":5022},{\"end\":5728,\"start\":5078},{\"end\":6335,\"start\":5730},{\"end\":7609,\"start\":6356},{\"end\":8591,\"start\":7611},{\"end\":9606,\"start\":8650},{\"end\":10649,\"start\":9608},{\"end\":11403,\"start\":10651},{\"end\":14418,\"start\":11462},{\"end\":15035,\"start\":14461},{\"end\":15468,\"start\":15118},{\"end\":15990,\"start\":15470},{\"end\":16778,\"start\":15992},{\"end\":17922,\"start\":16811},{\"end\":18464,\"start\":17953},{\"end\":18660,\"start\":18480},{\"end\":19685,\"start\":18662},{\"end\":20713,\"start\":19687},{\"end\":21761,\"start\":20738},{\"end\":22349,\"start\":21806},{\"end\":22553,\"start\":22351},{\"end\":23540,\"start\":22574}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3841,\"start\":3789},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14460,\"start\":14419},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15100,\"start\":15036}]", "table_ref": null, "section_header": "[{\"end\":1848,\"start\":1833},{\"end\":6354,\"start\":6338},{\"end\":8639,\"start\":8594},{\"end\":8648,\"start\":8642},{\"end\":11460,\"start\":11406},{\"end\":15116,\"start\":15102},{\"end\":16809,\"start\":16781},{\"end\":17951,\"start\":17925},{\"end\":18478,\"start\":18467},{\"end\":20736,\"start\":20716},{\"end\":21788,\"start\":21764},{\"end\":21804,\"start\":21791},{\"end\":22572,\"start\":22556},{\"end\":23550,\"start\":23542},{\"end\":23593,\"start\":23585},{\"end\":23693,\"start\":23685},{\"end\":23860,\"start\":23852},{\"end\":24030,\"start\":24024},{\"end\":24148,\"start\":24140},{\"end\":24252,\"start\":24244}]", "table": null, "figure_caption": "[{\"end\":23583,\"start\":23552},{\"end\":23683,\"start\":23595},{\"end\":23850,\"start\":23695},{\"end\":24022,\"start\":23862},{\"end\":24138,\"start\":24032},{\"end\":24242,\"start\":24150},{\"end\":24479,\"start\":24254}]", "figure_ref": "[{\"end\":13625,\"start\":13619},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15467,\"start\":15461},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18518,\"start\":18512},{\"end\":18659,\"start\":18653},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19149,\"start\":19134},{\"end\":20238,\"start\":20232},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20883,\"start\":20877},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22213,\"start\":22206},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22437,\"start\":22431}]", "bib_author_first_name": "[{\"end\":24720,\"start\":24719},{\"end\":24734,\"start\":24733},{\"end\":24744,\"start\":24743},{\"end\":24756,\"start\":24755},{\"end\":24769,\"start\":24768},{\"end\":24782,\"start\":24781},{\"end\":24790,\"start\":24789},{\"end\":25157,\"start\":25156},{\"end\":25167,\"start\":25166},{\"end\":25177,\"start\":25176},{\"end\":25189,\"start\":25188},{\"end\":25198,\"start\":25197},{\"end\":25212,\"start\":25211},{\"end\":25221,\"start\":25220},{\"end\":25487,\"start\":25486},{\"end\":25493,\"start\":25492},{\"end\":25501,\"start\":25500},{\"end\":25755,\"start\":25754},{\"end\":25765,\"start\":25764},{\"end\":25775,\"start\":25774},{\"end\":25785,\"start\":25784},{\"end\":25791,\"start\":25790},{\"end\":26113,\"start\":26112},{\"end\":26119,\"start\":26118},{\"end\":26125,\"start\":26124},{\"end\":26133,\"start\":26132},{\"end\":26141,\"start\":26140},{\"end\":26148,\"start\":26147},{\"end\":26506,\"start\":26505},{\"end\":26512,\"start\":26511},{\"end\":26519,\"start\":26518},{\"end\":26527,\"start\":26526},{\"end\":26534,\"start\":26533},{\"end\":26969,\"start\":26968},{\"end\":26978,\"start\":26977},{\"end\":26984,\"start\":26983},{\"end\":26999,\"start\":26998},{\"end\":27013,\"start\":27012},{\"end\":27033,\"start\":27032},{\"end\":27043,\"start\":27042},{\"end\":27556,\"start\":27555},{\"end\":27562,\"start\":27561},{\"end\":27569,\"start\":27568},{\"end\":27577,\"start\":27576},{\"end\":27585,\"start\":27584},{\"end\":27587,\"start\":27586},{\"end\":27600,\"start\":27599},{\"end\":27611,\"start\":27610},{\"end\":27956,\"start\":27955},{\"end\":27962,\"start\":27961},{\"end\":27970,\"start\":27969},{\"end\":27979,\"start\":27978},{\"end\":27986,\"start\":27985},{\"end\":27993,\"start\":27992},{\"end\":28349,\"start\":28348},{\"end\":28358,\"start\":28357},{\"end\":28366,\"start\":28365},{\"end\":28372,\"start\":28371},{\"end\":28379,\"start\":28378},{\"end\":28387,\"start\":28386},{\"end\":28395,\"start\":28394},{\"end\":28402,\"start\":28401},{\"end\":28410,\"start\":28409},{\"end\":28416,\"start\":28415},{\"end\":28433,\"start\":28432},{\"end\":28439,\"start\":28438},{\"end\":29145,\"start\":29144},{\"end\":29153,\"start\":29152},{\"end\":29166,\"start\":29165},{\"end\":29177,\"start\":29176},{\"end\":29179,\"start\":29178},{\"end\":29744,\"start\":29743},{\"end\":29750,\"start\":29749},{\"end\":29759,\"start\":29758},{\"end\":29767,\"start\":29766},{\"end\":29782,\"start\":29781},{\"end\":29795,\"start\":29794},{\"end\":29809,\"start\":29808},{\"end\":30426,\"start\":30425},{\"end\":30434,\"start\":30433},{\"end\":30443,\"start\":30442},{\"end\":30449,\"start\":30448},{\"end\":30457,\"start\":30456},{\"end\":30465,\"start\":30464},{\"end\":30821,\"start\":30820},{\"end\":30828,\"start\":30827},{\"end\":30835,\"start\":30834},{\"end\":30842,\"start\":30841},{\"end\":31368,\"start\":31367},{\"end\":31375,\"start\":31374},{\"end\":31383,\"start\":31382},{\"end\":31389,\"start\":31388},{\"end\":31397,\"start\":31396},{\"end\":31748,\"start\":31747},{\"end\":31764,\"start\":31763},{\"end\":31770,\"start\":31769},{\"end\":32072,\"start\":32071},{\"end\":32083,\"start\":32082},{\"end\":32094,\"start\":32093},{\"end\":32105,\"start\":32104},{\"end\":32362,\"start\":32361},{\"end\":32374,\"start\":32373},{\"end\":32383,\"start\":32382},{\"end\":32395,\"start\":32394},{\"end\":32402,\"start\":32401},{\"end\":32410,\"start\":32409},{\"end\":32420,\"start\":32419},{\"end\":32430,\"start\":32429},{\"end\":32851,\"start\":32850},{\"end\":32860,\"start\":32859},{\"end\":32862,\"start\":32861},{\"end\":32871,\"start\":32870},{\"end\":32883,\"start\":32882},{\"end\":33211,\"start\":33210},{\"end\":33220,\"start\":33219},{\"end\":33233,\"start\":33232},{\"end\":33470,\"start\":33469},{\"end\":33478,\"start\":33477},{\"end\":33490,\"start\":33489},{\"end\":34115,\"start\":34114},{\"end\":34127,\"start\":34126},{\"end\":34133,\"start\":34132},{\"end\":34142,\"start\":34141},{\"end\":34417,\"start\":34416},{\"end\":34685,\"start\":34684},{\"end\":34697,\"start\":34696},{\"end\":35104,\"start\":35103},{\"end\":35110,\"start\":35109},{\"end\":35119,\"start\":35118},{\"end\":35126,\"start\":35125}]", "bib_author_last_name": "[{\"end\":24731,\"start\":24721},{\"end\":24741,\"start\":24735},{\"end\":24753,\"start\":24745},{\"end\":24766,\"start\":24757},{\"end\":24779,\"start\":24770},{\"end\":24787,\"start\":24783},{\"end\":24807,\"start\":24791},{\"end\":25164,\"start\":25158},{\"end\":25174,\"start\":25168},{\"end\":25186,\"start\":25178},{\"end\":25195,\"start\":25190},{\"end\":25209,\"start\":25199},{\"end\":25218,\"start\":25213},{\"end\":25230,\"start\":25222},{\"end\":25490,\"start\":25488},{\"end\":25498,\"start\":25494},{\"end\":25506,\"start\":25502},{\"end\":25762,\"start\":25756},{\"end\":25772,\"start\":25766},{\"end\":25782,\"start\":25776},{\"end\":25788,\"start\":25786},{\"end\":25795,\"start\":25792},{\"end\":26116,\"start\":26114},{\"end\":26122,\"start\":26120},{\"end\":26130,\"start\":26126},{\"end\":26138,\"start\":26134},{\"end\":26145,\"start\":26142},{\"end\":26155,\"start\":26149},{\"end\":26509,\"start\":26507},{\"end\":26516,\"start\":26513},{\"end\":26524,\"start\":26520},{\"end\":26531,\"start\":26528},{\"end\":26541,\"start\":26535},{\"end\":26975,\"start\":26970},{\"end\":26981,\"start\":26979},{\"end\":26996,\"start\":26985},{\"end\":27010,\"start\":27000},{\"end\":27030,\"start\":27014},{\"end\":27040,\"start\":27034},{\"end\":27053,\"start\":27044},{\"end\":27559,\"start\":27557},{\"end\":27566,\"start\":27563},{\"end\":27574,\"start\":27570},{\"end\":27582,\"start\":27578},{\"end\":27597,\"start\":27588},{\"end\":27608,\"start\":27601},{\"end\":27619,\"start\":27612},{\"end\":27959,\"start\":27957},{\"end\":27967,\"start\":27963},{\"end\":27976,\"start\":27971},{\"end\":27983,\"start\":27980},{\"end\":27990,\"start\":27987},{\"end\":27998,\"start\":27994},{\"end\":28355,\"start\":28350},{\"end\":28363,\"start\":28359},{\"end\":28369,\"start\":28367},{\"end\":28376,\"start\":28373},{\"end\":28384,\"start\":28380},{\"end\":28392,\"start\":28388},{\"end\":28399,\"start\":28396},{\"end\":28407,\"start\":28403},{\"end\":28413,\"start\":28411},{\"end\":28430,\"start\":28417},{\"end\":28436,\"start\":28434},{\"end\":28442,\"start\":28440},{\"end\":29150,\"start\":29146},{\"end\":29163,\"start\":29154},{\"end\":29174,\"start\":29167},{\"end\":29186,\"start\":29180},{\"end\":29747,\"start\":29745},{\"end\":29756,\"start\":29751},{\"end\":29764,\"start\":29760},{\"end\":29779,\"start\":29768},{\"end\":29792,\"start\":29783},{\"end\":29806,\"start\":29796},{\"end\":29816,\"start\":29810},{\"end\":30431,\"start\":30427},{\"end\":30440,\"start\":30435},{\"end\":30446,\"start\":30444},{\"end\":30454,\"start\":30450},{\"end\":30462,\"start\":30458},{\"end\":30469,\"start\":30466},{\"end\":30825,\"start\":30822},{\"end\":30832,\"start\":30829},{\"end\":30839,\"start\":30836},{\"end\":30847,\"start\":30843},{\"end\":31372,\"start\":31369},{\"end\":31380,\"start\":31376},{\"end\":31386,\"start\":31384},{\"end\":31394,\"start\":31390},{\"end\":31402,\"start\":31398},{\"end\":31761,\"start\":31749},{\"end\":31767,\"start\":31765},{\"end\":31778,\"start\":31771},{\"end\":32080,\"start\":32073},{\"end\":32091,\"start\":32084},{\"end\":32102,\"start\":32095},{\"end\":32116,\"start\":32106},{\"end\":32371,\"start\":32363},{\"end\":32380,\"start\":32375},{\"end\":32392,\"start\":32384},{\"end\":32399,\"start\":32396},{\"end\":32407,\"start\":32403},{\"end\":32417,\"start\":32411},{\"end\":32427,\"start\":32421},{\"end\":32436,\"start\":32431},{\"end\":32857,\"start\":32852},{\"end\":32868,\"start\":32863},{\"end\":32880,\"start\":32872},{\"end\":32891,\"start\":32884},{\"end\":33217,\"start\":33212},{\"end\":33230,\"start\":33221},{\"end\":33241,\"start\":33234},{\"end\":33475,\"start\":33471},{\"end\":33487,\"start\":33479},{\"end\":33496,\"start\":33491},{\"end\":34124,\"start\":34116},{\"end\":34130,\"start\":34128},{\"end\":34139,\"start\":34134},{\"end\":34152,\"start\":34143},{\"end\":34428,\"start\":34418},{\"end\":34694,\"start\":34686},{\"end\":34707,\"start\":34698},{\"end\":35107,\"start\":35105},{\"end\":35116,\"start\":35111},{\"end\":35123,\"start\":35120},{\"end\":35130,\"start\":35127}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":225025209},\"end\":25086,\"start\":24649},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3997714},\"end\":25439,\"start\":25088},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":85547734},\"end\":25668,\"start\":25441},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":225068188},\"end\":26027,\"start\":25670},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4779404},\"end\":26424,\"start\":26029},{\"attributes\":{\"id\":\"b5\"},\"end\":26682,\"start\":26426},{\"attributes\":{\"id\":\"b6\"},\"end\":26916,\"start\":26684},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":225742649},\"end\":27491,\"start\":26918},{\"attributes\":{\"doi\":\"abs/2010.08658\",\"id\":\"b8\",\"matched_paper_id\":224706368},\"end\":27869,\"start\":27493},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":145999160},\"end\":28274,\"start\":27871},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":51919046},\"end\":29071,\"start\":28276},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":211096730},\"end\":29686,\"start\":29073},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":225214050},\"end\":30331,\"start\":29688},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":108366454},\"end\":30725,\"start\":30333},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":21045949},\"end\":31269,\"start\":30727},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3710372},\"end\":31685,\"start\":31271},{\"attributes\":{\"doi\":\"abs/1807.03748\",\"id\":\"b16\",\"matched_paper_id\":49670925},\"end\":31987,\"start\":31687},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":225086076},\"end\":32295,\"start\":31989},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3997350},\"end\":32756,\"start\":32297},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220793490},\"end\":33150,\"start\":32758},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":221970620},\"end\":33437,\"start\":33152},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":189762205},\"end\":34041,\"start\":33439},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":13561174},\"end\":34349,\"start\":34043},{\"attributes\":{\"doi\":\"abs/1404.5997\",\"id\":\"b23\",\"matched_paper_id\":5556470},\"end\":34614,\"start\":34351},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14124313},\"end\":35055,\"start\":34616},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206594692},\"end\":35490,\"start\":35057}]", "bib_title": "[{\"end\":24717,\"start\":24649},{\"end\":25154,\"start\":25088},{\"end\":25484,\"start\":25441},{\"end\":25752,\"start\":25670},{\"end\":26110,\"start\":26029},{\"end\":26966,\"start\":26918},{\"end\":27553,\"start\":27493},{\"end\":27953,\"start\":27871},{\"end\":28346,\"start\":28276},{\"end\":29142,\"start\":29073},{\"end\":29741,\"start\":29688},{\"end\":30423,\"start\":30333},{\"end\":30818,\"start\":30727},{\"end\":31365,\"start\":31271},{\"end\":31745,\"start\":31687},{\"end\":32069,\"start\":31989},{\"end\":32359,\"start\":32297},{\"end\":32848,\"start\":32758},{\"end\":33208,\"start\":33152},{\"end\":33467,\"start\":33439},{\"end\":34112,\"start\":34043},{\"end\":34414,\"start\":34351},{\"end\":34682,\"start\":34616},{\"end\":35101,\"start\":35057}]", "bib_author": "[{\"end\":24733,\"start\":24719},{\"end\":24743,\"start\":24733},{\"end\":24755,\"start\":24743},{\"end\":24768,\"start\":24755},{\"end\":24781,\"start\":24768},{\"end\":24789,\"start\":24781},{\"end\":24809,\"start\":24789},{\"end\":25166,\"start\":25156},{\"end\":25176,\"start\":25166},{\"end\":25188,\"start\":25176},{\"end\":25197,\"start\":25188},{\"end\":25211,\"start\":25197},{\"end\":25220,\"start\":25211},{\"end\":25232,\"start\":25220},{\"end\":25492,\"start\":25486},{\"end\":25500,\"start\":25492},{\"end\":25508,\"start\":25500},{\"end\":25764,\"start\":25754},{\"end\":25774,\"start\":25764},{\"end\":25784,\"start\":25774},{\"end\":25790,\"start\":25784},{\"end\":25797,\"start\":25790},{\"end\":26118,\"start\":26112},{\"end\":26124,\"start\":26118},{\"end\":26132,\"start\":26124},{\"end\":26140,\"start\":26132},{\"end\":26147,\"start\":26140},{\"end\":26157,\"start\":26147},{\"end\":26511,\"start\":26505},{\"end\":26518,\"start\":26511},{\"end\":26526,\"start\":26518},{\"end\":26533,\"start\":26526},{\"end\":26543,\"start\":26533},{\"end\":26977,\"start\":26968},{\"end\":26983,\"start\":26977},{\"end\":26998,\"start\":26983},{\"end\":27012,\"start\":26998},{\"end\":27032,\"start\":27012},{\"end\":27042,\"start\":27032},{\"end\":27055,\"start\":27042},{\"end\":27561,\"start\":27555},{\"end\":27568,\"start\":27561},{\"end\":27576,\"start\":27568},{\"end\":27584,\"start\":27576},{\"end\":27599,\"start\":27584},{\"end\":27610,\"start\":27599},{\"end\":27621,\"start\":27610},{\"end\":27961,\"start\":27955},{\"end\":27969,\"start\":27961},{\"end\":27978,\"start\":27969},{\"end\":27985,\"start\":27978},{\"end\":27992,\"start\":27985},{\"end\":28000,\"start\":27992},{\"end\":28357,\"start\":28348},{\"end\":28365,\"start\":28357},{\"end\":28371,\"start\":28365},{\"end\":28378,\"start\":28371},{\"end\":28386,\"start\":28378},{\"end\":28394,\"start\":28386},{\"end\":28401,\"start\":28394},{\"end\":28409,\"start\":28401},{\"end\":28415,\"start\":28409},{\"end\":28432,\"start\":28415},{\"end\":28438,\"start\":28432},{\"end\":28444,\"start\":28438},{\"end\":29152,\"start\":29144},{\"end\":29165,\"start\":29152},{\"end\":29176,\"start\":29165},{\"end\":29188,\"start\":29176},{\"end\":29749,\"start\":29743},{\"end\":29758,\"start\":29749},{\"end\":29766,\"start\":29758},{\"end\":29781,\"start\":29766},{\"end\":29794,\"start\":29781},{\"end\":29808,\"start\":29794},{\"end\":29818,\"start\":29808},{\"end\":30433,\"start\":30425},{\"end\":30442,\"start\":30433},{\"end\":30448,\"start\":30442},{\"end\":30456,\"start\":30448},{\"end\":30464,\"start\":30456},{\"end\":30471,\"start\":30464},{\"end\":30827,\"start\":30820},{\"end\":30834,\"start\":30827},{\"end\":30841,\"start\":30834},{\"end\":30849,\"start\":30841},{\"end\":31374,\"start\":31367},{\"end\":31382,\"start\":31374},{\"end\":31388,\"start\":31382},{\"end\":31396,\"start\":31388},{\"end\":31404,\"start\":31396},{\"end\":31763,\"start\":31747},{\"end\":31769,\"start\":31763},{\"end\":31780,\"start\":31769},{\"end\":32082,\"start\":32071},{\"end\":32093,\"start\":32082},{\"end\":32104,\"start\":32093},{\"end\":32118,\"start\":32104},{\"end\":32373,\"start\":32361},{\"end\":32382,\"start\":32373},{\"end\":32394,\"start\":32382},{\"end\":32401,\"start\":32394},{\"end\":32409,\"start\":32401},{\"end\":32419,\"start\":32409},{\"end\":32429,\"start\":32419},{\"end\":32438,\"start\":32429},{\"end\":32859,\"start\":32850},{\"end\":32870,\"start\":32859},{\"end\":32882,\"start\":32870},{\"end\":32893,\"start\":32882},{\"end\":33219,\"start\":33210},{\"end\":33232,\"start\":33219},{\"end\":33243,\"start\":33232},{\"end\":33477,\"start\":33469},{\"end\":33489,\"start\":33477},{\"end\":33498,\"start\":33489},{\"end\":34126,\"start\":34114},{\"end\":34132,\"start\":34126},{\"end\":34141,\"start\":34132},{\"end\":34154,\"start\":34141},{\"end\":34430,\"start\":34416},{\"end\":34696,\"start\":34684},{\"end\":34709,\"start\":34696},{\"end\":35109,\"start\":35103},{\"end\":35118,\"start\":35109},{\"end\":35125,\"start\":35118},{\"end\":35132,\"start\":35125}]", "bib_venue": "[{\"end\":28664,\"start\":28554},{\"end\":29311,\"start\":29258},{\"end\":29951,\"start\":29922},{\"end\":31022,\"start\":30944},{\"end\":33663,\"start\":33605},{\"end\":34785,\"start\":34767},{\"end\":35226,\"start\":35208},{\"end\":24843,\"start\":24809},{\"end\":25243,\"start\":25232},{\"end\":25529,\"start\":25508},{\"end\":25801,\"start\":25797},{\"end\":26199,\"start\":26157},{\"end\":26503,\"start\":26426},{\"end\":26769,\"start\":26684},{\"end\":27184,\"start\":27055},{\"end\":27639,\"start\":27635},{\"end\":28043,\"start\":28000},{\"end\":28552,\"start\":28444},{\"end\":29256,\"start\":29188},{\"end\":29920,\"start\":29818},{\"end\":30502,\"start\":30471},{\"end\":30942,\"start\":30849},{\"end\":31445,\"start\":31404},{\"end\":31798,\"start\":31794},{\"end\":32122,\"start\":32118},{\"end\":32506,\"start\":32438},{\"end\":32924,\"start\":32893},{\"end\":33247,\"start\":33243},{\"end\":33558,\"start\":33498},{\"end\":34169,\"start\":34154},{\"end\":34447,\"start\":34443},{\"end\":34765,\"start\":34709},{\"end\":35206,\"start\":35132}]"}}}, "year": 2023, "month": 12, "day": 17}