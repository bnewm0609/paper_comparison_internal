{"id": 218596234, "updated": "2023-10-06 15:28:57.95", "metadata": {"title": "Adipose Tissue Segmentation in Unlabeled Abdomen MRI using Cross Modality Domain Adaptation", "authors": "[{\"first\":\"Samira\",\"last\":\"Masoudi\",\"middle\":[]},{\"first\":\"Syed\",\"last\":\"Anwar\",\"middle\":[\"M.\"]},{\"first\":\"Stephanie\",\"last\":\"Harmon\",\"middle\":[\"A.\"]},{\"first\":\"Peter\",\"last\":\"Choyke\",\"middle\":[\"L.\"]},{\"first\":\"Baris\",\"last\":\"Turkbey\",\"middle\":[]},{\"first\":\"Ulas\",\"last\":\"Bagci\",\"middle\":[]}]", "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "publication_date": {"year": 2020, "month": 5, "day": 11}, "abstract": "Abdominal fat quantification is critical since multiple vital organs are located within this region. Although computed tomography (CT) is a highly sensitive modality to segment body fat, it involves ionizing radiations which makes magnetic resonance imaging (MRI) a preferable alternative for this purpose. Additionally, the superior soft tissue contrast in MRI could lead to more accurate results. Yet, it is highly labor intensive to segment fat in MRI scans. In this study, we propose an algorithm based on deep learning technique(s) to automatically quantify fat tissue from MR images through a cross modality adaptation. Our method does not require supervised labeling of MR scans, instead, we utilize a cycle generative adversarial network (C-GAN) to construct a pipeline that transforms the existing MR scans into their equivalent synthetic CT (s-CT) images where fat segmentation is relatively easier due to the descriptive nature of HU (hounsfield unit) in CT images. The fat segmentation results for MRI scans were evaluated by expert radiologist. Qualitative evaluation of our segmentation results shows average success score of 3.80/5 and 4.54/5 for visceral and subcutaneous fat segmentation in MR images.", "fields_of_study": "[\"Engineering\",\"Computer Science\",\"Physics\"]", "external_ids": {"arxiv": "2005.05761", "mag": "3082277506", "acl": null, "pubmed": "33018306", "pubmedcentral": null, "dblp": "conf/embc/MasoudiAHCTB20", "doi": "10.1109/embc44109.2020.9176009"}}, "content": {"source": {"pdf_hash": "248fdb2f4e4c92381a70243110608b6bdbe70258", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2005.05761v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2005.05761", "status": "GREEN"}}, "grobid": {"id": "16262fdff6c146f08eda5ebf459b3b2a0ff9f755", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/248fdb2f4e4c92381a70243110608b6bdbe70258.txt", "contents": "\nAdipose Tissue Segmentation in Unlabeled Abdomen MRI using Cross Modality Domain Adaptation\n\n\nSamira Masoudi \nSyed M Anwar \nStephanie A Harmon \nPeter L Choyke \nBaris Turkbey \nUlas Bagci \nAdipose Tissue Segmentation in Unlabeled Abdomen MRI using Cross Modality Domain Adaptation\n\nAbdominal fat quantification is critical since multiple vital organs are located within this region. Although computed tomography (CT) is a highly sensitive modality to segment body fat, it involves ionizing radiations which makes magnetic resonance imaging (MRI) a preferable alternative for this purpose. Additionally, the superior soft tissue contrast in MRI could lead to more accurate results. Yet, it is highly labor intensive to segment fat in MRI scans. In this study, we propose an algorithm based on deep learning technique(s) to automatically quantify fat tissue from MR images through a cross modality adaptation. Our method does not require supervised labeling of MR scans, instead, we utilize a cycle generative adversarial network (C-GAN) to construct a pipeline that transforms the existing MR scans into their equivalent synthetic CT (s-CT) images where fat segmentation is relatively easier due to the descriptive nature of HU (hounsfield unit) in CT images. The fat segmentation results for MRI scans were evaluated by expert radiologist. Qualitative evaluation of our segmentation results shows average success score of 3.80/5 and 4.54/5 for visceral and subcutaneous fat segmentation in MR images * .\n\nAbstract-Abdominal fat quantification is critical since multiple vital organs are located within this region. Although computed tomography (CT) is a highly sensitive modality to segment body fat, it involves ionizing radiations which makes magnetic resonance imaging (MRI) a preferable alternative for this purpose. Additionally, the superior soft tissue contrast in MRI could lead to more accurate results. Yet, it is highly labor intensive to segment fat in MRI scans. In this study, we propose an algorithm based on deep learning technique(s) to automatically quantify fat tissue from MR images through a cross modality adaptation. Our method does not require supervised labeling of MR scans, instead, we utilize a cycle generative adversarial network (C-GAN) to construct a pipeline that transforms the existing MR scans into their equivalent synthetic CT (s-CT) images where fat segmentation is relatively easier due to the descriptive nature of HU (hounsfield unit) in CT images. The fat segmentation results for MRI scans were evaluated by expert radiologist. Qualitative evaluation of our segmentation results shows average success score of 3.80/5 and 4.54/5 for visceral and subcutaneous fat segmentation in MR images * .\n\n\nI. INTRODUCTION\n\nAbdominal obesity is an increasingly prevalent condition (clinically considered as a disorder) among people of all ages including young adults and children. There is a significant body of evidence demonstrating a positive association between obesity indices and metabolic syndrome, higher risks of cancer, chronic inflammation, and diabetes as well as cardiovascular diseases. Visceral adipose tissue (VAT) and subcutaneous adipose tissue (SAT) have different roles in human metabolism, where VAT is known to be critically associated with major health risks [1].\n\nTherefore, quantifying the extents of adipose tissue in abdominal region in two forms of visceral and subcutaneous fat could help to better understand and evaluate a patient's condition [2]. The adipose tissue (SAT and VAT) can be identified using non-invasive imaging techniques such as computed tomography (CT), dual x-ray absorptiometry (DEXA), and magnetic resonance imaging (MRI). DEXA is the most widely used method for monitoring body fat with no direct information on anatomy nor separation between VAT and SAT [3]. Alternatively, by using either CT or MRI, the fat MD  content as well as the underlying anatomy can be visualized. MRI is known to be a safer imaging modality since it does not use potentially harmful ionizing radiations, which is imminent during CT and DEXA acquisition. Although MRI is safer, anatomically informative, and highly accurate, it is difficult to segment fat in magnetic resonance (MR) images as compared to CT images.\n\nThere are automated and semi-automated algorithms presented in literature to segment both VAT and SAT using different imaging modalities [4], [5]. The contrast created by Hounsfield unit (HU) values in CT images is found to be appropriate and hence resulted in a widespread use of segmentation methods based on thresholding. However, the same methods cannot be applied to fat segmentation in MRI scans. Moreover, these methods produce approximate segmentation, which demands further manual inspection by clinical experts [5]. Ideally, development of an automated segmentation algorithm using appropriate feature selection and machine learning techniques (such as deep learning) with high accuracy could save radiologists from a time-intensive workload.\n\nHerein, we propose a fully automated algorithm for adipose tissue segmentation in MR images of the abdominal region. The training of such a network would require a large amount of manually labeled MR images. To alleviate this requirement, we propose to segment visceral and subcutaneous fat tissues in MR images using an unpaired set of labeled CT scans. For this purpose, we first employ the idea of cycle generative adversarial network (C-GAN) to construct a pipeline that transforms our target MRI scans into synthetic CT (s-CT) by generating 2D slices with similar (to a large extent) spatial distribution. Next, we use a U-Net, trained to segment VAT and SAT in s-CT and thus its reference MR scan. Results obtained using our proposed methodology in both cases were evaluated (using a set of defined rules) by expert radiologist.\n\nOur algorithm has two major contributions: 1) we used an unpaired C-GAN architecture to generate abdominal s-CT images from input MR images, 2) we then utilized the underlying structural similarity (between input MR and s-CT images) to segment VAT and SAT in abdominal MRIs by using a U-Net model trained on the acquired CT (a-CT) data. Doing so, we obtained a significant performance in segmenting VAT and SAT in MR images without using labeled MR images for training. This work can significantly contribute towards solving problems (segmentation/detection) in certain radiology applications with scarce labeled data. \n\n\nII. PROPOSED METHODOLOGY\n\nWe employed two main modules in our proposed methodology ( Fig. 1) for fat tissue segmentation in abdomen MRI. These two subsequent modules imply transformation and segmentation in 2D radiology scans. The first module is a C-GAN which we used to convert MR scans to their equivalent CT (s-CT). The second module ( Fig. 2) includes two separate U-Nets for independent segmentation of VAT and SAT in CT slices. We hypothesize that the segmentation labels for SAT and VAT from the s-CT could be transferred to the associated MR images. Since we trained our C-GAN with a loss function that maintains a spatial registration (between the input MRI and s-CT), while transforming the input MRI scan to s-CT, we believe that our hypothesis holds true. Our experimental results and expert radiologist validation added further credence to this hypothesis. The details on how these modules were trained and how the VAT/SAT segmentation was inferred for MRI scans are presented in the following subsections.\n\nA. How to Train? 1) Pre-processing: The CT and MR scans were acquired from various centers, many patients at different depths, exposures, and physical conditions. Hence we utilized a prepossessing strategy to obtain meaningful intensity values among different MR images. To this end, we first used N-4 Bias correction algorithm to remove the low frequency nonuniformity or \"bias field\" which is present in MR scans. We then performed a patient level standardization of intensity distribution, where intensity values in each image were transformed such that the resulting histogram was in harmony with a predefined standard distribution [6], [7]. These preprocessing steps significantly facilitated the learning process of our proposed C-GAN architecture. Similarly, for CT scans, we cropped the intensity values within a window (-700 HUs -1300 HUs) belonging to the soft tissue (which is the focus of our segmentation). This was followed by intensity normalization in a fixed range among all patients.\n\n2) MR to CT domain adaptation: Conventionally, learning an image-to-image mapping requires a large amount of paired data. In medical imaging domain, the availability of sufficient paired data is not always guaranteed. A C-GAN could overcome this limitation by allowing image translation without the requirement to have paired examples for training the model [8].\n\nThe C-GAN architecture was built upon the GAN model to enable dual-domain image translation among images from domain A (MRI) and B (CT). C-GAN uses two generators (G A generating I BA from I B , and G B generating I AB from I A ) along with two discriminators (D A recognizing I A from I BA and D B recognizing I B from I AB ). We achieved an overreaching performance with this framework, by introducing a certain level of cycle consistency, to ensure successful image translation. Hence, we defined the loss function of our proposed generators using mean square error (MSE) given as, \n\nwhere \u03b1 and \u03b2 are optimized to be 10.0, and 2.0 respectively. While I AB , and I BA represents the synthetic CT and MR images generated from I A (MRI) and I B (CT) scans, I ABA and I BAB refer to images generated after completion of the cyclic (C-GAN) process. Our proposed loss function was designed to ensure intensity transformation while maintaining spatial registration between input MRI and the generated s-CT. This spatial consistency between MRI and s-CT is crucial as we used this property to indirectly segment fat in MR images through their equivalent s-CT scans. For this module, we used fat saturated T1-weighted MR and CT scans obtained at the National Institute of Health (NIH) to train our scans generated using C-GAN.\n\n\nScore Description\n\n\nNo correlation between MR and synthetic CT 2\n\nFew relations between MR and synthetic CT 3 Some correlation between acquired CT and synthetic CT 4 Comparable synthetic CT and acquired CT 5 synthetic CT can substitute acquired CT network and employed it later to perform domain adaption among MR and CT images.\n\n\n3) Fat segmentation in a-CT images:\n\nWe trained a supervised machine learning based algorithm to segment fat tissue in abdominal CT scans. Image segmentation is an important problem in computer vision, and recently deep learning has demonstrated significant performance in this regard [9], [10]. One of the segmentation challenges is how to define the segment boundaries, which depends on the application and input data. Additionally, segmentation in medical image applications suffers from either limited labeled data or expensive annotations (in terms of time and labour). In recent years, the U-Net based architecture and its variants have shown to perform significantly well in medical image segmentation with limited training data [11], [12].\n\nSince VAT and SAT are localized at different body regions representing different properties, we employed a slightly different structure to optimally perform segmentation task in each case. We trained both of our deep learning based models by customizing the U-Net architecture; 3-layers and 5-layers for subcutaneous and visceral fat segmentation, respectively. While training from scratch, we included data augmentation in our training to overcome the limited availability of annotated data.\n\nWe must note that it is relatively an easier task for radiologists to annotate fat (both VAT and SAT) in CT scans. That is why we used fat labels from CT scans to train our U-Net models, and transformed MRIs to s-CT for segmentation.\n\nB. How to infer? 1) Step 1: Generating s-CT from MR scans: Although our C-GAN model was enforced to learn a reciprocal transformation between MR/CT slices, we are specifically interested in mapping MR images to CT. Hence, during the inference stage, we fed MR slices (which we want to segment) to the trained C-GAN to generate its equivalent s-CT.\n\n2)\n\nStep 2: Fat segmentation in MR images using s-CT: The s-CT images obtained in Step 1 were fed into the trained models (section II-A.3) for segmenting visceral and subcutaneous fat. We hypothesize here that since the acquired MR and s-CT (generated using C-GAN) had an inherent spatial alignment, the segmented fat in s-CT (on a pixel level) can be directly assigned to the corresponding MR scan. Our results showed that this hypothesis holds true and we achieved a significant performance in segmenting fat in MR images. These finding were verified by expert observer adding credence to our hypothesis and results. \n\n\nIII. RESULTS\n\n\nA. Dataset\n\nIn this study, two different datasets were anonymized and used for segmentation and transformation tasks: 1) we utilized CT and T1-weighted MR images of 34 patients obtained within 6 months of each other at NIH to train our C-GAN architecture. These scans had an average of 100 slices per patient (for both CT and MRI), among them we used 85% for training, and 15% to evaluate our proposed algorithm. 2) we used CT scans (from the abdomen region) of 131 patients obtained from multiple centers (Mayo clinic) with their respective visceral and subcutaneous fat labels for segmentation. We randomly split these patients into 90-10% as train-test data to train our segmentation networks based on U-Net. This data set was sufficient to train a network that could accurately segment subcutaneous fat. But the complexity of visceral fat segmentation required a larger number of training images for successful training. To compensate for the limited number of annotated CT slices, we employed data augmentation using image translation (width and height), reflection (horizontal flip), and zooming to increase (by 4 folds) our set of training images.\n\n\nB. Evaluation metrics\n\nThe performance was evaluated separately at each Step. We used dice score to report segmentation performance in a-CT images. During inference, we exploited the judgment of an expert radiologist to measure our success during Steps 1 (Section II-B.1) and 2 (Section II-B.2) of our proposed method. Our expert radiologist followed a set of defined scoring rules to evaluate the results of our proposed MR to s-CT transformation design (Table I). Similarly, Table II represents the respective definitions in terms of false discovery rate (FDR) and false negative rate (FNR) which are used to evaluate fat segmentation in MR scans. It must be noted that the \"correlation\" in Table I was measured in terms of both visual structural and textural similarities.\n\n\nC. Quantitative and qualitative results\n\nOur trained U-Net segmentation networks reveal an average dice of 97.46% and 94.33%, respectively in segmenting the subcutaneous and visceral fat for the test data (a-CT scans). Some visual results are shown in Fig. 3 and Fig. 4 for both SAT and VAT, respectively. We trained our C-GAN using the s-CT and MR slices in abdomen region of patients in an unpaired fashion. It must be noted that 3D images in these two modalities do not necessarily occupy the exact same physical space due to inevitable differences between scanning devices, fieldof-view, and patient's physical conditions. Here, we tested our proposed framework using 320 randomly chosen MR slices as the test dataset and transformed them into their equivalent s-CT slices. We compared our resulting s-CT with a-CT slices of these patients to quantify the performance of our algorithm (figure 5). Our expert radiologist scored the generated images using the rules presented in Table I. Our qualitative investigation showed that the s-CT scans got an average score of 4.16 out of 5. The lowest scores belonged to slices in upper lung area, where the network was unable to see enough MR samples due to the limited field-of-view in MR acquisition.\n\n\nD. Fat tissue segmentation in abdomen MRI\n\nWe employed our trained U-Net models for visceral and subcutaneous fat segmentation in CT images to segment fat in s-CT (obtained using C-GAN) slices. The outputs of VAT and SAT segmentation in s-CT slices were assigned success scores (according to Table II) \n\n\nIV. CONCLUSIONS\n\nWe constrained a C-GAN to perform a forward-backward 2-D transformation between MR and CT scans and used it to transform MRI slices into s-CT slices. We segmented VAT and SAT in the generated s-CT slices. The segmentation labels were assigned (with the assumption of a spatial registration between MRI and s-CT scans) to the corresponding MRI scan. We used several prepossessing techniques to improve the performance of our proposed algorithm. The results are significantly accurate as are confirmed by expert radiologist. Our proposed solution is innovative and can be extended to other bio-medical applications where ground truth labels do not exist (or difficult to obtain) for certain imaging modalities but are easier to obtain for another modality. This idea would benefit the field of medical image analysis, where labeled data is scarce. The results are also significant for clinical studies which could benefit from fat segmentation and quantification in MRI.\n\nFigure 1 :\n1Our proposed solution for fat segmentation in unlabeled MR images using unpaired domain transformation and the U-Net architecture.\n\nFigure 2 :\n2U-Net based fat segmentation networks (for VAT and SAT) trained using CT (acquired) scans and manually labeled ground truth.\n\n\nG loss =MSE(1, D B (I AB )) + MSE(1, D A (I BA )) +\u03b1[MSE(I ABA , I A ) + MSE(I BAB , I B )] +\u03b2[MSE(I BA , I B ) + MSE(I AB , I A )],\n\nFigure 3 :Figure 4 :\n34Examples of subcutaneous fat segmentation: (a) the acquired CT, (b) ground truth, and (c) segmented SAT using U-Net based architecture. Examples of visceral fat segmentation: (a) the acquired CT, (b) ground truth, (c) segmented VAT using U-Net based architecture.\n\nFigure 5 :Figure 6 :\n56between 0-5 by expert radiologist. The average success scores in our experiment was 3.80 for visceral fat segmentation and 4.54 for subcutaneous fat segmentation. Most failures in correct segmentation were observed in the lung base-upper abdomen region. In these regions, C-GAN failed to generate accurate s-CTs due to MR and synthetic CT slices from two patients: (a) acquired MR slice, (b) acquired CT slice, and (c) generated synthetic CT slice. Results for subcutaneous and visceral fat segmentation: (a) the acquired MR slice, (b) subcutaneous fat segmentation result, and (c) visceral fat segmentation result. intensity overlap between visceral fat and lung parenchyma in fat-saturated T1-MRI which were used for training. A few representative examples of segmentation results are presented in Fig. 6.\n\n\n20814, USA ismail.turkbey@nih.gov Harmon is with Clinical Research Directorate, Frederick National Laboratory for Cancer Research, Frederick, MD 21701, USA3 stephanie.harmon@nih.gov \n\n *  The \nimplementation \nof \nthis \nwork \ncan \nbe \nfound \nat: \nhttps://github.com/SamMas-hub/ \nFat-Segmentation-in-MR-using-domain-adaptation \n\n\n\nTABLE I :\nIThe scoring rules for visual evaluation of synthetic CT (s-CT)\n\nTABLE II :\nIITerms used for visual evaluation of fat segmentation in MR slices based on false discovery rate (FDR) and false negative rate (FNR).Score \nDescription \nRanges \n\n1 \nComplete failure \n\nFDR> 70% & FNR> 80% \n\n2 \nConsiderable miss-predictions \n\n70% >FDR> 45% \n80% >FNR> 50% \n\n3 \nErroneous predictions \n\n45% >FDR> 30% \n50% >FNR> 30% \n\n4 \nSome false predictions \n\n30% >FDR> 15% \n30% >FNR> 20% \n\n5 \nVery few miss-predictions \n\nFDR< 15% & FNR< 20% \n\n\nMasoudi, Anwar, and Bagci are with University of Central Florida, Orlando, FL 32816, USA bagci@ucf.edu 2 Masoudi, Harmon, Turkbey, and Choyke are with Molecular Imaging Program, National Cancer Institute, National Institutes of Health, Bethesda,\n\nSubcutaneous and visceral adipose tissue: their relation to the metabolic syndrome. Wajchenberg Bernardo Leo, Endocrine reviews. 216Bernardo Leo Wajchenberg, \"Subcutaneous and visceral adipose tissue: their relation to the metabolic syndrome,\" Endocrine reviews, vol. 21, no. 6, pp. 697-738, 2000.\n\nSubcutaneous and visceral adipose tissue: structural and functional differences. Ibrahim M Mohsen, Obesity reviews. 111M Mohsen Ibrahim, \"Subcutaneous and visceral adipose tissue: structural and functional differences,\" Obesity reviews, vol. 11, no. 1, pp. 11-18, 2010.\n\nDual energy x-ray absorptiometry body composition reference values from nhanes. L Thomas, Kevin E Kelly, Steven B Wilson, Heymsfield, PloS one. 497038Thomas L Kelly, Kevin E Wilson, and Steven B Heymsfield, \"Dual energy x-ray absorptiometry body composition reference values from nhanes,\" PloS one, vol. 4, no. 9, pp. e7038, 2009.\n\nMr fat segmentation and quantification for abdominal volumetric and composition analysis. H Darryl, Passant Hwang, Mohamed, A Vinay, Suzanne L Duddalwar, Palmer, 14th International Symposium on Medical Information Processing and Analysis. International Society for Optics and Photonics. 10975109750Darryl H Hwang, Passant Mohamed, Vinay A Duddalwar, and Suzanne L Palmer, \"Mr fat segmentation and quantification for abdominal volumetric and composition analysis,\" in 14th Interna- tional Symposium on Medical Information Processing and Analysis. International Society for Optics and Photonics, 2018, vol. 10975, p. 109750J.\n\nFully convolutional networks for automated segmentation of abdominal adipose tissue depots in multicenter water-fat mri. Taro Langner, Anders Hedstr\u00f6m, Katharina M\u00f6rwald, Daniel Weghuber, Anders Forslund, Peter Bergsten, H\u00e5kan Ahlstr\u00f6m, Joel Kullberg, Magnetic resonance in medicine. 814Taro Langner, Anders Hedstr\u00f6m, Katharina M\u00f6rwald, Daniel Weghu- ber, Anders Forslund, Peter Bergsten, H\u00e5kan Ahlstr\u00f6m, and Joel Kullberg, \"Fully convolutional networks for automated segmentation of abdominal adipose tissue depots in multicenter water-fat mri,\" Magnetic resonance in medicine, vol. 81, no. 4, pp. 2736-2745, 2019.\n\nN4itk: improved n3 bias correction. Brian B Nicholas J Tustison, Avants, A Philip, Yuanjie Cook, Alexander Zheng, Egan, A Paul, James C Yushkevich, Gee, IEEE transactions on medical imaging. 2961310Nicholas J Tustison, Brian B Avants, Philip A Cook, Yuanjie Zheng, Alexander Egan, Paul A Yushkevich, and James C Gee, \"N4itk: improved n3 bias correction,\" IEEE transactions on medical imaging, vol. 29, no. 6, pp. 1310, 2010.\n\nOn standardizing the mr image intensity scale. G L\u00e1szl\u00f3, Ny\u00fal, K Jayaram, Udupa, Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine. 426L\u00e1szl\u00f3 G Ny\u00fal and Jayaram K Udupa, \"On standardizing the mr image intensity scale,\" Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine, vol. 42, no. 6, pp. 1072-1081, 1999.\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros, \"Un- paired image-to-image translation using cycle-consistent adversarial networks,\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2223-2232.\n\nAuto-deeplab: Hierarchical neural architecture search for semantic image segmentation. Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, Li Fei-Fei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, and Li Fei-Fei, \"Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 82-92.\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE transactions on pattern analysis and machine intelligence. 40Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille, \"Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs,\" IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 4, pp. 834-848, 2017.\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computerassisted intervention. SpringerOlaf Ronneberger, Philipp Fischer, and Thomas Brox, \"U-net: Convolutional networks for biomedical image segmentation,\" in International Conference on Medical image computing and computer- assisted intervention. Springer, 2015, pp. 234-241.\n\nMultiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation. Nabil Ibtehaz, Rahman, Neural Networks. 121Nabil Ibtehaz and M Sohel Rahman, \"Multiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation,\" Neural Networks, vol. 121, pp. 74-87, 2020.\n", "annotations": {"author": "[{\"end\":110,\"start\":95},{\"end\":124,\"start\":111},{\"end\":144,\"start\":125},{\"end\":160,\"start\":145},{\"end\":175,\"start\":161},{\"end\":187,\"start\":176}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":102},{\"end\":123,\"start\":118},{\"end\":143,\"start\":137},{\"end\":159,\"start\":153},{\"end\":174,\"start\":167},{\"end\":186,\"start\":181}]", "author_first_name": "[{\"end\":101,\"start\":95},{\"end\":115,\"start\":111},{\"end\":117,\"start\":116},{\"end\":134,\"start\":125},{\"end\":136,\"start\":135},{\"end\":150,\"start\":145},{\"end\":152,\"start\":151},{\"end\":166,\"start\":161},{\"end\":180,\"start\":176}]", "author_affiliation": null, "title": "[{\"end\":92,\"start\":1},{\"end\":279,\"start\":188}]", "venue": null, "abstract": "[{\"end\":1502,\"start\":281}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3315,\"start\":3312},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3507,\"start\":3504},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3840,\"start\":3837},{\"end\":3894,\"start\":3892},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4416,\"start\":4413},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4421,\"start\":4418},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4800,\"start\":4797},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8149,\"start\":8146},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8154,\"start\":8151},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8874,\"start\":8871},{\"end\":10310,\"start\":10306},{\"end\":10366,\"start\":10362},{\"end\":10408,\"start\":10404},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10820,\"start\":10817},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10826,\"start\":10822},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11272,\"start\":11268},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11278,\"start\":11274}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17616,\"start\":17473},{\"attributes\":{\"id\":\"fig_1\"},\"end\":17754,\"start\":17617},{\"attributes\":{\"id\":\"fig_2\"},\"end\":17889,\"start\":17755},{\"attributes\":{\"id\":\"fig_3\"},\"end\":18177,\"start\":17890},{\"attributes\":{\"id\":\"fig_4\"},\"end\":19009,\"start\":18178},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":19339,\"start\":19010},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":19414,\"start\":19340},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":19870,\"start\":19415}]", "paragraph": "[{\"end\":2734,\"start\":1504},{\"end\":3316,\"start\":2754},{\"end\":4274,\"start\":3318},{\"end\":5028,\"start\":4276},{\"end\":5864,\"start\":5030},{\"end\":6485,\"start\":5866},{\"end\":7508,\"start\":6514},{\"end\":8511,\"start\":7510},{\"end\":8875,\"start\":8513},{\"end\":9462,\"start\":8877},{\"end\":10198,\"start\":9464},{\"end\":10529,\"start\":10267},{\"end\":11279,\"start\":10569},{\"end\":11773,\"start\":11281},{\"end\":12008,\"start\":11775},{\"end\":12357,\"start\":12010},{\"end\":12361,\"start\":12359},{\"end\":12978,\"start\":12363},{\"end\":14150,\"start\":13008},{\"end\":14928,\"start\":14176},{\"end\":16179,\"start\":14972},{\"end\":16484,\"start\":16225},{\"end\":17472,\"start\":16504}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14617,\"start\":14608},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14638,\"start\":14630},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14853,\"start\":14846},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":15919,\"start\":15912},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16483,\"start\":16474}]", "section_header": "[{\"end\":2752,\"start\":2737},{\"end\":6512,\"start\":6488},{\"end\":10218,\"start\":10201},{\"attributes\":{\"n\":\"1\"},\"end\":10265,\"start\":10221},{\"end\":10567,\"start\":10532},{\"end\":12993,\"start\":12981},{\"end\":13006,\"start\":12996},{\"end\":14174,\"start\":14153},{\"end\":14970,\"start\":14931},{\"end\":16223,\"start\":16182},{\"end\":16502,\"start\":16487},{\"end\":17484,\"start\":17474},{\"end\":17628,\"start\":17618},{\"end\":17911,\"start\":17891},{\"end\":18199,\"start\":18179},{\"end\":19350,\"start\":19341},{\"end\":19426,\"start\":19416}]", "table": "[{\"end\":19339,\"start\":19167},{\"end\":19870,\"start\":19561}]", "figure_caption": "[{\"end\":17616,\"start\":17486},{\"end\":17754,\"start\":17630},{\"end\":17889,\"start\":17757},{\"end\":18177,\"start\":17914},{\"end\":19009,\"start\":18202},{\"end\":19167,\"start\":19012},{\"end\":19414,\"start\":19352},{\"end\":19561,\"start\":19429}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6579,\"start\":6573},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6834,\"start\":6828},{\"end\":15189,\"start\":15183},{\"end\":15200,\"start\":15194}]", "bib_author_first_name": "[{\"end\":20213,\"start\":20202},{\"end\":20505,\"start\":20498},{\"end\":20769,\"start\":20768},{\"end\":20783,\"start\":20778},{\"end\":20785,\"start\":20784},{\"end\":20799,\"start\":20793},{\"end\":20801,\"start\":20800},{\"end\":21111,\"start\":21110},{\"end\":21127,\"start\":21120},{\"end\":21145,\"start\":21144},{\"end\":21160,\"start\":21153},{\"end\":21162,\"start\":21161},{\"end\":21770,\"start\":21766},{\"end\":21786,\"start\":21780},{\"end\":21806,\"start\":21797},{\"end\":21822,\"start\":21816},{\"end\":21839,\"start\":21833},{\"end\":21855,\"start\":21850},{\"end\":21871,\"start\":21866},{\"end\":21886,\"start\":21882},{\"end\":22303,\"start\":22298},{\"end\":22305,\"start\":22304},{\"end\":22336,\"start\":22335},{\"end\":22352,\"start\":22345},{\"end\":22368,\"start\":22359},{\"end\":22383,\"start\":22382},{\"end\":22397,\"start\":22390},{\"end\":22736,\"start\":22735},{\"end\":22752,\"start\":22751},{\"end\":23216,\"start\":23209},{\"end\":23229,\"start\":23222},{\"end\":23243,\"start\":23236},{\"end\":23257,\"start\":23251},{\"end\":23259,\"start\":23258},{\"end\":23722,\"start\":23716},{\"end\":23739,\"start\":23728},{\"end\":23753,\"start\":23746},{\"end\":23770,\"start\":23763},{\"end\":23780,\"start\":23777},{\"end\":23790,\"start\":23786},{\"end\":23792,\"start\":23791},{\"end\":23803,\"start\":23801},{\"end\":24368,\"start\":24357},{\"end\":24381,\"start\":24375},{\"end\":24401,\"start\":24394},{\"end\":24417,\"start\":24412},{\"end\":24430,\"start\":24426},{\"end\":24432,\"start\":24431},{\"end\":24879,\"start\":24875},{\"end\":24900,\"start\":24893},{\"end\":24916,\"start\":24910},{\"end\":25358,\"start\":25353}]", "bib_author_last_name": "[{\"end\":20226,\"start\":20214},{\"end\":20514,\"start\":20506},{\"end\":20776,\"start\":20770},{\"end\":20791,\"start\":20786},{\"end\":20808,\"start\":20802},{\"end\":20820,\"start\":20810},{\"end\":21118,\"start\":21112},{\"end\":21133,\"start\":21128},{\"end\":21142,\"start\":21135},{\"end\":21151,\"start\":21146},{\"end\":21172,\"start\":21163},{\"end\":21180,\"start\":21174},{\"end\":21778,\"start\":21771},{\"end\":21795,\"start\":21787},{\"end\":21814,\"start\":21807},{\"end\":21831,\"start\":21823},{\"end\":21848,\"start\":21840},{\"end\":21864,\"start\":21856},{\"end\":21880,\"start\":21872},{\"end\":21895,\"start\":21887},{\"end\":22325,\"start\":22306},{\"end\":22333,\"start\":22327},{\"end\":22343,\"start\":22337},{\"end\":22357,\"start\":22353},{\"end\":22374,\"start\":22369},{\"end\":22380,\"start\":22376},{\"end\":22388,\"start\":22384},{\"end\":22408,\"start\":22398},{\"end\":22413,\"start\":22410},{\"end\":22743,\"start\":22737},{\"end\":22749,\"start\":22745},{\"end\":22760,\"start\":22753},{\"end\":22767,\"start\":22762},{\"end\":23220,\"start\":23217},{\"end\":23234,\"start\":23230},{\"end\":23249,\"start\":23244},{\"end\":23265,\"start\":23260},{\"end\":23726,\"start\":23723},{\"end\":23744,\"start\":23740},{\"end\":23761,\"start\":23754},{\"end\":23775,\"start\":23771},{\"end\":23784,\"start\":23781},{\"end\":23799,\"start\":23793},{\"end\":23811,\"start\":23804},{\"end\":24373,\"start\":24369},{\"end\":24392,\"start\":24382},{\"end\":24410,\"start\":24402},{\"end\":24424,\"start\":24418},{\"end\":24439,\"start\":24433},{\"end\":24891,\"start\":24880},{\"end\":24908,\"start\":24901},{\"end\":24921,\"start\":24917},{\"end\":25366,\"start\":25359},{\"end\":25374,\"start\":25368}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6250093},\"end\":20415,\"start\":20118},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":26489293},\"end\":20686,\"start\":20417},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9067939},\"end\":21018,\"start\":20688},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":58031352},\"end\":21643,\"start\":21020},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49655339},\"end\":22260,\"start\":21645},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":25310609},\"end\":22686,\"start\":22262},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":19019872},\"end\":23126,\"start\":22688},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195944196},\"end\":23627,\"start\":23128},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":57761158},\"end\":24242,\"start\":23629},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3429309},\"end\":24808,\"start\":24244},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3719281},\"end\":25257,\"start\":24810},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":60440399},\"end\":25569,\"start\":25259}]", "bib_title": "[{\"end\":20200,\"start\":20118},{\"end\":20496,\"start\":20417},{\"end\":20766,\"start\":20688},{\"end\":21108,\"start\":21020},{\"end\":21764,\"start\":21645},{\"end\":22296,\"start\":22262},{\"end\":22733,\"start\":22688},{\"end\":23207,\"start\":23128},{\"end\":23714,\"start\":23629},{\"end\":24355,\"start\":24244},{\"end\":24873,\"start\":24810},{\"end\":25351,\"start\":25259}]", "bib_author": "[{\"end\":20228,\"start\":20202},{\"end\":20516,\"start\":20498},{\"end\":20778,\"start\":20768},{\"end\":20793,\"start\":20778},{\"end\":20810,\"start\":20793},{\"end\":20822,\"start\":20810},{\"end\":21120,\"start\":21110},{\"end\":21135,\"start\":21120},{\"end\":21144,\"start\":21135},{\"end\":21153,\"start\":21144},{\"end\":21174,\"start\":21153},{\"end\":21182,\"start\":21174},{\"end\":21780,\"start\":21766},{\"end\":21797,\"start\":21780},{\"end\":21816,\"start\":21797},{\"end\":21833,\"start\":21816},{\"end\":21850,\"start\":21833},{\"end\":21866,\"start\":21850},{\"end\":21882,\"start\":21866},{\"end\":21897,\"start\":21882},{\"end\":22327,\"start\":22298},{\"end\":22335,\"start\":22327},{\"end\":22345,\"start\":22335},{\"end\":22359,\"start\":22345},{\"end\":22376,\"start\":22359},{\"end\":22382,\"start\":22376},{\"end\":22390,\"start\":22382},{\"end\":22410,\"start\":22390},{\"end\":22415,\"start\":22410},{\"end\":22745,\"start\":22735},{\"end\":22751,\"start\":22745},{\"end\":22762,\"start\":22751},{\"end\":22769,\"start\":22762},{\"end\":23222,\"start\":23209},{\"end\":23236,\"start\":23222},{\"end\":23251,\"start\":23236},{\"end\":23267,\"start\":23251},{\"end\":23728,\"start\":23716},{\"end\":23746,\"start\":23728},{\"end\":23763,\"start\":23746},{\"end\":23777,\"start\":23763},{\"end\":23786,\"start\":23777},{\"end\":23801,\"start\":23786},{\"end\":23813,\"start\":23801},{\"end\":24375,\"start\":24357},{\"end\":24394,\"start\":24375},{\"end\":24412,\"start\":24394},{\"end\":24426,\"start\":24412},{\"end\":24441,\"start\":24426},{\"end\":24893,\"start\":24875},{\"end\":24910,\"start\":24893},{\"end\":24923,\"start\":24910},{\"end\":25368,\"start\":25353},{\"end\":25376,\"start\":25368}]", "bib_venue": "[{\"end\":20245,\"start\":20228},{\"end\":20531,\"start\":20516},{\"end\":20830,\"start\":20822},{\"end\":21305,\"start\":21182},{\"end\":21927,\"start\":21897},{\"end\":22451,\"start\":22415},{\"end\":22884,\"start\":22769},{\"end\":23334,\"start\":23267},{\"end\":23890,\"start\":23813},{\"end\":24503,\"start\":24441},{\"end\":25008,\"start\":24923},{\"end\":25391,\"start\":25376},{\"end\":23388,\"start\":23336},{\"end\":23954,\"start\":23892}]"}}}, "year": 2023, "month": 12, "day": 17}