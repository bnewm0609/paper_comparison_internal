{"id": 222296313, "updated": "2023-11-11 00:58:54.839", "metadata": {"title": "InferLine: latency-aware provisioning and scaling for prediction serving pipelines", "authors": "[{\"first\":\"Daniel\",\"last\":\"Crankshaw\",\"middle\":[]},{\"first\":\"Gur-Eyal\",\"last\":\"Sela\",\"middle\":[]},{\"first\":\"Xiangxi\",\"last\":\"Mo\",\"middle\":[]},{\"first\":\"Corey\",\"last\":\"Zumar\",\"middle\":[]},{\"first\":\"Ion\",\"last\":\"Stoica\",\"middle\":[]},{\"first\":\"Joseph\",\"last\":\"Gonzalez\",\"middle\":[]},{\"first\":\"Alexey\",\"last\":\"Tumanov\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 11th ACM Symposium on Cloud Computing", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Serving ML prediction pipelines spanning multiple models and hardware accelerators is a key challenge in production machine learning. Optimally configuring these pipelines to meet tight end-to-end latency goals is complicated by the interaction between model batch size, the choice of hardware accelerator, and variation in the query arrival process. In this paper we introduce InferLine, a system which provisions and manages the individual stages of prediction pipelines to meet end-to-end tail latency constraints while minimizing cost. InferLine consists of a low-frequency combinatorial planner and a high-frequency auto-scaling tuner. The low-frequency planner leverages stage-wise profiling, discrete event simulation, and constrained combinatorial search to automatically select hardware type, replication, and batching parameters for each stage in the pipeline. The high-frequency tuner uses network calculus to auto-scale each stage to meet tail latency goals in response to changes in the query arrival process. We demonstrate that InferLine outperforms existing approaches by up to 7.6x in cost while achieving up to 34.5x lower latency SLO miss rate on realistic workloads and generalizes across state-of-the-art model serving frameworks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3095488153", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cloud/CrankshawSMZS0T20", "doi": "10.1145/3419111.3421285"}}, "content": {"source": {"pdf_hash": "7bb359b734643ebd8b1180128a1ad30b7c80a7d7", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3419111.3421285", "status": "BRONZE"}}, "grobid": {"id": "69439a21342bd2439872e810101c882e952ff95a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7bb359b734643ebd8b1180128a1ad30b7c80a7d7.txt", "contents": "\nInferLine: Latency-Aware Provisioning and Scaling for Prediction Serving Pipelines\nOctober 19-21, 2020. October\n\nDaniel Crankshaw \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nGur-Eyal Sela \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nXiangxi Mo \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nCorey Zumar Databricks \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nU C Berkeley \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nAnyscale \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nJoseph Gonzalez jegonzal@berkeley.edu \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nGeorgia Tech \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nDaniel Crankshaw \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nGur-Eyal Sela \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nXiangxi Mo \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nCorey Zumar czumar@berkeley.edu \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nIon Stoica istoica@berkeley.edu \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nJoseph Gonzalez \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nAlexey Tumanov atumanov@gatech.edu \nMicrosoft Research\nBerkeley, Berkeley, Anyscale, Berkeley\n\nInferLine: Latency-Aware Provisioning and Scaling for Prediction Serving Pipelines\n\nACM Symposium on Cloud Computing (SoCC '20)\n20October 19-21, 2020. October10.1145/3419111.3421285ACM Reference Format: 19-21, 2020, Virtual Event, USA. ACM, New York, NY, USA, 15 pages. https://CCS CONCEPTS \u2022 General and reference \u2192 ReliabilityPerformance\u2022 Com- puter systems organization \u2192 Availability\u2022 Computing methodologies \u2192 Machine learning KEYWORDS inference, serving, machine learning, autoscaling\nServing ML prediction pipelines spanning multiple models and hardware accelerators is a key challenge in production machine learning. Optimally configuring these pipelines to meet tight end-to-end latency goals is complicated by the interaction between model batch size, the choice of hardware accelerator, and variation in the query arrival process.In this paper we introduce InferLine, a system which provisions and manages the individual stages of prediction pipelines to meet end-to-end tail latency constraints while minimizing cost. InferLine consists of a low-frequency combinatorial planner and a high-frequency auto-scaling tuner. The low-frequency planner leverages stage-wise profiling, discrete event simulation, and constrained combinatorial search to automatically select hardware type, replication, and batching parameters for each stage in the pipeline. The high-frequency tuner uses network calculus to auto-scale each stage to meet tail latency goals in response to changes in the query arrival process. We demonstrate that InferLine outperforms existing approaches by up to 7.6x in cost while achieving up to 34.5x lower latency SLO miss rate on realistic workloads and generalizes across state-of-the-art model serving frameworks.\n\nINTRODUCTION\n\nCloud applications as well as cloud infrastructure providers today increasingly rely on ML inference over multiple models linked together in a dataflow DAG. Examples include a digital assistant service (e.g., Amazon Alexa), which combines audio pre-processing with downstream models for speech recognition, topic identification, question interpretation and response and text-to-speech to answer a user's question. The natural evolution of these applications leads to a growth in the complexity of the prediction pipelines. At the same time, their latency-sensitive nature dictates tight tail latency constraints (e.g., 200-300ms). As the pipelines grow and the models used become increasingly sophisticated, they present a unique set of systems challenges for provisioning and managing these pipelines.\n\nEach stage of the pipeline must be assigned the appropriate hardware accelerator (e.g., CPU, GPU, TPU) -a task complicated by increasing hardware heterogeneity. Each model must be configured with the appropriate query batch size -necessary for optimal utilization of the hardware. And each pipeline stage can be replicated to meet the application throughput requirements. Per-stage decisions with respect to the hardware type and batch size affect the latency contributed by each stage towards the end-to-end pipeline latency bound by the application-specified Service Level Objective (SLO). This creates a combinatorial search space with three control dimensions per model (hardware type, batch size, number of replicas) and constraints on aggregate latency. A number of prediction serving systems exist today, including Clipper [9], TensorFlow Serving [37], and NVIDIA TensorRT Inference Server [36] that optimize for single model serving. This pushes the complexity of coordinating crossmodel interaction and, particularly, the questions of per-model configuration to meet application-level requirements, to the application developer. To the best of our knowledge, no system exists today that automates the process of pipeline provisioning and configuration, subject to specified tail latency SLO in a cost-aware manner. Thus, the goal of this paper is to address the problem of configuring and managing multistage prediction pipelines subject to end-to-end tail latency constraints cost efficiently.\n\nWe propose InferLine -a system for provisioning and management of ML inference pipelines. It composes with existing prediction serving frameworks, such as Clipper and TensorFlow Serving. It is necessary for such a system to contain two principal components: a low-frequency planner and a high-frequency tuner. The low-frequency planner is responsible for navigating the combinatorial search space to produce per-model pipeline configuration relatively infrequently to minimize cost. It is intended to run periodically to correct for workload drift or fundamental changes in the steady-state, long-term query arrival process. It is also necessary for integrating new models added to the repository and to integrate new hardware accelerators. The high frequency component is intended to operate at time scales three orders of magnitude faster. It monitors instantaneous query arrival traffic and tunes the running pipeline to accommodate unexpected query spikes cost efficiently to maintain latency SLOs under bursty and stochastic workloads.\n\nTo enable efficient exploration of the combinatorial configuration space, InferLine profiles each stage in the pipeline individually and uses these profiles and a discrete event simulator to accurately estimate end-to-end pipeline latency given the hardware configuration and batchsize parameters. The low-frequency planner uses a constrained greedy search algorithm to find the cost-minimizing pipeline configuration that meets the end-to-end tail latency constraint determined using the discrete event simulator on a sample planning trace.\n\nThe InferLine high-frequency tuner leverages traffic envelopes built using network calculus tools to capture the arrival process dynamics across multiple time scales and determine when and how to react to changes in the arrival process. As a consequence, the tuner is able to maintain the latency SLO in the presence of transient spikes and sustained variation in the query arrival process.\n\nIn summary, the primary contribution of this paper is a system for provisioning and managing machine learning inference pipelines for latency-sensitive applications cost efficiently. It consists of two key components that operate at time scales orders of magnitude apart to configure the system for near-optimal performance. The planner builds on a highfidelity model-based networked queueing simulator, while the tuner uses network calculus techniques to rapidly adjust pipeline configuration, absorbing unexpected query traffic variation cost efficiently.\n\nWe apply InferLine to provision and manage resources for multiple state-of-the-art prediction serving systems. We show that InferLine significantly outperforms alternative pipeline configuration baselines by a factor of up to 7.6X on cost, while exceeding 99% latency SLO attainment-the highest level of attainment achieved in relevant prediction serving literature.\n\n\nBACKGROUND AND MOTIVATION\n\nPrediction pipelines combine multiple machine learning models and data transformations to support complex prediction tasks [32]. For instance, state-of-the-art visual question answering services [1,23] combine language models with vision models to answer the question.\n\nA prediction pipeline can be represented as a directed acyclic graph (DAG), where each vertex corresponds to a model (e.g., mapping images to objects in the image) or a data transformation (e.g., extracting key frames from a video) and edges represent dataflow between vertices.\n\nIn this paper we study several ( Figure 2) representative prediction pipeline motifs. The Image Processing pipeline consists of basic image pre-processing (e.g., cropping and resizing) followed by image classification using a deep neural network. The Video Monitoring pipeline was inspired by [40] and uses an object detection model to identify vehicles and people and then performs subsequent analysis including vehicle and person identification and license plate extraction on any relevant images. The Social Media pipeline translates and categorizes posts based on both text and linked images by combining computer vision models with multiple stages of language models to identify the source language and translate the post if necessary. The TensorFlow (TF) Cascade pipeline combines fast and slow TensorFlow models, invoking the slow model only when necessary.\n\nIn the Social Media, Video Monitoring, and TF Cascade pipelines, a subset of models are invoked based on the output of earlier models in the pipeline. This conditional evaluation pattern appears in bandit algorithms [3,20] used for model personalization as well as more general cascaded prediction pipelines [2,14,24,34].\n\nWe show that for such pipelines InferLine is able to maintain latency constraints with P99 service level objectives (99% of query latencies must be below the constraint) at low cost, even under bursty and unpredictable workloads.\n\n\nChallenges\n\nPrediction pipelines present new challenges for the design and provisioning of prediction serving systems. First, we discuss how the proliferation of specialized hardware accelerators and the need to meet end-to-end latency constraints leads to a combinatorially large configuration space. Second, we discuss some of the complexities of meeting tight latency SLOs under bursty stochastic query loads. Third, we contrast this work with ideas from the data stream processing literature, which shares some structural similarities, but is targeted at fundamentally different applications and performance goals. Combinatorial Configuration Space: Many machine learning models can be computationally intensive with substantial opportunities for parallelism. In some cases, this parallelism can result in orders of magnitude improvements in throughput and latency. For example, in our experiments we found that TensorFlow can render predictions for the relatively large ResNet152 neural network at 0.6 queries per second (QPS) on a CPU and at 50.6 QPS on an NVIDIA Tesla K80 GPU, an 84x difference in throughput (Fig. 3). However, not all models benefit equally from hardware accelerators. For example, several widely used classical models (e.g., decision trees [7]) can be difficult to parallelize on GPUs, and common data transformations (e.g. text feature extraction) often cannot be efficiently computed on GPUs. In many cases, to fully utilize the available parallel hardware, queries must be processed in batches (e.g., ResNet152 required a batch size of 32 to maximize throughput on the K80). However, processing queries in a batch can also increase latency, as we see in Fig. 3. Because most hardware accelerators operate at vector level parallelism, the first query in a batch is not returned until the last query is completed. As a consequence, it is often necessary to set a maximum batch size to bound query latency. However, the choice of the maximum batch size depends on the hardware and model and affects the end-to-end latency of the pipeline.\n\nFinally, in heavy query load settings it is often necessary to replicate individual operators in the pipeline to provide the throughput demanded by the workload. As we scale up pipelines through replication, each operator scales differently, an effect that can be amplified by the use of conditional control flow within a pipeline causing some components to be queried more frequently than others. Low cost configurations require fine-grained scaling of each operator.\n\nAllocating parallel hardware resources to a single model presents a complex model dependent trade-off space between cost, throughput, and latency. This trade-off space grows exponentially with each model in a prediction pipeline. Decisions made about the choice of hardware, batching parameters, and replication factor at one stage of the pipeline affect the set of feasible choices at the other stages due to the need to meet end-to-end latency constraints. For example, trading latency for increased throughput on one model by increasing the batch size reduces the latency budget of other models in the pipeline and, as a consequence, constrains feasible hardware configurations as well. Queueing Delays: As stages of a pipeline may operate at different speeds, due to resource and model heterogeneity, it is necessary to have a queue per stage. Queueing also allows to absorb query inter-arrival process irregularities and can be a significant end-to-end latency component. Queueing delay must be explicitly considered during pipeline configuration, as it directly depends on the relationship between the interarrival process and system configuration. Stochastic and Unpredictable Workloads: Prediction serving systems must respond to bursty, stochastic query streams. At a high-level these stochastic processes can be characterized by their average arrival rate \u03bb and their coefficient of variation, a dimensionless measure of variability defined by CV 2 A = \u03c3 2 \u03bc 2 , where \u03bc = 1 \u03bb and \u03c3 are the mean and standard-deviation of the query inter-arrival time. Processes with higher CV 2 A have higher variability and often require additional over-provisioning to meet latency objectives. Clearly, over-provisioning the whole pipeline on specialized hardware can be prohibitively expensive. Therefore, it is critical to be able to identify and provision the bottlenecks in a pipeline to accommodate the bursty arrival process. Finally, as the workload changes, we need mechanisms to monitor, quickly detect, and tune individual stages in the pipeline. Comparison to Stream Processing Systems: Many of the challenges around configuring and scaling pipelines have been studied in the context of generic data stream processing systems [8,10,30,33,38]. However, these systems focus their effort on supporting more traditional data processing workloads, which include stateful aggregation operators and support for a variety of windowing operations. These systems tend to focus on maximizing throughput while avoiding backpressure, with latency as a second order performance goal ( \u00a78). Even those that consider latency directly such as [8] do not manage tail latency.\n\n\nSYSTEM DESIGN\n\nIn this section, we provide a high-level overview of the main system components in InferLine (Fig. 1). The system requires a planner that operates infrequently and re-configures the whole pipeline w.r.t. all of the control parameters and a tuner that makes adjustments to the pipeline configurations in response to dynamically observed query traffic patterns.\n\nInferLine runs on top of any prediction serving system that meets a few simple requirements. The underlying serving system must be able to 1) deploy multiple replicas of a model and scale the number of replicas at runtime across a cluster of worker machines, 2) allow for batched inference with the ability to configure a maximum batch size, and 3) use a centralized batched queueing system to distribute batches among model replicas. The first two properties are necessary for InferLine to configure the serving engine, and a centralized queueing system provides deterministic queueing behavior that can be accurately simulated by the Estimator. In our experimental evaluation, we run InferLine with both Clipper [9] and TensorFlow Serving [37]. Both systems needed only minor modifications to meet these requirements. Using InferLine: To deploy a new prediction pipeline managed by InferLine, developers provide a driver program, sample query trace used for planning, and a latency service level objective. The driver function interleaves application-specific code with asynchronous calls to models hosted in the underlying serving system to execute the pipeline.\n\nThe Planner runs as a standalone Python process that runs periodically independent of the prediction serving framework. The Tuner runs as a standalone process implemented in C++. It observes the incoming arrival trace streamed to it by the centralized queueing system and triggers model addition/removal executed by serving-framework-specific APIs. Low-Frequency Planning: The first time planning is performed, InferLine uses the Profiler to create performance profiles of all the individual models referenced by the driver program. A performance profile captures model throughput as a function of hardware type and maximum batch size. An entry in the model profile is measured empirically by evaluating the model in isolation in the given configuration using the queries in the sample trace.\n\nThe Planner finds a cost-efficient initial pipeline configuration subject to the end-to-end latency SLO and the specified arrival process. It uses a globally-aware, cost-minimizing optimization algorithm to set the three control parameters for each model in the pipeline. In each iteration of the optimization algorithm, the Planner uses the model profiles to select a costminimizing step while relying on the Estimator to check for latency constraint violations. After the initial configuration is generated and the pipeline is deployed to serve live traffic, the Planner is re-run periodically (hours to days) on the most recent arrival history to find a cost-optimal configuration for the current workload. High-Frequency Tuning: The Tuner monitors the dynamic behavior of the arrival process to adjust per-model replication factors and maintain high SLO attainment at low cost. InferLine only adjusts per-model replication factors during tuning to avoid expensive hardware migration operations during live serving and to ensure that scaling decisions can be made quickly to maintain latency SLOs even during sharp bursts. The Tuner continuously monitors the current traffic envelope [19] to detect deviations from the planning trace traffic envelope at different timescales simultaneously. By analyzing the timescale at which the deviation occurred, the Tuner is able to take appropriate mitigating action within seconds to ensure that SLOs are met without unnecessarily increasing cost. It ensures that latency SLOs are maintained during unexpected changes to the arrival workload in between runs of the Planner.\n\n\nLOW-FREQUENCY PLANNING\n\nDuring planning, the Profiler, Estimator and Planner are used to estimate model performance characteristics and optimally provision and configure the system for a given sample workload and latency SLO. In this section, we expand on each of these three components.\n\n\nProfiler\n\nThe Profiler creates performance profiles for each of the models in the pipeline as a function of batch size and hardware. Profiling begins with InferLine executing the sample set of queries on the pipeline. This generates input data for profiling each of the component models individually. We also track the frequency of queries visiting each model, called the scale factor, s. The scale factor represents the conditional probability that a model will be queried given a query entering the pipeline, independent of the behavior of any other models. It is used by the Estimator to simulate the effects of conditional control flow on latency ( \u00a74.2) and the Tuner to make scaling decisions ( \u00a75).\n\nThe Profiler captures model throughput as a function of hardware type and batch size to create per-model performance profiles. An individual model configuration corresponds to a specific value for each of these parameters as well as the model's replication factor. Because the models scale horizontally, profiling a single replica is sufficient. Profiling only needs to be performed once for each hardware and batch size pair and is re-used in subsequent runs of the Planner.\n\n\nEstimator\n\nThe Estimator is responsible for rapidly estimating the end-toend latency of a given pipeline configuration for the sample query trace. It takes as input a pipeline configuration, the individual model profiles, and a sample trace of the query workload, and returns accurate estimates of the latency for each query in the trace. The Estimator is implemented as a continuous-time, discrete-event simulator [5], simulating the entire pipeline, including queueing delays and conditional control flow (using the scale factor s). The simulator maintains a global logical clock that is advanced from one discrete event to the next with each event triggering future events that are processed in temporal order. Because the simulation only models discrete events, we are able to faithfully simulate hours worth of real-world traces in hundreds of milliseconds. The Estimator simulates the deterministic behavior of queries flowing through a centralized batched queueing system. It combines this with the model profile information which informs the simulator how long a model running on a specific hardware configuration will take to process a batch of a given size.\n\n\nPlanning Algorithm\n\nAt a high-level, the planning algorithm is an iterative constrained optimization procedure that greedily minimizes cost while ensuring that the latency constraint is satisfied. The algorithm can be divided into two phases. In the first (Algorithm 1), it finds a feasible initial configuration that meets the latency SLO while ignoring cost. In the second (Algorithm 2), it greedily modifies the configuration to reduce the cost while using the Estimator to identify and reject configurations that violate the latency SLO. The algorithm converges when it can no longer make any cost reducing modifications to the configuration without violating the SLO. Initialization (Algorithm 1): First, an initial latencyminimizing configuration is constructed by setting the batch size to 1 using the lowest latency hardware available for each model (lines 2-5). If the service time under this configuration (the sum of the processing latencies of all the models on the longest path through the pipeline DAG) is greater than the SLO then the latency constraint is infeasible given the available hardware and the Planner terminates (lines 6-7). Otherwise, the Planner then iteratively determines the throughput bottleneck and increases that model's replication factor until it is no longer the bottleneck (lines 9-11).\n\nCost-Minimization (Algorithm 2): In each iteration of the cost-minimizing process, the Planner considers three candidate modifications for each model: increase the batch size, decrease the replication factor, or downgrade the hardware (line 5), searching for the modification that maximally decreases cost while still meeting the latency SLO. It evaluates each modification on each model in the pipeline (lines 8-10), discarding candidates that violate the latency SLO according to the Estimator (line 11).\n\nThe batch size only affects throughput and does not affect cost. It will therefore only be the cost-minimizing modification if the other two would create infeasible configurations. Increasing the batch size does increase latency. The batch size is increased by factors of two as the throughput improvements from larger batch sizes have diminishing returns (observe Fig. 3). In contrast, decreasing the replication factor directly reduces cost. Removing replicas is feasible when a previous iteration of the algorithm has increased the batch size for a model, increasing the per-replica throughput.\n\nDowngrading hardware is more involved than the other two actions, as the batch size and replication factor for the model must be re-evaluated to account for the differing batching behavior of the new hardware. It is often necessary to reduce the batch size and increase replication factor to find a feasible pipeline configuration. However, the reduction in hardware price sometimes compensates for the increased replication factor. For example, in Fig. 10, the steep decrease in cost when moving from an SLO of 0.1 to 0.15 can be largely attributed to downgrading the hardware of a language identification model from a GPU to a CPU. To evaluate a hardware downgrade, we first freeze the configurations of the other models in the pipeline and perform the initialization stage for that model using the next cheapest hardware. The planner then performs a localized version of the cost-minimizing algorithm to find the batch size and replication factor for the model on the newly downgraded resource allocation needed to reduce the cost of the previous configuration. If there is no cost reducing feasible configuration the hardware downgrade action is rejected.\n\nAt the point of termination, the planning algorithm provides the following guarantees: (1) If there is a configuration that meets the latency SLO, then the algorithm will return a valid configuration. (2) There is no single action that can be taken to reduce cost without violating the SLO.\n\n\nHIGH-FREQUENCY TUNING\n\nInferLine's Planner finds an efficient, low-cost configuration that is guaranteed to meet the provided latency objective. However, this guarantee only holds for the sample planning workload provided to the planner. Real workloads evolve over time, changing in both arrival rate (change in \u03bb ) as well as becoming more or less bursty (change in CV 2 A ). When the serving workload deviates from the sample, the original configuration will either suffer from queue buildups leading to SLO misses or be over-provisioned and incur unnecessary costs. The Tuner both detects these changes as they occur and takes the appropriate scaling action to maintain both the latency constraint and cost-efficiency objective.\n\nIn order to maintain P99 latency SLOs, the Tuner must be able to detect changes in the arrival workload dynamics across multiple timescales simultaneously. The Planner guarantees that the pipeline is adequately provisioned for the sample trace. The Tuner's detection mechanism detects when the current request workload exceeds the sample workload. To do this, we draw on the idea of traffic envelopes from network calculus [19] to characterize the workloads.\n\nA traffic envelope for a workload is constructed by sliding a window of size \u0394T i over the workload's inter-arrival max queries \u0394T The pipeline will be re-scaled for the new arrival rate r max =\nq 3 w 3 .\nprocess and capturing the maximum number of queries seen anywhere within this window (see Fig. 4). Thus, each x = \u0394T i is mapped to y = q i (number of queries) for all x over the duration of a trace. This powerful characterization captures how much the workload can burst in any given interval of time.\n\nIn practice, we discretize the x-axis by setting the smallest \u0394T i to T s , the service time of the system, and then double the window size up to 60 seconds. For each such interval, the maximum arrival rate r i for this interval can be computed as r i = q i \u0394T i . By measuring r i across all \u0394T i simultaneously we capture a fine-grain characterization of the arrival workload that enables simultaneous detection of changes in both short term (burstiness) and long term (average arrival rate) traffic behavior. Initialization: During planning, the Planner constructs the traffic envelope for the sample arrival trace. The Planner also computes the max-provisioning ratio for each model \u03c1 m = \u03bb \u03bc m , the ratio of the arrival rate \u03bb to the maximum throughput of the model \u03bc in its current configuration. While the max-provisioning ratio is not a fundamental property of the pipeline, it provides a useful heuristic to measure how much \"slack\" the Planner has determined is needed for this model to be able to absorb bursts and still meet the SLO. The Planner then provides the Tuner with the traffic envelope for the sample trace, the max-provisioning ratio \u03c1 m and single replica throughput \u03bc m for each model in the pipeline.\n\nIn the low-latency applications that InferLine targets, failing to scale up the pipeline in the case of an increased workload results in missed latency objectives and degraded quality of service, while failing to scale down the pipeline in the case of decreased workload only results in slightly higher costs. We therefore handle the two situations separately. Scaling Up (Algorithm 3): The Tuner continuously computes the traffic envelope for the current arrival workload. This yields a set of arrival rates for the current workload that can be directly compared to those of the sample workload (as in Fig. 5). If any of the current rates exceed their corresponding sample rates (lines 3-6), the pipeline is underprovisioned and the Tuner checks whether it should add replicas for any models in the pipeline. At this point, not only has the Tuner detected that rescaling may be necessary, it also knows what arrival rate it needs to reprovision the pipeline for: the current workload rate r max that triggered rescaling. If the overall \u03bb of the workload has not changed but it has become burstier, this will be a rate computed with a smaller \u0394T i , and if the burstiness of the workload is stationary but the \u03bb has increased, this will be a rate with a larger \u0394T i . In the case that multiple rates have exceeded their sample trace counterpart, we take the max rate.\n\nTo determine how to reprovision the pipeline, the Tuner computes the number of replicas needed for each model to process r max as k m = r max s m \u03bc m \u03c1 m (lines 9-10). s m is the scale factor for model m, which prevents over-provisioning for a model that only receives a portion of the queries due to conditional logic. \u03c1 m is the max-provisioning ratio, which ensures enough slack remains in the model to handle bursts. The Tuner then adds the additional replicas needed for any models that are underprovisioned (lines [11][12]. Scaling Down (Algorithm 4): InferLine takes a conservative approach to scaling down the pipeline to prevent unnecessary configuration oscillation which can cause SLO misses. Drawing on the work in [13], the Tuner waits for a period of time after any configuration changes to allow the system to stabilize before considering any down scaling actions. Infer-Line uses a delay of 15 seconds (3x the 5 second activation time of spinning up new replicas in the underlying prediction serving frameworks), but the precise value is unimportant as long as it provides enough time for the pipeline to stabilize after a scaling action. Once this delay has elapsed (line 2), the Tuner continuously computes the max request rate \u03bb new that has been observed over the last 30 seconds, using 5 second windows (line 3). The Tuner computes the number of replicas needed for each model to process \u03bb new similarly to the procedure for scaling up, setting k m = \u03bb new s m \u03bc m \u03c1 p (lines 5-6). In contrast to scaling up, when scaling down we use the minimum max provisioning factor in the pipeline \u03c1 p = min (\u03c1 m \u2200m \u2208 models). Because the max provisioning factor is a heuristic that has some dependence on the sample trace, using the min across the pipeline provides a more conservative downscaling algorithm and ensures the Tuner is not overly aggressive in removing replicas. If the workload has dropped substantially, the next time the Planner runs it will find a new lower-cost configuration that is optimal for the new workload.\n\n\nEXPERIMENTAL SETUP\n\nTo evaluate InferLine we constructed four prediction pipelines (Fig. 2) representing common application domains and using models trained in a variety of machine learning frameworks [25-27, 35]. We configure each pipeline with varying input arrival processes and latency budgets. We evaluate the latency SLO attainment and pipeline cost under a range of both synthetic and real world workload traces. Coarse-Grained Baseline Comparison: Current prediction serving systems do not provide functionality for provisioning and managing prediction pipelines with end-to-end latency constraints. Instead, the individual pipeline components are each deployed as a separate micro-service to a prediction serving system such as [9,15,36,37] and a pipeline is manually constructed by individual calls to each service.\n\nAny performance tuning for end-to-end latency or cost treats the entire pipeline as a single black-box service and tunes it as a whole. We therefore use this same approach as our baseline for comparison. Throughout the experimental evaluation we refer to this as the Coarse-Grained baseline. We deploy pipelines configured with both InferLine and the coarse-grained baseline to the same underlying predictionserving framework. All experiments used Clipper [9] as the prediction-serving framework except for those in Fig. 14 which compare InferLine running on Clipper and TensorFlow Serving [37]. Both prediction-serving frameworks were modified to add a centralized batched queueing system.\n\nWe use the techniques proposed in [13] to do both lowfrequency planning and high-frequency tuning for the coarsegrained pipelines as a baseline for comparison. In this baseline, we profile the entire pipeline as a single black box to identify the single maximum batch size capable of meeting the SLO, in contrast to InferLine's per-model profiling. The pipeline is then replicated as a single unit to achieve the required throughput as measured on the same sample arrival trace used by the Planner. We evaluate two strategies for determining required throughput. CG-Mean uses the mean request rate computed over the arrival trace while CG-Peak determines the peak request rate in the trace computed using a sliding window of size equal to the SLO. The coarse-grained tuning mechanism scales the number of pipeline replicas using the scaling algorithm introduced in [13]. Physical Execution Environment: We ran all experiments in a distributed cluster on Amazon EC2. The pipeline driver client was deployed on an m4.16xlarge instance which has 64 vCPUs, 256 GiB of memory, and 25Gbps networking across two NUMA zones. We used large client instance types to ensure that network bandwidth from the client is not a bottleneck. Models were deployed to a cluster of up to 16 p2.8xlarge GPU instances. This instance type has 8 NVIDIA K80 GPUs, 32 vCPUs, 488.0 GiB of memory and 10Gbps networking all within a single NUMA zone. All instances ran Ubuntu 16.04 with Linux Kernel version 4.4.0. CPU costs were computed by dividing the total hourly cost of an instance by the number of CPUs. GPU costs were computed by taking the difference between a GPU instance and its equivalent non-GPU instance (all other hardware matches), then dividing by the number of GPUs. This cost model provides consistent prices across instance sizes. Workload Setup: We generated synthetic traces by sampling inter-arrival times from a gamma distribution with differing mean \u03bc to vary the request rate, and CV 2 A to vary the workload burstiness. When reporting performance on a specific workload as characterized by \u03bb = 1 \u03bc and CV 2 A , a trace for that workload was generated once and reused across all comparison points to provide a more direct comparison of performance. We generated separate traces with the same performance characteristics for profiling and evaluation to avoid overfitting to the sample trace.\n\nTo generate synthetic time-varying workloads, we evolve the workload generating function between different Gamma distributions over a specified period of time, the transition time. This allows us to generate workloads that vary in mean throughput, CV 2 A , or both, and thus evaluate the performance of the Tuner under a wide range of conditions.\n\nIn Fig. 7 we evaluate InferLine on traces derived from real workloads studied in the AutoScale system [13]. These workloads only report the average request rate each minute for an hour, rather than providing the full trace of query inter-arrival times. To derive traces from these workloads, we followed the approach used by [13] to re-scale the max throughput to 300 QPS, the maximum throughput supported by the coarsegrained baseline pipelines on a 16 node (128 GPU) cluster. We then iterated through each of the mean request rates in the workload and sample from a Gamma distribution with CV 2 A 1.0 for 30 seconds. We use the first 25% of the trace as the sample for the Planner, and the remaining 75% as the live serving workload (see Fig. 7).\n\n\nEXPERIMENTAL EVALUATION\n\nIn this section we evaluate InferLine's performance. First, we evaluate end-to-end performance of InferLine relative to current state of the art methods for configuring and provisioning prediction pipelines with end-to-end latency constraints ( \u00a77.1). We show that InferLine outperforms the baselines on latency SLO attainment and cost for synthetic and realworld derived workloads with both stable and unpredictable workload dynamics. Second, we demonstrate that InferLine is robust to unplanned dynamics of the arrival process ( \u00a77.2): changes in the arrival rate as well as unexpected inter-arrival bursts, as the Tuner rapidly re-scales the pipeline in response to these changes. Third, we perform an ablation study to show that the system benefits from both the low-frequency planning and high-frequency tuning. We conclude by showing that InferLine composes with multiple underlying predictionserving frameworks ( \u00a77.4).\n\n\nEnd-to-end Evaluation\n\nWe first establish that InferLine's planning and tuning components outperform state-of-the-art pipeline-level configuration alternatives in an end-to-end evaluation ( \u00a77.1). InferLine is able to achieve the same throughput at significantly lower cost, while maintaining zero or near-zero latency SLO miss rate. Low-Frequency Planning: In the absence of a workloadaware planner ( \u00a74.3), the options are limited to either (a) provisioning for the peak (CG Peak), or (b) provisioning for the mean (CG Mean) request rate. We compare InferLine to these two end-points of the configuration continuum across 2 pipelines (Fig. 6). InferLine meets latency SLOs at the lowest cost. CG Peak meets SLOs, but at much higher cost, particularly for burstier workloads. And CG Mean is not provisioned to handle bursts which results in high SLO miss rates.\n\nThe Planner consistently finds lower cost configurations than both coarse-grained provisioning strategies and is able to achieve up to a 7.6x reduction in cost by minimizing pipeline imbalance. Finally, we observe that the Planner consistently finds configurations that meet the SLO for workloads with the same characteristics as the sample trace used for planning. Next, we evaluate the Tuner's ability to meet SLOs during unexpected changes in workload. High-Frequency Tuning: InferLine is able to (1) maintain a negligible SLO miss rate, and (2) and reduce cost by up to 4.2x when compared to the state-of-the-art approach [13] when handling unexpected changes in the arrival rate and burstiness. In Fig. 7 we evaluate the Social Media pipeline on 2 traces derived from real workloads studied in [13]. The Planner finds a 5x cheaper initial configuration than coarsegrained provisioning (Fig. 7(a)). Both systems achieve nearzero SLO miss rates throughout most of the workload, and when the big spike occurs we observe that InferLine's Tuner quickly reacts by scaling up the pipeline as described in \u00a75. As soon as the spike dissipates, InferLine scales the pipeline down to maintain a cost-efficient configuration. In contrast, the coarse-grained tuning mechanism operates much slower and, therefore, is ill-suited for reacting to rapid changes in the request rate of the arrival process.\n\nIn Fig. 7(b), InferLine scales up the pipeline smoothly and recovers rapidly from an instantaneous spike, unlike the CG baseline. As the workload drops quickly after 1000 seconds, InferLine rapidly responds by shutting down replicas to reduce cluster cost. In the end, InferLine and the coarse-grained pipelines converge to similar costs due to the low terminal request rate which hides the effects of pipeline imbalance, but InferLine has a 34.5x lower SLO miss rate than the baseline.\n\nWe further evaluate the differences between the InferLine and coarse-grained tuning algorithms on a set of synthetic workloads with increasing arrival rates in Fig. 8. We observe that the traffic envelope monitoring described in \u00a75 enables InferLine to detect the increase in arrival rate earlier and therefore scale up the pipeline sooner to maintain a low SLO miss rate. In contrast, the coarse-grained baseline only reacts to the increase in request rate at the point when the pipeline is overloaded and therefore reacts when the pipeline is already in an infeasible configuration. The effect of this delayed reaction is compounded by the longer provisioning time needed to replicate an entire pipeline, resulting in the coarse-grained baselines being unable to recover before the experiment ends. They will eventually recover as we see in Fig. 7 but only after suffering a period of 100% SLO miss rate.   [13]. Both workloads were evaluated on the Social Media pipeline with a 150ms SLO. In Fig. 7(a), InferLine maintains a 99.8% SLO attainment overall at a total cost of $8.50, while the coarse-grained baseline has a 93.7% SLO attainment at a cost of $36.30. In Fig. 7(b), InferLine has a 99.3% SLO attainment at a cost of $15.27, while the coarse-grained baseline has a 75.8% SLO attainment at a cost of $24.63, a 34.5x lower SLO miss rate.  \n\n\nSensitivity Analysis\n\nWe evaluate the sensitivity and robustness of the Planner and the Tuner. We analyze the accuracy of the Estimator in estimating tail latencies from the sample trace and the Planner's response to varying arrival rates, latency SLOs, and burstiness factors. We also analyze the Tuner's sensitivity to changes in the arrival process and ability to re-scale individual pipeline stages to maintain latency SLOs during these unexpected changes to the workload. Planner Sensitivity: We first evaluate how closely the latency distribution produced by the Estimator reflects the latency distribution of the running system in Fig. 9. We observe that the estimated and measured P99 latencies are close across all four experiments. Further, we see that the Estimator has the critical property of ensuring that the P99 latency of feasible configurations is below the latency objective. The near-zero SLO miss rates in Fig. 6 are a further demonstration of the Estimator's ability to detect infeasible configurations.\n\nNext, we evaluate the Planner's performance under varying load, burstiness, and end-to-end latency SLOs. We observe three important trends in Fig. 10. First, increasing burstiness (from CV 2 A =1 to CV 2 A =4) requires more costly configurations as the Planner provisions more capacity to ensure that transient bursts do not cause the queues to diverge more than the SLO allows. We also see the cost gap narrowing between CV 2 A =1 and CV 2 A =4 as the SLO increases. As the SLO increases, additional slack in the deadline can absorb more variability in the arrival process and therefore fewer pipeline replicas are needed to process transient bursts within the SLO. Second, the cost decreases as a function of the latency SLO. While this downward cost trend generally holds, the optimizer occasionally finds sub-optimal configurations, as it makes locally optimal decisions to change a resource assignment. Third, the cost increases as a function of expected arrival rate, as more queries require more replicas. Tuner Sensitivity: A common type of unpredictable behavior is a change in the arrival rate. We compare the behavior of InferLine with and without its Tuner enabled as the arrival rate changes from the planned-for 150 QPS to 250 QPS. We We observe that the Tuner quickly detects and scales up the pipeline in response to increases in \u03bb . Further, the Tuner finds cost-efficient configurations that either match or are close to those found by the Planner given full oracle knowledge of the trace.\n\nvary the rate of arrival throughput change \u03c4. InferLine is able to maintain the SLO miss rate close to zero while matching or beating two alternatives: (a) a pipeline with only the Planner enabled but given full oracle knowledge of the arrival trace, and (b) a pipeline with only the Planner enabled and provided only the sample planning trace. Neither of these baselines responds to changes in workload during live serving. As we see in Fig. 11, InferLine continues to meet the SLO, and increases the cost of the pipeline only for the duration of the unexpected burst. The oracle Planner with full knowledge of the workload is able to find the cheapest configuration at the peak because it is equipped with the ability to configure batch size and hardware type along with replication factor. But it pays this cost for the entire duration of the workload. The Planner without oracular knowledge starts missing latency SLOs as soon as the ingest rate increases as it is unable to respond to unexpected changes in the workload without the Tuner. A less obvious but potentially debilitating change in the arrival process is an increase in its burstiness, even while maintaining the same mean arrival rate \u03bb . This change is also harder to detect, as common practice is to look at moments of the arrival rate distribution, such as the mean or 99%. In Fig. 12 we show that Tuner is able to detect deviation from expected arrival burstiness and react to meet the latency SLOs by employing the traffic-envelope detection mechanism described in \u00a75.\n\n\nAttribution of Benefit\n\nInferLine benefits from (a) low-frequency planning and (b) high-frequency tuning. Thus, we evaluate the following comparison points: baseline coarse grain planning (Baseline Plan), InferLine's planning (InferLine Plan), InferLine planning with baseline tuning (InferLine Plan + Baseline Tune), and   Planner and high-frequency Tuner on the Image Processing pipeline. We observe that the Planner finds a more than 3x cheaper configuration than the baseline. We also observe that InferLine's Tuner is the only alternative that maintains the latency SLO throughout the workload.\n\nInferLine planning with InferLine tuning (InferLine Plan + In-ferLine Tune), building up from pipeline-level configuration to the full feature set InferLine provides. InferLine's Planner reduces the cost of the initial pipeline configuration by more than 3x (Fig. 13), but starts missing latency SLOs when the request rate increases. Adding the baseline tuning mechanism (InferLine Plan + Baseline Tune) adapts the configuration, but too late to completely avoid SLO misses, although it recovers faster than planning-only alternatives. The InferLine Tuner has the highest SLO attainment and is the only alternative that maintains the SLO across the entirety of the workload. This emphasizes the need for both the Planner for initial costefficient pipeline configuration, and the Tuner to promptly and cost-efficiently adapt to unexpected workload changes.\n\n\nMultiple Prediction-Serving Frameworks\n\nThe contributions of this work generalize to different underlying serving frameworks. Here, we evaluate the InferLine Planner running on top of both Clipper and TensorFlow Serving (TFS). In this experiment, we achieve the same low latency SLO miss rate for both prediction-serving frameworks. This indicates the generality of the planning algorithms used to configure individual models in InferLine. In Fig. 14 we show both the SLO attainment rates and the cost of pipeline provisioning when running InferLine on the two serving frameworks. The cost for running on TFS is slightly higher due to some additional RPC serialization overheads not present in Clipper.\n\n\nRELATED WORK\n\nA number of recent efforts study the design of generic prediction serving systems [4,9,36,37]. TensorFlow Serving [37] is a commercial grade prediction serving system primarily designed to support prediction pipelines implemented using TensorFlow [35], but does not provide any automatic provisioning or support latency constraints. Clipper adopts a containerized design allowing each model to be individually managed, configured, and deployed in separate containers, but does not support prediction pipelines or reasoning about latency deadlines across models. Several systems have explored offline pipeline configuration for data pipelines [6,16]. However, these target generic data streaming pipelines. They use black box optimization techniques that require running the pipeline end-to-end to measure the performance of each candidate configuration. In-ferLine instead leverages performance profiles of each stage and a simulation-based performance estimator to explore the configuration space without needing to run the pipeline.\n\nDynamic pipeline scaling is a critical feature in data streaming systems to avoid backpressure and over-provisioning. Systems such as [11,18] are throughput-oriented with the goal of maintaining a well-provisioned system under changes in the request rate. The DS2 autoscaler in [18] estimates true processing rates for each operator in the pipeline by instrumenting the underlying streaming system. They use these processing rates in conjunction with the pipeline topology structure to estimate the optimal degree of parallelism for all operators at once. In contrast, [11] identifies a single bottleneck stage at a time, taking several steps to converge to a well-provisioned system. Both systems provision for the average ingest rate and ignore any burstiness which can transiently overload the system. In contrast, InferLine maintains a traffic envelope of the request workload and uses this to ensure that the pipeline is well-provisioned for the peak workload across several timescales simultaneously, including any burstiness (see \u00a75).\n\nA few streaming autoscaling systems consider latencyoriented performance goals [12,21,22]. The closest work to InferLine, [21] treats each stage in a pipeline as a singleserver queueing system and uses queueing theory to estimate the total queue waiting time of a job under different degrees of parallelism. They leverage this queueing model to greedily increase the parallelism of the stage with the highest queue waiting time until they can meet the latency SLO. However, their queueing model only considers average latency and ignores tail latency. InferLine's Tuner automatically provisions for worst-case latencies.\n\nSEDA [39] studies dynamically controlling pipeline systems connected by queues. However, SEDA focuses on managing multi-threaded single-process systems using techniques such as thread pool management and adaptive load-shedding to remain performant even when overloaded. In contrast, Infer-Line is a distributed system that maintains end-to-end latency objectives and scales resource usage to avoid overload.\n\nVideoStorm [40] explores the design of a streaming video processing system with a distributed design with pipeline operators provisioned across compute nodes and explores the combinatorial search space of hardware and model configurations. VideoStorm jointly optimizes for quality and lag and does not provide latency guarantees.\n\nNexus [31] configures DNN inference pipelines for videostreaming applications. Similar to InferLine, it uses model profiles to understand model batching behavior and provisions pipelines for end-to-end latency objectives. However, they do not configure which hardware to use, instead assuming a homogeneous GPU cluster. They also rely on admission control to reject queries while InferLine's Tuner quickly re-scales the pipeline to maintain SLOs without rejecting queries.\n\nA large body of prior work leverages profiling for scheduling, including recent work on workflow-aware scheduling [17,29]. In contrast, InferLine exploits the computeintensive and side-effect free nature of ML models to estimate end-to-end pipeline performance based on individual model profiles.\n\nAutoscale [13] surveys work aimed at automatically scaling the number of servers reactively, subject to changing load in the context of web services. Autoscale works well for single model replication without batching as it assumes bitat-a-time instead of batch-at-a-time processing. However, we find that the InferLine Tuner outperforms the coarse-grain baselines using the Autoscale mechanism on both latency SLO attainment and cost ( \u00a77.1).\n\n\nLIMITATIONS\n\nOne assumption in the Planner is that the available hardware has a total ordering of latency across all batch sizes. As specialized accelerators for ML proliferate, there may be settings where one accelerator is slower than another at smaller batch sizes but faster at larger batch sizes. This would require modifications to the hardware downgrade portion of the Planner to account for this batch-size dependent ordering.\n\nA second assumption is that the inference latency of ML models is independent of their input. There are emerging classes of ML tasks [27,28] where state-of-the-art models have inference latency that varies based on the input. One way to account for this is to measure this latency distribution during profiling based on the variability in the sample queries and use the tail of the distribution as the processing time in the estimator, which will lead to feasible but more costly configurations.\n\n\nCONCLUSION\n\nIn this paper we studied the problem of provisioning and managing prediction pipelines to meet end-to-end tail latency requirements at low cost and across heterogeneous parallel hardware. We introduced InferLine--a system which efficiently provisions prediction pipelines subject to end-to-end latency constraints. InferLine combines a low-frequency Planner that finds cost-optimal configurations with a high-frequency Tuner that rapidly re-scales pipelines to meet latency SLOs in response to changes in the query workload. The low-frequency Planner combines profiling, discrete event simulation, and constrained combinatorial optimization to find the cost minimizing configuration that meets the end-to-end tail latency requirements without ever instantiating the system ( \u00a74). The high-frequency Tuner uses network-calculus to quickly autoscale each stage of the pipeline to accommodate changes in the query workload ( \u00a75). In combination, these components achieve the combined effect of cost-efficient heterogeneous prediction pipeline provisioning that can be deployed to a variety of prediction-serving frameworks to serve applications with a range of tight end-to-end latency objectives. As a result, we achieve up to 7.6x improvement in cost and 34.5x improvement in SLO attainment for the same throughput and latency objectives over state-of-the-art provisioning alternatives.\n\n\nACKNOWLEDGEMENTS\n\nIn addition to NSF CISE Expeditions Award CCF-1730628 and NSF CAREER Award 1846431, this research is supported by gifts from Amazon Web Services, Ant Group, Cap-italOne, Ericsson, Facebook, Futurewei, Google, Intel, Microsoft, Nvidia, Scotiabank, Splunk and VMware.\n\nFigure 1 :\n1InferLine System Overview\n\nFigure 2 :\n2Example Pipelines. We evaluate InferLine on four prediction pipelines that span a range of models, control flow, and input characteristics.\n\n\nin pipeline do 9 foreach action in actions do 10 new = action(model, pipeline); 11 if Feasible(new) then 12 if new.cost < best.cost then 13 best = new; 14 if best is not NULL then 15 pipeline = best; 16 until best == NULL; 17 return pipeline;\n\nFigure 4 :\n4Arrival and Service Curves. The arrival curve captures the maximum number of queries to be expected in any interval of time x seconds wide. The service curve plots the expected number of queries processed in an interval of time x seconds wide.\n\nFigure 5 :\n5Observed traffic envelope exceeds sample envelope. The observed traffic envelope (in red) exceeds the sample trace traffic envelope (in blue) at window w 3 , triggering the Tuner to scale up the pipeline.\n\n\nobs > SampleRates [i] then 6 r max = Max(r max , r obs ); 7 if r max > 0 then 8 foreach model in Pipeline do 9 k m = r max * model.scalefactor / (model.throughput * model.\u03c1); 10 reps = Ceil(k m ) -model.replicas; 11 if reps > 0 then 12 AddReps(model, reps); 13 LastUpdate = Now();\n\n\n= \u03bb new * model.scalefactor / (model.throughput * \u03c1 min ); 6 extraReps = model.replicas -Ceil(k m ); 7 if extraReps > 0 then 8 RemoveReps(model, extraReps);\n\nFigure 6 :\n6Comparison of InferLine's Planner to coarse-grained baselines (150ms SLO) InferLine outperforms both baselines, consistently providing both the lowest cost configuration and highest SLO attainment (lowest miss rate). CG-Peak was not evaluated on \u03bb > 300 because the configurations exceeded cluster capacity.\n\nFigure 7 :\n7Performance comparison of the high-frequency tuning algorithms on traces derived from real workloads\n\nFigure 8 :\n8Performance comparison of the high-frequency tuning algorithms on synthetic traces with increasing arrival rates. We observe that InferLine outperforms both coarse-grained baselines on cost while maintaining a near-zero SLO miss rate for the entire duration of the trace.\n\nFigure 9 :Figure 10 :\n910Comparison of estimated and measured tail latencies. We compare the latency distributions produced by the Estimator on a workload with \u03bb of 150 qps and CV 2 A of 4, observing that in all cases the estimated and measured latencies are both close to each other and below the latency SLO. Planner sensitivity: Variation in configuration cost across different arrival processes and latency SLOs for the Social Media pipeline. We observe that 1) cost decreases as SLO increases, 2) burstier workloads require higher cost configurations, and 3) cost increases as \u03bb increases.\n\nFigure 11 :\n11Sensitivity to arrival rate changes (Social Media pipeline).\n\nFigure 12 :\n12Sensitivity to arrival burstiness changes (Social Media Pipeline\n\nFigure 13 :\n13Attribution of benefit between the InferLine low-frequency\n\nFigure 14 :\n14Comparison of the InferLine Planner provisioning the TF Cascade pipeline in the Clipper and TensorFlow Serving (TFS) prediction-serving frameworks. The SLO is 0.15 and the CV 2 A is 1.0.\n\n\n). We observe that the network-calculus based detection mechanism of the Tuner detects changes in workload burstiness and takes the appropriate scaling action to maintain a near-zero SLO miss rate.0 \n100 \n200 \n300 \n400 \n\nTime (s) \n\n0 \n\n200 \n\nRequest Rate (qps) \n\n0 \n100 \n200 \n300 \n400 \n500 \n\nTime (s) \n\n0 \n\n2 \n\nCost ($/hr) \n\n0 \n200 \n400 \n\nTime (s) \n\n0.0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1.0 \n\nSLO Miss Rate \n\nBaseline Plan \nInferLine Plan \n\nInferLine Plan + Baseline Tune \nInferLine Plan + InferLine Tune \n\n\nThis work is licensed under a Creative Commons Attribution International 4.0 License.\n\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, arXiv:1511.02799Deep Compositional Question Answering with Neural Module Networks. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. Deep Compositional Question Answering with Neural Mod- ule Networks. CoRR abs/1511.02799 (2015). arXiv:1511.02799 http://arxiv.org/abs/1511.02799\n\nReal-Time Pedestrian Detection with Deep Network Cascades. Anelia Angelova, Alex Krizhevsky, Vincent Vanhoucke, S Abhijit, Dave Ogale, Ferguson, 32.1-32.12BMVC. Anelia Angelova, Alex Krizhevsky, Vincent Vanhoucke, Abhijit S Ogale, and Dave Ferguson. 2015. Real-Time Pedestrian Detection with Deep Network Cascades. BMVC (2015), 32.1-32.12.\n\nThe Nonstochastic Multiarmed Bandit Problem. Peter Auer, Nicol\u00f2 Cesa-Bianchi, Yoav Freund, Robert E Schapire, 10.1137/S0097539701398375SIAM J. Comput. 32Peter Auer, Nicol\u00f2 Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. 2003. The Nonstochastic Multiarmed Bandit Problem. SIAM J. Comput. 32, 1 (Jan. 2003), 48-77. https://doi.org/10.1137/S0097539701398375\n\nTFX: A TensorFlow-Based Production-Scale Machine Learning Platform. Denis Baylor, Eric Breck, Heng-Tze, Noah Cheng, Chuan Fiedel, Zakaria Yu Foo, Salem Haque, Mustafa Haykal, Vihan Ispir, Levent Jain, Koc, Lukasz Chiu Yuen Koo, Clemens Lew, Akshay Naresh Mewald, Neoklis Modi, Sukriti Polyzotis, Sudip Ramesh, Steven Euijong Roy, Martin Whang, Jarek Wicke, Wilkiewicz, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17). the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '17)ACMXin Zhang, and Martin ZinkevichDenis Baylor, Eric Breck, Heng-Tze Cheng, Noah Fiedel, Chuan Yu Foo, Zakaria Haque, Salem Haykal, Mustafa Ispir, Vihan Jain, Levent Koc, Chiu Yuen Koo, Lukasz Lew, Clemens Mewald, Akshay Naresh Modi, Neoklis Polyzotis, Sukriti Ramesh, Sudip Roy, Steven Euijong Whang, Martin Wicke, Jarek Wilkiewicz, Xin Zhang, and Martin Zinke- vich. 2017. TFX: A TensorFlow-Based Production-Scale Machine Learning Platform. In Proceedings of the 23rd ACM SIGKDD Interna- tional Conference on Knowledge Discovery and Data Mining (KDD '17). ACM, 1387-1395.\n\nSimulation: the practice of model development and use. Andrew Beck, 10.1057/palgrave.jos.4250031Journal of Simulation. 2Andrew Beck. 2008. Simulation: the practice of model development and use. Journal of Simulation 2, 1 (01 Mar 2008), 67-67. https: //doi.org/10.1057/palgrave.jos.4250031\n\nTowards Automatic Parameter Tuning of Stream Processing Systems. Muhammad Bilal, Marco Canini, 10.1145/3127479.3127492Proceedings of the 2017 Symposium on Cloud Computing. the 2017 Symposium on Cloud ComputingSanta Clara, California; New York, NY, USAACMSoCC '17)Muhammad Bilal and Marco Canini. 2017. Towards Automatic Pa- rameter Tuning of Stream Processing Systems. In Proceedings of the 2017 Symposium on Cloud Computing (Santa Clara, California) (SoCC '17). ACM, New York, NY, USA, 189-200. https://doi.org/10.1145/ 3127479.3127492\n\nClassification and Regression Trees. J H Leo Breiman, R A Friedman, C J Olshen, Stone, Wadsworth Publishing CompanyBelmont, California, U.S.ALeo Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. 1984. Classification and Regression Trees. Wadsworth Publishing Company, Belmont, California, U.S.A.\n\nTrill: A High-Performance Incremental Query Processor for Diverse Analytics. Badrish Chandramouli, Jonathan Goldstein, Mike Barnett, Robert De-Line, Danyel Fisher, John C Platt, James F Terwilliger, John Wernsing, 10.14778/2735496.2735503Proc. VLDB Endow. VLDB Endow8Badrish Chandramouli, Jonathan Goldstein, Mike Barnett, Robert De- Line, Danyel Fisher, John C. Platt, James F. Terwilliger, and John Wernsing. 2014. Trill: A High-Performance Incremental Query Pro- cessor for Diverse Analytics. Proc. VLDB Endow. 8, 4 (Dec. 2014), 401\u00e2\u0202\u015e412. https://doi.org/10.14778/2735496.2735503\n\nClipper: A Low-Latency Online Prediction Serving System. Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin, Joseph E Gonzalez, Ion Stoica, 10] flink 202014th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). USENIX Association. Boston, MA; Apache FlinkDaniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica. 2017. Clipper: A Low-Latency Online Prediction Serving System. In 14th USENIX Symposium on Net- worked Systems Design and Implementation (NSDI 17). USENIX As- sociation, Boston, MA, 613-627. https://www.usenix.org/conference/ nsdi17/technical-sessions/presentation/crankshaw [10] flink 2020. Apache Flink. https://flink.apache.org.\n\nDhalion: Self-regulating Stream Processing in Heron. Avrilia Floratou, Ashvin Agrawal, Bill Graham, Sriram Rao, Karthik Ramasamy, 10.14778/3137765.3137786Proc. VLDB Endow. VLDB Endow10Avrilia Floratou, Ashvin Agrawal, Bill Graham, Sriram Rao, and Karthik Ramasamy. 2017. Dhalion: Self-regulating Stream Process- ing in Heron. Proc. VLDB Endow. 10, 12 (Aug. 2017), 1825-1836. https://doi.org/10.14778/3137765.3137786\n\nDRS: Auto-Scaling for Real-Time Stream Analytics. Z J Tom, Jianbing Fu, Ding, T B Richard, Marianne Ma, Yin Winslett, Zhenjie Yang, Zhang, 10.1109/TNET.2017.2741969IEEE/ACM Trans. Netw. 25Tom Z. J. Fu, Jianbing Ding, Richard T. B. Ma, Marianne Winslett, Yin Yang, and Zhenjie Zhang. 2017. DRS: Auto-Scaling for Real- Time Stream Analytics. IEEE/ACM Trans. Netw. 25, 6 (Dec. 2017), 3338-3352. https://doi.org/10.1109/TNET.2017.2741969\n\nAutoScale: Dynamic, Robust Capacity Management for Multi-Tier Data Centers. Anshul Gandhi, Mor Harchol-Balter, Ram Raghunathan, Michael A Kozuch, 10.1145/2382553.2382556ACM Trans. Comput. Syst. 3014Anshul Gandhi, Mor Harchol-Balter, Ram Raghunathan, and Michael A. Kozuch. 2012. AutoScale: Dynamic, Robust Capacity Management for Multi-Tier Data Centers. ACM Trans. Comput. Syst. 30, 4, Article 14 (Nov. 2012), 26 pages. https://doi.org/10.1145/2382553.2382556\n\n. Jiaqi Guan, Yang Liu, Qiang Liu, Jian Peng, arXiv:1710.03368v1Energy-efficient Amortized Inference with Cascaded Deep Classifiers. arXiv.orgcs.LGJiaqi Guan, Yang Liu, Qiang Liu, and Jian Peng. 2017. Energy-efficient Amortized Inference with Cascaded Deep Classifiers. arXiv.org (Oct. 2017). arXiv:1710.03368v1 [cs.LG]\n\nAmazone Web Services Inc. 2017. Amazon SageMaker: Developer Guide. Amazone Web Services Inc. 2017. Amazon SageMaker: Developer Guide. http://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker- dg.pdf (2017).\n\nAn Uncertainty-Aware Approach to Optimal Configuration of Stream Processing Systems. Pooyan Jamshidi, Giuliano Casale, MASCOTS. Pooyan Jamshidi and Giuliano Casale. 2016. An Uncertainty-Aware Approach to Optimal Configuration of Stream Processing Systems. MASCOTS (2016), 39-48.\n\nMorpheus: Towards Automated SLOs for Enterprise Clusters. Carlo Sangeetha Abdu Jyothi, Ishai Curino, Menache, Alexey Shravan Matthur Narayanamurthy, Jonathan Tumanov, Ruslan Yaniv, \u00cd\u00f1igo Mavlyutov, Subru Goiri, Janardhan Krishnan, Sriram Kulkarni, Rao, Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation. the 12th USENIX Conference on Operating Systems Design and ImplementationSavannah, GA, USA; Berkeley, CA, USAOSDI'16). USENIX AssociationSangeetha Abdu Jyothi, Carlo Curino, Ishai Menache, Shravan Matthur Narayanamurthy, Alexey Tumanov, Jonathan Yaniv, Ruslan Mavlyutov, \u00cd\u00f1igo Goiri, Subru Krishnan, Janardhan Kulkarni, and Sriram Rao. 2016. Morpheus: Towards Automated SLOs for Enterprise Clusters. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation (Savannah, GA, USA) (OSDI'16). USENIX Association, Berkeley, CA, USA, 117-134. http://dl.acm.org/citation. cfm?id=3026877.3026887\n\nThree Steps is All You Need: Fast, Accurate, Automatic Scaling Decisions for Distributed Streaming Dataflows. Vasiliki Kalavri, John Liagouris, Moritz Hoffmann, Desislava Dimitrova, Matthew Forshaw, Timothy Roscoe, Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation. the 12th USENIX Conference on Operating Systems Design and ImplementationCarlsbad, CA, USA; Berkeley, CA, USAOSDI'18). USENIX AssociationVasiliki Kalavri, John Liagouris, Moritz Hoffmann, Desislava Dim- itrova, Matthew Forshaw, and Timothy Roscoe. 2018. Three Steps is All You Need: Fast, Accurate, Automatic Scaling Decisions for Dis- tributed Streaming Dataflows. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation (Carls- bad, CA, USA) (OSDI'18). USENIX Association, Berkeley, CA, USA, 783-798. http://dl.acm.org/citation.cfm?id=3291168.3291226\n\nNetwork Calculus: A Theory of Deterministic Queuing Systems for the Internet. Jean-Yves Le Boudec, Patrick Thiran, Springer-VerlagBerlin, HeidelbergJean-Yves Le Boudec and Patrick Thiran. 2001. Network Calculus: A Theory of Deterministic Queuing Systems for the Internet. Springer- Verlag, Berlin, Heidelberg.\n\nA Contextual-bandit Approach to Personalized News Article Recommendation. Lihong Li, Wei Chu, John Langford, Robert E Schapire, WWW. Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. 2010. A Contextual-bandit Approach to Personalized News Article Recommen- dation. In WWW.\n\nElastic Stream Processing with Latency Guarantees. B Lohrmann, P Janacik, O Kao, 10.1109/ICDCS.2015.48IEEE 35th International Conference on Distributed Computing Systems. 399-410. B. Lohrmann, P. Janacik, and O. Kao. 2015. Elastic Stream Processing with Latency Guarantees. In 2015 IEEE 35th International Conference on Distributed Computing Systems. 399-410. https://doi.org/10.1109/ ICDCS.2015.48\n\nNephele Streaming: Stream Processing Under QoS Constraints At Scale. Bj\u00e3\u0171rn Lohrmann, Daniel Warneke, Odej Kao, 10.1007/s10586-013-0281-8Cluster Computing. 17Bj\u00c3\u0171rn Lohrmann, Daniel Warneke, and Odej Kao. 2013. Nephele Streaming: Stream Processing Under QoS Constraints At Scale. Cluster Computing 17 (08 2013). https://doi.org/10.1007/s10586-013-0281-8\n\nAsk Your Neurons: A Neural-Based Approach to Answering Questions about Images. Mateusz Malinowski, Marcus Rohrbach, Mario Fritz, IEEE International Conference on Computer Vision (ICCV. Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2015. Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images. 2015 IEEE International Conference on Computer Vision (ICCV) (2015), 1-9.\n\nDeciding How to Decide: Dynamic Routing in Artificial Neural Networks. Mason Mcgill, Pietro Perona, arXiv:1703.06217v2[stat.ML][25]openalpr2020Mason McGill and Pietro Perona. 2017. Deciding How to Decide: Dynamic Routing in Artificial Neural Networks. arXiv.org (March 2017). arXiv:1703.06217v2 [stat.ML] [25] openalpr 2020. Open ALPR. https://www.openalpr.com/.\n\nAutomatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. (2017).\n\nYou only look once: Unified, real-time object detection. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition. 779-788.\n\nFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Kaiming Shaoqing Ren, Ross B He, Jian Girshick, Sun, arXiv:1506.01497Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Pro- posal Networks. CoRR abs/1506.01497 (2015). arXiv:1506.01497 http://arxiv.org/abs/1506.01497\n\nEnabling Workflow-Aware Scheduling on HPC Systems. Gonzalo P Rodrigo, Erik Elmroth, Per-Olov, Lavanya \u00d6stberg, Ramakrishnan, 10.1145/3078597.3078604Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing. the 26th International Symposium on High-Performance Parallel and Distributed ComputingWashington, DC, USA; New York, NY, USAACMGonzalo P. Rodrigo, Erik Elmroth, Per-Olov \u00d6stberg, and Lavanya Ramakrishnan. 2017. Enabling Workflow-Aware Scheduling on HPC Systems. In Proceedings of the 26th International Symposium on High- Performance Parallel and Distributed Computing (Washington, DC, USA) (HPDC '17). ACM, New York, NY, USA, 3-14. https://doi.org/ 10.1145/3078597.3078604\n\nsamza 2020. Apache Samza. samza 2020. Apache Samza. http://samza.apache.org.\n\nNexus: A GPU Cluster Engine for Accelerating DNN-based Video Analysis. Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, Ravi Sundaram, Proceedings of the 27th ACM Symposium on Operating Systems Principles. the 27th ACM Symposium on Operating Systems PrinciplesHuntsville, Ontario, Canada; NewACMSOSP '19Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong, Matthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. 2019. Nexus: A GPU Cluster Engine for Accelerating DNN-based Video Analysis. In Proceedings of the 27th ACM Symposium on Operating Sys- tems Principles (Huntsville, Ontario, Canada) (SOSP '19). ACM, New\n\n. N Y York, Usa, 10.1145/3341301.3359658York, NY, USA, 322-337. https://doi.org/10.1145/3341301.3359658\n\nDeep Convolutional Network Cascade for Facial Point Detection. Yi Sun, Xiaogang Wang, Xiaoou Tang, CVPR. Yi Sun, Xiaogang Wang, and Xiaoou Tang. 2013. Deep Convolutional Network Cascade for Facial Point Detection. CVPR (2013), 3476- 3483.\n\n. TensorRT Inference Server. 2020tensorrtserver 2020. TensorRT Inference Server. https://github.com/ NVIDIA/tensorrt-inference-server.\n\n. 37] tfserving 2020TensorFlow Serving. [37] tfserving 2020. TensorFlow Serving. https://tensorflow.github.io/ serving.\n\nSEDA: An Architecture for Well-Conditioned. Matt Welsh, David E Culler, Eric A Brewer, Scalable Internet Services. SOSP. Matt Welsh, David E Culler, and Eric A Brewer. 2001. SEDA: An Architecture for Well-Conditioned, Scalable Internet Services. SOSP (2001), 230-243.\n\nLive Video Analytics at Scale with Approximation and Delay-Tolerance. Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Philipose, Paramvir Bahl, Michael J Freedman, 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). USENIX Association. Boston, MAHaoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Phili- pose, Paramvir Bahl, and Michael J. Freedman. 2017. Live Video Analytics at Scale with Approximation and Delay-Tolerance. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). USENIX Association, Boston, MA, 377-392. https://www. usenix.org/conference/nsdi17/technical-sessions/presentation/zhang\n", "annotations": {"author": "[{\"end\":190,\"start\":114},{\"end\":264,\"start\":191},{\"end\":335,\"start\":265},{\"end\":418,\"start\":336},{\"end\":491,\"start\":419},{\"end\":560,\"start\":492},{\"end\":658,\"start\":561},{\"end\":731,\"start\":659},{\"end\":808,\"start\":732},{\"end\":882,\"start\":809},{\"end\":953,\"start\":883},{\"end\":1045,\"start\":954},{\"end\":1137,\"start\":1046},{\"end\":1213,\"start\":1138},{\"end\":1308,\"start\":1214}]", "publisher": null, "author_last_name": "[{\"end\":130,\"start\":121},{\"end\":204,\"start\":200},{\"end\":275,\"start\":273},{\"end\":358,\"start\":348},{\"end\":431,\"start\":423},{\"end\":576,\"start\":568},{\"end\":671,\"start\":667},{\"end\":748,\"start\":739},{\"end\":822,\"start\":818},{\"end\":893,\"start\":891},{\"end\":965,\"start\":960},{\"end\":1056,\"start\":1050},{\"end\":1153,\"start\":1145},{\"end\":1228,\"start\":1221}]", "author_first_name": "[{\"end\":120,\"start\":114},{\"end\":199,\"start\":191},{\"end\":272,\"start\":265},{\"end\":341,\"start\":336},{\"end\":347,\"start\":342},{\"end\":420,\"start\":419},{\"end\":422,\"start\":421},{\"end\":500,\"start\":492},{\"end\":567,\"start\":561},{\"end\":666,\"start\":659},{\"end\":738,\"start\":732},{\"end\":817,\"start\":809},{\"end\":890,\"start\":883},{\"end\":959,\"start\":954},{\"end\":1049,\"start\":1046},{\"end\":1144,\"start\":1138},{\"end\":1220,\"start\":1214}]", "author_affiliation": "[{\"end\":189,\"start\":132},{\"end\":263,\"start\":206},{\"end\":334,\"start\":277},{\"end\":417,\"start\":360},{\"end\":490,\"start\":433},{\"end\":559,\"start\":502},{\"end\":657,\"start\":600},{\"end\":730,\"start\":673},{\"end\":807,\"start\":750},{\"end\":881,\"start\":824},{\"end\":952,\"start\":895},{\"end\":1044,\"start\":987},{\"end\":1136,\"start\":1079},{\"end\":1212,\"start\":1155},{\"end\":1307,\"start\":1250}]", "title": "[{\"end\":83,\"start\":1},{\"end\":1391,\"start\":1309}]", "venue": "[{\"end\":1436,\"start\":1393}]", "abstract": "[{\"end\":3050,\"start\":1800}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4703,\"start\":4700},{\"end\":4728,\"start\":4724},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4771,\"start\":4767},{\"end\":8434,\"start\":8430},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8505,\"start\":8502},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8508,\"start\":8505},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9154,\"start\":9150},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9942,\"start\":9939},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9945,\"start\":9942},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10034,\"start\":10031},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10037,\"start\":10034},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10040,\"start\":10037},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10043,\"start\":10040},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11548,\"start\":11545},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15051,\"start\":15048},{\"end\":15054,\"start\":15051},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15057,\"start\":15054},{\"end\":15060,\"start\":15057},{\"end\":15063,\"start\":15060},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15451,\"start\":15448},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16575,\"start\":16572},{\"end\":16603,\"start\":16599},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19010,\"start\":19006},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21332,\"start\":21329},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27132,\"start\":27128},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30796,\"start\":30792},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30800,\"start\":30796},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31003,\"start\":30999},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33057,\"start\":33054},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33060,\"start\":33057},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33063,\"start\":33060},{\"end\":33065,\"start\":33063},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33603,\"start\":33600},{\"end\":33738,\"start\":33734},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33874,\"start\":33870},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34705,\"start\":34701},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36678,\"start\":36674},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36901,\"start\":36897},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":39771,\"start\":39767},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":39944,\"start\":39940},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":41936,\"start\":41932},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":48717,\"start\":48714},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":48719,\"start\":48717},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":48722,\"start\":48719},{\"end\":48725,\"start\":48722},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49277,\"start\":49274},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":49280,\"start\":49277},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49806,\"start\":49802},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":49809,\"start\":49806},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":49950,\"start\":49946},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":50241,\"start\":50237},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":50794,\"start\":50790},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":50797,\"start\":50794},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":50800,\"start\":50797},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":50837,\"start\":50833},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":51342,\"start\":51338},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":51757,\"start\":51753},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":52083,\"start\":52079},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":52665,\"start\":52661},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":52668,\"start\":52665},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":52859,\"start\":52855},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":53863,\"start\":53859},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":53866,\"start\":53863}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":55946,\"start\":55908},{\"attributes\":{\"id\":\"fig_1\"},\"end\":56099,\"start\":55947},{\"attributes\":{\"id\":\"fig_3\"},\"end\":56344,\"start\":56100},{\"attributes\":{\"id\":\"fig_4\"},\"end\":56601,\"start\":56345},{\"attributes\":{\"id\":\"fig_5\"},\"end\":56819,\"start\":56602},{\"attributes\":{\"id\":\"fig_6\"},\"end\":57102,\"start\":56820},{\"attributes\":{\"id\":\"fig_7\"},\"end\":57261,\"start\":57103},{\"attributes\":{\"id\":\"fig_8\"},\"end\":57582,\"start\":57262},{\"attributes\":{\"id\":\"fig_9\"},\"end\":57696,\"start\":57583},{\"attributes\":{\"id\":\"fig_11\"},\"end\":57981,\"start\":57697},{\"attributes\":{\"id\":\"fig_12\"},\"end\":58577,\"start\":57982},{\"attributes\":{\"id\":\"fig_13\"},\"end\":58653,\"start\":58578},{\"attributes\":{\"id\":\"fig_14\"},\"end\":58733,\"start\":58654},{\"attributes\":{\"id\":\"fig_15\"},\"end\":58807,\"start\":58734},{\"attributes\":{\"id\":\"fig_16\"},\"end\":59009,\"start\":58808},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":59512,\"start\":59010}]", "paragraph": "[{\"end\":3868,\"start\":3066},{\"end\":5373,\"start\":3870},{\"end\":6415,\"start\":5375},{\"end\":6958,\"start\":6417},{\"end\":7350,\"start\":6960},{\"end\":7909,\"start\":7352},{\"end\":8277,\"start\":7911},{\"end\":8575,\"start\":8307},{\"end\":8855,\"start\":8577},{\"end\":9721,\"start\":8857},{\"end\":10044,\"start\":9723},{\"end\":10275,\"start\":10046},{\"end\":12343,\"start\":10290},{\"end\":12813,\"start\":12345},{\"end\":15479,\"start\":12815},{\"end\":15856,\"start\":15497},{\"end\":17023,\"start\":15858},{\"end\":17817,\"start\":17025},{\"end\":19436,\"start\":17819},{\"end\":19726,\"start\":19463},{\"end\":20434,\"start\":19739},{\"end\":20911,\"start\":20436},{\"end\":22081,\"start\":20925},{\"end\":23409,\"start\":22104},{\"end\":23917,\"start\":23411},{\"end\":24516,\"start\":23919},{\"end\":25677,\"start\":24518},{\"end\":25969,\"start\":25679},{\"end\":26703,\"start\":25995},{\"end\":27163,\"start\":26705},{\"end\":27359,\"start\":27165},{\"end\":27672,\"start\":27370},{\"end\":28901,\"start\":27674},{\"end\":30270,\"start\":28903},{\"end\":32314,\"start\":30272},{\"end\":33142,\"start\":32337},{\"end\":33834,\"start\":33144},{\"end\":36222,\"start\":33836},{\"end\":36570,\"start\":36224},{\"end\":37320,\"start\":36572},{\"end\":38274,\"start\":37348},{\"end\":39139,\"start\":38300},{\"end\":40533,\"start\":39141},{\"end\":41021,\"start\":40535},{\"end\":42372,\"start\":41023},{\"end\":43400,\"start\":42397},{\"end\":44909,\"start\":43402},{\"end\":46451,\"start\":44911},{\"end\":47053,\"start\":46478},{\"end\":47910,\"start\":47055},{\"end\":48615,\"start\":47953},{\"end\":49666,\"start\":48632},{\"end\":50709,\"start\":49668},{\"end\":51331,\"start\":50711},{\"end\":51740,\"start\":51333},{\"end\":52071,\"start\":51742},{\"end\":52545,\"start\":52073},{\"end\":52843,\"start\":52547},{\"end\":53287,\"start\":52845},{\"end\":53724,\"start\":53303},{\"end\":54221,\"start\":53726},{\"end\":55621,\"start\":54236},{\"end\":55907,\"start\":55642}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":27369,\"start\":27360}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3064,\"start\":3052},{\"attributes\":{\"n\":\"2\"},\"end\":8305,\"start\":8280},{\"attributes\":{\"n\":\"2.1\"},\"end\":10288,\"start\":10278},{\"attributes\":{\"n\":\"3\"},\"end\":15495,\"start\":15482},{\"attributes\":{\"n\":\"4\"},\"end\":19461,\"start\":19439},{\"attributes\":{\"n\":\"4.1\"},\"end\":19737,\"start\":19729},{\"attributes\":{\"n\":\"4.2\"},\"end\":20923,\"start\":20914},{\"attributes\":{\"n\":\"4.3\"},\"end\":22102,\"start\":22084},{\"attributes\":{\"n\":\"5\"},\"end\":25993,\"start\":25972},{\"attributes\":{\"n\":\"6\"},\"end\":32335,\"start\":32317},{\"attributes\":{\"n\":\"7\"},\"end\":37346,\"start\":37323},{\"attributes\":{\"n\":\"7.1\"},\"end\":38298,\"start\":38277},{\"attributes\":{\"n\":\"7.2\"},\"end\":42395,\"start\":42375},{\"attributes\":{\"n\":\"7.3\"},\"end\":46476,\"start\":46454},{\"attributes\":{\"n\":\"7.4\"},\"end\":47951,\"start\":47913},{\"attributes\":{\"n\":\"8\"},\"end\":48630,\"start\":48618},{\"attributes\":{\"n\":\"9\"},\"end\":53301,\"start\":53290},{\"attributes\":{\"n\":\"10\"},\"end\":54234,\"start\":54224},{\"attributes\":{\"n\":\"11\"},\"end\":55640,\"start\":55624},{\"end\":55919,\"start\":55909},{\"end\":55958,\"start\":55948},{\"end\":56356,\"start\":56346},{\"end\":56613,\"start\":56603},{\"end\":57273,\"start\":57263},{\"end\":57594,\"start\":57584},{\"end\":57708,\"start\":57698},{\"end\":58004,\"start\":57983},{\"end\":58590,\"start\":58579},{\"end\":58666,\"start\":58655},{\"end\":58746,\"start\":58735},{\"end\":58820,\"start\":58809}]", "table": "[{\"end\":59512,\"start\":59209}]", "figure_caption": "[{\"end\":55946,\"start\":55921},{\"end\":56099,\"start\":55960},{\"end\":56344,\"start\":56102},{\"end\":56601,\"start\":56358},{\"end\":56819,\"start\":56615},{\"end\":57102,\"start\":56822},{\"end\":57261,\"start\":57105},{\"end\":57582,\"start\":57275},{\"end\":57696,\"start\":57596},{\"end\":57981,\"start\":57710},{\"end\":58577,\"start\":58008},{\"end\":58653,\"start\":58593},{\"end\":58733,\"start\":58669},{\"end\":58807,\"start\":58749},{\"end\":59009,\"start\":58823},{\"end\":59209,\"start\":59012}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8898,\"start\":8890},{\"end\":11403,\"start\":11395},{\"end\":11968,\"start\":11962},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15598,\"start\":15590},{\"end\":24290,\"start\":24284},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24974,\"start\":24967},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27466,\"start\":27460},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29512,\"start\":29506},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32408,\"start\":32400},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33667,\"start\":33660},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36581,\"start\":36575},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":37318,\"start\":37312},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":38921,\"start\":38913},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":39850,\"start\":39844},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":40041,\"start\":40031},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":40547,\"start\":40538},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":41189,\"start\":41183},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":41872,\"start\":41866},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":42027,\"start\":42018},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":42200,\"start\":42191},{\"end\":43019,\"start\":43013},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":43308,\"start\":43302},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43551,\"start\":43544},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45356,\"start\":45349},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46265,\"start\":46258},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47321,\"start\":47313},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":48363,\"start\":48356}]", "bib_author_first_name": "[{\"end\":59605,\"start\":59600},{\"end\":59621,\"start\":59615},{\"end\":59638,\"start\":59632},{\"end\":59651,\"start\":59648},{\"end\":60023,\"start\":60017},{\"end\":60038,\"start\":60034},{\"end\":60058,\"start\":60051},{\"end\":60071,\"start\":60070},{\"end\":60085,\"start\":60081},{\"end\":60349,\"start\":60344},{\"end\":60362,\"start\":60356},{\"end\":60381,\"start\":60377},{\"end\":60396,\"start\":60390},{\"end\":60398,\"start\":60397},{\"end\":60731,\"start\":60726},{\"end\":60744,\"start\":60740},{\"end\":60766,\"start\":60762},{\"end\":60779,\"start\":60774},{\"end\":60795,\"start\":60788},{\"end\":60809,\"start\":60804},{\"end\":60824,\"start\":60817},{\"end\":60838,\"start\":60833},{\"end\":60852,\"start\":60846},{\"end\":60870,\"start\":60864},{\"end\":60893,\"start\":60886},{\"end\":60912,\"start\":60899},{\"end\":60928,\"start\":60921},{\"end\":60942,\"start\":60935},{\"end\":60959,\"start\":60954},{\"end\":60974,\"start\":60968},{\"end\":60982,\"start\":60975},{\"end\":60994,\"start\":60988},{\"end\":61007,\"start\":61002},{\"end\":61866,\"start\":61860},{\"end\":62168,\"start\":62160},{\"end\":62181,\"start\":62176},{\"end\":62671,\"start\":62670},{\"end\":62673,\"start\":62672},{\"end\":62688,\"start\":62687},{\"end\":62690,\"start\":62689},{\"end\":62702,\"start\":62701},{\"end\":62704,\"start\":62703},{\"end\":63020,\"start\":63013},{\"end\":63043,\"start\":63035},{\"end\":63059,\"start\":63055},{\"end\":63075,\"start\":63069},{\"end\":63091,\"start\":63085},{\"end\":63104,\"start\":63100},{\"end\":63106,\"start\":63105},{\"end\":63119,\"start\":63114},{\"end\":63121,\"start\":63120},{\"end\":63139,\"start\":63135},{\"end\":63584,\"start\":63578},{\"end\":63599,\"start\":63596},{\"end\":63612,\"start\":63606},{\"end\":63626,\"start\":63619},{\"end\":63628,\"start\":63627},{\"end\":63645,\"start\":63639},{\"end\":63647,\"start\":63646},{\"end\":63661,\"start\":63658},{\"end\":64299,\"start\":64292},{\"end\":64316,\"start\":64310},{\"end\":64330,\"start\":64326},{\"end\":64345,\"start\":64339},{\"end\":64358,\"start\":64351},{\"end\":64707,\"start\":64706},{\"end\":64709,\"start\":64708},{\"end\":64723,\"start\":64715},{\"end\":64735,\"start\":64734},{\"end\":64737,\"start\":64736},{\"end\":64755,\"start\":64747},{\"end\":64763,\"start\":64760},{\"end\":64781,\"start\":64774},{\"end\":65173,\"start\":65167},{\"end\":65185,\"start\":65182},{\"end\":65205,\"start\":65202},{\"end\":65226,\"start\":65219},{\"end\":65228,\"start\":65227},{\"end\":65560,\"start\":65555},{\"end\":65571,\"start\":65567},{\"end\":65582,\"start\":65577},{\"end\":65592,\"start\":65588},{\"end\":66173,\"start\":66167},{\"end\":66192,\"start\":66184},{\"end\":66425,\"start\":66420},{\"end\":66454,\"start\":66449},{\"end\":66478,\"start\":66472},{\"end\":66519,\"start\":66511},{\"end\":66535,\"start\":66529},{\"end\":66548,\"start\":66543},{\"end\":66565,\"start\":66560},{\"end\":66582,\"start\":66573},{\"end\":66599,\"start\":66593},{\"end\":67446,\"start\":67438},{\"end\":67460,\"start\":67456},{\"end\":67478,\"start\":67472},{\"end\":67498,\"start\":67489},{\"end\":67517,\"start\":67510},{\"end\":67534,\"start\":67527},{\"end\":68313,\"start\":68301},{\"end\":68329,\"start\":68322},{\"end\":68614,\"start\":68608},{\"end\":68622,\"start\":68619},{\"end\":68632,\"start\":68628},{\"end\":68649,\"start\":68643},{\"end\":68651,\"start\":68650},{\"end\":68869,\"start\":68868},{\"end\":68881,\"start\":68880},{\"end\":68892,\"start\":68891},{\"end\":69292,\"start\":69286},{\"end\":69309,\"start\":69303},{\"end\":69323,\"start\":69319},{\"end\":69658,\"start\":69651},{\"end\":69677,\"start\":69671},{\"end\":69693,\"start\":69688},{\"end\":70047,\"start\":70042},{\"end\":70062,\"start\":70056},{\"end\":70377,\"start\":70373},{\"end\":70389,\"start\":70386},{\"end\":70404,\"start\":70397},{\"end\":70422,\"start\":70415},{\"end\":70437,\"start\":70431},{\"end\":70451,\"start\":70444},{\"end\":70466,\"start\":70460},{\"end\":70477,\"start\":70472},{\"end\":70493,\"start\":70489},{\"end\":70506,\"start\":70502},{\"end\":70775,\"start\":70769},{\"end\":70791,\"start\":70784},{\"end\":70805,\"start\":70801},{\"end\":70819,\"start\":70816},{\"end\":71276,\"start\":71269},{\"end\":71295,\"start\":71291},{\"end\":71297,\"start\":71296},{\"end\":71306,\"start\":71302},{\"end\":71620,\"start\":71613},{\"end\":71622,\"start\":71621},{\"end\":71636,\"start\":71632},{\"end\":71663,\"start\":71656},{\"end\":72446,\"start\":72439},{\"end\":72458,\"start\":72453},{\"end\":72471,\"start\":72465},{\"end\":72484,\"start\":72477},{\"end\":72497,\"start\":72491},{\"end\":72511,\"start\":72504},{\"end\":72529,\"start\":72523},{\"end\":72549,\"start\":72545},{\"end\":73060,\"start\":73059},{\"end\":73062,\"start\":73061},{\"end\":73227,\"start\":73225},{\"end\":73241,\"start\":73233},{\"end\":73254,\"start\":73248},{\"end\":73707,\"start\":73703},{\"end\":73720,\"start\":73715},{\"end\":73722,\"start\":73721},{\"end\":73735,\"start\":73731},{\"end\":73737,\"start\":73736},{\"end\":74003,\"start\":73998},{\"end\":74017,\"start\":74011},{\"end\":74041,\"start\":74036},{\"end\":74056,\"start\":74049},{\"end\":74076,\"start\":74068},{\"end\":74090,\"start\":74083},{\"end\":74092,\"start\":74091}]", "bib_author_last_name": "[{\"end\":59613,\"start\":59606},{\"end\":59630,\"start\":59622},{\"end\":59646,\"start\":59639},{\"end\":59657,\"start\":59652},{\"end\":60032,\"start\":60024},{\"end\":60049,\"start\":60039},{\"end\":60068,\"start\":60059},{\"end\":60079,\"start\":60072},{\"end\":60091,\"start\":60086},{\"end\":60101,\"start\":60093},{\"end\":60354,\"start\":60350},{\"end\":60375,\"start\":60363},{\"end\":60388,\"start\":60382},{\"end\":60407,\"start\":60399},{\"end\":60738,\"start\":60732},{\"end\":60750,\"start\":60745},{\"end\":60760,\"start\":60752},{\"end\":60772,\"start\":60767},{\"end\":60786,\"start\":60780},{\"end\":60802,\"start\":60796},{\"end\":60815,\"start\":60810},{\"end\":60831,\"start\":60825},{\"end\":60844,\"start\":60839},{\"end\":60857,\"start\":60853},{\"end\":60862,\"start\":60859},{\"end\":60884,\"start\":60871},{\"end\":60897,\"start\":60894},{\"end\":60919,\"start\":60913},{\"end\":60933,\"start\":60929},{\"end\":60952,\"start\":60943},{\"end\":60966,\"start\":60960},{\"end\":60986,\"start\":60983},{\"end\":61000,\"start\":60995},{\"end\":61013,\"start\":61008},{\"end\":61025,\"start\":61015},{\"end\":61871,\"start\":61867},{\"end\":62174,\"start\":62169},{\"end\":62188,\"start\":62182},{\"end\":62685,\"start\":62674},{\"end\":62699,\"start\":62691},{\"end\":62711,\"start\":62705},{\"end\":62718,\"start\":62713},{\"end\":63033,\"start\":63021},{\"end\":63053,\"start\":63044},{\"end\":63067,\"start\":63060},{\"end\":63083,\"start\":63076},{\"end\":63098,\"start\":63092},{\"end\":63112,\"start\":63107},{\"end\":63133,\"start\":63122},{\"end\":63148,\"start\":63140},{\"end\":63594,\"start\":63585},{\"end\":63604,\"start\":63600},{\"end\":63617,\"start\":63613},{\"end\":63637,\"start\":63629},{\"end\":63656,\"start\":63648},{\"end\":63668,\"start\":63662},{\"end\":64308,\"start\":64300},{\"end\":64324,\"start\":64317},{\"end\":64337,\"start\":64331},{\"end\":64349,\"start\":64346},{\"end\":64367,\"start\":64359},{\"end\":64713,\"start\":64710},{\"end\":64726,\"start\":64724},{\"end\":64732,\"start\":64728},{\"end\":64745,\"start\":64738},{\"end\":64758,\"start\":64756},{\"end\":64772,\"start\":64764},{\"end\":64786,\"start\":64782},{\"end\":64793,\"start\":64788},{\"end\":65180,\"start\":65174},{\"end\":65200,\"start\":65186},{\"end\":65217,\"start\":65206},{\"end\":65235,\"start\":65229},{\"end\":65565,\"start\":65561},{\"end\":65575,\"start\":65572},{\"end\":65586,\"start\":65583},{\"end\":65597,\"start\":65593},{\"end\":66182,\"start\":66174},{\"end\":66199,\"start\":66193},{\"end\":66447,\"start\":66426},{\"end\":66461,\"start\":66455},{\"end\":66470,\"start\":66463},{\"end\":66509,\"start\":66479},{\"end\":66527,\"start\":66520},{\"end\":66541,\"start\":66536},{\"end\":66558,\"start\":66549},{\"end\":66571,\"start\":66566},{\"end\":66591,\"start\":66583},{\"end\":66608,\"start\":66600},{\"end\":66613,\"start\":66610},{\"end\":67454,\"start\":67447},{\"end\":67470,\"start\":67461},{\"end\":67487,\"start\":67479},{\"end\":67508,\"start\":67499},{\"end\":67525,\"start\":67518},{\"end\":67541,\"start\":67535},{\"end\":68320,\"start\":68314},{\"end\":68336,\"start\":68330},{\"end\":68617,\"start\":68615},{\"end\":68626,\"start\":68623},{\"end\":68641,\"start\":68633},{\"end\":68660,\"start\":68652},{\"end\":68878,\"start\":68870},{\"end\":68889,\"start\":68882},{\"end\":68896,\"start\":68893},{\"end\":69301,\"start\":69293},{\"end\":69317,\"start\":69310},{\"end\":69327,\"start\":69324},{\"end\":69669,\"start\":69659},{\"end\":69686,\"start\":69678},{\"end\":69699,\"start\":69694},{\"end\":70054,\"start\":70048},{\"end\":70069,\"start\":70063},{\"end\":70384,\"start\":70378},{\"end\":70395,\"start\":70390},{\"end\":70413,\"start\":70405},{\"end\":70429,\"start\":70423},{\"end\":70442,\"start\":70438},{\"end\":70458,\"start\":70452},{\"end\":70470,\"start\":70467},{\"end\":70487,\"start\":70478},{\"end\":70500,\"start\":70494},{\"end\":70512,\"start\":70507},{\"end\":70782,\"start\":70776},{\"end\":70799,\"start\":70792},{\"end\":70814,\"start\":70806},{\"end\":70827,\"start\":70820},{\"end\":71289,\"start\":71277},{\"end\":71300,\"start\":71298},{\"end\":71315,\"start\":71307},{\"end\":71320,\"start\":71317},{\"end\":71630,\"start\":71623},{\"end\":71644,\"start\":71637},{\"end\":71654,\"start\":71646},{\"end\":71671,\"start\":71664},{\"end\":71685,\"start\":71673},{\"end\":72451,\"start\":72447},{\"end\":72463,\"start\":72459},{\"end\":72475,\"start\":72472},{\"end\":72489,\"start\":72485},{\"end\":72502,\"start\":72498},{\"end\":72521,\"start\":72512},{\"end\":72543,\"start\":72530},{\"end\":72558,\"start\":72550},{\"end\":73067,\"start\":73063},{\"end\":73072,\"start\":73069},{\"end\":73231,\"start\":73228},{\"end\":73246,\"start\":73242},{\"end\":73259,\"start\":73255},{\"end\":73713,\"start\":73708},{\"end\":73729,\"start\":73723},{\"end\":73744,\"start\":73738},{\"end\":74009,\"start\":74004},{\"end\":74034,\"start\":74018},{\"end\":74047,\"start\":74042},{\"end\":74066,\"start\":74057},{\"end\":74081,\"start\":74077},{\"end\":74101,\"start\":74093}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1511.02799\",\"id\":\"b0\"},\"end\":59956,\"start\":59600},{\"attributes\":{\"doi\":\"32.1-32.12\",\"id\":\"b1\",\"matched_paper_id\":15230091},\"end\":60297,\"start\":59958},{\"attributes\":{\"doi\":\"10.1137/S0097539701398375\",\"id\":\"b2\",\"matched_paper_id\":13209702},\"end\":60656,\"start\":60299},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":21930731},\"end\":61803,\"start\":60658},{\"attributes\":{\"doi\":\"10.1057/palgrave.jos.4250031\",\"id\":\"b4\",\"matched_paper_id\":60755082},\"end\":62093,\"start\":61805},{\"attributes\":{\"doi\":\"10.1145/3127479.3127492\",\"id\":\"b5\",\"matched_paper_id\":36004644},\"end\":62631,\"start\":62095},{\"attributes\":{\"id\":\"b6\"},\"end\":62934,\"start\":62633},{\"attributes\":{\"doi\":\"10.14778/2735496.2735503\",\"id\":\"b7\",\"matched_paper_id\":1978477},\"end\":63519,\"start\":62936},{\"attributes\":{\"doi\":\"10] flink 2020\",\"id\":\"b8\",\"matched_paper_id\":1701442},\"end\":64237,\"start\":63521},{\"attributes\":{\"doi\":\"10.14778/3137765.3137786\",\"id\":\"b9\",\"matched_paper_id\":28537568},\"end\":64654,\"start\":64239},{\"attributes\":{\"doi\":\"10.1109/TNET.2017.2741969\",\"id\":\"b10\",\"matched_paper_id\":4907947},\"end\":65089,\"start\":64656},{\"attributes\":{\"doi\":\"10.1145/2382553.2382556\",\"id\":\"b11\",\"matched_paper_id\":3036526},\"end\":65551,\"start\":65091},{\"attributes\":{\"doi\":\"arXiv:1710.03368v1\",\"id\":\"b12\"},\"end\":65872,\"start\":65553},{\"attributes\":{\"id\":\"b13\"},\"end\":66080,\"start\":65874},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":16059634},\"end\":66360,\"start\":66082},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9610842},\"end\":67326,\"start\":66362},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52988395},\"end\":68221,\"start\":67328},{\"attributes\":{\"id\":\"b17\"},\"end\":68532,\"start\":68223},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":207178795},\"end\":68815,\"start\":68534},{\"attributes\":{\"doi\":\"10.1109/ICDCS.2015.48\",\"id\":\"b19\",\"matched_paper_id\":14317726},\"end\":69215,\"start\":68817},{\"attributes\":{\"doi\":\"10.1007/s10586-013-0281-8\",\"id\":\"b20\",\"matched_paper_id\":8278873},\"end\":69570,\"start\":69217},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":738850},\"end\":69969,\"start\":69572},{\"attributes\":{\"doi\":\"arXiv:1703.06217v2[stat.ML][25]openalpr2020\",\"id\":\"b22\"},\"end\":70333,\"start\":69971},{\"attributes\":{\"id\":\"b23\"},\"end\":70710,\"start\":70335},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206594738},\"end\":71187,\"start\":70712},{\"attributes\":{\"doi\":\"arXiv:1506.01497\",\"id\":\"b25\"},\"end\":71560,\"start\":71189},{\"attributes\":{\"doi\":\"10.1145/3078597.3078604\",\"id\":\"b26\",\"matched_paper_id\":20507385},\"end\":72288,\"start\":71562},{\"attributes\":{\"id\":\"b27\"},\"end\":72366,\"start\":72290},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":204812163},\"end\":73055,\"start\":72368},{\"attributes\":{\"doi\":\"10.1145/3341301.3359658\",\"id\":\"b29\"},\"end\":73160,\"start\":73057},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6064403},\"end\":73400,\"start\":73162},{\"attributes\":{\"id\":\"b31\"},\"end\":73536,\"start\":73402},{\"attributes\":{\"doi\":\"37] tfserving 2020\",\"id\":\"b32\"},\"end\":73657,\"start\":73538},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":221316672},\"end\":73926,\"start\":73659},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":11894138},\"end\":74604,\"start\":73928}]", "bib_title": "[{\"end\":60015,\"start\":59958},{\"end\":60342,\"start\":60299},{\"end\":60724,\"start\":60658},{\"end\":61858,\"start\":61805},{\"end\":62158,\"start\":62095},{\"end\":63011,\"start\":62936},{\"end\":63576,\"start\":63521},{\"end\":64290,\"start\":64239},{\"end\":64704,\"start\":64656},{\"end\":65165,\"start\":65091},{\"end\":66165,\"start\":66082},{\"end\":66418,\"start\":66362},{\"end\":67436,\"start\":67328},{\"end\":68606,\"start\":68534},{\"end\":68866,\"start\":68817},{\"end\":69284,\"start\":69217},{\"end\":69649,\"start\":69572},{\"end\":70767,\"start\":70712},{\"end\":71611,\"start\":71562},{\"end\":72437,\"start\":72368},{\"end\":73223,\"start\":73162},{\"end\":73701,\"start\":73659},{\"end\":73996,\"start\":73928}]", "bib_author": "[{\"end\":59615,\"start\":59600},{\"end\":59632,\"start\":59615},{\"end\":59648,\"start\":59632},{\"end\":59659,\"start\":59648},{\"end\":60034,\"start\":60017},{\"end\":60051,\"start\":60034},{\"end\":60070,\"start\":60051},{\"end\":60081,\"start\":60070},{\"end\":60093,\"start\":60081},{\"end\":60103,\"start\":60093},{\"end\":60356,\"start\":60344},{\"end\":60377,\"start\":60356},{\"end\":60390,\"start\":60377},{\"end\":60409,\"start\":60390},{\"end\":60740,\"start\":60726},{\"end\":60752,\"start\":60740},{\"end\":60762,\"start\":60752},{\"end\":60774,\"start\":60762},{\"end\":60788,\"start\":60774},{\"end\":60804,\"start\":60788},{\"end\":60817,\"start\":60804},{\"end\":60833,\"start\":60817},{\"end\":60846,\"start\":60833},{\"end\":60859,\"start\":60846},{\"end\":60864,\"start\":60859},{\"end\":60886,\"start\":60864},{\"end\":60899,\"start\":60886},{\"end\":60921,\"start\":60899},{\"end\":60935,\"start\":60921},{\"end\":60954,\"start\":60935},{\"end\":60968,\"start\":60954},{\"end\":60988,\"start\":60968},{\"end\":61002,\"start\":60988},{\"end\":61015,\"start\":61002},{\"end\":61027,\"start\":61015},{\"end\":61873,\"start\":61860},{\"end\":62176,\"start\":62160},{\"end\":62190,\"start\":62176},{\"end\":62687,\"start\":62670},{\"end\":62701,\"start\":62687},{\"end\":62713,\"start\":62701},{\"end\":62720,\"start\":62713},{\"end\":63035,\"start\":63013},{\"end\":63055,\"start\":63035},{\"end\":63069,\"start\":63055},{\"end\":63085,\"start\":63069},{\"end\":63100,\"start\":63085},{\"end\":63114,\"start\":63100},{\"end\":63135,\"start\":63114},{\"end\":63150,\"start\":63135},{\"end\":63596,\"start\":63578},{\"end\":63606,\"start\":63596},{\"end\":63619,\"start\":63606},{\"end\":63639,\"start\":63619},{\"end\":63658,\"start\":63639},{\"end\":63670,\"start\":63658},{\"end\":64310,\"start\":64292},{\"end\":64326,\"start\":64310},{\"end\":64339,\"start\":64326},{\"end\":64351,\"start\":64339},{\"end\":64369,\"start\":64351},{\"end\":64715,\"start\":64706},{\"end\":64728,\"start\":64715},{\"end\":64734,\"start\":64728},{\"end\":64747,\"start\":64734},{\"end\":64760,\"start\":64747},{\"end\":64774,\"start\":64760},{\"end\":64788,\"start\":64774},{\"end\":64795,\"start\":64788},{\"end\":65182,\"start\":65167},{\"end\":65202,\"start\":65182},{\"end\":65219,\"start\":65202},{\"end\":65237,\"start\":65219},{\"end\":65567,\"start\":65555},{\"end\":65577,\"start\":65567},{\"end\":65588,\"start\":65577},{\"end\":65599,\"start\":65588},{\"end\":66184,\"start\":66167},{\"end\":66201,\"start\":66184},{\"end\":66449,\"start\":66420},{\"end\":66463,\"start\":66449},{\"end\":66472,\"start\":66463},{\"end\":66511,\"start\":66472},{\"end\":66529,\"start\":66511},{\"end\":66543,\"start\":66529},{\"end\":66560,\"start\":66543},{\"end\":66573,\"start\":66560},{\"end\":66593,\"start\":66573},{\"end\":66610,\"start\":66593},{\"end\":66615,\"start\":66610},{\"end\":67456,\"start\":67438},{\"end\":67472,\"start\":67456},{\"end\":67489,\"start\":67472},{\"end\":67510,\"start\":67489},{\"end\":67527,\"start\":67510},{\"end\":67543,\"start\":67527},{\"end\":68322,\"start\":68301},{\"end\":68338,\"start\":68322},{\"end\":68619,\"start\":68608},{\"end\":68628,\"start\":68619},{\"end\":68643,\"start\":68628},{\"end\":68662,\"start\":68643},{\"end\":68880,\"start\":68868},{\"end\":68891,\"start\":68880},{\"end\":68898,\"start\":68891},{\"end\":69303,\"start\":69286},{\"end\":69319,\"start\":69303},{\"end\":69329,\"start\":69319},{\"end\":69671,\"start\":69651},{\"end\":69688,\"start\":69671},{\"end\":69701,\"start\":69688},{\"end\":70056,\"start\":70042},{\"end\":70071,\"start\":70056},{\"end\":70386,\"start\":70373},{\"end\":70397,\"start\":70386},{\"end\":70415,\"start\":70397},{\"end\":70431,\"start\":70415},{\"end\":70444,\"start\":70431},{\"end\":70460,\"start\":70444},{\"end\":70472,\"start\":70460},{\"end\":70489,\"start\":70472},{\"end\":70502,\"start\":70489},{\"end\":70514,\"start\":70502},{\"end\":70784,\"start\":70769},{\"end\":70801,\"start\":70784},{\"end\":70816,\"start\":70801},{\"end\":70829,\"start\":70816},{\"end\":71291,\"start\":71269},{\"end\":71302,\"start\":71291},{\"end\":71317,\"start\":71302},{\"end\":71322,\"start\":71317},{\"end\":71632,\"start\":71613},{\"end\":71646,\"start\":71632},{\"end\":71656,\"start\":71646},{\"end\":71673,\"start\":71656},{\"end\":71687,\"start\":71673},{\"end\":72453,\"start\":72439},{\"end\":72465,\"start\":72453},{\"end\":72477,\"start\":72465},{\"end\":72491,\"start\":72477},{\"end\":72504,\"start\":72491},{\"end\":72523,\"start\":72504},{\"end\":72545,\"start\":72523},{\"end\":72560,\"start\":72545},{\"end\":73069,\"start\":73059},{\"end\":73074,\"start\":73069},{\"end\":73233,\"start\":73225},{\"end\":73248,\"start\":73233},{\"end\":73261,\"start\":73248},{\"end\":73715,\"start\":73703},{\"end\":73731,\"start\":73715},{\"end\":73746,\"start\":73731},{\"end\":74011,\"start\":73998},{\"end\":74036,\"start\":74011},{\"end\":74049,\"start\":74036},{\"end\":74068,\"start\":74049},{\"end\":74083,\"start\":74068},{\"end\":74103,\"start\":74083}]", "bib_venue": "[{\"end\":59740,\"start\":59675},{\"end\":60117,\"start\":60113},{\"end\":60448,\"start\":60434},{\"end\":61135,\"start\":61027},{\"end\":61922,\"start\":61901},{\"end\":62265,\"start\":62213},{\"end\":62668,\"start\":62633},{\"end\":63190,\"start\":63174},{\"end\":63782,\"start\":63684},{\"end\":64409,\"start\":64393},{\"end\":64840,\"start\":64820},{\"end\":65283,\"start\":65260},{\"end\":65939,\"start\":65874},{\"end\":66208,\"start\":66201},{\"end\":66703,\"start\":66615},{\"end\":67631,\"start\":67543},{\"end\":68299,\"start\":68223},{\"end\":68665,\"start\":68662},{\"end\":68995,\"start\":68919},{\"end\":69371,\"start\":69354},{\"end\":69755,\"start\":69701},{\"end\":70040,\"start\":69971},{\"end\":70371,\"start\":70335},{\"end\":70906,\"start\":70829},{\"end\":71267,\"start\":71189},{\"end\":71812,\"start\":71710},{\"end\":72314,\"start\":72290},{\"end\":72629,\"start\":72560},{\"end\":73265,\"start\":73261},{\"end\":73429,\"start\":73404},{\"end\":73576,\"start\":73558},{\"end\":73778,\"start\":73746},{\"end\":74201,\"start\":74103},{\"end\":61230,\"start\":61137},{\"end\":62346,\"start\":62267},{\"end\":63202,\"start\":63192},{\"end\":63808,\"start\":63784},{\"end\":64421,\"start\":64411},{\"end\":66814,\"start\":66705},{\"end\":67742,\"start\":67633},{\"end\":70970,\"start\":70908},{\"end\":71939,\"start\":71814},{\"end\":72717,\"start\":72631},{\"end\":74213,\"start\":74203}]"}}}, "year": 2023, "month": 12, "day": 17}