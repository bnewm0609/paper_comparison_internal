{"id": 244830353, "updated": "2023-10-05 19:24:39.504", "metadata": {"title": "An Automated Snow Mapper Powered by Machine Learning", "authors": "[{\"first\":\"Haojie\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Limin\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Lin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Hongyu\",\"last\":\"Luo\",\"middle\":[]}]", "venue": "Remote Sensing", "journal": "Remote Sensing", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": ": Snow preserves fresh water and impacts regional climate and the environment. Enabled by modern satellite Earth observations, fast and accurate automated snow mapping is now possible. In this study, we developed the Automated Snow Mapper Powered by Machine Learning (AutoSMILE), which is the \ufb01rst machine learning-based open-source system for snow mapping. It is built in a Python environment based on object-based analysis. AutoSMILE was \ufb01rst applied in a mountainous area of 1002 km 2 in Bome County, eastern Tibetan Plateau. A multispectral image from Sentinel-2B, a digital elevation model, and machine learning algorithms such as random forest and convolutional neural network, were utilized. Taking only 5% of the study area as the training zone, AutoSMILE yielded an extraordinarily satisfactory result over the rest of the study area: the producer\u2019s accuracy, user\u2019s accuracy, intersection over union and overall accuracy reached 99.42%, 98.78%, 98.21% and 98.76%, respectively, at object level, corresponding to 98.84%, 98.35%, 97.23% and 98.07%, respectively, at pixel level. The model trained in Bome County was subsequently used to map snow at the Qimantag Mountain region in the northern Tibetan Plateau, and a high overall accuracy of 97.22% was achieved. AutoSMILE outperformed threshold-based methods at both sites and exhibited superior performance especially in handling complex land covers. The outstanding performance and robustness of AutoSMILE in the case studies suggest that AutoSMILE is a fast and reliable tool for large-scale high-accuracy snow mapping and monitoring.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/remotesensing/WangZWHL21", "doi": "10.3390/rs13234826"}}, "content": {"source": {"pdf_hash": "15f7a1aefd74f4482bf60512119793def6da0570", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.mdpi.com/2072-4292/13/23/4826/pdf?version=1638023406", "status": "GOLD"}}, "grobid": {"id": "9599504a6e9d954877f365d7faff8017d7a794f5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/15f7a1aefd74f4482bf60512119793def6da0570.txt", "contents": "\nremote sensing An Automated Snow Mapper Powered by Machine Learning\nPublished: 27 November 2021\n\nHaojie Wang h.wang@connect.ust.hkh.w. \nDepartment of Civil and Environmental Engineering\nThe Hong Kong University of Science and Technology\nHong KongChina\n\nLimin Zhang \nDepartment of Civil and Environmental Engineering\nThe Hong Kong University of Science and Technology\nHong KongChina\n\nLin Wang wangl@ust.hkl.w. \nDepartment of Civil and Environmental Engineering\nThe Hong Kong University of Science and Technology\nHong KongChina\n\nJian He \nDepartment of Civil and Environmental Engineering\nThe Hong Kong University of Science and Technology\nHong KongChina\n\nHongyu Luo \nDepartment of Civil and Environmental Engineering\nThe Hong Kong University of Science and Technology\nHong KongChina\n\nAnnett Bartsch \nDepartment of Civil and Environmental Engineering\nThe Hong Kong University of Science and Technology\nHong KongChina\n\nGareth Rees \nDepartment of Civil and Environmental Engineering\nThe Hong Kong University of Science and Technology\nHong KongChina\n\nremote sensing An Automated Snow Mapper Powered by Machine Learning\nPublished: 27 November 202110.3390/rs13234826Received: 30 September 2021 Accepted: 18 November 2021Article Citation: Wang, H.; Zhang, L.; Wang, L.; He, J.; Luo, H. An Automated Snow Mapper Powered by Machine Learning. Remote Sens. 2021, 13, 4826. https://doi.org/ 10.3390/rs13234826 Academic Editors: Publisher's Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/).automated snow mappingsnow covermachine learningmultispectral imageobject-based analysisremote sensingSentinel-2\nSnow preserves fresh water and impacts regional climate and the environment. Enabled by modern satellite Earth observations, fast and accurate automated snow mapping is now possible. In this study, we developed the Automated Snow Mapper Powered by Machine Learning (AutoSMILE), which is the first machine learning-based open-source system for snow mapping. It is built in a Python environment based on object-based analysis. AutoSMILE was first applied in a mountainous area of 1002 km 2 in Bome County, eastern Tibetan Plateau. A multispectral image from Sentinel-2B, a digital elevation model, and machine learning algorithms such as random forest and convolutional neural network, were utilized. Taking only 5% of the study area as the training zone, AutoSMILE yielded an extraordinarily satisfactory result over the rest of the study area: the producer's accuracy, user's accuracy, intersection over union and overall accuracy reached 99.42%, 98.78%, 98.21% and 98.76%, respectively, at object level, corresponding to 98.84%, 98.35%, 97.23% and 98.07%, respectively, at pixel level. The model trained in Bome County was subsequently used to map snow at the Qimantag Mountain region in the northern Tibetan Plateau, and a high overall accuracy of 97.22% was achieved. AutoSMILE outperformed threshold-based methods at both sites and exhibited superior performance especially in handling complex land covers. The outstanding performance and robustness of AutoSMILE in the case studies suggest that AutoSMILE is a fast and reliable tool for large-scale high-accuracy snow mapping and monitoring.\n\nIntroduction\n\nSnow is one of the most common land covers [1]. As a major source of Earth surface fresh water, snow melts into water and runs to rivers and lakes, consequently impacting drinking water supply, regional ecosystem [2], agriculture [3], and the climate at large. Therefore, fast and accurate mapping of snow cover is essential for monitoring natural resources, regional climate change and environment evolution [4]. Owing to the fast growth of Earth observation (EO) data, frequent monitoring of large-scale snow covers becomes possible.\n\nVisual interpretation features high accuracy but requires a tremendous amount of time and effort, especially when dealing with large areas [5]. Apart from visual interpretation, many existing studies apply rule-based methods for fast extraction of snow areas [6][7][8][9]. The National Aeronautics and Space Administration (NASA) uses multiple thresholds for different bands and the normalized difference snow index (NDSI) to produce the fifth collection of Moderate Resolution Imaging Spectroradiometer (MODIS) snow products [10], with the highest resolution of 500 m. Gascoin et al. launched the Theia snow collection to provide high-resolution (20/30 m) snow products for the mountain regions in western Europe (e.g., Alps, Pyrenees) and the High Atlas in Morocco using multiple thresholds on DEM and indices calculated from Sentinel-2 and Landsat-8 images [9]. Although rule-based methods exhibit good capabilities in mapping snow of some regions [11], the \n\n\nData\n\nA multi-spectral Level-1C product of Sentinel-2B, which was captured on 30 April 2021, was acquired and used in this study for snow mapping (Figure 2a). The Sen2Cor processor developed by the European Space Agency (ESA) was first used to generate the Level-2A product by performing atmospheric corrections on the original product. A multi-spectral product of Sentinel-2B has thirteen bands: four bands with 10 m spatial resolution (Blue, Green, Red and Near Infra-Red bands), six with 20 m spatial resolution (Red Edge 1, Red Edge 2, Red Edge 3, Narrow Near Infra-Red, Short Wave Infrared 1 and Short Wave Infrared 2 bands) and another three with 60 m spatial resolution (Coastal Aerosol, Water Vapor and Cirrus bands). In order to have a uniform spatial resolution for feature extraction, Sen2Res, a processor recommended by the ESA, is used to enhance the spatial resolution of low-resolution bands to 10 m. Sen2Res uses a super-resolution method proposed by Brodu [27], which utilizes shared information between bands and bandspecific information, to produce super resolution products. After resolution enhancement, the spatial resolutions of all bands increase to 10 m except Coastal Aerosol, Water Vapor and Cirrus bands. The three bands are not used in this study due to their insufficient original resolution (60 m) and intended purposes (the Coastal Aerosol band for aerosol retrieval, the Water Vapor band for water vapor correction and the Cirrus band for cirrus detection). Detailed band statistics and information of the 13 bands within the study area are summarized in Table 1. In the case of cloud presence, the cloud mask which comes with each Sentinel-2 product can be used to remove cloud areas. Based on the true color image shown in Figure 2a, which is composed of red, green and blue channels, a pixel-level snow cover inside the study area was manually interpreted and presented in Figure 2b. A small part of the study area is chosen as the training zone (red box in Figure 2a) and the rest of the area is used for model testing. With the intention of developing a universally applicable tool which can be trained with limited data but predict accurately for a large area, the training zone is purposely set to only occupy approximately 5% (50.4 km 2 ) of the entire study area (1002 km 2 ). Other than the multi-spectral data, we also examined the contribution of auxiliary data in snow mapping. Thus, two kinds of auxiliary data are collected in this study and shown in Figure 3; namely, a digital elevation model (DEM) and its derivatives, and spectral band-derived data. The DEM data of 12.5 m resolution of the study area is collected from the Radiometric Terrain Correction (RTC) product created by the Alaska Satellite Facility (ASF), which originated from the NASA Shuttle Radar Topography Mission (SRTM) Global 1 arc second product. It is then resampled to 10 m using the nearest neighbor method (Figure 3a). Four widely used derivatives are then produced to describe the topographic characteristics of the study area; namely, planform curvature (Figure 3b), aspect (Figure 3c), topographic wetness index (TWI, Figure 3d) [28] and slope ( Figure 3e). The normalized difference vegetation index (NDVI, Figure 3f), normalized difference moisture index (NDMI, Figure 3g) [29] and the modified normalized difference water index (MNDWI, Figure 3h) [30] are chosen as the band-derived indices to provide additional information about the land cover. The three indices are given by:\nNDVI = NIR \u2212 RED NIR + RED (1) NDMI = NIR \u2212 SWIR1 NIR + SWIR1(2)MNDWI = NDSI = GREEN \u2212 SWIR1 GREEN + SWIR1(3)\nwhere NDVI is a common indicator of surface plant health; NDMI is sensitive to moisture levels in vegetation; and MNDWI, also known as the normalized difference snow index (NDSI), is generally used to enhance the open water features or identify snow cover. which is used to produce the NDVI, NDMI and MNDWI layers, is acquired from the Copernicus Open Access Hub of the European Space Agency).\n\n\nAutoSMILE: Automated Snow Mapper Powered by Machine Learning\n\nIn this study, we developed the system AutoSMILE (Automated Snow Mapper Powered by Machine Learning) with open-source packages in the Python 3.8 environment, to automate the process of snow mapping. As shown in Figure 4, AutoSMILE consists of three main modules; namely, a data processing module that includes image segmentation and feature extraction, a model training module that applies machine learning and deep learning and a snow mapping module that conducts snow mapping with trained models and visual inspection. Technical details of these modules will be introduced in the following sections. \n\n\nImage Segmentation\n\nImage segmentation is the first step after data preparation. In AutoSMILE, the quickshift mode-seeking algorithm proposed by Vedaldi and Soatto [31] is employed, which is available in the scikit-image package (https://scikit-image.org/, accessed on 30 September 2021). Given a set of data points, the goal of a mode-seeking algorithms is to locate the maxima (i.e., modes) of the density function estimated from the set of data. Quickshift, which computes a hierarchical segmentation on multiple scales concurrently, is essentially a modified version of traditional mean-shift algorithm. Given N data points x 1 , . . . , x N , quickshift starts by computing the Parzen density estimate P(x):\nP(x) = 1 N N \u2211 i=1 k(D(x i , x j )) \u00d7 100%, x i , x j \u2208 R d(4)\nwhere k(x) is a kernel function, which is often in the form of a Gaussian window. D(x i , x j ) is the distance function between x i and x j . Then, quickshift moves each point x i to the nearest neighbor for which there is an increment of density P(x) and subsequently connects all the points to a single hierarchical tree. By breaking the branches of the tree that are longer than a certain threshold, modes can be recovered and used for image segmentation.\n\nIn the quickshift function of scikit-image, two key parameters, which control the segmentation process, can be identified; namely, the kernel size (KZ) and maximum distance (MD). KZ controls the width of Gaussian kernels used in local density approximation and MD is the threshold that decides the level in the hierarchical segmentation. The impact of different values of KZ and MD on AutoSMILE's performance is also investigated and discussed in Section 3.1. Other parameters in this function (e.g., ratio and sigma) remain at their default values. The image segmentation is conducted on the true-color image of the study area ( Figure 2a). After the quickshift segmentation, similar pixels share the same mode, gather together and form objects. Note that an object is inherently a group of similar pixels.\n\n\nObject Labelling and Feature Extraction\n\nThe label of each object is assigned based on the manually interpreted pixel-level snow cover ( Figure 2b). First, the label of each pixel is determined by whether its location falls in the snow area. Then, a simple strategy is applied in AutoSMILE: objects with more than 50% of snow member pixels are assigned with snow object labels; otherwise, non-snow labels are given. Note that it is inevitable some objects contain both snow and non-snow pixels. Thus, the snow covers mapped by snow objects and snow pixels, so called object-based and pixel-based snow covers, will be slightly different and the difference is defined as the segmentation loss. Assume that an image has W \u00d7 L pixels. After certain segmentation, M objects are created. Each object i has n i member pixels and the fraction of snow member pixels is P i . Then the segmentation loss of the object-based snow cover can be defined as:\nsegmentation loss = \u2211 M i=1 f (n i , P i ) W \u00d7 L , f (n i , P i ) = n i \u00d7 P i n i \u00d7 (1 \u2212 P i ) P i < 0.5 P i \u2265 0.5(5)\nAs the data layers are already prepared at a uniform resolution (10 m in this study), the values of member pixels of a certain object can be easily extracted from different data layers to calculate object features. To fully characterize objects, AutoSMILE creates both high-level band and texture features for each object. The band features, which are calculated band-wise, include the minimum, maximum, mean, variance, skewness and kurtosis of the member pixels' values of an object. Regarding the texture features, AutoSMILE uses the Haralick method [32] to calculate the grey-level co-occurrence matrix (GLCM) of an object. Based on the GLCM, four-directional (i.e., 0 \u2022 , 45 \u2022 , 90 \u2022 and 135 \u2022 ) contrast, dissimilarity, homogeneity, angular second moment (ASM) and correlation are calculated using the scikit-image package. In this study, texture features are calculated based on red, green and blue bands. Thus, each object will obtain six band features from each layer and 20 more texture features (five features in each direction) if the red, green or blue band is encountered.\n\n\nMachine Learning and Deep Learning\n\nUnder the proposed framework of AutoSMILE, the snow mapping problem is treated as a binary classification task and no particular machine learning or deep learning algorithm is required. In this study, we select one popular machine learning algorithm (i.e., random forest, RF) and one well-known deep learning algorithm (i.e., convolutional neural network, CNN) due to their outstanding performance and robustness in various applications [33,34].\n\nRF, as an ensemble algorithm, is developed on the basis of decision trees and bagging [35]. In classification tasks, the output of RF is the class voted by the majority of the trees. The major improvement of RF is that it decorrelates the trees in it by only selecting a random subset of the features at each candidate split when constructing each decision tree.\n\nA typical practice is to randomly choose \u221a p (rounded down) features at each split for a classification task with p features. CNN is a popular deep learning algorithm in computer vision [36]. It is good at feature extraction and usually consists of four types of layers: convolutional layers that use a set of convolutional filters to activate certain features from the input; activation layers like the rectified linear unit (ReLU) to accelerate the training process; pooling layers which simplify the output by non-linear downsampling and other basic layers such as the fully connected layers and flatten layers. In this study, the input is taken as a 'long' picture with unit width to apply CNN for feature learning and snow mapping.\n\n\nPerformance Evaluation and Post-Processing\n\nTrained machine learning models are then used for mapping snow for the entire study area. Four popular machine learning performance indices in remote sensing are selected to evaluate the performance of trained models; namely, producer's accuracy (PA), user's accuracy (UA), intersection over union (IoU) and overall accuracy (OA). Definitions of the four indices are given as follows:\nPA = TP TP + FN \u00d7 100% (6) UA = TP TP + FP \u00d7 100% (7) IoU = TP TP + FP + FN \u00d7 100% (8) OA = TP + TN TP + TN + FP + FN \u00d7 100%(9)\nwhere TP (true positive) and FP (false positive) are the numbers of correct and incorrect positive/snow predictions. Similarly, TN (true negative) and FN (false negative) stand for the numbers of correct and incorrect negative/non-snow predictions, respectively. PA, also known as recall, is the percentage of TP predictions among all positive samples. UA quantifies the fraction of TP predictions among all samples predicted to be positive, which is also known as precision. IoU measures the ratio of the intersection between ground truth snow area and predicted snow area over their union. Due to the presence of two sets of ground truths: objectand pixel-based snow covers, the snow cover mapped by the trained ML model is compared with the two snow covers. The object-and pixel-based performance indices are then calculated, which reflect the model performance at different levels. The final step of AutoSMILE is visual inspection to ensure the snow mapping is satisfactory and to make necessary corrections before exporting the final snow cover product.\n\n\nResults\n\n\nSnow Mapping Results from Different Segmentations\n\nTo investigate the influence of segmentation parameters on AutoSMILE's performance, different combinations of KZ and MD are tested. Table 2 summarizes the numbers of objects given by different combinations. It can be found that a low MD will lead to a large number of objects and that the object number drops rapidly with an increase in MD. For example, when MD increases from 2 to 8, the maximum decrease of the object number reaches 99.5%. In contrast, KZ has a much less effect on the object number. In most cases, a higher KZ tends to have fewer objects. Figure 5 compares different segmentations that are mapped in a subarea of the training zone. The boundaries of objects produced with a higher MD overlap those of the objects produced with a lower MD. This is because MD controls the segmentation hierarchical level: objects segmented with higher MDs are inherently produced by merging objects segmented with lower MDs. In the cases of MD = 2 and 4, excessive oversegmentation can be observed as a large number of unnecessary small objects are created inside the same land cover. In view of computational efficiency, when the object number exceeds 100,000, AutoSMILE will slow down as object feature computation will require more resources. Thus, according to Table 2, quickshift with too small MDs (<8) is usually not recommended in AutoSMILE. When increasing KZ, some particularly large objects are created. In this case, the snow boundary cannot be properly handled, which will put the non-snow zone and the marginal snow zone into the same object. Thus, a small value of KZ (<8) is recommended in AutoSMILE to handle the segmentations in the margins between the snow area and the non-snow area. However, if an extremely large area is encountered, higher KZ and MD values will accelerate the analysis at the expense of some loss of accuracy. To further investigate the segmentation influence on the model performance, the samples built from different segmentations are used to train the RF models. Note that only features derived from the 10 multispectral bands are used and the models are only trained with the object samples that are within the training zone (i.e., the red box in Figure 2a). In this study, 100 decision trees are used in each RF model and the performance of the trained RF models is evaluated and listed in Table 3. Conditions of MD = 2 and 4 are not included in Table 3 due to excessive oversegmentation (Figure 5a,b,f,g,k,l,p,q,u,v). Referring to Tables 2 and 3, it can be found that the segmentation loss has a negative correlation with the object number. Due to the segmentation loss, all the pixel-based indices are lower than the corresponding object-based indices under different conditions, and the pixel-based indices decrease as the segmentation loss increases. The best pixel-level mapping result is achieved by the segmentation using KZ = 2 and MD = 8, which has the object and pixel IoUs of 98.21% and 97.23%, respectively, and the object and pixel OA values of 98.76% and 98.07%, respectively. Although the pixel-based mapping performance decreases with increasing segmentation loss, the lowest pixel IoU and OA still reach 92.12% and 94.42%, respectively, proving the robustness of AutoSMILE.  \n\n\nSnow Mapping Results of Different Datasets\n\nAs suggested in Table 3, KZ = 2 and MD = 8 are adopted to conduct snow mapping. The segmentation of the study area is visualized in Figure 6. It can be seen that small objects concentrate in the transition zones between snow and non-snow land covers, and the objects inside the same land cover are larger and much sparser. There are 77,848 objects inside the study area. Among all the segmented objects, 3934 objects that are within the training zone are used for model training, and the rest of the objects are used for model testing. Based on the segmentation in Figure 6, snow maps with different dataset combinations were examined to assess the contributions from extra auxiliary data. Three datasets were created using features derived from different data layers; namely, multispectral image dataset (MSID), multispectral image derived dataset (MSIDD) and digital elevation model dataset (DEMD). MSID only uses the 10 spectral layers, which derive 60 band features and 60 texture features. MSIDD includes 18 features which are calculated based on derived NDVI, NDMI and MNDWI layers. DEMD consists of 30 features which are extracted from the DEM and its derivatives. Table 4 compares results of snow mapping using different dataset combinations, where both CNN and RF are applied. The structure of the CNN is presented in Figure 7. In this study, the Adam optimizer was used to train the CNN model. The learning rate, maximum number of epochs and the mini-batch size were set to 0.0003, 100 and 256, respectively. The machine learning models trained by different dataset combinations all performed extraordinarily well with only minor differences. Interestingly, in most cases, the RF models performed slightly better than the CNN models with an OA improvement of around 0.2% to 0.4%. Due to the low complexity of the datasets, it is reasonable that deep learning models do not necessarily outperform conventional models. Adding the MSIDD slightly increased the pixel PA of the CNN models (~0.14%) but the RF performed similarly in both cases. By contrast, adding the DEMD did not improve the snow mapping and even caused minor pixel OA losses of the RF (~0.15%) model. If both the MSIDD and DEMD were included, the performance difference was also negligible: the pixel OA of RF decreased 0.22%. Overall, the pixel IoU and OA of CNN was always stable regardless of any supplement of new features. With pixel IoUs ranging from 96.66% to 97.23% and pixel OAs ranging from 97.67% to 98.07%, it can be concluded that the performance of AutoSMILE is not sensitive to extra auxiliary data. The snow cover can be mapped with very high accuracy using only the multispectral image.  To understand how AutoSMILE performs across the entire study area, Figure 8 classifies each pixel into four groups (TP, TN, FP and FN) by taking different snow covers as ground truths and predictions. In Figure 8a, the spatial distribution of segmentation loss can be directly observed by taking the object-based snow cover as a prediction and the pixel-based snow cover as ground truth. It can be spotted that most FN pixels are distributed near the outer boundary of the snow area and most FP pixels spread over the gullies inside the snow area. Segmentation loss can hardly be eliminated in the objectbased analysis as it is almost inevitable for some objects to have both snow and non-snow pixels after segmentation. The segmentation loss in this case is 1.17%, which is considered acceptable. Figure 8b shows the snow cover predicted by RF and MSID compared with the object-based snow cover. Although some objects are misclassified with RF, the overall snow area is complete, and the complex snow boundaries are successfully captured. Figure 8c compares the snow cover predicted by RF and MSID with the pixel-based snow cover. With less than 2% of misclassification pixels, the overall performance of AutoSMILE at pixel level is excellent. According to Figure 8, it is recommended that visual inspection of the final snow mapping product should focus more on the transition zones between snow and non-snow areas.\n\n\nLayer Importance Analysis Based on the Best Model\n\nTo evaluate the contribution of different data layers in training the best model, a permutation-based feature importance analysis is conducted. By permuting the sample value of a specific feature, influential features could be identified if ML model performance drops. The importance of a layer is calculated as the average importance of the features derived from that layer. According to Table 4, the best ML model, which is trained with RF, MSID, KZ = 2 and MD = 8, is used to estimate the layer importance. Figure 9 shows the data layer importance relative to the red band layer, which is shown to be the most important to the model. However, the differences among the first three important layers (red, blue and green) are not significant. As these three bands can be integrated into the true-color image, it is not surprising that they rank at top. Texture layers rank the lowest, indicating that land cover in this study area is not sensitive to texture change.  \n\n\nDiscussion\n\n\nThe Generalizability of the Trained AutoSMILE Model\n\nTo assess the generalizability of the trained model in mapping snow covers for new images, a new multispectral image covering Qimantag Mountain in the western segment of eastern Kunlun Mountains, northern Tibetan Plateau, was acquired for extra testing. The image was captured by Sentinel 2B on 2 June 2019. In terms of administrative division, the new testing zone was at the southeast corner of the Xinjiang Uyghur Autonomous Region (Figure 10a). The true-color RGB image of the new testing zone is shown in Figure 10b, which covers an area of 579 km 2 . The manually interpreted snow cover is shown in Figure 10c. The ML models trained in Bome County were then directly applied to predict snow covers for the testing zone in the Qimantag Mountain region. Results are summarized in Table 5. Although the place, time and general environment changed, the RF model still achieves a pixel OA of 97.22% for the new testing zone, indicting a good generalizability of the ML model produced by AutoSMILE.  \n\n\nComparison of AutoSMILE and Threshold-Based Methods\n\nTo further evaluate the performance of AutoSMILE, a comparative study between AutoSMILE and two existing threshold-based methods was conducted. The first thresholdbased method was developed by NASA to produce the fifth version of MODIS Snow collection products, in which the following rules are applied: NDSI > 0.4, near-infrared reflectance (bandwidth: 841-876 nm) >0.11, and the band 4 reflectance (bandwidth: 545-565 nm) >0.1 [10]. The second method was proposed by Zhang et al. (2019), who found that a NDSI threshold of 0.1 is more reasonable than that of 0.4 for applications in China [6], so the second method simply applied the recommended threshold value (0.1) as the sole criterion. In Figure 11a, AutoSMILE (RF, KZ = 2, MD = 8, trained with MSID) ranks the first among the three methods in terms of the two most important indices: pixel IoU and OA. Compared with the MODIS method, AutoSMILE achieves a performance gain of 1.7% in terms of IoU and 1.2% in terms of OA, corresponding to a 11.4 km 2 increase of correctly predicted area. When compared with the method of Zhang et al. (2019), the performance gains narrowed down to 1.0% of IoU and 0.7% of OA, corresponding to a 6.7 km 2 increase of correctly predicted area. In order to find the optimal threshold value for both study areas, we further adopted the third method: testing all the NDSI values ranging from 0.01 to 0.80. The result of the study area in Bome County is shown in Figure 11b. It was found that the optimal threshold was 0.17 that gave an IoU of 96.35% and an OA of 97.43%. When examining the testing zone in the Qimantag Mountain region, the optimal threshold turned out to be 0.19, which gave an IoU of 86.81% and an OA of 96.63%. The changing optimal thresholds indicated that the threshold-based method could hardly find a universally applicable threshold. It usually required a site-specific calibration in order to obtain the best performance. In addition, in the Qimantag Mountain region, the RF model trained in Bome County outperformed the threshold-based method by 2.3% and 0.6% in terms of pixel IoU and OA, respectively. A more detailed comparison is presented in Figure 12 to illustrate how different methods perform at sub-regions of the study area in Bome County. The whole study area was first discretized into tiles of 1 \u00d7 1 km 2 . As our study area occupied 20.12 \u00d7 49.79 km 2 , in total 980 tiles were created and some marginal southern and eastern areas were omitted. Then, inside each tile, pixel-based IoU and OA were calculated based on the AutoSMILE method and the best threshold method at the study area in Bome County (NDSI > 0.17), respectively. IoU and OA results are shown in Figure 12a,b. It can be seen that vast majority of the data points concentrate either above or near the 1:1 line, indicating that the threshold-based method barely outperforms AutoSMILE in the sub-regions. A maximum increase of 92% of IoU can be observed in Figure 12a. Figure 12c further shows the spatial distribution of tiles where AutoSMILE-based OA (OA AutoSMILE ) exceeds the thresholdbased OA (OA threshold ) by at least 1%. Most identified tiles are located in the transition zones, and the highest OA difference reaches about 13%, indicating that AutoSMILE is particularly good at handling complex land cover conditions. Figure 12d presents the tiles where OA threshold \u2212 OA AutoSMILE >1%. It can be noticed that very limited tiles fulfill this condition, and the highest OA difference is only around 2.5%. Combining Figure 12c,d, it can be concluded that both methods perform quite well inside the same land cover; however, when encountering complex land covers like the transition zones, AutoSMILE outperforms the threshold-based method substantially. Comparison results between AutoSMILE and the best threshold-based method (NDSI > 0.17) in the study area of Bome County: (a) scatter plot of IoUs of 1 \u00d7 1 km 2 tiles using the two methods; (b) scatter plot of OAs of 1 \u00d7 1 km 2 tiles using the two methods; (c) OA difference heatmap of 1 \u00d7 1 km 2 tiles where OA AutoSMILE \u2212 OA threshold > 1% and (d) OA difference heatmap of 1 \u00d7 1 km 2 tiles where OA threshold \u2212 OA AutoSMILE > 1%. (NDSI means the normalized difference snow index, IoU means the intersection over union, and OA means the overall accuracy).\n\n\nConclusions\n\nAn automated snow mapper powered by machine learning, AutoSMILE, which is the first machine learning-based open-source system for snow mapping, was developed and tested in two regions on the Tibetan Plateau. The first region was taken as the main study area (Bome County, 1102 km 2 ) and the second region was only used for model testing (Qimantag Mountain region, 579 km 2 ). The machine learning techniques and object-based analysis were successfully integrated for snow mapping in the two regions. The following conclusions can be drawn:\n\n\n1.\n\nUsing only 5% of the main study area for training (50.4 km 2 ), AutoSMILE achieves extraordinarily satisfactory results of snow mapping in the rest of the main study area: object and pixel PAs, UAs, IoUs and OA values reaching 99.42% and 98.78%, 98.21% and 98.76%, 98.84% and 98.35%, and 97.23% and 98.07%, respectively. When applying the trained models to the testing zone in Qimantag Mountain region, the highest OA reaches 97.22%, indicating the excellent performance, generalizability and robustness of AutoSMILE.\n\n\n2.\n\nAccording to the parametric study of segmentation parameters, KZ and MD should be carefully determined. In AutoSMILE, a low MD is not recommended to avoid excessive oversegmentation and low computational efficiency, but a small value of KZ is recommended to better handle the transition zone between snow and non-snow areas.\n\n\n3.\n\nResults of alternating dataset combinations indicate that auxiliary data like multispectral image derived indices and DEM derivatives play a limited role in enhancing the performance of AutoSMILE. High-quality snow mapping can be accomplished with only the multispectral image using AutoSMILE. Based on permutation importance analysis of the best ML model, the top five important layers are red, blue, green, red edge 1 and short wave infrared 1 band layers.\n\n\n4.\n\nAutoSMILE outperforms the existing threshold-based methods in both regions. The optimal NDSI thresholds of the two regions vary, suggesting that the threshold method typically requires site knowledge to achieve the best performance and it is hard to find a universally applicable threshold. When investigating the performance at the sub-regions, it was found that both AutoSMILE and threshold-based methods perform well when the land cover is simple. However, when encountering complex conditions like snow mapping in transition zones, AutoSMILE outperforms the threshold-based method substantially (up to 92% of IoU increase and up to 13% of OA increase).\n\n\n5.\n\nDue to the inevitable segmentation loss induced by object-based analysis, transition zones between snow and non-snow areas require more attention when inspecting the final snow cover products. \n\nFigure 1 .\n1Locations of (a) Bome County and (b) the study area on a digital elevation map. (Source of the digital elevation model: National Aeronautics and Space Administration Shuttle Radar Topography Mission Global 1 arc second product).\n\nFigure 2 .\n2(a) True-color red, green and blue (RGB) image of the study area in Bome County and the location of training zone and (b) pixel-level snow cover of the study area based on visual interpretation.\n\nFigure 3 .\n3Data layers of the study area: (a) digital elevation model; (b) planform curvature; (c) aspect; (d) topographic wetness index; (e) slope; (f) normalized difference vegetation index (NDVI); (g) normalized difference moisture index (NDMI) and (h) modified normalized difference water index (MNDWI). (The digital elevation model is acquired from the Alaska Satellite Facility Distributed Active Archive Center and originated from the National Aeronautics and Space Administration Shuttle Radar Topography Mission Global 1 arc second product. The Sentinel-2B multi-spectral image,\n\nFigure 4 .\n4Framework of AutoSMILE (automated Snow Mapper Powered by Machine Learning).\n\nFigure 5 .\n5Influence of different segmentation parameters in a subarea of the training zone. (Segmented objects are delineated in red polygons).\n\nFigure 6 .\n6Segmentation results using quickshift. (Kernel size = 2, maximum distance = 8, and segmented objects are delineated in blue polygons).\n\nFigure 7 .\n7Network architecture of the applied convolutional neural network model.\n\nFigure 8 .\n8Results of pixel classification using different snow covers as predictions and ground truths: (a) object-based snow cover as prediction and pixel-based snow cover as ground truth, (b) snow cover predicted by the random forest model as prediction and object-based snow cover as ground truth and (c) snow cover predicted by the random forest model as prediction and pixel-based snow cover as ground truth. (TP: true positive; TN: true negative; FP: false positive; and FN: false negative).\n\nFigure 9 .\n9Data layer importance relative to the red band layer.\n\nFigure 10 .\n10(a) Location of the new testing zone; (b) true-color RGB image of the new testing zone and (c) pixel-level snow cover of the new testing zone based on visual interpretation.\n\nFigure 11 .\n11Results of the comparative study: (a) performance of different methods over the study area in Bome County (excluding the training zone); (b) performance of MNDWI/NDSI-based method as functions of threshold value over the study area in Bome County (excluding the training zone); (c) performance of MNDWI/NDSI-based method as functions of threshold value over the new testing zone in the Qimantag Mountain region. (PA: producer's accuracy; UA: user's accuracy; IoU: intersection over union; OA: overall accuracy; MNDWI: modified normalized difference water index; and NDSI: normalized difference snow index).\n\nFigure 12 .\n12Figure 12. Comparison results between AutoSMILE and the best threshold-based method (NDSI > 0.17) in the study area of Bome County: (a) scatter plot of IoUs of 1 \u00d7 1 km 2 tiles using the two methods; (b) scatter plot of OAs of 1 \u00d7 1 km 2 tiles using the two methods; (c) OA difference heatmap of 1 \u00d7 1 km 2 tiles where OA AutoSMILE \u2212 OA threshold > 1% and (d) OA difference heatmap of 1 \u00d7 1 km 2 tiles where OA threshold \u2212 OA AutoSMILE > 1%. (NDSI means the normalized difference snow index, IoU means the intersection over union, and OA means the overall accuracy).\n\nTable 1 .\n1Band information and statistics of the multispectral image of the study area in Bome County.Band ID \nName \nOriginal \nResolution \n\nSuper \nResolution \n\nStatistics of Super-Resolution Products \n\nMinimum \nMaximum \nMean \nStandard \nDeviation \n\n1 \nCoastal Aerosol \n60 \n-\n-\n-\n-\n-\n2 \nBlue \n10 \n10 \n0 \n1.87 \n0.75 \n0.53 \n3 \nGreen \n10 \n10 \n0 \n1.77 \n0.72 \n0.48 \n4 \nRed \n10 \n10 \n0 \n1.69 \n0.67 \n0.43 \n5 \nRed Edge 1 \n20 \n10 \n0 \n1.52 \n0.67 \n0.41 \n6 \nRed Edge 2 \n20 \n10 \n0 \n1.3 \n0.64 \n0.37 \n7 \nRed Edge 3 \n20 \n10 \n0 \n1.2 \n0.61 \n0.33 \n8 \nNear Infra-Red (NIR) \n10 \n10 \n0 \n1.5 \n0.63 \n0.33 \n8a \nNarrow NIR \n20 \n10 \n0 \n1.11 \n0.57 \n0.29 \n9 \nWater Vapor \n60 \n-\n-\n-\n-\n-\n10 \nCirrus \n60 \n-\n-\n-\n-\n-\n\n11 \nShort Wave Infrared 1 \n(SWIR1) \n20 \n10 \n0 \n1.44 \n0.11 \n0.09 \n\n12 \nShort Wave Infrared 2 \n(SWIR2) \n20 \n10 \n0 \n1.73 \n0.1 \n0.07 \n\n\n\nTable 2 .\n2Number of objects based on different segmentation parameters.Kernel Size \nMaximum Distance \n\n2 \n4 \n8 \n16 \n32 \n\n2 \n3,484,716 \n703,033 \n77,848 \n35,329 \n30,263 \n4 \n3,567,314 \n674,536 \n31,822 \n9333 \n7391 \n8 \n3,601,859 \n711,186 \n17,106 \n3737 \n1989 \n16 \n3,618,718 \n749,797 \n17,466 \n1210 \n845 \n32 \n3,629,014 \n782,430 \n23,571 \n798 \n306 \n\n\n\nTable 3 .\n3Performance evaluation of snow mapping with different segmentation parameters using random forest (RF) over the study area in Bome County (excluding the training zone).Kernel \nSize \n\nMaximum \nDistance \n\nSegmentation \nLoss (%) \n\nObject \nPA (%) \n\nObject \nUA (%) \n\nObject \nIoU (%) \n\nObject \nOA (%) \n\nPixel \nPA (%) \n\nPixel \nUA (%) \n\nPixel \nIoU (%) \n\nPixel \nOA (%) \n\n2 \n\n8 \n1.17 \n99.42 \n98.78 \n98.21 \n98.76 \n98.84 \n98.35 \n97.23 \n98.07 \n16 \n2.04 \n99.50 \n99.25 \n98.76 \n99.15 \n98.08 \n98.45 \n96.58 \n97.62 \n32 \n2.64 \n99.30 \n99.61 \n98.91 \n99.26 \n97.39 \n98.43 \n95.90 \n97.15 \n\n4 \n\n8 \n1.74 \n99.23 \n99.41 \n98.65 \n99.07 \n98.04 \n98.66 \n96.76 \n97.75 \n16 \n2.61 \n99.52 \n99.65 \n99.18 \n99.44 \n97.29 \n98.60 \n95.96 \n97.19 \n32 \n3.09 \n99.39 \n99.87 \n99.26 \n99.50 \n96.71 \n98.61 \n95.41 \n96.81 \n\n8 \n\n8 \n2.81 \n99.54 \n99.68 \n99.23 \n99.48 \n97.06 \n98.57 \n95.71 \n97.02 \n16 \n3.12 \n99.70 \n99.84 \n99.54 \n99.69 \n96.76 \n98.58 \n95.43 \n96.82 \n32 \n3.34 \n99.64 \n100.00 \n99.63 \n99.75 \n96.41 \n98.68 \n95.18 \n96.65 \n\n16 \n\n8 \n3.38 \n99.54 \n99.82 \n99.36 \n99.57 \n96.29 \n98.56 \n94.95 \n96.49 \n16 \n3.63 \n99.71 \n98.53 \n98.25 \n98.81 \n96.43 \n97.48 \n94.09 \n95.85 \n32 \n3.69 \n100.00 \n98.81 \n98.81 \n99.20 \n96.47 \n97.66 \n94.29 \n96.00 \n\n32 \n\n8 \n4.16 \n99.73 \n99.53 \n99.27 \n99.50 \n96.15 \n97.61 \n93.94 \n95.75 \n16 \n5.30 \n99.94 \n99.72 \n99.67 \n99.78 \n95.13 \n97.14 \n92.54 \n94.74 \n32 \n5.43 \n99.79 \n99.29 \n99.08 \n99.38 \n95.08 \n96.73 \n92.12 \n94.42 \n\n\n\nTable 4 .\n4Performance evaluation of snow mapping with different datasets and machine learning algorithms over the study area (excluding the training zone). MSID: multispectral image dataset derived from 10 multispectral band layers. 2 CNN: convolution neural network. 3 RF: random forest. 4 MSIDD: multispectral image derived dataset derived from the NDVI, NDMI and MNDWI layers. 5 DEMD: digital elevation model dataset derived from the DEM, planform curvature, aspect, topographic wetness index (TWI) and slope layers.Dataset + Algorithm \nFeature \nNumber \n\nObject \nPA (%) \n\nObject \nUA (%) \n\nObject \nIoU (%) \n\nObject \nOA (%) \n\nPixel \nPA (%) \n\nPixel \nUA (%) \n\nPixel \nIoU (%) \n\nPixel \nOA (%) \n\nMSID 1 + CNN 2 \n120 \n98.69 \n98.86 \n97.58 \n98.32 \n98.15 \n98.47 \n96.67 \n97.68 \nMSID + RF 3 \n120 \n99.42 \n98.78 \n98.21 \n98.76 \n98.84 \n98.35 \n97.23 \n98.07 \nMSID + MSIDD 4 + CNN \n138 \n98.83 \n98.72 \n97.59 \n98.33 \n98.29 \n98.33 \n96.68 \n97.69 \nMSID + MSIDD + RF \n138 \n99.41 \n98.77 \n98.20 \n98.75 \n98.84 \n98.34 \n97.21 \n98.06 \nMSID + DEMD 5 + CNN \n150 \n98.85 \n98.68 \n97.56 \n98.31 \n98.31 \n98.29 \n96.66 \n97.67 \nMSID + DEMD + RF \n150 \n99.10 \n98.85 \n97.98 \n98.60 \n98.53 \n98.43 \n97.01 \n97.92 \nMSID + MSIDD + \nDEMD + CNN \n168 \n99.24 \n98.33 \n97.60 \n98.33 \n98.70 \n97.94 \n96.69 \n97.69 \n\nMSID + MSIDD + \nDEMD + RF \n168 \n98.94 \n98.90 \n97.87 \n98.52 \n98.38 \n98.49 \n96.92 \n97.85 \n\n1 \n\nTable 5 .\n5Performance evaluation of snow mapping in the testing zone (Qimantag Mountain region).ML Model \n\nObject \nPA \n(%) \n\nObject \nUA \n(%) \n\nObject \nIoU \n(%) \n\nObject \nOA \n(%) \n\nPixel \nPA \n(%) \n\nPixel \nUA \n(%) \n\nPixel \nIoU \n(%) \n\nPixel \nOA \n(%) \n\nCNN trained with \nMSID, KZ = 2 and MD \n= 8 in Bome County \n\n96.06 \n91.48 \n88.17 \n97.02 \n93.86 \n90.65 \n85.58 \n96.29 \n\nRF trained with MSID, \nKZ = 2 and MD = 8 in \nBome County \n\n99.12 \n92.54 \n91.78 \n97.95 \n96.89 \n91.73 \n89.11 \n97.22 \n\n\n\n\nAuthor Contributions: Conceptualization, H.W. and L.Z.; methodology, formal analysis, data curation, H.W. and L.W.; software, validation, investigation, H.W., J.H. and H.L.; writing-original draft preparation, H.W.; writing-review and editing, supervision, project administration, funding acquisition, L.Z. All authors have read and agreed to the published version of the manuscript. Funding: This research is supported by NSFC/RGC Joint Research Scheme (No. N_HKUST620/20), the National Natural Science Foundation of China (No. U20A20112), and the Research Grants Council of the Hong Kong SAR Government (Nos. 16203720 and 16205719). Availability Statement: The Sentinel-2 multispectral products in this study are available at Copernicus Open Access Hub of ESA (https://scihub.copernicus.eu/, accessed on 30 September 2021) and the DEM data are download from the ASF DAAC (https://search.asf.alaska.edu/, accessed on 30 September 2021).Data \nRemote Sens. 2021,13, 4826   \nAcknowledgments:The authors would like to thank ESA and ASF DAAC for providing the necessary data for this study.Conflicts of Interest:The authors declare no conflict of interest.\nThe Most Detailed Portrait of Earth. O Arino, P Bicheron, F Achard, J Latham, R Witt, J L Weber, Eur. Space Agency. 136Arino, O.; Bicheron, P.; Achard, F.; Latham, J.; Witt, R.; Weber, J.L. The Most Detailed Portrait of Earth. Eur. Space Agency 2008, 136, 25-31.\n\nOn the Importance of High-Resolution Time Series of Optical Imagery for Quantifying the Effects of Snow Cover Duration on Alpine Plant Habitat. J.-P Dedieu, B Z Carlson, S Bigot, P Sirguey, V Vionnet, P Choler, 10.3390/rs8060481Remote Sens. 2016, 8, 481. [CrossRefDedieu, J.-P.; Carlson, B.Z.; Bigot, S.; Sirguey, P.; Vionnet, V.; Choler, P. On the Importance of High-Resolution Time Series of Optical Imagery for Quantifying the Effects of Snow Cover Duration on Alpine Plant Habitat. Remote Sens. 2016, 8, 481. [CrossRef]\n\nImportance of Snow and Glacier Meltwater for Agriculture on the Indo-Gangetic Plain. H Biemans, C Siderius, A F Lutz, S Nepal, B Ahmad, T Hassan, W Von Bloh, R R Wijngaard, P Wester, A B Shrestha, 10.1038/s41893-019-0305-3Nat. Sustain. 2Biemans, H.; Siderius, C.; Lutz, A.F.; Nepal, S.; Ahmad, B.; Hassan, T.; von Bloh, W.; Wijngaard, R.R.; Wester, P.; Shrestha, A.B.; et al. Importance of Snow and Glacier Meltwater for Agriculture on the Indo-Gangetic Plain. Nat. Sustain. 2019, 2, 594-601. [CrossRef]\n\nThe Snow Load in Europe and the Climate Change. P Croce, P Formichi, F Landi, P Mercogliano, E Bucchignani, A Dosio, S Dimova, 10.1016/j.crm.2018.03.001Clim. Risk Manag. 20Croce, P.; Formichi, P.; Landi, F.; Mercogliano, P.; Bucchignani, E.; Dosio, A.; Dimova, S. The Snow Load in Europe and the Climate Change. Clim. Risk Manag. 2018, 20, 138-154. [CrossRef]\n\nUncertainties of Snow Cover Extraction Caused by the Nature of Topography and Underlying Surface. J Zhao, Y Shi, Y Huang, J Fu, 10.1007/s40333-015-0044-xJ. Arid Land. 7Zhao, J.; Shi, Y.; Huang, Y.; Fu, J. Uncertainties of Snow Cover Extraction Caused by the Nature of Topography and Underlying Surface. J. Arid Land 2015, 7, 285-295. [CrossRef]\n\nGround-Based Evaluation of MODIS Snow Cover Product V6 across China: Implications for the Selection of NDSI Threshold. H Zhang, F Zhang, G Zhang, T Che, W Yan, M Ye, N Ma, 10.1016/j.scitotenv.2018.10.128Sci. Total Environ. 651Zhang, H.; Zhang, F.; Zhang, G.; Che, T.; Yan, W.; Ye, M.; Ma, N. Ground-Based Evaluation of MODIS Snow Cover Product V6 across China: Implications for the Selection of NDSI Threshold. Sci. Total Environ. 2019, 651, 2712-2726. [CrossRef]\n\nOn the Need for a Time-and Location-Dependent Estimation of the NDSI Threshold Value for Reducing Existing Uncertainties in Snow Cover Maps at Different Scales. Cryosphere. S H\u00e4rer, M Bernhardt, M Siebers, K Schulz, 10.5194/tc-12-1629-201812H\u00e4rer, S.; Bernhardt, M.; Siebers, M.; Schulz, K. On the Need for a Time-and Location-Dependent Estimation of the NDSI Threshold Value for Reducing Existing Uncertainties in Snow Cover Maps at Different Scales. Cryosphere 2018, 12, 1629-1642. [CrossRef]\n\nAlgorithm Theoretical Basis Document (ATBD) for the MODIS Snow and Sea Ice-Mapping Algorithms. D K Hall, G A Riggs, V V Salomonson, J Barton, K Casey, J Chien, N Digirolamo, A Klein, H Powell, A Tait, NASA Goddard Space Flight Center. Hall, D.K.; Riggs, G.A.; Salomonson, V.V.; Barton, J.; Casey, K.; Chien, J.; DiGirolamo, N.; Klein, A.; Powell, H.; Tait, A. Algorithm Theoretical Basis Document (ATBD) for the MODIS Snow and Sea Ice-Mapping Algorithms; NASA Goddard Space Flight Center: Greenbelt, MD, USA, 2001.\n\nTheia Snow Collection: High-Resolution Operational Snow Cover Maps from Sentinel-2 and Landsat-8 Data. S Gascoin, M Grizonnet, M Bouchet, G Salgues, O Hagolle, 10.5194/essd-11-493-2019Earth Syst. Sci. Data. 11Gascoin, S.; Grizonnet, M.; Bouchet, M.; Salgues, G.; Hagolle, O. Theia Snow Collection: High-Resolution Operational Snow Cover Maps from Sentinel-2 and Landsat-8 Data. Earth Syst. Sci. Data 2019, 11, 493-514. [CrossRef]\n\nProducts User Guide to Collection 5. D K Hall, V V Salomonson, Snow, NASA Goddard Space Flight Center. Hall, D.K.; Salomonson, V.V. MODIS Snow Products User Guide to Collection 5; NASA Goddard Space Flight Center: Greenbelt, MD, USA, 2006.\n\nAccuracy Assessment of the MODIS Snow Products. D K Hall, G A Riggs, 10.1002/hyp.6715Hydrol. Process. 21Hall, D.K.; Riggs, G.A. Accuracy Assessment of the MODIS Snow Products. Hydrol. Process. 2007, 21, 1534-1547. [CrossRef]\n\nObject-Oriented Mapping of Landslides Using Random Forests. Remote Sens. Environ. A Stumpf, N Kerle, 10.1016/j.rse.2011.05.013115Stumpf, A.; Kerle, N. Object-Oriented Mapping of Landslides Using Random Forests. Remote Sens. Environ. 2011, 115, 2564-2577. [CrossRef]\n\nEvaluation of Different Machine Learning Methods and Deep-Learning Convolutional Neural Networks for Landslide Detection. O Ghorbanzadeh, T Blaschke, K Gholamnia, S R Meena, D Tiede, J Aryal, 10.3390/rs11020196Remote Sens. 2019, 11, 196. [CrossRefGhorbanzadeh, O.; Blaschke, T.; Gholamnia, K.; Meena, S.R.; Tiede, D.; Aryal, J. Evaluation of Different Machine Learning Methods and Deep-Learning Convolutional Neural Networks for Landslide Detection. Remote Sens. 2019, 11, 196. [CrossRef]\n\nA Comparative Analysis of Pixel-and Object-Based Detection of Landslides from Very High-Resolution Images. R N Keyport, T Oommen, T R Martha, K S Sajinkumar, J S Gierke, 10.1016/j.jag.2017.08.015Int. J. Appl. Earth Obs. Geoinf. 64Keyport, R.N.; Oommen, T.; Martha, T.R.; Sajinkumar, K.S.; Gierke, J.S. A Comparative Analysis of Pixel-and Object-Based Detection of Landslides from Very High-Resolution Images. Int. J. Appl. Earth Obs. Geoinf. 2018, 64, 1-11. [CrossRef]\n\nA Comparison of Pixel-and Object-Based Glacier Classification with Optical Satellite Images. P Rastner, T Bolch, C Notarnicola, F Paul, 10.1109/JSTARS.2013.2274668IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 7Rastner, P.; Bolch, T.; Notarnicola, C.; Paul, F. A Comparison of Pixel-and Object-Based Glacier Classification with Optical Satellite Images. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2014, 7, 853-862. [CrossRef]\n\nAn Automated Method for Surface Ice/Snow Mapping Based on Objects and Pixels from Landsat Imagery in a Mountainous Region. X Wang, X Gao, X Zhang, W Wang, F Yang, 10.3390/rs12030485Remote Sens. 2020, 12, 485. [CrossRefWang, X.; Gao, X.; Zhang, X.; Wang, W.; Yang, F. An Automated Method for Surface Ice/Snow Mapping Based on Objects and Pixels from Landsat Imagery in a Mountainous Region. Remote Sens. 2020, 12, 485. [CrossRef]\n\nDTM and Rainfall-Based Landslide Susceptibility Analysis Using Machine Learning: A Case Study of Lantau Island, Hong Kong. H J Wang, L M Zhang, T Xiao, Proceedings of the The Seventh Asian-Pacific Symposium on Structural Reliability and Its Applications (APSSRA 2020). the The Seventh Asian-Pacific Symposium on Structural Reliability and Its Applications (APSSRA 2020)Tokyo, JapanWang, H.J.; Zhang, L.M.; Xiao, T. DTM and Rainfall-Based Landslide Susceptibility Analysis Using Machine Learning: A Case Study of Lantau Island, Hong Kong. In Proceedings of the The Seventh Asian-Pacific Symposium on Structural Reliability and Its Applications (APSSRA 2020), Tokyo, Japan, 4-7 October 2020.\n\nA Novel Physically-Based Model for Updating Landslide Susceptibility. H J Wang, T Xiao, X Y Li, L L Zhang, L M Zhang, 10.1016/j.enggeo.2019.02.004Eng. Geol. 251Wang, H.J.; Xiao, T.; Li, X.Y.; Zhang, L.L.; Zhang, L.M. A Novel Physically-Based Model for Updating Landslide Susceptibility. Eng. Geol. 2019, 251, 71-80. [CrossRef]\n\nA Data-Driven Fuzzy Model for Prediction of Rockburst. A Rastegarmanesh, M Moosavi, A Kalhor, 10.1080/17499518.2020.1751208Georisk Assess. Manag. Risk Eng. Syst. Geohazards. 15Rastegarmanesh, A.; Moosavi, M.; Kalhor, A. A Data-Driven Fuzzy Model for Prediction of Rockburst. Georisk Assess. Manag. Risk Eng. Syst. Geohazards 2021, 15, 152-164. [CrossRef]\n\nOptimisation of Deep Mixing Technique by Artificial Neural Network Based on Laboratory and Field Experiments. S A A Hosseini, S F F Mojtahedi, H Sadeghi, 10.1080/17499518.2019.1612526Georisk Assess. Manag. Risk Eng. Syst. Geohazards. 14Hosseini, S.A.A.; Mojtahedi, S.F.F.; Sadeghi, H. Optimisation of Deep Mixing Technique by Artificial Neural Network Based on Laboratory and Field Experiments. Georisk Assess. Manag. Risk Eng. Syst. Geohazards 2020, 14, 142-157. [CrossRef]\n\nObject-Based Convolutional Neural Networks for Cloud and Snow Detection in High-Resolution Multispectral Imagers. L Wang, Y Chen, L Tang, R Fan, Y Yao, 10.3390/w1011166610Wang, L.; Chen, Y.; Tang, L.; Fan, R.; Yao, Y. Object-Based Convolutional Neural Networks for Cloud and Snow Detection in High-Resolution Multispectral Imagers. Water 2018, 10, 1666. [CrossRef]\n\nMODIS Fractional Snow Cover Mapping Using Machine Learning Technology in a Mountainous Area. C Liu, X Huang, X Li, T Liang, 10.3390/rs12060962Remote Sens. 2020, 12, 962. [CrossRefLiu, C.; Huang, X.; Li, X.; Liang, T. MODIS Fractional Snow Cover Mapping Using Machine Learning Technology in a Mountainous Area. Remote Sens. 2020, 12, 962. [CrossRef]\n\nSpatial Modeling of Snow Avalanche Using Machine Learning Models and Geo-Environmental Factors: Comparison of Effectiveness in Two Mountain Regions. O Rahmati, O Ghorbanzadeh, T Teimurian, F Mohammadi, J P Tiefenbacher, F Falah, S Pirasteh, P T T Ngo, D T Bui, 10.3390/rs11242995Remote Sens. 2019, 11, 2995. [CrossRefRahmati, O.; Ghorbanzadeh, O.; Teimurian, T.; Mohammadi, F.; Tiefenbacher, J.P.; Falah, F.; Pirasteh, S.; Ngo, P.T.T.; Bui, D.T. Spatial Modeling of Snow Avalanche Using Machine Learning Models and Geo-Environmental Factors: Comparison of Effectiveness in Two Mountain Regions. Remote Sens. 2019, 11, 2995. [CrossRef]\n\nWet and Dry Snow Detection Using Sentinel-1 SAR Data for Mountainous Areas with a Machine Learning Technique. Y.-L S Tsai, A Dietz, N Oppelt, C Kuenzer, 10.3390/rs11080895Remote Sens. 2019, 11, 895. [CrossRefTsai, Y.-L.S.; Dietz, A.; Oppelt, N.; Kuenzer, C. Wet and Dry Snow Detection Using Sentinel-1 SAR Data for Mountainous Areas with a Machine Learning Technique. Remote Sens. 2019, 11, 895. [CrossRef]\n\nHigh-Resolution Cubesat Imagery and Machine Learning for Detailed Snow-Covered Area. A F Cannistra, D E Shean, N C Cristea, 10.1016/j.rse.2021.112399Remote Sens. Environ. 2021, 258, 112399. [CrossRefCannistra, A.F.; Shean, D.E.; Cristea, N.C. High-Resolution Cubesat Imagery and Machine Learning for Detailed Snow-Covered Area. Remote Sens. Environ. 2021, 258, 112399. [CrossRef]\n\nSnow Cover Estimation from MODIS and Sentinel-1 SAR Data Using Machine Learning Algorithms in the Western Part of the Tianshan Mountains. Y Liu, X Chen, J.-S Hao, L.-H Li, 10.1007/s11629-019-5723-1J. Mt. Sci. 2020Liu, Y.; Chen, X.; Hao, J.-S.; Li, L.-H. Snow Cover Estimation from MODIS and Sentinel-1 SAR Data Using Machine Learning Algorithms in the Western Part of the Tianshan Mountains. J. Mt. Sci. 2020, 17, 884-897. [CrossRef]\n\nSuper-Resolving Multiresolution Images with Band-Independent Geometry of Multispectral Pixels. N Brodu, 10.1109/TGRS.2017.2694881IEEE Trans. Geosci. Remote Sens. 55Brodu, N. Super-Resolving Multiresolution Images with Band-Independent Geometry of Multispectral Pixels. IEEE Trans. Geosci. Remote Sens. 2017, 55, 4610-4617. [CrossRef]\n\nDigital Terrain Modelling: A Review of Hydrological. I D Moore, R B Grayson, A R Ladson, 10.1002/hyp.3360050103Geomorphological, and Biological Applications. Hydrol. Process. 5Moore, I.D.; Grayson, R.B.; Ladson, A.R. Digital Terrain Modelling: A Review of Hydrological, Geomorphological, and Biological Applications. Hydrol. Process. 1991, 5, 3-30. [CrossRef]\n\nDetection of Forest Harvest Type Using Multiple Dates of Landsat TM Imagery. E H Wilson, S A Sader, 10.1016/S0034-4257(01)00318-2Remote Sens. Environ. 80Wilson, E.H.; Sader, S.A. Detection of Forest Harvest Type Using Multiple Dates of Landsat TM Imagery. Remote Sens. Environ. 2002, 80, 385-396. [CrossRef]\n\nModification of Normalised Difference Water Index (NDWI) to Enhance Open Water Features in Remotely Sensed Imagery. H Q Xu, 10.1080/01431160600589179Int. J. Remote Sens. 27Xu, H.Q. Modification of Normalised Difference Water Index (NDWI) to Enhance Open Water Features in Remotely Sensed Imagery. Int. J. Remote Sens. 2006, 27, 3025-3033. [CrossRef]\n\nQuick Shift and Kernel Methods for Mode Seeking. A Vedaldi, S Soatto, Computer Vision-ECCV 2008, Proceedings of the European Conference on Computer Vision. Marseille, France; Berlin/Heidelberg, GermanySpringerVedaldi, A.; Soatto, S. Quick Shift and Kernel Methods for Mode Seeking. In Computer Vision-ECCV 2008, Proceedings of the European Conference on Computer Vision, Marseille, France, 23-28 August 2008; Springer: Berlin/Heidelberg, Germany, 2008; pp. 705-718.\n\nStatistical and Structural Approaches to Texture. R M Haralick, 10.1109/PROC.1979.11328Proc. IEEE 1979. IEEE 197967Haralick, R.M. Statistical and Structural Approaches to Texture. Proc. IEEE 1979, 67, 786-804. [CrossRef]\n\n. H Wang, L Zhang, K Yin, H Luo, Li, 10.1016/j.gsf.2020.02.012J. Landslide Identification Using Machine Learning. Geosci. Front. 2021. 12Wang, H.; Zhang, L.; Yin, K.; Luo, H.; Li, J. Landslide Identification Using Machine Learning. Geosci. Front. 2021, 12, 351-364. [CrossRef]\n\nAI-Powered Landslide Susceptibility Assessment in Hong Kong. H J Wang, L M Zhang, H Y Luo, J He, R W Cheung, 10.1016/j.enggeo.2021.106103Eng. Geol. 2021, 288, 106103. [CrossRefWang, H.J.; Zhang, L.M.; Luo, H.Y.; He, J.; Cheung, R.W.M. AI-Powered Landslide Susceptibility Assessment in Hong Kong. Eng. Geol. 2021, 288, 106103. [CrossRef]\n\nAn Introduction to Statistical Learning. G James, D Witten, T Hastie, R Tibshirani, SpringerNew York, NY, USAJames, G.; Witten, D.; Hastie, T.; Tibshirani, R. An Introduction to Statistical Learning; Springer: New York, NY, USA, 2013.\n\nDeep Learning. I Goodfellow, Y Bengio, A Courville, MIT PressLondon, UKGoodfellow, I.; Bengio, Y.; Courville, A. Deep Learning; MIT Press: London, UK, 2016.\n", "annotations": {"author": "[{\"end\":253,\"start\":98},{\"end\":383,\"start\":254},{\"end\":527,\"start\":384},{\"end\":653,\"start\":528},{\"end\":782,\"start\":654},{\"end\":915,\"start\":783},{\"end\":1045,\"start\":916}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":105},{\"end\":265,\"start\":260},{\"end\":392,\"start\":388},{\"end\":535,\"start\":533},{\"end\":664,\"start\":661},{\"end\":797,\"start\":790},{\"end\":927,\"start\":923}]", "author_first_name": "[{\"end\":104,\"start\":98},{\"end\":259,\"start\":254},{\"end\":387,\"start\":384},{\"end\":532,\"start\":528},{\"end\":660,\"start\":654},{\"end\":789,\"start\":783},{\"end\":922,\"start\":916}]", "author_affiliation": "[{\"end\":252,\"start\":137},{\"end\":382,\"start\":267},{\"end\":526,\"start\":411},{\"end\":652,\"start\":537},{\"end\":781,\"start\":666},{\"end\":914,\"start\":799},{\"end\":1044,\"start\":929}]", "title": "[{\"end\":68,\"start\":1},{\"end\":1113,\"start\":1046}]", "venue": null, "abstract": "[{\"end\":3432,\"start\":1836}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3494,\"start\":3491},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3664,\"start\":3661},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3681,\"start\":3678},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3860,\"start\":3857},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4127,\"start\":4124},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4247,\"start\":4244},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4250,\"start\":4247},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4253,\"start\":4250},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4256,\"start\":4253},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4515,\"start\":4511},{\"end\":4578,\"start\":4555},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4848,\"start\":4845},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4940,\"start\":4936},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5926,\"start\":5922},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8111,\"start\":8107},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8257,\"start\":8253},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8332,\"start\":8328},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9801,\"start\":9797},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13297,\"start\":13293},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14306,\"start\":14302},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14309,\"start\":14306},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14402,\"start\":14398},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14866,\"start\":14862},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27057,\"start\":27053},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27112,\"start\":27093},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27218,\"start\":27215},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27722,\"start\":27703},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":42489,\"start\":42486}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33911,\"start\":33670},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34119,\"start\":33912},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34709,\"start\":34120},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34798,\"start\":34710},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34945,\"start\":34799},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35093,\"start\":34946},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35178,\"start\":35094},{\"attributes\":{\"id\":\"fig_7\"},\"end\":35679,\"start\":35179},{\"attributes\":{\"id\":\"fig_8\"},\"end\":35746,\"start\":35680},{\"attributes\":{\"id\":\"fig_9\"},\"end\":35935,\"start\":35747},{\"attributes\":{\"id\":\"fig_10\"},\"end\":36557,\"start\":35936},{\"attributes\":{\"id\":\"fig_11\"},\"end\":37139,\"start\":36558},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37954,\"start\":37140},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38297,\"start\":37955},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39686,\"start\":38298},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41037,\"start\":39687},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41522,\"start\":41038},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":42467,\"start\":41523}]", "paragraph": "[{\"end\":3983,\"start\":3448},{\"end\":4946,\"start\":3985},{\"end\":8459,\"start\":4955},{\"end\":8963,\"start\":8570},{\"end\":9630,\"start\":9028},{\"end\":10345,\"start\":9653},{\"end\":10868,\"start\":10409},{\"end\":11677,\"start\":10870},{\"end\":12622,\"start\":11721},{\"end\":13826,\"start\":12741},{\"end\":14310,\"start\":13865},{\"end\":14674,\"start\":14312},{\"end\":15412,\"start\":14676},{\"end\":15843,\"start\":15459},{\"end\":17030,\"start\":15972},{\"end\":20333,\"start\":17094},{\"end\":24476,\"start\":20380},{\"end\":25499,\"start\":24530},{\"end\":26568,\"start\":25568},{\"end\":30930,\"start\":26624},{\"end\":31486,\"start\":30946},{\"end\":32010,\"start\":31493},{\"end\":32341,\"start\":32017},{\"end\":32806,\"start\":32348},{\"end\":33469,\"start\":32813},{\"end\":33669,\"start\":33476}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8524,\"start\":8460},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8569,\"start\":8524},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10408,\"start\":10346},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12740,\"start\":12623},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15971,\"start\":15844}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6544,\"start\":6537},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17233,\"start\":17226},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18368,\"start\":18361},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19438,\"start\":19431},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19494,\"start\":19487},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20403,\"start\":20396},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21559,\"start\":21552},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24926,\"start\":24919},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26359,\"start\":26352}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3446,\"start\":3434},{\"attributes\":{\"n\":\"2.2.\"},\"end\":4953,\"start\":4949},{\"attributes\":{\"n\":\"2.3.\"},\"end\":9026,\"start\":8966},{\"attributes\":{\"n\":\"2.3.1.\"},\"end\":9651,\"start\":9633},{\"attributes\":{\"n\":\"2.3.2.\"},\"end\":11719,\"start\":11680},{\"attributes\":{\"n\":\"2.3.3.\"},\"end\":13863,\"start\":13829},{\"attributes\":{\"n\":\"2.3.4.\"},\"end\":15457,\"start\":15415},{\"attributes\":{\"n\":\"3.\"},\"end\":17040,\"start\":17033},{\"attributes\":{\"n\":\"3.1.\"},\"end\":17092,\"start\":17043},{\"attributes\":{\"n\":\"3.2.\"},\"end\":20378,\"start\":20336},{\"attributes\":{\"n\":\"3.3.\"},\"end\":24528,\"start\":24479},{\"attributes\":{\"n\":\"4.\"},\"end\":25512,\"start\":25502},{\"attributes\":{\"n\":\"4.1.\"},\"end\":25566,\"start\":25515},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26622,\"start\":26571},{\"attributes\":{\"n\":\"5.\"},\"end\":30944,\"start\":30933},{\"end\":31491,\"start\":31489},{\"end\":32015,\"start\":32013},{\"end\":32346,\"start\":32344},{\"end\":32811,\"start\":32809},{\"end\":33474,\"start\":33472},{\"end\":33681,\"start\":33671},{\"end\":33923,\"start\":33913},{\"end\":34131,\"start\":34121},{\"end\":34721,\"start\":34711},{\"end\":34810,\"start\":34800},{\"end\":34957,\"start\":34947},{\"end\":35105,\"start\":35095},{\"end\":35190,\"start\":35180},{\"end\":35691,\"start\":35681},{\"end\":35759,\"start\":35748},{\"end\":35948,\"start\":35937},{\"end\":36570,\"start\":36559},{\"end\":37150,\"start\":37141},{\"end\":37965,\"start\":37956},{\"end\":38308,\"start\":38299},{\"end\":39697,\"start\":39688},{\"end\":41048,\"start\":41039}]", "table": "[{\"end\":37954,\"start\":37244},{\"end\":38297,\"start\":38028},{\"end\":39686,\"start\":38478},{\"end\":41037,\"start\":40208},{\"end\":41522,\"start\":41136},{\"end\":42467,\"start\":42462}]", "figure_caption": "[{\"end\":33911,\"start\":33683},{\"end\":34119,\"start\":33925},{\"end\":34709,\"start\":34133},{\"end\":34798,\"start\":34723},{\"end\":34945,\"start\":34812},{\"end\":35093,\"start\":34959},{\"end\":35178,\"start\":35107},{\"end\":35679,\"start\":35192},{\"end\":35746,\"start\":35693},{\"end\":35935,\"start\":35762},{\"end\":36557,\"start\":35951},{\"end\":37139,\"start\":36573},{\"end\":37244,\"start\":37152},{\"end\":38028,\"start\":37967},{\"end\":38478,\"start\":38310},{\"end\":40208,\"start\":39699},{\"end\":41136,\"start\":41050},{\"end\":42462,\"start\":41525}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5106,\"start\":5095},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6716,\"start\":6707},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6867,\"start\":6858},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6952,\"start\":6943},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7456,\"start\":7448},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":7891,\"start\":7881},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8041,\"start\":8031},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8061,\"start\":8051},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8105,\"start\":8096},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8133,\"start\":8124},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8195,\"start\":8186},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8251,\"start\":8242},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8326,\"start\":8317},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":9247,\"start\":9239},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11509,\"start\":11500},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11826,\"start\":11817},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":17661,\"start\":17653},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19297,\"start\":19287},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19539,\"start\":19529},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20520,\"start\":20512},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20953,\"start\":20945},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21715,\"start\":21707},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":23134,\"start\":23126},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":23272,\"start\":23263},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":23866,\"start\":23857},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":24108,\"start\":24099},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":24325,\"start\":24317},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25048,\"start\":25040},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26015,\"start\":26003},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26088,\"start\":26078},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26183,\"start\":26173},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27330,\"start\":27320},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28082,\"start\":28072},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28792,\"start\":28783},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29322,\"start\":29312},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29580,\"start\":29570},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29592,\"start\":29582},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29952,\"start\":29942},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30148,\"start\":30138}]", "bib_author_first_name": "[{\"end\":42716,\"start\":42715},{\"end\":42725,\"start\":42724},{\"end\":42737,\"start\":42736},{\"end\":42747,\"start\":42746},{\"end\":42757,\"start\":42756},{\"end\":42765,\"start\":42764},{\"end\":42767,\"start\":42766},{\"end\":43090,\"start\":43086},{\"end\":43100,\"start\":43099},{\"end\":43102,\"start\":43101},{\"end\":43113,\"start\":43112},{\"end\":43122,\"start\":43121},{\"end\":43133,\"start\":43132},{\"end\":43144,\"start\":43143},{\"end\":43553,\"start\":43552},{\"end\":43564,\"start\":43563},{\"end\":43576,\"start\":43575},{\"end\":43578,\"start\":43577},{\"end\":43586,\"start\":43585},{\"end\":43595,\"start\":43594},{\"end\":43604,\"start\":43603},{\"end\":43614,\"start\":43613},{\"end\":43626,\"start\":43625},{\"end\":43628,\"start\":43627},{\"end\":43641,\"start\":43640},{\"end\":43651,\"start\":43650},{\"end\":43653,\"start\":43652},{\"end\":44021,\"start\":44020},{\"end\":44030,\"start\":44029},{\"end\":44042,\"start\":44041},{\"end\":44051,\"start\":44050},{\"end\":44066,\"start\":44065},{\"end\":44081,\"start\":44080},{\"end\":44090,\"start\":44089},{\"end\":44432,\"start\":44431},{\"end\":44440,\"start\":44439},{\"end\":44447,\"start\":44446},{\"end\":44456,\"start\":44455},{\"end\":44799,\"start\":44798},{\"end\":44808,\"start\":44807},{\"end\":44817,\"start\":44816},{\"end\":44826,\"start\":44825},{\"end\":44833,\"start\":44832},{\"end\":44840,\"start\":44839},{\"end\":44846,\"start\":44845},{\"end\":45318,\"start\":45317},{\"end\":45327,\"start\":45326},{\"end\":45340,\"start\":45339},{\"end\":45351,\"start\":45350},{\"end\":45736,\"start\":45735},{\"end\":45738,\"start\":45737},{\"end\":45746,\"start\":45745},{\"end\":45748,\"start\":45747},{\"end\":45757,\"start\":45756},{\"end\":45759,\"start\":45758},{\"end\":45773,\"start\":45772},{\"end\":45783,\"start\":45782},{\"end\":45792,\"start\":45791},{\"end\":45801,\"start\":45800},{\"end\":45815,\"start\":45814},{\"end\":45824,\"start\":45823},{\"end\":45834,\"start\":45833},{\"end\":46260,\"start\":46259},{\"end\":46271,\"start\":46270},{\"end\":46284,\"start\":46283},{\"end\":46295,\"start\":46294},{\"end\":46306,\"start\":46305},{\"end\":46625,\"start\":46624},{\"end\":46627,\"start\":46626},{\"end\":46635,\"start\":46634},{\"end\":46637,\"start\":46636},{\"end\":46877,\"start\":46876},{\"end\":46879,\"start\":46878},{\"end\":46887,\"start\":46886},{\"end\":46889,\"start\":46888},{\"end\":47137,\"start\":47136},{\"end\":47147,\"start\":47146},{\"end\":47444,\"start\":47443},{\"end\":47460,\"start\":47459},{\"end\":47472,\"start\":47471},{\"end\":47485,\"start\":47484},{\"end\":47487,\"start\":47486},{\"end\":47496,\"start\":47495},{\"end\":47505,\"start\":47504},{\"end\":47919,\"start\":47918},{\"end\":47921,\"start\":47920},{\"end\":47932,\"start\":47931},{\"end\":47942,\"start\":47941},{\"end\":47944,\"start\":47943},{\"end\":47954,\"start\":47953},{\"end\":47956,\"start\":47955},{\"end\":47970,\"start\":47969},{\"end\":47972,\"start\":47971},{\"end\":48375,\"start\":48374},{\"end\":48386,\"start\":48385},{\"end\":48395,\"start\":48394},{\"end\":48410,\"start\":48409},{\"end\":48838,\"start\":48837},{\"end\":48846,\"start\":48845},{\"end\":48853,\"start\":48852},{\"end\":48862,\"start\":48861},{\"end\":48870,\"start\":48869},{\"end\":49268,\"start\":49267},{\"end\":49270,\"start\":49269},{\"end\":49278,\"start\":49277},{\"end\":49280,\"start\":49279},{\"end\":49289,\"start\":49288},{\"end\":49906,\"start\":49905},{\"end\":49908,\"start\":49907},{\"end\":49916,\"start\":49915},{\"end\":49924,\"start\":49923},{\"end\":49926,\"start\":49925},{\"end\":49932,\"start\":49931},{\"end\":49934,\"start\":49933},{\"end\":49943,\"start\":49942},{\"end\":49945,\"start\":49944},{\"end\":50219,\"start\":50218},{\"end\":50237,\"start\":50236},{\"end\":50248,\"start\":50247},{\"end\":50630,\"start\":50629},{\"end\":50634,\"start\":50631},{\"end\":50646,\"start\":50645},{\"end\":50650,\"start\":50647},{\"end\":50663,\"start\":50662},{\"end\":51110,\"start\":51109},{\"end\":51118,\"start\":51117},{\"end\":51126,\"start\":51125},{\"end\":51134,\"start\":51133},{\"end\":51141,\"start\":51140},{\"end\":51455,\"start\":51454},{\"end\":51462,\"start\":51461},{\"end\":51471,\"start\":51470},{\"end\":51477,\"start\":51476},{\"end\":51861,\"start\":51860},{\"end\":51872,\"start\":51871},{\"end\":51888,\"start\":51887},{\"end\":51901,\"start\":51900},{\"end\":51914,\"start\":51913},{\"end\":51916,\"start\":51915},{\"end\":51932,\"start\":51931},{\"end\":51941,\"start\":51940},{\"end\":51953,\"start\":51952},{\"end\":51957,\"start\":51954},{\"end\":51964,\"start\":51963},{\"end\":51966,\"start\":51965},{\"end\":52461,\"start\":52457},{\"end\":52463,\"start\":52462},{\"end\":52471,\"start\":52470},{\"end\":52480,\"start\":52479},{\"end\":52490,\"start\":52489},{\"end\":52841,\"start\":52840},{\"end\":52843,\"start\":52842},{\"end\":52856,\"start\":52855},{\"end\":52858,\"start\":52857},{\"end\":52867,\"start\":52866},{\"end\":52869,\"start\":52868},{\"end\":53275,\"start\":53274},{\"end\":53282,\"start\":53281},{\"end\":53293,\"start\":53289},{\"end\":53303,\"start\":53299},{\"end\":53667,\"start\":53666},{\"end\":53960,\"start\":53959},{\"end\":53962,\"start\":53961},{\"end\":53971,\"start\":53970},{\"end\":53973,\"start\":53972},{\"end\":53984,\"start\":53983},{\"end\":53986,\"start\":53985},{\"end\":54345,\"start\":54344},{\"end\":54347,\"start\":54346},{\"end\":54357,\"start\":54356},{\"end\":54359,\"start\":54358},{\"end\":54693,\"start\":54692},{\"end\":54695,\"start\":54694},{\"end\":54977,\"start\":54976},{\"end\":54988,\"start\":54987},{\"end\":55445,\"start\":55444},{\"end\":55447,\"start\":55446},{\"end\":55619,\"start\":55618},{\"end\":55627,\"start\":55626},{\"end\":55636,\"start\":55635},{\"end\":55643,\"start\":55642},{\"end\":55956,\"start\":55955},{\"end\":55958,\"start\":55957},{\"end\":55966,\"start\":55965},{\"end\":55968,\"start\":55967},{\"end\":55977,\"start\":55976},{\"end\":55979,\"start\":55978},{\"end\":55986,\"start\":55985},{\"end\":55992,\"start\":55991},{\"end\":55994,\"start\":55993},{\"end\":56274,\"start\":56273},{\"end\":56283,\"start\":56282},{\"end\":56293,\"start\":56292},{\"end\":56303,\"start\":56302},{\"end\":56484,\"start\":56483},{\"end\":56498,\"start\":56497},{\"end\":56508,\"start\":56507}]", "bib_author_last_name": "[{\"end\":42722,\"start\":42717},{\"end\":42734,\"start\":42726},{\"end\":42744,\"start\":42738},{\"end\":42754,\"start\":42748},{\"end\":42762,\"start\":42758},{\"end\":42773,\"start\":42768},{\"end\":43097,\"start\":43091},{\"end\":43110,\"start\":43103},{\"end\":43119,\"start\":43114},{\"end\":43130,\"start\":43123},{\"end\":43141,\"start\":43134},{\"end\":43151,\"start\":43145},{\"end\":43561,\"start\":43554},{\"end\":43573,\"start\":43565},{\"end\":43583,\"start\":43579},{\"end\":43592,\"start\":43587},{\"end\":43601,\"start\":43596},{\"end\":43611,\"start\":43605},{\"end\":43623,\"start\":43615},{\"end\":43638,\"start\":43629},{\"end\":43648,\"start\":43642},{\"end\":43662,\"start\":43654},{\"end\":44027,\"start\":44022},{\"end\":44039,\"start\":44031},{\"end\":44048,\"start\":44043},{\"end\":44063,\"start\":44052},{\"end\":44078,\"start\":44067},{\"end\":44087,\"start\":44082},{\"end\":44097,\"start\":44091},{\"end\":44437,\"start\":44433},{\"end\":44444,\"start\":44441},{\"end\":44453,\"start\":44448},{\"end\":44459,\"start\":44457},{\"end\":44805,\"start\":44800},{\"end\":44814,\"start\":44809},{\"end\":44823,\"start\":44818},{\"end\":44830,\"start\":44827},{\"end\":44837,\"start\":44834},{\"end\":44843,\"start\":44841},{\"end\":44849,\"start\":44847},{\"end\":45324,\"start\":45319},{\"end\":45337,\"start\":45328},{\"end\":45348,\"start\":45341},{\"end\":45358,\"start\":45352},{\"end\":45743,\"start\":45739},{\"end\":45754,\"start\":45749},{\"end\":45770,\"start\":45760},{\"end\":45780,\"start\":45774},{\"end\":45789,\"start\":45784},{\"end\":45798,\"start\":45793},{\"end\":45812,\"start\":45802},{\"end\":45821,\"start\":45816},{\"end\":45831,\"start\":45825},{\"end\":45839,\"start\":45835},{\"end\":46268,\"start\":46261},{\"end\":46281,\"start\":46272},{\"end\":46292,\"start\":46285},{\"end\":46303,\"start\":46296},{\"end\":46314,\"start\":46307},{\"end\":46632,\"start\":46628},{\"end\":46648,\"start\":46638},{\"end\":46654,\"start\":46650},{\"end\":46884,\"start\":46880},{\"end\":46895,\"start\":46890},{\"end\":47144,\"start\":47138},{\"end\":47153,\"start\":47148},{\"end\":47457,\"start\":47445},{\"end\":47469,\"start\":47461},{\"end\":47482,\"start\":47473},{\"end\":47493,\"start\":47488},{\"end\":47502,\"start\":47497},{\"end\":47511,\"start\":47506},{\"end\":47929,\"start\":47922},{\"end\":47939,\"start\":47933},{\"end\":47951,\"start\":47945},{\"end\":47967,\"start\":47957},{\"end\":47979,\"start\":47973},{\"end\":48383,\"start\":48376},{\"end\":48392,\"start\":48387},{\"end\":48407,\"start\":48396},{\"end\":48415,\"start\":48411},{\"end\":48843,\"start\":48839},{\"end\":48850,\"start\":48847},{\"end\":48859,\"start\":48854},{\"end\":48867,\"start\":48863},{\"end\":48875,\"start\":48871},{\"end\":49275,\"start\":49271},{\"end\":49286,\"start\":49281},{\"end\":49294,\"start\":49290},{\"end\":49913,\"start\":49909},{\"end\":49921,\"start\":49917},{\"end\":49929,\"start\":49927},{\"end\":49940,\"start\":49935},{\"end\":49951,\"start\":49946},{\"end\":50234,\"start\":50220},{\"end\":50245,\"start\":50238},{\"end\":50255,\"start\":50249},{\"end\":50643,\"start\":50635},{\"end\":50660,\"start\":50651},{\"end\":50671,\"start\":50664},{\"end\":51115,\"start\":51111},{\"end\":51123,\"start\":51119},{\"end\":51131,\"start\":51127},{\"end\":51138,\"start\":51135},{\"end\":51145,\"start\":51142},{\"end\":51459,\"start\":51456},{\"end\":51468,\"start\":51463},{\"end\":51474,\"start\":51472},{\"end\":51483,\"start\":51478},{\"end\":51869,\"start\":51862},{\"end\":51885,\"start\":51873},{\"end\":51898,\"start\":51889},{\"end\":51911,\"start\":51902},{\"end\":51929,\"start\":51917},{\"end\":51938,\"start\":51933},{\"end\":51950,\"start\":51942},{\"end\":51961,\"start\":51958},{\"end\":51970,\"start\":51967},{\"end\":52468,\"start\":52464},{\"end\":52477,\"start\":52472},{\"end\":52487,\"start\":52481},{\"end\":52498,\"start\":52491},{\"end\":52853,\"start\":52844},{\"end\":52864,\"start\":52859},{\"end\":52877,\"start\":52870},{\"end\":53279,\"start\":53276},{\"end\":53287,\"start\":53283},{\"end\":53297,\"start\":53294},{\"end\":53306,\"start\":53304},{\"end\":53673,\"start\":53668},{\"end\":53968,\"start\":53963},{\"end\":53981,\"start\":53974},{\"end\":53993,\"start\":53987},{\"end\":54354,\"start\":54348},{\"end\":54365,\"start\":54360},{\"end\":54698,\"start\":54696},{\"end\":54985,\"start\":54978},{\"end\":54995,\"start\":54989},{\"end\":55456,\"start\":55448},{\"end\":55624,\"start\":55620},{\"end\":55633,\"start\":55628},{\"end\":55640,\"start\":55637},{\"end\":55647,\"start\":55644},{\"end\":55651,\"start\":55649},{\"end\":55963,\"start\":55959},{\"end\":55974,\"start\":55969},{\"end\":55983,\"start\":55980},{\"end\":55989,\"start\":55987},{\"end\":56001,\"start\":55995},{\"end\":56280,\"start\":56275},{\"end\":56290,\"start\":56284},{\"end\":56300,\"start\":56294},{\"end\":56314,\"start\":56304},{\"end\":56495,\"start\":56485},{\"end\":56505,\"start\":56499},{\"end\":56518,\"start\":56509}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":191510460},\"end\":42940,\"start\":42678},{\"attributes\":{\"doi\":\"10.3390/rs8060481\",\"id\":\"b1\"},\"end\":43465,\"start\":42942},{\"attributes\":{\"doi\":\"10.1038/s41893-019-0305-3\",\"id\":\"b2\",\"matched_paper_id\":199110415},\"end\":43970,\"start\":43467},{\"attributes\":{\"doi\":\"10.1016/j.crm.2018.03.001\",\"id\":\"b3\",\"matched_paper_id\":135037553},\"end\":44331,\"start\":43972},{\"attributes\":{\"doi\":\"10.1007/s40333-015-0044-x\",\"id\":\"b4\",\"matched_paper_id\":128814497},\"end\":44677,\"start\":44333},{\"attributes\":{\"doi\":\"10.1016/j.scitotenv.2018.10.128\",\"id\":\"b5\",\"matched_paper_id\":53744788},\"end\":45142,\"start\":44679},{\"attributes\":{\"doi\":\"10.5194/tc-12-1629-2018\",\"id\":\"b6\"},\"end\":45638,\"start\":45144},{\"attributes\":{\"id\":\"b7\"},\"end\":46154,\"start\":45640},{\"attributes\":{\"doi\":\"10.5194/essd-11-493-2019\",\"id\":\"b8\",\"matched_paper_id\":197547754},\"end\":46585,\"start\":46156},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":130801931},\"end\":46826,\"start\":46587},{\"attributes\":{\"doi\":\"10.1002/hyp.6715\",\"id\":\"b10\",\"matched_paper_id\":16797236},\"end\":47052,\"start\":46828},{\"attributes\":{\"doi\":\"10.1016/j.rse.2011.05.013\",\"id\":\"b11\"},\"end\":47319,\"start\":47054},{\"attributes\":{\"doi\":\"10.3390/rs11020196\",\"id\":\"b12\"},\"end\":47809,\"start\":47321},{\"attributes\":{\"doi\":\"10.1016/j.jag.2017.08.015\",\"id\":\"b13\",\"matched_paper_id\":33241648},\"end\":48279,\"start\":47811},{\"attributes\":{\"doi\":\"10.1109/JSTARS.2013.2274668\",\"id\":\"b14\",\"matched_paper_id\":30456625},\"end\":48712,\"start\":48281},{\"attributes\":{\"doi\":\"10.3390/rs12030485\",\"id\":\"b15\"},\"end\":49142,\"start\":48714},{\"attributes\":{\"id\":\"b16\"},\"end\":49833,\"start\":49144},{\"attributes\":{\"doi\":\"10.1016/j.enggeo.2019.02.004\",\"id\":\"b17\",\"matched_paper_id\":134760908},\"end\":50161,\"start\":49835},{\"attributes\":{\"doi\":\"10.1080/17499518.2020.1751208\",\"id\":\"b18\",\"matched_paper_id\":216356088},\"end\":50517,\"start\":50163},{\"attributes\":{\"doi\":\"10.1080/17499518.2019.1612526\",\"id\":\"b19\",\"matched_paper_id\":164570355},\"end\":50993,\"start\":50519},{\"attributes\":{\"doi\":\"10.3390/w10111666\",\"id\":\"b20\"},\"end\":51359,\"start\":50995},{\"attributes\":{\"doi\":\"10.3390/rs12060962\",\"id\":\"b21\"},\"end\":51709,\"start\":51361},{\"attributes\":{\"doi\":\"10.3390/rs11242995\",\"id\":\"b22\"},\"end\":52345,\"start\":51711},{\"attributes\":{\"doi\":\"10.3390/rs11080895\",\"id\":\"b23\"},\"end\":52753,\"start\":52347},{\"attributes\":{\"doi\":\"10.1016/j.rse.2021.112399\",\"id\":\"b24\"},\"end\":53134,\"start\":52755},{\"attributes\":{\"doi\":\"10.1007/s11629-019-5723-1\",\"id\":\"b25\",\"matched_paper_id\":215725347},\"end\":53569,\"start\":53136},{\"attributes\":{\"doi\":\"10.1109/TGRS.2017.2694881\",\"id\":\"b26\",\"matched_paper_id\":9128000},\"end\":53904,\"start\":53571},{\"attributes\":{\"doi\":\"10.1002/hyp.3360050103\",\"id\":\"b27\",\"matched_paper_id\":14356434},\"end\":54265,\"start\":53906},{\"attributes\":{\"doi\":\"10.1016/S0034-4257(01)00318-2\",\"id\":\"b28\",\"matched_paper_id\":129740661},\"end\":54574,\"start\":54267},{\"attributes\":{\"doi\":\"10.1080/01431160600589179\",\"id\":\"b29\",\"matched_paper_id\":55700035},\"end\":54925,\"start\":54576},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":8489515},\"end\":55392,\"start\":54927},{\"attributes\":{\"doi\":\"10.1109/PROC.1979.11328\",\"id\":\"b31\",\"matched_paper_id\":1990278},\"end\":55614,\"start\":55394},{\"attributes\":{\"doi\":\"10.1016/j.gsf.2020.02.012\",\"id\":\"b32\"},\"end\":55892,\"start\":55616},{\"attributes\":{\"doi\":\"10.1016/j.enggeo.2021.106103\",\"id\":\"b33\"},\"end\":56230,\"start\":55894},{\"attributes\":{\"id\":\"b34\"},\"end\":56466,\"start\":56232},{\"attributes\":{\"id\":\"b35\"},\"end\":56624,\"start\":56468}]", "bib_title": "[{\"end\":42713,\"start\":42678},{\"end\":43550,\"start\":43467},{\"end\":44018,\"start\":43972},{\"end\":44429,\"start\":44333},{\"end\":44796,\"start\":44679},{\"end\":45733,\"start\":45640},{\"end\":46257,\"start\":46156},{\"end\":46622,\"start\":46587},{\"end\":46874,\"start\":46828},{\"end\":47916,\"start\":47811},{\"end\":48372,\"start\":48281},{\"end\":49265,\"start\":49144},{\"end\":49903,\"start\":49835},{\"end\":50216,\"start\":50163},{\"end\":50627,\"start\":50519},{\"end\":53272,\"start\":53136},{\"end\":53664,\"start\":53571},{\"end\":53957,\"start\":53906},{\"end\":54342,\"start\":54267},{\"end\":54690,\"start\":54576},{\"end\":54974,\"start\":54927},{\"end\":55442,\"start\":55394}]", "bib_author": "[{\"end\":42724,\"start\":42715},{\"end\":42736,\"start\":42724},{\"end\":42746,\"start\":42736},{\"end\":42756,\"start\":42746},{\"end\":42764,\"start\":42756},{\"end\":42775,\"start\":42764},{\"end\":43099,\"start\":43086},{\"end\":43112,\"start\":43099},{\"end\":43121,\"start\":43112},{\"end\":43132,\"start\":43121},{\"end\":43143,\"start\":43132},{\"end\":43153,\"start\":43143},{\"end\":43563,\"start\":43552},{\"end\":43575,\"start\":43563},{\"end\":43585,\"start\":43575},{\"end\":43594,\"start\":43585},{\"end\":43603,\"start\":43594},{\"end\":43613,\"start\":43603},{\"end\":43625,\"start\":43613},{\"end\":43640,\"start\":43625},{\"end\":43650,\"start\":43640},{\"end\":43664,\"start\":43650},{\"end\":44029,\"start\":44020},{\"end\":44041,\"start\":44029},{\"end\":44050,\"start\":44041},{\"end\":44065,\"start\":44050},{\"end\":44080,\"start\":44065},{\"end\":44089,\"start\":44080},{\"end\":44099,\"start\":44089},{\"end\":44439,\"start\":44431},{\"end\":44446,\"start\":44439},{\"end\":44455,\"start\":44446},{\"end\":44461,\"start\":44455},{\"end\":44807,\"start\":44798},{\"end\":44816,\"start\":44807},{\"end\":44825,\"start\":44816},{\"end\":44832,\"start\":44825},{\"end\":44839,\"start\":44832},{\"end\":44845,\"start\":44839},{\"end\":44851,\"start\":44845},{\"end\":45326,\"start\":45317},{\"end\":45339,\"start\":45326},{\"end\":45350,\"start\":45339},{\"end\":45360,\"start\":45350},{\"end\":45745,\"start\":45735},{\"end\":45756,\"start\":45745},{\"end\":45772,\"start\":45756},{\"end\":45782,\"start\":45772},{\"end\":45791,\"start\":45782},{\"end\":45800,\"start\":45791},{\"end\":45814,\"start\":45800},{\"end\":45823,\"start\":45814},{\"end\":45833,\"start\":45823},{\"end\":45841,\"start\":45833},{\"end\":46270,\"start\":46259},{\"end\":46283,\"start\":46270},{\"end\":46294,\"start\":46283},{\"end\":46305,\"start\":46294},{\"end\":46316,\"start\":46305},{\"end\":46634,\"start\":46624},{\"end\":46650,\"start\":46634},{\"end\":46656,\"start\":46650},{\"end\":46886,\"start\":46876},{\"end\":46897,\"start\":46886},{\"end\":47146,\"start\":47136},{\"end\":47155,\"start\":47146},{\"end\":47459,\"start\":47443},{\"end\":47471,\"start\":47459},{\"end\":47484,\"start\":47471},{\"end\":47495,\"start\":47484},{\"end\":47504,\"start\":47495},{\"end\":47513,\"start\":47504},{\"end\":47931,\"start\":47918},{\"end\":47941,\"start\":47931},{\"end\":47953,\"start\":47941},{\"end\":47969,\"start\":47953},{\"end\":47981,\"start\":47969},{\"end\":48385,\"start\":48374},{\"end\":48394,\"start\":48385},{\"end\":48409,\"start\":48394},{\"end\":48417,\"start\":48409},{\"end\":48845,\"start\":48837},{\"end\":48852,\"start\":48845},{\"end\":48861,\"start\":48852},{\"end\":48869,\"start\":48861},{\"end\":48877,\"start\":48869},{\"end\":49277,\"start\":49267},{\"end\":49288,\"start\":49277},{\"end\":49296,\"start\":49288},{\"end\":49915,\"start\":49905},{\"end\":49923,\"start\":49915},{\"end\":49931,\"start\":49923},{\"end\":49942,\"start\":49931},{\"end\":49953,\"start\":49942},{\"end\":50236,\"start\":50218},{\"end\":50247,\"start\":50236},{\"end\":50257,\"start\":50247},{\"end\":50645,\"start\":50629},{\"end\":50662,\"start\":50645},{\"end\":50673,\"start\":50662},{\"end\":51117,\"start\":51109},{\"end\":51125,\"start\":51117},{\"end\":51133,\"start\":51125},{\"end\":51140,\"start\":51133},{\"end\":51147,\"start\":51140},{\"end\":51461,\"start\":51454},{\"end\":51470,\"start\":51461},{\"end\":51476,\"start\":51470},{\"end\":51485,\"start\":51476},{\"end\":51871,\"start\":51860},{\"end\":51887,\"start\":51871},{\"end\":51900,\"start\":51887},{\"end\":51913,\"start\":51900},{\"end\":51931,\"start\":51913},{\"end\":51940,\"start\":51931},{\"end\":51952,\"start\":51940},{\"end\":51963,\"start\":51952},{\"end\":51972,\"start\":51963},{\"end\":52470,\"start\":52457},{\"end\":52479,\"start\":52470},{\"end\":52489,\"start\":52479},{\"end\":52500,\"start\":52489},{\"end\":52855,\"start\":52840},{\"end\":52866,\"start\":52855},{\"end\":52879,\"start\":52866},{\"end\":53281,\"start\":53274},{\"end\":53289,\"start\":53281},{\"end\":53299,\"start\":53289},{\"end\":53308,\"start\":53299},{\"end\":53675,\"start\":53666},{\"end\":53970,\"start\":53959},{\"end\":53983,\"start\":53970},{\"end\":53995,\"start\":53983},{\"end\":54356,\"start\":54344},{\"end\":54367,\"start\":54356},{\"end\":54700,\"start\":54692},{\"end\":54987,\"start\":54976},{\"end\":54997,\"start\":54987},{\"end\":55458,\"start\":55444},{\"end\":55626,\"start\":55618},{\"end\":55635,\"start\":55626},{\"end\":55642,\"start\":55635},{\"end\":55649,\"start\":55642},{\"end\":55653,\"start\":55649},{\"end\":55965,\"start\":55955},{\"end\":55976,\"start\":55965},{\"end\":55985,\"start\":55976},{\"end\":55991,\"start\":55985},{\"end\":56003,\"start\":55991},{\"end\":56282,\"start\":56273},{\"end\":56292,\"start\":56282},{\"end\":56302,\"start\":56292},{\"end\":56316,\"start\":56302},{\"end\":56497,\"start\":56483},{\"end\":56507,\"start\":56497},{\"end\":56520,\"start\":56507}]", "bib_venue": "[{\"end\":42792,\"start\":42775},{\"end\":43084,\"start\":42942},{\"end\":43701,\"start\":43689},{\"end\":44140,\"start\":44124},{\"end\":44498,\"start\":44486},{\"end\":44900,\"start\":44882},{\"end\":45315,\"start\":45144},{\"end\":45873,\"start\":45841},{\"end\":46361,\"start\":46340},{\"end\":46688,\"start\":46656},{\"end\":46928,\"start\":46913},{\"end\":47134,\"start\":47054},{\"end\":47441,\"start\":47321},{\"end\":48037,\"start\":48006},{\"end\":48490,\"start\":48444},{\"end\":48835,\"start\":48714},{\"end\":49411,\"start\":49296},{\"end\":49990,\"start\":49981},{\"end\":50335,\"start\":50286},{\"end\":50751,\"start\":50702},{\"end\":51107,\"start\":50995},{\"end\":51452,\"start\":51361},{\"end\":51858,\"start\":51711},{\"end\":52455,\"start\":52347},{\"end\":52838,\"start\":52755},{\"end\":53343,\"start\":53333},{\"end\":53731,\"start\":53700},{\"end\":54079,\"start\":54017},{\"end\":54416,\"start\":54396},{\"end\":54744,\"start\":54725},{\"end\":55081,\"start\":54997},{\"end\":55496,\"start\":55481},{\"end\":55749,\"start\":55678},{\"end\":55953,\"start\":55894},{\"end\":56271,\"start\":56232},{\"end\":56481,\"start\":56468},{\"end\":49525,\"start\":49413},{\"end\":55128,\"start\":55083},{\"end\":55507,\"start\":55498}]"}}}, "year": 2023, "month": 12, "day": 17}