{"id": 233376633, "updated": "2022-01-14 00:11:27.606", "metadata": {"title": "Revisiting HyperDimensional Learning for FPGA and Low-Power Architectures", "authors": "[{\"middle\":[],\"last\":\"Imani\",\"first\":\"Mohsen\"},{\"middle\":[],\"last\":\"Zou\",\"first\":\"Zhuowen\"},{\"middle\":[],\"last\":\"Bosch\",\"first\":\"Samuel\"},{\"middle\":[\"Anantha\"],\"last\":\"Rao\",\"first\":\"Sanjay\"},{\"middle\":[],\"last\":\"Salamat\",\"first\":\"Sahand\"},{\"middle\":[],\"last\":\"Kumar\",\"first\":\"Venkatesh\"},{\"middle\":[],\"last\":\"Kim\",\"first\":\"Yeseong\"},{\"middle\":[],\"last\":\"Rosing\",\"first\":\"Tajana\"}]", "venue": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "journal": "2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Today\u2019s applications are using machine learning algorithms to analyze the data collected from a swarm of devices on the Internet of Things (IoT). However, most existing learning algorithms are overcomplex to enable real-time learning on IoT devices with limited resources and computing power. Recently, Hyperdimensional computing (HDC) is introduced as an alternative computing paradigm for enabling efficient and robust learning. HDC emulates the cognitive task by representing the values as patterns of neural activity in high-dimensional space. HDC first encodes all data points to high-dimensional vectors. It then efficiently performs the learning task using a well-defined set of operations. Existing HDC solutions have two main issues that hinder their deployments on low-power embedded devices: (i) the encoding module is costly, dominating 80% of the entire training performance, (ii) the HDC model size and the computation cost grow significantly with the number of classes in online inference.In this paper, we proposed a novel architecture, LookHD, which enables real-time HDC learning on low-power edge devices. LookHD exploits computation reuse to memorize the encoding module and simplify its computation with single memory access. LookHD also address the inference scalability by exploiting HDC governing mathematics that compresses the HDC trained model into a single hypervector. We present how the proposed architecture can be implemented on the existing low power architectures: ARM processor and FPGA design. We evaluate the efficiency of the proposed approach on a wide range of practical classification problems such as activity recognition, face recognition, and speech recognition. Our evaluations show that LookHD can achieve, on average, $ 28.3\\times$ faster and $ 97.4\\times$ more energy-efficient training as compared to the state-of-the-art HDC implemented on the FPGA. Similarly, in the inference, LookHD is $ 2.2\\times$ faster, $ 4.1\\times$ more energy-efficient, and has $ 6.3\\times$ smaller model size than the same state-of-the-art algorithms.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/hpca/ImaniZBRSKKR21", "doi": "10.1109/hpca51647.2021.00028"}}, "content": {"source": {"pdf_hash": "59a5d097baa8b31ea886f623869aa2c1f8f27f36", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c9bdfac566ddd702e128c07266789d8828f6ebe0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/59a5d097baa8b31ea886f623869aa2c1f8f27f36.txt", "contents": "\nRevisiting HyperDimensional Learning for FPGA and Low-Power Architectures\n\n\nMohsen Imam&apos;1 &apos;* \nDepartment of Computer Science and Engineering\nUC San Diego\n\n\nDepartment of Electrical Engineering and Computer Science\nDaegu Gyeongbuk Institute of Science and Technology\nMassachusetts Institute of Technology ^Information and Communication Engineering\n\n\nZhuowen Zou \nDepartment of Computer Science and Engineering\nUC San Diego\n\n\nDepartment of Electrical Engineering and Computer Science\nDaegu Gyeongbuk Institute of Science and Technology\nMassachusetts Institute of Technology ^Information and Communication Engineering\n\n\nSamuel Bosch \nDepartment of Computer Science and Engineering\nUC San Diego\n\n\nDepartment of Electrical Engineering and Computer Science\nDaegu Gyeongbuk Institute of Science and Technology\nMassachusetts Institute of Technology ^Information and Communication Engineering\n\n\nAnantha Sanjay \nRao \nDepartment of Computer Science and Engineering\nUC San Diego\n\n\nDepartment of Electrical Engineering and Computer Science\nDaegu Gyeongbuk Institute of Science and Technology\nMassachusetts Institute of Technology ^Information and Communication Engineering\n\n\nSahand Salamat \nDepartment of Computer Science and Engineering\nUC San Diego\n\n\nDepartment of Electrical Engineering and Computer Science\nDaegu Gyeongbuk Institute of Science and Technology\nMassachusetts Institute of Technology ^Information and Communication Engineering\n\n\nVenkatesh Kumar \nDepartment of Computer Science and Engineering\nUC San Diego\n\n\nDepartment of Electrical Engineering and Computer Science\nDaegu Gyeongbuk Institute of Science and Technology\nMassachusetts Institute of Technology ^Information and Communication Engineering\n\n\nYeseong Kim^ \nTajana Rosing \nDepartment of Computer Science and Engineering\nUC San Diego\n\n\nDepartment of Electrical Engineering and Computer Science\nDaegu Gyeongbuk Institute of Science and Technology\nMassachusetts Institute of Technology ^Information and Communication Engineering\n\n\n\nDepartment of Computer Science\nUniversity of California Irvine\n\n\nRevisiting HyperDimensional Learning for FPGA and Low-Power Architectures\n10.1109/HPCA51647.2021.000282021 IEEE International Symposium on High-Performance Computer Architecture (H PC A ) | 978-1-6654-2235-2/20/$31.00\nToday s applications are using machine learning algorithms to analyze the data collected from a swarm of devices on the Internet of Things (IoT). However, most existing learning algorithms are overcomplex to enable real-time learning on IoT devices with limited resources and computing power. Recently, Hyperdimensional computing (HDC) is introduced as an alternative computing paradigm for enabling efficient and robust learning. HDC emulates the cognitive task by representing the values as patterns of neural activity in high-dimensional space. HDC first encodes all data points to high-dimensional vectors. It then efficiently performs the learning task using a well-defined set of operations. Existing HDC solutions have two main issues that hinder their deployments on low-power embedded devices: (i) the encoding module is costly, dominating 80% of the entire training performance, (ii) the HDC model size and the computation cost grow significantly with the number of classes in online inference.In this paper, we proposed a novel architecture, LookHD, which enables real-time HDC learning on low-power edge devices. LookHD exploits computation reuse to memorize the encoding module and simplify its computation with single memory access. LookHD also address the inference scalability by exploiting HDC governing mathematics that compresses the HDC trained model into a single hypervector. We present how the proposed architecture can be implemented on the existing low power architectures: ARM processor and FPGA design. We evaluate the efficiency of the proposed approach on a wide range of practical classification problems such as activity recognition, face recognition, and speech recognition. Our evaluations show that LookHD can achieve, on average, 28.3 x faster and 97.4 x more energy-efficient training as compared to the state-of-the-art HDC implemented on the FPGA. Similarly, in the inference, LookHD is 2.2 x faster, 4.1 x more energy-efficient, and has 6.3 x smaller model size than the same state-of-the-art algorithms.Index Terms-HyprDimensional computing, Brain-inspired computing, Machine learning, Real-time learning, FPGA Recently, several companies started exploiting the HDC ca pability to enable general intelligence in IoT devices, including WebFeet [24], Vicarious [25], Numenta [26], [27], IBM [28], and Google [29]. Most existing researches are focused on exploiting the HDC robustness to design approximate analog\n\nI. I n t r o d u c t io n\n\nWith the emergence of the Internet of Things (IoT), many applications run machine learning algorithms to perform cognitive tasks. The learning algorithms have been shown effectiveness for many tasks, e.g., object tracking [1], speech recognition [2], [3], image classification [4], [5], etc. However, the high computational complexity and memory requirement of existing deep learning algorithms hinder usability to a wide variety of real-life embedded applications where the device resources and power budget is limited [6], [7], [8], [9]. Therefore, we need alternative learning methods that can train on the less-powerful IoT devices, while providing good enough classification accuracy.\n\nTo achieve real-time performance with high energy efficiency, we need to rethink not only how we accelerate machine learning algorithms in hardware, but also to redesign the algorithms themselves using strategies that more closely model the ultimate efficient learning machine: the human brain. Hyperdimensional computing (HDc) is one such strategy developed by interdisciplinary research [10]. It is based on a short-term human memory model, Sparse distributed memory, emerged from theoretical neuroscience [11]. HDC is motivated by the understanding that the human brain operates on high dimensional representations of data originating from the large size of brain circuits [12]. It thereby models the human memory using points of a high-dimensional space, that is, with hyper vectors. The hyperspace typically refers to tens of thousand dimensions. HDC incorporates learning capability along with typical memory functions of storing/loading information. It mimics several essential functionalities of the human memory model with vector operations, which are computationally tractable and mathematically rigorous in describing human cognition.\n\nHDC is well suited to address learning tasks for IoT systems as: (i) HDC models are computationally efficient (highly parallel at heart) to train and amenable to hardware level optimization [13], (ii) HDC offers an intuitive and human-interpretable model [14], [15], [16], (iii) it offers a computational paradigm that can be applied to cognitive as well as learning problems [14], [17], [18], [19], [20], (iv) it provides strong robustness to noise -a key strength for IoT systems, and (v) HDC can naturally enable secure and light-weight learning. These features make HDC a promising solution for today's embedded devices with limited storage, battery, and resources, as well as future computing systems in deep nano-scaled technology, where devices may have high noise and variability [13], [21], [22], [23].\n\naccelerator [21], [22], [23]. However, to make the HDC practical, we need to exploit the HDC robustness on the existing embedded platforms, e.g., CPU and FPGA. As prior work showed, the existing digital platforms can get limited benefit from hardware approximation [30], [31], [32]. The key motivation of our work is to significantly speedup the HDC learning on embedded platforms by redesigning algorithmhardware that accelerates crucial HDC functionalities. HDC performs the learning task after encoding all data points to high-dimensional space. This encoding requires to compute thousands (e.g., 10,000) operations for each element of data in the original domain, e.g., performing permuta tion (rotational shift) and addition of randomly generated bipolar/binary hypervectors [10], [33], [34]. This makes the encoding computationally expensive. our experiments on several practical applications show that the encoding takes about 80% of the total training's execution time. During inference, HDC uses the same encoding module to map a test data point to high-dimensional space (query hypervector). Then, HDC checks the similarity of the query hypervector with all pre-trained class hypervectors. This similarity check takes a major cost of the HDC during the inference, i.e., 83% average performance for all tested applications, as it involves many multiplications that grow with the number of classes [21], [35]. This degrades the scalability of the HDC model and increases the computation cost of applications with many classes.\n\nIn this paper, we propose LookHD, an architectural solution that accelerates HDC in both training and inference, making it significantly efficient for today's embedded processors. LookHD exploits computational reuse to eliminate the costly encoding from the HDC training while addressing the scalabil ity issue of the HDC inference by compressing the model size and reducing the computation costs. The followings summarize the main contributions of the paper:\n\n\u2022 We propose a lookup-based approach that eliminates the costly encoding operations from the HDC. LookHD pre stores all possible encoded values and replaces the costly encoding module with single memory access. Unlike existing HDC algorithms [14], [33], [36] that needs to combine all the encoded hypervectors, LookHD only counts the number of patterns repeated in the pre-stored hypervectors and generates the hypervector model once at the end of the training. This results in eliminating the encoding module, significantly accelerating the HDC training. \u2022 LookHD addresses the scalability issue of the inference phase due to the model size. In contrast to conventional HDC approaches that use multiple hypervectors corresponding to each class, LookHD compresses them into a single hyper vector. The combined hypervector stores the information of all classes in significantly lower-sized memory, which makes it suitable for embedded devices with limited resources. This approach also significantly accelerates the inference by reducing the number of computations, e.g., multiplications. \u2022 We present two architecture options to implement LookHD on embedded devices (with less than 10W power budget). Along with a software framework design for low-power ARM processors, we propose an FPGA implementation that performs both training and inference. The FPGA design utilizes a fully pipelined structure in order to maximize the throughput and FPGA resource utilization. A ll proposed optimizations are general and can be implemented on any digital processor, including an ASIC chip. We evaluate the efficiency of the proposed approach on a wide range of practical learning problems, including speech recognition, activity recognition, and face recognition. FPGAbased LookHD is implemented on Kintex-7 FPGA KC705. Our evaluation shows that LookHD achieves, on average 28.3x faster and 97.4x higher energy efficient training, as compared to the state-of-the-art HDC solution [37], [38] implemented on the FPGA. For the inference tasks, LookHD is 2.2x faster, 4.1 x more energy-efficient, and has 6.3 x smaller model size.\n\nII. HDC: Fu n c t io n a l it y & Ch a l l e n g e s Figure 1a shows an overview of HDC performing the classification task on high-dimensional space. The first step of HDC is to map/encode data points from original into high dimensional space. The encoded hypervectors are combined during training in order to create a single hypervector repre senting each class. In the inference, a test data is encoded to high-dimensional space using the same encoding module used for training. The classification is performed by finding a pre-stored class hypervector, which has the highest similarity with the test hypervector. In the following, we explain the details of the HDC functionality. Figure 1b shows an overview of the HDC encoding module. Let us assume a feature vector F = {f 1, f 2,..., f n }, with n features (fi e N). The goal of encoding is to map a feature vector to a high-dimensional vector, H = {h1, h2,..., hD } with D dimensions (hi e N), where D is in order of thousands, e.g., 10,000 [21], [33], [34]. The encoding keeps the main information of original data as a pattern of values in high dimensional space. HDC represents the feature values as patterns of bitstreams in HDC space and combines them to preserve the position of each pattern [34].\n\n\nA. HD Encoding\n\nAlphabets Generation: Instead of representing the features using their value, HDC represents them using a set of hypervec tors where their patterns determine their values. First, we find the maximum and minimum feature values, {f min, f max}, and then quantize that range into q discrete levels, {f f , f %, , f q }. Then, we assign a single hypervector to each quantized level {L 1,L2,...,Lq }, where L 1, and Lq are corresponding to f min and f max values respectively. The level hypervectors are bipolar with D dimensions, Lj e { -1,1}D . The level hypervectors need to preserve the same similarity distance as the quantized values in the original space. To this end, we randomly generate the first level hypervector (representing f min). The other level hypervectors are generated by filliping D/q random dimensions of the previous level hypervector. Using this method, the Lq hypervector corresponding to f max will be nearly orthogonal to L 1, while the neighbor levels will have high pattern similarity.\n\nPreserve Position: In the feature vector, the information is stored as a pattern of features in different indices. The encoding needs to keep the information of both feature values and their corresponding position. HD preserves each index position by assigning a fixed permutation to it. Based on HDC theory [10], any permutation of a random hypervector will be nearly orthogonal with the original hypervector:\n8 (L ,p (i)L ) -0 (0 < i < n)\nwhere the similarity metric, 8 , is a cosine between the two hypervectors, and p (i)L is the i-bit rotational shift of L . The orthogonality of a hypervector and its permutation (i.e. circular bitwise rotation) is ensured as long as the hypervector dimensionality is large enough compared to the number of features in the original data point (D > > n). As Figure 1 shows, the aggregation of the n binary hypervectors is computed as follows:\nH = L 1 + p L2 + ... + p (n-1)Ln .\n(1) where, H is the (non-binary) aggregation and L i is the (binary) hypervector corresponding to the i-th feature of vector F .\n\n\nB. HD Initial Training & Retraining\n\nIn HDC, the training is performed by the element-wise addition of all encoded hypervectors in each existing class. The result of training will be k hypervectors with D dimensions, where k is the number of classes. For example, ith class hypervector can be computed as: Ci = 'Lyjeclassi Hj After the initial training, HDC revisit the trained model for a few iterations. We call this iterative process as retraining. During a single iteration of the retraining, HDC checks the similarity of all training data points, say H, with the trained model. I f a data is wrongly classified by the model, HDC updates the model by (i) adding the data hypervector to a class that it belongs to (Ccorrect = Ccorrect + H), and (ii) subtracting it from a class which it is wrongly matched with (Cwrong = Cwrong -H). The retraining needs to be continued for a few iterations until the HDC accuracy stabilized over the validation data, which is a part of the training dataset.\n\n\nC. HD Inference\n\nIn the inference, HDC uses the same encoding module as the training module to map a test data point to a query hypervector. In HDC space, the classification task is performed by checking the similarity of the query with all class hypervectors. Each data point is assigned to a class that has the highest similarity with it. Since HDC information is stored as the pattern of non-binary values, the cosine is suitable for similarity check. \n\n\nD. HDC Challenges\n\nTraining Challenges: As a light-weight classifier, HDC often needs to be trained and tested on embedded devices with limited resources. However, we observe that HDC can be computationally costly when it processes the practical classifi cation applications. Figure 2 shows the normalized execution time of HDC during training and inference on five practical applications including: speech recognition (SPEECH) [39], activity recognition (ACTIVITY) [40], physical monitoring (PHYSICAL) [41], face recognition (FACE) [42], and position recognition using extra sensory (EXTRA) [43]. All evaluations are performed on ARM Cortex A53 CPU using C++ imple mentation of HDC. The details of each application, such as the number of features (n) and the number of classes (k) are explained in Section VI-A. Our results show that for practical classification applications with large feature sizes, the encoding module dominates the training execution time. For example, for speech recognition with n = 617 features, the encoding can take 90% of total training time. This reduces the advantage of HDC as a light-weight classifier. In this paper, we propose a novel approach that significantly reduces the encoding cost. Our design ignores performing the encoding operations by pre-storing all possible encoding results. The details of the proposed approach are explained in Section III.\n\nInference Challenges: Figure 2 also shows the breakdown of the HDC execution time during the inference. As our results show, in the inference, the associative search takes the majority of the HDC cost. For five tested applications, the associative search takes about 83% of the total inference execution. This is because the cosine similarity involves a large number of multiplications between a query and class hypervectors. In addition, in existing HDC approaches [21], [33], [44], the model size and the computation cost increase linearly with the number of classes. For example, speech recognition with k = 26 classes has 13.0x larger model size and 5.2x slower inference computation as compared to face detection with k = 2 classes. Since embedded devices often do not have enough memory and computing resources, processing HDC applications with a large number of classes will result in huge computation cost.\n\nIn this paper, we propose a novel approach to addresses the HDC inference scalability. Our solution combines all classes into a single hypervector (regardless of the number of classes) by utilizing the orthogonality of high-dimensional vectors. This method not only reduces the HDC model size but also accelerates the inference by removing the majority  I T HE c h a r a c t e r is t ic o f f iv e p r a c t ic a l a p p l ic a t io n s p r o v id in g MAXIMUM ACCURACY. of multiplication operations from the cosine similarity. In Section IV, we explain the details of the proposed model compression method.\n\n\nIII. L o o k HD En c o d in g a n d Tr a i n i n g\n\nIn HDC, all training and testing (inference) computations hap pen on the encoded data in high-dimensional space. Therefore, HDC needs to pay the cost of encoding for all available data points. Practical classification problems are working with data points with hundreds/thousands of features. This significantly increases the cost of the encoding module.\n\nWhen encoding a data point with n features, as shown in Equation 1, each feature index gets a unique permutation, and each feature value gets one of the possible q quantized levels. Looking at all possible combinations, we can see that the encoded hypervector can get qn different possibilities. In order to avoid the costly encoding operations, one solution is to pre-store all qn possible hypervectors in a memory block. This enables computation reuse as we can simplify the encoding operation to a single lookup table search. Here we look at the feasibility of this approach for HDC. Table I shows the number of features for five practical applications. In addition, for each application, Table I lists the minimum number of quantized levels, which results in maximum classification accuracy. Our evaluation shows that in practical applications, the number of features, n, and the number of quantized levels, q, are much higher than a range that can be stored in reasonable memory size. For an example of speech recognition, each feature vector can be encoded to qn = 16617 different possible D-dimensional hypervectors. To make the lookup-based encoding feasible, in this section, we propose LookHD, which significantly reduces the number of features and the number of quantized levels. LookHD reduces the number of feature values by splitting the feature vector into small chunks, where all chunks can be encoded using the same encoding module. LookHD also proposes a novel quantization approach that enables HDC to provide the maximum classification accuracy using much lower quantization. In the following, we explain the details of the proposed approach.\n\n\nA. Splitting Features\n\nHere, we propose a novel approach which enables HDC to en code a feature vector which has been split into the small chunks. same encoding module. The encoding in the ith chunk is performed as follows:\nHi = L i + p L2 +------+ p r 1 L r (2)\nwhere L g { L 1; L2 ,...,L q} are the same quantized hypervec tors used in conventional encoding.\n\nTo complete the encoding module, we need to combine the encoded hypervectors through all chunks and represent them using a single hypervector. One naive approach is to aggregate the chunk hypervectors by simply adding them together. Although this method keeps the information of all chunks (i.e., the pattern of each individual encoded chunk) in a combined hypervector, it does not preserve the order of combination. In this paper, we exploit the mathematical orthogonality of random vectors in high-dimensional space to combine the chunk hypervectors. To consider the position of each chunk on a combined hypervector, LookHD gen erates m random bipolar hypervectors, { P1,P2 ...,P m} with D dimensions (Pi G { -1,1}D). Since these hypervector are generated randomly, they w ill have have nearly orthogonal distribution [34]:\n8 (Pi, Pj ) -0 (0 < i, j < m, i = j)\nWe preserve the the position of each chunk using:\nH = P1 * H 1 + P2 * H2 ,...,Pm * Hm(3)\nwhere the combined H hypervector stores the information of all chunks as well as their order of combination. However, this combination is not error-free, since the P hypervectors are not entirely orthogonal. The combined hypervector may lose information when we store the information of too many chunks. In Section VI-B, we discuss the impact of the number of chunks on the HDC classification accuracy.\n\n\nB. Quantization Reduction\n\nAlthough splitting the feature vector reduces the number of pre-stored hypervectors from qn to qr, the value of q can still be very large such that it makes the lookup approach infeasible. Looking at the Table I, we can see that applications usually require q = 8 or q = 16 to provide their highest accuracy. For example, speech recognition provides the maximum accuracy using q = 16. Using this quantization level even with tiny chunk size (e.g., r = 5), we still require to pre store 165 = 220 hypervectors, which is still very large for practical implementation. To further reduce the number of possible encoded hypervectors, we need to reduce the number of quantized levels.\n\nThe blue line in Figure 4 shows the impact of reducing the number of quantized levels on speech recognition accuracy. The results show that reducing the number of quantized levels to q = 2 and q = 4 degrades the HDC classification accuracy by 3.4% and 1.5%, respectively. Thus, reducing the q value comes at the expense of a large quality loss. The second approach to reducing the number of possible encoded hypervectors is to use a smaller chunk size (r < < ). However, this degrades the advantage of the lookup approach by increasing the cost of chunk aggregation.\n\nHere, we propose a novel approach that significantly reduces the number of quantized levels with no negative impact on classification accuracy [45]. Figure 3a shows the distribution of feature values, sampling 5% data points from the speech recognition dataset. The graph indicates that the feature values have non-uniform distribution. Therefore, increasing the number of quantizations linearly results in generating the levels that w ill be used rarely during the encoding. Instead of linearly quantizing the feature values, our approach selects the quantization boundaries such that all levels get a similar number of values. Figure 3b shows how our approach quantizes speech recognition feature values into q = 4 equalized levels. Figure 4 compares the classification accuracy of LookHD using conventional and proposed quantization. In conventional quantization, increasing the number of levels may degrade the classification accuracy, while the proposed quantization approach results in similar or better accuracy using larger q. This is because, in linear quantization, the new levels may be assigned to a range that may not be equally used during the classification, making the classification task more complicated. our results also show that the proposed quantization results in much higher accuracy than linear quantization. For example, in speech recognition, the proposed quantization to q = 4 levels provide 1.2% higher classification accuracy than HDC using q = 16 linear quantization.\n\nLookHD learns from highly quantized input data, which is not possible in the original space. HDC encoding is non-linear and projects input data with a small difference to relatively isolated data in sparse high-dimensional space. Due to the high-dimensionality of the HDC encoding module, there are several possibilities that each encoded value can get, thus a small feature difference in original space can be projected to large difference in HDC space.\n\n\nC. Lookup-based Encoding\n\nSplitting the feature vectors along with equalized quantiza tion reduces the number of possible encoded hypervectors. For an example of speech recognition, splitting the feature vector to r = 5 chunk size and using q = 4 equalized quantization reduces the number of possible encoded hypervectors from 16617 to 45, while ensuring the same accuracy as the baseline HD. Figure 5 shows the overview of lookup-based encoding in a single chunk. In the first step, we quantize each input features into one of the possible quantization levels. Then, we access encoding results by searching for r quantized features in a lookup table that pre-stored all possible combinations. In hardware, the lookup table is expensive since it involves many compare/search operations.\n\nTo avoid costly search operation, LookHD assigns a code book to each quantized level, where each codebook represents using log2q bits ( Figure 5). For example, for an application with q = 4 quantization levels {f q i , f%, f3, f }, the quantization levels are assigned to {00,01,10,11} codebooks. This simpli fies the costly lookup search with simple memory access. The concatenation of the codebooks in a chunk is a direct address to a memory row that pre-stored the encoded hypervector. This significantly reduces the cost of encoding in feature chunks. To encode a complete feature vector, LookHD first accesses all chunk hypervectors in parallel; then, it combines them using a set of randomly generated position, P, hypervectors (explained in Equation 3).\n\n\nD. LookHD Training\n\nAlthough LookHD accelerates the encoding, the main advantage of LookHD appears in training. In conventional HDC, training is implemented by sequentially adding the encoded hypervectors. In contrast, LookHD pre-stores all possible encoded hypervectors and counts the number of times that each one occurs during the training. This simplifies the training process to the multiplication of counter values with pre-stored encoded hypervectors once at the end of the training. Figure 6 shows the general structure of LookHD performing the training. The training starts with encoding feature vectors to high dimensional space. LookHD implement encoding by quantizing the feature values (K>). During quantization, each feature value is assigned to the closest quantized levels. This implements by subtracting the feature value from all quantized levels and finding a level with the absolute minimum distance. Depending on the selected level, each feature value is assigned to one of the codebooks ([ b>). The concatenation of the codebooks in a chunk is a direct address to pre-stored encoded hypervector (1 C ). Instead of looking up the encoded hypervector, LookHD assigns a counter to each chunk. The size of the counter is qr , which is the same as the number of pre-stored chunk hypervectors (D ). Thus, each counter row can be accessed using the same codebooks used to access encoded hypervectors. For all data points in a class, LookHD increments the counter values in all chunks without performing the encoding.\n\nAfter covering all data points in a class, LookHD multiplies the counter values to pre-stored chunk hypervectors. For each chunk, the multiplication results are accumulated to generate the chunk hypervector (E ). Next, each chunk hypervector multiplies with its corresponding P hypervector, and the results are accumulated over all chunks in order to generate a class hypervector (E ). The same procedure repeats for all data points in the training dataset in order to generate a hypervector for each existing class. This approach significantly accelerates the HDC training, since LookHD does not need to pay the encoding cost for each data point. In addition, factorizing the values using the counting approach significantly reduces the number of required additions.\n\n\nIV. L o o k HD I n f e r e n c e\n\nDuring the training, HDC creates a single hypervector representing each class. These hypervectors store as a trained HDC model and can be used for the rest of the classification task at inference. The main computations of the inference are the encoding and the associative search. In the inference, HDC uses the same encoding module to map a test data point to a hypervector, called query hypervector, H e ND . Then, it computes the similarity of the query hypervector with all k class hypervectors, { Ck , ,C2, C1}, where Ci e ND . Using cosine as a similarity metric, we measure the similarity of a query and ith class hypervector using: 8 (H , Q ) = H .Q /|H || Q |, where 8 denotes the cosine similarity. Finally, each query classifies to a class with the highest cosine similarity.\n\n\nA. Similarity Metric & Model Scalability\n\nAs a light-weight classifier, HDC operations need to be hardware friendly, meaning that the HDC model should fit on the on-chip memory of the embedded devices. The similarity computation can perform efficiently using limited available resources. However, HDC uses the cosine similarity for similarity check. Cosine calculates the inner product of a query and a class hypervector divided by class and query magnitude. In practice, this involves calculating three dot products: H.Q, H.H and Ci .Ci . To reduce the cost of cosine computation, we pre-normalize the class hypervectors to their magnitudes, C i = Ci / |Ci |, once after the training. This enables HDC to ignore repeatedly calculating the class magnitudes for every query. The goal of HDC inference is to find a class hypervector with the highest similarity to a query, i.e., not measuring the absolute cosine values. Thus, we can ignore calculating the query magnitude, | H | , since it is common among all classes. The above two facts simplify the cosine similarity to calculating a dot product between the hypervectors: 8 (H, Ci ) = h .c ,.\n\nAlthough dot product reduces the cost of cosine similarity by 1/ 3, the similarity check is still expensive due to many multiplications involved in the dot product. For example, for speech recognition with 26 classes and D = 10,000, a single query search involves in 26 x 10,000 multiplications. The model size scalability is another issue in HDC. HDC stores a single hypervector representing each class. This results in increasing the model size by the number of classes. For example, speech recognition with k = 26 classes has 13 x larger model size as compared to face recognition, which has only k = 2 classes.\n\n\nB. Model Compression\n\nHere, we propose a novel approach to compress the HDC model and address the model scalability issue. LookHD exploits mathematical orthogonality of random hypervectors in order to compress the HDC model and reduce the computation cost. Instead of using k hypervectors to represent the trained model of an application with k classes, LookHD combines all class hypervectors and represents them using a single one. The combined hypervector needs to store the information of all class hypervectors. Similar to the approach used for lookup-based encoding (explained in Section III-D), LookHD combines the class hypervectors while preserving the infor mation of each individual class. LookHD generates k random hypervectors, {P,1, P'2, , P'k }, where P, e { -1, 1}D . Since these hypervectors are generated randomly, they w ill have nearly orthogonal distribution. Using these hypervectors, we can uniquely store the information of each existing class in a combined hypervector as: C = P,1 * C1 + P,2 * C2 + ... + P,k * Ck (4) Figure 7 shows how LookHD creates a combined class hypervector using the trained class hypervectors and a set of P, hypervectors. Regardless of the number of classes, this approach reduces the HDC model size to a single hypervector. In the inference, HDC checks the similarity of a query with a combined class hypervector by calculating dot product between them:\n\nH.C = H.(P,1 * C1 + P,2 * C2 + ... + P,k * Ck) Next, we can find the similarity of a query and ith class hypervector using: \n\n\nC. Compression Noise\n\nDepending on the number of classes, the model compres sion may affect LookHD classification accuracy. Assume we calculate the similarity of a query with ith class:\n\n(H.C) P i = H.Ci * ( P;j P;j) + \u00a3 H.Cj * (P i P,j ) . (5)\nSignal l 'V j= ' ^ N oise\nwhere P,i P,i is equal to D, since each element of the base hypervector is either 1 or -1, while P,i P,j is almost zero.\n\nTo quantify the quality of model compression, we defined the signal to noise ratio as a quality metric. This metric determines the noise in the cosine similarity of each class. Although the noise is minimal, in HDC, the class hypervectors are highly correlated, and even small noise may change their ranking during the maximum similarity check. The blue bars in Figure 8 show the distribution of the cosine similarity on ACTIVITY [40] application. The results are reported over 1000 test data. our result shows that class hypervectors are highly correlated since all cosine similarities are distributed in a range of 0.9 to 1. This increases the sensitivity of the model as a small noise can change the ranking of the top class during the similarity check.\n\nHere, we develop a method that reduces the correlation between the class hypervectors by removing the common information from the classes. LookHD gets the average of the trained class hypervectors (Cave = 1/ k\u00a3 j ek C ), and then modifies each class hypervector using: C,i = Ci -Cave 8 (Ci , Cave), Vi e k The red bars in Figure 8 show the cosine similarity distribution of the modified class hypervectors. The results indicate that the new model has much wider cosine distribution. This significantly reduces the impact of noise coming from model compression on the similarity measurement. In Section VI-G, we explore the impact of model compression on HDC efficiency-accuracy.\n\n\nD. Retraining\n\nRetraining happens after the initial HDC training. Retraining has a very similar procedure as the inference. As we explained in Section II-B, the retraining needs to iteratively check the similarity of the encoded train data with the HDC model and accordingly updates the model. The similarity search takes a major portion of the retraining cost. LookHD accelerates the retraining by performing the similarity search over the compressed model. For each mispredicted data, LookHD looks at P, hypervectors an incorrectly predicted class (P,Wrong) and a class that the query belongs to it (P,correct ). LookHD accordingly updates the compressed class hypervectors using: C = C + P correct * H P wrong * H where P, e {P,1,...,P,k}. LookHD performs the retraining on the HDC model for a few iterations. Figure 9 shows the LookHD classification accuracy during different retraining iterations for three applications. The number of iterations depends on the application, but it is usually about ten iterations to ensure high enough accuracy. Figure 10 shows the hardware implementation of LookHD training consisting of four main steps:\n\n\nV. Ha r d w a r e Ac c e l e r a t io n A. Training Acceleration\n\nQuantization: As we explained in Section III-D, LookHD combines the training with the encoding module. our approach reads a feature vector and quantizes it by subtracting each feature value from the quantized levels ( Figure 10a). Each feature is assigned to a quantized level, which it has the closest absolute distance with it. Finally, each feature gets one of the q codebooks depending on the feature value. This computation is performed in parallel for all features using FPGA Lookup Tables (LUTs) and Flip-Flops (FFs) resources.\n\nParallel Counting: For each chunk, we use a register array with qr length and a single counter array to keep track of the number of times that each pre-stored hypervector repeats during the training (Figure 10a). The concatenation of the codebooks in a chunk is a direct address to a counter that increments the corresponding register. our implementation reads the selected register, increments it using the chunk counter, and writes the result back to the same address. This process can perform in parallel for all chunks.\n\nWeighted Accumulation: After covering all data points in a class, our implementation multiplies the counter values with pre-stored encoded hypervectors stored in BRAM blocks.\n\n\nBRAM\n\nWhen the size of pre-stored hypervectors is larger than BRAM capacity, LookHD stores the hypervectors in a RAM block. In that case, the performance of computation is limited by the RAM bandwidth. In this paper, we select the chunk size and the number of quantized levels small enough to ensure that the pre-stored hypervectors can fit in BRAM. To provide maximum data locality, our implementation is accessed to the first d elements of all qr pre-stored hypervectors in BRAM, and multiplies them with the counter values in all chunks. This multiplication can happen using LUTs and FFs, since each element of pre-stored hypervector has only log2r bits. Finally, the results of multiplications in all d dimensions are accumulated in a tree-based adder. To maximize the number of dimensions, we use both Digital Signal Processing (DSPs) and available LUTs/FFs resources for this accumulation. The number of d dimensions that can be processed in parallel depends on the number of chunks, m, and counter size, qr.\n\nChunk Aggregation: Finally, the generated hypervectors in all chunks need to be combined together using, P, position hypervectors ( Figure 10). Since the position hypervectors are bipolar, they need more than a single bit for data representation. our approach represents the position hypervectors with binary values, where -1 elements are replaced with 0. Instead of mul tiplying the position and chunk hypervectors, we use position hypervector to change the sign of the chunk hypervectors. A ll elements of a position hypervector with \" 0\" value flip the sign of the chunk element, while elements with \" 1\" value keep the sign of the corresponding chunk element the same. Finally, the chunk hypervectors that passed through the negation block are accumulated using a tree-based adder. We reuse the same hardware to generate each class. Figure 11 shows the FPGA implementation of the LookHD inference consisting of the encoding and associative search modules. In order to get maximum throughput, LookHD runs encoding and associative search in a pipeline. When the encoding module creates the d, dimensions of a query hypervector, the associative search is performed on the previous d dimensions generated by the encoding module. Since the encoding and associative search are majorly using different FPGA resources, this pipeline structure maximizes the through put and resource utilization. Here, we explain how to implement encoding and associative search in FPGA.\n\n\nB. Inference Acceleration\n\nEncoding Acceleration: Similar to the training, the first step of encoding is to quantize each feature value to one of the possible levels and represent it using Log2q bits codebook ( B ). LookHD accesses to the encoded hypervectors in all chunks in parallel (B ). Each chunk hypervector needs to be multiplied by the corresponding P hypervector. Similar to the training, we store P hypervectors as binary representation and use them as input to the negation block (B ). A ll P hypervector dimensions with \" 0\" value flip the sign of the corresponding encoded hypervector. Finally, all d dimensions of vectors are accumulated together. Since each dimension of encoded hypervectors only has Log2q bits, all the computation can perform using LUT/FF blocks (C ).\n\nAssociative Search Acceleration: The computation of the associative search summarizes into three main steps ( Figure 11): (i) multiplying the query and class hypervectors, (ii) multiplying the product result with different position hypervectors corresponding to each class, and (iii) accumulating all elements for each class. our implementation exploits DSPs to multiply query and class hypervectors (D ). However, since the number of available DSPs are much lower than the hypervector dimensions, this similarity happens serially over d dimensional windows. The similarity check, i.e., dot product, starts sequentially by multiplying d dimensions of a query and compressed class hypervector. Our implementation represents P, hypervectors using binary values, where the -1 elements in bipolar vector represent 0 element (E ). To parallelize the associative search for all classes, our implementation reads the first d dimensions of the position hypervector, P', and assigns them as control signals to DSP blocks. The P' dimensions with 0 values configure DSPs to subtraction, while P' elements with 1 elements configure DSP to perform an addition operation (F ). The accumulation of the product elements happens in parallel for all classes. Finally, the results of accumulation are written back to the same product vector (\u00a9 ). This operation repeats iteratively by moving the read windows over the query and class hypervectors until it covers all D dimensions. Finally, the results of all d dimensions are accumulated to calculate the result of the dot product. The size of windows d depends on the number of classes and the number of DSPs available on FPGA. For example, for activity recognition application with k = 6 classes, our implementation can parallelize the computation on d = 64 dimensions.\n\n\nC. Retraining Acceleration\n\nThe retraining can be accelerated using the similar hardware used for associative search. First, we check the similarity of a query and a compressed trained model using the hardware shown in Figure 11. When a query is misclassified, retraining updates the HDC model by adding and subtracting the query from two classes. This update happens once after going over the entire training dataset. our implementation applies all modifications on a copy of the compressed model while using the original model for inference tasks. Instead of applying one addition and subtraction to update the model, our implementation first calculates the AP' H term and adds it to the compressed model. The subtraction of two bipolar AP' = P'comet -P'wrong can get -2, +2, 0 values. Since in hardware we represent the P' s using binary values, we can decide to update each query elements depending on the P' bits. The following equating shows how H * 8P' can be modeled using negation and shift operations:\nC = C + AP H ( -h > > , if (P'correct, P'wrong) = (0, 0) AP H = S h, if (P correct, P wrong) = (0, 1) or (1,0) h > > if (P'correct, P'wrong) = (1, 1)\nwhere h is the element of the query hypervector and > > defines as a single right shift. Depending on the P's elements, each query element stays the same, shifted, or shifted and get a filliped sign bit. This functionality can apply to a query using LUT/FF blocks. After that, the result of the hypervector w ill be added to a created copy of the compressed model. To maximize the throughput, the similarity check and model update modules are implemented in a pipeline structure. For all tested applications, the number of DSPs limits retraining throughput.\n\n\nVI. Ev a l u a t io n\n\n\nA. Experimental Setup\n\nWe implement LookHD training and testing on two platforms: FPGA and CPU. For FPGA, we describe the LookHD functionality using Verilog and synthesize it using Xilinx Vivado Design Suite [46]. The synthesis code has been implemented on the Kintex-7 FPGA KC705 Evaluation Kit using 5ns clock frequency. For CPU, the LookHD code has been written in C++ and optimized for performance. The code has been implemented on the ARM Cortex A53 CPU, and its power is measured using Hioki 3334 power meter.\n\nWe compare the accuracy and efficiency of LookHD with baseline HDC [37], [38] implemented on both CPU and FPGA. To have a fair comparison and show the true benefits of our lookup based approach, we considered an optimized imple mentation of the HDC algorithm [38] explained in Section II as the baseline. The training is implemented by encoding the data points to high-dimensional space and adding the encoded hypervectors in a pipelined stage. In the inference, the baseline runs on the same hardware as LookHD (Section V-B), but uses the original encoding and performs similarity check over the non-compressed model. We evaluate LookHD on benchmarks range from relatively small datasets collected in a small IoT network to a large dataset that includes hundreds of thousands of face images. SPEECH: voice recognition [39], ACTIVITY: activity recognition using mobile device [40], PHYSICAL: physical monitoring using IMU sensors [41], FACE: face recognition [42], and EXTRA: phone position recognition [43].\n\n\nB. LookHD Accuracy\n\nLookHD trains the model by splitting the feature vector and quantizing the level hypervectors to discrete levels. Figure 12 shows the impact of chunk size and the number of quantized levels on the LookHD classification accuracy (D = 2000). We compare LookHD accuracy with the baseline HDC algorithm using linear quantization level [37], [47]. The baseline accuracy of each application is listed in the sub-figure title of Figure 12. In LookHD, increasing the chunk size generally improves the classification accuracy. A small chunk degrades the quality of encoding by increasing the number of required P hypervectors to aggregate the result of different chunks. The best chunk size depends on the distribution of the feature values. our evaluation shows that for most applications using r = 5 is enough to provide acceptable accuracy.\n\nLookHD classification accuracy also depends on the levels of quantization. The larger the number of quantization levels, results in a higher LookHD accuracy. However, with the proposed equalized quantization, the change in accuracy is minor. our results show that for most applications using q = 2 or 4 is enough to ensure acceptable classification accuracy. In contrast, the linear quantization of the existing HDC algorithms [33], [37], [47] results in generating several levels which are not equally used during the classification. This degrades the accuracy by making the classification task more complicated. our evaluation shows that LookHD with q = 2 and q = 4 achieve, on average, 2.1% and 2.4% higher accuracy than the baseline HDC using a non-binarized model. Table II illustrates   dimensions provides the similar accuracy as HDC using full D = 10,000 dimensions, i.e., less than 0.3% quality loss. In the rest of the paper, we show the energy/performance results of LookHD and the baseline HDC for D = 2000 dimensions.\n\n\nC. Training Acceleration\n\nWe optimize the baseline HDC implementation to maximize training throughput. In the baseline HD, the training consists of an encoding module, which involves several bitwise operations. This significantly improves the efficiency of the FPGA training as compared to the CPU implementation. our evaluation shows that the FPGA implementation of the baseline HDC is, on average, 830.2 x faster and 1,509.4x more energy efficient as compared to CPU. Figure 13 shows the training efficiency on both FPGA and CPU for different LookHD configurations, when the feature values are quantized into q = 2, 4 and 8 levels (r = 5).\n\nThe number of quantization levels presents a tradeoff between training efficiency and classification accuracy. The larger quantization levels slightly improve the classification accuracy, but they significantly degrade the training efficiency. In addition, LookHD memory requirement for training expo nentially increases with the number of quantization levels. For example, doubling the quantization levels from q = 2 to q = 4 (r = 5), increases the size of memory requirement to pre-store the encoded hypervectors from 25 = 32 to 45 = 1024 rows. This directly affects the training cost as LookHD requires more logic operations to calculate the counter-vector multiplications and aggregate the chunk hypervectors. Our evaluations show that FPGA-based (CPU-based) implementation of LookHD using q = 2 and q = 4 levels achieves, on average, 28.3 x and 97.4x (3.9x and 7.5x) faster and more energy-efficient training as compared to the state-of-the-art HDC. Similarly, using q = 4 results on average 14.1 x and 48.7 x (2.6 x and 3.8 x) faster and more energy-efficient training. This higher efficiency comes from the capability of LookHD to simplify the encoding operations and combine it with the training module. LookHD learns from highly quantized input data, which is not possible in the original space. This is because LookHD encoding is non-linear and projects input data with a small difference to relatively isolated data in sparse high dimensional space. For example, during projection, a single feature difference between two data points could result in mapping them into very different hypervectors in HDC space.\n\n\nD. Inference Acceleration\n\nIn the inference, HDC consists of the encoding and as sociative search. The encoding maps the input data to high dimensional space by performing several bitwise operations, while the computation of the associative search is mostly dot product of non-binarized hypervectors. Here, we compare the efficiency of the LookHD and the baseline HDC algorithm [33], [37], [47] on CPU and FPGA platforms.\n\nLookHD enhances the encoding efficiency by pre-storing all possible chunk hypervectors in a memory. This simplifies the encoding module by aggregating the pre-stored chunk hypervectors using m number of position (P) hypervectors. Since m < < n, this aggregation can happen much faster than original encoding, where m and n are the number of chunks and the number of features, respectively. In the associative search, combining the class hypervectors reduces the model size and the number of required multiplications. The small model size further improves the computation efficiency by (i) significantly reducing the number of multiplications, (ii) localizing the computation to existing cores. As Figure 14a shows, this advantage is more evident on FPGA, since it has smaller memory and computing resources to parallelize the computation. On average, on CPU, LookHD achieves, on   ity, including low-power or high-performance FPGA/ASIC. However, we target a low-power platform as HDC provides several features that make it promising for real-time learning on embedded devices. For example, HDC supports single-pass or few-pass training on devices with low on-chip memory and computational resources. Although our goal is to run LookHD on embedded platforms (e.g., FPGA or ARM CPU), here we compare it with the powerful GPU architecture to better show the efficiency of LookHD. Table III compares the speedup and energy efficiency of LookHD with the GPU implementation running on NVIDIA GTX 1080 GPU. We have an optimized implementation of HDC using Tensorflow [48]. Unlike the lowpower platforms that we aim to eliminate the computation, GPU can provide significant acceleration by enabling parallelism over different dimensions. All results are relative to the energy consumption and execution time of the CPU implementation. Our evaluation shows that although GPU training and inference are 1.5 x (1.3 x) faster than the FPGA implementation of the baseline HD, LookHD provides 1.1 x and 1.5 x faster training and inference than GPU. In terms of energy efficiency, LookHD is, on average, 67.5 x and 112.7 x more energy-efficient than GPU during training and inference, respectively. Table III also shows that LookHD can further improve computation efficiency by reducing hypervector dimensionality. For example, LookHD losing less than 2% quality loss provides 1.21 x and 1.25 x faster training and inference than LookHD using D = 2000 dimensions.\n\n\nG. LookHD Inference Scalability\n\nAs a light-weight classifier, HDC is desired to provide a small and scalable model that can be stored and processed on embedded devices with limited resources. In the conven tional HDC model, each class is represented using a single hypervector. Therefore, the HDC model size increases linearly with the number of classes. LookHD addresses the model size scalability issue by combining and compressing all class patterns into a single hypervector. Figure 15 shows the tradeoff between the LookHD classification accuracy and computation efficiency when the number of classes increases from k=2 to 48.  The results are reported for running 1000 queries on randomly generated class hypervectors with Gaussian distribution, where the classes have a similar correlation as five tested models. Figure 15a shows the impact of the number of classes on the classification accuracy and the average noise to signal ratio of the compressed model. LookHD with more number of classes has higher noise to signal ratio due to the error from non-orthogonality between the P; hypervectors (shown in Equation 5). This noise can change the rank of the classes during the search for maximum cosine similarity and result in misclassification. Our result shows that LookHD does not lose any accuracy for applications with 12 or fewer classes. However, since the noise in classification statistically grows with the number of classes, LookHD is likely to lose accuracy for applications with more than 12 classes. For example, an application with 26 classes can provide less than 0.8% quality loss as compared to HDC with the non-compressed model. For applications with more than 12 classes, LookHD compresses the trained model into multiple hypervectors in order to eliminate the quality loss. To this end, each compressed hypervector needs to keep the information of less than 12 classes. For example, for an application with 36 classes, LookHD can compress the model into three hypervectors, where each is a combination of 12 hypervectors. This ensures no quality loss while still providing 8.7 x smaller model size. Figure 15b reports the energy-delay product (EDP) improve ment of LookHD using the compressed model as compared to the baseline HDC running on the FPGA. Figure 15b also shows the LookHD model size reduction when LookHD compresses the HDC model into a single hypervector. Our result shows that LookHD can achieve 6.9 x and 12.0 x EDP improvement and smaller model size while providing the same accuracy as the non-compressed model. For applications with more classes, the EDP improvement, and model size improve significantly with minimal impact on the accuracy. For example, for an application with 48 classes, LookHD can achieve 14.6x and 19.2x EDP improvement and smaller model size with about 2% quality loss while compressing the model into a single hypervector. In an exact mode, LookHD compresses the model into four hypervectors, which still results in 10.8 x EDP improvement and 8.7 x model size reduction, while ensuring the same accuracy as the non-compressed model. Figure 16 shows the resource utilization of LookHD FPGA implementation during training and inference phases. The associative memory and encoding module utilize different resources depending on the number of classes and the feature size. Here, we show the resource utilization for SPEECH with k = 26 classes and n = 617 features. The resources utilized by the encoding module mostly consists of LUTs and FFs, which are used to count and store the pre-fetched base hypervectors. In inference and retraining, the associative memory mostly utilizes DSPs. However, since LookHD stores the compressed model, the associative search also uses LUTs and FFs in order to compute the similarity values. In addition, the model update in LookHD further increases the BRAM and LUT utilization in the training phase. As results in Figure 16 show, for SPEECH inference, the FPGA performance is limited by the number of DSPs, while in training, the LUT utilization is the performance bottleneck. Note that depending on the number of features or classes, applications may have different resource utilization. For example, for FACE application with k = 2 classes and n = 608 features (k < < n), the LUTs are the computation bottleneck in both training and inference phases.\n\n\nH. Resource Utilization\n\n\nI. LookHD vs Other Classifiers\n\nTable IV also compares the training/inference efficiency of LookHD with MLP on FPGA. We used DNNWeaver V2.0 [49] for efficient implementation of the NN inference, and FPDeep [50] for NN training on a single FPGA device. FPGA implementations are optimized to maximize performance by utilizing FPGA resources. A ll results listed in Table IV are relative to MLP performance and energy efficiency. Note that we compare the baseline MLP and LookHD, since several existing optimizations, e.g., model binarization or pruning [51], [52], can be applied to both methods. During training, LookHD achieves, on average, 23.1 x faster and43.6x more energy-efficient computation as compared to FPGA-based MLP implementation, respectively. The high efficiency of LookHD in training comes from: (i) LookHD capability in creating an initial model that significantly lowers the number of required retraining iterations. (ii) It eliminates the costly gradient descent for the model update. This results in a higher LookHD efficiency, even in terms of a single training iteration. In inference, LookHD provides 11.7 x faster and 5.1 x higher energy efficiency as compared to FPGA-based MLP implementation. This higher inference efficiency comes from LookHD ability in reducing the number of required resources (multiply-add), on average, by 38.1 x compared to the equivalent MLP. LookHD model compression further reduces memory footprints and provides an average 63.2 x smaller model size compared to the MLP.\n\n\nV II. Re l a t e d W o r k\n\nSince the neuroscientist P. Kanerva introduced the field of hyperdimensional computing [10], prior research have applied the idea into diverse cognitive tasks such as robotics [14], [53], analogy-based reasoning [54], latent semantic analysis [44], text classification [55], genome pattern matching [15], activity recognition [56], prediction from multimodal sensor fusion [17], [57], speech recognition [58]. For example, work in [14] showed how to use HDC mathematics to enables robots to model the human-like memory. The work in [18], [59] showed an HD application for feature vector classification. These approaches assume that H D c learning tasks are performed in the binary domain. However, working with binary vectors provides significantly lower accuracy when running on practical workloads (on average, 17.5% than LookHD). In contrast, LookHD works with the non-binary model and provides sig nificantly higher accuracy on complex classification problems.\n\nseveral existing works have presented the hardware for efficiently processing HDc, including both FPGA and in memory accelerators [21], [22], [23], [37], [60], [61], [62]. Work in [63] implemented HDC on FPGA, but it only works with binary hypervectors with applications limited to the classification of text and time-series signals. Work in [37], [64] designed a new FPGA-based architecture to accelerate the classification task. However, it requires an explicit encoding and training process, resulting in low computation efficiency. Work in [21] showed three digital, resistive, and analog circuits to accelerate the computation of Hamming distance similarity search in the HDC inference. Although these approaches accelerate the HDC computation, the encoding still takes a dominant part of the training procedure. Work in [22] designed and fabricated an HDC chip accelerator using emerging memory devices. However, this architecture only supports the binary model, and its application is limited to the text classification. To the best of our knowledge, LookHD is the first efficient architecture that systematically eliminates the cost of encoding from HDC, with no need for hardware acceleration. In addition, LookHD addresses the model/inference scalability of all prior HDC work by compressing the model and performing the training and inference on the compressed model.\n\n\nV III. CONCLUSION\n\nIn this paper, we propose LookHD to address two major issues of the HDC-based systems: the costly encoding and the scalability of the HDC model. LookHD exploits the computa tion reuse by bounding the number of possible values that the encoded hypervector can take. This eliminates the encoding computation and simplifies the encoding to a simple memory lookup. In the inference, LookHD uses the mathematical orthogonality of random vectors to significantly compress the HDC model and reduce the cost of inference. We accordingly design an embedded architecture to accelerate LookHD on FPGA. Our evaluations show that LookHD achieves 28.3 x speedup and 97.4x energy efficiency training as compared to the state-of-the-art design. \n\nFig. 2 .\n2Breakdown of the encoding, training and associative search execution time during training and inference.\n\nFig. 3 .\n3Assume a data point with n features, F = { f 1, f 2, , f n}, our approach splits the feature vector into m equal sequential chunks, F = {F 1,F2,...,F m}, where F = [ f i+ l, f M ,,fi+ r} and r = n/m. Instead of encoding all n features at once, our approach encodes each feature chunk individually using the Value Value (a) Linear Quantization (b) Equalized Quantization Feature quantization using linear and the proposed equalized approach.\n\nFig. 4 .\n4Impact of linear and equalized quantization on the speech recognition classification accuracy.\n\nFig. 5 .\n5Encoding of each feature chunk without and with using codebook.\n\nFig. 6 .\n6LookHD training using lookup encoding.\n\nFig. 7 .Fig. 8 .\n78argmaxj =1:k{8 (P,i * (H.C),Cj )} In LookHD, calculating the dot product of P,i with H . C does not involve multiplication. In hardware, it can be LookHD model compression and classification. Cosine distribution on original and less-correlated HDC model. by changing the sign of H . C elements on the dimension that P,i has -1 values. This significantly accelerates the LookHD inference by reducing the number of multiplications. We explain the details of hardware implementation in Section V-B.\n\nFig. 9 .\n9LookHD classification accuracy during different retraining iterations.\n\n)\nthe impact of the hypervector dimensions on the LookHD classification accuracy using a chunk size of r = 5 for the quantization levels listed in the table. The results indicate the robustness of LookHD to the reduction of hypervector dimensions. For example, LookHD with D = 2000 SPEECH(HD=91.4%) (b) ACTIVITY(HD=94.6%) (c) PHYSIC(HD=91.3%) (d) FACE(HD=94.1%) (e) EXTRA(HD=70.6%) Fig. 12. Impact of the chunk size and the quantization levels on the LookHD classification accuracy.\n\nFig. 13 .\n13Speedup and energy efficiency improvement of LookHD training running on different platforms.\n\nFig. 14 .\n14Execution time and energy consumption of LookHD and the baseline HDC running (a) a single query in the inference (b) a single retraining iteration.average, 1.7 x faster and 2.3 x more efficient computation than the baseline HDC.On FPGA, the LookHD encoding and associative search blocks are working in the pipeline. As we explained in Section V-B, when associative search checks the similarity of the d dimensions of a query and class hypervectors, the encoding module generates the next d dimensions. The encoding module is implemented using LUTs and FFs, while the associative search mostly utilizes DSPs. Since these two modules use different resources, the value of d depends on the resource constraints in each of the modules. Since the number of FPGA (Kintex-7) DSPs are limited to 840, in all tested cases, the associative search limits the d' values. For example, for ACTIVITY and FACE with 12 and 2 classes, the associative search can process at most d' = 64 and d' = 256 dimensions in parallel, respectively. The results show that FPGA implementation of LookHD is, on average, 2.2 x faster and 4.1 x more energy efficient as compared to the baseline HD. As compared to CPU-based implementation of LookHD, the FPGA-based implementation is, on average, 122.9 x faster and 238.6 x more energy efficient.E. RetrainingFigure 14b compares the execution time and energy con sumption of LookHD with the baseline HDC during a single iteration of the retraining. Similar to inference, the retraining uses the associative search to check the similarity of each training data with the trained model. LookHD updates a copy of the compressed model for each misclassified data. In our evaluations, for each application, we consider the average number of updates during the entire training iterations. The results show that the efficiency of retraining depends on the number of classes and the number of required updates. For SPEECH with the largest number of classes, i.e., 26 classes, LookHD provides the maximum advantage. Our results show that LookHD retraining is, on average, 2.4 x and 4.5 x (1.8 x and 2.3 x) faster and more energy-efficient than the baseline HDC running on FPGA (CPU), respectively. F. LookHD vs. GPU Implementation All proposed algorithm-hardware operations are general and can be implemented on any platform with bit-level granular TABLE III Av e r a g e L o o k HD s pe ed u p a n d e n e r g y e f f ic ie n c y o v e r N V ID IA GPU (N ORMALIZED TO CPU)\n\nFig. 15 .\n15Impact of LookHD model compression on the accuracy and the similarity noise/signal ratio. (b) LookHD scalability with the number of classes.\n\n\nwas partially supported by Semiconductor Re search Corporation (SRC) Task No. 2988.001. Mohsen Imani and Yeseong Kim are co-corresponding authors of the paper.\n\n\nFig. 1. (a) HDC classification Overview, (b) HDC encoding functionality.-J hD Query^h \u00ee \nSimilarity \n\n-> c 1D Class 1 J cV \n\nckD Class k ck1 \n\nAssociative Mem ory \n\n(a ) C lassification O vervie w \n\nBase \nhypervectors \n\nBase \nhypervectors \n\nBase \nhypervectors \nfn_ \n\nFeature \n\nVector [ffpj Query |qi \n\n(b ) Encoding \n\nTest \nData \n\nc2d Class 2 c21 \nTrain \nData \n\n\n\nTABLE\n\n\nTABLE II I\nIIMPACT OF DIMENSIONALITY ON L OOKHD ACCURACY.D \n1 q 1 1000 \n2000 \n4000 \n8000 \n10,000 \n\nSPEEC H \n\n4 \n\n9 4 .8 % \n9 5 .2 % \n9 5 .3 % \n9 5 .5 % \n95 .5 % \n\nA C T IV IT Y \n\n4 \n\n9 7 .3 % \n9 7 .9 % \n9 7 .9 % \n9 8 .0 % \n98 .2 % \n\nP H Y S IC A L \n\n2 \n\n9 1 .4 % \n9 2 .9 % \n9 2 .9 % \n9 3 .1% \n93 .1% \n\nFACE \n\n2 \n\n9 5 .7 % \n9 6 .5 % \n9 6 .6 % \n9 6 .7 % \n96 .8 % \n\nE X T R A \n\n4 \n\n7 2 .5 % \n7 3 .3 % \n7 3 .3 % \n7 3 .4 % \n73 .4 % \n\n\n\nTABLE IV L\nIVo o k HD e f f ic ie n c y v s . M L P o n FPGA Fig. 16. LookHD resource utilization.LUT \n\n97.5% \n| \ni \n81.2% \n| \n\nFF \n\n24.7% \n18.4% \n\nBRAM \n\n84.3% \n1 \ni \n39.3% \nSPEECH \nU C IH A R \nPAMAP2 \nFACE \nE X T R A \nDSP \n91.2% \n| \n\n1 \n\n96.1% \ns \n\u00ab \n* \n\nSpeedup \nEnergy Efficiency \n\n16.6x \n30.4x \n\n19.1 x \n41.2x \n\n26.8x \n48.5x \n\n31.7x \n61.3x \nr-oo \n*t c4 \n\nCXI T t \n\nTraining \n\ni \n\nInference \n\nH \n\nSpeedup \nEnergy Efficiency \n\n7.9x \n3.7x \n\n10.8x \n4.9 x \n\n12.6x \n5.4x \n\n17.3x \n6.3 x \n\n11.9x \n6.0x \n\n\n\nLearning to track: Online multi object tracking by decision making. Y Xiang, A Alahi, S Savarese, 2015 IEEE international confer ence on computer vision (ICCV), no. EPFL-CONF-230283. IEEEY. Xiang, A. Alahi, and S. Savarese, \"Learning to track: Online multi object tracking by decision making,\" in 2015 IEEE international confer ence on computer vision (ICCV), no. EPFL-CONF-230283, pp. 4705 4713, IEEE, 2015.\n\nDeep speech 2: End-to-end speech recognition in english and mandarin. D Amodei, R Anubhai, E Battenberg, C Case, J Casper, B Catanzaro, J Chen, M Chrzanowski, A Coates, G Diamos, International Conference on Machine Learning. D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, et al., \"Deep speech 2: End-to-end speech recognition in english and mandarin,\" in International Conference on Machine Learning, pp. 173-182, 2016.\n\nDeep speech: Scaling up end-to-end speech recognition. A Hannun, C Case, J Casper, B Catanzaro, G Diamos, E Elsen, R Prenger, S Satheesh, S Sengupta, A Coates, arXiv:1412.5567arXiv preprintA. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, et al., \"Deep speech: Scaling up end-to-end speech recognition,\" arXiv preprint arXiv:1412.5567, 2014.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" in Advances in neural information processing systems, pp. 1097-1105, 2012.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings o f the IEEE conference on computer vision and pattern recognition. o f the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings o f the IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.\n\nPredicting param eters in deep learning. M Denil, B Shakibi, L Dinh, N De Freitas, Advances in neural information processing systems. M. Denil, B. Shakibi, L. Dinh, N. De Freitas, et al., \"Predicting param eters in deep learning,\" in Advances in neural information processing systems, pp. 2148-2156, 2013.\n\nSensing as a service and big data. A Zaslavsky, C Perera, D Georgakopoulos, arXiv:1301.0159arXiv preprintA. Zaslavsky, C. Perera, and D. Georgakopoulos, \"Sensing as a service and big data,\" arXiv preprint arXiv:1301.0159, 2013.\n\nInternet of things and big data analytics for smart and connected communities. H Sun, A J Song, R Jara, Bie, IEEE Access. 4Y Sun, H. Song, A. J. Jara, and R. Bie, \"Internet of things and big data analytics for smart and connected communities,\" IEEE Access, vol. 4, pp. 766-773, 2016.\n\nFuture internet: the internet of things architecture, possible applications and key challenges. R Khan, S U Khan, R Zaheer, S Khan, Frontiers of Information Technology (FIT), 2012 10th International Conference on. IEEER. Khan, S. U. Khan, R. Zaheer, and S. Khan, \"Future internet: the internet of things architecture, possible applications and key challenges,\" in Frontiers of Information Technology (FIT), 2012 10th International Conference on, pp. 257-260, IEEE, 2012.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cognitive Computation, vol. 1, no. 2, pp. 139-159, 2009.\n\nP Kanerva, Sparse distributed memory. M IT press. P. Kanerva, Sparse distributed memory. M IT press, 1988.\n\nSparseness and expansion in sensory representations. B Babadi, H Sompolinsky, Neuron. 835B. Babadi and H. Sompolinsky, \"Sparseness and expansion in sensory representations,\" Neuron, vol. 83, no. 5, pp. 1213-1226, 2014.\n\nOnlinehd: Robust, efficient, and single-pass online learning using hyperdimensional system. A Cano, N Matsumoto, E Ping, M Imani, Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE2021A. Cano, N. Matsumoto, E. Ping, and M . Imani, \"Onlinehd: Robust, efficient, and single-pass online learning using hyperdimensional system,\" in Design, Automation & Test in Europe Conference & Exhibition (DATE), IEEE, 2021.\n\nLearning sen sorimotor control with neuromorphic sensors: Toward hyperdimensional active perception. A Mitrokhin, P Sutor, C Ferm\u00fcller, Y Aloimonos, Science Robotics. 4306736A. Mitrokhin, P. Sutor, C. Ferm\u00fcller, and Y. Aloimonos, \"Learning sen sorimotor control with neuromorphic sensors: Toward hyperdimensional active perception,\" Science Robotics, vol. 4, no. 30, p. eaaw6736, 2019.\n\nGeniehd: Efficient dna pattern matching accelerator using hyperdimensional computing. Y Kim, M Imani, N Moshiri, T Rosing, 2020Y. Kim, M . Imani, N. Moshiri, and T. Rosing, \"Geniehd: Efficient dna pattern matching accelerator using hyperdimensional computing,\" in 2020\n\nDesign, Europe Conference & Exhibition (DATE). IEEEDesign, Automation & Test in Europe Conference & Exhibition (DATE), pp. 115-120, IEEE, 2020.\n\nPrive-hd: Privacy-preserved hyperdimensional computing. B Khaleghi, M Imani, T Rosing, arXiv:2005.06716arXiv preprintB. Khaleghi, M . Imani, and T. Rosing, \"Prive-hd: Privacy-preserved hyperdimensional computing,\" arXiv preprint arXiv:2005.06716, 2020.\n\nModeling dependencies in multiple parallel data streams with hyperdimensional computing. O Rasanen, S Kakouros, IEEE Signal Processing Letters. 217O. Rasanen and S. Kakouros, \"Modeling dependencies in multiple parallel data streams with hyperdimensional computing,\" IEEE Signal Processing Letters, vol. 21, no. 7, pp. 899-903, 2014.\n\nA binary learning framework for hyperdimensional computing. M Imani, J Messerly, F Wu, W Pi, T Rosing, 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEEM . Imani, J. Messerly, F. Wu, W. Pi, and T. Rosing, \"A binary learning framework for hyperdimensional computing,\" in 2019 Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 126-131, IEEE, 2019.\n\nA framework for efficient and binary clustering in high-dimensional space. A Cano, Y Kim, M Imani, Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE2021A. Cano, Y. Kim, and M . Imani, \"A framework for efficient and binary clustering in high-dimensional space,\" in Design, Automation & Test in Europe Conference & Exhibition (DATE), IEEE, 2021.\n\nDual: Acceleration of clustering algorithms using digital-based processing in-memory. M Imani, S Pampana, S Gupta, M Zhou, Y Kim, T Rosing, 53rd International Symposium on Microarchitecture (MICRO). 2020M. Imani, S. Pampana, S. Gupta, M. Zhou, Y. Kim, and T. Rosing, \"Dual: Acceleration of clustering algorithms using digital-based processing in-memory,\" in 53rd International Symposium on Microarchitecture (MICRO), pp. 1-14, IEEE/ACM, 2020.\n\nExploring hyperdimensional associative memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, High Performance Computer Architecture (HPCA. IEEE2017 IEEE International Symposium onM. Imani, A. Rahimi, D. Kong, T. Rosing, and J. M. Rabaey, \"Exploring hyperdimensional associative memory,\" in High Performance Computer Architecture (HPCA), 2017 IEEE International Symposium on, pp. 445 456, IEEE, 2017.\n\nBrain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study. T Wu, P Huang, A Rahimi, H Li, J Rabaey, P Wong, S Mitra, IEEE Inti. Solid-State Circuits Conference (ISSCC). IEEET. Wu, P. Huang, A. Rahimi, H. Li, J. Rabaey, P. Wong, and S. Mitra, \"Brain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study,\" in IEEE Inti. Solid-State Circuits Conference (ISSCC), IEEE, 2018.\n\n. H Li, T F Wu, A Rahimi, K.-S Li, M Rusch, C.-H Lin, J.-L , H. Li, T. F. Wu, A. Rahimi, K.-S. Li, M . Rusch, C.-H. Lin, J.-L.\n\nHyperdimensional computing with 3d vrram in-memory kernels: Device-architecture co design for energy-efficient, error-resilient language recognition. M M Hsu, S B Sabry, J Eryilmaz, Sohn, Electron Devices Meeting (IEDM). IEEEHsu, M . M . Sabry, S. B. Eryilmaz, J. Sohn, et al., \"Hyperdimensional computing with 3d vrram in-memory kernels: Device-architecture co design for energy-efficient, error-resilient language recognition,\" in Electron Devices Meeting (IEDM), 2016 IEEE International, pp. 16-1, IEEE, 2016.\n\nWebFeet Research is Implementing Hyperdimensional Computing on FPGA enhanced with N V D IM M. \"WebFeet Research is Implementing Hyperdimensional Computing on FPGA enhanced with N V D IM M .\" http://www.webfeetresearch.com/.\n\nBeyond imitation: Zero-shot task transfer on robots by learning concepts as cognitive programs. M Lazaro-Gredilla, D Lin, J S Guntupalli, D George, arXiv:1812.02788arXiv preprintM . Lazaro-Gredilla, D. Lin, J. S. Guntupalli, and D. George, \"Beyond imitation: Zero-shot task transfer on robots by learning concepts as cognitive programs,\" arXiv preprint arXiv:1812.02788, 2018.\n\nNumenta Use Hyperdimensional Computing as Hierarchical Temporal Memory. \"Numenta Use Hyperdimensional Computing as Hierarchical Temporal Memory.\" https://numenta.org/.\n\nProperties of sparse distributed represen tations and their application to hierarchical temporal memory. S Ahmad, J Hawkins, arXiv:1503.07469arXiv preprintS. Ahmad and J. Hawkins, \"Properties of sparse distributed represen tations and their application to hierarchical temporal memory,\" arXiv preprint arXiv:1503.07469, 2015.\n\nG Karunaratne, M L Gallo, G Cherubini, L Benini, A Rahimi, A Sebastian, arXiv:1906.01548-memory hyperdimensional computing. arXiv preprintG. Karunaratne, M. L. Gallo, G. Cherubini, L. Benini, A. Rahimi, and A. Sebastian, \"In-memory hyperdimensional computing,\" arXiv preprint arXiv:1906.01548, 2019.\n\nThe kanerva machine: A generative distributed memory. Y Wu, G Wayne, A Graves, T Lillicrap, arXiv:1804.01756arXiv preprintY. Wu, G. Wayne, A. Graves, and T. Lillicrap, \"The kanerva machine: A generative distributed memory,\" arXiv preprint arXiv:1804.01756, 2018.\n\nGeneral-purpose code acceleration with limited-precision analog computation. R St Amant, A Yazdanbakhsh, J Park, B Thwaites, H Esmaeilzadeh, A Hassibi, L Ceze, D Burger, ACM SIGARCH Computer Architecture News. 423R. St Amant, A. Yazdanbakhsh, J. Park, B. Thwaites, H. Esmaeilzadeh, A. Hassibi, L. Ceze, and D. Burger, \"General-purpose code acceleration with limited-precision analog computation,\" ACM SIGARCH Computer Architecture News, vol. 42, no. 3, pp. 505-516, 2014.\n\nAxbench: A multiplatform benchmark suite for approximate computing. A Yazdanbakhsh, D Mahajan, H Esmaeilzadeh, P Lotfi-Kamran, IEEE Design & Test. 342A. Yazdanbakhsh, D. Mahajan, H. Esmaeilzadeh, and P. Lotfi-Kamran, \"Axbench: A multiplatform benchmark suite for approximate computing,\" IEEE Design & Test, vol. 34, no. 2, pp. 60-68, 2016.\n\nApproximate computing and the quest for computing efficiency. S Venkataramani, S T Chakradhar, K Roy, A Raghunathan, Proceedings o f the 52nd Annual Design Automation Conference. o f the 52nd Annual Design Automation ConferenceACM120S. Venkataramani, S. T. Chakradhar, K. Roy, and A. Raghunathan, \"Approximate computing and the quest for computing efficiency,\" in Proceedings o f the 52nd Annual Design Automation Conference, p. 120, ACM, 2015.\n\nA robust and energyefficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proceedings o f the 2016 International Symposium on Low Power Electronics and Design. o f the 2016 International Symposium on Low Power Electronics and DesignACMA. Rahimi, P. Kanerva, and J. M . Rabaey, \"A robust and energy- efficient classifier using brain-inspired hyperdimensional computing,\" in Proceedings o f the 2016 International Symposium on Low Power Electronics and Design, pp. 64-69, ACM, 2016.\n\nRandom indexing of text samples for latent semantic analysis. P Kanerva, J Kristofersson, A Holst, Proceedings of the 22nd annual conference of the cognitive science society. the 22nd annual conference of the cognitive science societyCiteseer1036P. Kanerva, J. Kristofersson, and A. Holst, \"Random indexing of text samples for latent semantic analysis,\" in Proceedings of the 22nd annual conference of the cognitive science society, vol. 1036, Citeseer, 2000.\n\nBrain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study. T F Wu, H Li, P.-C Huang, A Rahimi, J M Rabaey, H.-S P Wong, M M Shulaker, S Mitra, Solid-State Circuits Conference-(ISSCC). IEEET. F. Wu, H. Li, P.-C. Huang, A. Rahimi, J. M. Rabaey, H.-S. P. Wong, M . M . Shulaker, and S. Mitra, \"Brain-inspired computing exploiting carbon nanotube fets and resistive ram: Hyperdimensional computing case study,\" in Solid-State Circuits Conference-(ISSCC), 2018 IEEE International, pp. 492-494, IEEE, 2018.\n\nA framework for collaborative learning in secure high dimensional space. M Imani, Y Kim, S Riazi, J Messerly, P Liu, F Koushanfar, T Rosing, 2019 IEEE 12th International Conference on Cloud Computing (CLOUD). IEEEM . Imani, Y. Kim, S. Riazi, J. Messerly, P. Liu, F. Koushanfar, and T. Rosing, \"A framework for collaborative learning in secure high dimensional space,\" in 2019 IEEE 12th International Conference on Cloud Computing (CLOUD), pp. 435^4-6, IEEE, 2019.\n\nF5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing. S Salamat, M Imani, B Khaleghi, T Rosing, Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate ArraysACMS. Salamat, M. Imani, B. Khaleghi, and T. Rosing, \"F5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing,\" in Proceedings of the 2019 ACM/SIGDA International Symposium on Field- Programmable Gate Arrays, pp. 53-62, ACM, 2019.\n\nA Rahimi, T F Wu, H Li, J M Rabaey, H.-S P Wong, M M Shulaker, S Mitra, arXiv:1811.09557Hyperdimensional computing nanosystem. arXiv preprintA. Rahimi, T. F. Wu, H. Li, J. M. Rabaey, H.-S. P. Wong, M. M. Shulaker, and S. Mitra, \"Hyperdimensional computing nanosystem,\" arXiv preprint arXiv:1811.09557, 2018.\n\nUci machine learning repository. \"Uci machine learning repository.\" http://archive.ics.uci.edu/ml/datasets/ ISOLET.\n\nHuman activity recognition on smartphones using a multiclass hardware-friendly support vector machine. D Anguita, A Ghio, L Oneto, X Parra, J L Reyes-Ortiz, SpringerInternational workshop on ambient assisted livingD. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, \"Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine,\" in International workshop on ambient assisted living, pp. 216-223, Springer, 2012.\n\nIntroducing a new benchmarked dataset for activity monitoring. A Reiss, D Stricker, Wearable Computers (ISWC), 2012 16th International Symposium on. IEEEA. Reiss and D. Stricker, \"Introducing a new benchmarked dataset for activity monitoring,\" in Wearable Computers (ISWC), 2012 16th International Symposium on, pp. 108-109, IEEE, 2012.\n\nOrchard: Visual object recognition accelerator based on approximate in-memory processing. Y Kim, M Imani, T Rosing, 2017 IEEE/ACM International Conference on. IEEEComputer-Aided DesignY. Kim, M . Imani, and T. Rosing, \"Orchard: Visual object recognition accelerator based on approximate in-memory processing,\" in Computer- Aided Design (ICCAD), 2017 IEEE/ACM International Conference on, pp. 25-32, IEEE, 2017.\n\nRecognizing detailed human context in the wild from smartphones and smartwatches. Y Vaizman, K Ellis, G Lanckriet, IEEE Pervasive Computing. 164Y. Vaizman, K. Ellis, and G. Lanckriet, \"Recognizing detailed human context in the wild from smartphones and smartwatches,\" IEEE Pervasive Computing, vol. 16, no. 4, pp. 62-74, 2017.\n\nRandom indexing of text samples for latent semantic analysis. P Kanerva, J Kristofersson, A Holst, Proceedings of the 22nd annual conference of the cognitive science society. the 22nd annual conference of the cognitive science societyCiteseer1036P. Kanerva, J. Kristofersson, and A. Holst, \"Random indexing of text samples for latent semantic analysis,\" in Proceedings of the 22nd annual conference of the cognitive science society, vol. 1036, Citeseer, 2000.\n\nDeep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. S Han, H Mao, W J Dally, arXiv:1510.00149arXiv preprintS. Han, H . Mao, and W. J. Dally, \"Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding,\" arXiv preprint arXiv:1510.00149, 2015.\n\nVivado design suite. T Feist, White Paper. 5T. Feist, \"Vivado design suite,\" White Paper, vol. 5, 2012.\n\nHierarchical hyperdi mensional computing for energy efficient classification. M Imani, C Huang, D Kong, T Rosing, 55M . Imani, C. Huang, D. Kong, and T. Rosing, \"Hierarchical hyperdi mensional computing for energy efficient classification,\" in 2018 55th\n\nACM/ESDA/IEEE Design Automation Conference (DAC). IEEEACM/ESDA/IEEE Design Automation Conference (DAC), pp. 1-6, IEEE, 2018.\n\n. M Abadi, A Agarwal, P Barham, E Brevdo, Z Chen, C Citro, G S , M . Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.\n\nTensorflow: Large-scale machine learning on heterogeneous distributed systems. A Corrado, J Davis, M Dean, Devin, arXiv:1603.04467arXiv preprintCorrado, A. Davis, J. Dean, M . Devin, et al., \"Tensorflow: Large-scale machine learning on heterogeneous distributed systems,\" arXiv preprint arXiv:1603.04467, 2016.\n\nFrom high-level deep neural models to fpgas. H Sharma, J Park, D Mahajan, E Amaro, J K Kim, C Shao, A Mishra, H Esmaeilzadeh, 49th Annual IEEE/ACM International Symposium on. IEEEMicroarchitecture (MICRO)H. Sharma, J. Park, D. Mahajan, E. Amaro, J. K. Kim, C. Shao, A. Mishra, and H. Esmaeilzadeh, \"From high-level deep neural models to fpgas,\" in Microarchitecture (MICRO), 2016 49th Annual IEEE/ACM International Symposium on, pp. 1-12, IEEE, 2016.\n\nFpdeep: Acceleration and load balancing of cnn training on fpga clusters. T Geng, T Wang, A Sanaullah, C Yang, R Xu, R Patel, M Herbordt, 2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). IEEET. Geng, T. Wang, A. Sanaullah, C. Yang, R. Xu, R. Patel, and M . Herbordt, \"Fpdeep: Acceleration and load balancing of cnn training on fpga clusters,\" in 2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM), pp. 81-84, IEEE, 2018.\n\nM Courbariaux, I Hubara, D Soudry, R El-Yaniv, Y Bengio, arXiv:1602.02830Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1. arXiv preprintM . Courbariaux, I. Hubara, D. Soudry, R. El-Yaniv, and Y. Bengio, \"Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1,\" arXiv preprint arXiv:1602.02830, 2016.\n\nScalpel: Customizing dnn pruning to the underlying hardware parallelism. J Yu, A Lukefahr, D Palframan, G Dasika, R Das, S Mahlke, J. Yu, A. Lukefahr, D. Palframan, G. Dasika, R. Das, and S. Mahlke, \"Scalpel: Customizing dnn pruning to the underlying hardware parallelism,\"\n\n. ACM SIGARCH Computer Architecture News. 452ACM SIGARCH Computer Architecture News, vol. 45, no. 2, pp. 548-560, 2017.\n\nCrossmodal learning and prediction of autobiographical episodic experiences using a sparse distributed memory. S , S. Jockel, \"Crossmodal learning and prediction of autobiographical episodic experiences using a sparse distributed memory,\" 2010.\n\nWhat we mean when we say\" what s the dollar of mexico?\": Prototypes and mapping in concept space. AAAI fa ll symposium: quantum informatics fo r cognitive, social, and semantic processes. Citeseer1036p. kanerva, \"What we mean when we say\" what s the dollar of mexico?\": Prototypes and mapping in concept space,\" in AAAI fa ll symposium: quantum informatics fo r cognitive, social, and semantic processes, vol. 1036, Citeseer, 2010.\n\nHyperdi mensional computing for text classification. F R Najafabadi, A Rahimi, P Kanerva, J M Rabaey, Design. Automation Test in Europe Conference Exhibition (DATE), University BoothF. R. Najafabadi, A. Rahimi, P. Kanerva, and J. M . Rabaey, \"Hyperdi mensional computing for text classification,\" Design, Automation Test in Europe Conference Exhibition (DATE), University Booth, 2016.\n\nEfficient human activity recog nition using hyperdimensional computing. Y Kim, M Imani, T S Rosing, Proceedings o f the 8th International Conference on the Internet of Things. o f the 8th International Conference on the Internet of ThingsY. Kim, M . Imani, and T. S. Rosing, \"Efficient human activity recog nition using hyperdimensional computing,\" in Proceedings o f the 8th International Conference on the Internet of Things, pp. 1-6, 2018.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, J Saarinen, IEEE Transactions on Neural Networks and Learning Systems. 99O. Rasanen and J. Saarinen, \"Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns,\" IEEE Transactions on Neural Networks and Learning Systems, vol. PP, no. 99, pp. 1-12, 2015.\n\nVoicehd: Hyperdi mensional computing for efficient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, International Conference on Rebooting Computing (ICRC). IEEEM . Imani, D. Kong, A. Rahimi, and T. Rosing, \"Voicehd: Hyperdi mensional computing for efficient speech recognition,\" in International Conference on Rebooting Computing (ICRC), pp. 1-6, IEEE, 2017.\n\nBric: Locality-based encoding for energy-efficient brain-inspired hyperdimen sional computing. M Imani, J Morris, J Messerly, H Shu, T Deng, Rosing, Proceedings of the 56th Annual Design Automation Conference. the 56th Annual Design Automation ConferenceM. Imani, J. Morris, J. Messerly, H. Shu, Y Deng, and T. Rosing, \"Bric: Locality-based encoding for energy-efficient brain-inspired hyperdimen sional computing,\" in Proceedings of the 56th Annual Design Automation Conference 2019, pp. 1-6, 2019.\n\nAccelerating hyperdimensional computing on fpgas by exploiting computational reuse. S Salamat, M Imani, T Rosing, IEEE Transac tions on Computers. S. Salamat, M . Imani, and T. Rosing, \"Accelerating hyperdimensional computing on fpgas by exploiting computational reuse,\" IEEE Transac tions on Computers, 2020.\n\nThrifty: Training with hyperdimensional computing across flash hierarchy. S Gupta, J Morris, M Imani, R Ramkumar, J Yu, A Tiwari, B Aksanli, T S Rosing, 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD). IEEES. Gupta, J. Morris, M . Imani, R. Ramkumar, J. Yu, A. Tiwari, B. Aksanli, and T. S. Rosing, \"Thrifty: Training with hyperdimensional computing across flash hierarchy,\" in 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD), pp. 1-9, IEEE, 2020.\n\nQuanthd: A quantization framework for hyperdimensional computing. M Imani, S Bosch, S Datta, S Ramakrishna, S Salamat, J M Rabaey, T Rosing, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. M . Imani, S. Bosch, S. Datta, S. Ramakrishna, S. Salamat, J. M . Rabaey, and T. Rosing, \"Quanthd: A quantization framework for hyperdimensional computing,\" IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2019.\n\nHardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory. M Schmuck, L Benini, A Rahimi, arXiv:1807.08583arXiv preprintM . Schmuck, L. Benini, and A. Rahimi, \"Hardware optimizations of dense binary hyperdimensional computing: Rematerialization of hypervectors, binarized bundling, and combinational associative memory,\" arXiv preprint arXiv:1807.08583, 2018.\n\nSparsehd: Algorithm-hardware co-optimization for efficient high-dimensional computing. M Imani, S Salamat, B Khaleghi, M Samragh, F Koushanfar, T Rosing, 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). IEEEM . Imani, S. Salamat, B. Khaleghi, M . Samragh, F. Koushanfar, and T. Rosing, \"Sparsehd: Algorithm-hardware co-optimization for efficient high-dimensional computing,\" in 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM), pp. 190-198, IEEE, 2019.\n", "annotations": {"author": "[{\"start\":\"77\",\"end\":\"359\"},{\"start\":\"360\",\"end\":\"627\"},{\"start\":\"628\",\"end\":\"896\"},{\"start\":\"897\",\"end\":\"912\"},{\"start\":\"913\",\"end\":\"1172\"},{\"start\":\"1173\",\"end\":\"1443\"},{\"start\":\"1444\",\"end\":\"1715\"},{\"start\":\"1716\",\"end\":\"1729\"},{\"start\":\"1730\",\"end\":\"1999\"},{\"start\":\"2000\",\"end\":\"2065\"}]", "publisher": null, "author_last_name": "[{\"start\":\"84\",\"end\":\"103\"},{\"start\":\"368\",\"end\":\"371\"},{\"start\":\"635\",\"end\":\"640\"},{\"start\":\"905\",\"end\":\"911\"},{\"start\":\"913\",\"end\":\"916\"},{\"start\":\"1180\",\"end\":\"1187\"},{\"start\":\"1454\",\"end\":\"1459\"},{\"start\":\"1724\",\"end\":\"1728\"},{\"start\":\"1737\",\"end\":\"1743\"}]", "author_first_name": "[{\"start\":\"77\",\"end\":\"83\"},{\"start\":\"360\",\"end\":\"367\"},{\"start\":\"628\",\"end\":\"634\"},{\"start\":\"897\",\"end\":\"904\"},{\"start\":\"1173\",\"end\":\"1179\"},{\"start\":\"1444\",\"end\":\"1453\"},{\"start\":\"1716\",\"end\":\"1723\"},{\"start\":\"1730\",\"end\":\"1736\"}]", "author_affiliation": "[{\"start\":\"105\",\"end\":\"165\"},{\"start\":\"167\",\"end\":\"358\"},{\"start\":\"373\",\"end\":\"433\"},{\"start\":\"435\",\"end\":\"626\"},{\"start\":\"642\",\"end\":\"702\"},{\"start\":\"704\",\"end\":\"895\"},{\"start\":\"918\",\"end\":\"978\"},{\"start\":\"980\",\"end\":\"1171\"},{\"start\":\"1189\",\"end\":\"1249\"},{\"start\":\"1251\",\"end\":\"1442\"},{\"start\":\"1461\",\"end\":\"1521\"},{\"start\":\"1523\",\"end\":\"1714\"},{\"start\":\"1745\",\"end\":\"1805\"},{\"start\":\"1807\",\"end\":\"1998\"},{\"start\":\"2001\",\"end\":\"2064\"}]", "title": "[{\"start\":\"1\",\"end\":\"74\"},{\"start\":\"2066\",\"end\":\"2139\"}]", "venue": null, "abstract": "[{\"start\":\"2284\",\"end\":\"4734\"}]", "bib_ref": "[{\"start\":\"4985\",\"end\":\"4988\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"5009\",\"end\":\"5012\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"5014\",\"end\":\"5017\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"5040\",\"end\":\"5043\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"5045\",\"end\":\"5048\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"5283\",\"end\":\"5286\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"5288\",\"end\":\"5291\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"5293\",\"end\":\"5296\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"5298\",\"end\":\"5301\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"5843\",\"end\":\"5847\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"5962\",\"end\":\"5966\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"6130\",\"end\":\"6134\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"6791\",\"end\":\"6795\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"6856\",\"end\":\"6860\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"6862\",\"end\":\"6866\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"6868\",\"end\":\"6872\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"6977\",\"end\":\"6981\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"6983\",\"end\":\"6987\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"6989\",\"end\":\"6993\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"6995\",\"end\":\"6999\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"7001\",\"end\":\"7005\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"7389\",\"end\":\"7393\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"7395\",\"end\":\"7399\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"7401\",\"end\":\"7405\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"7407\",\"end\":\"7411\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"7426\",\"end\":\"7430\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"7432\",\"end\":\"7436\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"7438\",\"end\":\"7442\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"7679\",\"end\":\"7683\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"7685\",\"end\":\"7689\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"7691\",\"end\":\"7695\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"8194\",\"end\":\"8198\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"8200\",\"end\":\"8204\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"8206\",\"end\":\"8210\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"8820\",\"end\":\"8824\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"8826\",\"end\":\"8830\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"9653\",\"end\":\"9657\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"9659\",\"end\":\"9663\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"9665\",\"end\":\"9669\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"11381\",\"end\":\"11385\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"11387\",\"end\":\"11391\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"12526\",\"end\":\"12530\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"12532\",\"end\":\"12536\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"12538\",\"end\":\"12542\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"12783\",\"end\":\"12787\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"14127\",\"end\":\"14131\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"14289\",\"end\":\"14290\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"16750\",\"end\":\"16754\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"16788\",\"end\":\"16792\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"16825\",\"end\":\"16829\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"16855\",\"end\":\"16859\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"16914\",\"end\":\"16918\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"18180\",\"end\":\"18184\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"18186\",\"end\":\"18190\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"18192\",\"end\":\"18196\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"22495\",\"end\":\"22499\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"24450\",\"end\":\"24454\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"35060\",\"end\":\"35064\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"45550\",\"end\":\"45554\",\"attributes\":{\"ref_id\":\"b47\"}},{\"start\":\"45926\",\"end\":\"45930\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"45932\",\"end\":\"45936\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"46118\",\"end\":\"46122\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"46678\",\"end\":\"46682\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"46735\",\"end\":\"46739\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"46789\",\"end\":\"46793\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"46818\",\"end\":\"46822\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"46862\",\"end\":\"46866\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"47221\",\"end\":\"47225\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"47227\",\"end\":\"47231\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"48153\",\"end\":\"48157\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"48159\",\"end\":\"48163\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"48165\",\"end\":\"48169\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"51403\",\"end\":\"51407\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"51409\",\"end\":\"51413\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"51415\",\"end\":\"51419\",\"attributes\":{\"ref_id\":\"b48\"}},{\"start\":\"53008\",\"end\":\"53012\",\"attributes\":{\"ref_id\":\"b50\"}},{\"start\":\"58426\",\"end\":\"58430\",\"attributes\":{\"ref_id\":\"b52\"}},{\"start\":\"58492\",\"end\":\"58496\",\"attributes\":{\"ref_id\":\"b53\"}},{\"start\":\"58837\",\"end\":\"58841\",\"attributes\":{\"ref_id\":\"b54\"}},{\"start\":\"58843\",\"end\":\"58847\",\"attributes\":{\"ref_id\":\"b55\"}},{\"start\":\"59926\",\"end\":\"59930\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"60015\",\"end\":\"60019\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"60021\",\"end\":\"60025\",\"attributes\":{\"ref_id\":\"b57\"}},{\"start\":\"60051\",\"end\":\"60055\",\"attributes\":{\"ref_id\":\"b58\"}},{\"start\":\"60082\",\"end\":\"60086\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"60108\",\"end\":\"60112\",\"attributes\":{\"ref_id\":\"b59\"}},{\"start\":\"60138\",\"end\":\"60142\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"60165\",\"end\":\"60169\",\"attributes\":{\"ref_id\":\"b60\"}},{\"start\":\"60212\",\"end\":\"60216\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"60218\",\"end\":\"60222\",\"attributes\":{\"ref_id\":\"b61\"}},{\"start\":\"60243\",\"end\":\"60247\",\"attributes\":{\"ref_id\":\"b62\"}},{\"start\":\"60270\",\"end\":\"60274\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"60371\",\"end\":\"60375\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"60377\",\"end\":\"60381\",\"attributes\":{\"ref_id\":\"b63\"}},{\"start\":\"60935\",\"end\":\"60939\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"60941\",\"end\":\"60945\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"60947\",\"end\":\"60951\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"60953\",\"end\":\"60957\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"60959\",\"end\":\"60963\",\"attributes\":{\"ref_id\":\"b64\"}},{\"start\":\"60965\",\"end\":\"60969\",\"attributes\":{\"ref_id\":\"b65\"}},{\"start\":\"60971\",\"end\":\"60975\",\"attributes\":{\"ref_id\":\"b66\"}},{\"start\":\"60985\",\"end\":\"60989\",\"attributes\":{\"ref_id\":\"b67\"}},{\"start\":\"61147\",\"end\":\"61151\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"61153\",\"end\":\"61157\",\"attributes\":{\"ref_id\":\"b68\"}},{\"start\":\"61349\",\"end\":\"61353\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"61631\",\"end\":\"61635\",\"attributes\":{\"ref_id\":\"b22\"}}]", "figure": "[{\"start\":\"62935\",\"end\":\"63050\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"63051\",\"end\":\"63502\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"63503\",\"end\":\"63608\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"63609\",\"end\":\"63683\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"63684\",\"end\":\"63733\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"63734\",\"end\":\"64249\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"64250\",\"end\":\"64331\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"64332\",\"end\":\"64815\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"64816\",\"end\":\"64921\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"64922\",\"end\":\"67412\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"67413\",\"end\":\"67566\",\"attributes\":{\"id\":\"fig_10\"}},{\"start\":\"67567\",\"end\":\"67728\",\"attributes\":{\"id\":\"fig_11\"}},{\"start\":\"67729\",\"end\":\"68093\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"68094\",\"end\":\"68101\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"68102\",\"end\":\"68532\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"68533\",\"end\":\"69035\",\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"4763\",\"end\":\"5452\"},{\"start\":\"5454\",\"end\":\"6599\"},{\"start\":\"6601\",\"end\":\"7412\"},{\"start\":\"7414\",\"end\":\"8948\"},{\"start\":\"8950\",\"end\":\"9409\"},{\"start\":\"9411\",\"end\":\"11527\"},{\"start\":\"11529\",\"end\":\"12788\"},{\"start\":\"12807\",\"end\":\"13817\"},{\"start\":\"13819\",\"end\":\"14229\"},{\"start\":\"14260\",\"end\":\"14700\"},{\"start\":\"14736\",\"end\":\"14864\"},{\"start\":\"14904\",\"end\":\"15861\"},{\"start\":\"15881\",\"end\":\"16319\"},{\"start\":\"16341\",\"end\":\"17712\"},{\"start\":\"17714\",\"end\":\"18628\"},{\"start\":\"18630\",\"end\":\"19237\"},{\"start\":\"19292\",\"end\":\"19646\"},{\"start\":\"19648\",\"end\":\"21310\"},{\"start\":\"21336\",\"end\":\"21536\"},{\"start\":\"21576\",\"end\":\"21673\"},{\"start\":\"21675\",\"end\":\"22500\"},{\"start\":\"22538\",\"end\":\"22587\"},{\"start\":\"22627\",\"end\":\"23029\"},{\"start\":\"23059\",\"end\":\"23737\"},{\"start\":\"23739\",\"end\":\"24305\"},{\"start\":\"24307\",\"end\":\"25805\"},{\"start\":\"25807\",\"end\":\"26261\"},{\"start\":\"26290\",\"end\":\"27050\"},{\"start\":\"27052\",\"end\":\"27812\"},{\"start\":\"27835\",\"end\":\"29346\"},{\"start\":\"29348\",\"end\":\"30115\"},{\"start\":\"30152\",\"end\":\"30938\"},{\"start\":\"30983\",\"end\":\"32085\"},{\"start\":\"32087\",\"end\":\"32701\"},{\"start\":\"32726\",\"end\":\"34108\"},{\"start\":\"34110\",\"end\":\"34234\"},{\"start\":\"34259\",\"end\":\"34422\"},{\"start\":\"34424\",\"end\":\"34481\"},{\"start\":\"34508\",\"end\":\"34628\"},{\"start\":\"34630\",\"end\":\"35386\"},{\"start\":\"35388\",\"end\":\"36066\"},{\"start\":\"36084\",\"end\":\"37212\"},{\"start\":\"37281\",\"end\":\"37815\"},{\"start\":\"37817\",\"end\":\"38340\"},{\"start\":\"38342\",\"end\":\"38516\"},{\"start\":\"38525\",\"end\":\"39533\"},{\"start\":\"39535\",\"end\":\"41000\"},{\"start\":\"41030\",\"end\":\"41789\"},{\"start\":\"41791\",\"end\":\"43593\"},{\"start\":\"43624\",\"end\":\"44607\"},{\"start\":\"44758\",\"end\":\"45315\"},{\"start\":\"45365\",\"end\":\"45857\"},{\"start\":\"45859\",\"end\":\"46867\"},{\"start\":\"46890\",\"end\":\"47724\"},{\"start\":\"47726\",\"end\":\"48756\"},{\"start\":\"48785\",\"end\":\"49400\"},{\"start\":\"49402\",\"end\":\"51022\"},{\"start\":\"51052\",\"end\":\"51446\"},{\"start\":\"51448\",\"end\":\"53896\"},{\"start\":\"53932\",\"end\":\"58257\"},{\"start\":\"58318\",\"end\":\"59808\"},{\"start\":\"59839\",\"end\":\"60803\"},{\"start\":\"60805\",\"end\":\"62183\"},{\"start\":\"62205\",\"end\":\"62934\"}]", "formula": "[{\"start\":\"14230\",\"end\":\"14259\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"14701\",\"end\":\"14735\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"21537\",\"end\":\"21575\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"22501\",\"end\":\"22537\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"22588\",\"end\":\"22626\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"34482\",\"end\":\"34507\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"44608\",\"end\":\"44757\",\"attributes\":{\"id\":\"formula_6\"}}]", "table_ref": "[{\"start\":\"18984\",\"end\":\"18985\"},{\"start\":\"20235\",\"end\":\"20242\"},{\"start\":\"20340\",\"end\":\"20347\"},{\"start\":\"23263\",\"end\":\"23270\"},{\"start\":\"48496\",\"end\":\"48516\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"52825\",\"end\":\"52834\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"53632\",\"end\":\"53646\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"58649\",\"end\":\"58657\",\"attributes\":{\"ref_id\":\"tab_6\"}}]", "section_header": "[{\"start\":\"4736\",\"end\":\"4761\"},{\"start\":\"12791\",\"end\":\"12805\"},{\"start\":\"14867\",\"end\":\"14902\"},{\"start\":\"15864\",\"end\":\"15879\"},{\"start\":\"16322\",\"end\":\"16339\"},{\"start\":\"19240\",\"end\":\"19290\"},{\"start\":\"21313\",\"end\":\"21334\"},{\"start\":\"23032\",\"end\":\"23057\"},{\"start\":\"26264\",\"end\":\"26288\"},{\"start\":\"27815\",\"end\":\"27833\"},{\"start\":\"30118\",\"end\":\"30150\"},{\"start\":\"30941\",\"end\":\"30981\"},{\"start\":\"32704\",\"end\":\"32724\"},{\"start\":\"34237\",\"end\":\"34257\"},{\"start\":\"36069\",\"end\":\"36082\"},{\"start\":\"37215\",\"end\":\"37279\"},{\"start\":\"38519\",\"end\":\"38523\"},{\"start\":\"41003\",\"end\":\"41028\"},{\"start\":\"43596\",\"end\":\"43622\"},{\"start\":\"45318\",\"end\":\"45339\"},{\"start\":\"45342\",\"end\":\"45363\"},{\"start\":\"46870\",\"end\":\"46888\"},{\"start\":\"48759\",\"end\":\"48783\"},{\"start\":\"51025\",\"end\":\"51050\"},{\"start\":\"53899\",\"end\":\"53930\"},{\"start\":\"58260\",\"end\":\"58283\"},{\"start\":\"58286\",\"end\":\"58316\"},{\"start\":\"59811\",\"end\":\"59837\"},{\"start\":\"62186\",\"end\":\"62203\"},{\"start\":\"62936\",\"end\":\"62944\"},{\"start\":\"63052\",\"end\":\"63060\"},{\"start\":\"63504\",\"end\":\"63512\"},{\"start\":\"63610\",\"end\":\"63618\"},{\"start\":\"63685\",\"end\":\"63693\"},{\"start\":\"63735\",\"end\":\"63751\"},{\"start\":\"64251\",\"end\":\"64259\"},{\"start\":\"64333\",\"end\":\"64334\"},{\"start\":\"64817\",\"end\":\"64826\"},{\"start\":\"64923\",\"end\":\"64932\"},{\"start\":\"67414\",\"end\":\"67423\"},{\"start\":\"68095\",\"end\":\"68100\"},{\"start\":\"68103\",\"end\":\"68113\"},{\"start\":\"68534\",\"end\":\"68544\"}]", "table": "[{\"start\":\"67803\",\"end\":\"68093\"},{\"start\":\"68160\",\"end\":\"68532\"},{\"start\":\"68632\",\"end\":\"69035\"}]", "figure_caption": "[{\"start\":\"62946\",\"end\":\"63050\"},{\"start\":\"63062\",\"end\":\"63502\"},{\"start\":\"63514\",\"end\":\"63608\"},{\"start\":\"63620\",\"end\":\"63683\"},{\"start\":\"63695\",\"end\":\"63733\"},{\"start\":\"63754\",\"end\":\"64249\"},{\"start\":\"64261\",\"end\":\"64331\"},{\"start\":\"64335\",\"end\":\"64815\"},{\"start\":\"64829\",\"end\":\"64921\"},{\"start\":\"64935\",\"end\":\"67412\"},{\"start\":\"67426\",\"end\":\"67566\"},{\"start\":\"67569\",\"end\":\"67728\"},{\"start\":\"67731\",\"end\":\"67803\"},{\"start\":\"68116\",\"end\":\"68160\"},{\"start\":\"68547\",\"end\":\"68632\"}]", "figure_ref": "[{\"start\":\"11582\",\"end\":\"11591\"},{\"start\":\"12212\",\"end\":\"12221\"},{\"start\":\"14616\",\"end\":\"14624\"},{\"start\":\"16598\",\"end\":\"16606\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"17736\",\"end\":\"17744\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"23756\",\"end\":\"23764\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"24456\",\"end\":\"24465\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"24936\",\"end\":\"24945\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"25042\",\"end\":\"25050\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"26657\",\"end\":\"26665\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"27188\",\"end\":\"27196\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"27800\",\"end\":\"27810\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"28306\",\"end\":\"28314\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"33746\",\"end\":\"33754\"},{\"start\":\"34992\",\"end\":\"35000\"},{\"start\":\"35674\",\"end\":\"35685\"},{\"start\":\"35710\",\"end\":\"35718\"},{\"start\":\"36882\",\"end\":\"36890\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"37119\",\"end\":\"37128\"},{\"start\":\"37499\",\"end\":\"37509\"},{\"start\":\"38016\",\"end\":\"38027\"},{\"start\":\"39667\",\"end\":\"39676\"},{\"start\":\"40372\",\"end\":\"40381\"},{\"start\":\"41901\",\"end\":\"41910\"},{\"start\":\"43815\",\"end\":\"43824\"},{\"start\":\"47004\",\"end\":\"47013\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"47312\",\"end\":\"47321\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"49229\",\"end\":\"49238\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"52145\",\"end\":\"52155\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"54380\",\"end\":\"54389\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"54720\",\"end\":\"54730\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"55013\",\"end\":\"55023\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"56027\",\"end\":\"56037\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"56180\",\"end\":\"56190\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"57004\",\"end\":\"57013\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"57819\",\"end\":\"57828\",\"attributes\":{\"ref_id\":\"fig_4\"}}]", "bib_author_first_name": "[{\"start\":\"69105\",\"end\":\"69106\"},{\"start\":\"69114\",\"end\":\"69115\"},{\"start\":\"69123\",\"end\":\"69124\"},{\"start\":\"69517\",\"end\":\"69518\"},{\"start\":\"69527\",\"end\":\"69528\"},{\"start\":\"69538\",\"end\":\"69539\"},{\"start\":\"69552\",\"end\":\"69553\"},{\"start\":\"69560\",\"end\":\"69561\"},{\"start\":\"69570\",\"end\":\"69571\"},{\"start\":\"69583\",\"end\":\"69584\"},{\"start\":\"69591\",\"end\":\"69592\"},{\"start\":\"69606\",\"end\":\"69607\"},{\"start\":\"69616\",\"end\":\"69617\"},{\"start\":\"69995\",\"end\":\"69996\"},{\"start\":\"70005\",\"end\":\"70006\"},{\"start\":\"70013\",\"end\":\"70014\"},{\"start\":\"70023\",\"end\":\"70024\"},{\"start\":\"70036\",\"end\":\"70037\"},{\"start\":\"70046\",\"end\":\"70047\"},{\"start\":\"70055\",\"end\":\"70056\"},{\"start\":\"70066\",\"end\":\"70067\"},{\"start\":\"70078\",\"end\":\"70079\"},{\"start\":\"70090\",\"end\":\"70091\"},{\"start\":\"70413\",\"end\":\"70414\"},{\"start\":\"70427\",\"end\":\"70428\"},{\"start\":\"70440\",\"end\":\"70441\"},{\"start\":\"70442\",\"end\":\"70443\"},{\"start\":\"70739\",\"end\":\"70740\"},{\"start\":\"70745\",\"end\":\"70746\"},{\"start\":\"70754\",\"end\":\"70755\"},{\"start\":\"70761\",\"end\":\"70762\"},{\"start\":\"71143\",\"end\":\"71144\"},{\"start\":\"71152\",\"end\":\"71153\"},{\"start\":\"71163\",\"end\":\"71164\"},{\"start\":\"71171\",\"end\":\"71172\"},{\"start\":\"71173\",\"end\":\"71175\"},{\"start\":\"71444\",\"end\":\"71445\"},{\"start\":\"71457\",\"end\":\"71458\"},{\"start\":\"71467\",\"end\":\"71468\"},{\"start\":\"71717\",\"end\":\"71718\"},{\"start\":\"71724\",\"end\":\"71725\"},{\"start\":\"71726\",\"end\":\"71727\"},{\"start\":\"71734\",\"end\":\"71735\"},{\"start\":\"72019\",\"end\":\"72020\"},{\"start\":\"72027\",\"end\":\"72028\"},{\"start\":\"72029\",\"end\":\"72030\"},{\"start\":\"72037\",\"end\":\"72038\"},{\"start\":\"72047\",\"end\":\"72048\"},{\"start\":\"72520\",\"end\":\"72521\"},{\"start\":\"72753\",\"end\":\"72754\"},{\"start\":\"72914\",\"end\":\"72915\"},{\"start\":\"72924\",\"end\":\"72925\"},{\"start\":\"73173\",\"end\":\"73174\"},{\"start\":\"73181\",\"end\":\"73182\"},{\"start\":\"73194\",\"end\":\"73195\"},{\"start\":\"73202\",\"end\":\"73203\"},{\"start\":\"73613\",\"end\":\"73614\"},{\"start\":\"73626\",\"end\":\"73627\"},{\"start\":\"73635\",\"end\":\"73636\"},{\"start\":\"73648\",\"end\":\"73649\"},{\"start\":\"73985\",\"end\":\"73986\"},{\"start\":\"73992\",\"end\":\"73993\"},{\"start\":\"74001\",\"end\":\"74002\"},{\"start\":\"74012\",\"end\":\"74013\"},{\"start\":\"74370\",\"end\":\"74371\"},{\"start\":\"74382\",\"end\":\"74383\"},{\"start\":\"74391\",\"end\":\"74392\"},{\"start\":\"74657\",\"end\":\"74658\"},{\"start\":\"74668\",\"end\":\"74669\"},{\"start\":\"74962\",\"end\":\"74963\"},{\"start\":\"74971\",\"end\":\"74972\"},{\"start\":\"74983\",\"end\":\"74984\"},{\"start\":\"74989\",\"end\":\"74990\"},{\"start\":\"74995\",\"end\":\"74996\"},{\"start\":\"75374\",\"end\":\"75375\"},{\"start\":\"75382\",\"end\":\"75383\"},{\"start\":\"75389\",\"end\":\"75390\"},{\"start\":\"75753\",\"end\":\"75754\"},{\"start\":\"75762\",\"end\":\"75763\"},{\"start\":\"75773\",\"end\":\"75774\"},{\"start\":\"75782\",\"end\":\"75783\"},{\"start\":\"75790\",\"end\":\"75791\"},{\"start\":\"75797\",\"end\":\"75798\"},{\"start\":\"76158\",\"end\":\"76159\"},{\"start\":\"76167\",\"end\":\"76168\"},{\"start\":\"76177\",\"end\":\"76178\"},{\"start\":\"76185\",\"end\":\"76186\"},{\"start\":\"76195\",\"end\":\"76196\"},{\"start\":\"76197\",\"end\":\"76198\"},{\"start\":\"76630\",\"end\":\"76631\"},{\"start\":\"76636\",\"end\":\"76637\"},{\"start\":\"76645\",\"end\":\"76646\"},{\"start\":\"76655\",\"end\":\"76656\"},{\"start\":\"76661\",\"end\":\"76662\"},{\"start\":\"76671\",\"end\":\"76672\"},{\"start\":\"76679\",\"end\":\"76680\"},{\"start\":\"77000\",\"end\":\"77001\"},{\"start\":\"77006\",\"end\":\"77007\"},{\"start\":\"77008\",\"end\":\"77009\"},{\"start\":\"77014\",\"end\":\"77015\"},{\"start\":\"77024\",\"end\":\"77028\"},{\"start\":\"77033\",\"end\":\"77034\"},{\"start\":\"77042\",\"end\":\"77046\"},{\"start\":\"77052\",\"end\":\"77056\"},{\"start\":\"77276\",\"end\":\"77277\"},{\"start\":\"77278\",\"end\":\"77279\"},{\"start\":\"77285\",\"end\":\"77286\"},{\"start\":\"77287\",\"end\":\"77288\"},{\"start\":\"77296\",\"end\":\"77297\"},{\"start\":\"77961\",\"end\":\"77962\"},{\"start\":\"77980\",\"end\":\"77981\"},{\"start\":\"77987\",\"end\":\"77988\"},{\"start\":\"77989\",\"end\":\"77990\"},{\"start\":\"78003\",\"end\":\"78004\"},{\"start\":\"78517\",\"end\":\"78518\"},{\"start\":\"78526\",\"end\":\"78527\"},{\"start\":\"78739\",\"end\":\"78740\"},{\"start\":\"78754\",\"end\":\"78755\"},{\"start\":\"78756\",\"end\":\"78757\"},{\"start\":\"78765\",\"end\":\"78766\"},{\"start\":\"78778\",\"end\":\"78779\"},{\"start\":\"78788\",\"end\":\"78789\"},{\"start\":\"78798\",\"end\":\"78799\"},{\"start\":\"79094\",\"end\":\"79095\"},{\"start\":\"79100\",\"end\":\"79101\"},{\"start\":\"79109\",\"end\":\"79110\"},{\"start\":\"79119\",\"end\":\"79120\"},{\"start\":\"79381\",\"end\":\"79382\"},{\"start\":\"79393\",\"end\":\"79394\"},{\"start\":\"79409\",\"end\":\"79410\"},{\"start\":\"79417\",\"end\":\"79418\"},{\"start\":\"79429\",\"end\":\"79430\"},{\"start\":\"79445\",\"end\":\"79446\"},{\"start\":\"79456\",\"end\":\"79457\"},{\"start\":\"79464\",\"end\":\"79465\"},{\"start\":\"79845\",\"end\":\"79846\"},{\"start\":\"79861\",\"end\":\"79862\"},{\"start\":\"79872\",\"end\":\"79873\"},{\"start\":\"79888\",\"end\":\"79889\"},{\"start\":\"80180\",\"end\":\"80181\"},{\"start\":\"80197\",\"end\":\"80198\"},{\"start\":\"80199\",\"end\":\"80200\"},{\"start\":\"80213\",\"end\":\"80214\"},{\"start\":\"80220\",\"end\":\"80221\"},{\"start\":\"80653\",\"end\":\"80654\"},{\"start\":\"80663\",\"end\":\"80664\"},{\"start\":\"80674\",\"end\":\"80675\"},{\"start\":\"80676\",\"end\":\"80677\"},{\"start\":\"81156\",\"end\":\"81157\"},{\"start\":\"81167\",\"end\":\"81168\"},{\"start\":\"81184\",\"end\":\"81185\"},{\"start\":\"81670\",\"end\":\"81671\"},{\"start\":\"81672\",\"end\":\"81673\"},{\"start\":\"81678\",\"end\":\"81679\"},{\"start\":\"81684\",\"end\":\"81688\"},{\"start\":\"81696\",\"end\":\"81697\"},{\"start\":\"81706\",\"end\":\"81707\"},{\"start\":\"81708\",\"end\":\"81709\"},{\"start\":\"81718\",\"end\":\"81722\"},{\"start\":\"81723\",\"end\":\"81724\"},{\"start\":\"81731\",\"end\":\"81732\"},{\"start\":\"81733\",\"end\":\"81734\"},{\"start\":\"81745\",\"end\":\"81746\"},{\"start\":\"82186\",\"end\":\"82187\"},{\"start\":\"82195\",\"end\":\"82196\"},{\"start\":\"82202\",\"end\":\"82203\"},{\"start\":\"82211\",\"end\":\"82212\"},{\"start\":\"82223\",\"end\":\"82224\"},{\"start\":\"82230\",\"end\":\"82231\"},{\"start\":\"82244\",\"end\":\"82245\"},{\"start\":\"82663\",\"end\":\"82664\"},{\"start\":\"82674\",\"end\":\"82675\"},{\"start\":\"82683\",\"end\":\"82684\"},{\"start\":\"82695\",\"end\":\"82696\"},{\"start\":\"83134\",\"end\":\"83135\"},{\"start\":\"83144\",\"end\":\"83145\"},{\"start\":\"83146\",\"end\":\"83147\"},{\"start\":\"83152\",\"end\":\"83153\"},{\"start\":\"83158\",\"end\":\"83159\"},{\"start\":\"83160\",\"end\":\"83161\"},{\"start\":\"83170\",\"end\":\"83174\"},{\"start\":\"83175\",\"end\":\"83176\"},{\"start\":\"83183\",\"end\":\"83184\"},{\"start\":\"83185\",\"end\":\"83186\"},{\"start\":\"83197\",\"end\":\"83198\"},{\"start\":\"83663\",\"end\":\"83664\"},{\"start\":\"83674\",\"end\":\"83675\"},{\"start\":\"83682\",\"end\":\"83683\"},{\"start\":\"83691\",\"end\":\"83692\"},{\"start\":\"83700\",\"end\":\"83701\"},{\"start\":\"83702\",\"end\":\"83703\"},{\"start\":\"84090\",\"end\":\"84091\"},{\"start\":\"84099\",\"end\":\"84100\"},{\"start\":\"84455\",\"end\":\"84456\"},{\"start\":\"84462\",\"end\":\"84463\"},{\"start\":\"84471\",\"end\":\"84472\"},{\"start\":\"84859\",\"end\":\"84860\"},{\"start\":\"84870\",\"end\":\"84871\"},{\"start\":\"84879\",\"end\":\"84880\"},{\"start\":\"85167\",\"end\":\"85168\"},{\"start\":\"85178\",\"end\":\"85179\"},{\"start\":\"85195\",\"end\":\"85196\"},{\"start\":\"85672\",\"end\":\"85673\"},{\"start\":\"85679\",\"end\":\"85680\"},{\"start\":\"85686\",\"end\":\"85687\"},{\"start\":\"85688\",\"end\":\"85689\"},{\"start\":\"85930\",\"end\":\"85931\"},{\"start\":\"86092\",\"end\":\"86093\"},{\"start\":\"86101\",\"end\":\"86102\"},{\"start\":\"86110\",\"end\":\"86111\"},{\"start\":\"86118\",\"end\":\"86119\"},{\"start\":\"86397\",\"end\":\"86398\"},{\"start\":\"86406\",\"end\":\"86407\"},{\"start\":\"86417\",\"end\":\"86418\"},{\"start\":\"86427\",\"end\":\"86428\"},{\"start\":\"86437\",\"end\":\"86438\"},{\"start\":\"86445\",\"end\":\"86446\"},{\"start\":\"86454\",\"end\":\"86455\"},{\"start\":\"86456\",\"end\":\"86457\"},{\"start\":\"86610\",\"end\":\"86611\"},{\"start\":\"86621\",\"end\":\"86622\"},{\"start\":\"86630\",\"end\":\"86631\"},{\"start\":\"86888\",\"end\":\"86889\"},{\"start\":\"86898\",\"end\":\"86899\"},{\"start\":\"86906\",\"end\":\"86907\"},{\"start\":\"86917\",\"end\":\"86918\"},{\"start\":\"86926\",\"end\":\"86927\"},{\"start\":\"86928\",\"end\":\"86929\"},{\"start\":\"86935\",\"end\":\"86936\"},{\"start\":\"86943\",\"end\":\"86944\"},{\"start\":\"86953\",\"end\":\"86954\"},{\"start\":\"87369\",\"end\":\"87370\"},{\"start\":\"87377\",\"end\":\"87378\"},{\"start\":\"87385\",\"end\":\"87386\"},{\"start\":\"87398\",\"end\":\"87399\"},{\"start\":\"87406\",\"end\":\"87407\"},{\"start\":\"87412\",\"end\":\"87413\"},{\"start\":\"87421\",\"end\":\"87422\"},{\"start\":\"87820\",\"end\":\"87821\"},{\"start\":\"87835\",\"end\":\"87836\"},{\"start\":\"87845\",\"end\":\"87846\"},{\"start\":\"87855\",\"end\":\"87856\"},{\"start\":\"87867\",\"end\":\"87868\"},{\"start\":\"88309\",\"end\":\"88310\"},{\"start\":\"88315\",\"end\":\"88316\"},{\"start\":\"88327\",\"end\":\"88328\"},{\"start\":\"88340\",\"end\":\"88341\"},{\"start\":\"88350\",\"end\":\"88351\"},{\"start\":\"88357\",\"end\":\"88358\"},{\"start\":\"88743\",\"end\":\"88744\"},{\"start\":\"89364\",\"end\":\"89365\"},{\"start\":\"89366\",\"end\":\"89367\"},{\"start\":\"89380\",\"end\":\"89381\"},{\"start\":\"89390\",\"end\":\"89391\"},{\"start\":\"89401\",\"end\":\"89402\"},{\"start\":\"89403\",\"end\":\"89404\"},{\"start\":\"89769\",\"end\":\"89770\"},{\"start\":\"89776\",\"end\":\"89777\"},{\"start\":\"89785\",\"end\":\"89786\"},{\"start\":\"89787\",\"end\":\"89788\"},{\"start\":\"90263\",\"end\":\"90264\"},{\"start\":\"90274\",\"end\":\"90275\"},{\"start\":\"90663\",\"end\":\"90664\"},{\"start\":\"90672\",\"end\":\"90673\"},{\"start\":\"90680\",\"end\":\"90681\"},{\"start\":\"90690\",\"end\":\"90691\"},{\"start\":\"91055\",\"end\":\"91056\"},{\"start\":\"91064\",\"end\":\"91065\"},{\"start\":\"91074\",\"end\":\"91075\"},{\"start\":\"91086\",\"end\":\"91087\"},{\"start\":\"91093\",\"end\":\"91094\"},{\"start\":\"91545\",\"end\":\"91546\"},{\"start\":\"91556\",\"end\":\"91557\"},{\"start\":\"91565\",\"end\":\"91566\"},{\"start\":\"91846\",\"end\":\"91847\"},{\"start\":\"91855\",\"end\":\"91856\"},{\"start\":\"91865\",\"end\":\"91866\"},{\"start\":\"91874\",\"end\":\"91875\"},{\"start\":\"91886\",\"end\":\"91887\"},{\"start\":\"91892\",\"end\":\"91893\"},{\"start\":\"91902\",\"end\":\"91903\"},{\"start\":\"91913\",\"end\":\"91914\"},{\"start\":\"91915\",\"end\":\"91916\"},{\"start\":\"92335\",\"end\":\"92336\"},{\"start\":\"92344\",\"end\":\"92345\"},{\"start\":\"92353\",\"end\":\"92354\"},{\"start\":\"92362\",\"end\":\"92363\"},{\"start\":\"92377\",\"end\":\"92378\"},{\"start\":\"92388\",\"end\":\"92389\"},{\"start\":\"92390\",\"end\":\"92391\"},{\"start\":\"92400\",\"end\":\"92401\"},{\"start\":\"92892\",\"end\":\"92893\"},{\"start\":\"92903\",\"end\":\"92904\"},{\"start\":\"92913\",\"end\":\"92914\"},{\"start\":\"93281\",\"end\":\"93282\"},{\"start\":\"93290\",\"end\":\"93291\"},{\"start\":\"93301\",\"end\":\"93302\"},{\"start\":\"93313\",\"end\":\"93314\"},{\"start\":\"93324\",\"end\":\"93325\"},{\"start\":\"93338\",\"end\":\"93339\"}]", "bib_author_last_name": "[{\"start\":\"69107\",\"end\":\"69112\"},{\"start\":\"69116\",\"end\":\"69121\"},{\"start\":\"69125\",\"end\":\"69133\"},{\"start\":\"69519\",\"end\":\"69525\"},{\"start\":\"69529\",\"end\":\"69536\"},{\"start\":\"69540\",\"end\":\"69550\"},{\"start\":\"69554\",\"end\":\"69558\"},{\"start\":\"69562\",\"end\":\"69568\"},{\"start\":\"69572\",\"end\":\"69581\"},{\"start\":\"69585\",\"end\":\"69589\"},{\"start\":\"69593\",\"end\":\"69604\"},{\"start\":\"69608\",\"end\":\"69614\"},{\"start\":\"69618\",\"end\":\"69624\"},{\"start\":\"69997\",\"end\":\"70003\"},{\"start\":\"70007\",\"end\":\"70011\"},{\"start\":\"70015\",\"end\":\"70021\"},{\"start\":\"70025\",\"end\":\"70034\"},{\"start\":\"70038\",\"end\":\"70044\"},{\"start\":\"70048\",\"end\":\"70053\"},{\"start\":\"70057\",\"end\":\"70064\"},{\"start\":\"70068\",\"end\":\"70076\"},{\"start\":\"70080\",\"end\":\"70088\"},{\"start\":\"70092\",\"end\":\"70098\"},{\"start\":\"70415\",\"end\":\"70425\"},{\"start\":\"70429\",\"end\":\"70438\"},{\"start\":\"70444\",\"end\":\"70450\"},{\"start\":\"70741\",\"end\":\"70743\"},{\"start\":\"70747\",\"end\":\"70752\"},{\"start\":\"70756\",\"end\":\"70759\"},{\"start\":\"70763\",\"end\":\"70766\"},{\"start\":\"71145\",\"end\":\"71150\"},{\"start\":\"71154\",\"end\":\"71161\"},{\"start\":\"71165\",\"end\":\"71169\"},{\"start\":\"71176\",\"end\":\"71183\"},{\"start\":\"71446\",\"end\":\"71455\"},{\"start\":\"71459\",\"end\":\"71465\"},{\"start\":\"71469\",\"end\":\"71483\"},{\"start\":\"71719\",\"end\":\"71722\"},{\"start\":\"71728\",\"end\":\"71732\"},{\"start\":\"71736\",\"end\":\"71740\"},{\"start\":\"71742\",\"end\":\"71745\"},{\"start\":\"72021\",\"end\":\"72025\"},{\"start\":\"72031\",\"end\":\"72035\"},{\"start\":\"72039\",\"end\":\"72045\"},{\"start\":\"72049\",\"end\":\"72053\"},{\"start\":\"72522\",\"end\":\"72529\"},{\"start\":\"72755\",\"end\":\"72762\"},{\"start\":\"72916\",\"end\":\"72922\"},{\"start\":\"72926\",\"end\":\"72937\"},{\"start\":\"73175\",\"end\":\"73179\"},{\"start\":\"73183\",\"end\":\"73192\"},{\"start\":\"73196\",\"end\":\"73200\"},{\"start\":\"73204\",\"end\":\"73209\"},{\"start\":\"73615\",\"end\":\"73624\"},{\"start\":\"73628\",\"end\":\"73633\"},{\"start\":\"73637\",\"end\":\"73646\"},{\"start\":\"73650\",\"end\":\"73659\"},{\"start\":\"73987\",\"end\":\"73990\"},{\"start\":\"73994\",\"end\":\"73999\"},{\"start\":\"74003\",\"end\":\"74010\"},{\"start\":\"74014\",\"end\":\"74020\"},{\"start\":\"74169\",\"end\":\"74175\"},{\"start\":\"74372\",\"end\":\"74380\"},{\"start\":\"74384\",\"end\":\"74389\"},{\"start\":\"74393\",\"end\":\"74399\"},{\"start\":\"74659\",\"end\":\"74666\"},{\"start\":\"74670\",\"end\":\"74678\"},{\"start\":\"74964\",\"end\":\"74969\"},{\"start\":\"74973\",\"end\":\"74981\"},{\"start\":\"74985\",\"end\":\"74987\"},{\"start\":\"74991\",\"end\":\"74993\"},{\"start\":\"74997\",\"end\":\"75003\"},{\"start\":\"75376\",\"end\":\"75380\"},{\"start\":\"75384\",\"end\":\"75387\"},{\"start\":\"75391\",\"end\":\"75396\"},{\"start\":\"75755\",\"end\":\"75760\"},{\"start\":\"75764\",\"end\":\"75771\"},{\"start\":\"75775\",\"end\":\"75780\"},{\"start\":\"75784\",\"end\":\"75788\"},{\"start\":\"75792\",\"end\":\"75795\"},{\"start\":\"75799\",\"end\":\"75805\"},{\"start\":\"76160\",\"end\":\"76165\"},{\"start\":\"76169\",\"end\":\"76175\"},{\"start\":\"76179\",\"end\":\"76183\"},{\"start\":\"76187\",\"end\":\"76193\"},{\"start\":\"76199\",\"end\":\"76205\"},{\"start\":\"76632\",\"end\":\"76634\"},{\"start\":\"76638\",\"end\":\"76643\"},{\"start\":\"76647\",\"end\":\"76653\"},{\"start\":\"76657\",\"end\":\"76659\"},{\"start\":\"76663\",\"end\":\"76669\"},{\"start\":\"76673\",\"end\":\"76677\"},{\"start\":\"76681\",\"end\":\"76686\"},{\"start\":\"77002\",\"end\":\"77004\"},{\"start\":\"77010\",\"end\":\"77012\"},{\"start\":\"77016\",\"end\":\"77022\"},{\"start\":\"77029\",\"end\":\"77031\"},{\"start\":\"77035\",\"end\":\"77040\"},{\"start\":\"77047\",\"end\":\"77050\"},{\"start\":\"77280\",\"end\":\"77283\"},{\"start\":\"77289\",\"end\":\"77294\"},{\"start\":\"77298\",\"end\":\"77306\"},{\"start\":\"77308\",\"end\":\"77312\"},{\"start\":\"77963\",\"end\":\"77978\"},{\"start\":\"77982\",\"end\":\"77985\"},{\"start\":\"77991\",\"end\":\"78001\"},{\"start\":\"78005\",\"end\":\"78011\"},{\"start\":\"78519\",\"end\":\"78524\"},{\"start\":\"78528\",\"end\":\"78535\"},{\"start\":\"78741\",\"end\":\"78752\"},{\"start\":\"78758\",\"end\":\"78763\"},{\"start\":\"78767\",\"end\":\"78776\"},{\"start\":\"78780\",\"end\":\"78786\"},{\"start\":\"78790\",\"end\":\"78796\"},{\"start\":\"78800\",\"end\":\"78809\"},{\"start\":\"79096\",\"end\":\"79098\"},{\"start\":\"79102\",\"end\":\"79107\"},{\"start\":\"79111\",\"end\":\"79117\"},{\"start\":\"79121\",\"end\":\"79130\"},{\"start\":\"79383\",\"end\":\"79391\"},{\"start\":\"79395\",\"end\":\"79407\"},{\"start\":\"79411\",\"end\":\"79415\"},{\"start\":\"79419\",\"end\":\"79427\"},{\"start\":\"79431\",\"end\":\"79443\"},{\"start\":\"79447\",\"end\":\"79454\"},{\"start\":\"79458\",\"end\":\"79462\"},{\"start\":\"79466\",\"end\":\"79472\"},{\"start\":\"79847\",\"end\":\"79859\"},{\"start\":\"79863\",\"end\":\"79870\"},{\"start\":\"79874\",\"end\":\"79886\"},{\"start\":\"79890\",\"end\":\"79902\"},{\"start\":\"80182\",\"end\":\"80195\"},{\"start\":\"80201\",\"end\":\"80211\"},{\"start\":\"80215\",\"end\":\"80218\"},{\"start\":\"80222\",\"end\":\"80233\"},{\"start\":\"80655\",\"end\":\"80661\"},{\"start\":\"80665\",\"end\":\"80672\"},{\"start\":\"80678\",\"end\":\"80684\"},{\"start\":\"81158\",\"end\":\"81165\"},{\"start\":\"81169\",\"end\":\"81182\"},{\"start\":\"81186\",\"end\":\"81191\"},{\"start\":\"81674\",\"end\":\"81676\"},{\"start\":\"81680\",\"end\":\"81682\"},{\"start\":\"81689\",\"end\":\"81694\"},{\"start\":\"81698\",\"end\":\"81704\"},{\"start\":\"81710\",\"end\":\"81716\"},{\"start\":\"81725\",\"end\":\"81729\"},{\"start\":\"81735\",\"end\":\"81743\"},{\"start\":\"81747\",\"end\":\"81752\"},{\"start\":\"82188\",\"end\":\"82193\"},{\"start\":\"82197\",\"end\":\"82200\"},{\"start\":\"82204\",\"end\":\"82209\"},{\"start\":\"82213\",\"end\":\"82221\"},{\"start\":\"82225\",\"end\":\"82228\"},{\"start\":\"82232\",\"end\":\"82242\"},{\"start\":\"82246\",\"end\":\"82252\"},{\"start\":\"82665\",\"end\":\"82672\"},{\"start\":\"82676\",\"end\":\"82681\"},{\"start\":\"82685\",\"end\":\"82693\"},{\"start\":\"82697\",\"end\":\"82703\"},{\"start\":\"83136\",\"end\":\"83142\"},{\"start\":\"83148\",\"end\":\"83150\"},{\"start\":\"83154\",\"end\":\"83156\"},{\"start\":\"83162\",\"end\":\"83168\"},{\"start\":\"83177\",\"end\":\"83181\"},{\"start\":\"83187\",\"end\":\"83195\"},{\"start\":\"83199\",\"end\":\"83204\"},{\"start\":\"83665\",\"end\":\"83672\"},{\"start\":\"83676\",\"end\":\"83680\"},{\"start\":\"83684\",\"end\":\"83689\"},{\"start\":\"83693\",\"end\":\"83698\"},{\"start\":\"83704\",\"end\":\"83715\"},{\"start\":\"84092\",\"end\":\"84097\"},{\"start\":\"84101\",\"end\":\"84109\"},{\"start\":\"84457\",\"end\":\"84460\"},{\"start\":\"84464\",\"end\":\"84469\"},{\"start\":\"84473\",\"end\":\"84479\"},{\"start\":\"84861\",\"end\":\"84868\"},{\"start\":\"84872\",\"end\":\"84877\"},{\"start\":\"84881\",\"end\":\"84890\"},{\"start\":\"85169\",\"end\":\"85176\"},{\"start\":\"85180\",\"end\":\"85193\"},{\"start\":\"85197\",\"end\":\"85202\"},{\"start\":\"85674\",\"end\":\"85677\"},{\"start\":\"85681\",\"end\":\"85684\"},{\"start\":\"85690\",\"end\":\"85695\"},{\"start\":\"85932\",\"end\":\"85937\"},{\"start\":\"86094\",\"end\":\"86099\"},{\"start\":\"86103\",\"end\":\"86108\"},{\"start\":\"86112\",\"end\":\"86116\"},{\"start\":\"86120\",\"end\":\"86126\"},{\"start\":\"86399\",\"end\":\"86404\"},{\"start\":\"86408\",\"end\":\"86415\"},{\"start\":\"86419\",\"end\":\"86425\"},{\"start\":\"86429\",\"end\":\"86435\"},{\"start\":\"86439\",\"end\":\"86443\"},{\"start\":\"86447\",\"end\":\"86452\"},{\"start\":\"86612\",\"end\":\"86619\"},{\"start\":\"86623\",\"end\":\"86628\"},{\"start\":\"86632\",\"end\":\"86636\"},{\"start\":\"86638\",\"end\":\"86643\"},{\"start\":\"86890\",\"end\":\"86896\"},{\"start\":\"86900\",\"end\":\"86904\"},{\"start\":\"86908\",\"end\":\"86915\"},{\"start\":\"86919\",\"end\":\"86924\"},{\"start\":\"86930\",\"end\":\"86933\"},{\"start\":\"86937\",\"end\":\"86941\"},{\"start\":\"86945\",\"end\":\"86951\"},{\"start\":\"86955\",\"end\":\"86967\"},{\"start\":\"87371\",\"end\":\"87375\"},{\"start\":\"87379\",\"end\":\"87383\"},{\"start\":\"87387\",\"end\":\"87396\"},{\"start\":\"87400\",\"end\":\"87404\"},{\"start\":\"87408\",\"end\":\"87410\"},{\"start\":\"87414\",\"end\":\"87419\"},{\"start\":\"87423\",\"end\":\"87431\"},{\"start\":\"87822\",\"end\":\"87833\"},{\"start\":\"87837\",\"end\":\"87843\"},{\"start\":\"87847\",\"end\":\"87853\"},{\"start\":\"87857\",\"end\":\"87865\"},{\"start\":\"87869\",\"end\":\"87875\"},{\"start\":\"88311\",\"end\":\"88313\"},{\"start\":\"88317\",\"end\":\"88325\"},{\"start\":\"88329\",\"end\":\"88338\"},{\"start\":\"88342\",\"end\":\"88348\"},{\"start\":\"88352\",\"end\":\"88355\"},{\"start\":\"88359\",\"end\":\"88365\"},{\"start\":\"89368\",\"end\":\"89378\"},{\"start\":\"89382\",\"end\":\"89388\"},{\"start\":\"89392\",\"end\":\"89399\"},{\"start\":\"89405\",\"end\":\"89411\"},{\"start\":\"89771\",\"end\":\"89774\"},{\"start\":\"89778\",\"end\":\"89783\"},{\"start\":\"89789\",\"end\":\"89795\"},{\"start\":\"90265\",\"end\":\"90272\"},{\"start\":\"90276\",\"end\":\"90284\"},{\"start\":\"90665\",\"end\":\"90670\"},{\"start\":\"90674\",\"end\":\"90678\"},{\"start\":\"90682\",\"end\":\"90688\"},{\"start\":\"90692\",\"end\":\"90698\"},{\"start\":\"91057\",\"end\":\"91062\"},{\"start\":\"91066\",\"end\":\"91072\"},{\"start\":\"91076\",\"end\":\"91084\"},{\"start\":\"91088\",\"end\":\"91091\"},{\"start\":\"91095\",\"end\":\"91099\"},{\"start\":\"91101\",\"end\":\"91107\"},{\"start\":\"91547\",\"end\":\"91554\"},{\"start\":\"91558\",\"end\":\"91563\"},{\"start\":\"91567\",\"end\":\"91573\"},{\"start\":\"91848\",\"end\":\"91853\"},{\"start\":\"91857\",\"end\":\"91863\"},{\"start\":\"91867\",\"end\":\"91872\"},{\"start\":\"91876\",\"end\":\"91884\"},{\"start\":\"91888\",\"end\":\"91890\"},{\"start\":\"91894\",\"end\":\"91900\"},{\"start\":\"91904\",\"end\":\"91911\"},{\"start\":\"91917\",\"end\":\"91923\"},{\"start\":\"92337\",\"end\":\"92342\"},{\"start\":\"92346\",\"end\":\"92351\"},{\"start\":\"92355\",\"end\":\"92360\"},{\"start\":\"92364\",\"end\":\"92375\"},{\"start\":\"92379\",\"end\":\"92386\"},{\"start\":\"92392\",\"end\":\"92398\"},{\"start\":\"92402\",\"end\":\"92408\"},{\"start\":\"92894\",\"end\":\"92901\"},{\"start\":\"92905\",\"end\":\"92911\"},{\"start\":\"92915\",\"end\":\"92921\"},{\"start\":\"93283\",\"end\":\"93288\"},{\"start\":\"93292\",\"end\":\"93299\"},{\"start\":\"93303\",\"end\":\"93311\"},{\"start\":\"93315\",\"end\":\"93322\"},{\"start\":\"93326\",\"end\":\"93336\"},{\"start\":\"93340\",\"end\":\"93346\"}]", "bib_entry": "[{\"start\":\"69037\",\"end\":\"69445\",\"attributes\":{\"matched_paper_id\":\"10991044\",\"id\":\"b0\"}},{\"start\":\"69447\",\"end\":\"69938\",\"attributes\":{\"matched_paper_id\":\"11590585\",\"id\":\"b1\"}},{\"start\":\"69940\",\"end\":\"70346\",\"attributes\":{\"id\":\"b2\",\"doi\":\"arXiv:1412.5567\"}},{\"start\":\"70348\",\"end\":\"70691\",\"attributes\":{\"matched_paper_id\":\"195908774\",\"id\":\"b3\"}},{\"start\":\"70693\",\"end\":\"71100\",\"attributes\":{\"matched_paper_id\":\"206594692\",\"id\":\"b4\"}},{\"start\":\"71102\",\"end\":\"71407\",\"attributes\":{\"id\":\"b5\"}},{\"start\":\"71409\",\"end\":\"71636\",\"attributes\":{\"id\":\"b6\",\"doi\":\"arXiv:1301.0159\"}},{\"start\":\"71638\",\"end\":\"71921\",\"attributes\":{\"matched_paper_id\":\"10167346\",\"id\":\"b7\"}},{\"start\":\"71923\",\"end\":\"72393\",\"attributes\":{\"matched_paper_id\":\"2390993\",\"id\":\"b8\"}},{\"start\":\"72395\",\"end\":\"72751\",\"attributes\":{\"matched_paper_id\":\"733980\",\"id\":\"b9\"}},{\"start\":\"72753\",\"end\":\"72859\",\"attributes\":{\"id\":\"b10\"}},{\"start\":\"72861\",\"end\":\"73079\",\"attributes\":{\"matched_paper_id\":\"7108827\",\"id\":\"b11\"}},{\"start\":\"73081\",\"end\":\"73510\",\"attributes\":{\"matched_paper_id\":\"236151316\",\"id\":\"b12\"}},{\"start\":\"73512\",\"end\":\"73897\",\"attributes\":{\"matched_paper_id\":\"182118830\",\"id\":\"b13\"}},{\"start\":\"73899\",\"end\":\"74167\",\"attributes\":{\"id\":\"b14\"}},{\"start\":\"74169\",\"end\":\"74312\",\"attributes\":{\"id\":\"b15\"}},{\"start\":\"74314\",\"end\":\"74566\",\"attributes\":{\"id\":\"b16\",\"doi\":\"arXiv:2005.06716\"}},{\"start\":\"74568\",\"end\":\"74900\",\"attributes\":{\"matched_paper_id\":\"1690456\",\"id\":\"b17\"}},{\"start\":\"74902\",\"end\":\"75297\",\"attributes\":{\"matched_paper_id\":\"155109576\",\"id\":\"b18\"}},{\"start\":\"75299\",\"end\":\"75665\",\"attributes\":{\"matched_paper_id\":\"236151330\",\"id\":\"b19\"}},{\"start\":\"75667\",\"end\":\"76109\",\"attributes\":{\"matched_paper_id\":\"222305243\",\"id\":\"b20\"}},{\"start\":\"76111\",\"end\":\"76513\",\"attributes\":{\"matched_paper_id\":\"1677864\",\"id\":\"b21\"}},{\"start\":\"76515\",\"end\":\"76996\",\"attributes\":{\"matched_paper_id\":\"3869844\",\"id\":\"b22\"}},{\"start\":\"76998\",\"end\":\"77124\",\"attributes\":{\"id\":\"b23\"}},{\"start\":\"77126\",\"end\":\"77638\",\"attributes\":{\"matched_paper_id\":\"25209638\",\"id\":\"b24\"}},{\"start\":\"77640\",\"end\":\"77863\",\"attributes\":{\"id\":\"b25\"}},{\"start\":\"77865\",\"end\":\"78241\",\"attributes\":{\"id\":\"b26\",\"doi\":\"arXiv:1812.02788\"}},{\"start\":\"78243\",\"end\":\"78410\",\"attributes\":{\"id\":\"b27\"}},{\"start\":\"78412\",\"end\":\"78737\",\"attributes\":{\"id\":\"b28\",\"doi\":\"arXiv:1503.07469\"}},{\"start\":\"78739\",\"end\":\"79038\",\"attributes\":{\"id\":\"b29\",\"doi\":\"arXiv:1906.01548\"}},{\"start\":\"79040\",\"end\":\"79302\",\"attributes\":{\"id\":\"b30\",\"doi\":\"arXiv:1804.01756\"}},{\"start\":\"79304\",\"end\":\"79775\",\"attributes\":{\"matched_paper_id\":\"2050688\",\"id\":\"b31\"}},{\"start\":\"79777\",\"end\":\"80116\",\"attributes\":{\"matched_paper_id\":\"22745143\",\"id\":\"b32\"}},{\"start\":\"80118\",\"end\":\"80562\",\"attributes\":{\"matched_paper_id\":\"11940714\",\"id\":\"b33\"}},{\"start\":\"80564\",\"end\":\"81092\",\"attributes\":{\"matched_paper_id\":\"9812826\",\"id\":\"b34\"}},{\"start\":\"81094\",\"end\":\"81553\",\"attributes\":{\"matched_paper_id\":\"60571601\",\"id\":\"b35\"}},{\"start\":\"81555\",\"end\":\"82111\",\"attributes\":{\"matched_paper_id\":\"3869844\",\"id\":\"b36\"}},{\"start\":\"82113\",\"end\":\"82576\",\"attributes\":{\"matched_paper_id\":\"197642766\",\"id\":\"b37\"}},{\"start\":\"82578\",\"end\":\"83132\",\"attributes\":{\"matched_paper_id\":\"67872077\",\"id\":\"b38\"}},{\"start\":\"83134\",\"end\":\"83441\",\"attributes\":{\"id\":\"b39\",\"doi\":\"arXiv:1811.09557\"}},{\"start\":\"83443\",\"end\":\"83558\",\"attributes\":{\"id\":\"b40\"}},{\"start\":\"83560\",\"end\":\"84025\",\"attributes\":{\"id\":\"b41\"}},{\"start\":\"84027\",\"end\":\"84363\",\"attributes\":{\"matched_paper_id\":\"10337279\",\"id\":\"b42\"}},{\"start\":\"84365\",\"end\":\"84775\",\"attributes\":{\"matched_paper_id\":\"4701912\",\"id\":\"b43\"}},{\"start\":\"84777\",\"end\":\"85103\",\"attributes\":{\"matched_paper_id\":\"8728742\",\"id\":\"b44\"}},{\"start\":\"85105\",\"end\":\"85564\",\"attributes\":{\"matched_paper_id\":\"60571601\",\"id\":\"b45\"}},{\"start\":\"85566\",\"end\":\"85907\",\"attributes\":{\"id\":\"b46\",\"doi\":\"arXiv:1510.00149\"}},{\"start\":\"85909\",\"end\":\"86012\",\"attributes\":{\"matched_paper_id\":\"110511037\",\"id\":\"b47\"}},{\"start\":\"86014\",\"end\":\"86267\",\"attributes\":{\"id\":\"b48\"}},{\"start\":\"86269\",\"end\":\"86393\",\"attributes\":{\"id\":\"b49\"}},{\"start\":\"86395\",\"end\":\"86529\",\"attributes\":{\"id\":\"b50\"}},{\"start\":\"86531\",\"end\":\"86841\",\"attributes\":{\"id\":\"b51\",\"doi\":\"arXiv:1603.04467\"}},{\"start\":\"86843\",\"end\":\"87293\",\"attributes\":{\"matched_paper_id\":\"525898\",\"id\":\"b52\"}},{\"start\":\"87295\",\"end\":\"87818\",\"attributes\":{\"matched_paper_id\":\"21751865\",\"id\":\"b53\"}},{\"start\":\"87820\",\"end\":\"88234\",\"attributes\":{\"id\":\"b54\",\"doi\":\"arXiv:1602.02830\"}},{\"start\":\"88236\",\"end\":\"88509\",\"attributes\":{\"id\":\"b55\"}},{\"start\":\"88511\",\"end\":\"88630\",\"attributes\":{\"id\":\"b56\"}},{\"start\":\"88632\",\"end\":\"88876\",\"attributes\":{\"id\":\"b57\"}},{\"start\":\"88878\",\"end\":\"89309\",\"attributes\":{\"matched_paper_id\":\"7149851\",\"id\":\"b58\"}},{\"start\":\"89311\",\"end\":\"89695\",\"attributes\":{\"matched_paper_id\":\"201625025\",\"id\":\"b59\"}},{\"start\":\"89697\",\"end\":\"90139\",\"attributes\":{\"matched_paper_id\":\"52978766\",\"id\":\"b60\"}},{\"start\":\"90141\",\"end\":\"90590\",\"attributes\":{\"matched_paper_id\":\"15258913\",\"id\":\"b61\"}},{\"start\":\"90592\",\"end\":\"90958\",\"attributes\":{\"matched_paper_id\":\"21351739\",\"id\":\"b62\"}},{\"start\":\"90960\",\"end\":\"91459\",\"attributes\":{\"matched_paper_id\":\"163164623\",\"id\":\"b63\"}},{\"start\":\"91461\",\"end\":\"91770\",\"attributes\":{\"matched_paper_id\":\"218934679\",\"id\":\"b64\"}},{\"start\":\"91772\",\"end\":\"92267\",\"attributes\":{\"matched_paper_id\":\"227069231\",\"id\":\"b65\"}},{\"start\":\"92269\",\"end\":\"92730\",\"attributes\":{\"matched_paper_id\":\"211016154\",\"id\":\"b66\"}},{\"start\":\"92732\",\"end\":\"93192\",\"attributes\":{\"id\":\"b67\",\"doi\":\"arXiv:1807.08583\"}},{\"start\":\"93194\",\"end\":\"93751\",\"attributes\":{\"matched_paper_id\":\"189824904\",\"id\":\"b68\"}}]", "bib_title": "[{\"start\":\"69037\",\"end\":\"69103\"},{\"start\":\"69447\",\"end\":\"69515\"},{\"start\":\"70348\",\"end\":\"70411\"},{\"start\":\"70693\",\"end\":\"70737\"},{\"start\":\"71102\",\"end\":\"71141\"},{\"start\":\"71638\",\"end\":\"71715\"},{\"start\":\"71923\",\"end\":\"72017\"},{\"start\":\"72395\",\"end\":\"72518\"},{\"start\":\"72861\",\"end\":\"72912\"},{\"start\":\"73081\",\"end\":\"73171\"},{\"start\":\"73512\",\"end\":\"73611\"},{\"start\":\"74568\",\"end\":\"74655\"},{\"start\":\"74902\",\"end\":\"74960\"},{\"start\":\"75299\",\"end\":\"75372\"},{\"start\":\"75667\",\"end\":\"75751\"},{\"start\":\"76111\",\"end\":\"76156\"},{\"start\":\"76515\",\"end\":\"76628\"},{\"start\":\"77126\",\"end\":\"77274\"},{\"start\":\"79304\",\"end\":\"79379\"},{\"start\":\"79777\",\"end\":\"79843\"},{\"start\":\"80118\",\"end\":\"80178\"},{\"start\":\"80564\",\"end\":\"80651\"},{\"start\":\"81094\",\"end\":\"81154\"},{\"start\":\"81555\",\"end\":\"81668\"},{\"start\":\"82113\",\"end\":\"82184\"},{\"start\":\"82578\",\"end\":\"82661\"},{\"start\":\"84027\",\"end\":\"84088\"},{\"start\":\"84365\",\"end\":\"84453\"},{\"start\":\"84777\",\"end\":\"84857\"},{\"start\":\"85105\",\"end\":\"85165\"},{\"start\":\"85909\",\"end\":\"85928\"},{\"start\":\"86843\",\"end\":\"86886\"},{\"start\":\"87295\",\"end\":\"87367\"},{\"start\":\"88878\",\"end\":\"88974\"},{\"start\":\"89311\",\"end\":\"89362\"},{\"start\":\"89697\",\"end\":\"89767\"},{\"start\":\"90141\",\"end\":\"90261\"},{\"start\":\"90592\",\"end\":\"90661\"},{\"start\":\"90960\",\"end\":\"91053\"},{\"start\":\"91461\",\"end\":\"91543\"},{\"start\":\"91772\",\"end\":\"91844\"},{\"start\":\"92269\",\"end\":\"92333\"},{\"start\":\"93194\",\"end\":\"93279\"}]", "bib_author": "[{\"start\":\"69105\",\"end\":\"69114\"},{\"start\":\"69114\",\"end\":\"69123\"},{\"start\":\"69123\",\"end\":\"69135\"},{\"start\":\"69517\",\"end\":\"69527\"},{\"start\":\"69527\",\"end\":\"69538\"},{\"start\":\"69538\",\"end\":\"69552\"},{\"start\":\"69552\",\"end\":\"69560\"},{\"start\":\"69560\",\"end\":\"69570\"},{\"start\":\"69570\",\"end\":\"69583\"},{\"start\":\"69583\",\"end\":\"69591\"},{\"start\":\"69591\",\"end\":\"69606\"},{\"start\":\"69606\",\"end\":\"69616\"},{\"start\":\"69616\",\"end\":\"69626\"},{\"start\":\"69995\",\"end\":\"70005\"},{\"start\":\"70005\",\"end\":\"70013\"},{\"start\":\"70013\",\"end\":\"70023\"},{\"start\":\"70023\",\"end\":\"70036\"},{\"start\":\"70036\",\"end\":\"70046\"},{\"start\":\"70046\",\"end\":\"70055\"},{\"start\":\"70055\",\"end\":\"70066\"},{\"start\":\"70066\",\"end\":\"70078\"},{\"start\":\"70078\",\"end\":\"70090\"},{\"start\":\"70090\",\"end\":\"70100\"},{\"start\":\"70413\",\"end\":\"70427\"},{\"start\":\"70427\",\"end\":\"70440\"},{\"start\":\"70440\",\"end\":\"70452\"},{\"start\":\"70739\",\"end\":\"70745\"},{\"start\":\"70745\",\"end\":\"70754\"},{\"start\":\"70754\",\"end\":\"70761\"},{\"start\":\"70761\",\"end\":\"70768\"},{\"start\":\"71143\",\"end\":\"71152\"},{\"start\":\"71152\",\"end\":\"71163\"},{\"start\":\"71163\",\"end\":\"71171\"},{\"start\":\"71171\",\"end\":\"71185\"},{\"start\":\"71444\",\"end\":\"71457\"},{\"start\":\"71457\",\"end\":\"71467\"},{\"start\":\"71467\",\"end\":\"71485\"},{\"start\":\"71717\",\"end\":\"71724\"},{\"start\":\"71724\",\"end\":\"71734\"},{\"start\":\"71734\",\"end\":\"71742\"},{\"start\":\"71742\",\"end\":\"71747\"},{\"start\":\"72019\",\"end\":\"72027\"},{\"start\":\"72027\",\"end\":\"72037\"},{\"start\":\"72037\",\"end\":\"72047\"},{\"start\":\"72047\",\"end\":\"72055\"},{\"start\":\"72520\",\"end\":\"72531\"},{\"start\":\"72753\",\"end\":\"72764\"},{\"start\":\"72914\",\"end\":\"72924\"},{\"start\":\"72924\",\"end\":\"72939\"},{\"start\":\"73173\",\"end\":\"73181\"},{\"start\":\"73181\",\"end\":\"73194\"},{\"start\":\"73194\",\"end\":\"73202\"},{\"start\":\"73202\",\"end\":\"73211\"},{\"start\":\"73613\",\"end\":\"73626\"},{\"start\":\"73626\",\"end\":\"73635\"},{\"start\":\"73635\",\"end\":\"73648\"},{\"start\":\"73648\",\"end\":\"73661\"},{\"start\":\"73985\",\"end\":\"73992\"},{\"start\":\"73992\",\"end\":\"74001\"},{\"start\":\"74001\",\"end\":\"74012\"},{\"start\":\"74012\",\"end\":\"74022\"},{\"start\":\"74169\",\"end\":\"74177\"},{\"start\":\"74370\",\"end\":\"74382\"},{\"start\":\"74382\",\"end\":\"74391\"},{\"start\":\"74391\",\"end\":\"74401\"},{\"start\":\"74657\",\"end\":\"74668\"},{\"start\":\"74668\",\"end\":\"74680\"},{\"start\":\"74962\",\"end\":\"74971\"},{\"start\":\"74971\",\"end\":\"74983\"},{\"start\":\"74983\",\"end\":\"74989\"},{\"start\":\"74989\",\"end\":\"74995\"},{\"start\":\"74995\",\"end\":\"75005\"},{\"start\":\"75374\",\"end\":\"75382\"},{\"start\":\"75382\",\"end\":\"75389\"},{\"start\":\"75389\",\"end\":\"75398\"},{\"start\":\"75753\",\"end\":\"75762\"},{\"start\":\"75762\",\"end\":\"75773\"},{\"start\":\"75773\",\"end\":\"75782\"},{\"start\":\"75782\",\"end\":\"75790\"},{\"start\":\"75790\",\"end\":\"75797\"},{\"start\":\"75797\",\"end\":\"75807\"},{\"start\":\"76158\",\"end\":\"76167\"},{\"start\":\"76167\",\"end\":\"76177\"},{\"start\":\"76177\",\"end\":\"76185\"},{\"start\":\"76185\",\"end\":\"76195\"},{\"start\":\"76195\",\"end\":\"76207\"},{\"start\":\"76630\",\"end\":\"76636\"},{\"start\":\"76636\",\"end\":\"76645\"},{\"start\":\"76645\",\"end\":\"76655\"},{\"start\":\"76655\",\"end\":\"76661\"},{\"start\":\"76661\",\"end\":\"76671\"},{\"start\":\"76671\",\"end\":\"76679\"},{\"start\":\"76679\",\"end\":\"76688\"},{\"start\":\"77000\",\"end\":\"77006\"},{\"start\":\"77006\",\"end\":\"77014\"},{\"start\":\"77014\",\"end\":\"77024\"},{\"start\":\"77024\",\"end\":\"77033\"},{\"start\":\"77033\",\"end\":\"77042\"},{\"start\":\"77042\",\"end\":\"77052\"},{\"start\":\"77052\",\"end\":\"77059\"},{\"start\":\"77276\",\"end\":\"77285\"},{\"start\":\"77285\",\"end\":\"77296\"},{\"start\":\"77296\",\"end\":\"77308\"},{\"start\":\"77308\",\"end\":\"77314\"},{\"start\":\"77961\",\"end\":\"77980\"},{\"start\":\"77980\",\"end\":\"77987\"},{\"start\":\"77987\",\"end\":\"78003\"},{\"start\":\"78003\",\"end\":\"78013\"},{\"start\":\"78517\",\"end\":\"78526\"},{\"start\":\"78526\",\"end\":\"78537\"},{\"start\":\"78739\",\"end\":\"78754\"},{\"start\":\"78754\",\"end\":\"78765\"},{\"start\":\"78765\",\"end\":\"78778\"},{\"start\":\"78778\",\"end\":\"78788\"},{\"start\":\"78788\",\"end\":\"78798\"},{\"start\":\"78798\",\"end\":\"78811\"},{\"start\":\"79094\",\"end\":\"79100\"},{\"start\":\"79100\",\"end\":\"79109\"},{\"start\":\"79109\",\"end\":\"79119\"},{\"start\":\"79119\",\"end\":\"79132\"},{\"start\":\"79381\",\"end\":\"79393\"},{\"start\":\"79393\",\"end\":\"79409\"},{\"start\":\"79409\",\"end\":\"79417\"},{\"start\":\"79417\",\"end\":\"79429\"},{\"start\":\"79429\",\"end\":\"79445\"},{\"start\":\"79445\",\"end\":\"79456\"},{\"start\":\"79456\",\"end\":\"79464\"},{\"start\":\"79464\",\"end\":\"79474\"},{\"start\":\"79845\",\"end\":\"79861\"},{\"start\":\"79861\",\"end\":\"79872\"},{\"start\":\"79872\",\"end\":\"79888\"},{\"start\":\"79888\",\"end\":\"79904\"},{\"start\":\"80180\",\"end\":\"80197\"},{\"start\":\"80197\",\"end\":\"80213\"},{\"start\":\"80213\",\"end\":\"80220\"},{\"start\":\"80220\",\"end\":\"80235\"},{\"start\":\"80653\",\"end\":\"80663\"},{\"start\":\"80663\",\"end\":\"80674\"},{\"start\":\"80674\",\"end\":\"80686\"},{\"start\":\"81156\",\"end\":\"81167\"},{\"start\":\"81167\",\"end\":\"81184\"},{\"start\":\"81184\",\"end\":\"81193\"},{\"start\":\"81670\",\"end\":\"81678\"},{\"start\":\"81678\",\"end\":\"81684\"},{\"start\":\"81684\",\"end\":\"81696\"},{\"start\":\"81696\",\"end\":\"81706\"},{\"start\":\"81706\",\"end\":\"81718\"},{\"start\":\"81718\",\"end\":\"81731\"},{\"start\":\"81731\",\"end\":\"81745\"},{\"start\":\"81745\",\"end\":\"81754\"},{\"start\":\"82186\",\"end\":\"82195\"},{\"start\":\"82195\",\"end\":\"82202\"},{\"start\":\"82202\",\"end\":\"82211\"},{\"start\":\"82211\",\"end\":\"82223\"},{\"start\":\"82223\",\"end\":\"82230\"},{\"start\":\"82230\",\"end\":\"82244\"},{\"start\":\"82244\",\"end\":\"82254\"},{\"start\":\"82663\",\"end\":\"82674\"},{\"start\":\"82674\",\"end\":\"82683\"},{\"start\":\"82683\",\"end\":\"82695\"},{\"start\":\"82695\",\"end\":\"82705\"},{\"start\":\"83134\",\"end\":\"83144\"},{\"start\":\"83144\",\"end\":\"83152\"},{\"start\":\"83152\",\"end\":\"83158\"},{\"start\":\"83158\",\"end\":\"83170\"},{\"start\":\"83170\",\"end\":\"83183\"},{\"start\":\"83183\",\"end\":\"83197\"},{\"start\":\"83197\",\"end\":\"83206\"},{\"start\":\"83663\",\"end\":\"83674\"},{\"start\":\"83674\",\"end\":\"83682\"},{\"start\":\"83682\",\"end\":\"83691\"},{\"start\":\"83691\",\"end\":\"83700\"},{\"start\":\"83700\",\"end\":\"83717\"},{\"start\":\"84090\",\"end\":\"84099\"},{\"start\":\"84099\",\"end\":\"84111\"},{\"start\":\"84455\",\"end\":\"84462\"},{\"start\":\"84462\",\"end\":\"84471\"},{\"start\":\"84471\",\"end\":\"84481\"},{\"start\":\"84859\",\"end\":\"84870\"},{\"start\":\"84870\",\"end\":\"84879\"},{\"start\":\"84879\",\"end\":\"84892\"},{\"start\":\"85167\",\"end\":\"85178\"},{\"start\":\"85178\",\"end\":\"85195\"},{\"start\":\"85195\",\"end\":\"85204\"},{\"start\":\"85672\",\"end\":\"85679\"},{\"start\":\"85679\",\"end\":\"85686\"},{\"start\":\"85686\",\"end\":\"85697\"},{\"start\":\"85930\",\"end\":\"85939\"},{\"start\":\"86092\",\"end\":\"86101\"},{\"start\":\"86101\",\"end\":\"86110\"},{\"start\":\"86110\",\"end\":\"86118\"},{\"start\":\"86118\",\"end\":\"86128\"},{\"start\":\"86397\",\"end\":\"86406\"},{\"start\":\"86406\",\"end\":\"86417\"},{\"start\":\"86417\",\"end\":\"86427\"},{\"start\":\"86427\",\"end\":\"86437\"},{\"start\":\"86437\",\"end\":\"86445\"},{\"start\":\"86445\",\"end\":\"86454\"},{\"start\":\"86454\",\"end\":\"86460\"},{\"start\":\"86610\",\"end\":\"86621\"},{\"start\":\"86621\",\"end\":\"86630\"},{\"start\":\"86630\",\"end\":\"86638\"},{\"start\":\"86638\",\"end\":\"86645\"},{\"start\":\"86888\",\"end\":\"86898\"},{\"start\":\"86898\",\"end\":\"86906\"},{\"start\":\"86906\",\"end\":\"86917\"},{\"start\":\"86917\",\"end\":\"86926\"},{\"start\":\"86926\",\"end\":\"86935\"},{\"start\":\"86935\",\"end\":\"86943\"},{\"start\":\"86943\",\"end\":\"86953\"},{\"start\":\"86953\",\"end\":\"86969\"},{\"start\":\"87369\",\"end\":\"87377\"},{\"start\":\"87377\",\"end\":\"87385\"},{\"start\":\"87385\",\"end\":\"87398\"},{\"start\":\"87398\",\"end\":\"87406\"},{\"start\":\"87406\",\"end\":\"87412\"},{\"start\":\"87412\",\"end\":\"87421\"},{\"start\":\"87421\",\"end\":\"87433\"},{\"start\":\"87820\",\"end\":\"87835\"},{\"start\":\"87835\",\"end\":\"87845\"},{\"start\":\"87845\",\"end\":\"87855\"},{\"start\":\"87855\",\"end\":\"87867\"},{\"start\":\"87867\",\"end\":\"87877\"},{\"start\":\"88309\",\"end\":\"88315\"},{\"start\":\"88315\",\"end\":\"88327\"},{\"start\":\"88327\",\"end\":\"88340\"},{\"start\":\"88340\",\"end\":\"88350\"},{\"start\":\"88350\",\"end\":\"88357\"},{\"start\":\"88357\",\"end\":\"88367\"},{\"start\":\"88743\",\"end\":\"88747\"},{\"start\":\"89364\",\"end\":\"89380\"},{\"start\":\"89380\",\"end\":\"89390\"},{\"start\":\"89390\",\"end\":\"89401\"},{\"start\":\"89401\",\"end\":\"89413\"},{\"start\":\"89769\",\"end\":\"89776\"},{\"start\":\"89776\",\"end\":\"89785\"},{\"start\":\"89785\",\"end\":\"89797\"},{\"start\":\"90263\",\"end\":\"90274\"},{\"start\":\"90274\",\"end\":\"90286\"},{\"start\":\"90663\",\"end\":\"90672\"},{\"start\":\"90672\",\"end\":\"90680\"},{\"start\":\"90680\",\"end\":\"90690\"},{\"start\":\"90690\",\"end\":\"90700\"},{\"start\":\"91055\",\"end\":\"91064\"},{\"start\":\"91064\",\"end\":\"91074\"},{\"start\":\"91074\",\"end\":\"91086\"},{\"start\":\"91086\",\"end\":\"91093\"},{\"start\":\"91093\",\"end\":\"91101\"},{\"start\":\"91101\",\"end\":\"91109\"},{\"start\":\"91545\",\"end\":\"91556\"},{\"start\":\"91556\",\"end\":\"91565\"},{\"start\":\"91565\",\"end\":\"91575\"},{\"start\":\"91846\",\"end\":\"91855\"},{\"start\":\"91855\",\"end\":\"91865\"},{\"start\":\"91865\",\"end\":\"91874\"},{\"start\":\"91874\",\"end\":\"91886\"},{\"start\":\"91886\",\"end\":\"91892\"},{\"start\":\"91892\",\"end\":\"91902\"},{\"start\":\"91902\",\"end\":\"91913\"},{\"start\":\"91913\",\"end\":\"91925\"},{\"start\":\"92335\",\"end\":\"92344\"},{\"start\":\"92344\",\"end\":\"92353\"},{\"start\":\"92353\",\"end\":\"92362\"},{\"start\":\"92362\",\"end\":\"92377\"},{\"start\":\"92377\",\"end\":\"92388\"},{\"start\":\"92388\",\"end\":\"92400\"},{\"start\":\"92400\",\"end\":\"92410\"},{\"start\":\"92892\",\"end\":\"92903\"},{\"start\":\"92903\",\"end\":\"92913\"},{\"start\":\"92913\",\"end\":\"92923\"},{\"start\":\"93281\",\"end\":\"93290\"},{\"start\":\"93290\",\"end\":\"93301\"},{\"start\":\"93301\",\"end\":\"93313\"},{\"start\":\"93313\",\"end\":\"93324\"},{\"start\":\"93324\",\"end\":\"93338\"},{\"start\":\"93338\",\"end\":\"93348\"}]", "bib_venue": "[{\"start\":\"70848\",\"end\":\"70914\"},{\"start\":\"80297\",\"end\":\"80345\"},{\"start\":\"80772\",\"end\":\"80844\"},{\"start\":\"81269\",\"end\":\"81328\"},{\"start\":\"82798\",\"end\":\"82874\"},{\"start\":\"85280\",\"end\":\"85339\"},{\"start\":\"89873\",\"end\":\"89935\"},{\"start\":\"91170\",\"end\":\"91214\"},{\"start\":\"69135\",\"end\":\"69218\"},{\"start\":\"69626\",\"end\":\"69670\"},{\"start\":\"69940\",\"end\":\"69993\"},{\"start\":\"70452\",\"end\":\"70501\"},{\"start\":\"70768\",\"end\":\"70846\"},{\"start\":\"71185\",\"end\":\"71234\"},{\"start\":\"71409\",\"end\":\"71442\"},{\"start\":\"71747\",\"end\":\"71758\"},{\"start\":\"72055\",\"end\":\"72135\"},{\"start\":\"72531\",\"end\":\"72552\"},{\"start\":\"72764\",\"end\":\"72801\"},{\"start\":\"72939\",\"end\":\"72945\"},{\"start\":\"73211\",\"end\":\"73277\"},{\"start\":\"73661\",\"end\":\"73677\"},{\"start\":\"73899\",\"end\":\"73983\"},{\"start\":\"74177\",\"end\":\"74214\"},{\"start\":\"74314\",\"end\":\"74368\"},{\"start\":\"74680\",\"end\":\"74710\"},{\"start\":\"75005\",\"end\":\"75076\"},{\"start\":\"75398\",\"end\":\"75464\"},{\"start\":\"75807\",\"end\":\"75864\"},{\"start\":\"76207\",\"end\":\"76251\"},{\"start\":\"76688\",\"end\":\"76738\"},{\"start\":\"77314\",\"end\":\"77345\"},{\"start\":\"77640\",\"end\":\"77732\"},{\"start\":\"77865\",\"end\":\"77959\"},{\"start\":\"78243\",\"end\":\"78313\"},{\"start\":\"78412\",\"end\":\"78515\"},{\"start\":\"78827\",\"end\":\"78861\"},{\"start\":\"79040\",\"end\":\"79092\"},{\"start\":\"79474\",\"end\":\"79512\"},{\"start\":\"79904\",\"end\":\"79922\"},{\"start\":\"80235\",\"end\":\"80295\"},{\"start\":\"80686\",\"end\":\"80770\"},{\"start\":\"81193\",\"end\":\"81267\"},{\"start\":\"81754\",\"end\":\"81793\"},{\"start\":\"82254\",\"end\":\"82320\"},{\"start\":\"82705\",\"end\":\"82796\"},{\"start\":\"83222\",\"end\":\"83259\"},{\"start\":\"83443\",\"end\":\"83474\"},{\"start\":\"83560\",\"end\":\"83661\"},{\"start\":\"84111\",\"end\":\"84174\"},{\"start\":\"84481\",\"end\":\"84522\"},{\"start\":\"84892\",\"end\":\"84916\"},{\"start\":\"85204\",\"end\":\"85278\"},{\"start\":\"85566\",\"end\":\"85670\"},{\"start\":\"85939\",\"end\":\"85950\"},{\"start\":\"86014\",\"end\":\"86090\"},{\"start\":\"86269\",\"end\":\"86317\"},{\"start\":\"86531\",\"end\":\"86608\"},{\"start\":\"86969\",\"end\":\"87016\"},{\"start\":\"87433\",\"end\":\"87533\"},{\"start\":\"87893\",\"end\":\"88001\"},{\"start\":\"88236\",\"end\":\"88307\"},{\"start\":\"88513\",\"end\":\"88551\"},{\"start\":\"88632\",\"end\":\"88741\"},{\"start\":\"88976\",\"end\":\"89064\"},{\"start\":\"89413\",\"end\":\"89419\"},{\"start\":\"89797\",\"end\":\"89871\"},{\"start\":\"90286\",\"end\":\"90343\"},{\"start\":\"90700\",\"end\":\"90754\"},{\"start\":\"91109\",\"end\":\"91168\"},{\"start\":\"91575\",\"end\":\"91606\"},{\"start\":\"91925\",\"end\":\"91996\"},{\"start\":\"92410\",\"end\":\"92487\"},{\"start\":\"92732\",\"end\":\"92890\"},{\"start\":\"93348\",\"end\":\"93448\"}]"}}}, "year": 2023, "month": 12, "day": 17}