{"id": 224710744, "updated": "2023-10-06 10:51:16.729", "metadata": {"title": "Importance Reweighting for Biquality Learning", "authors": "[{\"first\":\"Pierre\",\"last\":\"Nodet\",\"middle\":[]},{\"first\":\"Vincent\",\"last\":\"Lemaire\",\"middle\":[]},{\"first\":\"Alexis\",\"last\":\"Bondu\",\"middle\":[]},{\"first\":\"Antoine\",\"last\":\"Cornu'ejols\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 10, "day": 19}, "abstract": "The field of Weakly Supervised Learning (WSL) has recently seen a surge of popularity, with numerous papers addressing different types of\"supervision deficiencies\", namely: poor quality, non adaptability, and insufficient quantity of labels. Regarding quality, label noise can be of different types, including completely-at-random, at-random or even not-at-random. All these kinds of label noise are addressed separately in the literature, leading to highly specialized approaches. This paper proposes an original, encompassing, view of Weakly Supervised Learning, which results in the design of generic approaches capable of dealing with any kind of label noise. For this purpose, an alternative setting called\"Biquality data\"is used. It assumes that a small trusted dataset of correctly labeled examples is available, in addition to an untrusted dataset of noisy examples. In this paper, we propose a new reweigthing scheme capable of identifying noncorrupted examples in the untrusted dataset. This allows one to learn classifiers using both datasets. Extensive experiments that simulate several types of label noise and that vary the quality and quantity of untrusted examples, demonstrate that the proposed approach outperforms baselines and state-of-the-art approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.09621", "mag": "3093395587", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcnn/NodetLBCO21", "doi": "10.1109/ijcnn52387.2021.9533349"}}, "content": {"source": {"pdf_hash": "f411bc32cc00d41a283023977fa73bc536e58f4f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.09621v5.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2010.09621", "status": "GREEN"}}, "grobid": {"id": "1ea74bc44f0b1d51072e2efc5048d2cf35629ca3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f411bc32cc00d41a283023977fa73bc536e58f4f.txt", "contents": "\nImportance Reweighting for Biquality Learning\n\n\nPierre Nodet \nVincent Lemaire \nAlexis Bondu \nAntoine Cornu\u00e9jols \n\nOrange Labs 2 av. P. Marzin Lannion\nOrange Labs AgroParisTech\nINRAe 46 av. de la R\u00e9publique Ch\u00e2tillonFrance, France\n\n\nOrange Labs 46 av. de la R\u00e9publique Ch\u00e2tillon\nFrance\n\n\nUMR MIA-Paris AgroParisTech\nINRAe Universit\u00e9 Paris-Saclay\n16 r. Claude BernardParisFrance\n\nImportance Reweighting for Biquality Learning\nIndex Terms-Supervised ClassificationWeakly Supervised LearningBiquality LearningTrusted dataLabel noise\nThe field of Weakly Supervised Learning (WSL) has recently seen a surge of popularity, with numerous papers addressing different types of \"supervision deficiencies\", namely: poor quality, non adaptability, and insufficient quantity of labels. Regarding quality, label noise can be of different types, including completely-at-random, at-random or even not-at-random. All these kinds of label noise are addressed separately in the literature, leading to highly specialized approaches. This paper proposes an original, encompassing, view of Weakly Supervised Learning, which results in the design of generic approaches capable of dealing with any kind of label noise. For this purpose, an alternative setting called \"Biquality data\" is used. It assumes that a small trusted dataset of correctly labeled examples is available, in addition to an untrusted dataset of noisy examples. In this paper, we propose a new reweigthing scheme capable of identifying noncorrupted examples in the untrusted dataset. This allows one to learn classifiers using both datasets. Extensive experiments that simulate several types of label noise and that vary the quality and quantity of untrusted examples, demonstrate that the proposed approach outperforms baselines and state-ofthe-art approaches.\n\nI. INTRODUCTION\n\nThe supervised classification problem aims to learn a classifier from a set of labeled training examples in order to predict the class of new examples. In practice, conventional classification techniques may fail because of the imperfections of realworld datasets. Accordingly, the field of Weakly Supervised Learning (WSL) has recently seen a surge of popularity, with numerous papers addressing different types of \"supervision deficiencies\" [1], namely:\n\nInsufficient quantity: when many training examples are available, but only a small portion is labeled, e.g. due to the cost of labelling. For instance, this occurs in the field of cyber security where human forensics is needed to label attacks. Usually, this issue is addressed by semi-supervised learning (SSL) [2] or active learning (AL) [3].\n\nPoor quality labels: in this case, all training examples are labeled but the labels may be corrupted. This may happen when the labeling task is outsourced to crowd labeling. The Robust Learning to Label Noise (RLL) approaches address this problem [4], with three identified types of label noise: i) the completely at random noise which correspond to a uniform probability of label change ; ii) the at-random label noise when the probability of label change depends upon each class, with uniform label changes within each class ; iii) the not-atrandom label noise when the probability of label change varies over the input space of the classifier. This last type of label noise is recognized as the most difficult to deal with [5], [6].\n\nInappropriate labels: for instance, in Multi Instance Learning (MIL) [7] the labels are assigned to bags of examples, with positive label indicating that at least one example of the bag is positive. Some scenarios in Transfer Learning (TL) [8] imply that only the labels in the source domain are provided while the target domain labels are not. Often, these non-adapted labels are associated with slightly different learning tasks (e.g. more precise and numerous classes are dividing the original categories). Alternatively, non-adapted labels may characterize a differing statistical individual [9] (e.g. a subpart of an image instead of the entire image).\n\nAll these types of supervision deficiencies are addressed separately in the literature, leading to highly specialized approaches. In practice, it is very difficult to identify the type(s) of deficiencies with which a real dataset is associated. For this reason, we argue that it would be very useful to find a unified framework for Weakly Supervised Learning, in order to design generic approaches capable of dealing with any type of supervision deficiency.\n\nIn Section II of this paper, we present \"biquality data\", an alternative WSL setting allowing a unified view of weakly supervised learning. A generic learning framework using the two training sets of biquality data (the one trusted and the other one untrusted) is suggested in Section III. We identify three possible ways of implementing this framework and consider one of them. This article proposes a new approach using example reweighting in Section IV. The effectiveness of this new approach in dealing with different types of supervision deficiencies, without a priori knowledge about them, is demonstrated through experiments with real datasets in Sections V and VI. Finally, perspectives and future works are discussed in Section VII.\n\n\nII. BIQUALITY DATA\n\nThis section presents an alternative setting called \"Biquality Data\" which covers a large range of supervision deficiencies and allows for unifying the WSL approaches. The interested reader may find a more detailed introduction on WSL and its links with Biquality Learning in [10].\n\nLearning using biquality data has recently been put forward in [11]- [13] and consists in learning a classifier from two distinct training sets, one trusted and the other untrusted. The initial motivation was to unify semi-supervised and robust learning through a combination of the two. We show in this paper that this scenario is not limited to this unification and that it can cover a larger range of supervision deficiencies as demonstrated with the algorithms we propose and the obtained results.\n\nThe trusted dataset D T consists of pairs of labeled examples (x i , y i ) where all labels y i \u2208 Y are supposed to be correct according to the true underlying conditional distribution P T (Y |X). In the untrusted dataset D U , examples x i may be associated with incorrect labels. We note P U (Y |X) the corresponding conditional distribution.\n\nAt this stage, no assumption is made about the nature of the supervision deficiencies which could be of any type including label noise, missing labels, concept drift, non-adapted labels... and more generally a mixture of these supervision deficiencies.\n\nThe difficulty of a learning task performed on biquality data can be characterised by two quantities. First, the ratio of trusted data over the whole data set, denoted by p:\np = |D T | |D T | + |D U |(1)\nSecond, a measure of the quality, denoted by q, which evaluates the usefulness of the untrusted data D U to learn the trusted concept P T (Y |X), where q \u2208 [0, 1] and 1 indicates high quality. For example in [13] q is defined using a ratio of Kullback-Leibler divergence between P T (Y |X) and P U (Y |X). The biquality setting covers a wide range of learning tasks by varying the quantities q and p (as represented in Figure 1 Only the edges of Figure 1 have been envisioned in previous works -i.e. the points mentioned above -and a whole new range of problems are addressed in this paper. Moreover, biquality learning may be used to tackle tasks belonging to WSL, for instance:\n\n\u2022 Positive Unlabeled Learning (PUL) [15]where only positive (trusted) and unlabeled instances are available, the later which can be considered as untrusted. \u2022 Self Training and Cotraining [14] could be addressed at the end of the self labeling process: the initial training set is then the trusted dataset, and all self-labeled examples can be considered as the untrusted ones. \u2022 Concept drift [16]: when a concept drift occurs, all the examples used before a detected drift may be considered as the untrusted examples, while the examples available after it are viewed as the trusted ones, assuming a perfect labeling process. \u2022 Self Supervised Learning system as illustrated by Snorkel [17] or Snuba [18]: the small initial training set can the trusted, whereas all examples automatically labeled using the labeling functions may be considered as untrusted.\n\nAs can be seen from the above list, the Biquality framework is quite general and its investigation seems a promising avenue to unify different aspects of the Weakly Supervised Learning. A main contribution of this paper is to suggest one generic framework for achieving biquality learning and thus covering many facets of WSL. This is presented in the next section. This framework will be then applied in the experiments part of this paper to the problem of label noise.\n\n\nIII. BIQUALITY LEARNING\n\nLearning the true concept 2 P T (Y |X) on D = D T \u222a D U means minimizing the risk R on D with a loss L for a probabilistic classifier f :\nRD,L(f ) = E D,(X,Y )\u223cT [L(f (X), Y )] = P(X \u2208 DT )E D T ,(X,Y )\u223cT [L(f (X), Y )] + P(X \u2208 DU )E D U ,(X,Y )\u223cT [L(f (X), Y )](2)\nwhere L(\u00b7, \u00b7) is a loss function, from R |Y| \u00d7 Y to R since f (X) is a vector of probability over the classes. Since the true concept P T (Y |X) cannot be learned from D U , the last line of Equation 2 is not tractable as it stands. That is why we propose a generic formalization based on a mapping function g that enables us to learn the true concept from the modified untrusted examples of D U . Equation 2 becomes:\nRD,L(f ) = P(X \u2208 DT )E D T ,(X,Y )\u223cT [L(f (X), Y )] + \u03bb P(X \u2208 DU )E D U ,(X,Y )\u223cU [g(L(f (X), Y ))](3)\nIn Equation 3, the parameter \u03bb \u2208 [0, 1] reflects the quality of the untrusted examples of D U modified by the function g. This time, the last line is tractable since it consists of a risk expectancy estimated over the training examples of D U which follows the untrusted concept P U (Y |X), modified by the function g.\n\nAccordingly, the estimation of the expected risk requires to learn three items: g, \u03bb and then f . To learn g, a mapping function between the two datasets, both D T and D U are used. Then, either \u03bb is considered as a hyper parameter to be learned using D T or \u03bb is provided by an appropriate quality measure and is considered as an input of the learning algorithm. Finally, f is learned by minimizing the risk R on D using the mapping g.\n\nIn this formalization, the mapping function g plays a central role. Not exhaustively, we identify three different ways of designing the mapping function. For each of these, a different function g enters the definition of function g: since it can be considered as included in the function g . 2 For reasons of space, we denote P T (Y |X) by T and P U (Y |X) by U .\n\nSection IV considers in-depth the last option and proposes a new approach where g acts as an Importance Reweighting for Biquality Learning.\n\n\nIV. A NEW IMPORTANCE REWEIGHTING APPROACH FOR\n\nBIQUALITY LEARNING To estimate the mapping function g , we suggest to adapt the importance reweigthing trick from the covariate shift literature [19] to biquality learning. This trick relies on reweighting untrusted samples by using the Radon-Nikodym derivative (RND) [20] \nof P T (X, Y ) in respect to P U (X, Y ) which is dP T (X,Y ) dP U (X,Y )\n. Contrary to the \"covariate shift\" setting, the biquality setting handles the same distribution P(X) in the trusted and untrusted datasets. However, the two underlying concepts P T (Y |X) and P U (Y |X) are possibly different due to a supervision deficiency. By using these assumptions and the Bayes Formula, we can further simplifying the reweighing function to the RND of\nP T (Y |X) in respect to P U (Y |X), dP T (Y |X) dP U (Y |X) . R (X,Y )\u223cT,L (f ) = E (X,Y )\u223cT [L(f (X), Y )] = L(f (X), Y ) dP T (X, Y ) = dP T (X, Y ) dP U (X, Y ) L(f (X), Y ) dP U (X, Y ) = E (X,Y )\u223cU [ P T (X, Y ) P U (X, Y ) L(f (X), Y )] = E (X,Y )\u223cU [ P T (Y |X)P(X) P U (Y |X)P(X) L(f (X), Y )] = E (X,Y )\u223cU [ P T (Y |X) P U (Y |X) L(f (X), Y )] = E (X,Y )\u223cU [\u03b2L(f (X), Y )] = R (X,Y )\u223cU,\u03b2L (f )(4)Equation 4 shows that \u03b2 = P T (Y |X) P U (Y |X)\nis an estimation of the mapping function g , thanks to Section III estimating \u03b2 is the last step before an actual Biquality Learning algorithm.\n\nAlgorithm: Importance Reweighting for Biquality Learning (IRBL)\nInput: Trusted Dataset D T , Untrusted Dataset D U , Probabilistic Classifier Familiy F 1 Learn f U \u2208 F on D U 2 Learn f T \u2208 F on D T 3 for (x i , y i ) \u2208 D U , where y i \u2208 [[1, K]] do 4\u03b2(x i , y i ) = f T (xi) f U (xi) yi 5 for (x i , y i ) \u2208 D T do 6\u03b2(x i , y i ) = 1 7 Learn f \u2208 F on D T \u222a D U with weights\u03b2 Output: f\nThe proposed algorithm, Importance Reweighting for Biquality Learning (IRBL), aims at estimating \u03b2 from D T and D U whatever the unknown supervision deficiency. It consists of two successive steps. First a probabilistic classifier f T is learned from the trusted dataset D T and another probabilistic classifier f U is learned from the untrusted dataset D U . Thanks to their probabilistic nature each of them estimates P T (Y |X) and P U (Y |X) by a probability distribution over the set of the K classes. Thus we can estimate the weight \u03b2 of an untrusted sample (x i , y i ) by dividing the prediction of f T (x i ) by f U (x i ) for the y i class (see line 4). The weight \u03b2 for all trusted samples will be fixed to 1 (see line 6). Then a final classifier is learned from both datasets D T and D U with examples reweighted by\u03b2.\n\nOur algorithm is theoretically grounded, since it is asymptotically equivalent to minimizing the risk on the true concept using the entire data set (see proof in Equation 5). Equation 5 is an asymptotic result: in practice our algorithm relies on the quality of the estimation of P T (Y |X) and P U (Y |X) in order to be efficient. In the biquality setting they both could be hard to estimate because of the small size of D T and the poor quality of D U .\nR D,\u03b2L (f ) = 1 |D| (xi,yi)\u2208D 1 (xi,yi)\u2208D T L(f (x i ), y i ) + 1 (xi,yi)\u2208D U\u03b2 (x i , y i )L(f (x i ), y i ) = 1 |D T | + |D U | (xi,yi)\u2208D T L(f (x i ), y i ) + 1 |D T | + |D U | (xi,yi)\u2208D U L(f (x i ), y i )\u03b2(x i , y i ) = p |D T | (xi,yi)\u2208D T L(f (x i ), y i ) + 1 \u2212 p |D U | (xi,yi)\u2208D U L(f (x i ), y i )\u03b2(x i , y i ) = pR D T ,L (f ) + (1 \u2212 p)R D U ,\u03b2L (f ) \u2248 pR D T ,L (f ) + (1 \u2212 p)R D T ,L (f ) \u2248R D T ,L (f ) (5) Proof in\n\nV. EXPERIMENTS\n\nThe aim of the experiments is to answer the following questions: i) is our algorithm properly designed and does it perform better than baselines approaches? ii) is our algorithm competitive with state-of-the-art approaches?\n\nFirst, Section V-A presents the supervision deficiencies which are simulated in our experiments. They correspond to two different kinds of weak supervision, namely, Noisy label Completely at Random (i.e. not X dependent) and Noisy label Not at Random (i.e. X dependent). From the Frenay's taxonomy [4] the former is the easiest to deal with and the later is often considered as difficult to manage. Then, Section V-B consists of three parts: a presentation of the baseline competitors, a brief report on the state-of-the-art competitors, and a description of the set of classifiers used. Finally, Section V-C describes the datasets used in the experiments, and the chosen criterion to evaluate the learned classifiers. For full reproducibility, source code, datasets and results are available at : https://github.com/pierrenodet/irbl.\n\n\nA. Simulated supervision deficiencies\n\nThe datasets listed in Section V-C consist of a collection of training examples that are assumed to be correctly labeled, denoted by D total . In order to obtain a trusted dataset D T and an untrusted one D U , each dataset is split in two parts using a stratified random draw, where p is the percentage for the trusted part. The trusted datasets are left untouched, whereas corrupted labels are simulated in the untrusted datasets by using two different techniques: a) Noisy Completely At Random (NCAR):: Corrupted untrusted examples are uniformly drawn from D U with a probability r, and are assigned a random label that is also uniformly drawn from Y.\n\nIn the particular case of binary classification problems, the conditional distribution of the untrusted labels is defined by Equation 6.\n\u2200y \u2208 Y, P U (Y = y|X) = r 2 + (1 \u2212 r)P T (Y = y|X) (6)\nHere, r controls the overall number of random labels and thus is our proxy for the quality: q = 1 \u2212 r. b) Noisy Not At Random (NNAR):: Corrupted untrusted examples are drawn from D U with a probability r(x) that depends on the instance value. To generate a instance dependent label noise, we design a noise that depends on the decision boundary of a classifier f total learned from D total . The probability of random label r(x) should be high when an instance x is close to the decision boundary, and low when it is far. In our experiments, the probability outputs of f total are used to model our label noise as follows:\n\u2200x \u2208 X , r(x) = 1 \u2212 \u03b8|1 \u2212 2f total (x)| 1 \u03b8(7)\nwhere \u03b8 \u2208 [0; 1] is a constant that controls the overall number of random labels and thus is our proxy for the quality: q = \u03b8. The parameter \u03b8 influences both the slope (factor) and the curvature (power) of r(x) to modify the area under curve of\nr(x): E[r(x)].\nFor binary classification problems, the conditional distribution of the untrusted labels is defined by Equation 8.\n\n\u2200x \u2208 X , \u2200y \u2208 Y,\nP U (Y = y|X = x) = r(x) 2 +(1\u2212 r(x))P T (Y = y|X = x)(8)\nB. Competitors a) Baseline competitors: The first part of our experiments consists of a sanity check which compares the performance of the proposed algorithm to the following baselines:\n\n\u2022 Trusted: The final classifier f obtained with our algorithm should be better than a classifier f T that learned only from the trusted dataset, insofar as untrusted data bring useful information about the trusted concept. At least, f should not be worse than using only trusted data.\n\n\u2022 Mixed:\n\nThe final classifier f should be better than a classifier f mixed learned from both trusted and untrusted dataset, without correction. A biquality learning algorithm should leverage the information provided by having two distinct datasets. \u2022 Untrusted: The final classifier should be better than a classifier f U that learns only from the untrusted dataset if there are trusted labels. Using trusted data should improve the classifier final performances.\n\nb) State-of-the-art-competitors: The second part of our experiments compares our algorithm with two state-of-the-art methods: (i) a method from the Robust Learning to Label noise (RLL) [21], [22] family and (ii) the GLC approach [12].\n\n\u2022 RLL: In recent literature a new emphasis is put on the research of new loss functions that are conducive to better risk minimization in presence of noisy labels. For example, [21], [22] show theoretically and experimentally that when the loss function satisfies a symmetry condition, described below, this contributes to the robustness of the classifier. Accordingly, in this paper we train a classifier with a symmetric loss function as a competitor. This first competitor is expected to have good results on completely-at-random label noise described in Section V-A. A loss function L s is said\nsymmetrical if y\u2208{\u22121;1} L s (f (x), y) = c, where c is a constant and f (x)\nis the score on the class y. This loss function is used on D T \u222a D U .\n\n\u2022 GLC: To the best of our knowledge, GLC [12] is among the best performing algorithm that can learn from biquality data. It has been successfully compared to many competing approaches. Like ours, it is a two steps approach which is simple and easy to implement. In a first step, a model f U is learned from the untrusted dataset D U . Then it is used to estimate a transition matrix C of P U |T (Y ) by making probabilistic predictions with f U on the trusted dataset D T and comparing it to the trusted labels. In a second step, this matrix is used to correct the labels from the untrusted dataset D U when learning the final model f . Indeed f is learned with L on D T and with\nL(C f (X), Y ) on D U .\nc) Classifiers: First of all, the choice of classifiers was guided by the idea of comparing algorithms for biquality learning and not searching for the best classifiers. This choice was also guided by the nature of the datasets used in the experiments (see section V-C). Secondly our algorithm, as well as GLC, implies two learning phases. For both reasons and for simplicity, we decided to use Logistic Regressions (LR) for each phase. LR is known to be limited, in the sense of the Vapnik-Chervonenkis dimension [23] since it can only learn linear separations of the input space X , which could underfit the conditional probabilities P(Y |X) on D T and D U and lead to bad \u03b2 estimations. But this impediment, if met, will affect equally all the compared algorithms. LR is also used for the RLL classifier using the Unhinged symmetric loss function.\n\nTo obtain reliable estimations of conditional probabilities P(Y |X), the outputs of all classifiers have been calibrated thanks to Isotonic Regression with the default parameters provided by scikit-learn [24].\n\nLogistic Regression is always be used and learned thanks to SGD with a learning rate of 0.005, a weight decay of 10 \u22126 during 20 epochs and a batch size of 24 with Pytorch [25].\n\n\nC. Datasets\n\nIn industrial applications familiar to us, such as fraud detection, Customer Relationship Management (CRM) and churn prediction, we are mostly faced with binary classification problems. The available data is of average size in terms of the number of explanatory variables and involves mixed variables (numerical and categorical).\n\nFor this reason we limited in this paper the experiments to binary classification tasks even if our algorithm can address multi-class problems. The chosen tabular datasets, used for the experiments, have similar characteristics than those of our real applications. They come from different sources: UCI [26], libsvm 3 and active learning challenge [27]. A part of these datasets comes from past challenges on active learning where high performances with a low number of labeled examples has proved difficult to obtain. For each dataset, 80 % of samples were used for training and 20% were used for the test. With this choice of datasets, a large range of the class ratio is covered: Australian is almost balanced while Web is really unbalanced. Also, the size varies significantly in number of rows or columns, with corresponding impact on the difficulty of the learning tasks.\n\n\nVI. RESULTS\n\nThe empirical performance of our approach, is evaluated in two steps. First, we investigate the efficiency of the reweighting scheme and its influence on the learning procedure of the final classifier. Second, our approach is benchmarked against competitors to evaluate its efficiency in real tasks.\n\n\nA. Behavior of the IRBL method\n\nIn order to illustrate the proposed reweighing scheme, we picked a dataset, here the \"ad\" dataset used with a ratio of trusted data p = 0.25, and examined the histogram of the weights assigned to each untrusted example either corrupted or not. The case of Random Label Completely at Random is chosen and the hardest case where all labels are at random q = 0 is considered. Figure 2 shows the histogram of the weights assigned to each untrusted example either corrupted or not. It is clear that the proposed method is able to detect corrupted and noncorrupted labels from the untrusted dataset. Figure 3 confirms this behavior when varying the value of the quality. For a perfect quality, the distribution of the \u03b2 is unimodal with a median equal to one and a very narrow inter quantile range, whereas, when the quality drops, the distribution of the \u03b2 for the corrupted labels decreases to zero. It is equally interesting to look at the classification error when q, the quality of the untrusted data, varies. Figure 4 reports the performance for the proposed method and for the baseline competitors. It is remarkable that the performance of our algorithm, IRBL, remains stable when q decreases while the performance of the mixed and untrusted algorithms worsens. In addition, IRBL always obtains better performances than the trusted baseline. \n\n\nB. Comparison with competitors\n\nFor a first global comparison, two critical diagrams are presented in Figures 5 and 6 which rank the various methods for the NCAR and NNAR label noise. The Nemenyi test [28] is used to rank the approaches in terms of mean accuracy, calculated for all values of p and q and over all the 20 data sets described in section V-C. The Nemenyi test consists of two successive steps. First, the Friedman test is applied to the mean accuracy of competing approaches to determine whether their overall performance is similar. Second, if not, the posthoc test is applied to determine groups of approaches whose overall performance is significantly different from that of the other groups.  These figures show that the IRBL method is ranked first for the two kinds of label noise and provides better performance than the other competitors. Table II provides a more detailed perspective by reporting the mean accuracy and its standard deviation. These values are computed for different values of p over all qualities q and all datasets. This table also helps to see how the methods fare as compared to learning on perfect  data. Overall, IRBL obtains the best results and with a lower variability.\n\nTo get more refined results, the Wilcoxon signed-rank test [29] is used 4 . It enables us to find out under which conditions -i.e. by varying the values of p and q -IRBL performs better or worse than the competitors. Figure 7 presents six graphics, each reporting the Wilcoxon test that evaluates our approach against a competitor, based on the mean accuracy over the 20 datasets. The two types of label noise (see Section V-A) correspond to the rows in Figure  7 and a wide range of q and p values are considered.\n\nThanks to these graphs we can compare in more details our method (IRBL) with the mixed methods, as well as with RLL and GLC. Regarding the mixed method, Figures 7 (a) and (b) return the results obtained versus varying values for p and q. For low quality values q, whatever is the value of p, IRBL is significantly better. For middle values of the quality there is no winner and for high quality values and low values of p, the mixed method is significantly better (this result seems to be observed in [12] as well). This is not surprising since at high 4 Here the test is used with a confidence level at 5 %. quality values, the mixed baseline is equivalent to learning with perfect labels.\n\nThese detailed results help us to understand why, in the critical diagram in Figure 5, although IRBL has a better ranking, it is not significantly better than the mixed method: mainly because of the presence of high quality value cases.\n\nRegarding the competitors RLL and GLC, Figures 7(b), 7(c), 7(e) and 7(f) show that IRBL has always better or indistinguishable performances. Indeed, IRBL performs well regardless of the type of noise. This is an important result since it shows that we are able to deal not only with NCAR noise but also with instance dependent label noise (NNAR) which is more difficult. The method RLL gets more ties with IRBL on NCAR than on NNAR as expected. It is noteworthy that GLC has ties with IRBL when the quality is high whatever the label noise.\n\nTo sum up, the proposed method has been tested on a large range of types and strengths of label corruptions. In all cases, IRBL has obtained top or competitive results. Consequently, IRBL appears to be a method of choice for applications where biquality learning is needed. Moreover, IRBL has no user parameter and a low computational complexity.\n\n\nVII. CONCLUSION\n\nThis paper has presented an original view of Weakly Supervised Learning and has described a generic approach capable of dealing with any kind of label noise. A formal framework for biquality learning has been developed where the empirical risk is minimized on the small set of trusted examples in addition to some appropriately chosen criterion using the untrusted examples. We identified three different ways to design a mapping function leading to three different such criteria within the biquality learning framework. We implemented one of them: a new Importance Reweighting approach for Biquality Learning (IRBL). Extensive experiments have shown that IRBL significantly outperforms state-of-theart approaches, by simulating completely-at-random and notat-random label noise over a wide range of quality and ratio values of untrusted data.\n\nFuture works will be done to extend experiments with multiclass classification datasets and other classifiers such as Gradient Boosted Trees [30]. An adaptation of IRBL to Deep Learning tasks with an online algorithm will be studied too.\n\nFig. 1 .\n1The different learning tasks covered by the biquality setting, represented on a 2D representation.\n\n\u2022 1 \u2022\n1When (p = 1 OR q = 1) 1 all examples can be trusted. Thus, this setting corresponds to a standard supervised learning (SL) task. 1 p = 1 =\u21d2 D U = \u2205 =\u21d2 q = When (p = 0 AND q = 0), there is no trusted examples and the untrusted labels are not informative. We are left with only the inputs {x i } 1\u2264i\u2264m as in unsupervised learning (UL). \u2022 On the vertical axis defined by q = 0, except for the two points (p, q) = (0, 0) and (p, q) = (1, 0), the untrusted labels are not informative, and trusted examples are available. The learning task becomes semi-supervised learning (SSL) with the untrusted examples as unlabeled and the trusted as labeled. \u2022 An upward move on the vertical axis, from a point (p, q) = ( , 0) characterized by a low proportion of labeled examples p = , to a point (p , 0), with p > p, corresponds to Active Learning, when an oracle provides new labels asked by a given strategy. The same upward move can also be realized in Self-training and Cotraining [14], where unlabeled training examples are labeled using the predictions of the current classifier(s). \u2022 On the horizontal axis defined by p = 0, except for the points (p, q) = (0, 0) and (p, q) = (0, 1), only untrusted examples are provided, which corresponds to the range of learning tasks typically addressed by Robust Learning to Label noise (RLL) approaches.\n\n\u2022\nThe first option consists in correcting the label for each untrusted examples of D U . The mapping function thus takes the form g(L(f (X), Y )) = L(f (X), g (Y, X)), with g (Y, X) denoting the new corrected labels and f (X) the predictions of the classifier. \u2022 In the second option, the untrusted labels are used unchanged. The untrusted examples X are moved in the input space where the untrusted labels becomes correct with respect to the true underlying concept. The mapping function becomes g(L(f (X), Y )) = L(f (g (X)), Y ), where g (X) is the \"moved\" input vector of the modified untrusted examples. \u2022 In the last option, g weights the contribution of the untrusted examples in the risk estimate. Accordingly, we have g(L(f (X), Y )) = g (Y, X)L(f (X), Y ). In this case, the parameter \u03bb may disappear from Equation 3\n\nFig. 2 .Fig. 3 .\n23Histogram of the \u03b2 values on AD for p = 0.25 and q = 0 for NCAR for the corrupted and noncorrupted examples. Boxplot the \u03b2 values on AD for p = 0.25 versus the quality, from q = 0 to q = 1 for NCAR.\n\nFig. 4 .\n4Classification error on test set for IRBL against baselines on a full range of quality level (AD dataset, p = 0.25, NCAR).\n\nFig. 5 .\n5Nemenyi test for the 20 datasets \u2200p, q for NCAR.\n\nFig. 6 .\n6Nemenyi test for the 20 datasets \u2200p, q for NNAR.\n\n\n) IRBL vs Mixed for NNAR (e) IRBL vs RLL for NNAR (f) IRBL vs GLC for NNAR Fig. 7. Results of the Wilcoxon signed rank test computed on the 20 datasets. Each figure compares IRBL versus one of the competitors. Figures a, b, c are in the case of Noisy label Completely at Random and Figures d, e, f for the case of Noisy label Not at Random. In eachfigure \"\u2022\", \"\u00b7\" and \"\u2022\" indicate respectively a win, a tie or a loss of IRBL compared to the competitors, the vertical axis is p and the horizontal axis is q.\n\nTABLE I BINARY\nICLASSIFICATION DATASETS USED FOR THE EVALUATION. COLUMNS: NUMBER OF EXAMPLES (|D|), NUMBER OF FEATURES (|X |), AND RATIO OF EXAMPLES FROM THE MINORITY CLASS (MIN).name \n|D| \n|X | \nmin \nname \n|D| \n|X | \nmin \n4class \n862 \n2 \n36 \nibnsina \n20,722 \n92 \n38 \nad \n3,278 \n1558 \n14 \nzebra \n61,488 \n154 \n4.6 \nadult \n48,842 \n14 \n23 \nmusk \n6,598 \n169 \n15 \naus \n690 \n14 \n44 \nphishing \n11055 \n30 \n44 \nbanknote \n1372 \n4 \n44 \nspam \n4,601 \n57 \n39 \nbreast \n683 \n9 \n35 \nijcnn1 \n141,691 \n22 \n9 \neeg \n1498 \n13 \n45 \nsvmg3 \n1284 \n4 \n26 \ndiabetes \n768 \n8 \n35 \nsvmg1 \n7,089 \n22 \n43 \ngerman \n1000 \n20 \n30 \nsylva \n145,252 \n108 \n6.5 \nhiva \n42,678 \n1617 \n3.5 \nweb \n49,749 \n300 \n3 \n\n\n\nTABLE II MEAN\nIIACCURACY (RESCALED SCORE TO BE FROM 0 TO 100) AND STANDARD DEVIATION COMPUTED ON THE 20 DATASETS \u2200q FOR (1) NCAR AND (2) NNAR. THE MEAN ACC WHEN USING ALL THE TRAINING DATA WITHOUT NOISE IS 88.65.p \ntrusted \nirbl \nmixed \nglc \nrll \n\n(1) \n\n0.02 \n72.48 \u00b1 5.70 83.46 \u00b1 3.56 \n83.40 \u00b1 8.30 \n78.34 \u00b1 7.94 \n77.94 \u00b1 6.37 \n0.05 \n78.50 \u00b1 4.33 84.94 \u00b1 2.24 \n83.85 \u00b1 7.35 \n81.19 \u00b1 5.15 \n77.97 \u00b1 6.44 \n0.10 \n81.40 \u00b1 3.33 86.56 \u00b1 1.68 \n85.44 \u00b1 5.34 \n83.00 \u00b1 3.90 \n78.98 \u00b1 5.26 \n0.25 \n85.61 \u00b1 2.39 87.96 \u00b1 1.18 \n86.99 \u00b1 2.80 \n86.27 \u00b1 2.03 \n79.86 \u00b1 2.61 \n\n(2) \n\n0.02 \n72.48 \u00b1 5.70 82.93 \u00b1 3.18 \n81.30 \u00b1 10.05 \n77.55 \u00b1 7.78 \n75.47 \u00b1 9.47 \n0.05 \n78.50 \u00b1 4.33 85.34 \u00b1 2.55 \n82.52 \u00b1 7.72 \n80.77 \u00b1 5.04 \n76.94 \u00b1 6.64 \n0.10 \n81.40 \u00b1 3.33 86.82 \u00b1 1.45 \n84.44 \u00b1 5.14 \n83.22 \u00b1 4.10 \n77.95 \u00b1 4.51 \n0.25 \n85.61 \u00b1 2.39 88.21 \u00b1 1.05 \n86.74 \u00b1 2.56 \n86.56 \u00b1 2.00 \n79.67 \u00b1 2.70 \nMean \n79.50 \u00b1 3.94 85.71 \u00b1 2.11 \n84.33 \u00b1 6.16 \n82.11 \u00b1 4.74 \n78.10 \u00b1 5.50 \n\n\nhttps://www.csie.ntu.edu.tw/ \u223c cjlin/libsvmtools/datasets/\n\nA brief introduction to weakly supervised learning. Z.-H Zhou, National Science Review. 51Z.-H. Zhou, \"A brief introduction to weakly supervised learning,\" National Science Review, vol. 5, no. 1, pp. 44-53, 08 2017.\n\nSemi-supervised learning. O Chapelle, B Scholkopf, A Zien, IEEE Transactions on Neural Networks. 203O. Chapelle, B. Scholkopf, and A. Zien, \"Semi-supervised learning,\" IEEE Transactions on Neural Networks, vol. 20, no. 3, pp. 542-542, 2009.\n\nActive learning literature survey. B Settles, University of Wisconsin-Madison Department of Computer Sciences, Tech. Rep.B. Settles, \"Active learning literature survey,\" University of Wisconsin- Madison Department of Computer Sciences, Tech. Rep., 2009.\n\nClassification in the presence of label noise: a survey. B Fr\u00e9nay, M Verleysen, IEEE transactions on neural networks and learning systems. 25B. Fr\u00e9nay and M. Verleysen, \"Classification in the presence of label noise: a survey,\" IEEE transactions on neural networks and learning systems, vol. 25, no. 5, pp. 845-869, 2013.\n\nLearning with bounded instance and label-dependent label noise. J Cheng, T Liu, K Ramamohanarao, D Tao, International Conference on Machine Learning (ICML). PMLR119J. Cheng, T. Liu, K. Ramamohanarao, and D. Tao, \"Learning with bounded instance and label-dependent label noise,\" in International Conference on Machine Learning (ICML), vol. 119. PMLR, 13-18 Jul 2020, pp. 1789-1799.\n\nLearning from binary labels with instance-dependent corruption. A Menon, B V Rooyen, N Natarajan, abs/1605.00751ArXiv. A. Menon, B. V. Rooyen, and N. Natarajan, \"Learning from binary labels with instance-dependent corruption,\" ArXiv, vol. abs/1605.00751, 2016.\n\nMultiple instance learning: A survey of problem characteristics and applications. M.-A Carbonneau, V Cheplygina, E Granger, G Gagnon, Pattern Recognition. 77M.-A. Carbonneau, V. Cheplygina, E. Granger, and G. Gagnon, \"Multi- ple instance learning: A survey of problem characteristics and applica- tions,\" Pattern Recognition, vol. 77, p. 329-353, May 2018.\n\nA survey of transfer learning. K Weiss, T M Khoshgoftaar, D Wang, Journal of Big data. 319K. Weiss, T. M. Khoshgoftaar, and D. Wang, \"A survey of transfer learning,\" Journal of Big data, vol. 3, no. 1, p. 9, 2016.\n\nA method for counting people in crowded scenes. D Conte, P Foggia, G Percannella, F Tufano, M Vento, 2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance. D. Conte, P. Foggia, G. Percannella, F. Tufano, and M. Vento, \"A method for counting people in crowded scenes,\" in 2010 7th IEEE International Conference on Advanced Video and Signal Based Surveillance, 2010, pp. 225-232.\n\nFrom Weakly Supervised Learning to Biquality Learning: an Introduction. P Nodet, V Lemaire, A Bondu, A Cornu\u00e9jols, A Ouorou, Proceedings of the International Joint Conference on Neural Networks (IJCNN). the International Joint Conference on Neural Networks (IJCNN)2021P. Nodet, V. Lemaire, A. Bondu, A. Cornu\u00e9jols, and A. Ouorou, \"From Weakly Supervised Learning to Biquality Learning: an Introduction,\" in In Proceedings of the International Joint Conference on Neural Networks (IJCNN), 2021.\n\nLearning from untrusted data. M Charikar, J Steinhardt, G Valiant, Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing. the 49th Annual ACM SIGACT Symposium on Theory of ComputingM. Charikar, J. Steinhardt, and G. Valiant, \"Learning from untrusted data,\" in Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, 2017, p. 47-60.\n\nUsing trusted data to train deep networks on labels corrupted by severe noise. D Hendrycks, M Mazeika, D Wilson, K Gimpel, Advances in Neural Information Processing Systems. 31465D. Hendrycks, M. Mazeika, D. Wilson, and K. Gimpel, \"Using trusted data to train deep networks on labels corrupted by severe noise,\" in Advances in Neural Information Processing Systems 31, 2018, pp. 10 456-10 465.\n\nUnifying semi-supervised and robust learning by mixup. R Hataya, H Nakayama, The 2nd Learning from Limited Labeled Data Workshop, ICLR. R. Hataya and H. Nakayama, \"Unifying semi-supervised and robust learning by mixup,\" in The 2nd Learning from Limited Labeled Data Workshop, ICLR, 2019.\n\nMulti-view learning overview: Recent progress and new challenges. J Zhao, X Xie, X Xu, S Sun, Information Fusion. 38J. Zhao, X. Xie, X. Xu, and S. Sun, \"Multi-view learning overview: Recent progress and new challenges,\" Information Fusion, vol. 38, pp. 43 -54, 2017.\n\nLearning from positive and unlabeled data: a survey. J Bekker, J Davis, Machine Learning. 109J. Bekker and J. Davis, \"Learning from positive and unlabeled data: a survey,\" Machine Learning, vol. 109, pp. 719-760, 2020.\n\nA survey on concept drift adaptation. J Gama, I Zliobaite, A Bifet, M Pechenizkiy, A Bouchachia, ACM Computing Surveys. 464J. Gama, I. Zliobaite, A. Bifet, M. Pechenizkiy, and A. Bouchachia, \"A survey on concept drift adaptation,\" ACM Computing Surveys, vol. 46, no. 4, 2014.\n\nSnorkel: Rapid training data creation with weak supervision. A Ratner, S H Bach, H Ehrenberg, J Fries, S Wu, C R\u00e9, The VLDB Journal. 292A. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu, and C. R\u00e9, \"Snorkel: Rapid training data creation with weak supervision,\" The VLDB Journal, vol. 29, no. 2, pp. 709-730, 2020.\n\nSnuba: Automating weak supervision to label training data. P Varma, C R\u00e9, Proc. VLDB Endow. VLDB Endow12P. Varma and C. R\u00e9, \"Snuba: Automating weak supervision to label training data,\" Proc. VLDB Endow., vol. 12, no. 3, p. 223-236, Nov. 2018.\n\nClassification with noisy labels by importance reweighting. T Liu, D Tao, IEEE Transactions on Pattern Analysis and Machine Intelligence. 383T. Liu and D. Tao, \"Classification with noisy labels by importance reweighting,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 3, p. 447-461, Mar 2016.\n\nSur une g\u00e9n\u00e9ralisation des int\u00e9grales de m. j. radon. O Nikodym, Fundamenta Mathematicae. 151O. Nikodym, \"Sur une g\u00e9n\u00e9ralisation des int\u00e9grales de m. j. radon,\" Fundamenta Mathematicae, vol. 15, no. 1, pp. 131-179, 1930. [Online]. Available: http://eudml.org/doc/212339\n\nLearning with symmetric label noise: The importance of being unhinged. B Van Rooyen, A Menon, R C Williamson, Advances in Neural Information Processing Systems. C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett28B. van Rooyen, A. Menon, and R. C. Williamson, \"Learning with sym- metric label noise: The importance of being unhinged,\" in Advances in Neural Information Processing Systems 28, C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, Eds., 2015, pp. 10-18.\n\nOn symmetric losses for learning from corrupted labels. N Charoenphakdee, J Lee, M Sugiyama, International Conference on Machine Learning. 97N. Charoenphakdee, J. Lee, and M. Sugiyama, \"On symmetric losses for learning from corrupted labels,\" in International Conference on Machine Learning, vol. 97, 2019, pp. 961-970.\n\nThe nature of statistical learning theory. V N Vapnik, V. N. Vapnik, \"The nature of statistical learning theory,\" 1995.\n\nScikit-learn: Machine learning in python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, Journal of machine learning research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al., \"Scikit-learn: Machine learning in python,\" Journal of machine learning research, vol. 12, pp. 2825-2830, 2011.\n\nPytorch: An imperative style, highperformance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Advances in Neural Information Processing Systems. 32A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, \"Pytorch: An imperative style, high- performance deep learning library,\" in Advances in Neural Information Processing Systems 32, 2019, pp. 8024-8035.\n\nUci machine learning repository. D Dua, C Graff, D. Dua and C. Graff, \"Uci machine learning repository,\" 2017. [Online]. Available: http://archive.ics.uci.edu/ml\n\nDatasets of the active learning challenge. I Guyon, University of Wisconsin-Madison Department of Computer Sciences, Tech. Rep.I. Guyon, \"Datasets of the active learning challenge,\" University of Wisconsin-Madison Department of Computer Sciences, Tech. Rep., 2010.\n\nDistribution-free multiple comparisons. P Nemenyi, Biometrics. 182263P. Nemenyi, \"Distribution-free multiple comparisons,\" Biometrics, vol. 18, no. 2, p. 263, 1962.\n\nIndividual comparisons by ranking methods. F Wilcoxon, Biometrics Bulletin. 16F. Wilcoxon, \"Individual comparisons by ranking methods,\" Biometrics Bulletin, vol. 1, no. 6, pp. 80-83, 1945. [Online]. Available: http://www.jstor.org/stable/3001968\n\nGreedy function approximation: a gradient boosting machine. J H Friedman, Annals of statisticsJ. H. Friedman, \"Greedy function approximation: a gradient boosting machine,\" Annals of statistics, pp. 1189-1232, 2001.\n", "annotations": {"author": "[{\"end\":62,\"start\":49},{\"end\":79,\"start\":63},{\"end\":93,\"start\":80},{\"end\":113,\"start\":94},{\"end\":231,\"start\":114},{\"end\":286,\"start\":232},{\"end\":378,\"start\":287}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":56},{\"end\":78,\"start\":71},{\"end\":92,\"start\":87},{\"end\":112,\"start\":102}]", "author_first_name": "[{\"end\":55,\"start\":49},{\"end\":70,\"start\":63},{\"end\":86,\"start\":80},{\"end\":101,\"start\":94}]", "author_affiliation": "[{\"end\":230,\"start\":115},{\"end\":285,\"start\":233},{\"end\":377,\"start\":288}]", "title": "[{\"end\":46,\"start\":1},{\"end\":424,\"start\":379}]", "venue": null, "abstract": "[{\"end\":1807,\"start\":530}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2272,\"start\":2269},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2598,\"start\":2595},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2626,\"start\":2623},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2879,\"start\":2876},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3358,\"start\":3355},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3363,\"start\":3360},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3438,\"start\":3435},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3609,\"start\":3606},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3965,\"start\":3962},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5528,\"start\":5524},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5598,\"start\":5594},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5604,\"start\":5600},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7050,\"start\":7046},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7559,\"start\":7555},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7711,\"start\":7707},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7917,\"start\":7913},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8210,\"start\":8206},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8224,\"start\":8220},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10715,\"start\":10714},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11125,\"start\":11121},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11248,\"start\":11244},{\"end\":13686,\"start\":13676},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14943,\"start\":14940},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16309,\"start\":16308},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18616,\"start\":18612},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18622,\"start\":18618},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18660,\"start\":18656},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18844,\"start\":18840},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18850,\"start\":18846},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19455,\"start\":19451},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20632,\"start\":20628},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21174,\"start\":21170},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21353,\"start\":21349},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22008,\"start\":22004},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22053,\"start\":22049},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24479,\"start\":24475},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25555,\"start\":25551},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26513,\"start\":26509},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26562,\"start\":26561},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28836,\"start\":28832}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29038,\"start\":28929},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30381,\"start\":29039},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31209,\"start\":30382},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31428,\"start\":31210},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31562,\"start\":31429},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31622,\"start\":31563},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31682,\"start\":31623},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32191,\"start\":31683},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32861,\"start\":32192},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33801,\"start\":32862}]", "paragraph": "[{\"end\":2281,\"start\":1826},{\"end\":2627,\"start\":2283},{\"end\":3364,\"start\":2629},{\"end\":4023,\"start\":3366},{\"end\":4482,\"start\":4025},{\"end\":5225,\"start\":4484},{\"end\":5529,\"start\":5248},{\"end\":6032,\"start\":5531},{\"end\":6378,\"start\":6034},{\"end\":6632,\"start\":6380},{\"end\":6807,\"start\":6634},{\"end\":7517,\"start\":6838},{\"end\":8377,\"start\":7519},{\"end\":8849,\"start\":8379},{\"end\":9014,\"start\":8877},{\"end\":9560,\"start\":9143},{\"end\":9982,\"start\":9664},{\"end\":10420,\"start\":9984},{\"end\":10785,\"start\":10422},{\"end\":10926,\"start\":10787},{\"end\":11249,\"start\":10976},{\"end\":11698,\"start\":11324},{\"end\":12296,\"start\":12153},{\"end\":12361,\"start\":12298},{\"end\":13512,\"start\":12683},{\"end\":13969,\"start\":13514},{\"end\":14640,\"start\":14417},{\"end\":15476,\"start\":14642},{\"end\":16172,\"start\":15518},{\"end\":16310,\"start\":16174},{\"end\":16988,\"start\":16366},{\"end\":17281,\"start\":17036},{\"end\":17411,\"start\":17297},{\"end\":17429,\"start\":17413},{\"end\":17673,\"start\":17488},{\"end\":17959,\"start\":17675},{\"end\":17969,\"start\":17961},{\"end\":18425,\"start\":17971},{\"end\":18661,\"start\":18427},{\"end\":19261,\"start\":18663},{\"end\":19408,\"start\":19338},{\"end\":20089,\"start\":19410},{\"end\":20964,\"start\":20114},{\"end\":21175,\"start\":20966},{\"end\":21354,\"start\":21177},{\"end\":21699,\"start\":21370},{\"end\":22578,\"start\":21701},{\"end\":22893,\"start\":22594},{\"end\":24271,\"start\":22928},{\"end\":25490,\"start\":24306},{\"end\":26006,\"start\":25492},{\"end\":26698,\"start\":26008},{\"end\":26936,\"start\":26700},{\"end\":27478,\"start\":26938},{\"end\":27826,\"start\":27480},{\"end\":28689,\"start\":27846},{\"end\":28928,\"start\":28691}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6837,\"start\":6808},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9142,\"start\":9015},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9663,\"start\":9561},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11323,\"start\":11250},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12105,\"start\":11699},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12152,\"start\":12105},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12682,\"start\":12362},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14399,\"start\":13970},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16365,\"start\":16311},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17035,\"start\":16989},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17296,\"start\":17282},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17487,\"start\":17430},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19337,\"start\":19262},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20113,\"start\":20090}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25142,\"start\":25134}]", "section_header": "[{\"end\":1824,\"start\":1809},{\"end\":5246,\"start\":5228},{\"end\":8875,\"start\":8852},{\"end\":10974,\"start\":10929},{\"end\":14415,\"start\":14401},{\"end\":15516,\"start\":15479},{\"end\":21368,\"start\":21357},{\"end\":22592,\"start\":22581},{\"end\":22926,\"start\":22896},{\"end\":24304,\"start\":24274},{\"end\":27844,\"start\":27829},{\"end\":28938,\"start\":28930},{\"end\":29045,\"start\":29040},{\"end\":30384,\"start\":30383},{\"end\":31227,\"start\":31211},{\"end\":31438,\"start\":31430},{\"end\":31572,\"start\":31564},{\"end\":31632,\"start\":31624},{\"end\":32207,\"start\":32193},{\"end\":32876,\"start\":32863}]", "table": "[{\"end\":32861,\"start\":32372},{\"end\":33801,\"start\":33075}]", "figure_caption": "[{\"end\":29038,\"start\":28940},{\"end\":30381,\"start\":29047},{\"end\":31209,\"start\":30385},{\"end\":31428,\"start\":31230},{\"end\":31562,\"start\":31440},{\"end\":31622,\"start\":31574},{\"end\":31682,\"start\":31634},{\"end\":32191,\"start\":31685},{\"end\":32372,\"start\":32209},{\"end\":33075,\"start\":32879}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7265,\"start\":7257},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7292,\"start\":7284},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":13699,\"start\":13689},{\"end\":23309,\"start\":23301},{\"end\":23530,\"start\":23522},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23945,\"start\":23937},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24391,\"start\":24376},{\"end\":25717,\"start\":25709},{\"end\":25955,\"start\":25946},{\"end\":26174,\"start\":26161},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26785,\"start\":26777},{\"end\":26989,\"start\":26977}]", "bib_author_first_name": "[{\"end\":33918,\"start\":33914},{\"end\":34106,\"start\":34105},{\"end\":34118,\"start\":34117},{\"end\":34131,\"start\":34130},{\"end\":34357,\"start\":34356},{\"end\":34634,\"start\":34633},{\"end\":34644,\"start\":34643},{\"end\":34964,\"start\":34963},{\"end\":34973,\"start\":34972},{\"end\":34980,\"start\":34979},{\"end\":34997,\"start\":34996},{\"end\":35346,\"start\":35345},{\"end\":35355,\"start\":35354},{\"end\":35357,\"start\":35356},{\"end\":35367,\"start\":35366},{\"end\":35629,\"start\":35625},{\"end\":35643,\"start\":35642},{\"end\":35657,\"start\":35656},{\"end\":35668,\"start\":35667},{\"end\":35933,\"start\":35932},{\"end\":35942,\"start\":35941},{\"end\":35944,\"start\":35943},{\"end\":35960,\"start\":35959},{\"end\":36165,\"start\":36164},{\"end\":36174,\"start\":36173},{\"end\":36184,\"start\":36183},{\"end\":36199,\"start\":36198},{\"end\":36209,\"start\":36208},{\"end\":36601,\"start\":36600},{\"end\":36610,\"start\":36609},{\"end\":36621,\"start\":36620},{\"end\":36630,\"start\":36629},{\"end\":36644,\"start\":36643},{\"end\":37054,\"start\":37053},{\"end\":37066,\"start\":37065},{\"end\":37080,\"start\":37079},{\"end\":37477,\"start\":37476},{\"end\":37490,\"start\":37489},{\"end\":37501,\"start\":37500},{\"end\":37511,\"start\":37510},{\"end\":37848,\"start\":37847},{\"end\":37858,\"start\":37857},{\"end\":38148,\"start\":38147},{\"end\":38156,\"start\":38155},{\"end\":38163,\"start\":38162},{\"end\":38169,\"start\":38168},{\"end\":38403,\"start\":38402},{\"end\":38413,\"start\":38412},{\"end\":38608,\"start\":38607},{\"end\":38616,\"start\":38615},{\"end\":38629,\"start\":38628},{\"end\":38638,\"start\":38637},{\"end\":38653,\"start\":38652},{\"end\":38908,\"start\":38907},{\"end\":38918,\"start\":38917},{\"end\":38920,\"start\":38919},{\"end\":38928,\"start\":38927},{\"end\":38941,\"start\":38940},{\"end\":38950,\"start\":38949},{\"end\":38956,\"start\":38955},{\"end\":39224,\"start\":39223},{\"end\":39233,\"start\":39232},{\"end\":39469,\"start\":39468},{\"end\":39476,\"start\":39475},{\"end\":39788,\"start\":39787},{\"end\":40076,\"start\":40075},{\"end\":40090,\"start\":40089},{\"end\":40099,\"start\":40098},{\"end\":40101,\"start\":40100},{\"end\":40559,\"start\":40558},{\"end\":40577,\"start\":40576},{\"end\":40584,\"start\":40583},{\"end\":40867,\"start\":40866},{\"end\":40869,\"start\":40868},{\"end\":40987,\"start\":40986},{\"end\":41000,\"start\":40999},{\"end\":41013,\"start\":41012},{\"end\":41025,\"start\":41024},{\"end\":41035,\"start\":41034},{\"end\":41046,\"start\":41045},{\"end\":41056,\"start\":41055},{\"end\":41067,\"start\":41066},{\"end\":41083,\"start\":41082},{\"end\":41092,\"start\":41091},{\"end\":41458,\"start\":41457},{\"end\":41468,\"start\":41467},{\"end\":41477,\"start\":41476},{\"end\":41486,\"start\":41485},{\"end\":41495,\"start\":41494},{\"end\":41507,\"start\":41506},{\"end\":41517,\"start\":41516},{\"end\":41528,\"start\":41527},{\"end\":41535,\"start\":41534},{\"end\":41549,\"start\":41548},{\"end\":41559,\"start\":41558},{\"end\":41572,\"start\":41571},{\"end\":41580,\"start\":41579},{\"end\":41588,\"start\":41587},{\"end\":41598,\"start\":41597},{\"end\":41608,\"start\":41607},{\"end\":41618,\"start\":41617},{\"end\":41634,\"start\":41633},{\"end\":41645,\"start\":41644},{\"end\":41653,\"start\":41652},{\"end\":41660,\"start\":41659},{\"end\":42149,\"start\":42148},{\"end\":42156,\"start\":42155},{\"end\":42322,\"start\":42321},{\"end\":42585,\"start\":42584},{\"end\":42754,\"start\":42753},{\"end\":43018,\"start\":43017},{\"end\":43020,\"start\":43019}]", "bib_author_last_name": "[{\"end\":33923,\"start\":33919},{\"end\":34115,\"start\":34107},{\"end\":34128,\"start\":34119},{\"end\":34136,\"start\":34132},{\"end\":34365,\"start\":34358},{\"end\":34641,\"start\":34635},{\"end\":34654,\"start\":34645},{\"end\":34970,\"start\":34965},{\"end\":34977,\"start\":34974},{\"end\":34994,\"start\":34981},{\"end\":35001,\"start\":34998},{\"end\":35352,\"start\":35347},{\"end\":35364,\"start\":35358},{\"end\":35377,\"start\":35368},{\"end\":35640,\"start\":35630},{\"end\":35654,\"start\":35644},{\"end\":35665,\"start\":35658},{\"end\":35675,\"start\":35669},{\"end\":35939,\"start\":35934},{\"end\":35957,\"start\":35945},{\"end\":35965,\"start\":35961},{\"end\":36171,\"start\":36166},{\"end\":36181,\"start\":36175},{\"end\":36196,\"start\":36185},{\"end\":36206,\"start\":36200},{\"end\":36215,\"start\":36210},{\"end\":36607,\"start\":36602},{\"end\":36618,\"start\":36611},{\"end\":36627,\"start\":36622},{\"end\":36641,\"start\":36631},{\"end\":36651,\"start\":36645},{\"end\":37063,\"start\":37055},{\"end\":37077,\"start\":37067},{\"end\":37088,\"start\":37081},{\"end\":37487,\"start\":37478},{\"end\":37498,\"start\":37491},{\"end\":37508,\"start\":37502},{\"end\":37518,\"start\":37512},{\"end\":37855,\"start\":37849},{\"end\":37867,\"start\":37859},{\"end\":38153,\"start\":38149},{\"end\":38160,\"start\":38157},{\"end\":38166,\"start\":38164},{\"end\":38173,\"start\":38170},{\"end\":38410,\"start\":38404},{\"end\":38419,\"start\":38414},{\"end\":38613,\"start\":38609},{\"end\":38626,\"start\":38617},{\"end\":38635,\"start\":38630},{\"end\":38650,\"start\":38639},{\"end\":38664,\"start\":38654},{\"end\":38915,\"start\":38909},{\"end\":38925,\"start\":38921},{\"end\":38938,\"start\":38929},{\"end\":38947,\"start\":38942},{\"end\":38953,\"start\":38951},{\"end\":38959,\"start\":38957},{\"end\":39230,\"start\":39225},{\"end\":39236,\"start\":39234},{\"end\":39473,\"start\":39470},{\"end\":39480,\"start\":39477},{\"end\":39796,\"start\":39789},{\"end\":40087,\"start\":40077},{\"end\":40096,\"start\":40091},{\"end\":40112,\"start\":40102},{\"end\":40574,\"start\":40560},{\"end\":40581,\"start\":40578},{\"end\":40593,\"start\":40585},{\"end\":40876,\"start\":40870},{\"end\":40997,\"start\":40988},{\"end\":41010,\"start\":41001},{\"end\":41022,\"start\":41014},{\"end\":41032,\"start\":41026},{\"end\":41043,\"start\":41036},{\"end\":41053,\"start\":41047},{\"end\":41064,\"start\":41057},{\"end\":41080,\"start\":41068},{\"end\":41089,\"start\":41084},{\"end\":41100,\"start\":41093},{\"end\":41465,\"start\":41459},{\"end\":41474,\"start\":41469},{\"end\":41483,\"start\":41478},{\"end\":41492,\"start\":41487},{\"end\":41504,\"start\":41496},{\"end\":41514,\"start\":41508},{\"end\":41525,\"start\":41518},{\"end\":41532,\"start\":41529},{\"end\":41546,\"start\":41536},{\"end\":41556,\"start\":41550},{\"end\":41569,\"start\":41560},{\"end\":41577,\"start\":41573},{\"end\":41585,\"start\":41581},{\"end\":41595,\"start\":41589},{\"end\":41605,\"start\":41599},{\"end\":41615,\"start\":41609},{\"end\":41631,\"start\":41619},{\"end\":41642,\"start\":41635},{\"end\":41650,\"start\":41646},{\"end\":41657,\"start\":41654},{\"end\":41669,\"start\":41661},{\"end\":42153,\"start\":42150},{\"end\":42162,\"start\":42157},{\"end\":42328,\"start\":42323},{\"end\":42593,\"start\":42586},{\"end\":42763,\"start\":42755},{\"end\":43029,\"start\":43021}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":44192968},\"end\":34077,\"start\":33862},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3869071},\"end\":34319,\"start\":34079},{\"attributes\":{\"id\":\"b2\"},\"end\":34574,\"start\":34321},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":6054025},\"end\":34897,\"start\":34576},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":34957763},\"end\":35279,\"start\":34899},{\"attributes\":{\"doi\":\"abs/1605.00751\",\"id\":\"b5\",\"matched_paper_id\":13255328},\"end\":35541,\"start\":35281},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3873540},\"end\":35899,\"start\":35543},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":740063},\"end\":36114,\"start\":35901},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18508626},\"end\":36526,\"start\":36116},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":237450775},\"end\":37021,\"start\":36528},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12023229},\"end\":37395,\"start\":37023},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3653594},\"end\":37790,\"start\":37397},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":92994087},\"end\":38079,\"start\":37792},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":35944072},\"end\":38347,\"start\":38081},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":53280687},\"end\":38567,\"start\":38349},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207208264},\"end\":38844,\"start\":38569},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6730236},\"end\":39162,\"start\":38846},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":54031871},\"end\":39406,\"start\":39164},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11777930},\"end\":39731,\"start\":39408},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":117116874},\"end\":40002,\"start\":39733},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6788443},\"end\":40500,\"start\":40004},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":59316910},\"end\":40821,\"start\":40502},{\"attributes\":{\"id\":\"b22\"},\"end\":40942,\"start\":40823},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10659969},\"end\":41386,\"start\":40944},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202786778},\"end\":42113,\"start\":41388},{\"attributes\":{\"id\":\"b25\"},\"end\":42276,\"start\":42115},{\"attributes\":{\"id\":\"b26\"},\"end\":42542,\"start\":42278},{\"attributes\":{\"id\":\"b27\"},\"end\":42708,\"start\":42544},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53662922},\"end\":42955,\"start\":42710},{\"attributes\":{\"id\":\"b29\"},\"end\":43171,\"start\":42957}]", "bib_title": "[{\"end\":33912,\"start\":33862},{\"end\":34103,\"start\":34079},{\"end\":34631,\"start\":34576},{\"end\":34961,\"start\":34899},{\"end\":35343,\"start\":35281},{\"end\":35623,\"start\":35543},{\"end\":35930,\"start\":35901},{\"end\":36162,\"start\":36116},{\"end\":36598,\"start\":36528},{\"end\":37051,\"start\":37023},{\"end\":37474,\"start\":37397},{\"end\":37845,\"start\":37792},{\"end\":38145,\"start\":38081},{\"end\":38400,\"start\":38349},{\"end\":38605,\"start\":38569},{\"end\":38905,\"start\":38846},{\"end\":39221,\"start\":39164},{\"end\":39466,\"start\":39408},{\"end\":39785,\"start\":39733},{\"end\":40073,\"start\":40004},{\"end\":40556,\"start\":40502},{\"end\":40984,\"start\":40944},{\"end\":41455,\"start\":41388},{\"end\":42582,\"start\":42544},{\"end\":42751,\"start\":42710}]", "bib_author": "[{\"end\":33925,\"start\":33914},{\"end\":34117,\"start\":34105},{\"end\":34130,\"start\":34117},{\"end\":34138,\"start\":34130},{\"end\":34367,\"start\":34356},{\"end\":34643,\"start\":34633},{\"end\":34656,\"start\":34643},{\"end\":34972,\"start\":34963},{\"end\":34979,\"start\":34972},{\"end\":34996,\"start\":34979},{\"end\":35003,\"start\":34996},{\"end\":35354,\"start\":35345},{\"end\":35366,\"start\":35354},{\"end\":35379,\"start\":35366},{\"end\":35642,\"start\":35625},{\"end\":35656,\"start\":35642},{\"end\":35667,\"start\":35656},{\"end\":35677,\"start\":35667},{\"end\":35941,\"start\":35932},{\"end\":35959,\"start\":35941},{\"end\":35967,\"start\":35959},{\"end\":36173,\"start\":36164},{\"end\":36183,\"start\":36173},{\"end\":36198,\"start\":36183},{\"end\":36208,\"start\":36198},{\"end\":36217,\"start\":36208},{\"end\":36609,\"start\":36600},{\"end\":36620,\"start\":36609},{\"end\":36629,\"start\":36620},{\"end\":36643,\"start\":36629},{\"end\":36653,\"start\":36643},{\"end\":37065,\"start\":37053},{\"end\":37079,\"start\":37065},{\"end\":37090,\"start\":37079},{\"end\":37489,\"start\":37476},{\"end\":37500,\"start\":37489},{\"end\":37510,\"start\":37500},{\"end\":37520,\"start\":37510},{\"end\":37857,\"start\":37847},{\"end\":37869,\"start\":37857},{\"end\":38155,\"start\":38147},{\"end\":38162,\"start\":38155},{\"end\":38168,\"start\":38162},{\"end\":38175,\"start\":38168},{\"end\":38412,\"start\":38402},{\"end\":38421,\"start\":38412},{\"end\":38615,\"start\":38607},{\"end\":38628,\"start\":38615},{\"end\":38637,\"start\":38628},{\"end\":38652,\"start\":38637},{\"end\":38666,\"start\":38652},{\"end\":38917,\"start\":38907},{\"end\":38927,\"start\":38917},{\"end\":38940,\"start\":38927},{\"end\":38949,\"start\":38940},{\"end\":38955,\"start\":38949},{\"end\":38961,\"start\":38955},{\"end\":39232,\"start\":39223},{\"end\":39238,\"start\":39232},{\"end\":39475,\"start\":39468},{\"end\":39482,\"start\":39475},{\"end\":39798,\"start\":39787},{\"end\":40089,\"start\":40075},{\"end\":40098,\"start\":40089},{\"end\":40114,\"start\":40098},{\"end\":40576,\"start\":40558},{\"end\":40583,\"start\":40576},{\"end\":40595,\"start\":40583},{\"end\":40878,\"start\":40866},{\"end\":40999,\"start\":40986},{\"end\":41012,\"start\":40999},{\"end\":41024,\"start\":41012},{\"end\":41034,\"start\":41024},{\"end\":41045,\"start\":41034},{\"end\":41055,\"start\":41045},{\"end\":41066,\"start\":41055},{\"end\":41082,\"start\":41066},{\"end\":41091,\"start\":41082},{\"end\":41102,\"start\":41091},{\"end\":41467,\"start\":41457},{\"end\":41476,\"start\":41467},{\"end\":41485,\"start\":41476},{\"end\":41494,\"start\":41485},{\"end\":41506,\"start\":41494},{\"end\":41516,\"start\":41506},{\"end\":41527,\"start\":41516},{\"end\":41534,\"start\":41527},{\"end\":41548,\"start\":41534},{\"end\":41558,\"start\":41548},{\"end\":41571,\"start\":41558},{\"end\":41579,\"start\":41571},{\"end\":41587,\"start\":41579},{\"end\":41597,\"start\":41587},{\"end\":41607,\"start\":41597},{\"end\":41617,\"start\":41607},{\"end\":41633,\"start\":41617},{\"end\":41644,\"start\":41633},{\"end\":41652,\"start\":41644},{\"end\":41659,\"start\":41652},{\"end\":41671,\"start\":41659},{\"end\":42155,\"start\":42148},{\"end\":42164,\"start\":42155},{\"end\":42330,\"start\":42321},{\"end\":42595,\"start\":42584},{\"end\":42765,\"start\":42753},{\"end\":43031,\"start\":43017}]", "bib_venue": "[{\"end\":36792,\"start\":36731},{\"end\":37225,\"start\":37166},{\"end\":39266,\"start\":39256},{\"end\":33948,\"start\":33925},{\"end\":34174,\"start\":34138},{\"end\":34354,\"start\":34321},{\"end\":34713,\"start\":34656},{\"end\":35054,\"start\":35003},{\"end\":35398,\"start\":35393},{\"end\":35696,\"start\":35677},{\"end\":35986,\"start\":35967},{\"end\":36303,\"start\":36217},{\"end\":36729,\"start\":36653},{\"end\":37164,\"start\":37090},{\"end\":37569,\"start\":37520},{\"end\":37926,\"start\":37869},{\"end\":38193,\"start\":38175},{\"end\":38437,\"start\":38421},{\"end\":38687,\"start\":38666},{\"end\":38977,\"start\":38961},{\"end\":39254,\"start\":39238},{\"end\":39544,\"start\":39482},{\"end\":39821,\"start\":39798},{\"end\":40163,\"start\":40114},{\"end\":40639,\"start\":40595},{\"end\":40864,\"start\":40823},{\"end\":41138,\"start\":41102},{\"end\":41720,\"start\":41671},{\"end\":42146,\"start\":42115},{\"end\":42319,\"start\":42278},{\"end\":42605,\"start\":42595},{\"end\":42784,\"start\":42765},{\"end\":43015,\"start\":42957}]"}}}, "year": 2023, "month": 12, "day": 17}