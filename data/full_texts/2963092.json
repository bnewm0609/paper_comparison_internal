{"id": 2963092, "updated": "2023-09-28 23:34:47.916", "metadata": {"title": "Personalizing a Dialogue System with Transfer Reinforcement Learning", "authors": "[{\"first\":\"Kaixiang\",\"last\":\"Mo\",\"middle\":[]},{\"first\":\"Shuangyin\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Jiajun\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence", "journal": "arXiv: Artificial Intelligence", "publication_date": {"year": 2016, "month": 10, "day": 10}, "abstract": "It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose\"PETAL\"(PErsonalized Task-oriented diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1610.02891", "mag": "2963560082", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/MoZLLY18", "doi": "10.1609/aaai.v32i1.11938"}}, "content": {"source": {"pdf_hash": "fac97f41594524788bc4d519f1a2c6f65d18451f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1610.02891v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2cee826ff9310ef478f35de0d7ce34b31b3dced2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fac97f41594524788bc4d519f1a2c6f65d18451f.txt", "contents": "\nPersonalizing a Dialogue System with Transfer Reinforcement Learning\n\n\nKaixiang Mo \nThe Hong Kong University of Science and Technology\nHong KongChina\n\nYu Zhang \nThe Hong Kong University of Science and Technology\nHong KongChina\n\nShuangyin Li \nThe Hong Kong University of Science and Technology\nHong KongChina\n\n\u2020 \nThe Hong Kong University of Science and Technology\nHong KongChina\n\nJiajun Li \nThe Hong Kong University of Science and Technology\nHong KongChina\n\nQiang Yang qyang@cse.ust.hk\u2021jiajun.li@alumni.ust.hk \nThe Hong Kong University of Science and Technology\nHong KongChina\n\nPersonalizing a Dialogue System with Transfer Reinforcement Learning\n\nIt is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset is likely to overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users as a source domain and an individual user as a target domain, and to perform transfer learning from the source to the target domain. By following this idea, we propose a PErsonalized Task-oriented diALogue (PETAL) system, a transfer learning framework based on POMDP to construct a personalized dialogue system. The PETAL system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target domain. The proposed PETAL system can avoid the negative transfer problem by considering differences between source and target users in a personalized Q-function. Experimental results on a real-world coffee-shopping data and simulation data show that the proposed PETAL system can learn different optimal policies for different users, and thus effectively improve the dialogue quality under the personalized setting.\n\nIntroduction\n\nDialogue systems can be classified into two classes: open domain dialogue systems [15,5,18,12,13] and task-oriented dialogue systems [11,27,24,25,26]. Open domain dialogue systems do not limit the dialogue topic to a specific domain, and typically do not have a clear dialogue goal. Task-oriented dialogue systems aim to solve a specific task via dialogues. In this paper we focus on the dialogue systems which aim to assist users to finish a task such as ordering a cup of coffee.\n\nPersonalized task-oriented dialogue systems aim to help a user complete a dialogue task better and faster than non-personalized dialogue systems. Personalized dialogue systems can learn about the preferences and habits of a user during interactions with the user, and then utilize these personalized information to speed up the conversation process. Personalized dialogue systems could be categorized into rule-based dialogue systems [22,10,1] and learning-based dialogue systems [4,8]. In rulebased personalized dialogue systems, the dialogue state, system speech act and user speech act are predefined by developers, hence it is difficult for us to use this system when the dialogue state and the speech act are hard to define manually. Learning-based personalized dialogue systems could learn states and actions from training data without requiring explicit rules designed by developers.\n\nHowever, it is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. A personalized dialogue system trained on a small dataset is likely to fail on unseen but common dialogue cases due to over-fitting. One solution is to consider a collection of multiple users as a source domain and an individual user as a target domain, and transfer common dialogue knowledges to the target domain. When transferring dialogue knowledge, the challenge lies in the difference between the source and target domains. Some works [4,8] have been proposed to transfer dialogue knowledge among similar users, but they did not model the difference between different users, which might harm the performance in the target domain. In this paper, we propose a PErsonalized Taskoriented diALogue (PETAL) system, which is a transfer learning framework based on the POMDP for learning a personalized dialogue system. The PETAL system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. To achieve this goal, the PETAL system models personalized policies with a personalized Q-function defined as the expected cumulative general reward plus expected cumulative personal reward. The personalized Q-function can model differences between the source and target users, and thus can avoid the negative transfer problem brought by differences between source and target users. The flowchart of the PETAL system on the coffee-ordering task is shown in Figure 1. Experimental results on a realworld coffee-shopping dataset and simulation data show that the proposed PETAL system can choose different optimal actions for different users, and thus effectively improves the dialogue quality under the personalized setting.\n\nOur contributions are three-fold: Firstly, we tackle the problem of learning common dialogue knowledge from the source domain and adapting to the target user in a personalized dialogue system. In multi-turn dialogue systems, learning optimal responses in different situations is a non-trivial problem. One na\u00efve policy is to always choose previous seen sentences, but it is not necessarily optimal. For example in the online coffee ordering task, such na\u00efve policy could incur many logical mistakes such as asking repeated questions and confirming the order before the user finishes ordering. Secondly, we propose a transfer learning framework on the POMDP to model the preferences of different users. Unlike existing methods, the proposed PETAL system does not require a manuallydefined ground truth state space and it can model the personalized future expected reward. Finally, we demonstrate the effectiveness of the PETAL system on a real-world dialogue dataset as well as simulation data.\n\n\nRelated Works\n\nPersonalized dialogue systems could be categorized into rule-based dialogue systems and learningbased dialogue systems. For rule-based personalized dialogue systems, Thompson et al. [22] propose an interactive system where users can choose a place via an interactive conversational process and the system could learn user preference to improve future conversations. Personalization frameworks proposed in [10,1] extract and utilize user-related facts (triples), and then generate responses by applying predefined templates to these facts. Different from rule-based personalized dialogue systems, learning-based personalized dialogue systems can learn states and actions from training data without requiring explicit rules. Casanueva et al. [4] propose to initialize personalized dialogue systems for a target speaker with data from similar speakers in the source domain to improve the performance for the target speaker. This work requires a predefined user similarity metric to select similar source users, and when the selected similar users are different from the target user, the performance for the target user will degrade. Genevay and Laroche [8] propose to select and transfer an optimized policy from source users to a target user by using a multi-armed stochastic bandit algorithm which does not require a predefined user similarity measure. However, this method has a high complexity since for each target user, it requires n 2 bandit selection operations where n is the number of source users. Moreover, similar to [4], the differences between selected source users and the target user will deteriorate the performance. Different from these works, the proposed method does not assume the predefined dialogue states and system speech acts required by the rule-based systems, and it explicitly models the differences between users.\n\nTransfer learning [21,14,19,20,23] has been applied to other tasks in dialogue systems. Gasic et al. [6] uses transfer learning to extend a dialogue system to include a previously unseen concept. Gasic et al. [7] propose an incremental scheme to adapt an existing dialogue management system to an extended domain. These two works transfer parameters in the policy of the source domain as a prior to the target domain. However, these two models do not deal with multiple source domains and they do not have explicit personalized mechanisms for different users. As a consequence, negative transfer might occur when the differences between users are large. In contrast, the proposed method has an explicit personalization mechanism and can alleviate negative transfer.\n\nIn argumentation agents, there are some works [9,17,16] which study personalized dialogue system. However, these works, which aim to influence users' goal, have different motivations from ours and their formulations are totally different from ours.\n\n\nPETAL: A Framework for Personalized Dialogue Management\n\nIn this section, we introduce the proposed PETAL system. Here we use PETAL to denote both the proposed framework and the proposed algorithm.\n\n\nProblem Setting\n\nMatrices are denoted in bold capital case, row vectors are in bold lower case and scalars are in lower case. The text in the dialogues, denoted in curlicue, is represented by the the bag-of-words assumption. Each of the bag-of-words representations is a vector in which each entry has a binary value.\n\nSince the current state of the dialogue is not observable and the ground truth dialogue states are assumed to be unknown, we formulate the dialogue as a POMDP, which is defined as 7-tuple {S, A, O, P, R, Z, \u03b3}, where S denotes the hidden unobservable states, A denotes the replies of the agent, O denotes users' utterances, P is the state transition probability function, R is the reward function, Z is the observation function, and \u03b3 \u2208 [0, 1] is the discounted factor. In the i-th turn of a dialogue with a user u, S u i is the hidden conversation state, O u i is the user utterance, A u i is the reply of the agent, and r u i is the reward. In the i-th turn, we only observe O u i , A u i and r u i . We define b u i as the belief state vector, which represents the probability distribution of unobserved S u i . Unlike previous work, we do not assume that the underlying ground truth state space S is provided. Instead we propose to learn a function to map the dialogue history\nH u i = {{O u k , A u k } i\u22121 k=0 , O u i } to a compact belief state vector b u i . The inputs for this problem include 1. Abundant dialogue data {{O us i , A us i } T i=0 } of source customers u s . 2. A few dialogue data {{O ut i , A ut i } T i=0 } of the target customer u t .\nThe expected output is 1. A policy \u03c0 ut for target user.\n\n\nThe Framework\n\nIn order to solve the problem, we aim to find a policy \u03c0 ut for the target user, which could choose an appropriate action A ut i at the i-th turn based on current dialogue history H ut i , to maximize the cumulative reward defined as \u03c0 ut = arg max \u03c0 E \u221e k=0 \u03b3 k r ut t+k+1 . To model belief states, we introduce a state projection matrix M to map dialogue history\nH u i to belief state b u i , i.e., b u i = f (H u i ; M)\n. The Q-function is defined as the expected cumulative reward according to policy \u03c0 u by starting from belief state b u i and taking action A u i as\nQ \u03c0u (H u i , A u i ) = E \u03c0 \u221e k=0 \u03b3 k r u t+k+1 |H u i , A u i .\nWe choose value-based approaches because there is usually a small number of training data in the target domain, while policy-based approaches, which generate responses word by word, require a lot of training data.\n\nIn order to build a personalized dialogue system for the target user, we need to learn a personalized Q-function Q \u03c0u t for this user. However, since the training data {{O ut i , A ut i } T i } for the target user u t is very limited, we can hardly estimate the personalized Q-function Q \u03c0u t . In order to learn an accurate Q \u03c0u t , we can transfer common dialogue knowledge from the source domain, which has a lot of data from many other users {{O us i , A us i } T i=0 }. However, different users may have different preferences, hence directly using the data from source users would bring negative effects. We propose to model the personalized Q-function as a general Q-function Q g plus a personal one Q p :\nQ \u03c0u (H u i , A u i ) =Q g (H u i , A u i ; w) + Q p (H u i , A u i ; p u , w p ) \u2248E \u03c0u \u221e k=0 \u03b3 k r u,g t+k+1 |H u i , A u i + E \u03c0u \u221e k=0 \u03b3 k r u,p t+k+1 |H u i , A u i ,\nwhere r u,g t and r u,p t denotes the general and personal rewards for user u at time t respectively, the general Q-function Q g (H u i , A u i ; w) captures the expected reward related to the general dialogue policy for all users, w is the set of parameters for the general Q-function and contains a large amount of parameters such that it requires a lot of training data, and the personal Q-function\nQ p (H u i , A u i ; p u , w p )\ncaptures the expected reward related to the preference of each user. The proposed framework is based on transfer learning. M, w and w p are shared across different users, which could be trained on source domains and then transferred to the target domain. These parameters contain the common dialogue knowledge, which is independent of users' preferences. Moreover, p u , which is user-specific, capture the preferences of different users.\n\n\nParametric Forms for Personalized Q-function\n\nIn this section, we introduce parametric forms for\nf (H u i ; M), Q g (H u i , A u i ; w) and Q p (H u i , A u i ; p u , w p )\nin the personalized Q-function. Dialogue states are defined as follows. All utterances and replies will be projected into state vectors with a state projection matrix M, where M is initialized with the word2vec and will be updated in the learning process. represents all previous agent replies, and a u i\u22121 represents the last agent reply. In order to model the correlations between entries in a u i and b u i , the general Q-function\nb u i = f (H u i ; M) maps the dialogue history, H u i = {{O u k , A u k } i\u22121 k=0 , O u i }, to a belief state vector. The belief state vector b u i is defined as b u i = o h,u i\u22121 , o u i , a h,u i\u22122 , a u i\u22121 , where \u03be = 0.Q g (H u i , A u i ; w) is defined as Q g (H u i , A u i ; w) = a u i W(b u i ) T ,\nwhere superscript T denotes the transpose of a vector or matrix, W \u2208 R d\u00d74d is a parameter matrix to be learned. Based on the properties of the Kronecker product and operator vec(\u00b7) which transforms a matrix to a vector in a columnwise manner, we can rewrite Q g (H u i , A u i ; w) as a linear function on\nw = vec(W) T \u2208 R 4d 2 : Q g (H u i , A u i ; w) = (b u i \u2297 a u i )w T , where b u i \u2297 a u i is the Kronecker product of b u\ni and a u i . In multi-round dialogue systems, there should be different optimal actions in different belief states. The rationale to use the Kronecker product is that the general Q-function should depend on the combination of belief state b u i and action a u i , but not independently on b u i and a u i . The personal Q-function learns personalized preference for each user to avoid the negative effect brought by transferring biased dialogue knowledge across users with different preferences. We denote by C j the set of all possible choices in the j-th choice set we want to collect and by {c u ij } m j=1 the choices proposed in the i-th agent response A u i , where m is the total number of order choices, hence c u ij is an exact choice in C j . For example, in the coffee-ordering task, C 1 = {Latte, Cappuccino,. . .} could be the type of coffees and c u i1 could be any coffee in C 1 . From the user side, c u ij is just the choice of user u for the j-th choice set in the i-th dialogue turn. For example, c u i1 could be \"latte\" and c u i2 could be \"iced\". Based on an assumption that different choice sets are independent of each other, for the j-th choice set, the probability of a user u to choose c u ij follows a categorical distribution C(c u ij ; p u j ) = p u j,c u ij where |C j | denotes the cardinality of a set, p u j \u2208 R |Cj | , and p u j,k denotes the k-th entry in p u j . Hence the personal Q-function for user u is formulated as\nQ p (H u i , A u i ; p u , w p ) = w p m j=1 C(c u ij ; p uj )\u03b4(C j , H u i ),\nwhere the personal preference p u = {p uj } m j=1 for user u is learned from training process, \u03b4(C j , H u i ) equals 1 if the user has not yet made a choice about C j in the dialogue history H u i and 0 otherwise. \u03b4(C j , H u i ) implies whether the system will receive a personal reward in the rest of the dialogue, as the Q-function models the cumulative future reward. Here w p controls the importance of the personalized reward and it is learned from data. When w p is close to zero, the Q-function will depend on the general dialogue policy. Note that m j=1 C(c u ij |p uj )\u03b4(C j , H u i ) is 0 if we know nothing about the user, or A u i does not show any personal preference of user u. Because the vocabulary of choices is much smaller than the whole vocabulary, we can estimate the personal preference parameters p u with a few dialogue data {{O ut i , A ut i } T i } from the target user. By combining the general and personal Q-functions, the personalized Q-function can finally be defined as\nQ \u03c0u (H u i , A u i ) = (b u i \u2297 a u i )w T + w p m j=1 C(c u ij |p uj )\u03b4(C j , H u i ).\nHere M, w, w p are shared across different users, which could be trained on the source domains and then transferred to the target domain.\n\n\nReward\n\nThe total reward is the sum of general reward and personal reward, which can be defined as follows:\n\n1. A personal reward r u,p of 0.3 will be received when the user confirms the suggestion of the agent, and a negative reward of \u22120.2 will be received if the user declines the suggestion by the agent. This is related to the personal information of the user. For example, the user could confirm the address suggested by the agent. 2. A general reward r u,g of 0.1 will be received when the user provides the information about each c j . 3. A general reward r u,g of 1.0 will be received when the user proceeds with payment. 4. A general reward r u,g of \u22120.05 will be received by the agent for each dialogue turn to encourage shorter dialogue, \u22120.2 will be received by the agent if it is generates non-logical responses such as asking repeated questions.\n\nNote that the personal reward could not be distinguished from the general reward during the training process.\n\n\nLoss Function and Parameter Learning\n\nThere are in total four sets of parameters to be learned. We denote all the parameters by \u0398 = {M, w, w p , {p u }}. When dealing with real-world data, the training set consists of (H u i , A u i , r u i ), which records optimal actions provided by human, and hence the loss function is defined as follows:\nL(\u0398) = E r u i + \u03b3Q(H u i+1 , A u i+1 |\u0398) \u2212 Q(H u i , A u i |\u0398) 2 .\nIn the on-policy training with a user simulator, the loss function is defined as\nL(\u0398) = E r u i + max A i+1 \u03b3Q(H u i+1 , A i+1 |\u0398) \u2212 Q(H u i , A u i |\u0398) 2 ,\nwhere r u i is the reward obtained at time step i and H u i+1 is the update dialogue history at time step i + 1.\n\nWe use the value iteration method [2] to learn both the general and personal Q-functions. We adopt an online stochastic gradient descent algorithm [3] with learning rate 0.0001 to optimize our model. Specifically, we use the State-Action-Reward-State-Action (SARSA) algorithm. In the on-policy training with the simulation, the model has decreasing probability \u03b7 = 0.2e \u2212 \u03b2 1000 of choosing a random reply in the candidate set so as to ensure the sufficient exploration, where \u03b2 is the number of training dialogues seen by the algorithm. \n\n\nTransfer Learning Algorithm\nfor {O u i , A u i } in D s do 8: for (H u i , A u i , r u i , H u i+1 , A u i+1 ) in {O u i , A u i } do\n\n9:\n\n\u0398t+1 \u2190 \u0398t + \u03b1\u2206\u0398L(\u0398t) return {M, w, wp} 10: function TRANSFER(D t , M, w, wp ) 11:\nfor {{O u i , A u i } T i } in D t do 12: for (H u i , A u i , r u i , H u i+1 , A u i+1 ) in {O u i , A u i } do 13: \u0398t+1 \u2190 \u0398t + \u03b1\u2206\u0398L(\u0398t) return {M, w, wp, {pu}}\nThe detailed PETAL algorithm is shown in Algorithm 1. We train our model for each user in the source domain. M, w and w p are shared by all users and there is a separate p u for each user in the source domain. We transfer M, w and w p to the target domain by using them to initialize the corresponding variables in the target domain, and then we train them as well as p u for each target user with limited training data. Since the source and target users might have different preferences, p u learned in source domain is not very useful in the target domain. The personal preference of each target user will be learned separately in each p u . Without modelling p u for each user, different preferences of source and target users might interfere with each other and thus cause negative transfer.\n\nThe number of parameters in our model is around d 2 + dv, where v is the total vocabulary size and d is the dimension of the state vector. In our experiment where v = 1, 500 and d = 50, the number of parameters in the general Q-function is about 85k and that for the personal Q-function is under 100 for each user, hence the parameters in the personalized Q-function could be learned accurately with the limited data in the target domain.\n\n\nExperiments\n\nIn this section, we experimentally verify the effectiveness of the proposed PETAL model by conducting experiments on a real-world dataset and a simulation dataset.\n\n\nBaselines\n\nWe compare the proposed PETAL model with six baseline algorithms including \"NoneTL\" which is trained only with the data from target users, \"Sim\" [4] which is trained with the data from both target user and the most similar user in the source domain, \"Bandit\" [8] in which for each target user, the most useful source user is identified by a bandit algorithm, \"PriorSim\" [6] in which for each target user, the policy from the most similar user in the source domain is used as a prior, \"PriorAll\" [6] in which for each target user, the dialogue policy trained on all the users in the source domain is used as a prior, and \"All\" where the policy is trained on all source users' data. In this section, we evaluate our model on a real-world dataset. This real-world dataset, which is collected between July 2015 and April 2016 from a O2O coffee ordering service, contains 2,185 coffee dialogues between 72 consumers and coffee makers. The users order coffee by providing the coffee type, the temperature, the cup size and the delivery address, hence there are 4 order choices. We select 52 users with more than 23 dialogues as the source domain. Each of the remaining 20 users is used separately as a target domain. In total, there are 1,859 coffee dialogues in the source domain and 329 coffee dialogues in the target domain. 221 earlier dialogues in the target domain are used as the training set and the remaining 108 dialogues form the test set. The statistics of this dataset is shown in Table 1.\n\n\nExperiments on Real-World Data\n\nFor each round of the testing conversation, a model will rank the ground truth reply A u i among 10 randomly chosen agent replies. The label assigned to A u i is 1 and those for randomly chosen agent replies are 0. By following [26], we calculate the AUC score for each turn in a conversation and the performance of an algorithm is measured by the average AUC score of each dialogue for every user in the test set.\n\nIn Figure 2(a), we report the mean and standard deviation of averaged AUC score with 5 different random seeds, which are used to randomly sample agent replies as candidates. The performance of \"NoneTL\", \"PriorSim\" and \"PriorAll\" are worse than \"All\" which directly transfers training data, because fitting only target domain data can cause the overfitting. Transferring data from similar users (i.e., \"Sim\") is not as good as transferring data from all source users (i.e., \"All\"), because common knowledge has to be learned from more data. The proposed \"PETAL\" method performs the best because it learns common knowledge from all users and avoids the negative transfer caused by different preferences among source and target users, which indicates that the proposed personalized model fits dialogues better and demonstrates the effectiveness of PETAL on this real-world dataset. Table 2: A case study on the real-world dataset. The last column shows candidate responses, where the ground truth response is marked with *. The first and second columns show predicted rewards of \"All\" and \"PETAL\" on these candidates. A case study is shown in Table 2 and due to space limit, we only show three candidates. From the results, we can see that the proposed \"PETAL\" method ranks the ground truth response in the first place based on the predicted reward given by the learned personalized Q-function but the \"All\" method without personalization ranks an wrong address higher, which demonstrates the effectiveness of the proposed method. \n\n\nExperiments on Simulation Data\n\nIn this section, we compare our model with baseline models on the simulated coffee-ordering dialogue data. The simulated users order coffee by providing their coffee type, temperature, size and delivery address, and the agents reply by choosing from a set of predefined candidate responses without knowing the speech act. We have 11 simulated users in the source domain, in which 10 users have  their own coffee preferences while the rest one has no preference. The target domain has 5 users, which have different preferences from users in the source domain. A simulator is designed based on the real-world dataset used in the previous section. The simulator will order according to his preference with probability 0.8 and otherwise the simulator will order coffee randomly. The training set of each user in the target domain has 20 dialogues and the test set has 300 dialogues. The reward in the experiment is the same as the reward defined in Section 3.4.\n\nEach model will choose a reply from a set of candidates generated with templates at each turn, and the simulator will react to the reply accordingly. For each model, we report the mean and standard deviation of averaged reward [8], averaged success rate [4] and averaged dialogue length over all possible target users, repeated for 5 times with different random seeds.\n\nThe results are shown in Figure 2(b), Figure 2(c) and Figure 2(d). PETAL outperforms all baselines and obtains the highest average reward, the highest success rate and the lowest dialogue length, which implies that PETAL has found a better dialogue policy which can adapt its behaviour according to the preference of target users and again demonstrates the effectiveness of PETAL in a live environment.\n\nWe show a typical case for the simulation data in Tables 3 and 4. The non-personalized dialogue system corresponding to the \"All\" model has to ask the users all the choices even for frequent users in Table 4, because there is no universal recommendation for all the frequent users with different preferences. However, PETAL has learned the target users' preferences in previous dialogues. As shown in Table 3, the response from the agent is specially tailored for the target user because personalized questions given by the PETAL method can guide the user to complete the coffee-ordering task faster than general questions, leading to shorter dialogue and higher averaged reward. If the user does not want everything as usual, which is shown in the second case of Table 3, PETAL can still react correctly due to the shared dialogue knowledge transferred from the source domain. These cases show that PETAL can choose different optimal actions for different users and effectively shorten the conversation.\n\n\nConclusion\n\nIn this paper, we tackle the problem of designing a personalized dialogue system. We propose the PETAL system, a transfer learning framework based on the POMDP, for learning a personalized dialogue system. The PETAL system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. We propose to model a personalized policy in the POMDP with a personalized Q-function. This framework can avoid the negative transfer problem brought by differences between the source users and the target user. Experimental results on the real-world coffee-ordering data and the simulation data show that PETAL can learn different optimal policies for different users, and thus effectively improve the dialogue quality under the personalized setting. As a future direction, we will investigate to transfer knowledge from heterogeneous domains such as knowledge graphs and images.\n\nFigure 1 :\n1The flowchart of the proposed PETAL system on the coffee-ordering task.\n\n\n8 is the memory factor to discount historical state vectors at each time step, o hi\u2212k a u k , and a u i\u22121 = A u i\u22121 M. Based on these definitions, we can see that o h,u i represents all previous user utterances, o u i represents the current user utterance, a h,u i\n\n\n: \u0398 = {M, w, wp {pu}} 3: procedure TRANSFER ALGORITHM(D s ,D t ) 4: {M, w, wp} \u2190 TRAIN-SOURCE-MODEL(D s ) 5: {M, w, wp, {pu}} \u2190TRANSFER(D t ,M,w,wp) 6: function TRAIN-SOURCE-MODEL(D s ) 7:\n\nFigure 2 :\n2Experimental results on real-world and simulations datasets.\n\nTable 1 :\n1Statistics of the datasetsSource Domain \nTarget Domain \nDataset \nUsers Dialogues Users Dialogues \nReal Data \n52 \n1,859 \n20 \n329 \nSimulation \n11 \n176,000 \n5 \n100 \n\n\n\nTable 3 :\n3Personalized Dialogue Cases User: I want a cup of coffee. Agent: Same as before? Tall hot macchiato and deliver to No.1199 Minsheng Road, Pudong District Shanghai?User: \nYes. \nAgent: \nPlease pay. \nUser: \nPayment completed. \nUser: \nI want a cup of coffee. \nAgent: \nSame as before? Tall hot macchiato and deliver to \nNo.1199 Minsheng Road, Pudong District, Shanghai? \nUser: \nI want iced mocha today. \nAgent: \nSure, please pay. \nUser: \nPayment completed. \n\n\n\nTable 4 :\n4A Non-Personalized Dialogue CaseUser: \nI want a cup of coffee. \nAgent: \nHot coffee? \nUser: \nYes. \nAgent: \nWhat would you like to drink? \nUser: \nHot macchiato, please. \nAgent: \nWhat's your address? \nUser: \nNo.1199 Minsheng Road, Pudong District, Shanghai. \nAgent: \nAlright. Tall, Grande, or Venti? \nUser: \nTall. \nAgent: \nPlease pay. \nUser: \nPayment completed. \n\n\n\nExample-based chatoriented dialogue system with personalized long-term memory. Jeesoo Bang, Hyungjong Noh, Yonghee Kim, Gary Geunbae Lee, Proceedings of International Conference on Big Data and Smart Computing. International Conference on Big Data and Smart ComputingJeesoo Bang, Hyungjong Noh, Yonghee Kim, and Gary Geunbae Lee. Example-based chat- oriented dialogue system with personalized long-term memory. In Proceedings of International Conference on Big Data and Smart Computing, pages 238-243, 2015.\n\nA Markovian decision process. Richard Bellman, DTIC DocumentTechnical reportRichard Bellman. A Markovian decision process. Technical report, DTIC Document, 1957.\n\nLarge-scale machine learning with stochastic gradient descent. L\u00e9on Bottou, Proceedings of 19th International Conference on Computational Statistics. 19th International Conference on Computational StatisticsL\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of 19th International Conference on Computational Statistics, pages 177-186, 2010.\n\nKnowledge transfer between speakers for personalised dialogue management. Inigo Casanueva, Thomas Hain, Heidi Christensen, Ricard Marxer, Phil Green, Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 16th Annual Meeting of the Special Interest Group on Discourse and DialogueInigo Casanueva, Thomas Hain, Heidi Christensen, Ricard Marxer, and Phil Green. Knowledge transfer between speakers for personalised dialogue management. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2015.\n\nMichel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao, Bill Dolan, arXiv:1506.06863deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets. arXiv preprintMichel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao, and Bill Dolan. deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets. arXiv preprint arXiv:1506.06863, 2015.\n\nPOMDP-based dialogue manager adaptation to extended domains. Milica Ga\u0161ic, Catherine Breslin, Matthew Henderson, Dongho Kim, Martin Szummer, Blaise Thomson, Pirros Tsiakoulis, Steve Young, Proceedings of the 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 14th Annual Meeting of the Special Interest Group on Discourse and DialogueMilica Ga\u0161ic, Catherine Breslin, Matthew Henderson, Dongho Kim, Martin Szummer, Blaise Thomson, Pirros Tsiakoulis, and Steve Young. POMDP-based dialogue manager adaptation to extended domains. In Proceedings of the 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue, 2013.\n\nIncremental on-line adaptation of POMDPbased dialogue managers to extended domains. Milica Gasic, Dongho Kim, Pirros Tsiakoulis, Catherine Breslin, Matthew Henderson, Martin Szummer, Blaise Thomson, Steve J Young, Proceedings of the 15th Annual Conference of the International Speech Communication Association. the 15th Annual Conference of the International Speech Communication AssociationMilica Gasic, Dongho Kim, Pirros Tsiakoulis, Catherine Breslin, Matthew Henderson, Martin Szummer, Blaise Thomson, and Steve J. Young. Incremental on-line adaptation of POMDP- based dialogue managers to extended domains. In Proceedings of the 15th Annual Conference of the International Speech Communication Association, pages 140-144, 2014.\n\nTransfer learning for user adaptation in spoken dialogue systems. Aude Genevay, Romain Laroche, Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems. the 2016 International Conference on Autonomous Agents & Multiagent SystemsAude Genevay and Romain Laroche. Transfer learning for user adaptation in spoken dialogue systems. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pages 975-983, 2016.\n\nReinforcement learning of cooperative persuasive dialogue policies using framing. Takuya Hiraoka, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura, Proceedings of the 25th International Conference on Computational Linguistics. the 25th International Conference on Computational LinguisticsTakuya Hiraoka, Graham Neubig, Sakriani Sakti, Tomoki Toda, and Satoshi Nakamura. Rein- forcement learning of cooperative persuasive dialogue policies using framing. In Proceedings of the 25th International Conference on Computational Linguistics, pages 1706-1717, 2014.\n\nAcquisition and use of long-term memory for personalized dialog systems. Yonghee Kim, Jeesoo Bang, Junhwi Choi, Seonghan Ryu, Sangjun Koo, Gary Geunbae Lee, Proceedings of International Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine Interaction. International Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine InteractionYonghee Kim, Jeesoo Bang, Junhwi Choi, Seonghan Ryu, Sangjun Koo, and Gary Geunbae Lee. Acquisition and use of long-term memory for personalized dialog systems. In Proceedings of International Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine Interaction, pages 78-87, 2014.\n\nLearning dialogue strategies within the Markov decision process framework. Esther Levin, Roberto Pieraccini, Wieland Eckert, Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding. IEEE Workshop on Automatic Speech Recognition and UnderstandingEsther Levin, Roberto Pieraccini, and Wieland Eckert. Learning dialogue strategies within the Markov decision process framework. In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding, pages 72-79, 1997.\n\nDeep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, arXiv:1606.01541arXiv preprintJiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.\n\nSequence to backward and forward sequences: A content-introducing approach to generative short-text conversation. Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, Zhi Jin, arXiv:1607.00970arXiv preprintLili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation. arXiv preprint arXiv:1607.00970, 2016.\n\nA survey on transfer learning. Qiang Sinno Jialin Pan, Yang, IEEE Transactions on Knowledge and Data Engineering. 2210Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345-1359, 2010.\n\nData-driven response generation in social media. Alan Ritter, Colin Cherry, William B Dolan, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingAlan Ritter, Colin Cherry, and William B Dolan. Data-driven response generation in social media. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583-593, 2011.\n\nProviding arguments in discussions on the basis of the prediction of human argumentative behavior. Ariel Rosenfeld, Sarit Kraus, ACM Transactions on Interactive Intelligent Systems. 6430Ariel Rosenfeld and Sarit Kraus. Providing arguments in discussions on the basis of the prediction of human argumentative behavior. ACM Transactions on Interactive Intelligent Systems, 6(4):30, 2016.\n\nStrategical argumentative agent for human persuasion. Ariel Rosenfeld, Sarit Kraus, Proceedings of the 22nd European Conference on Artificial Intelligence. the 22nd European Conference on Artificial IntelligenceAriel Rosenfeld and Sarit Kraus. Strategical argumentative agent for human persuasion. In Proceedings of the 22nd European Conference on Artificial Intelligence, 2016.\n\nHierarchical neural network generative models for movie dialogues. Alessandro Iulian V Serban, Yoshua Sordoni, Aaron Bengio, Joelle Courville, Pineau, arXiv:1507.04808arXiv preprintIulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808, 2015.\n\nMulti-transfer: Transfer learning with multiple views and multiple sources. Ben Tan, Erheng Zhong, Evan Wei Xiang, Qiang Yang, Statistical Analysis and Data Mining. 74Ben Tan, Erheng Zhong, Evan Wei Xiang, and Qiang Yang. Multi-transfer: Transfer learning with multiple views and multiple sources. Statistical Analysis and Data Mining, 7(4):282-293, 2014.\n\nTransitive transfer learning. Ben Tan, Yangqiu Song, Erheng Zhong, Qiang Yang, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningBen Tan, Yangqiu Song, Erheng Zhong, and Qiang Yang. Transitive transfer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1155-1164, 2015.\n\nTransfer learning for reinforcement learning domains: A survey. E Matthew, Peter Taylor, Stone, Journal of Machine Learning Research. 10Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10:1633-1685, 2009.\n\nA personalized system for conversational recommendations. Cynthia A Thompson, H Mehmet, Pat Goker, Langley, Journal of Artificial Intelligence Research. 21Cynthia A Thompson, Mehmet H Goker, and Pat Langley. A personalized system for conversa- tional recommendations. Journal of Artificial Intelligence Research, 21:393-428, 2004.\n\nTransfer knowledge between cities. Ying Wei, Yu Zheng, Qiang Yang, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningYing Wei, Yu Zheng, and Qiang Yang. Transfer knowledge between cities. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1905-1914, 2016.\n\nSemantically conditioned lstm-based natural language generation for spoken dialogue systems. Milica Tsung-Hsien Wen, Nikola Gasic, Pei-Hao Mrksic, David Su, Steve Vandyke, Young, arXiv:1508.01745arXiv preprintTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve Young. Semantically conditioned lstm-based natural language generation for spoken dialogue systems. arXiv preprint arXiv:1508.01745, 2015.\n\nA network-based end-to-end trainable task-oriented dialogue system. Milica Tsung-Hsien Wen, Nikola Gasic, Lina M Mrksic, Pei-Hao Rojas-Barahona, Stefan Su, David Ultes, Steve Vandyke, Young, arXiv:1604.04562arXiv preprintTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. A network-based end-to-end trainable task-oriented dialogue system. arXiv preprint arXiv:1604.04562, 2016.\n\nEnd-to-end LSTM-based dialog control optimized with supervised and reinforcement learning. D Jason, Geoffrey Williams, Zweig, arXiv:1606.01269arXiv preprintJason D Williams and Geoffrey Zweig. End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning. arXiv preprint arXiv:1606.01269, 2016.\n\nPOMDP-based statistical spoken dialog systems: A review. Steve Young, Milica Ga\u0161i\u0107, Blaise Thomson, Jason D Williams, Proceedings of the IEEE. 1015Steve Young, Milica Ga\u0161i\u0107, Blaise Thomson, and Jason D Williams. POMDP-based statistical spoken dialog systems: A review. Proceedings of the IEEE, 101(5):1160-1179, 2013.\n", "annotations": {"author": "[{\"end\":151,\"start\":72},{\"end\":228,\"start\":152},{\"end\":309,\"start\":229},{\"end\":379,\"start\":310},{\"end\":457,\"start\":380},{\"end\":577,\"start\":458}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":81},{\"end\":160,\"start\":155},{\"end\":241,\"start\":239},{\"end\":389,\"start\":387},{\"end\":468,\"start\":464}]", "author_first_name": "[{\"end\":80,\"start\":72},{\"end\":154,\"start\":152},{\"end\":238,\"start\":229},{\"end\":311,\"start\":310},{\"end\":386,\"start\":380},{\"end\":463,\"start\":458}]", "author_affiliation": "[{\"end\":150,\"start\":85},{\"end\":227,\"start\":162},{\"end\":308,\"start\":243},{\"end\":378,\"start\":313},{\"end\":456,\"start\":391},{\"end\":576,\"start\":511}]", "title": "[{\"end\":69,\"start\":1},{\"end\":646,\"start\":578}]", "venue": null, "abstract": "[{\"end\":1851,\"start\":648}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1953,\"start\":1949},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1955,\"start\":1953},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1958,\"start\":1955},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1961,\"start\":1958},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1964,\"start\":1961},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2004,\"start\":2000},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2007,\"start\":2004},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2010,\"start\":2007},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2013,\"start\":2010},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2016,\"start\":2013},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2788,\"start\":2784},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2791,\"start\":2788},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2793,\"start\":2791},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2833,\"start\":2830},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2835,\"start\":2833},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3836,\"start\":3833},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3838,\"start\":3836},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6262,\"start\":6258},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6485,\"start\":6481},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6487,\"start\":6485},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6819,\"start\":6816},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7229,\"start\":7226},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7606,\"start\":7603},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7941,\"start\":7937},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7944,\"start\":7941},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7947,\"start\":7944},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7950,\"start\":7947},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7953,\"start\":7950},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8023,\"start\":8020},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8131,\"start\":8128},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8735,\"start\":8732},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8738,\"start\":8735},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8741,\"start\":8738},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19216,\"start\":19213},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19329,\"start\":19326},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21680,\"start\":21677},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21794,\"start\":21791},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21905,\"start\":21902},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22030,\"start\":22027},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23295,\"start\":23291},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26231,\"start\":26228},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26258,\"start\":26255}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28794,\"start\":28710},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29061,\"start\":28795},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29252,\"start\":29062},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29326,\"start\":29253},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29502,\"start\":29327},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29969,\"start\":29503},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30343,\"start\":29970}]", "paragraph": "[{\"end\":2348,\"start\":1867},{\"end\":3240,\"start\":2350},{\"end\":5063,\"start\":3242},{\"end\":6058,\"start\":5065},{\"end\":7917,\"start\":6076},{\"end\":8684,\"start\":7919},{\"end\":8934,\"start\":8686},{\"end\":9134,\"start\":8994},{\"end\":9454,\"start\":9154},{\"end\":10436,\"start\":9456},{\"end\":10774,\"start\":10718},{\"end\":11156,\"start\":10792},{\"end\":11363,\"start\":11215},{\"end\":11642,\"start\":11429},{\"end\":12355,\"start\":11644},{\"end\":12928,\"start\":12527},{\"end\":13400,\"start\":12962},{\"end\":13499,\"start\":13449},{\"end\":14010,\"start\":13576},{\"end\":14627,\"start\":14321},{\"end\":16209,\"start\":14752},{\"end\":17292,\"start\":16289},{\"end\":17519,\"start\":17382},{\"end\":17629,\"start\":17530},{\"end\":18382,\"start\":17631},{\"end\":18493,\"start\":18384},{\"end\":18839,\"start\":18534},{\"end\":18988,\"start\":18908},{\"end\":19177,\"start\":19065},{\"end\":19717,\"start\":19179},{\"end\":19940,\"start\":19859},{\"end\":20899,\"start\":20104},{\"end\":21339,\"start\":20901},{\"end\":21518,\"start\":21355},{\"end\":23028,\"start\":21532},{\"end\":23477,\"start\":23063},{\"end\":25007,\"start\":23479},{\"end\":25999,\"start\":25042},{\"end\":26369,\"start\":26001},{\"end\":26773,\"start\":26371},{\"end\":27779,\"start\":26775},{\"end\":28709,\"start\":27794}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10717,\"start\":10437},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11214,\"start\":11157},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11428,\"start\":11364},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12526,\"start\":12356},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12961,\"start\":12929},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13575,\"start\":13500},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14237,\"start\":14011},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14320,\"start\":14237},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14751,\"start\":14628},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16288,\"start\":16210},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17381,\"start\":17293},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18907,\"start\":18840},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19064,\"start\":18989},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19853,\"start\":19748},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20103,\"start\":19941}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23027,\"start\":23020},{\"end\":24365,\"start\":24358},{\"end\":24626,\"start\":24619},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26839,\"start\":26825},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26982,\"start\":26975},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27183,\"start\":27176},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27546,\"start\":27539}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1865,\"start\":1853},{\"attributes\":{\"n\":\"2\"},\"end\":6074,\"start\":6061},{\"attributes\":{\"n\":\"3\"},\"end\":8992,\"start\":8937},{\"attributes\":{\"n\":\"3.1\"},\"end\":9152,\"start\":9137},{\"attributes\":{\"n\":\"3.2\"},\"end\":10790,\"start\":10777},{\"attributes\":{\"n\":\"3.3\"},\"end\":13447,\"start\":13403},{\"attributes\":{\"n\":\"3.4\"},\"end\":17528,\"start\":17522},{\"attributes\":{\"n\":\"3.5\"},\"end\":18532,\"start\":18496},{\"attributes\":{\"n\":\"3.6\"},\"end\":19747,\"start\":19720},{\"end\":19857,\"start\":19855},{\"attributes\":{\"n\":\"4\"},\"end\":21353,\"start\":21342},{\"attributes\":{\"n\":\"4.1\"},\"end\":21530,\"start\":21521},{\"attributes\":{\"n\":\"4.2\"},\"end\":23061,\"start\":23031},{\"attributes\":{\"n\":\"4.3\"},\"end\":25040,\"start\":25010},{\"attributes\":{\"n\":\"5\"},\"end\":27792,\"start\":27782},{\"end\":28721,\"start\":28711},{\"end\":29264,\"start\":29254},{\"end\":29337,\"start\":29328},{\"end\":29513,\"start\":29504},{\"end\":29980,\"start\":29971}]", "table": "[{\"end\":29502,\"start\":29365},{\"end\":29969,\"start\":29678},{\"end\":30343,\"start\":30014}]", "figure_caption": "[{\"end\":28794,\"start\":28723},{\"end\":29061,\"start\":28797},{\"end\":29252,\"start\":29064},{\"end\":29326,\"start\":29266},{\"end\":29365,\"start\":29339},{\"end\":29678,\"start\":29515},{\"end\":30014,\"start\":29982}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4805,\"start\":4797},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23490,\"start\":23482},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26404,\"start\":26396},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26417,\"start\":26409},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26433,\"start\":26425}]", "bib_author_first_name": "[{\"end\":30430,\"start\":30424},{\"end\":30446,\"start\":30437},{\"end\":30459,\"start\":30452},{\"end\":30477,\"start\":30465},{\"end\":30891,\"start\":30884},{\"end\":31084,\"start\":31080},{\"end\":31478,\"start\":31473},{\"end\":31496,\"start\":31490},{\"end\":31508,\"start\":31503},{\"end\":31528,\"start\":31522},{\"end\":31541,\"start\":31537},{\"end\":31990,\"start\":31984},{\"end\":32004,\"start\":31999},{\"end\":32025,\"start\":32015},{\"end\":32043,\"start\":32035},{\"end\":32055,\"start\":32048},{\"end\":32067,\"start\":32062},{\"end\":32083,\"start\":32075},{\"end\":32102,\"start\":32094},{\"end\":32112,\"start\":32108},{\"end\":32581,\"start\":32575},{\"end\":32598,\"start\":32589},{\"end\":32615,\"start\":32608},{\"end\":32633,\"start\":32627},{\"end\":32645,\"start\":32639},{\"end\":32661,\"start\":32655},{\"end\":32677,\"start\":32671},{\"end\":32695,\"start\":32690},{\"end\":33267,\"start\":33261},{\"end\":33281,\"start\":33275},{\"end\":33293,\"start\":33287},{\"end\":33315,\"start\":33306},{\"end\":33332,\"start\":33325},{\"end\":33350,\"start\":33344},{\"end\":33366,\"start\":33360},{\"end\":33381,\"start\":33376},{\"end\":33383,\"start\":33382},{\"end\":33981,\"start\":33977},{\"end\":33997,\"start\":33991},{\"end\":34478,\"start\":34472},{\"end\":34494,\"start\":34488},{\"end\":34511,\"start\":34503},{\"end\":34525,\"start\":34519},{\"end\":34539,\"start\":34532},{\"end\":35043,\"start\":35036},{\"end\":35055,\"start\":35049},{\"end\":35068,\"start\":35062},{\"end\":35083,\"start\":35075},{\"end\":35096,\"start\":35089},{\"end\":35114,\"start\":35102},{\"end\":35722,\"start\":35716},{\"end\":35737,\"start\":35730},{\"end\":35757,\"start\":35750},{\"end\":36199,\"start\":36194},{\"end\":36208,\"start\":36204},{\"end\":36221,\"start\":36217},{\"end\":36233,\"start\":36230},{\"end\":36539,\"start\":36535},{\"end\":36551,\"start\":36545},{\"end\":36561,\"start\":36558},{\"end\":36569,\"start\":36567},{\"end\":36576,\"start\":36574},{\"end\":36587,\"start\":36584},{\"end\":36875,\"start\":36870},{\"end\":37152,\"start\":37148},{\"end\":37166,\"start\":37161},{\"end\":37184,\"start\":37175},{\"end\":37665,\"start\":37660},{\"end\":37682,\"start\":37677},{\"end\":38007,\"start\":38002},{\"end\":38024,\"start\":38019},{\"end\":38405,\"start\":38395},{\"end\":38429,\"start\":38423},{\"end\":38444,\"start\":38439},{\"end\":38459,\"start\":38453},{\"end\":38783,\"start\":38780},{\"end\":38795,\"start\":38789},{\"end\":38807,\"start\":38803},{\"end\":38811,\"start\":38808},{\"end\":38824,\"start\":38819},{\"end\":39094,\"start\":39091},{\"end\":39107,\"start\":39100},{\"end\":39120,\"start\":39114},{\"end\":39133,\"start\":39128},{\"end\":39598,\"start\":39597},{\"end\":39613,\"start\":39608},{\"end\":39905,\"start\":39904},{\"end\":39917,\"start\":39914},{\"end\":40197,\"start\":40193},{\"end\":40205,\"start\":40203},{\"end\":40218,\"start\":40213},{\"end\":40705,\"start\":40699},{\"end\":40729,\"start\":40723},{\"end\":40744,\"start\":40737},{\"end\":40758,\"start\":40753},{\"end\":40768,\"start\":40763},{\"end\":41112,\"start\":41106},{\"end\":41136,\"start\":41130},{\"end\":41148,\"start\":41144},{\"end\":41150,\"start\":41149},{\"end\":41166,\"start\":41159},{\"end\":41189,\"start\":41183},{\"end\":41199,\"start\":41194},{\"end\":41212,\"start\":41207},{\"end\":41586,\"start\":41585},{\"end\":41602,\"start\":41594},{\"end\":41880,\"start\":41875},{\"end\":41894,\"start\":41888},{\"end\":41908,\"start\":41902},{\"end\":41923,\"start\":41918},{\"end\":41925,\"start\":41924}]", "bib_author_last_name": "[{\"end\":30435,\"start\":30431},{\"end\":30450,\"start\":30447},{\"end\":30463,\"start\":30460},{\"end\":30481,\"start\":30478},{\"end\":30899,\"start\":30892},{\"end\":31091,\"start\":31085},{\"end\":31488,\"start\":31479},{\"end\":31501,\"start\":31497},{\"end\":31520,\"start\":31509},{\"end\":31535,\"start\":31529},{\"end\":31547,\"start\":31542},{\"end\":31997,\"start\":31991},{\"end\":32013,\"start\":32005},{\"end\":32033,\"start\":32026},{\"end\":32046,\"start\":32044},{\"end\":32060,\"start\":32056},{\"end\":32073,\"start\":32068},{\"end\":32092,\"start\":32084},{\"end\":32106,\"start\":32103},{\"end\":32118,\"start\":32113},{\"end\":32587,\"start\":32582},{\"end\":32606,\"start\":32599},{\"end\":32625,\"start\":32616},{\"end\":32637,\"start\":32634},{\"end\":32653,\"start\":32646},{\"end\":32669,\"start\":32662},{\"end\":32688,\"start\":32678},{\"end\":32701,\"start\":32696},{\"end\":33273,\"start\":33268},{\"end\":33285,\"start\":33282},{\"end\":33304,\"start\":33294},{\"end\":33323,\"start\":33316},{\"end\":33342,\"start\":33333},{\"end\":33358,\"start\":33351},{\"end\":33374,\"start\":33367},{\"end\":33389,\"start\":33384},{\"end\":33989,\"start\":33982},{\"end\":34005,\"start\":33998},{\"end\":34486,\"start\":34479},{\"end\":34501,\"start\":34495},{\"end\":34517,\"start\":34512},{\"end\":34530,\"start\":34526},{\"end\":34548,\"start\":34540},{\"end\":35047,\"start\":35044},{\"end\":35060,\"start\":35056},{\"end\":35073,\"start\":35069},{\"end\":35087,\"start\":35084},{\"end\":35100,\"start\":35097},{\"end\":35118,\"start\":35115},{\"end\":35728,\"start\":35723},{\"end\":35748,\"start\":35738},{\"end\":35764,\"start\":35758},{\"end\":36202,\"start\":36200},{\"end\":36215,\"start\":36209},{\"end\":36228,\"start\":36222},{\"end\":36242,\"start\":36234},{\"end\":36543,\"start\":36540},{\"end\":36556,\"start\":36552},{\"end\":36565,\"start\":36562},{\"end\":36572,\"start\":36570},{\"end\":36582,\"start\":36577},{\"end\":36591,\"start\":36588},{\"end\":36892,\"start\":36876},{\"end\":36898,\"start\":36894},{\"end\":37159,\"start\":37153},{\"end\":37173,\"start\":37167},{\"end\":37190,\"start\":37185},{\"end\":37675,\"start\":37666},{\"end\":37688,\"start\":37683},{\"end\":38017,\"start\":38008},{\"end\":38030,\"start\":38025},{\"end\":38421,\"start\":38406},{\"end\":38437,\"start\":38430},{\"end\":38451,\"start\":38445},{\"end\":38469,\"start\":38460},{\"end\":38477,\"start\":38471},{\"end\":38787,\"start\":38784},{\"end\":38801,\"start\":38796},{\"end\":38817,\"start\":38812},{\"end\":38829,\"start\":38825},{\"end\":39098,\"start\":39095},{\"end\":39112,\"start\":39108},{\"end\":39126,\"start\":39121},{\"end\":39138,\"start\":39134},{\"end\":39606,\"start\":39599},{\"end\":39620,\"start\":39614},{\"end\":39627,\"start\":39622},{\"end\":39902,\"start\":39884},{\"end\":39912,\"start\":39906},{\"end\":39923,\"start\":39918},{\"end\":39932,\"start\":39925},{\"end\":40201,\"start\":40198},{\"end\":40211,\"start\":40206},{\"end\":40223,\"start\":40219},{\"end\":40721,\"start\":40706},{\"end\":40735,\"start\":40730},{\"end\":40751,\"start\":40745},{\"end\":40761,\"start\":40759},{\"end\":40776,\"start\":40769},{\"end\":40783,\"start\":40778},{\"end\":41128,\"start\":41113},{\"end\":41142,\"start\":41137},{\"end\":41157,\"start\":41151},{\"end\":41181,\"start\":41167},{\"end\":41192,\"start\":41190},{\"end\":41205,\"start\":41200},{\"end\":41220,\"start\":41213},{\"end\":41227,\"start\":41222},{\"end\":41592,\"start\":41587},{\"end\":41611,\"start\":41603},{\"end\":41618,\"start\":41613},{\"end\":41886,\"start\":41881},{\"end\":41900,\"start\":41895},{\"end\":41916,\"start\":41909},{\"end\":41934,\"start\":41926}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":30852,\"start\":30345},{\"attributes\":{\"id\":\"b1\"},\"end\":31015,\"start\":30854},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":115963355},\"end\":31397,\"start\":31017},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5736825},\"end\":31982,\"start\":31399},{\"attributes\":{\"doi\":\"arXiv:1506.06863\",\"id\":\"b4\"},\"end\":32512,\"start\":31984},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":11193872},\"end\":33175,\"start\":32514},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":12489276},\"end\":33909,\"start\":33177},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15497443},\"end\":34388,\"start\":33911},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13214001},\"end\":34961,\"start\":34390},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":35725744},\"end\":35639,\"start\":34963},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":62561424},\"end\":36139,\"start\":35641},{\"attributes\":{\"doi\":\"arXiv:1606.01541\",\"id\":\"b11\"},\"end\":36419,\"start\":36141},{\"attributes\":{\"doi\":\"arXiv:1607.00970\",\"id\":\"b12\"},\"end\":36837,\"start\":36421},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":740063},\"end\":37097,\"start\":36839},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":780171},\"end\":37559,\"start\":37099},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15788992},\"end\":37946,\"start\":37561},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11133579},\"end\":38326,\"start\":37948},{\"attributes\":{\"doi\":\"arXiv:1507.04808\",\"id\":\"b17\"},\"end\":38702,\"start\":38328},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5972669},\"end\":39059,\"start\":38704},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":12308197},\"end\":39531,\"start\":39061},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":17216004},\"end\":39824,\"start\":39533},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2411932},\"end\":40156,\"start\":39826},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10014803},\"end\":40604,\"start\":40158},{\"attributes\":{\"doi\":\"arXiv:1508.01745\",\"id\":\"b23\"},\"end\":41036,\"start\":40606},{\"attributes\":{\"doi\":\"arXiv:1604.04562\",\"id\":\"b24\"},\"end\":41492,\"start\":41038},{\"attributes\":{\"doi\":\"arXiv:1606.01269\",\"id\":\"b25\"},\"end\":41816,\"start\":41494},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2364633},\"end\":42135,\"start\":41818}]", "bib_title": "[{\"end\":30422,\"start\":30345},{\"end\":31078,\"start\":31017},{\"end\":31471,\"start\":31399},{\"end\":32573,\"start\":32514},{\"end\":33259,\"start\":33177},{\"end\":33975,\"start\":33911},{\"end\":34470,\"start\":34390},{\"end\":35034,\"start\":34963},{\"end\":35714,\"start\":35641},{\"end\":36868,\"start\":36839},{\"end\":37146,\"start\":37099},{\"end\":37658,\"start\":37561},{\"end\":38000,\"start\":37948},{\"end\":38778,\"start\":38704},{\"end\":39089,\"start\":39061},{\"end\":39595,\"start\":39533},{\"end\":39882,\"start\":39826},{\"end\":40191,\"start\":40158},{\"end\":41873,\"start\":41818}]", "bib_author": "[{\"end\":30437,\"start\":30424},{\"end\":30452,\"start\":30437},{\"end\":30465,\"start\":30452},{\"end\":30483,\"start\":30465},{\"end\":30901,\"start\":30884},{\"end\":31093,\"start\":31080},{\"end\":31490,\"start\":31473},{\"end\":31503,\"start\":31490},{\"end\":31522,\"start\":31503},{\"end\":31537,\"start\":31522},{\"end\":31549,\"start\":31537},{\"end\":31999,\"start\":31984},{\"end\":32015,\"start\":31999},{\"end\":32035,\"start\":32015},{\"end\":32048,\"start\":32035},{\"end\":32062,\"start\":32048},{\"end\":32075,\"start\":32062},{\"end\":32094,\"start\":32075},{\"end\":32108,\"start\":32094},{\"end\":32120,\"start\":32108},{\"end\":32589,\"start\":32575},{\"end\":32608,\"start\":32589},{\"end\":32627,\"start\":32608},{\"end\":32639,\"start\":32627},{\"end\":32655,\"start\":32639},{\"end\":32671,\"start\":32655},{\"end\":32690,\"start\":32671},{\"end\":32703,\"start\":32690},{\"end\":33275,\"start\":33261},{\"end\":33287,\"start\":33275},{\"end\":33306,\"start\":33287},{\"end\":33325,\"start\":33306},{\"end\":33344,\"start\":33325},{\"end\":33360,\"start\":33344},{\"end\":33376,\"start\":33360},{\"end\":33391,\"start\":33376},{\"end\":33991,\"start\":33977},{\"end\":34007,\"start\":33991},{\"end\":34488,\"start\":34472},{\"end\":34503,\"start\":34488},{\"end\":34519,\"start\":34503},{\"end\":34532,\"start\":34519},{\"end\":34550,\"start\":34532},{\"end\":35049,\"start\":35036},{\"end\":35062,\"start\":35049},{\"end\":35075,\"start\":35062},{\"end\":35089,\"start\":35075},{\"end\":35102,\"start\":35089},{\"end\":35120,\"start\":35102},{\"end\":35730,\"start\":35716},{\"end\":35750,\"start\":35730},{\"end\":35766,\"start\":35750},{\"end\":36204,\"start\":36194},{\"end\":36217,\"start\":36204},{\"end\":36230,\"start\":36217},{\"end\":36244,\"start\":36230},{\"end\":36545,\"start\":36535},{\"end\":36558,\"start\":36545},{\"end\":36567,\"start\":36558},{\"end\":36574,\"start\":36567},{\"end\":36584,\"start\":36574},{\"end\":36593,\"start\":36584},{\"end\":36894,\"start\":36870},{\"end\":36900,\"start\":36894},{\"end\":37161,\"start\":37148},{\"end\":37175,\"start\":37161},{\"end\":37192,\"start\":37175},{\"end\":37677,\"start\":37660},{\"end\":37690,\"start\":37677},{\"end\":38019,\"start\":38002},{\"end\":38032,\"start\":38019},{\"end\":38423,\"start\":38395},{\"end\":38439,\"start\":38423},{\"end\":38453,\"start\":38439},{\"end\":38471,\"start\":38453},{\"end\":38479,\"start\":38471},{\"end\":38789,\"start\":38780},{\"end\":38803,\"start\":38789},{\"end\":38819,\"start\":38803},{\"end\":38831,\"start\":38819},{\"end\":39100,\"start\":39091},{\"end\":39114,\"start\":39100},{\"end\":39128,\"start\":39114},{\"end\":39140,\"start\":39128},{\"end\":39608,\"start\":39597},{\"end\":39622,\"start\":39608},{\"end\":39629,\"start\":39622},{\"end\":39904,\"start\":39884},{\"end\":39914,\"start\":39904},{\"end\":39925,\"start\":39914},{\"end\":39934,\"start\":39925},{\"end\":40203,\"start\":40193},{\"end\":40213,\"start\":40203},{\"end\":40225,\"start\":40213},{\"end\":40723,\"start\":40699},{\"end\":40737,\"start\":40723},{\"end\":40753,\"start\":40737},{\"end\":40763,\"start\":40753},{\"end\":40778,\"start\":40763},{\"end\":40785,\"start\":40778},{\"end\":41130,\"start\":41106},{\"end\":41144,\"start\":41130},{\"end\":41159,\"start\":41144},{\"end\":41183,\"start\":41159},{\"end\":41194,\"start\":41183},{\"end\":41207,\"start\":41194},{\"end\":41222,\"start\":41207},{\"end\":41229,\"start\":41222},{\"end\":41594,\"start\":41585},{\"end\":41613,\"start\":41594},{\"end\":41620,\"start\":41613},{\"end\":41888,\"start\":41875},{\"end\":41902,\"start\":41888},{\"end\":41918,\"start\":41902},{\"end\":41936,\"start\":41918}]", "bib_venue": "[{\"end\":30612,\"start\":30556},{\"end\":31224,\"start\":31167},{\"end\":31724,\"start\":31645},{\"end\":32878,\"start\":32799},{\"end\":33568,\"start\":33488},{\"end\":34174,\"start\":34099},{\"end\":34691,\"start\":34629},{\"end\":35339,\"start\":35238},{\"end\":35909,\"start\":35846},{\"end\":37351,\"start\":37280},{\"end\":38159,\"start\":38104},{\"end\":39323,\"start\":39240},{\"end\":40408,\"start\":40325},{\"end\":30554,\"start\":30483},{\"end\":30882,\"start\":30854},{\"end\":31165,\"start\":31093},{\"end\":31643,\"start\":31549},{\"end\":32226,\"start\":32136},{\"end\":32797,\"start\":32703},{\"end\":33486,\"start\":33391},{\"end\":34097,\"start\":34007},{\"end\":34627,\"start\":34550},{\"end\":35236,\"start\":35120},{\"end\":35844,\"start\":35766},{\"end\":36192,\"start\":36141},{\"end\":36533,\"start\":36421},{\"end\":36951,\"start\":36900},{\"end\":37278,\"start\":37192},{\"end\":37741,\"start\":37690},{\"end\":38102,\"start\":38032},{\"end\":38393,\"start\":38328},{\"end\":38867,\"start\":38831},{\"end\":39238,\"start\":39140},{\"end\":39665,\"start\":39629},{\"end\":39977,\"start\":39934},{\"end\":40323,\"start\":40225},{\"end\":40697,\"start\":40606},{\"end\":41104,\"start\":41038},{\"end\":41583,\"start\":41494},{\"end\":41959,\"start\":41936}]"}}}, "year": 2023, "month": 12, "day": 17}