{"id": 80628410, "updated": "2023-10-02 02:56:30.737", "metadata": {"title": "Pose Graph Optimization for Unsupervised Monocular Visual Odometry", "authors": "[{\"first\":\"Yang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yoshitaka\",\"last\":\"Ushiku\",\"middle\":[]},{\"first\":\"Tatsuya\",\"last\":\"Harada\",\"middle\":[]}]", "venue": "2019 International Conference on Robotics and Automation (ICRA)", "journal": "2019 International Conference on Robotics and Automation (ICRA)", "publication_date": {"year": 2019, "month": 3, "day": 15}, "abstract": "Unsupervised Learning based monocular visual odometry (VO) has lately drawn significant attention for its potential in label-free leaning ability and robustness to camera parameters and environmental variations. However, partially due to the lack of drift correction technique, these methods are still by far less accurate than geometric approaches for large-scale odometry estimation. In this paper, we propose to leverage graph optimization and loop closure detection to overcome limitations of unsupervised learning based monocular visual odometry. To this end, we propose a hybrid VO system which combines an unsupervised monocular VO called NeuralBundler with a pose graph optimization back-end. NeuralBundler is a neural network architecture that uses temporal and spatial photometric loss as main supervision and generates a windowed pose graph consists of multi-view 6DoF constraints. We propose a novel pose cycle consistency loss to relieve the tensions in the windowed pose graph, leading to improved performance and robustness. In the back-end, a global pose graph is built from local and loop 6DoF constraints estimated by NeuralBundler and is optimized over SE(3). Empirical evaluation on the KITTI odometry dataset demonstrates that 1) NeuralBundler achieves state-of-the-art performance on unsupervised monocular VO estimation, and 2) our whole approach can achieve efficient loop closing and show favorable overall translational accuracy compared to established monocular SLAM systems.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1903.06315", "mag": "2967702344", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/LiUH19", "doi": "10.1109/icra.2019.8793706"}}, "content": {"source": {"pdf_hash": "809567afe8095c6db558b7bda667408710967b9a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1903.06315v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1903.06315", "status": "GREEN"}}, "grobid": {"id": "755bd2153011c76cca22112a2098907fadd0db60", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/809567afe8095c6db558b7bda667408710967b9a.txt", "contents": "\nPose Graph Optimization for Unsupervised Monocular Visual Odometry\n\n\nYang Li \nYoshitaka Ushiku \nTatsuya Harada \nPose Graph Optimization for Unsupervised Monocular Visual Odometry\n\nUnsupervised Learning based monocular visual odometry (VO) has lately drawn significant attention for its potential in label-free leaning ability and robustness to camera parameters and environmental variations. However, partially due to the lack of drift correction technique, these methods are still by far less accurate than geometric approaches for largescale odometry estimation. In this paper, we propose to leverage graph optimization and loop closure detection to overcome limitations of unsupervised learning based monocular visual odometry. To this end, we propose a hybrid VO system which combines an unsupervised monocular VO called NeuralBundler with a pose graph optimization back-end. NeuralBundler is a neural network architecture that uses temporal and spatial photometric loss as main supervision and generates a windowed pose graph consists of multi-view 6DoF constraints. We propose a novel pose cycle consistency loss to relieve the tensions in the windowed pose graph, leading to improved performance and robustness. In the back-end, a global pose graph is built from local and loop 6DoF constraints estimated by NeuralBundler, and is optimized over SE(3). Empirical evaluation on the KITTI odometry dataset demonstrates that 1) NeuralBundler achieves state-of-the-art performance on unsupervised monocular VO estimation, and 2) our whole approach can achieve efficient loop closing and show favorable overall translational accuracy compared to established monocular SLAM systems.\n\nI. INTRODUCTION\n\nNowadays, monocular visual odometry (VO) can be newly divided into two groups based on the technique and the framework adopted: geometry-based and learning-based approaches. The geometric approach is usually solved via shallow feature or photometric re-projection followed by online error minimization. Learning base visual odometry is a newly emerged solution and has already achieved promising results on some benchmarks. This approach is solved by offline training of End-to-End deep neural networks driven by a large number of image sequences. Benefiting from the nature of data-driven approach and the potential of deep neural networks, learning based monocular VO has shown clear advantages over geometric methods in the following aspects: 1) no need for parameter tuning effort, 2) robustness to tracking failure and scale drift [1], 3) capable of recovering metric scale from monocular image by using stereo image pairs in training phase [2][3] [4], 4) high potential in directly integrating semantic information for robust camera tracking [5] [6].\n\nDue to the existence of outliers, noises, etc., all frameframe VO systems suffer from drifts (the accumulation of  small errors over time). In the geometric lineup, the so-called graph-based SLAM mitigates this problem by combining the geometric VO with an optimization back-end to continuously regulate the landmark's positions and the camera's poses, as known as the graph optimization technique. Graph-based SLAM has achieved success in many cases with established systems, such as feature-based PTAM [7] and ORB-SLAM [8], and direct LSD-SLAM [9] and DSO [10]. Especially, with the help of loop closing, i.e. graph optimization with correctly established loop closure constraints, SLAM is able to significantly reduce global trajectory drift.\n\nPartially due to the lack of drift correction technique, learning based VO is still by far less accurate than the geometric approach. Therefore, it is worth exploiting the potential of SLAM's graph optimization technique for learning based visual odometry. However, such work has never been done. We presume that this is because most of the existing deep learning architectures are either trying to mimic the graph optimization process through memory-based LSTM network [11] or trying to integrate loop closing into a whole end-to-end learning process [13] [14]. These works ignore the fact that loop closing is randomly occurred event and requires simultaneously processing of sequence with arbitrary length, which the neural networks are inadequate to do.\n\nIn this work, we build on the main idea of the unsupervised VO architecture: SfMLeaner [1], the bag of visual word based place recognition tool: DBoW2 [15], and the insight of using Covisibility / Essential Graph optimization [7][8] [16] [17] for large-scale loop closing operation, to design a hybrid VO system with the following contributions:\n\n\u2022 An unsupervised learning based monocular visual odometry called NeuralBundler which produces a windowed pose graph from monocular image sequence with a novel training loss that enforces pose cycle consistency. \u2022 Efficient loop closing procedure based on the optimization of a pose graph which is built from local and loop 6DoF constraints estimated by the proposed unsupervised monocular VO. We present the evaluation on KITTI odometry dataset [18]. NeuralBundler achieves the state of the art performance on learning based visual odometry estimation and our whole approach is able to perform efficient loop closing and yields favorable overall translational accuracy compared to established monocular SLAM systems. To the best of our knowledge, this is the first attempt to combine deep learning based VO with the classic graph optimization technique. Our research provides insights into the design of future SLAM system which could directly integrate the robustness and perception ability of deep neural network.\n\n\nII. RELATED WORK\n\n\nA. Geometry based Visual Odometry\n\nGeometry based Visual Odometry is a well-studied problem with two main solutions: feature-based and direct methods. Feature-based approach usually consists of two stages: 1) data association, through hand-engineered feature extraction (e.g. SURF [19], SIFT [20] and ORB [21]) and matching, and 2) pose estimation via minimizing feature point re-projection error. Direct VO treats data association and pose estimation as a whole optimization problem and solve it by minimizing the photometric error. Although these methods are effective in many cases, they are usually hardcoded and require extensive parameter tuning effort in order to ensure performance in a given scenario. The reliance on accurate data association can lead to tracking failure in regions of low texture, illumination change, and occlusions. Moreover, geometric approaches inherently suffer from the fact that camera motion can only be estimated up to an unknown scale which also leads to scale drift over time.\n\n\nB. Learning based Visual Odometry\n\nThis line of research can be further divided into 2 categories: supervised and unsupervised methods. In supervised approaches, Wang et al. [11] train a deep recurrent network end-to-end to predict ego-motion using ground truth trajectory as supervision. Kendall et al. [22] directly regress the camera's world pose from RGB images with the convolutional neural network. Zhou et al. [12] proposed a coarse to fine deep learning framework to track camera based on keyframe. However, the cost of collecting ground truth poses limits the application of such methods.\n\nOn the other hand, the unsupervised approach attracts more attention for its label-free leaning ability. The first architecture that achieved unsupervised learning of egomotion from the video is SfMLeaner proposed by Zhou et al [1]. SfMLeaner takes consecutive temporal images to predict both depth and ego-motion with view synthesis as supervision. However, similar to geometric approaches, SfM-Leaner can only observe ego-motion in a relative scale from monocular image. Godard et al. [3] show that the solution to recovery metric scale for depth prediction is using stereo images constraints for network training. Soon after, Nan et al. [4] integrate such a monocular deep depth predictor into DSO [10] as direct virtual stereo measurements. The followup works from Ruihao et al. [2] and Huangying et al. [23] also show that leveraging stereo image pairs with known baseline in training phase enable the networks to recover metric scale for both depth and pose estimation.\n\nExisting learning based methods only focus on frameframe VO estimation. We propose to utilize the multiview discrepancies within a temporal window. Our method produces a windowed pose graph and uses a novel loss to ensure pose consistency in the graph. It will be described in Section III-B.\n\n\nC. Graph-based SLAM\n\nGraph-based SLAM maintains a global graph whose nodes represent camera's poses or landmarks and an edge represents a sensor measurement that constrains the connected poses [24]. Apparently, such constraints can be conflicting to each other since measurements are easily influenced by noise. Once such a graph is constructed, SLAM uses graph optimization method (i.e. nonlinear least-squares error minimization via the Gauss-Newton or Levenberg-Marquardt algorithm) to find a configuration of the nodes that is maximally consistent with all the constraints. The graph optimization procedure, with the presence of both camera pose and landmarks in the graph, is called Bundle-Adjustment (BA). Monocular SLAM systems that apply graph optimization include PTAM [7], LSD-SLAM [9], ORB-SLAM [8], DSO [10], etc.\n\nIn monocular SLAM, loop closure is solved through a pose graph optimization with 7DoF similarity constraints (Sim (3)) to correct the scale drift [25]. A pose graph is built on selected key-frame connected by the pose-pose constraints. Pose-pose constraints are defined by covisiblity [26] and estimated by a geometry base VO front end. Two poses are connected to each other if they share enough common features. The graph built on covisiblity is called Covisiblity Graph. In order to achieve scalable, real-time performance, Ral et al. [8] proposed to perform loop closing on a much lighter Essential Graph which retains all the nodes (key-frames) from Covisibility Graph and a subset of edges with high covisibility. As shown in [8], the optimization of a properly constructed Essential Graph is already very accurate that full Bundle Adjustment only makes marginal improvement. We take the idea of loop closing with graph optimization and apply it to an unsupervised learning based Loop closure is triggered by the place recognition technique. Appearance or image-image matching based methods, such as the bag of word approaches FAB-MAP [27] and DBoW2 [15], are dominating this area for their high efficiency. Ral Mur-Artal et al. [8] [28] proposed a bag of words place recognizer built on DBoW2 with ORB feature [21] and successful achieved real-time loop closing. In this work, for efficiency and simplicity, we used a similar loop closure detection procedure which is shown in Section IV-B.\n\n\nIII. UNSUPERVISED MONOCULAR VISUAL ODOMETRY\n\nIn this section, we will introduce our unsupervised approach for monocular visual estimation. The training procedure is shown in Fig. 2.\n\n\nA. Network Architecture\n\nWe name our model NeuralBundler in the sense that, similar to Bundle Adjustment, it performs jointly optimization of both poses and 3D position (depth map) in a neural network fashion. NeuralBundler consists of a pose estimation network and a depth estimation network, which are trained jointly and can be used separately in the testing phase.\n\nThe input to the pose estimation network is a stack of views from a sliding window of size N , and the output is a windowed pose graph which has N nodes representing the views and N \u00d7 (N \u2212 1) edges each of which represents the relative 6DoF motion between two views. The network consists of 7 stride-2 convolutions followed by two 1 \u00d7 1 convolutions with 6 \u00d7 N \u00d7 (N \u2212 1) output channels (corresponding to 3 Euler angles and a 3D translation for each edge in the windowed pose graph). Depth net produces one dense depth map for each RGB image. It has an encoder-decoder shape with skip connections between corresponding encoder and decoder blocks in order to generate high-resolution depth prediction with fine-grained details. The encoder is based on ResNet-50 [29] and each decoder block uses a nearestneighbor upsampling layer followed by a convolutional layer.\n\n\nB. Loss Function\n\nLet's denote N as the total number of views in the input window, I i as the i-th view. D i denotes the predicted depth map and T ij denotes the predicted relative motion from view i to view j. K is the camera's intrinsics. E is the set of the graph's edges, each of which is represented by a tuple of index: (i, j). The final loss is a weighted sum of the photometric loss and pose cycle consistency loss.\n\n1) Photometric Loss: Let p i be the homogeneous coordinates of a pixel in view i and p j as p i 's projected pixel onto the view j. Based on the epipolar geometry, we can obtain p j from p i through:\np j = KT ij D i K \u22121 p i\nThen, by applying the differentiable bilinear sampling mechanism proposed in spatial transformer networks [30], from view i we can synthesis view j, which is denoted as I i\u2192j . Inspired by Godard et al. [3], the quality of the reconstructed image is measured with the weighted sum of the l 1 loss and the single scale structural similarity (SSIM) [31] loss. Then the temporal photometric loss is:\nL temp pho = (i,j)\u2208E (1 \u2212 \u03b1)L l1 (I j , I i\u2192j ) + \u03b1L SSIM (I j , I i\u2192j )\nwhere \u03b1 is set to 0.25. In order to recover metric scale for depth and pose estimation, like [23], we apply the same photometric loss between spatial image pairs of the stereo camera. As shown in Fig. 2, we only use the stereo images from the beginning frame of the window. Therefore I 1 (default as left image) and I 1,R (right image) is the spatial pair. The projection I 1,R\u21921 is synthesized from the known stereo baseline pose and the predicted left depth map. Then the spatial photometric loss is:\n\nL spat pho = (1 \u2212 \u03b1)L l1 (I 1 , I 1,R\u21921 ) + \u03b1L SSIM (I 1 , I 1,R\u21921 )\n\n2) Pose Cycle Consistency Loss: The windowed pose graph is a complete directed graph containing N \u00d7 (N \u2212 1) 6DoF camera motion constraints. Obviously, these constraints may be contradictory to each other, which creates tensions in the pose graph. During network training, we relax the windowed pose graph by penalizing a pose cycle consistency loss. Say (i, j, k) are the indexes of three views in the input window. Then the cycle constraint T ij T jk T ki = I holds, where I is the identity matrix. Let's denote C as the set of possible cycles in the graph. We penalize the l 1 loss:\nL pos = (i,j,k)\u2208C L l1 (T ij T jk T ki , I)\nConsidering that this loss is similar to the objective of graph optimization (in Section IV-B), we are somewhat performing a windowed pose graph optimization in a neural network form. Section V-B demonstrates the efficiency of this loss.\n\n\nIV. BACK-END\n\nIn the back-end, as shown in Fig. 1, we maintain a global pose graph and insert new elements to it each time a new frame is processed by NeuralBundler. We optimize this graph if a loop is established.\n\n\nA. Pose Graph Construction\n\nPose graph is built on local and loop pose-pose constraints estimated by NeuralBundler. As shown in Fig. 3, local constraints are generated in a sliding window. Loop constraints are obtained in two crossed windows around the loop closure area. Specifically, when there is a loop closure detected between views I i and I j , the two input windows for NeuralBundler are < I i , I j\u22121 , ..., I j\u2212N +1 > and < I j , I i\u22121 , ..., I i\u2212N +1 >, where N is the window size.\n\n\nLocal window\n\nLoop window The system uses a bags of visual words place recognition tool, based on DBoW2, to perform loop closure detection. We use a predefined ORB-Vocabulary and ORB-Database which are created off-line with ORB descriptor extracted from a large set of images. Since querying the database with an image will return multiple candidates, similar to [8], we apply the following procedure to filter the candidates: in order to be accepted, 1) a loop candidate must be preceded by 6 or more consecutive loop detections, and 2) the loop candidate's corresponding 6DoF transformation must get enough in-liers after several RANSAC iterations.\n\n\nB. Pose Graph Optimization\n\nA 3D rigid body transformation T \u2208 SE(3), is defined by:\nT = R t 0 1 with R \u2208 SO(3) and t \u2208 R 3\nDuring optimization, T is mapped to a minimal representation in R 6 of the associated Lie-algebra through the logarithmic mapping function log SE(3) [32]. Given a pose graph constructed in the proposed way, the error in an edge is defined as:\ne i,j = log SE(3) (T \u22121 ij T \u22121 i T j )\nwhere T ij is the relative 6DoF transformation constraints, and the goal is to minimize the total energy:\n\u03c7 2 (T 2 , ..., T m ) = Tij (e T i,j e i,j )\nwith respect to the absolute poses T 2 , ..., T m . These absolute poses are initialized through a chain of relative 6DoF transformations starting from the world reference frame T 1 , which is fixed during the optimization. We use the Levenberg-Marquardt algorithm implemented in g2o [33] to carry out the optimization.\n\n\nV. EXPERIMENTAL RESULTS\n\n\nA. Implementation Detail\n\nWe implemented the networks using the publicly available TensorFlow [34] framework and train it with Tesla P100 GPUs. We trained our model from scratch for 30 epochs, with a mini-batch size of 4 using Adam optimizer [35], where \u03b2 1 = 0.9, \u03b2 2 = 0.999. We used an initial learning rate of 0.0001 and halve it every 1/5 of the total iterations. The size of the image window for the windowed pose graph estimation network is set to 3 and each image is resized to 416\u00d7128. In the testing phase, both the network inference and graph optimization are carried out on an Ubuntu PC equipped with GeForce GTX TITAN X GPU and Intel Core i5 2.4 GHz CPU. The pose estimation network, with about 168k parameters, can be used separately in testing time. Pose estimation network requires less than 400MB GPU memory with over 40 Hz real-time performance. For benchmarking, we apply the KIITI odometry datasets [18] which contain 11 sequences (00-10) with ground truth trajectory obtained through the IMU/GPS readings.\n\n\nB. Trajectory Evaluation\n\nQualitative comparisons of our trajectories and the ground truth are shown in Fig. 4, Fig. 5 and Fig. 6. We align the trajectories with the ground truth through rigid body transformations on SE(3) (No scaling is applied for that our VO system can recover the metric scale). Sequences 00, 02, 05, 06, 07, 09 contain loops that were correctly detected and closed by our system. Fig. 4 shows the effectiveness of our loop closing approach on sequence 00, 05 and 06. The top row shows the raw VO estimation of NeuralBundler, and the bottom row shows corresponding results after performing loop closing. The raw VO estimation gives decent   local accuracy while with gradually accumulated drift. After performing graph optimization with correctly detected loops, the drift is significantly reduced, leading to good fits with the ground truth.\n\nFor evaluation, first, we use the error metrics proposed in [18] to compare our approach with other unsupervised learning based visual odometry systems: UnDeepVO [2], SfMLeaner [1] and Huangying et al. [23]. As shown in table I. For ablation study, our NeuralBundler consistently yields better results than the variant without pose cycle consistency loss. NeuralBundler outperforms SfMLeaner and Huangying et al. with a large margin and shows comparable results to UnDeepVO (which trains the network with extra supervisions from point cloud alignment and stereo camera's left-right pose consistency [2]). Applying loop closing significantly reduced the error, while graph optimization without loop constraints (not shown) does not necessarily lead to better results, which makes clear the need of loop closures for global drift correction.\n\nThen, we compare the results of our method with the state of the art feature-based ORB-SLAM. This time, we directly assess the overall translational RMSE error of the trajectory. As shown in table II, our method achieves smaller overall translational error for most of the sequences (8 out of 11). This makes sense considering that though both methods use monocular image, our approach is more robust to scale drift.   6 shows the estimated trajectory on sequence 07 and 08. Geometry based ORB-SLAM suffers from sever scale drift, while NeuralBundler achieves superior performance on eliminating the scale drift. ORB-SLAM encounters tracking failure on sequence 01 (a high way sequence). However, partially due to the fact that geometric methods know exactly the pixel-pixel correspondence, ORB-SLAM yields smaller rotational errors, which could explain our 3 lost cases.\n\n\nVI. CONCLUSION AND DISCUSSION\n\nIn this work, we have presented a new monocular VO system which has an unsupervised learning based front-end called NeuralBundler and a graph optimization back-end. Results on KITTI odometry have proved the effectiveness of pose cycle consistency loss. Our whole approach can achieve efficient loop closure and show better overall RMSE than ORB-SLAM for most sequences in KITTI odometry datasets. A number of major challenges are yet to be addressed:\n\n\u2022 We have not applied any key-frame selection and edge removal procedure yet. In our method, every frame becomes a \"key-frame\", and each node is connected with 12 edges (in the case that window size N is 3 and the node is not on the head or tail), which lead to a much denser and more complex pose graph than the SLAM's version. Though real-time loop closing is achieved, such techniques are still necessary for the sake of scalability and efficiency. \u2022 The model is trained on sequences with fixed camera intrinsics, fixed input image size, and from limited scenes (car driving on the road). We need to solve the domain adaptation problem when applying to different situations. \u2022 Unsupervised VO still lose to traditional methods on Mean Rotation/Translation accuracy. A possible solution is to use the coarse-to-fine strategy to optimize the VO estimation as in [12].\n\n\nVII. ACKNOWLEDGMENTS\n\nYL would like to thank Tinghui Zhou and Huangying Zhan for helpful discussions.\n\n1\nThe authors are with the Department of Mechano-Informatics, Graduate School of Information Science and Technology, The University of Tokyo, 7-3-1 Hongo Bunkyo-ku, Tokyo, Japan {liyang, ushiku, harada}@mi.t.u-tokyo.ac.jp 2 Tatsuya Harada is with RIKEN\n\nFig. 1 :\n1Overview of the proposed VO system.\n\nFig. 2 :\n2Training procedure of NeuralBundler. 1 right image from the first frame and N left images are involved in the training phase. Both pose net and depth net only require monocular sequence in the testing phase. visual odometry. Details are shown in Section IV. Different from the monocular SLAM approach, we only optimize the pose graph with 6DoF constraints, i.e. SE(3). The reason is that perhaps owing to the nature of data-driven method, in our learning based VO, scale drift is so small, that, in the experiments, optimization over SE(3) and Sim(3) almost leads to the identical result.\n\nFig. 3 :\n3Image windows for building local and loop posepose constraints. The red link indicates a detected loop. Live frames are marked with blue color.\n\nFig. 4 :Fig. 5 :\n45Results on sequence 00, 05 and 07 from the KITII Odometry dataset. Top row: Raw estimation of NeuralBundler (Trajectory is constructed using the inter-frame motion: T 1\u21920 from the windowed pose graph, see Section III). Bottom row: After performing loop closing (Graph optimization with loop constraints). Results of our approach on sequence 01, 02, 03, and 09 from the KITII Odometry dataset.\n\nFig. 6 :\n6Results on sequence 08 (left) and 07 (right). Sequence 08 does not contain a loop. For sequence 07, we turn off the loop closing thread of ORB-SLAM. ORB-SLAM suffers from severe scale drift, and heavily rely on loop closing to eliminate scale drift.\n\nFig.\nFig. 6 shows the estimated trajectory on sequence 07 and 08. Geometry based ORB-SLAM suffers from sever scale drift, while NeuralBundler achieves superior performance on eliminating the scale drift. ORB-SLAM encounters tracking failure on sequence 01 (a high way sequence). However, partially due to the fact that geometric methods know exactly the pixel-pixel correspondence, ORB-SLAM yields smaller rotational errors, which could explain our 3 lost cases.\n\nTABLE I :\nIComparison with unsupervised learning based approaches. t rel (%) is translational error and r rel ( o ) is rotational error. Both are averaged over 100m to 800m intervals. Result of UndeepVO is obtained from[2] and for SfMLearner[1] and Huangying et al.[23] we ran their pre-trained model. All models share the same training setup. Three variants of our approach is included, NeuralBundler (w/o cycle) : pose cycle consistency loss is not used for training, NeuralBundler: pose cycle consistency is used for training, NeuralBundler+loop closing: our whole approach. Both the pose cycle consistency loss and loop closing show improvement over the baseline.Approach \n00 \n01 \n02 \n03 \n04 \n05 \n06 \n07 \n08 \n09 \n10 \n\nUnDeepVO [2] \nt rel \n4.41 \n-\n5.58 \n-\n-\n3.4 \n-\n3.15 \n4.08 \n-\n-\nr rel \n1.92 \n-\n2.44 \n-\n-\n1.5 \n-\n2.48 \n1.79 \n-\n-\n\nSfMLeaner [1] \nt rel \n60.53 35.17 58.75 11.0 \n4.49 \n19.14 25.88 21.33 21.90 54.38 14.33 \nr rel \n6.12 \n2.72 \n3.56 \n3.94 \n5.24 \n4.11 \n4.81 \n6.65 \n2.91 \n3.21 \n3.30 \n\nHuangying et al. [23] \nt rel \n6.62 \n23.98 6.93 \n-\n3.01 \n5.12 \n5.92 \n7.06 \n5.85 \n12.16 13.00 \nr rel \n3.57 \n1.78 \n2.39 \n-\n2.02 \n2.43 \n2.10 \n3.8 \n2.54 \n3.61 \n3.54 \n\nOurs, NeuralBundler (w/o pose cycle loss) \nt rel \n10.31 45.45 7.32 \n7.97 \n3.5 \n5.80 \n4.64 \n6.70 \n5.54 \n11.98 12.28 \nr rel \n2.78 \n2.13 \n2.71 \n4.16 \n2.01 \n2.26 \n3.85 \n3.46 \n2.10 \n3.43 \n3.97 \n\nOurs, NeuralBundler \nt rel \n4.33 \n17.98 6.89 \n4.51 \n2.3 \n3.91 \n4.6 \n3.56 \n4.04 \n8.10 \n12.9 \nr rel \n1.85 \n1.44 \n2.61 \n2.82 \n0.87 \n1.64 \n2.85 \n2.39 \n1.53 \n2.81 \n3.17 \n\nOurs, NeuralBundler + loop closing \nt rel \n3.24 \n-\n4.85 \n-\n-\n1.83 \n2.74 \n3.53 \n-\n6.23 \n-\nr rel \n1.35 \n-\n1.60 \n-\n-\n0.7 \n2.6 \n2.02 \n-\n2.11 \n-\n\n\n\nTABLE II :\nIIBundler Adjustment afterward. For trajectory alignment, we use 6DoF transformations and ORB-SLAM uses 7DoF transformations. Transformations are optimized to achieve the best alignment.RMSE error of estimated trajectories on KITTI Odometry Dataset. All the methods listed in the table perform \nloop closing if any loop is detected. Results of ORB-SLAM and ORB-SLAM + Global BA (20 its.) are taken from [8]. \nORB-SLAM: perform optimization on the Essential Graph [8], ORB-SLAM + Global BA (20 its): perform 20 iterations of \nglobal 00  *  \n02  *  \n05  *  \n06  *  \n07  *  \n09  *  \n01 \n03 \n04 \n08 \n10 \n\nApproach \nRMSE (m) \n\nOurs, NeuralBundler + Opt \n4.36 \n52.75 2.57 \n3.71 \n1.67 \n14.85 57.4 \n1.43 \n0.82 \n16.4 \n11.2 \n\nORB-SLAM \n6.68 \n21.75 8.23 \n14.68 3.36 \n7.62 \nX \n1.59 \n1.79 \n46.58 8.68 \n\nORB-SLAM + Global BA (20 its.) \n5.33 \n21.28 4.85 \n12.34 2.26 \n6.62 \nX \n1.51 \n1.62 \n46.68 8.80 \n* with loop closure. \nX unavailable due to tracking failure. \n\n\n\nUnsupervised learning of depth and ego-motion from video. T Zhou, M Brown, N Snavely, D G Lowe, CVPR. 2Zhou, T., Brown, M., Snavely, N., & Lowe, D. G. (2017, July). Unsupervised learning of depth and ego-motion from video. In CVPR (Vol. 2, No. 6, p. 7).\n\nUnDeepVO: Monocular visual odometry through unsupervised deep learning. R Li, S Wang, Z Long, D Gu, arXiv:1709.06841arXiv preprintLi, R., Wang, S., Long, Z., & Gu, D. (2017). UnDeepVO: Monocular visual odometry through unsupervised deep learning. arXiv preprint arXiv:1709.06841.\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, CVPR. 2Godard, C., Mac Aodha, O., & Brostow, G. J. (2017, July). Unsuper- vised monocular depth estimation with left-right consistency. In CVPR (Vol. 2, No. 6, p. 7).\n\nN Yang, R Wang, J Stckler, D Cremers, arXiv:1807.02570Deep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry. arXiv preprintYang, N., Wang, R., Stckler, J., & Cremers, D. (2018). Deep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for Monocular Direct Sparse Odometry. arXiv preprint arXiv:1807.02570.\n\nM Tomasz, The Future of Real-Time SLAM and Deep Learning vs SLAM. Tomasz M. (2016, January 13). The Future of Real-Time SLAM and Deep Learning vs SLAM. Retrieved September 4, 2018, from http://www.computervisionblog.com/2016/01/why-slam-matters- future-of-real-time.html\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionLong, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431- 3440).\n\nParallel tracking and mapping for small AR workspaces. G Klein, D Murray, 6th IEEE and ACM International Symposium on. IEEEMixed and Augmented RealityKlein, G., & Murray, D. (2007, November). Parallel tracking and mapping for small AR workspaces. In Mixed and Augmented Reality, 2007. ISMAR 2007. 6th IEEE and ACM International Symposium on (pp. 225-234). IEEE.\n\nORB-SLAM: a versatile and accurate monocular SLAM system. R Mur-Artal, J M M Montiel, J D Tardos, IEEE Transactions on Robotics. 315Mur-Artal, R., Montiel, J. M. M., & Tardos, J. D. (2015). ORB-SLAM: a versatile and accurate monocular SLAM system. IEEE Transactions on Robotics, 31(5), 1147-1163.\n\nLSD-SLAM: Large-scale direct monocular SLAM. J Engel, T Schps, D Cremers, European Conference on Computer Vision. ChamSpringerEngel, J., Schps, T., & Cremers, D. (2014, September). LSD-SLAM: Large-scale direct monocular SLAM. In European Conference on Computer Vision (pp. 834-849). Springer, Cham.\n\nDirect sparse odometry. J Engel, V Koltun, D Cremers, IEEE transactions on pattern analysis and machine intelligence. 40Engel, J., Koltun, V., & Cremers, D. (2018). Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3), 611-625.\n\nDeepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. S Wang, R Clark, H Wen, N Trigoni, 2017 IEEE International Conference on. IEEERobotics and Automation (ICRAWang, S., Clark, R., Wen, H., & Trigoni, N. (2017, May). Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks. In Robotics and Automation (ICRA), 2017 IEEE International Conference on (pp. 2043-2050). IEEE.\n\nH Zhou, B Ummenhofer, T Brox, DeepTAM: Deep Tracking and Mapping. H.Zhou, B. Ummenhofer, T. Brox, DeepTAM: Deep Tracking and Mapping, ECCV 2018.\n\nGlobal pose estimation with an attention-based recurrent network. E Parisotto, D S Chaplot, J Zhang, R Salakhutdinov, arXiv:1802.06857arXiv preprintParisotto, E., Chaplot, D. S., Zhang, J., & Salakhutdinov, R. (2018). Global pose estimation with an attention-based recurrent network. arXiv preprint arXiv:1802.06857.\n\nLearning to navigate in complex environments. P Mirowski, R Pascanu, F Viola, H Soyer, A J Ballard, A Banino, . . Kumaran, D , arXiv:1611.03673arXiv preprintMirowski, P., Pascanu, R., Viola, F., Soyer, H., Ballard, A. J., Banino, A., ... & Kumaran, D. (2016). Learning to navigate in complex environments. arXiv preprint arXiv:1611.03673.\n\nBags of binary words for fast place recognition in image sequences. D Glvez-Lpez, J D Tardos, IEEE Transactions on Robotics. 285Glvez-Lpez, D., & Tardos, J. D. (2012). Bags of binary words for fast place recognition in image sequences. IEEE Transactions on Robotics, 28(5), 1188-1197.\n\nDouble window optimisation for constant time visual SLAM. H Strasdat, A J Davison, J M Montiel, K Konolige, Computer Vision (ICCV), 2011 IEEE International Conference on. IEEEStrasdat, H., Davison, A. J., Montiel, J. M., & Konolige, K. (2011, November). Double window optimisation for constant time visual SLAM. In Computer Vision (ICCV), 2011 IEEE International Con- ference on (pp. 2352-2359). IEEE.\n\nClosing loops without places. C Mei, G Sibley, P Newman, Intelligent Robots and Systems (IROS). Mei, C., Sibley, G., & Newman, P. (2010, October). Closing loops without places. In Intelligent Robots and Systems (IROS), 2010\n\nIEEE/RSJ International Conference on. IEEEIEEE/RSJ International Conference on (pp. 3738-3744). IEEE.\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. A Geiger, P Lenz, R Urtasun, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEEGeiger, A., Lenz, P., & Urtasun, R. (2012, June). Are we ready for autonomous driving? the kitti vision benchmark suite. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on (pp. 3354-3361). IEEE.\n\nSurf: Speeded up robust features. H Bay, T Tuytelaars, L Van Gool, European conference on computer vision. Berlin, HeidelbergSpringerBay, H., Tuytelaars, T., & Van Gool, L. (2006, May). Surf: Speeded up robust features. In European conference on computer vision (pp. 404-417). Springer, Berlin, Heidelberg.\n\nDistinctive image features from scale-invariant keypoints. D G Lowe, International journal of computer vision. 602Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2), 91-110.\n\nORB: An efficient alternative to SIFT or SURF. E Rublee, V Rabaud, K Konolige, G Bradski, Computer Vision (ICCV), 2011 IEEE international conference on. IEEENovember)Rublee, E., Rabaud, V., Konolige, K., & Bradski, G. (2011, Novem- ber). ORB: An efficient alternative to SIFT or SURF. In Computer Vision (ICCV), 2011 IEEE international conference on (pp. 2564- 2571). IEEE.\n\nPosenet: A convolutional network for real-time 6-dof camera relocalization. A Kendall, M Grimes, R Cipolla, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKendall, A., Grimes, M., & Cipolla, R. (2015). Posenet: A convolu- tional network for real-time 6-dof camera relocalization. In Proceed- ings of the IEEE international conference on computer vision (pp. 2938-2946).\n\nUnsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction. H Zzhan, R Garg, C S Weerasekera, K Li, H Agarwal, I Reid, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZZhan, H., Garg, R., Weerasekera, C. S., Li, K., Agarwal, H., & Reid, I. (2018, March). Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 340-349).\n\nA tutorial on graph-based SLAM. G Grisetti, R Kummerle, C Stachniss, W Burgard, IEEE Intelligent Transportation Systems Magazine. 24Grisetti, G., Kummerle, R., Stachniss, C., & Burgard, W. (2010). A tutorial on graph-based SLAM. IEEE Intelligent Transportation Systems Magazine, 2(4), 31-43.\n\nH Strasdat, J Montiel, A J Davison, Scale drift-aware large scale monocular SLAM. Robotics: Science and Systems VI. 2Strasdat, H., Montiel, J., & Davison, A. J. (2010). Scale drift-aware large scale monocular SLAM. Robotics: Science and Systems VI, 2.\n\nClosing loops without places. C Mei, G Sibley, P Newman, Intelligent Robots and Systems (IROS). Mei, C., Sibley, G., & Newman, P. (2010, October). Closing loops without places. In Intelligent Robots and Systems (IROS), 2010\n\nIEEE/RSJ International Conference on. IEEEIEEE/RSJ International Conference on (pp. 3738-3744). IEEE.\n\nAppearance-only SLAM at large scale with FAB-MAP 2.0. M Cummins, P Newman, The International Journal of Robotics Research. 309Cummins, M., & Newman, P. (2011). Appearance-only SLAM at large scale with FAB-MAP 2.0. The International Journal of Robotics Research, 30(9), 1100-1123.\n\nFast relocalisation and loop closing in keyframe-based SLAM. R Mur-Artal, J D Tards, IEEE International Conference on. IEEERobotics and Automation (ICRA)Mur-Artal, R., & Tards, J. D. (2014, May). Fast relocalisation and loop closing in keyframe-based SLAM. In Robotics and Automation (ICRA), 2014 IEEE International Conference on (pp. 846-853). IEEE.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, Advances in neural information processing systems. Jaderberg, M., Simonyan, K., & Zisserman, A. (2015). Spatial trans- former networks. In Advances in neural information processing sys- tems (pp. 2017-2025).\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE transactions on image processing. 134Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4), 600-612.\n\nLocal accuracy and global consistency for efficient visual SLAM (Doctoral dissertation, Department of Computing. H Strasdat, Imperial College LondonStrasdat, H. (2012). Local accuracy and global consistency for effi- cient visual SLAM (Doctoral dissertation, Department of Computing, Imperial College London).\n\nMay). g 2 o: A general framework for graph optimization. R Kmmerle, G Grisetti, H Strasdat, K Konolige, W Burgard, Robotics and Automation (ICRA), 2011 IEEE International Conference on. IEEEKmmerle, R., Grisetti, G., Strasdat, H., Konolige, K., & Burgard, W. (2011, May). g 2 o: A general framework for graph optimization. In Robotics and Automation (ICRA), 2011 IEEE International Confer- ence on (pp. 3607-3613). IEEE.\n\nTensorflow: a system for large-scale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, . . Kudlur, M , OSDI. 16Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., ... & Kudlur, M. (2016, November). Tensorflow: a system for large-scale machine learning. In OSDI (Vol. 16, pp. 265-283).\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n", "annotations": {"author": "[{\"end\":78,\"start\":70},{\"end\":96,\"start\":79},{\"end\":112,\"start\":97}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":75},{\"end\":95,\"start\":89},{\"end\":111,\"start\":105}]", "author_first_name": "[{\"end\":74,\"start\":70},{\"end\":88,\"start\":79},{\"end\":104,\"start\":97}]", "author_affiliation": null, "title": "[{\"end\":67,\"start\":1},{\"end\":179,\"start\":113}]", "venue": null, "abstract": "[{\"end\":1683,\"start\":181}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2541,\"start\":2538},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2651,\"start\":2648},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2658,\"start\":2655},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2753,\"start\":2750},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2757,\"start\":2754},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3267,\"start\":3264},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3284,\"start\":3281},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3309,\"start\":3306},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3322,\"start\":3318},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3981,\"start\":3977},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4063,\"start\":4059},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4068,\"start\":4064},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4356,\"start\":4353},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4421,\"start\":4417},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4495,\"start\":4492},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4503,\"start\":4499},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4508,\"start\":4504},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5063,\"start\":5059},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5936,\"start\":5932},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5947,\"start\":5943},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5960,\"start\":5956},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6847,\"start\":6843},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6977,\"start\":6973},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7090,\"start\":7086},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7499,\"start\":7496},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7758,\"start\":7755},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7911,\"start\":7908},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7973,\"start\":7969},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8054,\"start\":8051},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8080,\"start\":8076},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8736,\"start\":8732},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9320,\"start\":9317},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9334,\"start\":9331},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9348,\"start\":9345},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9358,\"start\":9354},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9516,\"start\":9512},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9655,\"start\":9651},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9906,\"start\":9903},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10100,\"start\":10097},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10510,\"start\":10506},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10525,\"start\":10521},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10603,\"start\":10600},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10608,\"start\":10604},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10686,\"start\":10682},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12184,\"start\":12180},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13045,\"start\":13041},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13141,\"start\":13138},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13286,\"start\":13282},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13502,\"start\":13498},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15926,\"start\":15923},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16490,\"start\":16486},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17059,\"start\":17055},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17217,\"start\":17213},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17365,\"start\":17361},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18042,\"start\":18038},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19077,\"start\":19073},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19178,\"start\":19175},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19193,\"start\":19190},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19219,\"start\":19215},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19615,\"start\":19612},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22079,\"start\":22075},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24602,\"start\":24599},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24624,\"start\":24621},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24649,\"start\":24645}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22438,\"start\":22185},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22485,\"start\":22439},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23085,\"start\":22486},{\"attributes\":{\"id\":\"fig_3\"},\"end\":23240,\"start\":23086},{\"attributes\":{\"id\":\"fig_4\"},\"end\":23653,\"start\":23241},{\"attributes\":{\"id\":\"fig_5\"},\"end\":23914,\"start\":23654},{\"attributes\":{\"id\":\"fig_6\"},\"end\":24378,\"start\":23915},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26035,\"start\":24379},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":26996,\"start\":26036}]", "paragraph": "[{\"end\":2758,\"start\":1702},{\"end\":3505,\"start\":2760},{\"end\":4264,\"start\":3507},{\"end\":4611,\"start\":4266},{\"end\":5629,\"start\":4613},{\"end\":6666,\"start\":5686},{\"end\":7266,\"start\":6704},{\"end\":8243,\"start\":7268},{\"end\":8536,\"start\":8245},{\"end\":9364,\"start\":8560},{\"end\":10862,\"start\":9366},{\"end\":11046,\"start\":10910},{\"end\":11417,\"start\":11074},{\"end\":12282,\"start\":11419},{\"end\":12708,\"start\":12303},{\"end\":12909,\"start\":12710},{\"end\":13331,\"start\":12935},{\"end\":13907,\"start\":13405},{\"end\":13977,\"start\":13909},{\"end\":14563,\"start\":13979},{\"end\":14845,\"start\":14608},{\"end\":15062,\"start\":14862},{\"end\":15557,\"start\":15093},{\"end\":16210,\"start\":15574},{\"end\":16297,\"start\":16241},{\"end\":16579,\"start\":16337},{\"end\":16725,\"start\":16620},{\"end\":17090,\"start\":16771},{\"end\":18145,\"start\":17145},{\"end\":19011,\"start\":18174},{\"end\":19852,\"start\":19013},{\"end\":20725,\"start\":19854},{\"end\":21209,\"start\":20759},{\"end\":22080,\"start\":21211},{\"end\":22184,\"start\":22105}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12934,\"start\":12910},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13404,\"start\":13332},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14607,\"start\":14564},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16336,\"start\":16298},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16619,\"start\":16580},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16770,\"start\":16726}]", "table_ref": null, "section_header": "[{\"end\":1700,\"start\":1685},{\"end\":5648,\"start\":5632},{\"end\":5684,\"start\":5651},{\"end\":6702,\"start\":6669},{\"end\":8558,\"start\":8539},{\"end\":10908,\"start\":10865},{\"end\":11072,\"start\":11049},{\"end\":12301,\"start\":12285},{\"end\":14860,\"start\":14848},{\"end\":15091,\"start\":15065},{\"end\":15572,\"start\":15560},{\"end\":16239,\"start\":16213},{\"end\":17116,\"start\":17093},{\"end\":17143,\"start\":17119},{\"end\":18172,\"start\":18148},{\"end\":20757,\"start\":20728},{\"end\":22103,\"start\":22083},{\"end\":22187,\"start\":22186},{\"end\":22448,\"start\":22440},{\"end\":22495,\"start\":22487},{\"end\":23095,\"start\":23087},{\"end\":23258,\"start\":23242},{\"end\":23663,\"start\":23655},{\"end\":23920,\"start\":23916},{\"end\":24389,\"start\":24380},{\"end\":26047,\"start\":26037}]", "table": "[{\"end\":26035,\"start\":25047},{\"end\":26996,\"start\":26234}]", "figure_caption": "[{\"end\":22438,\"start\":22188},{\"end\":22485,\"start\":22450},{\"end\":23085,\"start\":22497},{\"end\":23240,\"start\":23097},{\"end\":23653,\"start\":23261},{\"end\":23914,\"start\":23665},{\"end\":24378,\"start\":23921},{\"end\":25047,\"start\":24391},{\"end\":26234,\"start\":26050}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11045,\"start\":11039},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13607,\"start\":13601},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14897,\"start\":14891},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15199,\"start\":15193},{\"end\":18266,\"start\":18252},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18277,\"start\":18271},{\"end\":18556,\"start\":18550},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20274,\"start\":20273}]", "bib_author_first_name": "[{\"end\":27057,\"start\":27056},{\"end\":27065,\"start\":27064},{\"end\":27074,\"start\":27073},{\"end\":27085,\"start\":27084},{\"end\":27087,\"start\":27086},{\"end\":27326,\"start\":27325},{\"end\":27332,\"start\":27331},{\"end\":27340,\"start\":27339},{\"end\":27348,\"start\":27347},{\"end\":27604,\"start\":27603},{\"end\":27614,\"start\":27613},{\"end\":27627,\"start\":27626},{\"end\":27629,\"start\":27628},{\"end\":27808,\"start\":27807},{\"end\":27816,\"start\":27815},{\"end\":27824,\"start\":27823},{\"end\":27835,\"start\":27834},{\"end\":28167,\"start\":28166},{\"end\":28495,\"start\":28494},{\"end\":28503,\"start\":28502},{\"end\":28516,\"start\":28515},{\"end\":28926,\"start\":28925},{\"end\":28935,\"start\":28934},{\"end\":29292,\"start\":29291},{\"end\":29305,\"start\":29304},{\"end\":29309,\"start\":29306},{\"end\":29320,\"start\":29319},{\"end\":29322,\"start\":29321},{\"end\":29577,\"start\":29576},{\"end\":29586,\"start\":29585},{\"end\":29595,\"start\":29594},{\"end\":29856,\"start\":29855},{\"end\":29865,\"start\":29864},{\"end\":29875,\"start\":29874},{\"end\":30196,\"start\":30195},{\"end\":30204,\"start\":30203},{\"end\":30213,\"start\":30212},{\"end\":30220,\"start\":30219},{\"end\":30551,\"start\":30550},{\"end\":30559,\"start\":30558},{\"end\":30573,\"start\":30572},{\"end\":30763,\"start\":30762},{\"end\":30776,\"start\":30775},{\"end\":30778,\"start\":30777},{\"end\":30789,\"start\":30788},{\"end\":30798,\"start\":30797},{\"end\":31061,\"start\":31060},{\"end\":31073,\"start\":31072},{\"end\":31084,\"start\":31083},{\"end\":31093,\"start\":31092},{\"end\":31102,\"start\":31101},{\"end\":31104,\"start\":31103},{\"end\":31115,\"start\":31114},{\"end\":31125,\"start\":31124},{\"end\":31127,\"start\":31126},{\"end\":31138,\"start\":31137},{\"end\":31423,\"start\":31422},{\"end\":31437,\"start\":31436},{\"end\":31439,\"start\":31438},{\"end\":31699,\"start\":31698},{\"end\":31711,\"start\":31710},{\"end\":31713,\"start\":31712},{\"end\":31724,\"start\":31723},{\"end\":31726,\"start\":31725},{\"end\":31737,\"start\":31736},{\"end\":32074,\"start\":32073},{\"end\":32081,\"start\":32080},{\"end\":32091,\"start\":32090},{\"end\":32443,\"start\":32442},{\"end\":32453,\"start\":32452},{\"end\":32461,\"start\":32460},{\"end\":32803,\"start\":32802},{\"end\":32810,\"start\":32809},{\"end\":32824,\"start\":32823},{\"end\":33136,\"start\":33135},{\"end\":33138,\"start\":33137},{\"end\":33375,\"start\":33374},{\"end\":33385,\"start\":33384},{\"end\":33395,\"start\":33394},{\"end\":33407,\"start\":33406},{\"end\":33779,\"start\":33778},{\"end\":33790,\"start\":33789},{\"end\":33800,\"start\":33799},{\"end\":34254,\"start\":34253},{\"end\":34263,\"start\":34262},{\"end\":34271,\"start\":34270},{\"end\":34273,\"start\":34272},{\"end\":34288,\"start\":34287},{\"end\":34294,\"start\":34293},{\"end\":34305,\"start\":34304},{\"end\":34777,\"start\":34776},{\"end\":34789,\"start\":34788},{\"end\":34801,\"start\":34800},{\"end\":34814,\"start\":34813},{\"end\":35038,\"start\":35037},{\"end\":35050,\"start\":35049},{\"end\":35061,\"start\":35060},{\"end\":35063,\"start\":35062},{\"end\":35321,\"start\":35320},{\"end\":35328,\"start\":35327},{\"end\":35338,\"start\":35337},{\"end\":35673,\"start\":35672},{\"end\":35684,\"start\":35683},{\"end\":35961,\"start\":35960},{\"end\":35974,\"start\":35973},{\"end\":35976,\"start\":35975},{\"end\":36298,\"start\":36297},{\"end\":36304,\"start\":36303},{\"end\":36313,\"start\":36312},{\"end\":36320,\"start\":36319},{\"end\":36687,\"start\":36686},{\"end\":36700,\"start\":36699},{\"end\":36712,\"start\":36711},{\"end\":37008,\"start\":37007},{\"end\":37016,\"start\":37015},{\"end\":37018,\"start\":37017},{\"end\":37027,\"start\":37026},{\"end\":37029,\"start\":37028},{\"end\":37039,\"start\":37038},{\"end\":37041,\"start\":37040},{\"end\":37407,\"start\":37406},{\"end\":37662,\"start\":37661},{\"end\":37673,\"start\":37672},{\"end\":37685,\"start\":37684},{\"end\":37697,\"start\":37696},{\"end\":37709,\"start\":37708},{\"end\":38082,\"start\":38081},{\"end\":38091,\"start\":38090},{\"end\":38101,\"start\":38100},{\"end\":38109,\"start\":38108},{\"end\":38117,\"start\":38116},{\"end\":38126,\"start\":38125},{\"end\":38134,\"start\":38133},{\"end\":38136,\"start\":38135},{\"end\":38146,\"start\":38145},{\"end\":38389,\"start\":38388},{\"end\":38391,\"start\":38390},{\"end\":38401,\"start\":38400}]", "bib_author_last_name": "[{\"end\":27062,\"start\":27058},{\"end\":27071,\"start\":27066},{\"end\":27082,\"start\":27075},{\"end\":27092,\"start\":27088},{\"end\":27329,\"start\":27327},{\"end\":27337,\"start\":27333},{\"end\":27345,\"start\":27341},{\"end\":27351,\"start\":27349},{\"end\":27611,\"start\":27605},{\"end\":27624,\"start\":27615},{\"end\":27637,\"start\":27630},{\"end\":27813,\"start\":27809},{\"end\":27821,\"start\":27817},{\"end\":27832,\"start\":27825},{\"end\":27843,\"start\":27836},{\"end\":28174,\"start\":28168},{\"end\":28500,\"start\":28496},{\"end\":28513,\"start\":28504},{\"end\":28524,\"start\":28517},{\"end\":28932,\"start\":28927},{\"end\":28942,\"start\":28936},{\"end\":29302,\"start\":29293},{\"end\":29317,\"start\":29310},{\"end\":29329,\"start\":29323},{\"end\":29583,\"start\":29578},{\"end\":29592,\"start\":29587},{\"end\":29603,\"start\":29596},{\"end\":29862,\"start\":29857},{\"end\":29872,\"start\":29866},{\"end\":29883,\"start\":29876},{\"end\":30201,\"start\":30197},{\"end\":30210,\"start\":30205},{\"end\":30217,\"start\":30214},{\"end\":30228,\"start\":30221},{\"end\":30556,\"start\":30552},{\"end\":30570,\"start\":30560},{\"end\":30578,\"start\":30574},{\"end\":30773,\"start\":30764},{\"end\":30786,\"start\":30779},{\"end\":30795,\"start\":30790},{\"end\":30812,\"start\":30799},{\"end\":31070,\"start\":31062},{\"end\":31081,\"start\":31074},{\"end\":31090,\"start\":31085},{\"end\":31099,\"start\":31094},{\"end\":31112,\"start\":31105},{\"end\":31122,\"start\":31116},{\"end\":31135,\"start\":31128},{\"end\":31434,\"start\":31424},{\"end\":31446,\"start\":31440},{\"end\":31708,\"start\":31700},{\"end\":31721,\"start\":31714},{\"end\":31734,\"start\":31727},{\"end\":31746,\"start\":31738},{\"end\":32078,\"start\":32075},{\"end\":32088,\"start\":32082},{\"end\":32098,\"start\":32092},{\"end\":32450,\"start\":32444},{\"end\":32458,\"start\":32454},{\"end\":32469,\"start\":32462},{\"end\":32807,\"start\":32804},{\"end\":32821,\"start\":32811},{\"end\":32833,\"start\":32825},{\"end\":33143,\"start\":33139},{\"end\":33382,\"start\":33376},{\"end\":33392,\"start\":33386},{\"end\":33404,\"start\":33396},{\"end\":33415,\"start\":33408},{\"end\":33787,\"start\":33780},{\"end\":33797,\"start\":33791},{\"end\":33808,\"start\":33801},{\"end\":34260,\"start\":34255},{\"end\":34268,\"start\":34264},{\"end\":34285,\"start\":34274},{\"end\":34291,\"start\":34289},{\"end\":34302,\"start\":34295},{\"end\":34310,\"start\":34306},{\"end\":34786,\"start\":34778},{\"end\":34798,\"start\":34790},{\"end\":34811,\"start\":34802},{\"end\":34822,\"start\":34815},{\"end\":35047,\"start\":35039},{\"end\":35058,\"start\":35051},{\"end\":35071,\"start\":35064},{\"end\":35325,\"start\":35322},{\"end\":35335,\"start\":35329},{\"end\":35345,\"start\":35339},{\"end\":35681,\"start\":35674},{\"end\":35691,\"start\":35685},{\"end\":35971,\"start\":35962},{\"end\":35982,\"start\":35977},{\"end\":36301,\"start\":36299},{\"end\":36310,\"start\":36305},{\"end\":36317,\"start\":36314},{\"end\":36324,\"start\":36321},{\"end\":36697,\"start\":36688},{\"end\":36709,\"start\":36701},{\"end\":36722,\"start\":36713},{\"end\":37013,\"start\":37009},{\"end\":37024,\"start\":37019},{\"end\":37036,\"start\":37030},{\"end\":37052,\"start\":37042},{\"end\":37416,\"start\":37408},{\"end\":37670,\"start\":37663},{\"end\":37682,\"start\":37674},{\"end\":37694,\"start\":37686},{\"end\":37706,\"start\":37698},{\"end\":37717,\"start\":37710},{\"end\":38088,\"start\":38083},{\"end\":38098,\"start\":38092},{\"end\":38106,\"start\":38102},{\"end\":38114,\"start\":38110},{\"end\":38123,\"start\":38118},{\"end\":38131,\"start\":38127},{\"end\":38143,\"start\":38137},{\"end\":38398,\"start\":38392},{\"end\":38404,\"start\":38402}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11977588},\"end\":27251,\"start\":26998},{\"attributes\":{\"doi\":\"arXiv:1709.06841\",\"id\":\"b1\"},\"end\":27532,\"start\":27253},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206596513},\"end\":27805,\"start\":27534},{\"attributes\":{\"doi\":\"arXiv:1807.02570\",\"id\":\"b3\"},\"end\":28164,\"start\":27807},{\"attributes\":{\"id\":\"b4\"},\"end\":28436,\"start\":28166},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1629541},\"end\":28868,\"start\":28438},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206986664},\"end\":29231,\"start\":28870},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206775100},\"end\":29529,\"start\":29233},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14547347},\"end\":29829,\"start\":29531},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3299195},\"end\":30099,\"start\":29831},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9114952},\"end\":30548,\"start\":30101},{\"attributes\":{\"id\":\"b11\"},\"end\":30694,\"start\":30550},{\"attributes\":{\"doi\":\"arXiv:1802.06857\",\"id\":\"b12\"},\"end\":31012,\"start\":30696},{\"attributes\":{\"doi\":\"arXiv:1611.03673\",\"id\":\"b13\"},\"end\":31352,\"start\":31014},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":710586},\"end\":31638,\"start\":31354},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":5441718},\"end\":32041,\"start\":31640},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1925381},\"end\":32266,\"start\":32043},{\"attributes\":{\"id\":\"b17\"},\"end\":32369,\"start\":32268},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6724907},\"end\":32766,\"start\":32371},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":161878},\"end\":33074,\"start\":32768},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":174065},\"end\":33325,\"start\":33076},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206769866},\"end\":33700,\"start\":33327},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":12888763},\"end\":34145,\"start\":33702},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4578162},\"end\":34742,\"start\":34147},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":5970112},\"end\":35035,\"start\":34744},{\"attributes\":{\"id\":\"b25\"},\"end\":35288,\"start\":35037},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1925381},\"end\":35513,\"start\":35290},{\"attributes\":{\"id\":\"b27\"},\"end\":35616,\"start\":35515},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":18537011},\"end\":35897,\"start\":35618},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15140191},\"end\":36249,\"start\":35899},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206594692},\"end\":36654,\"start\":36251},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6099034},\"end\":36931,\"start\":36656},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":207761262},\"end\":37291,\"start\":36933},{\"attributes\":{\"id\":\"b33\"},\"end\":37602,\"start\":37293},{\"attributes\":{\"id\":\"b34\"},\"end\":38024,\"start\":37604},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6287870},\"end\":38342,\"start\":38026},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b36\"},\"end\":38542,\"start\":38344}]", "bib_title": "[{\"end\":27054,\"start\":26998},{\"end\":27601,\"start\":27534},{\"end\":28492,\"start\":28438},{\"end\":28923,\"start\":28870},{\"end\":29289,\"start\":29233},{\"end\":29574,\"start\":29531},{\"end\":29853,\"start\":29831},{\"end\":30193,\"start\":30101},{\"end\":31420,\"start\":31354},{\"end\":31696,\"start\":31640},{\"end\":32071,\"start\":32043},{\"end\":32440,\"start\":32371},{\"end\":32800,\"start\":32768},{\"end\":33133,\"start\":33076},{\"end\":33372,\"start\":33327},{\"end\":33776,\"start\":33702},{\"end\":34251,\"start\":34147},{\"end\":34774,\"start\":34744},{\"end\":35318,\"start\":35290},{\"end\":35670,\"start\":35618},{\"end\":35958,\"start\":35899},{\"end\":36295,\"start\":36251},{\"end\":36684,\"start\":36656},{\"end\":37005,\"start\":36933},{\"end\":37659,\"start\":37604},{\"end\":38079,\"start\":38026}]", "bib_author": "[{\"end\":27064,\"start\":27056},{\"end\":27073,\"start\":27064},{\"end\":27084,\"start\":27073},{\"end\":27094,\"start\":27084},{\"end\":27331,\"start\":27325},{\"end\":27339,\"start\":27331},{\"end\":27347,\"start\":27339},{\"end\":27353,\"start\":27347},{\"end\":27613,\"start\":27603},{\"end\":27626,\"start\":27613},{\"end\":27639,\"start\":27626},{\"end\":27815,\"start\":27807},{\"end\":27823,\"start\":27815},{\"end\":27834,\"start\":27823},{\"end\":27845,\"start\":27834},{\"end\":28176,\"start\":28166},{\"end\":28502,\"start\":28494},{\"end\":28515,\"start\":28502},{\"end\":28526,\"start\":28515},{\"end\":28934,\"start\":28925},{\"end\":28944,\"start\":28934},{\"end\":29304,\"start\":29291},{\"end\":29319,\"start\":29304},{\"end\":29331,\"start\":29319},{\"end\":29585,\"start\":29576},{\"end\":29594,\"start\":29585},{\"end\":29605,\"start\":29594},{\"end\":29864,\"start\":29855},{\"end\":29874,\"start\":29864},{\"end\":29885,\"start\":29874},{\"end\":30203,\"start\":30195},{\"end\":30212,\"start\":30203},{\"end\":30219,\"start\":30212},{\"end\":30230,\"start\":30219},{\"end\":30558,\"start\":30550},{\"end\":30572,\"start\":30558},{\"end\":30580,\"start\":30572},{\"end\":30775,\"start\":30762},{\"end\":30788,\"start\":30775},{\"end\":30797,\"start\":30788},{\"end\":30814,\"start\":30797},{\"end\":31072,\"start\":31060},{\"end\":31083,\"start\":31072},{\"end\":31092,\"start\":31083},{\"end\":31101,\"start\":31092},{\"end\":31114,\"start\":31101},{\"end\":31124,\"start\":31114},{\"end\":31137,\"start\":31124},{\"end\":31141,\"start\":31137},{\"end\":31436,\"start\":31422},{\"end\":31448,\"start\":31436},{\"end\":31710,\"start\":31698},{\"end\":31723,\"start\":31710},{\"end\":31736,\"start\":31723},{\"end\":31748,\"start\":31736},{\"end\":32080,\"start\":32073},{\"end\":32090,\"start\":32080},{\"end\":32100,\"start\":32090},{\"end\":32452,\"start\":32442},{\"end\":32460,\"start\":32452},{\"end\":32471,\"start\":32460},{\"end\":32809,\"start\":32802},{\"end\":32823,\"start\":32809},{\"end\":32835,\"start\":32823},{\"end\":33145,\"start\":33135},{\"end\":33384,\"start\":33374},{\"end\":33394,\"start\":33384},{\"end\":33406,\"start\":33394},{\"end\":33417,\"start\":33406},{\"end\":33789,\"start\":33778},{\"end\":33799,\"start\":33789},{\"end\":33810,\"start\":33799},{\"end\":34262,\"start\":34253},{\"end\":34270,\"start\":34262},{\"end\":34287,\"start\":34270},{\"end\":34293,\"start\":34287},{\"end\":34304,\"start\":34293},{\"end\":34312,\"start\":34304},{\"end\":34788,\"start\":34776},{\"end\":34800,\"start\":34788},{\"end\":34813,\"start\":34800},{\"end\":34824,\"start\":34813},{\"end\":35049,\"start\":35037},{\"end\":35060,\"start\":35049},{\"end\":35073,\"start\":35060},{\"end\":35327,\"start\":35320},{\"end\":35337,\"start\":35327},{\"end\":35347,\"start\":35337},{\"end\":35683,\"start\":35672},{\"end\":35693,\"start\":35683},{\"end\":35973,\"start\":35960},{\"end\":35984,\"start\":35973},{\"end\":36303,\"start\":36297},{\"end\":36312,\"start\":36303},{\"end\":36319,\"start\":36312},{\"end\":36326,\"start\":36319},{\"end\":36699,\"start\":36686},{\"end\":36711,\"start\":36699},{\"end\":36724,\"start\":36711},{\"end\":37015,\"start\":37007},{\"end\":37026,\"start\":37015},{\"end\":37038,\"start\":37026},{\"end\":37054,\"start\":37038},{\"end\":37418,\"start\":37406},{\"end\":37672,\"start\":37661},{\"end\":37684,\"start\":37672},{\"end\":37696,\"start\":37684},{\"end\":37708,\"start\":37696},{\"end\":37719,\"start\":37708},{\"end\":38090,\"start\":38081},{\"end\":38100,\"start\":38090},{\"end\":38108,\"start\":38100},{\"end\":38116,\"start\":38108},{\"end\":38125,\"start\":38116},{\"end\":38133,\"start\":38125},{\"end\":38145,\"start\":38133},{\"end\":38149,\"start\":38145},{\"end\":38400,\"start\":38388},{\"end\":38406,\"start\":38400}]", "bib_venue": "[{\"end\":28667,\"start\":28605},{\"end\":29649,\"start\":29645},{\"end\":32893,\"start\":32875},{\"end\":33931,\"start\":33879},{\"end\":34453,\"start\":34391},{\"end\":36467,\"start\":36405},{\"end\":27098,\"start\":27094},{\"end\":27323,\"start\":27253},{\"end\":27643,\"start\":27639},{\"end\":27960,\"start\":27861},{\"end\":28230,\"start\":28176},{\"end\":28603,\"start\":28526},{\"end\":28987,\"start\":28944},{\"end\":29360,\"start\":29331},{\"end\":29643,\"start\":29605},{\"end\":29947,\"start\":29885},{\"end\":30267,\"start\":30230},{\"end\":30614,\"start\":30580},{\"end\":30760,\"start\":30696},{\"end\":31058,\"start\":31014},{\"end\":31477,\"start\":31448},{\"end\":31809,\"start\":31748},{\"end\":32137,\"start\":32100},{\"end\":32304,\"start\":32268},{\"end\":32542,\"start\":32471},{\"end\":32873,\"start\":32835},{\"end\":33185,\"start\":33145},{\"end\":33478,\"start\":33417},{\"end\":33877,\"start\":33810},{\"end\":34389,\"start\":34312},{\"end\":34872,\"start\":34824},{\"end\":35151,\"start\":35073},{\"end\":35384,\"start\":35347},{\"end\":35551,\"start\":35515},{\"end\":35739,\"start\":35693},{\"end\":36016,\"start\":35984},{\"end\":36403,\"start\":36326},{\"end\":36773,\"start\":36724},{\"end\":37091,\"start\":37054},{\"end\":37404,\"start\":37293},{\"end\":37788,\"start\":37719},{\"end\":38153,\"start\":38149},{\"end\":38386,\"start\":38344}]"}}}, "year": 2023, "month": 12, "day": 17}