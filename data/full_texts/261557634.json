{"id": 261557634, "updated": "2023-12-14 16:26:57.646", "metadata": {"title": "Norm Tweaking: High-performance Low-bit Quantization of Large Language Models", "authors": "[{\"first\":\"Liang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Qingyuan\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xiangxiang\",\"last\":\"Chu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "As the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment. While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower-bit quantization often result in severe performance degradation. In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient. Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs. To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization. We conduct extensive experiments on various datasets using several open-sourced LLMs. Our method demonstrates significant improvements in both weight-only quantization and joint quantization of weights and activations, surpassing existing PTQ methods. On GLM-130B and OPT-66B, our method even achieves the same level of accuracy at 2-bit quantization as their float ones. Our simple and effective approach makes it more practical for real-world applications.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2309-02784", "doi": "10.48550/arxiv.2309.02784"}}, "content": {"source": {"pdf_hash": "e2bd297422fb6575e7c78bef4666dbdc96f7c4fb", "pdf_src": "ArXiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2309.02784v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2309.02784", "status": "CLOSED"}}, "grobid": {"id": "6241473827d14ef3221c21528384c296f7fb2e88", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/e2bd297422fb6575e7c78bef4666dbdc96f7c4fb.txt", "contents": "\nNorm Tweaking: High-performance Low-bit Quantization of Large Language Models\n13 Dec 2023\n\nLiang Li \nQingyuan Li \nXiangxiangBo Zhang \nChu Meituan \nNorm Tweaking: High-performance Low-bit Quantization of Large Language Models\n13 Dec 2023D70C11D55AB41176D41DA187605B5EE3arXiv:2309.02784v2[cs.LG]\nAs the size of large language models (LLMs) continues to grow, model compression without sacrificing accuracy has become a crucial challenge for deployment.While some quantization methods, such as GPTQ, have made progress in achieving acceptable 4-bit weight-only quantization, attempts at lower bit quantization often result in severe performance degradation.In this paper, we introduce a technique called norm tweaking, which can be used as a plugin in current PTQ methods to achieve high precision while being cost-efficient.Our approach is inspired by the observation that rectifying the quantized activation distribution to match its float counterpart can readily restore accuracy for LLMs.To achieve this, we carefully design a tweaking strategy that includes calibration data generation and channel-wise distance constraint to update the weights of normalization layers for better generalization.We conduct extensive experiments on various datasets using several open-sourced LLMs.Our method demonstrates significant improvements in both weight-only quantization and joint quantization of weights and activations, surpassing existing PTQ methods.On GLM-130B and OPT-66B, our method even achieves the same level of accuracy at 2bit quantization as their float ones.Our simple and effective approach makes it more practical for real-world applications.\n\nIntroduction\n\nRecently, OpenAI's ChatGPT (OpenAI 2023b) has demonstrated outstanding performance on text generation, sparking a research frenzy in large language models (LLMs).Some of the most famous LLMs include GPT series like GPT-3 (Brown et al. 2020), GPT-4 (OpenAI 2023a), and PaLM (Chowdhery et al. 2022), Ernie (Zhang et al. 2019).Open-sourced ones like GLM (Du et al. 2021), BLOOM (Laurenc \u00b8on et al. 2022), OPT (Zhang et al. 2022) and LLaMa series (Touvron et al. 2023) have remarkably accelerated the development of the community.In essence, LLMs are generative models are trained on excessively large amounts of text data that mimics how humans use language, and they exhibit superior zero-shot performance in a large range of natural language processing (NLP) tasks, including language translation, sentiment analysis, text classification, and question answering, etc.They are increasingly be- ing used in applications such as chatbots, language understanding, and speech recognition systems.Nevertheless, due to the large scale (normally tens of billions or even trillions of parameters) of large language models, it causes large resource consumption even for deployment.Taking GPT-3 as an example, it has 175 billion parameters and uses FP16 for inference, occupying approximately 350 GB of GPU memory, which means at least 8 NVIDIA A100 GPUs are needed to support the deployment of a single model.Therefore, it is more than a necessity to reduce the cost.\n\nModel quantization, as a classic method of model compression, can effectively reduce the memory consumption of LLMs.For example, when using 4-bit quantization, GPT-3 can be deployed on 2 A100 GPUs due to one-fourth of memory reduction.GPTQ (Frantar et al. 2022) is currently the most prominent low-bit weight-only quantization method, which can compress some LLMs to 4-bit while maintaining acceptable precision degradation.Smoothquant (Xiao et al. 2023) could achieve 8-bit quantization for both weights and activations, by equivalently transferring the multiplication factors in weights and activations.However, these methods suffer from significant accuracy loss when applied to lower-bit quantization, such as 2-bit weight-only quantization using GPTQ or W4A8(4-bit for weights and 8-bit for activation) quantization using SmoothQuant.According to ZeroQuant-V2 (Yao et al. 2023), LLaMa-65B with GPTQ 2-bit quantization, the accuracy on the LAMBADA dataset (Paperno et al. 2016) decreased from 79% to 57%, for which reason it proposes a quantization-aware training method based on low-rank compensation.However, it not only requires additional training costs but also introduces additional parameters, which is not a viable choice for efficient deployment.\n\nTo improve the lower-bit performance of quantized models, we first draw an intuition that LLMs have sufficient noise resilience, such that it calls a tender solution for precision recovery.It is demonstrated in Prompt Quantization (Xu et al. 2023) that for a compressed LLM, providing an appropriate prompt can yield high-precision generation without updating parameters.ZeroQuantV2 (Yao et al. 2023) indicates that the larger parameter a model has, the less degradation will the quantization have.Next, we explore why LLMs behave poorly on lower-bit quantization from a numerical perspective.We observe that the distribution of the quantized model's output tensor deviates significantly from that of the original float model, and it accumulates layer by layer to become intractable, see Figure 1.Therefore a question is raised: could we improve the performance of the quantized model by simply matching its activation distribution to that of the float model?\n\nTo achieve this goal, we propose a method called Norm-Tweaking to enhance the quantized model by slightly adjusting the parameters of the LayerNorm layer to tweak the quantized distribution.This method can be widely applied to a variety of quantization methods, achieving significant accuracy improvement with only minimal additional computational cost.Our method is evaluated on various models and datasets, and the results indicate that Norm-Tweaking consistently improves the performance of GPTQ and SmoothQuant on different large language models.For LLaMa models, Norm-Tweaking demonstrates a general performance enhancement over GPTQ on diverse datasets, with a notable accuracy improvement of approximately 10% on the LAMBADA dataset.Moreover, during subjective evaluations of quantized models, we observe that Norm-Tweaking excels in preserving the general semantic ability of extremely low-bit quantized models.In a nutshell, our contribution is three-fold, 1.Firstly, we discover that large language models in general are robust against weight distortion, merely slight partial weight adjustment could recover its accuracy even in extreme low-bit regime.(Frantar et al. 2022) compresses and stores weight parameters, and decompresses them to FP16 for inference during calculation.This approach can effectively reduce the proportion of memory access time during inference while maintaining model accuracy.LLM.int8() (Dettmers et al. 2022) proposes to use float calculation or to adjust the multiplication operations of LayerNorm to reduce quantization loss.Smoothquant (Xiao et al. 2023) proposes a method to reduce the activation ranges by equivalently transferring the multiplication factors in weights and activations.GPTQ (Frantar et al. 2022) reconstruct weights based on the method in OBS (Hassibi, Stork, and Wolff 1993) via Hessian matrix to reduce quantization error.GPTQ has been widely applied in many scenarios where some LLMs could achieve high precision at 4-bit quantization.RPTQ (Yuan et al. 2023) and AWQ (Lin et al. 2023) further improve this method.\n\nQuantization-aware Training.Another method to improve the performance of the quantized models is quantization-aware training (QAT), which is to fine-tune the quantized models to match the original float models.QAT is widely studied in convolutional networks, but it encounters significant setbacks in large language model quantization.As the training process of LLMs consumes a huge amount of text data (usually in the order of trillions of tokens), how to efficiently fine-tune the quantized LLMs while maintaining their general knowledge and generalization ability remains an open question.To name a few attempts, LLM-QAT (Liu et al. 2023) requires the update the whole parameters of the LLMs on a set of at least 100k sampled data.Zero-QuantV2 (Yao et al. 2023) introduces a Low Rank Com-pensation to achieve parameter-efficient fine-tuning, but this approach neither eliminates the need for a large amount of training data nor avoids the introduction of additional parameters.\n\n\nMethod Motivation\n\nBased on the observation shown in Figure 1, the difference between the output tensors of each layer in the quantized model and its floating counterpart accumulates, while the output of the quantized model gradually deviates from the quantization-friendly zero-mean distribution.This is somewhat expected since LayerNorm magnifies the outlier (Xiao et al. 2023) and no measure is taken to deal with this effect.Hence when we iteratively update the quantized weights of each layer using GPTQ, it inevitably disrupts the zero-mean distribution of the current layer and increases the deviation.\n\nTo this end, we aim to improve the quantized model's performance by adjusting its output distribution to approach that of its float counterpart.Complete fine-tuning of the quantized model through QAT is a direct approach, but the large number of parameters in the LLM model and the huge amount of required training data make QAT unacceptable.In order to achieve high performance the quantized model within the time constraint, we are driven to improve current PTQ methods.As LayerNorm is very handy to manipulate distribution, we choose to adjust this layer to achieve the goal.It is also economical to update its weight considering the small number of parameters.Furthermore, nearly all mainstream LLMs use LayerNorm or similar operators, so that the method can be applied universally to a variety of large language models.Therefore, our core objective can be summarized as adjusting the parameters of LayerNorm to make the output distribution of the quantized model approach that of the float model, which can be expressed formally as, arg min\nW ln L dist (T (X), T (X))(1)\nwhere T (X|W attn , W mlp , W ln ) denotes a Transformer block, including the Attention module, MLP module, Layer-Norm layer, and activation functions, and T (X) represents its quantized version.L dist (\u2022) denotes the distribution loss function between the quantized and float models.Our goal is then to design a strategy to optimize \u0174ln to minimize L dist (\u2022), while keeping \u0174attn and \u0174mlp frozen.\n\n\nNorm Tweaking\n\nMotivated by the above analysis, we propose a PTQ method for LLMs, called Norm-Tweaking, to quickly restore models' performance by slightly tweaking LayerNorm layers of the quantized model.Norm tweaking serves as a plugin to be easily embedded into other quantization methods.Here, we take GPTQ as an example and present a weight-only postquantization algorithm pipeline, as shown in Algorithm 1. Firstly, we use the LLM model to generate a set of text data as for calibration (explained in detail in the section on Calibration Dataset Generation), instead of directly sampling from real datasets.Next, we iteratively process each transformer layer, quantizing and updating the weights of the Linear layers, just like GPTQ.Finally, we compute a channelwise loss based on the difference between the distribution of quantized output and float output.We then use stochastic gradient descent to update the parameters of LayerNorm in this layer, forcing the activation distribution of the quantized model to mimic that of the float model.During this process, the rest parameters of the current layer such as Linear are frozen and do not participate in the weight update.\n\nAlthough only the parameters of LayerNorm are updated, our process is distinct from parameter-efficient finetuning strategies.It should be noted that the parameters of the LayerNorm layer are very sensitive and excessive tuning can seriously damage the quantized models' performance (see Table 6).We slightly update the LayerNorm with a relaxed constraint, whose goal is to make the quantized models' distribution approaching that of float ones.This is the very reason why we definite our method as a tweaking, instead of finetuning.\n\nAt a glimpse, we carefully design the entire tweaking procedure to achieve our goal.For example, we use a very small number of iterations during tuning, typically only one iteration on the calibration text is required.We also adopt a small learning rate and design a step scheduler to assign different learning rates for the subsequent layers.In addition, our calibration data generation and the design of the distribution loss function harmoniously resonate with our tweaking principle.Freeze all Linear's weights in layer l 11:\n\nfor each it for total Iters do 12:\n\nCalculate the float output qOut l 13:\n\nCalculate L dist between f Out l and qOut l 14:\n\nBackward and update LayerNorms' parameters 15:\n\nend for 16: end for 17: Get the high-performance quantized LLMs\n\n\nCalibration Data Generation\n\nA crucial problem that matters in the generalization ability of the quantized model is the appropriate choice of calibra-tion data.We found that different calibration datasets substantially affect the performance of the quantized model.It usually performs well on the calibration dataset, but it generally suppresses the performance on other datasets.LLM-QAT (Liu et al. 2023) demonstrated that training the quantized model with a specific dataset further damages LLMs' generalization ability.Therefore, we adopt a data generation scheme following LLM-QAT that utilizes the generated data of the model itself for calibration instead of a specific real dataset.The benefit is that thus-generated data can effectively activate the neurons of the LLM which facilitates model quantization.It also enjoys rich semantic information stored in the model and it is less biased towards a specific dataset which is more generalizable.\n\nOur generation process is a variant of that of LLM-QAT.Firstly, a random token is taken from a list of given languages and then a two-stage pattern proposed by LLM-QAT is employed where the picked token is fed as the input prompt to let LLMs generate subsequent tokens.We enhance this data generation process by enforcing a restriction on the first random token.We observe a significant disparity in terms of proportions between the language categories in the training corpus and tokenization vocabulary.As shown in Table 1, taking BLOOM as an example, it is trained on a total of 1.61 TB of text, with the top five language types accounting for over 75% of the corpus.If we consider the related corpus (e.g.zht as a traditional version of zhs) and derivative ones (e.g.programming languages) of these five language types, the proportion exceeds 90%.In contrast, there are 250680 tokens in the tokenization vocabulary, whose total number of tokens corresponding to these five languages only accounts for 17%.Therefore, the first token of input directly affects the language type of the generated text.If we randomly select from the entire vocabulary, we cannot get appropriate calibration data that matches the training corpus.To this very purpose, we restrict the first random token to be selected only from the language categories in the list of top languages that have the highest proportion, which turns out to effectively improve the generalization of the quantized model on different datasets (\n\n\nChannel-wise Distribution Loss\n\nTo guide the direction of parameter updates, it is crucial to design a corresponding loss function.In this context, we aim to minimize the difference between the activation distribution of the quantized model and its original float model.Firstly, as the activation distribution of LLMs exhibits significant differences along the channel dimension, with some channels displaying extreme values (referred to as outliers) (Xiao et al. 2023), it poses great challenges for the quantization process.In order to preserve the differences between channels while tweaking the model parameters and to retain the original model capacity as much as possible, we enforce a channel-wise constraint.Secondly, a strict alignment of the point-wise activation values between quantized and float models may result in overfitting to calibration data, thereby compromising the generalization performance across different datasets.Therefore, we adopt a more relaxed alignment strategy by directly aligning the mean and variance between each channel, instead of strictly aligning the targets at the point-wise level.As a result, we introduce a channel-wise distribution loss function, as shown below:\nL dist = 1 C C c=1 ( \u00b5 c f \u2212 \u00b5 c q 2 + (\u03c3 c f ) 2 \u2212 (\u03c3 c q ) 2 2 ) (2)\nwhere C is the number of channels, \u00b5 and \u03c3 represent the mean and variance of each channel in tensor T , the subscript f and q indicates the float and quantized model.Furthermore, current algorithms like GPTQ iteratively quantize LLMs layer by layer, whose deviation of intermediate activation distributions gradually accumulates, resulting in large errors in the final layers.Thus, we apply a layer-level scheduler to adjust the learning rate of each layer during the tweaking process where we simply adopt a step increase to allocate different learning rates on different layers.\nlr i = lr 0 * (1 + scale * (i/L))(3)\n\nExperiments Settings\n\nWe tested our method on LLMs of different sizes and types, including GLM (Du et al. 2021) In our experiments, we typically use a grid search to obtain the optimal learning rate, with an initial value set at 1e-5.Our primary experimental evaluations are performed on the LAMBADA dataset (Paperno et al. 2016), which is renowned for its high demand for the understanding ability of natural language.This dataset necessitates a comprehensive understanding of the entire text to provide precise answers.To further substantiate the generalization of our method on different datasets, we employed Benchmark Harness (Gao et al. 2021) to conduct tests on a broader spectrum of datasets, encompassing HellaSwag (Zellers et al. 2019), PIQA (Bisk et al. 2020), WinoGrande (Sakaguchi et al. 2021), OpenBookQA (Mihaylov et al. 2018) (Marcus et al. 1994), C4 (Raffel et al. 2020) in Table 5, to provide some demonstrations of text generated by quantized LLMs, which helps to more intuitively visualize the performance recovery of Norm-Tweaking.Following the settings in GPTQ, we used a calibration dataset size with n samples=128, with the maximum sequence length token length=2048.\n\n\nTweaking Cost\n\nWe demonstrate that Norm-Tweaking incurs extremely low costs.Taking BLOOM (Laurenc \u00b8on et al. 2022) as an example, given the hidden dimension as h, each transformer block generally has 4 Linear layers, with a total parameter count of about 12h 2 + 9h, while LayerNorm has two layers, with a parameter count of 4h.The hidden dimension h is typically very large (for example, 14336 for BLOOM-176B), so the parameter quantity of the Linear layer is much larger than that of the LayerNorm layer (on the order of 10 7 \u223c 10 9 ).In addition, to avoid overfitting on specific calibration data, we only perform one iteration on each sample of text.Therefore, the proposed Norm- Table 3 shows the time cost taken to quantize LLMs using GPTQ and Norm-Tweaking.All experiments were conducted on a single NVIDIA A100 GPU.The additional time cost of Norm-Tweaking is less than the time cost of GPTQ itself, and our method still remains within the category of post-quantization.For BLOOM-7B, the time cost increase accounts for only 16%.\n\n\nResults on LAMBADA\n\nAs shown in Table 2, our proposed model quantization method is applied to LLMs at different scales, including BLOOM, LLaMa, GLM, and OPT, where the accuracy of each quantized model is evaluated on the LAMBADA dataset and is compared comprehensively with GPTQ.In addition, we also conduct experiments on 2-bit weight-only quantization with a fine-grained quantization with a group of 64.Our Norm-Tweaking post-quantization method generally outperforms the GPTQ algorithm in terms of model accuracy.In 2-bit quantization, the GPTQ algorithm caused significant accuracy loss for most models, making the results almost unusable.However, our proposed quantization method is able to achieve accuracy performance close to the floating-point model even on the GLM-130B and OPT-66B models, and it outperforms GPTQ by nearly 10% on LLaMa.\n\n\nComparison with RTN and SmoothQuant\n\nWe integrate Norm-Tweaking into two commonly used postquantization methods, round-to-nearest (RTN) (Yao et al. 2022;Dettmers et al. 2022) and SmoothQuant (Xiao et al. 2023), to verify its general effectiveness across different algorithms.Several LLMs are quantized in different modes and evaluated on the LAMBADA dataset, results are shown in Table 4. Specifically, we apply 4-bit weight-only quantization to RTN, and W4A8 (4-bit for weight and 8-bit for activation) quantization to the SmoothQuant.Note OPT-13b is severely compromised when using SmoothQuant W4A8 quantization, resulting in an accuracy of 0. The results demonstrate the universality of Norm-Tweaking, as it provides stable performance improvements for different quantization methods, including RTN, GPTQ, and SmoothQuant, as well as for different quantization modes, including weight-only and both weight and activation.More results are reported in appendix.\n\n\nBenchmark Harness\n\nWe benchmark the 2-bit quantized LLMs on the few-shot evaluation framework LM Evaluation Harness (Gao et al. 2021)  Beijing is an interesting city, with the Forbidden City in the Forbidden City, which is a world heritage site.Norm-Tweaking (2-bit) Beijing is the capital of China.The country has a population of around 1.3 billion Chinese people.The country is one of the leading exporters in the world, and also one of the leading importers of the world.China is one of the leading manufacturers of the world.China is a large country, and is one of the largest countries in the world.and strong generalizability to a wide range of datasets.We discuss the performance variations among datasets in the appendix.\n\n\nSubjective evaluation\n\nSubjective evaluation of the generated results is a common and effective method for evaluating the performance of language models such as LLM.In Table 5, the FP16 mode of LLaMa-65B and BLOOM-176B, as well as quantized model with GPTQ and Norm-Tweaking are evaluated through the lens of human evaluation on generated results.With the same input prompt, it can be seen that different models give significantly different results, especially the GPTQ lowbit quantization model, which is subject to obvious errors.These errors mainly manifest either grammatical errors (e.g.misspelled words or incorrect use of punctuation or spaces), logical errors in the language (e.g.repeated statements), and factual errors (e.g.birth date).Nevertheless, adopting the quantization method proposed in this paper, the quantized model obtained under the same settings does not have these obvious errors in the output results, suggesting the robustness of our quantization method.\n\n\nAblation Tweaking Iterations\n\nWe investigate the effect of the number of iterations for Norm-Tweaking and report the results of BLOOM-7B tested on LAMBADA dataset in Table 6.It turns out that increasing the iteration numbers during the tweaking process significantly damages the model's accuracy performance.This is as expected since the parameters of LayerNorm are highly sensitive, where excessive iterations can easily lead to the collapse of model performance.This is also why we recommend tweaking instead of tuning, which also clearly distinguishes us from those QAT methods such as LLM-QAT.\n\n\nCalibration Data\n\nTable 8 shows how the choice of calibration dataset significantly affects the performance of quantized models on different datasets.We use three real datasets WikiText2 (Merity et al. 2016), PTB (Marcus et al. 1994), and C4 (Raffel et al. 2020), as well as random data and generated data, as calibration sets to quantize the BLOOM-7B model using GPTQ.And we give the perplexity (PPL) on WikiText2, PTB, and C4 respectively, with lower PPL indicating better performance.The first three rows show the strong correlation between GPTQ and the calibration dataset, that is, a LLM calibrated on a certain dataset performs better on that dataset, but correspondingly worse on other datasets.\n\nTo avoid the dependence on real data, we randomly sample data from Gaussian distribution with the same mean and variance of the real data for calibration.However, the performance of the quantized model was extremely poor.We guess that this is because random data is without actual semantic meaning, which cannot produce positive activations for LLMs when being used as a calibration dataset.We exploit the LLM itself to generate calibration data.It can produce meaningful text and effectively activate the model.The results show that using generated data for calibration can improve the performance of the quantized model, and it does not show dependence on specific data.Using the language scope restriction proposed in this paper can further improve the quality of generated data.\n\n\nModel\n\n\nLoss Function\n\nTo showcase the importance of our proposed channel-wise distribution loss L Dist , we compare it with several different loss functions like mean square error L M SE and Kullback-Leibler Divergence loss L KD (Hinton, Vinyals, and Dean 2015), the result is shown in Table 9 where the proposed L Dist works best in all cases.This result echos our analysis that channel-wise treatment is necessary (better than L KL ) to deal with outliers while point-wise alignment (L M SE ) harms the performance.As a collaborative result of multiple components in Norm-Tweaking, the difference of quantized activation distribution to its float counterpart is largely narrowed, as shown in Figure 1.This observation fairly answers our original question that minimizing the activation distribution of LLMs between two precisions readily renders high performance, even for extremely low-bit quantization.\n\n\nConclusion\n\nIn conclusion, we have proposed a novel quantization compression method for large-scale language models (LLM) that surpasses existing state-of-the-art methods such as GPTQ and SmoothQuant.Our method is characterized by generating generalizable calibration data and tweaking the normalization layer with channel-wise distribution loss, enabling us to quickly achieve high-precision model quantization in a low-cost manner.Notably, we have explored LLM model compression at the 2-bit range, marking state-of-the-art performance.Our approach delivers a promising solution for reducing the computational and storage costs associated with LLMs while maintaining their high performance.\n\nFigure 1 :\n1\nFigure 1: Activation distribution of norm tweaking is closer to its float counterpart compared with GPTQ.A batch size of 128 is used to compute the mean difference \u2206 \u00b5 .\n\n\n\n\nAlgorithm 1: Norm-Tweaking Input: Pre-trained LLM model Output: Quantized LLM model 1: Generate calibration dataset (n samples = 128, token length = 2048) using pre-trained LLM model 2: for each layer-l in the Transformer structure (L layers total)\n\n\nTable 8\n8\n).\nLanguageenzhsfresptCorpus(MB) 485.0 261.0 208.2 175.1 79.3Vocab794338015483 6999 8669\n\nTable 1 :\n1\nText size and token count for the top 5 languages.\n\n\nTable 3 :\n3\nQuantization runtime measured in minutes for GPTQ and Norm-Tweaking on various LLMs.\nTweaking method has minimal re-source consumption and extra time.ModelBLOOM-7B LLaMA-7B OPT-13BGPTQ19.615.527GPTQ+NT22.827.346.6\n\nTable 4 :\n4\nNorm-Tweaking (NT) on various LLM PTQ methods.Note for OPT-13B, W5A8 is used for SmoothQuant and SmoothQuant+NT.\nMethodMode BLOOM-7B OPT-13Bw/o PTQFP1657.675169.0860RTNW4A1648.360262.7402RTN+NTW4A1651.562264.7584SmoothQuantW4A853.949268.6590SmoothQuant+NT W4A854.589669.7264\n\n\n\nin Table7.Our proposed method generally outperforms GPTQ 2-bit results, with some even better than FP16 accuracy.This again proves the robustness of our method capital of China.Beijing is also one of the world's oldest cities.It is also one of the most modern and beautiful cities of China.As of 2016, it had a population of over 20 million.Beijing is considered the most beautiful city in China.It is known for its many beautiful and ancient buildings and sites.These include the Forbidden City, the Temple of Heaven, and the Great Wall of China.GPTQ (4-bit)Beijing is the capital of China.There are 13.7 million people in the city of Beijing.The city is divided into 16 districts, and the most important districts are Dongcheng, Xicheng,Haidian, Chaoyang,Fengtai, Shijingshan and Daxing.The city is located in the north-east of China.The city is located in the north-east of China, close to the Pacific coast.The city is located on the left bank of the Beijing River.Norm-Tweaking (4-bit) Beijing is the capital of China, and the city is one of the most popular tourist destinations in the world.You can find here a lot of interesting places.You can visit the Forbidden City.It is located in the center of the city and is considered the most significant attraction.Also you can visit the Temple of This place is located a few kilometers from the center of the city.You can admire the beauty of the Beijing city from the observation deck of the World capital of China, and also one of the largest cities in the world.It is a modern city that has successfully managed to retain its ancient Chinese roots and the essence of its culture.A city of over 21 million people, Beijing is a major hub for international business, and as such attracts a large number of travelers.GPTQ (2-bit)Beijing is the capital of China, and has a rich history datin back to 5th in 1910s.Peking was the old capital in 1910s and renamed as Beijing in 1913, and became capital in 1972.\nModelBLOOM -176BFP16Beijing is the Trade Center.ModelLLaMa-65BFP16Beijing is the\n\nTable 5 :\n5\nExample of 4-bit quantized BLOOM-176B and 2-bit quantized LLaMa-65B text generation on the specified prompt \"Beijing is the capital of China\".The text in red is either grammatically wrong or counterfactual.\nIters125Acc 57.4811 55.7539 52.1056Iters102050Acc 46.8465 32.3307 11.3332\n\nTable 6 :\n6\nEffect of tweaking iterations.\n\n\nTable 7 :\n7\nThe quantized accuracy results of LLMs on the LM Evaluation Harness benchmark.\nCalibration Data WikiText2 PTBC4WikiText212.1621.17 18.28PTB12.5120.72 18.42C412.2820.97 18.16Random13.2522.82 19.60GenData V112.4321.25 18.34GenData V212.3220.95 18.28\n\nTable 8 :\n8\nEffects of different calibration datasets.V1 is the official data generation implementation of LLM-QAT, and V2 is our improved version.\n\n\nTable 9 :\n9\nComparison of different loss functions for normtweaking.\nL M SEL KLL DistBLOOM-7b 55.8704 56.2779 57.4811LLaMa-7b72.3850 71.7446 72.4820OPT-13b68.3291 68.2709 68.7173\nAcknowledgements: This work was supported by National Key R&D Program of China (No. 2022ZD0118700).Discussion about different datasetsAs depicted in Table7 and Table 11, not all tasks exhibit improved performance through Norm-Tweaking.We are intrigued by this phenomenon and endeavor to analyze potential general patterns.However, we observed that quantized models did not exhibit distinct characteristics across different datasets.Instead, different bit quantizations of the same model yielded similar results on the same dataset.We believe that this phenomenon is more likely associated with the pre-training models themselves rather than our method.Results on OmniQuantIn table below, we provide PPL on WikiText2 and C4 datasets when applying Norm-Tweaking on OmniQuant, which is considered to be the best PTQ for LLMs so far available.The results indicate that Norm-Tweaking further improves the performance of OmniQuant, especially at lower bit quantization.Additionally, its performance significantly surpasses that of AWQ.Methods\n. Winogrande Hellaswag, Rte Mrpc Qnli Boolq Cb Copa Openbookqa, Wic, LLaMa-7b (FP16\n\n. / Gptq, \n\nLLaMa-65b (FP16). \n\n. / Gptq, \n\nBLOOM-176b (FP16). \n\nOPT-66b (FP16). \n\n. / Gptq, \n\nR Y References Aminabadi, S Rajbhandari, M Zhang, A A Awan, C Li, D Li, E Zheng, J Rasley, S Smith, O Ruwase, Y He, arXiv:2207.00032DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. 2022\n\nPiqa: Reasoning about physical commonsense in natural language. Y Bisk, R Zellers, J Gao, Y Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034\n\nLanguage models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Conference on Neural Information Processing Systems (NeurIPS). 2020\n\nA Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint\n\nT Dao, D Y Fu, S Ermon, A Rudra, C R\u00e9, T Dettmers, M Lewis, Y Belkada, L Zettlemoyer, arXiv:2205.14135FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. 2022arXiv preprint\n\narXiv:2208.073398-bit Matrix Multiplication for Transformers at Scale. arXiv preprintint8(\n\nZ Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, arXiv:2103.10360Glm: General language model pretraining with autoregressive blank infilling. 2021arXiv preprint\n\nE Frantar, D Alistarh, arXiv:2301.00774SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot. 2023\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. E Frantar, S Ashkboos, T Hoefler, D Alistarh, arXiv:2210.173232022arXiv preprint\n\n. L Gao, J Tow, S Biderman, S Black, A Dipofi, C Foster, L Golding, J Hsu, K Mcdonell, N Muennighoff, J Phang, L Reynolds, E Tang, A Thite, B Wang, K Wang, Zou, A. 2021. A framework for few-shot language model evaluation\n\nOptimal brain surgeon and general network pruning. B Hassibi, D G Stork, G J Wolff, IEEE International Conference on Neural Networks. 1993\n\nDistilling the Knowledge in a Neural Network. G Hinton, O Vinyals, J Dean, D P Kingma, J Ba, Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR). 2015. 2015NIPS Deep Learning and Representation Learning Workshop\n\nH Laurenc \u00b8on, L Saulnier, T Wang, C Akiki, A V Del Moral, T Le Scao, L Von Werra, C Mou, E G Ponferrada, H Nguyen, The BigScience Corpus: A 1.6 TB Composite Multilingual Dataset. 2022\n\nLoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. Y Li, Y Yu, Q Zhang, C Liang, P He, W Chen, T Zhao, Proceedings of the 40th International Conference on Machine Learning. A Krause, E Brunskill, K Cho, B Engelhardt, S Sabato, J Scarlett, the 40th International Conference on Machine LearningPMLR2023202\n\nAWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. J Lin, J Tang, H Tang, S Yang, X Dang, S Han, arXiv:2306.009782023\n\nZ Liu, B Oguz, C Zhao, E Chang, P Stock, Y Mehdad, Y Shi, R Krishnamoorthi, V Chandra, arXiv:2305.17888arXiv:2305.11627LLM-QAT: Data-Free Quantization Aware Training for Large Language Models. X Ma, G Fang, X Wang, 2023arXiv preprintLLM-Pruner: On the Structural Pruning of Large Language Models\n\nThe Penn treebank: Annotating predicate argument structure. M Marcus, G Kim, M A Marcinkiewicz, R Macintyre, A Bies, M Ferguson, K Katz, B Schasberger, S Merity, C Xiong, J Bradbury, R Socher, arXiv:1609.07843Human Language Technology: Proceedings of a Workshop. Plainsboro, New Jersey1994. March 8-11, 1994. 2016arXiv preprintPointer sentinel mixture models\n\nCan a suit of armor conduct electricity? a new dataset for open book question answering. T Mihaylov, P Clark, T Khot, A Sabharwal, arXiv:1809.02789.NVIDIA.2023arXiv:2303.08774FasterTransformer. OpenAI. 2023a. GPT-4 Technical Report. 2018arXiv preprint\n\nIntroducing ChatGPT. 2023bOpenAI\n\nD Paperno, G Kruszewski, A Lazaridou, Q N Pham, R Bernardi, S Pezzelle, M Baroni, G Boleda, R Fern\u00e1ndez, arXiv:1606.06031The LAMBADA dataset: Word prediction requiring a broad discourse context. 2016arXiv preprint\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P Liu, Journal of Machine Learning Research. 211402020\n\n. K Sakaguchi, R L Bras, C Bhagavatula, Y Choi, \n\nAn adversarial winograd schema challenge at scale. Winogrande, Communications of the ACM. 649\n\nY Sheng, L Zheng, B Yuan, Z Li, M Ryabinin, D Y Fu, Z Xie, B Chen, C Barrett, J E Gonzalez, P Liang, C R\u00e9, I Stoica, C Zhang, arXiv:2303.06865FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU. 2023\n\nH Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozi\u00e8re, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Conference on Neural Information Processing Systems (NeurIPS). 2017\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. G Xiao, J Lin, M Seznec, H Wu, J Demouth, S Han, International Conference on Machine Learning. PMLR2023\n\nZ Xu, Z Liu, B Chen, Y Tang, J Wang, K Zhou, X Hu, A Shrivastava, arXiv:2305.11186Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt. 2023arXiv preprint\n\nZ Yao, R Y Aminabadi, M Zhang, X Wu, C Li, Y He, arXiv:2206.01861ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. 2022arXiv preprint\n\nZ Yao, X Wu, C Li, S Youn, Y He, arXiv:2303.08302ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation. 2023\n\nZ Yuan, L Niu, J Liu, W Liu, X Wang, Y Shang, G Sun, Q Wu, J Wu, B Wu, arXiv:2304.01089RPTQ: Reorderbased Post-training Quantization for Large Language Models. 2023\n\nHellaswag: Can a machine really finish your sentence?. R Zellers, A Holtzman, Y Bisk, A Farhadi, Y Choi, arXiv:1905.078302019arXiv preprint\n\nRoot Mean Square Layer Normalization. B Zhang, R Sennrich, arXiv:1910.074672019\n\nC Zhang, Y Yang, J Liu, J Wang, Y Xian, B Wang, D Song, arXiv:2305.12129Lifting the Curse of Capacity Gap in Distilling Language Models. 2023\n\nS Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.01068OPT: Open pre-trained transformer language models. 2022arXiv preprint\n\nZ Zhang, X Han, Z Liu, X Jiang, M Sun, Q Liu, arXiv:1905.07129ERNIE: Enhanced language representation with informative entities. Model2019arXiv preprintPrecision\n\n. Winogrande Hellaswag, Rte Mrpc Qnli Boolq Cb Copa Openbookqa, Wic, LLaMa-7b (FP16\n\n. / Gptq, 4\n\n. / Gptq, \n\nLLaMa-65b (FP16). \n\n. / Gptq, 4\n\n. / Gptq, \n\nTable 11. The quantized accuracy results of LLMs on the LM Evaluation Harness benchmark. \n", "annotations": {"author": "[{\"end\":101,\"start\":92},{\"end\":114,\"start\":102},{\"end\":134,\"start\":115},{\"end\":147,\"start\":135}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":98},{\"end\":113,\"start\":111},{\"end\":133,\"start\":128},{\"end\":146,\"start\":139}]", "author_first_name": "[{\"end\":97,\"start\":92},{\"end\":110,\"start\":102},{\"end\":127,\"start\":125},{\"end\":138,\"start\":135}]", "author_affiliation": null, "title": "[{\"end\":78,\"start\":1},{\"end\":225,\"start\":148}]", "venue": null, "abstract": "[{\"end\":1652,\"start\":295}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1908,\"start\":1889},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1964,\"start\":1941},{\"end\":1991,\"start\":1972},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2035,\"start\":2019},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2068,\"start\":2043},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2093,\"start\":2074},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2131,\"start\":2111},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3387,\"start\":3366},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3579,\"start\":3562},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4008,\"start\":3991},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4106,\"start\":4086},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4633,\"start\":4618},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4787,\"start\":4770},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6532,\"start\":6511},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6794,\"start\":6772},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6943,\"start\":6925},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7103,\"start\":7082},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7183,\"start\":7151},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7369,\"start\":7351},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7395,\"start\":7378},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8067,\"start\":8050},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8189,\"start\":8173},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8787,\"start\":8770},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13387,\"start\":13371},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15909,\"start\":15891},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17452,\"start\":17436},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17670,\"start\":17649},{\"end\":17989,\"start\":17972},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18086,\"start\":18065},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18111,\"start\":18093},{\"end\":18147,\"start\":18124},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18182,\"start\":18160},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18203,\"start\":18183},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18228,\"start\":18208},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20578,\"start\":20561},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20599,\"start\":20578},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20634,\"start\":20616},{\"end\":21523,\"start\":21506},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23940,\"start\":23920},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23969,\"start\":23949},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25458,\"start\":25426}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26983,\"start\":26799},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27236,\"start\":26984},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27336,\"start\":27237},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27401,\"start\":27337},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":27628,\"start\":27402},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":27916,\"start\":27629},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":29960,\"start\":27917},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":30254,\"start\":29961},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":30299,\"start\":30255},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":30560,\"start\":30300},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":30710,\"start\":30561},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":30890,\"start\":30711}]", "paragraph": "[{\"end\":3124,\"start\":1668},{\"end\":4385,\"start\":3126},{\"end\":5346,\"start\":4387},{\"end\":7424,\"start\":5348},{\"end\":8406,\"start\":7426},{\"end\":9018,\"start\":8428},{\"end\":10065,\"start\":9020},{\"end\":10494,\"start\":10096},{\"end\":11677,\"start\":10512},{\"end\":12212,\"start\":11679},{\"end\":12743,\"start\":12214},{\"end\":12779,\"start\":12745},{\"end\":12818,\"start\":12781},{\"end\":12867,\"start\":12820},{\"end\":12915,\"start\":12869},{\"end\":12980,\"start\":12917},{\"end\":13935,\"start\":13012},{\"end\":15437,\"start\":13937},{\"end\":16649,\"start\":15472},{\"end\":17302,\"start\":16721},{\"end\":18531,\"start\":17363},{\"end\":19571,\"start\":18549},{\"end\":20422,\"start\":19594},{\"end\":21387,\"start\":20462},{\"end\":22119,\"start\":21409},{\"end\":23104,\"start\":22145},{\"end\":23704,\"start\":23137},{\"end\":24409,\"start\":23725},{\"end\":25193,\"start\":24411},{\"end\":26103,\"start\":25219},{\"end\":26798,\"start\":26118},{\"end\":26982,\"start\":26813},{\"end\":27235,\"start\":26987},{\"end\":27250,\"start\":27248},{\"end\":27400,\"start\":27350},{\"end\":27499,\"start\":27415},{\"end\":27754,\"start\":27642},{\"end\":29879,\"start\":27920},{\"end\":30180,\"start\":29974},{\"end\":30298,\"start\":30268},{\"end\":30391,\"start\":30313},{\"end\":30709,\"start\":30574},{\"end\":30780,\"start\":30724}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10095,\"start\":10066},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16720,\"start\":16650},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17339,\"start\":17303}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":11974,\"start\":11973},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14460,\"start\":14459},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":18239,\"start\":18238},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19225,\"start\":19224},{\"end\":19613,\"start\":19612},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":20812,\"start\":20811},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":22297,\"start\":22296},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":23280,\"start\":23279},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23732,\"start\":23731},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":25490,\"start\":25489}]", "section_header": "[{\"end\":1666,\"start\":1654},{\"end\":8426,\"start\":8409},{\"end\":10510,\"start\":10497},{\"end\":13010,\"start\":12983},{\"end\":15470,\"start\":15440},{\"end\":17361,\"start\":17341},{\"end\":18547,\"start\":18534},{\"end\":19592,\"start\":19574},{\"end\":20460,\"start\":20425},{\"end\":21407,\"start\":21390},{\"end\":22143,\"start\":22122},{\"end\":23135,\"start\":23107},{\"end\":23723,\"start\":23707},{\"end\":25201,\"start\":25196},{\"end\":25217,\"start\":25204},{\"end\":26116,\"start\":26106},{\"end\":26810,\"start\":26800},{\"end\":27245,\"start\":27238},{\"end\":27347,\"start\":27338},{\"end\":27412,\"start\":27403},{\"end\":27639,\"start\":27630},{\"end\":29971,\"start\":29962},{\"end\":30265,\"start\":30256},{\"end\":30310,\"start\":30301},{\"end\":30571,\"start\":30562},{\"end\":30721,\"start\":30712}]", "table": "[{\"end\":27336,\"start\":27251},{\"end\":27628,\"start\":27500},{\"end\":27916,\"start\":27755},{\"end\":29960,\"start\":29880},{\"end\":30254,\"start\":30181},{\"end\":30560,\"start\":30392},{\"end\":30890,\"start\":30781}]", "figure_caption": "[{\"end\":26983,\"start\":26812},{\"end\":27236,\"start\":26986},{\"end\":27251,\"start\":27247},{\"end\":27401,\"start\":27349},{\"end\":27500,\"start\":27414},{\"end\":27755,\"start\":27641},{\"end\":29880,\"start\":27919},{\"end\":30181,\"start\":29973},{\"end\":30299,\"start\":30267},{\"end\":30392,\"start\":30312},{\"end\":30710,\"start\":30573},{\"end\":30781,\"start\":30723}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5183,\"start\":5182},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8470,\"start\":8469},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25899,\"start\":25898}]", "bib_author_first_name": "[{\"end\":31940,\"start\":31930},{\"end\":31979,\"start\":31952},{\"end\":32016,\"start\":32015},{\"end\":32048,\"start\":32047},{\"end\":32099,\"start\":32098},{\"end\":32109,\"start\":32108},{\"end\":32111,\"start\":32110},{\"end\":32135,\"start\":32134},{\"end\":32150,\"start\":32149},{\"end\":32159,\"start\":32158},{\"end\":32161,\"start\":32160},{\"end\":32169,\"start\":32168},{\"end\":32175,\"start\":32174},{\"end\":32181,\"start\":32180},{\"end\":32190,\"start\":32189},{\"end\":32200,\"start\":32199},{\"end\":32209,\"start\":32208},{\"end\":32219,\"start\":32218},{\"end\":32407,\"start\":32406},{\"end\":32415,\"start\":32414},{\"end\":32426,\"start\":32425},{\"end\":32433,\"start\":32432},{\"end\":32597,\"start\":32596},{\"end\":32606,\"start\":32605},{\"end\":32614,\"start\":32613},{\"end\":32623,\"start\":32622},{\"end\":32634,\"start\":32633},{\"end\":32636,\"start\":32635},{\"end\":32646,\"start\":32645},{\"end\":32658,\"start\":32657},{\"end\":32673,\"start\":32672},{\"end\":32682,\"start\":32681},{\"end\":32692,\"start\":32691},{\"end\":32771,\"start\":32770},{\"end\":32784,\"start\":32783},{\"end\":32794,\"start\":32793},{\"end\":32804,\"start\":32803},{\"end\":32813,\"start\":32812},{\"end\":32823,\"start\":32822},{\"end\":32834,\"start\":32833},{\"end\":32844,\"start\":32843},{\"end\":32846,\"start\":32845},{\"end\":32855,\"start\":32854},{\"end\":32865,\"start\":32864},{\"end\":32960,\"start\":32959},{\"end\":32967,\"start\":32966},{\"end\":32969,\"start\":32968},{\"end\":32975,\"start\":32974},{\"end\":32984,\"start\":32983},{\"end\":32993,\"start\":32992},{\"end\":32999,\"start\":32998},{\"end\":33011,\"start\":33010},{\"end\":33020,\"start\":33019},{\"end\":33031,\"start\":33030},{\"end\":33251,\"start\":33250},{\"end\":33257,\"start\":33256},{\"end\":33265,\"start\":33264},{\"end\":33272,\"start\":33271},{\"end\":33280,\"start\":33279},{\"end\":33287,\"start\":33286},{\"end\":33295,\"start\":33294},{\"end\":33416,\"start\":33415},{\"end\":33427,\"start\":33426},{\"end\":33617,\"start\":33616},{\"end\":33628,\"start\":33627},{\"end\":33640,\"start\":33639},{\"end\":33651,\"start\":33650},{\"end\":33701,\"start\":33700},{\"end\":33708,\"start\":33707},{\"end\":33715,\"start\":33714},{\"end\":33727,\"start\":33726},{\"end\":33736,\"start\":33735},{\"end\":33746,\"start\":33745},{\"end\":33756,\"start\":33755},{\"end\":33767,\"start\":33766},{\"end\":33774,\"start\":33773},{\"end\":33786,\"start\":33785},{\"end\":33801,\"start\":33800},{\"end\":33810,\"start\":33809},{\"end\":33822,\"start\":33821},{\"end\":33830,\"start\":33829},{\"end\":33839,\"start\":33838},{\"end\":33847,\"start\":33846},{\"end\":33972,\"start\":33971},{\"end\":33983,\"start\":33982},{\"end\":33985,\"start\":33984},{\"end\":33994,\"start\":33993},{\"end\":33996,\"start\":33995},{\"end\":34107,\"start\":34106},{\"end\":34117,\"start\":34116},{\"end\":34128,\"start\":34127},{\"end\":34136,\"start\":34135},{\"end\":34138,\"start\":34137},{\"end\":34148,\"start\":34147},{\"end\":34326,\"start\":34325},{\"end\":34341,\"start\":34340},{\"end\":34353,\"start\":34352},{\"end\":34361,\"start\":34360},{\"end\":34370,\"start\":34369},{\"end\":34372,\"start\":34371},{\"end\":34385,\"start\":34384},{\"end\":34396,\"start\":34395},{\"end\":34409,\"start\":34408},{\"end\":34416,\"start\":34415},{\"end\":34418,\"start\":34417},{\"end\":34432,\"start\":34431},{\"end\":34614,\"start\":34613},{\"end\":34620,\"start\":34619},{\"end\":34626,\"start\":34625},{\"end\":34635,\"start\":34634},{\"end\":34644,\"start\":34643},{\"end\":34650,\"start\":34649},{\"end\":34658,\"start\":34657},{\"end\":34736,\"start\":34735},{\"end\":34746,\"start\":34745},{\"end\":34759,\"start\":34758},{\"end\":34766,\"start\":34765},{\"end\":34780,\"start\":34779},{\"end\":34790,\"start\":34789},{\"end\":34948,\"start\":34947},{\"end\":34955,\"start\":34954},{\"end\":34963,\"start\":34962},{\"end\":34971,\"start\":34970},{\"end\":34979,\"start\":34978},{\"end\":34987,\"start\":34986},{\"end\":35016,\"start\":35015},{\"end\":35023,\"start\":35022},{\"end\":35031,\"start\":35030},{\"end\":35039,\"start\":35038},{\"end\":35048,\"start\":35047},{\"end\":35057,\"start\":35056},{\"end\":35067,\"start\":35066},{\"end\":35074,\"start\":35073},{\"end\":35092,\"start\":35091},{\"end\":35209,\"start\":35208},{\"end\":35215,\"start\":35214},{\"end\":35223,\"start\":35222},{\"end\":35373,\"start\":35372},{\"end\":35383,\"start\":35382},{\"end\":35390,\"start\":35389},{\"end\":35392,\"start\":35391},{\"end\":35409,\"start\":35408},{\"end\":35422,\"start\":35421},{\"end\":35430,\"start\":35429},{\"end\":35442,\"start\":35441},{\"end\":35450,\"start\":35449},{\"end\":35465,\"start\":35464},{\"end\":35475,\"start\":35474},{\"end\":35484,\"start\":35483},{\"end\":35496,\"start\":35495},{\"end\":35762,\"start\":35761},{\"end\":35774,\"start\":35773},{\"end\":35783,\"start\":35782},{\"end\":35791,\"start\":35790},{\"end\":35960,\"start\":35959},{\"end\":35971,\"start\":35970},{\"end\":35985,\"start\":35984},{\"end\":35998,\"start\":35997},{\"end\":36000,\"start\":35999},{\"end\":36008,\"start\":36007},{\"end\":36020,\"start\":36019},{\"end\":36032,\"start\":36031},{\"end\":36042,\"start\":36041},{\"end\":36052,\"start\":36051},{\"end\":36258,\"start\":36257},{\"end\":36268,\"start\":36267},{\"end\":36279,\"start\":36278},{\"end\":36290,\"start\":36289},{\"end\":36297,\"start\":36296},{\"end\":36307,\"start\":36306},{\"end\":36317,\"start\":36316},{\"end\":36325,\"start\":36324},{\"end\":36331,\"start\":36330},{\"end\":36389,\"start\":36388},{\"end\":36402,\"start\":36401},{\"end\":36404,\"start\":36403},{\"end\":36412,\"start\":36411},{\"end\":36427,\"start\":36426},{\"end\":36532,\"start\":36531},{\"end\":36541,\"start\":36540},{\"end\":36550,\"start\":36549},{\"end\":36558,\"start\":36557},{\"end\":36564,\"start\":36563},{\"end\":36576,\"start\":36575},{\"end\":36578,\"start\":36577},{\"end\":36584,\"start\":36583},{\"end\":36591,\"start\":36590},{\"end\":36599,\"start\":36598},{\"end\":36610,\"start\":36609},{\"end\":36612,\"start\":36611},{\"end\":36624,\"start\":36623},{\"end\":36633,\"start\":36632},{\"end\":36639,\"start\":36638},{\"end\":36649,\"start\":36648},{\"end\":36770,\"start\":36769},{\"end\":36781,\"start\":36780},{\"end\":36791,\"start\":36790},{\"end\":36802,\"start\":36801},{\"end\":36817,\"start\":36813},{\"end\":36828,\"start\":36827},{\"end\":36839,\"start\":36838},{\"end\":36850,\"start\":36849},{\"end\":36859,\"start\":36858},{\"end\":36869,\"start\":36868},{\"end\":36995,\"start\":36994},{\"end\":37006,\"start\":37005},{\"end\":37017,\"start\":37016},{\"end\":37027,\"start\":37026},{\"end\":37040,\"start\":37039},{\"end\":37049,\"start\":37048},{\"end\":37051,\"start\":37050},{\"end\":37060,\"start\":37059},{\"end\":37070,\"start\":37069},{\"end\":37243,\"start\":37242},{\"end\":37251,\"start\":37250},{\"end\":37258,\"start\":37257},{\"end\":37268,\"start\":37267},{\"end\":37274,\"start\":37273},{\"end\":37285,\"start\":37284},{\"end\":37348,\"start\":37347},{\"end\":37354,\"start\":37353},{\"end\":37361,\"start\":37360},{\"end\":37369,\"start\":37368},{\"end\":37377,\"start\":37376},{\"end\":37385,\"start\":37384},{\"end\":37393,\"start\":37392},{\"end\":37399,\"start\":37398},{\"end\":37546,\"start\":37545},{\"end\":37553,\"start\":37552},{\"end\":37555,\"start\":37554},{\"end\":37568,\"start\":37567},{\"end\":37577,\"start\":37576},{\"end\":37583,\"start\":37582},{\"end\":37589,\"start\":37588},{\"end\":37724,\"start\":37723},{\"end\":37731,\"start\":37730},{\"end\":37737,\"start\":37736},{\"end\":37743,\"start\":37742},{\"end\":37751,\"start\":37750},{\"end\":37889,\"start\":37888},{\"end\":37897,\"start\":37896},{\"end\":37904,\"start\":37903},{\"end\":37911,\"start\":37910},{\"end\":37918,\"start\":37917},{\"end\":37926,\"start\":37925},{\"end\":37935,\"start\":37934},{\"end\":37942,\"start\":37941},{\"end\":37948,\"start\":37947},{\"end\":37954,\"start\":37953},{\"end\":38110,\"start\":38109},{\"end\":38121,\"start\":38120},{\"end\":38133,\"start\":38132},{\"end\":38141,\"start\":38140},{\"end\":38152,\"start\":38151},{\"end\":38234,\"start\":38233},{\"end\":38243,\"start\":38242},{\"end\":38277,\"start\":38276},{\"end\":38286,\"start\":38285},{\"end\":38294,\"start\":38293},{\"end\":38301,\"start\":38300},{\"end\":38309,\"start\":38308},{\"end\":38317,\"start\":38316},{\"end\":38325,\"start\":38324},{\"end\":38420,\"start\":38419},{\"end\":38429,\"start\":38428},{\"end\":38439,\"start\":38438},{\"end\":38448,\"start\":38447},{\"end\":38459,\"start\":38458},{\"end\":38467,\"start\":38466},{\"end\":38475,\"start\":38474},{\"end\":38484,\"start\":38483},{\"end\":38492,\"start\":38491},{\"end\":38498,\"start\":38497},{\"end\":38500,\"start\":38499},{\"end\":38594,\"start\":38593},{\"end\":38603,\"start\":38602},{\"end\":38610,\"start\":38609},{\"end\":38617,\"start\":38616},{\"end\":38626,\"start\":38625},{\"end\":38633,\"start\":38632},{\"end\":38768,\"start\":38758},{\"end\":38807,\"start\":38780},{\"end\":38844,\"start\":38843},{\"end\":38857,\"start\":38856},{\"end\":38889,\"start\":38888},{\"end\":38902,\"start\":38901}]", "bib_author_last_name": "[{\"end\":31950,\"start\":31941},{\"end\":31990,\"start\":31980},{\"end\":31995,\"start\":31992},{\"end\":32021,\"start\":32017},{\"end\":32053,\"start\":32049},{\"end\":32104,\"start\":32100},{\"end\":32132,\"start\":32112},{\"end\":32147,\"start\":32136},{\"end\":32156,\"start\":32151},{\"end\":32166,\"start\":32162},{\"end\":32172,\"start\":32170},{\"end\":32178,\"start\":32176},{\"end\":32187,\"start\":32182},{\"end\":32197,\"start\":32191},{\"end\":32206,\"start\":32201},{\"end\":32216,\"start\":32210},{\"end\":32222,\"start\":32220},{\"end\":32412,\"start\":32408},{\"end\":32423,\"start\":32416},{\"end\":32430,\"start\":32427},{\"end\":32438,\"start\":32434},{\"end\":32603,\"start\":32598},{\"end\":32611,\"start\":32607},{\"end\":32620,\"start\":32615},{\"end\":32631,\"start\":32624},{\"end\":32643,\"start\":32637},{\"end\":32655,\"start\":32647},{\"end\":32670,\"start\":32659},{\"end\":32679,\"start\":32674},{\"end\":32689,\"start\":32683},{\"end\":32699,\"start\":32693},{\"end\":32781,\"start\":32772},{\"end\":32791,\"start\":32785},{\"end\":32801,\"start\":32795},{\"end\":32810,\"start\":32805},{\"end\":32820,\"start\":32814},{\"end\":32831,\"start\":32824},{\"end\":32841,\"start\":32835},{\"end\":32852,\"start\":32847},{\"end\":32862,\"start\":32856},{\"end\":32874,\"start\":32866},{\"end\":32964,\"start\":32961},{\"end\":32972,\"start\":32970},{\"end\":32981,\"start\":32976},{\"end\":32990,\"start\":32985},{\"end\":32996,\"start\":32994},{\"end\":33008,\"start\":33000},{\"end\":33017,\"start\":33012},{\"end\":33028,\"start\":33021},{\"end\":33043,\"start\":33032},{\"end\":33254,\"start\":33252},{\"end\":33262,\"start\":33258},{\"end\":33269,\"start\":33266},{\"end\":33277,\"start\":33273},{\"end\":33284,\"start\":33281},{\"end\":33292,\"start\":33288},{\"end\":33300,\"start\":33296},{\"end\":33424,\"start\":33417},{\"end\":33436,\"start\":33428},{\"end\":33625,\"start\":33618},{\"end\":33637,\"start\":33629},{\"end\":33648,\"start\":33641},{\"end\":33660,\"start\":33652},{\"end\":33705,\"start\":33702},{\"end\":33712,\"start\":33709},{\"end\":33724,\"start\":33716},{\"end\":33733,\"start\":33728},{\"end\":33743,\"start\":33737},{\"end\":33753,\"start\":33747},{\"end\":33764,\"start\":33757},{\"end\":33771,\"start\":33768},{\"end\":33783,\"start\":33775},{\"end\":33798,\"start\":33787},{\"end\":33807,\"start\":33802},{\"end\":33819,\"start\":33811},{\"end\":33827,\"start\":33823},{\"end\":33836,\"start\":33831},{\"end\":33844,\"start\":33840},{\"end\":33852,\"start\":33848},{\"end\":33857,\"start\":33854},{\"end\":33980,\"start\":33973},{\"end\":33991,\"start\":33986},{\"end\":34002,\"start\":33997},{\"end\":34114,\"start\":34108},{\"end\":34125,\"start\":34118},{\"end\":34133,\"start\":34129},{\"end\":34145,\"start\":34139},{\"end\":34151,\"start\":34149},{\"end\":34338,\"start\":34327},{\"end\":34350,\"start\":34342},{\"end\":34358,\"start\":34354},{\"end\":34367,\"start\":34362},{\"end\":34382,\"start\":34373},{\"end\":34393,\"start\":34386},{\"end\":34406,\"start\":34397},{\"end\":34413,\"start\":34410},{\"end\":34429,\"start\":34419},{\"end\":34439,\"start\":34433},{\"end\":34617,\"start\":34615},{\"end\":34623,\"start\":34621},{\"end\":34632,\"start\":34627},{\"end\":34641,\"start\":34636},{\"end\":34647,\"start\":34645},{\"end\":34655,\"start\":34651},{\"end\":34663,\"start\":34659},{\"end\":34743,\"start\":34737},{\"end\":34756,\"start\":34747},{\"end\":34763,\"start\":34760},{\"end\":34777,\"start\":34767},{\"end\":34787,\"start\":34781},{\"end\":34799,\"start\":34791},{\"end\":34952,\"start\":34949},{\"end\":34960,\"start\":34956},{\"end\":34968,\"start\":34964},{\"end\":34976,\"start\":34972},{\"end\":34984,\"start\":34980},{\"end\":34991,\"start\":34988},{\"end\":35020,\"start\":35017},{\"end\":35028,\"start\":35024},{\"end\":35036,\"start\":35032},{\"end\":35045,\"start\":35040},{\"end\":35054,\"start\":35049},{\"end\":35064,\"start\":35058},{\"end\":35071,\"start\":35068},{\"end\":35089,\"start\":35075},{\"end\":35100,\"start\":35093},{\"end\":35212,\"start\":35210},{\"end\":35220,\"start\":35216},{\"end\":35228,\"start\":35224},{\"end\":35380,\"start\":35374},{\"end\":35387,\"start\":35384},{\"end\":35406,\"start\":35393},{\"end\":35419,\"start\":35410},{\"end\":35427,\"start\":35423},{\"end\":35439,\"start\":35431},{\"end\":35447,\"start\":35443},{\"end\":35462,\"start\":35451},{\"end\":35472,\"start\":35466},{\"end\":35481,\"start\":35476},{\"end\":35493,\"start\":35485},{\"end\":35503,\"start\":35497},{\"end\":35771,\"start\":35763},{\"end\":35780,\"start\":35775},{\"end\":35788,\"start\":35784},{\"end\":35801,\"start\":35792},{\"end\":35968,\"start\":35961},{\"end\":35982,\"start\":35972},{\"end\":35995,\"start\":35986},{\"end\":36005,\"start\":36001},{\"end\":36017,\"start\":36009},{\"end\":36029,\"start\":36021},{\"end\":36039,\"start\":36033},{\"end\":36049,\"start\":36043},{\"end\":36062,\"start\":36053},{\"end\":36265,\"start\":36259},{\"end\":36276,\"start\":36269},{\"end\":36287,\"start\":36280},{\"end\":36294,\"start\":36291},{\"end\":36304,\"start\":36298},{\"end\":36314,\"start\":36308},{\"end\":36322,\"start\":36318},{\"end\":36328,\"start\":36326},{\"end\":36335,\"start\":36332},{\"end\":36399,\"start\":36390},{\"end\":36409,\"start\":36405},{\"end\":36424,\"start\":36413},{\"end\":36432,\"start\":36428},{\"end\":36497,\"start\":36487},{\"end\":36538,\"start\":36533},{\"end\":36547,\"start\":36542},{\"end\":36555,\"start\":36551},{\"end\":36561,\"start\":36559},{\"end\":36573,\"start\":36565},{\"end\":36581,\"start\":36579},{\"end\":36588,\"start\":36585},{\"end\":36596,\"start\":36592},{\"end\":36607,\"start\":36600},{\"end\":36621,\"start\":36613},{\"end\":36630,\"start\":36625},{\"end\":36636,\"start\":36634},{\"end\":36646,\"start\":36640},{\"end\":36655,\"start\":36650},{\"end\":36778,\"start\":36771},{\"end\":36788,\"start\":36782},{\"end\":36799,\"start\":36792},{\"end\":36811,\"start\":36803},{\"end\":36825,\"start\":36818},{\"end\":36836,\"start\":36829},{\"end\":36847,\"start\":36840},{\"end\":36856,\"start\":36851},{\"end\":36866,\"start\":36860},{\"end\":36875,\"start\":36870},{\"end\":37003,\"start\":36996},{\"end\":37014,\"start\":37007},{\"end\":37024,\"start\":37018},{\"end\":37037,\"start\":37028},{\"end\":37046,\"start\":37041},{\"end\":37057,\"start\":37052},{\"end\":37067,\"start\":37061},{\"end\":37081,\"start\":37071},{\"end\":37248,\"start\":37244},{\"end\":37255,\"start\":37252},{\"end\":37265,\"start\":37259},{\"end\":37271,\"start\":37269},{\"end\":37282,\"start\":37275},{\"end\":37289,\"start\":37286},{\"end\":37351,\"start\":37349},{\"end\":37358,\"start\":37355},{\"end\":37366,\"start\":37362},{\"end\":37374,\"start\":37370},{\"end\":37382,\"start\":37378},{\"end\":37390,\"start\":37386},{\"end\":37396,\"start\":37394},{\"end\":37411,\"start\":37400},{\"end\":37550,\"start\":37547},{\"end\":37565,\"start\":37556},{\"end\":37574,\"start\":37569},{\"end\":37580,\"start\":37578},{\"end\":37586,\"start\":37584},{\"end\":37592,\"start\":37590},{\"end\":37728,\"start\":37725},{\"end\":37734,\"start\":37732},{\"end\":37740,\"start\":37738},{\"end\":37748,\"start\":37744},{\"end\":37754,\"start\":37752},{\"end\":37894,\"start\":37890},{\"end\":37901,\"start\":37898},{\"end\":37908,\"start\":37905},{\"end\":37915,\"start\":37912},{\"end\":37923,\"start\":37919},{\"end\":37932,\"start\":37927},{\"end\":37939,\"start\":37936},{\"end\":37945,\"start\":37943},{\"end\":37951,\"start\":37949},{\"end\":37957,\"start\":37955},{\"end\":38118,\"start\":38111},{\"end\":38130,\"start\":38122},{\"end\":38138,\"start\":38134},{\"end\":38149,\"start\":38142},{\"end\":38157,\"start\":38153},{\"end\":38240,\"start\":38235},{\"end\":38252,\"start\":38244},{\"end\":38283,\"start\":38278},{\"end\":38291,\"start\":38287},{\"end\":38298,\"start\":38295},{\"end\":38306,\"start\":38302},{\"end\":38314,\"start\":38310},{\"end\":38322,\"start\":38318},{\"end\":38330,\"start\":38326},{\"end\":38426,\"start\":38421},{\"end\":38436,\"start\":38430},{\"end\":38445,\"start\":38440},{\"end\":38456,\"start\":38449},{\"end\":38464,\"start\":38460},{\"end\":38472,\"start\":38468},{\"end\":38481,\"start\":38476},{\"end\":38489,\"start\":38485},{\"end\":38495,\"start\":38493},{\"end\":38504,\"start\":38501},{\"end\":38600,\"start\":38595},{\"end\":38607,\"start\":38604},{\"end\":38614,\"start\":38611},{\"end\":38623,\"start\":38618},{\"end\":38630,\"start\":38627},{\"end\":38637,\"start\":38634},{\"end\":38778,\"start\":38769},{\"end\":38818,\"start\":38808},{\"end\":38823,\"start\":38820},{\"end\":38849,\"start\":38845},{\"end\":38862,\"start\":38858},{\"end\":38894,\"start\":38890},{\"end\":38907,\"start\":38903}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32011,\"start\":31928},{\"attributes\":{\"id\":\"b1\"},\"end\":32023,\"start\":32013},{\"attributes\":{\"id\":\"b2\"},\"end\":32043,\"start\":32025},{\"attributes\":{\"id\":\"b3\"},\"end\":32055,\"start\":32045},{\"attributes\":{\"id\":\"b4\"},\"end\":32076,\"start\":32057},{\"attributes\":{\"id\":\"b5\"},\"end\":32094,\"start\":32078},{\"attributes\":{\"id\":\"b6\"},\"end\":32106,\"start\":32096},{\"attributes\":{\"doi\":\"arXiv:2207.00032\",\"id\":\"b7\"},\"end\":32340,\"start\":32108},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":208290939},\"end\":32555,\"start\":32342},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":218971783},\"end\":32768,\"start\":32557},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b10\"},\"end\":32957,\"start\":32770},{\"attributes\":{\"doi\":\"arXiv:2205.14135\",\"id\":\"b11\"},\"end\":33156,\"start\":32959},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b12\"},\"end\":33248,\"start\":33158},{\"attributes\":{\"doi\":\"arXiv:2103.10360\",\"id\":\"b13\"},\"end\":33413,\"start\":33250},{\"attributes\":{\"doi\":\"arXiv:2301.00774\",\"id\":\"b14\"},\"end\":33531,\"start\":33415},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b15\"},\"end\":33696,\"start\":33533},{\"attributes\":{\"id\":\"b16\"},\"end\":33918,\"start\":33698},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":61815367},\"end\":34058,\"start\":33920},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7200347},\"end\":34323,\"start\":34060},{\"attributes\":{\"id\":\"b19\"},\"end\":34509,\"start\":34325},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":259203385},\"end\":34865,\"start\":34511},{\"attributes\":{\"doi\":\"arXiv:2306.00978\",\"id\":\"b21\"},\"end\":35013,\"start\":34867},{\"attributes\":{\"doi\":\"arXiv:2305.17888\",\"id\":\"b22\"},\"end\":35310,\"start\":35015},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5151364},\"end\":35670,\"start\":35312},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":52183757},\"end\":35923,\"start\":35672},{\"attributes\":{\"id\":\"b25\"},\"end\":35957,\"start\":35925},{\"attributes\":{\"id\":\"b26\"},\"end\":36172,\"start\":35959},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":204838007},\"end\":36384,\"start\":36174},{\"attributes\":{\"id\":\"b28\"},\"end\":36434,\"start\":36386},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":199370376},\"end\":36529,\"start\":36436},{\"attributes\":{\"id\":\"b30\"},\"end\":36767,\"start\":36531},{\"attributes\":{\"id\":\"b31\"},\"end\":36965,\"start\":36769},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13756489},\"end\":37150,\"start\":36967},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":253708271},\"end\":37345,\"start\":37152},{\"attributes\":{\"id\":\"b34\"},\"end\":37543,\"start\":37347},{\"attributes\":{\"id\":\"b35\"},\"end\":37721,\"start\":37545},{\"attributes\":{\"id\":\"b36\"},\"end\":37886,\"start\":37723},{\"attributes\":{\"id\":\"b37\"},\"end\":38052,\"start\":37888},{\"attributes\":{\"id\":\"b38\"},\"end\":38193,\"start\":38054},{\"attributes\":{\"id\":\"b39\"},\"end\":38274,\"start\":38195},{\"attributes\":{\"id\":\"b40\"},\"end\":38417,\"start\":38276},{\"attributes\":{\"id\":\"b41\"},\"end\":38591,\"start\":38419},{\"attributes\":{\"id\":\"b42\"},\"end\":38754,\"start\":38593},{\"attributes\":{\"id\":\"b43\"},\"end\":38839,\"start\":38756},{\"attributes\":{\"id\":\"b44\"},\"end\":38852,\"start\":38841},{\"attributes\":{\"id\":\"b45\"},\"end\":38864,\"start\":38854},{\"attributes\":{\"id\":\"b46\"},\"end\":38884,\"start\":38866},{\"attributes\":{\"id\":\"b47\"},\"end\":38897,\"start\":38886},{\"attributes\":{\"id\":\"b48\"},\"end\":38909,\"start\":38899},{\"attributes\":{\"id\":\"b49\"},\"end\":39000,\"start\":38911}]", "bib_title": "[{\"end\":32404,\"start\":32342},{\"end\":32594,\"start\":32557},{\"end\":33969,\"start\":33920},{\"end\":34104,\"start\":34060},{\"end\":34611,\"start\":34511},{\"end\":35370,\"start\":35312},{\"end\":35759,\"start\":35672},{\"end\":36255,\"start\":36174},{\"end\":36485,\"start\":36436},{\"end\":36992,\"start\":36967},{\"end\":37240,\"start\":37152}]", "bib_author": "[{\"end\":31952,\"start\":31930},{\"end\":31992,\"start\":31952},{\"end\":31997,\"start\":31992},{\"end\":32023,\"start\":32015},{\"end\":32055,\"start\":32047},{\"end\":32106,\"start\":32098},{\"end\":32134,\"start\":32108},{\"end\":32149,\"start\":32134},{\"end\":32158,\"start\":32149},{\"end\":32168,\"start\":32158},{\"end\":32174,\"start\":32168},{\"end\":32180,\"start\":32174},{\"end\":32189,\"start\":32180},{\"end\":32199,\"start\":32189},{\"end\":32208,\"start\":32199},{\"end\":32218,\"start\":32208},{\"end\":32224,\"start\":32218},{\"end\":32414,\"start\":32406},{\"end\":32425,\"start\":32414},{\"end\":32432,\"start\":32425},{\"end\":32440,\"start\":32432},{\"end\":32605,\"start\":32596},{\"end\":32613,\"start\":32605},{\"end\":32622,\"start\":32613},{\"end\":32633,\"start\":32622},{\"end\":32645,\"start\":32633},{\"end\":32657,\"start\":32645},{\"end\":32672,\"start\":32657},{\"end\":32681,\"start\":32672},{\"end\":32691,\"start\":32681},{\"end\":32701,\"start\":32691},{\"end\":32783,\"start\":32770},{\"end\":32793,\"start\":32783},{\"end\":32803,\"start\":32793},{\"end\":32812,\"start\":32803},{\"end\":32822,\"start\":32812},{\"end\":32833,\"start\":32822},{\"end\":32843,\"start\":32833},{\"end\":32854,\"start\":32843},{\"end\":32864,\"start\":32854},{\"end\":32876,\"start\":32864},{\"end\":32966,\"start\":32959},{\"end\":32974,\"start\":32966},{\"end\":32983,\"start\":32974},{\"end\":32992,\"start\":32983},{\"end\":32998,\"start\":32992},{\"end\":33010,\"start\":32998},{\"end\":33019,\"start\":33010},{\"end\":33030,\"start\":33019},{\"end\":33045,\"start\":33030},{\"end\":33256,\"start\":33250},{\"end\":33264,\"start\":33256},{\"end\":33271,\"start\":33264},{\"end\":33279,\"start\":33271},{\"end\":33286,\"start\":33279},{\"end\":33294,\"start\":33286},{\"end\":33302,\"start\":33294},{\"end\":33426,\"start\":33415},{\"end\":33438,\"start\":33426},{\"end\":33627,\"start\":33616},{\"end\":33639,\"start\":33627},{\"end\":33650,\"start\":33639},{\"end\":33662,\"start\":33650},{\"end\":33707,\"start\":33700},{\"end\":33714,\"start\":33707},{\"end\":33726,\"start\":33714},{\"end\":33735,\"start\":33726},{\"end\":33745,\"start\":33735},{\"end\":33755,\"start\":33745},{\"end\":33766,\"start\":33755},{\"end\":33773,\"start\":33766},{\"end\":33785,\"start\":33773},{\"end\":33800,\"start\":33785},{\"end\":33809,\"start\":33800},{\"end\":33821,\"start\":33809},{\"end\":33829,\"start\":33821},{\"end\":33838,\"start\":33829},{\"end\":33846,\"start\":33838},{\"end\":33854,\"start\":33846},{\"end\":33859,\"start\":33854},{\"end\":33982,\"start\":33971},{\"end\":33993,\"start\":33982},{\"end\":34004,\"start\":33993},{\"end\":34116,\"start\":34106},{\"end\":34127,\"start\":34116},{\"end\":34135,\"start\":34127},{\"end\":34147,\"start\":34135},{\"end\":34153,\"start\":34147},{\"end\":34340,\"start\":34325},{\"end\":34352,\"start\":34340},{\"end\":34360,\"start\":34352},{\"end\":34369,\"start\":34360},{\"end\":34384,\"start\":34369},{\"end\":34395,\"start\":34384},{\"end\":34408,\"start\":34395},{\"end\":34415,\"start\":34408},{\"end\":34431,\"start\":34415},{\"end\":34441,\"start\":34431},{\"end\":34619,\"start\":34613},{\"end\":34625,\"start\":34619},{\"end\":34634,\"start\":34625},{\"end\":34643,\"start\":34634},{\"end\":34649,\"start\":34643},{\"end\":34657,\"start\":34649},{\"end\":34665,\"start\":34657},{\"end\":34954,\"start\":34947},{\"end\":34962,\"start\":34954},{\"end\":34970,\"start\":34962},{\"end\":34978,\"start\":34970},{\"end\":34986,\"start\":34978},{\"end\":34993,\"start\":34986},{\"end\":35022,\"start\":35015},{\"end\":35030,\"start\":35022},{\"end\":35038,\"start\":35030},{\"end\":35047,\"start\":35038},{\"end\":35056,\"start\":35047},{\"end\":35066,\"start\":35056},{\"end\":35073,\"start\":35066},{\"end\":35091,\"start\":35073},{\"end\":35102,\"start\":35091},{\"end\":35382,\"start\":35372},{\"end\":35389,\"start\":35382},{\"end\":35408,\"start\":35389},{\"end\":35421,\"start\":35408},{\"end\":35429,\"start\":35421},{\"end\":35441,\"start\":35429},{\"end\":35449,\"start\":35441},{\"end\":35464,\"start\":35449},{\"end\":35474,\"start\":35464},{\"end\":35483,\"start\":35474},{\"end\":35495,\"start\":35483},{\"end\":35505,\"start\":35495},{\"end\":35773,\"start\":35761},{\"end\":35782,\"start\":35773},{\"end\":35790,\"start\":35782},{\"end\":35803,\"start\":35790},{\"end\":35970,\"start\":35959},{\"end\":35984,\"start\":35970},{\"end\":35997,\"start\":35984},{\"end\":36007,\"start\":35997},{\"end\":36019,\"start\":36007},{\"end\":36031,\"start\":36019},{\"end\":36041,\"start\":36031},{\"end\":36051,\"start\":36041},{\"end\":36064,\"start\":36051},{\"end\":36267,\"start\":36257},{\"end\":36278,\"start\":36267},{\"end\":36289,\"start\":36278},{\"end\":36296,\"start\":36289},{\"end\":36306,\"start\":36296},{\"end\":36316,\"start\":36306},{\"end\":36324,\"start\":36316},{\"end\":36330,\"start\":36324},{\"end\":36337,\"start\":36330},{\"end\":36401,\"start\":36388},{\"end\":36411,\"start\":36401},{\"end\":36426,\"start\":36411},{\"end\":36434,\"start\":36426},{\"end\":36499,\"start\":36487},{\"end\":36540,\"start\":36531},{\"end\":36549,\"start\":36540},{\"end\":36557,\"start\":36549},{\"end\":36563,\"start\":36557},{\"end\":36575,\"start\":36563},{\"end\":36583,\"start\":36575},{\"end\":36590,\"start\":36583},{\"end\":36598,\"start\":36590},{\"end\":36609,\"start\":36598},{\"end\":36623,\"start\":36609},{\"end\":36632,\"start\":36623},{\"end\":36638,\"start\":36632},{\"end\":36648,\"start\":36638},{\"end\":36657,\"start\":36648},{\"end\":36780,\"start\":36769},{\"end\":36790,\"start\":36780},{\"end\":36801,\"start\":36790},{\"end\":36813,\"start\":36801},{\"end\":36827,\"start\":36813},{\"end\":36838,\"start\":36827},{\"end\":36849,\"start\":36838},{\"end\":36858,\"start\":36849},{\"end\":36868,\"start\":36858},{\"end\":36877,\"start\":36868},{\"end\":37005,\"start\":36994},{\"end\":37016,\"start\":37005},{\"end\":37026,\"start\":37016},{\"end\":37039,\"start\":37026},{\"end\":37048,\"start\":37039},{\"end\":37059,\"start\":37048},{\"end\":37069,\"start\":37059},{\"end\":37083,\"start\":37069},{\"end\":37250,\"start\":37242},{\"end\":37257,\"start\":37250},{\"end\":37267,\"start\":37257},{\"end\":37273,\"start\":37267},{\"end\":37284,\"start\":37273},{\"end\":37291,\"start\":37284},{\"end\":37353,\"start\":37347},{\"end\":37360,\"start\":37353},{\"end\":37368,\"start\":37360},{\"end\":37376,\"start\":37368},{\"end\":37384,\"start\":37376},{\"end\":37392,\"start\":37384},{\"end\":37398,\"start\":37392},{\"end\":37413,\"start\":37398},{\"end\":37552,\"start\":37545},{\"end\":37567,\"start\":37552},{\"end\":37576,\"start\":37567},{\"end\":37582,\"start\":37576},{\"end\":37588,\"start\":37582},{\"end\":37594,\"start\":37588},{\"end\":37730,\"start\":37723},{\"end\":37736,\"start\":37730},{\"end\":37742,\"start\":37736},{\"end\":37750,\"start\":37742},{\"end\":37756,\"start\":37750},{\"end\":37896,\"start\":37888},{\"end\":37903,\"start\":37896},{\"end\":37910,\"start\":37903},{\"end\":37917,\"start\":37910},{\"end\":37925,\"start\":37917},{\"end\":37934,\"start\":37925},{\"end\":37941,\"start\":37934},{\"end\":37947,\"start\":37941},{\"end\":37953,\"start\":37947},{\"end\":37959,\"start\":37953},{\"end\":38120,\"start\":38109},{\"end\":38132,\"start\":38120},{\"end\":38140,\"start\":38132},{\"end\":38151,\"start\":38140},{\"end\":38159,\"start\":38151},{\"end\":38242,\"start\":38233},{\"end\":38254,\"start\":38242},{\"end\":38285,\"start\":38276},{\"end\":38293,\"start\":38285},{\"end\":38300,\"start\":38293},{\"end\":38308,\"start\":38300},{\"end\":38316,\"start\":38308},{\"end\":38324,\"start\":38316},{\"end\":38332,\"start\":38324},{\"end\":38428,\"start\":38419},{\"end\":38438,\"start\":38428},{\"end\":38447,\"start\":38438},{\"end\":38458,\"start\":38447},{\"end\":38466,\"start\":38458},{\"end\":38474,\"start\":38466},{\"end\":38483,\"start\":38474},{\"end\":38491,\"start\":38483},{\"end\":38497,\"start\":38491},{\"end\":38506,\"start\":38497},{\"end\":38602,\"start\":38593},{\"end\":38609,\"start\":38602},{\"end\":38616,\"start\":38609},{\"end\":38625,\"start\":38616},{\"end\":38632,\"start\":38625},{\"end\":38639,\"start\":38632},{\"end\":38780,\"start\":38758},{\"end\":38820,\"start\":38780},{\"end\":38825,\"start\":38820},{\"end\":38851,\"start\":38843},{\"end\":38864,\"start\":38856},{\"end\":38896,\"start\":38888},{\"end\":38909,\"start\":38901}]", "bib_venue": "[{\"end\":32041,\"start\":32025},{\"end\":32074,\"start\":32057},{\"end\":32092,\"start\":32078},{\"end\":32334,\"start\":32240},{\"end\":32501,\"start\":32440},{\"end\":32762,\"start\":32701},{\"end\":32937,\"start\":32892},{\"end\":33136,\"start\":33061},{\"end\":33227,\"start\":33174},{\"end\":33393,\"start\":33318},{\"end\":33525,\"start\":33454},{\"end\":33614,\"start\":33533},{\"end\":34052,\"start\":34004},{\"end\":34256,\"start\":34153},{\"end\":34503,\"start\":34441},{\"end\":34733,\"start\":34665},{\"end\":34945,\"start\":34867},{\"end\":35206,\"start\":35134},{\"end\":35573,\"start\":35521},{\"end\":35903,\"start\":35847},{\"end\":35944,\"start\":35925},{\"end\":36152,\"start\":36080},{\"end\":36373,\"start\":36337},{\"end\":36524,\"start\":36499},{\"end\":36761,\"start\":36673},{\"end\":36945,\"start\":36893},{\"end\":37144,\"start\":37083},{\"end\":37335,\"start\":37291},{\"end\":37523,\"start\":37429},{\"end\":37701,\"start\":37610},{\"end\":37880,\"start\":37772},{\"end\":38046,\"start\":37975},{\"end\":38107,\"start\":38054},{\"end\":38231,\"start\":38195},{\"end\":38411,\"start\":38348},{\"end\":38571,\"start\":38522},{\"end\":38720,\"start\":38655},{\"end\":38882,\"start\":38866},{\"end\":38998,\"start\":38911},{\"end\":32549,\"start\":32503},{\"end\":34854,\"start\":34801},{\"end\":35597,\"start\":35575},{\"end\":38727,\"start\":38722}]"}}}, "year": 2023, "month": 12, "day": 17}