{"id": 228376475, "updated": "2023-10-06 07:47:30.715", "metadata": {"title": "Exploring wav2vec 2.0 on speaker verification and language identification", "authors": "[{\"first\":\"Zhiyun\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Meng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Shiyu\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 12, "day": 11}, "abstract": "Wav2vec 2.0 is a recently proposed self-supervised framework for speech representation learning. It follows a two-stage training process of pre-training and fine-tuning, and performs well in speech recognition tasks especially ultra-low resource cases. In this work, we attempt to extend self-supervised framework to speaker verification and language identification. First, we use some preliminary experiments to indicate that wav2vec 2.0 can capture the information about the speaker and language. Then we demonstrate the effectiveness of wav2vec 2.0 on the two tasks respectively. For speaker verification, we obtain a new state-of-the-art result, Equal Error Rate (EER) of 3.61% on the VoxCeleb1 dataset. For language identification, we obtain an EER of 12.02% on 1 second condition and an EER of 3.47% on full-length condition of the AP17-OLR dataset. Finally, we utilize one model to achieve the unified modeling by the multi-task learning for the two tasks.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2012.06185", "mag": "3112616666", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/FanLZX21", "doi": "10.21437/interspeech.2021-1280"}}, "content": {"source": {"pdf_hash": "9a9d374d1dad72a0349c3a64be93660151274f41", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2012.06185v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2012.06185", "status": "GREEN"}}, "grobid": {"id": "4dde18f71f86f2e5ab06427b277cddb2ba788d47", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9a9d374d1dad72a0349c3a64be93660151274f41.txt", "contents": "\nEXPLORING WAV2VEC 2.0 ON SPEAKER VERIFICATION AND LANGUAGE IDENTIFICATION\n11 Dec 2020\n\nZhiyun Fan fanzhiyun2017@ia.ac.cn \nInstitute of Automation\nChinese Academy of Sciences\nChina\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nMeng Li limeng@ia.ac.cn \nInstitute of Automation\nChinese Academy of Sciences\nChina\n\nShiyu Zhou zhoushiyu2013@ia.ac.cn \nInstitute of Automation\nChinese Academy of Sciences\nChina\n\nBo Xu xubo@ia.ac.cn \nInstitute of Automation\nChinese Academy of Sciences\nChina\n\nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\nBeijingChina\n\nEXPLORING WAV2VEC 2.0 ON SPEAKER VERIFICATION AND LANGUAGE IDENTIFICATION\n11 Dec 2020Index Terms-Self-supervisedspeaker verificationlanguage identificationmulti-task learningwav2vec 20\nWav2vec 2.0 is a recently proposed self-supervised framework for speech representation learning. It follows a two-stage training process of pre-training and fine-tuning, and performs well in speech recognition tasks especially ultra-low resource cases. In this work, we attempt to extend self-supervised framework to speaker verification and language identification. First, we use some preliminary experiments to indicate that wav2vec 2.0 can capture the information about the speaker and language. Then we demonstrate the effectiveness of wav2vec 2.0 on the two tasks respectively. For speaker verification, we obtain a new state-of-the-art result, Equal Error Rate (EER) of 3.61% on the VoxCeleb1 dataset. For language identification, we obtain an EER of 12.02% on 1 second condition and an EER of 3.47% on full-length condition of the AP17-OLR dataset. Finally, we utilize one model to achieve the unified modeling by the multi-task learning for the two tasks.\n\nINTRODUCTION\n\nRecently, neural networks trained with a large amount of labeled data can meet most industrial needs in the field of speech processing [1,2,3,4,5,6]. However, purely supervised learning seems to be inconsistent with the mechanism of human learning. Early on in their lives, human infants learn language by watching and listening to adults around them, which resembles an unsupervised learning process. Later, they learn reading and writing, which seems to be a supervised learning process. To simulate the two-stage learning process, a lot of self-supervised frameworks are proposed [7,8,9,10,11,12].\n\nIn the field of speech processing, most self-supervised methods can be divided into two categories. One kind of method is conducted by the reconstruction loss, such as autoregressive predictive coding (APC) [13], masked predictive coding (MPC) [14] and so on. The other kind of method is conducted by contrastive predictive loss. The most representative work is the contrastive predictive coding (CPC) [15] and wav2vec [16]. The wav2vec 2.0 [17] used in this paper belongs to the latter category. Most of these self-supervised pretraining methods are applied to speech recognition. However, there is almost no work on whether pre-training methods could work on speaker verification (SV) or language identification (LID). In this paper, we use the framework of wav2vec 2.0 [17] to explore this feasibility.\n\nWe denote the model structure used in wav2vec 2.0 as w2vencoder in this paper. It is illustrated in the dashed box of Fig. 1.\n\nIt mainly consists of a convolutional neural network (CNN) encoder and a Transformer [18]. The CNN transfers raw waveform input to latent speech representations. They are fed to the Transformer after being masked and converted to context representations. A quantization module converts the latent speech representations to a discrete version which is used as the target. The whole model is trained to solve a contrastive task, which requires identifying the true quantized latent speech representations for a masked time step within a set of distractors [17]. After pre-training, Baevski et al. applied it to ultra-low resource speech recognition. Using only ten minutes labeled data, their approach achieved word error rate (WER) of 5.7/10.1% on the clean/noisy test sets of Librispeech. The results demonstrate that the phoneme-related information is preserved during the pre-training of w2v-encoder and the drownstream task such as speech recognition can benefit a lot from it. Speech is a complex signal that contains not only phoneme-related information but also factors about speaker, language, environment, noise, etc. However, there is very little work on pre-training for SV and LID.\n\nIn this paper, we explore the effectiveness of self-supervised pre-training on SV and LID tasks. We utilize the pre-trained w2vencoder to extract context representations, and use t-SNE [19] tools to visualize them. We find that they have distinguishability among different speakers and languages even if pre-training of wav2vec 2.0 is problem-agnostic. Moreover, we find the lower layer has the stronger distinguishability. This distinguishability is exactly what SV and LID tasks need. It also verifies the feasibility of applying the self-supervised pre-training to the two tasks. Thus, we attempt to fine-tune the pre-trained model on these two downstream tasks respectively. For SV task, we obtain an EER of 3.61% on the test set of the VoxCeleb1 dataset. For LID task, we obtain an EER of 12.02% on 1 second condition and 3.47% on full-length condition of the AP17-OLR dataset. Furthermore, in order to simplify the finetuning process and reduce model parameters, we utilize the multitask learning to conduct the fine-tuning on the two tasks simultaneously.\n\n\nMETHOD\n\nIn this section, we first review the pre-training of the wav2vec 2.0 [17]. Then we introduce how to apply the pre-trained model to downstream tasks. Fig. 1 illustrates the pre-training and fine-tuning. quantization module. The CNN encoder stacks seven blocks, and in each block the temporal convolutions followed by GELU activation function [20] have 512 channels with strides (5, 2, 2, 2, 2, 2, 2) and kernel widths (10, 3, 3, 3, 3, 2, 2). The CNN encoder maps the raw audio X into latent speech representations Z. The context network stacks 12 Transformer blocks with model dimension 768, inner dimension 3, 072, and 8 attention heads. Before sending Z into the context network, all time steps of Z are randomly sampled as starting indices with p = 0.065, and M = 10 consecutive time steps from every sampled index are masked. Then the relative positional embedding is added to the masked representations. The Transformer contextualizes the masked representations and generates context representations C.\n\nThe quantization module is used to discretize latent speech representations Z into Q. There are G = 2 codebooks in the quantization module. Each of them contains V = 320 entries with a size of 128. The quantization module firstly maps the Z to logits l \u2208 R G\u00d7V . Then the gumbel softmax [21] is used to choose one entry from each codebook in a fully differentiable way. All the entries selected are concatenated to resulting vectors [e1; e2; ...; eG], which are linearly mapped to q. The loss function is as follows:\nL = LM + \u03b1LD + \u03b2LF (1) LM = \u2212log exp(sim(ct, qt))/k q\u223cQt exp(sim(ct,q))/k (2) LD = 1 GV G g=1 V v=1p g,vlogpg,v(3)p = GumbelSof tmax(l) (4)\nThe loss is the weighted sum of three terms. In the Eq. 1, LF is a L2 penalty. The weight \u03b2 is set to 10. The LM is the contrastive loss to make model distinguish true discrete representations from latent distractorsq. The distractors are uniformly sampled from other masked time steps of the same utterance. The LD is the diversity loss designed to increase the use of the quantized codebook representations. The \u03b1 in Eq. 1 is set to 0.1. In Eq. 2, the sim represents cosine similarity, and the Qt includes qt and K = 100 distractors, and the temperature k is set to 0.1. Thel in Eq. 4 represents the average of logits l across utterances in a batch.\n\nThe pre-training process is optimized with Adam [22]. During the first 8% of the updates, learning rate warms up to a peak of 5 \u00d7 10 \u22123 , and then it decays linearly. For more details about the pretraining of wav2vec 2.0, we refer readers to [17].\n\n\nFine-tuning\n\nBefore the post-training, we add an average pooling layer and a fully connected layer on the top of w2v-encoder. The average pooling layer converts the frame-level context representations given by w2vencoder into sentence-level representations, and the fully connected layer classifies each sentence into some speaker or some language.\n\nThe newly added fully connected layer is randomly initialized, and w2v-encoder is initialized with the base model released by Baevski et al 1 . The cross-entropy criteria is employed as the loss function for the the classification of speakers or languages. Specially, for the training of speaker classification, AM-softmax [23] is used to increase the discrimination of the learned embedding to the speaker.\n\nIn the multi-task fine-tuning, we add a pooling layer and two parallel fully connected layers to predict the speaker and language respectively. The training loss is obtained by the weighted sum of the losses of these two tasks. The Lsv and L lid in Eq. 5 represent the CE loss of SV and LID tasks respectively.\nL mul = \u03bbLsv + (1 \u2212 \u03bb)L lid(5)\nDue to the problem of unbalanced data volume in the datasets of the speakers and languages, the batch is generated by sampling from two datasets with equal probability to ensure the data used in the training process is balanced. In addition, the two tasks also have the problem of inconsistent convergence speed. We mitigate this issue by adjusting the weight of the loss of the two tasks through the development set. can learn about any other factors. In the experiment part, we take speaker and language factors as examples to explore this question, and try to apply wav2vec 2.0 to SV and LID tasks.\n\n\nDatasets\n\nVoxCeleb1 [24] and AP17-OLR [25] datasets are used in our experiments for SV and LID respectively.\n\nSpeaker verification dataset: VoxCeleb1 [24] contains over 100,000 utterances from 1,251 celebrities. It can be used for both speaker identification and verification. We use VoxCeleb1 to conduct the SV task. The data split of the VoxCeleb1 dataset for verification is listed in Table 1. Language identification dataset: AP17-OLR [25] consists of 10 different languages (Mandarin, Cantonese, Indonesian, Japanese, Russian, Korean, Vietnamese, Kazakh, Tibetan and Uyghur). The duration of training data for each language is about 10 hours with the speech sampled at 16 kHz. The test set contains three subsets with different durations (1 second, 3 second, and full length). These subsets respectively contain 17964, 16404 and 17964 utterances.\n\n\nModel description\n\nIn the experiments, we utilize the base model released by Baevski et al. and three models fine-tuned by us. For simplicity, we use some symbols to represent them, and the explanations are as follows:\n\n\u2022 M-nofinetune: the base model pre-trained on the Librispeech corpus [26].\n\n\u2022 M-sv: We fine-tune M-nofinetune on the VoxCeleb1 dataset for speaker verification.\n\n\u2022 M-lid: We fine-tune M-nofinetune on the AP17-OLR dataset for language identification.\n\n\u2022 M-multi: We fine-tune M-nofinetune on the AP17-OLR and VoxCeleb1 dataset simultaneously in a multi-task form.\n\n\nFeasibility analysis\n\nIn this section, we explore whether the speaker and language factors is retained during the pre-training of wav2vec 2.0. It determines whether the pre-training method can be used for these two tasks. We directly extract context representations from the test set of AP17-OLR and VoxCeleb1 with the M-nofinetune model. Then we visualize the context representations by t-SNE [19], a nonlinear dimensionality reduction algorithm for visualizing high-dimensional data. The results are shown in Fig. 2. The left three images are the visualization results of the features from the three layers of the Transformer. Different colors represent different speakers. It is not difficult to find that all the three layers have certain speaker distinguishability, and this distinguishability is more obvious at the bottom of the Transformer. In the three images on the right, different colors represent different languages. It can also be found that these features are distinguished by languages, and the lower the layer, the stronger the distinction. The phenomena presented in Fig. 2 show that the model pre-trained by wav2vec 2.0 can effectively extract the characteristics of the speaker and language of the speech. We further quantify this claim by performing SV and LID with a simple fully connected layer. The pre-trained w2v-encoder (Mnofinetune) acts as a feature extractor. The fully connected layer is optimized to distinguish the 10 languages or 1211 speakers for the two tasks respectively. The test results are listed in Table 2. The random results are evaluated on the randomly initialized w2v-encoder. The comparison of these two results in Table 2 further illustrates that the model pre-trained by wav2vec 2.0 can extract speaker and language-related characteristics, which provides a basis for the application of wav2vec 2.0 to SV and LID.\n(a1) layer12 (b1) layer12 (a2) layer6 (b2) layer6 (a3) layer1 (b3) layer1\n\nSpeaker verification\n\nFrom the experiments in the previous section, we can see that the pre-trained w2v-encoder, M-nofinetune, can extract features that contain a certain speaker distinguishability. This kind of speaker distinguishing learning is exactly required in the SV task. Then we attempt to fine-tune the pre-trained w2v-encoder, M-nofinetune, to finish SV task. We initialize the w2v-encoder with M-nofinetune, and add a randomly initialized fully connected layer on the top of it to predict speakers. The fine-tuning is conducted on the VoxCeleb1 dataset. All parameters are adjustable during fine-tuning. However, at the first 10000 steps, the w2v-encoder is frozen. We optimize the model with Adam, the learning rate warms up to 5 \u00d7 10 \u22123 during the first 6000 steps, and then it decays linearly during the rest 7000 steps. \n\n\nModel\n\nEER I-vectors + PLDA [24] 8.8 CNN + Embedding [24] 7.8 LDE-ASoftmax [27] 4.41 attentive statistics [28] 3.85 no pre-training 24.28 M-sv 3.61\n\nThe no pre-training in Table 3 represents that using Vox-Celeb1 to train the w2v-encoder added a fully connected layer without pre-training. Our fine-tuning model, M-sv, outperforms the no pre-training result by a significant margin (EER of 3.61% vs 24.28%). The gap between them illustrates the benefits of pretraining. Moreover, our model outperforms all baselines in Table 3, and obtains a new state-of-the-art results on VoxCeleb1 dataset. It means the pre-training of wav2vec 2.0 is useful to SV task and can work well without any task-specific adjustment of model structure.\n\n\nLanguage identification\n\nAlthough Baevski et al. only used English data during the pretraining of M-nofinetune, it can be seen from the visualization results in section 3.3 that the features extracted by M-nofinetune still retain the distinction of language. It means that the model obtained by this pre-training method may be useful to the language identification system. Similarly, we add a fully connected layer on top of the w2v-encoder to predict language. We initialize w2v-encoder with M-nofinetune and randomly initialize the extra fully connected layer. Then the whole model is fine-tuned on the AP17-OLR dataset. We optimize the model with Adam, the learning rate warms up to 5 \u00d7 10 \u22123 during the first 5000 steps, and then it decays linearly during the rest 8000 steps. The parameters of w2v-encoder part are frozen at the first 5000 steps. After training, we test on the model, which obtains the best performance on the development set. The first two rows in Table 4 are the two baselines released by the organizer of the AP17-OLR challenge. The T SM -DN N -BN -LST M [29] is one of the best models on this benchmark. The no pre-training in Table 4 means that the fine-tuning starts from the scratch on the AP17-OLR dataset. The M-lid, which is finetuned from the pre-trained M-nofinetune, outperforms the no pretraining result by a large margin on both 1 second condition and full-length condition. The gap between them illustrates the benefits brought by pre-training to the LID task. Compared with baselines released by the organizer, M-lid shows a clear performance advantage on the two test conditions. However, it is far from the best results. It means that wav2vec 2.0 is useful to LID task. However, its effectiveness on LID task is not good as SV task. We consider that the use of multiple languages during pre-training (not just English) can mitigate this issue. In addition, we find that the performance of the no pre-training is influenced by overfitting seriously. This problem is obviously alleviated during the fine-tuning of M-lid, which benefits from pre-training.\n\n\nMulti-task system\n\nThe parameters of the w2v-encoder have reached 94M. Fine-tuning two models for SV and LID tasks independently will take up a lot of resources. Hence, we try to use one model to finish these two tasks simultaneously. On the top of w2v-encoder we connect two fully connected layers in parallel to predict the speaker and language respectively. We follow the experiment settings described in section 2.2. The \u03bb in Eq. 5 is set to 0.7. Results in Table 5 show that compared with single-task training, although the performance of multi-task form is a bit reduced, it achieves good results with fewer parameters on the SV and LID tasks. It shows that the pre-training of wav2vec 2.0 can be combined with multi-task learning to achieve unified modeling of the two tasks. This greatly simplifies the use of pre-trained model and can save a lot of time spent on fine-tuning to each task. In addition, it can reduce the demand for storage.\n\n\nCONCLUSION\n\nIn this paper, we explore the application of wav2vec 2.0 on speaker verification and language identification. First of all, through some preliminary experiments and visualization methods, we find that the features extracted by the pre-trained w2v-encoder have the distinction between speakers and languages, and this distinction is more obvious in lower layers. This illustrates the feasibility of using the pre-trained model for SV and LID tasks. Then we verify the effectiveness of the pre-trained model on the two tasks and obtain competitive results on the VoxCeleb1 and AP17-OLR datasets. Finally, in order to simplify the fine-tuning process on multiple tasks and reduce parameters, we use a multi-task learning mechanism, so as to realize the unified modeling for SV and LID. In future work, we are planning to extend wav2vec 2.0 to more speech processing tasks with the multi-task learning.\n\n2. 1 .Fig. 1 :\n11Pre-training of wav2vec 2.0The left side ofFig. 1gives an illustration of the w2v-encoder and its pre-training. The main body of the model consists of a CNNbased feature encoder, a Transformer-based context network and a An overview of the pre-training and fine-tuning. The models architecture used in pre-training stage and fine-tuning stage are identical, except the quantization modules and extra output layers.\n\n\ninformative factors are mixed in speech signals, including semantics, speaker, emotion, channel, background noise, etc. Baevski et al. have shown that the representations underlying pretrained w2v-encoder can capture the linguistic factors. It remains unclear whether the problem-agnostic pre-training of wav2vec 2.0\n\nFig. 2 :\n22D t-SNE plot of representations extracted from the bottom layer (layer1), the middle layer (layer6), and the high layer (layer12) of the Transformer of M-nofinetune. The left column is the clustering of the representations of 10 speakers in the test set of VoxCeleb1 extracted by M-nofinetune, and each color represents a speaker. The right column is the clustering of the representations of 1000 samples in the test set of AP17-OLR extracted by M-nofinetune, and each color represents a language.\n\nTable 1 :\n1Data split of the VoxCeleb1 dataset for verification.Train \nValid \nTest \n#speakers \n1211 \n1145 \n40 \n#Utterances \n143642 \n5000 \n4874 \nDur(hrs.) \n329.06 \n11.34 \n11.20 \n\n\n\nTable 2 :\n2The EER (%) results of using context representations extracted by M-nofinetune to finish SV and LID.Model \nSV \nLID \nrandom \n47.93 \n50.05 \nM-nofinetuning \n15.62 \n42.34 \n\n\n\nTable 3 :\n3Comparison with the previously published EER (%) results on Voxceleb1 dataset.\n\nTable 4 :\n4Comparison with the previously published Cavg and EER (%) results on AP-17 dataset.Model \n1 second \nFull-Length \n\nCavg EER Cavg EER \n\ni-vector + PLDA[25] \n0.1746 17.51 0.0596 5.86 \nTDNN [25] \n0.1282 14.04 0.1034 11.31 \nTSM-DNN-BN-LSTM [29] 0.067 6.95 0.007 0.86 \nno pre-training \n0.2813 29.25 0.1254 13.83 \nM-lid \n0.1158 12.02 0.0310 3.47 \n\n\n\nTable 5 :\n5Performance of single-task model and multi-task model on the VoxCeleb1 and AP17-OLR dataset in terms of EER(%).Model \nSV \nLID \nM-sv \n3.61 \n-\nM-lid \n-\n3.47 \nM-multi \n4.18 \n4.88 \n\n\nhttps://github.com/pytorch/fairseq/blob/master/ examples/wav2vec/\n\nThe speech transformer for large-scale mandarin chinese speech recognition. Jie Li, Xiaorui Wang, Yan Li, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEJie Li, Xiaorui Wang, Yan Li, et al., \"The speech transformer for large-scale mandarin chinese speech recognition,\" in ICASSP 2019-2019 IEEE International Conference on Acous- tics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 7095-7099.\n\nLarge-scale multilingual speech recognition with a streaming end-to-end model. Anjuli Kannan, Arindrima Datta, Tara N Sainath, Eugene Weinstein, Bhuvana Ramabhadran, Yonghui Wu, Ankur Bapna, Zhifeng Chen, Seungji Lee, arXiv:1909.05330arXiv preprintAnjuli Kannan, Arindrima Datta, Tara N Sainath, Eugene We- instein, Bhuvana Ramabhadran, Yonghui Wu, Ankur Bapna, Zhifeng Chen, and Seungji Lee, \"Large-scale multilingual speech recognition with a streaming end-to-end model,\" arXiv preprint arXiv:1909.05330, 2019.\n\nUtterance-level aggregation for speaker recognition in the wild. Weidi Xie, Arsha Nagrani, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing. Joon Son Chung, and Andrew Zisserman. ICASSPWeidi Xie, Arsha Nagrani, Joon Son Chung, and Andrew Zis- serman, \"Utterance-level aggregation for speaker recognition in the wild,\" in ICASSP 2019-2019 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP).\n\n. IEEE. IEEE, 2019, pp. 5791-5795.\n\nVoxceleb: Large-scale speaker verification in the wild. Arsha Nagrani, Joon Son Chung, Weidi Xie, Andrew Zisserman, Computer Speech & Language. 60101027Arsha Nagrani, Joon Son Chung, Weidi Xie, and Andrew Zis- serman, \"Voxceleb: Large-scale speaker verification in the wild,\" Computer Speech & Language, vol. 60, pp. 101027, 2020.\n\nAttention based hybrid i-vector blstm model for language recognition. Bharat Padi, Anand Mohan, Sriram Ganapathy, INTERSPEECH. Bharat Padi, Anand Mohan, and Sriram Ganapathy, \"Attention based hybrid i-vector blstm model for language recognition.,\" in INTERSPEECH, 2019, pp. 1263-1267.\n\nInteractive learning of teacher-student model for short utterance spoken language identification. Peng Shen, Xugang Lu, Sheng Li, Hisashi Kawai, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEPeng Shen, Xugang Lu, Sheng Li, and Hisashi Kawai, \"In- teractive learning of teacher-student model for short utterance spoken language identification,\" in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP). IEEE, 2019, pp. 5981-5985.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, \"Bert: Pre-training of deep bidirectional transformers for language understanding,\" arXiv preprint arXiv:1810.04805, 2018.\n\nE Matthew, Mark Peters, Mohit Neumann, Matt Iyyer, Christopher Gardner, Kenton Clark, Luke Lee, Zettlemoyer, arXiv:1802.05365Deep contextualized word representations. arXiv preprintMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gard- ner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer, \"Deep contextualized word representations,\" arXiv preprint arXiv:1802.05365, 2018.\n\nImproving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever, \"Improving language understanding by generative pre-training,\" 2018.\n\nData-efficient image recognition with contrastive predictive coding. J Olivier, Aravind H\u00e9naff, Jeffrey De Srinivas, Ali Fauw, Carl Razavi, Doersch, Aaron Sm Eslami, Van Den Oord, arXiv:1905.09272arXiv preprintOlivier J H\u00e9naff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord, \"Data-efficient image recognition with contrastive predictive coding,\" arXiv preprint arXiv:1905.09272, 2019.\n\nLearning representations by maximizing mutual information across views. Philip Bachman, Devon Hjelm, William Buchwalter, Advances in Neural Information Processing Systems. Philip Bachman, R Devon Hjelm, and William Buchwalter, \"Learning representations by maximizing mutual information across views,\" in Advances in Neural Information Processing Systems, 2019, pp. 15535-15545.\n\nMulti-task self-supervised learning for robust speech recognition. Mirco Ravanelli, Jianyuan Zhong, Santiago Pascual, Pawel Swietojanski, Joao Monteiro, Jan Trmal, Yoshua Bengio, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPMirco Ravanelli, Jianyuan Zhong, Santiago Pascual, Pawel Swietojanski, Joao Monteiro, Jan Trmal, and Yoshua Bengio, \"Multi-task self-supervised learning for robust speech recog- nition,\" in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6989-6993.\n\nAn unsupervised autoregressive model for speech representation learning. Yu-An Chung, Wei-Ning Hsu, Hao Tang, James Glass, arXiv:1904.03240arXiv preprintYu-An Chung, Wei-Ning Hsu, Hao Tang, and James Glass, \"An unsupervised autoregressive model for speech represen- tation learning,\" arXiv preprint arXiv:1904.03240, 2019.\n\nImproving transformer-based speech recognition using unsupervised pre-training. Dongwei Jiang, Xiaoning Lei, Wubo Li, Ne Luo, Yuxuan Hu, Wei Zou, Xiangang Li, arXiv:1910.09932arXiv preprintDongwei Jiang, Xiaoning Lei, Wubo Li, Ne Luo, Yuxuan Hu, Wei Zou, and Xiangang Li, \"Improving transformer-based speech recognition using unsupervised pre-training,\" arXiv preprint arXiv:1910.09932, 2019.\n\nRepresentation learning with contrastive predictive coding. Aaron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748arXiv preprintAaron van den Oord, Yazhe Li, and Oriol Vinyals, \"Repre- sentation learning with contrastive predictive coding,\" arXiv preprint arXiv:1807.03748, 2018.\n\nwav2vec: Unsupervised pre-training for speech recognition. Steffen Schneider, Alexei Baevski, Ronan Collobert, Michael Auli, arXiv:1904.05862arXiv preprintSteffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli, \"wav2vec: Unsupervised pre-training for speech recognition,\" arXiv preprint arXiv:1904.05862, 2019.\n\nwav2vec 2.0: A framework for selfsupervised learning of speech representations. Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli, arXiv:2006.11477arXiv preprintAlexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli, \"wav2vec 2.0: A framework for self- supervised learning of speech representations,\" arXiv preprint arXiv:2006.11477, 2020.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, \"Attention is all you need,\" in Advances in neural information processing systems, 2017, pp. 5998-6008.\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9Laurens van der Maaten and Geoffrey Hinton, \"Visualizing data using t-sne,\" Journal of machine learning research, vol. 9, no. Nov, pp. 2579-2605, 2008.\n\nDan Hendrycks, Kevin Gimpel, arXiv:1606.08415Gaussian error linear units (gelus). arXiv preprintDan Hendrycks and Kevin Gimpel, \"Gaussian error linear units (gelus),\" arXiv preprint arXiv:1606.08415, 2016.\n\nStatistical theory of extreme values and some practical applications. Emil Julius Gumbel, NBS Applied Mathematics Series. 33Emil Julius Gumbel, \"Statistical theory of extreme values and some practical applications,\" NBS Applied Mathematics Series, vol. 33, 1954.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba, \"Adam: A method for stochastic optimization,\" arXiv preprint arXiv:1412.6980, 2014.\n\nAdditive margin softmax for face verification. Feng Wang, Jian Cheng, Weiyang Liu, Haijun Liu, IEEE Signal Processing Letters. 257Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu, \"Ad- ditive margin softmax for face verification,\" IEEE Signal Pro- cessing Letters, vol. 25, no. 7, pp. 926-930, 2018.\n\nVoxceleb: A large-scale speaker identification dataset. Arsha Nagrani, Joon Son Chung, Andrew Zisserman, arXiv:1706.08612arXiv preprintArsha Nagrani, Joon Son Chung, and Andrew Zisserman, \"Voxceleb: A large-scale speaker identification dataset,\" arXiv preprint arXiv:1706.08612, 2017.\n\nAP18-OLR challenge: Three tasks and their baselines. Zhiyuan Tang, Dong Wang, Qing Chen, abs/1806.00616CoRR. Zhiyuan Tang, Dong Wang, and Qing Chen, \"AP18-OLR challenge: Three tasks and their baselines,\" CoRR, vol. abs/1806.00616, 2018.\n\nLibrispeech: an asr corpus based on public domain audio books. Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur, 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, \"Librispeech: an asr corpus based on public do- main audio books,\" in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206-5210.\n\nExploring the encoding layer and loss function in end-to-end speaker and language recognition system. Weicheng Cai, Jinkun Chen, Ming Li, arXiv:1804.05160arXiv preprintWeicheng Cai, Jinkun Chen, and Ming Li, \"Exploring the en- coding layer and loss function in end-to-end speaker and lan- guage recognition system,\" arXiv preprint arXiv:1804.05160, 2018.\n\nAttentive statistics pooling for deep speaker embedding. Koji Okabe, Takafumi Koshinaka, Koichi Shinoda, Proc. Koji Okabe, Takafumi Koshinaka, and Koichi Shinoda, \"At- tentive statistics pooling for deep speaker embedding,\" Proc. Interspeech 2018, pp. 2252-2256, 2018.\n\nShort utterance based speech language identification in intelligent vehicles with time-scale modifications and deep bottleneck features. Zhanyu Ma, Hong Yu, Wei Chen, Jun Guo, IEEE transactions on vehicular technology. 681Zhanyu Ma, Hong Yu, Wei Chen, and Jun Guo, \"Short ut- terance based speech language identification in intelligent ve- hicles with time-scale modifications and deep bottleneck fea- tures,\" IEEE transactions on vehicular technology, vol. 68, no. 1, pp. 121-128, 2018.\n", "annotations": {"author": "[{\"end\":271,\"start\":88},{\"end\":355,\"start\":272},{\"end\":449,\"start\":356},{\"end\":619,\"start\":450}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":95},{\"end\":279,\"start\":277},{\"end\":366,\"start\":362},{\"end\":455,\"start\":453}]", "author_first_name": "[{\"end\":94,\"start\":88},{\"end\":276,\"start\":272},{\"end\":361,\"start\":356},{\"end\":452,\"start\":450}]", "author_affiliation": "[{\"end\":180,\"start\":123},{\"end\":270,\"start\":182},{\"end\":354,\"start\":297},{\"end\":448,\"start\":391},{\"end\":528,\"start\":471},{\"end\":618,\"start\":530}]", "title": "[{\"end\":74,\"start\":1},{\"end\":693,\"start\":620}]", "venue": null, "abstract": "[{\"end\":1768,\"start\":805}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1922,\"start\":1919},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1924,\"start\":1922},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1926,\"start\":1924},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1928,\"start\":1926},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1930,\"start\":1928},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1932,\"start\":1930},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2370,\"start\":2367},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2372,\"start\":2370},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2374,\"start\":2372},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2377,\"start\":2374},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2380,\"start\":2377},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2383,\"start\":2380},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2597,\"start\":2593},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2634,\"start\":2630},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2792,\"start\":2788},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2809,\"start\":2805},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2831,\"start\":2827},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3162,\"start\":3158},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3409,\"start\":3405},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3878,\"start\":3874},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4703,\"start\":4699},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5660,\"start\":5656},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5932,\"start\":5928},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6886,\"start\":6882},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7957,\"start\":7953},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8151,\"start\":8147},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8832,\"start\":8828},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9884,\"start\":9880},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9902,\"start\":9898},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10014,\"start\":10010},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10303,\"start\":10299},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11007,\"start\":11003},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11697,\"start\":11693},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14110,\"start\":14106},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14135,\"start\":14131},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14157,\"start\":14153},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14188,\"start\":14184},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15894,\"start\":15890}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19200,\"start\":18768},{\"attributes\":{\"id\":\"fig_1\"},\"end\":19519,\"start\":19201},{\"attributes\":{\"id\":\"fig_2\"},\"end\":20029,\"start\":19520},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":20209,\"start\":20030},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20391,\"start\":20210},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":20482,\"start\":20392},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":20836,\"start\":20483},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":21027,\"start\":20837}]", "paragraph": "[{\"end\":2384,\"start\":1784},{\"end\":3191,\"start\":2386},{\"end\":3318,\"start\":3193},{\"end\":4512,\"start\":3320},{\"end\":5576,\"start\":4514},{\"end\":6593,\"start\":5587},{\"end\":7111,\"start\":6595},{\"end\":7903,\"start\":7252},{\"end\":8152,\"start\":7905},{\"end\":8503,\"start\":8168},{\"end\":8912,\"start\":8505},{\"end\":9224,\"start\":8914},{\"end\":9857,\"start\":9256},{\"end\":9968,\"start\":9870},{\"end\":10711,\"start\":9970},{\"end\":10932,\"start\":10733},{\"end\":11008,\"start\":10934},{\"end\":11094,\"start\":11010},{\"end\":11183,\"start\":11096},{\"end\":11296,\"start\":11185},{\"end\":13163,\"start\":11321},{\"end\":14075,\"start\":13261},{\"end\":14225,\"start\":14085},{\"end\":14807,\"start\":14227},{\"end\":16903,\"start\":14835},{\"end\":17854,\"start\":16925},{\"end\":18767,\"start\":17869}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7226,\"start\":7112},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7251,\"start\":7226},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9255,\"start\":9225},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13237,\"start\":13164}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10255,\"start\":10248},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12848,\"start\":12841},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12970,\"start\":12963},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14257,\"start\":14250},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":14604,\"start\":14597},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15788,\"start\":15781},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15970,\"start\":15963},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17375,\"start\":17368}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1782,\"start\":1770},{\"attributes\":{\"n\":\"2.\"},\"end\":5585,\"start\":5579},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8166,\"start\":8155},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9868,\"start\":9860},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10731,\"start\":10714},{\"attributes\":{\"n\":\"3.3.\"},\"end\":11319,\"start\":11299},{\"attributes\":{\"n\":\"3.4.\"},\"end\":13259,\"start\":13239},{\"end\":14083,\"start\":14078},{\"attributes\":{\"n\":\"3.5.\"},\"end\":14833,\"start\":14810},{\"attributes\":{\"n\":\"3.6.\"},\"end\":16923,\"start\":16906},{\"attributes\":{\"n\":\"4.\"},\"end\":17867,\"start\":17857},{\"end\":18783,\"start\":18769},{\"end\":19529,\"start\":19521},{\"end\":20040,\"start\":20031},{\"end\":20220,\"start\":20211},{\"end\":20402,\"start\":20393},{\"end\":20493,\"start\":20484},{\"end\":20847,\"start\":20838}]", "table": "[{\"end\":20209,\"start\":20095},{\"end\":20391,\"start\":20322},{\"end\":20836,\"start\":20578},{\"end\":21027,\"start\":20960}]", "figure_caption": "[{\"end\":19200,\"start\":18786},{\"end\":19519,\"start\":19203},{\"end\":20029,\"start\":19531},{\"end\":20095,\"start\":20042},{\"end\":20322,\"start\":20222},{\"end\":20482,\"start\":20404},{\"end\":20578,\"start\":20495},{\"end\":20960,\"start\":20849}]", "figure_ref": "[{\"end\":3317,\"start\":3311},{\"end\":5742,\"start\":5736},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11816,\"start\":11810},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12391,\"start\":12385}]", "bib_author_first_name": "[{\"end\":21174,\"start\":21171},{\"end\":21186,\"start\":21179},{\"end\":21196,\"start\":21193},{\"end\":21639,\"start\":21633},{\"end\":21657,\"start\":21648},{\"end\":21669,\"start\":21665},{\"end\":21671,\"start\":21670},{\"end\":21687,\"start\":21681},{\"end\":21706,\"start\":21699},{\"end\":21727,\"start\":21720},{\"end\":21737,\"start\":21732},{\"end\":21752,\"start\":21745},{\"end\":21766,\"start\":21759},{\"end\":22138,\"start\":22133},{\"end\":22149,\"start\":22144},{\"end\":22630,\"start\":22625},{\"end\":22644,\"start\":22640},{\"end\":22648,\"start\":22645},{\"end\":22661,\"start\":22656},{\"end\":22673,\"start\":22667},{\"end\":22977,\"start\":22971},{\"end\":22989,\"start\":22984},{\"end\":23003,\"start\":22997},{\"end\":23289,\"start\":23285},{\"end\":23302,\"start\":23296},{\"end\":23312,\"start\":23307},{\"end\":23324,\"start\":23317},{\"end\":23809,\"start\":23804},{\"end\":23826,\"start\":23818},{\"end\":23840,\"start\":23834},{\"end\":23854,\"start\":23846},{\"end\":24087,\"start\":24086},{\"end\":24101,\"start\":24097},{\"end\":24115,\"start\":24110},{\"end\":24129,\"start\":24125},{\"end\":24148,\"start\":24137},{\"end\":24164,\"start\":24158},{\"end\":24176,\"start\":24172},{\"end\":24530,\"start\":24526},{\"end\":24547,\"start\":24540},{\"end\":24563,\"start\":24560},{\"end\":24578,\"start\":24574},{\"end\":24798,\"start\":24797},{\"end\":24815,\"start\":24808},{\"end\":24831,\"start\":24824},{\"end\":24834,\"start\":24832},{\"end\":24848,\"start\":24845},{\"end\":24859,\"start\":24855},{\"end\":24882,\"start\":24877},{\"end\":25241,\"start\":25235},{\"end\":25256,\"start\":25251},{\"end\":25271,\"start\":25264},{\"end\":25614,\"start\":25609},{\"end\":25634,\"start\":25626},{\"end\":25650,\"start\":25642},{\"end\":25665,\"start\":25660},{\"end\":25684,\"start\":25680},{\"end\":25698,\"start\":25695},{\"end\":25712,\"start\":25706},{\"end\":26218,\"start\":26213},{\"end\":26234,\"start\":26226},{\"end\":26243,\"start\":26240},{\"end\":26255,\"start\":26250},{\"end\":26551,\"start\":26544},{\"end\":26567,\"start\":26559},{\"end\":26577,\"start\":26573},{\"end\":26584,\"start\":26582},{\"end\":26596,\"start\":26590},{\"end\":26604,\"start\":26601},{\"end\":26618,\"start\":26610},{\"end\":26923,\"start\":26918},{\"end\":26943,\"start\":26938},{\"end\":26953,\"start\":26948},{\"end\":27212,\"start\":27205},{\"end\":27230,\"start\":27224},{\"end\":27245,\"start\":27240},{\"end\":27264,\"start\":27257},{\"end\":27558,\"start\":27552},{\"end\":27573,\"start\":27568},{\"end\":27591,\"start\":27580},{\"end\":27608,\"start\":27601},{\"end\":27869,\"start\":27863},{\"end\":27883,\"start\":27879},{\"end\":27897,\"start\":27893},{\"end\":27911,\"start\":27906},{\"end\":27928,\"start\":27923},{\"end\":27941,\"start\":27936},{\"end\":27943,\"start\":27942},{\"end\":27957,\"start\":27951},{\"end\":27971,\"start\":27966},{\"end\":28304,\"start\":28297},{\"end\":28329,\"start\":28321},{\"end\":28533,\"start\":28530},{\"end\":28550,\"start\":28545},{\"end\":28811,\"start\":28807},{\"end\":28818,\"start\":28812},{\"end\":29046,\"start\":29045},{\"end\":29062,\"start\":29057},{\"end\":29272,\"start\":29268},{\"end\":29283,\"start\":29279},{\"end\":29298,\"start\":29291},{\"end\":29310,\"start\":29304},{\"end\":29585,\"start\":29580},{\"end\":29599,\"start\":29595},{\"end\":29603,\"start\":29600},{\"end\":29617,\"start\":29611},{\"end\":29870,\"start\":29863},{\"end\":29881,\"start\":29877},{\"end\":29892,\"start\":29888},{\"end\":30117,\"start\":30111},{\"end\":30135,\"start\":30129},{\"end\":30148,\"start\":30142},{\"end\":30163,\"start\":30156},{\"end\":30631,\"start\":30623},{\"end\":30643,\"start\":30637},{\"end\":30654,\"start\":30650},{\"end\":30938,\"start\":30934},{\"end\":30954,\"start\":30946},{\"end\":30972,\"start\":30966},{\"end\":31290,\"start\":31284},{\"end\":31299,\"start\":31295},{\"end\":31307,\"start\":31304},{\"end\":31317,\"start\":31314}]", "bib_author_last_name": "[{\"end\":21177,\"start\":21175},{\"end\":21191,\"start\":21187},{\"end\":21199,\"start\":21197},{\"end\":21646,\"start\":21640},{\"end\":21663,\"start\":21658},{\"end\":21679,\"start\":21672},{\"end\":21697,\"start\":21688},{\"end\":21718,\"start\":21707},{\"end\":21730,\"start\":21728},{\"end\":21743,\"start\":21738},{\"end\":21757,\"start\":21753},{\"end\":21770,\"start\":21767},{\"end\":22142,\"start\":22139},{\"end\":22157,\"start\":22150},{\"end\":22638,\"start\":22631},{\"end\":22654,\"start\":22649},{\"end\":22665,\"start\":22662},{\"end\":22683,\"start\":22674},{\"end\":22982,\"start\":22978},{\"end\":22995,\"start\":22990},{\"end\":23013,\"start\":23004},{\"end\":23294,\"start\":23290},{\"end\":23305,\"start\":23303},{\"end\":23315,\"start\":23313},{\"end\":23330,\"start\":23325},{\"end\":23816,\"start\":23810},{\"end\":23832,\"start\":23827},{\"end\":23844,\"start\":23841},{\"end\":23864,\"start\":23855},{\"end\":24095,\"start\":24088},{\"end\":24108,\"start\":24102},{\"end\":24123,\"start\":24116},{\"end\":24135,\"start\":24130},{\"end\":24156,\"start\":24149},{\"end\":24170,\"start\":24165},{\"end\":24180,\"start\":24177},{\"end\":24193,\"start\":24182},{\"end\":24538,\"start\":24531},{\"end\":24558,\"start\":24548},{\"end\":24572,\"start\":24564},{\"end\":24588,\"start\":24579},{\"end\":24806,\"start\":24799},{\"end\":24822,\"start\":24816},{\"end\":24843,\"start\":24835},{\"end\":24853,\"start\":24849},{\"end\":24866,\"start\":24860},{\"end\":24875,\"start\":24868},{\"end\":24892,\"start\":24883},{\"end\":24906,\"start\":24894},{\"end\":25249,\"start\":25242},{\"end\":25262,\"start\":25257},{\"end\":25282,\"start\":25272},{\"end\":25624,\"start\":25615},{\"end\":25640,\"start\":25635},{\"end\":25658,\"start\":25651},{\"end\":25678,\"start\":25666},{\"end\":25693,\"start\":25685},{\"end\":25704,\"start\":25699},{\"end\":25719,\"start\":25713},{\"end\":26224,\"start\":26219},{\"end\":26238,\"start\":26235},{\"end\":26248,\"start\":26244},{\"end\":26261,\"start\":26256},{\"end\":26557,\"start\":26552},{\"end\":26571,\"start\":26568},{\"end\":26580,\"start\":26578},{\"end\":26588,\"start\":26585},{\"end\":26599,\"start\":26597},{\"end\":26608,\"start\":26605},{\"end\":26621,\"start\":26619},{\"end\":26936,\"start\":26924},{\"end\":26946,\"start\":26944},{\"end\":26961,\"start\":26954},{\"end\":27222,\"start\":27213},{\"end\":27238,\"start\":27231},{\"end\":27255,\"start\":27246},{\"end\":27269,\"start\":27265},{\"end\":27566,\"start\":27559},{\"end\":27578,\"start\":27574},{\"end\":27599,\"start\":27592},{\"end\":27613,\"start\":27609},{\"end\":27877,\"start\":27870},{\"end\":27891,\"start\":27884},{\"end\":27904,\"start\":27898},{\"end\":27921,\"start\":27912},{\"end\":27934,\"start\":27929},{\"end\":27949,\"start\":27944},{\"end\":27964,\"start\":27958},{\"end\":27982,\"start\":27972},{\"end\":28319,\"start\":28305},{\"end\":28336,\"start\":28330},{\"end\":28543,\"start\":28534},{\"end\":28557,\"start\":28551},{\"end\":28825,\"start\":28819},{\"end\":29055,\"start\":29047},{\"end\":29069,\"start\":29063},{\"end\":29073,\"start\":29071},{\"end\":29277,\"start\":29273},{\"end\":29289,\"start\":29284},{\"end\":29302,\"start\":29299},{\"end\":29314,\"start\":29311},{\"end\":29593,\"start\":29586},{\"end\":29609,\"start\":29604},{\"end\":29627,\"start\":29618},{\"end\":29875,\"start\":29871},{\"end\":29886,\"start\":29882},{\"end\":29897,\"start\":29893},{\"end\":30127,\"start\":30118},{\"end\":30140,\"start\":30136},{\"end\":30154,\"start\":30149},{\"end\":30173,\"start\":30164},{\"end\":30635,\"start\":30632},{\"end\":30648,\"start\":30644},{\"end\":30657,\"start\":30655},{\"end\":30944,\"start\":30939},{\"end\":30964,\"start\":30955},{\"end\":30980,\"start\":30973},{\"end\":31293,\"start\":31291},{\"end\":31302,\"start\":31300},{\"end\":31312,\"start\":31308},{\"end\":31321,\"start\":31318}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":146108083},\"end\":21552,\"start\":21095},{\"attributes\":{\"doi\":\"arXiv:1909.05330\",\"id\":\"b1\"},\"end\":22066,\"start\":21554},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":67856245},\"end\":22531,\"start\":22068},{\"attributes\":{\"id\":\"b3\"},\"end\":22567,\"start\":22533},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":208114131},\"end\":22899,\"start\":22569},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":202705983},\"end\":23185,\"start\":22901},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":145874546},\"end\":23720,\"start\":23187},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b7\"},\"end\":24084,\"start\":23722},{\"attributes\":{\"doi\":\"arXiv:1802.05365\",\"id\":\"b8\"},\"end\":24463,\"start\":24086},{\"attributes\":{\"id\":\"b9\"},\"end\":24726,\"start\":24465},{\"attributes\":{\"doi\":\"arXiv:1905.09272\",\"id\":\"b10\"},\"end\":25161,\"start\":24728},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":173990164},\"end\":25540,\"start\":25163},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":210920485},\"end\":26138,\"start\":25542},{\"attributes\":{\"doi\":\"arXiv:1904.03240\",\"id\":\"b13\"},\"end\":26462,\"start\":26140},{\"attributes\":{\"doi\":\"arXiv:1910.09932\",\"id\":\"b14\"},\"end\":26856,\"start\":26464},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b15\"},\"end\":27144,\"start\":26858},{\"attributes\":{\"doi\":\"arXiv:1904.05862\",\"id\":\"b16\"},\"end\":27470,\"start\":27146},{\"attributes\":{\"doi\":\"arXiv:2006.11477\",\"id\":\"b17\"},\"end\":27834,\"start\":27472},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13756489},\"end\":28265,\"start\":27836},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5855042},\"end\":28528,\"start\":28267},{\"attributes\":{\"doi\":\"arXiv:1606.08415\",\"id\":\"b20\"},\"end\":28735,\"start\":28530},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":116601104},\"end\":28999,\"start\":28737},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b22\"},\"end\":29219,\"start\":29001},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":9683805},\"end\":29522,\"start\":29221},{\"attributes\":{\"doi\":\"arXiv:1706.08612\",\"id\":\"b24\"},\"end\":29808,\"start\":29524},{\"attributes\":{\"doi\":\"abs/1806.00616\",\"id\":\"b25\",\"matched_paper_id\":46929624},\"end\":30046,\"start\":29810},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2191379},\"end\":30519,\"start\":30048},{\"attributes\":{\"doi\":\"arXiv:1804.05160\",\"id\":\"b27\"},\"end\":30875,\"start\":30521},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4407761},\"end\":31145,\"start\":30877},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":57966281},\"end\":31634,\"start\":31147}]", "bib_title": "[{\"end\":21169,\"start\":21095},{\"end\":22131,\"start\":22068},{\"end\":22623,\"start\":22569},{\"end\":22969,\"start\":22901},{\"end\":23283,\"start\":23187},{\"end\":25233,\"start\":25163},{\"end\":25607,\"start\":25542},{\"end\":27861,\"start\":27836},{\"end\":28295,\"start\":28267},{\"end\":28805,\"start\":28737},{\"end\":29266,\"start\":29221},{\"end\":29861,\"start\":29810},{\"end\":30109,\"start\":30048},{\"end\":30932,\"start\":30877},{\"end\":31282,\"start\":31147}]", "bib_author": "[{\"end\":21179,\"start\":21171},{\"end\":21193,\"start\":21179},{\"end\":21201,\"start\":21193},{\"end\":21648,\"start\":21633},{\"end\":21665,\"start\":21648},{\"end\":21681,\"start\":21665},{\"end\":21699,\"start\":21681},{\"end\":21720,\"start\":21699},{\"end\":21732,\"start\":21720},{\"end\":21745,\"start\":21732},{\"end\":21759,\"start\":21745},{\"end\":21772,\"start\":21759},{\"end\":22144,\"start\":22133},{\"end\":22159,\"start\":22144},{\"end\":22640,\"start\":22625},{\"end\":22656,\"start\":22640},{\"end\":22667,\"start\":22656},{\"end\":22685,\"start\":22667},{\"end\":22984,\"start\":22971},{\"end\":22997,\"start\":22984},{\"end\":23015,\"start\":22997},{\"end\":23296,\"start\":23285},{\"end\":23307,\"start\":23296},{\"end\":23317,\"start\":23307},{\"end\":23332,\"start\":23317},{\"end\":23818,\"start\":23804},{\"end\":23834,\"start\":23818},{\"end\":23846,\"start\":23834},{\"end\":23866,\"start\":23846},{\"end\":24097,\"start\":24086},{\"end\":24110,\"start\":24097},{\"end\":24125,\"start\":24110},{\"end\":24137,\"start\":24125},{\"end\":24158,\"start\":24137},{\"end\":24172,\"start\":24158},{\"end\":24182,\"start\":24172},{\"end\":24195,\"start\":24182},{\"end\":24540,\"start\":24526},{\"end\":24560,\"start\":24540},{\"end\":24574,\"start\":24560},{\"end\":24590,\"start\":24574},{\"end\":24808,\"start\":24797},{\"end\":24824,\"start\":24808},{\"end\":24845,\"start\":24824},{\"end\":24855,\"start\":24845},{\"end\":24868,\"start\":24855},{\"end\":24877,\"start\":24868},{\"end\":24894,\"start\":24877},{\"end\":24908,\"start\":24894},{\"end\":25251,\"start\":25235},{\"end\":25264,\"start\":25251},{\"end\":25284,\"start\":25264},{\"end\":25626,\"start\":25609},{\"end\":25642,\"start\":25626},{\"end\":25660,\"start\":25642},{\"end\":25680,\"start\":25660},{\"end\":25695,\"start\":25680},{\"end\":25706,\"start\":25695},{\"end\":25721,\"start\":25706},{\"end\":26226,\"start\":26213},{\"end\":26240,\"start\":26226},{\"end\":26250,\"start\":26240},{\"end\":26263,\"start\":26250},{\"end\":26559,\"start\":26544},{\"end\":26573,\"start\":26559},{\"end\":26582,\"start\":26573},{\"end\":26590,\"start\":26582},{\"end\":26601,\"start\":26590},{\"end\":26610,\"start\":26601},{\"end\":26623,\"start\":26610},{\"end\":26938,\"start\":26918},{\"end\":26948,\"start\":26938},{\"end\":26963,\"start\":26948},{\"end\":27224,\"start\":27205},{\"end\":27240,\"start\":27224},{\"end\":27257,\"start\":27240},{\"end\":27271,\"start\":27257},{\"end\":27568,\"start\":27552},{\"end\":27580,\"start\":27568},{\"end\":27601,\"start\":27580},{\"end\":27615,\"start\":27601},{\"end\":27879,\"start\":27863},{\"end\":27893,\"start\":27879},{\"end\":27906,\"start\":27893},{\"end\":27923,\"start\":27906},{\"end\":27936,\"start\":27923},{\"end\":27951,\"start\":27936},{\"end\":27966,\"start\":27951},{\"end\":27984,\"start\":27966},{\"end\":28321,\"start\":28297},{\"end\":28338,\"start\":28321},{\"end\":28545,\"start\":28530},{\"end\":28559,\"start\":28545},{\"end\":28827,\"start\":28807},{\"end\":29057,\"start\":29045},{\"end\":29071,\"start\":29057},{\"end\":29075,\"start\":29071},{\"end\":29279,\"start\":29268},{\"end\":29291,\"start\":29279},{\"end\":29304,\"start\":29291},{\"end\":29316,\"start\":29304},{\"end\":29595,\"start\":29580},{\"end\":29611,\"start\":29595},{\"end\":29629,\"start\":29611},{\"end\":29877,\"start\":29863},{\"end\":29888,\"start\":29877},{\"end\":29899,\"start\":29888},{\"end\":30129,\"start\":30111},{\"end\":30142,\"start\":30129},{\"end\":30156,\"start\":30142},{\"end\":30175,\"start\":30156},{\"end\":30637,\"start\":30623},{\"end\":30650,\"start\":30637},{\"end\":30659,\"start\":30650},{\"end\":30946,\"start\":30934},{\"end\":30966,\"start\":30946},{\"end\":30982,\"start\":30966},{\"end\":31295,\"start\":31284},{\"end\":31304,\"start\":31295},{\"end\":31314,\"start\":31304},{\"end\":31323,\"start\":31314}]", "bib_venue": "[{\"end\":21299,\"start\":21201},{\"end\":21631,\"start\":21554},{\"end\":22248,\"start\":22159},{\"end\":22539,\"start\":22535},{\"end\":22711,\"start\":22685},{\"end\":23026,\"start\":23015},{\"end\":23430,\"start\":23332},{\"end\":23802,\"start\":23722},{\"end\":24251,\"start\":24211},{\"end\":24524,\"start\":24465},{\"end\":24795,\"start\":24728},{\"end\":25333,\"start\":25284},{\"end\":25810,\"start\":25721},{\"end\":26211,\"start\":26140},{\"end\":26542,\"start\":26464},{\"end\":26916,\"start\":26858},{\"end\":27203,\"start\":27146},{\"end\":27550,\"start\":27472},{\"end\":28033,\"start\":27984},{\"end\":28374,\"start\":28338},{\"end\":28610,\"start\":28575},{\"end\":28857,\"start\":28827},{\"end\":29043,\"start\":29001},{\"end\":29346,\"start\":29316},{\"end\":29578,\"start\":29524},{\"end\":29917,\"start\":29913},{\"end\":30261,\"start\":30175},{\"end\":30621,\"start\":30521},{\"end\":30986,\"start\":30982},{\"end\":31364,\"start\":31323}]"}}}, "year": 2023, "month": 12, "day": 17}