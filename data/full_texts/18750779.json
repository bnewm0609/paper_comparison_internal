{"id": 18750779, "updated": "2023-09-28 14:18:43.313", "metadata": {"title": "End-to-End Task-Completion Neural Dialogue Systems", "authors": "[{\"first\":\"Xiujun\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yun-Nung\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Lihong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jianfeng\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Asli\",\"last\":\"Celikyilmaz\",\"middle\":[]}]", "venue": "IJCNLP", "journal": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)", "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "One of the major drawbacks of modularized task-completion dialogue systems is that each module is trained individually, which presents several challenges. For example, downstream modules are affected by earlier modules, and the performance of the entire system is not robust to the accumulated errors. This paper presents a novel end-to-end learning framework for task-completion dialogue systems to tackle such issues.Our neural dialogue system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. The reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of the dialogue system. Our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines for both objective and subjective evaluation, but also is robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to the language understanding module.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1703.01008", "mag": "2949252816", "acl": "I17-1074", "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/LiCLG17", "doi": null}}, "content": {"source": {"pdf_hash": "e9dac477b3c78affd4ee71c7049cd55ca2465967", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/I17-1074.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "93d903da4d0832a214f9c56b320bf36585e608bf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e9dac477b3c78affd4ee71c7049cd55ca2465967.txt", "contents": "\nEnd-to-End Task-Completion Neural Dialogue Systems\nNovember 27 -December 1, 2017 c 2017\n\nXiujun Li \nMicrosoft Research\nRedmondWAUSA\n\nNational Taiwan University\nTaipeiTaiwan\n\nYun-Nung Chen y.v.chen@ieee.org\u2020xiul \nLihong Li \nMicrosoft Research\nRedmondWAUSA\n\nNational Taiwan University\nTaipeiTaiwan\n\nJianfeng Gao \nMicrosoft Research\nRedmondWAUSA\n\nNational Taiwan University\nTaipeiTaiwan\n\nAsli Celikyilmaz \nMicrosoft Research\nRedmondWAUSA\n\nNational Taiwan University\nTaipeiTaiwan\n\nEnd-to-End Task-Completion Neural Dialogue Systems\n\nProceedings of the The 8th International Joint Conference on Natural Language Processing\nthe The 8th International Joint Conference on Natural Language ProcessingTaipei, TaiwanNovember 27 -December 1, 2017 c 2017\nOne of the major drawbacks of modularized task-completion dialogue systems is that each module is trained individually, which presents several challenges. For example, downstream modules are affected by earlier modules, and the performance of the entire system is not robust to the accumulated errors. This paper presents a novel end-to-end learning framework for task-completion dialogue systems to tackle such issues. Our neural dialogue system can directly interact with a structured database to assist users in accessing information and accomplishing certain tasks. The reinforcement learning based dialogue manager offers robust capabilities to handle noises caused by other components of the dialogue system. Our experiments in a movie-ticket booking domain show that our end-to-end system not only outperforms modularized dialogue system baselines for both objective and subjective evaluation, but also is robust to noises as demonstrated by several systematic experiments with different error granularity and rates specific to the language understanding module 1 .\n\nIntroduction\n\nIn the past decade, goal-oriented dialogue systems have been the most prominent component in today's virtual personal assistants, which allow users to speak naturally in order to accomplish tasks more efficiently. Traditional systems have a rather complex and modularized pipeline, consisting of a language understanding (LU) module, a dialogue manager (DM), and a natural language generation (NLG) component (Rudnicky et al., 1999;Zue et al., 2000;Zue and Glass, 2000).\n\nRecent advances of deep learning have inspired many applications of neural models to dialogue systems. Wen et al. (2017) and Bordes et al. (2017) introduced a network-based end-to-end trainable task-oriented dialogue system, which treated dialogue system learning as the problem of learning a mapping from dialogue histories to system responses, and applied an encoder-decoder model to train the whole system. However, the system is trained in a supervised fashion: not only does it require a lot of training data, but it may also fail to find a good policy robustly due to lack of exploration of dialogue control in the training data. Zhao and Eskenazi (2016) first presented an end-to-end reinforcement learning (RL) approach to dialogue state tracking and policy learning in the DM. This approach is shown to be promising when applied to the task-oriented dialogue problem of guessing the famous person a user thinks of. In the conversation, the agent asks the user a series of Yes/No questions to find the correct answer. However, this simplified task may not generalize to practical problems due to the following:\n\n1. Inflexible question types -asking request questions is more natural and efficient than Yes/No questions. For example, it is more natural and efficient for the system to ask \"Where are you located?\" instead of \"Are you located in Palo Alto?\", when there are a large number of possible values for the location slot. 2. Poor robustness -the user answers are too simple to be misunderstood, so the system lacks the robustness against noise in real user utterances. 3. User requests during dialogues -in a taskoriented dialogue, user may ask questions for \n\n\nError Model Controller\n\nBackend Database Figure 1: Illustration of the end-to-end neural dialogue system: given user utterances, reinforcement learning is used to train all components in an end-to-end fashion.\n\nselecting the preferred slot values. In a flightbooking example, user might ask \"What flight is available tomorrow?\".\n\nFor the second issue, Su et al. (2016) briefly investigated the effect of dialogue action level semantic error rates on the dialogue performance. Lemon and Liu (2007) compared policy transfer properties under different environments, showing that policies trained in high-noise conditions have better transfer properties than those trained in lownoise conditions. Recently, Dhingra et al. (2017) proposed an end-to-end differentiable KB-Infobot to provide the solutions to the first two issues, but the last one remained unsolved.\n\nThis paper addresses all three issues above by redefining the targeted system as a taskcompletion neural dialogue system. Our framework is more practical in that the information can be easily accessed by the user during the conversations, while the final goal of the system is to complete a task, such as movie-ticket booking. This paper is the first attempt of training a realworld task-completion dialogue system in an endto-end fashion by leveraging supervised learning and reinforcement learning techniques. To further understand the robustness of reinforcement learning based dialogue systems, we conduct extensive experiments and quantitative analysis on a fine-grained level of LU errors, and provide meaningful insights on how the language understanding component impacts the overall performance of the dialogue system.\n\nOur contributions are three-fold:\n\n\u2022 Robustness -We propose a neural dialogue system with greater robustness by automatically selecting actions based on uncertainty and confusion by reinforcement learning. We also provide the first systematic analysis to investigate the impact of different types of natural language understanding errors on dialogue system performance. We show that slot-level errors have a greater impact on the system performance than intent-level ones, and that slot value replacement degrades the performance most. Our findings shed some light on how to design multi-task natural language understanding models (intent classification and slot labeling) in the dialogue systems with consideration of error control. \u2022 Flexibility -The system is the first neural dialogue system that allows user-initiated behaviors during conversations, where the users can interact with the system with higher flexibility that is important in realistic scenarios. \u2022 Reproducibility -We demonstrate how to evaluate RL dialogue agents using crowdsourced task-specific datasets and simulated users in an end-to-end fashion, guaranteeing reproducibility and consistent comparisons of competing methods in an identical setting.  Figure 2: An example utterance with annotations of semantic slots in IOB format (S) and intent (I), B-date and I-date denote the date slot.\n\n\nProposed Framework\n\nThe proposed framework 2 is illustrated in Figure 1. It includes a user simulator (left part) and a neural dialogue system (right part). In the user simulator, an agenda-based user modeling component based at the dialogue act level is applied to control the conversation exchange conditioned on the generated user goal, to ensure the user behaves in a consistent, goal-oriented manner. An NLG module is used to generate natural language texts corresponding to the user dialogue actions. In a neural dialogue system, an input sentence (recognized utterance or text input) passes through an LU module and becomes a corresponding semantic frame, and an DM, which includes a state tracker and policy learner, is to accumulate the semantics from each utterance, robustly track the dialogue states during the conversation, and generate the next system action.\n\n\nNeural Dialogue System\n\nLanguage Understanding (LU): A major task of LU is to automatically classify the domain of a user query along with domain specific intents and fill in a set of slots to form a semantic frame. The popular IOB (in-out-begin) format is used for representing the slot tags, as shown in Figure 2.\n\nx = w 1 , ..., w n , <EOS> y = s 1 , ..., s n , i m where x is the input word sequence and y contains the associated slots, s k , and the sentence-level intent i m . The LU component is implemented with a single LSTM, which performs intent prediction and slot filling simultaneously (Hakkani-T\u00fcr et al., 2016;:\ny = LSTM( x) .(1)\nThe LU objective is to maximize the conditional probability of the slots and the intent y given the 2 The source code is available at: https://github. com/MiuLab/TC-Bot word sequence x:\np( y | x) = n i p(s i | w 1 , . . . , w i ) p(i m | y).\nThe weights of the LSTM model are trained using backpropagation to maximize the conditional likelihood of the training set labels. The predicted tag set is a concatenated set of IOB-format slot tags and intent tags; therefore, this model can be trained using all available dialogue actions and utterance pairs in our labeled dataset in a supervised manner.\n\nDialogue Management (DM): The symbolic LU output is passed to the DM in the dialogue act form (or semantic frame). The classic DM includes two stages, dialogue state tracking and policy learning.\n\n\u2022 Dialogue state tracking: Given the LU symbolic output, such as request(moviename; genre=action; date=this weekend), three major functions are performed by the state tracker: a symbolic query is formed to interact with the database to retrieve the available results; the state tracker will be updated based on the available results from the database and the latest user dialogue action; and the state tracker will prepare the state representation s t for policy learning. \u2022 Policy learning: The state representation for the policy learning includes the latest user action (e.g., request(moviename; genre=action; date=this weekend)), the latest agent action (request(location)), the available database results, turn information, and history dialogue turns, etc. Conditioned on the state representation s t from the state tracker, the policy \u03c0 is to generate the next available system action a t according to \u03c0(s t ).\n\nEither supervised learning or reinforcement learning can be used to optimize \u03c0. Details about RL-based policy learning can be found in section 3. Prior work used different implementation approaches summarized below. Dialogue state tracking is the process of constantly updating the state of the dialogue, and Lee (2014) showed that there is a positive correlation between state tracking performance and dialogue performance. Most production systems use manually designed heuristics, often based on rules, to update the dialogue states based on the highly confident output from LU. Williams et al. (2013) formalized the tracking problem as a supervised sequence labeling task, where the input is LU outputs and the output is the true slot values, and the state tracker's results can be translated into a dialogue policy. Zhao and Eskenazi (2016) proposed to jointly train the state tracker and the policy learner in order to optimize the system actions more robustly. Instead of explicitly incorporating the state tracking labels, this paper learns the system actions with implicit dialogue states, so that the proposed DM can be more flexible and robust to the noise propagated from the previous components (Su et al., 2016;Liu and Lane, 2017). A rule-based agent is employed to warm-start the system, via supervised learning on labels generated by the rules. The system is then further trained end-to-end with RL, as explained in section 3.\n\n\nUser Simulation\n\nIn order to perform end-to-end training for the proposed neural dialogue systems, a user simulator is required to automatically and naturally interact with the dialogue system. In the task-completion dialogue setting, the user simulator first generates a user goal. The agent does not know the user goal, but tries to help the user accomplish it in the course of conversations. Hence, the entire conversation exchange is around this goal implicitly. A user goal generally consists of two parts: inform slots for slot-value pairs that serve as constraints from the user, and request slots for slots whose value the user has no information about, but wants to get the values from the agent during the conversation. The user goals are generated using a set of labeled conversational data.\n\nUser Agenda Modeling: During the course of a dialogue, the user simulator maintains a compact, stack-like representation called user agenda (Schatzmann and Young, 2009), where the user state s u is factored into an agenda A and a goal G. The goal consists of constraints C and request R. At each time-step t, the user simulator generates the next user action a u,t based on the current state s u,t and the last agent action a m,t\u22121 , and then updates the current status s u,t .\n\nNatural Language Generation (NLG): Given the user's dialogue actions, the NLG module generates natural language texts. To control the quality of user simulation given limited labeled data, a hybrid approach including a template-based NLG and a model-based NLG is employed, where the model-based NLG is trained on the labeled dataset with a sequence-to-sequence model. It takes dialogue acts as input, and generates sentence sketch with slot placeholders via an LSTM decoder. Then a post-processing scan is performed to replace the slot placeholders with their actual values (Wen et al., 2015). In the LSTM decoder, we apply beam search, which iteratively considers the top k best sub-sentences when generating the next token.\n\nIn the hybrid model, if the user dialogue actions can be found in the predefined sentence templates, the template-based NLG is applied; otherwise, the utterance is generated by the model-based NLG. This hybrid approach allows a dialogue system developer to easily improve NLG by providing templates for sentences that the machine-learned model does not handle well.\n\n\nError Model Controller\n\nWhen training or testing a policy based on semantic frames of user actions, an error model (Schatzmann et al., 2007) is introduced to simulate noises from the LU component, and noisy communication between the user and the agent in order to test the model robustness. Here, we introduce different levels of noises in the error model: one type of errors is at the intent level, another is at the slot level. For each level, there are more fine-grained noises.\n\nIntent-Level Error: At the intent level, we categorize all intents into three groups:\n\n\u2022 Group 1: general greeting, thanks, closing, etc. \u2022 Group 2: users may inform, to tell the slot values (or constraints) to the agent, for example, inform(moviename='Titanic', start-time='7pm'). \u2022 Group 3: users may request information for specific slots. In a movie-booking scenario, users might ask \"request(starttime; moviename='Titanic')\". In the specific task of movie-booking, for instance, there exist multiple inform and request intents, such as request starttime, request moviename, inform starttime and inform moviename, etc. Based on the above intent categories, there are three types of intent errors:\n\n\u2022 Random error (I0): the random noisy intent from the same category (within group error) or other categories (between group error). \u2022 Within-group error (I1): the noisy intent is from the same group of the real intent, for example, the real intent is request theater, but the predicted intent from LU module might be request moviename. \u2022 Between-group error (I2): the noisy intent is from the different group, for example, a real intent request moviename might be predicted as the intent inform moviename.\n\nSlot-level Error: At the slot level, there are four error types:\n\n\u2022 Random error (S0): to simulate the noise that is randomly set to the following three types. \u2022 Slot deletion (S1): is to simulate the scenario where the slot is not recognized by the LU component. \u2022 Incorrect slot value (S2): is to simulate the scenario where the slot name is correctly recognized, but the slot value is wrong, e.g., wrong word segmentation. \u2022 Incorrect slot (S3): is to simulate the scenario where both the slot and its value are incorrectly recognized.\n\n\nEnd-to-End Reinforcement Learning\n\nTo learn the interactive policy of our system, we apply reinforcement learning to the DM training in an end-to-end fashion, where each neural network component can be fine tuned. The policy is represented as a deep Q-network (DQN) (Mnih et al., 2015), which takes the state s t from the state tracker as input, and outputs Q(s t , a; \u03b8) for all actions a. Two important DQN tricks, target network usage and experience replay are applied, where the experience replay strategy is changed for the dialogue setting. During training, we use -greedy exploration and an experience replay buffer with dynamically changing buffer size. At each simulation epoch, we simulate N (N = 100) dialogues and add these state transition tuples (s t , a t , r t , s t+1 ) to the experience replay buffer for training. In one simulation epoch, the current DQN will be updated multiple times (depending on the batch size and the current size of experience replay buffer). At the last simulation epoch, the target network will be replaced by the current DQN, the target DQN network is only updated for once in one simulation epoch.\n\nThe experience replay strategy is critical for RL training (Schaul et al., 2015). In our buffer update strategy, we accumulate all experience tuples from the simulation and flush the pool till the current RL agent reaches a success rate threshold (i.e., a threshold which is equal to the performance of a rule-based agent), and then use the experience tuples from the current RL agent to re-fill the buffer. The intuition is that the initial performance of the DQN is not strong enough to generate good experience replay tuples, thus we do not flush the experience replay pool till the current RL agent can reach a certain success rate (for example, the success rate of a rule-based agent). In the rest of the training process, at every simulation epoch, we estimate the success rate of the current DQN agent (by running it multiple dialogues on simulated users). If the current DQN agent is better than the target network, the experience replay buffer will be flushed.\n\n\nExperiments\n\nWe consider a task-completion dialogue system for helping users book movie tickets. Over the course of conversation, the dialogue system gathers information about the customer's desires and ultimately books the movie tickets. The environment then assesses a binary outcome (success or failure) at the end of the conversation, based on (1) whether a movie is booked, and (2) whether the movie satisfies the users constraints.\n\nDataset: The raw conversational data were collected via Amazon Mechanical Turk, with annotations provided by domain experts. In total, we have labeled 280 dialogues, and the average number of turns per dialogue is approximately 11. The annotated data includes 11 dialogue acts and 29 slots, most of the slots are informable slots, which users can use to constrain the search, and some are requestable slots, of which users can ask values from the agent. For example, numberofpeople cannot be a requestable slot, since arguably user knows how many tickets he or she wants to buy. The detailed annotations can be found in Appendix A.  actions, a noise controller described in section 2.3 is used to simulate LU errors and noisy communications between the user and the agent. 2. natural language: when training or testing a policy on natural language level, in which LU and NLG may introduce noises. In our experiments, the NLG decoder uses beam size = 3 to balance speed and performance. Figure 3(a) shows a learning curve for the dialogue system performance trained with the framelevel information (user semantic frames and system actions), where the number is the average of 10 runs. Figure 3(b) is a learning curve for the system trained at the natural language level. In  both settings, the RL agents significantly outperform the rule-based systems, showing the potential of a neural dialogue system that can perform real-world tasks and be improved autonomously through interactions with users. Also, the end-toend system in Figure 3(b) takes longer for the RL agent to adapt to the noises from LU and NLG, indicating the difficulty of maintaining the system robustness. The consistently increasing trend of our proposed end-to-end system also suggests greater robustness in noisy, real-world scenarios. To further investigate and understand the real impact of the LU component to the robustness of RL agent in the dialogue system, we conduct a series of experiments under different error settings (intent and slot errors from LU) summarized in Table 1, where the learning curves are averaged over 10 runs. \n\n\nSimulated User Evaluation\n\n\nBasic Error Analysis\n\nThe group of basic experiments (from B1 to B3) are in the settings that combine the noise from both intent and slot: 1) For both intent and slot, the error types are random, and the error rates are in {0.00, 0.10, 0.20}. The rule-based agent reports 41%, 21%, and 12% success rates under 0.00, 0.10, and 0.20 error rates respectively. In contrast, the RL-based agent achieves 90%, 79%, and 76% success rate under the same error rates, respectively. We compare the performance between two types of agents and find that the RL-based agent has greater robustness and is less sensitive to noisy inputs. Therefore, the following experiments are performed using a RL dialogue agent due to robustness consideration. From Fig. 4, the dialogue agents degrade remarkably when the error rate increases (leading to lower success rates and higher average turns).\n\n\nIntent Error Analysis\n\nTo further understand the impact of intent-level noises to dialogue systems, two experimental groups are performed: the first group (I0-I2) focuses on the difference among all intent error types; the second group (I3-I5) focuses on the impact of intent error rates. Other factors are identical for the two groups, with the random slot error type and a 5% slot error rate.\n\n\nIntent Error Type\n\nExperiments with the settings of I0-I2 are under the same slot errors and same intent error rate (10%), but with different intent error types: I1 includes the noisy intents from the same categories, I2 includes the noisy intents from different categories, and I0 includes both via random selection. Fig. 5(a) shows the learning curves for all intent error types, where the difference among three curves is insignificant, indicating that the incorrect intents have similar impact no matter what categories they belong to.\n\n\nIntent Error Rate\n\nExperiments with the settings I3-I5 investigate the difference among different intent error rates. When the intent error rate increases, the dialogue agent performs slightly worse, but the difference is subtle. It suggests that the RL-based agent has better robustness to noisy intents. As shown in Fig. 5(a,b), all RL agents can converge to a similar success rate in both intent error type and intent error rate settings.\n\n\nSlot Error Analysis\n\nWe further conducted two groups of experiments to investigate the impact of slot-level noises where other factors are fixed -with the random intent error type and a 10% intent error rate.\n\n\nSlot Error Type\n\nExperiments (S0 -S3) investigate the impact of different slot error types. Corresponding learning curves are given in Fig. 5(c). Among single error types (S1-S3), incorrect slot value (S2) performs worst, which means that the slot name is recognized correctly, but a wrong value is extracted with the slot (such as wrong word segmentation); in this case, the agent receives a wrong value for the slot, and eventually books a wrong ticket or fails to book it. The probable reason is that the dialogue agent has difficulty identifying the mistakes, and using the incorrect slot values for the following dialogue actions could significantly degrade the performance. Between slot deletion (S1) and incorrect slot (S3), the difference is limited, indicating that the RL agent has similar capability of handling these two kinds of slot-level noises.\n\n\nSlot Error Rate\n\nExperiments with the settings from S4 to S6 focus on different slot error rates (0%, 10%, and 20%) and report the results in Fig. 5(d). It is clear from Fig. 5(d) that the dialogue agent performs worse as the slot error rate increases (the curve of the success rate drops and the curve of average turns rises). Comparing with Fig. 5(b), the dialogue system performance is more sensitive to the slot error rate than the intent error rate.\n\n\nHuman Evaluation\n\nWe further evaluated the rule-based and DQN agents against real human users recruited from the authors affiliation, where the DQN agent was trained on the simulated user in the frame-level with 5% random slot errors. In each dialogue session, one of the agents was randomly picked to converse with a user, and the user was presented with a predefined user goal sampled from our corpus, and was instructed to converse with the agent to complete the presented task. At the end of each session, the user was asked to give a rating on a scale from 1 (worst) to 5 (best) based on both naturalness and coherence of the dialogue. We collected a total of 110 dialogue sessions from 8 human users. Figure 6(a) presents the performance of these agents against real users in terms of success rate. Figure 6(b) shows the subjective evaluation in terms of user rating. For all the cases, the RL agent significantly outperforms the rule-based agent for both objective (success rate) and subjective evaluation (user rating).\n\n\nDiscussion and Future Work\n\nThis paper presents an end-to-end learning framework for task-completion neural dialogue systems. Our experiments, both on simulated and real users, show that reinforcement learning systems outperform rule-based agents and have better robustness to allow natural interactions with users in realworld task-completion scenarios. Furthermore, we conduct a series of extensive experiments to understand the impact of natural language understanding errors on the performance of a reinforcement learning based, task-completion neural dialogue system. Our empirical results suggest several interesting findings: 1) slot-level errors have a greater impact than intent-level errors; A possible explanation is related to our dialogue action rep-resentation, intent(slot-value pairs). If an intent is predicted wrong, for example, inform was predicted incorrectly as request ticket, the dialogue agent can handle this unreliable situation and decide to make confirmation in order to keep the correct information for the following conversation. In contrast, if a slot moviename is predicted wrong, or a slot value is not identified correctly, this dialogue turn might directly pass the wrong information to the agent, which might lead the agent to book a wrong ticket. Another reason is that the dialogue agent can still maintain a correct intent based on slot information even though the predicted intent is wrong. In order to verify the hypotheses, further experiments are needed, which we leave as future work. 2) different slot error types have different impacts on the RL agents.\n\n3) RL agents are more robust to certain types of slot-level errors -the agents can learn to doublecheck or confirm with users, at the cost of slightly longer conversations. Finally, it should be noted that the experiments in this paper focus on task-completion dialogues. Another type of dialogues known as chit-chats has different optimization goals . It would be interesting to extend our analysis from this paper to chit-chat dialogues to gain useful insights for impacts of LU errors.    Table 3 shows one success and one failure dialogue examples generated by the rule-based agent and RL agent interacting with user simulator in the movie-booking domain. To be informative, we also explicitly show the user goal at the head of the dialogue, the purpose is to help the user to accomplish this goal and book the right movie tickets, but the agent knows nothing about the user goal.\n\n\nA Dataset Annotation\n\n\nB Sample Dialogues\n\nFigure 4 :\n4Learning curves for different LU error rates.\n\nFigure 5 :\n5Learning curves of the different intent and slot errors in terms of success rate (left axis) and average turns (right axis).\n\nFigure 6 :\n6Performance of the rule-based agent versus DQN agent tested with real users: (a) success rate, number of tested dialogues and p-values are indicated on each bar; (b) Distribution of user ratings for two agents (difference in mean is significant with p < 0.01).\n\n\nTwo sets of experiments are conducted in the DM training, where two input formats are used for training the RL agents:1. frame-level semantics: when training or testing a policy based on semantic frames of user6LPXODWLRQ(SRFK \n\n6XFFHVV5DWH \n\n5/$JHQW \n5XOH$JHQW \n8SSHU%RXQG \n\n(a) Frame-level semantics for training \n\n\n\n\n6LPXODWLRQ(SRFK \n\n6XFFHVV5DWH \n\n5/$JHQW \n5XOH$JHQW \n8SSHU%RXQG \n\n(b) Natural language for end-to-end training \n\nFigure 3: Learning curves for policy training (average of 10 runs). The blue solid lines show the rule \nagent performance, where we employ to initialize the experience replay buffer pool; the orange dotted \nline is the optimal upper bound, which is the percentage of reachable user goals. \n\nSetting \nIntent Error \nSlot Error \nType \nRate Type \nRate \n\nBasic \nB1 \n0: random \n\n0.00 \n0: random \n\n0.00 \nB2 \n0.10 \n0.10 \nB3 \n0.20 \n0.20 \n\nIntent \n\nI0 \n0: random \n0.10 \n\n0: random \n0.05 \n\nI1 \n1: within group \n0.10 \nI2 \n2: between group 0.10 \nI3 0: random \n0.00 \nI4 0: random \n0.10 \nI5 0: random \n0.20 \n\nSlot \n\nS0 \n\n0: random \n0.10 \n\n0: random 0.10 \nS1 \n1: deletion 0.10 \nS2 \n2: value \n0.10 \nS3 \n3: slot \n0.10 \nS4 \n0: random \n0.00 \nS5 \n0: random \n0.10 \nS6 \n0: random \n0.20 \n\n\n\nTable 1 :\n1Experimental settings with different intent/slot error types described in section 2.3 and different error rates.\n\n\nTsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Lina M Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. 2017. A networkbased end-to-end trainable task-oriented dialogue system. pages 438-449. Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, and Steve Young. 2015. Semantically conditioned LSTM-based natural language generation for spoken dialogue systems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1711-1721. Jason Williams, Antoine Raux, Deepak Ramachandran, and Alan Black. 2013. The dialog state tracking challenge. In Proceedings of the SIGDIAL 2013 Conference. pages 404-413. Tiancheng Zhao and Maxine Eskenazi. 2016. Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pages 1-10. Victor Zue, Stephanie Seneff, James R Glass, Joseph Polifroni, Christine Pao, Timothy J Hazen, and Lee Hetherington. 2000. JUPITER: a telephonebased conversational interface for weather information. IEEE Transactions on speech and audio processing 8(1):85-96. Victor W Zue and James R Glass. 2000. Conversational interfaces: Advances and challenges. Proceedings of the IEEE 88(8):1166-1180.\n\nTable 2\n2lists all annotated dialogue acts and slots in detail. request, inform, deny, confirm question, Intent confirm answer, greeting, closing, not sure, multiple choice, thanks, welcome Slot actor, actress, city, closing, critic rating, date, description, distanceconstraints, greeting, implicit value, movie series, moviename, mpaa rating, numberofpeople, numberofkids, taskcomplete, other, price, seating, starttime, state, theater, theater chain, video format, zip, result, ticket, mc listAnnotations \n\n\nTable 2 :\n2The data annotation.\nThe source code is available at: https://github/ com/MiuLab/TC-Bot.\nAcknowledgmentsWe would like to thank Dilek Hakkani-T\u00fcr and reviewers for their insightful comments on the paper. Yun-Nung Chen is supported by the Ministry of Science and Technology of Taiwan and Medi-aTek Inc..User: Could you help me to book the tickets? Agent: Great -I was able to purchase 2 tickets for you to see zoolander 2 tomorrow at regal meridian 16 theater in seattle at 9:25 pm. Agent: Okay -I was able to book 3 tickets for you to see 10 cloverfield lane at regal la live stadium 14 at 11:45am tomorrow. User: Thank you.User:Thank you. Agent: Thank you.Agent: Thank you. Success SuccessTable 3: Two sample dialogues generated by rule-based and RL agents with user simulator: Left column shows both rule and RL agents succeed; Right column shows that rule-based agent fails, while RL agent succeeds.\nLearning end-to-end goal-oriented dialog. Antoine Bordes, Y-Lan Boureau, Jason Weston, Proceedings of ICLR. ICLRAntoine Bordes, Y-Lan Boureau, and Jason Weston. 2017. Learning end-to-end goal-oriented dialog. In Proceedings of ICLR.\n\nSyntax or semantics? knowledge-guided joint semantic frame parsing. Yun-Nung Chen, Dilek Hakanni-T\u00fcr, Gokhan Tur, Asli Celikyilmaz, Jianfeng Gao, Li Deng, Proceedings of the 6th IEEE Workshop on Spoken Language Technology. the 6th IEEE Workshop on Spoken Language TechnologyYun-Nung Chen, Dilek Hakanni-T\u00fcr, Gokhan Tur, Asli Celikyilmaz, Jianfeng Gao, and Li Deng. 2016. Syntax or semantics? knowledge-guided joint se- mantic frame parsing. In Proceedings of the 6th IEEE Workshop on Spoken Language Technology. pages 348-355.\n\nTowards end-to-end reinforcement learning of dialogue agents for information access. Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen, Faisal Ahmed, Li Deng, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsLong Papers1Bhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao, Yun-Nung Chen, Faisal Ahmed, and Li Deng. 2017. Towards end-to-end reinforcement learning of dia- logue agents for information access. In Proceed- ings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers). pages 484-495.\n\nMulti-domain joint semantic frame parsing using bi-directional rnn-lstm. Dilek Hakkani-T\u00fcr, Gokhan Tur, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, Ye-Yi Wang, Proceedings of Interspeech. InterspeechDilek Hakkani-T\u00fcr, Gokhan Tur, Asli Celikyilmaz, Yun-Nung Chen, Jianfeng Gao, Li Deng, and Ye- Yi Wang. 2016. Multi-domain joint semantic frame parsing using bi-directional rnn-lstm. In Proceed- ings of Interspeech. pages 715-719.\n\nExtrinsic evaluation of dialog state tracking and predictive metrics for dialog policy optimization. Sungjin Lee, 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue. 310Sungjin Lee. 2014. Extrinsic evaluation of dialog state tracking and predictive metrics for dialog policy op- timization. In 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue. page 310.\n\nDialogue policy learning for combinations of noise and user simulation: transfer results. Oliver Lemon, Xingkun Liu, Proc. SIGdial. SIGdialOliver Lemon and Xingkun Liu. 2007. Dialogue policy learning for combinations of noise and user simula- tion: transfer results. In Proc. SIGdial.\n\nDeep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky, Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016. Deep rein- forcement learning for dialogue generation .\n\nAn end-to-end trainable neural network model with belief tracking for task-oriented dialog. Bing Liu, Ian Lane, Proceedings of Interspeech. InterspeechBing Liu and Ian Lane. 2017. An end-to-end train- able neural network model with belief tracking for task-oriented dialog. In Proceedings of Interspeech. pages 2506-2510.\n\nHuman-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Nature. 518Shane Legg, and Demis HassabisVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidje- land, Georg Ostrovski, Stig Petersen, Charles Beat- tie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level con- trol through deep reinforcement learning. Nature 518:529-533.\n\nCreating natural dialogs in the carnegie mellon communicator system. Eric H Alexander I Rudnicky, Thayer, C Paul, Chris Constantinides, Tchou, Kevin A Shern, Wei Lenzo, Alice Xu, Oh, In EurospeechAlexander I Rudnicky, Eric H Thayer, Paul C Constan- tinides, Chris Tchou, R Shern, Kevin A Lenzo, Wei Xu, and Alice Oh. 1999. Creating natural dialogs in the carnegie mellon communicator system. In Eu- rospeech.\n\nError simulation for training statistical dialogue systems. Jost Schatzmann, Blaise Thomson, Steve Young, IEEE Workshop on Automatic Speech Recognition & Understanding. Jost Schatzmann, Blaise Thomson, and Steve Young. 2007. Error simulation for training statistical di- alogue systems. In IEEE Workshop on Automatic Speech Recognition & Understanding.\n\nThe hidden agenda user simulation model. IEEE transactions on audio, speech, and language processing. Jost Schatzmann, Steve Young, 17Jost Schatzmann and Steve Young. 2009. The hid- den agenda user simulation model. IEEE trans- actions on audio, speech, and language processing 17(4):733-747.\n\nTom Schaul, John Quan, Ioannis Antonoglou, David Silver, arXiv:1511.05952Prioritized experience replay. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. 2015. Prioritized experience replay. arXiv:1511.05952 .\n\n10 cloverfield lane\" } } Rule-based agent Rule-based Agent User: Can I get some tickets for zoolander 2? User: What is the start time for 10 cloverfield lane? Agent: What movie are you interested in? Agent: What movie are you interested in? User: I want to watch zoolander 2. User: I want to watch at 10 cloverfield lane. Agent: What time would you like to see it? Agent: What time would you like to see it? User: I want to watch at 9:25 pm. User: What time is it playing tomorrow? I need 3 tickets. Agent: What city you would like? Agent: What city you would like? User: I want to watch at seattle. User: I do not care. Agent: What date would you like to watch it? Agent: What date would you like to watch it? User: I want to set it up tomorrow. User: I want to set it up tomorrow. Agent: Which theater would you like? Agent: Which theater would you like? User: I want to watch at regal meridian 16. User: Which theater is available? Agent: How many tickets do you need? Agent: How many tickets do you need? User: I want 2 tickets please! User: I. Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young, arXiv:1606.02689Continuously learning neural dialogue management. New episode, user goal: { \"request slots. want 3 tickets please! Agent: Great -I was able to purchase 2 tickets for you to see zoolander 2 tomorrow at regal meridian 16 theater in seattle at 9:25 pmPei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas- Barahona, Stefan Ultes, David Vandyke, Tsung- Hsien Wen, and Steve Young. 2016. Con- tinuously learning neural dialogue management. arXiv:1606.02689 . New episode, user goal: { \"request slots\": { \"ticket\": \"UNK\" }, \"inform slots\": { \"city\": \"seattle\", \"numberofpeople\": \"2\", \"theater\": \"regal meridian 16\", \"starttime\": \"9:25 pm\", \"date\": \"tomorrow\", \"moviename\": \"zoolander 2\" } } New episode, user goal: { \"request slots\": { \"ticket\": \"UNK\" \"theater\": \"UNK\" \"starttime\": \"UNK\" }, \"inform slots\": { \"numberofpeople\": \"3\", \"date\": \"tomorrow\", \"moviename\": \"10 cloverfield lane\" } } Rule-based agent Rule-based Agent User: Can I get some tickets for zoolander 2? User: What is the start time for 10 cloverfield lane? Agent: What movie are you interested in? Agent: What movie are you interested in? User: I want to watch zoolander 2. User: I want to watch at 10 cloverfield lane. Agent: What time would you like to see it? Agent: What time would you like to see it? User: I want to watch at 9:25 pm. User: What time is it playing tomorrow? I need 3 tickets. Agent: What city you would like? Agent: What city you would like? User: I want to watch at seattle. User: I do not care. Agent: What date would you like to watch it? Agent: What date would you like to watch it? User: I want to set it up tomorrow. User: I want to set it up tomorrow. Agent: Which theater would you like? Agent: Which theater would you like? User: I want to watch at regal meridian 16. User: Which theater is available? Agent: How many tickets do you need? Agent: How many tickets do you need? User: I want 2 tickets please! User: I want 3 tickets please! Agent: Great -I was able to purchase 2 tickets for you to see zoolander 2 tomorrow at regal meridian 16 theater in seattle at 9:25 pm.\n", "annotations": {"author": "[{\"end\":174,\"start\":90},{\"end\":212,\"start\":175},{\"end\":297,\"start\":213},{\"end\":385,\"start\":298},{\"end\":477,\"start\":386}]", "publisher": null, "author_last_name": "[{\"end\":99,\"start\":97},{\"end\":188,\"start\":184},{\"end\":222,\"start\":220},{\"end\":310,\"start\":307},{\"end\":402,\"start\":391}]", "author_first_name": "[{\"end\":96,\"start\":90},{\"end\":183,\"start\":175},{\"end\":219,\"start\":213},{\"end\":306,\"start\":298},{\"end\":390,\"start\":386}]", "author_affiliation": "[{\"end\":132,\"start\":101},{\"end\":173,\"start\":134},{\"end\":255,\"start\":224},{\"end\":296,\"start\":257},{\"end\":343,\"start\":312},{\"end\":384,\"start\":345},{\"end\":435,\"start\":404},{\"end\":476,\"start\":437}]", "title": "[{\"end\":51,\"start\":1},{\"end\":528,\"start\":478}]", "venue": "[{\"end\":618,\"start\":530}]", "abstract": "[{\"end\":1815,\"start\":743}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2263,\"start\":2240},{\"end\":2280,\"start\":2263},{\"end\":2300,\"start\":2280},{\"end\":2423,\"start\":2406},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2448,\"start\":2428},{\"end\":2963,\"start\":2939},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4348,\"start\":4332},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4476,\"start\":4456},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4704,\"start\":4683},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8540,\"start\":8514},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10594,\"start\":10584},{\"end\":10878,\"start\":10856},{\"end\":11119,\"start\":11095},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11499,\"start\":11482},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11518,\"start\":11499},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12691,\"start\":12663},{\"end\":13594,\"start\":13576},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14237,\"start\":14212},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16615,\"start\":16596},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17555,\"start\":17534}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":28434,\"start\":28376},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28572,\"start\":28435},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28846,\"start\":28573},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30045,\"start\":28847},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30170,\"start\":30046},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31504,\"start\":30171},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32015,\"start\":31505},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32048,\"start\":32016}]", "paragraph": "[{\"end\":2301,\"start\":1831},{\"end\":3421,\"start\":2303},{\"end\":3977,\"start\":3423},{\"end\":4189,\"start\":4004},{\"end\":4308,\"start\":4191},{\"end\":4839,\"start\":4310},{\"end\":5668,\"start\":4841},{\"end\":5703,\"start\":5670},{\"end\":7035,\"start\":5705},{\"end\":7911,\"start\":7058},{\"end\":8229,\"start\":7938},{\"end\":8541,\"start\":8231},{\"end\":8745,\"start\":8560},{\"end\":9158,\"start\":8802},{\"end\":9355,\"start\":9160},{\"end\":10273,\"start\":9357},{\"end\":11716,\"start\":10275},{\"end\":12521,\"start\":11736},{\"end\":13000,\"start\":12523},{\"end\":13727,\"start\":13002},{\"end\":14094,\"start\":13729},{\"end\":14578,\"start\":14121},{\"end\":14665,\"start\":14580},{\"end\":15280,\"start\":14667},{\"end\":15787,\"start\":15282},{\"end\":15853,\"start\":15789},{\"end\":16327,\"start\":15855},{\"end\":17473,\"start\":16365},{\"end\":18444,\"start\":17475},{\"end\":18884,\"start\":18460},{\"end\":20996,\"start\":18886},{\"end\":21898,\"start\":21049},{\"end\":22295,\"start\":21924},{\"end\":22837,\"start\":22317},{\"end\":23281,\"start\":22859},{\"end\":23492,\"start\":23305},{\"end\":24355,\"start\":23512},{\"end\":24812,\"start\":24375},{\"end\":25842,\"start\":24833},{\"end\":27445,\"start\":25873},{\"end\":28331,\"start\":27447}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8559,\"start\":8542},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8801,\"start\":8746}]", "table_ref": "[{\"end\":27946,\"start\":27939}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1829,\"start\":1817},{\"end\":4002,\"start\":3980},{\"attributes\":{\"n\":\"2\"},\"end\":7056,\"start\":7038},{\"attributes\":{\"n\":\"2.1\"},\"end\":7936,\"start\":7914},{\"attributes\":{\"n\":\"2.2\"},\"end\":11734,\"start\":11719},{\"attributes\":{\"n\":\"2.3\"},\"end\":14119,\"start\":14097},{\"attributes\":{\"n\":\"3\"},\"end\":16363,\"start\":16330},{\"attributes\":{\"n\":\"4\"},\"end\":18458,\"start\":18447},{\"attributes\":{\"n\":\"4.1\"},\"end\":21024,\"start\":20999},{\"attributes\":{\"n\":\"4.2\"},\"end\":21047,\"start\":21027},{\"attributes\":{\"n\":\"4.3\"},\"end\":21922,\"start\":21901},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":22315,\"start\":22298},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":22857,\"start\":22840},{\"attributes\":{\"n\":\"4.4\"},\"end\":23303,\"start\":23284},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":23510,\"start\":23495},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":24373,\"start\":24358},{\"attributes\":{\"n\":\"4.5\"},\"end\":24831,\"start\":24815},{\"attributes\":{\"n\":\"5\"},\"end\":25871,\"start\":25845},{\"end\":28354,\"start\":28334},{\"end\":28375,\"start\":28357},{\"end\":28387,\"start\":28377},{\"end\":28446,\"start\":28436},{\"end\":28584,\"start\":28574},{\"end\":30056,\"start\":30047},{\"end\":31513,\"start\":31506},{\"end\":32026,\"start\":32017}]", "table": "[{\"end\":30045,\"start\":29059},{\"end\":32015,\"start\":32002}]", "figure_caption": "[{\"end\":28434,\"start\":28389},{\"end\":28572,\"start\":28448},{\"end\":28846,\"start\":28586},{\"end\":29059,\"start\":28849},{\"end\":30170,\"start\":30058},{\"end\":31504,\"start\":30173},{\"end\":32002,\"start\":31515},{\"end\":32048,\"start\":32028}]", "figure_ref": "[{\"end\":4029,\"start\":4021},{\"end\":6904,\"start\":6896},{\"end\":7107,\"start\":7101},{\"end\":8228,\"start\":8220},{\"end\":19880,\"start\":19872},{\"end\":20081,\"start\":20070},{\"end\":20422,\"start\":20414},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21769,\"start\":21758},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22625,\"start\":22616},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23169,\"start\":23158},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23639,\"start\":23630},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24509,\"start\":24500},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24537,\"start\":24528},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24710,\"start\":24701},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25530,\"start\":25522},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25628,\"start\":25620}]", "bib_author_first_name": "[{\"end\":32979,\"start\":32972},{\"end\":32993,\"start\":32988},{\"end\":33008,\"start\":33003},{\"end\":33240,\"start\":33232},{\"end\":33252,\"start\":33247},{\"end\":33272,\"start\":33266},{\"end\":33282,\"start\":33278},{\"end\":33304,\"start\":33296},{\"end\":33312,\"start\":33310},{\"end\":33783,\"start\":33777},{\"end\":33799,\"start\":33793},{\"end\":33810,\"start\":33804},{\"end\":33823,\"start\":33815},{\"end\":33837,\"start\":33829},{\"end\":33850,\"start\":33844},{\"end\":33860,\"start\":33858},{\"end\":34441,\"start\":34436},{\"end\":34461,\"start\":34455},{\"end\":34471,\"start\":34467},{\"end\":34493,\"start\":34485},{\"end\":34508,\"start\":34500},{\"end\":34516,\"start\":34514},{\"end\":34528,\"start\":34523},{\"end\":34914,\"start\":34907},{\"end\":35309,\"start\":35303},{\"end\":35324,\"start\":35317},{\"end\":35557,\"start\":35552},{\"end\":35566,\"start\":35562},{\"end\":35579,\"start\":35575},{\"end\":35594,\"start\":35588},{\"end\":35611,\"start\":35603},{\"end\":35620,\"start\":35617},{\"end\":35873,\"start\":35869},{\"end\":35882,\"start\":35879},{\"end\":36166,\"start\":36157},{\"end\":36178,\"start\":36173},{\"end\":36197,\"start\":36192},{\"end\":36212,\"start\":36206},{\"end\":36214,\"start\":36213},{\"end\":36225,\"start\":36221},{\"end\":36238,\"start\":36234},{\"end\":36240,\"start\":36239},{\"end\":36256,\"start\":36252},{\"end\":36271,\"start\":36265},{\"end\":36291,\"start\":36284},{\"end\":36293,\"start\":36292},{\"end\":36310,\"start\":36305},{\"end\":36326,\"start\":36322},{\"end\":36850,\"start\":36846},{\"end\":36852,\"start\":36851},{\"end\":36884,\"start\":36883},{\"end\":36896,\"start\":36891},{\"end\":36925,\"start\":36920},{\"end\":36927,\"start\":36926},{\"end\":36938,\"start\":36935},{\"end\":36951,\"start\":36946},{\"end\":37251,\"start\":37247},{\"end\":37270,\"start\":37264},{\"end\":37285,\"start\":37280},{\"end\":37647,\"start\":37643},{\"end\":37665,\"start\":37660},{\"end\":37838,\"start\":37835},{\"end\":37851,\"start\":37847},{\"end\":37865,\"start\":37858},{\"end\":37883,\"start\":37878},{\"end\":39113,\"start\":39106},{\"end\":39124,\"start\":39118},{\"end\":39138,\"start\":39132},{\"end\":39151,\"start\":39147},{\"end\":39174,\"start\":39168},{\"end\":39187,\"start\":39182},{\"end\":39208,\"start\":39197},{\"end\":39219,\"start\":39214}]", "bib_author_last_name": "[{\"end\":32986,\"start\":32980},{\"end\":33001,\"start\":32994},{\"end\":33015,\"start\":33009},{\"end\":33245,\"start\":33241},{\"end\":33264,\"start\":33253},{\"end\":33276,\"start\":33273},{\"end\":33294,\"start\":33283},{\"end\":33308,\"start\":33305},{\"end\":33317,\"start\":33313},{\"end\":33791,\"start\":33784},{\"end\":33802,\"start\":33800},{\"end\":33813,\"start\":33811},{\"end\":33827,\"start\":33824},{\"end\":33842,\"start\":33838},{\"end\":33856,\"start\":33851},{\"end\":33865,\"start\":33861},{\"end\":34453,\"start\":34442},{\"end\":34465,\"start\":34462},{\"end\":34483,\"start\":34472},{\"end\":34498,\"start\":34494},{\"end\":34512,\"start\":34509},{\"end\":34521,\"start\":34517},{\"end\":34533,\"start\":34529},{\"end\":34918,\"start\":34915},{\"end\":35315,\"start\":35310},{\"end\":35328,\"start\":35325},{\"end\":35560,\"start\":35558},{\"end\":35573,\"start\":35567},{\"end\":35586,\"start\":35580},{\"end\":35601,\"start\":35595},{\"end\":35615,\"start\":35612},{\"end\":35629,\"start\":35621},{\"end\":35877,\"start\":35874},{\"end\":35887,\"start\":35883},{\"end\":36171,\"start\":36167},{\"end\":36190,\"start\":36179},{\"end\":36204,\"start\":36198},{\"end\":36219,\"start\":36215},{\"end\":36232,\"start\":36226},{\"end\":36250,\"start\":36241},{\"end\":36263,\"start\":36257},{\"end\":36282,\"start\":36272},{\"end\":36303,\"start\":36294},{\"end\":36320,\"start\":36311},{\"end\":36335,\"start\":36327},{\"end\":36873,\"start\":36853},{\"end\":36881,\"start\":36875},{\"end\":36889,\"start\":36885},{\"end\":36911,\"start\":36897},{\"end\":36918,\"start\":36913},{\"end\":36933,\"start\":36928},{\"end\":36944,\"start\":36939},{\"end\":36954,\"start\":36952},{\"end\":36958,\"start\":36956},{\"end\":37262,\"start\":37252},{\"end\":37278,\"start\":37271},{\"end\":37291,\"start\":37286},{\"end\":37658,\"start\":37648},{\"end\":37671,\"start\":37666},{\"end\":37845,\"start\":37839},{\"end\":37856,\"start\":37852},{\"end\":37876,\"start\":37866},{\"end\":37890,\"start\":37884},{\"end\":39116,\"start\":39114},{\"end\":39130,\"start\":39125},{\"end\":39145,\"start\":39139},{\"end\":39166,\"start\":39152},{\"end\":39180,\"start\":39175},{\"end\":39195,\"start\":39188},{\"end\":39212,\"start\":39209},{\"end\":39225,\"start\":39220}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2129889},\"end\":33162,\"start\":32930},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13241311},\"end\":33690,\"start\":33164},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8951658},\"end\":34361,\"start\":33692},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5775306},\"end\":34804,\"start\":34363},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15680042},\"end\":35211,\"start\":34806},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15531316},\"end\":35497,\"start\":35213},{\"attributes\":{\"id\":\"b6\"},\"end\":35775,\"start\":35499},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":22017558},\"end\":36098,\"start\":35777},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":205242740},\"end\":36775,\"start\":36100},{\"attributes\":{\"id\":\"b9\"},\"end\":37185,\"start\":36777},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2394627},\"end\":37539,\"start\":37187},{\"attributes\":{\"id\":\"b11\"},\"end\":37833,\"start\":37541},{\"attributes\":{\"doi\":\"arXiv:1511.05952\",\"id\":\"b12\"},\"end\":38055,\"start\":37835},{\"attributes\":{\"doi\":\"arXiv:1606.02689\",\"id\":\"b13\"},\"end\":41306,\"start\":38057}]", "bib_title": "[{\"end\":32970,\"start\":32930},{\"end\":33230,\"start\":33164},{\"end\":33775,\"start\":33692},{\"end\":34434,\"start\":34363},{\"end\":34905,\"start\":34806},{\"end\":35301,\"start\":35213},{\"end\":35867,\"start\":35777},{\"end\":36155,\"start\":36100},{\"end\":37245,\"start\":37187},{\"end\":39104,\"start\":38057}]", "bib_author": "[{\"end\":32988,\"start\":32972},{\"end\":33003,\"start\":32988},{\"end\":33017,\"start\":33003},{\"end\":33247,\"start\":33232},{\"end\":33266,\"start\":33247},{\"end\":33278,\"start\":33266},{\"end\":33296,\"start\":33278},{\"end\":33310,\"start\":33296},{\"end\":33319,\"start\":33310},{\"end\":33793,\"start\":33777},{\"end\":33804,\"start\":33793},{\"end\":33815,\"start\":33804},{\"end\":33829,\"start\":33815},{\"end\":33844,\"start\":33829},{\"end\":33858,\"start\":33844},{\"end\":33867,\"start\":33858},{\"end\":34455,\"start\":34436},{\"end\":34467,\"start\":34455},{\"end\":34485,\"start\":34467},{\"end\":34500,\"start\":34485},{\"end\":34514,\"start\":34500},{\"end\":34523,\"start\":34514},{\"end\":34535,\"start\":34523},{\"end\":34920,\"start\":34907},{\"end\":35317,\"start\":35303},{\"end\":35330,\"start\":35317},{\"end\":35562,\"start\":35552},{\"end\":35575,\"start\":35562},{\"end\":35588,\"start\":35575},{\"end\":35603,\"start\":35588},{\"end\":35617,\"start\":35603},{\"end\":35631,\"start\":35617},{\"end\":35879,\"start\":35869},{\"end\":35889,\"start\":35879},{\"end\":36173,\"start\":36157},{\"end\":36192,\"start\":36173},{\"end\":36206,\"start\":36192},{\"end\":36221,\"start\":36206},{\"end\":36234,\"start\":36221},{\"end\":36252,\"start\":36234},{\"end\":36265,\"start\":36252},{\"end\":36284,\"start\":36265},{\"end\":36305,\"start\":36284},{\"end\":36322,\"start\":36305},{\"end\":36337,\"start\":36322},{\"end\":36875,\"start\":36846},{\"end\":36883,\"start\":36875},{\"end\":36891,\"start\":36883},{\"end\":36913,\"start\":36891},{\"end\":36920,\"start\":36913},{\"end\":36935,\"start\":36920},{\"end\":36946,\"start\":36935},{\"end\":36956,\"start\":36946},{\"end\":36960,\"start\":36956},{\"end\":37264,\"start\":37247},{\"end\":37280,\"start\":37264},{\"end\":37293,\"start\":37280},{\"end\":37660,\"start\":37643},{\"end\":37673,\"start\":37660},{\"end\":37847,\"start\":37835},{\"end\":37858,\"start\":37847},{\"end\":37878,\"start\":37858},{\"end\":37892,\"start\":37878},{\"end\":39118,\"start\":39106},{\"end\":39132,\"start\":39118},{\"end\":39147,\"start\":39132},{\"end\":39168,\"start\":39147},{\"end\":39182,\"start\":39168},{\"end\":39197,\"start\":39182},{\"end\":39214,\"start\":39197},{\"end\":39227,\"start\":39214}]", "bib_venue": "[{\"end\":33042,\"start\":33038},{\"end\":33438,\"start\":33387},{\"end\":34039,\"start\":33956},{\"end\":34574,\"start\":34563},{\"end\":35352,\"start\":35345},{\"end\":35928,\"start\":35917},{\"end\":33036,\"start\":33017},{\"end\":33385,\"start\":33319},{\"end\":33954,\"start\":33867},{\"end\":34561,\"start\":34535},{\"end\":34995,\"start\":34920},{\"end\":35343,\"start\":35330},{\"end\":35550,\"start\":35499},{\"end\":35915,\"start\":35889},{\"end\":36343,\"start\":36337},{\"end\":36844,\"start\":36777},{\"end\":37354,\"start\":37293},{\"end\":37641,\"start\":37541},{\"end\":37937,\"start\":37908},{\"end\":39291,\"start\":39243}]"}}}, "year": 2023, "month": 12, "day": 17}