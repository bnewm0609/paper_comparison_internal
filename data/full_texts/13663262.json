{"id": 13663262, "updated": "2023-09-30 22:56:29.642", "metadata": {"title": "A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization", "authors": "[{\"first\":\"Li\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Junlin\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Yunzhe\",\"last\":\"Tao\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Zhong\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Qiang\",\"last\":\"Du\",\"middle\":[]}]", "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence", "journal": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence", "publication_date": {"year": 2018, "month": 5, "day": 9}, "abstract": "In this paper, we propose a deep learning approach to tackle the automatic summarization tasks by incorporating topic information into the convolutional sequence-to-sequence (ConvS2S) model and using self-critical sequence training (SCST) for optimization. Through jointly attending to topics and word-level alignment, our approach can improve coherence, diversity, and informativeness of generated summaries via a biased probability generation mechanism. On the other hand, reinforcement training, like SCST, directly optimizes the proposed model with respect to the non-differentiable metric ROUGE, which also avoids the exposure bias during inference. We carry out the experimental evaluation with state-of-the-art methods over the Gigaword, DUC-2004, and LCSTS datasets. The empirical results demonstrate the superiority of our proposed method in the abstractive summarization.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1805.03616", "mag": "2964032708", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcai/WangYTZLD18", "doi": "10.24963/ijcai.2018/619"}}, "content": {"source": {"pdf_hash": "2aca75709b589dd124aab89717048b416253fdd2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1805.03616v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.ijcai.org/proceedings/2018/0619.pdf", "status": "BRONZE"}}, "grobid": {"id": "f3034b37eba0d1ecaf55b296384bd04791f57591", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2aca75709b589dd124aab89717048b416253fdd2.txt", "contents": "\nA Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization\n\n\nLi Wang lilianwang@tencent.com \nTencent Data Center\nSNG\n\nJunlin Yao jyao@student.ethz.ch \nETH Z\u00fcrich\n\n\nYunzhe Tao y.tao@columbia.edu \nColumbia University\n\n\nLi Zhong \nTencent Data Center\nSNG\n\nWei Liu \nTencent AI Lab\n\n\nQiang Du \nColumbia University\n\n\nA Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization\n\nIn this paper, we propose a deep learning approach to tackle the automatic summarization tasks by incorporating topic information into the convolutional sequence-to-sequence (ConvS2S) model and using self-critical sequence training (SCST) for optimization. Through jointly attending to topics and word-level alignment, our approach can improve coherence, diversity, and informativeness of generated summaries via a biased probability generation mechanism. On the other hand, reinforcement training, like SCST, directly optimizes the proposed model with respect to the non-differentiable metric ROUGE, which also avoids the exposure bias during inference. We carry out the experimental evaluation with state-of-the-art methods over the Gigaword, DUC-2004, and LCSTS datasets. The empirical results demonstrate the superiority of our proposed method in the abstractive summarization.\n\nIntroduction\n\nAutomatic text summarization has played an important role in a variety of natural language processing (NLP) applications, such as news headlines generation [Kraaij et al., 2002] and feeds stream digests [Barzilay and McKeown, 2005]. It is of interest to generate informative and representative natural language summaries which are capable of retaining the main ideas of source articles. The key challenges in automatic text summarization are correctly evaluating and selecting important information, efficiently filtering redundant contents, and properly aggregating related segments and making humanreadable summaries. Compared to other NLP tasks, the automatic summarization has its own difficulties. For example, unlike machine translation tasks where input and output sequences often share similar lengths, summarization tasks are more likely to have input and output sequences greatly imbalanced. Besides, machine translation tasks usually have some direct word-level alignment between input and output sequences, which is less obvious in summarization.\n\nThere are two genres of automatic summarization techniques, namely extraction and abstraction. The goal of extrac-tive summarization [Neto et al., 2002] is to produce a summary by selecting important pieces of the source document and concatenating them verbatim, while abstractive summarization  generates summaries based on the core ideas of the document, therefore the summaries could be paraphrased in more general terms. Other than extraction, abstractive methods should be able to properly rewrite the core ideas of the source document and assure that the generated summaries are grammatically correct and human readable, which is close to the way how humans do summarization and thus is of interest to us in this paper.\n\nRecently, deep neural network models have been widely used for NLP tasks such as machine translation [Bahdanau et al., 2014], and text summarization [Nallapati et al., 2016b]. In particular, the attention based sequence-tosequence framework [Bahdanau et al., 2014] with recurrent neural networks (RNNs) [Sutskever et al., 2014] prevails in the NLP tasks. However, RNN-based models are more prone to gradient vanishing due to their chain structure of nonlinearities compared to the hierarchical structure of CNNbased models [Dauphin et al., 2016]. In addition, the temporal dependence among the hidden states of RNNs prevents parallelization over the elements of a sequence, which makes the training inefficient.\n\nIn this paper, we propose a new approach based on the convolutional sequence-to-sequence (ConvS2S) framework [Gehring et al., 2017] jointly with a topic-aware attention mechanism. To the best of our knowledge, this is the first work for automatic abstractive summarization that incorporates the topic information, which can provide themed and contextual alignment information into deep learning architectures. In addition, we also optimize our proposed model by employing the reinforcement training [Paulus et al., 2017]. The main contributions of this paper include:\n\n\u2022 We propose a joint attention and biased probability generation mechanism to incorporate the topic information into an automatic summarization model, which introduces contextual information to help the model generate more coherent summaries with increased diversity and informativeness. \u2022 We employ the self-critical sequence training technique in ConvS2S to directly optimize the model with respect to the non-differentiable summarization metric ROUGE, which also remedies the exposure bias issue.\n\n\u2022 Extensive experimental results on three benchmark datasets demonstrate that by fully exploiting the power of the ConvS2S architecture enhanced by topic embedding and SCST, our proposed model yields high accuracy for abstractive summarization, advancing the stateof-the-art methods.\n\n\nRelated Work\n\nAutomatic text summarization has been widely investigated. Many approaches have been proposed to address this challenging task. Various methods [Neto et al., 2002] focus on the extractive summarization, which select important contents of text and combine them verbatim to produce a summary. On the other hand, abstractive summarization models are able to produce a grammatical summary with a novel expression, most of which [Rush et al., 2015;Nallapati et al., 2016a] are built upon the neural attentionbased sequence-to-sequence framework [Sutskever et al., 2014].\n\nThe predominant models are based on the RNNs [Nallapati et al., 2016b;Shen et al., 2016;Paulus et al., 2017], where the encoder and decoder are constructed using either Long Short-Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] or Gated Recurrent Unit (GRU) [Cho et al., 2014]. However, very few methods have explored the performance of convolutional structure in summarization tasks. Compared to RNNs, convolutional neural networks (CNNs) enjoy several advantages, including the efficient training by leveraging parallel computing, and mitigating the gradient vanishing problem due to fewer non-linearities [Dauphin et al., 2016]. Notably, the recently proposed gated convolutional network [Dauphin et al., 2016;Gehring et al., 2017] outperforms state-of-the-art RNN-based models in the language modeling and machine translation tasks. While the ConvS2S model is also evaluated on the abstractive summarization [Gehring et al., 2017], there are several limitations. First, the model is trained by minimizing a maximum-likelihood loss which is sometimes inconsistent with the quality of a summary and the metric that is evaluated from the whole sentences, such as ROUGE [Lin, 2004] In addition, the exposure bias [Ranzato et al., 2015] occurs due to only exposing the model to the training data distribution instead of its own predictions. More importantly, the ConvS2S model utilizes only word-level alignment which may be insufficient for summarization and prone to incoherent generalized summaries. Therefore, the higher level alignment could be a potential assist. For example, the topic information has been introduced to a RNN-based sequence-to-sequence model [Xing et al., 2017] for chatbots to generate more informative responses.\n\n\nReinforced Topic-Aware Convolutional\n\nSequence-to-Sequence Model\n\nIn this section, we propose the Reinforced Topic-Aware Convolutional Sequence-to-Sequence model, which consists of a Figure 1: A graphical illustration of the topic-aware convolutional architecture. Word and topic embeddings of the source sequence are encoded by the associated convolutional blocks (bottom left and bottom right). Then we jointly attend to words and topics by computing dot products of decoder representations (top left) and word/topic encoder representations. Finally, we produce the target sequence through a biased probability generation mechanism.\n\nconvolutional architecture with both input words and topics, a joint multi-step attention mechanism, a biased generation structure, and a reinforcement learning procedure. The graphical illustration of the topic-aware convolutional architecture can be found in Figure 1.\n\n\nConvS2S Architecture\n\nWe exploit the ConvS2S architecture [Gehring et al., 2017] as the basic infrastructure of our model. In this paper, two convolutional blocks are employed, associated with the wordlevel and topic-level embeddings, respectively. We introduce the former in this section and the latter in next, along with the new joint attention and the biased generation mechanism.\n\nPosition Embeddings Let x = (x 1 , . . . , x m ) denote the input sentence. We first embed the input elements (words) in a distributional space as w = (w 1 , . . . , w m ), where w i \u2208 R d are rows of a randomly initialized matrix D word \u2208 R V \u00d7d with V being the size of vocabulary. We also add a positional embedding, p = (p 1 , . . . , p m ) with p i \u2208 R d , to retain the order information. Thus, the final embedding for the input is e = (w 1 +p 1 , . . . , w m +p m ). Similarly, let q = (q 1 , . . . , q n ) denote the embedding for output elements that were already generated by the decoder and being fed back to the next step.\n\n\nConvolutional Layer\n\nBoth encoder and decoder networks are built by stacking several convolutional layers. Suppose that the kernel has width of k and the input embedding dimension is d. The convolution takes a concatenation of k input elements X \u2208 R kd and maps it to an output element Y \u2208 R 2d , namely,\nY = f conv (X) . = W Y X + b Y ,(1)\nwhere the kernel matrix W Y \u2208 R 2d\u00d7kd and the bias term b Y \u2208 R 2d are the parameters to be learned.\n\nRewrite the output as Y = [A; B], where A, B \u2208 R d . Then the gated linear unit (GLU) [Dauphin et al., 2016] is given by\ng([A; B]) = A \u2297 \u03c3(B) ,\n(2) where \u03c3 is the sigmoid function, \u2297 is the point-wise multiplication, and the output of GLU is in R d .\n\nWe denote the outputs of the l-th layer as h l = (h l 1 , . . . , h l n ) for the decoder, and z l = (z l 1 , . . . , z l m ) for the encoder. Take the decoder for illustration. The convolution unit i on the l-th layer is computed by residual connections as\nh l i = g \u2022 f conv h l\u22121 i\u2212k/2 ; \u00b7 \u00b7 \u00b7 ; h l\u22121 i+k/2 + h l\u22121 i ,(3)\nwhere h l i \u2208 R d and \u2022 is the function composition operator.\n\n\nMulti-step Attention\n\nThe attention mechanism is introduced to make the model access historical information. To compute the attention, we first embed the current decoder state h l i as\nd l i = W l d h l i + b l d + q i , (4) where q i \u2208 R d is the embedding of the previous decoded element. Weight matrix W l d \u2208 R d\u00d7d and bias b l d \u2208 R d are the parameters to be learned.\nThe attention weights \u03b1 l ij of state i and source input element j is computed as a dot product between d l i and the output z uo j of the last encoder block u o , namely,\n\u03b1 l ij = exp(d l i \u00b7 z uo j ) m t=1 exp(d l i \u00b7 z uo t )\n.\n\nThe conditional input c l i \u2208 R d for the current decoder layer is computed as\nc l i = m j=1 \u03b1 l ij (z uo j + e j ) ,(6)\nwhere e j is the input element embedding that can provide point information about a specific input element. Once c l i has been computed, it is added to the output of the corresponding decoder layer h l i and serves as a part of the input to h l+1 i .\n\n\nTopic-Aware Attention Mechanism\n\nA topic model is a type of statistical model for discovering the abstract ideas or hidden semantic structures that occur in a collection of source articles. In this paper, we employ the topic model to acquire latent knowledge of documents and incorporate a topic-aware mechanism into the multi-step attention-based ConvS2S model, which is expected to bring prior knowledge for text summarization. Now we present the novel approach on how to incorporate the topic model into the basic ConvS2S framework via the joint attention mechanism and biased probability generation process.\n\n\nTopic Embeddings\n\nThe topic embeddings are obtained by classical topic models such as Latent Dirichlet Allocation (LDA) [Blei et al., 2003]. During pre-training, we use LDA to assign topics to the input texts. The top N non-universal words with the highest probabilities of each topic are chosen into the topic vocabulary K. More details will be given in Section 4. While the vocabulary of texts is denoted as V , we assume that K \u2282 V . Given an input sentence x = (x 1 , . . . , x m ), if a word x i / \u2208 K, we embed it as before to attain w i . However, if a word x i \u2208 K, we can embed this topic word as t i \u2208 R d , which is a row in the topic embedding matrix D topic \u2208 R K\u00d7d , where K is the size of topic vocabulary. The embedding matrix D topic is normalized from the corresponding pre-trained topic distribution matrix, whose row is proportional to the number of times that each word is assigned to each topic. In this case, the positional embedding vectors are also added to the encoder and decoder elements, respectively, to obtain the final topic embeddings r = (r 1 , . . . , r m ) and s = (s 1 , . . . , s n ).\n\n\nJoint Attention\n\nAgain we take the decoder for illustration. Following the convolutional layer introduced before, we can obtain the convolution unit i on the l-th layer in the decoder of topic level as\nh l i \u2208 R d . Similar to (4), we hav\u1ebd d l i =W l dh l i +b l d + s i .(7)\nWe then incorporate the topic information into the model through a joint attention mechanism. During decoding, the joint attention weight \u03b2 l ij is given by\n\u03b2 l ij = exp(d l i \u00b7 z uo j +d l i \u00b7 z ut j ) m t=1 exp(d l i \u00b7 z uo t +d l i \u00b7 z ut t ) ,(8)\nwhere z ut j is the output of the last topic-level encoder block u t . Then the conditional inputc l i \u2208 R d is computed as\nc l i = m j=1 \u03b2 l ij (z ut j + r j ) .(9)\nIn the joint attention mechanism, bothc l i and c l i are added to the output of the corresponding decoder layerh l i and are a part of the input toh l+1 i . Biased Probability Generation Finally, we compute a distribution over all possible next target elements y i+1 \u2208 R T , namely\np \u03b8 (y i+1 ) := p(y i+1 |y 1 , . . . , y i , x) \u2208 R T ,(10)\nby transforming the top word-level decoder outputs h Lo and topic-level decoder outputsh Lt via a linear layer \u03a8(\u00b7), which is computed by\n\u03a8(h) = W o h + b o ,(11)\nwhere W o \u2208 R T \u00d7d and b o \u2208 R T are the parameters to be learned. Then the biased generation distribution is given as\np \u03b8 (y i+1 ) = 1 Z exp \u03a8(h Lo i ) + exp \u03a8(h Lt i ) \u2297 I {w\u2208K} ,(12)\nwhere Z is the normalizer, h Lo i andh Lt i denote the i-th top decoder outputs of word and topic, respectively, and I is the one-hot indicator vector of each candidate word w in y i+1 . When the candidate word w is a topic word, we bias the generation distribution by the topic information. Otherwise, we ignore the topic part. To some extent, the complexity of the search space is reduced by introducing the topic bias since important words are more likely to be generated directly.\n\n\nReinforcement Learning\n\nThe teacher forcing algorithm [Williams and Zipser, 1989] aims to minimize the maximum-likelihood loss at each decoding step, namely,\nL ml = \u2212 T i=1 log p \u03b8 (y * i |y * 1 , y * 2 , . . . , y * i\u22121 , x) ,(13)\nwhere x refers to an input sequence and y * = (y * 1 ,y * 2 ,. . . ,y * T ) is the corresponding ground-truth output sequence.\n\nMinimizing the objective in Eq. (13) often produces suboptimal results with respect to the evaluation metrics, such as ROUGE which measures the sentence-level accuracy of the generated summaries. The sub-optimality is related to the problem called exposure bias [Ranzato et al., 2015], which is caused by only exposing a model to the distribution of training data instead of its own distribution. During the training process, models are fed by ground-truth output sequences to predict the next word, whereas during inference they generate the next word given the predicted words as inputs. Therefore, in the test process, the error of each step accumulates and leads to the deterioration of performance.\n\nThe second reason for sub-optimality comes from the flexibility of summaries. The maximum-likelihood objective rewards models that can predict exactly the same summaries as references while penalizing those that produce different texts even though they are semantically similar. Providing multiple reference summaries is helpful yet insufficient since there are alternatives to rephrase a given summary. Therefore, minimizing the objective in Eq. (13) neglects the intrinsic property of summarization. ROUGE, on the other hand, provides more flexible evaluation, encouraging models to focus more on semantic meanings than on word-level correspondences.\n\nIn order to address such issues, we utilize self-critical sequence training (SCST) [Rennie et al., 2016], a policy gradient algorithm for reinforcement learning, to directly maximize the non-differentiable ROUGE metric. During reinforcement learning, we generate two output sequences given the input sequence x. The first sequence\u0177 is obtained by greedily selecting words that maximize the output probability distribution, and the other output sequence y s is generated by sampling from the distribution. After obtaining ROUGE scores of both sequences as our rewards, i.e., r(y s ) and r(\u0177), we minimize the reinforcement loss\nL rl = \u2212(r(y s ) \u2212 r(\u0177)) log p \u03b8 (y s ),(14)\nand update model parameters by gradient descent techniques. With SCST, we can directly optimize the discrete evaluation metric. In addition, the \"self-critical\" test-time estimate of the reward r(\u0177) provides a simple yet effective baseline \n\n\nTopic Information\n\nThe classical LDA with Gibbs Sampling technique is used to pre-train the corpus for topic embedding initialization and provide candidates for the biased probability generation process. The topic embedding values are normalized to a distribution with mean zero and variance of 0.1 for adaption to the neural network structure. In this paper, we pick top N = 200 words with the highest probabilities in each topic to obtain the topic word set. Note that the universal words are filtered out during pre-training. Randomly selected examples of topic words of the Gigaword corpus are presented in Table 1.\n\n\nModel Parameters and Optimization\n\nWe employ six convolutional layers for both the encoder and decoder. All embeddings, including the initialized embed-  Table 2: Accuracy on the Gigaword corpus in terms of the fulllength ROUGE-1 (RG-1), ROUGE-2 (RG-2), and ROUGE-L (RG-L). Best performance on each score is displayed in boldface.  ding and the output produced by the decoder before the final linear layer, have a dimensionality of 256. We also adopt the same dimensionality for the size of linear layer mapping between hidden and embedding states. We use a learning rate of 0.25 and reduce it by a decay rate of 0.1 once the validation ROUGE score stops increasing after each epoch until the learning rate falls below 10 \u22125 . We first train the basic topic-aware convolutional model with respect to a standard maximum likelihood objective, and then switch to further minimize a mixed training objective [Paulus et al., 2017], incorporating the reinforcement learning objective L rl and the original maximum likelihood L ml , which is given as\n\n\nRG-1 (F) RG-2 (F) RG-L (F) ABS (beam) [Rush\nL mixed = \u03bbL rl + (1 \u2212 \u03bb)L ml ,(15)\nwhere the scaling factor \u03bb is set to be 0.99 in our experiments. Moreover, we choose the ROUGE-L metric as the reinforcement reward function. \n\n\nResults and Analysis\n\nWe follow the existing work and adopt the ROUGE metric [Lin, 2004] for evaluation.\n\n\nGigaword Corpus\n\nWe demonstrate the effectiveness of our proposed model via a step-by-step justification. First, the basic ConvS2S structure with topic-aware model or reinforcement learning is tested, respectively. Then we combine the two to show the performance of our Reinforced-Topic-ConvS2S model. We report Examples of summaries D: the sri lankan government on wednesday announced the closure of government schools with immediate effect as a military campaign against tamil separatists escalated in the north of the country. R: sri lanka closes schools as war escalates OR: sri lanka closes schools with immediate effect OT: sri lanka closes schools in wake of military attacks D: a us citizen who spied for communist east germany was given a suspended jail sentence of ## months here friday. R: us citizen who spied for east germans given suspended sentence OR: us man gets suspended jail term for communist spying OT: us man jailed for espionage D: malaysian prime minister mahathir mohamad indicated he would soon relinquish control of the ruling party to his deputy anwar ibrahim. R: mahathir wants leadership change to be smooth OR: malaysia's mahathir to relinquish control of ruling party OT: malaysia's mahathir to submit control of ruling party D: a french crocodile farm said it had stepped up efforts to breed one of the world's most endangered species, the indian UNK, with the hope of ultimately returning animals to their habitat in south asia. R: french farm offers hope for endangered asian crocs UNK picture OR: french crocodile farm steps up efforts to breed endangered species OT: french crocodile farm says steps up efforts to save endangered species the full-length F-1 scores of the ROUGE-1 (RG-1), ROUGE-2 (RG-2), and ROUGE-L (RG-L) metrics and compare the results with various neural abstractive summarization methods, which are presented in Table 2. The ABS and ABS+ models are attention-based neural models for text summarization. The RAS-Elman model introduces a conditional RNN, in which the conditioner is provided by a convolutional attention-based encoder. The words-lvt5k-1sent model is also a RNN-based attention model which implements a largevocabulary trick. Besides, RNN+MRT employs the minimum risk training strategy which directly optimizes model parameters in sentence level with respect to the evaluation metrics. SEASS (beam) extends the sequence-to-sequence framework with a selective encoding model. The results have demonstrated that both the topic-aware module and the reinforcement learning process can improve the accuracy on text summarization. Moreover, our proposed model exhibits best scores of RG-1, RG-2 and RG-L. In addition, [Zhou et al., 2017] further selects 2000 pairs of summaries as an internal test set of Gigaword. We also evaluate our proposed model on this set and present the results in Table 3. Again, our proposed model achieves the best performance in terms of all the three ROUGE scores.\n\nTo further demonstrate the improvement of readability and diversity by the topic information, we also present some qualitative results by randomly extracting several summaries from test. We compare the reference summaries to the summaries generated by our proposed model with or without topic-aware mechanism. The examples are presented in Table 4. We can observe that when the topic model is adopted, it can generate some accurately delivered topic words which are not in RG-1 (R) RG-2 (R) RG-L (R) ABS [Rush et al., 2015] 26.   Table 6: Accuracy on the LCSTS dataset in terms of the full-length RG-1, RG-2, and RG-L. In last three rows, the word-level ROUGE scores are presented on the left and the character-level on the right.\n\nthe reference summaries or the original texts. It is believed that the joint learning with a pre-trained topic model can offer more insightful information and improve the diversity and readability for the summarization.\n\n\nDUC-2004 Dataset\n\nSince the DUC-2004 dataset is an evaluation-only dataset, we train the models on the Gigaword corpus first and then evaluate their performance on the DUC dataset. As the standard practice, we report the recall-based scores of the RG-1, RG-2, and RG-L metrics in this experiment, which are given in Table 5. From Table 5 we can observe that the proposed Reinforced-Topic-ConvS2S model achieves best scores of the RG-1 and RG-L metrics, and is comparable on the RG-2 score. Due to the similarity of the two datasets, we do not provide qualitative summarization examples in this experiment.\n\n\nLCSTS Dataset\n\nWe now consider the abstractive summarization task on the LCSTS dataset. Since this is a large-scale Chinese dataset, suitable data preprocessing approaches should be proposed first. Basically, there are two approaches to preprocessing the Chinese dataset: character-based and word-based. The former takes each Chinese character as the input, while the latter splits an input sentence into Chinese words. [Hu et al., 2015] provides a baseline result on both preprocessing approaches. [Shen et al., 2016] also conducts experiments on the LCSTS corpus based on character inputs. [Gu et al., 2016] proposes a neural model, the COPYNET, with both character-based and word-based preprocessing by incorporating the copying mechanism into the sequence-to-sequence framework. In this work, we adopt the word-based approach as we believe that in the case of Chinese, words are more relevant to latent knowledge of documents than characters are. Since the standard ROUGE package 2 is usually used to evaluate the English summaries, directly employing the package to evaluate Chinese summaries would yield underrated results. In order to evaluate the summarization on the LC-STS dataset, we follow the suggestion of [Hu et al., 2015] by mapping Chinese words/characters to numerical IDs, on which we then perform the ROUGE evaluation. Since not all previous work explicitly mentioned whether word-based or character-based ROUGE metrics were reported, we evaluate our proposed model with both metrics in order to obtain a comprehensive comparison. The results of both scores are presented in Table 6, which are displayed as word-based score/character-based score.\n\nFrom the results shown in Table 6, we see that one can always achieve higher ROUGE scores in the character level than that based on Chinese words by our proposed model. We can also observe that the character-based results of our Reinforced-Topic-ConvS2S model outperforms every other method. Regarding to word-based ROUGE scores, our model obtains the best performance in terms of RG-1 and RG-L metrics. However, our best model does not achieve a good RG-2 score as its RG-1 and RG-L scores. We suspect that it may be partly caused by the biased probability generation mechanism that influences word order, which requires further studies.\n\nIn addition to ROUGE scores, we also present some randomly picked examples of generated summaries in Table 7.\n\nThe original examples (in Chinese) are shown and all the texts are carefully translated to English for the convenience of reading. The examples demonstrate that the topic-aware mechanism can also improve the diversity in Chinese summarization tasks.\n\n\nConclusion and Future Work\n\nIn this work, we propose a topic-aware ConvS2S model with reinforcement learning for abstractive text summarization. It is demonstrated that the new topic-aware attention mechanism introduces some high-level contextual information for summarization. The performance of the proposed model advances state-of-the-art methods on various benchmark datasets. In addition, our model can produce summaries with better informativeness, coherence, and diversity.\n\nNote that the experiments in this work are mainly based on the sentence summarization. In the future, we aim to evaluate our model on the datasets where the source texts can be long paragraphs or multi-documents. Moreover, we also note that how to evaluate the performance on Chinese summaries remains an open problem. It is also of great interest to study on this subject in the future.\n\nExamples of summaries D: \u6839\u636e#### \u5e74# \u6708# \u65e5\u56fd\u5bb6\u53d1\u6539\u59d4\u7b49\u90e8\u95e8\u8054\u5408\u53d1\u5e03\u7684\u300a\u5173\u4e8e\u8fdb\u4e00\u6b65\u505a\u597d\u65b0\u80fd\u6e90\u6c7d\u8f66\u63a8\u5e7f\u5e94\u7528\u5de5\u4f5c\u7684\u901a\u77e5\u300b\uff0c#### \u5e74\u7684 \u8865\u8d34\u91d1\u989d\u76f8\u6bd4#### \u5e74\u5c06\u964d\u4f4e##% \u3002(\u5206\u4eab\u81ea@ \u7535\u52a8\u90a6) D: According to the notice On the further promotion and application of new energy vehicles, jointly released by the National Development and Reform Commission and other departments on ##/##/#### (date), the compensation of #### (year) will be reduced by ##% compared to #### (year). (reposted from @electric nation) R: \u8865\u8d34\u91d1\u989d\u518d\u7f29\u6c34#### \u5e74\u65b0\u80fd\u6e90\u8f66\u653f\u7b56\u89e3\u8bfb R: The compensation has been reduced again: #### (year) policy analysis of new energy automobiles OR: #### \u5e74\u65b0\u80fd\u6e90\u6c7d\u8f66\u63a8\u5e7f\u5e94\u7528\u5de5\u4f5c\u7684\u901a\u77e5 OR: #### (year) notice on the promotion and application of new energy vehicles OT : \u56fd\u5bb6\u53d1\u6539\u59d4 \u53d1\u6587 \u8fdb\u4e00\u6b65\u505a\u597d \u65b0\u80fd\u6e90\u6c7d\u8f66 \u63a8\u5e7f\u5e94\u7528\u5de5\u4f5c OT : The National Development and Reform Commission issued a policy on further promotion and application of new energy vehicles D: \u6210\u90fd\u5e02\u8f6f\u4ef6\u548c\u4fe1\u606f\u6280\u672f\u670d\u52a1\u4e1a\u8fd1\u5e74\u6765\u4e00\u76f4\u4fdd\u6301\u5feb\u901f\u589e\u957f\u52bf\u5934\uff0c\u7a33\u5c45\u4e2d\u897f\u90e8\u57ce\u5e02\u4e4b\u9996\uff0c\u5df2\u6210\u4e3a\u6211\u56fd\u897f\u90e8\" \u7845\u8c37\" \u3002 \u300a#### \u5e74\u5ea6\u6210\u90fd\u5e02\u8f6f\u4ef6\u548c\u4fe1\u606f\u6280\u672f\u670d\u52a1\u4ea7\u4e1a\u53d1\u5c55\u62a5\u544a\u300b\u65e5\u524d\u53d1\u5e03. . . . . . \u8be6\u60c5\u8bf7\u89c1: @ \u6210\u90fd\u65e5\u62a5@ \u6210\u90fd\u53d1\u5e03 D: In recent years, the service industry of software and information technology in Chengdu has been growing rapidly, ranking first among the cities in Midwest China. Chengdu has become China's western \"Silicon Valley\". The #### (year) Annual Chengdu Software and Information Technology Service Industry Development Report has been released recently ... ... see details: @ Chengdu Daily @ Chengdu release R: \u6210\u90fd\u503e\u529b\u6253\u9020\u897f\u90e8\" \u7845\u8c37\" R: Chengdu makes every effort to build the western \"Silicon Valley\" OR: \u6210\u90fd\u8f6f\u4ef6 \u548c\u4fe1\u606f\u6280\u672f\u670d\u52a1\u4e1a\u53d1\u5c55\u62a5\u544a\u53d1\u5e03 OR: The report of Chengdu software and information technology service industry development has been released OT : \u6210\u90fd\u8f6f\u4ef6 \u548c\u4fe1\u606f\u6280\u672f\u670d\u52a1\u4e1a \u8dc3\u5c45 \u897f\u90e8\" \u7845\u8c37\" OT : The service industry of software and information technology in Chengdu rockets to make it the western \"Silicon Valley\" D: \u65b0\u7586\u72ec\u7279\u7684\u533a\u4f4d\u4f18\u52bf\uff0c\u4f7f\u5176\u6210\u4e3a\" \u4e00\u5e26\u4e00\u8def\" \u6218\u7565\u91cd\u8981\u4e00\u73af\u3002\u8bb0\u8005\u4ece\u65b0\u7586\u53d1\u6539\u59d4\u83b7\u6089\uff0c\u5e93\u5c14\u52d2\u81f3\u683c\u5c14\u6728\u94c1\u8def\u5148\u671f\u5f00\u5de5 \u6bb5\u5df2\u8fdb\u5165\u62db\u6295\u6807\u9636\u6bb5\uff0c\u8ba1\u5212#### \u5e74## \u6708\u4e2d\u65ec\u6b63\u5f0f\u5f00\u5de5\u5efa\u8bbe\u3002#### \u5e74\u8ba1\u5212\u5b8c\u6210\u6295\u8d44## \u4ebf\u5143\u3002 D: Xinjiang's unique geographical advantages make it an important part of The Belt and Road strategy. The reporter learned from the Xinjiang Development and Reform Commission that the initial railway construction project from Korla to Golmud had been on tendering procedure. The project was scheduled to officially launch in mid ## (month) of #### (year) and attract the investment of ## billion yuan by #### (year). R: \" \u4e00\u5e26\u4e00\u8def\" \u6218\u7565\u60e0\u53ca\u65b0\u7586<unk>, \u94c1\u8def\u5e74\u5e95\u5f00\u5efa R: The Belt and Road strategy benefits Xinjiang <unk> and the railway construction starts by the end of #### (year) OR: \u65b0\u7586<unk> \u81f3\u683c\u5c14\u6728\u94c1\u8def\u8ba1\u5212#### \u5e74\u5f00\u5efa OR: The railway from <unk> to Golmud is scheduled to start construction in #### (year) OT : \u5e93\u5c14\u52d2\u81f3\u683c\u5c14\u6728\u94c1\u8def\u62df ## \u6708\u5f00\u5de5\u5efa\u8bbe OT : The railway construction project from Korla to Golmud is planned to launch in ## (month) D: \u6628\u65e5\uff0c\u5546\u62a5\u8bb0\u8005\u4ece\u4ee3\u8868\u56fd\u5185\u5a5a\u5c1a\u4ea7\u4e1a\" \u98ce\u5411\u6807\" \u7684\u4e0a\u6d77\u56fd\u9645\u5a5a\u7eb1\u6444\u5f71\u5668\u6750\u5c55\u89c8\u4f1a\u4e0a\u4e86\u89e3\u5230\uff0c\u90e8\u5206\u5546\u5bb6\u5f00\u59cb\u5c06\u5a5a\u5e86\u5e03 \u7f6e\u3001\u5a5a\u793c\u6d41\u7a0b\u3001\u5f62\u5f0f\u4ea4\u7ed9\u65b0\u4eba\u51b3\u5b9a\u4ee5\u8fce\u5408## \u540e\u65b0\u4eba\u7684\u9700\u6c42\u3002\u6b64\u6b21\u5c55\u89c8\u4f1a\u7684\u89c4\u6a21\u8d85\u8fc7# \u4e07\u5e73\u65b9\u7c73\uff0c\u5438\u5f15\u53c2\u5c55\u4f01\u4e1a\u8d85\u8fc7### \u5bb6\u3002 D: The day before, the reporters of Commercial News learned from the Shanghai International Wedding Photographic Equipment Exhibition, which has been leading and defining the domestic wedding industry, that some companies began to cater for the requirements of ##s-generation newly married couples by self-decided wedding decoration, wedding process and forms. The venue of the exhibition is more than # tens of thousands square meters, attracting more than ### exhibitors. R: \u5a5a\u5e86\" \u79c1\u4eba\u5b9a\u5236\" \u53d7## \u540e\u65b0\u4eba\u8ffd\u6367 R: The personalized wedding is admired by ##s-generation newly married couples OR: \u4e0a\u6d77 \u56fd\u9645\u5a5a\u7eb1\u6444\u5f71 \u5668\u6750\u5c55\u89c8\u4f1a\u4e3e\u884c OR: Shanghai International Wedding Photographic Equipment Exhibition was held OT : \u4e0a\u6d77 \u56fd\u9645\u5a5a\u7eb1\u6444\u5f71 \u5668\u6750\u5c55\u89c8\u4f1a\u6628 \u4e3e\u884c OT : Shanghai International Wedding Photographic Equipment Exhibition was held yesterday Table 7: Examples of generated summaries on the LCSTS dataset. D: source document, R: reference summary, OR: output of the Reinforced-ConvS2S model, OT: output of the Reinforced-Topic-ConvS2S model. The words marked in blue are topic words not in the reference summaries. The words marked in red are topic words neither in the reference summaries nor in the source documents. All the texts are carefully translated from Chinese.\n\n\n190K validation samples and 1951 test samples for evaluation. The input summary pairs consist of the headline and the first sentence of the source articles. We also evaluate various models on the DUC-2004 test set 1[Over et al., 2007]. The dataset is a standard summarization evaluation set, which consists of 500 news articles.No. \nTopic Words \n1 \nprime, minister, talks, leader, elections, visit \n2 \nbird, flu, officials, opens, poultry, die \n3 \ntrade, free, EU, army, urges, ban \n4 \nBush, world, talks, foreign, investment, markets \n5 \nworld, Malaysia, Thailand, meet, Vietnam, U.S. \n\nTable 1: Examples of topic words for the Gigaword corpus. \n\nand improves training/test time consistency. Since during \nlearning we set the baseline of the REINFORCE algorithm \nas the reward obtained by the current model in the test-time \ninference, the SCST exposes the model to its own distribu-\ntion and encourages it to produce the sequence output\u0177 with \na high ROUGE score, avoiding the exposure bias issue and \nthus improving the test performance. \n\n4 Experimental Setup \n\n4.1 Datasets \n\nIn this paper, we consider three datasets to evaluate the per-\nformance of different methods in the abstractive text sum-\nmarization task. First, we consider the annotated Gigaword \ncorpus [Graff and Cieri, 2003] preprocessed identically to \n[Rush et al., 2015], which leads to around 3.8M training \nsamples, Unlike the \nGigaword corpus, each article in DUC-2004 is paired with \nfour human-generated reference summaries, which makes the \nevaluation more objective. The last dataset for evaluation is \na large corpus of Chinese short text summarization (LCSTS) \ndataset [Hu et al., 2015] collected and constructed from the \nChinese microblogging website Sina Weibo. Following the \nsetting in the original paper, we use the first part of LCSTS \ndataset for training, which contains 2.4M text-summary pairs, \nand choose 725 pairs from the last part with high annotation \nscores as our test set. \n\n\n\nTable 3 :\n3Accuracy on the internal test set of Gigaword corpus in terms of the full-length RG-1, RG-2, and RG-L. Best performance on each score is displayed in boldface.\n\nTable 4 :\n4Examples of generated summaries on the Gigaword cor-\npus. D: source document, R: reference summary, OR: output of the \nReinforced-ConvS2S model, OT: output of the Reinforced-Topic-\nConvS2S model. The words marked in blue are topic words not in \nthe reference summaries. The words marked in red are topic words \nneither in the reference summaries nor in the source documents. \n\n\n\nTable 5 :\n5Accuracy on the DUC-2004 dataset in terms of the recall-\nonly RG-1, RG-2, and RG-L. Best performance on each score is \ndisplayed in boldface. \n\nRG-1 (F) \nRG-2 (F) \nRG-L (F) \ncharacter-based preprocessing \nRNN context [Hu et al., 2015] \n29.90 \n17.40 \n27.20 \nCOPYNET [Gu et al., 2016] \n34.40 \n21.60 \n31.30 \nRNN+MLE [Shen et al., 2016] \n34.90 \n23.30 \n32.70 \nRNN+MRT [Shen et al., 2016] \n38.20 \n25.20 \n35.40 \nword-based preprocessing \nRNN context [Hu et al., 2015] \n26.80 \n16.10 \n24.10 \nCOPYNET [Gu et al., 2016] \n35.00 \n22.30 \n32.00 \nTopic-ConvS2S \n38.94/44.42 21.05/32.65 37.03/42.09 \nReinforced-ConvS2S \n36.68/42.61 18.69/29.79 34.85/40.03 \nReinforced-Topic-ConvS2S \n39.93/45.12 21.58/33.08 37.92/42.68 \n\n\nhttp://duc.nist.gov/data.html\nhttp://www.berouge.com/Pages/default.aspx\nAcknowledgementsQiang Du is supported in part by the US NSF TRIPODs project through CCF-170483.\nLearning phrase representations using rnn encoder-decoder for statistical machine translation. [ References, Bahdanau, arXiv:1409.0473arXiv:1705.03122Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMichael Auli31arXiv preprintJonas Gehring. David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learningReferences [Bahdanau et al., 2014] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. [Barzilay and McKeown, 2005] Regina Barzilay and Kath- leen R McKeown. Sentence fusion for multidocu- ment news summarization. Computational Linguistics, 31(3):297-328, 2005. [Blei et al., 2003] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993-1022, 2003. [Cho et al., 2014] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. [Chopra et al., 2016] Sumit Chopra, Michael Auli, and Alexander M Rush. Abstractive sentence summarization with attentive recurrent neural networks. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 93-98, 2016. [Dauphin et al., 2016] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. arXiv preprint arXiv:1612.08083, 2016. [Gehring et al., 2017] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convo- lutional sequence to sequence learning. arXiv preprint arXiv:1705.03122, 2017.\n\nHeadline extraction based on a combination of uni-and multidocument summarization techniques. Cieri ; David Graff, C Cieri, ; Gu, arXiv:1603.06393arXiv:1506.05865Proceedings of the ACL workshop on Automatic Summarization/Document Understanding Conference. Kraaij et al., 2002] Wessel Kraaij, Martijn Spitters, and Anette Hulththe ACL workshop on Automatic Summarization/Document Understanding ConferenceACL9tion dataset. arXiv preprintIncorporating copying mechanism in sequence-to-sequence learningand Cieri, 2003] David Graff and C Cieri. English gi- gaword corpus. Linguistic Data Consortium, 2003. [Gu et al., 2016] Jiatao Gu, Zhengdong Lu, Hang Li, and Victor OK Li. Incorporating copying mecha- nism in sequence-to-sequence learning. arXiv preprint arXiv:1603.06393, 2016. [Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. [Hu et al., 2015] Baotian Hu, Qingcai Chen, and Fangze Zhu. Lcsts: A large scale chinese short text summariza- tion dataset. arXiv preprint arXiv:1506.05865, 2015. [Kraaij et al., 2002] Wessel Kraaij, Martijn Spitters, and Anette Hulth. Headline extraction based on a combina- tion of uni-and multidocument summarization techniques. In Proceedings of the ACL workshop on Automatic Sum- marization/Document Understanding Conference (DUC 2002). ACL, 2002.\n\nRouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out: Proceedings of the ACL-04 workshop. Ramesh Nallapati, Bing Xiang, and Bowen ZhouBarcelona, SpainSequence-to-sequence rnns for text summarization, 2004] Chin-Yew Lin. Rouge: A package for auto- matic evaluation of summaries. In Text summarization branches out: Proceedings of the ACL-04 workshop, vol- ume 8. Barcelona, Spain, 2004. [Nallapati et al., 2016a] Ramesh Nallapati, Bing Xiang, and Bowen Zhou. Sequence-to-sequence rnns for text summa- rization. 2016.\n\nJoel Neto, Alex Freitas, and Celso Kaestner. Automatic text summarization using a machine learning approach. [ Nallapati, arXiv:1602.06023Advances in Artificial Intelligence. arXiv preprintAbstractive text summarization using sequence-to-sequence rnns and beyond[Nallapati et al., 2016b] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text sum- marization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016. [Neto et al., 2002] Joel Neto, Alex Freitas, and Celso Kaest- ner. Automatic text summarization using a machine learn- ing approach. Advances in Artificial Intelligence, pages 205-215, 2002.\n\nRomain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. CoRR, abs/1705.04304. arXiv:1511.06732Sequence level training with recurrent neural networks. 43arXiv preprintDuc in context. Information Processing & Managementet al., 2007] Paul Over, Hoa Dang, and Donna Har- man. Duc in context. Information Processing & Man- agement, 43(6):1506-1520, 2007. [Paszke et al., 2017] Adam Paszke, Sam Gross, and Soumith Chintala. Pytorch, 2017. [Paulus et al., 2017] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. CoRR, abs/1705.04304, 2017. [Ranzato et al., 2015] Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.\n\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. Rennie , arXiv:1612.00563arXiv:1604.01904A learning algorithm for continually running fully recurrent neural networks. Williams and Zipser1arXiv preprintAdvances in neural information processing systemsRennie et al., 2016] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self- critical sequence training for image captioning. arXiv preprint arXiv:1612.00563, 2016. [Rush et al., 2015] Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for ab- stractive sentence summarization. arXiv preprint arXiv:1509.00685, 2015. [Shen et al., 2016] Shiqi Shen, Yu Zhao, Zhiyuan Liu, Maosong Sun, et al. Neural headline genera- tion with sentence-wise optimization. arXiv preprint arXiv:1604.01904, 2016. [Sutskever et al., 2013] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In International conference on machine learning, pages 1139-1147, 2013. [Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112, 2014. [Williams and Zipser, 1989] R. J. Williams and D. Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural Computation, 1(2):270-280, June 1989.\n\nSelective encoding for abstractive sentence summarization. arXiv:1704.07073AAAI. Nan Yang, Furu Wei, and Ming ZhouarXiv preprintTopic aware neural response generationet al., 2017] Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma. Topic aware neural response generation. In AAAI, pages 3351-3357, 2017. [Zhou et al., 2017] Qingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou. Selective encoding for abstractive sentence summarization. arXiv preprint arXiv:1704.07073, 2017.\n", "annotations": {"author": "[{\"end\":160,\"start\":104},{\"end\":206,\"start\":161},{\"end\":259,\"start\":207},{\"end\":294,\"start\":260},{\"end\":320,\"start\":295},{\"end\":352,\"start\":321}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":107},{\"end\":171,\"start\":168},{\"end\":217,\"start\":214},{\"end\":268,\"start\":263},{\"end\":302,\"start\":299},{\"end\":329,\"start\":327}]", "author_first_name": "[{\"end\":106,\"start\":104},{\"end\":167,\"start\":161},{\"end\":213,\"start\":207},{\"end\":262,\"start\":260},{\"end\":298,\"start\":295},{\"end\":326,\"start\":321}]", "author_affiliation": "[{\"end\":159,\"start\":136},{\"end\":205,\"start\":194},{\"end\":258,\"start\":238},{\"end\":293,\"start\":270},{\"end\":319,\"start\":304},{\"end\":351,\"start\":331}]", "title": "[{\"end\":101,\"start\":1},{\"end\":453,\"start\":353}]", "venue": null, "abstract": "[{\"end\":1336,\"start\":455}]", "bib_ref": "[{\"end\":1529,\"start\":1508},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1583,\"start\":1555},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2564,\"start\":2545},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3263,\"start\":3240},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3313,\"start\":3288},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3403,\"start\":3380},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3466,\"start\":3442},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3684,\"start\":3662},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3983,\"start\":3961},{\"end\":4372,\"start\":4351},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5385,\"start\":5366},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5665,\"start\":5646},{\"end\":5689,\"start\":5665},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5786,\"start\":5762},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5859,\"start\":5834},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5877,\"start\":5859},{\"end\":5897,\"start\":5877},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6022,\"start\":5988},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6071,\"start\":6053},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6425,\"start\":6403},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6508,\"start\":6486},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6529,\"start\":6508},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6729,\"start\":6707},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6976,\"start\":6965},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7030,\"start\":7008},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7480,\"start\":7461},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8525,\"start\":8503},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10019,\"start\":9997},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12287,\"start\":12268},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15226,\"start\":15199},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15789,\"start\":15767},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16968,\"start\":16947},{\"end\":19326,\"start\":19305},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19760,\"start\":19749},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22483,\"start\":22464},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23265,\"start\":23246},{\"end\":24741,\"start\":24724},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24822,\"start\":24803},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24913,\"start\":24896},{\"end\":25541,\"start\":25524},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32035,\"start\":32016}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33776,\"start\":31799},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33948,\"start\":33777},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34338,\"start\":33949},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":35055,\"start\":34339}]", "paragraph": "[{\"end\":2410,\"start\":1352},{\"end\":3137,\"start\":2412},{\"end\":3850,\"start\":3139},{\"end\":4419,\"start\":3852},{\"end\":4920,\"start\":4421},{\"end\":5205,\"start\":4922},{\"end\":5787,\"start\":5222},{\"end\":7533,\"start\":5789},{\"end\":7600,\"start\":7574},{\"end\":8170,\"start\":7602},{\"end\":8442,\"start\":8172},{\"end\":8829,\"start\":8467},{\"end\":9465,\"start\":8831},{\"end\":9772,\"start\":9489},{\"end\":9909,\"start\":9809},{\"end\":10031,\"start\":9911},{\"end\":10161,\"start\":10055},{\"end\":10420,\"start\":10163},{\"end\":10550,\"start\":10489},{\"end\":10737,\"start\":10575},{\"end\":11098,\"start\":10927},{\"end\":11157,\"start\":11156},{\"end\":11237,\"start\":11159},{\"end\":11531,\"start\":11280},{\"end\":12145,\"start\":11567},{\"end\":13270,\"start\":12166},{\"end\":13474,\"start\":13290},{\"end\":13705,\"start\":13549},{\"end\":13923,\"start\":13800},{\"end\":14248,\"start\":13966},{\"end\":14446,\"start\":14309},{\"end\":14590,\"start\":14472},{\"end\":15142,\"start\":14658},{\"end\":15302,\"start\":15169},{\"end\":15503,\"start\":15377},{\"end\":16208,\"start\":15505},{\"end\":16862,\"start\":16210},{\"end\":17490,\"start\":16864},{\"end\":17776,\"start\":17536},{\"end\":18398,\"start\":17798},{\"end\":19444,\"start\":18436},{\"end\":19669,\"start\":19527},{\"end\":19776,\"start\":19694},{\"end\":22740,\"start\":19796},{\"end\":23472,\"start\":22742},{\"end\":23693,\"start\":23474},{\"end\":24301,\"start\":23714},{\"end\":25970,\"start\":24319},{\"end\":26610,\"start\":25972},{\"end\":26721,\"start\":26612},{\"end\":26972,\"start\":26723},{\"end\":27455,\"start\":27003},{\"end\":27844,\"start\":27457},{\"end\":31798,\"start\":27846}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9808,\"start\":9773},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10054,\"start\":10032},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10488,\"start\":10421},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10926,\"start\":10738},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11155,\"start\":11099},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11279,\"start\":11238},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13548,\"start\":13475},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13799,\"start\":13706},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13965,\"start\":13924},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14308,\"start\":14249},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14471,\"start\":14447},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14657,\"start\":14591},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15376,\"start\":15303},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17535,\"start\":17491},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19526,\"start\":19491}]", "table_ref": "[{\"end\":18397,\"start\":18390},{\"end\":18562,\"start\":18555},{\"end\":21657,\"start\":21650},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22643,\"start\":22636},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23089,\"start\":23082},{\"end\":23279,\"start\":23272},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":24019,\"start\":24012},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":24033,\"start\":24026},{\"end\":25906,\"start\":25899},{\"end\":26005,\"start\":25998},{\"end\":26720,\"start\":26713},{\"end\":31377,\"start\":31370}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1350,\"start\":1338},{\"attributes\":{\"n\":\"2\"},\"end\":5220,\"start\":5208},{\"attributes\":{\"n\":\"3\"},\"end\":7572,\"start\":7536},{\"attributes\":{\"n\":\"3.1\"},\"end\":8465,\"start\":8445},{\"end\":9487,\"start\":9468},{\"end\":10573,\"start\":10553},{\"attributes\":{\"n\":\"3.2\"},\"end\":11565,\"start\":11534},{\"end\":12164,\"start\":12148},{\"end\":13288,\"start\":13273},{\"attributes\":{\"n\":\"3.3\"},\"end\":15167,\"start\":15145},{\"attributes\":{\"n\":\"4.2\"},\"end\":17796,\"start\":17779},{\"attributes\":{\"n\":\"4.3\"},\"end\":18434,\"start\":18401},{\"end\":19490,\"start\":19447},{\"attributes\":{\"n\":\"5\"},\"end\":19692,\"start\":19672},{\"attributes\":{\"n\":\"5.1\"},\"end\":19794,\"start\":19779},{\"attributes\":{\"n\":\"5.2\"},\"end\":23712,\"start\":23696},{\"attributes\":{\"n\":\"5.3\"},\"end\":24317,\"start\":24304},{\"attributes\":{\"n\":\"6\"},\"end\":27001,\"start\":26975},{\"end\":33787,\"start\":33778},{\"end\":33959,\"start\":33950},{\"end\":34349,\"start\":34340}]", "table": "[{\"end\":33776,\"start\":32129},{\"end\":34338,\"start\":33961},{\"end\":35055,\"start\":34351}]", "figure_caption": "[{\"end\":32129,\"start\":31801},{\"end\":33948,\"start\":33789}]", "figure_ref": "[{\"end\":7727,\"start\":7719},{\"end\":8441,\"start\":8433}]", "bib_author_first_name": "[{\"end\":35320,\"start\":35319},{\"end\":37374,\"start\":37361},{\"end\":37383,\"start\":37382},{\"end\":37392,\"start\":37391},{\"end\":38706,\"start\":38698},{\"end\":39318,\"start\":39317},{\"end\":40802,\"start\":40796}]", "bib_author_last_name": "[{\"end\":35331,\"start\":35321},{\"end\":35341,\"start\":35333},{\"end\":37380,\"start\":37375},{\"end\":37389,\"start\":37384},{\"end\":37395,\"start\":37393},{\"end\":38710,\"start\":38707},{\"end\":39328,\"start\":39319}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b0\",\"matched_paper_id\":5590763},\"end\":37265,\"start\":35224},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8703373},\"end\":38640,\"start\":37267},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":964287},\"end\":39206,\"start\":38642},{\"attributes\":{\"id\":\"b3\"},\"end\":39868,\"start\":39208},{\"attributes\":{\"id\":\"b4\"},\"end\":40696,\"start\":39870},{\"attributes\":{\"id\":\"b5\"},\"end\":42157,\"start\":40698},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1770102},\"end\":42652,\"start\":42159}]", "bib_title": "[{\"end\":35317,\"start\":35224},{\"end\":37359,\"start\":37267},{\"end\":38696,\"start\":38642},{\"end\":39315,\"start\":39208},{\"end\":39995,\"start\":39870},{\"end\":40794,\"start\":40698},{\"end\":42216,\"start\":42159}]", "bib_author": "[{\"end\":35333,\"start\":35319},{\"end\":35343,\"start\":35333},{\"end\":37382,\"start\":37361},{\"end\":37391,\"start\":37382},{\"end\":37397,\"start\":37391},{\"end\":38712,\"start\":38698},{\"end\":39330,\"start\":39317},{\"end\":40805,\"start\":40796}]", "bib_venue": "[{\"end\":35516,\"start\":35374},{\"end\":37521,\"start\":37429},{\"end\":38779,\"start\":38712},{\"end\":39381,\"start\":39346},{\"end\":40067,\"start\":40013},{\"end\":40913,\"start\":40837},{\"end\":42238,\"start\":42234},{\"end\":35657,\"start\":35518},{\"end\":37670,\"start\":37593},{\"end\":38841,\"start\":38825}]"}}}, "year": 2023, "month": 12, "day": 17}