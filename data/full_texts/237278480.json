{"id": 237278480, "updated": "2023-10-05 23:41:46.577", "metadata": {"title": "Matching Bayesian and frequentist coverage probabilities when using an approximate data covariance matrix", "authors": "[{\"first\":\"Will\",\"last\":\"Percival\",\"middle\":[\"J.\"]},{\"first\":\"Oliver\",\"last\":\"Friedrich\",\"middle\":[]},{\"first\":\"Elena\",\"last\":\"Sellentin\",\"middle\":[]},{\"first\":\"Alan\",\"last\":\"Heavens\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Observational astrophysics consists of making inferences about the Universe by comparing data and models. The credible intervals placed on model parameters are often as important as the maximum a posteriori probability values, as the intervals indicate concordance or discordance between models and with measurements from other data. Intermediate statistics (e.g. the power spectrum) are usually measured and inferences made by fitting models to these rather than the raw data, assuming that the likelihood for these statistics has multivariate Gaussian form. The covariance matrix used to calculate the likelihood is often estimated from simulations, such that it is itself a random variable. This is a standard problem in Bayesian statistics, which requires a prior to be placed on the true model parameters and covariance matrix, influencing the joint posterior distribution. As an alternative to the commonly-used Independence-Jeffreys prior, we introduce a prior that leads to a posterior that has approximately frequentist matching coverage. This is achieved by matching the covariance of the posterior to that of the distribution of true values of the parameters around the maximum likelihood values in repeated trials, under certain assumptions. Using this prior, credible intervals derived from a Bayesian analysis can be interpreted approximately as confidence intervals, containing the truth a certain proportion of the time for repeated trials. Linking frequentist and Bayesian approaches that have previously appeared in the astronomical literature, this offers a consistent and conservative approach for credible intervals quoted on model parameters for problems where the covariance matrix is itself an estimate.", "fields_of_study": "[\"Physics\"]", "external_ids": {"arxiv": "2108.10402", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1093/mnras/stab3540"}}, "content": {"source": {"pdf_hash": "f9a61b49a244a4a398a6e5798674cf050e2f3394", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.10402v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2108.10402", "status": "GREEN"}}, "grobid": {"id": "585c5791a87147fdcff634286425abfb5e43cc4a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f9a61b49a244a4a398a6e5798674cf050e2f3394.txt", "contents": "\nMatching Bayesian and frequentist coverage probabilities when using an approximate data covariance matrix\n2021\n\nWill J Percival \nWaterloo Centre for Astrophysics\nUniversity of Waterloo\nN2L 3G1WaterlooONCanada\n\nDepartment of Physics and Astronomy\nUniversity of Waterloo\nN2L 3G1WaterlooONCanada\n\nPerimeter Institute for Theoretical Physics\n31 Caroline St. NorthN2L 2Y5WaterlooONCanada\n\nOliver Friedrich \nKavli Institute for Cosmology\nUniversity of Cambridge\nCB3 0HACambridgeUnited Kingdom\n\nChurchill College\nUniversity of Cambridge\nCB3 0DSCambridgeUnited Kingdom\n\nElena Sellentin \nMathematical Institute\nLeiden University\nSnellius Gebouw, Niels Bohrweg 1NL-2333 CALeidenThe Netherlands\n\nLeiden Observatory\nLeiden University\nNiels Bohrweg 2NL-2333 CAOort Gebouw, LeidenThe Netherlands\n\nAlan Heavens \nDepartment of Physics\nImperial Centre for Inference and Cosmology (ICIC)\nBlackett Laboratory\nImperial College London\nPrince Consort RoadSW7 2AZLondonUK\n\nMatching Bayesian and frequentist coverage probabilities when using an approximate data covariance matrix\n\nMNRAS\n0002021Accepted XXX. Received YYY; in original form ZZZPreprint 2 December 2021 Compiled using MNRAS L A T E X style file v3.0methods: statistical -methods: data analysis -cosmology: observation\nObservational astrophysics consists of making inferences about the Universe by comparing data and models. The credible intervals placed on model parameters are often as important as the maximum a posteriori probability values, as the intervals indicate concordance or discordance between models and with measurements from other data. Intermediate statistics (e.g. the power spectrum) are usually measured and inferences made by fitting models to these rather than the raw data, assuming that the likelihood for these statistics has multivariate Gaussian form. The covariance matrix used to calculate the likelihood is often estimated from simulations, such that it is itself a random variable. This is a standard problem in Bayesian statistics, which requires a prior to be placed on the true model parameters and covariance matrix, influencing the joint posterior distribution. As an alternative to the commonly-used independence Jeffreys prior, we introduce a prior that leads to a posterior that has approximately frequentist matching coverage. This is achieved by matching the covariance of the posterior to that of the distribution of true values of the parameters around the maximum likelihood values in repeated trials, under certain assumptions. Using this prior, credible intervals derived from a Bayesian analysis can be interpreted approximately as confidence intervals, containing the truth a certain proportion of the time for repeated trials. Linking frequentist and Bayesian approaches that have previously appeared in the astronomical literature, this offers a consistent and conservative approach for credible intervals quoted on model parameters for problems where the covariance matrix is itself an estimate.\n\nINTRODUCTION\n\nThe problem of fitting a model to multivariate Normal (hereafter referred to as Gaussian) distributed data, where only an approximation to the true data covariance matrix is available, often arises in astrophysics. In a Bayesian sense, the problem can be considered as jointly fitting a model for the data and the covariance matrix, which is a standard one in statistics with a long history. For Gaussian-distributed data, the standard estimate of the covariance matrix is drawn from a Wishart distribution, such as when a covariance matrix is estimated using a limited number of simulations, or when a covariance matrix is constructed from Jackknife samples (e.g. Norberg et al. 2009;Friedrich et al. 2016). Examples of cosmological inferences made within this framework include the recent measurements from BOSS and eBOSS (Alam et al. 2017;eBOSS Collaboration et al. 2020) as well as the galaxy clustering part of Heymans et al. (2021). For analyses of 2-point statistics in line-of-sight projected data the \u2605 E-mail: will.percival@uwaterloo.ca covariance matrix is often modelled analytically instead of estimating it from simulations (see e.g. Krause & Eifler 2017;Heymans et al. 2021;DES Collaboration et al. 2021, for recent examples). This is because the 4-point functions constituting those covariances are accurately approximated in a Gaussian model, that is easy to evaluate (Joachimi et al. 2021;Friedrich et al. 2020). In contrast, analyses of non-standard summary statistics almost exclusively rely on estimated covariances, because analytical covariance models are not easily obtained for them (e.g. Kacprzak et al. 2016;Gruen et al. 2018;Brouwer et al. 2018;Martinet et al. 2018;Halder et al. 2021).\n\nThere are two common ways to characterise our uncertainty about a model parameter when comparing data and model, which lie at the heart of the difference between Bayesian and frequentist approaches. One can perform a Bayesian analysis using the posterior to define credible intervals, within which a model parameter falls with a particular probability given the prior information and experimental data. One can also define a mechanism to produce frequentist confidence regions, a set proportion of which contain the true parameters in repeated trials. For astrophysical problems we can consider the trials to be experiments performed in parallel universes that are independent and identically distributed realisations of the same data generating process (so the universal constants are considered the same). Confidence regions determined, for example, by the distribution of the difference between truth and the maximum likelihood solution, will not in general be the same as the credible regions, and it is self-evidently wrong to identify them for asymmetric distributions (see e.g. Loredo 2012). That they are not generally the same is evident since credible regions are clearly dependent on the prior, while maximum likelihood estimates are not. In other words: the fraction of times the credible intervals contain the true parameters for repeated analyses (the frequentist coverage probability) is not necessary equal to the posterior probability enclosed within these intervals. The difference has previously been used in astrophysics to search for unrecognized biases during data analysis (Sellentin & Starck 2019).\n\nIn this paper, we seek a prior that gives a frequentist matching posterior, so that we can define credible regions that have the property that, for a given parametrisation, the x% credible regions contain the true parameter values in approximately x% of repeated trials. This means that we can interpret the mechanism used to define these regions (the Bayesian mechanism) as providing confidence regions with a frequentist coverage probability that matches the Bayesian probability associated with interpreting the same regions as credible regions. This match always holds in the asymptotic limit of infinite data (the Bernstein-von Mises theorem), which includes having a perfect covariance matrix estimate; here our prior ensures the distributions match at the level of equal parameter covariances, for Gaussian linear models and approximately for nonlinear models.\n\nNote that, in general, frequentist matching priors are not a panacea, as they may not perform well in all circumstances, such as in making predictive distributions (Sun & Berger 2006), and they are not invariant to reparametrisation. Note also that the differences between the different priors diminish, as expected, when the number of simulations is large and the posterior is dominated by data.\n\nBefore we introduce the problem further and the frequentist matching solution, we introduce the notation adopted: 0 are the compressed experimental data of dimension (e.g. a power spectrum), while is the simulated data with 1 \u2264 \u2264 , assumed to be Gaussian distributed around the true model. From the simulations, we construct an unbiased estimate of the covariance matrix ,\n= 1 \u2212 1 \u2211\ufe01 =1 ( \u2212\u00af) ( \u2212\u00af) ,(1)\nwhere\u00afis the mean of over all simulations. The expectation value of 0 is , and \u03a3 its (unknown) covariance. We only use the simulated data to calculate , and so we consider the data to be ( 0 , ). We will consider fitting a model with parameters , such that our model for the data is ( ), while the covariance matrix used to form the posterior remains of dimension . Without loss of generality we shall assume that the expected values of and are zero, such that they can be ignored in our equations and we can, for example, write the covariance for estimates of\u02c6as \u02c6\u02c6 .\n\nErrors in the covariance matrix used to determine the likelihood have a number of effects on the inferences we make from the data, and particularly the credible intervals quoted in a Bayesian analysis. Hartlap et al. (2007) was the first to point out in the astronomical literature that, for calculated using Eq. 1 and therefore drawn from a Wishart distribution with degrees of freedom \u2212 1 and scale matrix \u03a3/( \u22121), \u22121 is a biased estimator for the inverse covariance matrix\n\u03a3 \u22121 , whereas (\u210e ) \u22121 is not, where \u210e = \u2212 1 \u2212 \u2212 2 (2)\nis commonly (by astronomers) called the Hartlap factor (although knowledge of this effect reaches at least as far back as Kaufman 1967). We discuss the application of the Hartlap factor further in Section 8. Taking a frequentist stance, Dodelson & Schneider (2013) and Taylor & Joachimi (2014) showed that the nature of has a strong effect on the confidence intervals derived based on the distribution of maximum a posteriori probability (MAP) model parameters (commonly called the best-fit parameters). In fact, we will show later that for the priors and linear models that we consider, the maximum likelihood and MAP parameters are the same. So, we could have considered this distribution as the distribution of maximum likelihood solutions. However, as most analyses only work with the posterior, we simply refer to these as the MAP model parameters. Dodelson & Schneider (2013) provided a second order calculation deriving the distribution of MAP model parameters recovered after repeated experiments, averaging over a set of estimated covariance matrices. This derivation is reviewed in Section 3.3. Percival et al. (2014) pointed out that the offset found by Dodelson & Schneider (2013) cannot be applied directly to change credible intervals as the average posterior from a set of repeated experiments itself depends on the distribution of , and they provided a factor by which the credible intervals recovered assuming a Gaussian posterior could be adjusted to match the confidence intervals obtained from the distribution of MAP parameters recovered from mocks. This is discussed further in Section 7.\n\nThe Bayesian solution was introduced in the astronomical literature by Sellentin & Heavens (2016) based on the independence Jeffreys prior and marginalising over the unknown covariance matrix. The resulting posterior has multivariate t-distribution form. The derivation follows from Bayes theorem, starting from the joint posterior\n( , \u03a3| 0 , ) \u221d ( 0 , | , \u03a3) ( , \u03a3) ,(3)\nwhere ( , \u03a3) is the prior, and ( 0 , | , \u03a3) the likelihood. Because of the independence of 0 and , the likelihood can be written\n( 0 , | , \u03a3) = ( 0 | , \u03a3) ( |\u03a3) .(4)\nTo make model inferences, we wish to know the distribution of the data-generating mechanism (or its parameters) given the data and , which we can calculate by marginalising over the true covariance:\n( | 0 , ) = \u222b \u03a3 ( , \u03a3| 0 , ) .(5)\nThe key question in a Bayesian analysis performed under these conditions is the form for the prior proposed for and the covariance matrix. Sun & Berger (2006) listed a number of options for prior choices, including the Jeffreys prior,\n( , \u03a3) \u221d |\u03a3| \u2212 +2 2 ,(6)\nand independence Jeffreys prior (adopted by Sellentin & Heavens 2016),\n( , \u03a3) \u221d |\u03a3| \u2212 +1 2 .(7)\nGiesser & Cornfield (1963) consider a range of priors\n( , \u03a3) \u221d |\u03a3| \u2212 ,(8)\nwhere is an integer with \u2264 . Various other potential priors have also been introduced (e.g. Haar prior, right-Haar prior, left-Haar prior, Chang & Eaves 1990 reference prior) with more complicated forms. Each has advocates and interesting properties in various situations.\n\nThe prior that we introduce is a member of the class of frequentist matching priors (Lindley 1958;Welch & Peers 1963;Reid et al. 2003), designed to match a posterior to frequentist expectations. A discussion of such priors is given in Ghosh (2011). Priors that match posterior predictive probabilities with the corresponding frequentist probabilities are attractive when constructing credible / confidence intervals. In general, matching priors can be constructed only for particular models and matching is determined by the order of approximation to the integrated probability. The selection of a matching prior is usually accompanied by a discussion of the degree of matching, with various definitions of matching available (e.g. Reid et al. 2003). Although matching is usually considered between cumulative probabilities, we match on the expected model parameter covariance. This second moment is commonly used as the basis for model parameter confidence intervals in physics, and can be broadly interpreted as fixing the multi-dimensional \"width\" of a distribution.\n\nMatching priors are candidates for non-informative priors in Bayesian inference, in that it is often assumed (explicitly or not) that the frequentist-style determination of confidence intervals incorporates no information from a prior. Really, there is simply no such thing as a non-informative prior. The frequentist philosophy is different from the Bayesian approach and provides different guarantees across notionally repeated experiments. However, given that the concept of \"errors\" is often interpreted according to the frequentist philosophy, we think there is merit in making the widths of the errors consistent.\n\nMatching priors (and frequentist analyses) violate the Likelihood Principle by using priors that vary with the sampling distribution of the experiment to be performed and the dimension of the model parameter space onto which the data distribution is projected. However, in general they only rely on the performance characteristics of that distribution under repeated sampling, as a way to \"break the tie\" among a choice of prior distributions, in order to draw an inference. Thus, while there is debate about their validity and usage, it is clear that there are situations where they are useful.\n\nIn this paper, we argue that the analyses presented in Hartlap et al. (2007) and Dodelson & Schneider (2013) provide a method for calculating frequentist based confidence intervals for model parameters, and we show that these can be matched to credible intervals obtained from a Bayesian analysis as advocated by Sellentin & Heavens (2016). A similar calculation was performed by Percival et al. (2014) but we now use the methodology and resulting form for the posterior adopted by Sellentin & Heavens (2016), albeit using a different prior. This demonstrates how these different methods are related and the different assumptions being (sometimes implicitly) made when adopting one of these procedures for determining and quoting the coverage probability associated with an interval. The frequentist matched credible intervals are larger than those from Bayesian analyses with previously used priors, and hence this matching can also be considered conservative for inferences made from experiments.\n\nThe layout of our paper is as follows: Section 2 introduces the Bayesian problem that we want to solve, and considers how the posterior depends on the prior chosen, extending the Sellentin & Heavens (2016) approach to more general priors. Section 3 considers probabilities under the posterior and relates them to the distribution of the truth after repeated trials, allowing us to define a frequentist matching prior in Section 4. Section 5 demonstrates this approach using the simple problem of fitting a mean to correlated data, using both analytic derivations and Monte Carlo simulations. We apply our approach to a realistic cosmological analysis in Section 6, fitting mock tomographic cosmic shear data vector including auto-and cross-correlations matching that expected from the 5-year data of the Dark Energy Survey, demonstrating that this works well in a practical test, providing Bayesian credible intervals on model parameters that match the expected frequentist confidence intervals. We summarise our proposed method in Section 7, and conclude in Section 8.\n\n\nCHOICE OF PRIOR TO USE IN A MODEL FIT\n\nIn this section we consider a full Bayesian analysis of the problem, considering different choices for the prior.\n\n\nPosterior with an independence Jeffreys prior\n\nThe uninformative nature of the independence Jeffreys prior in general was introduced at the very start of Bayesian statistics (Jeffreys 1939) and is discussed in this specific situation in Sun & Berger (2006). It assumes for Gaussian data a uniform prior for the means, and a Jeffreys prior for the covariance matrix with means given (Berger & Sun 2008). The derivation of the posterior using this choice of prior, and application to astronomical situations was presented in Sellentin & Heavens (2016).\n\nWe assume the independence Jeffreys joint prior on the expectation value of the data and its covariance matrix given by Eq. 7. To calculate the required posterior using Eq. 3, we first note that follows a Wishart distribution, , and we can write\n(\u03a3| ) \u221d ( |\u03a3/( \u2212 1), \u2212 1) ( , \u03a3) ,(9)\u221d |\u03a3| \u2212 + 2 exp \u2212 \u2212 1 2 (\u03a3 \u22121 ) ,(10)\u221d \u22121 (\u03a3|( \u2212 1) , \u2212 1) ,(11)\nwhich shows how, with this prior, the posterior for \u03a3 has an inverse Wishart distribution, \u22121 . The definitions of the multivariate distributions used in our work are included in Appendix A.\n\nWe now multiply by the Gaussian likelihood ( 0 | , \u03a3), which is simplest to consider in the form given in Appendix A, and integrate over \u03a3 to find that\n( | 0 , ) \u221d \u222b \u03a3 |\u03a3| \u2212 + +1 2 exp \u2212 1 2 (\u03a3 \u22121 ) ,(12)\nwhere\n= ( \u2212 1) + ( 0 \u2212 )( 0 \u2212 ) .(13)\nThis is an integral over the unnormalised inverse Wishart distribution (with parameter ), so we can read off the result from the normalisation constant in Eq. A2.\n( | 0 , ) \u221d | | \u2212 2 .(14)\nComparing with the form of the multivariate t-distribution in Eq. A4, we see that\n( | 0 , ) = , \u2212 0 , \u2212 1 \u2212 ,(15)\nwhich has mean 0 and covariance\n( \u2212 0 )( \u2212 0 ) = \u2212 1 \u2212 \u2212 2 = \u210e .(16)\nThe use of the multivariate t-distribution as a replacement for the Gaussian assumption is often advocated on the grounds of robustness to outliers (Lange et al. 1989), with the parameter , which in our context is \u2212 used as a robustness tuning factor. In this section we have shown how it also arises when the covariance matrix is itself a random variable. It is also interesting to see that, with an independence Jeffreys prior, the Hartlap factor emerges in the recovered covariance, which could be considered natural given that using this prior brings in no further information on the posterior, and the inclusion of the Hartlap factor in some sense unbiases the posterior covariance. However, inferences made from the posterior about the covariance on model parameters are biased by the inclusion of this factor -while it unbiases the posterior against repeated trials of , inferences about model parameter covariances made from the posterior are biased -and so it is not clear that this is what we actually want (see Section 8 for further discussion of this). We also note that a Gaussian posterior with a Hartlap correction yields a posterior covariance that agrees with that derived here, but has tail probabilities that are lower than the t-distribution, and may be in considerable error when datasets in tension are discussed and compared (see Appendix D).\n\nIn the next section we see that the multivariate t-distribution form for the posterior follows from any prior that is a power-law in |\u03a3|, and that the exponent of the power-law affects the recovered credible intervals.\n\n\nPosterior with a general power-law prior\n\nLet us now consider a more general joint prior on the mean and covariance matrix\n( , \u03a3) \u221d |\u03a3| \u2212 \u2212 + +1 2 .(17)\nThe independence Jeffreys prior of Sellentin & Heavens (2016) corresponds to = . Both priors are uniform in the mean, which makes sense for a location parameter. The exact linear form for the exponent is chosen to simplify the downstream analysis, but is not important. It changes our conditional likelihood\n(\u03a3| ) \u221d |\u03a3| \u2212 + 2 exp \u2212 \u2212 1 2 (\u03a3 \u22121 ) ,(18)\nand we now have that\n( | 0 , ) \u221d \u222b \u03a3|\u03a3| \u2212 + +1 2 exp \u2212 1 2 (\u03a3 \u22121 ) ,(19)\nwhere is given by Eq. 13. The form of this equation still matches that of an unnormalised inverse Wishart distribution, but with different parameters, so we now have\n( | 0 , ) \u221d | | \u2212 2 .(20)\nFollowing through the derivation,\n( | 0 , ) = , \u2212 0 , \u2212 1 \u2212 .(21)\nFrom the known properties of the multivariate t-distribution, this has mean 0 and covariance\n( \u2212 0 ) ( \u2212 0 ) = \u2212 1 \u2212 \u2212 2 .(22)\nAs expected, setting = gets us back to Eq. 16, and an expected covariance of \u210e . The covariance recovered from the distribution is directly related to the prior through -as is natural in a Bayesian analysis.\n\n\nMODEL PARAMETER COVARIANCES FROM POSTERIORS AND FROM THE PARAMETER DISTRIBUTION\n\nWe now consider different methods for characterising our uncertainty about model parameters by comparing the model parameter covariances calculated using different assumptions.\n\nGiven a set of data ( 0 , ) and a prior parameterised by , we first determine the Fisher matrix (Section 3.1) and then consider the model parameter covariance derived by computing probabilities under the posterior (Section 3.2). In order to construct a matching prior, for probabilities estimated using the Fisher matrix and probabilities calculated under the posterior, we need to determine the frequentist coverage probability that can be associated with the derived credible intervals. Formally, the coverage probability is a property of the procedure for constructing frequentist confidence intervals, and gives the proportion of repeated trials for which the interval contains the true value of interest. As we want to be able to interpret x% credible intervals as x% confidence intervals, we need to calculate the average size of the credible intervals of fixed probability over repeated trials. Finding the prior for which this is equal to the probability of finding the truth within each interval after repeated trials would then mean that we could interpret Bayesian credible intervals containing a particular probability with the same coverage probability. For simplicity, we work with the covariance rather than the intervals directly and hence we wish to know the average model parameter covariance recovered from the Fisher matrix or the posterior over repeated trials. For this, the multivariate t-distribution posterior has some differences from the expectation for a Gaussian posterior because the covariance of the posterior around the MAP model parameters depends on 0 in addition to . Consequently, the distribution assumed for the data is important as we demonstrate by contrasting results assuming the data is drawn from a t-distribution, or from a Gaussian as is correct for our problem. The dependence of the model parameter covariance on 0 also affects data compression as we show in Appendix C.\n\nWe contrast the covariance estimated by integrating under the posterior with that calculated for the distribution of MAP solutions given the truth in Section 3.3, formally showing that, for our problem, they are very different for most choices of prior. In Section 4 we present the prior that matches these results.\n\n\nUsing the Fisher matrix\n\nThe Fisher information matrix (or simply the Fisher matrix), defined as\n( ) = log ( 0 | ) log ( 0 | ) ,(23)\nis a function of the likelihood. In Bayesian inference, the Bernsteinvon Mises theorem provides the basis for using the Fisher matrix to provide confidence statements on parametric models, and the Cram\u00e9r-Rao theorem shows that it forms a lower bound for the covariance of unbiased estimators of . In our case, we work from the posterior, as given in Eq. 21, and convert this to a likelihood assuming a uniform prior (albeit possibly improper) on the model parameters. Thus, in this section, we are not calculating the Fisher matrix from the true likelihood of the data (remember that 0 are drawn from a Gaussian distribution with covariance \u03a3), but instead we use the Fisher matrix to estimate the expected information given the form of the posterior assumed. We start by assuming that, around the peak of the posterior, we can define a patch of parameter space for which we can apply Bayes theorem to Eq. 21 with a uniform prior on . For this patch the likelihood for 0 is\n( 0 | , ) = , \u2212 0 , \u2212 1 \u2212 .(24)\nThe Fisher information matrix for the multivariate t-distribution with degrees of freedom and covariance \u03a3 (Lange et al. 1989;Sellentin & Heavens 2017) is\n= ( + ) ( \u2212 2) ( + + 2) \u03a3 \u22121 .(25)\nWe see an extra term compared with the true Fisher Information matrix if the covariance matrix were known:\n\u03a3 = \u03a3 \u22121 .(26)\nFor completeness, the Gaussian Fisher Information matrix with covariance matrix is\n= \u22121 .(27)\nFor the likelihood of Eq. 24, we have = \u2212 degrees of freedom and a covariance ( \u2212 1) /( \u2212 \u2212 2), so we have\n= ( \u2212 ) ( + 2) ( \u2212 1) .(28)\nThis is the t-distribution Fisher matrix given the approximate scale matrix .\n\nAs discussed at the start this section, we also want to determine the average credible interval that would be recovered given a set of realisations of drawn from a Wishart distribution (i.e. by observers in parallel universes). To calculate this, we note that a property of the Wishart distribution is that for\n( |\u03a3) = ( |\u03a3/( \u2212 1), \u2212 1) ,(29)\nand a \u00d7 matrix, then\n( ( \u22121 ) \u22121 |\u03a3) = ( \u22121 ) \u22121 ( \u03a3 \u22121 ) \u22121 \u2212 1 , \u2212 + \u2212 1 ,(30)\n(see theorem 3.2.11 of Muirhead 1982). Thus, from Eq. 27, and using the mean of the Wishart distribution, we have that\n\u22121 = \u2212 + \u2212 1 \u2212 1 \u22121 \u03a3 .(31)\nThis equation can also be approximated by writing (\u210e ) \u22121 as a perturbation around \u03a3 \u22121 and considering the second order terms, as discussed in Appendix B, and used in Percival et al. (2014). For the t-distribution Fisher matrix, from Eq. 28, we have that\n\u22121 = ( + 2) ( \u2212 + \u2212 1) ( \u2212 ) \u22121 \u03a3 .(32)\nThis shows that the error in the covariance matrix has an additional effect on the average model parameter credible intervals derived from a set of realisations of the scale matrix.\n\n\nComputing probabilities under the posterior\n\nWe now consider credible intervals derived by computing probabilities under the posterior, based on the 2nd moment of the distribution. While the Fisher matrix gives the form of the likelihood around the expected value, calculating probabilities under the posterior is the more common approach used for model parameter credible interval determination. We consider the case where we have a linear model with = , for some generally non-square matrix . Using Eq. 21 the posterior can be written\n( | 0 , ) \u221d 1 + 1 \u2212 1 ( 0 \u2212 ) \u22121 ( 0 \u2212 ) \u2212 2 .(33)\nThis can be manipulated to describe the posterior as a distribution around the MAP estimate. For a simple example of this for a Gaussian posterior, and a single-parameter model -fitting the mean to data -see Appendix E1. The same derivation can be seen in Appendix E2 for the case of fitting the mean using a t-distribution posterior. Keeping to a more general linear model, expanding the distribution, we have\n( | 0 , ) \u221d 1 + 0 \u22121 0 \u2212 2 \u22121 0 + \u22121 \u2212 1 \u2212 2 ,(34)\nusing the symmetry of \u22121 to simplify the cross terms. Setting\n= \u22121 and = \u22121 0 gives ( | 0 , ) \u221d 1 + 0 \u22121 0 \u2212 \u22121 + ( \u2212 \u22121 ) ( \u2212 \u22121 ) \u2212 1 \u2212 2 .(35)\nTo finish the derivation, we need to complete the square, noting that if we now define\n= ( \u2212 \u22121 ) \u2212 1 \u2212 \u2212 1 2 1 + 0 \u22121 0 \u2212 \u22121 \u2212 1 \u2212 1 2 ,(36)\nthen the posterior reduces to the simple form\n( | 0 , ) \u221d 1 + \u22121 \u2212 \u2212 2 .(37)\nThis shows that is distributed with a multivariate t-distribution with \u2212 degrees of freedom, such that the mean = 0, and covariance = ( \u2212 ) \u22121 /( \u2212 \u2212 2). We can write in the form = + , which has the property that = + , and ( \u2212\u02c6)( \u2212\u02c6) = 2 . From this, we see that the distribution of has mean\u02c6= = \u22121 . The covariance of around this for any value of 0 and is\n( \u2212\u02c6)( \u2212\u02c6) = \u2212 1 \u2212 \u2212 2 \u22121 1 + 0 \u22121 0 \u2212 \u22121 \u2212 1 .(38)\nFor a linear model, this expression can be used instead of integrating under the posterior for any realisation of the data ( 0 , ). Crucially, unlike the equivalent calculation for the Gaussian distribution (see Appendix E1 for this calculation in the special case of fitting the mean to data), the model parameter covariance depends on the value of 0 . Thus, the size of the credible intervals we derive from our fit will change if we change the data.\n\nWe now consider the model parameter covariance recovered by integrating under the posterior, averaged over a set of values of 0 and . We start by considering 0 distributed according to the tdistribution, and a Wishart distributed . However, while we adopt a posterior that has multivariate t-distribution form, the data itself are actually Gaussian distributed with covariance \u03a3, and so we consider this case afterwards.\n\n\nData distributed according to the t-distribution\n\nWe can now calculate the expected covariance recovered for the model parameters, averaging over multiple realisations of the data ( 0 , ). We start by assuming that the same covariance matrix approximation is used for all realisations. In this case, \u22121 is fixed, and we need to replace the terms 0 \u22121 0 and \u22121 by the relevant expected values. To calculate these, we make use of the fact that we have set up the problem such that 0 is the zero vector, and make use of the identity 0 \u22121 0 = ( \u22121 0 0 ). We find that, for a set of data drawn from a multivariate t-distribution as in Eq. 24, we have\n0 \u22121 0 = \u2212 1 \u2212 \u2212 2 ,(39)\u22121 = \u2212 1 \u2212 \u2212 2 .(40)\nPutting these values in to Eq. 38, the covariance for reduces to\n( \u2212\u02c6) ( \u2212\u02c6) = \u2212 1 \u2212 \u2212 2 \u22121 .(41)\nThe expectation over multiple matrices drawn from a Wishart distribution can easily be calculated using Eq. 31,\n( \u2212\u02c6) ( \u2212\u02c6) , = \u2212 + \u2212 1 \u2212 \u2212 2 \u22121 \u03a3 .(42)\n\nGaussian distributed data\n\nFor a set of data drawn from a Gaussian distribution with covariance \u03a3, we have\n0 \u22121 0 = [ \u22121 \u03a3] ,(43)\u22121 = [ \u22121 \u22121 \u03a3 \u22121 ] .(44)\nTo go one step further and consider the expected model parameter covariance allowing for multiple matrices drawn from a Wishart distribution, we now need to find expressions for the expectation of all of the terms in Eq. 38. We have Eq. 31 for \u22121 , and\n\u22121 [ \u22121 \u03a3] [ + ( ( + 1) \u2212 2)] \u22121 \u03a3 , (45) \u22121 [ \u22121 \u22121 \u03a3 \u22121 ] [ + ( ( + 1) \u2212 2)] \u22121 \u03a3 ,(46)\nwhere is given in Eq. B2. To get these expressions, we have used the perturbative expressions as described in Appendix B.\n\nThe end result is that we should expect the average model parameter covariance recovered integrating under the posterior after repeated trials where the data is drawn from a Gaussian distribution with true covariance \u03a3, and is drawn from a Wishart distribution to be\n( \u2212\u02c6) ( \u2212\u02c6) , \u2212 1 + ( \u2212 ) \u2212 \u2212 2 \u22121 \u03a3 ,(47)\nto second order. The difference between this expression and that of Eq. 42 shows the importance of the distribution of 0 in calculating the average model parameter covariance recovered. The situation with Gaussian distributed data matches the setup of our problem: that of considering observers in multiple universes.\n\n\nThe distribution of the difference between MAP estimate and the truth\n\nWe now contrast these estimates of the model parameter covariance against the distribution of recovered maximum a posteriori model parameter values recovered from reruns of the experiment being performed. A linear model is assumed, so we have the symmetry that the distribution of MAP solutions about the truth is the same as the distribution of the truth around a particular MAP solution (when the truth is sampled from a uniform prior). By comparing the results in Section 3.2 to those from a Gaussian posterior, we see that the MAP estimate for the model parameters is the same whether using a Gaussian or t-distribution posterior and so we do not need to distinguish between these choices. We therefore start assuming a Gaussian posterior distribution as in Dodelson & Schneider (2013). As discussed in Section 3.2, the MAP estimate for a linear model can be written\n= \u22121 = \u22121 \u22121 0 ,(48)\nwhich can also be recovered as the first order solution for more general models by Taylor expanding the posterior around the MAP estimates of the model parameters. Here we have assumed, without loss of generality, that the true values are\u02c6= 0.\n\nWe can now obtain an estimate of the scatter on model parameters provided by different experiments, where we consider different 0 drawn from a Gaussian distribution, and from a Wishart distribution given the true model \u02c6\u02c6 , . To do this, we use the fact that 0 0 = \u03a3, so that\n\u02c6\u02c6 = \u22121 \u22121 \u03a3 \u22121 \u22121 .(49)\nThis can be solved to second order, using the expression in Eq. B1, considering an expansion of (\u210e ) \u22121 around \u03a3 \u22121 . As described in Appendix B, the second order solution is\n\u02c6\u02c6 , [1 + ( \u2212 )] \u22121 \u03a3 ,(50)\nwhich is the distribution of MAP estimates made from a set of simulations that is independent of those used to estimate the covariance matrix . This was the primary result of Dodelson & Schneider (2013). Because we assume a linear model, this model parameter covariance is also that of the distribution of the truth around the MAP solution, assuming a uniform prior on the model parameters. It is therefore the covariance of the distribution from which frequentist confidence intervals on model parameters are derived.\n\n\nFREQUENTIST MATCHING PRIOR\n\nWe now consider how to derive a matching prior that will allow the average model parameter covariance derived from the Bayesian analysis described above to match the recovered covariance of the truth around the MAP estimate. To do this, we compare and match Eqns. 47 & 50, to derive a Bayesian posterior parameterised by match that gives a posterior distribution that, averaged over multiple trials, has a model parameter covariance that matches the distribution of MAP estimates that we would get from repeating the experiment. This assumes that, for these repeated trials, 0 is drawn from a Gaussian distribution around the true cosmological model. In this case, the equation for match is\nmatch = + 2 + \u2212 1 + ( \u2212 ) 1 + ( \u2212 ) .(51)\nThe resulting values of match are compared in Fig. 1 for a range of values of , , and . As can be seen, match tends towards the Sellentin & Heavens (2016) solution =\n\nfor large values of . However there are differences, especially when \u223c and the posterior is more influenced by the prior than when many more simulations are available. We note that this is derived under a number of assumptions, particularly that of a linear model, and so this is still an approximation to a true matched posterior given a more complicated shape and non-linear model dependence. In particular, we caution that the moment-matching prior is not invariant to reparametrisation. We find that the exponent for = 2, = 1 is very close to that derived from the right-Haar prior (based on Cholesky decomposition of the covariance matrix), which has some exact matching properties for Gaussian variables (Sun & Berger 2006).\n\n\nTESTING WITH A SIMPLE MEAN FITTING MODEL\n\nThe resulting covariance matrix for the model parameters is tested and explored by considering a simple model -that of fitting a mean value to correlated data. We create Monte Carlo simulations that step through different realisations of the data (Gaussian distributed with covariance \u03a3, chosen for convenience to be the identity matrix) and analysed with covariance matrix drawn from a Wishart distribution (degrees of freedom \u2212 1 and scale matrix \u03a3). Inferences are made about credible intervals assuming different choices for the posterior, and the derived estimates of the model parameter covariances are then averaged over multiple realisations. Averaging over realisations of the data and covariance matrix in this way most naturally follows the ethos behind the derivation in Section 3.2. We also record the MAP estimates for the model, and consider the distribution of these MAP estimates around the true values and measure the variance of this distribution.\n\nWe create large numbers of realisations of data 0 and covariance matrices and then fit to each assuming different expressions for the posterior. For each covariance matrix , we create different versions of 0 , and we create 100 000 different covariance matrices. To speed up these calculations we use analytic marginalisation over the posterior for each , as outlined in Appendix E, rather than numerically integrating under the posterior for each, and use library routines to calculate realisations of Wishart matrices. We still use Monte Carlo results for different values of , and the distribution of MAP parameters. Results are shown in Fig. 2, which shows that, as expected, we can choose a prior to match the model parameter covariance recovered from the posterior to that calculated in a frequentist style approach where we look at the spread of recovered MAP estimates. Given that this derivation best matched the setup of the Monte Carlo simulations, using match provides an excellent fit to the numerical results.\n\n\nTESTING AGAINST A NON-LINEAR MODEL\n\nTo test the performance of the different posterior distributions discussed in Sections 2 and 4 in a realistic cosmological setting we adopt a mock experiment as also considered by Friedrich & Eifler (2018). They simulated a tomographic cosmic shear data vector including auto-and cross-correlations of \u00b1 in 5 source redshift bins on a survey area of 5000 deg 2 (hence mimicking 5-year data of the Dark The difference is likely due to the perturbative nature of the derivation of the expectation. Green and orange lines and symbols show that we recover the similar distributions from both Hartlap-corrected Gaussian and t-distribution posteriors calculated with an independence Jeffreys prior as advocated in Sellentin & Heavens (2016). This can easily be understood as the posteriors have the same variance. The blue dashed lines and triangles show the result of using a t-distribution posterior with match , corresponding to Eq. 51. As can be seen, this prior is able to match the variance recovered by integrating under the posterior averaged over our realisations, with the scatter recovered from MAP estimates. Energy Survey, cf. their table 1 for details). Overall this data vector contains 450 data points. Around a true data vector computed at a cosmology with (\u03a9 , 8 , 0 ) = (0.3156, 0.831, \u22121) we draw 1000 Gaussian random realisations assuming a theoretical covariance matrix derived using the halo model to describe non-linear clustering.\n\nHere \u03a9 is the present day cosmological matter density, 8 is the rms density fluctuations in spheres of radius 8 h \u22121 Mpc, and 0 is the Dark Energy equation of state parameter. Both the covariance calculation and subsequent analyses of the mock data vectors are carried out with the CosmoLike toolkit (Krause & Eifler 2017).\n\nIn Fig. 3 we show marginalised posterior constraints in the \u03a9 -8 plane obtained from the first three of our random realisations using different posteriors. The grey shaded contours were obtained using the true analytic covariance that was also used to draw our mock data vectors. The orange contours assume that there is a covariance estimate from 650 simulations (i.e. 200 more than data points) and that this estimate is used in the posterior of Sellentin & Heavens (2016) to obtain the constraints (we draw a new covariance estimate for each data vector from a Wishart distribution). Note that all contours within each individual panel of Fig. 3 are derived from the same data vector realisations. Despite that, there is a noticeable additional scatter between the two sets of contours -this is exactly the effect of additional scatter of MAP estimates due to noisy covariance estimates described by Dodelson & Schneider (2013). The blue contours are the modified version of the posterior with a match prior chosen to to match this additional scatter.\n\nTo assess the performance of our matched prior more quantitatively we run our Markov Chain Monte Carlo routine to explore the posteriors around all 1000 random realisations of our data vector. Fig. 4 compares how often the true cosmology underlying our numerical experiment is located inside the 68% (left panel) and 95% (right panel) confidence regions of the full 3-dimensional parameter space when using different covariance matrices and different posterior distributions. Here we are considering the credible intervals derived from our Bayesian analysis work as frequentist confidence intervals. The grey band in each panel assumes that the true covariance is known. The green crosses represent the commonly used approach of a Gaussian likelihood with Hartlap corrected precision matrix as estimated from different numbers of simulations (x-axis in both panels). The orange dots use the independence Jeffreys prior advocated by Sellentin & Heavens (2016) and the resulting t-distribution instead of the Hartlap-corrected Gaussian likelihood. The blue triangles show the coverage achieved with a matched prior that uses Eq. 51 to compute the exponent . This likelihood indeed manages to achieve coverage factions of approximately 68% and 95% respectively. The red squares show the coverage obtained from simply re-scaling the Gaussian loglikelihood in the manner advocated by Percival et al. (2014), which is also close to 68% and 95% respectively. The dash-dotted line show the coverage that is expected for the standard Gaussian likelihood based on the calculations of Dodelson & Schneider (2013).\n\n\nSUMMARY\n\nOur suggested way forwards is quite simple -in situations where the covariance matrix for Gaussian data is itself a random variable drawn from a Wishart distribution with \u2212 1 degrees of freedom, for example when it is constructed from mock samples, then we propose a frequentist matching prior that is uniform in and depends on \u03a3 as |\u03a3| \u2212( \u2212 + +1)/2 , leading to a posterior\n( | 0 , ) \u221d 1 + 2 ( \u2212 1) \u2212 2 ,(52)where 2 = ( 0 \u2212 ) \u22121 ( 0 \u2212 ) .(53)\nThe power law index is given by Eq. 51, and repeated here for completeness\n= + 2 + \u2212 1 + ( \u2212 ) 1 + ( \u2212 ) ,(54)= ( \u2212 \u2212 2) ( \u2212 \u2212 1)( \u2212 \u2212 4) ,(55)\nwhere is the number of data points and the number of parameters. This will lead to credible intervals that can also be interpreted as confidence intervals with approximately the same coverage probability. Note that this expression does not require any extra factors of \u210e, or other terms -i.e. is the approximate covariance matrix, and \u22121 its inverse. This enables a Bayesian analysis, with a matching prior designed with this frequency-matching property. In general, this procedure increases the model parameter credible intervals compared with those derived from the more usual independence Jeffreys prior on the true data covariance, and therefore can be considered a more conservative choice for making deductions from data.\n\nIf the reader prefers to approximate the posterior using a Gaussian distribution, then rather than inverting or \u210e , the matrix ( ) \u22121 to be used when calculating 2 should be the inverse of\n= ( \u2212 1) [1 + ( \u2212 )] \u2212 + \u2212 1 ,(56)\nwhich matches the method proposed in Percival et al. (2014), replacing one of the approximations used there with an exact expression.\n\nTo derive this, consider the factor by which we must multiply Eq. 31 to obtain Eq. 50 -matching the model parameter covariance expected from integrating under the posterior with that from the distribution of MAP solutions. Both the Gaussian approximation and our preferred t-distribution solution give model parameter covariances that are very similar to the suggestion of Friedrich & Eifler (2018) \n\n\nCONCLUSIONS\n\nThe primary result in our paper is presented in Section 7, which provides a frequentist-matching prior: i.e. the exponent in a power law prior on the determinant of the true data covariance matrix required to yield a posterior model parameter covariance matching the distribution of true parameter values with respect to maximum likelihood estimates (and vice-versa for the linear models we consider). Our analysis lies at the interface between Bayesian and frequentist analyses: allowing an analysis that results in multiple interpretations of the same parameter intervals with the same probability. In order to derive this, we have assumed a linearised model, but have demonstrated broader applicability using a realistic non-linear model fit. Note that, in general, our results will not be valid for arbitrary nonlinear models or reparametrisations. The use of this formalism for parameter inference when the covariance matrix is itself approximate offers a way to satisfy scientists whose intuition is based on frequentist style measures and those who wish for the analysis to be Bayesian in construct (which is often simpler for practical application).\n\nWe initially considered an independence Jeffreys prior on the true covariance matrix, as advocated in Sellentin & Heavens (2016). We showed that this leads to a posterior with covariance around the  . Comparing how often the true cosmology underlying our numerical experiment of Sec. 6 is found inside the 68% (left panel) and 95% (right panel) credible regions when using different covariance matrices and different posterior distributions. Note that we are comparing how well a credible region works as a confidence interval, and hence this test is not fair. The grey band in each panel assumes that the true covariance is known, in which case, with the problem being considered the credible interval also works as a confidence interval. The width of the band indicates the expected credible interval containing a coverage probability of 68% from 1000 realisations, assuming a binomial distribution for the number of successes. Hence the horizontal black dashed line, which indicated the expected value does not have to lie in the middle of this band. The green crosses represent the common approach of a Gaussian likelihood with Hartlap corrected precision matrix for different numbers of simulations used to estimate the covariance matrix (x-axis in both panels). The vertical dotted line marks = .\n\nThe orange dots use the independence Jeffreys prior advocated by Sellentin & Heavens (2016) and the resulting t-distribution instead of the Hartlap corrected Gaussian likelihood. The blue triangles show the coverage achieved with a matched prior that uses Eq. 51 to compute the exponent , and the red squares show the coverage obtained from simply rescaling the Gaussian log-likelihood in the manner advocated by Percival et al. (2014), as in Eq. 56. The dash-dotted line show the coverage that is expected for the t-distribution posterior calculated using the independence Jeffreys prior. model parameters that matches that assuming a Gaussian posterior after scaling the data covariance matrix by the Hartlap factor. The derived model parameter covariance does not match that from the distribution of MAP estimates found by Dodelson & Schneider (2013), which is understandable given that they are calculating different distributions. We have considered alternative priors that are powers of the determinant of the true covariance matrix and which yield posteriors with frequentist coverage, at least at the level of covariance of the distributions. Using this allows the interpretation of credible intervals as confidence intervals with approximately the same probability. Because of the choice of a power-law prior, the posteriors of interest have the form of a multivariate t-distribution. For this form, the distribution of the posterior around the MAP estimate depends on the specific data realisation -this can clearly be seen in Eq. 38. In comparison, for a Gaussian posterior, the distribution around the MAP estimates is independent of the data and depends only on the data covariance matrix . This complicates the matching. We therefore consider the recovered model parameter covariance averaged over a set of data: here the distribution of that data matters. Formally, we calculate the frequentist coverage probability for a set of credible intervals, with a view to matching this probability to that from the distribution of MAP solutions.\n\nAlthough we have a t-distribution posterior, the distribution of data is Gaussian, and so we cannot directly use either the t-distribution Fisher matrix (this led to expected covariance on model parameters as in Eq. 32), or integrate under the posterior assuming the data is distributed according to a multivariate t-distribution (leading to Eq. 42). Instead, we have to consider the Gaussian distribution of data when determining the average model parameter covariance that would be recovered from the posterior after repeated trials (giving Eq. 47). We also note that this dependence on the data complicates data compression: the credible intervals recovered from compressed data do not necessarily match those recovered from the full data even for linear models where the compression is optimally performed to give the same MAP estimates (see Appendix C).\n\nThe prior that we advocate depends on the properties of the data and the problem, particularly , and . Having priors that depend on the expected form of the posterior is quite common (although they should obviously not depend on the actual data observed), especially in the objective Bayesian approach (see Heavens & Sellentin 2018 for an application to cosmology), so we do not see this as a fundamental problem, although it does conflict with the Bayesian notion of the prior as an expression of the state of knowledge before the experiment is performed.\n\nOne might also worry that our matching criterion is, in a sense, linking the posterior and properties of the data that depend on the likelihood. But the posterior should answer the question of what is the truth given the data, while the likelihood considers the data given the truth. These are fundamentally different things, and so why are we matching posterior and likelihood widths? If we compare the covariance inherent in the likelihood and the posterior for multivariate Gaussian distributions, then we might consider an approximate link where 2 post = 2 like + 2 prior . This would be exact if all distributions were Gaussian, or we were working in the Gaussian limit. In this limit, the standard prior on the covariance used in the posterior directly adds to the covariance we assume for our experimental result. Translating through to model parameters, both contributions still contribute. So we see that the prior choice is related to the credible interval quoted for experimental measurements and forms the link between posterior and likelihood. A prior is chosen such that it does not change this covariance, and so in this sense our matching prior is an uninformative prior for the model parameters.\n\nUsing the multivariate t-distribution posterior makes the analysis attractive in a Bayesian sense, as it matches the problem with fewer approximations. In general, approximating the posterior as Gaussian has a relatively small effect on the posterior surface for 1 and 2 intervals, and in the examples we have considered less so than the choice of prior (see Appendix D). Even so, we recommend using the multivariate t-distribution with the revised prior as this represents a consistent Bayesian approach. Moreover, the tail probabilities can be much greater than those of the equivalent gaussian, which can be in error when tensions between datasets are considered. In this case, we need to be careful about the interpretation of confidence intervals, as discussed in Appendix D. For those that cannot contemplate a posterior with a form other than Gaussian, we have included the alternative correction to use instead of the Hartlap factor for an approximate Gaussian posterior in Section 7. When = , Eq. 51 gives that = + + 1, and the prior reduces to |\u03a3| \u2212( +1) . For this prior, the covariance of the posterior distribution as given in Eq. 22 reduces to . From the properties of the Wishart distribution, this has expected value \u03a3 matching the covariance of the frequentist distribution from which the data were assumed to be drawn. Note that no factor of \u210e is required in the posterior, or in the Gaussian approximation to get this result. To understand why not, note that the rationale often used to justify using a Gaussian posterior based on a covariance \u210e (i.e. including a factor \u210e) is that the inverse matrix \u22121 is a biased estimate of \u03a3 \u22121 , and this is corrected by using \u210e rather than . Thus the argument goes that we should use \u210e in the posterior. However, we should consider that the model parameter covariance derived from the posterior is biased in the opposite way requiring an extra factor \u210e \u22121 following the same rationale. To see this, consider Eq. 30, which shows that the model parameter covariance from a set of repeated trials each with a different (with no \u210e factor) is Wishart distributed with expectation given by a function of \u03a3. Where = , and we fit for the values of , the derived covariance reduces to with expectation \u03a3, matching that we would expect given the Gaussian distribution of the data. Including \u210e would have biased our errors compared to this expected value. Thus, explicitly including the Hartlap factor in a posterior to correct for a bias in \u22121 is not just wrong from a Bayesian standpoint, but the standard rationale for its application misses a crucial step. Our proposed posterior consistently corrects for any potential biases due to having skewed distributions without any need for extra ad-hoc factors.\n\nFinally we note that we form a matching prior based on the recovered model parameter covariance and not the distribution, as is more standard in statistical analyses. We do this because the covariance of the posterior distribution for model parameters offers a simple way to match the \"width\" of two distributions, and that we can determine simple results for a power-law prior where we only have one degree of freedom and so only one degree of matching is possible. An extension to this work would be to consider varying the form of the prior beyond a simple power-law of the determinant of the true data covariance matrix to better match the shape of the posterior, in line with the more standard matching criterion used in statistics. We could also have directly compared credible intervals and confidence intervals -i.e. averaged over rather than the model parameter covariance where necessary, but we do not expect that this would change our results significantly compared with our chosen matching criterion based on covariance. Reid N., Mukerjee R., Fraser D. A. S., 2003, Lecture Notes-Monograph Series, 42, 31 Sellentin E., Heavens A. F., 2016, MNRAS, 456, L132 Sellentin E., Heavens A. F., 2017, MNRAS, 464, 4658 Sellentin E., Starck J.-L., 2019, J. Cosmology Astropart. Phys., 2019, 021 Sun D., Berger J., 2006 ., Joachimi B., Kitching T., 2013, MNRAS, 432, 1928Welch B. L., Peers H. W., 1963, Journal of the Royal Statistical Society. Series B (Methodological), 25, 318 eBOSS Collaboration et al., 2020, arXiv e-prints, p. arXiv:2007 \n\n\nAPPENDIX A: MULTIVARIATE DISTRIBUTIONS\n\nSome multivariate distributions with data dimension are listed here for reference:\n\nThe Wishart distribution\n( | , ) = | | \u2212 \u22121 2 exp \u2212 1 2 ( \u22121 ) 2 2 | | 2 \u0393 2 ,(A1)\nwhere is the degrees of freedom, and the scale matrix. The mean is [ ] = , and the variance is\nVar[ ] = [ 2 \u2212 ]. The inverse Wishart distribution \u22121 ( | , ) = | | 2 | | \u2212 + +1 2 exp \u2212 1 2 ( \u22121 ) 2 2 \u0393 2 ,(A2)\nwhere is the degrees of freedom, and the scale matrix. The mean is [ ] = /( \u2212 \u2212 1). The multivariate Normal or Gaussian distribution written in a form using the Trace operator\n( 0 | , ) = (2 ) \u2212 2 | | \u2212 1 2 exp \u2212 1 2 \u22121 ( 0 \u2212 )( 0 \u2212 ) ,(A3)\nwith mean [ 0 ] = and variance Var[ 0 ] = .\n\nThe multivariate t-distribution\n, ( 0 | , ) = \u0393[( + )/2] \u0393( /2)( ) /2 | | 1/2 \u00d7 1 + ( 0 \u2212 ) ( ) \u22121 ( 0 \u2212 ) \u2212 + 2 ,(A4)\nwhere is the degrees of freedom, and the scale matrix. The mean is [ 0 ] = , and the variance is Var[ 0 ] = \u22122 .\n\n\nAPPENDIX B: PERTURBATIVE BASED APPROACH FOR EXPRESSIONS INVOLVING THE COVARIANCE OF WISHART-DISTRIBUTED MATRICES\n\nIn this Appendix, we consider the perturbation based approach to understanding the biases involved in a statistical analysis of data when the covariance matrix itself is a random variable . To do this, we use the expressions between estimated and true covariance matrix as provided by Taylor et al. (2013). Let (\u210e ) \u22121 = \u03a3 \u22121 + \u0394 \u03a3 \u22121 . As is drawn from a Wishart distribution, the errors \u0394 \u03a3 \u22121 can be written\n(\u0394 \u03a3 \u22121 ) (\u0394 \u03a3 \u22121 ) = \u03a3 \u22121 \u03a3 \u22121 + (\u03a3 \u22121 \u03a3 \u22121 + \u03a3 \u22121 \u03a3 \u22121 ) , (B1) where = 2 ( \u2212 \u2212 1)( \u2212 \u2212 4) , = ( \u2212 \u2212 2) ( \u2212 \u2212 1)( \u2212 \u2212 4) .(B2)\nFirst, we consider a perturbative expansion of \u22121 = \u210e \u22121 ( \u03a3 + \u0394 ) \u22121 , with \u0394 defined as a standard Gaussian Fisher matrix with inverse covariance \u0394 \u03a3 \u22121 as required in Section 3.1. Expanding this, and taking the expected value, the first order terms in \u0394 tend to zero (as (\u210e ) \u22121 is an unbiased estimator of \u03a3 \u22121 ), and so we are only interested in the second order term in \u0394 , which can be written\n( \u03a3 + \u0394 ) \u22121 . . = \u22121 \u03a3 \u0394 \u22121 \u03a3 \u0394 \u22121 \u03a3 .(B3)\nPutting the relationships given in Eq. B1 into Eq. B3, we find that\n\u22121 \u210e \u22121 [1 + + ( + 1)] \u22121 \u03a3 .(B4)\nThe calculation of the inverse Fisher matrix averaged over using this perturbation based approach was performed in Percival et al. (2014) for the Gaussian Fisher matrix. As shown in the derivation leading to Eq. 32, this expression does not have to be solved perturbatively as an exact solution is possible. The non-perturbative solution is given in Eq. 31. The next expression that we wish to understand perturbatively is \u22121 [ \u22121 \u03a3] , as required in Section 3.2.2 and given in Eq. 45. The expression for which we are taking the expectation can be written\n( \u22121 [ \u22121 \u03a3]) = [ \u22121 ] \u22121 \u03a3 .(B5)\nThe second order term from \u22121 is given by Eq. B4, leading a term [1+ + ( +1)], with the factor \u210e coming from the summation over the term \u22121 \u03a3 . There is also a second order cross term from \u22121 and \u22121 , which gives \u2212[ + 2 ]. Adding these together, we find the result in Eq. 45.\n\nTo approximate the expression in Eq. 46, note that there are eight possible ways that we can have pairs of \u0394 \u03a3 \u22121 in\n[ \u22121 ] \u22121 \u03a3 \u22121 [ \u22121 ] ,(B6)\nwith one at second order from each \u22121 , the cross pair between the two \u22121 and the cross pair from the two \u22121 , and four cross pairs between \u22121 and \u22121 . Treating each in turn and expanding using Eq. B1 leads to the result in Eq. 46. Finally, we note that the expression in Eq. 50 can be derived similarly. To see this, note that there are eight possible ways that we can have pairs of \u0394 \u03a3 \u22121 in\n\u02c6\u02c6 , = [ \u22121 ] \u22121 \u03a3 \u22121 [ \u22121 ] ,(B7)\nsimilar to the expansion of Eq. B6. These expressions are different -for example in the limit as \u210e \u2192 \u03a3, Eq. B6 tends towards ( \u22121 \u03a3 ) , while Eq. B7 tends towards ( \u22121 \u03a3 ) . Treating each of the eight possible combinations of two \u0394 \u03a3 \u22121 separately, expanding using Eq. B1 and summing the terms gives the result in Eq. 50, which was the primary result of Dodelson & Schneider (2013).\n\n\nAPPENDIX C: COMPRESSING THE DATA\n\nThe effect of a linear compression of the data on model parameter inference can be considered using a property of the multivariate t-distribution. For some \u00d7 matrix , assuming that\n( | 0 , ) = , \u2212 0 , \u2212 1 \u2212 ,(C1)\nthen a property of the multivariate t-distribution is that\n( | 0 , ) = , \u2212 0 , \u2212 1 \u2212 . (C2)\nNow consider an analysis of the compressed data, where we apply a compression with = and\n= \u22121 \u22121 ,(C3)\nsuch that the MAP estimate\u02c6= 0 , and\n= \u22121 \u22121 \u22121 \u22121 = \u22121 .(C4)\nFor data analysed with a Gaussian posterior and linear model, such a compression is sufficient in that the analysis of the reduced data gives the same inferences as those from the full data set, including the covariance on . Assuming a t-distribution posterior for 0 , we find that the posterior for the reduced data is\n( | 0 , ) = , \u2212 0 , \u2212 1 \u2212 .(C5)\nNow, defining = , as an estimator for the MAP values, we see that\n( |\u02c6, ) = , \u2212 \u02c6, \u2212 1 \u2212 \u22121 .(C6)\nThis gives that the covariance for is\n( \u2212\u02c6) ( \u2212\u02c6) = \u2212 1 \u2212 \u2212 2 \u22121 .(C7)\nThis is the covariance recovered from the compressed data as given by Eq. C3, for a measurement of the MAP estimates\u02c6. This does not match the expression in Eq. 38, but does match the solution of Eq. 41 where we integrate under the posterior and then average over 0 , assuming that this was drawn from a multivariate t-distribution. Our interpretation of this is that the linear compression of the data analysed with a t-distribution posterior does not include information about the distribution of the data around the MAP estimate, as is used in Eq. 38 to determine a specific model parameter covariance for that realisation of the data. Without this extra information, compressing the data means that the model parameter covariance recovered corresponds to the average for a distribution of 0 , rather than that for a particular 0 recovered if using more data. Furthermore the model parameter covariance corresponds to that recovered on average for data distributed according to a multivariate t-distribution. We therefore conclude that data compression works differently than when analysing using a Gaussian posterior for which linear compression is sufficient in terms of giving the same MAP estimate and covariance. For multivariate t-distribution posteriors, this is not the case, and additional information is used on the distribution of the data around the MAP estimate in order to determine the model parameter covariance as shown in Eq. 38. This will be considered further in future work.\n\n\nAPPENDIX D: INTERPRETATION OF CREDIBLE INTERVALS BASED ON\n\nWe now consider how the use of a multivariate t-distribution affects the interpretation of confidence intervals. Where credible intervals  Figure D2. The difference between the linear factors required to define credible intervals containing a fixed probability for t-distribution and Gaussian posteriors. Intervals are defined using the Gaussian probability within the \u00b1 interval, where = 1 ... 5. So, for example, tracing the 5 curve (solid black line), we see that for = 100, to match the Gaussian \u00b15 coverage probability, we would need to consider a \u00b15.29 interval for the t-distribution. are derived directly from the posterior, for example, by considering the fraction of points within a given interval for a MCMC chain exploring a posterior volume, then the interpretation of results is correct whatever the form of the posterior. However, if one wants to define or interpret intervals based on \u00b1 contours, then one needs to be careful when interpreting a posterior with t-distribution form, as explored in this Appendix.\n\nAs our favoured solution assumes a power law prior, the posterior, when written in terms of the model parameters for linear models, has a multivariate t-distribution form with degrees of freedom = \u2212 . When marginalised over other parameters, the posterior probability for each model parameter has a form matching the student t-distribution for the parameter \u221a\ufe01 ( \u2212 2)/ / . In general, the t-distribution has broader tails and a narrower core than the Gaussian distribution, matching the Gaussian distribution in the limit \u2192 \u221e. The variance of the standard t-distribution is /( \u2212 2), and so we need a broader range of integration to determine a \u00b1 interval, integrating over \u00b1 \u221a\ufe01 /( \u2212 2) rather than \u00b1 as with a Gaussian for distribution with unit variance. The probabilities associated with credible intervals based on \u00b1 are compared in Fig. D1: the \u00b11 credible interval is more probable for the t-distribution compared with the Gaussian distribution with the same variance. However, the tail probabilities are larger for the t-distribution than the Gaussian to fixed \u00b1 limits for \u2265 2. Fig. D2 instead shows the change in required to match tail probabilities from the t-distribution to those from the Gaussian distribution. For example, with a t-distribution posterior with = 100, one would need to define an interval based on the \u00b15.29 threshold to match the inference (including tail probabilities) made from a 5 result with a Gaussian posterior. We would therefore need to integrate to larger intervals for the t-distribution to reduce the tail probabilities to match the Gaussian values for \u2265 2. For smaller we need to integrate to larger intervals in .\n\n\nAPPENDIX E: ANALYTIC MARGINALISATION FOR ESTIMATING THE MEAN OF DATA\n\nIn this Appendix we outline the derivations that allow us to significantly speed up our Monte Carlo simulations fitting a single mean value\u00afto correlated data values 0 , and ultimately would make them superfluous as we could perform all of the necessary calculations analytically. These are a special case of the derivation given in Section 3.2.2 and are therefore not strictly necessary, but we include it as we feel that it gives insight into the problem being solved. To help with this, we first consider the more familiar case of a Gaussian posterior.\n\n\nE1 Fitting the mean with a multivariate Gaussian posterior\n\nWe start with the simple case of a Gaussian posterior. For this, we can use the standard definition of 2 = \u22122 ln for fitting a meant o data 0 with inverse covariance matrix (\u210e ) \u22121 2 \u2261 \u2211\ufe01 (( 0 ) \u2212\u00af) (\u210e ) \u22121 ( 0 ) \u2212\u00af .\n\nExpanding, we can write\n2 = \u210e \u22121 1 \u2212 2 2\u00af+ 3\u00af2 ,(E2)where 1 = \u2211\ufe01 ( 0 ) \u22121 ( 0 ) ,(E3)2 = \u2211\ufe01 \u22121 ( 0 ) ,(E4)3 = \u2211\ufe01 \u22121 .(E5)\nTo align with the notation used elsewhere in this paper, we note that for this problem, the parameter =\u00af, the model is =\u00af, and we have / = 1, = 3 and \u22121 = 1/ 3 . The derivative = , where the unit vector is a vector of 1's, and 2 = which matches Eq. 38 for a fit to the mean. Thus, rather than numerically integrate under the posterior for any realisation of 0 and , we can instead use this expression for the variance recovered. We have confirmed numerically that this result is correct, and that the variance depends on the data as given in this equation. Unlike for the Gaussian distribution, here the recovered variance depends on the value of 0 through 1 and 2 . These terms do not cancel in general.\n\nWe can now consider the expected value, averaging over multiple sets of data, but using the same covariance matrix approximation to determine the posterior. In this case, 3 is fixed, and we need to replace the terms 1 and 2 2 by the relevant expected values. Remembering that was drawn from a Gaussian distribution with covariance \u03a3 and zero mean, we have\n1 = \u2211\ufe01 [ \u22121 \u03a3] ,(E15)2 2 = \u2211\ufe01 [ \u22121 \u03a3 \u22121 ] .(E16)\nFigure 1 .\n1Variation of the power law exponent match required for a prior that, on average over repeated trials gives a posterior with covariance that matches that expected for the distribution of MAP values (solid lines as given in Eq. 51). We show how match / varies with (x-axis), (different lines) and (different panels).\n\nFigure 2 .\n2Average recovered model parameter variance calculated in different ways when fitting the mean\u00af, to a set of correlated Gaussian data. The grey solid line shows the result that would have been obtained from the posterior if we had known the data covariance matrix perfectly, taking the confidence interval as the root of the variance. The black solid line (model from Dodelson & Schneider 2013) and solid black points (Monte Carlo measurements) show the root of the variance determined from the distribution of recovered values.\n\nFigure 3 .\n3Contours containing 68% and 95% probability, marginalising under the posterior in the \u03a9 -8 plane, obtained from realizations of DES-like weak lensing data vectors. Each panel is for a different random set of data 0 and covariance drawn from Gaussian and Wishart distributions respectively. The relevant parameters of this run for the posterior are = 650, = 450, and = 4. Contours are shown calculated using the true covariance matrix with a Gaussian posterior (grey shading), and two versions of the t-distribution posterior, one with = as derived using an independence Jeffreys prior for the true covariance as in Sellentin & Heavens (2016) (orange), and one using a covariance-matching prior derived for linear models (blue). The dashed lines mark the expected values of both parameters.\n\n\nwhen is small. They proposed multiplying the Sellentin & Heavens (2016) posterior by the Dodelson & Schneider (2013) factor of 1 + ( \u2212 ). To see the empirical similarity, note that the Sellentin & Heavens (2016) posterior gives a covariance for the distribution of around 0 of \u210e , and compare Eq. 56 to \u210e \u00d7 [1 + ( \u2212 )].\n\nFigure 4\n4Figure 4. Comparing how often the true cosmology underlying our numerical experiment of Sec. 6 is found inside the 68% (left panel) and 95% (right panel) credible regions when using different covariance matrices and different posterior distributions. Note that we are comparing how well a credible region works as a confidence interval, and hence this test is not fair. The grey band in each panel assumes that the true covariance is known, in which case, with the problem being considered the credible interval also works as a confidence interval. The width of the band indicates the expected credible interval containing a coverage probability of 68% from 1000 realisations, assuming a binomial distribution for the number of successes. Hence the horizontal black dashed line, which indicated the expected value does not have to lie in the middle of this band. The green crosses represent the common approach of a Gaussian likelihood with Hartlap corrected precision matrix for different numbers of simulations used to estimate the covariance matrix (x-axis in both panels). The vertical dotted line marks = . The orange dots use the independence Jeffreys prior advocated by Sellentin & Heavens (2016) and the resulting t-distribution instead of the Hartlap corrected Gaussian likelihood. The blue triangles show the coverage achieved with a matched prior that uses Eq. 51 to compute the exponent , and the red squares show the coverage obtained from simply rescaling the Gaussian log-likelihood in the manner advocated by Percival et al. (2014), as in Eq. 56. The dash-dotted line show the coverage that is expected for the t-distribution posterior calculated using the independence Jeffreys prior.\n\nFigure D1 .\nD1The ratio of tail probabilities for t-distribution and Gaussian posteriors outside of \u00b1 credible intervals, where = 1 ..\nWill J.Percival et al.   \nMNRAS 000, 1-15(2021)\nMNRAS 000, 1-15(2021)4 Will J.Percival et al.   \nACKNOWLEDGEMENTSWJP acknowledges useful conversations with Michael Matesic and En Long regarding speeding up running the Monte Carlo realisations. We thank Daniel Farewell for helpful comments on an early version of the draft and Tim Eifler for useful comments on the draft and for providing the CosmoLike toolkit. We thank the referee, James Buchanan, for their careful review of the paper and for the corrections and suggestions provided.Research at Perimeter Institute is supported in part by the Government of Canada through the Department of Innovation, Science and Economic Development Canada and by the Province of Ontario through the Ministry of Colleges and Universities.This research was enabled in part by support provided by Compute Ontario (www.computeontario.ca) and Compute Canada (www.computecanada.ca).DATA AVAILABILITYNo data was used in this paper, which is theoretical in nature.\u22121 0 . We now \"complete the square\" for the model dependent part of 2 \u22122 2\u00af+ 3\u00af2 = 3 \u00af\u2212 2 3 2 \u2212 2 2 3 .(E6)We can then write the posterior as a Gaussian distribution around the MAP estimateThe mean, as derived from the posterior therefore has a Gaussian distribution, and the expected value for\u00afand the variance can then be read off, \u00af = 2 / 3 , and 2 = \u210e/ 3 . As expected for a Gaussian posterior and a linear model, the MAP estimate matches the value given in Eq. 48, and the expected model parameter variance integrating under the posterior matches the inverse of the Fisher matrix. So we see that a Gaussian fit to the peak of the posterior also describes the results from the full distribution.E2 Fitting the mean with multivariate t-distribution posteriorThis section replicates Section 3.2.2, but now for the special case of fitting the mean to a set of data, as considered in Section 5. We do this as we used these equations to speed-up the Monte Carlo runs presented in Section 5, and in order to allow them to be used as an aide to understanding the derivation in Section 3.2.2. Consequently, we try to keep the layout and structure similar and make no apologies for replication. We only present the derivation for Gaussian distributed data.Assuming that the posterior has a scale matrix ( \u22121)/( \u2212 ) , and degrees of freedom = \u2212 , as in Eq. 22 we can write the posterior where the model is a constant mean value(\u00af|Expanding as in the Gaussian case, we haveand completing the square givesWe now defineso thatWe see that is distributed with a t-distribution with \u2212 1 degrees of freedom, such that the mean = 0, and the variance 2 = ( \u2212 1)/( \u2212 3).We can write\u00afin the form\u00af= + , which has the property that \u00af = + , and Var(\u00af) = 2 Var( ).From this, we see that the distribution of\u00afhas mean \u00af = 2 / 3 , as expected given the discussion in Section 3.3. The variance for any realisation isThis is the expected result for the variance recovered for many Gaussian distributed realisations of the data 0 . Eq. E14, together with the expressions of Eqns. E15 & E16, allow us not to run Monte Carlo simulations for different data for the same covariance, as we can accurately predict the result using these equations.To go one step further when finding analytic expressions for the Monte Carlo runs, we now need to find expressions for the relevant terms in Eq. E14, now considering the expected values averaging over all possible covariance matrices . We can do this using the expressions given in Section B for the expansion of around the true matrix \u03a3. These giveAs expected, this final two equations match Eqns. 45 & 46 with = 1. For the first expression we write here the perturbative result rather than the exact form as used in Section 3.2.1. In terms of , this is \u22121 = \u210e \u22121 [1 + + ( + 1)] \u22121 \u03a3 . Note that by using these expressions we would have removed any need to do the Monte Carlo simulations, as we have analytic expressions for all stages of the Monte Carlo runs being performed, albeit to second order in the covariance matrix approximation.We can also consider how, for this case of fitting the mean to correlated data, we can derive an analytic expression for the scatter in recovered MAP estimates. To determine this, note from Eq. E9 that the MAP estimate (obtained by taking the log and setting the derivative with respect to\u00afto zero in the posterior) is 2 / 3 . From the definition of these quantities, 2 / 3 = \u22121 0 / 3 . Remembering that 0 are drawn from a Gaussian distribution with covariance \u03a3, we see that 2 / 3 is also Gaussian distributed with zero mean and variance \u22121 \u03a3 \u22121 / 2 3 = 2 2 / 2 3 . Eq. 50 then shows that this matches theDodelson & Schneider (2013)result.A reader having reached this stage of the paper firstly needs congratulating, but also might well be asking why we need to run the Monte Carlo simulations presented in Section 5 at all given that we have analytically approximated all of the results we will extract from those simulations. And they would be correct. However, we keepFig. 2as it adds colour and confirms the validity of the approximations -using the Fisher matrix to determine confidence intervals from the posterior, and the second order expansions through which we estimated the impact of . This paper has been typeset from a T E X/L A T E X file prepared by the author.\n. S Alam, 10.1093/mnras/stx721MNRAS. 4702617Alam S., et al., 2017, MNRAS, 470, 2617\n\n. J Berger, D Sun, Annals of Statistics. 36963Berger J., Sun D., 2008, Annals of Statistics, 36, 963\n\n. M M Brouwer, arXiv:1805.00562preprintBrouwer M. M., et al., 2018, preprint, (arXiv:1805.00562)\n\nThe Annals of Statistics. T Chang, D Eaves, 181595Chang T., Eaves D., 1990, The Annals of Statistics, 18, 1595\n\n. S Dodelson, M D Schneider, 10.1103/PhysRevD.88.063537Phys. Rev. D. 8863537Dodelson S., Schneider M. D., 2013, Phys. Rev. D, 88, 063537\n\n. O Friedrich, T Eifler, 10.1093/mnras/stx2566MNRAS. 4734150Friedrich O., Eifler T., 2018, MNRAS, 473, 4150\n\n. O Friedrich, S Seitz, T F Eifler, D Gruen, 10.1093/mnras/stv2833MNRAS. 4562662Friedrich O., Seitz S., Eifler T. F., Gruen D., 2016, MNRAS, 456, 2662\n\n. O Friedrich, arXiv:2012.08568arXiv e-printsFriedrich O., et al., 2020, arXiv e-prints, p. arXiv:2012.08568\n\n. M Ghosh, arXiv:1108.2120Ghosh M., 2011, arXiv e-prints, p. arXiv:1108.2120\n\n. S Giesser, J Cornfield, Journal of the Royal Statistical Society. Series B. 25368Giesser S., Cornfield J., 1963, Journal of the Royal Statistical Society. Series B, 25, 368\n\n. D Gruen, 10.1103/PhysRevD.98.023507Phys. Rev. D. 9823507Gruen D., et al., 2018, Phys. Rev. D, 98, 023507\n\n. A Halder, O Friedrich, S Seitz, T N Varga, arXiv:2102.10177Halder A., Friedrich O., Seitz S., Varga T. N., 2021, arXiv e-prints, p. arXiv:2102.10177\n\n. J Hartlap, P Simon, P Schneider, 10.1051/0004-6361:20066170A&A. 464399Hartlap J., Simon P., Schneider P., 2007, A&A, 464, 399\n\n. A F Heavens, E Sellentin, 10.1088/1475-7516/2018/04/047J. Cosmology Astropart. Phys. 47Heavens A. F., Sellentin E., 2018, J. Cosmology Astropart. Phys., 2018, 047\n\n. C Heymans, 10.1051/0004-6361/202039063A&A. 646140Heymans C., et al., 2021, A&A, 646, A140\n\nH Jeffreys, B Joachimi, 10.1051/0004-6361/202038831Theory of Probability. OxfordThe Clarendon Press646129Jeffreys H., 1939, Theory of Probability. The Clarendon Press, Oxford Joachimi B., et al., 2021, A&A, 646, A129\n\n. T Kacprzak, 10.1093/mnras/stw2070MNRAS. 4633653Kacprzak T., et al., 2016, MNRAS, 463, 3653\n\n. G M Kaufman, Heverlee, BelgiumCenter for Operations Research and Econometrics, Catholic University of LouvainReportKaufman G. M., 1967, Report No. 6710, Center for Operations Research and Econometrics, Catholic University of Louvain, Heverlee, Belgium\n\n. E Krause, T Eifler, 10.1093/mnras/stx1261Mon. Not. Roy. Astron. Soc. 4702100Krause E., Eifler T., 2017, Mon. Not. Roy. Astron. Soc., 470, 2100\n\n. K L Lange, R J A Little, J M G Taylor, Journal of the American Statistical Association. 84881Lange K. L., Little R. J. A., Taylor J. M. G., 1989, Journal of the American Statistical Association, 84, 881\n\nJournal of the royal statistical society series bmethodological. D Lindley, 20102Lindley D., 1958, Journal of the royal statistical society series b- methodological, 20, 102\n\nAstrostatistical Challenges for the New Astronomy. T J Loredo, 10.1007/978-1-4614-3508-2_2Loredo T. J., 2012, Astrostatistical Challenges for the New Astronomy, p. 15-40\n\n. N Martinet, 10.1093/mnras/stx2793MNRAS. 474712Martinet N., et al., 2018, MNRAS, 474, 712\n\nAspects of Multivariate Statistical Theory. R Muirhead, P Norberg, C M Baugh, E Gazta\u00f1aga, D J Croton, 10.1111/j.1365-2966.2009.14389.xMonthly Notices of the Royal Astronomical Society. 39619WileyMuirhead R., 1982, Aspects of Multivariate Statistical Theory. Wiley, New Jersey Norberg P., Baugh C. M., Gazta\u00f1aga E., Croton D. J., 2009, Monthly Notices of the Royal Astronomical Society, 396, 19\n\n. W J Percival, 10.1093/mnras/stu112MNRAS. 4392531Percival W. J., et al., 2014, MNRAS, 439, 2531\n", "annotations": {"author": "[{\"end\":384,\"start\":113},{\"end\":562,\"start\":385},{\"end\":783,\"start\":563},{\"end\":950,\"start\":784}]", "publisher": null, "author_last_name": "[{\"end\":128,\"start\":120},{\"end\":401,\"start\":392},{\"end\":578,\"start\":569},{\"end\":796,\"start\":789}]", "author_first_name": "[{\"end\":117,\"start\":113},{\"end\":119,\"start\":118},{\"end\":391,\"start\":385},{\"end\":568,\"start\":563},{\"end\":788,\"start\":784}]", "author_affiliation": "[{\"end\":209,\"start\":130},{\"end\":293,\"start\":211},{\"end\":383,\"start\":295},{\"end\":487,\"start\":403},{\"end\":561,\"start\":489},{\"end\":684,\"start\":580},{\"end\":782,\"start\":686},{\"end\":949,\"start\":798}]", "title": "[{\"end\":106,\"start\":1},{\"end\":1056,\"start\":951}]", "venue": "[{\"end\":1063,\"start\":1058}]", "abstract": "[{\"end\":2986,\"start\":1259}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3687,\"start\":3667},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3709,\"start\":3687},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3844,\"start\":3826},{\"end\":3875,\"start\":3844},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3939,\"start\":3918},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4171,\"start\":4150},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4191,\"start\":4171},{\"end\":4242,\"start\":4191},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4409,\"start\":4387},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4431,\"start\":4409},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4637,\"start\":4616},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4655,\"start\":4637},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4675,\"start\":4655},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4696,\"start\":4675},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4714,\"start\":4696},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5814,\"start\":5803},{\"end\":6339,\"start\":6314},{\"end\":7394,\"start\":7375},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8806,\"start\":8785},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9249,\"start\":9236},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9378,\"start\":9351},{\"end\":9407,\"start\":9383},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9995,\"start\":9968},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10241,\"start\":10219},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10306,\"start\":10279},{\"end\":11655,\"start\":11636},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12084,\"start\":12066},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12299,\"start\":12285},{\"end\":12318,\"start\":12299},{\"end\":12335,\"start\":12318},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12448,\"start\":12436},{\"end\":12950,\"start\":12933},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14566,\"start\":14545},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14598,\"start\":14571},{\"end\":14829,\"start\":14803},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14892,\"start\":14870},{\"end\":14998,\"start\":14972},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16906,\"start\":16891},{\"end\":16973,\"start\":16954},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17118,\"start\":17099},{\"end\":17266,\"start\":17240},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18591,\"start\":18572},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24948,\"start\":24929},{\"end\":24972,\"start\":24948},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25903,\"start\":25889},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26203,\"start\":26181},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32248,\"start\":32221},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33302,\"start\":33275},{\"end\":35278,\"start\":35259},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37559,\"start\":37534},{\"end\":38088,\"start\":38062},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":39127,\"start\":39105},{\"end\":39604,\"start\":39578},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":40060,\"start\":40033},{\"end\":41144,\"start\":41118},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":41587,\"start\":41565},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":41787,\"start\":41760},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":43400,\"start\":43378},{\"end\":45178,\"start\":45152},{\"end\":46445,\"start\":46419},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":46789,\"start\":46767},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":47207,\"start\":47180},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":49599,\"start\":49575},{\"end\":54875,\"start\":54832},{\"end\":54949,\"start\":54875},{\"end\":55001,\"start\":54949},{\"end\":55052,\"start\":55001},{\"end\":55089,\"start\":55052},{\"end\":55118,\"start\":55089},{\"end\":55152,\"start\":55119},{\"end\":55170,\"start\":55152},{\"end\":55200,\"start\":55170},{\"end\":55311,\"start\":55200},{\"end\":55342,\"start\":55311},{\"end\":56701,\"start\":56681},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":57620,\"start\":57598},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":59305,\"start\":59278}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":67048,\"start\":66721},{\"attributes\":{\"id\":\"fig_1\"},\"end\":67589,\"start\":67049},{\"attributes\":{\"id\":\"fig_2\"},\"end\":68392,\"start\":67590},{\"attributes\":{\"id\":\"fig_3\"},\"end\":68714,\"start\":68393},{\"attributes\":{\"id\":\"fig_5\"},\"end\":70427,\"start\":68715},{\"attributes\":{\"id\":\"fig_6\"},\"end\":70563,\"start\":70428}]", "paragraph": "[{\"end\":4716,\"start\":3002},{\"end\":6340,\"start\":4718},{\"end\":7209,\"start\":6342},{\"end\":7607,\"start\":7211},{\"end\":7981,\"start\":7609},{\"end\":8581,\"start\":8013},{\"end\":9058,\"start\":8583},{\"end\":10724,\"start\":9114},{\"end\":11057,\"start\":10726},{\"end\":11226,\"start\":11098},{\"end\":11462,\"start\":11264},{\"end\":11731,\"start\":11497},{\"end\":11827,\"start\":11757},{\"end\":11906,\"start\":11853},{\"end\":12199,\"start\":11927},{\"end\":13270,\"start\":12201},{\"end\":13891,\"start\":13272},{\"end\":14488,\"start\":13893},{\"end\":15488,\"start\":14490},{\"end\":16559,\"start\":15490},{\"end\":16714,\"start\":16601},{\"end\":17267,\"start\":16764},{\"end\":17514,\"start\":17269},{\"end\":17807,\"start\":17617},{\"end\":17960,\"start\":17809},{\"end\":18019,\"start\":18014},{\"end\":18214,\"start\":18052},{\"end\":18322,\"start\":18241},{\"end\":18386,\"start\":18355},{\"end\":19789,\"start\":18424},{\"end\":20009,\"start\":19791},{\"end\":20134,\"start\":20054},{\"end\":20472,\"start\":20165},{\"end\":20537,\"start\":20517},{\"end\":20755,\"start\":20590},{\"end\":20815,\"start\":20782},{\"end\":20940,\"start\":20848},{\"end\":21182,\"start\":20975},{\"end\":21442,\"start\":21266},{\"end\":23363,\"start\":21444},{\"end\":23680,\"start\":23365},{\"end\":23779,\"start\":23708},{\"end\":24789,\"start\":23816},{\"end\":24976,\"start\":24822},{\"end\":25118,\"start\":25012},{\"end\":25216,\"start\":25134},{\"end\":25334,\"start\":25228},{\"end\":25440,\"start\":25363},{\"end\":25752,\"start\":25442},{\"end\":25805,\"start\":25785},{\"end\":25984,\"start\":25866},{\"end\":26268,\"start\":26013},{\"end\":26490,\"start\":26309},{\"end\":27029,\"start\":26538},{\"end\":27491,\"start\":27081},{\"end\":27604,\"start\":27543},{\"end\":27775,\"start\":27689},{\"end\":27876,\"start\":27831},{\"end\":28264,\"start\":27908},{\"end\":28769,\"start\":28317},{\"end\":29191,\"start\":28771},{\"end\":29839,\"start\":29244},{\"end\":29949,\"start\":29885},{\"end\":30094,\"start\":29983},{\"end\":30243,\"start\":30164},{\"end\":30544,\"start\":30292},{\"end\":30756,\"start\":30635},{\"end\":31024,\"start\":30758},{\"end\":31385,\"start\":31068},{\"end\":32329,\"start\":31459},{\"end\":32594,\"start\":32351},{\"end\":32871,\"start\":32596},{\"end\":33071,\"start\":32897},{\"end\":33618,\"start\":33100},{\"end\":34339,\"start\":33649},{\"end\":34547,\"start\":34382},{\"end\":35279,\"start\":34549},{\"end\":36290,\"start\":35324},{\"end\":37315,\"start\":36292},{\"end\":38803,\"start\":37354},{\"end\":39128,\"start\":38805},{\"end\":40184,\"start\":39130},{\"end\":41788,\"start\":40186},{\"end\":42174,\"start\":41800},{\"end\":42318,\"start\":42244},{\"end\":43115,\"start\":42388},{\"end\":43305,\"start\":43117},{\"end\":43474,\"start\":43341},{\"end\":43875,\"start\":43476},{\"end\":45048,\"start\":43891},{\"end\":46352,\"start\":45050},{\"end\":48406,\"start\":46354},{\"end\":49266,\"start\":48408},{\"end\":49824,\"start\":49268},{\"end\":51038,\"start\":49826},{\"end\":53796,\"start\":51040},{\"end\":55343,\"start\":53798},{\"end\":55468,\"start\":55386},{\"end\":55494,\"start\":55470},{\"end\":55647,\"start\":55553},{\"end\":55937,\"start\":55762},{\"end\":56046,\"start\":56003},{\"end\":56079,\"start\":56048},{\"end\":56279,\"start\":56167},{\"end\":56806,\"start\":56396},{\"end\":57336,\"start\":56936},{\"end\":57448,\"start\":57381},{\"end\":58038,\"start\":57483},{\"end\":58348,\"start\":58073},{\"end\":58466,\"start\":58350},{\"end\":58888,\"start\":58495},{\"end\":59306,\"start\":58924},{\"end\":59523,\"start\":59343},{\"end\":59614,\"start\":59556},{\"end\":59736,\"start\":59648},{\"end\":59787,\"start\":59751},{\"end\":60132,\"start\":59813},{\"end\":60230,\"start\":60165},{\"end\":60300,\"start\":60263},{\"end\":61832,\"start\":60334},{\"end\":62921,\"start\":61894},{\"end\":64579,\"start\":62923},{\"end\":65207,\"start\":64652},{\"end\":65487,\"start\":65270},{\"end\":65512,\"start\":65489},{\"end\":66315,\"start\":65611},{\"end\":66672,\"start\":66317}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8012,\"start\":7982},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9113,\"start\":9059},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11097,\"start\":11058},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11263,\"start\":11227},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11496,\"start\":11463},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11756,\"start\":11732},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11852,\"start\":11828},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11926,\"start\":11907},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17552,\"start\":17515},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17589,\"start\":17552},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17616,\"start\":17589},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18013,\"start\":17961},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18051,\"start\":18020},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18240,\"start\":18215},{\"attributes\":{\"id\":\"formula_14\"},\"end\":18354,\"start\":18323},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18423,\"start\":18387},{\"attributes\":{\"id\":\"formula_16\"},\"end\":20164,\"start\":20135},{\"attributes\":{\"id\":\"formula_17\"},\"end\":20516,\"start\":20473},{\"attributes\":{\"id\":\"formula_18\"},\"end\":20589,\"start\":20538},{\"attributes\":{\"id\":\"formula_19\"},\"end\":20781,\"start\":20756},{\"attributes\":{\"id\":\"formula_20\"},\"end\":20847,\"start\":20816},{\"attributes\":{\"id\":\"formula_21\"},\"end\":20974,\"start\":20941},{\"attributes\":{\"id\":\"formula_22\"},\"end\":23815,\"start\":23780},{\"attributes\":{\"id\":\"formula_23\"},\"end\":24821,\"start\":24790},{\"attributes\":{\"id\":\"formula_24\"},\"end\":25011,\"start\":24977},{\"attributes\":{\"id\":\"formula_25\"},\"end\":25133,\"start\":25119},{\"attributes\":{\"id\":\"formula_26\"},\"end\":25227,\"start\":25217},{\"attributes\":{\"id\":\"formula_27\"},\"end\":25362,\"start\":25335},{\"attributes\":{\"id\":\"formula_28\"},\"end\":25784,\"start\":25753},{\"attributes\":{\"id\":\"formula_29\"},\"end\":25865,\"start\":25806},{\"attributes\":{\"id\":\"formula_30\"},\"end\":26012,\"start\":25985},{\"attributes\":{\"id\":\"formula_31\"},\"end\":26308,\"start\":26269},{\"attributes\":{\"id\":\"formula_32\"},\"end\":27080,\"start\":27030},{\"attributes\":{\"id\":\"formula_33\"},\"end\":27542,\"start\":27492},{\"attributes\":{\"id\":\"formula_34\"},\"end\":27688,\"start\":27605},{\"attributes\":{\"id\":\"formula_35\"},\"end\":27830,\"start\":27776},{\"attributes\":{\"id\":\"formula_36\"},\"end\":27907,\"start\":27877},{\"attributes\":{\"id\":\"formula_37\"},\"end\":28316,\"start\":28265},{\"attributes\":{\"id\":\"formula_38\"},\"end\":29864,\"start\":29840},{\"attributes\":{\"id\":\"formula_39\"},\"end\":29884,\"start\":29864},{\"attributes\":{\"id\":\"formula_40\"},\"end\":29982,\"start\":29950},{\"attributes\":{\"id\":\"formula_41\"},\"end\":30135,\"start\":30095},{\"attributes\":{\"id\":\"formula_42\"},\"end\":30266,\"start\":30244},{\"attributes\":{\"id\":\"formula_43\"},\"end\":30291,\"start\":30266},{\"attributes\":{\"id\":\"formula_44\"},\"end\":30634,\"start\":30545},{\"attributes\":{\"id\":\"formula_45\"},\"end\":31067,\"start\":31025},{\"attributes\":{\"id\":\"formula_46\"},\"end\":32350,\"start\":32330},{\"attributes\":{\"id\":\"formula_47\"},\"end\":32896,\"start\":32872},{\"attributes\":{\"id\":\"formula_48\"},\"end\":33099,\"start\":33072},{\"attributes\":{\"id\":\"formula_49\"},\"end\":34381,\"start\":34340},{\"attributes\":{\"id\":\"formula_50\"},\"end\":42209,\"start\":42175},{\"attributes\":{\"id\":\"formula_51\"},\"end\":42243,\"start\":42209},{\"attributes\":{\"id\":\"formula_52\"},\"end\":42354,\"start\":42319},{\"attributes\":{\"id\":\"formula_53\"},\"end\":42387,\"start\":42354},{\"attributes\":{\"id\":\"formula_54\"},\"end\":43340,\"start\":43306},{\"attributes\":{\"id\":\"formula_55\"},\"end\":55552,\"start\":55495},{\"attributes\":{\"id\":\"formula_56\"},\"end\":55761,\"start\":55648},{\"attributes\":{\"id\":\"formula_57\"},\"end\":56002,\"start\":55938},{\"attributes\":{\"id\":\"formula_58\"},\"end\":56166,\"start\":56080},{\"attributes\":{\"id\":\"formula_59\"},\"end\":56935,\"start\":56807},{\"attributes\":{\"id\":\"formula_60\"},\"end\":57380,\"start\":57337},{\"attributes\":{\"id\":\"formula_61\"},\"end\":57482,\"start\":57449},{\"attributes\":{\"id\":\"formula_62\"},\"end\":58072,\"start\":58039},{\"attributes\":{\"id\":\"formula_63\"},\"end\":58494,\"start\":58467},{\"attributes\":{\"id\":\"formula_64\"},\"end\":58923,\"start\":58889},{\"attributes\":{\"id\":\"formula_65\"},\"end\":59555,\"start\":59524},{\"attributes\":{\"id\":\"formula_66\"},\"end\":59647,\"start\":59615},{\"attributes\":{\"id\":\"formula_67\"},\"end\":59750,\"start\":59737},{\"attributes\":{\"id\":\"formula_68\"},\"end\":59812,\"start\":59788},{\"attributes\":{\"id\":\"formula_69\"},\"end\":60164,\"start\":60133},{\"attributes\":{\"id\":\"formula_70\"},\"end\":60262,\"start\":60231},{\"attributes\":{\"id\":\"formula_71\"},\"end\":60333,\"start\":60301},{\"attributes\":{\"id\":\"formula_73\"},\"end\":65541,\"start\":65513},{\"attributes\":{\"id\":\"formula_74\"},\"end\":65574,\"start\":65541},{\"attributes\":{\"id\":\"formula_75\"},\"end\":65595,\"start\":65574},{\"attributes\":{\"id\":\"formula_76\"},\"end\":65610,\"start\":65595},{\"attributes\":{\"id\":\"formula_77\"},\"end\":66694,\"start\":66673},{\"attributes\":{\"id\":\"formula_78\"},\"end\":66721,\"start\":66694}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3000,\"start\":2988},{\"attributes\":{\"n\":\"2\"},\"end\":16599,\"start\":16562},{\"attributes\":{\"n\":\"2.1\"},\"end\":16762,\"start\":16717},{\"attributes\":{\"n\":\"2.2\"},\"end\":20052,\"start\":20012},{\"attributes\":{\"n\":\"3\"},\"end\":21264,\"start\":21185},{\"attributes\":{\"n\":\"3.1\"},\"end\":23706,\"start\":23683},{\"attributes\":{\"n\":\"3.2\"},\"end\":26536,\"start\":26493},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":29242,\"start\":29194},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":30162,\"start\":30137},{\"attributes\":{\"n\":\"3.3\"},\"end\":31457,\"start\":31388},{\"attributes\":{\"n\":\"4\"},\"end\":33647,\"start\":33621},{\"attributes\":{\"n\":\"5\"},\"end\":35322,\"start\":35282},{\"attributes\":{\"n\":\"6\"},\"end\":37352,\"start\":37318},{\"attributes\":{\"n\":\"7\"},\"end\":41798,\"start\":41791},{\"attributes\":{\"n\":\"8\"},\"end\":43889,\"start\":43878},{\"end\":55384,\"start\":55346},{\"end\":56394,\"start\":56282},{\"end\":59341,\"start\":59309},{\"end\":61892,\"start\":61835},{\"end\":64650,\"start\":64582},{\"end\":65268,\"start\":65210},{\"end\":66732,\"start\":66722},{\"end\":67060,\"start\":67050},{\"end\":67601,\"start\":67591},{\"end\":68724,\"start\":68716},{\"end\":70440,\"start\":70429}]", "table": null, "figure_caption": "[{\"end\":67048,\"start\":66734},{\"end\":67589,\"start\":67062},{\"end\":68392,\"start\":67603},{\"end\":68714,\"start\":68395},{\"end\":70427,\"start\":68726},{\"end\":70563,\"start\":70443}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34434,\"start\":34428},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36939,\"start\":36933},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":39139,\"start\":39133},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":39778,\"start\":39772},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":40385,\"start\":40379},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":62042,\"start\":62033},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":63766,\"start\":63759},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":64015,\"start\":64008}]", "bib_author_first_name": "[{\"end\":75895,\"start\":75894},{\"end\":75980,\"start\":75979},{\"end\":75990,\"start\":75989},{\"end\":76082,\"start\":76081},{\"end\":76084,\"start\":76083},{\"end\":76204,\"start\":76203},{\"end\":76213,\"start\":76212},{\"end\":76292,\"start\":76291},{\"end\":76304,\"start\":76303},{\"end\":76306,\"start\":76305},{\"end\":76430,\"start\":76429},{\"end\":76443,\"start\":76442},{\"end\":76539,\"start\":76538},{\"end\":76552,\"start\":76551},{\"end\":76561,\"start\":76560},{\"end\":76563,\"start\":76562},{\"end\":76573,\"start\":76572},{\"end\":76691,\"start\":76690},{\"end\":76801,\"start\":76800},{\"end\":76879,\"start\":76878},{\"end\":76890,\"start\":76889},{\"end\":77055,\"start\":77054},{\"end\":77163,\"start\":77162},{\"end\":77173,\"start\":77172},{\"end\":77186,\"start\":77185},{\"end\":77195,\"start\":77194},{\"end\":77197,\"start\":77196},{\"end\":77315,\"start\":77314},{\"end\":77326,\"start\":77325},{\"end\":77335,\"start\":77334},{\"end\":77444,\"start\":77443},{\"end\":77446,\"start\":77445},{\"end\":77457,\"start\":77456},{\"end\":77610,\"start\":77609},{\"end\":77701,\"start\":77700},{\"end\":77713,\"start\":77712},{\"end\":77921,\"start\":77920},{\"end\":78015,\"start\":78014},{\"end\":78017,\"start\":78016},{\"end\":78270,\"start\":78269},{\"end\":78280,\"start\":78279},{\"end\":78416,\"start\":78415},{\"end\":78418,\"start\":78417},{\"end\":78427,\"start\":78426},{\"end\":78431,\"start\":78428},{\"end\":78441,\"start\":78440},{\"end\":78445,\"start\":78442},{\"end\":78685,\"start\":78684},{\"end\":78846,\"start\":78845},{\"end\":78848,\"start\":78847},{\"end\":78968,\"start\":78967},{\"end\":79102,\"start\":79101},{\"end\":79114,\"start\":79113},{\"end\":79125,\"start\":79124},{\"end\":79127,\"start\":79126},{\"end\":79136,\"start\":79135},{\"end\":79149,\"start\":79148},{\"end\":79151,\"start\":79150},{\"end\":79456,\"start\":79455},{\"end\":79458,\"start\":79457}]", "bib_author_last_name": "[{\"end\":75900,\"start\":75896},{\"end\":75987,\"start\":75981},{\"end\":75994,\"start\":75991},{\"end\":76092,\"start\":76085},{\"end\":76210,\"start\":76205},{\"end\":76219,\"start\":76214},{\"end\":76301,\"start\":76293},{\"end\":76316,\"start\":76307},{\"end\":76440,\"start\":76431},{\"end\":76450,\"start\":76444},{\"end\":76549,\"start\":76540},{\"end\":76558,\"start\":76553},{\"end\":76570,\"start\":76564},{\"end\":76579,\"start\":76574},{\"end\":76701,\"start\":76692},{\"end\":76807,\"start\":76802},{\"end\":76887,\"start\":76880},{\"end\":76900,\"start\":76891},{\"end\":77061,\"start\":77056},{\"end\":77170,\"start\":77164},{\"end\":77183,\"start\":77174},{\"end\":77192,\"start\":77187},{\"end\":77203,\"start\":77198},{\"end\":77323,\"start\":77316},{\"end\":77332,\"start\":77327},{\"end\":77345,\"start\":77336},{\"end\":77454,\"start\":77447},{\"end\":77467,\"start\":77458},{\"end\":77618,\"start\":77611},{\"end\":77710,\"start\":77702},{\"end\":77722,\"start\":77714},{\"end\":77930,\"start\":77922},{\"end\":78025,\"start\":78018},{\"end\":78277,\"start\":78271},{\"end\":78287,\"start\":78281},{\"end\":78424,\"start\":78419},{\"end\":78438,\"start\":78432},{\"end\":78452,\"start\":78446},{\"end\":78693,\"start\":78686},{\"end\":78855,\"start\":78849},{\"end\":78977,\"start\":78969},{\"end\":79111,\"start\":79103},{\"end\":79122,\"start\":79115},{\"end\":79133,\"start\":79128},{\"end\":79146,\"start\":79137},{\"end\":79158,\"start\":79152},{\"end\":79467,\"start\":79459}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1093/mnras/stx721\",\"id\":\"b0\"},\"end\":75975,\"start\":75892},{\"attributes\":{\"id\":\"b1\"},\"end\":76077,\"start\":75977},{\"attributes\":{\"doi\":\"arXiv:1805.00562\",\"id\":\"b2\"},\"end\":76175,\"start\":76079},{\"attributes\":{\"id\":\"b3\"},\"end\":76287,\"start\":76177},{\"attributes\":{\"doi\":\"10.1103/PhysRevD.88.063537\",\"id\":\"b4\"},\"end\":76425,\"start\":76289},{\"attributes\":{\"doi\":\"10.1093/mnras/stx2566\",\"id\":\"b5\"},\"end\":76534,\"start\":76427},{\"attributes\":{\"doi\":\"10.1093/mnras/stv2833\",\"id\":\"b6\"},\"end\":76686,\"start\":76536},{\"attributes\":{\"doi\":\"arXiv:2012.08568\",\"id\":\"b7\"},\"end\":76796,\"start\":76688},{\"attributes\":{\"doi\":\"arXiv:1108.2120\",\"id\":\"b8\"},\"end\":76874,\"start\":76798},{\"attributes\":{\"id\":\"b9\"},\"end\":77050,\"start\":76876},{\"attributes\":{\"doi\":\"10.1103/PhysRevD.98.023507\",\"id\":\"b10\"},\"end\":77158,\"start\":77052},{\"attributes\":{\"doi\":\"arXiv:2102.10177\",\"id\":\"b11\"},\"end\":77310,\"start\":77160},{\"attributes\":{\"doi\":\"10.1051/0004-6361:20066170\",\"id\":\"b12\"},\"end\":77439,\"start\":77312},{\"attributes\":{\"doi\":\"10.1088/1475-7516/2018/04/047\",\"id\":\"b13\"},\"end\":77605,\"start\":77441},{\"attributes\":{\"doi\":\"10.1051/0004-6361/202039063\",\"id\":\"b14\"},\"end\":77698,\"start\":77607},{\"attributes\":{\"doi\":\"10.1051/0004-6361/202038831\",\"id\":\"b15\"},\"end\":77916,\"start\":77700},{\"attributes\":{\"doi\":\"10.1093/mnras/stw2070\",\"id\":\"b16\"},\"end\":78010,\"start\":77918},{\"attributes\":{\"id\":\"b17\"},\"end\":78265,\"start\":78012},{\"attributes\":{\"doi\":\"10.1093/mnras/stx1261\",\"id\":\"b18\"},\"end\":78411,\"start\":78267},{\"attributes\":{\"id\":\"b19\"},\"end\":78617,\"start\":78413},{\"attributes\":{\"id\":\"b20\"},\"end\":78792,\"start\":78619},{\"attributes\":{\"doi\":\"10.1007/978-1-4614-3508-2_2\",\"id\":\"b21\"},\"end\":78963,\"start\":78794},{\"attributes\":{\"doi\":\"10.1093/mnras/stx2793\",\"id\":\"b22\"},\"end\":79055,\"start\":78965},{\"attributes\":{\"doi\":\"10.1111/j.1365-2966.2009.14389.x\",\"id\":\"b23\",\"matched_paper_id\":123513635},\"end\":79451,\"start\":79057},{\"attributes\":{\"doi\":\"10.1093/mnras/stu112\",\"id\":\"b24\"},\"end\":79549,\"start\":79453}]", "bib_title": "[{\"end\":79099,\"start\":79057}]", "bib_author": "[{\"end\":75902,\"start\":75894},{\"end\":75989,\"start\":75979},{\"end\":75996,\"start\":75989},{\"end\":76094,\"start\":76081},{\"end\":76212,\"start\":76203},{\"end\":76221,\"start\":76212},{\"end\":76303,\"start\":76291},{\"end\":76318,\"start\":76303},{\"end\":76442,\"start\":76429},{\"end\":76452,\"start\":76442},{\"end\":76551,\"start\":76538},{\"end\":76560,\"start\":76551},{\"end\":76572,\"start\":76560},{\"end\":76581,\"start\":76572},{\"end\":76703,\"start\":76690},{\"end\":76809,\"start\":76800},{\"end\":76889,\"start\":76878},{\"end\":76902,\"start\":76889},{\"end\":77063,\"start\":77054},{\"end\":77172,\"start\":77162},{\"end\":77185,\"start\":77172},{\"end\":77194,\"start\":77185},{\"end\":77205,\"start\":77194},{\"end\":77325,\"start\":77314},{\"end\":77334,\"start\":77325},{\"end\":77347,\"start\":77334},{\"end\":77456,\"start\":77443},{\"end\":77469,\"start\":77456},{\"end\":77620,\"start\":77609},{\"end\":77712,\"start\":77700},{\"end\":77724,\"start\":77712},{\"end\":77932,\"start\":77920},{\"end\":78027,\"start\":78014},{\"end\":78279,\"start\":78269},{\"end\":78289,\"start\":78279},{\"end\":78426,\"start\":78415},{\"end\":78440,\"start\":78426},{\"end\":78454,\"start\":78440},{\"end\":78695,\"start\":78684},{\"end\":78857,\"start\":78845},{\"end\":78979,\"start\":78967},{\"end\":79113,\"start\":79101},{\"end\":79124,\"start\":79113},{\"end\":79135,\"start\":79124},{\"end\":79148,\"start\":79135},{\"end\":79160,\"start\":79148},{\"end\":79469,\"start\":79455}]", "bib_venue": "[{\"end\":75927,\"start\":75922},{\"end\":76016,\"start\":75996},{\"end\":76201,\"start\":76177},{\"end\":76356,\"start\":76344},{\"end\":76478,\"start\":76473},{\"end\":76607,\"start\":76602},{\"end\":76952,\"start\":76902},{\"end\":77101,\"start\":77089},{\"end\":77376,\"start\":77373},{\"end\":77526,\"start\":77498},{\"end\":77650,\"start\":77647},{\"end\":77772,\"start\":77751},{\"end\":77958,\"start\":77953},{\"end\":78336,\"start\":78310},{\"end\":78501,\"start\":78454},{\"end\":78682,\"start\":78619},{\"end\":78843,\"start\":78794},{\"end\":79005,\"start\":79000},{\"end\":79241,\"start\":79192},{\"end\":79494,\"start\":79489},{\"end\":77780,\"start\":77774}]"}}}, "year": 2023, "month": 12, "day": 17}