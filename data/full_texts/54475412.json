{"id": 54475412, "updated": "2023-10-04 15:11:50.751", "metadata": {"title": "Fast Online Object Tracking and Segmentation: A Unifying Approach", "authors": "[{\"first\":\"Qiang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Luca\",\"last\":\"Bertinetto\",\"middle\":[]},{\"first\":\"Weiming\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Philip\",\"last\":\"Torr\",\"middle\":[\"H.S.\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 12, "day": 12}, "abstract": "In this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 35 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semi-supervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is http://www.robots.ox.ac.uk/~qwang/SiamMask.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1812.05050", "mag": "2963227409", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Wang0BHT19", "doi": "10.1109/cvpr.2019.00142"}}, "content": {"source": {"pdf_hash": "02a5b7a41ffa8518eb3b7cae9914a2bd2bbc886b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1812.05050v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1812.05050", "status": "GREEN"}}, "grobid": {"id": "ef99a1fdd76eba84f9c9b8d1a8afda7bfbc1dd5d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/02a5b7a41ffa8518eb3b7cae9914a2bd2bbc886b.txt", "contents": "\nFast Online Object Tracking and Segmentation: A Unifying Approach\n\n\nQiang Wang qiang.wang@nlpr.ia.ac.cn \nLi Zhang \nWeiming Hu wmhu@nlpr.ia.ac.cn \nCasia \nPhilip H S Torr philip.torr@eng.ox.ac.uk \n\nCASIA\nUniversity of Oxford\nLuca Bertinetto Five AI\n\n\nUniversity of Oxford\n\n\nFast Online Object Tracking and Segmentation: A Unifying Approach\n\nIn this paper we illustrate how to perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach. Our method, dubbed SiamMask, improves the offline training procedure of popular fully-convolutional Siamese approaches for object tracking by augmenting their loss with a binary segmentation task. Once trained, SiamMask solely relies on a single bounding box initialisation and operates online, producing class-agnostic object segmentation masks and rotated bounding boxes at 35 frames per second. Despite its simplicity, versatility and fast speed, our strategy allows us to establish a new state-of-the-art among real-time trackers on VOT-2018, while at the same time demonstrating competitive performance and the best speed for the semisupervised video object segmentation task on DAVIS-2016 and DAVIS-2017. The project website is\n\nIntroduction\n\nTracking is a fundamental task in any video application requiring some degree of reasoning about objects of interest, as it allows to establish object correspondances between frames [38]. It finds use in a wide range of scenarios such as automatic surveillance, vehicle navigation, video labelling, human-computer interaction and activity recognition. Given the location of an arbitrary target of interest in the first frame of a video, the aim of visual object tracking is to estimate its position in all the subsequent frames [70,66,55].\n\nFor many applications, it is important that tracking can be performed online, while the video is streaming. In other words, the tracker should not make use of future frames to * Equal contribution. Work done while at University of Oxford.\n\nInit Estimates Figure 1. Our proposed method aims at distilling the best from the two tasks of object tracking and video object segmentation. Like conventional object trackers, it relies on a simple bounding box initialisation (blue) and operates online. Differently from state-ofthe-art trackers such as ECO [15] (red), SiamMask (green) is able to produce binary segmentation masks, which can more accurately describe the target object.\n\nreason about the current position of the object [30]. This is the scenario portrayed by visual object tracking benchmarks, which represent the target object with a simple axisaligned [63,34,42,60,43] or rotated [30] bounding box. Such a simple annotation helps to keep the cost of data labelling low; what is more, it allows a user to perform a quick and simple initialisation of the target.\n\nSimilar to object tracking, the task of semi-supervised video object segmentation (VOS) requires estimating the position of an arbitrary target specified in the first frame of a video. However, in this case the object representation consists of a binary segmentation mask which expresses whether each pixel belongs to the target or not [46]. Such a detailed representation is more desirable for applications that require pixel-level information, like video editing [44] and rotoscoping [41]. Understandably, producing pixel-level estimates requires more computational re-sources than a simple bounding box. As a consequence, VOS methods have been traditionally slow, often requiring several seconds per frame (e.g. [62,58,45,2]). Very recently, there has been a surge of interest in faster approaches [67,40,64,11,10,24,23]. However, even the fastest still cannot operate in real-time.\n\nIn this paper, we aim at narrowing the gap between arbitrary object tracking and VOS by proposing SiamMask, a simple multi-task learning approach that can be used to address both problems. Our method is motivated by the success of fast tracking approaches based on fullyconvolutional Siamese networks trained offline on millions of pairs of video frames [4,31] and by the very recent availability of a large video dataset with pixel-wise annotations such as YouTube-VOS [65]. We aim at retaining the offline trainability and online speed of these methods while at the same time significantly refining their representation of the target object, which is limited to a simple axis-aligned bounding box.\n\nTo achieve this goal, we simultaneously train a Siamese network on three tasks, each corresponding to a different strategy to establish correspondances between the target object and candidate regions in the new frames. As in the fully-convolutional approach of Bertinetto et al. [4], one task is to learn a measure of similarity between the target object and multiple candidates in a sliding window fashion. The output is a dense response map which only indicates the location of the object, without providing any information about its spatial extent. To refine this information, we simultaneously learn two further tasks: bounding box regression using a Region Proposal Network [52,31] and classagnostic binary segmentation [49]. Notably, binary labels are only required during offline training to compute the segmentation loss and not during tracking. In our proposed architecture, each task is represented by a different branch departing from a shared CNN and contributes towards a final loss, which sums the three outputs together.\n\nOnce trained, SiamMask solely relies on a single bounding box initialisation, operates online without updates and produces object segmentation masks and rotated bounding boxes at 35 frames per second. Despite its simplicity and fast speed, SiamMask establishes a new state-of-the-art on VOT-2018 for the problem of real-time object tracking. Moreover, the same method is also very competitive against recent semi-supervised VOS approaches on DAVIS-2016 and DAVIS-2017, while being the fastest by a large margin. This result is achieved with a simple bounding box initialisation (as opposed to a mask) and without adopting costly techniques often used by VOS approaches such as fine-tuning [39,45,2,61], data augmentation [25,33] and optical flow [58,2,45,33,11].\n\nThe rest of this paper is organised as follows. Section 2 briefly outlines some of the most relevant prior work in vi-sual object tracking and semi-supervised VOS; Section 3 describes our proposal; Section 4 evaluates it on four benchmarks and illustrates several ablative studies; Section 5 concludes the paper.\n\n\nRelated Work\n\nIn this section, we briefly cover most representative techniques for the two problems tackled in this paper.\n\nVisual object tracking. Arguably, until very recently, the most popular paradigm for tracking arbitrary objects has been to train online a discriminative classifier exclusively from the ground-truth information provided in the first frame of a video (and then update it online). This strategy has often been referred to as tracking-by-detection (e.g. [1,55]). In the past few years, the Correlation Filter, a simple algorithm that allows to discriminate between the template of an arbitrary target and its 2D translations, rose to prominence as particularly fast and effective strategy for tracking-by-detection thanks to the pioneering work of Bolme et al. [6]. Performance of Correlation Filter-based trackers has then been notably improved with the adoption of multi-channel formulations [26,22], spatial constraints [27,16,37,32] and deep features (e.g. [15,59]).\n\nRecently, a radically different approach has been introduced [4,21,57]. Instead of learning a discrimative classifier online, the idea is to train (offline) a similarity function on pairs of video frames. At test time, this function can be simply evaluated on a new video, once per frame. Evolutions of the fully-convolutional Siamese approach [4] considerably improved tracking performance by making use of region proposals [31], hard negative mining [72], ensembling [17] and memory networks [68].\n\nMost modern trackers, including all the ones mentioned above, use a rectangular bounding box both to initialise the target and to estimate its position in the subsequent frames. Despite its convenience, a simple rectangle often fails to properly represent an object, as it is evident in the examples of Figure 1. This motivated us to propose a tracker able to produce binary segmentation masks while still only relying on a bounding box initialisation.\n\nInterestingly, in the past it was not uncommon for trackers to produce a coarse binary mask of the target object (e.g. [14,48,5]). However, to the best of our knowledge, the only recent tracker that, like ours, is able to operate online and produce a binary mask starting from a bounding box initialisation is the superpixel-based approach of Yeo et al. [69]. However, at 4 frames per seconds (fps), its fastest variant is significantly slower that our proposal (35 fps). Furthermore, When using CNN features, its speed is affected by a 60-fold decrease, plummeting below 0.1 fps. Finally, it has not demonstrated to be competitive on mod-ern tracking or VOS benchmarks 1 . Similar to us, the methods of Perazzi et al. [45] and Ci et al. [13] can also start from a rectangle and output per-frame masks. However, they require fine-tuning at test time, which makes them slow.\n\n\nSemi-supervised video object segmentation.\n\nBenchmarks for arbitrary object tracking (e.g. [55,30]) assume that trackers receive input frames in a sequential fashion. This aspect is generally referred to with the attributes online or causal [30]. Moreover, methods are often focused on achieving a speed that exceeds the ones of typical video framerates [29]. Conversely, semi-supervised VOS algorithms have been traditionally more concerned with an accurate representation of the object of interest [44,46].\n\nIn order to exploit consistency between video frames, several methods propagate the supervisory segmentation mask of the first frame to the temporally adjacent ones via graph labeling approaches (e.g. [62,47,58,40,2]). In particular, Bao et al. [2] recently proposed a very accurate method that makes use of a spatio-temporal MRF in which temporal dependencies are modelled by optical flow, while spatial dependencies are expressed by a CNN.\n\nAnother popular strategy is to process video frames independently (e.g. [39,45,61]), similarly to what happens in most tracking approaches. For example, in OSVOS-S Maninis et al. [39] do not make use of any temporal information. They rely on a fully-convolutional network pretrained for classification and then, at test time, they finetune it using the ground-truth mask provided in the first frame. MaskTrack [45] instead is trained from scratch on individual images, but it does exploit some form of temporality at test time by using the latest mask prediction and optical flow as additional input to the network.\n\nAiming towards the highest possible accuracy, at test time VOS methods often feature computationally intensive techniques such as fine-tuning [39,45,2,61], data augmentation [25,33] and optical flow [58,2,45,33,11]. Therefore, these approaches are generally characterised by low framerates and the inability to operate online. For example, it is not uncommon for methods to require minutes [45,12] or even hours [58,2] for videos that are just a few seconds long.\n\nRecently, there has been an increasing interest in the VOS community towards faster methods [40,64,11,10,24,23]. To the best of our knowledge, the fastest approaches with a performance competitive with the state-of-the-art are the ones of Yang et al. [67] and Wug et al. [64]. The former uses a meta-network \"modulator\" to quickly adapt the parameters of a segmentation network during test time, while the latter does not use any fine-tuning and adopts an encoder-decoder Siamese architecture trained in multiple stages. Both these methods run below 10 frames per sec-\n\n\nMethodology\n\nTo allow online operability and fast speed, we adopt the fully-convolutional Siamese framework [4]. Moreover, to illustratate that our approach is agnostic to the specific fully-convolutional method used as a starting point (e.g. [4,31,72,68,18]), we consider the popular SiamFC [4] and SiamRPN [31] as two representative examples. We first introduce them in Section 3.1 and then describe our approach in Section 3.2.\n\n\nFully-convolutional Siamese networks\n\nSiamFC. Bertinetto et al. [4] propose to use, as a fundamental building block of a tracking system, an offlinetrained fully-convolutional Siamese network that compares an exemplar image z against a (larger) search image x to obtain a dense response map. z and x are, respectively, a w\u00d7h crop centered on the target object and a larger crop centered on the last estimated position of the target. The two inputs are processed by the same CNN f \u03b8 , yielding two feature maps that are cross-correlated:\ng \u03b8 (z, x) = f \u03b8 (z) f \u03b8 (x).\n(1)\n\nIn this paper, we refer to each spatial element of the response map (left-hand side of Eq. 1) as response of a candidate window (RoW). For example, g n \u03b8 (z, x), encodes a similarity between the examplar z and n-th candidate window in x. For SiamFC, the goal is for the maximum value of the response map to correspond to the target location in the search area x. Instead, in order to allow each RoW to encode richer information about the target object, we replace the simple cross-correlation of Eq. 1 with depth-wise crosscorrelation [3] and produce a multi-channel response map. SiamFC is trained offline on millions of video frames with the logistic loss [4, Section 2.2], which we refer to as L sim . SiamRPN. Li et al. [31] considerably improve the performance of SiamFC by relying on a region proposal network (RPN) [52], which allows to estimate the target location with a bounding box of variable aspect ratio. In particular, in SiamRPN each RoW encodes a set of k anchor box proposals and corresponding object/background scores. Therefore, SiamRPN outputs box predictions in parallel with classification scores. The two output branches are trained using the smooth L 1 and the cross-entropy losses [31, Section 3.2]. In the following, we refer to them as L box and L score respectively.\n\n\nSiamMask\n\nUnlike existing tracking methods that rely on lowfidelity object representations, we argue the importance of producing per-frame binary segmentation masks. To this aim we show that, besides similarity scores and bounding box coordinates, it is possible for the RoW of a fullyconvolutional Siamese network to also encode the information necessary to produce a pixel-wise binary mask. This can be achieved by extending existing Siamese trackers with an extra branch and loss.\n\nWe predict w\u00d7h binary masks (one for each RoW) using a simple two-layers neural network h \u03c6 with learnable parameters \u03c6. Let m n denote the predicted mask corresponding to the n-th RoW,\nm n = h \u03c6 (g n \u03b8 (z, x)).(2)\nFrom Eq. 2 we can see that the mask prediction is a function of both the image to segment x and the target object in z. In this way, z can be used as a reference to guide the segmentation process, such that objects of any arbitrary class can be tracked. This clearly means that, given a different reference image z, the network will produce a different segmentation mask for x. Loss function. During training, each RoW is labelled with a ground-truth binary label y n \u2208 {\u00b11} and also associated with a pixel-wise ground-truth mask c n of size w\u00d7h. Let c ij n \u2208 {\u00b11} denote the label corresponding to pixel (i, j) of the object mask in the n-th candidate RoW. The loss function L mask (Eq. 3) for the mask prediction task is a binary logistic regression loss over all RoWs:\nL mask (\u03b8, \u03c6) = n ( 1 + y n 2wh ij log(1 + e \u2212c ij n m ij n )). (3)\nThus, the classification layer of h \u03c6 consists of w\u00d7h classifiers, each indicating whether a given pixel belongs to the object in the candidate window or not. Note that L mask is considered only for positive RoWs (i.e. with y n = 1). Mask representation. In contrast to semantic segmentation methods\u00e0-la FCN [36] and Mask R-CNN [19], which maintain explicit spatial information throughout the network, our approach follows the spirit of [49,50] and generates masks starting from a flattened representation of the object. In particular, in our case this representation corresponds to one of the 1\u00d71\u00d7256 RoWs produced by the depth-wise cross-correlation between f \u03b8 (z) and f \u03b8 (x). Importantly, the network h \u03c6 of the segmentation task is composed of two 1\u00d71 convolutional layers, one with 256 and the other with 63 2 channels ( Figure 2). This allows every pixel classifier to utilise information contained in the entire RoW and thus to have a complete view of its corresponding candidate window in x, which is critical to disambiguate between instances that look like the target (e.g. last row of Figure 5), also known as distractors [72]. With the aim of producing a more accurate object mask, we follow the strategy of [50], which merges low and high resolution features using multiple refinement modules made of upsampling layers and skip connections. Further details can be found in the appendix A. Two variants. For our experiments, we augment the architectures of SiamFC [4] and SiamRPN [31] with our segmentation branch and the loss L mask , obtaining what we call the two-branch and three-branch variants of SiamMask. These respectively optimise the multi-task losses L 2B and L 3B , defined as:\nL 2B = \u03bb 1 \u00b7 L mask + \u03bb 2 \u00b7 L sim ,(4)L 3B = \u03bb 1 \u00b7 L mask + \u03bb 2 \u00b7 L score + \u03bb 3 \u00b7 L box .(5)\nWe refer the reader to [4, Section 2.2] for L sim and to [31,Section 3.2] for L box and L score . For L 3B , a RoW is considered positive (y n = 1) if one of its anchor boxes has IOU with the ground-truth box of at least 0.6 and negative (y n = \u22121) otherwise. For L 2B , we adopt the same strategy of [4] to define positive and negative samples. We did not search over the hyperparameters of Eq. 4 and Eq. 5 and simply set \u03bb 1 = 32 like in [49] and \u03bb 2 = \u03bb 3 = 1. The taskspecific branches for the box and score outputs are consti- tuted by two 1\u00d71 convolutional layers. Figure 2 illustrates the two variants of SiamMask. Box generation. Note that, while VOS benchmarks require binary masks, typical tracking benchmarks such as VOT [30] require a bounding box as final representation of the target object. We consider three different strategies to generate a bounding box from a binary mask ( Figure 3):\n\n(1) axis-aligned bounding rectangle (Min-max), (2) rotated minimum bounding rectangle (MBR) and (3) the optimisation strategy used for the automatic bounding box generation proposed in VOT-2016 [28] (Opt). We empirically evaluate these alternatives in Section 4 ( Table 1).\n\n\nImplementation details\n\nNetwork architecture. For both our variants, we use a ResNet-50 [20] until the final convolutional layer of the 4-th stage as our backbone f \u03b8 . In order to obtain a high spatial resolution in deeper layers, we reduce the output stride to 8 by using convolutions with stride 1. Moreover, we increase the receptive field by using dilated convolutions [9]. In our model, we add to the shared backbone f \u03b8 an unshared adjust layer (1\u00d71 conv with 256 outputs). For simplicity, we omit it in Eq. 1. We describe the network architectures in more detail in Appendix A.\n\nTraining. Like SiamFC [4], we use examplar and search image patches of 127\u00d7127 and 255\u00d7255 pixels respectively. During training, we randomly jitter examplar and search patches. Specifically, we consider random translations (up to \u00b18 pixels) and rescaling (of 2 \u00b11/8 and 2 \u00b11/4 for examplar and search respectively). The network backbone is pre-trained on the ImageNet-1k classification task. We use SGD with a first warmup phase in which the learning rate increases linearly from 10 \u22123 to 5\u00d710 \u22123 for the first 5 epochs and then descreases logarithmically until 5\u00d710 \u22124 for 15 more epochs. We train all our models using COCO [35], ImageNet-VID [54] and YouTube-VOS [65]. Inference. During tracking, SiamMask is simply evaluated once per frame, without any adaptation. In both our variants, we select the output mask using the location attaining the maximum score in the classification branch. Then, after having applied a per-pixel sigmoid, we binarise the output of the mask branch with a threshold 0.5. In the twobranch variant, for each video frame after the first one, we fit the output mask with the Min-max box and use it as reference to crop the next frame search region. Instead, in the three-branch variant, we find more effecitve to exploit the highest-scoring output of the box branch as reference.\n\nFor the implementation of SiamMask, we used PyTorch. Code, pre-computed results and pre-trained models will be made available.\n\n\nExperiments\n\nIn this section, we evaluate our approach on two related tasks: visual object tracking (on VOT-2016 and VOT-2018) and semi-supervised video object segmentation (on DAVIS-2016 and DAVIS-2017). We refer to our two-branch and three-branch variants with SiamMask-2B and SiamMask respectively.\n\n\nEvaluation for visual object tracking\n\nDatasets and settings. We adopt two widely used benchmarks for the evaluation of the object tracking task: VOT-2016 [28] and VOT-2018 [29], both annotated with rotated bounding boxes. We use VOT-2016 to conduct an experiment to understand how different types of representation affect the performance. For this first experiment, we use mean intersection over union (IOU) and Average Precision (AP)@{0.5, 0.7} IOU. We then compare against the stateof-the-art on VOT-2018, using the official VOT toolkit and the Expected Average Overlap (EAO), a measure that considers both accuracy and robustness of a tracker [29].\n\nHow much does the object representation matter? Existing tracking methods typically predict axis-aligned bounding boxes with a fixed [4,22,16,37] or variable [31,21,72] aspect ratio. We are interested in understanding to which extent producing a per-frame binary mask can improve tracking. In order to focus on representation accuracy, for this experiment only we ignore the temporal aspect and sample video frames at random. The approaches described in the following paragraph are tested on randomly cropped search patches (with random shifts within \u00b116 pixels and scale deformations up to 2 1\u00b10.25 ) from the sequences of VOT-2016.\n\nIn Table 1, we compare our three-branch variant using the Min-max, MBR and Opt approaches (described at the end of Section 3.2 and in Figure 3). For perspective, we also report results for SiamFC and SiamRPN as representative of the fixed and variable aspect-ratio approaches, together with three oracles that have access to per-frame groundtruth information and serve as upper bound for the different representation strategies. (1) The fixed aspect-ratio oracle uses the per-frame ground-truth area and center location, but fixes the aspect reatio to the one of the first frame mIOU (%) mAP@0. 5  and produces an axis-aligned bounding box. (2) The Minmax oracle uses the minimal enclosing rectangle of the rotated ground-truth bounding box to produce an axis-aligned bounding box. (3) Finally, the MBR oracle uses the rotated minimum bounding rectangle of the ground-truth. Note that (1), (2) and (3) can be considered, respectively, the performance upper bounds for the representation strategies of SiamFC, SiamRPN and SiamMask. Table 1 shows that our method achieves the best mIOU, no matter the box generation strategy used (Figure 3). Albeit SiamMask-Opt offers the highest IOU and mAP, it requires significant computational resources due to its slow optimisation procedure [28]. Instead, we adopt the MBR strategy (whose computational overhead is negligible) for our final object tracking evaluation. SiamMask-MBR achieves a mAP@0.5 IOU of 85.4, with a respective improvement of +29 and +9.2 points w.r.t. the two fully-convolutional baselines. Interestingly, the gap significantly widens when considering mAP at the higher accuracy regime of 0.7 IOU: +41.6 and +18.4 respectively. Notably, our accuracy results are not far from the fixed aspectratio oracle. Moreover, comparing the upper bound performance represented by the oracles, it is possible to notice how, by simply changing the bounding box representation, there is a great room of improvement (e.g. +10.6% mIOU improvement between the fixed aspect-ratio and the MBR oracles).\n\nOverall, this study shows how the MBR strategy to obtain a rotated bounding box from a binary mask of the object offers a significant advantage over popular strategies that simply report axis-aligned bounding boxes.\n\nResults on VOT-2018. In Table 2 we compare the two variants of SiamMask against seven recently published state-of-the-art trackers on the VOT-2018 benchmark. Both achieve outstanding performance and run in real-time. In particular, our three-branch variant (SiamMask) significantly outperforms the very recent and top performing DaSiamRPN [72]. Even without box regression branch, our simpler two-branch variant (SiamMask-2B) achieves a high EAO of 0.334, which is in par with SA Siam R [17] and superior to any other real-time method in the published lit- erature. SiamMask provides a further relative gain of 3.9%, achieving a EAO of 0.347, which establishes a new stateof-the-art for real-time tracking. Our model is particularly strong under the accuracy metric, showing a significant advantage with respect to the Correlation Filter-based trackers CSRDCF [37], STRCF [32] and ECO [15]. This is not surprising, as SiamMask relies on a richer object representation, as outlined in Table 1. Interestingly, similarly to us He et al. (SA Siam R) [17] are motivated to achieve a more accurate target representation. To this aim, they consider multiple rotated and rescaled bounding boxes as candidates. However, like SiamFC [4], SA Siam R is still constrained to a fixed aspect-ratio bounding box.\n\n\nEvaluation for semi-supervised VOS\n\nIn the following we show how, once trained, the same method can also be used for the task of video object segmentation, achieving competitive performance without requiring any adaptation at test time. Importantly, differently to typical VOS approaches, ours can operate online, runs in real-time and only requires a simple bounding box initialisation. Datasets and settings. We report the performance of SiamMask on the popular DAVIS-2016 [46] and DAVIS-2017 [51] benchmarks. For both datasets, we use the official performance measures: the Jaccard index (J ) to express region similarity and the F-measure (F) to express contour accuracy. For each measure C \u2208 {J , F}, three statistics are considered: mean C M , recall C O , and decay C D , which informs us about the gain/loss of performance over time [46].\n\nTo initialise SiamMask, we extract the axis-aligned bounding box (Min-max strategy, Figure 3) from the mask provided in the first frame. Similarly to most VOS methods, in case of multiple objects in the same video (DAVIS-2017) we simply perform multiple inferences.\n\n\nResults on DAVIS-2016 and 2017.\n\nIn the semisupervised setting, VOS methods are initialised with a binary mask [44] and many of them require computa-  Table 3. Results on DAVIS 2016 (validation set). FT and M respectively denote if the method requires fine-tuning and whether it is initialised with a mask () or a bounding box (). Speed is measured in frames per second (fps).  tionally intensive techniques at test time such as finetuning [39,45,2,61], data augmentation [25,33], inference on MRF/CRF [62,58,40,2] and optical flow [58,2,45,33,11]. As a consequence, it is not uncommon for VOS techniques to require several minutes to process the short sequences of DAVIS. Clearly, these strategies make the online applicability (which is our focus) impossible. For this reason, in our comparison (Table 3 and 4, Figure 4) we mainly concentrate on fast state-of-the-art approaches. Table 3 shows how SiamMask can be considered as a strong baseline for online VOS. First, it is almost two orders of magnitude faster than accurate approaches such as OnAVOS [61] or SFL [12]. Second, it is competitive with recent VOS methods that do not employ fine-tuning, while being four times more efficient than the fastest ones (i.e. OSMN [67] and RGMP [64]). Interestingly, we note that SiamMask achieves the best decay [46] for region similarity (J D ,) and contour accuracy (F D ) on both DAVIS-2016 and DAVIS-2017. This suggests that our method is robust over time and thus it is indicated for particularly long sequences.  Table 5. Ablation studies on VOT-2018 and DAVIS-2016. Figure 4 offers a clearer overview of the tradeoff between segmentation accuracy (as mean IOU, which corresponds to J M ) and speed (in frames per second). Among the methods of Table 3 that do not fine tune the model online, the recently proposed FAVOS [11] obtains the best results. However, it combines several independent modules (a part-based tracker, a segmentation network and a similarity-based aggregation module), while SiamMask is only evaluated with a single model and it is almost 50 times faster.\n\nQualitative results of SiamMask for both VOT and DAVIS sequences are shown in Figure 5, 10 and 11. Despite the high speed, SiamMask produces accurate segmentation masks even in presence of distractors.\n\n\nFurther analysis\n\nIn this section, we illustrate ablation studies, failure cases and timings of our methods.\n\nNetwork architecture. In Table 5, AN and RN denote whether we use AlexNet or ResNet-50 as the shared backbone f \u03b8 (Figure 2), while with \"w/o R\" we mean that the method does not use the refinement strategy of Pinheiro et al. [50]. From the results of Table 5, it is possible to make several observations. (1) The first set of rows shows that, by simply updating the architecture of f \u03b8 , it is possible to achieve an important performance improvement. However, this comes at the cost of speed, especially for SiamRPN. (2) SiamMask-2B and SiamMask considerably improve over their baselines (with same f \u03b8 ) SiamFC and SiamRPN. With a relative +33% in EAO, the gap of the two-branch variant is particularly important. (3) Interestingly, the refinement approach of Pinheiro et al. [50] is very important for the contour accuracy F M , but less so for the other metrics. Multi-task learning. We conducted two further experiments to disentangle the effect of multi-task learning [8,53] to the one of using the MBR box (from a binary mask) as representation for the target object instead of a traditional axis-aligned bounding box. Results are reported in Table 5. To achieve this, we modified the two variants of SiamMask so that, respectively, they report an axis-aligned bounding box from the score branch (SiamMask-2B-box) or the box branch (SiamMask-box). Therefore, despite having been trained, the mask branch is not used during inference. We can observe how both variants obtain an improvement with respect to their simpler counterparts: from 0.251 to 0.265 EAO for the two-branch and from 0.329 to 0.331 for the three-branch. However, for both variants the gap between SiamMask-box and SiamMask is higher. This implies that, despite being meaningful, the improvement brought simply by training multiple related tasks together is less relevant than the type of target object representation used.\n\nTiming. SiamMask operates online without any adaptation to the test sequence. On a single NVIDIA Titan X GPU, we measured an average speed of 35 and 40 frames per second, respectively for the two-branch and three-branch variants. Note that the highest computational burden comes from the feature extractor f \u03b8 . For this reason, changing architecture is a convenient way to obtain different speed/performance trade-offs. 1 Figure 6. Failure cases: motion blur and \"non-object\" pattern.\n\nFailure cases. Finally, we discuss two scenarios in which SiamMask fails: motion blur and \"non-object\" pattern (Figure 6). Despite being different in nature, these two cases arguably arise from the complete lack of similar training samples in a training set such as YouTube-VOS [65], which is focused on objects that can be unambiguously segmented from the foreground.\n\n\nConclusion\n\nWe introduced SiamMask, a simple approach that enables fully-convolutional Siamese trackers to produce classagnostic binary segmentation masks of the target object. We show how it can be applied with success to both tasks of visual object tracking and semi-supervised video object segmentation, showing better accuracy than state-of-the-art trackers and, at the same time, the fastest speed among VOS methods. The two variants of SiamMask we proposed are initialised with a simple bounding box, operate online, run in real-time and do not require any adaptation to the test sequence. We hope that our work will inspire further studies that consider the two problems of visual object tracking and video object segmentation together.\n\nA. Network architecture details Network backbone. Table 6 illustrates the details of our backbone architecture (f \u03b8 in the main paper). For both variants, we use a ResNet-50 [20] until the final convolutional layer of the 4-th stage. In order to obtain a higher spatial resolution in deep layers, we reduce the output stride to 8 by using convolutions with stride 1. Moreover, we increase the receptive field by using dilated convolutions [9]. Specifically, we set the stride to 1 and the dilation rate to 2 in the 3\u00d73 conv layer of conv4 1. Differently to the original ResNet-50, there is no downsampling in conv4 x. We also add to the backbone an adjust layer (a 1\u00d71 convolutional layer with 256 output channels). Examplar and search patches share the network's parameters from conv1 to conv4 x, while the parameters of the adjust layer are not shared. The output features of the adjust layer are then depth-wise cross-correlated, resulting a feature map of size 17 \u00d7 17. Network heads. The network architecture of the branches of both variants are shows in Table 7   Mask refinement module. With the aim of producing a more accurate object mask, we follow the strategy of [50], which merges low and high resolution features using multiple refinement modules made of upsampling layers and skip connections. Figure 9 illustrates how a mask is generated with stacked refinement modules. Figure 7 gives an example of refinement module U 3 .   \n\n\nB. Further qualitative results\n\nDifferent masks at different locations. Our model generates a mask for each RoW. During inference, we rely on the score branch to select the final output mask (using the location attaining the maximum score). The example of Figure 8 illustrates the multiple output masks produced by the mask branch, each corresponding to a different RoW.\n\nBenchmark sequences. More qualitative results for VOT and DAVIS sequences are shown in Figure 10   ResNet-50  . Further qualitative results of our method on sequences from the semi-supervised video object segmentation benchmarks DAVIS-2016 [46] and DAVIS-2017 [51].\n\nFigure 2 .\n2Schematic illustration of SiamMask variants: (a) three-branch architecture (full), (b) two-branch architecture (head). d denotes depth-wise cross correlation. For simplicity, upsampling layer and mask refinement module are omitted here and detailed in Appendix A.\n\nFigure 3 .\n3We experiment with three different methods to generated a bounding box from predicted binary masks (in yellow). Minmax: Axis-aligned box (red), MBR: minimum bounding rectangle (green) and Opt: the optimisation strategy proposed in VOT-2016[28] (blue).\n\nFigure 4 .\n4Segmentation quality (as mean IOU) vs. operating speed (in frames per second) on DAVIS-2016. Note how SiamMask demonstrates a competitive accuracy despite being orders of magnitude faster than most methods. x-axis is in log scale.\n\nFT M J\nMM\u2191 J O\u2191 J D\u2193 F M\u2191 F O\u2191 F D\u2193 Speed OnAVOS [61] 61.6 67.4 27.9 69.1 75.4 26.\n\nFigure 5 .\n5Qualitative results of our method for sequences belonging to both object tracking and video object segmentation benchmarks. Basketball and Nature are from VOT-2018[29]; Car-Shadow is from DAVIS-2016[46]; Dogs-Jump and Pigs are DAVIS-2017[51].\n\nFigure 7 .Figure 8 .\n78Example of a refinement module U3. Score maps from Mask branch at different locations.\n\n\nand 11.\n\nFigure 9 .Figure 10 .\n910Schematic illustration of mask generation with stacked refinement modules. Further qualitative results of our method on sequences from the visual object tracking benchmark VOT-2018[29].\n\n\nFT M J M\u2191 J O\u2191 J D\u2193 F M\u2191 F O\u2191 F D\u2193 SpeedSiamMask SiamMask-2B DaSiamRPN [72] SiamRPN [31] SA Siam R [17] CSRDCF [37] STRCF [32] LSART [56] ECO [15] \nEAO \u2191 \n0.347 \n0.334 \n0.326 \n0.244 \n0.337 \n0.263 \n0.345 \n0.323 \n0.280 \nAccuracy \u2191 \n0.602 \n0.575 \n0.569 \n0.490 \n0.566 \n0.466 \n0.523 \n0.495 \n0.484 \nRobustness \u2193 \n0.288 \n0.306 \n0.337 \n0.460 \n0.258 \n0.318 \n0.215 \n0.218 \n0.276 \nSpeed (fps) \u2191 \n35 \n40 \n160 \n200 \n32.4 \n48.9 \n2.9 \n1.7 \n3.7 \n\nTable 2. Comparison with the state-of-the-art under EAO, Accuracy, and Robustness on the VOT-2018 benchmark. \n\nOnAVOS [61] 86.1 96.1 5.2 84.9 89.7 5.8 0.08 \nMSK [45] \n79.7 93.1 8.9 75.4 87.1 9.0 \n0.1 \nMSK b [45] \n69.6 \n-\n-\n-\n-\n-\n0.1 \nSFL [12] \n76.1 90.6 12.1 76.0 85.5 10.4 0.1 \n\nFAVOS [11] \n82.4 96.5 4.5 79.5 89.4 5.5 \n0.8 \nRGMP [64] \n81.5 91.7 10.9 82.0 90.8 10.1 \n8 \nPML [10] \n75.5 89.6 8.5 79.3 93.4 7.8 \n3.6 \nOSMN [67] \n74.0 87.6 9.0 72.9 84.0 10.6 8.0 \nPLM [71] \n70.2 86.3 11.2 62.5 73.2 14.7 6.7 \nVPN [24] \n70.2 82.3 12.4 65.5 69.0 14.4 1.6 \nSiamMask \n71.7 86.8 3.0 67.8 79.8 2.1 \n35 \n\n\n\nTable 4 .\n4Results on DAVIS 2017 (validation set).\n\n\nAN RN EAO \u2191 J M\u2191 F M\u2191 Speed (fps)SiamFC \n\n0.188 \n-\n-\n86 \nSiamFC \n\n0.251 \n-\n-\n40 \nSiamRPN \n\n0.243 \n-\n-\n200 \nSiamRPN \n\n0.329 \n-\n-\n58 \nSiamMask-2B w/o R \n\n0.326 62.3 55.6 \n43 \nSiamMask w/o R \n\n0.342 68.6 57.8 \n37 \nSiamMask-2B-box \n\n0.265 \n-\n-\n40 \nSiamMask-box \n\n0.331 \n-\n-\n35 \nSiamMask-2B \n\n0.334 67.4 63.5 \n40 \nSiamMask \n\n0.347 71.7 67.8 \n35 \n\n\n\n\nand 8. The conv5 block in both variants contains a normalisation layer and ReLU non-linearity while conv6 only consists of a 1\u00d71 convolutional layer. examplar output size search output sizeblock backbone \nconv1 \n61\u00d761 \n125\u00d7125 \n7\u00d77, 64, stride 2 \n\nconv2 x \n31\u00d731 \n63\u00d763 \n\n3\u00d73 max pool, stride 2 \n\uf8ee \n\n\uf8f0 \n\n1\u00d71, 64 \n3\u00d73, 64 \n1\u00d71, 256 \n\n\uf8f9 \n\n\uf8fb \u00d73 \n\nconv3 x \n15\u00d715 \n31\u00d731 \n\n\uf8ee \n\n\uf8f0 \n\n1\u00d71, 128 \n3\u00d73, 128 \n1\u00d71, 512 \n\n\uf8f9 \n\n\uf8fb \u00d74 \n\nconv4 x \n15\u00d715 \n31\u00d731 \n\n\uf8ee \n\n\uf8f0 \n\n1\u00d71, 256 \n3\u00d73, 256 \n1\u00d71, 1024 \n\n\uf8f9 \n\n\uf8fb \u00d76 \n\nadjust \n15\u00d715 \n31\u00d731 \n1\u00d71, 256 \nxcorr \n17 \u00d7 17 \ndepth-wise \n\n\n\nTable 6 .\n6Backbone architecture. Details of each building block are shown in square brackets.\n\n\n\u00d7 1, 256 1 \u00d7 1, 256 1 \u00d7 1, 256 conv6 1 \u00d7 1, 2k 1 \u00d7 1, 4k 1 \u00d7 1, (63 \u00d7 63)block \nscore \nbox \nmask \nconv5 1 \n\nTable 7 .Table 8 .\n78Architectural details of the three-branch head. k denotes the number of anchor boxes per RoW. \u00d7 1, 1 1 \u00d7 1, (63 \u00d7 63) Architectural details of the two-branch head.block \nscore \nmask \nconv5 1 \u00d7 1, 256 \n1 \u00d7 1, 256 \nconv6 1 conv 2 \n\n31*31*16 \n\n31*31*256 \n\nconv, \n3*3, 16 \n\nconv, \n3*3, 16 \n\nconv, \n3*3, 64 \n\nconv, \n3*3, 32 \n\nconv, \n3*3, 16 \n\n+ \n\n31*31*16 \n\n31*31*16 \n\n61*61*8 \n\n2x, up \n\nReLU \n\n+ Element-wise \n\nsum \n\nRefinement module \n\n\nDespite being available, its code is not usable. ond, while we are more than four times faster and only rely on a bounding box initialisation.\n\nVisual tracking with online multiple instance learning. B Babenko, M.-H Yang, S Belongie, IEEE Conference on Computer Vision and Pattern Recognition. B. Babenko, M.-H. Yang, and S. Belongie. Visual tracking with online multiple instance learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2009. 2\n\nCnn in mrf: Video object segmentation via inference in a cnn-based higher-order spatiotemporal mrf. L Bao, B Wu, W Liu, IEEE Conference on Computer Vision and Pattern Recognition. 27L. Bao, B. Wu, and W. Liu. Cnn in mrf: Video object seg- mentation via inference in a cnn-based higher-order spatio- temporal mrf. In IEEE Conference on Computer Vision and Pattern Recognition, 2018. 2, 3, 7\n\nLearning feed-forward one-shot learners. L Bertinetto, J F Henriques, J Valmadre, P H S Torr, A Vedaldi, Advances in Neural Information Processing Systems. L. Bertinetto, J. F. Henriques, J. Valmadre, P. H. S. Torr, and A. Vedaldi. Learning feed-forward one-shot learners. In Ad- vances in Neural Information Processing Systems, 2016. 3\n\nFully-convolutional siamese networks for object tracking. L Bertinetto, J Valmadre, J F Henriques, A Vedaldi, P H Torr, European Conference on Computer Vision workshops. 56L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr. Fully-convolutional siamese networks for object tracking. In European Conference on Computer Vision work- shops, 2016. 2, 3, 4, 5, 6\n\nRobust Real-Time Visual Tracking using Pixel-Wise Posteriors. C Bibby, I Reid, European Conference on Computer Vision. C. Bibby and I. Reid. Robust Real-Time Visual Tracking us- ing Pixel-Wise Posteriors. In European Conference on Com- puter Vision, 2008. 2\n\nVisual object tracking using adaptive correlation filters. D S Bolme, J R Beveridge, B A Draper, Y M Lui, IEEE Conference on Computer Vision and Pattern Recognition. D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui. Visual object tracking using adaptive correlation filters. In IEEE Conference on Computer Vision and Pattern Recogni- tion, 2010. 2\n\nOne-shot video object segmentation. S Caelles, K.-K Maninis, J Pont-Tuset, L Leal-Taix\u00e9, D Cremers, L Van Gool, IEEE Conference on Computer Vision and Pattern Recognition. S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taix\u00e9, D. Cremers, and L. Van Gool. One-shot video object seg- mentation. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 7\n\nMultitask learning. Machine learning. R Caruana, R. Caruana. Multitask learning. Machine learning, 1997. 8\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, IEEE Transactions on Pattern Analysis and Machine Intelligence. 59L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully con- nected crfs. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, 2018. 5, 9\n\nBlazingly fast video object segmentation with pixel-wise metric learning. Y Chen, J Pont-Tuset, A Montes, L Van Gool, IEEE Conference on Computer Vision and Pattern Recognition. 27Y. Chen, J. Pont-Tuset, A. Montes, and L. Van Gool. Blaz- ingly fast video object segmentation with pixel-wise metric learning. In IEEE Conference on Computer Vision and Pat- tern Recognition, 2018. 2, 3, 7\n\nFast and accurate online video object segmentation via tracking parts. J Cheng, Y.-H Tsai, W.-C Hung, S Wang, M.-H Yang, IEEE Conference on Computer Vision and Pattern Recognition. 27J. Cheng, Y.-H. Tsai, W.-C. Hung, S. Wang, and M.-H. Yang. Fast and accurate online video object segmentation via track- ing parts. In IEEE Conference on Computer Vision and Pat- tern Recognition, 2018. 2, 3, 7\n\nSegflow: Joint learning for video object segmentation and optical flow. J Cheng, Y.-H Tsai, S Wang, M.-H Yang, IEEE International Conference on Computer Vision. 37J. Cheng, Y.-H. Tsai, S. Wang, and M.-H. Yang. Segflow: Joint learning for video object segmentation and optical flow. In IEEE International Conference on Computer Vision, 2017. 3, 7\n\nVideo object segmentation by learning location-sensitive embeddings. H Ci, C Wang, Y Wang, European Conference on Computer Vision. H. Ci, C. Wang, and Y. Wang. Video object segmentation by learning location-sensitive embeddings. In European Con- ference on Computer Vision, 2018. 2\n\nReal-time tracking of non-rigid objects using mean shift. D Comaniciu, V Ramesh, P Meer, IEEE Conference on Computer Vision and Pattern Recognition. D. Comaniciu, V. Ramesh, and P. Meer. Real-time tracking of non-rigid objects using mean shift. In IEEE Conference on Computer Vision and Pattern Recognition, 2000. 2\n\nEfficient convolution operators for tracking. M Danelljan, G Bhat, F S Khan, M Felsberg, IEEE Conference on Computer Vision and Pattern Recognition. 7M. Danelljan, G. Bhat, F. S. Khan, M. Felsberg, et al. Eco: Efficient convolution operators for tracking. In IEEE Con- ference on Computer Vision and Pattern Recognition, 2017. 1, 2, 6, 7\n\nLearning spatially regularized correlation filters for visual tracking. M Danelljan, G H\u00e4ger, F S Khan, M Felsberg, IEEE International Conference on Computer Vision. 25M. Danelljan, G. H\u00e4ger, F. S. Khan, and M. Felsberg. Learn- ing spatially regularized correlation filters for visual track- ing. In IEEE International Conference on Computer Vision, 2015. 2, 5\n\nTowards a better match in siamese network based visual object tracker. A He, C Luo, X Tian, W Zeng, European Conference on Computer Vision workshops. 67A. He, C. Luo, X. Tian, and W. Zeng. Towards a better match in siamese network based visual object tracker. In European Conference on Computer Vision workshops, 2018. 2, 6, 7\n\nA twofold siamese network for real-time object tracking. A He, C Luo, X Tian, W Zeng, IEEE Conference on Computer Vision and Pattern Recognition. A. He, C. Luo, X. Tian, and W. Zeng. A twofold siamese network for real-time object tracking. In IEEE Conference on Computer Vision and Pattern Recognition, 2018. 3\n\nMask rcnn. K He, G Gkioxari, P Doll\u00e1r, R Girshick, IEEE International Conference on Computer Vision. K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick. Mask r- cnn. In IEEE International Conference on Computer Vision, 2017. 4\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition. 59K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2016. 5, 9\n\nLearning to track at 100 fps with deep regression networks. D Held, S Thrun, S Savarese, European Conference on Computer Vision. 25D. Held, S. Thrun, and S. Savarese. Learning to track at 100 fps with deep regression networks. In European Conference on Computer Vision, 2016. 2, 5\n\nHighspeed tracking with kernelized correlation filters. J F Henriques, R Caseiro, P Martins, J Batista, IEEE Transactions on Pattern Analysis and Machine Intelligence. 25J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High- speed tracking with kernelized correlation filters. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015. 2, 5\n\nVideomatch: Matching based video object segmentation. Y.-T Hu, J.-B Huang, A G Schwing, European Conference on Computer Vision. 23Y.-T. Hu, J.-B. Huang, and A. G. Schwing. Videomatch: Matching based video object segmentation. In European Conference on Computer Vision, 2018. 2, 3\n\nVideo propagation networks. V Jampani, R Gadde, P V Gehler, IEEE Conference on Computer Vision and Pattern Recognition. 27V. Jampani, R. Gadde, and P. V. Gehler. Video propagation networks. In IEEE Conference on Computer Vision and Pat- tern Recognition, 2017. 2, 3, 7\n\nLucid data dreaming for object tracking. A Khoreva, R Benenson, E Ilg, T Brox, B Schiele, IEEE Conference on Computer Vision and Pattern Recognition workshops. 27A. Khoreva, R. Benenson, E. Ilg, T. Brox, and B. Schiele. Lucid data dreaming for object tracking. In IEEE Con- ference on Computer Vision and Pattern Recognition work- shops, 2017. 2, 3, 7\n\nMulti-channel correlation filters. H Galoogahi, T Sim, S Lucey, IEEE International Conference on Computer Vision. H. Kiani Galoogahi, T. Sim, and S. Lucey. Multi-channel correlation filters. In IEEE International Conference on Computer Vision, 2013. 2\n\nCorrelation filters with limited boundaries. H Galoogahi, T Sim, S Lucey, IEEE Conference on Computer Vision and Pattern Recognition. H. Kiani Galoogahi, T. Sim, and S. Lucey. Correlation filters with limited boundaries. In IEEE Conference on Computer Vision and Pattern Recognition, 2015. 2\n\nThe visual object tracking vot2016 challenge results. M Kristan, A Leonardis, J Matas, M Felsberg, R Pflugfelder, L \u010cehovin, T Voj\u00edr, G H\u00e4ger, A Luke\u017ei\u010d, G Fern\u00e1ndez, European Conference on Computer Vision. 56M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pflugfelder, L.\u010cehovin, T. Voj\u00edr, G. H\u00e4ger, A. Luke\u017ei\u010d, G. Fern\u00e1ndez, et al. The visual object tracking vot2016 chal- lenge results. In European Conference on Computer Vision, 2016. 5, 6\n\nThe sixth visual object tracking vot-2018 challenge results. M Kristan, A Leonardis, J Matas, M Felsberg, R Pfugfelder, L C Zajc, T Vojir, G Bhat, A Lukezic, A Eldesokey, G Fernandez, European Conference on Computer Vision workshops. 810M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pfugfelder, L. C. Zajc, T. Vojir, G. Bhat, A. Lukezic, A. Eldesokey, G. Fernandez, and et al. The sixth visual object tracking vot-2018 challenge results. In European Conference on Computer Vision workshops, 2018. 3, 5, 8, 10\n\nA novel performance evaluation methodology for single-target trackers. M Kristan, J Matas, A Leonardis, T Voj\u00ed\u0159, R Pflugfelder, G Fernandez, G Nebehay, F Porikli, L \u010cehovin, IEEE Transactions on Pattern Analysis and Machine Intelligence. 15M. Kristan, J. Matas, A. Leonardis, T. Voj\u00ed\u0159, R. Pflugfelder, G. Fernandez, G. Nebehay, F. Porikli, and L.\u010cehovin. A novel performance evaluation methodology for single-target trackers. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, 2016. 1, 3, 5\n\nHigh performance visual tracking with siamese region proposal network. B Li, J Yan, W Wu, Z Zhu, X Hu, IEEE Conference on Computer Vision and Pattern Recognition. 57B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu. High performance visual tracking with siamese region proposal network. In IEEE Conference on Computer Vision and Pattern Recogni- tion, 2018. 2, 3, 4, 5, 7\n\nLearning spatial-temporal regularized correlation filters for visual tracking. F Li, C Tian, W Zuo, L Zhang, M.-H Yang, IEEE Conference on Computer Vision and Pattern Recognition. 67F. Li, C. Tian, W. Zuo, L. Zhang, and M.-H. Yang. Learn- ing spatial-temporal regularized correlation filters for visual tracking. In IEEE Conference on Computer Vision and Pat- tern Recognition, 2018. 2, 6, 7\n\nVideo object segmentation with joint re-identification and attention-aware mask propagation. X Li, C C Loy, European Conference on Computer Vision. 27X. Li and C. C. Loy. Video object segmentation with joint re-identification and attention-aware mask propagation. In European Conference on Computer Vision, 2018. 2, 3, 7\n\nEncoding color information for visual tracking: Algorithms and benchmark. P Liang, E Blasch, H Ling, IEEE Transactions on Image Processing. P. Liang, E. Blasch, and H. Ling. Encoding color information for visual tracking: Algorithms and benchmark. In IEEE Transactions on Image Processing, 2015. 1\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European Conference on Computer Vision. T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Com- mon objects in context. In European Conference on Com- puter Vision, 2014. 5\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, IEEE Conference on Computer Vision and Pattern Recognition. J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, 2015. 4\n\nDiscriminative correlation filter with channel and spatial reliability. A Lukezic, T Vojir, L C Zajc, J Matas, M Kristan, IEEE Conference on Computer Vision and Pattern Recognition. 67A. Lukezic, T. Vojir, L. C. Zajc, J. Matas, and M. Kristan. Discriminative correlation filter with channel and spatial reli- ability. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 2, 5, 6, 7\n\nVisual learning in multiple-object tracking. T Makovski, G A Vazquez, Y V Jiang, PLoS One. 1T. Makovski, G. A. Vazquez, and Y. V. Jiang. Visual learning in multiple-object tracking. PLoS One, 2008. 1\n\nVideo object segmentation without temporal information. K.-K Maninis, S Caelles, Y Chen, J Pont-Tuset, L Leal-Taix\u00e9, D Cremers, L Van Gool, IEEE Transactions on Pattern Analysis and Machine Intelligence. 27K.-K. Maninis, S. Caelles, Y. Chen, J. Pont-Tuset, L. Leal- Taix\u00e9, D. Cremers, and L. Van Gool. Video object segmen- tation without temporal information. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. 2, 3, 7\n\nBilateral space video segmentation. N M\u00e4rki, F Perazzi, O Wang, A Sorkine-Hornung, IEEE Conference on Computer Vision and Pattern Recognition. 27N. M\u00e4rki, F. Perazzi, O. Wang, and A. Sorkine-Hornung. Bi- lateral space video segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, 2016. 2, 3, 7\n\nRoam: a rich object appearance model with application to rotoscoping. O Miksik, J.-M P\u00e9rez-R\u00faa, P H Torr, P P\u00e9rez, IEEE Conference on Computer Vision and Pattern Recognition. O. Miksik, J.-M. P\u00e9rez-R\u00faa, P. H. Torr, and P. P\u00e9rez. Roam: a rich object appearance model with application to rotoscop- ing. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 1\n\nA benchmark and simulator for uav tracking. M Mueller, N Smith, B Ghanem, European Conference on Computer Vision. M. Mueller, N. Smith, and B. Ghanem. A benchmark and simulator for uav tracking. In European Conference on Com- puter Vision, 2016. 1\n\nTrackingnet: A large-scale dataset and benchmark for object tracking in the wild. M M\u00fcller, A Bibi, S Giancola, S Al-Subaihi, B Ghanem, European Conference on Computer Vision. M. M\u00fcller, A. Bibi, S. Giancola, S. Al-Subaihi, and B. Ghanem. Trackingnet: A large-scale dataset and bench- mark for object tracking in the wild. In European Conference on Computer Vision, 2018. 1\n\nVideo Object Segmentation. F Perazzi, ETH ZurichPhD thesisF. Perazzi. Video Object Segmentation. PhD thesis, ETH Zurich, 2017. 1, 3, 6\n\nLearning video object segmentation from static images. F Perazzi, A Khoreva, R Benenson, B Schiele, A Sorkine-Hornung, IEEE Conference on Computer Vision and Pattern Recognition. 27F. Perazzi, A. Khoreva, R. Benenson, B. Schiele, and A. Sorkine-Hornung. Learning video object segmentation from static images. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 2, 3, 7\n\nA benchmark dataset and evaluation methodology for video object segmentation. F Perazzi, J Pont-Tuset, B Mcwilliams, L Van Gool, M Gross, A Sorkine-Hornung, IEEE Conference on Computer Vision and Pattern Recognition. 811F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In IEEE Conference on Computer Vision and Pattern Recog- nition, 2017. 1, 3, 6, 7, 8, 11\n\nFully connected object proposals for video segmentation. F Perazzi, O Wang, M Gross, A Sorkine-Hornung, IEEE International Conference on Computer Vision. F. Perazzi, O. Wang, M. Gross, and A. Sorkine-Hornung. Fully connected object proposals for video segmentation. In IEEE International Conference on Computer Vision, 2015. 3\n\nColor-Based Probabilistic Tracking. P P\u00e9rez, C Hue, J Vermaak, M Gangnet, European Conference on Computer Vision. P. P\u00e9rez, C. Hue, J. Vermaak, and M. Gangnet. Color-Based Probabilistic Tracking. In European Conference on Com- puter Vision, 2002. 2\n\nLearning to segment object candidates. P O Pinheiro, R Collobert, P Doll\u00e1r, Advances in Neural Information Processing Systems. 24P. O. Pinheiro, R. Collobert, and P. Doll\u00e1r. Learning to seg- ment object candidates. In Advances in Neural Information Processing Systems, 2015. 2, 4\n\nLearning to refine object segments. P O Pinheiro, T.-Y Lin, R Collobert, P Doll\u00e1r, European Conference on Computer Vision. 49P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll\u00e1r. Learn- ing to refine object segments. In European Conference on Computer Vision, 2016. 4, 7, 9\n\nThe 2017 davis challenge on video object segmentation. J Pont-Tuset, F Perazzi, S Caelles, P Arbel\u00e1ez, A Sorkine-Hornung, L Van Gool, arXiv:1704.00675811arXiv preprintJ. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel\u00e1ez, A. Sorkine- Hornung, and L. Van Gool. The 2017 davis chal- lenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 6, 8, 11\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in Neural Information Processing Systems. 23S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems, 2015. 2, 3\n\nAn overview of multi-task learning in. S Ruder, arXiv:1706.05098deep neural networks. arXiv preprintS. Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017. 8\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International Journal of Computer Vision. 5O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. In- ternational Journal of Computer Vision, 2015. 5\n\nVisual tracking: An experimental survey. A W Smeulders, D M Chu, R Cucchiara, S Calderara, A Dehghan, M Shah, IEEE Transactions on Pattern Analysis and Machine Intelligence. 13A. W. Smeulders, D. M. Chu, R. Cucchiara, S. Calderara, A. Dehghan, and M. Shah. Visual tracking: An experimental survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014. 1, 2, 3\n\nLearning spatial-aware regressions for visual tracking. C Sun, H Lu, M.-H Yang, IEEE Conference on Computer Vision and Pattern Recognition. C. Sun, H. Lu, and M.-H. Yang. Learning spatial-aware re- gressions for visual tracking. In IEEE Conference on Com- puter Vision and Pattern Recognition, 2018. 7\n\nSiamese instance search for tracking. R Tao, E Gavves, A W Smeulders, IEEE Conference on Computer Vision and Pattern Recognition. R. Tao, E. Gavves, and A. W. Smeulders. Siamese instance search for tracking. In IEEE Conference on Computer Vision and Pattern Recognition, 2016. 2\n\nVideo segmentation via object flow. Y.-H Tsai, M.-H Yang, M J Black, IEEE Conference on Computer Vision and Pattern Recognition. 27Y.-H. Tsai, M.-H. Yang, and M. J. Black. Video segmenta- tion via object flow. In IEEE Conference on Computer Vision and Pattern Recognition, 2016. 2, 3, 7\n\nEnd-to-end representation learning for correlation filter based tracking. J Valmadre, L Bertinetto, J Henriques, A Vedaldi, P H S Torr, IEEE Conference on Computer Vision and Pattern Recognition. J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and P. H. S. Torr. End-to-end representation learning for correla- tion filter based tracking. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 2\n\nLong-term tracking in the wild: A benchmark. J Valmadre, * , L Bertinetto, * , J F Henriques, R Tao, A Vedaldi, A Smeulders, P H S Torr, E Gavves, * , European Conference on Computer Vision. J. Valmadre*, L. Bertinetto*, J. F. Henriques, R. Tao, A. Vedaldi, A. Smeulders, P. H. S. Torr, and E. Gavves*. Long-term tracking in the wild: A benchmark. In European Conference on Computer Vision, 2018. 1\n\nOnline adaptation of convolutional neural networks for video object segmentation. P Voigtlaender, B Leibe, British Machine Vision Conference. 27P. Voigtlaender and B. Leibe. Online adaptation of convo- lutional neural networks for video object segmentation. In British Machine Vision Conference, 2017. 2, 3, 7\n\nJots: Joint online tracking and segmentation. L Wen, D Du, Z Lei, S Z Li, M.-H Yang, IEEE Conference on Computer Vision and Pattern Recognition. 27L. Wen, D. Du, Z. Lei, S. Z. Li, and M.-H. Yang. Jots: Joint online tracking and segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, 2015. 2, 3, 7\n\nOnline object tracking: A benchmark. Y Wu, J Lim, M.-H Yang, IEEE Conference on Computer Vision and Pattern Recognition. Y. Wu, J. Lim, and M.-H. Yang. Online object tracking: A benchmark. In IEEE Conference on Computer Vision and Pattern Recognition, 2013. 1\n\nFast video object segmentation by reference-guided mask propagation. S Oh, J.-Y Lee, K Sunkavalli, S. Joo Kim, IEEE Conference on Computer Vision and Pattern Recognition. 27S. Wug Oh, J.-Y. Lee, K. Sunkavalli, and S. Joo Kim. Fast video object segmentation by reference-guided mask propa- gation. In IEEE Conference on Computer Vision and Pattern Recognition, 2018. 2, 3, 7\n\nYoutube-vos: Sequence-tosequence video object segmentation. N Xu, L Yang, Y Fan, J Yang, D Yue, Y Liang, B Price, S Cohen, T Huang, European Conference on Computer Vision. 5N. Xu, L. Yang, Y. Fan, J. Yang, D. Yue, Y. Liang, B. Price, S. Cohen, and T. Huang. Youtube-vos: Sequence-to- sequence video object segmentation. In European Confer- ence on Computer Vision, 2018. 2, 5, 8\n\nRecent advances and trends in visual tracking: A review. Neurocomputing. H Yang, L Shao, F Zheng, L Wang, Z Song, H. Yang, L. Shao, F. Zheng, L. Wang, and Z. Song. Recent advances and trends in visual tracking: A review. Neurocom- puting, 2011. 1\n\nEfficient video object segmentation via network modulation. L Yang, Y Wang, X Xiong, J Yang, A K Katsaggelos, IEEE Conference on Computer Vision and Pattern Recognition. 7L. Yang, Y. Wang, X. Xiong, J. Yang, and A. K. Katsaggelos. Efficient video object segmentation via network modulation. In IEEE Conference on Computer Vision and Pattern Recog- nition, June 2018. 2, 3, 7\n\nLearning dynamic memory networks for object tracking. T Yang, A B Chan, European Conference on Computer Vision. 23T. Yang and A. B. Chan. Learning dynamic memory net- works for object tracking. In European Conference on Com- puter Vision, 2018. 2, 3\n\nSuperpixel-based tracking-by-segmentation using markov chains. D Yeo, J Son, B Han, J H Han, IEEE Conference on Computer Vision and Pattern Recognition. D. Yeo, J. Son, B. Han, and J. H. Han. Superpixel-based tracking-by-segmentation using markov chains. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 2\n\nObject tracking: A survey. A Yilmaz, O Javed, M Shah, Acm computing surveys (CSUR). A. Yilmaz, O. Javed, and M. Shah. Object tracking: A sur- vey. Acm computing surveys (CSUR), 2006. 1\n\nPixel-level matching for video object segmentation using convolutional neural networks. J S Yoon, F Rameau, J Kim, S Lee, S Shin, I S Kweon, IEEE International Conference on Computer Vision. J. S. Yoon, F. Rameau, J. Kim, S. Lee, S. Shin, and I. S. Kweon. Pixel-level matching for video object segmentation using convolutional neural networks. In IEEE International Conference on Computer Vision, 2017. 7\n\nDistractor-aware siamese networks for visual object tracking. Z Zhu, Q Wang, B Li, W Wu, J Yan, W Hu, European Conference on Computer Vision. 67Z. Zhu, Q. Wang, B. Li, W. Wu, J. Yan, and W. Hu. Distractor-aware siamese networks for visual object track- ing. In European Conference on Computer Vision, 2018. 2, 3, 4, 5, 6, 7\n", "annotations": {"author": "[{\"end\":105,\"start\":69},{\"end\":115,\"start\":106},{\"end\":146,\"start\":116},{\"end\":153,\"start\":147},{\"end\":195,\"start\":154},{\"end\":248,\"start\":196},{\"end\":272,\"start\":249}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":75},{\"end\":114,\"start\":109},{\"end\":126,\"start\":124},{\"end\":169,\"start\":165}]", "author_first_name": "[{\"end\":74,\"start\":69},{\"end\":108,\"start\":106},{\"end\":123,\"start\":116},{\"end\":152,\"start\":147},{\"end\":160,\"start\":154},{\"end\":164,\"start\":161}]", "author_affiliation": "[{\"end\":247,\"start\":197},{\"end\":271,\"start\":250}]", "title": "[{\"end\":66,\"start\":1},{\"end\":338,\"start\":273}]", "venue": null, "abstract": "[{\"end\":1236,\"start\":340}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1438,\"start\":1434},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":1784,\"start\":1780},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":1787,\"start\":1784},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":1790,\"start\":1787},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2346,\"start\":2342},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2524,\"start\":2520},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":2659,\"start\":2655},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2662,\"start\":2659},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2665,\"start\":2662},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2668,\"start\":2665},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2671,\"start\":2668},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2687,\"start\":2683},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3205,\"start\":3201},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3334,\"start\":3330},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3355,\"start\":3351},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3584,\"start\":3580},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":3587,\"start\":3584},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3590,\"start\":3587},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3592,\"start\":3590},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":3670,\"start\":3666},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3673,\"start\":3670},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":3676,\"start\":3673},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3679,\"start\":3676},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3682,\"start\":3679},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3685,\"start\":3682},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3688,\"start\":3685},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4109,\"start\":4106},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4112,\"start\":4109},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":4226,\"start\":4222},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4735,\"start\":4732},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5136,\"start\":5132},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5139,\"start\":5136},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5182,\"start\":5178},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6183,\"start\":6179},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6186,\"start\":6183},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6188,\"start\":6186},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":6191,\"start\":6188},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6215,\"start\":6211},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6218,\"start\":6215},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6240,\"start\":6236},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6242,\"start\":6240},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6245,\"start\":6242},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6248,\"start\":6245},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6251,\"start\":6248},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7047,\"start\":7044},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7050,\"start\":7047},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7354,\"start\":7351},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7488,\"start\":7484},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7491,\"start\":7488},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7517,\"start\":7513},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7520,\"start\":7517},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7523,\"start\":7520},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7526,\"start\":7523},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7555,\"start\":7551},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7558,\"start\":7555},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7626,\"start\":7623},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7629,\"start\":7626},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7632,\"start\":7629},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7909,\"start\":7906},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7991,\"start\":7987},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":8018,\"start\":8014},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8035,\"start\":8031},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8060,\"start\":8056},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8640,\"start\":8636},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8643,\"start\":8640},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8645,\"start\":8643},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8875,\"start\":8871},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9240,\"start\":9236},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9259,\"start\":9255},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9488,\"start\":9484},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9491,\"start\":9488},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9638,\"start\":9634},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9751,\"start\":9747},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9897,\"start\":9893},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9900,\"start\":9897},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10108,\"start\":10104},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10111,\"start\":10108},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10114,\"start\":10111},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10117,\"start\":10114},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10119,\"start\":10117},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10151,\"start\":10148},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10422,\"start\":10418},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10425,\"start\":10422},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10428,\"start\":10425},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10529,\"start\":10525},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10760,\"start\":10756},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11109,\"start\":11105},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11112,\"start\":11109},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11114,\"start\":11112},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":11117,\"start\":11114},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11141,\"start\":11137},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11144,\"start\":11141},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11166,\"start\":11162},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11168,\"start\":11166},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11171,\"start\":11168},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11174,\"start\":11171},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11177,\"start\":11174},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11357,\"start\":11353},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11360,\"start\":11357},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11379,\"start\":11375},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11381,\"start\":11379},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11524,\"start\":11520},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":11527,\"start\":11524},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11530,\"start\":11527},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11533,\"start\":11530},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11536,\"start\":11533},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11539,\"start\":11536},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":11683,\"start\":11679},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":11703,\"start\":11699},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12110,\"start\":12107},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12245,\"start\":12242},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12248,\"start\":12245},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":12251,\"start\":12248},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":12254,\"start\":12251},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12257,\"start\":12254},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12294,\"start\":12291},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12311,\"start\":12307},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12499,\"start\":12496},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13542,\"start\":13539},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13732,\"start\":13728},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13830,\"start\":13826},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16155,\"start\":16151},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16175,\"start\":16171},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16284,\"start\":16280},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16287,\"start\":16284},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":16982,\"start\":16978},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17069,\"start\":17065},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17324,\"start\":17321},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17341,\"start\":17337},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17702,\"start\":17698},{\"end\":17714,\"start\":17702},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17945,\"start\":17942},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":18085,\"start\":18081},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18377,\"start\":18373},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18744,\"start\":18740},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18914,\"start\":18910},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19199,\"start\":19196},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19434,\"start\":19431},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20038,\"start\":20034},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":20057,\"start\":20053},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":20078,\"start\":20074},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21312,\"start\":21308},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21330,\"start\":21326},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21804,\"start\":21800},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21943,\"start\":21940},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21946,\"start\":21943},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":21949,\"start\":21946},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21952,\"start\":21949},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21969,\"start\":21965},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21972,\"start\":21969},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":21975,\"start\":21972},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23038,\"start\":23037},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23725,\"start\":23721},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":25046,\"start\":25042},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25194,\"start\":25190},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25567,\"start\":25563},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25579,\"start\":25575},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25592,\"start\":25588},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25753,\"start\":25749},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25929,\"start\":25926},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":26481,\"start\":26477},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":26501,\"start\":26497},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":26847,\"start\":26843},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27233,\"start\":27229},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27562,\"start\":27558},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27565,\"start\":27562},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27567,\"start\":27565},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":27570,\"start\":27567},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27594,\"start\":27590},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27597,\"start\":27594},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":27624,\"start\":27620},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27627,\"start\":27624},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27630,\"start\":27627},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27632,\"start\":27630},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27654,\"start\":27650},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27656,\"start\":27654},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27659,\"start\":27656},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27662,\"start\":27659},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27665,\"start\":27662},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":28177,\"start\":28173},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28189,\"start\":28185},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":28348,\"start\":28344},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":28362,\"start\":28358},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":28430,\"start\":28426},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28944,\"start\":28940},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":29741,\"start\":29737},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30033,\"start\":30030},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":30294,\"start\":30290},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30489,\"start\":30486},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":30492,\"start\":30489},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":32180,\"start\":32176},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33192,\"start\":33188},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33456,\"start\":33453},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":34193,\"start\":34189},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":35075,\"start\":35071},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":35095,\"start\":35091},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35630,\"start\":35626},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36147,\"start\":36143},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":36182,\"start\":36178},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":36221,\"start\":36217},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36554,\"start\":36550}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35373,\"start\":35097},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35638,\"start\":35374},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35882,\"start\":35639},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35966,\"start\":35883},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36222,\"start\":35967},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36333,\"start\":36223},{\"attributes\":{\"id\":\"fig_6\"},\"end\":36343,\"start\":36334},{\"attributes\":{\"id\":\"fig_8\"},\"end\":36555,\"start\":36344},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37583,\"start\":36556},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37635,\"start\":37584},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37980,\"start\":37636},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38537,\"start\":37981},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38633,\"start\":38538},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38742,\"start\":38634},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":39198,\"start\":38743}]", "paragraph": "[{\"end\":1791,\"start\":1252},{\"end\":2031,\"start\":1793},{\"end\":2470,\"start\":2033},{\"end\":2863,\"start\":2472},{\"end\":3750,\"start\":2865},{\"end\":4451,\"start\":3752},{\"end\":5488,\"start\":4453},{\"end\":6252,\"start\":5490},{\"end\":6566,\"start\":6254},{\"end\":6691,\"start\":6583},{\"end\":7560,\"start\":6693},{\"end\":8061,\"start\":7562},{\"end\":8515,\"start\":8063},{\"end\":9390,\"start\":8517},{\"end\":9901,\"start\":9437},{\"end\":10344,\"start\":9903},{\"end\":10961,\"start\":10346},{\"end\":11426,\"start\":10963},{\"end\":11996,\"start\":11428},{\"end\":12429,\"start\":12012},{\"end\":12968,\"start\":12470},{\"end\":13002,\"start\":12999},{\"end\":14299,\"start\":13004},{\"end\":14785,\"start\":14312},{\"end\":14972,\"start\":14787},{\"end\":15774,\"start\":15002},{\"end\":17547,\"start\":15843},{\"end\":18544,\"start\":17641},{\"end\":18819,\"start\":18546},{\"end\":19407,\"start\":18846},{\"end\":20718,\"start\":19409},{\"end\":20846,\"start\":20720},{\"end\":21150,\"start\":20862},{\"end\":21805,\"start\":21192},{\"end\":22440,\"start\":21807},{\"end\":24484,\"start\":22442},{\"end\":24701,\"start\":24486},{\"end\":25999,\"start\":24703},{\"end\":26848,\"start\":26038},{\"end\":27115,\"start\":26850},{\"end\":29196,\"start\":27151},{\"end\":29399,\"start\":29198},{\"end\":29510,\"start\":29420},{\"end\":31409,\"start\":29512},{\"end\":31896,\"start\":31411},{\"end\":32266,\"start\":31898},{\"end\":33012,\"start\":32281},{\"end\":34456,\"start\":33014},{\"end\":34829,\"start\":34491},{\"end\":35096,\"start\":34831}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12998,\"start\":12969},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15001,\"start\":14973},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15842,\"start\":15775},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17586,\"start\":17548},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17640,\"start\":17586}]", "table_ref": "[{\"end\":18817,\"start\":18810},{\"end\":22452,\"start\":22445},{\"end\":23480,\"start\":23473},{\"end\":24734,\"start\":24727},{\"end\":25694,\"start\":25687},{\"end\":27276,\"start\":27269},{\"end\":27923,\"start\":27915},{\"end\":28007,\"start\":28000},{\"end\":28640,\"start\":28633},{\"end\":28871,\"start\":28864},{\"end\":29544,\"start\":29537},{\"end\":29770,\"start\":29763},{\"end\":30669,\"start\":30662},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33071,\"start\":33064},{\"end\":34081,\"start\":34074}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1250,\"start\":1238},{\"attributes\":{\"n\":\"2.\"},\"end\":6581,\"start\":6569},{\"end\":9435,\"start\":9393},{\"attributes\":{\"n\":\"3.\"},\"end\":12010,\"start\":11999},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12468,\"start\":12432},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14310,\"start\":14302},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18844,\"start\":18822},{\"attributes\":{\"n\":\"4.\"},\"end\":20860,\"start\":20849},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21190,\"start\":21153},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26036,\"start\":26002},{\"end\":27149,\"start\":27118},{\"attributes\":{\"n\":\"4.3.\"},\"end\":29418,\"start\":29402},{\"attributes\":{\"n\":\"5.\"},\"end\":32279,\"start\":32269},{\"end\":34489,\"start\":34459},{\"end\":35108,\"start\":35098},{\"end\":35385,\"start\":35375},{\"end\":35650,\"start\":35640},{\"end\":35890,\"start\":35884},{\"end\":35978,\"start\":35968},{\"end\":36244,\"start\":36224},{\"end\":36366,\"start\":36345},{\"end\":37594,\"start\":37585},{\"end\":38548,\"start\":38539},{\"end\":38762,\"start\":38744}]", "table": "[{\"end\":37583,\"start\":36598},{\"end\":37980,\"start\":37671},{\"end\":38537,\"start\":38172},{\"end\":38742,\"start\":38709},{\"end\":39198,\"start\":38928}]", "figure_caption": "[{\"end\":35373,\"start\":35110},{\"end\":35638,\"start\":35387},{\"end\":35882,\"start\":35652},{\"end\":35966,\"start\":35892},{\"end\":36222,\"start\":35980},{\"end\":36333,\"start\":36247},{\"end\":36343,\"start\":36336},{\"end\":36555,\"start\":36370},{\"end\":36598,\"start\":36558},{\"end\":37635,\"start\":37596},{\"end\":37671,\"start\":37638},{\"end\":38172,\"start\":37983},{\"end\":38633,\"start\":38550},{\"end\":38709,\"start\":38636},{\"end\":38928,\"start\":38765}]", "figure_ref": "[{\"end\":2056,\"start\":2048},{\"end\":8374,\"start\":8366},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16679,\"start\":16671},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16949,\"start\":16941},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18220,\"start\":18212},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18542,\"start\":18534},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22584,\"start\":22576},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23579,\"start\":23570},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26942,\"start\":26934},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27939,\"start\":27931},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28695,\"start\":28687},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29284,\"start\":29276},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29635,\"start\":29626},{\"end\":31842,\"start\":31834},{\"end\":32018,\"start\":32009},{\"end\":34331,\"start\":34323},{\"end\":34409,\"start\":34401},{\"end\":34723,\"start\":34715},{\"end\":34927,\"start\":34918}]", "bib_author_first_name": "[{\"end\":39400,\"start\":39399},{\"end\":39414,\"start\":39410},{\"end\":39422,\"start\":39421},{\"end\":39763,\"start\":39762},{\"end\":39770,\"start\":39769},{\"end\":39776,\"start\":39775},{\"end\":40095,\"start\":40094},{\"end\":40109,\"start\":40108},{\"end\":40111,\"start\":40110},{\"end\":40124,\"start\":40123},{\"end\":40136,\"start\":40135},{\"end\":40140,\"start\":40137},{\"end\":40148,\"start\":40147},{\"end\":40450,\"start\":40449},{\"end\":40464,\"start\":40463},{\"end\":40476,\"start\":40475},{\"end\":40478,\"start\":40477},{\"end\":40491,\"start\":40490},{\"end\":40502,\"start\":40501},{\"end\":40504,\"start\":40503},{\"end\":40833,\"start\":40832},{\"end\":40842,\"start\":40841},{\"end\":41089,\"start\":41088},{\"end\":41091,\"start\":41090},{\"end\":41100,\"start\":41099},{\"end\":41102,\"start\":41101},{\"end\":41115,\"start\":41114},{\"end\":41117,\"start\":41116},{\"end\":41127,\"start\":41126},{\"end\":41129,\"start\":41128},{\"end\":41424,\"start\":41423},{\"end\":41438,\"start\":41434},{\"end\":41449,\"start\":41448},{\"end\":41463,\"start\":41462},{\"end\":41477,\"start\":41476},{\"end\":41488,\"start\":41487},{\"end\":41794,\"start\":41793},{\"end\":41980,\"start\":41976},{\"end\":41988,\"start\":41987},{\"end\":42002,\"start\":42001},{\"end\":42014,\"start\":42013},{\"end\":42024,\"start\":42023},{\"end\":42026,\"start\":42025},{\"end\":42438,\"start\":42437},{\"end\":42446,\"start\":42445},{\"end\":42460,\"start\":42459},{\"end\":42470,\"start\":42469},{\"end\":42823,\"start\":42822},{\"end\":42835,\"start\":42831},{\"end\":42846,\"start\":42842},{\"end\":42854,\"start\":42853},{\"end\":42865,\"start\":42861},{\"end\":43219,\"start\":43218},{\"end\":43231,\"start\":43227},{\"end\":43239,\"start\":43238},{\"end\":43250,\"start\":43246},{\"end\":43563,\"start\":43562},{\"end\":43569,\"start\":43568},{\"end\":43577,\"start\":43576},{\"end\":43835,\"start\":43834},{\"end\":43848,\"start\":43847},{\"end\":43858,\"start\":43857},{\"end\":44140,\"start\":44139},{\"end\":44153,\"start\":44152},{\"end\":44161,\"start\":44160},{\"end\":44163,\"start\":44162},{\"end\":44171,\"start\":44170},{\"end\":44505,\"start\":44504},{\"end\":44518,\"start\":44517},{\"end\":44527,\"start\":44526},{\"end\":44529,\"start\":44528},{\"end\":44537,\"start\":44536},{\"end\":44866,\"start\":44865},{\"end\":44872,\"start\":44871},{\"end\":44879,\"start\":44878},{\"end\":44887,\"start\":44886},{\"end\":45180,\"start\":45179},{\"end\":45186,\"start\":45185},{\"end\":45193,\"start\":45192},{\"end\":45201,\"start\":45200},{\"end\":45446,\"start\":45445},{\"end\":45452,\"start\":45451},{\"end\":45464,\"start\":45463},{\"end\":45474,\"start\":45473},{\"end\":45705,\"start\":45704},{\"end\":45711,\"start\":45710},{\"end\":45720,\"start\":45719},{\"end\":45727,\"start\":45726},{\"end\":46014,\"start\":46013},{\"end\":46022,\"start\":46021},{\"end\":46031,\"start\":46030},{\"end\":46292,\"start\":46291},{\"end\":46294,\"start\":46293},{\"end\":46307,\"start\":46306},{\"end\":46318,\"start\":46317},{\"end\":46329,\"start\":46328},{\"end\":46654,\"start\":46650},{\"end\":46663,\"start\":46659},{\"end\":46672,\"start\":46671},{\"end\":46674,\"start\":46673},{\"end\":46906,\"start\":46905},{\"end\":46917,\"start\":46916},{\"end\":46926,\"start\":46925},{\"end\":46928,\"start\":46927},{\"end\":47189,\"start\":47188},{\"end\":47200,\"start\":47199},{\"end\":47212,\"start\":47211},{\"end\":47219,\"start\":47218},{\"end\":47227,\"start\":47226},{\"end\":47536,\"start\":47535},{\"end\":47549,\"start\":47548},{\"end\":47556,\"start\":47555},{\"end\":47799,\"start\":47798},{\"end\":47812,\"start\":47811},{\"end\":47819,\"start\":47818},{\"end\":48101,\"start\":48100},{\"end\":48112,\"start\":48111},{\"end\":48125,\"start\":48124},{\"end\":48134,\"start\":48133},{\"end\":48146,\"start\":48145},{\"end\":48161,\"start\":48160},{\"end\":48172,\"start\":48171},{\"end\":48181,\"start\":48180},{\"end\":48190,\"start\":48189},{\"end\":48201,\"start\":48200},{\"end\":48557,\"start\":48556},{\"end\":48568,\"start\":48567},{\"end\":48581,\"start\":48580},{\"end\":48590,\"start\":48589},{\"end\":48602,\"start\":48601},{\"end\":48616,\"start\":48615},{\"end\":48618,\"start\":48617},{\"end\":48626,\"start\":48625},{\"end\":48635,\"start\":48634},{\"end\":48643,\"start\":48642},{\"end\":48654,\"start\":48653},{\"end\":48667,\"start\":48666},{\"end\":49083,\"start\":49082},{\"end\":49094,\"start\":49093},{\"end\":49103,\"start\":49102},{\"end\":49116,\"start\":49115},{\"end\":49125,\"start\":49124},{\"end\":49140,\"start\":49139},{\"end\":49153,\"start\":49152},{\"end\":49164,\"start\":49163},{\"end\":49175,\"start\":49174},{\"end\":49590,\"start\":49589},{\"end\":49596,\"start\":49595},{\"end\":49603,\"start\":49602},{\"end\":49609,\"start\":49608},{\"end\":49616,\"start\":49615},{\"end\":49961,\"start\":49960},{\"end\":49967,\"start\":49966},{\"end\":49975,\"start\":49974},{\"end\":49982,\"start\":49981},{\"end\":49994,\"start\":49990},{\"end\":50368,\"start\":50367},{\"end\":50374,\"start\":50373},{\"end\":50376,\"start\":50375},{\"end\":50671,\"start\":50670},{\"end\":50680,\"start\":50679},{\"end\":50690,\"start\":50689},{\"end\":50942,\"start\":50938},{\"end\":50949,\"start\":50948},{\"end\":50958,\"start\":50957},{\"end\":50970,\"start\":50969},{\"end\":50978,\"start\":50977},{\"end\":50988,\"start\":50987},{\"end\":50999,\"start\":50998},{\"end\":51009,\"start\":51008},{\"end\":51011,\"start\":51010},{\"end\":51315,\"start\":51314},{\"end\":51323,\"start\":51322},{\"end\":51336,\"start\":51335},{\"end\":51646,\"start\":51645},{\"end\":51657,\"start\":51656},{\"end\":51666,\"start\":51665},{\"end\":51668,\"start\":51667},{\"end\":51676,\"start\":51675},{\"end\":51685,\"start\":51684},{\"end\":52018,\"start\":52017},{\"end\":52030,\"start\":52029},{\"end\":52032,\"start\":52031},{\"end\":52043,\"start\":52042},{\"end\":52045,\"start\":52044},{\"end\":52233,\"start\":52229},{\"end\":52244,\"start\":52243},{\"end\":52255,\"start\":52254},{\"end\":52263,\"start\":52262},{\"end\":52277,\"start\":52276},{\"end\":52291,\"start\":52290},{\"end\":52302,\"start\":52301},{\"end\":52652,\"start\":52651},{\"end\":52661,\"start\":52660},{\"end\":52672,\"start\":52671},{\"end\":52680,\"start\":52679},{\"end\":53002,\"start\":53001},{\"end\":53015,\"start\":53011},{\"end\":53028,\"start\":53027},{\"end\":53030,\"start\":53029},{\"end\":53038,\"start\":53037},{\"end\":53349,\"start\":53348},{\"end\":53360,\"start\":53359},{\"end\":53369,\"start\":53368},{\"end\":53636,\"start\":53635},{\"end\":53646,\"start\":53645},{\"end\":53654,\"start\":53653},{\"end\":53666,\"start\":53665},{\"end\":53680,\"start\":53679},{\"end\":53956,\"start\":53955},{\"end\":54120,\"start\":54119},{\"end\":54131,\"start\":54130},{\"end\":54142,\"start\":54141},{\"end\":54154,\"start\":54153},{\"end\":54165,\"start\":54164},{\"end\":54530,\"start\":54529},{\"end\":54541,\"start\":54540},{\"end\":54555,\"start\":54554},{\"end\":54569,\"start\":54568},{\"end\":54581,\"start\":54580},{\"end\":54590,\"start\":54589},{\"end\":54986,\"start\":54985},{\"end\":54997,\"start\":54996},{\"end\":55005,\"start\":55004},{\"end\":55014,\"start\":55013},{\"end\":55293,\"start\":55292},{\"end\":55302,\"start\":55301},{\"end\":55309,\"start\":55308},{\"end\":55320,\"start\":55319},{\"end\":55546,\"start\":55545},{\"end\":55548,\"start\":55547},{\"end\":55560,\"start\":55559},{\"end\":55573,\"start\":55572},{\"end\":55824,\"start\":55823},{\"end\":55826,\"start\":55825},{\"end\":55841,\"start\":55837},{\"end\":55848,\"start\":55847},{\"end\":55861,\"start\":55860},{\"end\":56120,\"start\":56119},{\"end\":56134,\"start\":56133},{\"end\":56145,\"start\":56144},{\"end\":56156,\"start\":56155},{\"end\":56168,\"start\":56167},{\"end\":56187,\"start\":56186},{\"end\":56508,\"start\":56507},{\"end\":56515,\"start\":56514},{\"end\":56521,\"start\":56520},{\"end\":56533,\"start\":56532},{\"end\":56818,\"start\":56817},{\"end\":57042,\"start\":57041},{\"end\":57057,\"start\":57056},{\"end\":57065,\"start\":57064},{\"end\":57071,\"start\":57070},{\"end\":57081,\"start\":57080},{\"end\":57093,\"start\":57092},{\"end\":57099,\"start\":57098},{\"end\":57108,\"start\":57107},{\"end\":57120,\"start\":57119},{\"end\":57130,\"start\":57129},{\"end\":57449,\"start\":57448},{\"end\":57451,\"start\":57450},{\"end\":57464,\"start\":57463},{\"end\":57466,\"start\":57465},{\"end\":57473,\"start\":57472},{\"end\":57486,\"start\":57485},{\"end\":57499,\"start\":57498},{\"end\":57510,\"start\":57509},{\"end\":57841,\"start\":57840},{\"end\":57848,\"start\":57847},{\"end\":57857,\"start\":57853},{\"end\":58126,\"start\":58125},{\"end\":58133,\"start\":58132},{\"end\":58143,\"start\":58142},{\"end\":58145,\"start\":58144},{\"end\":58407,\"start\":58403},{\"end\":58418,\"start\":58414},{\"end\":58426,\"start\":58425},{\"end\":58428,\"start\":58427},{\"end\":58730,\"start\":58729},{\"end\":58742,\"start\":58741},{\"end\":58756,\"start\":58755},{\"end\":58769,\"start\":58768},{\"end\":58780,\"start\":58779},{\"end\":58784,\"start\":58781},{\"end\":59118,\"start\":59117},{\"end\":59130,\"start\":59129},{\"end\":59134,\"start\":59133},{\"end\":59148,\"start\":59147},{\"end\":59152,\"start\":59151},{\"end\":59154,\"start\":59153},{\"end\":59167,\"start\":59166},{\"end\":59174,\"start\":59173},{\"end\":59185,\"start\":59184},{\"end\":59198,\"start\":59197},{\"end\":59202,\"start\":59199},{\"end\":59210,\"start\":59209},{\"end\":59220,\"start\":59219},{\"end\":59555,\"start\":59554},{\"end\":59571,\"start\":59570},{\"end\":59830,\"start\":59829},{\"end\":59837,\"start\":59836},{\"end\":59843,\"start\":59842},{\"end\":59850,\"start\":59849},{\"end\":59852,\"start\":59851},{\"end\":59861,\"start\":59857},{\"end\":60141,\"start\":60140},{\"end\":60147,\"start\":60146},{\"end\":60157,\"start\":60153},{\"end\":60434,\"start\":60433},{\"end\":60443,\"start\":60439},{\"end\":60450,\"start\":60449},{\"end\":60469,\"start\":60463},{\"end\":60800,\"start\":60799},{\"end\":60806,\"start\":60805},{\"end\":60814,\"start\":60813},{\"end\":60821,\"start\":60820},{\"end\":60829,\"start\":60828},{\"end\":60836,\"start\":60835},{\"end\":60845,\"start\":60844},{\"end\":60854,\"start\":60853},{\"end\":60863,\"start\":60862},{\"end\":61193,\"start\":61192},{\"end\":61201,\"start\":61200},{\"end\":61209,\"start\":61208},{\"end\":61218,\"start\":61217},{\"end\":61226,\"start\":61225},{\"end\":61428,\"start\":61427},{\"end\":61436,\"start\":61435},{\"end\":61444,\"start\":61443},{\"end\":61453,\"start\":61452},{\"end\":61461,\"start\":61460},{\"end\":61463,\"start\":61462},{\"end\":61798,\"start\":61797},{\"end\":61806,\"start\":61805},{\"end\":61808,\"start\":61807},{\"end\":62058,\"start\":62057},{\"end\":62065,\"start\":62064},{\"end\":62072,\"start\":62071},{\"end\":62079,\"start\":62078},{\"end\":62081,\"start\":62080},{\"end\":62349,\"start\":62348},{\"end\":62359,\"start\":62358},{\"end\":62368,\"start\":62367},{\"end\":62596,\"start\":62595},{\"end\":62598,\"start\":62597},{\"end\":62606,\"start\":62605},{\"end\":62616,\"start\":62615},{\"end\":62623,\"start\":62622},{\"end\":62630,\"start\":62629},{\"end\":62638,\"start\":62637},{\"end\":62640,\"start\":62639},{\"end\":62976,\"start\":62975},{\"end\":62983,\"start\":62982},{\"end\":62991,\"start\":62990},{\"end\":62997,\"start\":62996},{\"end\":63003,\"start\":63002},{\"end\":63010,\"start\":63009}]", "bib_author_last_name": "[{\"end\":39408,\"start\":39401},{\"end\":39419,\"start\":39415},{\"end\":39431,\"start\":39423},{\"end\":39767,\"start\":39764},{\"end\":39773,\"start\":39771},{\"end\":39780,\"start\":39777},{\"end\":40106,\"start\":40096},{\"end\":40121,\"start\":40112},{\"end\":40133,\"start\":40125},{\"end\":40145,\"start\":40141},{\"end\":40156,\"start\":40149},{\"end\":40461,\"start\":40451},{\"end\":40473,\"start\":40465},{\"end\":40488,\"start\":40479},{\"end\":40499,\"start\":40492},{\"end\":40509,\"start\":40505},{\"end\":40839,\"start\":40834},{\"end\":40847,\"start\":40843},{\"end\":41097,\"start\":41092},{\"end\":41112,\"start\":41103},{\"end\":41124,\"start\":41118},{\"end\":41133,\"start\":41130},{\"end\":41432,\"start\":41425},{\"end\":41446,\"start\":41439},{\"end\":41460,\"start\":41450},{\"end\":41474,\"start\":41464},{\"end\":41485,\"start\":41478},{\"end\":41497,\"start\":41489},{\"end\":41802,\"start\":41795},{\"end\":41985,\"start\":41981},{\"end\":41999,\"start\":41989},{\"end\":42011,\"start\":42003},{\"end\":42021,\"start\":42015},{\"end\":42033,\"start\":42027},{\"end\":42443,\"start\":42439},{\"end\":42457,\"start\":42447},{\"end\":42467,\"start\":42461},{\"end\":42479,\"start\":42471},{\"end\":42829,\"start\":42824},{\"end\":42840,\"start\":42836},{\"end\":42851,\"start\":42847},{\"end\":42859,\"start\":42855},{\"end\":42870,\"start\":42866},{\"end\":43225,\"start\":43220},{\"end\":43236,\"start\":43232},{\"end\":43244,\"start\":43240},{\"end\":43255,\"start\":43251},{\"end\":43566,\"start\":43564},{\"end\":43574,\"start\":43570},{\"end\":43582,\"start\":43578},{\"end\":43845,\"start\":43836},{\"end\":43855,\"start\":43849},{\"end\":43863,\"start\":43859},{\"end\":44150,\"start\":44141},{\"end\":44158,\"start\":44154},{\"end\":44168,\"start\":44164},{\"end\":44180,\"start\":44172},{\"end\":44515,\"start\":44506},{\"end\":44524,\"start\":44519},{\"end\":44534,\"start\":44530},{\"end\":44546,\"start\":44538},{\"end\":44869,\"start\":44867},{\"end\":44876,\"start\":44873},{\"end\":44884,\"start\":44880},{\"end\":44892,\"start\":44888},{\"end\":45183,\"start\":45181},{\"end\":45190,\"start\":45187},{\"end\":45198,\"start\":45194},{\"end\":45206,\"start\":45202},{\"end\":45449,\"start\":45447},{\"end\":45461,\"start\":45453},{\"end\":45471,\"start\":45465},{\"end\":45483,\"start\":45475},{\"end\":45708,\"start\":45706},{\"end\":45717,\"start\":45712},{\"end\":45724,\"start\":45721},{\"end\":45731,\"start\":45728},{\"end\":46019,\"start\":46015},{\"end\":46028,\"start\":46023},{\"end\":46040,\"start\":46032},{\"end\":46304,\"start\":46295},{\"end\":46315,\"start\":46308},{\"end\":46326,\"start\":46319},{\"end\":46337,\"start\":46330},{\"end\":46657,\"start\":46655},{\"end\":46669,\"start\":46664},{\"end\":46682,\"start\":46675},{\"end\":46914,\"start\":46907},{\"end\":46923,\"start\":46918},{\"end\":46935,\"start\":46929},{\"end\":47197,\"start\":47190},{\"end\":47209,\"start\":47201},{\"end\":47216,\"start\":47213},{\"end\":47224,\"start\":47220},{\"end\":47235,\"start\":47228},{\"end\":47546,\"start\":47537},{\"end\":47553,\"start\":47550},{\"end\":47562,\"start\":47557},{\"end\":47809,\"start\":47800},{\"end\":47816,\"start\":47813},{\"end\":47825,\"start\":47820},{\"end\":48109,\"start\":48102},{\"end\":48122,\"start\":48113},{\"end\":48131,\"start\":48126},{\"end\":48143,\"start\":48135},{\"end\":48158,\"start\":48147},{\"end\":48169,\"start\":48162},{\"end\":48178,\"start\":48173},{\"end\":48187,\"start\":48182},{\"end\":48198,\"start\":48191},{\"end\":48211,\"start\":48202},{\"end\":48565,\"start\":48558},{\"end\":48578,\"start\":48569},{\"end\":48587,\"start\":48582},{\"end\":48599,\"start\":48591},{\"end\":48613,\"start\":48603},{\"end\":48623,\"start\":48619},{\"end\":48632,\"start\":48627},{\"end\":48640,\"start\":48636},{\"end\":48651,\"start\":48644},{\"end\":48664,\"start\":48655},{\"end\":48677,\"start\":48668},{\"end\":49091,\"start\":49084},{\"end\":49100,\"start\":49095},{\"end\":49113,\"start\":49104},{\"end\":49122,\"start\":49117},{\"end\":49137,\"start\":49126},{\"end\":49150,\"start\":49141},{\"end\":49161,\"start\":49154},{\"end\":49172,\"start\":49165},{\"end\":49183,\"start\":49176},{\"end\":49593,\"start\":49591},{\"end\":49600,\"start\":49597},{\"end\":49606,\"start\":49604},{\"end\":49613,\"start\":49610},{\"end\":49619,\"start\":49617},{\"end\":49964,\"start\":49962},{\"end\":49972,\"start\":49968},{\"end\":49979,\"start\":49976},{\"end\":49988,\"start\":49983},{\"end\":49999,\"start\":49995},{\"end\":50371,\"start\":50369},{\"end\":50380,\"start\":50377},{\"end\":50677,\"start\":50672},{\"end\":50687,\"start\":50681},{\"end\":50695,\"start\":50691},{\"end\":50946,\"start\":50943},{\"end\":50955,\"start\":50950},{\"end\":50967,\"start\":50959},{\"end\":50975,\"start\":50971},{\"end\":50985,\"start\":50979},{\"end\":50996,\"start\":50989},{\"end\":51006,\"start\":51000},{\"end\":51019,\"start\":51012},{\"end\":51320,\"start\":51316},{\"end\":51333,\"start\":51324},{\"end\":51344,\"start\":51337},{\"end\":51654,\"start\":51647},{\"end\":51663,\"start\":51658},{\"end\":51673,\"start\":51669},{\"end\":51682,\"start\":51677},{\"end\":51693,\"start\":51686},{\"end\":52027,\"start\":52019},{\"end\":52040,\"start\":52033},{\"end\":52051,\"start\":52046},{\"end\":52241,\"start\":52234},{\"end\":52252,\"start\":52245},{\"end\":52260,\"start\":52256},{\"end\":52274,\"start\":52264},{\"end\":52288,\"start\":52278},{\"end\":52299,\"start\":52292},{\"end\":52311,\"start\":52303},{\"end\":52658,\"start\":52653},{\"end\":52669,\"start\":52662},{\"end\":52677,\"start\":52673},{\"end\":52696,\"start\":52681},{\"end\":53009,\"start\":53003},{\"end\":53025,\"start\":53016},{\"end\":53035,\"start\":53031},{\"end\":53044,\"start\":53039},{\"end\":53357,\"start\":53350},{\"end\":53366,\"start\":53361},{\"end\":53376,\"start\":53370},{\"end\":53643,\"start\":53637},{\"end\":53651,\"start\":53647},{\"end\":53663,\"start\":53655},{\"end\":53677,\"start\":53667},{\"end\":53687,\"start\":53681},{\"end\":53964,\"start\":53957},{\"end\":54128,\"start\":54121},{\"end\":54139,\"start\":54132},{\"end\":54151,\"start\":54143},{\"end\":54162,\"start\":54155},{\"end\":54181,\"start\":54166},{\"end\":54538,\"start\":54531},{\"end\":54552,\"start\":54542},{\"end\":54566,\"start\":54556},{\"end\":54578,\"start\":54570},{\"end\":54587,\"start\":54582},{\"end\":54606,\"start\":54591},{\"end\":54994,\"start\":54987},{\"end\":55002,\"start\":54998},{\"end\":55011,\"start\":55006},{\"end\":55030,\"start\":55015},{\"end\":55299,\"start\":55294},{\"end\":55306,\"start\":55303},{\"end\":55317,\"start\":55310},{\"end\":55328,\"start\":55321},{\"end\":55557,\"start\":55549},{\"end\":55570,\"start\":55561},{\"end\":55580,\"start\":55574},{\"end\":55835,\"start\":55827},{\"end\":55845,\"start\":55842},{\"end\":55858,\"start\":55849},{\"end\":55868,\"start\":55862},{\"end\":56131,\"start\":56121},{\"end\":56142,\"start\":56135},{\"end\":56153,\"start\":56146},{\"end\":56165,\"start\":56157},{\"end\":56184,\"start\":56169},{\"end\":56196,\"start\":56188},{\"end\":56512,\"start\":56509},{\"end\":56518,\"start\":56516},{\"end\":56530,\"start\":56522},{\"end\":56537,\"start\":56534},{\"end\":56824,\"start\":56819},{\"end\":57054,\"start\":57043},{\"end\":57062,\"start\":57058},{\"end\":57068,\"start\":57066},{\"end\":57078,\"start\":57072},{\"end\":57090,\"start\":57082},{\"end\":57096,\"start\":57094},{\"end\":57105,\"start\":57100},{\"end\":57117,\"start\":57109},{\"end\":57127,\"start\":57121},{\"end\":57140,\"start\":57131},{\"end\":57461,\"start\":57452},{\"end\":57470,\"start\":57467},{\"end\":57483,\"start\":57474},{\"end\":57496,\"start\":57487},{\"end\":57507,\"start\":57500},{\"end\":57515,\"start\":57511},{\"end\":57845,\"start\":57842},{\"end\":57851,\"start\":57849},{\"end\":57862,\"start\":57858},{\"end\":58130,\"start\":58127},{\"end\":58140,\"start\":58134},{\"end\":58155,\"start\":58146},{\"end\":58412,\"start\":58408},{\"end\":58423,\"start\":58419},{\"end\":58434,\"start\":58429},{\"end\":58739,\"start\":58731},{\"end\":58753,\"start\":58743},{\"end\":58766,\"start\":58757},{\"end\":58777,\"start\":58770},{\"end\":58789,\"start\":58785},{\"end\":59127,\"start\":59119},{\"end\":59145,\"start\":59135},{\"end\":59164,\"start\":59155},{\"end\":59171,\"start\":59168},{\"end\":59182,\"start\":59175},{\"end\":59195,\"start\":59186},{\"end\":59207,\"start\":59203},{\"end\":59217,\"start\":59211},{\"end\":59568,\"start\":59556},{\"end\":59577,\"start\":59572},{\"end\":59834,\"start\":59831},{\"end\":59840,\"start\":59838},{\"end\":59847,\"start\":59844},{\"end\":59855,\"start\":59853},{\"end\":59866,\"start\":59862},{\"end\":60144,\"start\":60142},{\"end\":60151,\"start\":60148},{\"end\":60162,\"start\":60158},{\"end\":60437,\"start\":60435},{\"end\":60447,\"start\":60444},{\"end\":60461,\"start\":60451},{\"end\":60473,\"start\":60470},{\"end\":60803,\"start\":60801},{\"end\":60811,\"start\":60807},{\"end\":60818,\"start\":60815},{\"end\":60826,\"start\":60822},{\"end\":60833,\"start\":60830},{\"end\":60842,\"start\":60837},{\"end\":60851,\"start\":60846},{\"end\":60860,\"start\":60855},{\"end\":60869,\"start\":60864},{\"end\":61198,\"start\":61194},{\"end\":61206,\"start\":61202},{\"end\":61215,\"start\":61210},{\"end\":61223,\"start\":61219},{\"end\":61231,\"start\":61227},{\"end\":61433,\"start\":61429},{\"end\":61441,\"start\":61437},{\"end\":61450,\"start\":61445},{\"end\":61458,\"start\":61454},{\"end\":61475,\"start\":61464},{\"end\":61803,\"start\":61799},{\"end\":61813,\"start\":61809},{\"end\":62062,\"start\":62059},{\"end\":62069,\"start\":62066},{\"end\":62076,\"start\":62073},{\"end\":62085,\"start\":62082},{\"end\":62356,\"start\":62350},{\"end\":62365,\"start\":62360},{\"end\":62373,\"start\":62369},{\"end\":62603,\"start\":62599},{\"end\":62613,\"start\":62607},{\"end\":62620,\"start\":62617},{\"end\":62627,\"start\":62624},{\"end\":62635,\"start\":62631},{\"end\":62646,\"start\":62641},{\"end\":62980,\"start\":62977},{\"end\":62988,\"start\":62984},{\"end\":62994,\"start\":62992},{\"end\":63000,\"start\":62998},{\"end\":63007,\"start\":63004},{\"end\":63013,\"start\":63011}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6478676},\"end\":39660,\"start\":39343},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4322524},\"end\":40051,\"start\":39662},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2169827},\"end\":40389,\"start\":40053},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14309034},\"end\":40768,\"start\":40391},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":10355093},\"end\":41027,\"start\":40770},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2451356},\"end\":41385,\"start\":41029},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5381099},\"end\":41753,\"start\":41387},{\"attributes\":{\"id\":\"b7\"},\"end\":41861,\"start\":41755},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3429309},\"end\":42361,\"start\":41863},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4710115},\"end\":42749,\"start\":42363},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13798128},\"end\":43144,\"start\":42751},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6360790},\"end\":43491,\"start\":43146},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":52955886},\"end\":43774,\"start\":43493},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":559739},\"end\":44091,\"start\":43776},{\"attributes\":{\"id\":\"b14\"},\"end\":44430,\"start\":44093},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206770621},\"end\":44792,\"start\":44432},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52165085},\"end\":45120,\"start\":44794},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3520436},\"end\":45432,\"start\":45122},{\"attributes\":{\"id\":\"b18\"},\"end\":45656,\"start\":45434},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206594692},\"end\":45951,\"start\":45658},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":15703426},\"end\":46233,\"start\":45953},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5378407},\"end\":46594,\"start\":46235},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52154491},\"end\":46875,\"start\":46596},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5704781},\"end\":47145,\"start\":46877},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10412111},\"end\":47498,\"start\":47147},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1395808},\"end\":47751,\"start\":47500},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":905234},\"end\":48044,\"start\":47753},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2519672},\"end\":48493,\"start\":48046},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":59222267},\"end\":49009,\"start\":48495},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1118174},\"end\":49516,\"start\":49011},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":52255840},\"end\":49879,\"start\":49518},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4321186},\"end\":50272,\"start\":49881},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3844018},\"end\":50594,\"start\":50274},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6094550},\"end\":50893,\"start\":50596},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14113767},\"end\":51256,\"start\":50895},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1629541},\"end\":51571,\"start\":51258},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":47346364},\"end\":51970,\"start\":51573},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":7414842},\"end\":52171,\"start\":51972},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":9095022},\"end\":52613,\"start\":52173},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":14249129},\"end\":52929,\"start\":52615},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":16327959},\"end\":53302,\"start\":52931},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":10184155},\"end\":53551,\"start\":53304},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4455970},\"end\":53926,\"start\":53553},{\"attributes\":{\"id\":\"b43\"},\"end\":54062,\"start\":53928},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3155322},\"end\":54449,\"start\":54064},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1949934},\"end\":54926,\"start\":54451},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":15764549},\"end\":55254,\"start\":54928},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":13940786},\"end\":55504,\"start\":55256},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":140529},\"end\":55785,\"start\":55506},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":15278025},\"end\":56062,\"start\":55787},{\"attributes\":{\"doi\":\"arXiv:1704.00675\",\"id\":\"b50\"},\"end\":56425,\"start\":56064},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":10328909},\"end\":56776,\"start\":56427},{\"attributes\":{\"doi\":\"arXiv:1706.05098\",\"id\":\"b52\",\"matched_paper_id\":90063862},\"end\":56988,\"start\":56778},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2930547},\"end\":57405,\"start\":56990},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":6795574},\"end\":57782,\"start\":57407},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":1665250},\"end\":58085,\"start\":57784},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":1188600},\"end\":58365,\"start\":58087},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":5563298},\"end\":58653,\"start\":58367},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":10520310},\"end\":59070,\"start\":58655},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":4305586},\"end\":59470,\"start\":59072},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":6413853},\"end\":59781,\"start\":59472},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":14946436},\"end\":60101,\"start\":59783},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":1660289},\"end\":60362,\"start\":60103},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":52847087},\"end\":60737,\"start\":60364},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":52154988},\"end\":61117,\"start\":60739},{\"attributes\":{\"id\":\"b65\"},\"end\":61365,\"start\":61119},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":3655063},\"end\":61741,\"start\":61367},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":3989638},\"end\":61992,\"start\":61743},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":30132037},\"end\":62319,\"start\":61994},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":11962297},\"end\":62505,\"start\":62321},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":574347},\"end\":62911,\"start\":62507},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":52045903},\"end\":63236,\"start\":62913}]", "bib_title": "[{\"end\":39397,\"start\":39343},{\"end\":39760,\"start\":39662},{\"end\":40092,\"start\":40053},{\"end\":40447,\"start\":40391},{\"end\":40830,\"start\":40770},{\"end\":41086,\"start\":41029},{\"end\":41421,\"start\":41387},{\"end\":41974,\"start\":41863},{\"end\":42435,\"start\":42363},{\"end\":42820,\"start\":42751},{\"end\":43216,\"start\":43146},{\"end\":43560,\"start\":43493},{\"end\":43832,\"start\":43776},{\"end\":44137,\"start\":44093},{\"end\":44502,\"start\":44432},{\"end\":44863,\"start\":44794},{\"end\":45177,\"start\":45122},{\"end\":45443,\"start\":45434},{\"end\":45702,\"start\":45658},{\"end\":46011,\"start\":45953},{\"end\":46289,\"start\":46235},{\"end\":46648,\"start\":46596},{\"end\":46903,\"start\":46877},{\"end\":47186,\"start\":47147},{\"end\":47533,\"start\":47500},{\"end\":47796,\"start\":47753},{\"end\":48098,\"start\":48046},{\"end\":48554,\"start\":48495},{\"end\":49080,\"start\":49011},{\"end\":49587,\"start\":49518},{\"end\":49958,\"start\":49881},{\"end\":50365,\"start\":50274},{\"end\":50668,\"start\":50596},{\"end\":50936,\"start\":50895},{\"end\":51312,\"start\":51258},{\"end\":51643,\"start\":51573},{\"end\":52015,\"start\":51972},{\"end\":52227,\"start\":52173},{\"end\":52649,\"start\":52615},{\"end\":52999,\"start\":52931},{\"end\":53346,\"start\":53304},{\"end\":53633,\"start\":53553},{\"end\":54117,\"start\":54064},{\"end\":54527,\"start\":54451},{\"end\":54983,\"start\":54928},{\"end\":55290,\"start\":55256},{\"end\":55543,\"start\":55506},{\"end\":55821,\"start\":55787},{\"end\":56505,\"start\":56427},{\"end\":56815,\"start\":56778},{\"end\":57039,\"start\":56990},{\"end\":57446,\"start\":57407},{\"end\":57838,\"start\":57784},{\"end\":58123,\"start\":58087},{\"end\":58401,\"start\":58367},{\"end\":58727,\"start\":58655},{\"end\":59115,\"start\":59072},{\"end\":59552,\"start\":59472},{\"end\":59827,\"start\":59783},{\"end\":60138,\"start\":60103},{\"end\":60431,\"start\":60364},{\"end\":60797,\"start\":60739},{\"end\":61425,\"start\":61367},{\"end\":61795,\"start\":61743},{\"end\":62055,\"start\":61994},{\"end\":62346,\"start\":62321},{\"end\":62593,\"start\":62507},{\"end\":62973,\"start\":62913}]", "bib_author": "[{\"end\":39410,\"start\":39399},{\"end\":39421,\"start\":39410},{\"end\":39433,\"start\":39421},{\"end\":39769,\"start\":39762},{\"end\":39775,\"start\":39769},{\"end\":39782,\"start\":39775},{\"end\":40108,\"start\":40094},{\"end\":40123,\"start\":40108},{\"end\":40135,\"start\":40123},{\"end\":40147,\"start\":40135},{\"end\":40158,\"start\":40147},{\"end\":40463,\"start\":40449},{\"end\":40475,\"start\":40463},{\"end\":40490,\"start\":40475},{\"end\":40501,\"start\":40490},{\"end\":40511,\"start\":40501},{\"end\":40841,\"start\":40832},{\"end\":40849,\"start\":40841},{\"end\":41099,\"start\":41088},{\"end\":41114,\"start\":41099},{\"end\":41126,\"start\":41114},{\"end\":41135,\"start\":41126},{\"end\":41434,\"start\":41423},{\"end\":41448,\"start\":41434},{\"end\":41462,\"start\":41448},{\"end\":41476,\"start\":41462},{\"end\":41487,\"start\":41476},{\"end\":41499,\"start\":41487},{\"end\":41804,\"start\":41793},{\"end\":41987,\"start\":41976},{\"end\":42001,\"start\":41987},{\"end\":42013,\"start\":42001},{\"end\":42023,\"start\":42013},{\"end\":42035,\"start\":42023},{\"end\":42445,\"start\":42437},{\"end\":42459,\"start\":42445},{\"end\":42469,\"start\":42459},{\"end\":42481,\"start\":42469},{\"end\":42831,\"start\":42822},{\"end\":42842,\"start\":42831},{\"end\":42853,\"start\":42842},{\"end\":42861,\"start\":42853},{\"end\":42872,\"start\":42861},{\"end\":43227,\"start\":43218},{\"end\":43238,\"start\":43227},{\"end\":43246,\"start\":43238},{\"end\":43257,\"start\":43246},{\"end\":43568,\"start\":43562},{\"end\":43576,\"start\":43568},{\"end\":43584,\"start\":43576},{\"end\":43847,\"start\":43834},{\"end\":43857,\"start\":43847},{\"end\":43865,\"start\":43857},{\"end\":44152,\"start\":44139},{\"end\":44160,\"start\":44152},{\"end\":44170,\"start\":44160},{\"end\":44182,\"start\":44170},{\"end\":44517,\"start\":44504},{\"end\":44526,\"start\":44517},{\"end\":44536,\"start\":44526},{\"end\":44548,\"start\":44536},{\"end\":44871,\"start\":44865},{\"end\":44878,\"start\":44871},{\"end\":44886,\"start\":44878},{\"end\":44894,\"start\":44886},{\"end\":45185,\"start\":45179},{\"end\":45192,\"start\":45185},{\"end\":45200,\"start\":45192},{\"end\":45208,\"start\":45200},{\"end\":45451,\"start\":45445},{\"end\":45463,\"start\":45451},{\"end\":45473,\"start\":45463},{\"end\":45485,\"start\":45473},{\"end\":45710,\"start\":45704},{\"end\":45719,\"start\":45710},{\"end\":45726,\"start\":45719},{\"end\":45733,\"start\":45726},{\"end\":46021,\"start\":46013},{\"end\":46030,\"start\":46021},{\"end\":46042,\"start\":46030},{\"end\":46306,\"start\":46291},{\"end\":46317,\"start\":46306},{\"end\":46328,\"start\":46317},{\"end\":46339,\"start\":46328},{\"end\":46659,\"start\":46650},{\"end\":46671,\"start\":46659},{\"end\":46684,\"start\":46671},{\"end\":46916,\"start\":46905},{\"end\":46925,\"start\":46916},{\"end\":46937,\"start\":46925},{\"end\":47199,\"start\":47188},{\"end\":47211,\"start\":47199},{\"end\":47218,\"start\":47211},{\"end\":47226,\"start\":47218},{\"end\":47237,\"start\":47226},{\"end\":47548,\"start\":47535},{\"end\":47555,\"start\":47548},{\"end\":47564,\"start\":47555},{\"end\":47811,\"start\":47798},{\"end\":47818,\"start\":47811},{\"end\":47827,\"start\":47818},{\"end\":48111,\"start\":48100},{\"end\":48124,\"start\":48111},{\"end\":48133,\"start\":48124},{\"end\":48145,\"start\":48133},{\"end\":48160,\"start\":48145},{\"end\":48171,\"start\":48160},{\"end\":48180,\"start\":48171},{\"end\":48189,\"start\":48180},{\"end\":48200,\"start\":48189},{\"end\":48213,\"start\":48200},{\"end\":48567,\"start\":48556},{\"end\":48580,\"start\":48567},{\"end\":48589,\"start\":48580},{\"end\":48601,\"start\":48589},{\"end\":48615,\"start\":48601},{\"end\":48625,\"start\":48615},{\"end\":48634,\"start\":48625},{\"end\":48642,\"start\":48634},{\"end\":48653,\"start\":48642},{\"end\":48666,\"start\":48653},{\"end\":48679,\"start\":48666},{\"end\":49093,\"start\":49082},{\"end\":49102,\"start\":49093},{\"end\":49115,\"start\":49102},{\"end\":49124,\"start\":49115},{\"end\":49139,\"start\":49124},{\"end\":49152,\"start\":49139},{\"end\":49163,\"start\":49152},{\"end\":49174,\"start\":49163},{\"end\":49185,\"start\":49174},{\"end\":49595,\"start\":49589},{\"end\":49602,\"start\":49595},{\"end\":49608,\"start\":49602},{\"end\":49615,\"start\":49608},{\"end\":49621,\"start\":49615},{\"end\":49966,\"start\":49960},{\"end\":49974,\"start\":49966},{\"end\":49981,\"start\":49974},{\"end\":49990,\"start\":49981},{\"end\":50001,\"start\":49990},{\"end\":50373,\"start\":50367},{\"end\":50382,\"start\":50373},{\"end\":50679,\"start\":50670},{\"end\":50689,\"start\":50679},{\"end\":50697,\"start\":50689},{\"end\":50948,\"start\":50938},{\"end\":50957,\"start\":50948},{\"end\":50969,\"start\":50957},{\"end\":50977,\"start\":50969},{\"end\":50987,\"start\":50977},{\"end\":50998,\"start\":50987},{\"end\":51008,\"start\":50998},{\"end\":51021,\"start\":51008},{\"end\":51322,\"start\":51314},{\"end\":51335,\"start\":51322},{\"end\":51346,\"start\":51335},{\"end\":51656,\"start\":51645},{\"end\":51665,\"start\":51656},{\"end\":51675,\"start\":51665},{\"end\":51684,\"start\":51675},{\"end\":51695,\"start\":51684},{\"end\":52029,\"start\":52017},{\"end\":52042,\"start\":52029},{\"end\":52053,\"start\":52042},{\"end\":52243,\"start\":52229},{\"end\":52254,\"start\":52243},{\"end\":52262,\"start\":52254},{\"end\":52276,\"start\":52262},{\"end\":52290,\"start\":52276},{\"end\":52301,\"start\":52290},{\"end\":52313,\"start\":52301},{\"end\":52660,\"start\":52651},{\"end\":52671,\"start\":52660},{\"end\":52679,\"start\":52671},{\"end\":52698,\"start\":52679},{\"end\":53011,\"start\":53001},{\"end\":53027,\"start\":53011},{\"end\":53037,\"start\":53027},{\"end\":53046,\"start\":53037},{\"end\":53359,\"start\":53348},{\"end\":53368,\"start\":53359},{\"end\":53378,\"start\":53368},{\"end\":53645,\"start\":53635},{\"end\":53653,\"start\":53645},{\"end\":53665,\"start\":53653},{\"end\":53679,\"start\":53665},{\"end\":53689,\"start\":53679},{\"end\":53966,\"start\":53955},{\"end\":54130,\"start\":54119},{\"end\":54141,\"start\":54130},{\"end\":54153,\"start\":54141},{\"end\":54164,\"start\":54153},{\"end\":54183,\"start\":54164},{\"end\":54540,\"start\":54529},{\"end\":54554,\"start\":54540},{\"end\":54568,\"start\":54554},{\"end\":54580,\"start\":54568},{\"end\":54589,\"start\":54580},{\"end\":54608,\"start\":54589},{\"end\":54996,\"start\":54985},{\"end\":55004,\"start\":54996},{\"end\":55013,\"start\":55004},{\"end\":55032,\"start\":55013},{\"end\":55301,\"start\":55292},{\"end\":55308,\"start\":55301},{\"end\":55319,\"start\":55308},{\"end\":55330,\"start\":55319},{\"end\":55559,\"start\":55545},{\"end\":55572,\"start\":55559},{\"end\":55582,\"start\":55572},{\"end\":55837,\"start\":55823},{\"end\":55847,\"start\":55837},{\"end\":55860,\"start\":55847},{\"end\":55870,\"start\":55860},{\"end\":56133,\"start\":56119},{\"end\":56144,\"start\":56133},{\"end\":56155,\"start\":56144},{\"end\":56167,\"start\":56155},{\"end\":56186,\"start\":56167},{\"end\":56198,\"start\":56186},{\"end\":56514,\"start\":56507},{\"end\":56520,\"start\":56514},{\"end\":56532,\"start\":56520},{\"end\":56539,\"start\":56532},{\"end\":56826,\"start\":56817},{\"end\":57056,\"start\":57041},{\"end\":57064,\"start\":57056},{\"end\":57070,\"start\":57064},{\"end\":57080,\"start\":57070},{\"end\":57092,\"start\":57080},{\"end\":57098,\"start\":57092},{\"end\":57107,\"start\":57098},{\"end\":57119,\"start\":57107},{\"end\":57129,\"start\":57119},{\"end\":57142,\"start\":57129},{\"end\":57463,\"start\":57448},{\"end\":57472,\"start\":57463},{\"end\":57485,\"start\":57472},{\"end\":57498,\"start\":57485},{\"end\":57509,\"start\":57498},{\"end\":57517,\"start\":57509},{\"end\":57847,\"start\":57840},{\"end\":57853,\"start\":57847},{\"end\":57864,\"start\":57853},{\"end\":58132,\"start\":58125},{\"end\":58142,\"start\":58132},{\"end\":58157,\"start\":58142},{\"end\":58414,\"start\":58403},{\"end\":58425,\"start\":58414},{\"end\":58436,\"start\":58425},{\"end\":58741,\"start\":58729},{\"end\":58755,\"start\":58741},{\"end\":58768,\"start\":58755},{\"end\":58779,\"start\":58768},{\"end\":58791,\"start\":58779},{\"end\":59129,\"start\":59117},{\"end\":59133,\"start\":59129},{\"end\":59147,\"start\":59133},{\"end\":59151,\"start\":59147},{\"end\":59166,\"start\":59151},{\"end\":59173,\"start\":59166},{\"end\":59184,\"start\":59173},{\"end\":59197,\"start\":59184},{\"end\":59209,\"start\":59197},{\"end\":59219,\"start\":59209},{\"end\":59223,\"start\":59219},{\"end\":59570,\"start\":59554},{\"end\":59579,\"start\":59570},{\"end\":59836,\"start\":59829},{\"end\":59842,\"start\":59836},{\"end\":59849,\"start\":59842},{\"end\":59857,\"start\":59849},{\"end\":59868,\"start\":59857},{\"end\":60146,\"start\":60140},{\"end\":60153,\"start\":60146},{\"end\":60164,\"start\":60153},{\"end\":60439,\"start\":60433},{\"end\":60449,\"start\":60439},{\"end\":60463,\"start\":60449},{\"end\":60475,\"start\":60463},{\"end\":60805,\"start\":60799},{\"end\":60813,\"start\":60805},{\"end\":60820,\"start\":60813},{\"end\":60828,\"start\":60820},{\"end\":60835,\"start\":60828},{\"end\":60844,\"start\":60835},{\"end\":60853,\"start\":60844},{\"end\":60862,\"start\":60853},{\"end\":60871,\"start\":60862},{\"end\":61200,\"start\":61192},{\"end\":61208,\"start\":61200},{\"end\":61217,\"start\":61208},{\"end\":61225,\"start\":61217},{\"end\":61233,\"start\":61225},{\"end\":61435,\"start\":61427},{\"end\":61443,\"start\":61435},{\"end\":61452,\"start\":61443},{\"end\":61460,\"start\":61452},{\"end\":61477,\"start\":61460},{\"end\":61805,\"start\":61797},{\"end\":61815,\"start\":61805},{\"end\":62064,\"start\":62057},{\"end\":62071,\"start\":62064},{\"end\":62078,\"start\":62071},{\"end\":62087,\"start\":62078},{\"end\":62358,\"start\":62348},{\"end\":62367,\"start\":62358},{\"end\":62375,\"start\":62367},{\"end\":62605,\"start\":62595},{\"end\":62615,\"start\":62605},{\"end\":62622,\"start\":62615},{\"end\":62629,\"start\":62622},{\"end\":62637,\"start\":62629},{\"end\":62648,\"start\":62637},{\"end\":62982,\"start\":62975},{\"end\":62990,\"start\":62982},{\"end\":62996,\"start\":62990},{\"end\":63002,\"start\":62996},{\"end\":63009,\"start\":63002},{\"end\":63015,\"start\":63009}]", "bib_venue": "[{\"end\":39491,\"start\":39433},{\"end\":39840,\"start\":39782},{\"end\":40207,\"start\":40158},{\"end\":40559,\"start\":40511},{\"end\":40887,\"start\":40849},{\"end\":41193,\"start\":41135},{\"end\":41557,\"start\":41499},{\"end\":41791,\"start\":41755},{\"end\":42097,\"start\":42035},{\"end\":42539,\"start\":42481},{\"end\":42930,\"start\":42872},{\"end\":43305,\"start\":43257},{\"end\":43622,\"start\":43584},{\"end\":43923,\"start\":43865},{\"end\":44240,\"start\":44182},{\"end\":44596,\"start\":44548},{\"end\":44942,\"start\":44894},{\"end\":45266,\"start\":45208},{\"end\":45533,\"start\":45485},{\"end\":45791,\"start\":45733},{\"end\":46080,\"start\":46042},{\"end\":46401,\"start\":46339},{\"end\":46722,\"start\":46684},{\"end\":46995,\"start\":46937},{\"end\":47305,\"start\":47237},{\"end\":47612,\"start\":47564},{\"end\":47885,\"start\":47827},{\"end\":48251,\"start\":48213},{\"end\":48727,\"start\":48679},{\"end\":49247,\"start\":49185},{\"end\":49679,\"start\":49621},{\"end\":50059,\"start\":50001},{\"end\":50420,\"start\":50382},{\"end\":50734,\"start\":50697},{\"end\":51059,\"start\":51021},{\"end\":51404,\"start\":51346},{\"end\":51753,\"start\":51695},{\"end\":52061,\"start\":52053},{\"end\":52375,\"start\":52313},{\"end\":52756,\"start\":52698},{\"end\":53104,\"start\":53046},{\"end\":53416,\"start\":53378},{\"end\":53727,\"start\":53689},{\"end\":53953,\"start\":53928},{\"end\":54241,\"start\":54183},{\"end\":54666,\"start\":54608},{\"end\":55080,\"start\":55032},{\"end\":55368,\"start\":55330},{\"end\":55631,\"start\":55582},{\"end\":55908,\"start\":55870},{\"end\":56117,\"start\":56064},{\"end\":56588,\"start\":56539},{\"end\":56862,\"start\":56842},{\"end\":57182,\"start\":57142},{\"end\":57579,\"start\":57517},{\"end\":57922,\"start\":57864},{\"end\":58215,\"start\":58157},{\"end\":58494,\"start\":58436},{\"end\":58849,\"start\":58791},{\"end\":59261,\"start\":59223},{\"end\":59612,\"start\":59579},{\"end\":59926,\"start\":59868},{\"end\":60222,\"start\":60164},{\"end\":60533,\"start\":60475},{\"end\":60909,\"start\":60871},{\"end\":61190,\"start\":61119},{\"end\":61535,\"start\":61477},{\"end\":61853,\"start\":61815},{\"end\":62145,\"start\":62087},{\"end\":62403,\"start\":62375},{\"end\":62696,\"start\":62648},{\"end\":63053,\"start\":63015}]"}}}, "year": 2023, "month": 12, "day": 17}