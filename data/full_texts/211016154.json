{"id": 211016154, "updated": "2022-01-31 18:17:22.74", "metadata": {"title": "QuantHD: A Quantization Framework for Hyperdimensional Computing", "authors": "[{\"middle\":[],\"last\":\"Imani\",\"first\":\"Mohsen\"},{\"middle\":[],\"last\":\"Bosch\",\"first\":\"Samuel\"},{\"middle\":[],\"last\":\"Datta\",\"first\":\"Sohum\"},{\"middle\":[],\"last\":\"Ramakrishna\",\"first\":\"Sharadhi\"},{\"middle\":[],\"last\":\"Salamat\",\"first\":\"Sahand\"},{\"middle\":[\"M.\"],\"last\":\"Rabaey\",\"first\":\"Jan\"},{\"middle\":[],\"last\":\"Rosing\",\"first\":\"Tajana\"}]", "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "journal": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Brain-inspired hyperdimensional (HD) computing models cognition by exploiting properties of high dimensional statistics\u2014high-dimensional vectors, instead of working with numeric values used in contemporary processors. A fundamental weakness of existing HD computing algorithms is that they require to use floating point models in order to provide acceptable accuracy on realistic classification problems. However, working with floating point values significantly increases the HD computation cost. To address this issue, we proposed QuantHD, a novel framework for quantization of HD computing model during training. QuantHD enables HD computing to work with a low-cost quantized model (binary or ternary model) while providing a similar accuracy as the floating point model. We accordingly propose an FPGA implementation which accelerates HD computing in both training and inference phases. We evaluate QuantHD accuracy and efficiency on various real-world applications, and observe that QuantHD can achieve on average 17.2% accuracy improvement as compared to the existing binarized HD computing algorithms which provide a similar computation cost. In terms of efficiency, QuantHD FPGA implementation can achieve on average <inline-formula> <tex-math notation=\"LaTeX\">$42.3\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$4.7\\times $ </tex-math></inline-formula> (<inline-formula> <tex-math notation=\"LaTeX\">$34.1\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$4.1\\times $ </tex-math></inline-formula>) energy efficiency improvement and speedup during inference (training) as compared to the state-of-the-art HD computing algorithms.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2990838824", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tcad/ImaniBDRSRR20", "doi": "10.1109/tcad.2019.2954472"}}, "content": {"source": {"pdf_hash": "1f8417edfee81403eb2e1a55376902872a369981", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e0a7927b903c87b5a77e9d2e295a7668d5525a61", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1f8417edfee81403eb2e1a55376902872a369981.txt", "contents": "\nQuantHD: A Quantization Framework for Hyperdimensional Computing\nOCTOBER 2020\n\nMohsen Imani \nSamuel Bosch \nSohum Datta \nSharadhi Ramakrishna \nSahand Salamat \nFellow, IEEEJan M Rabaey \nFellow, IEEETajana Rosing \nQuantHD: A Quantization Framework for Hyperdimensional Computing\n\nIEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS\n3910OCTOBER 202010.1109/TCAD.2019.29544722268\nBrain-inspired hyperdimensional (HD) computing models cognition by exploiting properties of high dimensional statistics-high-dimensional vectors, instead of working with numeric values used in contemporary processors. A fundamental weakness of existing HD computing algorithms is that they require to use floating point models in order to provide acceptable accuracy on realistic classification problems. However, working with floating point values significantly increases the HD computation cost. To address this issue, we proposed QuantHD, a novel framework for quantization of HD computing model during training. QuantHD enables HD computing to work with a low-cost quantized model (binary or ternary model) while providing a similar accuracy as the floating point model. We accordingly propose an FPGA implementation which accelerates HD computing in both training and inference phases. We evaluate QuantHD accuracy and efficiency on various real-world applications, and observe that QuantHD can achieve on average 17.2% accuracy improvement as compared to the existing binarized HD computing algorithms which provide a similar computation cost. In terms of efficiency, QuantHD FPGA implementation can achieve on average 42.3\u00d7 and 4.7\u00d7 (34.1\u00d7 and 4.1\u00d7) energy efficiency improvement and speedup during inference (training) as compared to the state-of-the-art HD computing algorithms.Index Terms-Brain-inspired computing, energy-efficiency, FPGA acceleration, hyperdimensional (HD) computing.\n\napplications analyze data by running machine learning algorithms in data centers. However, it is well-known that existing learning algorithms may be overcomplex for many real-world applications [2]. Simpler algorithms can also deliver the same task with lower computational complexity and hardware/energy requirements. For instance, deep neural networks (DNNs) are used for complicated classification problems, such as image classification tasks, e.g., ImageNet dataset [3]. However, the computational complexity and memory requirements of DNNs makes them inefficient for a broad variety of real-life embedded applications. Therefore, it is crucial to design a light-weight learning method since in IoT systems sending all the data to the cloud for processing is not scalable, cannot guarantee real-time response [4].\n\nBrain-inspired hyperdimensional (HD) computing [5], [6] has been proposed as a light-weight learning methodology. HD computing is developed based on the fact that brains compute with patterns of neural activity which are not readily associated with numbers [5]. HD computing builds upon a simple set of operations with random HD vectors, is robust in the presence of hardware failures. It also offers an alternative computational paradigm that can be applied to learning problems [5], [6]. The first step in HD computing is to encode all data points to a high-dimensional space. During training, HD computing linearly combines the encoded hypervectors in order to create a hypervector representing each class. During inference, the classification task is performed by checking the similarity of an encoded query hypervector with all class hypervectors and returning the class with the highest similarity score. In this article, we argue that in most practical applications, HD computing algorithms require to be trained and tested using floating point values. HD computing with binary model provides significant low classification accuracy which often is not acceptable by users. On the other hand, working with floating point values increases the HD computation cost and hinders use of HD as a light-weight classifier.\n\nIn this article, we observe that the low classification accuracy of HD computing using binarized/quantized model is due to the weakness of the existing training procedure [7]. In this article, we proposed QuantHD, a novel quantization framework which enables HD computing to be trained and tested on a low-cost binary/ternary model with high classification accuracy. The main contributions of this article are listed as follows.\n\n1) To the best of our knowledge, this is the first HD computing framework which enables model quantization with minimal impact on the classification accuracy. We develop an iterative training approach which adapts the HD model to work with the quantized values. 2) QuantHD significantly accelerates HD computing during training and inference by removing the majority of nonbinary computations from the similarity check. Unlike the existing HD computing algorithms that require the use of floating point model, QuantHD performs similarity check with the quantized (binary/ternary) model. The quantized model simplifies the costly cosine similarity to more hardware-friendly metrics such as Hamming distance. 3) We accordingly proposed a pipelined FPGA implementation which accelerates both training and inference by reducing the cost of the associative search. We evaluate QuantHD on several practical classification problems, including face and activity recognition. Our evaluations show that QuantHD can improve the HD classification accuracy by 17.2% as compared to the existing HD computing algorithms [8], [9]. In terms of efficiency, QuantHD FPGA implementation can achieve on average 34.1\u00d7 and 4.1\u00d7 (42.3\u00d7 and 4.7\u00d7) energy efficiency improvement and speedup during training (inference) as compared to the state-of-the-art HD computing algorithms [8], [9]. Comparing QuantHD with multilayer perceptron (MLP) and binarized neural network (BNN) classifiers, we observe that QuantHD can provide 8.2\u00d7 and 13.4\u00d7 faster computing in training and testing, respectively, while providing similar classification accuracy.\n\n\nII. HYPERDIMENSIONAL COMPUTING\n\nHD computing can be applied to different learning problems. Here, we focus on classification, one of the most popular supervised learning algorithms. Fig. 1(a) shows an overview of HD computing for classification. The first step of HD classification is to use the encoding module to map data points to a high-dimensional space. The training module combines the encoded hypervectors in order to create a model representing each class. The information in each class stored as a pattern of values distributed in D = 10 000 dimensions. The class hypervectors represents a trained model and are placed in the associative memory. During inference, the same encoding module maps the input data to high-dimensional space. The reasoning task finds a class hypervector which has the most similarity to a query hypervector. In the following, we explain the details of encoding, training, and similarity check used for inference.\n\n\nA. Encoding\n\nThe first step of HD is to encode each data point into a hypervector. Original data point is assumed to have n features, i.e., f = f 1 , . . . , f n . Our goal is to encode each feature into a hypervector that has D dimensions, e.g., D = 10 000. Each feature vector in original domain stores the feature values and their corresponding positions.\n\nEncoding Alphabets: To differentiate the position of each feature, we exploit a set of randomly generated base hypervectors, i.e., {B i , B 2 , . . . , B n }, where n is the feature size of an original data point (B i \u2208 {0, 1} D ). Due to random generation and the size of hypervector, the base hypervectors are nearly orthogonal [10], meaning that the vectors only have about 50% common elements\n\u03b4(B i , B j ) D/2 (0 < i, j \u2264 n, i = j)\nwhere \u03b4 is the Hamming distance similarity between the two hypervectors. The feature hypervectors are more likely to be orthogonal when dimensionality, D, is large enough as compared to the size of the feature vector in the original domain (D >> n). We also differentiate the feature values using a set of hypervectors. We first find the minimum and maximum feature values across all training data points, say {f min , f max }, and then discretize the values to m different levels. Note that this discretization can happen linearly or nonlinearly depending on the feature distributions. We select the discretization by looking at the distribution of the feature values. We generate a single hypervector represent each level, {L 1 , L 2 , . . . , L n }. For the first level, L 1 , we generate a single hypervector representing f min . Then each time, we select D/m random bits and flip them to generate the next level hypervector. This process ensures that the last level hypervector, L m , is nearly orthogonal to L 1 , while other level hypervectors are correlated. For example, \nH = B 1 \u2295 L 1 + B 2 \u2295 L 2 + \u00b7 \u00b7 \u00b7 + B n \u2295 L n(1)\nwhere L i \u2208 {L 1 , . . . , L m }, and \u2295 denotes an XOR operation.\n\n\nB. HD Training\n\nThe HD training happens with the encoded hypervectors. HD computing is its fast learning capability. Most existing HD computing algorithms perform the training in a single iteration. The training adds all the encoded data points that belong to the same class. For example, of face recognition problem, HD computing creates two hypervectors representing \"Face\" and \"No-face\" classes. These hypervectors can be created by separately adding all encoded hypervectors which have the Face and No-face labels.\n\n\nC. Inference: Nonbinary or Binary Model?\n\nIn HD computing, the trained model has nonbinarized elements with positive or negative floating point values. The existing HD computing methods [8], [15]- [18] binarize the class hypervectors to eliminate costly Cosine operation used for the associative search. In addition, most of the existing hardware accelerators for HD computing only accelerate the binary model [16], [19]. We observe that HD computing using binary hypervectors cannot provide acceptable classification accuracy on the majority of practical problems. Table I reports the HD classification accuracy for four classification applications, including speech recognition [11], activity recognition [12], physical monitoring [13], and face detection [14]. The results are reported for the-based HD computing [9] using binary and nonbinary class elements. Note that the baseline HD is using Ngram-based encoding which is different from one explained in Section II-A. In this table, N and n are the numbers of classes and features, respectively. To get an acceptable classification accuracy, HD computing requires to use class hypervectors with nonbinary elements. For example, for face recognition, HD computing with binary model gets 68.4% accuracy, which is much lower than 95.9% accuracy that HD using a nonbinary model can achieve.\n\nThe results in Table I also compare the execution time of HD with binary and nonbinarized models running on embedded devices (Raspberry Pi 3) using ARM Cortex A53 CPU. Our results show that HD with floating-point model provides on average 17.5% higher accuracy, but is 6.5\u00d7 slower as compared to HD computing with the binary model. The lower efficiency of the nonbinary model comes from the costly Cosine similarity metric which involves a large number of additions and multiplications.\n\n\nIII. PROPOSED QUANTHD\n\n\nA. Overview\n\nIn this section, we present QuantHD, a novel framework for quantization of HD computing model during training. QuantHD enables quantizing (binarizing/ternarizing) of the HD model with no or minor impact on the classification accuracy. QuantHD consists of three main steps: 1) Initial Training: it creates an initial HD model by accumulating all encoded hypervectors corresponding to each class. The initial training step is the same as conventional HD computing algorithms; 2) Quantization, which projects the HD model to a binary or ternary model. Since the HD model has been trained to work with the floating-point values, the quantization results in a significant quality loss; and 3) Retraining compensates the quality loss due to the model quantization. QuantHD iteratively retrains the HD model such that it adopts to work with the quantized model.\n\n\nB. QuantHD Framework\n\nInitial Training: QuantHD trains the class hypervectors by accumulating all encoded hypervectors which belong to the same class. As Fig. 2(a) shows, each accumulated hypervector represents a class. For example, for an application with k classes, the initial HD model contains k nonquantized\nhypervectors {C 1 , . . . , C k }, where C i \u2208 N D ( \u2022 1 ).\nModel Projection: We develop a model projection method which maps this model to a quantized hypervectors, {C q 1 , . . . , C q k }, with binary or ternary representation ( \u2022 2 ).\n\nThe binary and ternary models represent the class hypervectors using {0, 1} and {\u22121, 0, + 1} elements, respectively. The details of model quantization are explained in Section III-C.\n\nIterative Learning: Although the initial trained model provides high classification accuracy, the quantization of the model significantly degrades the accuracy. This accuracy degradation comes from mapping the binary domain which  does not preserve distances between the vectors. To compensate for the possible quality loss, QuantHD supports a retraining procedure which iteratively modifies the HD model in order to adapt it to work with the quantization constraints. QuantHD keeps both quantized and nonquantized models. For each data point in the training dataset, say H, we first quantize the encoded hypervector, H q , and then check its similarity with the quantized model. The similarity metric is the Hamming distance for the binary model and dot product for the ternary\nmodel ( \u2022 3 )\n. If the quantized model correctly classifies H q , we do not update the model. However, if H q is incorrectly classified, we only update the nonquantized model, while the quantized model stays the same. This update happens on two class hypervectors: 1) a class that data is misclassified to (C miss ) and 2) a class that data point belongs to (C match ). Since in HD the information stored as a pattern of distribution in high-dimensional space, the update of the nonquantized model can perform by ( \u2022 4 )\nC miss = C miss \u2212 \u03b1H and C match = C match + \u03b1H\nwhere \u03b1 is a learning rate (0 < \u03b1 < 1 Model Validation: We examine the classification accuracy of the projected model on the validation data, which is 5% of the training data ( \u2022 6 ). If the projected model accuracy is changed less than \u03b5, we send the new model to inference to perform the reset of computation; otherwise, we start retraining the quantized model by checking the similarity of all training data points and accordingly updating the nonquantized model ( \u2022 7 ).\n\nNote that the QuantHD stops after a predefined number of iterations if the convergence condition does not satisfy. For all experiments in this article, we use \u03b5 = 0.01 and limit the maximum number of iterations to 30. Fig. 3(a) and (b) shows the classification accuracy of QuantHD with a binary model during different retraining iterations. The results are reported for speech recognition (ISOLET) and face recognition (FACE) applications using three different learning rates. Note that, here we use the encoding module introduced in Section II-A which provides higher accuracy than the baseline HD computing reported in Table I. Our evaluation shows that QuantHD using small learning rate slow down the learning process. For example, QuantHD with \u03b1 = 0.01 cannot get the maximum accuracy in 30 retraining iterations. Increasing the learning rate to \u03b1 = 0.05 improves the learning speed and the final classification accuracy. However, using a learning rate of alpha = 0.3 or larger increases the fluctuation on the accuracy during the retraining phase and can cause possible divergence. Fig. 3(c) and (d) shows the impact of learning rate on the classification accuracy of QuantHD using binary and ternary models. Our result shows that QuantHD provides maximum classification accuracy using a learning rate of around \u03b1 = 0.05. Note that retraining the original QuantHD model with nonquantized values requires \u03b1 > 1 (\u2248 1.5 \u2212 2) for fast and stable training.\n(a) (b) (c) (d)\n\nC. Details of Model Quantization\n\nAfter training the HD model, the class hypervectors is represented using nonquantized (integer or floating point) elements which can take negative or positive values. The goal of model quantization is to map these values to a new domain where the computation can be performed with much higher efficiency. Here, we explain how we quantize the hypervectors to binary and ternary domains.\n\nBinary Model: QuantHD binarizes the model using the sign function by assigning all positive and negative elements to 1 and 0 bits, respectively. In fact, binarization maps each class hypervector from N D to {0, 1} D . Unlike the fixedpoint/floating-point model which uses costly cosine metric, binary model exploits Hamming distance for similarity check.\n\nTernary Model: The HD model can also be mapped into a ternary domain where each class element can take {\u22121, 0, + 1} values. Ternarization gives more flexibility to a model to select more suitable weights during quantization [shown in Fig. 2(b)]. In the ternary model, the similarity check performs using the dot product between the hypervectors.\n\nTernarization determines the sparsity of the model by selecting a boundary where the elements can get \u22121, 0, and +1 values. One naive way is to normalize the class hypervectors and linearly split a range between minimum and maximum feature into three equal regions. Each class elements can be assigned to one of the regions depending on the dimension values. For example, we can assign numbers in rage of [\u2212b, +b] to zero, while [\u22121, \u2212b] and [+b, 1] are assigned to \u22121 and +1, respectively [see Fig. 2(b)]. This method does not properly work since the class values are not uniformly distributed. Instead, we observe that in all of our tested datasets the class hypervectors have a Gaussian distribution. We can select the ternarization boundary to balance the portion of dimensions that are assigned to 0 value.\n\nThe boundary values can set based on the data distribution and its variance (\u03c3 ). Fig. 4 also shows the impact of the ternarization boundary on the classification accuracy of two applications: 1) activity recognition (UCIHAR) and 2) speech recognition (ISOLET). The x-axis in a graph shows the ternary boundary which changes from 0 to 0.8\u03c3 . Increasing the b boundary at first improves the classification accuracy by giving more flexibility to the weights to be zero. This also results in higher sparsity of the ternarized model. However, increasing the b boundary more than a specific value results in lower classification accuracy, since high sparsity in the trained model may result in information loss.\n\n\nIV. FPGA IMPLEMENTATION\n\nHD computing can be implemented in different hardware platforms, such as CPU, GPU, or FPGA. Due to a large amount of bit-wise operations exist in training and inference of HD computation, FPGA is a suitable candidate for efficient HD computing acceleration. QuantHD has three main phases: 1) training; 2) retraining; and 3) inference. These phases are sharing similar blocks. For example, the encoding is a commonly used block in all phases. Similarly, the retraining and inference are using associative search block on their computation.\n\n\nA. Encoding Acceleration\n\nThe encoding happens based on two sets of pregenerated hypervectors: 1) level hypervector ({L 1 , . . . , L m }) and 2) base hypervectors ({B 1 , . . . , B n }) [7]- [9]. Both base and level hypervectors are binary, B i , L i \u2208 {0, 1} D . This binary representation enables FPGA to store each dimension of level and base hypervector using a single bit. Depending on the number of features, n, and hypervectors dimensionality, D, FPGA may not have enough resources to generate all D dimensions of encoded hypervector once at a time. In that case, our implementation performs encoding only on d dimensions of the level and base hypervectors (d < D). We further discuss about the encoding throughput at Section IV-B.\n\n\nB. Training Acceleration\n\nTraining involves in the accumulation of all encoded hypervectors corresponding to a class, where each hypervector represents using D nonquantized values. FPGA performs the addition of the encoded hypervectors using DSP blocks. Since the training does not share many resources with the encoding module, they can be run in parallel to accelerate the training. Fig. 5 shows an overview of the FPGA implementation of the training module. When an encoding module generates the d dimensions of the encoded hypervector, DSPs accumulate the previously d encoded dimensions with the corresponding class hypervector. The encoding and training modules are working in a pipeline structure, which results in hiding the latency of the encoding module. Depending on a class that the data point belongs to (a tag bit shown in Fig. 5), FPGA reads the class hypervectors ( \u2022 E ). Then, this class hypervector is accumulated with the encoded hypervector in a tree-based adder ( \u2022 F ).\n\nTo accelerate the training, we sort the data such that all data points corresponding to a class process sequentially. Note that this sorting can have negative impact on the classification accuracy. As it has been shown by several work [20], [21], training with randomly shuffled data reduces the chance of over-fitting; thus results in providing a higher classification accuracy. For example, in ISOLET, we observe that QuantHD provides 0.7% higher accuracy if it trains on shuffled data. Here, we sort training data point in order to improve the FPGA efficiency. This sorting enables the accumulation operation happens on a class hypervector without accessing another class hypervector from the internal FPGA block RAM (BRAM). Finally, the accumulated class hypervector can be written back into the BRAM block ( \u2022 G ).\n\nThe value d can be limited either by the encoding or training module. In the encoding module, the maximum d which can be generated at each iteration depends on the features size, and it is limited by the number of available LUT/FF resources. On the other hand, the maximum d value that training module can process depending on the number of available DSPs on the FPGA. For applications, such as physical monitoring with n = 75 features and k = 6 classes, the number of DSPs limits the d value to 64. However, for face recognition with n = 608 features and k = 2 classes, the d value is limited to 192 by LUT utilization. This training process continues over the entire training data points until generating a hypervector for each existing class.\n\n\nC. Retraining and Inference Acceleration\n\nQuantHD uses similar hardware to accelerate retraining and inference modules. The associative search is the main functionality of QuantHD during inference and retraining. Retraining also uses another module to update the class hypervectors and quantize the model, when a train data point misclassifies with a model. Here, we explain the FPGA implementation of each module in details.\n\nModel Quantization: After training the nonquantized model, FPGA quantizes the model by comparing each class dimension with a threshold value. This process is performed by reading class hypervectors from distributed RAMs and comparing each dimension with a threshold value(s) [shown in Fig. 6(c)]. For binarization, all dimensions with a smaller value than a \u03bc threshold are assigned to \"0\" bit, while other dimensions get \"1\" bit. Similarly, ternarization happens similarly by comparing each class element with two threshold values. As we discussed in Section III-C, we assign values smaller than a TH 1 = \u03bc\u22120.42\u03c3 to \"01,\" values larger than TH 2 = \u03bc+0.42\u03c3 to \"11,\" and other values to \"00.\" FPGA stores both quantized and fixed-point models to perform the retraining. Associative Search: During retraining and inference, HD requires to perform the associative search over the training and testing data, respectively. The existing HD computing algorithms [8] perform the retrain on the nonquantized model. Thus, they require to use costly cosine similarity during retraining. In contrast, QuantHD associative search happens on the quantized model. The search over quantized model significantly accelerates the retraining/inference procedure since it avoids the costly cosine similarity between a query and nonquantized model. Fig. 6(a) shows an overview of the FPGA implementing the associative search block. Regardless of using a binary or a ternary model, our approach uses binary query hypervector (H q ) to perform the similarity check. The binarization of the query hypervector happens right after the encoding module by comparing each element with n/2, where n is the number of features in the application. In our implementation, the encoding and associative search modules are working in a pipeline structure. As we explained, the encoding module cannot generate all D = 10 000 dimensions of a query hypervector in a single iteration. Therefore, when associative search performs the similarity check on d dimensions of a query hypervector, the encoding module generates the next d dimensions of a query hypervector. In associative search, we first read the first d dimensions of all class hypervectors and then perform the similarity of the binary query with each class hypervector. Fig. 6(d) shows the FPGA implementation supporting dot product between a single class and a query hypervector. Depending on the model, i.e., binary or ternary, the class elements can be represented using one or two bits. For the binary model, class elements represent using a single bit. Thus, the similarity check between a query and the class hypervectors simplifies to Hamming distance. As Fig. 6(d) shows, this operation can be implemented using a single XOR gate. For ternary model, each class element represents using two bits, where {01, 00, 11}. In the ternary model using two bits to represent each class value, the similarity search can be implemented using two AND, a NOT, and an OR gates. The similarity logic in both binary and ternary models create a single bit for each dimension. The results of similarity logic, which are d bits, are added together in a d-bits tree-based adder [shown in Fig. 6(a)]. This d window sequentially moves through all class/query dimensions to cover all D dimensions.\n\nModel Update: In the case of an incorrect match, the retraining uses another module to update the nonquantized model [ Fig. 6(b)]. For each data point in training data, QuantHD checks the result of a similarity search with the label of training data. In the case of misclassification, the \"update indices\" generates the address of two classes that need to be updated. FPGA reads those class hypervectors which are already stored in distributed RAM blocks and updates them by addition and subtraction of them with a query hypervector (H). Finally, the class hypervectors are written back into distributed RAM blocks. QuantHD continues the search operation on the next encoded data points using the same quantized model. Finally, QuantHD quantizes the model [ Fig. 6(b)] after going over the entire training data. This process continues iteratively for a predefined number of iterations.\n\n\nV. EVALUATION\n\n\nA. Experimental Setup\n\nWe implement and verify the functionality of QuantHD training and inference using Verilog. We synthesize the code on Xilinx Vivado Design Suite [22] and implement it on the Kintex-7 FPGA KC705 evaluation kit. We used Vivado XPower tool to estimate the device power. All QuantHD software support, including training, retraining, and inference have been implemented on CPU. For CPU, the QuantHD has been written in C++ and optimized to provide the maximum performance on embedded devices (Raspberry Pi 3) using ARM Cortex A53 CPU. As a baseline, we compare the accuracy and efficiency of QuantHD with state-of-the-art HD computing algorithm [7], [9], multilevel perceptron [23], and binary neural network [24] implemented on FPGA. To show the advantage of QuantHD in both algorithm and hardware aspects, we implement the baseline HD computing using the same implementation as QuantHD, explained in Section IV. Table II summarizes the evaluated datasets. The tested benchmarks range from relatively small datasets collected in a small IoT network, e.g., PHYSICAL, to a large dataset which includes hundreds of thousands of images of facial and nonfacial data. Note that in image-like data, QuantHD works on features which are extracted from the original data. For example, we use histogram of oriented gradients (HOG) feature extraction for face detection problem. In contrast, the convolution layer in neural networks can directly extract information from the original image. Our future work in to include feature extraction as a part of the encoding method.\n\n\nB. Accuracy\n\nTable III compares the classification accuracy of QuantHD using binary and ternary models with the state-of-the-art HD computing algorithm using binary and nonquantized models [9]. The baseline uses Ngram-based encoding, while QuantHD uses the encoding proposed in Section II-A. To have  TABLE III  COMPARISON OF QUANTHD CLASSIFICATION ACCURACY  WITH THE STATE-OF-THE-ART HD COMPUTING a fair comparison, we give an advantage to the baseline HD to retrain the nonquantized model for the same number of iterations as QuantHD model. The results also reported for the baseline HD with the binary model, when the HD models have been turned into binary once after the training. For QuantHD, the models have been retrained for 40 iterations with \u03b1 = 0.05 learning rate. For ternary models, we use ternary boundary b = 0.42\u03c3 which results in maximum classification accuracy. Our evaluation shows that the baseline HD provides high classification accuracy using nonquantized model. However, in baseline model, both training and retraining are significantly costly. The training process involves retraining that involves several iterations of the similarity check over nonquantized model. Similarly, in the inference phase, the associative search between query and trained model required costly consine metric. The binarization of the baseline model has been proposed to reduce the inference cost, by replacing cosine with Hamming distance similarity. However, this binarization used inthe baseline HD computing [8], [9] has two main disadvantages: 1) it results in a significant drop in the classification accuracy, as the model never trained to work with this constraint and 2) the retraining is as costly as the nonquantized model as the similarity check needs to perform using the cosine metric.\n\nQuantHD addresses the several existing issues in the HD computing algorithms. QuantHD provides an iterative procedure which enables the HD to learn to work with binary or ternary models. In addition, QuantHD defines a learning rate for training procedure which further improves the HD classification as compared to prior work with no learning rate (\u03b1 = 1). Our evaluation on Table III shows that QuantHD using binary and ternary models can provide comparable accuracy to the nonquantized model. The accuracy is higher for the ternary model as it gives more flexibility to the training module to select better class values. Our evaluation shows that QuantHD using binary and ternary models provide on average 16.2% and 17.4% higher accuracy as compared to the baseline HD computing [9] using a binary model (see Table III). In addition, we observe that QuantHD accuracy using binary and ternary models is 1.9% and 3.1% higher than baseline HD using nonquantized model.\n\n\nC. Training Efficiency\n\nWe compare the QuantHD with the baseline HD computing in terms of training/retraining efficiency. All HD-based designs have the same performance/energy during the generation of the initial training model. However, during retraining which involves the significant training cost, they have different computation efficiency. In the baseline HD, the retraining is performed by checking the similarity of each training data point with the nonquantized model. This search significantly increases the cost of retraining, since the associative search in the nonquantized domain is much more costly than binary or ternary domains. In contrast, the retraining in QuantHD performs by checking the similarity of training data points with the binary/ternary model. After each similarity check, QuantHD updates the nonquantized model by adding and subtracting a query hypervector from two class hypervectors. Fig. 7 compares the energy consumption and execution time of the baseline HD and QuantHD. Our evaluation shows that QuantHD with the binary (ternary) model can achieve on average 36.4\u00d7 and 4.5\u00d7 (34.1\u00d7 and 4.1\u00d7) energy efficiency improvement and speedup as compared to the baseline HD computing algorithm. Fig. 8 compares the energy consumption and execution time of running a single query in the baseline HD computing with QuantHD using binary and ternary models. All reported results are the average energy and execution time of a single prediction, processed on the entire test data. In QuantHD, the encoding and associative search modules are working in a pipeline stage. Therefore, the execution time of the encoding module hides under the execution time of the associative search. However, FPGA still needs to pay the cost of energy consumption in the encoding module (as shown in top graph in Fig. 8).\n\n\nD. Inference Efficiency\n\nWe used Vivado XPower tool to estimate the device power. Our evaluation shows that HD using the nonquantized model is the most inefficient design due to its significant cost during the associative search. Regardless of the training procedure, the binary models provide the maximum efficiency during inference. We also observe that for most of the applications,  both binary and ternary models can provide higher efficiency than BNN due to their lower number of computations. The results show that QuantHD using binary and ternary model can achieve 45.7\u00d7 and 42.3\u00d7 energy efficiency improvement and 5.2\u00d7 and 4.7\u00d7 speedup as compared to baseline HD while providing comparable accuracy. Fig. 9 shows the energy consumption and execution time of our FPGA implementation with AMD Radeon R390 GPU and ARM Cortex A53 CPU during the inference. All platforms run the baseline HD code using nonquantized model with D = 10 000 dimensions. For each application, the results of energy and execution time are normalized to GPU running HD computing with D = 10 000 dimensions. Our evaluation shows that for all tested applications, FPGA can provide on average 7.6\u00d7 (5.9\u00d7) lower energy consumption and 1.7\u00d7 (41.5\u00d7) faster computation as compared to the GPU (CPU) when running HD in full dimension. The higher efficiency of the FPGA comes from its optimized implementation, high level of parallelism, and storing the HD model close to the computing units.\n\n\nE. QuantHD Trade-Off: Dimensionality\n\nHD computing has been designed to work with the pattern of vectors in high-dimensional space, i.e., D = 10 000. The correct dimensions depend on the actual dataset. However, we can reduce the hypervectors dimension to accelerate both training and testing computation. Fig. 10 shows the impact of dimensionality on the classification accuracy of different applications using nonquantized, ternary, and binary models. Our evaluation shows that QuantHD using binary/ternary  \n(a) (b) (c) (d)\n\nF. Breakdown\n\nTable IV compares the resource utilization of QuantHD using nonquantized, binary, and ternary models. The results are reported for UCIHAR datasets with n = 561 features, k = 12 classes, and using D = 10 000. In terms of resource utilization, all models are using the same utilization for encoding and initial training. The difference of these methods is on the associative search where the nonquantized model takes larger resource to implement a similarity check. The larger associative memory in ternary model reduces the encoder size since these blocks are both implemented using the same LUT and FFs logics. Therefore, the lower number of dimensions can be generated by the encoder at each time. Our results in Table IV show that while in QuantHD with the nonquantized model, the DSP utilization is the main bottleneck of retraining and inference performance. However, QuantHD using binary/ternary model can provide higher performance by removing the necessity of DSPs to perform the associative search. Therefore, QuantHD with the quantized model processes a larger d in parallel.\n\n\nG. QuantHD versus Other Algorithms\n\nAs a light-weight classifier, we compared QuantHD accuracy and efficiency with the state-of-the-art light-weight classifiers, including MLP and BNN. For MLP and BNN, we used the similar models proposed in [24] and made a small modification in input and output layers to run different applications (shown in Table V). We use Keras with Tensorflow backend to train the MLP (BNN) models using Adam optimizer for 10 (100) epochs and learning rate of 0.01. Dropout with a drop rate of 0.5 is applied to DNN layers. Table V lists the classification accuracy and the training/inference execution time of all algorithms. The training results are reported on an embedded device (Raspberry Pi 3) using ARM Cortex A53 CPU. All testing results are reported on Kintex-7 FPGA. For testing, we used the same BNN implementation available on [24] to synthesize the networks on Kintex-7 FPGA. For BNN models, we used the best design parameters (# of SIMD and processing elements) which result in maximum resource utilization and performance. For MLP models, we used the framework proposed in [23] to implement MLP high-level code on FPGA efficiently.\n\nOur evaluation shows that QuantHD can provide comparable accuracy to advanced algorithms while providing much faster computation in both training and testing. For example, QuantHD is on average 8.2\u00d7 and 67.4\u00d7 faster than MLP and BNN during training, when running on ARM Cortex A53 CPU. In addition, using FPGA acceleration for training, QuantHD can further achieve 48.9\u00d7 speedup as compared to ARM CPU. Similarity during testing, QuantHD achieves on average 48.1\u00d7 and 13.4\u00d7 speedup as compared to the MLP and the BNN models, while providing the similar classification accuracy. Table V also compares the QuantHD, MLP, and BNN in terms of model size. Our evaluation shows that QuantHD can provide on average 52.2\u00d7 and 1.6\u00d7 smaller model size as compared to MLP and BNN, respectively.\n\n\nVI. RELATED WORK\n\nThe idea of HD computing has been mapped into several practical problems, including supervised, semi-supervised, and unsupervised learning tasks [26]- [29]. For classification, many existing approaches use HD computing for a single-pass training [9], [30]. For example, the work in [9] proposed a text classification algorithm based on random indexing. However, this method of training provides significantly lower classification accuracy on practical applications. Work in [8] proposed a retraining approach which improves the HD computing accuracy by iteratively updating the HD model over the training dataset. However, this approach forces both retraining and inference phases to use associative search on the fixed-point model which requires costly cosine metric for similarity check.\n\nIn addition, the lack of definition of learning rate results in low stability and possible divergence during the retraining. Prior work tried to binarize the model after the retraining to reduce the inference cost [7], [8]. However, this approach results in a significant drop in classification accuracy. Moreover, the retraining is still expensive as it needs to be processed on the fixed-point model. In contrast, we propose a novel framework which enables both retraining and inference phases to perform on a quantized model using hardware friendly similarity metrics, i.e., Hamming distance. Our approach introduces the definition of the learning rate in retraining and adapts the HD model to work with the quantization constraints.\n\nOn the other side, prior work tried to design efficient hardware to accelerate HD computing, focusing on binary hypervectors. Work in [16], [19], and [31]- [33] designed inmemory architecture based on emerging nonvolatile memory to accelerate HD computation. For example, work in [16] showed three memory-centric hardware to accelerate the associative search in 10 000 dimensions. However, since these hardware are working with binary hypervectors, they provide very low classification accuracy on practical applications (explained in Section II-C). Work in [34] and [35] proposed an efficient implementation of HD computing on FPGA, by exploiting the sparsity to enhance computation efficiency. Our proposed framework enables model quantization, and it opens a new opportunity for binary-based hardware to be used for a wide range of classification problems. In addition, unlike prior work that accelerates HD computing at inference, we proposed FPGA implementation which accelerates all phases on HD computing, including training, retraining, and inference.\n\n\nVII. CONCLUSION\n\nIn this article, we proposed a novel framework for model quantization of HD computing. In contrast to prior work that quantization results in a significant quality loss, QuantHD enables binarization and ternarization of the HD model with minimal impact of the accuracy. QuantHD is an adaptive framework which retrains the HD model to compensate for the possible quality loss due to quantization. Our framework is general and can be used for any types of quantization or enabling sparsity in HD computing. We observe that the gain of quantization is more evident in high-dimensional space. Going toward more restricted quantization, e.g., binarization, dimension reduction has a more destructive impact on the quality of the model. Therefore, a designer can devise to either process HD computing using a low dimensional nonquantized model or high-dimensional quantized model, depending on the underlying platform.\n\nFig. 1 .\n1Overview of HD computing for classification. (a) Classification overview. (b) Encoding.\n\nFig. 2 .\n2(a) QuantHD framework overview. (b) Binarizing and ternarizing the trained HD model.\n\nFig. 3 .\n3QuantHD classification accuracy during (a and b) different retraining iterations and (c and d) using different learning rates.\n\nFig. 4 .\n4Impact of the ternarization boundary on the classification accuracy.\n\n\nFig. 5 shows an overview of the FPGA implementation of the encoding module. Encoding module first reads the feature values, assigns them to pregenerated level hypervectors ( \u2022 A ). Each feature value is compared with m quantized feature values and then assigned to a level with the closest distance ( \u2022 B ). For each feature, the encoding module performs the XOR operation between the level hypervector and the corresponding base hypervector, i.e., B i \u2295 L i for ith feature index, where L i \u2208 {L 1 , . . . , L m } ( \u2022 C ). Finally, for each dimension, the results of XOR operations are accumulated using an adder working in a pipeline structure ( \u2022 D ).All encoding operations, including the XOR and adders are implemented using lookup tables (LUTs) and flip flops (FFs).\n\nFig. 5 .\n5Hardware acceleration of the encoding and initial training modules.\n\nFig. 6 .\n6Hardware acceleration of inference and retraining. (a) Associative search of the binary query with the quantized model. (b) Updating nonquantized model in case of misclassification. (c) Quantization of the model. (d) Similarity metric used for quantized model.\n\nFig. 7 .\n7Energy consumption and execution time of QuantHD, conventional HD, and BNN during training.\n\nFig. 8 .\n8Energy consumption and execution time of QuantHD during inference.\n\nFig. 9 .\n9Normalized Energy consumption and execution time of our FPGA as compared to GPU.\n\nFig. 10 .\n10Impact of dimension reduction on the accuracy of QuantHD with binary and ternary models. (a) ISOLET. (b) FACE. (c) UCIHAR. (d) PAMPA.\n\nTABLE I CLASSIFICATION\nIACCURACY AND EFFICIENCY OF HD RUNNING ON NONQUANTIZED AND BINARY MODEL hypervectors assign to neighbor levels have a high correlation as they have at most D/m bits difference.Aggregation: The encoding of each data point occurs by binding (XORing) each base hypervector with the corresponding level hypervector. For each feature, the level hypervector is selected as the nearest quantized level close to absolute feature value. Finally, we add all the results for all the features\n\nTABLE II DATASETS\nII(n: FEATURE SIZE, k: NUMBER OF CLASSES)\n\nTABLE IV RESOURCE\nIVUTILIZATION OF FPGA DURING TRAINING, RETRAINING, AND INFERENCEmodel can provide similar accuracy as QuantHD with the nonquantized model when the dimensionality is high. However, QuantHD using all models starts losing accuracy when the hypervector dimensions reduce. This accuracy drop is higher for the binary and ternary model since they use low accurate metric, i.e., Hamming distance, for similarity check. Dimension reduction also increases the gap between the accuracy of the nonquantized and quantized model. In particular, we observe a large accuracy drop on QuantHD with the binary model when the dimensionality gets lower than 8000. The results show that QuantHD using D = 8000 can achieve similar classification accuracy as full dimensionality while providing 17.6% energy efficiency and 14.3% speedup. In addition, QuantHD using ternary model can provide 26.4% and 19.8% (34.9% and 27.9%) energy efficiency and speedup while providing 1% (2%) quality loss, as compared to QuantHD with full dimension.\n\nTABLE V COMPARISON\nVOF QUANTHD WITH MLP AND BNN IN TERMS OF ACCURACY, EFFICIENCY, AND MODEL SIZE\n\nInternet of Things (IoT): A vision, architectural elements, and future directions. J Gubbi, R Buyya, S Marusic, M Palaniswami, Future Gener. Comput. Syst. 297J. Gubbi, R. Buyya, S. Marusic, and M. Palaniswami, \"Internet of Things (IoT): A vision, architectural elements, and future directions,\" Future Gener. Comput. Syst., vol. 29, no. 7, pp. 1645-1660, 2013.\n\nPredicting parameters in deep learning. M Denil, B Shakibi, L Dinh, M Ranzato, N. De Freitas, Proc. nullM. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. De Freitas, \"Predicting parameters in deep learning,\" in Proc. Adv. Neural Inf. Process. Syst., 2013, pp. 2148-2156.\n\nImageNet large scale visual recognition challenge. O Russakovsky, Int. J. Comput. Vis. 1153O. Russakovsky et al., \"ImageNet large scale visual recognition chal- lenge,\" Int. J. Comput. Vis., vol. 115, no. 3, pp. 211-252, 2015.\n\nEse: Efficient speech recognition engine with sparse LSTM on FPGA. S Han, Proc. ACM/SIGDA Int. Symp. Field Program. Gate Arrays. ACM/SIGDA Int. Symp. Field Program. Gate ArraysMonterey, CA, USAS. Han et al., \"Ese: Efficient speech recognition engine with sparse LSTM on FPGA,\" in Proc. ACM/SIGDA Int. Symp. Field Program. Gate Arrays, Monterey, CA, USA, 2017, pp. 75-84.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cogn. Comput. 12P. Kanerva, \"Hyperdimensional computing: An introduction to comput- ing in distributed representation with high-dimensional random vectors,\" Cogn. Comput., vol. 1, no. 2, pp. 139-159, 2009.\n\nA framework for collaborative learning in secure high-dimensional space. M Imani, Proc. IEEE 12th Int. Conf. Cloud Comput. (CLOUD). IEEE 12th Int. Conf. Cloud Comput. (CLOUD)Milan, ItalyM. Imani et al., \"A framework for collaborative learning in secure high-dimensional space,\" in Proc. IEEE 12th Int. Conf. Cloud Comput. (CLOUD), Milan, Italy, 2019, pp. 435-446.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, C Huang, D Kong, T Rosing, Proc. 55th Annu. Design Autom. Conf. 55th Annu. Design Autom. ConfSan Francisco, CA, USAM. Imani, C. Huang, D. Kong, and T. Rosing, \"Hierarchical hyperdimensional computing for energy efficient classification,\" in Proc. 55th Annu. Design Autom. Conf., San Francisco, CA, USA, 2018, pp. 1-6.\n\nVoiceHD: Hyperdimensional computing for efficient speech recognition. M Imani, D Kong, A Rahimi, T Rosing, Proc. Int. Conf. Rebooting Comput. (ICRC). Int. Conf. Rebooting Comput. (ICRC)Washington, DC, USAM. Imani, D. Kong, A. Rahimi, and T. Rosing, \"VoiceHD: Hyperdimensional computing for efficient speech recognition,\" in Proc. Int. Conf. Rebooting Comput. (ICRC), Washington, DC, USA, 2017, pp. 1-8.\n\nA robust and energyefficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proc. Int. Symp. Low Power Electron. Design. Int. Symp. Low Power Electron. DesignA. Rahimi, P. Kanerva, and J. M. Rabaey, \"A robust and energy- efficient classifier using brain-inspired hyperdimensional comput- ing,\" in Proc. Int. Symp. Low Power Electron. Design, 2016, pp. 64-69.\n\nRandom indexing of text samples for latent semantic analysis. P Kanerva, J Kristoferson, A Holst, Proc. 22nd Annu. Conf. 22nd Annu. Conf1036P. Kanerva, J. Kristoferson, and A. Holst, \"Random indexing of text samples for latent semantic analysis,\" in Proc. 22nd Annu. Conf. Cogn. Sci. Soc., vol. 1036, 2000, pp. 103-106.\n\nUCI Machine Learning Repository. (2012). UCI Machine Learning Repository. [Online]. Available: http://archive.ics.uci.edu/ml/datasets/ISOLET\n\nHuman activity recognition on smartphones using a multiclass hardware-friendly support vector machine. D Anguita, A Ghio, L Oneto, X Parra, J L Reyes-Ortiz, Proc. Int. Workshop Ambient Assist. Living. Int. Workshop Ambient Assist. LivingD. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, \"Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine,\" in Proc. Int. Workshop Ambient Assist. Living, 2012, pp. 216-223.\n\nIntroducing a new benchmarked dataset for activity monitoring. A Reiss, D Stricker, Proc. 16th Int. Symp. Wearable Comput. (ISWC). 16th Int. Symp. Wearable Comput. (ISWC)A. Reiss and D. Stricker, \"Introducing a new benchmarked dataset for activity monitoring,\" in Proc. 16th Int. Symp. Wearable Comput. (ISWC), 2012, pp. 108-109.\n\nOrchard: Visual object recognition accelerator based on approximate in-memory processing. Y Kim, M Imani, T Rosing, Proc. IEEE/ACM Int. Conf. Comput.-Aided Design (ICCAD). IEEE/ACM Int. Conf. Comput.-Aided Design (ICCAD)Irvine, CA, USAY. Kim, M. Imani, and T. Rosing, \"Orchard: Visual object recogni- tion accelerator based on approximate in-memory processing,\" in Proc. IEEE/ACM Int. Conf. Comput.-Aided Design (ICCAD), Irvine, CA, USA, 2017, pp. 25-32.\n\nHigh-dimensional computing as a nanoscalable paradigm. A Rahimi, IEEE Trans. Circuits Syst. I, Reg. Papers. 649A. Rahimi et al., \"High-dimensional computing as a nanoscalable paradigm,\" IEEE Trans. Circuits Syst. I, Reg. Papers, vol. 64, no. 9, pp. 2508-2521, Sep. 2017.\n\nExploring hyperdimensional associative memory. M Imani, A Rahimi, D Kong, T Rosing, J M Rabaey, Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA). IEEE Int. Symp. High Perform. Comput. Archit. (HPCA)Austin, TX, USAM. Imani, A. Rahimi, D. Kong, T. Rosing, and J. M. Rabaey, \"Exploring hyperdimensional associative memory,\" in Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA), Austin, TX, USA, 2017, pp. 445-456.\n\nHyperdimensional computing for blind and one-shot classification of EEG error-related potentials. A Rahimi, A Tchouprina, P Kanerva, J D R Mill\u00e1n, J M Rabaey, Mobile Netw. Appl. 3A. Rahimi, A. Tchouprina, P. Kanerva, J. D. R. Mill\u00e1n, and J. M. Rabaey, \"Hyperdimensional computing for blind and one-shot classification of EEG error-related potentials,\" Mobile Netw. Appl., vol. 3, pp. 1-12, Oct. 2017.\n\nHDNA: Energyefficient dna sequencing using hyperdimensional computing. M Imani, T Nassar, A Rahimi, T Rosing, Proc. IEEE EMBS Int. Conf. Biomed. Health Informat. (BHI). IEEE EMBS Int. Conf. Biomed. Health Informat. (BHI)Las Vegas, NV, USAM. Imani, T. Nassar, A. Rahimi, and T. Rosing, \"HDNA: Energy- efficient dna sequencing using hyperdimensional computing,\" in Proc. IEEE EMBS Int. Conf. Biomed. Health Informat. (BHI), Las Vegas, NV, USA, 2018, pp. 271-274.\n\nBrain-inspired computing exploiting carbon nanotube FETs and resistive RAM: Hyperdimensional computing case study. T F Wu, Proc. IEEE Int. Solid-State Circuits Conf. (ISSCC). IEEE Int. Solid-State Circuits Conf. (ISSCC)San Francisco, CA, USAT. F. Wu et al., \"Brain-inspired computing exploiting carbon nanotube FETs and resistive RAM: Hyperdimensional computing case study,\" in Proc. IEEE Int. Solid-State Circuits Conf. (ISSCC), San Francisco, CA, USA, 2018, pp. 492-494.\n\nDropout: A simple way to prevent neural networks from overfitting. N Srivastava, G E Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, J. Mach. Learn. Res. 151N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: A simple way to prevent neural networks from overfitting,\" J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929-1958, 2014.\n\nA study on overfitting in deep reinforcement learning. C Zhang, O Vinyals, R Munos, S Bengio, arXiv:1804.06893arXiv preprintC. Zhang, O. Vinyals, R. Munos, and S. Bengio, \"A study on overfitting in deep reinforcement learning,\" arXiv preprint arXiv:1804.06893, 2018.\n\nVivado design suite. T Feist, White Paper. 5T. Feist, \"Vivado design suite,\" White Paper, vol. 5, 2012.\n\nFrom high-level deep neural models to FPGAs. H Sharma, Proc. 49th Annu. 49th AnnuH. Sharma et al., \"From high-level deep neural models to FPGAs,\" in Proc. 49th Annu. IEEE/ACM Int. Symp. Microarchit. (MICRO), 2016, pp. 1-12.\n\nFinn: A framework for fast, scalable binarized neural network inference. Y Umuroglu, Proc. ACM/SIGDA Int. Symp. Field Program. Gate Arrays. ACM/SIGDA Int. Symp. Field Program. Gate ArraysMonterey, CA, USAY. Umuroglu et al., \"Finn: A framework for fast, scalable binarized neu- ral network inference,\" in Proc. ACM/SIGDA Int. Symp. Field Program. Gate Arrays, Monterey, CA, USA, 2017, pp. 65-74.\n\nRecognizing detailed human context in the wild from smartphones and smartwatches. Y Vaizman, K Ellis, G Lanckriet, IEEE Pervasive Comput. 164Y. Vaizman, K. Ellis, and G. Lanckriet, \"Recognizing detailed human context in the wild from smartphones and smart- watches,\" IEEE Pervasive Comput., vol. 16, no. 4, pp. 62-74, Oct.-Dec. 2017.\n\nBRIC: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing. M Imani, J Morris, J Messerly, H Shu, Y Deng, T Rosing, Proc. 56th Annu. Design Autom. Conf. 56th Annu. Design Autom. ConfLas Vegas, NV, USA52M. Imani, J. Morris, J. Messerly, H. Shu, Y. Deng, and T. Rosing, \"BRIC: Locality-based encoding for energy-efficient brain-inspired hyperdimensional computing,\" in Proc. 56th Annu. Design Autom. Conf., Las Vegas, NV, USA, 2019, p. 52.\n\nEfficient human activity recognition using hyperdimensional computing. Y Kim, M Imani, T S Rosing, Proc. 8th Int. Conf. Internet Things. 8th Int. Conf. Internet ThingsSanta Barbara, CA, USA38Y. Kim, M. Imani, and T. S. Rosing, \"Efficient human activity recogni- tion using hyperdimensional computing,\" in Proc. 8th Int. Conf. Internet Things, Santa Barbara, CA, USA, 2018, p. 38.\n\nHDcluster: An accurate clustering using brain-inspired high-dimensional computing. M Imani, Y Kim, T Worley, S Gupta, T Rosing, Proc. Design Autom. Test Europe Conf. Exhibit. (DATE). Design Autom. Test Europe Conf. Exhibit. (DATE)Florence, ItalyM. Imani, Y. Kim, T. Worley, S. Gupta, and T. Rosing, \"HDcluster: An accurate clustering using brain-inspired high-dimensional computing,\" in Proc. Design Autom. Test Europe Conf. Exhibit. (DATE), Florence, Italy, 2019, pp. 1591-1594.\n\nSemiHD: Semi-supervised learning using hyperdimensional computing. M Imani, Proc. IEEE/ACM Int. Conf. Comput.-Aided Design (ICCAD). IEEE/ACM Int. Conf. Comput.-Aided Design (ICCAD)M. Imani et al., \"SemiHD: Semi-supervised learning using hyperdimen- sional computing,\" in Proc. IEEE/ACM Int. Conf. Comput.-Aided Design (ICCAD), 2019, pp. 1-8.\n\nHyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition. A Rahimi, S Benatti, P Kanerva, L Benini, J M Rabaey, Proc. IEEE Int. Conf. Rebooting Comput. (ICRC). IEEE Int. Conf. Rebooting Comput. (ICRC)San Diego, CA, USAA. Rahimi, S. Benatti, P. Kanerva, L. Benini, and J. M. Rabaey, \"Hyperdimensional biosignal processing: A case study for EMG-based hand gesture recognition,\" in Proc. IEEE Int. Conf. Rebooting Comput. (ICRC), San Diego, CA, USA, 2016, pp. 1-8.\n\nHyperdimensional computing with 3D VRRAM in-memory kernels: Device-architecture co-design for energyefficient, error-resilient language recognition. H Li, Proc. IEEE Int. Electron Devices Meeting (IEDM). IEEE Int. Electron Devices Meeting (IEDM)San Francisco, CA, USA16.1.1-16.1.4.H. Li et al., \"Hyperdimensional computing with 3D VRRAM in-memory kernels: Device-architecture co-design for energy- efficient, error-resilient language recognition,\" in Proc. IEEE Int. Electron Devices Meeting (IEDM), San Francisco, CA, USA, 2016, pp. 16.1.1-16.1.4.\n\nFELIX: Fast and energy-efficient logic in memory. S Gupta, M Imani, T Rosing, Proc. IEEE/ACM Int. Conf. Comput.-Aided Design (ICCAD). IEEE/ACM Int. Conf. Comput.-Aided Design (ICCAD)San Diego, CA, USAS. Gupta, M. Imani, and T. Rosing, \"FELIX: Fast and energy-efficient logic in memory,\" in Proc. IEEE/ACM Int. Conf. Comput.-Aided Design (ICCAD), San Diego, CA, USA, 2018, pp. 1-7.\n\nSearcHD: A memory-centric hyperdimensional computing with stochastic training. M Imani, IEEE Trans. Comput.-Aided Design Integr. Circuits Syst. to be publishedM. Imani et al., \"SearcHD: A memory-centric hyperdimensional com- puting with stochastic training,\" IEEE Trans. Comput.-Aided Design Integr. Circuits Syst., to be published.\n\nF5-HD: Fast flexible FPGA-based framework for refreshing hyperdimensional computing. S Salamat, M Imani, B Khaleghi, T Rosing, Proc. ACM/SIGDA Int. Symp. Field Program. Gate Arrays. ACM/SIGDA Int. Symp. Field Program. Gate ArraysS. Salamat, M. Imani, B. Khaleghi, and T. Rosing, \"F5-HD: Fast flexible FPGA-based framework for refreshing hyperdimensional computing,\" in Proc. ACM/SIGDA Int. Symp. Field Program. Gate Arrays, 2019, pp. 53-62.\n\nSparseHD: Algorithm-hardware co-optimization for efficient high-dimensional computing. M Imani, S Salamat, B Khaleghi, M Samragh, F Koushanfar, T Rosing, Proc. IEEE 27th Annu. Int. Symp. Field Program. Custom Comput. Mach. (FCCM). IEEE 27th Annu. Int. Symp. Field Program. Custom Comput. Mach. (FCCM)San Diego, CA, USAM. Imani, S. Salamat, B. Khaleghi, M. Samragh, F. Koushanfar, and T. Rosing, \"SparseHD: Algorithm-hardware co-optimization for efficient high-dimensional computing,\" in Proc. IEEE 27th Annu. Int. Symp. Field Program. Custom Comput. Mach. (FCCM), San Diego, CA, USA, 2019, pp. 190-198.\n", "annotations": {"author": "[{\"start\":\"80\",\"end\":\"93\"},{\"start\":\"94\",\"end\":\"107\"},{\"start\":\"108\",\"end\":\"120\"},{\"start\":\"121\",\"end\":\"142\"},{\"start\":\"143\",\"end\":\"158\"},{\"start\":\"159\",\"end\":\"184\"},{\"start\":\"185\",\"end\":\"211\"}]", "publisher": null, "author_last_name": "[{\"start\":\"87\",\"end\":\"92\"},{\"start\":\"101\",\"end\":\"106\"},{\"start\":\"114\",\"end\":\"119\"},{\"start\":\"130\",\"end\":\"141\"},{\"start\":\"150\",\"end\":\"157\"},{\"start\":\"177\",\"end\":\"183\"},{\"start\":\"204\",\"end\":\"210\"}]", "author_first_name": "[{\"start\":\"80\",\"end\":\"86\"},{\"start\":\"94\",\"end\":\"100\"},{\"start\":\"108\",\"end\":\"113\"},{\"start\":\"121\",\"end\":\"129\"},{\"start\":\"143\",\"end\":\"149\"},{\"start\":\"171\",\"end\":\"174\"},{\"start\":\"175\",\"end\":\"176\"},{\"start\":\"197\",\"end\":\"203\"}]", "author_affiliation": null, "title": "[{\"start\":\"1\",\"end\":\"65\"},{\"start\":\"212\",\"end\":\"276\"}]", "venue": "[{\"start\":\"278\",\"end\":\"355\"}]", "abstract": "[{\"start\":\"402\",\"end\":\"1897\"}]", "bib_ref": "[{\"start\":\"2093\",\"end\":\"2096\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"2369\",\"end\":\"2372\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"2712\",\"end\":\"2715\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"2765\",\"end\":\"2768\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2770\",\"end\":\"2773\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"2975\",\"end\":\"2978\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3198\",\"end\":\"3201\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"3203\",\"end\":\"3206\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"4210\",\"end\":\"4213\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"5574\",\"end\":\"5577\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"5579\",\"end\":\"5582\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"5821\",\"end\":\"5824\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"5826\",\"end\":\"5829\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"7730\",\"end\":\"7734\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"9742\",\"end\":\"9745\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"9747\",\"end\":\"9751\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"9753\",\"end\":\"9757\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"9966\",\"end\":\"9970\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"9972\",\"end\":\"9976\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"10236\",\"end\":\"10240\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"10263\",\"end\":\"10267\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"10289\",\"end\":\"10293\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"10314\",\"end\":\"10318\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"10372\",\"end\":\"10375\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"19722\",\"end\":\"19725\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"21501\",\"end\":\"21505\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"21507\",\"end\":\"21511\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"24217\",\"end\":\"24220\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"27636\",\"end\":\"27640\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"28131\",\"end\":\"28134\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"28136\",\"end\":\"28139\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"28163\",\"end\":\"28167\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"28195\",\"end\":\"28199\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"29240\",\"end\":\"29243\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"30566\",\"end\":\"30569\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"30571\",\"end\":\"30574\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"31636\",\"end\":\"31639\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"36990\",\"end\":\"36994\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"37610\",\"end\":\"37614\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"37859\",\"end\":\"37863\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"38867\",\"end\":\"38871\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"38873\",\"end\":\"38877\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"38968\",\"end\":\"38971\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"38973\",\"end\":\"38977\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"39004\",\"end\":\"39007\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"39196\",\"end\":\"39199\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"39727\",\"end\":\"39730\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"39732\",\"end\":\"39735\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"40385\",\"end\":\"40389\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"40391\",\"end\":\"40395\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"40401\",\"end\":\"40405\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"40407\",\"end\":\"40411\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"40531\",\"end\":\"40535\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"40809\",\"end\":\"40813\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"40818\",\"end\":\"40822\",\"attributes\":{\"ref_id\":\"b34\"}}]", "figure": "[{\"start\":\"42243\",\"end\":\"42341\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"42342\",\"end\":\"42437\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"42438\",\"end\":\"42575\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"42576\",\"end\":\"42655\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"42656\",\"end\":\"43430\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"43431\",\"end\":\"43509\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"43510\",\"end\":\"43781\",\"attributes\":{\"id\":\"fig_7\"}},{\"start\":\"43782\",\"end\":\"43884\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"43885\",\"end\":\"43962\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"43963\",\"end\":\"44054\",\"attributes\":{\"id\":\"fig_10\"}},{\"start\":\"44055\",\"end\":\"44201\",\"attributes\":{\"id\":\"fig_11\"}},{\"start\":\"44202\",\"end\":\"44706\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"44707\",\"end\":\"44767\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"44768\",\"end\":\"45800\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"45801\",\"end\":\"45898\",\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1899\",\"end\":\"2716\"},{\"start\":\"2718\",\"end\":\"4037\"},{\"start\":\"4039\",\"end\":\"4467\"},{\"start\":\"4469\",\"end\":\"6085\"},{\"start\":\"6120\",\"end\":\"7037\"},{\"start\":\"7053\",\"end\":\"7398\"},{\"start\":\"7400\",\"end\":\"7796\"},{\"start\":\"7837\",\"end\":\"8917\"},{\"start\":\"8967\",\"end\":\"9032\"},{\"start\":\"9051\",\"end\":\"9553\"},{\"start\":\"9598\",\"end\":\"10898\"},{\"start\":\"10900\",\"end\":\"11386\"},{\"start\":\"11426\",\"end\":\"12280\"},{\"start\":\"12305\",\"end\":\"12595\"},{\"start\":\"12656\",\"end\":\"12834\"},{\"start\":\"12836\",\"end\":\"13018\"},{\"start\":\"13020\",\"end\":\"13798\"},{\"start\":\"13813\",\"end\":\"14319\"},{\"start\":\"14368\",\"end\":\"14842\"},{\"start\":\"14844\",\"end\":\"16300\"},{\"start\":\"16352\",\"end\":\"16737\"},{\"start\":\"16739\",\"end\":\"17093\"},{\"start\":\"17095\",\"end\":\"17440\"},{\"start\":\"17442\",\"end\":\"18253\"},{\"start\":\"18255\",\"end\":\"18961\"},{\"start\":\"18989\",\"end\":\"19527\"},{\"start\":\"19556\",\"end\":\"20269\"},{\"start\":\"20298\",\"end\":\"21264\"},{\"start\":\"21266\",\"end\":\"22085\"},{\"start\":\"22087\",\"end\":\"22832\"},{\"start\":\"22877\",\"end\":\"23260\"},{\"start\":\"23262\",\"end\":\"26563\"},{\"start\":\"26565\",\"end\":\"27450\"},{\"start\":\"27492\",\"end\":\"29048\"},{\"start\":\"29064\",\"end\":\"30853\"},{\"start\":\"30855\",\"end\":\"31822\"},{\"start\":\"31849\",\"end\":\"33651\"},{\"start\":\"33679\",\"end\":\"35117\"},{\"start\":\"35158\",\"end\":\"35630\"},{\"start\":\"35662\",\"end\":\"36746\"},{\"start\":\"36785\",\"end\":\"37917\"},{\"start\":\"37919\",\"end\":\"38701\"},{\"start\":\"38722\",\"end\":\"39511\"},{\"start\":\"39513\",\"end\":\"40249\"},{\"start\":\"40251\",\"end\":\"41310\"},{\"start\":\"41330\",\"end\":\"42242\"}]", "formula": "[{\"start\":\"7797\",\"end\":\"7836\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"8918\",\"end\":\"8966\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"12596\",\"end\":\"12655\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"13799\",\"end\":\"13812\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"14320\",\"end\":\"14367\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"16301\",\"end\":\"16316\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"35631\",\"end\":\"35646\",\"attributes\":{\"id\":\"formula_6\"}}]", "table_ref": "[{\"start\":\"10122\",\"end\":\"10129\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"10915\",\"end\":\"10922\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"15465\",\"end\":\"15472\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"28400\",\"end\":\"28408\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"29346\",\"end\":\"29448\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"31230\",\"end\":\"31239\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"31666\",\"end\":\"31675\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"36376\",\"end\":\"36384\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"37092\",\"end\":\"37100\",\"attributes\":{\"ref_id\":\"tab_4\"}},{\"start\":\"37295\",\"end\":\"37302\",\"attributes\":{\"ref_id\":\"tab_4\"}},{\"start\":\"38497\",\"end\":\"38504\",\"attributes\":{\"ref_id\":\"tab_4\"}}]", "section_header": "[{\"start\":\"6088\",\"end\":\"6118\"},{\"start\":\"7040\",\"end\":\"7051\"},{\"start\":\"9035\",\"end\":\"9049\"},{\"start\":\"9556\",\"end\":\"9596\"},{\"start\":\"11389\",\"end\":\"11410\"},{\"start\":\"11413\",\"end\":\"11424\"},{\"start\":\"12283\",\"end\":\"12303\"},{\"start\":\"16318\",\"end\":\"16350\"},{\"start\":\"18964\",\"end\":\"18987\"},{\"start\":\"19530\",\"end\":\"19554\"},{\"start\":\"20272\",\"end\":\"20296\"},{\"start\":\"22835\",\"end\":\"22875\"},{\"start\":\"27453\",\"end\":\"27466\"},{\"start\":\"27469\",\"end\":\"27490\"},{\"start\":\"29051\",\"end\":\"29062\"},{\"start\":\"31825\",\"end\":\"31847\"},{\"start\":\"33654\",\"end\":\"33677\"},{\"start\":\"35120\",\"end\":\"35156\"},{\"start\":\"35648\",\"end\":\"35660\"},{\"start\":\"36749\",\"end\":\"36783\"},{\"start\":\"38704\",\"end\":\"38720\"},{\"start\":\"41313\",\"end\":\"41328\"},{\"start\":\"42244\",\"end\":\"42252\"},{\"start\":\"42343\",\"end\":\"42351\"},{\"start\":\"42439\",\"end\":\"42447\"},{\"start\":\"42577\",\"end\":\"42585\"},{\"start\":\"43432\",\"end\":\"43440\"},{\"start\":\"43511\",\"end\":\"43519\"},{\"start\":\"43783\",\"end\":\"43791\"},{\"start\":\"43886\",\"end\":\"43894\"},{\"start\":\"43964\",\"end\":\"43972\"},{\"start\":\"44056\",\"end\":\"44065\"},{\"start\":\"44203\",\"end\":\"44225\"},{\"start\":\"44708\",\"end\":\"44725\"},{\"start\":\"44769\",\"end\":\"44786\"},{\"start\":\"45802\",\"end\":\"45820\"}]", "table": null, "figure_caption": "[{\"start\":\"42254\",\"end\":\"42341\"},{\"start\":\"42353\",\"end\":\"42437\"},{\"start\":\"42449\",\"end\":\"42575\"},{\"start\":\"42587\",\"end\":\"42655\"},{\"start\":\"42658\",\"end\":\"43430\"},{\"start\":\"43442\",\"end\":\"43509\"},{\"start\":\"43521\",\"end\":\"43781\"},{\"start\":\"43793\",\"end\":\"43884\"},{\"start\":\"43896\",\"end\":\"43962\"},{\"start\":\"43974\",\"end\":\"44054\"},{\"start\":\"44068\",\"end\":\"44201\"},{\"start\":\"44227\",\"end\":\"44706\"},{\"start\":\"44728\",\"end\":\"44767\"},{\"start\":\"44789\",\"end\":\"45800\"},{\"start\":\"45822\",\"end\":\"45898\"}]", "figure_ref": "[{\"start\":\"6270\",\"end\":\"6279\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"12437\",\"end\":\"12443\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"15062\",\"end\":\"15068\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"15931\",\"end\":\"15937\",\"attributes\":{\"ref_id\":\"fig_3\"}},{\"start\":\"17329\",\"end\":\"17338\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"17937\",\"end\":\"17946\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"18337\",\"end\":\"18343\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"19694\",\"end\":\"19716\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"20657\",\"end\":\"20663\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"21109\",\"end\":\"21116\",\"attributes\":{\"ref_id\":\"fig_6\"}},{\"start\":\"23547\",\"end\":\"23556\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"24588\",\"end\":\"24594\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"25552\",\"end\":\"25561\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"25945\",\"end\":\"25954\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"26457\",\"end\":\"26466\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"26684\",\"end\":\"26693\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"27323\",\"end\":\"27332\",\"attributes\":{\"ref_id\":\"fig_7\"}},{\"start\":\"32744\",\"end\":\"32750\",\"attributes\":{\"ref_id\":\"fig_8\"}},{\"start\":\"33049\",\"end\":\"33055\",\"attributes\":{\"ref_id\":\"fig_9\"}},{\"start\":\"33643\",\"end\":\"33650\",\"attributes\":{\"ref_id\":\"fig_9\"}},{\"start\":\"34363\",\"end\":\"34369\",\"attributes\":{\"ref_id\":\"fig_10\"}},{\"start\":\"35426\",\"end\":\"35433\",\"attributes\":{\"ref_id\":\"fig_0\"}}]", "bib_author_first_name": "[{\"start\":\"45983\",\"end\":\"45984\"},{\"start\":\"45992\",\"end\":\"45993\"},{\"start\":\"46001\",\"end\":\"46002\"},{\"start\":\"46012\",\"end\":\"46013\"},{\"start\":\"46302\",\"end\":\"46303\"},{\"start\":\"46311\",\"end\":\"46312\"},{\"start\":\"46322\",\"end\":\"46323\"},{\"start\":\"46330\",\"end\":\"46331\"},{\"start\":\"46341\",\"end\":\"46346\"},{\"start\":\"46585\",\"end\":\"46586\"},{\"start\":\"46829\",\"end\":\"46830\"},{\"start\":\"47259\",\"end\":\"47260\"},{\"start\":\"47550\",\"end\":\"47551\"},{\"start\":\"47919\",\"end\":\"47920\"},{\"start\":\"47928\",\"end\":\"47929\"},{\"start\":\"47937\",\"end\":\"47938\"},{\"start\":\"47945\",\"end\":\"47946\"},{\"start\":\"48317\",\"end\":\"48318\"},{\"start\":\"48326\",\"end\":\"48327\"},{\"start\":\"48334\",\"end\":\"48335\"},{\"start\":\"48344\",\"end\":\"48345\"},{\"start\":\"48740\",\"end\":\"48741\"},{\"start\":\"48750\",\"end\":\"48751\"},{\"start\":\"48761\",\"end\":\"48762\"},{\"start\":\"48763\",\"end\":\"48764\"},{\"start\":\"49119\",\"end\":\"49120\"},{\"start\":\"49130\",\"end\":\"49131\"},{\"start\":\"49146\",\"end\":\"49147\"},{\"start\":\"49623\",\"end\":\"49624\"},{\"start\":\"49634\",\"end\":\"49635\"},{\"start\":\"49642\",\"end\":\"49643\"},{\"start\":\"49651\",\"end\":\"49652\"},{\"start\":\"49660\",\"end\":\"49661\"},{\"start\":\"49662\",\"end\":\"49663\"},{\"start\":\"50056\",\"end\":\"50057\"},{\"start\":\"50065\",\"end\":\"50066\"},{\"start\":\"50414\",\"end\":\"50415\"},{\"start\":\"50421\",\"end\":\"50422\"},{\"start\":\"50430\",\"end\":\"50431\"},{\"start\":\"50835\",\"end\":\"50836\"},{\"start\":\"51099\",\"end\":\"51100\"},{\"start\":\"51108\",\"end\":\"51109\"},{\"start\":\"51118\",\"end\":\"51119\"},{\"start\":\"51126\",\"end\":\"51127\"},{\"start\":\"51136\",\"end\":\"51137\"},{\"start\":\"51138\",\"end\":\"51139\"},{\"start\":\"51581\",\"end\":\"51582\"},{\"start\":\"51591\",\"end\":\"51592\"},{\"start\":\"51605\",\"end\":\"51606\"},{\"start\":\"51616\",\"end\":\"51617\"},{\"start\":\"51618\",\"end\":\"51621\"},{\"start\":\"51630\",\"end\":\"51631\"},{\"start\":\"51632\",\"end\":\"51633\"},{\"start\":\"51956\",\"end\":\"51957\"},{\"start\":\"51965\",\"end\":\"51966\"},{\"start\":\"51975\",\"end\":\"51976\"},{\"start\":\"51985\",\"end\":\"51986\"},{\"start\":\"52462\",\"end\":\"52463\"},{\"start\":\"52464\",\"end\":\"52465\"},{\"start\":\"52888\",\"end\":\"52889\"},{\"start\":\"52902\",\"end\":\"52903\"},{\"start\":\"52904\",\"end\":\"52905\"},{\"start\":\"52914\",\"end\":\"52915\"},{\"start\":\"52928\",\"end\":\"52929\"},{\"start\":\"52941\",\"end\":\"52942\"},{\"start\":\"53246\",\"end\":\"53247\"},{\"start\":\"53255\",\"end\":\"53256\"},{\"start\":\"53266\",\"end\":\"53267\"},{\"start\":\"53275\",\"end\":\"53276\"},{\"start\":\"53480\",\"end\":\"53481\"},{\"start\":\"53609\",\"end\":\"53610\"},{\"start\":\"53862\",\"end\":\"53863\"},{\"start\":\"54267\",\"end\":\"54268\"},{\"start\":\"54278\",\"end\":\"54279\"},{\"start\":\"54287\",\"end\":\"54288\"},{\"start\":\"54614\",\"end\":\"54615\"},{\"start\":\"54623\",\"end\":\"54624\"},{\"start\":\"54633\",\"end\":\"54634\"},{\"start\":\"54645\",\"end\":\"54646\"},{\"start\":\"54652\",\"end\":\"54653\"},{\"start\":\"54660\",\"end\":\"54661\"},{\"start\":\"55064\",\"end\":\"55065\"},{\"start\":\"55071\",\"end\":\"55072\"},{\"start\":\"55080\",\"end\":\"55081\"},{\"start\":\"55082\",\"end\":\"55083\"},{\"start\":\"55457\",\"end\":\"55458\"},{\"start\":\"55466\",\"end\":\"55467\"},{\"start\":\"55473\",\"end\":\"55474\"},{\"start\":\"55483\",\"end\":\"55484\"},{\"start\":\"55492\",\"end\":\"55493\"},{\"start\":\"55922\",\"end\":\"55923\"},{\"start\":\"56290\",\"end\":\"56291\"},{\"start\":\"56300\",\"end\":\"56301\"},{\"start\":\"56311\",\"end\":\"56312\"},{\"start\":\"56322\",\"end\":\"56323\"},{\"start\":\"56332\",\"end\":\"56333\"},{\"start\":\"56334\",\"end\":\"56335\"},{\"start\":\"56844\",\"end\":\"56845\"},{\"start\":\"57295\",\"end\":\"57296\"},{\"start\":\"57304\",\"end\":\"57305\"},{\"start\":\"57313\",\"end\":\"57314\"},{\"start\":\"57706\",\"end\":\"57707\"},{\"start\":\"58046\",\"end\":\"58047\"},{\"start\":\"58057\",\"end\":\"58058\"},{\"start\":\"58066\",\"end\":\"58067\"},{\"start\":\"58078\",\"end\":\"58079\"},{\"start\":\"58490\",\"end\":\"58491\"},{\"start\":\"58499\",\"end\":\"58500\"},{\"start\":\"58510\",\"end\":\"58511\"},{\"start\":\"58522\",\"end\":\"58523\"},{\"start\":\"58533\",\"end\":\"58534\"},{\"start\":\"58547\",\"end\":\"58548\"}]", "bib_author_last_name": "[{\"start\":\"45985\",\"end\":\"45990\"},{\"start\":\"45994\",\"end\":\"45999\"},{\"start\":\"46003\",\"end\":\"46010\"},{\"start\":\"46014\",\"end\":\"46025\"},{\"start\":\"46304\",\"end\":\"46309\"},{\"start\":\"46313\",\"end\":\"46320\"},{\"start\":\"46324\",\"end\":\"46328\"},{\"start\":\"46332\",\"end\":\"46339\"},{\"start\":\"46347\",\"end\":\"46354\"},{\"start\":\"46587\",\"end\":\"46598\"},{\"start\":\"46831\",\"end\":\"46834\"},{\"start\":\"47261\",\"end\":\"47268\"},{\"start\":\"47552\",\"end\":\"47557\"},{\"start\":\"47921\",\"end\":\"47926\"},{\"start\":\"47930\",\"end\":\"47935\"},{\"start\":\"47939\",\"end\":\"47943\"},{\"start\":\"47947\",\"end\":\"47953\"},{\"start\":\"48319\",\"end\":\"48324\"},{\"start\":\"48328\",\"end\":\"48332\"},{\"start\":\"48336\",\"end\":\"48342\"},{\"start\":\"48346\",\"end\":\"48352\"},{\"start\":\"48742\",\"end\":\"48748\"},{\"start\":\"48752\",\"end\":\"48759\"},{\"start\":\"48765\",\"end\":\"48771\"},{\"start\":\"49121\",\"end\":\"49128\"},{\"start\":\"49132\",\"end\":\"49144\"},{\"start\":\"49148\",\"end\":\"49153\"},{\"start\":\"49625\",\"end\":\"49632\"},{\"start\":\"49636\",\"end\":\"49640\"},{\"start\":\"49644\",\"end\":\"49649\"},{\"start\":\"49653\",\"end\":\"49658\"},{\"start\":\"49664\",\"end\":\"49675\"},{\"start\":\"50058\",\"end\":\"50063\"},{\"start\":\"50067\",\"end\":\"50075\"},{\"start\":\"50416\",\"end\":\"50419\"},{\"start\":\"50423\",\"end\":\"50428\"},{\"start\":\"50432\",\"end\":\"50438\"},{\"start\":\"50837\",\"end\":\"50843\"},{\"start\":\"51101\",\"end\":\"51106\"},{\"start\":\"51110\",\"end\":\"51116\"},{\"start\":\"51120\",\"end\":\"51124\"},{\"start\":\"51128\",\"end\":\"51134\"},{\"start\":\"51140\",\"end\":\"51146\"},{\"start\":\"51583\",\"end\":\"51589\"},{\"start\":\"51593\",\"end\":\"51603\"},{\"start\":\"51607\",\"end\":\"51614\"},{\"start\":\"51622\",\"end\":\"51628\"},{\"start\":\"51634\",\"end\":\"51640\"},{\"start\":\"51958\",\"end\":\"51963\"},{\"start\":\"51967\",\"end\":\"51973\"},{\"start\":\"51977\",\"end\":\"51983\"},{\"start\":\"51987\",\"end\":\"51993\"},{\"start\":\"52466\",\"end\":\"52468\"},{\"start\":\"52890\",\"end\":\"52900\"},{\"start\":\"52906\",\"end\":\"52912\"},{\"start\":\"52916\",\"end\":\"52926\"},{\"start\":\"52930\",\"end\":\"52939\"},{\"start\":\"52943\",\"end\":\"52956\"},{\"start\":\"53248\",\"end\":\"53253\"},{\"start\":\"53257\",\"end\":\"53264\"},{\"start\":\"53268\",\"end\":\"53273\"},{\"start\":\"53277\",\"end\":\"53283\"},{\"start\":\"53482\",\"end\":\"53487\"},{\"start\":\"53611\",\"end\":\"53617\"},{\"start\":\"53864\",\"end\":\"53872\"},{\"start\":\"54269\",\"end\":\"54276\"},{\"start\":\"54280\",\"end\":\"54285\"},{\"start\":\"54289\",\"end\":\"54298\"},{\"start\":\"54616\",\"end\":\"54621\"},{\"start\":\"54625\",\"end\":\"54631\"},{\"start\":\"54635\",\"end\":\"54643\"},{\"start\":\"54647\",\"end\":\"54650\"},{\"start\":\"54654\",\"end\":\"54658\"},{\"start\":\"54662\",\"end\":\"54668\"},{\"start\":\"55066\",\"end\":\"55069\"},{\"start\":\"55073\",\"end\":\"55078\"},{\"start\":\"55084\",\"end\":\"55090\"},{\"start\":\"55459\",\"end\":\"55464\"},{\"start\":\"55468\",\"end\":\"55471\"},{\"start\":\"55475\",\"end\":\"55481\"},{\"start\":\"55485\",\"end\":\"55490\"},{\"start\":\"55494\",\"end\":\"55500\"},{\"start\":\"55924\",\"end\":\"55929\"},{\"start\":\"56292\",\"end\":\"56298\"},{\"start\":\"56302\",\"end\":\"56309\"},{\"start\":\"56313\",\"end\":\"56320\"},{\"start\":\"56324\",\"end\":\"56330\"},{\"start\":\"56336\",\"end\":\"56342\"},{\"start\":\"56846\",\"end\":\"56848\"},{\"start\":\"57297\",\"end\":\"57302\"},{\"start\":\"57306\",\"end\":\"57311\"},{\"start\":\"57315\",\"end\":\"57321\"},{\"start\":\"57708\",\"end\":\"57713\"},{\"start\":\"58048\",\"end\":\"58055\"},{\"start\":\"58059\",\"end\":\"58064\"},{\"start\":\"58068\",\"end\":\"58076\"},{\"start\":\"58080\",\"end\":\"58086\"},{\"start\":\"58492\",\"end\":\"58497\"},{\"start\":\"58501\",\"end\":\"58508\"},{\"start\":\"58512\",\"end\":\"58520\"},{\"start\":\"58524\",\"end\":\"58531\"},{\"start\":\"58535\",\"end\":\"58545\"},{\"start\":\"58549\",\"end\":\"58555\"}]", "bib_entry": "[{\"start\":\"45900\",\"end\":\"46260\",\"attributes\":{\"matched_paper_id\":\"204982032\",\"id\":\"b0\"}},{\"start\":\"46262\",\"end\":\"46532\",\"attributes\":{\"matched_paper_id\":\"1639981\",\"id\":\"b1\"}},{\"start\":\"46534\",\"end\":\"46760\",\"attributes\":{\"matched_paper_id\":\"2930547\",\"id\":\"b2\"}},{\"start\":\"46762\",\"end\":\"47132\",\"attributes\":{\"matched_paper_id\":\"3351553\",\"id\":\"b3\"}},{\"start\":\"47134\",\"end\":\"47475\",\"attributes\":{\"matched_paper_id\":\"733980\",\"id\":\"b4\"}},{\"start\":\"47477\",\"end\":\"47840\",\"attributes\":{\"matched_paper_id\":\"197642766\",\"id\":\"b5\"}},{\"start\":\"47842\",\"end\":\"48245\",\"attributes\":{\"matched_paper_id\":\"49301394\",\"id\":\"b6\"}},{\"start\":\"48247\",\"end\":\"48649\",\"attributes\":{\"matched_paper_id\":\"21351739\",\"id\":\"b7\"}},{\"start\":\"48651\",\"end\":\"49055\",\"attributes\":{\"matched_paper_id\":\"9812826\",\"id\":\"b8\"}},{\"start\":\"49057\",\"end\":\"49376\",\"attributes\":{\"matched_paper_id\":\"60571601\",\"id\":\"b9\"}},{\"start\":\"49378\",\"end\":\"49518\",\"attributes\":{\"id\":\"b10\"}},{\"start\":\"49520\",\"end\":\"49991\",\"attributes\":{\"matched_paper_id\":\"13178535\",\"id\":\"b11\"}},{\"start\":\"49993\",\"end\":\"50322\",\"attributes\":{\"matched_paper_id\":\"10337279\",\"id\":\"b12\"}},{\"start\":\"50324\",\"end\":\"50778\",\"attributes\":{\"matched_paper_id\":\"4701912\",\"id\":\"b13\"}},{\"start\":\"50780\",\"end\":\"51050\",\"attributes\":{\"matched_paper_id\":\"10569020\",\"id\":\"b14\"}},{\"start\":\"51052\",\"end\":\"51481\",\"attributes\":{\"matched_paper_id\":\"1677864\",\"id\":\"b15\"}},{\"start\":\"51483\",\"end\":\"51883\",\"attributes\":{\"matched_paper_id\":\"12928560\",\"id\":\"b16\"}},{\"start\":\"51885\",\"end\":\"52345\",\"attributes\":{\"matched_paper_id\":\"4708051\",\"id\":\"b17\"}},{\"start\":\"52347\",\"end\":\"52819\",\"attributes\":{\"matched_paper_id\":\"3869844\",\"id\":\"b18\"}},{\"start\":\"52821\",\"end\":\"53189\",\"attributes\":{\"matched_paper_id\":\"6844431\",\"id\":\"b19\"}},{\"start\":\"53191\",\"end\":\"53457\",\"attributes\":{\"id\":\"b20\",\"doi\":\"arXiv:1804.06893\"}},{\"start\":\"53459\",\"end\":\"53562\",\"attributes\":{\"matched_paper_id\":\"110511037\",\"id\":\"b21\"}},{\"start\":\"53564\",\"end\":\"53787\",\"attributes\":{\"matched_paper_id\":\"525898\",\"id\":\"b22\"}},{\"start\":\"53789\",\"end\":\"54183\",\"attributes\":{\"matched_paper_id\":\"10530917\",\"id\":\"b23\"}},{\"start\":\"54185\",\"end\":\"54518\",\"attributes\":{\"matched_paper_id\":\"8728742\",\"id\":\"b24\"}},{\"start\":\"54520\",\"end\":\"54991\",\"attributes\":{\"matched_paper_id\":\"163164623\",\"id\":\"b25\"}},{\"start\":\"54993\",\"end\":\"55372\",\"attributes\":{\"matched_paper_id\":\"52978766\",\"id\":\"b26\"}},{\"start\":\"55374\",\"end\":\"55853\",\"attributes\":{\"matched_paper_id\":\"155106744\",\"id\":\"b27\"}},{\"start\":\"55855\",\"end\":\"56196\",\"attributes\":{\"matched_paper_id\":\"209497828\",\"id\":\"b28\"}},{\"start\":\"56198\",\"end\":\"56693\",\"attributes\":{\"matched_paper_id\":\"12008695\",\"id\":\"b29\"}},{\"start\":\"56695\",\"end\":\"57243\",\"attributes\":{\"matched_paper_id\":\"25209638\",\"id\":\"b30\"}},{\"start\":\"57245\",\"end\":\"57625\",\"attributes\":{\"matched_paper_id\":\"53235957\",\"id\":\"b31\"}},{\"start\":\"57627\",\"end\":\"57959\",\"attributes\":{\"matched_paper_id\":\"209093915\",\"id\":\"b32\"}},{\"start\":\"57961\",\"end\":\"58401\",\"attributes\":{\"matched_paper_id\":\"67872077\",\"id\":\"b33\"}},{\"start\":\"58403\",\"end\":\"59005\",\"attributes\":{\"matched_paper_id\":\"189824904\",\"id\":\"b34\"}}]", "bib_title": "[{\"start\":\"45900\",\"end\":\"45981\"},{\"start\":\"46262\",\"end\":\"46300\"},{\"start\":\"46534\",\"end\":\"46583\"},{\"start\":\"46762\",\"end\":\"46827\"},{\"start\":\"47134\",\"end\":\"47257\"},{\"start\":\"47477\",\"end\":\"47548\"},{\"start\":\"47842\",\"end\":\"47917\"},{\"start\":\"48247\",\"end\":\"48315\"},{\"start\":\"48651\",\"end\":\"48738\"},{\"start\":\"49057\",\"end\":\"49117\"},{\"start\":\"49520\",\"end\":\"49621\"},{\"start\":\"49993\",\"end\":\"50054\"},{\"start\":\"50324\",\"end\":\"50412\"},{\"start\":\"50780\",\"end\":\"50833\"},{\"start\":\"51052\",\"end\":\"51097\"},{\"start\":\"51483\",\"end\":\"51579\"},{\"start\":\"51885\",\"end\":\"51954\"},{\"start\":\"52347\",\"end\":\"52460\"},{\"start\":\"52821\",\"end\":\"52886\"},{\"start\":\"53459\",\"end\":\"53478\"},{\"start\":\"53564\",\"end\":\"53607\"},{\"start\":\"53789\",\"end\":\"53860\"},{\"start\":\"54185\",\"end\":\"54265\"},{\"start\":\"54520\",\"end\":\"54612\"},{\"start\":\"54993\",\"end\":\"55062\"},{\"start\":\"55374\",\"end\":\"55455\"},{\"start\":\"55855\",\"end\":\"55920\"},{\"start\":\"56198\",\"end\":\"56288\"},{\"start\":\"56695\",\"end\":\"56842\"},{\"start\":\"57245\",\"end\":\"57293\"},{\"start\":\"57627\",\"end\":\"57704\"},{\"start\":\"57961\",\"end\":\"58044\"},{\"start\":\"58403\",\"end\":\"58488\"}]", "bib_author": "[{\"start\":\"45983\",\"end\":\"45992\"},{\"start\":\"45992\",\"end\":\"46001\"},{\"start\":\"46001\",\"end\":\"46012\"},{\"start\":\"46012\",\"end\":\"46027\"},{\"start\":\"46302\",\"end\":\"46311\"},{\"start\":\"46311\",\"end\":\"46322\"},{\"start\":\"46322\",\"end\":\"46330\"},{\"start\":\"46330\",\"end\":\"46341\"},{\"start\":\"46341\",\"end\":\"46356\"},{\"start\":\"46585\",\"end\":\"46600\"},{\"start\":\"46829\",\"end\":\"46836\"},{\"start\":\"47259\",\"end\":\"47270\"},{\"start\":\"47550\",\"end\":\"47559\"},{\"start\":\"47919\",\"end\":\"47928\"},{\"start\":\"47928\",\"end\":\"47937\"},{\"start\":\"47937\",\"end\":\"47945\"},{\"start\":\"47945\",\"end\":\"47955\"},{\"start\":\"48317\",\"end\":\"48326\"},{\"start\":\"48326\",\"end\":\"48334\"},{\"start\":\"48334\",\"end\":\"48344\"},{\"start\":\"48344\",\"end\":\"48354\"},{\"start\":\"48740\",\"end\":\"48750\"},{\"start\":\"48750\",\"end\":\"48761\"},{\"start\":\"48761\",\"end\":\"48773\"},{\"start\":\"49119\",\"end\":\"49130\"},{\"start\":\"49130\",\"end\":\"49146\"},{\"start\":\"49146\",\"end\":\"49155\"},{\"start\":\"49623\",\"end\":\"49634\"},{\"start\":\"49634\",\"end\":\"49642\"},{\"start\":\"49642\",\"end\":\"49651\"},{\"start\":\"49651\",\"end\":\"49660\"},{\"start\":\"49660\",\"end\":\"49677\"},{\"start\":\"50056\",\"end\":\"50065\"},{\"start\":\"50065\",\"end\":\"50077\"},{\"start\":\"50414\",\"end\":\"50421\"},{\"start\":\"50421\",\"end\":\"50430\"},{\"start\":\"50430\",\"end\":\"50440\"},{\"start\":\"50835\",\"end\":\"50845\"},{\"start\":\"51099\",\"end\":\"51108\"},{\"start\":\"51108\",\"end\":\"51118\"},{\"start\":\"51118\",\"end\":\"51126\"},{\"start\":\"51126\",\"end\":\"51136\"},{\"start\":\"51136\",\"end\":\"51148\"},{\"start\":\"51581\",\"end\":\"51591\"},{\"start\":\"51591\",\"end\":\"51605\"},{\"start\":\"51605\",\"end\":\"51616\"},{\"start\":\"51616\",\"end\":\"51630\"},{\"start\":\"51630\",\"end\":\"51642\"},{\"start\":\"51956\",\"end\":\"51965\"},{\"start\":\"51965\",\"end\":\"51975\"},{\"start\":\"51975\",\"end\":\"51985\"},{\"start\":\"51985\",\"end\":\"51995\"},{\"start\":\"52462\",\"end\":\"52470\"},{\"start\":\"52888\",\"end\":\"52902\"},{\"start\":\"52902\",\"end\":\"52914\"},{\"start\":\"52914\",\"end\":\"52928\"},{\"start\":\"52928\",\"end\":\"52941\"},{\"start\":\"52941\",\"end\":\"52958\"},{\"start\":\"53246\",\"end\":\"53255\"},{\"start\":\"53255\",\"end\":\"53266\"},{\"start\":\"53266\",\"end\":\"53275\"},{\"start\":\"53275\",\"end\":\"53285\"},{\"start\":\"53480\",\"end\":\"53489\"},{\"start\":\"53609\",\"end\":\"53619\"},{\"start\":\"53862\",\"end\":\"53874\"},{\"start\":\"54267\",\"end\":\"54278\"},{\"start\":\"54278\",\"end\":\"54287\"},{\"start\":\"54287\",\"end\":\"54300\"},{\"start\":\"54614\",\"end\":\"54623\"},{\"start\":\"54623\",\"end\":\"54633\"},{\"start\":\"54633\",\"end\":\"54645\"},{\"start\":\"54645\",\"end\":\"54652\"},{\"start\":\"54652\",\"end\":\"54660\"},{\"start\":\"54660\",\"end\":\"54670\"},{\"start\":\"55064\",\"end\":\"55071\"},{\"start\":\"55071\",\"end\":\"55080\"},{\"start\":\"55080\",\"end\":\"55092\"},{\"start\":\"55457\",\"end\":\"55466\"},{\"start\":\"55466\",\"end\":\"55473\"},{\"start\":\"55473\",\"end\":\"55483\"},{\"start\":\"55483\",\"end\":\"55492\"},{\"start\":\"55492\",\"end\":\"55502\"},{\"start\":\"55922\",\"end\":\"55931\"},{\"start\":\"56290\",\"end\":\"56300\"},{\"start\":\"56300\",\"end\":\"56311\"},{\"start\":\"56311\",\"end\":\"56322\"},{\"start\":\"56322\",\"end\":\"56332\"},{\"start\":\"56332\",\"end\":\"56344\"},{\"start\":\"56844\",\"end\":\"56850\"},{\"start\":\"57295\",\"end\":\"57304\"},{\"start\":\"57304\",\"end\":\"57313\"},{\"start\":\"57313\",\"end\":\"57323\"},{\"start\":\"57706\",\"end\":\"57715\"},{\"start\":\"58046\",\"end\":\"58057\"},{\"start\":\"58057\",\"end\":\"58066\"},{\"start\":\"58066\",\"end\":\"58078\"},{\"start\":\"58078\",\"end\":\"58088\"},{\"start\":\"58490\",\"end\":\"58499\"},{\"start\":\"58499\",\"end\":\"58510\"},{\"start\":\"58510\",\"end\":\"58522\"},{\"start\":\"58522\",\"end\":\"58533\"},{\"start\":\"58533\",\"end\":\"58547\"},{\"start\":\"58547\",\"end\":\"58557\"}]", "bib_venue": "[{\"start\":\"46027\",\"end\":\"46053\"},{\"start\":\"46356\",\"end\":\"46360\"},{\"start\":\"46600\",\"end\":\"46619\"},{\"start\":\"46836\",\"end\":\"46889\"},{\"start\":\"47270\",\"end\":\"47282\"},{\"start\":\"47559\",\"end\":\"47607\"},{\"start\":\"47955\",\"end\":\"47990\"},{\"start\":\"48354\",\"end\":\"48395\"},{\"start\":\"48773\",\"end\":\"48816\"},{\"start\":\"49155\",\"end\":\"49176\"},{\"start\":\"49378\",\"end\":\"49409\"},{\"start\":\"49677\",\"end\":\"49719\"},{\"start\":\"50077\",\"end\":\"50122\"},{\"start\":\"50440\",\"end\":\"50494\"},{\"start\":\"50845\",\"end\":\"50886\"},{\"start\":\"51148\",\"end\":\"51206\"},{\"start\":\"51642\",\"end\":\"51659\"},{\"start\":\"51995\",\"end\":\"52052\"},{\"start\":\"52470\",\"end\":\"52520\"},{\"start\":\"52958\",\"end\":\"52977\"},{\"start\":\"53191\",\"end\":\"53244\"},{\"start\":\"53489\",\"end\":\"53500\"},{\"start\":\"53619\",\"end\":\"53634\"},{\"start\":\"53874\",\"end\":\"53927\"},{\"start\":\"54300\",\"end\":\"54321\"},{\"start\":\"54670\",\"end\":\"54705\"},{\"start\":\"55092\",\"end\":\"55128\"},{\"start\":\"55502\",\"end\":\"55555\"},{\"start\":\"55931\",\"end\":\"55985\"},{\"start\":\"56344\",\"end\":\"56390\"},{\"start\":\"56850\",\"end\":\"56897\"},{\"start\":\"57323\",\"end\":\"57377\"},{\"start\":\"57715\",\"end\":\"57769\"},{\"start\":\"58088\",\"end\":\"58141\"},{\"start\":\"58557\",\"end\":\"58632\"},{\"start\":\"46362\",\"end\":\"46366\"},{\"start\":\"46891\",\"end\":\"46955\"},{\"start\":\"47609\",\"end\":\"47663\"},{\"start\":\"47992\",\"end\":\"48043\"},{\"start\":\"48397\",\"end\":\"48451\"},{\"start\":\"48818\",\"end\":\"48855\"},{\"start\":\"49178\",\"end\":\"49193\"},{\"start\":\"49721\",\"end\":\"49757\"},{\"start\":\"50124\",\"end\":\"50163\"},{\"start\":\"50496\",\"end\":\"50559\"},{\"start\":\"51208\",\"end\":\"51275\"},{\"start\":\"52054\",\"end\":\"52123\"},{\"start\":\"52522\",\"end\":\"52588\"},{\"start\":\"53636\",\"end\":\"53645\"},{\"start\":\"53929\",\"end\":\"53993\"},{\"start\":\"54707\",\"end\":\"54754\"},{\"start\":\"55130\",\"end\":\"55182\"},{\"start\":\"55557\",\"end\":\"55619\"},{\"start\":\"55987\",\"end\":\"56035\"},{\"start\":\"56392\",\"end\":\"56450\"},{\"start\":\"56899\",\"end\":\"56962\"},{\"start\":\"57379\",\"end\":\"57445\"},{\"start\":\"58143\",\"end\":\"58190\"},{\"start\":\"58634\",\"end\":\"58721\"}]"}}}, "year": 2023, "month": 12, "day": 17}