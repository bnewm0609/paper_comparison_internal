{"id": 258999941, "updated": "2023-10-09 06:30:22.604", "metadata": {"title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration", "authors": "[{\"first\":\"Ji\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Jiaming\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Haotian\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Shang\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Xingyu\",\"last\":\"Dang\",\"middle\":[]},{\"first\":\"Chuang\",\"last\":\"Gan\",\"middle\":[]},{\"first\":\"Song\",\"last\":\"Han\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth). In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks. Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement an efficient and flexible inference framework tailored for LLMs on the edge, offering more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPU (NVIDIA Jetson Orin 64GB).", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.00978", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-00978", "doi": "10.48550/arxiv.2306.00978"}}, "content": {"source": {"pdf_hash": "2c5ab7d87e3342d2dba7d1d113ca1b16c545e344", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.00978v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d6967ab3766b7715233fa7447e668009e72d80a9", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/2c5ab7d87e3342d2dba7d1d113ca1b16c545e344.txt", "contents": "\nAWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\n3 Oct 2023\n\nJi Lin \nTsinghua University\n\n\nJiaming Tang \nTsinghua University\n\n\nHaotian Tang \nTsinghua University\n\n\nShang Yang \nTsinghua University\n\n\nXingyu Dang \nTsinghua University\n\n\nChuang Gan \nTsinghua University\n\n\nSong Han \nTsinghua University\n\n\nSjtu \nTsinghua University\n\n\nAWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\n3 Oct 2023C07B66DE6692784E7448248606BA548CarXiv:2306.00978v2[cs.CL]\nLarge language models (LLMs) have shown excellent performance on various tasks, but the astronomical model size raises the hardware barrier for serving (memory size) and slows down token generation (memory bandwidth).In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization.Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error.We then propose to search for the optimal perchannel scaling that protects the salient weights by observing the activation, not weights.AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set.AWQ outperforms existing work on various language modeling and domain-specific benchmarks.Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs.Alongside AWQ, we implement an efficient and flexible inference framework tailored for LLMs on the edge, offering more than 3\u00d7 speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs.It also democratizes the deployment of the 70B Llama-2 model on mobile GPU (NVIDIA Jetson Orin 64GB).* indicates equal contributions.Preprint.Under review.\n\nIntroduction\n\nLarge language models (LLMs) based on transformers [40] have shown excellent performance on various benchmarks [4,49,38,34].However, the large model size leads to the high serving costs.For example, GPT-3 has 175B parameters, which is 350GB in FP16, while the latest H100 GPU only has 96GB memory, let alone edge devices.\n\nLow-bit weight quantization for LLMs can save memory but is hard.Quantization-aware training (QAT) is not practical due to the high training cost, while post-training quantization (PTQ) suffers from large accuracy degradation under a low-bit setting.The closest work is GPTQ [14], which uses second-order information to perform error compensation.It may over-fit the calibration set during reconstruction, distorting the learned features on out-of-distribution domains (Figure 6), which could be problematic since LLMs are generalist models.\n\nIn this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly low-bit weight-only quantization method for LLMs.Our method is based on the observation that weights are not equally important for LLMs' performance.There is a small fraction (0.1%-1%) of salient weights; skipping the quantization of these salient weights will significantly reduce the quantization loss (Table 1).To find the salient weight channels, the insight is that we should refer to the activation distribution instead of the weight distribution, despite we are doing weight-only quantization: weight channels corresponding to larger activation magnitudes are more salient since they process more Figure 1.We observe that we can find 1% of the salient weights in LLMs by observing the activation distribution (middle).Keeping the salient weights in FP16 can significantly improve the quantized performance (PPL from 43.2 (left) to 13.0 (middle)), but the mixed-precision format is not hardware-efficient.We follow the activationawareness principle and propose AWQ (right).AWQ performs per-channel scaling to protect the salient weights, leading to reduced quantized error.PPL is measured with OPT-6.7Bunder INT3-g128 quantization.\n\nimportant features.To avoid the hardware-inefficient mixed-precision implementation, we analyze the error from weight quantization and derive that scaling up the salient channels can reduce their relative quantization error (Equation 2).Following the intuition, we designed a per-channel scaling method to automatically search for the optimal scaling that minimizes the quantization error under full-weight quantization.AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on various domains and modalities without overfitting to the calibration set.Furthermore, we implemented an efficient serving framework to convert theoretical memory savings from AWQ to practical speedup.Our framework takes advantage of kernel fusion to minimize the inference overhead (e.g., intermediate DRAM access and kernel launch overhead), so that we can better realize the speed up from quantizing linear layers (AWQ is applied to linear layers which consist most of the parameters).\n\nExperiments show that AWQ outperforms existing work on various tasks for different model families (e.g., LLaMA [38], OPT [49]) and model sizes.Thanks to better generalization, it also achieves good quantization performance for instruction-tuned LMs (e.g., Vicuna) and, for the first time, multi-modal LMs (OpenFlamingo [2]).With our efficient system implementation, we consistently observe a 3.2-3.3\u00d7average speedup compared to the FP16 implementation by Huggingface across a diverse spectrum of LLMs.Furthermore, it facilitates effortless deployment of the Llama-2-70B model on a single NVIDIA Jetson Orin with 64GB of memory.It also democratizes LLMs with up to 13 billion parameters at an interactive pace of 30 tokens per second on a laptop RTX 4070 GPU with only 8GB of memory.\n\nAWQ has been widely adopted by various open-source LLM serving solutions including FastChat, vLLM, HuggingFace TGI, LMDeploy, etc.\n\n\nAWQ: Activation-aware Weight Quantization\n\nQuantization maps a floating-point number into lower-bit integers.It is an effective method to reduce the model size and inference costs of LLMs [9,14,47,46].In this section, we first propose a weight-only quantization method to improve accuracy without training/regression by protecting more \"important\" weights.And then develop a data-driven method to search for the optimal scaling that reduces quantization errors (Figure 1).\n\n\nImproving LLM Quantization by Preserving 1% Salient Weights\n\nWe observe that the weights of LLMs are not equally important: there is a small fraction of salient weights that are much more important for LLMs' performance compared to others.Skipping the quantization of these salient weights can help bridge the performance degradation due to the quantization loss without any training or regression (Figure 1(b)).To verify the idea, we benchmark the performance of quantized LLMs when skipping part of the weight channels in Table 1.We measured the performance of INT3 quantized models while keeping some ratios of weight channels in FP16.A widely used method to determine the importance of weights is to look at its magnitude or L 2 -norm [18,13].But we find skipping the weight channels with large norm (i.e., FP16% (based on W)) does not significantly improve the quantized performance, leading to a similar marginal improvement as random selection.Interestingly, selecting weights based on activation magnitude can significantly improve the performance: keeping only 0.1%-1% of the channels corresponding to larger activation significantly improves the quantized performance, even matching a strong reconstructionbased method GPTQ [14].We hypothesize that the input features with larger magnitudes are generally more important.Keeping the corresponding weights in FP16 can preserve those features, which contributes to better model performance.\n\nLimitations: Despite keeping 0.1% of weights in FP16 can improve the quantized performance without a noticeable increase in model size (measured in total bits), such a mixed-precision data type will make the system implementation difficult.We need to come up with a method to protect the important weights without actually keeping them as FP16.\n\n\nProtecting Salient Weights by Activation-aware Scaling\n\nWe propose an alternative method to reduce the quantization error of the salient weight by per-channel scaling, which does not suffer from the hardware inefficiency issue.\n\nAnalyzing the quantization error.We start by analyzing the error from weight-only quantization.Consider a group/block of weight w; the linear operation can be written as y = wx, and the quantized counterpart is y = Q(w)x.Specifically, the quantization function is defined as:\nQ(w) = \u2206 \u2022 Round( w \u2206 ), \u2206 = max(|w|) 2 N \u22121 ,(1)\nwhere N is the number of quantization bits, and \u2206 is the quantization scaler determined by the absolute maximum value.Now consider a weight element w \u2208 w, if we multiply w with s > 1 and the inversely scale x, we will have Q(w \u2022 s)(x/s), which is:\nQ(w \u2022 s) \u2022 x s = \u2206 \u2032 \u2022 Round( ws \u2206 ) \u2022 x \u2022 1 s ,(2)\nwhere \u2206 \u2032 is the new quantization scaler after applying s.We empirically find that: (1) The expected error from Round(\u2022) (denoted as RoundErr) does not vary: since the round function maps a floatingpoint number to an integer, the error is roughly uniformly distributed from 0-0.5, resulting in an average error of 0.25; (2) Scaling up a single element w usually does not change the extreme value from the group w.Therefore we have \u2206 \u2032 \u2248 \u2206;\n\n(3) The error from equation 2 can be expressed as\nErr \u2032 = \u2206 \u2032 \u2022 RoundErr \u2022 1 s , the ratio compared to the original error RoundErr is \u2206 \u2032 \u2206 \u2022 1 s . Given \u2206\n\u2032 \u2248 \u2206 and s > 1, the relative error is smaller for the salient weight w.\n\nTo verify the idea, we multiply the 1% salient channels with s > 1 for the OPT-6.7Bmodel, and measure the change in \u2206 for each group in Table 2.We find that scaling up the salient channels is quite effective: the perplexity improves from 23.54 for s = 1 (simply RTN) to 11.92 for s = 2.\n\nAs s goes larger, the percentage of changed \u2206 generally gets larger, but the proportion is still quite small for s < 2; the relative error for the salient channels continues to go smaller as s increases.Nonetheless, the best PPL actually appears at s = 2.This is because if we use a very large s, it will increase the relative error for the non-salient channels when \u2206 increases (the error of non-salient channels will be amplified by \u2206 \u2032 \u2206 , and the ratio is larger than 1 for 21.2% of the channels under s = 4), which can damage the model's overall accuracy.Therefore, we need to also consider the error from the non-salient channels when protecting salient ones.\n\nSearching to scale.To consider both salient and non-salient weights, we choose to automatically search for an optimal (per input channel) scaling factor that minimizes the output difference after OPT-6.7Bquantization for a certain layer.Formally, we want to optimize the following objective:\ns = 1 s = 1.25 s = 1.5 s = 2 s = 4 proportion of \u2206 \u2032 \u0338 = \u2206 0% 2.s * = arg min s L(s), L(s) = \u2225Q(W \u2022 s)(s \u22121 \u2022 X) \u2212 WX\u2225(3)\nHere Q means the weight quantization function (e.g., INT3/INT4 quantization with group size 128), W is the original weights in FP16, and X is the input features cached from a small calibration set (we take a small calibration set from he pre-training dataset in order not to overfit to a specific task).s is a per-(input) channel scaling factor; for s \u22121 \u2022 X, it can usually be fused into the previous operator [44,46].Since the quantization function is not differentiable, we are not able to directly optimize the problem with vanilla backpropagation.There are some techniques relying on approximated gradients [3,12], which we found still suffers from unstable convergence.\n\nTo make the process more stable, we define a search space for the optimal scale by analyzing the factors that will affect the choice of scaling factor.As shown in the last section, the saliency of weight channels is actually determined by the activation scale (thus \"activation-awareness\").Therefore, we simply use a very simple search space:\ns = s X \u03b1 , \u03b1 * = arg min \u03b1 L(s X \u03b1 )(4)\ns is only related to the magnitude of activation s X , and we use a single hyper-parameter \u03b1 to balance between the protection of salient and non-salient channels.We can find the best \u03b1 by a fast grid search over the interval of [0, 1] (0 means we do not scale; 1 corresponds to the most aggressive scaling).We further apply weight clipping also by minimizing the MSE error, since clipping the weights can further help to reduce \u2206 \u2032 in Equation 2; thus reducing quantization error.We provide an ablation study on OPT models under INT3-g128 quantization in Table 3; AWQ consistently outperforms round-to-nearest quantization (RTN) and achieves comparable performance as mixed-precision (1% FP16) while being more hardware-friendly.\n\nAdvantages.Our method does not rely on any regression [14] or backpropagation, which is required by many quantization-aware training methods.It has minimal reliance on the calibration set since we only measure the average magnitude per channel, thus preventing over-fitting (Figure 6).Therefore, our method requires fewer data for the quantization process and can preserve LLMs' knowledge outside of the calibration set's distribution.See Section 3.3 for more details.3 Experiments\n\n\nSettings\n\nQuantization.We focus on weight-only grouped quantization in this work.As shown in previous work [10,14], grouped quantization is always helpful for improving performance/model size tradeoff.We used a group size of 128 throughout the work, except otherwise specified.We focus on INT4/INT3 quantization since they are able to mostly preserve the LLMs' performance [10].For AWQ, we used a small calibration set from the Pile [15] dataset in order not to overfit to a specific downstream domain.We used a grid size of 20 to search for the optimal \u03b1 in Equation 4.\n\nModels.We benchmarked our method on LLaMA [38] and OPT [49] families.There are other open LLMs like BLOOM [34], but they are generally worse in quality, so we do not include them in our study.We further benchmark an instruction-tuned model Vicuna [6] and visual language models OpenFlamingo-9B [2] and LLaVA-13B [26] to demonstrate the generability of our method.\n\nEvaluations.Following previous literature [9,46,14,10,47], we mainly profiled the quantized models on language modeling tasks (perplexity evaluation on WikiText-2 [27]) since perplexity can stably reflect the LLM's performance [10].\n\nBaselines.Our primary baseline is vanilla round-to-nearest quantization (RTN).It is actually quite strong when using a small group size like 128 [14,10].We also compare with a state-of-the-art method GPTQ [14] for LLM weight quantization.For GPTQ, we also compare with an updated version that uses a \"reorder\" trick (denoted as GPTQ-Reorder or GPTQ-R).Other techniques like ZeroQuant [47], AdaRound [28], and BRECQ [23] rely on backpropagation to update the quantized weights, which may not easily scale up to large model sizes; they also do not outperform GPTQ [14], thus not included for study.\n\n\nEvaluation\n\nResults on LLaMA models.We focus our study on LLaMA models (LLaMA [38] and Llama-2 [39]) due to their superior performance compared to other open-source LLMs [49,34]; it is also the foundation of many popular open-source models [36,6].We evaluate the perplexity before and after quantization in Table 4.We can see that AWQ consistently outperforms round-to-nearest (RTN) and GPTQ [14] (w/ and w/o reordering) across different model scales (7B-70B) and generations.\n\nQuantization of instruction-tuned models.Instruction tuning can significantly improve the models' performance and usability [42,33,31,8].It has become an essential procedure before model deployment.We further benchmark our method's performance on a popular instructiontuned model Vicuna [6] in Figure 2. We used the GPT-4 score to evaluate the quantized models' performance against the FP16 counterpart on 80 sample questions [6].We compare the responses with both orders (quantized-FP16, FP16-quantized) to get rid of the ordering effect (we found GPT-4 tends to increase the rating of the first input), leading to 160 trials.AWQ consistently improves Comparing INT3-g128 quantized Vicuna models with FP16 counterparts under GPT-4 evaluation protocol [6].More winning cases (in blue) indicate better performance.AWQ consistently improves the quantized performance compared to RTN and GPTQ [14], showing generalization to instruction-tuned models.the INT3-g128 quantized Vicuna models over RTN and GPTQ under both scales (7B and 13B), demonstrating the generability to instruction-tuned models.\n\n\nCOCO (CIDEr\n\nQuantization of multi-modal language models.Large multi-modal models (LMMs) or visual language models (VLMs) are LLMs augmented with vision inputs [1,22,21,11,48,26].Such models are able to perform text generation conditioned on image/video inputs.Since our method does not have the overfitting issue to the calibration set, it can be directly applied to VLMs to provide accurate and efficient quantization.We perform experiments with the OpenFlamingo-9B model [2] (an open-source reproduction of [1]) on COCO captioning [5] dataset (Table 5).We measured the average performance of 5k samples under different few-shot settings.We only quantize the language part of the model since it dominates the model size.AWQ outperforms existing methods under zero-shot and various few-shot settings, demonstrating the generability to different modalities and in-context learning workloads.It reduces the quantization degradation (32-shot) from 4.57 to 1.17 under INT4-g128, providing 4\u00d7 model size reduction with negligible performance loss.We further provide some qualitative captioning results in Figure 3 to show our advantage over RTN.Our method provides a push-the-button solution for LMM/VLM quantization.It is the first study of VLM low-bit quantization to the best of our knowledge.\n\nVisual reasoning results.We further provide some qualitative visual reasoning examples of the LLaVA-13B [26] model in Figure 4. AWQ improves the responses compared to the round-to-nearest (RTN) baseline for INT4-g128 quantization, leading to more reasonable answers.In this first example, the AWQ model can understand the meme as it resembles the Earth when looking from space, while RTN produces wrong descriptions (marked in red).In the second example, AWQ correctly answers the question (the artist of the painting), while RTN does not provide any information about the artist.In the last example, RTN falsely points out a bird in the picture, while AWQ provides more information by noticing the image is taken in a mountain area.AWQ improves the visual reasoning ability of VLMs by reducing factual errors in the responses; RTN is not good enough even for 4 bits.\n\nExtreme low-bit quantization.We further quantize LLM to INT2 to accommodate limited device memory (Table 6).RTN completely fails, and AWQ brings significant perplexity improvement on top of GPTQ, though there is still a performance gap compared to FP16.Our method is orthogonal to GPTQ.We can combine our method with GPTQ to further improve the INT2 quantization performance, making it a more practical setting.\n\n\nW4-RTN:\n\nA man and a dog walking past some bushes.\n\n\nW4-AWQ:\n\nTwo dogs are walking on the street.\n\n\nW4-RTN:\n\nA man is holding a baby elephant in his arms.\n\n\nW4-AWQ:\n\nA man and his daughter pose with an elephant.\n\n\nW4-RTN:\n\nA model airplane flying in the sky.\n\n\nW4-AWQ:\n\nTwo toy airplanes sit on a grass field.The painting depicts a woman, who some believe to be a representation of the famous artist, Mona Lisa.However, it is not certain that the woman in the painting is indeed Mona Lisa.The artwork is a popular subject of artistic analysis and admiration, often considered a symbol of feminine beauty and enigma.The portrait is characterized by its intricate details, captivating the viewer with the woman's expressive gaze and the subtle, enigmatic smile that has become synonymous with the Mona Lisa.\n\n\nINT4-g128 AWQ:\n\nThe painting depicts a woman, who is believed to be a portrait of Mona Lisa, the famous artwork by the artist Leonardo da Vinci.Although the painting is widely recognized as a representation of Mona Lisa, it is not possible to confirm the artist who drew this particular painting without additional information.Leonardo da Vinci was a renowned artist known for his exceptional skill in painting detailed, realistic portraits, and the Mona Lisa is one of his most famous works.\n\nQuestion: What are the things I should be cautious about when I visit here?INT4-g128 RTN: When visiting this location, one should be cautious about the wooden pier extending out over the water.\n\nThe wooden planks may be slippery, especially if they are wet or if there is any movement of water underneath.It is important to walk carefully and maintain balance while on the pier to avoid any accidents or falls into the water.\n\nAdditionally, be mindful of the surrounding environment, as there might be wildlife or natural elements that could pose a risk.For example, there is a bird in the image, which may indicate the presence of more wildlife in the area.Considering these factors will help ensure a safe and enjoyable experience while visiting this picturesque location.\n\n\nINT4-g128 AWQ:\n\nWhen visiting this scenic location with a wooden pier extending over a large body of water, surrounded by mountains, it is essential to be cautious about a few things.Firstly, be mindful of the slippery wooden surface of the pier, especially when it is wet or in a snowy environment, as it could pose a risk of slipping and falling into the water.Secondly, pay attention to the weather conditions, as they can change rapidly in mountainous areas, potentially affecting visibility and making it difficult to return to the shore safely.Lastly, be respectful of the natural environment and any wildlife that may be present in the area.Disturbing the ecosystem or littering can have negative consequences on the local flora and fauna.By following these precautions, you can ensure a safe and enjoyable visit to this picturesque location.\n\n\nFigure 4.\n\nVisual reasoning examples from LLaVA-13B model [26].AWQ improves over the round-to-nearest (RTN) baseline, providing more reasonable answers.We color the text to show the correct or wrong responses.\n\nSpeedup Evaluation.In Figure 5, we demonstrate the system acceleration results for AWQ.We optimize both linear layers and layers that do not have quantized weights.We conduct benchmarking experiments on RTX 4090 (desktop GPU), RTX 4070 (laptop GPU) and Jetson Orin (mobile GPU).We perform batch size = 1 inference for all LLMs using a fixed prompt length of 4 tokens.We generate 200 tokens for each inference run and calculate the median latency as the final result.As in Figure 5(a), our system brings 2.7-3.9\u00d7speedup to three families of LLMs (Llama-2, MPT and Falcon) on 4090 compared with the Huggingface FP16 implementation.Notably, on the laptop 4070 GPU with only 8GB memory, we are still able to run Llama-2-13B models at 33 tokens / second, while the FP16 implementation cannot fit 7B models.\n\nOur system also exhibits promising performance on the NVIDIA Jetson Orin (32GB).As shown in Figure 5(b), our system achieves an interactive processing rate of 33 tokens per second when running Llama-2 models.Thanks to AWQ, even larger models such as MPT-30B can operate smoothly on this resource-constrained edge device, delivering a processing speed of 7.8 tokens per second.It's worth noting that we implement the forward pass for all AWQ models using native PyTorch APIs, and this code is reused across various GPU architectures.Consequently, our system provides the best of both worlds: state-of-the-art inference speed and exceptional extensibility.  .AWQ provides a turn-key solution to transform the theoretical memory footprint reduction into a quantifiable speedup.As a result, AWQ is up to 3.9\u00d7 and 3.5\u00d7 faster than the FP16 implementation from Huggingface on 4090 (desktop GPU) and Orin (mobile GPU), respectively.AWQ also democratizes Llama-2-13B deployment on laptop GPUs (4070) with merely 8GB memory.\n\n\nAnalysis\n\nBetter data-efficiency for the calibration set.Our method requires a smaller calibration set since we do not rely on regression/backpropagation; we only measure the average activation scale from the calibration set, which is data-efficient.To demonstrate the idea, we compare the perplexity of the OPT-6.7Bmodel with INT3-g128 quantization in Figure 6 (a).AWQ needs a much smaller calibration to reach a good quantized performance; it can achieve better perplexity using 10\u00d7 smaller calibration set compared to GPTQ (16 sequences v.s.192 sequences).\n\nRobust to the calibration set distributions.Our method is less sensitive to the calibration set distribution since we only measure the average activation scale from the calibration set, which is more generalizable across different dataset distributions.We further benchmarked the effect of the different calibration set distributions in Figure 6(b).We took two subsets from the Pile dataset [15]: PubMed Abstracts and Enron Emails [20].We use each of the subsets as the calibration set and evaluate the quantized model on both sets (the calibration and evaluation sets are split with no overlapping; we used 1k samples for evaluation).Overall, using the same calibration and evaluation distribution works the best (PubMed-PubMed, Enron-Enron).But when using a different calibration distribution (PubMed-Enron, Enron-PubMed), AWQ only increases the perplexity by 0.5-0.6,while GPTQ has 2.3-4.9 worse perplexity.This demonstrates the robustness of AWQ to the calibration set distribution.\n\n\nRelated Work\n\nModel quantization methods.Quantization reduces the bit-precision of deep learning models [17,19,29,41,28,25], which helps to reduce the model size and accelerate inference.Quantization techniques generally fall into two categories: quantization-aware training (QAT, which relies on backpropagation to update the quantized weights) [3,16,30,7] and post-training quantization [19,29,28] (PTQ, usually training-free).The QAT methods cannot easily scale up to large models like LLMs.Therefore, people usually use PTQ methods to quantize LLMs.\n\nQuantization of LLMs.People study two settings for LLM quantization: (1) W8A8 quantization, where both activation and weights are quantized to INT8 [9,46,47,45,43]; (2) Low-bit weight-only quantization (e.g., W4A16), where only weights are quantized into low-bit integers [14,10,35,32].We focus on the second setting in this work since it not only reduces the hardware barrier (requiring a smaller memory size) but also speeds up the token generation (remedies memory-bound workload).\n\nApart from the vanilla round-to-nearest baseline (RTN), GPTQ [14] is the closest to our work.However, the reconstruction process of GPTQ leads to an over-fitting issue to the calibration set and may not preserve the generalist abilities of LLMs for other modalities and domains.It also requires a reordering trick to work for some models (e.g., LLaMA-7B [38] and OPT-66B [49]).\n\nSystem support for low-bit quantized LLMs.Low-bit quantized LLMs have been a popular setting to reduce inference There are some system supports to achieve a practical speed-up.GPTQ [14] provides INT3 kernels for OPT models and GPTQ-for-LLaMA extends kernel support for INT4 reordered quantization with the help of Triton [37].FlexGen [35] and llama.cpp* perform group-wise INT4 quantization to reduce I/O costs and offloading.FasterTransformer \u2020 implements FP16\u00d7INT4 GEMM for weight-only per-tensor quantization but does not support group quantization.LUT-GEMM [32] performs bitwise computation on GPU CUDA cores with the help of lookup tables.AWQ kernels are adaptively executed on both tensor cores and CUDA cores, suitable for both context and generation phases in LLM inference.Consequently, we run state-of-the-art LLaMA models with 3.2-3.3\u00d7speedup over the FP16 implementation from Huggingface.\n\n\nConclusion\n\nIn this work, we propose Activation-aware Weight Quantization (AWQ), a simple yet effective method for low-bit weight-only LLM compression AWQ is based on the observation that weights are not equally important in LLMs and performs per-channel scaling to reduce the quantization loss of salient weights.AWQ does not over-fit the calibration set and preserves the generalist abilities of LLMs in various domains and modalities.It outperforms existing work on language modeling and can be applicable to instruction-tuned LMs and multi-modal LMs.Our system implementation further translates the theoretical memory savings achieved by AWQ into 3.2-3.3\u00d7measured speedups over the FP16 implementations from Huggingface on desktop and mobile GPUs, democratizing LLM deployment on the edge.\n\n\nA Broader Impacts and Limitations\n\nBroader impacts.In this paper, we propose a general technique to enable accurate and efficient low-bit weight-only quantization of large language models (LLMs).It makes LLMs more efficient and accessible and thus may inherit the impacts of LLMs.On the positive side, quantization helps to democratize LLMs, which helps to benefit more people (especially those with lower income).It reduces the costs and hardware barrier of deploying LLMs and facilitates edge inference of these models, addressing the data privacy issue (since we no longer need to send data to the cloud).On the negative side, LLMs may be exploited by malicious users to produce misinformation and manipulation.Quantization can not prevent such negative effects but it does not make it worse.\n\nLimitations.In this paper, we follow previous work [9,14,46,47,10] to mostly benchmark the quantized models on standard accuracy metrics like perplexity and accuracy.However, besides accuracy, there are other important metrics for LLM benchmark like robustness, fairness, bias, toxicity, helpfulness, calibration, etc. [24].\n\nWe think it would be helpful to perform a more holistic evaluation of quantized LLMs covering these aspects, which we leave to future work.Furthermore, we only study low-bit integer quantization of LLMs due to easier data type casting on hardware.There might be a further improvement from changing data types (e.g., FP4 [10]), which we do not include in the study.\n\n\nB Amount of Computation\n\nWe study the post-training quantization (PTQ) of LLMs in this work.The computation requirement is generally modest since we do not rely on any backpropagation.We used one NVIDIA A100 GPU for smaller models (<40B parameters) and 2-4 A100 GPUs for larger models due to memory limits.\n\nThe quantization process is generally fast, requiring a few GPU hours (ranging from 0.1 to 3, depending on the model size).The accuracy measurement time depends on the model and dataset sizes: testing LLaMA-65B (the biggest model we tested on multiple datasets) on 4 common sense QA tasks requires 3 GPU hours; testing it on MMLU (consisting of 57 sub-datasets) requires 5 GPU hours.The GPU hours would be smaller for smaller models and datasets (e.g., WikiText-2).\n\n\nC Limitation with No-group Quantization\n\nOur method searches for good scaling to protect the salient weight channels.It works pretty well under grouped quantization, matching the same accuracy as keeping salient weights in FP16 (Figure 1).However, such a scaling-based method can only protect one salient channel for each group.It is not a problem for grouped quantization (we only need to protect 0.1%-1% of salient channels, the group size is usually small, like 128, so we need to protect fewer than 1 channel in each group on average).But for no-group quantization, we can only protect one input channel for the entire weight, which may not be enough to bridge the performance degradation.As shown in\n\nFigure 2 .\n2\nFigure2.Comparing INT3-g128 quantized Vicuna models with FP16 counterparts under GPT-4 evaluation protocol[6].More winning cases (in blue) indicate better performance.AWQ consistently improves the quantized performance compared to RTN and GPTQ[14], showing generalization to instruction-tuned models.\n\n\nFigure 3 .\n3\nFigure 3. Qualitative results of quantized OpenFlamingo-9B [2] on COCO captioning dataset (4-shot, INT4-g128 quantization).Our method significantly improves the captioning quality compared to the round-to-nearest (RTN) baseline.We color the text to show the correct or wrong captions.\n\n\nFigure 5\n5\nFigure5.AWQ provides a turn-key solution to transform the theoretical memory footprint reduction into a quantifiable speedup.As a result, AWQ is up to 3.9\u00d7 and 3.5\u00d7 faster than the FP16 implementation from Huggingface on 4090 (desktop GPU) and Orin (mobile GPU), respectively.AWQ also democratizes Llama-2-13B deployment on laptop GPUs (4070) with merely 8GB memory.\n\n\nTable 2 .\n2\nStatistics when multiplying the 1% salient channels by s > 1. Scaling up the salient channels significantly improves the perplexity (23.54 to 11.92).As s goes larger, the percentage of changed \u2206 increases, and the error reduction rate for salient channels also increases.However, the best perplexity is achieved at s = 2, since further increasing s will increase the quantization error for non-salient channels.\n8%4.4%8.2% 21.2%\n\nTable 3 .\n3\nAWQ\n\nprotects salient weights and reduces quantization error by using a scaling-based method.It consistently outperforms Round-to-nearest quantization (RTN) and achieves comparable performance as mixedprecision (1% FP16) while being more hardware-friendly.\n\n\nTable 4 .\n4\nAWQ improves over round-to-nearest quantization (RTN) for different model sizes and different bit-precisions.It consistently achieves better perplexity than GPTQ (w/ and w/o reordering) on LLaMA & Llama-2 models.\nPPL\u2193Llama-2LLaMA7B13B70B7B13B30B65BFP16-5.474.883.325.685.094.103.53RTN6.665.523.987.015.884.884.24INT3GPTQ6.435.483.888.815.664.884.17g128GPTQ-R6.425.413.866.535.644.744.21AWQ6.245.323.746.355.524.613.95RTN5.734.983.465.965.254.233.67INT4GPTQ5.694.983.426.225.234.243.66g128GPTQ-R5.634.993.435.835.204.223.66AWQ5.604.973.415.785.194.213.62\n\nTable 5 .\n5\n[2]ntization results of a visual language model OpenFlamingo-9B[2]on COCO Captioning datasets.AWQ outperforms existing methods under zero-shot and various few-shot settings, demonstrating the generability to different modalities and in-context learning workloads.AWQ reduces the quantization degradation (32-shot) from 4.57 to 1.17 under INT4-g128, providing 4\u00d7 model size reduction with negligible performance loss.\n\u2191)0-shot4-shot8-shot16-shot32-shot\u2206(32-shot)FP16-63.7372.1876.9579.7481.70-INT4 g128RTN GPTQ AWQ60.24 59.72 62.5768.07 67.68 71.0272.46 72.53 74.7574.09 74.98 78.2377.13 74.98 80.53-4.57 -6.72 -1.17INT3 g128RTN GPTQ AWQ46.07 29.84 56.3355.13 50.77 64.7360.46 56.55 68.7963.21 60.54 72.8664.79 64.77 74.47-16.91 -16.93 -7.23\n\nTable 6 .\n6\nOur method is orthogonal to GPTQ: it further closes the performance gap under extreme low-bit quantization (INT2-g64) when combined with GPTQ.Results are WikiText-2 perplexity of OPT models.\nOPT / Wiki PPL\u21931.3B2.7B6.7B13B30BFP16-14.6212.4710.8610.139.56INT2 g64RTN GPTQ10476 46.67193210 28.157622 16.6517564 16.748170 11.75AWQ +GPTQ35.7125.7015.7113.2511.38\n\nTable 7 ,\n7\nunder INT3-g128 quantization, AWQ achieves similar performance compared to keeping 1% salient weights in FP16.While under INT3 no-group quantization, there is still a noticeable gap.Nonetheless, we want to stress that the performance of no-group quantization is still far behind grouped quantization at a similar cost.Therefore, grouped quantization is a more practical solution for LLM compression for edge deployment and AWQ can effectively improve the quantized performance under this setting.\nPPL \u2193FP16INT3 (group 128)INT3 (no group)RTN 1% FP16 AWQRTN1% FP16 AWQOPT-6.7B12.29 43.1613.0212.99 2116014.6718.11LLaMA-7B9.4912.1010.7710.82 50.4514.0620.52\n\nTable 7 .\n7\nAWQ can match the performance of keeping 1% salient weights in FP16 under grouped quantization without introducing mixed-precisions, but not for no-group quantization.Nonetheless, grouped quantization has a far better performance compared to no-group, making it a far more practical setting for weight-only quantization of LLMs, while AWQ performs quite well under this setting.Results are perplexity on the WikiText-2 dataset.\n\nAcknowledgementsWe thank MIT AI Hardware Program, National Science Foundation, NVIDIA Academic Partnership Award, MIT-IBM Watson AI Lab, Amazon and MIT Science Hub, Qualcomm Innovation Fellowship, Microsoft Turing Academic Program for supporting this research.\nFlamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 202235\n\n. Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Yonatan Kalyani Marathe, Samir Bitton, Jenia Gadre, Simon Jitsev, Pang Wei Kornblith, Gabriel Koh, Mitchell Ilharco, Ludwig Wortsman, Schmidt, Openflamingo, March 2023\n\nEstimating or propagating gradients through stochastic neurons for conditional computation. Yoshua Bengio, Nicholas L\u00e9onard, Aaron Courville, arXiv:1308.34322013arXiv preprint\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc202033Ilya Sutskever, and Dario Amodei\n\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, Lawrence Zitnick, arXiv:1504.00325Microsoft coco captions: Data collection and evaluation server. 2015arXiv preprint\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023\n\nPact: Parameterized clipping activation for quantized neural networks. Jungwook Choi, Zhuo Wang, Swagath Venkataramani, I-Jen Pierce, Vijayalakshmi Chuang, Kailash Srinivasan, Gopalakrishnan, arXiv:1805.060852018arXiv preprint\n\nScaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. 2022arXiv preprintint8(\n\nThe case for 4-bit precision: k-bit inference scaling laws. Tim Dettmers, Luke Zettlemoyer, arXiv:2212.097202022arXiv preprint\n\nPalm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.033782023arXiv preprint\n\nJeffrey L Steven K Esser, Deepika Mckinstry, Rathinakumar Bablani, Dharmendra S Appuswamy, Modha, arXiv:1902.08153Learned step size quantization. 2019arXiv preprint\n\nThe lottery ticket hypothesis: Finding sparse, trainable neural networks. Jonathan Frankle, Michael Carbin, arXiv:1803.036352018arXiv preprint\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.173232022arXiv preprint\n\nThe pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.000272020arXiv preprint\n\nAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, Kurt Keutzer, arXiv:2103.13630A survey of quantization methods for efficient neural network inference. 2021arXiv preprint\n\nDeep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. Song Han, Huizi Mao, William J Dally, ICLR2016\n\nLearning both weights and connections for efficient neural network. Song Han, Jeff Pool, John Tran, William Dally, Advances in neural information processing systems. 282015\n\nQuantization and training of neural networks for efficient integerarithmetic-only inference. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018\n\nThe enron corpus: A new dataset for email classification research. Bryan Klimt, Yiming Yang, Machine Learning: ECML 2004: 15th European Conference on Machine Learning. Pisa, ItalySpringerSeptember 20-24, 2004. 200415\n\nGrounding language models to images for multimodal generation. Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried, arXiv:2301.138232023arXiv preprint\n\nBlip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.125972023arXiv preprint\n\nBrecq: Pushing the limit of post-training quantization by block reconstruction. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, Shi Gu, arXiv:2102.054262021arXiv preprint\n\nHolistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.091102022arXiv preprint\n\nMcunet: Tiny deep learning on iot devices. Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, Advances in Neural Information Processing Systems. 202033\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. 2023\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, 2016\n\nUp or down? adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning. PMLR2020\n\nData-free quantization through weight equalization and bias correction. Markus Nagel, Mart Van Baalen, Tijmen Blankevoort, Max Welling, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019\n\nMarkus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, Tijmen Blankevoort, arXiv:2106.08295A white paper on neural network quantization. 2021arXiv preprint\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235\n\nGunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee, arXiv:2206.09557Quantized matmul for efficient inference of large-scale generative language models. 2022arXiv preprint\n\nMultitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, arXiv:2110.082072021arXiv preprint\n\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022arXiv preprint\n\nHigh-throughput generative inference of large language models with a single gpu. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Y Daniel, Zhiqiang Fu, Beidi Xie, Clark Chen, Joseph E Barrett, Gonzalez, arXiv:2303.068652023arXiv preprint\n\nStanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023\n\nTriton: an intermediate language and compiler for tiled neural network computations. Philippe Tillet, Hsiang-Tsung Kung, David Cox, Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages. the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages2019\n\nThibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth\u00e9e Lachaux, Baptiste Lacroix, Naman Rozi\u00e8re, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint\n\nLlama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint\n\nAdvances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, 201730Attention is all you need\n\nHAQ: Hardware-Aware Automated Quantization with Mixed Precision. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, Song Han, CVPR. 2019\n\nJason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint\n\nOutlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu, arXiv:2304.091452023arXiv preprint\n\nOutlier suppression: Pushing the limit of low-bit transformer language models. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu, arXiv:2209.133252022arXiv preprint\n\nOutlier suppression: Pushing the limit of low-bit transformer language models. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu, 2022\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han, arXiv:2211.104382022arXiv preprint\n\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, 2022\n\nLlama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao, arXiv:2303.161992023arXiv preprint\n\n. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 2022Opt: Open pre-trained transformer language models\n", "annotations": {"author": "[{\"end\":121,\"start\":92},{\"end\":157,\"start\":122},{\"end\":193,\"start\":158},{\"end\":227,\"start\":194},{\"end\":262,\"start\":228},{\"end\":296,\"start\":263},{\"end\":328,\"start\":297},{\"end\":356,\"start\":329}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":95},{\"end\":134,\"start\":130},{\"end\":170,\"start\":166},{\"end\":204,\"start\":200},{\"end\":239,\"start\":235},{\"end\":273,\"start\":270},{\"end\":305,\"start\":302},{\"end\":333,\"start\":329}]", "author_first_name": "[{\"end\":94,\"start\":92},{\"end\":129,\"start\":122},{\"end\":165,\"start\":158},{\"end\":199,\"start\":194},{\"end\":234,\"start\":228},{\"end\":269,\"start\":263},{\"end\":301,\"start\":297}]", "author_affiliation": "[{\"end\":120,\"start\":100},{\"end\":156,\"start\":136},{\"end\":192,\"start\":172},{\"end\":226,\"start\":206},{\"end\":261,\"start\":241},{\"end\":295,\"start\":275},{\"end\":327,\"start\":307},{\"end\":355,\"start\":335}]", "title": "[{\"end\":79,\"start\":1},{\"end\":435,\"start\":357}]", "venue": null, "abstract": "[{\"end\":1945,\"start\":504}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2016,\"start\":2012},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2075,\"start\":2072},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2078,\"start\":2075},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2081,\"start\":2078},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2084,\"start\":2081},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2563,\"start\":2559},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5197,\"start\":5193},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5207,\"start\":5203},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5404,\"start\":5401},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6190,\"start\":6187},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6193,\"start\":6190},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6196,\"start\":6193},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6199,\"start\":6196},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7217,\"start\":7213},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7220,\"start\":7217},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7712,\"start\":7708},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11580,\"start\":11576},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11583,\"start\":11580},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11780,\"start\":11777},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11783,\"start\":11780},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13016,\"start\":13012},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13553,\"start\":13549},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13556,\"start\":13553},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13819,\"start\":13815},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13879,\"start\":13875},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14060,\"start\":14056},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14073,\"start\":14069},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14124,\"start\":14120},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14264,\"start\":14261},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14311,\"start\":14308},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14330,\"start\":14326},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14424,\"start\":14421},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14427,\"start\":14424},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14430,\"start\":14427},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14433,\"start\":14430},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14436,\"start\":14433},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14546,\"start\":14542},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14610,\"start\":14606},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14762,\"start\":14758},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14765,\"start\":14762},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14822,\"start\":14818},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15001,\"start\":14997},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15016,\"start\":15012},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15032,\"start\":15028},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15179,\"start\":15175},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15294,\"start\":15290},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15311,\"start\":15307},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":15386,\"start\":15382},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15389,\"start\":15386},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15456,\"start\":15452},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15458,\"start\":15456},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15608,\"start\":15604},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15818,\"start\":15814},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15821,\"start\":15818},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15824,\"start\":15821},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15826,\"start\":15824},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15980,\"start\":15977},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16119,\"start\":16116},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16445,\"start\":16442},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16584,\"start\":16580},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16950,\"start\":16947},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16953,\"start\":16950},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16956,\"start\":16953},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16959,\"start\":16956},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":16962,\"start\":16959},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16965,\"start\":16962},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17264,\"start\":17261},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17300,\"start\":17297},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17324,\"start\":17321},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18189,\"start\":18185},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22357,\"start\":22353},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25283,\"start\":25279},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25323,\"start\":25319},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25985,\"start\":25981},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25988,\"start\":25985},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25991,\"start\":25988},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25994,\"start\":25991},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25997,\"start\":25994},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26000,\"start\":25997},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26226,\"start\":26223},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26229,\"start\":26226},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26232,\"start\":26229},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26234,\"start\":26232},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26270,\"start\":26266},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26273,\"start\":26270},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26276,\"start\":26273},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26583,\"start\":26580},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":26586,\"start\":26583},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":26589,\"start\":26586},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26592,\"start\":26589},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26595,\"start\":26592},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26708,\"start\":26704},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26711,\"start\":26708},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26714,\"start\":26711},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26717,\"start\":26714},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26983,\"start\":26979},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":27276,\"start\":27272},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":27293,\"start\":27289},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27482,\"start\":27478},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27622,\"start\":27618},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":27635,\"start\":27631},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27862,\"start\":27858},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29847,\"start\":29844},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29850,\"start\":29847},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":29853,\"start\":29850},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":29856,\"start\":29853},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29859,\"start\":29856},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30116,\"start\":30112},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30443,\"start\":30439},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32090,\"start\":32087},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32228,\"start\":32224}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":32282,\"start\":31967},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32582,\"start\":32283},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32962,\"start\":32583},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33404,\"start\":32963},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33675,\"start\":33405},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34242,\"start\":33676},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34996,\"start\":34243},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":35367,\"start\":34997},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":36035,\"start\":35368},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":36477,\"start\":36036}]", "paragraph": "[{\"end\":2282,\"start\":1961},{\"end\":2825,\"start\":2284},{\"end\":4054,\"start\":2827},{\"end\":5080,\"start\":4056},{\"end\":5864,\"start\":5082},{\"end\":5996,\"start\":5866},{\"end\":6471,\"start\":6042},{\"end\":7921,\"start\":6535},{\"end\":8267,\"start\":7923},{\"end\":8497,\"start\":8326},{\"end\":8774,\"start\":8499},{\"end\":9072,\"start\":8825},{\"end\":9564,\"start\":9125},{\"end\":9615,\"start\":9566},{\"end\":9794,\"start\":9722},{\"end\":10082,\"start\":9796},{\"end\":10749,\"start\":10084},{\"end\":11042,\"start\":10751},{\"end\":11840,\"start\":11165},{\"end\":12184,\"start\":11842},{\"end\":12956,\"start\":12226},{\"end\":13439,\"start\":12958},{\"end\":14012,\"start\":13452},{\"end\":14377,\"start\":14014},{\"end\":14611,\"start\":14379},{\"end\":15209,\"start\":14613},{\"end\":15688,\"start\":15224},{\"end\":16784,\"start\":15690},{\"end\":18079,\"start\":16800},{\"end\":18948,\"start\":18081},{\"end\":19361,\"start\":18950},{\"end\":19414,\"start\":19373},{\"end\":19461,\"start\":19426},{\"end\":19518,\"start\":19473},{\"end\":19575,\"start\":19530},{\"end\":19622,\"start\":19587},{\"end\":20169,\"start\":19634},{\"end\":20664,\"start\":20188},{\"end\":20859,\"start\":20666},{\"end\":21091,\"start\":20861},{\"end\":21440,\"start\":21093},{\"end\":22292,\"start\":21459},{\"end\":22504,\"start\":22306},{\"end\":23307,\"start\":22506},{\"end\":24324,\"start\":23309},{\"end\":24886,\"start\":24337},{\"end\":25874,\"start\":24888},{\"end\":26430,\"start\":25891},{\"end\":26916,\"start\":26432},{\"end\":27295,\"start\":26918},{\"end\":28197,\"start\":27297},{\"end\":28993,\"start\":28212},{\"end\":29791,\"start\":29031},{\"end\":30117,\"start\":29793},{\"end\":30483,\"start\":30119},{\"end\":30792,\"start\":30511},{\"end\":31259,\"start\":30794},{\"end\":31966,\"start\":31303},{\"end\":32281,\"start\":31981},{\"end\":32581,\"start\":32297},{\"end\":32961,\"start\":32595},{\"end\":33387,\"start\":32976},{\"end\":33421,\"start\":33418},{\"end\":33674,\"start\":33423},{\"end\":33901,\"start\":33689},{\"end\":34672,\"start\":34256},{\"end\":35200,\"start\":35010},{\"end\":35877,\"start\":35381},{\"end\":36476,\"start\":36049}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8824,\"start\":8775},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9124,\"start\":9073},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9721,\"start\":9616},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11107,\"start\":11043},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11164,\"start\":11107},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12225,\"start\":12185}]", "table_ref": "[{\"end\":3229,\"start\":3228},{\"end\":7005,\"start\":7004},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":9939,\"start\":9938},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":12789,\"start\":12788},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15526,\"start\":15525},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17341,\"start\":17340},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19056,\"start\":19055}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1959,\"start\":1947},{\"attributes\":{\"n\":\"2\"},\"end\":6040,\"start\":5999},{\"attributes\":{\"n\":\"2.1\"},\"end\":6533,\"start\":6474},{\"attributes\":{\"n\":\"2.2\"},\"end\":8324,\"start\":8270},{\"attributes\":{\"n\":\"3.1\"},\"end\":13450,\"start\":13442},{\"attributes\":{\"n\":\"3.2\"},\"end\":15222,\"start\":15212},{\"end\":16798,\"start\":16787},{\"end\":19371,\"start\":19364},{\"end\":19424,\"start\":19417},{\"end\":19471,\"start\":19464},{\"end\":19528,\"start\":19521},{\"end\":19585,\"start\":19578},{\"end\":19632,\"start\":19625},{\"end\":20186,\"start\":20172},{\"end\":21457,\"start\":21443},{\"end\":22304,\"start\":22295},{\"attributes\":{\"n\":\"3.3\"},\"end\":24335,\"start\":24327},{\"attributes\":{\"n\":\"4\"},\"end\":25889,\"start\":25877},{\"attributes\":{\"n\":\"5\"},\"end\":28210,\"start\":28200},{\"end\":29029,\"start\":28996},{\"end\":30509,\"start\":30486},{\"end\":31301,\"start\":31262},{\"end\":31978,\"start\":31968},{\"end\":32294,\"start\":32284},{\"end\":32592,\"start\":32584},{\"end\":32973,\"start\":32964},{\"end\":33415,\"start\":33406},{\"end\":33686,\"start\":33677},{\"end\":34253,\"start\":34244},{\"end\":35007,\"start\":34998},{\"end\":35378,\"start\":35369},{\"end\":36046,\"start\":36037}]", "table": "[{\"end\":33404,\"start\":33388},{\"end\":34242,\"start\":33902},{\"end\":34996,\"start\":34673},{\"end\":35367,\"start\":35201},{\"end\":36035,\"start\":35878}]", "figure_caption": "[{\"end\":32282,\"start\":31980},{\"end\":32582,\"start\":32296},{\"end\":32962,\"start\":32594},{\"end\":33388,\"start\":32975},{\"end\":33422,\"start\":33417},{\"end\":33902,\"start\":33688},{\"end\":34673,\"start\":34255},{\"end\":35201,\"start\":35009},{\"end\":35878,\"start\":35380},{\"end\":36477,\"start\":36048}]", "figure_ref": "[{\"end\":2762,\"start\":2761},{\"end\":3529,\"start\":3528},{\"end\":6469,\"start\":6468},{\"end\":6881,\"start\":6880},{\"end\":13241,\"start\":13240},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15992,\"start\":15991},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17896,\"start\":17895},{\"end\":18207,\"start\":18206},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22536,\"start\":22535},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22986,\"start\":22985},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23409,\"start\":23408},{\"end\":24688,\"start\":24687},{\"end\":25233,\"start\":25232},{\"end\":31499,\"start\":31498}]", "bib_author_first_name": "[{\"end\":36809,\"start\":36796},{\"end\":36823,\"start\":36819},{\"end\":36840,\"start\":36833},{\"end\":36853,\"start\":36846},{\"end\":36865,\"start\":36861},{\"end\":36876,\"start\":36872},{\"end\":36890,\"start\":36885},{\"end\":36903,\"start\":36897},{\"end\":36921,\"start\":36912},{\"end\":36939,\"start\":36932},{\"end\":37015,\"start\":37011},{\"end\":37031,\"start\":37026},{\"end\":37043,\"start\":37037},{\"end\":37057,\"start\":37053},{\"end\":37071,\"start\":37066},{\"end\":37087,\"start\":37080},{\"end\":37100,\"start\":37093},{\"end\":37123,\"start\":37118},{\"end\":37137,\"start\":37132},{\"end\":37150,\"start\":37145},{\"end\":37163,\"start\":37159},{\"end\":37167,\"start\":37164},{\"end\":37186,\"start\":37179},{\"end\":37200,\"start\":37192},{\"end\":37216,\"start\":37210},{\"end\":37360,\"start\":37354},{\"end\":37377,\"start\":37369},{\"end\":37392,\"start\":37387},{\"end\":37481,\"start\":37478},{\"end\":37497,\"start\":37489},{\"end\":37508,\"start\":37504},{\"end\":37523,\"start\":37516},{\"end\":37538,\"start\":37533},{\"end\":37540,\"start\":37539},{\"end\":37557,\"start\":37549},{\"end\":37574,\"start\":37568},{\"end\":37594,\"start\":37588},{\"end\":37608,\"start\":37602},{\"end\":37623,\"start\":37617},{\"end\":37640,\"start\":37632},{\"end\":37655,\"start\":37650},{\"end\":37678,\"start\":37670},{\"end\":37691,\"start\":37688},{\"end\":37707,\"start\":37702},{\"end\":37721,\"start\":37715},{\"end\":37736,\"start\":37730},{\"end\":37753,\"start\":37746},{\"end\":37765,\"start\":37758},{\"end\":37779,\"start\":37774},{\"end\":37791,\"start\":37787},{\"end\":37802,\"start\":37798},{\"end\":37818,\"start\":37811},{\"end\":37879,\"start\":37878},{\"end\":37893,\"start\":37892},{\"end\":37904,\"start\":37903},{\"end\":37915,\"start\":37914},{\"end\":37917,\"start\":37916},{\"end\":37927,\"start\":37926},{\"end\":38089,\"start\":38083},{\"end\":38099,\"start\":38096},{\"end\":38114,\"start\":38106},{\"end\":38131,\"start\":38120},{\"end\":38149,\"start\":38142},{\"end\":38162,\"start\":38157},{\"end\":38179,\"start\":38171},{\"end\":38296,\"start\":38289},{\"end\":38312,\"start\":38305},{\"end\":38319,\"start\":38317},{\"end\":38329,\"start\":38325},{\"end\":38345,\"start\":38337},{\"end\":38353,\"start\":38350},{\"end\":38368,\"start\":38361},{\"end\":38382,\"start\":38376},{\"end\":38398,\"start\":38391},{\"end\":38413,\"start\":38407},{\"end\":38415,\"start\":38414},{\"end\":38622,\"start\":38614},{\"end\":38633,\"start\":38629},{\"end\":38647,\"start\":38640},{\"end\":38668,\"start\":38663},{\"end\":38690,\"start\":38677},{\"end\":38706,\"start\":38699},{\"end\":38823,\"start\":38818},{\"end\":38837,\"start\":38835},{\"end\":38849,\"start\":38843},{\"end\":38865,\"start\":38859},{\"end\":38874,\"start\":38872},{\"end\":38887,\"start\":38880},{\"end\":38899,\"start\":38895},{\"end\":38910,\"start\":38904},{\"end\":38924,\"start\":38917},{\"end\":38945,\"start\":38935},{\"end\":38993,\"start\":38990},{\"end\":39008,\"start\":39004},{\"end\":39022,\"start\":39016},{\"end\":39036,\"start\":39032},{\"end\":39209,\"start\":39206},{\"end\":39224,\"start\":39220},{\"end\":39326,\"start\":39321},{\"end\":39338,\"start\":39335},{\"end\":39345,\"start\":39344},{\"end\":39347,\"start\":39346},{\"end\":39360,\"start\":39355},{\"end\":39379,\"start\":39370},{\"end\":39392,\"start\":39387},{\"end\":39410,\"start\":39404},{\"end\":39427,\"start\":39419},{\"end\":39439,\"start\":39435},{\"end\":39455,\"start\":39449},{\"end\":39510,\"start\":39503},{\"end\":39512,\"start\":39511},{\"end\":39536,\"start\":39529},{\"end\":39560,\"start\":39548},{\"end\":39580,\"start\":39570},{\"end\":39582,\"start\":39581},{\"end\":39751,\"start\":39743},{\"end\":39768,\"start\":39761},{\"end\":39901,\"start\":39896},{\"end\":39918,\"start\":39911},{\"end\":39938,\"start\":39935},{\"end\":40063,\"start\":40060},{\"end\":40075,\"start\":40069},{\"end\":40089,\"start\":40086},{\"end\":40105,\"start\":40097},{\"end\":40121,\"start\":40115},{\"end\":40136,\"start\":40129},{\"end\":40150,\"start\":40145},{\"end\":40164,\"start\":40158},{\"end\":40174,\"start\":40169},{\"end\":40185,\"start\":40182},{\"end\":40237,\"start\":40233},{\"end\":40253,\"start\":40247},{\"end\":40263,\"start\":40259},{\"end\":40276,\"start\":40270},{\"end\":40289,\"start\":40282},{\"end\":40291,\"start\":40290},{\"end\":40305,\"start\":40301},{\"end\":40534,\"start\":40530},{\"end\":40545,\"start\":40540},{\"end\":40558,\"start\":40551},{\"end\":40560,\"start\":40559},{\"end\":40650,\"start\":40646},{\"end\":40660,\"start\":40656},{\"end\":40671,\"start\":40667},{\"end\":40685,\"start\":40678},{\"end\":40851,\"start\":40845},{\"end\":40869,\"start\":40859},{\"end\":40880,\"start\":40878},{\"end\":40895,\"start\":40887},{\"end\":40908,\"start\":40901},{\"end\":40921,\"start\":40915},{\"end\":40937,\"start\":40930},{\"end\":40950,\"start\":40944},{\"end\":41184,\"start\":41179},{\"end\":41198,\"start\":41192},{\"end\":41397,\"start\":41393},{\"end\":41412,\"start\":41406},{\"end\":41434,\"start\":41428},{\"end\":41588,\"start\":41582},{\"end\":41599,\"start\":41593},{\"end\":41610,\"start\":41604},{\"end\":41627,\"start\":41621},{\"end\":41755,\"start\":41749},{\"end\":41766,\"start\":41760},{\"end\":41775,\"start\":41773},{\"end\":41785,\"start\":41781},{\"end\":41796,\"start\":41792},{\"end\":41803,\"start\":41801},{\"end\":41818,\"start\":41811},{\"end\":41826,\"start\":41823},{\"end\":41836,\"start\":41833},{\"end\":41922,\"start\":41917},{\"end\":41935,\"start\":41930},{\"end\":41951,\"start\":41947},{\"end\":41965,\"start\":41957},{\"end\":41981,\"start\":41975},{\"end\":41998,\"start\":41989},{\"end\":42013,\"start\":42009},{\"end\":42027,\"start\":42021},{\"end\":42045,\"start\":42039},{\"end\":42056,\"start\":42050},{\"end\":42145,\"start\":42143},{\"end\":42159,\"start\":42151},{\"end\":42171,\"start\":42166},{\"end\":42183,\"start\":42177},{\"end\":42193,\"start\":42189},{\"end\":42265,\"start\":42258},{\"end\":42279,\"start\":42271},{\"end\":42292,\"start\":42284},{\"end\":42301,\"start\":42297},{\"end\":42305,\"start\":42302},{\"end\":42384,\"start\":42377},{\"end\":42400,\"start\":42393},{\"end\":42413,\"start\":42408},{\"end\":42431,\"start\":42424},{\"end\":42514,\"start\":42508},{\"end\":42526,\"start\":42522},{\"end\":42542,\"start\":42538},{\"end\":42563,\"start\":42555},{\"end\":42579,\"start\":42573},{\"end\":42727,\"start\":42721},{\"end\":42739,\"start\":42735},{\"end\":42758,\"start\":42752},{\"end\":42775,\"start\":42772},{\"end\":42926,\"start\":42920},{\"end\":42940,\"start\":42934},{\"end\":42958,\"start\":42954},{\"end\":42977,\"start\":42970},{\"end\":42994,\"start\":42990},{\"end\":43013,\"start\":43007},{\"end\":43182,\"start\":43178},{\"end\":43198,\"start\":43191},{\"end\":43205,\"start\":43203},{\"end\":43218,\"start\":43213},{\"end\":43235,\"start\":43228},{\"end\":43254,\"start\":43248},{\"end\":43269,\"start\":43264},{\"end\":43285,\"start\":43277},{\"end\":43303,\"start\":43295},{\"end\":43315,\"start\":43311},{\"end\":43385,\"start\":43380},{\"end\":43400,\"start\":43392},{\"end\":43409,\"start\":43407},{\"end\":43431,\"start\":43421},{\"end\":43445,\"start\":43437},{\"end\":43458,\"start\":43451},{\"end\":43657,\"start\":43651},{\"end\":43670,\"start\":43664},{\"end\":43684,\"start\":43679},{\"end\":43700,\"start\":43693},{\"end\":43702,\"start\":43701},{\"end\":43716,\"start\":43709},{\"end\":43731,\"start\":43727},{\"end\":43749,\"start\":43742},{\"end\":43765,\"start\":43759},{\"end\":43781,\"start\":43776},{\"end\":43795,\"start\":43791},{\"end\":43843,\"start\":43838},{\"end\":43859,\"start\":43853},{\"end\":43876,\"start\":43865},{\"end\":43889,\"start\":43884},{\"end\":43905,\"start\":43899},{\"end\":43918,\"start\":43912},{\"end\":43933,\"start\":43928},{\"end\":43953,\"start\":43944},{\"end\":43959,\"start\":43954},{\"end\":43978,\"start\":43970},{\"end\":43993,\"start\":43985},{\"end\":44180,\"start\":44176},{\"end\":44195,\"start\":44188},{\"end\":44210,\"start\":44203},{\"end\":44224,\"start\":44217},{\"end\":44232,\"start\":44229},{\"end\":44244,\"start\":44243},{\"end\":44261,\"start\":44253},{\"end\":44271,\"start\":44266},{\"end\":44282,\"start\":44277},{\"end\":44295,\"start\":44289},{\"end\":44297,\"start\":44296},{\"end\":44413,\"start\":44408},{\"end\":44427,\"start\":44421},{\"end\":44445,\"start\":44439},{\"end\":44457,\"start\":44453},{\"end\":44473,\"start\":44466},{\"end\":44484,\"start\":44478},{\"end\":44500,\"start\":44495},{\"end\":44517,\"start\":44508},{\"end\":44519,\"start\":44518},{\"end\":44630,\"start\":44622},{\"end\":44651,\"start\":44639},{\"end\":44663,\"start\":44658},{\"end\":44875,\"start\":44868},{\"end\":44897,\"start\":44890},{\"end\":44912,\"start\":44906},{\"end\":44932,\"start\":44922},{\"end\":44951,\"start\":44943},{\"end\":44969,\"start\":44961},{\"end\":44984,\"start\":44979},{\"end\":44998,\"start\":44994},{\"end\":45182,\"start\":45178},{\"end\":45197,\"start\":45192},{\"end\":45211,\"start\":45206},{\"end\":45224,\"start\":45219},{\"end\":45238,\"start\":45233},{\"end\":45257,\"start\":45250},{\"end\":45273,\"start\":45266},{\"end\":45291,\"start\":45285},{\"end\":45307,\"start\":45299},{\"end\":45324,\"start\":45318},{\"end\":45427,\"start\":45421},{\"end\":45441,\"start\":45437},{\"end\":45455,\"start\":45451},{\"end\":45469,\"start\":45464},{\"end\":45486,\"start\":45481},{\"end\":45499,\"start\":45494},{\"end\":45501,\"start\":45500},{\"end\":45515,\"start\":45509},{\"end\":45529,\"start\":45524},{\"end\":45644,\"start\":45640},{\"end\":45658,\"start\":45651},{\"end\":45669,\"start\":45664},{\"end\":45677,\"start\":45675},{\"end\":45687,\"start\":45683},{\"end\":45710,\"start\":45705},{\"end\":45723,\"start\":45716},{\"end\":45732,\"start\":45731},{\"end\":45748,\"start\":45742},{\"end\":45760,\"start\":45755},{\"end\":45764,\"start\":45761},{\"end\":45775,\"start\":45770},{\"end\":45783,\"start\":45780},{\"end\":45798,\"start\":45792},{\"end\":45800,\"start\":45799},{\"end\":45811,\"start\":45805},{\"end\":46031,\"start\":46024},{\"end\":46044,\"start\":46037},{\"end\":46058,\"start\":46052},{\"end\":46071,\"start\":46063},{\"end\":46085,\"start\":46079},{\"end\":46099,\"start\":46092},{\"end\":46114,\"start\":46105},{\"end\":46242,\"start\":46235},{\"end\":46255,\"start\":46248},{\"end\":46271,\"start\":46263},{\"end\":46285,\"start\":46279},{\"end\":46301,\"start\":46292},{\"end\":46311,\"start\":46309},{\"end\":46326,\"start\":46319},{\"end\":46340,\"start\":46331},{\"end\":46468,\"start\":46461},{\"end\":46481,\"start\":46474},{\"end\":46497,\"start\":46489},{\"end\":46511,\"start\":46505},{\"end\":46527,\"start\":46518},{\"end\":46537,\"start\":46535},{\"end\":46552,\"start\":46545},{\"end\":46566,\"start\":46557},{\"end\":46677,\"start\":46668},{\"end\":46686,\"start\":46684},{\"end\":46699,\"start\":46692},{\"end\":46714,\"start\":46708},{\"end\":46728,\"start\":46724},{\"end\":46869,\"start\":46863},{\"end\":46879,\"start\":46875},{\"end\":46905,\"start\":46899},{\"end\":46920,\"start\":46913},{\"end\":46933,\"start\":46925},{\"end\":46945,\"start\":46938},{\"end\":47044,\"start\":47038},{\"end\":47059,\"start\":47052},{\"end\":47070,\"start\":47065},{\"end\":47085,\"start\":47077},{\"end\":47096,\"start\":47090},{\"end\":47105,\"start\":47102},{\"end\":47119,\"start\":47110},{\"end\":47128,\"start\":47124},{\"end\":47136,\"start\":47134},{\"end\":47186,\"start\":47181},{\"end\":47201,\"start\":47194},{\"end\":47215,\"start\":47210},{\"end\":47228,\"start\":47223},{\"end\":47242,\"start\":47238},{\"end\":47256,\"start\":47249},{\"end\":47274,\"start\":47263},{\"end\":47286,\"start\":47282},{\"end\":47297,\"start\":47293},{\"end\":47304,\"start\":47302},{\"end\":47324,\"start\":47319},{\"end\":47339,\"start\":47335},{\"end\":47348,\"start\":47345},{\"end\":47363,\"start\":47359},{\"end\":47379,\"start\":47373},{\"end\":47392,\"start\":47387},{\"end\":47412,\"start\":47406},{\"end\":47428,\"start\":47422},{\"end\":47439,\"start\":47435}]", "bib_author_last_name": "[{\"end\":36817,\"start\":36810},{\"end\":36831,\"start\":36824},{\"end\":36844,\"start\":36841},{\"end\":36859,\"start\":36854},{\"end\":36870,\"start\":36866},{\"end\":36883,\"start\":36877},{\"end\":36895,\"start\":36891},{\"end\":36910,\"start\":36904},{\"end\":36930,\"start\":36922},{\"end\":36948,\"start\":36940},{\"end\":37024,\"start\":37016},{\"end\":37035,\"start\":37032},{\"end\":37051,\"start\":37044},{\"end\":37064,\"start\":37058},{\"end\":37078,\"start\":37072},{\"end\":37091,\"start\":37088},{\"end\":37116,\"start\":37101},{\"end\":37130,\"start\":37124},{\"end\":37143,\"start\":37138},{\"end\":37157,\"start\":37151},{\"end\":37177,\"start\":37168},{\"end\":37190,\"start\":37187},{\"end\":37208,\"start\":37201},{\"end\":37225,\"start\":37217},{\"end\":37234,\"start\":37227},{\"end\":37248,\"start\":37236},{\"end\":37367,\"start\":37361},{\"end\":37385,\"start\":37378},{\"end\":37402,\"start\":37393},{\"end\":37487,\"start\":37482},{\"end\":37502,\"start\":37498},{\"end\":37514,\"start\":37509},{\"end\":37531,\"start\":37524},{\"end\":37547,\"start\":37541},{\"end\":37566,\"start\":37558},{\"end\":37586,\"start\":37575},{\"end\":37600,\"start\":37595},{\"end\":37615,\"start\":37609},{\"end\":37630,\"start\":37624},{\"end\":37648,\"start\":37641},{\"end\":37668,\"start\":37656},{\"end\":37686,\"start\":37679},{\"end\":37700,\"start\":37692},{\"end\":37713,\"start\":37708},{\"end\":37728,\"start\":37722},{\"end\":37744,\"start\":37737},{\"end\":37756,\"start\":37754},{\"end\":37772,\"start\":37766},{\"end\":37785,\"start\":37780},{\"end\":37796,\"start\":37792},{\"end\":37809,\"start\":37803},{\"end\":37825,\"start\":37819},{\"end\":37890,\"start\":37880},{\"end\":37901,\"start\":37894},{\"end\":37912,\"start\":37905},{\"end\":37924,\"start\":37918},{\"end\":37931,\"start\":37928},{\"end\":38094,\"start\":38090},{\"end\":38104,\"start\":38100},{\"end\":38118,\"start\":38115},{\"end\":38140,\"start\":38132},{\"end\":38155,\"start\":38150},{\"end\":38169,\"start\":38163},{\"end\":38187,\"start\":38180},{\"end\":38303,\"start\":38297},{\"end\":38315,\"start\":38313},{\"end\":38323,\"start\":38320},{\"end\":38335,\"start\":38330},{\"end\":38348,\"start\":38346},{\"end\":38359,\"start\":38354},{\"end\":38374,\"start\":38369},{\"end\":38389,\"start\":38383},{\"end\":38405,\"start\":38399},{\"end\":38424,\"start\":38416},{\"end\":38627,\"start\":38623},{\"end\":38638,\"start\":38634},{\"end\":38661,\"start\":38648},{\"end\":38675,\"start\":38669},{\"end\":38697,\"start\":38691},{\"end\":38717,\"start\":38707},{\"end\":38733,\"start\":38719},{\"end\":38833,\"start\":38824},{\"end\":38841,\"start\":38838},{\"end\":38857,\"start\":38850},{\"end\":38870,\"start\":38866},{\"end\":38878,\"start\":38875},{\"end\":38893,\"start\":38888},{\"end\":38902,\"start\":38900},{\"end\":38915,\"start\":38911},{\"end\":38933,\"start\":38925},{\"end\":38952,\"start\":38946},{\"end\":39002,\"start\":38994},{\"end\":39014,\"start\":39009},{\"end\":39030,\"start\":39023},{\"end\":39048,\"start\":39037},{\"end\":39218,\"start\":39210},{\"end\":39236,\"start\":39225},{\"end\":39333,\"start\":39327},{\"end\":39342,\"start\":39339},{\"end\":39353,\"start\":39348},{\"end\":39368,\"start\":39361},{\"end\":39385,\"start\":39380},{\"end\":39402,\"start\":39393},{\"end\":39417,\"start\":39411},{\"end\":39433,\"start\":39428},{\"end\":39447,\"start\":39440},{\"end\":39461,\"start\":39456},{\"end\":39465,\"start\":39463},{\"end\":39527,\"start\":39513},{\"end\":39546,\"start\":39537},{\"end\":39568,\"start\":39561},{\"end\":39592,\"start\":39583},{\"end\":39599,\"start\":39594},{\"end\":39759,\"start\":39752},{\"end\":39775,\"start\":39769},{\"end\":39909,\"start\":39902},{\"end\":39933,\"start\":39919},{\"end\":39946,\"start\":39939},{\"end\":39956,\"start\":39948},{\"end\":40067,\"start\":40064},{\"end\":40084,\"start\":40076},{\"end\":40095,\"start\":40090},{\"end\":40113,\"start\":40106},{\"end\":40127,\"start\":40122},{\"end\":40143,\"start\":40137},{\"end\":40156,\"start\":40151},{\"end\":40167,\"start\":40165},{\"end\":40180,\"start\":40175},{\"end\":40195,\"start\":40186},{\"end\":40245,\"start\":40238},{\"end\":40257,\"start\":40254},{\"end\":40268,\"start\":40264},{\"end\":40280,\"start\":40277},{\"end\":40299,\"start\":40292},{\"end\":40313,\"start\":40306},{\"end\":40538,\"start\":40535},{\"end\":40549,\"start\":40546},{\"end\":40566,\"start\":40561},{\"end\":40654,\"start\":40651},{\"end\":40665,\"start\":40661},{\"end\":40676,\"start\":40672},{\"end\":40691,\"start\":40686},{\"end\":40857,\"start\":40852},{\"end\":40876,\"start\":40870},{\"end\":40885,\"start\":40881},{\"end\":40899,\"start\":40896},{\"end\":40913,\"start\":40909},{\"end\":40928,\"start\":40922},{\"end\":40942,\"start\":40938},{\"end\":40963,\"start\":40951},{\"end\":41190,\"start\":41185},{\"end\":41203,\"start\":41199},{\"end\":41404,\"start\":41398},{\"end\":41426,\"start\":41413},{\"end\":41440,\"start\":41435},{\"end\":41591,\"start\":41589},{\"end\":41602,\"start\":41600},{\"end\":41619,\"start\":41611},{\"end\":41631,\"start\":41628},{\"end\":41758,\"start\":41756},{\"end\":41771,\"start\":41767},{\"end\":41779,\"start\":41776},{\"end\":41790,\"start\":41786},{\"end\":41799,\"start\":41797},{\"end\":41809,\"start\":41804},{\"end\":41821,\"start\":41819},{\"end\":41831,\"start\":41827},{\"end\":41839,\"start\":41837},{\"end\":41928,\"start\":41923},{\"end\":41945,\"start\":41936},{\"end\":41955,\"start\":41952},{\"end\":41973,\"start\":41966},{\"end\":41987,\"start\":41982},{\"end\":42007,\"start\":41999},{\"end\":42019,\"start\":42014},{\"end\":42037,\"start\":42028},{\"end\":42048,\"start\":42046},{\"end\":42062,\"start\":42057},{\"end\":42149,\"start\":42146},{\"end\":42164,\"start\":42160},{\"end\":42175,\"start\":42172},{\"end\":42187,\"start\":42184},{\"end\":42197,\"start\":42194},{\"end\":42269,\"start\":42266},{\"end\":42282,\"start\":42280},{\"end\":42295,\"start\":42293},{\"end\":42309,\"start\":42306},{\"end\":42391,\"start\":42385},{\"end\":42406,\"start\":42401},{\"end\":42422,\"start\":42414},{\"end\":42438,\"start\":42432},{\"end\":42520,\"start\":42515},{\"end\":42536,\"start\":42527},{\"end\":42553,\"start\":42543},{\"end\":42571,\"start\":42564},{\"end\":42591,\"start\":42580},{\"end\":42733,\"start\":42728},{\"end\":42750,\"start\":42740},{\"end\":42770,\"start\":42759},{\"end\":42783,\"start\":42776},{\"end\":42932,\"start\":42927},{\"end\":42952,\"start\":42941},{\"end\":42968,\"start\":42959},{\"end\":42988,\"start\":42978},{\"end\":43005,\"start\":42995},{\"end\":43025,\"start\":43014},{\"end\":43189,\"start\":43183},{\"end\":43201,\"start\":43199},{\"end\":43211,\"start\":43206},{\"end\":43226,\"start\":43219},{\"end\":43246,\"start\":43236},{\"end\":43262,\"start\":43255},{\"end\":43275,\"start\":43270},{\"end\":43293,\"start\":43286},{\"end\":43309,\"start\":43304},{\"end\":43319,\"start\":43316},{\"end\":43390,\"start\":43386},{\"end\":43405,\"start\":43401},{\"end\":43419,\"start\":43410},{\"end\":43435,\"start\":43432},{\"end\":43449,\"start\":43446},{\"end\":43462,\"start\":43459},{\"end\":43662,\"start\":43658},{\"end\":43677,\"start\":43671},{\"end\":43691,\"start\":43685},{\"end\":43707,\"start\":43703},{\"end\":43725,\"start\":43717},{\"end\":43740,\"start\":43732},{\"end\":43757,\"start\":43750},{\"end\":43774,\"start\":43766},{\"end\":43789,\"start\":43782},{\"end\":43800,\"start\":43796},{\"end\":43851,\"start\":43844},{\"end\":43863,\"start\":43860},{\"end\":43882,\"start\":43877},{\"end\":43897,\"start\":43890},{\"end\":43910,\"start\":43906},{\"end\":43926,\"start\":43919},{\"end\":43942,\"start\":43934},{\"end\":43968,\"start\":43960},{\"end\":43983,\"start\":43979},{\"end\":43999,\"start\":43994},{\"end\":44186,\"start\":44181},{\"end\":44201,\"start\":44196},{\"end\":44215,\"start\":44211},{\"end\":44227,\"start\":44225},{\"end\":44241,\"start\":44233},{\"end\":44251,\"start\":44245},{\"end\":44264,\"start\":44262},{\"end\":44275,\"start\":44272},{\"end\":44287,\"start\":44283},{\"end\":44305,\"start\":44298},{\"end\":44315,\"start\":44307},{\"end\":44419,\"start\":44414},{\"end\":44437,\"start\":44428},{\"end\":44451,\"start\":44446},{\"end\":44464,\"start\":44458},{\"end\":44476,\"start\":44474},{\"end\":44493,\"start\":44485},{\"end\":44506,\"start\":44501},{\"end\":44529,\"start\":44520},{\"end\":44637,\"start\":44631},{\"end\":44656,\"start\":44652},{\"end\":44667,\"start\":44664},{\"end\":44888,\"start\":44876},{\"end\":44904,\"start\":44898},{\"end\":44920,\"start\":44913},{\"end\":44941,\"start\":44933},{\"end\":44959,\"start\":44952},{\"end\":44977,\"start\":44970},{\"end\":44992,\"start\":44985},{\"end\":45004,\"start\":44999},{\"end\":45012,\"start\":45006},{\"end\":45190,\"start\":45183},{\"end\":45204,\"start\":45198},{\"end\":45217,\"start\":45212},{\"end\":45231,\"start\":45225},{\"end\":45248,\"start\":45239},{\"end\":45264,\"start\":45258},{\"end\":45283,\"start\":45274},{\"end\":45297,\"start\":45292},{\"end\":45316,\"start\":45308},{\"end\":45332,\"start\":45325},{\"end\":45435,\"start\":45428},{\"end\":45449,\"start\":45442},{\"end\":45462,\"start\":45456},{\"end\":45479,\"start\":45470},{\"end\":45492,\"start\":45487},{\"end\":45507,\"start\":45502},{\"end\":45522,\"start\":45516},{\"end\":45540,\"start\":45530},{\"end\":45649,\"start\":45645},{\"end\":45662,\"start\":45659},{\"end\":45673,\"start\":45670},{\"end\":45681,\"start\":45678},{\"end\":45691,\"start\":45688},{\"end\":45714,\"start\":45711},{\"end\":45729,\"start\":45724},{\"end\":45740,\"start\":45733},{\"end\":45753,\"start\":45749},{\"end\":45768,\"start\":45765},{\"end\":45778,\"start\":45776},{\"end\":45790,\"start\":45784},{\"end\":45803,\"start\":45801},{\"end\":45815,\"start\":45812},{\"end\":45819,\"start\":45817},{\"end\":46035,\"start\":46032},{\"end\":46050,\"start\":46045},{\"end\":46061,\"start\":46059},{\"end\":46077,\"start\":46072},{\"end\":46090,\"start\":46086},{\"end\":46103,\"start\":46100},{\"end\":46118,\"start\":46115},{\"end\":46246,\"start\":46243},{\"end\":46261,\"start\":46256},{\"end\":46277,\"start\":46272},{\"end\":46290,\"start\":46286},{\"end\":46307,\"start\":46302},{\"end\":46317,\"start\":46312},{\"end\":46329,\"start\":46327},{\"end\":46344,\"start\":46341},{\"end\":46472,\"start\":46469},{\"end\":46487,\"start\":46482},{\"end\":46503,\"start\":46498},{\"end\":46516,\"start\":46512},{\"end\":46533,\"start\":46528},{\"end\":46543,\"start\":46538},{\"end\":46555,\"start\":46553},{\"end\":46570,\"start\":46567},{\"end\":46682,\"start\":46678},{\"end\":46690,\"start\":46687},{\"end\":46706,\"start\":46700},{\"end\":46722,\"start\":46715},{\"end\":46732,\"start\":46729},{\"end\":46873,\"start\":46870},{\"end\":46897,\"start\":46880},{\"end\":46911,\"start\":46906},{\"end\":46923,\"start\":46921},{\"end\":46936,\"start\":46934},{\"end\":46948,\"start\":46946},{\"end\":47050,\"start\":47045},{\"end\":47063,\"start\":47060},{\"end\":47075,\"start\":47071},{\"end\":47088,\"start\":47086},{\"end\":47100,\"start\":47097},{\"end\":47108,\"start\":47106},{\"end\":47122,\"start\":47120},{\"end\":47132,\"start\":47129},{\"end\":47141,\"start\":47137},{\"end\":47192,\"start\":47187},{\"end\":47208,\"start\":47202},{\"end\":47221,\"start\":47216},{\"end\":47236,\"start\":47229},{\"end\":47247,\"start\":47243},{\"end\":47261,\"start\":47257},{\"end\":47280,\"start\":47275},{\"end\":47291,\"start\":47287},{\"end\":47300,\"start\":47298},{\"end\":47317,\"start\":47305},{\"end\":47333,\"start\":47325},{\"end\":47343,\"start\":47340},{\"end\":47357,\"start\":47349},{\"end\":47371,\"start\":47364},{\"end\":47385,\"start\":47380},{\"end\":47404,\"start\":47393},{\"end\":47420,\"start\":47413},{\"end\":47433,\"start\":47429},{\"end\":47451,\"start\":47440}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":248476411},\"end\":37007,\"start\":36739},{\"attributes\":{\"id\":\"b1\"},\"end\":37260,\"start\":37009},{\"attributes\":{\"doi\":\"arXiv:1308.3432\",\"id\":\"b2\"},\"end\":37437,\"start\":37262},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218971783},\"end\":38081,\"start\":37439},{\"attributes\":{\"doi\":\"arXiv:1504.00325\",\"id\":\"b4\"},\"end\":38287,\"start\":38083},{\"attributes\":{\"id\":\"b5\"},\"end\":38541,\"start\":38289},{\"attributes\":{\"doi\":\"arXiv:1805.06085\",\"id\":\"b6\"},\"end\":38769,\"start\":38543},{\"attributes\":{\"doi\":\"arXiv:2210.11416\",\"id\":\"b7\"},\"end\":38988,\"start\":38771},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b8\"},\"end\":39144,\"start\":38990},{\"attributes\":{\"doi\":\"arXiv:2212.09720\",\"id\":\"b9\"},\"end\":39272,\"start\":39146},{\"attributes\":{\"doi\":\"arXiv:2303.03378\",\"id\":\"b10\"},\"end\":39501,\"start\":39274},{\"attributes\":{\"doi\":\"arXiv:1902.08153\",\"id\":\"b11\"},\"end\":39667,\"start\":39503},{\"attributes\":{\"doi\":\"arXiv:1803.03635\",\"id\":\"b12\"},\"end\":39811,\"start\":39669},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b13\"},\"end\":39992,\"start\":39813},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b14\"},\"end\":40231,\"start\":39994},{\"attributes\":{\"doi\":\"arXiv:2103.13630\",\"id\":\"b15\"},\"end\":40422,\"start\":40233},{\"attributes\":{\"id\":\"b16\"},\"end\":40576,\"start\":40424},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2238772},\"end\":40750,\"start\":40578},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":39867659},\"end\":41110,\"start\":40752},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":13451873},\"end\":41328,\"start\":41112},{\"attributes\":{\"doi\":\"arXiv:2301.13823\",\"id\":\"b20\"},\"end\":41476,\"start\":41330},{\"attributes\":{\"doi\":\"arXiv:2301.12597\",\"id\":\"b21\"},\"end\":41667,\"start\":41478},{\"attributes\":{\"doi\":\"arXiv:2102.05426\",\"id\":\"b22\"},\"end\":41875,\"start\":41669},{\"attributes\":{\"doi\":\"arXiv:2211.09110\",\"id\":\"b23\"},\"end\":42098,\"start\":41877},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":220646526},\"end\":42256,\"start\":42100},{\"attributes\":{\"id\":\"b25\"},\"end\":42342,\"start\":42258},{\"attributes\":{\"id\":\"b26\"},\"end\":42444,\"start\":42344},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":216056295},\"end\":42647,\"start\":42446},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":184487878},\"end\":42918,\"start\":42649},{\"attributes\":{\"doi\":\"arXiv:2106.08295\",\"id\":\"b29\"},\"end\":43107,\"start\":42920},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":246426909},\"end\":43378,\"start\":43109},{\"attributes\":{\"doi\":\"arXiv:2206.09557\",\"id\":\"b31\"},\"end\":43582,\"start\":43380},{\"attributes\":{\"doi\":\"arXiv:2110.08207\",\"id\":\"b32\"},\"end\":43836,\"start\":43584},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b33\"},\"end\":44093,\"start\":43838},{\"attributes\":{\"doi\":\"arXiv:2303.06865\",\"id\":\"b34\"},\"end\":44351,\"start\":44095},{\"attributes\":{\"id\":\"b35\"},\"end\":44535,\"start\":44353},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":184488182},\"end\":44866,\"start\":44537},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b37\"},\"end\":45123,\"start\":44868},{\"attributes\":{\"doi\":\"arXiv:2307.09288\",\"id\":\"b38\"},\"end\":45368,\"start\":45125},{\"attributes\":{\"id\":\"b39\"},\"end\":45573,\"start\":45370},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":102350477},\"end\":45703,\"start\":45575},{\"attributes\":{\"doi\":\"arXiv:2109.01652\",\"id\":\"b41\"},\"end\":45905,\"start\":45705},{\"attributes\":{\"doi\":\"arXiv:2304.09145\",\"id\":\"b42\"},\"end\":46154,\"start\":45907},{\"attributes\":{\"doi\":\"arXiv:2209.13325\",\"id\":\"b43\"},\"end\":46380,\"start\":46156},{\"attributes\":{\"id\":\"b44\"},\"end\":46576,\"start\":46382},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b45\"},\"end\":46768,\"start\":46578},{\"attributes\":{\"id\":\"b46\"},\"end\":46954,\"start\":46770},{\"attributes\":{\"doi\":\"arXiv:2303.16199\",\"id\":\"b47\"},\"end\":47177,\"start\":46956},{\"attributes\":{\"id\":\"b48\"},\"end\":47506,\"start\":47179}]", "bib_title": "[{\"end\":36794,\"start\":36739},{\"end\":37476,\"start\":37439},{\"end\":40644,\"start\":40578},{\"end\":40843,\"start\":40752},{\"end\":41177,\"start\":41112},{\"end\":42141,\"start\":42100},{\"end\":42506,\"start\":42446},{\"end\":42719,\"start\":42649},{\"end\":43176,\"start\":43109},{\"end\":44620,\"start\":44537},{\"end\":45638,\"start\":45575}]", "bib_author": "[{\"end\":36819,\"start\":36796},{\"end\":36833,\"start\":36819},{\"end\":36846,\"start\":36833},{\"end\":36861,\"start\":36846},{\"end\":36872,\"start\":36861},{\"end\":36885,\"start\":36872},{\"end\":36897,\"start\":36885},{\"end\":36912,\"start\":36897},{\"end\":36932,\"start\":36912},{\"end\":36950,\"start\":36932},{\"end\":37026,\"start\":37011},{\"end\":37037,\"start\":37026},{\"end\":37053,\"start\":37037},{\"end\":37066,\"start\":37053},{\"end\":37080,\"start\":37066},{\"end\":37093,\"start\":37080},{\"end\":37118,\"start\":37093},{\"end\":37132,\"start\":37118},{\"end\":37145,\"start\":37132},{\"end\":37159,\"start\":37145},{\"end\":37179,\"start\":37159},{\"end\":37192,\"start\":37179},{\"end\":37210,\"start\":37192},{\"end\":37227,\"start\":37210},{\"end\":37236,\"start\":37227},{\"end\":37250,\"start\":37236},{\"end\":37369,\"start\":37354},{\"end\":37387,\"start\":37369},{\"end\":37404,\"start\":37387},{\"end\":37489,\"start\":37478},{\"end\":37504,\"start\":37489},{\"end\":37516,\"start\":37504},{\"end\":37533,\"start\":37516},{\"end\":37549,\"start\":37533},{\"end\":37568,\"start\":37549},{\"end\":37588,\"start\":37568},{\"end\":37602,\"start\":37588},{\"end\":37617,\"start\":37602},{\"end\":37632,\"start\":37617},{\"end\":37650,\"start\":37632},{\"end\":37670,\"start\":37650},{\"end\":37688,\"start\":37670},{\"end\":37702,\"start\":37688},{\"end\":37715,\"start\":37702},{\"end\":37730,\"start\":37715},{\"end\":37746,\"start\":37730},{\"end\":37758,\"start\":37746},{\"end\":37774,\"start\":37758},{\"end\":37787,\"start\":37774},{\"end\":37798,\"start\":37787},{\"end\":37811,\"start\":37798},{\"end\":37827,\"start\":37811},{\"end\":38096,\"start\":38083},{\"end\":38106,\"start\":38096},{\"end\":38120,\"start\":38106},{\"end\":38142,\"start\":38120},{\"end\":38157,\"start\":38142},{\"end\":38171,\"start\":38157},{\"end\":38189,\"start\":38171},{\"end\":38305,\"start\":38289},{\"end\":38317,\"start\":38305},{\"end\":38325,\"start\":38317},{\"end\":38337,\"start\":38325},{\"end\":38350,\"start\":38337},{\"end\":38361,\"start\":38350},{\"end\":38376,\"start\":38361},{\"end\":38391,\"start\":38376},{\"end\":38407,\"start\":38391},{\"end\":38426,\"start\":38407},{\"end\":38629,\"start\":38614},{\"end\":38640,\"start\":38629},{\"end\":38663,\"start\":38640},{\"end\":38677,\"start\":38663},{\"end\":38699,\"start\":38677},{\"end\":38719,\"start\":38699},{\"end\":38735,\"start\":38719},{\"end\":38835,\"start\":38818},{\"end\":38843,\"start\":38835},{\"end\":38859,\"start\":38843},{\"end\":38872,\"start\":38859},{\"end\":38880,\"start\":38872},{\"end\":38895,\"start\":38880},{\"end\":38904,\"start\":38895},{\"end\":38917,\"start\":38904},{\"end\":38935,\"start\":38917},{\"end\":38954,\"start\":38935},{\"end\":39004,\"start\":38990},{\"end\":39016,\"start\":39004},{\"end\":39032,\"start\":39016},{\"end\":39050,\"start\":39032},{\"end\":39220,\"start\":39206},{\"end\":39238,\"start\":39220},{\"end\":39335,\"start\":39321},{\"end\":39344,\"start\":39335},{\"end\":39355,\"start\":39344},{\"end\":39370,\"start\":39355},{\"end\":39387,\"start\":39370},{\"end\":39404,\"start\":39387},{\"end\":39419,\"start\":39404},{\"end\":39435,\"start\":39419},{\"end\":39449,\"start\":39435},{\"end\":39463,\"start\":39449},{\"end\":39467,\"start\":39463},{\"end\":39529,\"start\":39503},{\"end\":39548,\"start\":39529},{\"end\":39570,\"start\":39548},{\"end\":39594,\"start\":39570},{\"end\":39601,\"start\":39594},{\"end\":39761,\"start\":39743},{\"end\":39777,\"start\":39761},{\"end\":39911,\"start\":39896},{\"end\":39935,\"start\":39911},{\"end\":39948,\"start\":39935},{\"end\":39958,\"start\":39948},{\"end\":40069,\"start\":40060},{\"end\":40086,\"start\":40069},{\"end\":40097,\"start\":40086},{\"end\":40115,\"start\":40097},{\"end\":40129,\"start\":40115},{\"end\":40145,\"start\":40129},{\"end\":40158,\"start\":40145},{\"end\":40169,\"start\":40158},{\"end\":40182,\"start\":40169},{\"end\":40197,\"start\":40182},{\"end\":40247,\"start\":40233},{\"end\":40259,\"start\":40247},{\"end\":40270,\"start\":40259},{\"end\":40282,\"start\":40270},{\"end\":40301,\"start\":40282},{\"end\":40315,\"start\":40301},{\"end\":40540,\"start\":40530},{\"end\":40551,\"start\":40540},{\"end\":40568,\"start\":40551},{\"end\":40656,\"start\":40646},{\"end\":40667,\"start\":40656},{\"end\":40678,\"start\":40667},{\"end\":40693,\"start\":40678},{\"end\":40859,\"start\":40845},{\"end\":40878,\"start\":40859},{\"end\":40887,\"start\":40878},{\"end\":40901,\"start\":40887},{\"end\":40915,\"start\":40901},{\"end\":40930,\"start\":40915},{\"end\":40944,\"start\":40930},{\"end\":40965,\"start\":40944},{\"end\":41192,\"start\":41179},{\"end\":41205,\"start\":41192},{\"end\":41406,\"start\":41393},{\"end\":41428,\"start\":41406},{\"end\":41442,\"start\":41428},{\"end\":41593,\"start\":41582},{\"end\":41604,\"start\":41593},{\"end\":41621,\"start\":41604},{\"end\":41633,\"start\":41621},{\"end\":41760,\"start\":41749},{\"end\":41773,\"start\":41760},{\"end\":41781,\"start\":41773},{\"end\":41792,\"start\":41781},{\"end\":41801,\"start\":41792},{\"end\":41811,\"start\":41801},{\"end\":41823,\"start\":41811},{\"end\":41833,\"start\":41823},{\"end\":41841,\"start\":41833},{\"end\":41930,\"start\":41917},{\"end\":41947,\"start\":41930},{\"end\":41957,\"start\":41947},{\"end\":41975,\"start\":41957},{\"end\":41989,\"start\":41975},{\"end\":42009,\"start\":41989},{\"end\":42021,\"start\":42009},{\"end\":42039,\"start\":42021},{\"end\":42050,\"start\":42039},{\"end\":42064,\"start\":42050},{\"end\":42151,\"start\":42143},{\"end\":42166,\"start\":42151},{\"end\":42177,\"start\":42166},{\"end\":42189,\"start\":42177},{\"end\":42199,\"start\":42189},{\"end\":42271,\"start\":42258},{\"end\":42284,\"start\":42271},{\"end\":42297,\"start\":42284},{\"end\":42311,\"start\":42297},{\"end\":42393,\"start\":42377},{\"end\":42408,\"start\":42393},{\"end\":42424,\"start\":42408},{\"end\":42440,\"start\":42424},{\"end\":42522,\"start\":42508},{\"end\":42538,\"start\":42522},{\"end\":42555,\"start\":42538},{\"end\":42573,\"start\":42555},{\"end\":42593,\"start\":42573},{\"end\":42735,\"start\":42721},{\"end\":42752,\"start\":42735},{\"end\":42772,\"start\":42752},{\"end\":42785,\"start\":42772},{\"end\":42934,\"start\":42920},{\"end\":42954,\"start\":42934},{\"end\":42970,\"start\":42954},{\"end\":42990,\"start\":42970},{\"end\":43007,\"start\":42990},{\"end\":43027,\"start\":43007},{\"end\":43191,\"start\":43178},{\"end\":43203,\"start\":43191},{\"end\":43213,\"start\":43203},{\"end\":43228,\"start\":43213},{\"end\":43248,\"start\":43228},{\"end\":43264,\"start\":43248},{\"end\":43277,\"start\":43264},{\"end\":43295,\"start\":43277},{\"end\":43311,\"start\":43295},{\"end\":43321,\"start\":43311},{\"end\":43392,\"start\":43380},{\"end\":43407,\"start\":43392},{\"end\":43421,\"start\":43407},{\"end\":43437,\"start\":43421},{\"end\":43451,\"start\":43437},{\"end\":43464,\"start\":43451},{\"end\":43664,\"start\":43651},{\"end\":43679,\"start\":43664},{\"end\":43693,\"start\":43679},{\"end\":43709,\"start\":43693},{\"end\":43727,\"start\":43709},{\"end\":43742,\"start\":43727},{\"end\":43759,\"start\":43742},{\"end\":43776,\"start\":43759},{\"end\":43791,\"start\":43776},{\"end\":43802,\"start\":43791},{\"end\":43853,\"start\":43838},{\"end\":43865,\"start\":43853},{\"end\":43884,\"start\":43865},{\"end\":43899,\"start\":43884},{\"end\":43912,\"start\":43899},{\"end\":43928,\"start\":43912},{\"end\":43944,\"start\":43928},{\"end\":43970,\"start\":43944},{\"end\":43985,\"start\":43970},{\"end\":44001,\"start\":43985},{\"end\":44188,\"start\":44176},{\"end\":44203,\"start\":44188},{\"end\":44217,\"start\":44203},{\"end\":44229,\"start\":44217},{\"end\":44243,\"start\":44229},{\"end\":44253,\"start\":44243},{\"end\":44266,\"start\":44253},{\"end\":44277,\"start\":44266},{\"end\":44289,\"start\":44277},{\"end\":44307,\"start\":44289},{\"end\":44317,\"start\":44307},{\"end\":44421,\"start\":44408},{\"end\":44439,\"start\":44421},{\"end\":44453,\"start\":44439},{\"end\":44466,\"start\":44453},{\"end\":44478,\"start\":44466},{\"end\":44495,\"start\":44478},{\"end\":44508,\"start\":44495},{\"end\":44531,\"start\":44508},{\"end\":44639,\"start\":44622},{\"end\":44658,\"start\":44639},{\"end\":44669,\"start\":44658},{\"end\":44890,\"start\":44868},{\"end\":44906,\"start\":44890},{\"end\":44922,\"start\":44906},{\"end\":44943,\"start\":44922},{\"end\":44961,\"start\":44943},{\"end\":44979,\"start\":44961},{\"end\":44994,\"start\":44979},{\"end\":45006,\"start\":44994},{\"end\":45014,\"start\":45006},{\"end\":45192,\"start\":45178},{\"end\":45206,\"start\":45192},{\"end\":45219,\"start\":45206},{\"end\":45233,\"start\":45219},{\"end\":45250,\"start\":45233},{\"end\":45266,\"start\":45250},{\"end\":45285,\"start\":45266},{\"end\":45299,\"start\":45285},{\"end\":45318,\"start\":45299},{\"end\":45334,\"start\":45318},{\"end\":45437,\"start\":45421},{\"end\":45451,\"start\":45437},{\"end\":45464,\"start\":45451},{\"end\":45481,\"start\":45464},{\"end\":45494,\"start\":45481},{\"end\":45509,\"start\":45494},{\"end\":45524,\"start\":45509},{\"end\":45542,\"start\":45524},{\"end\":45651,\"start\":45640},{\"end\":45664,\"start\":45651},{\"end\":45675,\"start\":45664},{\"end\":45683,\"start\":45675},{\"end\":45693,\"start\":45683},{\"end\":45716,\"start\":45705},{\"end\":45731,\"start\":45716},{\"end\":45742,\"start\":45731},{\"end\":45755,\"start\":45742},{\"end\":45770,\"start\":45755},{\"end\":45780,\"start\":45770},{\"end\":45792,\"start\":45780},{\"end\":45805,\"start\":45792},{\"end\":45817,\"start\":45805},{\"end\":45821,\"start\":45817},{\"end\":46037,\"start\":46024},{\"end\":46052,\"start\":46037},{\"end\":46063,\"start\":46052},{\"end\":46079,\"start\":46063},{\"end\":46092,\"start\":46079},{\"end\":46105,\"start\":46092},{\"end\":46120,\"start\":46105},{\"end\":46248,\"start\":46235},{\"end\":46263,\"start\":46248},{\"end\":46279,\"start\":46263},{\"end\":46292,\"start\":46279},{\"end\":46309,\"start\":46292},{\"end\":46319,\"start\":46309},{\"end\":46331,\"start\":46319},{\"end\":46346,\"start\":46331},{\"end\":46474,\"start\":46461},{\"end\":46489,\"start\":46474},{\"end\":46505,\"start\":46489},{\"end\":46518,\"start\":46505},{\"end\":46535,\"start\":46518},{\"end\":46545,\"start\":46535},{\"end\":46557,\"start\":46545},{\"end\":46572,\"start\":46557},{\"end\":46684,\"start\":46668},{\"end\":46692,\"start\":46684},{\"end\":46708,\"start\":46692},{\"end\":46724,\"start\":46708},{\"end\":46734,\"start\":46724},{\"end\":46875,\"start\":46863},{\"end\":46899,\"start\":46875},{\"end\":46913,\"start\":46899},{\"end\":46925,\"start\":46913},{\"end\":46938,\"start\":46925},{\"end\":46950,\"start\":46938},{\"end\":47052,\"start\":47038},{\"end\":47065,\"start\":47052},{\"end\":47077,\"start\":47065},{\"end\":47090,\"start\":47077},{\"end\":47102,\"start\":47090},{\"end\":47110,\"start\":47102},{\"end\":47124,\"start\":47110},{\"end\":47134,\"start\":47124},{\"end\":47143,\"start\":47134},{\"end\":47194,\"start\":47181},{\"end\":47210,\"start\":47194},{\"end\":47223,\"start\":47210},{\"end\":47238,\"start\":47223},{\"end\":47249,\"start\":47238},{\"end\":47263,\"start\":47249},{\"end\":47282,\"start\":47263},{\"end\":47293,\"start\":47282},{\"end\":47302,\"start\":47293},{\"end\":47319,\"start\":47302},{\"end\":47335,\"start\":47319},{\"end\":47345,\"start\":47335},{\"end\":47359,\"start\":47345},{\"end\":47373,\"start\":47359},{\"end\":47387,\"start\":47373},{\"end\":47406,\"start\":47387},{\"end\":47422,\"start\":47406},{\"end\":47435,\"start\":47422},{\"end\":47453,\"start\":47435}]", "bib_venue": "[{\"end\":36999,\"start\":36950},{\"end\":37352,\"start\":37262},{\"end\":37876,\"start\":37827},{\"end\":38267,\"start\":38205},{\"end\":38529,\"start\":38426},{\"end\":38612,\"start\":38543},{\"end\":38816,\"start\":38771},{\"end\":39119,\"start\":39066},{\"end\":39204,\"start\":39146},{\"end\":39319,\"start\":39274},{\"end\":39647,\"start\":39617},{\"end\":39741,\"start\":39669},{\"end\":39894,\"start\":39813},{\"end\":40058,\"start\":39994},{\"end\":40402,\"start\":40331},{\"end\":40528,\"start\":40424},{\"end\":40742,\"start\":40693},{\"end\":41042,\"start\":40965},{\"end\":41278,\"start\":41205},{\"end\":41391,\"start\":41330},{\"end\":41580,\"start\":41478},{\"end\":41747,\"start\":41669},{\"end\":41915,\"start\":41877},{\"end\":42248,\"start\":42199},{\"end\":42336,\"start\":42311},{\"end\":42375,\"start\":42344},{\"end\":42637,\"start\":42593},{\"end\":42856,\"start\":42785},{\"end\":43087,\"start\":43043},{\"end\":43370,\"start\":43321},{\"end\":43562,\"start\":43480},{\"end\":43649,\"start\":43584},{\"end\":44073,\"start\":44017},{\"end\":44174,\"start\":44095},{\"end\":44406,\"start\":44353},{\"end\":44772,\"start\":44669},{\"end\":45103,\"start\":45030},{\"end\":45176,\"start\":45125},{\"end\":45419,\"start\":45370},{\"end\":45697,\"start\":45693},{\"end\":45885,\"start\":45837},{\"end\":46022,\"start\":45907},{\"end\":46233,\"start\":46156},{\"end\":46459,\"start\":46382},{\"end\":46666,\"start\":46578},{\"end\":46861,\"start\":46770},{\"end\":47036,\"start\":46956},{\"end\":38021,\"start\":37933},{\"end\":41106,\"start\":41044},{\"end\":41291,\"start\":41280},{\"end\":42914,\"start\":42858},{\"end\":44862,\"start\":44774}]"}}}, "year": 2023, "month": 12, "day": 17}