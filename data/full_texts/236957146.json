{"id": 236957146, "updated": "2023-10-06 00:31:49.127", "metadata": {"title": "A Categorized Reflection Removal Dataset with Diverse Real-world Scenes", "authors": "[{\"first\":\"Chenyang\",\"last\":\"Lei\",\"middle\":[]},{\"first\":\"Xuhua\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Chenyang\",\"last\":\"Qi\",\"middle\":[]},{\"first\":\"Yankun\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Wenxiu\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Qiong\",\"last\":\"Yan\",\"middle\":[]},{\"first\":\"Qifeng\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 8, "day": 7}, "abstract": "Due to the lack of a large-scale reflection removal dataset with diverse real-world scenes, many existing reflection removal methods are trained on synthetic data plus a small amount of real-world data, which makes it difficult to evaluate the strengths or weaknesses of different reflection removal methods thoroughly. Furthermore, existing real-world benchmarks and datasets do not categorize image data based on the types and appearances of reflection (e.g., smoothness, intensity), making it hard to analyze reflection removal methods. Hence, we construct a new reflection removal dataset that is categorized, diverse, and real-world (CDR). A pipeline based on RAW data is used to capture perfectly aligned input images and transmission images. The dataset is constructed using diverse glass types under various environments to ensure diversity. By analyzing several reflection removal methods and conducting extensive experiments on our dataset, we show that state-of-the-art reflection removal methods generally perform well on blurry reflection but fail in obtaining satisfying performance on other types of real-world reflection. We believe our dataset can help develop novel methods to remove real-world reflection better. Our dataset is available at https://alexzhao-hugga.github.io/Real-World-Reflection-Removal/.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.03380", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LeiHQZSYC22", "doi": "10.1109/cvprw56347.2022.00343"}}, "content": {"source": {"pdf_hash": "ebb4a07e786be7016f4544f5f4cfc9054ec46cf2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.03380v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6f5e49baa14676d6d454d7bb5eb2c0553a73754f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ebb4a07e786be7016f4544f5f4cfc9054ec46cf2.txt", "contents": "\nA Categorized Reflection Removal Dataset with Diverse Real-world Scenes\n\n\nChenyang Lei \nXuhua Huang \nChenyang Qi \nYankun Zhao \nWenxiu Sun \nQiong Yan \nQifeng Chen \nHkust 2 Cmu \nSensetime \nA Categorized Reflection Removal Dataset with Diverse Real-world Scenes\n\nDue to the lack of a large-scale reflection removal dataset with diverse real-world scenes, many existing reflection removal methods are trained on synthetic data plus a small amount of real-world data, which makes it difficult to evaluate the strengths or weaknesses of different reflection removal methods thoroughly. Furthermore, existing real-world benchmarks and datasets do not categorize image data based on the types and appearances of reflection (e.g., smoothness, intensity), making it hard to analyze reflection removal methods. Hence, we construct a new reflection removal dataset that is categorized, diverse, and real-world (CDR). A pipeline based on RAW data is used to capture perfectly aligned input images and transmission images. The dataset is constructed using diverse glass types under various environments to ensure diversity. By analyzing several reflection removal methods and conducting extensive experiments on our dataset, we show that state-of-the-art reflection removal methods generally perform well on blurry reflection but fail in obtaining satisfying performance on other types of real-world reflection. We believe our dataset can help develop novel methods to remove real-world reflection better. Our dataset is available at https://alexzhao-hugga.github.io/Real-World-Reflection-Removal/.\n\nIntroduction\n\nReflection removal is a task to remove undesirable reflection artifacts from a photograph. Existing deep learning based approaches for reflection removal have demonstrated superior performance on synthetic data, but we find that their performance degrades severely in diverse real-world data, as shown in Fig. 1. Most existing learning-based methods [5,40] are trained on synthetic data created under various assumptions. Hence, their performance is limited by the domain gap between real-world and synthetic data. The assumptions to create the synthetic data are often simplified, * Equal contribution and thus these approaches have sub-optimal performance on real-world data.\n\nThe existing real-world benchmark dataset SIR 2 facilitates the research in reflection removal, but it has weaknesses as reported in the paper [29]. In this dataset, most images only contain flat objects or small object (i.e., postcards and solid objects) under controlled lighting. These images do not represent the scenes in our daily live because object distance, scales, and natural illumination variation are quite diverse in the wild. In the SIR 2 dataset, only collected 55 pairs of images with ground-truth transmission in the wild. There are also other real-world datasets [40,15] with highquality ground truth but the they have a small number of images.\n\nTo address the limitations of existing reflection removal datasets, we present a large reflection removal dataset CDR that contains diverse scene in the real world. Compared with prior work, CDR has several advantages, as shown in Table 1. In terms of the number of real-world images, ours is much larger and more diverse than existing ones. We construct the dataset following several principles to ensure image quality and diversity. First, we capture our data in the wild since it is similar to images captured in daily life. Second, to ensure perfect alignment between transmission and input mixed images, we obtain the transmission by subtracting reflection from the mixed image in the raw data space [14]. Third, we capture the images using different glasses in diverse scenes to ensure the diversity of our dataset. We believe that the performance of a reflection removal method is related to the smoothness of reflection, and thus we carefully categorize the collected data into different types of reflection. We split the data based on the smoothness of reflection (i.e., sharp reflection or blurry reflection). In our experiments, all existing methods are sensitive to the smoothness of reflection.\n\nWe hope the CDR dataset can facilitate the research in reflection removal. The CDR dataset can provide a more extensive evaluation for reflection removal methods. In addition, the detailed categorization can help analyze the bottlenecks of existing methods. Zhang et al. [40] Wei et al. [32] CoRRN [30] BDN [37] Wen et al. [33] Figure 1. The performance of existing methods on different types of reflection is quite different. Most algorithms can remove the blurry reflection but cannot remove the sharp reflection well.\n\n\nBackground\n\n\nSingle Image Reflection Removal\n\nMost single image reflection removal methods [5,40,37,38] rely on various assumptions. Considering image gradients, Arvanitopoulos et al. [3] propose the idea of suppressing the reflection, and Yang et al. [38] propose a faster method based on convex optimization. These methods fail to remove sharp reflection. Under the assumption that transmission is in focus, Punnappurath et al. [23] design a method based on dual-pixel camera input. CEILNet [5], Zhang et al. [40], and BDN [37] assume that reflection is out of focus and synthesize images to train their neural networks. CEILNet [5] estimates target edges first and uses it as guidance to predict the transmission layer. Zhang et al. [40] use perceptual and adversarial losses to capture the difference between reflection and transmission. BDN [37] estimates the reflection images, which is then used to estimate the transmission layer. These methods [40,5,37] work well when reflection is more defocused than transmission but fail otherwise. For these deep learning based approaches, training data is critical for good performance. To bridge the gap between synthetic and real-world data, Zhang et al. [40] and Wei et al. [32] collected some realworld images for training. However, their images have misalignment issues between transmission and input images, and the dataset size is small. Wei et al. [32] propose to use high-level features that are less sensitive to small misalignment to calculate losses. To obtain more realistic and diverse data, Wen et al. [33] and Ma et al. [20] propose to synthesize data using a deep neural network and achieve better performance and generalization. Kim et al. [11] propose a physics based method to render the reflection and achieve better performance than using synthetic images.\n\n\nReflection Removal with Multiple Images\n\nUtilizing multiple images as input provides additional information, which makes it possible to relax some strict assumptions used in prior work. A number of approaches [28,25,24,16,8,9,36,27,2,18] exploit the relative motion between reflection and transmission with multiple images captured with camera movement to remove reflection. Some other methods may take a sequence of images under specific conditions or camera settings. For example, pairs of flash and no-flash images [1,13], near-infrared cameras [10], light field cameras [31], dual-pixel cameras [23], and polarization cameras [19,22,26,12,6] can be used.\n\nDifferent from supervised learning models, Double-DIP [7] separates a mixed image into reflection and transmission layers based on internal self-similarities in multiple superpositions, in an unsupervised fashion.\n\nAlthough the methods based on multiple images do not rely on strict assumptions for appearances of reflections (e.g., blurry reflection, ghosting effects), they need additional requirements on data [36] or special devices [14], which may prevent them from broader applications. Therefore, in this work, we focus on building a dataset and eval-  [29,40,15]. Scene-level data: the data that is captured in the wild instead of lab environments.\n\nuating single image reflection removal methods.\n\n\nSIR 2 benchmark dataset\n\nWan et al. [29] propose the SIR 2 benchmark dataset for single image reflection removal. This dataset contains images taken in controlled scenes and in the wild. To solve the misalignment problem, they calibrate the alignment between the mixed image M and the background B. The dataset was captured with three glasses of different thicknesses, various combinations of aperture sizes, and different exposure times to improve the diversity in this dataset.\n\nAs described by Wan et al. [29], most of the objects in their controlled scene contain only flat objects (postcard) or objects with similar scales (solid objects). However, realworld scenes contains objects at different depths, and the natural environment illumination also varies greatly, while the controlled scenes are mostly captured in an indoor office environment. To address this limitation, 55 pairs of images with ground-truth reflection and transmission are captured in the wild, but 55 pairs are far from large scale. Also, this dataset does not provide a standard split among the training set, the validation set, and the test set.\n\n\nCDR Dataset\n\nIn this section, we describe the features of our CDR dataset for reflection removal, which stands for \"Categorized, Diverse, and Real-world.\" A triplet {M, R, T } is collected in each scene where M is the mixed image, R is the reflection image, and T is the transmission image.\n\nThis dataset is collected mainly by three cameras: a DSLR (Digital Single-Lens Reflex) camera Canon EOS 50D, a MILC (Mirrorless Interchangeable Lens Cameras) Nikon Z6, and a smartphone camera Huawei Mate30. In total, we provide 1,063 triplets in our dataset.\n\nWe collected the real-world data in the wild. Compared with data captured in a controlled environment, real-wold data in the wild contains objects of various distances and illumination variation, as reported by SIR 2 [29]. We improve our dataset in various aspects. Table 1 summarizes the main differences between the CDR dataset and existing datasets [29,40,15]. Specifically, our main advantages are listed as followed: (a) Data categorization. We notice that the performance of a reflection removal model is related to the appearance of reflection. To facilitate in-depth analysis, we split all the images according to the reflection types. (b) Perfect alignment. We provide perfect alignment between the mixed image M and transmission T . Existing datasets [29,40] has the misalignment issue between M and T . The misalignment issue does not only degrade a model's performance but also makes evaluation less accurate. A reflection removal model trained on misaligned paired data often generate blurry images [32]. Note that a single pixel shift in an image can affect evaluation metrics PSNR and SSIM significantly. (c) Diversity. We provide much more diverse data by utilizing different types of glasses exsiting in our daily life, including colored and curved glasses shown in Fig. 3. We capture various objects in different environments and lighting conditions with three different types of cameras. As mentioned before, when we capture images in the wild, we also guarantee diversity of the smoothness and intensity of reflection.\n\n(d) Large-scale data. Our dataset is significantly larger than existing datasets or benchmarks. In total, our dataset contains 1,063 triplets M, R, T .We believe our evaluation is more accurate because there is no misalignment between the mixed image and the ground truth. (e) Other advantages.\n\nCompared with existing datasets [40,32,15], we provide reflection image since many works have demonstrate the effectiveness of using  . More examples about the data diversity. In addition to glass types, we are also able to capture dynamic scenes, which enriches the scene diversity.\n\nOurs-M 1 Ours-R1 Ours-M 2 Ours-R2 reflection [37,14]. We also provide raw images instead of only RGB images for future study in reflection removal. Our CDR dataset is publicly available, which can be used for training and evaluation. The detailed categories can help researchers understand the strengths and weaknesses of existing methods. We hope it can accelerate the research in reflection removal.\n\n\nData Aquisition\n\nIn order to collect diverse data with perfect alignment in the wild, we adopt the M-R pipeline proposed by Lei et al. [14] where T is obtained by M \u2212 R. Different from Lei et al. [14] which implements M \u2212R on raw data obtained by a polarization sensor, we use normal cameras that provide the raw data to construct the CDR dataset. Fig. 4 shows our data collection pipeline. The first step is to have an appropriate glass. Since we do not need to remove the glass to obtain background B as other methods do [29,32,40], we can utilize immovable glasses in the real world. As the second step, we use a piece of black cloth behind the glass to block transmission to obtain the reflection R. In the end, we remove the cloth to collect the mixed image M . Please refer to Fig. 3 for some captured example iamges.\n\nThere are some details in real capturing progress:\n\n\u2022 To ensure perfect alignment between M and R, we use a tripod to fix the camera to take images.\n\n\u2022 To ensure the exposure is consistent between M and R, all the cameras are set to the manual mode with a fixed camera setting including ISO and exposure time.\n\n\u2022 To reduce the noise level in M and R as much as possible, we capture data with a long exposure time and a small ISO.\n\n\u2022 The objects in the reflection (not transmission) need be static in both M and R to ensure the perfect alignment in M \u2212 R.\n\n\nPost Processing\n\nFig . 5 shows the overall pipeline of our post-processing step. With raw M and raw R, we calculate raw T by T = M \u2212 R in the raw data space. Note that M \u2212 R should be implemented in the RGB space because the linearity between light intensities and RGB values does not hold, as shown in Fig. 6. In T = M \u2212 R, negative values may appear due to noise, and they are set to zeros directly. The black level of a camera is added back to ensure its ISP can be applied to T . So far, the raw data format of T is the same as M and R.\n\nSince most existing reflection removal algorithms adopt RGB images as input, we need to convert raw images from the raw data space to RGB space. However, the camera default ISP is not public. Therefore, we implement our own image signal processing (ISP) pipeline to generate RGB images for M , T , and R using the same ISP. As for the metadata of T , we simply apply the metadata of M to T directly.\n\nAfter obtaining RGB T images, we need to crop the area of interest for T . In the dataset, we eliminate the triplets in the following two cases: (1) If multiple glasses exist behind the covered glass (i.e., glasses exist in the background), the obtained T still has glasses. (2) If there is no glass in M , then no black cloth exists to cover the transmission. In this case, the obtained T will equal 0, which contradicts the ground truth. . The first row shows that our ISP generates similar results compared with Lightroom and Camera Output. The second row shows that different ISPs can be applied to T to achieve similar results. Note that Lightroom is a professional ISP software supported by Adobe.\n\nDifferent from SIR 2 that classify images as bright or dark scenes according to absolute intensity, focus, or thickness of glass, we categorize images according to three criteria: relative intensity, smoothness between R and T and the ghosting effect. First, we observe that the impact of reflection is decided by the relative intensity instead of absolute intensity. We calculate the mean intensity ratio for each pair of R and T and categorize them as weak reflection, moderate reflection or strong reflection. Second, we sort the data based on the smoothness of reflection and transmission: BRST (blurry reflection and sharp transmission), SRST (sharp reflection and sharp transmission), BRBT (blurry reflection and blurry transmission). In fact, the BRST type has been generally used in previous work. At last, we pick up the data which has ghosting effect as a class.\n\n\nExperiments\n\nWe first describe our experimental setup. The images from all three cameras are provided for training, which can reduce the impacts of the domain gap. Following previous work [29], we choose PSNR, SSIM and NCC as our main evaluation metrics.\n\nThe baselines for comparison in our experiment include CEILNet [5], CoRRN [30], BDN [37], Zhang et al. [40], Wei et al. [32], Yang et al. [38], Arvanitopoulos et al. [3], Li et al. [17], IBCLN [15], and Kim et al. [11]. All these methods take a single RGB image as input. For learningbased methods, we use their pre-trained models by default as some methods do not provide training code. For Yang et al. [38], different thresholds might result in different output images. Therefore, except for the original threshold, multiple thresholds are used for comparison, and the best result is reported.\n\n\nEvaluation\n\nOne interesting observation is that the performance of most existing single image reflection removal methods is highly related to the type of reflection.\n\nQuantitative results. Table 2 shows the performance of the evaluated methods in terms of PSNR, SSIM, and NCC. We also split data according to smoothness, relative intensity, and ghosting effect of reflection and report separate re- sults to analyze the impact of different image patterns. We find that learning free methods [3,38] achieve good performance in cases that follow their model assumptions. Although these methods usually rank poorly on average, they can perform quite well when the reflection is blurry, weak, or has ghosting effect.\n\nIn the following, we analyze the impact of different factors on the results.\n\n1) Impact of real data. The methods [32,30] trained on real data achieve better performance on general real-scene data. Since Kim et al. [11] synthesized physically-based data for training, they also achieve good performance on real-world data. These data include SRST data, reflection with moderate intensity or strong intensity. As we can see, these methods [11,32,30] are often the first, second, and third best-performing methods.\n\n2) The smoothness of reflection/transmission. The results on different smoothness of reflection and transmission are consistent with previously adopted assumptions. All methods perform relatively well the BRST (blurry reflection and sharp transmission) set. However, when the assumption does not hold (e.g., on SRST set), the performance degrades heavily. This phenomenon occurs to all evaluated algorithms. Note that in our dataset, almost 50% percents of M are SRST.\n\n3) Ghosting effect. Another assumption about reflection is the ghosting effect. Although most deep learning methods do not synthesize ghosting reflection data, they still achieve better performance on such kind of data.\n\n4) Reflection Intensity. The difficulty of reflection removal increases significantly when the intensity of the reflection increases. However, the importance of reflection removal also increases in this case. When reflection is too weak, we even do not need to remove it because it is almost invisible. When the reflection is moderate or strong, the image quality suffers heavily.\n\nQualitative results. We present qualitative results in Fig. 8 to analyze the results further. The perceptual performance is consistent with the quantitative results on different reflection types.\n\n\nBRST\n\n\nInput\n\nYang et al. [38] Arvanitopoulos et al. [3] CEILNet [5] CoRRN [30] Zhang et al. [40] BDN [37] Wei et al. [32] SRST Input GT Zhang et al. [40] Wei et al. [32] CoRRN [30] BDN [37] Yang et al. [38] Li et al. [17] Figure 8. Most methods cannot remove the sharp reflection. This is probably because learning-based methods are trained on synthetic data where R is blurry. Learning free methods often assume reflection is blurry. However, sharp reflection is quite common in the real world. The performance on SRST is not satisfactory. For SRST, most methods cannot remove most reflection. For BRST, most benchmarked methods can remove the reflection when the reflection is weak. Learning free methods achieve good performance in this case [38,3]. However, it is quite rare when the reflection is both blurry and weak.\n\nAnother problem that occurs commonly is the degradation of image quality as shown in Fig. 8. In some cases, a method may remove the reflection nearly completely, but transmission is modified at the same time.\n\n\nOpen Problems and Discussion\n\nFrom the quantitative evaluation and perceptual results, we find that state-of-the-art single image reflection removal methods are still far from perfect, although they have achieved great performance on synthetic data or real-world data in a controlled environment.\n\nFrom our experiment, we find that evaluation on synthetic data is flawed. The improvement of results on synthetic data or real-world data in a controlled environment cannot represent real improvement. If we want to apply the reflection removal method in our daily life, we should aim to achieve excellent performance on real-world data collected in the wild.\n\nWe believe we should relax strong assumptions in reflection removal methods. Strong assumptions may make the method perform well on a certain type of reflection but fail on other types. As analyzed above, most methods can achieve satisfactory results on blurry reflection but have poor performance on sharp reflection. It is definitely a dif-ficult task to distinguish between the sharp reflection and transmission.\n\n\nReflection Removal on Raw Images\n\nIt is unclear whether single image reflection removal can benefit from raw images. However, we keep the raw data in our dataset since applying raw images has achieved amazing results on low-level computer vision tasks, including low-light image enhancement [4], super-resolution [35], image denoising [39] and ISP [21,34]. We leave the study for raw images on single image reflection removal to future work. To the best of our knowledge, our dataset is the first dataset containing raw images for single image reflection removal.\n\n\nConclusion\n\nIn this work, we propose a new dataset CDR for single image reflection removal. Compared with other reflection removal datasets, our dataset is categorized according to reflection types, has the perfect alignment, and contains diverse scenes. We carefully categorize the captured images into different classes and analyze the performance of stateof-the-art methods. The experimental results show that the performance of these state-of-the-art methods is highly related to the appearance and intensity of reflection. When the pre-adopted assumptions do no hold on real-world images, the methods based on these assumptions cannot achieve top performance. We believe researchers can utilize our benchmark to do research on real-world data in the wild. In addition to RGB images, the raw data is also provided for future study.\n\nFigure 2 .\n2Due to refraction, spatial shift and intensity difference exist between B and T . The difference map visualizes the misalignment between B and T . The sum of the reflection R and the transmission T equals to the mixed image M in the raw data space.\n\n\n{M ,R,T } on curved glass {M ,R,T } on colored (red) glass {M ,R,T } on dynamic transmission {M ,R,T } on dynamic transmission\n\nFigure 3\n3Figure 3. More examples about the data diversity. In addition to glass types, we are also able to capture dynamic scenes, which enriches the scene diversity.\n\nFigure 4 .\n4With the M-R pipeline proposed by Lei et al.[14], we can utilize a diverse set of glasses existing in our daily life (e.g., the curved and colored glass on the telephone booth, and the glass as a door).\n\nFigure 5 .Figure 6 .Figure 7\n567The post-processing pipeline. Ground-truth transmission T is obtained in the RAW space. Then all the RAW images are passed through an \"ISP\" to obtain the corresponding RGB images. Finally, the regions of interest regions are cropped out.RGB M RGB R RGB M \u2212 R Gamma M \u2212 R Raw M \u2212 R If M \u2212 R isapplied other than the raw data space, undesirable residuals will appear. \"RGB M \u2212 R\": do M \u2212 R on RGB images. \"Gamma M \u2212 R\": use M 2.2 \u2212 R 2.2 to reduce the impact of gamma correction. \"Raw M \u2212 R\": do M \u2212 R on raw data. (a) M : Our ISP (b) M : Lightroom (c) M : Camera (d) M : Our ISP1 (e) T : Our ISP1 (f) T : Our ISP2\n\n\nFigure best viewed in the electronic version.\n\n\nGlass type Categorized Scene-level data Alignment Reflection image Curve glass Data type Training setSIR 2 -Wild [29] \n3 \nNo \n55 \nCalibrated \nYes \nNo \nRGB \nNo \n\nZhang et al. [40] \n1 \nNo \n<110 \nCalibrated \nNo \nNo \nRGB \nNo \n\nNature [15] \n2 \nNo \n< 220 \nMisalignment \nNo \nNo \nRGB \nNo \n\nOurs \n>200 \nYes \n1,063 \nPerfect \nYes \nYes \nRGB&Raw \nYes \n\nTable 1. The comparisons between our data and existing datasets \n\nRemoving photography artifacts using gradient projection and flash-exposure sampling. Amit Agrawal, Ramesh Raskar, K Shree, Yuanzhen Nayar, Li, TOG. 2Amit Agrawal, Ramesh Raskar, Shree K. Nayar, and Yuanzhen Li. Removing photography artifacts using gra- dient projection and flash-exposure sampling. TOG, 2005. 2\n\nThe visual centrifuge: Model-free layered video representations. Joao Jean-Baptiste Alayrac, Andrew Carreira, Zisserman, CVPR. Jean-Baptiste Alayrac, Joao Carreira, and Andrew Zisser- man. The visual centrifuge: Model-free layered video repre- sentations. In CVPR, 2019. 2\n\nSingle image reflection suppression. Nikolaos Arvanitopoulos, Radhakrishna Achanta, Sabine Susstrunk, CVPR. 67Nikolaos Arvanitopoulos, Radhakrishna Achanta, and Sabine Susstrunk. Single image reflection suppression. In CVPR, 2017. 2, 5, 6, 7\n\nLearning to see in the dark. Chen Chen, Qifeng Chen, Jia Xu, Vladlen Koltun, CVPR. Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. In CVPR, 2018. 8\n\nA generic deep architecture for single image reflection removal and image smoothing. Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, David Wipf, ICCV. 67Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, and David Wipf. A generic deep architecture for single image reflection removal and image smoothing. In ICCV, 2017. 1, 2, 5, 6, 7\n\nSeparating reflections and lighting using independent components analysis. H Farid, E H Adelson, CVPR. H. Farid and E. H. Adelson. Separating reflections and light- ing using independent components analysis. In CVPR, 1999. 2\n\ndouble-dip\": Unsupervised image decomposition via coupled deep-image-priors. Yossi Gandelsman, Assaf Shocher, Michal Irani, CVPR. Yossi Gandelsman, Assaf Shocher, and Michal Irani. \"double-dip\": Unsupervised image decomposition via cou- pled deep-image-priors. In CVPR, 2019. 2\n\nRobust separation of reflection from multiple images. Xiaojie Guo, Xiaochun Cao, Yi Ma, CVPR. Xiaojie Guo, Xiaochun Cao, and Yi Ma. Robust separation of reflection from multiple images. In CVPR, 2014. 2\n\nReflection removal using low-rank matrix completion. Byeong-Ju Han, Jae-Young Sim, CVPR. Byeong-Ju Han and Jae-Young Sim. Reflection removal us- ing low-rank matrix completion. In CVPR, 2017. 2\n\nNear-infrared image guided reflection removal. Y Hong, Y Lyu, S Li, B Shi, ICME. 2020Y. Hong, Y. Lyu, S. Li, and B. Shi. Near-infrared image guided reflection removal. In ICME, 2020. 2\n\nSingle image reflection removal with physically-based training images. Soomin Kim, Yuchi Huo, Sung-Eui Yoon, CVPR. 6Soomin Kim, Yuchi Huo, and Sung-Eui Yoon. Single image reflection removal with physically-based training images. In CVPR, 2020. 2, 5, 6\n\nA physically-based approach to reflection separation: from physical modeling to constrained optimization. Naejin Kong, Yu-Wing Tai, Joseph S Shin, TPAMI. 2Naejin Kong, Yu-Wing Tai, and Joseph S. Shin. A physically-based approach to reflection separation: from physical modeling to constrained optimization. TPAMI, 2014. 2\n\nRobust reflection removal with reflection-free flash-only cues. Chenyang Lei, Qifeng Chen, CVPR. Chenyang Lei and Qifeng Chen. Robust reflection removal with reflection-free flash-only cues. In CVPR, 2021. 2\n\nPolarized reflection removal with perfect alignment in the wild. Chenyang Lei, Xuhua Huang, Mengdi Zhang, Wenxiu Sun, Qiong Yan, Qifeng Chen, CVPR, 2020. 1. 24Chenyang Lei, Xuhua Huang, Mengdi Zhang, Wenxiu Sun, Qiong Yan, and Qifeng Chen. Polarized reflection removal with perfect alignment in the wild. In CVPR, 2020. 1, 2, 4\n\nSingle image reflection removal through cascaded refinement. Chao Li, Yixiao Yang, Kun He, Stephen Lin, John E Hopcroft, CVPR. 56Chao Li, Yixiao Yang, Kun He, Stephen Lin, and John E. Hopcroft. Single image reflection removal through cascaded refinement. In CVPR, 2020. 1, 3, 5, 6\n\nExploiting reflection change for automatic reflection removal. Yu Li, Michael S Brown, ICCV. Yu Li and Michael S Brown. Exploiting reflection change for automatic reflection removal. In ICCV, 2013. 2\n\nSingle image layer separation using relative smoothness. Yu Li, Michael S Brown, CVPR. 67Yu Li and Michael S Brown. Single image layer separation using relative smoothness. In CVPR, 2014. 5, 6, 7\n\nLearning to see through obstructions. Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, Jia-Bin Huang, CVPR. 2020Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, and Jia-Bin Huang. Learning to see through ob- structions. In CVPR, 2020. 2\n\nReflection separation using a pair of unpolarized and polarized images. Youwei Lyu, Zhaopeng Cui, Si Li, Marc Pollefeys, Boxin Shi, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editorsCurran Associates, Inc32Youwei Lyu, Zhaopeng Cui, Si Li, Marc Pollefeys, and Boxin Shi. Reflection separation using a pair of unpolar- ized and polarized images. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, ed- itors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. 2\n\nLearning to jointly generate and separate reflections. Daiqian Ma, Renjie Wan, Boxin Shi, Alex C Kot, Ling-Yu Duan, ICCV. Daiqian Ma, Renjie Wan, Boxin Shi, Alex C. Kot, and Ling- Yu Duan. Learning to jointly generate and separate reflec- tions. In ICCV, 2019. 2\n\nNeural camera simulators. Hao Ouyang, Zifan Shi, Chenyang Lei, Ka Lung Law, Qifeng Chen, CVPR. Hao Ouyang, Zifan Shi, Chenyang Lei, Ka Lung Law, and Qifeng Chen. Neural camera simulators. In CVPR, 2021. 8\n\nSeparating reflection and transmission images in the wild. Wieschollek Patrick, Gallo Orazio, Gu Jinwei, Kautz , ECCV. Wieschollek Patrick, Gallo Orazio, Gu Jinwei, and Kautz Jan. Separating reflection and transmission images in the wild. In ECCV, 2018. 2\n\nReflection removal using a dual-pixel sensor. Abhijith Punnappurath, Michael S Brown, CVPR. Abhijith Punnappurath and Michael S. Brown. Reflection removal using a dual-pixel sensor. In CVPR, 2019. 2\n\nSeparating transparent layers through layer information exchange. Bernard Sarel, Michal Irani, ECCV. Bernard Sarel and Michal Irani. Separating transparent lay- ers through layer information exchange. In ECCV, 2004. 2\n\nSeparating transparent layers of repetitive dynamic behaviors. Bernard Sarel, Michal Irani, ICCV. Bernard Sarel and Michal Irani. Separating transparent lay- ers of repetitive dynamic behaviors. In ICCV, 2005. 2\n\nPolarization and statistical analysis of scenes containing a semireflector. Yoav Schechner, Joseph Shamir, Nahum Kiryati, Optics, image science, and vision. Yoav Schechner, Joseph Shamir, and Nahum Kiryati. Polar- ization and statistical analysis of scenes containing a semire- flector. Journal of the Optical Society of America. A, Optics, image science, and vision, 2000. 2\n\nAutomatic reflection removal using gradient intensity and motion cues. Chao Sun, Shuaicheng Liu, Taotao Yang, Bing Zeng, Zhengning Wang, Guanghui Liu, ACM-MM. Chao Sun, Shuaicheng Liu, Taotao Yang, Bing Zeng, Zhengning Wang, and Guanghui Liu. Automatic reflection removal using gradient intensity and motion cues. In ACM- MM, 2016. 2\n\nLayer extraction from multiple images containing reflections and transparency. Richard Szeliski, P Shai Avidan, Anandan, CVPR. Richard Szeliski, Shai Avidan, and P Anandan. Layer extrac- tion from multiple images containing reflections and trans- parency. In CVPR, 2000. 2\n\nBenchmarking single-image reflection removal algorithms. Renjie Wan, Boxin Shi, Ling-Yu Duan, Ah-Hwee Tan, Alex C Kot, ICCV. Renjie Wan, Boxin Shi, Ling-Yu Duan, Ah-Hwee Tan, and Alex C Kot. Benchmarking single-image reflection removal algorithms. In ICCV, 2017. 1, 3, 4, 5\n\nCorrn: Cooperative reflection removal network. Renjie Wan, Boxin Shi, Haoliang Li, Ling-Yu Duan, Ah-Hwee Tan, Alex Kot Chichung, TPAMI. 67Renjie Wan, Boxin Shi, Haoliang Li, Ling-Yu Duan, Ah- Hwee Tan, and Alex Kot Chichung. Corrn: Cooperative re- flection removal network. TPAMI, 2019. 2, 5, 6, 7\n\nAutomatic layer separation using light field imaging. Qiaosong Wang, Haiting Lin, Yi Ma, Bing Sing, Jingyi Kang, Yu, arXiv:1506.04721arXiv preprintQiaosong Wang, Haiting Lin, Yi Ma, Sing Bing Kang, and Jingyi Yu. Automatic layer separation using light field imag- ing. arXiv preprint arXiv:1506.04721, 2015. 2\n\nSingle image reflection removal exploiting misaligned training data and network enhancements. Kaixuan Wei, Jiaolong Yang, Ying Fu, David Wipf, Hua Huang, CVPR. 67Kaixuan Wei, Jiaolong Yang, Ying Fu, David Wipf, and Hua Huang. Single image reflection removal exploiting mis- aligned training data and network enhancements. In CVPR, 2019. 2, 3, 4, 5, 6, 7\n\nSingle image reflection removal beyond linearity. Qiang Wen, Yinjie Tan, Jing Qin, Wenxi Liu, Guoqiang Han, Shengfeng He, CVPR. Qiang Wen, Yinjie Tan, Jing Qin, Wenxi Liu, Guoqiang Han, and Shengfeng He. Single image reflection removal beyond linearity. In CVPR, 2019. 2\n\nInvertible image signal processing. Yazhou Xing, Zian Qian, Qifeng Chen, CVPR. Yazhou Xing, Zian Qian, and Qifeng Chen. Invertible image signal processing. In CVPR, 2021. 8\n\nTowards real scene super-resolution with raw images. Xiangyu Xu, Yongrui Ma, Wenxiu Sun, CVPR. Xiangyu Xu, Yongrui Ma, and Wenxiu Sun. Towards real scene super-resolution with raw images. In CVPR, 2019. 8\n\nA computational approach for obstruction-free photography. Tianfan Xue, Michael Rubinstein, Ce Liu, William T Freeman, TOG. 2Tianfan Xue, Michael Rubinstein, Ce Liu, and William T Freeman. A computational approach for obstruction-free photography. TOG, 2015. 2\n\nSeeing deeply and bidirectionally: a deep learning approach for single image reflection removal. Jie Yang, Dong Gong, Lingqiao Liu, Qinfeng Shi, ECCV. 67Jie Yang, Dong Gong, Lingqiao Liu, and Qinfeng Shi. See- ing deeply and bidirectionally: a deep learning approach for single image reflection removal. In ECCV, 2018. 2, 4, 5, 6, 7\n\nFast single image reflection suppression via convex optimization. Yang Yang, Wenye Ma, Yin Zheng, Jian-Feng Cai, Weiyu Xu, CVPR. 67Yang Yang, Wenye Ma, Yin Zheng, Jian-Feng Cai, and Weiyu Xu. Fast single image reflection suppression via con- vex optimization. In CVPR, 2019. 2, 5, 6, 7\n\nCycleisp: Real image restoration via improved data synthesis. Aditya Syed Waqas Zamir, Salman Arora, Munawar Khan, Hayat, Ming-Hsuan Fahad Shahbaz Khan, Ling Yang, Shao, CVPR. Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Cycleisp: Real image restoration via improved data synthesis. In CVPR, 2020. 8\n\nSingle image reflection separation with perceptual losses. Xuaner Zhang, Ren Ng, Qifeng Chen, CVPR. 67Xuaner Zhang, Ren Ng, and Qifeng Chen. Single image reflection separation with perceptual losses. In CVPR, 2018. 1, 2, 3, 4, 5, 6, 7\n", "annotations": {"author": "[{\"end\":88,\"start\":75},{\"end\":101,\"start\":89},{\"end\":114,\"start\":102},{\"end\":127,\"start\":115},{\"end\":139,\"start\":128},{\"end\":150,\"start\":140},{\"end\":163,\"start\":151},{\"end\":176,\"start\":164},{\"end\":187,\"start\":177}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":84},{\"end\":100,\"start\":95},{\"end\":113,\"start\":111},{\"end\":126,\"start\":122},{\"end\":138,\"start\":135},{\"end\":149,\"start\":146},{\"end\":162,\"start\":158},{\"end\":175,\"start\":172},{\"end\":186,\"start\":177}]", "author_first_name": "[{\"end\":83,\"start\":75},{\"end\":94,\"start\":89},{\"end\":110,\"start\":102},{\"end\":121,\"start\":115},{\"end\":134,\"start\":128},{\"end\":145,\"start\":140},{\"end\":157,\"start\":151},{\"end\":169,\"start\":164},{\"end\":171,\"start\":170}]", "author_affiliation": null, "title": "[{\"end\":72,\"start\":1},{\"end\":259,\"start\":188}]", "venue": null, "abstract": "[{\"end\":1585,\"start\":261}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1954,\"start\":1951},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1957,\"start\":1954},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2427,\"start\":2423},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2866,\"start\":2862},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2869,\"start\":2866},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3654,\"start\":3650},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4429,\"start\":4425},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4445,\"start\":4441},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4456,\"start\":4452},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4465,\"start\":4461},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4481,\"start\":4477},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4771,\"start\":4768},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4774,\"start\":4771},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4777,\"start\":4774},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4780,\"start\":4777},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4864,\"start\":4861},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4933,\"start\":4929},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5111,\"start\":5107},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5173,\"start\":5170},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5192,\"start\":5188},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5206,\"start\":5202},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5311,\"start\":5308},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5417,\"start\":5413},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5527,\"start\":5523},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5634,\"start\":5630},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5636,\"start\":5634},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5639,\"start\":5636},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5886,\"start\":5882},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5906,\"start\":5902},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6085,\"start\":6081},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6246,\"start\":6242},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6265,\"start\":6261},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6387,\"start\":6383},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6719,\"start\":6715},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6722,\"start\":6719},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6725,\"start\":6722},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6728,\"start\":6725},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6730,\"start\":6728},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6732,\"start\":6730},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6735,\"start\":6732},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6738,\"start\":6735},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6740,\"start\":6738},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6743,\"start\":6740},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7027,\"start\":7024},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7030,\"start\":7027},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7058,\"start\":7054},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7084,\"start\":7080},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7109,\"start\":7105},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7140,\"start\":7136},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7143,\"start\":7140},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7146,\"start\":7143},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7149,\"start\":7146},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7151,\"start\":7149},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7223,\"start\":7220},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7583,\"start\":7579},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7607,\"start\":7603},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7730,\"start\":7726},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7733,\"start\":7730},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7736,\"start\":7733},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7914,\"start\":7910},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8386,\"start\":8382},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9774,\"start\":9770},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9909,\"start\":9905},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9912,\"start\":9909},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9915,\"start\":9912},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10318,\"start\":10314},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10321,\"start\":10318},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10569,\"start\":10565},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11425,\"start\":11421},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11428,\"start\":11425},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11431,\"start\":11428},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11723,\"start\":11719},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11726,\"start\":11723},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12217,\"start\":12213},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12278,\"start\":12274},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12605,\"start\":12601},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12608,\"start\":12605},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12611,\"start\":12608},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16175,\"start\":16171},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16305,\"start\":16302},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16317,\"start\":16313},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16327,\"start\":16323},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":16346,\"start\":16342},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16363,\"start\":16359},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16381,\"start\":16377},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16408,\"start\":16405},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16424,\"start\":16420},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16436,\"start\":16432},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16457,\"start\":16453},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16647,\"start\":16643},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17331,\"start\":17328},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17334,\"start\":17331},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17669,\"start\":17665},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17672,\"start\":17669},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17770,\"start\":17766},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17993,\"start\":17989},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17996,\"start\":17993},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17999,\"start\":17996},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19366,\"start\":19362},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19392,\"start\":19389},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19404,\"start\":19401},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19415,\"start\":19411},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19433,\"start\":19429},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19442,\"start\":19438},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19458,\"start\":19454},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19490,\"start\":19486},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19506,\"start\":19502},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19517,\"start\":19513},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19526,\"start\":19522},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19543,\"start\":19539},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19558,\"start\":19554},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20086,\"start\":20082},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20088,\"start\":20086},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21743,\"start\":21740},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21766,\"start\":21762},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21788,\"start\":21784},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21801,\"start\":21797},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21804,\"start\":21801},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23472,\"start\":23468}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":23112,\"start\":22851},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23241,\"start\":23113},{\"attributes\":{\"id\":\"fig_3\"},\"end\":23410,\"start\":23242},{\"attributes\":{\"id\":\"fig_4\"},\"end\":23626,\"start\":23411},{\"attributes\":{\"id\":\"fig_5\"},\"end\":24272,\"start\":23627},{\"attributes\":{\"id\":\"fig_6\"},\"end\":24320,\"start\":24273},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":24727,\"start\":24321}]", "paragraph": "[{\"end\":2278,\"start\":1601},{\"end\":2943,\"start\":2280},{\"end\":4152,\"start\":2945},{\"end\":4674,\"start\":4154},{\"end\":6503,\"start\":4723},{\"end\":7164,\"start\":6547},{\"end\":7379,\"start\":7166},{\"end\":7822,\"start\":7381},{\"end\":7871,\"start\":7824},{\"end\":8353,\"start\":7899},{\"end\":8998,\"start\":8355},{\"end\":9291,\"start\":9014},{\"end\":9551,\"start\":9293},{\"end\":11091,\"start\":9553},{\"end\":11387,\"start\":11093},{\"end\":11672,\"start\":11389},{\"end\":12075,\"start\":11674},{\"end\":12901,\"start\":12095},{\"end\":12953,\"start\":12903},{\"end\":13051,\"start\":12955},{\"end\":13212,\"start\":13053},{\"end\":13332,\"start\":13214},{\"end\":13457,\"start\":13334},{\"end\":14000,\"start\":13477},{\"end\":14401,\"start\":14002},{\"end\":15106,\"start\":14403},{\"end\":15980,\"start\":15108},{\"end\":16237,\"start\":15996},{\"end\":16834,\"start\":16239},{\"end\":17002,\"start\":16849},{\"end\":17549,\"start\":17004},{\"end\":17627,\"start\":17551},{\"end\":18063,\"start\":17629},{\"end\":18533,\"start\":18065},{\"end\":18754,\"start\":18535},{\"end\":19136,\"start\":18756},{\"end\":19333,\"start\":19138},{\"end\":20160,\"start\":19350},{\"end\":20370,\"start\":20162},{\"end\":20669,\"start\":20403},{\"end\":21029,\"start\":20671},{\"end\":21446,\"start\":21031},{\"end\":22012,\"start\":21483},{\"end\":22850,\"start\":22027}]", "formula": null, "table_ref": "[{\"end\":3183,\"start\":3176},{\"end\":9826,\"start\":9819},{\"end\":17033,\"start\":17026}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1599,\"start\":1587},{\"attributes\":{\"n\":\"2.\"},\"end\":4687,\"start\":4677},{\"attributes\":{\"n\":\"2.1.\"},\"end\":4721,\"start\":4690},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6545,\"start\":6506},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7897,\"start\":7874},{\"attributes\":{\"n\":\"3.\"},\"end\":9012,\"start\":9001},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12093,\"start\":12078},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13475,\"start\":13460},{\"attributes\":{\"n\":\"4.\"},\"end\":15994,\"start\":15983},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16847,\"start\":16837},{\"end\":19340,\"start\":19336},{\"end\":19348,\"start\":19343},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20401,\"start\":20373},{\"attributes\":{\"n\":\"4.3.\"},\"end\":21481,\"start\":21449},{\"attributes\":{\"n\":\"5.\"},\"end\":22025,\"start\":22015},{\"end\":22862,\"start\":22852},{\"end\":23251,\"start\":23243},{\"end\":23422,\"start\":23412},{\"end\":23656,\"start\":23628}]", "table": "[{\"end\":24727,\"start\":24424}]", "figure_caption": "[{\"end\":23112,\"start\":22864},{\"end\":23241,\"start\":23115},{\"end\":23410,\"start\":23253},{\"end\":23626,\"start\":23424},{\"end\":24272,\"start\":23660},{\"end\":24320,\"start\":24275},{\"end\":24424,\"start\":24323}]", "figure_ref": "[{\"end\":1912,\"start\":1906},{\"end\":4490,\"start\":4482},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10842,\"start\":10836},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12432,\"start\":12426},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12867,\"start\":12861},{\"end\":13484,\"start\":13481},{\"end\":13769,\"start\":13763},{\"end\":19199,\"start\":19193},{\"end\":19567,\"start\":19559},{\"end\":20253,\"start\":20247}]", "bib_author_first_name": "[{\"end\":24819,\"start\":24815},{\"end\":24835,\"start\":24829},{\"end\":24845,\"start\":24844},{\"end\":24861,\"start\":24853},{\"end\":25112,\"start\":25108},{\"end\":25142,\"start\":25136},{\"end\":25362,\"start\":25354},{\"end\":25391,\"start\":25379},{\"end\":25407,\"start\":25401},{\"end\":25593,\"start\":25589},{\"end\":25606,\"start\":25600},{\"end\":25616,\"start\":25613},{\"end\":25628,\"start\":25621},{\"end\":25834,\"start\":25827},{\"end\":25848,\"start\":25840},{\"end\":25859,\"start\":25855},{\"end\":25872,\"start\":25865},{\"end\":25884,\"start\":25879},{\"end\":26158,\"start\":26157},{\"end\":26167,\"start\":26166},{\"end\":26169,\"start\":26168},{\"end\":26390,\"start\":26385},{\"end\":26408,\"start\":26403},{\"end\":26424,\"start\":26418},{\"end\":26648,\"start\":26641},{\"end\":26662,\"start\":26654},{\"end\":26670,\"start\":26668},{\"end\":26853,\"start\":26844},{\"end\":26868,\"start\":26859},{\"end\":27034,\"start\":27033},{\"end\":27042,\"start\":27041},{\"end\":27049,\"start\":27048},{\"end\":27055,\"start\":27054},{\"end\":27249,\"start\":27243},{\"end\":27260,\"start\":27255},{\"end\":27274,\"start\":27266},{\"end\":27537,\"start\":27531},{\"end\":27551,\"start\":27544},{\"end\":27563,\"start\":27557},{\"end\":27565,\"start\":27564},{\"end\":27820,\"start\":27812},{\"end\":27832,\"start\":27826},{\"end\":28030,\"start\":28022},{\"end\":28041,\"start\":28036},{\"end\":28055,\"start\":28049},{\"end\":28069,\"start\":28063},{\"end\":28080,\"start\":28075},{\"end\":28092,\"start\":28086},{\"end\":28351,\"start\":28347},{\"end\":28362,\"start\":28356},{\"end\":28372,\"start\":28369},{\"end\":28384,\"start\":28377},{\"end\":28394,\"start\":28390},{\"end\":28396,\"start\":28395},{\"end\":28633,\"start\":28631},{\"end\":28828,\"start\":28826},{\"end\":29010,\"start\":29004},{\"end\":29025,\"start\":29016},{\"end\":29041,\"start\":29031},{\"end\":29055,\"start\":29048},{\"end\":29071,\"start\":29064},{\"end\":29304,\"start\":29298},{\"end\":29318,\"start\":29310},{\"end\":29326,\"start\":29324},{\"end\":29335,\"start\":29331},{\"end\":29352,\"start\":29347},{\"end\":29916,\"start\":29909},{\"end\":29927,\"start\":29921},{\"end\":29938,\"start\":29933},{\"end\":29948,\"start\":29944},{\"end\":29950,\"start\":29949},{\"end\":29963,\"start\":29956},{\"end\":30147,\"start\":30144},{\"end\":30161,\"start\":30156},{\"end\":30175,\"start\":30167},{\"end\":30183,\"start\":30181},{\"end\":30188,\"start\":30184},{\"end\":30200,\"start\":30194},{\"end\":30394,\"start\":30383},{\"end\":30409,\"start\":30404},{\"end\":30420,\"start\":30418},{\"end\":30434,\"start\":30429},{\"end\":30635,\"start\":30627},{\"end\":30657,\"start\":30650},{\"end\":30659,\"start\":30658},{\"end\":30854,\"start\":30847},{\"end\":30868,\"start\":30862},{\"end\":31070,\"start\":31063},{\"end\":31084,\"start\":31078},{\"end\":31293,\"start\":31289},{\"end\":31311,\"start\":31305},{\"end\":31325,\"start\":31320},{\"end\":31665,\"start\":31661},{\"end\":31681,\"start\":31671},{\"end\":31693,\"start\":31687},{\"end\":31704,\"start\":31700},{\"end\":31720,\"start\":31711},{\"end\":31735,\"start\":31727},{\"end\":32011,\"start\":32004},{\"end\":32023,\"start\":32022},{\"end\":32262,\"start\":32256},{\"end\":32273,\"start\":32268},{\"end\":32286,\"start\":32279},{\"end\":32300,\"start\":32293},{\"end\":32310,\"start\":32306},{\"end\":32312,\"start\":32311},{\"end\":32527,\"start\":32521},{\"end\":32538,\"start\":32533},{\"end\":32552,\"start\":32544},{\"end\":32564,\"start\":32557},{\"end\":32578,\"start\":32571},{\"end\":32588,\"start\":32584},{\"end\":32592,\"start\":32589},{\"end\":32835,\"start\":32827},{\"end\":32849,\"start\":32842},{\"end\":32857,\"start\":32855},{\"end\":32866,\"start\":32862},{\"end\":32879,\"start\":32873},{\"end\":33185,\"start\":33178},{\"end\":33199,\"start\":33191},{\"end\":33210,\"start\":33206},{\"end\":33220,\"start\":33215},{\"end\":33230,\"start\":33227},{\"end\":33494,\"start\":33489},{\"end\":33506,\"start\":33500},{\"end\":33516,\"start\":33512},{\"end\":33527,\"start\":33522},{\"end\":33541,\"start\":33533},{\"end\":33556,\"start\":33547},{\"end\":33753,\"start\":33747},{\"end\":33764,\"start\":33760},{\"end\":33777,\"start\":33771},{\"end\":33945,\"start\":33938},{\"end\":33957,\"start\":33950},{\"end\":33968,\"start\":33962},{\"end\":34157,\"start\":34150},{\"end\":34170,\"start\":34163},{\"end\":34185,\"start\":34183},{\"end\":34200,\"start\":34191},{\"end\":34453,\"start\":34450},{\"end\":34464,\"start\":34460},{\"end\":34479,\"start\":34471},{\"end\":34492,\"start\":34485},{\"end\":34757,\"start\":34753},{\"end\":34769,\"start\":34764},{\"end\":34777,\"start\":34774},{\"end\":34794,\"start\":34785},{\"end\":34805,\"start\":34800},{\"end\":35042,\"start\":35036},{\"end\":35067,\"start\":35061},{\"end\":35082,\"start\":35075},{\"end\":35106,\"start\":35096},{\"end\":35131,\"start\":35127},{\"end\":35407,\"start\":35401},{\"end\":35418,\"start\":35415},{\"end\":35429,\"start\":35423}]", "bib_author_last_name": "[{\"end\":24827,\"start\":24820},{\"end\":24842,\"start\":24836},{\"end\":24851,\"start\":24846},{\"end\":24867,\"start\":24862},{\"end\":24871,\"start\":24869},{\"end\":25134,\"start\":25113},{\"end\":25151,\"start\":25143},{\"end\":25162,\"start\":25153},{\"end\":25377,\"start\":25363},{\"end\":25399,\"start\":25392},{\"end\":25417,\"start\":25408},{\"end\":25598,\"start\":25594},{\"end\":25611,\"start\":25607},{\"end\":25619,\"start\":25617},{\"end\":25635,\"start\":25629},{\"end\":25838,\"start\":25835},{\"end\":25853,\"start\":25849},{\"end\":25863,\"start\":25860},{\"end\":25877,\"start\":25873},{\"end\":25889,\"start\":25885},{\"end\":26164,\"start\":26159},{\"end\":26177,\"start\":26170},{\"end\":26401,\"start\":26391},{\"end\":26416,\"start\":26409},{\"end\":26430,\"start\":26425},{\"end\":26652,\"start\":26649},{\"end\":26666,\"start\":26663},{\"end\":26673,\"start\":26671},{\"end\":26857,\"start\":26854},{\"end\":26872,\"start\":26869},{\"end\":27039,\"start\":27035},{\"end\":27046,\"start\":27043},{\"end\":27052,\"start\":27050},{\"end\":27059,\"start\":27056},{\"end\":27253,\"start\":27250},{\"end\":27264,\"start\":27261},{\"end\":27279,\"start\":27275},{\"end\":27542,\"start\":27538},{\"end\":27555,\"start\":27552},{\"end\":27570,\"start\":27566},{\"end\":27824,\"start\":27821},{\"end\":27837,\"start\":27833},{\"end\":28034,\"start\":28031},{\"end\":28047,\"start\":28042},{\"end\":28061,\"start\":28056},{\"end\":28073,\"start\":28070},{\"end\":28084,\"start\":28081},{\"end\":28097,\"start\":28093},{\"end\":28354,\"start\":28352},{\"end\":28367,\"start\":28363},{\"end\":28375,\"start\":28373},{\"end\":28388,\"start\":28385},{\"end\":28405,\"start\":28397},{\"end\":28636,\"start\":28634},{\"end\":28653,\"start\":28638},{\"end\":28831,\"start\":28829},{\"end\":28848,\"start\":28833},{\"end\":29014,\"start\":29011},{\"end\":29029,\"start\":29026},{\"end\":29046,\"start\":29042},{\"end\":29062,\"start\":29056},{\"end\":29077,\"start\":29072},{\"end\":29308,\"start\":29305},{\"end\":29322,\"start\":29319},{\"end\":29329,\"start\":29327},{\"end\":29345,\"start\":29336},{\"end\":29356,\"start\":29353},{\"end\":29919,\"start\":29917},{\"end\":29931,\"start\":29928},{\"end\":29942,\"start\":29939},{\"end\":29954,\"start\":29951},{\"end\":29968,\"start\":29964},{\"end\":30154,\"start\":30148},{\"end\":30165,\"start\":30162},{\"end\":30179,\"start\":30176},{\"end\":30192,\"start\":30189},{\"end\":30205,\"start\":30201},{\"end\":30402,\"start\":30395},{\"end\":30416,\"start\":30410},{\"end\":30427,\"start\":30421},{\"end\":30648,\"start\":30636},{\"end\":30665,\"start\":30660},{\"end\":30860,\"start\":30855},{\"end\":30874,\"start\":30869},{\"end\":31076,\"start\":31071},{\"end\":31090,\"start\":31085},{\"end\":31303,\"start\":31294},{\"end\":31318,\"start\":31312},{\"end\":31333,\"start\":31326},{\"end\":31669,\"start\":31666},{\"end\":31685,\"start\":31682},{\"end\":31698,\"start\":31694},{\"end\":31709,\"start\":31705},{\"end\":31725,\"start\":31721},{\"end\":31739,\"start\":31736},{\"end\":32020,\"start\":32012},{\"end\":32035,\"start\":32024},{\"end\":32044,\"start\":32037},{\"end\":32266,\"start\":32263},{\"end\":32277,\"start\":32274},{\"end\":32291,\"start\":32287},{\"end\":32304,\"start\":32301},{\"end\":32316,\"start\":32313},{\"end\":32531,\"start\":32528},{\"end\":32542,\"start\":32539},{\"end\":32555,\"start\":32553},{\"end\":32569,\"start\":32565},{\"end\":32582,\"start\":32579},{\"end\":32601,\"start\":32593},{\"end\":32840,\"start\":32836},{\"end\":32853,\"start\":32850},{\"end\":32860,\"start\":32858},{\"end\":32871,\"start\":32867},{\"end\":32884,\"start\":32880},{\"end\":32888,\"start\":32886},{\"end\":33189,\"start\":33186},{\"end\":33204,\"start\":33200},{\"end\":33213,\"start\":33211},{\"end\":33225,\"start\":33221},{\"end\":33236,\"start\":33231},{\"end\":33498,\"start\":33495},{\"end\":33510,\"start\":33507},{\"end\":33520,\"start\":33517},{\"end\":33531,\"start\":33528},{\"end\":33545,\"start\":33542},{\"end\":33559,\"start\":33557},{\"end\":33758,\"start\":33754},{\"end\":33769,\"start\":33765},{\"end\":33782,\"start\":33778},{\"end\":33948,\"start\":33946},{\"end\":33960,\"start\":33958},{\"end\":33972,\"start\":33969},{\"end\":34161,\"start\":34158},{\"end\":34181,\"start\":34171},{\"end\":34189,\"start\":34186},{\"end\":34208,\"start\":34201},{\"end\":34458,\"start\":34454},{\"end\":34469,\"start\":34465},{\"end\":34483,\"start\":34480},{\"end\":34496,\"start\":34493},{\"end\":34762,\"start\":34758},{\"end\":34772,\"start\":34770},{\"end\":34783,\"start\":34778},{\"end\":34798,\"start\":34795},{\"end\":34808,\"start\":34806},{\"end\":35059,\"start\":35043},{\"end\":35073,\"start\":35068},{\"end\":35087,\"start\":35083},{\"end\":35094,\"start\":35089},{\"end\":35125,\"start\":35107},{\"end\":35136,\"start\":35132},{\"end\":35142,\"start\":35138},{\"end\":35413,\"start\":35408},{\"end\":35421,\"start\":35419},{\"end\":35434,\"start\":35430}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":149024},\"end\":25041,\"start\":24729},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":54444725},\"end\":25315,\"start\":25043},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13095034},\"end\":25558,\"start\":25317},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4691825},\"end\":25740,\"start\":25560},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8621123},\"end\":26080,\"start\":25742},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":74822},\"end\":26306,\"start\":26082},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":54437326},\"end\":26585,\"start\":26308},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8976032},\"end\":26789,\"start\":26587},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":31926300},\"end\":26984,\"start\":26791},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":221119784},\"end\":27170,\"start\":26986},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":219631747},\"end\":27423,\"start\":27172},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2033459},\"end\":27746,\"start\":27425},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":232146879},\"end\":27955,\"start\":27748},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":214713439},\"end\":28284,\"start\":27957},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":208076308},\"end\":28566,\"start\":28286},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15044065},\"end\":28767,\"start\":28568},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16165913},\"end\":28964,\"start\":28769},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":214774925},\"end\":29224,\"start\":28966},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":202780994},\"end\":29852,\"start\":29226},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":208005242},\"end\":30116,\"start\":29854},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":233210074},\"end\":30322,\"start\":30118},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":34766267},\"end\":30579,\"start\":30324},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":198365186},\"end\":30779,\"start\":30581},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14798441},\"end\":30998,\"start\":30781},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":5897870},\"end\":31211,\"start\":31000},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1900248},\"end\":31588,\"start\":31213},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4255150},\"end\":31923,\"start\":31590},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8798317},\"end\":32197,\"start\":31925},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6158161},\"end\":32472,\"start\":32199},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":184485819},\"end\":32771,\"start\":32474},{\"attributes\":{\"doi\":\"arXiv:1506.04721\",\"id\":\"b30\"},\"end\":33082,\"start\":32773},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":90259427},\"end\":33437,\"start\":33084},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":198333802},\"end\":33709,\"start\":33439},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":232404439},\"end\":33883,\"start\":33711},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":168169764},\"end\":34089,\"start\":33885},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":7600506},\"end\":34351,\"start\":34091},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52957534},\"end\":34685,\"start\":34353},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":73728864},\"end\":34972,\"start\":34687},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":212737191},\"end\":35340,\"start\":34974},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":49209675},\"end\":35576,\"start\":35342}]", "bib_title": "[{\"end\":24813,\"start\":24729},{\"end\":25106,\"start\":25043},{\"end\":25352,\"start\":25317},{\"end\":25587,\"start\":25560},{\"end\":25825,\"start\":25742},{\"end\":26155,\"start\":26082},{\"end\":26383,\"start\":26308},{\"end\":26639,\"start\":26587},{\"end\":26842,\"start\":26791},{\"end\":27031,\"start\":26986},{\"end\":27241,\"start\":27172},{\"end\":27529,\"start\":27425},{\"end\":27810,\"start\":27748},{\"end\":28020,\"start\":27957},{\"end\":28345,\"start\":28286},{\"end\":28629,\"start\":28568},{\"end\":28824,\"start\":28769},{\"end\":29002,\"start\":28966},{\"end\":29296,\"start\":29226},{\"end\":29907,\"start\":29854},{\"end\":30142,\"start\":30118},{\"end\":30381,\"start\":30324},{\"end\":30625,\"start\":30581},{\"end\":30845,\"start\":30781},{\"end\":31061,\"start\":31000},{\"end\":31287,\"start\":31213},{\"end\":31659,\"start\":31590},{\"end\":32002,\"start\":31925},{\"end\":32254,\"start\":32199},{\"end\":32519,\"start\":32474},{\"end\":33176,\"start\":33084},{\"end\":33487,\"start\":33439},{\"end\":33745,\"start\":33711},{\"end\":33936,\"start\":33885},{\"end\":34148,\"start\":34091},{\"end\":34448,\"start\":34353},{\"end\":34751,\"start\":34687},{\"end\":35034,\"start\":34974},{\"end\":35399,\"start\":35342}]", "bib_author": "[{\"end\":24829,\"start\":24815},{\"end\":24844,\"start\":24829},{\"end\":24853,\"start\":24844},{\"end\":24869,\"start\":24853},{\"end\":24873,\"start\":24869},{\"end\":25136,\"start\":25108},{\"end\":25153,\"start\":25136},{\"end\":25164,\"start\":25153},{\"end\":25379,\"start\":25354},{\"end\":25401,\"start\":25379},{\"end\":25419,\"start\":25401},{\"end\":25600,\"start\":25589},{\"end\":25613,\"start\":25600},{\"end\":25621,\"start\":25613},{\"end\":25637,\"start\":25621},{\"end\":25840,\"start\":25827},{\"end\":25855,\"start\":25840},{\"end\":25865,\"start\":25855},{\"end\":25879,\"start\":25865},{\"end\":25891,\"start\":25879},{\"end\":26166,\"start\":26157},{\"end\":26179,\"start\":26166},{\"end\":26403,\"start\":26385},{\"end\":26418,\"start\":26403},{\"end\":26432,\"start\":26418},{\"end\":26654,\"start\":26641},{\"end\":26668,\"start\":26654},{\"end\":26675,\"start\":26668},{\"end\":26859,\"start\":26844},{\"end\":26874,\"start\":26859},{\"end\":27041,\"start\":27033},{\"end\":27048,\"start\":27041},{\"end\":27054,\"start\":27048},{\"end\":27061,\"start\":27054},{\"end\":27255,\"start\":27243},{\"end\":27266,\"start\":27255},{\"end\":27281,\"start\":27266},{\"end\":27544,\"start\":27531},{\"end\":27557,\"start\":27544},{\"end\":27572,\"start\":27557},{\"end\":27826,\"start\":27812},{\"end\":27839,\"start\":27826},{\"end\":28036,\"start\":28022},{\"end\":28049,\"start\":28036},{\"end\":28063,\"start\":28049},{\"end\":28075,\"start\":28063},{\"end\":28086,\"start\":28075},{\"end\":28099,\"start\":28086},{\"end\":28356,\"start\":28347},{\"end\":28369,\"start\":28356},{\"end\":28377,\"start\":28369},{\"end\":28390,\"start\":28377},{\"end\":28407,\"start\":28390},{\"end\":28638,\"start\":28631},{\"end\":28655,\"start\":28638},{\"end\":28833,\"start\":28826},{\"end\":28850,\"start\":28833},{\"end\":29016,\"start\":29004},{\"end\":29031,\"start\":29016},{\"end\":29048,\"start\":29031},{\"end\":29064,\"start\":29048},{\"end\":29079,\"start\":29064},{\"end\":29310,\"start\":29298},{\"end\":29324,\"start\":29310},{\"end\":29331,\"start\":29324},{\"end\":29347,\"start\":29331},{\"end\":29358,\"start\":29347},{\"end\":29921,\"start\":29909},{\"end\":29933,\"start\":29921},{\"end\":29944,\"start\":29933},{\"end\":29956,\"start\":29944},{\"end\":29970,\"start\":29956},{\"end\":30156,\"start\":30144},{\"end\":30167,\"start\":30156},{\"end\":30181,\"start\":30167},{\"end\":30194,\"start\":30181},{\"end\":30207,\"start\":30194},{\"end\":30404,\"start\":30383},{\"end\":30418,\"start\":30404},{\"end\":30429,\"start\":30418},{\"end\":30437,\"start\":30429},{\"end\":30650,\"start\":30627},{\"end\":30667,\"start\":30650},{\"end\":30862,\"start\":30847},{\"end\":30876,\"start\":30862},{\"end\":31078,\"start\":31063},{\"end\":31092,\"start\":31078},{\"end\":31305,\"start\":31289},{\"end\":31320,\"start\":31305},{\"end\":31335,\"start\":31320},{\"end\":31671,\"start\":31661},{\"end\":31687,\"start\":31671},{\"end\":31700,\"start\":31687},{\"end\":31711,\"start\":31700},{\"end\":31727,\"start\":31711},{\"end\":31741,\"start\":31727},{\"end\":32022,\"start\":32004},{\"end\":32037,\"start\":32022},{\"end\":32046,\"start\":32037},{\"end\":32268,\"start\":32256},{\"end\":32279,\"start\":32268},{\"end\":32293,\"start\":32279},{\"end\":32306,\"start\":32293},{\"end\":32318,\"start\":32306},{\"end\":32533,\"start\":32521},{\"end\":32544,\"start\":32533},{\"end\":32557,\"start\":32544},{\"end\":32571,\"start\":32557},{\"end\":32584,\"start\":32571},{\"end\":32603,\"start\":32584},{\"end\":32842,\"start\":32827},{\"end\":32855,\"start\":32842},{\"end\":32862,\"start\":32855},{\"end\":32873,\"start\":32862},{\"end\":32886,\"start\":32873},{\"end\":32890,\"start\":32886},{\"end\":33191,\"start\":33178},{\"end\":33206,\"start\":33191},{\"end\":33215,\"start\":33206},{\"end\":33227,\"start\":33215},{\"end\":33238,\"start\":33227},{\"end\":33500,\"start\":33489},{\"end\":33512,\"start\":33500},{\"end\":33522,\"start\":33512},{\"end\":33533,\"start\":33522},{\"end\":33547,\"start\":33533},{\"end\":33561,\"start\":33547},{\"end\":33760,\"start\":33747},{\"end\":33771,\"start\":33760},{\"end\":33784,\"start\":33771},{\"end\":33950,\"start\":33938},{\"end\":33962,\"start\":33950},{\"end\":33974,\"start\":33962},{\"end\":34163,\"start\":34150},{\"end\":34183,\"start\":34163},{\"end\":34191,\"start\":34183},{\"end\":34210,\"start\":34191},{\"end\":34460,\"start\":34450},{\"end\":34471,\"start\":34460},{\"end\":34485,\"start\":34471},{\"end\":34498,\"start\":34485},{\"end\":34764,\"start\":34753},{\"end\":34774,\"start\":34764},{\"end\":34785,\"start\":34774},{\"end\":34800,\"start\":34785},{\"end\":34810,\"start\":34800},{\"end\":35061,\"start\":35036},{\"end\":35075,\"start\":35061},{\"end\":35089,\"start\":35075},{\"end\":35096,\"start\":35089},{\"end\":35127,\"start\":35096},{\"end\":35138,\"start\":35127},{\"end\":35144,\"start\":35138},{\"end\":35415,\"start\":35401},{\"end\":35423,\"start\":35415},{\"end\":35436,\"start\":35423}]", "bib_venue": "[{\"end\":24876,\"start\":24873},{\"end\":25168,\"start\":25164},{\"end\":25423,\"start\":25419},{\"end\":25641,\"start\":25637},{\"end\":25895,\"start\":25891},{\"end\":26183,\"start\":26179},{\"end\":26436,\"start\":26432},{\"end\":26679,\"start\":26675},{\"end\":26878,\"start\":26874},{\"end\":27065,\"start\":27061},{\"end\":27285,\"start\":27281},{\"end\":27577,\"start\":27572},{\"end\":27843,\"start\":27839},{\"end\":28112,\"start\":28099},{\"end\":28411,\"start\":28407},{\"end\":28659,\"start\":28655},{\"end\":28854,\"start\":28850},{\"end\":29083,\"start\":29079},{\"end\":29407,\"start\":29358},{\"end\":29974,\"start\":29970},{\"end\":30211,\"start\":30207},{\"end\":30441,\"start\":30437},{\"end\":30671,\"start\":30667},{\"end\":30880,\"start\":30876},{\"end\":31096,\"start\":31092},{\"end\":31368,\"start\":31335},{\"end\":31747,\"start\":31741},{\"end\":32050,\"start\":32046},{\"end\":32322,\"start\":32318},{\"end\":32608,\"start\":32603},{\"end\":32825,\"start\":32773},{\"end\":33242,\"start\":33238},{\"end\":33565,\"start\":33561},{\"end\":33788,\"start\":33784},{\"end\":33978,\"start\":33974},{\"end\":34213,\"start\":34210},{\"end\":34502,\"start\":34498},{\"end\":34814,\"start\":34810},{\"end\":35148,\"start\":35144},{\"end\":35440,\"start\":35436}]"}}}, "year": 2023, "month": 12, "day": 17}