{"id": 219687798, "updated": "2023-10-06 14:24:45.766", "metadata": {"title": "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning", "authors": "[{\"first\":\"Jean-Bastien\",\"last\":\"Grill\",\"middle\":[]},{\"first\":\"Florian\",\"last\":\"Strub\",\"middle\":[]},{\"first\":\"Florent\",\"last\":\"Altch'e\",\"middle\":[]},{\"first\":\"Corentin\",\"last\":\"Tallec\",\"middle\":[]},{\"first\":\"Pierre\",\"last\":\"Richemond\",\"middle\":[\"H.\"]},{\"first\":\"Elena\",\"last\":\"Buchatskaya\",\"middle\":[]},{\"first\":\"Carl\",\"last\":\"Doersch\",\"middle\":[]},{\"first\":\"Bernardo\",\"last\":\"Pires\",\"middle\":[\"Avila\"]},{\"first\":\"Zhaohan\",\"last\":\"Guo\",\"middle\":[\"Daniel\"]},{\"first\":\"Mohammad\",\"last\":\"Azar\",\"middle\":[\"Gheshlaghi\"]},{\"first\":\"Bilal\",\"last\":\"Piot\",\"middle\":[]},{\"first\":\"Koray\",\"last\":\"Kavukcuoglu\",\"middle\":[]},{\"first\":\"R'emi\",\"last\":\"Munos\",\"middle\":[]},{\"first\":\"Michal\",\"last\":\"Valko\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 13}, "abstract": "We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\\%$ top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2006.07733", "mag": "3101821705", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/GrillSATRBDPGAP20", "doi": null}}, "content": {"source": {"pdf_hash": "38f93092ece8eee9771e61c1edaf11b1293cae1b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.07733v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "eb2d8ade73f70eb3a82ca12435e7bb9c1ba7c521", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/38f93092ece8eee9771e61c1edaf11b1293cae1b.txt", "contents": "\nBootstrap Your Own Latent A New Approach to Self-Supervised Learning\n\n\nJean-Bastien Grill [jbgrill@google.com \nImperial College\n\n\nFlorian Strub fstrub@google.com \nImperial College\n\n\nFlorent Altch\u00e9 altche@google.com \nImperial College\n\n\nCorentin Tallec corentint@google.com \nImperial College\n\n\nPierre H Richemond richemond]@google.com \nImperial College\n\n\nElena Buchatskaya \nImperial College\n\n\nCarl Doersch \nImperial College\n\n\nBernardo Avila Pires \nImperial College\n\n\nZhaohan Daniel Guo \nImperial College\n\n\nMohammad Gheshlaghi Azar \nImperial College\n\n\nBilal Piot \nImperial College\n\n\nKoray Kavukcuoglu \nImperial College\n\n\nR\u00e9mi Munos \nImperial College\n\n\nMichal Valko \nImperial College\n\n\nDeepmind \nImperial College\n\n\nBootstrap Your Own Latent A New Approach to Self-Supervised Learning\n\nWe introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a ResNet-50 architecture and 79.6% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. * Equal contribution; the order of first authors was randomly selected.\n\nIntroduction\n [8]\n.\n\nLearning good image representations is a key challenge in computer vision [1,2,3] as it allows for efficient training on downstream tasks [4,5,6,7]. Many different training approaches have been proposed to learn such representations, usually relying on visual pretext tasks. Among them, state-of-the-art contrastive methods [8,9,10,11,12] are trained by reducing the distance between representations of different augmented views of the same image ('positive pairs'), and increasing the distance between representations of augmented views from different images ('negative pairs'). These methods require careful treatment of negative pairs [13] by either relying on large batch sizes [8,12], memory banks [9] or customized mining strategies [14,15] to retrieve the negative pairs. In addition, their performance critically depends on the choice of image augmentations [8,12].\n\nIn this paper, we introduce Bootstrap Your Own Latent (BYOL), a new algorithm for self-supervised learning of image representations. BYOL achieves higher performance than state-of-the-art contrastive methods without using negative pairs. It iteratively bootstraps 2 the outputs of a network to serve as targets for an enhanced representation. Moreover, BYOL is more robust to the choice of image augmentations than contrastive methods; we suspect that not relying on negative pairs is one of the leading reasons for its improved robustness. While previous methods based on bootstrapping have used pseudo-labels or cluster indices [16,17] as targets, we propose to directly bootstrap the representations. In particular, BYOL uses two neural networks, referred to as online and target networks, that interact and learn from each other. Starting from an augmented view of an image, BYOL trains its online network to predict the target network's representation of another augmented view of the same image. Even if this problem admits trivial solutions, e.g., outputting zero for all images, we empirically show that using a slow-moving average of the online network as the target network suffices to avoid such collapse to uninformative solutions.\n\nWe evaluate the representation learned by BYOL on ImageNet [18] and other vision benchmarks using ResNet architectures [19]. Under the linear evaluation protocol on ImageNet, consisting in training a linear classifier on top of the frozen representation, BYOL reaches 74.3% top-1 accuracy with a standard ResNet-50 and 79.6% top-1 accuracy with a larger ResNet (Figure 1). In the semi-supervised and transfer settings on ImageNet, we obtain results on par or superior to the current state of the art. Our contributions are: (i) We introduce BYOL, a self-supervised representation learning method (Section 3) which achieves state-of-the-art results under the linear evaluation protocol on ImageNet without using negative pairs. (ii) We show that our learned representation outperforms the state of the art on semi-supervised and transfer benchmarks (Section 4). (iii) We show that BYOL is more resilient to changes in the batch size and in the set of image augmentations compared to its contrastive counterparts (Section 5).\n\nIn particular, BYOL suffers a much smaller performance drop than SimCLR, a strong contrastive baseline, when only using random crops as image augmentations.\n\n\nRelated work\n\nMost unsupervised methods for representation learning can be categorized as either generative or discriminative [20,8]. Generative approaches to representation learning build a distribution over data and latent embedding and use the learned embeddings as image representations. Many of these approaches rely either on auto-encoding of images [21,22,23] or on adversarial learning [24], jointly modelling data and representation [25,26,27,28]. Generative methods typically operate directly in pixel space. This however is computationally expensive, and the high level of detail required for image generation may not be necessary for representation learning.\n\nAmong discriminative methods, contrastive methods [9,10,29,30,31,11,32, 33] currently achieve state-of-the-art performance in self-supervised learning [34, 8,12]. Contrastive approaches avoid a costly generation step in pixel space by bringing representation of different views of the same image closer ('positive pairs'), and spreading representations of views from different images ('negative pairs') apart [35,36]. Contrastive methods often require comparing each example with many other examples to work well [9,8] prompting the question of whether using negative pairs is necessary.\n\nDeepCluster [17] partially answers this question. It uses bootstrapping on previous versions of its representation to produce targets for the next representation; it clusters data points using the prior representation, and uses the cluster index of each sample as a classification target for the new representation. While avoiding the use of negative pairs, this requires a costly clustering phase and specific precautions to avoid collapsing to trivial solutions.\n\nSome self-supervised methods are not contrastive but rely on using auxiliary handcrafted prediction tasks to learn their representation. In particular, relative patch prediction [20,36], colorizing gray-scale images [37,38], image inpainting [39], image jigsaw puzzle [40], image super-resolution [41], and geometric transformations [42,43] have been shown to be useful. Yet, even with suitable architectures [44], these methods are being outperformed by contrastive methods [34, 8,12].\n\nOur approach has some similarities with Predictions of Bootstrapped Latents (PBL, [45]), a self-supervised representation learning technique for reinforcement learning. PBL jointly trains the agent's history representation and an encoding of future observations. The observation encoding is used as a target to train the agent's representation, and the agent's representation as a target to train the observation encoding. Unlike PBL, BYOL uses a slow-moving average of its representation to provide its targets, and does not require a second network.\n\nIn self-supervised learning, MoCo [9] uses a moving average network (momentum encoder) to maintain consistent representations of negative pairs drawn from a memory bank. Instead, BYOL uses a moving average network to produce prediction targets as a means of stabilizing the bootstrap step. We show in Section 5 that this mere stabilizing effect can also improve existing contrastive methods.\n\n\nMethod\n\nWe start by motivating our method before explaining its details in Section 3.1. Many successful self-supervised learning approaches build upon the cross-view prediction framework introduced in [46]. Typically, these approaches learn representations by predicting different views (e.g., different random crops) of the same image from one another. Many such approaches cast the prediction problem directly in representation space: the representation of an augmented view of an image should be predictive of the representation of another augmented view of the same image. However, predicting directly in representation space can lead to collapsed representations: for instance, a representation that is constant across views is always fully predictive of itself. Contrastive methods circumvent this problem by reformulating the prediction problem into one of discrimination: from the representation of an augmented view, they learn to discriminate between the representation of another augmented view of the same image, and the representations of augmented views of different images. In the vast majority of cases, this prevents the training from finding collapsed representations. Yet, this discriminative approach typically requires comparing each representation of an augmented view with many negative examples, to find ones sufficiently close to make the discrimination task challenging. In this work, we thus tasked ourselves to find out whether these negative examples are indispensable to prevent collapsing while preserving high performance.\n\nTo prevent collapse, a straightforward solution is to use a fixed randomly initialized network to produce the targets for our predictions. While avoiding collapse, it empirically does not result in very good representations. Nonetheless, it is interesting to note that the representation obtained using this procedure can already be much better than the initial fixed representation. In our ablation study (Section 5), we apply this procedure by predicting a fixed randomly initialized network and achieve 18.8% top-1 accuracy (Table 5a) on the linear evaluation protocol on ImageNet, whereas the randomly initialized network only achieves 1.4% by itself. This experimental finding is the core motivation for BYOL: from a given representation, referred to as target, we can train a new, potentially enhanced representation, referred to as online, by predicting the target representation. From there, we can expect to build a sequence of representations of increasing quality by iterating this procedure, using subsequent online networks as new target networks for further training. In practice, BYOL generalizes this bootstrapping procedure by iteratively refining its representation, but using a slowly moving exponential average of the online network as the target network instead of fixed checkpoints. 3 \n\n\nDescription of BYOL\n\n\nMethod\n\nWe start by motivating our method before explaining its details in Section 3.1. Many successful self-supervised learning approaches build upon the cross-view prediction framework introduced in [46]. Typically, these approaches learn representations by predicting different views (e.g., different random crops) of the same image from one another. Many such approaches cast the prediction problem directly in representation space: the representation of an augmented view of an image should be predictive of the representation of another augmented view of the same image. However, predicting directly in representation space can lead to collapsed representations: for instance, a representation that is constant across views is always fully predictive of itself. Contrastive methods circumvent this problem by reformulating the prediction problem into one of discrimination: from the representation of an augmented view, they learn to discriminate between the representation of another augmented view of the same image, and the representations of augmented views of different images. In the vast majority of cases, this prevents the training from finding collapsed representations. Yet, this discriminative approach typically requires comparing each representation of an augmented view with many negative examples, to find ones sufficiently close to make the discrimination task challenging. In this work, we thus tasked ourselves to find out whether these negative examples are indispensable to prevent collapsing while preserving high performance.\n\nTo prevent collapse, a straightforward solution is to use a fixed randomly initialized network to produce the targets for our predictions. While avoiding collapse, it empirically does not result in very good representations. Nonetheless, it is interesting to note that the representation obtained using this procedure can already be much better than the initial fixed representation. In our ablation study (Section 5), we apply this procedure by predicting a fixed randomly initialized network and achieve 18.8% top-1 accuracy (Table 5a) on the linear evaluation protocol on ImageNet, whereas the randomly initialized network only achieves 1.4% by itself. This experimental finding is the core motivation for BYOL: from a given representation, referred to as target, we can train a new, potentially enhanced representation, referred to as online, by predicting the target representation. From there, we can expect to build a sequence of representations of increasing quality by iterating this procedure, using subsequent online networks as new target networks for further training. In practice, BYOL generalizes this bootstrapping procedure by iteratively refining its representation, but using a slowly moving exponential average of the online network as the target network instead of fixed checkpoints. 3 \n\n\nDescription of BYOL\n\nx Figure 2: BYOL's architecture. BYOL minimizes a similarity loss between q \u2713 (z) and sg(z 0 ), where \u2713 are the trained weights, \u21e0 are an exponential moving average of \u2713 and sg means stop-gradient. At the end of training, everything but f \u2713 is discarded and y is used as the image representation.\nv y z q \u2713 (z) v 0 y 0 z 0 sg(z 0 ) view input image representation projection prediction t f \u2713 g \u2713 q \u2713 t 0 f \u21e0 g \u21e0 sg loss online target\nBYOL's goal is to learn a representation y which can then be used for downstream tasks. As described previously, BYOL uses two neural networks to learn: the online and target networks. The online network is defined by a set of weights \u2713 and is comprised of three stages: an encoder f \u2713 , a projector g \u2713 and a predictor q \u2713 , as shown in Figure 2. 3 Target networks have been introduced in [47] and are commonly used [48,49,50] in deep reinforcement learning (RL). In deep RL, target networks stabilize the bootstrapping updates provided by the Bellman equation, making them appealing to stabilize the bootstrap mechanism in BYOL. While fixed target networks are more common in deep RL, BYOL uses a weighted moving average of previous networks, similar to [51], in order to provide smoother changes in the target representation.\n\n3 Figure 2: BYOL's architecture. BYOL minimizes a similarity loss between q \u03b8 (z) and sg(z ), where \u03b8 are the trained weights, \u03be are an exponential moving average of \u03b8 and sg means stop-gradient. At the end of training, everything but f \u03b8 is discarded and y is used as the image representation.\n\nBYOL's goal is to learn a representation y which can then be used for downstream tasks. As described previously, BYOL uses two neural networks to learn: the online and target networks. The online network is defined by a set of weights \u03b8 and is comprised of three stages: an encoder f \u03b8 , a projector g \u03b8 and a predictor q \u03b8 , as shown in Figure 2.\n\nThe target network has the same architecture as the online network, but uses a different set of weights \u03be. The target network provides the regression targets to train the online network, and its parameters \u03be are an exponential moving average of the online parameters \u03b8 [51]. More precisely, given a target decay rate \u03c4 \u2208 [0, 1], after each training step we perform the following update,\n\u03be \u2190 \u03c4 \u03be + (1 \u2212 \u03c4 )\u03b8.(1)\nGiven a set of images D, an image x \u223c D sampled uniformly from D, and two distributions of image augmentations T and T , BYOL produces two augmented views v = \u2206 t(x) and v = \u2206 t (x) from x by applying respectively image augmentations t \u223c T and t \u223c T . From the first augmented view v, the online network outputs a representation y = \u2206 f \u03b8 (v) and a projection z \u03b8 = \u2206 g \u03b8 (y). The target network outputs y = \u2206 f \u03be (v ) and the target projection z \u03be = \u2206 g \u03be (y ) from the second augmented view v . We then output a prediction q \u03b8 (z \u03b8 ) of z \u03be and 2 -normalize both q \u03b8 (z \u03b8 ) and z \u03be to q \u03b8 (z \u03b8 ) = \u2206 q \u03b8 (z \u03b8 )/ q \u03b8 (z \u03b8 ) 2 and z \u03be = \u2206 z \u03be / z \u03be 2 . Finally we define the following mean squared error between the normalized predictions and target projections, 4\nL BYOL \u03b8 = \u2206 q \u03b8 (z \u03b8 ) \u2212 z \u03be 2 2 = 2 \u2212 2 \u00b7 q \u03b8 (z \u03b8 ), z \u03be q \u03b8 (z \u03b8 ) 2 \u00b7 z \u03be 2 \u00b7(2)\nWe symmetrize the loss L BYOL \u03b8 in Eq. 2 by separately feeding v to the online network and v to the target network to compute L BYOL \u03b8 . At each training step, we perform a stochastic optimization step to minimize L BYOL \u03b8 + L BYOL \u03b8 with respect to \u03b8 only, but not \u03be, as depicted by the stop gradient in Figure 2.\n\nAt the end of training, we only keep the encoder f \u03b8 ; as in [9]. When comparing to other methods we consider the number of inference-time weights only in the final representation f \u03b8 . The full training procedure is summarized in Appendix A, and python pseudo-code based on the libraries JAX [52] and Haiku [53] is provided in Appendix G.\n\n\nImplementation details\n\nImage augmentations BYOL uses the same set of image augmentations as in SimCLR [8]. First, a random patch of the image is selected and resized to 224 \u00d7 224 with a random horizontal flip, followed by a color distortion, consisting of a random sequence of brightness, contrast, saturation, hue adjustments, and an optional grayscale conversion. Finally Gaussian blur and solarization are applied to the patches. Additional details on the image augmentations are in Appendix B.\n\nArchitecture We use a convolutional residual network [19] with 50 layers and post-activation (ResNet-50(1\u00d7) v1) as our base parametric encoders f \u03b8 and f \u03be . We also use deeper (50, 101, 152 and 200 layers) and wider (from 1\u00d7 to 4\u00d7) ResNets, as in [54,44,8]. Specifically, the representation y corresponds to the output of the final average pooling layer, which has a feature dimension of 2048 (for a width multiplier of 1\u00d7). As in SimCLR [8], the representation y is projected to a smaller space by a multi-layer perceptron (MLP) g \u03b8 , and similarly for the target projection g \u03be . This MLP consists in a linear layer with output size 4096 followed by batch normalization [55], rectified linear units (ReLU) [56], and a final linear layer with output dimension 256. Contrary to SimCLR, the output of this MLP is not batch normalized. The predictor q \u03b8 uses the same architecture as g \u03b8 .\n\n\nOptimization\n\nWe use the LARS optimizer [57] with a cosine decay learning rate schedule [58], without restarts, over 1000 epochs, with a warm-up period of 10 epochs. We set the base learning rate to 0.2, scaled linearly [59] with the batch size (LearningRate = 0.2 \u00d7 BatchSize/256). In addition, we use a global weight decay parameter of 1.5 \u00b7 10 \u22126 while excluding the biases and batch normalization parameters from both LARS adaptation and weight decay. For the target network, the exponential moving average parameter \u03c4 starts from \u03c4 base = 0.996 and is increased to one during training. Specifically, we set \u03c4 1 \u2212 (1 \u2212 \u03c4 base ) \u00b7 (cos(\u03c0k/K) + 1)/2 with k the current training step and K the maximum number of training steps. We use a batch size of 4096 split over 512 Cloud TPU v3 cores. With this setup, training takes approximately 8 hours for a ResNet-50(\u00d71). All hyperparameters are summarized in Appendix G.\n\n\nExperimental evaluation\n\nWe assess the performance of BYOL's representation 5 after self-supervised pretraining on the training set of the ImageNet ILSVRC-2012 dataset [18]. We first evaluate it on ImageNet (IN) in both linear evaluation and semisupervised setups. We then measure its transfer capabilities on other datasets and tasks, including classification, segmentation, object detection and depth estimation. For comparison, we also report scores for a representation trained using labels from the train ImageNet subset, referred to as Supervised-IN. In Appendix E, we assess the generality of BYOL by pretraining a representation on the Places365-Standard dataset [60] before reproducing this evaluation protocol.\n\nLinear evaluation on ImageNet We first evaluate BYOL's representation by training a linear classifier on top of the frozen representation, following the procedure described in [44,61,37,10,8], and appendix C.1; we report top-1 and top-5 accuracies in % on the test set in Table 1. With a standard ResNet-50 (\u00d71) BYOL obtains 74.3% top-1 accuracy (91.6% top-5 accuracy), which is a 1.3% (resp. 0.5%) improvement over the previous self-supervised state of the art [12]. This tightens the gap with respect to the supervised baseline of [8], 76.5%, but is still significantly below the stronger supervised baseline of [62], 78.9%. With deeper and wider architectures, BYOL consistently outperforms the previous state of the art (Appendix C.2), and obtains a best performance of 79.6% top-1 accuracy, ranking higher than previous self-supervised approaches. On a ResNet-50 (4\u00d7) BYOL achieves 78.6%, similar to the 78.9% of the best supervised baseline in [8] for the same architecture.  Semi-supervised training on ImageNet Next, we evaluate the performance obtained when fine-tuning BYOL's representation on a classification task with a small subset of ImageNet's train set, this time using label information.\n\nWe follow the semi-supervised protocol of [61,63,8,29] detailed in Appendix C.1, and use the same fixed splits of respectively 1% and 10% of ImageNet labeled training data as in [8]. We report both top-1 and top-5 accuracies on the test set in Table 2. BYOL consistently outperforms previous approaches across a wide range of architectures. Additionally, as detailed in Appendix C.1, BYOL reaches 77.7% top-1 accuracy with ResNet-50 when fine-tuning over 100% of ImageNet labels.  Table 2: Semi-supervised training with a fraction of ImageNet labels. 5 Our training code and pre-trained models will be publicly released at a later date.  Transfer to other classification tasks We evaluate our representation on other classification datasets to assess whether the features learned on ImageNet (IN) are generic and thus useful across image domains, or if they are ImageNet-specific. We perform linear evaluation and fine-tuning on the same set of classification tasks used in [8,61], and carefully follow their evaluation protocol, as detailed in Appendix D. Performance is reported using standard metrics for each benchmark, and results are provided on a held-out test set after hyperparameter selection on a validation set. We report results in Table 3, both for linear evaluation and fine-tuning. BYOL outperforms SimCLR on all benchmarks and the Supervised-IN baseline on 7 of the 12 benchmarks, providing only slightly worse performance on the 5 remaining benchmarks. BYOL's representation can be transferred over to small images, e.g., CIFAR [65], landscapes, e.g., SUN397 [66] or VOC2007 [67], and textures, e.g., DTD [68].\n\nTransfer to other vision tasks We evaluate our representation on different tasks relevant to computer vision practitioners, namely semantic segmentation, object detection and depth estimation. With this evaluation, we assess whether BYOL's representation generalizes beyond classification tasks.\n\nWe first evaluate BYOL on the VOC2012 semantic segmentation task as detailed in Appendix D.4, where the goal is to classify each pixel in the image [7]. We report the results in Table 4a. BYOL outperforms both the Supervised-IN baseline (+1.9 mIoU) and SimCLR (+1.1 mIoU).\n\nSimilarly, we evaluate on object detection by reproducing the setup in [9] using a Faster R-CNN architecture [69], as detailed in Appendix D.5. We fine-tune on trainval2007 and report results on test2007 using the standard AP 50 metric; BYOL is significantly better than the Supervised-IN baseline (+3.1 AP 50 ) and SimCLR (+2.3 AP 50 ).\n\nFinally, we evaluate on depth estimation on the NYU v2 dataset, where the depth map of a scene is estimated given a single RGB image. Depth prediction measures how well a network represents geometry, and how well that information can be localized to pixel accuracy [36]. The setup is based on [70] and detailed in Appendix D.6. We evaluate on the commonly used test subset of 654 images and report results using several common metrics in Table 4b: relative (rel) error, root mean squared (rms) error, and the percent of pixels (pct) where the error,  \nmax(d gt /d p , d p /d gt ),\n\nBuilding intuitions with ablations\n\nWe present ablations on BYOL to give an intuition of its behavior and performance. For reproducibility, we run each configuration of parameters over three seeds, and report the average performance. We also report the half difference between the best and worst runs when it is larger than 0.25. Although previous works perform ablations at 100 epochs [8,12], we notice that relative improvements at 100 epochs do not always hold over longer training. For this reason, we run ablations over 300 epochs on 64 TPU v3 cores, which yields consistent results compared to our baseline training of 1000 epochs. For all the experiments in this section, we set the initial learning rate to 0.3 with batch size 4096, the weight decay to 10 \u22126 as in SimCLR [8] and the base target decay rate \u03c4 base to 0.99. In this section we report results in top-1 accuracy on ImageNet under the linear evaluation protocol as in Appendix C.1.\n\nBatch size Among contrastive methods, the ones that draw negative examples from the minibatch suffer performance drops when their batch size is reduced. BYOL does not use negative examples and we expect it to be more robust to smaller batch sizes. To empirically verify this hypothesis, we train both BYOL and SimCLR using different batch sizes from 128 to 4096. To avoid re-tuning other hyperparameters, we average gradients over N consecutive steps before updating the online network when reducing the batch size by a factor N . The target network is updated once every N steps, after the update of the online network; we accumulate the N -steps in parallel in our runs.\n\nAs shown in Figure  Image augmentations Contrastive methods are sensitive to the choice of image augmentations. For instance, SimCLR does not work well when removing color distortion from its image augmentations. As an explanation, SimCLR shows that crops of the same image mostly share their color histograms. At the same time, color histograms vary across images. Therefore, when a contrastive task only relies on random crops as image augmentations, it can be mostly solved by focusing on color histograms alone. As a result the representation is not incentivized to retain information beyond color histograms. To prevent that, SimCLR adds color distortion to its set of image augmentations. Instead, BYOL is incentivized to keep any information captured by the target representation into its online network, to improve its predictions. Therefore, even if augmented views of a same image share the same color histogram, BYOL is still incentivized to retain additional features in its representation. For that reason, we believe that BYOL is more robust to the choice of image augmentations than contrastive methods.\n\nResults presented in Figure 3b support this hypothesis: the performance of BYOL is much less affected than the performance of SimCLR when removing color distortions from the set of image augmentations (\u22129.1 accuracy points for BYOL, \u221222.2 accuracy points for SimCLR). When image augmentations are reduced to mere random crops, BYOL still displays good performance (59.4%, i.e. \u221213.1 points from 72.5% ), while SimCLR loses more than a third of its performance (40.3%, i.e. \u221227.6 points from 67.9%). We report additional ablations in Appendix F.3.\n\nBootstrapping BYOL uses the projected representation of a target network, whose weights are an exponential moving average of the weights of the online network, as target for its predictions. This way, the weights of the target network represent a delayed and more stable version of the weights of the online network. When the target decay rate is 1, the target network is never updated, and remains at a constant value corresponding to its initialization. When the target decay rate is 0, the target network is instantaneously updated to the online network at each step. There is a trade-off between updating the targets too often and updating them too slowly, as illustrated in Table 5a. Instantaneously updating the target network (\u03c4 = 0) destabilizes training, yielding very poor performance while never updating the target (\u03c4 = 1) makes the training stable but prevents iterative improvement, ending with low-quality final representation. All values of the decay rate between 0.9 and 0.999 yield performance above 68.4% top-1 accuracy at 300 epochs.  Ablation to contrastive methods In this subsection, we recast SimCLR and BYOL using the same formalism to better understand where the improvement of BYOL over SimCLR comes from. Let us consider the following objective that extends the InfoNCE objective [10],\nInfoNCE \u03b8 = \u2206 2 B B i=1 S \u03b8 (v i , v i )\u2212 \u03b2 \u00b7 2\u03b1 B B i=1 ln \uf8eb \uf8ed j =i exp S \u03b8 (v i , v j ) \u03b1 + j exp S \u03b8 (v i , v j ) \u03b1 \uf8f6 \uf8f8,(3)\nwhere \u03b1 > 0 is a fixed temperature, \u03b2 \u2208 [0, 1] a weighting coefficient, B the batch size, v and v are batches of augmented views where for any batch index i, v i and v i are augmented views from the same image; the realvalued function S \u03b8 quantifies pairwise similarity between augmented views. For any augmented view u we denote z \u03b8 (u) f \u03b8 (g \u03b8 (u)) and z \u03be (u) f \u03be (g \u03be (u)). For given \u03c6 and \u03c8, we consider the normalized dot product\nS \u03b8 (u 1 , u 2 ) = \u2206 \u03c6(u 1 ), \u03c8(u 2 ) \u03c6(u 1 ) 2 \u00b7 \u03c8(u 2 ) 2 \u00b7(4)\nUp to minor details (cf. Appendix F.5), we recover the SimCLR loss with \u03c6(u 1 ) = z \u03b8 (u 1 ) (no predictor), \u03c8(u 2 ) = z \u03b8 (u 2 ) (no target network) and \u03b2 = 1. We recover the BYOL loss when using a predictor and a target network, i.e., \u03c6(u 1 ) = p \u03b8 (z \u03b8 (u 1 )) and \u03c8(u 2 ) = z \u03be (u 2 ) with \u03b2 = 0. To evaluate the influence of the target network, the predictor and the coefficient \u03b2, we perform an ablation over them. Results are presented in Table 5b and more details are given in Appendix F.4.\n\nThe only variant that performs well without negative examples (i.e., with \u03b2 = 0) is BYOL, using both a bootstrap target network and a predictor. Adding the negative pairs to BYOL's loss without re-tuning the temperature parameter hurts its performance. In Appendix F.4, we show that we can add back negative pairs and still match the performance of BYOL with proper tuning of the temperature.\n\nSimply adding a target network to SimCLR already improves performance (+1.6 points). This sheds new light on the use of the target network in MoCo [9], where the target network is used to provide more negative examples. Here, we show that by mere stabilization effect, even when using the same number of negative examples, using a target network is beneficial. Finally, we observe that modifying the architecture of S \u03b8 to include a predictor only mildly affects the performance of SimCLR.\n\n\nConclusion\n\nWe introduced BYOL, a new algorithm for self-supervised learning of image representations. BYOL learns its representation by predicting previous versions of its outputs, without using negative pairs. We show that BYOL achieves state-of-the-art results on various benchmarks. In particular, under the linear evaluation protocol on ImageNet with a ResNet-50 (1\u00d7), BYOL achieves a new state of the art and bridges most of the remaining gap between self-supervised methods and the supervised learning baseline of [8]. Using a ResNet-200 (2\u00d7), BYOL reaches a top-1 accuracy of 79.6% which improves over the previous state of the art (76.8%) while using 30% fewer parameters.\n\nNevertheless, BYOL remains dependent on existing sets of augmentations that are specific to vision applications.\n\nTo generalize BYOL to other modalities (e.g., audio, video, text, . . . ) it is necessary to obtain similarly suitable augmentations for each of them. Designing such augmentations may require significant effort and expertise. Therefore, automating the search for these augmentations would be an important next step to generalize BYOL to other modalities.\n\n\nBroader impact\n\nThe presented research should be categorized as research in the field of unsupervised learning. This work may inspire new algorithms, theoretical, and experimental investigation. The algorithm presented here can be used for many different vision applications and a particular use may have both positive or negative impacts, which is known as the dual use problem. Besides, as vision datasets could be biased, the representation learned by BYOL could be susceptible to replicate these biases. \n\n\nA Algorithm\n\nAlgorithm 1: BYOL: Bootstrap Your Own Latent Inputs : D, T , and T set of images and distributions of transformations \u03b8, f \u03b8 , g \u03b8 , and q \u03b8 initial online parameters, encoder, projector, and predictor \u03be, f \u03be , g \u03be initial target parameters, target encoder, and target projector optimizer optimizer, updates online parameters using the loss gradient K and N total number of optimization steps and batch size {\u03c4 k } K k=1 and {\u03b7 k } K k=1 target network update schedule and learning rate schedule \u2022 random cropping: a random patch of the image is selected, with an area uniformly sampled between 8% and 100% of that of the original image, and an aspect ratio logarithmically sampled between 3/4 and 4/3. This patch is then resized to the target size of 224 \u00d7 224 using bicubic interpolation;\n1 for k = 1 to K do 2 B \u2190 {x i \u223c D} N i=1 // sample a batch of N images 3 for x i \u2208 B do 4 t \u223c T and t \u223c T // sample image transformations 5 z 1 \u2190 g \u03b8 (f \u03b8 (t(x i ))) and z 2 \u2190 g \u03b8 (f \u03b8 (t (x i ))) // compute projections 6 z 1 \u2190 g \u03be (f \u03be (t (x i ))) and z 2 \u2190 g \u03be (f \u03be (t(x i ))) // compute target projections 7 l i \u2190 \u22122 \u00b7 q \u03b8 (z1),z 1 q \u03b8 (z1) 2 \u00b7 z 1 2 + q \u03b8 (z2),z 2 q \u03b8 (z2) 2 \u00b7 z 2 2\n\u2022 optional left-right flip;\n\n\u2022 color jittering: the brightness, contrast, saturation and hue of the image are shifted by a uniformly random offset applied on all the pixels of the same image. The order in which these shifts are performed is randomly selected for each patch;\n\n\u2022 color dropping: an optional conversion to grayscale. When applied, output intensity for a pixel (r, g, b) corresponds to its luma component, computed as 0.2989r + 0.5870g + 0.1140b; Augmentations from the sets T and T (introduced in Section 3) are compositions of the above image augmentations in the listed order, each applied with a predetermined probability. The image augmentations parameters are listed in Table 6.\n\nDuring evaluation, we use a center crop similar to [8]: images are resized to 256 pixels along the shorter side using bicubic resampling, after which a 224 \u00d7 224 center crop is applied. In both training and evaluation, we normalize color channels by subtracting the average color and dividing by the standard deviation, computed on ImageNet, after applying the augmentations.  C Evaluation on ImageNet training C.1 Self-supervised learning evaluation on ImageNet Linear evaluation protocol on ImageNet As in [44,61,8, 34], we use the standard linear evaluation protocol on ImageNet, which consists in training a linear classifier on top of the frozen representation, i.e., without updating the network parameters nor the batch statistics. At training time, we apply spatial augmentations, i.e., random crops with resize to 224 \u00d7 224 pixels, and random flips. At test time, images are resized to 256 pixels along the shorter side using bicubic resampling, after which a 224 \u00d7 224 center crop is applied. In both cases, we normalize the color channels by subtracting the average color and dividing by the standard deviation (computed on ImageNet), after applying the augmentations. We optimize the cross-entropy loss using SGD with Nesterov momentum over 80 epochs, using a batch size of 4096 and a momentum of 0.9. We do not use any regularization methods such as weight decay, gradient clipping [71], tclip [31], or logits regularization. We finally sweep over 5 learning rates {0.4, 0.3, 0.2, 0.1, 0.05} on a local validation set (10009 images from ImageNet train set), and report the accuracy of the best validation hyperparameter on the test set (which is the public validation set of the original ILSVRC2012 ImageNet dataset).\n\n\nVariant on linear evaluation on ImageNet\n\nIn this paragraph only, we deviate from the protocol of [8,34] and propose another way of performing linear evaluation on top of a frozen representation. This method achieves better performance both in top-1 and top-5 accuracy.\n\n\u2022 We replace the spatial augmentations (random crops with resize to 224 \u00d7 224 pixels and random flips) with the pre-train augmentations of Appendix B. This method was already used in [29] with a different subset of pre-train augmentations.\n\n\u2022 We regularize the linear classifier as in [31] 7 by clipping the logits using a hyperbolic tangent function\ntclip(x) \u03b1 \u00b7 tanh(x/\u03b1),\nwhere \u03b1 is a positive scalar, and by adding a logit-regularization penalty term in the loss\nLoss(x, y) cross_entropy(tclip(x), y) + \u03b2 \u00b7 average(tclip(x) 2 ),\nwhere x are the logits, y are the target labels, and \u03b2 is the regularization parameter. We set \u03b1 = 20 and \u03b2 = 1e\u22122.\n\nWe report in Table 7 the top-1 and top-5 accuracy on ImageNet using this modified protocol. These modifications in the evaluation protocol increase the BYOL's top-1 accuracy from 74.3% to 74.8% with a ResNet-50 (1\u00d7).\n\nSemi-supervised learning on ImageNet We follow the semi-supervised learning protocol of [8,64]. We first initialize the network with the parameters of the pretrained representation, and finetune it with a subset of ImageNet labels. At training time, we apply spatial augmentations, i.e., random crops with resize to 224 \u00d7 224 pixels and random flips. At test time, images are resized to 256 pixels along the shorter side using bicubic resampling, after which a 224 \u00d7 224 center crop is applied. In both cases, we normalize the color channels by subtracting the average color and dividing by the standard deviation (computed on ImageNet), after applying the augmentations. We optimize the cross-entropy loss using SGD with Nesterov momentum. We used a batch size of 4096, a momentum of   In Table 2 presented in the main text, we finetune the representation over the 1% and 10% ImageNet splits from [8] with various ResNet architectures.\n\nIn Figure 4, we finetune the representation over 1%, 2%, 5%, 10%, 20%, 50%, and 100% of the ImageNet dataset as in [29] with a ResNet-50 (1\u00d7) architecture. In this case and contrary to Table 2 we don't reuse the splits from SimCLR but we create our own via a balanced selection.\n\nFinally, we finetune the representation over the full ImageNet dataset. We report the results in Table 8 along with supervised baselines trained on ImageNet. We observe that finetuning the SimCLR checkpoint does not yield better results (in our reproduction, which matches the results reported in the original paper [8]) than using a random initialization (76.5 top-1). Instead, BYOL's initialization checkpoint leads to a high final score (77.7 top-1), higher than the vanilla supervised baseline of [8], matching the strong supervised baseline of AutoAugment [72] but still 1.2 points below the stronger supervised baseline [62], which uses advanced supervised learning techniques.   Table 9: Linear evaluation of BYOL on ImageNet using larger encoders. Top-1 and top-5 accuracies are reported in %.\n\nHere we investigate the performance of BYOL with deeper and wider ResNet architectures. We compare ourselves to the best supervised baselines from [8] when available (rightmost column in table 9), which are also presented in Figure 1. Importantly, we close in on those baselines using the ResNet-50 (2\u00d7) and the ResNet-50 (4\u00d7) architectures, where we are within 0.4 accuracy points of the supervised performance. To the best of our knowledge, this is the first time that the gap to supervised has been closed to such an extent using a self-supervised method under the linear evaluation protocol. Therefore, in order to ensure fair comparison, and suspecting that the supervised baselines' performance in [8] could be even further improved with appropriate data augmentations, we also report on our own reproduction of strong supervised baselines. We use RandAugment [72] data augmentation for all large ResNet architectures (which are all version 1, as per [19]). We train our supervised baselines for up to 200 epochs, using SGD with a Nesterov momentum value of 0.9, a cosine-annealed learning rate after a 5 epochs linear warmup period, weight decay with a value of 1e \u2212 4, and a label smoothing [73] value of 0.1. Results are presented in Figure 5.   We perform transfer via linear classification and fine-tuning on the same set of datasets as in [8], namely Food-101 dataset [74], CIFAR-10 [65] and CIFAR-100 [65], Birdsnap [75], the SUN397 scene dataset [66], Stanford Cars [76], FGVC Aircraft [77], the PASCAL VOC 2007 classification task [67], the Describable Textures Dataset (DTD) [68], Oxford-IIIT Pets [78], Caltech-101 [79], and Oxford 102 Flowers [80]. As in [8], we used the validation sets specified by the dataset creators to select hyperparameters for FGVC Aircraft, PASCAL VOC 2007, DTD, and Oxford 102 Flowers. On other datasets, we use the validation examples as test set, and hold out a subset of the training examples that we use as validation set. We use standard metrics for each datasets:\n\n\u2022 Top-1: We compute the proportion of correctly classified examples.\n\n\u2022 Mean per class: We compute the top-1 accuracy for each class separately and then compute the empirical mean over the classes.\n\n\u2022 Point 11-mAP: We compute the empirical mean average precision as defined in [67].\n\n\u2022 Mean IoU: We compute the empirical mean Intersection-Over-Union as defined in [67].\n\n\u2022 AP50: We compute the Average Precision as defined in [67].\n\nWe detail the validation procedures for some specific datasets:\n\n\u2022 For Sun397 [66], the original dataset specifies 10 train/test splits, all of which contain 50 examples/images of 397 different classes. We use the first train/test split. The original dataset specifies no validation split and therefore, the training images have been further subdivided into 40 images per class for the train split and 10 images per class for the valid split.\n\n\u2022 For Birdsnap [75], we use a random selection of valid images with the same number of images per category as the test split.\n\n\u2022 For DTD [68], the original dataset specifies 10 train/validation/test splits, we only use the first split.\n\n\u2022 For Caltech-101 [79], the original does not dataset specifies any train/test splits. We have followed the approach used in [81]: This file defines datasets for 5 random splits of 25 training images per category, with 5 validation images per category and the remaining images used for testing.\n\n\u2022 For ImageNet, we took the last 10009 last images of the official tensorflow ImageNet split.\n\n\u2022 For Oxford-IIIT Pets, the valid set consists of 20 randomly selected images per class.\n\nInformation about the dataset are summarized in Table 10.\n\n\nD.2 Transfer via linear classification\n\nWe follow the linear evaluation protocol of [44,61,8] that we detail next for completeness. We train a regularized multinomial logistic regression classifier on top of the frozen representation, i.e., with frozen pretrained parameters and without re-computing batch-normalization statistics. In training and testing, we do not perform any image augmentations; images are resized to 224 pixels along the shorter side using bicubic resampling and then normalized with ImageNet statistics. Finally, we minimize the cross-entropy objective using LBFGS with 2 -regularization, where we select the regularization parameters from a range of 45 logarithmically-spaced values between 10 \u22126 and 10 5 . After choosing the best-performing hyperparameters on the validation set, the model is retrained on combined training and validation images together, using the chosen parameters. The final accuracy is reported on the test set.\n\n\nD.3 Transfer via fine-tuning\n\nWe follow the same fine-tuning protocol as in [29,44,63,8] that we also detail for completeness. Specifically, we initialize the network with the parameters of the pretrained representation. At training time, we apply spatial transformation, i.e., random crops with resize to 224 \u00d7 224 pixels and random flips. At test time, images are resized to 256 pixels along the shorter side using bicubic resampling, after which a 224 \u00d7 224 center crop is extracted. In both cases, we normalize the color channels by subtracting the average color and dividing by the standard deviation (computed on ImageNet), after applying the augmentations. We optimize the loss using SGD with Nesterov momentum for 20000 steps with a batch size of 256 and with a momentum of 0.9. We set the momentum parameter for the batch normalization statistics to max(1 \u2212 10/s, 0.9) where s is the number of steps per epoch. The learning rate and weight decay are selected respectively with a grid of seven logarithmically spaced learning rates between 0.0001 and 0.1, and 7 logarithmically-spaced values of weight decay between 10 \u22126 and 10 \u22123 , as well as no weight decay. These values of weight decay are divided by the learning rate. After choosing the best-performing hyperparameters on the validation set, the model is retrained on combined training and validation images together, using the chosen parameters. The final accuracy is reported on the test set.\n\n\nD.4 Implementation details for semantic segmentation\n\nWe use the same fully-convolutional network (FCN)-based [7] architecture as [9]. The backbone consists of the convolutional layers in ResNet-50. The 3 \u00d7 3 convolutions in the conv5 blocks use dilation 2 and stride 1. This is followed by two extra 3 \u00d7 3 convolutions with 256 channels, each followed by batch normalization and ReLU activations, and a 1 \u00d7 1 convolution for per-pixel classification. The dilation is set to 6 in the two extra 3 \u00d7 3 convolutions. The total stride is 16 (FCN-16s [7]).\n\nWe train on the train_aug2012 set and report results on val2012. Hyperparameters are selected on a 2119 images held-out validation set. We use a standard per-pixel softmax cross-entropy loss to train the FCN. Training is done with random scaling (by a ratio in \n\n\nD.5 Implementation details for object detection\n\nFor object detection, we follow prior work on Pascal detection transfer [36, 20] wherever possible. We use a Faster R-CNN [69] detector with a R50-C4 backbone with a frozen representation. The R50-C4 backbone ends with the conv4 stage of a ResNet-50, and the box prediction head consists of the conv5 stage (including global pooling). We preprocess the images by applying multi-scale augmentation (rescaling the image so its longest edge is between 480 and 1024 pixels) but no other augmentation. We use an asynchronous SGD optimizer with 9 workers and train for 1.5M steps. We used an initial learning rate of 10 \u22123 , which is reduced to 10 \u22124 at 1M steps and to 10 \u22125 at 1.2M steps.\n\n\nD.6 Implementation details for depth estimation\n\nFor depth estimation, we follow the same protocol as in [70], and report its core components for completeness. We use a standard ResNet-50 backbone and feed the conv5 features into 4 fast up-projection blocks with respective filter sizes 512, 256, 128, and 64. We use a reverse Huber loss function for training [70,82].\n\nThe original NYU Depth v2 frames of size [640, 480] are down-sampled by a factor 0.5 and center-cropped to [304,228] pixels. Input images are randomly horizontally flipped and the following color transformations are applied:\n\n\u2022 Grayscale with an application probability of 0.3.\n\n\u2022 Brightness with a maximum brightness difference of 0.1255.\n\n\u2022 Saturation with a saturation factor randomly picked in the interval [0.5, 1.5].\n\n\u2022 Hue with a hue adjustment factor randomly picked in the interval [\u22120.2, 0.2].\n\nWe train for 7500 steps with batch size 256, weight decay 0.0005, and learning rate 0.16 (scaled linearly from the setup of [70] to account for the bigger batch size).\n\n\nD.7 Further comparisons on PASCAL and NYU v2 Depth\n\nFor completeness, Table 11 and 12 extends Table 4 with other published baselines which use comparable networks. We see that in almost all settings, BYOL outperforms these baselines, even when those baselines use more data or deeper models. One notable exception is RMS error for NYU Depth prediction, which is a metric that's sensitive to outliers. The reason for this is unclear, but one possibility is that the network is producing higher-variance predictions due to being more confident about a test-set scene's similarities with those in the training set.   \n\n\nE Pretraining on Places 365\n\nTo ascertain that BYOL learns good representations on other datasets, we applied our representation learning protocol on the scene recognition dataset Places365-Standard [60] before performing linear evaluation. This dataset contains 1.80 million training images and 36500 validation images with labels, making it roughly similar to ImageNet in scale. We reuse the exact same parameters as in Section 4 and train the representation for 1000 epochs, using BYOL and our SimCLR reproduction. Results for the linear evaluation setup (using the protocol of Appendix C.1 for ImageNet and Places365, and that of Appendix D on other datasets) are reported in Table 13.\n\nInterestingly, the representation trained by using BYOL on Places365 (BYOL-PL) consistently outperforms that of SimCLR on the same dataset, but underperforms the BYOL representation trained on ImageNet (BYOL-IN) on all tasks except Places365 and SUN397 [66], another scene understanding dataset. Interestingly, all three unsupervised representation learning methods achieve a relatively high performance on the Places365 task; for comparison, reference [60] (in its linked repository) reports a top-1 accuracy of 55.2% for a ResNet-50v2 trained from scratch using labels on this dataset.  \n\n\nF Additional ablation results\n\nTo extend on the above results, we provide additional ablations obtained using the same experimental setup as in Section 5, i.e., 300 epochs, averaged over 3 seeds with the initial learning rate set to 0.3, the batch size to 4096, the weight decay to 10 \u22126 and the base target decay rate \u03c4 base to 0.99 unless specified otherwise. Confidence intervals correspond to the half-difference between the maximum and minimum score of these seeds; we omit them for half-differences lower than 0.25 accuracy points. Table 14 shows the influence of projector and predictor architecture on BYOL. We examine the effect of different depths for both the projector and predictor, as well as the effect of the projection size. We do not apply a ReLU activation nor a batch normalization on the final linear layer of our MLPs such that a depth of 1 corresponds to a linear layer. Using the default projector and predictor of depth 2 yields the best performance.   Table 15a shows the influence of the initial learning rate on BYOL. Note that the optimal value depends on the number of training epochs. Table 15b displays the influence of the weight decay on BYOL.\n\n\nF.1 Architecture settings\n\n\nF.2 Batch size\n\nWe run a sweep over the batch size for both BYOL and our reproduction of SimCLR. As explained in Section 5, when reducing the batch size by a factor N , we average gradients over N consecutive steps and update the target network once every N steps. We report in Table 16, the performance of both our reproduction of SimCLR and BYOL for batch sizes between 4096 (BYOL and SimCLR default) down to 64. We observe that the performance of SimCLR deteriorates faster than the one of BYOL which stays mostly constant for batch sizes larger than 256. We believe that the performance at batch size 256 could match the performance of the large 4096 batch size with proper parameter tuning when accumulating the gradient. We think that the drop in performance at batch size 64 in table 16 is mainly related to the ill behaviour of batch normalization at low batch sizes [85].     \n\n\nF.4 Details on the relation to contrastive methods\n\nAs mentioned in Section 5, the BYOL loss Eq. 2 can be derived from the InfoNCE loss  with\nInfoNCE \u03b8 = \u2206 2 B B i=1 S \u03b8 (v i , v i )\u2212 2\u03b1 \u00b7 \u03b2 B B i=1 ln \uf8eb \uf8ed j =i exp S \u03b8 (v i , v j ) \u03b1 + j exp S \u03b8 (v i , v j ) \u03b1 \uf8f6 \uf8f8,(5)S \u03b8 (u 1 , u 2 ) = \u2206 \u03c6(u 1 ), \u03c8(u 2 ) \u03c6(u 1 ) 2 \u00b7 \u03c8(u 2 ) 2 \u00b7(6)\nIn our ablation in Table 5b, we set the temperature \u03b1 to its best value in the SimCLR setting (i.e., \u03b1 = 0.1). With this value, setting \u03b2 to 1 (which adds negative examples), in the BYOL setting (i.e., with both a predictor and a target network) hurts the performances. In Table 18, we report results of a sweep over both the temperature \u03b1 and the weight parameter \u03b2 with a predictor and a target network where BYOL corresponds to \u03b2 = 0. No run significantly outperforms BYOL and some values of \u03b1 and \u03b2 hurt the performance. While the best temperature for SimCLR (without the target network and a predictor) is 0.1, after adding a predictor and a target network the best temperature \u03b1 is higher than 0.3. Using a target network in the loss has two effects: stopping the gradient through the prediction targets and stabilizing the targets with averaging. Stopping the gradient through the target change the objective while averaging makes the target stable and stale. In Table 5b we only shows results of the ablation when either using the online network as the prediction target (and flowing the gradient through it) or with a target network (both stopping the gradient into the prediction targets and computing the prediction targets with a moving average of the online network). We shown in Table 5b that using a target network is beneficial but it has two distinct effects we would like to understand from which effect the improvement comes from. We report in Table 19 the results already in Table 5b but also when the prediction target is computed with a stop gradient of the online network (the gradient does not flow into the prediction targets). This shows that making the prediction targets stable and stale is the main cause of the improvement rather than the change in the objective due to the stop gradient.\n\n\nF.5 SimCLR baseline of Section 5\n\nThe SimCLR baseline in Section 5 (\u03b2 = 1, without predictor nor target network) is slightly different from the original one in [8]. First we multiply the original loss by 2\u03b1. For comparaison here is the original SimCLR loss,\nInfoNCE \u03b8 = \u2206 1 B B i=1 S \u03b8 (v i , v i ) \u03b1 \u2212 1 B B i=1 ln \uf8eb \uf8ed j =i exp S \u03b8 (v i , v j ) \u03b1 + j exp S \u03b8 (v i , v j ) \u03b1 \uf8f6 \uf8f8 \u00b7(7)\nNote that this multiplication by 2\u03b1 matters as the LARS optimizer is not completely invariant with respect to the scale of the loss. Indeed, LARS applies a preconditioning to gradient updates on all weights, except for biases and batch normalization parameters. Updates on preconditioned weights are invariant by multiplicative scaling of the  loss. However, the bias and batch normalization parameter updates remain sensitive to multiplicative scaling of the loss.\n\nWe also increase the original SimCLR hidden and output size of the projector to respectively 4096 and 256. In our reproduction of SimCLR, these three combined changes improves the top-1 accuracy at 300 epochs from 67.9% (without the changes) to 69.2% (with the changes).  BYOL minimizes a squared error between the 2 -normalized prediction and target. We report results of BYOL at 300 epochs using different normalization function and no normalization at all. More precisely, given batch of prediction and targets in R d , (p i , t i ) i\u2264B with B the batch size, BYOL uses the loss function 1\n\n\nF.6 Ablation on the normalization in the loss function\nB B i=1 n 2 (p i ) \u2212 n 2 (z i ) 2 2\nwith n 2 :\n\nx \u2192 x/ x 2 . We run BYOL with other normalization functions: non-trainable batch-normalization and layer-normalization and no normalization. We divide the batch normalization and layer normalization by \u221a d to have a consistent scale with the 2 -normalization. We report results in Table 20 where 2 , LAYERNORM, no normalization and BATCHNORM respectively denote using n 2 , n BN , n LN and n ID with When using no normalization at all, the projection 2 norm rapidly increases during the first 100 epochs and stabilizes at around 3 \u00b7 10 6 as shown in Figure 6. Despite this behaviour, using no normalization still performs reasonably well (67.4%). The 2 normalization performs the best.\nn j BNi : x \u2192 x j i \u2212 \u00b5 j BN (x) \u03c3 j BN (x) \u00b7 \u221a d , n j LNi : x \u2192 x j i \u2212 \u00b5 LNi (x) \u03c3 LNi (x) \u00b7 \u221a d , n ID : x \u2192 x, \u00b5 j BN : x \u2192 1 B B i=1 x j i , \u03c3 j BN : x \u2192 1 B B i=1 x j i 2 \u2212 \u00b5 j BN (x) 2 , \u00b5 LNi : x \u2192 1 d d j=1 x j i , \u03c3 LNi : x \u2192 x i \u2212 \u00b5 LNi (x) 2 \u221a d\nG BYOL pseudo-code in JAX \n\nFigure 1 :\n1Performance of BYOL on ImageNet (linear evaluation) using ResNet-50 and our best architecture ResNet-200 (2\u00d7), compared to other unsupervised and supervised (Sup.) baselines\n\nFigure 3 :\n33a, the performance of SimCLR rapidly deteriorates with batch size, likely due to the decrease in the number of negative examples. In contrast, the performance of BYOL remains stable over a wide range of batch sizes from 256 to 4096, and only drops for smaller values due to batch normalization layers in the encoder. Decrease in top-1 accuracy (in % points) of BYOL and our own reproduction of SimCLR at 300 epochs, under linear evaluation on ImageNet.\n\n\nl i // compute the total loss gradient w.r.t. \u03b8 10 \u03b8 \u2190 optimizer(\u03b8, \u03b4\u03b8, \u03b7 k ) // update online parameters11 \u03be \u2190 \u03c4 k \u03be + (1 \u2212 \u03c4 k )\u03b8 // update target parameters 12 endOutput :encoder f \u03b8B Image augmentationsDuring self-supervised training, BYOL uses the following image augmentations (which are a subset of the ones presented in[8]):\n\n\u2022\nGaussian blurring: for a 224 \u00d7 224 image, a square Gaussian kernel of size 23 \u00d7 23 is used, with a standard deviation uniformly sampled over [0.1, 2.0]; \u2022 solarization: an optional color transformation x \u2192 x \u00b7 1 {x<0.5} + (1 \u2212 x) \u00b7 1 {x\u22650.5} for pixels with values in [0, 1].\n\nFigure 4 :\n4Semi-supervised training with a fraction of ImageNet labels on a ResNet-50 (\u00d71).\n\nFigure 5 :\n5Results for linear evaluation of BYOL compared to fully supervised baselines with various ResNet architectures.\n\n\n[0.5, 2.0]), cropping, and horizontal flipping. The crop size is 513. Inference is performed on the [513, 513] central crop. For training we use a batch size of 16 and weight decay of 0.0001.We select the base learning rate by sweeping across 5 logarithmically spaced values between 10 \u22123 and 10 \u22121 . The learning rate is multiplied by 0.1 at the 70-th and 90-th percentile of training. We train for 30000 iterations, and average the results on 5 seeds.\n\nFigure 6 :\n6Effect of normalization on the 2 norm of network outputs.\n\nTable 1 :\n1Top-1 and top-5 accuracies (in %) under linear evaluation on ImageNet.\n\nTable 3 :\n3Transfer learning results from ImageNet (IN) with the standard ResNet-50 architecture.\n\n\nis below 1.25 n thresholds where d p is the predicted depth and d gt is the ground truth depth[36]. BYOL is better or on par with other methods for each metric. For instance, the challenging pct.<1.25 measure is respectively improved by +3.5 points and +1.3 points compared to supervised and SimCLR baselines.Method \nAP50 mIoU \n\nSupervised-IN [9] \n74.4 \n74.4 \nMoCo [9] \n74.9 \n72.5 \nSimCLR (repro) \n75.2 \n75.2 \nBYOL (ours) \n77.5 \n76.3 \n\n(a) Transfer results in semantic \nsegmentation and object detection. \n\nHigher better \nLower better \nMethod \npct.< 1.25 pct.< 1.25 2 pct.< 1.25 3 \nrms \nrel \n\nSupervised-IN [70] \n81.1 \n95.3 \n98.8 \n0.573 0.127 \nSimCLR (repro) \n83.3 \n96.5 \n99.1 \n0.557 \n0.134 \nBYOL (ours) \n84.6 \n96.7 \n99.1 \n0.541 0.129 \n\n(b) Transfer results on NYU v2 depth estimation. \n\n\n\nTable 4 :\n4Results on transferring BYOL's representation to other vision tasks.\n\n\n(a) Results for different target modes. \u2020 In the stop gradient of online, \u03c4 = \u03c4base = 0 is kept constant throughout training.(b) Intermediate variants between BYOL and SimCLR.Target \n\u03c4base \nTop-1 \n\nConstant random network \n1 18.8\u00b10.7 \nMoving average of online 0.999 \n69.8 \nMoving average of online \n0.99 \n72.5 \nMoving average of online \n0.9 \n68.4 \nStop gradient of online  \u2020 \n0 \n0.3 \n\nMethod Predictor Target network \u03b2 Top-1 \n\nBYOL \n0 \n72.5 \n1 \n70.9 \n1 \n70.7 \nSimCLR \n1 \n69.4 \n1 \n69.1 \n0 \n0.3 \n0 \n0.2 \n0 \n0.1 \n\n\n\nTable 5 :\n5Ablations with top-1 accuracy (in %) at 300 epochs under linear evaluation on ImageNet.\n\n[ 13 ]\n13Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A theoretical analysis of contrastive unsupervised representation learning. In International Conference on Machine Learning, 2019. [14] R. Manmatha, Chao-Yuan Wu, Alexander J. Smola, and Philipp Kr\u00e4henb\u00fchl. Sampling matters in deep embedding learning. In International Conference on Computer Vision, 2017. [15] Ben Harwood, Vijay B. G. Kumar, Gustavo Carneiro, Ian Reid, and Tom Drummond. Smart mining for deep metric learning. In International Conference on Computer Vision, 2017. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Neural Information Processing Systems, 2014. [25] Jeff Donahue, Philipp Kr\u00e4henb\u00fchl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016. [26] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Mart\u00edn Arjovsky, Olivier Mastropietro, and Aaron C. Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2017. Olivier J. H\u00e9naff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, and A\u00e4ron van den Oord. Data-efficient image recognition with contrastive predictive coding. In International Conference on Machine Learning, 2019. [30] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2019. [31] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In Neural Information Processing Systems, 2019. Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context encoders: Feature learning by inpainting. In Computer Vision and Pattern Recognition, 2016.[16] Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In \nInternational Conference on Machine Learning, 2013. \n\n[17] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of \nvisual features. In European Conference on Computer Vision, 2018. \n\n[18] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, \nAditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. \nInternational Journal of Computer Vision, 115(3):211-252, 2015. \n\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Computer \nVision and Pattern Recognition, 2016. \n\n[20] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In \nComputer Vision and Pattern Recognition, 2015. \n\n[21] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features \nwith denoising autoencoders. In International Conference on Machine Learning, 2008. \n\n[22] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. \n\n[23] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic back-propagation and variational inference in \ndeep latent gaussian models. arXiv preprint arXiv:1401.4082, 2014. \n\n[24] [27] Jeff Donahue and Karen Simonyan. Large scale adversarial representation learning. In Neural Information Processing \nSystems, 2019. \n\n[28] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. \narXiv preprint arXiv:1809.11096, 2018. \n\n[29] [32] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. arXiv preprint \narXiv:1912.01991, 2019. \n\n[33] Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven CH Hoi. Prototypical contrastive learning of unsupervised \nrepresentations. arXiv preprint arXiv:2005.04966, 2020. \n\n[34] Rishabh Jain, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. \narXiv preprint arXiv:2003.04297, 2020. \n\n[35] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance \ndiscrimination. In Computer Vision and Pattern Recognition, 2018. \n\n[36] Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In International Conference on Computer \nVision, 2017. \n\n[37] Richard Zhang, Phillip Isola, and Alexei A. Efros. Colorful image colorization. In European Conference on Computer \nVision, 2016. \n\n[38] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic colorization. In \nEuropean Conference on Computer Vision, 2016. \n\n[39] \n\nTable 6 :\n6Parameters used to generate image augmentations.\n\nTable 7 :\n7Different linear evaluation protocols on ResNet architectures by either replacing the spatial \naugmentations with pre-train augmentations, or regularizing the linear classifier. No pre-train augmenta-\ntions and no logits regularization correspond to the evaluation protocol of the main paper, which is the \nsame as in [8, 34]. \n\n0.9. We do not use any regularization methods such as weight decay, gradient clipping [71], tclip [31], or logits \nrescaling. We sweep over the learning rate {0.01, 0.02, 0.05, 0.1, 0.005} and the number of epochs {30, 50} and \nselect the hyperparameters achieving the best performance on our local validation set to report test performance. \n\n\n\nTable 8 :\n8Semi-supervised training with the full ImageNet on a ResNet-50 (\u00d71). We also report other fully supervised methods for extensive comparisons.C.2 Linear evaluation on larger architectures and supervised baselines \n\nBYOL \nSupervised (ours) Supervised [8] \nArchitecture Multiplier Weights Top-1 Top-5 Top-1 \nTop-5 \nTop-1 \n\nResNet-50 \n1\u00d7 \n24M \n74.3 \n91.6 \n76.4 \n92.9 \n76.5 \nResNet-101 \n1\u00d7 \n43M \n76.4 \n93.0 \n78.0 \n94.0 \n-\nResNet-152 \n1\u00d7 \n58M \n77.3 \n93.7 \n79.1 \n94.5 \n-\nResNet-200 \n1\u00d7 \n63M \n77.8 \n93.9 \n79.3 \n94.6 \n-\nResNet-50 \n2\u00d7 \n94M \n77.4 \n93.6 \n79.9 \n95.0 \n77.8 \nResNet-101 \n2\u00d7 \n170M \n78.7 \n94.3 \n80.3 \n95.0 \n-\nResNet-50 \n3\u00d7 \n211M \n78.2 \n93.9 \n80.2 \n95.0 \n-\nResNet-152 \n2\u00d7 \n232M \n79.0 \n94.6 \n80.6 \n95.3 \n-\nResNet-200 \n2\u00d7 \n250M \n79.6 \n94.9 \n80.1 \n95.2 \n-\nResNet-50 \n4\u00d7 \n375M \n78.6 \n94.2 \n80.7 \n95.3 \n78.9 \nResNet-101 \n3\u00d7 \n382M \n78.4 \n94.2 \n80.7 \n95.3 \n-\nResNet-152 \n3\u00d7 \n522M \n79.5 \n94.6 \n80.9 \n95.2 \n-\n\n\n\nTable 10 :\n10Characteristics of image datasets used in transfer learning. When an official test split with \nlabels is not publicly available, we use the official validation split as test set, and create a held-out \nvalidation set from the training examples. \n\n\n\nTable 11 :\n11Transfer results in semantic segmentation and object detection. * uses a larger model (ResNet-101). * * uses an even larger model (ResNet-161).Higher better \nLower better \nMethod \npct.< 1.25 \npct.< 1.25 2 \npct.< 1.25 3 \nrms \nrel \n\nSupervised-IN [70] \n81.1 \n95.3 \n98.8 \n0.573 0.127 \nRelPos [20], by [36]  *  \n80.6 \n94.7 \n98.3 \n0.399 0.146 \nColor [37], by [36]  *  \n76.8 \n93.5 \n97.7 \n0.444 \n0.164 \nExemplar [42, 36]  *  \n71.3 \n90.6 \n96.5 \n0.513 \n0.191 \nMot. Seg. [84], by [36]  *  \n74.2 \n92.4 \n97.4 \n0.473 \n0.177 \nMulti-task [36]  *  \n79.3 \n94.2 \n98.1 \n0.422 \n0.152 \nSimCLR (repro) \n83.3 \n96.5 \n99.1 \n0.557 \n0.134 \nBYOL (ours) \n84.6 \n96.7 \n99.1 \n0.541 \n0.129 \n\n\n\nTable 12 :\n12Transfer results on NYU v2 depth estimation.\n\n\nMethodPlaces365 ImageNet Food101 CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft DTD Pets Caltech-101 FlowersBYOL-IN \n51.0 \n74.3 \n75.3 \n91.3 \n78.4 \n57.3 \n62.6 67.2 60.6 76.5 90.4 \n94.3 \n96.1 \nBYOL-PL \n53.2 \n58.5 \n64.7 \n84.5 \n66.1 \n28.8 64.2 55.6 55.9 68.5 66.1 \n84.3 \n90.0 \nSimCLR-PL 53.0 \n56.5 \n61.7 \n80.8 \n61.1 \n21.2 \n62.5 40.1 44.3 64.3 59.4 \n77.1 \n85.9 \n\n\n\nTable 13 :\n13Transfer learning results (linear evaluation, ResNet-50) from Places365 (PL). For comparison purposes, we also report the results from BYOL trained on ImageNet (BYOL-IN).\n\n\n(a) Projector and predictor depth (i.e. the number of Linear layers).Proj. g \u03b8 \ndepth \nPred. q \u03b8 \ndepth \nTop-1 Top-5 \n\n1 \n\n1 \n61.9 \n86.0 \n2 \n65.0 \n86.8 \n3 \n65.7 \n86.8 \n\n2 \n\n1 \n71.5 \n90.7 \n2 \n72.5 \n90.8 \n3 \n71.4 \n90.4 \n\n3 \n\n1 \n71.4 \n90.4 \n2 \n72.1 \n90.5 \n3 \n72.1 \n90.5 \n\nProjector g \u03b8 \noutput dim \nTop-1 Top-5 \n\n16 \n69.9\u00b10.3 \n89.9 \n32 \n71.3 \n90.6 \n64 \n72.2 \n90.9 \n128 \n72.5 \n91.0 \n256 \n72.5 \n90.8 \n512 \n72.6 \n91.0 \n\n(b) Projection dimension. \n\n\n\nTable 14 :\n14Effect of architectural settings where top-1 and top-5 accuracies are reported in %.\n\nTable 15 :\n15Effect of learning rate and weight decay. We note that BYOL's performance is quite robust within a range of hyperparameters.Batch \nTop-1 \nTop-5 \nsize \nBYOL (ours) SimCLR (repro) BYOL (ours) SimCLR (repro) \n\n4096 \n72.5 \n67.9 \n90.8 \n88.5 \n2048 \n72.4 \n67.8 \n90.7 \n88.5 \n1024 \n72.2 \n67.4 \n90.7 \n88.1 \n512 \n72.2 \n66.5 \n90.8 \n87.6 \n256 \n71.8 \n64.3\u00b12.1 \n90.7 \n86.3\u00b11.0 \n128 \n69.6\u00b10.5 \n63.6 \n89.6 \n85.9 \n64 \n59.7\u00b11.5 \n59.2\u00b12.9 \n83.2\u00b11.2 \n83.0\u00b11.9 \n\n\n\nTable 16 :\n16Influence of the batch size.F.3 Image augmentations \n\n\n\nTable 17\n17compares the impact of individual image transformations on BYOL and SimCLR. BYOL is more resilient to changes of image augmentations across the board.Top-1 \nTop-5 \nImage augmentation \nBYOL (ours) SimCLR (repro) BYOL (ours) SimCLR (repro) \n\nBaseline \n72.5 \n67.9 \n90.8 \n88.5 \nRemove flip \n71.9 \n67.3 \n90.6 \n88.2 \nRemove blur \n71.2 \n65.2 \n90.3 \n86.6 \nRemove color (jittering and grayscale) \n63.4\u00b10.7 \n45.7 \n85.3\u00b10.5 \n70.6 \nRemove color jittering \n71.8 \n63.7 \n90.7 \n85.9 \nRemove grayscale \n70.3 \n61.9 \n89.8 \n84.1 \nRemove blur in T \n72.4 \n67.5 \n90.8 \n88.4 \nRemove solarize in T \n72.3 \n67.7 \n90.8 \n88.2 \nRemove blur and solarize in T \n72.2 \n67.4 \n90.8 \n88.1 \nCrop only \n59.4\u00b10.3 \n40.3\u00b10.3 \n82.4 \n64.8\u00b10.4 \nCrop and flip only \n60.1\u00b10.3 \n40.2 \n83.0\u00b10.3 \n64.8 \nCrop and color only \n70.7 \n64.2 \n90.0 \n86.2 \nCrop and blur only \n61.1\u00b10.3 \n41.7 \n83.9 \n66.4 \n\n\n\nTable 17 :\n17Ablation on image transformations.\n\n\nLoss weight \u03b2 Temperature \u03b1Top-1 \nTop-5 \n\n0 \n0.1 \n72.5 \n90.8 \n\n0.1 \n\n0.01 \n72.2 \n90.7 \n0.1 \n72.4 \n90.9 \n0.3 \n72.7 \n91.0 \n1 \n72.6 \n90.9 \n3 \n72.5 \n90.9 \n10 \n72.5 \n90.9 \n\n0.5 \n\n0.01 \n70.9 \n90.2 \n0.1 \n72.0 \n90.8 \n0.3 \n72.7 \n91.2 \n1 \n72.7 \n91.1 \n3 \n72.6 \n91.1 \n10 \n72.5 \n91.0 \n\n1 \n\n0.01 53.9\u00b10.5 77.5\u00b10.5 \n0.1 \n70.9 \n90.3 \n0.3 \n72.7 \n91.1 \n1 \n72.7 \n91.1 \n3 \n72.6 \n91.0 \n10 \n72.6 \n91.1 \n\n\n\nTable 18 :\n18Top-1 accuracy in % under linear evaluation protocol at 300 epochs of sweep over the temperature \u03b1 and the dispersion term weight \u03b2 when using a predictor and a target network.\n\n\nMethod PredictorTarget parameters \u03b2 Top-1BYOL \n\u03be \n0 \n72.5 \n\u03be \n1 \n70.9 \n\u03be \n1 \n70.7 \nsg(\u03b8) \n1 \n70.2 \nSimCLR \n\u03b8 \n1 \n69.4 \nsg(\u03b8) \n1 \n70.1 \nsg(\u03b8) \n1 \n69.2 \n\u03b8 \n1 \n69.0 \nsg(\u03b8) \n0 \n5.5 \n\u03b8 \n0 \n0.3 \n\u03be \n0 \n0.2 \nsg(\u03b8) \n0 \n0.1 \n\u03b8 \n0 \n0.1 \n\n\n\nTable 19 :\n19Top-1 accuracy in %, under linear evaluation protocol at 300 epochs, of intermediate variants between BYOL and SimCLR (with caveats discussed in Appendix F.5). sg means stop gradient.\n\nTable 20 :\n20Top-1 accuracy in % under linear evaluation protocol at 300 epochs for different normalizations in the loss.\n\n\nAs in SimCLR and official implementation of LARS, we exclude bias # and batchnorm weight from the Lars adaptation and weightdecay. G.2 Network definition def network(inputs): \"\"\"Build the encoder, projector and predictor.\"\"\" embedding = ResNet(name= encoder , configuration= ResNetV1_50x1 )(inputs) proj_out = MLP(name= projector )(embedding) pred_out = MLP(name= predictor )(proj_out) return dict(projection=proj_out, prediction=pred_out) class MLP(hk.Module): \"\"\"Multi Layer Perceptron, with normalization.\"\"\" def __call__(self, inputs): out = hk.Linear(output_size=HPS[ mlp_hidden_size ])(inputs) out = hk.BatchNorm(**HPS[ batchnorm_kwargs ])(out) out = jax.nn.relu(out) out = hk.Linear(output_size=HPS[ projection_size ])(out) return out # For simplicity, we omit BatchNorm related states. # In the actual code, we use hk.transform_with_state. The corresponding # net_init function outputs both a params and a state variable, # with state containing the moving averages computed by BatchNorm net_init, net_apply = hk.transform(network)G.1 Hyper-parameters \n\nHPS = dict( \nmax_steps=int(1000. * 1281167 / 4096), # 1000 epochs \nbatch_size=4096, \nmlp_hidden_size=4096, \nprojection_size=256, \nbase_target_ema=4e-3, \noptimizer_config=dict( \noptimizer_name= lars , \nbeta=0.9, \ntrust_coef=1e-3, \nweight_decay=1.5e-6, \n# exclude_bias_from_adaption=True), \nlearning_rate_schedule=dict( \n# The learning rate is linearly increase up to \n# its base value * batchisze / 256 after warmup_steps \n# global steps and then anneal with a cosine schedule. \nbase_learning_rate=0.2, \nwarmup_steps=int(10. * 1281167 / 4096), \nanneal_schedule= cosine ), \nbatchnorm_kwargs=dict( \ndecay_rate=0.9, \neps=1e-5), \nseed=1337, \n) \n\ndef __init(self, name): \nsuper().__init__(name=name) \n\n\nThroughout this paper, the term bootstrap is used in its idiomatic sense rather than the statistical sense.\nTarget networks have been introduced in[47] and are commonly used[48,49,50] in deep reinforcement learning (RL). In deep RL, target networks stabilize the bootstrapping updates provided by the Bellman equation, making them appealing to stabilize the bootstrap mechanism in BYOL. While fixed target networks are more common in deep RL, BYOL uses a weighted moving average of previous networks, similar to[51], in order to provide smoother changes in the target representation.\nWhile we could directly predict the representation y and not a projection z, previous work[8] have empirically shown that using this projection improves performance.\nThe only dependency on batch size in our training pipeline sits within the batch normalization layers.\nhttps://github.com/Philip-Bachman/amdim-public/blob/master/costs.py\nAcknowledgementsThe authors would like to thank the following people for their help throughout the process of writing this paper, in alphabetical order: Aaron van den Oord, Andrew Brock, Jason Ramapuram, Jeffrey De Fauw, Karen Simonyan, Katrina McKinney, Nathalie Beauguerlange, Olivier Henaff, Oriol Vinyals, Pauline Luc, Razvan Pascanu, Sander Dieleman, and the DeepMind team.\nNeocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Kunihiko Fukushima, Biological Cybernetics. 364Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4):193-202, 1980.\n\nSlow feature analysis: Unsupervised learning of invariances. Laurenz Wiskott, Terrence J Sejnowski, Neural Computation. 144Laurenz Wiskott and Terrence J Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural Computation, 14(4), 2002.\n\nA fast learning algorithm for deep belief nets. Geoffrey E Hinton, Simon Osindero, Yee-Whye Teh, Neural Computation. 187Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):1527-1554, 2006.\n\nLearning and transferring mid-level image representations using convolutional neural networks. Maxime Oquab, Leon Bottou, Ivan Laptev, Josef Sivic, Computer Vision and Pattern Recognition. Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level image representations using convolutional neural networks. In Computer Vision and Pattern Recognition, 2014.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Computer Vision and Pattern Recognition. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition, 2014.\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, Computer Vision and Pattern Recognition. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Computer Vision and Pattern Recognition, 2015.\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey E Hinton, arXiv:2002.05709arXiv preprintTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross B Girshick, arXiv:1911.05722arXiv preprintKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.\n\nA\u00e4ron Van Den Oord, Yazhe Li, Oriol Vinyals, arXiv:1807.03748Representation learning with contrastive predictive coding. arXiv preprintA\u00e4ron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\n\nYonglong Tian, Dilip Krishnan, Phillip Isola, arXiv:1906.05849v4Contrastive multiview coding. arXiv preprintYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint arXiv:1906.05849v4, 2019.\n\nWhat makes for good views for contrastive learning. Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, Phillip Isola, arXiv:2005.10243arXiv preprintYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning. arXiv preprint arXiv:2005.10243, 2020.\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. Mehdi Noroozi, Paolo Favaro, European Conference on Computer Vision. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, 2016.\n\nPhoto-realistic single image super-resolution using a generative adversarial network. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Computer Vision and Pattern Recognition. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In Computer Vision and Pattern Recognition, 2017.\n\nDiscriminative unsupervised feature learning with convolutional neural networks. Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox, Neural Information Processing Systems. Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In Neural Information Processing Systems, 2014.\n\nUnsupervised representation learning by predicting image rotations. Spyros Gidaris, Praveer Singh, Nikos Komodakis, arXiv:1803.07728arXiv preprintSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018.\n\nRevisiting self-supervised visual representation learning. Alexander Kolesnikov, Xiaohua Zhai, Lucas Beyer, Computer Vision and Pattern Recognition. Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation learning. In Computer Vision and Pattern Recognition, 2019.\n\nBootstrap latent-predictive representations for multitask reinforcement learning. Daniel Guo, Bilal Bernardo Avila Pires, Jean-Bastien Piot, Florent Grill, R\u00e9mi Altch\u00e9, Mohammad Gheshlaghi Munos, Azar, International Conference on Machine Learning. Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altch\u00e9, R\u00e9mi Munos, and Mohammad Gheshlaghi Azar. Bootstrap latent-predictive representations for multitask reinforcement learning. In International Conference on Machine Learning, 2020.\n\nSelf-organizing neural network that discovers surfaces in random-dot stereograms. Suzanna Becker, Geoffrey E Hinton, Nature. 3556356Suzanna Becker and Geoffrey E. Hinton. Self-organizing neural network that discovers surfaces in random-dot stereograms. Nature, 355(6356):161-163, 1992.\n\nHuman-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin A Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Legg, and Demis Hassabis. Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen. King, Dharshan Kumaran, Daan Wierstra, Shane518Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin A. Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen. King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529-533, 2015.\n\nAsynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International Conference on Machine Learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016.\n\nRainbow: Combining improvements in deep reinforcement learning. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, David Silver, AAAI Conference on Artificial Intelligence. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Gheshlaghi Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In AAAI Conference on Artificial Intelligence, 2018.\n\nDeep reinforcement learning and the deadly triad. Yotam Hado Van Hasselt, Florian Doron, Matteo Strub, Nicolas Hessel, Joseph Sonnerat, Modayil, Deep Reinforcement Learning Workshop NeurIPS. Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. Deep reinforcement learning and the deadly triad. Deep Reinforcement Learning Workshop NeurIPS, 2018.\n\nP Timothy, Jonathan J Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.\n\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, Skye Wanderman-Milne, JAX: composable transformations of Python+NumPy programs. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy programs, 2018.\n\nHaiku: Sonnet for JAX. Tom Hennigan, Trevor Cai, Tamara Norman, Igor Babuschkin, Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin. Haiku: Sonnet for JAX, 2020.\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, arXiv:1605.07146arXiv preprintSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, International Conference on Machine Learning. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015.\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, International Conference on Machine Learning. Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In International Conference on Machine Learning, 2010.\n\nScaling SGD batch size to 32k for imagenet training. Yang You, Igor Gitman, Boris Ginsburg, arXiv:1708.03888arXiv preprintYang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for imagenet training. arXiv preprint arXiv:1708.03888, 2017.\n\nSGDR: stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017.\n\nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, arXiv:1706.02677Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprintPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\nPlaces: A 10 million image database for scene recognition. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba, Transactions on Pattern Analysis and Machine Intelligence. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. Transactions on Pattern Analysis and Machine Intelligence, 2017.\n\nDo better ImageNet models transfer better?. Simon Kornblith, Jonathon Shlens, Quoc V Le, Computer Cision and Pattern Recognition. Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better ImageNet models transfer better? In Computer Cision and Pattern Recognition, 2019.\n\nMaxup: A simple way to improve generalization of neural network training. Chengyue Gong, Tongzheng Ren, Mao Ye, Qiang Liu, arXiv:2002.09024arXiv preprintChengyue Gong, Tongzheng Ren, Mao Ye, and Qiang Liu. Maxup: A simple way to improve generalization of neural network training. arXiv preprint arXiv:2002.09024, 2020.\n\nXiaohua Zhai, Joan Puigcerver, Alexander I Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andr\u00e9 Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv: Computer Vision and Pattern Recognition. Xiaohua Zhai, Joan Puigcerver, Alexander I Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andr\u00e9 Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv: Computer Vision and Pattern Recognition, 2019.\n\nS4L: Self-supervised semi-supervised learning. Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer, International Conference on Computer Vision. Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-supervised semi-supervised learning. In International Conference on Computer Vision, 2019.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, University of TorontoTechnical reportAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\nSun database: Large-scale scene recognition from abbey to zoo. Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, Antonio Torralba, Computer Vision and Pattern Recognition. Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In Computer Vision and Pattern Recognition, 2010.\n\nThe Pascal visual object classes (VOC) challenge. Mark Everingham, Luc Van Gool, K I Christopher, John Williams, Andrew Winn, Zisserman, International Journal of Computer Vision. 882Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The Pascal visual object classes (VOC) challenge. International Journal of Computer Vision, 88(2):303-338, 2010.\n\nIasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. Mircea Cimpoi, Subhransu Maji, Computer Vision and Pattern Recognition. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Computer Vision and Pattern Recognition, 2014.\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Neural Information Processing Systems. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In Neural Information Processing Systems, 2015.\n\nDeeper depth prediction with fully convolutional residual networks. I Laina, C Rupprecht, V Belagiannis, F Tombari, N Navab, International Conference on 3D Vision. I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab. Deeper depth prediction with fully convolutional residual networks. In International Conference on 3D Vision, 2016.\n\nOn the difficulty of training recurrent neural networks. Razvan Pascanu, Tomas Mikolov, Yoshua Bengio, International Conference on Machine Learning. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning, 2013.\n\nRandaugment: Practical automated data augmentation with a reduced search space. Barret Ekin D Cubuk, Jonathon Zoph, Quoc V Shlens, Le, arXiv:1909.13719arXiv preprintEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. arXiv preprint arXiv:1909.13719, 2019.\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, Computer Vision and Pattern Recognition. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Computer Vision and Pattern Recognition, pages 2818-2826, 2016.\n\nFood-101 -mining discriminative components with random forests. Lukas Bossard, Matthieu Guillaumin, Luc Van Gool, European Conference on Computer Vision. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 -mining discriminative components with random forests. In European Conference on Computer Vision, 2014.\n\nBirdsnap: Large-scale fine-grained visual categorization of birds. Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, Peter N Belhumeur, Computer Vision and Pattern Recognition. Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L. Alexander, David W. Jacobs, and Peter N. Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. In Computer Vision and Pattern Recognition, 2014.\n\n3D object representations for fine-grained categorization. Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei, Workshop on 3D Representation and Recognition. Sydney, AustraliaJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D object representations for fine-grained categorization. In Workshop on 3D Representation and Recognition, Sydney, Australia, 2013.\n\nFine-grained visual classification of aircraft. Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew B Blaschko, Andrea Vedaldi, arXiv:1306.5151arXiv preprintSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew B. Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n\nCats and dogs. O M Parkhi, A Vedaldi, A Zisserman, C V Jawahar, Computer Vision and Pattern Recognition. O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. Cats and dogs. In Computer Vision and Pattern Recognition, 2012.\n\nLearning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Pattern Recognition Workshop. Li Fei-Fei, Rob Fergus, Pietro Perona, Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Pattern Recognition Workshop, 2004.\n\nAutomated flower classification over a large number of classes. Maria-Elena Nilsback, Andrew Zisserman, Indian Conference on Computer Vision, Graphics and Image Processing. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In Indian Conference on Computer Vision, Graphics and Image Processing, 2008.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell, International Conference on Machine Learning. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, 2014.\n\nA robust hybrid of lasso and ridge regression. B Art, Owen, Contemporary Mathematics. 4437Art B Owen. A robust hybrid of lasso and ridge regression. Contemporary Mathematics, 443(7):59-72, 2007.\n\nLocal aggregation for unsupervised learning of visual embeddings. Chengxu Zhuang, Alex Lin Zhai, Daniel Yamins, International Conference on Computer Vision. Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning of visual embeddings. In International Conference on Computer Vision, 2019.\n\nLearning features by watching objects move. Deepak Pathak, Ross Girshick, Piotr Doll\u00e1r, Trevor Darrell, Bharath Hariharan, Conference on Computer Vision and Pattern Recognition. Deepak Pathak, Ross Girshick, Piotr Doll\u00e1r, Trevor Darrell, and Bharath Hariharan. Learning features by watching objects move. In Conference on Computer Vision and Pattern Recognition, 2017.\n\nGroup normalization. Yuxin Wu, Kaiming He, European Conference on Computer Vision. Yuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision, 2018.\n\nLoss function def loss_fn(online_params, target_params, image_1, image_2): \"\"\"Compute BYOL s loss function. G.3 Loss function def loss_fn(online_params, target_params, image_1, image_2): \"\"\"Compute BYOL s loss function.\n\nUpdate function optimizer = Optimizer(**HPS. G , G.5 Update function optimizer = Optimizer(**HPS[ optimizer_config ])\n\ndef update_fn(online_params, target_params, opt_state, global_step, image_1, image_2): \"\"\"Update online and target parameters. def update_fn(online_params, target_params, opt_state, global_step, image_1, image_2): \"\"\"Update online and target parameters.\n", "annotations": {"author": "[{\"end\":130,\"start\":72},{\"end\":182,\"start\":131},{\"end\":235,\"start\":183},{\"end\":292,\"start\":236},{\"end\":353,\"start\":293},{\"end\":391,\"start\":354},{\"end\":424,\"start\":392},{\"end\":465,\"start\":425},{\"end\":504,\"start\":466},{\"end\":549,\"start\":505},{\"end\":580,\"start\":550},{\"end\":618,\"start\":581},{\"end\":649,\"start\":619},{\"end\":682,\"start\":650},{\"end\":711,\"start\":683}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":85},{\"end\":144,\"start\":139},{\"end\":197,\"start\":191},{\"end\":251,\"start\":245},{\"end\":311,\"start\":302},{\"end\":371,\"start\":360},{\"end\":404,\"start\":397},{\"end\":445,\"start\":440},{\"end\":484,\"start\":481},{\"end\":529,\"start\":525},{\"end\":560,\"start\":556},{\"end\":598,\"start\":587},{\"end\":629,\"start\":624},{\"end\":662,\"start\":657}]", "author_first_name": "[{\"end\":84,\"start\":72},{\"end\":138,\"start\":131},{\"end\":190,\"start\":183},{\"end\":244,\"start\":236},{\"end\":299,\"start\":293},{\"end\":301,\"start\":300},{\"end\":359,\"start\":354},{\"end\":396,\"start\":392},{\"end\":433,\"start\":425},{\"end\":439,\"start\":434},{\"end\":473,\"start\":466},{\"end\":480,\"start\":474},{\"end\":513,\"start\":505},{\"end\":524,\"start\":514},{\"end\":555,\"start\":550},{\"end\":586,\"start\":581},{\"end\":623,\"start\":619},{\"end\":656,\"start\":650},{\"end\":691,\"start\":683}]", "author_affiliation": "[{\"end\":129,\"start\":112},{\"end\":181,\"start\":164},{\"end\":234,\"start\":217},{\"end\":291,\"start\":274},{\"end\":352,\"start\":335},{\"end\":390,\"start\":373},{\"end\":423,\"start\":406},{\"end\":464,\"start\":447},{\"end\":503,\"start\":486},{\"end\":548,\"start\":531},{\"end\":579,\"start\":562},{\"end\":617,\"start\":600},{\"end\":648,\"start\":631},{\"end\":681,\"start\":664},{\"end\":710,\"start\":693}]", "title": "[{\"end\":69,\"start\":1},{\"end\":780,\"start\":712}]", "venue": null, "abstract": "[{\"end\":1759,\"start\":782}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1778,\"start\":1775},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1859,\"start\":1856},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1861,\"start\":1859},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1863,\"start\":1861},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1923,\"start\":1920},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1925,\"start\":1923},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1927,\"start\":1925},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1929,\"start\":1927},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2109,\"start\":2106},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2111,\"start\":2109},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2114,\"start\":2111},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2117,\"start\":2114},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2120,\"start\":2117},{\"end\":2424,\"start\":2420},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2467,\"start\":2464},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2470,\"start\":2467},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2488,\"start\":2485},{\"end\":2525,\"start\":2521},{\"end\":2528,\"start\":2525},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2651,\"start\":2648},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2654,\"start\":2651},{\"end\":3291,\"start\":3287},{\"end\":3294,\"start\":3291},{\"end\":4025,\"start\":4021},{\"end\":5216,\"start\":5212},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5218,\"start\":5216},{\"end\":5446,\"start\":5442},{\"end\":5449,\"start\":5446},{\"end\":5452,\"start\":5449},{\"end\":5484,\"start\":5480},{\"end\":5532,\"start\":5528},{\"end\":5535,\"start\":5532},{\"end\":5538,\"start\":5535},{\"end\":5541,\"start\":5538},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5811,\"start\":5808},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5814,\"start\":5811},{\"end\":5817,\"start\":5814},{\"end\":5820,\"start\":5817},{\"end\":5823,\"start\":5820},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5826,\"start\":5823},{\"end\":5828,\"start\":5826},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5916,\"start\":5914},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5919,\"start\":5916},{\"end\":6171,\"start\":6167},{\"end\":6174,\"start\":6171},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6274,\"start\":6271},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6276,\"start\":6274},{\"end\":6363,\"start\":6359},{\"end\":6995,\"start\":6991},{\"end\":6998,\"start\":6995},{\"end\":7033,\"start\":7029},{\"end\":7036,\"start\":7033},{\"end\":7059,\"start\":7055},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7085,\"start\":7081},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7114,\"start\":7110},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7150,\"start\":7146},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7153,\"start\":7150},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7226,\"start\":7222},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7295,\"start\":7293},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7298,\"start\":7295},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7387,\"start\":7383},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8453,\"start\":8449},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11110,\"start\":11109},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11341,\"start\":11337},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13998,\"start\":13997},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14806,\"start\":14805},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14851,\"start\":14847},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14878,\"start\":14874},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14881,\"start\":14878},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14884,\"start\":14881},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15217,\"start\":15213},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16205,\"start\":16201},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17574,\"start\":17571},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17807,\"start\":17803},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17822,\"start\":17818},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17958,\"start\":17955},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18604,\"start\":18600},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18607,\"start\":18604},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18609,\"start\":18607},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18794,\"start\":18791},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19029,\"start\":19025},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19065,\"start\":19061},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19287,\"start\":19283},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19335,\"start\":19331},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19467,\"start\":19463},{\"end\":20334,\"start\":20330},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20837,\"start\":20833},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21064,\"start\":21060},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21067,\"start\":21064},{\"end\":21070,\"start\":21067},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21073,\"start\":21070},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21075,\"start\":21073},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21350,\"start\":21346},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21420,\"start\":21417},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21502,\"start\":21498},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21837,\"start\":21834},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22137,\"start\":22133},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22140,\"start\":22137},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22142,\"start\":22140},{\"end\":22145,\"start\":22142},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22272,\"start\":22269},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22643,\"start\":22642},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23068,\"start\":23065},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23071,\"start\":23068},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23641,\"start\":23637},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23672,\"start\":23668},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23688,\"start\":23684},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23718,\"start\":23714},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24169,\"start\":24166},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24366,\"start\":24363},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24405,\"start\":24401},{\"end\":24900,\"start\":24896},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24928,\"start\":24924},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25602,\"start\":25599},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25605,\"start\":25602},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25996,\"start\":25993},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29820,\"start\":29816},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31495,\"start\":31492},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32361,\"start\":32358},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35448,\"start\":35445},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35906,\"start\":35902},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":35909,\"start\":35906},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35910,\"start\":35909},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36793,\"start\":36789},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":37228,\"start\":37225},{\"end\":37231,\"start\":37228},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":38357,\"start\":38354},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":38360,\"start\":38357},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39167,\"start\":39164},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39803,\"start\":39800},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39988,\"start\":39985},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":40049,\"start\":40045},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":40114,\"start\":40110},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":40437,\"start\":40434},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":40994,\"start\":40991},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":41157,\"start\":41153},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":41490,\"start\":41486},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41641,\"start\":41638},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":41671,\"start\":41667},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":41686,\"start\":41682},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":41705,\"start\":41701},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":41720,\"start\":41716},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":41751,\"start\":41747},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":41771,\"start\":41767},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":41791,\"start\":41787},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":41837,\"start\":41833},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41882,\"start\":41878},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":41905,\"start\":41901},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":41923,\"start\":41919},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":41952,\"start\":41948},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41963,\"start\":41960},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":42584,\"start\":42580},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":42671,\"start\":42667},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":42733,\"start\":42729},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":42818,\"start\":42814},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":43199,\"start\":43195},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":43321,\"start\":43317},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":43439,\"start\":43435},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":43546,\"start\":43542},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":44046,\"start\":44042},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":44049,\"start\":44046},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":44051,\"start\":44049},{\"end\":44999,\"start\":44995},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":45002,\"start\":44999},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":45005,\"start\":45002},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":45007,\"start\":45005},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":46494,\"start\":46491},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":46514,\"start\":46511},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":46930,\"start\":46927},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":47373,\"start\":47369},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":48043,\"start\":48039},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":48298,\"start\":48294},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":48301,\"start\":48298},{\"end\":48416,\"start\":48411},{\"end\":48420,\"start\":48416},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":48937,\"start\":48933},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":49799,\"start\":49795},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":50544,\"start\":50540},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":50744,\"start\":50740},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":52966,\"start\":52962},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":55292,\"start\":55289},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":58636,\"start\":58633},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":74469,\"start\":74465},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":74495,\"start\":74491},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":74498,\"start\":74495},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":74501,\"start\":74498},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":74833,\"start\":74829},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":74995,\"start\":74992}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":57836,\"start\":57650},{\"attributes\":{\"id\":\"fig_1\"},\"end\":58303,\"start\":57837},{\"attributes\":{\"id\":\"fig_2\"},\"end\":58638,\"start\":58304},{\"attributes\":{\"id\":\"fig_3\"},\"end\":58917,\"start\":58639},{\"attributes\":{\"id\":\"fig_5\"},\"end\":59011,\"start\":58918},{\"attributes\":{\"id\":\"fig_6\"},\"end\":59136,\"start\":59012},{\"attributes\":{\"id\":\"fig_7\"},\"end\":59592,\"start\":59137},{\"attributes\":{\"id\":\"fig_8\"},\"end\":59663,\"start\":59593},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":59746,\"start\":59664},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":59845,\"start\":59747},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":60636,\"start\":59846},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":60717,\"start\":60637},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":61231,\"start\":60718},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":61331,\"start\":61232},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":66251,\"start\":61332},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":66312,\"start\":66252},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":66998,\"start\":66313},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":67911,\"start\":66999},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":68173,\"start\":67912},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":68847,\"start\":68174},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":68906,\"start\":68848},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":69267,\"start\":68907},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":69452,\"start\":69268},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":69897,\"start\":69453},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":69996,\"start\":69898},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":70452,\"start\":69997},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":70521,\"start\":70453},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":71380,\"start\":70522},{\"attributes\":{\"id\":\"tab_28\",\"type\":\"table\"},\"end\":71429,\"start\":71381},{\"attributes\":{\"id\":\"tab_29\",\"type\":\"table\"},\"end\":71814,\"start\":71430},{\"attributes\":{\"id\":\"tab_30\",\"type\":\"table\"},\"end\":72005,\"start\":71815},{\"attributes\":{\"id\":\"tab_31\",\"type\":\"table\"},\"end\":72235,\"start\":72006},{\"attributes\":{\"id\":\"tab_32\",\"type\":\"table\"},\"end\":72433,\"start\":72236},{\"attributes\":{\"id\":\"tab_34\",\"type\":\"table\"},\"end\":72556,\"start\":72434},{\"attributes\":{\"id\":\"tab_35\",\"type\":\"table\"},\"end\":74317,\"start\":72557}]", "paragraph": "[{\"end\":1780,\"start\":1779},{\"end\":2655,\"start\":1782},{\"end\":3900,\"start\":2657},{\"end\":4925,\"start\":3902},{\"end\":5083,\"start\":4927},{\"end\":5756,\"start\":5100},{\"end\":6345,\"start\":5758},{\"end\":6811,\"start\":6347},{\"end\":7299,\"start\":6813},{\"end\":7852,\"start\":7301},{\"end\":8245,\"start\":7854},{\"end\":9802,\"start\":8256},{\"end\":11111,\"start\":9804},{\"end\":12690,\"start\":11144},{\"end\":13999,\"start\":12692},{\"end\":14319,\"start\":14023},{\"end\":15285,\"start\":14457},{\"end\":15581,\"start\":15287},{\"end\":15930,\"start\":15583},{\"end\":16318,\"start\":15932},{\"end\":17107,\"start\":16343},{\"end\":17508,\"start\":17194},{\"end\":17849,\"start\":17510},{\"end\":18350,\"start\":17876},{\"end\":19240,\"start\":18352},{\"end\":20159,\"start\":19257},{\"end\":20882,\"start\":20187},{\"end\":22089,\"start\":20884},{\"end\":23719,\"start\":22091},{\"end\":24016,\"start\":23721},{\"end\":24290,\"start\":24018},{\"end\":24629,\"start\":24292},{\"end\":25182,\"start\":24631},{\"end\":26164,\"start\":25249},{\"end\":26838,\"start\":26166},{\"end\":27958,\"start\":26840},{\"end\":28506,\"start\":27960},{\"end\":29821,\"start\":28508},{\"end\":30385,\"start\":29949},{\"end\":30949,\"start\":30451},{\"end\":31343,\"start\":30951},{\"end\":31834,\"start\":31345},{\"end\":32518,\"start\":31849},{\"end\":32632,\"start\":32520},{\"end\":32988,\"start\":32634},{\"end\":33499,\"start\":33007},{\"end\":34305,\"start\":33515},{\"end\":34722,\"start\":34695},{\"end\":34969,\"start\":34724},{\"end\":35392,\"start\":34971},{\"end\":37124,\"start\":35394},{\"end\":37396,\"start\":37169},{\"end\":37637,\"start\":37398},{\"end\":37748,\"start\":37639},{\"end\":37864,\"start\":37773},{\"end\":38046,\"start\":37931},{\"end\":38264,\"start\":38048},{\"end\":39202,\"start\":38266},{\"end\":39482,\"start\":39204},{\"end\":40285,\"start\":39484},{\"end\":42301,\"start\":40287},{\"end\":42371,\"start\":42303},{\"end\":42500,\"start\":42373},{\"end\":42585,\"start\":42502},{\"end\":42672,\"start\":42587},{\"end\":42734,\"start\":42674},{\"end\":42799,\"start\":42736},{\"end\":43178,\"start\":42801},{\"end\":43305,\"start\":43180},{\"end\":43415,\"start\":43307},{\"end\":43711,\"start\":43417},{\"end\":43806,\"start\":43713},{\"end\":43896,\"start\":43808},{\"end\":43955,\"start\":43898},{\"end\":44916,\"start\":43998},{\"end\":46378,\"start\":44949},{\"end\":46932,\"start\":46435},{\"end\":47195,\"start\":46934},{\"end\":47931,\"start\":47247},{\"end\":48302,\"start\":47983},{\"end\":48528,\"start\":48304},{\"end\":48581,\"start\":48530},{\"end\":48643,\"start\":48583},{\"end\":48726,\"start\":48645},{\"end\":48807,\"start\":48728},{\"end\":48976,\"start\":48809},{\"end\":49593,\"start\":49031},{\"end\":50285,\"start\":49625},{\"end\":50876,\"start\":50287},{\"end\":52056,\"start\":50910},{\"end\":52972,\"start\":52103},{\"end\":53116,\"start\":53027},{\"end\":55126,\"start\":53308},{\"end\":55386,\"start\":55163},{\"end\":55978,\"start\":55513},{\"end\":56572,\"start\":55980},{\"end\":56676,\"start\":56666},{\"end\":57363,\"start\":56678},{\"end\":57649,\"start\":57623}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14456,\"start\":14320},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16342,\"start\":16319},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17193,\"start\":17108},{\"attributes\":{\"id\":\"formula_3\"},\"end\":25211,\"start\":25183},{\"attributes\":{\"id\":\"formula_4\"},\"end\":29948,\"start\":29822},{\"attributes\":{\"id\":\"formula_5\"},\"end\":30450,\"start\":30386},{\"attributes\":{\"id\":\"formula_6\"},\"end\":34694,\"start\":34306},{\"attributes\":{\"id\":\"formula_7\"},\"end\":37772,\"start\":37749},{\"attributes\":{\"id\":\"formula_8\"},\"end\":37930,\"start\":37865},{\"attributes\":{\"id\":\"formula_9\"},\"end\":53243,\"start\":53117},{\"attributes\":{\"id\":\"formula_10\"},\"end\":53307,\"start\":53243},{\"attributes\":{\"id\":\"formula_11\"},\"end\":55512,\"start\":55387},{\"attributes\":{\"id\":\"formula_12\"},\"end\":56665,\"start\":56630},{\"attributes\":{\"id\":\"formula_13\"},\"end\":57622,\"start\":57364}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":10340,\"start\":10331},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":13228,\"start\":13219},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21163,\"start\":21156},{\"end\":22342,\"start\":22335},{\"end\":22579,\"start\":22572},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23343,\"start\":23336},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24204,\"start\":24196},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":25077,\"start\":25069},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":29195,\"start\":29187},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":30905,\"start\":30897},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":35391,\"start\":35384},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":38068,\"start\":38061},{\"end\":39063,\"start\":39056},{\"end\":39396,\"start\":39389},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":39588,\"start\":39581},{\"end\":40177,\"start\":40170},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":43954,\"start\":43946},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49080,\"start\":49049},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":50284,\"start\":50276},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":51425,\"start\":51417},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":51866,\"start\":51857},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":52004,\"start\":51995},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":52373,\"start\":52365},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":53335,\"start\":53327},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":53589,\"start\":53581},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":54286,\"start\":54278},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":54609,\"start\":54601},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":54779,\"start\":54771},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":54811,\"start\":54803},{\"attributes\":{\"ref_id\":\"tab_34\"},\"end\":56967,\"start\":56959}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1773,\"start\":1761},{\"attributes\":{\"n\":\"2\"},\"end\":5098,\"start\":5086},{\"attributes\":{\"n\":\"3\"},\"end\":8254,\"start\":8248},{\"attributes\":{\"n\":\"3.1\"},\"end\":11133,\"start\":11114},{\"attributes\":{\"n\":\"3\"},\"end\":11142,\"start\":11136},{\"attributes\":{\"n\":\"3.1\"},\"end\":14021,\"start\":14002},{\"attributes\":{\"n\":\"3.2\"},\"end\":17874,\"start\":17852},{\"end\":19255,\"start\":19243},{\"attributes\":{\"n\":\"4\"},\"end\":20185,\"start\":20162},{\"attributes\":{\"n\":\"5\"},\"end\":25247,\"start\":25213},{\"attributes\":{\"n\":\"6\"},\"end\":31847,\"start\":31837},{\"end\":33005,\"start\":32991},{\"end\":33513,\"start\":33502},{\"end\":37167,\"start\":37127},{\"end\":43996,\"start\":43958},{\"end\":44947,\"start\":44919},{\"end\":46433,\"start\":46381},{\"end\":47245,\"start\":47198},{\"end\":47981,\"start\":47934},{\"end\":49029,\"start\":48979},{\"end\":49623,\"start\":49596},{\"end\":50908,\"start\":50879},{\"end\":52084,\"start\":52059},{\"end\":52101,\"start\":52087},{\"end\":53025,\"start\":52975},{\"end\":55161,\"start\":55129},{\"end\":56629,\"start\":56575},{\"end\":57661,\"start\":57651},{\"end\":57848,\"start\":57838},{\"end\":58641,\"start\":58640},{\"end\":58929,\"start\":58919},{\"end\":59023,\"start\":59013},{\"end\":59604,\"start\":59594},{\"end\":59674,\"start\":59665},{\"end\":59757,\"start\":59748},{\"end\":60647,\"start\":60638},{\"end\":61242,\"start\":61233},{\"end\":61339,\"start\":61333},{\"end\":66262,\"start\":66253},{\"end\":66323,\"start\":66314},{\"end\":67009,\"start\":67000},{\"end\":67923,\"start\":67913},{\"end\":68185,\"start\":68175},{\"end\":68859,\"start\":68849},{\"end\":69279,\"start\":69269},{\"end\":69909,\"start\":69899},{\"end\":70008,\"start\":69998},{\"end\":70464,\"start\":70454},{\"end\":70531,\"start\":70523},{\"end\":71392,\"start\":71382},{\"end\":71826,\"start\":71816},{\"end\":72247,\"start\":72237},{\"end\":72445,\"start\":72435}]", "table": "[{\"end\":60636,\"start\":60157},{\"end\":61231,\"start\":60895},{\"end\":66251,\"start\":63291},{\"end\":66998,\"start\":66325},{\"end\":67911,\"start\":67152},{\"end\":68173,\"start\":67926},{\"end\":68847,\"start\":68331},{\"end\":69267,\"start\":69017},{\"end\":69897,\"start\":69524},{\"end\":70452,\"start\":70135},{\"end\":70521,\"start\":70495},{\"end\":71380,\"start\":70684},{\"end\":71814,\"start\":71459},{\"end\":72235,\"start\":72049},{\"end\":74317,\"start\":73598}]", "figure_caption": "[{\"end\":57836,\"start\":57663},{\"end\":58303,\"start\":57850},{\"end\":58638,\"start\":58306},{\"end\":58917,\"start\":58642},{\"end\":59011,\"start\":58931},{\"end\":59136,\"start\":59025},{\"end\":59592,\"start\":59139},{\"end\":59663,\"start\":59606},{\"end\":59746,\"start\":59676},{\"end\":59845,\"start\":59759},{\"end\":60157,\"start\":59848},{\"end\":60717,\"start\":60649},{\"end\":60895,\"start\":60720},{\"end\":61331,\"start\":61244},{\"end\":63291,\"start\":61342},{\"end\":66312,\"start\":66264},{\"end\":67152,\"start\":67011},{\"end\":68331,\"start\":68188},{\"end\":68906,\"start\":68862},{\"end\":69017,\"start\":68909},{\"end\":69452,\"start\":69282},{\"end\":69524,\"start\":69455},{\"end\":69996,\"start\":69912},{\"end\":70135,\"start\":70011},{\"end\":70495,\"start\":70467},{\"end\":70684,\"start\":70534},{\"end\":71429,\"start\":71395},{\"end\":71459,\"start\":71432},{\"end\":72005,\"start\":71829},{\"end\":72049,\"start\":72008},{\"end\":72433,\"start\":72250},{\"end\":72556,\"start\":72448},{\"end\":73598,\"start\":72559}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4272,\"start\":4263},{\"end\":14033,\"start\":14025},{\"end\":14803,\"start\":14795},{\"end\":15297,\"start\":15289},{\"end\":15929,\"start\":15921},{\"end\":17507,\"start\":17499},{\"end\":26858,\"start\":26852},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":39215,\"start\":39207},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40520,\"start\":40512},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41538,\"start\":41530},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":57236,\"start\":57228}]", "bib_author_first_name": "[{\"end\":75751,\"start\":75743},{\"end\":76049,\"start\":76042},{\"end\":76067,\"start\":76059},{\"end\":76069,\"start\":76068},{\"end\":76297,\"start\":76289},{\"end\":76299,\"start\":76298},{\"end\":76313,\"start\":76308},{\"end\":76332,\"start\":76324},{\"end\":76607,\"start\":76601},{\"end\":76619,\"start\":76615},{\"end\":76632,\"start\":76628},{\"end\":76646,\"start\":76641},{\"end\":76971,\"start\":76966},{\"end\":76988,\"start\":76982},{\"end\":77259,\"start\":77255},{\"end\":77274,\"start\":77270},{\"end\":77290,\"start\":77284},{\"end\":77308,\"start\":77300},{\"end\":77619,\"start\":77611},{\"end\":77630,\"start\":77626},{\"end\":77648,\"start\":77642},{\"end\":77932,\"start\":77928},{\"end\":77944,\"start\":77939},{\"end\":77964,\"start\":77956},{\"end\":77982,\"start\":77974},{\"end\":77984,\"start\":77983},{\"end\":78278,\"start\":78271},{\"end\":78288,\"start\":78283},{\"end\":78299,\"start\":78294},{\"end\":78311,\"start\":78304},{\"end\":78321,\"start\":78317},{\"end\":78323,\"start\":78322},{\"end\":78544,\"start\":78539},{\"end\":78564,\"start\":78559},{\"end\":78574,\"start\":78569},{\"end\":78831,\"start\":78823},{\"end\":78843,\"start\":78838},{\"end\":78861,\"start\":78854},{\"end\":79113,\"start\":79105},{\"end\":79124,\"start\":79120},{\"end\":79133,\"start\":79130},{\"end\":79146,\"start\":79141},{\"end\":79165,\"start\":79157},{\"end\":79181,\"start\":79174},{\"end\":79479,\"start\":79474},{\"end\":79494,\"start\":79489},{\"end\":79795,\"start\":79786},{\"end\":79808,\"start\":79803},{\"end\":79822,\"start\":79816},{\"end\":79835,\"start\":79831},{\"end\":79853,\"start\":79847},{\"end\":79875,\"start\":79866},{\"end\":79890,\"start\":79884},{\"end\":79906,\"start\":79899},{\"end\":79923,\"start\":79915},{\"end\":79935,\"start\":79930},{\"end\":80370,\"start\":80364},{\"end\":80388,\"start\":80384},{\"end\":80395,\"start\":80389},{\"end\":80416,\"start\":80410},{\"end\":80435,\"start\":80429},{\"end\":80767,\"start\":80761},{\"end\":80784,\"start\":80777},{\"end\":80797,\"start\":80792},{\"end\":81067,\"start\":81058},{\"end\":81087,\"start\":81080},{\"end\":81099,\"start\":81094},{\"end\":81399,\"start\":81393},{\"end\":81410,\"start\":81405},{\"end\":81445,\"start\":81433},{\"end\":81459,\"start\":81452},{\"end\":81471,\"start\":81467},{\"end\":81499,\"start\":81480},{\"end\":81910,\"start\":81903},{\"end\":81927,\"start\":81919},{\"end\":81929,\"start\":81928},{\"end\":82174,\"start\":82165},{\"end\":82186,\"start\":82181},{\"end\":82205,\"start\":82200},{\"end\":82220,\"start\":82214},{\"end\":82222,\"start\":82221},{\"end\":82233,\"start\":82229},{\"end\":82246,\"start\":82242},{\"end\":82248,\"start\":82247},{\"end\":82264,\"start\":82260},{\"end\":82279,\"start\":82273},{\"end\":82281,\"start\":82280},{\"end\":82301,\"start\":82294},{\"end\":82303,\"start\":82302},{\"end\":82320,\"start\":82315},{\"end\":82336,\"start\":82332},{\"end\":82937,\"start\":82928},{\"end\":82949,\"start\":82944},{\"end\":82962,\"start\":82950},{\"end\":82975,\"start\":82970},{\"end\":82987,\"start\":82983},{\"end\":83003,\"start\":82996},{\"end\":83018,\"start\":83015},{\"end\":83032,\"start\":83027},{\"end\":83046,\"start\":83041},{\"end\":83422,\"start\":83416},{\"end\":83437,\"start\":83431},{\"end\":83451,\"start\":83447},{\"end\":83468,\"start\":83465},{\"end\":83482,\"start\":83477},{\"end\":83498,\"start\":83494},{\"end\":83510,\"start\":83507},{\"end\":83524,\"start\":83519},{\"end\":83539,\"start\":83531},{\"end\":83550,\"start\":83540},{\"end\":83562,\"start\":83557},{\"end\":83947,\"start\":83942},{\"end\":83973,\"start\":83966},{\"end\":83987,\"start\":83981},{\"end\":84002,\"start\":83995},{\"end\":84017,\"start\":84011},{\"end\":84286,\"start\":84285},{\"end\":84304,\"start\":84296},{\"end\":84306,\"start\":84305},{\"end\":84327,\"start\":84318},{\"end\":84341,\"start\":84334},{\"end\":84354,\"start\":84351},{\"end\":84367,\"start\":84362},{\"end\":84379,\"start\":84374},{\"end\":84391,\"start\":84387},{\"end\":84719,\"start\":84714},{\"end\":84733,\"start\":84730},{\"end\":84748,\"start\":84743},{\"end\":84765,\"start\":84758},{\"end\":84771,\"start\":84766},{\"end\":84786,\"start\":84781},{\"end\":84800,\"start\":84794},{\"end\":84816,\"start\":84812},{\"end\":85107,\"start\":85104},{\"end\":85124,\"start\":85118},{\"end\":85136,\"start\":85130},{\"end\":85149,\"start\":85145},{\"end\":85284,\"start\":85278},{\"end\":85301,\"start\":85296},{\"end\":85545,\"start\":85539},{\"end\":85562,\"start\":85553},{\"end\":85871,\"start\":85866},{\"end\":85886,\"start\":85878},{\"end\":85888,\"start\":85887},{\"end\":86153,\"start\":86149},{\"end\":86163,\"start\":86159},{\"end\":86177,\"start\":86172},{\"end\":86412,\"start\":86408},{\"end\":86430,\"start\":86425},{\"end\":86650,\"start\":86645},{\"end\":86663,\"start\":86658},{\"end\":86676,\"start\":86672},{\"end\":86693,\"start\":86687},{\"end\":86711,\"start\":86705},{\"end\":86728,\"start\":86724},{\"end\":86743,\"start\":86737},{\"end\":87175,\"start\":87170},{\"end\":87187,\"start\":87182},{\"end\":87205,\"start\":87199},{\"end\":87218,\"start\":87214},{\"end\":87233,\"start\":87226},{\"end\":87555,\"start\":87550},{\"end\":87575,\"start\":87567},{\"end\":87590,\"start\":87584},{\"end\":87861,\"start\":87853},{\"end\":87877,\"start\":87868},{\"end\":87886,\"start\":87883},{\"end\":87896,\"start\":87891},{\"end\":88106,\"start\":88099},{\"end\":88117,\"start\":88113},{\"end\":88139,\"start\":88130},{\"end\":88141,\"start\":88140},{\"end\":88160,\"start\":88154},{\"end\":88176,\"start\":88170},{\"end\":88192,\"start\":88187},{\"end\":88205,\"start\":88200},{\"end\":88221,\"start\":88216},{\"end\":88241,\"start\":88236},{\"end\":88257,\"start\":88251},{\"end\":88276,\"start\":88271},{\"end\":88291,\"start\":88284},{\"end\":88307,\"start\":88300},{\"end\":88325,\"start\":88319},{\"end\":88344,\"start\":88337},{\"end\":89014,\"start\":89007},{\"end\":89027,\"start\":89021},{\"end\":89045,\"start\":89036},{\"end\":89063,\"start\":89058},{\"end\":89345,\"start\":89341},{\"end\":89587,\"start\":89578},{\"end\":89599,\"start\":89594},{\"end\":89612,\"start\":89606},{\"end\":89614,\"start\":89613},{\"end\":89628,\"start\":89624},{\"end\":89643,\"start\":89636},{\"end\":89944,\"start\":89940},{\"end\":89960,\"start\":89957},{\"end\":89972,\"start\":89971},{\"end\":89974,\"start\":89973},{\"end\":89992,\"start\":89988},{\"end\":90009,\"start\":90003},{\"end\":90367,\"start\":90361},{\"end\":90385,\"start\":90376},{\"end\":90688,\"start\":90681},{\"end\":90707,\"start\":90703},{\"end\":90716,\"start\":90712},{\"end\":91024,\"start\":91023},{\"end\":91033,\"start\":91032},{\"end\":91046,\"start\":91045},{\"end\":91061,\"start\":91060},{\"end\":91072,\"start\":91071},{\"end\":91365,\"start\":91359},{\"end\":91380,\"start\":91375},{\"end\":91396,\"start\":91390},{\"end\":91700,\"start\":91694},{\"end\":91723,\"start\":91715},{\"end\":91736,\"start\":91730},{\"end\":92026,\"start\":92017},{\"end\":92043,\"start\":92036},{\"end\":92061,\"start\":92055},{\"end\":92072,\"start\":92069},{\"end\":92089,\"start\":92081},{\"end\":92418,\"start\":92413},{\"end\":92436,\"start\":92428},{\"end\":92452,\"start\":92449},{\"end\":92744,\"start\":92738},{\"end\":92759,\"start\":92751},{\"end\":92774,\"start\":92765},{\"end\":92788,\"start\":92780},{\"end\":92790,\"start\":92789},{\"end\":92807,\"start\":92802},{\"end\":92809,\"start\":92808},{\"end\":92823,\"start\":92818},{\"end\":92825,\"start\":92824},{\"end\":93169,\"start\":93161},{\"end\":93185,\"start\":93178},{\"end\":93196,\"start\":93193},{\"end\":93205,\"start\":93203},{\"end\":93529,\"start\":93520},{\"end\":93539,\"start\":93536},{\"end\":93551,\"start\":93547},{\"end\":93568,\"start\":93561},{\"end\":93570,\"start\":93569},{\"end\":93587,\"start\":93581},{\"end\":93811,\"start\":93810},{\"end\":93813,\"start\":93812},{\"end\":93823,\"start\":93822},{\"end\":93834,\"start\":93833},{\"end\":93847,\"start\":93846},{\"end\":93849,\"start\":93848},{\"end\":94205,\"start\":94203},{\"end\":94218,\"start\":94215},{\"end\":94233,\"start\":94227},{\"end\":94545,\"start\":94534},{\"end\":94562,\"start\":94556},{\"end\":94912,\"start\":94908},{\"end\":94930,\"start\":94922},{\"end\":94941,\"start\":94936},{\"end\":94955,\"start\":94951},{\"end\":94969,\"start\":94965},{\"end\":94981,\"start\":94977},{\"end\":94995,\"start\":94989},{\"end\":95335,\"start\":95334},{\"end\":95556,\"start\":95549},{\"end\":95569,\"start\":95565},{\"end\":95573,\"start\":95570},{\"end\":95586,\"start\":95580},{\"end\":95861,\"start\":95855},{\"end\":95874,\"start\":95870},{\"end\":95890,\"start\":95885},{\"end\":95905,\"start\":95899},{\"end\":95922,\"start\":95915},{\"end\":96207,\"start\":96202},{\"end\":96219,\"start\":96212},{\"end\":96627,\"start\":96626}]", "bib_author_last_name": "[{\"end\":75761,\"start\":75752},{\"end\":76057,\"start\":76050},{\"end\":76079,\"start\":76070},{\"end\":76306,\"start\":76300},{\"end\":76322,\"start\":76314},{\"end\":76336,\"start\":76333},{\"end\":76613,\"start\":76608},{\"end\":76626,\"start\":76620},{\"end\":76639,\"start\":76633},{\"end\":76652,\"start\":76647},{\"end\":76980,\"start\":76972},{\"end\":76998,\"start\":76989},{\"end\":77268,\"start\":77260},{\"end\":77282,\"start\":77275},{\"end\":77298,\"start\":77291},{\"end\":77314,\"start\":77309},{\"end\":77624,\"start\":77620},{\"end\":77640,\"start\":77631},{\"end\":77656,\"start\":77649},{\"end\":77937,\"start\":77933},{\"end\":77954,\"start\":77945},{\"end\":77972,\"start\":77965},{\"end\":77991,\"start\":77985},{\"end\":78281,\"start\":78279},{\"end\":78292,\"start\":78289},{\"end\":78302,\"start\":78300},{\"end\":78315,\"start\":78312},{\"end\":78332,\"start\":78324},{\"end\":78557,\"start\":78545},{\"end\":78567,\"start\":78565},{\"end\":78582,\"start\":78575},{\"end\":78836,\"start\":78832},{\"end\":78852,\"start\":78844},{\"end\":78867,\"start\":78862},{\"end\":79118,\"start\":79114},{\"end\":79128,\"start\":79125},{\"end\":79139,\"start\":79134},{\"end\":79155,\"start\":79147},{\"end\":79172,\"start\":79166},{\"end\":79187,\"start\":79182},{\"end\":79487,\"start\":79480},{\"end\":79501,\"start\":79495},{\"end\":79801,\"start\":79796},{\"end\":79814,\"start\":79809},{\"end\":79829,\"start\":79823},{\"end\":79845,\"start\":79836},{\"end\":79864,\"start\":79854},{\"end\":79882,\"start\":79876},{\"end\":79897,\"start\":79891},{\"end\":79913,\"start\":79907},{\"end\":79928,\"start\":79924},{\"end\":79940,\"start\":79936},{\"end\":80382,\"start\":80371},{\"end\":80408,\"start\":80396},{\"end\":80427,\"start\":80417},{\"end\":80440,\"start\":80436},{\"end\":80775,\"start\":80768},{\"end\":80790,\"start\":80785},{\"end\":80807,\"start\":80798},{\"end\":81078,\"start\":81068},{\"end\":81092,\"start\":81088},{\"end\":81105,\"start\":81100},{\"end\":81403,\"start\":81400},{\"end\":81431,\"start\":81411},{\"end\":81450,\"start\":81446},{\"end\":81465,\"start\":81460},{\"end\":81478,\"start\":81472},{\"end\":81505,\"start\":81500},{\"end\":81511,\"start\":81507},{\"end\":81917,\"start\":81911},{\"end\":81936,\"start\":81930},{\"end\":82179,\"start\":82175},{\"end\":82198,\"start\":82187},{\"end\":82212,\"start\":82206},{\"end\":82227,\"start\":82223},{\"end\":82240,\"start\":82234},{\"end\":82258,\"start\":82249},{\"end\":82271,\"start\":82265},{\"end\":82292,\"start\":82282},{\"end\":82313,\"start\":82304},{\"end\":82330,\"start\":82321},{\"end\":82345,\"start\":82337},{\"end\":82942,\"start\":82938},{\"end\":82968,\"start\":82963},{\"end\":82981,\"start\":82976},{\"end\":82994,\"start\":82988},{\"end\":83013,\"start\":83004},{\"end\":83025,\"start\":83019},{\"end\":83039,\"start\":83033},{\"end\":83058,\"start\":83047},{\"end\":83429,\"start\":83423},{\"end\":83445,\"start\":83438},{\"end\":83463,\"start\":83452},{\"end\":83475,\"start\":83469},{\"end\":83492,\"start\":83483},{\"end\":83505,\"start\":83499},{\"end\":83517,\"start\":83511},{\"end\":83529,\"start\":83525},{\"end\":83555,\"start\":83551},{\"end\":83569,\"start\":83563},{\"end\":83964,\"start\":83948},{\"end\":83979,\"start\":83974},{\"end\":83993,\"start\":83988},{\"end\":84009,\"start\":84003},{\"end\":84026,\"start\":84018},{\"end\":84035,\"start\":84028},{\"end\":84294,\"start\":84287},{\"end\":84316,\"start\":84307},{\"end\":84332,\"start\":84328},{\"end\":84349,\"start\":84342},{\"end\":84360,\"start\":84355},{\"end\":84372,\"start\":84368},{\"end\":84385,\"start\":84380},{\"end\":84398,\"start\":84392},{\"end\":84408,\"start\":84400},{\"end\":84728,\"start\":84720},{\"end\":84741,\"start\":84734},{\"end\":84756,\"start\":84749},{\"end\":84779,\"start\":84772},{\"end\":84792,\"start\":84787},{\"end\":84810,\"start\":84801},{\"end\":84832,\"start\":84817},{\"end\":85116,\"start\":85108},{\"end\":85128,\"start\":85125},{\"end\":85143,\"start\":85137},{\"end\":85160,\"start\":85150},{\"end\":85294,\"start\":85285},{\"end\":85311,\"start\":85302},{\"end\":85551,\"start\":85546},{\"end\":85570,\"start\":85563},{\"end\":85876,\"start\":85872},{\"end\":85895,\"start\":85889},{\"end\":86157,\"start\":86154},{\"end\":86170,\"start\":86164},{\"end\":86186,\"start\":86178},{\"end\":86423,\"start\":86413},{\"end\":86437,\"start\":86431},{\"end\":86656,\"start\":86651},{\"end\":86670,\"start\":86664},{\"end\":86685,\"start\":86677},{\"end\":86703,\"start\":86694},{\"end\":86722,\"start\":86712},{\"end\":86735,\"start\":86729},{\"end\":86751,\"start\":86744},{\"end\":87180,\"start\":87176},{\"end\":87197,\"start\":87188},{\"end\":87212,\"start\":87206},{\"end\":87224,\"start\":87219},{\"end\":87242,\"start\":87234},{\"end\":87565,\"start\":87556},{\"end\":87582,\"start\":87576},{\"end\":87593,\"start\":87591},{\"end\":87866,\"start\":87862},{\"end\":87881,\"start\":87878},{\"end\":87889,\"start\":87887},{\"end\":87900,\"start\":87897},{\"end\":88111,\"start\":88107},{\"end\":88128,\"start\":88118},{\"end\":88152,\"start\":88142},{\"end\":88168,\"start\":88161},{\"end\":88185,\"start\":88177},{\"end\":88198,\"start\":88193},{\"end\":88214,\"start\":88206},{\"end\":88234,\"start\":88222},{\"end\":88249,\"start\":88242},{\"end\":88269,\"start\":88258},{\"end\":88282,\"start\":88277},{\"end\":88298,\"start\":88292},{\"end\":88317,\"start\":88308},{\"end\":88335,\"start\":88326},{\"end\":88353,\"start\":88345},{\"end\":89019,\"start\":89015},{\"end\":89034,\"start\":89028},{\"end\":89056,\"start\":89046},{\"end\":89069,\"start\":89064},{\"end\":89356,\"start\":89346},{\"end\":89592,\"start\":89588},{\"end\":89604,\"start\":89600},{\"end\":89622,\"start\":89615},{\"end\":89634,\"start\":89629},{\"end\":89652,\"start\":89644},{\"end\":89955,\"start\":89945},{\"end\":89969,\"start\":89961},{\"end\":89986,\"start\":89975},{\"end\":90001,\"start\":89993},{\"end\":90014,\"start\":90010},{\"end\":90025,\"start\":90016},{\"end\":90374,\"start\":90368},{\"end\":90390,\"start\":90386},{\"end\":90701,\"start\":90689},{\"end\":90710,\"start\":90708},{\"end\":90725,\"start\":90717},{\"end\":90730,\"start\":90727},{\"end\":91030,\"start\":91025},{\"end\":91043,\"start\":91034},{\"end\":91058,\"start\":91047},{\"end\":91069,\"start\":91062},{\"end\":91078,\"start\":91073},{\"end\":91373,\"start\":91366},{\"end\":91388,\"start\":91381},{\"end\":91403,\"start\":91397},{\"end\":91713,\"start\":91701},{\"end\":91728,\"start\":91724},{\"end\":91743,\"start\":91737},{\"end\":91747,\"start\":91745},{\"end\":92034,\"start\":92027},{\"end\":92053,\"start\":92044},{\"end\":92067,\"start\":92062},{\"end\":92079,\"start\":92073},{\"end\":92095,\"start\":92090},{\"end\":92426,\"start\":92419},{\"end\":92447,\"start\":92437},{\"end\":92461,\"start\":92453},{\"end\":92749,\"start\":92745},{\"end\":92763,\"start\":92760},{\"end\":92778,\"start\":92775},{\"end\":92800,\"start\":92791},{\"end\":92816,\"start\":92810},{\"end\":92835,\"start\":92826},{\"end\":93176,\"start\":93170},{\"end\":93191,\"start\":93186},{\"end\":93201,\"start\":93197},{\"end\":93213,\"start\":93206},{\"end\":93534,\"start\":93530},{\"end\":93545,\"start\":93540},{\"end\":93559,\"start\":93552},{\"end\":93579,\"start\":93571},{\"end\":93595,\"start\":93588},{\"end\":93820,\"start\":93814},{\"end\":93831,\"start\":93824},{\"end\":93844,\"start\":93835},{\"end\":93857,\"start\":93850},{\"end\":94213,\"start\":94206},{\"end\":94225,\"start\":94219},{\"end\":94240,\"start\":94234},{\"end\":94554,\"start\":94546},{\"end\":94572,\"start\":94563},{\"end\":94920,\"start\":94913},{\"end\":94934,\"start\":94931},{\"end\":94949,\"start\":94942},{\"end\":94963,\"start\":94956},{\"end\":94975,\"start\":94970},{\"end\":94987,\"start\":94982},{\"end\":95003,\"start\":94996},{\"end\":95339,\"start\":95336},{\"end\":95345,\"start\":95341},{\"end\":95563,\"start\":95557},{\"end\":95578,\"start\":95574},{\"end\":95593,\"start\":95587},{\"end\":95868,\"start\":95862},{\"end\":95883,\"start\":95875},{\"end\":95897,\"start\":95891},{\"end\":95913,\"start\":95906},{\"end\":95932,\"start\":95923},{\"end\":96210,\"start\":96208},{\"end\":96222,\"start\":96220}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206775608},\"end\":75979,\"start\":75618},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12366835},\"end\":76239,\"start\":75981},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2309950},\"end\":76504,\"start\":76241},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206592191},\"end\":76896,\"start\":76506},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b4\"},\"end\":77171,\"start\":76898},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":215827080},\"end\":77553,\"start\":77173},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1629541},\"end\":77855,\"start\":77555},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b7\"},\"end\":78202,\"start\":77857},{\"attributes\":{\"doi\":\"arXiv:1911.05722\",\"id\":\"b8\"},\"end\":78537,\"start\":78204},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b9\"},\"end\":78821,\"start\":78539},{\"attributes\":{\"doi\":\"arXiv:1906.05849v4\",\"id\":\"b10\"},\"end\":79051,\"start\":78823},{\"attributes\":{\"doi\":\"arXiv:2005.10243\",\"id\":\"b11\"},\"end\":79397,\"start\":79053},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":187547},\"end\":79698,\"start\":79399},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":211227},\"end\":80281,\"start\":79700},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3244218},\"end\":80691,\"start\":80283},{\"attributes\":{\"doi\":\"arXiv:1803.07728\",\"id\":\"b15\"},\"end\":80997,\"start\":80693},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":59292019},\"end\":81309,\"start\":80999},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":216867201},\"end\":81819,\"start\":81311},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4332326},\"end\":82106,\"start\":81821},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":205242740},\"end\":82872,\"start\":82108},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6875312},\"end\":83350,\"start\":82874},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":19135734},\"end\":83890,\"start\":83352},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":54446702},\"end\":84283,\"start\":83892},{\"attributes\":{\"doi\":\"arXiv:1509.02971\",\"id\":\"b23\"},\"end\":84712,\"start\":84285},{\"attributes\":{\"id\":\"b24\"},\"end\":85079,\"start\":84714},{\"attributes\":{\"id\":\"b25\"},\"end\":85252,\"start\":85081},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b26\"},\"end\":85443,\"start\":85254},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5808102},\"end\":85802,\"start\":85445},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":15539264},\"end\":86094,\"start\":85804},{\"attributes\":{\"doi\":\"arXiv:1708.03888\",\"id\":\"b29\"},\"end\":86352,\"start\":86096},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14337532},\"end\":86643,\"start\":86354},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b31\"},\"end\":87109,\"start\":86645},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2608922},\"end\":87504,\"start\":87111},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":43928547},\"end\":87777,\"start\":87506},{\"attributes\":{\"doi\":\"arXiv:2002.09024\",\"id\":\"b34\"},\"end\":88097,\"start\":87779},{\"attributes\":{\"id\":\"b35\"},\"end\":88958,\"start\":88099},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":167209887},\"end\":89284,\"start\":88960},{\"attributes\":{\"id\":\"b37\"},\"end\":89513,\"start\":89286},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1309931},\"end\":89888,\"start\":89515},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":4246903},\"end\":90273,\"start\":89890},{\"attributes\":{\"id\":\"b40\"},\"end\":90599,\"start\":90275},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":10328909},\"end\":90953,\"start\":90601},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":11091110},\"end\":91300,\"start\":90955},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":14650762},\"end\":91612,\"start\":91302},{\"attributes\":{\"doi\":\"arXiv:1909.13719\",\"id\":\"b44\"},\"end\":91956,\"start\":91614},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":206593880},\"end\":92347,\"start\":91958},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":12726540},\"end\":92669,\"start\":92349},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":10860374},\"end\":93100,\"start\":92671},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14342571},\"end\":93470,\"start\":93102},{\"attributes\":{\"doi\":\"arXiv:1306.5151\",\"id\":\"b49\"},\"end\":93793,\"start\":93472},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":383200},\"end\":94023,\"start\":93795},{\"attributes\":{\"id\":\"b51\"},\"end\":94468,\"start\":94025},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":15193013},\"end\":94827,\"start\":94470},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":6161478},\"end\":95285,\"start\":94829},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":1617819},\"end\":95481,\"start\":95287},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":88523028},\"end\":95809,\"start\":95483},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":10054272},\"end\":96179,\"start\":95811},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":4076251},\"end\":96358,\"start\":96181},{\"attributes\":{\"id\":\"b58\"},\"end\":96579,\"start\":96360},{\"attributes\":{\"id\":\"b59\"},\"end\":96698,\"start\":96581},{\"attributes\":{\"id\":\"b60\"},\"end\":96953,\"start\":96700}]", "bib_title": "[{\"end\":75741,\"start\":75618},{\"end\":76040,\"start\":75981},{\"end\":76287,\"start\":76241},{\"end\":76599,\"start\":76506},{\"end\":77253,\"start\":77173},{\"end\":77609,\"start\":77555},{\"end\":79472,\"start\":79399},{\"end\":79784,\"start\":79700},{\"end\":80362,\"start\":80283},{\"end\":81056,\"start\":80999},{\"end\":81391,\"start\":81311},{\"end\":81901,\"start\":81821},{\"end\":82163,\"start\":82108},{\"end\":82926,\"start\":82874},{\"end\":83414,\"start\":83352},{\"end\":83940,\"start\":83892},{\"end\":85537,\"start\":85445},{\"end\":85864,\"start\":85804},{\"end\":86406,\"start\":86354},{\"end\":87168,\"start\":87111},{\"end\":87548,\"start\":87506},{\"end\":89005,\"start\":88960},{\"end\":89576,\"start\":89515},{\"end\":89938,\"start\":89890},{\"end\":90359,\"start\":90275},{\"end\":90679,\"start\":90601},{\"end\":91021,\"start\":90955},{\"end\":91357,\"start\":91302},{\"end\":92015,\"start\":91958},{\"end\":92411,\"start\":92349},{\"end\":92736,\"start\":92671},{\"end\":93159,\"start\":93102},{\"end\":93808,\"start\":93795},{\"end\":94532,\"start\":94470},{\"end\":94906,\"start\":94829},{\"end\":95332,\"start\":95287},{\"end\":95547,\"start\":95483},{\"end\":95853,\"start\":95811},{\"end\":96200,\"start\":96181}]", "bib_author": "[{\"end\":75763,\"start\":75743},{\"end\":76059,\"start\":76042},{\"end\":76081,\"start\":76059},{\"end\":76308,\"start\":76289},{\"end\":76324,\"start\":76308},{\"end\":76338,\"start\":76324},{\"end\":76615,\"start\":76601},{\"end\":76628,\"start\":76615},{\"end\":76641,\"start\":76628},{\"end\":76654,\"start\":76641},{\"end\":76982,\"start\":76966},{\"end\":77000,\"start\":76982},{\"end\":77270,\"start\":77255},{\"end\":77284,\"start\":77270},{\"end\":77300,\"start\":77284},{\"end\":77316,\"start\":77300},{\"end\":77626,\"start\":77611},{\"end\":77642,\"start\":77626},{\"end\":77658,\"start\":77642},{\"end\":77939,\"start\":77928},{\"end\":77956,\"start\":77939},{\"end\":77974,\"start\":77956},{\"end\":77993,\"start\":77974},{\"end\":78283,\"start\":78271},{\"end\":78294,\"start\":78283},{\"end\":78304,\"start\":78294},{\"end\":78317,\"start\":78304},{\"end\":78334,\"start\":78317},{\"end\":78559,\"start\":78539},{\"end\":78569,\"start\":78559},{\"end\":78584,\"start\":78569},{\"end\":78838,\"start\":78823},{\"end\":78854,\"start\":78838},{\"end\":78869,\"start\":78854},{\"end\":79120,\"start\":79105},{\"end\":79130,\"start\":79120},{\"end\":79141,\"start\":79130},{\"end\":79157,\"start\":79141},{\"end\":79174,\"start\":79157},{\"end\":79189,\"start\":79174},{\"end\":79489,\"start\":79474},{\"end\":79503,\"start\":79489},{\"end\":79803,\"start\":79786},{\"end\":79816,\"start\":79803},{\"end\":79831,\"start\":79816},{\"end\":79847,\"start\":79831},{\"end\":79866,\"start\":79847},{\"end\":79884,\"start\":79866},{\"end\":79899,\"start\":79884},{\"end\":79915,\"start\":79899},{\"end\":79930,\"start\":79915},{\"end\":79942,\"start\":79930},{\"end\":80384,\"start\":80364},{\"end\":80410,\"start\":80384},{\"end\":80429,\"start\":80410},{\"end\":80442,\"start\":80429},{\"end\":80777,\"start\":80761},{\"end\":80792,\"start\":80777},{\"end\":80809,\"start\":80792},{\"end\":81080,\"start\":81058},{\"end\":81094,\"start\":81080},{\"end\":81107,\"start\":81094},{\"end\":81405,\"start\":81393},{\"end\":81433,\"start\":81405},{\"end\":81452,\"start\":81433},{\"end\":81467,\"start\":81452},{\"end\":81480,\"start\":81467},{\"end\":81507,\"start\":81480},{\"end\":81513,\"start\":81507},{\"end\":81919,\"start\":81903},{\"end\":81938,\"start\":81919},{\"end\":82181,\"start\":82165},{\"end\":82200,\"start\":82181},{\"end\":82214,\"start\":82200},{\"end\":82229,\"start\":82214},{\"end\":82242,\"start\":82229},{\"end\":82260,\"start\":82242},{\"end\":82273,\"start\":82260},{\"end\":82294,\"start\":82273},{\"end\":82315,\"start\":82294},{\"end\":82332,\"start\":82315},{\"end\":82347,\"start\":82332},{\"end\":82944,\"start\":82928},{\"end\":82970,\"start\":82944},{\"end\":82983,\"start\":82970},{\"end\":82996,\"start\":82983},{\"end\":83015,\"start\":82996},{\"end\":83027,\"start\":83015},{\"end\":83041,\"start\":83027},{\"end\":83060,\"start\":83041},{\"end\":83431,\"start\":83416},{\"end\":83447,\"start\":83431},{\"end\":83465,\"start\":83447},{\"end\":83477,\"start\":83465},{\"end\":83494,\"start\":83477},{\"end\":83507,\"start\":83494},{\"end\":83519,\"start\":83507},{\"end\":83531,\"start\":83519},{\"end\":83557,\"start\":83531},{\"end\":83571,\"start\":83557},{\"end\":83966,\"start\":83942},{\"end\":83981,\"start\":83966},{\"end\":83995,\"start\":83981},{\"end\":84011,\"start\":83995},{\"end\":84028,\"start\":84011},{\"end\":84037,\"start\":84028},{\"end\":84296,\"start\":84285},{\"end\":84318,\"start\":84296},{\"end\":84334,\"start\":84318},{\"end\":84351,\"start\":84334},{\"end\":84362,\"start\":84351},{\"end\":84374,\"start\":84362},{\"end\":84387,\"start\":84374},{\"end\":84400,\"start\":84387},{\"end\":84410,\"start\":84400},{\"end\":84730,\"start\":84714},{\"end\":84743,\"start\":84730},{\"end\":84758,\"start\":84743},{\"end\":84781,\"start\":84758},{\"end\":84794,\"start\":84781},{\"end\":84812,\"start\":84794},{\"end\":84834,\"start\":84812},{\"end\":85118,\"start\":85104},{\"end\":85130,\"start\":85118},{\"end\":85145,\"start\":85130},{\"end\":85162,\"start\":85145},{\"end\":85296,\"start\":85278},{\"end\":85313,\"start\":85296},{\"end\":85553,\"start\":85539},{\"end\":85572,\"start\":85553},{\"end\":85878,\"start\":85866},{\"end\":85897,\"start\":85878},{\"end\":86159,\"start\":86149},{\"end\":86172,\"start\":86159},{\"end\":86188,\"start\":86172},{\"end\":86425,\"start\":86408},{\"end\":86439,\"start\":86425},{\"end\":86658,\"start\":86645},{\"end\":86672,\"start\":86658},{\"end\":86687,\"start\":86672},{\"end\":86705,\"start\":86687},{\"end\":86724,\"start\":86705},{\"end\":86737,\"start\":86724},{\"end\":86753,\"start\":86737},{\"end\":87182,\"start\":87170},{\"end\":87199,\"start\":87182},{\"end\":87214,\"start\":87199},{\"end\":87226,\"start\":87214},{\"end\":87244,\"start\":87226},{\"end\":87567,\"start\":87550},{\"end\":87584,\"start\":87567},{\"end\":87595,\"start\":87584},{\"end\":87868,\"start\":87853},{\"end\":87883,\"start\":87868},{\"end\":87891,\"start\":87883},{\"end\":87902,\"start\":87891},{\"end\":88113,\"start\":88099},{\"end\":88130,\"start\":88113},{\"end\":88154,\"start\":88130},{\"end\":88170,\"start\":88154},{\"end\":88187,\"start\":88170},{\"end\":88200,\"start\":88187},{\"end\":88216,\"start\":88200},{\"end\":88236,\"start\":88216},{\"end\":88251,\"start\":88236},{\"end\":88271,\"start\":88251},{\"end\":88284,\"start\":88271},{\"end\":88300,\"start\":88284},{\"end\":88319,\"start\":88300},{\"end\":88337,\"start\":88319},{\"end\":88355,\"start\":88337},{\"end\":89021,\"start\":89007},{\"end\":89036,\"start\":89021},{\"end\":89058,\"start\":89036},{\"end\":89071,\"start\":89058},{\"end\":89358,\"start\":89341},{\"end\":89594,\"start\":89578},{\"end\":89606,\"start\":89594},{\"end\":89624,\"start\":89606},{\"end\":89636,\"start\":89624},{\"end\":89654,\"start\":89636},{\"end\":89957,\"start\":89940},{\"end\":89971,\"start\":89957},{\"end\":89988,\"start\":89971},{\"end\":90003,\"start\":89988},{\"end\":90016,\"start\":90003},{\"end\":90027,\"start\":90016},{\"end\":90376,\"start\":90361},{\"end\":90392,\"start\":90376},{\"end\":90703,\"start\":90681},{\"end\":90712,\"start\":90703},{\"end\":90727,\"start\":90712},{\"end\":90732,\"start\":90727},{\"end\":91032,\"start\":91023},{\"end\":91045,\"start\":91032},{\"end\":91060,\"start\":91045},{\"end\":91071,\"start\":91060},{\"end\":91080,\"start\":91071},{\"end\":91375,\"start\":91359},{\"end\":91390,\"start\":91375},{\"end\":91405,\"start\":91390},{\"end\":91715,\"start\":91694},{\"end\":91730,\"start\":91715},{\"end\":91745,\"start\":91730},{\"end\":91749,\"start\":91745},{\"end\":92036,\"start\":92017},{\"end\":92055,\"start\":92036},{\"end\":92069,\"start\":92055},{\"end\":92081,\"start\":92069},{\"end\":92097,\"start\":92081},{\"end\":92428,\"start\":92413},{\"end\":92449,\"start\":92428},{\"end\":92463,\"start\":92449},{\"end\":92751,\"start\":92738},{\"end\":92765,\"start\":92751},{\"end\":92780,\"start\":92765},{\"end\":92802,\"start\":92780},{\"end\":92818,\"start\":92802},{\"end\":92837,\"start\":92818},{\"end\":93178,\"start\":93161},{\"end\":93193,\"start\":93178},{\"end\":93203,\"start\":93193},{\"end\":93215,\"start\":93203},{\"end\":93536,\"start\":93520},{\"end\":93547,\"start\":93536},{\"end\":93561,\"start\":93547},{\"end\":93581,\"start\":93561},{\"end\":93597,\"start\":93581},{\"end\":93822,\"start\":93810},{\"end\":93833,\"start\":93822},{\"end\":93846,\"start\":93833},{\"end\":93859,\"start\":93846},{\"end\":94215,\"start\":94203},{\"end\":94227,\"start\":94215},{\"end\":94242,\"start\":94227},{\"end\":94556,\"start\":94534},{\"end\":94574,\"start\":94556},{\"end\":94922,\"start\":94908},{\"end\":94936,\"start\":94922},{\"end\":94951,\"start\":94936},{\"end\":94965,\"start\":94951},{\"end\":94977,\"start\":94965},{\"end\":94989,\"start\":94977},{\"end\":95005,\"start\":94989},{\"end\":95341,\"start\":95334},{\"end\":95347,\"start\":95341},{\"end\":95565,\"start\":95549},{\"end\":95580,\"start\":95565},{\"end\":95595,\"start\":95580},{\"end\":95870,\"start\":95855},{\"end\":95885,\"start\":95870},{\"end\":95899,\"start\":95885},{\"end\":95915,\"start\":95899},{\"end\":95934,\"start\":95915},{\"end\":96212,\"start\":96202},{\"end\":96224,\"start\":96212},{\"end\":96630,\"start\":96626}]", "bib_venue": "[{\"end\":82473,\"start\":82373},{\"end\":93279,\"start\":93262},{\"end\":75785,\"start\":75763},{\"end\":76099,\"start\":76081},{\"end\":76356,\"start\":76338},{\"end\":76693,\"start\":76654},{\"end\":76964,\"start\":76898},{\"end\":77355,\"start\":77316},{\"end\":77697,\"start\":77658},{\"end\":77926,\"start\":77857},{\"end\":78269,\"start\":78204},{\"end\":78658,\"start\":78600},{\"end\":78915,\"start\":78887},{\"end\":79103,\"start\":79053},{\"end\":79541,\"start\":79503},{\"end\":79981,\"start\":79942},{\"end\":80479,\"start\":80442},{\"end\":80759,\"start\":80693},{\"end\":81146,\"start\":81107},{\"end\":81557,\"start\":81513},{\"end\":81944,\"start\":81938},{\"end\":82371,\"start\":82347},{\"end\":83104,\"start\":83060},{\"end\":83613,\"start\":83571},{\"end\":84081,\"start\":84037},{\"end\":84477,\"start\":84426},{\"end\":84890,\"start\":84834},{\"end\":85102,\"start\":85081},{\"end\":85276,\"start\":85254},{\"end\":85616,\"start\":85572},{\"end\":85941,\"start\":85897},{\"end\":86147,\"start\":86096},{\"end\":86491,\"start\":86439},{\"end\":86857,\"start\":86769},{\"end\":87301,\"start\":87244},{\"end\":87634,\"start\":87595},{\"end\":87851,\"start\":87779},{\"end\":88524,\"start\":88355},{\"end\":89114,\"start\":89071},{\"end\":89339,\"start\":89286},{\"end\":89693,\"start\":89654},{\"end\":90067,\"start\":90027},{\"end\":90431,\"start\":90392},{\"end\":90769,\"start\":90732},{\"end\":91117,\"start\":91080},{\"end\":91449,\"start\":91405},{\"end\":91692,\"start\":91614},{\"end\":92136,\"start\":92097},{\"end\":92501,\"start\":92463},{\"end\":92876,\"start\":92837},{\"end\":93260,\"start\":93215},{\"end\":93518,\"start\":93472},{\"end\":93898,\"start\":93859},{\"end\":94201,\"start\":94025},{\"end\":94641,\"start\":94574},{\"end\":95049,\"start\":95005},{\"end\":95371,\"start\":95347},{\"end\":95638,\"start\":95595},{\"end\":95987,\"start\":95934},{\"end\":96262,\"start\":96224},{\"end\":96466,\"start\":96360},{\"end\":96624,\"start\":96581},{\"end\":96825,\"start\":96700}]"}}}, "year": 2023, "month": 12, "day": 17}