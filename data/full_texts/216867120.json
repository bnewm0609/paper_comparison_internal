{"id": 216867120, "updated": "2023-09-27 21:12:49.036", "metadata": {"title": "The Effect of Natural Distribution Shift on Question Answering Models", "authors": "[{\"first\":\"John\",\"last\":\"Miller\",\"middle\":[]},{\"first\":\"Karl\",\"last\":\"Krauth\",\"middle\":[]},{\"first\":\"Benjamin\",\"last\":\"Recht\",\"middle\":[]},{\"first\":\"Ludwig\",\"last\":\"Schmidt\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We build four new test sets for the Stanford Question Answering Dataset (SQuAD) and evaluate the ability of question-answering systems to generalize to new data. Our first test set is from the original Wikipedia domain and measures the extent to which existing systems overfit the original test set. Despite several years of heavy test set re-use, we find no evidence of adaptive overfitting. The remaining three test sets are constructed from New York Times articles, Reddit posts, and Amazon product reviews and measure robustness to natural distribution shifts. Across a broad range of models, we observe average performance drops of 3.8, 14.0, and 17.4 F1 points, respectively. In contrast, a strong human baseline matches or exceeds the performance of SQuAD models on the original domain and exhibits little to no drop in new domains. Taken together, our results confirm the surprising resilience of the holdout method and emphasize the need to move towards evaluation metrics that incorporate robustness to natural distribution shifts.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2004.14444", "mag": "3034834768", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/MillerKRS20", "doi": null}}, "content": {"source": {"pdf_hash": "2fe6759b0e9757df70ca3db1e1dd9bd1c5a5bda5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2004.14444v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2e02fc1ba1880da0cef587ae24c1e3af11649828", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2fe6759b0e9757df70ca3db1e1dd9bd1c5a5bda5.txt", "contents": "\nThe Effect of Natural Distribution Shift on Question Answering Models\n\n\nJohn Miller \nUC Berkeley\nBerkeley, Berkeley\n\nU C Berkeley \nUC Berkeley\nBerkeley, Berkeley\n\nKarl Krauth \nUC Berkeley\nBerkeley, Berkeley\n\nBenjamin Recht \nUC Berkeley\nBerkeley, Berkeley\n\nLudwig Schmidt \nUC Berkeley\nBerkeley, Berkeley\n\nThe Effect of Natural Distribution Shift on Question Answering Models\n\nWe build four new test sets for the Stanford Question Answering Dataset (SQuAD) and evaluate the ability of question-answering systems to generalize to new data. Our first test set is from the original Wikipedia domain and measures the extent to which existing systems overfit the original test set. Despite several years of heavy test set re-use, we find no evidence of adaptive overfitting. The remaining three test sets are constructed from New York Times articles, Reddit posts, and Amazon product reviews and measure robustness to natural distribution shifts. Across a broad range of models, we observe average performance drops of 3.8, 14.0, and 17.4 F1 points, respectively. In contrast, a strong human baseline matches or exceeds the performance of SQuAD models on the original domain and exhibits little to no drop in new domains. Taken together, our results confirm the surprising resilience of the holdout method and emphasize the need to move towards evaluation metrics that incorporate robustness to natural distribution shifts.\n\nIntroduction\n\nSince its release in 2016, the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) has generated intense interest from the natural language processing community. At first glance, this intense interest has lead to impressive results. The best performing models in 2020  have F1 scores more than 40 points higher than the baseline presented by Rajpurkar et al. (2016). At the same time, it remains unclear to what extent progress on these benchmark numbers is a reliable indicator of progress more broadly.\n\nThe goal of building question answering systems is not merely to obtain high scores on the SQuAD leaderboard, but rather to generalize to new examples beyond the SQuAD test set. However, the competition format of SQuAD puts pressure on the validity of leaderboard scores. It is well-known that repeatedly evaluating models on a held-out test set can give overly optimistic estimates of model performance, a phenomenon known as adaptive overfitting Dwork et al. (2015). Moreover, the standard SQuAD evaluation only measures model performance on new examples from a single distribution, i.e., paragraphs derived from Wikipedia articles. Nevertheless, we often use models in settings different from the one in which they were trained. While Jia and Liang (2017) demonstrated that SQuAD models are not robust to adversarial distribution shifts, one might still hope that the models are more robust to natural distribution shifts, for instance changing from Wikipedia to newspaper articles.\n\nThis state of affairs raises two important questions:\n\nAre SQuAD models overfit to the SQuAD test set?\n\nAre SQuAD models robust to natural distribution shifts? In this work, we address both questions by replicating the SQuAD dataset creation process and generating four new SQuAD test sets on both the original Wikipedia domain, as well as three new domains: New York Times articles, Reddit posts, and Amazon product reviews.\n\nWe first show that there is no evidence of adaptive overfitting on SQuAD. Across a large collection of SQuAD models, there is little to no difference between the F1 scores from the original SQuAD test set and our replication. This even holds when comparing scores from the SQuAD development set (which was publicly released with answers) to our new test set. The lack of adaptive overfitting is consistent with recent replication studies in the context of image classification ; Yadav and Bottou (2019). These studies leave open the possibility that this phenomenon is specific to the data or models typical in computer vision research. Our result demonstrates this same phenomenon also holds for natural language processing.\n\nBeyond adaptive overfitting, we also demonstrate that SQuAD models exhibit robustness to some of our natural distribution shifts, though they still suffer substantial performance degradation on others. On the New York Times dataset, models in our testbed on average drop 3.8 F1 points. On the Reddit and Amazon datasets, the drop is on average 14.0 and 17.4 F1 points, respectively. All of our datasets were collected using the same data generation pipeline, so this degradation can be attributed purely to changes in the source text rather than differences in the annotation procedures across datasets.\n\nWe complement each of these experiments with a strong human baseline comprised of the authors of this paper. On the original SQuAD data, our human accuracy numbers are on par with the best SQuAD models  and significantly better than the Mechanical Turk baseline reported by Rajpurkar et al. (2016). On our new test sets, average human F1 scores decrease by 0.1 F1 on New York Times, 2.9 on Reddit, and 3.0 on Amazon. All of the resulting F1 scores are substantially higher than the best SQuAD models on the respective test sets. Figure 1 summarizes the main results of our experiments. Humans show consistent behavior on all four test sets, while models are substantially less robust against two of the distribution shifts. Although there has been steady progress on the SQuAD leaderboard, there has been markedly less progress in this robustness dimension.\n\nTo enable future research, all of our new tests sets are freely available online. 1 \n\n\nBackground\n\nIn this section, we briefly introduce the SQuAD dataset and present a formal model for reasoning about performance drops between our test sets.\n\n\nStanford Question Answering Dataset\n\nSQuAD is an extractive question answering dataset introduced by Rajpurkar et al. (2016). An example in SQuAD consists of a passage of text, a question, and one or more spans of text within the passage that answer the question. An example is given in Figure 2. Model performance is evaluated using one of two metrics: exact match (EM) or F1. Exact match measures the percentage of predictions that exactly match at least one of the ground truth answers. F1 measures the maximum overlap between the tokens in the predicted span and any of the ground truth answers, treating both the prediction and each answer as a bag of words. Both metrics are described formally in Appendix A.\n\nAfter releasing the original SQuAD v1.1 dataset, Rajpurkar et al. (2018) introduced a new variant of the dataset, SQuAD 2.0, that includes unanswerable questions. Since SQuAD v1.1 has been public for longer and potentially subjected to more adaptivity, we focus on SQuAD v1.1 and refer to it as the SQuAD dataset throughout our paper. The SQuAD v1.1 test set is not publically available. Therefore, while we use public test set evaluation numbers, we otherwise use the public SQuAD v1.1 development set for analysis.\n\nPassage: \"In our neighborhood, we were the small family, at least among the Irish and Italians\u2026 We could almost field a full baseball team. But the Flynns, they could put an entire football lineup\u2026 We loved Robert F. Kennedy's family: 11 kids, and Ethel looks great. Bobby himself was the seventh of nine.\" Question: How many kids did Robert F. Kennedy have? Answer: 11\n\nQuestion: The author believes his family could fill a team of which sport? Answer: baseball \n\n\nA Model for Generalization\n\nAlthough progress on SQuAD is measured through performance on a held-out test set, the implicit goal is not to achieve high F1 scores on the test set, but rather to generalize to unseen examples. Our experiments test the extent to which this assumption holds-if models with high leaderboard scores on the test set continue to perform well on new examples, whether from the same or different distributions.\n\nTo be more formal, suppose the original test set S is sampled from some underlying distribution D, and consider a model f submitted to the SQuAD leaderboard. Let L S (f ) denote the empirical loss of model f on the sample S, and let L D (f ) denote the corresponding population loss. In our experiment, we gather a new dataset of examples S from a distribution D , potentially different from D. We wish for the loss on the new sample, L S (f ) to be close to the original, L S (f ). Omitting f , we can decompose this gap into three terms . The adaptivity gap L S \u2212 L D measures how much adapting the model to the held-out test set S biases the estimate of the population loss. Since recent models are in part chosen on the basis of past test set information, the model f is not independent of S. Hence L S (f ) can underestimate L D (f ), a phenomenon called adaptive overfitting. The distribution gap measures how much changing the distribution from D to D affects the model's performance. Finally, the generalization gap L S \u2212 L D captures the difference between the sample and the population losses due to random sampling of S . Since S is sampled independently of the model f , this gap is typically small and well-controlled by standard concentration results. For example, on the new Wikipedia test set, the average size of Student's t-confidence intervals for models in our testbed is \u00b10.6 F1.\n\nIn the sequel, we empirically measure both the adaptivity gap and the distribution gap for a wide range of SQuAD models by collecting new test sets from a variety of distributions D . We first review related work that motivates our choice of SQuAD and natural distribution shifts.\n\n\nRelated Work\n\nAdaptive data analysis. Although repeated test-set reuse puts pressure on the statistical guarantees of the holdout method Dwork et al. (2015), a series of replication studies established there is no adaptive overfitting on popular classification benchmarks like MNIST (Yadav and Bottou, 2019), CIFAR-10 , and ImageNet . Furthermore,  also found little to no evidence of adaptive overfitting in a host of classification competitions on the Kaggle platform. These investigations either concern image classification or smaller competitions that have not been subject to intense, multi-year community scrutiny. Our work establishes similar results for natural language processing on a heavily studied benchmark.\n\nA number of works have proffered explanations for why adaptive overfitting does not occur in the standard machine learning workflow ( . We focus our analysis on SQuAD for two reasons. First, SQuAD has been the focus of intense research for almost four years, and the competitive nature of the leaderboard format makes it an excellent example to study adaptive overfitting in natural language processing. Second, SQuAD requires all submissions to be uploaded to CodaLab 2 , which ensures reproducibility and makes it possible to evaluate every submission on our new datasets using the same configuration and environment as the original evaluation. In these cases, the datasets encountered at test time vary across a number of dimensions: the question collection procedure, the origin of the input text, the question answering interface, the crowd worker population, etc. These differences are confounding factors that make it difficult to interpret performance differences across datasets. For example, human performance differs by 10 F1 points between SQuAD v1.1 and NewsQA (Trischler et al., 2017). In contrast, our datasets focus on a single factor of variation-the input text corpus. In this controlled setting, we observe non-trivial F1 drops across a large collection of models, while human F1 scores are essentially constant. \n\n\nCollecting New Test Sets\n\nIn this section, we describe our data collection methodology. Data collection primarily proceeds in two stages: curating passages from a text corpus and crowdsourcing question-answer pairs over the passages. In both of these stages, we take great care to replicate the original SQuAD data generation process. Where possible, we obtained and used the original SQuAD generation code kindly provided by Rajpurkar et al. (2016). We ran our dataset creation pipeline on four different corpora: Wikipedia articles, New York Times articles, Reddit posts, and Amazon product reviews.\n\n\nPassage Curation\n\nThe first step in the dataset generation process is selecting the articles from which the passages or contexts are drawn.\n\nWikipedia. We sampled 48 articles uniformly at random from the same list of 10,000 Wikipedia articles as Rajpurkar et al. (2016), ensuring that there is no overlap between our articles and those in the SQuAD v1.1 training or development sets. To minimize distribution shift due to temporal language variation, we extracted the text of the Wikipedia articles from around the publication date of the SQuAD v1.0 dataset (June 16, 2016). For each article, we extracted individual paragraphs and stripped out images, figures, and tables using the same data processing code as Rajpurkar et al. (2016). Then, we subsampled the resulting paragraphs to match the passage length statistics of the original SQuAD dataset. 3 See Appendix D.1 for a detailed comparison of the paragraph distribution of the original SQuAD dev set and our new SQuAD test set.\n\nNew York Times. We sampled New York Times articles from the set of all articles published in 2015 using the NYTimes Archive API. We scraped each article with the Wayback Machine 4 , using the same snapshot timestamp as our Wikipedia dataset, and removed foreign language articles. Since the average paragraph length for NYT articles is significantly shorter than the average paragraph length for Wikipedia articles, we merged each NYT paragraph with its subsequent paragraph with some probability. Then we subsampled the merged paragraphs to match the passage length statistics of the original SQuAD v1.1 dataset.\n\nReddit Posts. We sampled Reddit posts from the set of all posts across all subreddits during the month of January 2016 in the Pushshift Reddit Corpus (Baumgartner et al., 2020). Then we restricted the set of posts to those marked as \"safe for work\" and manually removed inappropriate posts from the remaining ones. We concatenated each post's title with its body, removed Markdown, and replaced all links with a single token, LINKREMOVED. We then subsampled the posts to match the passage length statistics of the original SQuAD v1.1 dataset.\n\nAmazon Product Reviews. We sampled Amazon product reviews belonging to the \"Home and Kitchen\" category from the dataset released by McAuley et al. (2015). As in the previous datasets, we then subsampled the reviews to match the passage length statistics of SQuAD v1.1.\n\n\nCrowdsourcing Question-Answer Pairs\n\nWe employed crowdworkers on Amazon Mechanical Turk (MTurk) to ask and answer questions on the passages in each dataset. We followed a nearly identical protocol to the original SQuAD dataset creation process. We used the same MTurk user interface, task instructions, MTurk worker qualifications, time per task, and hourly rate (adjusted for inflation) as Rajpurkar et al. (2016). For full details and examples of the user interface, refer to Appendix D.2.\n\nFor each paragraph, one crowdworker first asked and answered up to five questions on the content of the paragraph. Then we obtained at least two additional answers for each question using separate crowdworkers. There are two points of discrepancy between our crowdsourcing protocol and the one used to create the original SQuAD dataset. First, we interfaced directly with MTurk rather than via the Daemo platform because the Daemo platform has been discontinued. Second, in our MTurk tasks, workers asked and answered questions for at most five paragraphs rather than for the entire article because MTurk workers preferred smaller units of work. Although each difference is a potential source of distribution shift, in Section 5 we show that the effect of these changes is negligible-models achieve roughly the same scores on both the original and new Wikipedia datasets. On average, the difference in F1 scores is 1.5 F1, and 95% of models in our testbed are within 2.7 F1.\n\nAfter gathering question and answer pairs for each paragraph, we apply the same postprocessing and data cleaning as SQuAD v1.1. We adjusted answer whitespace for consistency, filtered malformed answers, and removed all documents that had less than an average of two questions per paragraph after filtering. In Appendix C.7, we show that further manual filtering of incorrect, ungrammatical, or otherwise malformed questions and answers has negligible impact on our results. Table 1 summarizes the overall statistics of our datasets.\n\n\nHuman Evaluation\n\nAlthough both SQuAD and our new test sets have answers from MTurk workers, it is not clear whether these answers represent a compelling human baseline. At minimum, workers are not familiar with the typical style of answers in SQuAD (e.g., how much detail to include), and they receive no feedback on their performance. To obtain a stronger human baseline, the graduate student and postdoc authors of this paper also answered approximately 1,000 questions on each of the four new test sets and the original SQuAD development set, following the same procedure and using the same UI as the MTurk workers. To take feedback into account, each participant first labelled 500 practice examples from the training set and compared their answers with the ground truth.\n\n\nMain Results\n\nWe use the four new datasets generated in the previous part to test for adaptive overfitting on SQuAD and probe the robustness of SQuAD models to natural distribution shifts.\n\nWe evaluated a broad set of over 100 models submitted to the SQuAD leaderboard, including state-of-the-art models like XLNet  and BERT , as well as older, but popular models like BiDAF . All of the models were submitted to the CodaLab platform, and we evaluate every model using the exact same configuration (model weights, hyperparameters, command-line arguments, execution environment) as the original submission. Tables 2 and 3 contain a brief summary of the results for key models. Detailed results table and citations for the models, where available, are given in Appendix E.\n\n\nAdaptive Overfitting\n\nThe SQuAD models in our testbed come from a long sequence of papers that incrementally improve F1 and EM scores over a period of several years. Consequently, if there is adaptive overfitting, we should expect the later models to have larger drops in F1 scores because they are the result of more interaction with the test set. In this case, the higher F1 scores are partially the result of a larger adaptivity gap, and we would expect that, as the observed scores L S continue to rise, the population scores L D would begin to plateau.\n\nTo check for adaptive overfitting on the existing test set, we plot the SQuAD v1.1 test F1 scores against F1 scores on our new Wikipedia test set. Figure 1 in Section 1 provides strong evidence against the adaptive overfitting hypothesis. Across the entire model collection, the F1 scores on the new test set closely replicate the original F1 scores. The observed linear fit is in contrast to the concave curve one would expect from adaptive overfitting. We use 95% Student's t-confidence intervals, which make a large-sample Gaussian assumption, to capture the error in the new F1 scores due to random variation. No such confidence intervals are available for the original test set scores since the test set is not publicly available. A similar plot for EM scores is provided in Appendix C.1.\n\nNot only is there little evidence for adaptive overfitting on the test set, there is also little evidence of adaptive overfitting on the SQuAD development set. In Figure 3, we plot F1 scores on the SQuAD v1.1 development set against F1 scores on the SQuAD v1.1 test set. With the exception of three models, the F1 scores on the dev set closely match the scores on the test set, despite the fact that the development set is aggressively used during model selection. Moreover, the models that do not lie on the linear trend line-Common-sense Governed BERT-123 (April \n\n\nRobustness to Natural Distribution Shifts\n\nGiven the correspondence between the old and new Wikipedia test set F1 scores, the adaptivity gap and the distribution gap are small or non-existent. Consequently, the distribution shift stemming from our data generation pipeline affects the models only minimally. This allows us to probe the sensitivity of the SQuAD models to a set of controlled distribution shifts, namely the choice of text corpus. Since all of the datasets are constructed with the same preprocessing pipeline, crowd-worker population, and post-processing, the datasets are free of confounding factors that would otherwise arise when comparing model performance across different datasets. Figure 1 in Section 1 shows F1 scores on the SQuAD v1.1 test set versus the F1 scores on each of our new test sets for all the models in our testbed. All models experience an F1 drop on the new test sets, though the magnitude strongly depends on the specific test set. On New York Times, for instance, BERT only drops around 2.1 F1 points, whereas it drops around 11.9 F1 points on Amazon and 11.5 F1 points on Reddit. The top performing XLNet model ) is a clear outlier. Despite generalizing well to the new Wikipedia dataset, XLNet drops nearly 10 F1 and 40 EM points on New York Times, substantially more than models with similar performance on SQuAD v1.1 as well as other XLNet variants, e.g., XLNet-123 5 . Table 3 summarizes the F1 scores for a select set of models. Full results for all models, datasets, and EM scores are given in Appendix E. In general, F1 scores on the original SQuAD test set are highly predictive of F1 scores on the new test sets. Interestingly, the relationship is well-captured by a linear fit even under distribution shifts. Similar to , in Figure 4, we observe the linear fits are better under a probit scaling of F1 scores. See Appendix C.2 for more details. Moreover, the gap between perfect robustness (y = x) and the observed linear fits varies with the dataset: 3.8 F1 points for New York Times, 14.0 points for Reddit, and 17.4 F1 for Amazon. In each case, however, higher performance on SQuAD v1.1 translates into higher performance on these natural distribution shift instances.\n\nDespite the robustness demonstrated by the models, on all of the test sets with distribution shift, human performance is substantially higher than model performance and well above the linear fits shown in Figure 1 and Figure 4. This rules out the possibility that the shift in F1 scores are entirely by a change in the Bayes error rate. Moreover, it points towards substantial room for improvement for models on our new test sets.\n\n\nFurther Analysis\n\nIn this section, we further explore the properties of our new test sets. We first study the extent to which common measures of dataset difficulty can explain the performance drops on our new test sets. Then, we evaluate whether training models with more data or more diverse data improves robustness to our distribution shifts.   Syntactic divergence. We also stratify our datasets using the automatic syntactic divergence measure of Rajpurkar et al. (2016). Syntactic divergence measures the similarity between the syntactic dependency tree structure of both the question and answer sentences and provides another metric of example difficulty. In Figure 6, we compare the histograms of syntactic divergence for the SQuAD v1.  Although there are differences between the datasets, e.g., New York Times has more person answers, the four datasets are very similar. Moreover, we show in Appendix C.4 that differences in answer categorization across datasets do not explain the performance drops we observe.\n\nreweighing these scores based on the relative frequency of each category on new test sets. This model explains virtually none of the observed changes in F1 scores.\n\n6.2 Are Models Trained with More Data More Robust to Natural Distribution Shifts?\n\nHigh performance on our new datasets requires models to generalize to data distributions that may be different from those on which they were trained. Our primary evaluation only concerns the robustness of SQuAD models, and a natural follow-up question is whether models trained on more data, or explicitly trained for out-of-distribution question-answering, perform better on our new test sets.\n\nTo test this claim, we evaluated a collection of models from the Machine Reading for Question Answering (MRQA) 2019 Shared Task on Generalization (Fisch et al., 2019). In the shared task, models were trained on 6 question-answering datasets, including SQuAD v1.1, and then evaluated on 12 held-out datasets. The datasets simultaneously differed not just in the passage distribution, as in our experiments, but also in confounders like the data collection procedure, the question distribution, and the relationship between questions and passages.\n\nIn Figure 7, we plot the F1 scores of MRQA models on the SQuAD v1.1 dataset against the F1 scores on each of our new test sets, along with the linear fits from Figure 1. On the Reddit and Amazon test sets, the best MRQA model in our testbed, Delphi (Longpre et al., 2019), achieves higher F1 scores than any SQuAD model and is substantially above the linear fit. However, many of the models trained on more data exhibit little to no improved robustness. In addition, all of the models are still substantially below the human F1 scores and robustness. See Appendix E.2 for the full results table.\n\n\nDiscussion\n\nDespite years of test set reuse, we find no evidence of adaptive overfitting on SQuAD. Our findings demonstrate that natural language processing benchmarks like SQuAD continue to support progress much longer than than reasoning from first principles might have suggested.\n\nWhile SQuAD models generalize well to new examples from the same distribution, results on our new test sets also show that robustness to distribution shift remains a challenge. On each  Although the MRQA models still lag human performance and robustness across datasets, these models, particularly those with high F1 scores on the original SQuAD, exhibit increased robustness and generalization across each of the datasets compared to models that are only trained on SQuAD.\n\nof our new test sets, a strong human baseline is largely unchanged, but SQuAD models suffer non-trivial and nearly uniform performance drops. While question answering models have made substantial progress on SQuAD, there has been less progress towards closing the robustness gap under non-adversarial distribution shifts. This highlights the need to move beyond model evaluation in the standard, i.i.d. setting, and to explicitly incorporate distribution shifts into evaluation. We hope our new test sets offer a helpful starting point. There are multiple promising avenues for future work. One direction is constructing metrics for comparing datasets that can explain the performance differences we observe. Why do models perform so well on New York Times, but experience much larger drops on Reddit and Amazon? Stratifying our datasets using common criteria like answer type or reasoning required appears insufficient to answer this question. Another important direction is to better understand the interplay between additional data and model robustness. Some of the models from the MRQA challenge, e.g., Delphi (Longpre et al., 2019), benefit substantially from training with additional data, while other models remain near the same linear trend line as the SQuAD models. From both empirical and theoretical perspectives, it would be interesting to better understand when and why training with additional data improves robustness, and to offer concrete guidance on how to collect and use additional data to improve robustness to distribution shifts. \n\n\nA Evaluation Metrics\n\nIn this section, we formally define the evaluation metrics used throughout our experiments. Let (p, q, (a 1 , . . . , a n ) denote a passage p, a question q, and a set of n answers (a 1 , . . . , a n ). Let S denote the sampled dataset, let f denote some model, and f (p, q) =\u00e2 be its predicted answer.\n\nF1 Score. F1 measures the average overlap between the prediction and the ground-truth answer. Given answer a and prediction\u00e2, consider a and\u00e2 as bags of words (sets), and let v(a,\u00e2) be their associated F1 score, i.e. the harmonic mean of precision and recall between the two sets. Then,\nF1(f ) = 1 |S| (p,q,(a 1 ,...,a n ))\u2208S max i=1,...,n v(a i , f (p, q)).\nExact match. Exact match measures the percentage of predictions that exactly match any one of the ground truth answers.\nExactMatch(f ) = 1 |S| (p,q,(a 1 ,...,a n ))\u2208S max i=1,...,n 1{f(p, q) = a i }.\nAll of our results are reported using the evaluation script provided by Rajpurkar et al. (2016), which ignores punctuation and the articles \"a\", \"an\", and \"the\" when computing the above metrics.\n\n\nB Comparing Natural and Adversarial Distribution Shift\n\nTo contrast natural and adversarial distribution shifts, we evaluated all of the models in our testbed against the adversarial attacks described in Jia and Liang (2017) on the original SQuAD v1.1 dataset.\n\nAddSent. In the AddSent attack, for every passage, question, and answer pair (p, q, a), Jia and Liang (2017) procedurally generate up to five new sentences to append to the passage p that do not contradict the correct answer. Each of the sentences are generated to be similar to the correct answer, and ungrammatical or contradictory sentences are removed by crowdworkers. This results in a set of new examples (p 1 , q, a), . . . , (p 5 , q, a) for each original example. The adversary evaluates the model f on each of the 5 examples and picks the one that gives the lowest score, min i=1,...,5 s(f (p i , q), a), where s is the scoring function (exact match or F1). In Figure 8, we compare F1 and EM scores on the original SQuAD v1.1 test set with F1 and EM scores against the adversarial AddSent attack. 50  Similar to the natural distribution shift examples, we observe the relationship between the original test F1 scores and the adversarial F1 test scores broadly follow a linear trend. However, the linear fit is not as good compared to the natural distribution shifts. There is more variability in model performance around the trend line, and this is reflected in lower a R 2 statistic, e.g. 0.72 for AddSent F1, compared to 0.99, 0.97, 0.91, and 0.89 for the New Wikipedia, New York Times, Reddit, and Amazon test sets, respectively. As with the natural distribution shift datasets, the linear fit is better in the probit domain, which we visualize in Figure 9. However, the R 2 statistic is still smaller than the corresponding statistics for our distribution shift datasets in AddOneSent. The AddOneSent attack similar to the AddSent attack. However, rather than take the worst of the 5 altered passages, it randomly selects one of the five on which to evaluate the model. In Figure 10, we compare F1 and EM scores on the original SQuAD v1.1 test set with F1 and EM scores against the adversarial AddSent attack. Since this attack does not require model access or evaluations, it is closer in spirit to the natural distribution shifts we consider. We observe much the same phenomenon as we see with AddSent. Model performance broadly follows a linear trend, and there is more variability around the linear trend line than in our natural distribution shift datasets. \n\n\nC Additional Analysis and Results\n\nIn this appendix, we present additional results and analysis to better understand our distribution shift experiments.\n\n\nC.1 Exact Match Scatterplots\n\nSimilar to Figure 1 in Section 1, we compare the EM scores of all models in our testbed on the SQuAD v1.1 test set versus the EM scores of all models on each of the new test sets. The results are shown in Figure 11. In each case, we observe a more pronounced drop than the F1 scores with average drops of 4.6, 5.75, 20.0, and 24.8 for each of the new Wikipedia, New York Times, Reddit, and Amazon datasets, respectively. However, the primary trends are the same. In particular, we observe little evidence of overfitting on Wikipedia (the linear model nicely describes the data), and we observe a similar ranking of magnitudes of the drop on each of the other three datasets-New York Times exhibits a small drop, followed by larger drops on Reddit and Amazon. 40 \n\n\nC.2 Linear Fits in the Probit Domain\n\nIn many cases, a linear model of F1 or EM scores is not a good fit when the scores span a wide range. In these cases, we find that a probit model describes the data better. In the main text, Figure 4 shows the F1 scores for the Amazon dataset on both the linear scale used throughout the data and a probit scale obtained by transforming all of the F1 scores with the inverse Gaussian CDF. We observe a better linear fit for our data. Original F1 Score D Dataset collection details.\n\nIn this section, we provide further details regarding our data collection pipeline.\n\n\nD.1 Passage Length Statistics\n\nWe report statistics on various text length statistics. We split each paragraph into individual sentences, words, and characters using spaCy (Honnibal and Montani, 2017) and compute histograms showing the passage sentence, word, and character length distributions across each dataset. Figures 19, 20, and 21 show the paragraph lengths in characters, words, and sentences across each dataset. In Figures 22 and 23, we show the small differences in the distribution of passage length in terms of words or sentences does not explain the observed performance drops.     \n\n\nD.2 MTurk Experiment and UI Examples\n\nWorker Details. Crowdworkers were required to have a 97% HIT acceptance rate, a minimum of 1000 HITs, and be located in the United States or Canada. Workers were asked to spend four minutes per paragraph when asking questions and one minute per question when answering questions. We paid workers $9.60 per hour for the amount of time required to complete each task, using an inflation rate of 6.52% between 2016 and 2019. UI Examples. The task directions and website UI are identical to the original SQuAD data collection setup with the sole exception that the original tasks had workers ask and answer questions for all of the paragraphs for each article, whereas our tasks limit each worker to at most 5 paragraphs. Figures 24 and 25 show the directions and an example HIT for the Ask task, whereby workers pose questions for the article. Figures 26 and 27 show the directions and an example HIT for the Answer task, whereby workers answer questions posed during the Ask task.    \n\n\nE Complete Model Testbed and Results Tables\n\nIn this section, we detail the complete model testbed and provide evaluation results for each model on each of our four distribution shift datasets, as well as the adversarial distributions discussed in Section B.\n\n\nE.1 Models Evaluated\n\nWe evaluated a representative subset of over 100 models submitted to the SQuAD leaderboard since 2016. All of the models were submitted to the CodaLab platform, and thus we evaluate every model in the exact same configuration (weights, hyperparameters, command-line arguments, execution environment, etc.) as the original submission. Below, we list all of the models we evaluated with references, where available, and links to the Codalab submission bundle. The models are listed in sorted order based on their SQuAD v1.1 Test F1 score to allow easy reference to the subsequent tables.\n\nWe also evaluated a subset of five models from the Machine Reading for Question Answering (MRQA) Shared Task (Fisch et al., 2019) on our new test sets. As in our primary experiments, all of the models were submitted to the CodaLab platform, and we evaluated every model in the exact same configuration as the original submission. Below, we list all of the models we evaluated with references and links to the submission bundle. The remaining models submitted to the competition were either not publicly accessible or otherwise unable to run on Codalab.\n\n\nE.2 Full Results Tables\n\nMain Results. In this section, we present the results for each SQuAD model and the 5 MRQA model listed in Appendix E.1, along with results for the three student and postdoc authors of this paper, on each of our new test sets. Tables 5, 6 7, and 8 contain the results for each our models and the three human annotators in terms of F1 score for the New Wikipedia, New York Times, Reddit, and Amazon test sets, respectively. Tables 9, 10, 11, and 12 contain the same data for exact match scores. For a particular dataset, some models are not listed if we were unable to evaluate the model on the dataset in Codalab.           \n\nFigure 1 :\n1Model and human F1 scores on the original SQuAD v1.1 test set compared to our new test sets. Each point corresponds to a model evaluation, shown with 95% Student's t-confidence intervals (mostly covered by the point markers). The plots reveal three main phenomena: (i) There is no evidence of adaptive overfitting on SQuAD, (ii) all of the models suffer F1 drops on the new datasets, with the magnitude of the drop strongly depending on the corpus, and (iii) humans are substantially more robust to natural distribution shifts than the models. The slopes of the linear fits are 0.92, 1.02, 1.19, and 1.36, respectively, and the R 2 statistics for the linear fits are 0.99, 0.97, 0.9, and 0.89, respectively. This means that every point of F1 improvement on the original dataset translates into roughly 1 point of improvement on our new datasets.\n\nFigure 2 :\n2Question and answer pairs from a sample passage in our New York Times SQuAD test set. Answers are text spans from the passage that answer the question.\n\nL\nS \u2212 L S = (L S \u2212 L D ) Adaptivity gap + (L D \u2212 L D ) Distribution gap + (L D \u2212 L S ) Generalization gap\n\n\nBlum and Hardt, 2015; Mania et al., 2019; Feldman et al., 2019; Zrnic and Hardt, 2019). Complementary to these results, our work provides a new data point with which to validate and deepen our conceptual understanding of overfitting. Datasets for question answering. Beyond SQuAD, a number of works have proposed datasets for question answering (Richardson et al., 2013; Berant et al., 2014; Joshi et al., 2017; Trischler et al., 2017; Dunn et al., 2017; Yang et al., 2018; Kwiatkowski et al., 2019)\n\n\nGeneralization in question answering. Given the plethora of question-answering datasets, Yogatama et al. (2019), Talmor and Berant (2019), and Sen and Saffari (2020) evaluate the extent to which models trained on SQuAD generalize to other question-answering datasets. Hendrycks et al. (2020) evaluates generalization under distribution shift for question answering, among other tasks, by carefully splitting subsets of the ReCoRD Zhang et al. (2018) dataset. In a similar vein, Fisch et al. (2019) conduct a shared task competition that evaluates how well models trained on a collection of six datasets generalize to unseen datasets at test time.\n\nFigure 3 :\n3Comparison of F1 scores between the SQuAD v1.1 dev set and the SQuAD v1.1 test set. Despite heavy use of the dev set during model development, the dev set and test set scores closely match, with the exception of three models that were explicitly trained on the dev set, Common-sense Governed BERT-123 (April 21), Common-sense Governed BERT-123 (May 9), and XLNet-123++.(Qiu, 2020). The slope of the linear fit is 0.97.\n\nFigure 5 :\n5Comparison of answers types in the original and new datasets. We automatically partition our answers into the same categories asRajpurkar et al. (2016).\n\nFigure 6 :\n6Histograms of syntactic divergence between question and answer sentences for both the original and new datasets. All of the datasets have a similar distribution of syntactic divergence, though the Reddit and Amazon datasets have more question-answers pairs with small (1-2) syntactic divergence.\n\nFigure 7 :\n7Model from the MRQA Shared Task 2019, trained on 5 datasets beyond SQuAD, and human F1 scores on the original SQuAD test set and each of our new test sets. The error bars are 95% Student's t-confidence intervals.\n\nFigure 9 :\n9Comparison of F1 and EM scores on the original SQuAD test set versus the adversarial AddSent attack from Jia and Liang (2017) with probit scaling. For F1 scores, the slope of the linear fit is 0.99, and for EM, the slopes is 1.11. In the probit domain, the R 2 statistics are 0.82 and 0.81, respectively. the probit domain: 0.82 compared to 0.99, 0.96, 0.94, and 0.94, for New Wikipedia, New York Times, Reddit, and Amazon, respectively.\n\nFigure 18 :\n18Comparison between model F1 scores on our New-Wikipedia and Amazon datasets and F1 scores on subsets of the datasets with additional human filtering to remove malformed, unanswerable, incorrect, ungrammatical questions and answers. To focus annotator effort on potentially bad questions, if all three MTurk annotators agreed on the answer, the question and answer were automatically marked as valid. For the New Wikipedia dataset, we manually inspected an additional 1,894 questions, removed 85 questions, and removed answers for an 444 questions. For the Amazon dataset, we manually inspected an additional 1,839 questions, removed 46 questions, and removed answers for an 282 questions. This process resulted in human curated subsets of 5574 questions for the New Wikipedia dataset and 6471 questions for the Amazon datasets. On the New Wikipedia dataset, models improve an average of 0.86 F1 points on this filtered dataset. For the filtered Amazon dataset, models slightly decreased their performance by 0.09 F1 points on average. In both cases, the rank order of the models and the linear trend observed on the full datasets without additional human filtering is preserved.\n\nFigure 19 :\n19Histograms of the number of characters in each paragraph for the original SQuAD v1.1 development set and our new test sets. The histograms lengths match exactly since we sample in a way that ensures the character length will match for each new dataset.\n\nFigure 20 :\n20Histograms of the number of words in each paragraph for the original SQuAD v1.1 development set and our new test sets. The Wikipedia histograms match closely, while the Amazon and Reddit datasets' paragraphs have slightly more words. However, these differences do not explain the performance drops we observe, asFigure 22demonstrates.\n\nFigure 21 :\n21Histograms of the number of sentences in each paragraph for both the original and new datasets. The new Wikipedia dataset matches the SQuAD v1.1 dataset, while the other new test sets have a slightly longer tail. These slight difference in sentences per paragraph do not explain the performance drops we observe, asFigure 23demonstrates.\n\nFigure 24 :\n24Ask task directions.\n\nFigure 25 :\n25Ask task example.\n\nFigure 26 :\n26Answer task directions.\n\nFigure 27 :\n27Answer task example.\n\n\nFrom a different perspective, Jia and Liang (2017) and Ribeiro et al. (2018) consider robustness to adversarial dataset corruptions. Kaushik et al. (2019) and Gardner et al. (2020) evaluate model performance when individual examples are perturbed in small, but semantically meaningful ways. While we instead focus on naturally occurring distribution shifts, we also evaluate our model testbed on adversarial distribution shifts for comparison in Appendix B.\n\nTable 1 :\n1Dataset statistics of our four new test sets compared to the original SQuAD 1.1 development and test sets.Dataset \nTotal Articles Total Examples \n\nSQuAD v1.1 Dev \n48 \n10,570 \nSQuAD v1.1 Test \n46 \n9,533 \nNew Wikipedia \n48 \n7,938 \nNew York Times \n797 \n10,065 \nReddit \n1969 \n9,803 \nAmazon \n1909 \n9,885 \n\n\n\nTable 2 :\n2Comparison of model F1 scores on the original SQuAD test set and our new Wikipedia test set. Rank refers to the relative ordering of the models in our testbed using the original SQuAD v1.1 F1 scores, new rank refers to the ordering using the new Wikipedia test set scores, and \u2206 rank is the relative difference in ranking from the original test set to the new test set. The confidence intervals are 95% Student's t-intervals. No confidence intervals are provided for the SQuAD v1.1 dataset since the dataset is not public and only the average scores are available. A complete table with data for the entire model testbed, references, and analogous data for EM scores is in Appendix E.New-Wiki F1 Score Summary \n\nRank Name \nSQuAD \nNew-Wiki \nGap \nNew \nRank \n\u2206 Rank \n\n-\nHuman average (this study) \n95.1 \n92.4 \n2.7 \n-\n-\n1 \nXLNet \n95.1 \n92.3 [91.9, 92.8] 2.7 \n1 \n0 \n2 \nXLNET-123 \n94.9 \n92.2 [91.7, 92.7] 2.7 \n4 \n-2 \n6 \nTuned BERT-1seq Large \n93.3 \n91.0 [90.5, 91.5] 2.3 \n7 \n-1 \n8 \nBERT-Large Baseline \n92.7 \n90.8 [90.3, 91.3] 1.9 \n9 \n-1 \n42 \nBiDAF+SelfAttention+ELMo \n85.9 \n83.8 [83.1, 84.5] 2.1 \n45 \n-3 \n62 \nJenga \n82.8 \n80.1 [79.3, 80.9] 2.7 \n71 \n-9 \n85 \nAllenNLP BiDAF \n77.2 \n76.5 [75.7, 77.3] 0.7 \n88 \n-3 \n\n21), Common-sense Governed BERT-123 (May 9), and XLNet-123++-are directly trained on \nthe development set (Qiu, 2020). \n\n\n\n\nStudent's t-confidence intervals. The left plot shows the model F1 scores under a linear axis scaling, whereas the right plot uses an probit scale on both axes. In other words, model F1 score x appears at \u03a6 \u22121 (x), where \u03a6 \u22121 is the inverse Gaussian CDF. Visual inspection shows the linear fit is better in the probit domain. Quantitatively, the R60 \n70 \n80 \n90 \n100 \n\nSQuAD Test F1 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nAmazon F1 \n\nLinear Scaling \n\n50 \n60 \n70 \n80 \n90 \n95 \n\nSQuAD Test F1 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n95 \n\nProbit Scaling \n\ny = x \nLinear Fit \nModel F1 \nHuman F1 \n\nFigure 4: Comparison of model and human F1 scores on the original SQuAD v1.1 test set and \nour new Amazon test set. Each datapoint corresponds to one model in the testbed and is shown \nwith 95% 2 statistic is 0.89 in the linear \ndomain, compared to 0.94 in the probit domain. See Appendix C.2 for similar comparisons for \nall datasets. \n\n6.1 Are The New Test Sets Harder Than The Original? \n\nOne hypothesis for the performance drops observed in Section 5.2 is that our new dataset are \nharder in some sense. For instance, the diversity of answers may be greater among Reddit \ncomments than Wikipedia articles. To better understand this question, we compare the original \nSQuAD development set to our four new test sets using the three difficulty measures introduced \nin Rajpurkar et al. (2016). \n\nAnswer diversity. Following Rajpurkar et al. (2016), we automatically categorize each an-\nswer into numerical and non-numerical answers, named entities, and constituents using spaCy \n(Honnibal and Montani, 2017) and the constituency parser from Kitaev and Klein (2018). His-\ntograms of answer types for each data are shown in Figure 5. Since the original pipeline is \nnot available, our implementation differs slightly from Rajpurkar et al. (2016) and we include \nresults on the SQuAD v1.1 development set for comparison. Both the original and our new \nWikipedia test set have very similar answer type histograms. The distribution shift datasets \nhave slight variations in the answer distributions. For instance, NYT has more person answers, \nwhereas Amazon has more adjective phrases. However, changes in the answer type distribution \nbetween datasets are not sufficient to explain the performance differences between the datasets. \nIn Appendix C.4, we consider a simple model that predicts F1 scores on our new test sets by \nstratifying the dataset by answer type, computing model F1 scores for each type, and then \n\n\nTable 3 :\n3Comparison of model F1 scores on the original SQuAD test set and our new Amazon test set. Rank refers to the relative ordering of the models in our testbed using the original SQuAD v1.1 F1 scores, new rank refers to the ordering using the Amazon test set scores, and \u2206 rank is the relative difference in ranking from the original test set to the new test set. reweighing these scores by the relative frequency of each answer type in our new test set. This model explains only a small fraction of the performance differences across test sets.The \n\n\n\n1 development set and our new test sets. All of the datasets have similar histograms, though both the Reddit and Amazon test sets have slightly more examples with small syntactic divergence. As in the previous part, in Appendix C.5, we consider a \nsimple model that predicts F1 scores on the new test sets by stratifying the dataset according \nto syntactic divergence and reweighting based on the relative frequence of examples with a \ngiven syntactic divergence measure. As before, this model explains only a small fraction of the \nperformance differences across test sets. \n\nReasoning required. Finally, we compare our new test sets in terms of the reasoning re-\nquired to answer each question-answer pair, using the same non-mutually exclusive categories \nas Rajpurkar et al. (2016). For each test set, as well as the SQuAD development set, we ran-\ndomly sampled and manually labeled 192 examples. The results for each dataset are presented \nin Table 4. Both the Amazon and Reddit dataset have more examples requiring world knowl-\nedge to resolve lexical variation, while the New York Times dataset has more examples requiring \nmulti-sentence reasoning. Differences in reasoning required between test sets do not explain the \nobserved performance drops. In Appendix C.6, we present a another model that predicts F1 \nscores on our new test sets by computing model F1 scores in each reasoning category and then \n\n\nTable 4 :\n4Manual comparison of the reasoning required to answer each question-answer pair on a random sample of 192 examples from each dataset using the categories fromRajpurkar et al. (2016). The Reddit and Amazon datasets have more examples requiring world knowledge to resolve lexical variation, whereas the New York Times and Amazon datasets require more multisentence reasoning. We show in Appendix C.6 that these differences in reasoning required do not explain the performance drops we observe.Reasoning Type \nSQuAD v1.1 New Wiki NYT Reddit Amazon \n\nLexical Variation (Synonomy) \n39.1 \n39.1 \n31.8 \n35.9 \n36.5 \nLexical Variation (World Knowledge) \n8.3 \n4.7 \n9.9 \n20.3 \n18.8 \nSyntactic Variation \n62.5 \n53.6 \n50.5 \n53.1 \n46.4 \nMultiple Sentence Reasoning \n8.9 \n8.3 \n16.7 \n12.0 \n16.7 \nAmbiguous \n1.6 \n3.6 \n1.6 \n1.6 \n1.0 \n\n75 \n85 \n95 \nOriginal Test F1 \n\n75 \n\n85 \n\n95 \n\nNew Test Set F1 \n\nNew Wikipedia \n\n75 \n85 \n95 \nOriginal Test F1 \n\nNew York Times \n\n75 \n85 \n95 \nOriginal Test F1 \n\nReddit \n\n75 \n85 \n95 \nOriginal Test F1 \n\nAmazon \n\ny = x \nSQuAD Model Linear Fit \nMRQA Model F1 \nHuman F1 \nBest SQuAD Model \n\n\n\n\nCynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Leon Roth. Preserving statistical validity in adaptive data analysis. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 117-126, 2015. Vitaly Feldman, Roy Frostig, and Moritz Hardt. The advantages of multiple classes for reducing overfitting from test set reuse. arXiv preprint arXiv:1905.10360, 2019. Yichen Gong and Samuel Bowman. Ruminating reader: Reasoning with gated multi-hop attention. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 1-11, 2018. Kenton Lee, Shimi Salant, Tom Kwiatkowski, Ankur Parikh, Dipanjan Das, and Jonathan Berant. Learning recurrent span representations for extractive question answering. arXiv preprint arXiv:1611.01436, 2016. Reham Osama, Nagwa El-Makky, and Marwan Torki. Question answering using hierarchical attention on top of BERT features. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, November 2019. Exact Match Scatterplots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.2 Linear Fits in the Probit Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.3 Does Annotator Agreement Correlate with Performance Drops? . . . . . . . . . . 28 C.4 Do Shifts in Answer Category Distributions Predict Performance Drops? . . . . . 28 C.5 Do Shifts in Syntactic Divergence Predict Performance Drops? . . . . . . . . . . 29 C.6 Do Shifts in Reasoning Required Distributions Predict Performance Drops? . . . 30 C.7 Does Manual Data Curation Reduce Performance Drops? . . . . . . . . . . . . . 31 D Dataset collection details. 32 D.1 Passage Length Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 D.2 MTurk Experiment and UI Examples . . . . . . . . . . . . . . . . . . . . . . . . . 37 E Complete Model Testbed and Results Tables 42 E.1 Models Evaluated . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 E.2 Full Results Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA \n2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the \n2nd Workshop on Machine Reading for Question Answering, pages 1-13, Hong Kong, China, \nNovember 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5801. \n\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F Liu, \nMatthew Peters, Michael Schmitz, and Luke Zettlemoyer. Allennlp: A deep semantic natural \nlanguage processing platform. In Proceedings of Workshop for NLP Open Source Software \n(NLP-OSS), pages 1-6, 2018. \n\nMatt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, \nPradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. Evaluating nlp \nmodels via contrast sets. arXiv preprint arXiv:2004.02709, 2020. \n\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn \nSong. Pretrained transformers improve out-of-distribution robustness. arXiv preprint \narXiv:2004.06100, 2020. \n\nMatthew Honnibal and Ines Montani. spaCy 2: Natural language understanding with Bloom \nembeddings, convolutional neural networks and incremental parsing. To appear, 2017. \n\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. Reinforced \nmnemonic reader for machine reading comprehension. In Proceedings of the 27th International \nJoint Conference on Artificial Intelligence, pages 4099-4106, 2018. \n\nHsin-Yuan Huang, Chenguang Zhu, Yelong Shen, and Weizhu Chen. Fusionnet: Fusing via fully-\naware attention with application to machine comprehension. arXiv preprint arXiv:1711.07341, \n2017. \n\nRobin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. \nIn Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, \npages 2021-2031, 2017. \n\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale \ndistantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th \nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), \npages 1601-1611, 2017. \n\nMandar Joshi, Eunsol Choi, Omer Levy, Daniel S Weld, and Luke Zettlemoyer. pair2vec: \nCompositional word-pair embeddings for cross-sentence inference. In Proceedings of the 2019 \nConference of the North American Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 3597-3608, 2019. \n\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. \nSpanbert: Improving pre-training by representing and predicting spans. Transactions of the \nAssociation for Computational Linguistics, 8:64-77, 2020. \n\nDivyansh Kaushik, Eduard Hovy, and Zachary C Lipton. Learning the difference that makes a \ndifference with counterfactually-augmented data. arXiv preprint arXiv:1909.12434, 2019. \n\nNikita Kitaev and Dan Klein. Constituency parsing with a self-attentive encoder. In Proceedings \nof the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long \nPapers), Melbourne, Australia, July 2018. Association for Computational Linguistics. \n\nLingpeng Kong, Cyprien de Masson d'Autume, Wang Ling, Lei Yu, Zihang Dai, and Dani Yo-\ngatama. A mutual information maximization perspective of language representation learning. \narXiv preprint arXiv:1910.08350, 2019. \n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris \nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural ques-\ntions: a benchmark for question answering research. Transactions of the Association for \nComputational Linguistics, 7:453-466, 2019. \n\nSeanie Lee, Donggyu Kim, and Jangwon Park. Domain-agnostic question-answering with ad-\nversarial training. arXiv preprint arXiv:1910.09342, 2019. \n\nRui Liu, Wei Wei, Weiguang Mao, and Maria Chikina. Phase conductor on multi-layered atten-\ntions for machine comprehension. arXiv preprint arXiv:1710.10504, 2017. \n\nShayne Longpre, Yi Lu, Zhucheng Tu, and Chris DuBois. An exploration of data augmentation \nand sampling techniques for domain-agnostic question answering. In Proceedings of the 2nd \nWorkshop on Machine Reading for Question Answering, pages 220-227, Hong Kong, China, \nNovember 2019. Association for Computational Linguistics. \n\nHoria Mania, John Miller, Ludwig Schmidt, Moritz Hardt, and Benjamin Recht. Model similarity \nmitigates test set overuse. In Advances in Neural Information Processing Systems, pages 9993-\n10002, 2019. \n\nJulian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based \nrecommendations on styles and substitutes. In Proceedings of the 38th International ACM \nSIGIR Conference on Research and Development in Information Retrieval, pages 43-52, 2015. \n\nContents \n\nA Evaluation Metrics \n21 \n\nB Comparing Natural and Adversarial Distribution Shift \n22 \n\nC Additional Analysis and Results \n24 \nC.1 \n\n\nFigure 8: Comparison of F1 and EM scores on the original SQuAD test set versus the adversarial AddSent attack from Jia and Liang (2017). The models exhibit substantially more variability around the linear trend line compared to natural distribution shifts. For F1 scores, the slope of the linear fit is 1.51, for EM scores, the slope is 1.33. Similarly, the R 2 statistics are 0.73 and 0.74, respectively.60 \n70 \n80 \n90 \n100 \n\nOriginal Test F1 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nAddSent F1 \n\nAddSent F1 Scores \n\n50 \n60 \n70 \n80 \n90 \n100 \n\nOriginal Test EM \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nAddSent EM \n\nAddSent EM Scores \n\ny=x \nLinear Fit \n\n\n\n\nFigure 10: Comparison of F1 and EM scores on the original SQuAD test set versus the adversarial AddOneSent attack from Jia and Liang (2017). We observe similar phenomenon as with AddSent. Model performance broadly follows a linear trend, with more variability around the trend line than with our natural distribution test sets. For F1 scores, the slope of the linear fit is 1.48, and for EM, the slopes is 1.34. The R 2 statistics are 0.79 and 0.80, respectively.50 \n\n60 \n70 \n80 \n90 \n100 \n\nOriginal Test F1 \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nAddOneSent F1 \n\nAddOneSent F1 Scores \n\n50 \n60 \n70 \n80 \n90 \n100 \n\nOriginal Test EM \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nAddOneSent EM \n\nAddOneSent EM Scores \n\ny=x \nLinear Fit \n\n\n\n\nFigure 11: Model and human EM scores on the original SQuAD test set compared to our new test sets (shown with 95% Clopper-Pearson confidence intervals). The slopes of the linear fits are 0.92, 0.95, 1.05, and 1.18, respectively. The R 2 statistics are 0.99, 0.83, 0.82, and 0.85, respectively.50 60 70 80 90 100 \n\nSQuAD Test EM \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nNew Test Set EM \n\nNew Wikipedia \n\n40 50 60 70 80 90 100 \n\nSQuAD Test EM \n\nNew York Times \n\n40 50 60 70 80 90 100 \n\nSQuAD Test EM \n\nReddit \n\n40 50 60 70 80 90 100 \n\nSQuAD Test EM \n\nAmazon \n\ny = x \nLinear Fit \nModel EM \nHuman EM \n\n\n\n\nFigures 12 and Figures 13 show similar probit models for each of our new datasets.Figure 12: Comparison between linear and probit axis scaling for model and human F1 scores on the original SQuAD test and each of our new test sets. For linear axis scaling, the slopes of the linear fit are 0.92, 1.02, 1.19, and 1.36, respectively, and the R Figure 13: Comparison between linear and probit axis scaling for model and human EM scores on the original SQuAD test and each of our new test sets. Under linear axis scaling, the slopes of the linear fit are 0.92, 0.95, 1.05, and 1.18, respectively. The R Figure 14: Model and human F1 scores on the original SQuAD v1.1 test set compared to our new test sets, stratified by the agreement between the answers given by the labellers, e.g. if three labellers agree, then three labellers provided identical (up to text normalization) answers to the question. Each point corresponds to a model evaluation.Figure 15: Changes in answer type distributions introduced in Section 6 explain little of the observed performance differences across our new datasets. For each model, we compute the F1 score on each of the answer types on the SQuAD v1.1 dev set, and then we predict the F1 score on the new test set by reweighing these F1 scores based on the frequency of answer types in the new test set. Concretely, if SQuAD v1.1 was 50% NP answers and 50% Places answers, and a model has average F1 scores of 100 for NP and 75 for Places, then if a new dataset had 30% NP answers and 70% Places answers, the predicted F1 score would be 82.5 (versus 87.5 for the original). The y = x line represents the trivial model that predicts the same F1 score on the new test sets as the original. For each of the distribution shift datasets, predictions based on answer category shifts are exceedingly optimistic and explain little of the observed drops. For instance, on the Reddit dataset, answer category shifts suggest models would lose, on average, 2-3 F1 points. However, the average observed shift is 14.0 F1 points.Figure 16: Changes in syntactic distributions introduced in Section 6 explain only a small amount of the observed performance differences across our new datasets. As in the previous plot, for each model, we compute the F1 score for each observed value of syntactic divergence on the SQuAD v1.1 dev set, and then we predict the F1 score on the new test set by reweighing these F1 scores based on the frequency of examples with a given syntactic divergence in the new test set. For each of the distribution shift datasets, predictions based on answer category shifts are optimistic. For instance, on the Reddit dataset, syntactic divergence shifts suggest models would lose, on average, 1.9 F1 points, while the average observed shift is 14.0 F1 points.Figure 17: Changes in reasoning required distributions introduced in Section 6 explain little of the observed performance differences across our new datasets. Similar to the previous plot, for each model, we compute the F1 score on each of the reasoning required categories on the SQuAD v1.1 dev set, and then we predict the F1 score on the new test set by reweighing these F1 scores based on the reasoning required distribution of the new test set. For each of the distribution shift datasets, predictions based on reasoning required shifts closely follow the y = x line corresponding to the trivial model that predicts the same F1 score on the new test sets as the original.50 60 70 80 90 100 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nNew Test Set F1 \n\nNew Wikipedia \nLinear Fit \n\n50 60 70 80 90 100 \n\nNew York Times \nLinear Fit \n\n50 60 70 80 90 100 \n\nReddit \nLinear Fit \n\n50 60 70 80 90 100 \n\nAmazon \nLinear Fit \n\n50 60 70 80 90 95 \n\nSQuAD Test F1 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n95 \n\nNew Test Set F1 \n\nProbit Fit \n\n50 60 70 80 90 95 \n\nSQuAD Test F1 \n\nProbit Fit \n\n50 60 70 80 90 95 \n\nSQuAD Test F1 \n\nProbit Fit \n\n50 60 70 80 90 95 \n\nSQuAD Test F1 \n\nProbit Fit \n\ny = x \nLinear Fit \nModel F1 \nHuman F1 \n\n2 statistics are 0.99, 0.97, 0.91, 0.89, \nrespectively. Under probit axis scaling, the slopes of the linear fit are 0.83, 0.89, 0.84, and 0.95, \nrespectively, and the R 2 statistics are 0.99, 0.96, 0.94, 0.94, respectively. \n40 50 60 70 80 90 100 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nNew Test Set EM \n\nNew Wikipedia \nLinear Fit \n\n40 50 60 70 80 90 100 \n\nNew York Times \nLinear Fit \n\n40 50 60 70 80 90 100 \n\nReddit \nLinear Fit \n\n40 50 60 70 80 90 100 \n\nAmazon \nLinear Fit \n\n30 40 50 60 70 80 90 \n\nSQuAD Test EM \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\nNew Test Set EM \n\nProbit Fit \n\n30 40 50 60 70 80 90 \n\nSQuAD Test EM \n\nProbit Fit \n\n30 40 50 60 70 80 90 \n\nSQuAD Test EM \n\nProbit Fit \n\n30 40 50 60 70 80 90 \n\nSQuAD Test EM \n\nProbit Fit \n\ny = x \nLinear Fit \nModel EM \nHuman EM \n\n2 statistics are 0.99, 0.83, 0.82, \nand 0.85, respectively. Under probit scaling, the slopes of the linear fit are 0.82, 0.85, 0.83, and \n0.94, respectively. The R 2 statistics are 0.99, 0.82, 0.83, and 0.88, respectively. \nC.3 Does Annotator Agreement Correlate with Performance Drops? \n\n50 \n60 \n70 \n80 \n90 \n100 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nNew Test Set F1 \n\nNew Wikipedia \n\n50 \n60 \n70 \n80 \n90 \n100 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nNew York Times \n\n50 \n60 \n70 \n80 \n90 \n100 \n\nOriginal Wikipedia F1 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nNew Test Set F1 \n\nReddit \n\n50 \n60 \n70 \n80 \n90 \n100 \n\nOriginal Wikipedia F1 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nAmazon \n\ny = x \n1 Labelers Agree \n2 Labelers Agree \n3 Labelers Agree \n\nLabel agreement roughly corresponds \nto question difficulty (and ambiguity). For clear and simple questions, all of the labellers typically \nagree. For more subtle or potentially ambiguous questions, the labeller's answers are more varied \nand tend to disagree more often. Across each dataset, when the questions are easier or less \nambiguous (as measured by higher labeller agreement), the models experience proportionally \nsmaller drops on the new dataset. \n\nC.4 Do Shifts in Answer Category Distributions Predict Performance Drops? \n50 \n\n60 \n70 \n80 \n90 \n100 \n\nSQuAD v1.1 Dev F1 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nActual Test F1 \n\nActual F1 Scores \n\ny = x \n\nNew Wikipedia \nNew York Times \nReddit \nAmazon \n\n50 \n60 \n70 \n80 \n90 \n100 \n\nSQuAD v1.1 Dev F1 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nPredicted F1 \n\nPredicted F1 Based on Answer Categorization \n\ny = x \n\nNew Wikipedia \nNew York Times \nReddit \nAmazon \n\nC.5 Do Shifts in Syntactic Divergence Predict Performance Drops? \n50 \n\n60 \n70 \n80 \n90 \n100 \n\nSQuAD v1.1 Dev F1 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nActual Test F1 \n\nActual F1 Scores \n\ny = x \n\nNew Wikipedia \nNew York Times \nReddit \nAmazon \n\n50 \n60 \n70 \n80 \n90 \n100 \n\nSQuAD v1.1 Dev F1 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nPredicted F1 \n\nPredicted F1 Based on Syntactic Divergence \n\ny = x \n\nNew Wikipedia \nNew York Times \nReddit \nAmazon \n\nC.6 Do Shifts in Reasoning Required Distributions Predict Performance \nDrops? \n50 \n\n60 \n70 \n80 \n90 \n100 \n\nSQuAD v1.1 Dev F1 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nActual Test F1 \n\nActual F1 Scores \n\ny = x \n\nNew Wikipedia \nNew York Times \nReddit \nAmazon \n\n50 \n60 \n70 \n80 \n90 \n100 \n\nSQuAD v1.1 Dev F1 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nPredicted Test F1 \n\nPredicted F1 Based on Reasoning Required \n\ny = x \n\nNew Wikipedia \nNew York Times \nReddit \nAmazon \n\nC.7 Does Manual Data Curation Reduce Performance Drops? \n70 \n\n80 \n90 \n100 \n\nOriginal F1 Score \n\n70 \n\n80 \n\n90 \n\n100 \n\nF1 Score on Human Filtered Subset \n\nNew-Wiki \n\n50 \n60 \n70 \n80 \n90 \n100 \n\n\n\n\nChanges in the distribution of words per paragraph across our new test sets do not explain the differences in F1 scores we observe. Concretely, we stratify the datasets by words per paragraph, and, for each model, we compute the F1 score for each bucket on the SQuAD v1.1 development set. We then predict F1 scores on the new test set by reweighing these F1 scores based on the paragraph length distribution of the new test set.SQuAD v1.1 Dev F1 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nActual Test F1 \n\nActual F1 Scores \n\ny = x \n\nNew Wikipedia \nNew York Times \nReddit \nAmazon \n\n50 \n60 \n70 \n80 \n90 \n100 \n\nSQuAD v1.1 Dev F1 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nPredicted F1 \n\nPredicted F1 Based on Number of Words in Paragraph \n\ny = x \n\nNew Wikipedia \nNew York Times \nReddit \nAmazon \n\nFigure 22: 50 \n60 \n70 \n80 \n90 \n100 \n\nSQuAD v1.1 Dev F1 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nActual Test F1 \n\nActual F1 Scores \n\ny = x \n\nNew Wikipedia \nNew York Times \nReddit \nAmazon \n\n50 \n60 \n70 \n80 \n90 \n100 \n\nSQuAD v1.1 Dev F1 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n100 \n\nPredicted F1 \n\nPredicted F1 Based on Number of Sentences in Paragraph \n\ny = x \n\nNew Wikipedia \nNew York Times \nReddit \nAmazon \n\nFigure 23: Changes in the distribution of sentences per paragraph across our new test sets do \nnot explain the differences in F1 scores we observe. As in the previous plot, we stratify the \ndatasets by sentence per paragraph, compute the F1 score for each bucket on the SQuAD v1.1 \ndevelopment set, and then predict F1 scores on the new test set by reweighing these F1 scores \nbased on the paragraph length distribution of the new test set. \n\n\n1 .\n1Delphi (Longpre et al., 2019) https://worksheets.codalab.org/bundles/0x9a53e9c50f1244699c4a24aee483bd4c2. HierAtt (Osama et al., 2019) \nhttps://worksheets.codalab.org/bundles/0x8d851db3255b485c97646c5c0ba812a2 \n\n3. Bert-Large+Adv Train (Lee et al., 2019) \nhttps://worksheets.codalab.org/bundles/0xa113983bc3fc42ff89bf3838a6177a0c \n\n4. BERT-cased-whole-word \nhttps://worksheets.codalab.org/bundles/0x456676760aae452cb44ade00bb515b64 \n\n5. BERT-Multi-Finetune \nhttps://worksheets.codalab.org/bundles/0x5716df3b477a452a997bcebb9e179c89 \n\n\n\nTable 5 :\n5Comparison of model F1 scores on the original SQuAD test set and our new Wikipedia test set. Rank refers to the relative ordering using the original SQuAD v1.1 F1 scores, new rank refers to the ordering using the new test set scores, and \u2206 rank is the relative difference in ranking. The confidence intervals are 95% Student's t-intervals. Unless noted, all models are single models and only the first 30 characters of each name is shown.New Wikipedia F1 Score Summary \n\nRank Name \nSQuAD \nNew Wikipedia Gap \nNew \nRank \n\u2206 Rank \n\n-\nHuman-0 \n94.9 [93.8, 96.0] 92.5 [91.1, 93.8] \n2.4 \n-\n-\n-\nHuman-1 \n94.9 [93.8, 96.0] 92.4 [91.0, 93.8] \n2.5 \n-\n-\n-\nHuman-2 \n95.6 [94.5, 96.6] 92.3 [90.8, 93.8] \n3.2 \n-\n-\n1 \nXLNet (single model) \n95.1 \n92.3 [91.9, 92.8] \n2.7 \n1 \n0 \n2 \nXLNET-123 (single model) \n94.9 \n92.2 [91.7, 92.7] \n2.7 \n5 \n-3 \n3 \nXLNET-123++ (single model) \n94.9 \n92.3 [91.8, 92.7] \n2.6 \n3 \n0 \n4 \nDelphi \n94.7 \n92.2 [91.7, 92.7] \n2.5 \n4 \n0 \n5 \nSpanBERT (single model) \n94.6 \n92.3 [91.8, 92.8] \n2.3 \n2 \n3 \n6 \nBERT+WWM+MT \n94.4 \n91.8 [91.3, 92.3] \n2.6 \n6 \n0 \n7 \nBERT-cased-whole-word \n93.4 \n91.5 [91.0, 92.0] \n1.9 \n7 \n0 \n8 \nTuned BERT-1seq Large Cased (s \n93.3 \n91.0 [90.5, 91.5] \n2.3 \n9 \n-1 \n9 \nInfoWord (large) \n93.1 \n91.0 [90.5, 91.6] \n2.1 \n8 \n1 \n10 \nBERT-Large Baseline (single mo \n92.7 \n90.8 [90.3, 91.3] \n1.9 \n11 \n-1 \n11 \nBERT+MT (single model) \n92.6 \n90.4 [89.9, 90.9] \n2.3 \n13 \n-2 \n12 \nTuned BERT Large Cased (single \n92.6 \n90.6 [90.1, 91.1] \n2.0 \n12 \n0 \n13 \nDPN (single model) \n92.0 \n89.7 [89.2, 90.3] \n2.3 \n14 \n-1 \n14 \nST bl (single model) \n92.0 \n89.6 [89.0, 90.1] \n2.4 \n16 \n-2 \n15 \nBERT-uncased (single model) \n91.9 \n89.4 [88.8, 89.9] \n2.6 \n20 \n-5 \n16 \nEL-BERT (single model) \n91.8 \n89.6 [89.0, 90.1] \n2.2 \n17 \n-1 \n17 \nBISAN (single model) \n91.8 \n89.4 [88.9, 90.0] \n2.3 \n18 \n-1 \n18 \nBERT+Sparse-Transformer(single \n91.6 \n89.4 [88.9, 90.0] \n2.2 \n19 \n-1 \n19 \nInfoWord (base) \n91.4 \n89.2 [88.6, 89.8] \n2.2 \n23 \n-4 \n20 \nInfoWord-Base (single model) \n91.4 \n89.2 [88.6, 89.8] \n2.1 \n22 \n-2 \n21 \nInfoWord BERT baseline (large) \n91.3 \n90.8 [90.3, 91.3] \n0.5 \n10 \n11 \n22 \nOriginal BERT Large Cased \n91.3 \n89.6 [89.1, 90.2] \n1.6 \n15 \n7 \n23 \nCommon-sense Governed BERT-123 \n91.1 \n89.3 [88.7, 89.8] \n1.8 \n21 \n2 \n24 \nInfoWord BERT baseline (base) \n90.9 \n88.7 [88.1, 89.2] \n2.2 \n24 \n0 \n25 \nCommon-sense Governed BERT-123 \n90.6 \n88.1 [87.5, 88.7] \n2.5 \n25 \n0 \n26 \nMARS (ensemble, June 20 2018) \n89.8 \n88.0 [87.4, 88.6] \n1.8 \n26 \n0 \n27 \nMARS (single model) \n89.5 \n85.6 [85.0, 86.3] \n3.9 \n39 \n-12 \n28 \nMARS (single model, June 21 20 \n89.2 \n87.0 [86.4, 87.7] \n2.2 \n27 \n1 \n29 \nMMIPN (single model) \n88.9 \n87.0 [86.3, 87.6] \n2.0 \n28 \n1 \n30 \nMARS (single model, May 9 2018 \n88.9 \n86.8 [86.1, 87.4] \n2.1 \n30 \n0 \nContinued on next page \n\n\nTable 6 :\n6Comparison of model F1 scores on the original SQuAD test set and our New York Times test set . Rank refers to the relative ordering using the original SQuAD v1.1 F1 scores, new rank refers to the ordering using the new test set scores, and \u2206 rank is the relative difference in ranking. The confidence intervals are 95% Student's t-intervals. Unless noted, all models are single models and only the first 30 characters of each name is shown.NYT F1 Score Summary \n\nRank Name \nSQuAD \nNYT \nGap \nNew \nRank \n\u2206 Rank \n\n-\nHuman-0 \n94.9 [93.8, 96.0] 95.0 [93.9, 96.1] -0.1 \n-\n-\n-\nHuman-1 \n94.9 [93.8, 96.0] 96.3 [95.4, 97.1] -1.4 \n-\n-\n-\nHuman-2 \n95.6 [94.5, 96.6] 93.7 [92.4, 95.0] \n1.9 \n-\n-\n1 \nXLNet (single model) \n95.1 \n84.4 [84.0, 84.9] 10.7 \n34 \n-33 \n2 \nXLNET-123 (single model) \n94.9 \n92.8 [92.3, 93.2] \n2.2 \n3 \n-1 \n3 \nXLNET-123++ (single model) \n94.9 \n92.9 [92.5, 93.3] \n2.0 \n2 \n1 \n4 \nDelphi \n94.7 \n93.4 [93.0, 93.8] \n1.3 \n1 \n3 \n5 \nSpanBERT (single model) \n94.6 \n92.4 [92.0, 92.8] \n2.2 \n4 \n1 \n6 \nBERT+WWM+MT \n94.4 \n89.4 [88.9, 89.9] \n5.0 \n11 \n-5 \n7 \nBERT-cased-whole-word \n93.4 \n91.7 [91.3, 92.2] \n1.7 \n5 \n2 \n8 \nTuned BERT-1seq Large Cased (s \n93.3 \n90.8 [90.3, 91.3] \n2.5 \n7 \n1 \n9 \nInfoWord (large) \n93.1 \n91.1 [90.7, 91.6] \n2.0 \n6 \n3 \n10 \nBERT-Large Baseline (single mo \n92.7 \n90.6 [90.1, 91.1] \n2.1 \n8 \n2 \n11 \nBERT+MT (single model) \n92.6 \n88.3 [87.7, 88.8] \n4.4 \n22 \n-11 \n12 \nTuned BERT Large Cased (single \n92.6 \n90.5 [90.0, 91.0] \n2.1 \n10 \n2 \n13 \nDPN (single model) \n92.0 \n88.8 [88.3, 89.4] \n3.2 \n14 \n-1 \nContinued on next page \n\n\nTable 7 :\n7Comparison of model F1 scores on the original SQuAD test set and our Reddit test set. Rank refers to the relative ordering using the original SQuAD v1.1 F1 scores, new rank refers to the ordering using the new test set scores, and \u2206 rank is the relative difference in ranking. The confidence intervals are 95% Student's t-intervals. Unless noted, all models are single models and only the first 30 characters of each name is shown.Reddit F1 Score Summary \n\nRank Name \nSQuAD \nReddit \nGap \nNew \nRank \n\u2206 Rank \n\n-\nHuman-0 \n94.9 [93.8, 96.0] 92.4 [91.1, 93.7] \n2.5 \n-\n-\n-\nHuman-1 \n94.9 [93.8, 96.0] 92.6 [91.3, 93.9] \n2.3 \n-\n-\n-\nHuman-2 \n95.6 [94.5, 96.6] 91.7 [90.2, 93.2] \n3.8 \n-\n-\n1 \nXLNet (single model) \n95.1 \n79.0 [78.5, 79.6] 16.0 \n21 \n-20 \n2 \nXLNET-123 (single model) \n94.9 \n84.9 [84.2, 85.5] 10.1 \n3 \n-1 \n3 \nXLNET-123++ (single model) \n94.9 \n84.8 [84.2, 85.4] 10.1 \n4 \n-1 \n4 \nDelphi \n94.7 \n88.0 [87.5, 88.6] \n6.7 \n1 \n3 \n5 \nSpanBERT (single model) \n94.6 \n85.4 [84.9, 86.0] \n9.2 \n2 \n3 \n6 \nBERT+WWM+MT \n94.4 \n83.0 [82.3, 83.6] 11.4 \n6 \n0 \n7 \nBERT-cased-whole-word \n93.4 \n84.0 [83.4, 84.7] \n9.4 \n5 \n2 \n8 \nTuned BERT-1seq Large Cased (s \n93.3 \n82.2 [81.5, 82.9] 11.1 \n8 \n0 \n9 \nInfoWord (large) \n93.1 \n82.5 [81.8, 83.1] 10.6 \n7 \n2 \n10 \nBERT-Large Baseline (single mo \n92.7 \n81.2 [80.6, 81.9] 11.5 \n11 \n-1 \n11 \nBERT+MT (single model) \n92.6 \n81.9 [81.3, 82.6] 10.7 \n9 \n2 \n12 \nTuned BERT Large Cased (single \n92.6 \n81.5 [80.9, 82.2] 11.1 \n10 \n2 \n13 \nDPN (single model) \n92.0 \n80.7 [80.0, 81.4] 11.3 \n17 \n-4 \n14 \nST bl (single model) \n92.0 \n80.9 [80.2, 81.6] 11.1 \n14 \n0 \n15 \nBERT-uncased (single model) \n91.9 \n80.1 [79.5, 80.8] 11.8 \n19 \n-4 \n16 \nEL-BERT (single model) \n91.8 \n78.2 [77.5, 78.9] 13.6 \n24 \n-8 \n17 \nBISAN (single model) \n91.8 \n80.3 [79.6, 81.0] 11.5 \n18 \n-1 \n18 \nBERT+Sparse-Transformer(single \n91.6 \n81.1 [80.4, 81.8] 10.5 \n13 \n5 \n19 \nInfoWord (base) \n91.4 \n78.5 [77.8, 79.2] 12.9 \n22 \n-3 \n20 \nInfoWord-Base (single model) \n91.4 \n78.5 [77.8, 79.2] 12.9 \n23 \n-3 \n21 \nInfoWord BERT baseline (large) \n91.3 \n81.2 [80.6, 81.9] 10.1 \n12 \n9 \n22 \nOriginal BERT Large Cased \n91.3 \n80.7 [80.1, 81.4] 10.5 \n16 \n6 \n23 \nCommon-sense Governed BERT-123 \n91.1 \n80.8 [80.2, 81.5] 10.2 \n15 \n8 \n24 \nInfoWord BERT baseline (base) \n90.9 \n78.1 [77.4, 78.8] 12.8 \n25 \n-1 \n25 \nCommon-sense Governed BERT-123 \n90.6 \n80.0 [79.3, 80.6] 10.7 \n20 \n5 \n26 \nMARS (ensemble, June 20 2018) \n89.8 \n77.9 [77.2, 78.7] 11.9 \n26 \n0 \n27 \nMARS (single model) \n89.5 \n73.5 [72.8, 74.3] 16.0 \n43 \n-16 \n28 \nMARS (single model, June 21 20 \n89.2 \n76.2 [75.4, 76.9] 13.1 \n31 \n-3 \n29 \nMMIPN (single model) \n88.9 \n76.6 [75.8, 77.3] 12.4 \n30 \n-1 \n30 \nMARS (single model, May 9 2018 \n88.9 \n75.5 [74.8, 76.3] 13.3 \n33 \n-3 \n31 \nBert-Large+Adv. Train \n88.6 \n76.8 [76.0, 77.5] 11.8 \n29 \n2 \n32 \nReinforced Mnemonic Reader (en \n88.5 \n74.2 [73.4, 75.0] 14.3 \n40 \n-8 \n33 \nAttentionReader+ (ensemble) \n88.2 \n75.5 [74.8, 76.3] 12.6 \n32 \n1 \n34 \nReinforced Mnemonic Reader + A \n88.1 \n73.2 [72.4, 73.9] 15.0 \n44 \n-10 \n35 \nReinforced Mnemonic Reader + A \n88.1 \n73.6 [72.8, 74.3] 14.6 \n42 \n-7 \n36 \nBERT-COMPOUND-DSS (single mode \n88.0 \n75.4 [74.7, 76.2] 12.6 \n34 \n2 \n37 \nBERT-COMPOUND (single model) \n87.8 \n74.8 [74.0, 75.5] 13.0 \n36 \n1 \n38 \nHierAtt \n87.8 \n77.4 [76.7, 78.1] 10.3 \n28 \n10 \n39 \nBERT-Multi-Finetune \n87.7 \n77.7 [77.0, 78.4] 10.0 \n27 \n12 \n40 \nBiDAF + Self Attention + ELMo \n87.4 \n74.5 [73.8, 75.3] 12.9 \n39 \n1 \n41 \nAVIQA+ (ensemble) (aviqa team) \n87.3 \n74.7 [73.9, 75.5] 12.6 \n37 \n4 \n42 \nEAZI (ensemble) \n86.9 \n74.2 [73.4, 75.0] 12.7 \n41 \n1 \nContinued on next page \n\n\nTable 8 :\n8Comparison of model F1 scores on the original SQuAD test set and our Amazon test \nset. Rank refers to the relative ordering using the original SQuAD v1.1 F1 scores, new rank \nrefers to the ordering using the new test set scores, and \u2206 rank is the relative difference in \nranking. The confidence intervals are 95% Student's t-intervals. Unless noted, all models are \nsingle models and only the first 30 characters of each name is shown. \n\nAmazon F1 Score Summary \n\nRank Name \nSQuAD \nAmazon \nGap \nNew \nRank \n\u2206 Rank \n\n-\nHuman-0 \n94.9 [93.8, 96.0] 92.6 [91.3, 93.9] \n2.3 \n-\n-\n-\nHuman-1 \n94.9 [93.8, 96.0] 92.4 [91.1, 93.7] \n2.5 \n-\n-\n-\nHuman-2 \n95.6 [94.5, 96.6] 91.2 [89.6, 92.8] \n4.4 \n-\n-\n1 \nXLNet (single model) \n95.1 \n81.7 [81.1, 82.2] 13.4 \n7 \n-6 \n2 \nXLNET-123 (single model) \n94.9 \n85.7 [85.1, 86.3] \n9.2 \n3 \n-1 \n3 \nXLNET-123++ (single model) \n94.9 \n87.2 [86.7, 87.7] \n7.7 \n2 \n1 \n4 \nDelphi \n94.7 \n87.7 [87.2, 88.3] \n6.9 \n1 \n3 \n5 \nSpanBERT (single model) \n94.6 \n84.8 [84.2, 85.3] \n9.9 \n4 \n1 \n6 \nBERT+WWM+MT \n94.4 \n81.6 [81.0, 82.3] 12.8 \n8 \n-2 \n7 \nBERT-cased-whole-word \n93.4 \n82.9 [82.2, 83.5] 10.6 \n5 \n2 \n8 \nTuned BERT-1seq Large Cased (s \n93.3 \n82.5 [81.9, 83.2] 10.8 \n6 \n2 \n9 \nInfoWord (large) \n93.1 \n81.5 [80.8, 82.1] 11.6 \n9 \n0 \n10 \nBERT-Large Baseline (single mo \n92.7 \n80.8 [80.2, 81.5] 11.9 \n10 \n0 \n11 \nBERT+MT (single model) \n92.6 \n80.2 [79.5, 80.8] 12.5 \n13 \n-2 \n12 \nTuned BERT Large Cased (single \n92.6 \n80.3 [79.6, 81.0] 12.3 \n12 \n0 \n13 \nDPN (single model) \n92.0 \n79.3 [78.6, 80.0] 12.7 \n18 \n-5 \n14 \nST bl (single model) \n92.0 \n79.6 [79.0, 80.3] 12.3 \n16 \n-2 \n15 \nBERT-uncased (single model) \n91.9 \n79.9 [79.3, 80.6] 12.0 \n15 \n0 \n16 \nEL-BERT (single model) \n91.8 \n77.2 [76.4, 77.9] 14.7 \n24 \n-8 \n17 \nBISAN (single model) \n91.8 \n79.2 [78.5, 79.9] 12.6 \n19 \n-2 \n18 \nBERT+Sparse-Transformer(single \n91.6 \n80.0 [79.3, 80.6] 11.6 \n14 \n4 \n19 \nInfoWord (base) \n91.4 \n78.0 [77.3, 78.7] 13.4 \n23 \n-4 \n20 \nInfoWord-Base (single model) \n91.4 \n78.0 [77.3, 78.7] 13.3 \n22 \n-2 \n21 \nInfoWord BERT baseline (large) \n91.3 \n80.8 [80.2, 81.5] 10.5 \n11 \n10 \n22 \nOriginal BERT Large Cased \n91.3 \n79.4 [78.7, 80.0] 11.9 \n17 \n5 \n23 \nCommon-sense Governed BERT-123 \n91.1 \n79.0 [78.3, 79.7] 12.1 \n20 \n3 \n24 \nInfoWord BERT baseline (base) \n90.9 \n77.1 [76.4, 77.8] 13.8 \n25 \n-1 \n25 \nCommon-sense Governed BERT-123 \n90.6 \n78.5 [77.8, 79.2] 12.1 \n21 \n4 \n26 \nMARS (ensemble, June 20 2018) \n89.8 \n73.5 [72.8, 74.3] 16.2 \n32 \n-6 \n27 \nMARS (single model) \n89.5 \n68.6 [67.8, 69.4] 20.9 \n54 \n-27 \n28 \nMARS (single model, June 21 20 \n89.2 \n72.0 [71.2, 72.7] 17.3 \n34 \n-6 \n29 \nMMIPN (single model) \n88.9 \n75.0 [74.3, 75.7] 14.0 \n29 \n0 \n30 \nMARS (single model, May 9 2018 \n88.9 \n71.4 [70.6, 72.1] 17.5 \n37 \n-7 \n31 \nBert-Large+Adv. Train \n88.6 \n76.0 [75.3, 76.7] 12.6 \n26 \n5 \n32 \nReinforced Mnemonic Reader (en \n88.5 \n70.1 [69.3, 70.9] 18.4 \n44 \n-12 \n33 \nAttentionReader+ (ensemble) \n88.2 \n71.3 [70.5, 72.1] 16.8 \n38 \n-5 \nContinued on next page \n\n\nTable 9 :\n9Comparison of model EM scores on the original SQuAD test set and our new Wikipedia test set. Rank refers to the relative ordering using the original SQuAD v1.1 EM scores, new rank refers to the ordering using the new test set scores, and \u2206 rank is the relative difference in ranking. The confidence intervals are 95% Clopper-Pearson intervals. Unless noted, all models are single models and only the first 30 characters of each name is shown.New Wiki EM Score Summary \n\nRank Name \nSQuAD \nNew Wiki \nGap \nNew \nRank \n\u2206 Rank \n\n-\nHuman-0 \n89.1 [87.1, 91.0] 82.6 [80.0, 85.0] \n6.6 \n-\n-\n-\nHuman-1 \n88.7 [86.6, 90.6] 83.2 [80.6, 85.6] \n5.5 \n-\n-\n-\nHuman-2 \n90.5 [88.5, 92.2] 85.4 [82.9, 87.6] \n5.1 \n-\n-\n1 \nXLNet (single model) \n89.9 \n84.2 [83.3, 85.0] \n5.7 \n1 \n0 \n2 \nXLNET-123++ (single model) \n89.9 \n83.4 [82.6, 84.2] \n6.4 \n5 \n-3 \n3 \nXLNET-123 (single model) \n89.6 \n83.5 [82.7, 84.3] \n6.1 \n3 \n0 \n4 \nDelphi \n89.6 \n84.0 [83.1, 84.8] \n5.6 \n2 \n2 \n5 \nSpanBERT (single model) \n88.8 \n83.5 [82.7, 84.3] \n5.3 \n4 \n1 \n6 \nBERT+WWM+MT \n88.7 \n83.2 [82.3, 84.0] \n5.5 \n6 \n0 \n7 \nTuned BERT-1seq Large Cased (s \n87.5 \n82.1 [81.3, 83.0] \n5.3 \n9 \n-2 \n8 \nInfoWord (large) \n87.3 \n82.2 [81.4, 83.0] \n5.1 \n8 \n0 \nContinued on next page \n\n\nTable 10 :\n10Comparison of model EM scores on the original SQuAD test set and our New York Times test set. Rank refers to the relative ordering using the original SQuAD v1.1 EM scores, new rank refers to the ordering using the new test set scores, and \u2206 rank is the relative difference in ranking. The confidence intervals are 95% Clopper-Pearson intervals. Unless noted, all models are single models and only the first 30 characters of each name is shown.NYT EM Score SummaryNYT EM Score Summary \n\nRank Name \nSQuAD \nNYT \nGap \nNew \nRank \n\u2206 Rank \n\n-\nHuman-0 \n89.1 [87.1, 91.0] 86.0 [83.6, 88.1] \n3.2 \n-\n-\n-\nHuman-1 \n88.7 [86.6, 90.6] 88.5 [86.3, 90.5] \n0.2 \n-\n-\n-\nHuman-2 \n90.5 [88.5, 92.2] 85.8 [83.4, 87.9] \n4.7 \n-\n-\n1 \nXLNet (single model) \n89.9 \n50.9 [50.0, 51.9] 38.9 \n102 \n-101 \n2 \nXLNET-123++ (single model) \n89.9 \n85.9 [85.2, 86.6] \n3.9 \n3 \n-1 \n3 \nXLNET-123 (single model) \n89.6 \n86.0 [85.3, 86.7] \n3.7 \n2 \n1 \n4 \nDelphi \n89.6 \n86.9 [86.3, 87.6] \n2.7 \n1 \n3 \n5 \nSpanBERT (single model) \n88.8 \n85.3 [84.6, 86.0] \n3.5 \n4 \n1 \n6 \nBERT+WWM+MT \n88.7 \n79.4 [78.6, 80.2] \n9.2 \n21 \n-15 \n7 \nTuned BERT-1seq Large Cased (s \n87.5 \n83.5 [82.8, 84.2] \n4.0 \n7 \n0 \n8 \nInfoWord (large) \n87.3 \n83.8 [83.1, 84.5] \n3.5 \n5 \n3 \n9 \nBERT-cased-whole-word \n87.1 \n83.8 [83.1, 84.5] \n3.3 \n6 \n3 \n10 \nBERT-Large Baseline (single mo \n86.6 \n83.0 [82.2, 83.7] \n3.7 \n9 \n1 \n11 \nTuned BERT Large Cased (single \n86.5 \n82.8 [82.1, 83.6] \n3.7 \n10 \n1 \n12 \nBERT+MT (single model) \n86.5 \n78.6 [77.8, 79.4] \n7.8 \n25 \n-13 \n13 \nST bl (single model) \n85.4 \n80.5 [79.7, 81.3] \n4.9 \n13 \n0 \n14 \nEL-BERT (single model) \n85.3 \n80.3 [79.5, 81.0] \n5.1 \n16 \n-2 \n15 \nBISAN (single model) \n85.3 \n80.1 [79.3, 80.8] \n5.3 \n19 \n-4 \n16 \nBERT+Sparse-Transformer(single \n85.1 \n79.7 [78.9, 80.5] \n5.4 \n20 \n-4 \n17 \nDPN (single model) \n85.0 \n80.2 [79.4, 80.9] \n4.8 \n18 \n-1 \n18 \nBERT-uncased (single model) \n84.9 \n80.4 [79.6, 81.1] \n4.5 \n15 \n3 \n19 \nInfoWord-Base (single model) \n84.7 \n80.6 [79.8, 81.4] \n4.1 \n11 \n8 \n20 \nInfoWord (base) \n84.7 \n80.6 [79.8, 81.4] \n4.1 \n12 \n8 \n21 \nInfoWord BERT baseline (base) \n84.4 \n80.5 [79.7, 81.3] \n3.9 \n14 \n7 \n22 \nOriginal BERT Large Cased \n84.3 \n80.2 [79.4, 81.0] \n4.1 \n17 \n5 \n23 \nInfoWord BERT baseline (large) \n84.3 \n83.0 [82.2, 83.7] \n1.3 \n8 \n15 \n24 \nMARS (ensemble, June 20 2018) \n84.0 \n78.7 [77.9, 79.5] \n5.3 \n24 \n0 \n25 \nCommon-sense Governed BERT-123 \n83.9 \n79.1 [78.3, 79.9] \n4.8 \n22 \n3 \n26 \nMARS (single model) \n83.2 \n74.5 [73.6, 75.3] \n8.7 \n42 \n-16 \n27 \nMARS (single model, June 21 20 \n83.1 \n77.1 [76.3, 77.9] \n6.0 \n27 \n0 \n28 \nCommon-sense Governed BERT-123 \n82.9 \n78.7 [77.9, 79.5] \n4.2 \n23 \n5 \n29 \nMARS (single model, May 9 2018 \n82.6 \n76.8 [75.9, 77.6] \n5.8 \n29 \n0 \n30 \nReinforced Mnemonic Reader (en \n82.3 \n75.9 [75.1, 76.7] \n6.4 \n32 \n-2 \n31 \nAttentionReader+ (ensemble) \n81.8 \n76.4 [75.5, 77.2] \n5.4 \n30 \n1 \n32 \nMMIPN (single model) \n81.6 \n76.9 [76.1, 77.7] \n4.7 \n28 \n4 \n33 \nReinforced Mnemonic Reader + A \n81.5 \n75.5 [74.6, 76.3] \n6.0 \n37 \n-4 \n34 \nReinforced Mnemonic Reader + A \n81.4 \n74.9 [74.0, 75.7] \n6.5 \n40 \n-6 \n35 \nBERT-COMPOUND-DSS (single mode \n81.0 \n76.3 [75.4, 77.1] \n4.8 \n31 \n4 \n36 \nBiDAF + Self Attention + ELMo \n81.0 \n75.9 [75.0, 76.7] \n5.1 \n33 \n3 \n37 \nBERT-COMPOUND (single model) \n80.7 \n75.8 [75.0, 76.7] \n4.9 \n34 \n3 \n38 \nAVIQA+ (ensemble) (aviqa team) \n80.6 \n75.3 [74.5, 76.2] \n5.3 \n39 \n-1 \n39 \nEAZI (ensemble) \n80.4 \n75.8 [74.9, 76.6] \n4.6 \n35 \n4 \n40 \nMEMEN+ (Ensemble) \n80.4 \n72.4 [71.5, 73.2] \n8.0 \n53 \n-13 \n41 \nDNET (ensemble) \n80.2 \n75.4 [74.5, 76.2] \n4.8 \n38 \n3 \n42 \nBert-Large+Adv. Train \n80.1 \n74.5 [73.6, 75.3] \n5.6 \n41 \n1 \nContinued on next page \n\n\nTable 11 :\n11Comparison of model EM scores on the original SQuAD test set and our Reddit test set. Rank refers to the relative ordering using the original SQuAD v1.1 EM scores, new rank refers to the ordering using the new test set scores, and \u2206 rank is the relative difference in ranking. The confidence intervals are 95% Clopper-Pearson intervals. Unless noted, all models are single models and only the first 30 characters of each name is shown.Reddit EM Score SummaryReddit EM Score Summary \n\nRank Name \nSQuAD \nReddit \nGap \nNew \nRank \n\u2206 Rank \n\n-\nHuman-0 \n89.1 [87.1, 91.0] 80.1 [77.2, 82.7] \n9.1 \n-\n-\n-\nHuman-1 \n88.7 [86.6, 90.6] 80.7 [77.8, 83.3] \n8.0 \n-\n-\n-\nHuman-2 \n90.5 [88.5, 92.2] 81.0 [78.2, 83.6] \n9.4 \n-\n-\n1 \nXLNet (single model) \n89.9 \n43.1 [42.1, 44.1] 46.8 \n89 \n-88 \n2 \nXLNET-123++ (single model) \n89.9 \n70.8 [69.9, 71.7] 19.0 \n5 \n-3 \n3 \nXLNET-123 (single model) \n89.6 \n73.8 [72.9, 74.7] 15.8 \n2 \n1 \n4 \nDelphi \n89.6 \n77.9 [77.1, 78.8] 11.7 \n1 \n3 \n5 \nSpanBERT (single model) \n88.8 \n73.3 [72.4, 74.2] 15.6 \n3 \n2 \n6 \nBERT+WWM+MT \n88.7 \n69.2 [68.3, 70.1] 19.4 \n9 \n-3 \n7 \nTuned BERT-1seq Large Cased (s \n87.5 \n70.8 [69.9, 71.7] 16.7 \n6 \n1 \n8 \nInfoWord (large) \n87.3 \n70.5 [69.6, 71.4] 16.8 \n7 \n1 \n9 \nBERT-cased-whole-word \n87.1 \n72.0 [71.1, 72.9] 15.1 \n4 \n5 \n10 \nBERT-Large Baseline (single mo \n86.6 \n68.9 [68.0, 69.8] 17.7 \n10 \n0 \n11 \nTuned BERT Large Cased (single \n86.5 \n69.5 [68.6, 70.4] 17.0 \n8 \n3 \n12 \nBERT+MT (single model) \n86.5 \n68.4 [67.4, 69.3] 18.1 \n14 \n-2 \n13 \nST bl (single model) \n85.4 \n68.7 [67.7, 69.6] 16.8 \n13 \n0 \n14 \nEL-BERT (single model) \n85.3 \n65.6 [64.6, 66.5] 19.8 \n24 \n-10 \n15 \nBISAN (single model) \n85.3 \n67.7 [66.8, 68.7] 17.6 \n16 \n-1 \n16 \nBERT+Sparse-Transformer(single \n85.1 \n68.9 [67.9, 69.8] 16.2 \n12 \n4 \n17 \nDPN (single model) \n85.0 \n67.2 [66.3, 68.2] 17.8 \n20 \n-3 \n18 \nBERT-uncased (single model) \n84.9 \n67.2 [66.3, 68.2] 17.7 \n19 \n-1 \n19 \nInfoWord-Base (single model) \n84.7 \n65.8 [64.9, 66.8] 18.9 \n23 \n-4 \n20 \nInfoWord (base) \n84.7 \n65.8 [64.9, 66.8] 18.9 \n22 \n-2 \n21 \nInfoWord BERT baseline (base) \n84.4 \n65.4 [64.4, 66.3] 19.1 \n25 \n-4 \n22 \nOriginal BERT Large Cased \n84.3 \n68.3 [67.3, 69.2] 16.1 \n15 \n7 \n23 \nInfoWord BERT baseline (large) \n84.3 \n68.9 [68.0, 69.8] 15.4 \n11 \n12 \n24 \nMARS (ensemble, June 20 2018) \n84.0 \n66.0 [65.0, 66.9] 18.0 \n21 \n3 \n25 \nCommon-sense Governed BERT-123 \n83.9 \n67.7 [66.7, 68.6] 16.2 \n17 \n8 \nContinued on next page \n\n\nTable 13 :\n13Model F1 scores for against the adversarial distribution shifts AddSent.AddSent F1 Score SummaryAddSent F1 Score Summary \n\nRank Name \nSQuAD AddSent Gap \nNew \nRank \n\u2206 Rank \n\n1 \nXLNet (single model) \n95.1 \n76.5 \n18.6 \n2 \n-1 \n2 \nXLNET-123 (single model) \n94.9 \n68.9 \n26.0 \n8 \n-6 \n3 \nXLNET-123++ (single model) \n94.9 \n77.2 \n17.7 \n1 \n2 \n4 \nSpanBERT (single model) \n94.6 \n71.5 \n23.2 \n4 \n0 \n5 \nBERT+WWM+MT \n94.4 \n73.9 \n20.5 \n3 \n2 \n6 \nTuned BERT-1seq Large Cased (s \n93.3 \n71.2 \n22.1 \n5 \n1 \n7 \nInfoWord (large) \n93.1 \n64.4 \n28.7 \n12 \n-5 \n8 \nBERT-Large Baseline (single mo \n92.7 \n61.3 \n31.5 \n16 \n-8 \n9 \nBERT+MT (single model) \n92.6 \n70.0 \n22.6 \n6 \n3 \n10 \nTuned BERT Large Cased (single \n92.6 \n65.3 \n27.3 \n9 \n1 \n11 \nDPN (single model) \n92.0 \n65.1 \n26.9 \n10 \n1 \n12 \nST bl (single model) \n92.0 \n60.2 \n31.8 \n22 \n-10 \n13 \nBERT-uncased (single model) \n91.9 \n59.7 \n32.2 \n26 \n-13 \n14 \nEL-BERT (single model) \n91.8 \n61.1 \n30.7 \n18 \n-4 \n15 \nBISAN (single model) \n91.8 \n60.1 \n31.6 \n23 \n-8 \n16 \nBERT+Sparse-Transformer(single \n91.6 \n60.1 \n31.5 \n24 \n-8 \n17 \nInfoWord (base) \n91.4 \n61.0 \n30.4 \n19 \n-2 \n18 \nInfoWord-Base (single model) \n91.4 \n61.0 \n30.3 \n20 \n-2 \n19 \nInfoWord BERT baseline (large) \n91.3 \n61.3 \n30.0 \n17 \n2 \n20 \nOriginal BERT Large Cased \n91.3 \n64.4 \n26.8 \n13 \n7 \n21 \nCommon-sense Governed BERT-123 \n91.1 \n64.0 \n27.1 \n14 \n7 \n22 \nInfoWord BERT baseline (base) \n90.9 \n56.5 \n34.4 \n30 \n-8 \n23 \nCommon-sense Governed BERT-123 \n90.6 \n69.4 \n21.2 \n7 \n16 \n24 \nMARS (ensemble, June 20 2018) \n89.8 \n59.9 \n29.8 \n25 \n-1 \n25 \nMARS (single model) \n89.5 \n57.2 \n32.3 \n29 \n-4 \n26 \nMARS (single model, June 21 20 \n89.2 \n58.4 \n30.8 \n28 \n-2 \n27 \nMMIPN (single model) \n88.9 \n50.4 \n38.6 \n37 \n-10 \n28 \nReinforced Mnemonic Reader (en \n88.5 \n60.2 \n28.3 \n21 \n7 \n29 \nAttentionReader+ (ensemble) \n88.2 \n50.7 \n37.5 \n35 \n-6 \n30 \nReinforced Mnemonic Reader + A \n88.1 \n61.3 \n26.9 \n15 \n15 \n31 \nReinforced Mnemonic Reader + A \n88.1 \n64.8 \n23.3 \n11 \n20 \n32 \nBERT-COMPOUND (single model) \n87.8 \n50.5 \n37.3 \n36 \n-4 \n33 \nBiDAF + Self Attention + ELMo \n87.4 \n43.7 \n43.7 \n56 \n-23 \n34 \nAVIQA+ (ensemble) (aviqa team) \n87.3 \n45.3 \n42.1 \n51 \n-17 \n35 \nEAZI (ensemble) \n86.9 \n42.7 \n44.2 \n63 \n-28 \n36 \nMEMEN+ (Ensemble) \n86.8 \n44.5 \n42.4 \n53 \n-17 \n37 \nDNET (ensemble) \n86.7 \n49.8 \n36.9 \n39 \n-2 \n38 \nBERT-INDEPENDENT (single model \n86.7 \n50.1 \n36.5 \n38 \n0 \n39 \nReinforced Mnemonic Reader (si \n86.7 \n58.5 \n28.2 \n27 \n12 \n40 \nMDReader (single model) \n86.0 \n45.7 \n40.3 \n48 \n-8 \n41 \nBiDAF + Self Attention + ELMo \n85.9 \n44.3 \n41.6 \n55 \n-14 \n42 \nBiDAF + Self-Attention + ELMo \n85.8 \n44.4 \n41.4 \n54 \n-12 \n43 \nMDReader0 (single model) \n85.5 \n43.4 \n42.2 \n57 \n-14 \n44 \nMEMEN+ (Single) \n85.5 \n42.8 \n42.7 \n62 \n-18 \n45 \nMEMEN (single model) \n85.3 \n43.4 \n42.0 \n58 \n-13 \n46 \nInteractive AoA Reader (Ensemb \n85.3 \n39.7 \n45.6 \n69 \n-23 \n47 \nEAZI (single model) \n85.1 \n43.2 \n42.0 \n60 \n-13 \n48 \nAttentionReader+ (single) \n84.9 \n53.9 \n31.0 \n31 \n17 \n49 \nDNET (single model) \n84.9 \n49.2 \n35.7 \n40 \n9 \n50 \nMARS (single model, Jan 23) \n84.7 \n52.4 \n32.4 \n33 \n17 \nContinued on next page \n\n\nTable 14 :\n14Model F1 scores for against the adversarial distribution shifts AddOneSent.AddOneSent F1 Score SummaryAddOneSent F1 Score Summary \n\nRank Name \nSQuAD AddOneSent Gap \nNew \nRank \n\u2206 Rank \n\n1 \nXLNet (single model) \n95.1 \n83.5 \n11.6 \n2 \n-1 \n2 \nXLNET-123 (single model) \n94.9 \n78.4 \n16.5 \n7 \n-5 \n3 \nXLNET-123++ (single model) \n94.9 \n84.5 \n10.4 \n1 \n2 \n4 \nSpanBERT (single model) \n94.6 \n79.8 \n14.9 \n4 \n0 \n5 \nBERT+WWM+MT \n94.4 \n80.8 \n13.6 \n3 \n2 \n6 \nTuned BERT-1seq Large Cased (s \n93.3 \n78.6 \n14.7 \n6 \n0 \n7 \nInfoWord (large) \n93.1 \n72.7 \n20.4 \n12 \n-5 \n8 \nBERT-Large Baseline (single mo \n92.7 \n71.1 \n21.6 \n15 \n-7 \n9 \nBERT+MT (single model) \n92.6 \n77.1 \n15.5 \n8 \n1 \n10 \nTuned BERT Large Cased (single \n92.6 \n73.8 \n18.8 \n10 \n0 \n11 \nDPN (single model) \n92.0 \n73.6 \n18.5 \n11 \n0 \n12 \nST bl (single model) \n92.0 \n69.8 \n22.2 \n21 \n-9 \n13 \nBERT-uncased (single model) \n91.9 \n69.5 \n22.4 \n23 \n-10 \n14 \nEL-BERT (single model) \n91.8 \n70.5 \n21.3 \n17 \n-3 \n15 \nBISAN (single model) \n91.8 \n70.5 \n21.3 \n18 \n-3 \n16 \nBERT+Sparse-Transformer(single \n91.6 \n69.4 \n22.3 \n24 \n-8 \n17 \nInfoWord (base) \n91.4 \n70.3 \n21.1 \n19 \n-2 \n18 \nInfoWord-Base (single model) \n91.4 \n70.3 \n21.0 \n20 \n-2 \n19 \nInfoWord BERT baseline (large) \n91.3 \n71.1 \n20.2 \n16 \n3 \n20 \nOriginal BERT Large Cased \n91.3 \n72.5 \n18.7 \n13 \n7 \n21 \nCommon-sense Governed BERT-123 \n91.1 \n74.4 \n16.7 \n9 \n12 \n22 \nInfoWord BERT baseline (base) \n90.9 \n67.0 \n23.9 \n28 \n-6 \n23 \nCommon-sense Governed BERT-123 \n90.6 \n78.8 \n11.8 \n5 \n18 \n24 \nMARS (ensemble, June 20 2018) \n89.8 \n68.7 \n21.1 \n25 \n-1 \n25 \nMARS (single model) \n89.5 \n65.6 \n24.0 \n30 \n-5 \n26 \nMARS (single model, June 21 20 \n89.2 \n67.5 \n21.8 \n27 \n-1 \n27 \nMMIPN (single model) \n88.9 \n60.6 \n28.4 \n36 \n-9 \n28 \nReinforced Mnemonic Reader (en \n88.5 \n68.3 \n20.2 \n26 \n2 \n29 \nAttentionReader+ (ensemble) \n88.2 \n61.4 \n26.7 \n34 \n-5 \n30 \nReinforced Mnemonic Reader + A \n88.1 \n69.5 \n18.6 \n22 \n8 \n31 \nReinforced Mnemonic Reader + A \n88.1 \n71.4 \n16.7 \n14 \n17 \n32 \nBERT-COMPOUND (single model) \n87.8 \n61.4 \n26.3 \n35 \n-3 \n33 \nBiDAF + Self Attention + ELMo \n87.4 \n54.8 \n32.6 \n54 \n-21 \n34 \nAVIQA+ (ensemble) (aviqa team) \n87.3 \n56.1 \n31.2 \n48 \n-14 \n35 \nEAZI (ensemble) \n86.9 \n54.4 \n32.5 \n57 \n-22 \n36 \nMEMEN+ (Ensemble) \n86.8 \n55.0 \n31.8 \n53 \n-17 \n37 \nDNET (ensemble) \n86.7 \n60.5 \n26.2 \n38 \n-1 \n38 \nBERT-INDEPENDENT (single model \n86.7 \n61.6 \n25.0 \n32 \n6 \n39 \nReinforced Mnemonic Reader (si \n86.7 \n66.8 \n19.9 \n29 \n10 \n40 \nMDReader (single model) \n86.0 \n57.0 \n29.0 \n45 \n-5 \n41 \nBiDAF + Self Attention + ELMo \n85.9 \n54.7 \n31.2 \n56 \n-15 \n42 \nBiDAF + Self-Attention + ELMo \n85.8 \n54.7 \n31.2 \n55 \n-13 \n43 \nMDReader0 (single model) \n85.5 \n54.0 \n31.6 \n60 \n-17 \n44 \nMEMEN+ (Single) \n85.5 \n53.8 \n31.7 \n61 \n-17 \n45 \nMEMEN (single model) \n85.3 \n54.3 \n31.0 \n58 \n-13 \n46 \nInteractive AoA Reader (Ensemb \n85.3 \n49.5 \n35.8 \n71 \n-25 \n47 \nEAZI (single model) \n85.1 \n53.6 \n31.5 \n62 \n-15 \n48 \nAttentionReader+ (single) \n84.9 \n63.3 \n21.7 \n31 \n17 \n49 \nDNET (single model) \n84.9 \n59.4 \n25.5 \n40 \n9 \n50 \nMARS (single model, Jan 23) \n84.7 \n60.5 \n24.2 \n37 \n13 \nContinued on next page \n\n\nTable 15 :\n15Model EM scores for against the adversarial distribution shifts AddSent.AddSent EM Score Summary \n\nRank Name \nSQuAD AddSent Gap \nNew \nRank \n\u2206 Rank \n\n1 \nXLNet (single model) \n89.9 \n71.2 \n18.7 \n2 \n-1 \n2 \nXLNET-123++ (single model) \n89.9 \n73.3 \n16.6 \n1 \n1 \n3 \nXLNET-123 (single model) \n89.6 \n63.7 \n25.9 \n8 \n-5 \n4 \nSpanBERT (single model) \n88.8 \n66.3 \n22.5 \n5 \n-1 \n5 \nBERT+WWM+MT \n88.7 \n69.4 \n19.2 \n3 \n2 \n6 \nTuned BERT-1seq Large Cased (s \n87.5 \n65.8 \n21.7 \n6 \n0 \n7 \nInfoWord (large) \n87.3 \n59.6 \n27.7 \n10 \n-3 \n8 \nBERT-Large Baseline (single mo \n86.6 \n56.2 \n30.4 \n16 \n-8 \n9 \nTuned BERT Large Cased (single \n86.5 \n60.4 \n26.1 \n9 \n0 \n10 \nBERT+MT (single model) \n86.5 \n64.4 \n22.1 \n7 \n3 \n11 \nST bl (single model) \n85.4 \n55.0 \n30.4 \n23 \n-12 \n12 \nEL-BERT (single model) \n85.3 \n56.3 \n29.0 \n15 \n-3 \n13 \nBISAN (single model) \n85.3 \n54.8 \n30.5 \n24 \n-11 \n14 \nBERT+Sparse-Transformer(single \n85.1 \n54.4 \n30.7 \n25 \n-11 \n15 \nDPN (single model) \n85.0 \n58.9 \n26.1 \n13 \n2 \n16 \nBERT-uncased (single model) \n84.9 \n54.0 \n30.9 \n26 \n-10 \n17 \nInfoWord-Base (single model) \n84.7 \n55.7 \n29.0 \n19 \n-2 \n18 \nInfoWord (base) \n84.7 \n55.7 \n29.0 \n20 \n-2 \n19 \nInfoWord BERT baseline (base) \n84.4 \n51.4 \n33.0 \n30 \n-11 \n20 \nOriginal BERT Large Cased \n84.3 \n58.8 \n25.5 \n14 \n6 \n21 \nInfoWord BERT baseline (large) \n84.3 \n56.2 \n28.1 \n17 \n4 \n22 \nMARS (ensemble, June 20 2018) \n84.0 \n55.3 \n28.7 \n22 \n0 \n23 \nCommon-sense Governed BERT-123 \n83.9 \n66.6 \n17.3 \n4 \n19 \n24 \nMARS (single model) \n83.2 \n51.8 \n31.4 \n29 \n-5 \n25 \nMARS (single model, June 21 20 \n83.1 \n53.2 \n29.9 \n27 \n-2 \n26 \nCommon-sense Governed BERT-123 \n82.9 \n59.3 \n23.6 \n12 \n14 \n27 \nReinforced Mnemonic Reader (en \n82.3 \n55.6 \n26.7 \n21 \n6 \n28 \nAttentionReader+ (ensemble) \n81.8 \n45.2 \n36.6 \n35 \n-7 \n29 \nMMIPN (single model) \n81.6 \n44.4 \n37.2 \n38 \n-9 \n30 \nReinforced Mnemonic Reader + A \n81.5 \n56.0 \n25.5 \n18 \n12 \n31 \nReinforced Mnemonic Reader + A \n81.4 \n59.5 \n21.9 \n11 \n20 \n32 \nBiDAF + Self Attention + ELMo \n81.0 \n38.7 \n42.3 \n54 \n-22 \n33 \nBERT-COMPOUND (single model) \n80.7 \n44.9 \n35.8 \n36 \n-3 \n34 \nAVIQA+ (ensemble) (aviqa team) \n80.6 \n40.7 \n39.9 \n47 \n-13 \n35 \nEAZI (ensemble) \n80.4 \n38.1 \n42.3 \n58 \n-23 \n36 \nMEMEN+ (Ensemble) \n80.4 \n39.5 \n40.9 \n51 \n-15 \n37 \nDNET (ensemble) \n80.2 \n44.6 \n35.6 \n37 \n0 \n38 \nReinforced Mnemonic Reader (si \n79.5 \n53.0 \n26.5 \n28 \n10 \n39 \nMDReader (single model) \n79.0 \n41.1 \n37.9 \n45 \n-6 \n40 \nBERT-INDEPENDENT (single model \n78.7 \n44.1 \n34.6 \n40 \n0 \n41 \nBiDAF + Self Attention + ELMo \n78.6 \n38.6 \n40.0 \n57 \n-16 \n42 \nBiDAF + Self-Attention + ELMo \n78.6 \n38.7 \n39.9 \n55 \n-13 \n43 \nMEMEN (single model) \n78.2 \n38.0 \n40.2 \n59 \n-16 \n44 \nMEMEN+ (Single) \n78.2 \n37.5 \n40.7 \n61 \n-17 \n45 \nMDReader0 (single model) \n78.2 \n38.7 \n39.5 \n56 \n-11 \n46 \nEAZI (single model) \n78.0 \n37.7 \n40.3 \n60 \n-14 \n47 \nInteractive AoA Reader (Ensemb \n77.8 \n35.2 \n42.6 \n68 \n-21 \n48 \nDNET (single model) \n77.6 \n44.3 \n33.3 \n39 \n9 \n49 \nRaSoR + TR + LM (single model) \n77.6 \n42.2 \n35.4 \n43 \n6 \n50 \nAttentionReader+ (single) \n77.3 \n48.3 \n29.0 \n31 \n19 \nContinued on next page \n\n\nTable 16 :\n16Model EM scores for against the adversarial distribution shifts AddOneSent. EM Score SummaryAddOneSent Rank Name \nSQuAD AddOneSent Gap \nNew \nRank \n\u2206 Rank \n\n1 \nXLNet (single model) \n89.9 \n78.2 \n11.7 \n2 \n-1 \n2 \nXLNET-123++ (single model) \n89.9 \n80.3 \n9.6 \n1 \n1 \n3 \nXLNET-123 (single model) \n89.6 \n73.5 \n16.1 \n6 \n-3 \n4 \nSpanBERT (single model) \n88.8 \n74.3 \n14.5 \n5 \n-1 \n5 \nBERT+WWM+MT \n88.7 \n75.6 \n13.1 \n4 \n1 \n6 \nTuned BERT-1seq Large Cased (s \n87.5 \n73.1 \n14.4 \n7 \n-1 \n7 \nInfoWord (large) \n87.3 \n67.5 \n19.8 \n11 \n-4 \n8 \nBERT-Large Baseline (single mo \n86.6 \n65.4 \n21.2 \n15 \n-7 \n9 \nTuned BERT Large Cased (single \n86.5 \n68.6 \n17.9 \n9 \n0 \n10 \nBERT+MT (single model) \n86.5 \n70.6 \n15.9 \n8 \n2 \n11 \nST bl (single model) \n85.4 \n63.5 \n21.9 \n22 \n-11 \n12 \nEL-BERT (single model) \n85.3 \n64.7 \n20.6 \n17 \n-5 \n13 \nBISAN (single model) \n85.3 \n64.6 \n20.7 \n18 \n-5 \n14 \nBERT+Sparse-Transformer(single \n85.1 \n63.2 \n21.9 \n24 \n-10 \n15 \nDPN (single model) \n85.0 \n66.5 \n18.5 \n12 \n3 \n16 \nBERT-uncased (single model) \n84.9 \n62.8 \n22.1 \n25 \n-9 \n17 \nInfoWord-Base (single model) \n84.7 \n63.8 \n20.9 \n19 \n-2 \n18 \nInfoWord (base) \n84.7 \n63.8 \n20.9 \n20 \n-2 \n19 \nInfoWord BERT baseline (base) \n84.4 \n61.2 \n23.2 \n28 \n-9 \n20 \nOriginal BERT Large Cased \n84.3 \n65.9 \n18.4 \n13 \n7 \n21 \nInfoWord BERT baseline (large) \n84.3 \n65.4 \n18.9 \n16 \n5 \n22 \nMARS (ensemble, June 20 2018) \n84.0 \n63.5 \n20.5 \n23 \n-1 \n23 \nCommon-sense Governed BERT-123 \n83.9 \n76.0 \n7.9 \n3 \n20 \n24 \nMARS (single model) \n83.2 \n59.0 \n24.2 \n30 \n-6 \n25 \nMARS (single model, June 21 20 \n83.1 \n61.6 \n21.5 \n27 \n-2 \n26 \nCommon-sense Governed BERT-123 \n82.9 \n68.6 \n14.3 \n10 \n16 \n27 \nReinforced Mnemonic Reader (en \n82.3 \n62.8 \n19.5 \n26 \n1 \n28 \nAttentionReader+ (ensemble) \n81.8 \n55.2 \n26.6 \n33 \n-5 \n29 \nMMIPN (single model) \n81.6 \n53.6 \n28.0 \n37 \n-8 \n30 \nReinforced Mnemonic Reader + A \n81.5 \n63.6 \n17.9 \n21 \n9 \n31 \nReinforced Mnemonic Reader + A \n81.4 \n65.6 \n15.8 \n14 \n17 \n32 \nBiDAF + Self Attention + ELMo \n81.0 \n48.7 \n32.3 \n53 \n-21 \n33 \nBERT-COMPOUND (single model) \n80.7 \n55.5 \n25.2 \n32 \n1 \n34 \nAVIQA+ (ensemble) (aviqa team) \n80.6 \n50.7 \n29.9 \n46 \n-12 \n35 \nEAZI (ensemble) \n80.4 \n49.0 \n31.4 \n50 \n-15 \n36 \nMEMEN+ (Ensemble) \n80.4 \n49.3 \n31.1 \n48 \n-12 \n37 \nDNET (ensemble) \n80.2 \n54.9 \n25.3 \n34 \n3 \n38 \nReinforced Mnemonic Reader (si \n79.5 \n60.5 \n19.0 \n29 \n9 \n39 \nMDReader (single model) \n79.0 \n51.2 \n27.8 \n45 \n-6 \n40 \nBERT-INDEPENDENT (single model \n78.7 \n54.2 \n24.5 \n36 \n4 \n41 \nBiDAF + Self Attention + ELMo \n78.6 \n47.9 \n30.7 \n58 \n-17 \n42 \nBiDAF + Self-Attention + ELMo \n78.6 \n48.0 \n30.6 \n57 \n-15 \n43 \nMEMEN (single model) \n78.2 \n47.4 \n30.8 \n60 \n-17 \n44 \nMEMEN+ (Single) \n78.2 \n47.4 \n30.8 \n61 \n-17 \n45 \nMDReader0 (single model) \n78.2 \n48.1 \n30.1 \n56 \n-11 \n46 \nEAZI (single model) \n78.0 \n47.0 \n31.0 \n62 \n-16 \n47 \nInteractive AoA Reader (Ensemb \n77.8 \n43.4 \n34.4 \n70 \n-23 \n48 \nDNET (single model) \n77.6 \n53.3 \n24.3 \n40 \n8 \n49 \nRaSoR + TR + LM (single model) \n77.6 \n51.4 \n26.2 \n44 \n5 \n50 \nAttentionReader+ (single) \n77.3 \n56.7 \n20.6 \n31 \n19 \nContinued on next page \n\nhttps://modestyachts.github.io/squadshifts-website/\nhttps://worksheets.codalab.org/\nThe minimum 500 character per paragraph rule mentioned inRajpurkar et al. (2016) was adopted midway through their data collection, and hence the original dataset also includes shorter paragraphs(Rajpurkar, 2019).4 https://archive.org/web/\nThis large drop persists even when normalizing Unicode characers and replacing Unicode punctuation with Ascii approximations.\nAcknowledgmentsWe thank Pranav Rajpurkar, Robin Jia, and Percy Liang for providing us with the original SQuAD data generation pipeline and answering our many questions about the SQuAD dataset. We thank Nelson Liu for generously providing many of the SQuAD models we evaluated, substantially increasing the size of our testbed. We also thank the Codalab team for supporting our model evaluation efforts. This research was generously supported in part by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE 1752814 ABC, an Amazon AWS AI Research Award, and a gift from Microsoft Research.Adversarial Results. The model evaluation data used to construct the adversarial plots from Appendix B is summarized inTables 13 and 14 for F1 scores and Tables 15 and 16for EM scores.\nDzmitry Bahdanau, Tom Bosc, Edward Stanis Law Jastrzebski, Pascal Grefenstette, Yoshua Vincent, Bengio, arXiv:1706.00286Learning to compute word embeddings on the fly. arXiv preprintDzmitry Bahdanau, Tom Bosc, Stanis law Jastrzebski, Edward Grefenstette, Pascal Vincent, and Yoshua Bengio. Learning to compute word embeddings on the fly. arXiv preprint arXiv:1706.00286, 2017.\n\n. Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, Jeremy Blackburn, arXiv:2001.08435The pushshift reddit dataset. arXiv preprintJason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. The pushshift reddit dataset. arXiv preprint arXiv:2001.08435, 2020.\n\nModeling biological processes for reading comprehension. Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby Vander Linden, Brittany Harding, Brad Huang, Peter Clark, Christopher D Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby Vander Linden, Brittany Harding, Brad Huang, Peter Clark, and Christopher D Manning. Modeling biological processes for reading comprehension. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1499-1510, 2014.\n\nThe ladder: A reliable leaderboard for machine learning competitions. Avrim Blum, Moritz Hardt, International Conference on Machine Learning. Avrim Blum and Moritz Hardt. The ladder: A reliable leaderboard for machine learning com- petitions. In International Conference on Machine Learning, pages 1006-1014, 2015.\n\nSmarnet: Teaching machines to read and comprehend like human. Zheqian Chen, Rongqin Yang, Bin Cao, Zhou Zhao, Deng Cai, Xiaofei He, arXiv:1710.02772arXiv preprintZheqian Chen, Rongqin Yang, Bin Cao, Zhou Zhao, Deng Cai, and Xiaofei He. Smarnet: Teaching machines to read and comprehend like human. arXiv preprint arXiv:1710.02772, 2017.\n\nSimple and effective multi-paragraph reading comprehension. Christopher Clark, Matt Gardner, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading compre- hension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 845-855, 2018.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nMatthew Dunn, Levent Sagun, Mike Higgins, Volkan Ugur Guney, Kyunghyun Cirik, Cho, arXiv:1704.05179Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprintMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179, 2017.\n\nMemen: Multi-layer embedding with memory networks for machine comprehension. Boyuan Pan, Hao Li, Zhou Zhao, Bin Cao, Deng Cai, Xiaofei He, arXiv:1707.09098arXiv preprintBoyuan Pan, Hao Li, Zhou Zhao, Bin Cao, Deng Cai, and Xiaofei He. Memen: Multi-layer em- bedding with memory networks for machine comprehension. arXiv preprint arXiv:1707.09098, 2017.\n\nDeep contextualized word representations. E Matthew, Mark Peters, Mohit Neumann, Matt Iyyer, Christopher Gardner, Kenton Clark, Luke Lee, Zettlemoyer, Proceedings of NAACL-HLT. NAACL-HLTMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227-2237, 2018.\n\nPersonal Communication. Riyi Qiu, Riyi Qiu. Personal Communication, 2020.\n\n. Pranav Rajpurkar. Personal Communication. Pranav Rajpurkar. Personal Communication, 2019.\n\nSquad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ ques- tions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, 2016.\n\nKnow what you dont know: Unanswerable questions for squad. Pranav Rajpurkar, Robin Jia, Percy Liang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsShort Papers2Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 2: Short Papers), pages 784-789, 2018.\n\nDo imagenet classifiers generalize to imagenet. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, International Conference on Machine Learning. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet clas- sifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389-5400, 2019.\n\nSemantically equivalent adversarial rules for debugging nlp models. Sameer Marco Tulio Ribeiro, Carlos Singh, Guestrin, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Semantically equivalent adversarial rules for debugging nlp models. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 856-865, 2018.\n\nMctest: A challenge dataset for the open-domain machine comprehension of text. Matthew Richardson, J C Christopher, Erin Burges, Renshaw, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingMatthew Richardson, Christopher JC Burges, and Erin Renshaw. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 193-203, 2013.\n\nA meta-analysis of overfitting in machine learning. Rebecca Roelofs, Sara Fridovich-Keil, John Miller, Vaishaal Shankar, Moritz Hardt, Benjamin Recht, Ludwig Schmidt, Advances in Neural Information Processing Systems. Rebecca Roelofs, Sara Fridovich-Keil, John Miller, Vaishaal Shankar, Moritz Hardt, Benjamin Recht, and Ludwig Schmidt. A meta-analysis of overfitting in machine learning. In Advances in Neural Information Processing Systems, pages 9175-9185. 2019.\n\nContextualized word representations for reading comprehension. Shimi Salant, Jonathan Berant, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2Short PapersShimi Salant and Jonathan Berant. Contextualized word representations for reading compre- hension. In Proceedings of the 2018 Conference of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 554-559, 2018.\n\nWhat do models learn from question answering datasets?. Priyanka Sen, Amir Saffari, arXiv:2004.03490arXiv preprintPriyanka Sen and Amir Saffari. What do models learn from question answering datasets? arXiv preprint arXiv:2004.03490, 2020.\n\nBidirectional attention flow for machine comprehension. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi, arXiv:1611.01603arXiv preprintMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional atten- tion flow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016.\n\nReasonet: Learning to stop reading in machine comprehension. Yelong Shen, Po-Sen Huang, Jianfeng Gao, Weizhu Chen, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningYelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. Reasonet: Learning to stop reading in machine comprehension. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1047-1055, 2017.\n\nMultiqa: An empirical investigation of generalization and transfer in reading comprehension. Alon Talmor, Jonathan Berant, arXiv:1905.13453arXiv preprintAlon Talmor and Jonathan Berant. Multiqa: An empirical investigation of generalization and transfer in reading comprehension. arXiv preprint arXiv:1905.13453, 2019.\n\nNewsqa: A machine comprehension dataset. Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, Kaheer Suleman, Proceedings of the 2nd Workshop on Representation Learning for NLP. the 2nd Workshop on Representation Learning for NLPAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. Newsqa: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 191-200, 2017.\n\nMachine comprehension using match-lstm and answer pointer. Shuohang Wang, Jing Jiang, arXiv:1608.07905arXiv preprintShuohang Wang and Jing Jiang. Machine comprehension using match-lstm and answer pointer. arXiv preprint arXiv:1608.07905, 2016.\n\nDcn+: Mixed objective and deep residual coattention for question answering. Caiming Xiong, Victor Zhong, Richard Socher, arXiv:1711.00106arXiv preprintCaiming Xiong, Victor Zhong, and Richard Socher. Dcn+: Mixed objective and deep residual coattention for question answering. arXiv preprint arXiv:1711.00106, 2017.\n\nCold case: The lost mnist digits. Chhavi Yadav, L\u00e9on Bottou, Advances in Neural Information Processing Systems. Chhavi Yadav and L\u00e9on Bottou. Cold case: The lost mnist digits. In Advances in Neural Information Processing Systems, pages 13443-13452, 2019.\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, 2018.\n\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V Le, Xlnet, arXiv:1906.08237Generalized autoregressive pretraining for language understanding. arXiv preprintZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.\n\nLearning and evaluating general linguistic intelligence. Dani Yogatama, Jerome Cyprien De Masson D&apos;autume, Tomas Connor, Mike Kocisky, Lingpeng Chrzanowski, Angeliki Kong, Wang Lazaridou, Lei Ling, Chris Yu, Dyer, arXiv:1901.11373arXiv preprintDani Yogatama, Cyprien de Masson d'Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019.\n\nA multi-stage memory augmented neural network for machine reading comprehension. Seunghak Yu, Seohyun Sathish Reddy Indurthi, Haejun Back, Lee, Proceedings of the Workshop on Machine Reading for Question Answering. the Workshop on Machine Reading for Question AnsweringSeunghak Yu, Sathish Reddy Indurthi, Seohyun Back, and Haejun Lee. A multi-stage mem- ory augmented neural network for machine reading comprehension. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 21-30, 2018.\n\nEnd-to-end answer chunk extraction and ranking for reading comprehension. Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang, Bowen Zhou, arXiv:1610.09996arXiv preprintYang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang, and Bowen Zhou. End-to-end answer chunk extraction and ranking for reading comprehension. arXiv preprint arXiv:1610.09996, 2016.\n\nExploring question understanding and adaptation in neural-network-based question answering. Junbei Zhang, Xiaodan Zhu, Qian Chen, Lirong Dai, Si Wei, Hui Jiang, arXiv:1703.04617arXiv preprintJunbei Zhang, Xiaodan Zhu, Qian Chen, Lirong Dai, Si Wei, and Hui Jiang. Exploring question understanding and adaptation in neural-network-based question answering. arXiv preprint arXiv:1703.04617, 2017.\n\nRecord: Bridging the gap between human and machine commonsense reading comprehension. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme, arXiv:1810.12885arXiv preprintSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885, 2018.\n\nNatural analysts in adaptive data analysis. Tijana Zrnic, Moritz Hardt, International Conference on Machine Learning (ICML). Tijana Zrnic and Moritz Hardt. Natural analysts in adaptive data analysis. In International Conference on Machine Learning (ICML), 2019. https://arxiv.org/abs/1901.11143.\n\n. ; Xlnet, Yang, SingleXLNet (Single) (Yang et al., 2019) https://worksheets.codalab.org/bundles/0x74ebcd1a59044db49472900ae9936cf3\n\n. Xlnet-123, XLNet-123 (Single) https://worksheets.codalab.org/bundles/0x519d3e06a3544b0e85b7477ea512ec01\n\n. Xlnet-123++, XLNet-123++ (Single) https://worksheets.codalab.org/bundles/0x8a03e7cddcea47fa9395ca96870b62fd\n\n. ; Spanbert, Joshi, SingleSpanBERT (Single) (Joshi et al., 2020) https://worksheets.codalab.org/bundles/0xe7315e3e35c64097af5351bb2dbdf9a5\n\n. Bert + Wwm + Mt, BERT + WWM + MT (Single) https://worksheets.codalab.org/bundles/0x3975475041324f8c8b14626c932d09f4\n\n. Bert-1seq Large Tuned, ; Cased, Joshi, SingleTuned BERT-1seq Large Cased (Single) (Joshi et al., 2020) https://worksheets.codalab.org/bundles/0xa62618d05255460a83adfe1bfd1784f7\n\n. Kong, InfoWord Large (SingleInfoWord Large (Single) (Kong et al., 2019) https://worksheets.codalab.org/bundles/0x4a19b4d7c2fb40ef913bd97f611e66bd\n\n. Bert-Large Baseline, single modelBERT-Large Baseline (single model) https://worksheets.codalab.org/bundles/0xcd68d4f224b0425ab2b8b34ffb140a75\n\n. Bert + Mt, BERT + MT (Single) https://worksheets.codalab.org/bundles/0x8e20cbb02fa64883afdb4f8e50357858\n\n. Bert Large Tuned, ; Cased, Devlin, SingleTuned BERT Large Cased (Single) (Devlin et al., 2018; Joshi et al., 2020) https://worksheets.codalab.org/bundles/0x766e1c3149bd424fb154e31ee530845a\n\n. Dpn, DPN (Single) https://worksheets.codalab.org/bundles/0xd362627c900146178b5c190161bf61cf\n\n. BERT uncased (Single). BERT uncased (Single) https://worksheets.codalab.org/bundles/0x1bbff660e00c4445a3dc11277039edc3\n\n. El-Bert, EL-BERT (Single) https://worksheets.codalab.org/bundles/0x2d65a49640394cba8632f765b237a41f\n\n. BISAN. BISAN (single model) https://worksheets.codalab.org/bundles/0xfd43e046161f4ba89716d5d48b25ca2f\n\n. Bert + Sparse-Transformer, BERT + Sparse-Transformer (Single) https://worksheets.codalab.org/bundles/0xb1a4af82c1364cc1a41aef78543d0f52\n\n. Kong, InfoWord Base (SingleInfoWord Base (Single) (Kong et al., 2019) https://worksheets.codalab.org/bundles/0xd2067806f74c4da79e81b73eca08bcba\n\n. Infoword-Base, single modelInfoWord-Base (single model) https://worksheets.codalab.org/bundles/0xa41e1de495f84786a1c84d6f6036af0d\n\n. Bert Large Infoword, ; Baseline, Devlin, SingleInfoWord BERT Large Baseline (Single) (Devlin et al., 2018; Kong et al., 2019) https://worksheets.codalab.org/bundles/0x86fb7e7680b6488daa585dcd11e41a36\n\n. Bert Large Original, ; Cased, Devlin, SingleOriginal BERT Large Cased (Single) (Devlin et al., 2018; Joshi et al., 2020) https://worksheets.codalab.org/bundles/0x6603ef1196fd409d81948e3af7b44e58\n\nCommonsense Governed BERT-123 (Single. Commonsense Governed BERT-123 (Single; May 8th) https://worksheets.codalab.org/bundles/0x8eecf515978a4fd382e077efecbf90e1\n\n. Bert Base Infoword, ; Baseline, Devlin, SingleInfoWord BERT Base Baseline (Single) (Devlin et al., 2018; Kong et al., 2019) https://worksheets.codalab.org/bundles/0xb6d9adf28e4241e181602540aeafa5a0\n\nCommonsense Governed BERT-123 (Single. Commonsense Governed BERT-123 (Single; April 21st) https://worksheets.codalab.org/bundles/0x008044bbd7f74a7a81b51cbcfdf5a654\n\nMARS (Ensemble. MARS (Ensemble; June 20th) https://worksheets.codalab.org/bundles/0xb320588e9f424639b54f1f40de9b0cf9\n\n. MARS. SingleMARS (Single; September 1st) https://worksheets.codalab.org/bundles/0xfaf7cb0df0af4bf5a4050a53b81be174\n\nMARS (Single. MARS (Single; June 21st) https://worksheets.codalab.org/bundles/0xfc0c5b744d2a4c6b9f709c98bc2cf4e9\n\n. Mmipn, MMIPN (Single) https://worksheets.codalab.org/bundles/0xc2c7813ec5e241e2a0c43da45c7ecc91\n\nMARS (Single. MARS (Single; May 9th) https://worksheets.codalab.org/bundles/0x6d7c8a0f92374218ab4d419b397d67eb\n\n. Hu, Reinforced Mnemonic Reader (EnsembleReinforced Mnemonic Reader (Ensemble) (Hu et al., 2018) https://worksheets.codalab.org/bundles/0x0a5ea1308bad49b2bcdd37250bdf844a\n\n. Attentionreader+, AttentionReader+ (Ensemble) https://worksheets.codalab.org/bundles/0x50985a93bf734c40b76b8cc915fe967b\n\n. Reinforced Mnemonic Reader +. 2Reinforced Mnemonic Reader + A2D (Single) https://worksheets.codalab.org/bundles/0x6ada3ab4807442a4944de1e8ec1f5681\n\n. + A2d + Reinforced Mnemonic Reader, Da, Reinforced Mnemonic Reader + A2D + DA (Single) https://worksheets.codalab.org/bundles/0xeb52c2067dca498d852ca693eb9fd68a\n\n. Bert-Compound-Dss, BERT-Compound-DSS (Single) https://worksheets.codalab.org/bundles/0xd74488aac2e04d47983cbee5e7a8a106\n\n. Bert-Compound, BERT-Compound (Single) https://worksheets.codalab.org/bundles/0xc0dc1a25c03e4ba493ec28eef0e643b6\n\n. + Bidaf + Self-Attention, ; Elmo, Peters, EnsembleBiDAF + Self-Attention + ELMo (Ensemble) (Peters et al., 2018) https://worksheets.codalab.org/bundles/0x35b427e3105a46498256e3ccd502e442\n\n. AVIQA+. AVIQA+ (Ensemble) https://worksheets.codalab.org/bundles/0x0109d51630ac45599a85523d4690afd1\n\n. Eazi, EAZI (Ensemble) https://worksheets.codalab.org/bundles/0x55c1434feb8d48dfb990756ee1ce86d8\n\n. Eazi+, EAZI+ (Ensemble) https://worksheets.codalab.org/bundles/0x0b44f79d1e8042dd94943a35a057d7ea\n\n. Memen+, MEMEN+ (Ensemble) https://worksheets.codalab.org/bundles/0x065d328704784db7b093f3e750ff1b46\n\n. Dnet, DNET (Ensemble) https://worksheets.codalab.org/bundles/0x5b80aaba5fde4f65823746bb9b8a8fdc\n\n. Bert-Independent, BERT-Independent (Single) https://worksheets.codalab.org/bundles/0x82178b8ab098491aabec5b3a1ed18994\n\n. Hu, Reinforced Mnemonic Reader (SingleReinforced Mnemonic Reader (Single) (Hu et al., 2018) https://worksheets.codalab.org/bundles/0x78c31b2a1b9846a4b9de7dd71124656b\n\n. ; Fusionnet, Huang, FusionNet (Ensemble) (Huang et al., 2017) https://worksheets.codalab.org/bundles/0xd4ff6ed2458e4df099ea677a20115128\n\n. Mdreader, MDReader (Single) https://worksheets.codalab.org/bundles/0xed0bb85059b04ce79db37982b1381801\n\n. + Bidaf + Self Attention, Elmo, single modelBiDAF + Self Attention + ELMo (single model) https://worksheets.codalab.org/bundles/0x11f631b3e7cb4a0f8acbd60491f729b6\n\n. + Bidaf + Self-Attention, ; Elmo, Peters, SingleBiDAF + Self-Attention + ELMo (Single) (Peters et al., 2018) https://worksheets.codalab.org/bundles/0x5ab1fa7d11f04c5991d5011471ebdc4c\n\n. Mdreader0, MDReader0 (Single) https://worksheets.codalab.org/bundles/0x17bde05ef4b4483a9acf9e1ef8cc9326\n\n. + Bidaf++, Joshi, SingleBiDAF++ + pair2vec (Single) (Joshi et al., 2019) https://worksheets.codalab.org/bundles/0x1720fa746b0243e19692820fd930b14e\n\n( Liu, Conductor-net (Ensemble). Conductor-net (Ensemble) (Liu et al., 2017) https://worksheets.codalab.org/bundles/0x21d981f8667141b5bf6871714e3d5fd2\n\n. Memen+, MEMEN+ (Single) https://worksheets.codalab.org/bundles/0xf4709036e11843f88f870f4e7dea50a0\n\n. Aviqa, AVIQA v2 (Ensemble) https://worksheets.codalab.org/bundles/0x796847815444478c842f63a97cef93a0\n\n. Memen (single ; Pan, model submitted after paperMEMEN (Single; model submitted after paper) (Pan et al., 2017) https://worksheets.codalab.org/bundles/0x55fcc3f13d664944969bf05c59f402a4\n\n. Interactive AoA Reader. Interactive AoA Reader (Ensemble) https://worksheets.codalab.org/bundles/0x00599dfa3921413cab3a75a70722234d\n\n. Attentionreader+, AttentionReader+ (Single) https://worksheets.codalab.org/bundles/0x334adb7624674e90aff7be232fb52005\n\n. Dnet, DNET (Single) https://worksheets.codalab.org/bundles/0x5eb36fb24feb4911888760f8554f90ac\n\n. ; Bidaf++, Joshi, SingleBiDAF++ (Single) (Joshi et al., 2019) https://worksheets.codalab.org/bundles/0xb9a6b77b0163453c8fb942bafa1e2cfe\n\nMARS (Single. MARS (Single; January 23rd) https://worksheets.codalab.org/bundles/0x92ce58765d194debbadc1a165399a454\n\n. FRC. FRC (Single) https://worksheets.codalab.org/bundles/0x346b188552ed4d1cb6c1bccaa6d243eb\n\n. Jenga, Jenga (Ensemble) https://worksheets.codalab.org/bundles/0xbc23efc53a1f4735bad72aa01546ace1\n\n. + Rasor, Tr + Lm, Salant and Berant. SingleRaSoR + TR + LM (Single) (Salant and Berant, 2018) https://worksheets.codalab.org/bundles/0xec9321a11b0f44e19ca8d325dcda75eb\n\n. ; Fusionnet, Huang, SingleFusionNet (Single) (Huang et al., 2017) https://worksheets.codalab.org/bundles/0xbe9fefbe5b544675aafee4e83ccbe1e1\n\n. Chen , Smarnet (EnsembleSmarnet (Ensemble) (Chen et al., 2017) https://worksheets.codalab.org/bundles/0x622060479ede4552bf490c942598ac3c\n\n. Aviqa, AVIQA v2 (Single) https://worksheets.codalab.org/bundles/0x58ce6e7730b241dea20597b0a0e51b7e\n\n. ; Dcn+ (single, Xiong, DCN+ (Single) (Xiong et al., 2017) https://worksheets.codalab.org/bundles/0xd38944b81f484cf6a40955778204a0cf\n\n. Jenga (single model. Jenga (single model) https://worksheets.codalab.org/bundles/0x38bce62d659e43d19f56fc2ba34c3c4d\n\n. Mixedmodel, MixedModel (Ensemble) https://worksheets.codalab.org/bundles/0x761449f9e327450e85938688a002bc72\n\n. Two-Attention + Self-Attention. Two-Attention + Self-Attention (Ensemble) https://worksheets.codalab.org/bundles/0xf9087be2e1a34b96809b71e8ccaf9c56\n\nMEMEN (Ensemble; original model in paper. Pan, MEMEN (Ensemble; original model in paper) (Pan et al., 2017) https://worksheets.codalab.org/bundles/0x5596d3b1dceb414eab5653c5ec8f1607\n\n. Shen, ReasoNet (EnsembleReasoNet (Ensemble) (Shen et al., 2017) https://worksheets.codalab.org/bundles/0xe117260a328f484590e34b91839ce9ad\n\n. Hu, Mnemonic Reader (EnsembleMnemonic Reader (Ensemble) (Hu et al., 2018) https://worksheets.codalab.org/bundles/0xa860db3ea8854156b68da2e3a9a2f962\n\n( Liu, Conductor-net (Single). Conductor-net (Single) (Liu et al., 2017) https://worksheets.codalab.org/bundles/0x6fce3642dc574820949b0ae40bbac564\n\n. Interactive AoA Reader. Interactive AoA Reader (Single) https://worksheets.codalab.org/bundles/0x6541c8fd5acb44cf85572d6827c22f44\n\n. Jenga, Jenga (Single) https://worksheets.codalab.org/bundles/0x4b25320ab45d459fb4274c15ed925322\n\n. Ssae, SSAE (Ensemble) https://worksheets.codalab.org/bundles/0x34a9c6dd5f3145ce9130ddba8a951254\n\n. Zhang, jNet (Ensemble) (Zhang et al., 2017) https://worksheets.codalab.org/bundles/0x9ba8c5bbe77c4fd399d670ca11e42695\n\n. Gardner Clark, BiDAF + Self-Attention. SingleBiDAF + Self-Attention (Single) (Clark and Gardner, 2018) https://worksheets.codalab.org/bundles/0xe0b60a2436ef407cbf5fa0641c5350ba\n\n. Two-Attention + Self-Attention. Two-Attention + Self-Attention (Single) https://worksheets.codalab.org/bundles/0xfcb73b26ac0049478c0b4ae4f09cb3c9\n\n. AVIQA. AVIQA (Single) https://worksheets.codalab.org/bundles/0x513d75fb3d554dd6bc11dafb7ef1f5c3\n\n. Attention + Self-Attention. Attention + Self-Attention (Single) https://worksheets.codalab.org/bundles/0xbd549e52d11b42b39bd3d2fc0bbbe1da\n\n. ; Smarnet, Chen, SingleSmarnet (Single) (Chen et al., 2017) https://worksheets.codalab.org/bundles/0x733cef4d589743b8bc95a6108206c8a0\n\n. Mnemonic Reader, ; Hu, SingleMnemonic Reader (Single) (Hu et al., 2018) https://worksheets.codalab.org/bundles/0x28ff5339d7164a2ea95db1a4a3a2a750\n\n. ; Mamcn (single, Yu, MAMCN (Single) (Yu et al., 2018) https://worksheets.codalab.org/bundles/0x3d6ebcc7d54d44798d477e94fc840830\n\n. M-Net , M-NET (Single) https://worksheets.codalab.org/bundles/0x978c1865473f4a34bf23c14b152ec4e1\n\n. Zhang, jNet (Single) (Zhang et al., 2017) https://worksheets.codalab.org/bundles/0x8c62efeae93743018965441fe6e7ced0\n\n. Ruminating Reader (Single) (Gong and Bowman. Ruminating Reader (Single) (Gong and Bowman, 2018) https://worksheets.codalab.org/bundles/0x5abfb433377c45f3b6e3d26c3f6cd050\n\n. ; Reasonet, Shen, SingleReasoNet (Single) (Shen et al., 2017) https://worksheets.codalab.org/bundles/0x2356880cbc5347069d99a8cf38815dbc\n\n. ; Rasor, Lee, SingleRaSoR (Single) (Lee et al., 2016) https://worksheets.codalab.org/bundles/0x9dba642677a4489eb8fc78969601c893\n\n. Simplebaseline, SimpleBaseline (Single) https://worksheets.codalab.org/bundles/0xd78f5da9c45d4fa5bde361f9370b8a40\n\n. PQMN. PQMN (Single) https://worksheets.codalab.org/bundles/0x0f29cad4f3e94dcfb4560e4347d946d5\n\n. Seo, AllenNLP BiDAF (Single. AllenNLP BiDAF (Single) (Seo et al., 2016; Gardner et al., 2018) https://worksheets.codalab.org/bundles/0x8704f9226d884b5687fba7f73a462195\n\n. Iterative Co-Attention Network. Iterative Co-Attention Network (Single) https://worksheets.codalab.org/bundles/0x801a86cd3dbd44ae930c7134b7ababe5\n\n. Bidaf-Compound-Dss, BiDAF-Compound-DSS (Single) https://worksheets.codalab.org/bundles/0xc46b10050145494aa93708faa40b4013\n\n. Bidaf-Independent-Dss, BiDAF-Independent-DSS (Single) https://worksheets.codalab.org/bundles/0x2254478ccad84effbd92de915ff063be\n\n. Bidaf-Independent, BiDAF-Independent (Single) https://worksheets.codalab.org/bundles/0x3d6cd49604b8466ca952fda73bfb2527\n\n. Bidaf-Compound, BiDAF-Compound (Single) https://worksheets.codalab.org/bundles/0x450cd98f9ab548049b8e28c9f225910e\n\n. + Otf Dict, ; Spelling, Bahdanau, SingleOTF Dict + Spelling (Single) (Bahdanau et al., 2017) https://worksheets.codalab.org/bundles/0xd33f2fbd7eca4819b2c2b45371abcdf4\n\n. ; Otf Spelling, Bahdanau, SingleOTF Spelling (Single) (Bahdanau et al., 2017) https://worksheets.codalab.org/bundles/0x5ce7b655beb0454da5240c17f36bce6c\n\n. + Otf Spelling, ; Lemma, Bahdanau, SingleOTF Spelling + Lemma (Single) (Bahdanau et al., 2017) https://worksheets.codalab.org/bundles/0x308cfd9f735d4965835ec496610ea91d\n\nDynamic Chunk Reader (Single. Yu , Dynamic Chunk Reader (Single) (Yu et al., 2016) https://worksheets.codalab.org/bundles/0x345be18cbe4541de841de3ac79d5b441\n\n. UQA. UQA (single model) https://worksheets.codalab.org/bundles/0x64206b3164ea47e7a3d8a2df833c8f9b\n\nUnsupervisedQA V1. UnsupervisedQA V1 https://worksheets.codalab.org/bundles/0xe1c53a62c8644e9b9d9fdfd18feb6a85\n", "annotations": {"author": "[{\"end\":117,\"start\":73},{\"end\":163,\"start\":118},{\"end\":208,\"start\":164},{\"end\":256,\"start\":209},{\"end\":304,\"start\":257}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":78},{\"end\":130,\"start\":122},{\"end\":175,\"start\":169},{\"end\":223,\"start\":218},{\"end\":271,\"start\":264}]", "author_first_name": "[{\"end\":77,\"start\":73},{\"end\":119,\"start\":118},{\"end\":121,\"start\":120},{\"end\":168,\"start\":164},{\"end\":217,\"start\":209},{\"end\":263,\"start\":257}]", "author_affiliation": "[{\"end\":116,\"start\":86},{\"end\":162,\"start\":132},{\"end\":207,\"start\":177},{\"end\":255,\"start\":225},{\"end\":303,\"start\":273}]", "title": "[{\"end\":70,\"start\":1},{\"end\":374,\"start\":305}]", "venue": null, "abstract": "[{\"end\":1417,\"start\":376}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1532,\"start\":1508},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1815,\"start\":1792},{\"end\":2423,\"start\":2417},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3872,\"start\":3849},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4999,\"start\":4976},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5644,\"start\":5643},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5930,\"start\":5907},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6594,\"start\":6571},{\"end\":9782,\"start\":9776},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9933,\"start\":9909},{\"end\":10483,\"start\":10482},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11448,\"start\":11424},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12134,\"start\":12111},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12558,\"start\":12535},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13024,\"start\":13001},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14066,\"start\":14040},{\"end\":14587,\"start\":14566},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15119,\"start\":15096},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23317,\"start\":23294},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28949,\"start\":28926},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":30122,\"start\":30120},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":32539,\"start\":32537},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":39522,\"start\":39511},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":39725,\"start\":39702},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":49795,\"start\":49772},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":99665,\"start\":99642},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":99798,\"start\":99797}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37705,\"start\":36847},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37870,\"start\":37706},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37977,\"start\":37871},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38479,\"start\":37978},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39128,\"start\":38480},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39560,\"start\":39129},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39726,\"start\":39561},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40035,\"start\":39727},{\"attributes\":{\"id\":\"fig_9\"},\"end\":40261,\"start\":40036},{\"attributes\":{\"id\":\"fig_10\"},\"end\":40712,\"start\":40262},{\"attributes\":{\"id\":\"fig_11\"},\"end\":41906,\"start\":40713},{\"attributes\":{\"id\":\"fig_12\"},\"end\":42174,\"start\":41907},{\"attributes\":{\"id\":\"fig_13\"},\"end\":42524,\"start\":42175},{\"attributes\":{\"id\":\"fig_14\"},\"end\":42877,\"start\":42525},{\"attributes\":{\"id\":\"fig_16\"},\"end\":42913,\"start\":42878},{\"attributes\":{\"id\":\"fig_17\"},\"end\":42946,\"start\":42914},{\"attributes\":{\"id\":\"fig_18\"},\"end\":42985,\"start\":42947},{\"attributes\":{\"id\":\"fig_19\"},\"end\":43021,\"start\":42986},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43481,\"start\":43022},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43795,\"start\":43482},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45135,\"start\":43796},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47626,\"start\":45136},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":48185,\"start\":47627},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49601,\"start\":48186},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":50713,\"start\":49602},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":57874,\"start\":50714},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":58544,\"start\":57875},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":59289,\"start\":58545},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":59887,\"start\":59290},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":67358,\"start\":59888},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":68980,\"start\":67359},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":69521,\"start\":68981},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":72255,\"start\":69522},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":73800,\"start\":72256},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":77334,\"start\":73801},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":80267,\"start\":77335},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":81484,\"start\":80268},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":85051,\"start\":81485},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":87449,\"start\":85052},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":90469,\"start\":87450},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":93499,\"start\":90470},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":96499,\"start\":93500},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":99500,\"start\":96500}]", "paragraph": "[{\"end\":1954,\"start\":1433},{\"end\":2941,\"start\":1956},{\"end\":2996,\"start\":2943},{\"end\":3045,\"start\":2998},{\"end\":3368,\"start\":3047},{\"end\":4095,\"start\":3370},{\"end\":4700,\"start\":4097},{\"end\":5559,\"start\":4702},{\"end\":5645,\"start\":5561},{\"end\":5803,\"start\":5660},{\"end\":6520,\"start\":5843},{\"end\":7038,\"start\":6522},{\"end\":7409,\"start\":7040},{\"end\":7503,\"start\":7411},{\"end\":7939,\"start\":7534},{\"end\":9341,\"start\":7941},{\"end\":9623,\"start\":9343},{\"end\":10348,\"start\":9640},{\"end\":11682,\"start\":10350},{\"end\":12286,\"start\":11711},{\"end\":12428,\"start\":12307},{\"end\":13273,\"start\":12430},{\"end\":13888,\"start\":13275},{\"end\":14432,\"start\":13890},{\"end\":14702,\"start\":14434},{\"end\":15196,\"start\":14742},{\"end\":16172,\"start\":15198},{\"end\":16706,\"start\":16174},{\"end\":17485,\"start\":16727},{\"end\":17676,\"start\":17502},{\"end\":18258,\"start\":17678},{\"end\":18818,\"start\":18283},{\"end\":19613,\"start\":18820},{\"end\":20180,\"start\":19615},{\"end\":22407,\"start\":20226},{\"end\":22839,\"start\":22409},{\"end\":23862,\"start\":22860},{\"end\":24027,\"start\":23864},{\"end\":24110,\"start\":24029},{\"end\":24506,\"start\":24112},{\"end\":25053,\"start\":24508},{\"end\":25650,\"start\":25055},{\"end\":25936,\"start\":25665},{\"end\":26411,\"start\":25938},{\"end\":27966,\"start\":26413},{\"end\":28293,\"start\":27991},{\"end\":28581,\"start\":28295},{\"end\":28773,\"start\":28654},{\"end\":29048,\"start\":28854},{\"end\":29311,\"start\":29107},{\"end\":31590,\"start\":29313},{\"end\":31745,\"start\":31628},{\"end\":32540,\"start\":31778},{\"end\":33062,\"start\":32581},{\"end\":33147,\"start\":33064},{\"end\":33747,\"start\":33181},{\"end\":34770,\"start\":33788},{\"end\":35031,\"start\":34818},{\"end\":35641,\"start\":35056},{\"end\":36195,\"start\":35643},{\"end\":36846,\"start\":36223}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":28653,\"start\":28582},{\"attributes\":{\"id\":\"formula_1\"},\"end\":28853,\"start\":28774}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16655,\"start\":16648},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18108,\"start\":18094},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21606,\"start\":21599}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1431,\"start\":1419},{\"attributes\":{\"n\":\"2\"},\"end\":5658,\"start\":5648},{\"attributes\":{\"n\":\"2.1\"},\"end\":5841,\"start\":5806},{\"attributes\":{\"n\":\"2.2\"},\"end\":7532,\"start\":7506},{\"attributes\":{\"n\":\"3\"},\"end\":9638,\"start\":9626},{\"attributes\":{\"n\":\"4\"},\"end\":11709,\"start\":11685},{\"attributes\":{\"n\":\"4.1\"},\"end\":12305,\"start\":12289},{\"attributes\":{\"n\":\"4.2\"},\"end\":14740,\"start\":14705},{\"attributes\":{\"n\":\"4.3\"},\"end\":16725,\"start\":16709},{\"attributes\":{\"n\":\"5\"},\"end\":17500,\"start\":17488},{\"attributes\":{\"n\":\"5.1\"},\"end\":18281,\"start\":18261},{\"attributes\":{\"n\":\"5.2\"},\"end\":20224,\"start\":20183},{\"attributes\":{\"n\":\"6\"},\"end\":22858,\"start\":22842},{\"attributes\":{\"n\":\"7\"},\"end\":25663,\"start\":25653},{\"end\":27989,\"start\":27969},{\"end\":29105,\"start\":29051},{\"end\":31626,\"start\":31593},{\"end\":31776,\"start\":31748},{\"end\":32579,\"start\":32543},{\"end\":33179,\"start\":33150},{\"end\":33786,\"start\":33750},{\"end\":34816,\"start\":34773},{\"end\":35054,\"start\":35034},{\"end\":36221,\"start\":36198},{\"end\":36858,\"start\":36848},{\"end\":37717,\"start\":37707},{\"end\":37873,\"start\":37872},{\"end\":39140,\"start\":39130},{\"end\":39572,\"start\":39562},{\"end\":39738,\"start\":39728},{\"end\":40047,\"start\":40037},{\"end\":40273,\"start\":40263},{\"end\":40725,\"start\":40714},{\"end\":41919,\"start\":41908},{\"end\":42187,\"start\":42176},{\"end\":42537,\"start\":42526},{\"end\":42890,\"start\":42879},{\"end\":42926,\"start\":42915},{\"end\":42959,\"start\":42948},{\"end\":42998,\"start\":42987},{\"end\":43492,\"start\":43483},{\"end\":43806,\"start\":43797},{\"end\":47637,\"start\":47628},{\"end\":49612,\"start\":49603},{\"end\":68985,\"start\":68982},{\"end\":69532,\"start\":69523},{\"end\":72266,\"start\":72257},{\"end\":73811,\"start\":73802},{\"end\":77345,\"start\":77336},{\"end\":80278,\"start\":80269},{\"end\":81496,\"start\":81486},{\"end\":85063,\"start\":85053},{\"end\":87461,\"start\":87451},{\"end\":90481,\"start\":90471},{\"end\":93511,\"start\":93501},{\"end\":96511,\"start\":96501}]", "table": "[{\"end\":43795,\"start\":43600},{\"end\":45135,\"start\":44492},{\"end\":47626,\"start\":45485},{\"end\":48185,\"start\":48180},{\"end\":49601,\"start\":48418},{\"end\":50713,\"start\":50105},{\"end\":57874,\"start\":52818},{\"end\":58544,\"start\":58282},{\"end\":59289,\"start\":59010},{\"end\":59887,\"start\":59585},{\"end\":67358,\"start\":63359},{\"end\":68980,\"start\":67789},{\"end\":69521,\"start\":69090},{\"end\":72255,\"start\":69972},{\"end\":73800,\"start\":72708},{\"end\":77334,\"start\":74244},{\"end\":80267,\"start\":77347},{\"end\":81484,\"start\":80722},{\"end\":85051,\"start\":81962},{\"end\":87449,\"start\":85524},{\"end\":90469,\"start\":87560},{\"end\":93499,\"start\":90586},{\"end\":96499,\"start\":93586},{\"end\":99500,\"start\":96606}]", "figure_caption": "[{\"end\":37705,\"start\":36860},{\"end\":37870,\"start\":37719},{\"end\":37977,\"start\":37874},{\"end\":38479,\"start\":37980},{\"end\":39128,\"start\":38482},{\"end\":39560,\"start\":39142},{\"end\":39726,\"start\":39574},{\"end\":40035,\"start\":39740},{\"end\":40261,\"start\":40049},{\"end\":40712,\"start\":40275},{\"end\":41906,\"start\":40728},{\"end\":42174,\"start\":41922},{\"end\":42524,\"start\":42190},{\"end\":42877,\"start\":42540},{\"end\":42913,\"start\":42893},{\"end\":42946,\"start\":42929},{\"end\":42985,\"start\":42962},{\"end\":43021,\"start\":43001},{\"end\":43481,\"start\":43024},{\"end\":43600,\"start\":43494},{\"end\":44492,\"start\":43808},{\"end\":45485,\"start\":45138},{\"end\":48180,\"start\":47639},{\"end\":48418,\"start\":48188},{\"end\":50105,\"start\":49614},{\"end\":52818,\"start\":50716},{\"end\":58282,\"start\":57877},{\"end\":59010,\"start\":58547},{\"end\":59585,\"start\":59292},{\"end\":63359,\"start\":59890},{\"end\":67789,\"start\":67361},{\"end\":69090,\"start\":68987},{\"end\":69972,\"start\":69534},{\"end\":72708,\"start\":72268},{\"end\":74244,\"start\":73813},{\"end\":80722,\"start\":80280},{\"end\":81962,\"start\":81499},{\"end\":85524,\"start\":85066},{\"end\":87560,\"start\":87464},{\"end\":90586,\"start\":90484},{\"end\":93586,\"start\":93514},{\"end\":96606,\"start\":96514}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5239,\"start\":5231},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6101,\"start\":6093},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18975,\"start\":18967},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19786,\"start\":19778},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20895,\"start\":20887},{\"end\":21969,\"start\":21961},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22622,\"start\":22614},{\"end\":22635,\"start\":22627},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":23516,\"start\":23508},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":25066,\"start\":25058},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25223,\"start\":25215},{\"end\":29992,\"start\":29984},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":30782,\"start\":30774},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31109,\"start\":31100},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31797,\"start\":31789},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31992,\"start\":31983},{\"end\":32780,\"start\":32772},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33488,\"start\":33466},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33593,\"start\":33576},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34523,\"start\":34506},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34646,\"start\":34629}]", "bib_author_first_name": "[{\"end\":100765,\"start\":100758},{\"end\":100779,\"start\":100776},{\"end\":100792,\"start\":100786},{\"end\":100823,\"start\":100817},{\"end\":100844,\"start\":100838},{\"end\":101143,\"start\":101138},{\"end\":101163,\"start\":101157},{\"end\":101180,\"start\":101175},{\"end\":101194,\"start\":101189},{\"end\":101209,\"start\":101203},{\"end\":101503,\"start\":101495},{\"end\":101517,\"start\":101512},{\"end\":101536,\"start\":101528},{\"end\":101547,\"start\":101543},{\"end\":101571,\"start\":101563},{\"end\":101585,\"start\":101581},{\"end\":101598,\"start\":101593},{\"end\":101619,\"start\":101606},{\"end\":102197,\"start\":102192},{\"end\":102210,\"start\":102204},{\"end\":102507,\"start\":102500},{\"end\":102521,\"start\":102514},{\"end\":102531,\"start\":102528},{\"end\":102541,\"start\":102537},{\"end\":102552,\"start\":102548},{\"end\":102565,\"start\":102558},{\"end\":102847,\"start\":102836},{\"end\":102859,\"start\":102855},{\"end\":103283,\"start\":103278},{\"end\":103300,\"start\":103292},{\"end\":103314,\"start\":103308},{\"end\":103328,\"start\":103320},{\"end\":103338,\"start\":103329},{\"end\":103646,\"start\":103639},{\"end\":103659,\"start\":103653},{\"end\":103671,\"start\":103667},{\"end\":103687,\"start\":103681},{\"end\":103709,\"start\":103700},{\"end\":104110,\"start\":104104},{\"end\":104119,\"start\":104116},{\"end\":104128,\"start\":104124},{\"end\":104138,\"start\":104135},{\"end\":104148,\"start\":104144},{\"end\":104161,\"start\":104154},{\"end\":104424,\"start\":104423},{\"end\":104438,\"start\":104434},{\"end\":104452,\"start\":104447},{\"end\":104466,\"start\":104462},{\"end\":104485,\"start\":104474},{\"end\":104501,\"start\":104495},{\"end\":104513,\"start\":104509},{\"end\":104802,\"start\":104798},{\"end\":105009,\"start\":105003},{\"end\":105025,\"start\":105021},{\"end\":105043,\"start\":105033},{\"end\":105058,\"start\":105053},{\"end\":105535,\"start\":105529},{\"end\":105552,\"start\":105547},{\"end\":105563,\"start\":105558},{\"end\":106047,\"start\":106039},{\"end\":106062,\"start\":106055},{\"end\":106078,\"start\":106072},{\"end\":106096,\"start\":106088},{\"end\":106420,\"start\":106414},{\"end\":106448,\"start\":106442},{\"end\":106987,\"start\":106980},{\"end\":107001,\"start\":107000},{\"end\":107003,\"start\":107002},{\"end\":107021,\"start\":107017},{\"end\":107510,\"start\":107503},{\"end\":107524,\"start\":107520},{\"end\":107545,\"start\":107541},{\"end\":107562,\"start\":107554},{\"end\":107578,\"start\":107572},{\"end\":107594,\"start\":107586},{\"end\":107608,\"start\":107602},{\"end\":107986,\"start\":107981},{\"end\":108003,\"start\":107995},{\"end\":108655,\"start\":108647},{\"end\":108665,\"start\":108661},{\"end\":108894,\"start\":108887},{\"end\":108909,\"start\":108900},{\"end\":108923,\"start\":108920},{\"end\":108941,\"start\":108933},{\"end\":109220,\"start\":109214},{\"end\":109233,\"start\":109227},{\"end\":109249,\"start\":109241},{\"end\":109261,\"start\":109255},{\"end\":109794,\"start\":109790},{\"end\":109811,\"start\":109803},{\"end\":110061,\"start\":110057},{\"end\":110077,\"start\":110073},{\"end\":110090,\"start\":110084},{\"end\":110103,\"start\":110097},{\"end\":110122,\"start\":110112},{\"end\":110138,\"start\":110132},{\"end\":110154,\"start\":110148},{\"end\":110595,\"start\":110587},{\"end\":110606,\"start\":110602},{\"end\":110856,\"start\":110849},{\"end\":110870,\"start\":110864},{\"end\":110885,\"start\":110878},{\"end\":111129,\"start\":111123},{\"end\":111141,\"start\":111137},{\"end\":111426,\"start\":111420},{\"end\":111437,\"start\":111433},{\"end\":111450,\"start\":111442},{\"end\":111464,\"start\":111458},{\"end\":111480,\"start\":111473},{\"end\":111494,\"start\":111488},{\"end\":111523,\"start\":111510},{\"end\":112005,\"start\":111999},{\"end\":112018,\"start\":112012},{\"end\":112030,\"start\":112024},{\"end\":112042,\"start\":112037},{\"end\":112060,\"start\":112054},{\"end\":112082,\"start\":112076},{\"end\":112458,\"start\":112454},{\"end\":112475,\"start\":112469},{\"end\":112514,\"start\":112509},{\"end\":112527,\"start\":112523},{\"end\":112545,\"start\":112537},{\"end\":112567,\"start\":112559},{\"end\":112578,\"start\":112574},{\"end\":112593,\"start\":112590},{\"end\":112605,\"start\":112600},{\"end\":112996,\"start\":112988},{\"end\":113008,\"start\":113001},{\"end\":113039,\"start\":113033},{\"end\":113498,\"start\":113494},{\"end\":113506,\"start\":113503},{\"end\":113518,\"start\":113514},{\"end\":113528,\"start\":113526},{\"end\":113537,\"start\":113533},{\"end\":113550,\"start\":113545},{\"end\":113866,\"start\":113860},{\"end\":113881,\"start\":113874},{\"end\":113891,\"start\":113887},{\"end\":113904,\"start\":113898},{\"end\":113912,\"start\":113910},{\"end\":113921,\"start\":113918},{\"end\":114255,\"start\":114250},{\"end\":114271,\"start\":114263},{\"end\":114285,\"start\":114277},{\"end\":114299,\"start\":114291},{\"end\":114310,\"start\":114305},{\"end\":114324,\"start\":114316},{\"end\":114632,\"start\":114626},{\"end\":114646,\"start\":114640},{\"end\":114882,\"start\":114881},{\"end\":115233,\"start\":115232},{\"end\":115507,\"start\":115492},{\"end\":115516,\"start\":115515},{\"end\":115831,\"start\":115821},{\"end\":116083,\"start\":116073},{\"end\":116092,\"start\":116091},{\"end\":117119,\"start\":117109},{\"end\":117131,\"start\":117130},{\"end\":117322,\"start\":117312},{\"end\":117334,\"start\":117333},{\"end\":117681,\"start\":117672},{\"end\":117693,\"start\":117692},{\"end\":119052,\"start\":119045},{\"end\":119448,\"start\":119447},{\"end\":119474,\"start\":119473},{\"end\":120433,\"start\":120432},{\"end\":120677,\"start\":120676},{\"end\":120843,\"start\":120842},{\"end\":120869,\"start\":120868},{\"end\":121136,\"start\":121135},{\"end\":121284,\"start\":121283},{\"end\":122184,\"start\":122183},{\"end\":122636,\"start\":122635},{\"end\":122807,\"start\":122806},{\"end\":122953,\"start\":122949},{\"end\":123192,\"start\":123191},{\"end\":124181,\"start\":124180},{\"end\":124789,\"start\":124782},{\"end\":125352,\"start\":125351},{\"end\":125496,\"start\":125488},{\"end\":125506,\"start\":125505},{\"end\":125638,\"start\":125637},{\"end\":125773,\"start\":125768},{\"end\":126161,\"start\":126160},{\"end\":126300,\"start\":126299},{\"end\":127461,\"start\":127460},{\"end\":127473,\"start\":127472},{\"end\":127631,\"start\":127630},{\"end\":127786,\"start\":127785},{\"end\":127802,\"start\":127801},{\"end\":127987,\"start\":127985}]", "bib_author_last_name": "[{\"end\":100774,\"start\":100766},{\"end\":100784,\"start\":100780},{\"end\":100815,\"start\":100793},{\"end\":100836,\"start\":100824},{\"end\":100852,\"start\":100845},{\"end\":100860,\"start\":100854},{\"end\":101155,\"start\":101144},{\"end\":101173,\"start\":101164},{\"end\":101187,\"start\":101181},{\"end\":101201,\"start\":101195},{\"end\":101219,\"start\":101210},{\"end\":101510,\"start\":101504},{\"end\":101526,\"start\":101518},{\"end\":101541,\"start\":101537},{\"end\":101561,\"start\":101548},{\"end\":101579,\"start\":101572},{\"end\":101591,\"start\":101586},{\"end\":101604,\"start\":101599},{\"end\":101627,\"start\":101620},{\"end\":102202,\"start\":102198},{\"end\":102216,\"start\":102211},{\"end\":102512,\"start\":102508},{\"end\":102526,\"start\":102522},{\"end\":102535,\"start\":102532},{\"end\":102546,\"start\":102542},{\"end\":102556,\"start\":102553},{\"end\":102568,\"start\":102566},{\"end\":102853,\"start\":102848},{\"end\":102867,\"start\":102860},{\"end\":103290,\"start\":103284},{\"end\":103306,\"start\":103301},{\"end\":103318,\"start\":103315},{\"end\":103343,\"start\":103339},{\"end\":103651,\"start\":103647},{\"end\":103665,\"start\":103660},{\"end\":103679,\"start\":103672},{\"end\":103698,\"start\":103688},{\"end\":103715,\"start\":103710},{\"end\":103720,\"start\":103717},{\"end\":104114,\"start\":104111},{\"end\":104122,\"start\":104120},{\"end\":104133,\"start\":104129},{\"end\":104142,\"start\":104139},{\"end\":104152,\"start\":104149},{\"end\":104164,\"start\":104162},{\"end\":104432,\"start\":104425},{\"end\":104445,\"start\":104439},{\"end\":104460,\"start\":104453},{\"end\":104472,\"start\":104467},{\"end\":104493,\"start\":104486},{\"end\":104507,\"start\":104502},{\"end\":104517,\"start\":104514},{\"end\":104530,\"start\":104519},{\"end\":104806,\"start\":104803},{\"end\":105019,\"start\":105010},{\"end\":105031,\"start\":105026},{\"end\":105051,\"start\":105044},{\"end\":105064,\"start\":105059},{\"end\":105545,\"start\":105536},{\"end\":105556,\"start\":105553},{\"end\":105569,\"start\":105564},{\"end\":106053,\"start\":106048},{\"end\":106070,\"start\":106063},{\"end\":106086,\"start\":106079},{\"end\":106104,\"start\":106097},{\"end\":106440,\"start\":106421},{\"end\":106454,\"start\":106449},{\"end\":106464,\"start\":106456},{\"end\":106998,\"start\":106988},{\"end\":107015,\"start\":107004},{\"end\":107028,\"start\":107022},{\"end\":107037,\"start\":107030},{\"end\":107518,\"start\":107511},{\"end\":107539,\"start\":107525},{\"end\":107552,\"start\":107546},{\"end\":107570,\"start\":107563},{\"end\":107584,\"start\":107579},{\"end\":107600,\"start\":107595},{\"end\":107616,\"start\":107609},{\"end\":107993,\"start\":107987},{\"end\":108010,\"start\":108004},{\"end\":108659,\"start\":108656},{\"end\":108673,\"start\":108666},{\"end\":108898,\"start\":108895},{\"end\":108918,\"start\":108910},{\"end\":108931,\"start\":108924},{\"end\":108952,\"start\":108942},{\"end\":109225,\"start\":109221},{\"end\":109239,\"start\":109234},{\"end\":109253,\"start\":109250},{\"end\":109266,\"start\":109262},{\"end\":109801,\"start\":109795},{\"end\":109818,\"start\":109812},{\"end\":110071,\"start\":110062},{\"end\":110082,\"start\":110078},{\"end\":110095,\"start\":110091},{\"end\":110110,\"start\":110104},{\"end\":110130,\"start\":110123},{\"end\":110146,\"start\":110139},{\"end\":110162,\"start\":110155},{\"end\":110600,\"start\":110596},{\"end\":110612,\"start\":110607},{\"end\":110862,\"start\":110857},{\"end\":110876,\"start\":110871},{\"end\":110892,\"start\":110886},{\"end\":111135,\"start\":111130},{\"end\":111148,\"start\":111142},{\"end\":111431,\"start\":111427},{\"end\":111440,\"start\":111438},{\"end\":111456,\"start\":111451},{\"end\":111471,\"start\":111465},{\"end\":111486,\"start\":111481},{\"end\":111508,\"start\":111495},{\"end\":111531,\"start\":111524},{\"end\":112010,\"start\":112006},{\"end\":112022,\"start\":112019},{\"end\":112035,\"start\":112031},{\"end\":112052,\"start\":112043},{\"end\":112074,\"start\":112061},{\"end\":112085,\"start\":112083},{\"end\":112092,\"start\":112087},{\"end\":112467,\"start\":112459},{\"end\":112507,\"start\":112476},{\"end\":112521,\"start\":112515},{\"end\":112535,\"start\":112528},{\"end\":112557,\"start\":112546},{\"end\":112572,\"start\":112568},{\"end\":112588,\"start\":112579},{\"end\":112598,\"start\":112594},{\"end\":112608,\"start\":112606},{\"end\":112614,\"start\":112610},{\"end\":112999,\"start\":112997},{\"end\":113031,\"start\":113009},{\"end\":113044,\"start\":113040},{\"end\":113049,\"start\":113046},{\"end\":113501,\"start\":113499},{\"end\":113512,\"start\":113507},{\"end\":113524,\"start\":113519},{\"end\":113531,\"start\":113529},{\"end\":113543,\"start\":113538},{\"end\":113555,\"start\":113551},{\"end\":113872,\"start\":113867},{\"end\":113885,\"start\":113882},{\"end\":113896,\"start\":113892},{\"end\":113908,\"start\":113905},{\"end\":113916,\"start\":113913},{\"end\":113927,\"start\":113922},{\"end\":114261,\"start\":114256},{\"end\":114275,\"start\":114272},{\"end\":114289,\"start\":114286},{\"end\":114303,\"start\":114300},{\"end\":114314,\"start\":114311},{\"end\":114334,\"start\":114325},{\"end\":114638,\"start\":114633},{\"end\":114652,\"start\":114647},{\"end\":114888,\"start\":114883},{\"end\":114894,\"start\":114890},{\"end\":115023,\"start\":115014},{\"end\":115132,\"start\":115121},{\"end\":115242,\"start\":115234},{\"end\":115249,\"start\":115244},{\"end\":115388,\"start\":115373},{\"end\":115513,\"start\":115508},{\"end\":115522,\"start\":115517},{\"end\":115529,\"start\":115524},{\"end\":115676,\"start\":115672},{\"end\":115840,\"start\":115832},{\"end\":115975,\"start\":115966},{\"end\":116089,\"start\":116084},{\"end\":116098,\"start\":116093},{\"end\":116106,\"start\":116100},{\"end\":116268,\"start\":116265},{\"end\":116489,\"start\":116482},{\"end\":116715,\"start\":116690},{\"end\":116833,\"start\":116829},{\"end\":116989,\"start\":116976},{\"end\":117128,\"start\":117120},{\"end\":117140,\"start\":117132},{\"end\":117148,\"start\":117142},{\"end\":117331,\"start\":117323},{\"end\":117340,\"start\":117335},{\"end\":117348,\"start\":117342},{\"end\":117690,\"start\":117682},{\"end\":117702,\"start\":117694},{\"end\":117710,\"start\":117704},{\"end\":118393,\"start\":118388},{\"end\":118601,\"start\":118599},{\"end\":118788,\"start\":118772},{\"end\":119079,\"start\":119053},{\"end\":119083,\"start\":119081},{\"end\":119226,\"start\":119209},{\"end\":119345,\"start\":119332},{\"end\":119471,\"start\":119449},{\"end\":119479,\"start\":119475},{\"end\":119487,\"start\":119481},{\"end\":119744,\"start\":119740},{\"end\":119844,\"start\":119839},{\"end\":119946,\"start\":119940},{\"end\":120047,\"start\":120043},{\"end\":120158,\"start\":120142},{\"end\":120265,\"start\":120263},{\"end\":120443,\"start\":120434},{\"end\":120450,\"start\":120445},{\"end\":120579,\"start\":120571},{\"end\":120700,\"start\":120678},{\"end\":120706,\"start\":120702},{\"end\":120866,\"start\":120844},{\"end\":120874,\"start\":120870},{\"end\":120882,\"start\":120876},{\"end\":121037,\"start\":121028},{\"end\":121144,\"start\":121137},{\"end\":121151,\"start\":121146},{\"end\":121288,\"start\":121285},{\"end\":121443,\"start\":121437},{\"end\":121543,\"start\":121538},{\"end\":121661,\"start\":121642},{\"end\":121981,\"start\":121965},{\"end\":122090,\"start\":122086},{\"end\":122192,\"start\":122185},{\"end\":122199,\"start\":122194},{\"end\":122539,\"start\":122534},{\"end\":122642,\"start\":122637},{\"end\":122651,\"start\":122644},{\"end\":122817,\"start\":122808},{\"end\":122824,\"start\":122819},{\"end\":123094,\"start\":123089},{\"end\":123205,\"start\":123193},{\"end\":123212,\"start\":123207},{\"end\":123455,\"start\":123445},{\"end\":123750,\"start\":123747},{\"end\":123894,\"start\":123890},{\"end\":124033,\"start\":124031},{\"end\":124185,\"start\":124182},{\"end\":124468,\"start\":124463},{\"end\":124566,\"start\":124562},{\"end\":124666,\"start\":124661},{\"end\":124795,\"start\":124790},{\"end\":125360,\"start\":125353},{\"end\":125366,\"start\":125362},{\"end\":125503,\"start\":125497},{\"end\":125509,\"start\":125507},{\"end\":125652,\"start\":125639},{\"end\":125656,\"start\":125654},{\"end\":125873,\"start\":125868},{\"end\":126170,\"start\":126162},{\"end\":126176,\"start\":126172},{\"end\":126306,\"start\":126301},{\"end\":126311,\"start\":126308},{\"end\":126444,\"start\":126430},{\"end\":126647,\"start\":126644},{\"end\":126982,\"start\":126964},{\"end\":127110,\"start\":127089},{\"end\":127237,\"start\":127220},{\"end\":127357,\"start\":127343},{\"end\":127470,\"start\":127462},{\"end\":127482,\"start\":127474},{\"end\":127492,\"start\":127484},{\"end\":127644,\"start\":127632},{\"end\":127654,\"start\":127646},{\"end\":127799,\"start\":127787},{\"end\":127808,\"start\":127803},{\"end\":127818,\"start\":127810}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1706.00286\",\"id\":\"b0\"},\"end\":101134,\"start\":100758},{\"attributes\":{\"doi\":\"arXiv:2001.08435\",\"id\":\"b1\"},\"end\":101436,\"start\":101136},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8471750},\"end\":102120,\"start\":101438},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1493191},\"end\":102436,\"start\":102122},{\"attributes\":{\"doi\":\"arXiv:1710.02772\",\"id\":\"b4\"},\"end\":102774,\"start\":102438},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":223637},\"end\":103276,\"start\":102776},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b6\"},\"end\":103637,\"start\":103278},{\"attributes\":{\"doi\":\"arXiv:1704.05179\",\"id\":\"b7\"},\"end\":104025,\"start\":103639},{\"attributes\":{\"doi\":\"arXiv:1707.09098\",\"id\":\"b8\"},\"end\":104379,\"start\":104027},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3626819},\"end\":104772,\"start\":104381},{\"attributes\":{\"id\":\"b10\"},\"end\":104847,\"start\":104774},{\"attributes\":{\"id\":\"b11\"},\"end\":104940,\"start\":104849},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11816014},\"end\":105468,\"start\":104942},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":47018994},\"end\":105989,\"start\":105470},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":67855879},\"end\":106344,\"start\":105991},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":21740766},\"end\":106899,\"start\":106346},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2100831},\"end\":107449,\"start\":106901},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207979247},\"end\":107916,\"start\":107451},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3834355},\"end\":108589,\"start\":107918},{\"attributes\":{\"doi\":\"arXiv:2004.03490\",\"id\":\"b19\"},\"end\":108829,\"start\":108591},{\"attributes\":{\"doi\":\"arXiv:1611.01603\",\"id\":\"b20\"},\"end\":109151,\"start\":108831},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6300274},\"end\":109695,\"start\":109153},{\"attributes\":{\"doi\":\"arXiv:1905.13453\",\"id\":\"b22\"},\"end\":110014,\"start\":109697},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1167588},\"end\":110526,\"start\":110016},{\"attributes\":{\"doi\":\"arXiv:1608.07905\",\"id\":\"b24\"},\"end\":110771,\"start\":110528},{\"attributes\":{\"doi\":\"arXiv:1711.00106\",\"id\":\"b25\"},\"end\":111087,\"start\":110773},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":166227957},\"end\":111343,\"start\":111089},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52822214},\"end\":111997,\"start\":111345},{\"attributes\":{\"doi\":\"arXiv:1906.08237\",\"id\":\"b28\"},\"end\":112395,\"start\":111999},{\"attributes\":{\"doi\":\"arXiv:1901.11373\",\"id\":\"b29\"},\"end\":112905,\"start\":112397},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":51876026},\"end\":113418,\"start\":112907},{\"attributes\":{\"doi\":\"arXiv:1610.09996\",\"id\":\"b31\"},\"end\":113766,\"start\":113420},{\"attributes\":{\"doi\":\"arXiv:1703.04617\",\"id\":\"b32\"},\"end\":114162,\"start\":113768},{\"attributes\":{\"doi\":\"arXiv:1810.12885\",\"id\":\"b33\"},\"end\":114580,\"start\":114164},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":59523759},\"end\":114877,\"start\":114582},{\"attributes\":{\"id\":\"b35\"},\"end\":115010,\"start\":114879},{\"attributes\":{\"id\":\"b36\"},\"end\":115117,\"start\":115012},{\"attributes\":{\"id\":\"b37\"},\"end\":115228,\"start\":115119},{\"attributes\":{\"id\":\"b38\"},\"end\":115369,\"start\":115230},{\"attributes\":{\"id\":\"b39\"},\"end\":115488,\"start\":115371},{\"attributes\":{\"id\":\"b40\"},\"end\":115668,\"start\":115490},{\"attributes\":{\"id\":\"b41\"},\"end\":115817,\"start\":115670},{\"attributes\":{\"id\":\"b42\"},\"end\":115962,\"start\":115819},{\"attributes\":{\"id\":\"b43\"},\"end\":116069,\"start\":115964},{\"attributes\":{\"id\":\"b44\"},\"end\":116261,\"start\":116071},{\"attributes\":{\"id\":\"b45\"},\"end\":116356,\"start\":116263},{\"attributes\":{\"id\":\"b46\"},\"end\":116478,\"start\":116358},{\"attributes\":{\"id\":\"b47\"},\"end\":116581,\"start\":116480},{\"attributes\":{\"id\":\"b48\"},\"end\":116686,\"start\":116583},{\"attributes\":{\"id\":\"b49\"},\"end\":116825,\"start\":116688},{\"attributes\":{\"id\":\"b50\"},\"end\":116972,\"start\":116827},{\"attributes\":{\"id\":\"b51\"},\"end\":117105,\"start\":116974},{\"attributes\":{\"id\":\"b52\"},\"end\":117308,\"start\":117107},{\"attributes\":{\"id\":\"b53\"},\"end\":117506,\"start\":117310},{\"attributes\":{\"id\":\"b54\"},\"end\":117668,\"start\":117508},{\"attributes\":{\"id\":\"b55\"},\"end\":117869,\"start\":117670},{\"attributes\":{\"id\":\"b56\"},\"end\":118034,\"start\":117871},{\"attributes\":{\"id\":\"b57\"},\"end\":118152,\"start\":118036},{\"attributes\":{\"id\":\"b58\"},\"end\":118270,\"start\":118154},{\"attributes\":{\"id\":\"b59\"},\"end\":118384,\"start\":118272},{\"attributes\":{\"id\":\"b60\"},\"end\":118483,\"start\":118386},{\"attributes\":{\"id\":\"b61\"},\"end\":118595,\"start\":118485},{\"attributes\":{\"id\":\"b62\"},\"end\":118768,\"start\":118597},{\"attributes\":{\"id\":\"b63\"},\"end\":118891,\"start\":118770},{\"attributes\":{\"id\":\"b64\"},\"end\":119041,\"start\":118893},{\"attributes\":{\"id\":\"b65\"},\"end\":119205,\"start\":119043},{\"attributes\":{\"id\":\"b66\"},\"end\":119328,\"start\":119207},{\"attributes\":{\"id\":\"b67\"},\"end\":119443,\"start\":119330},{\"attributes\":{\"id\":\"b68\"},\"end\":119633,\"start\":119445},{\"attributes\":{\"id\":\"b69\"},\"end\":119736,\"start\":119635},{\"attributes\":{\"id\":\"b70\"},\"end\":119835,\"start\":119738},{\"attributes\":{\"id\":\"b71\"},\"end\":119936,\"start\":119837},{\"attributes\":{\"id\":\"b72\"},\"end\":120039,\"start\":119938},{\"attributes\":{\"id\":\"b73\"},\"end\":120138,\"start\":120041},{\"attributes\":{\"id\":\"b74\"},\"end\":120259,\"start\":120140},{\"attributes\":{\"id\":\"b75\"},\"end\":120428,\"start\":120261},{\"attributes\":{\"id\":\"b76\"},\"end\":120567,\"start\":120430},{\"attributes\":{\"id\":\"b77\"},\"end\":120672,\"start\":120569},{\"attributes\":{\"id\":\"b78\"},\"end\":120838,\"start\":120674},{\"attributes\":{\"id\":\"b79\"},\"end\":121024,\"start\":120840},{\"attributes\":{\"id\":\"b80\"},\"end\":121131,\"start\":121026},{\"attributes\":{\"id\":\"b81\"},\"end\":121281,\"start\":121133},{\"attributes\":{\"id\":\"b82\"},\"end\":121433,\"start\":121283},{\"attributes\":{\"id\":\"b83\"},\"end\":121534,\"start\":121435},{\"attributes\":{\"id\":\"b84\"},\"end\":121638,\"start\":121536},{\"attributes\":{\"id\":\"b85\"},\"end\":121826,\"start\":121640},{\"attributes\":{\"id\":\"b86\"},\"end\":121961,\"start\":121828},{\"attributes\":{\"id\":\"b87\"},\"end\":122082,\"start\":121963},{\"attributes\":{\"id\":\"b88\"},\"end\":122179,\"start\":122084},{\"attributes\":{\"id\":\"b89\"},\"end\":122318,\"start\":122181},{\"attributes\":{\"id\":\"b90\"},\"end\":122435,\"start\":122320},{\"attributes\":{\"id\":\"b91\"},\"end\":122530,\"start\":122437},{\"attributes\":{\"id\":\"b92\"},\"end\":122631,\"start\":122532},{\"attributes\":{\"id\":\"b93\"},\"end\":122802,\"start\":122633},{\"attributes\":{\"id\":\"b94\"},\"end\":122945,\"start\":122804},{\"attributes\":{\"id\":\"b95\"},\"end\":123085,\"start\":122947},{\"attributes\":{\"id\":\"b96\"},\"end\":123187,\"start\":123087},{\"attributes\":{\"id\":\"b97\"},\"end\":123322,\"start\":123189},{\"attributes\":{\"id\":\"b98\"},\"end\":123441,\"start\":123324},{\"attributes\":{\"id\":\"b99\"},\"end\":123552,\"start\":123443},{\"attributes\":{\"id\":\"b100\"},\"end\":123703,\"start\":123554},{\"attributes\":{\"id\":\"b101\"},\"end\":123886,\"start\":123705},{\"attributes\":{\"id\":\"b102\"},\"end\":124027,\"start\":123888},{\"attributes\":{\"id\":\"b103\"},\"end\":124178,\"start\":124029},{\"attributes\":{\"id\":\"b104\"},\"end\":124326,\"start\":124180},{\"attributes\":{\"id\":\"b105\"},\"end\":124459,\"start\":124328},{\"attributes\":{\"id\":\"b106\"},\"end\":124558,\"start\":124461},{\"attributes\":{\"id\":\"b107\"},\"end\":124657,\"start\":124560},{\"attributes\":{\"id\":\"b108\"},\"end\":124778,\"start\":124659},{\"attributes\":{\"id\":\"b109\"},\"end\":124958,\"start\":124780},{\"attributes\":{\"id\":\"b110\"},\"end\":125107,\"start\":124960},{\"attributes\":{\"id\":\"b111\"},\"end\":125206,\"start\":125109},{\"attributes\":{\"id\":\"b112\"},\"end\":125347,\"start\":125208},{\"attributes\":{\"id\":\"b113\"},\"end\":125484,\"start\":125349},{\"attributes\":{\"id\":\"b114\"},\"end\":125633,\"start\":125486},{\"attributes\":{\"id\":\"b115\"},\"end\":125764,\"start\":125635},{\"attributes\":{\"id\":\"b116\"},\"end\":125864,\"start\":125766},{\"attributes\":{\"id\":\"b117\"},\"end\":125983,\"start\":125866},{\"attributes\":{\"id\":\"b118\"},\"end\":126156,\"start\":125985},{\"attributes\":{\"id\":\"b119\"},\"end\":126295,\"start\":126158},{\"attributes\":{\"id\":\"b120\"},\"end\":126426,\"start\":126297},{\"attributes\":{\"id\":\"b121\"},\"end\":126543,\"start\":126428},{\"attributes\":{\"id\":\"b122\"},\"end\":126640,\"start\":126545},{\"attributes\":{\"id\":\"b123\"},\"end\":126811,\"start\":126642},{\"attributes\":{\"id\":\"b124\"},\"end\":126960,\"start\":126813},{\"attributes\":{\"id\":\"b125\"},\"end\":127085,\"start\":126962},{\"attributes\":{\"id\":\"b126\"},\"end\":127216,\"start\":127087},{\"attributes\":{\"id\":\"b127\"},\"end\":127339,\"start\":127218},{\"attributes\":{\"id\":\"b128\"},\"end\":127456,\"start\":127341},{\"attributes\":{\"id\":\"b129\"},\"end\":127626,\"start\":127458},{\"attributes\":{\"id\":\"b130\"},\"end\":127781,\"start\":127628},{\"attributes\":{\"id\":\"b131\"},\"end\":127953,\"start\":127783},{\"attributes\":{\"id\":\"b132\"},\"end\":128111,\"start\":127955},{\"attributes\":{\"id\":\"b133\"},\"end\":128212,\"start\":128113},{\"attributes\":{\"id\":\"b134\"},\"end\":128324,\"start\":128214}]", "bib_title": "[{\"end\":101493,\"start\":101438},{\"end\":102190,\"start\":102122},{\"end\":102834,\"start\":102776},{\"end\":104421,\"start\":104381},{\"end\":105001,\"start\":104942},{\"end\":105527,\"start\":105470},{\"end\":106037,\"start\":105991},{\"end\":106412,\"start\":106346},{\"end\":106978,\"start\":106901},{\"end\":107501,\"start\":107451},{\"end\":107979,\"start\":107918},{\"end\":109212,\"start\":109153},{\"end\":110055,\"start\":110016},{\"end\":111121,\"start\":111089},{\"end\":111418,\"start\":111345},{\"end\":112986,\"start\":112907},{\"end\":114624,\"start\":114582}]", "bib_author": "[{\"end\":100776,\"start\":100758},{\"end\":100786,\"start\":100776},{\"end\":100817,\"start\":100786},{\"end\":100838,\"start\":100817},{\"end\":100854,\"start\":100838},{\"end\":100862,\"start\":100854},{\"end\":101157,\"start\":101138},{\"end\":101175,\"start\":101157},{\"end\":101189,\"start\":101175},{\"end\":101203,\"start\":101189},{\"end\":101221,\"start\":101203},{\"end\":101512,\"start\":101495},{\"end\":101528,\"start\":101512},{\"end\":101543,\"start\":101528},{\"end\":101563,\"start\":101543},{\"end\":101581,\"start\":101563},{\"end\":101593,\"start\":101581},{\"end\":101606,\"start\":101593},{\"end\":101629,\"start\":101606},{\"end\":102204,\"start\":102192},{\"end\":102218,\"start\":102204},{\"end\":102514,\"start\":102500},{\"end\":102528,\"start\":102514},{\"end\":102537,\"start\":102528},{\"end\":102548,\"start\":102537},{\"end\":102558,\"start\":102548},{\"end\":102570,\"start\":102558},{\"end\":102855,\"start\":102836},{\"end\":102869,\"start\":102855},{\"end\":103292,\"start\":103278},{\"end\":103308,\"start\":103292},{\"end\":103320,\"start\":103308},{\"end\":103345,\"start\":103320},{\"end\":103653,\"start\":103639},{\"end\":103667,\"start\":103653},{\"end\":103681,\"start\":103667},{\"end\":103700,\"start\":103681},{\"end\":103717,\"start\":103700},{\"end\":103722,\"start\":103717},{\"end\":104116,\"start\":104104},{\"end\":104124,\"start\":104116},{\"end\":104135,\"start\":104124},{\"end\":104144,\"start\":104135},{\"end\":104154,\"start\":104144},{\"end\":104166,\"start\":104154},{\"end\":104434,\"start\":104423},{\"end\":104447,\"start\":104434},{\"end\":104462,\"start\":104447},{\"end\":104474,\"start\":104462},{\"end\":104495,\"start\":104474},{\"end\":104509,\"start\":104495},{\"end\":104519,\"start\":104509},{\"end\":104532,\"start\":104519},{\"end\":104808,\"start\":104798},{\"end\":105021,\"start\":105003},{\"end\":105033,\"start\":105021},{\"end\":105053,\"start\":105033},{\"end\":105066,\"start\":105053},{\"end\":105547,\"start\":105529},{\"end\":105558,\"start\":105547},{\"end\":105571,\"start\":105558},{\"end\":106055,\"start\":106039},{\"end\":106072,\"start\":106055},{\"end\":106088,\"start\":106072},{\"end\":106106,\"start\":106088},{\"end\":106442,\"start\":106414},{\"end\":106456,\"start\":106442},{\"end\":106466,\"start\":106456},{\"end\":107000,\"start\":106980},{\"end\":107017,\"start\":107000},{\"end\":107030,\"start\":107017},{\"end\":107039,\"start\":107030},{\"end\":107520,\"start\":107503},{\"end\":107541,\"start\":107520},{\"end\":107554,\"start\":107541},{\"end\":107572,\"start\":107554},{\"end\":107586,\"start\":107572},{\"end\":107602,\"start\":107586},{\"end\":107618,\"start\":107602},{\"end\":107995,\"start\":107981},{\"end\":108012,\"start\":107995},{\"end\":108661,\"start\":108647},{\"end\":108675,\"start\":108661},{\"end\":108900,\"start\":108887},{\"end\":108920,\"start\":108900},{\"end\":108933,\"start\":108920},{\"end\":108954,\"start\":108933},{\"end\":109227,\"start\":109214},{\"end\":109241,\"start\":109227},{\"end\":109255,\"start\":109241},{\"end\":109268,\"start\":109255},{\"end\":109803,\"start\":109790},{\"end\":109820,\"start\":109803},{\"end\":110073,\"start\":110057},{\"end\":110084,\"start\":110073},{\"end\":110097,\"start\":110084},{\"end\":110112,\"start\":110097},{\"end\":110132,\"start\":110112},{\"end\":110148,\"start\":110132},{\"end\":110164,\"start\":110148},{\"end\":110602,\"start\":110587},{\"end\":110614,\"start\":110602},{\"end\":110864,\"start\":110849},{\"end\":110878,\"start\":110864},{\"end\":110894,\"start\":110878},{\"end\":111137,\"start\":111123},{\"end\":111150,\"start\":111137},{\"end\":111433,\"start\":111420},{\"end\":111442,\"start\":111433},{\"end\":111458,\"start\":111442},{\"end\":111473,\"start\":111458},{\"end\":111488,\"start\":111473},{\"end\":111510,\"start\":111488},{\"end\":111533,\"start\":111510},{\"end\":112012,\"start\":111999},{\"end\":112024,\"start\":112012},{\"end\":112037,\"start\":112024},{\"end\":112054,\"start\":112037},{\"end\":112076,\"start\":112054},{\"end\":112087,\"start\":112076},{\"end\":112094,\"start\":112087},{\"end\":112469,\"start\":112454},{\"end\":112509,\"start\":112469},{\"end\":112523,\"start\":112509},{\"end\":112537,\"start\":112523},{\"end\":112559,\"start\":112537},{\"end\":112574,\"start\":112559},{\"end\":112590,\"start\":112574},{\"end\":112600,\"start\":112590},{\"end\":112610,\"start\":112600},{\"end\":112616,\"start\":112610},{\"end\":113001,\"start\":112988},{\"end\":113033,\"start\":113001},{\"end\":113046,\"start\":113033},{\"end\":113051,\"start\":113046},{\"end\":113503,\"start\":113494},{\"end\":113514,\"start\":113503},{\"end\":113526,\"start\":113514},{\"end\":113533,\"start\":113526},{\"end\":113545,\"start\":113533},{\"end\":113557,\"start\":113545},{\"end\":113874,\"start\":113860},{\"end\":113887,\"start\":113874},{\"end\":113898,\"start\":113887},{\"end\":113910,\"start\":113898},{\"end\":113918,\"start\":113910},{\"end\":113929,\"start\":113918},{\"end\":114263,\"start\":114250},{\"end\":114277,\"start\":114263},{\"end\":114291,\"start\":114277},{\"end\":114305,\"start\":114291},{\"end\":114316,\"start\":114305},{\"end\":114336,\"start\":114316},{\"end\":114640,\"start\":114626},{\"end\":114654,\"start\":114640},{\"end\":114890,\"start\":114881},{\"end\":114896,\"start\":114890},{\"end\":115025,\"start\":115014},{\"end\":115134,\"start\":115121},{\"end\":115244,\"start\":115232},{\"end\":115251,\"start\":115244},{\"end\":115390,\"start\":115373},{\"end\":115515,\"start\":115492},{\"end\":115524,\"start\":115515},{\"end\":115531,\"start\":115524},{\"end\":115678,\"start\":115672},{\"end\":115842,\"start\":115821},{\"end\":115977,\"start\":115966},{\"end\":116091,\"start\":116073},{\"end\":116100,\"start\":116091},{\"end\":116108,\"start\":116100},{\"end\":116270,\"start\":116265},{\"end\":116491,\"start\":116482},{\"end\":116717,\"start\":116690},{\"end\":116835,\"start\":116829},{\"end\":116991,\"start\":116976},{\"end\":117130,\"start\":117109},{\"end\":117142,\"start\":117130},{\"end\":117150,\"start\":117142},{\"end\":117333,\"start\":117312},{\"end\":117342,\"start\":117333},{\"end\":117350,\"start\":117342},{\"end\":117692,\"start\":117672},{\"end\":117704,\"start\":117692},{\"end\":117712,\"start\":117704},{\"end\":118395,\"start\":118388},{\"end\":118603,\"start\":118599},{\"end\":118790,\"start\":118772},{\"end\":119081,\"start\":119045},{\"end\":119085,\"start\":119081},{\"end\":119228,\"start\":119209},{\"end\":119347,\"start\":119332},{\"end\":119473,\"start\":119447},{\"end\":119481,\"start\":119473},{\"end\":119489,\"start\":119481},{\"end\":119746,\"start\":119740},{\"end\":119846,\"start\":119839},{\"end\":119948,\"start\":119940},{\"end\":120049,\"start\":120043},{\"end\":120160,\"start\":120142},{\"end\":120267,\"start\":120263},{\"end\":120445,\"start\":120432},{\"end\":120452,\"start\":120445},{\"end\":120581,\"start\":120571},{\"end\":120702,\"start\":120676},{\"end\":120708,\"start\":120702},{\"end\":120868,\"start\":120842},{\"end\":120876,\"start\":120868},{\"end\":120884,\"start\":120876},{\"end\":121039,\"start\":121028},{\"end\":121146,\"start\":121135},{\"end\":121153,\"start\":121146},{\"end\":121290,\"start\":121283},{\"end\":121445,\"start\":121437},{\"end\":121545,\"start\":121538},{\"end\":121663,\"start\":121642},{\"end\":121983,\"start\":121965},{\"end\":122092,\"start\":122086},{\"end\":122194,\"start\":122183},{\"end\":122201,\"start\":122194},{\"end\":122541,\"start\":122534},{\"end\":122644,\"start\":122635},{\"end\":122653,\"start\":122644},{\"end\":122819,\"start\":122806},{\"end\":122826,\"start\":122819},{\"end\":122956,\"start\":122949},{\"end\":123096,\"start\":123089},{\"end\":123207,\"start\":123191},{\"end\":123214,\"start\":123207},{\"end\":123457,\"start\":123445},{\"end\":123752,\"start\":123747},{\"end\":123896,\"start\":123890},{\"end\":124035,\"start\":124031},{\"end\":124187,\"start\":124180},{\"end\":124470,\"start\":124463},{\"end\":124568,\"start\":124562},{\"end\":124668,\"start\":124661},{\"end\":124797,\"start\":124782},{\"end\":125362,\"start\":125351},{\"end\":125368,\"start\":125362},{\"end\":125505,\"start\":125488},{\"end\":125511,\"start\":125505},{\"end\":125654,\"start\":125637},{\"end\":125658,\"start\":125654},{\"end\":125776,\"start\":125768},{\"end\":125875,\"start\":125868},{\"end\":126172,\"start\":126160},{\"end\":126178,\"start\":126172},{\"end\":126308,\"start\":126299},{\"end\":126313,\"start\":126308},{\"end\":126446,\"start\":126430},{\"end\":126649,\"start\":126644},{\"end\":126984,\"start\":126964},{\"end\":127112,\"start\":127089},{\"end\":127239,\"start\":127220},{\"end\":127359,\"start\":127343},{\"end\":127472,\"start\":127460},{\"end\":127484,\"start\":127472},{\"end\":127494,\"start\":127484},{\"end\":127646,\"start\":127630},{\"end\":127656,\"start\":127646},{\"end\":127801,\"start\":127785},{\"end\":127810,\"start\":127801},{\"end\":127820,\"start\":127810},{\"end\":127990,\"start\":127985}]", "bib_venue": "[{\"end\":100924,\"start\":100878},{\"end\":101723,\"start\":101629},{\"end\":102262,\"start\":102218},{\"end\":102498,\"start\":102438},{\"end\":102956,\"start\":102869},{\"end\":103435,\"start\":103361},{\"end\":103809,\"start\":103738},{\"end\":104102,\"start\":104027},{\"end\":104556,\"start\":104532},{\"end\":104796,\"start\":104774},{\"end\":104891,\"start\":104851},{\"end\":105152,\"start\":105066},{\"end\":105658,\"start\":105571},{\"end\":106150,\"start\":106106},{\"end\":106553,\"start\":106466},{\"end\":107125,\"start\":107039},{\"end\":107667,\"start\":107618},{\"end\":108154,\"start\":108012},{\"end\":108645,\"start\":108591},{\"end\":108885,\"start\":108831},{\"end\":109366,\"start\":109268},{\"end\":109788,\"start\":109697},{\"end\":110230,\"start\":110164},{\"end\":110585,\"start\":110528},{\"end\":110847,\"start\":110773},{\"end\":111199,\"start\":111150},{\"end\":111619,\"start\":111533},{\"end\":112175,\"start\":112110},{\"end\":112452,\"start\":112397},{\"end\":113120,\"start\":113051},{\"end\":113492,\"start\":113420},{\"end\":113858,\"start\":113768},{\"end\":114248,\"start\":114164},{\"end\":114705,\"start\":114654},{\"end\":116381,\"start\":116360},{\"end\":116590,\"start\":116585},{\"end\":117545,\"start\":117508},{\"end\":117908,\"start\":117871},{\"end\":118050,\"start\":118036},{\"end\":118160,\"start\":118156},{\"end\":118284,\"start\":118272},{\"end\":118497,\"start\":118485},{\"end\":118923,\"start\":118895},{\"end\":119643,\"start\":119637},{\"end\":121314,\"start\":121290},{\"end\":121852,\"start\":121830},{\"end\":122332,\"start\":122320},{\"end\":122442,\"start\":122439},{\"end\":122670,\"start\":122653},{\"end\":123345,\"start\":123326},{\"end\":123586,\"start\":123556},{\"end\":123745,\"start\":123705},{\"end\":124209,\"start\":124187},{\"end\":124352,\"start\":124330},{\"end\":124819,\"start\":124797},{\"end\":124992,\"start\":124962},{\"end\":125116,\"start\":125111},{\"end\":125236,\"start\":125210},{\"end\":126030,\"start\":125987},{\"end\":126551,\"start\":126547},{\"end\":126671,\"start\":126649},{\"end\":126845,\"start\":126815},{\"end\":127983,\"start\":127955},{\"end\":128118,\"start\":128115},{\"end\":128231,\"start\":128214},{\"end\":101804,\"start\":101725},{\"end\":103030,\"start\":102958},{\"end\":104567,\"start\":104558},{\"end\":105225,\"start\":105154},{\"end\":105732,\"start\":105660},{\"end\":106627,\"start\":106555},{\"end\":107198,\"start\":107127},{\"end\":108283,\"start\":108156},{\"end\":109451,\"start\":109368},{\"end\":110283,\"start\":110232},{\"end\":111692,\"start\":111621},{\"end\":113176,\"start\":113122}]"}}}, "year": 2023, "month": 12, "day": 17}