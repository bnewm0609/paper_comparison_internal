{"id": 258437096, "updated": "2023-11-27 13:29:30.782", "metadata": {"title": "Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation", "authors": "[{\"first\":\"Yuval\",\"last\":\"Kirstain\",\"middle\":[]},{\"first\":\"Adam\",\"last\":\"Polyak\",\"middle\":[]},{\"first\":\"Uriel\",\"last\":\"Singer\",\"middle\":[]},{\"first\":\"Shahbuland\",\"last\":\"Matiana\",\"middle\":[]},{\"first\":\"Joe\",\"last\":\"Penna\",\"middle\":[]},{\"first\":\"Omer\",\"last\":\"Levy\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images. We leverage this dataset to train a CLIP-based scoring function, PickScore, which exhibits superhuman performance on the task of predicting human preferences. Then, we test PickScore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. Therefore, we recommend using PickScore for evaluating future text-to-image generation models, and using Pick-a-Pic prompts as a more relevant dataset than MS-COCO. Finally, we demonstrate how PickScore can enhance existing text-to-image models via ranking.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-01569", "doi": "10.48550/arxiv.2305.01569"}}, "content": {"source": {"pdf_hash": "291fdfc768ea76805abea2b276dca646a60f4de7", "pdf_src": "ArXiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.01569v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2305.01569", "status": "CLOSED"}}, "grobid": {"id": "de32f9c86018b8108b03b91eced499dd45d2c69c", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/291fdfc768ea76805abea2b276dca646a60f4de7.txt", "contents": "\nPick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation\n23 Nov 2023\n\nYuval Kirstain yuval.kirstain@cs.tau.ac.il \nTel Aviv University \u03c3 Stability AI\n\n\nAdam Polyak \nTel Aviv University \u03c3 Stability AI\n\n\nUriel Singer \nTel Aviv University \u03c3 Stability AI\n\n\nShahbuland Matiana \nTel Aviv University \u03c3 Stability AI\n\n\nJoe Penna \nTel Aviv University \u03c3 Stability AI\n\n\nOmer Levy \nTel Aviv University \u03c3 Stability AI\n\n\nPick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation\n23 Nov 20234F854AA83AB5001049842C43F2D3B5F4arXiv:2305.01569v2[cs.CV]\nThe ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public.To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences.Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users' preferences over generated images.We leverage this dataset to train a CLIP-based scoring function, PickScore, which exhibits superhuman performance on the task of predicting human preferences.Then, we test PickScore's ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics.Therefore, we recommend using PickScore for evaluating future text-to-image generation models, and using Pick-a-Pic prompts as a more relevant dataset than MS-COCO.Finally, we demonstrate how PickScore can enhance existing text-to-image models via ranking. 1 \"jedi duck holding a lightsaber\" \"a square green owl made of fimo\" \"Two-faced biomechanical cyborg...\" \"insanely detailed portrait, wise man\" \"A bird with 8 spider legs\" \"A butterfly flying above an ocean\"\n\nIntroduction\n\nRecent advances in aligning language models with user behaviors and expectations have placed a significant emphasis on the ability to model user preferences [10,1,3].However, little attention has been paid to this ability in the realm of text-to-image generation.This lack of attention can largely be attributed to the absence of a large and open dataset of human preferences over state-of-the-art image generation models.\n\nTo fill this void, we create a web application that enables users to generate images using state-of-theart text-to-image models while specifying their preferences.With explicit consent from the users, we collect their prompts and preferences to create Pick-a-Pic, a publicly available dataset comprising over half-a-million examples of human preferences over model-generated images. 2 Each example in our dataset includes a prompt, two generated images, and a label indicating the preferred image, or a tie when no image is significantly preferred over the other.Notably, Pick-a-Pic was created by real users with a genuine interest in generating images.This interest differs from that of crowd workers who lack the intrinsic motivation to produce creative prompts or the original intent of the prompt's author to judge which image better aligns with their needs.\n\nTapping into authentic user preferences allows us to train a scoring function that estimates the user's satisfaction from a particular generated image given a prompt.To train such a scoring function we finetune CLIP-H [12,7] using human preference data and an analogous objective to that of InstructGPT's reward model [10].This objective aims to maximize the probability of a preferred image being picked over an unpreferred one, or even the probability in cases of a tie.We find that the resulting scoring function, PickScore 3 , achieves superhuman performance in the task of predicting user preferences (a 70.5% accuracy rate, compared to humans' 68.0%), while zero-shot CLIP-H (60.8%) and the popular aesthetics predictor [14] (56.8%) perform closer to chance (56.8%).\n\nEquipped with a dataset for human preferences and a state-of-the-art scoring function, we propose updating the standard protocol for evaluating text-to-image generation models.First, we suggest that researchers evaluate their text-to-image models using prompts from Pick-a-Pic, which better represent what humans want to generate than mundane captions, such as those found in MS-COCO [2,9].Second, to compare PickScore with FID, we conduct a human evaluation study and find that even when evaluated against MS-COCO captions, PickScore exhibits a strong correlation with human preferences (0.917), while ranking with FID yields a negative correlation (-0.900).Importantly, we also compare PickScore with other evaluation metrics using model rankings inferred from real user preferences.We observe that PickScore is more strongly correlated with ground truth rankings, as determined by real users, than other evaluation metrics.Thus, we recommend using PickScore as a more reliable evaluation metric than existing ones.\n\nFinally, we explore how PickScore can improve the quality of vanilla text-to-image models via ranking.To accomplish this, we generate images with different initial random noises as well as different templates (e.g.\"breathtaking [prompt].award-winning, professional, highly detailed\") to slightly alter the user prompt.We then test the impact of selecting the top image according to different scoring functions.Our findings indicate that human raters prefer images selected by PickScore more than those selected by CLIP-H [7] (win rate of 71.3%), an aesthetics predictor [14] (win rate of 85.1%), and the vanilla text-to-image model (win rate of 71.4%).\n\nIn summary, the presented work addresses a gap in the field of text-to-image generation by creating a large, open, high-quality dataset of human preferences over user-prompted model-generated images.We demonstrate the potential of this dataset by training a scoring function, PickScore, which exhibits a performance superior to any other publicly-available automatic scoring function, in predicting human preferences, evaluating text-to-image models, and improving them via ranking.We encourage the research community to adopt Pick-a-Pic and PickScore as a basis for further advances in text-to-image modeling and incorporating human preferences into the learning process. 2 Pick-a-Pic Dataset\n\nThe Pick-a-Pic dataset 4 was created by logging user interactions with the Pick-a-Pic web application for text-to-image generation.Overall, the Pick-a-Pic dataset contains over 500,000 examples and 35,000 distinct prompts.Each example contains a prompt, two generated images, and a label for which image is preferred, or if there is a tie when no image is significantly preferred over the other.The images in the dataset were generated by employing multiple backbone models, namely, Stable Diffusion 2.1, Dreamlike Photoreal 2.05 , and Stable Diffusion XL variants [13] while sampling different classifier-free guidance scale values [6].As we continue with our efforts to collect more user interactions through the Pick-a-Pic web app and decrease the number of NSFW examples included in the dataset, we will periodically upload new revisions of the dataset.\n\nThe Pick-a-Pic Web App To ensure maximum accessibility for a wide range of users, the user interface was designed with simplicity in mind.The application allows users to write creative prompts and generate images.At each turn, the user is presented with two generated images (conditioned on their prompt), and asked to select their preferred option or indicate a tie if they have no strong preference.Upon selection, the rejected (non-preferred) image is replaced with a newly generated image, and the process repeats.The user can also clear or edit the prompt at any time, and the app will generate new images appropriately.Figure 2 illustrates the usage flow.\n\nReal Data from Real Users A key advantage of Pick-a-Pic is that our data is collected from real, intrinsically-motivated users, rather than paid crowd workers.We achieve this by approaching a wide audience through various social media channels such as Twitter, Facebook, Discord, and Reddit.At the same time, we mitigate the risk of collecting low-quality data resulting from potential misuse of the application by implementing several quality control measures.First, users are required to authenticate their identity using either a Gmail or a Discord account. 6Second, we closely monitor user activity logs and take action to ban users who generate NSFW content, use multiple instances of the web app simultaneously, or make judgments at an unreasonably fast pace.Third, we use a list of NSFW phrases to prevent users from generating harmful content.Last, we limit users to 1000 interactions and periodically increase the limit.These measures work in tandem to ensure the integrity and reliability of Pick-a-Pic's data.with ties.We found that the latter option (2 images, with ties) exceeds the other two in terms of user engagement and inter-rater agreement.\n\nPreprocessing When processing the collected interactions, we filter prompts with NSFW phrases and banned users.We acknowledge that there are still NSFW images and prompts, and will periodically attempt to update the dataset and reduce such occurrences.To divide the dataset into training, validation, and testing subsets, we first sample one thousand prompts, ensuring that each prompt was created by a unique user.Next, we randomly divide those prompts into two sets of equal size to create the validation and test sets.We then sample exactly one example for each prompt to include in these sets.For the training set, we include all examples that do not share a prompt with the validation and test sets.This approach ensures that no split shares prompts with another split, and the validation and test sets do not suffer from being non-proportionally fitted to a specific prompt or user.\n\nStatistics Since the creation of the Pick-a-Pic web app we have gathered 968,965 rankings which originated from 66,798 prompts and 6,394 users.However, as the Pick-a-Pic dataset is constantly updating, the reported experiments in this paper involve an NSFW filtered and not fully updated version of Pick-a-Pic, that contains 583,747 training examples, and 500 validation and test examples.\n\nThe training set of this dataset contains 37,523 prompts from 4,375 distinct users.\n\n\nModel Selection and Evaluation\n\nThe Pick-a-Pic dataset offers a unique opportunity for a model selection and evaluation methodology, leveraging users' preferences for unbiased analysis.To illustrate this opportunity, we use the collected data and analyze the impact of changing the classifierfree guidance scale of Stable Diffusion XL (Alpha variant) on its performance.Specifically, we compare human preferences made when both images were generated by Stable Diffusion XL (Alpha variant) but using different classifier-free guidance scales 7 .For each scale, we compute the win ratio, representing the percentage of judgments where its use led to a preferred image.We also calculate the corresponding tie and lose ratios for each scale, enabling a detailed analysis of which classifier-free guidance scales are more effective.Our results are depicted in Figure 3 (a), and verify for example, that a guidance scale of 9 usually yields preferred images when compared to a guidance scale of 3.\n\nFurthermore, by examining user preferences between images generated by different backbone models, we can determine which model is preferred more by users.For instance, considering judgments in which one image was generated by Dreamlike Photoreal 2.0 and the other by Stable Diffusion 2.1, we can evaluate which model is more performant.As shown in fig. 3 (b), users usually prefer Dreamlike Photoreal 2.0 over Stable Diffusion 2.1.We encourage researchers to contact us and include their text-to-image models in the Pick-a-Pic web app for the purpose of model selection and evaluation.\n\n\nPickScore\n\nOne valuable outcome from collecting a large, natural dataset of user preferences is that we can use it to train a function that scores the quality of a generated image given a prompt.We train the PickScore scoring function over Pick-a-Pic by combining a CLIP-style model with a variant of 7 The frequency of different guidance scales is similar.\n\nInstructGPT's reward model objective [10].PickScore is able to predict user preferences in held-out Pick-a-Pic prompts better than any other publicly-available scoring function, surpassing even expert human annotators (Section 4).Such a scoring function can be of value for various scenarios, such as performing model evaluation (Section 5), increasing the quality of generated images via ranking (Section 6), building better large-scale datasets to improve text-to-image models [14], and improving text-to-image models through weak supervision (e.g.RLHF).\n\nModel PickScore follows the architecture of CLIP [12]; given a prompt x and an image y, our scoring function s computes a real number by representing x using a transformer text encoder and y using a transformer image encoder as d-dimensional vectors, and returning their inner product:\ns(x, y) = E txt (x) \u2022 E img (y) \u2022 T (1)\nWhere T is the learned scalar temperature parameter of CLIP.\n\nObjective The input for our objective includes a scoring function s, a prompt x, two images y 1 , y 2 , and a preference distribution vector p, which captures the user's preference over the two images.Specifically, p takes a value of\n[1, 0] if y 1 is preferred, [0, 1] if y 2 is preferred, or [0.5, 0.5]\nfor ties.Given this input, the objective optimizes the scoring function's parameters by minimizing the KL-divergence between the preference p and the softmax-normalized scores of y 1 and y 2 :\npi = exp s(x, y i ) 2 j=1 exp s(x, y j )(2)L pref = 2 i=1 p i (log p i \u2212 log pi )(3)\nSince many examples can originate from the same prompt, we mitigate the risk of overfitting to a small set of prompts by applying a weighted average when reducing the loss across examples in the batch.Specifically, we weigh each example in the batch, with an inverse proportion to its prompt frequency in the dataset.This objective is analogous to InstructGPT's reward model objective [10].\n\nWe also experiment with incorporating in-batch negatives into the objective, but find that this yields a less accurate scoring function (see Appendix).\n\nTraining We finetune CLIP-H [7] using our framework 8 on the Pick-a-Pic training set.We train the model for 4,000 steps, with a learning rate of 3e-6, a total batch size of 128, and a warmup period of 500 steps, which follows a linearly decaying learning rate; the experiment is completed in less than an hour with 8 A100 GPUs.We did not perform hyperparameter search, which might further improve results.For model selection, we evaluate the model's accuracy on the validation set (without the option for a tie) in intervals of 100 steps, and keep the best-performing checkpoint.\n\n\nPreference Prediction\n\nWe first evaluate PickScore on the task it was trained to do: predict human preferences.We find that PickScore outperforms all other baselines, including expert human annotators.\n\nMetric To evaluate the ability of models to predict human preferences, we use an adapted accuracy metric that accounts for the possibility of a tie.Our metric assigns one point to the model for predicting the same label as the user, half a point if either label or prediction is a tie (but not both), and zero otherwise.\n\nTie Threshold Selection Utilizing the notation specified in Section 3, each model requires a tie threshold probability t to predict a tied outcome when |p 1 \u2212 p2 | < t.To achieve this, we evaluate each model on the validation set using various tie threshold probabilities and subsequently determine the most optimal tie threshold for each model.For human experts, we do not perform tie selection and explicitly allow them to select a tie.\n\n\"Highway to hell\" \"Robot\"\n\n\nCLIP-H\n\nPickScore \"\u2026 animal with fur of a great ape\u2026 \" \"A demon exiting through a portal\u2026\"\n\n\"\u2026beautiful girl\u2026 field of flowers\"\n\n\"A cat on a propaganda poster\"   Aesthetics [14] 56.8 CLIP-H [7] 60.8 ImageReward [18] 61.1 HPS [17] 66.7 PickScore (Ours) 70.5\n\n\nCLIP-H PickScore\n\n(b) Performance on the Pick-a-Pic test set.\n\nBaselines We compare our model with CLIP-H [7], an aesthetics predictor [14] built on top of CLIP-L [12], a random classifier, and human experts. 9For completeness, we also compare our results with models from concurrent work, namely, HPS [17] and ImageReward [18].\n\nResults First, we compare the models' performance on the validation set across different tie thresholds.Figure 1a shows that PickScore outperforms the baselines across almost all thresholds, and achieves the highest global score by a wide margin.After selecting the best-performing tie threshold for each model, we use the threshold to evaluate the different models on the test set.\n\nTable 1b shows that the aesthetics score (56.8) and CLIP-H (60.8) perform closer to a random chance baseline (56.8), while PickScore (70.5 \u00b1 0.142)10 achieves superhuman performance, as it even outperforms human experts (68.0).It is important to emphasize a core difference between real users that produce the ground truth labels and annotators used to evaluate human performance.The users that produce the ground truth are actual text-to-image users, which have an idea (which may be incomplete) for an image, and invent a prompt (which may lack details) with hope that the resulting image will match their preferences.In contrast, annotators that are used to measure human performance are oblivious to the original user's context, idea, and motivation.Therefore, superhuman performance on this task means that the model is able to outperform a human annotator that is oblivious to the original user's context, idea, and motivation.The superhuman performance of PickScore showcases the importance of using real users as ground truth rather than expert annotators when collecting human preferences.Moreover, the relatively modest human performance (68.0) on the task, when compared to a random baseline (56.8), shows that predicting human preferences in text-to-image generation is a difficult task for human annotators.\n\nFor completeness, we also include the results of concurrent work from HPS [17], which scores 66.7, and ImageReward [18], which scores 61.1; PickScore outperforms both.To further illustrate the differences between CLIP-H and PickScore, we showcase examples of disagreement from the Pick-a-Pic validation set in Figure 4. We notice that PickScore often chooses more aesthetically pleasing images than CLIP-H; at times, at the cost of faithfulness to the prompt.7: Correlation between Elo ratings of real users and Elo ratings by CLIP-H, ImageReward [18], HPS [17], and PickScore.\n\n\nModel Evaluation\n\nDespite significant progress in the generative capabilities of text-to-image models, the standard and most popular prompt dataset has remained the Microsoft Common Objects in Context dataset (MS-COCO) [9].Similarly, the Fr\u00e9chet Inception Distance (FID) [5] is still the main metric used for model evaluation.In this section, we explain why we recommend researchers to evaluate their models using prompts from Pick-a-Pic rather than (or at least alongside) MS-COCO, and show that when evaluating state-of-the-art text-to-image models, PickScore is more aligned with human judgments than other automatic evaluation metrics.\n\n\nModel Evaluation Prompts\n\nThe prompts contained in the MS-COCO dataset are captions of photographs taken by amateur photographers, depicting objects and humans in everyday settings.While certain captions within this dataset may pose a challenge to text-to-image models, it is evident that the scope of interest for text-to-image users extends beyond commonplace objects and humans.Moreover, the main use-case of image generation is arguably to generate fiction, which cannot be captured by camera.By construction, Pick-a-Pic's prompts are sampled from real users, and thus better represent the natural distribution of text-to-image intents.We therefore strongly advocate that the research community employ prompts from Pick-a-Pic when assessing the performance of text-to-image models.\n\nFID The FID metric [5] gauges the degree of resemblance between a set of generated images and a set of authentic images, at the set level.To do so, it first embeds the real and generated images into the feature space of an Inception net [15], and then estimates the mean and covariance of both sets of images and calculates their similarity.FID is thus geared towards measuring the realism of a set of images, but is oblivious to the prompts.In contrast, PickScore provides a per-instance score, and is directly conditioned on the prompt.We thus hypothesize that PickScore will correlate better with human judgements of generation quality.\n\nTo empirically test our hypothesis with the most \"convenient\" settings for the FID metric, we select 100 random captions from MS-COCO validation split.For each caption, we generate images from 9 different models based on the same set of prompts, and ask human experts to rank the 9 generated images (with ties), inducing pairwise preferences.Specifically, we use Stable Diffusion 1.5, Stable Diffusion 2.1, and Dreamlike Photoreal 2.0 combined with three different classifier-free guidance scales (3, 6, and 9).We then repeat the labeling process (over the same images) using FID, and PickScore instead of humans to determine preferences.Since FID does not operate on a per-example base, when \"labeling\" with FID, we simply choose the model that has a lower (better) FID score on MS-COCO.\n\nFigure 6 shows the correlation between model win rates induced by human rankings (horizontal) and model win rates induced by each automatic scoring function.PickScore exhibits a stronger correlation (0.917) with human raters on MS-COCO captions than FID (-0.900), which surprisingly, exhibits a strong negative correlation.As FID is oblivious to the prompt, one would expect zero correlation, and not a strong negative correlation.We hypothesize that this is related to the classifier-free guidance scale hyperparameter -larger scales tend to produce more vivid images (which humans typically prefer), but differ from the distribution of ground truth images in MS-COCO, yielding worse (higher) FID scores.Figure 5 visualizes these differences by presenting pairs of images generated with the same random seed but with different classifier-free guidance (CFG) scales.\n\nOther Evaluation Metrics When comparing with evaluation metrics that do not assume a set of ground truth images, we are able to use a more reliable evaluation procedure.In this evaluation procedure, we consider real user preferences rather than human annotators, as well as more models (i.e. more data points).Specifically, we take all the 14,000 collected preferences that correspond to prompts from the Pick-a-Pic test set.This set of examples contains images generated by 45 different models -four different backbone models, each with different guidance scales.We then use these real user preferences to calculate Elo ratings [4] for the different models.Afterward, we repeat the process while replacing the real user preferences with CLIP-H [7], and PickScore predictions.Similarly to Section 4, we also compare against the concurrent work from ImageReward [18] and HPS [17].Since the Elo rating system is iterative by nature, we repeat the process 50 times.Each time we randomly shuffle the order of examples, and calculate the correlation with human ratings.Finally, for each metric, we output the mean and standard deviation of its 50 corresponding correlations.Figure 7 displays the correlation between real users' Elo ratings with the different metrics' rating, showing that PickScore exhibits a stronger correlation (0.790 \u00b1 0.054) with real users than all other automatic metrics, namely CLIP-H (0.313 \u00b1 0.075), ImageReward (0.492 \u00b1 0.086), and HPS (0.670 \u00b1 0.071).\n\n\nText-to-Image Ranking\n\nAnother possible application for scoring functions is improving the performance of generations made by text-to-image models through ranking: generate a sample of images, and select the one with the highest score.To test this approach, we generate one hundred images for each prompt of the one hundred prompts we take from the Pick-a-Pic test set.We generate the images with Dreamlike Photoreal 2.0, using a classifier-free guidance scale of 7.5, and to increase image diversity, we use 5 initial random noises and 20 different prompt templates.These templates include the null template \"[prompt]\" and other templates like \"breathtaking [prompt].award-winning, professional, highly detailed\".From each set of 100 generated images, we select the best one according to PickScore, CLIP-H, the aesthetics score, or randomly; in addition, we randomly select one image from the null template for control.We then ask expert human annotators to compare PickScore's chosen image to each of the other functions' choices, and decide which one they prefer.\n\nTable 2 shows that PickScore consistently selects more preferable images than the baselines.By manually analyzing some examples, we find that PickScore typically selects images that are both more aesthetic and better aligned with the prompt.We also measure this explicitly by using the aesthetic scoring function instead of a human rater when comparing PickScore's choice to CLIP-H's,  and find that for 68.5% of the prompts, PickScore selects an image with a higher aesthetic score.Likewise, when comparing PickScore to the aesthetic scorer, 90.5% of PickScore's choices have a higher CLIP-H text-alignment score than the images chosen by the aesthetic scorer.Figure 8 visualizes the benefits of selecting images with PickScore.\n\n\nRelated Work\n\nCollecting and learning from human preferences is an active area of research in natural language processing (NLP) [3,1,10].However, in the domain of text-to-image generation, related research questions have received little attention.One notable work that focuses on collecting human judgments in text-to-image generation is the Simulacra Aesthetic Captions (SAC) dataset [11].This dataset contains almost 200,000 human ratings of generated images.However, unlike Pick-a-Pic which focuses on general user preferences, and allows users to compare between generated images, the human raters of SAC provide an absolute score for the aesthetic quality of the generated images.\n\nThere has been some concurrent work that involves collection and learning from human preferences that we describe below.Lee et al. [8] consider three simple categories of challenges (count, color, and background), enumerating through templates (e.g.\"[number] dogs\") to synthetically create prompts.Then, they instruct crowd workers to choose if a generated image is good or bad and use the collected 30,000 examples to train a scoring function.In contrast, Pick-a-Pick allows real users to write any prompts they choose, and provide pairwise comparisons between images that yield more than 500,000 examples.\n\nImageReward [18] selects prompts and images from the DiffusionDB dataset [16], and collects for them image preference ratings via crowd workers.Since they employ crowd workers that lack the intrinsic motivation to select images that they prefer, the authors define criteria to assess the quality of generated images and instruct crowd workers to follow these criteria when ranking images.They use this strategy to collect 136,892 examples which originate from 8,878 prompts.Importantly, they have not publicly released this dataset.For completeness, we tested ImageReward on the Pick-a-Pic dataset and confirmed that PickScore outperforms it.\n\nAnother concurrent work from Wu et al. [17] collects a dataset of human judgments by scraping about 25,000 human ratings (which include about 100,000 images) from the Discord channel of StabilityAI.Similarly to us, they use an objective analogous to that of InstructGPT to train a scoring function that they name Human Preference Score (HPS).As with ImageReward, we evaluated HPS on Pick-a-Pic and saw that PickScore achieves better performance.\n\nThe observed superior performance of PickScore over the concurrent work from both HPS and ImageReward on the Pick-a-Pic dataset could be attributed to several factors.For example, differences in implementation (e.g., model size, backbone, hyperparameters), differences in the scales of data, or variations in the distribution of data.Notably, Pick-a-Pic is more than five times larger than the data used to train HPS and ImageReward.Furthermore, ImageReward collects judgments from crowd workers, which may lead to significant differences in data distribution.In contrast, HPS scrapes ratings from the StabilityAI discord channel for real text-to-image users but may be more aligned with this more specific distribution of text-to-image users.We leave Isolating and identifying the specific effects of these factors for future work.\n\n\nLimitations and Broader Impact\n\nIt is important to acknowledge that despite our efforts to ensure data quality (see section 2), some images and prompts may contain NSFW content that could potentially bias the data, and some users may have made judgments without due care.Moreover, the preferences of users may include biases that may be reflected in the collected data.These limitations may affect the overall quality and reliability of the data collected and should be taken into consideration when considering the broader impact of the dataset.Nonetheless, we believe that the advantages of collecting data from intrinsically-motivated users and publicly releasing it will enable the text-to-image community to better align text-to-image models with human preferences.\n\n\nConclusions\n\nWe build a web application that serves text-to-image users and (willingly) collects their preferences.We use the collected data and present the Pick-a-Pic dataset: an open dataset of over half-a-million examples of text-to-image prompts, generated images, and user-labeled preferences.The quantity and quality of the data enables us to train PickScore, a state-of-the-art text-image scoring function, which achieves superhuman performance when predicting user preferences.PickScore aligns better with human judgements than any other publicly-available automatic metric, and together with Pick-a-Pic's natural distribution prompts, enables much more relevant text-to-image model evaluation than existing evaluation standards, such as FID over MS-COCO.Finally, we demonstrate the effectiveness of using our scoring function for selecting images in improving the quality of text-to-image models.There are still many opportunities for building upon Pick-a-Pic and PickScore, such as RLHF and other alignment approaches, and we are excited to see how the research community will utilize this work in the near future.\n\nFigure 1 :\n1\nFigure 1: Images generated via our web application, showing darkened non-preferred images (left) and preferred images (right).\n\n\nFigure 2 :\n2\nFigure 2: How Pick-a-Pic data is collected through the app: (a) the user first writes a caption, and receives two images; (b) the user makes a preference judgment; (c) a new image is presented instead of the rejected image.This flow repeats itself until the user changes the prompt.\n\n\nFigure 3 :\n3\nFigure 3: The Pick-a-Pic dataset enables us to perform model selection (a), and model evaluation (b).\n\n\nFigure 4 :\n4\nFigure 4: Disagreement between CLIP-H (left) and PickScore (right) on the Pick-a-Pic validation set.We add green borders around images that humans preferred.\n\n\n\" 9 \"Figure 5 :\n95\nFigure 5: Images generated using the same seed and model, but using different classifier-free guidance (CFG) scales.Even though high guidance scales lead to worse FID, humans usually find them more pleasing than low guidance scales.\n\n\nFigure 6 :\n6\nFigure 6: Correlation between the win ratio of different models according to FID and PickScore to human experts on the MS-COCO validation set.\n\n\nFigure\n\nFigure 7: Correlation between Elo ratings of real users and Elo ratings by CLIP-H, ImageReward[18], HPS[17], and PickScore.\n\n\nFigure 8 :\n8\nFigure 8: Comparing the image from the vanilla text-to-image model (left) with the image selected by PickScore from a set of 100 generations (right).\n\n\nTable 1 :\n1\nQuantitative results on Pick-a-Pic.\n\n\nTable 2 :\n2\nPercentage of instances where humans prefer PickScore's choice over another scoring function's choice when selecting one image out of 100.\nComparisonWin RatePickScore vs Random Seed+ Null Template71.4+ Random Template82.0PickScore vs Aesthetics [14]85.1PickScore vs CLIP-H [7]71.3VanillaPickScoreVanillaPickScoreVanillaPickScoreVanillaPickScore\"Highway to hell\"\"A private futuristic cabin on\u2026 mars\u2026\"\"A bee devouring the world\"\"Silver furred Lion man hybrid\u2026\"\"Superman's cropse found\"\"A cute fluffy easter bunny singing\"\"A mushroom inspired car concept\"\"Car made of zebra skin\"\nhttps://github.com/yuvalkirstain/PickScore 37th Conference on Neural Information Processing Systems (NeurIPS\n).\nWe recently open-sourced a new version of the Pick-a-Pic dataset that has more than one million examples.\nThe model is available at https://huggingface.co/yuvalkirstain/PickScore_v1\nThe dataset is available at https://huggingface.co/datasets/yuvalkirstain/pickapic_v1, and its updated version is available at https://huggingface.co/datasets/yuvalkirstain/pickapic_v2.\nDreamlike Photoreal 2.0 is a finetuned version of Stable Diffusion 1.5\nAll users provide explicit consent to share their interactions with Pick-a-Pic's application as part of a public dataset. To ensure privacy, we anonymize user IDs.\nhttps://github.com/yuvalkirstain/PickScore\nThroughout this paper, we use friends and colleagues of the authors for expert human annotation.\nWe train PickScore using three different random seeds and report the mean and variance.\nAcknowledgmentsWe gratefully acknowledge the support of Stability AI, the Google TRC program, and Jonathan Berant, who provided us with invaluable compute resources, credits, and storage that were crucial to the success of this project.We extend our sincerest thanks for their contributions and support.Furthermore, we extend our gratitude to Roni Herman, Or Honovich, Avia Efrat, Samuel Amouyal, Tomer Ronen, Uri Shaham, Maor Ivgi, Itay Itzhak, Illy Sachs, and other friends and colleagues for their contribution to the annotation endeavor of this project.Appendix Comparing Pick-a-Pic Prompts with MS-COCO CaptionsTo illustrate the difference between MS-COCO captions and Pick-a-Pic prompts we show prompts from each dataset.Pick-a-Pic -\"forest with ruins, photo\", \"A panda bear as a mad scientist\", \"product photo of a sneakers\", \"photo of a bicycle, detailed, 8k uhd, dslr, high quality, film grain, Fujifilm XT3\", \"female portrait photo\", \"Alexander the great, cover art, colorful\", \"A galactic eldritch squid towering over the planet Earth, stars, galaxies and nebulas in the background...\", \"Portrait of a giant, fluffy, ninja teddy bear\", \"insanely detailed portrait, darth vader, shiny, extremely intricate, high res, 8k, award winning\", \"Giant ice cream cone melting and creating a river through a city\".MS-COCO -\"A man with a red helmet on a small moped on a dirt road.\",\"A woman wearing a net on her head cutting a cake.\",\"there is a woman that is cutting a white cake\", \"a little boy wearing headphones and looking at a computer monitor\", \"A young girl is preparing to blow out her candle.\",\"A commercial stainless kitchen with a pot of food cooking.\",\"Two men that are standing in a kitchen.\",\"A man riding a bike past a train traveling along tracks.\",\"The pantry door of the small kitchen is closed.\",\"A man is doing a trick on a skateboard\".Training a Scoring FunctionWe further explored an alternative loss function that is closer to CLIP's original objective.In this loss function, we also incorporate the remaining examples in the batch as in-batch negatives.To elaborate, considering the notation established in section 3 and y k 1 , y k 2 denoting the images corresponding to the k-th example in the batch, we formulate pk as follows:We anticipated that this objective function would maintain the general capabilities of CLIP with minimal loss in performance.However, our findings demonstrated that PickScore significantly outperforms this objective function, as the latter only produced a scoring function that achieves an accuracy of 65.2 on the Pick-a-Pic test set.\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, T J Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, ArXiv, abs/2204.05862Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and Jared Kaplan2022Neel Nanda, Catherine Olsson\n\nMicrosoft coco captions: Data collection and evaluation server. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence Zitnick, ArXiv, abs/1504.003252015\n\nDeep reinforcement learning from human preferences. Paul Francis, Christiano , Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, Dario Amodei, ArXiv, abs/1706.037412017\n\nThe rating of chessplayers, past and present. E Arpad, Elo, 1978\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, NIPS. 2017\n\nClassifier-free diffusion guidance. Jonathan Ho, ArXiv, abs/2207.125982022\n\n. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, Ludwig Schmidt, Openclip, July 2021If you use this software, please cite it as below\n\nAligning text-to-image models using human feedback. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, P Abbeel, Mohammad Ghavamzadeh, Shixiang Shane Gu, ArXiv, abs/2302.121922023\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge J Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. 2014\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, Ryan J Lowe, ArXiv, abs/2203.021552022\n\nKatherine Crowson, and Simulacra Captions Contributors. Simulacra aesthetic captions. John David, Pressman , Version 1.0Stability AI. 2022Technical Report\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, International Conference on Machine Learning. 2021\n\nHigh-resolution image synthesis with latent diffusion models. Robin Rombach, A Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2022. 2021\n\nLAION-5b: An open large-scale dataset for training next generation image-text models. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Ross Cade W Gordon, Mehdi Wightman, Theo Cherti, Aarush Coombes, Clayton Katta, Mitchell Mullis, Patrick Wortsman, Schramowski, Katherine Srivatsa R Kundurthy, Ludwig Crowson, Robert Schmidt, Jenia Kaczmarczyk, Jitsev, Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016. 2015\n\nDiffusiondb: A large-scale prompt gallery dataset for text-to-image generative models. J Zijie, Evan Wang, David Montoya, Haoyang Munechika, Benjamin Yang, Duen Hoover, Chau Horng, ArXiv, abs/2210.148962022\n\nBetter aligning text-to-image models with human preference. Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li, ArXiv, abs/2303.144202023\n\nImagereward: Learning and evaluating human preferences for text-to-image generation. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, Yuxiao Dong, ArXiv, abs/2304.059772023\n", "annotations": {"author": "[{\"end\":171,\"start\":91},{\"end\":221,\"start\":172},{\"end\":272,\"start\":222},{\"end\":329,\"start\":273},{\"end\":377,\"start\":330},{\"end\":425,\"start\":378}]", "publisher": null, "author_last_name": "[{\"end\":105,\"start\":97},{\"end\":183,\"start\":177},{\"end\":234,\"start\":228},{\"end\":291,\"start\":284},{\"end\":339,\"start\":334},{\"end\":387,\"start\":383}]", "author_first_name": "[{\"end\":96,\"start\":91},{\"end\":176,\"start\":172},{\"end\":227,\"start\":222},{\"end\":283,\"start\":273},{\"end\":333,\"start\":330},{\"end\":382,\"start\":378}]", "author_affiliation": "[{\"end\":170,\"start\":135},{\"end\":220,\"start\":185},{\"end\":271,\"start\":236},{\"end\":328,\"start\":293},{\"end\":376,\"start\":341},{\"end\":424,\"start\":389}]", "title": "[{\"end\":77,\"start\":1},{\"end\":502,\"start\":426}]", "venue": null, "abstract": "[{\"end\":1782,\"start\":572}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1959,\"start\":1955},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1961,\"start\":1959},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1963,\"start\":1961},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3309,\"start\":3305},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3311,\"start\":3309},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3409,\"start\":3405},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3817,\"start\":3813},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4248,\"start\":4245},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4250,\"start\":4248},{\"end\":5116,\"start\":5108},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5404,\"start\":5401},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5454,\"start\":5450},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6798,\"start\":6794},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6865,\"start\":6862},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12261,\"start\":12257},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12703,\"start\":12699},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12831,\"start\":12827},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14137,\"start\":14133},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14324,\"start\":14321},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16045,\"start\":16041},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16061,\"start\":16058},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16083,\"start\":16079},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16097,\"start\":16093},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16236,\"start\":16233},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16266,\"start\":16262},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16294,\"start\":16290},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16433,\"start\":16429},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16454,\"start\":16450},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18241,\"start\":18237},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18282,\"start\":18278},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18623,\"start\":18622},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18714,\"start\":18710},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18724,\"start\":18720},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18965,\"start\":18962},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19017,\"start\":19014},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20194,\"start\":20191},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20413,\"start\":20409},{\"end\":21323,\"start\":21310},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23103,\"start\":23100},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23219,\"start\":23216},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23336,\"start\":23332},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23349,\"start\":23345},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25881,\"start\":25878},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25883,\"start\":25881},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25886,\"start\":25883},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26139,\"start\":26135},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26571,\"start\":26568},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27062,\"start\":27058},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27123,\"start\":27119},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27733,\"start\":27729},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32119,\"start\":32115},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32128,\"start\":32124}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31011,\"start\":30870},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31309,\"start\":31012},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31426,\"start\":31310},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31599,\"start\":31427},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31853,\"start\":31600},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32011,\"start\":31854},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32145,\"start\":32012},{\"attributes\":{\"id\":\"fig_8\"},\"end\":32310,\"start\":32146},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32360,\"start\":32311},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32950,\"start\":32361}]", "paragraph": "[{\"end\":2220,\"start\":1798},{\"end\":3085,\"start\":2222},{\"end\":3859,\"start\":3087},{\"end\":4878,\"start\":3861},{\"end\":5532,\"start\":4880},{\"end\":6227,\"start\":5534},{\"end\":7086,\"start\":6229},{\"end\":7749,\"start\":7088},{\"end\":8911,\"start\":7751},{\"end\":9801,\"start\":8913},{\"end\":10192,\"start\":9803},{\"end\":10277,\"start\":10194},{\"end\":11271,\"start\":10312},{\"end\":11858,\"start\":11273},{\"end\":12218,\"start\":11872},{\"end\":12776,\"start\":12220},{\"end\":13063,\"start\":12778},{\"end\":13164,\"start\":13104},{\"end\":13399,\"start\":13166},{\"end\":13662,\"start\":13470},{\"end\":14138,\"start\":13748},{\"end\":14291,\"start\":14140},{\"end\":14872,\"start\":14293},{\"end\":15076,\"start\":14898},{\"end\":15398,\"start\":15078},{\"end\":15838,\"start\":15400},{\"end\":15865,\"start\":15840},{\"end\":15958,\"start\":15876},{\"end\":15995,\"start\":15960},{\"end\":16124,\"start\":15997},{\"end\":16188,\"start\":16145},{\"end\":16455,\"start\":16190},{\"end\":16839,\"start\":16457},{\"end\":18161,\"start\":16841},{\"end\":18740,\"start\":18163},{\"end\":19382,\"start\":18761},{\"end\":20170,\"start\":19411},{\"end\":20811,\"start\":20172},{\"end\":21601,\"start\":20813},{\"end\":22469,\"start\":21603},{\"end\":23947,\"start\":22471},{\"end\":25016,\"start\":23973},{\"end\":25747,\"start\":25018},{\"end\":26435,\"start\":25764},{\"end\":27044,\"start\":26437},{\"end\":27688,\"start\":27046},{\"end\":28135,\"start\":27690},{\"end\":28969,\"start\":28137},{\"end\":29742,\"start\":29004},{\"end\":30869,\"start\":29758},{\"end\":31010,\"start\":30884},{\"end\":31308,\"start\":31026},{\"end\":31425,\"start\":31324},{\"end\":31598,\"start\":31441},{\"end\":31852,\"start\":31620},{\"end\":32010,\"start\":31868},{\"end\":32144,\"start\":32021},{\"end\":32309,\"start\":32160},{\"end\":32359,\"start\":32324},{\"end\":32512,\"start\":32374}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13103,\"start\":13064},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13469,\"start\":13400},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13706,\"start\":13663},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13747,\"start\":13706}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16849,\"start\":16847},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25025,\"start\":25024}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1796,\"start\":1784},{\"end\":10310,\"start\":10280},{\"attributes\":{\"n\":\"3\"},\"end\":11870,\"start\":11861},{\"attributes\":{\"n\":\"4\"},\"end\":14896,\"start\":14875},{\"end\":15874,\"start\":15868},{\"end\":16143,\"start\":16127},{\"attributes\":{\"n\":\"5\"},\"end\":18759,\"start\":18743},{\"end\":19409,\"start\":19385},{\"attributes\":{\"n\":\"6\"},\"end\":23971,\"start\":23950},{\"attributes\":{\"n\":\"7\"},\"end\":25762,\"start\":25750},{\"attributes\":{\"n\":\"8\"},\"end\":29002,\"start\":28972},{\"attributes\":{\"n\":\"9\"},\"end\":29756,\"start\":29745},{\"end\":30881,\"start\":30871},{\"end\":31023,\"start\":31013},{\"end\":31321,\"start\":31311},{\"end\":31438,\"start\":31428},{\"end\":31616,\"start\":31601},{\"end\":31865,\"start\":31855},{\"end\":32019,\"start\":32013},{\"end\":32157,\"start\":32147},{\"end\":32321,\"start\":32312},{\"end\":32371,\"start\":32362}]", "table": "[{\"end\":32950,\"start\":32513}]", "figure_caption": "[{\"end\":31011,\"start\":30883},{\"end\":31309,\"start\":31025},{\"end\":31426,\"start\":31323},{\"end\":31599,\"start\":31440},{\"end\":31853,\"start\":31619},{\"end\":32011,\"start\":31867},{\"end\":32145,\"start\":32020},{\"end\":32310,\"start\":32159},{\"end\":32360,\"start\":32323},{\"end\":32513,\"start\":32373}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7721,\"start\":7720},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11143,\"start\":11142},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11627,\"start\":11626},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16570,\"start\":16568},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18481,\"start\":18480},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21611,\"start\":21610},{\"end\":22316,\"start\":22315},{\"end\":23648,\"start\":23647},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25687,\"start\":25686}]", "bib_author_first_name": "[{\"end\":36581,\"start\":36575},{\"end\":36591,\"start\":36587},{\"end\":36604,\"start\":36599},{\"end\":36620,\"start\":36614},{\"end\":36633,\"start\":36629},{\"end\":36644,\"start\":36640},{\"end\":36659,\"start\":36655},{\"end\":36676,\"start\":36667},{\"end\":36687,\"start\":36683},{\"end\":36698,\"start\":36697},{\"end\":36700,\"start\":36699},{\"end\":36719,\"start\":36711},{\"end\":36734,\"start\":36728},{\"end\":36749,\"start\":36745},{\"end\":36762,\"start\":36759},{\"end\":36777,\"start\":36772},{\"end\":36794,\"start\":36788},{\"end\":36806,\"start\":36803},{\"end\":36828,\"start\":36823},{\"end\":36847,\"start\":36840},{\"end\":36859,\"start\":36854},{\"end\":36876,\"start\":36870},{\"end\":36890,\"start\":36885},{\"end\":37129,\"start\":37123},{\"end\":37139,\"start\":37136},{\"end\":37154,\"start\":37146},{\"end\":37171,\"start\":37160},{\"end\":37189,\"start\":37182},{\"end\":37202,\"start\":37197},{\"end\":37212,\"start\":37211},{\"end\":37221,\"start\":37213},{\"end\":37314,\"start\":37310},{\"end\":37334,\"start\":37324},{\"end\":37340,\"start\":37337},{\"end\":37351,\"start\":37348},{\"end\":37353,\"start\":37352},{\"end\":37367,\"start\":37361},{\"end\":37381,\"start\":37376},{\"end\":37393,\"start\":37388},{\"end\":37476,\"start\":37475},{\"end\":37584,\"start\":37578},{\"end\":37599,\"start\":37593},{\"end\":37616,\"start\":37610},{\"end\":37638,\"start\":37630},{\"end\":37652,\"start\":37648},{\"end\":37721,\"start\":37713},{\"end\":37762,\"start\":37755},{\"end\":37780,\"start\":37772},{\"end\":37795,\"start\":37791},{\"end\":37810,\"start\":37806},{\"end\":37827,\"start\":37819},{\"end\":37842,\"start\":37837},{\"end\":37855,\"start\":37850},{\"end\":37870,\"start\":37862},{\"end\":37888,\"start\":37880},{\"end\":37903,\"start\":37899},{\"end\":37920,\"start\":37912},{\"end\":37936,\"start\":37933},{\"end\":37952,\"start\":37946},{\"end\":38089,\"start\":38084},{\"end\":38098,\"start\":38095},{\"end\":38113,\"start\":38104},{\"end\":38125,\"start\":38119},{\"end\":38141,\"start\":38135},{\"end\":38151,\"start\":38146},{\"end\":38164,\"start\":38163},{\"end\":38181,\"start\":38173},{\"end\":38203,\"start\":38195},{\"end\":38209,\"start\":38204},{\"end\":38292,\"start\":38284},{\"end\":38305,\"start\":38298},{\"end\":38318,\"start\":38313},{\"end\":38320,\"start\":38319},{\"end\":38336,\"start\":38331},{\"end\":38349,\"start\":38343},{\"end\":38362,\"start\":38358},{\"end\":38377,\"start\":38372},{\"end\":38387,\"start\":38386},{\"end\":38396,\"start\":38388},{\"end\":38491,\"start\":38487},{\"end\":38504,\"start\":38500},{\"end\":38511,\"start\":38509},{\"end\":38524,\"start\":38519},{\"end\":38541,\"start\":38534},{\"end\":38543,\"start\":38542},{\"end\":38562,\"start\":38556},{\"end\":38577,\"start\":38572},{\"end\":38593,\"start\":38585},{\"end\":38611,\"start\":38603},{\"end\":38623,\"start\":38619},{\"end\":38633,\"start\":38629},{\"end\":38649,\"start\":38644},{\"end\":38664,\"start\":38658},{\"end\":38677,\"start\":38673},{\"end\":38679,\"start\":38678},{\"end\":38694,\"start\":38688},{\"end\":38709,\"start\":38703},{\"end\":38723,\"start\":38718},{\"end\":38738,\"start\":38734},{\"end\":38762,\"start\":38759},{\"end\":38774,\"start\":38770},{\"end\":38776,\"start\":38775},{\"end\":38900,\"start\":38896},{\"end\":38916,\"start\":38908},{\"end\":39041,\"start\":39037},{\"end\":39055,\"start\":39051},{\"end\":39060,\"start\":39056},{\"end\":39071,\"start\":39066},{\"end\":39087,\"start\":39081},{\"end\":39103,\"start\":39096},{\"end\":39117,\"start\":39109},{\"end\":39133,\"start\":39127},{\"end\":39148,\"start\":39142},{\"end\":39163,\"start\":39157},{\"end\":39177,\"start\":39173},{\"end\":39193,\"start\":39185},{\"end\":39207,\"start\":39203},{\"end\":39338,\"start\":39333},{\"end\":39349,\"start\":39348},{\"end\":39368,\"start\":39361},{\"end\":39384,\"start\":39377},{\"end\":39397,\"start\":39392},{\"end\":39583,\"start\":39574},{\"end\":39601,\"start\":39595},{\"end\":39619,\"start\":39612},{\"end\":39631,\"start\":39627},{\"end\":39652,\"start\":39647},{\"end\":39667,\"start\":39663},{\"end\":39682,\"start\":39676},{\"end\":39699,\"start\":39692},{\"end\":39715,\"start\":39707},{\"end\":39731,\"start\":39724},{\"end\":39764,\"start\":39755},{\"end\":39793,\"start\":39787},{\"end\":39809,\"start\":39803},{\"end\":39824,\"start\":39819},{\"end\":40016,\"start\":40007},{\"end\":40033,\"start\":40026},{\"end\":40051,\"start\":40045},{\"end\":40067,\"start\":40059},{\"end\":40084,\"start\":40076},{\"end\":40259,\"start\":40258},{\"end\":40271,\"start\":40267},{\"end\":40283,\"start\":40278},{\"end\":40300,\"start\":40293},{\"end\":40320,\"start\":40312},{\"end\":40331,\"start\":40327},{\"end\":40344,\"start\":40340},{\"end\":40446,\"start\":40439},{\"end\":40458,\"start\":40451},{\"end\":40468,\"start\":40464},{\"end\":40477,\"start\":40474},{\"end\":40493,\"start\":40484},{\"end\":40618,\"start\":40610},{\"end\":40627,\"start\":40623},{\"end\":40639,\"start\":40633},{\"end\":40650,\"start\":40644},{\"end\":40663,\"start\":40657},{\"end\":40672,\"start\":40668},{\"end\":40682,\"start\":40679},{\"end\":40695,\"start\":40689}]", "bib_author_last_name": "[{\"end\":36585,\"start\":36582},{\"end\":36597,\"start\":36592},{\"end\":36612,\"start\":36605},{\"end\":36627,\"start\":36621},{\"end\":36638,\"start\":36634},{\"end\":36653,\"start\":36645},{\"end\":36665,\"start\":36660},{\"end\":36681,\"start\":36677},{\"end\":36695,\"start\":36688},{\"end\":36709,\"start\":36701},{\"end\":36726,\"start\":36720},{\"end\":36743,\"start\":36735},{\"end\":36757,\"start\":36750},{\"end\":36770,\"start\":36763},{\"end\":36786,\"start\":36778},{\"end\":36801,\"start\":36795},{\"end\":36821,\"start\":36807},{\"end\":36838,\"start\":36829},{\"end\":36852,\"start\":36848},{\"end\":36868,\"start\":36860},{\"end\":36883,\"start\":36877},{\"end\":36897,\"start\":36891},{\"end\":37134,\"start\":37130},{\"end\":37144,\"start\":37140},{\"end\":37158,\"start\":37155},{\"end\":37180,\"start\":37172},{\"end\":37195,\"start\":37190},{\"end\":37209,\"start\":37203},{\"end\":37229,\"start\":37222},{\"end\":37322,\"start\":37315},{\"end\":37346,\"start\":37341},{\"end\":37359,\"start\":37354},{\"end\":37374,\"start\":37368},{\"end\":37386,\"start\":37382},{\"end\":37400,\"start\":37394},{\"end\":37482,\"start\":37477},{\"end\":37487,\"start\":37484},{\"end\":37591,\"start\":37585},{\"end\":37608,\"start\":37600},{\"end\":37628,\"start\":37617},{\"end\":37646,\"start\":37639},{\"end\":37663,\"start\":37653},{\"end\":37724,\"start\":37722},{\"end\":37770,\"start\":37763},{\"end\":37789,\"start\":37781},{\"end\":37804,\"start\":37796},{\"end\":37817,\"start\":37811},{\"end\":37835,\"start\":37828},{\"end\":37848,\"start\":37843},{\"end\":37860,\"start\":37856},{\"end\":37878,\"start\":37871},{\"end\":37897,\"start\":37889},{\"end\":37910,\"start\":37904},{\"end\":37931,\"start\":37921},{\"end\":37944,\"start\":37937},{\"end\":37960,\"start\":37953},{\"end\":37970,\"start\":37962},{\"end\":38093,\"start\":38090},{\"end\":38102,\"start\":38099},{\"end\":38117,\"start\":38114},{\"end\":38133,\"start\":38126},{\"end\":38144,\"start\":38142},{\"end\":38161,\"start\":38152},{\"end\":38171,\"start\":38165},{\"end\":38193,\"start\":38182},{\"end\":38212,\"start\":38210},{\"end\":38296,\"start\":38293},{\"end\":38311,\"start\":38306},{\"end\":38329,\"start\":38321},{\"end\":38341,\"start\":38337},{\"end\":38356,\"start\":38350},{\"end\":38370,\"start\":38363},{\"end\":38384,\"start\":38378},{\"end\":38404,\"start\":38397},{\"end\":38498,\"start\":38492},{\"end\":38507,\"start\":38505},{\"end\":38517,\"start\":38512},{\"end\":38532,\"start\":38525},{\"end\":38554,\"start\":38544},{\"end\":38570,\"start\":38563},{\"end\":38583,\"start\":38578},{\"end\":38601,\"start\":38594},{\"end\":38617,\"start\":38612},{\"end\":38627,\"start\":38624},{\"end\":38642,\"start\":38634},{\"end\":38656,\"start\":38650},{\"end\":38671,\"start\":38665},{\"end\":38686,\"start\":38680},{\"end\":38701,\"start\":38695},{\"end\":38716,\"start\":38710},{\"end\":38732,\"start\":38724},{\"end\":38757,\"start\":38739},{\"end\":38768,\"start\":38763},{\"end\":38781,\"start\":38777},{\"end\":38906,\"start\":38901},{\"end\":39049,\"start\":39042},{\"end\":39064,\"start\":39061},{\"end\":39079,\"start\":39072},{\"end\":39094,\"start\":39088},{\"end\":39107,\"start\":39104},{\"end\":39125,\"start\":39118},{\"end\":39140,\"start\":39134},{\"end\":39155,\"start\":39149},{\"end\":39171,\"start\":39164},{\"end\":39183,\"start\":39178},{\"end\":39201,\"start\":39194},{\"end\":39217,\"start\":39208},{\"end\":39346,\"start\":39339},{\"end\":39359,\"start\":39350},{\"end\":39375,\"start\":39369},{\"end\":39390,\"start\":39385},{\"end\":39403,\"start\":39398},{\"end\":39593,\"start\":39584},{\"end\":39610,\"start\":39602},{\"end\":39625,\"start\":39620},{\"end\":39645,\"start\":39632},{\"end\":39661,\"start\":39653},{\"end\":39674,\"start\":39668},{\"end\":39690,\"start\":39683},{\"end\":39705,\"start\":39700},{\"end\":39722,\"start\":39716},{\"end\":39740,\"start\":39732},{\"end\":39753,\"start\":39742},{\"end\":39785,\"start\":39765},{\"end\":39801,\"start\":39794},{\"end\":39817,\"start\":39810},{\"end\":39836,\"start\":39825},{\"end\":39844,\"start\":39838},{\"end\":40024,\"start\":40017},{\"end\":40043,\"start\":40034},{\"end\":40057,\"start\":40052},{\"end\":40074,\"start\":40068},{\"end\":40090,\"start\":40085},{\"end\":40265,\"start\":40260},{\"end\":40276,\"start\":40272},{\"end\":40291,\"start\":40284},{\"end\":40310,\"start\":40301},{\"end\":40325,\"start\":40321},{\"end\":40338,\"start\":40332},{\"end\":40350,\"start\":40345},{\"end\":40449,\"start\":40447},{\"end\":40462,\"start\":40459},{\"end\":40472,\"start\":40469},{\"end\":40482,\"start\":40478},{\"end\":40496,\"start\":40494},{\"end\":40621,\"start\":40619},{\"end\":40631,\"start\":40628},{\"end\":40642,\"start\":40640},{\"end\":40655,\"start\":40651},{\"end\":40666,\"start\":40664},{\"end\":40677,\"start\":40673},{\"end\":40687,\"start\":40683},{\"end\":40700,\"start\":40696}]", "bib_entry": "[{\"attributes\":{\"doi\":\"ArXiv, abs/2204.05862\",\"id\":\"b0\"},\"end\":37057,\"start\":36484},{\"attributes\":{\"doi\":\"ArXiv, abs/1504.00325\",\"id\":\"b1\"},\"end\":37256,\"start\":37059},{\"attributes\":{\"doi\":\"ArXiv, abs/1706.03741\",\"id\":\"b2\"},\"end\":37427,\"start\":37258},{\"attributes\":{\"id\":\"b3\"},\"end\":37493,\"start\":37429},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":326772},\"end\":37675,\"start\":37495},{\"attributes\":{\"doi\":\"ArXiv, abs/2207.12598\",\"id\":\"b5\"},\"end\":37751,\"start\":37677},{\"attributes\":{\"id\":\"b6\"},\"end\":38030,\"start\":37753},{\"attributes\":{\"doi\":\"ArXiv, abs/2302.12192\",\"id\":\"b7\"},\"end\":38239,\"start\":38032},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14113767},\"end\":38416,\"start\":38241},{\"attributes\":{\"doi\":\"ArXiv, abs/2203.02155\",\"id\":\"b9\"},\"end\":38808,\"start\":38418},{\"attributes\":{\"doi\":\"Version 1.0\",\"id\":\"b10\"},\"end\":38964,\"start\":38810},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":231591445},\"end\":39269,\"start\":38966},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":245335280},\"end\":39486,\"start\":39271},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":252917726},\"end\":39946,\"start\":39488},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206593880},\"end\":40169,\"start\":39948},{\"attributes\":{\"doi\":\"ArXiv, abs/2210.14896\",\"id\":\"b15\"},\"end\":40377,\"start\":40171},{\"attributes\":{\"doi\":\"ArXiv, abs/2303.14420\",\"id\":\"b16\"},\"end\":40523,\"start\":40379},{\"attributes\":{\"doi\":\"ArXiv, abs/2304.05977\",\"id\":\"b17\"},\"end\":40727,\"start\":40525}]", "bib_title": "[{\"end\":37576,\"start\":37495},{\"end\":38282,\"start\":38241},{\"end\":38894,\"start\":38810},{\"end\":39035,\"start\":38966},{\"end\":39331,\"start\":39271},{\"end\":39572,\"start\":39488},{\"end\":40005,\"start\":39948}]", "bib_author": "[{\"end\":36587,\"start\":36575},{\"end\":36599,\"start\":36587},{\"end\":36614,\"start\":36599},{\"end\":36629,\"start\":36614},{\"end\":36640,\"start\":36629},{\"end\":36655,\"start\":36640},{\"end\":36667,\"start\":36655},{\"end\":36683,\"start\":36667},{\"end\":36697,\"start\":36683},{\"end\":36711,\"start\":36697},{\"end\":36728,\"start\":36711},{\"end\":36745,\"start\":36728},{\"end\":36759,\"start\":36745},{\"end\":36772,\"start\":36759},{\"end\":36788,\"start\":36772},{\"end\":36803,\"start\":36788},{\"end\":36823,\"start\":36803},{\"end\":36840,\"start\":36823},{\"end\":36854,\"start\":36840},{\"end\":36870,\"start\":36854},{\"end\":36885,\"start\":36870},{\"end\":36899,\"start\":36885},{\"end\":37136,\"start\":37123},{\"end\":37146,\"start\":37136},{\"end\":37160,\"start\":37146},{\"end\":37182,\"start\":37160},{\"end\":37197,\"start\":37182},{\"end\":37211,\"start\":37197},{\"end\":37231,\"start\":37211},{\"end\":37324,\"start\":37310},{\"end\":37337,\"start\":37324},{\"end\":37348,\"start\":37337},{\"end\":37361,\"start\":37348},{\"end\":37376,\"start\":37361},{\"end\":37388,\"start\":37376},{\"end\":37402,\"start\":37388},{\"end\":37484,\"start\":37475},{\"end\":37489,\"start\":37484},{\"end\":37593,\"start\":37578},{\"end\":37610,\"start\":37593},{\"end\":37630,\"start\":37610},{\"end\":37648,\"start\":37630},{\"end\":37665,\"start\":37648},{\"end\":37726,\"start\":37713},{\"end\":37772,\"start\":37755},{\"end\":37791,\"start\":37772},{\"end\":37806,\"start\":37791},{\"end\":37819,\"start\":37806},{\"end\":37837,\"start\":37819},{\"end\":37850,\"start\":37837},{\"end\":37862,\"start\":37850},{\"end\":37880,\"start\":37862},{\"end\":37899,\"start\":37880},{\"end\":37912,\"start\":37899},{\"end\":37933,\"start\":37912},{\"end\":37946,\"start\":37933},{\"end\":37962,\"start\":37946},{\"end\":37972,\"start\":37962},{\"end\":38095,\"start\":38084},{\"end\":38104,\"start\":38095},{\"end\":38119,\"start\":38104},{\"end\":38135,\"start\":38119},{\"end\":38146,\"start\":38135},{\"end\":38163,\"start\":38146},{\"end\":38173,\"start\":38163},{\"end\":38195,\"start\":38173},{\"end\":38214,\"start\":38195},{\"end\":38298,\"start\":38284},{\"end\":38313,\"start\":38298},{\"end\":38331,\"start\":38313},{\"end\":38343,\"start\":38331},{\"end\":38358,\"start\":38343},{\"end\":38372,\"start\":38358},{\"end\":38386,\"start\":38372},{\"end\":38406,\"start\":38386},{\"end\":38500,\"start\":38487},{\"end\":38509,\"start\":38500},{\"end\":38519,\"start\":38509},{\"end\":38534,\"start\":38519},{\"end\":38556,\"start\":38534},{\"end\":38572,\"start\":38556},{\"end\":38585,\"start\":38572},{\"end\":38603,\"start\":38585},{\"end\":38619,\"start\":38603},{\"end\":38629,\"start\":38619},{\"end\":38644,\"start\":38629},{\"end\":38658,\"start\":38644},{\"end\":38673,\"start\":38658},{\"end\":38688,\"start\":38673},{\"end\":38703,\"start\":38688},{\"end\":38718,\"start\":38703},{\"end\":38734,\"start\":38718},{\"end\":38759,\"start\":38734},{\"end\":38770,\"start\":38759},{\"end\":38783,\"start\":38770},{\"end\":38908,\"start\":38896},{\"end\":38919,\"start\":38908},{\"end\":39051,\"start\":39037},{\"end\":39066,\"start\":39051},{\"end\":39081,\"start\":39066},{\"end\":39096,\"start\":39081},{\"end\":39109,\"start\":39096},{\"end\":39127,\"start\":39109},{\"end\":39142,\"start\":39127},{\"end\":39157,\"start\":39142},{\"end\":39173,\"start\":39157},{\"end\":39185,\"start\":39173},{\"end\":39203,\"start\":39185},{\"end\":39219,\"start\":39203},{\"end\":39348,\"start\":39333},{\"end\":39361,\"start\":39348},{\"end\":39377,\"start\":39361},{\"end\":39392,\"start\":39377},{\"end\":39405,\"start\":39392},{\"end\":39595,\"start\":39574},{\"end\":39612,\"start\":39595},{\"end\":39627,\"start\":39612},{\"end\":39647,\"start\":39627},{\"end\":39663,\"start\":39647},{\"end\":39676,\"start\":39663},{\"end\":39692,\"start\":39676},{\"end\":39707,\"start\":39692},{\"end\":39724,\"start\":39707},{\"end\":39742,\"start\":39724},{\"end\":39755,\"start\":39742},{\"end\":39787,\"start\":39755},{\"end\":39803,\"start\":39787},{\"end\":39819,\"start\":39803},{\"end\":39838,\"start\":39819},{\"end\":39846,\"start\":39838},{\"end\":40026,\"start\":40007},{\"end\":40045,\"start\":40026},{\"end\":40059,\"start\":40045},{\"end\":40076,\"start\":40059},{\"end\":40092,\"start\":40076},{\"end\":40267,\"start\":40258},{\"end\":40278,\"start\":40267},{\"end\":40293,\"start\":40278},{\"end\":40312,\"start\":40293},{\"end\":40327,\"start\":40312},{\"end\":40340,\"start\":40327},{\"end\":40352,\"start\":40340},{\"end\":40451,\"start\":40439},{\"end\":40464,\"start\":40451},{\"end\":40474,\"start\":40464},{\"end\":40484,\"start\":40474},{\"end\":40498,\"start\":40484},{\"end\":40623,\"start\":40610},{\"end\":40633,\"start\":40623},{\"end\":40644,\"start\":40633},{\"end\":40657,\"start\":40644},{\"end\":40668,\"start\":40657},{\"end\":40679,\"start\":40668},{\"end\":40689,\"start\":40679},{\"end\":40702,\"start\":40689}]", "bib_venue": "[{\"end\":36573,\"start\":36484},{\"end\":37121,\"start\":37059},{\"end\":37308,\"start\":37258},{\"end\":37473,\"start\":37429},{\"end\":37669,\"start\":37665},{\"end\":37711,\"start\":37677},{\"end\":38082,\"start\":38032},{\"end\":38410,\"start\":38406},{\"end\":38485,\"start\":38418},{\"end\":38942,\"start\":38930},{\"end\":39263,\"start\":39219},{\"end\":39474,\"start\":39405},{\"end\":39940,\"start\":39846},{\"end\":40157,\"start\":40092},{\"end\":40256,\"start\":40171},{\"end\":40437,\"start\":40379},{\"end\":40608,\"start\":40525}]"}}}, "year": 2023, "month": 12, "day": 17}