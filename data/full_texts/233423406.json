{"id": 233423406, "updated": "2023-10-06 04:21:02.096", "metadata": {"title": "Inpainting Transformer for Anomaly Detection", "authors": "[{\"first\":\"Jonathan\",\"last\":\"Pirnay\",\"middle\":[]},{\"first\":\"Keng\",\"last\":\"Chai\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Anomaly detection in computer vision is the task of identifying images which deviate from a set of normal images. A common approach is to train deep convolutional autoencoders to inpaint covered parts of an image and compare the output with the original image. By training on anomaly-free samples only, the model is assumed to not being able to reconstruct anomalous regions properly. For anomaly detection by inpainting we suggest it to be beneficial to incorporate information from potentially distant regions. In particular we pose anomaly detection as a patch-inpainting problem and propose to solve it with a purely self-attention based approach discarding convolutions. The proposed Inpainting Transformer (InTra) is trained to inpaint covered patches in a large sequence of image patches, thereby integrating information across large regions of the input image. When training from scratch, in comparison to other methods not using extra training data, InTra achieves results on par with the current state-of-the-art on the MVTec AD dataset for detection and surpassing them on segmentation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.13897", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iciap/PirnayC22", "doi": "10.1007/978-3-031-06430-2_33"}}, "content": {"source": {"pdf_hash": "19862af96b6af51e879e6e3f1d3d421af5427005", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.13897v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4e04666c81fa56b5168e17973f56dddd63643bc2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/19862af96b6af51e879e6e3f1d3d421af5427005.txt", "contents": "\nInpainting Transformer for Anomaly Detection\n\n\nJonathan Pirnay jonathan.pirnay@fujitsu.com \nDigital Incubation\nFujitsu Technology Solutions GmbH\nMunichGermany\n\nKeng Chai keng.chai@fujitsu.com \nDigital Incubation\nFujitsu Technology Solutions GmbH\nMunichGermany\n\nInpainting Transformer for Anomaly Detection\nAnomaly Detection \u00b7 Self-attention \u00b7 Transformer\nAnomaly detection in computer vision is the task of identifying images which deviate from a set of normal images. A common approach is to train deep convolutional autoencoders to inpaint covered parts of an image and compare the output with the original image. By training on anomaly-free samples only, the model is assumed to not being able to reconstruct anomalous regions properly. For anomaly detection by inpainting we suggest it to be beneficial to incorporate information from potentially distant regions. In particular we pose anomaly detection as a patch-inpainting problem and propose to solve it with a purely self-attention based approach discarding convolutions. The proposed Inpainting Transformer (InTra) is trained to inpaint covered patches in a large sequence of image patches, thereby integrating information across large regions of the input image. When training from scratch, in comparison to other methods not using extra training data, InTra achieves results on par with the current state-of-the-art on the MVTec AD dataset for detection and surpassing them on segmentation.\n\nIntroduction\n\nAnomaly detection and localization in vision describe the problem of deciding whether a given image is atypical with respect to a set of normal samples, and to identify the respective anomalous subregions within the image. Both problems have strong implications for industrial inspection [3] and medical applications [8]. In practical industrial applications, anomalies occur rarely. Due to the lack of sufficient anomalous samples, and as anomalies can be of unexpected shape and texture, it is hard to deal with this problem with supervised methods. Current approaches follow unsupervised methods and try to model the distribution of normal data only. At test time an anomaly score is given to each image to indicate how much it deviates from normal samples. For anomaly localization a similar score is assigned to subregions or individual pixels of the image.\n\nA common approach following this paradigm is to use deep convolutional autoencoders or generative models such adversarial networks in order to model the manifold of normal training data. The difference between the input and reconstructed image is then used to compute the anomaly scores. In practice this approach often suffers from the drawback that convolutional autoencoders generalize strongly and anomalies are reconstructed well, leading to misdetection. Recent methods propose to mitigate this effect by posing the generative part as an inpainting problem: Parts of the input image are covered and the model is trained to reconstruct the covered parts in a self-supervised way [4,10,22,13]. By conditioning on the neighborhood of the excluded part only, small anomalies get effectively retouched. Due to their limited receptive field, fully convolutional neural networks (CNNs) are partially ineffective in modeling distant contextual information, which makes the removal of larger anomalous regions difficult. For inpainting in general settings, this can be effectively addressed by introducing contextual attention in the model [21]. For inpainting in the context of anomaly detection we suggest it to be beneficial to learn the relevant patterns alone by combining information from large regions around the covered image part via attention. Inspired by the recent success of self-attention based models such as Transformers [17] in image recognition [7], we pose anomaly detection as a patchinpainting problem and propose to solve it without convolutions: images are split into square patches, and a Transformer model is trained to reconstruct covered patches on the basis of a long sequence of neighboring patches. By recovering the whole image in this way, a full reconstructed image is obtained where the reconstruction of an individual patch incorporates a large context and not only the appearance of its immediate neighborhood. Thus patches are not reconstructed by simply mimicking the local neighborhood, leading to high anomaly scores even for spacious anomalous regions.\n\nOur contributions enfold the modeling of anomaly detection as a patchsequence inpainting problem which we solve using a deep Transformer network consisting of a simple stack of multiheaded self-attention blocks. Within this network convolutional operations are removed entirely. Furthermore we propose to employ long residual connections between the Transformer blocks and to perform a nonlinear dimension reduction for keys and queries when computing self-attention in order to improve the network's reconstruction capabilities for difficult surfaces. By adding embeddings of the position of individual patches within an image to the sequence of patches, it is possible to perform the inpainting in a global context even if the sequence of patches does not cover the full image.\n\nWe evaluate our method on the challenging MVTec AD dataset [3] for both detection and segmentation. Although Transformer networks are usually trained on huge amounts of data, we effectively train our networks with \u223c55M parameters from scratch only on the 60-400 images available for each category in MVTec AD. We compare our results to the current state-of-the-art not using any extra training data. Our proposed method InTra achieves on par results on the detection task and slightly better results on the segmentation task.\n\n\nRelated Work\n\nIn anomaly detection, reconstruction-based methods try to model only normal, defect-free samples. For this, deep CNN autoencoders are widely used to learn the manifold of defect-free images in a latent bottleneck. Given defective test data, these models should not be able to properly reconstruct the anomalous image since they only model normal data [5,16,2]. An anomaly map for segmentation is usually generated via pixel-wise difference or similarity measures between the input image and its model reconstruction, leading to noticeable anomalies.\n\nEven though in reconstruction-based methods the models are trained on defect-free samples only, they often generalize well to anomalies in practice [9]. An inpainting scheme can be used to effectively hide anomalous regions to further restrict a model's capability to reconstruct anomalies [4,10,22,13]. By covering parts of the original image, the reconstruction method needs to have semantic understanding of the image to be able to generate a coherent and realistic image. Zavrtanik et al. propose to use a U-Net architecture [15] taking advantage of long residual connections. Their reconstruction-based method randomly selects multiple parts of the image to inpaint, yielding the current state-of-the-art results for anomaly detection via inpainting for different benchmarks [22].\n\nAnomalies which span over a large area may still cause problems as these will not be covered up sufficiently enough. As such we propose to add global context by replacing CNNs with a Transformer-based framework applied in vision.\n\nTransformer models were originally introduced in natural language processing (NLP) and have since evolved to be the modern design for various sequence tasks like text translation, generation and document classification [17,6,20]. In a Transformer model, self-attention is used to relate elements of a sequence to each other. Based on the relative weighted importance a shared representation is calculated taking into account the relative dependencies between sequence elements. This is able to replace recurrent neural networks in sequence-to-sequence modeling because long-range dependencies are processed globally. The general architecture can be found in the original work [17].\n\nWhile Transformer architectures have been widely studied in NLP and sequence modeling, convolutional architectures have been essentially the standard tool in recent years due to weight sharing, translation equivariance and locality.\n\nDue to the induced bias in fully convolutional autoencoders, the restricted receptive field limits global context [21]. Even though in theory the self-attention framework may mitigate this problem, running self-attention on the whole image without further simplifications is not feasible [11,14].\n\nRecently Dosovitskiy et al. have proposed Vision Transformer [7], where the image data is split up into square non-overlapping uniform patches. Each patch and position gets embedded into a latent space and every image is treated as a sequence of these embedded patches. A Transformer architecture is applied on the restructured data achieving comparable results to state of the art CNNs and even surpassing them on some tasks while reducing model bias.\n\n\nInpainting Transformer for Anomaly Detection\n\nOur approach is based on a simple stack of Transformer blocks which are trained to inpaint covered image patches based on neighboring patches. An overview of the method is shown in Figure 1.\n\n\nPatch Embeddings and Multihead Feature Self-attention\n\nWe use a similar notation as in [7]. Let x \u2208 R H\u00d7W \u00d7C be an input image, where (H, W ) denotes the (height, width)-size and C the number of channels of the image. Let K be the desired side length of a square patch and N := H K , M := W K (the image is resized such that K divides H and W ). We split the image x into a N \u00d7 M grid of flattened square patches\nx p \u2208 R (N \u00d7 M ) \u00d7 (K 2 \u00b7 C) , where x (i,j) p \u2208 R K 2 \u00b7C\nis the patch in the i-th row and j-th column. Our aim is to choose square subgrids of some side length L in this patch grid and train a network to reconstruct any covered patch in the subgrid based on the rest of the subgrid's patches. Formally, this inpainting problem is as follows:\n\nLet \nx (i,j) p (i, j) \u2208 S be(i,j) p (i,j)\u2208S\\{(t,u)} in the window.\nAs by definition Transformers are invariant with respect to reorderings of the input, the one-dimensional positional information f (i, j) :\n= (i \u2212 1) \u00b7 N + j of a patch x (i,j) p\nis used. To use as a sequence input to the Transformer model, we map the window of patches and their positional information into some latent space of dimension D, i.e. for each patch x with learnable weight matrix E \u2208 R (K 2 \u00b7C)\u00d7D , and where posemb denotes a standard learnable one-dimensional position embeddings.\n(i,j) p with (i, j) \u2208 S \\ {(t, u)} we set y (i,j) := x (i,j) p E + posemb(f (i, j)) \u2208 R D (1)\nTo account for the patch at position (t, u) to inpaint, we add a single learnable embedding x inpaint \u2208 R D to the position embedding via\nz := x inpaint + posemb(f (t, u)) \u2208 R D .(2)\nThe embedding x inpaint is comparable to the class token in [6]. The vectors z and y (i,j) for all (i, j) \u2208 S \\ {(t, u)} build the final sequence of embedded patches which serves as an input sequence of length L 2 to the Inpainting Transformer model.\n\nApplying multihead self-attention (MSA) to an input sequence forms the heart of a standard Transformer block as in [17]. For this, queries q, keys k and values v are obtained by mapping the input sequence with learnable weight matrices W q , W k , W v \u2208 R D\u00d7D . Self-attention is then computed over slices of q, k, v. In cases where the patches of the training images are very similar but indistinct the dot product of queries and keys are very close to each other, leading to an almost uniform softmax-weighted sum in the calculation of MSA. To mitigate this, we propose to perform a nonlinear dimension reduction when computing q and k. For this, W q and W k are swapped with multilayer perceptrons (MLP) with a single hidden layer. In all our models we used an output dimension of D 2 and a hidden layer dimension of 2 \u00b7 D with GELU non-linearity. We refer to this modified MSA as multihead feature self-attention (MFSA). We experienced improved detection results with MFSA (see Section 4.3). However, depending on the output and hidden dimension of the MLPs, the number of learnable parameters increases strongly with MFSA.\n\n\nNetwork Architecture and Training\n\nOur network architecture for inpainting is composed of a simple stack of n Transformer blocks. Figure 2 illustrates the architecture. The structure of each Trans-former block mainly follows [7] and consists of MFSA followed by a multilayer perceptron (MLP). Layer normalization is applied before (\"pre-norm\" [18]), and residual connections after MFSA and MLP. Each MLP has a single hidden layer with GELU nonlinearity and maps R D \u2192 R 4\u00b7D \u2192 R D . In particular the input and output of each Transformer block is a sequence in R L 2 \u00d7D (see Fig. 2).\n\nTo obtain the inpainted patch, we average over the output sequence of the last Transformer block to get a single vector in R D which is mapped back to the pixel space of the flattened patches R K 2 \u00b7C via a learnable affine transformation followed by a sigmoidal.\n\nIn early experiments an inspection of the attention weights showed that a large spatial context is present in earlier layers. In addition to that, Attention Rollout [1] has been used in [7] to illustrate that information across the entire input image is integrated already in the lowest layers. In order to carry this early information to deeper blocks of the network, we put additional long residual connections between early and late layers in a U-Net fashion [15]. We found that the use of long residual connections leads to more structural detail in the overall reconstruction, slightly improving both detection and segmentation (see Section 4.3).\n\nThe network is trained by randomly sampling batches of patch windows with a fixed side length L from normal image data. In each window a random patch position (t, u) is chosen, which is inpainted by the network as described in the previous sections.\n\nFor the loss function, we compare the original and reconstructed patch with pixel-wise L 2 loss. To account for perceptual differences, we also include structural similarity [23] and gradient magnitude similarity [19].\n\nGiven an original and reconstructed patch x p ,x p \u2208 R K\u00d7K\u00d7C , the full loss function L is given by\nL(x p ,x p ) = L 2 (x p ,x p ) + \u03b1 K 2 (i,j)\u2208K\u00d7K (1 \u2212 GMS avg (x p ,x p ) (i,j) ) + \u03b2 K 2 (i,j)\u2208K\u00d7K (1 \u2212 SSIM avg (x p ,x p ) (i,j) )(3)\nwhere \u03b1, \u03b2 are individual scaling parameters, 1 is a matrix of ones and GMS avg (resp. SSIM avg ) denotes the gradient magnitude similarity maps (resp. structural similarity maps) averaged over the color channels.\n\n\nInference and Anomaly Detection\n\nThe inferencing process is divided into two steps: First a complete inpainted image is generated, afterwards the difference between the reconstruction and original is used to compute a pixel-wise anomaly map. Let x \u2208 R H\u00d7W \u00d7C be an input image with an N \u00d7M patch grid as introduced above. For each patch position (t, u) \u2208 N \u00d7 M , we choose an appropriate patch window of side length L which is used as a basis to inpaint the patch at position x (t,u) p . In particular we define the window by its upper left patch x\n(r,s) p via r =g(t) \u2212 max(0, g(t) + L \u2212 N \u2212 1),(4)s =g(u) \u2212 max(0, g(u) + L \u2212 M \u2212 1),(5)\nwhere the map g is given by g(c) := max 1, c \u2212 L 2 . The above equations choose (r, s) such that (t, u) is as much centered in the L \u00d7 L patch-window as possible. Using this window, the patch x (t,u) p is reconstructed by the network as described. By reconstructing all patches in the N \u00d7 M grid, we obtain a full reconstructionx of the whole image.\n\nFor the generation of an expressive anomaly map from x andx we use a simplified variant of the GMS-based scheme proposed in [22]. Denote by x l an image x resized to scale l. Now for original and reconstructed images x,x \u2208 R H\u00d7W \u00d7C and scale l \u2208 { 1 2 , 1 4 }, we set m l (x,x) := bluravg l (1 \u2212 GMS avg (x,x)) \u2208 R l\u00b7H\u00d7l\u00b7W (6) for a scaled and smoothed version of the gradient difference. To ease notation, we denote by bluravg l the application of an averaging filter followed by some Gaussian blur operation, both with a predefined kernel size and variance. As in [22], smoothing improves robustness with respect to small, poorly reconstructed anomalous regions. We resize the two-dimensional maps m 1 2 and m 1 4 back to the image's original size and take the pixel-wise mean which yields a difference map diff(x,x) \u2208 R H\u00d7W .\n\nTo finally obtain an anomaly map for x during inference, we take the squared deviation of the difference map to the normal training data, i.e.\nanomap(x) := diff(x,x) \u2212 1 |T | z\u2208T diff(z,\u1e91) 2 \u2208 R H\u00d7W \u22650 ,(7)\nwhere T is the set of normal training samples. The pixel-wise maximum of anomap(x) is taken as a scalar anomaly score for detection on the image level. An example of an anomaly map can be seen in Fig. 1b.).\n\n\nExperiments\n\nWe evaluate our method on the MVTec AD dataset which contains high resolution samples of 5 texture and 10 object categories stemming from manufacturing [3]. The dataset has been a widely used benchmark for anomaly detection and localization in the manufacturing domain. Each category consists of around 60 to 400 normal, defect-free samples for training. For each test image there is a ground-truth binary image labeled on pixel-level for segmentation of anomalous test images. Based on an image's anomaly score we report standard ROC AUC as a detection metric. For localisation, the image's anomaly map (7) is used for an evaluation of pixel-wise ROC AUC. \n\n\nImplementation Details\n\nWe train our model on each product category from scratch. We randomly choose 10% of images from the normal training data (however a maximum of 20) and use them as a validation set to control the quality of reconstructions. In each epoch 600 patch windows are sampled randomly per image. To augment the dataset, random rotation and flipping is used.\n\nThe choice of three parameters has an obvious significant impact on the performance: Side length K of square patches, side length L of a patch window and the choice of height H and width W (with H = W , as all images are square) to which the original image is resized during training and inference. The patch size determines how much of the image is covered, the size of the patch window determines the dilation of context we include during inpainting, the image size implicitly influences both. For all models we choose K = 16, L = 7. For the choice of image size in our pipeline, a balance needs to be struck between enlarging the image context of the 7\u00d77 window, quality of patch reconstructions and computation time, as Transformer models usually take a long time to train. The heuristics is to choose the image as small as possible while keeping patch reconstructions at a high level of detail. Hence we train the model with image dimensions 256 \u00d7 256, 320 \u00d7 320, 512 \u00d7 512 for 200 epochs and compare the best (epoch-wise) validation losses (averaged over \u00b15 epochs). If there is no significant improvement in the validation loss of at least 10 \u22124 for an image dimension with the next in size, the smaller dimension is chosen. The rightmost column of Table  1 shows the resulting image sizes for each category. We note that in practice K, L, H, W could be tuned for the detection task at hand if prior knowledge about possible defects is present. The Inpainting Transformer model trained consists of 13 blocks with 8 attention heads each and a latent dimension of D = 512, using MFSA. In total this amounts to \u223c55M learnable parameters.\n\nGiven an image size of 512 \u00d7 512, a kernel size of 21 (resp. 11) is used for averaging and Gaussian blur (with \u03c3 = 2) for bluravg 1 2 (resp. bluravg 1 4 ) in (6). The kernel sizes are scaled linearly for smaller image sizes. Anomaly maps are resized back to their original high image resolution for proper segmentation comparison.\n\nFor the loss function L in (3) we set \u03b1 = \u03b2 = 0.01. The network is trained using the Adam optimizer with a learning rate of 0.0001 and a batch size of 256 until no improvement on the validation loss is observed for 50 consecutive epochs. The model weights at the epoch with the best validation loss are chosen for evaluation. Although common for training Transformers, we don't apply dropout at any point.\n\n\nResults and Discussion\n\nThe results for detection and segmentation are reported in Table 1. We compare our method to RIAD [22], which also uses an inpainting reconstruction-based method and computes anomaly maps based on GMS. Furthermore we compare the results to CutPaste [12] which uses a special data augmentation strategy to train a one-class classifier in a self-supervised way. CutPaste also offers results using pretrained representations, however we focus on the results without extra training data in accordance to our training procedure.\n\nTo our knowledge RIAD offers the current best performing model based on an inpainting scheme, whereas CutPaste is the best performing model on the MVTec AD benchmark not using extra training data. Our method outperforms RIAD on both detection and segmentation. On the detection task, CutPaste is superior to our method by 0.2%, however on the segmentation task, we can improve the result by 0.6%.\n\nIt is worth noting that there are two strongly underperforming categories: Cable contains many anomalous images where the defect lies in the overall constitution of the product (such as missing pieces). Combined with noise in large areas this makes these anomalies hard to detect via inpainting. Although the defects in capsule are per-se easily visible on the generated anomaly maps, our method does not learn the reconstruction of the typography sufficiently well, leading to high anomaly scores also on normal samples. \n\n\nAblation Studies\n\nWe examine the influence of certain building blocks in the architecture. All categories except Leather are trained for 200 epochs with settings as described in Section 4.1, if not stated otherwise. Leather takes the longest until details start to show in the inpainted patches, so for comparability the network is trained for 700 epochs. It should be noted that training for only 200 epochs for most categories does not lead to results comparable to Table 1. The average results of the ablation studies are reported in Table 2. We observe a decline in both detection and segmentation when omitting long residual connections. We furthermore examine the effect of using normal multihead self-attention (MSA) instead of MFSA as described in Section 3.1. Segmentation results do not improve using MFSA over MSA, however the average detection results improve by 0.7% when using MFSA. Lastly we test how the side length L of a patch window influences the performance by training the network also with L \u2208 {5, 9}. Detection and segmentation improve with growing patch windows, as more information from distant pixels can be used for the inpainting task. This comes with high computational cost however, as the computation of the dot product in self-attention is quadratic in the sequence length.\n\n\nConclusion\n\nInspired by the success of self-attention in vision tasks, we have used a Transformer model for visual anomaly detection by using an inpainting reconstruction approach. We argued that by discarding convolutions and using only selfattention to incorporate global context into reconstructions, anomalies can be successfully detected and localized. Hyperparameters such as the input image size, patch sequence length and patch dimension have a strong impact on the overall performance, and including the detection of good values for them in the training pipeline is paramount. With a simple pipeline as proposed, we have shown that InTra can reach state-of-the-art results on the popular MVTec AD dataset not using extra training data. \n\nFig. 1 .\n1Schematic overview of the proposed method. a.) The image is split into square patches. An inpainting transformer model (InTra) is trained to reconstruct a covered patch (black) from a long sequence of surrounding patches (red). Positional embeddings are added to the patches to include spatial context. b.) Examples: By reconstruction of all patches of an input image (left), a full reconstruction is obtained (middle). Comparison of original and reconstruction yields a pixel-wise anomaly score (right).\n\n\nsuch a square subgrid (\"window\") of patches defined by some index set S = {r, . . . , r + L \u2212 1} \u00d7 {s, . . . , s + L \u2212 1}. Here L is the side length of the window, and (r, s) is the grid position of the window's upper left patch. If (t, u) \u2208 S is the position of some patch, the formal task to inpaint (t, u) given S is to approximate the patch x (t,u) p using only the content and positions of all other patches x\n\nFig. 2 .\n2Overview of the proposed architecture. Left: Parts of an individual Transformer block. Right: A stack of Transformer blocks builds the full architecture. Long residual connections are used to add information from earlier blocks to later ones.\n\nFig. 4 .\n4Examples of qualitative results continued fromFigure 3. Categories from top to bottom: hazelnut, metal nut, pill, screw, toothbrush, transistor, zipper.\n\nTable 1 .\n1Detection/Segmentation results for MVTec AD. Results are presented in ROC AUC % on image level for detection, and on pixel level for segmentation.Category \nRIAD [22] CutPaste [12] InTra (Ours) InTra Image Size \nDet. / Seg. Det. / Seg. Det. / Seg. \nCarpet \n84.2 / 96.3 \n93.1 / 98.3 98.8 / 99.2 \n512 \u00d7 512 \nGrid \n99.6 / 98.8 99.9 / 97.5 100.0 / 98.8 \n256 \u00d7 256 \nLeather \n100.0 / 99.4 100.0 / 99.5 100.0 / 99.5 \n512 \u00d7 512 \nTile \n93.4 / 89.1 \n93.4 / 90.5 98.2 / 94.4 \n512 \u00d7 512 \nWood \n93.0 / 85.8 98.6 / 95.5 97.5 / 88.7 \n512 \u00d7 512 \navg. textures \n95.1 / 93.9 97.0 / 96.3 98.9 / 96.1 \nBottle \n99.9 / 98.4 98.3 / 97.6 100.0 / 97.1 \n256 \u00d7 256 \nCable \n81.9 / 94.2 80.6 / 90.0 \n70.3 / 91.0 \n256 \u00d7 256 \nCapsule \n88.4 / 92.8 96.2 / 97.4 86.5 / 97.7 \n320 \u00d7 320 \nHazelnut \n83.3 / 96.1 97.3 / 97.3 95.7 / 98.3 \n256 \u00d7 256 \nMetal Nut \n88.5 / 92.5 99.3 / 93.1 96.9 / 93.3 \n256 \u00d7 256 \nPill \n83.8 / 95.7 92.4 / 95.7 90.2 / 98.3 \n512 \u00d7 512 \nScrew \n84.5 / 98.8 \n86.3 / 96.7 95.7 / 99.5 \n320 \u00d7 320 \nToothbrush \n100.0 / 98.9 98.3 / 98.1 100.0 / 98.9 \n256 \u00d7 256 \nTransistor \n90.9 / 87.7 \n95.5 / 93.0 95.8 / 96.1 \n256 \u00d7 256 \nZipper \n98.1 / 97.8 99.4 / 99.3 99.4 / 99.2 \n512 \u00d7 512 \navg. objects \n89.9 / 94.3 94.3 / 95.8 93.0 / 96.9 \navg. all categories 91.7 / 94.2 95.2 / 96.0 95.0 / 96.6 \n\n\n\nTable 2 .\n2Detection/Segmentation results for the ablation studies. 'Regular' refers to the architecture as described in the previous sections, and 'NLR' to 'no long residual connections'. Det. / Seg. Det. / Seg.Regular \nNLR \nMSA \nL = 5 \nL = 9 \nDet. / Seg. \nDet. / Seg. \nDet. / Seg. \navg. text. 98.2 / 95.3 \n98.6 / 95 \n98.4 / 95.5 \n98.4 / 95.8 \n98.3 / 96.2 \navg. obj. \n90.7 / 95.5 90.0 / 95.3 \n89.6 / 95.5 \n90.0 / 95.2 \n90.8 / 95.8 \navg. all \n93.2 / 95.4 92.9 / 95.2 \n92.5 / 95.5 \n92.8 / 95.4 \n93.3 / 95.9 \ndiff. to Reg. \n-0.3 / -0.2 -0.7 / +0.1 \n-0.5 / 0.0 \n+0.1 / +0.5 \n\n\nA Appendix\nQuantifying attention flow in transformers. S Abnar, W H Zuidema, 10.18653/v1/2020.acl-main.385Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational Linguistics2020Abnar, S., Zuidema, W.H.: Quantifying attention flow in transformers. In: Proceed- ings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020. pp. 4190-4197. Association for Computational Linguistics (2020). https://doi.org/10.18653/v1/2020.acl-main.385\n\nDeep autoencoding models for unsupervised anomaly segmentation in brain mr images. C Baur, B Wiestler, S Albarqouni, N Navab, 10.1007/978-3-030-11723-8_16Lecture Notes in Com. Baur, C., Wiestler, B., Albarqouni, S., Navab, N.: Deep autoencoding models for unsupervised anomaly segmentation in brain mr images. Lecture Notes in Com- puter Science p. 161-169 (2019). https://doi.org/10.1007/978-3-030-11723-8 16\n\nMvtec ad -a comprehensive real-world dataset for unsupervised anomaly detection. P Bergmann, M Fauser, D Sattleger, C Steger, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBergmann, P., Fauser, M., Sattleger, D., Steger, C.: Mvtec ad -a comprehensive real-world dataset for unsupervised anomaly detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 9592-9600 (2019)\n\nDetecting anomalous faces with 'no peeking' autoencoders. A Bhattad, J Rock, D A Forsyth, CoRR abs/1802.05798Bhattad, A., Rock, J., Forsyth, D.A.: Detecting anomalous faces with 'no peek- ing' autoencoders. CoRR abs/1802.05798 (2018), http://arxiv.org/abs/1802. 05798\n\nAnomaly detection of defects on concrete structures with the convolutional autoencoder. J Chow, Z Su, J Wu, P Tan, X Mao, Y Wang, 10.1016/j.aei.2020.101105Advanced Engineering Informatics. 45101105Chow, J., Su, Z., Wu, J., Tan, P., Mao, X., Wang, Y.: Anomaly detection of defects on concrete structures with the convolutional autoencoder. Advanced Engineering Informatics 45, 101105 (2020). https://doi.org/10.1016/j.aei.2020.101105\n\nBERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M Chang, K Lee, K Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USALong and Short Papers1Association for Computational LinguisticsDevlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidi- rectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). pp. 4171-4186. Associa- tion for Computational Linguistics (2019). https://doi.org/10.18653/v1/n19-1423\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, International Conference on Learning Representations. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2021)\n\nDeep learning for medical anomaly detection -a survey. T Fernando, H Gammulle, S Denman, S Sridharan, C Fookes, 10.1145/3464423Fernando, T., Gammulle, H., Denman, S., Sridharan, S., Fookes, C.: Deep learning for medical anomaly detection -a survey (2021). https://doi.org/10.1145/3464423\n\nMemorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. D Gong, L Liu, V Le, B Saha, M R Mansour, S Venkatesh, A V Hengel, IEEE International Conference on Computer Vision (ICCV). Gong, D., Liu, L., Le, V., Saha, B., Mansour, M.R., Venkatesh, S., Hengel, A.v.d.: Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In: IEEE International Conference on Com- puter Vision (ICCV) (2019)\n\nAnomaly detection using deep learning based image completion. M Haselmann, D P Gruber, P Tabatabai, 10.1109/ICMLA.2018.0020117th IEEE International Conference on Machine Learning and Applications (ICMLA). Haselmann, M., Gruber, D.P., Tabatabai, P.: Anomaly detection using deep learning based image completion. In: 2018 17th IEEE International Confer- ence on Machine Learning and Applications (ICMLA). pp. 1237-1242 (2018). https://doi.org/10.1109/ICMLA.2018.00201\n\nAxial attention in multidimensional transformers. J Ho, N Kalchbrenner, D Weissenborn, T Salimans, Ho, J., Kalchbrenner, N., Weissenborn, D., Salimans, T.: Axial attention in multi- dimensional transformers (2019), http://arxiv.org/abs/1912.12180\n\nCutpaste: Self-supervised learning for anomaly detection and localization. C L Li, K Sohn, J Yoon, T Pfister, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Li, C.L., Sohn, K., Yoon, J., Pfister, T.: Cutpaste: Self-supervised learning for anomaly detection and localization. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9664-9674 (June 2021)\n\nUnsupervised region-based anomaly detection in brain mri with adversarial image inpainting. B Nguyen, A Feldman, S Bethapudi, A Jennings, C G Willcocks, 10.1109/ISBI48211.2021.94341152021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). Nguyen, B., Feldman, A., Bethapudi, S., Jennings, A., Willcocks, C.G.: Unsuper- vised region-based anomaly detection in brain mri with adversarial image inpaint- ing. In: 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). pp. 1127-1131 (2021). https://doi.org/10.1109/ISBI48211.2021.9434115\n\nImage transformer. N Parmar, A Vaswani, J Uszkoreit, L Kaiser, N Shazeer, A Ku, D Tran, PMLR (10-15Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research. the 35th International Conference on Machine Learning. Machine Learning Research80Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.: Image transformer. In: Proceedings of the 35th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 80, pp. 4055- 4064. PMLR (10-15 Jul 2018)\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015. ChamSpringer International PublishingRonneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi- cal image segmentation. In: Medical Image Computing and Computer-Assisted In- tervention -MICCAI 2015. pp. 234-241. Springer International Publishing, Cham (2015)\n\nAnomaly detection using autoencoders with nonlinear dimensionality reduction. M Sakurada, T Yairi, Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis. the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data AnalysisSakurada, M., Yairi, T.: Anomaly detection using autoencoders with non- linear dimensionality reduction. In: Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data Analysis. p. 4-11.\n\n. Mlsda&apos;14, 10.1145/2689746.2689747Association for Computing MachineryNew York, NY, USAMLSDA'14, Association for Computing Machinery, New York, NY, USA (2014). https://doi.org/10.1145/2689746.2689747\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems. p. 6000-6010. NIPS'17. the 31st International Conference on Neural Information Processing Systems. p. 6000-6010. NIPS'17Red Hook, NY, USACurran Associates IncVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Proceedings of the 31st International Conference on Neural Information Processing Systems. p. 6000-6010. NIPS'17, Curran Associates Inc., Red Hook, NY, USA (2017)\n\nLearning deep transformer models for machine translation. Q Wang, B Li, T Xiao, J Zhu, C Li, D F Wong, L S Chao, 10.18653/v1/P19-1176Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D.F., Chao, L.S.: Learn- ing deep transformer models for machine translation. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 1810-1822. Association for Computational Linguistics, Florence, Italy (Jul 2019). https://doi.org/10.18653/v1/P19-1176\n\nGradient magnitude similarity deviation: A highly efficient perceptual image quality index. W Xue, L Zhang, X Mou, A C Bovik, 10.1109/TIP.2013.2293423IEEE Transactions on Image Processing. 232Xue, W., Zhang, L., Mou, X., Bovik, A.C.: Gradient magnitude similarity devia- tion: A highly efficient perceptual image quality index. IEEE Transactions on Image Processing 23(2), 684-695 (Feb 2014). https://doi.org/10.1109/TIP.2013.2293423\n\nXlnet: Generalized autoregressive pretraining for language understanding. Z Yang, Z Dai, Y Yang, J Carbonell, R R Salakhutdinov, Q V Le, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, CanadaYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet: Generalized autoregressive pretraining for language understanding. In: Advances in Neural Information Processing Systems 32: Annual Conference on Neural Infor- mation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada. pp. 5754-5764 (2019)\n\nGenerative image inpainting with contextual attention. J Yu, Z Lin, J Yang, X Shen, X Lu, T S Huang, 10.1109/CVPR.2018.005772018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USAYu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Generative image inpainting with contextual attention. In: 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. pp. 5505-5514. Computer Vision Foundation / IEEE Computer Society (2018). https://doi.org/10.1109/CVPR.2018.00577\n\nReconstruction by inpainting for visual anomaly detection. V Zavrtanik, M Kristan, D Sko\u010daj, Pattern Recognition. 112107706Zavrtanik, V., Kristan, M., Sko\u010daj, D.: Reconstruction by inpainting for visual anomaly detection. Pattern Recognition 112, 107706 (2021).\n\n. 10.1016/j.patcog.2020.107706https://doi.org/10.1016/j.patcog.2020.107706\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, A C Bovik, H R Sheikh, E P Simoncelli, 10.1109/TIP.2003.819861IEEE Transactions on Image Processing. 134Zhou Wang, Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assess- ment: from error visibility to structural similarity. IEEE Transactions on Image Processing 13(4), 600-612 (2004). https://doi.org/10.1109/TIP.2003.819861\n", "annotations": {"author": "[{\"end\":160,\"start\":48},{\"end\":261,\"start\":161}]", "publisher": null, "author_last_name": "[{\"end\":63,\"start\":57},{\"end\":170,\"start\":166}]", "author_first_name": "[{\"end\":56,\"start\":48},{\"end\":165,\"start\":161}]", "author_affiliation": "[{\"end\":159,\"start\":93},{\"end\":260,\"start\":194}]", "title": "[{\"end\":45,\"start\":1},{\"end\":306,\"start\":262}]", "venue": null, "abstract": "[{\"end\":1453,\"start\":356}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1760,\"start\":1757},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1789,\"start\":1786},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3020,\"start\":3017},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3023,\"start\":3020},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3026,\"start\":3023},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3029,\"start\":3026},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3474,\"start\":3470},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3771,\"start\":3767},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3796,\"start\":3793},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5268,\"start\":5265},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6102,\"start\":6099},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6105,\"start\":6102},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6107,\"start\":6105},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6450,\"start\":6447},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6592,\"start\":6589},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6595,\"start\":6592},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6598,\"start\":6595},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6601,\"start\":6598},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6832,\"start\":6828},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7083,\"start\":7079},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7540,\"start\":7536},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7542,\"start\":7540},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7545,\"start\":7542},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7997,\"start\":7993},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8352,\"start\":8348},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8526,\"start\":8522},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8529,\"start\":8526},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8596,\"start\":8593},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9316,\"start\":9313},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10885,\"start\":10882},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11193,\"start\":11189},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12432,\"start\":12429},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12551,\"start\":12547},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13221,\"start\":13218},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13242,\"start\":13239},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13519,\"start\":13515},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14135,\"start\":14131},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14174,\"start\":14170},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15747,\"start\":15743},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16189,\"start\":16185},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17033,\"start\":17030},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19716,\"start\":19713},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20421,\"start\":20417},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20572,\"start\":20568}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24337,\"start\":23822},{\"attributes\":{\"id\":\"fig_1\"},\"end\":24754,\"start\":24338},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25008,\"start\":24755},{\"attributes\":{\"id\":\"fig_3\"},\"end\":25172,\"start\":25009},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26451,\"start\":25173},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27026,\"start\":26452}]", "paragraph": "[{\"end\":2331,\"start\":1469},{\"end\":4423,\"start\":2333},{\"end\":5204,\"start\":4425},{\"end\":5731,\"start\":5206},{\"end\":6297,\"start\":5748},{\"end\":7084,\"start\":6299},{\"end\":7315,\"start\":7086},{\"end\":7998,\"start\":7317},{\"end\":8232,\"start\":8000},{\"end\":8530,\"start\":8234},{\"end\":8984,\"start\":8532},{\"end\":9223,\"start\":9033},{\"end\":9638,\"start\":9281},{\"end\":9981,\"start\":9697},{\"end\":9987,\"start\":9983},{\"end\":10189,\"start\":10050},{\"end\":10544,\"start\":10229},{\"end\":10776,\"start\":10639},{\"end\":11072,\"start\":10822},{\"end\":12201,\"start\":11074},{\"end\":12786,\"start\":12239},{\"end\":13051,\"start\":12788},{\"end\":13704,\"start\":13053},{\"end\":13955,\"start\":13706},{\"end\":14175,\"start\":13957},{\"end\":14276,\"start\":14177},{\"end\":14627,\"start\":14414},{\"end\":15178,\"start\":14663},{\"end\":15617,\"start\":15268},{\"end\":16447,\"start\":15619},{\"end\":16591,\"start\":16449},{\"end\":16862,\"start\":16656},{\"end\":17535,\"start\":16878},{\"end\":17910,\"start\":17562},{\"end\":19553,\"start\":17912},{\"end\":19885,\"start\":19555},{\"end\":20292,\"start\":19887},{\"end\":20842,\"start\":20319},{\"end\":21240,\"start\":20844},{\"end\":21764,\"start\":21242},{\"end\":23073,\"start\":21785},{\"end\":23821,\"start\":23088}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9696,\"start\":9639},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10011,\"start\":9988},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10049,\"start\":10011},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10228,\"start\":10190},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10638,\"start\":10545},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10821,\"start\":10777},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14413,\"start\":14277},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15229,\"start\":15179},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15267,\"start\":15229},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16655,\"start\":16592}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19176,\"start\":19168},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20385,\"start\":20378},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22242,\"start\":22235},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22311,\"start\":22304}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1467,\"start\":1455},{\"attributes\":{\"n\":\"2\"},\"end\":5746,\"start\":5734},{\"attributes\":{\"n\":\"3\"},\"end\":9031,\"start\":8987},{\"attributes\":{\"n\":\"3.1\"},\"end\":9279,\"start\":9226},{\"attributes\":{\"n\":\"3.2\"},\"end\":12237,\"start\":12204},{\"attributes\":{\"n\":\"3.3\"},\"end\":14661,\"start\":14630},{\"attributes\":{\"n\":\"4\"},\"end\":16876,\"start\":16865},{\"attributes\":{\"n\":\"4.1\"},\"end\":17560,\"start\":17538},{\"attributes\":{\"n\":\"4.2\"},\"end\":20317,\"start\":20295},{\"attributes\":{\"n\":\"4.3\"},\"end\":21783,\"start\":21767},{\"attributes\":{\"n\":\"5\"},\"end\":23086,\"start\":23076},{\"end\":23831,\"start\":23823},{\"end\":24764,\"start\":24756},{\"end\":25018,\"start\":25010},{\"end\":25183,\"start\":25174},{\"end\":26462,\"start\":26453}]", "table": "[{\"end\":26451,\"start\":25331},{\"end\":27026,\"start\":26665}]", "figure_caption": "[{\"end\":24337,\"start\":23833},{\"end\":24754,\"start\":24340},{\"end\":25008,\"start\":24766},{\"end\":25172,\"start\":25020},{\"end\":25331,\"start\":25185},{\"end\":26665,\"start\":26464}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9222,\"start\":9214},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12342,\"start\":12334},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12785,\"start\":12778},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16860,\"start\":16852}]", "bib_author_first_name": "[{\"end\":27083,\"start\":27082},{\"end\":27092,\"start\":27091},{\"end\":27094,\"start\":27093},{\"end\":27741,\"start\":27740},{\"end\":27749,\"start\":27748},{\"end\":27761,\"start\":27760},{\"end\":27775,\"start\":27774},{\"end\":28150,\"start\":28149},{\"end\":28162,\"start\":28161},{\"end\":28172,\"start\":28171},{\"end\":28185,\"start\":28184},{\"end\":28633,\"start\":28632},{\"end\":28644,\"start\":28643},{\"end\":28652,\"start\":28651},{\"end\":28654,\"start\":28653},{\"end\":28932,\"start\":28931},{\"end\":28940,\"start\":28939},{\"end\":28946,\"start\":28945},{\"end\":28952,\"start\":28951},{\"end\":28959,\"start\":28958},{\"end\":28966,\"start\":28965},{\"end\":29360,\"start\":29359},{\"end\":29370,\"start\":29369},{\"end\":29379,\"start\":29378},{\"end\":29386,\"start\":29385},{\"end\":30353,\"start\":30352},{\"end\":30368,\"start\":30367},{\"end\":30377,\"start\":30376},{\"end\":30391,\"start\":30390},{\"end\":30406,\"start\":30405},{\"end\":30414,\"start\":30413},{\"end\":30429,\"start\":30428},{\"end\":30441,\"start\":30440},{\"end\":30453,\"start\":30452},{\"end\":30464,\"start\":30463},{\"end\":30473,\"start\":30472},{\"end\":30486,\"start\":30485},{\"end\":30915,\"start\":30914},{\"end\":30927,\"start\":30926},{\"end\":30939,\"start\":30938},{\"end\":30949,\"start\":30948},{\"end\":30962,\"start\":30961},{\"end\":31259,\"start\":31258},{\"end\":31267,\"start\":31266},{\"end\":31274,\"start\":31273},{\"end\":31280,\"start\":31279},{\"end\":31288,\"start\":31287},{\"end\":31290,\"start\":31289},{\"end\":31301,\"start\":31300},{\"end\":31314,\"start\":31313},{\"end\":31316,\"start\":31315},{\"end\":31708,\"start\":31707},{\"end\":31721,\"start\":31720},{\"end\":31723,\"start\":31722},{\"end\":31733,\"start\":31732},{\"end\":32163,\"start\":32162},{\"end\":32169,\"start\":32168},{\"end\":32185,\"start\":32184},{\"end\":32200,\"start\":32199},{\"end\":32436,\"start\":32435},{\"end\":32438,\"start\":32437},{\"end\":32444,\"start\":32443},{\"end\":32452,\"start\":32451},{\"end\":32460,\"start\":32459},{\"end\":32965,\"start\":32964},{\"end\":32975,\"start\":32974},{\"end\":32986,\"start\":32985},{\"end\":32999,\"start\":32998},{\"end\":33011,\"start\":33010},{\"end\":33013,\"start\":33012},{\"end\":33454,\"start\":33453},{\"end\":33464,\"start\":33463},{\"end\":33475,\"start\":33474},{\"end\":33488,\"start\":33487},{\"end\":33498,\"start\":33497},{\"end\":33509,\"start\":33508},{\"end\":33515,\"start\":33514},{\"end\":34059,\"start\":34058},{\"end\":34074,\"start\":34073},{\"end\":34085,\"start\":34084},{\"end\":34528,\"start\":34527},{\"end\":34540,\"start\":34539},{\"end\":35154,\"start\":35153},{\"end\":35165,\"start\":35164},{\"end\":35176,\"start\":35175},{\"end\":35186,\"start\":35185},{\"end\":35199,\"start\":35198},{\"end\":35208,\"start\":35207},{\"end\":35210,\"start\":35209},{\"end\":35219,\"start\":35218},{\"end\":35229,\"start\":35228},{\"end\":35850,\"start\":35849},{\"end\":35858,\"start\":35857},{\"end\":35864,\"start\":35863},{\"end\":35872,\"start\":35871},{\"end\":35879,\"start\":35878},{\"end\":35885,\"start\":35884},{\"end\":35887,\"start\":35886},{\"end\":35895,\"start\":35894},{\"end\":35897,\"start\":35896},{\"end\":36580,\"start\":36579},{\"end\":36587,\"start\":36586},{\"end\":36596,\"start\":36595},{\"end\":36603,\"start\":36602},{\"end\":36605,\"start\":36604},{\"end\":36997,\"start\":36996},{\"end\":37005,\"start\":37004},{\"end\":37012,\"start\":37011},{\"end\":37020,\"start\":37019},{\"end\":37033,\"start\":37032},{\"end\":37035,\"start\":37034},{\"end\":37052,\"start\":37051},{\"end\":37054,\"start\":37053},{\"end\":37613,\"start\":37612},{\"end\":37619,\"start\":37618},{\"end\":37626,\"start\":37625},{\"end\":37634,\"start\":37633},{\"end\":37642,\"start\":37641},{\"end\":37648,\"start\":37647},{\"end\":37650,\"start\":37649},{\"end\":38180,\"start\":38179},{\"end\":38193,\"start\":38192},{\"end\":38204,\"start\":38203},{\"end\":38537,\"start\":38533},{\"end\":38545,\"start\":38544},{\"end\":38547,\"start\":38546},{\"end\":38556,\"start\":38555},{\"end\":38558,\"start\":38557},{\"end\":38568,\"start\":38567},{\"end\":38570,\"start\":38569}]", "bib_author_last_name": "[{\"end\":27089,\"start\":27084},{\"end\":27102,\"start\":27095},{\"end\":27746,\"start\":27742},{\"end\":27758,\"start\":27750},{\"end\":27772,\"start\":27762},{\"end\":27781,\"start\":27776},{\"end\":28159,\"start\":28151},{\"end\":28169,\"start\":28163},{\"end\":28182,\"start\":28173},{\"end\":28192,\"start\":28186},{\"end\":28641,\"start\":28634},{\"end\":28649,\"start\":28645},{\"end\":28662,\"start\":28655},{\"end\":28937,\"start\":28933},{\"end\":28943,\"start\":28941},{\"end\":28949,\"start\":28947},{\"end\":28956,\"start\":28953},{\"end\":28963,\"start\":28960},{\"end\":28971,\"start\":28967},{\"end\":29367,\"start\":29361},{\"end\":29376,\"start\":29371},{\"end\":29383,\"start\":29380},{\"end\":29396,\"start\":29387},{\"end\":30365,\"start\":30354},{\"end\":30374,\"start\":30369},{\"end\":30388,\"start\":30378},{\"end\":30403,\"start\":30392},{\"end\":30411,\"start\":30407},{\"end\":30426,\"start\":30415},{\"end\":30438,\"start\":30430},{\"end\":30450,\"start\":30442},{\"end\":30461,\"start\":30454},{\"end\":30470,\"start\":30465},{\"end\":30483,\"start\":30474},{\"end\":30494,\"start\":30487},{\"end\":30924,\"start\":30916},{\"end\":30936,\"start\":30928},{\"end\":30946,\"start\":30940},{\"end\":30959,\"start\":30950},{\"end\":30969,\"start\":30963},{\"end\":31264,\"start\":31260},{\"end\":31271,\"start\":31268},{\"end\":31277,\"start\":31275},{\"end\":31285,\"start\":31281},{\"end\":31298,\"start\":31291},{\"end\":31311,\"start\":31302},{\"end\":31323,\"start\":31317},{\"end\":31718,\"start\":31709},{\"end\":31730,\"start\":31724},{\"end\":31743,\"start\":31734},{\"end\":32166,\"start\":32164},{\"end\":32182,\"start\":32170},{\"end\":32197,\"start\":32186},{\"end\":32209,\"start\":32201},{\"end\":32441,\"start\":32439},{\"end\":32449,\"start\":32445},{\"end\":32457,\"start\":32453},{\"end\":32468,\"start\":32461},{\"end\":32972,\"start\":32966},{\"end\":32983,\"start\":32976},{\"end\":32996,\"start\":32987},{\"end\":33008,\"start\":33000},{\"end\":33023,\"start\":33014},{\"end\":33461,\"start\":33455},{\"end\":33472,\"start\":33465},{\"end\":33485,\"start\":33476},{\"end\":33495,\"start\":33489},{\"end\":33506,\"start\":33499},{\"end\":33512,\"start\":33510},{\"end\":33520,\"start\":33516},{\"end\":34071,\"start\":34060},{\"end\":34082,\"start\":34075},{\"end\":34090,\"start\":34086},{\"end\":34537,\"start\":34529},{\"end\":34546,\"start\":34541},{\"end\":34935,\"start\":34922},{\"end\":35162,\"start\":35155},{\"end\":35173,\"start\":35166},{\"end\":35183,\"start\":35177},{\"end\":35196,\"start\":35187},{\"end\":35205,\"start\":35200},{\"end\":35216,\"start\":35211},{\"end\":35226,\"start\":35220},{\"end\":35240,\"start\":35230},{\"end\":35855,\"start\":35851},{\"end\":35861,\"start\":35859},{\"end\":35869,\"start\":35865},{\"end\":35876,\"start\":35873},{\"end\":35882,\"start\":35880},{\"end\":35892,\"start\":35888},{\"end\":35902,\"start\":35898},{\"end\":36584,\"start\":36581},{\"end\":36593,\"start\":36588},{\"end\":36600,\"start\":36597},{\"end\":36611,\"start\":36606},{\"end\":37002,\"start\":36998},{\"end\":37009,\"start\":37006},{\"end\":37017,\"start\":37013},{\"end\":37030,\"start\":37021},{\"end\":37049,\"start\":37036},{\"end\":37057,\"start\":37055},{\"end\":37616,\"start\":37614},{\"end\":37623,\"start\":37620},{\"end\":37631,\"start\":37627},{\"end\":37639,\"start\":37635},{\"end\":37645,\"start\":37643},{\"end\":37656,\"start\":37651},{\"end\":38190,\"start\":38181},{\"end\":38201,\"start\":38194},{\"end\":38211,\"start\":38205},{\"end\":38542,\"start\":38538},{\"end\":38553,\"start\":38548},{\"end\":38565,\"start\":38559},{\"end\":38581,\"start\":38571}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.385\",\"id\":\"b0\",\"matched_paper_id\":218487351},\"end\":27655,\"start\":27038},{\"attributes\":{\"doi\":\"10.1007/978-3-030-11723-8_16\",\"id\":\"b1\",\"matched_paper_id\":4792240},\"end\":28066,\"start\":27657},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":189857704},\"end\":28572,\"start\":28068},{\"attributes\":{\"doi\":\"CoRR abs/1802.05798\",\"id\":\"b3\"},\"end\":28841,\"start\":28574},{\"attributes\":{\"doi\":\"10.1016/j.aei.2020.101105\",\"id\":\"b4\",\"matched_paper_id\":218961406},\"end\":29275,\"start\":28843},{\"attributes\":{\"doi\":\"10.18653/v1/n19-1423\",\"id\":\"b5\",\"matched_paper_id\":52967399},\"end\":30274,\"start\":29277},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":225039882},\"end\":30857,\"start\":30276},{\"attributes\":{\"doi\":\"10.1145/3464423\",\"id\":\"b7\"},\"end\":31146,\"start\":30859},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":102353587},\"end\":31643,\"start\":31148},{\"attributes\":{\"doi\":\"10.1109/ICMLA.2018.00201\",\"id\":\"b9\",\"matched_paper_id\":53669904},\"end\":32110,\"start\":31645},{\"attributes\":{\"id\":\"b10\"},\"end\":32358,\"start\":32112},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":233204792},\"end\":32870,\"start\":32360},{\"attributes\":{\"doi\":\"10.1109/ISBI48211.2021.9434115\",\"id\":\"b12\",\"matched_paper_id\":222134193},\"end\":33432,\"start\":32872},{\"attributes\":{\"doi\":\"PMLR (10-15\",\"id\":\"b13\",\"matched_paper_id\":3353110},\"end\":33991,\"start\":33434},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3719281},\"end\":34447,\"start\":33993},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14613395},\"end\":34918,\"start\":34449},{\"attributes\":{\"doi\":\"10.1145/2689746.2689747\",\"id\":\"b16\"},\"end\":35124,\"start\":34920},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":13756489},\"end\":35789,\"start\":35126},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1176\",\"id\":\"b18\",\"matched_paper_id\":174799399},\"end\":36485,\"start\":35791},{\"attributes\":{\"doi\":\"10.1109/TIP.2013.2293423\",\"id\":\"b19\",\"matched_paper_id\":478859},\"end\":36920,\"start\":36487},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195069387},\"end\":37555,\"start\":36922},{\"attributes\":{\"doi\":\"10.1109/CVPR.2018.00577\",\"id\":\"b21\",\"matched_paper_id\":4072789},\"end\":38118,\"start\":37557},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":225114154},\"end\":38381,\"start\":38120},{\"attributes\":{\"doi\":\"10.1016/j.patcog.2020.107706\",\"id\":\"b23\"},\"end\":38457,\"start\":38383},{\"attributes\":{\"doi\":\"10.1109/TIP.2003.819861\",\"id\":\"b24\",\"matched_paper_id\":207761262},\"end\":38880,\"start\":38459}]", "bib_title": "[{\"end\":27080,\"start\":27038},{\"end\":27738,\"start\":27657},{\"end\":28147,\"start\":28068},{\"end\":28929,\"start\":28843},{\"end\":29357,\"start\":29277},{\"end\":30350,\"start\":30276},{\"end\":31256,\"start\":31148},{\"end\":31705,\"start\":31645},{\"end\":32433,\"start\":32360},{\"end\":32962,\"start\":32872},{\"end\":33451,\"start\":33434},{\"end\":34056,\"start\":33993},{\"end\":34525,\"start\":34449},{\"end\":35151,\"start\":35126},{\"end\":35847,\"start\":35791},{\"end\":36577,\"start\":36487},{\"end\":36994,\"start\":36922},{\"end\":37610,\"start\":37557},{\"end\":38177,\"start\":38120},{\"end\":38531,\"start\":38459}]", "bib_author": "[{\"end\":27091,\"start\":27082},{\"end\":27104,\"start\":27091},{\"end\":27748,\"start\":27740},{\"end\":27760,\"start\":27748},{\"end\":27774,\"start\":27760},{\"end\":27783,\"start\":27774},{\"end\":28161,\"start\":28149},{\"end\":28171,\"start\":28161},{\"end\":28184,\"start\":28171},{\"end\":28194,\"start\":28184},{\"end\":28643,\"start\":28632},{\"end\":28651,\"start\":28643},{\"end\":28664,\"start\":28651},{\"end\":28939,\"start\":28931},{\"end\":28945,\"start\":28939},{\"end\":28951,\"start\":28945},{\"end\":28958,\"start\":28951},{\"end\":28965,\"start\":28958},{\"end\":28973,\"start\":28965},{\"end\":29369,\"start\":29359},{\"end\":29378,\"start\":29369},{\"end\":29385,\"start\":29378},{\"end\":29398,\"start\":29385},{\"end\":30367,\"start\":30352},{\"end\":30376,\"start\":30367},{\"end\":30390,\"start\":30376},{\"end\":30405,\"start\":30390},{\"end\":30413,\"start\":30405},{\"end\":30428,\"start\":30413},{\"end\":30440,\"start\":30428},{\"end\":30452,\"start\":30440},{\"end\":30463,\"start\":30452},{\"end\":30472,\"start\":30463},{\"end\":30485,\"start\":30472},{\"end\":30496,\"start\":30485},{\"end\":30926,\"start\":30914},{\"end\":30938,\"start\":30926},{\"end\":30948,\"start\":30938},{\"end\":30961,\"start\":30948},{\"end\":30971,\"start\":30961},{\"end\":31266,\"start\":31258},{\"end\":31273,\"start\":31266},{\"end\":31279,\"start\":31273},{\"end\":31287,\"start\":31279},{\"end\":31300,\"start\":31287},{\"end\":31313,\"start\":31300},{\"end\":31325,\"start\":31313},{\"end\":31720,\"start\":31707},{\"end\":31732,\"start\":31720},{\"end\":31745,\"start\":31732},{\"end\":32168,\"start\":32162},{\"end\":32184,\"start\":32168},{\"end\":32199,\"start\":32184},{\"end\":32211,\"start\":32199},{\"end\":32443,\"start\":32435},{\"end\":32451,\"start\":32443},{\"end\":32459,\"start\":32451},{\"end\":32470,\"start\":32459},{\"end\":32974,\"start\":32964},{\"end\":32985,\"start\":32974},{\"end\":32998,\"start\":32985},{\"end\":33010,\"start\":32998},{\"end\":33025,\"start\":33010},{\"end\":33463,\"start\":33453},{\"end\":33474,\"start\":33463},{\"end\":33487,\"start\":33474},{\"end\":33497,\"start\":33487},{\"end\":33508,\"start\":33497},{\"end\":33514,\"start\":33508},{\"end\":33522,\"start\":33514},{\"end\":34073,\"start\":34058},{\"end\":34084,\"start\":34073},{\"end\":34092,\"start\":34084},{\"end\":34539,\"start\":34527},{\"end\":34548,\"start\":34539},{\"end\":34937,\"start\":34922},{\"end\":35164,\"start\":35153},{\"end\":35175,\"start\":35164},{\"end\":35185,\"start\":35175},{\"end\":35198,\"start\":35185},{\"end\":35207,\"start\":35198},{\"end\":35218,\"start\":35207},{\"end\":35228,\"start\":35218},{\"end\":35242,\"start\":35228},{\"end\":35857,\"start\":35849},{\"end\":35863,\"start\":35857},{\"end\":35871,\"start\":35863},{\"end\":35878,\"start\":35871},{\"end\":35884,\"start\":35878},{\"end\":35894,\"start\":35884},{\"end\":35904,\"start\":35894},{\"end\":36586,\"start\":36579},{\"end\":36595,\"start\":36586},{\"end\":36602,\"start\":36595},{\"end\":36613,\"start\":36602},{\"end\":37004,\"start\":36996},{\"end\":37011,\"start\":37004},{\"end\":37019,\"start\":37011},{\"end\":37032,\"start\":37019},{\"end\":37051,\"start\":37032},{\"end\":37059,\"start\":37051},{\"end\":37618,\"start\":37612},{\"end\":37625,\"start\":37618},{\"end\":37633,\"start\":37625},{\"end\":37641,\"start\":37633},{\"end\":37647,\"start\":37641},{\"end\":37658,\"start\":37647},{\"end\":38192,\"start\":38179},{\"end\":38203,\"start\":38192},{\"end\":38213,\"start\":38203},{\"end\":38544,\"start\":38533},{\"end\":38555,\"start\":38544},{\"end\":38567,\"start\":38555},{\"end\":38583,\"start\":38567}]", "bib_venue": "[{\"end\":27220,\"start\":27133},{\"end\":27831,\"start\":27811},{\"end\":28271,\"start\":28194},{\"end\":28630,\"start\":28574},{\"end\":29030,\"start\":28998},{\"end\":29576,\"start\":29418},{\"end\":30548,\"start\":30496},{\"end\":30912,\"start\":30859},{\"end\":31380,\"start\":31325},{\"end\":31848,\"start\":31769},{\"end\":32160,\"start\":32112},{\"end\":32558,\"start\":32470},{\"end\":33122,\"start\":33055},{\"end\":33643,\"start\":33533},{\"end\":34163,\"start\":34092},{\"end\":34636,\"start\":34548},{\"end\":35354,\"start\":35242},{\"end\":36011,\"start\":35924},{\"end\":36674,\"start\":36637},{\"end\":37171,\"start\":37059},{\"end\":37744,\"start\":37681},{\"end\":38232,\"start\":38213},{\"end\":38643,\"start\":38606},{\"end\":27300,\"start\":27222},{\"end\":28335,\"start\":28273},{\"end\":29741,\"start\":29578},{\"end\":32633,\"start\":32560},{\"end\":33725,\"start\":33645},{\"end\":34169,\"start\":34165},{\"end\":34711,\"start\":34638},{\"end\":35470,\"start\":35356},{\"end\":36100,\"start\":36013},{\"end\":37203,\"start\":37173},{\"end\":37769,\"start\":37746}]"}}}, "year": 2023, "month": 12, "day": 17}