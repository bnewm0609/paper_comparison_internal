{"id": 3681715, "updated": "2023-09-29 06:49:25.101", "metadata": {"title": "Driven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments", "authors": "[{\"first\":\"Dan\",\"last\":\"Barnes\",\"middle\":[]},{\"first\":\"Will\",\"last\":\"Maddern\",\"middle\":[]},{\"first\":\"Geoffrey\",\"last\":\"Pascoe\",\"middle\":[]},{\"first\":\"Ingmar\",\"last\":\"Posner\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 11, "day": 17}, "abstract": "We present a self-supervised approach to ignoring\"distractors\"in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from the Oxford RobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1711.06623", "mag": "2963170338", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/BarnesMPP18", "doi": "10.1109/icra.2018.8460564"}}, "content": {"source": {"pdf_hash": "7bf3981825a77d1a7973577a1dbc9c5d39270e12", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1711.06623v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1711.06623", "status": "GREEN"}}, "grobid": {"id": "b3f6b08fdc49cdabd0b4c91bb3a6c01821f9e80c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7bf3981825a77d1a7973577a1dbc9c5d39270e12.txt", "contents": "\nDriven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments\n\n\nDan Barnes \nWill Maddern \nGeoffrey Pascoe \nIngmar Posner \nDriven to Distraction: Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban Environments\n\nWe present a self-supervised approach to ignoring \"distractors\" in camera images for the purposes of robustly estimating vehicle motion in cluttered urban environments. We leverage offline multi-session mapping approaches to automatically generate a per-pixel ephemerality mask and depth map for each input image, which we use to train a deep convolutional network. At run-time we use the predicted ephemerality and depth as an input to a monocular visual odometry (VO) pipeline, using either sparse features or dense photometric matching. Our approach yields metric-scale VO using only a single camera and can recover the correct egomotion even when 90% of the image is obscured by dynamic, independently moving objects. We evaluate our robust VO methods on more than 400km of driving from the Oxford RobotCar Dataset and demonstrate reduced odometry drift and significantly improved egomotion estimation in the presence of large moving vehicles in urban traffic.\n\nI. INTRODUCTION\n\nAutonomous vehicle operation in crowded urban environments presents a number of key challenges to any system based on visual navigation and motion estimation. In urban traffic where up to 90% of an image can be obscured by a large moving object (e.g. bus or truck), standard outlier rejection schemes such as RANSAC [1] will produce incorrect motion estimates due to the large consensus of features tracked on the moving object. The key to robust \"distractionfree\" visual navigation is a deeper understanding of which image regions are static and which are ephemeral in order to better decide which features to use for motion estimation.\n\nIn this paper we leverage large-scale offline mapping and deep learning approaches to produce a per-pixel ephemerality mask at run-time without requiring any semantic classification or manual labelling, as illustrated in Fig. 1 and the project video 1 . The ephemerality mask predicts stable image regions (e.g. buildings, road markings, static landmarks) that are likely to be useful for motion estimation, in contrast to dynamic or ephemeral objects (e.g. pedestrian and vehicle traffic, vegetation, temporary signage). In contrast to semantic segmentation approaches that explicitly label objects belonging to a-priori chosen classes and hence require manually annotated training data, our approach is trained using repeated traversals of the same route with a LIDAR-equipped survey vehicle producing per-pixel depth and ephemerality labels for a deep convolutional network as a fully selfsupervised process.\n\nWe integrate the ephemerality mask as a component of a monocular visual odometry (VO) pipeline as an outlier Robust motion estimation in urban environments using a single camera and a learned ephemerality mask. When making a left turn onto a main road, a large bus passes in front of the vehicle (green arrow) obscuring the view of the scene (top left). Our learned ephemerality mask correctly identifies the bus as an unreliable region of the image for the purposes of motion estimation (top right). Traditional visual odometry (VO) approaches will incorrectly estimate a strong translational motion to the right due to the dominant motion of the bus (bottom left), whereas our approach correctly recovers the vehicle egomotion (bottom right). rejection scheme. By leveraging the depth and ephemerality outputs of the network, we can produce robust metricscale VO using only a single camera mounted to a vehicle. Our approach leads to significantly more reliable motion estimation when evaluated over hundreds of kilometres of driving in complex urban environments in the presence of heavy traffic and other challenging conditions.\n\n\nII. RELATED WORK\n\nEstimating an ephemerality mask is closely related to background subtraction approaches [2], [3], which build statistics over background appearance based on training data from a static camera to identify discrepancies in live images. These methods are typically used in surveillance applications and have limited robustness to general 3D camera motion in complex scenes, as experienced on a vehicle [4], [5].\n\nConversely, there is a significant body of work on detection and tracking of moving (foreground) objects [6], [7], [8], which has been applied to robust VO in dynamic environments [9] and scale references for monocular SLAM [10]. However, these approaches require large quantities of manually-labelled training data of moving objects (e.g. cars, pedestrians) and the chosen object classes must cover all possibly-moving objects to avoid false negatives. Recent 3D SLAM approaches have integrated per-pixel semantic segmentation layers to improve reconstruction quality [11], [12], but again rely on laboriously manually-annotated training data and chosen classes that encompass all object categories.\n\nUnsupervised approaches have recently been introduced to estimate depth [13], egomotion [14] and 3D reconstruction [15]. These methods are attractive for large-scale use as they only require raw video footage from a monocular or stereo camera, without any ground-truth motion estimates or semantic labels. In particular, [14] introduces an \"explainability mask\", which highlights image regions that disagree with the dominant motion estimate. However, the explainability mask differs from the ephemerality mask in that it only recognises non-dominant moving objects, and hence will still produce incorrect motion estimates when significantly occluded by a large, independently moving object.\n\nOur approach is inspired by the distraction-suppression methods presented in [16], [17]. Both methods use a prior 3D map to estimate a mask that quantifies reliability for motion estimation, which is integrated into a VO pipeline. We significantly extend the map prior approach of [16] to multi-session mapping and quantify ephemerality using a structural entropy metric, and use the result to automatically generate training data for a deep convolutional network. As a result, our approach does not rely on live localisation against a prior map or live dense depth estimation from stereo, and hence can operate in a wider range of (unmapped) locations with a reduced (monocular-only) sensor suite.\n\n\nIII. LEARNING EPHEMERALITY MASKS\n\nIn this section we outline our approach for automatically building ephemerality masks by leveraging an offline 3D mapping pipeline. Note that LIDAR and stereo camera sensors are only required for the survey vehicle to collect training data; at run-time only a monocular camera is required. Our method takes the following steps: 1) Prior 3D Mapping: Using a survey vehicle equipped with a stereo camera and LIDAR scanner, we perform multiple traversals of the target environment. By analysing structural consistency across multiple mapping sessions with an entropy-based approach, we determine what constitutes the static (non-ephemeral) structure of the scene. 2) Ephemerality Labelling: We project the prior 3D static structure into every stereo camera image collected during the survey, and compare it to the structure computed by a dense stereo approach (similar to [16]). In the presence of\nG LC L j t C j t W G C j t W p j 1 p j 2 p j n p j 3 N (p j 3 ) N (p j 2 ) p 1 n p 2 n p m n p j 4 p j 5 p j 6 Fig. 2.\nMulti-session mapping and 3D pointcloud entropy computation. For each traversal j, we compute the global pose of the vehicle G C j t W at each timestamp t and project points p into the global frame W . We then analyse the neighbourhood N of each point p j i ; in neighbourhoods where points are well distributed between traversals {1 \u00b7 \u00b7 \u00b7 j} such as N (p j 2 ) the scene is likely to be static, and where points are mostly derived from one traversal such as N (p j 3 ) the structure is ephemeral. We quantify static scenes using an entropy metric applied to each neighbourhood N (p).\n\ntraffic or dynamic objects these will differ considerably; we compute ephemerality as a weighted sum of disparity and normal difference between prior and true 3D structure.\n\n3) Network Training: We train a deep convolutional network to predict the resulting pixel-wise depth and ephemerality mask using only input monocular images. At run-time we produce live depth and ephemerality masks even in locations not traversed by the survey vehicle.\n\nIn the following sections we describe these steps in detail.\n\n\nA. Prior 3D Mapping\n\nGiven a survey vehicle equipped with a camera C and LIDAR L illustrated in Fig. 2 that has performed a number of traverses j of an environment, we recover each global camera pose G C j t W at time t relative to world frame W with a largescale offline process using the stereo mapping and navigation approach in [18]. We then compute the position of each 3D LIDAR point p j i \u2208 R 3 in world frame W using the camera pose and LIDAR-camera calibration G LC as follows:\nW p j i = G C j t W G LC p j i(1)\nGiven the pointcloud of all points p collected from all traversals j, we wish to compute the local entropy of each region of the pointcloud, to quantify how reliable the region is across each traversal. We define a neighbourhood function N (\u00b7), where a point p k t belongs to a neighbourhood if it satisfies the following condition:\np k Fig. 3.\nPrior 3D mapping to determine the static 3D scene structure. Alignment of multiple traversals of a route (top left) will yield a large number of points only present in single traversals, e.g. traffic or parked vehicles, here shown in white. These points will corrupt a synthetic depth map (top right). Our entropy-based approach removes 3D points that were only observed in some traversals, and retains the structure that remained static for the duration of data collection (bottom left), resulting in high-quality synthetic depth maps (bottom right). sourced from one or two traversals are likely to be ephemeral objects. We compute the neighbourhood entropy H(p i ) of each point across all n traversals as follows:\nH (p i ) = \u2212 n j=1 p i (j) log (p i (j))(4)\nWe classify a point p i as static structure p S i if the neighbourhood entropy H(p i ) exceeds a minimum threshold \u03b2; all other points are estimated to be ephemeral and are removed from the static 3D prior. The pointcloud construction, neighbourhood entropy and ephemeral point removal process are illustrated in Fig. 3.\n\n\nB. Ephemerality Labelling\n\nGiven the prior 3D static pointcloud p S and globally aligned camera poses C, we can produce a synthetic depth map for each survey image, as illustrated in Fig. 4. To handle visibility constraints we make use of the hidden point removal approach in [19]. For every pixel i into which a valid prior 3D point projects, we compute the expected disparity d S i and normal n S i using the local 3D structure of the pointcloud.\n\nIn the presence of dynamic objects, the scene observed from the camera will differ from the expected prior 3D map. We use the offline dense stereo reconstruction approach of [20] to compute the true disparity d i and normal n i for each pixel in the survey image, illustrated in Fig. 4. We define the ephemerality mask E i as the weighted difference between the expected static and true disparity and normals as follows:\nE i = \u03b3 d S i \u2212 d i 1 + \u03b4 cos \u22121 n S i \u00b7 n i(5)\nwhere \u03b3 and \u03b4 are weighting parameters, and E i is bounded to [0, 1] after computation.\n\n\nC. Network Architecture\n\nWe adopt a convolutional encoder-multi-decoder network architecture to predict both disparity and ephemerality masks from a single image, as illustrated in Fig. 5. To preserve fine structure we add skip connections between the higher convolutional layers, similar to the U-Net approach [21]. To train the disparity output we use the stereo photometric loss proposed in [22], optionally semi-supervised using the prior LIDAR disparity d S i to ensure metric-scaled outputs. We balance the losses between the different output stages using the multi-task learning approach in [23], which continuously updates the inter-task weighting during training.\n\n\nIV. EPHEMERALITY-AWARE VISUAL ODOMETRY\n\nWe leverage the live depth and ephemerality mask produced by the network to produce reliable visual odometry estimates accurate to metric scale. We present two robust VO approaches: a sparse feature-based approach and a dense photometric approach. Each integrates the ephemerality mask in order to estimate egomotion using only static parts of the scene, and uses the learned depth to estimate relative motion to the correct scale. This improves upon traditional monocular VO systems that cannot recover absolute scale [24]. Both our odometry approaches are optimised for realtime performance on a vehicle platform.\n\n\nA. Sparse Monocular Odometry\n\nOur sparse monocular VO approach is derived from wellknown stereo approaches [25], where sets of features are  Fig. 4. Ephemerality labelling process. From input images (left) we compute the true disparity d i and normals n i using an offline dense stereo approach. We then project the prior 3D pointcloud p S into the image to form the prior disparity d S i and prior normal n S i . The disparity and normal error terms are combined to form the ephemerality mask (right).\n\n\nEncoder\n\n\nEphemerality Decoder\n\n\nDisparity Decoder\n\n\nFig. 5.\n\nNetwork architecture for ephemerality and disparity learning. We use a common encoder which splits to multiple decoders for the ephemerality mask and disparity outputs. Fig. 6. Input data for ephemerality-aware visual odometry. For a given input image (top left), the network predicts a dense depth map (top right) and an ephemerality mask. For sparse VO approaches, the ephemerality mask is used to select which features are used for optimisation (bottom left), and for dense VO approaches the photometric error term is weighted directly by the ephemerality mask (bottom right). detected and matched across successive frames to build a relative pose estimate. Each feature x i is parameterised as follows:\nx i = \uf8ee \uf8f0 u i v i d i \uf8f9 \uf8fb (6)\nwhere (u i , v i ) are the pixel coordinates and d i is the disparity predicted by the deep convolutional network. The relative pose \u03be \u2208 SE(3) is recovered by minimising the reprojection error between matched features x i andx i :\narg min \u03be i\u2208F s (E i ) x i \u2212 \u03c9 (x i , \u03be) 2 2(7)\nThe warping function \u03c9(\u00b7) \u2192 R 2 projects the matched featurex i into the current image according to relative pose \u03be and the camera intrinsics. The set of all extracted features F is typically a small subset of the total number of pixels in the image. The step function s(E i ) is used to disable the residual according to the predicted ephemerality as follows:\ns (E i ) = 1, E i < \u03c4 0, otherwise(8)\nwhere \u03c4 is the maximum ephemerality threshold for a valid feature, typically set to 0.5. In practice we detect sparse features using FAST corners [26] and match using BRIEF descriptors [27] for real-time operation.\n\n\nB. Dense Monocular Odometry\n\nFor the dense monocular approach, we adopt the method of [28] and combine our learned depth maps with the photometric relative pose estimation of [29]. Rather than a subset of pixels F, all pixels i within the reference keyframe image I r are warped into the current image I c and the relative pose \u03be is recovered by minimising the photometric error as follows:\narg min \u03be i\u2208Ir (1 \u2212 E i ) I r (x i ) \u2212 I c (\u03c9 (x i , \u03be)) 2 2(9)\nwhere the image function I(x i ) \u2192 R + returns the pixel intensity at location (u i , v i ). Note that the ephemerality mask is used directly to weight the photometric residual; no thresholding is required. Fig. 6 illustrates the predicted depth, selected sparse features and weighted dense intensity values used for a typical urban scene.\n\n\nV. EXPERIMENTAL SETUP\n\nWe benchmarked our approach using hundreds of kilometres of data collected from an autonomous vehicle platform in a complex urban environment. Our goal was to quantify the performance of the ephemerality-aware visual odometry approach in the presence of large dynamic objects in traffic.\n\n\nA. Network Training\n\nWe train our approach using eight 10km traversals from the Oxford RobotCar dataset [30] for a total of approximately 80km of driving. The RobotCar vehicle is equipped with a Bumblebee XB3 stereo camera and a LMS-151 pushbroom LIDAR scanner. For training we downsample the input images to 640 \u00d7 256 pixels and subsample to one image every metre before use; a total of 60,850 images were used for training. At run-time we produce ephemerality masks and depth maps at 50Hz using a single GTX 1080 Ti GPU.\n\n\nB. Evaluation Metrics\n\nWe evaluate our approach using 42 further traversals of the Oxford route for a total of over 400km. The evaluation datasets contain multiple detours and alternate routes, ensuring the method is tested in (unmapped) locations not present in the training datasets. To quantify the performance of the ephemerality-aware VO systems, we compute translational and rotational drift rates using the approach proposed in the KITTI odometry benchmark [31]. Specifically, we compute the average end-point-error for all subsequences of length (100, 200, . . . , 800) metres compared to the INS system installed on the vehicle.\n\nIn addition, we compare the instantaneous translational velocities of each method to that reported by the INS system (based on doppler velocity measurements). We manually selected 6,000 locations that include distractors, and evaluate velocity estimation errors in comparison to the average of all locations. This allows us to focus on dynamic scenes where independently moving objects produce erroneous velocity estimates in the baseline VO methods.\n\n\nVI. RESULTS\n\nIn addition to the quantitative results listed below, we present qualitative results for ephemerality masks produced in a range of different locations in Fig. 7.\n\n\nA. Odometry Drift Rates\n\nThe end-point-error evaluation for each of the methods is presented in Table I. In both cases, the addition of the ephemerality mask reduced both average translational and rotational drift over the full set of evaluation datasets. Note that the metric scale for translational drift is derived from the depth map produced by the network, and hence both systems report translation in units of metres with low overall error rates using only a monocular camera. The sparse VO approach provided lower overall translational drift, whereas the dense approach produced lower orientation drift.\n\n\nB. Velocity Estimates\n\nThe velocity error evaluation for each of the methods is presented in Table II. Across all the evaluation datasets, the ephemerality-aware odometry approaches produce lower average velocity errors. However, in locations with distractors, the ephemerality-aware approaches produce significantly more accurate velocity estimates than the baseline approaches. In particular, the robust sparse VO approach is almost unaffected by distractors, whereas the baseline method reports errors 4 times greater. The dense VO approach generally produces poorer translational velocity estimates than the sparse approach, which corresponds with higher translational drift rates reported in the previous section. Fig.  8 presents the distribution of velocity errors for each of the approaches in the presence of distractors.\n\n\nVII. CONCLUSIONS\n\nIn this paper we introduced the concept of an ephemerality mask, which estimates the likelihood that any pixel in an input image corresponds to either reliable static structure or dynamic objects in the environment, and can be learned using an automatic self-supervised approach. Crucially, we do not require any manual labelling or choice of semantic classes in order to train our approach, and at run-time we only require a single monocular camera to produce reliable ephemerality-aware visual odometry to metric scale. Over hundreds of kilometres our approach produces improved odometry resulting in lower drift rates, and significantly more robust velocity estimates in the presence of large dynamic objects in urban scenes.\n\nThe benefits of our approach are not restricted to improving motion estimation, and there are a number of avenues to explore in future work. Fig. 9 illustrates a foreground/background segmentation performed using the ephemerality mask; where we currently use the background to guide motion estimation, a detection and classification approach could be guided by the foreground mask to efficiently track dynamic objects in the scene. We plan to integrate the approaches in this paper for improved localisation, motion estimation, obstacle avoidance and scene understanding for fully autonomous vehicles operating in complex urban environments.\n\n\nVIII. ACKNOWLEDGEMENTS\n\nThis work was supported by the UK EPSRC Doctoral Training Partnership and Programme Grant EP/M019918/1. Fig. 7. Ephemerality masks produced in challenging urban environments. The masks reliably highlight a diverse range of dynamic objects (cars, buses, trucks, cyclists, pedestrians, strollers) with highly varied distances and orientations. Even buses and trucks that almost entirely obscure the camera image are successfully masked despite the lack of other scene context. Robust VO approaches that make use of the ephemerality mask can provide correct motion estimates even when more than 90% of the static scene is occluded by an independently moving object. The sparse ephemerality-aware approach significantly outperforms the baseline approach, producing far fewer outliers above 0.5 m/s. The dense ephemeralityaware approach does not perform as well, but still outperforms the baseline. The vertical axis is scaled to highlight the outliers. Fig. 9. Ephemerality masks are widely applicable for autonomous vehicles.\n\nIn the above scene the ephemerality mask can be used to inform localisation against only the static scene (bottom left) whilst guiding object detection to only the ephemeral elements (bottom right).\n\nFig. 8 .\n8Velocity estimation errors in the presence of distractors.\n\nTABLE I ODOMETRY\nIDRIFT EVALUATIONVO Method \nTranslation \n[%] \nRotation [deg/m] \n\nSparse \n6.55 \n0.0353 \nSparse w/Ephemerality \n6.38 \n0.0321 \nDense \n7.15 \n0.0373 \nDense w/Ephemerality \n6.52 \n0.0307 \n\nTABLE II \nVELOCITY ERROR EVALUATION \n\nVO Method \nAll [m/s] \nDistractors [m/s] \nSparse \n0.0548 \n0.220 \nSparse w/Ephemerality \n0.0406 \n0.0489 \nDense \n0.0568 \n0.766 \nDense w/Ephemerality \n0.0407 \n0.424 \n\n\nhttps://youtu.be/ebIrBn_nc-k\nt \u2208 N (p i ) \u21d0\u21d2 p i \u2212 p k t 2 < \u03b1(2)where \u03b1 is a neighbourhood size parameter, typically set to 0.5m in our experiments. For each query point p i , we then build a distribution p i (j) over the traverses j from which points fell in the neighbourhood of the query point as follows:p i (j) = 1 |N (p i )| p k t \u2208N (pi) 1, j = k 0, otherwise(3)Intuitively, neighbourhoods of points that are welldistributed between different traversals indicate static structure, whereas neighbourhoods of points that were only\n\nRandom sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. M A Fischler, R C Bolles, Communications of the ACM. 246M. A. Fischler and R. C. Bolles, \"Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography,\" Communications of the ACM, vol. 24, no. 6, pp. 381-395, 1981.\n\nBackground subtraction techniques: a review. M Piccardi, Systems, man and cybernetics. IEEE4M. Piccardi, \"Background subtraction techniques: a review,\" in Sys- tems, man and cybernetics, 2004 IEEE international conference on, vol. 4. IEEE, 2004, pp. 3099-3104.\n\nSurvey on background modeling and foreground detection for real time video surveillance. S Jeeva, M Sivabalakrishnan, Procedia Computer Science. 50S. Jeeva and M. Sivabalakrishnan, \"Survey on background modeling and foreground detection for real time video surveillance,\" Procedia Computer Science, vol. 50, pp. 566-571, 2015.\n\nStatistical background subtraction for a mobile observer. E Hayman, J.-O Eklundh, CVPR. IEEE. 67E. Hayman and J.-O. Eklundh, \"Statistical background subtraction for a mobile observer,\" in CVPR. IEEE, 2003, p. 67.\n\nBackground subtraction for freely moving cameras. Y Sheikh, O Javed, T Kanade, Computer Vision, 2009 IEEE 12th International Conference on. IEEEY. Sheikh, O. Javed, and T. Kanade, \"Background subtraction for freely moving cameras,\" in Computer Vision, 2009 IEEE 12th Inter- national Conference on. IEEE, 2009, pp. 1219-1225.\n\nObject tracking: A survey. A Yilmaz, O Javed, M Shah, Acm computing surveys (CSUR). 3813A. Yilmaz, O. Javed, and M. Shah, \"Object tracking: A survey,\" Acm computing surveys (CSUR), vol. 38, no. 4, p. 13, 2006.\n\nObject detection with discriminatively trained part-based models. P F Felzenszwalb, R B Girshick, D Mcallester, D Ramanan, IEEE transactions on pattern analysis and machine intelligence. 32P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, \"Object detection with discriminatively trained part-based models,\" IEEE transactions on pattern analysis and machine intelligence, vol. 32, no. 9, pp. 1627-1645, 2010.\n\nFaster R-CNN: towards realtime object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in neural information processing systems. S. Ren, K. He, R. Girshick, and J. Sun, \"Faster R-CNN: towards real- time object detection with region proposal networks,\" in Advances in neural information processing systems, 2015, pp. 91-99.\n\nDynamic objects detection through visual odometry and stereo-vision: a study of inaccuracy and improvement sources. A Bak, S Bouchafa, D Aubert, Machine vision and applications. A. Bak, S. Bouchafa, and D. Aubert, \"Dynamic objects detection through visual odometry and stereo-vision: a study of inaccuracy and improvement sources,\" Machine vision and applications, pp. 1-17, 2014.\n\nRobust scale estimation in real-time monocular SFM for autonomous driving. S Song, M Chandraker, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS. Song and M. Chandraker, \"Robust scale estimation in real-time monocular SFM for autonomous driving,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1566-1573.\n\nTowards semantic SLAM using a monocular camera. J Civera, D G\u00e1lvez-L\u00f3pez, L Riazuelo, J D Tard\u00f3s, J Montiel, Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on. IEEEJ. Civera, D. G\u00e1lvez-L\u00f3pez, L. Riazuelo, J. D. Tard\u00f3s, and J. Montiel, \"Towards semantic SLAM using a monocular camera,\" in Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ International Conference on. IEEE, 2011, pp. 1277-1284.\n\nProbabilistic data association for semantic SLAM. S L Bowman, N Atanasov, K Daniilidis, G J Pappas, 2017 IEEE International Conference on. Robotics and Automation (ICRAS. L. Bowman, N. Atanasov, K. Daniilidis, and G. J. Pappas, \"Probabilistic data association for semantic SLAM,\" in Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE, 2017, pp. 1722-1729.\n\nUnsupervised CNN for single view depth estimation: Geometry to the rescue. R Garg, G Carneiro, I Reid, European Conference on Computer Vision. SpringerR. Garg, G. Carneiro, and I. Reid, \"Unsupervised CNN for single view depth estimation: Geometry to the rescue,\" in European Conference on Computer Vision. Springer, 2016, pp. 740-756.\n\nUnsupervised learning of depth and ego-motion from video. T Zhou, M Brown, N Snavely, D G Lowe, CVPR. T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, \"Unsupervised learning of depth and ego-motion from video,\" in CVPR, 2017.\n\nSfM-Net: learning of structure and motion from video. S Vijayanarasimhan, S Ricco, C Schmid, R Sukthankar, K Fragkiadaki, arXiv:1704.07804arXiv preprintS. Vijayanarasimhan, S. Ricco, C. Schmid, R. Sukthankar, and K. Fragkiadaki, \"SfM-Net: learning of structure and motion from video,\" arXiv preprint arXiv:1704.07804, 2017.\n\nDistraction suppression for vision-based pose estimation at city scales. C Mcmanus, W Churchill, A Napier, B Davis, P Newman, Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEEC. McManus, W. Churchill, A. Napier, B. Davis, and P. Newman, \"Distraction suppression for vision-based pose estimation at city scales,\" in Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEE, 2013, pp. 3762-3769.\n\nProbabilistic obstacle partitioning of monocular video for autonomous vehicles. R W Wolcott, R Eustice, BMVC. R. W. Wolcott and R. Eustice, \"Probabilistic obstacle partitioning of monocular video for autonomous vehicles,\" in BMVC, 2016.\n\nMade to measure: Bespoke landmarks for 24-hour, all-weather localisation with a camera. C Linegar, W Churchill, P Newman, 2016 IEEE International Conference on Robotics and Automation (ICRA). C. Linegar, W. Churchill, and P. Newman, \"Made to measure: Bespoke landmarks for 24-hour, all-weather localisation with a camera,\" in 2016 IEEE International Conference on Robotics and Automation (ICRA).\n\n. IEEE. IEEE, 2016, pp. 787-794.\n\nDirect visibility of point sets. S Katz, A Tal, R Basri, ACM Transactions on Graphics (TOG). 26324ACMS. Katz, A. Tal, and R. Basri, \"Direct visibility of point sets,\" in ACM Transactions on Graphics (TOG), vol. 26, no. 3. ACM, 2007, p. 24.\n\nAccurate and efficient stereo processing by semiglobal matching and mutual information. H Hirschmuller, Computer Vision and Pattern Recognition. IEEE2H. Hirschmuller, \"Accurate and efficient stereo processing by semi- global matching and mutual information,\" in Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 2. IEEE, 2005, pp. 807-814.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerO. Ronneberger, P. Fischer, and T. Brox, \"U-net: Convolutional networks for biomedical image segmentation,\" in International Con- ference on Medical Image Computing and Computer-Assisted Inter- vention. Springer, 2015, pp. 234-241.\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, CVPR. C. Godard, O. Mac Aodha, and G. J. Brostow, \"Unsupervised monoc- ular depth estimation with left-right consistency,\" in CVPR, 2017.\n\nMulti-task learning using uncertainty to weigh losses for scene geometry and semantics. A Kendall, Y Gal, R Cipolla, arXiv:1705.07115arXiv preprintA. Kendall, Y. Gal, and R. Cipolla, \"Multi-task learning using un- certainty to weigh losses for scene geometry and semantics,\" arXiv preprint arXiv:1705.07115, 2017.\n\nScale drift-aware large scale monocular SLAM. H Strasdat, J Montiel, A J Davison, Robotics: Science and Systems VI. 2H. Strasdat, J. Montiel, and A. J. Davison, \"Scale drift-aware large scale monocular SLAM,\" Robotics: Science and Systems VI, vol. 2, 2010.\n\nVisual odometry for ground vehicle applications. D Nist\u00e9r, O Naroditsky, J Bergen, Journal of Field Robotics. 231D. Nist\u00e9r, O. Naroditsky, and J. Bergen, \"Visual odometry for ground vehicle applications,\" Journal of Field Robotics, vol. 23, no. 1, pp. 3-20, 2006.\n\nMachine learning for high-speed corner detection. E Rosten, T Drummond, E. Rosten and T. Drummond, \"Machine learning for high-speed corner detection,\" Computer Vision-ECCV 2006, pp. 430-443, 2006.\n\nBRIEF: binary robust independent elementary features. M Calonder, V Lepetit, C Strecha, P Fua, Computer Vision-ECCV 2010. M. Calonder, V. Lepetit, C. Strecha, and P. Fua, \"BRIEF: binary robust independent elementary features,\" Computer Vision-ECCV 2010, pp. 778-792, 2010.\n\nCNN-SLAM: realtime dense monocular SLAM with learned depth prediction. K Tateno, F Tombari, I Laina, N Navab, CVPR. K. Tateno, F. Tombari, I. Laina, and N. Navab, \"CNN-SLAM: real- time dense monocular SLAM with learned depth prediction,\" in CVPR, 2017.\n\nSemi-dense visual odometry for a monocular camera. J Engel, J Sturm, D Cremers, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionJ. Engel, J. Sturm, and D. Cremers, \"Semi-dense visual odometry for a monocular camera,\" in Proceedings of the IEEE international conference on computer vision, 2013, pp. 1449-1456.\n\n1 year, 1000 km: The Oxford RobotCar dataset. W Maddern, G Pascoe, C Linegar, P Newman, The International Journal of Robotics Research. 361W. Maddern, G. Pascoe, C. Linegar, and P. Newman, \"1 year, 1000 km: The Oxford RobotCar dataset,\" The International Journal of Robotics Research, vol. 36, no. 1, pp. 3-15, 2017.\n\nAre we ready for autonomous driving? The KITTI vision benchmark suite. A Geiger, P Lenz, R Urtasun, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEEA. Geiger, P. Lenz, and R. Urtasun, \"Are we ready for autonomous driving? The KITTI vision benchmark suite,\" in Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 3354-3361.\n", "annotations": {"author": "[{\"end\":132,\"start\":121},{\"end\":146,\"start\":133},{\"end\":163,\"start\":147},{\"end\":178,\"start\":164}]", "publisher": null, "author_last_name": "[{\"end\":131,\"start\":125},{\"end\":145,\"start\":138},{\"end\":162,\"start\":156},{\"end\":177,\"start\":171}]", "author_first_name": "[{\"end\":124,\"start\":121},{\"end\":137,\"start\":133},{\"end\":155,\"start\":147},{\"end\":170,\"start\":164}]", "author_affiliation": null, "title": "[{\"end\":118,\"start\":1},{\"end\":296,\"start\":179}]", "venue": null, "abstract": "[{\"end\":1262,\"start\":298}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1600,\"start\":1597},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2171,\"start\":2170},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4077,\"start\":4074},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4082,\"start\":4079},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4388,\"start\":4385},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4393,\"start\":4390},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4504,\"start\":4501},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4509,\"start\":4506},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4514,\"start\":4511},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4579,\"start\":4576},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4624,\"start\":4620},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4969,\"start\":4965},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4975,\"start\":4971},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5174,\"start\":5170},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5190,\"start\":5186},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5217,\"start\":5213},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5423,\"start\":5419},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5872,\"start\":5868},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5878,\"start\":5874},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6076,\"start\":6072},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7399,\"start\":7395},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8970,\"start\":8966},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10865,\"start\":10861},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11213,\"start\":11209},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11909,\"start\":11905},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11992,\"start\":11988},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12196,\"start\":12192},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12832,\"start\":12828},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13038,\"start\":13034},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15059,\"start\":15055},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15098,\"start\":15094},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15216,\"start\":15212},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15305,\"start\":15301},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16344,\"start\":16340},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17229,\"start\":17225}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22184,\"start\":22115},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":22586,\"start\":22185}]", "paragraph": "[{\"end\":1918,\"start\":1281},{\"end\":2831,\"start\":1920},{\"end\":3965,\"start\":2833},{\"end\":4394,\"start\":3986},{\"end\":5096,\"start\":4396},{\"end\":5789,\"start\":5098},{\"end\":6489,\"start\":5791},{\"end\":7420,\"start\":6526},{\"end\":8124,\"start\":7540},{\"end\":8298,\"start\":8126},{\"end\":8569,\"start\":8300},{\"end\":8631,\"start\":8571},{\"end\":9120,\"start\":8655},{\"end\":9487,\"start\":9155},{\"end\":10217,\"start\":9500},{\"end\":10582,\"start\":10262},{\"end\":11033,\"start\":10612},{\"end\":11455,\"start\":11035},{\"end\":11591,\"start\":11504},{\"end\":12266,\"start\":11619},{\"end\":12924,\"start\":12309},{\"end\":13429,\"start\":12957},{\"end\":14200,\"start\":13494},{\"end\":14461,\"start\":14231},{\"end\":14870,\"start\":14510},{\"end\":15123,\"start\":14909},{\"end\":15516,\"start\":15155},{\"end\":15920,\"start\":15581},{\"end\":16233,\"start\":15946},{\"end\":16758,\"start\":16257},{\"end\":17398,\"start\":16784},{\"end\":17850,\"start\":17400},{\"end\":18027,\"start\":17866},{\"end\":18640,\"start\":18055},{\"end\":19473,\"start\":18666},{\"end\":20222,\"start\":19494},{\"end\":20865,\"start\":20224},{\"end\":21914,\"start\":20892},{\"end\":22114,\"start\":21916}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7539,\"start\":7421},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9154,\"start\":9121},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9499,\"start\":9488},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10261,\"start\":10218},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11503,\"start\":11456},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14230,\"start\":14201},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14509,\"start\":14462},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14908,\"start\":14871},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15580,\"start\":15517}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18133,\"start\":18126},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18744,\"start\":18736}]", "section_header": "[{\"end\":1279,\"start\":1264},{\"end\":3984,\"start\":3968},{\"end\":6524,\"start\":6492},{\"end\":8653,\"start\":8634},{\"end\":10610,\"start\":10585},{\"end\":11617,\"start\":11594},{\"end\":12307,\"start\":12269},{\"end\":12955,\"start\":12927},{\"end\":13439,\"start\":13432},{\"end\":13462,\"start\":13442},{\"end\":13482,\"start\":13465},{\"end\":13492,\"start\":13485},{\"end\":15153,\"start\":15126},{\"end\":15944,\"start\":15923},{\"end\":16255,\"start\":16236},{\"end\":16782,\"start\":16761},{\"end\":17864,\"start\":17853},{\"end\":18053,\"start\":18030},{\"end\":18664,\"start\":18643},{\"end\":19492,\"start\":19476},{\"end\":20890,\"start\":20868},{\"end\":22124,\"start\":22116},{\"end\":22202,\"start\":22186}]", "table": "[{\"end\":22586,\"start\":22220}]", "figure_caption": "[{\"end\":22184,\"start\":22126},{\"end\":22220,\"start\":22204}]", "figure_ref": "[{\"end\":2147,\"start\":2141},{\"end\":8736,\"start\":8730},{\"end\":10581,\"start\":10575},{\"end\":10774,\"start\":10768},{\"end\":11320,\"start\":11314},{\"end\":11781,\"start\":11775},{\"end\":13074,\"start\":13068},{\"end\":13669,\"start\":13663},{\"end\":15794,\"start\":15788},{\"end\":18026,\"start\":18020},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19369,\"start\":19362},{\"end\":20371,\"start\":20365},{\"end\":21002,\"start\":20996},{\"end\":21847,\"start\":21841}]", "bib_author_first_name": "[{\"end\":23243,\"start\":23242},{\"end\":23245,\"start\":23244},{\"end\":23257,\"start\":23256},{\"end\":23259,\"start\":23258},{\"end\":23559,\"start\":23558},{\"end\":23865,\"start\":23864},{\"end\":23874,\"start\":23873},{\"end\":24162,\"start\":24161},{\"end\":24175,\"start\":24171},{\"end\":24368,\"start\":24367},{\"end\":24378,\"start\":24377},{\"end\":24387,\"start\":24386},{\"end\":24671,\"start\":24670},{\"end\":24681,\"start\":24680},{\"end\":24690,\"start\":24689},{\"end\":24921,\"start\":24920},{\"end\":24923,\"start\":24922},{\"end\":24939,\"start\":24938},{\"end\":24941,\"start\":24940},{\"end\":24953,\"start\":24952},{\"end\":24967,\"start\":24966},{\"end\":25360,\"start\":25359},{\"end\":25367,\"start\":25366},{\"end\":25373,\"start\":25372},{\"end\":25385,\"start\":25384},{\"end\":25754,\"start\":25753},{\"end\":25761,\"start\":25760},{\"end\":25773,\"start\":25772},{\"end\":26095,\"start\":26094},{\"end\":26103,\"start\":26102},{\"end\":26514,\"start\":26513},{\"end\":26524,\"start\":26523},{\"end\":26540,\"start\":26539},{\"end\":26552,\"start\":26551},{\"end\":26554,\"start\":26553},{\"end\":26564,\"start\":26563},{\"end\":26945,\"start\":26944},{\"end\":26947,\"start\":26946},{\"end\":26957,\"start\":26956},{\"end\":26969,\"start\":26968},{\"end\":26983,\"start\":26982},{\"end\":26985,\"start\":26984},{\"end\":27352,\"start\":27351},{\"end\":27360,\"start\":27359},{\"end\":27372,\"start\":27371},{\"end\":27671,\"start\":27670},{\"end\":27679,\"start\":27678},{\"end\":27688,\"start\":27687},{\"end\":27699,\"start\":27698},{\"end\":27701,\"start\":27700},{\"end\":27892,\"start\":27891},{\"end\":27912,\"start\":27911},{\"end\":27921,\"start\":27920},{\"end\":27931,\"start\":27930},{\"end\":27945,\"start\":27944},{\"end\":28236,\"start\":28235},{\"end\":28247,\"start\":28246},{\"end\":28260,\"start\":28259},{\"end\":28270,\"start\":28269},{\"end\":28279,\"start\":28278},{\"end\":28683,\"start\":28682},{\"end\":28685,\"start\":28684},{\"end\":28696,\"start\":28695},{\"end\":28929,\"start\":28928},{\"end\":28940,\"start\":28939},{\"end\":28953,\"start\":28952},{\"end\":29305,\"start\":29304},{\"end\":29313,\"start\":29312},{\"end\":29320,\"start\":29319},{\"end\":29601,\"start\":29600},{\"end\":29969,\"start\":29968},{\"end\":29984,\"start\":29983},{\"end\":29995,\"start\":29994},{\"end\":30401,\"start\":30400},{\"end\":30411,\"start\":30410},{\"end\":30415,\"start\":30412},{\"end\":30424,\"start\":30423},{\"end\":30426,\"start\":30425},{\"end\":30664,\"start\":30663},{\"end\":30675,\"start\":30674},{\"end\":30682,\"start\":30681},{\"end\":30937,\"start\":30936},{\"end\":30949,\"start\":30948},{\"end\":30960,\"start\":30959},{\"end\":30962,\"start\":30961},{\"end\":31198,\"start\":31197},{\"end\":31208,\"start\":31207},{\"end\":31222,\"start\":31221},{\"end\":31464,\"start\":31463},{\"end\":31474,\"start\":31473},{\"end\":31666,\"start\":31665},{\"end\":31678,\"start\":31677},{\"end\":31689,\"start\":31688},{\"end\":31700,\"start\":31699},{\"end\":31957,\"start\":31956},{\"end\":31967,\"start\":31966},{\"end\":31978,\"start\":31977},{\"end\":31987,\"start\":31986},{\"end\":32191,\"start\":32190},{\"end\":32200,\"start\":32199},{\"end\":32209,\"start\":32208},{\"end\":32570,\"start\":32569},{\"end\":32581,\"start\":32580},{\"end\":32591,\"start\":32590},{\"end\":32602,\"start\":32601},{\"end\":32913,\"start\":32912},{\"end\":32923,\"start\":32922},{\"end\":32931,\"start\":32930}]", "bib_author_last_name": "[{\"end\":23254,\"start\":23246},{\"end\":23266,\"start\":23260},{\"end\":23568,\"start\":23560},{\"end\":23871,\"start\":23866},{\"end\":23891,\"start\":23875},{\"end\":24169,\"start\":24163},{\"end\":24183,\"start\":24176},{\"end\":24375,\"start\":24369},{\"end\":24384,\"start\":24379},{\"end\":24394,\"start\":24388},{\"end\":24678,\"start\":24672},{\"end\":24687,\"start\":24682},{\"end\":24695,\"start\":24691},{\"end\":24936,\"start\":24924},{\"end\":24950,\"start\":24942},{\"end\":24964,\"start\":24954},{\"end\":24975,\"start\":24968},{\"end\":25364,\"start\":25361},{\"end\":25370,\"start\":25368},{\"end\":25382,\"start\":25374},{\"end\":25389,\"start\":25386},{\"end\":25758,\"start\":25755},{\"end\":25770,\"start\":25762},{\"end\":25780,\"start\":25774},{\"end\":26100,\"start\":26096},{\"end\":26114,\"start\":26104},{\"end\":26521,\"start\":26515},{\"end\":26537,\"start\":26525},{\"end\":26549,\"start\":26541},{\"end\":26561,\"start\":26555},{\"end\":26572,\"start\":26565},{\"end\":26954,\"start\":26948},{\"end\":26966,\"start\":26958},{\"end\":26980,\"start\":26970},{\"end\":26992,\"start\":26986},{\"end\":27357,\"start\":27353},{\"end\":27369,\"start\":27361},{\"end\":27377,\"start\":27373},{\"end\":27676,\"start\":27672},{\"end\":27685,\"start\":27680},{\"end\":27696,\"start\":27689},{\"end\":27706,\"start\":27702},{\"end\":27909,\"start\":27893},{\"end\":27918,\"start\":27913},{\"end\":27928,\"start\":27922},{\"end\":27942,\"start\":27932},{\"end\":27957,\"start\":27946},{\"end\":28244,\"start\":28237},{\"end\":28257,\"start\":28248},{\"end\":28267,\"start\":28261},{\"end\":28276,\"start\":28271},{\"end\":28286,\"start\":28280},{\"end\":28693,\"start\":28686},{\"end\":28704,\"start\":28697},{\"end\":28937,\"start\":28930},{\"end\":28950,\"start\":28941},{\"end\":28960,\"start\":28954},{\"end\":29310,\"start\":29306},{\"end\":29317,\"start\":29314},{\"end\":29326,\"start\":29321},{\"end\":29614,\"start\":29602},{\"end\":29981,\"start\":29970},{\"end\":29992,\"start\":29985},{\"end\":30000,\"start\":29996},{\"end\":30408,\"start\":30402},{\"end\":30421,\"start\":30416},{\"end\":30434,\"start\":30427},{\"end\":30672,\"start\":30665},{\"end\":30679,\"start\":30676},{\"end\":30690,\"start\":30683},{\"end\":30946,\"start\":30938},{\"end\":30957,\"start\":30950},{\"end\":30970,\"start\":30963},{\"end\":31205,\"start\":31199},{\"end\":31219,\"start\":31209},{\"end\":31229,\"start\":31223},{\"end\":31471,\"start\":31465},{\"end\":31483,\"start\":31475},{\"end\":31675,\"start\":31667},{\"end\":31686,\"start\":31679},{\"end\":31697,\"start\":31690},{\"end\":31704,\"start\":31701},{\"end\":31964,\"start\":31958},{\"end\":31975,\"start\":31968},{\"end\":31984,\"start\":31979},{\"end\":31993,\"start\":31988},{\"end\":32197,\"start\":32192},{\"end\":32206,\"start\":32201},{\"end\":32217,\"start\":32210},{\"end\":32578,\"start\":32571},{\"end\":32588,\"start\":32582},{\"end\":32599,\"start\":32592},{\"end\":32609,\"start\":32603},{\"end\":32920,\"start\":32914},{\"end\":32928,\"start\":32924},{\"end\":32939,\"start\":32932}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":972888},\"end\":23511,\"start\":23125},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":12127129},\"end\":23773,\"start\":23513},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":57626175},\"end\":24101,\"start\":23775},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15331388},\"end\":24315,\"start\":24103},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15909542},\"end\":24641,\"start\":24317},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":11962297},\"end\":24852,\"start\":24643},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3198903},\"end\":25278,\"start\":24854},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":10328909},\"end\":25635,\"start\":25280},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14994292},\"end\":26017,\"start\":25637},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3015455},\"end\":26463,\"start\":26019},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":6622200},\"end\":26892,\"start\":26465},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1657925},\"end\":27274,\"start\":26894},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":299085},\"end\":27610,\"start\":27276},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11977588},\"end\":27835,\"start\":27612},{\"attributes\":{\"doi\":\"arXiv:1704.07804\",\"id\":\"b14\"},\"end\":28160,\"start\":27837},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8018996},\"end\":28600,\"start\":28162},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":15763528},\"end\":28838,\"start\":28602},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1186925},\"end\":29235,\"start\":28840},{\"attributes\":{\"id\":\"b18\"},\"end\":29269,\"start\":29237},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":12342434},\"end\":29510,\"start\":29271},{\"attributes\":{\"id\":\"b20\"},\"end\":29901,\"start\":29512},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3719281},\"end\":30329,\"start\":29903},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206596513},\"end\":30573,\"start\":30331},{\"attributes\":{\"doi\":\"arXiv:1705.07115\",\"id\":\"b23\"},\"end\":30888,\"start\":30575},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":9307872},\"end\":31146,\"start\":30890},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10286462},\"end\":31411,\"start\":31148},{\"attributes\":{\"id\":\"b26\"},\"end\":31609,\"start\":31413},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1815530},\"end\":31883,\"start\":31611},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206596482},\"end\":32137,\"start\":31885},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7110290},\"end\":32521,\"start\":32139},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":22556995},\"end\":32839,\"start\":32523},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6724907},\"end\":33229,\"start\":32841}]", "bib_title": "[{\"end\":23240,\"start\":23125},{\"end\":23556,\"start\":23513},{\"end\":23862,\"start\":23775},{\"end\":24159,\"start\":24103},{\"end\":24365,\"start\":24317},{\"end\":24668,\"start\":24643},{\"end\":24918,\"start\":24854},{\"end\":25357,\"start\":25280},{\"end\":25751,\"start\":25637},{\"end\":26092,\"start\":26019},{\"end\":26511,\"start\":26465},{\"end\":26942,\"start\":26894},{\"end\":27349,\"start\":27276},{\"end\":27668,\"start\":27612},{\"end\":28233,\"start\":28162},{\"end\":28680,\"start\":28602},{\"end\":28926,\"start\":28840},{\"end\":29302,\"start\":29271},{\"end\":29598,\"start\":29512},{\"end\":29966,\"start\":29903},{\"end\":30398,\"start\":30331},{\"end\":30934,\"start\":30890},{\"end\":31195,\"start\":31148},{\"end\":31663,\"start\":31611},{\"end\":31954,\"start\":31885},{\"end\":32188,\"start\":32139},{\"end\":32567,\"start\":32523},{\"end\":32910,\"start\":32841}]", "bib_author": "[{\"end\":23256,\"start\":23242},{\"end\":23268,\"start\":23256},{\"end\":23570,\"start\":23558},{\"end\":23873,\"start\":23864},{\"end\":23893,\"start\":23873},{\"end\":24171,\"start\":24161},{\"end\":24185,\"start\":24171},{\"end\":24377,\"start\":24367},{\"end\":24386,\"start\":24377},{\"end\":24396,\"start\":24386},{\"end\":24680,\"start\":24670},{\"end\":24689,\"start\":24680},{\"end\":24697,\"start\":24689},{\"end\":24938,\"start\":24920},{\"end\":24952,\"start\":24938},{\"end\":24966,\"start\":24952},{\"end\":24977,\"start\":24966},{\"end\":25366,\"start\":25359},{\"end\":25372,\"start\":25366},{\"end\":25384,\"start\":25372},{\"end\":25391,\"start\":25384},{\"end\":25760,\"start\":25753},{\"end\":25772,\"start\":25760},{\"end\":25782,\"start\":25772},{\"end\":26102,\"start\":26094},{\"end\":26116,\"start\":26102},{\"end\":26523,\"start\":26513},{\"end\":26539,\"start\":26523},{\"end\":26551,\"start\":26539},{\"end\":26563,\"start\":26551},{\"end\":26574,\"start\":26563},{\"end\":26956,\"start\":26944},{\"end\":26968,\"start\":26956},{\"end\":26982,\"start\":26968},{\"end\":26994,\"start\":26982},{\"end\":27359,\"start\":27351},{\"end\":27371,\"start\":27359},{\"end\":27379,\"start\":27371},{\"end\":27678,\"start\":27670},{\"end\":27687,\"start\":27678},{\"end\":27698,\"start\":27687},{\"end\":27708,\"start\":27698},{\"end\":27911,\"start\":27891},{\"end\":27920,\"start\":27911},{\"end\":27930,\"start\":27920},{\"end\":27944,\"start\":27930},{\"end\":27959,\"start\":27944},{\"end\":28246,\"start\":28235},{\"end\":28259,\"start\":28246},{\"end\":28269,\"start\":28259},{\"end\":28278,\"start\":28269},{\"end\":28288,\"start\":28278},{\"end\":28695,\"start\":28682},{\"end\":28706,\"start\":28695},{\"end\":28939,\"start\":28928},{\"end\":28952,\"start\":28939},{\"end\":28962,\"start\":28952},{\"end\":29312,\"start\":29304},{\"end\":29319,\"start\":29312},{\"end\":29328,\"start\":29319},{\"end\":29616,\"start\":29600},{\"end\":29983,\"start\":29968},{\"end\":29994,\"start\":29983},{\"end\":30002,\"start\":29994},{\"end\":30410,\"start\":30400},{\"end\":30423,\"start\":30410},{\"end\":30436,\"start\":30423},{\"end\":30674,\"start\":30663},{\"end\":30681,\"start\":30674},{\"end\":30692,\"start\":30681},{\"end\":30948,\"start\":30936},{\"end\":30959,\"start\":30948},{\"end\":30972,\"start\":30959},{\"end\":31207,\"start\":31197},{\"end\":31221,\"start\":31207},{\"end\":31231,\"start\":31221},{\"end\":31473,\"start\":31463},{\"end\":31485,\"start\":31473},{\"end\":31677,\"start\":31665},{\"end\":31688,\"start\":31677},{\"end\":31699,\"start\":31688},{\"end\":31706,\"start\":31699},{\"end\":31966,\"start\":31956},{\"end\":31977,\"start\":31966},{\"end\":31986,\"start\":31977},{\"end\":31995,\"start\":31986},{\"end\":32199,\"start\":32190},{\"end\":32208,\"start\":32199},{\"end\":32219,\"start\":32208},{\"end\":32580,\"start\":32569},{\"end\":32590,\"start\":32580},{\"end\":32601,\"start\":32590},{\"end\":32611,\"start\":32601},{\"end\":32922,\"start\":32912},{\"end\":32930,\"start\":32922},{\"end\":32941,\"start\":32930}]", "bib_venue": "[{\"end\":26257,\"start\":26195},{\"end\":32340,\"start\":32288},{\"end\":23293,\"start\":23268},{\"end\":23598,\"start\":23570},{\"end\":23918,\"start\":23893},{\"end\":24195,\"start\":24185},{\"end\":24455,\"start\":24396},{\"end\":24725,\"start\":24697},{\"end\":25039,\"start\":24977},{\"end\":25440,\"start\":25391},{\"end\":25813,\"start\":25782},{\"end\":26193,\"start\":26116},{\"end\":26654,\"start\":26574},{\"end\":27031,\"start\":26994},{\"end\":27417,\"start\":27379},{\"end\":27712,\"start\":27708},{\"end\":27889,\"start\":27837},{\"end\":28357,\"start\":28288},{\"end\":28710,\"start\":28706},{\"end\":29030,\"start\":28962},{\"end\":29243,\"start\":29239},{\"end\":29362,\"start\":29328},{\"end\":29655,\"start\":29616},{\"end\":30088,\"start\":30002},{\"end\":30440,\"start\":30436},{\"end\":30661,\"start\":30575},{\"end\":31004,\"start\":30972},{\"end\":31256,\"start\":31231},{\"end\":31461,\"start\":31413},{\"end\":31731,\"start\":31706},{\"end\":31999,\"start\":31995},{\"end\":32286,\"start\":32219},{\"end\":32657,\"start\":32611},{\"end\":33012,\"start\":32941}]"}}}, "year": 2023, "month": 12, "day": 17}