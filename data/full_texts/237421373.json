{"id": 237421373, "updated": "2023-10-06 04:36:49.63", "metadata": {"title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions", "authors": "[{\"first\":\"Swaroop\",\"last\":\"Mishra\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Khashabi\",\"middle\":[]},{\"first\":\"Chitta\",\"last\":\"Baral\",\"middle\":[]},{\"first\":\"Hannaneh\",\"last\":\"Hajishirzi\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.08773", "mag": null, "acl": "2022.acl-long.244", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/MishraKBH22", "doi": "10.18653/v1/2022.acl-long.244"}}, "content": {"source": {"pdf_hash": "73282485913088de056517e3d1a5146ccbe488bb", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.acl-long.244.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "873ed55c5c5ec7fc273368cf3a280edb94bb5771", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/73282485913088de056517e3d1a5146ccbe488bb.txt", "contents": "\nCross-Task Generalization via Natural Language Crowdsourcing Instructions\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsMay 22-27, 2022 c 2022\n\nSwaroop Mishra \nArizona State University\n\n\nAniel Khashabi \nAllen Institute for AI\n\n\nChitta Baral \nArizona State University\n\n\nHannaneh Hajishirzi \nAllen Institute for AI\n\n\nUniversity of Washington\n\n\nCross-Task Generalization via Natural Language Crowdsourcing Instructions\n\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nthe 60th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics1May 22-27, 2022 c 2022\nHumans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the humanreadable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their humanauthored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound, indicating significant room for more progress in this direction. 1\n\nIntroduction\n\nWe have witnessed great progress in solving many NLP datasets through fine-tuning pre-trained language models (LMs) (Peters et al., 2018;Brown et al., 2020). More recent studies show tremendous promise in generalization within the set of observed tasks through multi-task training and unified encoding (Khashabi et al., 2020;Aghajanyan et al., Work done while interning at Allen Institute for AI. 1 Figure 1: We construct the NATURAL INSTRUCTIONS dataset from crowdsourcing instructions and instances of different NLP datasets. We study if models can learn from seen tasks and generalize to unseen tasks given their natural crowdsourcing instructions. 2021). However, cross-task generalization -generalization to unseen tasks -has generally remained under-explored. For example, can we supervise a model with instances of grammar checking or question answering tasks, yet expect it to solve a different task like question typing (Fig.1). Evidently, humans are capable of such generalizations; an average human can follow natural language instructions to solve a variety of problems, as evident by the success of crowdsourcing platforms (also argued in Efrat and Levy (2020)). In this paper, we study if models can generalize to unseen tasks given their crowdsourcing instructions (Fig.1).\n\nWe build NATURAL INSTRUCTIONS, a dataset consisting of natural crowdsourcing instructions for various tasks and their instances. Training on seen tasks T seen in our dataset, we build a model that learns to follow natural instructions that define a task and perform tasks (i.e., mapping input to output). Testing on unseen tasks T unseen , we evaluate if the model can perform unseen tasks solely from Task Instance-Level Generalization\n\n\nTask-Level Generalization\n\nTraining data X train , Y train pIt, X train t , Y train t q t P Tseen Evaluation x \u00d1 y where: px, yq P pX test , Y test q px, Itq \u00d1 y where: px, yq P pX test t , Y test t q t P Tunseen (a) A comparison of task vs instance-level generalization It, Xt and Yt indicate natural language instructions, input, and output sets respectively for task t. In the conventional setup, training and evaluation are done on the instances of the same task. However, in task-level generalization, a model is expected to generalize to unseen tasks, where Tunseen X Tseen\" H. No Instruction With Instruction  (b) BART evaluation on unseen tasks (y-axis is perf. on Tunseen) when supervised with seen tasks (x-axis is |Tseen|). A model using instructions (It) consistently improves with more observed tasks. In contrast, models with no access to the instructions show no sign of improved generalization. Details in \u00a76.3. their instructions and without any task-specific labeled data (Table 2a; right). In contrast to the instance-level generalization (Table 2a; left), our model uses instruction as additional input, and evaluations are done on tasks that were not observed in the training stage.\n\nWe compile NATURAL INSTRUCTIONS from task instructions written by researchers for crowdsourcing existing NLP datasets. Such crowdsourcing instructions often elaborate a variety of details about how a task should (and should not) be done. To provide a systematic study of various elements of crowdsourcing instructions, we map them to a unified schema to cover the most important elements of task descriptions -such as definition, constraints, positive and negative examples. We collect tasks in NATURAL INSTRUCTIONS as minimal stand-alone steps provided to crowdworkers to complete a downstream NLP task. For example, tasks collected from QASC (Khot et al., 2020) include sub-tasks about generating topic words or combining facts, as well as answering multi-hop questions. Therefore our dataset not only contains typical downstream tasks in NLP, but also the intermediate subtasks that are not well-represented in the common benchmarks. The unified schema and the collection of minimal subtasks enable training LMs that can generalize across different tasks by learning from instructions. In total, our dataset consists of 61 distinct NLP tasks and 193k instances.\n\nOur experimental results indicate that LMs learn to leverage natural language instructions as they show improved generalization to new tasks. For example, a BART (Lewis et al., 2019) achieves a 19% gain in terms of cross-task generalization compared to a model not using instructions ( \u00a76). Importantly, LMs can generalize better to unseen tasks if they observe more tasks in training (Fig.2b). This upward trajectory suggests the potential for stronger cross-task generalizable models upon scaling up the diversity of tasks represented in a metadataset of task instructions. Despite the benefits of instructions, we observe a sizable gap between models' generalization and their estimated upperbounds (6.4), encouraging the community to work on this challenging problem.\n\nContributions: In summary, the contributions of this work are as follows: (a) we introduce NATURAL INSTRUCTIONS, a dataset of humanauthored instructions curated from existing wellknown datasets mapped to a unified schema, providing training and evaluation data for learning from instructions; (b) we build models that can encode instructions and show: (b.1) the benefit of crosstask generalization by leveraging instructions; (b.2) the importance of different elements of instructions in the performance; (b.3) noteworthy headroom for improvement on our benchmark, which hopefully will motivate further work in this direction.\n\n\nRelated Works\n\nLearning from instructions. There is recent literature on the extent to which models follow language instructions (Hase and Bansal, 2021;Gupta et al., 2021;Zhong et al., 2021). For example, Efrat and Levy (2020) examine if language models can follow crowdsourcing instructions with no further training. On the contrary, our work is pursuing a fundamentally different goal: creating a dataset of crowdsourcing instructions and task instances and formulating cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. Weller et al. (2020) construct a crowdsourced dataset with short question-like task descriptions. Compared to this work, our instructions are longer, more complex and natural since they were used to collect datasets through crowdsourcing.\n\nPromptSource and FLAN (Wei et al., 2022;Sanh et al., 2022) are two concurrent works that pursue a similar goal as ours. A key difference between our work to these works is in terms of data collection strategy. Our work uses natural instructions created by NLP researchers before the dataset instances were created by crowd workers, and hence it contains the complete definition of each task (definition, things to avoid, negative examples, etc.). On the other hand, instructions in the concurrent work are collected retroactively based on the alreadyavailable task instances. Our natural instructions enable evaluating models on how they learn tasks given different elements of task descriptions. (See \u00a7A.5 for further comparisons.) Nevertheless, we believe that all these approaches to constructing instructions and task categories are complementary and the community will benefit from considering both towards solving the challenging problem of cross-task generalization.\n\nPrompt engineering. Constructing effective discrete prompts for language models to perform NLP tasks is an active area of research (Schick and Sch\u00fctze, 2021;Reynolds and McDonell, 2021;Liu et al., 2021). Such prompts are often extremely short and may not include a complete definition of complex tasks. In contrast, our instructions encode detailed instructions as they were used to collect the datasets. Moreover, the goals are different: Most prompt-engineering approaches seek prompts with higher performance on a particular task, typically through assumptions about their target task which make them non-trivial to generalize to any other task. However, our introduced meta dataset enables the measurement of generalization to unseen tasks. Beyond standard multi-task learning. Multitask learning is a long-standing goal for AI (Caruana, 1997) and has led to successful models that can support a wider range of tasks (McCann et al., 2018;Raffel et al., 2020;Khashabi et al., 2020;Mishra et al., 2020;Aghajanyan et al., 2021;. Most of the conventional setups in the multi-tasking literature evaluate on instances that belong to the tasks that are seen, i.e., their labeled instances were observed during training (1st column of Table 2a). We augment this setup by introducing natural language instructions which enable our models to bridge to tasks that were not seen during training.\n\n\nDefining Cross-Task Generalization\n\nHere we formally define the problem setup for generalization across tasks. Each task t consists of input/output instances pX t , Y t q and is described in terms of its natural language instructions I t .\n\nTask-specific models. Standard supervised learning algorithms use task-specific labeled instances to learn a mapping from input x to output y: M pxq \" y for px, yq P pX train t , Y train t q and is evaluated on the test instances of the same (or similar) task pX test t , Y test t q. We refer to this as the instance-level generalization (Table 2a; left).\n\nCross-task models. In this setup, the goal is to learn a model M that at inference obtains the output y given the input x and the task instruction I t : M pI t , xq \" y, for px, yq P pX t , Y t q. In contrast to the task-specific models, no task-specific training data is used to learn the mapping M . We collect NATURAL INSTRUCTIONS ( \u00a74) to study this question: can a model be trained to follow instructions via training tasks T seen and be generalized to follow instructions for a task t 1 P T unseen . We refer to this as a task-level generalization (Table 2a; right).\n\n\nNATURAL INSTRUCTIONS\n\nNATURAL INSTRUCTIONS consists of instructions that describe a task (e.g., question answering) and instances of that task (e.g., answers extracted for a given question). Fig.3 shows an example instruction for the task of 'generating questions that require an understanding of event duration' accompanied with positive and negative examples that contextualize the task. Here we introduce a schema for representing instructions ( \u00a74.1) and then describe how existing datasets (their crowdsourcing templates) are mapped into our schema ( \u00a74.2).\n\n\nInstruction Schema\n\nInstructions used in crowdsourcing various datasets, are written by distinct authors for different purposes, and they are different in a variety of ways (see Appendix A.2 for their differences.) We introduce a unified schema (Fig.4) to consistently represent these diverse forms of instructions. Our instruction schema is the result of our pilot Instructions for MC-TACO question generation task -Title: Writing questions that involve commonsense understanding of \"event duration\".\n\n-Definition: In this task, we ask you to write a question that involves ?event duration\", based on a given sentence. Here, event duration is defined as the understanding of how long events typically last. For example, ?brushing teeth?, usually takes few minutes.\n\n-Emphasis & Caution: The written questions are not required to have a single correct answer.\n\n-Things to avoid: Don't create questions which have explicit mentions of answers in text. Instead, it has to be implied from what is given. In other words, we want you to use \"instinct\" or \"common sense\".\n\n-Input: Sentence: Jack played basketball after school, after which he was very tired.\n\n-Output: How long did Jack play basketball? -Reason: the question asks about the duration of an event; therefore it's a temporal event duration question. Example task instances -Input: Sentence: It's hail crackled across the comm, and Tara spun to retake her seat at the helm.  study conducted on a subset of datasets. Below we describe the ingredients of this schema:\n\n\u2022 TITLE provides a high-level description of a task and its associated skill (such as question generation, answer generation).\n\n\u2022 PROMPT is a single sentence command that often appears before the input instance and connects it to the instructions.\n\n\u2022 DEFINITION provides the core detailed instructions for a task.\n\n\u2022 THINGS TO AVOID contain instructions regarding undesirable annotations that must be avoided. These help to define the scope of a task and the space of acceptable responses.\n\n\u2022 EMPHASIS AND CAUTION are short, but important statements highlighted in the crowdsourcing templates which were intended to be emphasized or warned against.\n\n\u2022 POSITIVE EXAMPLES contain inputs/outputs similar to the input given to a worker/system and its expected output, helping crowdworkers better understand a task (Ali, 1981).  to emphasize THINGS TO AVOID by providing examples that must not be produced.\n\n\n\u2022 NEGATIVE EXAMPLES contain inputs/outputs\n\n\u2022 REASON provides explanations behind why an example is positive or negative.\n\n\u2022 SUGGESTION contains suggestions on how a negative example could be modified to turn it into a positive example. The next section describes the process of mapping the raw instructions (designed for crowdworkers) to our instruction schema.\n\n\nConstructing NATURAL INSTRUCTIONS\n\n\nCollecting Data\n\nCollecting raw instructions and instances. We use existing, widely adopted NLP benchmarks that are collected via crowdsourcing platforms and hence, come with crowdsourcing templates. In the first step, we identified several datasets and engaged with their authors to get their crowdsourcing templates and raw data. This yields the following datasets: CosmosQA ( (Sakaguchi et al., 2020). 2 Splitting crowdsourcing instructions into minimal tasks. Almost all the crowdworking instructions include sequences of steps to guide crowdworkers in creating task instances. For example, QASC and MCTACO include 7 and 19 steps in the data creation process, respectively. We divide  Table 1: Examples of the datasets and the tasks formed from them. The extracted tasks are independent annotation assignments in the crowdsourcing templates of the datasets. The complete list is in Table 10   crowdsourcing instructions into their underlying steps and generate multiple subtasks that are minimal and standalone. 3 Table 1 shows subtasks extracted for Quoref and QASC. For example, the main task in Quoref is to answer a question given a context paragraph, but the crowdsourcing template consists of two sub-tasks of question generation and answer generation with their separate instructions. This process results in a more consistent definition of tasks, enabling a successful mapping of instructions into our schema, in contrast to the work of Efrat and Levy (2020) that uses crowdsourcing instructions as-is.\n\nIn total, there are 61 tasks, which are categorized into 6 semantic categories (Table 2). We assigned these broad categories to the tasks to understand their collective behavior in the experiments. It is noteworthy that, despite the apparent resemblance of the tasks included in the same category, any pair of tasks are distinct. For example, while question generation is part of Quoref, CosmosQA, and QASC, each has its own separate variant of the question generation task (see Fig.10 in Appendix).\n\n\nMapping Raw Instructions to Schema\n\nWe manually fill in the fields of our instruction schema with the content from the crowdsourcing 3 We eliminate tasks that involve model-in-the-loop.\n\ninstructions. For instance, parts of the raw instructions that are highlighted for emphasis are incorporated as part of our emphasis/caution field. The modifications suggested in this step were applied by one author and were verified by another author. 4 Improving description quality and consistency. We edit raw instructions to ensure their quality. Particularly, we fix writing issues (typos, ambiguities, etc.) and redact repetitions. While repetition often helps in augmenting human understanding, short and concise instructions are often more effective for computers due to their limited attention span (Beltagy et al., 2020). Collecting input/output instances for subtasks. Most of our tasks are the intermediate steps in the crowdsourcing process. Therefore, to extract input/output instances for each task, we need to parse the raw annotations of crowdworkers for every step. Since each dataset stores its annotations in a slightly different format, extracting and unifying such intermediate annotations can be non-trivial.\n\nVerification. An annotator verified the quality of the resulting data in consultation with dataset authors. The annotator iterated on the authors' feedback (avg of 3 iters) until they were satisfied. Quality assessment. We ask independent human annotators to answer 240 random instances (20 instances from 12 random tasks, used later for our evaluation \u00a75.1). The subsequent evaluation of the human-generated responses results in more than 96% accuracy, which indicates that humans can effortlessly understand and execute our instructions.\n\n\nNATURAL INSTRUCTIONS Statistics\n\nIn summary, NATURAL INSTRUCTIONS consists of subtasks each with a set of instructions and in-put/output instances ( Fig.3 and 4). The complete list of instructions is included in the appendix. In total, the dataset includes 61 tasks and 193k instances. Table 2 shows data statistics for each task category. 5 On average, instructions contain 4.9 positive examples and 2.2 negative examples. The longest element of instructions is usually DEFINI-TIONS with 65.5 tokens and the shortest is TITLE with 8.3 tokens (more statistics in Table 3). statistic value \"title\" length 8.3 tokens \"prompt\" length 12.6 tokens \"definition\" length 65.5 tokens \"things to avoid\" length 24.1 tokens \"emphasis/caution\" length 45.0 tokens \"reason\" length 24.9 tokens \"suggestion\" length 19.  \n\n\nProblem Setup and Models\n\nHere we define different cross-task generalization settings ( \u00a75.1) and the models ( \u00a75.2).\n\n\nTask Splits and Generalizations Types\n\nRandom split. This setup follows the common practice in benchmarking NLP models with random data splits. Here, two tasks from each task category (Table 2) in NATURAL INSTRUCTIONS are randomly selected for evaluation, and the rest of the tasks are used for training. This leads to 12 tasks in T unseen and 49 tasks in T seen . 6\n\nLeave-one-out generalization. To better understand the nature of cross-task generalization, we study more restrictive settings of dividing training and evaluation tasks. leave-one-category: evaluates how well a model generalizes to a task category if it is trained on others -no task of that category is in T seen . leave-one-dataset: evaluates how well a model can generalize to all tasks in a particular dataset if it is trained on all other tasks -no task of that dataset is in T seen . This split prevents any leakage across tasks that belong to the same source datasets. 5 We limit the number of instances in each task to 6.5k to avoid massive instance imbalance. 6 Those tasks that do not accept a relatively reliable automatic evaluation are excluded from Tunseen. leave-one-task: evaluates how well a model can learn a single task by training on all other tasks.\n\n\nModels\n\nWe build models using pre-trained LMs with encoder-decoder architectures BART (Lewis et al., 2019) for fine-tuning and GPT3 (Brown et al., 2020) for few-shot experiments.\n\nEncoding instructions and instances. For every problem setup, we map a given instruction I t and an input instance x into a textual format and decode an output y and obtain encpI t , xq. This encoding function is then fed to an encoder-decoder model to predict y: M : encpI t , xq \u00d1 y.\n\nEncoding instances follows a standard NLP paradigm of mapping an input instance to text. Each instruction I t consists of multiple elements as described in our instruction schema ( \u00a74.1). Here, we map each element of the instruction to a textual format and append it before the input instance. Fig.5 shows how we encode the full instruction.\n\nTo study the impact of each instruction element for cross-task generalization, we compare these encodings: (1)  BART. We use BART (base) (Lewis et al., 2019) which allows us to fine-tune its model parameters. This is an encoder-decoder architecture with 140m parameters. For each setup, the input is encoded  Table 4: Cross-task generalization of BART under various splits ( \u00a75.1). Fine-tuned BART shows improved performance when provided with instructions. It also archives better performance than GPT3, despite being over 1k times smaller. All numbers are ROUGE-L.\n\nusing different instruction elements, trained on all T seen tasks, and evaluated on T unseen ( \u00a75.1). GPT3. As a comparison, we evaluate GPT3 (Brown et al., 2020) which is a 175B parameter autoregressive LM (\u02c61.2k larger than BART) and has shown promising results in mimicking demonstrations provided in its prompt. We cannot fine-tune the parameters of this massive model and use it as-is under its default setting on the evaluation tasks in T unseen ( \u00a75.1) using the encoding introduced earlier.\n\n\nExperiments\n\nEvaluation metrics. We treat all of our tasks as text generation problems and evaluate them with automated evaluation metrics for text generation.\n\nIn particular, we use ROUGE-L (Lin, 2004) to automatically evaluate the generated outputs. 7 Implementation details. For BART, our models are trained for 3 epochs with a learning rate of 5e-5 for a given training split and input encoding. For GPT3, we use the davinci-instruct engine and produce outputs with greedy decoding, generating up to a maximum number of tokens of 16 (the default value). We use the default stop condition which is 2 newline tokens. 8 Table 4 reports the results of the BART model train and evaluated with various task splits ( \u00a75.1). For comparison, we evaluate GPT3 which uses no finetuning, unlike BART that is fine-tuned with the T seen tasks. The first column corresponds to random split of tasks, while the remaining columns report cross-task generalization results of the BART model under leave-one-x splits ( \u00a75.1). For x \" category, the tasks in question-generation category 7 Our experiments show that other metrics, e.g. BLEURT (Sellam et al., 2020) are also correlated with ROUGE-L, which has also been used in generative QA tasks. 8 The relevant code is available at: https://github. com/allenai/natural-instructions-v1 are held out during training. For x \" dataset, the tasks that were extracted from the QASC dataset were excluded from training. For x \" task, we train a model on all tasks, except QASC question generation task which is used for evaluation. Instructions benefit cross-task generalization.\n\n\nGeneralization Under Various Task Splits\n\nThe results indicate that BART benefits from instructions in generalizing to new tasks, regardless of task splits. For example, under random split, the model using FULL INSTRUCTIONS results in +19% gains over a model that is not using instructions. This is particularly interesting for leave-one-category-out split since the trained model can generalize to the tasks of a particular semantic category, without being exposed to it. In comparison to GPT3, the fine-tuned BART model that utilizes instructions achieves a stronger performance despite being\u02c61k smaller than GPT3. For example, a BART models using FULL INSTRUCTIONS achieves 8% higher performance than GPT3 under random split of tasks.\n\nNote that the absolute values in leave-onecategory are lower due to the difficulty of this setup compared to, for example, the random split setup. While all settings involve evaluating on tasks not seen during training, the leave-one-category setting enforces more dissimilarity among training and evaluation tasks.\n\n\nGeneralization Under Instruction\n\nEncoding and Task Categories  . We hypothesize these tasks are inherently more difficult, partially because of their distinctness from the rest of the tasks in the dataset. We hope future work on this line will study a wider variety of tasks and will improve our understanding of such failure cases.\n\n\nGeneralization vs. Number of Seen Tasks\n\nFig.2b compares the impact of the number of seen tasks for cross-task generalization. For supervision, we randomly sample a few tasks as T seen and evaluate on 6 tasks (one from each category).\n\n(each point in the figure is averaged over 5 random subsamples.) The results show that with NO-INSTRUCTION encoding there is no tangible value in observing more tasks. In contrast, the generalization of the models that encode instructions improves with observing more tasks. This is an exciting observation since it suggests that scaling up our dataset to more tasks may lead to stronger instruction-following systems.\n\n\nAnalyses\n\nUpperbound: Task-specific Models. For each task, we obtain a task-specific model ( \u00a7 3) by training BART separately on each task's annotated training data. We evaluate these task-specific models to obtain a loose estimate of upperbounds for each task. On average, task-specific models score  66% which is considerably higher than our models' best generalization (32%; Table 4). This indicates that there is considerable room for improving generalization-based models that use instructions.   Perceived Impact of Instruction Elements. We survey human annotators to find out the value of instruction elements to humans. Except for the negative examples which were shown to be difficult for models, we observe similar trends between humans' perceived value of those elements (Table 7) and their contributions to the model performance (Table 5). For example, humans viewed DEFINITION and THINGS TO AVOID as necessary fields for classification and minimal text modification categories, respectively, which is compatible with our empirical observations (e.g., PROMPT + DEFINITION has the highest score on CF category in Table 5).\n\n\nConclusion\n\nIn this paper, we studied the goal of building models that generalize to new tasks by encoding and understanding crowdsourcing instructions. We introduced NATURAL INSTRUCTIONS, which is built based on existing crowdsourced datasets, that enables building such models and systematically evaluate them. To the best of our knowledge, this is the first work to show the benefit of instructions towards improved cross-task generalization. Additionally, we observe that our proposed task has a large room for improvement, which we believe will bring more attention to building stronger models that can generalize to a wider range of tasks.\n\n\nSupplemental Material A Datasets and their Templates\n\nA.1 Division of Crowdsourcing Instructions into Minimal Tasks Fig. 9 shows an example of how a task is divided into multiple subtasks for the MC-TACO dataset. MC-TACO has five categories (Event Duration, Event Frequency etc.). Each category contributes to 2 subtasks one for question generation and one for answer generation.\n\nNumber of tasks in each dataset. Fig. 6 illustrates how the number of steps in the data creation process varies across the 6 datasets. QASC and MC-TACO contain a relatively higher number of steps in the data creation process in comparison to DROP, Quoref, CosmosQA, and Winogrande.  (Sakaguchi et al., 2020). Our intention behind the analysis is to identify similarities and differences across templates and subsequently decide regarding the collection of more templates.\n\nSize of the instructions. We observe significant variation in size across the 6 datasets (Fig. 8). In the case of QASC, the instruction size associated with each step of the data creation process is very high, whereas for Winogrande, it is exactly the opposite-instruction size associated with each step of the data creation process is very low. Instead, the size of the common instruction (i.e., the instruction preceding the first step of the data creation process) is high in Winogrande; this is also seen for DROP. The major mode of instruction varies across datasets. Examples and instructions associated with each step of data creation respectively take up the majority of space in Quoref and CosmosQA. MC-TACO relies on examples to explain the crowdsourcing task, while Winogrande and QASC depend mostly on common instructions and instructions associated with each step of the data creation process respectively, to explain the task to the crowdworker.   \n\n\nA.3 Qualitative Analysis\n\nWriting Style. There are significant variation in writing style across the datasets, even among those datasets that have the common a objective (e.g., DROP, Quoref and QASC). DROP instructions say \"There is an AI running in the background which will also try to answer the question. You won't be able to submit the question if the AI gives the same response.\" The writing style in Quoref however is different: \"We also want you to avoid questions that can be answered correctly by someone without actually understanding the paragraph. ...\"\n\nInformation. We observe that sometimes instructions of a dataset contain information that is relevant to several other datasets, which do not contain similar instruction information. For example, Quoref, DROP and CosmosQA are datasets that are all based on reading comprehension tasks. Cos-mosQA contains a step in the data creation process asking users to skip passages containing inappropriate or offensive content. This information is also relevant to Quoref and DROP, but is not mentioned in their respective instructions.\n\nHardness. In a typical crowdsourcing task, certain tasks may be harder than the others, often these are the core tasks, e.g.: question generation, adversarial data creation, etc. Additional information, especially in the form of tips is always helpful in solving these hard tasks. Figure 10 illustrates that the task of question generation is stated differently in Quoref, CosmosQA, and QASC. QASC mentions an easy and detailed way to create questions, whereas CosmosQA mentions several different attributes of a good quality question. Knowing about the CosmosQA and QASC question generation processes may help with data creation for Quoref and other such question generation tasks, where less additional information is provided regarding question creation. Table 9 shows the effort distribution in the data curation process of NATURAL INSTRUCTIONS.\n\n\nA.4 Data Curation Effort\n\nStep-8 which involves parsing instances is the main bottleneck in the data curation process. Table 10 shows the detailed structure of tasks in NATURAL INSTRUCTIONS. Fig. 11 shows examples of four different tasks in NATURAL INSTRUCTIONS.   -TACO) -Title: Writing questions that involve commonsense understanding of \"event duration\".\n\n-Definition: In this task, we ask you to write a question that involves ?event duration\", based on a given sentence. Here, event duration is defined as the understanding of how long events typically last. For example, ?brushing teeth?, usually takes few minutes.\n\n-Emphasis & Caution: The written questions are not required to have a single correct answer.\n\n-Things to avoid: Don't create questions which have explicit mentions of answers in text. Instead, it has to be implied from what is given. In other words, we want you to use \"instinct\" or \"common sense\".\n\n-Input: Sentence: Jack played basketball after school, after which he was very tired. -Input: Sentence: Still, Preetam vows to marry Nandini if she meets him again.\n\n-Expected Output: How long had they known each other? Task Instance   answer generation (from Winogrande) -Title: Answering a fill in the blank question on objects -Definition: You need to answer a given question containing a blank (_). Your answer must be one of the two objects mentioned in the question for example \"trophy\" and \"suitcase\".\n\n-Emphasis & Caution: --Things to avoid: Your answer must not contain a word that is not present in the question.\n\n-Input: Context word: fit. Question: The trophy doesn't fit into the brown suitcase because _ is too large.\n\n-Output: trophy -Reason: Answer is one of the objects (\"trophy\" and \"suitcase\") in the question. Since the blank is a \"large\" object that didn't fit the \"suitcase\", the answer must be \"trophy\".\n\nPositive Example -Input: Context word: fit. Question: The trophy doesn't fit into the brown suitcase because _ is too large.\n\n-Output: bottle -Reason: The issue is that the answer is not one of the objects present in the question which are \"trophy\" and \"suitcase\". Note that, a valid answer must be one of the objects present in the question.\n\n-Suggestion: -\n\n\nNegative Example\n\n-Prompt: Answer a fill in the blank question that is based on a provided context word. \n\n\nclassification (from DROP)\n\n-Title: Finding the answer type of a reasoning question -Definition: This task involves annotating the answer type to a given question that involve some kind of complex reasoning (including numerical reasoning). Note that the questions require looking at more than one part of the passage to answer. There are 3 possible answer types (i) spans, (ii) numbers and (iii) dates. If the answer can be found in the passage, label it as \"span\". If the answer is a number, label as \"number\". Similarly, label \"date\" if you think the answer to the given question is a date. -Input: Passage: Hoping to rebound from their loss to the Patriots, the Raiders stayed at home for a Week 16 duel with the Houston Texans. Oakland would get the early lead in the first quarter as quarterback JaMarcus Russell completed a 20-yard touchdown pass to rookie wide receiver Chaz Schilens. The Texans would respond with fullback Vonta Leach getting a 1-yard touchdown run, yet the Raiders would answer with kicker Sebastian Janikowski getting a 33-yard and a 30-yard field goal. Houston would tie the game in the second quarter with kicker Kris Brown getting a 53-yard and a 24-yard field goal. Oakland would take the lead in the third quarter [...] Question: How many field goals did Kris Brown kick? -Expected Output: Number Task Instance minimal text modification (from Winogrande) -Title: Modifying a fill in the blank question on persons -Definition: You're given a fill-in-the-blank question where the answer is PersonX. You need to minimally change the given question so that the answer flips to PersonY. This task typically involves replacing one word i.e. the 'trigger word' by its antonym (e.g. changing from \"sympathetic\" to \"stern\"). -Input: Context word: upset. Question: PersonX yelled at PersonY because _ was so upset about the news. Answer: PersonX.\n\n-Output: PersonX comforted at PersonY because _ was so upset about the news. -Reason: On replacing the trigger word \"yelled\" by its antonym \"comforted\", the answer flips to PersonY which is as per the given instruction. So, this is a valid question.  -Reason: Here, the issue is that the usage order of PersonX and PersonY has been changed in the generated question. Remember that, for a question to be valid, PersonX should appear earlier than PersonY.\n\n-Suggestion: -Negative Example Figure 11: Examples from NATURAL INSTRUCTIONS. Each task follows the schema provided in Fig. 4 \n\n\n3484\n\n\nA.5 Qualitative Comparison to PromptSource\n\nWe provide a comparison between our proposed dataset and PromptSource (Sanh et al., 2022  * Definition: In this task we ask you to write answer to a question that involves \"absolute timepoint\" of events, which is defined as understanding of when events usually happen. For example, \"going to school\" usually happens during the day (not at 2 A.M). * Emphasis: Note that a lot of the questions could have more than one correct answers. We only need a single most-likely answer. Please try to keep your \"answer\" as simple as possible. Concise and simple \"answer\" is preferred over those complex and verbose ones. * Definition: Craft one correct answer to the question given in input. To make it more interesting, try to use non-stereotypical language if possible. Make sure your correct answer is reasonably long, consistent with the context, and requires common sense (instead of explicit extraction from the context.) * Emphasis: 1. In your answer, use as few words as possible from the given context. 2. Use a response that is uncommon/non-stereotypical, so that it is less predictable. 3. To be less repetitive, please vary your language for each question. * Prompt: Craft one correct answer to the question given in input. * Definition: This task involves creating answers to complex questions, from a given passage. Answering these questions, typically involve understanding multiple sentences. Make sure that your answer has the same type as the \"answer type\" mentioned in input. The provided \"answer type\" can be of any of the following types: \"span\", \"date\", \"number\". A \"span\" answer is a continuous phrase taken directly from the passage or question. You can directly copy-paste the text from the passage or the question for span type answers. If you find multiple spans, please add them all as a comma separated list. Please restrict each span to five words. A \"number\" type answer can include a digit specifying an actual value. For \"date\" type answers, use DD MM YYYY format e.g. 11 Jan 1992. If full date is not available in the passage you can write partial date such as 1992 or Jan 1992. * Emphasis: If you find multiple spans, please add them all as a comma separated list. Please restrict each span to five words. * Prompt: Write an answer to the given question, such that the answer matches the \"anwer type\" in the input.\n\nPassage Definition: You need to answer a given question containing a blank (_). Your answer must be one of the two objects mentioned in the question for example \"trophy\" and \"suitcase\". Things to avoid: Your answer must not contain a word that is not present in the question. Prompt: Answer a fill in the blank question that is based on a provided context word.   (Sanh et al., 2022).\n\n\nC Analysis on Baseline Results\n\n\nC.1 Comparison to Raw Instructions\n\nWe seek to understand the value of breaking the tasks into sub-tasks and mapping them into our proposed schema ( \u00a74.2). We compute performance of raw instructions (first sub-task of four datasets), in the same vein as (Efrat and Levy, 2020)'s setup. We compare this to our FULL INSTRUCTION -NEG EXAMPLES encoding. The results in Table 13 indicate that GPT3 leads to higher performance with our encoding (2nd row) compared to raw instructions (first row). Weak performance of LMs on raw instructions aligns with (Efrat and Levy, 2020)'s finding that \"language model performs poorly\".  This might be partly due to the verbose language of the raw instructions: the average length of the raw instructions is 2.5k tokens, in comparison to 950 tokens for our encoding. While repetition often helps human understanding, concise instructions seem to be more effective for computers.\n\nFigure 2 :\n2The formal definition of generalization to unseen tasks (a) and a summary of its empirical outcome (b).\n\n-\nInput: Sentence: He spent two hours on his homework. -Output: How long did he do his homework? -Reason: We DO NOT want this question as the answer is directly mentioned in the text. -Suggestion: -Negative Example -Prompt: Ask a question on \"event duration\" based on the provided sentence.\n\n\n-Expected Output: How long was the storm? Instance -Input: Sentence: During breakfast one morning, he seemed lost in thought and ignored his food. -Expected Output: How long was he lost in thoughts? Instance ...\n\nFigure 3 :\n3An example from our dataset. Note that it follows the schema provided inFig.4. SeeFig .11for more examples.\n\nFigure 4 :\n4The schema used for representing instruction in NATURAL INSTRUCTIONS ( \u00a74.1), shown in plate notation.\n\n\nHuang et al., 2019), DROP (Dua et al., 2019), Essential-Terms (Khashabi et al., 2017), MCTACO (Zhou et al., 2019), MultiRC (Khashabi et al., 2018), QASC (Khot et al., 2020), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019) and Winogrande\n\n\nAugmenting examples and reasons. There is a large variance in the number of examples provided in the raw instructions. Instructions often include more positive examples, or some instructions do not include any negative examples (e.g., QASC). Whenever possible, we add negative examples such that each task has at least two negative examples. Furthermore, not all raw instructions contain REA-SONS or SUGGESTIONS for each of their examples. For example, positive examples are usually not accompanied by explanations, and most datasets do not include suggestions. We add them, wherever such information is missing in the instructions.\n\nFigure 5 :\n5: I pos. ex. t , output : I pos. ex. t , reason : I pos. ex. t PositiveExample1\u00ed nput : I pos. ex. t , output : I pos. ex. t reason : I pos. ex. t input : x, output :\" Encoding instruction I t , where I c t refers to the text of a component c in the instruction schema.\n\n\nPROMPT, (2) POS. EXAMPLES, (3) PROMPT + DEFINITION, (4) PROMPT + THINGS TO AVOID, (5) PROMPT + EMPHASIS , (6) PROMPT + POS. EXAMPLES, (7) PROMPT + + DEFINITION + POS. EXAMPLES, and (8) FULL INSTRUCTION.Each of these (e.g.,PROMPT and POS. EXAMPLES)   correspond to prompting setups in the recent literature(Le Scao and Rush, 2021; Lu et al., 2021).\n\nFigure 6 :\n6Variations in the number of subtasks A.2 Analysis of Crowdsourcing Templates We analyzed crowdsourcing templates of 6 datasets: CosmosQA (Huang et al., 2019), DROP (Dua et al., 2019), MC-TACO (Zhou et al., 2019), QASC (Khot et al., 2020), Quoref (Dasigi et al., 2019), and Winogrande\n\nFigure 7 :Figure 8 :\n78Variation in the number of positive and negative examples Variation in the number of sentences in the crowdsourcing instructions across datasets\n\nFigure 10 :\n10Variation in Task Specification: Quoref contains a single line instruction whereas the CosomosQA contains a detailed instruction. QASC on the other hand, contains examples along with instruction.\n\n\n-Output: How long did Jack play basketball? -Reason: the question asks about the duration of an event; therefore it's a temporal event duration question. Positive Example -Input: Sentence: He spent two hours on his homework. -Output: How long did he do his homework? -Reason: We DO NOT want this question as the answer is directly mentioned in the text. -Suggestion: -Negative Example -Prompt: Ask a question on \"event duration\" based on the provided sentence.\n\n--\nEmphasis & Caution: --Things to avoid: --Input: Passage: The outbreak of the Seven Years' War in Europe in 1756 resulted in renewed conflict between French and British forces in India. The Third Carnatic War spread beyond southern India and into Bengal where British forces captured the French settlement of Chandernagore in 1757. However, the war was decided in the south, where the British successfully defended Madras, and Sir Eyre Coote decisively defeated the French, commanded by Comte de Lally at the Battle of Wandiwash in 1760. After Wandiwash, the French capital of Pondicherry fell to the British in 1761. The war concluded with the signing of the Treaty of Paris in 1763, which returned Chandernagore [...] Question: Which french settlement did the British capture first, Chandernagore or Pondicherry? -Output: Span -Reason:The answer \"Chandernagore\" is a word from the passage. So, the answer type is \"span\"Prompt: What is the type of the answer corresponding to the given question? Number, Date, or Span?\n\n\n-Emphasis & Caution: 1. Your question must contain at least 15 and at most 30 words. 2. Your question must have atleast 70% overlapping words with the given question 3. Your question must contain only one blank. 4. Make sure that PersonX and PersonY have the same gender. 6. In your question, PersonX and PersonY should be used only ONCE and PersonX should appear earlier than PersonY. [...] -Things to avoid: 1. You should not change any content in the given question beyond a word or two i.e. the trigger word/ phrase. [...]\n\n*\n* Prompt: Answer the given question on \"absolute timepoint\" of events. Sentence: {{ sentence }} Question: {{ question }} Given the context, {{sentence}} observe the following QA pair and check if the answer Definition: In this task, you're expected to write answers to questions involving multiple refences to the same entity. Emphasis: The answer to the question should be unambiguous and a phrase in the paragraph. Most questions can have only one correct answer. * Prompt: Answer the given question. Your answer must be a single span in the passage. Passage: {{ passage }} Question:\n\n\nDataset is available at https://instructions. apps.allenai.org Crowdsourcing I nstr uction: Label \"yes\" if the sentence contains any grammatical issues. Otherwise, [...] Crowdsourcing I nstr uction: Answer the provided question based on a given [...]grammar \ncheck \n\ntagging \nessential \nphrases \n\nquestion \ntyping \n\nanswering \nquestions \n\nI nput: She chose to make a salad for lunch on Sunday. \nQuestion: how long did it take for her to make a salad? \n\nCrowdsourcing I nstr uction: List all \nthe words that are essential for \nanswering it correctly. [...] \n\nCrowdsourcing I nstr uction: Label \nthe type of the temporal phenomena \nin the question. Example are [...] \n\nOutput: \n30mins \n\nOutput: \nmaking \nsalad \n\nOutput: \nno \n\n? supervision with seen tasks \n\nOutput: \nEvent \nduration \n\n? evaluation on unseen tasks \n\n\n\n\nin Appendix.category \n# of tasks # of instances \n\nquestion generation \n13 \n38k \nanswer generation \n16 \n53k \nclassification \n12 \n36k \nincorrect answer generation \n8 \n18k \nminimal modification \n10 \n39k \nverification \n2 \n9k \n\nTotal \n61 \n193k \n\n\n\nTable 2 :\n2Task categories and their statistics.\n\nTable 3 :\n3Statistics of NATURAL INSTRUCTIONS\n\nTable 5\n5reports the results of the BART model per \nencodings of different instruction elements ( \u00a75.2) \nand for different task categories. The table shows \nthat encoding more elements of the instructions \ngenerally achieves better results than just using \nPROMPT or POSITIVE EXAMPLES. It additionally \nshows that the benefit of the instruction elements \nseems to depend on the target task category. We ob-\nserve that the question-generation (QG) tasks ben-\nefit the most from POSITIVE EXAMPLES, whereas \nin classification (CF), POSITIVE EXAMPLES are of \n\n\nTable 5 :\n5Cross-task generalization under random split ( \u00a75.1). Models show improved results when provided with \ninstructions. The numbers in parenthesis indicate absolute gains compared to 'NO INSTRUCTIONS' baseline. \nFine-tuned BART archives better performance than GPT3, despite being over 1k times smaller. Category names: \nQG: Question Generation, AG: Answer Generation, CF: Classification, IAG: Incorrect Answer Generation, MM: \nMinimal Text Modification, VF: Verification. All numbers are ROUGE-L (in percentage). \n\nlittle help. We hypothesis this is because it is easier \nto mimic question-generation based on a few ex-\namples, whereas it is difficult to define classes via \na few examples, where DEFINITION can be more \nhelpful. The models show little improvement in \nverification (VF)\n\nTable 6 :\n6Effect of excluding negative examples from FULL INSTRUCTION encoding. Negative instructions are surprisingly difficult for the models to learn from.\n\nTable 7 :\n7Results of humans' perceived importance of instruction elements. Our annotators, for example, find DEF-INITION and THING TO AVOID to be helpful for Classification and Minimal Text Modification tasks, respectively.question generation; MC-TACO incorrect answer \ngeneration). We categorize the errors into common \npatterns (Table 8). \n\nerror type \nBART \n\nGenerates a nonsensical/vague question \n47 \nGenerate an invalid question \n8 \nGenerates a yes/no question \n4 \nCopies the given fact or a subset of it \n3 \nGenerates unanswerable questions \n3 \n\n\n\nTable 8 :\n8Percentage of errors on QASC QG task. The numbers do not sum to 100 since the error types are not mutually exclusive.Unlike GPT3 which generally suffers from generating redundant content irrelevant to instructions, our BART model provides more control, however, it can fail to generate proper output. Here are several erroneous predictions from our model:Generates a nonsensical/vague question BART input:[...]input: Fact: graduations are used for measuring the volume of substances. output: ? BART output: How many substances are there in volume? Copies the given fact or a subset of it BART input:[...] input: Fact: If someone is not breathing, begin with ventilation. output: ? BART output: If someone is breathing, begin with ventilation. Generates a yes/no question BART input:[...] input: Fact: Lack of saliva hinders the break down of food into components the body can absorb. output: ? BART output: Does saliva hinders the break down of food into components the body can absorb?\n\nTable 9 :\n9Steps taken to curate each task in NATURAL INSTRUCTIONS and their estimated times.question generation (from MC\n\nPositive Example -\nExamplePrompt: What is the type of the answer corresponding to the given question? Number, Date, or Span? -Input: Context Word: day. Question: PersonX learned new organizational skills from PersonY because _ 's day schedule was very chaotic. Answer: PersonX -Expected Output: PersonX learned new organizational skills from PersonY because _ 's day schedule was very efficient. Context word: step. Question: PersonX was always ahead of PersonY, as _ walked with a quick step. Answer: PersonX. -Output: PersonY was always ahead of PersonY, as _ walked with a quick step.task instance \n\n-Input: \n\n\n.task id title \n\nsource dataset \ntask category \n\n1 \ntask001_quoref_question_generation \nQuoref \nQuestion Generation \n2 \ntask002_quoref_answer_generation \nQuoref \nAnswer Generation \n\n3 \ntask003_mctaco_question_generation_event_duration \nMC-TACO \nQuestion Generation \n4 \ntask004_mctaco_answer_generation_event_duration \nMC-TACO \nAnswer Generation \n5 \ntask005_mctaco_wrong_answer_generation_event_duration \nMC-TACO \nIncorrect Answer Generation \n6 \ntask006_mctaco_question_generation_transient_stationary \nMC-TACO \nQuestion Generation \n7 \ntask007_mctaco_answer_generation_transient_stationary \nMC-TACO \nAnswer Generation \n8 \ntask008_mctaco_wrong_answer_generation_transient_stationary \nMC-TACO \nIncorrect Answer Generation \n9 \ntask009_mctaco_question_generation_event_ordering \nMC-TACO \nQuestion Generation \n10 \ntask010_mctaco_answer_generation_event_ordering \nMC-TACO \nAnswer Generation \n11 \ntask011_mctaco_wrong_answer_generation_event_ordering \nMC-TACO \nIncorrect Answer Generation \n12 \ntask012_mctaco_question_generation_absolute_timepoint \nMC-TACO \nQuestion Generation \n13 \ntask013_mctaco_answer_generation_absolute_timepoint \nMC-TACO \nAnswer Generation \n14 \ntask014_mctaco_wrong_answer_generation_absolute_timepoint \nMC-TACO \nIncorrect Answer Generation \n15 \ntask015_mctaco_question_generation_frequency \nMC-TACO \nQuestion Generation \n16 \ntask016_mctaco_answer_generation_frequency \nMC-TACO \nAnswer Generation \n17 \ntask017_mctaco_wrong_answer_generation_frequency \nMC-TACO \nIncorrect Answer Generation \n18 \ntask018_mctaco_temporal_reasoning_presence \nMC-TACO \nClassification \n19 \ntask019_mctaco_temporal_reasoning_category \nMC-TACO \nClassification \n20 \ntask020_mctaco_span_based_question \nMC-TACO \nClassification \n21 \ntask021_mctaco_grammatical_logical \nMC-TACO \nClassification \n\n22 \ntask022_cosmosqa_passage_inappropriate_binary \nCosmosqa \nClassification \n23 \ntask023_cosmosqa_question_generation \nCosmosqa \nQuestion Generation \n24 \ntask024_cosmosqa_answer_generation \nCosmosqa \nAnswer Generation \n25 \ntask025_cosmosqa_incorrect_answer_generation \nCosmosqa \nIncorrect Answer Generation \n\n26 \ntask026_drop_question_generation \nDROP \nQuestion Generation \n27 \ntask027_drop_answer_type_generation \nDROP \nClassification \n28 \ntask028_drop_answer_generation \nDROP \nAnswer Generation \n\n29 \ntask029_winogrande_full_object \nWinogrande \nMinimal Text Modification \n30 \ntask030_winogrande_full_person \nWinogrande \nMinimal Text Modification \n31 \ntask031_winogrande_question_generation_object \nWinogrande \nQuestion Generation \n32 \ntask032_winogrande_question_generation_person \nWinogrande \nQuestion Generation \n33 \ntask033_winogrande_answer_generation \nWinogrande \nAnswer Generation \n34 \ntask034_winogrande_question_modification_object \nWinogrande \nMinimal Text Modification \n35 \ntask035_winogrande_question_modification_person \nWinogrande \nMinimal Text Modification \n\n36 \ntask036_qasc_topic_word_to_generate_related_fact \nQASC \nMinimal Text Modification \n37 \ntask037_qasc_generate_related_fact \nQASC \nMinimal Text Modification \n38 \ntask038_qasc_combined_fact \nQASC \nMinimal Text Modification \n39 \ntask039_qasc_find_overlapping_words \nQASC \nVerification \n40 \ntask040_qasc_question_generation \nQASC \nQuestion Generation \n41 \ntask041_qasc_answer_generation \nQASC \nAnswer Generation \n42 \ntask042_qasc_incorrect_option_generation \nQASC \nIncorrect Answer Generation \n\n43 \ntask043_essential_terms_answering_incomplete_questions \nEssential Terms \nAnswer Generation \n44 \ntask044_essential_terms_identifying_essential_words \nEssential Terms \nVerification \n\n45 \ntask045_miscellaneous_sentence_paraphrasing \nMiscellaneous \nMinimal Text Modification \n46 \ntask046_miscellaenous_question_typing \nMiscellaenous \nClassification \n47 \ntask047_miscellaenous_answering_science_questions \nMiscellaenous \nAnswer Generation \n\n48 \ntask048_multirc_question_generation \nMultiRC \nQuestion Generation \n49 \ntask049_multirc_questions_needed_to_answer \nMultiRC \nClassification \n50 \ntask050_multirc_answerability \nMultiRC \nClassification \n51 \ntask051_multirc_correct_answer_single_sentence \nMultiRC \nAnswer Generation \n52 \ntask052_multirc_identify_bad_question \nMultiRC \nClassification \n53 \ntask053_multirc_correct_bad_question \nMultiRC \nMinimal Text Modification \n54 \ntask054_multirc_write_correct_answer \nMultiRC \nAnswer Generation \n55 \ntask055_multirc_write_incorrect_answer \nMultiRC \nIncorrect Answer Generation \n56 \ntask056_multirc_classify_correct_answer \nMultiRC \nClassification \n57 \ntask057_multirc_classify_incorrect_answer \nMultiRC \nClassification \n58 \ntask058_multirc_question_answering \nMultiRC \nAnswer Generation \n\n59 \ntask059_ropes_story_generation \nROPES \nMinimal Text Modification \n60 \ntask060_ropes_question_generation \nROPES \nQuestion Generation \n61 \ntask061_ropes_answer_generation \nROPES \nAnswer Generation \n\n\n\nTable 10 :\n10Detailed set of tasks included in NATURAL INSTRUCTIONS\n\n\n). Prompt-Source tasks are mainly focused on the common NLP downstream tasks (such as question-answering, coreference, NLI, etc). However, since we create tasks from various steps (including the intermediate steps) in a data creation process, our instructions contain a broader variety of tasks. For example, tasks for chaining facts (task 38;Table 10), question typing (task 27;Table 10) or detecting inappropriate content (task 22;Table 10) are unique additions in NATURAL INSTRUCTIONS. Additionally, since our instructions were originally written by various researchers targeted for crowdworkers, they are elaborate and contain the complete definition of each task. This is somewhat evident from observation that GPT3 leads to higher performance on our instructions(Table 11). Last but not least, since we represent the instructions in a structured format, we are able to ablate various elements of the instructions (definition, negative/positive examples, etc.) and empirically quantify their contributions ( \u00a76).Task \nModel \nPromptSource NATURAL INSTRUCTIONS \n\nQuoref QA (002) \nGPT3-Instruct \n43 \n47 \nGPT3 \n2 \n13 \n\nDROP QA (028) \nGPT3-Instruct \n6 \n10 \nGPT3 \n2 \n3 \n\n\n\nTable 11 :\n11Comparing zero-shot performance of GPT3 on our instructions vs. PromptSource. The instructions curated in this work, despite being lengthier, lead to higher performance.task \nNatural Instructions \nPromptSource (Sanh et al. 2021) \n\nMC-TACO \n(question \nanswering) \n\n\n\n\nContext: {{ context }} Question: {{ question }} {{ context }} According to the above context, choose the best option to answer the following question. Question: {{ question }} Options: {{answer_choices}}DROP \n(question \nanswering) \n\n\n\n\n: {{ passage }} Question: {{ question }}Context: {{passage}} \nI am trying to figure out the \nanswer to the question from the \nabove context. Can you tell me \nthe answer? \nQuestion: {{question}} \nAnswer: \n\nWinogrande \n(pronoun \nresolution) \n\n\n\n\nSentence: {{ sentence }} The _ in the sentence below refers to {{option1}}. True or False? {{sentence}}\n\nTable 12 :\n12Qualitative comparison of the task instructions for several shared tasks among NATURAL INSTRUCTIONS and PromptSource\n\nTable 13 :\n13Comparing GPT3 performance on raw crowdsourcing instructions vs. our encoding. All numbers are ROUGE-L.\nWe only focus on textual instructions and avoid datasets that involve visual or auditory steps, mostly focusing on QA datasets that were available to the authors.\nOn average, the process of data curation for each task takes around 5 hrs-34 hrs (details in Appendix;Table 9).\nAcknowledgementsWe thank OpenAI for providing access to the GPT3 API, authors who generously shared their dataset templates with us, Matt Peters and Nicholas Lourie for helpful input, the Beaker team for their support with experiments, and the anonymous reviewers for their helpful feedback. The support of DARPA SAIL-ON, DARPA CHESS program, NSF IIS-2044660, ONR N00014-18-1-2826, and Paul G. Allen Foundation is gratefully acknowledged.In this section, we provide several details on the baselines included in our work.B.1 Encoding of the instructionsAccording to our schema ( \u00a74.1), each instruction I t for the t-th task is a set that contains the following fields:To feed the instances to LMs, we first encoder them into plain text. Let encpI, xq define a function that maps a given instruction I and input instance x to plain text. Evidently, there are many choices for this function. In our study, we consider the following encodings:NO-INSTRUCTIONS encoding. This encoding is the conventional paradigm where no instructions exist: encpIt, xq :\"input : x output :\"(1)PROMPT encoding. In this encoding, we append the prompt message before the input: encpIt, xq :\"Prompt : I prompt t input : x output :\"(2)PROMPT + DEFINITION encoding. In this encoding, the prompt message and the task definition appear before the input: encpIt, xq :\"\"Definition : I def.Intuitively, this encoding is more informative and more complex than \"prompt\" encoding. . .input : x output :\"where enc ex pI t q is an alternating encoding positive and negative examples. We include as many examples as possible, before exceeding the input limit.Such example-only have been used in several recent studies in the field(Zhao et al., 2021).\nPrompt programming for large language models: Beyond the few-shot paradigm. Laria Reynolds, Kyle Mcdonell, Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. Laria Reynolds and Kyle McDonell. 2021. Prompt pro- gramming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Com- puting Systems, pages 1-7.\n\nWinogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Proceedings of the AAAI. the AAAIKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat- ula, and Yejin Choi. 2020. Winogrande: An adver- sarial winograd schema challenge at scale. In Pro- ceedings of the AAAI.\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Shen, Proceedings of ICLR. Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le ScaoICLRStella Biderman, Leo Gao, Thomas Wolfand Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalizationVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Tae- woon Kim, Gunjan Chhablani, Nihal Nayak, De- bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Ab- heesht Sharma, Andrea Santilli, Thibault Fevry, Ja- son Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexan- der M Rush. 2022. Multitask prompted training en- ables zero-shot task generalization. In Proceedings of ICLR.\n\nFew-shot text generation with natural language instructions. Timo Schick, Hinrich Sch\u00fctze, Proceedings of EMNLP. EMNLPTimo Schick and Hinrich Sch\u00fctze. 2021. Few-shot text generation with natural language instructions. In Proceedings of EMNLP.\n\nBleurt: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, Proceedings of ACL. ACLThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. Bleurt: Learning robust metrics for text gen- eration. In Proceedings of ACL, pages 7881-7892.\n\n. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, M Andrew, Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\n\nFinetuned language models are zero-shot learners. Dai, Le, Proceedings of ICLR. ICLRDai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In Proceedings of ICLR.\n\nLearning from task descriptions. Orion Weller, Nicholas Lourie, Matt Gardner, Matthew Peters, Proceedings of EMNLP. EMNLPOrion Weller, Nicholas Lourie, Matt Gardner, and Matthew Peters. 2020. Learning from task descrip- tions. In Proceedings of EMNLP, pages 1361-1375.\n\nHard negative examples are hard, but useful. Hong Xuan, Abby Stylianou, Xiaotong Liu, Robert Pless, Proceedings of ECCV. ECCVSpringerHong Xuan, Abby Stylianou, Xiaotong Liu, and Robert Pless. 2020. Hard negative examples are hard, but useful. In Proceedings of ECCV, pages 126-142. Springer.\n\nCrossfit: A few-shot learning challenge for crosstask generalization in nlp. Qinyuan Ye, Xiang Bill Yuchen Lin, Ren, Proceedings of EMNLP. EMNLPQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. Crossfit: A few-shot learning challenge for cross- task generalization in nlp. In Proceedings of EMNLP.\n\nZero-shot learning by generating task-specific adapters. Qinyuan Ye, Xiang Ren, arXiv:2101.00420arXiv preprintQinyuan Ye and Xiang Ren. 2021. Zero-shot learning by generating task-specific adapters. arXiv preprint arXiv:2101.00420.\n\nCalibrate before use: Improving few-shot performance of language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, Proceedings of ICML. ICMLZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improv- ing few-shot performance of language models. In Proceedings of ICML, pages 12697-12706.\n\nAdapting language models for zeroshot learning by meta-tuning on dataset and prompt collections. Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein, Proceedings of EMNLP: Findings. EMNLP: FindingsRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021. Adapting language models for zero- shot learning by meta-tuning on dataset and prompt collections. In Proceedings of EMNLP: Findings, pages 2856-2878.\n\ngoing on a vacation\" takes longer than \"going for a walk\": A study of temporal commonsense understanding. Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth, Proceedings of EMNLP-IJCNLP. EMNLP-IJCNLPBen Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019. \"going on a vacation\" takes longer than \"going for a walk\": A study of temporal common- sense understanding. In Proceedings of EMNLP- IJCNLP, pages 3354-3360.\n", "annotations": {"author": "[{\"end\":233,\"start\":191},{\"end\":274,\"start\":234},{\"end\":315,\"start\":275},{\"end\":388,\"start\":316}]", "publisher": "[{\"end\":116,\"start\":75},{\"end\":665,\"start\":624}]", "author_last_name": "[{\"end\":205,\"start\":199},{\"end\":248,\"start\":240},{\"end\":287,\"start\":282},{\"end\":335,\"start\":325}]", "author_first_name": "[{\"end\":198,\"start\":191},{\"end\":239,\"start\":234},{\"end\":281,\"start\":275},{\"end\":324,\"start\":316}]", "author_affiliation": "[{\"end\":232,\"start\":207},{\"end\":273,\"start\":250},{\"end\":314,\"start\":289},{\"end\":360,\"start\":337},{\"end\":387,\"start\":362}]", "title": "[{\"end\":74,\"start\":1},{\"end\":462,\"start\":389}]", "venue": "[{\"end\":551,\"start\":464}]", "abstract": "[{\"end\":2093,\"start\":689}]", "bib_ref": "[{\"end\":2246,\"start\":2225},{\"end\":2265,\"start\":2246},{\"end\":2434,\"start\":2411},{\"end\":2452,\"start\":2434},{\"end\":2507,\"start\":2506},{\"end\":5706,\"start\":5687},{\"end\":7763,\"start\":7740},{\"end\":7782,\"start\":7763},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7801,\"start\":7782},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8221,\"start\":8201},{\"end\":8481,\"start\":8463},{\"end\":8499,\"start\":8481},{\"end\":8886,\"start\":8832},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9573,\"start\":9547},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9601,\"start\":9573},{\"end\":9618,\"start\":9601},{\"end\":10263,\"start\":10248},{\"end\":10358,\"start\":10337},{\"end\":10378,\"start\":10358},{\"end\":10400,\"start\":10378},{\"end\":10420,\"start\":10400},{\"end\":10444,\"start\":10420},{\"end\":14889,\"start\":14878},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15776,\"start\":15752},{\"end\":17525,\"start\":17524},{\"end\":18209,\"start\":18187},{\"end\":21025,\"start\":21024},{\"end\":21118,\"start\":21117},{\"end\":24271,\"start\":24270},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24346,\"start\":24325},{\"end\":24431,\"start\":24430},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29331,\"start\":29307},{\"end\":36858,\"start\":36850},{\"end\":37322,\"start\":37304},{\"end\":39957,\"start\":39938}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":41021,\"start\":40905},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41313,\"start\":41022},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41527,\"start\":41314},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41648,\"start\":41528},{\"attributes\":{\"id\":\"fig_5\"},\"end\":41764,\"start\":41649},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42010,\"start\":41765},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42645,\"start\":42011},{\"attributes\":{\"id\":\"fig_9\"},\"end\":42928,\"start\":42646},{\"attributes\":{\"id\":\"fig_10\"},\"end\":43278,\"start\":42929},{\"attributes\":{\"id\":\"fig_11\"},\"end\":43575,\"start\":43279},{\"attributes\":{\"id\":\"fig_12\"},\"end\":43744,\"start\":43576},{\"attributes\":{\"id\":\"fig_13\"},\"end\":43955,\"start\":43745},{\"attributes\":{\"id\":\"fig_14\"},\"end\":44418,\"start\":43956},{\"attributes\":{\"id\":\"fig_15\"},\"end\":45441,\"start\":44419},{\"attributes\":{\"id\":\"fig_16\"},\"end\":45970,\"start\":45442},{\"attributes\":{\"id\":\"fig_17\"},\"end\":46559,\"start\":45971},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47376,\"start\":46560},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":47620,\"start\":47377},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47670,\"start\":47621},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47717,\"start\":47671},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":48274,\"start\":47718},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":49071,\"start\":48275},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":49232,\"start\":49072},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":49788,\"start\":49233},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":50787,\"start\":49789},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":50910,\"start\":50788},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":51523,\"start\":50911},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":56311,\"start\":51524},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":56380,\"start\":56312},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":57553,\"start\":56381},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":57832,\"start\":57554},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":58068,\"start\":57833},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":58312,\"start\":58069},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":58418,\"start\":58313},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":58549,\"start\":58419},{\"attributes\":{\"id\":\"tab_28\",\"type\":\"table\"},\"end\":58667,\"start\":58550}]", "paragraph": "[{\"end\":3397,\"start\":2109},{\"end\":3835,\"start\":3399},{\"end\":5041,\"start\":3865},{\"end\":6207,\"start\":5043},{\"end\":6980,\"start\":6209},{\"end\":7608,\"start\":6982},{\"end\":8439,\"start\":7626},{\"end\":9414,\"start\":8441},{\"end\":10803,\"start\":9416},{\"end\":11045,\"start\":10842},{\"end\":11402,\"start\":11047},{\"end\":11976,\"start\":11404},{\"end\":12541,\"start\":12001},{\"end\":13045,\"start\":12564},{\"end\":13309,\"start\":13047},{\"end\":13403,\"start\":13311},{\"end\":13609,\"start\":13405},{\"end\":13696,\"start\":13611},{\"end\":14066,\"start\":13698},{\"end\":14194,\"start\":14068},{\"end\":14315,\"start\":14196},{\"end\":14381,\"start\":14317},{\"end\":14557,\"start\":14383},{\"end\":14716,\"start\":14559},{\"end\":14969,\"start\":14718},{\"end\":15093,\"start\":15016},{\"end\":15334,\"start\":15095},{\"end\":16887,\"start\":15390},{\"end\":17388,\"start\":16889},{\"end\":17576,\"start\":17427},{\"end\":18610,\"start\":17578},{\"end\":19151,\"start\":18612},{\"end\":19957,\"start\":19187},{\"end\":20077,\"start\":19986},{\"end\":20446,\"start\":20119},{\"end\":21318,\"start\":20448},{\"end\":21499,\"start\":21329},{\"end\":21786,\"start\":21501},{\"end\":22129,\"start\":21788},{\"end\":22697,\"start\":22131},{\"end\":23197,\"start\":22699},{\"end\":23359,\"start\":23213},{\"end\":24806,\"start\":23361},{\"end\":25546,\"start\":24851},{\"end\":25863,\"start\":25548},{\"end\":26199,\"start\":25900},{\"end\":26436,\"start\":26243},{\"end\":26856,\"start\":26438},{\"end\":27992,\"start\":26869},{\"end\":28640,\"start\":28007},{\"end\":29022,\"start\":28697},{\"end\":29495,\"start\":29024},{\"end\":30459,\"start\":29497},{\"end\":31027,\"start\":30488},{\"end\":31555,\"start\":31029},{\"end\":32406,\"start\":31557},{\"end\":32766,\"start\":32435},{\"end\":33030,\"start\":32768},{\"end\":33124,\"start\":33032},{\"end\":33330,\"start\":33126},{\"end\":33496,\"start\":33332},{\"end\":33840,\"start\":33498},{\"end\":33954,\"start\":33842},{\"end\":34063,\"start\":33956},{\"end\":34258,\"start\":34065},{\"end\":34384,\"start\":34260},{\"end\":34602,\"start\":34386},{\"end\":34618,\"start\":34604},{\"end\":34726,\"start\":34639},{\"end\":36597,\"start\":34757},{\"end\":37052,\"start\":36599},{\"end\":37180,\"start\":37054},{\"end\":39572,\"start\":37234},{\"end\":39958,\"start\":39574},{\"end\":40904,\"start\":40030}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":4837,\"start\":4828},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":4906,\"start\":4896},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":10655,\"start\":10647},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11401,\"start\":11385},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11967,\"start\":11958},{\"end\":16069,\"start\":16062},{\"attributes\":{\"ref_id\":\"tab_21\"},\"end\":16267,\"start\":16259},{\"end\":16398,\"start\":16391},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16977,\"start\":16968},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19447,\"start\":19440},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19724,\"start\":19717},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20273,\"start\":20264},{\"end\":22447,\"start\":22440},{\"end\":23828,\"start\":23821},{\"end\":27244,\"start\":27237},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":27709,\"start\":27700},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":27990,\"start\":27983},{\"attributes\":{\"ref_id\":\"tab_17\"},\"end\":32322,\"start\":32315},{\"attributes\":{\"ref_id\":\"tab_21\"},\"end\":32536,\"start\":32528},{\"end\":32680,\"start\":32674},{\"end\":33603,\"start\":33552},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":40367,\"start\":40359}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2107,\"start\":2095},{\"end\":3863,\"start\":3838},{\"attributes\":{\"n\":\"2\"},\"end\":7624,\"start\":7611},{\"attributes\":{\"n\":\"3\"},\"end\":10840,\"start\":10806},{\"attributes\":{\"n\":\"4\"},\"end\":11999,\"start\":11979},{\"attributes\":{\"n\":\"4.1\"},\"end\":12562,\"start\":12544},{\"end\":15014,\"start\":14972},{\"attributes\":{\"n\":\"4.2\"},\"end\":15370,\"start\":15337},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":15388,\"start\":15373},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":17425,\"start\":17391},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":19185,\"start\":19154},{\"attributes\":{\"n\":\"5\"},\"end\":19984,\"start\":19960},{\"attributes\":{\"n\":\"5.1\"},\"end\":20117,\"start\":20080},{\"attributes\":{\"n\":\"5.2\"},\"end\":21327,\"start\":21321},{\"attributes\":{\"n\":\"6\"},\"end\":23211,\"start\":23200},{\"attributes\":{\"n\":\"6.1\"},\"end\":24849,\"start\":24809},{\"attributes\":{\"n\":\"6.2\"},\"end\":25898,\"start\":25866},{\"attributes\":{\"n\":\"6.3\"},\"end\":26241,\"start\":26202},{\"attributes\":{\"n\":\"6.4\"},\"end\":26867,\"start\":26859},{\"attributes\":{\"n\":\"7\"},\"end\":28005,\"start\":27995},{\"end\":28695,\"start\":28643},{\"end\":30486,\"start\":30462},{\"end\":32433,\"start\":32409},{\"end\":34637,\"start\":34621},{\"end\":34755,\"start\":34729},{\"end\":37187,\"start\":37183},{\"end\":37232,\"start\":37190},{\"end\":39991,\"start\":39961},{\"end\":40028,\"start\":39994},{\"end\":40916,\"start\":40906},{\"end\":41024,\"start\":41023},{\"end\":41539,\"start\":41529},{\"end\":41660,\"start\":41650},{\"end\":42657,\"start\":42647},{\"end\":43290,\"start\":43280},{\"end\":43597,\"start\":43577},{\"end\":43757,\"start\":43746},{\"end\":44422,\"start\":44420},{\"end\":45973,\"start\":45972},{\"end\":47631,\"start\":47622},{\"end\":47681,\"start\":47672},{\"end\":47726,\"start\":47719},{\"end\":48285,\"start\":48276},{\"end\":49082,\"start\":49073},{\"end\":49243,\"start\":49234},{\"end\":49799,\"start\":49790},{\"end\":50798,\"start\":50789},{\"end\":50930,\"start\":50912},{\"end\":56323,\"start\":56313},{\"end\":57565,\"start\":57555},{\"end\":58430,\"start\":58420},{\"end\":58561,\"start\":58551}]", "table": "[{\"end\":47376,\"start\":46812},{\"end\":47620,\"start\":47391},{\"end\":48274,\"start\":47728},{\"end\":49071,\"start\":48287},{\"end\":49788,\"start\":49458},{\"end\":50910,\"start\":50882},{\"end\":51523,\"start\":51499},{\"end\":56311,\"start\":51527},{\"end\":57553,\"start\":57400},{\"end\":57832,\"start\":57737},{\"end\":58068,\"start\":58038},{\"end\":58312,\"start\":58111}]", "figure_caption": "[{\"end\":41021,\"start\":40918},{\"end\":41313,\"start\":41025},{\"end\":41527,\"start\":41316},{\"end\":41648,\"start\":41541},{\"end\":41764,\"start\":41662},{\"end\":42010,\"start\":41767},{\"end\":42645,\"start\":42013},{\"end\":42928,\"start\":42659},{\"end\":43278,\"start\":42931},{\"end\":43575,\"start\":43292},{\"end\":43744,\"start\":43600},{\"end\":43955,\"start\":43760},{\"end\":44418,\"start\":43958},{\"end\":45441,\"start\":44423},{\"end\":45970,\"start\":45444},{\"end\":46559,\"start\":45974},{\"end\":46812,\"start\":46562},{\"end\":47391,\"start\":47379},{\"end\":47670,\"start\":47633},{\"end\":47717,\"start\":47683},{\"end\":49232,\"start\":49084},{\"end\":49458,\"start\":49245},{\"end\":50787,\"start\":49801},{\"end\":50882,\"start\":50800},{\"end\":51499,\"start\":50938},{\"end\":51527,\"start\":51526},{\"end\":56380,\"start\":56326},{\"end\":57400,\"start\":56383},{\"end\":57737,\"start\":57568},{\"end\":58038,\"start\":57835},{\"end\":58111,\"start\":58071},{\"end\":58418,\"start\":58315},{\"end\":58549,\"start\":58433},{\"end\":58667,\"start\":58564}]", "figure_ref": "[{\"end\":2516,\"start\":2508},{\"end\":3045,\"start\":3038},{\"end\":3396,\"start\":3389},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6601,\"start\":6594},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12175,\"start\":12170},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":12796,\"start\":12789},{\"end\":15751,\"start\":15750},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":17374,\"start\":17368},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":19315,\"start\":19303},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":22087,\"start\":22082},{\"end\":28765,\"start\":28759},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":29063,\"start\":29057},{\"end\":29594,\"start\":29586},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":31847,\"start\":31838},{\"end\":32607,\"start\":32600},{\"end\":37094,\"start\":37085},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37179,\"start\":37173}]", "bib_author_first_name": "[{\"end\":60738,\"start\":60733},{\"end\":60753,\"start\":60749},{\"end\":61139,\"start\":61132},{\"end\":61153,\"start\":61151},{\"end\":61168,\"start\":61161},{\"end\":61180,\"start\":61175},{\"end\":61414,\"start\":61408},{\"end\":61427,\"start\":61421},{\"end\":61441,\"start\":61436},{\"end\":61457,\"start\":61450},{\"end\":61471,\"start\":61464},{\"end\":61486,\"start\":61482},{\"end\":61504,\"start\":61497},{\"end\":61520,\"start\":61514},{\"end\":61535,\"start\":61531},{\"end\":61547,\"start\":61542},{\"end\":61559,\"start\":61553},{\"end\":61572,\"start\":61566},{\"end\":61583,\"start\":61577},{\"end\":61598,\"start\":61593},{\"end\":61621,\"start\":61614},{\"end\":61639,\"start\":61633},{\"end\":61650,\"start\":61645},{\"end\":61671,\"start\":61662},{\"end\":61687,\"start\":61679},{\"end\":61699,\"start\":61695},{\"end\":61721,\"start\":61718},{\"end\":61735,\"start\":61729},{\"end\":61747,\"start\":61742},{\"end\":62881,\"start\":62877},{\"end\":62897,\"start\":62890},{\"end\":63121,\"start\":63113},{\"end\":63138,\"start\":63130},{\"end\":63149,\"start\":63144},{\"end\":63339,\"start\":63334},{\"end\":63352,\"start\":63345},{\"end\":63367,\"start\":63360},{\"end\":63380,\"start\":63374},{\"end\":63391,\"start\":63386},{\"end\":63395,\"start\":63392},{\"end\":63405,\"start\":63400},{\"end\":63417,\"start\":63414},{\"end\":63423,\"start\":63422},{\"end\":63754,\"start\":63749},{\"end\":63771,\"start\":63763},{\"end\":63784,\"start\":63780},{\"end\":63801,\"start\":63794},{\"end\":64035,\"start\":64031},{\"end\":64046,\"start\":64042},{\"end\":64066,\"start\":64058},{\"end\":64078,\"start\":64072},{\"end\":64363,\"start\":64356},{\"end\":64373,\"start\":64368},{\"end\":64642,\"start\":64635},{\"end\":64652,\"start\":64647},{\"end\":64889,\"start\":64884},{\"end\":64900,\"start\":64896},{\"end\":64913,\"start\":64910},{\"end\":64923,\"start\":64920},{\"end\":64937,\"start\":64931},{\"end\":65262,\"start\":65257},{\"end\":65276,\"start\":65270},{\"end\":65287,\"start\":65282},{\"end\":65298,\"start\":65295},{\"end\":65673,\"start\":65670},{\"end\":65686,\"start\":65680},{\"end\":65702,\"start\":65697},{\"end\":65712,\"start\":65709}]", "bib_author_last_name": "[{\"end\":60747,\"start\":60739},{\"end\":60762,\"start\":60754},{\"end\":61149,\"start\":61140},{\"end\":61159,\"start\":61154},{\"end\":61173,\"start\":61169},{\"end\":61192,\"start\":61181},{\"end\":61198,\"start\":61194},{\"end\":61419,\"start\":61415},{\"end\":61434,\"start\":61428},{\"end\":61448,\"start\":61442},{\"end\":61462,\"start\":61458},{\"end\":61480,\"start\":61472},{\"end\":61495,\"start\":61487},{\"end\":61512,\"start\":61505},{\"end\":61529,\"start\":61521},{\"end\":61540,\"start\":61536},{\"end\":61551,\"start\":61548},{\"end\":61564,\"start\":61560},{\"end\":61575,\"start\":61573},{\"end\":61591,\"start\":61584},{\"end\":61612,\"start\":61599},{\"end\":61631,\"start\":61622},{\"end\":61643,\"start\":61640},{\"end\":61660,\"start\":61651},{\"end\":61677,\"start\":61672},{\"end\":61693,\"start\":61688},{\"end\":61705,\"start\":61700},{\"end\":61716,\"start\":61707},{\"end\":61727,\"start\":61722},{\"end\":61740,\"start\":61736},{\"end\":61754,\"start\":61748},{\"end\":61760,\"start\":61756},{\"end\":62888,\"start\":62882},{\"end\":62905,\"start\":62898},{\"end\":63128,\"start\":63122},{\"end\":63142,\"start\":63139},{\"end\":63156,\"start\":63150},{\"end\":63343,\"start\":63340},{\"end\":63358,\"start\":63353},{\"end\":63372,\"start\":63368},{\"end\":63384,\"start\":63381},{\"end\":63398,\"start\":63396},{\"end\":63412,\"start\":63406},{\"end\":63420,\"start\":63418},{\"end\":63430,\"start\":63424},{\"end\":63584,\"start\":63581},{\"end\":63588,\"start\":63586},{\"end\":63761,\"start\":63755},{\"end\":63778,\"start\":63772},{\"end\":63792,\"start\":63785},{\"end\":63808,\"start\":63802},{\"end\":64040,\"start\":64036},{\"end\":64056,\"start\":64047},{\"end\":64070,\"start\":64067},{\"end\":64084,\"start\":64079},{\"end\":64366,\"start\":64364},{\"end\":64389,\"start\":64374},{\"end\":64394,\"start\":64391},{\"end\":64645,\"start\":64643},{\"end\":64656,\"start\":64653},{\"end\":64894,\"start\":64890},{\"end\":64908,\"start\":64901},{\"end\":64918,\"start\":64914},{\"end\":64929,\"start\":64924},{\"end\":64943,\"start\":64938},{\"end\":65268,\"start\":65263},{\"end\":65280,\"start\":65277},{\"end\":65293,\"start\":65288},{\"end\":65304,\"start\":65299},{\"end\":65678,\"start\":65674},{\"end\":65695,\"start\":65687},{\"end\":65707,\"start\":65703},{\"end\":65717,\"start\":65713}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":231925131},\"end\":61067,\"start\":60657},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":198893658},\"end\":61406,\"start\":61069},{\"attributes\":{\"id\":\"b2\"},\"end\":62814,\"start\":61408},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":238260199},\"end\":63058,\"start\":62816},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":215548699},\"end\":63330,\"start\":63060},{\"attributes\":{\"id\":\"b5\"},\"end\":63529,\"start\":63332},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":237416585},\"end\":63714,\"start\":63531},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":226262281},\"end\":63984,\"start\":63716},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":220793249},\"end\":64277,\"start\":63986},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":233296709},\"end\":64576,\"start\":64279},{\"attributes\":{\"doi\":\"arXiv:2101.00420\",\"id\":\"b10\"},\"end\":64809,\"start\":64578},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":231979430},\"end\":65158,\"start\":64811},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":237304362},\"end\":65562,\"start\":65160},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":202541184},\"end\":65977,\"start\":65564}]", "bib_title": "[{\"end\":60731,\"start\":60657},{\"end\":61130,\"start\":61069},{\"end\":62875,\"start\":62816},{\"end\":63111,\"start\":63060},{\"end\":63579,\"start\":63531},{\"end\":63747,\"start\":63716},{\"end\":64029,\"start\":63986},{\"end\":64354,\"start\":64279},{\"end\":64882,\"start\":64811},{\"end\":65255,\"start\":65160},{\"end\":65668,\"start\":65564}]", "bib_author": "[{\"end\":60749,\"start\":60733},{\"end\":60764,\"start\":60749},{\"end\":61151,\"start\":61132},{\"end\":61161,\"start\":61151},{\"end\":61175,\"start\":61161},{\"end\":61194,\"start\":61175},{\"end\":61200,\"start\":61194},{\"end\":61421,\"start\":61408},{\"end\":61436,\"start\":61421},{\"end\":61450,\"start\":61436},{\"end\":61464,\"start\":61450},{\"end\":61482,\"start\":61464},{\"end\":61497,\"start\":61482},{\"end\":61514,\"start\":61497},{\"end\":61531,\"start\":61514},{\"end\":61542,\"start\":61531},{\"end\":61553,\"start\":61542},{\"end\":61566,\"start\":61553},{\"end\":61577,\"start\":61566},{\"end\":61593,\"start\":61577},{\"end\":61614,\"start\":61593},{\"end\":61633,\"start\":61614},{\"end\":61645,\"start\":61633},{\"end\":61662,\"start\":61645},{\"end\":61679,\"start\":61662},{\"end\":61695,\"start\":61679},{\"end\":61707,\"start\":61695},{\"end\":61718,\"start\":61707},{\"end\":61729,\"start\":61718},{\"end\":61742,\"start\":61729},{\"end\":61756,\"start\":61742},{\"end\":61762,\"start\":61756},{\"end\":62890,\"start\":62877},{\"end\":62907,\"start\":62890},{\"end\":63130,\"start\":63113},{\"end\":63144,\"start\":63130},{\"end\":63158,\"start\":63144},{\"end\":63345,\"start\":63334},{\"end\":63360,\"start\":63345},{\"end\":63374,\"start\":63360},{\"end\":63386,\"start\":63374},{\"end\":63400,\"start\":63386},{\"end\":63414,\"start\":63400},{\"end\":63422,\"start\":63414},{\"end\":63432,\"start\":63422},{\"end\":63586,\"start\":63581},{\"end\":63590,\"start\":63586},{\"end\":63763,\"start\":63749},{\"end\":63780,\"start\":63763},{\"end\":63794,\"start\":63780},{\"end\":63810,\"start\":63794},{\"end\":64042,\"start\":64031},{\"end\":64058,\"start\":64042},{\"end\":64072,\"start\":64058},{\"end\":64086,\"start\":64072},{\"end\":64368,\"start\":64356},{\"end\":64391,\"start\":64368},{\"end\":64396,\"start\":64391},{\"end\":64647,\"start\":64635},{\"end\":64658,\"start\":64647},{\"end\":64896,\"start\":64884},{\"end\":64910,\"start\":64896},{\"end\":64920,\"start\":64910},{\"end\":64931,\"start\":64920},{\"end\":64945,\"start\":64931},{\"end\":65270,\"start\":65257},{\"end\":65282,\"start\":65270},{\"end\":65295,\"start\":65282},{\"end\":65306,\"start\":65295},{\"end\":65680,\"start\":65670},{\"end\":65697,\"start\":65680},{\"end\":65709,\"start\":65697},{\"end\":65719,\"start\":65709}]", "bib_venue": "[{\"end\":61233,\"start\":61225},{\"end\":62006,\"start\":61965},{\"end\":62934,\"start\":62929},{\"end\":63181,\"start\":63178},{\"end\":63615,\"start\":63611},{\"end\":63837,\"start\":63832},{\"end\":64111,\"start\":64107},{\"end\":64423,\"start\":64418},{\"end\":64970,\"start\":64966},{\"end\":65353,\"start\":65338},{\"end\":65760,\"start\":65748},{\"end\":60847,\"start\":60764},{\"end\":61223,\"start\":61200},{\"end\":61781,\"start\":61762},{\"end\":62927,\"start\":62907},{\"end\":63176,\"start\":63158},{\"end\":63609,\"start\":63590},{\"end\":63830,\"start\":63810},{\"end\":64105,\"start\":64086},{\"end\":64416,\"start\":64396},{\"end\":64633,\"start\":64578},{\"end\":64964,\"start\":64945},{\"end\":65336,\"start\":65306},{\"end\":65746,\"start\":65719}]"}}}, "year": 2023, "month": 12, "day": 17}