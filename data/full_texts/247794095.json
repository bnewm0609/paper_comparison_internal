{"id": 247794095, "updated": "2023-10-05 16:00:24.363", "metadata": {"title": "PerfectDou: Dominating DouDizhu with Perfect Information Distillation", "authors": "[{\"first\":\"Guan\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Minghuan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Weijun\",\"last\":\"Hong\",\"middle\":[]},{\"first\":\"Weinan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Fei\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Guangjun\",\"last\":\"Zeng\",\"middle\":[]},{\"first\":\"Yue\",\"last\":\"Lin\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "As a challenging multi-player card game, DouDizhu has recently drawn much attention for analyzing competition and collaboration in imperfect-information games. In this paper, we propose PerfectDou, a state-of-the-art DouDizhu AI system that dominates the game, in an actor-critic framework with a proposed technique named perfect information distillation. In detail, we adopt a perfect-training-imperfect-execution framework that allows the agents to utilize the global information to guide the training of the policies as if it is a perfect information game and the trained policies can be used to play the imperfect information game during the actual gameplay. To this end, we characterize card and game features for DouDizhu to represent the perfect and imperfect information. To train our system, we adopt proximal policy optimization with generalized advantage estimation in a parallel training paradigm. In experiments we show how and why PerfectDou beats all existing AI programs, and achieves state-of-the-art performance.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.16406", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/YangLH0FZL22", "doi": "10.48550/arxiv.2203.16406"}}, "content": {"source": {"pdf_hash": "e06afabc93bb10873d65d983c6da326f88f8c2fc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2203.16406v6.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cb273ec38eae2248ef78a018d5cd8f35dd364c11", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e06afabc93bb10873d65d983c6da326f88f8c2fc.txt", "contents": "\nPerfectDou: Dominating DouDizhu with Perfect Information Distillation\n\n\nGuan Yang yangguan@corp.netease.com \nNetEase Games AI Lab\n\n\nMinghuan Liu \nShanghai Jiao Tong University\n\n\nWeijun Hong \nNetEase Games AI Lab\n\n\nWeinan Zhang wnzhang@sjtu.edu.cn \nShanghai Jiao Tong University\n\n\nFei Fang feif@cs.cmu.edu \nCarnegie Mellon University\n\n\nGuangjun Zeng gzzengguanjun@corp.netease.com \nNetEase Games AI Lab\n\n\nYue Lin gzlinyue@corp.netease.com \nNetEase Games AI Lab\n\n\nPerfectDou: Dominating DouDizhu with Perfect Information Distillation\n\nAs a challenging multi-player card game, DouDizhu has recently drawn much attention for analyzing competition and collaboration in imperfect-information games. In this paper, we propose PerfectDou, a state-of-the-art DouDizhu AI system that dominates the game, in an actor-critic framework with a proposed technique named perfect information distillation. In detail, we adopt a perfecttraining-imperfect-execution framework that allows the agents to utilize the global information to guide the training of the policies as if it is a perfect information game and the trained policies can be used to play the imperfect information game during the actual gameplay. To this end, we characterize card and game features for DouDizhu to represent the perfect and imperfect information. To train our system, we adopt proximal policy optimization with generalized advantage estimation in a parallel training paradigm. In experiments we show how and why PerfectDou beats all existing AI programs, and achieves state-of-the-art performance. * Equal contribution. Yang is responsible for the basic idea, system design and implementation details; Liu mainly contributes to the methodology, writing and experimental design. \u2020Corresponding author. Project page at https://github.com/Netease-Games-AI-Lab-Guangzhou/PerfectDou/.36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n\nIntroduction\n\nWith the fast development of Reinforcement Learning (RL), game AI has achieved great success in many types of games, including board games (e.g., Go [22], chess [21]), card games (e.g., Texas Hold'em [3], Mahjong [15]), and video games (e.g., Starcraft [26], Dota [1]). As one of the most popular card games in China, DouDizhu has not been studied in depth until very recently. In perfectinformation games such as Go, agent can observe all the events occurred previously including initial hand of each agent and all agents' actions. In contrast, DouDizhu is an imperfect-information game with special structure, and an agent does not know other agents' initial hands but can observe all agents' actions. One challenge in DouDizhu is that it is a three-player game with both competition and collaboration: the two Peasant players need to cooperate as a team to compete with the third Landlord player. In addition, DouDizhu has a large action space that is hard to be abstracted for search-based methods [31].\n\nAlthough various methods have been proposed for tackling these challenges [29,10], they are either computationally expensive or far from optimal, and highly rely on abstractions with human knowledge [31]. Recently, Zha et al. [31] proposed DouZero, which applies simple Deep Monte-Carlo (DMC) method to learn the value function with pre-designed features and reward function. DouZero is regarded as the state-of-the-art (SoTA) AI system of DouDizhu for its superior performance and training efficiency compared with previous works.\n\nUnfortunately, we find DouZero still has severe limitations in many battle scenarios, which will be characterized in Section D. 3. To establish a stronger and more robust DouDizhu bot, in this paper, we present a new AI system named PerfectDou, and show that it leads to significantly better performance than existing AI systems including DouZero. The name of our program follows the key technique we use -perfect information distillation. The proposed technique utilizes a Perfect-Training-Imperfect-Execution (PTIE) framework, a variant of the popular Centralized-Training-Decentralized-Execution (CTDE) paradigm in multi-agent RL literature [5,11]. Namely, we feed perfect-information to the agent in the training phase to guide the training of the policy, and only imperfect-information can be used when deploying the learned policy for actual game play. Correspondingly, we further design the card and game features to represent the perfect and imperfect information. To train PerfectDou, we utilize Proximal Policy Optimization (PPO) [19] with Generalized Advantage Estimation (GAE) [18] by self-play in a distributed training system.\n\nIn experiments, we show PerfectDou beats all the existing DouDizhu AI systems and achieves the SoTA performance in a 10k-decks tournament; moreover, PerfectDou is the most training efficient, such that the number of samples required is an order of magnitude lower than the previous SoTA method; for application usage, PerfectDou can be deployed in online game environment due to its low inference time.\n\n\nPreliminaries\n\nImperfect-Information Extensive-Form Games. An imperfect-information extensive-form (or tree-form) game can be described as a tuple G = (P, H, Z, A, T , \u03c7, \u03c1, r, I), where P denotes a finite set of players, A is a finite set of actions, and H is a finite set of nodes at which players can take actions and are similar to states in an RL problem. At a node h \u2208 H, \u03c7 : H \u2192 2 A is the action function that assigns to each node h \u2208 H a set of possible actions, and \u03c1 : H \u2192 P represents the unique acting player. An action a \u2208 A(h) that leads from h to h is denoted by the successor function T : H \u00d7 A \u2192 H as h = T (h, a). Z \u2286 H are the sets of terminal nodes for which no actions are available. For each player p \u2208 P, there is a reward function r p \u2208 r = r 1 , r 2 , . . . , r |P| : Z \u2192 R. Furthermore, I = {I p |p \u2208 P} describes the information sets (infosets) in the game where I p is a partition of all the nodes with acting player p. If two nodes h, h belong to the same infoset I of player p, i.e., h, h \u2208 I \u2208 I p , these two nodes are indistinguishable to p and will share the same action set. We use I(h) to denote the infoset of node h. Upon a certain infoset, a policy (or a behavior strategy) \u03c0 p for player p describes which action the player would take at each infoset. A policy can be stochastic, and we use \u03c0 p (I) to denote the probability vector over player p's available actions at infoset I. With a slight abuse of notation, we use \u03c0 p (h) to denote the stochastic action player p will take at node h. For two nodes h, h that belong to the same infoset I, it is clear that \u03c0 p (h) = \u03c0 p (h ) = \u03c0 p (I). Therefore, the objective for each player p is to maximize its own total expected return at the end of the game: R p E Z\u223c\u03c0 [r p (Z)], Z \u2208 Z.\n\nThe Three-Players DouDizhu Game. DouDizhu (a.k.a. Fight the Landload) is a three-player card game that is popular in China and is played by hundreds of millions of people. Among the three players, two of them are called the Peasants, and they need to cooperate as a team to compete against another player called the Landlord. The standard game consists of two phases, bidding and cardplay. The bidding phase designates the roles to the players and deals leftover cards to the Landlord. In the cardplay phase, the three players play cards in turn in clock-wise order. Within a game episode, there are several rounds, and each begins with one player showing a legal combination of cards (solo, pair, etc.). The subsequent players must either choose to pass or beat the previous hand by playing a more superior combination of cards, usually in the same category. The round continues until two consecutive players choose to pass and the player who played the last hand initiates to the next round. DouDizhu is in the genre of shedding where the player wins by emptying his's hand, or loses vice versa. Therefore, in this game, the suit does not matter but the rank does. The score of a game is calculated as the base score multiplied by a multiplier determined by specialized categories of cards (Appendix B.2 shows the details). In this paper, we only consider the cardplay phase, which can be formulated as an imperfect-information game. More detailed information about the game can be referred to [31].\n\nThe key challenges of DouDizhu include how the Peasants work as a team to beat the Landlord with card number advantage using only imperfect information. For example, one Peasant can try to help his teammate to win by always trying the best to beat the Landlord's cards and play cards in a category where the teammate has an advantage. In addition, the action space of DouDizhu is particularly large, and there are 27,472 possible combinations of cards that can be played in total with hundreds of legal actions in a hand. Furthermore, the action space cannot be easily abstracted since improperly playing a card may break other potential card combinations in the following rounds and lead to losing the game.\n\n\nMethodology\n\nIn this section, we first introduce how perfect-training-imperfect-execution works for a general imperfect-information game. Then, we formulate the DouDizhu game as an imperfect-information game to solve. In card games as DouDizhu, the imperfect-information property comes from the fact that players do not show their hand cards to the others. And therefore the critical challenge for each player is to deal with the indistinguishable nodes from the same infoset. For such games, consider we can construct a strategically identical perfect-information game and allow one player to observe distinguishable nodes, then, the decisions at each node can rely on the global information and he may have more chances to win the game, like owning a cheating plug-in. This motivates us to utilize the distinguishable nodes for training the agents of imperfect-information games, and therefore we propose the technique of perfect information distillation.\n\n\nPerfect Information Distillation\n\nIn general, the perfect information distillation is a framework trained in perfect-training-imperfect-execution (PTIE) paradigm, a variant of centralized-training-decentralizedexecution (CTDE) [5,11], as illustrated in Fig. 1. Particularly, CTDE constructs the value function with all agents' observations and actions for general multi-agent tasks. By comparison, our proposed PTIE is designed for imperfectinformation games where additional perfect information is introduced in the training stage. In this work, we consider to initiate PTIE with actor-critic [25] (but PTIE is not limited to actor-critic), which is a template of policy gradient (PG) methods, proposed in the RL literature towards maximizing the expected reward of the policy function through PG with a value function:\n\u2207 \u03b8p J = E \u03c0 [\u2207 \u03b8p log \u03c0 \u03b8p (a|s)Q \u03c0 (s, a)] ,(1)\nwhere s denotes a state in an RL problem, Q is the state-action value function learned by a function approximator, usually called the critic. Notice that the critic is playing the role of evaluating how good an action is taken at a specific situation, but only at the training time. When the agent is deployed into inference, only the policy \u03c0 can be used to inferring feasible actions. Therefore, for imperfect-information games, we can provide additional information about the exact node the player is in to train the critic with self-play, as long as the actor does not take such information for decision making. Intuitively, we are distilling the perfect information into the imperfect policy.\n\nFormally, for each node h, we construct a distinguishable node D(h) for the strategically identical perfect-information game. Then, we define the value function at\nD(h), V \u03c0p (D(h)) = E a\u223c\u03c0p,h 0 =h [Q \u03c0p (D(h), a)] = E Z|\u03c0p,h 0 =h [r p (Z)]\nas the expected value of distinguishable nodes. In the sequel, we propose a simple extension of actor-critic policy gradient considering parameterized policy \u03c0 \u03b8p for each player p:\n\u2207 \u03b8p J = E \u03c0p [\u2207 \u03b8p log \u03c0 \u03b8p (a|h)Q \u03c0p (D(h), a)] .(2)\nIn practice, we use a policy network (actor) to represent the policy \u03c0 p for each player, which takes as input a vector describing the representation at an indistinguishable node h that the player can observe during the game. For estimating the critic, a value network is utilized, which takes representation of the global information at the distinguishable node D(h). In other words, the value network takes additional information as inputs (such as other players' cards in Poker games) or targets (such as immediate rewards computed upon others' hands), while the policy network does not. During the training, the value function updates the values for all distinguishable nodes; then, it trains the policy on every node on the same infoset from sampled data, which implicitly gives an expected value estimation on each infoset. In practice, the generalization ability of neural networks enables the policy to find a better solution, which is also the advantage for using the proposed PTIE framework.\n\nPTIE is a general way for training imperfect-information game agents using RL. In addition, the optimality point of independent RL in multi-agent learning is exactly one of an Nash equilibria (NE) and it has been proved that training RL algorithms with self-play can converge to an NE in two-player cases (although no convergence guarantees for three-player games) [9,12], which may provide insights why PTIE works. We expect that with PTIE, players can leverage the perfect information during inference to derive coordination and strategic policies. In experiments, we show that this allows PerfectDou to cooperate with each other (as Peasants) or compete against the team (as the Landlord).\n\n\nDouDizhu as An Imperfect-Information Game\n\nAs is mentioned in Section 2, the cardplay phase of DouDizhu can be regarded as an imperfectinformation game with three players. At each node h, its infoset I p contains all combinations of the other's invisible handcards. Then at each level of the game tree, the three players take clockwise turns to choose an available action with policies \u03c0 p depending on the infoset I p (h) of the current node h with the reward function r p . The path from the root of the game tree to a node h contains the initial hand of all players and all the historical moves of all players. The reward functions at leaf nodes are set to be the score the players win or lose at the end of the game. In this section, we explain how we construct our PerfectDou system in detail, with the proposed perfect information distillation technique, and several novel components designed for DouDizhu that help it summit the game. Particularly, PTIE requires different representations as input layer for the policy and the value network by feeding the value function with perfect information (distinguishable nodes) and the policy with imperfect information (indistinguishable nodes).\n\n\nPerfectDou System Design\n\n\nCard Representation\n\nIn our system, we encode each feasible card combination with a 12 \u00d7 15 matrix, as shown in Fig. 2. Specifically, we first encode different ranks and numbers with a 4 \u00d7 15 matrix, where the columns correspond to the 15 ranks (including jokers) and similar to Zha et al. [31], the number of ones in the four rows of a single column represents the number of cards of that rank in the player's hand. Different from Zha et al. [31], we further propose to encode the legal combination of cards with the player's current hand, to help the agent realize the different property of various kinds of cards (see Appendix B.1). The feature sizes of each part are shown in Appendix C.1. \n\n\nNode Representation\n\nIn the game of DouDizhu, the distinguishable node D(h) should cover all players' hand cards at h, along with the game and player status. Therefore, we propose to represent h with imperfect features and D(h) with perfect feature designs, shown in Tab. 1. In detail, the imperfect features include a flatten matrix 2 of 23 \u00d7 12 \u00d7 15 and a game state array of 6 \u00d7 1. On the contrary, the perfect features consist of a flatten card matrix of 25 \u00d7 12 \u00d7 15 and a game state array of 8 \u00d7 1. Therefore, they are totally asymmetric, and the imperfect features are a subset of the perfect features.\n\n\nNetwork Structure and Action Representation\n\nThe PerfectDou system follows the general actor-critic design, and we take PPO [19] with GAE [18] as the learning algorithm. Slightly different from Eq. (2), PPO estimates the advantage A p = R p \u2212 V \u03c0p as the critic instead of Q \u03c0p . For value network, we use an MLP to handle encoded features (the detailed structure is shown in Appendix C.3). As for the policy network, we first utilize an LSTM to encode all designed features; to encourage the agent to pay attention to specific card types, the proposed network structure will encode all the available actions into feature vectors, as depicted in Tab. 6. The output of the legal action probability is then computed with the action and game features, as illustrated in Fig. 3. Formally, we concatenate the node representation e s with each action representation e a i separately, and get the legal action distribution:\np(a) = softmax(f ([e s , e a i ] N i=1 ) ,(3)\nwhere a i is the i-th action, [\u00b7] denotes the concatenation operation for N available actions, and f are layers of MLPs. This resembles the target attention mechanism in Ye et al. [28].\n\n\nPerfect Reward Design\n\nIf we only care about the result at the end of the game, the reward at leaf nodes is rather sparse; in addition, players can only estimate their advantage of winning the game using imperfect information during the game, which could be inaccurate and fluctuated. Thanks to PTIE, we are allowed to impose an oracle reward function for DouDizhu at each node to enhance the perfect information modeled by the value function. In the training of PerfectDou, instead of estimating the advantage, we utilize an oracle 3 for evaluating each player, particularly, the minimum steps needed to play out all cards, which can be treated as a simple estimation of the distance to win. The reward function is 2 Short for a matrix flattened to a one-dimensional vector. 3 Implemented as a dynamic programming algorithm, see Appendix E for details. then defined as the advantage difference computed by the relative distance to win of the two camps in two consecutive timesteps. Formally, at timestep t, the reward function is:\nr t = \u22121.0 \u00d7 (Adv t \u2212 Adv t\u22121 ) \u00d7 l, Landlord 0.5 \u00d7 (Adv t \u2212 Adv t\u22121 ) \u00d7 l, Peasant (4) Adv t = N Landlord t \u2212 min N Peasant1 t , N Peasant2 t ,(5)\nwhere l is a scaling factor, and N t is the minimum steps to play out all cards at timestep t.\n\nFor instance, in a round, at timestamp t, the distance of the Landlord to win is 5 and the distances of two Peasants are 3 and 7, which means Peasants have a larger advantage since the relative distance is 2 for Peasants and -2 for the Landlord. However, if the Landlord plays a good hand such that both Peasants can not suppress, the Landlord will in result get a positive reward due to the decreased relative distance of the Landlord, i.e., from 2 to 1. Correspondingly, the Peasants would get a negative reward as their relative distances are getting larger. Such a reward function can encourage the cooperation between Peasants, since the winning distance is defined by the minimum steps of both players. In our implementation, the computation of the rewards is carried out after a round of the game, hence to promote training efficiency. To further expedite the training procedure, we design a distributed training system represented in Fig. 4. Specifically, the system contains a set of rollout workers for collecting the self-play experience data and sending it to a pool of GPUs; these GPUs asynchronously receive the data and store it into their local buffers. Then, each GPU learner randomly samples mini-batches from its own buffer and compute the gradient separately, which is then synchronously averaged across all GPUs and back propagated to update the neural networks. After each round of updating, new parameters are sent to every rollout worker. And each worker will load the latest model after 24 (8 for each player) steps sampling. Such a decoupled training-sampling structure will allow PerfectDou to be extended to large scale experiments. Our design of the distributed system borrows a lot from IMPALA [4], which also keeps a set of rollout workers to receive the updated model, interact with the environment and send back rollout trajectories to learners. The main difference is derived from the learning algorithm where we use PPO with GAE instead of actor-critic with V-trace [4]. Moreover, we keep three different models for Landlord and two Peasants separately which are only updated by their own data against the latest opponent models.    [35,17,3] are also well-used for a fully competitive game, Hold'em Poker, whose action space is generally designed at the scale of tens (fold, call, check and kinds of bet) and the legal actions at each decision point are even less [17,33]. In addition, CFR does not proven to converged to an Nash equilibrium in games with more than two players [14]. Consequently, they can be hardly designed for DouDizhu due to the large action space with difficult abstraction and the mixed game property (both cooperative and competitive, although there are few success cases for such a setting [16]).\n\n\nDistributed Training Details\n\n\nRLCard\n\nDouDizhu AI systems. Besides the recent SoTA work DouZero [31], many researchers have made efforts on utilizing the power of RL into solving DouDizhu. However, simply applying RL algorithms such as DQN and A3C into the game can hardly make benefits [29]. Therefore, You et al. [29] proposed Combinational Q-Network (CQN) that reduces the action space by heuristics action decoupling; moreover, DeltaDou [10] utilized Monte-Carlo Tree Search (MCTS) for DouDizhu, along with Bayesian inference for the hidden information and a pre-trained kicker network for action abstraction. DeltaDou was also reported as reaching human-level performance. Zhang et al. [32] also relied on MCTS with predicting other players' actions. Recently, Zhao et al. [34] similarly proposed to model opponents' actions and train the policy model based on the DouZero architecture, reaching limited improvements. Nevertheless, as shown in this paper, we can instead distill such perfect-information knowledge like opponents' hand cards to the policy in a perfect-trainingimperfect-execution style and reach a better performance.\n\n\nExperiments\n\nWe conduct comprehensive experiments to investigate the following research questions. RQ1: How good is PerfectDou against SoTA DouDizhu AI? RQ2: What are the key ingredients of PerfectDou? RQ3: How is the inference efficiency of PerfectDou? To answer RQ1, we empirically evaluate the performance against existing DouDizhu programs. Regarding RQ2, we conduct ablation studies on key components in our design. And for RQ3, we calculate the average inference time for all algorithms involved. Finally, we conduct in-depth analysis, provide interesting case studies of PerfectDou. In the appendix, we report more results including a battle against skilled human players. \n\n\nRule-Based Algorithms:\n\nIncluding the open-source heuristic-based program RHCP-v2 [10,31], the rule model in RLCard and a Random program with uniform legal moves. For evaluation, we directly take their public (or provided) codes and pre-trained models.\n\nMetrics. The performance of DouDizhu are mainly quantified following the same metrics in previous researches [10,31]. Specifically, given two algorithms A against B, we calculate: 1) WP (Winning Percentage): The proportion of winning by A in a number of games. 2) ADP (Average Difference in Points): The per-game averaged difference of scores between A and B. In other words, positive ADP means gaining scores while the negative represents losing it. This is a more reasonable metric for evaluating DouDizhu AI systems, because in real games players are evaluated by the scores obtained instead of their winning rates, as further discussed in Appendix B.2.\n\nIn our experiments, we choose ADP as the basic reward for all experiments training PerfectDou, which is augmented with the proposed reward signal in Section 4.4 during the training stage.\n\n\nComparative Evaluations\n\nWe conduct a tournament to demonstrate the advantage of our PerfectDou, where each pair of the algorithms play 10,000 decks, shown in Tab. 2 (RQ1). Since the bidding performance in each algorithm varies and poor bidding would affect game results significantly, for fair comparison, we omit the bidding phase and focus on the phase of cardplay. In detail, all games are randomly generated and each game would be played two times, i.e., each competing algorithm is assigned as Landlord or Peasant once. We use WP and ADP as the basic reward respectively for comparing over these two metrics for all evaluating methods.\n\nOverall, PerfectDou dominates the leaderboard by beating all the existing AI programs, no matter rule-based or learning based algorithms, with significant advantage on both WP and ADP. Specifically, as noted that DouDizhu has a large variance where the initial hand cards can seriously determine the advantage of the game; even though, PerfectDou still consistently outperforms the current SoTA baseline -DouZero. However, we find that PerfectDou is worse than the result published in DouZero paper [31] when competing against RHCP. To verify this problem, we test the public model of DouZero (denoted as DouZero (Public) with grey color). To our surprise, its performance can match most of the reported results in their paper except the one against RHCP, where it only takes a WP of 0.452 lower than 0.5, indicating that the public model of DouZero can not beat RHCP as suggested in the original paper, and in fact PerfectDou is the better one.\n\nIt is also observed that some competition outcome has a high WP and a negative ADP. A potential reason can be explained as such agents are reckless to play out the bigger cards without considering the left hand, leading to winning many games of low score, but losing high score in the other games when their hand cards are not good enough. From our statistics of online human matches, the WP of winner is usually in a range of 0.52 \u223c 0.55 when the player tries to maximize its ADP. , which is even better than the 1e10 sample-trained DouZero. This indicates that PerfectDou is not only the best performance but also the most training efficient. The related training curves are shown in Appendix D.5.\n\n\nAblation Studies\n\nWe want to further investigate the key to the success of our AI system (RQ2). Specifically, we would like to analyse how our design of the feature and the training framework help PerfectDou dominate the tournament of DouDizhu.\n\nTo this end, we evalu-ate different variants of PerfectDou and the previous SoTA AI system -DouZero, including: a) ImperfectDouZero 4 : DouZero with our proposed imperfect-information features. b) ImperfectDou: PerfectDou with only imperfect-information features as inputs for the value function. c) RewardlessDou: PerfectDou without node reward. d) Vanilla PPO: Naive actor-critic training with imperfect-information features only and without additional reward. The ablation experiments are designed as competitions among Im-perfectDou, RewardlessDou and Per-fectDou against DouZero for comparing the effectiveness of perfect information distillation and perfect intermediate reward separately; while the battle of ImperfectDouZero and DouZero against PerfectDou are designed for excluding the benefit from feature engineering. Results for all comparisons are shown in Tab. 4. Even with the imperfect features only, ImperfectDou can still easily beat DouZero with the same training steps; however, DouZero turns the tide with much more training data. Furthermore, our proposed node features seem not appropriate for DouZero to achieve a better results compared with its original design. Additionally, without the node reward, PerfectDou still beats DouZero with higher WP (in spite of sacrificing a lot of ADP), indicating the effectiveness of perfect reward in training, without which it would risk losing points to win one game. Finally, without both node reward and perfect feature design for the value function, vanilla PPO simply can not perform well. Therefore, we can conclude that our actor-critic based algorithm along with the PTIE training provides a high sample efficiency under our feature design, and the node reward benefits the rationality of our AI. We further conduct runtime analysis to show the efficiency of PerfectDou w.r.t. the inference time (RQ3), which is reported in Fig. 5. All evaluations are conducted on a single core of Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz. The inference time of each AI could be attributed to its pipeline and implementation in the playing time. CQN uses a large Q network (nearly 10\u00d7 parameters larger than ours) with a complex card decomposer to derive reasonable hands. As a result, the inference time of CQN is the longest. Besides, both DeltaDou and RHCP-V2 contain lots of times of Monte Carlo simulations, thus slowing down the inference time. As comparisons, DouZero and Perfect-Dou only require one network forward inference time with a similar number of parameters. For RLCard, only handcraft rules are computed. Therefore, we can notice that PerfectDou is significantly faster than previous programs like DeltaDou, CQN and RHCP, yet is slightly slower than DouZero. To be more accurate, the average inference time of DouZero is 2 milliseconds compared with 6 milliseconds of PerfectDou. And the reason why PerfectDou is a bit slower than DouZero may due to the more complex feature processing procedure. Note that this can be further optimized in practice such as changing the implementation from Python to C++, which is common for AI deployment. In addition, the model inference time of DouZero we test is 1.9 ms while PerfectDou is 4 ms, the difference is slight. The above analysis suggests that PerfectDou is applicable and affordable to real-world applications such as advanced game AI.\n\n\nRuntime Analysis\n\n\nIn-depth Analysis and Case Studies\n\nIn our experiments, we find that DouZero is leaky and unreasonable in many battle scenarios, while PerfectDou performs better therein. To quantitatively evaluate whether PerfectDou is stronger and more reasonable, we conduct an in-depth analysis by collecting the statistics among the games, and additionally illustrate some of the observations for qualitatively comparing the behavior of DouZero and PerfectDou. Detailed results can be further referred to Appendix D.2 and Appendix D.3, and here we list the key conclusions below:\n\nThe statistic analysis claims the rationality of PerfectDou: (i) when playing as the Landlord, Perfect-Dou plays fewer bombs to avoid losing scores and tends to control the game even when the Peasants play more bombs; (ii) when playing as the Peasant, two PerfectDou agents cooperate better with more bombs to reduce the control time of the Landlord and its chance to play bombs; (iii) when playing as the Peasant, the right-side Peasant agent (play after the Landlord) of PerfectDou throws more bombs to suppress the Landlord than DouZero, which is more like human strategy. From behavior observations, we also find: 1) DouZero is more aggressive but less thinking. 2) PerfectDou is better at guessing and suppressing. 3) PerfectDou is better at card combination. 4) PerfectDou is more calm. Beyond these, we also include battle results against skilled human players, additional training results and complete tournament results in the appendix.\n\n\nConclusion\n\nIn this paper, we propose PerfectDou, a SoTA DouDizhu AI system that dominates the game. PerfectDou takes the advantage of the perfect-training-imperfection-execution (PTIE) training paradigm, and is trained within a distributed training framework. In experiments we extensively investigate how and why PefectDou can achieve the SoTA performance by beating all existing AI programs with reasonable strategic actions. In fact, the PTIE paradigm is actually a variant of centralized-training-decentralized-execution (CTDE), applied for imperfect-information games in particular. Intuitively, PTIE is a general way for training imperfect-information game AI, with which we expect the value function can distill the perfect information to the policy which can only receive imperfect information. Although in this paper we only discuss its success on one of the hard poker games, DouDizhu, we believe it has the power to further improve the ability for other imperfectinformation games, remaining a space of imagination to be explored by more future works.  \n\n\nAppendix A Additional Related Work\n\nTrain with global information, test with local one. Utilizing global information to reduce the complexity of imperfect-information games has also been investigated in some works. For example, AlphaStar [26], a grand-master level AI system for StarCraft II. In their implementation, the value network of the agent can observe the full information about the game state, including those that are hidden from the policy. They argue that such a training style improves training performance. In our work, we formulate the idea as Perfect-Training-Imperfect-Execution (PTIE) or perfect information distillation technique for imperfect-information games, and show the effectiveness on complicated card games like DouDizhu. Moreover, in Suphx [15], a strong Mahjong AI system, they used a similar method namely oracle guiding. Particularly, in the beginning of the training stage, all global information is utilized; then, as the training goes, the additional information would be dropped out slowly to none, and only the information that the agent is allowed to observe is reserved in the subsequent training stage. However, there are obvious difference between Suphx and PerfectDou.\n\nIn Suphx, the perfect information is used by the actor and thus has to be dropped before the inference stage; on the contrary, PerfectDou feeds the critic with additional observations and distill the global information to the actor. Beyond games, Fang et al.\n\n[6] worked on trading for order execution and proposed a different technique using global information other than PTIE, which trained a student policy with imperfect (real) market information and policy distillation from a teacher policy trained with perfect (oracle) market information.\n\nRelation to sample-based CFR. CFR aims to minimize the total regret of policy by minimizing the cumulative counterfactual regret in each infoset. The definition of regret highly relates to the definition of advantage used in RL community, which has been shown in lots of previous works [23,7]. Vanilla CFR [35] and many variants [27,2] apply model-based approach to calculated all the weights of the game tree to update and obtain a good strategy (policy). However, when the game has long episodes and is hard for searching across the game tree, it is necessary to compute through trajectory samples, called sampled-based CFR methods [24,8]. This resembles the learning procedure of RL algorithms. Recently, Fu et al. [7] proposed a new form of sample-based CFR algorithm, and shown that PPO is exactly a practical implementation of it (but not PTIE), revealing close connections between CFR and RL.\n\n\nB More About DouDizhu\n\n\nB.1 Term of Categories\n\nIn the work of Zha et al. [31], they had shown a comprehensive introduction of DouDizhu game, so we think it may be wordy to repeat the stereotyped rules. However, for better understanding the cases shown in this paper, we introduce the typical term of categories in DouDizhu that are commonly used as follows. Note that all cards can suppress the cards in the same category with a higher rank, yet bomb can suppress any categories except the bomb with a higher rank. Rocket is the highest-rank bomb. Kicker refer to the unrelated or useless cards that players can deal out when playing some kind of categories of main cards (see below), which can be either a solo or a pair. 10. Quad with Solo : Four-of-a-kind with two Solos as the kicker and Four-of-a-kind as the main cards.\n\n11. Quad with Pair : Four-of-a-kind with a set of Pair as the kicker and Quad with Pair as the main cards.\n\n12. Quad with Pairs : Four-of-a-kind with two sets of Pair as the kicker and Quad with Pairs as the main cards.\n\n13. Bomb : Four-of-a-kind.\n\n14. Rocket : Red and black jokers.\n\n\nB.2 Scoring Rules\n\nIn Zha et al. [31], they pay more attention to the win/lose result of the game but care less about the score. However, in real competitions, players must play for numbers of games and are ranked by the scores they win. And that is why we think ADP is a better metric for evaluating DouDizhu AI systems because a bad AI player can win a game with few scores but lose with much more scores.\n\nSpecifically, in each game, the Landlord and the Peasants have base scores of 2 and 1 respectively. When there is a bomb shown in a game, the score of each player doubles. For example, a Peasant player first shows a bomb of 4 and then the Landlord player suppresses it with a rocket, then the base score of each Peasant becomes 4 and the Landlord becomes 8. A player will win all his scores after winning the game, or loses all of them vice versa.\n\n\nC Additional System Design Details\n\n\nC.1 Card Representation Details\n\nIn the system of PerfectDou, we augment the basic card in hand matrix with explicitly encoded card types as additional features, in order to allow the agent realizing the different properties of different kind of cards. The size details are shown in Tab. 5.   The action features are a flatten matrix from 12 \u00d7 15 action card matrix plus 1 \u00d7 6 extra dimensions describing the property of the cards as shown in Tab. 6. Since the number of actions in each game state varies, which can lead to different lengths of action features, a fixed length matrix is flattened to store all action features where the non-available ones are marked as zero.\n\n\nC.2 Action Feature Details\n\n\nC.3 Value Network Structure\n\nThe value network of PerfectDou is designed to evaluate the current situation of players, and we expect that the value function can utilize the global information, in other words, know the exact node the player is in. Therefore, we should feed additional information that the policy is not allowed to see in our design. Specifically, as shown in Fig. 6, the imperfect feature for indistinguishable nodes is encoded using the shared network as in the policy network; besides, we also encode the perfect feature of distinguishable nodes that the policy cannot observe during its game playing. The encoded vector are then concatenated to a simple MLP to get the scalar value output. \n\n\nD Experiments\n\n\nD.1 Setups, Hyperparameters and Training Details\n\nIn our implementation, a small distributed training cluster is built using 880 CPUs cores and 8 GPUs.\n\nHorovod [20] is used to synchronize gradients between GPUs, the total batch size is 1024, 128 for each GPU. The most important hyperparameters in our experiment are shown in Tab. 7. Specifically, in our design, we simplify the discrete action space from 27472 (include all possible combinations) into an abstract action space of 621 for learning the actor, followed by an decoding strategy to get the final action (see Appendix E.2 for more details).\n\nDuring self-play training, we find a better practical solution for DouDizhu is to keep three different models for Landlord and two Peasants separately which is only updated by their own data against the latest opponent model. In the main training stage, the total reward function will be a basic reward (in this paper we use ADP all the time) augmented with the designed oracle reward as shown in Section 4.4, which is found to be extremely useful for accelerating convergence. In the final stage, the oracle reward is removed and only the ADP reward is used to fine-tune the model for reaching a better performance measured by the ADP metric.\n\n\nD.2 In-Depth Statistical Analysis\n\nIn our experiments, we find that DouZero is leaky and unreasonable in many battle scenarios, while PerfectDou performs better therein. To quantitatively evaluate whether PerfectDou is stronger and more reasonable, we conduct an in-depth analysis and collect the statistics among the games between DouZero and PerfectDou. Particularly, we organize games between PerfectDou and Douzero to play in different roles for 100,000 decks in each setting. Since the roles are assigned randomly instead of opting by agents themselves in our experiments, and the Landlord has a higher base score with three extra cards, we observe that playing as a Landlord is always harder to win and leads to negative ADPs. From the statistics shown in Tab. 8, we learn many lessons about the rationality of PerfectDou: (i) when playing as the Landlord, PerfectDou plays fewer bombs to avoid losing  scores and tends to control the game even the Peasants play more bombs; (ii) when playing as the Peasant, two PerfectDou agents cooperate better with more bombs to reduce the control time of the Landlord and its chance to play bombs; (iii) when playing as the Peasant, the right-side Peasant agent (play after the Landlord) of PerfectDou throws more bombs to suppress the Landlord than DouZero, which is more like human strategy.\n\n\nD.3 Case Study: Behavior of DouZero vs PerfectDou\n\nIn this section, we list some of the observations during the games for comparing the behavior of DouZero and PerfectDou to qualitatively support our analysis.\n\nDouZero is more aggressive but less thinking. The first observation is that DouZero is extremely aggressive without considering the left hands. For instance, as shown in Fig. 7(a), in the beginning DouZero chooses a chain of solo but leaves the pair of 3, which can be dangerous since the pair of 3 is the one of the minimum cards and cannot suppress any card; Fig. 7(b) illustrates another strong case, where DouZero also chooses a chain of solo to suppress the opponent without considering the consequence of leaving a hand of solos. On the contrary, PerfectDou is more conservative and steady. We believe the proposed perfect information distillation mechanism helps PerfectDou to infer global information in a more reasonable way.\n\nPerfectDou is better at guessing and suppressing. We observe another fact that the usage of perfect information distillation within the PTIE framework benefits PerfectDou a lot by suppressing the opponents in advance. In Fig. 7(c) shows a case when the teammate puts a pair of T 5 , DouZero chooses to pass; on the contrary, PerfectDou chooses suppressing by a pair of Q -the minimal pair of the Landlord.\n\nPerfectDou is better at card combination. In the battle shown in Fig. 7(d)  takes the trio, which will be easily suppressed by the opponent. This benefits from the proper design of the card representation and the action feature of PerfectDou.\n\nPerfectDou is more calm. Fig. 7(e) depicts a typical and interesting scenario where PerfectDou shows its calm and careful consideration over the whole. In the game, the last hand is of the Landlord with a solo 2, and it only has 8 cards left in the hand. DouZero seems afraid and splits the rocket bomb; however, PerfectDou benefits from the advantage reward design and is calm considering there is a greater chance on winning the game with a higher score by keeping the bomb.\n\n\nD.4 Battle Results Against Skilled Human Players\n\nWe further invite some skilled human players to play against PerfectDou. Particularly, each human player plays with two AI players. In other words, each game is involved with either two AI Peasants against one human Landlord, or one AI Peasant cooperating with one human Peasant against one AI Landlord. The results are shown in Tab. 9. One can easily observe that PerfectDou takes evident advantage during the game.  Fig. 8 shows the learning curves of WP and ADP against DouZero for PerfectDou with a single run, and every evaluation contains 10,000 decks. As shown in the figure, PerfectDou can easily beat DouZero (on WP) without considering the scores (ADP) at the beginning of the training; but after 1.5e9 steps of training, PerfectDou is able to fully beat DouZero (both WP and ADP).\n\n\nD.6 Extended Competition against DouZero\n\nCycling Different Hands for the Landlord. In our main competition conducted in Section 6.2, all games are randomly generated and played twice with the same assigned hand cards for the Landlord, once each algorithm controlling the Landlord and once two Peasants. In this section, we test cycling the 3 hand cards of every randomly generated game for the Landlord, and test PerfectDou against DouZero by controlling the Landlord separately, which leads to 6 times of battle. In this situation, we obtain the results as follows showing that PerfectDou has consistent advantage (Tab. 10).  We report the complete tournament results of ADP and WP for Landlord and Peasants in Tab. 12 and Tab. 13. PerfectDou tends to have more advantage of Peasants than that of Landlord, especially when competes against stronger baselines. We believe that the proposed perfect information distillation technique allows for better cooperation between two Peasants. In addition, since the roles are assigned instead of opting according to hand in our competition, and the Landlord has extra three cards and can lose a higher base score, the Peasants seems having more chance to win the game. Therefore, almost all methods can play better results as a Peasant than that as a Landlord.  In our paper, as mentioned in Section 4.4, we utilize an oracle for evaluating the minimum steps to play out all cards. Particularly, the oracle is implemented by a dynamic programming algorithm combined with depth-first-search, which can be referred to https://www.cnblogs.com/ SYCstudio/p/7628971.html (which is also a competition problem of National Olympiad in Informatics in Provinces (NOIP) 2015). For completeness, we summarize the pseudocode for implementing such an algorithm in Algo. 1.\n\n\nE.2 Detailed Action Space\n\nIn our paper, we utilize a simplified discrete action space of 621 for learning the actor, since we observe that the original action space of 27472 (include all possible combinations) contains a large number of actions that can be abstract. For instance, actions like Bomb with kickers and Trio with kickers occupy the action space most due to the large number of combinations of kickers. To this end, we abstract actions with the same main cards into one action, and significantly reduce the action space. When the policy chooses an abstract action, we further deploy a simple decoding function to obtain the most preferred action in the original action space, as illustrated in Algo. 2. We note that this part is similar to an old implementation of RLCard [30]: https://github.com/ datamllab/rlcard/blob/d100952f144e4b0fd7186cc06e79ef277cda9722/ rlcard/envs/doudizhu.py#L67.\n\nBelow we list all the 621 discrete actions of PerfectDou in 15 categories, where the notation * in category (9) (10) (11) and (12) denotes the kicker. for action \u2208 {Solo, Pair, Trio, Bomb, Chain-of-Trio, Trio-with-Pair, Quad-with-Solos, Quad-with-Pair, Quad-with-Pairs} do if Rocket \u2208 Cards then 23: lef t cards \u2190 left cards after playing out Rocket 24: return min( NowStep ( lef t cards ) + 1, NowStep ( lef t cards ) + 2 ) 25: end if 26: if only one Joker \u2208 Cards then 27: lef t cards \u2190 left cards after playing out Joker 28: return NowStep ( lef t cards ) + 1 29: end if 30: if no Joker \u2208 Cards then \n\nFigure 1 :\n1Overview of perfect information distillation within a perfect-trainingimperfect-execution framework.The value network takes additional information (such as other players' cards in Poker games) as input, while the policy network does not.\n\nFigure 2 :\n2Card representation matrix. Columns stand for 15 different card ranks and rows stand for correspondingly designed features. The first 4 rows are the same as Zha et al.[31], and the last 8 rows are additional design for encoding the legal combination of cards.\n\nFigure 3 :\n3The policy network structure of PerfectDou system. The network predicts the action distribution given the current imperfect information of the game, including state information and available actions feature.\n\nFigure 4 :\n4Illustration of the distributed training system.\n\nFigure 5 :\n5Comparison of the inference time.\n\n[\n\n\n1 IF\n1THIS ACTION EQUALS THE NUMBER OF LEFT PLAYER'S CARDS IN HAND 1 IF THIS ACTION EQUALS THE NUMBER OF RIGHT PLAYER'S CARDS IN HAND 1 THE MINIMUM STEPS TO PLAY-OUT ALL LEFT CARDS AFTER THIS ACTION PLAYED1\n\nFigure 6 :\n6The value network structure of the proposed PerfectDou system. The network predicts values using both the imperfect feature and the perfect feature and distill the knowledge into the policy in the training.\n\n\n, PerfectDou shows the better ability on the strategy of card combination. Specifically, PerfectDou chooses to split the plane (999, T T T since it considers there is a chain of solo (9T JQK) left. However, DouZero only Case study: DouZero is more aggressive by choosing a chain of solo in the beginning but leaves the pair of 3 in the hand. Case study: DouZero is more aggressive by suppressing the Landlord but less thinking on the consequence of the left hands of solos. Case study: the teammate shows a pair of T and DouZero chooses to pass; on the contrary, PerfectDou chooses suppressing by a pair of Q -the minimal pair of the opponent. Case study: PerfectDou chooses to split the plane (999, T T T ) since it considers there is a chain of solo (9T JQK) left. Case study: DouZero splits the rocket bomb while PerfectDou chooses to keep it.\n\nFigure 7 :\n7Case studies.\n\nFF\n\u2190 the number of Solo card in action 9: d 2 \u2190 the number of Pair card in action 10: d 3 \u2190 the number of Trio card in action 11: d 4 \u2190 the number of Bomb card in action 12:F [N 1 , N 2 , N 3 , N 4 ] \u2190 min(F [N 1 , N 2 , N 3 , N 4 ], F [N 1 \u2212 d 1 , N 2 \u2212 d 2 , N 3 \u2212 d 3 , [N 1 , N 2 , N 3 , N 4 ] \u2190 min(F [N 1 , N 2 , N 3 , N 4 ], F [N 1 + 1, N 2 + 2, N 3 \u2212 1, N 4 ]) [N 1 , N 2 , N 3 , N 4 ] \u2190 min(F [N 1 , N 2 , N 3 , N 4 ], F [N 1 + 1, N 2 + 2, N 3 \u2212 1, N 4 ])\n\nfor\nof Solo N 1 , Pair N 2 , Trio N 3 and Bomb N 4 of Cards 32: return F [N 1 , N 2 , N 3 , N 4 ] \u2190 min( step, NowStep(Cards) ) 40: for Chain-of-Solo \u2208 Cards do 41: lef t cards \u2190 left cards after playing out Chain-of-Solo 42: DFS(step + 1, ans, lef t cards of Cards) Chain-of-Pair in Cards do 45: lef t cards \u2190 left cards after playing out Chain-of-Pair 46: DFS(step + 1, ans, lef t cards of Cards) matrix F of size [N 1 , N 2 , N 3 , N 4 ] 54: InitMatrix(F ) 55: step \u2190 0 ,ans \u2190 +\u221e, Cards \u2190 all Cards to be calculated 56: DFS( 0, ans, Cards ) 57: end function\n\nTable 1 :\n1Feature design of perfect-information (distinguishable nodes) and imperfect-information (indistinguishable nodes) for the game. Perfect features include all imperfect features.FEATURE DESIGN \nSIZE \n\nIMPERFECT \nFEATURE \n\nCURRENT PLAYER'S HAND \n\n1 \u00d7 12 \u00d7 15 \n\nUNPLAYED CARDS \n\n1 \u00d7 12 \u00d7 15 \n\nCURRENT PLAYER'S PLAYED CARDS \n\n1 \u00d7 12 \u00d7 15 \n\nPREVIOUS PLAYER'S PLAYED CARDS \n\n1 \u00d7 12 \u00d7 15 \n\nNEXT PLAYER'S PLAYED CARDS \n\n1 \u00d7 12 \u00d7 15 \n3 ADDITIONAL BOTTOM CARDS \n1 \u00d7 12 \u00d7 15 \nLAST 15 MOVES \n15 \u00d7 12 \u00d7 15 \n\nPREVIOUS PLAYER'S LAST MOVE \n\n1 \u00d7 12 \u00d7 15 \n\nNEXT PLAYER'S LAST MOVE \n\n1 \u00d7 12 \u00d7 15 \n\nMINIMUM PLAY-OUT STEPS OF HAND CARDS \n\n1 \n\nNUMBER OF CARDS IN CURRENT PLAYER'S HAND \n\n1 \n\nNUMBER OF CARDS IN PREVIOUS PLAYER'S HAND \n\n1 \n\nNUMBER OF CARDS IN NEXT PLAYER'S HAND \n\n1 \n\nNUMBER OF BOMBS \n\n1 \n\nFLAG OF GAME CONTROL BY CURRENT PLAYER \n\n1 \n\nADDITIONAL \nPERFECT \nFEATURE \n\nPREVIOUS PLAYER'S HAND CARDS \n\n1 \u00d7 12 \u00d7 15 \n\nNEXT PLAYER'S HAND CARDS \n\n1 \u00d7 12 \u00d7 15 \n\nMINIMUM PLAY-OUT STEPS OF PREVIOUS PLAYER'S HAND CARDS \n\n1 \n\nMINIMUM PLAY-OUT STEPS OF NEXT PLAYER'S HAND CARDS \n\n1 \n\n\n\nTable 2 :\n2DouDizhu tournaments for existing AI programs by playing 10k randomly generated decks. Player A outperforms B if WP is larger than 0.5 or ADP is larger than 0 (highlighted in boldface). The algorithms are ranked according to the number of the other algorithms that they beat. We note that DouZero is the current SoTA DouDizhu bot and the gray rows highlight the comparison. Numerical results except marked * are directly borrowed from Zha et al.[31].Imperfect-Information Games. Many popular card games are imperfect-information games and have attracted much attention. For instance, Li et al.[15]  worked on the four-player game Mahjong and proposed a distributed RL algorithm combined with techniques like global reward prediction, oracle guiding, and run-time policy adaptation to win against most top human players; in addition, Lerer et al.[13]  adopted search-based and imitation methods to learn the playing policy for Hanabi. Iterative algorithms such as Counterfactual Regret Minimization (CFR) and its variantsRank A \nB PerfectDou \nDouZero \nDeltaDou \nRHCP-v2 \nCQN \nRandom \nWP \nADP \nWP \nADP \nWP \nADP \nWP \nADP \nWP ADP \nWP ADP \n\n1 \nPerfectDou (Ours) \n-\n-\n0.543  *  0.143  *  0.584  *  0.420  *  0.543  *  0.506  *  0.862  *  2.090  *  0.994  *  3.146  *  \n2 \nDouZero (Paper) \n-\n-\n-\n-\n0.586 0.258 0.764 1.671 0.810 1.685 0.989 3.036 \n-\nDouZero (Public) 0.457  *  -0.143  *  \n-\n-\n0.585  *  0.253  *  0.451  *  0.060  *  0.828  *  1.950  *  0.986  *  3.050  *  \n3 \nDeltaDou \n0.416  *  -0.420  *  0.414 -0.258 \n-\n-\n0.691  *  1.528  *  0.784 1.534 0.992 3.099 \n4 \nRHCP-v2 \n0.457  *  -0.506  *  0.549  *  -0.060  *  0.309  *  -1.423  *  \n-\n-\n0.770  *  1.414  *  0.990  *  2.670  *  \n5 \nCQN \n0.138  *  -2.090  *  0.190 -1.685 0.216 -1.534 0.230  *  -1.414  *  \n-\n-\n0.889 1.912 \n6 \nRandom \n0.006  *  -3.146  *  0.011 -3.036 0.008 -3.099 0.010  *  -2.670  *  0.111 -1.912 \n-\n-\n\n5 Related Work \n\n\n\n\n6.1 Experimental Setups Baselines. We evaluate PerfectDou against the following algorithms under the open-source RL-Card Environment [30]: 1) DouZero [31]: A recent SoTA baseline method that had beaten every existing DouDizhu AI system using Deep Monte-Carlo algorithm. 2) DeltaDou [10]: An MCTSbased algorithm with Beyesian inference. It achieved comparable performance as human experts. 3)Combinational Q-Network (CQN) [29]: Based on card decomposition and Deep Q-Learning. 4) \n\n\nTable 3 :\n3Training efficiency comparison over 100k decks.A \n\nB DouZero (\u223c1e9) DouZero (\u223c1e10) \nWP \nADP \nWP \nADP \n\nPerfectDou (2.5e9) \n-\n-\n0.541 \n0.130 \nPerfectDou (1e9) \n0.732 \n1.270 \n0.524 \n0.014 \nDouZero (\u223c1e10) \n0.698 \n1.150 \n-\n-\n\n\n\nTable 4 :\n4Ablation studies over 100k decks.A \n\nB DouZero (\u223c1e9) DouZero (\u223c1e10) ImperfectDouZero (\u223c1e9) \nWP \nADP \nWP \nADP \nWP \nADP \n\nPerfectDou (1e9) \n0.732 \n1.270 \n0.524 \n0.014 \n0.731 \n1.350 \nImperfectDou (1e9) \n0.717 \n1.180 \n0.486 \n-0.057 \n0.723 \n1.320 \nRewardlessDou (1e9) \n0.738 \n0.490 \n0.540 \n-0.201 \n0.659 \n0.587 \nVanilla PPO(1e9) \n0.509 \n-0.307 \n0.346 \n-0.709 \n0.433 \n-0.023 \n\n\n\n\n4] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.In International Conference on Machine \nLearning, pages 1407-1416. PMLR, 2018. \n\n[5] Jakob N. F., Yannis M. A., Nando de F., and Shimon W. Learning to communicate with deep \nmulti-agent reinforcement learning. In Advances in Neural Information Processing Systems, \npages 2137-2145, 2016. \n\n[6] Yuchen Fang, Kan Ren, Weiqing Liu, Dong Zhou, Weinan Zhang, Jiang Bian, Yong Yu, and \nTie-Yan Liu. Universal trading for order execution with oracle policy distillation. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, volume 35, pages 107-115, 2021. \n\n\n\nPlane with Solo: Two or more consecutive Trios with each has a distinct individual kicker card and Plane as the main cards.1. Solo : Any single card. \n2. Pair : Two matching cards of equal rank. \n3. Trio : Three individual cards of equal rank. \n4. Trio with Solo : Three individual cards of equal rank with a Solo as the kicker. \n5. Trio with Pair : Three individual cards of equal rank with a Pair as the kicker. \n6. Chain of Solo : Five or more consecutive individual cards. \n7. Chain of Pair : Three or more consecutive Pairs. \n8. Chain of Trio (Plane) : Two or more consecutive Trios. \n9. \n\nTable 5 :\n5Card representation design.CARD MATRIX FEATURE \nSIZE \n\nCARD IN HAND \n\n4 \u00d7 15 \n\nSOLO \n\n1 \u00d7 15 \n\nPAIR \n\n1 \u00d7 15 \n\nTRIO \n\n1 \u00d7 15 \n\nBOMB \n\n1 \u00d7 15 \n\nROCKET \n\n1 \u00d7 15 \n\nCHAIN OF SOLO \n\n1 \u00d7 15 \n\nCHAIN OF PAIR \n\n1 \u00d7 15 \n\nCHAIN OF TRIO \n\n1 \u00d7 15 \n\n\n\nTable 6 :\n6Action feature design.FEATURE DESIGN \nSIZE \n\nCARD MATRIX OF ACTION \n\n12 \u00d7 15 \n\nIF THIS ACTION IS BOMB \n\n1 \n\nIF THIS ACTION IS THE LARGEST ONE \n\n\n\nTable 7 :\n7Hyperparameters. * refers to the maximum version gap allowed between the models used for sampling and training.Learning rate \n3e-4 \nOptimizer \nAdam \nDiscount factor \u03b3 \n1.0 \n\u03bb of GAE \n0.95 \nStep of GAE \n24 (8 for each player) \nBatch size \n1024 \nEntropy weight of PPO \n0.1 \nLength of LSTM \n15 (5 for each player) \nMax model lag  *  \n1 \nIntermediate reward scale \n50 \nPolicy MLP hidden sizes \n[256, 256, 256, 512] \nPolicy MLP output size (action space size) \n621 \nValue MLP hidden sizes \n[256, 256, 256, 256] \nValue MLP output size \n1 \n\n\n\nTable 8 :\n8Average per game statistics of important behaviors over 100k decks: Game Len is the average number of rounds in a game; % Bomb represents the average percentage of bombs (a type of card can suppress any categories except the bomb with a higher rank, see Appendix B) played in the game; Left and Right are the relative position to the Landlord; and Landlord Control Time measures the number of rounds that the landlord plays an action suppressing all other players. Game Len %Bomb of Left Peasant %Bomb of Landlord %Bomb of Right Peasant Landlord Control TimeLandlord Agent \nWP \nADP \n\nTable 9 :\n9Battle results against skilled human players for 1260 episodes of game.Figure 8: Learning curves of WP and ADP against the final model of DouZero w.r.t. timesteps for PerfectDou. Every evaluation contains 10,000 decks. PerfectDou is able to beat DouZero without considering the scores at the beginning of the training, around 1.5e6 steps.A \n\nB Skilled human \nWP \nADP \n\nPerfectDou (2.5e9) \n0.625 0.590 \n\nD.5 Additional Training Results \n\n0.0 \n0.5 \n1.0 \n1.5 \n2.0 \n2.5 \n1e9 \n\n\u22121.00 \n\n\u22120.75 \n\n\u22120.50 \n\n\u22120.25 \n\n0.00 \n\n0.25 \n\n0.50 \n\nWP \nADP \n\n\n\nTable 10 :\n10Results of cycling different hands for Landlord by playing 100k decks.Peasants Paired with Different AIs. We also include an interesting battle by pairing the peasants with different algorithms. Specifically, DouZero or PerfectDou plays the Landlord while one Peasant is assigned with DouZero and the other Peasant is played by PerfectDou. The results are concluded in Tab. 11, which reveal that when playing as the peasant, PerfectDou can better cooperate with its teammate than DouZero. And when playing as the landlord, PerfectDou outperforms DouZero against all types of opponents.A \n\nB \nPerfectDou \nDouZero \nWP \nADP \n\nPerfectDou \n-\n-\n0.544 0.150 \nDouZero \n0.456 -0.150 \n-\n-\n\n\n\nTable 11 :\n11Results of battles that the Peasants are paired with different AIs by 100k decks and 6 times of battle per deck. The results are evaluated from the Landlord side. D.7 Complete Tournament Results of ADP for Landlord and PeasantsLandlord \n\nPeasant PerfectDou + DouZero \nPerfectDou \nDouZero \nWP \nADP \nWP \nADP \nWP \nADP \n\nPerfectDou \n0.424 \n-0.448 \n0.389 -0.606 0.452 -0.375 \nDouZero \n0.395 \n-0.534 \n0.363 -0.676 0.421 -0.465 \n\n\n\nTable 12 :\n12ADP results of DouDizhu tournaments for existing AI programs by playing 10k decks. L: ADP of A as Landlord; P: ADP of A as Peasants. Algorithm A outperforms B if the ADP of L or P is larger than 0 (highlighted in boldface). We note that DouZero is the current SoTA DouDizhu bot. Numerical results except marked * are directly borrowed from Zha et al.[31].Ours) 0.656 * -0.656 * 0.686 * -0.407 * 0.980 * -0.145 * 0.872 * 0.138 * 2.020 * 2.160 * 3.008 * 3.283 * 2 DouZero (Public) 0.407 * -0.686 * 0.435 * -0.435 * 0.858 * -0.342 * 0.166 * -0.046 * 2.001 * 1.368 * 2.818 * 3.254 * 3 DeltaDou 0.145 * -0.980 * 0.342 * -0.858 * 0.476 -0.476 1.878 * 0.974 * 1.849 1.218 2.930 3.268 4 RHCP-v2 -0.138 * -0.872 * 0.046 * -0.166 * -0.974 * -1.878 * 0.182 * -0.182 * 1.069 * 1.758 * 2.560 * 2.780 * 5 CQN -2.160 * -2.020 * -1.368 * -2.001 * -1.218 -1.849 -1.758 * -1.069 * 0.056 -0.056 1.992 1.832 6 Random -3.283 * -3.008 * -3.254 * -2.818 * -3.268 -2.930 -2.780 -2.560 * -1.832 -1.991 0.883 -0.883Rank A \nB \nPerfectDou \nDouZero \nDeltaDou \nRHCP-v2 \nCQN \nRandom \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \n\n1 \nPerfectDou (\n\nTable 13 :\n13WP results of DouDizhu tournaments for existing AI programs by playing 10k decks. L: WP of A as Landlord; P: WP of A as Peasants. Algorithm A outperforms B if the WP of L or P is larger than 0.5 (highlighted in boldface). Numerical results except marked * are directly borrowed from Zha et al.[31].Ours) 0.622 * 0.378 * 0.640 * 0.446 * 0.693 * 0.474 * 0.609 * 0.478 * 0.894 * 0.830 * 0.998 * 0.990 * 2 DouZero (Public) 0.554 * 0.360 * 0.584 * 0.416 * 0.684 * 0.487 * 0.427 * 0.475 * 0.851 * 0.769 * 0.992 * 0.986 * 3 DeltaDou 0.526 * 0.307 * 0.513 * 0.317 * 0.588 0.412 0.768 * 0.614 * 0.835 0.733 0.996 0.987 4 RHCP-v2 0.522 * 0.391 * 0.525 * 0.573 * 0.386 * 0.232 * 0.536 * 0.434 * 0.687 * 0.853 * 0.994 * 0.985 * 5 CQN 0.170 * 0.106 * 0.231 * 0.149 * 0.267 0.165 0.147 * 0.313 * 0.476 0.524 0.921 0.857 6 Random 0.010 * 0.002 * 0.014 * 0.008 * 0.013 0.004 0.015 * 0.006 * 0.143 0.080 0.654 0.346 E More Implementation Details E.1 The Oracle for Minimum Steps to Play Out All cardsRank A \nB PerfectDou \nDouZero \nDeltaDou \nRHCP-v2 \nCQN \nRandom \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \nP \nL \n\n1 \nPerfectDou (\nNote that DouZero cannot acquire any perfect-information feature since it will play in a cheating style if so.\nWe denote T (en) as the card 10 for simplicity.\nAcknowledgementThe SJTU team is partially supported by \"New Generation of AI 2030\" Major Project (2018AAA0100900) and National Natural Science Foundation of China (62076161). Minghuan Liu is also supported by Wu Wen Jun Honorary Doctoral Scholarship, AI Institute, SJTU. We thank Ming Zhou for helpful discussions on game theory, and anonymous reviewers for constructive suggestions.A \u2190get all available actions from current hand3:K \u2190get all kickers using the main card M from A4:for k in K do5:calculate score s of each k888999TTT, 999TTTJJJ, TTTJJJQQQ, JJJQQQKKK, QQQKKKAAA, 333444555666,  444555666777, 555666777888, 666777888999, 777888999TTT, 888999TTTJJJ, 999TT-TJJJQQQ, TTTJJJQQQKKK, JJJQQQKKKAAA, 333444555666777, 444555666777888,  555666777888999, 666777888999TTT, 777888999TTTJJJ, 888999TTTJJJQQQ, 999TT-TJJJQQQKKK,  TTTJJJQQQKKKAAA,  333444555666777888,  444555666777888999,  555666777888999TTT,  666777888999TTTJJJ,  777888999TTTJJJQQQ,  888999TT-TJJJQQQKKK, 999TTTJJJQQQKKKAAA\nDota 2 with large scale deep reinforcement learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.06680arXiv preprintChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n\nDeep counterfactual regret minimization. Noam Brown, Adam Lerer, Sam Gross, Tuomas Sandholm, International conference on machine learning. PMLRNoam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret minimization. In International conference on machine learning, pages 793-802. PMLR, 2019.\n\nSuperhuman ai for heads-up no-limit poker: Libratus beats top professionals. Noam Brown, Tuomas Sandholm, Science. 3596374Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418-424, 2018.\n\nActor-critic policy optimization in a large-scale imperfect-information game. Haobo Fu, Weiming Liu, Shuang Wu, Yijia Wang, Tao Yang, Kai Li, Junliang Xing, Bin Li, Bo Ma, Qiang Fu, International Conference on Learning Representations. Haobo Fu, Weiming Liu, Shuang Wu, Yijia Wang, Tao Yang, Kai Li, Junliang Xing, Bin Li, Bo Ma, Qiang Fu, et al. Actor-critic policy optimization in a large-scale imperfect-information game. In International Conference on Learning Representations, 2021.\n\nThe advantage regret-matching actor-critic. Audr\u016bnas Gruslys, Marc Lanctot, R\u00e9mi Munos, Finbarr Timbers, Martin Schmid, Julien Perolat, Dustin Morrill, Vinicius Zambaldi, Jean-Baptiste Lespiau, John Schultz, arXiv:2008.12234arXiv preprintAudr\u016bnas Gruslys, Marc Lanctot, R\u00e9mi Munos, Finbarr Timbers, Martin Schmid, Julien Pero- lat, Dustin Morrill, Vinicius Zambaldi, Jean-Baptiste Lespiau, John Schultz, et al. The advan- tage regret-matching actor-critic. arXiv preprint arXiv:2008.12234, 2020.\n\nDeep reinforcement learning from self-play in imperfectinformation games. Johannes Heinrich, David Silver, arXiv:1603.01121arXiv preprintJohannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect- information games. arXiv preprint arXiv:1603.01121, 2016.\n\nDeltadou: Expert-level doudizhu ai through self-play. Qiqi Jiang, Kuangzheng Li, Boyao Du, Hao Chen, Hai Fang, IJCAI. Qiqi Jiang, Kuangzheng Li, Boyao Du, Hao Chen, and Hai Fang. Deltadou: Expert-level doudizhu ai through self-play. In IJCAI, pages 1265-1271, 2019.\n\nMulti-agent actor-critic for mixed cooperative-competitive environments. L Ryan, W Yi, T Aviv, H Jean, A Pieter, Igor M , Advances in Neural Information Processing Systems. Ryan L., Yi W., Aviv T., Jean H., Pieter A., and Igor M. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Sys- tems, pages 6379-6390, 2017.\n\nA unified game-theoretic approach to multiagent reinforcement learning. Advances in neural information processing systems. Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P\u00e9rolat, David Silver, Thore Graepel, 30Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P\u00e9rolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. Advances in neural information processing systems, 30, 2017.\n\nImproving policies via search in cooperative partially observable games. Adam Lerer, Hengyuan Hu, Jakob Foerster, Noam Brown, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Adam Lerer, Hengyuan Hu, Jakob Foerster, and Noam Brown. Improving policies via search in cooperative partially observable games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7187-7194, 2020.\n\nA survey of nash equilibrium strategy solving based on cfr. Huale Li, Xuan Wang, Fengwei Jia, Yifan Li, Qian Chen, Archives of Computational Methods in Engineering. 284Huale Li, Xuan Wang, Fengwei Jia, Yifan Li, and Qian Chen. A survey of nash equilibrium strategy solving based on cfr. Archives of Computational Methods in Engineering, 28(4):2749- 2760, 2021.\n\nJunjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang, Li Zhao, Tao Qin, Tie-Yan Liu, Hsiao-Wuen Hon, arXiv:2003.13590Suphx: Mastering mahjong with deep reinforcement learning. arXiv preprintJunjie Li, Sotetsu Koyamada, Qiwei Ye, Guoqing Liu, Chao Wang, Ruihan Yang, Li Zhao, Tao Qin, Tie-Yan Liu, and Hsiao-Wuen Hon. Suphx: Mastering mahjong with deep reinforcement learning. arXiv preprint arXiv:2003.13590, 2020.\n\nAutomating collusion detection in sequential games. Parisa Mazrooei, Christopher Archibald, Michael Bowling, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence27Parisa Mazrooei, Christopher Archibald, and Michael Bowling. Automating collusion detec- tion in sequential games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 27, pages 675-682, 2013.\n\nDeepstack: Expert-level artificial intelligence in heads-up no-limit poker. Matej Morav\u010d\u00edk, Martin Schmid, Neil Burch, Viliam Lis\u1ef3, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, Michael Bowling, Science. 3566337Matej Morav\u010d\u00edk, Martin Schmid, Neil Burch, Viliam Lis\u1ef3, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level arti- ficial intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.\n\nHighdimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.02438arXiv preprintJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High- dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nHorovod: fast and easy distributed deep learning in tensorflow. Alexander Sergeev, Mike Del Balso, arXiv:1802.05799arXiv preprintAlexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in tensorflow. arXiv preprint arXiv:1802.05799, 2018.\n\nMastering chess and shogi by self-play with a general reinforcement learning algorithm. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, arXiv:1712.01815arXiv preprintDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.\n\nMastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, nature. 5507676David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354-359, 2017.\n\nActor-critic policy optimization in partially observable multiagent environments. Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien P\u00e9rolat, Karl Tuyls, R\u00e9mi Munos, Michael Bowling, Advances in neural information processing systems. 31Sriram Srinivasan, Marc Lanctot, Vinicius Zambaldi, Julien P\u00e9rolat, Karl Tuyls, R\u00e9mi Munos, and Michael Bowling. Actor-critic policy optimization in partially observable multiagent en- vironments. Advances in neural information processing systems, 31, 2018.\n\nDream: Deep regret minimization with advantage baselines and model-free learning. Eric Steinberger, Adam Lerer, Noam Brown, arXiv:2006.10410arXiv preprintEric Steinberger, Adam Lerer, and Noam Brown. Dream: Deep regret minimization with advantage baselines and model-free learning. arXiv preprint arXiv:2006.10410, 2020.\n\nReinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nGrandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, H David, Richard Choi, Timo Powell, Petko Ewalds, Georgiev, Nature. 5757782Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grand- master level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350- 354, 2019.\n\nSolving games with functional regret estimation. Kevin Waugh, Dustin Morrill, James Andrew Bagnell, Michael Bowling, Twenty-ninth AAAI conference on artificial intelligence. Kevin Waugh, Dustin Morrill, James Andrew Bagnell, and Michael Bowling. Solving games with functional regret estimation. In Twenty-ninth AAAI conference on artificial intelligence, 2015.\n\nMastering complex control in moba games with deep reinforcement learning. Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, Xipeng Wu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 6672-6679, 2020.\n\n. Yang You, Liangwei Li, Baisong Guo, Weiming Wang, Cewu Lu, arXiv:1901.08925Combinational qlearning for dou di zhu. arXiv preprintYang You, Liangwei Li, Baisong Guo, Weiming Wang, and Cewu Lu. Combinational q- learning for dou di zhu. arXiv preprint arXiv:1901.08925, 2019.\n\nRlcard: A toolkit for reinforcement learning in card games. Daochen Zha, Kwei-Herng Lai, Yuanpu Cao, Songyi Huang, Ruzhe Wei, Junyu Guo, Xia Hu, arXiv:1910.04376arXiv preprintDaochen Zha, Kwei-Herng Lai, Yuanpu Cao, Songyi Huang, Ruzhe Wei, Junyu Guo, and Xia Hu. Rlcard: A toolkit for reinforcement learning in card games. arXiv preprint arXiv:1910.04376, 2019.\n\nDouzero: Mastering doudizhu with self-play deep reinforcement learning. Daochen Zha, Jingru Xie, Wenye Ma, Sheng Zhang, Xiangru Lian, Xia Hu, Ji Liu, Proceedings of the 38th International Conference on Machine Learning. Marina Meila and Tong Zhangthe 38th International Conference on Machine LearningPMLR139Daochen Zha, Jingru Xie, Wenye Ma, Sheng Zhang, Xiangru Lian, Xia Hu, and Ji Liu. Douzero: Mastering doudizhu with self-play deep reinforcement learning. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 12333-12344. PMLR, 2021.\n\nCombining tree search and action prediction for state-of-the-art performance in doudizhu. Yunsheng Zhang, Dong Yan, Bei Shi, Haobo Fu, Qiang Fu, Hang Su, Jun Zhu, Ning Chen, IJCAI. Yunsheng Zhang, Dong Yan, Bei Shi, Haobo Fu, Qiang Fu, Hang Su, Jun Zhu, and Ning Chen. Combining tree search and action prediction for state-of-the-art performance in doudizhu. In IJCAI, pages 3413-3419, 2021.\n\nAlphaholdem: Highperformance artificial intelligence for heads-up no-limit texas hold'em from end-to-end reinforcement learning. Enmin Zhao, Renye Yan, Jinqiu Li, Kai Li, Junliang Xing, Enmin Zhao, Renye Yan, Jinqiu Li, Kai Li, and Junliang Xing. Alphaholdem: High- performance artificial intelligence for heads-up no-limit texas hold'em from end-to-end re- inforcement learning. 2022.\n\nDouzero+: Improving doudizhu ai by opponent modeling and coach-guided learning. Youpeng Zhao, Jian Zhao, Xunhan Hu, Wengang Zhou, Houqiang Li, arXiv:2204.02558arXiv preprintYoupeng Zhao, Jian Zhao, Xunhan Hu, Wengang Zhou, and Houqiang Li. Douzero+: Improving doudizhu ai by opponent modeling and coach-guided learning. arXiv preprint arXiv:2204.02558, 2022.\n\nRegret minimization in games with incomplete information. Martin Zinkevich, Michael Johanson, Michael Bowling, Carmelo Piccione, Advances in neural information processing systems. 20Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret min- imization in games with incomplete information. Advances in neural information processing systems, 20:1729-1736, 2007.\n\n. J , Q , K , A , ; Tt, Jj, Qq, A A Kk, 44222, B, R (2) Pair15 actions. 13 actionsSolo (15 actions): 3, 4, 5, 6, 7, 8, 9, T 6 , J, Q, K, A, 2, B, R (2) Pair (13 actions): 33, 44, 55, 66, 77, 88, 99, TT, JJ, QQ, KK, AA, 22\n\n. Jjj Ttt, Qqq, Aaa Kkk, 44422213 actionsTrio (13 actions): 333, 444, 555, 666, 777, 888, 999, TTT, JJJ, QQQ, KKK, AAA, 222\n\nTttj, Tttq, Tttk, Ttta, Ttt2, Tttb, Tttr, Tjjj, Jjjq, Jjjk, Tqqq, Jqqq, Qqqk, Qqqa, Qqq2, Qqqb, Qqqr, Tkkk, Jkkk, Qkkk, Kkka, Kkk2, Kkkb, Kkkr, Taaa, Jaaa, Qaaa, Kaaa, Aaa2, Aaar Aaab, 999T, 999J, 999Q, 999K, 999A, 9992, 999B. 3222222Trio with Solo (182 actions. 4222, 5222, 6222, 7222, 8222, 9222, T222, J222, Q222, K222, A222, 222BTrio with Solo (182 actions): 3334, 3335, 3336, 3337, 3338, 3339, 333T, 333J, 333Q, 333K, 333A, 3332, 333B, 333R, 3444, 4445, 4446, 4447, 4448, 4449, 444T, 444J, 444Q, 444K, 444A, 4442, 444B, 444R, 3555, 4555, 5556, 5557, 5558, 5559, 555T, 555J, 555Q, 555K, 555A, 5552, 555B, 555R, 3666, 4666, 5666, 6667, 6668, 6669, 666T, 666J, 666Q, 666K, 666A, 6662, 666B, 666R, 3777, 4777, 5777, 6777, 7778, 7779, 777T, 777J, 777Q, 777K, 777A, 7772, 777B, 777R, 3888, 4888, 5888, 6888, 7888, 8889, 888T, 888J, 888Q, 888K, 888A, 8882, 888B, 888R, 3999, 4999, 5999, 6999, 7999, 8999, 999T, 999J, 999Q, 999K, 999A, 9992, 999B, 999R, 3TTT, 4TTT, 5TTT, 6TTT, 7TTT, 8TTT, 9TTT, TTTJ, TTTQ, TTTK, TTTA, TTT2, TTTB, TTTR, 3JJJ, 4JJJ, 5JJJ, 6JJJ, 7JJJ, 8JJJ, 9JJJ, TJJJ, JJJQ, JJJK, JJJA, JJJ2, JJJB, JJJR, 3QQQ, 4QQQ, 5QQQ, 6QQQ, 7QQQ, 8QQQ, 9QQQ, TQQQ, JQQQ, QQQK, QQQA, QQQ2, QQQB, QQQR, 3KKK, 4KKK, 5KKK, 6KKK, 7KKK, 8KKK, 9KKK, TKKK, JKKK, QKKK, KKKA, KKK2, KKKB, KKKR, 3AAA, 4AAA, 5AAA, 6AAA, 7AAA, 8AAA, 9AAA, TAAA, JAAA, QAAA, KAAA, AAA2, AAAB, AAAR, 3222, 4222, 5222, 6222, 7222, 8222, 9222, T222, J222, Q222, K222, A222, 222B, 222R\n\n. Tttjj 99ttt, Tttqq, Tttkk, Tttaa, Ttjjj, Jjjqq, Jjjkk, Ttqqq, Jjqqq, Qqqkk, Qqqaa, Ttkkk, Jjkkk, Qqkkk, Kkkaa, 33344Trio with Pair (156 actions. 888TT, 888JJ, 888QQ, 888KK, 888AA, 88822, 33999, 44999, 55999, 66999, 77999, 88999, 999TT, 999JJ, 999QQ, 999KK, 999AA, 99922, 33TTT, 44TTT, 55TTT, 66TTT, 77TTT, 88TTT,. KKK22, 33AAA, 44AAA, 55AAA, 66AAA, 77AAA, 6 T for TenTrio with Pair (156 actions): 33344, 33355, 33366, 33377, 33388, 33399, 333TT, 333JJ, 333QQ, 333KK, 333AA, 33322, 33444, 44455, 44466, 44477, 44488, 44499, 444TT, 444JJ, 444QQ, 444KK, 444AA, 44422, 33555, 44555, 55566, 55577, 55588, 55599, 555TT, 555JJ, 555QQ, 555KK, 555AA, 55522, 33666, 44666, 55666, 66677, 66688, 66699, 666TT, 666JJ, 666QQ, 666KK, 666AA, 66622, 33777, 44777, 55777, 66777, 77788, 77799, 777TT, 777JJ, 777QQ, 777KK, 777AA, 77722, 33888, 44888, 55888, 66888, 77888, 88899, 888TT, 888JJ, 888QQ, 888KK, 888AA, 88822, 33999, 44999, 55999, 66999, 77999, 88999, 999TT, 999JJ, 999QQ, 999KK, 999AA, 99922, 33TTT, 44TTT, 55TTT, 66TTT, 77TTT, 88TTT, 99TTT, TTTJJ, TTTQQ, TTTKK, TTTAA, TTT22, 33JJJ, 44JJJ, 55JJJ, 66JJJ, 77JJJ, 88JJJ, 99JJJ, TTJJJ, JJJQQ, JJJKK, JJJAA, JJJ22, 33QQQ, 44QQQ, 55QQQ, 66QQQ, 77QQQ, 88QQQ, 99QQQ, TTQQQ, JJQQQ, QQQKK, QQQAA, QQQ22, 33KKK, 44KKK, 55KKK, 66KKK, 77KKK, 88KKK, 99KKK, TTKKK, JJKKK, QQKKK, KKKAA, KKK22, 33AAA, 44AAA, 55AAA, 66AAA, 77AAA, 6 T for Ten (10).\n\n** Tttjjj**, Jjjqqq**, Qqqkkk**, Kkkaaa**, 333444555***, 333444**, 444555**, 555666**, 666777**, 777888**, 888999**, 999TTT. Plane with Solo (38 actionsPlane with Solo (38 actions): 333444**, 444555**, 555666**, 666777**, 777888**, 888999**, 999TTT**, TTTJJJ**, JJJQQQ**, QQQKKK**, KKKAAA**, 333444555***,\n\n. ***, 555666777***, 666777888***, 777888999***, 888999ttt***, 999tttjjj***, Jjjqqqkkk*** Tttjjjqqq***, **** Qqqkkkaaa***, 333444555666****, 444555666777****, 555666777888****, 666777888999, 777888999ttt****, 999tt-Tjjjqqq**** 888999tttjjj****, Tttjjjqqqkkk****, Jjjqqqkkkaaa****, 333444555666777*****, 444555666777888*****, 555666777888999*****, 666777888999ttt*****, 777888999tt-Tjjj*****, 888999tttjjjqqq*****, 999tttjjjqqqkkk*****, Tttjjjqqqkkkaaa*****, ***, 555666777***, 666777888***, 777888999***, 888999TTT***, 999TTTJJJ***, TTTJJJQQQ***, JJJQQQKKK***, QQQKKKAAA***, 333444555666****, 444555666777****, 555666777888****, 666777888999****, 777888999TTT****, 888999TTTJJJ****, 999TT- TJJJQQQ****, TTTJJJQQQKKK****, JJJQQQKKKAAA****, 333444555666777*****, 444555666777888*****, 555666777888999*****, 666777888999TTT*****, 777888999TT- TJJJ*****, 888999TTTJJJQQQ*****, 999TTTJJJQQQKKK*****, TTTJJJQQQKKKAAA*****\n\n. **** ****, 444555****, 555666****, 666777****, 777888****, 888999, 999ttt****, Tttjjj****, Jjjqqq****, Qqqkkk****, Kkkaaa****, 333444555******, 444555666******, 555666777******, 666777888******, 777888999******, 888999ttt******, 999tttjjj******, Tttjjjqqq******, Jjjqqqkkk******, Qqqkkkaaa******, 333444555666********, 444555666777********, 555666777888********, 666777888999********, 777888999ttt********, Plane with Pair (30 actionsPlane with Pair (30 actions): 333444****, 444555****, 555666****, 666777****, 777888****, 888999****, 999TTT****, TTTJJJ****, JJJQQQ****, QQQKKK****, KKKAAA****, 333444555******, 444555666******, 555666777******, 666777888******, 777888999******, 888999TTT******, 999TTTJJJ******, TTTJJJQQQ******, JJJQQQKKK******, QQQKKKAAA******, 333444555666********, 444555666777********, 555666777888********, 666777888999********, 777888999TTT********,\n\n. ********, 999tttjjjqqq********, Tttjjjqqqkkk********, Jjjqqqkkkaaa********, ********, 999TTTJJJQQQ********, TTTJJJQQQKKK********, JJJQQQKKKAAA********\n\n** Tttt**, Jjjj**, Qqqq**, Kkkk**, Aaaa**, 3333**, 4444**, 5555**, 6666**, 7777**, 8888**, 9999. 2222Quad with Solo (13 actionsQuad with Solo (13 actions): 3333**, 4444**, 5555**, 6666**, 7777**, 8888**, 9999**, TTTT**, JJJJ**, QQQQ**, KKKK**, AAAA**, 2222**\n\n. **** Tttt****, Jjjj****, Qqqq****, Kkkk****, Aaaa****, 2222****, ****, TTTT****, JJJJ****, QQQQ****, KKKK****, AAAA****, 2222****\n\n. Jjjj Tttt, Qqqq, Aaaa Kkkk, Bomb. 4444222213 actionsBomb (13 actions): 3333, 4444, 5555, 6666, 7777, 8888, 9999, TTTT, JJJJ, QQQQ, KKKK, AAAA, 2222\n\n. Pass (1 action. PASSPass (1 action): PASS\n", "annotations": {"author": "[{\"end\":132,\"start\":73},{\"end\":178,\"start\":133},{\"end\":214,\"start\":179},{\"end\":280,\"start\":215},{\"end\":335,\"start\":281},{\"end\":404,\"start\":336},{\"end\":462,\"start\":405}]", "publisher": null, "author_last_name": "[{\"end\":82,\"start\":78},{\"end\":145,\"start\":142},{\"end\":190,\"start\":186},{\"end\":227,\"start\":222},{\"end\":289,\"start\":285},{\"end\":349,\"start\":345},{\"end\":412,\"start\":409}]", "author_first_name": "[{\"end\":77,\"start\":73},{\"end\":141,\"start\":133},{\"end\":185,\"start\":179},{\"end\":221,\"start\":215},{\"end\":284,\"start\":281},{\"end\":344,\"start\":336},{\"end\":408,\"start\":405}]", "author_affiliation": "[{\"end\":131,\"start\":110},{\"end\":177,\"start\":147},{\"end\":213,\"start\":192},{\"end\":279,\"start\":249},{\"end\":334,\"start\":307},{\"end\":403,\"start\":382},{\"end\":461,\"start\":440}]", "title": "[{\"end\":70,\"start\":1},{\"end\":532,\"start\":463}]", "venue": null, "abstract": "[{\"end\":1917,\"start\":534}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2086,\"start\":2082},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2098,\"start\":2094},{\"end\":2136,\"start\":2133},{\"end\":2150,\"start\":2146},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2190,\"start\":2186},{\"end\":2200,\"start\":2197},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2939,\"start\":2935},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3020,\"start\":3016},{\"end\":3023,\"start\":3020},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3145,\"start\":3141},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3172,\"start\":3168},{\"end\":3604,\"start\":3603},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4122,\"start\":4119},{\"end\":4125,\"start\":4122},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4519,\"start\":4515},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4568,\"start\":4564},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8295,\"start\":8291},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10199,\"start\":10196},{\"end\":10202,\"start\":10199},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10567,\"start\":10563},{\"end\":13388,\"start\":13385},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13391,\"start\":13388},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15234,\"start\":15230},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15387,\"start\":15383},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16377,\"start\":16373},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16391,\"start\":16387},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17396,\"start\":17392},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18117,\"start\":18116},{\"end\":18177,\"start\":18176},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20403,\"start\":20400},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20680,\"start\":20677},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20848,\"start\":20844},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20851,\"start\":20848},{\"end\":20853,\"start\":20851},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21080,\"start\":21076},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21083,\"start\":21080},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21194,\"start\":21190},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21431,\"start\":21427},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21537,\"start\":21533},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21728,\"start\":21724},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21756,\"start\":21752},{\"end\":21882,\"start\":21878},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22132,\"start\":22128},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22219,\"start\":22215},{\"end\":23347,\"start\":23343},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23350,\"start\":23347},{\"end\":23628,\"start\":23624},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23631,\"start\":23628},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25509,\"start\":25505},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33107,\"start\":33103},{\"end\":33639,\"start\":33635},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":34916,\"start\":34912},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34918,\"start\":34916},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34936,\"start\":34932},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34959,\"start\":34955},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34961,\"start\":34959},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":35264,\"start\":35260},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35266,\"start\":35264},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35347,\"start\":35344},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35606,\"start\":35602},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":36679,\"start\":36675},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39137,\"start\":39133},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":45205,\"start\":45203},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":47078,\"start\":47074},{\"end\":47493,\"start\":47490},{\"end\":47547,\"start\":47544},{\"end\":47622,\"start\":47619},{\"end\":47633,\"start\":47630},{\"end\":47668,\"start\":47665},{\"end\":47721,\"start\":47718},{\"end\":47760,\"start\":47757},{\"end\":47771,\"start\":47768},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":48233,\"start\":48229},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":52524,\"start\":52520},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":60069,\"start\":60065},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":61132,\"start\":61128}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":48048,\"start\":47798},{\"attributes\":{\"id\":\"fig_1\"},\"end\":48321,\"start\":48049},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48542,\"start\":48322},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48604,\"start\":48543},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48651,\"start\":48605},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48655,\"start\":48652},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48863,\"start\":48656},{\"attributes\":{\"id\":\"fig_7\"},\"end\":49083,\"start\":48864},{\"attributes\":{\"id\":\"fig_8\"},\"end\":49932,\"start\":49084},{\"attributes\":{\"id\":\"fig_9\"},\"end\":49959,\"start\":49933},{\"attributes\":{\"id\":\"fig_11\"},\"end\":50425,\"start\":49960},{\"attributes\":{\"id\":\"fig_12\"},\"end\":50987,\"start\":50426},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":52062,\"start\":50988},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":53968,\"start\":52063},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":54451,\"start\":53969},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":54688,\"start\":54452},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":55075,\"start\":54689},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":55874,\"start\":55076},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":56470,\"start\":55875},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":56719,\"start\":56471},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":56876,\"start\":56720},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":57423,\"start\":56877},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":58018,\"start\":57424},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":58567,\"start\":58019},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":59262,\"start\":58568},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":59700,\"start\":59263},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":60820,\"start\":59701},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":61933,\"start\":60821}]", "paragraph": "[{\"end\":2940,\"start\":1933},{\"end\":3473,\"start\":2942},{\"end\":4615,\"start\":3475},{\"end\":5019,\"start\":4617},{\"end\":6793,\"start\":5037},{\"end\":8296,\"start\":6795},{\"end\":9006,\"start\":8298},{\"end\":9966,\"start\":9022},{\"end\":10789,\"start\":10003},{\"end\":11537,\"start\":10840},{\"end\":11702,\"start\":11539},{\"end\":11961,\"start\":11780},{\"end\":13018,\"start\":12017},{\"end\":13712,\"start\":13020},{\"end\":14910,\"start\":13758},{\"end\":15634,\"start\":14961},{\"end\":16246,\"start\":15658},{\"end\":17165,\"start\":16294},{\"end\":17397,\"start\":17212},{\"end\":18431,\"start\":17423},{\"end\":18674,\"start\":18580},{\"end\":21433,\"start\":18676},{\"end\":22575,\"start\":21475},{\"end\":23258,\"start\":22591},{\"end\":23513,\"start\":23285},{\"end\":24171,\"start\":23515},{\"end\":24360,\"start\":24173},{\"end\":25004,\"start\":24388},{\"end\":25951,\"start\":25006},{\"end\":26652,\"start\":25953},{\"end\":26899,\"start\":26673},{\"end\":30258,\"start\":26901},{\"end\":30847,\"start\":30316},{\"end\":31794,\"start\":30849},{\"end\":32862,\"start\":31809},{\"end\":34076,\"start\":32901},{\"end\":34336,\"start\":34078},{\"end\":34624,\"start\":34338},{\"end\":35525,\"start\":34626},{\"end\":36354,\"start\":35576},{\"end\":36462,\"start\":36356},{\"end\":36575,\"start\":36464},{\"end\":36603,\"start\":36577},{\"end\":36639,\"start\":36605},{\"end\":37049,\"start\":36661},{\"end\":37498,\"start\":37051},{\"end\":38212,\"start\":37571},{\"end\":38953,\"start\":38273},{\"end\":39123,\"start\":39022},{\"end\":39575,\"start\":39125},{\"end\":40220,\"start\":39577},{\"end\":41561,\"start\":40258},{\"end\":41773,\"start\":41615},{\"end\":42509,\"start\":41775},{\"end\":42916,\"start\":42511},{\"end\":43160,\"start\":42918},{\"end\":43638,\"start\":43162},{\"end\":44482,\"start\":43691},{\"end\":46286,\"start\":44527},{\"end\":47192,\"start\":46316},{\"end\":47797,\"start\":47194}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10839,\"start\":10790},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11779,\"start\":11703},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12016,\"start\":11962},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17211,\"start\":17166},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18579,\"start\":18432}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1931,\"start\":1919},{\"attributes\":{\"n\":\"2\"},\"end\":5035,\"start\":5022},{\"attributes\":{\"n\":\"3\"},\"end\":9020,\"start\":9009},{\"attributes\":{\"n\":\"3.1\"},\"end\":10001,\"start\":9969},{\"attributes\":{\"n\":\"3.2\"},\"end\":13756,\"start\":13715},{\"attributes\":{\"n\":\"4\"},\"end\":14937,\"start\":14913},{\"attributes\":{\"n\":\"4.1\"},\"end\":14959,\"start\":14940},{\"attributes\":{\"n\":\"4.2\"},\"end\":15656,\"start\":15637},{\"attributes\":{\"n\":\"4.3\"},\"end\":16292,\"start\":16249},{\"attributes\":{\"n\":\"4.4\"},\"end\":17421,\"start\":17400},{\"attributes\":{\"n\":\"4.5\"},\"end\":21464,\"start\":21436},{\"end\":21473,\"start\":21467},{\"attributes\":{\"n\":\"6\"},\"end\":22589,\"start\":22578},{\"end\":23283,\"start\":23261},{\"attributes\":{\"n\":\"6.2\"},\"end\":24386,\"start\":24363},{\"attributes\":{\"n\":\"6.3\"},\"end\":26671,\"start\":26655},{\"attributes\":{\"n\":\"6.4\"},\"end\":30277,\"start\":30261},{\"attributes\":{\"n\":\"6.5\"},\"end\":30314,\"start\":30280},{\"attributes\":{\"n\":\"7\"},\"end\":31807,\"start\":31797},{\"end\":32899,\"start\":32865},{\"end\":35549,\"start\":35528},{\"end\":35574,\"start\":35552},{\"end\":36659,\"start\":36642},{\"end\":37535,\"start\":37501},{\"end\":37569,\"start\":37538},{\"end\":38241,\"start\":38215},{\"end\":38271,\"start\":38244},{\"end\":38969,\"start\":38956},{\"end\":39020,\"start\":38972},{\"end\":40256,\"start\":40223},{\"end\":41613,\"start\":41564},{\"end\":43689,\"start\":43641},{\"end\":44525,\"start\":44485},{\"end\":46314,\"start\":46289},{\"end\":47809,\"start\":47799},{\"end\":48060,\"start\":48050},{\"end\":48333,\"start\":48323},{\"end\":48554,\"start\":48544},{\"end\":48616,\"start\":48606},{\"end\":48654,\"start\":48653},{\"end\":48661,\"start\":48657},{\"end\":48875,\"start\":48865},{\"end\":49944,\"start\":49934},{\"end\":49963,\"start\":49961},{\"end\":50430,\"start\":50427},{\"end\":50998,\"start\":50989},{\"end\":52073,\"start\":52064},{\"end\":54462,\"start\":54453},{\"end\":54699,\"start\":54690},{\"end\":56481,\"start\":56472},{\"end\":56730,\"start\":56721},{\"end\":56887,\"start\":56878},{\"end\":57434,\"start\":57425},{\"end\":58029,\"start\":58020},{\"end\":58579,\"start\":58569},{\"end\":59274,\"start\":59264},{\"end\":59712,\"start\":59702},{\"end\":60832,\"start\":60822}]", "table": "[{\"end\":52062,\"start\":51176},{\"end\":53968,\"start\":53095},{\"end\":54451,\"start\":54362},{\"end\":54688,\"start\":54511},{\"end\":55075,\"start\":54734},{\"end\":55874,\"start\":55309},{\"end\":56470,\"start\":56000},{\"end\":56719,\"start\":56510},{\"end\":56876,\"start\":56754},{\"end\":57423,\"start\":57000},{\"end\":58018,\"start\":57994},{\"end\":58567,\"start\":58369},{\"end\":59262,\"start\":59167},{\"end\":59700,\"start\":59504},{\"end\":60820,\"start\":60704},{\"end\":61933,\"start\":61818}]", "figure_caption": "[{\"end\":48048,\"start\":47811},{\"end\":48321,\"start\":48062},{\"end\":48542,\"start\":48335},{\"end\":48604,\"start\":48556},{\"end\":48651,\"start\":48618},{\"end\":48863,\"start\":48663},{\"end\":49083,\"start\":48877},{\"end\":49932,\"start\":49086},{\"end\":49959,\"start\":49946},{\"end\":50425,\"start\":49964},{\"end\":50987,\"start\":50431},{\"end\":51176,\"start\":51000},{\"end\":53095,\"start\":52075},{\"end\":54362,\"start\":53971},{\"end\":54511,\"start\":54464},{\"end\":54734,\"start\":54701},{\"end\":55309,\"start\":55078},{\"end\":56000,\"start\":55877},{\"end\":56510,\"start\":56483},{\"end\":56754,\"start\":56732},{\"end\":57000,\"start\":56889},{\"end\":57994,\"start\":57436},{\"end\":58369,\"start\":58031},{\"end\":59167,\"start\":58582},{\"end\":59504,\"start\":59277},{\"end\":60704,\"start\":59715},{\"end\":61818,\"start\":60835}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10228,\"start\":10222},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15058,\"start\":15052},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17022,\"start\":17016},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19624,\"start\":19618},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28802,\"start\":28796},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":38625,\"start\":38619},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":41951,\"start\":41945},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":42142,\"start\":42136},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":42738,\"start\":42732},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":42992,\"start\":42983},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":43193,\"start\":43187},{\"end\":44115,\"start\":44109}]", "bib_author_first_name": "[{\"end\":63147,\"start\":63136},{\"end\":63160,\"start\":63156},{\"end\":63177,\"start\":63171},{\"end\":63189,\"start\":63184},{\"end\":63208,\"start\":63198},{\"end\":63224,\"start\":63217},{\"end\":63240,\"start\":63235},{\"end\":63254,\"start\":63248},{\"end\":63270,\"start\":63264},{\"end\":63284,\"start\":63279},{\"end\":63623,\"start\":63619},{\"end\":63635,\"start\":63631},{\"end\":63646,\"start\":63643},{\"end\":63660,\"start\":63654},{\"end\":63976,\"start\":63972},{\"end\":63990,\"start\":63984},{\"end\":64244,\"start\":64239},{\"end\":64256,\"start\":64249},{\"end\":64268,\"start\":64262},{\"end\":64278,\"start\":64273},{\"end\":64288,\"start\":64285},{\"end\":64298,\"start\":64295},{\"end\":64311,\"start\":64303},{\"end\":64321,\"start\":64318},{\"end\":64328,\"start\":64326},{\"end\":64338,\"start\":64333},{\"end\":64702,\"start\":64694},{\"end\":64716,\"start\":64712},{\"end\":64730,\"start\":64726},{\"end\":64745,\"start\":64738},{\"end\":64761,\"start\":64755},{\"end\":64776,\"start\":64770},{\"end\":64792,\"start\":64786},{\"end\":64810,\"start\":64802},{\"end\":64834,\"start\":64821},{\"end\":64848,\"start\":64844},{\"end\":65229,\"start\":65221},{\"end\":65245,\"start\":65240},{\"end\":65494,\"start\":65490},{\"end\":65512,\"start\":65502},{\"end\":65522,\"start\":65517},{\"end\":65530,\"start\":65527},{\"end\":65540,\"start\":65537},{\"end\":65777,\"start\":65776},{\"end\":65785,\"start\":65784},{\"end\":65791,\"start\":65790},{\"end\":65799,\"start\":65798},{\"end\":65807,\"start\":65806},{\"end\":65820,\"start\":65816},{\"end\":65822,\"start\":65821},{\"end\":66213,\"start\":66209},{\"end\":66231,\"start\":66223},{\"end\":66250,\"start\":66242},{\"end\":66268,\"start\":66260},{\"end\":66284,\"start\":66280},{\"end\":66298,\"start\":66292},{\"end\":66313,\"start\":66308},{\"end\":66327,\"start\":66322},{\"end\":66682,\"start\":66678},{\"end\":66698,\"start\":66690},{\"end\":66708,\"start\":66703},{\"end\":66723,\"start\":66719},{\"end\":67138,\"start\":67133},{\"end\":67147,\"start\":67143},{\"end\":67161,\"start\":67154},{\"end\":67172,\"start\":67167},{\"end\":67181,\"start\":67177},{\"end\":67441,\"start\":67435},{\"end\":67453,\"start\":67446},{\"end\":67469,\"start\":67464},{\"end\":67481,\"start\":67474},{\"end\":67491,\"start\":67487},{\"end\":67504,\"start\":67498},{\"end\":67513,\"start\":67511},{\"end\":67523,\"start\":67520},{\"end\":67536,\"start\":67529},{\"end\":67552,\"start\":67542},{\"end\":67931,\"start\":67925},{\"end\":67953,\"start\":67942},{\"end\":67972,\"start\":67965},{\"end\":68388,\"start\":68383},{\"end\":68405,\"start\":68399},{\"end\":68418,\"start\":68414},{\"end\":68432,\"start\":68426},{\"end\":68445,\"start\":68439},{\"end\":68460,\"start\":68455},{\"end\":68473,\"start\":68467},{\"end\":68486,\"start\":68481},{\"end\":68501,\"start\":68494},{\"end\":68519,\"start\":68512},{\"end\":68887,\"start\":68883},{\"end\":68905,\"start\":68898},{\"end\":68920,\"start\":68914},{\"end\":68936,\"start\":68929},{\"end\":68951,\"start\":68945},{\"end\":69192,\"start\":69188},{\"end\":69208,\"start\":69203},{\"end\":69225,\"start\":69217},{\"end\":69240,\"start\":69236},{\"end\":69254,\"start\":69250},{\"end\":69567,\"start\":69558},{\"end\":69581,\"start\":69577},{\"end\":69585,\"start\":69582},{\"end\":69858,\"start\":69853},{\"end\":69873,\"start\":69867},{\"end\":69888,\"start\":69882},{\"end\":69911,\"start\":69904},{\"end\":69931,\"start\":69924},{\"end\":69943,\"start\":69937},{\"end\":69954,\"start\":69950},{\"end\":69971,\"start\":69964},{\"end\":69987,\"start\":69979},{\"end\":70002,\"start\":69997},{\"end\":70391,\"start\":70386},{\"end\":70406,\"start\":70400},{\"end\":70427,\"start\":70422},{\"end\":70445,\"start\":70438},{\"end\":70461,\"start\":70458},{\"end\":70475,\"start\":70469},{\"end\":70488,\"start\":70482},{\"end\":70502,\"start\":70497},{\"end\":70517,\"start\":70510},{\"end\":70529,\"start\":70523},{\"end\":70884,\"start\":70878},{\"end\":70901,\"start\":70897},{\"end\":70919,\"start\":70911},{\"end\":70936,\"start\":70930},{\"end\":70950,\"start\":70946},{\"end\":70962,\"start\":70958},{\"end\":70977,\"start\":70970},{\"end\":71385,\"start\":71381},{\"end\":71403,\"start\":71399},{\"end\":71415,\"start\":71411},{\"end\":71663,\"start\":71662},{\"end\":71679,\"start\":71673},{\"end\":71681,\"start\":71680},{\"end\":71883,\"start\":71878},{\"end\":71897,\"start\":71893},{\"end\":71918,\"start\":71910},{\"end\":71920,\"start\":71919},{\"end\":71939,\"start\":71932},{\"end\":71955,\"start\":71949},{\"end\":71972,\"start\":71964},{\"end\":71981,\"start\":71980},{\"end\":71996,\"start\":71989},{\"end\":72007,\"start\":72003},{\"end\":72021,\"start\":72016},{\"end\":72390,\"start\":72385},{\"end\":72404,\"start\":72398},{\"end\":72419,\"start\":72414},{\"end\":72426,\"start\":72420},{\"end\":72443,\"start\":72436},{\"end\":72778,\"start\":72772},{\"end\":72787,\"start\":72783},{\"end\":72800,\"start\":72793},{\"end\":72809,\"start\":72806},{\"end\":72821,\"start\":72815},{\"end\":72831,\"start\":72828},{\"end\":72845,\"start\":72836},{\"end\":72857,\"start\":72850},{\"end\":72870,\"start\":72864},{\"end\":72882,\"start\":72875},{\"end\":73303,\"start\":73299},{\"end\":73317,\"start\":73309},{\"end\":73329,\"start\":73322},{\"end\":73342,\"start\":73335},{\"end\":73353,\"start\":73349},{\"end\":73640,\"start\":73633},{\"end\":73656,\"start\":73646},{\"end\":73668,\"start\":73662},{\"end\":73680,\"start\":73674},{\"end\":73693,\"start\":73688},{\"end\":73704,\"start\":73699},{\"end\":73713,\"start\":73710},{\"end\":74016,\"start\":74009},{\"end\":74028,\"start\":74022},{\"end\":74039,\"start\":74034},{\"end\":74049,\"start\":74044},{\"end\":74064,\"start\":74057},{\"end\":74074,\"start\":74071},{\"end\":74081,\"start\":74079},{\"end\":74651,\"start\":74643},{\"end\":74663,\"start\":74659},{\"end\":74672,\"start\":74669},{\"end\":74683,\"start\":74678},{\"end\":74693,\"start\":74688},{\"end\":74702,\"start\":74698},{\"end\":74710,\"start\":74707},{\"end\":74720,\"start\":74716},{\"end\":75080,\"start\":75075},{\"end\":75092,\"start\":75087},{\"end\":75104,\"start\":75098},{\"end\":75112,\"start\":75109},{\"end\":75125,\"start\":75117},{\"end\":75420,\"start\":75413},{\"end\":75431,\"start\":75427},{\"end\":75444,\"start\":75438},{\"end\":75456,\"start\":75449},{\"end\":75471,\"start\":75463},{\"end\":75757,\"start\":75751},{\"end\":75776,\"start\":75769},{\"end\":75794,\"start\":75787},{\"end\":75811,\"start\":75804},{\"end\":76085,\"start\":76084},{\"end\":76089,\"start\":76088},{\"end\":76093,\"start\":76092},{\"end\":76097,\"start\":76096},{\"end\":76101,\"start\":76100},{\"end\":76115,\"start\":76114},{\"end\":76117,\"start\":76116},{\"end\":76310,\"start\":76307},{\"end\":76324,\"start\":76321},{\"end\":76608,\"start\":76604},{\"end\":77892,\"start\":77887},{\"end\":79264,\"start\":79262},{\"end\":79658,\"start\":79646},{\"end\":79677,\"start\":79673},{\"end\":79795,\"start\":79778},{\"end\":80492,\"start\":80488},{\"end\":81521,\"start\":81519},{\"end\":81785,\"start\":81781},{\"end\":81918,\"start\":81914},{\"end\":81935,\"start\":81931}]", "bib_author_last_name": "[{\"end\":63154,\"start\":63148},{\"end\":63169,\"start\":63161},{\"end\":63182,\"start\":63178},{\"end\":63196,\"start\":63190},{\"end\":63215,\"start\":63209},{\"end\":63233,\"start\":63225},{\"end\":63246,\"start\":63241},{\"end\":63262,\"start\":63255},{\"end\":63277,\"start\":63271},{\"end\":63290,\"start\":63285},{\"end\":63629,\"start\":63624},{\"end\":63641,\"start\":63636},{\"end\":63652,\"start\":63647},{\"end\":63669,\"start\":63661},{\"end\":63982,\"start\":63977},{\"end\":63999,\"start\":63991},{\"end\":64247,\"start\":64245},{\"end\":64260,\"start\":64257},{\"end\":64271,\"start\":64269},{\"end\":64283,\"start\":64279},{\"end\":64293,\"start\":64289},{\"end\":64301,\"start\":64299},{\"end\":64316,\"start\":64312},{\"end\":64324,\"start\":64322},{\"end\":64331,\"start\":64329},{\"end\":64341,\"start\":64339},{\"end\":64710,\"start\":64703},{\"end\":64724,\"start\":64717},{\"end\":64736,\"start\":64731},{\"end\":64753,\"start\":64746},{\"end\":64768,\"start\":64762},{\"end\":64784,\"start\":64777},{\"end\":64800,\"start\":64793},{\"end\":64819,\"start\":64811},{\"end\":64842,\"start\":64835},{\"end\":64856,\"start\":64849},{\"end\":65238,\"start\":65230},{\"end\":65252,\"start\":65246},{\"end\":65500,\"start\":65495},{\"end\":65515,\"start\":65513},{\"end\":65525,\"start\":65523},{\"end\":65535,\"start\":65531},{\"end\":65545,\"start\":65541},{\"end\":65782,\"start\":65778},{\"end\":65788,\"start\":65786},{\"end\":65796,\"start\":65792},{\"end\":65804,\"start\":65800},{\"end\":65814,\"start\":65808},{\"end\":66221,\"start\":66214},{\"end\":66240,\"start\":66232},{\"end\":66258,\"start\":66251},{\"end\":66278,\"start\":66269},{\"end\":66290,\"start\":66285},{\"end\":66306,\"start\":66299},{\"end\":66320,\"start\":66314},{\"end\":66335,\"start\":66328},{\"end\":66688,\"start\":66683},{\"end\":66701,\"start\":66699},{\"end\":66717,\"start\":66709},{\"end\":66729,\"start\":66724},{\"end\":67141,\"start\":67139},{\"end\":67152,\"start\":67148},{\"end\":67165,\"start\":67162},{\"end\":67175,\"start\":67173},{\"end\":67186,\"start\":67182},{\"end\":67444,\"start\":67442},{\"end\":67462,\"start\":67454},{\"end\":67472,\"start\":67470},{\"end\":67485,\"start\":67482},{\"end\":67496,\"start\":67492},{\"end\":67509,\"start\":67505},{\"end\":67518,\"start\":67514},{\"end\":67527,\"start\":67524},{\"end\":67540,\"start\":67537},{\"end\":67556,\"start\":67553},{\"end\":67940,\"start\":67932},{\"end\":67963,\"start\":67954},{\"end\":67980,\"start\":67973},{\"end\":68397,\"start\":68389},{\"end\":68412,\"start\":68406},{\"end\":68424,\"start\":68419},{\"end\":68437,\"start\":68433},{\"end\":68453,\"start\":68446},{\"end\":68465,\"start\":68461},{\"end\":68479,\"start\":68474},{\"end\":68492,\"start\":68487},{\"end\":68510,\"start\":68502},{\"end\":68527,\"start\":68520},{\"end\":68896,\"start\":68888},{\"end\":68912,\"start\":68906},{\"end\":68927,\"start\":68921},{\"end\":68943,\"start\":68937},{\"end\":68958,\"start\":68952},{\"end\":69201,\"start\":69193},{\"end\":69215,\"start\":69209},{\"end\":69234,\"start\":69226},{\"end\":69248,\"start\":69241},{\"end\":69261,\"start\":69255},{\"end\":69575,\"start\":69568},{\"end\":69591,\"start\":69586},{\"end\":69865,\"start\":69859},{\"end\":69880,\"start\":69874},{\"end\":69902,\"start\":69889},{\"end\":69922,\"start\":69912},{\"end\":69935,\"start\":69932},{\"end\":69948,\"start\":69944},{\"end\":69962,\"start\":69955},{\"end\":69977,\"start\":69972},{\"end\":69995,\"start\":69988},{\"end\":70010,\"start\":70003},{\"end\":70398,\"start\":70392},{\"end\":70420,\"start\":70407},{\"end\":70436,\"start\":70428},{\"end\":70456,\"start\":70446},{\"end\":70467,\"start\":70462},{\"end\":70480,\"start\":70476},{\"end\":70495,\"start\":70489},{\"end\":70508,\"start\":70503},{\"end\":70521,\"start\":70518},{\"end\":70536,\"start\":70530},{\"end\":70895,\"start\":70885},{\"end\":70909,\"start\":70902},{\"end\":70928,\"start\":70920},{\"end\":70944,\"start\":70937},{\"end\":70956,\"start\":70951},{\"end\":70968,\"start\":70963},{\"end\":70985,\"start\":70978},{\"end\":71397,\"start\":71386},{\"end\":71409,\"start\":71404},{\"end\":71421,\"start\":71416},{\"end\":71671,\"start\":71664},{\"end\":71688,\"start\":71682},{\"end\":71695,\"start\":71690},{\"end\":71891,\"start\":71884},{\"end\":71908,\"start\":71898},{\"end\":71930,\"start\":71921},{\"end\":71947,\"start\":71940},{\"end\":71962,\"start\":71956},{\"end\":71978,\"start\":71973},{\"end\":71987,\"start\":71982},{\"end\":72001,\"start\":71997},{\"end\":72014,\"start\":72008},{\"end\":72028,\"start\":72022},{\"end\":72038,\"start\":72030},{\"end\":72396,\"start\":72391},{\"end\":72412,\"start\":72405},{\"end\":72434,\"start\":72427},{\"end\":72451,\"start\":72444},{\"end\":72781,\"start\":72779},{\"end\":72791,\"start\":72788},{\"end\":72804,\"start\":72801},{\"end\":72813,\"start\":72810},{\"end\":72826,\"start\":72822},{\"end\":72834,\"start\":72832},{\"end\":72848,\"start\":72846},{\"end\":72862,\"start\":72858},{\"end\":72873,\"start\":72871},{\"end\":72886,\"start\":72883},{\"end\":73307,\"start\":73304},{\"end\":73320,\"start\":73318},{\"end\":73333,\"start\":73330},{\"end\":73347,\"start\":73343},{\"end\":73356,\"start\":73354},{\"end\":73644,\"start\":73641},{\"end\":73660,\"start\":73657},{\"end\":73672,\"start\":73669},{\"end\":73686,\"start\":73681},{\"end\":73697,\"start\":73694},{\"end\":73708,\"start\":73705},{\"end\":73716,\"start\":73714},{\"end\":74020,\"start\":74017},{\"end\":74032,\"start\":74029},{\"end\":74042,\"start\":74040},{\"end\":74055,\"start\":74050},{\"end\":74069,\"start\":74065},{\"end\":74077,\"start\":74075},{\"end\":74085,\"start\":74082},{\"end\":74657,\"start\":74652},{\"end\":74667,\"start\":74664},{\"end\":74676,\"start\":74673},{\"end\":74686,\"start\":74684},{\"end\":74696,\"start\":74694},{\"end\":74705,\"start\":74703},{\"end\":74714,\"start\":74711},{\"end\":74725,\"start\":74721},{\"end\":75085,\"start\":75081},{\"end\":75096,\"start\":75093},{\"end\":75107,\"start\":75105},{\"end\":75115,\"start\":75113},{\"end\":75130,\"start\":75126},{\"end\":75425,\"start\":75421},{\"end\":75436,\"start\":75432},{\"end\":75447,\"start\":75445},{\"end\":75461,\"start\":75457},{\"end\":75474,\"start\":75472},{\"end\":75767,\"start\":75758},{\"end\":75785,\"start\":75777},{\"end\":75802,\"start\":75795},{\"end\":75820,\"start\":75812},{\"end\":76104,\"start\":76102},{\"end\":76108,\"start\":76106},{\"end\":76112,\"start\":76110},{\"end\":76120,\"start\":76118},{\"end\":76314,\"start\":76311},{\"end\":76319,\"start\":76316},{\"end\":76328,\"start\":76325},{\"end\":76434,\"start\":76430},{\"end\":76440,\"start\":76436},{\"end\":76446,\"start\":76442},{\"end\":76452,\"start\":76448},{\"end\":76458,\"start\":76454},{\"end\":76464,\"start\":76460},{\"end\":76470,\"start\":76466},{\"end\":76476,\"start\":76472},{\"end\":76482,\"start\":76478},{\"end\":76488,\"start\":76484},{\"end\":76494,\"start\":76490},{\"end\":76500,\"start\":76496},{\"end\":76506,\"start\":76502},{\"end\":76512,\"start\":76508},{\"end\":76518,\"start\":76514},{\"end\":76524,\"start\":76520},{\"end\":76530,\"start\":76526},{\"end\":76536,\"start\":76532},{\"end\":76542,\"start\":76538},{\"end\":76548,\"start\":76544},{\"end\":76554,\"start\":76550},{\"end\":76560,\"start\":76556},{\"end\":76566,\"start\":76562},{\"end\":76572,\"start\":76568},{\"end\":76578,\"start\":76574},{\"end\":76584,\"start\":76580},{\"end\":76590,\"start\":76586},{\"end\":76596,\"start\":76592},{\"end\":76602,\"start\":76598},{\"end\":76613,\"start\":76609},{\"end\":77898,\"start\":77893},{\"end\":77905,\"start\":77900},{\"end\":77912,\"start\":77907},{\"end\":77919,\"start\":77914},{\"end\":77926,\"start\":77921},{\"end\":77933,\"start\":77928},{\"end\":77940,\"start\":77935},{\"end\":77947,\"start\":77942},{\"end\":77954,\"start\":77949},{\"end\":77961,\"start\":77956},{\"end\":77968,\"start\":77963},{\"end\":77975,\"start\":77970},{\"end\":77982,\"start\":77977},{\"end\":77989,\"start\":77984},{\"end\":77996,\"start\":77991},{\"end\":79273,\"start\":79265},{\"end\":79283,\"start\":79275},{\"end\":79293,\"start\":79285},{\"end\":79317,\"start\":79295},{\"end\":79616,\"start\":79571},{\"end\":79630,\"start\":79618},{\"end\":79644,\"start\":79632},{\"end\":79671,\"start\":79659},{\"end\":79758,\"start\":79678},{\"end\":79776,\"start\":79760},{\"end\":79812,\"start\":79796},{\"end\":79830,\"start\":79814},{\"end\":79914,\"start\":79832},{\"end\":79959,\"start\":79916},{\"end\":79981,\"start\":79961},{\"end\":80003,\"start\":79983},{\"end\":80025,\"start\":80005},{\"end\":80553,\"start\":80493},{\"end\":80565,\"start\":80555},{\"end\":80577,\"start\":80567},{\"end\":80589,\"start\":80579},{\"end\":80601,\"start\":80591},{\"end\":80698,\"start\":80603},{\"end\":80715,\"start\":80700},{\"end\":80732,\"start\":80717},{\"end\":80749,\"start\":80734},{\"end\":80766,\"start\":80751},{\"end\":80871,\"start\":80768},{\"end\":80893,\"start\":80873},{\"end\":81375,\"start\":81367},{\"end\":81397,\"start\":81377},{\"end\":81419,\"start\":81399},{\"end\":81441,\"start\":81421},{\"end\":81528,\"start\":81522},{\"end\":81536,\"start\":81530},{\"end\":81544,\"start\":81538},{\"end\":81552,\"start\":81546},{\"end\":81560,\"start\":81554},{\"end\":81794,\"start\":81786},{\"end\":81804,\"start\":81796},{\"end\":81814,\"start\":81806},{\"end\":81824,\"start\":81816},{\"end\":81844,\"start\":81826},{\"end\":81923,\"start\":81919},{\"end\":81929,\"start\":81925},{\"end\":81940,\"start\":81936}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1912.06680\",\"id\":\"b0\"},\"end\":63576,\"start\":63083},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":53183381},\"end\":63893,\"start\":63578},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5003977},\"end\":64159,\"start\":63895},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":251647993},\"end\":64648,\"start\":64161},{\"attributes\":{\"doi\":\"arXiv:2008.12234\",\"id\":\"b4\"},\"end\":65145,\"start\":64650},{\"attributes\":{\"doi\":\"arXiv:1603.01121\",\"id\":\"b5\"},\"end\":65434,\"start\":65147},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":199465788},\"end\":65701,\"start\":65436},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":26419660},\"end\":66084,\"start\":65703},{\"attributes\":{\"id\":\"b8\"},\"end\":66603,\"start\":66086},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":208637409},\"end\":67071,\"start\":66605},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":225361871},\"end\":67433,\"start\":67073},{\"attributes\":{\"doi\":\"arXiv:2003.13590\",\"id\":\"b11\"},\"end\":67871,\"start\":67435},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2012189},\"end\":68305,\"start\":67873},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1586260},\"end\":68806,\"start\":68307},{\"attributes\":{\"doi\":\"arXiv:1506.02438\",\"id\":\"b14\"},\"end\":69186,\"start\":68808},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b15\"},\"end\":69492,\"start\":69188},{\"attributes\":{\"doi\":\"arXiv:1802.05799\",\"id\":\"b16\"},\"end\":69763,\"start\":69494},{\"attributes\":{\"doi\":\"arXiv:1712.01815\",\"id\":\"b17\"},\"end\":70334,\"start\":69765},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":205261034},\"end\":70794,\"start\":70336},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":53046335},\"end\":71297,\"start\":70796},{\"attributes\":{\"doi\":\"arXiv:2006.10410\",\"id\":\"b20\"},\"end\":71619,\"start\":71299},{\"attributes\":{\"id\":\"b21\"},\"end\":71800,\"start\":71621},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":204972004},\"end\":72334,\"start\":71802},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7252167},\"end\":72696,\"start\":72336},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":209439841},\"end\":73295,\"start\":72698},{\"attributes\":{\"doi\":\"arXiv:1901.08925\",\"id\":\"b25\"},\"end\":73571,\"start\":73297},{\"attributes\":{\"doi\":\"arXiv:1910.04376\",\"id\":\"b26\"},\"end\":73935,\"start\":73573},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":235417032},\"end\":74551,\"start\":73937},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":237101076},\"end\":74944,\"start\":74553},{\"attributes\":{\"id\":\"b29\"},\"end\":75331,\"start\":74946},{\"attributes\":{\"doi\":\"arXiv:2204.02558\",\"id\":\"b30\"},\"end\":75691,\"start\":75333},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7739250},\"end\":76080,\"start\":75693},{\"attributes\":{\"id\":\"b32\"},\"end\":76303,\"start\":76082},{\"attributes\":{\"id\":\"b33\"},\"end\":76428,\"start\":76305},{\"attributes\":{\"id\":\"b34\"},\"end\":77883,\"start\":76430},{\"attributes\":{\"id\":\"b35\"},\"end\":79260,\"start\":77885},{\"attributes\":{\"id\":\"b36\"},\"end\":79567,\"start\":79262},{\"attributes\":{\"id\":\"b37\"},\"end\":80484,\"start\":79569},{\"attributes\":{\"id\":\"b38\"},\"end\":81363,\"start\":80486},{\"attributes\":{\"id\":\"b39\"},\"end\":81517,\"start\":81365},{\"attributes\":{\"id\":\"b40\"},\"end\":81777,\"start\":81519},{\"attributes\":{\"id\":\"b41\"},\"end\":81910,\"start\":81779},{\"attributes\":{\"id\":\"b42\"},\"end\":82061,\"start\":81912},{\"attributes\":{\"id\":\"b43\"},\"end\":82106,\"start\":82063}]", "bib_title": "[{\"end\":63617,\"start\":63578},{\"end\":63970,\"start\":63895},{\"end\":64237,\"start\":64161},{\"end\":65488,\"start\":65436},{\"end\":65774,\"start\":65703},{\"end\":66676,\"start\":66605},{\"end\":67131,\"start\":67073},{\"end\":67923,\"start\":67873},{\"end\":68381,\"start\":68307},{\"end\":70384,\"start\":70336},{\"end\":70876,\"start\":70796},{\"end\":71876,\"start\":71802},{\"end\":72383,\"start\":72336},{\"end\":72770,\"start\":72698},{\"end\":74007,\"start\":73937},{\"end\":74641,\"start\":74553},{\"end\":75749,\"start\":75693}]", "bib_author": "[{\"end\":63156,\"start\":63136},{\"end\":63171,\"start\":63156},{\"end\":63184,\"start\":63171},{\"end\":63198,\"start\":63184},{\"end\":63217,\"start\":63198},{\"end\":63235,\"start\":63217},{\"end\":63248,\"start\":63235},{\"end\":63264,\"start\":63248},{\"end\":63279,\"start\":63264},{\"end\":63292,\"start\":63279},{\"end\":63631,\"start\":63619},{\"end\":63643,\"start\":63631},{\"end\":63654,\"start\":63643},{\"end\":63671,\"start\":63654},{\"end\":63984,\"start\":63972},{\"end\":64001,\"start\":63984},{\"end\":64249,\"start\":64239},{\"end\":64262,\"start\":64249},{\"end\":64273,\"start\":64262},{\"end\":64285,\"start\":64273},{\"end\":64295,\"start\":64285},{\"end\":64303,\"start\":64295},{\"end\":64318,\"start\":64303},{\"end\":64326,\"start\":64318},{\"end\":64333,\"start\":64326},{\"end\":64343,\"start\":64333},{\"end\":64712,\"start\":64694},{\"end\":64726,\"start\":64712},{\"end\":64738,\"start\":64726},{\"end\":64755,\"start\":64738},{\"end\":64770,\"start\":64755},{\"end\":64786,\"start\":64770},{\"end\":64802,\"start\":64786},{\"end\":64821,\"start\":64802},{\"end\":64844,\"start\":64821},{\"end\":64858,\"start\":64844},{\"end\":65240,\"start\":65221},{\"end\":65254,\"start\":65240},{\"end\":65502,\"start\":65490},{\"end\":65517,\"start\":65502},{\"end\":65527,\"start\":65517},{\"end\":65537,\"start\":65527},{\"end\":65547,\"start\":65537},{\"end\":65784,\"start\":65776},{\"end\":65790,\"start\":65784},{\"end\":65798,\"start\":65790},{\"end\":65806,\"start\":65798},{\"end\":65816,\"start\":65806},{\"end\":65825,\"start\":65816},{\"end\":66223,\"start\":66209},{\"end\":66242,\"start\":66223},{\"end\":66260,\"start\":66242},{\"end\":66280,\"start\":66260},{\"end\":66292,\"start\":66280},{\"end\":66308,\"start\":66292},{\"end\":66322,\"start\":66308},{\"end\":66337,\"start\":66322},{\"end\":66690,\"start\":66678},{\"end\":66703,\"start\":66690},{\"end\":66719,\"start\":66703},{\"end\":66731,\"start\":66719},{\"end\":67143,\"start\":67133},{\"end\":67154,\"start\":67143},{\"end\":67167,\"start\":67154},{\"end\":67177,\"start\":67167},{\"end\":67188,\"start\":67177},{\"end\":67446,\"start\":67435},{\"end\":67464,\"start\":67446},{\"end\":67474,\"start\":67464},{\"end\":67487,\"start\":67474},{\"end\":67498,\"start\":67487},{\"end\":67511,\"start\":67498},{\"end\":67520,\"start\":67511},{\"end\":67529,\"start\":67520},{\"end\":67542,\"start\":67529},{\"end\":67558,\"start\":67542},{\"end\":67942,\"start\":67925},{\"end\":67965,\"start\":67942},{\"end\":67982,\"start\":67965},{\"end\":68399,\"start\":68383},{\"end\":68414,\"start\":68399},{\"end\":68426,\"start\":68414},{\"end\":68439,\"start\":68426},{\"end\":68455,\"start\":68439},{\"end\":68467,\"start\":68455},{\"end\":68481,\"start\":68467},{\"end\":68494,\"start\":68481},{\"end\":68512,\"start\":68494},{\"end\":68529,\"start\":68512},{\"end\":68898,\"start\":68883},{\"end\":68914,\"start\":68898},{\"end\":68929,\"start\":68914},{\"end\":68945,\"start\":68929},{\"end\":68960,\"start\":68945},{\"end\":69203,\"start\":69188},{\"end\":69217,\"start\":69203},{\"end\":69236,\"start\":69217},{\"end\":69250,\"start\":69236},{\"end\":69263,\"start\":69250},{\"end\":69577,\"start\":69558},{\"end\":69593,\"start\":69577},{\"end\":69867,\"start\":69853},{\"end\":69882,\"start\":69867},{\"end\":69904,\"start\":69882},{\"end\":69924,\"start\":69904},{\"end\":69937,\"start\":69924},{\"end\":69950,\"start\":69937},{\"end\":69964,\"start\":69950},{\"end\":69979,\"start\":69964},{\"end\":69997,\"start\":69979},{\"end\":70012,\"start\":69997},{\"end\":70400,\"start\":70386},{\"end\":70422,\"start\":70400},{\"end\":70438,\"start\":70422},{\"end\":70458,\"start\":70438},{\"end\":70469,\"start\":70458},{\"end\":70482,\"start\":70469},{\"end\":70497,\"start\":70482},{\"end\":70510,\"start\":70497},{\"end\":70523,\"start\":70510},{\"end\":70538,\"start\":70523},{\"end\":70897,\"start\":70878},{\"end\":70911,\"start\":70897},{\"end\":70930,\"start\":70911},{\"end\":70946,\"start\":70930},{\"end\":70958,\"start\":70946},{\"end\":70970,\"start\":70958},{\"end\":70987,\"start\":70970},{\"end\":71399,\"start\":71381},{\"end\":71411,\"start\":71399},{\"end\":71423,\"start\":71411},{\"end\":71673,\"start\":71662},{\"end\":71690,\"start\":71673},{\"end\":71697,\"start\":71690},{\"end\":71893,\"start\":71878},{\"end\":71910,\"start\":71893},{\"end\":71932,\"start\":71910},{\"end\":71949,\"start\":71932},{\"end\":71964,\"start\":71949},{\"end\":71980,\"start\":71964},{\"end\":71989,\"start\":71980},{\"end\":72003,\"start\":71989},{\"end\":72016,\"start\":72003},{\"end\":72030,\"start\":72016},{\"end\":72040,\"start\":72030},{\"end\":72398,\"start\":72385},{\"end\":72414,\"start\":72398},{\"end\":72436,\"start\":72414},{\"end\":72453,\"start\":72436},{\"end\":72783,\"start\":72772},{\"end\":72793,\"start\":72783},{\"end\":72806,\"start\":72793},{\"end\":72815,\"start\":72806},{\"end\":72828,\"start\":72815},{\"end\":72836,\"start\":72828},{\"end\":72850,\"start\":72836},{\"end\":72864,\"start\":72850},{\"end\":72875,\"start\":72864},{\"end\":72888,\"start\":72875},{\"end\":73309,\"start\":73299},{\"end\":73322,\"start\":73309},{\"end\":73335,\"start\":73322},{\"end\":73349,\"start\":73335},{\"end\":73358,\"start\":73349},{\"end\":73646,\"start\":73633},{\"end\":73662,\"start\":73646},{\"end\":73674,\"start\":73662},{\"end\":73688,\"start\":73674},{\"end\":73699,\"start\":73688},{\"end\":73710,\"start\":73699},{\"end\":73718,\"start\":73710},{\"end\":74022,\"start\":74009},{\"end\":74034,\"start\":74022},{\"end\":74044,\"start\":74034},{\"end\":74057,\"start\":74044},{\"end\":74071,\"start\":74057},{\"end\":74079,\"start\":74071},{\"end\":74087,\"start\":74079},{\"end\":74659,\"start\":74643},{\"end\":74669,\"start\":74659},{\"end\":74678,\"start\":74669},{\"end\":74688,\"start\":74678},{\"end\":74698,\"start\":74688},{\"end\":74707,\"start\":74698},{\"end\":74716,\"start\":74707},{\"end\":74727,\"start\":74716},{\"end\":75087,\"start\":75075},{\"end\":75098,\"start\":75087},{\"end\":75109,\"start\":75098},{\"end\":75117,\"start\":75109},{\"end\":75132,\"start\":75117},{\"end\":75427,\"start\":75413},{\"end\":75438,\"start\":75427},{\"end\":75449,\"start\":75438},{\"end\":75463,\"start\":75449},{\"end\":75476,\"start\":75463},{\"end\":75769,\"start\":75751},{\"end\":75787,\"start\":75769},{\"end\":75804,\"start\":75787},{\"end\":75822,\"start\":75804},{\"end\":76088,\"start\":76084},{\"end\":76092,\"start\":76088},{\"end\":76096,\"start\":76092},{\"end\":76100,\"start\":76096},{\"end\":76106,\"start\":76100},{\"end\":76110,\"start\":76106},{\"end\":76114,\"start\":76110},{\"end\":76122,\"start\":76114},{\"end\":76316,\"start\":76307},{\"end\":76321,\"start\":76316},{\"end\":76330,\"start\":76321},{\"end\":76436,\"start\":76430},{\"end\":76442,\"start\":76436},{\"end\":76448,\"start\":76442},{\"end\":76454,\"start\":76448},{\"end\":76460,\"start\":76454},{\"end\":76466,\"start\":76460},{\"end\":76472,\"start\":76466},{\"end\":76478,\"start\":76472},{\"end\":76484,\"start\":76478},{\"end\":76490,\"start\":76484},{\"end\":76496,\"start\":76490},{\"end\":76502,\"start\":76496},{\"end\":76508,\"start\":76502},{\"end\":76514,\"start\":76508},{\"end\":76520,\"start\":76514},{\"end\":76526,\"start\":76520},{\"end\":76532,\"start\":76526},{\"end\":76538,\"start\":76532},{\"end\":76544,\"start\":76538},{\"end\":76550,\"start\":76544},{\"end\":76556,\"start\":76550},{\"end\":76562,\"start\":76556},{\"end\":76568,\"start\":76562},{\"end\":76574,\"start\":76568},{\"end\":76580,\"start\":76574},{\"end\":76586,\"start\":76580},{\"end\":76592,\"start\":76586},{\"end\":76598,\"start\":76592},{\"end\":76604,\"start\":76598},{\"end\":76615,\"start\":76604},{\"end\":77900,\"start\":77887},{\"end\":77907,\"start\":77900},{\"end\":77914,\"start\":77907},{\"end\":77921,\"start\":77914},{\"end\":77928,\"start\":77921},{\"end\":77935,\"start\":77928},{\"end\":77942,\"start\":77935},{\"end\":77949,\"start\":77942},{\"end\":77956,\"start\":77949},{\"end\":77963,\"start\":77956},{\"end\":77970,\"start\":77963},{\"end\":77977,\"start\":77970},{\"end\":77984,\"start\":77977},{\"end\":77991,\"start\":77984},{\"end\":77998,\"start\":77991},{\"end\":79275,\"start\":79262},{\"end\":79285,\"start\":79275},{\"end\":79295,\"start\":79285},{\"end\":79319,\"start\":79295},{\"end\":79618,\"start\":79571},{\"end\":79632,\"start\":79618},{\"end\":79646,\"start\":79632},{\"end\":79673,\"start\":79646},{\"end\":79760,\"start\":79673},{\"end\":79778,\"start\":79760},{\"end\":79814,\"start\":79778},{\"end\":79832,\"start\":79814},{\"end\":79916,\"start\":79832},{\"end\":79961,\"start\":79916},{\"end\":79983,\"start\":79961},{\"end\":80005,\"start\":79983},{\"end\":80027,\"start\":80005},{\"end\":80555,\"start\":80488},{\"end\":80567,\"start\":80555},{\"end\":80579,\"start\":80567},{\"end\":80591,\"start\":80579},{\"end\":80603,\"start\":80591},{\"end\":80700,\"start\":80603},{\"end\":80717,\"start\":80700},{\"end\":80734,\"start\":80717},{\"end\":80751,\"start\":80734},{\"end\":80768,\"start\":80751},{\"end\":80873,\"start\":80768},{\"end\":80895,\"start\":80873},{\"end\":81377,\"start\":81367},{\"end\":81399,\"start\":81377},{\"end\":81421,\"start\":81399},{\"end\":81443,\"start\":81421},{\"end\":81530,\"start\":81519},{\"end\":81538,\"start\":81530},{\"end\":81546,\"start\":81538},{\"end\":81554,\"start\":81546},{\"end\":81562,\"start\":81554},{\"end\":81796,\"start\":81781},{\"end\":81806,\"start\":81796},{\"end\":81816,\"start\":81806},{\"end\":81826,\"start\":81816},{\"end\":81846,\"start\":81826},{\"end\":81925,\"start\":81914},{\"end\":81931,\"start\":81925},{\"end\":81942,\"start\":81931}]", "bib_venue": "[{\"end\":63134,\"start\":63083},{\"end\":63715,\"start\":63671},{\"end\":64008,\"start\":64001},{\"end\":64395,\"start\":64343},{\"end\":64692,\"start\":64650},{\"end\":65219,\"start\":65147},{\"end\":65552,\"start\":65547},{\"end\":65874,\"start\":65825},{\"end\":66207,\"start\":66086},{\"end\":66792,\"start\":66731},{\"end\":67236,\"start\":67188},{\"end\":67631,\"start\":67574},{\"end\":68043,\"start\":67982},{\"end\":68536,\"start\":68529},{\"end\":68881,\"start\":68808},{\"end\":69318,\"start\":69279},{\"end\":69556,\"start\":69494},{\"end\":69851,\"start\":69765},{\"end\":70544,\"start\":70538},{\"end\":71036,\"start\":70987},{\"end\":71379,\"start\":71299},{\"end\":71660,\"start\":71621},{\"end\":72046,\"start\":72040},{\"end\":72508,\"start\":72453},{\"end\":72949,\"start\":72888},{\"end\":73631,\"start\":73573},{\"end\":74155,\"start\":74087},{\"end\":74732,\"start\":74727},{\"end\":75073,\"start\":74946},{\"end\":75411,\"start\":75333},{\"end\":75871,\"start\":75822},{\"end\":76655,\"start\":76615},{\"end\":79385,\"start\":79319},{\"end\":81614,\"start\":81562},{\"end\":81946,\"start\":81942},{\"end\":82079,\"start\":82065},{\"end\":66840,\"start\":66794},{\"end\":68091,\"start\":68045},{\"end\":72997,\"start\":72951},{\"end\":74237,\"start\":74184}]"}}}, "year": 2023, "month": 12, "day": 17}