{"id": 243755567, "updated": "2023-10-06 09:23:10.427", "metadata": {"title": "Why Do Better Loss Functions Lead to Less Transferable Features?", "authors": "[{\"first\":\"Simon\",\"last\":\"Kornblith\",\"middle\":[]},{\"first\":\"Ting\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Honglak\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Mohammad\",\"last\":\"Norouzi\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "28648-28662", "publication_date": {"year": 2020, "month": 10, "day": 30}, "abstract": "Previous work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2010.16402", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/KornblithCLN21", "doi": null}}, "content": {"source": {"pdf_hash": "62bc27534e9915307acced33c1601b525af5281c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.16402v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ca90002e6f890497a7a2e7521a2061879b52b2c2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/62bc27534e9915307acced33c1601b525af5281c.txt", "contents": "\nWhy Do Better Loss Functions Lead to Less Transferable Features?\n\n\nSimon Kornblith \nGoogle Research\nToronto\n\nTing Chen \nGoogle Research\nToronto\n\nHonglak Lee \nUniversity of Michigan\n\n\n\u2020 \nMohammad Norouzi \nGoogle Research\nToronto\n\nWhy Do Better Loss Functions Lead to Less Transferable Features?\n\nPrevious work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks.\n\nIntroduction\n\nFeatures learned by deep neural networks on ImageNet transfer effectively to a wide range of computer vision tasks [24,70,41]. These networks are often pretrained using vanilla softmax cross-entropy [10,11], but recent work reports that other loss functions such as label smoothing [76] and sigmoid cross-entropy [8] outperform softmax cross-entropy on ImageNet. Although a previous investigation suggested that label smoothing and dropout can hurt transfer learning [41], the vast majority of work studying alternatives to softmax cross-entropy has restricted its empirical investigation to the accuracy of the trained networks on the original dataset. While it is valuable to study the impact of objective functions on classification accuracy, improving transfer accuracy of learned representations can have a more significant practical impact for many applications. This paper takes a comparative approach to understand the effects of different training objectives for image classification through the lens of neural network representations and their transferability. We carefully tune hyperparameters of each loss function and confirm that several loss functions outperform softmax cross-entropy by a statistically significant margin on ImageNet. However, we find that these improvements do not transfer to other tasks (see Table 1 and Figure 1). We delve deeper and explain these empirical findings in terms of effects of different objectives on neural network representations. Our key findings are as follows:\n\n\u2022 We analyze the performance of 9 different objectives on ImageNet and in transfer settings. Although many loss functions and regularizers lead to statistically significant improvements over vanilla * Correspondence to: skornblith@google.com \u2020 Work performed while at Google. softmax cross-entropy on ImageNet, these gains do not transfer. These alternative loss functions produce fixed feature extractors that transfer substantially worse to other tasks, in terms of both linear and k-nearest neighbors classification accuracy, and provide no benefit when representations are fully fine-tuned. \u2022 The choice of objective primarily affects representations in network layers close to the output.\n\nCentered kernel alignment (CKA) reveals large differences in representations of the last few layers of the network, whereas earlier layers are similar regardless of which training objective is used. This helps explain why the choice of objective has little impact on fine-tuning transfer accuracy. \u2022 All objectives that improve accuracy over softmax cross-entropy also lead to greater separation between representations of different classes in the penultimate layer features. These alternative objectives appear to collapse within-class variability in representations, which accounts for both the improvement in accuracy on the original task and the reduction in the quality of the features on downstream tasks.\n\n\nLoss Functions and Output Layer Regularizers\n\nWe investigate 9 loss functions and output layer regularizers. Let z \u2208 R K denote the network's output (\"logit\") vector, and let t \u2208 {0, 1} K denote a one-hot vector of targets. Let x \u2208 R M denote the vector of penultimate layer activations, which gives rise to the output vector as z = W x + b, where W \u2208 R K\u00d7M is the matrix of final layer weights, and b is a vector of biases.\n\nAll investigated loss functions include a term that encourages z to have a high dot product with t.\n\nTo avoid solutions that make this dot product large simply by increasing the scale of z, these loss functions must also include one or more contractive terms and/or normalize z. Many \"regularizers\" correspond to additional contractive terms added to the loss, so we do not draw a firm distinction between loss functions and regularizers. We describe each objective in detail below, and provide hyperparameters in Appendix A.1.\n\nSoftmax cross-entropy [10,11] is the de facto loss function for multi-class classification in deep learning:\nL softmax (z, t) = \u2212 K k=1 t k log e z k K j=1 e zj = \u2212 K k=1 t k z k + log K k=1 e z k .(1)\nThe loss consists of a term that maximizes the dot product between the logits and targets, as well as a contractive term that minimizes the LogSumExp of the logits.\n\nLabel smoothing [76] \"smooths\" the targets for softmax cross-entropy. The new targets are given by mixing the original targets with a uniform distribution over all labels, t = t \u00d7 (1 \u2212 \u03b1) + \u03b1/K, where \u03b1 determines the weighting of the original and uniform targets. In order to maintain the same scale for the gradient with respect to the positive logit, in our experiments, we scale the label smoothing loss by 1/(1 \u2212 \u03b1). The resulting loss is:\nL smooth (z, t; \u03b1) = \u2212 1 1 \u2212 \u03b1 K k=1 (1 \u2212 \u03b1)t k + \u03b1 K log e z k K j=1 e zj(2)\nM\u00fcller et al. [51] previously showed that label smoothing improves calibration and encourages class centroids to lie at the vertices of a regular simplex.\n\nDropout on penultimate layer: Dropout [74] is among the most prominent regularizers in the deep learning literature. We consider dropout applied to the penultimate layer of the neural network, i.e., when inputs to the final layer are randomly kept with some probability \u03c1. When employing dropout, we replace the penultimate layer activations x withx = x \u03be/\u03c1 where \u03be i \u223c Bernoulli(\u03c1). Writing the dropped out logits asz = Wx + b, the dropout loss is:\nL dropout (W , b, x, t; p) = E \u03be [L softmax (z, t)] = \u2212 K k=1 t k z k + E \u03be log K k=1 ez k .(3)\nDropout produces both implicit regularization, by introducing noise into the optimization process [86], and explicit regularization, by changing the parameters that minimize the loss [79].\n\nExtra final layer L 2 regularization: It is common to place the same L 2 regularization on the final layer as elsewhere in the network. However, we find that applying greater L 2 regularization to the final layer can improve performance. The corresponding loss is:\nL extra_l2 (W , z, t; \u03bb final ) = L softmax (z, t) + \u03bb final W 2 F .(4)\nIn architectures with batch normalization, adding additional L 2 regularization has no explicit regularizing effect if the learnable scale (\u03b3) parameters are unregularized, but it still exerts an implicit regularizing effect by altering optimization.\n\nLogit penalty: Whereas label smoothing encourages logits not to be too negative, and dropout imposes a penalty on the logits that depends on the covariance of the weights, an alternative possibility is simply to explicitly constrain logits to be small in L 2 norm:\n\nL logit_penalty (z, t; \u03b2) = L softmax (z, t) + \u03b2 z 2 .\n\nDauphin and Cubuk [20] showed that this regularizer yields accuracy improvements comparable to dropout.\n\nLogit normalization: We consider the use of L 2 normalization, rather than regularization, of the logits. Because the entropy of the output of the softmax function depends on the scale of the logits, which is lost after normalization, we introduce an additional temperature parameter \u03c4 that controls the magnitude of the logit vector, and thus, indirectly, the minimum entropy of the output distribution:\nL logit_norm (z, t; \u03c4 ) = L softmax (z/(\u03c4 z ), t) = \u2212 1 \u03c4 z K k=1 t k z k + log K k=1 e z k /(\u03c4 z ) .(6)\nCosine softmax: We additionally consider L 2 normalization of both the penultimate layer features and the final layer weights corresponding to each class. This loss is equivalent to softmax crossentropy loss if the logits are given by cosine similarity sim(x, y) = x T y/( x y ) between the weight vector and the penultimate layer plus a per-class bias:\nL cos (W , b, x, t; \u03c4 ) = \u2212 K k=1\nt k (sim(W k,: , x)/\u03c4 + b k ) + log K k=1 e sim(W k,: ,x)/\u03c4 +b k ,\n\nwhere \u03c4 is a temperature parameter as above. Similar losses have appeared in previous literature [64,81,87,82,83,22,45,15], and variants have introduced explicit additive or multiplicative margins to this loss that we do not consider here [45,82,83,22]. We observe that, even without an explicit margin, manipulating the temperature has a large impact on observed class separation.\n\nSigmoid cross-entropy, also known as binary cross-entropy, is the natural analog to softmax crossentropy for multi-label classification problems. Although we investigate only single-label multi-class classification tasks, we train networks with sigmoid cross-entropy and evaluate accuracy by ranking the logits of the sigmoids. This approach is related to the one-versus-rest strategy for converting binary classifiers to multi-class classifiers. The sigmoid cross-entropy loss is:\nL sce (z, t) = \u2212 K k=1 t k log e z k e z k +1 + (1 \u2212 t k ) log 1 \u2212 e z k e z k +1 = \u2212 K k=1 t k z k + K k=1 log(e z k + 1).(8)\nThe LogSumExp term of softmax loss is replaced with the sum of the softplus-transformed logits. We initialize the biases of the logits b to \u2212 log(K) so that the initial output probabilities are approximately 1/K. Beyer et al. [8] have previously shown that sigmoid cross-entropy loss leads to improved accuracy on ImageNet relative to softmax cross-entropy.\n\nSquared error: Finally, we investigate squared error loss, as formulated by Hui and Belkin [36]:\nL mse (z, t; \u03ba, M ) = 1 K K k=1 \u03bat k (z k \u2212 M ) 2 + (1 \u2212 t k )z 2 k ,(9)\nwhere \u03ba and M are hyperparameters. \u03ba sets the strength of the loss for the correct class relative to incorrect classes, whereas M controls the magnitude of the correct class target. When \u03ba = M = 1, the loss is simply the mean squared error between z and t. Like Hui and Belkin [36], we find that placing greater weight on the correct class slightly improves ImageNet accuracy.\n\n\nResults\n\nFor each loss, we trained 8 ResNet-50 [32,29] models on the ImageNet ILSVRC 2012 dataset [21,67]. To tune loss hyperparameters and the epoch for early stopping, we performed 3 training runs per  hyperparameter configuration where we held out a validation set of 50,046 ImageNet training  examples. We provide further details regarding the experimental setup in Appendix A. We also confirm that our main findings hold for Inception v3 [76] models in Appendix B.\n\n3.1 Better objectives improve accuracy, but do not transfer better Table 1: Objectives that produce higher ImageNet accuracy lead to less transferable fixed features. \"Im-ageNet\" columns reflect accuracy of ResNet-50 models on the ImageNet validation set. \"Transfer\" columns reflect accuracy of L 2 -regularized multinomial logistic regression or k-nearest neighbors classifiers trained to classify different transfer datasets using the fixed penultimate layer features of the ImageNet-trained networks. Numbers are averaged over 8 different pretraining runs; values not significantly different than the best are bold-faced (p < 0.05, t-test). The strength of L 2 regularization is selected on a holdout set, and k is selected using leave-one-out cross-validation on the training set. See Appendix  We found that, when properly tuned, all investigated objectives except squared error provide a statistically significant improvement over softmax cross-entropy, as shown in the left two columns of Table 1. The gains are small but meaningful, with sigmoid crossentropy and cosine softmax both leading to an improvement of 0.9% in top-1 accuracy over the baseline. For further discussion of differences in the training curves, robustness, calibration, and predictions of these models, see Appendix C.\n\nAlthough networks trained with softmax cross-entropy attain lower ImageNet top-1 accuracy than any other loss function, they nonetheless provide the most transferable features. We evaluated the transferability of the fixed features of our ImageNet-pretrained models by training linear or k-nearest neighbors (kNN) classifiers to classify 8 different natural image datasets: Food-101 [9], CIFAR-10 and CIFAR-100 [43], Birdsnap [6], SUN397 [88], Stanford Cars [42], Oxford-IIIT Pets [60], and Oxford Flowers [56]. The results of these experiments are shown in Table 1 and Figure 1. In both linear and kNN settings, representations learned with vanilla softmax cross-entropy perform best for most tasks. As shown in Table 2, when networks are fully fine-tuned on downstream tasks, the pretraining objective has little effect on the resulting accuracy. When averaging across all tasks, the best loss provides only a 0.2% improvement over softmax cross-entropy, which does not reach statistical significance and is much smaller than the 0.9% difference in ImageNet top-1 accuracy. Thus, using a different loss function for pretraining can improve accuracy on the pretraining task, but this improvement does not appear to transfer to downstream tasks.\n\n\nThe choice of objective primarily affects hidden representations close to the output\n\nOur observation that \"improved\" objectives improve only on ImageNet and not on transfer tasks raises questions about which representations, exactly, they affect. We use two tools to investigate differences in the hidden representations of networks trained with different loss functions. First, we use centered kernel alignment [40,17,18] to directly measure the similarity of hidden representations across networks trained with different loss functions. Second, we measure the sparsity of the ReLU activations in each layer. Both analyses suggest that all loss functions learn similar representations throughout the majority of the network, and differences are present only in the last few ResNet blocks.\n\nLinear centered kernel alignment (CKA) provides a way to measure similarity of neural network representations that is invariant to rotation and isotropic scaling in representation space [40,17,18]. Unlike other ways of measuring representational similarity between neural networks, linear CKA can identify architectural correspondences between layers of networks trained from different initializations [40], a prerequisite for comparing networks trained with different objectives. Given two matrices X \u2208 R n\u00d7p1 and Y \u2208 R n\u00d7p2 containing activations to the same n examples, linear CKA computes the cosine similarity between the reshaped n \u00d7 n covariance matrices between examples:\nCKA linear (X, Y ) = vec(cov(X)) \u00b7 vec(cov(Y )) cov(X) F cov(Y ) F .(10)\nWe measured CKA between all possible pairings of ResNet blocks from 18 networks (2 different initializations for each objective). To reduce memory requirements, we used minibatch CKA [55] with minibatches of size 1500 and processed the ImageNet validation set for 10 epochs.\n\nAs shown in Figure 2, representations of the majority of network layers are highly similar regardless of loss function, but late layers differ substantially. Figure 2a shows similarity between all pairs of blocks from pairs of networks, where one network is trained with vanilla softmax cross-entropy and the other is trained with either softmax or a different loss function. The diagonals of these plots indicate the similarity between architecturally corresponding layers. For all network pairs, the diagonals are stronger than the off-diagonals, indicating that architecturally corresponding layers are more similar than non-corresponding layers. However, in the last few layers of the network, the diagonals are substantially brighter when both networks are trained with softmax than when the second network is trained with a different loss, indicating representational differences in these layers. Figure 2b shows similarity of representations among all loss functions for a subset of blocks. Consistent differences among loss functions are present only in the last third of the network, starting around block 13.    The sparsity of activations reveals a similar pattern of layer-wise differences between networks trained with different loss functions. Figure 3 shows the proportion of non-zero activations in different layers. In all networks, the percentage of non-zero ReLU activations decreases with depth, attaining its minimum at the last convolutional layer. In the first three ResNet stages, activation sparsity is broadly similar regardless of the loss. However, in the final stage and penultimate average pooling layer, the  degree of sparsity depends greatly on the loss. Penultimate layer representations of vanilla softmax networks are the least sparse, with 92.8% non-zero activations. Logit normalization, cosine softmax, and squared error all result in much sparser (<25% non-zero) activations.\n\nThese results immediately suggest an explanation for the limited effect of the training objective on networks' fine-tuning performance. We observe differences among objectives only in later network layers, but previous work has found that these layers change substantially during finetuning [90,63,54], an observation we replicate in Appendix D.4. Thus, the choice of training objective appears to affect parts of the network that are specific to the pretraining task, and do not transfer when the network is fine-tuned on other tasks.\n\n\nRegularization and alternative losses increase class separation\n\nThe previous section suggests that different loss functions learn very different penultimate layer representations, even when their overall accuracy is similar. However, as shown in Appendix C.4, combining different losses and penultimate layer regularizers provides no accuracy improvements over training with only one, suggesting that they share similar mechanisms. In this section, we demonstrate that a simple property of networks' penultimate layer representations can explain their beneficial effect on accuracy relative to vanilla softmax cross-entropy, as well as their harmful effect on fixed features. Specifically, compared to vanilla softmax cross-entropy, all investigated losses cause the network to reduce the relative within-class variance in the penultimate layer representation space. This reduction in within-class variance corresponds to increased separation between classes, and is harmful to linear transfer.\n\nThe ratio of the average within-class cosine distance to the overall average cosine distance measures the dispersion of representations of examples belonging to the same class relative to the overall dispersion of embeddings. We take one minus this quantity to get a closed-form index of class separation that is between 0 and 1: For a balanced dataset, R 2 is equivalent to centered kernel alignment [17,18] between the embeddings and the one-hot label matrix, with a cosine kernel. See Appendix E.2 for results with other distance metrics.\nR 2 = 1 \u2212d within /d total (11) d within = K k=1 N k m=1 N k n=1 1 \u2212 sim(x k,m , x k,n ) KN 2 k ,d total = K j=1 K k=1 Nj m=1 N k n=1 1 \u2212 sim(x j,m , x k,n ) K 2 N j N k where x k,\nAs shown in Table 3 and Figure 4, all regularizers and alternative loss functions we investigate produce greater class separation in penultimate layer representations as compared to vanilla softmax loss. Importantly, this increase in class separation is specific to forms of regularization that affect networks'  final layers. In Appendix E.3, we show that adding regularization through data augmentation improves accuracy without a substantial change in class separation. We further investigate the training dynamics of class separation in Appendix E.4, finding that, for softmax cross-entropy, class separation peaks early in training and then falls, whereas for other objectives, class separation either saturates or continues to rise as training progresses.\n\n\nGreater class separation is associated with less transferable features\n\nAlthough losses that improve class separation lead to higher accuracy on the ImageNet validation set, the feature extractors they learn transfer worse to other tasks. Figure 5a plots mean linear transfer accuracy versus class separation for each of the losses we investigate. We observe a significant negative correlation (Spearman's \u03c1 = \u22120.93, p = 0.002). Notably, vanilla softmax cross-entropy produces the least class separation and the most transferable features, whereas squared error produces much greater class separation than other losses and leads to much lower transfer performance.\n\nTo confirm this relationship between class separation, ImageNet accuracy, and transfer, we trained models with cosine softmax with varying values of the temperature parameter \u03c4 . 1 As shown in Table 4, lower temperatures yield lower top-1 accuracies and worse class separation. However, even though the lowest temperature of \u03c4 = 0.01 achieved 2.7% lower accuracy on ImageNet than the best temperature, this lowest temperature gave the best features for nearly all transfer datasets. Thus, \u03c4 controls a tradeoff between the generalizability of penultimate-layer features and the accuracy on the target dataset. For \u03c4 \u2265 0.04, ImageNet accuracy saturates, but, as shown in Figure 5b, class separation continues to increase and transfer accuracy continues to decrease.\n\nIs there any situation where features with greater class separation could be beneficial for a downstream task? In Figure 6, we use L 2 -regularized logistic regression to relearn the original 1000-way ImageNet classification head from penultimate layer representations of 40,000 examples from the ImageNet validation set. 2 We find that features from networks trained with vanilla softmax loss perform worst, whereas features from networks with greater class separation perform substantially better. Thus, it seems that representations with greater class separation are \"overfit,\" not to the pretraining datapoints, but to the pretraining classes-they perform better for classifying these classes, but worse when the downstream task requires classifying different classes.\n\nOur results above provide some further evidence that greater class separation can be beneficial in real-world scenarios where downstream datasets share classes with the pretraining dataset. In Tables 1 and 4, representations with the lowest class separation perform best on all datasets except for Oxford-IIIT Pets, where representations with slightly greater class separation consistently perform slightly better. We thus explore class overlap between transfer datasets and ImageNet in Appendix F, and find that, of the 37 cat and dog breeds in Oxford-IIIT Pets, 25 correspond directly to ImageNet classes. Furthermore, it is possible to achieve 71.2% accuracy on Oxford-IIIT Pets simply by taking the top-1 predictions of an ImageNet classifier and and mapping them directly to its classes, but this strategy achieves much lower accuracy on other datasets.\n\n\nRelated work\n\nUnderstanding training objectives. There is a great deal of previous work that investigates why some objectives perform better than others, using both theoretical and empirical approaches. On linearly separable data, theoretical analysis shows that gradient descent on both unregularized logistic or multinomial logistic regression objectives (i.e., linear models with sigmoid or softmax crossentropy loss) eventually converges to the minimum norm solution [73]. These results can be extended to neural networks in certain restricted settings [30,85]. However, the convergence to this solution is very slow. Theoretical analysis of dropout has bounded the excess risk of single-layer [80] and two-layer [50] networks. Empirical studies have attempted to understand regularization in more realistic settings. Label smoothing has been explained in terms of mitigation of label noise [47,13], entropy regularization [61,49], and accelerated convergence [89]. Other work has shown that dropout on intermediate layers has both implicit and explicit effects, and both are important to accuracy [85]. Janocha and Czarnecki [38] previously studied both empirical and mathematical similarities and differences among loss functions for supervised classification in the context of deep learning.\n\nTraining objectives for transfer. Our study of the transferability of networks trained with different objectives extends the previous investigation of Kornblith et al. [41], which showed that two of the regularizers considered here (label smoothing and dropout) lead to features that transfer worse. Similar results have been reported for self-supervised learning, where the loss parameters that maximize accuracy on the contrastive task do not provide the best features for linear classification [14]. Our work demonstrates that this phenomenon is pervasive, and connects it to properties of hidden representations. By contrast, work that explores out-of-distribution (OOD) generalization, where the training and evaluation datasets consist of the same classes but with some degree of distribution shift, finds that ImageNet accuracy is highly predictive of OOD accuracy across many pretrained models [65,77]. In Appendix Table C.2, we show that, across loss functions, ImageNet validation set accuracy is not entirely correlated with OOD accuracy, but different losses produce much smaller differences in OOD accuracy than linear transfer accuracy.\n\nOther related work has attempted to devise loss functions to learn more transferable embeddings for few-shot learning [72,75,7,15,58], but embeddings of models trained with softmax cross-entropy often perform on par with these more sophisticated techniques [78]. Doersch et al. [23] motivate their few-shot learning method as a way to mitigate \"supervision collapse,\" which they do not directly quantify, but is closely related to our notion of class separation. While our paper was under review, two studies reported advantages for increased final layer L 2 regularization when performing few-shot linear transfer from very large pretraining datasets to some, but not all, downstream tasks [91,1]. Our findings suggest that these advantages may arise because the pretraining and downstream tasks share classes, but further investigation is needed to confirm this hypothesis.\n\nClass separation. Prior work has investigated class separation in neural networks in a variety of different ways. Theoretical work connects various concepts of the normalized margin between classes to the generalization properties of neural networks [4,52,5,53]. Empirically, Chen et al. [12] measure class separation via \"angular visual hardness,\" the arccosine-transformed cosine similarity between the weight vectors and examples, and suggested an association between this metric and accuracy. However, as shown in Appendix Figure E.1, this metric is unable to differentiate between networks trained with softmax and sigmoid cross-entropy. Papyan et al. [59] describe a phenomenon they call \"neural collapse,\" which occurs after training error vanishes and is associated with the collapse of the class means, classifier, and activations to the vertices of an equiangular tight frame, implying maximal class separation. We relate our observations to theirs in Appendix Table E.2. More generally, studies have investigated how class information evolves through the hidden layers of neural networks using linear classifiers [2], binning estimators of mutual information [71,68,27], Euclidean distances [69], the soft nearest neighbor loss [25], and manifold geometry [3,16]. In the context of a study of representational consistency across networks trained from different initializations, Mehrer et al. [48] previously reported increased class separation in CIFAR-10 networks trained with dropout.\n\nOther work has explored relationships between measures of class separation in embedding spaces and accuracy in few-shot and deep metric learning settings. In deep metric learning, Roth et al. [66] show that objectives that lead to greater class separation generally produce higher recall, and find a similar relationship for a measure of the uniformity of the singular value spectrum of the representation space. In few-shot learning, different work has reached different conclusions: Goldblum et al. [26] find that regularization that increases class separation improves few-shot classification performance on mini-ImageNet and CIFAR-FS, whereas Liu et al. [44] find that introducing a negative margin into a cosine softmax loss yields both lower class separation and higher accuracy.\n\n\nLimitations\n\nAlthough we investigate many losses and multiple architectures, our experiments are limited to moderately sized datasets with moderately sized models, and our conclusions are limited to supervised classification settings. ResNet-50 on trained on ImageNet is a realistic transfer scenario where our analysis is computationally tractable, but bigger models trained on bigger datasets with more classes achieve better performance. These bigger datasets with richer label spaces could potentially help to mitigate the trade-off between pretraining accuracy and feature quality, and may also have class-level overlap with many downstream tasks, making greater class separation helpful rather than harmful.\n\nOur results establish that a wide variety of losses that improve over vanilla softmax cross-entropy lead to greater class separation, but this observation does not immediately lead to a recipe for higher pretraining accuracy. The relationship between class separation and pretraining accuracy is nonmonotonic: squared error achieves the highest class separation but does not significantly outperform vanilla softmax cross-entropy.\n\n\nConclusion\n\nIn this study, we find that the properties of a loss that yields good performance on the pretraining task are different from the properties of a loss that learns good generic features. Training objectives that lead to better performance on the pretraining task learn more invariant representations with greater class separation. However, these same properties are detrimental when fixed features are transferred to other tasks. Moreover, because different loss functions produce different representations only in later network layers, which change substantially during fine-tuning, gains in pretraining accuracy do not lead to gains when models are fine-tuned on downstream tasks.\n\nOur work suggests opportunities for improving fixed feature representations in deep neural networks. We see no inherent reason that features learned by softmax cross-entropy should be optimal for transfer, but previous work has optimized for pretraining accuracy, rather than transferable features. With the increasing importance of transfer learning in deep learning, we believe that future research into loss functions should explicitly target and evaluate performance under transfer settings. We trained ImageNet models (ResNet-50 [32,29,28] \"v1.5\" 3 ) with SGD with Nesterov momentum of 0.9 and a batch size 4096 and weight decay of 8 \u00d7 10 \u22125 (applied to the weights but not batch norm parameters). After 10 epochs of linear warmup to a maximum learning rate of 1.6, we decayed the learning rate by a factor of 0.975 per epoch. We took an exponential moving average of the weights over training as in Szegedy et al. [76], with a momentum factor of 0.9999. We used standard data augmentation comprising random crops of 10-100% of the image with aspect ratios of 0. 75 4 After determining the hyperparameters, we trained models on the full training set. We note that early stopping is important to achieve maximal performance with our learning rate schedule, but does not affect the conclusions we draw regarding transferability and class separation, as we confirm in Appendices D.2 and E.4. \n\n\nA.2 Training and tuning multinomial logistic regression classifiers\n\nTo train multinomial logistic regression classifiers on fixed features, we use L-BFGS [57], following a similar approach to previous work [41,62]. We first extracted features for every image in the training set, by resizing them to 224 pixels on the shortest side and taking a 224 \u00d7 224 pixel center crop. We held out a validation set from the training set, and used this validation set to select the L 2 regularization hyperparameter, which we selected from 45 logarithmically spaced values between 10 \u22126 and 10 5 , applied to the sum of the per-example losses. Because the optimization problem is convex, we used the previous weights as a warm start as we increased the L 2 regularization hyperparameter. After finding the optimal hyperparameter on this validation set, we retrained on the training + validation sets and evaluated accuracy on the test set. We measured either top-1 or mean per-class accuracy, depending on which was suggested by the dataset creators. See Table A.2 for further details of the datasets investigated. 3 The torchvision ResNet-50 model and the \"official\" TensorFlow ResNet both implement this architecture, which was first proposed by Gross and Wilber [29] and differs from the ResNet v1 described by He et al. [32] in performing strided convolution in the first 3 \u00d7 3 convolution in each stage rather than the first 1 \u00d7 1 convolution. Our implementation initializes the \u03b3 parameters of the last batch normalization layer in each block to 0, as in Goyal et al. [28]. 4 Due to the large number of hyperparameter configurations, for squared error, we performed only 1 run per configuration to select hyperparameters, but 3 to select the epoch at which to stop. We manually narrowed the hyperparameter search range until all trained networks achieved similar accuracy. The resulting hyperparaameters performed better than those suggested by Hui and Belkin [36]. \n\n\nA.3 Fine-tuning\n\nIn our fine-tuning experiments in Table 2, we used standard ImageNet-style data augmentation and trained for 20,000 steps with SGD with momentum of 0.9 and cosine annealing [46] without restarts. We performed hyperparameter tuning on a validation set, selecting learning rate values from a logarithmically spaced grid of 8 values between 10 \u22125.5 and 10 \u22121 and weight decay values from a logarithmically spaced grid of 8 values between 10 \u22126.5 and 10 \u22123 , as well as no weight decay, dividing the weight decay by the learning rate. We manually verified that optimal hyperparameter combinations for each loss and dataset fall inside this grid. We averaged the accuracies obtained by hyperparameter tuning over 3 runs starting from 3 different pretrained ImageNet models and picked the best. We then retrained each model on combined training + validation sets and tested on the provided test sets.\n\n\nB Confirmation of main findings with Inception v3\n\nTo confirm that our findings hold across architectures, we performed experiments using Inception v3 [76], which does not have residual connections but still attains good performance on ImageNet ILSVRC. Because our goal was to validate the consistency of our observations, rather than to achieve maximum accuracy, we used the same hyperparameters as for ResNet-50, but selected the epoch for early stopping on a holdout set. Table B.1 confirms our main findings involving class separation and transfer accuracy. As in Table 1, we observe that softmax learns more transferable features than other loss functions, and as in Table 3, we find that lower class separation is associated with greater transferability. Figure B.1 confirms our finding that the choice of loss function affects representations only in later layers of the network.    Validation accuracy rises rapidly due to the use of an exponential moving average of the weights for evaluation. Some loss functions, such as logit normalization, appear to provide higher accuracy than vanilla softmax cross-entropy over the entire training run.  \n\n\nC Additional evaluation of regularizers and losses C.1 Training accuracy and learning curves\n\n\nC.2 Robustness and calibration\n\nIn addition to the differences in class separation and accuracy described in the text, losses differed in out-of-distribution robustness, and in the calibration of the resulting predictions. Table C.2 shows results for ImageNet-trained ResNet-50 models on the out-of-distribution test sets ImageNet-V2 [65], ImageNet-A [34], ImageNet-Sketch [84], ImageNet-R [35], and ImageNet-C [33]. In almost all cases, alternative loss functions outperformed softmax cross-entropy, with logit normalization and cosine softmax typically performing slightly better than alternatives. Effects on calibration, shown in Table C.3, were mixed. Label smoothing substantially reduced expected calibration error [31], as previously shown by M\u00fcller et al. [51], although cosine softmax achieved a lower negative log likelihood. However, there was no clear relationship between calibration and accuracy. Logit penalty performed well in terms of accuracy, but provided the worst calibration of any objective investigated.  Table C.3: Some regularizers and alternative losses improve calibration. We report negative log likelihood (NLL) and expected calibration error (ECE), averaged over 3 ResNet-50 models trained with each loss on the ImageNet validation set, before and after scaling the temperature of the probability of the distribution to minimize NLL, as in Guo et al. [31]. These models were trained with a holdout set of 50,046 ImageNet training examples, which were then used to perform temperature scaling to minimize NLL. ECE is computed with 15 evenly spaced bins. For networks trained with sigmoid loss, we normalize the probability distribution by summing probabilities over all classes.\n\n\nUncalibrated\n\nWith temperature scaling \n\n\nC.3 Similarity of model predictions\n\nGiven that many loss functions resulted in similar improvements in accuracy over softmax loss, we sought to determine whether they also produced similar effects on network predictions. For each pair of models, we selected validation set examples that both models classified incorrectly, and measured the percentage of these examples for which the models gave the same prediction. As shown in Figure C.4, models' predictions cluster into distinct groups according to their objectives. Models trained with the same objective (from different initializations) are more similar than models trained with different objectives. In addition, models trained with (regularized) softmax loss or sigmoid loss are more similar to each other than to models trained with logit normalization or cosine softmax, and networks trained with squared error are dissimilar to all others examined. Figure \n\n\nC.4 Combining regularizers does not improve accuracy\n\nGiven the clear differences in the effects of different objectives on network predictions, we next asked whether combining regularization or normalization strategies might result in better predictions. Table C.4 shows that these combinations do not improve accuracy. However, as shown in Table C.5, improved data augmentation [19,92] provides a similar additive gain in accuracy to networks trained with alternative losses as it does to networks trained with softmax cross-entropy. These results suggest that the objectives that improve over softmax cross-entropy do so via similar mechanisms, but data augmentation acts differently.\n\nWith longer training, both sigmoid cross-entropy and cosine softmax achieve state-of-the-art accuracy among ResNet-50 networks trained with AutoAugment (Table C.6), matching or outperforming supervised contrastive learning [39]. Combining cosine softmax loss, AutoAugment, and Mixup, we achieve 79.1% top-1 accuracy and 94.5% top-5 accuracy, which was, at the time this paper was first posted, the best reported 224 \u00d7 224 pixel single-crop accuracy with an unmodified ResNet-50 architecture trained from scratch. Table C.4: Combining final-layer regularizers and/or improved losses does not meaningfully enhance performance. Accuracy of ResNet-50 models on our ImageNet holdout set when combining losses and regularizers between models. All results reflect the maximum accuracy on the holdout set at any point during training, averaged across 3 training runs. Accuracy numbers are higher on the holdout set than the official ImageNet validation set. This difference in accuracy is likely due to a difference in image distributions between the ImageNet training and validation sets, as previously noted in Section C.3.1 of Recht et al. [65].      \n\n\nBaseline\n\n\nD.3 Results on Chexpert\n\nIn Table D.1, we evaluate the performance of linear classifiers trained to classify the Chexpert chest X-ray dataset [37] based on the penultimate layer representations of our ImageNet-pretrained models, using the procedure described in Appendix A.2. We treat both uncertain and unmentioned observations as negative. We tune the L 2 regularization hyperparameter separately for each class. We approximate AUC using 1000 evenly-spaced bins. The official validation set of 234 images is very small and results in high variance; vanilla softmax cross-entropy achieves the best numerical results on all but one pathology, but many losses are statistically tied. We thus examine a second setting where we split 22,431 images from the training set and evaluate on these images. On this split, we find that softmax cross-entropy performs significantly better than all other losses on 4 of the 5 pathologies, and is tied for the best AUC on the fifth.\n\nWe note that the domain shift between Chexpert and ImageNet is very large. Given the extent of the domain shift, linear transfer will always perform far worse than fine-tuning. However, fine-tuning is unlikely to reveal differences among losses, particularly given that Raghu et al. [63] previously reported that ImageNet pretraining provides no accuracy advantage over training from scratch on this dataset. Nonetheless, we find that, even in this somewhat extreme setting, the fixed features learned by vanilla softmax cross-entropy on ImageNet work better than features learned by other losses.  We measure CKA between ResNet-50 models trained with softmax cross-entropy before and after transfer, following the same procedure as described in Section 3.2 of the main text. Consistent with previous work, we find that later layers change more than earlier layers, although interestingly, the extent of the changes differs greatly by dataset.\n\n\nE Additional class separation results\n\n\nE.1 Relation of class separation index to variance ratios\n\nThe class separation index we use is a simple multidimensional generalization of \u03b7 2 in ANOVA or R 2 in linear regression with categorical predictors when it is applied to normalized embeddings. Its properties are likely to be familiar to many readers. In this section, for completeness, we derive these properties and provide connections to related work.\n\nThe ratio of the average within-class cosine distance to the overall average cosine distance provides a measure of how distributed examples within a class are that is between 0 and 1. We take one minus this quantity to get a closed-form measure of class separation\nR 2 = 1 \u2212 K k=1 N k m=1 N k n=1 1 \u2212 sim(X k m,: , X k n,: ) /(KN 2 k ) K j=1 K k=1 Nj m=1 N k n=1 1 \u2212 sim(X j m,: , X k n,: ) /(K 2 N j N k ) ,(12)\nwhere N k is the number of examples in class k, X k \u2208 R N k \u00d7P is the matrix of P -dimensional embeddings of these examples, and sim(x, y) = x T y/( x y ) is cosine similarity.\n\nRelation of R 2 to ratio of within-class vs. total variance: R 2 is one minus the ratio of the withinclass to weighted total variances of L 2 -normalized embeddings, summed over the feature dimension. To see this, first note that\nx/ x \u2212 y/ y 2 = (x/ x \u2212 y/ y ) T (x/ x \u2212 y/ y ) (13) = 2 \u2212 2sim(x, y),(14)\nso, lettingX k \u2208 R N k \u00d7P be matrices of L 2 -normalized embeddingsX k m,: = X k m,: / X k m,:\nR 2 = 1 \u2212 K k=1 N k m=1 N k n=1\nX k m,: \u2212X k n,:\n2 /(KN 2 K ) K j=1 K k=1\nNj m=1 N k n=1 X j m,: \u2212X k n,:\n2 /(K 2 N j N k ) .(15)\nThe variance of a vector is a V-statistic with the kernel h(x, y) = (x \u2212 y) 2 /2, i.e.,\nVar(y) = 1 n n i=1 \uf8eb \uf8ed y i \u2212 n j=1 y j /n \uf8f6 \uf8f8 2 = 1 n 2 n i=1 n j=1 (y i \u2212 y j ) 2 /2,(16)\nand thus the sum of the variances of the columns of a matrix Y \u2208 R N \u00d7P is\nP p=1 Var(Y :,p ) = 1 n 2 P p=1 N m=1 N n=1 (Y m,p \u2212 Y n,p ) 2 /2 = 1 n 2 N m=1 N n=1 Y m,: \u2212 Y n,: 2 /2.(17)\nIf all N k are equal 5 , we can use (17) to write R 2 in terms of the ratio of the average within-class variance to the total variance of the normalized embeddings, where each variance is summed over the embedding dimensions:\n\u03c3 2 within = P p=1 K k=1 Var(X k :,p )/K \u03c3 2 total = P p=1\nVar(X all :,p )\nR 2 = 1 \u2212 \u03c3 2 within \u03c3 2 total ,(18)\nwhereX all \u2208 R kN \u00d7P is the matrix of all examples.\n\nRelation of R 2 to ratio of between-class vs. total variance: Letting M \u2208 K \u00d7 P be the matrix of mean normalized embeddings of each class M k,: = 1 N k N k m=1X k m,: , the law of total variance states that the variance of each dimension is the sum of the within-class and between-class variances:\nVar(X all :,p ) = K k=1\nVar(X k :,p )/K + Var(M :,p ).\n\nThus, if we let \u03c3 2 between = P p=1 Var(M :,p ), the variance of the class means summed across dimensions, (19) implies that \u03c3 2 total = \u03c3 2 within + \u03c3 2 between . Thus, we have\nR 2 = \u03c3 2 between /\u03c3 2 total .(20)\nRelation of R 2 with other variance ratios: Other work has used the alternative variance ratios \u03c3 2 within /\u03c3 2 between [26], \u03c3 2 between /\u03c3 2 within [44], or (\u03c3 2 between \u2212 \u03c3 2 within )/(\u03c3 2 between + \u03c3 2 within ) [48] to measure class separation. These ratios are monotonic functions of R 2 and can be computed directly from the numbers we provide:  [59] measure various quantities to demonstrate that the representations of neural networks converge to the simplex equiangular tight frame as the training error goes to 0. Collapse to the equiangular tight frame implies both that class separation is maximal (i.e., R 2 \u2192 1) and that class means are maximally distributed. Here we compute the same quantities for networks trained with different losses and optimal early stopping.\u03bcc indicates the mean embedding of the c th of C = 1000 total classes after subtracting the global mean across all classes, and w indicates the classifier weights corresponding to the c th class. W is the matrix of all classifier weights, whereas M is the matrix of all global-mean-subtracted class mean embeddings. \u03a3B is the covariance matrix of the class means and \u03a3W is the within-class covariance matrix. Tr(\u03a3W \u03a3 \u2020 B )/C, where \u03a3 \u2020 B is the Moore-Penrose pseudoinverse of \u03a3B, measures the collapse of within-class variability relative to between-class variability. This quantity reveals the greatest difference between softmax and other losses, and it is also the most related to class separation; for isotropic covariance matrices,\n\u03c3 2 within \u03c3 2 between = 1 R 2 \u2212 1, \u03c3 2 between \u03c3 2 within = R 2 1 \u2212 R 2 , \u03c3 2 between \u2212 \u03c3 2 within \u03c3 2 between + \u03c3 2 within = 2R 2 \u2212 1.(21)\n\nE.2 Other class separation indexes and measurements\nTr(\u03a3W \u03a3 \u2020 B )/C = P/C(1/R 2 \u22121)\nwhere P is the number of penultimate layer features. Std c,c (sim(\u03bcc,\u03bc c )) and Avg c,c |sim(\u03bcc,\u03bc c ) + 1 C\u22121 |, which measure the discrepancy between the class mean directions and those that would result in the maximal separation between the class means, also differentiate softmax from other losses. Other quantities do not.\nStdc ( \u03bc c ) Avg c ( \u03bc c 2 ) Stdc ( wc ) Avg c ( wc 2 )\nStd c,c (sim(\u03bcc,\u03bc c )) Std c,c (sim(wc, w c ))    \nAvg c,c |sim(\u03bcc,\u03bc c ) + 1 C\u22121 | Avg c,c |sim(wc, w c ) + 1 C\u22121 | W / W F \u2212 M / M F 2 F Tr(\u03a3 W \u03a3 \u2020 B )/C\n\nE.3 Augmentation can improve accuracy without increasing class separation\n\nIn Section C.4, we show that improved loss functions and AutoAugment are additive, whereas combinations of improved loss functions or regularizers lead to no significant accuracy improvements. In Table E.3 below, we show that AutoAugment also does not increase class separation. These results confirm that data augmentation and modifications to networks' final layers exert their effects via different (and complementary) mechanisms. \n\n\nE.4 Training dynamics of class separation\n\nAs we discuss in detail in Section 3.3 of the main text, different loss functions lead to different values of class separation. However, we train models with different loss functions for different numbers of epochs, reported in Table A.1. In this section, we confirm that differences in the number of training epochs alone do not explain differences in observed class separation among losses. Instead, as shown in Figure E.4, differences among losses are established early in training and the relative ordering changes little. We observe that, for the softmax cross-entropy model, class separation peaks at epoch 32 and then falls, whereas all models trained with different objectives achieve maximum class separation on the training set at the last checkpoint. On the validation set, for most losses, class separation peaks before optimal accuracy is reached. \n\n\nF Class overlap between ImageNet and transfer datasets\n\nIn this section, we investigate overlap in the classes contained in the ImageNet ILSVRC 2012 dataset and those contained in the downstream datasets investigated in this work. We previously reported the overlap in the images contained in these datasets and those contained in ImageNet in Appendix H of Kornblith et al. [41].\n\nWe first measure the number of classes in each dataset where the class names correspond semantically to classes in ImageNet. Due to differences in the granularity of different datasets, semantic class overlap is somewhat ambiguous. For example, CIFAR-10 contains a single \"dog\" class that corresponds to 90 dog breeds contained in ImageNet, but ImageNet contains only a single \"hummingbird\" class whereas Birdsnap contains 9 different species. We consider classes as \"overlapping\" when the name of the downstream either directly or nearly corresponds to an ImageNet class, or is a superclass of ImageNet classes.\n\nIn addition to being ambiguous, semantic class overlap does not consider shift in class-conditional distributions. Simply because classes in two datasets refer to the same kinds of real-world objects does not mean that the images those classes contain are similar. For example, 61 of the 100 classes in CIFAR-100 are superclasses of ImageNet classes, but because CIFAR images are much lower resolution, a classifier trained on ImageNet does not perform well at classifying them.\n\nTo develop a measure of class overlap that takes distribution shift into consideration, we map each ImageNet class to a class in the downstream dataset and use this mapping in combination with the original 1000-way ImageNet-trained vanilla softmax cross-entropy network to measure classification accuracy. Finding the optimal class mapping is an instance of the minimum-cost flow problem, but can also be solved somewhat less efficiently as a variant of the assignment problem. For each downstream task, we apply an ImageNet classifier to the task's training set and compute the matching matrix. The cost matrix for the assignment problem is the negative matching matrix. To allow multiple ImageNet classes to be assigned to the same downstream task class, we replicate all classes in the downstream task k times, where we select k so that k + 1 replications provides no improvement in accuracy, then use scipy.optimize.linear_sum_assignment to find the mapping. We call the accuracy of the resulting mapping the \"assignment accuracy.\"\n\nResults are shown in Table F.1. Semantic class overlap is generally low, but CIFAR-10, CIFAR-100, and Pets all have non-trivial semantic class overlap. For most datasets, the assignment accuracy is less than half the linear transfer accuracy; the only exceptions are CIFAR-10 and Pets, where the drop is smaller. Pets has comparable linear transfer accuracy to CIFAR-10 but higher assignment accuracy, and thus arguably has the greatest class overlap with ImageNet. \n\nFigure 1 :\n1Higher ImageNet accuracy is not associated with higher linear transfer accuracy. Points represent individual training runs. See Appendix Figure D.1 for similar plots for individual datasets.\n\nFigure 2 :\n2The choice of loss function affects representations only in later network layers. All plots show linear centered kernel alignment (CKA) between representations computed on the ImageNet validation set. a: CKA between network layers, for pairs of ResNet-50 models trained from different initializations with different losses. As controls, the top-right plot shows CKA between two networks trained with softmax, and the bottom-right plot shows CKA between a model trained with softmax loss and a model at initialization where batch norm moments have been computed on the training set. b: CKA between representations extracted from corresponding layers of networks trained with different loss functions. Diagonal reflects similarity of networks with the same loss function trained from different initalizations. See Appendix Figure B.1 for a similar figure for Inception v3 models.\n\nFigure 3 :\n3Loss functions affect sparsity of later layer representations. Plot shows the average % non-zero activations for each ResNet-50 block, after the residual connection and subsequent nonlinearity, on the ImageNet validation set. Dashed lines indicate boundaries between stages.\n\nFigure 4 :\n4Class separation in different layers of ResNet-50, averaged over 8 models per loss on the ImageNet training set. For convolutional layers, we compute cosine distances by flattening the representations of examples across spatial dimensions.\n\n\nm is the embedding of example m in class k \u2208 {1, . . . , K}, N k is the number of examples in class k, and sim(x, y) = x T y/( x y ) is cosine similarity between vectors. As we show in Appendix E.1, if the embeddings are first L 2 normalized, then 1 \u2212 R 2 is the ratio of the average within-class variance to the weighted total variance, where the weights are inversely proportional to the number of examples in each class.\n\nFigure 5 :\n5Class\n\nFigure 6 :\n6Class separation positively correlates with accuracy when relearning how to classify ImageNet classes from limited data. All accuracy numbers are computed on a 10,000 example subset of the ImageNet validation set (10 examples per class). Blue dots indicate accuracy of the weights of the original ImageNettrained model. Orange dots indicate accuracy of a linear classifier trained on the other 40,000 examples from the ImageNet validation set. Lines connect blue and orange dots corresponding to the same objective. The gap between the accuracies of the original and relearned classifiers narrows as class separation increases. Class separation is measured on the ImageNet training set. All numbers are averaged over 8 models.\n\nFigure B. 1 :\n1The choice of loss function affects representations only in later network layers, for Inception v3. All plots show linear centered kernel alignment (CKA) between representations computed on the ImageNet validation set. a: CKA between network layers, for pairs of Inception v3 models trained from different initializations with the same or different losses. b: CKA between representations extracted from corresponding layers of networks trained with different loss functions. Diagonal reflects similarity of networks with the same loss function trained from different initalizations. SeeFigure 2for results with ResNet-50.\n\nFigure C. 1 :\n1Evolution of ImageNet validation accuracy over training. Each curve represents a different model. For each loss function, curves terminate at the epoch that provided the highest holdout set accuracy.\n\nFigure C. 2 :\n2Evolution of ImageNet training accuracy. Each curve represents a different model. For each loss function, curves terminate at the epoch that provided the highest holdout set accuracy.\n\nFigure C. 3 :\n3Validation versus training accuracy. Each curve represents a different model. For each loss function, curves terminate at the training accuracy that provided the highest holdout set accuracy. Regularized models achieve higher validation accuracy at a given training accuracy as compared to softmax.\n\nFigure C. 4 :Figure C. 5 :Figure C. 6 :Figure C. 7 :\n4567C.5 shows that other ways of measuring the similarity of models' predictions yielded qualitatively similar results. Although it was possible to identify the loss used to train individual models from their predictions, models trained with the same loss nonetheless disagreed on many examples. Standard deviations in Different losses produce different predictions, even when accuracies are close. a: For each pair of models, we take examples incorrectly classified by both and measure the percentage where the models' top-1 predictions agree. We show results for 8 different initializations trained with each objective. See Figure C.5 for qualitatively similar plots that show percentages of all examples on which models agree, and percentages of images where both models are either correct or incorrect. b: Dendrogram based on similarity matrix. All models naturally cluster according to loss, except for \"Dropout\" and \"More Final Layer L2\" models. Different ways of measuring similarity of single-model ResNet-50 predictions yield similar qualitative results. In the left panel, we compute the top-1 predictions for pairs of models on the ImageNet validation set and determine the percentage of examples where these predictions match. In the middle panel, we measure the percentage of examples where models either get both right or both wrong. In the right panel, we restrict our analysis to examples that both models get incorrect, and measure the percentage of these examples where both models make the same (incorrect) top-1 prediction. top-1 accuracy are <0.2% for all losses, but even the most similar pair of models provides different predictions on 13.9% of all validation set examples (Figure C.5). Ensembling can substantially reduce the level of disagreement between models and objectives: When ensembling the 8 models trained with the same loss, the least similar losses (softmax and squared error) disagree on only 11.5% of examples (Figure C.6). However, there was little accuracy benefit to ensembling models trained with different objectives over ensembling models trained with the same objective (Figure C.Ensemble predictions are substantially more similar than single-model predictions. Predictions of the ensemble were computed by taking 8 ResNet-50 models trained from different random initializations with the same loss and picking the most common top-1 prediction for each example. Ensembling models trained with different losses provides only modest performance benefits. Ensembles consist of 8 ResNet-50 models, half of which are trained with the objective on the x-axis, the other half with the objective on the y-axis. The ensemble prediction is the modal class prediction of the 8 models.\n\nC. 5 :\n5AutoAugment and Mixup provide consistent accuracy gains beyond well-tuned losses and regularizers. Top-1 accuracy of ResNet-50 models trained with and without AutoAugment, averaged over 3 (with AutoAugment) or 8 (without AutoAugment) runs. Models trained with AutoAugment use the loss hyperparameters chosen for models trained without AutoAugment, but the point at which to stop training was chosen independently on our holdout set. For models trained with Mixup, the mixing parameter \u03b1 is chosen from [0.1, 0.2, 0.3, 0.4] on the holdout set. Best results in each column, as well as results insignificantly different from the best (p > 0.05, t-test), are bold-faced. .0 \u00b1 0.06 93.40 \u00b1 0.02 77.7 \u00b1 0.05 93.74 \u00b1 0.05 78.0 \u00b1 0.05 93.98 \u00b1 0.03 Sigmoid 77.9 \u00b1 0.05 93.50 \u00b1 0.02 78.5 \u00b1 0.04 93.82 \u00b1 0.02 78.5 \u00b1 0.07 93.94 \u00b1 0.04 Logit penalty 77.7 \u00b1 0.02 93.83 \u00b1 0.02 78.3 \u00b1 0.05 94.10 \u00b1 0.03 78.0 \u00b1 0.05 93.95 \u00b1 0.05 Cosine softmax 77.9 \u00b1 0.02 93.86 \u00b1 0.01 78.3 \u00b1 0.02 94.12 \u00b1 0.04 78.4 \u00b1 0.04 94.14 \u00b1 0.02\n\nFigure D. 1 :Figure D. 2 :\n12Higher ImageNet accuracy is not associated with higher linear transfer accuracy. Points represent the accuracies of individual training runs. Panels represent different datasets. SeeFigure 1for a similar plot of transfer accuracy averaged across datasets. On most datasets, softmax cross-entropy achieves the highest linear transfer accuracy over the entire training run. For each loss function, we evaluate the linear transfer accuracy of a ResNet-50 model every 6 epochs over the course of a single ImageNet pretraining run. Lines terminate at the final checkpoint, selected as described in Appendix A.1. Conclusions regarding the superiority of different loss functions match those from\n\nFigure D. 3 :\n3Transfer produces large changes in later network layer representations.\n\nFigure E. 1 :\n1Angular visual hardness of different loss functions. Kernel density estimate of the angular visual hardness[12] scores of the 50,000 examples in the ImageNet validation set, computed with a Gaussian kernel of bandwidth 5 \u00d7 10 \u22126 , for ResNet-50 networks trained with different losses. Legend shows ImageNet top-1 accuracy for each loss function in parentheses. Although alternative loss functions generally reduce angular visual hardness vs. softmax loss, sigmoid loss does not, yet it is tied for the highest accuracy of any loss function.\n\nFigure E. 2 :Figure E. 3 :\n23Singular value spectra of activations and weights learned by different losses. Singular value spectra computed for penultimate layer activations, final layer weights, and class centroids of ResNet-50 models on the ImageNet training set. Penultimate layer activations and final layer weights fail to differentiate sigmoid cross-entropy from softmax cross-entropy. By contrast, the singular value spectrum of the class centroids clearly distinguishes these losses. The distribution of cosine distance between examples. Kernel density estimate of the cosine distance between examples of the same class (solid lines) and of different classes (dashed lines), for penultimate layer embeddings of 10,000 training set examples from ResNet-50 on ImageNet. Top and bottom plots show the same data with different y scales.\n\nFigure E. 4 :\n4Differences in training dynamics of class separation across loss functions. Plots show evolution of class separation over training on the ImageNet training set (left) and validation set (right), for a single ResNet-50 model of each loss function type evaluated every 8 epochs. Curves terminate at the epoch that provided the highest holdout set accuracy.\n\nTable B .\nB1 for a similar table for Inception \nv3 models, Appendix D.2 for linear transfer accuracy evaluated at different epochs of ImageNet pretraining, and \nAppendix D.3 for similar findings on the Chexpert chest X-ray dataset [37]. \n\nImageNet \nTransfer \n\nPretraining loss \nTop-1 Top-5 Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Pets Flowers \n\nLinear transfer: \nSoftmax \n77.0 93.40 74.6 \n92.4 \n76.9 \n55.4 \n62.0 60.3 92.0 \n94.0 \nLabel smoothing \n77.6 93.78 72.7 \n91.6 \n75.2 \n53.6 \n61.6 54.8 92.9 \n91.9 \nSigmoid \n77.9 93.50 73.4 \n91.7 \n75.7 \n52.3 \n62.0 56.1 92.5 \n92.9 \nMore final layer L 2 \n77.7 93.79 70.6 \n91.0 \n73.7 \n51.5 \n60.1 50.3 92.4 \n89.8 \nDropout \n77.5 93.62 72.6 \n91.4 \n75.0 \n53.6 \n61.2 54.7 92.6 \n92.1 \nLogit penalty \n77.7 93.83 68.1 \n90.2 \n72.3 \n48.1 \n59.0 48.3 92.3 \n86.6 \nLogit normalization \n77.8 93.71 66.3 \n90.5 \n72.9 \n50.7 \n58.1 45.4 92.0 \n82.9 \nCosine softmax \n77.9 93.86 62.0 \n89.9 \n71.3 \n45.4 \n55.0 36.7 91.1 \n75.3 \nSquared error \n77.2 92.79 39.8 \n82.2 \n56.3 \n21.8 \n39.9 15.3 84.7 \n46.7 \n\nK-nearest neighbors: \nSoftmax \n77.0 93.40 60.9 \n88.8 \n67.4 \n38.4 \n53.0 28.9 88.8 \n83.6 \nLabel smoothing \n77.6 93.78 59.2 \n88.3 \n66.3 \n39.2 \n53.5 27.3 91.5 \n80.3 \nSigmoid \n77.9 93.50 58.5 \n88.2 \n66.6 \n34.4 \n52.8 26.7 90.8 \n81.5 \nMore final layer L 2 \n77.7 93.79 55.0 \n87.7 \n65.0 \n36.2 \n51.1 24.4 90.9 \n75.6 \nDropout \n77.5 93.62 58.4 \n88.2 \n66.4 \n38.1 \n52.4 27.8 91.1 \n80.1 \nLogit penalty \n77.7 93.83 52.5 \n86.5 \n62.9 \n31.0 \n49.3 22.4 90.8 \n68.1 \nLogit normalization \n77.8 93.71 50.9 \n86.6 \n63.1 \n35.1 \n45.8 24.1 88.2 \n63.1 \nCosine softmax \n77.9 93.86 45.5 \n86.0 \n61.5 \n28.8 \n41.8 19.0 87.0 \n52.7 \nSquared error \n77.2 92.79 27.7 \n74.5 \n44.3 \n13.8 \n28.6 \n9.1 82.6 \n28.2 \n\n77.0 \n77.5 \n78.0 \n78.5 \nImageNet Top-1 Accuracy \n\n50 \n\n60 \n\n70 \n\nAverage Transfer Accuracy \n\nSoftmax \nLabel S. \nSigmoid \nMore L2 \nDropout \n\nLogit Penalty \nLogit Norm \nCos. Softmax \nSquared Error \n\n\n\nTable 2 :\n2The training objective has little impact on the performance of fine-tuned networks. Accuracy of fine-tuning pretrained networks on transfer datasets, averaged over 3 different pretraining initializations. We tune hyperparameters separately for each objective and dataset. Numbers not significantly different than the best are bold-faced (p < 0.05, t-test for individual datasets, two-way ANOVA for average). See Appendix A.3 for training details.Pretraining loss \nFood CIFAR10 CIFAR100 Birdsnap SUN397 Cars Pets Flowers Avg. \n\nSoftmax \n88.2 \n96.9 \n84.1 \n76.2 \n63.4 91.3 93.1 \n96.7 86.2 \nLabel smoothing \n88.3 \n96.7 \n84.0 \n76.3 \n63.6 91.2 93.7 \n96.3 86.3 \nSigmoid \n88.3 \n96.9 \n83.6 \n76.2 \n63.5 91.8 93.7 \n96.3 86.3 \nMore final layer L 2 88.1 \n96.9 \n84.4 \n75.9 \n64.5 91.5 93.8 \n96.2 86.4 \nDropout \n88.3 \n96.7 \n84.2 \n76.5 \n63.9 91.2 93.9 \n96.3 86.4 \nLogit penalty \n88.4 \n96.9 \n83.9 \n76.4 \n63.3 91.2 93.4 \n96.0 86.2 \nLogit normalization 87.9 \n96.9 \n82.9 \n76.0 \n58.3 91.2 92.7 \n95.9 85.2 \nCosine softmax \n88.3 \n96.9 \n83.2 \n75.6 \n56.9 91.3 92.5 \n95.9 85.1 \nSquared error \n87.8 \n96.9 \n84.0 \n75.7 \n61.0 91.4 93.0 \n95.2 85.6 \n\n\n\nTable 3 :\n3Regularization and alternative losses improve class separation in the penultimate layer. Results are averaged over 8 ResNet-50 models per loss on the ImageNet training set.Loss/regularizer ImageNet top-1 Class sep. (R 2 ) \n\nSoftmax \n77.0 \u00b1 0.06 0.349 \u00b1 0.0002 \nLabel smooth. \n77.6 \u00b1 0.03 0.420 \u00b1 0.0003 \nSigmoid \n77.9 \u00b1 0.05 0.427 \u00b1 0.0003 \nExtra L 2 \n77.7 \u00b1 0.03 0.572 \u00b1 0.0006 \nDropout \n77.5 \u00b1 0.04 0.461 \u00b1 0.0003 \nLogit penalty \n77.7 \u00b1 0.04 0.601 \u00b1 0.0004 \nLogit norm \n77.8 \u00b1 0.02 0.517 \u00b1 0.0002 \nCosine softmax \n77.9 \u00b1 0.02 0.641 \u00b1 0.0003 \nSquared error \n77.2 \u00b1 0.04 0.845 \u00b1 0.0002 \n\n2 \n4 \n6 \n8 \n10 12 14 16 \nResNet Block \n0.0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1.0 \n\nClass separation (R 2 \n\n) \n\n56x56 28x28 \n14x14 \n7x7 Pool \nSoftmax \nSquared Error \nDropout \nLabel Smoothing \nMore Final Layer L2 \n\nLogit Penalty \nLogit Normalization \nCosine Softmax \nSigmoid \n\n\n\nTable 4 :\n4Temperatureof cosine softmax loss controls ImageNet accuracy, class separation (R 2 ), and linear \ntransfer accuracy. \n\nImageNet \nTransfer \n\nTemp. Top-1 \nR 2 \nFood CIFAR10 CIFAR100 Birdsnap SUN397 Cars Pets Flowers \n\n0.01 \n74.9 0.236 73.4 \n91.9 \n76.5 \n57.2 \n60.5 62.9 91.7 \n93.6 \n0.02 \n77.0 0.358 72.1 \n91.8 \n76.2 \n56.5 \n60.4 58.5 92.2 \n91.2 \n0.03 \n77.5 0.475 69.1 \n91.5 \n74.9 \n53.7 \n59.1 51.8 92.3 \n87.4 \n0.04 \n77.6 0.562 66.0 \n90.7 \n73.8 \n50.3 \n57.4 45.1 91.7 \n82.2 \n0.05 \n77.6 0.634 62.8 \n90.4 \n72.2 \n47.6 \n55.4 38.6 91.0 \n78.3 \n0.06 \n77.5 0.693 60.3 \n89.3 \n69.8 \n43.3 \n53.8 33.3 91.0 \n72.7 \n0.07 \n77.5 0.738 57.1 \n88.7 \n68.6 \n39.6 \n51.4 29.1 90.2 \n67.9 \n0.08 \n77.6 0.770 53.7 \n87.7 \n66.5 \n35.5 \n49.4 25.7 89.3 \n63.2 \n\n\n\n\nAppendix A Details of training and hyperparameter tuning 17 A.1 Training and tuning neural networks . . . . . . . . . . . . . . . . . . . . . . . . . 17 A.2 Training and tuning multinomial logistic regression classifiers . . . . . . . . . . . 17 A.3 Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Training accuracy and learning curves . . . . . . . . . . . . . . . . . . . . . . . . 20 C.2 Robustness and calibration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.3 Similarity of model predictions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C.4 Combining regularizers does not improve accuracy . . . . . . . . . . . . . . . . . 24 D.1 Scatterplots of ImageNet vs. transfer accuracy by dataset . . . . . . . . . . . . . . 26 D.2 Training dynamics of transfer accuracy . . . . . . . . . . . . . . . . . . . . . . . . 26 D.3 Results on Chexpert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 D.4 CKA between models before and after transfer . . . . . . . . . . . . . . . . . . . 28 Relation of class separation index to variance ratios . . . . . . . . . . . . . . . . . 28 E.2 Other class separation indexes and measurements . . . . . . . . . . . . . . . . . . 29 E.3 Augmentation can improve accuracy without increasing class separation . . . . . . 32 E.4 Training dynamics of class separation . . . . . . . . . . . . . . . . . . . . . . . . 32B Confirmation of main findings with Inception v3 \n18 \n\nC Additional evaluation of regularizers and losses \n20 \n\nC.1 D Additional transfer learning results \n26 \n\nE Additional class separation results \n28 \n\nE.1 F Class overlap between ImageNet and transfer datasets \n32 \nA.1 Training and tuning neural networks \n\n\n\nTable A . 1 :\nA1Hyperparameters for ResNet-50 on ImageNet.Loss/regularizer \nHyperparameters \nEpochs \n\nSoftmax \nN/A \n146 \nLabel smoothing \n\u03b1 = {0.08, 0.09, 0.1, 0.11.0.12} \n180 \nSigmoid \nN/A \n166 \nExtra final layer L 2 \n\u03bbfinal = {4e-4, 6e-4, 8e-4, 1e-3} \n168 \nDropout \n\u03c1 = {0.6, 0.65, 0.7, 0.75, 0.8, 0.85} \n172 \nLogit penalty \n\u03b2 = {5e-5, 1e-4, 2e-4, 4e-4, 6e-4, 8e-4} \n180 \nLogit normalization \u03c4 = {0.03, 0.04, 0.05, 0.06} \n152 \nCosine softmax \n\u03c4 = {0.04, 0.045, 0.05, 0.06, 0.07, 0.08} 158 \nSquared error \n\u03ba = 9, M = 60, loss scale = 10 \n196 \n\n\n\nTable A . 2 :\nA2Datasets examined in transfer learning.Dataset \nClasses Size (train/test) Accuracy measure \n\nFood-101 [9] \n101 \n75,750/25,250 \ntop-1 \nCIFAR-10 [43] \n10 \n50,000/10,000 \ntop-1 \nCIFAR-100 [43] \n10 \n50,000/10,000 \ntop-1 \nBirdsnap [6] \n500 \n47,386/2,443 \ntop-1 \nSUN397 [88] \n397 \n19,850/19,850 \ntop-1 \nStanford Cars [42] \n196 \n8,144/8,041 \ntop-1 \nOxford-IIIT Pets [60] \n37 \n3,680/3,369 \nmean per-class \nOxford 102 Flowers [56] 102 \n2,040/6,149 \nmean per-class \n\n\n\nTable B . 1 :\nB1Objectives that produce higher ImageNet accuracy lead to less transferable fixed features, for Inception v3. \"ImageNet\" columns reflect accuracy of each model on the ImageNet validation set. \"Transfer\" columns reflect accuracy of L 2 -regularized multinomial logistic regression classifiers trained to classify different transfer datasets using the fixed penultimate layer features of the ImageNet-trained networks. Numbers are averaged over 3 different pretraining initializations, and all values not significantly different than the best are bold-faced (p < 0.05, t-test). The strength of L 2 regularization is selected on a validation set that is independent of the test set. SeeTable 1for results with ResNet-50.ImageNet \nTransfer \n\nLoss \nTop-1 Top-5 R 2 Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Pets Flowers \n\nSoftmax \n78.6 94.24 0.356 74.5 \n92.4 \n76.2 \n59.3 \n63.1 64.4 92.2 \n94.0 \nLabel smoothing \n78.8 94.60 0.441 73.3 \n91.6 \n75.0 \n56.1 \n62.4 60.3 93.0 \n92.4 \nSigmoid \n79.1 94.17 0.444 73.7 \n91.3 \n74.7 \n55.0 \n62.0 60.7 92.8 \n93.0 \nMore final layer L 2 \n79.0 94.52 0.586 70.1 \n91.0 \n73.3 \n52.4 \n61.0 51.1 92.5 \n89.6 \nDropout \n79.0 94.50 0.454 72.6 \n91.5 \n74.7 \n56.3 \n62.1 59.2 92.7 \n92.2 \nLogit penalty \n78.9 94.63 0.638 69.1 \n90.6 \n72.1 \n49.3 \n59.2 52.3 92.3 \n87.9 \nLogit normalization 78.8 94.34 0.559 67.4 \n90.6 \n72.2 \n50.9 \n58.5 45.6 92.1 \n84.2 \nCosine softmax \n78.9 94.38 0.666 63.1 \n90.3 \n71.5 \n45.8 \n55.6 38.0 90.6 \n75.2 \nSquared error \n77.7 93.28 0.838 45.3 \n84.1 \n57.6 \n25.0 \n41.1 18.8 85.7 \n54.8 \n2 4 6 8 10 12 14 16 18 20 \n\nNet A Layer \n\nConv2d_2a_3x3 \nMaxPool_3a_3x3 \nConv2d_4a_3x3 \nMixed_5b \nMixed_5d \nMixed_6b \nMixed_6d \nMixed_7a \nMixed_7c \nAvgPool_1a Logits \n\nNet B Layer \n\nSoftmax vs. \nSoftmax \n\n2 4 6 8 10 12 14 16 18 20 \n\nLabel Smoothing Layer \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n\nSoftmax Net Layer \n\nSoftmax vs. \nLabel Smoothing \n\n2 4 6 8 10 12 14 16 18 20 \n\nSigmoid Layer \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n\nSoftmax Net Layer \n\nSoftmax vs. \nSigmoid \n\n2 4 6 8 10 12 14 16 18 20 \n\nMore Final Layer L2 Layer \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n\nSoftmax Net Layer \n\nSoftmax vs. \nMore Final Layer L2 \n\n2 4 6 8 10 12 14 16 18 20 \n\nDropout Layer \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n\nSoftmax Net Layer \n\nSoftmax vs. \nDropout \n\n2 4 6 8 10 12 14 16 18 20 \n\nLogit Penalty Layer \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n\nSoftmax Net Layer \n\nSoftmax vs. \nLogit Penalty \n\n2 4 6 8 10 12 14 16 18 20 \n\nLogit Normalization Layer \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n\nSoftmax Net Layer \n\nSoftmax vs. \nLogit Normalization \n\n2 4 6 8 10 12 14 16 18 20 \n\nCosine Softmax Layer \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n\nSoftmax Net Layer \n\nSoftmax vs. \nCosine Softmax \n\n2 4 6 8 10 12 14 16 18 20 \n\nSquared Error Layer \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n20 \n\nSoftmax Net Layer \n\nSoftmax vs. \nSquared Error \n\n0.0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1.0 \n\nSimilarity (CKA) \n\nSoftmax \nLabel Smoothing \nSigmoid \nMore Final Layer L2 \nDropout \nLogit Penalty \nLogit Normalization \nCosine Softmax \nSquared Error \n\nConv2d_1a_3x3 \nMaxPool_3a_3x3 \nMaxPool_5a_3x3 \nMixed_5c \nMixed_5d \n\nSoftmax \nLabel Smoothing \nSigmoid \nMore Final Layer L2 \nDropout \nLogit Penalty \nLogit Normalization \nCosine Softmax \nSquared Error \n\nMixed_6a \nMixed_6b \nMixed_6c \nMixed_6d \nMixed_6e \n\nSoftmax \n\nLabel Smoothing \n\nSigmoid \n\nMore Final Layer L2 \n\nDropout \n\nLogit Penalty \n\nLogit Normalization \nCosine Softmax \nSquared Error \n\nSoftmax \nLabel Smoothing \nSigmoid \nMore Final Layer L2 \nDropout \nLogit Penalty \nLogit Normalization \nCosine Softmax \nSquared Error \n\nMixed_7a \n\nSoftmax \n\nLabel Smoothing \n\nSigmoid \n\nMore Final Layer L2 \n\nDropout \n\nLogit Penalty \n\nLogit Normalization \nCosine Softmax \nSquared Error \n\nMixed_7b \n\nSoftmax \n\nLabel Smoothing \n\nSigmoid \n\nMore Final Layer L2 \n\nDropout \n\nLogit Penalty \n\nLogit Normalization \nCosine Softmax \nSquared Error \n\nMixed_7c \n\nSoftmax \n\nLabel Smoothing \n\nSigmoid \n\nMore Final Layer L2 \n\nDropout \n\nLogit Penalty \n\nLogit Normalization \nCosine Softmax \nSquared Error \n\nAvgPool_1a \n\nSoftmax \n\nLabel Smoothing \n\nSigmoid \n\nMore Final Layer L2 \n\nDropout \n\nLogit Penalty \n\nLogit Normalization \nCosine Softmax \nSquared Error \n\n\n\nTable C . 1 :\nC1Training accuracy of ResNet-50 models. Regularizers and modified losses resulted in lower ImageNet training set accuracy, consistent with the notion that regularization sacrifices training accuracy to attain greater test accuracy. However, label smoothing was statstically tied with vanilla softmax cross-entropy in terms of training top-1 accuracy, and performed slightly better in terms of training top-5 accuracy. SeeTable 1for validation set accuracy.Loss/regularizer \nTop-1 Acc. (%) Top-5 Acc. (%) \n\nSoftmax \n93.61 \u00b1 0.01 \n99.33 \u00b1 0.002 \nLabel smoothing \n93.62 \u00b1 0.04 \n99.43 \u00b1 0.007 \nSigmoid \n93.22 \u00b1 0.01 \n99.19 \u00b1 0.002 \nExtra final layer L 2 \n91.62 \u00b1 0.01 \n98.85 \u00b1 0.003 \nDropout \n92.25 \u00b1 0.01 \n99.03 \u00b1 0.003 \nLogit penalty \n93.04 \u00b1 0.01 \n99.13 \u00b1 0.002 \nLogit normalization 92.86 \u00b1 0.01 \n99.01 \u00b1 0.003 \nCosine softmax \n92.47 \u00b1 0.01 \n98.75 \u00b1 0.004 \nSquared error \n91.65 \u00b1 0.01 \n98.59 \u00b1 0.002 \n\n0 \n25 \n50 \n75 \n100 \n125 \n150 \n175 \n200 \nEpochs \n60.0 \n\n62.5 \n\n65.0 \n\n67.5 \n\n70.0 \n\n72.5 \n\n75.0 \n\n77.5 \n\nImageNet Top-1 Accuracy \n\nSoftmax \nLabel Smoothing \nSigmoid \nMore Final Layer L2 \nDropout \nLogit Penalty \nLogit Normalization \nCosine Softmax \nSquared Error \n\n\n\nTable C . 2 :\nC2Regularizers and alternative losses improve performance on out-of-distribution test sets. Accuracy averaged over 8 ResNet-50 models per loss.\n\nTable\n\n\nTable C . 6 :\nC6Comparison with state-of-the-art. All results are for ResNet-50 models trained with AutoAugment. Loss hyperparameters are the same as inTable C.5, but the learning schedule decays exponentially at a rate of 0.985 per epoch, rather than 0.975 per epoch. This learning rate schedule takes approximately 2\u00d7 as many epochs before it reaches peak accuracy, and provides a \u223c0.4% improvement in top-1 accuracy across settings.Loss \nEpochs \nTop-1 (%) \nTop-5 (%) \n\nSoftmax [19] \n270 \n77.6 \n93.8 \nSupervised contrastive [39] \n700 \n78.8 \n93.9 \n\nOurs: \nSoftmax \n306 \n77.9 \u00b1 0.02 \n93.77 \u00b1 0.03 \nSigmoid \n324 78.9 \u00b1 0.04 \n93.96 \u00b1 0.06 \nLogit penalty \n346 78.6 \u00b1 0.07 94.30 \u00b1 0.01 \nCosine softmax \n308 78.7 \u00b1 0.04 94.24 \u00b1 0.02 \nOurs (with Mixup): \nSigmoid \n384 \n79.1 \u00b1 0.06 \n94.28 \u00b1 0.03 \nCosine softmax \n348 \n79.1 \u00b1 0.09 \n94.49 \u00b1 0.01 \n\n\nTable 1 :\n1Vanilla softmax cross-entropy achieves greater accuracy than other losses except on SUN397, where it is tied with sigmoid cross-entropy, and Pets, where other losses perform better. We do not show results for squared error because they are off the scale of the plots for all datasets except Pets.\n\nTable D . 1 :\nD1Transfer learning results on Chexpert. AUC of classifiers learned using L 2 -regularized multinomial logistic regression on the fixed penultimate layer features of the ImageNet-trained networks. Numbers are averaged over 8 different pretraining initializations, and all values not significantly different than the best are bold-faced (p < 0.05, t-test). The strength of L 2 regularization is selected on a validation set that is independent of the test set. SeeTable 1for results for natural image datasets.Pretraining loss \nAtelectasis Cardiomegaly Consolidation Edema Pleural effusion \n\nOfficial validation set (234 images): \nSoftmax \n74.9 \n75.3 \n86.9 \n87.4 \n86.4 \nLabel smoothing \n74.0 \n73.6 \n85.8 \n86.5 \n85.8 \nSigmoid \n74.6 \n75.0 \n86.9 \n87.3 \n85.3 \nMore final layer L 2 \n74.6 \n74.1 \n85.2 \n85.2 \n84.8 \nDropout \n74.9 \n73.3 \n86.0 \n86.3 \n84.0 \nLogit penalty \n74.6 \n75.9 \n83.5 \n84.8 \n83.3 \nLogit normalization \n74.2 \n73.6 \n85.6 \n82.2 \n83.8 \nCosine softmax \n73.3 \n71.9 \n83.5 \n81.6 \n82.2 \nSquared error \n71.2 \n67.5 \n76.2 \n73.0 \n75.0 \n\ni.i.d. split from training set (22,431 images): \nSoftmax \n65.8 \n76.2 \n66.8 \n79.9 \n81.4 \nLabel smoothing \n64.9 \n74.9 \n66.3 \n79.2 \n80.2 \nSigmoid \n65.4 \n74.9 \n66.5 \n79.3 \n80.5 \nMore final layer L 2 \n64.7 \n73.6 \n65.8 \n78.4 \n79.7 \nDropout \n65.0 \n74.6 \n66.5 \n79.0 \n80.4 \nLogit penalty \n64.1 \n72.7 \n65.3 \n78.1 \n78.9 \nLogit normalization \n63.9 \n72.0 \n65.2 \n77.6 \n78.4 \nCosine softmax \n62.6 \n70.0 \n64.2 \n76.4 \n76.7 \nSquared error \n59.6 \n64.9 \n59.9 \n72.2 \n69.5 \nD.4 CKA between models before and after transfer \n\n2 4 6 8 1012141618 \n\nImageNet Block \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n\nAvg. Pool \nLogits \n\nImageNet \nBlock \n\nImageNet \n\n2 4 6 8 1012141618 \n\nFood Block \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n\nImageNet Block \n\nFood \n\n2 4 6 8 1012141618 \n\nCIFAR-10 Block \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n\nImageNet Block \n\nCIFAR-10 \n\n2 4 6 8 1012141618 \n\nCIFAR-100 Block \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n\nImageNet Block \n\nCIFAR-100 \n\n2 4 6 8 1012141618 \n\nBirdsnap Block \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n\nImageNet Block \n\nBirdsnap \n\n2 4 6 8 1012141618 \n\nSUN397 Block \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n\nImageNet Block \n\nSUN397 \n\n2 4 6 8 1012141618 \n\nCars Block \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n\nImageNet Block \n\nCars \n\n2 4 6 8 1012141618 \n\nPets Block \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n\nImageNet Block \n\nPets \n\n2 4 6 8 1012141618 \n\nFlowers Block \n\n2 \n4 \n6 \n8 \n10 \n12 \n14 \n16 \n18 \n\nImageNet Block \n\n\n\nTable E . 1 :\nE1Comparison of class separation under different distance indexes. Cosine (mean-subtracted) subtracts the mean of the activations before computing the cosine distance. All results reported for ResNet-50 on the ImageNet training set.Table E.2: Simplex ETF measurements. Papyan et al.Loss/regularizer \nCosine \nCosine (mean-\nsubtracted) \n\nEuclidean distance \n\nSoftmax \n0.3494 \u00b1 0.0002 0.3472 \u00b1 0.0002 \n0.3366 \u00b1 0.0002 \nSquared error \n0.8452 \u00b1 0.0002 0.8450 \u00b1 0.0002 \n0.8421 \u00b1 0.0007 \nDropout \n0.4606 \u00b1 0.0003 0.4559 \u00b1 0.0002 \n0.4524 \u00b1 0.0003 \nLabel smoothing \n0.4197 \u00b1 0.0003 0.4124 \u00b1 0.0004 \n0.3662 \u00b1 0.0005 \nExtra final layer L 2 \n0.5718 \u00b1 0.0006 0.5629 \u00b1 0.0005 \n0.5561 \u00b1 0.0005 \nLogit penalty \n0.6012 \u00b1 0.0004 0.5950 \u00b1 0.0004 \n0.5672 \u00b1 0.0004 \nLogit normalization 0.5167 \u00b1 0.0002 0.5157 \u00b1 0.0002 \n0.5326 \u00b1 0.0002 \nCosine softmax \n0.6406 \u00b1 0.0003 0.6389 \u00b1 0.0003 \n0.6406 \u00b1 0.0003 \nSigmoid \n0.4267 \u00b1 0.0003 0.4315 \u00b1 0.0003 \n0.4272 \u00b1 0.0003 \n\n\nTable E . 3 :\nE3AutoAugment increases ImageNet top-1 accuracy without increasing class separation. Top-1 accuracy is computed on the ImageNet validation set; class separation is computed on the ImageNet training set. Results are averaged over 3 (with AutoAugment) or 8 (standard augmentation) models.Standard augmentation \nAutoAugment \n\nLoss \nImageNet top-1 Class sep. (R 2 ) ImageNet top-1 Class sep. (R 2 ) \n\nSoftmax \n77.0 \u00b1 0.06 0.349 \u00b1 0.0002 \n77.7 \u00b1 0.05 0.353 \u00b1 0.0002 \nSigmoid \n77.9 \u00b1 0.05 0.427 \u00b1 0.0003 \n78.5 \u00b1 0.04 0.432 \u00b1 0.0001 \nLogit penalty \n77.7 \u00b1 0.04 0.601 \u00b1 0.0004 \n78.3 \u00b1 0.05 0.595 \u00b1 0.0003 \nCosine softmax \n77.9 \u00b1 0.02 0.641 \u00b1 0.0003 \n78.3 \u00b1 0.05 0.632 \u00b1 0.0001 \n\n\n\nTable F . 1 :\nF1Class overlap between ImageNet and transfer datasets.Dataset \nNumber of \nclasses \n\nSemantic \nclass overlap \n\nAssignment \naccuracy \n\nLinear transfer \naccuracy \n\nFood \n101 \n7 \n15.4 \n74.6 \nCIFAR-10 \n10 \n9 \n65.1 \n92.4 \nCIFAR-100 \n100 \n61 \n34.6 \n76.9 \nBirdsnap \n500 \n11 \n7.0 \n55.4 \nSUN397 \n397 \n39 \n20.2 \n62.0 \nCars \n196 \n0 \n4.0 \n60.3 \nPets \n37 \n25 \n71.2 \n92.0 \nFlowers \n102 \n1 \n7.6 \n94.0 \n\nTraining at \u03c4 < 0.05 was unstable; we scale the loss by the \u03c4 to reduce the effect of \u03c4 on the size of the gradient WRT the correct class logit. Relationships for \u03c4 \u2265 0.05 remain consistent without loss scaling.2 We use this experimental setup to avoid training linear classifiers on examples that were also seen during pretraining. However, results are qualitatively similar if we train the linear classifier on a 50,046 example subset of the training set and test on the full validation set.\nThis equivalence also holds for unequal N k if the variance is replaced by the inverse-frequency-weighted variance.\nAcknowledgementsWe thank Pieter-Jan Kindermans for comments on the manuscript, and Geoffrey Hinton and David Fleet for useful discussions.\nExploring the limits of large scale pre-training. S Abnar, M Dehghani, B Neyshabur, H Sedghi, arXiv:2110.02095arXiv preprintS. Abnar, M. Dehghani, B. Neyshabur, and H. Sedghi. Exploring the limits of large scale pre-training. arXiv preprint arXiv:2110.02095, 2021.\n\nUnderstanding intermediate layers using linear classifier probes. G Alain, Y Bengio, arXiv:1610.01644arXiv preprintG. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644, 2016.\n\nIntrinsic dimension of data representations in deep neural networks. A Ansuini, A Laio, J H Macke, D Zoccolan, Advances in Neural Information Processing Systems. A. Ansuini, A. Laio, J. H. Macke, and D. Zoccolan. Intrinsic dimension of data representations in deep neural networks. In Advances in Neural Information Processing Systems, 2019.\n\nFor valid generalization, the size of the weights is more important than the size of the network. P L Bartlett, Advances in Neural Information Processing Systems. P. L. Bartlett. For valid generalization, the size of the weights is more important than the size of the network. Advances in Neural Information Processing Systems, pages 134-140, 1997.\n\nSpectrally-normalized margin bounds for neural networks. P L Bartlett, D J Foster, M Telgarsky, Advances in Neural Information Processing Systems. P. L. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks. Advances in Neural Information Processing Systems, 2017:6241-6250, 2017.\n\nBirdsnap: Large-scale fine-grained visual categorization of birds. T Berg, J Liu, S W Lee, M L Alexander, D W Jacobs, P N Belhumeur, IEEE Conference on Computer Vision and Pattern Recognition. IEEET. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs, and P. N. Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2019-2026. IEEE, 2014.\n\nMeta-learning with differentiable closed-form solvers. L Bertinetto, J F Henriques, P Torr, A Vedaldi, International Conference on Learning Representations. L. Bertinetto, J. F. Henriques, P. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form solvers. In International Conference on Learning Representations, 2019.\n\nL Beyer, O J H\u00e9naff, A Kolesnikov, X Zhai, A V D Oord, arXiv:2006.07159Are we done with ImageNet? arXiv preprint. L. Beyer, O. J. H\u00e9naff, A. Kolesnikov, X. Zhai, and A. v. d. Oord. Are we done with ImageNet? arXiv preprint arXiv:2006.07159, 2020.\n\nFood-101 -mining discriminative components with random forests. L Bossard, M Guillaumin, L Van Gool, European Conference on Computer Vision. SpringerL. Bossard, M. Guillaumin, and L. Van Gool. Food-101 -mining discriminative components with random forests. In European Conference on Computer Vision, pages 446-461. Springer, 2014.\n\nProbabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. J S Bridle, Neurocomputing. SpringerJ. S. Bridle. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neurocomputing, pages 227-236. Springer, 1990.\n\nTraining stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. J S Bridle, Advances in Neural Information Processing Systems. J. S. Bridle. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. In Advances in Neural Information Processing Systems, pages 211-217, 1990.\n\nAngular visual hardness. B Chen, W L A Garg, Z Yu, A Shrivastava, J Kautz, A Anandkumar, International Conference on Machine Learning. B. Chen, W. L. A. Garg, Z. Yu, A. Shrivastava, J. Kautz, and A. Anandkumar. Angular visual hardness. In International Conference on Machine Learning, 2020.\n\nAn investigation of how label smoothing affects generalization. B Chen, L Ziyin, Z Wang, P P Liang, arXiv:2010.12648arXiv preprintB. Chen, L. Ziyin, Z. Wang, and P. P. Liang. An investigation of how label smoothing affects generalization. arXiv preprint arXiv:2010.12648, 2020.\n\nA simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, International Conference on Machine Learning. T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020.\n\nA closer look at few-shot classification. W.-Y Chen, Y.-C Liu, Z Kira, Y.-C F Wang, J.-B Huang, International Conference on Learning Representations. W.-Y. Chen, Y.-C. Liu, Z. Kira, Y.-C. F. Wang, and J.-B. Huang. A closer look at few-shot classification. In International Conference on Learning Representations, 2019.\n\nSeparability and geometry of object manifolds in deep neural networks. U Cohen, S Chung, D D Lee, H Sompolinsky, Nature Communications. 111U. Cohen, S. Chung, D. D. Lee, and H. Sompolinsky. Separability and geometry of object manifolds in deep neural networks. Nature Communications, 11(1):1-13, 2020.\n\nAlgorithms for learning kernels based on centered alignment. C Cortes, M Mohri, A Rostamizadeh, The Journal of Machine Learning Research. 131C. Cortes, M. Mohri, and A. Rostamizadeh. Algorithms for learning kernels based on centered alignment. The Journal of Machine Learning Research, 13(1):795-828, 2012.\n\nOn kernel-target alignment. N Cristianini, J Shawe-Taylor, A Elisseeff, J S Kandola, Advances in Neural Information Processing Systems. N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and J. S. Kandola. On kernel-target alignment. In Advances in Neural Information Processing Systems, pages 367-373, 2002.\n\nAutoaugment: Learning augmentation strategies from data. E D Cubuk, B Zoph, D Mane, V Vasudevan, Q V Le, IEEE Conference on Computer Vision and Pattern Recognition. E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation strategies from data. In IEEE Conference on Computer Vision and Pattern Recognition, pages 113-123, 2019.\n\nDeconstructing the regularization of batchnorm. Y Dauphin, E D Cubuk, International Conference on Learning Representations. Y. Dauphin and E. D. Cubuk. Deconstructing the regularization of batchnorm. In International Conference on Learning Representations, 2021.\n\nImageNet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, IEEE Conference on Computer Vision and Pattern Recognition. IeeeJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pages 248-255. Ieee, 2009.\n\nArcface: Additive angular margin loss for deep face recognition. J Deng, J Guo, N Xue, S Zafeiriou, IEEE Conference on Computer Vision and Pattern Recognition. J. Deng, J. Guo, N. Xue, and S. Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4690-4699, 2019.\n\nCrosstransformers: spatially-aware few-shot transfer. C Doersch, A Gupta, A Zisserman, arXiv:2007.11498arXiv preprintC. Doersch, A. Gupta, and A. Zisserman. Crosstransformers: spatially-aware few-shot transfer. arXiv preprint arXiv:2007.11498, 2020.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, International Conference on Machine Learning. J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, 2014.\n\nAnalyzing and improving representations with the soft nearest neighbor loss. N Frosst, N Papernot, G Hinton, International Conference on Machine Learning. PMLRN. Frosst, N. Papernot, and G. Hinton. Analyzing and improving representations with the soft nearest neighbor loss. In International Conference on Machine Learning, pages 2012-2020. PMLR, 2019.\n\nUnraveling metalearning: Understanding feature representations for few-shot tasks. M Goldblum, S Reich, L Fowl, R Ni, V Cherepanova, T Goldstein, International Conference on Machine Learning. PMLRM. Goldblum, S. Reich, L. Fowl, R. Ni, V. Cherepanova, and T. Goldstein. Unraveling meta- learning: Understanding feature representations for few-shot tasks. In International Conference on Machine Learning, pages 3607-3616. PMLR, 2020.\n\nEstimating information flow in deep neural networks. Z Goldfeld, E Berg, K Greenewald, I Melnyk, N Nguyen, B Kingsbury, Y Polyanskiy, International Conference on Machine Learning. Z. Goldfeld, E. v. d. Berg, K. Greenewald, I. Melnyk, N. Nguyen, B. Kingsbury, and Y. Polyan- skiy. Estimating information flow in deep neural networks. In International Conference on Machine Learning, 2018.\n\nP Goyal, P Doll\u00e1r, R Girshick, P Noordhuis, L Wesolowski, A Kyrola, A Tulloch, Y Jia, K He, arXiv:1706.02677Accurate, large minibatch SGD: training ImageNet in 1 hour. arXiv preprintP. Goyal, P. Doll\u00e1r, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch SGD: training ImageNet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n\nTraining and investigating residual nets. S Gross, M Wilber, The Torch Blog. S. Gross and M. Wilber. Training and investigating residual nets. In The Torch Blog. http: //torch.ch/blog/2016/02/04/resnets.html, 2016.\n\nImplicit bias of gradient descent on linear convolutional networks. S Gunasekar, J D Lee, D Soudry, N Srebro, Advances in Neural Information Processing Systems. S. Gunasekar, J. D. Lee, D. Soudry, and N. Srebro. Implicit bias of gradient descent on linear convolutional networks. In Advances in Neural Information Processing Systems, pages 9461-9471, 2018.\n\nOn calibration of modern neural networks. C Guo, G Pleiss, Y Sun, K Q Weinberger, International Conference on Machine Learning. C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321-1330, 2017.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.\n\nBenchmarking neural network robustness to common corruptions and perturbations. D Hendrycks, T Dietterich, International Conference on Learning Representations. D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corrup- tions and perturbations. International Conference on Learning Representations, 2019.\n\nD Hendrycks, K Zhao, S Basart, J Steinhardt, D Song, arXiv:1907.07174Natural adversarial examples. arXiv preprintD. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples. arXiv preprint arXiv:1907.07174, 2019.\n\nThe many faces of robustness: A critical analysis of out-of-distribution generalization. D Hendrycks, S Basart, N Mu, S Kadavath, F Wang, E Dorundo, R Desai, T Zhu, S Parajuli, M Guo, arXiv:2006.16241arXiv preprintD. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Para- juli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.\n\nEvaluation of neural architectures trained with square loss vs crossentropy in classification tasks. L Hui, M Belkin, International Conference on Learning Representations. L. Hui and M. Belkin. Evaluation of neural architectures trained with square loss vs cross- entropy in classification tasks. In International Conference on Learning Representations, 2021.\n\nChexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. J Irvin, P Rajpurkar, M Ko, Y Yu, S Ciurea-Ilcus, C Chute, H Marklund, B Haghgoo, R Ball, K Shpanskaya, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence33J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. Haghgoo, R. Ball, K. Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 590-597, 2019.\n\nOn loss functions for deep neural networks in classification. K Janocha, W M Czarnecki, Schedae Informaticae. 25K. Janocha and W. M. Czarnecki. On loss functions for deep neural networks in classification. Schedae Informaticae, 25, 2016.\n\nSupervised contrastive learning. P Khosla, P Teterwak, C Wang, A Sarna, Y Tian, P Isola, A Maschinot, C Liu, D Krishnan, Advances in Neural Information Processing Systems. P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan. Supervised contrastive learning. In Advances in Neural Information Processing Systems, 2020.\n\nSimilarity of neural network representations revisited. S Kornblith, M Norouzi, H Lee, G Hinton, International Conference on Machine Learning. S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations revisited. In International Conference on Machine Learning, 2019.\n\nDo better ImageNet models transfer better. S Kornblith, J Shlens, Q V Le, IEEE Conference on Computer Vision and Pattern Recognition. S. Kornblith, J. Shlens, and Q. V. Le. Do better ImageNet models transfer better? In IEEE Conference on Computer Vision and Pattern Recognition, pages 2661-2671, 2019.\n\nCollecting a large-scale dataset of fine-grained cars. J Krause, J Deng, M Stark, L Fei-Fei, Second Workshop on Fine-Grained Visual Categorization. J. Krause, J. Deng, M. Stark, and L. Fei-Fei. Collecting a large-scale dataset of fine-grained cars. In Second Workshop on Fine-Grained Visual Categorization, 2013.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, University of TorontoTechnical reportA. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\nNegative margin matters: Understanding margin in few-shot classification. B Liu, Y Cao, Y Lin, Q Li, Z Zhang, M Long, H Hu, European Conference on Computer Vision. SpringerB. Liu, Y. Cao, Y. Lin, Q. Li, Z. Zhang, M. Long, and H. Hu. Negative margin matters: Understanding margin in few-shot classification. In European Conference on Computer Vision, pages 438-455. Springer, 2020.\n\nSphereface: Deep hypersphere embedding for face recognition. W Liu, Y Wen, Z Yu, M Li, B Raj, L Song, IEEE Conference on Computer Vision and Pattern Recognition. W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song. Sphereface: Deep hypersphere embedding for face recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 212-220, 2017.\n\nSGDR: Stochastic gradient descent with warm restarts. I Loshchilov, F Hutter, International Conference on Learning Representations. I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017.\n\nDoes label smoothing mitigate label noise. M Lukasik, S Bhojanapalli, A Menon, S Kumar, International Conference on Machine Learning. PMLRM. Lukasik, S. Bhojanapalli, A. Menon, and S. Kumar. Does label smoothing mitigate label noise? In International Conference on Machine Learning, pages 6448-6458. PMLR, 2020.\n\nIndividual differences among deep neural network models. J Mehrer, C J Spoerer, N Kriegeskorte, T C Kietzmann, Nature Communications. 111J. Mehrer, C. J. Spoerer, N. Kriegeskorte, and T. C. Kietzmann. Individual differences among deep neural network models. Nature Communications, 11(1):1-12, 2020.\n\nGeneralized entropy regularization or: There's nothing special about label smoothing. C Meister, E Salesky, R Cotterell, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsC. Meister, E. Salesky, and R. Cotterell. Generalized entropy regularization or: There's nothing special about label smoothing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6870-6886, 2020.\n\nOn convergence and generalization of dropout training. P Mianjy, R Arora, Advances in Neural Information Processing Systems. 33P. Mianjy and R. Arora. On convergence and generalization of dropout training. Advances in Neural Information Processing Systems, 33, 2020.\n\nWhen does label smoothing help?. R M\u00fcller, S Kornblith, G E Hinton, Advances in Neural Information Processing Systems. R. M\u00fcller, S. Kornblith, and G. E. Hinton. When does label smoothing help? In Advances in Neural Information Processing Systems, pages 4696-4705, 2019.\n\nNorm-based capacity control in neural networks. B Neyshabur, R Tomioka, N Srebro, Conference on Learning Theory. PMLRB. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In Conference on Learning Theory, pages 1376-1401. PMLR, 2015.\n\nA pac-bayesian approach to spectrally-normalized margin bounds for neural networks. B Neyshabur, S Bhojanapalli, N Srebro, International Conference on Learning Representations. B. Neyshabur, S. Bhojanapalli, and N. Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations, 2018.\n\nWhat is being transferred in transfer learning?. B Neyshabur, H Sedghi, C Zhang, Advances in Neural Information Processing Systems. B. Neyshabur, H. Sedghi, and C. Zhang. What is being transferred in transfer learning? In Advances in Neural Information Processing Systems, 2020.\n\nDo wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth. T Nguyen, M Raghu, S Kornblith, International Conference on Learning Representations. T. Nguyen, M. Raghu, and S. Kornblith. Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth. In International Conference on Learning Representations, 2021.\n\nAutomated flower classification over a large number of classes. M.-E Nilsback, A Zisserman, Computer Vision, Graphics & Image Processing. IEEEICVGIP'08. Sixth Indian Conference onM.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In Computer Vision, Graphics & Image Processing, 2008. ICVGIP'08. Sixth Indian Conference on, pages 722-729. IEEE, 2008.\n\nUpdating quasi-newton matrices with limited storage. J , Mathematics of computation. 35151J. Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of computation, 35(151):773-782, 1980.\n\nTadam: Task dependent adaptive metric for improved few-shot learning. B N Oreshkin, P Rodriguez, A Lacoste, arXiv:1805.10123arXiv preprintB. N. Oreshkin, P. Rodriguez, and A. Lacoste. Tadam: Task dependent adaptive metric for improved few-shot learning. arXiv preprint arXiv:1805.10123, 2018.\n\nPrevalence of neural collapse during the terminal phase of deep learning training. V Papyan, X Han, D L Donoho, Proceedings of the National Academy of Sciences. 11740V. Papyan, X. Han, and D. L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40): 24652-24663, 2020.\n\nCats and dogs. O M Parkhi, A Vedaldi, A Zisserman, C Jawahar, IEEE Conference on Computer Vision and Pattern Recognition. IEEEO. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3498-3505. IEEE, 2012.\n\nG Pereyra, G Tucker, J Chorowski, \u0141 Kaiser, G Hinton, arXiv:1701.06548Regularizing neural networks by penalizing confident output distributions. arXiv preprintG. Pereyra, G. Tucker, J. Chorowski, \u0141. Kaiser, and G. Hinton. Regularizing neural networks by penalizing confident output distributions. arXiv preprint arXiv:1701.06548, 2017.\n\nLearning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, arXiv:2103.00020arXiv preprintA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.\n\nTransfusion: Understanding transfer learning for medical imaging. M Raghu, C Zhang, J Kleinberg, S Bengio, Advances in Neural Information Processing Systems. Curran Associates, IncM. Raghu, C. Zhang, J. Kleinberg, and S. Bengio. Transfusion: Understanding transfer learning for medical imaging. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2019.\n\nR Ranjan, C D Castillo, R Chellappa, arXiv:1703.09507L2-constrained softmax loss for discriminative face verification. arXiv preprintR. Ranjan, C. D. Castillo, and R. Chellappa. L2-constrained softmax loss for discriminative face verification. arXiv preprint arXiv:1703.09507, 2017.\n\nDo ImageNet classifiers generalize to ImageNet?. B Recht, R Roelofs, L Schmidt, V Shankar, International Conference on Machine Learning. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning, 2019.\n\nRevisiting training strategies and generalization performance in deep metric learning. K Roth, T Milbich, S Sinha, P Gupta, B Ommer, J P Cohen, International Conference on Machine Learning. PMLRK. Roth, T. Milbich, S. Sinha, P. Gupta, B. Ommer, and J. P. Cohen. Revisiting training strategies and generalization performance in deep metric learning. In International Conference on Machine Learning, pages 8242-8252. PMLR, 2020.\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, International journal of computer vision. 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211-252, 2015.\n\nOn the information bottleneck theory of deep learning. A M Saxe, Y Bansal, J Dapello, M Advani, A Kolchinsky, B D Tracey, D D Cox, Journal of Statistical Mechanics: Theory and Experiment. 201912124020A. M. Saxe, Y. Bansal, J. Dapello, M. Advani, A. Kolchinsky, B. D. Tracey, and D. D. Cox. On the information bottleneck theory of deep learning. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019.\n\nHow deep is deep enough?-quantifying class separability in the hidden layers of deep neural networks. A Schilling, C Metzner, J Rietsch, R Gerum, H Schulze, P Krauss, arXiv:1811.01753arXiv preprintA. Schilling, C. Metzner, J. Rietsch, R. Gerum, H. Schulze, and P. Krauss. How deep is deep enough?-quantifying class separability in the hidden layers of deep neural networks. arXiv preprint arXiv:1811.01753, 2018.\n\nCnn features off-the-shelf: an astounding baseline for recognition. A Sharif Razavian, H Azizpour, J Sullivan, S Carlsson, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsA. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2014.\n\nR Shwartz-Ziv, N Tishby, arXiv:1703.00810Opening the black box of deep neural networks via information. arXiv preprintR. Shwartz-Ziv and N. Tishby. Opening the black box of deep neural networks via information. arXiv preprint arXiv:1703.00810, 2017.\n\nPrototypical networks for few-shot learning. J Snell, K Swersky, R Zemel, Advances in Neural Information Processing Systems. J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. In Advances in Neural Information Processing Systems, pages 4077-4087, 2017.\n\nThe implicit bias of gradient descent on separable data. D Soudry, E Hoffer, M S Nacson, S Gunasekar, N Srebro, The Journal of Machine Learning Research. 191D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822-2878, 2018.\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, The Journal of Machine Learning Research. 151N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929-1958, 2014.\n\nLearning to compare: Relation network for few-shot learning. F Sung, Y Yang, L Zhang, T Xiang, P H Torr, T M Hospedales, IEEE Conference on Computer Vision and Pattern Recognition. F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. Torr, and T. M. Hospedales. Learning to compare: Relation network for few-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1199-1208, 2018.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, IEEE Conference on Computer Vision and Pattern Recognition. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architec- ture for computer vision. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2818-2826, 2016.\n\nMeasuring robustness to natural distribution shifts in image classification. R Taori, A Dave, V Shankar, N Carlini, B Recht, L Schmidt, Advances in Neural Information Processing Systems. R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 2020.\n\nRethinking few-shot image classification: a good embedding is all you need. Y Tian, Y Wang, D Krishnan, J B Tenenbaum, P Isola, European Conference on Computer Vision. Y. Tian, Y. Wang, D. Krishnan, J. B. Tenenbaum, and P. Isola. Rethinking few-shot image classification: a good embedding is all you need? In European Conference on Computer Vision, 2020.\n\nDropout training as adaptive regularization. S Wager, S Wang, P S Liang, Advances in Neural Information Processing Systems. S. Wager, S. Wang, and P. S. Liang. Dropout training as adaptive regularization. In Advances in Neural Information Processing Systems, pages 351-359, 2013.\n\nAltitude training: Strong bounds for single-layer dropout. S Wager, W Fithian, S Wang, P S Liang, Advances in Neural Information Processing Systems. S. Wager, W. Fithian, S. Wang, and P. S. Liang. Altitude training: Strong bounds for single-layer dropout. In Advances in Neural Information Processing Systems, 2014.\n\nNormface: L2 hypersphere embedding for face verification. F Wang, X Xiang, J Cheng, A L Yuille, Proceedings of the 25th ACM international Conference on Multimedia. the 25th ACM international Conference on MultimediaF. Wang, X. Xiang, J. Cheng, and A. L. Yuille. Normface: L2 hypersphere embedding for face verification. In Proceedings of the 25th ACM international Conference on Multimedia, pages 1041-1049, 2017.\n\nAdditive margin softmax for face verification. F Wang, J Cheng, W Liu, H Liu, IEEE Signal Processing Letters. 257F. Wang, J. Cheng, W. Liu, and H. Liu. Additive margin softmax for face verification. IEEE Signal Processing Letters, 25(7):926-930, 2018.\n\nCosface: Large margin cosine loss for deep face recognition. H Wang, Y Wang, Z Zhou, X Ji, D Gong, J Zhou, Z Li, W Liu, IEEE Conference on Computer Vision and Pattern Recognition. H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu. Cosface: Large margin cosine loss for deep face recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5265-5274, 2018.\n\nH Wang, S Ge, E P Xing, Z C Lipton, arXiv:1905.13549Learning robust global representations by penalizing local predictive power. arXiv preprintH. Wang, S. Ge, E. P. Xing, and Z. C. Lipton. Learning robust global representations by penalizing local predictive power. arXiv preprint arXiv:1905.13549, 2019.\n\nRegularization matters: Generalization and optimization of neural nets vs their induced kernel. C Wei, J D Lee, Q Liu, T Ma, Advances in Neural Information Processing Systems. C. Wei, J. D. Lee, Q. Liu, and T. Ma. Regularization matters: Generalization and optimization of neural nets vs their induced kernel. In Advances in Neural Information Processing Systems, pages 9712-9724, 2019.\n\nThe implicit and explicit regularization effects of dropout. C Wei, S Kakade, T Ma, International Conference on Machine Learning. C. Wei, S. Kakade, and T. Ma. The implicit and explicit regularization effects of dropout. In International Conference on Machine Learning, 2020.\n\nDeep cosine metric learning for person re-identification. N Wojke, A Bewley, 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEEN. Wojke and A. Bewley. Deep cosine metric learning for person re-identification. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 748-756. IEEE, 2018.\n\nSun database: Large-scale scene recognition from abbey to zoo. J Xiao, J Hays, K A Ehinger, A Oliva, A Torralba, IEEE Conference on Computer Vision and Pattern Recognition. IEEEJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3485-3492. IEEE, 2010.\n\nY Xu, Y Xu, Q Qian, H Li, R Jin, arXiv:2006.11653Towards understanding label smoothing. arXiv preprintY. Xu, Y. Xu, Q. Qian, H. Li, and R. Jin. Towards understanding label smoothing. arXiv preprint arXiv:2006.11653, 2020.\n\nHow transferable are features in deep neural networks?. J Yosinski, J Clune, Y Bengio, H Lipson, Neural Information Processing Systems. Curran Associates, Inc27J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.\n\nScaling vision transformers. X Zhai, A Kolesnikov, N Houlsby, L Beyer, arXiv:2106.04560arXiv preprintX. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. arXiv preprint arXiv:2106.04560, 2021.\n\nmixup: Beyond empirical risk minimization. H Zhang, M Cisse, Y N Dauphin, D Lopez-Paz, International Conference on Learning Representations. H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk mini- mization. In International Conference on Learning Representations, 2018.\n\nLoss/regularizer ImageNet-V2 (%) ImageNet-A (%) IN-Sketch (%) ImageNet-R (%) ImageNet-C (mCE). Loss/regularizer ImageNet-V2 (%) ImageNet-A (%) IN-Sketch (%) ImageNet-R (%) ImageNet-C (mCE)\n", "annotations": {"author": "[{\"end\":109,\"start\":68},{\"end\":145,\"start\":110},{\"end\":183,\"start\":146},{\"end\":186,\"start\":184},{\"end\":229,\"start\":187}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":74},{\"end\":119,\"start\":115},{\"end\":157,\"start\":154},{\"end\":203,\"start\":196}]", "author_first_name": "[{\"end\":73,\"start\":68},{\"end\":114,\"start\":110},{\"end\":153,\"start\":146},{\"end\":185,\"start\":184},{\"end\":195,\"start\":187}]", "author_affiliation": "[{\"end\":108,\"start\":85},{\"end\":144,\"start\":121},{\"end\":182,\"start\":159},{\"end\":228,\"start\":205}]", "title": "[{\"end\":65,\"start\":1},{\"end\":294,\"start\":230}]", "venue": null, "abstract": "[{\"end\":1695,\"start\":296}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1830,\"start\":1826},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":1833,\"start\":1830},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1836,\"start\":1833},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1914,\"start\":1910},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1917,\"start\":1914},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":1997,\"start\":1993},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2027,\"start\":2024},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2182,\"start\":2178},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5618,\"start\":5614},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5621,\"start\":5618},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":5980,\"start\":5976},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6501,\"start\":6497},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":6681,\"start\":6677},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":7287,\"start\":7283},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":7372,\"start\":7368},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8308,\"start\":8304},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9458,\"start\":9454},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":9461,\"start\":9458},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":9464,\"start\":9461},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":9467,\"start\":9464},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":9470,\"start\":9467},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9473,\"start\":9470},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9476,\"start\":9473},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9479,\"start\":9476},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9600,\"start\":9596},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":9603,\"start\":9600},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":9606,\"start\":9603},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9609,\"start\":9606},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10578,\"start\":10575},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10803,\"start\":10799},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11159,\"start\":11155},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11308,\"start\":11304},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11311,\"start\":11308},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11359,\"start\":11355},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":11362,\"start\":11359},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":11704,\"start\":11700},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13413,\"start\":13410},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13442,\"start\":13438},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13456,\"start\":13453},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":13469,\"start\":13465},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13489,\"start\":13485},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13512,\"start\":13508},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13537,\"start\":13533},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14692,\"start\":14688},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14695,\"start\":14692},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14698,\"start\":14695},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15257,\"start\":15253},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15260,\"start\":15257},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15263,\"start\":15260},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15473,\"start\":15469},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16007,\"start\":16003},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":18308,\"start\":18304},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":18311,\"start\":18308},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":18314,\"start\":18311},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19953,\"start\":19949},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19956,\"start\":19953},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21881,\"start\":21880},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22790,\"start\":22789},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":24577,\"start\":24573},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24663,\"start\":24659},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":24666,\"start\":24663},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":24804,\"start\":24800},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24823,\"start\":24819},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":25001,\"start\":24997},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25004,\"start\":25001},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":25033,\"start\":25029},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":25036,\"start\":25033},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":25070,\"start\":25066},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":25208,\"start\":25204},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25236,\"start\":25232},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25574,\"start\":25570},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25903,\"start\":25899},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":26308,\"start\":26304},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":26311,\"start\":26308},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":26676,\"start\":26672},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":26679,\"start\":26676},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26681,\"start\":26679},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26684,\"start\":26681},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":26687,\"start\":26684},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":26815,\"start\":26811},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26836,\"start\":26832},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":27249,\"start\":27245},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27251,\"start\":27249},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27684,\"start\":27681},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":27687,\"start\":27684},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27689,\"start\":27687},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":27692,\"start\":27689},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27723,\"start\":27719},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":28092,\"start\":28088},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28558,\"start\":28555},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":28605,\"start\":28601},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":28608,\"start\":28605},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28611,\"start\":28608},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":28637,\"start\":28633},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28674,\"start\":28670},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28701,\"start\":28698},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28704,\"start\":28701},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":28838,\"start\":28834},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":29126,\"start\":29122},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29435,\"start\":29431},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29592,\"start\":29588},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":32098,\"start\":32094},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32101,\"start\":32098},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32104,\"start\":32101},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":32484,\"start\":32480},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":32630,\"start\":32628},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32632,\"start\":32631},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":33116,\"start\":33112},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":33168,\"start\":33164},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":33171,\"start\":33168},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34061,\"start\":34060},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":34214,\"start\":34210},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34273,\"start\":34269},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34523,\"start\":34519},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34526,\"start\":34525},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34915,\"start\":34911},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":35114,\"start\":35110},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":35989,\"start\":35985},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":37423,\"start\":37419},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":37440,\"start\":37436},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":37462,\"start\":37458},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":37479,\"start\":37475},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37500,\"start\":37496},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":37811,\"start\":37807},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":37854,\"start\":37850},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":38472,\"start\":38468},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":40143,\"start\":40139},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":40146,\"start\":40143},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":40675,\"start\":40671},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":41587,\"start\":41583},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":41754,\"start\":41750},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":42865,\"start\":42861},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":45505,\"start\":45501},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46321,\"start\":46317},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":46547,\"start\":46543},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":46577,\"start\":46573},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":46642,\"start\":46638},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":46779,\"start\":46775},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":50502,\"start\":50498},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":62013,\"start\":62009},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":82377,\"start\":82376}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53306,\"start\":53103},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54197,\"start\":53307},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54485,\"start\":54198},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54738,\"start\":54486},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55164,\"start\":54739},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55183,\"start\":55165},{\"attributes\":{\"id\":\"fig_7\"},\"end\":55923,\"start\":55184},{\"attributes\":{\"id\":\"fig_8\"},\"end\":56561,\"start\":55924},{\"attributes\":{\"id\":\"fig_9\"},\"end\":56777,\"start\":56562},{\"attributes\":{\"id\":\"fig_10\"},\"end\":56977,\"start\":56778},{\"attributes\":{\"id\":\"fig_11\"},\"end\":57292,\"start\":56978},{\"attributes\":{\"id\":\"fig_12\"},\"end\":60066,\"start\":57293},{\"attributes\":{\"id\":\"fig_13\"},\"end\":61077,\"start\":60067},{\"attributes\":{\"id\":\"fig_14\"},\"end\":61797,\"start\":61078},{\"attributes\":{\"id\":\"fig_15\"},\"end\":61885,\"start\":61798},{\"attributes\":{\"id\":\"fig_16\"},\"end\":62442,\"start\":61886},{\"attributes\":{\"id\":\"fig_17\"},\"end\":63284,\"start\":62443},{\"attributes\":{\"id\":\"fig_18\"},\"end\":63655,\"start\":63285},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":65536,\"start\":63656},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":66667,\"start\":65537},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":67535,\"start\":66668},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":68270,\"start\":67536},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":70029,\"start\":68271},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":70576,\"start\":70030},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":71051,\"start\":70577},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":75198,\"start\":71052},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":76379,\"start\":75199},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":76538,\"start\":76380},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":76546,\"start\":76539},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":77386,\"start\":76547},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":77695,\"start\":77387},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":80118,\"start\":77696},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":81074,\"start\":80119},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":81761,\"start\":81075},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":82164,\"start\":81762}]", "paragraph": "[{\"end\":3226,\"start\":1711},{\"end\":3921,\"start\":3228},{\"end\":4634,\"start\":3923},{\"end\":5061,\"start\":4683},{\"end\":5162,\"start\":5063},{\"end\":5590,\"start\":5164},{\"end\":5700,\"start\":5592},{\"end\":5958,\"start\":5794},{\"end\":6404,\"start\":5960},{\"end\":6637,\"start\":6483},{\"end\":7088,\"start\":6639},{\"end\":7373,\"start\":7185},{\"end\":7639,\"start\":7375},{\"end\":7962,\"start\":7712},{\"end\":8228,\"start\":7964},{\"end\":8284,\"start\":8230},{\"end\":8389,\"start\":8286},{\"end\":8795,\"start\":8391},{\"end\":9254,\"start\":8901},{\"end\":9355,\"start\":9289},{\"end\":9738,\"start\":9357},{\"end\":10221,\"start\":9740},{\"end\":10706,\"start\":10349},{\"end\":10804,\"start\":10708},{\"end\":11254,\"start\":10878},{\"end\":11726,\"start\":11266},{\"end\":13025,\"start\":11728},{\"end\":14272,\"start\":13027},{\"end\":15065,\"start\":14361},{\"end\":15746,\"start\":15067},{\"end\":16094,\"start\":15820},{\"end\":18011,\"start\":16096},{\"end\":18548,\"start\":18013},{\"end\":19546,\"start\":18616},{\"end\":20089,\"start\":19548},{\"end\":21032,\"start\":20271},{\"end\":21699,\"start\":21107},{\"end\":22465,\"start\":21701},{\"end\":23239,\"start\":22467},{\"end\":24099,\"start\":23241},{\"end\":25400,\"start\":24116},{\"end\":26552,\"start\":25402},{\"end\":27429,\"start\":26554},{\"end\":28928,\"start\":27431},{\"end\":29715,\"start\":28930},{\"end\":30431,\"start\":29731},{\"end\":30863,\"start\":30433},{\"end\":31558,\"start\":30878},{\"end\":32954,\"start\":31560},{\"end\":34917,\"start\":33026},{\"end\":35831,\"start\":34937},{\"end\":36987,\"start\":35885},{\"end\":38794,\"start\":37117},{\"end\":38836,\"start\":38811},{\"end\":39756,\"start\":38876},{\"end\":40446,\"start\":39813},{\"end\":41594,\"start\":40448},{\"end\":42576,\"start\":41633},{\"end\":43521,\"start\":42578},{\"end\":43978,\"start\":43623},{\"end\":44244,\"start\":43980},{\"end\":44569,\"start\":44393},{\"end\":44800,\"start\":44571},{\"end\":44970,\"start\":44876},{\"end\":45019,\"start\":45003},{\"end\":45076,\"start\":45045},{\"end\":45188,\"start\":45101},{\"end\":45354,\"start\":45280},{\"end\":45690,\"start\":45465},{\"end\":45765,\"start\":45750},{\"end\":45854,\"start\":45803},{\"end\":46153,\"start\":45856},{\"end\":46208,\"start\":46178},{\"end\":46387,\"start\":46210},{\"end\":47939,\"start\":46423},{\"end\":48492,\"start\":48166},{\"end\":48599,\"start\":48549},{\"end\":49214,\"start\":48780},{\"end\":50121,\"start\":49260},{\"end\":50503,\"start\":50180},{\"end\":51117,\"start\":50505},{\"end\":51597,\"start\":51119},{\"end\":52634,\"start\":51599},{\"end\":53102,\"start\":52636}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5793,\"start\":5701},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6482,\"start\":6405},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7184,\"start\":7089},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7711,\"start\":7640},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8900,\"start\":8796},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9288,\"start\":9255},{\"attributes\":{\"id\":\"formula_8\"},\"end\":10348,\"start\":10222},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10877,\"start\":10805},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15819,\"start\":15747},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20270,\"start\":20090},{\"attributes\":{\"id\":\"formula_12\"},\"end\":44392,\"start\":44245},{\"attributes\":{\"id\":\"formula_13\"},\"end\":44875,\"start\":44801},{\"attributes\":{\"id\":\"formula_14\"},\"end\":45002,\"start\":44971},{\"attributes\":{\"id\":\"formula_15\"},\"end\":45044,\"start\":45020},{\"attributes\":{\"id\":\"formula_16\"},\"end\":45100,\"start\":45077},{\"attributes\":{\"id\":\"formula_17\"},\"end\":45279,\"start\":45189},{\"attributes\":{\"id\":\"formula_18\"},\"end\":45464,\"start\":45355},{\"attributes\":{\"id\":\"formula_19\"},\"end\":45749,\"start\":45691},{\"attributes\":{\"id\":\"formula_20\"},\"end\":45802,\"start\":45766},{\"attributes\":{\"id\":\"formula_21\"},\"end\":46177,\"start\":46154},{\"attributes\":{\"id\":\"formula_23\"},\"end\":46422,\"start\":46388},{\"attributes\":{\"id\":\"formula_24\"},\"end\":48080,\"start\":47940},{\"attributes\":{\"id\":\"formula_25\"},\"end\":48165,\"start\":48134},{\"attributes\":{\"id\":\"formula_26\"},\"end\":48548,\"start\":48493},{\"attributes\":{\"id\":\"formula_27\"},\"end\":48703,\"start\":48600}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":3046,\"start\":3039},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":11563,\"start\":11364},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11802,\"start\":11795},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12731,\"start\":12724},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13592,\"start\":13585},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13747,\"start\":13740},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20290,\"start\":20283},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21901,\"start\":21894},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23448,\"start\":23434},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26332,\"start\":26325},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28410,\"start\":28402},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34007,\"start\":34000},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34978,\"start\":34971},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":36316,\"start\":36309},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":36409,\"start\":36402},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":36513,\"start\":36506},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37317,\"start\":37308},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37726,\"start\":37719},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":38122,\"start\":38115},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40022,\"start\":40015},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40108,\"start\":40101},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40608,\"start\":40600},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40968,\"start\":40961},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41643,\"start\":41636},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":48983,\"start\":48976},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49496,\"start\":49488},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":52664,\"start\":52657}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1709,\"start\":1697},{\"attributes\":{\"n\":\"2\"},\"end\":4681,\"start\":4637},{\"attributes\":{\"n\":\"3\"},\"end\":11264,\"start\":11257},{\"attributes\":{\"n\":\"3.2\"},\"end\":14359,\"start\":14275},{\"attributes\":{\"n\":\"3.3\"},\"end\":18614,\"start\":18551},{\"attributes\":{\"n\":\"3.4\"},\"end\":21105,\"start\":21035},{\"attributes\":{\"n\":\"4\"},\"end\":24114,\"start\":24102},{\"attributes\":{\"n\":\"5\"},\"end\":29729,\"start\":29718},{\"attributes\":{\"n\":\"6\"},\"end\":30876,\"start\":30866},{\"end\":33024,\"start\":32957},{\"end\":34935,\"start\":34920},{\"end\":35883,\"start\":35834},{\"end\":37082,\"start\":36990},{\"end\":37115,\"start\":37085},{\"end\":38809,\"start\":38797},{\"end\":38874,\"start\":38839},{\"end\":39811,\"start\":39759},{\"end\":41605,\"start\":41597},{\"end\":41631,\"start\":41608},{\"end\":43561,\"start\":43524},{\"end\":43621,\"start\":43564},{\"end\":48133,\"start\":48082},{\"end\":48778,\"start\":48705},{\"end\":49258,\"start\":49217},{\"end\":50178,\"start\":50124},{\"end\":53114,\"start\":53104},{\"end\":53318,\"start\":53308},{\"end\":54209,\"start\":54199},{\"end\":54497,\"start\":54487},{\"end\":55176,\"start\":55166},{\"end\":55195,\"start\":55185},{\"end\":55938,\"start\":55925},{\"end\":56576,\"start\":56563},{\"end\":56792,\"start\":56779},{\"end\":56992,\"start\":56979},{\"end\":57346,\"start\":57294},{\"end\":60074,\"start\":60068},{\"end\":61105,\"start\":61079},{\"end\":61812,\"start\":61799},{\"end\":61900,\"start\":61887},{\"end\":62470,\"start\":62444},{\"end\":63299,\"start\":63286},{\"end\":63666,\"start\":63657},{\"end\":65547,\"start\":65538},{\"end\":66678,\"start\":66669},{\"end\":67546,\"start\":67537},{\"end\":70044,\"start\":70031},{\"end\":70591,\"start\":70578},{\"end\":71066,\"start\":71053},{\"end\":75213,\"start\":75200},{\"end\":76394,\"start\":76381},{\"end\":76545,\"start\":76540},{\"end\":76561,\"start\":76548},{\"end\":77397,\"start\":77388},{\"end\":77710,\"start\":77697},{\"end\":80133,\"start\":80120},{\"end\":81089,\"start\":81076},{\"end\":81776,\"start\":81763}]", "table": "[{\"end\":65536,\"start\":63668},{\"end\":66667,\"start\":65995},{\"end\":67535,\"start\":66852},{\"end\":68270,\"start\":67559},{\"end\":70029,\"start\":69717},{\"end\":70576,\"start\":70089},{\"end\":71051,\"start\":70633},{\"end\":75198,\"start\":71785},{\"end\":76379,\"start\":75671},{\"end\":77386,\"start\":76983},{\"end\":80118,\"start\":78220},{\"end\":81074,\"start\":80416},{\"end\":81761,\"start\":81376},{\"end\":82164,\"start\":81832}]", "figure_caption": "[{\"end\":53306,\"start\":53116},{\"end\":54197,\"start\":53320},{\"end\":54485,\"start\":54211},{\"end\":54738,\"start\":54499},{\"end\":55164,\"start\":54741},{\"end\":55183,\"start\":55178},{\"end\":55923,\"start\":55197},{\"end\":56561,\"start\":55940},{\"end\":56777,\"start\":56578},{\"end\":56977,\"start\":56794},{\"end\":57292,\"start\":56994},{\"end\":60066,\"start\":57351},{\"end\":61077,\"start\":60076},{\"end\":61797,\"start\":61108},{\"end\":61885,\"start\":61814},{\"end\":62442,\"start\":61902},{\"end\":63284,\"start\":62473},{\"end\":63655,\"start\":63301},{\"end\":65995,\"start\":65549},{\"end\":66852,\"start\":66680},{\"end\":67559,\"start\":67548},{\"end\":69717,\"start\":68273},{\"end\":70089,\"start\":70047},{\"end\":70633,\"start\":70594},{\"end\":71785,\"start\":71069},{\"end\":75671,\"start\":75216},{\"end\":76538,\"start\":76397},{\"end\":76983,\"start\":76564},{\"end\":77695,\"start\":77399},{\"end\":78220,\"start\":77713},{\"end\":80416,\"start\":80136},{\"end\":81376,\"start\":81092},{\"end\":81832,\"start\":81779}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3059,\"start\":3051},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13605,\"start\":13597},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16116,\"start\":16108},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16263,\"start\":16254},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17008,\"start\":16999},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17362,\"start\":17354},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20303,\"start\":20295},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21283,\"start\":21274},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22380,\"start\":22371},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":22589,\"start\":22581},{\"end\":27966,\"start\":27958},{\"end\":36603,\"start\":36595},{\"end\":39276,\"start\":39268},{\"end\":39755,\"start\":39749},{\"end\":49682,\"start\":49674}]", "bib_author_first_name": "[{\"end\":82965,\"start\":82964},{\"end\":82974,\"start\":82973},{\"end\":82986,\"start\":82985},{\"end\":82999,\"start\":82998},{\"end\":83247,\"start\":83246},{\"end\":83256,\"start\":83255},{\"end\":83495,\"start\":83494},{\"end\":83506,\"start\":83505},{\"end\":83514,\"start\":83513},{\"end\":83516,\"start\":83515},{\"end\":83525,\"start\":83524},{\"end\":83867,\"start\":83866},{\"end\":83869,\"start\":83868},{\"end\":84176,\"start\":84175},{\"end\":84178,\"start\":84177},{\"end\":84190,\"start\":84189},{\"end\":84192,\"start\":84191},{\"end\":84202,\"start\":84201},{\"end\":84512,\"start\":84511},{\"end\":84520,\"start\":84519},{\"end\":84527,\"start\":84526},{\"end\":84529,\"start\":84528},{\"end\":84536,\"start\":84535},{\"end\":84538,\"start\":84537},{\"end\":84551,\"start\":84550},{\"end\":84553,\"start\":84552},{\"end\":84563,\"start\":84562},{\"end\":84565,\"start\":84564},{\"end\":84937,\"start\":84936},{\"end\":84951,\"start\":84950},{\"end\":84953,\"start\":84952},{\"end\":84966,\"start\":84965},{\"end\":84974,\"start\":84973},{\"end\":85215,\"start\":85214},{\"end\":85224,\"start\":85223},{\"end\":85226,\"start\":85225},{\"end\":85236,\"start\":85235},{\"end\":85250,\"start\":85249},{\"end\":85258,\"start\":85257},{\"end\":85262,\"start\":85259},{\"end\":85527,\"start\":85526},{\"end\":85538,\"start\":85537},{\"end\":85552,\"start\":85551},{\"end\":85926,\"start\":85925},{\"end\":85928,\"start\":85927},{\"end\":86284,\"start\":86283},{\"end\":86286,\"start\":86285},{\"end\":86588,\"start\":86587},{\"end\":86596,\"start\":86595},{\"end\":86600,\"start\":86597},{\"end\":86608,\"start\":86607},{\"end\":86614,\"start\":86613},{\"end\":86629,\"start\":86628},{\"end\":86638,\"start\":86637},{\"end\":86919,\"start\":86918},{\"end\":86927,\"start\":86926},{\"end\":86936,\"start\":86935},{\"end\":86944,\"start\":86943},{\"end\":86946,\"start\":86945},{\"end\":87205,\"start\":87204},{\"end\":87213,\"start\":87212},{\"end\":87226,\"start\":87225},{\"end\":87237,\"start\":87236},{\"end\":87515,\"start\":87511},{\"end\":87526,\"start\":87522},{\"end\":87533,\"start\":87532},{\"end\":87544,\"start\":87540},{\"end\":87546,\"start\":87545},{\"end\":87557,\"start\":87553},{\"end\":87861,\"start\":87860},{\"end\":87870,\"start\":87869},{\"end\":87879,\"start\":87878},{\"end\":87881,\"start\":87880},{\"end\":87888,\"start\":87887},{\"end\":88154,\"start\":88153},{\"end\":88164,\"start\":88163},{\"end\":88173,\"start\":88172},{\"end\":88429,\"start\":88428},{\"end\":88444,\"start\":88443},{\"end\":88460,\"start\":88459},{\"end\":88473,\"start\":88472},{\"end\":88475,\"start\":88474},{\"end\":88764,\"start\":88763},{\"end\":88766,\"start\":88765},{\"end\":88775,\"start\":88774},{\"end\":88783,\"start\":88782},{\"end\":88791,\"start\":88790},{\"end\":88804,\"start\":88803},{\"end\":88806,\"start\":88805},{\"end\":89121,\"start\":89120},{\"end\":89132,\"start\":89131},{\"end\":89134,\"start\":89133},{\"end\":89390,\"start\":89389},{\"end\":89398,\"start\":89397},{\"end\":89406,\"start\":89405},{\"end\":89419,\"start\":89415},{\"end\":89425,\"start\":89424},{\"end\":89431,\"start\":89430},{\"end\":89777,\"start\":89776},{\"end\":89785,\"start\":89784},{\"end\":89792,\"start\":89791},{\"end\":89799,\"start\":89798},{\"end\":90121,\"start\":90120},{\"end\":90132,\"start\":90131},{\"end\":90141,\"start\":90140},{\"end\":90397,\"start\":90396},{\"end\":90408,\"start\":90407},{\"end\":90415,\"start\":90414},{\"end\":90426,\"start\":90425},{\"end\":90437,\"start\":90436},{\"end\":90446,\"start\":90445},{\"end\":90455,\"start\":90454},{\"end\":90804,\"start\":90803},{\"end\":90814,\"start\":90813},{\"end\":90826,\"start\":90825},{\"end\":91164,\"start\":91163},{\"end\":91176,\"start\":91175},{\"end\":91185,\"start\":91184},{\"end\":91193,\"start\":91192},{\"end\":91199,\"start\":91198},{\"end\":91214,\"start\":91213},{\"end\":91567,\"start\":91566},{\"end\":91579,\"start\":91578},{\"end\":91587,\"start\":91586},{\"end\":91601,\"start\":91600},{\"end\":91611,\"start\":91610},{\"end\":91621,\"start\":91620},{\"end\":91634,\"start\":91633},{\"end\":91903,\"start\":91902},{\"end\":91912,\"start\":91911},{\"end\":91922,\"start\":91921},{\"end\":91934,\"start\":91933},{\"end\":91947,\"start\":91946},{\"end\":91961,\"start\":91960},{\"end\":91971,\"start\":91970},{\"end\":91982,\"start\":91981},{\"end\":91989,\"start\":91988},{\"end\":92332,\"start\":92331},{\"end\":92341,\"start\":92340},{\"end\":92574,\"start\":92573},{\"end\":92587,\"start\":92586},{\"end\":92589,\"start\":92588},{\"end\":92596,\"start\":92595},{\"end\":92606,\"start\":92605},{\"end\":92906,\"start\":92905},{\"end\":92913,\"start\":92912},{\"end\":92923,\"start\":92922},{\"end\":92930,\"start\":92929},{\"end\":92932,\"start\":92931},{\"end\":93202,\"start\":93201},{\"end\":93208,\"start\":93207},{\"end\":93217,\"start\":93216},{\"end\":93224,\"start\":93223},{\"end\":93539,\"start\":93538},{\"end\":93552,\"start\":93551},{\"end\":93795,\"start\":93794},{\"end\":93808,\"start\":93807},{\"end\":93816,\"start\":93815},{\"end\":93826,\"start\":93825},{\"end\":93840,\"start\":93839},{\"end\":94129,\"start\":94128},{\"end\":94142,\"start\":94141},{\"end\":94152,\"start\":94151},{\"end\":94158,\"start\":94157},{\"end\":94170,\"start\":94169},{\"end\":94178,\"start\":94177},{\"end\":94189,\"start\":94188},{\"end\":94198,\"start\":94197},{\"end\":94205,\"start\":94204},{\"end\":94217,\"start\":94216},{\"end\":94598,\"start\":94597},{\"end\":94605,\"start\":94604},{\"end\":94948,\"start\":94947},{\"end\":94957,\"start\":94956},{\"end\":94970,\"start\":94969},{\"end\":94976,\"start\":94975},{\"end\":94982,\"start\":94981},{\"end\":94998,\"start\":94997},{\"end\":95007,\"start\":95006},{\"end\":95019,\"start\":95018},{\"end\":95030,\"start\":95029},{\"end\":95038,\"start\":95037},{\"end\":95535,\"start\":95534},{\"end\":95546,\"start\":95545},{\"end\":95548,\"start\":95547},{\"end\":95745,\"start\":95744},{\"end\":95755,\"start\":95754},{\"end\":95767,\"start\":95766},{\"end\":95775,\"start\":95774},{\"end\":95784,\"start\":95783},{\"end\":95792,\"start\":95791},{\"end\":95801,\"start\":95800},{\"end\":95814,\"start\":95813},{\"end\":95821,\"start\":95820},{\"end\":96135,\"start\":96134},{\"end\":96148,\"start\":96147},{\"end\":96159,\"start\":96158},{\"end\":96166,\"start\":96165},{\"end\":96426,\"start\":96425},{\"end\":96439,\"start\":96438},{\"end\":96449,\"start\":96448},{\"end\":96451,\"start\":96450},{\"end\":96741,\"start\":96740},{\"end\":96751,\"start\":96750},{\"end\":96759,\"start\":96758},{\"end\":96768,\"start\":96767},{\"end\":97055,\"start\":97054},{\"end\":97069,\"start\":97068},{\"end\":97322,\"start\":97321},{\"end\":97329,\"start\":97328},{\"end\":97336,\"start\":97335},{\"end\":97343,\"start\":97342},{\"end\":97349,\"start\":97348},{\"end\":97358,\"start\":97357},{\"end\":97366,\"start\":97365},{\"end\":97691,\"start\":97690},{\"end\":97698,\"start\":97697},{\"end\":97705,\"start\":97704},{\"end\":97711,\"start\":97710},{\"end\":97717,\"start\":97716},{\"end\":97724,\"start\":97723},{\"end\":98043,\"start\":98042},{\"end\":98057,\"start\":98056},{\"end\":98311,\"start\":98310},{\"end\":98322,\"start\":98321},{\"end\":98338,\"start\":98337},{\"end\":98347,\"start\":98346},{\"end\":98638,\"start\":98637},{\"end\":98648,\"start\":98647},{\"end\":98650,\"start\":98649},{\"end\":98661,\"start\":98660},{\"end\":98677,\"start\":98676},{\"end\":98679,\"start\":98678},{\"end\":98967,\"start\":98966},{\"end\":98978,\"start\":98977},{\"end\":98989,\"start\":98988},{\"end\":99462,\"start\":99461},{\"end\":99472,\"start\":99471},{\"end\":99708,\"start\":99707},{\"end\":99718,\"start\":99717},{\"end\":99731,\"start\":99730},{\"end\":99733,\"start\":99732},{\"end\":99995,\"start\":99994},{\"end\":100008,\"start\":100007},{\"end\":100019,\"start\":100018},{\"end\":100301,\"start\":100300},{\"end\":100314,\"start\":100313},{\"end\":100330,\"start\":100329},{\"end\":100637,\"start\":100636},{\"end\":100650,\"start\":100649},{\"end\":100660,\"start\":100659},{\"end\":100990,\"start\":100989},{\"end\":101000,\"start\":100999},{\"end\":101009,\"start\":101008},{\"end\":101368,\"start\":101364},{\"end\":101380,\"start\":101379},{\"end\":101752,\"start\":101751},{\"end\":101976,\"start\":101975},{\"end\":101978,\"start\":101977},{\"end\":101990,\"start\":101989},{\"end\":102003,\"start\":102002},{\"end\":102283,\"start\":102282},{\"end\":102293,\"start\":102292},{\"end\":102300,\"start\":102299},{\"end\":102302,\"start\":102301},{\"end\":102579,\"start\":102578},{\"end\":102581,\"start\":102580},{\"end\":102591,\"start\":102590},{\"end\":102602,\"start\":102601},{\"end\":102615,\"start\":102614},{\"end\":102854,\"start\":102853},{\"end\":102865,\"start\":102864},{\"end\":102875,\"start\":102874},{\"end\":102888,\"start\":102887},{\"end\":102898,\"start\":102897},{\"end\":103262,\"start\":103261},{\"end\":103273,\"start\":103272},{\"end\":103275,\"start\":103274},{\"end\":103282,\"start\":103281},{\"end\":103293,\"start\":103292},{\"end\":103303,\"start\":103302},{\"end\":103310,\"start\":103309},{\"end\":103321,\"start\":103320},{\"end\":103331,\"start\":103330},{\"end\":103341,\"start\":103340},{\"end\":103352,\"start\":103351},{\"end\":103685,\"start\":103684},{\"end\":103694,\"start\":103693},{\"end\":103703,\"start\":103702},{\"end\":103716,\"start\":103715},{\"end\":104000,\"start\":103999},{\"end\":104010,\"start\":104009},{\"end\":104012,\"start\":104011},{\"end\":104024,\"start\":104023},{\"end\":104333,\"start\":104332},{\"end\":104342,\"start\":104341},{\"end\":104353,\"start\":104352},{\"end\":104364,\"start\":104363},{\"end\":104662,\"start\":104661},{\"end\":104670,\"start\":104669},{\"end\":104681,\"start\":104680},{\"end\":104690,\"start\":104689},{\"end\":104699,\"start\":104698},{\"end\":104708,\"start\":104707},{\"end\":104710,\"start\":104709},{\"end\":105054,\"start\":105053},{\"end\":105069,\"start\":105068},{\"end\":105077,\"start\":105076},{\"end\":105083,\"start\":105082},{\"end\":105093,\"start\":105092},{\"end\":105105,\"start\":105104},{\"end\":105111,\"start\":105110},{\"end\":105120,\"start\":105119},{\"end\":105132,\"start\":105131},{\"end\":105142,\"start\":105141},{\"end\":105490,\"start\":105489},{\"end\":105492,\"start\":105491},{\"end\":105500,\"start\":105499},{\"end\":105510,\"start\":105509},{\"end\":105521,\"start\":105520},{\"end\":105531,\"start\":105530},{\"end\":105545,\"start\":105544},{\"end\":105547,\"start\":105546},{\"end\":105557,\"start\":105556},{\"end\":105559,\"start\":105558},{\"end\":105963,\"start\":105962},{\"end\":105976,\"start\":105975},{\"end\":105987,\"start\":105986},{\"end\":105998,\"start\":105997},{\"end\":106007,\"start\":106006},{\"end\":106018,\"start\":106017},{\"end\":106343,\"start\":106342},{\"end\":106362,\"start\":106361},{\"end\":106374,\"start\":106373},{\"end\":106386,\"start\":106385},{\"end\":106789,\"start\":106788},{\"end\":106804,\"start\":106803},{\"end\":107085,\"start\":107084},{\"end\":107094,\"start\":107093},{\"end\":107105,\"start\":107104},{\"end\":107381,\"start\":107380},{\"end\":107391,\"start\":107390},{\"end\":107401,\"start\":107400},{\"end\":107403,\"start\":107402},{\"end\":107413,\"start\":107412},{\"end\":107426,\"start\":107425},{\"end\":107736,\"start\":107735},{\"end\":107750,\"start\":107749},{\"end\":107760,\"start\":107759},{\"end\":107774,\"start\":107773},{\"end\":107787,\"start\":107786},{\"end\":108120,\"start\":108119},{\"end\":108128,\"start\":108127},{\"end\":108136,\"start\":108135},{\"end\":108145,\"start\":108144},{\"end\":108154,\"start\":108153},{\"end\":108156,\"start\":108155},{\"end\":108164,\"start\":108163},{\"end\":108166,\"start\":108165},{\"end\":108519,\"start\":108518},{\"end\":108530,\"start\":108529},{\"end\":108543,\"start\":108542},{\"end\":108552,\"start\":108551},{\"end\":108562,\"start\":108561},{\"end\":108917,\"start\":108916},{\"end\":108926,\"start\":108925},{\"end\":108934,\"start\":108933},{\"end\":108945,\"start\":108944},{\"end\":108956,\"start\":108955},{\"end\":108965,\"start\":108964},{\"end\":109307,\"start\":109306},{\"end\":109315,\"start\":109314},{\"end\":109323,\"start\":109322},{\"end\":109335,\"start\":109334},{\"end\":109337,\"start\":109336},{\"end\":109350,\"start\":109349},{\"end\":109632,\"start\":109631},{\"end\":109641,\"start\":109640},{\"end\":109649,\"start\":109648},{\"end\":109651,\"start\":109650},{\"end\":109927,\"start\":109926},{\"end\":109936,\"start\":109935},{\"end\":109947,\"start\":109946},{\"end\":109955,\"start\":109954},{\"end\":109957,\"start\":109956},{\"end\":110243,\"start\":110242},{\"end\":110251,\"start\":110250},{\"end\":110260,\"start\":110259},{\"end\":110269,\"start\":110268},{\"end\":110271,\"start\":110270},{\"end\":110647,\"start\":110646},{\"end\":110655,\"start\":110654},{\"end\":110664,\"start\":110663},{\"end\":110671,\"start\":110670},{\"end\":110914,\"start\":110913},{\"end\":110922,\"start\":110921},{\"end\":110930,\"start\":110929},{\"end\":110938,\"start\":110937},{\"end\":110944,\"start\":110943},{\"end\":110952,\"start\":110951},{\"end\":110960,\"start\":110959},{\"end\":110966,\"start\":110965},{\"end\":111252,\"start\":111251},{\"end\":111260,\"start\":111259},{\"end\":111266,\"start\":111265},{\"end\":111268,\"start\":111267},{\"end\":111276,\"start\":111275},{\"end\":111278,\"start\":111277},{\"end\":111654,\"start\":111653},{\"end\":111661,\"start\":111660},{\"end\":111663,\"start\":111662},{\"end\":111670,\"start\":111669},{\"end\":111677,\"start\":111676},{\"end\":112007,\"start\":112006},{\"end\":112014,\"start\":112013},{\"end\":112024,\"start\":112023},{\"end\":112281,\"start\":112280},{\"end\":112290,\"start\":112289},{\"end\":112622,\"start\":112621},{\"end\":112630,\"start\":112629},{\"end\":112638,\"start\":112637},{\"end\":112640,\"start\":112639},{\"end\":112651,\"start\":112650},{\"end\":112660,\"start\":112659},{\"end\":112952,\"start\":112951},{\"end\":112958,\"start\":112957},{\"end\":112964,\"start\":112963},{\"end\":112972,\"start\":112971},{\"end\":112978,\"start\":112977},{\"end\":113231,\"start\":113230},{\"end\":113243,\"start\":113242},{\"end\":113252,\"start\":113251},{\"end\":113262,\"start\":113261},{\"end\":113553,\"start\":113552},{\"end\":113561,\"start\":113560},{\"end\":113575,\"start\":113574},{\"end\":113586,\"start\":113585},{\"end\":113787,\"start\":113786},{\"end\":113796,\"start\":113795},{\"end\":113805,\"start\":113804},{\"end\":113807,\"start\":113806},{\"end\":113818,\"start\":113817}]", "bib_author_last_name": "[{\"end\":82971,\"start\":82966},{\"end\":82983,\"start\":82975},{\"end\":82996,\"start\":82987},{\"end\":83006,\"start\":83000},{\"end\":83253,\"start\":83248},{\"end\":83263,\"start\":83257},{\"end\":83503,\"start\":83496},{\"end\":83511,\"start\":83507},{\"end\":83522,\"start\":83517},{\"end\":83534,\"start\":83526},{\"end\":83878,\"start\":83870},{\"end\":84187,\"start\":84179},{\"end\":84199,\"start\":84193},{\"end\":84212,\"start\":84203},{\"end\":84517,\"start\":84513},{\"end\":84524,\"start\":84521},{\"end\":84533,\"start\":84530},{\"end\":84548,\"start\":84539},{\"end\":84560,\"start\":84554},{\"end\":84575,\"start\":84566},{\"end\":84948,\"start\":84938},{\"end\":84963,\"start\":84954},{\"end\":84971,\"start\":84967},{\"end\":84982,\"start\":84975},{\"end\":85221,\"start\":85216},{\"end\":85233,\"start\":85227},{\"end\":85247,\"start\":85237},{\"end\":85255,\"start\":85251},{\"end\":85267,\"start\":85263},{\"end\":85535,\"start\":85528},{\"end\":85549,\"start\":85539},{\"end\":85561,\"start\":85553},{\"end\":85935,\"start\":85929},{\"end\":86293,\"start\":86287},{\"end\":86593,\"start\":86589},{\"end\":86605,\"start\":86601},{\"end\":86611,\"start\":86609},{\"end\":86626,\"start\":86615},{\"end\":86635,\"start\":86630},{\"end\":86649,\"start\":86639},{\"end\":86924,\"start\":86920},{\"end\":86933,\"start\":86928},{\"end\":86941,\"start\":86937},{\"end\":86952,\"start\":86947},{\"end\":87210,\"start\":87206},{\"end\":87223,\"start\":87214},{\"end\":87234,\"start\":87227},{\"end\":87244,\"start\":87238},{\"end\":87520,\"start\":87516},{\"end\":87530,\"start\":87527},{\"end\":87538,\"start\":87534},{\"end\":87551,\"start\":87547},{\"end\":87563,\"start\":87558},{\"end\":87867,\"start\":87862},{\"end\":87876,\"start\":87871},{\"end\":87885,\"start\":87882},{\"end\":87900,\"start\":87889},{\"end\":88161,\"start\":88155},{\"end\":88170,\"start\":88165},{\"end\":88186,\"start\":88174},{\"end\":88441,\"start\":88430},{\"end\":88457,\"start\":88445},{\"end\":88470,\"start\":88461},{\"end\":88483,\"start\":88476},{\"end\":88772,\"start\":88767},{\"end\":88780,\"start\":88776},{\"end\":88788,\"start\":88784},{\"end\":88801,\"start\":88792},{\"end\":88809,\"start\":88807},{\"end\":89129,\"start\":89122},{\"end\":89140,\"start\":89135},{\"end\":89395,\"start\":89391},{\"end\":89403,\"start\":89399},{\"end\":89413,\"start\":89407},{\"end\":89422,\"start\":89420},{\"end\":89428,\"start\":89426},{\"end\":89439,\"start\":89432},{\"end\":89782,\"start\":89778},{\"end\":89789,\"start\":89786},{\"end\":89796,\"start\":89793},{\"end\":89809,\"start\":89800},{\"end\":90129,\"start\":90122},{\"end\":90138,\"start\":90133},{\"end\":90151,\"start\":90142},{\"end\":90405,\"start\":90398},{\"end\":90412,\"start\":90409},{\"end\":90423,\"start\":90416},{\"end\":90434,\"start\":90427},{\"end\":90443,\"start\":90438},{\"end\":90452,\"start\":90447},{\"end\":90463,\"start\":90456},{\"end\":90811,\"start\":90805},{\"end\":90823,\"start\":90815},{\"end\":90833,\"start\":90827},{\"end\":91173,\"start\":91165},{\"end\":91182,\"start\":91177},{\"end\":91190,\"start\":91186},{\"end\":91196,\"start\":91194},{\"end\":91211,\"start\":91200},{\"end\":91224,\"start\":91215},{\"end\":91576,\"start\":91568},{\"end\":91584,\"start\":91580},{\"end\":91598,\"start\":91588},{\"end\":91608,\"start\":91602},{\"end\":91618,\"start\":91612},{\"end\":91631,\"start\":91622},{\"end\":91645,\"start\":91635},{\"end\":91909,\"start\":91904},{\"end\":91919,\"start\":91913},{\"end\":91931,\"start\":91923},{\"end\":91944,\"start\":91935},{\"end\":91958,\"start\":91948},{\"end\":91968,\"start\":91962},{\"end\":91979,\"start\":91972},{\"end\":91986,\"start\":91983},{\"end\":91992,\"start\":91990},{\"end\":92338,\"start\":92333},{\"end\":92348,\"start\":92342},{\"end\":92584,\"start\":92575},{\"end\":92593,\"start\":92590},{\"end\":92603,\"start\":92597},{\"end\":92613,\"start\":92607},{\"end\":92910,\"start\":92907},{\"end\":92920,\"start\":92914},{\"end\":92927,\"start\":92924},{\"end\":92943,\"start\":92933},{\"end\":93205,\"start\":93203},{\"end\":93214,\"start\":93209},{\"end\":93221,\"start\":93218},{\"end\":93228,\"start\":93225},{\"end\":93549,\"start\":93540},{\"end\":93563,\"start\":93553},{\"end\":93805,\"start\":93796},{\"end\":93813,\"start\":93809},{\"end\":93823,\"start\":93817},{\"end\":93837,\"start\":93827},{\"end\":93845,\"start\":93841},{\"end\":94139,\"start\":94130},{\"end\":94149,\"start\":94143},{\"end\":94155,\"start\":94153},{\"end\":94167,\"start\":94159},{\"end\":94175,\"start\":94171},{\"end\":94186,\"start\":94179},{\"end\":94195,\"start\":94190},{\"end\":94202,\"start\":94199},{\"end\":94214,\"start\":94206},{\"end\":94221,\"start\":94218},{\"end\":94602,\"start\":94599},{\"end\":94612,\"start\":94606},{\"end\":94954,\"start\":94949},{\"end\":94967,\"start\":94958},{\"end\":94973,\"start\":94971},{\"end\":94979,\"start\":94977},{\"end\":94995,\"start\":94983},{\"end\":95004,\"start\":94999},{\"end\":95016,\"start\":95008},{\"end\":95027,\"start\":95020},{\"end\":95035,\"start\":95031},{\"end\":95049,\"start\":95039},{\"end\":95543,\"start\":95536},{\"end\":95558,\"start\":95549},{\"end\":95752,\"start\":95746},{\"end\":95764,\"start\":95756},{\"end\":95772,\"start\":95768},{\"end\":95781,\"start\":95776},{\"end\":95789,\"start\":95785},{\"end\":95798,\"start\":95793},{\"end\":95811,\"start\":95802},{\"end\":95818,\"start\":95815},{\"end\":95830,\"start\":95822},{\"end\":96145,\"start\":96136},{\"end\":96156,\"start\":96149},{\"end\":96163,\"start\":96160},{\"end\":96173,\"start\":96167},{\"end\":96436,\"start\":96427},{\"end\":96446,\"start\":96440},{\"end\":96454,\"start\":96452},{\"end\":96748,\"start\":96742},{\"end\":96756,\"start\":96752},{\"end\":96765,\"start\":96760},{\"end\":96776,\"start\":96769},{\"end\":97066,\"start\":97056},{\"end\":97076,\"start\":97070},{\"end\":97326,\"start\":97323},{\"end\":97333,\"start\":97330},{\"end\":97340,\"start\":97337},{\"end\":97346,\"start\":97344},{\"end\":97355,\"start\":97350},{\"end\":97363,\"start\":97359},{\"end\":97369,\"start\":97367},{\"end\":97695,\"start\":97692},{\"end\":97702,\"start\":97699},{\"end\":97708,\"start\":97706},{\"end\":97714,\"start\":97712},{\"end\":97721,\"start\":97718},{\"end\":97729,\"start\":97725},{\"end\":98054,\"start\":98044},{\"end\":98064,\"start\":98058},{\"end\":98319,\"start\":98312},{\"end\":98335,\"start\":98323},{\"end\":98344,\"start\":98339},{\"end\":98353,\"start\":98348},{\"end\":98645,\"start\":98639},{\"end\":98658,\"start\":98651},{\"end\":98674,\"start\":98662},{\"end\":98689,\"start\":98680},{\"end\":98975,\"start\":98968},{\"end\":98986,\"start\":98979},{\"end\":98999,\"start\":98990},{\"end\":99469,\"start\":99463},{\"end\":99478,\"start\":99473},{\"end\":99715,\"start\":99709},{\"end\":99728,\"start\":99719},{\"end\":99740,\"start\":99734},{\"end\":100005,\"start\":99996},{\"end\":100016,\"start\":100009},{\"end\":100026,\"start\":100020},{\"end\":100311,\"start\":100302},{\"end\":100327,\"start\":100315},{\"end\":100337,\"start\":100331},{\"end\":100647,\"start\":100638},{\"end\":100657,\"start\":100651},{\"end\":100666,\"start\":100661},{\"end\":100997,\"start\":100991},{\"end\":101006,\"start\":101001},{\"end\":101019,\"start\":101010},{\"end\":101377,\"start\":101369},{\"end\":101390,\"start\":101381},{\"end\":101987,\"start\":101979},{\"end\":102000,\"start\":101991},{\"end\":102011,\"start\":102004},{\"end\":102290,\"start\":102284},{\"end\":102297,\"start\":102294},{\"end\":102309,\"start\":102303},{\"end\":102588,\"start\":102582},{\"end\":102599,\"start\":102592},{\"end\":102612,\"start\":102603},{\"end\":102623,\"start\":102616},{\"end\":102862,\"start\":102855},{\"end\":102872,\"start\":102866},{\"end\":102885,\"start\":102876},{\"end\":102895,\"start\":102889},{\"end\":102905,\"start\":102899},{\"end\":103270,\"start\":103263},{\"end\":103279,\"start\":103276},{\"end\":103290,\"start\":103283},{\"end\":103300,\"start\":103294},{\"end\":103307,\"start\":103304},{\"end\":103318,\"start\":103311},{\"end\":103328,\"start\":103322},{\"end\":103338,\"start\":103332},{\"end\":103349,\"start\":103342},{\"end\":103358,\"start\":103353},{\"end\":103691,\"start\":103686},{\"end\":103700,\"start\":103695},{\"end\":103713,\"start\":103704},{\"end\":103723,\"start\":103717},{\"end\":104007,\"start\":104001},{\"end\":104021,\"start\":104013},{\"end\":104034,\"start\":104025},{\"end\":104339,\"start\":104334},{\"end\":104350,\"start\":104343},{\"end\":104361,\"start\":104354},{\"end\":104372,\"start\":104365},{\"end\":104667,\"start\":104663},{\"end\":104678,\"start\":104671},{\"end\":104687,\"start\":104682},{\"end\":104696,\"start\":104691},{\"end\":104705,\"start\":104700},{\"end\":104716,\"start\":104711},{\"end\":105066,\"start\":105055},{\"end\":105074,\"start\":105070},{\"end\":105080,\"start\":105078},{\"end\":105090,\"start\":105084},{\"end\":105102,\"start\":105094},{\"end\":105108,\"start\":105106},{\"end\":105117,\"start\":105112},{\"end\":105129,\"start\":105121},{\"end\":105139,\"start\":105133},{\"end\":105152,\"start\":105143},{\"end\":105497,\"start\":105493},{\"end\":105507,\"start\":105501},{\"end\":105518,\"start\":105511},{\"end\":105528,\"start\":105522},{\"end\":105542,\"start\":105532},{\"end\":105554,\"start\":105548},{\"end\":105563,\"start\":105560},{\"end\":105973,\"start\":105964},{\"end\":105984,\"start\":105977},{\"end\":105995,\"start\":105988},{\"end\":106004,\"start\":105999},{\"end\":106015,\"start\":106008},{\"end\":106025,\"start\":106019},{\"end\":106359,\"start\":106344},{\"end\":106371,\"start\":106363},{\"end\":106383,\"start\":106375},{\"end\":106395,\"start\":106387},{\"end\":106801,\"start\":106790},{\"end\":106811,\"start\":106805},{\"end\":107091,\"start\":107086},{\"end\":107102,\"start\":107095},{\"end\":107111,\"start\":107106},{\"end\":107388,\"start\":107382},{\"end\":107398,\"start\":107392},{\"end\":107410,\"start\":107404},{\"end\":107423,\"start\":107414},{\"end\":107433,\"start\":107427},{\"end\":107747,\"start\":107737},{\"end\":107757,\"start\":107751},{\"end\":107771,\"start\":107761},{\"end\":107784,\"start\":107775},{\"end\":107801,\"start\":107788},{\"end\":108125,\"start\":108121},{\"end\":108133,\"start\":108129},{\"end\":108142,\"start\":108137},{\"end\":108151,\"start\":108146},{\"end\":108161,\"start\":108157},{\"end\":108177,\"start\":108167},{\"end\":108527,\"start\":108520},{\"end\":108540,\"start\":108531},{\"end\":108549,\"start\":108544},{\"end\":108559,\"start\":108553},{\"end\":108568,\"start\":108563},{\"end\":108923,\"start\":108918},{\"end\":108931,\"start\":108927},{\"end\":108942,\"start\":108935},{\"end\":108953,\"start\":108946},{\"end\":108962,\"start\":108957},{\"end\":108973,\"start\":108966},{\"end\":109312,\"start\":109308},{\"end\":109320,\"start\":109316},{\"end\":109332,\"start\":109324},{\"end\":109347,\"start\":109338},{\"end\":109356,\"start\":109351},{\"end\":109638,\"start\":109633},{\"end\":109646,\"start\":109642},{\"end\":109657,\"start\":109652},{\"end\":109933,\"start\":109928},{\"end\":109944,\"start\":109937},{\"end\":109952,\"start\":109948},{\"end\":109963,\"start\":109958},{\"end\":110248,\"start\":110244},{\"end\":110257,\"start\":110252},{\"end\":110266,\"start\":110261},{\"end\":110278,\"start\":110272},{\"end\":110652,\"start\":110648},{\"end\":110661,\"start\":110656},{\"end\":110668,\"start\":110665},{\"end\":110675,\"start\":110672},{\"end\":110919,\"start\":110915},{\"end\":110927,\"start\":110923},{\"end\":110935,\"start\":110931},{\"end\":110941,\"start\":110939},{\"end\":110949,\"start\":110945},{\"end\":110957,\"start\":110953},{\"end\":110963,\"start\":110961},{\"end\":110970,\"start\":110967},{\"end\":111257,\"start\":111253},{\"end\":111263,\"start\":111261},{\"end\":111273,\"start\":111269},{\"end\":111285,\"start\":111279},{\"end\":111658,\"start\":111655},{\"end\":111667,\"start\":111664},{\"end\":111674,\"start\":111671},{\"end\":111680,\"start\":111678},{\"end\":112011,\"start\":112008},{\"end\":112021,\"start\":112015},{\"end\":112027,\"start\":112025},{\"end\":112287,\"start\":112282},{\"end\":112297,\"start\":112291},{\"end\":112627,\"start\":112623},{\"end\":112635,\"start\":112631},{\"end\":112648,\"start\":112641},{\"end\":112657,\"start\":112652},{\"end\":112669,\"start\":112661},{\"end\":112955,\"start\":112953},{\"end\":112961,\"start\":112959},{\"end\":112969,\"start\":112965},{\"end\":112975,\"start\":112973},{\"end\":112982,\"start\":112979},{\"end\":113240,\"start\":113232},{\"end\":113249,\"start\":113244},{\"end\":113259,\"start\":113253},{\"end\":113269,\"start\":113263},{\"end\":113558,\"start\":113554},{\"end\":113572,\"start\":113562},{\"end\":113583,\"start\":113576},{\"end\":113592,\"start\":113587},{\"end\":113793,\"start\":113788},{\"end\":113802,\"start\":113797},{\"end\":113815,\"start\":113808},{\"end\":113828,\"start\":113819}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2110.02095\",\"id\":\"b0\"},\"end\":83178,\"start\":82914},{\"attributes\":{\"doi\":\"arXiv:1610.01644\",\"id\":\"b1\"},\"end\":83423,\"start\":83180},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":170079070},\"end\":83766,\"start\":83425},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14197727},\"end\":84116,\"start\":83768},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":90880},\"end\":84442,\"start\":84118},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":10860374},\"end\":84879,\"start\":84444},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":29153681},\"end\":85212,\"start\":84881},{\"attributes\":{\"doi\":\"arXiv:2006.07159\",\"id\":\"b7\"},\"end\":85460,\"start\":85214},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12726540},\"end\":85792,\"start\":85462},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":59636530},\"end\":86155,\"start\":85794},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":18865663},\"end\":86560,\"start\":86157},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":198907236},\"end\":86852,\"start\":86562},{\"attributes\":{\"doi\":\"arXiv:2010.12648\",\"id\":\"b12\"},\"end\":87131,\"start\":86854},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":211096730},\"end\":87467,\"start\":87133},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":102351185},\"end\":87787,\"start\":87469},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":181936349},\"end\":88090,\"start\":87789},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9137763},\"end\":88398,\"start\":88092},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17466014},\"end\":88704,\"start\":88400},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":196208260},\"end\":89070,\"start\":88706},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235613425},\"end\":89334,\"start\":89072},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":57246310},\"end\":89709,\"start\":89336},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8923541},\"end\":90064,\"start\":89711},{\"attributes\":{\"doi\":\"arXiv:2007.11498\",\"id\":\"b22\"},\"end\":90315,\"start\":90066},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6161478},\"end\":90724,\"start\":90317},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":59606338},\"end\":91078,\"start\":90726},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":211132835},\"end\":91511,\"start\":91080},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":170078981},\"end\":91900,\"start\":91513},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b27\"},\"end\":92287,\"start\":91902},{\"attributes\":{\"id\":\"b28\"},\"end\":92503,\"start\":92289},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":44099388},\"end\":92861,\"start\":92505},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":28671436},\"end\":93153,\"start\":92863},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206594692},\"end\":93456,\"start\":93155},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":56657912},\"end\":93792,\"start\":93458},{\"attributes\":{\"doi\":\"arXiv:1907.07174\",\"id\":\"b33\"},\"end\":94037,\"start\":93794},{\"attributes\":{\"doi\":\"arXiv:2006.16241\",\"id\":\"b34\"},\"end\":94494,\"start\":94039},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":219635787},\"end\":94855,\"start\":94496},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":58981871},\"end\":95470,\"start\":94857},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14382246},\"end\":95709,\"start\":95472},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":216080787},\"end\":96076,\"start\":95711},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":141460329},\"end\":96380,\"start\":96078},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":43928547},\"end\":96683,\"start\":96382},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":16632981},\"end\":96997,\"start\":96685},{\"attributes\":{\"id\":\"b42\"},\"end\":97245,\"start\":96999},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":214667377},\"end\":97627,\"start\":97247},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":206596594},\"end\":97986,\"start\":97629},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":14337532},\"end\":98265,\"start\":97988},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":212414865},\"end\":98578,\"start\":98267},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":213890060},\"end\":98878,\"start\":98580},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":218487494},\"end\":99404,\"start\":98880},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":225068823},\"end\":99672,\"start\":99406},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":174802983},\"end\":99944,\"start\":99674},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":14338058},\"end\":100214,\"start\":99946},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":3531730},\"end\":100585,\"start\":100216},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":221319407},\"end\":100865,\"start\":100587},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":225103395},\"end\":101298,\"start\":100867},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":15193013},\"end\":101696,\"start\":101300},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":9033333},\"end\":101903,\"start\":101698},{\"attributes\":{\"doi\":\"arXiv:1805.10123\",\"id\":\"b57\"},\"end\":102197,\"start\":101905},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":221172897},\"end\":102561,\"start\":102199},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":383200},\"end\":102851,\"start\":102563},{\"attributes\":{\"doi\":\"arXiv:1701.06548\",\"id\":\"b60\"},\"end\":103188,\"start\":102853},{\"attributes\":{\"doi\":\"arXiv:2103.00020\",\"id\":\"b61\"},\"end\":103616,\"start\":103190},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":170078822},\"end\":103997,\"start\":103618},{\"attributes\":{\"doi\":\"arXiv:1703.09507\",\"id\":\"b63\"},\"end\":104281,\"start\":103999},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":67855879},\"end\":104572,\"start\":104283},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":211204852},\"end\":105000,\"start\":104574},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":2930547},\"end\":105432,\"start\":105002},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":49584497},\"end\":105858,\"start\":105434},{\"attributes\":{\"doi\":\"arXiv:1811.01753\",\"id\":\"b68\"},\"end\":106272,\"start\":105860},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":6383532},\"end\":106786,\"start\":106274},{\"attributes\":{\"doi\":\"arXiv:1703.00810\",\"id\":\"b70\"},\"end\":107037,\"start\":106788},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":309759},\"end\":107321,\"start\":107039},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":3994909},\"end\":107666,\"start\":107323},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":6844431},\"end\":108056,\"start\":107668},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":4412459},\"end\":108457,\"start\":108058},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":206593880},\"end\":108837,\"start\":108459},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":220280805},\"end\":109228,\"start\":108839},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":214641252},\"end\":109584,\"start\":109230},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":7182867},\"end\":109865,\"start\":109586},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":14191091},\"end\":110182,\"start\":109867},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":7680631},\"end\":110597,\"start\":110184},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":9683805},\"end\":110850,\"start\":110599},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":68589},\"end\":111249,\"start\":110852},{\"attributes\":{\"doi\":\"arXiv:1905.13549\",\"id\":\"b83\"},\"end\":111555,\"start\":111251},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":170078599},\"end\":111943,\"start\":111557},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":211572620},\"end\":112220,\"start\":111945},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":19209388},\"end\":112556,\"start\":112222},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":1309931},\"end\":112949,\"start\":112558},{\"attributes\":{\"doi\":\"arXiv:2006.11653\",\"id\":\"b88\"},\"end\":113172,\"start\":112951},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":362467},\"end\":113521,\"start\":113174},{\"attributes\":{\"doi\":\"arXiv:2106.04560\",\"id\":\"b90\"},\"end\":113741,\"start\":113523},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":3162051},\"end\":114044,\"start\":113743},{\"attributes\":{\"id\":\"b92\"},\"end\":114234,\"start\":114046}]", "bib_title": "[{\"end\":83492,\"start\":83425},{\"end\":83864,\"start\":83768},{\"end\":84173,\"start\":84118},{\"end\":84509,\"start\":84444},{\"end\":84934,\"start\":84881},{\"end\":85524,\"start\":85462},{\"end\":85923,\"start\":85794},{\"end\":86281,\"start\":86157},{\"end\":86585,\"start\":86562},{\"end\":87202,\"start\":87133},{\"end\":87509,\"start\":87469},{\"end\":87858,\"start\":87789},{\"end\":88151,\"start\":88092},{\"end\":88426,\"start\":88400},{\"end\":88761,\"start\":88706},{\"end\":89118,\"start\":89072},{\"end\":89387,\"start\":89336},{\"end\":89774,\"start\":89711},{\"end\":90394,\"start\":90317},{\"end\":90801,\"start\":90726},{\"end\":91161,\"start\":91080},{\"end\":91564,\"start\":91513},{\"end\":92329,\"start\":92289},{\"end\":92571,\"start\":92505},{\"end\":92903,\"start\":92863},{\"end\":93199,\"start\":93155},{\"end\":93536,\"start\":93458},{\"end\":94595,\"start\":94496},{\"end\":94945,\"start\":94857},{\"end\":95532,\"start\":95472},{\"end\":95742,\"start\":95711},{\"end\":96132,\"start\":96078},{\"end\":96423,\"start\":96382},{\"end\":96738,\"start\":96685},{\"end\":97319,\"start\":97247},{\"end\":97688,\"start\":97629},{\"end\":98040,\"start\":97988},{\"end\":98308,\"start\":98267},{\"end\":98635,\"start\":98580},{\"end\":98964,\"start\":98880},{\"end\":99459,\"start\":99406},{\"end\":99705,\"start\":99674},{\"end\":99992,\"start\":99946},{\"end\":100298,\"start\":100216},{\"end\":100634,\"start\":100587},{\"end\":100987,\"start\":100867},{\"end\":101362,\"start\":101300},{\"end\":101749,\"start\":101698},{\"end\":102280,\"start\":102199},{\"end\":102576,\"start\":102563},{\"end\":103682,\"start\":103618},{\"end\":104330,\"start\":104283},{\"end\":104659,\"start\":104574},{\"end\":105051,\"start\":105002},{\"end\":105487,\"start\":105434},{\"end\":106340,\"start\":106274},{\"end\":107082,\"start\":107039},{\"end\":107378,\"start\":107323},{\"end\":107733,\"start\":107668},{\"end\":108117,\"start\":108058},{\"end\":108516,\"start\":108459},{\"end\":108914,\"start\":108839},{\"end\":109304,\"start\":109230},{\"end\":109629,\"start\":109586},{\"end\":109924,\"start\":109867},{\"end\":110240,\"start\":110184},{\"end\":110644,\"start\":110599},{\"end\":110911,\"start\":110852},{\"end\":111651,\"start\":111557},{\"end\":112004,\"start\":111945},{\"end\":112278,\"start\":112222},{\"end\":112619,\"start\":112558},{\"end\":113228,\"start\":113174},{\"end\":113784,\"start\":113743}]", "bib_author": "[{\"end\":82973,\"start\":82964},{\"end\":82985,\"start\":82973},{\"end\":82998,\"start\":82985},{\"end\":83008,\"start\":82998},{\"end\":83255,\"start\":83246},{\"end\":83265,\"start\":83255},{\"end\":83505,\"start\":83494},{\"end\":83513,\"start\":83505},{\"end\":83524,\"start\":83513},{\"end\":83536,\"start\":83524},{\"end\":83880,\"start\":83866},{\"end\":84189,\"start\":84175},{\"end\":84201,\"start\":84189},{\"end\":84214,\"start\":84201},{\"end\":84519,\"start\":84511},{\"end\":84526,\"start\":84519},{\"end\":84535,\"start\":84526},{\"end\":84550,\"start\":84535},{\"end\":84562,\"start\":84550},{\"end\":84577,\"start\":84562},{\"end\":84950,\"start\":84936},{\"end\":84965,\"start\":84950},{\"end\":84973,\"start\":84965},{\"end\":84984,\"start\":84973},{\"end\":85223,\"start\":85214},{\"end\":85235,\"start\":85223},{\"end\":85249,\"start\":85235},{\"end\":85257,\"start\":85249},{\"end\":85269,\"start\":85257},{\"end\":85537,\"start\":85526},{\"end\":85551,\"start\":85537},{\"end\":85563,\"start\":85551},{\"end\":85937,\"start\":85925},{\"end\":86295,\"start\":86283},{\"end\":86595,\"start\":86587},{\"end\":86607,\"start\":86595},{\"end\":86613,\"start\":86607},{\"end\":86628,\"start\":86613},{\"end\":86637,\"start\":86628},{\"end\":86651,\"start\":86637},{\"end\":86926,\"start\":86918},{\"end\":86935,\"start\":86926},{\"end\":86943,\"start\":86935},{\"end\":86954,\"start\":86943},{\"end\":87212,\"start\":87204},{\"end\":87225,\"start\":87212},{\"end\":87236,\"start\":87225},{\"end\":87246,\"start\":87236},{\"end\":87522,\"start\":87511},{\"end\":87532,\"start\":87522},{\"end\":87540,\"start\":87532},{\"end\":87553,\"start\":87540},{\"end\":87565,\"start\":87553},{\"end\":87869,\"start\":87860},{\"end\":87878,\"start\":87869},{\"end\":87887,\"start\":87878},{\"end\":87902,\"start\":87887},{\"end\":88163,\"start\":88153},{\"end\":88172,\"start\":88163},{\"end\":88188,\"start\":88172},{\"end\":88443,\"start\":88428},{\"end\":88459,\"start\":88443},{\"end\":88472,\"start\":88459},{\"end\":88485,\"start\":88472},{\"end\":88774,\"start\":88763},{\"end\":88782,\"start\":88774},{\"end\":88790,\"start\":88782},{\"end\":88803,\"start\":88790},{\"end\":88811,\"start\":88803},{\"end\":89131,\"start\":89120},{\"end\":89142,\"start\":89131},{\"end\":89397,\"start\":89389},{\"end\":89405,\"start\":89397},{\"end\":89415,\"start\":89405},{\"end\":89424,\"start\":89415},{\"end\":89430,\"start\":89424},{\"end\":89441,\"start\":89430},{\"end\":89784,\"start\":89776},{\"end\":89791,\"start\":89784},{\"end\":89798,\"start\":89791},{\"end\":89811,\"start\":89798},{\"end\":90131,\"start\":90120},{\"end\":90140,\"start\":90131},{\"end\":90153,\"start\":90140},{\"end\":90407,\"start\":90396},{\"end\":90414,\"start\":90407},{\"end\":90425,\"start\":90414},{\"end\":90436,\"start\":90425},{\"end\":90445,\"start\":90436},{\"end\":90454,\"start\":90445},{\"end\":90465,\"start\":90454},{\"end\":90813,\"start\":90803},{\"end\":90825,\"start\":90813},{\"end\":90835,\"start\":90825},{\"end\":91175,\"start\":91163},{\"end\":91184,\"start\":91175},{\"end\":91192,\"start\":91184},{\"end\":91198,\"start\":91192},{\"end\":91213,\"start\":91198},{\"end\":91226,\"start\":91213},{\"end\":91578,\"start\":91566},{\"end\":91586,\"start\":91578},{\"end\":91600,\"start\":91586},{\"end\":91610,\"start\":91600},{\"end\":91620,\"start\":91610},{\"end\":91633,\"start\":91620},{\"end\":91647,\"start\":91633},{\"end\":91911,\"start\":91902},{\"end\":91921,\"start\":91911},{\"end\":91933,\"start\":91921},{\"end\":91946,\"start\":91933},{\"end\":91960,\"start\":91946},{\"end\":91970,\"start\":91960},{\"end\":91981,\"start\":91970},{\"end\":91988,\"start\":91981},{\"end\":91994,\"start\":91988},{\"end\":92340,\"start\":92331},{\"end\":92350,\"start\":92340},{\"end\":92586,\"start\":92573},{\"end\":92595,\"start\":92586},{\"end\":92605,\"start\":92595},{\"end\":92615,\"start\":92605},{\"end\":92912,\"start\":92905},{\"end\":92922,\"start\":92912},{\"end\":92929,\"start\":92922},{\"end\":92945,\"start\":92929},{\"end\":93207,\"start\":93201},{\"end\":93216,\"start\":93207},{\"end\":93223,\"start\":93216},{\"end\":93230,\"start\":93223},{\"end\":93551,\"start\":93538},{\"end\":93565,\"start\":93551},{\"end\":93807,\"start\":93794},{\"end\":93815,\"start\":93807},{\"end\":93825,\"start\":93815},{\"end\":93839,\"start\":93825},{\"end\":93847,\"start\":93839},{\"end\":94141,\"start\":94128},{\"end\":94151,\"start\":94141},{\"end\":94157,\"start\":94151},{\"end\":94169,\"start\":94157},{\"end\":94177,\"start\":94169},{\"end\":94188,\"start\":94177},{\"end\":94197,\"start\":94188},{\"end\":94204,\"start\":94197},{\"end\":94216,\"start\":94204},{\"end\":94223,\"start\":94216},{\"end\":94604,\"start\":94597},{\"end\":94614,\"start\":94604},{\"end\":94956,\"start\":94947},{\"end\":94969,\"start\":94956},{\"end\":94975,\"start\":94969},{\"end\":94981,\"start\":94975},{\"end\":94997,\"start\":94981},{\"end\":95006,\"start\":94997},{\"end\":95018,\"start\":95006},{\"end\":95029,\"start\":95018},{\"end\":95037,\"start\":95029},{\"end\":95051,\"start\":95037},{\"end\":95545,\"start\":95534},{\"end\":95560,\"start\":95545},{\"end\":95754,\"start\":95744},{\"end\":95766,\"start\":95754},{\"end\":95774,\"start\":95766},{\"end\":95783,\"start\":95774},{\"end\":95791,\"start\":95783},{\"end\":95800,\"start\":95791},{\"end\":95813,\"start\":95800},{\"end\":95820,\"start\":95813},{\"end\":95832,\"start\":95820},{\"end\":96147,\"start\":96134},{\"end\":96158,\"start\":96147},{\"end\":96165,\"start\":96158},{\"end\":96175,\"start\":96165},{\"end\":96438,\"start\":96425},{\"end\":96448,\"start\":96438},{\"end\":96456,\"start\":96448},{\"end\":96750,\"start\":96740},{\"end\":96758,\"start\":96750},{\"end\":96767,\"start\":96758},{\"end\":96778,\"start\":96767},{\"end\":97068,\"start\":97054},{\"end\":97078,\"start\":97068},{\"end\":97328,\"start\":97321},{\"end\":97335,\"start\":97328},{\"end\":97342,\"start\":97335},{\"end\":97348,\"start\":97342},{\"end\":97357,\"start\":97348},{\"end\":97365,\"start\":97357},{\"end\":97371,\"start\":97365},{\"end\":97697,\"start\":97690},{\"end\":97704,\"start\":97697},{\"end\":97710,\"start\":97704},{\"end\":97716,\"start\":97710},{\"end\":97723,\"start\":97716},{\"end\":97731,\"start\":97723},{\"end\":98056,\"start\":98042},{\"end\":98066,\"start\":98056},{\"end\":98321,\"start\":98310},{\"end\":98337,\"start\":98321},{\"end\":98346,\"start\":98337},{\"end\":98355,\"start\":98346},{\"end\":98647,\"start\":98637},{\"end\":98660,\"start\":98647},{\"end\":98676,\"start\":98660},{\"end\":98691,\"start\":98676},{\"end\":98977,\"start\":98966},{\"end\":98988,\"start\":98977},{\"end\":99001,\"start\":98988},{\"end\":99471,\"start\":99461},{\"end\":99480,\"start\":99471},{\"end\":99717,\"start\":99707},{\"end\":99730,\"start\":99717},{\"end\":99742,\"start\":99730},{\"end\":100007,\"start\":99994},{\"end\":100018,\"start\":100007},{\"end\":100028,\"start\":100018},{\"end\":100313,\"start\":100300},{\"end\":100329,\"start\":100313},{\"end\":100339,\"start\":100329},{\"end\":100649,\"start\":100636},{\"end\":100659,\"start\":100649},{\"end\":100668,\"start\":100659},{\"end\":100999,\"start\":100989},{\"end\":101008,\"start\":100999},{\"end\":101021,\"start\":101008},{\"end\":101379,\"start\":101364},{\"end\":101392,\"start\":101379},{\"end\":101755,\"start\":101751},{\"end\":101989,\"start\":101975},{\"end\":102002,\"start\":101989},{\"end\":102013,\"start\":102002},{\"end\":102292,\"start\":102282},{\"end\":102299,\"start\":102292},{\"end\":102311,\"start\":102299},{\"end\":102590,\"start\":102578},{\"end\":102601,\"start\":102590},{\"end\":102614,\"start\":102601},{\"end\":102625,\"start\":102614},{\"end\":102864,\"start\":102853},{\"end\":102874,\"start\":102864},{\"end\":102887,\"start\":102874},{\"end\":102897,\"start\":102887},{\"end\":102907,\"start\":102897},{\"end\":103272,\"start\":103261},{\"end\":103281,\"start\":103272},{\"end\":103292,\"start\":103281},{\"end\":103302,\"start\":103292},{\"end\":103309,\"start\":103302},{\"end\":103320,\"start\":103309},{\"end\":103330,\"start\":103320},{\"end\":103340,\"start\":103330},{\"end\":103351,\"start\":103340},{\"end\":103360,\"start\":103351},{\"end\":103693,\"start\":103684},{\"end\":103702,\"start\":103693},{\"end\":103715,\"start\":103702},{\"end\":103725,\"start\":103715},{\"end\":104009,\"start\":103999},{\"end\":104023,\"start\":104009},{\"end\":104036,\"start\":104023},{\"end\":104341,\"start\":104332},{\"end\":104352,\"start\":104341},{\"end\":104363,\"start\":104352},{\"end\":104374,\"start\":104363},{\"end\":104669,\"start\":104661},{\"end\":104680,\"start\":104669},{\"end\":104689,\"start\":104680},{\"end\":104698,\"start\":104689},{\"end\":104707,\"start\":104698},{\"end\":104718,\"start\":104707},{\"end\":105068,\"start\":105053},{\"end\":105076,\"start\":105068},{\"end\":105082,\"start\":105076},{\"end\":105092,\"start\":105082},{\"end\":105104,\"start\":105092},{\"end\":105110,\"start\":105104},{\"end\":105119,\"start\":105110},{\"end\":105131,\"start\":105119},{\"end\":105141,\"start\":105131},{\"end\":105154,\"start\":105141},{\"end\":105499,\"start\":105489},{\"end\":105509,\"start\":105499},{\"end\":105520,\"start\":105509},{\"end\":105530,\"start\":105520},{\"end\":105544,\"start\":105530},{\"end\":105556,\"start\":105544},{\"end\":105565,\"start\":105556},{\"end\":105975,\"start\":105962},{\"end\":105986,\"start\":105975},{\"end\":105997,\"start\":105986},{\"end\":106006,\"start\":105997},{\"end\":106017,\"start\":106006},{\"end\":106027,\"start\":106017},{\"end\":106361,\"start\":106342},{\"end\":106373,\"start\":106361},{\"end\":106385,\"start\":106373},{\"end\":106397,\"start\":106385},{\"end\":106803,\"start\":106788},{\"end\":106813,\"start\":106803},{\"end\":107093,\"start\":107084},{\"end\":107104,\"start\":107093},{\"end\":107113,\"start\":107104},{\"end\":107390,\"start\":107380},{\"end\":107400,\"start\":107390},{\"end\":107412,\"start\":107400},{\"end\":107425,\"start\":107412},{\"end\":107435,\"start\":107425},{\"end\":107749,\"start\":107735},{\"end\":107759,\"start\":107749},{\"end\":107773,\"start\":107759},{\"end\":107786,\"start\":107773},{\"end\":107803,\"start\":107786},{\"end\":108127,\"start\":108119},{\"end\":108135,\"start\":108127},{\"end\":108144,\"start\":108135},{\"end\":108153,\"start\":108144},{\"end\":108163,\"start\":108153},{\"end\":108179,\"start\":108163},{\"end\":108529,\"start\":108518},{\"end\":108542,\"start\":108529},{\"end\":108551,\"start\":108542},{\"end\":108561,\"start\":108551},{\"end\":108570,\"start\":108561},{\"end\":108925,\"start\":108916},{\"end\":108933,\"start\":108925},{\"end\":108944,\"start\":108933},{\"end\":108955,\"start\":108944},{\"end\":108964,\"start\":108955},{\"end\":108975,\"start\":108964},{\"end\":109314,\"start\":109306},{\"end\":109322,\"start\":109314},{\"end\":109334,\"start\":109322},{\"end\":109349,\"start\":109334},{\"end\":109358,\"start\":109349},{\"end\":109640,\"start\":109631},{\"end\":109648,\"start\":109640},{\"end\":109659,\"start\":109648},{\"end\":109935,\"start\":109926},{\"end\":109946,\"start\":109935},{\"end\":109954,\"start\":109946},{\"end\":109965,\"start\":109954},{\"end\":110250,\"start\":110242},{\"end\":110259,\"start\":110250},{\"end\":110268,\"start\":110259},{\"end\":110280,\"start\":110268},{\"end\":110654,\"start\":110646},{\"end\":110663,\"start\":110654},{\"end\":110670,\"start\":110663},{\"end\":110677,\"start\":110670},{\"end\":110921,\"start\":110913},{\"end\":110929,\"start\":110921},{\"end\":110937,\"start\":110929},{\"end\":110943,\"start\":110937},{\"end\":110951,\"start\":110943},{\"end\":110959,\"start\":110951},{\"end\":110965,\"start\":110959},{\"end\":110972,\"start\":110965},{\"end\":111259,\"start\":111251},{\"end\":111265,\"start\":111259},{\"end\":111275,\"start\":111265},{\"end\":111287,\"start\":111275},{\"end\":111660,\"start\":111653},{\"end\":111669,\"start\":111660},{\"end\":111676,\"start\":111669},{\"end\":111682,\"start\":111676},{\"end\":112013,\"start\":112006},{\"end\":112023,\"start\":112013},{\"end\":112029,\"start\":112023},{\"end\":112289,\"start\":112280},{\"end\":112299,\"start\":112289},{\"end\":112629,\"start\":112621},{\"end\":112637,\"start\":112629},{\"end\":112650,\"start\":112637},{\"end\":112659,\"start\":112650},{\"end\":112671,\"start\":112659},{\"end\":112957,\"start\":112951},{\"end\":112963,\"start\":112957},{\"end\":112971,\"start\":112963},{\"end\":112977,\"start\":112971},{\"end\":112984,\"start\":112977},{\"end\":113242,\"start\":113230},{\"end\":113251,\"start\":113242},{\"end\":113261,\"start\":113251},{\"end\":113271,\"start\":113261},{\"end\":113560,\"start\":113552},{\"end\":113574,\"start\":113560},{\"end\":113585,\"start\":113574},{\"end\":113594,\"start\":113585},{\"end\":113795,\"start\":113786},{\"end\":113804,\"start\":113795},{\"end\":113817,\"start\":113804},{\"end\":113830,\"start\":113817}]", "bib_venue": "[{\"end\":95160,\"start\":95114},{\"end\":99162,\"start\":99090},{\"end\":106558,\"start\":106486},{\"end\":110399,\"start\":110348},{\"end\":82962,\"start\":82914},{\"end\":83244,\"start\":83180},{\"end\":83585,\"start\":83536},{\"end\":83929,\"start\":83880},{\"end\":84263,\"start\":84214},{\"end\":84635,\"start\":84577},{\"end\":85036,\"start\":84984},{\"end\":85326,\"start\":85285},{\"end\":85601,\"start\":85563},{\"end\":85951,\"start\":85937},{\"end\":86344,\"start\":86295},{\"end\":86695,\"start\":86651},{\"end\":86916,\"start\":86854},{\"end\":87290,\"start\":87246},{\"end\":87617,\"start\":87565},{\"end\":87923,\"start\":87902},{\"end\":88228,\"start\":88188},{\"end\":88534,\"start\":88485},{\"end\":88869,\"start\":88811},{\"end\":89194,\"start\":89142},{\"end\":89499,\"start\":89441},{\"end\":89869,\"start\":89811},{\"end\":90118,\"start\":90066},{\"end\":90509,\"start\":90465},{\"end\":90879,\"start\":90835},{\"end\":91270,\"start\":91226},{\"end\":91691,\"start\":91647},{\"end\":92068,\"start\":92010},{\"end\":92364,\"start\":92350},{\"end\":92664,\"start\":92615},{\"end\":92989,\"start\":92945},{\"end\":93288,\"start\":93230},{\"end\":93617,\"start\":93565},{\"end\":93891,\"start\":93863},{\"end\":94126,\"start\":94039},{\"end\":94666,\"start\":94614},{\"end\":95112,\"start\":95051},{\"end\":95580,\"start\":95560},{\"end\":95881,\"start\":95832},{\"end\":96219,\"start\":96175},{\"end\":96514,\"start\":96456},{\"end\":96831,\"start\":96778},{\"end\":97052,\"start\":96999},{\"end\":97409,\"start\":97371},{\"end\":97789,\"start\":97731},{\"end\":98118,\"start\":98066},{\"end\":98399,\"start\":98355},{\"end\":98712,\"start\":98691},{\"end\":99088,\"start\":99001},{\"end\":99529,\"start\":99480},{\"end\":99791,\"start\":99742},{\"end\":100057,\"start\":100028},{\"end\":100391,\"start\":100339},{\"end\":100717,\"start\":100668},{\"end\":101073,\"start\":101021},{\"end\":101436,\"start\":101392},{\"end\":101781,\"start\":101755},{\"end\":101973,\"start\":101905},{\"end\":102358,\"start\":102311},{\"end\":102683,\"start\":102625},{\"end\":102996,\"start\":102923},{\"end\":103259,\"start\":103190},{\"end\":103774,\"start\":103725},{\"end\":104116,\"start\":104052},{\"end\":104418,\"start\":104374},{\"end\":104762,\"start\":104718},{\"end\":105194,\"start\":105154},{\"end\":105620,\"start\":105565},{\"end\":105960,\"start\":105860},{\"end\":106484,\"start\":106397},{\"end\":106890,\"start\":106829},{\"end\":107162,\"start\":107113},{\"end\":107475,\"start\":107435},{\"end\":107843,\"start\":107803},{\"end\":108237,\"start\":108179},{\"end\":108628,\"start\":108570},{\"end\":109024,\"start\":108975},{\"end\":109396,\"start\":109358},{\"end\":109708,\"start\":109659},{\"end\":110014,\"start\":109965},{\"end\":110346,\"start\":110280},{\"end\":110707,\"start\":110677},{\"end\":111030,\"start\":110972},{\"end\":111378,\"start\":111303},{\"end\":111731,\"start\":111682},{\"end\":112073,\"start\":112029},{\"end\":112368,\"start\":112299},{\"end\":112729,\"start\":112671},{\"end\":113037,\"start\":113000},{\"end\":113308,\"start\":113271},{\"end\":113550,\"start\":113523},{\"end\":113882,\"start\":113830},{\"end\":114139,\"start\":114046}]"}}}, "year": 2023, "month": 12, "day": 17}