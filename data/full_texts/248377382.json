{"id": 248377382, "updated": "2023-12-14 08:01:20.507", "metadata": {"title": "Learning-to-Rank at the Speed of Sampling: Plackett-Luce Gradient Estimation With Minimal Computational Complexity", "authors": "[{\"first\":\"Harrie\",\"last\":\"Oosterhuis\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Plackett-Luce gradient estimation enables the optimization of stochastic ranking models within feasible time constraints through sampling techniques. Unfortunately, the computational complexity of existing methods does not scale well with the length of the rankings, i.e. the ranking cutoff, nor with the item collection size. In this paper, we introduce the novel PL-Rank-3 algorithm that performs unbiased gradient estimation with a computational complexity comparable to the best sorting algorithms. As a result, our novel learning-to-rank method is applicable in any scenario where standard sorting is feasible in reasonable time. Our experimental results indicate large gains in the time required for optimization, without any loss in performance. For the field, our contribution could potentially allow state-of-the-art learning-to-rank methods to be applied to much larger scales than previously feasible.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sigir/Oosterhuis22", "doi": "10.1145/3477495.3531842"}}, "content": {"source": {"pdf_hash": "047370d3a8d7e9e28a6f0bd70ed389decf0e2d74", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.10872v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "28867954f2ea2e61942dd30186cf5faeb6d92241", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/047370d3a8d7e9e28a6f0bd70ed389decf0e2d74.txt", "contents": "\nLearning-to-Rank at the Speed of Sampling: Plackett-Luce Gradient Estimation With Minimal Computational Complexity\n\n\nHarrie Oosterhuis harrie.oosterhuis@ru.nl \nRadboud University Nijmegen\nThe Netherlands\n\nLearning-to-Rank at the Speed of Sampling: Plackett-Luce Gradient Estimation With Minimal Computational Complexity\n10.1145/3477495.3531842\nPlackett-Luce gradient estimation enables the optimization of stochastic ranking models within feasible time constraints through sampling techniques. Unfortunately, the computational complexity of existing methods does not scale well with the length of the rankings, i.e. the ranking cutoff, nor with the item collection size.In this paper, we introduce the novel PL-Rank-3 algorithm that performs unbiased gradient estimation with a computational complexity comparable to the best sorting algorithms. As a result, our novel learning-to-rank method is applicable in any scenario where standard sorting is feasible in reasonable time. Our experimental results indicate large gains in the time required for optimization, without any loss in performance. For the field, our contribution could potentially allow state-of-the-art learning-to-rank methods to be applied to much larger scales than previously feasible.CCS CONCEPTS\u2022 Information systems \u2192 Learning to rank.\n\ncategory does not have issues with differentiability or smoothness because their methods optimize a probabilistic distribution over rankings. Recently, probabilistic ranking models have also received additional attention for their applicability to ranking fairness tasks [8]. Since probabilistic models are able to divide exposure over items more fairly than deterministic models.\n\nOosterhuis [18] introduced the PL-Rank method to efficiently optimize Plackett-Luce (PL) ranking models: a specific type of ranking model based on decision theory [16,19]. They use Gumbel sampling [2,11] to quickly sample many rankings from a PL ranking model, and subsequently, apply the PL-Rank-1 or PL-Rank-2 algorithm to these samples to unbiasedly approximate the gradient of a ranking metric w.r.t. the model. While PL-Rank provides a significant contribution to the LTR field, we recognize two shortcomings in the work of Oosterhuis [18]: neither PL-Rank-1 or PL-Rank-2 scale well with long rankings; and they do not compare PL-Rank to the earlier and comparable StochasticRank algorithm [23].\n\nThis paper addresses both of these shortcomings: our main contribution is the novel PL-Rank-3 algorithm that computes the same approximation as PL-Rank-2 while minimizing its computational complexity. PL-Rank-3 has computional costs in the same order as the best sorting algorithms; we posit that this is the lowest order possible for a metric-based LTR method. Our experimental comparison includes both the previous PL-Rank-2 and StochasticRank as baselines, providing the first comparison between these two methods. The introduction of PL-Rank-3 is exciting for the LTR field, as it pushes the limit of the minimal computational complexity that LTR methods can have. Potentially, it may enable future LTR to be applied to much larger scales than currently feasible.\n\n\nBACKGROUND: PL-RANK-2\n\nLet indicate a PL ranking model based on a scoring function with ( ) indicating the score for item ; the probability of sampling ranking from item set from is then:\n( ) = ( | 1: \u22121 , ), ( | 1: \u22121 , ) = ( ) 1[ \u2209 1: \u22121 ] \u2032 \u2208 \\ 1: \u22121 ( \u2032 )\n, where 1: \u22121 indicates the ranking from rank 1 up to \u2212 1. In other words, the probability of placing at rank is ( ) divided by the ( \u2032 ) of all unplaced items (similar to a SoftMax activation function), unless was already placed at an earlier rank then it has a zero probability. The probability of the entire ranking is simply the product of all its individual item placements. PL-Rank optimizes so that the metric value of sampled rankings are maximized in expectation [18]. PL-Rank assumes ranking metrics can be decomposed into weights per rank and relevance of an item , [17]. Its objective is thus to maximize the expected Table 1: Overview of LTR methods in terms of their relevant theoretical properties: (i) Reliance on an iteration over item pairs; (ii) Directed by the model's full-ranking behavior; (iii) Directed by metric to be optimized; (iv) Usage of sample-based approximation; (v) Applicability to general rank-based exposure metrics (e.g. for ranking fairness); (vi) Computational complexity w.r.t. number of items and length of ranking . The properties of a method are indicated by \u2713or ? when it is open to interpretation and 2 * indicates the number of item-pairs with unequal relevance labels. method name pairwise ranking-metric-sample rank-based computational notes based based approximation exposure complexity\n\nPointwise [10] not an LTR loss SoftMax Cross-Entropy [21] Pairwise [14] \u2713 2 * memory efficient Listwise/ListMLE [5,25] \u2713 SoftRank [22] \u2713\n? \u2713 3 ApproxNDCG [3] \u2713 ? \u2713 2\nproven bound LambdaRank/Loss [4,24] \u2713 \u2713 \u2713 2 * + log( ) proven bound StochasticRank [23] ?\n\u2713 \u2713 \u2713 \u2713 policy-gradient PL-Rank-1/2 [18] \u2713 \u2713 \u2713 \u2713 policy-gradient PL-Rank-3 (ours) \u2713 \u2713 \u2713 \u2713 + log( ) policy-gradient\nmetric value over its sampling procedure, for a single query :\nR ( ) = \u2211\ufe01 ( ) \u2211\ufe01 \u2208 rank(f,q,d) , = E \u2211\ufe01 \u2208 rank(f,q,d) , .\nThe rank weights can be chosen to match well-known ranking metrics, e.g. DCG: DCG@K = 1[ \u2264 ]/log 2 (1 + ) [13]; or precision: prec = 1[ \u2264 ]. To keep our notation brief, we will omit when denoting the relevances , = . Oosterhuis [18] based the PL-Rank-2 method on the following formulation of the policy gradient:\nR ( ) ( ) = E \u2211\ufe01 =rank( , )+1 + rank( , ) \u2211\ufe01 =1 ( | 1: \u22121 ) \u2212 \u2211\ufe01 = .\n(1)\n\nThe PL-Rank-2 algorithm [18] can approximate the gradient based on sampled rankings for items and a ranking length of with a computational complexity of: O ( ). Table 1 compares the theoretical properties of PL-Rank with other LTR methods. Importantly, PL-Rank is the only method that is not a pairwise method, while also being based on the actual metric that is optimized. 1 Furthermore, it shares most properties with StochasticRank: an alternative method for approximating the policy gradient. Grad( ) \u2190 Grad( ) + 1 ( +1 + ( ) ( \u2212 )) 18: return Grad pseudo-code, we will now show that it produces the same approximation as PL-Rank-2 (cf. Algorithm 1 in [18]).\n\nTo start, we define , as the placement reward, the reward following the item placed at rank in ranking :\n, = min( , ) \u2211\ufe01 = , , = , ( , )+1 .(2)\nImportantly, all , values for a ranking can be computed in steps (Line 9). Next, we point out that a summation over item placement probabilities similar to Eq. 1 can be formulated as:  Using this insight, we define , to later compute the risk imposed by items with a non-zero placement probability at rank in :\nrank( , ) \u2211\ufe01 =1 ( | 1: \u22121 ) = ( ) rank( , ) \u2211\ufe01 =1 1 \u2032 \u2208 \\ 1: \u22121 ( \u2032 ) .(3), = min( , ) \u2211\ufe01 =1 , \u2032 \u2208 \\ 1: \u22121 ( \u2032 ) , , = ,rank( , ) .(4)\nSimilarly, we define , to compute the expected direct reward:\n, = min( , ) \u2211\ufe01 =1 \u2032 \u2208 \\ 1: \u22121 ( \u2032 ) , , = ,rank( , ) . (5)\nCrucial is that all , and , values can also be computed in steps (Line 12 & 13). With these newly defined variables, we can reformulate Eq. 1 without any summation over :\nR ( ) ( ) = E , + ( ) , \u2212 , .(6)\nAccordingly, Algorithm 1 approximates the gradient using:\nR ( ) ( ) \u2248 1 \u2211\ufe01 =1 ( ) , + ( ) ( ) , \u2212 ( ) , .(7)\nPL-Rank-3 computes the , and values in steps and then reuses them for each of the items, resulting in a computational complexity of O ( ( + )) given sampled rankings. However, when we consider that sampling a ranking relies on (partial) sorting, the full complexity of applying PL-Rank-3 becomes O ( ( + log( )). Table 1 reveals that -to the best of our knowledge -PL-Rank-3 has the best computational complexity of all metric-based LTR methods. Moreover, because its computational complexity is limited by the underlying sorting procedure, we posit that PL-Rank-3 has reached the minimum order of computational complexity that is possible for a LTR method that is based on the full-ranking behavior of the model it optimizes.\n\n\nEXPERIMENTAL SETUP\n\nWe experimentally evaluate how the improvements in computational complexity translate to improvements in practical costs. Our experimental runs optimize the @ of neural ranking models on the Yahoo! Webscope-Set1 [6], MSLR-Web30k [20] and Istella [7] datasets. The neural models have two-hidden layers of 32 sigmoid activation nodes, backpropagation via standard gradient descent with a learning rate of 0.01 was applied using Tensorflow [1]. We compare our PL-Rank-3 algorithm, with PL-Rank-2 [18] and StochasticRank [23]. StochasticRank was chosen as a baseline because it shares many properties with PL-Rank (see Table 1) yet was not compared with PL-Rank in previous work [18]. Our StochasticRank implementation uses the Gumbel distribution as stochastic noise instead of the normal distribution of the original algorithm [23], this makes the method applicable and effective to optimize a PL model. We reimplemented the PL-Rank and StochasticRank algorithms in Numpy [12] and performed all our experiments on AMD EPYC\u2122 7H12 CPUs. 2 The ranking length-/metric cutoff was varied: \u2208 {5, 10, 25, 50, 100} and number of sampled rankings: \u2208 {100, 1000}, to measure their impact on computational costs. Performance was measured over 200 minutes Yahoo! Webscope-Set1 MSLR-Web30k Istella of training time, in addition to the average time required to complete one training epoch. All reported results are averages over 30 independent runs performed under identical circumstances. We report Normalized @ (NDCG@K) as our ranking performance metric computed on the held-out test-set of each dataset; following the advice of Ferrante et al. [9], we do not use querynormalization but dataset-normalization: we divide the @ of a ranking model by the maximum possible @ value on the entire test-set of the dataset. Figure 1 and Table 2 shows the effect of ranking length on the computational costs of the LTR methods. As expected, Figure 1 reveals that PL-Rank-2 and StochasticRank are heavily affected by increases in : there appears to be a clear linear trend on the MSLR and Istella datasets. We note that many queries in the Yahoo! dataset have less than 100 documents ( \u2264 100) which could explain why the effect is sub-linear on that dataset. In contrast, PL-Rank-3 appears barely affected by on all of the datasets. Table 2 allows us to also compare the computational costs in more detail. Regardless of whether = 100 or = 1000, the required times of PL-Rank-2 and StochasticRank scale close to linearly with , but those for PL-Rank-3 do not. For instance, on Istella with = 100 and = 5, PL-Rank-2 needs 1.96 minutes, StochasticRank needs 2.79 and PL-Rank-3 needs 1.24 minutes, when compared to = 100, PL-Rank-2 needs an additional 25 minutes, StochasticRank 49 minutes more but PL-Rank-3 only requires an increase of 28 seconds. Moreover, PL-Rank-3 has the lowest computational costs when compared to the other methods with the same and values, across all three datasets. We thus conclude that in terms of time required to complete a single epoch, PL-Rank-3 is a clear improvement over PL-Rank-2 and StochasticRank. Additionally, in terms of scalability of computational costs w.r.t. , PL-Rank-3 is the best choice by a considerable margin. Now that we have established that PL-Rank-3 completes training epochs considerably faster than the other methods, we consider its effect on how quickly certain performance can be reached. Figure 2 show the performance of the LTR methods when optimizing @ over 200 minutes for various values. In all but two scenarios, PL-Rank-3 provides the highest performance at all times, where it seems to mostly depend on whether = 100 or = 1000 is a better choice. In particular, when \u2208 {5, 10} PL-Rank-3 with = 100 has the highest performance on MSLR and Istella and is only outperformed by PL-Rank-3 with = 1000 on Yahoo! Conversely, when \u2208 {50, 100}, = 1000 is the better choice for PL-Rank-3 on all datasets. This suggests that gradient estimation for larger values of is more prone to variance and thus requires more samples for stable optimization. Overall, PL-Rank-2 appears very affected by variance across datasets and values, we mostly attribute this to the small number of epochs it can complete in 200 minutes. For example, on Istella with = 100, PL-Rank-2 completes less than eight epochs whereas PL-Rank-3 can complete 116 epochs. Interestingly, StochasticRank with = 100 has stable performance that is sometimes comparable with PL-Rank-3 with = 1000 on the Yahoo! dataset. This indicates that the sample-efficiency of StochasticRank is actually better than PL-Rank-3, however, due to its low computational costs, PL-Rank-3 still outperforms it on MSLR and Istella and when \u2208 {5, 10, 20} on Yahoo! We conclude that PL-Rank-3 provides a clear and substantial improvement over PL-Rank-2, and in most scenarios also outperforms StochasticRank.\n\n\nRESULTS\n\n\nCONCLUSION\n\nWe have introduced PL-Rank-3 an LTR algorithm to estimate the gradient of a PL ranking model with the same computational complexity as the best sorting algorithms. PL-Rank-3 could enable future metric-based LTR to be applicable to ranking lengths and item collection sizes of much larger scales than previously feasible.\n\n\nCode and data\n\nTo facilitate reproducibility, this work only made use of publicly available data and our experimental implementation is publicly available at https://github.com/HarrieO/2022-SIGIR-plackett-luce.\n\n\u2190\nInput: items: D; Relevances: ; Metric weights: ; Score function: ; Number of samples: . 2: { (1) , (2) , . . . , ( ) } \u2190 Gumbel_Sample( , ) min(rank( , ), ) // index of the pre-computed values 17:\n\nFigure 1 :\n1Mean number of minutes required to complete one training epoch as varies on three datasets.\n\nFigure 2 :\n2NDCG@K performance of PL-Rank-2, PL-Rank-3 and StochasticRank with = 100 and = 1000 when trained up to 200 minutes and for various values evaluated on the test-set of three datasets. All displayed results are averages over 30 independent runs. NDCG was normalized on dataset-level instead of query-level[9].\n\nTable 2 :\n2Mean number of minutes taken to complete one \nepoch with various methods, ranking length/cutoff and \nnumber of samples on three datasets. Bold numbers indi-\ncate the minimal time per and values on each dataset. \nmethod \n= 5 \n= 10 \n= 25 \n= 50 \n= 100 \n\nYahoo! \n\nStoc.Rank \n100 0.52 \n0.80 \n1.64 \n2.58 \n3.01 \n1000 3.38 \n6.18 15.41 26.17 \n31.03 \n\nPL-Rank-2 \n100 0.43 \n0.58 \n0.96 \n1.35 \n1.50 \n1000 2.41 \n4.13 \n8.96 13.30 \n15.19 \n\nPL-Rank-3 \n100 0.33 \n0.34 \n0.38 \n0.40 \n0.40 \n1000 1.33 \n1.53 \n1.92 \n2.18 \n2.19 \n\nMSLR \n\nStoc.Rank \n100 1.55 \n2.53 \n6.01 13.23 \n31.12 \n1000 14.15 25.46 65.25 150.00 353.01 \n\nPL-Rank-2 \n100 1.19 \n1.75 \n4.04 \n8.36 \n15.71 \n1000 10.34 18.91 46.27 94.81 185.42 \n\nPL-Rank-3 \n100 0.74 \n0.77 \n0.86 \n1.00 \n1.20 \n1000 4.77 \n5.09 \n5.93 \n7.31 \n9.37 \n\nIstella \n\nStoc.Rank \n100 2.79 \n4.61 10.75 22.25 \n51.94 \n1000 27.34 49.07 116.73 240.40 531.09 \n\nPL-Rank-2 \n100 1.96 \n2.87 \n6.72 13.25 \n27.07 \n1000 18.49 30.37 74.09 142.55 278.01 \n\nPL-Rank-3 \n100 1.24 \n1.26 \n1.34 \n1.46 \n1.72 \n1000 9.93 10.14 10.87 12.20 \n14.75 \n\n\narXiv:2204.10872v2 [cs.LG] 28 Apr 2022\nMETHOD: A FASTER PL-RANK ALGORITHMWe will now introduce PL-Rank-3: an algorithm for computing the same approximation as PL-Rank-2 but with a significantly better computational complexity. Algorithm 1 displays PL-Rank-3 in1 We define a pairwise method as any method that uses an iteration over item pairs in their algorithm. Thus under our definition methods like ApproxNDCG -that approximates the ranks of items -and SoftRank -that approximates ranking via a distribution over ranks per item -are considered pairwise methods because they iterate over all possible item pairs to compute their approximations and gradients.\nhttps://www.amd.com/en/products/cpu/amd-epyc-7H12\nACKNOWLEDGMENTSThis work was partially supported by the Google Research Scholar Program and made use of the Dutch national e-infrastructure with the support of the SURF Cooperative using grant no. EINF-1748.All content represents the opinion of the author, which is not necessarily shared or endorsed by their respective employers and/or sponsors.\nTensorflow: A System for Large-Scale Machine Learning. Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, 12th USENIX symposium on operating systems design and implementation OSDI'16. Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. Tensorflow: A System for Large-Scale Machine Learning. In 12th USENIX symposium on operating systems design and implementation OSDI'16). 265-283.\n\nA Stochastic Treatment of Learning to Rank Scoring Functions. Sebastian Bruch, Shuguang Han, Michael Bendersky, Marc Najork, Proceedings of the 13th International Conference on Web Search and Data Mining. the 13th International Conference on Web Search and Data MiningSebastian Bruch, Shuguang Han, Michael Bendersky, and Marc Najork. 2020. A Stochastic Treatment of Learning to Rank Scoring Functions. In Proceedings of the 13th International Conference on Web Search and Data Mining. 61-69.\n\nRevisiting Approximate Metric Optimization in the Age of Deep Neural Networks. Sebastian Bruch, Masrour Zoghi, Michael Bendersky, Marc Najork, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 42nd International ACM SIGIR Conference on Research and Development in Information RetrievalSebastian Bruch, Masrour Zoghi, Michael Bendersky, and Marc Najork. 2019. Revisiting Approximate Metric Optimization in the Age of Deep Neural Networks. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 1241-1244.\n\nFrom RankNet to LambdaRank to LambdaMART: An Overview. J C Christopher, Burges, MSR-TR-2010-82. MicrosoftTechnical ReportChristopher J.C. Burges. 2010. From RankNet to LambdaRank to LambdaMART: An Overview. Technical Report MSR-TR-2010-82. Microsoft.\n\nLearning to Rank: From Pairwise Approach to Listwise Approach. Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, Hang Li, Proceedings of the 24th international conference on Machine learning. the 24th international conference on Machine learningZhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to Rank: From Pairwise Approach to Listwise Approach. In Proceedings of the 24th international conference on Machine learning. 129-136.\n\nYahoo! Learning to Rank Challenge Overview. Olivier Chapelle, Yi Chang, Journal of Machine Learning Research. 14Olivier Chapelle and Yi Chang. 2011. Yahoo! Learning to Rank Challenge Overview. Journal of Machine Learning Research 14 (2011), 1-24.\n\nFast Ranking with Additive Ensembles of Oblivious and Non-Oblivious Regression Trees. Domenico Dato, Claudio Lucchese, Maria Franco, Salvatore Nardini, Raffaele Orlando, Nicola Perego, Rossano Tonellotto, Venturini, ACM Transactions on Information Systems (TOIS). 3515Domenico Dato, Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, and Rossano Venturini. 2016. Fast Ranking with Additive Ensembles of Oblivious and Non-Oblivious Regression Trees. ACM Transactions on Information Systems (TOIS) 35, 2 (2016), Article 15.\n\nEvaluating Stochastic Rankings with Expected Exposure. Fernando Diaz, Bhaskar Mitra, Michael D Ekstrand, Asia J Biega, Ben Carterette, Proceedings of the 29th ACM International Conference on Information & Knowledge Management. the 29th ACM International Conference on Information & Knowledge ManagementNew York, NY, USAAssociation for Computing MachineryFernando Diaz, Bhaskar Mitra, Michael D. Ekstrand, Asia J. Biega, and Ben Carterette. 2020. Evaluating Stochastic Rankings with Expected Exposure. In Proceedings of the 29th ACM International Conference on Information & Knowl- edge Management. Association for Computing Machinery, New York, NY, USA, 275-284.\n\nTowards Meaningful Statements in IR Evaluation: Mapping Evaluation Measures to Interval Scales. Marco Ferrante, Nicola Ferro, Norbert Fuhr, IEEE Access. 9Marco Ferrante, Nicola Ferro, and Norbert Fuhr. 2021. Towards Meaningful Statements in IR Evaluation: Mapping Evaluation Measures to Interval Scales. IEEE Access 9 (2021), 136182-136216.\n\nOptimum Polynomial Retrieval Functions Based on the Probability Ranking Principle. Norbert Fuhr, ACM Transactions on Information Systems (TOIS). 7Norbert Fuhr. 1989. Optimum Polynomial Retrieval Functions Based on the Probability Ranking Principle. ACM Transactions on Information Systems (TOIS) 7, 3 (1989), 183-204.\n\nEmil Julius Gumbel, Statistical Theory of Extreme Values and Some Practical Applications: A Series of Lectures. US Government Printing Office33Emil Julius Gumbel. 1954. Statistical Theory of Extreme Values and Some Practical Applications: A Series of Lectures. Vol. 33. US Government Printing Office.\n\nArray Programming with NumPy. Charles R Harris, K Jarrod Millman, J St&apos;efan, Ralf Van Der Walt, Pauli Gommers, David Virtanen, Eric Cournapeau, Julian Wieser, Sebastian Taylor, Nathaniel J Berg, Robert Smith, Matti Kern, Stephan Picus, Marten H Hoyer, Matthew Van Kerkwijk, Allan Brett, Jaime Haldane, Mark Fern, Pearu Wiebe, Peterson, G Pierre, Kevin &apos;erard-Marchant, Tyler Sheppard, Warren Reddy, Hameer Weckesser, Christoph Abbasi, Travis E Gohlke, Oliphant, Nature. 585Charles R. Harris, K. Jarrod Millman, St'efan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern'andez del R'\u0131o, Mark Wiebe, Pearu Peterson, Pierre G'erard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. 2020. Array Programming with NumPy. Nature 585, 7825 (2020), 357-362.\n\nCumulated Gain-Based Evaluation of IR Techniques. Kalervo J\u00e4rvelin, Jaana Kek\u00e4l\u00e4inen, ACM Transactions on Information Systems (TOIS). 20Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated Gain-Based Evaluation of IR Techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422-446.\n\nOptimizing Search Engines Using Clickthrough Data. Thorsten Joachims, Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. the eighth ACM SIGKDD international conference on Knowledge discovery and data miningACMThorsten Joachims. 2002. Optimizing Search Engines Using Clickthrough Data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 133-142.\n\nLearning to Rank for Information Retrieval. Tie-Yan Liu, Foundations and Trends in Information Retrieval. 3Tie-Yan Liu. 2009. Learning to Rank for Information Retrieval. Foundations and Trends in Information Retrieval 3, 3 (2009), 225-331.\n\nIndividual Choice Behavior: A Theoretical Analysis. Luce R Duncan, Courier Corporation. R Duncan Luce. 2012. Individual Choice Behavior: A Theoretical Analysis. Courier Corporation.\n\nIncorporating User Expectations and Behavior into the Measurement of Search Effectiveness. Alistair Moffat, Peter Bailey, Falk Scholer, Paul Thomas, ACM Transactions on Information Systems (TOIS). 35Alistair Moffat, Peter Bailey, Falk Scholer, and Paul Thomas. 2017. Incorporating User Expectations and Behavior into the Measurement of Search Effectiveness. ACM Transactions on Information Systems (TOIS) 35, 3 (2017), 1-38.\n\nComputationally Efficient Optimization of Plackett-Luce Ranking Models for Relevance and Fairness. Harrie Oosterhuis, Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21). the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21)ACMHarrie Oosterhuis. 2021. Computationally Efficient Optimization of Plackett- Luce Ranking Models for Relevance and Fairness. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR '21). ACM, 1023-1032.\n\nThe Analysis of Permutations. L Robin, Plackett, Journal of the Royal Statistical Society: Series C (Applied Statistics). 24Robin L Plackett. 1975. The Analysis of Permutations. Journal of the Royal Statistical Society: Series C (Applied Statistics) 24, 2 (1975), 193-202.\n\n. Tao Qin, Tie-Yan Liu, arXiv:1306.2597Introducing LETOR 4.0 datasets. arXiv preprintTao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 datasets. arXiv preprint arXiv:1306.2597 (2013).\n\nAre Neural Rankers still Outperformed by Gradient Boosted Decision Trees. Zhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Kumar Pasumarthi, Xuanhui Wang, Mike Bendersky, Marc Najork, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaZhen Qin, Le Yan, Honglei Zhuang, Yi Tay, Rama Kumar Pasumarthi, Xuanhui Wang, Mike Bendersky, and Marc Najork. 2021. Are Neural Rankers still Out- performed by Gradient Boosted Decision Trees?. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum?id=Ut1vF_q_vC\n\nSoftrank: Optimizing Non-Smooth Rank Metrics. Michael Taylor, John Guiver, Stephen Robertson, Tom Minka, Proceedings of the 2008 International Conference on Web Search and Data Mining. the 2008 International Conference on Web Search and Data MiningMichael Taylor, John Guiver, Stephen Robertson, and Tom Minka. 2008. Softrank: Optimizing Non-Smooth Rank Metrics. In Proceedings of the 2008 International Conference on Web Search and Data Mining. 77-86.\n\nStochasticRank: Global Optimization of Scale-Free Discrete Functions. Aleksei Ustimenko, Liudmila Prokhorenkova, International Conference on Machine Learning. PMLR. Aleksei Ustimenko and Liudmila Prokhorenkova. 2020. StochasticRank: Global Optimization of Scale-Free Discrete Functions. In International Conference on Machine Learning. PMLR, 9669-9679.\n\nThe LambdaLoss Framework for Ranking Metric Optimization. Xuanhui Wang, Cheng Li, Nadav Golbandi, Michael Bendersky, Marc Najork, Proceedings of the 27th ACM International Conference on Information and Knowledge Management. the 27th ACM International Conference on Information and Knowledge ManagementACMXuanhui Wang, Cheng Li, Nadav Golbandi, Michael Bendersky, and Marc Najork. 2018. The LambdaLoss Framework for Ranking Metric Optimization. In Pro- ceedings of the 27th ACM International Conference on Information and Knowledge Management. ACM, 1313-1322.\n\nListwise Approach to Learning to Rank: Theory and Algorithm. Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, Hang Li, Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learningFen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise Approach to Learning to Rank: Theory and Algorithm. In Proceedings of the 25th international conference on Machine learning. 1192-1199.\n", "annotations": {"author": "[{\"end\":205,\"start\":118}]", "publisher": null, "author_last_name": "[{\"end\":135,\"start\":125}]", "author_first_name": "[{\"end\":124,\"start\":118}]", "author_affiliation": "[{\"end\":204,\"start\":161}]", "title": "[{\"end\":115,\"start\":1},{\"end\":320,\"start\":206}]", "venue": null, "abstract": "[{\"end\":1309,\"start\":345}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1585,\"start\":1582},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1708,\"start\":1704},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1860,\"start\":1856},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1863,\"start\":1860},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1893,\"start\":1890},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1896,\"start\":1893},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2237,\"start\":2233},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2392,\"start\":2388},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3901,\"start\":3897},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4006,\"start\":4002},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4777,\"start\":4773},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4820,\"start\":4816},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4834,\"start\":4830},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4878,\"start\":4875},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4881,\"start\":4878},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4897,\"start\":4893},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4961,\"start\":4958},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4964,\"start\":4961},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5016,\"start\":5012},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5366,\"start\":5362},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5488,\"start\":5484},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5671,\"start\":5667},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6018,\"start\":6017},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6303,\"start\":6299},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8296,\"start\":8293},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8314,\"start\":8310},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8330,\"start\":8327},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8521,\"start\":8518},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8578,\"start\":8574},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8602,\"start\":8598},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8760,\"start\":8756},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8910,\"start\":8906},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9055,\"start\":9051},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9115,\"start\":9114},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9714,\"start\":9711},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14141,\"start\":14138},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15442,\"start\":15441}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":13716,\"start\":13517},{\"attributes\":{\"id\":\"fig_1\"},\"end\":13821,\"start\":13717},{\"attributes\":{\"id\":\"fig_2\"},\"end\":14142,\"start\":13822},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":15180,\"start\":14143}]", "paragraph": "[{\"end\":1691,\"start\":1311},{\"end\":2393,\"start\":1693},{\"end\":3162,\"start\":2395},{\"end\":3352,\"start\":3188},{\"end\":4761,\"start\":3425},{\"end\":4899,\"start\":4763},{\"end\":5018,\"start\":4929},{\"end\":5196,\"start\":5134},{\"end\":5568,\"start\":5256},{\"end\":5641,\"start\":5638},{\"end\":6305,\"start\":5643},{\"end\":6411,\"start\":6307},{\"end\":6761,\"start\":6451},{\"end\":6958,\"start\":6897},{\"end\":7189,\"start\":7019},{\"end\":7280,\"start\":7223},{\"end\":8058,\"start\":7332},{\"end\":12958,\"start\":8081},{\"end\":13303,\"start\":12983},{\"end\":13516,\"start\":13321}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3424,\"start\":3353},{\"attributes\":{\"id\":\"formula_1\"},\"end\":4928,\"start\":4900},{\"attributes\":{\"id\":\"formula_2\"},\"end\":5133,\"start\":5019},{\"attributes\":{\"id\":\"formula_3\"},\"end\":5255,\"start\":5197},{\"attributes\":{\"id\":\"formula_4\"},\"end\":5637,\"start\":5569},{\"attributes\":{\"id\":\"formula_5\"},\"end\":6450,\"start\":6412},{\"attributes\":{\"id\":\"formula_6\"},\"end\":6836,\"start\":6762},{\"attributes\":{\"id\":\"formula_7\"},\"end\":6896,\"start\":6836},{\"attributes\":{\"id\":\"formula_8\"},\"end\":7018,\"start\":6959},{\"attributes\":{\"id\":\"formula_9\"},\"end\":7222,\"start\":7190},{\"attributes\":{\"id\":\"formula_10\"},\"end\":7331,\"start\":7281}]", "table_ref": "[{\"end\":4062,\"start\":4055},{\"end\":5811,\"start\":5804},{\"end\":7652,\"start\":7645},{\"end\":8703,\"start\":8696},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9902,\"start\":9895},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10396,\"start\":10389}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":3186,\"start\":3165},{\"attributes\":{\"n\":\"4\"},\"end\":8079,\"start\":8061},{\"attributes\":{\"n\":\"5\"},\"end\":12968,\"start\":12961},{\"attributes\":{\"n\":\"6\"},\"end\":12981,\"start\":12971},{\"end\":13319,\"start\":13306},{\"end\":13519,\"start\":13518},{\"end\":13728,\"start\":13718},{\"end\":13833,\"start\":13823},{\"end\":14153,\"start\":14144}]", "table": "[{\"end\":15180,\"start\":14155}]", "figure_caption": "[{\"end\":13716,\"start\":13520},{\"end\":13821,\"start\":13730},{\"end\":14142,\"start\":13835}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9890,\"start\":9882},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10006,\"start\":9998},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11511,\"start\":11503}]", "bib_author_first_name": "[{\"end\":16301,\"start\":16295},{\"end\":16313,\"start\":16309},{\"end\":16329,\"start\":16322},{\"end\":16343,\"start\":16336},{\"end\":16354,\"start\":16350},{\"end\":16369,\"start\":16362},{\"end\":16384,\"start\":16376},{\"end\":16398,\"start\":16392},{\"end\":16417,\"start\":16409},{\"end\":16433,\"start\":16426},{\"end\":16896,\"start\":16887},{\"end\":16912,\"start\":16904},{\"end\":16925,\"start\":16918},{\"end\":16941,\"start\":16937},{\"end\":17407,\"start\":17398},{\"end\":17422,\"start\":17415},{\"end\":17437,\"start\":17430},{\"end\":17453,\"start\":17449},{\"end\":18008,\"start\":18007},{\"end\":18010,\"start\":18009},{\"end\":18270,\"start\":18267},{\"end\":18279,\"start\":18276},{\"end\":18292,\"start\":18285},{\"end\":18307,\"start\":18298},{\"end\":18318,\"start\":18314},{\"end\":18709,\"start\":18702},{\"end\":18722,\"start\":18720},{\"end\":19000,\"start\":18992},{\"end\":19014,\"start\":19007},{\"end\":19030,\"start\":19025},{\"end\":19048,\"start\":19039},{\"end\":19066,\"start\":19058},{\"end\":19082,\"start\":19076},{\"end\":19098,\"start\":19091},{\"end\":19536,\"start\":19528},{\"end\":19550,\"start\":19543},{\"end\":19565,\"start\":19558},{\"end\":19567,\"start\":19566},{\"end\":19582,\"start\":19578},{\"end\":19584,\"start\":19583},{\"end\":19595,\"start\":19592},{\"end\":20238,\"start\":20233},{\"end\":20255,\"start\":20249},{\"end\":20270,\"start\":20263},{\"end\":20569,\"start\":20562},{\"end\":20802,\"start\":20798},{\"end\":20809,\"start\":20803},{\"end\":21137,\"start\":21130},{\"end\":21139,\"start\":21138},{\"end\":21149,\"start\":21148},{\"end\":21156,\"start\":21150},{\"end\":21167,\"start\":21166},{\"end\":21186,\"start\":21182},{\"end\":21206,\"start\":21201},{\"end\":21221,\"start\":21216},{\"end\":21236,\"start\":21232},{\"end\":21255,\"start\":21249},{\"end\":21273,\"start\":21264},{\"end\":21291,\"start\":21282},{\"end\":21293,\"start\":21292},{\"end\":21306,\"start\":21300},{\"end\":21319,\"start\":21314},{\"end\":21333,\"start\":21326},{\"end\":21347,\"start\":21341},{\"end\":21349,\"start\":21348},{\"end\":21364,\"start\":21357},{\"end\":21384,\"start\":21379},{\"end\":21397,\"start\":21392},{\"end\":21411,\"start\":21407},{\"end\":21423,\"start\":21418},{\"end\":21442,\"start\":21441},{\"end\":21456,\"start\":21451},{\"end\":21484,\"start\":21479},{\"end\":21501,\"start\":21495},{\"end\":21515,\"start\":21509},{\"end\":21536,\"start\":21527},{\"end\":21551,\"start\":21545},{\"end\":21553,\"start\":21552},{\"end\":22165,\"start\":22158},{\"end\":22181,\"start\":22176},{\"end\":22469,\"start\":22461},{\"end\":22917,\"start\":22910},{\"end\":23163,\"start\":23159},{\"end\":23389,\"start\":23381},{\"end\":23403,\"start\":23398},{\"end\":23416,\"start\":23412},{\"end\":23430,\"start\":23426},{\"end\":23821,\"start\":23815},{\"end\":24443,\"start\":24442},{\"end\":24691,\"start\":24688},{\"end\":24704,\"start\":24697},{\"end\":24952,\"start\":24948},{\"end\":24960,\"start\":24958},{\"end\":24973,\"start\":24966},{\"end\":24984,\"start\":24982},{\"end\":24994,\"start\":24990},{\"end\":25020,\"start\":25013},{\"end\":25031,\"start\":25027},{\"end\":25047,\"start\":25043},{\"end\":25566,\"start\":25559},{\"end\":25579,\"start\":25575},{\"end\":25595,\"start\":25588},{\"end\":25610,\"start\":25607},{\"end\":26044,\"start\":26037},{\"end\":26064,\"start\":26056},{\"end\":26386,\"start\":26379},{\"end\":26398,\"start\":26393},{\"end\":26408,\"start\":26403},{\"end\":26426,\"start\":26419},{\"end\":26442,\"start\":26438},{\"end\":26945,\"start\":26942},{\"end\":26958,\"start\":26951},{\"end\":26967,\"start\":26964},{\"end\":26982,\"start\":26974},{\"end\":26994,\"start\":26990}]", "bib_author_last_name": "[{\"end\":16307,\"start\":16302},{\"end\":16320,\"start\":16314},{\"end\":16334,\"start\":16330},{\"end\":16348,\"start\":16344},{\"end\":16360,\"start\":16355},{\"end\":16374,\"start\":16370},{\"end\":16390,\"start\":16385},{\"end\":16407,\"start\":16399},{\"end\":16424,\"start\":16418},{\"end\":16439,\"start\":16434},{\"end\":16902,\"start\":16897},{\"end\":16916,\"start\":16913},{\"end\":16935,\"start\":16926},{\"end\":16948,\"start\":16942},{\"end\":17413,\"start\":17408},{\"end\":17428,\"start\":17423},{\"end\":17447,\"start\":17438},{\"end\":17460,\"start\":17454},{\"end\":18022,\"start\":18011},{\"end\":18030,\"start\":18024},{\"end\":18274,\"start\":18271},{\"end\":18283,\"start\":18280},{\"end\":18296,\"start\":18293},{\"end\":18312,\"start\":18308},{\"end\":18321,\"start\":18319},{\"end\":18718,\"start\":18710},{\"end\":18728,\"start\":18723},{\"end\":19005,\"start\":19001},{\"end\":19023,\"start\":19015},{\"end\":19037,\"start\":19031},{\"end\":19056,\"start\":19049},{\"end\":19074,\"start\":19067},{\"end\":19089,\"start\":19083},{\"end\":19109,\"start\":19099},{\"end\":19120,\"start\":19111},{\"end\":19541,\"start\":19537},{\"end\":19556,\"start\":19551},{\"end\":19576,\"start\":19568},{\"end\":19590,\"start\":19585},{\"end\":19606,\"start\":19596},{\"end\":20247,\"start\":20239},{\"end\":20261,\"start\":20256},{\"end\":20275,\"start\":20271},{\"end\":20574,\"start\":20570},{\"end\":20816,\"start\":20810},{\"end\":21146,\"start\":21140},{\"end\":21164,\"start\":21157},{\"end\":21180,\"start\":21168},{\"end\":21199,\"start\":21187},{\"end\":21214,\"start\":21207},{\"end\":21230,\"start\":21222},{\"end\":21247,\"start\":21237},{\"end\":21262,\"start\":21256},{\"end\":21280,\"start\":21274},{\"end\":21298,\"start\":21294},{\"end\":21312,\"start\":21307},{\"end\":21324,\"start\":21320},{\"end\":21339,\"start\":21334},{\"end\":21355,\"start\":21350},{\"end\":21377,\"start\":21365},{\"end\":21390,\"start\":21385},{\"end\":21405,\"start\":21398},{\"end\":21416,\"start\":21412},{\"end\":21429,\"start\":21424},{\"end\":21439,\"start\":21431},{\"end\":21449,\"start\":21443},{\"end\":21477,\"start\":21457},{\"end\":21493,\"start\":21485},{\"end\":21507,\"start\":21502},{\"end\":21525,\"start\":21516},{\"end\":21543,\"start\":21537},{\"end\":21560,\"start\":21554},{\"end\":21570,\"start\":21562},{\"end\":22174,\"start\":22166},{\"end\":22192,\"start\":22182},{\"end\":22478,\"start\":22470},{\"end\":22921,\"start\":22918},{\"end\":23172,\"start\":23164},{\"end\":23396,\"start\":23390},{\"end\":23410,\"start\":23404},{\"end\":23424,\"start\":23417},{\"end\":23437,\"start\":23431},{\"end\":23832,\"start\":23822},{\"end\":24449,\"start\":24444},{\"end\":24459,\"start\":24451},{\"end\":24695,\"start\":24692},{\"end\":24708,\"start\":24705},{\"end\":24956,\"start\":24953},{\"end\":24964,\"start\":24961},{\"end\":24980,\"start\":24974},{\"end\":24988,\"start\":24985},{\"end\":25011,\"start\":24995},{\"end\":25025,\"start\":25021},{\"end\":25041,\"start\":25032},{\"end\":25054,\"start\":25048},{\"end\":25573,\"start\":25567},{\"end\":25586,\"start\":25580},{\"end\":25605,\"start\":25596},{\"end\":25616,\"start\":25611},{\"end\":26054,\"start\":26045},{\"end\":26078,\"start\":26065},{\"end\":26391,\"start\":26387},{\"end\":26401,\"start\":26399},{\"end\":26417,\"start\":26409},{\"end\":26436,\"start\":26427},{\"end\":26449,\"start\":26443},{\"end\":26949,\"start\":26946},{\"end\":26962,\"start\":26959},{\"end\":26972,\"start\":26968},{\"end\":26988,\"start\":26983},{\"end\":26997,\"start\":26995}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6287870},\"end\":16823,\"start\":16240},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":209388063},\"end\":17317,\"start\":16825},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":197928353},\"end\":17950,\"start\":17319},{\"attributes\":{\"doi\":\"MSR-TR-2010-82. Microsoft\",\"id\":\"b3\"},\"end\":18202,\"start\":17952},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207163577},\"end\":18656,\"start\":18204},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5763769},\"end\":18904,\"start\":18658},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":16488846},\"end\":19471,\"start\":18906},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":216562537},\"end\":20135,\"start\":19473},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":230799597},\"end\":20477,\"start\":20137},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16632383},\"end\":20796,\"start\":20479},{\"attributes\":{\"id\":\"b10\"},\"end\":21098,\"start\":20798},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":219792763},\"end\":22106,\"start\":21100},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1981391},\"end\":22408,\"start\":22108},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":207605508},\"end\":22864,\"start\":22410},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":28826624},\"end\":23105,\"start\":22866},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":123178801},\"end\":23288,\"start\":23107},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10662255},\"end\":23714,\"start\":23290},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":233481799},\"end\":24410,\"start\":23716},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":116534299},\"end\":24684,\"start\":24412},{\"attributes\":{\"doi\":\"arXiv:1306.2597\",\"id\":\"b19\"},\"end\":24872,\"start\":24686},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":235614327},\"end\":25511,\"start\":24874},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5496423},\"end\":25965,\"start\":25513},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":211989371},\"end\":26319,\"start\":25967},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":53033881},\"end\":26879,\"start\":26321},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207168334},\"end\":27333,\"start\":26881}]", "bib_title": "[{\"end\":16293,\"start\":16240},{\"end\":16885,\"start\":16825},{\"end\":17396,\"start\":17319},{\"end\":18265,\"start\":18204},{\"end\":18700,\"start\":18658},{\"end\":18990,\"start\":18906},{\"end\":19526,\"start\":19473},{\"end\":20231,\"start\":20137},{\"end\":20560,\"start\":20479},{\"end\":21128,\"start\":21100},{\"end\":22156,\"start\":22108},{\"end\":22459,\"start\":22410},{\"end\":22908,\"start\":22866},{\"end\":23157,\"start\":23107},{\"end\":23379,\"start\":23290},{\"end\":23813,\"start\":23716},{\"end\":24440,\"start\":24412},{\"end\":24946,\"start\":24874},{\"end\":25557,\"start\":25513},{\"end\":26035,\"start\":25967},{\"end\":26377,\"start\":26321},{\"end\":26940,\"start\":26881}]", "bib_author": "[{\"end\":16309,\"start\":16295},{\"end\":16322,\"start\":16309},{\"end\":16336,\"start\":16322},{\"end\":16350,\"start\":16336},{\"end\":16362,\"start\":16350},{\"end\":16376,\"start\":16362},{\"end\":16392,\"start\":16376},{\"end\":16409,\"start\":16392},{\"end\":16426,\"start\":16409},{\"end\":16441,\"start\":16426},{\"end\":16904,\"start\":16887},{\"end\":16918,\"start\":16904},{\"end\":16937,\"start\":16918},{\"end\":16950,\"start\":16937},{\"end\":17415,\"start\":17398},{\"end\":17430,\"start\":17415},{\"end\":17449,\"start\":17430},{\"end\":17462,\"start\":17449},{\"end\":18024,\"start\":18007},{\"end\":18032,\"start\":18024},{\"end\":18276,\"start\":18267},{\"end\":18285,\"start\":18276},{\"end\":18298,\"start\":18285},{\"end\":18314,\"start\":18298},{\"end\":18323,\"start\":18314},{\"end\":18720,\"start\":18702},{\"end\":18730,\"start\":18720},{\"end\":19007,\"start\":18992},{\"end\":19025,\"start\":19007},{\"end\":19039,\"start\":19025},{\"end\":19058,\"start\":19039},{\"end\":19076,\"start\":19058},{\"end\":19091,\"start\":19076},{\"end\":19111,\"start\":19091},{\"end\":19122,\"start\":19111},{\"end\":19543,\"start\":19528},{\"end\":19558,\"start\":19543},{\"end\":19578,\"start\":19558},{\"end\":19592,\"start\":19578},{\"end\":19608,\"start\":19592},{\"end\":20249,\"start\":20233},{\"end\":20263,\"start\":20249},{\"end\":20277,\"start\":20263},{\"end\":20576,\"start\":20562},{\"end\":20818,\"start\":20798},{\"end\":21148,\"start\":21130},{\"end\":21166,\"start\":21148},{\"end\":21182,\"start\":21166},{\"end\":21201,\"start\":21182},{\"end\":21216,\"start\":21201},{\"end\":21232,\"start\":21216},{\"end\":21249,\"start\":21232},{\"end\":21264,\"start\":21249},{\"end\":21282,\"start\":21264},{\"end\":21300,\"start\":21282},{\"end\":21314,\"start\":21300},{\"end\":21326,\"start\":21314},{\"end\":21341,\"start\":21326},{\"end\":21357,\"start\":21341},{\"end\":21379,\"start\":21357},{\"end\":21392,\"start\":21379},{\"end\":21407,\"start\":21392},{\"end\":21418,\"start\":21407},{\"end\":21431,\"start\":21418},{\"end\":21441,\"start\":21431},{\"end\":21451,\"start\":21441},{\"end\":21479,\"start\":21451},{\"end\":21495,\"start\":21479},{\"end\":21509,\"start\":21495},{\"end\":21527,\"start\":21509},{\"end\":21545,\"start\":21527},{\"end\":21562,\"start\":21545},{\"end\":21572,\"start\":21562},{\"end\":22176,\"start\":22158},{\"end\":22194,\"start\":22176},{\"end\":22480,\"start\":22461},{\"end\":22923,\"start\":22910},{\"end\":23174,\"start\":23159},{\"end\":23398,\"start\":23381},{\"end\":23412,\"start\":23398},{\"end\":23426,\"start\":23412},{\"end\":23439,\"start\":23426},{\"end\":23834,\"start\":23815},{\"end\":24451,\"start\":24442},{\"end\":24461,\"start\":24451},{\"end\":24697,\"start\":24688},{\"end\":24710,\"start\":24697},{\"end\":24958,\"start\":24948},{\"end\":24966,\"start\":24958},{\"end\":24982,\"start\":24966},{\"end\":24990,\"start\":24982},{\"end\":25013,\"start\":24990},{\"end\":25027,\"start\":25013},{\"end\":25043,\"start\":25027},{\"end\":25056,\"start\":25043},{\"end\":25575,\"start\":25559},{\"end\":25588,\"start\":25575},{\"end\":25607,\"start\":25588},{\"end\":25618,\"start\":25607},{\"end\":26056,\"start\":26037},{\"end\":26080,\"start\":26056},{\"end\":26393,\"start\":26379},{\"end\":26403,\"start\":26393},{\"end\":26419,\"start\":26403},{\"end\":26438,\"start\":26419},{\"end\":26451,\"start\":26438},{\"end\":26951,\"start\":26942},{\"end\":26964,\"start\":26951},{\"end\":26974,\"start\":26964},{\"end\":26990,\"start\":26974},{\"end\":26999,\"start\":26990}]", "bib_venue": "[{\"end\":17093,\"start\":17030},{\"end\":17671,\"start\":17575},{\"end\":18446,\"start\":18393},{\"end\":19792,\"start\":19700},{\"end\":22667,\"start\":22582},{\"end\":24115,\"start\":23983},{\"end\":25147,\"start\":25140},{\"end\":25761,\"start\":25698},{\"end\":26622,\"start\":26545},{\"end\":27122,\"start\":27069},{\"end\":16517,\"start\":16441},{\"end\":17028,\"start\":16950},{\"end\":17573,\"start\":17462},{\"end\":18005,\"start\":17952},{\"end\":18391,\"start\":18323},{\"end\":18766,\"start\":18730},{\"end\":19168,\"start\":19122},{\"end\":19698,\"start\":19608},{\"end\":20288,\"start\":20277},{\"end\":20622,\"start\":20576},{\"end\":20908,\"start\":20818},{\"end\":21578,\"start\":21572},{\"end\":22240,\"start\":22194},{\"end\":22580,\"start\":22480},{\"end\":22970,\"start\":22923},{\"end\":23193,\"start\":23174},{\"end\":23485,\"start\":23439},{\"end\":23981,\"start\":23834},{\"end\":24532,\"start\":24461},{\"end\":25138,\"start\":25056},{\"end\":25696,\"start\":25618},{\"end\":26130,\"start\":26080},{\"end\":26543,\"start\":26451},{\"end\":27067,\"start\":26999}]"}}}, "year": 2023, "month": 12, "day": 17}