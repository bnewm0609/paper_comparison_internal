{"id": 15928602, "updated": "2023-09-27 18:14:51.822", "metadata": {"title": "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis", "authors": "[{\"first\":\"Amir\",\"last\":\"Shahroudy\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Tian-Tsong\",\"last\":\"Ng\",\"middle\":[]},{\"first\":\"Gang\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2016, "month": 4, "day": 11}, "abstract": "Recent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classification of action classes. Currently available depth-based and RGB+D-based action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for RGB+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classification. Experimental results show the advantages of applying deep learning methods over state-of-the-art hand-crafted features on the suggested cross-subject and cross-view evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and RGB+D-based human activity analysis.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1604.02808", "mag": "2964134613", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/ShahroudyLNW16", "doi": "10.1109/cvpr.2016.115"}}, "content": {"source": {"pdf_hash": "77b83507c542d9bb2f6bddbe2eb9354ef8e0a989", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1604.02808v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1604.02808", "status": "GREEN"}}, "grobid": {"id": "8dd80dfca4b311ff0992c1080fb672a22bc452e5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/77b83507c542d9bb2f6bddbe2eb9354ef8e0a989.txt", "contents": "\nNTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis\n\n\nAmir Shahroudy \nSchool of Electrical and Electronic Engineering\nNanyang Technological University\nSingapore\n\nInstitute for Infocomm Research\nSingapore\n\nJun Liu \nSchool of Electrical and Electronic Engineering\nNanyang Technological University\nSingapore\n\nTian-Tsong Ng ttng@i2r.a-star.edu.sg \nInstitute for Infocomm Research\nSingapore\n\nGang Wang wanggang@ntu.edu.sg \nSchool of Electrical and Electronic Engineering\nNanyang Technological University\nSingapore\n\nNTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis\n\nRecent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classification of action classes. Currently available depth-based and RGB+Dbased action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for RGB+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classification. Experimental results show the advantages of applying deep learning methods over state-of-the-art handcrafted features on the suggested cross-subject and crossview evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and RGB+D-based human activity analysis.\n\nIntroduction\n\nRecent development of depth sensors enabled us to obtain effective 3D structures of the scenes and objects [13]. This empowers the vision solutions to move one important step towards 3D vision, e.g. 3D object recognition, 3D scene understanding, and 3D action recognition [1].\n\nUnlike the RGB-based counterpart, 3D video analysis suffers from the lack of large-sized benchmark datasets. Yet there are no any sources of publicly shared 3D videos such as YouTube to supply \"in-the-wild\" samples. This limits our ability to build large-sized benchmarks to eval- * Corresponding author uate and compare the strengths of different methods, especially the recent data-hungry techniques like deep learning approaches. To the best of our knowledge, all the current 3D action recognition benchmarks have limitations in various aspects.\n\nFirst is the small number of subjects and very narrow range of performers' ages, which makes the intra-class variation of the actions very limited. The constitution of human activities depends on the age, gender, culture and even physical conditions of the subjects. Therefore, variation of human subjects is crucial for an action recognition benchmark.\n\nSecond factor is the number of the action classes. When only a very small number of classes are available, each action class can be easily distinguishable by finding a simple motion pattern or even the appearance of an interacted object. But when the number of classes grows, the motion patterns and interacting objects will be shared between classes and the classification task will be more challenging.\n\nThird is the highly restricted camera views. For most of the datasets, all the samples are captured from a front view with a fixed camera viewpoint. For some others, views are bounded to fixed front and side views, using multiple cameras at the same time.\n\nFinally and most importantly, the highly limited number of video samples prevents us from applying the most advanced data-driven learning methods to this problem. Although some attempts have been done [9,42], they suffered from overfitting and had to scale down the size of learning parameters; as a result, they clearly need many more samples to generalize and perform better on testing data.\n\nTo overcome these limitations, we develop a new largescale benchmark dataset for 3D human activity analysis. The proposed dataset consists of 56, 880 RGB+D video samples, captured from 40 different human subjects, using Microsoft Kinect v2. We have collected RGB videos, depth sequences, skeleton data (3D locations of 25 major body joints), and infrared frames. Samples are captured in 80 distinct camera viewpoints. The age range of the subjects in our dataset is from 10 to 35 years which brings more realis- tic variation to the quality of actions. Although our dataset is limited to indoor scenes, due to the operational limitation of the acquisition sensor, we provide the ambiance inconstancy by capturing in various background conditions. This large amount of variation in subjects and views makes it possible to have more accurate cross-subject and cross-view evaluations for various 3D-based action analysis methods. The proposed dataset can help the community to move steps forward in 3D human activity analysis and makes it possible to apply data-hungry methods such as deep learning techniques for this task.\n\nAs another contribution, inspired by the physical characteristics of human body motion, we propose a novel partaware extension of the long short-term memory (LSTM) model [14]. Human actions can be interpreted as interactions of different parts of the body. In this way, the joints of each body part always move together and the combination of their 3D trajectories form more complex motion patterns. By splitting the memory cell of the LSTM into part-based sub-cells, the recurrent network will learn the long-term patterns specifically for each body part and the output of the unit will be learned from the combination of all the subcells.\n\nOur experimental results on the proposed dataset shows the clear advantages of data-driven learning methods over state-of-the-art hand-crafted features.\n\nThe rest of this paper is organized as follows: Section 2 explores the current 3D-based human action recognition methods and benchmarks. Section 3 introduces the proposed dataset, its structure, and defined evaluation criteria. Section 4 presents our new part-aware long short-term memory network for action analysis in a recurrent neural network fashion. Section 5 shows the experimental evaluations of state-of-the-art hand-crafted features alongside the proposed recurrent learning method on our benchmark, and section 6 concludes the paper.\n\n\nRelated work\n\nIn this section we briefly review publicly available 3D activity analysis benchmark datasets and recent methods in this domain. Here we introduce a limited number of the most famous ones. For a more extensive list of current 3D activity analysis datasets and methods, readers are referred to these survey papers [47,1,5,12,21,45,3].\n\n\n3D activity analysis datasets\n\nAfter the release of Microsoft Kinect [48], several datasets are collected by different groups to perform research on 3D action recognition and to evaluate different methods in this field.\n\nMSR-Action3D dataset [19] was one of the earliest ones which opened up the research in depth-based action analysis. The samples of this dataset were limited to depth sequences of gaming actions e.g. forward punch, side-boxing, forward kick, side kick, tennis swing, tennis serve, golf swing, etc. Later the body joint data was added to the dataset. Joint information includes the 3D locations of 20 different body joints in each frame. A decent number of methods are evaluated on this benchmark and recent ones reported close to saturation accuracies [22,20,32].\n\nCAD-60 [34] and CAD-120 [18] contain RGB, depth, and skeleton data of human actions. The special character-istic of these datasets is the variety of camera views. Unlike most of the other datasets, camera is not bound to frontview or side-views. However, the limited number of video samples (60 and 120) is the downside of them.\n\nRGBD-HuDaAct [23] was one of the largest datasets. It contains RGB and depth sequences of 1189 videos of 12 human daily actions (plus one background class), with high variation in time lengths. The special characteristic of this dataset was the synced and aligned RGB and depth channels which enabled local multimodal analysis of RBGD signals 1 .\n\nMSR-DailyActivity [38] was among the most challenging benchmarks in this field. It contains 320 samples of 16 daily activities with higher intra-class variation. Small number of samples and the fixed viewpoint of the camera are the limitations of this dataset. Recently reported results on this dataset also achieved very high accuracies [20,15,22,31].\n\n3D Action Pairs [25] was proposed to provide multiple pairs of action classes. Each pair contains very closely related actions with differences along temporal axis e.g. pick up/put down a box, push/pull a chair, wear/take off a hat, etc. State-of-the-art methods [17,32,31] achieved perfect accuracy on this benchmark.\n\nMultiview 3D event [43] and Northwestern-UCLA [40] datasets used more than one Kincect cameras at the same time to collect multi-view representations of the same action, and scale up the number of samples.\n\nIt is worth mentioning, there are more than 40 datasets specifically for 3D human action recognition [47]. Although each of them provided important challenges of human activity analysis, they have limitations in some aspects. Table 1 shows the comparison between some of the current datasets with our large-scale RGB+D action recognition dataset.\n\nTo summarize the advantages of our dataset over the existing ones, NTU RGB+D has: 1-many more action classes, 2-many more samples for each action class, 3-much more intra-class variations (poses, environmental conditions, interacted objects, age of actors, ...), 4-more camera views, 5-more camera-to-subject distances, and 6-used Kinect v.2 which provides more accurate depth-maps and 3D joints, especially in a multi-camera setup compared to the previous version of Kinect.\n\n\n3D action recognition methods\n\nAfter the introduction of first few benchmarks, a decent number of methods were proposed and evaluated on them.\n\nOreifej et al. [25] calculated the four-dimensional normals (X-Y-depth-time) from depth sequences and accumulates them on spatio-temporal cubes as quantized his-tograms over 120 vertices of a regular polychoron. The work of [26] proposed histograms of oriented principle components of depth cloud points, in order to extract robust features against viewpoint variations. Lu et al. [20] applied \u03c4 test based binary range-sample features on depth maps and achieved robust representation against noise, scaling, camera views, and background clutter. Yang and Tian [44] proposed supernormal vectors as aggregated dictionarybased codewords of four-dimensional normals over spacetime grids.\n\nTo have a view-invariant representation of the actions, features can be extracted from the 3D body joint positions which are available for each frame. Evangelidis et al. [10] divided the body into part-based joint quadruples and encodes the configuration of each part with a succinct 6D feature vector, so called skeletal quads. To aggregate the skeletal quads, they applied Fisher vectors and classified the samples by a linear SVM. In [37] different skeleton configurations were represented as points on a Lie group. Actions as time-series of skeletal configurations, were encoded as curves on this manifold. The work of [22] utilized group sparsity based class-specific dictionary coding with geometric constraints to extract skeleton-based features. Rahmani and Mian [29] introduced a nonlinear knowledge transfer model to transform different views of human actions to a canonical view. To apply ConvNet-based learning to this domain, [30] used synthetically generated data and fitted them to real mocap data. Their learning method was able to recognize actions from novel poses and viewpoints.\n\nIn most of 3D action recognition scenarios, there are more than one modality of information and combining them helps to improve the classification accuracy. Ohn-Bar and Trivedi [24] combined second order joint-angle similarity representations of skeletons with a modified two step HOG feature on spatio-temporal depth maps to build global representation of each video sample and utilized a linear SVM to classify the actions. Wang et al. [39], combined Fourier temporal pyramids of skeletal information with local occupancy pattern features extracted from depth maps and applied a data mining framework to discover the most discriminative combinations of body joints. A structured sparsity based multimodal feature fusion technique was introduced by [33] for action recognition in RGB+D domain. In [27] random decision forests were utilized for learning and feature pruning over a combination of depth and skeletonbased features. The work of [32] proposed hierarchical mixed norms to fuse different features and select most informative body parts in a joint learning framework. Hu et al. [15] proposed dynamic skeletons as Fourier temporal pyramids of spline-based interpolated skeleton points and their gradients, and HOG-based dynamic color and depth patterns to be used in a RGB+D joint-learning model for action classification. RNN based 3D action recognition: The applications of recurrent neural networks for 3D human action recognition were explored very recently [36,9,49].\n\nDifferential RNN [36] added a new gating mechanism to the traditional LSTM to extract the derivatives of internal state (DoS). The derived DoS was fed to the LSTM gates to learn salient dynamic patterns in 3D skeleton data.\n\nHBRNN-L [9] proposed a multilayer RNN framework for action recognition on a hierarchy of skeleton-based inputs. At the first layer, each subnetwork received the inputs from one body part. On next layers, the combined hidden representation of previous layers were fed as inputs in a hierarchical combination of body parts.\n\nThe work of [49] introduced an internal dropout mechanism applied to LSTM gates for stronger regularization in the RNN-based 3D action learning network. To further regularize the learning, a co-occurrence inducing norm was added to the network's cost function which enforced the learning to discover the groups of co-occurring and discriminative joints for better action recognition.\n\nDifferent from these, our Part-aware LSTM (section 4) is a new RNN-based learning framework which has internal part-based memory sub-cells with a novel gating mechanism.\n\n\nThe Dataset\n\nThis section introduces the details and the evaluation criteria of NTU RGB+D action recognition dataset. 2 \n\n\nThe RGB+D Action Dataset\n\nData Modalities: To collect this dataset, we utilized Microsoft Kinect v2 sensors. We collected four major data modalities provided by this sensor: depth maps, 3D joint information, RGB frames, and IR sequences.\n\nDepth maps are sequences of two dimensional depth values in millimeters. To maintain all the information, we applied lossless compression for each individual frame. The resolution of each depth frame is 512 \u00d7 424.\n\nJoint information consists of 3-dimensional locations of 25 major body joints for detected and tracked human bodies in the scene. The corresponding pixels on RGB frames and depth maps are also provided for each joint and every frame. The configuration of body joints is illustrated in Figure 1.\n\nRGB videos are recorded in the provided resolution of 1920 \u00d7 1080.\n\nInfrared sequences are also collected and stored frame by frame in 512 \u00d7 424.\n\nAction Classes: We have 60 action classes in total, which are divided into three major groups: 40 daily actions (drinking, eating, reading, etc.), 9 health-related actions (sneezing, staggering, falling down, etc.), and 11 mutual actions (punching, kicking, hugging, etc.).\n\nSubjects: We invited 40 distinct subjects for our data collection. The ages of the subjects are between 10 and 35. Figure 4 shows the variety of the subjects in age, gender, and height. Each subject is assigned a consistent ID number over the entire dataset.\n\nViews: We used three cameras at the same time to capture three different horizontal views from the same action. For each setup, the three cameras were located at the same height but from three different horizontal angles: \u221245 \u2022 , 0 \u2022 , +45 \u2022 . Each subject was asked to perform each action twice, once towards the left camera and once towards the right camera. In this way, we capture two front views, one left side view, one right side view, one left side 45 degrees view, and one right side 45 degrees view. The three cameras are assigned consistent camera numbers. Camera 1 always observes the 45 degrees views, while camera 2 and 3 observe front and side views.\n\nTo further increase the camera views, on each setup we changed the height and distances of the cameras to the subjects, as reported in Table 2. All the camera and setup numbers are provided for each video sample. \n\n\nBenchmark Evaluations\n\nTo have standard evaluations for all the reported results on this benchmark, we define precise criteria for two types of action classification evaluation, as described in this section. For each of these two, we report the classification accuracy in percentage.\n\n\nCross-Subject Evaluation\n\nIn cross-subject evaluation, we split the 40 subjects into training and testing groups. Each group consists of 20 subjects. For this evaluation, the training and testing sets have 40, 320 and 16, 560 samples, respectively. The IDs of training subjects in this evaluation are: 1, 2, 4, 5, 8, 9, 13, 14, 15, 16, 17, 18, 19, 25, 27, 28, 31, 34, 35, 38; remaining subjects are reserved for testing.\n\n\nCross-View Evaluation\n\nFor cross-view evaluation, we pick all the samples of camera 1 for testing and samples of cameras 2 and 3 for training. In other words, the training set consists of front and two side views of the actions, while testing set includes left and right 45 degree views of the action performances. For this evaluation, the training and testing sets have 37, 920 and 18, 960 samples, respectively.\n\n\nPart-Aware LSTM Network\n\nIn this section, we introduce a new data-driven learning method to model the human actions using our collected 3D action sequences.\n\nHuman actions can be interpreted as time series of body configurations. These body configurations can be effectively and succinctly represented by the 3D locations of major joints of the body. In this fashion, each video sample can be modeled as a sequential representation of configurations.\n\nRecurrent Neural Networks (RNNs) and Long Short-Term Memory Networks (LSTMs) [14] have been shown to be among the most successful deep learning models to encode and learn sequential data in various applications [35,8,2,16].\n\nIn this section, we introduce the traditional recurrent neural networks and then propose our part-aware LSTM model.\n\n\nTraditional RNN and LSTM\n\nA recurrent neural network transforms an input sequence (X) to another sequence (Y) by updating its internal state representation (h t ) at each time step (t) as a linear function of the last step's state and the input at the current step, followed by a nonlinear scaling function. Mathematically:\nh t = \u03c3 W x t h t\u22121 (1) y t = \u03c3 Vh t(2)\nwhere t \u2208 {1, .., T } represents time steps, and \u03c3 \u2208 {Sigm, T anh} is a nonlinear scaling function. Layers of RNNs can be stacked to build a deep recurrent network:\nh l t = \u03c3 W l h l\u22121 t h l t\u22121 (3) h 0 t := x t (4) y t = \u03c3 Vh L t (5)\nwhere l \u2208 {1, ..., L} represents layers.\n\nTraditional RNNs have limited abilities to keep longterm representation of the sequences and were unable to discover relations among long-ranges of inputs. To alleviate this drawback, Long Short-Term Memory Network [14] was introduced to keep a long term memory inside each RNN unit and learn when to remember or forget information stored inside its internal memory cell (c t ):\n\uf8eb \uf8ec \uf8ec \uf8ed i f o g \uf8f6 \uf8f7 \uf8f7 \uf8f8 = \uf8eb \uf8ec \uf8ec \uf8ed Sigm Sigm Sigm T anh \uf8f6 \uf8f7 \uf8f7 \uf8f8 W x t h t\u22121 (6) c t = f c t\u22121 + i g (7) h t = o T anh(c t )(8)\nIn this model, i, f, o, and g denote input gate, forget gate, output gate, and input modulation gate respectively. Operator denotes element-wise multiplication. Figure 2 shows the schema of this recurrent unit. The output y t is fed to a softmax layer to transform the output codes to probability values of class labels. To train such networks for action recognition, we fix the training output label for each input sample over time.\nc t i g f o h t h t\u22121\nx t Figure 2. Schema of a long short-term memory (LSTM) unit. o is the output gate, i is the input gate, g is the input modulation gate, and f is the forget gate. c is the memory cell to keep the long term context.\n\n\nProposed Part-Aware LSTM\n\nIn human actions, body joints move together in groups. Each group can be assigned to a major part of the body, and actions can be interpreted based on the interactions between body parts or with other objects. Based on this intuition, we propose a part-aware LSTM human action learning model. We dub the method P-LSTM.\n\nInstead of keeping a long-term memory of the entire body's motion in the cell, we split it to part-based cells. It is intuitive and more efficient to keep the context of each body part independently and represent the output of the P-LSTM unit as a combination of independent body part context information. In this fashion, each part's cell has its individual input, forget, and modulation gates, but the output gate will be shared among the body parts. In our model, we group the body joints into five part groups: torso, two hands, and two legs.\n\nAt each frame t, we concatenate the 3D coordinates of the joints inside each part p \u2208 {1, ..., P } and consider them as the input representation of that part, denoted as x p t . Thusly, the proposed P-LSTM is modeled as:\n\uf8eb \uf8ed i p f p g p \uf8f6 \uf8f8 = \uf8eb \uf8ed Sigm Sigm T anh \uf8f6 \uf8f8 W p x p t h t\u22121 (9) c p t = f p c p t\u22121 + i p g p (10) o = Sigm \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed W o \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed x 1 t . . . x P t h t\u22121 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8(11)h t = o T anh \uf8eb \uf8ec \uf8ed c 1 t . . . c P t \uf8f6 \uf8f7 \uf8f8(12)\nA graphical representation of the propsed P-LSTM is illustrated in Figure 3.\n\nThe LSTM baseline has full connections between all the memory cells and all the input features via input modula- Figure 3. Illustration of the proposed part-aware long short-term memory (P-LSTM) unit. tion gate and the memory cell was supposed to represent the long-term dynamics of the entire skeleton over time. This leads to a very large size of training parameters which are prone to overfitting. We propose to regularize this by dropping unnecessary links. We divide the entire body's dynamics (represented in the memory cell) to the dynamics of body parts (part-based cells) and learn the final classifier over their concatenation. Our P-LSTM learns the common temporal patterns of the parts independently and combines them in the global level representation for action recognition.\nc 1 t i 1 g 1 f 1 h t h t\u22121 x 1 t c P t i P g P f P x P t o c t\n\nExperiments\n\nIn our experiments, we evaluate state-of-the-art depthbased action recognition methods and compare them with RNN, LSTM, and the proposed P-LSTM based on the evaluation criteria of our dataset.\n\n\nExperimental Setup\n\nWe use the publicly available implementation of six depth-based action recognition methods and apply them on our new dataset benchmark. Among them, HOG 2 [24], Super Normal Vector [44], and HON4D [25] extract features directly from depth maps without using the skeletal information. Lie group [37], Skeletal Quads [10], and FTP Dynamic Skeletons [15] are skeleton-based methods.\n\nThe other evaluated methods are RNN, LSTM, and the proposed P-LSTM method.\n\nFor skeletal representation, we apply a normalization preprocessing step. The original 3D locations of the body joints are provided in camera coordinate system. We translate them to the body coordinate system with its origin on the \"middle of the spine\" joint (number 2 in Figure 1), fol-lowed by a 3D rotation to fix the X axis parallel to the 3D vector from \"right shoulder\" to \"left shoulder\", and Y axis towards the 3D vector from \"spine base\" to \"spine\". The Z axis is fixed as the new X \u00d7 Y . In the last step of normalization, we scale all the 3D points based on the distance between \"spine base\" and \"spine\" joints.\n\nIn the cases of having more than one body in the scene, we transform all of them with regard to the main actor's skeleton. To choose the main actor among the available skeletons, we pick the one with the highest amount of 3D body motion.\n\nKinect's body tracker is prone to detecting some objects e.g. seats or tables as bodies. To filter out these noisy detections, for each tracked skeleton we calculate the spread of the joint locations towards image axis and filtered out the ones whose X spread were more than 0.8 of their Y spread. For our recurrent model evaluation, we reserve about five percent of the training data as validation set. The networks are trained on a large number of iterations and we pick the network with the least validation error among all the iterations and report its performance on testing data.\n\nFor each video sample at each training iteration, we split the video to T = 8 equal sized temporal segments and randomly pick one frame from each segment to feed the skeletal information of that frame as input to the recurrent leaning models in t \u2208 {1, ..., T } time steps.\n\nFor the baseline methods which use SVM as their classifier, to be able to manage the large scale of the data, we use Libliner SVM toolbox [11].\n\nOur RNN, LSTM, and P-LSTM implementations are done on the Torch toolbox platform [7]. We use a Nvidia Tesla K40 GPU to run our experiments.\n\n\nExperimental Evaluations\n\nThe results of our evaluations of the above-mentioned methods are reported in Table 3. First three rows show the accuracies of the evaluated depth-map features. They perform better in cross-subject evaluation compared to the cross-view one. The reason for this difference is that in the cross-view scenario, the depth appearance of the actions are different and these methods are more prone to learning the appearances or view-dependent motion patterns.\n\nSkeletal-based features (Lie group [37], Skeletal Quads [10], and FTP Dynamic Skeletons [15]), perform better with a notable gap on both settings. They are stronger to generalize between the views because the 3D skeletal representation is view-invariant in essence, but it's prone to errors of the body tracker.\n\nAs the most relevant baseline, we implemented HBRNN-L [9] which achieved competitive results to the best hand-crafted methods. Although [9] reported the ineffectiveness of dropout on their experiments, we found it effective on all of our evaluations (including their method).\n\n\nCross\n\nCross Method Subject View Accuracy Accuracy HOG 2 [24] 32.24% 22.27% Super Normal Vector [44] 31.82% 13.61% HON4D [25] 30.56% 7.26% Lie Group [37] 50.08% 52.76% Skeletal Quads [10] 38.62% 41.36% FTP Dynamic Skeletons [15] 60.23% 65.22% HBRNN-L [9] 59  This shows they have their model was prone to overfitting due to the lack of training data and proves the demand for a bigger dataset and approves our motivation for proposing NTU RGB+D dataset. At the next step, we evaluate the discussed recurrent networks on this benchmark. Although RNN has the limitation in discovering long-term interdependency of inputs, they perform competitively with the hand-crafted methods. Stacking one more RNN layer improves the overall performance of the network, especially in cross-view scenario.\n\nBy utilizing long-term context in LSTM, the performances are improved significantly. LSTM's performance improves slightly by stacking one more layer.\n\nAt the last step, we evaluate the proposed P-LSTM model. By isolating the context memory of each body part and training the classifier based on their combination, we model a new way of regularization in the learning process of LSTM parameters. It utilizes the high intra-part and low inter-part correlation of input features to improve the learning process of the LSTM network. As shown in Table 3 P-LSTM outperforms all other methods by achieving 62.93% in cross-subject, and 70.27% in cross-view evaluations.\n\n\nConclusion\n\nA large-scale RGB+D action recognition dataset is introduced in this paper. Our dataset includes 56880 video samples collected from 60 action classes in highly variant Figure 4. Sample frames of the NTU RGB+D dataset. First four rows show the variety in human subjects and camera views. Fifth row depicts the intra-class variation of the performances. The last row illustrates RGB, RGB+joints, depth, depth+joints, and IR modalities of a sample frame. camera settings. Compared to the current datasets for this task, our dataset is larger in orders and contains much more variety in different aspects.\n\nThe large scale of the collected data enables us to apply data-driven learning methods like Long Short-Term Memory networks in this problem and achieve better performance accuracies compared to hand-crafted features.\n\nWe also propose a Part-aware LSTM model to utilize the physical structure of the human body to further improve the performance of the LSTM learning framework.\n\nThe provided experimental results show the availability of large-scale data enables the data-driven learning frameworks to outperform hand-crafted features. They also show the effectiveness of the proposed P-LSTM model over traditional recurrent models.\n\n\nAcknowledgement\n\nFigure 1 .\n1Configuration of 25 body joints in our dataset. The labels of the joints are: 1-base of the spine 2-middle of the spine 3-neck 4-head 5-left shoulder 6-left elbow 7-left wrist 8left hand 9-right shoulder 10-right elbow 11-right wrist 12right hand 13-left hip 14-left knee 15-left ankle 16-left foot 17right hip 18-right knee 19-right ankle 20-right foot 21-spine 22tip of the left hand 23-left thumb 24-tip of the right hand 25right thumb\n\n\nTable 1. Comparison between NTU RGB+D dataset and some of the other publicly available datasets for 3D action recognition. Our dataset provides many more samples, action classes, human subjects, and camera views in comparison with other available datasets for RGB+D action recogniton.Datasets \n\nSamples Classes Subjects Views Sensor \nModalities \nYear \nMSR-Action3D \n[19] \n567 \n20 \n10 \n1 \nN/A \nD+3DJoints \n2010 \nCAD-60 \n[34] \n60 \n12 \n4 \n-\nKinect v1 \nRGB+D+3DJoints \n2011 \nRGBD-HuDaAct \n[23] \n1189 \n13 \n30 \n1 \nKinect v1 \nRGB+D \n2011 \nMSRDailyActivity3D [38] \n320 \n16 \n10 \n1 \nKinect v1 \nRGB+D+3DJoints \n2012 \nAct4 2 \n[6] \n6844 \n14 \n24 \n4 \nKinect v1 \nRGB+D \n2012 \nCAD-120 \n[18] \n120 10+10 \n4 \n-\nKinect v1 \nRGB+D+3DJoints \n2013 \n3D Action Pairs \n[25] \n360 \n12 \n10 \n1 \nKinect v1 \nRGB+D+3DJoints \n2013 \nMultiview 3D Event \n[43] \n3815 \n8 \n8 \n3 \nKinect v1 \nRGB+D+3DJoints \n2013 \nOnline RGB+D Action [46] \n336 \n7 \n24 \n1 \nKinect v1 \nRGB+D+3DJoints \n2014 \nNorthwestern-UCLA [40] \n1475 \n10 \n10 \n3 \nKinect v1 \nRGB+D+3DJoints \n2014 \nUWA3D Multiview \n[28] \n\u223c900 \n30 \n10 \n1 \nKinect v1 \nRGB+D+3DJoints \n2014 \nOffice Activity \n[41] \n1180 \n20 \n10 \n3 \nKinect v1 \nRGB+D \n2014 \nUTD-MHAD \n[4] \n861 \n27 \n8 \n1 \nKinect v1+WIS RGB+D+3DJoints+ID 2015 \nUWA3D Multiview II [26] \n1075 \n30 \n10 \n5 \nKinect v1 \nRGB+D+3DJoints \n2015 \nNTU RGB+D \n56880 \n60 \n40 80 Kinect v2 \nRGB+D+IR+3DJoints 2016 \n\n\n\nTable 3 .\n3The results of the two evaluation settings of our benchmark using different methods. First three rows are depth-map based baseline methods. Rows 4, 5, and 6 are three skeletonbased baseline methods. Following rows report the performance of RNN, LSTM and the proposed P-LSTM model. Our P-LSTM learning model outperforms other methods on both of the evaluation settings.\nWe emphasize the difference between RGBD and RGB+D terms. We suggest to use RGBD when the two modalities are aligned pixel-wise, and RGB+D when the resolutions of the two are different and frames are not aligned.\nhttp://rose1.ntu.edu.sg/datasets/actionrecognition.asp\nThis research was carried out at the Rapid-Rich Object Search (ROSE) Lab at the Nanyang Technological University, Singapore. The ROSE Lab is supported by the National Research Foundation, Singapore, under its Interactive Digital Media (IDM) Strategic Research Programme.The research is in part supported by Singapore Ministry of Education (MOE) Tier 2 ARC28/14, and Singapore A*STAR Science and Engineering Research Council PSF1321202099.We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.\nHuman activity recognition from 3d data: A review. J Aggarwal, L Xia, PR LettersJ. Aggarwal and L. Xia. Human activity recognition from 3d data: A review. PR Letters, 2014.\n\nW Byeon, T M Breuel, F Raue, M Liwicki, Scene labeling with lstm recurrent neural networks. In CVPR. W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene la- beling with lstm recurrent neural networks. In CVPR, 2015.\n\nRgb-d datasets using microsoft kinect or similar sensors: a survey. Z Cai, J Han, L Liu, L Shao, Multimedia Tools and Applications. Z. Cai, J. Han, L. Liu, and L. Shao. Rgb-d datasets using microsoft kinect or similar sensors: a survey. Multimedia Tools and Applications, 2016.\n\nUtd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor. C Chen, R Jafari, N Kehtarnavaz, ICIP. C. Chen, R. Jafari, and N. Kehtarnavaz. Utd-mhad: A multi- modal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor. In ICIP, Sept 2015.\n\nA survey of human motion analysis using depth imagery. L Chen, H Wei, J Ferryman, PR LettersL. Chen, H. Wei, and J. Ferryman. A survey of human mo- tion analysis using depth imagery. PR Letters, 2013.\n\nHuman daily action analysis with multi-view and color-depth data. Z Cheng, L Qin, Y Ye, Q Huang, Q Tian, ECCV Workshops. Z. Cheng, L. Qin, Y. Ye, Q. Huang, and Q. Tian. Human daily action analysis with multi-view and color-depth data. In ECCV Workshops. 2012.\n\nTorch7: A matlab-like environment for machine learning. R Collobert, K Kavukcuoglu, C Farabet, BigLearn, NIPS Workshop. R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, 2011.\n\nLong-term recurrent convolutional networks for visual recognition and description. J Donahue, L Hendricks, S Guadarrama, M Rohrbach, S Venugopalan, K Saenko, T Darrell, CVPR. J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar- rell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015.\n\nHierarchical recurrent neural network for skeleton based action recognition. Y Du, W Wang, L Wang, CVPR. Y. Du, W. Wang, and L. Wang. Hierarchical recurrent neu- ral network for skeleton based action recognition. In CVPR, 2015.\n\nSkeletal quads: Human action recognition using joint quadruples. G Evangelidis, G Singh, R Horaud, ICPR. G. Evangelidis, G. Singh, and R. Horaud. Skeletal quads: Human action recognition using joint quadruples. In ICPR, 2014.\n\nLIBLINEAR: A library for large linear classification. R.-E Fan, K.-W Chang, C.-J Hsieh, X.-R Wang, C.-J Lin, JMLRR.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. JMLR, 2008.\n\nF Han, B Reily, W Hoff, H Zhang, Space-Time Representation of People Based on 3D Skeletal Data: A Review. arXiv. F. Han, B. Reily, W. Hoff, and H. Zhang. Space-Time Rep- resentation of People Based on 3D Skeletal Data: A Review. arXiv, 2016.\n\nEnhanced computer vision with microsoft kinect sensor: A review. J Han, L Shao, D Xu, J Shotton, IEEE Transactions on Cybernetics. J. Han, L. Shao, D. Xu, and J. Shotton. Enhanced computer vision with microsoft kinect sensor: A review. IEEE Trans- actions on Cybernetics, 2013.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.\n\nJointly learning heterogeneous features for rgb-d activity recognition. J.-F Hu, W.-S Zheng, J Lai, J Zhang, CVPR. J.-F. Hu, W.-S. Zheng, J. Lai, and J. Zhang. Jointly learn- ing heterogeneous features for rgb-d activity recognition. In CVPR, 2015.\n\nVisualizing and understanding recurrent networks. A Karpathy, J Johnson, F Li, arXivA. Karpathy, J. Johnson, and F. Li. Visualizing and under- standing recurrent networks. arXiv, 2015.\n\nBilinear heterogeneous information machine for rgb-d action recognition. Y Kong, Y Fu, CVPR. Y. Kong and Y. Fu. Bilinear heterogeneous information ma- chine for rgb-d action recognition. In CVPR, 2015.\n\nLearning human activities and object affordances from rgb-d videos. IJRR. H S Koppula, R Gupta, A Saxena, H. S. Koppula, R. Gupta, and A. Saxena. Learning human activities and object affordances from rgb-d videos. IJRR, 2013.\n\nAction recognition based on a bag of 3d points. W Li, Z Zhang, Z Liu, CVPR Workshops. W. Li, Z. Zhang, and Z. Liu. Action recognition based on a bag of 3d points. In CVPR Workshops, 2010.\n\nRange-sample depth feature for action recognition. C Lu, J Jia, C.-K Tang, CVPR. C. Lu, J. Jia, and C.-K. Tang. Range-sample depth feature for action recognition. In CVPR, 2014.\n\nA survey of applications and human motion recognition with microsoft kinect. R Lun, W Zhao, IJPRAIR. Lun and W. Zhao. A survey of applications and human motion recognition with microsoft kinect. IJPRAI, 2015.\n\nGroup sparsity and geometry constrained dictionary learning for action recognition from depth maps. J Luo, W Wang, H Qi, ICCV. J. Luo, W. Wang, and H. Qi. Group sparsity and geometry constrained dictionary learning for action recognition from depth maps. In ICCV, 2013.\n\nRgbd-hudaact: A color-depth video database for human daily activity recognition. B Ni, G Wang, P Moulin, ICCV Workshops. B. Ni, G. Wang, and P. Moulin. Rgbd-hudaact: A color-depth video database for human daily activity recognition. In ICCV Workshops, 2011.\n\nJoint angles similarities and hog 2 for action recognition. E Ohn-Bar, M Trivedi, CVPR Workshops. E. Ohn-Bar and M. Trivedi. Joint angles similarities and hog 2 for action recognition. In CVPR Workshops, 2013.\n\nHon4d: Histogram of oriented 4d normals for activity recognition from depth sequences. O Oreifej, Z Liu, CVPR. O. Oreifej and Z. Liu. Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences. In CVPR, 2013.\n\nHistogram of oriented principal components for cross-view action recognition. H Rahmani, A Mahmood, D Huynh, A Mian, TPAMIH. Rahmani, A. Mahmood, D. Huynh, and A. Mian. His- togram of oriented principal components for cross-view ac- tion recognition. TPAMI, 2016.\n\nReal time action recognition using histograms of depth gradients and random decision forests. H Rahmani, A Mahmood, D Q Huynh, A Mian, WACV. H. Rahmani, A. Mahmood, D. Q. Huynh, and A. Mian. Real time action recognition using histograms of depth gradients and random decision forests. In WACV, 2014.\n\nHopc: Histogram of oriented principal components of 3d pointclouds for action recognition. H Rahmani, A Mahmood, D Huynh, A Mian, ECCV. H. Rahmani, A. Mahmood, D. Q Huynh, and A. Mian. Hopc: Histogram of oriented principal components of 3d point- clouds for action recognition. In ECCV. 2014.\n\nLearning a non-linear knowledge transfer model for cross-view action recognition. H Rahmani, A Mian, CVPR. H. Rahmani and A. Mian. Learning a non-linear knowledge transfer model for cross-view action recognition. In CVPR, 2015.\n\n3d action recognition from novel viewpoints. H Rahmani, A Mian, CVPR. H. Rahmani and A. Mian. 3d action recognition from novel viewpoints. In CVPR, June 2016.\n\nA Shahroudy, T.-T Ng, Y Gong, G Wang, Deep Multimodal Feature Analysis for Action Recognition in RGB+D Videos. arXiv. A. Shahroudy, T.-T. Ng, Y. Gong, and G. Wang. Deep Mul- timodal Feature Analysis for Action Recognition in RGB+D Videos. arXiv, 2016.\n\nMultimodal multipart learning for action recognition in depth videos. A Shahroudy, T T Ng, Q Yang, G Wang, TPAMIA. Shahroudy, T. T. Ng, Q. Yang, and G. Wang. Multimodal multipart learning for action recognition in depth videos. TPAMI, 2016.\n\nMulti-modal feature fusion for action recognition in rgb-d sequences. A Shahroudy, G Wang, T.-T Ng, ISCCSP. A. Shahroudy, G. Wang, and T.-T. Ng. Multi-modal feature fusion for action recognition in rgb-d sequences. In ISCCSP, 2014.\n\nHuman activity detection from rgbd images. J Sung, C Ponce, B Selman, A Saxena, AAAI Workshops. J. Sung, C. Ponce, B. Selman, and A. Saxena. Human activ- ity detection from rgbd images. In AAAI Workshops, 2011.\n\nSequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V V Le, NIPS. I. Sutskever, O. Vinyals, and Q. V. V. Le. Sequence to se- quence learning with neural networks. In NIPS. 2014.\n\nDifferential recurrent neural networks for action recognition. V Veeriah, N Zhuang, G.-J Qi, ICCV. V. Veeriah, N. Zhuang, and G.-J. Qi. Differential recurrent neural networks for action recognition. In ICCV, 2015.\n\nHuman action recognition by representing 3d skeletons as points in a lie group. R Vemulapalli, F Arrate, R Chellappa, CVPR. R. Vemulapalli, F. Arrate, and R. Chellappa. Human action recognition by representing 3d skeletons as points in a lie group. In CVPR, 2014.\n\nMining actionlet ensemble for action recognition with depth cameras. J Wang, Z Liu, Y Wu, J Yuan, CVPR. J. Wang, Z. Liu, Y. Wu, and J. Yuan. Mining actionlet en- semble for action recognition with depth cameras. In CVPR, 2012.\n\nLearning actionlet ensemble for 3d human action recognition. J Wang, Z Liu, Y Wu, J Yuan, TPAMIJ. Wang, Z. Liu, Y. Wu, and J. Yuan. Learning actionlet ensemble for 3d human action recognition. TPAMI, 2014.\n\nCross-view action modeling, learning, and recognition. J Wang, X Nie, Y Xia, Y Wu, S.-C Zhu, CVPR. J. Wang, X. Nie, Y. Xia, Y. Wu, and S.-C. Zhu. Cross-view action modeling, learning, and recognition. In CVPR, 2014.\n\nK Wang, X Wang, L Lin, M Wang, W Zuo, 3d human activity recognition with reconfigurable convolutional neural networks. In ACM MM. K. Wang, X. Wang, L. Lin, M. Wang, and W. Zuo. 3d human activity recognition with reconfigurable convolutional neural networks. In ACM MM, 2014.\n\nAction recognition from depth maps using deep convolutional neural networks. P Wang, W Li, Z Gao, J Zhang, C Tang, P Ogunbona, THMS. P. Wang, W. Li, Z. Gao, J. Zhang, C. Tang, and P. Ogun- bona. Action recognition from depth maps using deep con- volutional neural networks. In THMS, 2015.\n\nModeling 4d human-object interactions for event and object recognition. P Wei, Y Zhao, N Zheng, S.-C Zhu, ICCV. P. Wei, Y. Zhao, N. Zheng, and S.-C. Zhu. Modeling 4d human-object interactions for event and object recognition. In ICCV, 2013.\n\nSuper normal vector for activity recognition using depth sequences. X Yang, Y Tian, CVPR. X. Yang and Y. Tian. Super normal vector for activity recog- nition using depth sequences. In CVPR, 2014.\n\nA survey on human motion analysis from depth data. M Ye, Q Zhang, L Wang, J Zhu, R Yang, J Gall, Timeof-Flight and Depth Imaging. Sensors, Algorithms, and Applications. M. Ye, Q. Zhang, L. Wang, J. Zhu, R. Yang, and J. Gall. A survey on human motion analysis from depth data. In Time- of-Flight and Depth Imaging. Sensors, Algorithms, and Ap- plications. 2013.\n\nDiscriminative orderlet mining for real-time recognition of human-object interaction. G Yu, Z Liu, J Yuan, ACCV. G. Yu, Z. Liu, and J. Yuan. Discriminative orderlet min- ing for real-time recognition of human-object interaction. In ACCV, 2014.\n\nRgbd-based action recognition datasets: A survey. J Zhang, W Li, P O Ogunbona, P Wang, C Tang, arXivJ. Zhang, W. Li, P. O. Ogunbona, P. Wang, and C. Tang. Rgb- d-based action recognition datasets: A survey. arXiv, 2016.\n\nMicrosoft kinect sensor and its effect. Z Zhang, IEEE Mul-tiMedia. Z. Zhang. Microsoft kinect sensor and its effect. IEEE Mul- tiMedia, 2012.\n\nCo-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks. W Zhu, C Lan, J Xing, W Zeng, Y Li, L Shen, X Xie, AAAIW. Zhu, C. Lan, J. Xing, W. Zeng, Y. Li, L. Shen, and X. Xie. Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks. AAAI, 2016.\n", "annotations": {"author": "[{\"end\":217,\"start\":67},{\"end\":318,\"start\":218},{\"end\":399,\"start\":319},{\"end\":522,\"start\":400}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":72},{\"end\":225,\"start\":222},{\"end\":332,\"start\":330},{\"end\":409,\"start\":405}]", "author_first_name": "[{\"end\":71,\"start\":67},{\"end\":221,\"start\":218},{\"end\":329,\"start\":319},{\"end\":404,\"start\":400}]", "author_affiliation": "[{\"end\":173,\"start\":83},{\"end\":216,\"start\":175},{\"end\":317,\"start\":227},{\"end\":398,\"start\":357},{\"end\":521,\"start\":431}]", "title": "[{\"end\":64,\"start\":1},{\"end\":586,\"start\":523}]", "venue": null, "abstract": "[{\"end\":1863,\"start\":588}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1990,\"start\":1986},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2154,\"start\":2151},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3929,\"start\":3926},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3932,\"start\":3929},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5417,\"start\":5413},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6916,\"start\":6912},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6918,\"start\":6916},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6920,\"start\":6918},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6923,\"start\":6920},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6926,\"start\":6923},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6929,\"start\":6926},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6930,\"start\":6929},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7008,\"start\":7004},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7181,\"start\":7177},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7711,\"start\":7707},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7714,\"start\":7711},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7717,\"start\":7714},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7731,\"start\":7727},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7748,\"start\":7744},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8067,\"start\":8063},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8420,\"start\":8416},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8740,\"start\":8736},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8743,\"start\":8740},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8746,\"start\":8743},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8749,\"start\":8746},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8772,\"start\":8768},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9019,\"start\":9015},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9022,\"start\":9019},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9025,\"start\":9022},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9095,\"start\":9091},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9122,\"start\":9118},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9384,\"start\":9380},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10268,\"start\":10264},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10477,\"start\":10473},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10634,\"start\":10630},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10814,\"start\":10810},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11109,\"start\":11105},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11376,\"start\":11372},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11562,\"start\":11558},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11710,\"start\":11706},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11878,\"start\":11874},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12216,\"start\":12212},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12477,\"start\":12473},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12789,\"start\":12785},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12837,\"start\":12833},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12981,\"start\":12977},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13127,\"start\":13123},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13510,\"start\":13506},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13512,\"start\":13510},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13515,\"start\":13512},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13539,\"start\":13535},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13754,\"start\":13751},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14082,\"start\":14078},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14742,\"start\":14741},{\"end\":17722,\"start\":17649},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18719,\"start\":18715},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":18853,\"start\":18849},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18855,\"start\":18853},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18857,\"start\":18855},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18860,\"start\":18857},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19841,\"start\":19837},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23462,\"start\":23458},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23488,\"start\":23484},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23504,\"start\":23500},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23601,\"start\":23597},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23622,\"start\":23618},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23654,\"start\":23650},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25628,\"start\":25624},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25715,\"start\":25712},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26293,\"start\":26289},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26314,\"start\":26310},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26346,\"start\":26342},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26624,\"start\":26621},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26706,\"start\":26703},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26906,\"start\":26902},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26945,\"start\":26941},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26970,\"start\":26966},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26998,\"start\":26994},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27032,\"start\":27028},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27073,\"start\":27069},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27099,\"start\":27096},{\"end\":28488,\"start\":28480}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30016,\"start\":29565},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31381,\"start\":30017},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31762,\"start\":31382}]", "paragraph": "[{\"end\":2155,\"start\":1879},{\"end\":2705,\"start\":2157},{\"end\":3060,\"start\":2707},{\"end\":3466,\"start\":3062},{\"end\":3723,\"start\":3468},{\"end\":4118,\"start\":3725},{\"end\":5241,\"start\":4120},{\"end\":5883,\"start\":5243},{\"end\":6037,\"start\":5885},{\"end\":6583,\"start\":6039},{\"end\":6932,\"start\":6600},{\"end\":7154,\"start\":6966},{\"end\":7718,\"start\":7156},{\"end\":8048,\"start\":7720},{\"end\":8396,\"start\":8050},{\"end\":8750,\"start\":8398},{\"end\":9070,\"start\":8752},{\"end\":9277,\"start\":9072},{\"end\":9625,\"start\":9279},{\"end\":10102,\"start\":9627},{\"end\":10247,\"start\":10136},{\"end\":10933,\"start\":10249},{\"end\":12033,\"start\":10935},{\"end\":13516,\"start\":12035},{\"end\":13741,\"start\":13518},{\"end\":14064,\"start\":13743},{\"end\":14449,\"start\":14066},{\"end\":14620,\"start\":14451},{\"end\":14743,\"start\":14636},{\"end\":14983,\"start\":14772},{\"end\":15198,\"start\":14985},{\"end\":15494,\"start\":15200},{\"end\":15562,\"start\":15496},{\"end\":15641,\"start\":15564},{\"end\":15916,\"start\":15643},{\"end\":16176,\"start\":15918},{\"end\":16843,\"start\":16178},{\"end\":17058,\"start\":16845},{\"end\":17344,\"start\":17084},{\"end\":17767,\"start\":17373},{\"end\":18183,\"start\":17793},{\"end\":18342,\"start\":18211},{\"end\":18636,\"start\":18344},{\"end\":18861,\"start\":18638},{\"end\":18978,\"start\":18863},{\"end\":19304,\"start\":19007},{\"end\":19509,\"start\":19345},{\"end\":19620,\"start\":19580},{\"end\":20000,\"start\":19622},{\"end\":20560,\"start\":20127},{\"end\":20797,\"start\":20583},{\"end\":21144,\"start\":20826},{\"end\":21692,\"start\":21146},{\"end\":21914,\"start\":21694},{\"end\":22220,\"start\":22144},{\"end\":23010,\"start\":22222},{\"end\":23281,\"start\":23089},{\"end\":23682,\"start\":23304},{\"end\":23758,\"start\":23684},{\"end\":24383,\"start\":23760},{\"end\":24622,\"start\":24385},{\"end\":25209,\"start\":24624},{\"end\":25484,\"start\":25211},{\"end\":25629,\"start\":25486},{\"end\":25770,\"start\":25631},{\"end\":26252,\"start\":25799},{\"end\":26565,\"start\":26254},{\"end\":26842,\"start\":26567},{\"end\":27634,\"start\":26852},{\"end\":27785,\"start\":27636},{\"end\":28297,\"start\":27787},{\"end\":28913,\"start\":28312},{\"end\":29131,\"start\":28915},{\"end\":29291,\"start\":29133},{\"end\":29546,\"start\":29293}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":19344,\"start\":19305},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19579,\"start\":19510},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20126,\"start\":20001},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20582,\"start\":20561},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22096,\"start\":21915},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22143,\"start\":22096},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23074,\"start\":23011}]", "table_ref": "[{\"end\":9512,\"start\":9505},{\"end\":16987,\"start\":16980},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25884,\"start\":25877},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28184,\"start\":28177}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1877,\"start\":1865},{\"attributes\":{\"n\":\"2.\"},\"end\":6598,\"start\":6586},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6964,\"start\":6935},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10134,\"start\":10105},{\"attributes\":{\"n\":\"3.\"},\"end\":14634,\"start\":14623},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14770,\"start\":14746},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17082,\"start\":17061},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":17371,\"start\":17347},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":17791,\"start\":17770},{\"attributes\":{\"n\":\"4.\"},\"end\":18209,\"start\":18186},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19005,\"start\":18981},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20824,\"start\":20800},{\"attributes\":{\"n\":\"5.\"},\"end\":23087,\"start\":23076},{\"attributes\":{\"n\":\"5.1.\"},\"end\":23302,\"start\":23284},{\"attributes\":{\"n\":\"5.2.\"},\"end\":25797,\"start\":25773},{\"end\":26850,\"start\":26845},{\"attributes\":{\"n\":\"6.\"},\"end\":28310,\"start\":28300},{\"attributes\":{\"n\":\"7.\"},\"end\":29564,\"start\":29549},{\"end\":29576,\"start\":29566},{\"end\":31392,\"start\":31383}]", "table": "[{\"end\":31381,\"start\":30303}]", "figure_caption": "[{\"end\":30016,\"start\":29578},{\"end\":30303,\"start\":30019},{\"end\":31762,\"start\":31394}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15493,\"start\":15485},{\"end\":16041,\"start\":16033},{\"end\":20296,\"start\":20288},{\"end\":20595,\"start\":20587},{\"end\":22219,\"start\":22211},{\"end\":22343,\"start\":22335},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24041,\"start\":24033}]", "bib_author_first_name": "[{\"end\":32644,\"start\":32643},{\"end\":32656,\"start\":32655},{\"end\":32767,\"start\":32766},{\"end\":32776,\"start\":32775},{\"end\":32778,\"start\":32777},{\"end\":32788,\"start\":32787},{\"end\":32796,\"start\":32795},{\"end\":33055,\"start\":33054},{\"end\":33062,\"start\":33061},{\"end\":33069,\"start\":33068},{\"end\":33076,\"start\":33075},{\"end\":33383,\"start\":33382},{\"end\":33391,\"start\":33390},{\"end\":33401,\"start\":33400},{\"end\":33657,\"start\":33656},{\"end\":33665,\"start\":33664},{\"end\":33672,\"start\":33671},{\"end\":33870,\"start\":33869},{\"end\":33879,\"start\":33878},{\"end\":33886,\"start\":33885},{\"end\":33892,\"start\":33891},{\"end\":33901,\"start\":33900},{\"end\":34121,\"start\":34120},{\"end\":34134,\"start\":34133},{\"end\":34149,\"start\":34148},{\"end\":34405,\"start\":34404},{\"end\":34416,\"start\":34415},{\"end\":34429,\"start\":34428},{\"end\":34443,\"start\":34442},{\"end\":34455,\"start\":34454},{\"end\":34470,\"start\":34469},{\"end\":34480,\"start\":34479},{\"end\":34777,\"start\":34776},{\"end\":34783,\"start\":34782},{\"end\":34791,\"start\":34790},{\"end\":34994,\"start\":34993},{\"end\":35009,\"start\":35008},{\"end\":35018,\"start\":35017},{\"end\":35213,\"start\":35209},{\"end\":35223,\"start\":35219},{\"end\":35235,\"start\":35231},{\"end\":35247,\"start\":35243},{\"end\":35258,\"start\":35254},{\"end\":35400,\"start\":35399},{\"end\":35407,\"start\":35406},{\"end\":35416,\"start\":35415},{\"end\":35424,\"start\":35423},{\"end\":35708,\"start\":35707},{\"end\":35715,\"start\":35714},{\"end\":35723,\"start\":35722},{\"end\":35729,\"start\":35728},{\"end\":35946,\"start\":35945},{\"end\":35960,\"start\":35959},{\"end\":36155,\"start\":36151},{\"end\":36164,\"start\":36160},{\"end\":36173,\"start\":36172},{\"end\":36180,\"start\":36179},{\"end\":36380,\"start\":36379},{\"end\":36392,\"start\":36391},{\"end\":36403,\"start\":36402},{\"end\":36589,\"start\":36588},{\"end\":36597,\"start\":36596},{\"end\":36793,\"start\":36792},{\"end\":36795,\"start\":36794},{\"end\":36806,\"start\":36805},{\"end\":36815,\"start\":36814},{\"end\":36994,\"start\":36993},{\"end\":37000,\"start\":36999},{\"end\":37009,\"start\":37008},{\"end\":37186,\"start\":37185},{\"end\":37192,\"start\":37191},{\"end\":37202,\"start\":37198},{\"end\":37391,\"start\":37390},{\"end\":37398,\"start\":37397},{\"end\":37624,\"start\":37623},{\"end\":37631,\"start\":37630},{\"end\":37639,\"start\":37638},{\"end\":37876,\"start\":37875},{\"end\":37882,\"start\":37881},{\"end\":37890,\"start\":37889},{\"end\":38114,\"start\":38113},{\"end\":38125,\"start\":38124},{\"end\":38352,\"start\":38351},{\"end\":38363,\"start\":38362},{\"end\":38580,\"start\":38579},{\"end\":38591,\"start\":38590},{\"end\":38602,\"start\":38601},{\"end\":38611,\"start\":38610},{\"end\":38861,\"start\":38860},{\"end\":38872,\"start\":38871},{\"end\":38883,\"start\":38882},{\"end\":38885,\"start\":38884},{\"end\":38894,\"start\":38893},{\"end\":39159,\"start\":39158},{\"end\":39170,\"start\":39169},{\"end\":39181,\"start\":39180},{\"end\":39190,\"start\":39189},{\"end\":39444,\"start\":39443},{\"end\":39455,\"start\":39454},{\"end\":39636,\"start\":39635},{\"end\":39647,\"start\":39646},{\"end\":39751,\"start\":39750},{\"end\":39767,\"start\":39763},{\"end\":39773,\"start\":39772},{\"end\":39781,\"start\":39780},{\"end\":40074,\"start\":40073},{\"end\":40087,\"start\":40086},{\"end\":40089,\"start\":40088},{\"end\":40095,\"start\":40094},{\"end\":40103,\"start\":40102},{\"end\":40316,\"start\":40315},{\"end\":40329,\"start\":40328},{\"end\":40340,\"start\":40336},{\"end\":40522,\"start\":40521},{\"end\":40530,\"start\":40529},{\"end\":40539,\"start\":40538},{\"end\":40549,\"start\":40548},{\"end\":40743,\"start\":40742},{\"end\":40756,\"start\":40755},{\"end\":40767,\"start\":40766},{\"end\":40771,\"start\":40768},{\"end\":40959,\"start\":40958},{\"end\":40970,\"start\":40969},{\"end\":40983,\"start\":40979},{\"end\":41191,\"start\":41190},{\"end\":41206,\"start\":41205},{\"end\":41216,\"start\":41215},{\"end\":41445,\"start\":41444},{\"end\":41453,\"start\":41452},{\"end\":41460,\"start\":41459},{\"end\":41466,\"start\":41465},{\"end\":41665,\"start\":41664},{\"end\":41673,\"start\":41672},{\"end\":41680,\"start\":41679},{\"end\":41686,\"start\":41685},{\"end\":41866,\"start\":41865},{\"end\":41874,\"start\":41873},{\"end\":41881,\"start\":41880},{\"end\":41888,\"start\":41887},{\"end\":41897,\"start\":41893},{\"end\":42028,\"start\":42027},{\"end\":42036,\"start\":42035},{\"end\":42044,\"start\":42043},{\"end\":42051,\"start\":42050},{\"end\":42059,\"start\":42058},{\"end\":42381,\"start\":42380},{\"end\":42389,\"start\":42388},{\"end\":42395,\"start\":42394},{\"end\":42402,\"start\":42401},{\"end\":42411,\"start\":42410},{\"end\":42419,\"start\":42418},{\"end\":42666,\"start\":42665},{\"end\":42673,\"start\":42672},{\"end\":42681,\"start\":42680},{\"end\":42693,\"start\":42689},{\"end\":42904,\"start\":42903},{\"end\":42912,\"start\":42911},{\"end\":43084,\"start\":43083},{\"end\":43090,\"start\":43089},{\"end\":43099,\"start\":43098},{\"end\":43107,\"start\":43106},{\"end\":43114,\"start\":43113},{\"end\":43122,\"start\":43121},{\"end\":43481,\"start\":43480},{\"end\":43487,\"start\":43486},{\"end\":43494,\"start\":43493},{\"end\":43690,\"start\":43689},{\"end\":43699,\"start\":43698},{\"end\":43705,\"start\":43704},{\"end\":43707,\"start\":43706},{\"end\":43719,\"start\":43718},{\"end\":43727,\"start\":43726},{\"end\":43901,\"start\":43900},{\"end\":44111,\"start\":44110},{\"end\":44118,\"start\":44117},{\"end\":44125,\"start\":44124},{\"end\":44133,\"start\":44132},{\"end\":44141,\"start\":44140},{\"end\":44147,\"start\":44146},{\"end\":44155,\"start\":44154}]", "bib_author_last_name": "[{\"end\":32653,\"start\":32645},{\"end\":32660,\"start\":32657},{\"end\":32773,\"start\":32768},{\"end\":32785,\"start\":32779},{\"end\":32793,\"start\":32789},{\"end\":32804,\"start\":32797},{\"end\":33059,\"start\":33056},{\"end\":33066,\"start\":33063},{\"end\":33073,\"start\":33070},{\"end\":33081,\"start\":33077},{\"end\":33388,\"start\":33384},{\"end\":33398,\"start\":33392},{\"end\":33413,\"start\":33402},{\"end\":33662,\"start\":33658},{\"end\":33669,\"start\":33666},{\"end\":33681,\"start\":33673},{\"end\":33876,\"start\":33871},{\"end\":33883,\"start\":33880},{\"end\":33889,\"start\":33887},{\"end\":33898,\"start\":33893},{\"end\":33906,\"start\":33902},{\"end\":34131,\"start\":34122},{\"end\":34146,\"start\":34135},{\"end\":34157,\"start\":34150},{\"end\":34413,\"start\":34406},{\"end\":34426,\"start\":34417},{\"end\":34440,\"start\":34430},{\"end\":34452,\"start\":34444},{\"end\":34467,\"start\":34456},{\"end\":34477,\"start\":34471},{\"end\":34488,\"start\":34481},{\"end\":34780,\"start\":34778},{\"end\":34788,\"start\":34784},{\"end\":34796,\"start\":34792},{\"end\":35006,\"start\":34995},{\"end\":35015,\"start\":35010},{\"end\":35025,\"start\":35019},{\"end\":35217,\"start\":35214},{\"end\":35229,\"start\":35224},{\"end\":35241,\"start\":35236},{\"end\":35252,\"start\":35248},{\"end\":35262,\"start\":35259},{\"end\":35404,\"start\":35401},{\"end\":35413,\"start\":35408},{\"end\":35421,\"start\":35417},{\"end\":35430,\"start\":35425},{\"end\":35712,\"start\":35709},{\"end\":35720,\"start\":35716},{\"end\":35726,\"start\":35724},{\"end\":35737,\"start\":35730},{\"end\":35957,\"start\":35947},{\"end\":35972,\"start\":35961},{\"end\":36158,\"start\":36156},{\"end\":36170,\"start\":36165},{\"end\":36177,\"start\":36174},{\"end\":36186,\"start\":36181},{\"end\":36389,\"start\":36381},{\"end\":36400,\"start\":36393},{\"end\":36406,\"start\":36404},{\"end\":36594,\"start\":36590},{\"end\":36600,\"start\":36598},{\"end\":36803,\"start\":36796},{\"end\":36812,\"start\":36807},{\"end\":36822,\"start\":36816},{\"end\":36997,\"start\":36995},{\"end\":37006,\"start\":37001},{\"end\":37013,\"start\":37010},{\"end\":37189,\"start\":37187},{\"end\":37196,\"start\":37193},{\"end\":37207,\"start\":37203},{\"end\":37395,\"start\":37392},{\"end\":37403,\"start\":37399},{\"end\":37628,\"start\":37625},{\"end\":37636,\"start\":37632},{\"end\":37642,\"start\":37640},{\"end\":37879,\"start\":37877},{\"end\":37887,\"start\":37883},{\"end\":37897,\"start\":37891},{\"end\":38122,\"start\":38115},{\"end\":38133,\"start\":38126},{\"end\":38360,\"start\":38353},{\"end\":38367,\"start\":38364},{\"end\":38588,\"start\":38581},{\"end\":38599,\"start\":38592},{\"end\":38608,\"start\":38603},{\"end\":38616,\"start\":38612},{\"end\":38869,\"start\":38862},{\"end\":38880,\"start\":38873},{\"end\":38891,\"start\":38886},{\"end\":38899,\"start\":38895},{\"end\":39167,\"start\":39160},{\"end\":39178,\"start\":39171},{\"end\":39187,\"start\":39182},{\"end\":39195,\"start\":39191},{\"end\":39452,\"start\":39445},{\"end\":39460,\"start\":39456},{\"end\":39644,\"start\":39637},{\"end\":39652,\"start\":39648},{\"end\":39761,\"start\":39752},{\"end\":39770,\"start\":39768},{\"end\":39778,\"start\":39774},{\"end\":39786,\"start\":39782},{\"end\":40084,\"start\":40075},{\"end\":40092,\"start\":40090},{\"end\":40100,\"start\":40096},{\"end\":40108,\"start\":40104},{\"end\":40326,\"start\":40317},{\"end\":40334,\"start\":40330},{\"end\":40343,\"start\":40341},{\"end\":40527,\"start\":40523},{\"end\":40536,\"start\":40531},{\"end\":40546,\"start\":40540},{\"end\":40556,\"start\":40550},{\"end\":40753,\"start\":40744},{\"end\":40764,\"start\":40757},{\"end\":40774,\"start\":40772},{\"end\":40967,\"start\":40960},{\"end\":40977,\"start\":40971},{\"end\":40986,\"start\":40984},{\"end\":41203,\"start\":41192},{\"end\":41213,\"start\":41207},{\"end\":41226,\"start\":41217},{\"end\":41450,\"start\":41446},{\"end\":41457,\"start\":41454},{\"end\":41463,\"start\":41461},{\"end\":41471,\"start\":41467},{\"end\":41670,\"start\":41666},{\"end\":41677,\"start\":41674},{\"end\":41683,\"start\":41681},{\"end\":41691,\"start\":41687},{\"end\":41871,\"start\":41867},{\"end\":41878,\"start\":41875},{\"end\":41885,\"start\":41882},{\"end\":41891,\"start\":41889},{\"end\":41901,\"start\":41898},{\"end\":42033,\"start\":42029},{\"end\":42041,\"start\":42037},{\"end\":42048,\"start\":42045},{\"end\":42056,\"start\":42052},{\"end\":42063,\"start\":42060},{\"end\":42386,\"start\":42382},{\"end\":42392,\"start\":42390},{\"end\":42399,\"start\":42396},{\"end\":42408,\"start\":42403},{\"end\":42416,\"start\":42412},{\"end\":42428,\"start\":42420},{\"end\":42670,\"start\":42667},{\"end\":42678,\"start\":42674},{\"end\":42687,\"start\":42682},{\"end\":42697,\"start\":42694},{\"end\":42909,\"start\":42905},{\"end\":42917,\"start\":42913},{\"end\":43087,\"start\":43085},{\"end\":43096,\"start\":43091},{\"end\":43104,\"start\":43100},{\"end\":43111,\"start\":43108},{\"end\":43119,\"start\":43115},{\"end\":43127,\"start\":43123},{\"end\":43484,\"start\":43482},{\"end\":43491,\"start\":43488},{\"end\":43499,\"start\":43495},{\"end\":43696,\"start\":43691},{\"end\":43702,\"start\":43700},{\"end\":43716,\"start\":43708},{\"end\":43724,\"start\":43720},{\"end\":43732,\"start\":43728},{\"end\":43907,\"start\":43902},{\"end\":44115,\"start\":44112},{\"end\":44122,\"start\":44119},{\"end\":44130,\"start\":44126},{\"end\":44138,\"start\":44134},{\"end\":44144,\"start\":44142},{\"end\":44152,\"start\":44148},{\"end\":44159,\"start\":44156}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32764,\"start\":32592},{\"attributes\":{\"id\":\"b1\"},\"end\":32984,\"start\":32766},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4351505},\"end\":33263,\"start\":32986},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":549946},\"end\":33599,\"start\":33265},{\"attributes\":{\"id\":\"b4\"},\"end\":33801,\"start\":33601},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14481032},\"end\":34062,\"start\":33803},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14365368},\"end\":34319,\"start\":34064},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5736847},\"end\":34697,\"start\":34321},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8040013},\"end\":34926,\"start\":34699},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":215776139},\"end\":35153,\"start\":34928},{\"attributes\":{\"id\":\"b10\"},\"end\":35397,\"start\":35155},{\"attributes\":{\"id\":\"b11\"},\"end\":35640,\"start\":35399},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206663593},\"end\":35919,\"start\":35642},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1915014},\"end\":36077,\"start\":35921},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5652420},\"end\":36327,\"start\":36079},{\"attributes\":{\"id\":\"b15\"},\"end\":36513,\"start\":36329},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6474290},\"end\":36716,\"start\":36515},{\"attributes\":{\"id\":\"b17\"},\"end\":36943,\"start\":36718},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4333408},\"end\":37132,\"start\":36945},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1271270},\"end\":37311,\"start\":37134},{\"attributes\":{\"id\":\"b20\"},\"end\":37521,\"start\":37313},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":10527367},\"end\":37792,\"start\":37523},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":12186966},\"end\":38051,\"start\":37794},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6428408},\"end\":38262,\"start\":38053},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6482700},\"end\":38499,\"start\":38264},{\"attributes\":{\"id\":\"b25\"},\"end\":38764,\"start\":38501},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7405026},\"end\":39065,\"start\":38766},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8541055},\"end\":39359,\"start\":39067},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":9118111},\"end\":39588,\"start\":39361},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":206593470},\"end\":39748,\"start\":39590},{\"attributes\":{\"id\":\"b30\"},\"end\":40001,\"start\":39750},{\"attributes\":{\"id\":\"b31\"},\"end\":40243,\"start\":40003},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14855641},\"end\":40476,\"start\":40245},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":606161},\"end\":40688,\"start\":40478},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":7961699},\"end\":40893,\"start\":40690},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":11060863},\"end\":41108,\"start\":40895},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1732632},\"end\":41373,\"start\":41110},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":11964979},\"end\":41601,\"start\":41375},{\"attributes\":{\"id\":\"b38\"},\"end\":41808,\"start\":41603},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":2239612},\"end\":42025,\"start\":41810},{\"attributes\":{\"id\":\"b40\"},\"end\":42301,\"start\":42027},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":32229496},\"end\":42591,\"start\":42303},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":10631900},\"end\":42833,\"start\":42593},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":574767},\"end\":43030,\"start\":42835},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":12627049},\"end\":43392,\"start\":43032},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":10497540},\"end\":43637,\"start\":43394},{\"attributes\":{\"id\":\"b46\"},\"end\":43858,\"start\":43639},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":8629444},\"end\":44001,\"start\":43860},{\"attributes\":{\"id\":\"b48\"},\"end\":44345,\"start\":44003}]", "bib_title": "[{\"end\":33052,\"start\":32986},{\"end\":33380,\"start\":33265},{\"end\":33867,\"start\":33803},{\"end\":34118,\"start\":34064},{\"end\":34402,\"start\":34321},{\"end\":34774,\"start\":34699},{\"end\":34991,\"start\":34928},{\"end\":35705,\"start\":35642},{\"end\":35943,\"start\":35921},{\"end\":36149,\"start\":36079},{\"end\":36586,\"start\":36515},{\"end\":36991,\"start\":36945},{\"end\":37183,\"start\":37134},{\"end\":37621,\"start\":37523},{\"end\":37873,\"start\":37794},{\"end\":38111,\"start\":38053},{\"end\":38349,\"start\":38264},{\"end\":38858,\"start\":38766},{\"end\":39156,\"start\":39067},{\"end\":39441,\"start\":39361},{\"end\":39633,\"start\":39590},{\"end\":40313,\"start\":40245},{\"end\":40519,\"start\":40478},{\"end\":40740,\"start\":40690},{\"end\":40956,\"start\":40895},{\"end\":41188,\"start\":41110},{\"end\":41442,\"start\":41375},{\"end\":41863,\"start\":41810},{\"end\":42378,\"start\":42303},{\"end\":42663,\"start\":42593},{\"end\":42901,\"start\":42835},{\"end\":43081,\"start\":43032},{\"end\":43478,\"start\":43394},{\"end\":43898,\"start\":43860}]", "bib_author": "[{\"end\":32655,\"start\":32643},{\"end\":32662,\"start\":32655},{\"end\":32775,\"start\":32766},{\"end\":32787,\"start\":32775},{\"end\":32795,\"start\":32787},{\"end\":32806,\"start\":32795},{\"end\":33061,\"start\":33054},{\"end\":33068,\"start\":33061},{\"end\":33075,\"start\":33068},{\"end\":33083,\"start\":33075},{\"end\":33390,\"start\":33382},{\"end\":33400,\"start\":33390},{\"end\":33415,\"start\":33400},{\"end\":33664,\"start\":33656},{\"end\":33671,\"start\":33664},{\"end\":33683,\"start\":33671},{\"end\":33878,\"start\":33869},{\"end\":33885,\"start\":33878},{\"end\":33891,\"start\":33885},{\"end\":33900,\"start\":33891},{\"end\":33908,\"start\":33900},{\"end\":34133,\"start\":34120},{\"end\":34148,\"start\":34133},{\"end\":34159,\"start\":34148},{\"end\":34415,\"start\":34404},{\"end\":34428,\"start\":34415},{\"end\":34442,\"start\":34428},{\"end\":34454,\"start\":34442},{\"end\":34469,\"start\":34454},{\"end\":34479,\"start\":34469},{\"end\":34490,\"start\":34479},{\"end\":34782,\"start\":34776},{\"end\":34790,\"start\":34782},{\"end\":34798,\"start\":34790},{\"end\":35008,\"start\":34993},{\"end\":35017,\"start\":35008},{\"end\":35027,\"start\":35017},{\"end\":35219,\"start\":35209},{\"end\":35231,\"start\":35219},{\"end\":35243,\"start\":35231},{\"end\":35254,\"start\":35243},{\"end\":35264,\"start\":35254},{\"end\":35406,\"start\":35399},{\"end\":35415,\"start\":35406},{\"end\":35423,\"start\":35415},{\"end\":35432,\"start\":35423},{\"end\":35714,\"start\":35707},{\"end\":35722,\"start\":35714},{\"end\":35728,\"start\":35722},{\"end\":35739,\"start\":35728},{\"end\":35959,\"start\":35945},{\"end\":35974,\"start\":35959},{\"end\":36160,\"start\":36151},{\"end\":36172,\"start\":36160},{\"end\":36179,\"start\":36172},{\"end\":36188,\"start\":36179},{\"end\":36391,\"start\":36379},{\"end\":36402,\"start\":36391},{\"end\":36408,\"start\":36402},{\"end\":36596,\"start\":36588},{\"end\":36602,\"start\":36596},{\"end\":36805,\"start\":36792},{\"end\":36814,\"start\":36805},{\"end\":36824,\"start\":36814},{\"end\":36999,\"start\":36993},{\"end\":37008,\"start\":36999},{\"end\":37015,\"start\":37008},{\"end\":37191,\"start\":37185},{\"end\":37198,\"start\":37191},{\"end\":37209,\"start\":37198},{\"end\":37397,\"start\":37390},{\"end\":37405,\"start\":37397},{\"end\":37630,\"start\":37623},{\"end\":37638,\"start\":37630},{\"end\":37644,\"start\":37638},{\"end\":37881,\"start\":37875},{\"end\":37889,\"start\":37881},{\"end\":37899,\"start\":37889},{\"end\":38124,\"start\":38113},{\"end\":38135,\"start\":38124},{\"end\":38362,\"start\":38351},{\"end\":38369,\"start\":38362},{\"end\":38590,\"start\":38579},{\"end\":38601,\"start\":38590},{\"end\":38610,\"start\":38601},{\"end\":38618,\"start\":38610},{\"end\":38871,\"start\":38860},{\"end\":38882,\"start\":38871},{\"end\":38893,\"start\":38882},{\"end\":38901,\"start\":38893},{\"end\":39169,\"start\":39158},{\"end\":39180,\"start\":39169},{\"end\":39189,\"start\":39180},{\"end\":39197,\"start\":39189},{\"end\":39454,\"start\":39443},{\"end\":39462,\"start\":39454},{\"end\":39646,\"start\":39635},{\"end\":39654,\"start\":39646},{\"end\":39763,\"start\":39750},{\"end\":39772,\"start\":39763},{\"end\":39780,\"start\":39772},{\"end\":39788,\"start\":39780},{\"end\":40086,\"start\":40073},{\"end\":40094,\"start\":40086},{\"end\":40102,\"start\":40094},{\"end\":40110,\"start\":40102},{\"end\":40328,\"start\":40315},{\"end\":40336,\"start\":40328},{\"end\":40345,\"start\":40336},{\"end\":40529,\"start\":40521},{\"end\":40538,\"start\":40529},{\"end\":40548,\"start\":40538},{\"end\":40558,\"start\":40548},{\"end\":40755,\"start\":40742},{\"end\":40766,\"start\":40755},{\"end\":40776,\"start\":40766},{\"end\":40969,\"start\":40958},{\"end\":40979,\"start\":40969},{\"end\":40988,\"start\":40979},{\"end\":41205,\"start\":41190},{\"end\":41215,\"start\":41205},{\"end\":41228,\"start\":41215},{\"end\":41452,\"start\":41444},{\"end\":41459,\"start\":41452},{\"end\":41465,\"start\":41459},{\"end\":41473,\"start\":41465},{\"end\":41672,\"start\":41664},{\"end\":41679,\"start\":41672},{\"end\":41685,\"start\":41679},{\"end\":41693,\"start\":41685},{\"end\":41873,\"start\":41865},{\"end\":41880,\"start\":41873},{\"end\":41887,\"start\":41880},{\"end\":41893,\"start\":41887},{\"end\":41903,\"start\":41893},{\"end\":42035,\"start\":42027},{\"end\":42043,\"start\":42035},{\"end\":42050,\"start\":42043},{\"end\":42058,\"start\":42050},{\"end\":42065,\"start\":42058},{\"end\":42388,\"start\":42380},{\"end\":42394,\"start\":42388},{\"end\":42401,\"start\":42394},{\"end\":42410,\"start\":42401},{\"end\":42418,\"start\":42410},{\"end\":42430,\"start\":42418},{\"end\":42672,\"start\":42665},{\"end\":42680,\"start\":42672},{\"end\":42689,\"start\":42680},{\"end\":42699,\"start\":42689},{\"end\":42911,\"start\":42903},{\"end\":42919,\"start\":42911},{\"end\":43089,\"start\":43083},{\"end\":43098,\"start\":43089},{\"end\":43106,\"start\":43098},{\"end\":43113,\"start\":43106},{\"end\":43121,\"start\":43113},{\"end\":43129,\"start\":43121},{\"end\":43486,\"start\":43480},{\"end\":43493,\"start\":43486},{\"end\":43501,\"start\":43493},{\"end\":43698,\"start\":43689},{\"end\":43704,\"start\":43698},{\"end\":43718,\"start\":43704},{\"end\":43726,\"start\":43718},{\"end\":43734,\"start\":43726},{\"end\":43909,\"start\":43900},{\"end\":44117,\"start\":44110},{\"end\":44124,\"start\":44117},{\"end\":44132,\"start\":44124},{\"end\":44140,\"start\":44132},{\"end\":44146,\"start\":44140},{\"end\":44154,\"start\":44146},{\"end\":44161,\"start\":44154}]", "bib_venue": "[{\"end\":32641,\"start\":32592},{\"end\":32865,\"start\":32806},{\"end\":33116,\"start\":33083},{\"end\":33419,\"start\":33415},{\"end\":33654,\"start\":33601},{\"end\":33922,\"start\":33908},{\"end\":34182,\"start\":34159},{\"end\":34494,\"start\":34490},{\"end\":34802,\"start\":34798},{\"end\":35031,\"start\":35027},{\"end\":35207,\"start\":35155},{\"end\":35510,\"start\":35432},{\"end\":35771,\"start\":35739},{\"end\":35992,\"start\":35974},{\"end\":36192,\"start\":36188},{\"end\":36377,\"start\":36329},{\"end\":36606,\"start\":36602},{\"end\":36790,\"start\":36718},{\"end\":37029,\"start\":37015},{\"end\":37213,\"start\":37209},{\"end\":37388,\"start\":37313},{\"end\":37648,\"start\":37644},{\"end\":37913,\"start\":37899},{\"end\":38149,\"start\":38135},{\"end\":38373,\"start\":38369},{\"end\":38577,\"start\":38501},{\"end\":38905,\"start\":38901},{\"end\":39201,\"start\":39197},{\"end\":39466,\"start\":39462},{\"end\":39658,\"start\":39654},{\"end\":39866,\"start\":39788},{\"end\":40071,\"start\":40003},{\"end\":40351,\"start\":40345},{\"end\":40572,\"start\":40558},{\"end\":40780,\"start\":40776},{\"end\":40992,\"start\":40988},{\"end\":41232,\"start\":41228},{\"end\":41477,\"start\":41473},{\"end\":41662,\"start\":41603},{\"end\":41907,\"start\":41903},{\"end\":42155,\"start\":42065},{\"end\":42434,\"start\":42430},{\"end\":42703,\"start\":42699},{\"end\":42923,\"start\":42919},{\"end\":43199,\"start\":43129},{\"end\":43505,\"start\":43501},{\"end\":43687,\"start\":43639},{\"end\":43925,\"start\":43909},{\"end\":44108,\"start\":44003}]"}}}, "year": 2023, "month": 12, "day": 17}