{"id": 231977234, "updated": "2022-01-29 21:13:22.116", "metadata": {"title": "A Visual Real-time Fire Detection using Single Shot MultiBox Detector for UAV-based Fire Surveillance", "authors": "[{\"middle\":[\"Q.\"],\"last\":\"Nguyen\",\"first\":\"A.\"},{\"middle\":[\"T.\"],\"last\":\"Nguyen\",\"first\":\"H.\"},{\"middle\":[\"C.\"],\"last\":\"Tran\",\"first\":\"V.\"},{\"middle\":[\"X\"],\"last\":\"Pham\",\"first\":\"Huy\"},{\"middle\":[],\"last\":\"Pestana\",\"first\":\"J.\"}]", "venue": "2020 IEEE Eighth International Conference on Communications and Electronics (ICCE)", "journal": "2020 IEEE Eighth International Conference on Communications and Electronics (ICCE)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Early fire detection and alarm are significantly important to reduce the losses caused by fire. Conventional methods in fire detection using smoke and heat detectors have disadvantages in accuracy, latency as well as the detection area. In this paper, we propose and implement a real-time fire detection solution for large area surveillance using the unmanned aerial vehicle with an integrated visual detection and alarm system. The system includes a low-cost camera, a light weight companion computer, a flight controller as well as localization and telemetry modules. To achieve real-time detection, Single Shot MultiBox Detector (SSD) algorithm is implemented as the heart of the system. We used MobileNets base model, which more efficient for mobile and embedded vision applications, instead of conventional VGG-16/ResNet model to achieve the mean average precision of 92.7% with the detection speed of 26 FPS.", "fields_of_study": null, "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1109/icce48956.2021.9352080"}}, "content": {"source": {"pdf_hash": "22d5354270bfb0cccd153a0b54f7ee514e1dc0f4", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "357b3b2acfe603e84b9904cf9688f2c65996dcba", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/22d5354270bfb0cccd153a0b54f7ee514e1dc0f4.txt", "contents": "\nA Visual Real-time Fire Detection using Single Shot MultiBox Detector for UAV-based Fire Surveillance\n\n\nA Q Nguyen \nSchool of Electronics and Telecommunications\nHanoi University of Science and Technology\nVietnam\n\nInstitute for Computer Graphics and Vision (ICG)\nGraz University of Technology (TU Graz)\nAustria\n\nH T Nguyen \nV C Tran \nSchool of Electronics and Telecommunications\nHanoi University of Science and Technology\nVietnam\n\nInstitute for Computer Graphics and Vision (ICG)\nGraz University of Technology (TU Graz)\nAustria\n\nHuy X Pham huy.pham@eng.au.dk \nSchool of Electronics and Telecommunications\nHanoi University of Science and Technology\nVietnam\n\nDepartment of Engineering\nAarhus University\nDenmark\n\nInstitute for Computer Graphics and Vision (ICG)\nGraz University of Technology (TU Graz)\nAustria\n\nJ Pestana pestana@icg.tugraz.at \nSchool of Electronics and Telecommunications\nHanoi University of Science and Technology\nVietnam\n\nInstitute for Computer Graphics and Vision (ICG)\nGraz University of Technology (TU Graz)\nAustria\n\nA Visual Real-time Fire Detection using Single Shot MultiBox Detector for UAV-based Fire Surveillance\n10.1109/ICCE48956.2021.9352080/20/$31.00Index Terms-fire detectionSSDreal-time object detectionCNNUAV\nEarly fire detection and alarm are significantly important to reduce the losses caused by fire. Conventional methods in fire detection using smoke and heat detectors have disadvantages in accuracy, latency as well as the detection area. In this paper, we propose and implement a real-time fire detection solution for large area surveillance using the unmanned aerial vehicle with an integrated visual detection and alarm system. The system includes a low-cost camera, a light weight companion computer, a flight controller as well as localization and telemetry modules. To achieve real-time detection, Single Shot MultiBox Detector (SSD) algorithm is implemented as the heart of the system. We used MobileNets base model, which more efficient for mobile and embedded vision applications, instead of conventional VGG-16/ResNet model to achieve the mean average precision of 92.7% with the detection speed of 26 FPS.\n\nI. INTRODUCTION\n\nForest fires are a common problem across the world posing significant threats to terrestrial, aquatic and atmospheric systems also. Every year, extreme wildfires lead to loss of forest wealth, biodiversity, ecosystem functioning and landscape stability. In the US, the 2018 wildfire season was the deadliest and most destructive wildfire season ever recorded in California, with a total of 8,527 fires burning an area of 1,893,913 acres, the largest area of burned acreage recorded in a fire season, according to the California Department of Forestry and Fire Protection, as of December 21, 2018 [1]. Vietnam, with a large forest area, also faces with a high risk of fire forest every year due to both natural causes and human causes. Therefore, early fire detection and alarm with high sensitivity and accuracy are essential to reduce fire losses. However, traditional fire detection technologies, like smoke and heat detectors [2], are not suitable for large spaces such as forests. Besides, existing fire detection system cannot detect fire at an early stage since it requires some input to sense. Furthermore, for remote sensing detection systems, sensors have to be placed at appropriate locations and the system needs some specific level of these inputs to raise an alarm. In larger areas, more sensors will be placed, increasing investment cost of the system. Due to the limitations of the above detection technologies such as missed detection, false alarms, detection delays, making it difficult to achieve early and accurate fire warnings.\n\nWith rapidly growing technologies such as advance digital cameras, image processing, artificial intelligence including computer vision and deep learning, the inefficient conventional fire detection methods could be replaced by the vision-based systems. Image fire detection system has many advantages such as early fire detection, high accuracy, flexible system installation. A low-cost camera in combination with a small but powerful embedded computer can be used for visionbased fire detection and alarm system. Image data taken from the camera will be real-time processed by some algorithms to determine the presence of a fire or fire risk. There are different methods for detecting the fire using image processing algorithms. The simplest one is to use RGB color model with a threshold since the fire area in an image usually has red color [3]. However, this method subjects to false positives by nature red colors from the surrounding environment. On the other hand, image recognition algorithms based on Convolutional Neural Networks (CNNs) can automatically learn and extract complex image features effectively. This kind of algorithms has attracted great attentions and achieved excellent performances on visual search, automatic driving, medical diagnosis, etc. Therefore, some scholars introduce CNNs into the field of image fire detection, thereby developing the self-learned algorithm in collection of fire image features [4]- [8]. There are many CNNs that has been researched for fire detection such as Faster R-CNN, Yolo. Wei Liu [9] proposed Single Shot MultiBox Detector (SSD), a fast and high accuracy object detection method, which makes it suitable for real-time detectors even in limited computational resources.\n\nFor fire detection in large area of forest without the implementation of many sensors/cameras, unmanned aerial vehicles (UAV) can be used, such as in [10]. Recently, UAVs, or more commonly known as drones, have achieved great development and rapidly become a hot topic because of their diverse applications such as surveillance, disaster management, pollution monitoring, film-making, and military reconnaissance. UAV based computer vision system is now as a very promising option for forest fires surveillance 978-1 -7281 -5471 -8/20/$31.00 \u00a92020 IEEE and detection [11]. With the integration of a fire detection algorithm and autonomous flight controller flying drones in a large area could help to reduce much human effort to find and detect fire. Whenever a drone detects a fire, an urgent alarm including the information of drone's localization (GPS) and the fire detected image will be sent to a monitoring center via wireless link using a telemetry module. Then, rescue actions could be performed by the people in charge in time.\n\nThis paper proposes a completed real-time solution using SSD method for fire detection using a low-cost camera and a companion computer for UAV-based fire surveillance [12]. The remain parts of the paper are organized as follows. Part II describes the design of the visual fire detection system. Part III shows the implementation of an SSD algorithm by using MobileNet base network for training and the verification by using some low-cost companion computers. The final results and further discussion are shown in part IV. The visual fire detection system for UAV-based fire surveil lance is shown in Fig. 1a. The drone flies through a large area such as a forest to find and detect fire by using an integrated camera. once a fire is detected as surrounded by the dashed line, the drone will send an urgent alarm through a wireless communication link to a monitoring center. The alarm includes the detected image and the GPS position of the drone. Base on the information, the monitoring center could have a quick response to solve the fire problem. Fig. 1b shows the block design of the UAV for fire detection and alarm. The Flight controller with localization and state estimation modules using GPS and IMU are used to control the flight path of the drone. In this paper, we use PX4, one of the most popular and open-source flight controllers [13]. For image capturing, a low-cost digital camera Logitech C270 which has max resolution 720p/30fps, fixed focus, standard lens technology, the field of view 60 degrees, and USB interface is used. To perform autonomous flight and real-time fire detection algorithms, companion computers will be used and connected to the digital camera using a USB interface and to the flight controller using serial (UART) interface. Two lowcost companion computers, Raspberry Pi3 and Jetson Nano [14] are used to verify real-time fire detection algorithms as shown later. Telemetry modules of different wireless links can be used such as UHF modules(415/900 MHz), mobile network modules (4G/5G) for sending an alarm to the monitoring center. image. Second steps is object classification that tries to label the bounding box using the training knowledge. For object detection, there is two different approaches: one-stage and two-stage. The two-stage approach happens as follows: (i) first, the model proposes a set of regions of interests by select search or regional proposal network. The proposed regions are sparse as the potential bounding box candidates can be infinite. (ii) Then a classifier only processes the region candidates. The methods followed this approach are Regions with Convolutional Neural Network features (RCNN) [15], Faster-RCNN [16]. This approach has high accuracy but the speed is so slow that it cannot be applied to real-time detection applications. The other approach skips the region proposal stage and runs detection directly over a dense sampling of pos sible locations. Therefore, this approach is faster and simpler, with a bit degrade of the performance. The methods followed this approach are You only Look once (Y oL o) [17], Single Shot Detector (SSD) [9]. To achieve real-time detection speed with a relatively high accuracy on a limited computational resource, we choose the SSD method in this paper since Y o L o has higher detection speed but lower accuracy [9].\n\n\nA. Single Shot Detector (SSD)\n\nFire detection is a particular use case of object detection, which refers to the methods of identifying and correctly labeling all the objects present in the image frame. Object detection consists of two steps. First one is object localization to create bounding boxes surrounding detected objects in the The SSD architecture is a single convolution network that learns to predict bounding box locations and classify these locations in one stage. SSD architecture contains three main parts [9]: (i) the feature extraction convolutional network such as VGG, ResNet, Inception, Inception Resnet-v2, MobileNets, etc. The layer of this part generates a large-scale feature map, which could be divided into more smaller cells and has smaller size of receptive fields, to detect smaller objects. (ii) The convolutional layers of the second part generate multi-scale feature maps having larger sizes of receptive fields, to detect larger objects. (iii) The last convolutional layers using a small kernel predict bounding box locations and confidences for multiple categories.\n\nIn our work, the SSD network consists of the MobileNets base model followed by several classifier convolutional layers and extra feature layers to create 8732 boxes per class and a Non-Maximum Suppression to choose the boxes with highest result as shown in Fig. 2. The input images has a size of 300 x 300 (SSD300). We used MobileNets [18] base network instead of VGG-16 or RESNET to improve the accuracy and detection speed. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to built light-weight deep neural networks. In comparison to VGG-16, MobileNets can achieve a similar accuracy while it is much smaller and less compute-intensive [18]. MobileNets usually uses for real time object detection because of faster processing time with lower requirements in computational resources.\n\n\nExtra Feature Layers\n\nFire im age 300x300 first match these default boxes to the ground truth boxes. For example, we have matched one default box with the cat for 8 x 8 scales (blue) and one box with the flowers for 4 x 4 scales (red), which are treated as positives and the rest as negatives. The model loss is a weighted sum between localization loss (e.g. Smooth L1 [19]) and confidence loss (e.g. Softmax). SSD creates a large numbers of boxes which has different ratio to compare with ground truth, so to detect an object using SSD we will choose the box fit with ground truth. As shown in Fig. 3, SSD creates many feature map to fit with object and label exactly the name of object. Therefore, to evaluate the SSD model, we use the value of loss function that consists of localization loss and a classification loss.\n\nWe can compute the loss like this [9]:\nh = N (L conf + L loc)(1)\nWhere N is the number of positive match, L loc is the localization loss and L conf is the confidence loss.\nN L ioc{x , i,o) y \\ 4 iGPos mGcx,cy,w,h x k sm o o t h -g m ) j = j -dcx )/d: d- gj cy = ( j -dcy )/dh gw log - dw dj gj gh l o g j(2)\nThe localization loss is a Smooth L1 loss [19] between the predicted box (l) and the ground truth box (g) parameters.\n\nAnd (cx,xy) is the center of the default bounding box (d) and for its width (w) and height (h).   As shown in Fig. 3a, SSD only requires an input image and ground truth boxes for each object during training. In a convolutional fashion, we evaluate a small set (e.g. 4) of default boxes of different aspect ratios at each location in several feature maps with different scales (e.g. 8 x 8 and 4 x 4 as shown in Fig. 3b and Fig. 3c). For each default box, we predict both the shape offsets and the confidences for all object categories (e.g. cat, flowers). At training time, we\n_ -T _ J_ r \u25a0FT --- ------\u00ab T T r ! [ 1 f \u25a0 i ! 1i j \u25a0 ; r -1 i L _ -h - ----- i i --j-i.\nThe confidence loss is the softmax loss over multiple classes confidence (c) that will be shown below:\nN N h conf (x,y) = -^ xd log(cf) -^ log(c\u00b0) (3) iGPos iGNeg where c p exP cf i E p exp 4\nB. Training 1) Dataset: Object detection needs a large number of images in the dataset for training. In this paper, we create the dataset for training from different sources as follows:\n\n\u2022 Part of the dataset provided by Foggia et al. [20] which contains 31 fire and non-fire videos. \u2022 Part of the dataset provided by Sharma et al. [21] contains 1124 fire images and 1301 non-fire images. \u2022 Images of fire accident from Google, Pixabay. \u2022 Part of image from 04 high quality Fire videos from Youtube which described details in Table I. Each image in the dataset is labeled with one class \"fire\" by using LabelImg tool [22]-a graphical image annotation tool. Annotations are saved as XML files in PASCAL VOC format, the format used by ImageNet [23]. In the Table II we   Videol [24] Fire in house 1280x720 2730\n\nVideo2 [25] Fire outdoor 1280x720 1100\n\nVideo3 [26] Car fire accident 1920x1080 1675\n\nVideo4 [27] Airplane fire ac cident 1920x1080 2050 The class of the object. Our class is \"fire\" < truncated> Describe the object is truncated: \"0\" indicates no trunca tion, \"1\" indicates truncation < difficult> The difficult field is set to 1 indicates that the object has been annotated as ''difficult\"\n\n<bndbox> Indicates the position of the object in the image 2) SSD Training: Based on the SSD model that defined above, we train the fire model detection by using a powerful workstation that has a work station with Geforce GTX 1080Ti GPU (11GB GDDR5X, memory speed 11 Gbps with NVIDIA Cuda 3584 Cores) running on Ubuntu 18.0.4.\n\nWe used Tensorflow API that set values of initial learning rate is 0.0001 and the total number of iterations is 50K. The training time is over 10 hours.\n\n3) Implementation o f trained model on embedded boards to verify real-time fire detection: After training on the work station, we have output model of fire detection. This model will be used for real-time object fire detection for UAVbased fire surveillance. The output model is implemented on companion computers which are low-cost, light weight and limited computational resources. The Table III shows the specifications of two embedded boards that commonly used in a lot of embedded systems. They are NVIDIA Jetson Nano vs. Raspberry Pi 3 B+.  In this section, we show our experiments results of fire detection using the SSD algorithm with MobileNet based model as shown in Section III. The algorithm's performance is evaluated on both NVIDIA Jetson Nano vs. Raspberry Pi 3 B+ to verify the detection speed of each of companion computers. .5 shows the some images on which fire is real-time detected using our the trained model based on NVIDIA Jestson Nano. The detection speed of Jetson Nano is verified at 26 FPS, which is more than sufficient for real-time video surveillance (24 FPS). That of Raspberry Pi 3 B+ is only 16 FPS since the Pi has lower computational resources as shown in Table 3. Table 4 shows the comparison of our work and previous fire detection work using SSD and VGG-16 [12]. The real-time detection speed of 26 FPS is achieved since we replaced the conventional VGG-16 base model to the MobileNet model, which is more efficient to mobile and embedded vision application like UAV-based fire surveillance. By means of the mean average precision (mAP), we achieve the mAP of 92.7% because our dataset is used only for fire detection while the dataset used in [12 ] has several more categories such as objects (fire, smoke) and disturbances (likefire, like-smoke).\n\n\nA. Results\n\n\nFig\n\nThe visual fire detection system including the companion computer (Jetson Nano) and the digital camera is integrated on the Tarot Drone as shown in the Fig. 6 a. The companion computer is connected to the flight controller PX4 (including GPS/IMU module) via serial (UART) interface and to the digital camera via USB interface to perform both autonomous  \n\n\nB. Discussions\n\nIn this section, we discuss our proposed approach for future work, specifically regarding the precise localization of the fires in global coordinates using UAVs. In the presented work, at our current stage of development, as soon as a fire is detected by our system, an alarm is issued to the monitoring center, which is to be redirected to the team that is fighting the fire on the ground. The information included in the alarm is: the GPS location of the drone, the viewing direction of the drone (both of which can be visualized in a satellite image of the area -such as provided by google-maps) and an image with the detected fire and its corresponding bounding box.\n\nHowever, it would be much more efficient for the firefighters and rescue teams that the fire could be mapped precisely. For this we propose the following improvements in our system:\n\n\u2022 First improvement: the GPS location of the drone -and the corresponding camera pose on the drone's frame, to take into account the possible usage of a Gimbal camera-mount -together with the bounding box of the fire detection can be used to project the position of the fire on a 2.5D elevation map of the area. The global position of the fire could is calculated as shown in the Fig. 6b by using 3D geometry transformations [29] and projecting a ray [31] onto the 2.5D elevation map. \u2022 Second improvement: the location of the past fire detec tions could be stored by the system. This way the drone could keep track of fires on the area that it is monitoring and send status updates with \"areas on fire\" rather than single fire detections. \u2022 Third improvement: images that correspond to fire de tections of the same spot could be used to triangulate [31] the position of the fire better. In the best case, when the images are far away that there is enough visual structure not blurred -or covered -by the fire, a visionbased map of the area could be calculated. For this, either, visual Simultaneous Localization And Mapping (vSLAM) [28] technology and/or Structure from Motion (SfM) algorithms [30]- [33] would be implemented. This approach would allow to have very accurate camera positions corresponding to the images of the fire, which can be geo-referenced [34] with the GPS global positions acquired by the drone at the time of acquisition of the images. In this case, the usage of a 2.5D elevation map of the area is probably not necessary.\n\nThe first improvement would allow to provide an estimated position of a single fire -on a satellite image of the area -per alarm issued to the monitoring center. The second and third improvements would allow to provide the monitoring center with a map of the active fires on the area (note that the fires may have advanced, but the timing of the images and fire detections can also be provided). In all cases the images with the fire detections, locations & viewpoints of the drone are provided -this set of data could be visualized on a Graphical User Interface (GUI). However with the third improvement the drone & camera positions would be estimated with more accuracy, which may be of better interest in some use casesfor instance, for fires in cluttered and residential areas.\n\nThe visual Simultaneous Localization And Mapping (vS-LAM) [28] and Structure from Motion (SfM) algorithms [30]- [33] include several fundamental steps: feature detection, feature matching, triangulation, bundle adjustment and so on. These computationally expensive algorithms have been re searched for years. In collaboration with the 2 above institutes, we are working to implement the visual SLAM and SfM algorithms on the companion computers to further improve the accuracy of fire detection for UAV-base fire surveillance.\n\nFurther future work that we envisage are: (1) to research on avoiding false positive fire detections; and to add (2) smoke detection and (3) heat detection to our vision-based processing framework. We believe that these are consecutive steps towards the improvement of fire detection systems on UAVs.\n\nV. Co n c l u s i o n\n\nWe propose a visual real-time fire detection for UAV-based fire surveillance in a large area such as forest. To achieve real-time performance of fire detection on a low-cost visual system and limited computational resources, we applied SSD technology with MobileNets base network which improves the detection speed. The proposed algorithms automatically and successfully detect fire in different real-time experiments with different embedded boards. The design and implementation of the visual fire detection system on practical drone are also successfully completed. Further work in obtaining global position of the fire and improve the detection accuracy are discussed.\n\nFig. 1 :\n1(a) Visual Fire Detection using UAV. (b) Design of the UAV for fire detection and alarm\n\nFig. 2 :\n2SSD architecture using MobileNets base model for fire detection J #\n\nFig. 3 :\n3SSD Fram ew ork a) Image from dataset with ground truth boxes, b) SSD create an 8x8 feature map to detect object cat and c) an 4x4 feature map to detect object flowers\n\n\nof 1043 fire images and annotations including the bounding box information of the fire in each of the images.\n\nFig. 4 :\n4Some labeled images from our training dataset. a) Videol. b) Video2. c) Video3. d)Video4.\n\nFig. 5 :\n5Images on which fire is real-time detected by SSD algorithm implemented on NVIDIA Jetson NanoTABLE IV: Comparison of the performance of SSD with VGG-16 and MobileNets modelA lg o rith m m A P (% ) D e te c tio n sp ee d (F P S ) and visual fire detection functions. We have been developing autonomous flight functions using both GPS and visual localization in collaboration with Department of Engineering, Aarhus University, Denmark and Institute of Computer Graphics and Vision, Technical University of Graz, Austria.\n\nFig. 6 :\n6(a) Visual fire detection system integrated on the Tarot Drone for UAV-based fire surveillance (b) Projection of fire detection bounding-box on the 2.5D elevation map of the area from the camera position -estimated from the GPS+IMU fusion provided by the drone's autopilot and camera position on the drone's body frame.\n\nTABLE I :\nIFire videos used to create the datasetV id eo \nD escrip tio n \nR eso lu tio n \nF ra m es p e r se c \no n d (F P S ) \n\n\n\nTABLE II :\nIIDetails of annotationsF ie ld \nD e scrip tio n \n\n< filename > \nImage name \n\n< path> \nPath to the location of image \n\n< source> \nThe data set name and source of the image \n\n< size> \n\nConsists of three field < w idth> , \n<height>, \n<depth>denoting the width, height and channel \nnumber of the image \n\n< name> \n\n\nTABLE III :\nIIISpecifications of NVIDIA Jetson Nano vs. Rasp berry Pi 3 B+\n\n. Cal Fire. Cal Fire, 2018, [online] Available: https://www.fire.ca.gov/incidents/2018/ (accessed on Aug 10, 2020)\n\nVideo Smoke Detection Based on Convolution Neural Network. Z Zhang, Y Hu, 2017 International Conference on Computer Technology, Electronics and Communication (ICCTEC). Z. Zhang, Y. Hu, \"Video Smoke Detection Based on Convolution Neural Network\". In 2017 International Conference on Computer Technology, Electronics and Communication (ICCTEC)\n\nFire Detection System using RGB Color Model. S S Nalawade, In IJESC. 85S.S. Nalawade, \" Fire Detection System using RGB Color Model.\" In IJESC (2018), Volume 8 Issue No.5.\n\nEfficient Deep CNN-Based Fire Detection and Localization in Video Surveillance Applica tions. K Muhammad, J Ahmad, Z Lv, P Bellavista, IEEE Transactions on Systems, Man, and Cybernetics: Systems \u25a0. K. Muhammad, J. Ahmad, z. Lv and P. Bellavista, \" Efficient Deep CNN- Based Fire Detection and Localization in Video Surveillance Applica tions.\" In IEEE Transactions on Systems, Man, and Cybernetics: Systems \u25a0 March 2018\n\nDeep Convolutional Neural Networks for Forest Fire Detection. Q Zhang, J Xu, L Xu, H Guo, IFMEITA. Q. Zhang, J. Xu, L. XU and H.Guo, \" Deep Convolutional Neural Networks for Forest Fire Detection.\" In IFMEITA 2016.\n\nReal-time fire detection method combining AdaBoost, LBP and convolutional neural network in video sequence. O Maksymiv, T Rak, D Deleshko, 2017 14th International Conference The Experience of De signing and Application of CAD Systems in Microelectronics (CADSM). O. Maksymiv, T. Rak and D. Deleshko, \"Real-time fire detection method combining AdaBoost, LBP and convolutional neural network in video sequence.\" In 2017 14th International Conference The Experience of De signing and Application of CAD Systems in Microelectronics (CADSM)\n\nFire Detection in Infrared Video Surveillance Based on Convolutional Neural Network and SVM. K Wang, J Wang, Y Zhang, Q Zhang, B Chen, D Liu, 2018 IEEE 3rd International Conference on Signal and Image Processing. ICSIPK. Wang, J. Wang, Y. Zhang, Q. Zhang, B. Chen and D. Liu, \"Fire Detection in Infrared Video Surveillance Based on Convolutional Neural Network and SVM.\" In 2018 IEEE 3rd International Conference on Signal and Image Processing (ICSIP)\n\nDeep convo lutional neural networks for fire detection in images. J Sharma, O C Granmo, M Goodwin, J T Fidje, International Conference on Engineering Applications of Neural Networks. J. Sharma, O. C. Granmo, M. Goodwin and J. T. Fidje, \"Deep convo lutional neural networks for fire detection in images\". In International Conference on Engineering Applications of Neural Networks(2017)\n\nSSD: Single Shot MultiBox Detector. W Liu, A Anguelov, D Erhan, C Szegedy, S Reed, C Y Fu, A Berg, ECCV. W.Liu, A. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Y. Fu and A. Berg, \"SSD: Single Shot MultiBox Detector.\" In ECCV(2016).\n\nA distributed control framework for a team of unmanned aerial vehicles for dynamic wildfire tracking. X Huy, Hung M Pham, David La, Matthew Feil-Seifer, Deans, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Huy X. Pham, Hung M. La, David Feil-Seifer, Matthew Deans, \" A distributed control framework for a team of unmanned aerial vehicles for dynamic wildfire tracking\" In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n\nFire detection using infrared images for UAV-based forest fire surveillance. Z Yuan, Y Liu, Zhang, 2017 International Conference on Unmanned Aircraft Systems (ICUAS). c. Yuan, Z. Liu and Y. Zhang. \"Fire detection using infrared images for UAV-based forest fire surveillance\". In 2017 International Conference on Unmanned Aircraft Systems (ICUAS)\n\nImage fire detection algorithms based on convolutional neural networks. P Li, W Zhao, ELSEVIERP. Li, W. Zhao, \"Image fire detection algorithms based on convolutional neural networks.\" In ELSEVIER (2020).\n\n. Dronecode, 4Dronecode: PX-4\n\n. Jetson Nano Nvidia. Jetson Nano Nvidia, 2020, [online] Available: https://www.nvidia.com/en-us/autonomous-machines/embedded- systems/jetson-nano (accessed on Aug 29, 2020)\n\nRich Feature Hier archies for Accurate Object Detection and Semantic Segmentation. R Girshick, J Donahue, T Darrell, J Malik, 2014 IEEE Conference on Computer Vision and Pattern Recognition. R. Girshick, J. Donahue, T. Darrell and J. Malik, \"Rich Feature Hier archies for Accurate Object Detection and Semantic Segmentation\". In 2014 IEEE Conference on Computer Vision and Pattern Recognition\n\nFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. S Ren, K He, R Girshick, J Sun, IEEE Transactions on Pattern Analysis and Machine Intelligence. 39S. Ren, K. He, R. Girshick, and J. Sun, \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\". In IEEE Transactions on Pattern Analysis and Machine Intelligence ( Volume: 39 , Issue: 6 , June 1 2017 )\n\nYou Only Look Once: Unified, Real-Time Object Detection. J Redmon, S Divvala, R Girshick, A Farhadi, 2016 IEEE Conference on Computer Vision and Pattern Recognition. CVPRJ. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You Only Look Once: Unified, Real-Time Object Detection\". In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n\nMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. A G Howard, M Zhu, B Chen, D Kalenichenko, W Wang, T Weyand, M Andreetto, Ha, Adam, arXiv:1704.0486v1A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto and Ha. Adam, \" MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.\" In arXiv:1704.0486v1 (2017)\n\nFast R-CNN. R Girshick, ICCV. R. Girshick, \"Fast R-CNN\". In ICCV(2015).\n\nReal-time fire detection for video surveillance applications using a combination of experts based on color, shape, and motion. P Foggia, A Saggese, M Vento, IEEE TRANSACTIONS on circuits and systems for video technology. P. Foggia, A. Saggese, and M. Vento, \"Real-time fire detection for video surveillance applications using a combination of experts based on color, shape, and motion\". In IEEE TRANSACTIONS on circuits and systems for video technology(2015)\n\nFireNet: A Specialized Lightweight Fire & Smoke Detection Model for Real-time IoT Applications. A Jadon, M Omama, A Varshney, M S Ansari, R Sharma, arXiv:1905.11922v2A. Jadon, M. Omama, A. Varshney, M. S. Ansari and R. Sharma, \" FireNet: A Specialized Lightweight Fire & Smoke Detection Model for Real-time IoT Applications.\" In arXiv: 1905.11922v2 (2019)\n\n. Imagenet, ImageNet, [online] Available: http://www.image-net.org/ (accessed on Jul 6, 2020)\n\n. Video1, Video1, [online] Available: https://www.youtube.com/watch?v=xr6b9b8FYKk (accessed on May 20, 2020)\n\n. Video2, Video2, [online] Available: https://www.youtube.com/watch?v=J-U8zouLntg (accessed on May 20, 2020)\n\n. Video3, Video3, [online] Available: https://www.youtube.com/watch?v=Mdr1aB1wODM (accessed on May 20, 2020)\n\n. Video4, Video4, [online] Available: https://www.youtube.com/watch?v=zubndM2hrpE (accessed on May 20, 2020)\n\nVisual simultaneous localization and mapping: a survey. J Fuentes-Pacheco, J Ruiz-Ascencio, J Rendon-Mancha, Artif Intell Rev. 43Fuentes-Pacheco, J., Ruiz-Ascencio, J. & Rendon-Mancha, J.M. ''Visual simultaneous localization and mapping: a survey\". In Artif Intell Rev 43, 55-81 (2015).\n\nTransformation Geometry -An Introduction to Symmetry. George E Martin, SpingerNew York, NY, USAGeorge E. Martin, \"Transformation Geometry -An Introduction to Symmetry\", New York, NY, USA: Spinger, 1982\n\nStructure from Motion (SfM) -Approaches and Applications. Penjani &amp; Nyimbili, Demirel, &amp; Hande, Seker, &amp; Dursun, Turan Erden, Structure from Motion (SfM) -Approaches and Applications Conference. AntalyaNyimbili, Penjani & Demirel, Hande & Seker, Dursun & Erden, Turan, \"Structure from Motion (SfM) -Approaches and Applications\". In Structure from Motion (SfM) -Approaches and Applications Conference, Antalya (2016).\n\nMultiple view geometry in computer vision. Richard Hartley, Andrew Zisserman, Cambridge university pressHartley, Richard, and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press (2003).\n\nInter national workshop on vision algorithms. Bill Triggs, SpringerBerlin, HeidelbergBundle adjustment-a modern synthesisTriggs, Bill, et al. \"Bundle adjustment-a modern synthesis.\" Inter national workshop on vision algorithms. Springer, Berlin, Heidelberg (1999).\n\nStructure-frommotion revisited. Johannes L Schonberger, Jan-Michael Frahm, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSchonberger, Johannes L., and Jan-Michael Frahm. \"Structure-from- motion revisited.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016).\n\nAutomated end-to-end workflow for precise and geo-accurate reconstructions using fiducial markers. Markus Rumpler, ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences. 3Rumpler, Markus, et al. \"Automated end-to-end workflow for precise and geo-accurate reconstructions using fiducial markers.\" ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences 3 (2014): 135-142.\n", "annotations": {"author": "[{\"start\":\"105\",\"end\":\"311\"},{\"start\":\"312\",\"end\":\"323\"},{\"start\":\"324\",\"end\":\"528\"},{\"start\":\"529\",\"end\":\"807\"},{\"start\":\"808\",\"end\":\"1035\"}]", "publisher": null, "author_last_name": "[{\"start\":\"109\",\"end\":\"115\"},{\"start\":\"316\",\"end\":\"322\"},{\"start\":\"328\",\"end\":\"332\"},{\"start\":\"535\",\"end\":\"539\"},{\"start\":\"810\",\"end\":\"817\"}]", "author_first_name": "[{\"start\":\"105\",\"end\":\"106\"},{\"start\":\"107\",\"end\":\"108\"},{\"start\":\"312\",\"end\":\"313\"},{\"start\":\"314\",\"end\":\"315\"},{\"start\":\"324\",\"end\":\"325\"},{\"start\":\"326\",\"end\":\"327\"},{\"start\":\"529\",\"end\":\"532\"},{\"start\":\"533\",\"end\":\"534\"},{\"start\":\"808\",\"end\":\"809\"}]", "author_affiliation": "[{\"start\":\"117\",\"end\":\"212\"},{\"start\":\"214\",\"end\":\"310\"},{\"start\":\"334\",\"end\":\"429\"},{\"start\":\"431\",\"end\":\"527\"},{\"start\":\"560\",\"end\":\"655\"},{\"start\":\"657\",\"end\":\"708\"},{\"start\":\"710\",\"end\":\"806\"},{\"start\":\"841\",\"end\":\"936\"},{\"start\":\"938\",\"end\":\"1034\"}]", "title": "[{\"start\":\"1\",\"end\":\"102\"},{\"start\":\"1036\",\"end\":\"1137\"}]", "venue": null, "abstract": "[{\"start\":\"1240\",\"end\":\"2154\"}]", "bib_ref": "[{\"start\":\"2769\",\"end\":\"2772\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"3102\",\"end\":\"3105\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"4567\",\"end\":\"4570\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"5157\",\"end\":\"5160\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"5162\",\"end\":\"5165\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"5267\",\"end\":\"5270\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"5607\",\"end\":\"5611\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"6024\",\"end\":\"6028\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"6663\",\"end\":\"6667\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"7840\",\"end\":\"7844\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"8324\",\"end\":\"8328\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"9162\",\"end\":\"9166\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"9180\",\"end\":\"9184\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"9585\",\"end\":\"9589\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"9618\",\"end\":\"9621\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"9828\",\"end\":\"9831\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"10356\",\"end\":\"10359\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"11271\",\"end\":\"11275\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"11620\",\"end\":\"11624\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"12138\",\"end\":\"12142\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"12627\",\"end\":\"12630\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"12943\",\"end\":\"12947\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"14113\",\"end\":\"14117\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"14210\",\"end\":\"14214\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"14620\",\"end\":\"14624\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"14654\",\"end\":\"14658\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"14695\",\"end\":\"14699\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"14735\",\"end\":\"14739\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"14781\",\"end\":\"14785\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"16857\",\"end\":\"16861\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"17244\",\"end\":\"17249\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"19022\",\"end\":\"19026\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"19048\",\"end\":\"19052\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"19447\",\"end\":\"19451\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"19730\",\"end\":\"19734\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"19792\",\"end\":\"19796\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"19798\",\"end\":\"19802\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"19959\",\"end\":\"19963\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"20987\",\"end\":\"20991\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"21035\",\"end\":\"21039\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"21041\",\"end\":\"21045\",\"attributes\":{\"ref_id\":\"b31\"}}]", "figure": "[{\"start\":\"22454\",\"end\":\"22552\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"22553\",\"end\":\"22631\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"22632\",\"end\":\"22810\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"22811\",\"end\":\"22922\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"22923\",\"end\":\"23023\",\"attributes\":{\"id\":\"fig_6\"}},{\"start\":\"23024\",\"end\":\"23553\",\"attributes\":{\"id\":\"fig_8\"}},{\"start\":\"23554\",\"end\":\"23884\",\"attributes\":{\"id\":\"fig_9\"}},{\"start\":\"23885\",\"end\":\"24015\",\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"}},{\"start\":\"24016\",\"end\":\"24338\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"24339\",\"end\":\"24414\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2173\",\"end\":\"3721\"},{\"start\":\"3723\",\"end\":\"5455\"},{\"start\":\"5457\",\"end\":\"6493\"},{\"start\":\"6495\",\"end\":\"9832\"},{\"start\":\"9866\",\"end\":\"10934\"},{\"start\":\"10936\",\"end\":\"11766\"},{\"start\":\"11791\",\"end\":\"12591\"},{\"start\":\"12593\",\"end\":\"12631\"},{\"start\":\"12658\",\"end\":\"12764\"},{\"start\":\"12901\",\"end\":\"13018\"},{\"start\":\"13020\",\"end\":\"13595\"},{\"start\":\"13686\",\"end\":\"13788\"},{\"start\":\"13878\",\"end\":\"14063\"},{\"start\":\"14065\",\"end\":\"14686\"},{\"start\":\"14688\",\"end\":\"14726\"},{\"start\":\"14728\",\"end\":\"14772\"},{\"start\":\"14774\",\"end\":\"15077\"},{\"start\":\"15079\",\"end\":\"15405\"},{\"start\":\"15407\",\"end\":\"15559\"},{\"start\":\"15561\",\"end\":\"17348\"},{\"start\":\"17369\",\"end\":\"17723\"},{\"start\":\"17742\",\"end\":\"18412\"},{\"start\":\"18414\",\"end\":\"18595\"},{\"start\":\"18597\",\"end\":\"20144\"},{\"start\":\"20146\",\"end\":\"20927\"},{\"start\":\"20929\",\"end\":\"21455\"},{\"start\":\"21457\",\"end\":\"21757\"},{\"start\":\"21759\",\"end\":\"21780\"},{\"start\":\"21782\",\"end\":\"22453\"}]", "formula": "[{\"start\":\"12632\",\"end\":\"12657\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"12765\",\"end\":\"12900\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"13596\",\"end\":\"13685\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"13789\",\"end\":\"13877\",\"attributes\":{\"id\":\"formula_3\"}}]", "table_ref": "[{\"start\":\"14404\",\"end\":\"14411\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"14633\",\"end\":\"14644\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"15949\",\"end\":\"15958\",\"attributes\":{\"ref_id\":\"tab_0\"}},{\"start\":\"16753\",\"end\":\"16760\"},{\"start\":\"16762\",\"end\":\"16769\"}]", "section_header": "[{\"start\":\"2156\",\"end\":\"2171\"},{\"start\":\"9835\",\"end\":\"9864\"},{\"start\":\"11769\",\"end\":\"11789\"},{\"start\":\"17351\",\"end\":\"17361\"},{\"start\":\"17364\",\"end\":\"17367\"},{\"start\":\"17726\",\"end\":\"17740\"},{\"start\":\"22455\",\"end\":\"22463\"},{\"start\":\"22554\",\"end\":\"22562\"},{\"start\":\"22633\",\"end\":\"22641\"},{\"start\":\"22924\",\"end\":\"22932\"},{\"start\":\"23025\",\"end\":\"23033\"},{\"start\":\"23555\",\"end\":\"23563\"},{\"start\":\"23886\",\"end\":\"23895\"},{\"start\":\"24017\",\"end\":\"24027\"},{\"start\":\"24340\",\"end\":\"24351\"}]", "table": "[{\"start\":\"23935\",\"end\":\"24015\"},{\"start\":\"24052\",\"end\":\"24338\"}]", "figure_caption": "[{\"start\":\"22465\",\"end\":\"22552\"},{\"start\":\"22564\",\"end\":\"22631\"},{\"start\":\"22643\",\"end\":\"22810\"},{\"start\":\"22813\",\"end\":\"22922\"},{\"start\":\"22934\",\"end\":\"23023\"},{\"start\":\"23035\",\"end\":\"23553\"},{\"start\":\"23565\",\"end\":\"23884\"},{\"start\":\"23897\",\"end\":\"23935\"},{\"start\":\"24030\",\"end\":\"24052\"},{\"start\":\"24355\",\"end\":\"24414\"}]", "figure_ref": "[{\"start\":\"7096\",\"end\":\"7103\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"7545\",\"end\":\"7552\",\"attributes\":{\"ref_id\":\"fig_1\"}},{\"start\":\"11193\",\"end\":\"11199\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"12364\",\"end\":\"12370\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"13130\",\"end\":\"13137\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"13430\",\"end\":\"13449\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"16403\",\"end\":\"16405\",\"attributes\":{\"ref_id\":\"fig_8\"}},{\"start\":\"17521\",\"end\":\"17527\",\"attributes\":{\"ref_id\":\"fig_9\"}},{\"start\":\"18977\",\"end\":\"18984\",\"attributes\":{\"ref_id\":\"fig_9\"}}]", "bib_author_first_name": "[{\"start\":\"24591\",\"end\":\"24592\"},{\"start\":\"24600\",\"end\":\"24601\"},{\"start\":\"24920\",\"end\":\"24921\"},{\"start\":\"24922\",\"end\":\"24923\"},{\"start\":\"25142\",\"end\":\"25143\"},{\"start\":\"25154\",\"end\":\"25155\"},{\"start\":\"25163\",\"end\":\"25164\"},{\"start\":\"25169\",\"end\":\"25170\"},{\"start\":\"25531\",\"end\":\"25532\"},{\"start\":\"25540\",\"end\":\"25541\"},{\"start\":\"25546\",\"end\":\"25547\"},{\"start\":\"25552\",\"end\":\"25553\"},{\"start\":\"25793\",\"end\":\"25794\"},{\"start\":\"25805\",\"end\":\"25806\"},{\"start\":\"25812\",\"end\":\"25813\"},{\"start\":\"26315\",\"end\":\"26316\"},{\"start\":\"26323\",\"end\":\"26324\"},{\"start\":\"26331\",\"end\":\"26332\"},{\"start\":\"26340\",\"end\":\"26341\"},{\"start\":\"26349\",\"end\":\"26350\"},{\"start\":\"26357\",\"end\":\"26358\"},{\"start\":\"26741\",\"end\":\"26742\"},{\"start\":\"26751\",\"end\":\"26752\"},{\"start\":\"26753\",\"end\":\"26754\"},{\"start\":\"26763\",\"end\":\"26764\"},{\"start\":\"26774\",\"end\":\"26775\"},{\"start\":\"26776\",\"end\":\"26777\"},{\"start\":\"27097\",\"end\":\"27098\"},{\"start\":\"27104\",\"end\":\"27105\"},{\"start\":\"27116\",\"end\":\"27117\"},{\"start\":\"27125\",\"end\":\"27126\"},{\"start\":\"27136\",\"end\":\"27137\"},{\"start\":\"27144\",\"end\":\"27145\"},{\"start\":\"27146\",\"end\":\"27147\"},{\"start\":\"27152\",\"end\":\"27153\"},{\"start\":\"27395\",\"end\":\"27396\"},{\"start\":\"27402\",\"end\":\"27406\"},{\"start\":\"27407\",\"end\":\"27408\"},{\"start\":\"27415\",\"end\":\"27420\"},{\"start\":\"27425\",\"end\":\"27432\"},{\"start\":\"27858\",\"end\":\"27859\"},{\"start\":\"27866\",\"end\":\"27867\"},{\"start\":\"28200\",\"end\":\"28201\"},{\"start\":\"28206\",\"end\":\"28207\"},{\"start\":\"28622\",\"end\":\"28623\"},{\"start\":\"28634\",\"end\":\"28635\"},{\"start\":\"28645\",\"end\":\"28646\"},{\"start\":\"28656\",\"end\":\"28657\"},{\"start\":\"29013\",\"end\":\"29014\"},{\"start\":\"29020\",\"end\":\"29021\"},{\"start\":\"29026\",\"end\":\"29027\"},{\"start\":\"29038\",\"end\":\"29039\"},{\"start\":\"29397\",\"end\":\"29398\"},{\"start\":\"29407\",\"end\":\"29408\"},{\"start\":\"29418\",\"end\":\"29419\"},{\"start\":\"29430\",\"end\":\"29431\"},{\"start\":\"29780\",\"end\":\"29781\"},{\"start\":\"29782\",\"end\":\"29783\"},{\"start\":\"29792\",\"end\":\"29793\"},{\"start\":\"29799\",\"end\":\"29800\"},{\"start\":\"29807\",\"end\":\"29808\"},{\"start\":\"29823\",\"end\":\"29824\"},{\"start\":\"29831\",\"end\":\"29832\"},{\"start\":\"29841\",\"end\":\"29842\"},{\"start\":\"30104\",\"end\":\"30105\"},{\"start\":\"30292\",\"end\":\"30293\"},{\"start\":\"30302\",\"end\":\"30303\"},{\"start\":\"30313\",\"end\":\"30314\"},{\"start\":\"30721\",\"end\":\"30722\"},{\"start\":\"30730\",\"end\":\"30731\"},{\"start\":\"30739\",\"end\":\"30740\"},{\"start\":\"30751\",\"end\":\"30752\"},{\"start\":\"30753\",\"end\":\"30754\"},{\"start\":\"30763\",\"end\":\"30764\"},{\"start\":\"31573\",\"end\":\"31574\"},{\"start\":\"31592\",\"end\":\"31593\"},{\"start\":\"31609\",\"end\":\"31610\"},{\"start\":\"31859\",\"end\":\"31865\"},{\"start\":\"31866\",\"end\":\"31867\"},{\"start\":\"32066\",\"end\":\"32073\"},{\"start\":\"32074\",\"end\":\"32079\"},{\"start\":\"32099\",\"end\":\"32104\"},{\"start\":\"32119\",\"end\":\"32124\"},{\"start\":\"32133\",\"end\":\"32138\"},{\"start\":\"32481\",\"end\":\"32488\"},{\"start\":\"32498\",\"end\":\"32504\"},{\"start\":\"32707\",\"end\":\"32711\"},{\"start\":\"32959\",\"end\":\"32967\"},{\"start\":\"32968\",\"end\":\"32969\"},{\"start\":\"32983\",\"end\":\"32994\"},{\"start\":\"33414\",\"end\":\"33420\"}]", "bib_author_last_name": "[{\"start\":\"24593\",\"end\":\"24598\"},{\"start\":\"24602\",\"end\":\"24604\"},{\"start\":\"24924\",\"end\":\"24932\"},{\"start\":\"25144\",\"end\":\"25152\"},{\"start\":\"25156\",\"end\":\"25161\"},{\"start\":\"25165\",\"end\":\"25167\"},{\"start\":\"25171\",\"end\":\"25181\"},{\"start\":\"25533\",\"end\":\"25538\"},{\"start\":\"25542\",\"end\":\"25544\"},{\"start\":\"25548\",\"end\":\"25550\"},{\"start\":\"25554\",\"end\":\"25557\"},{\"start\":\"25795\",\"end\":\"25803\"},{\"start\":\"25807\",\"end\":\"25810\"},{\"start\":\"25814\",\"end\":\"25822\"},{\"start\":\"26317\",\"end\":\"26321\"},{\"start\":\"26325\",\"end\":\"26329\"},{\"start\":\"26333\",\"end\":\"26338\"},{\"start\":\"26342\",\"end\":\"26347\"},{\"start\":\"26351\",\"end\":\"26355\"},{\"start\":\"26359\",\"end\":\"26362\"},{\"start\":\"26743\",\"end\":\"26749\"},{\"start\":\"26755\",\"end\":\"26761\"},{\"start\":\"26765\",\"end\":\"26772\"},{\"start\":\"26778\",\"end\":\"26783\"},{\"start\":\"27099\",\"end\":\"27102\"},{\"start\":\"27106\",\"end\":\"27114\"},{\"start\":\"27118\",\"end\":\"27123\"},{\"start\":\"27127\",\"end\":\"27134\"},{\"start\":\"27138\",\"end\":\"27142\"},{\"start\":\"27148\",\"end\":\"27150\"},{\"start\":\"27154\",\"end\":\"27158\"},{\"start\":\"27397\",\"end\":\"27400\"},{\"start\":\"27409\",\"end\":\"27413\"},{\"start\":\"27421\",\"end\":\"27423\"},{\"start\":\"27433\",\"end\":\"27444\"},{\"start\":\"27446\",\"end\":\"27451\"},{\"start\":\"27860\",\"end\":\"27864\"},{\"start\":\"27868\",\"end\":\"27871\"},{\"start\":\"27873\",\"end\":\"27878\"},{\"start\":\"28202\",\"end\":\"28204\"},{\"start\":\"28208\",\"end\":\"28212\"},{\"start\":\"28335\",\"end\":\"28344\"},{\"start\":\"28624\",\"end\":\"28632\"},{\"start\":\"28636\",\"end\":\"28643\"},{\"start\":\"28647\",\"end\":\"28654\"},{\"start\":\"28658\",\"end\":\"28663\"},{\"start\":\"29015\",\"end\":\"29018\"},{\"start\":\"29022\",\"end\":\"29024\"},{\"start\":\"29028\",\"end\":\"29036\"},{\"start\":\"29040\",\"end\":\"29043\"},{\"start\":\"29399\",\"end\":\"29405\"},{\"start\":\"29409\",\"end\":\"29416\"},{\"start\":\"29420\",\"end\":\"29428\"},{\"start\":\"29432\",\"end\":\"29439\"},{\"start\":\"29784\",\"end\":\"29790\"},{\"start\":\"29794\",\"end\":\"29797\"},{\"start\":\"29801\",\"end\":\"29805\"},{\"start\":\"29809\",\"end\":\"29821\"},{\"start\":\"29825\",\"end\":\"29829\"},{\"start\":\"29833\",\"end\":\"29839\"},{\"start\":\"29843\",\"end\":\"29852\"},{\"start\":\"29854\",\"end\":\"29856\"},{\"start\":\"29858\",\"end\":\"29862\"},{\"start\":\"30106\",\"end\":\"30114\"},{\"start\":\"30294\",\"end\":\"30300\"},{\"start\":\"30304\",\"end\":\"30311\"},{\"start\":\"30315\",\"end\":\"30320\"},{\"start\":\"30723\",\"end\":\"30728\"},{\"start\":\"30732\",\"end\":\"30737\"},{\"start\":\"30741\",\"end\":\"30749\"},{\"start\":\"30755\",\"end\":\"30761\"},{\"start\":\"30765\",\"end\":\"30771\"},{\"start\":\"30984\",\"end\":\"30992\"},{\"start\":\"31079\",\"end\":\"31085\"},{\"start\":\"31189\",\"end\":\"31195\"},{\"start\":\"31299\",\"end\":\"31305\"},{\"start\":\"31409\",\"end\":\"31415\"},{\"start\":\"31575\",\"end\":\"31590\"},{\"start\":\"31594\",\"end\":\"31607\"},{\"start\":\"31611\",\"end\":\"31624\"},{\"start\":\"31868\",\"end\":\"31874\"},{\"start\":\"32080\",\"end\":\"32088\"},{\"start\":\"32090\",\"end\":\"32097\"},{\"start\":\"32105\",\"end\":\"32110\"},{\"start\":\"32112\",\"end\":\"32117\"},{\"start\":\"32125\",\"end\":\"32131\"},{\"start\":\"32139\",\"end\":\"32144\"},{\"start\":\"32489\",\"end\":\"32496\"},{\"start\":\"32505\",\"end\":\"32514\"},{\"start\":\"32712\",\"end\":\"32718\"},{\"start\":\"32970\",\"end\":\"32981\"},{\"start\":\"32995\",\"end\":\"33000\"},{\"start\":\"33421\",\"end\":\"33428\"}]", "bib_entry": "[{\"start\":\"24416\",\"end\":\"24530\",\"attributes\":{\"id\":\"b0\"}},{\"start\":\"24532\",\"end\":\"24873\",\"attributes\":{\"matched_paper_id\":\"199510556\",\"id\":\"b1\"}},{\"start\":\"24875\",\"end\":\"25046\",\"attributes\":{\"id\":\"b2\"}},{\"start\":\"25048\",\"end\":\"25467\",\"attributes\":{\"matched_paper_id\":\"126222952\",\"id\":\"b3\"}},{\"start\":\"25469\",\"end\":\"25683\",\"attributes\":{\"matched_paper_id\":\"113359046\",\"id\":\"b4\"}},{\"start\":\"25685\",\"end\":\"26220\",\"attributes\":{\"matched_paper_id\":\"40561898\",\"id\":\"b5\"}},{\"start\":\"26222\",\"end\":\"26673\",\"attributes\":{\"matched_paper_id\":\"57376639\",\"id\":\"b6\"}},{\"start\":\"26675\",\"end\":\"27059\",\"attributes\":{\"matched_paper_id\":\"206703868\",\"id\":\"b7\"}},{\"start\":\"27061\",\"end\":\"27291\",\"attributes\":{\"matched_paper_id\":\"2141740\",\"id\":\"b8\"}},{\"start\":\"27293\",\"end\":\"27779\",\"attributes\":{\"matched_paper_id\":\"3734682\",\"id\":\"b9\"}},{\"start\":\"27781\",\"end\":\"28126\",\"attributes\":{\"matched_paper_id\":\"45783060\",\"id\":\"b10\"}},{\"start\":\"28128\",\"end\":\"28331\",\"attributes\":{\"id\":\"b11\"}},{\"start\":\"28333\",\"end\":\"28362\",\"attributes\":{\"id\":\"b12\"}},{\"start\":\"28364\",\"end\":\"28537\",\"attributes\":{\"id\":\"b13\"}},{\"start\":\"28539\",\"end\":\"28931\",\"attributes\":{\"matched_paper_id\":\"215827080\",\"id\":\"b14\"}},{\"start\":\"28933\",\"end\":\"29338\",\"attributes\":{\"matched_paper_id\":\"10328909\",\"id\":\"b15\"}},{\"start\":\"29340\",\"end\":\"29694\",\"attributes\":{\"matched_paper_id\":\"206594738\",\"id\":\"b16\"}},{\"start\":\"29696\",\"end\":\"30090\",\"attributes\":{\"id\":\"b17\",\"doi\":\"arXiv:1704.0486v1\"}},{\"start\":\"30092\",\"end\":\"30163\",\"attributes\":{\"matched_paper_id\":\"206770307\",\"id\":\"b18\"}},{\"start\":\"30165\",\"end\":\"30623\",\"attributes\":{\"matched_paper_id\":\"32973589\",\"id\":\"b19\"}},{\"start\":\"30625\",\"end\":\"30980\",\"attributes\":{\"id\":\"b20\",\"doi\":\"arXiv:1905.11922v2\"}},{\"start\":\"30982\",\"end\":\"31075\",\"attributes\":{\"id\":\"b21\"}},{\"start\":\"31077\",\"end\":\"31185\",\"attributes\":{\"id\":\"b22\"}},{\"start\":\"31187\",\"end\":\"31295\",\"attributes\":{\"id\":\"b23\"}},{\"start\":\"31297\",\"end\":\"31405\",\"attributes\":{\"id\":\"b24\"}},{\"start\":\"31407\",\"end\":\"31515\",\"attributes\":{\"id\":\"b25\"}},{\"start\":\"31517\",\"end\":\"31803\",\"attributes\":{\"matched_paper_id\":\"17942780\",\"id\":\"b26\"}},{\"start\":\"31805\",\"end\":\"32006\",\"attributes\":{\"id\":\"b27\"}},{\"start\":\"32008\",\"end\":\"32436\",\"attributes\":{\"id\":\"b28\"}},{\"start\":\"32438\",\"end\":\"32659\",\"attributes\":{\"id\":\"b29\"}},{\"start\":\"32661\",\"end\":\"32925\",\"attributes\":{\"id\":\"b30\"}},{\"start\":\"32927\",\"end\":\"33313\",\"attributes\":{\"id\":\"b31\"}},{\"start\":\"33315\",\"end\":\"33735\",\"attributes\":{\"matched_paper_id\":\"9438853\",\"id\":\"b32\"}}]", "bib_title": "[{\"start\":\"24532\",\"end\":\"24589\"},{\"start\":\"24875\",\"end\":\"24918\"},{\"start\":\"25048\",\"end\":\"25140\"},{\"start\":\"25469\",\"end\":\"25529\"},{\"start\":\"25685\",\"end\":\"25791\"},{\"start\":\"26222\",\"end\":\"26313\"},{\"start\":\"26675\",\"end\":\"26739\"},{\"start\":\"27061\",\"end\":\"27095\"},{\"start\":\"27293\",\"end\":\"27393\"},{\"start\":\"27781\",\"end\":\"27856\"},{\"start\":\"28539\",\"end\":\"28620\"},{\"start\":\"28933\",\"end\":\"29011\"},{\"start\":\"29340\",\"end\":\"29395\"},{\"start\":\"30092\",\"end\":\"30102\"},{\"start\":\"30165\",\"end\":\"30290\"},{\"start\":\"31517\",\"end\":\"31571\"},{\"start\":\"32008\",\"end\":\"32064\"},{\"start\":\"32927\",\"end\":\"32957\"},{\"start\":\"33315\",\"end\":\"33412\"}]", "bib_author": "[{\"start\":\"24591\",\"end\":\"24600\"},{\"start\":\"24600\",\"end\":\"24606\"},{\"start\":\"24920\",\"end\":\"24934\"},{\"start\":\"25142\",\"end\":\"25154\"},{\"start\":\"25154\",\"end\":\"25163\"},{\"start\":\"25163\",\"end\":\"25169\"},{\"start\":\"25169\",\"end\":\"25183\"},{\"start\":\"25531\",\"end\":\"25540\"},{\"start\":\"25540\",\"end\":\"25546\"},{\"start\":\"25546\",\"end\":\"25552\"},{\"start\":\"25552\",\"end\":\"25559\"},{\"start\":\"25793\",\"end\":\"25805\"},{\"start\":\"25805\",\"end\":\"25812\"},{\"start\":\"25812\",\"end\":\"25824\"},{\"start\":\"26315\",\"end\":\"26323\"},{\"start\":\"26323\",\"end\":\"26331\"},{\"start\":\"26331\",\"end\":\"26340\"},{\"start\":\"26340\",\"end\":\"26349\"},{\"start\":\"26349\",\"end\":\"26357\"},{\"start\":\"26357\",\"end\":\"26364\"},{\"start\":\"26741\",\"end\":\"26751\"},{\"start\":\"26751\",\"end\":\"26763\"},{\"start\":\"26763\",\"end\":\"26774\"},{\"start\":\"26774\",\"end\":\"26785\"},{\"start\":\"27097\",\"end\":\"27104\"},{\"start\":\"27104\",\"end\":\"27116\"},{\"start\":\"27116\",\"end\":\"27125\"},{\"start\":\"27125\",\"end\":\"27136\"},{\"start\":\"27136\",\"end\":\"27144\"},{\"start\":\"27144\",\"end\":\"27152\"},{\"start\":\"27152\",\"end\":\"27160\"},{\"start\":\"27395\",\"end\":\"27402\"},{\"start\":\"27402\",\"end\":\"27415\"},{\"start\":\"27415\",\"end\":\"27425\"},{\"start\":\"27425\",\"end\":\"27446\"},{\"start\":\"27446\",\"end\":\"27453\"},{\"start\":\"27858\",\"end\":\"27866\"},{\"start\":\"27866\",\"end\":\"27873\"},{\"start\":\"27873\",\"end\":\"27880\"},{\"start\":\"28200\",\"end\":\"28206\"},{\"start\":\"28206\",\"end\":\"28214\"},{\"start\":\"28335\",\"end\":\"28346\"},{\"start\":\"28622\",\"end\":\"28634\"},{\"start\":\"28634\",\"end\":\"28645\"},{\"start\":\"28645\",\"end\":\"28656\"},{\"start\":\"28656\",\"end\":\"28665\"},{\"start\":\"29013\",\"end\":\"29020\"},{\"start\":\"29020\",\"end\":\"29026\"},{\"start\":\"29026\",\"end\":\"29038\"},{\"start\":\"29038\",\"end\":\"29045\"},{\"start\":\"29397\",\"end\":\"29407\"},{\"start\":\"29407\",\"end\":\"29418\"},{\"start\":\"29418\",\"end\":\"29430\"},{\"start\":\"29430\",\"end\":\"29441\"},{\"start\":\"29780\",\"end\":\"29792\"},{\"start\":\"29792\",\"end\":\"29799\"},{\"start\":\"29799\",\"end\":\"29807\"},{\"start\":\"29807\",\"end\":\"29823\"},{\"start\":\"29823\",\"end\":\"29831\"},{\"start\":\"29831\",\"end\":\"29841\"},{\"start\":\"29841\",\"end\":\"29854\"},{\"start\":\"29854\",\"end\":\"29858\"},{\"start\":\"29858\",\"end\":\"29864\"},{\"start\":\"30104\",\"end\":\"30116\"},{\"start\":\"30292\",\"end\":\"30302\"},{\"start\":\"30302\",\"end\":\"30313\"},{\"start\":\"30313\",\"end\":\"30322\"},{\"start\":\"30721\",\"end\":\"30730\"},{\"start\":\"30730\",\"end\":\"30739\"},{\"start\":\"30739\",\"end\":\"30751\"},{\"start\":\"30751\",\"end\":\"30763\"},{\"start\":\"30763\",\"end\":\"30773\"},{\"start\":\"30984\",\"end\":\"30994\"},{\"start\":\"31079\",\"end\":\"31087\"},{\"start\":\"31189\",\"end\":\"31197\"},{\"start\":\"31299\",\"end\":\"31307\"},{\"start\":\"31409\",\"end\":\"31417\"},{\"start\":\"31573\",\"end\":\"31592\"},{\"start\":\"31592\",\"end\":\"31609\"},{\"start\":\"31609\",\"end\":\"31626\"},{\"start\":\"31859\",\"end\":\"31876\"},{\"start\":\"32066\",\"end\":\"32090\"},{\"start\":\"32090\",\"end\":\"32099\"},{\"start\":\"32099\",\"end\":\"32112\"},{\"start\":\"32112\",\"end\":\"32119\"},{\"start\":\"32119\",\"end\":\"32133\"},{\"start\":\"32133\",\"end\":\"32146\"},{\"start\":\"32481\",\"end\":\"32498\"},{\"start\":\"32498\",\"end\":\"32516\"},{\"start\":\"32707\",\"end\":\"32720\"},{\"start\":\"32959\",\"end\":\"32983\"},{\"start\":\"32983\",\"end\":\"33002\"},{\"start\":\"33414\",\"end\":\"33430\"}]", "bib_venue": "[{\"start\":\"24418\",\"end\":\"24426\"},{\"start\":\"24606\",\"end\":\"24698\"},{\"start\":\"24934\",\"end\":\"24942\"},{\"start\":\"25183\",\"end\":\"25244\"},{\"start\":\"25559\",\"end\":\"25566\"},{\"start\":\"25824\",\"end\":\"25946\"},{\"start\":\"26364\",\"end\":\"26433\"},{\"start\":\"26785\",\"end\":\"26856\"},{\"start\":\"27160\",\"end\":\"27164\"},{\"start\":\"27453\",\"end\":\"27532\"},{\"start\":\"27880\",\"end\":\"27946\"},{\"start\":\"28128\",\"end\":\"28198\"},{\"start\":\"28366\",\"end\":\"28384\"},{\"start\":\"28665\",\"end\":\"28728\"},{\"start\":\"29045\",\"end\":\"29107\"},{\"start\":\"29441\",\"end\":\"29504\"},{\"start\":\"29696\",\"end\":\"29778\"},{\"start\":\"30116\",\"end\":\"30120\"},{\"start\":\"30322\",\"end\":\"30384\"},{\"start\":\"30625\",\"end\":\"30719\"},{\"start\":\"31626\",\"end\":\"31642\"},{\"start\":\"31805\",\"end\":\"31857\"},{\"start\":\"32146\",\"end\":\"32213\"},{\"start\":\"32438\",\"end\":\"32479\"},{\"start\":\"32661\",\"end\":\"32705\"},{\"start\":\"33002\",\"end\":\"33079\"},{\"start\":\"33430\",\"end\":\"33509\"},{\"start\":\"32215\",\"end\":\"32222\"},{\"start\":\"33081\",\"end\":\"33143\"}]"}}}, "year": 2023, "month": 12, "day": 17}