{"id": 502946, "updated": "2023-09-27 18:28:34.179", "metadata": {"title": "The Cityscapes Dataset for Semantic Urban Scene Understanding", "authors": "[{\"first\":\"Marius\",\"last\":\"Cordts\",\"middle\":[]},{\"first\":\"Mohamed\",\"last\":\"Omran\",\"middle\":[]},{\"first\":\"Sebastian\",\"last\":\"Ramos\",\"middle\":[]},{\"first\":\"Timo\",\"last\":\"Rehfeld\",\"middle\":[]},{\"first\":\"Markus\",\"last\":\"Enzweiler\",\"middle\":[]},{\"first\":\"Rodrigo\",\"last\":\"Benenson\",\"middle\":[]},{\"first\":\"Uwe\",\"last\":\"Franke\",\"middle\":[]},{\"first\":\"Stefan\",\"last\":\"Roth\",\"middle\":[]},{\"first\":\"Bernt\",\"last\":\"Schiele\",\"middle\":[]}]", "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2016, "month": 4, "day": 6}, "abstract": "Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1604.01685", "mag": "2953139137", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/CordtsORREBFRS16", "doi": "10.1109/cvpr.2016.350"}}, "content": {"source": {"pdf_hash": "fcc9d0ca4e0337e89f23da5a0aaeef6b8ce0790c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1604.01685v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1604.01685", "status": "GREEN"}}, "grobid": {"id": "2d14cec66b076b3662470e3312174a2d61dc4faa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fcc9d0ca4e0337e89f23da5a0aaeef6b8ce0790c.txt", "contents": "\nThe Cityscapes Dataset for Semantic Urban Scene Understanding\n\n\nMarius Cordts \nDaimler AG R&D\n3 MPI Informatics2 TU, 4 TUDarmstadt, Dresden\n\nMohamed Omran \nDaimler AG R&D\n3 MPI Informatics2 TU, 4 TUDarmstadt, Dresden\n\nSebastian Ramos \nDaimler AG R&D\n3 MPI Informatics2 TU, 4 TUDarmstadt, Dresden\n\nTimo Rehfeld \nDaimler AG R&D\n3 MPI Informatics2 TU, 4 TUDarmstadt, Dresden\n\nMarkus Enzweiler \nDaimler AG R&D\n3 MPI Informatics2 TU, 4 TUDarmstadt, Dresden\n\nRodrigo Benenson \nDaimler AG R&D\n3 MPI Informatics2 TU, 4 TUDarmstadt, Dresden\n\nUwe Franke \nDaimler AG R&D\n3 MPI Informatics2 TU, 4 TUDarmstadt, Dresden\n\nStefan Roth \nDaimler AG R&D\n3 MPI Informatics2 TU, 4 TUDarmstadt, Dresden\n\nBernt Schiele \nDaimler AG R&D\n3 MPI Informatics2 TU, 4 TUDarmstadt, Dresden\n\nThe Cityscapes Dataset for Semantic Urban Scene Understanding\nwww.cityscapes-dataset.net train/val -fine annotation -3475 images train -coarse annotation -20 000 images test -fine annotation -1525 images\nVisual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\n\nIntroduction\n\nVisual scene understanding has moved from an elusive goal to a focus of much recent research in computer vision [27]. Semantic reasoning about the contents of a scene is thereby done on several levels of abstraction. Scene recognition aims to determine the overall scene category by putting emphasis on understanding its global properties, e.g. [46,82]. Scene labeling methods, on the other hand, seek to identify the individual constituent parts of a whole scene as well as their interrelations on a more local pixel-and instance-level, e.g. [41,71]. Specialized object-centric methods fall somewhere in between by focusing on detecting a certain subset of (mostly dynamic) scene constituents, e.g. [6,12,13,15]. Despite significant advances, visual scene understanding remains challenging, particularly when taking human performance as a reference.\n\nThe resurrection of deep learning [34] has had a major impact on the current state-of-the-art in machine learning and computer vision. Many top-performing methods in a variety of applications are nowadays built around deep neural networks [30,41,66]. A major contributing factor to their success is the availability of large-scale, publicly available datasets such as ImageNet [59], PASCAL VOC [14], PASCAL-Context [45], and Microsoft COCO [38] that allow deep neural networks to develop their full potential.\n\nDespite the existing gap to human performance, scene understanding approaches have started to become essential components of advanced real-world systems. A particularly popular and challenging application involves selfdriving cars, which make extreme demands on system performance and reliability. Consequently, significant research efforts have gone into new vision technologies for understanding complex traffic scenes and driving scenarios [4, 16-18, 58, 62]. Also in this area, research progress can be heavily linked to the existence of datasets such as the KITTI Vision Benchmark Suite [19], CamVid [7], Leuven [35], and Daimler Urban Segmentation [61] datasets. These urban scene datasets are often much smaller than datasets addressing more general settings. Moreover, we argue that they do not fully capture the variability and complexity of real-world inner-city traffic scenes. Both shortcomings currently inhibit further progress in visual understanding of street scenes. To this end, we propose the Cityscapes benchmark suite and a corresponding dataset, specifically  tailored for autonomous driving in an urban environment and involving a much wider range of highly complex innercity street scenes that were recorded in 50 different cities. Cityscapes significantly exceeds previous efforts in terms of size, annotation richness, and, more importantly, regarding scene complexity and variability. We go beyond pixel-level semantic labeling by also considering instance-level semantic labeling in both our annotations and evaluation metrics.\n\nTo facilitate research on 3D scene understanding, we also provide depth information through stereo vision. Very recently, [75] announced a new semantic scene labeling dataset for suburban traffic scenes. It provides temporally consistent 3D semantic instance annotations with 2D annotations obtained through back-projection. We consider our efforts to be complementary given the differences in the way that semantic annotations are obtained, and in the type of scenes considered, i.e. suburban vs. inner-city traffic. To maximize synergies between both datasets, a common label definition that allows for cross-dataset evaluation has been mutually agreed upon and implemented.\n\n\nDataset\n\nDesigning a large-scale dataset requires a multitude of decisions, e.g. on the modalities of data recording, data preparation, and the annotation protocol. Our choices were guided by the ultimate goal of enabling significant progress in the field of semantic urban scene understanding.\n\n\nData specifications\n\nOur data recording and annotation methodology was carefully designed to capture the high variability of outdoor street scenes. Several hundreds of thousands of frames were acquired from a moving vehicle during the span of several months, covering spring, summer, and fall in 50 cities, primarily in Germany but also in neighboring countries. We deliberately did not record in adverse weather conditions, such as heavy rain or snow, as we believe such conditions to require specialized techniques and datasets [51].\n\nOur camera system and post-processing reflect the current state-of-the-art in the automotive domain. Images were recorded with an automotive-grade 22 cm baseline stereo camera using 1 /3 in CMOS 2 MP sensors (OnSemi AR0331) with rolling shutters at a frame-rate of 17 Hz. The sensors were mounted behind the windshield and yield high dynamic-range (HDR) images with 16 bits linear color depth. Each 16 bit stereo image pair was subsequently debayered and rectified. We relied on [31] for extrinsic and intrinsic calibration. To ensure calibration accuracy we recalibrated on-site before each recording session.\n\nFor comparability and compatibility with existing datasets we also provide low dynamic-range (LDR) 8 bit RGB images that are obtained by applying a logarithmic compression curve. Such tone mappings are common in automotive vision, since they can be computed efficiently and independently for each pixel. To facilitate highest annotation quality, we applied a separate tone mapping to each image. The resulting images are less realistic, but visually more pleasing and proved easier to annotate. 5000 images were manually selected from 27 cities for dense pixel-level annotation, aiming for high diversity of foreground objects, background, and overall scene layout. The annotations (see Sec. 2.2) were done on the 20 th frame of a 30-frame video snippet, which we provide in full to supply context information. For the remaining 23 cities, a single image every 20 s or 20 m driving distance (whatever comes first) was selected for coarse annotation, yielding 20 000 images in total.\n\nIn addition to the rectified 16 bit HDR and 8 bit LDR stereo image pairs and corresponding annotations, our dataset includes vehicle odometry obtained from in-vehicle sensors, outside temperature, and GPS tracks.\n\n\nClasses and annotations\n\nWe provide coarse and fine annotations at pixel level including instance-level labels for humans and vehicles.\n\nOur 5000 fine pixel-level annotations consist of layered polygons (\u00e0 la LabelMe [60]) and were realized in-house to guarantee highest quality levels. Annotation and quality control required more than 1.5 h on average for a single image. Annotators were asked to label the image from back to front such that no object boundary was marked more than once. Each annotation thus implicitly provides a depth ordering of the objects in the scene. Given our label scheme, . Proportion of annotated pixels (y-axis) per category (x-axis) for Cityscapes, CamVid [7], DUS [61], and KITTI [19].\n\nannotations can be easily extended to cover additional or more fine-grained classes. For our 20 000 coarse pixel-level annotations, accuracy on object boundaries was traded off for annotation speed. We aimed to correctly annotate as many pixels as possible within a given span of less than 7 min of annotation time per image. This was achieved by labeling coarse polygons under the sole constraint that each polygon must only include pixels belonging to a single object class.\n\nIn two experiments we assessed the quality of our labeling. First, 30 images were finely annotated twice by different annotators and passed the same quality control. It turned out that 96 % of all pixels were assigned to the same label. Since our annotators were instructed to choose a void label if unclear (such that the region is ignored in training and evaluation), we exclude pixels having at least one void label and recount, yielding 98 % agreement. Second, all our fine annotations were additionally coarsely annotated such that we can enable research on densifying coarse labels. We found that 97 % of all labeled pixels in the coarse annotations were assigned the same class as in the fine annotations.\n\nWe defined 30 visual classes for annotation, which are grouped into eight categories: flat, construction, nature, vehicle, sky, object, human, and void. Classes were selected based on their frequency, relevance from an application standpoint, practical considerations regarding the annotation effort, as well as to facilitate compatibility with existing datasets, e.g. [7,19,75]. Classes that are too rare are excluded from our benchmark, leaving 19 classes for evaluation, see Fig. 1 for details. We plan to release our annotation tool upon publication of the dataset.\n\n\nDataset splits\n\nWe split our densely annotated images into separate training, validation, and test sets. The coarsely annotated images serve as additional training data only. We chose not to split the data randomly, but rather in a way that ensures each split to be representative of the variability of different street scene scenarios. The underlying split criteria involve a balanced distribution of geographic location and population size of the individual cities, as well as regarding the time of year when recordings took place. Specifically, each of the three split sets is comprised of data recorded with the #pixels [ following properties in equal shares: (i) in large, medium, and small cities; (ii) in the geographic west, center, and east; (iii) in the geographic north, center, and south; (iv) at the beginning, middle, and end of the year. Note that the data is split at the city level, i.e. a city is completely within a single split. Following this scheme, we arrive at a unique split consisting of 2975 training and 500 validation images with publicly available annotations, as well as 1525 test images with annotations withheld for benchmarking purposes. In order to assess how uniform (representative) the splits are regarding the four split characteristics, we trained a fully convolutional network [41] on the 500 images in our validation set. This model was then evaluated on the whole test set, as well as eight subsets thereof that reflect the extreme values of the four characteristics. With the exception of the time of year, the performance is very homogeneous, varying less than 1.5 % points (often much less). Interestingly, the performance on the end of the year subset is 3.8 % points better than on the whole test set. We hypothesize that this is due to softer lighting conditions in the frequently cloudy fall. To verify this hypothesis, we additionally tested on images taken in low-or high-temperature conditions, finding a 4.5 % point increase in low temperatures (cloudy) and a 0.9 % point decrease in warm (sunny) weather. Moreover, specifically training for either condition leads to an improvement on the respective test set, but not on the balanced set. These findings support our hypothesis and underline the importance of a dataset covering a wide range of conditions encountered in the real world in a balanced way.\n\n\nStatistical analysis\n\nWe compare Cityscapes to other datasets in terms of (i) annotation volume and density, (ii) the distribution of visual classes, and (iii) scene complexity. Regarding the first two aspects, we compare Cityscapes to other datasets with semantic pixel-wise annotations, i.e. CamVid [7], DUS [62], and KITTI [19]. Note that there are many other datasets with dense semantic annotations, e.g. [2,56,65,69,70]. However, we restrict this part of the analysis to those with a focus on autonomous driving. CamVid consists of ten minutes of video footage with pixel-wise annotations for over 700 frames. DUS consists of a video sequence of 5000 images from which 500 have been annotated. KITTI addresses several different tasks including semantic labeling and object detection. As no official pixel-wise annotations exist for KITTI, several independent groups have annotated approximately 700 frames [22,29,32,33,58,64,77,80]. We map those labels to our high-level categories and analyze this consolidated set. In comparison, Cityscapes provides significantly more annotated images, i.e. 5000 fine and 20 000 coarse annotations. Moreover, the annotation quality and richness is notably better. As Cityscapes provides recordings from 50 different cities, it also covers a significantly larger area than previous datasets that contain images from a single city only, e.g. Cambridge (CamVid), Heidelberg (DUS), and Karlsruhe (KITTI). In terms of absolute and relative numbers of semantically annotated pixels (training, validation, and test data), Cityscapes compares favorably to CamVid, DUS, and KITTI with up to two orders of magnitude more annotated pixels, c.f . Tab. 1. The majority of all annotated pixels in Cityscapes belong to the coarse annotations, providing many individual (but correlated) training samples, but missing information close to object boundaries. Figures 1 and 2 compare the distribution of annotations across individual classes and their associated higher-level categories. Notable differences stem from the inherently different configurations of the datasets. Cityscapes involves dense inner-city traffic with wide roads and large intersections, whereas KITTI is composed of less busy suburban traffic scenes. As a result, KITTI exhibits significantly fewer flat ground structures, fewer humans, and more nature. In terms of overall composition, DUS and CamVid seem more aligned with Cityscapes. Exceptions are an abundance of sky pixels in CamVid due to cameras with a comparably large vertical field-of-view and the absence of certain categories in DUS, i.e. nature and object. Finally, we assess scene complexity, where density and scale of traffic participants (humans and vehicles) serve as proxy measures. Out of the previously discussed datasets, only Cityscapes and KITTI provide instance-level annotations for humans and vehicles. We additionally compare to the Caltech Pedestrian Dataset [12], which only contains annotations for humans, but none for vehicles. Furthermore, KITTI and Caltech only provide instance-level annotations in terms of axis-aligned bounding boxes. We use the respective training and validation splits for our analysis, since test set annotations are not publicly available for all datasets. In absolute terms, Cityscapes contains significantly more object instance annotations than KITTI, see Tab. 2. Being a specialized benchmark, the Caltech dataset provides the most annotations for humans by a margin. The major share of those labels was obtained, however, by interpolation between a sparse set of manual annotations resulting in significantly degraded label quality. The relative statistics emphasize the much higher complexity of Cityscapes, as the average numbers of object instances per image notably exceed those of KITTI and Caltech. We extend our analysis to MS COCO [38] and PASCAL VOC [14] that also contain street scenes while not being specific for them. We analyze the frequency of scenes with a certain number of traffic participant instances, see Fig. 3. We find our dataset to cover a greater variety of scene complexity and to have a higher portion of highly complex scenes than previous datasets. Using stereo data, we analyze the distribution of vehicle distances to the camera. From Fig. 4 we observe, that in comparison to KITTI, Cityscapes covers a larger distance range. We attribute this to both our higher-resolution imagery and the careful annotation procedure. As a consequence, algorithms need to take a larger range of scales and object sizes into account to score well in our benchmark.\n\n\nSemantic Labeling\n\nThe first Cityscapes task involves predicting a per-pixel semantic labeling of the image without considering higherlevel object instance or boundary information.\n\n\nTasks and metrics\n\nTo assess labeling performance, we rely on a standard and a novel metric. The first is the standard Jaccard Index, commonly known as the PASCAL VOC intersection-overunion metric IoU = TP TP+FP+FN [14], where TP, FP, and FN are the numbers of true positive, false positive, and false negative pixels, respectively, determined over the whole test set. Owing to the two semantic granularities, i.e. classes and categories, we report two separate mean performance scores: IoU category and IoU class . In either case, pixels labeled as void do not contribute to the score.\n\nThe global IoU measure is biased toward object instances that cover a large image area. In street scenes with their strong scale variation this can be problematic. Specifically for traffic participants, which are the key classes in our scenario, we aim to evaluate how well the individual instances in the scene are represented in the labeling. To address this, we additionally evaluate the semantic labeling using an instance-level intersection-over-union metric iIoU = iTP iTP+FP+iFN . Here, iTP, and iFN denote weighted counts of true positive and false negative pixels, respectively. In contrast to the standard IoU measure, the contribution of each pixel is weighted by the ratio of the class' average instance size to the size of the respective ground truth instance. As before, FP is the number of false positive pixels. It is important to note here that unlike the instancelevel task in Sec. 4, we assume that the methods only yield a standard per-pixel semantic class labeling as output. Therefore, the false positive pixels are not associated with any instance and thus do not require normalization. The final scores, iIoU category and iIoU class , are obtained as the means for the two semantic granularities, while only classes with instance annotations are included.\n\n\nControl experiments\n\nWe conduct several control experiments to put our baseline results below into perspective. First, we count the relative frequency of every class label at each pixel location of the fine (coarse) training annotations. Using the most frequent label at each pixel as a constant prediction irrespective of the test image (called static fine, SF, and static coarse, SC) results in roughly 10 % IoU class , as shown in Tab. 3. These low scores emphasize the high diversity of our data. SC and SF having similar performance indicates the value of our additional coarse annotations. Even if the ground truth (GT) segments are re-classified using the most frequent training label (SF or SC) within each segment mask, the performance does not notably increase.\n\nSecondly, we re-classify each ground truth segment using FCN-8s [41], c.f . Sec. 3.4. We compute the average scores within each segment and assign the maximizing label. The performance is significantly better than the static predictors but still far from 100 %. We conclude that it is necessary to optimize both classification and segmentation quality at the same time.\n\nThirdly, we evaluate the performance of subsampled ground truth annotations as predictors. Subsampling was done by majority voting of neighboring pixels, followed by resampling back to full resolution. This yields an upper bound on the performance at a fixed output resolution and is particularly relevant for deep learning approaches that often apply downscaling due to constraints on time, memory, or the network architecture itself. Downsampling factors 2 and 4 correspond to the most common setting of our 3 rd -party baselines (Sec. 3.4). Note that while subsampling by a factor of 2 hardly affects the IoU score, it clearly decreases the iIoU score given its comparatively large impact on small, but nevertheless important objects. This underlines the importance of the separate instance-normalized evaluation. The downsampling factors of 8, 16, and 32 are motivated by the corresponding strides of the FCN model. The performance of a GT downsampling by a factor of 64 is comparable to the current state of the art, while downsampling by a factor of 128 is the smallest (power of 2) downsampling for which all images have a distinct labeling.\n\nLastly, we employ 128-times subsampled annotations and retrieve the nearest training annotation in terms of the Hamming distance. The full resolution version of this training annotation is then used as prediction, resulting in 21 % IoU class . While outperforming the static predictions, the poor result demonstrates the high variability of our dataset and its demand for approaches that generalize well.\n\n\nState of the art\n\nDrawing on the success of deep learning algorithms, a number of semantic labeling approaches have shown very promising results and significantly advanced the state of the art. These new approaches take enormous advantage from recently introduced large-scale datasets, e.g. PASCAL-Context [45] and Microsoft COCO [38]. Cityscapes aims to complement these, particularly in the context of understanding complex urban scenarios, in order to enable further research in this area.\n\nThe popular work of Long et al. [41] showed how a topperforming Convolutional Neural Network (CNN) for image classification can be successfully adapted for the task of semantic labeling. Following this line, [9,37,40,63,81] propose different approaches that combine the strengths of CNNs and Conditional Random Fields (CRFs).\n\nOther work takes advantage of deep learning for explicitly integrating global scene context in the prediction of pixel-wise semantic labels, in particular through CNNs [4,39,44,67] or Recurrent Neural Networks (RNNs) [8,52]. Furthermore, a novel CNN architecture explicitly designed for dense prediction has been proposed recently by [79].\n\nLast but not least, several studies [5,11,[48][49][50]53,74,76] lately have explored different forms of weak supervision, such as bounding boxes or image-level labels, for training CNNs for pixel-level semantic labeling. We hope our coarse annotations can further advance this area.\n\n\nBaselines\n\nOur own baseline experiments (Tab. 4, top) rely on fully convolutional networks (FCNs), as they are central to most state-of-the-art methods [9,37,41,63,81]. We adopted VGG16 [68] and utilize the PASCAL-context setup [41] with a modified learning rate to match our image resolution under an unnormalized loss. According to the notation in [41], we denote the different models as FCN-32s, FCN-16s, and FCN-8s, where the numbers are the stride of the finest heatmap. Since VGG16 training on 2 MP images exceeds even the largest GPU memory available, we split each image into two halves with sufficiently large overlap. Additionally, we trained a model on images downscaled by a factor of 2. We first train on our training set (train) until the performance on our validation set (val) saturates, and then retrain on train+val with the same number of epochs.\n\nTo obtain further baseline results, we asked selected groups that have proposed state-of-the-art semantic labeling approaches to optimize their methods on our dataset and evaluated their predictions on our test set. The resulting scores are given in Tab. 4 (bottom) and qualitative examples of three selected methods are shown in Fig. 5. Interestingly enough, the performance ranking in terms of the main IoU class score on Cityscapes is highly different from PAS-CAL VOC [14]. While DPN [40] is the 2 nd best method on PASCAL, it is only the 6 th best on Cityscapes. FCN-8s [41] is last on PASCAL, but 3 rd best on Cityscapes. Adelaide [37] performs consistently well on both datasets with rank 1 on PASCAL and 2 on Cityscapes.\n\nFrom studying these results, we draw several conclusions: (1) The amount of downscaling applied during training and testing has a strong and consistent negative influence on performance (c.f . FCN-8s vs. FCN-8s at half resolution, as well as the 2 nd half of the table). The ranking according to IoU class is strictly consistent with the degree of downscaling. We attribute this to the large scale variation present in our dataset, c.f . Fig. 4. This observation clearly indicates the demand for additional research in the direction of memory and computationally efficient CNNs when facing such a large-scale dataset with high-resolution images. (2) Our novel iIoU metric treats instances of any size equally and is therefore more sensitive to errors in predicting small objects compared to the IoU. Methods that leverage a CRF for regularization [9,40,48,81] tend to over smooth small objects, c.f . Fig. 5, hence show a larger drop from IoU to iIoU than [4] or FCN-8s [41]. [37] is the only exception; its specific FCN-derived pairwise terms apparently allow for a more selective regularization.\n\n(3) When considering IoU category , Dilated10 [79] and FCN-8s [41] perform particularly well, indicating that these approaches produce comparatively many confusions between the classes within the same category, c.f . the buses in Fig. 5 [48] outperforming [9], which it extends by exploiting both dense and weak annotations (e.g. bounding boxes). Our dataset will hopefully stimulate research on exploiting the coarse labels further, especially given the interest in this area, e.g. [25,43,47].\n\nOverall, we believe that the unique characteristics of our dataset (e.g. scale variation, amount of small objects, focus on urban street scenes) allow for more such novel insights.\n\n\nCross-dataset evaluation\n\nIn order to show the compatibility and complementarity of Cityscapes regarding related datasets, we applied an FCN model trained on our data to Camvid [7] and two subsets of KITTI [58,64]. We use the half-resolution model (c.f . 4 th row in Tab. 4) to better match the target datasets, but we do not apply any specific training or fine-tuning. In all cases, we follow the evaluation of the respective dataset to be able to compare to previously reported results [4,73]. The obtained results in Tab. 5 show that our large-scale dataset enables us to train models that are on a par with or even outperforming methods that are specifically trained on another benchmark and specialized for its test data. Further, our analysis shows that our new dataset integrates well with existing ones and allows for cross-dataset research.\n\n\nInstance-Level Semantic Labeling\n\nThe pixel-level task, c.f . Sec. 3, does not aim to segment individual object instances. In contrast, in the instancelevel semantic labeling task, we focus on simultaneously detecting objects and segmenting them. This is an extension to both traditional object detection, since per-instance segments must be provided, and semantic labeling, since each instance is treated as a separate label.\n\n\nTasks and metrics\n\nFor instance-level semantic labeling, algorithms are required to deliver a set of detections of traffic participants in the scene, each associated with a confidence score and a per-instance segmentation mask. To assess instance-level performance, we compute the average precision on the region level (AP [23]) for each class and average it across a range of overlap thresholds to avoid a bias towards a specific value. Specifically, we follow [38] and use 10 different overlaps ranging from 0.5 to 0.95 in steps of 0.05. The overlap is computed at the region level, making it equivalent to the IoU of a single instance. We penalize multiple predictions of the same ground truth instance as false positives. To obtain a single, easy to compare compound score, we report the mean average precision AP, obtained by also averaging over the class label set. As minor scores, we add AP 50% for an overlap value of 50 %, as well as AP 100m and AP 50m where the evaluation is restricted to objects within 100 m and 50 m distance, respectively.\n\n\nState of the art\n\nAs detection results have matured (70 % mean AP on PASCAL [14,55]), the last years have seen a rising interest in more difficult settings. Detections with pixel-level segments rather than traditional bounding boxes provide a richer output and allow (in principle) for better occlusion handling. We group existing methods into three categories.\n\nThe first encompasses segmentation, then detection and most prominently the R-CNN detection framework [21], relying on object proposals for generating detections. Many of the commonly used bounding box proposal methods [28,54] first generate a set of overlapping segments, e.g. Selective Search [72] or MCG [1]. In R-CNN, bounding boxes of each segment are then scored using a CNN-based classifier, while each segment is treated independently.\n\nThe second category encompasses detection, then segmentation, where bounding-box detections are refined to instance specific segmentations. Either CNNs [23,24] or non-parametric methods [10] are typically used, however, in both cases without coupling between individual predictions.\n\nThird, simultaneous detection and segmentation is significantly more delicate. Earlier methods relied on Hough voting [36,57]. More recent works formulate a joint inference problem on pixel and instance level using CRFs [11,26,42,71,78,80]. Differences lie in the generation of proposals (exemplars, average class shape, direct regression), the cues considered (pixel-level labeling, depth ordering), and the inference method (probabilistic, heuristics).\n\n\nLower bounds, oracles & baselines\n\nIn Tab. 6, we provide lower-bounds that any sensible method should improve upon, as well as oracle-case results Figure 5. Qualitative examples of selected baselines. From left to right: image with stereo depth maps partially overlayed, annotation, DeepLab [48], Adelaide [37], and Dilated10 [79]. The color coding of the semantic classes matches Fig. 1 Table 6. Baseline results on instance-level semantic labeling task using the metrics described in Sec. 4. All numbers in %.\n\n(i.e. using the test time ground truth). For our experiments, we rely on publicly available implementations. We train a Fast-R-CNN (FRCN) detector [20] on our training data in order to score MCG object proposals [1]. Then, we use either its output bounding boxes as (rectangular) segmentations, the associated region proposal, or its convex hull as a per-instance segmentation. The best main score AP is 4.6 %, is obtained with convex hull proposals, and becomes larger when restricting the evaluation to 50 % overlap or close instances. We contribute these rather low scores to our challenging dataset, biased towards busy and cluttered scenes, where many, often highly occluded, objects occur at various scales, c.f . Sec. 2. Further, the MCG bottom-up proposals seem to be unsuited for such street scenes and cause extremely low scores when requiring large overlaps. We confirm this interpretation with oracle experiments, where we replace the proposals at test-time with ground truth segments or replace the FRCN classifier with an oracle. In doing so, the task of object localization is decoupled from the classification task. The results in Tab. 6 show that when bound to MCG proposals, the oracle classifier is only slightly better than FRCN. On the other hand, when the proposals are perfect, FRCN achieves decent results. Overall, these observations unveil that the instance-level performance of our baseline is bound by the region proposals.\n\n\nConclusion and Outlook\n\nIn this work, we presented Cityscapes, a comprehensive benchmark suite that has been carefully designed to spark progress in semantic urban scene understanding by: (i) creating the largest and most diverse dataset of street scenes with high-quality and coarse annotations to date; (ii) developing a sound evaluation methodology for pixel-level and instance-level semantic labeling; (iii) providing an in-depth analysis of the characteristics of our dataset; (iv) evaluating several state-of-the-art approaches on our benchmark. To keep pace with the rapid progress in scene understanding, we plan to adapt Cityscapes to future needs over time.\n\nThe significance of Cityscapes is all the more apparent from three observations. First, the relative order of performance for state-of-the-art methods on our dataset is notably different than on more generic datasets such as PASCAL VOC. Our conclusion is that serious progress in urban scene understanding may not be achievable through such generic datasets. Second, the current state-of-the-art in semantic labeling on KITTI and CamVid is easily reached and to some extent even outperformed by applying an off-the-shelf fullyconvolutional network [41] trained on Cityscapes only, as demonstrated in Sec. 3.5. This underlines the compatibility and unique benefit of our dataset. Third, Cityscapes will pose a significant new challenge for our field given that it is currently far from being solved. The best performing baseline for pixel-level semantic segmentation obtains an IoU score of 67.1 %, whereas the best current methods on PAS-CAL VOC and KITTI reach IoU levels of 77.9 % [3] and 72.5 % [73], respectively. In addition, the instance-level task is particularly challenging with an AP score of 4.6 %.\n\n\nA. Related Datasets\n\nIn Tab. 7 we provide a comparison to other related datasets in terms of the type of annotations, the meta information provided, the camera perspective, the type of scenes, and their size. The selected datasets are either of large scale or focus on street scenes. Table 8 provides precise definitions of our annotated classes. These definitions were used to guide our labeling process, as well as quality control. In addition, we include a typical example for each class.\n\n\nB. Class Definitions\n\nThe annotators were instructed to make use of the depth ordering and occlusions of the scene to accelerate labeling, analogously to LabelMe [60]; see Fig. 6 for an example. In doing so, distant objects are annotated first, while occluded parts are annotated with a coarser, conservative boundary (possibly larger than the actual object). Subsequently, the occluder is annotated with a polygon that lies in front of the occluded part. Thus, the boundary between these objects is shared and consistent.\n\nHoles in an object through which a background region can be seen are considered to be part of the object. This allows keeping the labeling effort within reasonable bounds such that objects can be described via simple polygons forming simply-connected sets. \n\n\nC. Example Annotations\n\n\nD. Detailed Results\n\nIn this section, we present additional details regarding our control experiments and baselines. Specifically, we give individual class scores that complement the aggregated scores in the main paper. Moreover, we provide details on the training procedure for all baselines. Finally, we show additional qualitative results of all methods.  Tables 10 and 12 give the corresponding instance-normalized iIoU scores. In addition, Figs. 8 and 9 contain qualitative examples of these methods.\n\n\nD.1. Semantic labeling\n\nBasic setup. All baselines relied on single frame, monocular LDR images and were pretrained on ImageNet [59], i.e. Figure 6. Exemplary labeling process. Distant objects are annotated first and subsequently their occluders. This ensures the boundary between these objects to be shared and consistent. their underlying CNN was generally initialized with Ima-geNet VGG weights [68]. Subsequently, the CNNs were finetuned on Cityscapes using the respective portions listed in Tab. 4. In our own FCN [41] experiments, we additionally investigated first pretraining on PASCAL-Context [45], but found this to not influence performance given a sufficiently large number of training iterations. Most baselines applied a subsampling of the input image, c.f . Tab. 4, probably due to time or memory constraints. Only Adelaide [37], Dilated10 [79], and our FCN experiments were conducted on the full-resolution images. In the first case, a new random patch of size 614 \u00d7 614 pixels was drawn at each iteration. In our FCN training, we split each image into two halves (left and right) with an overlap that is sufficiently large considering the network's receptive field.\n\nOwn baselines. The training procedure of all our FCN experiments follows [41]. We use three-stage training with subsequently smaller strides, i.e. first FCN-32s, then FCN-16s, and then FCN-8s, always initializing with the parameters from the previous stage. We add a 4 th stage for which we reduce the learning rate by a factor of 10. The training parameters are identical to those publicly available for training on PASCAL-Context [45], except that we reduce the learning rate to account for the increased image resolution. Each stage is trained until convergence on the validation set; pixels with void ground truth are ignored such that they do not induce any gradient. Eventually, we retrain on train and val together with the same number of epochs, yielding 243 250, 69 500, 62 550, and 5950 iterations for stages 1 through 4. Note that each iteration corresponds to half of an image (see above). For the variant with factor 2 downsampling, no image splitting is necessary, yielding    [22,29,32,33,58,64,77,80] Table 7. Comparison to related datasets. We list the type of labels provided, i.e. object bounding boxes (B), dense pixel-level semantic labels (D), coarse labels (C) that do not aim to label the whole image. Further, we mark if color, video, and depth information are available. We list the camera perspective, the scene type, the number of images, and the number of semantic classes.\n\ntions of the 3 rd -party baselines, we have to rely on authorprovided information.\n\nSegNet [4] training for both the basic and extended variant was performed until convergence, yielding approximately 50 epochs. Inference takes 0.12 s per image.\n\nDPN [40] was trained using the original procedure, while using all available Cityscapes annotations.\n\nFor training CRF as RNN [81], an FCN-32s model was trained for 3 days on train using a GPU. Subsequently an FCN-8s model was trained for 2 days, and eventually the model was further finetuned including the CRF-RNN layers. Testing takes 0.7 s on half-resolution images.\n\nFor training DeepLab on the fine annotations, denoted DeepLab-LargeFOV-Strong, the authors applied the training procedure from [9]. The model was trained on train for 40 000 iterations until convergence on val. Then val was included in the training set for another 40 000 iterations. In both cases, a mini-batch size of 10 was applied. Each training iteration lasts 0.5 s, while inference including the dense CRF takes 4 s per image. The DeepLab variant including our coarse annotations, termed DeepLab-LargeFOV-StrongWeak, followed the protocol in [48] and is initialized from the DeepLab-LargeFOV-Strong model. Each mini-batch consists of 5 finely and 5 coarsely annotated images and training is performed for 20 000 iterations until convergence on val. Then, training was continued for another 20 000 iterations on train and val.\n\nAdelaide [37] was trained for 8 days using random crops of the input image as described above. Inference on a single image takes 35 s.\n\nThe best performing baseline, Dilated10 [79], is a convolutional network that consists of a front-end prediction module and a context aggregation module. The front-end module is an adaptation of the VGG-16 network based on dilated convolutions. The context module uses dilated convolutions to systematically expand the receptive field and aggregate contextual information. This module is derived from the \"Basic\" network, where each layer has C = 19 feature maps. The total number of layers in the context module is 10, hence the name Dilation10. The increased number of layers in the context module (10 for Cityscapes versus 8 for PASCAL VOC) is due to the higher input resolution. The complete Dilation10 model is a pure convolutional network: there is no CRF and no structured prediction. The Dila-tion10 network was trained in three stages. First, the frontend prediction module was trained for 40 000 iterations on randomly sampled crops of size 628\u00d7628, with learning rate 10 \u22124 , momentum 0.99, and batch size 8. Second, the context module was trained for 24 000 iterations on whole (uncropped) images, with learning rate 10 \u22124 , momentum 0.99, and batch size 100. Third, the complete model (front-end + context) was jointly trained for 60 000 iterations on halves of images (input size 1396\u00d71396, including padding), with learning rate 10 \u22125 , momentum 0.99, and batch size 1.\n\n\nD.2. Instance-level semantic labeling\n\nFor our instance-level semantic labeling baselines and control experiments, we rely on Fast R-CNN [20] and proposal regions from either MCG (Multiscale Combinatorial Grouping [1]) or from the ground truth annotations.\n\nWe use the standard training and testing parameters for Fast R-CNN. Training starts with a model pre-trained on ImageNet [59]. We use a learning rate of 0.001 and stop when the validation error plateaus after 120 000 iterations.\n\nAt test time, one score per class is assigned to each object proposal. Subsequently, thresholding and non-maximum suppression is applied and either the bounding boxes, the original proposal regions or their convex hull are used to ii generate the predicted masks of each instance. Quantitative results of all classes can be found in Tables 13 to 16 and qualitative results in Fig. 12. iii Category\n\n\nClass\n\nDefinition Examples human person 1\n\nAll humans that would primarily rely on their legs to move if necessary. Consequently, this label includes people who are standing/sitting, or otherwise stationary. This class also includes babies, people pushing a bicycle, or standing next to it with both legs on the same side of the bicycle. rider 1\n\nHumans relying on some device for movement. This includes drivers, passengers, or riders of bicycles, motorcycles, scooters, skateboards, horses, Segways, (inline) skates, wheelchairs, road cleaning cars, or convertibles. Note that a visible driver of a closed car can only be seen through the window. Since holes are considered part of the surrounding object, the human is included in the car label.\n\nvehicle car 1\n\nThis includes cars, jeeps, SUVs, vans with a continuous body shape (i.e. the driver's cabin and cargo compartment are one). Does not include trailers, which have their own separate class.\ntruck 1\nThis includes trucks, vans with a body that is separate from the driver's cabin, pickup trucks, as well as their trailers.\n\nbus 1 This includes buses that are intended for 9+ persons for public or long-distance transport.\n\ntrain 1 All vehicles that move on rails, e.g. trams, trains.\n\n1 Single instance annotation available. 2 Not included in challenges. This includes motorcycles, mopeds, and scooters without the driver or other passengers. The latter receive the label rider.\n\nbicycle 1 This includes bicycles without the cyclist or other passengers. The latter receive the label rider.\ncaravan 1,2\nVehicles that (appear to) contain living quarters. This also includes trailers that are used for living and has priority over the trailer class.\n\ntrailer 1,2 Includes trailers that can be attached to any vehicle, but excludes trailers attached to trucks. The latter are included in the truck label.\n\n\nnature vegetation\n\nTrees, hedges, and all kinds of vertically growing vegetation. Plants attached to buildings/walls/fences are not annotated separately, and receive the same label as the surface they are supported by.\n\nterrain Grass, all kinds of horizontally spreading vegetation, soil, or sand. These are areas that are not meant to be driven on. This label may also include a possibly adjacent curb. Single grass stalks or very small patches of grass are not annotated separately and thus are assigned to the label of the region they are growing on. 1 Single instance annotation available. 2 Not included in challenges.    Since a part of the vehicle from which our data was recorded is visible in all frames, it is assigned to this special label. This label is also available at test time.\n\nunlabeled 2 Pixels that were not explicitly assigned to a label.\n\nout of roi 2 Narrow strip of 5 pixels along the image borders that is not considered for training or evaluation. This label is also available at test-time.\n\n\nrectification border 2\n\nAreas close to the image border that contain artifacts resulting from the stereo pair rectification. This label is also available at test time. 1 Single instance annotation available. 2 Not included in challenges.  GT segmentation with [41] Table 9. Detailed results of our control experiments for the pixel-level semantic labeling task in terms of the IoU score on the class level. All numbers are given in percent. See the main paper for details on the listed methods.          SegNet basic [4] DPN [40] CRF as RNN [81] DeepLab LargeFOV StrongWeak [48] Adelaide [37] Dilated10 [79] Figure 11. Exemplary output of our baseline experiments for the pixel-level semantic labeling task, see the main paper for details. The image is part of our test set and has the largest number of car instances.  Figure 12. Exemplary output of our control experiments and baselines for the instance-level semantic labeling task, see the main paper for details.\n\nFigure 1 .\n1Number of finely annotated pixels (y-axis) per class and their associated categories (x-axis).\n\nFigure 2\n2Figure 2. Proportion of annotated pixels (y-axis) per category (x-axis) for Cityscapes, CamVid [7], DUS [61], and KITTI [19].\n\nFigure 3 .Figure 4 .\n34Dataset statistics regarding scene complexity. Only MS COCO and Cityscapes provide instance segmentation masks. Histogram of object distances in meters for class vehicle.\n\nFigure 7\n7presents several examples of annotated frames from our dataset that exemplify its diversity and difficulty. All examples are taken from the train and val splits and were chosen by searching for the extremes in terms of the number of traffic participant instances in the scene; see Fig. 7 for details.\n\n\n9 and 11 list all individual class-level IoU scores for all control experiments and baselines.\n\n\n80 325, 68 425, 35 700, and 5950 iterations in the respective stages. The variant only trained on val (full resolution) uses train for validation, leading to 130 000, 35 700, 47 600, and 0 iterations in the 4 stages. Our last FCN variant is trained using the coarse annotations only, with 386 750, 113 050, 35 700, and 0 iterations in the respective stage; pixels with void ground truth are ignored here as well.\n\n\n3 rd -party baselines. Note that for the following descrip-\n\nFigure 7 .\n7Examples of our annotations on various images of our train and val sets. The images were selected based on criteria overlayed on each image.\n\nFigure 8 .\n8Exemplary output of our control experiments for the pixel-level semantic labeling task, see the main paper for details. The image is part of our test set and has both, the largest number of instances and persons.\n\nFigure 9 .Figure 10 .\n910Exemplary output of our baselines for the pixel-level semantic labeling task, see the main paper for details. The image is part of our test set and has both, the largest number of instances and persons. Exemplary output of our control experiments for the pixel-level semantic labeling task, see the main paper for details. The image is part of our test set and has the largest number of car instances.\n\n\n10 9  ] annot. density[%]    Ours (fine) \n9.43 \n97.1 \nOurs (coarse) \n26.0 \n67.5 \nCamVid \n0.62 \n96.2 \nDUS \n0.14 \n63.0 \nKITTI \n0.23 \n88.9 \n\nTable 1. Absolute number and density of annotated pix-\nels for Cityscapes, DUS, KITTI, and CamVid (upscaled to \n1280 \u00d7 720 pixels to maintain the original aspect ratio). \n\n\n\n\nTable 3. Quantitative results of control experiments for semantic labeling using the metrics presented in Sec. 3.1.Table 4. Quantitative results of baselines for semantic labeling using the metrics presented in Sec. 3.1. The first block lists results from our own experiments, the second from those provided by 3 rd parties. All numbers are given in percent and we indicate the used training data for each method, i.e. train fine, val fine, coarse extra as well as a potential downscaling factor (sub) of the input image.Average over \n\nClasses \nCategories \n\nMetric [%] IoU iIoU IoU iIoU \n\nstatic fine (SF) \n10.1 \n4.7 \n26.3 19.9 \nstatic coarse (SC) \n10.3 \n5.0 \n27.5 21.7 \nGT segmentation with SF \n10.1 \n6.3 \n26.5 25.0 \nGT segmentation with SC \n10.9 \n6.3 \n29.6 27.0 \n\nGT segmentation with [41] \n79.4 52.6 93.3 80.9 \n\nGT subsampled by 2 \n97.2 92.6 97.6 93.3 \nGT subsampled by 4 \n95.2 90.4 96.0 91.2 \nGT subsampled by 8 \n90.7 82.8 92.1 83.9 \nGT subsampled by 16 \n84.6 70.8 87.4 72.9 \nGT subsampled by 32 \n75.4 53.7 80.2 58.1 \nGT subsampled by 64 \n63.8 35.1 71.0 39.6 \nGT subsampled by 128 \n50.6 21.1 60.6 29.9 \n\nnearest training neighbor \n21.3 \n5.9 \n39.7 18.6 \n\ntrain \nval \ncoarse \nsub \n\nClasses \nCategories \n\nIoU \niIoU \nIoU \niIoU \n\nFCN-32s \n61.3 \n38.2 \n82.2 \n65.4 \nFCN-16s \n64.3 \n41.1 \n84.5 \n69.2 \nFCN-8s \n65.3 \n41.7 \n85.7 \n70.1 \nFCN-8s \n2 \n61.9 \n33.6 \n81.6 \n60.9 \nFCN-8s \n58.3 \n37.4 \n83.4 \n67.2 \nFCN-8s \n58.0 \n31.8 \n78.2 \n58.4 \n\n[4] extended \n4 \n56.1 \n34.2 \n79.8 \n66.4 \n[4] basic \n4 \n57.0 \n32.0 \n79.1 \n61.9 \n[40] \n3 \n59.1 \n28.1 \n79.5 \n57.9 \n[81] \n2 \n62.5 \n34.4 \n82.7 \n66.0 \n[9] \n2 \n63.1 \n34.5 \n81.2 \n58.7 \n[48] \n2 \n64.8 \n34.9 \n81.3 \n58.7 \n[37] \n66.4 46.7 82.8 \n67.4 \n[79] \n67.1 42.0 86.5 71.1 \n\n\n\n\n.Proposals \nClassif. AP \nAP 50% AP 100m AP 50m \n\nMCG regions FRCN 2.6 \n9.0 \n4.4 \n5.5 \nMCG bboxes FRCN 3.8 \n11.3 \n6.5 \n8.9 \nMCG hulls \nFRCN 4.6 \n12.9 \n7.7 \n10.3 \n\nGT bboxes \nFRCN 8.2 \n23.7 \n12.6 \n15.2 \nGT regions \nFRCN 41.3 \n41.3 \n58.1 \n64.9 \n\nMCG regions GT \n10.5 \n27.0 \n16.0 \n18.7 \nMCG bboxes GT \n9.9 \n25.8 \n15.3 \n18.9 \nMCG hulls \nGT \n11.6 \n29.1 \n17.7 \n21.4 \n\n\n\nTable 8 .\n8List of annotated classes including their definition and typical example.iv \n\n\n\nTable 8 .\n8List of annotated classes including their definition and typical example. (continued)v \n\n\n\nTable 8 .\n8List of annotated classes including their definition and typical example. (continued)vi \n\n\n\nTable 8 .\n8List of annotated classes including their definition and typical example. (continued)vii \n\n\n\nTable 8 .\n8List of annotated classes including their definition and typical example. (continued)viii \n\n\n\nTable 8 .\n8List of annotated classes including their definition and typical example. (continued)ix \n\n\n\nTable 10 .\n10Detailed results of our control experiments for the pixel-level semantic labeling task in terms of the instance-normalized iIoU score on the class level. All numbers are given in percent. See the main paper for details on the listed methods.Table 11. Detailed results of our baseline experiments for the pixel-level semantic labeling task in terms of the IoU score on the class level. All numbers are given in percent and we indicate the used training data for each method, i.e. train fine, val fine, coarse extra, as well as a potential downscaling factor (sub) of the input image. See the main paper and Sec. D.1 for details on the listed methods.xi \n\n\n\nTable 12 .\n12Detailed results of our baseline experiments for the pixel-level semantic labeling task in terms of the instance-normalized iIoU score on the class level. All numbers are given in percent and we indicate the used training data for each method, i.e. train fine, val fine, coarse extra, as well as a potential downscaling factor (sub) of the input image. See the main paper and Sec. D.1 for details on the listed methods.Table 13. Detailed results of our baseline experiments for the instance-level semantic labeling task in terms of the region-level average precision scores AP on the class level. All numbers are given in percent. See the main paper and Sec. D.2 for details on the listed methods.Proposals \nClassifier \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorcycle \n\nbicycle \nmean AP \n\nMCG regions FRCN \n1.9 1.0 6.2 4.0 3.1 2.8 1.5 0.6 \n2.6 \nMCG bboxes FRCN \n0.5 0.1 7.8 6.4 10.3 4.5 0.9 0.2 \n3.8 \nMCG hulls \nFRCN \n1.3 0.6 10.5 6.1 9.7 5.9 1.7 0.5 \n4.6 \n\nGT bboxes \nFRCN \n7.6 0.5 17.5 10.7 15.7 8.4 2.6 2.9 \n8.2 \nGT regions \nFRCN \n65.5 40.6 65.9 21.1 31.9 30.2 28.8 46.4 41.3 \n\nMCG regions GT \n3.7 4.4 11.9 19.9 21.5 12.4 7.8 2.6 \n10.5 \nMCG bboxes GT \n2.0 2.0 10.9 18.2 22.1 15.9 6.0 2.2 \n9.9 \nMCG hulls \nGT \n3.4 4.1 13.4 20.4 24.1 16.0 8.3 2.8 \n11.6 \n\nxii \n\n\n\nTable 14 .\n14Detailed results of our baseline experiments for the instance-level semantic labeling task in terms of the region-level average precision scores AP 50% for an overlap value of 50 %. All numbers are given in percent. See the main paper and Sec. D.2 for details on the listed methods.Proposals \nClassifier \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorcycle \n\nbicycle \nmean AP 100m \n\nMCG regions FRCN \n3.7 1.6 10.2 6.8 5.4 4.2 2.2 1.1 \n4.4 \nMCG bboxes FRCN \n0.9 0.1 12.9 11.3 18.5 6.9 1.3 0.3 \n6.5 \nMCG hulls \nFRCN \n2.6 1.1 17.5 10.6 17.4 9.2 2.6 0.9 \n7.7 \n\nGT bboxes \nFRCN \n8.8 0.8 25.3 18.4 27.1 13.0 3.9 3.6 \n12.6 \nGT regions \nFRCN \n79.1 66.0 78.9 33.6 53.9 47.1 42.6 63.5 58.1 \n\nMCG regions GT \n6.8 6.8 18.9 28.7 32.7 19.0 10.5 4.3 \n16.0 \nMCG bboxes GT \n3.5 2.9 17.3 27.3 34.5 24.9 8.2 3.7 \n15.3 \nMCG hulls \nGT \n6.1 6.2 21.4 29.9 37.2 24.7 11.4 4.7 \n17.7 \n\n\n\nTable 15 .\n15Detailed results of our baseline experiments for the instance-level semantic labeling task in terms of the region-level average precision scores AP 100m for objects within 100 m. All numbers are given in percent. See the main paper and Sec. D.2 for details on the listed methods.Proposals \nClassifier \nperson \nrider \ncar \ntruck \nbus \ntrain \nmotorcycle \n\nbicycle \nmean AP 50m \n\nMCG regions FRCN \n4.0 1.7 12.0 9.0 7.8 6.4 2.4 1.1 \n5.5 \nMCG bboxes FRCN \n1.0 0.1 15.5 14.9 27.7 10.0 1.4 0.4 \n8.9 \nMCG hulls \nFRCN \n2.7 1.1 21.2 14.0 25.2 14.2 2.7 1.0 \n10.3 \n\nGT bboxes \nFRCN \n8.5 0.8 26.6 23.2 37.2 17.7 4.1 3.6 \n15.2 \nGT regions \nFRCN \n79.1 68.3 80.5 42.9 69.4 67.9 46.2 64.7 64.9 \n\nMCG regions GT \n7.2 7.0 21.7 32.4 42.4 23.6 11.1 4.5 \n18.7 \nMCG bboxes GT \n3.7 3.0 19.9 33.0 46.0 32.9 8.6 3.8 \n18.9 \nMCG hulls \nGT \n6.5 6.4 24.8 35.4 49.6 31.8 12.2 4.9 \n21.4 \n\n\n\nTable 16 .\n16Detailed results of our baseline experiments for the instance-level semantic labeling task in terms of the region-level average precision scores AP 50m for objects within 50 m. All numbers are given in percent. See the main paper and Sec. D.2 for details on the listed methods.xiii \n\n\n\nMultiscale combinatorial grouping. P Arbelaez, J Pont-Tuset, J Barron, F Marqu\u00e9s, J Malik, CVPR. 7, 8, iiP. Arbelaez, J. Pont-Tuset, J. Barron, F. Marqu\u00e9s, and J. Ma- lik. Multiscale combinatorial grouping. In CVPR, 2014. 7, 8, ii\n\nGeosemantic segmentation. S Ardeshir, K M Collins-Sibley, M Shah, CVPR. ii4S. Ardeshir, K. M. Collins-Sibley, and M. Shah. Geo- semantic segmentation. In CVPR, 2015. 4, ii\n\nA Arnab, S Jayasumana, S Zheng, P H S Torr, arXiv:1511.08119v3Higher order conditional random fields in deep neural networks. cs.CVA. Arnab, S. Jayasumana, S. Zheng, and P. H. S. Torr. Higher order conditional random fields in deep neural networks. arXiv:1511.08119v3 [cs.CV], 2015. 8\n\nSegnet: A deep convolutional encoder-decoder architecture for image segmentation. V Badrinarayanan, A Kendall, R Cipolla, arXiv:1511.00561v21, 6, 7, ii, xii, xv, xviics.CVV. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. arXiv:1511.00561v2 [cs.CV], 2015. 1, 6, 7, ii, xii, xv, xvii\n\nWhat's the point: Semantic segmentation with point supervision. A Bearman, O Russakovsky, V Ferrari, L Fei-Fei, arXiv:1506.02106v4cs.CVA. Bearman, O. Russakovsky, V. Ferrari, and L. Fei-Fei. What's the point: Semantic segmentation with point super- vision. arXiv:1506.02106v4 [cs.CV], 2015. 6\n\nPedestrian detection at 100 frames per second. R Benenson, M Mathias, R Timofte, L Van Gool, CVPR. R. Benenson, M. Mathias, R. Timofte, and L. Van Gool. Pedestrian detection at 100 frames per second. In CVPR, 2012. 1\n\nSemantic object classes in video: A high-definition ground truth database. G J Brostow, J Fauqueur, R Cipolla, Pattern Recognition Letters. 302G. J. Brostow, J. Fauqueur, and R. Cipolla. Semantic object classes in video: A high-definition ground truth database. Pattern Recognition Letters, 30(2):88-97, 2009. 1, 3, 4, 7, ii\n\nScene Labeling with LSTM Recurrent Neural Networks. W Byeon, T M Breuel, F Raue, M Liwicki, CVPR. W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene Labeling with LSTM Recurrent Neural Networks. In CVPR, 2015. 6\n\nSemantic image segmentation with deep convolutional nets and fully connected CRFs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, ICLR. 6, 7, ii, xiiL.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep con- volutional nets and fully connected CRFs. In ICLR, 2015. 5, 6, 7, ii, xii\n\nMulti-instance object segmentation with occlusion handling. Y.-T Chen, X Liu, M.-H Yang, CVPR. Y.-T. Chen, X. Liu, and M.-H. Yang. Multi-instance object segmentation with occlusion handling. In CVPR, 2015. 7\n\nConvolutional feature masking for joint object and stuff segmentation. J Dai, K He, J Sun, CVPR. 67J. Dai, K. He, and J. Sun. Convolutional feature masking for joint object and stuff segmentation. In CVPR, 2015. 6, 7\n\nPedestrian detection: An evaluation of the state of the art. P Doll\u00e1r, C Wojek, B Schiele, P Perona, Trans. PAMI. 3444P. Doll\u00e1r, C. Wojek, B. Schiele, and P. Perona. Pedestrian detection: An evaluation of the state of the art. Trans. PAMI, 34(4):743-761, 2012. 1, 4\n\nMonocular pedestrian detection: Survey and experiments. M Enzweiler, D M Gavrila, 31M. Enzweiler and D. M. Gavrila. Monocular pedestrian de- tection: Survey and experiments. 31(12):2179-2195, 2009. 1\n\nThe PASCAL visual object classes challenge: A retrospective. M Everingham, S M A Eslami, L Van Gool, C K I Williams, J Winn, A Zisserman, IJCV. 1111M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL visual object classes challenge: A retrospective. IJCV, 111(1):98- 136, 2015. 1, 4, 5, 6, 7, ii\n\nObject detection with discriminatively trained partbased models. P F Felzenszwalb, R B Girshick, D Mcallester, D Ramanan, Trans. PAMI. 3291P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra- manan. Object detection with discriminatively trained part- based models. Trans. PAMI, 32(9):1627-1645, 2010. 1\n\nMaking Bertha see. U Franke, D Pfeiffer, C Rabe, C Kn\u00f6ppel, M Enzweiler, F Stein, R G Herrtwich, ICCV Workshops. U. Franke, D. Pfeiffer, C. Rabe, C. Kn\u00f6ppel, M. Enzweiler, F. Stein, and R. G. Herrtwich. Making Bertha see. In ICCV Workshops, 2013. 1\n\nToward automated driving in cities using close-to-market sensors: An overview of the V-Charge project. P Furgale, U Schwesinger, M Rufli, W Derendarz, H Grimmett, P M\u00fchlfellner, S Wonneberger, B Li, IV Symposium. P. Furgale, U. Schwesinger, M. Rufli, W. Derendarz, H. Grimmett, P. M\u00fchlfellner, S. Wonneberger, B. Li, et al. Toward automated driving in cities using close-to-market sensors: An overview of the V-Charge project. In IV Sympo- sium, 2013. 1\n\n3D traffic scene understanding from movable platforms. A Geiger, M Lauer, C Wojek, C Stiller, R Urtasun, Trans. PAMI. 365A. Geiger, M. Lauer, C. Wojek, C. Stiller, and R. Urtasun. 3D traffic scene understanding from movable platforms. Trans. PAMI, 36(5):1012-1025, 2014. 1\n\nA Geiger, P Lenz, C Stiller, R Urtasun, Vision meets robotics: The KITTI dataset. IJRR. 32A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The KITTI dataset. IJRR, 32(11), 2013. 1, 3, 4, ii\n\nFast R-CNN. R Girshick, ICCV. ii8R. Girshick. Fast R-CNN. In ICCV, 2015. 8, ii\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, CVPR. R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea- ture hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 7\n\nDisplets: Resolving stereo ambiguities using object knowledge. F Gueney, A Geiger, CVPR. ii4F. Gueney and A. Geiger. Displets: Resolving stereo ambi- guities using object knowledge. In CVPR, 2015. 4, ii\n\nSimultaneous detection and segmentation. B Hariharan, P Arbel\u00e1ez, R B Girshick, J Malik, ECCV. B. Hariharan, P. Arbel\u00e1ez, R. B. Girshick, and J. Malik. Si- multaneous detection and segmentation. In ECCV, 2014. 7\n\nHypercolumns for object segmentation and fine-grained localization. B Hariharan, P A Arbel\u00e1ez, R B Girshick, J Malik, CVPR. B. Hariharan, P. A. Arbel\u00e1ez, R. B. Girshick, and J. Malik. Hypercolumns for object segmentation and fine-grained lo- calization. In CVPR, 2015. 7\n\nLearning scene-specific pedestrian detectors without real data. H Hattori, V N Boddeti, K M Kitani, T Kanade, CVPR. H. Hattori, V. N. Boddeti, K. M. Kitani, and T. Kanade. Learning scene-specific pedestrian detectors without real data. In CVPR, 2015. 7\n\nAn exemplar-based CRF for multiinstance object segmentation. X He, S Gould, CVPR. X. He and S. Gould. An exemplar-based CRF for multi- instance object segmentation. In CVPR, 2014. 7\n\n. D Hoiem, J Hays, J Xiao, A Khosla, Guest editorial: Scene understanding. IJCVD. Hoiem, J. Hays, J. Xiao, and A. Khosla. Guest editorial: Scene understanding. IJCV, 2015. 1\n\nWhat makes for effective detection proposals?. J Hosang, R Benenson, P Doll\u00e1r, B Schiele, Trans. PAMI. 384J. Hosang, R. Benenson, P. Doll\u00e1r, and B. Schiele. What makes for effective detection proposals? Trans. PAMI, 38(4):814-830, 2015. 7\n\nNonparametric semantic segmentation for 3D street scenes. H Hu, B Upcroft, IROS. ii4H. Hu and B. Upcroft. Nonparametric semantic segmenta- tion for 3D street scenes. In IROS, 2013. 4, ii\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012. 1\n\nInfactory calibration of multiocular camera systems. L Kr\u00fcger, C W\u00f6hler, A W\u00fcrz-Wessel, F Stein, SPIE Photonics Europe (Optical Metrology in Production Engineering). L. Kr\u00fcger, C. W\u00f6hler, A. W\u00fcrz-Wessel, and F. Stein. In- factory calibration of multiocular camera systems. In SPIE Photonics Europe (Optical Metrology in Production Engi- neering), 2004. 2\n\nJoint semantic segmentation and 3D reconstruction from monocular video. A Kundu, Y Li, F Dellaert, F Li, J Rehg, ECCV. ii4A. Kundu, Y. Li, F. Dellaert, F. Li, and J. Rehg. Joint se- mantic segmentation and 3D reconstruction from monocular video. In ECCV, 2014. 4, ii\n\nPulling things out of perspective. L Ladicky, J Shi, M Pollefeys, CVPR. ii4L. Ladicky, J. Shi, and M. Pollefeys. Pulling things out of perspective. In CVPR, 2014. 4, ii\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 1Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 2015. 1\n\nDynamic 3D scene analysis from a moving vehicle. B Leibe, N Cornelis, K Cornelis, L Van Gool, CVPR. ii1B. Leibe, N. Cornelis, K. Cornelis, and L. Van Gool. Dy- namic 3D scene analysis from a moving vehicle. In CVPR, 2007. 1, ii\n\nRobust object detection with interleaved categorization and segmentation. B Leibe, A Leonardis, B Schiele, IJCVB. Leibe, A. Leonardis, and B. Schiele. Robust object detec- tion with interleaved categorization and segmentation. IJCV, 77(1-3):259-289, 2008. 7\n\nEfficient piecewise training of deep structured models for semantic segmentation. G Lin, C Shen, A Van Den Hengel, I Reid, CVPR. 6, 8, i, ii, xii, xv, xviito appear. 5G. Lin, C. Shen, A. van den Hengel, and I. Reid. Efficient piecewise training of deep structured models for semantic segmentation. In CVPR, 2016, to appear. 5, 6, 8, i, ii, xii, xv, xvii\n\nMicrosoft COCO: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, ECCV. 7T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft COCO: Com- mon objects in context. In ECCV, 2014. 1, 4, 5, 7, ii\n\nW Liu, A Rabinovich, A C Berg, arXiv:1506.04579v2Parsenet: Looking wider to see better. cs.CVW. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking wider to see better. arXiv:1506.04579v2 [cs.CV], 2015. 6\n\nSemantic image segmentation via deep parsing network. Z Liu, X Li, P Luo, C C Loy, X Tang, ICCV. 6, ii, xii, xv, xviiZ. Liu, X. Li, P. Luo, C. C. Loy, and X. Tang. Semantic im- age segmentation via deep parsing network. In ICCV, 2015. 5, 6, ii, xii, xv, xvii\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, CVPR. 6, 8, i, xi, xiv, xviJ. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 1, 3, 5, 6, 8, i, xi, xiv, xvi\n\nObject detection and segmentation from joint embedding of parts and pixels. M Maire, S X Yu, P Perona, ICCV. M. Maire, S. X. Yu, and P. Perona. Object detection and segmentation from joint embedding of parts and pixels. In ICCV, 2011. 7\n\nWatch and learn: Semi-supervised learning for object detectors from video. I Misra, A Shrivastava, M Hebert, CVPR. I. Misra, A. Shrivastava, and M. Hebert. Watch and learn: Semi-supervised learning for object detectors from video. In CVPR, 2015. 7\n\nFeedforward semantic segmentation with zoom-out features. M Mostajabi, P Yadollahpour, G Shakhnarovich, CVPR. M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich. Feed- forward semantic segmentation with zoom-out features. In CVPR, 2015. 6\n\nThe role of context for object detection and semantic segmentation in the wild. R Mottaghi, X Chen, X Liu, N.-G Cho, S.-W Lee, S Fidler, R Urtasun, A Yuille, CVPR. i, iiR. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fi- dler, R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014. 1, 5, i, ii\n\nModeling the shape of the scene: A holistic representation of the spatial envelope. A Oliva, A Torralba, IJCV. 423A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. IJCV, 42(3):145-175, 2001. 1\n\nIs object localization for free? Weakly-supervised learning with convolutional neural networks. M Oquab, L Bottou, I Laptev, J Sivic, CVPR. M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object local- ization for free? Weakly-supervised learning with convolu- tional neural networks. In CVPR, 2015. 7\n\nWeakly-and semi-supervised learning of a DCNN for semantic image segmentation. G Papandreou, L.-C Chen, K Murphy, A L Yuille, ICCV. 6, 7, 8, ii, xii, xv, xviiG. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille. Weakly-and semi-supervised learning of a DCNN for se- mantic image segmentation. In ICCV, 2015. 6, 7, 8, ii, xii, xv, xvii\n\nConstrained convolutional neural networks for weakly supervised segmentation. D Pathak, P Kraehenbuehl, T Darrell, ICCV. D. Pathak, P. Kraehenbuehl, and T. Darrell. Constrained con- volutional neural networks for weakly supervised segmenta- tion. In ICCV, 2015. 6\n\nFully convolutional multi-class multiple instance learning. D Pathak, E Shelhamer, J Long, T Darrell, ICLR. D. Pathak, E. Shelhamer, J. Long, and T. Darrell. Fully con- volutional multi-class multiple instance learning. In ICLR, 2015. 6\n\nExploiting the power of stereo confidences. D Pfeiffer, S K Gehrig, N Schneider, CVPR. D. Pfeiffer, S. K. Gehrig, and N. Schneider. Exploiting the power of stereo confidences. In CVPR, 2013. 2\n\nRecurrent convolutional neural networks for scene parsing. P H Pinheiro, R Collobert, ICML. P. H. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene parsing. In ICML, 2014. 6\n\nFrom image-level to pixellevel labeling with convolutional networks. P H Pinheiro, R Collobert, CVPR. P. H. Pinheiro and R. Collobert. From image-level to pixel- level labeling with convolutional networks. In CVPR, 2015. 6\n\nBoosting object proposals: From Pascal to COCO. J Pont-Tuset, L Van Gool, ICCV. J. Pont-Tuset and L. Van Gool. Boosting object proposals: From Pascal to COCO. In ICCV, 2015. 7\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, NIPS. S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To- wards real-time object detection with region proposal net- works. In NIPS, 2015. 7\n\nLearning where to classify in multi-view semantic segmentation. H Riemenschneider, A B\u00f3dis-Szomor\u00fa, J Weissenberg, L Van Gool, ECCV. ii4H. Riemenschneider, A. B\u00f3dis-Szomor\u00fa, J. Weissenberg, and L. Van Gool. Learning where to classify in multi-view semantic segmentation. In ECCV. 2014. 4, ii\n\nHough regions for joining instance localization and segmentation. H Riemenschneider, S Sternig, M Donoser, P M Roth, H Bischof, ECCV. H. Riemenschneider, S. Sternig, M. Donoser, P. M. Roth, and H. Bischof. Hough regions for joining instance localization and segmentation. In ECCV, 2012. 7\n\nVision-based offline-online perception paradigm for autonomous driving. G Ros, S Ramos, M Granados, D Vazquez, A M Lopez, WACV. 7G. Ros, S. Ramos, M. Granados, D. Vazquez, and A. M. Lopez. Vision-based offline-online perception paradigm for autonomous driving. In WACV, 2015. 1, 4, 7, ii\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, IJCV. 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog- nition challenge. IJCV, 115(3):211-252, 2015. 1, i, ii\n\nFreeman. LabelMe: A database and web-based tool for image annotation. B C Russell, A Torralba, K P Murphy, W , IJCV. 771-3B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Free- man. LabelMe: A database and web-based tool for image annotation. IJCV, 77(1-3):157-173, 2008. 2, i\n\nEfficient multi-cue scene segmentation. T Scharw\u00e4chter, M Enzweiler, U Franke, S Roth, GCPR. 3T. Scharw\u00e4chter, M. Enzweiler, U. Franke, and S. Roth. Ef- ficient multi-cue scene segmentation. In GCPR, 2013. 1, 3, ii\n\nStixmantics: A medium-level model for real-time semantic scene understanding. T Scharw\u00e4chter, M Enzweiler, U Franke, S Roth, ECCV. 14T. Scharw\u00e4chter, M. Enzweiler, U. Franke, and S. Roth. Stix- mantics: A medium-level model for real-time semantic scene understanding. In ECCV, 2014. 1, 4\n\nA Schwing, R Urtasun, arXiv:1503.02351v1[cs.CVFully connected deep structured networks. 56A. Schwing and R. Urtasun. Fully connected deep structured networks. arXiv:1503.02351v1 [cs.CV], 2015. 5, 6\n\nSemantic modelling of urban scenes. S Sengupta, E Greveson, A Shahrokni, P H S Torr, ICRA. 7S. Sengupta, E. Greveson, A. Shahrokni, and P. H. S. Torr. Semantic modelling of urban scenes. In ICRA, 2013. 4, 7, ii\n\nAutomatic dense visual semantic mapping from street-level imagery. S Sengupta, P Sturgess, L Ladicky, P H S Torr, IROS. ii4S. Sengupta, P. Sturgess, L. Ladicky, and P. H. S. Torr. Au- tomatic dense visual semantic mapping from street-level im- agery. In IROS, 2012. 4, ii\n\nOverFeat: Integrated recognition, localization and detection using convolutional networks. P Sermanet, D Eigen, X Zhang, M Mathieu, R Fergus, Y Lecun, ICLR. P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. 1\n\nDeep hierarchical parsing for semantic segmentation. A Sharma, O Tuzel, D W Jacobs, CVPR. A. Sharma, O. Tuzel, and D. W. Jacobs. Deep hierarchical parsing for semantic segmentation. In CVPR, 2015. 6\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556v66cs.CVK. Simonyan and A. Zisserman. Very deep con- volutional networks for large-scale image recognition. arXiv:1409.1556v6 [cs.CV], 2014. 6, i\n\nSun RGB-D: A RGB-D scene understanding benchmark suite. S Song, S P Lichtenberg, J Xiao, CVPR. ii4S. Song, S. P. Lichtenberg, and J. Xiao. Sun RGB-D: A RGB-D scene understanding benchmark suite. In CVPR, 2015. 4, ii\n\n. J Tighe, S Lazebnik, Superparsing, IJCV. 1012J. Tighe and S. Lazebnik. Superparsing. IJCV, 101(2):329- 349, 2013. 4\n\nScene parsing with object instance inference using regions and perexemplar detectors. J Tighe, M Niethammer, S Lazebnik, IJCV. 11227J. Tighe, M. Niethammer, and S. Lazebnik. Scene pars- ing with object instance inference using regions and per- exemplar detectors. IJCV, 112(2):150-171, 2015. 1, 7\n\nSelective search for object recognition. J R R Uijlings, K E A Van De Sande, T Gevers, A W M Smeulders, International journal of computer vision. 1042J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders. Selective search for object recognition. International journal of computer vision, 104(2):154-171, 2013. 7\n\nIncremental dense semantic stereo fusion for large-scale semantic scene reconstruction. V Vineet, O Miksik, M Lidegaard, M Niessner, S Golodetz, V A Prisacariu, O Kahler, D W Murray, S Izadi, P Perez, P H S Torr, ICRA. 7V. Vineet, O. Miksik, M. Lidegaard, M. Niessner, S. Golodetz, V. A. Prisacariu, O. Kahler, D. W. Murray, S. Izadi, P. Perez, and P. H. S. Torr. Incremental dense se- mantic stereo fusion for large-scale semantic scene recon- struction. In ICRA, 2015. 7, 8\n\nSTC: A simple to complex framework for weakly-supervised semantic segmentation. Y Wei, X Liang, Y Chen, X Shen, M.-M Cheng, Y Zhao, S Yan, arXiv:1509.03150v1cs.CVY. Wei, X. Liang, Y. Chen, X. Shen, M.-M. Cheng, Y. Zhao, and S. Yan. STC: A simple to complex framework for weakly-supervised semantic segmentation. arXiv:1509.03150v1 [cs.CV], 2015. 6\n\nSemantic instance annotation of street scenes by 3D to 2D label transfer. J Xie, M Kiefel, M.-T Sun, A Geiger, CVPR. to appear. 2, 3, iiJ. Xie, M. Kiefel, M.-T. Sun, and A. Geiger. Semantic in- stance annotation of street scenes by 3D to 2D label transfer. In CVPR, 2016, to appear. 2, 3, ii\n\nLearning to segment under various forms of weak supervision. J Xu, A G Schwing, R Urtasun, CVPR. J. Xu, A. G. Schwing, and R. Urtasun. Learning to segment under various forms of weak supervision. In CVPR, 2015. 6\n\nInformation fusion on oversegmented images: An application for urban scene understanding. P Xu, F Davoine, J.-B Bordes, H Zhao, T Denoeux, MVA. ii4P. Xu, F. Davoine, J.-B. Bordes, H. Zhao, and T. Denoeux. Information fusion on oversegmented images: An applica- tion for urban scene understanding. In MVA, 2013. 4, ii\n\nDescribing the scene as a whole: Joint object detection, scene classification and semantic segmentation. J Yao, S Fidler, R Urtasun, CVPR. J. Yao, S. Fidler, and R. Urtasun. Describing the scene as a whole: Joint object detection, scene classification and se- mantic segmentation. In CVPR, 2012. 7\n\nMulti-scale context aggregation by dilated convolutions. F Yu, V Koltun, ICLR. i, ii, xii, xv, xviito appear. 6, 8F. Yu and V. Koltun. Multi-scale context aggregation by di- lated convolutions. In ICLR, 2016, to appear. 6, 8, i, ii, xii, xv, xvii\n\nMonocular object instance segmentation and depth ordering with CNNs. Z Zhang, A Schwing, S Fidler, R Urtasun, ICCV. 4Z. Zhang, A. Schwing, S. Fidler, and R. Urtasun. Monocular object instance segmentation and depth ordering with CNNs. In ICCV, 2015. 4, 7, ii\n\nS Zheng, S Jayasumana, B Romera-Paredes, V Vineet, Z Su, D Du, C Huang, P H S Torr, Conditional random fields as recurrent neural networks. In ICCV. 6, ii, xii, xv, xviiS. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. H. S. Torr. Conditional ran- dom fields as recurrent neural networks. In ICCV, 2015. 5, 6, ii, xii, xv, xvii\n\nLearning deep features for scene recognition using places database. B Zhou, A Lapedriza, J Xiao, A Torralba, A Oliva, NIPS. B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In NIPS, 2014. 1\n", "annotations": {"author": "[{\"end\":141,\"start\":65},{\"end\":218,\"start\":142},{\"end\":297,\"start\":219},{\"end\":373,\"start\":298},{\"end\":453,\"start\":374},{\"end\":533,\"start\":454},{\"end\":607,\"start\":534},{\"end\":682,\"start\":608},{\"end\":759,\"start\":683}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":72},{\"end\":155,\"start\":150},{\"end\":234,\"start\":229},{\"end\":310,\"start\":303},{\"end\":390,\"start\":381},{\"end\":470,\"start\":462},{\"end\":544,\"start\":538},{\"end\":619,\"start\":615},{\"end\":696,\"start\":689}]", "author_first_name": "[{\"end\":71,\"start\":65},{\"end\":149,\"start\":142},{\"end\":228,\"start\":219},{\"end\":302,\"start\":298},{\"end\":380,\"start\":374},{\"end\":461,\"start\":454},{\"end\":537,\"start\":534},{\"end\":614,\"start\":608},{\"end\":688,\"start\":683}]", "author_affiliation": "[{\"end\":140,\"start\":80},{\"end\":217,\"start\":157},{\"end\":296,\"start\":236},{\"end\":372,\"start\":312},{\"end\":452,\"start\":392},{\"end\":532,\"start\":472},{\"end\":606,\"start\":546},{\"end\":681,\"start\":621},{\"end\":758,\"start\":698}]", "title": "[{\"end\":62,\"start\":1},{\"end\":821,\"start\":760}]", "venue": null, "abstract": "[{\"end\":2109,\"start\":964}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2241,\"start\":2237},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2474,\"start\":2470},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":2477,\"start\":2474},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2672,\"start\":2668},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":2675,\"start\":2672},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2828,\"start\":2825},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2831,\"start\":2828},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2834,\"start\":2831},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2837,\"start\":2834},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3015,\"start\":3011},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3220,\"start\":3216},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3223,\"start\":3220},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":3226,\"start\":3223},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":3358,\"start\":3354},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3375,\"start\":3371},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3396,\"start\":3392},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3421,\"start\":3417},{\"end\":3949,\"start\":3931},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4084,\"start\":4080},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4096,\"start\":4093},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4109,\"start\":4105},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":4146,\"start\":4142},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":5171,\"start\":5167},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6555,\"start\":6551},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7041,\"start\":7037},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8590,\"start\":8586},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9060,\"start\":9057},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":9070,\"start\":9066},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9086,\"start\":9082},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10653,\"start\":10650},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10656,\"start\":10653},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":10659,\"start\":10656},{\"end\":11478,\"start\":11477},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12175,\"start\":12171},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13518,\"start\":13515},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":13528,\"start\":13524},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13544,\"start\":13540},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13627,\"start\":13624},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13630,\"start\":13627},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":13633,\"start\":13630},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":13636,\"start\":13633},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":13639,\"start\":13636},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14130,\"start\":14126},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14133,\"start\":14130},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14136,\"start\":14133},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14139,\"start\":14136},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":14142,\"start\":14139},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":14145,\"start\":14142},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":14148,\"start\":14145},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":14151,\"start\":14148},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16154,\"start\":16150},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17069,\"start\":17065},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17089,\"start\":17085},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18211,\"start\":18207},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20703,\"start\":20699},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22873,\"start\":22869},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22897,\"start\":22893},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23093,\"start\":23089},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23268,\"start\":23265},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23271,\"start\":23268},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23274,\"start\":23271},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":23277,\"start\":23274},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":23280,\"start\":23277},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23555,\"start\":23552},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23558,\"start\":23555},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23561,\"start\":23558},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":23564,\"start\":23561},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23604,\"start\":23601},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23607,\"start\":23604},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":23722,\"start\":23718},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23764,\"start\":23761},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23767,\"start\":23764},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23771,\"start\":23767},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23775,\"start\":23771},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23779,\"start\":23775},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23782,\"start\":23779},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":23785,\"start\":23782},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":23788,\"start\":23785},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24165,\"start\":24162},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24168,\"start\":24165},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24171,\"start\":24168},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":24174,\"start\":24171},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":24177,\"start\":24174},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":24200,\"start\":24196},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24242,\"start\":24238},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24364,\"start\":24360},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25353,\"start\":25349},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25369,\"start\":25365},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25456,\"start\":25452},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25518,\"start\":25514},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26457,\"start\":26454},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26460,\"start\":26457},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":26463,\"start\":26460},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":26466,\"start\":26463},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26566,\"start\":26563},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26581,\"start\":26577},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26587,\"start\":26583},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":26756,\"start\":26752},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26772,\"start\":26768},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":26947,\"start\":26943},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26965,\"start\":26962},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27193,\"start\":27189},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27196,\"start\":27193},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27199,\"start\":27196},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27565,\"start\":27562},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27595,\"start\":27591},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":27598,\"start\":27595},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27876,\"start\":27873},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":27879,\"start\":27876},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28993,\"start\":28989},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29132,\"start\":29128},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29803,\"start\":29799},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":29806,\"start\":29803},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30192,\"start\":30188},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30309,\"start\":30305},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30312,\"start\":30309},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":30385,\"start\":30381},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30396,\"start\":30393},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30687,\"start\":30683},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30690,\"start\":30687},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":30721,\"start\":30717},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30937,\"start\":30933},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30940,\"start\":30937},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31039,\"start\":31035},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31042,\"start\":31039},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31045,\"start\":31042},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":31048,\"start\":31045},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":31051,\"start\":31048},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":31054,\"start\":31051},{\"end\":31427,\"start\":31419},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":31567,\"start\":31563},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31582,\"start\":31578},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":31602,\"start\":31598},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31936,\"start\":31932},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32000,\"start\":31997},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34460,\"start\":34456},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":34910,\"start\":34906},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":35680,\"start\":35676},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":36963,\"start\":36959},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":37233,\"start\":37229},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":37354,\"start\":37350},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":37437,\"start\":37433},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":37674,\"start\":37670},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":37690,\"start\":37686},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":38092,\"start\":38088},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":38451,\"start\":38447},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39010,\"start\":39006},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":39013,\"start\":39010},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":39016,\"start\":39013},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39019,\"start\":39016},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":39022,\"start\":39019},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":39025,\"start\":39022},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":39028,\"start\":39025},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":39031,\"start\":39028},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39513,\"start\":39510},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":39673,\"start\":39669},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":39795,\"start\":39791},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":40167,\"start\":40164},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":40590,\"start\":40586},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":40884,\"start\":40880},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":41051,\"start\":41047},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":42535,\"start\":42531},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":42611,\"start\":42608},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":42777,\"start\":42773},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":44371,\"start\":44370},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":44568,\"start\":44567},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":45000,\"start\":44998},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":45001,\"start\":45000},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":45700,\"start\":45699},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":45740,\"start\":45739},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46334,\"start\":46333},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46374,\"start\":46373},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":46429,\"start\":46425},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":46685,\"start\":46682},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":46694,\"start\":46690},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":46710,\"start\":46706},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":46743,\"start\":46739},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":46757,\"start\":46753},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":46772,\"start\":46768}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47240,\"start\":47133},{\"attributes\":{\"id\":\"fig_1\"},\"end\":47377,\"start\":47241},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47572,\"start\":47378},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47884,\"start\":47573},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47981,\"start\":47885},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48396,\"start\":47982},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48458,\"start\":48397},{\"attributes\":{\"id\":\"fig_7\"},\"end\":48612,\"start\":48459},{\"attributes\":{\"id\":\"fig_9\"},\"end\":48838,\"start\":48613},{\"attributes\":{\"id\":\"fig_10\"},\"end\":49266,\"start\":48839},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":49579,\"start\":49267},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":51275,\"start\":49580},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":51639,\"start\":51276},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":51730,\"start\":51640},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":51832,\"start\":51731},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":51935,\"start\":51833},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":52039,\"start\":51936},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":52144,\"start\":52040},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":52247,\"start\":52145},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":52916,\"start\":52248},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":54195,\"start\":52917},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":55068,\"start\":54196},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":55940,\"start\":55069},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":56239,\"start\":55941}]", "paragraph": "[{\"end\":2975,\"start\":2125},{\"end\":3486,\"start\":2977},{\"end\":5043,\"start\":3488},{\"end\":5721,\"start\":5045},{\"end\":6018,\"start\":5733},{\"end\":6556,\"start\":6042},{\"end\":7168,\"start\":6558},{\"end\":8152,\"start\":7170},{\"end\":8366,\"start\":8154},{\"end\":8504,\"start\":8394},{\"end\":9087,\"start\":8506},{\"end\":9565,\"start\":9089},{\"end\":10279,\"start\":9567},{\"end\":10850,\"start\":10281},{\"end\":13211,\"start\":10869},{\"end\":17806,\"start\":13236},{\"end\":17989,\"start\":17828},{\"end\":18578,\"start\":18011},{\"end\":19859,\"start\":18580},{\"end\":20633,\"start\":19883},{\"end\":21004,\"start\":20635},{\"end\":22154,\"start\":21006},{\"end\":22560,\"start\":22156},{\"end\":23055,\"start\":22581},{\"end\":23382,\"start\":23057},{\"end\":23723,\"start\":23384},{\"end\":24007,\"start\":23725},{\"end\":24875,\"start\":24021},{\"end\":25605,\"start\":24877},{\"end\":26704,\"start\":25607},{\"end\":27200,\"start\":26706},{\"end\":27382,\"start\":27202},{\"end\":28234,\"start\":27411},{\"end\":28663,\"start\":28271},{\"end\":29720,\"start\":28685},{\"end\":30084,\"start\":29741},{\"end\":30529,\"start\":30086},{\"end\":30813,\"start\":30531},{\"end\":31269,\"start\":30815},{\"end\":31783,\"start\":31307},{\"end\":33236,\"start\":31785},{\"end\":33906,\"start\":33263},{\"end\":35017,\"start\":33908},{\"end\":35511,\"start\":35041},{\"end\":36036,\"start\":35536},{\"end\":36295,\"start\":36038},{\"end\":36828,\"start\":36344},{\"end\":38013,\"start\":36855},{\"end\":39417,\"start\":38015},{\"end\":39501,\"start\":39419},{\"end\":39663,\"start\":39503},{\"end\":39765,\"start\":39665},{\"end\":40035,\"start\":39767},{\"end\":40869,\"start\":40037},{\"end\":41005,\"start\":40871},{\"end\":42391,\"start\":41007},{\"end\":42650,\"start\":42433},{\"end\":42880,\"start\":42652},{\"end\":43279,\"start\":42882},{\"end\":43323,\"start\":43289},{\"end\":43627,\"start\":43325},{\"end\":44029,\"start\":43629},{\"end\":44044,\"start\":44031},{\"end\":44233,\"start\":44046},{\"end\":44364,\"start\":44242},{\"end\":44463,\"start\":44366},{\"end\":44525,\"start\":44465},{\"end\":44720,\"start\":44527},{\"end\":44831,\"start\":44722},{\"end\":44988,\"start\":44844},{\"end\":45142,\"start\":44990},{\"end\":45363,\"start\":45164},{\"end\":45939,\"start\":45365},{\"end\":46005,\"start\":45941},{\"end\":46162,\"start\":46007},{\"end\":47132,\"start\":46189}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":44241,\"start\":44234},{\"attributes\":{\"id\":\"formula_1\"},\"end\":44843,\"start\":44832}]", "table_ref": "[{\"end\":31667,\"start\":31660},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":35311,\"start\":35304},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":36698,\"start\":36682},{\"end\":39039,\"start\":39032},{\"attributes\":{\"ref_id\":\"tab_19\"},\"end\":43230,\"start\":43215},{\"end\":46437,\"start\":46430}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2123,\"start\":2111},{\"attributes\":{\"n\":\"2.\"},\"end\":5731,\"start\":5724},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6040,\"start\":6021},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8392,\"start\":8369},{\"attributes\":{\"n\":\"2.3.\"},\"end\":10867,\"start\":10853},{\"attributes\":{\"n\":\"2.4.\"},\"end\":13234,\"start\":13214},{\"attributes\":{\"n\":\"3.\"},\"end\":17826,\"start\":17809},{\"attributes\":{\"n\":\"3.1.\"},\"end\":18009,\"start\":17992},{\"attributes\":{\"n\":\"3.2.\"},\"end\":19881,\"start\":19862},{\"attributes\":{\"n\":\"3.3.\"},\"end\":22579,\"start\":22563},{\"attributes\":{\"n\":\"3.4.\"},\"end\":24019,\"start\":24010},{\"attributes\":{\"n\":\"3.5.\"},\"end\":27409,\"start\":27385},{\"attributes\":{\"n\":\"4.\"},\"end\":28269,\"start\":28237},{\"attributes\":{\"n\":\"4.1.\"},\"end\":28683,\"start\":28666},{\"attributes\":{\"n\":\"4.2.\"},\"end\":29739,\"start\":29723},{\"attributes\":{\"n\":\"4.3.\"},\"end\":31305,\"start\":31272},{\"attributes\":{\"n\":\"5.\"},\"end\":33261,\"start\":33239},{\"end\":35039,\"start\":35020},{\"end\":35534,\"start\":35514},{\"end\":36320,\"start\":36298},{\"end\":36342,\"start\":36323},{\"end\":36853,\"start\":36831},{\"end\":42431,\"start\":42394},{\"end\":43287,\"start\":43282},{\"end\":45162,\"start\":45145},{\"end\":46187,\"start\":46165},{\"end\":47144,\"start\":47134},{\"end\":47250,\"start\":47242},{\"end\":47399,\"start\":47379},{\"end\":47582,\"start\":47574},{\"end\":48470,\"start\":48460},{\"end\":48624,\"start\":48614},{\"end\":48861,\"start\":48840},{\"end\":51650,\"start\":51641},{\"end\":51741,\"start\":51732},{\"end\":51843,\"start\":51834},{\"end\":51946,\"start\":51937},{\"end\":52050,\"start\":52041},{\"end\":52155,\"start\":52146},{\"end\":52259,\"start\":52249},{\"end\":52928,\"start\":52918},{\"end\":54207,\"start\":54197},{\"end\":55080,\"start\":55070},{\"end\":55952,\"start\":55942}]", "table": "[{\"end\":49579,\"start\":49298},{\"end\":51275,\"start\":50103},{\"end\":51639,\"start\":51279},{\"end\":51730,\"start\":51725},{\"end\":51832,\"start\":51828},{\"end\":51935,\"start\":51930},{\"end\":52039,\"start\":52033},{\"end\":52144,\"start\":52137},{\"end\":52247,\"start\":52242},{\"end\":52916,\"start\":52911},{\"end\":54195,\"start\":53628},{\"end\":55068,\"start\":54492},{\"end\":55940,\"start\":55362},{\"end\":56239,\"start\":56232}]", "figure_caption": "[{\"end\":47240,\"start\":47146},{\"end\":47377,\"start\":47252},{\"end\":47572,\"start\":47402},{\"end\":47884,\"start\":47584},{\"end\":47981,\"start\":47887},{\"end\":48396,\"start\":47984},{\"end\":48458,\"start\":48399},{\"end\":48612,\"start\":48472},{\"end\":48838,\"start\":48626},{\"end\":49266,\"start\":48865},{\"end\":49298,\"start\":49269},{\"end\":50103,\"start\":49582},{\"end\":51279,\"start\":51278},{\"end\":51725,\"start\":51652},{\"end\":51828,\"start\":51743},{\"end\":51930,\"start\":51845},{\"end\":52033,\"start\":51948},{\"end\":52137,\"start\":52052},{\"end\":52242,\"start\":52157},{\"end\":52911,\"start\":52262},{\"end\":53628,\"start\":52931},{\"end\":54492,\"start\":54210},{\"end\":55362,\"start\":55083},{\"end\":56232,\"start\":55955}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10765,\"start\":10759},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15112,\"start\":15097},{\"end\":17258,\"start\":17252},{\"end\":17499,\"start\":17493},{\"end\":25213,\"start\":25207},{\"end\":26051,\"start\":26045},{\"end\":26514,\"start\":26508},{\"end\":26942,\"start\":26936},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31659,\"start\":31653},{\"end\":35692,\"start\":35686},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36781,\"start\":36768},{\"end\":36978,\"start\":36970},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43266,\"start\":43258},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46994,\"start\":46985}]", "bib_author_first_name": "[{\"end\":56277,\"start\":56276},{\"end\":56289,\"start\":56288},{\"end\":56303,\"start\":56302},{\"end\":56313,\"start\":56312},{\"end\":56324,\"start\":56323},{\"end\":56500,\"start\":56499},{\"end\":56512,\"start\":56511},{\"end\":56514,\"start\":56513},{\"end\":56532,\"start\":56531},{\"end\":56647,\"start\":56646},{\"end\":56656,\"start\":56655},{\"end\":56670,\"start\":56669},{\"end\":56679,\"start\":56678},{\"end\":56683,\"start\":56680},{\"end\":57015,\"start\":57014},{\"end\":57033,\"start\":57032},{\"end\":57044,\"start\":57043},{\"end\":57359,\"start\":57358},{\"end\":57370,\"start\":57369},{\"end\":57385,\"start\":57384},{\"end\":57396,\"start\":57395},{\"end\":57636,\"start\":57635},{\"end\":57648,\"start\":57647},{\"end\":57659,\"start\":57658},{\"end\":57670,\"start\":57669},{\"end\":57882,\"start\":57881},{\"end\":57884,\"start\":57883},{\"end\":57895,\"start\":57894},{\"end\":57907,\"start\":57906},{\"end\":58185,\"start\":58184},{\"end\":58194,\"start\":58193},{\"end\":58196,\"start\":58195},{\"end\":58206,\"start\":58205},{\"end\":58214,\"start\":58213},{\"end\":58436,\"start\":58432},{\"end\":58444,\"start\":58443},{\"end\":58458,\"start\":58457},{\"end\":58470,\"start\":58469},{\"end\":58480,\"start\":58479},{\"end\":58482,\"start\":58481},{\"end\":58761,\"start\":58757},{\"end\":58769,\"start\":58768},{\"end\":58779,\"start\":58775},{\"end\":58978,\"start\":58977},{\"end\":58985,\"start\":58984},{\"end\":58991,\"start\":58990},{\"end\":59186,\"start\":59185},{\"end\":59196,\"start\":59195},{\"end\":59205,\"start\":59204},{\"end\":59216,\"start\":59215},{\"end\":59448,\"start\":59447},{\"end\":59461,\"start\":59460},{\"end\":59463,\"start\":59462},{\"end\":59654,\"start\":59653},{\"end\":59668,\"start\":59667},{\"end\":59672,\"start\":59669},{\"end\":59682,\"start\":59681},{\"end\":59694,\"start\":59693},{\"end\":59698,\"start\":59695},{\"end\":59710,\"start\":59709},{\"end\":59718,\"start\":59717},{\"end\":60005,\"start\":60004},{\"end\":60007,\"start\":60006},{\"end\":60023,\"start\":60022},{\"end\":60025,\"start\":60024},{\"end\":60037,\"start\":60036},{\"end\":60051,\"start\":60050},{\"end\":60273,\"start\":60272},{\"end\":60283,\"start\":60282},{\"end\":60295,\"start\":60294},{\"end\":60303,\"start\":60302},{\"end\":60314,\"start\":60313},{\"end\":60327,\"start\":60326},{\"end\":60336,\"start\":60335},{\"end\":60338,\"start\":60337},{\"end\":60607,\"start\":60606},{\"end\":60618,\"start\":60617},{\"end\":60633,\"start\":60632},{\"end\":60642,\"start\":60641},{\"end\":60655,\"start\":60654},{\"end\":60667,\"start\":60666},{\"end\":60682,\"start\":60681},{\"end\":60697,\"start\":60696},{\"end\":61014,\"start\":61013},{\"end\":61024,\"start\":61023},{\"end\":61033,\"start\":61032},{\"end\":61042,\"start\":61041},{\"end\":61053,\"start\":61052},{\"end\":61233,\"start\":61232},{\"end\":61243,\"start\":61242},{\"end\":61251,\"start\":61250},{\"end\":61262,\"start\":61261},{\"end\":61458,\"start\":61457},{\"end\":61608,\"start\":61607},{\"end\":61620,\"start\":61619},{\"end\":61631,\"start\":61630},{\"end\":61642,\"start\":61641},{\"end\":61873,\"start\":61872},{\"end\":61883,\"start\":61882},{\"end\":62055,\"start\":62054},{\"end\":62068,\"start\":62067},{\"end\":62080,\"start\":62079},{\"end\":62082,\"start\":62081},{\"end\":62094,\"start\":62093},{\"end\":62295,\"start\":62294},{\"end\":62308,\"start\":62307},{\"end\":62310,\"start\":62309},{\"end\":62322,\"start\":62321},{\"end\":62324,\"start\":62323},{\"end\":62336,\"start\":62335},{\"end\":62563,\"start\":62562},{\"end\":62574,\"start\":62573},{\"end\":62576,\"start\":62575},{\"end\":62587,\"start\":62586},{\"end\":62589,\"start\":62588},{\"end\":62599,\"start\":62598},{\"end\":62814,\"start\":62813},{\"end\":62820,\"start\":62819},{\"end\":62938,\"start\":62937},{\"end\":62947,\"start\":62946},{\"end\":62955,\"start\":62954},{\"end\":62963,\"start\":62962},{\"end\":63158,\"start\":63157},{\"end\":63168,\"start\":63167},{\"end\":63180,\"start\":63179},{\"end\":63190,\"start\":63189},{\"end\":63409,\"start\":63408},{\"end\":63415,\"start\":63414},{\"end\":63604,\"start\":63603},{\"end\":63618,\"start\":63617},{\"end\":63631,\"start\":63630},{\"end\":63633,\"start\":63632},{\"end\":63832,\"start\":63831},{\"end\":63842,\"start\":63841},{\"end\":63852,\"start\":63851},{\"end\":63867,\"start\":63866},{\"end\":64207,\"start\":64206},{\"end\":64216,\"start\":64215},{\"end\":64222,\"start\":64221},{\"end\":64234,\"start\":64233},{\"end\":64240,\"start\":64239},{\"end\":64438,\"start\":64437},{\"end\":64449,\"start\":64448},{\"end\":64456,\"start\":64455},{\"end\":64588,\"start\":64587},{\"end\":64597,\"start\":64596},{\"end\":64607,\"start\":64606},{\"end\":64743,\"start\":64742},{\"end\":64752,\"start\":64751},{\"end\":64764,\"start\":64763},{\"end\":64776,\"start\":64775},{\"end\":64997,\"start\":64996},{\"end\":65006,\"start\":65005},{\"end\":65019,\"start\":65018},{\"end\":65264,\"start\":65263},{\"end\":65271,\"start\":65270},{\"end\":65279,\"start\":65278},{\"end\":65297,\"start\":65296},{\"end\":65583,\"start\":65579},{\"end\":65590,\"start\":65589},{\"end\":65599,\"start\":65598},{\"end\":65611,\"start\":65610},{\"end\":65619,\"start\":65618},{\"end\":65629,\"start\":65628},{\"end\":65640,\"start\":65639},{\"end\":65650,\"start\":65649},{\"end\":65652,\"start\":65651},{\"end\":65844,\"start\":65843},{\"end\":65851,\"start\":65850},{\"end\":65865,\"start\":65864},{\"end\":65867,\"start\":65866},{\"end\":66106,\"start\":66105},{\"end\":66113,\"start\":66112},{\"end\":66119,\"start\":66118},{\"end\":66126,\"start\":66125},{\"end\":66128,\"start\":66127},{\"end\":66135,\"start\":66134},{\"end\":66368,\"start\":66367},{\"end\":66376,\"start\":66375},{\"end\":66389,\"start\":66388},{\"end\":66645,\"start\":66644},{\"end\":66654,\"start\":66653},{\"end\":66656,\"start\":66655},{\"end\":66662,\"start\":66661},{\"end\":66882,\"start\":66881},{\"end\":66891,\"start\":66890},{\"end\":66906,\"start\":66905},{\"end\":67114,\"start\":67113},{\"end\":67127,\"start\":67126},{\"end\":67143,\"start\":67142},{\"end\":67377,\"start\":67376},{\"end\":67389,\"start\":67388},{\"end\":67397,\"start\":67396},{\"end\":67407,\"start\":67403},{\"end\":67417,\"start\":67413},{\"end\":67424,\"start\":67423},{\"end\":67434,\"start\":67433},{\"end\":67445,\"start\":67444},{\"end\":67750,\"start\":67749},{\"end\":67759,\"start\":67758},{\"end\":68016,\"start\":68015},{\"end\":68025,\"start\":68024},{\"end\":68035,\"start\":68034},{\"end\":68045,\"start\":68044},{\"end\":68303,\"start\":68302},{\"end\":68320,\"start\":68316},{\"end\":68328,\"start\":68327},{\"end\":68338,\"start\":68337},{\"end\":68340,\"start\":68339},{\"end\":68640,\"start\":68639},{\"end\":68650,\"start\":68649},{\"end\":68666,\"start\":68665},{\"end\":68887,\"start\":68886},{\"end\":68897,\"start\":68896},{\"end\":68910,\"start\":68909},{\"end\":68918,\"start\":68917},{\"end\":69109,\"start\":69108},{\"end\":69121,\"start\":69120},{\"end\":69123,\"start\":69122},{\"end\":69133,\"start\":69132},{\"end\":69318,\"start\":69317},{\"end\":69320,\"start\":69319},{\"end\":69332,\"start\":69331},{\"end\":69530,\"start\":69529},{\"end\":69532,\"start\":69531},{\"end\":69544,\"start\":69543},{\"end\":69733,\"start\":69732},{\"end\":69747,\"start\":69746},{\"end\":69942,\"start\":69941},{\"end\":69949,\"start\":69948},{\"end\":69955,\"start\":69954},{\"end\":69967,\"start\":69966},{\"end\":70186,\"start\":70185},{\"end\":70205,\"start\":70204},{\"end\":70222,\"start\":70221},{\"end\":70237,\"start\":70236},{\"end\":70481,\"start\":70480},{\"end\":70500,\"start\":70499},{\"end\":70511,\"start\":70510},{\"end\":70522,\"start\":70521},{\"end\":70524,\"start\":70523},{\"end\":70532,\"start\":70531},{\"end\":70777,\"start\":70776},{\"end\":70784,\"start\":70783},{\"end\":70793,\"start\":70792},{\"end\":70805,\"start\":70804},{\"end\":70816,\"start\":70815},{\"end\":70818,\"start\":70817},{\"end\":71045,\"start\":71044},{\"end\":71060,\"start\":71059},{\"end\":71068,\"start\":71067},{\"end\":71074,\"start\":71073},{\"end\":71084,\"start\":71083},{\"end\":71096,\"start\":71095},{\"end\":71102,\"start\":71101},{\"end\":71111,\"start\":71110},{\"end\":71123,\"start\":71122},{\"end\":71133,\"start\":71132},{\"end\":71146,\"start\":71145},{\"end\":71148,\"start\":71147},{\"end\":71156,\"start\":71155},{\"end\":71477,\"start\":71476},{\"end\":71479,\"start\":71478},{\"end\":71490,\"start\":71489},{\"end\":71502,\"start\":71501},{\"end\":71504,\"start\":71503},{\"end\":71514,\"start\":71513},{\"end\":71728,\"start\":71727},{\"end\":71744,\"start\":71743},{\"end\":71757,\"start\":71756},{\"end\":71767,\"start\":71766},{\"end\":71982,\"start\":71981},{\"end\":71998,\"start\":71997},{\"end\":72011,\"start\":72010},{\"end\":72021,\"start\":72020},{\"end\":72193,\"start\":72192},{\"end\":72204,\"start\":72203},{\"end\":72428,\"start\":72427},{\"end\":72440,\"start\":72439},{\"end\":72452,\"start\":72451},{\"end\":72465,\"start\":72464},{\"end\":72469,\"start\":72466},{\"end\":72671,\"start\":72670},{\"end\":72683,\"start\":72682},{\"end\":72695,\"start\":72694},{\"end\":72706,\"start\":72705},{\"end\":72710,\"start\":72707},{\"end\":72968,\"start\":72967},{\"end\":72980,\"start\":72979},{\"end\":72989,\"start\":72988},{\"end\":72998,\"start\":72997},{\"end\":73009,\"start\":73008},{\"end\":73019,\"start\":73018},{\"end\":73266,\"start\":73265},{\"end\":73276,\"start\":73275},{\"end\":73285,\"start\":73284},{\"end\":73287,\"start\":73286},{\"end\":73481,\"start\":73480},{\"end\":73493,\"start\":73492},{\"end\":73724,\"start\":73723},{\"end\":73732,\"start\":73731},{\"end\":73734,\"start\":73733},{\"end\":73749,\"start\":73748},{\"end\":73887,\"start\":73886},{\"end\":73896,\"start\":73895},{\"end\":74090,\"start\":74089},{\"end\":74099,\"start\":74098},{\"end\":74113,\"start\":74112},{\"end\":74343,\"start\":74342},{\"end\":74347,\"start\":74344},{\"end\":74359,\"start\":74358},{\"end\":74363,\"start\":74360},{\"end\":74379,\"start\":74378},{\"end\":74389,\"start\":74388},{\"end\":74393,\"start\":74390},{\"end\":74725,\"start\":74724},{\"end\":74735,\"start\":74734},{\"end\":74745,\"start\":74744},{\"end\":74758,\"start\":74757},{\"end\":74770,\"start\":74769},{\"end\":74782,\"start\":74781},{\"end\":74784,\"start\":74783},{\"end\":74798,\"start\":74797},{\"end\":74808,\"start\":74807},{\"end\":74810,\"start\":74809},{\"end\":74820,\"start\":74819},{\"end\":74829,\"start\":74828},{\"end\":74838,\"start\":74837},{\"end\":74842,\"start\":74839},{\"end\":75194,\"start\":75193},{\"end\":75201,\"start\":75200},{\"end\":75210,\"start\":75209},{\"end\":75218,\"start\":75217},{\"end\":75229,\"start\":75225},{\"end\":75238,\"start\":75237},{\"end\":75246,\"start\":75245},{\"end\":75537,\"start\":75536},{\"end\":75544,\"start\":75543},{\"end\":75557,\"start\":75553},{\"end\":75564,\"start\":75563},{\"end\":75817,\"start\":75816},{\"end\":75823,\"start\":75822},{\"end\":75825,\"start\":75824},{\"end\":75836,\"start\":75835},{\"end\":76060,\"start\":76059},{\"end\":76066,\"start\":76065},{\"end\":76080,\"start\":76076},{\"end\":76090,\"start\":76089},{\"end\":76098,\"start\":76097},{\"end\":76393,\"start\":76392},{\"end\":76400,\"start\":76399},{\"end\":76410,\"start\":76409},{\"end\":76644,\"start\":76643},{\"end\":76650,\"start\":76649},{\"end\":76904,\"start\":76903},{\"end\":76913,\"start\":76912},{\"end\":76924,\"start\":76923},{\"end\":76934,\"start\":76933},{\"end\":77095,\"start\":77094},{\"end\":77104,\"start\":77103},{\"end\":77118,\"start\":77117},{\"end\":77136,\"start\":77135},{\"end\":77146,\"start\":77145},{\"end\":77152,\"start\":77151},{\"end\":77158,\"start\":77157},{\"end\":77167,\"start\":77166},{\"end\":77171,\"start\":77168},{\"end\":77528,\"start\":77527},{\"end\":77536,\"start\":77535},{\"end\":77549,\"start\":77548},{\"end\":77557,\"start\":77556},{\"end\":77569,\"start\":77568}]", "bib_author_last_name": "[{\"end\":56286,\"start\":56278},{\"end\":56300,\"start\":56290},{\"end\":56310,\"start\":56304},{\"end\":56321,\"start\":56314},{\"end\":56330,\"start\":56325},{\"end\":56509,\"start\":56501},{\"end\":56529,\"start\":56515},{\"end\":56537,\"start\":56533},{\"end\":56653,\"start\":56648},{\"end\":56667,\"start\":56657},{\"end\":56676,\"start\":56671},{\"end\":56688,\"start\":56684},{\"end\":57030,\"start\":57016},{\"end\":57041,\"start\":57034},{\"end\":57052,\"start\":57045},{\"end\":57367,\"start\":57360},{\"end\":57382,\"start\":57371},{\"end\":57393,\"start\":57386},{\"end\":57404,\"start\":57397},{\"end\":57645,\"start\":57637},{\"end\":57656,\"start\":57649},{\"end\":57667,\"start\":57660},{\"end\":57679,\"start\":57671},{\"end\":57892,\"start\":57885},{\"end\":57904,\"start\":57896},{\"end\":57915,\"start\":57908},{\"end\":58191,\"start\":58186},{\"end\":58203,\"start\":58197},{\"end\":58211,\"start\":58207},{\"end\":58222,\"start\":58215},{\"end\":58441,\"start\":58437},{\"end\":58455,\"start\":58445},{\"end\":58467,\"start\":58459},{\"end\":58477,\"start\":58471},{\"end\":58489,\"start\":58483},{\"end\":58766,\"start\":58762},{\"end\":58773,\"start\":58770},{\"end\":58784,\"start\":58780},{\"end\":58982,\"start\":58979},{\"end\":58988,\"start\":58986},{\"end\":58995,\"start\":58992},{\"end\":59193,\"start\":59187},{\"end\":59202,\"start\":59197},{\"end\":59213,\"start\":59206},{\"end\":59223,\"start\":59217},{\"end\":59458,\"start\":59449},{\"end\":59471,\"start\":59464},{\"end\":59665,\"start\":59655},{\"end\":59679,\"start\":59673},{\"end\":59691,\"start\":59683},{\"end\":59707,\"start\":59699},{\"end\":59715,\"start\":59711},{\"end\":59728,\"start\":59719},{\"end\":60020,\"start\":60008},{\"end\":60034,\"start\":60026},{\"end\":60048,\"start\":60038},{\"end\":60059,\"start\":60052},{\"end\":60280,\"start\":60274},{\"end\":60292,\"start\":60284},{\"end\":60300,\"start\":60296},{\"end\":60311,\"start\":60304},{\"end\":60324,\"start\":60315},{\"end\":60333,\"start\":60328},{\"end\":60348,\"start\":60339},{\"end\":60615,\"start\":60608},{\"end\":60630,\"start\":60619},{\"end\":60639,\"start\":60634},{\"end\":60652,\"start\":60643},{\"end\":60664,\"start\":60656},{\"end\":60679,\"start\":60668},{\"end\":60694,\"start\":60683},{\"end\":60700,\"start\":60698},{\"end\":61021,\"start\":61015},{\"end\":61030,\"start\":61025},{\"end\":61039,\"start\":61034},{\"end\":61050,\"start\":61043},{\"end\":61061,\"start\":61054},{\"end\":61240,\"start\":61234},{\"end\":61248,\"start\":61244},{\"end\":61259,\"start\":61252},{\"end\":61270,\"start\":61263},{\"end\":61467,\"start\":61459},{\"end\":61617,\"start\":61609},{\"end\":61628,\"start\":61621},{\"end\":61639,\"start\":61632},{\"end\":61648,\"start\":61643},{\"end\":61880,\"start\":61874},{\"end\":61890,\"start\":61884},{\"end\":62065,\"start\":62056},{\"end\":62077,\"start\":62069},{\"end\":62091,\"start\":62083},{\"end\":62100,\"start\":62095},{\"end\":62305,\"start\":62296},{\"end\":62319,\"start\":62311},{\"end\":62333,\"start\":62325},{\"end\":62342,\"start\":62337},{\"end\":62571,\"start\":62564},{\"end\":62584,\"start\":62577},{\"end\":62596,\"start\":62590},{\"end\":62606,\"start\":62600},{\"end\":62817,\"start\":62815},{\"end\":62826,\"start\":62821},{\"end\":62944,\"start\":62939},{\"end\":62952,\"start\":62948},{\"end\":62960,\"start\":62956},{\"end\":62970,\"start\":62964},{\"end\":63165,\"start\":63159},{\"end\":63177,\"start\":63169},{\"end\":63187,\"start\":63181},{\"end\":63198,\"start\":63191},{\"end\":63412,\"start\":63410},{\"end\":63423,\"start\":63416},{\"end\":63615,\"start\":63605},{\"end\":63628,\"start\":63619},{\"end\":63640,\"start\":63634},{\"end\":63839,\"start\":63833},{\"end\":63849,\"start\":63843},{\"end\":63864,\"start\":63853},{\"end\":63873,\"start\":63868},{\"end\":64213,\"start\":64208},{\"end\":64219,\"start\":64217},{\"end\":64231,\"start\":64223},{\"end\":64237,\"start\":64235},{\"end\":64245,\"start\":64241},{\"end\":64446,\"start\":64439},{\"end\":64453,\"start\":64450},{\"end\":64466,\"start\":64457},{\"end\":64594,\"start\":64589},{\"end\":64604,\"start\":64598},{\"end\":64614,\"start\":64608},{\"end\":64749,\"start\":64744},{\"end\":64761,\"start\":64753},{\"end\":64773,\"start\":64765},{\"end\":64785,\"start\":64777},{\"end\":65003,\"start\":64998},{\"end\":65016,\"start\":65007},{\"end\":65027,\"start\":65020},{\"end\":65268,\"start\":65265},{\"end\":65276,\"start\":65272},{\"end\":65294,\"start\":65280},{\"end\":65302,\"start\":65298},{\"end\":65587,\"start\":65584},{\"end\":65596,\"start\":65591},{\"end\":65608,\"start\":65600},{\"end\":65616,\"start\":65612},{\"end\":65626,\"start\":65620},{\"end\":65637,\"start\":65630},{\"end\":65647,\"start\":65641},{\"end\":65660,\"start\":65653},{\"end\":65848,\"start\":65845},{\"end\":65862,\"start\":65852},{\"end\":65872,\"start\":65868},{\"end\":66110,\"start\":66107},{\"end\":66116,\"start\":66114},{\"end\":66123,\"start\":66120},{\"end\":66132,\"start\":66129},{\"end\":66140,\"start\":66136},{\"end\":66373,\"start\":66369},{\"end\":66386,\"start\":66377},{\"end\":66397,\"start\":66390},{\"end\":66651,\"start\":66646},{\"end\":66659,\"start\":66657},{\"end\":66669,\"start\":66663},{\"end\":66888,\"start\":66883},{\"end\":66903,\"start\":66892},{\"end\":66913,\"start\":66907},{\"end\":67124,\"start\":67115},{\"end\":67140,\"start\":67128},{\"end\":67157,\"start\":67144},{\"end\":67386,\"start\":67378},{\"end\":67394,\"start\":67390},{\"end\":67401,\"start\":67398},{\"end\":67411,\"start\":67408},{\"end\":67421,\"start\":67418},{\"end\":67431,\"start\":67425},{\"end\":67442,\"start\":67435},{\"end\":67452,\"start\":67446},{\"end\":67756,\"start\":67751},{\"end\":67768,\"start\":67760},{\"end\":68022,\"start\":68017},{\"end\":68032,\"start\":68026},{\"end\":68042,\"start\":68036},{\"end\":68051,\"start\":68046},{\"end\":68314,\"start\":68304},{\"end\":68325,\"start\":68321},{\"end\":68335,\"start\":68329},{\"end\":68347,\"start\":68341},{\"end\":68647,\"start\":68641},{\"end\":68663,\"start\":68651},{\"end\":68674,\"start\":68667},{\"end\":68894,\"start\":68888},{\"end\":68907,\"start\":68898},{\"end\":68915,\"start\":68911},{\"end\":68926,\"start\":68919},{\"end\":69118,\"start\":69110},{\"end\":69130,\"start\":69124},{\"end\":69143,\"start\":69134},{\"end\":69329,\"start\":69321},{\"end\":69342,\"start\":69333},{\"end\":69541,\"start\":69533},{\"end\":69554,\"start\":69545},{\"end\":69744,\"start\":69734},{\"end\":69756,\"start\":69748},{\"end\":69946,\"start\":69943},{\"end\":69952,\"start\":69950},{\"end\":69964,\"start\":69956},{\"end\":69971,\"start\":69968},{\"end\":70202,\"start\":70187},{\"end\":70219,\"start\":70206},{\"end\":70234,\"start\":70223},{\"end\":70246,\"start\":70238},{\"end\":70497,\"start\":70482},{\"end\":70508,\"start\":70501},{\"end\":70519,\"start\":70512},{\"end\":70529,\"start\":70525},{\"end\":70540,\"start\":70533},{\"end\":70781,\"start\":70778},{\"end\":70790,\"start\":70785},{\"end\":70802,\"start\":70794},{\"end\":70813,\"start\":70806},{\"end\":70824,\"start\":70819},{\"end\":71057,\"start\":71046},{\"end\":71065,\"start\":71061},{\"end\":71071,\"start\":71069},{\"end\":71081,\"start\":71075},{\"end\":71093,\"start\":71085},{\"end\":71099,\"start\":71097},{\"end\":71108,\"start\":71103},{\"end\":71120,\"start\":71112},{\"end\":71130,\"start\":71124},{\"end\":71143,\"start\":71134},{\"end\":71153,\"start\":71149},{\"end\":71164,\"start\":71157},{\"end\":71487,\"start\":71480},{\"end\":71499,\"start\":71491},{\"end\":71511,\"start\":71505},{\"end\":71741,\"start\":71729},{\"end\":71754,\"start\":71745},{\"end\":71764,\"start\":71758},{\"end\":71772,\"start\":71768},{\"end\":71995,\"start\":71983},{\"end\":72008,\"start\":71999},{\"end\":72018,\"start\":72012},{\"end\":72026,\"start\":72022},{\"end\":72201,\"start\":72194},{\"end\":72212,\"start\":72205},{\"end\":72437,\"start\":72429},{\"end\":72449,\"start\":72441},{\"end\":72462,\"start\":72453},{\"end\":72474,\"start\":72470},{\"end\":72680,\"start\":72672},{\"end\":72692,\"start\":72684},{\"end\":72703,\"start\":72696},{\"end\":72715,\"start\":72711},{\"end\":72977,\"start\":72969},{\"end\":72986,\"start\":72981},{\"end\":72995,\"start\":72990},{\"end\":73006,\"start\":72999},{\"end\":73016,\"start\":73010},{\"end\":73025,\"start\":73020},{\"end\":73273,\"start\":73267},{\"end\":73282,\"start\":73277},{\"end\":73294,\"start\":73288},{\"end\":73490,\"start\":73482},{\"end\":73503,\"start\":73494},{\"end\":73729,\"start\":73725},{\"end\":73746,\"start\":73735},{\"end\":73754,\"start\":73750},{\"end\":73893,\"start\":73888},{\"end\":73905,\"start\":73897},{\"end\":73919,\"start\":73907},{\"end\":74096,\"start\":74091},{\"end\":74110,\"start\":74100},{\"end\":74122,\"start\":74114},{\"end\":74356,\"start\":74348},{\"end\":74376,\"start\":74364},{\"end\":74386,\"start\":74380},{\"end\":74403,\"start\":74394},{\"end\":74732,\"start\":74726},{\"end\":74742,\"start\":74736},{\"end\":74755,\"start\":74746},{\"end\":74767,\"start\":74759},{\"end\":74779,\"start\":74771},{\"end\":74795,\"start\":74785},{\"end\":74805,\"start\":74799},{\"end\":74817,\"start\":74811},{\"end\":74826,\"start\":74821},{\"end\":74835,\"start\":74830},{\"end\":74847,\"start\":74843},{\"end\":75198,\"start\":75195},{\"end\":75207,\"start\":75202},{\"end\":75215,\"start\":75211},{\"end\":75223,\"start\":75219},{\"end\":75235,\"start\":75230},{\"end\":75243,\"start\":75239},{\"end\":75250,\"start\":75247},{\"end\":75541,\"start\":75538},{\"end\":75551,\"start\":75545},{\"end\":75561,\"start\":75558},{\"end\":75571,\"start\":75565},{\"end\":75820,\"start\":75818},{\"end\":75833,\"start\":75826},{\"end\":75844,\"start\":75837},{\"end\":76063,\"start\":76061},{\"end\":76074,\"start\":76067},{\"end\":76087,\"start\":76081},{\"end\":76095,\"start\":76091},{\"end\":76106,\"start\":76099},{\"end\":76397,\"start\":76394},{\"end\":76407,\"start\":76401},{\"end\":76418,\"start\":76411},{\"end\":76647,\"start\":76645},{\"end\":76657,\"start\":76651},{\"end\":76910,\"start\":76905},{\"end\":76921,\"start\":76914},{\"end\":76931,\"start\":76925},{\"end\":76942,\"start\":76935},{\"end\":77101,\"start\":77096},{\"end\":77115,\"start\":77105},{\"end\":77133,\"start\":77119},{\"end\":77143,\"start\":77137},{\"end\":77149,\"start\":77147},{\"end\":77155,\"start\":77153},{\"end\":77164,\"start\":77159},{\"end\":77176,\"start\":77172},{\"end\":77533,\"start\":77529},{\"end\":77546,\"start\":77537},{\"end\":77554,\"start\":77550},{\"end\":77566,\"start\":77558},{\"end\":77575,\"start\":77570}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4517687},\"end\":56471,\"start\":56241},{\"attributes\":{\"id\":\"b1\"},\"end\":56644,\"start\":56473},{\"attributes\":{\"doi\":\"arXiv:1511.08119v3\",\"id\":\"b2\"},\"end\":56930,\"start\":56646},{\"attributes\":{\"doi\":\"arXiv:1511.00561v2\",\"id\":\"b3\"},\"end\":57292,\"start\":56932},{\"attributes\":{\"doi\":\"arXiv:1506.02106v4\",\"id\":\"b4\"},\"end\":57586,\"start\":57294},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13510913},\"end\":57804,\"start\":57588},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10759568},\"end\":58130,\"start\":57806},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15134598},\"end\":58347,\"start\":58132},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1996665},\"end\":58695,\"start\":58349},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2901004},\"end\":58904,\"start\":58697},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206593096},\"end\":59122,\"start\":58906},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206764948},\"end\":59389,\"start\":59124},{\"attributes\":{\"id\":\"b12\"},\"end\":59590,\"start\":59391},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":207252270},\"end\":59937,\"start\":59592},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3198903},\"end\":60251,\"start\":59939},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":18012947},\"end\":60501,\"start\":60253},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2218472},\"end\":60956,\"start\":60503},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206765269},\"end\":61230,\"start\":60958},{\"attributes\":{\"id\":\"b18\"},\"end\":61443,\"start\":61232},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206770307},\"end\":61523,\"start\":61445},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":215827080},\"end\":61807,\"start\":61525},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":122320},\"end\":62011,\"start\":61809},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9272368},\"end\":62224,\"start\":62013},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12225766},\"end\":62496,\"start\":62226},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":7030481},\"end\":62750,\"start\":62498},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":9506138},\"end\":62933,\"start\":62752},{\"attributes\":{\"id\":\"b26\"},\"end\":63108,\"start\":62935},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14742646},\"end\":63348,\"start\":63110},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":12908564},\"end\":63536,\"start\":63350},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":195908774},\"end\":63776,\"start\":63538},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":17961792},\"end\":64132,\"start\":63778},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":10385138},\"end\":64400,\"start\":64134},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":11106957},\"end\":64570,\"start\":64402},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1779661},\"end\":64691,\"start\":64572},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":12056046},\"end\":64920,\"start\":64693},{\"attributes\":{\"id\":\"b35\"},\"end\":65179,\"start\":64922},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14554538},\"end\":65534,\"start\":65181},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14113767},\"end\":65841,\"start\":65536},{\"attributes\":{\"doi\":\"arXiv:1506.04579v2\",\"id\":\"b38\"},\"end\":66049,\"start\":65843},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":8254931},\"end\":66309,\"start\":66051},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1629541},\"end\":66566,\"start\":66311},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":13031234},\"end\":66804,\"start\":66568},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":3238213},\"end\":67053,\"start\":66806},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":215824593},\"end\":67294,\"start\":67055},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":6529084},\"end\":67663,\"start\":67296},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":11664336},\"end\":67917,\"start\":67665},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":206592664},\"end\":68221,\"start\":67919},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3035960},\"end\":68559,\"start\":68223},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2359761},\"end\":68824,\"start\":68561},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":6679306},\"end\":69062,\"start\":68826},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":9572862},\"end\":69256,\"start\":69064},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":15453764},\"end\":69458,\"start\":69258},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":7656505},\"end\":69682,\"start\":69460},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":9288280},\"end\":69859,\"start\":69684},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":10328909},\"end\":70119,\"start\":69861},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":8630664},\"end\":70412,\"start\":70121},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":12711310},\"end\":70702,\"start\":70414},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":15508056},\"end\":70991,\"start\":70704},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":2930547},\"end\":71404,\"start\":70993},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":1900911},\"end\":71685,\"start\":71406},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":8784827},\"end\":71901,\"start\":71687},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":14628172},\"end\":72190,\"start\":71903},{\"attributes\":{\"doi\":\"arXiv:1503.02351v1[cs.CV\",\"id\":\"b62\"},\"end\":72389,\"start\":72192},{\"attributes\":{\"id\":\"b63\"},\"end\":72601,\"start\":72391},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":14028050},\"end\":72874,\"start\":72603},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":4071727},\"end\":73210,\"start\":72876},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":14181546},\"end\":73410,\"start\":73212},{\"attributes\":{\"doi\":\"arXiv:1409.1556v6\",\"id\":\"b67\"},\"end\":73665,\"start\":73412},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":6242669},\"end\":73882,\"start\":73667},{\"attributes\":{\"id\":\"b69\"},\"end\":74001,\"start\":73884},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":8453985},\"end\":74299,\"start\":74003},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":216077384},\"end\":74634,\"start\":74301},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":6544192},\"end\":75111,\"start\":74636},{\"attributes\":{\"doi\":\"arXiv:1509.03150v1\",\"id\":\"b73\"},\"end\":75460,\"start\":75113},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":1002681},\"end\":75753,\"start\":75462},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":8499745},\"end\":75967,\"start\":75755},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":3818317},\"end\":76285,\"start\":75969},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":3014704},\"end\":76584,\"start\":76287},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":17127188},\"end\":76832,\"start\":76586},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":1894392},\"end\":77092,\"start\":76834},{\"attributes\":{\"id\":\"b80\"},\"end\":77457,\"start\":77094},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":1849990},\"end\":77726,\"start\":77459}]", "bib_title": "[{\"end\":56274,\"start\":56241},{\"end\":56497,\"start\":56473},{\"end\":57633,\"start\":57588},{\"end\":57879,\"start\":57806},{\"end\":58182,\"start\":58132},{\"end\":58430,\"start\":58349},{\"end\":58755,\"start\":58697},{\"end\":58975,\"start\":58906},{\"end\":59183,\"start\":59124},{\"end\":59651,\"start\":59592},{\"end\":60002,\"start\":59939},{\"end\":60270,\"start\":60253},{\"end\":60604,\"start\":60503},{\"end\":61011,\"start\":60958},{\"end\":61455,\"start\":61445},{\"end\":61605,\"start\":61525},{\"end\":61870,\"start\":61809},{\"end\":62052,\"start\":62013},{\"end\":62292,\"start\":62226},{\"end\":62560,\"start\":62498},{\"end\":62811,\"start\":62752},{\"end\":63155,\"start\":63110},{\"end\":63406,\"start\":63350},{\"end\":63601,\"start\":63538},{\"end\":63829,\"start\":63778},{\"end\":64204,\"start\":64134},{\"end\":64435,\"start\":64402},{\"end\":64585,\"start\":64572},{\"end\":64740,\"start\":64693},{\"end\":65261,\"start\":65181},{\"end\":65577,\"start\":65536},{\"end\":66103,\"start\":66051},{\"end\":66365,\"start\":66311},{\"end\":66642,\"start\":66568},{\"end\":66879,\"start\":66806},{\"end\":67111,\"start\":67055},{\"end\":67374,\"start\":67296},{\"end\":67747,\"start\":67665},{\"end\":68013,\"start\":67919},{\"end\":68300,\"start\":68223},{\"end\":68637,\"start\":68561},{\"end\":68884,\"start\":68826},{\"end\":69106,\"start\":69064},{\"end\":69315,\"start\":69258},{\"end\":69527,\"start\":69460},{\"end\":69730,\"start\":69684},{\"end\":69939,\"start\":69861},{\"end\":70183,\"start\":70121},{\"end\":70478,\"start\":70414},{\"end\":70774,\"start\":70704},{\"end\":71042,\"start\":70993},{\"end\":71474,\"start\":71406},{\"end\":71725,\"start\":71687},{\"end\":71979,\"start\":71903},{\"end\":72425,\"start\":72391},{\"end\":72668,\"start\":72603},{\"end\":72965,\"start\":72876},{\"end\":73263,\"start\":73212},{\"end\":73721,\"start\":73667},{\"end\":74087,\"start\":74003},{\"end\":74340,\"start\":74301},{\"end\":74722,\"start\":74636},{\"end\":75534,\"start\":75462},{\"end\":75814,\"start\":75755},{\"end\":76057,\"start\":75969},{\"end\":76390,\"start\":76287},{\"end\":76641,\"start\":76586},{\"end\":76901,\"start\":76834},{\"end\":77525,\"start\":77459}]", "bib_author": "[{\"end\":56288,\"start\":56276},{\"end\":56302,\"start\":56288},{\"end\":56312,\"start\":56302},{\"end\":56323,\"start\":56312},{\"end\":56332,\"start\":56323},{\"end\":56511,\"start\":56499},{\"end\":56531,\"start\":56511},{\"end\":56539,\"start\":56531},{\"end\":56655,\"start\":56646},{\"end\":56669,\"start\":56655},{\"end\":56678,\"start\":56669},{\"end\":56690,\"start\":56678},{\"end\":57032,\"start\":57014},{\"end\":57043,\"start\":57032},{\"end\":57054,\"start\":57043},{\"end\":57369,\"start\":57358},{\"end\":57384,\"start\":57369},{\"end\":57395,\"start\":57384},{\"end\":57406,\"start\":57395},{\"end\":57647,\"start\":57635},{\"end\":57658,\"start\":57647},{\"end\":57669,\"start\":57658},{\"end\":57681,\"start\":57669},{\"end\":57894,\"start\":57881},{\"end\":57906,\"start\":57894},{\"end\":57917,\"start\":57906},{\"end\":58193,\"start\":58184},{\"end\":58205,\"start\":58193},{\"end\":58213,\"start\":58205},{\"end\":58224,\"start\":58213},{\"end\":58443,\"start\":58432},{\"end\":58457,\"start\":58443},{\"end\":58469,\"start\":58457},{\"end\":58479,\"start\":58469},{\"end\":58491,\"start\":58479},{\"end\":58768,\"start\":58757},{\"end\":58775,\"start\":58768},{\"end\":58786,\"start\":58775},{\"end\":58984,\"start\":58977},{\"end\":58990,\"start\":58984},{\"end\":58997,\"start\":58990},{\"end\":59195,\"start\":59185},{\"end\":59204,\"start\":59195},{\"end\":59215,\"start\":59204},{\"end\":59225,\"start\":59215},{\"end\":59460,\"start\":59447},{\"end\":59473,\"start\":59460},{\"end\":59667,\"start\":59653},{\"end\":59681,\"start\":59667},{\"end\":59693,\"start\":59681},{\"end\":59709,\"start\":59693},{\"end\":59717,\"start\":59709},{\"end\":59730,\"start\":59717},{\"end\":60022,\"start\":60004},{\"end\":60036,\"start\":60022},{\"end\":60050,\"start\":60036},{\"end\":60061,\"start\":60050},{\"end\":60282,\"start\":60272},{\"end\":60294,\"start\":60282},{\"end\":60302,\"start\":60294},{\"end\":60313,\"start\":60302},{\"end\":60326,\"start\":60313},{\"end\":60335,\"start\":60326},{\"end\":60350,\"start\":60335},{\"end\":60617,\"start\":60606},{\"end\":60632,\"start\":60617},{\"end\":60641,\"start\":60632},{\"end\":60654,\"start\":60641},{\"end\":60666,\"start\":60654},{\"end\":60681,\"start\":60666},{\"end\":60696,\"start\":60681},{\"end\":60702,\"start\":60696},{\"end\":61023,\"start\":61013},{\"end\":61032,\"start\":61023},{\"end\":61041,\"start\":61032},{\"end\":61052,\"start\":61041},{\"end\":61063,\"start\":61052},{\"end\":61242,\"start\":61232},{\"end\":61250,\"start\":61242},{\"end\":61261,\"start\":61250},{\"end\":61272,\"start\":61261},{\"end\":61469,\"start\":61457},{\"end\":61619,\"start\":61607},{\"end\":61630,\"start\":61619},{\"end\":61641,\"start\":61630},{\"end\":61650,\"start\":61641},{\"end\":61882,\"start\":61872},{\"end\":61892,\"start\":61882},{\"end\":62067,\"start\":62054},{\"end\":62079,\"start\":62067},{\"end\":62093,\"start\":62079},{\"end\":62102,\"start\":62093},{\"end\":62307,\"start\":62294},{\"end\":62321,\"start\":62307},{\"end\":62335,\"start\":62321},{\"end\":62344,\"start\":62335},{\"end\":62573,\"start\":62562},{\"end\":62586,\"start\":62573},{\"end\":62598,\"start\":62586},{\"end\":62608,\"start\":62598},{\"end\":62819,\"start\":62813},{\"end\":62828,\"start\":62819},{\"end\":62946,\"start\":62937},{\"end\":62954,\"start\":62946},{\"end\":62962,\"start\":62954},{\"end\":62972,\"start\":62962},{\"end\":63167,\"start\":63157},{\"end\":63179,\"start\":63167},{\"end\":63189,\"start\":63179},{\"end\":63200,\"start\":63189},{\"end\":63414,\"start\":63408},{\"end\":63425,\"start\":63414},{\"end\":63617,\"start\":63603},{\"end\":63630,\"start\":63617},{\"end\":63642,\"start\":63630},{\"end\":63841,\"start\":63831},{\"end\":63851,\"start\":63841},{\"end\":63866,\"start\":63851},{\"end\":63875,\"start\":63866},{\"end\":64215,\"start\":64206},{\"end\":64221,\"start\":64215},{\"end\":64233,\"start\":64221},{\"end\":64239,\"start\":64233},{\"end\":64247,\"start\":64239},{\"end\":64448,\"start\":64437},{\"end\":64455,\"start\":64448},{\"end\":64468,\"start\":64455},{\"end\":64596,\"start\":64587},{\"end\":64606,\"start\":64596},{\"end\":64616,\"start\":64606},{\"end\":64751,\"start\":64742},{\"end\":64763,\"start\":64751},{\"end\":64775,\"start\":64763},{\"end\":64787,\"start\":64775},{\"end\":65005,\"start\":64996},{\"end\":65018,\"start\":65005},{\"end\":65029,\"start\":65018},{\"end\":65270,\"start\":65263},{\"end\":65278,\"start\":65270},{\"end\":65296,\"start\":65278},{\"end\":65304,\"start\":65296},{\"end\":65589,\"start\":65579},{\"end\":65598,\"start\":65589},{\"end\":65610,\"start\":65598},{\"end\":65618,\"start\":65610},{\"end\":65628,\"start\":65618},{\"end\":65639,\"start\":65628},{\"end\":65649,\"start\":65639},{\"end\":65662,\"start\":65649},{\"end\":65850,\"start\":65843},{\"end\":65864,\"start\":65850},{\"end\":65874,\"start\":65864},{\"end\":66112,\"start\":66105},{\"end\":66118,\"start\":66112},{\"end\":66125,\"start\":66118},{\"end\":66134,\"start\":66125},{\"end\":66142,\"start\":66134},{\"end\":66375,\"start\":66367},{\"end\":66388,\"start\":66375},{\"end\":66399,\"start\":66388},{\"end\":66653,\"start\":66644},{\"end\":66661,\"start\":66653},{\"end\":66671,\"start\":66661},{\"end\":66890,\"start\":66881},{\"end\":66905,\"start\":66890},{\"end\":66915,\"start\":66905},{\"end\":67126,\"start\":67113},{\"end\":67142,\"start\":67126},{\"end\":67159,\"start\":67142},{\"end\":67388,\"start\":67376},{\"end\":67396,\"start\":67388},{\"end\":67403,\"start\":67396},{\"end\":67413,\"start\":67403},{\"end\":67423,\"start\":67413},{\"end\":67433,\"start\":67423},{\"end\":67444,\"start\":67433},{\"end\":67454,\"start\":67444},{\"end\":67758,\"start\":67749},{\"end\":67770,\"start\":67758},{\"end\":68024,\"start\":68015},{\"end\":68034,\"start\":68024},{\"end\":68044,\"start\":68034},{\"end\":68053,\"start\":68044},{\"end\":68316,\"start\":68302},{\"end\":68327,\"start\":68316},{\"end\":68337,\"start\":68327},{\"end\":68349,\"start\":68337},{\"end\":68649,\"start\":68639},{\"end\":68665,\"start\":68649},{\"end\":68676,\"start\":68665},{\"end\":68896,\"start\":68886},{\"end\":68909,\"start\":68896},{\"end\":68917,\"start\":68909},{\"end\":68928,\"start\":68917},{\"end\":69120,\"start\":69108},{\"end\":69132,\"start\":69120},{\"end\":69145,\"start\":69132},{\"end\":69331,\"start\":69317},{\"end\":69344,\"start\":69331},{\"end\":69543,\"start\":69529},{\"end\":69556,\"start\":69543},{\"end\":69746,\"start\":69732},{\"end\":69758,\"start\":69746},{\"end\":69948,\"start\":69941},{\"end\":69954,\"start\":69948},{\"end\":69966,\"start\":69954},{\"end\":69973,\"start\":69966},{\"end\":70204,\"start\":70185},{\"end\":70221,\"start\":70204},{\"end\":70236,\"start\":70221},{\"end\":70248,\"start\":70236},{\"end\":70499,\"start\":70480},{\"end\":70510,\"start\":70499},{\"end\":70521,\"start\":70510},{\"end\":70531,\"start\":70521},{\"end\":70542,\"start\":70531},{\"end\":70783,\"start\":70776},{\"end\":70792,\"start\":70783},{\"end\":70804,\"start\":70792},{\"end\":70815,\"start\":70804},{\"end\":70826,\"start\":70815},{\"end\":71059,\"start\":71044},{\"end\":71067,\"start\":71059},{\"end\":71073,\"start\":71067},{\"end\":71083,\"start\":71073},{\"end\":71095,\"start\":71083},{\"end\":71101,\"start\":71095},{\"end\":71110,\"start\":71101},{\"end\":71122,\"start\":71110},{\"end\":71132,\"start\":71122},{\"end\":71145,\"start\":71132},{\"end\":71155,\"start\":71145},{\"end\":71166,\"start\":71155},{\"end\":71489,\"start\":71476},{\"end\":71501,\"start\":71489},{\"end\":71513,\"start\":71501},{\"end\":71517,\"start\":71513},{\"end\":71743,\"start\":71727},{\"end\":71756,\"start\":71743},{\"end\":71766,\"start\":71756},{\"end\":71774,\"start\":71766},{\"end\":71997,\"start\":71981},{\"end\":72010,\"start\":71997},{\"end\":72020,\"start\":72010},{\"end\":72028,\"start\":72020},{\"end\":72203,\"start\":72192},{\"end\":72214,\"start\":72203},{\"end\":72439,\"start\":72427},{\"end\":72451,\"start\":72439},{\"end\":72464,\"start\":72451},{\"end\":72476,\"start\":72464},{\"end\":72682,\"start\":72670},{\"end\":72694,\"start\":72682},{\"end\":72705,\"start\":72694},{\"end\":72717,\"start\":72705},{\"end\":72979,\"start\":72967},{\"end\":72988,\"start\":72979},{\"end\":72997,\"start\":72988},{\"end\":73008,\"start\":72997},{\"end\":73018,\"start\":73008},{\"end\":73027,\"start\":73018},{\"end\":73275,\"start\":73265},{\"end\":73284,\"start\":73275},{\"end\":73296,\"start\":73284},{\"end\":73492,\"start\":73480},{\"end\":73505,\"start\":73492},{\"end\":73731,\"start\":73723},{\"end\":73748,\"start\":73731},{\"end\":73756,\"start\":73748},{\"end\":73895,\"start\":73886},{\"end\":73907,\"start\":73895},{\"end\":73921,\"start\":73907},{\"end\":74098,\"start\":74089},{\"end\":74112,\"start\":74098},{\"end\":74124,\"start\":74112},{\"end\":74358,\"start\":74342},{\"end\":74378,\"start\":74358},{\"end\":74388,\"start\":74378},{\"end\":74405,\"start\":74388},{\"end\":74734,\"start\":74724},{\"end\":74744,\"start\":74734},{\"end\":74757,\"start\":74744},{\"end\":74769,\"start\":74757},{\"end\":74781,\"start\":74769},{\"end\":74797,\"start\":74781},{\"end\":74807,\"start\":74797},{\"end\":74819,\"start\":74807},{\"end\":74828,\"start\":74819},{\"end\":74837,\"start\":74828},{\"end\":74849,\"start\":74837},{\"end\":75200,\"start\":75193},{\"end\":75209,\"start\":75200},{\"end\":75217,\"start\":75209},{\"end\":75225,\"start\":75217},{\"end\":75237,\"start\":75225},{\"end\":75245,\"start\":75237},{\"end\":75252,\"start\":75245},{\"end\":75543,\"start\":75536},{\"end\":75553,\"start\":75543},{\"end\":75563,\"start\":75553},{\"end\":75573,\"start\":75563},{\"end\":75822,\"start\":75816},{\"end\":75835,\"start\":75822},{\"end\":75846,\"start\":75835},{\"end\":76065,\"start\":76059},{\"end\":76076,\"start\":76065},{\"end\":76089,\"start\":76076},{\"end\":76097,\"start\":76089},{\"end\":76108,\"start\":76097},{\"end\":76399,\"start\":76392},{\"end\":76409,\"start\":76399},{\"end\":76420,\"start\":76409},{\"end\":76649,\"start\":76643},{\"end\":76659,\"start\":76649},{\"end\":76912,\"start\":76903},{\"end\":76923,\"start\":76912},{\"end\":76933,\"start\":76923},{\"end\":76944,\"start\":76933},{\"end\":77103,\"start\":77094},{\"end\":77117,\"start\":77103},{\"end\":77135,\"start\":77117},{\"end\":77145,\"start\":77135},{\"end\":77151,\"start\":77145},{\"end\":77157,\"start\":77151},{\"end\":77166,\"start\":77157},{\"end\":77178,\"start\":77166},{\"end\":77535,\"start\":77527},{\"end\":77548,\"start\":77535},{\"end\":77556,\"start\":77548},{\"end\":77568,\"start\":77556},{\"end\":77577,\"start\":77568}]", "bib_venue": "[{\"end\":56346,\"start\":56338},{\"end\":56547,\"start\":56545},{\"end\":58510,\"start\":58497},{\"end\":61477,\"start\":61475},{\"end\":61900,\"start\":61898},{\"end\":63433,\"start\":63431},{\"end\":64255,\"start\":64253},{\"end\":64476,\"start\":64474},{\"end\":64795,\"start\":64793},{\"end\":65336,\"start\":65310},{\"end\":66168,\"start\":66148},{\"end\":66426,\"start\":66405},{\"end\":67465,\"start\":67460},{\"end\":68381,\"start\":68355},{\"end\":70256,\"start\":70254},{\"end\":72725,\"start\":72723},{\"end\":73764,\"start\":73762},{\"end\":76115,\"start\":76113},{\"end\":76685,\"start\":76665},{\"end\":77263,\"start\":77243},{\"end\":56336,\"start\":56332},{\"end\":56543,\"start\":56539},{\"end\":56770,\"start\":56708},{\"end\":57012,\"start\":56932},{\"end\":57356,\"start\":57294},{\"end\":57685,\"start\":57681},{\"end\":57944,\"start\":57917},{\"end\":58228,\"start\":58224},{\"end\":58495,\"start\":58491},{\"end\":58790,\"start\":58786},{\"end\":59001,\"start\":58997},{\"end\":59236,\"start\":59225},{\"end\":59445,\"start\":59391},{\"end\":59734,\"start\":59730},{\"end\":60072,\"start\":60061},{\"end\":60364,\"start\":60350},{\"end\":60714,\"start\":60702},{\"end\":61074,\"start\":61063},{\"end\":61318,\"start\":61272},{\"end\":61473,\"start\":61469},{\"end\":61654,\"start\":61650},{\"end\":61896,\"start\":61892},{\"end\":62106,\"start\":62102},{\"end\":62348,\"start\":62344},{\"end\":62612,\"start\":62608},{\"end\":62832,\"start\":62828},{\"end\":63211,\"start\":63200},{\"end\":63429,\"start\":63425},{\"end\":63646,\"start\":63642},{\"end\":63942,\"start\":63875},{\"end\":64251,\"start\":64247},{\"end\":64472,\"start\":64468},{\"end\":64622,\"start\":64616},{\"end\":64791,\"start\":64787},{\"end\":64994,\"start\":64922},{\"end\":65308,\"start\":65304},{\"end\":65666,\"start\":65662},{\"end\":65929,\"start\":65892},{\"end\":66146,\"start\":66142},{\"end\":66403,\"start\":66399},{\"end\":66675,\"start\":66671},{\"end\":66919,\"start\":66915},{\"end\":67163,\"start\":67159},{\"end\":67458,\"start\":67454},{\"end\":67774,\"start\":67770},{\"end\":68057,\"start\":68053},{\"end\":68353,\"start\":68349},{\"end\":68680,\"start\":68676},{\"end\":68932,\"start\":68928},{\"end\":69149,\"start\":69145},{\"end\":69348,\"start\":69344},{\"end\":69560,\"start\":69556},{\"end\":69762,\"start\":69758},{\"end\":69977,\"start\":69973},{\"end\":70252,\"start\":70248},{\"end\":70546,\"start\":70542},{\"end\":70830,\"start\":70826},{\"end\":71170,\"start\":71166},{\"end\":71521,\"start\":71517},{\"end\":71778,\"start\":71774},{\"end\":72032,\"start\":72028},{\"end\":72278,\"start\":72238},{\"end\":72480,\"start\":72476},{\"end\":72721,\"start\":72717},{\"end\":73031,\"start\":73027},{\"end\":73300,\"start\":73296},{\"end\":73478,\"start\":73412},{\"end\":73760,\"start\":73756},{\"end\":73925,\"start\":73921},{\"end\":74128,\"start\":74124},{\"end\":74445,\"start\":74405},{\"end\":74853,\"start\":74849},{\"end\":75191,\"start\":75113},{\"end\":75577,\"start\":75573},{\"end\":75850,\"start\":75846},{\"end\":76111,\"start\":76108},{\"end\":76424,\"start\":76420},{\"end\":76663,\"start\":76659},{\"end\":76948,\"start\":76944},{\"end\":77241,\"start\":77178},{\"end\":77581,\"start\":77577}]"}}}, "year": 2023, "month": 12, "day": 17}