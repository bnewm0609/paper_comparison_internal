{"id": 53108229, "updated": "2023-10-02 10:36:44.908", "metadata": {"title": "Methods for Segmentation and Classification of Digital Microscopy Tissue Images", "authors": "[{\"first\":\"Quoc\",\"last\":\"Vu\",\"middle\":[\"Dang\"]},{\"first\":\"Simon\",\"last\":\"Graham\",\"middle\":[]},{\"first\":\"Tahsin\",\"last\":\"Kurc\",\"middle\":[]},{\"first\":\"Minh\",\"last\":\"To\",\"middle\":[\"Nguyen\",\"Nhat\"]},{\"first\":\"Muhammad\",\"last\":\"Shaban\",\"middle\":[]},{\"first\":\"Talha\",\"last\":\"Qaiser\",\"middle\":[]},{\"first\":\"Navid\",\"last\":\"Koohbanani\",\"middle\":[\"Alemi\"]},{\"first\":\"Syed\",\"last\":\"Khurram\",\"middle\":[\"Ali\"]},{\"first\":\"Jayashree\",\"last\":\"Kalpathy-Cramer\",\"middle\":[]},{\"first\":\"Tianhao\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Rajarsi\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Jin\",\"last\":\"Kwak\",\"middle\":[\"Tae\"]},{\"first\":\"Nasir\",\"last\":\"Rajpoot\",\"middle\":[]},{\"first\":\"Joel\",\"last\":\"Saltz\",\"middle\":[]},{\"first\":\"Keyvan\",\"last\":\"Farahani\",\"middle\":[]}]", "venue": "Frontiers in Bioengineering and Biotechnology", "journal": "Frontiers in Bioengineering and Biotechnology", "publication_date": {"year": 2019, "month": 4, "day": 2}, "abstract": "High-resolution microscopy images of tissue specimens provide detailed information about the morphology of normal and diseased tissue. Image analysis of tissue morphology can help cancer researchers develop a better understanding of cancer biology. Segmentation of nuclei and classification of tissue images are two common tasks in tissue image analysis. Development of accurate and efficient algorithms for these tasks is a challenging problem because of the complexity of tissue morphology and tumor heterogeneity. In this paper we present two computer algorithms; one designed for segmentation of nuclei and the other for classification of whole slide tissue images. The segmentation algorithm implements a multiscale deep residual aggregation network to accurately segment nuclear material and then separate clumped nuclei into individual nuclei. The classification algorithm initially carries out patch-level classification via a deep learning method, then patch-level statistical and morphological features are used as input to a random forest regression model for whole slide image classification. The segmentation and classification algorithms were evaluated in the MICCAI 2017 Digital Pathology challenge. The segmentation algorithm achieved an accuracy score of 0.78. The classification algorithm achieved an accuracy score of 0.81. These scores were the highest in the challenge.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1810.13230", "mag": "2964282006", "acl": null, "pubmed": "31001524", "pubmedcentral": "6454006", "dblp": "journals/corr/abs-1810-13230", "doi": "10.3389/fbioe.2019.00053"}}, "content": {"source": {"pdf_hash": "4d552fd46f02848785e0f694b113a7ac28900a69", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://www.frontiersin.org/articles/10.3389/fbioe.2019.00053/pdf", "status": "GOLD"}}, "grobid": {"id": "10faa32656ce9c5c4d7e92dc4ab98eb31580c8d5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4d552fd46f02848785e0f694b113a7ac28900a69.txt", "contents": "\nArticle 53 K (2019) Methods for Segmentation and Classification of Digital Microscopy Tissue Images\nApril 2019\n\nApril Khademi \nAndreas Holzinger \nQuoc Dang Vu \nDepartment of Computer Science and Engineering\nSejong University\nSeoulSouth Korea\n\nSimon Graham \nDepartment of Computer Science\nUniversity of Warwick\nCoventryUnited Kingdom\n\nTahsin Kurc tahsin.kurc@stonybrook.edu \nDepartment of Biomedical Informatics\nStony Brook University\nStony BrookNYUnited States\n\nMinh Nguyen \nNhat To \nDepartment of Computer Science and Engineering\nSejong University\nSeoulSouth Korea\n\nMuhammad Shaban \nDepartment of Computer Science\nUniversity of Warwick\nCoventryUnited Kingdom\n\nTalha Qaiser \nDepartment of Computer Science\nUniversity of Warwick\nCoventryUnited Kingdom\n\nNavid Alemi Koohbanani \nDepartment of Computer Science\nUniversity of Warwick\nCoventryUnited Kingdom\n\nSyed Ali Khurram \nSchool of Clinical Dentistry\nThe University of Sheffield\nSheffieldUnited Kingdom\n\nJayashree Kalpathy-Cramer \nDepartment of Radiology\nHarvard Medical School and Mass General Hospital\nBostonMAUnited States\n\nTianhao Zhao \nDepartment of Biomedical Informatics\nStony Brook University\nStony BrookNYUnited States\n\nDepartment of Pathology\nStony Brook University\nStony Brook, NY, United States, 7 Cancer Imaging Program\n\nNational Cancer Institute\nNational Institutes of Health\nBethesdaMDUnited States\n\nRajarsi Gupta \nDepartment of Biomedical Informatics\nStony Brook University\nStony BrookNYUnited States\n\nDepartment of Pathology\nStony Brook University\nStony Brook, NY, United States, 7 Cancer Imaging Program\n\nNational Cancer Institute\nNational Institutes of Health\nBethesdaMDUnited States\n\nJin Tae Kwak \nDepartment of Computer Science and Engineering\nSejong University\nSeoulSouth Korea\n\nNasir Rajpoot \nDepartment of Computer Science\nUniversity of Warwick\nCoventryUnited Kingdom\n\nJoel Saltz \nDepartment of Biomedical Informatics\nStony Brook University\nStony BrookNYUnited States\n\nKeyvan Farahani \n\nRyerson University\nCanada\n\n\nMedical University of Graz\nShandar AhmadAustria\n\n\nJawaharlal Nehru University\nIndia\n\n\nKhurram SA, Kalpathy-Cramer J, Zhao T, Gupta R, Kwak JT, Rajpoot N, Saltz J and Farahani\nTahsin Kurc\n\n\nArticle 53 K (2019) Methods for Segmentation and Classification of Digital Microscopy Tissue Images\n\nBioinformatics ; and Computational Biology, a section of the journal Frontiers in Bioengineering and Biotechnology\n\nFront. Bioeng. Biotechnol\n753April 201910.3389/fbioe.2019.00053Specialty section: This article was submitted to Received: 30 October 2018 Accepted: 01 March 2019ORIGINAL RESEARCH Edited by: Reviewed by: *Correspondence: Citation: Vu QD, Graham S, Kurc T, To MNN, Shaban M, Qaiser T, Koohbanani NA,digital pathologytissue imagesimage analysissegmentationclassification\nHigh-resolution microscopy images of tissue specimens provide detailed information about the morphology of normal and diseased tissue. Image analysis of tissue morphology can help cancer researchers develop a better understanding of cancer biology. Segmentation of nuclei and classification of tissue images are two common tasks in tissue image analysis. Development of accurate and efficient algorithms for these tasks is a challenging problem because of the complexity of tissue morphology and tumor heterogeneity. In this paper we present two computer algorithms; one designed for segmentation of nuclei and the other for classification of whole slide tissue images. The segmentation algorithm implements a multiscale deep residual aggregation network to accurately segment nuclear material and then separate clumped nuclei into individual nuclei. The classification algorithm initially carries out patch-level classification via a deep learning method, then patch-level statistical and morphological features are used as input to a random forest regression model for whole slide image classification. The segmentation and classification algorithms were evaluated in the MICCAI 2017 Digital Pathology challenge. The segmentation algorithm achieved an accuracy score of 0.78. The classification algorithm achieved an accuracy score of 0.81. These scores were the highest in the challenge.\n\nINTRODUCTION\n\nCancer causes changes in tissue at the sub-cellular scale. Pathologists examine a tissue specimen under a powerful microscope to look for abnormalities which indicate cancer. This manual process has traditionally been the de facto standard for diagnosis and grading of cancer tumors. While it continues to be widely applied in clinical settings, manual examination of tissue is a subjective, qualitative analysis and is not scalable to translational and clinical research studies involving hundreds or thousands of tissue specimens. A quantitative analysis of normal and tumor tissue, on the other hand, can provide novel insights into observed and latent sub-cellular tissue characteristics and can lead to a better understanding of mechanisms underlying cancer onset and progression (Madabhushi, 2009;Chennubhotla et al., 2017;Cooper et al., 2018).\n\nTechnology for whole slide tissue imaging has advanced significantly over the past 20 years. We refer to digital images of tissue specimens that are stained and fixated on a glass slide as whole slide tissue images (WSIs). Highly detailed images of tissue, ranging from 20,000 \u00d7 20,000 pixels to over 100,000 \u00d7 100,000 pixels in resolution, can be captured rapidly with the stateof-the-art tissue image scanners. Improvements in storage and computational technology also have made it possible to store and analyze WSIs. These advances provide significant opportunities for quantitative analysis of tissue morphology. Quantitative analyses not only allow researchers to assemble a more detailed description of tumor structure and heterogeneity but also enable studies with large numbers of tissue samples. As the capacity to rapidly generate large quantities of WSI data has become feasible and is more widely deployed, there is an increasing need for reliable and efficient (semi-)automated computer methods to complement the traditional manual examination of tissue.\n\nThe two most common tasks in whole slide tissue image analysis are the segmentation of microscopic structures, like nuclei and cells, in tumor and non-tumor regions and the classification of image regions and whole images. Computerized detection and segmentation of nuclei is one of the core operations in histopathology image analysis. This operation is crucial to extracting, mining, and interpreting sub-cellular morphologic information from digital slide images. Cancer nuclei differ from other nuclei in many ways and influence tissue in a variety of ways. Accurate quantitative characterizations of the shape, size, and texture properties of nuclei are key components of the study of the tumor systems biology and the complex patterns of interaction between tumor cells and other cells. Image classification, carried out with or without segmentation, assigns a class label to an image region or an image. It is a key step in computing a categorization via imaging features of patients into groups for cohort selection and correlation analysis. Methods for segmentation and classification have been proposed by several research projects (Gurcan et al., 2009;Ghaznavi et al., 2013;Xie et al., 2015;Xu et al., 2015;Manivannan et al., 2016;Peikari and Martel, 2016;Sirinukunwattana et al., 2016;Wang et al., 2016;Al-Milaji et al., 2017;Chen et al., 2017;Zheng et al., 2017;Graham and Rajpoot, 2018;Senaras and Gurcan, 2018).  provide a good review of segmentation algorithms for histopathology images. A CNN algorithm was developed by Zheng et al. (2017) to analyze histopathology images for extraction and characterizations of distribution of nuclei in images of tissue specimens. A method based on ensembles of support vector machines for detection and classification of cellular patterns in tissue images was proposed by Manivannan et al. (2016). Al-Milaji et al. developed a CNN-based approach to classify tissue regions into stromal and epithelial in images of Hematoxylin and Eosin (H&E) stained tissues (Al-Milaji et al., 2017). Xu et al. (2015) used a pre-trained CNN model to extract features on patches. These features are aggregated to classify whole slide tissue images. A method that learns class-specific dictionaries for classification of histopathology images was proposed by Vu et al. (2016). Kahya et al. (2017) employed support vector machines for classification of breast cancer histopathology images. Their method employs sparse support vector machines and Wilcoxon rank sum test to assign and assess weights of imaging features. Peikari et al. (2018) devised an approach in which clustering is executed on input data to detect the structure of the data space. This is followed by a semi-supervised learning method to carry out classification using clustering information. Peikari and Martel (2016) propose a color transformation step that maps the Red-Green-Blue color space by computing eigenvectors of the RGB space. The color mapped image is then used in cell segmentation. Chen et al. (2017) propose a deep learning network that implements a multi-task learning framework through multi-level convolutional networks for detection and segmentation of objects in tissue images.\n\nDespite a large body of research work on image classification and segmentation, the process of extracting, mining, and interpreting information from digital slide images remains a difficult task Chennubhotla et al., 2017;Senaras and Gurcan, 2018). There are a number of challenges that segmentation and classification algorithms have to address. First, the morphology of tumor and normal tissue varies across tissue specimens-both across cancer types as well as across tissue specimens within a cancer type. Even a single tissue specimen will contain a variety of nuclei and other structures. Algorithms have to take into account tissue heterogeneity and learn and dynamically adapt to variations in tissue morphology across tissue specimens. It is not uncommon that an algorithm using fixed input parameters will do well for an image but poorly for another one. Second, nuclei in a tissue image touch or overlap each other. This is both a result of biological processes and an artifact of image capture. A tissue slide will have some depth, however small it is. Scanning a tissue specimen through a digitizing light microscope may inadvertently capture nuclei in different focal planes. Clumped nuclei make the segmentation process difficult. Third, whole slide tissue images are very high-resolution images and will not fit in main and GPU memory on most machines. Thus, it may not be feasible for a classification algorithm to work on an entire image as a whole. Algorithms have to be designed to work on at multiple resolutions or image tiles.\n\nIn this paper we present and experimentally evaluate two novel algorithms, one devised for segmentation of nuclei and the other developed for classification of whole slide tissue images:\n\n\u2022 The segmentation algorithm proposes a multiscale deep residual aggregation network for accurate segmentation of nuclei and separation of clumped nuclei. Our method consists of three main steps. It first detects nuclear blobs and boundaries via a group of CNNs. It then applies a watershed algorithm on the results from the first step to perform an initial separation of clumped nuclei. The last step carries out a refined segmentation of separate nuclei from the second step. The proposed method employs a multi-scale approach in order to improve the detection and segmentation performance, because the sizes of nuclei vary across tissue specimens and within a tissue specimen. An evaluation of the segmentation algorithm using a set of image tiles from glioblastoma multiforme (GBM), lower grade glioma (LGG), head and neck squamous cell carcinoma (HNSCC), and non-small cell lung cancer (NSCLC) cases showed that the algorithm was able to achieve a segmentation accuracy of 0.78. The algorithm is not only able to accurately segment nuclear material but also separate touching and overlapped nuclei into individual objects. \u2022 The classification algorithm proposes a two-part automated method to address the challenge of classifying non-small cell lung cancer (NSCLC) histology images. This method first classifies all input patches from an unseen WSI as NSCLC adeno (LUAD) or NSCLC squamous cell (LUSC) or nondiagnostic (ND) and obtains the corresponding probability maps for each class. Next, it extracts a collection of statistical and morphological features from the LUAD and LUSC probability maps as input into a random forest regression model to classify each WSI. This method is the first 3class network that aims to classify each WSI into diagnostic and non-diagnostic areas. The experimental results show an accuracy of 0.81.\n\nThese two algorithms achieved the highest scores in the Computational Precision Medicine digital pathology challenge organized at the 20th International Conference on Medical Image Computing and Computer Assisted Intervention 2017 (MICCAI 2017), Quebec City, Canada. This challenge was organized by some of the co-authors of this manuscript (Keyvan Farahani, Tahsin Kurc, Jayashree Kalpathy-Cramer, Joel Saltz with expert pathologist support provided by Tianhao Zhao and Rajarsi Gupta in preparation of challenge datasets) to provide a platform for evaluation of classification and segmentation algorithms and is part of a series of annual digital pathology challenges organized since 2014. The 2017 challenge targeted tissue images obtained from patients with non-small cell lung cancer (NSCLC), head and neck squamous cell carcinoma (HNSCC), glioblastoma multiforme (GBM), and lower grade glioma (LGG) tumors. These cancer types are complex and deadly diseases accounting for a large number of diagnostic patient deaths in spite of application of various treatment strategies. In addition to methodology contributions presented in this paper, we will make the datasets used in the MICCAI 2017 Digital Pathology challenge publicly available for other researchers to use. The rest of the manuscript is organized as follows. In section Materials And Methods we introduce the nucleus segmentation algorithm and the classification algorithm. We present the experimental evaluation of the algorithms in section Results. We describe the MICCAI 2017 Digital Pathology challenge and the challenge datasets in the same section. We conclude in section Discussion.\n\n\nMATERIALS AND METHODS\n\n\nSegmentation of Nuclei by a Deep Learning Method\n\nWe developed an approach of convolutional neural networks (CNNs) to precisely segment nuclei. The method is composed of three major steps: (1) nuclei blob and boundary detection via CNNs, (2) separation of touching (or overlapping) nuclei by combining the nuclei blob and boundary detection results through a watershed algorithm, and (3) final segmentation of individual nuclei. The entire workflow is shown in Figure 1.\n\nTwo CNNs are trained to perform the initial nuclei blob and boundary detection. These CNNs consist of two consecutive processing paths-contracting path and expanding path-and are aimed at obtaining all the nuclei pixels and nuclei boundary pixels within the tissue image, generating nuclei blob, and border masks, respectively. Provided with the blob and border masks, the initial nuclei segmentation is performed in two stages: (1) removal of the identified nuclei boundaries and (2) separation of the remaining clumped nuclei. In order to remove nuclei boundaries, we simply subtract the border mask from the blob mask after dilating the border mask with a kernel of size 3 \u00d7 3. A watershed algorithm is then applied to identify individual nuclei cores. Subsequently, each of the removed boundary pixels are assigned to its closest nuclei core, resulting in the segmentation of individual nuclei. Meanwhile, the size of each nuclei blob is examined to eliminate artifacts (>13 \u00b5m 2 ).\n\n\nNetwork Architecture\n\nThe proposed deep residual aggregation network (DRAN) is illustrated in Figure 2. It follows the renowned paradigm of two consecutive processing paths: contracting path (down-sampling the input) and expanding path (up-sampling the output of contracting path), such as in U-Net (Ronneberger et al., 2015), SegNet (Badrinarayanan et al., 2017), FCN (Long et al., 2015), and Hypercolumns (Hariharan et al., 2015), with several major and minor modifications.\n\n\nContracting Path\n\nThe contracting path can be seen as a feature extraction step, recognizing the approximate position of the targeted objects, and encoding their local characteristics. For this purpose, we utilize pre-activated ResNet50 (He et al., 2016a). Unlike the original ResNet50 (He et al., 2016b) which uses the layout of convolution-batch normalization-rectified linear unit (ReLU) for residual units, the pre-activation architecture instead adopts the layout of batch normalization-ReLU-convolution and facilitates the direct propagation of the input via the shortcut path. Several modifications are made to the pre-activated ResNet50; the first 7 \u00d7 7 convolution is performed with a stride 1 and no-padding. The max pooling, following the first 7 \u00d7 7 convolution, has been removed.\n\n\nExpanding Path\n\nThe expanding path comprises four processing (or decoding) layers. The first layer receives the output of the last layer of the contracting path and performs transpose convolution and up-sampling. The second, third, and fourth layers receive two inputs-one from the preceding layer and the other from the contracting path. The two inputs are added together and go through a decoder and resizing unit. Unlike U-Net or DCAN (Chen et al., 2017), the resizing unit simply doubles the size of the input with the nearest neighbor interpolation, which is computationally inexpensive. Moreover, instead of  using concatenation as in Ronneberger et al. (2015), adopting addition operators reduces memory usage without substantially losing the learning capability of the network. The decoder is a primary processing unit that plays a key role in interpreting information from differing levels of abstraction and producing finer segmentation maps in the expanding path. It performs a series of convolution operations by employing multipath architecture , where the input and output channels are divided into a number of disjoint groups (or paths), and each separately performs convolution. All the convolution operations in the decoder use no padding and a stride 1. Due to the convolution with no padding, the size of the segmentation map becomes smaller than that of an input image. Three decoders are utilized in our network. They share the same layout, but with differing number of channels and paths. The details are provided in Table 1.\n\n\nMultiscale Aggregation\n\nThe sizes of nuclei substantially vary among tissue samples, even within a single specimen. To better characterize nuclei and improve segmentation performance, we adopt a multiscale approach. The tissue specimen images are resized by a factor of 2 and 0.5. The resized images are separately fed into DRANs. Hence, three DRANs are, in total, trained, and prepared: one at the original scale (x1.0) and the other two are at x2 and x0.5 scales. The last softmax layer of each DRAN is removed, and the output of decoder1 is aggregated through another decoder (decoder4), generating the final segmentation map at the original scale. We note that decoder4 uses padding convolution. The details of the multiscale architecture are illustrated in Figure 3. This network is called as multiscale deep residual aggregation network (MDRAN).\n\n\nClassification of Whole Slide Images by a Two-Part Automated Method\n\nIn recent years, there have been a number of published methods for automated NSCLC classification. Yu et al. (2016) extracted a range of quantitative image features from tissue regions and used an array of classical machine techniques to classify each WSI. Although hand crafted approaches perform well, there is a growing trend toward deep learning approaches, where networks are capable of learning a strong feature representation. As a result of this strong feature representation, recent deep networks (Simonyan and Zisserman, 2014;Szegedy et al., 2015;He et al., 2016a;Huang et al., 2017) have achieved remarkable accuracy in large-scale image recognition tasks (Deng et al., 2009). Most WSI classification methods use a patch-based approach due to the computational difficulty in processing multi-gigapixel images. Coudray et al. (2018) classified NSCLC WSIs using deep learning on a patch-by-patch basis, but also predicted the ten most commonly mutated genes. For lung cancer classification, the authors used an Inception v3 network architecture to classify input patches into LUAD, LUSC, and normal. They assumed that all patches within each WSI had the same label and therefore did not differentiate between diagnostic and non-diagnostic regions. This method may result in a large number of false positives in non-diagnostic regions and training may take a long time to converge. Hou et al. (2016) trained a patch-level classifier to classify glioma and NSCLC WSIs into different cancer types. This was done by aggregating discriminative patch-level predictions from a deep network using either a multi-class logistic regression model or support vector machine. The selection of discriminative patches was done in a weakly supervised manner, where an expectation-maximization approach was used to iteratively select patches. These patches were then fed into a conventional two-class CNN to classify input patches as LUAD or LUSC. The authors of this method counter the problem of differentiating diagnostic and non-diagnostic regions by only considering discriminative patches. Although successful, this technique would likely fail if presented with a small unrepresentative dataset. As a result of the above shortcomings, we present a method for non-small cell lung cancer classification, that primarily focuses on the diagnostic areas within the image for determining the cancer type. In section Network Architecture, we describe the deep learning framework for patch-based classification. In section Extraction of Statistical and Morphological Features and Random Forest Regression Model we describe the random forest regression model for classifying a whole slide image as LUAD or LUSC. A high-level overview of the classification framework can be viewed in Figure 4.\n\n\nNetwork Architecture\n\nInspired by the success of ResNet (He et al., 2016b) in imagerecognition tasks (Huang et al., 2017), we implemented a deep neural network with residual blocks at its core to classify NSCLC input patches. This network architecture is a variant of ResNet50, as described by He et al. (2016b), but we use a 3 \u00d7 3 kernel as opposed to a 7 \u00d7 7 kernel during the first convolution and reduce the number of parameters throughout the network. Using a 3 \u00d7 3 kernel is important in this domain because a smaller receptive field is needed to locate small features that are common in histology images. Reducing the number of parameters allows the network to be more generalized and reduces the possibility of over-fitting. In order to reduce the number of parameters, we modified ResNet50 (He et al., 2016b) by reducing the number of residual blocks throughout the network so that we had 32 layers as opposed to 50. Due to the high variability between images, and therefore between the training and validation set, consideration for preventing over-fitting is crucial. Figure 5 gives an overview of the network architecture.\n\nOnce training was complete, we selected the optimal epoch corresponding to the greatest average validation accuracy and processed patches from each test WSI. This resulted in three probability maps; one for each class.\n\n\nExtraction of Statistical and Morphological Features\n\nFor classifying each WSI as either lung adenocarcinoma or lung squamous cell carcinoma, we extracted features from both the LUAD and LUSC probability maps. We explored two post processing techniques: max voting and a random forest  regression model. Max voting simply assigns the class of the WSI to be class with the largest number of positive patches in its corresponding probability map. Therefore, max voting only requires the positive patch count for both the LUAD and LUSC probability maps in order to make a classification. For the random forest regression model, we extracted 50 statistical and morphological features from both the LUAD and LUSC training probability maps and then selected the top 25 features based on class separability. We gained the training probability maps by processing each training WSI with a late epoch. This ensured that the network had over-fit to the training data and gave a good segmentation of LUAD and LUSC diagnostic regions. In other words, using this method allowed us to transition from a non-exhaustive to an exhaustive labeled probability map. Once the model was trained with these features, they were then input as features into the random forest regression model. Statistical features that were extracted included: mean, median, and variance of the probability maps. We also calculated the ratio between the LUAD and LUSC probability maps. Morphological features that were extracted included the size of the top five connected components at different thresholds.\n\n\nRandom Forest Regression Model\n\nAn ensemble method is a collection of classifiers that are combined together to give improved results. An example of such an ensemble method is a random forest, where multiple decision trees are combined to yield a greater classification accuracy. Decision trees continuously split the input data, according to a certain parameter until a criterion is met. Specifically, a random forest regression model fits a number of decision trees on various sub-samples of the data and then calculates the mean output of all decision trees. We optimized our random forest model by selecting an ensemble of 10 bagged trees, randomly selecting one third of variables for each decision split and setting the minimum leaf size as 5. We finally selected a threshold value to convert the output of the random forest regression model into a binary value, indicating whether the WSI was LUAD or LUSC.\n\n\nRESULTS\n\n\nDigital Pathology Challenge and Datasets\n\nWe organized the MICCAI 2017 digital pathology challenge to provide a venue for comparing algorithms using a common, curated set of datasets and help in advancing algorithm development in digital pathology. The 2017 challenge consisted of two sub-challenges; segmentation of nuclei in tissue images and the classification of whole slide tissue images (WSIs). It used tissue images obtained from patients with non-small cell lung cancer (NSCLC), head and neck squamous cell carcinoma (HNSCC), glioblastoma multiforme (GBM), and lower grade glioma (LGG) tumors. These cancer types are complex and deadly diseases accounting for a large number of diagnostic patient deaths in spite of application of various treatment strategies.\n\n\nSegmentation of Nuclei in Images\n\nIn this sub-challenge, challenge participants were asked to apply automated algorithms to detect and segment all of the nuclei in a set of tissue images. The tissue image dataset consisted of image tiles extracted from whole slide tissue images. Image tiles were used instead of whole slide tissue images because of the significant time and resource cost of manually and accurately segmented nuclei a WSI. In our experience, a WSI may have hundreds of thousands to millions of nuclei. It would be infeasible to generate a ground truth dataset from even a single WSI, let alone from tens of WSIs. In addition, processing a WSI for nucleus segmentation may require significant computing power. Using tiles instead of WSIs in the challenge reduced computational and memory requirements, as the primary objective of the challenge was to evaluate the accuracy performance of an algorithm.\n\nThis sub-challenge used images from The Cancer Genome Atlas (TCGA) repository (The Cancer Genome Atlas (TCGA), 2018). The image tiles for the training and test sets were selected from a set of GBM, LGG, HNSCC, and NSCLC whole slide tissue images by Pathologists and extracted using Aperio's ImageScope software. The training set and the test set each consisted of 32 image tiles with 8 tiles from each cancer type. We recruited a group of students to manually segment all the nuclei in the image tiles. Each tile was segmented by multiple students using a desktop software called iPhotoDraw (http://iphotodraw.com). The student segmentations were reviewed by Pathologists in review sessions with the students. In the review sessions, the manual segmentations were refined, and a consensus segmentation was generated for each image tile. Then labeled masks were generated to represent manual segmentations. A labeled mask represents each segmented nucleus in an image tile with a different id. All the pixels that are part of the same nucleus are assigned the same id.\n\nThe score of a segmentation output was computed using the DICE coefficient (DICE_1) (Dice, 1945) and a variant of the DICE coefficient which we implemented and called \"Ensemble Dice\" (DICE_2, see Algorithm 1). The DICE coefficient measures overlap between ground truth and algorithm segmentation output but does not take into splits and merges. A \"Split\" is the case in which the human segments a region in a single nucleus, but the algorithm segments the same region in multiple nuclei. A \"Merge\" is the case in which the algorithm segments a region in a single nucleus, but the human segments the same region in multiple nuclei. With the DICE coefficient, an algorithm that segments two touching (or overlapping) nuclei as a single object will have the same DICE_1 value as an algorithm that correctly segments the nuclei as two separate objects. DICE_2 was implemented to capture mismatch in the way ground truth and an algorithm segmentation of an image region are split. The pseudo-code for DICE_2 is given below.\n\nHere, Q and P are the sets of segmented objects (nuclei). The two DICE coefficients were computed for each image tile in the test dataset. The score for the image tile was calculated as the average of the two dice coefficients. The score for the entire test dataset was computed as the average of the scores of all the image tiles.\n\nThe images and image patches for the challenges were selected by the pathologists to have a representative set of cases (relatively easy and harder cases in segmentation for example). Generation of manually annotated, accurate datasets for training, and test purposes is labor-intensive work and requires involvement of pathologists, whose time is limited and expensive. Thus, such datasets are relatively small. While there are some recent approaches for generation of synthetic datasets (e.g., Hou et al., 2017;Mahmood et al., 2018), manually annotated datasets continue to represent gold standard data for training and testing, and the paucity of large, manually annotated datasets remains to be a reality. Hence, this represents another challenge that automated algorithms have to address. One of the limitations of \n\n\nClassification of Whole Slide Tissue Images\n\nThis sub-challenge used images from NSCLC cases. All the images were also obtained from whole slide tissue images in TCGA repository. Each whole slide tissue image stored in the TCGA repository has diagnostic information about the category of cancer tumor (e.g., gbm, lgg, ovarian) as well as associated clinical outcome data and genomics data. The images were reviewed and selected by a pathologist. In the NSCLC cases, the images were selected from NSCLC adeno (LUAD) and NSCLC squamous cell (LUSC) cases. Each case in the dataset had one image-so a classification of images would correspond to a classification of cases. Challenge participants were asked to apply their algorithms to classify each image as NSCLC adeno or NSCLS squamous cell. The training dataset had a total of 32 cases; 16 LUAD and 16 LUSC cases. The test dataset had a total of 32 cases with 16 LUAD and 16 LUSC cases. The images were made available in the original file format (i.e., Aperio svs format). The original TCGA filename of each image was mapped to a generic filename (i.e., image1.svs, image2.svs, etc.). The label image and image metadata showing the TCGA case id were removed from the image files. Ground truth was supplied for the training images that gave the cancer type of each WSI, whereas this ground truth was held back for the test images. The score of an analysis algorithm was computed as the number of correctly classified cases divided by the total number of cases.\n\n\nExperimental Evaluation of Deep Learning Method for Segmentation of Nuclei\n\nFrom the original 32 training image tiles, with no additional preprocessing steps, multiple patches (\u223c100 per image) of size 200 \u00d7 200 are extracted. Three training datasets are generated ( Table 2). By sliding a window with a step of 54 pixels and During training, data augmentation is applied as follows: (1) a random vertical and horizontal shift in a range of [\u22120.05, 0.05] with respected to the patch's width and height (2) a random rotation in a range of [\u221245 \u2022 , 45 \u2022 ] degree (3) a random vertical and horizontal flipping with probability 0.5 (4) a random shear with intensity in a range of [\u22120.4\u03c0, 0.4\u03c0] (5) a random resizing with a ratio in a range of [0.6, 2.0]. This augmentation is to address variations of nuclei in contrast, shapes, and etc. that are often observed in pathology images. This is known to be helpful in coping with the natural variations present in the images as well as ensuring the robustness of the network (Ronneberger et al., 2015). Following the augmentation, the center region of size 102 \u00d7 102 is extracted prior to being fed into the network (Figure 6). Augmentation is performed 3 times per patch.\n\nUsing the generated training data above, DRAN is trained via Adam optimizer with default parameter values (\u03b21 = 0.9, \u03b22 = 0.999, \u01eb = 1e-8). A mini batch size of 32 is maintained throughout the whole training process. L2 regularization loss is also applied with a factor of 1.0e-5 to improve the generalizability of the proposed network. K.He initialization (He et al., 2015) is utilized to initialize the weights for the convolutional layers in the expanding path. Training is performed in two phases. In the first phase (35 epochs), the pretrained weights of preactivated ResNet50 are loaded into the contracting path and is kept frozen (no update on weights), i.e., only the expanding path is trainable in this phase. The learning rate is initially set to 1.0e-4, then changes to 5.0e-5, 1.0e-5, 7.5e-6, and 5.0e-6 at the 0th, 1st, 15th, 25th, and 35th epoch, respectively. In this phase, the network is trained with NBL dataset for nuclei blob detection and NBD dataset for nuclei boundary detection. In the second phase (40 epochs), the contracting path is unfrozen, that is, the whole network becomes trainable. For nuclei blob detection, both NBL and SN datasets are used to further refine the network. Only NBD dataset is utilized for nuclei boundary detection. In addition, differing penalties for the loss function are imposed to alleviate the heavy bias in NBD dataset; each border pixel has a weight of 5.0 and miss-classifying a border pixel as background gains a weight 6.0. Background pixels have 1.0 weight while miss-classifying them is penalized with 4.0.\n\nOn the other hand, with the same training data as DRAN, the training procedure of the multiscale model is detailed as followed: each DRAN branch of the MDRAN is loaded with the pretrained DRAN weights that are obtained from the procedure described above and is kept frozen. The network then proceeds to train decoder4 for 10 epochs with the learning rate of 1.0e-4. Afterwards, the expanding path of DRANs is unfrozen and finetuned for additional 35 epochs while the learning rate is set to be 1.0e-4, 1.0e-5, and 1.0e-6 at the 1st epoch, 15th epoch, and 30th epoch, respectively.\n\nOverall, utilizing NBL+SN dataset, MDRAN BL was trained for nuclei blob detection. DRAN BD was trained on NBD dataset for nuclei boundary detection. Combining the two models (MDRAN BL +DRAN BD ), nuclei segmentation was performed. Table 3 shows the segmentation results on the test set. Our method (MDRAN BL +DRAN BD ) achieved 0.862 DICE_1, 0.703 DICE_2, and the average score of 0.783. The effect of the multiscale aggregation (MDRAN BL ) was examined. Using a single scale nuclei segmentation method (DRAN BL +DRAN BD ), we obtained 0.853 DICE_1, 0.701 DICE_2, and the average score of 0.777, worse than those of the multiscale aggregation. In a head-to-head comparison of the test set, the multiscale aggregation substantially improved the segmentation performance, especially on three test images that were scanned at 20x magnification (Figure 7). As for other test images, scanned at 40x magnification, the multiscale aggregation, in general, slightly outperformed the single scale method. This suggests that the multiscale aggregation, in particular, aids in improving the segmentation of (relatively) smaller nuclei. Figure 8 shows the segmentation results by the multiscale aggregation and single scale method; the single scale method missed several small nuclei that were, however, identified by the multiscale aggregation. Notably, a huge discrepancy between DICE_1 and DICE_2 was observed for several test images (Figure 7). Upon closer inspection, we found that these are mainly due to staining variation and instability as well as densely overlapping nuclei. As shown in Figure 9, the identified nuclei boundaries are often fragmented and imperfect, leading to inaccurate segmentation of the overlapping nuclei. This indicates that advanced and sophisticated touching nuclei separation method may hold a great potential for improving the segmentation performance.\n\n\nClassification of Whole Slide Images\n\nWe used a total of 64 Hematoxylin and Eosin (H&E) NSCLC WSIs that were split into 32 training and 32 test images. We had an even breakdown of NSCLC images in both the training and the test set, giving a total of 32 LUAD slides and 32 LUSC slides. We divided our dataset so that we had 24 WSIs for training and 8 for validation, with 4 validation images taken from LUAD and LUSC respectively. We extracted a 3-class dataset comprising of patches of size 256 \u00d7 256 at 20\u00d7 magnification, from non-exhaustive labeled regions, confirmed by an expert pathologist (AK). This 3-class dataset consisted of LUAD, LUSC and non-diagnostic areas (ND). LUAD diagnostic regions within the slide consisted of: tumor; growth pattern structures and tumor stroma. LUSC diagnostic regions consisted of: tumor; keratin pearls and tumor stroma. Non-diagnostic regions included: fat; lymphocytes; blood vessels; alveoli; red blood cells; normal stroma; cartilage, and necrosis. We considered necrosis to be non-diagnostic because, FIGURE 7 | Head-to-head comparison between MDRAN BL and DRAN BL on the test set. Test images are ordered by the ascending order of MDRAN DICE_1. The shaded area indicates that the images were scanned at 20x magnification.  despite LUSC generally having more necrotic areas than LUAD, it is not indicative of lung squamous cell carcinoma on a patchby-patch basis. In this case, it was particularly important to incorporate non-diagnostic areas because there were no normal cases within the dataset. Overall, our network is optimized on 65,788 training image patches.\n\nThere was a high level of stain variation between all images, due to images being acquired from different centers. To counter this stain variability, we applied Reinhard et al. (2001) stain normalization to all images by mapping each image to the statistics of a pre-defined target image. During training we performed random crop, flip and rotation data augmentation to make the network invariant to these transformations. After performing a random crop to all input patches, we were left with a patch size of 224 \u00d7 224.\n\nAn increase in the amount of labeled data coupled with a surge in computing power has allowed deep convolutional neural networks to achieve state-of-the-art performance in computer vision tasks. The hierarchical architecture of such networks allows them to have a strong representational power, where the complexity of learned features increases with the depth of the network. The proposed network f is a composition of a sequence of L functions of layers (f 1 , . . . , f L ) that maps an input vector x to an output vector y, i.e.,\ny = f (x; w 1 , . . . , w L ) = f L (. ; w L ) \u2022 f L\u22121 (. ; w L\u22121 ) \u2022 . . . \u2022 f 2 (. ; w 2 ) \u2022 f 1 (. ; w 1 )(1)\nwhere w L is the weight and bias vector for the L th layer f L .\n\nIn practice, f L most commonly performs one of the following operations: (a) convolution with a set of filters; (b) spatial pooling; and (c) non-linear activation.\n\nGiven a set of training data (x (i) , y (i) ) , where i ranges from 1 to N. We can estimate the vectors w 1 . . . w L by solving: where l is the defined loss function. We perform numerical optimization of (2) conventionally via the back-propagation algorithm and stochastic gradient descent methods. In addition to the above operations, residual networks (ResNets) (He et al., 2016b) have recently been proposed that enable networks to be trained deeper and as a result, benefit from a greater accuracy. Current-state-of-the-art networks (Simonyan and Zisserman, 2014;Szegedy et al., 2015;He et al., 2016b;Huang et al., 2017) indicate that network depth is of crucial importance, yet within conventional CNNs, accuracy gets saturated and then degrades rapidly as the depth becomes significantly large. The intuition behind a residual network is that it is easier to optimize the residual mapping than to optimize the original unreferenced mapping. Residual blocks are the core components of ResNet and consist of a feed-forward skip connection, that performs identity mapping, without adding any extra parameters. These connections propagate the gradient throughout the model, which in turn enables the network to be trained deeper, often achieving greater accuracy. Table 4 summarizes the experiments we carried out for classification of input patches into LUAD, LUSC, and ND. We choose to train the specified networks, due to their state-ofthe-art performance in recent image recognition tasks (Deng et al., 2009). During training, all networks quickly over-fit to the training data. This was because of two reasons: (i) The networks architectures that were used have been optimized for large-scale computer vision tasks with millions of images and thousands of FIGURE 10 | Test WSIs with overlaid probability maps. Blue/purple indicates a region classified as diagnostic LUAD, green indicates a region classified as diagnostic LUSC, yellow/orange refers to a region classified as non-diagnostic.\nargimn W 1 ,...,W L 1 N N i=1 l(f x (i) ; w 1 , . . . , w L , y (i)(2)\nclasses; (ii) We have a fairly limited training set size. Due to the size of our dataset, (iii) it is difficult to avoid over-fitting, given a sufficient number of model parameters. Therefore, we modify the network architecture to counter the problem of over-fitting by reducing the number of layers. We make a modification to the original implementation of ResNet by reducing the number of residual units, such that we only have a total of 32 layers within the model. Modification of ResNet50 to give ResNet32 helped alleviate the problem of over-fitting and gave the best patch-level performance. Despite only achieving 0.4% greater accuracy than InceptionV3, ResNet32 resulted in a significantly greater average LUAD and LUSC patch-level accuracy. The average LUAD and LUSC patch-level accuracy for InceptionV3 was 0.678, whereas the average accuracy for ResNet32 was 0.776. As a consequence of the superior patch-level performance, we chose to use ResNet32 for processing images in the test set. Figure 10 shows four test WSIs with their overlaid probability maps. Green regions show regions classified as LUSC, blue/purple regions show regions classified as LUAD and yellow/orange regions show regions classified as ND. Table 5 shows the overall accuracy for NSCLC WSI classification, as processed by the challenge organizers. We observe that using the random forest regression model with statistical and morphological features from the labeled WSI increases the classification accuracy. Max voting is sufficient when either LUAD or LUSC is a dominant class within the \n\n\nMethod Accuracy\n\nResNet32-MV 0.78\n\nResNet32-RF 0.81\n\nResNet32-MV refers to classifying input patches using ResNet32, then using majority voting as a post processing classification technique. ResNet32-RF refers to classifying input patches using ResNet32 and then using a random forest regression model as a post processing technique for classification.\n\nlabeled WSI, but when there is no obvious dominant class, the random forest regression model increases performance. This is because the features used as input to the random forest model are more informative than simply using a voting scheme and can therefore better differentiate between each cancer type. The proposed method achieves good accuracy with a score of 0.81. Given the limitation of the dataset, it is clear that classifying NSCLC WSIs into diagnostic and non-diagnostic regions is of crucial importance. This is particularly important for this specific implementation because the training set did not contain any normal cases. Without the consideration of non-diagnostic areas, the algorithm would be forced to make a prediction for non-informative image tiles. Furthermore, it is evident that the analysis of the morphology of classified regions can empower the classification of the whole-slide image and is superior to a max voting approach. The consideration of contextual information can provide additional assistance in classification tasks within computational pathology (Agarwalla et al., 2017;Bejnordi et al., 2017). For example, growth patterns in LUAD cases and how the tumor grows with the stroma is of significant importance when classifying NSCLC cases. These patterns are often very hard to visualize in a 224 \u00d7 224 patch at 20\u00d7 resolution. In future work, developing our proposed network to accurately include more contextual information may improve patch-level accuracy and therefore overall classification accuracy. To develop this work, we also aim to use a larger dataset so that our patch-level classifier is able to extract more representative features for subsequent NSCLC classification.\n\n\nDISCUSSION\n\nUnderstanding the inter-relationship between morphology and molecular mechanisms is a central component of research targeting complex diseases, in particular cancer. There are, for example, latent and observable changes in tissue structure and function at the onset of cancer and over the course of its progression. Traditionally whole slide tissues are manually examined under a high-power light microscope to render a diagnosis. This manual process is laborious, limiting the number of tissue samples that can be used in a study. Digital Pathology enables quantitative studies of these changes and underlying disease mechanisms at the sub-cellular scales. Integrating information gleaned from analysis of Pathology imaging data into the landscape of the entire spectrum of clinical information can help drive both disease specific and patient specific information which can be used to drive high risk high reward cancer trials to better results faster.\n\nDeep learning methods have garnered a lot of interest in the computer vision and image analysis communities in recent years. They have shown superior performance to statistics-based methods and other machine learning techniques in a variety of image analysis tasks. There is a growing set of deep learningbased analysis pipelines in the digital pathology domain. In this paper we presented two novel pipelines for analysis of tissue images. These methods target two core steps in tissue image analysis; segmentation of nuclei/cells and classification of images. The multiscale deep residual aggregation network is designed to segment nuclei in images. The experimental evaluation suggests that (1) the multiscale aggregation aids in improving the segmentation of (relatively) smaller nuclei and (2) advanced and sophisticated touching nuclei separation methods may hold a great potential for improving the segmentation performance in tissue specimens with discernable staining variation and instability as well as densely overlapping nuclei. The second method implements a deep neural network that classifies image patches in whole slide tissue images of nonsmall cell lung cancer tissue specimens as lung adenocarcinoma, lung squamous cell carcinoma or non-diagnostic regions. The experimental results show that use of a deep learning network and a random forest regression model, which uses statistical and morphological features extracted from images, can achieve good classification accuracy.\n\nWhile the methods presented in this work and others have demonstrated promising results, there are several other considerations and challenges in the development of such methods and in the integration of information obtained from automatic segmentation and grading methods in research and clinical workflows. Human involvement is still a critical component in effective application of deep learning and other automated techniques in biomedical image analysis. Human expert review of image analysis results is important not only for quality assessment and interpretation of analysis results but also for iterative or active improvement of segmentation and classification models (and producing increasingly more accurate and robust results) (Holzinger, 2016;Holzinger et al., 2018). One of the challenges to human-in-the-loop in pathology image analysis is the sheer volume of data. A whole slide tissue image can contain more than one million nuclei and hundreds of thousands of image patches. Even a moderate size research project with thousands of images can generate a very large volume of classification and segmentation data. It is not feasible for human experts to sift through these data for quality control and iterative model improvement. Some recent projects (e.g., Beluch et al., 2018) are looking at deep learning methods for reducing interactive/iterative labeling costs. These methods aim to estimate uncertainty in analysis results and intelligently select subsets of data to reduce number of iterations while achieving a certain level of analysis accuracy. Some projects (e.g., Wen et al., 2017) are investigating the use of machine learning methods to assist in quality assessment of analysis results generated from automated methods. We believe that integrated approaches that combine deep/machine learning methods for analysis with deep/machine learning methods for quality assessment and data selection for iterative model refinement will provide an effective platform in pathology image analysis. These methods will facilitate application of a human expert's knowledge to improve analysis results while reducing manual load on the human expert.\n\nA more challenging task is the explainability of deep learning models. In most cases deep learning methods are treated as black-boxes. Even though some mathematical foundations of deep learning have been developed, explaining how a deep learning model arrived at a particular decision remains an open problem. We expect that explainability will increasingly be a central challenge to deeper integration of deep learning and other automated methods in clinical settings. This relatively new topic in machine/deep learning in biomedicine is rapidly gaining popularity; there are an increasing number of recent works on defining explainable AI and seeking approaches to realize it (Do\u0161ilovi\u0107 et al., 2018;Holzinger, 2018;Sadeghi et al., 2018).\n\nAnother important consideration in the development of more accurate and robust automated methods is the systematic evaluation of different strategies and contributions from different research groups. We argue that advances in the accuracy, robustness, and efficiency of digital pathology image analysis methods and pipelines will be accelerated through engagement of the community of method developers. To this end, image analysis challenges, such as the \"Computational Precision Medicine Digital Pathology Challenge\" held at the MICCAI 2017 conference, play a critical role.\n\nAs we have presented in this paper through two novel approaches, deep learning methods are improving in prediction performance and getting better at dealing with relatively small training datasets. However, there are no one-size-fits-all solutions and efficient integration of human expert knowledge for model refinement, quality control, and interpretation/explanation is very important to more effective application of deep learning methods in biomedical research.\n\n\nDATA AVAILABILITY\n\nThe datasets generated for this study are available on request to the corresponding author.\n\n\nAUTHOR CONTRIBUTIONS\n\nQV, MT, and JK proposed, implemented, and evaluated the deep learning method for nucleus segmentation algorithm in section Segmentation of Nuclei by a Deep Learning Method. SG, MS, TQ, NK, SK, and NR proposed, implemented, and evaluated the two-part method for whole slide tissue classification in section Classification of Whole Slide Images by a Two-Part Automated Method. KF, TK, JK-C, and JS are the main organizers of the MICCAI 2017 Digital Pathology Challenge. TK, JS, TZ, and RG put together the challenge datasets, supervised the generation of ground truth data, and developed the ensemble dice index scoring method. All authors contributed text to the manuscript and edited it. \n\nFIGURE 1 |\n1Overview of the nuclei segmentation procedure. DRAN BL and DRAN BD are the models for nuclei blob detection and boundary detection, respectively.\n\nFIGURE 2 |\n2Architecture of a deep residual aggregation network (DRAN) for nuclei segmentation. Modified pre-activated ResNet50 is used for the contracting path.\n\nFIGURE 3 |\n3Multiscale Deep Residual Network (MDRAN) architecture. MDRAN composes of 3 DRANs at 3 scales (x0.5, x1.0, x2.0) and a decoder (in dash rectangle), aggregating 3 scales together and generating a segmentation map at x1.0 scale. In the decoder, the convolution block [128, 5 \u00d7 5, 256] denotes [128 input channels, 5 \u00d7 5 kernel, 256 output channels].\n\nFIGURE 4 |\n4Overview of the NSCLC classification framework. (A) Workflow for training the neural network to classify input patches as either non-diagnostic (ND), lung adenocarcinoma (LUAD), or lung squamous cell carcinoma (LUSC). (B) Workflow for processing the WSIs within the test set to obtain probability maps for each class. (C) Workflow for the random forest regression model. Features are extracted from LUAD and LUSC probability maps and then fed as input into the random forest model. SN stands for stain normalization by method ofReinhard et al. (2001).\n\nFIGURE 5 |\n5The deep convolutional neural network. (A) Network architecture, (B) residual unit. Within the residual block, \u2295 refers to the summation operator. (C) Key highlighting each component within the workflow. Note, the number within each convolutional operator denotes the output depth. Above each residual block we denote how many residual units are used.\n\nFIGURE 6 |\n6Image patch generation. To avoid zero-padding in augmentation, a patch of size 200 \u00d7 200 is first provided. Subsequently, the center region of 102 \u00d7 102 is cropped and fed into the network as input. For an input of size 102 \u00d7 102, the network provides a segmentation map of size 54 \u00d7 54.\n\nFIGURE 8 |\n8Examples of nuclei segmentation via the multiscale aggregation (MDRAN BL +DRAN BD ) and single scale (DRAN BL +DRAN BD ) approach. The images from top to bottom are the 1st, 11th, 20th, and 25th image tile in the test set.\n\nFIGURE 9 |\n9Examples of correct and incorrect nuclei segmentation. Our method (Bottom) is able to distinguish the boundary of the non-highly overlapping nuclei fairly well but (Top) fails on the highly overlapping nuclei with disproportionate stains.\n\nFUNDING\nThis work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIP) (No. 2016R1C1B2012433), in part by 1U24CA180924-01A1 from the National Cancer Institute, R01LM011119-01 and R01LM009239 from the U.S. National Library of Medicine.\n\nTABLE 1 |\n1Details of three decoders.Decoder3 \nDecoder2 \nDecoder1 \n\n\uf8ee \n\n\uf8ef \n\uf8ef \n\uf8f0 \n\n5x5, 1024, \n\n3x3, 1024, C=256 \n\n1x1, 512, \n\n\uf8f9 \n\n\uf8fa \n\uf8fa \n\uf8fb \n\n\uf8ee \n\n\uf8ef \n\uf8ef \n\uf8f0 \n\n5x5, 512, \n\n3x3, 512, C=128 \n\n1x1, 256, \n\n\uf8f9 \n\n\uf8fa \n\uf8fa \n\uf8fb \n\n\uf8ee \n\n\uf8ef \n\uf8ef \n\uf8f0 \n\n5x5, 256, \n\n3x3, 256, C=64 \n\n1x1, 128, \n\n\uf8f9 \n\n\uf8fa \n\uf8fa \n\uf8fb \n\nC is the number of paths (or groups). [5 \u00d7 5, 1,024] denotes a kernel size of 5 \u00d7 5 and \n1,024 channels. \n\n\n\n\nAlgorithm 1 Computing Ensemble Dice Coefficient (DICE_2) Input: An image contains a set of segmented nuclear Q where each nuclei is indexed by q and a second image contains a sets of segmented nuclear P where each nuclei is indexed by p. DICE_2 \u2190 2 * IntersectionArea/TotalMarkupArea relatively small training and test datasets is they may not fully capture batch-effects.Output: Ensemble Dice Score DICE_2 \n1: Initialize total intersection and markup pixels count: \nIntersectionArea \u2190 0; \nTotalMarkupArea \u2190 0; \n2: foreach q in Q do: \n\n3: \n\nforeach p in P do: \n\n4: \n\nif q intersects p then: \n\n5: \n\nIntersectionArea \n\u2190 \nIntersectionArea \n+ \nAreaOfOverlap(q,p) \nTotalMarkupArea \u2190 TotalMarkupArea + (Area(q) + \nArea(p)) \n\n6: \n\nend if \n\n7: \n\nend for \n\n8: \n\nend for \n9: \n\nTABLE 2 |\n2Generation of three datasets from the original 32 image tiles. NBL) dataset and used for nuclei blob detection. Nuclei Boundary (NBD) dataset is generated by centering each nucleus at the center of each patch, producing 2,785 patches that are used for nuclei boundary detection. Small Nuclei (SN) dataset is the duplicate dataset of NBL that only contains nuclei blob patches possessing \u2264 50% nuclei pixels. SN dataset is only used for training DRAN for nuclei blob detection.Dataset \nNumber of \npatches \n\nExtraction details \n\nNuclei Blob \n(NBL) \n\n4,732 \n\u2022 Window-slide cropping with a step of 54 \npixels per image tile \n\u2022 Random-cropping per image tiles, 30 times \n\nNuclei \nBoundary \n(NBD) \n\n2,785 \n\u2022 Each patch centering a single nucleus \n\u2022 Nuclei near image tile's edges are ignored \n\nSmall Nuclei \n(SN) \n\n14,552 \n\u2022 Duplicate patches from NBL which contain \n\u226450% nuclei pixels in the center region of \n54 \u00d7 54, 3 times \n\u2022 Only for training nuclei blob detection \n\nrandom cropping, 4,732 patches are generated, designated as \nNuclei Blob (\n\nTABLE 3 |\n3Segmentation of nuclei performance.Method \nDICE_1 \nDICE_2 \nAverage score \n\nDRAN BL +DRAN BD \n0.8532 \n0.7010 \n0.777 \n\nMDRAN BL +DRAN BD \n0.8620 \n0.7033 \n0.783 \n\n\n\nTABLE 4 |\n4Patch-level accuracy.Network \nResolution \nLUAD \nLUSC \nND \nAverage \n\nVGG \n20x \n0.634 \n0.663 \n0.826 \n0.708 \n\nInceptionV3 \n20x \n0.623 \n0.733 \n0.924 \n0.760 \n\nResNet50 \n20x \n0.601 \n0.597 \n0.889 \n0.695 \n\nResNet32 \n20x \n0.702 \n0.849 \n0.742 \n0.764 \n\nLUAD refers to lung adenocarcinoma, LUSC refers to lung squamous cell carcinoma, ND \nrefers to non-diagnostic area of interest. \n\n\n\nTABLE 5 |\n5Overall WSI classification accuracy.\nFrontiers in Bioengineering and Biotechnology | www.frontiersin.org\nApril 2019 | Volume 7 | Article 53\n\nRepresentationaggregation networks for segmentation of multi-gigapixel histology images. A Agarwalla, M Shaban, N M Rajpoot, arXiv:1707.08814arXiv preprintAgarwalla, A., Shaban, M., and Rajpoot, N. M. (2017). Representation- aggregation networks for segmentation of multi-gigapixel histology images. arXiv preprint arXiv:1707.08814.\n\nIntegrating segmen-tation with deep learning for enhanced classification of epithelial and stromal tissues in H&E images. Z Al-Milaji, I Ersoy, A Hafiane, K Palaniappan, F Bunyak, 10.1016/j.patrec.2017.09.015Pattern Recognit. Lett. 119Al-Milaji, Z., Ersoy, I., Hafiane, A., Palaniappan, K., and Bunyak, F. (2017). Integrating segmen-tation with deep learning for enhanced classification of epithelial and stromal tissues in H&E images. Pattern Recognit. Lett. 119, 214-221. doi: 10.1016/j.patrec.2017.09.015\n\nSegNet: a deep convolutional encoder-decoder architecture for image segmentation. V Badrinarayanan, A Kendall, R Cipolla, 10.1109/TPAMI.2016.2644615IEEE Trans. Pattern Anal. Mach. Intell. 1Badrinarayanan, V., Kendall, A., and Cipolla, R. (2017). SegNet: a deep convolutional encoder-decoder architecture for image segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 1, 2481-2495. doi: 10.1109/TPAMI.2016.2644615\n\nContext-aware stacked convolutional neural networks for classification of breast carcinomas in whole-slide histopathology images. B E Bejnordi, G C A Zuidhof, M Balkenhol, M Hermsen, P Bult, B Van Ginneken, 10.1117/1.JMI.4.4.044504J. Med. Imag. 444504Bejnordi, B. E., Zuidhof, G. C. A., Balkenhol, M., Hermsen, M., Bult, P., van Ginneken, B., et al. (2017). Context-aware stacked convolutional neural networks for classification of breast carcinomas in whole-slide histopathology images. J. Med. Imag. 4:044504. doi: 10.1117/1.JMI.4.4.044504\n\nThe power of ensembles for active learning in image classification. W H Beluch, T Genewein, A N\u00fcrnberger, J M K\u00f6hler, 10.1109/CVPR.2018.00976Proceedings of the IEEE Conference on Computer Vision and Pattern, Recognition. the IEEE Conference on Computer Vision and Pattern, RecognitionSalt Lake City, UTBeluch, W. H., Genewein, T., N\u00fcrnberger, A., K\u00f6hler, J. M. (2018). \"The power of ensembles for active learning in image classification, \" in Proceedings of the IEEE Conference on Computer Vision and Pattern, Recognition (Salt Lake City, UT), 9368-9377. doi: 10.1109/CVPR.2018.00976\n\nDCAN: deep contour-aware networks for object instance segmentation from histology images. H Chen, X Qi, L Yu, Q Dou, J Qin, P.-A Heng, 10.1016/j.media.2016.11.004Med. Image Anal. 36Chen, H., Qi, X., Yu, L., Dou, Q., Qin, J., and Heng, P.-A. (2017). DCAN: deep contour-aware networks for object instance segmentation from histology images. Med. Image Anal. 36, 135-146. doi: 10.1016/j.media.2016.11.004\n\nAn assessment of imaging informatics for precision medicine in Cancer. C Chennubhotla, L P Clarke, A Fedorov, D Foran, G Harris, E Helton, 10.15265/IY-2017-041Yearbook Med. Informatics. 26Chennubhotla, C., Clarke, L. P., Fedorov, A., Foran, D., Harris, G., Helton, E., et al. (2017). An assessment of imaging informatics for precision medicine in Cancer. Yearbook Med. Informatics 26, 110-119. doi: 10.15265/IY-2017-041\n\nPanCancer insights from The Cancer Genome Atlas: the pathologist's perspective. L A Cooper, E G Demicco, J H Saltz, R T Powell, A Rao, A J Lazar, 10.1002/path.5028J. Pathol. 244Cooper, L. A., Demicco, E. G., Saltz, J. H., Powell, R. T., Rao, A., and Lazar, A. J. (2018). PanCancer insights from The Cancer Genome Atlas: the pathologist's perspective. J. Pathol. 244, 512-524. doi: 10.1002/path.5028\n\nClassification and mutation prediction from non-small cell lung cancer histopathology images using deep learning. N Coudray, A L Moreira, T Sakellaropoulos, D Fenyo, N Razavian, A Tsirigos, 10.1038/s41591-018-0177-5Nat. Med. 175Coudray, N., Moreira, A. L., Sakellaropoulos, T., Fenyo, D., Razavian, N., and Tsirigos, A. (2018). Classification and mutation prediction from non-small cell lung cancer histopathology images using deep learning. Nat. Med. 17:5. doi: 10.1038/s41591-018-0177-5\n\nImageNet: a large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 10.1109/CVPR.2009.5206848IEEE Confer. Comput. Vis. Pattern Recogn. 20Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: a large-scale hierarchical image database. IEEE Confer. Comput. Vis. Pattern Recogn. 20, 248-255. doi: 10.1109/CVPR.2009.5206848\n\nMeasures of the amount of ecologic association between species. L R Dice, 10.2307/1932409Ecology. 26Dice, L. R. (1945). Measures of the amount of ecologic association between species. Ecology 26, 297-302. doi: 10.2307/1932409\n\nExplainable artificial intelligence: a survey. F K Do\u0161ilovi\u0107, M Br\u010di\u0107, N Hlupi\u0107, 10.23919/MIPRO.2018.8400040IEEE 2018 41st Int. Convention Inform. Commun. Technol. Electron. Microelectron. 21Do\u0161ilovi\u0107, F. K., Br\u010di\u0107, M., and Hlupi\u0107, N. (2018). Explainable artificial intelligence: a survey. IEEE 2018 41st Int. Convention Inform. Commun. Technol. Electron. Microelectron. 21, 210-215. doi: 10.23919/MIPRO.2018.8400040\n\nDigital imaging in pathology: whole-slide imaging and beyond. F Ghaznavi, A Evans, A Madabhushi, M Feldman, 10.1146/annurev-pathol-011811-120902Ann. Rev. Pathol. 8Ghaznavi, F., Evans, A., Madabhushi, A., and Feldman, M. (2013). Digital imaging in pathology: whole-slide imaging and beyond. Ann. Rev. Pathol. 8, 331-359. doi: 10.1146/annurev-pathol-011811-120902\n\nSAMS-Net: stain-aware multi-scale network for instance-based nuclei segmentation in histology images. S Graham, N Rajpoot, 10.1109/ISBI.2018.8363645IEEE 15th International Symposium on. IEEE. Washington, DCBiomedical Imaging (ISBI 2018)Graham, S., and Rajpoot, N. (2018). \"SAMS-Net: stain-aware multi-scale network for instance-based nuclei segmentation in histology images, \" in Biomedical Imaging (ISBI 2018), 2018 IEEE 15th International Symposium on. IEEE (Washington, DC), 590-594. doi: 10.1109/ISBI.2018.8363645\n\nHistopathological image analysis: a review. M N Gurcan, L E Boucheron, A Can, A Madabhushi, N Rajpoot, B Yener, 10.1109/RBME.2009.2034865IEEE Rev. Biomed. Eng. 2147Gurcan, M. N., Boucheron, L. E., Can, A., Madabhushi, A., Rajpoot, N., and Yener, B. (2009). Histopathological image analysis: a review. IEEE Rev. Biomed. Eng. 2:147. doi: 10.1109/RBME.2009.2034865\n\nHypercolumns for object segmentation and fine-grained localization. B Hariharan, P Arbel\u00e1ez, R Girshick, Malik , J , 10.1109/CVPR.2015.7298642Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBoston, MAHariharan, B., Arbel\u00e1ez, P., Girshick, R., and Malik, J. (2015). \"Hypercolumns for object segmentation and fine-grained localization, \" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Boston, MA), 447-456. doi: 10.1109/CVPR.2015.7298642\n\nDelving deep into rectifiers: surpassing human-level performance on imagenet classification. K He, X Zhang, S Ren, J Sun, 10.1109/ICCV.2015.123Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionSantiagoHe, K., Zhang, X., Ren, S., and Sun, J. (2015). \"Delving deep into rectifiers: surpassing human-level performance on imagenet classification, \" in Proceedings of the IEEE International Conference on Computer Vision (Santiago), 1026-1034. doi: 10.1109/ICCV.2015.123\n\nIdentity mappings in deep residual networks. K He, X Zhang, S Ren, J Sun, 10.1007/978-3-319-46493-0_38European Conference on Computer Vision. ChamSpringerHe, K., Zhang, X., Ren, S., and Sun, J. (2016a). \"Identity mappings in deep residual networks, \" in European Conference on Computer Vision (Cham: Springer), 630-645. doi: 10.1007/978-3-319-46493-0_38\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 10.1109/CVPR.2016.90Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAmsterdamHe, K., Zhang, X., Ren, S., and Sun, J. (2016b). \"Deep residual learning for image recognition, \" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Amsterdam), 770-778. doi: 10.1109/CVPR.2016.90\n\nInteractive machine learning for health informatics: when do we need the human-in-the-loop?. A Holzinger, 10.1007/978-3-319-50478-0Brain Informatics. 3Holzinger, A. (2016). Interactive machine learning for health informatics: when do we need the human-in-the-loop? Brain Informatics 3, 119-131. doi: 10.1007/978-3-319-50478-0\n\nFrom machine learning to explainable AI. A Holzinger, 10.1109/DISA.2018.8490530IEEE 2018 World Symposium Digital Intell. Syst. Mach. 23Holzinger, A. (2018). From machine learning to explainable AI. IEEE 2018 World Symposium Digital Intell. Syst. Mach. 23, 55-66. doi: 10.1109/DISA.2018.8490530\n\nInteractive machine learning: experimental evidence for the human in the algorithmic loop. A Holzinger, M Plass, M Kickmeier-Rust, K Holzinger, G C Cri\u015fan, C M Pintea, 10.1007/s10489-018-1361-5Appl. Intell. 7Holzinger, A., Plass, M., Kickmeier-Rust, M., Holzinger, K., Cri\u015fan, G. C., Pintea, C. M., et al. (2018). Interactive machine learning: experimental evidence for the human in the algorithmic loop. Appl. Intell. 7, 1-4. doi: 10.1007/s10489-018-1361-5\n\nL Hou, A Agarwal, D Samaras, T M Kurc, R R Gupta, J H Saltz, arXiv:1712.05021Unsupervised histopathology image synthesis. Hou, L., Agarwal, A., Samaras, D., Kurc, T. M., Gupta, R. R., and Saltz, J. H. (2017). Unsupervised histopathology image synthesis. arXiv arXiv:1712.05021.\n\nPatch-based convolutional neural network for whole slide tissue image classification. L Hou, D Samaras, T M Kurc, Y Gao, J E Davis, J H Saltz, 10.1109/CVPR.2016.266Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLas Vegas, NVHou, L., Samaras, D., Kurc, T. M., Gao, Y., Davis, J. E., and Saltz, J. H. (2016). \"Patch-based convolutional neural network for whole slide tissue image classification, \" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Las Vegas, NV), 2424-2433. doi: 10.1109/CVPR.2016.266\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, 10.1109/CVPR.2017.243Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHonolulu, HIHuang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (2017). \"Densely connected convolutional networks, \" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Honolulu, HI). doi: 10.1109/CVPR.2017.243\n\nClassification of breast cancer histopathology images based on adaptive sparse support vector machine. M A Kahya, W Al-Hayani, Algamal , Z Y , J. Appl. Math. Bioinformatics. 749Kahya, M. A., Al-Hayani, W., and Algamal, Z. Y. (2017). Classification of breast cancer histopathology images based on adaptive sparse support vector machine. J. Appl. Math. Bioinformatics 7:49.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, Darrell , T , 10.1109/CVPR.2015.7298965Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBoston, MALong, J., Shelhamer, E., and Darrell, T. (2015). \"Fully convolutional networks for semantic segmentation, \" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Boston, MA), 3431-3440. doi: 10.1109/CVPR.2015.7298965\n\nDigital pathology image analysis: opportunities and challenges. A Madabhushi, 10.2217/IIM.09.9Imag. Med. 1Madabhushi, A. (2009). Digital pathology image analysis: opportunities and challenges. Imag. Med. 1:7-10. doi: 10.2217/IIM.09.9\n\nF Mahmood, D Borders, R Chen, G N Mckay, K J Salimian, A Baras, arXiv:1810.00236Deep adversarial training for multi-organ nuclei segmentation in histopathology images. Mahmood, F., Borders, D., Chen, R., McKay, G. N., Salimian, K. J., Baras, A., et al. (2018). Deep adversarial training for multi-organ nuclei segmentation in histopathology images. arXiv arXiv:1810.00236.\n\nAn automated pattern recognition system for classifying indirect immunofluorescence images of hep-2 cells and specimens. S Manivannan, W Li, S Akbar, R Wang, J Zhang, S Mckenna, 10.1016/j.patcog.2015.09.015Pattern Recognit. 51Manivannan, S., Li, W., Akbar, S., Wang, R., Zhang, J., and McKenna, S. (2016). An automated pattern recognition system for classifying indirect immunofluorescence images of hep-2 cells and specimens. Pattern Recognit. 51, 12-26. doi: 10.1016/j.patcog.2015.09.015\n\nAutomatic cell detection and segmentation from H and E stained pathology slides using colorspace decorrelation stretching. M Peikari, A L Martel, Medical Imaging 2016: Digital Pathology. International Society for Optics and Photonics. San Diego, CA979114Peikari, M., and Martel, A. L. (2016). \"Automatic cell detection and segmentation from H and E stained pathology slides using colorspace decorrelation stretching, \" in Medical Imaging 2016: Digital Pathology. International Society for Optics and Photonics (San Diego, CA), 9791:979114.\n\nA cluster-thenlabel semi-supervised learning approach for pathology image classification. M Peikari, S Salama, S Nofech-Mozes, A L Martel, 10.1038/s41598-018-24876-0Sci. Rep. 87193Peikari, M., Salama, S., Nofech-Mozes, S., and Martel AL (2018). A cluster-then- label semi-supervised learning approach for pathology image classification. Sci. Rep. 8:7193. doi: 10.1038/s41598-018-24876-0\n\nColor transfer between images. E Reinhard, M Ashikhmin, B Gooch, P Shirley, 10.1109/38.946629IEEE Comput. Graph. Appl. 21Reinhard, E., Ashikhmin, M., Gooch, B., and Shirley, P. (2001). Color transfer between images. IEEE Comput. Graph. Appl. 21, 34-41. doi: 10.1109/38.946629\n\nU-Net: convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, 10.1007/978-3-319-24574-4_28Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015: 18th International Conference. N. Navab, J. Hornegger, W. M. Wells, and A. F. FrangiMunich, Germany; ChamSpringer International PublishingProceedings, Part IIIRonneberger, O., Fischer, P., and Brox, T. (2015). \"U-Net: convolutional networks for biomedical image segmentation, \" in Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015: 18th International Conference, eds N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, Munich, Germany, October 5-9, 2015, Proceedings, Part III. Cham: Springer International Publishing, 234-241. doi: 10.1007/978-3-319-24574-4_28\n\nHow users perceive content-based image retrieval for identifying skin images. M Sadeghi, P K Chilana, M S Atkins, 10.1007/978-3-030-02628-8_16Understanding and Interpreting Machine Learning in Medical Image Computing Applications. ChamSpringerSadeghi, M., Chilana, P. K., and Atkins, M. S. (2018). \"How users perceive content-based image retrieval for identifying skin images, \" in Understanding and Interpreting Machine Learning in Medical Image Computing Applications, Cham: Springer, 141-148. doi: 10.1007/978-3-030-02628-8_16\n\nDeep learning for medical image analysis. C Senaras, M N Gurcan, 10.4103/jpi.jpi_27_18J. Pathol. Informatics. 9Senaras, C., and Gurcan, M. N. (2018). Deep learning for medical image analysis. J. Pathol. Informatics 9, 423-433. doi: 10.4103/jpi.jpi_27_18\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, CoRR abs/1409.1556Simonyan, K., and Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. CoRR abs/1409.1556.\n\nLocality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. K Sirinukunwattana, S E Ahmed Raza, Y W Tsang, D Snead, I Cree, N Rajpoot, 10.1109/TMI.2016.2525803Med. Imag. 35Sirinukunwattana, K., Ahmed Raza, S. E., Tsang, Y. W., Snead, D., Cree, I., and Rajpoot, N. (2016). Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. Med. Imag. 35, 1196-1206. doi: 10.1109/TMI.2016.2525803\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, 10.1109/CVPR.2015.7298594Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBoston, MASzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., et al. (2015). \"Going deeper with convolutions, \" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Boston, MA), 1-9. doi: 10.1109/CVPR.2015.7298594\n\n. The Cancer Genome Atlas (TCGA). The Cancer Genome Atlas (TCGA) (2018). Available online at: https:// cancergenome.nih.gov\n\nHistopathological image classification using discriminative featureoriented dictionary learning. T H Vu, H S Mousavi, V Monga, G Rao, U A Rao, 10.1109/TMI.2015.2493530IEEE Trans. Med. Imag. 35Vu, T. H., Mousavi, H. S., Monga, V., Rao, G., and Rao, U. A. (2016). Histopathological image classification using discriminative feature- oriented dictionary learning. IEEE Trans. Med. Imag. 35, 738-751. doi: 10.1109/TMI.2015.2493530\n\nSubtype cell detection with an accelerated deep convolution neural network. S Wang, J Yao, Z Xu, J Huang, 10.1007/978-3-319-46723-8_74International Conference on Medical Image Computing and Computer-Assisted Intervention. AthensWang, S., Yao, J., Xu, Z., and Huang, J. (2016). \"Subtype cell detection with an accelerated deep convolution neural network, \" in International Conference on Medical Image Computing and Computer-Assisted Intervention (Athens), 640-648. doi: 10.1007/978-3-319-46723-8_74\n\nA methodology for texture feature-based quality assessment in nucleus segmentation of histopathology image. S Wen, T M Kurc, Y Gao, T Zhao, J H Saltz, Zhu , 10.4103/jpi.jpi_43_17J. Pathol. Informatics. 838Wen, S., Kurc, T. M., Gao, Y., Zhao, T., Saltz, J. H., and Zhu W (2017). A methodology for texture feature-based quality assessment in nucleus segmentation of histopathology image. J. Pathol. Informatics 8:38. doi: 10.4103/jpi.jpi_43_17\n\nAggregated residual transformations for deep neural networks. S Xie, R Girshick, P Doll\u00e1r, Z Tu, K He, 10.1109/CVPR.2017.634IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HIXie, S., Girshick, R., Doll\u00e1r, P., Tu, Z., and He, K. (2017). \"Aggregated residual transformations for deep neural networks, \" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2017) (Honolulu, HI), 5987-5995. doi: 10.1109/CVPR.2017.634\n\nDeep voting: a robust approach toward nucleus localization in microscopy images. Y Xie, X Kong, F Xing, F Liu, H Su, Yang , L , 10.1007/978-3-319-24574-4_45International Conference on Medical Image Computing and Computer-Assisted Intervention. MunichXie, Y., Kong, X., Xing, F., Liu, F., Su, H., and Yang, L. (2015). \"Deep voting: a robust approach toward nucleus localization in microscopy images, \" in International Conference on Medical Image Computing and Computer-Assisted Intervention (Munich), 374-382. doi: 10.1007/978-3-319-24 574-4_45\n\nSpatial clockwork recurrent neural net-work for muscle perimysium segmentation. Y Xie, Z Zhang, M Sapkota, Yang , L , International Conference on Medical Image Computing and Computer-Assisted Intervention. AthensXie, Y., Zhang, Z., Sapkota, M., and Yang, L. (2016). \"Spatial clockwork recurrent neural net-work for muscle perimysium segmentation, \" in International Conference on Medical Image Computing and Computer-Assisted Intervention (Athens).\n\nTransfer shape modeling towards high-throughput microscopy image segmentation. F Xing, X Shi, Z Zhang, J Cai, Y Xie, Yang , L , 10.1007/978-3-319-46726-9_22International Conference on Medical Image Computing and Computer-Assisted Intervention (Athens). Xing, F., Shi, X., Zhang, Z., Cai, J., Xie, Y., and Yang, L. (2016). \"Transfer shape modeling towards high-throughput microscopy image segmentation, \" in International Conference on Medical Image Computing and Computer-Assisted Intervention (Athens). doi: 10.1007/978-3-319-46726-9_22\n\nRobust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review. F Xing, Yang , L , 10.1109/RBME.2016.2515127IEEE Rev. Biomed. Eng. 9Xing, F., and Yang, L. (2016). Robust nucleus/cell detection and segmentation in digital pathology and microscopy images: a comprehensive review. IEEE Rev. Biomed. Eng. 9, 234-263. doi: 10.1109/RBME.2016.25\n\nDeep convolutional activation features for large scale brain tumor histopathology image classification and segmentation. Y Xu, Z Jia, Y Ai, F Zhang, M Lai, Chang , E , 10.1109/ICASSP.2015.7178109IEEE International Conference on Acoustics, Speech and Signal Processing. South Brisbane, QLDXu, Y., Jia, Z., Ai, Y., Zhang, F., Lai, M., and Chang, E. (2015). \"Deep convolutional activation features for large scale brain tumor histopathology image classification and segmentation, \" in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2015) (South Brisbane, QLD), 947-951. doi: 10.1109/ICASSP.2015.7178109\n\nPredicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features. K H Yu, C Zhang, G J Berry, R B Altman, D L Rubin, 10.1038/ncomms12474Nat. Commun. 712474Yu, K. H., Zhang, C., Berry, G. J., Altman, R. B., Rubin, D. L., et al. (2016). Predicting non-small cell lung cancer prognosis by fully automated microscopic pathology image features. Nat. Commun. 7:12474. doi: 10.1038/ncomms12474\n\nFeature extraction from histopathological images based on nucleus-guided convolutional neural network for breast lesion classification. Y Zheng, Z Jiang, F Xie, H Zhang, Y Ma, H Shi, 10.1016/j.patcog.2017.05.010Pattern Recognit. 71Zheng, Y., Jiang, Z., Xie, F., Zhang, H., Ma, Y., Shi, H., et al. (2017). Feature extraction from histopathological images based on nucleus-guided convolutional neural network for breast lesion classification. Pattern Recognit. 71, 14-25. doi: 10.1016/j.patcog.2017.05.010\n", "annotations": {"author": "[{\"end\":127,\"start\":113},{\"end\":146,\"start\":128},{\"end\":243,\"start\":147},{\"end\":334,\"start\":244},{\"end\":462,\"start\":335},{\"end\":475,\"start\":463},{\"end\":567,\"start\":476},{\"end\":661,\"start\":568},{\"end\":752,\"start\":662},{\"end\":853,\"start\":753},{\"end\":953,\"start\":854},{\"end\":1076,\"start\":954},{\"end\":1364,\"start\":1077},{\"end\":1653,\"start\":1365},{\"end\":1750,\"start\":1654},{\"end\":1842,\"start\":1751},{\"end\":1942,\"start\":1843},{\"end\":1959,\"start\":1943},{\"end\":1987,\"start\":1960},{\"end\":2037,\"start\":1988},{\"end\":2073,\"start\":2038},{\"end\":2177,\"start\":2074}]", "publisher": null, "author_last_name": "[{\"end\":126,\"start\":119},{\"end\":145,\"start\":136},{\"end\":159,\"start\":152},{\"end\":256,\"start\":250},{\"end\":346,\"start\":342},{\"end\":474,\"start\":468},{\"end\":483,\"start\":481},{\"end\":583,\"start\":577},{\"end\":674,\"start\":668},{\"end\":775,\"start\":765},{\"end\":870,\"start\":863},{\"end\":979,\"start\":964},{\"end\":1089,\"start\":1085},{\"end\":1378,\"start\":1373},{\"end\":1666,\"start\":1662},{\"end\":1764,\"start\":1757},{\"end\":1853,\"start\":1848},{\"end\":1958,\"start\":1950}]", "author_first_name": "[{\"end\":118,\"start\":113},{\"end\":135,\"start\":128},{\"end\":151,\"start\":147},{\"end\":249,\"start\":244},{\"end\":341,\"start\":335},{\"end\":467,\"start\":463},{\"end\":480,\"start\":476},{\"end\":576,\"start\":568},{\"end\":667,\"start\":662},{\"end\":758,\"start\":753},{\"end\":764,\"start\":759},{\"end\":858,\"start\":854},{\"end\":862,\"start\":859},{\"end\":963,\"start\":954},{\"end\":1084,\"start\":1077},{\"end\":1372,\"start\":1365},{\"end\":1657,\"start\":1654},{\"end\":1661,\"start\":1658},{\"end\":1756,\"start\":1751},{\"end\":1847,\"start\":1843},{\"end\":1949,\"start\":1943}]", "author_affiliation": "[{\"end\":242,\"start\":161},{\"end\":333,\"start\":258},{\"end\":461,\"start\":375},{\"end\":566,\"start\":485},{\"end\":660,\"start\":585},{\"end\":751,\"start\":676},{\"end\":852,\"start\":777},{\"end\":952,\"start\":872},{\"end\":1075,\"start\":981},{\"end\":1177,\"start\":1091},{\"end\":1282,\"start\":1179},{\"end\":1363,\"start\":1284},{\"end\":1466,\"start\":1380},{\"end\":1571,\"start\":1468},{\"end\":1652,\"start\":1573},{\"end\":1749,\"start\":1668},{\"end\":1841,\"start\":1766},{\"end\":1941,\"start\":1855},{\"end\":1986,\"start\":1961},{\"end\":2036,\"start\":1989},{\"end\":2072,\"start\":2039},{\"end\":2176,\"start\":2075}]", "title": "[{\"end\":100,\"start\":1},{\"end\":2277,\"start\":2178},{\"end\":2393,\"start\":2279}]", "venue": "[{\"end\":2420,\"start\":2395}]", "abstract": "[{\"end\":4153,\"start\":2763}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4972,\"start\":4954},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4998,\"start\":4972},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5018,\"start\":4998},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7253,\"start\":7232},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7275,\"start\":7253},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7292,\"start\":7275},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7308,\"start\":7292},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7332,\"start\":7308},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7357,\"start\":7332},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7387,\"start\":7357},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7405,\"start\":7387},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7428,\"start\":7405},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7446,\"start\":7428},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7465,\"start\":7446},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7490,\"start\":7465},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7515,\"start\":7490},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7646,\"start\":7627},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7940,\"start\":7916},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8126,\"start\":8102},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8144,\"start\":8128},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8400,\"start\":8384},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8421,\"start\":8402},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8664,\"start\":8643},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8911,\"start\":8886},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9109,\"start\":9091},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9515,\"start\":9489},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9540,\"start\":9515},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16337,\"start\":16311},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16375,\"start\":16346},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16400,\"start\":16381},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16443,\"start\":16419},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16746,\"start\":16728},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16795,\"start\":16777},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17743,\"start\":17724},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17952,\"start\":17927},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19874,\"start\":19858},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20295,\"start\":20265},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20316,\"start\":20295},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20333,\"start\":20316},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20352,\"start\":20333},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20445,\"start\":20426},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20601,\"start\":20580},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21166,\"start\":21149},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22617,\"start\":22599},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22664,\"start\":22644},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22854,\"start\":22837},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23360,\"start\":23342},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29249,\"start\":29237},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31019,\"start\":31002},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31040,\"start\":31019},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":33883,\"start\":33857},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34430,\"start\":34413},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":39888,\"start\":39866},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":41488,\"start\":41470},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":41673,\"start\":41643},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":41694,\"start\":41673},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":41711,\"start\":41694},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":41730,\"start\":41711},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":42620,\"start\":42601},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46221,\"start\":46197},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":46243,\"start\":46221},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":50055,\"start\":50038},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":50078,\"start\":50055},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":50594,\"start\":50574},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":50909,\"start\":50892},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":52167,\"start\":52143},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":52183,\"start\":52167},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":52204,\"start\":52183},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":55322,\"start\":55300}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54235,\"start\":54077},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54398,\"start\":54236},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54758,\"start\":54399},{\"attributes\":{\"id\":\"fig_3\"},\"end\":55323,\"start\":54759},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55688,\"start\":55324},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55989,\"start\":55689},{\"attributes\":{\"id\":\"fig_6\"},\"end\":56225,\"start\":55990},{\"attributes\":{\"id\":\"fig_7\"},\"end\":56477,\"start\":56226},{\"attributes\":{\"id\":\"fig_8\"},\"end\":56770,\"start\":56478},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":57158,\"start\":56771},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":57926,\"start\":57159},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":58980,\"start\":57927},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":59153,\"start\":58981},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":59538,\"start\":59154},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":59587,\"start\":59539}]", "paragraph": "[{\"end\":5019,\"start\":4169},{\"end\":6088,\"start\":5021},{\"end\":9292,\"start\":6090},{\"end\":10841,\"start\":9294},{\"end\":11029,\"start\":10843},{\"end\":12868,\"start\":11031},{\"end\":14524,\"start\":12870},{\"end\":15021,\"start\":14601},{\"end\":16009,\"start\":15023},{\"end\":16488,\"start\":16034},{\"end\":17283,\"start\":16509},{\"end\":18833,\"start\":17302},{\"end\":19687,\"start\":18860},{\"end\":22540,\"start\":19759},{\"end\":23677,\"start\":22565},{\"end\":23897,\"start\":23679},{\"end\":25465,\"start\":23954},{\"end\":26381,\"start\":25500},{\"end\":27162,\"start\":26436},{\"end\":28082,\"start\":27199},{\"end\":29151,\"start\":28084},{\"end\":30171,\"start\":29153},{\"end\":30504,\"start\":30173},{\"end\":31326,\"start\":30506},{\"end\":32838,\"start\":31374},{\"end\":34054,\"start\":32917},{\"end\":35628,\"start\":34056},{\"end\":36210,\"start\":35630},{\"end\":38089,\"start\":36212},{\"end\":39703,\"start\":38130},{\"end\":40225,\"start\":39705},{\"end\":40760,\"start\":40227},{\"end\":40938,\"start\":40874},{\"end\":41103,\"start\":40940},{\"end\":43103,\"start\":41105},{\"end\":44749,\"start\":43175},{\"end\":44785,\"start\":44769},{\"end\":44803,\"start\":44787},{\"end\":45104,\"start\":44805},{\"end\":46830,\"start\":45106},{\"end\":47799,\"start\":46845},{\"end\":49297,\"start\":47801},{\"end\":51463,\"start\":49299},{\"end\":52205,\"start\":51465},{\"end\":52782,\"start\":52207},{\"end\":53250,\"start\":52784},{\"end\":53363,\"start\":53272},{\"end\":54076,\"start\":53388}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":40873,\"start\":40761},{\"attributes\":{\"id\":\"formula_1\"},\"end\":43174,\"start\":43104}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18832,\"start\":18825},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":33115,\"start\":33107},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":36450,\"start\":36443},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":42379,\"start\":42372},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":44407,\"start\":44400}]", "section_header": "[{\"end\":4167,\"start\":4155},{\"end\":14548,\"start\":14527},{\"end\":14599,\"start\":14551},{\"end\":16032,\"start\":16012},{\"end\":16507,\"start\":16491},{\"end\":17300,\"start\":17286},{\"end\":18858,\"start\":18836},{\"end\":19757,\"start\":19690},{\"end\":22563,\"start\":22543},{\"end\":23952,\"start\":23900},{\"end\":25498,\"start\":25468},{\"end\":26391,\"start\":26384},{\"end\":26434,\"start\":26394},{\"end\":27197,\"start\":27165},{\"end\":31372,\"start\":31329},{\"end\":32915,\"start\":32841},{\"end\":38128,\"start\":38092},{\"end\":44767,\"start\":44752},{\"end\":46843,\"start\":46833},{\"end\":53270,\"start\":53253},{\"end\":53386,\"start\":53366},{\"end\":54088,\"start\":54078},{\"end\":54247,\"start\":54237},{\"end\":54410,\"start\":54400},{\"end\":54770,\"start\":54760},{\"end\":55335,\"start\":55325},{\"end\":55700,\"start\":55690},{\"end\":56001,\"start\":55991},{\"end\":56237,\"start\":56227},{\"end\":56486,\"start\":56479},{\"end\":56781,\"start\":56772},{\"end\":57937,\"start\":57928},{\"end\":58991,\"start\":58982},{\"end\":59164,\"start\":59155},{\"end\":59549,\"start\":59540}]", "table": "[{\"end\":57158,\"start\":56809},{\"end\":57926,\"start\":57533},{\"end\":58980,\"start\":58415},{\"end\":59153,\"start\":59028},{\"end\":59538,\"start\":59187}]", "figure_caption": "[{\"end\":54235,\"start\":54090},{\"end\":54398,\"start\":54249},{\"end\":54758,\"start\":54412},{\"end\":55323,\"start\":54772},{\"end\":55688,\"start\":55337},{\"end\":55989,\"start\":55702},{\"end\":56225,\"start\":56003},{\"end\":56477,\"start\":56239},{\"end\":56770,\"start\":56487},{\"end\":56809,\"start\":56783},{\"end\":57533,\"start\":57161},{\"end\":58415,\"start\":57939},{\"end\":59028,\"start\":58993},{\"end\":59187,\"start\":59166},{\"end\":59587,\"start\":59551}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15020,\"start\":15012},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16114,\"start\":16106},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19606,\"start\":19598},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22539,\"start\":22531},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23630,\"start\":23622},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34008,\"start\":33998},{\"end\":37063,\"start\":37053},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37345,\"start\":37337},{\"end\":37647,\"start\":37637},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37805,\"start\":37797},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44184,\"start\":44175}]", "bib_author_first_name": "[{\"end\":59782,\"start\":59781},{\"end\":59795,\"start\":59794},{\"end\":59805,\"start\":59804},{\"end\":59807,\"start\":59806},{\"end\":60149,\"start\":60148},{\"end\":60162,\"start\":60161},{\"end\":60171,\"start\":60170},{\"end\":60182,\"start\":60181},{\"end\":60197,\"start\":60196},{\"end\":60618,\"start\":60617},{\"end\":60636,\"start\":60635},{\"end\":60647,\"start\":60646},{\"end\":61081,\"start\":61080},{\"end\":61083,\"start\":61082},{\"end\":61095,\"start\":61094},{\"end\":61099,\"start\":61096},{\"end\":61110,\"start\":61109},{\"end\":61123,\"start\":61122},{\"end\":61134,\"start\":61133},{\"end\":61142,\"start\":61141},{\"end\":61562,\"start\":61561},{\"end\":61564,\"start\":61563},{\"end\":61574,\"start\":61573},{\"end\":61586,\"start\":61585},{\"end\":61600,\"start\":61599},{\"end\":61602,\"start\":61601},{\"end\":62169,\"start\":62168},{\"end\":62177,\"start\":62176},{\"end\":62183,\"start\":62182},{\"end\":62189,\"start\":62188},{\"end\":62196,\"start\":62195},{\"end\":62206,\"start\":62202},{\"end\":62553,\"start\":62552},{\"end\":62569,\"start\":62568},{\"end\":62571,\"start\":62570},{\"end\":62581,\"start\":62580},{\"end\":62592,\"start\":62591},{\"end\":62601,\"start\":62600},{\"end\":62611,\"start\":62610},{\"end\":62983,\"start\":62982},{\"end\":62985,\"start\":62984},{\"end\":62995,\"start\":62994},{\"end\":62997,\"start\":62996},{\"end\":63008,\"start\":63007},{\"end\":63010,\"start\":63009},{\"end\":63019,\"start\":63018},{\"end\":63021,\"start\":63020},{\"end\":63031,\"start\":63030},{\"end\":63038,\"start\":63037},{\"end\":63040,\"start\":63039},{\"end\":63417,\"start\":63416},{\"end\":63428,\"start\":63427},{\"end\":63430,\"start\":63429},{\"end\":63441,\"start\":63440},{\"end\":63460,\"start\":63459},{\"end\":63469,\"start\":63468},{\"end\":63481,\"start\":63480},{\"end\":63846,\"start\":63845},{\"end\":63854,\"start\":63853},{\"end\":63862,\"start\":63861},{\"end\":63875,\"start\":63871},{\"end\":63881,\"start\":63880},{\"end\":63887,\"start\":63886},{\"end\":64246,\"start\":64245},{\"end\":64248,\"start\":64247},{\"end\":64456,\"start\":64455},{\"end\":64458,\"start\":64457},{\"end\":64471,\"start\":64470},{\"end\":64480,\"start\":64479},{\"end\":64889,\"start\":64888},{\"end\":64901,\"start\":64900},{\"end\":64910,\"start\":64909},{\"end\":64924,\"start\":64923},{\"end\":65292,\"start\":65291},{\"end\":65302,\"start\":65301},{\"end\":65753,\"start\":65752},{\"end\":65755,\"start\":65754},{\"end\":65765,\"start\":65764},{\"end\":65767,\"start\":65766},{\"end\":65780,\"start\":65779},{\"end\":65787,\"start\":65786},{\"end\":65801,\"start\":65800},{\"end\":65812,\"start\":65811},{\"end\":66140,\"start\":66139},{\"end\":66153,\"start\":66152},{\"end\":66165,\"start\":66164},{\"end\":66181,\"start\":66176},{\"end\":66185,\"start\":66184},{\"end\":66730,\"start\":66729},{\"end\":66736,\"start\":66735},{\"end\":66745,\"start\":66744},{\"end\":66752,\"start\":66751},{\"end\":67220,\"start\":67219},{\"end\":67226,\"start\":67225},{\"end\":67235,\"start\":67234},{\"end\":67242,\"start\":67241},{\"end\":67576,\"start\":67575},{\"end\":67582,\"start\":67581},{\"end\":67591,\"start\":67590},{\"end\":67598,\"start\":67597},{\"end\":68096,\"start\":68095},{\"end\":68371,\"start\":68370},{\"end\":68716,\"start\":68715},{\"end\":68729,\"start\":68728},{\"end\":68738,\"start\":68737},{\"end\":68756,\"start\":68755},{\"end\":68769,\"start\":68768},{\"end\":68771,\"start\":68770},{\"end\":68781,\"start\":68780},{\"end\":68783,\"start\":68782},{\"end\":69084,\"start\":69083},{\"end\":69091,\"start\":69090},{\"end\":69102,\"start\":69101},{\"end\":69113,\"start\":69112},{\"end\":69115,\"start\":69114},{\"end\":69123,\"start\":69122},{\"end\":69125,\"start\":69124},{\"end\":69134,\"start\":69133},{\"end\":69136,\"start\":69135},{\"end\":69449,\"start\":69448},{\"end\":69456,\"start\":69455},{\"end\":69467,\"start\":69466},{\"end\":69469,\"start\":69468},{\"end\":69477,\"start\":69476},{\"end\":69484,\"start\":69483},{\"end\":69486,\"start\":69485},{\"end\":69495,\"start\":69494},{\"end\":69497,\"start\":69496},{\"end\":70032,\"start\":70031},{\"end\":70041,\"start\":70040},{\"end\":70048,\"start\":70047},{\"end\":70066,\"start\":70065},{\"end\":70068,\"start\":70067},{\"end\":70599,\"start\":70598},{\"end\":70601,\"start\":70600},{\"end\":70610,\"start\":70609},{\"end\":70629,\"start\":70622},{\"end\":70633,\"start\":70632},{\"end\":70635,\"start\":70634},{\"end\":70925,\"start\":70924},{\"end\":70933,\"start\":70932},{\"end\":70952,\"start\":70945},{\"end\":70956,\"start\":70955},{\"end\":71446,\"start\":71445},{\"end\":71617,\"start\":71616},{\"end\":71628,\"start\":71627},{\"end\":71639,\"start\":71638},{\"end\":71647,\"start\":71646},{\"end\":71649,\"start\":71648},{\"end\":71658,\"start\":71657},{\"end\":71660,\"start\":71659},{\"end\":71672,\"start\":71671},{\"end\":72112,\"start\":72111},{\"end\":72126,\"start\":72125},{\"end\":72132,\"start\":72131},{\"end\":72141,\"start\":72140},{\"end\":72149,\"start\":72148},{\"end\":72158,\"start\":72157},{\"end\":72605,\"start\":72604},{\"end\":72616,\"start\":72615},{\"end\":72618,\"start\":72617},{\"end\":73113,\"start\":73112},{\"end\":73124,\"start\":73123},{\"end\":73134,\"start\":73133},{\"end\":73150,\"start\":73149},{\"end\":73152,\"start\":73151},{\"end\":73442,\"start\":73441},{\"end\":73454,\"start\":73453},{\"end\":73467,\"start\":73466},{\"end\":73476,\"start\":73475},{\"end\":73753,\"start\":73752},{\"end\":73768,\"start\":73767},{\"end\":73779,\"start\":73778},{\"end\":74554,\"start\":74553},{\"end\":74565,\"start\":74564},{\"end\":74567,\"start\":74566},{\"end\":74578,\"start\":74577},{\"end\":74580,\"start\":74579},{\"end\":75049,\"start\":75048},{\"end\":75060,\"start\":75059},{\"end\":75062,\"start\":75061},{\"end\":75330,\"start\":75329},{\"end\":75342,\"start\":75341},{\"end\":75620,\"start\":75619},{\"end\":75640,\"start\":75639},{\"end\":75642,\"start\":75641},{\"end\":75656,\"start\":75655},{\"end\":75658,\"start\":75657},{\"end\":75667,\"start\":75666},{\"end\":75676,\"start\":75675},{\"end\":75684,\"start\":75683},{\"end\":76039,\"start\":76038},{\"end\":76050,\"start\":76049},{\"end\":76057,\"start\":76056},{\"end\":76064,\"start\":76063},{\"end\":76076,\"start\":76075},{\"end\":76084,\"start\":76083},{\"end\":76745,\"start\":76744},{\"end\":76747,\"start\":76746},{\"end\":76753,\"start\":76752},{\"end\":76755,\"start\":76754},{\"end\":76766,\"start\":76765},{\"end\":76775,\"start\":76774},{\"end\":76782,\"start\":76781},{\"end\":76784,\"start\":76783},{\"end\":77152,\"start\":77151},{\"end\":77160,\"start\":77159},{\"end\":77167,\"start\":77166},{\"end\":77173,\"start\":77172},{\"end\":77684,\"start\":77683},{\"end\":77691,\"start\":77690},{\"end\":77693,\"start\":77692},{\"end\":77701,\"start\":77700},{\"end\":77708,\"start\":77707},{\"end\":77716,\"start\":77715},{\"end\":77718,\"start\":77717},{\"end\":77729,\"start\":77726},{\"end\":78081,\"start\":78080},{\"end\":78088,\"start\":78087},{\"end\":78100,\"start\":78099},{\"end\":78110,\"start\":78109},{\"end\":78116,\"start\":78115},{\"end\":78552,\"start\":78551},{\"end\":78559,\"start\":78558},{\"end\":78567,\"start\":78566},{\"end\":78575,\"start\":78574},{\"end\":78582,\"start\":78581},{\"end\":78591,\"start\":78587},{\"end\":78595,\"start\":78594},{\"end\":79097,\"start\":79096},{\"end\":79104,\"start\":79103},{\"end\":79113,\"start\":79112},{\"end\":79127,\"start\":79123},{\"end\":79131,\"start\":79130},{\"end\":79546,\"start\":79545},{\"end\":79554,\"start\":79553},{\"end\":79561,\"start\":79560},{\"end\":79570,\"start\":79569},{\"end\":79577,\"start\":79576},{\"end\":79587,\"start\":79583},{\"end\":79591,\"start\":79590},{\"end\":80121,\"start\":80120},{\"end\":80132,\"start\":80128},{\"end\":80136,\"start\":80135},{\"end\":80518,\"start\":80517},{\"end\":80524,\"start\":80523},{\"end\":80531,\"start\":80530},{\"end\":80537,\"start\":80536},{\"end\":80546,\"start\":80545},{\"end\":80557,\"start\":80552},{\"end\":80561,\"start\":80560},{\"end\":81137,\"start\":81136},{\"end\":81139,\"start\":81138},{\"end\":81145,\"start\":81144},{\"end\":81154,\"start\":81153},{\"end\":81156,\"start\":81155},{\"end\":81165,\"start\":81164},{\"end\":81167,\"start\":81166},{\"end\":81177,\"start\":81176},{\"end\":81179,\"start\":81178},{\"end\":81595,\"start\":81594},{\"end\":81604,\"start\":81603},{\"end\":81613,\"start\":81612},{\"end\":81620,\"start\":81619},{\"end\":81629,\"start\":81628},{\"end\":81635,\"start\":81634}]", "bib_author_last_name": "[{\"end\":59792,\"start\":59783},{\"end\":59802,\"start\":59796},{\"end\":59815,\"start\":59808},{\"end\":60159,\"start\":60150},{\"end\":60168,\"start\":60163},{\"end\":60179,\"start\":60172},{\"end\":60194,\"start\":60183},{\"end\":60204,\"start\":60198},{\"end\":60633,\"start\":60619},{\"end\":60644,\"start\":60637},{\"end\":60655,\"start\":60648},{\"end\":61092,\"start\":61084},{\"end\":61107,\"start\":61100},{\"end\":61120,\"start\":61111},{\"end\":61131,\"start\":61124},{\"end\":61139,\"start\":61135},{\"end\":61155,\"start\":61143},{\"end\":61571,\"start\":61565},{\"end\":61583,\"start\":61575},{\"end\":61597,\"start\":61587},{\"end\":61609,\"start\":61603},{\"end\":62174,\"start\":62170},{\"end\":62180,\"start\":62178},{\"end\":62186,\"start\":62184},{\"end\":62193,\"start\":62190},{\"end\":62200,\"start\":62197},{\"end\":62211,\"start\":62207},{\"end\":62566,\"start\":62554},{\"end\":62578,\"start\":62572},{\"end\":62589,\"start\":62582},{\"end\":62598,\"start\":62593},{\"end\":62608,\"start\":62602},{\"end\":62618,\"start\":62612},{\"end\":62992,\"start\":62986},{\"end\":63005,\"start\":62998},{\"end\":63016,\"start\":63011},{\"end\":63028,\"start\":63022},{\"end\":63035,\"start\":63032},{\"end\":63046,\"start\":63041},{\"end\":63425,\"start\":63418},{\"end\":63438,\"start\":63431},{\"end\":63457,\"start\":63442},{\"end\":63466,\"start\":63461},{\"end\":63478,\"start\":63470},{\"end\":63490,\"start\":63482},{\"end\":63851,\"start\":63847},{\"end\":63859,\"start\":63855},{\"end\":63869,\"start\":63863},{\"end\":63878,\"start\":63876},{\"end\":63884,\"start\":63882},{\"end\":63895,\"start\":63888},{\"end\":64253,\"start\":64249},{\"end\":64468,\"start\":64459},{\"end\":64477,\"start\":64472},{\"end\":64487,\"start\":64481},{\"end\":64898,\"start\":64890},{\"end\":64907,\"start\":64902},{\"end\":64921,\"start\":64911},{\"end\":64932,\"start\":64925},{\"end\":65299,\"start\":65293},{\"end\":65310,\"start\":65303},{\"end\":65762,\"start\":65756},{\"end\":65777,\"start\":65768},{\"end\":65784,\"start\":65781},{\"end\":65798,\"start\":65788},{\"end\":65809,\"start\":65802},{\"end\":65818,\"start\":65813},{\"end\":66150,\"start\":66141},{\"end\":66162,\"start\":66154},{\"end\":66174,\"start\":66166},{\"end\":66733,\"start\":66731},{\"end\":66742,\"start\":66737},{\"end\":66749,\"start\":66746},{\"end\":66756,\"start\":66753},{\"end\":67223,\"start\":67221},{\"end\":67232,\"start\":67227},{\"end\":67239,\"start\":67236},{\"end\":67246,\"start\":67243},{\"end\":67579,\"start\":67577},{\"end\":67588,\"start\":67583},{\"end\":67595,\"start\":67592},{\"end\":67602,\"start\":67599},{\"end\":68106,\"start\":68097},{\"end\":68381,\"start\":68372},{\"end\":68726,\"start\":68717},{\"end\":68735,\"start\":68730},{\"end\":68753,\"start\":68739},{\"end\":68766,\"start\":68757},{\"end\":68778,\"start\":68772},{\"end\":68790,\"start\":68784},{\"end\":69088,\"start\":69085},{\"end\":69099,\"start\":69092},{\"end\":69110,\"start\":69103},{\"end\":69120,\"start\":69116},{\"end\":69131,\"start\":69126},{\"end\":69142,\"start\":69137},{\"end\":69453,\"start\":69450},{\"end\":69464,\"start\":69457},{\"end\":69474,\"start\":69470},{\"end\":69481,\"start\":69478},{\"end\":69492,\"start\":69487},{\"end\":69503,\"start\":69498},{\"end\":70038,\"start\":70033},{\"end\":70045,\"start\":70042},{\"end\":70063,\"start\":70049},{\"end\":70079,\"start\":70069},{\"end\":70607,\"start\":70602},{\"end\":70620,\"start\":70611},{\"end\":70930,\"start\":70926},{\"end\":70943,\"start\":70934},{\"end\":71457,\"start\":71447},{\"end\":71625,\"start\":71618},{\"end\":71636,\"start\":71629},{\"end\":71644,\"start\":71640},{\"end\":71655,\"start\":71650},{\"end\":71669,\"start\":71661},{\"end\":71678,\"start\":71673},{\"end\":72123,\"start\":72113},{\"end\":72129,\"start\":72127},{\"end\":72138,\"start\":72133},{\"end\":72146,\"start\":72142},{\"end\":72155,\"start\":72150},{\"end\":72166,\"start\":72159},{\"end\":72613,\"start\":72606},{\"end\":72625,\"start\":72619},{\"end\":73121,\"start\":73114},{\"end\":73131,\"start\":73125},{\"end\":73147,\"start\":73135},{\"end\":73159,\"start\":73153},{\"end\":73451,\"start\":73443},{\"end\":73464,\"start\":73455},{\"end\":73473,\"start\":73468},{\"end\":73484,\"start\":73477},{\"end\":73765,\"start\":73754},{\"end\":73776,\"start\":73769},{\"end\":73784,\"start\":73780},{\"end\":74562,\"start\":74555},{\"end\":74575,\"start\":74568},{\"end\":74587,\"start\":74581},{\"end\":75057,\"start\":75050},{\"end\":75069,\"start\":75063},{\"end\":75339,\"start\":75331},{\"end\":75352,\"start\":75343},{\"end\":75637,\"start\":75621},{\"end\":75653,\"start\":75643},{\"end\":75664,\"start\":75659},{\"end\":75673,\"start\":75668},{\"end\":75681,\"start\":75677},{\"end\":75692,\"start\":75685},{\"end\":76047,\"start\":76040},{\"end\":76054,\"start\":76051},{\"end\":76061,\"start\":76058},{\"end\":76073,\"start\":76065},{\"end\":76081,\"start\":76077},{\"end\":76093,\"start\":76085},{\"end\":76750,\"start\":76748},{\"end\":76763,\"start\":76756},{\"end\":76772,\"start\":76767},{\"end\":76779,\"start\":76776},{\"end\":76788,\"start\":76785},{\"end\":77157,\"start\":77153},{\"end\":77164,\"start\":77161},{\"end\":77170,\"start\":77168},{\"end\":77179,\"start\":77174},{\"end\":77688,\"start\":77685},{\"end\":77698,\"start\":77694},{\"end\":77705,\"start\":77702},{\"end\":77713,\"start\":77709},{\"end\":77724,\"start\":77719},{\"end\":78085,\"start\":78082},{\"end\":78097,\"start\":78089},{\"end\":78107,\"start\":78101},{\"end\":78113,\"start\":78111},{\"end\":78119,\"start\":78117},{\"end\":78556,\"start\":78553},{\"end\":78564,\"start\":78560},{\"end\":78572,\"start\":78568},{\"end\":78579,\"start\":78576},{\"end\":78585,\"start\":78583},{\"end\":79101,\"start\":79098},{\"end\":79110,\"start\":79105},{\"end\":79121,\"start\":79114},{\"end\":79551,\"start\":79547},{\"end\":79558,\"start\":79555},{\"end\":79567,\"start\":79562},{\"end\":79574,\"start\":79571},{\"end\":79581,\"start\":79578},{\"end\":80126,\"start\":80122},{\"end\":80521,\"start\":80519},{\"end\":80528,\"start\":80525},{\"end\":80534,\"start\":80532},{\"end\":80543,\"start\":80538},{\"end\":80550,\"start\":80547},{\"end\":81142,\"start\":81140},{\"end\":81151,\"start\":81146},{\"end\":81162,\"start\":81157},{\"end\":81174,\"start\":81168},{\"end\":81185,\"start\":81180},{\"end\":81601,\"start\":81596},{\"end\":81610,\"start\":81605},{\"end\":81617,\"start\":81614},{\"end\":81626,\"start\":81621},{\"end\":81632,\"start\":81630},{\"end\":81639,\"start\":81636}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1707.08814\",\"id\":\"b0\"},\"end\":60024,\"start\":59692},{\"attributes\":{\"doi\":\"10.1016/j.patrec.2017.09.015\",\"id\":\"b1\",\"matched_paper_id\":53369654},\"end\":60533,\"start\":60026},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2016.2644615\",\"id\":\"b2\",\"matched_paper_id\":60814714},\"end\":60948,\"start\":60535},{\"attributes\":{\"doi\":\"10.1117/1.JMI.4.4.044504\",\"id\":\"b3\",\"matched_paper_id\":1685059},\"end\":61491,\"start\":60950},{\"attributes\":{\"doi\":\"10.1109/CVPR.2018.00976\",\"id\":\"b4\",\"matched_paper_id\":52838058},\"end\":62076,\"start\":61493},{\"attributes\":{\"doi\":\"10.1016/j.media.2016.11.004\",\"id\":\"b5\",\"matched_paper_id\":23873598},\"end\":62479,\"start\":62078},{\"attributes\":{\"doi\":\"10.15265/IY-2017-041\",\"id\":\"b6\",\"matched_paper_id\":4337655},\"end\":62900,\"start\":62481},{\"attributes\":{\"doi\":\"10.1002/path.5028\",\"id\":\"b7\",\"matched_paper_id\":4156338},\"end\":63300,\"start\":62902},{\"attributes\":{\"doi\":\"10.1038/s41591-018-0177-5\",\"id\":\"b8\",\"matched_paper_id\":52289081},\"end\":63790,\"start\":63302},{\"attributes\":{\"doi\":\"10.1109/CVPR.2009.5206848\",\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":64179,\"start\":63792},{\"attributes\":{\"doi\":\"10.2307/1932409\",\"id\":\"b10\",\"matched_paper_id\":53335638},\"end\":64406,\"start\":64181},{\"attributes\":{\"doi\":\"10.23919/MIPRO.2018.8400040\",\"id\":\"b11\",\"matched_paper_id\":49560076},\"end\":64824,\"start\":64408},{\"attributes\":{\"doi\":\"10.1146/annurev-pathol-011811-120902\",\"id\":\"b12\",\"matched_paper_id\":15286576},\"end\":65187,\"start\":64826},{\"attributes\":{\"doi\":\"10.1109/ISBI.2018.8363645\",\"id\":\"b13\",\"matched_paper_id\":44146054},\"end\":65706,\"start\":65189},{\"attributes\":{\"doi\":\"10.1109/RBME.2009.2034865\",\"id\":\"b14\",\"matched_paper_id\":8575576},\"end\":66069,\"start\":65708},{\"attributes\":{\"doi\":\"10.1109/CVPR.2015.7298642\",\"id\":\"b15\",\"matched_paper_id\":12225766},\"end\":66634,\"start\":66071},{\"attributes\":{\"doi\":\"10.1109/ICCV.2015.123\",\"id\":\"b16\",\"matched_paper_id\":13740328},\"end\":67172,\"start\":66636},{\"attributes\":{\"doi\":\"10.1007/978-3-319-46493-0_38\",\"id\":\"b17\",\"matched_paper_id\":6447277},\"end\":67527,\"start\":67174},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.90\",\"id\":\"b18\",\"matched_paper_id\":206594692},\"end\":68000,\"start\":67529},{\"attributes\":{\"doi\":\"10.1007/978-3-319-50478-0\",\"id\":\"b19\",\"matched_paper_id\":4649427},\"end\":68327,\"start\":68002},{\"attributes\":{\"doi\":\"10.1109/DISA.2018.8490530\",\"id\":\"b20\",\"matched_paper_id\":52984817},\"end\":68622,\"start\":68329},{\"attributes\":{\"doi\":\"10.1007/s10489-018-1361-5\",\"id\":\"b21\",\"matched_paper_id\":54450835},\"end\":69081,\"start\":68624},{\"attributes\":{\"doi\":\"arXiv:1712.05021\",\"id\":\"b22\"},\"end\":69360,\"start\":69083},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.266\",\"id\":\"b23\",\"matched_paper_id\":1645469},\"end\":69987,\"start\":69362},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.243\",\"id\":\"b24\",\"matched_paper_id\":9433631},\"end\":70493,\"start\":69989},{\"attributes\":{\"id\":\"b25\"},\"end\":70866,\"start\":70495},{\"attributes\":{\"doi\":\"10.1109/CVPR.2015.7298965\",\"id\":\"b26\",\"matched_paper_id\":1629541},\"end\":71379,\"start\":70868},{\"attributes\":{\"doi\":\"10.2217/IIM.09.9\",\"id\":\"b27\",\"matched_paper_id\":760579},\"end\":71614,\"start\":71381},{\"attributes\":{\"doi\":\"arXiv:1810.00236\",\"id\":\"b28\"},\"end\":71988,\"start\":71616},{\"attributes\":{\"doi\":\"10.1016/j.patcog.2015.09.015\",\"id\":\"b29\",\"matched_paper_id\":29815184},\"end\":72479,\"start\":71990},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":21676076},\"end\":73020,\"start\":72481},{\"attributes\":{\"doi\":\"10.1038/s41598-018-24876-0\",\"id\":\"b31\",\"matched_paper_id\":13679282},\"end\":73408,\"start\":73022},{\"attributes\":{\"doi\":\"10.1109/38.946629\",\"id\":\"b32\",\"matched_paper_id\":14088925},\"end\":73685,\"start\":73410},{\"attributes\":{\"doi\":\"10.1007/978-3-319-24574-4_28\",\"id\":\"b33\",\"matched_paper_id\":3719281},\"end\":74473,\"start\":73687},{\"attributes\":{\"doi\":\"10.1007/978-3-030-02628-8_16\",\"id\":\"b34\",\"matched_paper_id\":53090217},\"end\":75004,\"start\":74475},{\"attributes\":{\"doi\":\"10.4103/jpi.jpi_27_18\",\"id\":\"b35\",\"matched_paper_id\":7961631},\"end\":75259,\"start\":75006},{\"attributes\":{\"doi\":\"CoRR abs/1409.1556\",\"id\":\"b36\"},\"end\":75499,\"start\":75261},{\"attributes\":{\"doi\":\"10.1109/TMI.2016.2525803\",\"id\":\"b37\",\"matched_paper_id\":54556210},\"end\":76004,\"start\":75501},{\"attributes\":{\"doi\":\"10.1109/CVPR.2015.7298594\",\"id\":\"b38\",\"matched_paper_id\":206592484},\"end\":76520,\"start\":76006},{\"attributes\":{\"id\":\"b39\"},\"end\":76645,\"start\":76522},{\"attributes\":{\"doi\":\"10.1109/TMI.2015.2493530\",\"id\":\"b40\",\"matched_paper_id\":2823911},\"end\":77073,\"start\":76647},{\"attributes\":{\"doi\":\"10.1007/978-3-319-46723-8_74\",\"id\":\"b41\",\"matched_paper_id\":1193352},\"end\":77573,\"start\":77075},{\"attributes\":{\"doi\":\"10.4103/jpi.jpi_43_17\",\"id\":\"b42\",\"matched_paper_id\":3144424},\"end\":78016,\"start\":77575},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.634\",\"id\":\"b43\",\"matched_paper_id\":8485068},\"end\":78468,\"start\":78018},{\"attributes\":{\"doi\":\"10.1007/978-3-319-24574-4_45\",\"id\":\"b44\",\"matched_paper_id\":2011900},\"end\":79014,\"start\":78470},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":9976580},\"end\":79464,\"start\":79016},{\"attributes\":{\"doi\":\"10.1007/978-3-319-46726-9_22\",\"id\":\"b46\",\"matched_paper_id\":36160950},\"end\":80003,\"start\":79466},{\"attributes\":{\"doi\":\"10.1109/RBME.2016.2515127\",\"id\":\"b47\",\"matched_paper_id\":6538247},\"end\":80394,\"start\":80005},{\"attributes\":{\"doi\":\"10.1109/ICASSP.2015.7178109\",\"id\":\"b48\",\"matched_paper_id\":14714524},\"end\":81029,\"start\":80396},{\"attributes\":{\"doi\":\"10.1038/ncomms12474\",\"id\":\"b49\",\"matched_paper_id\":3892869},\"end\":81456,\"start\":81031},{\"attributes\":{\"doi\":\"10.1016/j.patcog.2017.05.010\",\"id\":\"b50\",\"matched_paper_id\":8766069},\"end\":81961,\"start\":81458}]", "bib_title": "[{\"end\":60146,\"start\":60026},{\"end\":60615,\"start\":60535},{\"end\":61078,\"start\":60950},{\"end\":61559,\"start\":61493},{\"end\":62166,\"start\":62078},{\"end\":62550,\"start\":62481},{\"end\":62980,\"start\":62902},{\"end\":63414,\"start\":63302},{\"end\":63843,\"start\":63792},{\"end\":64243,\"start\":64181},{\"end\":64453,\"start\":64408},{\"end\":64886,\"start\":64826},{\"end\":65289,\"start\":65189},{\"end\":65750,\"start\":65708},{\"end\":66137,\"start\":66071},{\"end\":66727,\"start\":66636},{\"end\":67217,\"start\":67174},{\"end\":67573,\"start\":67529},{\"end\":68093,\"start\":68002},{\"end\":68368,\"start\":68329},{\"end\":68713,\"start\":68624},{\"end\":69446,\"start\":69362},{\"end\":70029,\"start\":69989},{\"end\":70596,\"start\":70495},{\"end\":70922,\"start\":70868},{\"end\":71443,\"start\":71381},{\"end\":72109,\"start\":71990},{\"end\":72602,\"start\":72481},{\"end\":73110,\"start\":73022},{\"end\":73439,\"start\":73410},{\"end\":73750,\"start\":73687},{\"end\":74551,\"start\":74475},{\"end\":75046,\"start\":75006},{\"end\":75617,\"start\":75501},{\"end\":76036,\"start\":76006},{\"end\":76742,\"start\":76647},{\"end\":77149,\"start\":77075},{\"end\":77681,\"start\":77575},{\"end\":78078,\"start\":78018},{\"end\":78549,\"start\":78470},{\"end\":79094,\"start\":79016},{\"end\":79543,\"start\":79466},{\"end\":80118,\"start\":80005},{\"end\":80515,\"start\":80396},{\"end\":81134,\"start\":81031},{\"end\":81592,\"start\":81458}]", "bib_author": "[{\"end\":59794,\"start\":59781},{\"end\":59804,\"start\":59794},{\"end\":59817,\"start\":59804},{\"end\":60161,\"start\":60148},{\"end\":60170,\"start\":60161},{\"end\":60181,\"start\":60170},{\"end\":60196,\"start\":60181},{\"end\":60206,\"start\":60196},{\"end\":60635,\"start\":60617},{\"end\":60646,\"start\":60635},{\"end\":60657,\"start\":60646},{\"end\":61094,\"start\":61080},{\"end\":61109,\"start\":61094},{\"end\":61122,\"start\":61109},{\"end\":61133,\"start\":61122},{\"end\":61141,\"start\":61133},{\"end\":61157,\"start\":61141},{\"end\":61573,\"start\":61561},{\"end\":61585,\"start\":61573},{\"end\":61599,\"start\":61585},{\"end\":61611,\"start\":61599},{\"end\":62176,\"start\":62168},{\"end\":62182,\"start\":62176},{\"end\":62188,\"start\":62182},{\"end\":62195,\"start\":62188},{\"end\":62202,\"start\":62195},{\"end\":62213,\"start\":62202},{\"end\":62568,\"start\":62552},{\"end\":62580,\"start\":62568},{\"end\":62591,\"start\":62580},{\"end\":62600,\"start\":62591},{\"end\":62610,\"start\":62600},{\"end\":62620,\"start\":62610},{\"end\":62994,\"start\":62982},{\"end\":63007,\"start\":62994},{\"end\":63018,\"start\":63007},{\"end\":63030,\"start\":63018},{\"end\":63037,\"start\":63030},{\"end\":63048,\"start\":63037},{\"end\":63427,\"start\":63416},{\"end\":63440,\"start\":63427},{\"end\":63459,\"start\":63440},{\"end\":63468,\"start\":63459},{\"end\":63480,\"start\":63468},{\"end\":63492,\"start\":63480},{\"end\":63853,\"start\":63845},{\"end\":63861,\"start\":63853},{\"end\":63871,\"start\":63861},{\"end\":63880,\"start\":63871},{\"end\":63886,\"start\":63880},{\"end\":63897,\"start\":63886},{\"end\":64255,\"start\":64245},{\"end\":64470,\"start\":64455},{\"end\":64479,\"start\":64470},{\"end\":64489,\"start\":64479},{\"end\":64900,\"start\":64888},{\"end\":64909,\"start\":64900},{\"end\":64923,\"start\":64909},{\"end\":64934,\"start\":64923},{\"end\":65301,\"start\":65291},{\"end\":65312,\"start\":65301},{\"end\":65764,\"start\":65752},{\"end\":65779,\"start\":65764},{\"end\":65786,\"start\":65779},{\"end\":65800,\"start\":65786},{\"end\":65811,\"start\":65800},{\"end\":65820,\"start\":65811},{\"end\":66152,\"start\":66139},{\"end\":66164,\"start\":66152},{\"end\":66176,\"start\":66164},{\"end\":66184,\"start\":66176},{\"end\":66188,\"start\":66184},{\"end\":66735,\"start\":66729},{\"end\":66744,\"start\":66735},{\"end\":66751,\"start\":66744},{\"end\":66758,\"start\":66751},{\"end\":67225,\"start\":67219},{\"end\":67234,\"start\":67225},{\"end\":67241,\"start\":67234},{\"end\":67248,\"start\":67241},{\"end\":67581,\"start\":67575},{\"end\":67590,\"start\":67581},{\"end\":67597,\"start\":67590},{\"end\":67604,\"start\":67597},{\"end\":68108,\"start\":68095},{\"end\":68383,\"start\":68370},{\"end\":68728,\"start\":68715},{\"end\":68737,\"start\":68728},{\"end\":68755,\"start\":68737},{\"end\":68768,\"start\":68755},{\"end\":68780,\"start\":68768},{\"end\":68792,\"start\":68780},{\"end\":69090,\"start\":69083},{\"end\":69101,\"start\":69090},{\"end\":69112,\"start\":69101},{\"end\":69122,\"start\":69112},{\"end\":69133,\"start\":69122},{\"end\":69144,\"start\":69133},{\"end\":69455,\"start\":69448},{\"end\":69466,\"start\":69455},{\"end\":69476,\"start\":69466},{\"end\":69483,\"start\":69476},{\"end\":69494,\"start\":69483},{\"end\":69505,\"start\":69494},{\"end\":70040,\"start\":70031},{\"end\":70047,\"start\":70040},{\"end\":70065,\"start\":70047},{\"end\":70081,\"start\":70065},{\"end\":70609,\"start\":70598},{\"end\":70622,\"start\":70609},{\"end\":70632,\"start\":70622},{\"end\":70638,\"start\":70632},{\"end\":70932,\"start\":70924},{\"end\":70945,\"start\":70932},{\"end\":70955,\"start\":70945},{\"end\":70959,\"start\":70955},{\"end\":71459,\"start\":71445},{\"end\":71627,\"start\":71616},{\"end\":71638,\"start\":71627},{\"end\":71646,\"start\":71638},{\"end\":71657,\"start\":71646},{\"end\":71671,\"start\":71657},{\"end\":71680,\"start\":71671},{\"end\":72125,\"start\":72111},{\"end\":72131,\"start\":72125},{\"end\":72140,\"start\":72131},{\"end\":72148,\"start\":72140},{\"end\":72157,\"start\":72148},{\"end\":72168,\"start\":72157},{\"end\":72615,\"start\":72604},{\"end\":72627,\"start\":72615},{\"end\":73123,\"start\":73112},{\"end\":73133,\"start\":73123},{\"end\":73149,\"start\":73133},{\"end\":73161,\"start\":73149},{\"end\":73453,\"start\":73441},{\"end\":73466,\"start\":73453},{\"end\":73475,\"start\":73466},{\"end\":73486,\"start\":73475},{\"end\":73767,\"start\":73752},{\"end\":73778,\"start\":73767},{\"end\":73786,\"start\":73778},{\"end\":74564,\"start\":74553},{\"end\":74577,\"start\":74564},{\"end\":74589,\"start\":74577},{\"end\":75059,\"start\":75048},{\"end\":75071,\"start\":75059},{\"end\":75341,\"start\":75329},{\"end\":75354,\"start\":75341},{\"end\":75639,\"start\":75619},{\"end\":75655,\"start\":75639},{\"end\":75666,\"start\":75655},{\"end\":75675,\"start\":75666},{\"end\":75683,\"start\":75675},{\"end\":75694,\"start\":75683},{\"end\":76049,\"start\":76038},{\"end\":76056,\"start\":76049},{\"end\":76063,\"start\":76056},{\"end\":76075,\"start\":76063},{\"end\":76083,\"start\":76075},{\"end\":76095,\"start\":76083},{\"end\":76752,\"start\":76744},{\"end\":76765,\"start\":76752},{\"end\":76774,\"start\":76765},{\"end\":76781,\"start\":76774},{\"end\":76790,\"start\":76781},{\"end\":77159,\"start\":77151},{\"end\":77166,\"start\":77159},{\"end\":77172,\"start\":77166},{\"end\":77181,\"start\":77172},{\"end\":77690,\"start\":77683},{\"end\":77700,\"start\":77690},{\"end\":77707,\"start\":77700},{\"end\":77715,\"start\":77707},{\"end\":77726,\"start\":77715},{\"end\":77732,\"start\":77726},{\"end\":78087,\"start\":78080},{\"end\":78099,\"start\":78087},{\"end\":78109,\"start\":78099},{\"end\":78115,\"start\":78109},{\"end\":78121,\"start\":78115},{\"end\":78558,\"start\":78551},{\"end\":78566,\"start\":78558},{\"end\":78574,\"start\":78566},{\"end\":78581,\"start\":78574},{\"end\":78587,\"start\":78581},{\"end\":78594,\"start\":78587},{\"end\":78598,\"start\":78594},{\"end\":79103,\"start\":79096},{\"end\":79112,\"start\":79103},{\"end\":79123,\"start\":79112},{\"end\":79130,\"start\":79123},{\"end\":79134,\"start\":79130},{\"end\":79553,\"start\":79545},{\"end\":79560,\"start\":79553},{\"end\":79569,\"start\":79560},{\"end\":79576,\"start\":79569},{\"end\":79583,\"start\":79576},{\"end\":79590,\"start\":79583},{\"end\":79594,\"start\":79590},{\"end\":80128,\"start\":80120},{\"end\":80135,\"start\":80128},{\"end\":80139,\"start\":80135},{\"end\":80523,\"start\":80517},{\"end\":80530,\"start\":80523},{\"end\":80536,\"start\":80530},{\"end\":80545,\"start\":80536},{\"end\":80552,\"start\":80545},{\"end\":80560,\"start\":80552},{\"end\":80564,\"start\":80560},{\"end\":81144,\"start\":81136},{\"end\":81153,\"start\":81144},{\"end\":81164,\"start\":81153},{\"end\":81176,\"start\":81164},{\"end\":81187,\"start\":81176},{\"end\":81603,\"start\":81594},{\"end\":81612,\"start\":81603},{\"end\":81619,\"start\":81612},{\"end\":81628,\"start\":81619},{\"end\":81634,\"start\":81628},{\"end\":81641,\"start\":81634}]", "bib_venue": "[{\"end\":59779,\"start\":59692},{\"end\":60256,\"start\":60234},{\"end\":60721,\"start\":60683},{\"end\":61193,\"start\":61181},{\"end\":61712,\"start\":61634},{\"end\":62255,\"start\":62240},{\"end\":62665,\"start\":62640},{\"end\":63074,\"start\":63065},{\"end\":63525,\"start\":63517},{\"end\":63962,\"start\":63922},{\"end\":64277,\"start\":64270},{\"end\":64595,\"start\":64516},{\"end\":64986,\"start\":64970},{\"end\":65379,\"start\":65337},{\"end\":65866,\"start\":65845},{\"end\":66290,\"start\":66213},{\"end\":66846,\"start\":66779},{\"end\":67314,\"start\":67276},{\"end\":67701,\"start\":67624},{\"end\":68150,\"start\":68133},{\"end\":68460,\"start\":68408},{\"end\":68829,\"start\":68817},{\"end\":69203,\"start\":69160},{\"end\":69603,\"start\":69526},{\"end\":70179,\"start\":70102},{\"end\":70667,\"start\":70638},{\"end\":71061,\"start\":70984},{\"end\":71484,\"start\":71475},{\"end\":71782,\"start\":71696},{\"end\":72212,\"start\":72196},{\"end\":72714,\"start\":72627},{\"end\":73195,\"start\":73187},{\"end\":73527,\"start\":73503},{\"end\":73916,\"start\":73814},{\"end\":74704,\"start\":74617},{\"end\":75114,\"start\":75092},{\"end\":75327,\"start\":75261},{\"end\":75727,\"start\":75718},{\"end\":76197,\"start\":76120},{\"end\":76554,\"start\":76524},{\"end\":76835,\"start\":76814},{\"end\":77295,\"start\":77209},{\"end\":77775,\"start\":77753},{\"end\":78200,\"start\":78142},{\"end\":78712,\"start\":78626},{\"end\":79220,\"start\":79134},{\"end\":79717,\"start\":79622},{\"end\":80185,\"start\":80164},{\"end\":80663,\"start\":80591},{\"end\":81217,\"start\":81206},{\"end\":81685,\"start\":81669},{\"end\":61795,\"start\":61714},{\"end\":65395,\"start\":65381},{\"end\":66364,\"start\":66292},{\"end\":66908,\"start\":66848},{\"end\":67320,\"start\":67316},{\"end\":67774,\"start\":67703},{\"end\":69680,\"start\":69605},{\"end\":70255,\"start\":70181},{\"end\":71135,\"start\":71063},{\"end\":72729,\"start\":72716},{\"end\":73992,\"start\":73971},{\"end\":74710,\"start\":74706},{\"end\":76271,\"start\":76199},{\"end\":77303,\"start\":77297},{\"end\":78214,\"start\":78202},{\"end\":78720,\"start\":78714},{\"end\":79228,\"start\":79222},{\"end\":80684,\"start\":80665}]"}}}, "year": 2023, "month": 12, "day": 17}