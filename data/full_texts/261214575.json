{"id": 261214575, "updated": "2023-10-24 12:54:14.859", "metadata": {"title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models", "authors": "[{\"first\":\"Wenqi\",\"last\":\"Shao\",\"middle\":[]},{\"first\":\"Mengzhao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Zhaoyang\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Peng\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Lirui\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Zhiqian\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Kaipeng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Peng\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Qiao\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Luo\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights through a learnable equivalent transformation. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family with the size of 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4, W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. Codes and models are available at \\url{https://github.com/OpenGVLab/OmniQuant}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2308.13137", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2308-13137", "doi": "10.48550/arxiv.2308.13137"}}, "content": {"source": {"pdf_hash": "eb2c2330177f765038a2b17e2ee3498965865797", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.13137v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7f01636779f1d65a5a679bbd69a7654b3810837d", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/eb2c2330177f765038a2b17e2ee3498965865797.txt", "contents": "\nWork in progress OMNIQUANT: OMNIDIRECTIONALLY CALIBRATED QUANTIZATION FOR LARGE LANGUAGE MODELS\n22 Oct 2023\n\nWenqi Shao \nOpenGVLab\nShanghai AI Laboratory\n\nMengzhao Chen \nOpenGVLab\nShanghai AI Laboratory\n\nZhaoyang Zhang \nThe Chinese University of Hong Kong\n\n\nPeng Xu \nOpenGVLab\nShanghai AI Laboratory\n\nThe University of Hong Kong\n\n\nLirui Zhao \nZhiqian Li \nThe University of Hong Kong\n\n\nKaipeng Zhang \nOpenGVLab\nShanghai AI Laboratory\n\nPeng Gao \nOpenGVLab\nShanghai AI Laboratory\n\nYu Qiao \nOpenGVLab\nShanghai AI Laboratory\n\nPing Luo \nOpenGVLab\nShanghai AI Laboratory\n\nThe University of Hong Kong\n\n\nWork in progress OMNIQUANT: OMNIDIRECTIONALLY CALIBRATED QUANTIZATION FOR LARGE LANGUAGE MODELS\n22 Oct 2023104F47F01D7F7C45F578BB22E60D8369arXiv:2308.13137v2[cs.LG]\nLarge language models (LLMs) have revolutionized natural language processing tasks.However, their practical deployment is hindered by their immense memory and computation requirements.Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization.To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters.OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET).LWC modulates the extreme values of weights by optimizing the clipping threshold.Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights through a learnable equivalent transformation.Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weightactivation quantization.For instance, the LLaMA-2 model family with the size of 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples.Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16.Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices.\n\nINTRODUCTION\n\nLarge language models (LLMs) such as GPT-4 (Bubeck et al., 2023) and LLaMA (Touvron et al., 2023a), have demonstrated impressive performance across various natural language benchmarks (Hendrycks et al., 2020;Bisk et al., 2020;Zellers et al., 2019).Furthermore, the language understanding capabilities inherent in LLMs can be successfully transferred into multimodal models (Mu et al., 2023;Xu et al., 2023;Zhang et al., 2023).Thereby, LLMs can be regarded as precursors to artificial general intelligence (Bubeck et al., 2023).However, the considerable computational and memory requirements of LLMs pose substantial challenges.For instance, the GPT-3 model (Brown et al., 2020) requires 350G of memory to load its parameters in FP16 format, which corresponds to the requirement of at least five A100-80G GPUs for inference.This significant demand for computational resources and associated communication overheads impedes the practical deployment of LLMs in real-world applications.\n\nQuantization has shown to be promising to mitigate both computational and memory overhead in LLMs.In general, it comes in two types including post-training quantization (PTQ) and quantization-aware training (QAT).Although QAT can lead to more competitive accuracy than PTQ, it is not practical due to the high training cost because the whole model is trained with the awareness of the quantization process.As a result, PTQ is commonly utilized in existing quantization methods on LLMs.For example, lots of PTQ methods (Frantar et al., 2022;Lin et al., 2023;Dettmers et al., 2023b;Lee et al., 2023) reduce memory consumption by weight-only quantization which quantizes the weights while maintaining full-precision activation.To further reduce the computational overhead, another line of work (Xiao et al., 2023;Wei et al., 2022;Yuan et al., 2023;Wei et al., 2023) employs weight-activation quantization which quantizes both weight and activation into low-bit values for the execution of low-bit matrix multiplication.\n\nExisting quantization methods have demonstrated significant achievements in various scenarios, including W4A16 (i.e.4-bit weight and 16-bit activation) weight-only quantization such as (Lin et al., 2023;Dettmers et al., 2023b;Lee et al., 2023), as well as W8A8 weight-activation quantization (Wei et al., 2023).However, they usually exhibit significant performance degradation when confronted with low-bit quantization, such as W2A16 and W4A4, as illustrated in Figure 1 (b & c).This performance shortfall in low-bit quantization can be attributed to the fact that these methods (Frantar et al., 2022;Lin et al., 2023;Wei et al., 2023) primarily rely on handcrafted quantization parameters such as migration strength (Xiao et al., 2023) and scaling parameters (Wei et al., 2023), which often leads to lower performance.Although Quantization-Aware Training (QAT) (Liu et al., 2023a) is effective in determining the optimal quantization configurations, it introduces substantial training overhead in both training and data efficiency.It is thus hard to quantize LLMs with QAT-based techniques efficiently such as LLMQAT (Liu et al., 2023a).For instance, GPTQ (Frantar et al., 2022), a PTQ approach, can complete the quantization of LLaMA-13B in an hour using 128 samples on a single A100 GPU, while LLM-QAT (Liu et al., 2023a) requires 100k samples and hundreds of GPU hours.This leads us to a central question: can we attain the performance of QAT, while maintaining the time and data efficiency of PTQ?\n\nThis paper introduces a novel quantization technique, OmniQuant, which effectively addresses the above question.OmniQuant achieves state-of-the-art performance across various quantization scenarios, particularly in low-bit settings, while preserving the time and data efficiency of PTQ, as illustrated in Figure 1.Unlike Quantization-Aware Training (QAT) (Liu et al., 2023a) which involves cumbersome weight optimization, OmniQuant freezes the original full-precision weight and only incorporates a few learnable quantization parameters.As shown in Figure 2, OmniQuant consists of two key components that incorporate different types of learnable quantization parameters, including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET).Specifically, LWC modulates the extreme values of weights by optimizing the clipping threshold.In the meanwhile, LET tackles activation outliers by learning mathematically equivalent transformations in a transformer encoder.\n\nInstead of jointly optimizing all parameters across the LLM, OmniQuant sequentially quantizes the parameters of one layer before moving on to the next under a block-wise quantization error minimization framework.In this way, OminiQuant can be optimized efficiently using a simple Stochastic Gradient Descent (SGD) algorithm.Thanks to the differentiable optimization, LWC and LET can be seamlessly integrated into the quantization.We find that LWC can mitigate the difficulty in quantizing weights and LET further shifts the challenge of quantization from activations to weights, facilitating OmniQuant a versatile quantization framework for both weight-only and weight-activation quantization.\n\n\nRELATED WORK\n\n2.1 QUANTIZATION METHODS.\n\nQuantization reduces neural network bit-precision, leading to smaller models and faster inference.Current methods are largely divided into Quantization Aware Training (QAT) (Liu et al., 2023a) and Post-training Quantization (PTQ) (Xiao et al., 2023;Frantar et al., 2022).While QAT maintains performance by simulating quantization during training, its training cost makes it unsuitable for LLM.PTQ techniques like AdaRound (Nagel et al., 2020) and BRECQ (Li et al., 2021) use gradient optimization to determine optimal rounding, but tuning all weights is time-intensive for larger models.Thus, most LLM quantization methods (Xiao et al., 2023;Frantar et al., 2022;Dettmers et al., 2023b;Lee et al., 2023;Wei et al., 2023) prioritize training-free PTQ, which limit performance in lower-bit situations.Our goal is to integrate gradient updates in LLM quantization, mirroring QAT's approach, while retaining PTQ's efficiency.\n\n\nQUANTIZATION OF LLM.\n\nConsider the quantized object, exiting LLM quantization can be classified into two fields: weightonly quantization and weight-activation quantization.\n\nWeight-only quantization.Weight-only quantization focuses on converting weights to low-bit values.For instance, GPTQ (Frantar et al., 2022) uses block-wise reconstruction for 3/4-bit quantization.SpQR (Dettmers et al., 2023b), OWQ (Lee et al., 2023), and AWQ (Lin et al., 2023) emphasize the significance of weights tied to higher-magnitude activations.Therefore, SpQR and OWQ employ mixed-precision quantization ot safeguard vital weights, while AWQ opts for channel-wise scaling to avoid mixed-precision's hardware inefficiency.Qlora (Dettmers et al., 2023a) and INT2.1 (Chee et al., 2023) restore the capabilities of the quantized model through parameter-efficient fine-tuning.Our method, in contrast, enhances the quantization process directly, making OmniQuant complementary to Qlora and INT2.1.Weight-activation quantization.Weight-activation quantization compresses both weights and activations.SmoothQuant (Xiao et al., 2023), LLM.int8() (Dettmers et al., 2022), and Outlier Suppression (Wei et al., 2022) achieve W8A8 quantization by managing activation outliers.LLM.int8() uses mixed-precision decomposition, while the other two employ channel-wise scaling.Furthermore, Outlier Suppression+ (Wei et al., 2023) adds channel-wise shifting to drive W6A6 quantization.Unlike previous heuristic designs, we use gradient optimization and expand equivalent transformations to attention mechanisms, further boosting the K/V cache quantization.Recently, RPTQ (Yuan et al., 2023) and LLM-QAT (Liu et al., 2023a) have achieved W4A4 quantization.However, RPTQ adopts deployment-unfriendly group-wise activation quantization, and LLM-QAT employs timeconsuming QAT.In distinction from RPTQ and LLM-QAT, we achieve W4A4 quantization through deployment-friendly per-token quantization and maintain the PTQ efficiency.\n\n\nOMNIQUANT\n\nChallenge of LLM quantization.Two main difficulties lie in quantizing an LLM.First, the activation is hard to quantize due to the existence of outlier channels.Considering that weight distribution is flat and uniform, SmoothQuant (Xiao et al., 2023) and Outlier Suppression+ (Wei et al., 2023) tackle this issue by migrating the quantization difficulty from activations to weights with a predefined migration strength.Second, the quantization error of weights also plays a pivotal role in the final performance due to the importance of weights corresponding to activations.SqQR (Dettmers et al., 2023b) and OWQ (Lee et al., 2023) propose to retain crucial weights in full-precision, while AWQ (Lin et al., 2023) safeguards these weights using grid-searched channel-wise scaling.Although these methods have achieved certain success in compressing various LLMs, they often lead to suboptimal performance and fail to deal with extremely low-bit quantization due to the crude design of hand-crafted quantization parameters such as migration strength and scaling factors.\n\nIn this section, we introduce a differentiable quantization technique for LLM called OmniQuant where quantization parameters are learned with better flexibility.Towards this goal, OmniQuant is implemented with a block-wise quantization error minimization framework as presented in Sec.3.1.\n\nTo tackle the aforementioned challenges of LLM quantization, we devise two novel strategies for additional learnable quantization parameters including a learnable weight clipping (LWC) to mitigate the difficulty in quantizing weights and a learnable equivalent transformation (LET) to further shift the challenge of quantization from activations to weights.We introduce LWC and LCT in Sec.\n\n3.2 and Sec.3.3, respectively.\n\n\nBLOCK-WISE QUANTIZATION ERROR MINIMIZATION\n\nPrevious PTQ methods with gradient optimization, such as AdaRound (Nagel et al., 2020), BRECQ (Li et al., 2021) cannot be applied in models with billions of parameters because they are hard to optimize due to the huge solution space.Instead of turning the whole model, we propose a new optimization pipeline with block-wise quantization error minimization where the additional quantization parameters can be optimized in a differentiable manner.We formulate the optimization goal as follows.\n\narg min\n\u03981,\u03982 ||F(W, X) \u2212 F Q w (W; \u0398 1 , \u0398 2 ), Q a (X, \u0398 2 ) ||,(1)\nwhere F represents the mapping function for a transformer block in the LLM, W and X are fullprecision weight and activation, Q w (\u2022) and Q a (\u2022) represent weight and activation quantizer, respectively, \u0398 1 and \u0398 2 are quantization parameters in learnable weight clipping (LWC) and learnable equivalent transformation (LET), respectively.The Block-wise quantization in Eqn.\n\n(1) sequentially quantizes the parameters of one transformer block before moving on to the next.\n\nBlock-wise minimization in Eqn.\n\n(1) has two advantages.First, equipped with block-wise minimization in Eqn.\n\n(1), OmniQuant can optimize quantization parameters in LWC and LET jointly, making it capable enough to encompass both weight-only and weight-activation quantization.Second, block-wise minimization is easy to optimize with minimal resource requirements.OmniQuant only determines a few quantization parameters with optimality, which is easier than optimizing the whole weights in previous PTQ-based methods (Nagel et al., 2020;Li et al., 2021).Empirically, we find that all models from the LLaMA-2 family (Touvron et al., 2023b) can be quantized on a single A100-40G GPU utilizing only 128 training samples.\n\n\nLEARNABLE WEIGHT CLIPPING\n\nOmniQuant employs a module of learnable weight clipping (LWC) to reduce the difficulty of quantizing the weights in an LLM.Similar to previous methods with learnable clipping threshold (Esser et al., 2019;Liu et al., 2022;Choi et al., 2018), LWC also determines the optimal dynamic range of the weights by optimizing a clipping threshold.However, we find that directly employing prior arts such as PACT (Choi et al., 2018) and LSQ (Esser et al., 2019) in quantization would produce unsatisfactory performance, as demonstrated in LLM-QAT (Liu et al., 2023a).A similar result has been also observed in Table A8 in the Appendix.\n\nInstead of directly learning a clipping threshold as did in previous methods (Esser et al., 2019;Choi et al., 2018), LWC optimizes a clipping strength as formulated by\nW q = clamp(\u230a W h \u2309 + z, 0, 2 N \u2212 1), where h = \u03b3 max(W) \u2212 \u03b2 min(W) 2 N \u2212 1 , z = \u2212\u230a \u03b2 min(W) h \u2309\n(2) where \u230a\u2022\u2309 indicates round operation.N is the target bit number.W q and W denote the quantized and full-precision weights, respectively.h is the normalization factor for weights and z is the zeropoint value.The clamp operation constrains the value within the range of N -bit integer, specifically [0, 2 N \u2212 1].In Eqn.(2), \u03b3 \u2208 [0, 1] and \u03b2 \u2208 [0, 1] are learnable clipping strengths for the upper and the lower bound of weights, respectively.We instantiate \u03b3 and \u03b2 by the sigmoid function1 .Hence, \u0398 1 = {\u03b3, \u03b2} in Eqn.(1).\n\nNote that LWC degrades into a vanilla MinMax quantization scheme used in existing works (Xiao et al., 2023), Frantar et al. (2022) when \u03b3 = 1 and \u03b2 = 1.By inheriting the benefits of Min-Max quantization, LWC only needs to adjust the clipping strengths to determine an optimal clipping threshold, which would reduce the optimization difficulty.Clipped by an optimal threshold, the original weights would be easy to quantize.As indicated by the experiments in Table 1, our proposed learnable weight clipping method significantly outperforms previous weight-only quantization techniques (Frantar et al., 2022;Lin et al., 2023)).\n\n\nLEARNABLE EQUIVALENT TRANSFORMATION\n\nOther than LWC which enables quantization-friendly weights by optimizing the clipping threshold, we further reduce the difficulty of weight-activation quantization by a learnable equivalent transformation (LET).Considering that outliers in the activation map are systematic and unique to specific channels, previous methods such as SmoothQuant (Xiao et al., 2023) migrate the difficulty of quantization from activations to weights with a mathematically equivalent transformation.However, they hand-craft the equivalent parameters, leading to suboptimal results.\n\nThanks to the inclusion of block-wise quantization error minimization, our LET can determine the optimal equivalent parameters in a differentiable way.Inspired by SmoothQuant (Xiao et al., 2023) and Outlier Suppression+ (Wei et al., 2023), we adopt channel-wise scaling and channel-wise shifting to manipulate the activation distribution, providing an effective solution for the outlier issue.Specifically, we investigate the equivalent transformation across both the linear layer and attention operation, as illustrated in Figure3.\n\nLinear layer.The linear layer takes an input token sequence X \u2208 R T \u00d7Cin where T is the token length and is the multiplication of the weight matrix W \u2208 R Cin\u00d7Cout and bias vector B \u2208 R 1\u00d7Cout .A mathematically equivalent linear layer is expressed as:\nY = XW + B = [(X \u2212 \u03b4) \u2298 s X ] \u2022 [s \u2299 W W ] + [B + \u03b4W B ](3)\nwhere Y represents the output, s \u2208 R 1\u00d7Cin and \u03b4 \u2208 R 1\u00d7Cin are channel-wise scaling and shifting parameters, respectively, X, W and B are equivalent activation, weight and bias, respectively, '\u2298' and '\u2299' are elementwise division and multiplication.By Eqn.\n\n(3), the activations are transformed to be quantization-friendly at a cost of increased quantization difficulty in weights.In this sense, LWC in Sec.3.2 can improve the performance of weight-activation quantization achieved by LET because it renders weights quantization-friendly.Finally, we perform quantization on transformed activations and weights, as given by\nY = Q a ( X)Q w ( W) + B,(4)\nwhere Q a is the vanilla MinMax quantizer and Q w is the MinMax quantizer with learnable weight clipping (i.e.our LWC).\n\nNote that the scaling and shifting parameters in X can be absorbed into the previous normalization or linear layer and the the scaling factors in W can be fused into the original linear weight W. Therefore, the equivalent transformation in Eqn.(3) can effectively reduce quantization errors without introducing additional parameters or costs.We employ this equivalent transformation in all linear layers of the LLM except for the second linear layer of FFN as shown in Figure3.This may be because the high sparsity of features after the non-linear layer (Liu et al., 2023b) leads to unstable gradients when applying learnable equivalent transformations.\n\nAttention operation.Beyond the linear layer, the attention operation also accounts for a significant proportion of the computation.Additionally, the auto-regressive pattern of LLM necessitates storing the key-value(KV) cache for each token, which results in substantial memory demands for long sequences.Therefore, we also quantize Q/K/V matrixes into low-bit in the weight-activation quantization setting.Specifically, the learnable equivalent transform of the self-attention affinity matrix can be written as:\nP = Softmax(QK T ) = Softmax((Q \u2298 s a Q )(s a \u2299 K T KT )). (5)\nwhere s a \u2208 R 1\u00d7Cout is the scaling factor in the affinity matrix.Similar to Eqn.(4), the quantized affinity matrix calculation is expressed as\nP = Softmax(Q a ( Q)Q a ( K T )).\nHere we also use Min-Max quantization scheme as Q a to quantize Q/ K matrixes.From Eqn.(4) and Eqn.( 5) we know that \u0398 2 = {\u03b4, s, s a } in Eqn.(1).\n\nThe channel-wise scaling factors in Q and K, as seen in Eq.( 5), can be absorbed into linear weights of the query and key projection, respectively.It is worth mentioning that the explicit transformation of V is omitted as its distribution has already been channel-wise altered by the inverse transformation associated with the output projection linear layer.Training The channel-wise scaling factor is initialized with SmoothQuant (Xiao et al., 2023), and the channel-wise shifting factor is initialized using Outlier Suppression+ (Wei et al., 2023).To optimize the learnable parameters, we utilize the AdamW optimizer with zero weight decay.The learning rate for learnable weight clipping and equivalent transformation is set as 5e \u2212 3 and 1e \u2212 2, respectively.We employ a calibration dataset consisting of 128 randomly selected 2048-token segments from WikiText2 (Merity et al., 2016).The entire training process is facilitated on a single Nvidia A100 GPU, using a batch size of 1 over 20 epochs, except for W2A16 quantization that leverages 40 epochs.For weight-activation quantization, both learnable weight clipping and equivalent transformation are activated.For weight-only, both are used for OPT, but only the clipping is for LLaMA, as Table A1 shows negligible benefits from the equivalent transformation for LLaMA.\n\nModels.We test on OPT(125M-66B) (Zhang et al., 2022)), LLaMA(7B-65B) (Touvron et al., 2023a), LLaMA-2(7B-70B) (Touvron et al., 2023b), Falcon-180B (Penedo et al., 2023), and instruction-tuned LLaMA-2-chat (Touvron et al., 2023b) for generalizability.While the main paper highlights the LLaMA results, comprehensive details for other models are available in Sec.A6 of the Appendix.\n\nEvaluation.Following the previous work (Lin et al., 2023;Frantar et al., 2022), we evaluate quantized models by reporting the perplexity of language generation experiments, specifically on Wiki-Text2 (Merity et al., 2016), PTB (Marcus et al., 1994)), C4 (Raffel et al., 2020).Moreover, accuracy is evaluated in zero-shot tasks including PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), BoolQ (Clark et al., 2019), and HellaSwag (Clark et al., 2018).We adhere to the GPTQ (Frantar et al., 2022) settings for language generation experiments, and implement the lm-eval-harness (Gao et al., 2021) for the execution of all zero-shot tasks.\n\nBaselines.For weight-only quantization, we compare with vanilla round-to-nearest quantization (RTN), GPTQ (Frantar et al., 2022), and AWQ (Lin et al., 2023).For weight-activation quantization, we compare our method with SmoothQuant (Xiao et al., 2023), RPTQ (Yuan et al., 2023), and the recent QAT method LLM-QAT (Liu et al., 2023a).Note that we reproduce SmoothQuant with per-channel weight quantization and per-token activation quantization for fair comparisons.\n\n\nWEIGHT-ONLY QUANTIZATION RESULTS\n\nThe results of the LLaMA family can be found in Table 1, while the results for OPT are presented in the Sec.A6 of Appendix.As illustrated by the tables, OmniQuant consistently outperforms the prior LLM weight-only quantization method across various LLM families (OPT, LLaMA-1, LLaMA-2) and diverse quantization configurations, including W2A16, W2A16g128, W2A16g64, W3A16, W3A16g128, W4A16, and W4A16g128.These findings suggest OmniQuant's versatility, being adaptable to a multitude of quantization configurations.For instance, while AWQ (Lin et al., 2023) is particularly effective with group-wise quantization, OmniQuant demonstrates superior performance across both channel-wise and group-wise quantization.Furthermore, the performance benefits of OmniQuant become more pronounced as the quantization bit size decreases.\n\n\nWEIGHT-ACTIVATION QUANTIZATION RESULTS\n\nIn weight-activation quantization, our main focus lies on W6A6 and W4A4 quantization.We exclude W8A8 quantization as SmoothQuant can nearly achieve lossless W8A8 quantized models when compared with full-precision counterparts.The results of the LLaMA family can be found in Table 2, while the results for OPT are presented in Table A16 of Appendix.Table 2 illustrates the zero-shot task accuracy of LLaMA weight-activation quantization.Notably, OmniQuant markedly enhances the average accuracy by +4.99% \u223c +11.80% across various models at W4A4 quantization.\n\nRemarkably, in the LLaMA-7B, OmniQuant even surpasses the recent QAT method, LLM-QAT (Liu et al., 2023a), by an impressive margin of +6.22%.This improvement demonstrates the efficacy of incorporating additional learnable parameters, which proves to be more beneficial than the global weight tuning utilized by QAT.(Lin et al., 2023), and proposed OmniQuant under GPT-4 evaluation protocol (Chiang et al., 2023).Win rates are calculated without considering tie samples.A higher win rate indicates the better performance of the former of vs. pairs.To validate the generalization capability of our method, we test the quantization on LLaMA-2chat (Touvron et al., 2023b), an instruction-tuned model for chatbots.Using the GPT-4 evaluation protocol (Chiang et al., 2023), performance is assessed on the Vicuna benchmark (Chiang et al., 2023) comprising 80 questions.To negate position bias (Zheng et al., 2023), each pair is compared in both sequences, totaling 160 trials per comparison.Figure 4 compares RTN, AWQ (Lin et al., 2023), and OmniQuant.In LLaMA-2-7b-chat, OmniQuant matches AWQ with a 50% win rate but surpasses RTN more (80.3%vs. 69.4%).In LLaMA-2-13b-chat, while AWQ lags behind RTN, OmniQuant consistently improves quantization model performance.\n\n\nACCELERATION ON REAL DEVICE\n\nMLC-LLM2 provides a versatile deployment solution for diverse language models across various hardwares.It particularly excels in deploying quantized models on CUDA.One of OmniQuant's strengths lies in its ability to avoid extra operations for quantized models, allowing MLC-LLM to seamlessly run models created with OmniQuant.Table,3 shows memory requirements and inference speeds of the LLaMA family on an NVIDIA A100-80G.'Weights Memory (WM)' represents quantized weight storage, and 'Running Memory (RM)' indicates the memory for inference, with the latter being higher due to certain retained activations.Inference speed is gauged by generating 512 tokens.It is evident that quantized models significantly reduce memory usage compared to 16-bit full-precision models.For instance, models with W4A16g128 and W2A16g128 quantization almost double the inference speed.However, MLC-LLM's support for INT3/INT2 is currently suboptimal, particularly for INT3.Enhancements to INT3/INT2 quantization speed are in our future roadmap.Additionally, we only explore the deployment of weight-only quantization in this study due to that W4A4 and W6A6 quantization methods lack out-of-the-box hardware support.\n\n\nCONCLUSION\n\nWe present OmniQuant, a method advancing weight-only and weight-activation quantization to lowbit formats.OmniQuant's core principle is to retain original full-precision weights while adding learnable parameters.It uses learnable weight clipping and learnable equivalent transformation to optimize weight and activation for quantization.While incorporating gradient updates, OmniQuant maintains training efficiency comparable to existing PTQ methods.It outperforms current methods in language generation and zero-shot tasks, and is suited for instruction-tuned LLMs.Additionally, OmniQuant also ensures hardware compatibility as its added parameters can be absorbed.\n\nIn this appendix, we provide further details as follows:\n\n\u2022 Sec.A1: Presents the pseudo code for our OmniQuant algorithm.\n\n\u2022 Sec.A2: Details ablation studies, encompassing the efficacy of each component, design choices for the learnable equivalent transformation, training time, and calibration data.\n\n\u2022 Sec.A3: Provides the detailed training time for the LLaMA family.\n\n\u2022 Sec.A4: Explores the internal mechanisms of the proposed method.\n\n\u2022 Sec.A5: Compares the proposed LWC with other clipping-based quantization approaches.\n\n\u2022 Sec.A6: Showcases the complete results for OPT, LLaMA-1, LLaMA-2, and Falcon models.\n\n\nA1 OVERALL ALGORITHM\n\nThe comprehensive training algorithm of OmniQuant is illustrated in Algorithm 1.We employ a block-wise calibration strategy comprising three steps: initialization of learnable parameters (Lines 4-5), training these learnable parameters (Lines 6-15), transforming the model with learned parameters, and then quantization(Lines 16-18).The OmniQuant algorithm finds the optimal transformation to enhance the quantization compatibility of the LLM model.Additionally, due to the elegant design, OmniQuant can achieve rapid convergence using a small calibration dataset.\n\nAlgorithm 1 Overall algorithm of OmniQuant.\n\nInput: calibration dataset X, pre-trained LLM model M Output: quantized model.1: X f p = X q = X \u25b7 init inputs of full-precision and quantized models.2: for B i in M do:\n\n\u25b7 block-wise calibration 3: for k in epochs do:\nX f p = B i (X f p ) \u25b7 update\n7:\n\nfor (x q ,x f p ) in (X q ,X f p ) do 8:\nB \u2032 i = LET(B i ,\u0398 2 )\n\u25b7 With Eq.(3),Eq.( 5)\n\n9:\nB \u2032 i = Quantization with LWC(B \u2032 i ,\u0398 1 ) \u25b7 With Eq.(2) 10: x \u2032 q = B \u2032 i (x q ) 11: loss = ||x f p \u2212 x \u2032 q || 2 \u25b7 WithB i = LET(B i ,\u0398 2 )\n17:\nB i = Quantization with LWC(B i ,\u0398 1 )\n\u25b7 obtain the quantized block 18:\nX q = B i (X q )\n\u25b7 update the input of quantized model 19: end for 20: return quantized model M\n\n\nA2 ABLATION STUDIES\n\nEfficacy of each component.Table A1 reveals that the baseline model incorporates both LWC and LET, labeled as 'LWC+LET'.We further investigate their individual contributions by remove each component.Both components positively influence performance, but LET proves essential for weight-activation quantization.Disabling it for W4A4 results in a sharp rise in perplexity to e3, primarily due to challenges with activation quantization outliers.For weight-only quantization, LET significantly boosts OPT's performance but offers slight enhancement for LLaMA, explained by LLaMA's few weight outliers.For example, in naive W3A16 quantization (-LWC-LET), LLaMA Design choices of learnable equivalent transformation.In comparison to the equivalent transformation incorporated in SmoothQuant (Xiao et al. (2023)), our approach additionally implements channel-wise shifting and attention transformation.The effects of these innovations are evaluated in Table A2.We can observe that both modifications enhance the performance of weight-activation quantization.However, the incremental benefit from the equivalent transformation in the attention operation is comparatively minor.This discrepancy is primarily due to the majority of outliers existing in the output of the normalization layer while being less prevalent in the Q/K/V matrix.\n\nTraining Time As illustrated in Table A3, LLaMA-7B was trained across various epochs to determine the optimal convergence time.Most quantization configurations converge within 20 epochs, with the exception of W2A16, which necessitates 80 epochs.Consequently, we establish a training epoch of 20 for all configurations, except for W2A16, for which we set it to 40 in consideration of the training time.\n\nCalibration Data OmniQuant utilizes gradient optimization on constrained calibration datasets, sourced from WikiText2 and comprising 128 segments with 2048 tokens each.This prompts concerns about potential overfitting to the calibration dataset.To explore this, we evaluated the calibration dataset's influence using two other datasets: Pile (Gao et al. (2020)) and c4 (Raffel et al. (2020)).As depicted in Table A4, the variance in perplexity across diverse calibration datasets is marginal, fluctuating between 0.0006 and 0.17.This underscores OmniQuant's robustness concerning calibration set distribution.Furthermore, the data efficiency of OmniQuant was gauged by modulating the number of training samples, as presented in Table A5.Remarkably, OmniQuant converges with as few as 16 samples.Our selection of 128 samples aligns with established practices in prior works (Frantar et al. (2022); Lin et al. ( 2023)).\n\n\nA3 TRAINING TIME\n\nAs shown in Table A6, we report the training time of the proposed OmniQuant within the LLaMA family.Note that for LLaMA, we only activate learnable weight clipping for weight-only quantization.Therefore, the training time for weight-only quantization is shorter relative to weight-activation quantization, given the fewer learnable parameters involved.While our proposed method necessitates a training time that is approximately 5\u00d7 greater than GPTQ, it remains markedly faster than QAT methods, which demand hundreds of GPU hours.\n\n\nA4 PERFORMANCE ANALYSIS\n\nIn this section, we investigate the internal mechanism of learnable weight clipping and learnable equivalent transformation respectively.Further, we show that with OmniQuant, 3-bit and 4-bit achieve similar trade-off between model bits and perplexity.\n\nLearnable weight clipping.In addition to perplexity and accuracy, the quality of a quantization method can intuitively be evaluated by calculating the distance between quantized models and their full-precision counterparts.This is demonstrated in Table A7, where we detail the l 1 distance of weights and activations for LLaMA-7B's weight-only quantization.We can observe that the proposed Learned Weight Clipping (LWC) substantially decreases the l 1 distance for both weights and activations.It's noteworthy that, in certain instances, the l 1 distance for quantized models without LWC is similar to that of those utilizing LWC.However, models incorporating LWC exhibit markedly lower activation l 1 distances.This observation underpins the argument that LWC can effectively balance quantization precision between outlier and regular values.\n\nAdditionally, we illustrate the distribution of the learned clipping scale (\u03b3 and \u03b2) as delineated in Eq. ( 2) in Figure A1.It is apparent that LWC can learn different clippings for diverse quantization configurations.For instance, with per-channel weight quantization W3A16 as depicted in Figure A1(a), the learned clipping scale showcases a normal distribution.This suggests that approximately half of the outliers are being clipped.In the case of group-wise quantization, the learned clipping scale exhibits a long-tailed distribution, implying that most quantized groups are associated with minimal clipping.Note that lower bits exhibit more pronounced clipping.For example, W2A16g128 Table A7: l 1 distance between quantized model and full-precision model.||W \u2212 W q || indicates the average l 1 distance between quantized weight and full-precision weight.||X \u2212 X q || denotes the l 1 distance between the output of last transformer block.\n\nLLaMA  possesses a 50% clipping scale larger than 0.95, whereas, in W3A16g128, this percentage rises to 70%.\n\nLearnable equivalent transformation Figure A2 provides visualizations of the intermediate activation in the linear layer.It is apparent that several outlier channels in the original activation (Figure A2(a)) possess significantly larger magnitudes compared to the regular channels, thereby creating an incompatibility with activation quantization.Although SmoothQuant mitigates this issue to some degree, such as reducing the outlier magnitude from 70 to 2, Figure A2(b) reveals that the magnitude of outlier channels still remains notably larger than that of other regular channels after SmoothQuant.This phenomenon can be attributed to SmoothQuant's heuristic approach in deriving channel-wise scaling, which inevitably makes it challenging to discover an optimal solution.The impact of the proposed LET is depicted in Figure A2(c).It is noteworthy that the magnitude disparity between the outlier and regular channels is markedly diminished.This homogenization of the activation distribution, facilitated by the LET, empowers OmniQuant to efficiently steer the weight-activation quantization towards a low-bit scheme.Scaling laws.Quantization serves as a potent strategy to curtail the total model bits, thereby facilitating the deployment of LLMs on edge or consumer devices with restricted memory.However, the total model bits are contingent on both the number of parameters within the original model and the quantization bits.Therefore, given a model bits constraint, the challenge arises: how does one optimally determine the number of parameters for the full-precision model and the quantization bits?Tim Dettmers (Dettmers & Zettlemoyer (2023)) demonstrated that 4-bit quantization establishes a universally optimal balance between the total model bits and zero-shot accuracy.Nonetheless, in this study, as shown in Figure A3,we would like to claim that OmniQuant can make 3-bit quantization achieve comparable performance like 4-bit quantization in the trade off between model bits and perplexity.\n\n\nA5 COMPARISONS WITH CLIPPING-BASED METHOD\n\nIn this paper, we proposed a novel method, learnable weight clipping (LWC), designed to adaptively determine the weight clipping threshold.LWC sets the threshold by scaling the original minimum and maximum values to delineate the solution space.We compare LWC against existing clippingbased methods: PACT and LSQ.While PACT directly determines the clipping threshold, LSQ focuses on the direct derivation of the scaling factor and zero-point.Both PACT and LSQ were initially formulated as QAT methods, accounting for both weight and activation clipping.For an equitable comparison, our examination is restricted to weight clipping.We integrated PACT and LSQ into our optimization pipeline in lieu of LWC.Table A8 illustrates that while PACT and LSQ enhance the performance of weight-only quantization in comparison to MinMax quantization, their efficacy diminishes in the weight-activation quantization setting.This decline can be attributed to the proposed LET during activation quantization, which alters the weight distribution in each training iteration, undermining the convergence of both LSQ and PACT.In contrast, LWC defines relative scaling values instead of absolute metrics, making it proficient in handling changes in weight distribution.\n\n\nA6 FULL RESULTS\n\nIn this section, we provide a comprehensive presentation of our results across various datasets to complement the main paper.Specifically, the results include:\n\n\u2022 The perform overview (Figure A4)\n\n\u2022 Experiments results on extreme large model Falcon-180B (Table A9).\n\n\u2022 C4 perplexity with weight-only quantization in the LLaMA families (Table A10).\n\n\u2022 PTB perplexity with weight-only quantization in OPT families (Table A12).\n\n\u2022 C4 perplexity with weight-only quantization in OPT families (Table A13).\n\n\u2022 WikiText2 perplexity for weight-activation quantization in the LLaMA families (Table A14).\n\n\u2022 C4 perplexity for weight-activation quantization in the LLaMA families (Table A15).\n\n\u2022 WikiText2/PTB/C4 perplexity for weight-activation quantization in the LLaMA families (Table A16).\n\nFigure 1 :\n1\nFigure 1: (a) provides a performance overview of the proposed OmniQuant, highlighting its ability to achieve quantization-aware training (QAT) performance with post-training quantization (PTQ) time and data efficiency.(b) and (c) showcase the perplexity (low is better) of quantized LLaMA-13B across different bit-widths on WikiText2.\n\n\nFigure 3 :\n3\nFigure 3: Details of OmniQuant in a transformer block.Note that all learnable parameters can be eliminated after quantization.\n\n\nFigure A1 :\nA1\nFigure A1: Visualization of learned clipping scale in different quantization settings in LLaMA-7B.\n\n\n\n\nFigure A2: Visualization of activation of a linear layer in OPT-13B.(a) Original activation.(b) Activation after SmoothQuant.(c) Activation after proposed learnable equivalent transformation.Similar phenomena can be observed in different layer and different models.\n\n\nFigure A3 :\nA3\nFigure A3: Bit-level scaling laws for perplexity.\n\n\nFigure A4 :\nA4\nFigure A4: Performance overview.We display the trade-off curves for three model families.Each model showcases two quantization variants: W4A16g128 and W3A16g128.It is evident that Omni-Quant markedly enhances the trade-off between perplexity and model size.Specifically, OmniQuant delivers a reduction of 0.81 in perplexity for an equivalent model size and achieves the same perplexity with only 0.33x of the model size.\n\n\n\n\nNotably, OmniQuant introduces no extra computation or parameters for the quantized model because the clipping threshold in LWC and equivalent factors in LET can be fused into quantized weights.\nAs depicted in Figure 2, OmniQuant is easy toLearnableFixedimplement even with limited resources. Espe-cially, taking the LLaMA-2 model family (7B-70B) as an example, all models can be quan-tized on a single A100-40G GPU utilizing onlyQuantization-hardly FP models (7B-70B)Equivalent Transformation Weight Clipping128 training samples. The training time ranges from 1 to 16 hours, depending on the size ofQuantization-friendly FP models (7B-70B)Single A100-40G GPUthe quantized model, which ranges from 7B toquantization128 Training Samples70B. Owing to the seamless integration of LWCand LET achieved by differentiable optimiza-Quantized models1-16 Hours Trainingtion, OmniQuant exhibits superior performance compared to prior PTQ-based methods in vari-ous quantization settings. For example, whenFigure 2: Characteristics of OmniQuant on LLaMA family.LLaMA-13B is quantized into W2A16, OmniQuant achieves a perplexity of 13.21, while GPTQ in-\ncurs a significant increase in perplexity to 3832, as demonstrated in Figure1.A similar performance advancement is also observed in the W4A4 quantization.The contributions of OmniQuant are summarized as follows.1) We formulate a novel quantization pipeline for LLM, OmniQuant, which freezes original full-precision weights while incorporating a restrained set of learnable parameters.OmniQuant imbues quantization with gradient updates while preserving the time and data efficiency of PTQ methods.2) OmniQuant consists of Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET).These strategies make full-precision weights and activations more amenable to quantization.3) Through extensive experiments, we demonstrate that OmniQuant outperforms previous methods across a spectrum of quantization settings (W416, W3A16, W2A16, W6A6, W4A4), various model families (OPT, LLaMA, LLaMA-2, LLaMA-2-chat, Falcon), and a range of model sizes (125M-180B).The computation speedup and memory reduction of OmniQuant are also demonstrated on real devices.\n\n\nTable 1 :\n1\nWeight-only quantization Results of LLaMA-1 and LLaMA-2 Models.We report WikiText2 perplexity in this table, C4 perplexity can be found in TableA10in Appendix.\nLLaMA1&2 / PPL\u21931-7B1-13B1-30B1-65B2-7B2-13B2-70BFP16-5.685.094.103.535.474.883.31RTN1.1e56.8e42.4e42.2e43.8e45.6e42.0e4W2A16GPTQ2.1e35.5e3499.7555.917.7e32.1e377.95OmniQuant15.4713.218.717.5837.3717.217.81RTN1.9e3781.2068.0415.084.2e3122.0827.27W2A16GPTQ44.0115.6010.929.5136.7728.14NANg128AWQ2.6e52.8e52.4e57.4e42.2e51.2e5-OmniQuant9.727.937.125.9511.068.266.55RTN188.32101.8719.209.39431.9726.2210.31W2A16GPTQ22.1010.068.548.3120.8522.44NANg64AWQ2.5e52.7e52.3e57.4e42.1e51.2e5-OmniQuant8.907.346.595.659.627.566.11RTN25.7311.3914.9510.68539.4810.687.52W3A16GPTQ AWQ8.06 11.886.76 7.455.84 10.075.06 5.218.37 24.006.44 10.454.82 -OmniQuant6.495.684.744.046.585.583.92RTN7.015.884.874.246.665.513.97W3A16GPTQ6.555.624.804.176.295.423.85g128AWQ6.465.514.633.996.245.32-OmniQuant6.155.444.563.946.035.283.78RTN6.435.554.573.876.115.203.67W4A16GPTQ AWQ6.13 6.085.40 5.344.48 4.393.83 3.765.83 6.155.13 5.123.58 -OmniQuant5.865.214.253.715.745.023.47RTN5.965.254.233.675.724.983.46W4A16GPTQ5.855.204.233.655.614.983.42g128AWQ5.815.204.213.625.624.97-OmniQuant5.775.174.193.625.584.953.404 EXPERIMENTS4.1 SETTINGS\n(Dettmers et al., 2022)ent with both weight-only and weight-activation quantization.For the former, default settings are INT4/INT3/INT2 per-channel weight quantization.Group-wise weight quantization is represented by 'g', e.g., W3A16g128 means 3-bit weight-only quantization with a 128-group size.In weight-activation quantization, defaults are INT6/INT4 per-channel weight and per-token activation quantization(Dettmers et al., 2022).All intermediate activations are quantized into low-bit, excluding the SoftMax output, kept at full precision due to its long-tail distribution making it unsuitable for uniform quantization.\n\n\nTable 2 :\n2\nWeight-activation quantization results of LLaMA Models.This table reports the accuracy of 6 zero-shot tasks.Perplexity results can be found in TableA14& A15 at Appendix.\nLLaMA / Acc\u2191 #Bits MethodPIQA ARC-e Arc-c BoolQ HellaSwag Winogrande Avg.FP16 -77.47 52.48 41.46 73.0873.0067.0764.09W6A6 SmoothQuant 76.75 51.64 39.88 71.7571.6765.0362.81W6A6 OmniQuant77.09 51.89 40.87 72.5371.6165.0363.17LLaMA-1-7BW4A4 SmoothQuant 49.80 30.40 25.80 49.1027.4048.0038.41W4A4 LLM-QAT51.50 27.90 23.90 61.3031.1051.9041.27W4A4 LLM-QAT+SQ 55.90 35.50 26.40 62.4047.8050.6046.43W4A4 OmniQuant66.15 45.20 31.14 63.5156.4453.4352.65FP16 -79.10 59.89 44.45 68.0176.2170.3166.33W6A6 SmoothQuant 77.91 56.60 42.40 64.9575.3669.3664.43LLaMA-1-13BW6A6 OmniQuant78.40 57.28 42.91 67.0075.8268.2764.95W4A4 SmoothQuant 61.04 39.18 30.80 61.8052.2951.0649.36W4A4 OmniQuant69.69 47.39 33.10 62.8458.9655.8054.37FP16 -80.08 58.92 45.47 68.4479.2172.5367.44W6A6 SmoothQuant 77.14 57.61 42.91 65.5678.0769.9265.20LLaMA-1-30BW6A6 OmniQuant79.81 58.79 45.22 68.3878.9572.2167.23W4A4 SmoothQuant 58.65 35.53 27.73 60.4235.5648.0644.83W4A4 OmniQuant71.21 49.45 34.47 65.3364.6559.1956.63FP16 -80.79 58.71 46.24 82.2980.7277.5071.04W6A6 SmoothQuant 80.25 57.92 45.50 80.2280.1874.7669.80LLaMA-1-65BW6A6 OmniQuant81.01 58.12 46.33 80.6479.9175.6970.28W4A4 SmoothQuant 64.47 40.44 29.82 59.3839.9052.2447.71W4A4 OmniQuant71.81 48.02 35.92 73.2766.8159.5159.22\n\nTable 3 :\n3\nDeployment of weight-only quantization through MLC-LLM.We report the memory size of quantized weights (denoted as 'WM') and the running memory (denoted as 'RM') and speed in NVIDIA A100-80G.\nLLaMA7B13B30B65BWM RM token/s WM RM token/s WM RM token/s WM RM token/sFP12.6G 14.4G 69.2 24.3G 27.1G 52.5 60.6G 66.1G 23.9 OOM --W4A16g128 3.8G 5.7G 134.2 7.0G 10.0G 91.3 16.7G 21.7G 43.6 33.0G 41.0G 24.3W3A16g128 3.2G 5.1G 83.4 5.8G 8.7G 57.6 13.7G 18.7G 29.0 27.0G 35.1G 15.2W2A16g128 2.2G 4.1G 83.9 4.0G 7.5G 92.6 9.2G 14.1G 36.7 18.0G 25.6G 24.84.4 QUANTIZATION OF INSTRUCTION-TUNED MODELS\n\nTable A1 :\nA1\nAblation of each component.WikiText2 perplexity1 is reported in this table.'-' indicats remove the corresponding module from the overall proposed methods.\nPPL\u2193LLaMA-13BOPT-13BMethodW4A4 W3A16 W4A4 W3A16LWC+LET 10.875.6511.6510.87-LWC20.757.6515.2312.98components-LET5.4e35.687.8e311.29-LWC-LET1.8e310.687.8e54.6e3Table A2: Ablation of learnable equivalent transformation. WikiText2 perplexity1 is reported inthis table.PPL\u2193LLaMA-13BOPT-13BMethodW4A4 W3A16 W4A4 W3A16LWC+LET 11.865.6511.6510.87LET-shifting -attention12.37 11.985.65 5.6513.64 11.7910.87 10.87\n\nTable A3 :\nA3\nAblation of training time.We train LLaMA-7B with different quantization configuration on 128 2048-tokens segments from WikiText2 over various epochs.'0' indicates only initialization without fine-tuning.Wikitext perplexity is reported in this table.\nEpochs W4A16 W3A16 W2A16 W6A6 W4A406.2924.041.1e56.1633.93105.876.5127.495.9612.04205.856.4917.465.9511.26405.866.4715.475.9511.2380--14.77--\n\nTable A4 :\nA4\nAblation of calibration dataset.\nLLaMA-7B/PPL\u2193W3A16W4A4Calibration Dataset WikiText2C4WikiText2C4WikiText26.478.1911.2314.61C46.678.1312.1714.24Pile6.698.1712.0414.22Varience0.0090.00060.170.03\n\nTable A5 :\nA5\nAblation of sample number of calibration dataset.\nLLaMA-7B/PPL\u2193W3A16W4A4Sample NumberWikiText2 C4 WikiText2C4166.478.1811.5614.84326.478.1811.4814.80646.488.1911.4014.571286.478.1911.2314.612566.468.1911.4114.90reaches a perplexity of 10.68, while OPT's spikes to 4.6e3. Consequently, LET is turned off forLLaMA in weight-only quantization given its limited advantage for faster training.\n\nTable A6 :\nA6\nOmniquant runtime on LLaMA family.The time correspond to training 128 2048-tokes segment over 20 epochs and a batch size of 1 on a single NVIDIA A100-80G.\nLLaMA7B13B 30B65Bweight-only1.1h 2.2h 4.5h 8.9hweight-activation 1.6h 3.3h 7.3h 14.4h\n\nTable A8 :\nA8\nWikiText2 perplexity of clipping-based quantization methods.For fair comparison, we reproduce LSQ and PACT by replace LWC in our pipeline with them.\nLLaMA-7B/PPL\u2193PerplexityMethodW3A16 W4A4FP5.68MinMax25.7314.49PACT (Choi et al. (2018))6.9518.25LSQ (Esser et al. (2019))6.6315.03LWC (Ours)6.4711.26\n\nTable A9 :\nA9\nWeight-only quantization on Falcon-180B.\nFalcon-180bPPL\u2193Acc\u2191MethodBit#MemoryDevicesWiki PTB C4 PIQA ARC-e Arc-c BoolQ HellaSwag Winogrande-BF16/FP16 335GB 5xA100 80GB 3.29 6.64 6.31 84.82 84.20 60.83 86.85 85.9180.58RTNW3A16g512 65GB 1xA100 80GB 5.33 8.08 8.34 83.48 80.85 55.46 78.37 81.0577.97OmniQuant W3A16g512 65GB 1xA100 80GB 3.71 6.95 6.71 84.71 82.91 60.92 84.03 84.9679.40\n\nTable A10 :\nA10\nC4 perplexity of Weight-only quantization results in LLaMA-1 and LLaMA-2 models Continue of Table1.\nLLaMA1&2 / PPL\u21931-7B1-13B1-30B1-65B2-7B2-13B2-70BFP16-7.086.615.985.626.976.465.52RTN1.3e55.6e42.7e42.2e44.8e47.2e42.4e4W2A16GPTQ689.132.5e3169.8040.58NAN323.1248.82OmniQuant24.8918.3113.8910.7790.6426.7612.28RTN1.0e3447.6499.4517.154.9e3139.6542.13W2A16GPTQ27.7115.2911.9311.9933.7020.97NANg128AWQ1.9e52.3e52.4e57.5e41.7e59.4e4-OmniQuant12.9710.369.368.0015.0211.058.52RTN151.4376.0030.0711.34475.3528.6913.43W2A16GPTQ17.7111.709.9210.0719.4012.48NANg64AWQ2.8e52.2e52.3e57.4e41.6e59.5e4-OmniQuant11.789.758.657.6012.7210.057.88RTN28.2613.2228.6612.79402.3512.5110.02W3A16GPTQ AWQ9.49 13.268.16 9.137.29 12.676.71 7.119.81 23.858.02 13.076.57 -OmniQuant8.197.326.576.078.657.446.06RTN8.627.496.586.108.407.186.02W3A16GPTQ7.857.106.476.007.897.005.85g128AWQ7.927.076.375.947.846.94-OmniQuant7.056.375.937.756.985.85RTN7.936.986.345.857.716.835.79W4A16GPTQ AWQ7.43 7.526.84 6.866.20 6.175.80 5.777.37 7.686.70 6.745.67 -OmniQuant7.346.766.115.737.356.655.65RTN7.376.696.065.697.246.585.63W4A16GPTQ7.216.696.065.697.126.565.58g128AWQ7.216.706.055.687.136.56-OmniQuant7.216.696.065.687.126.565.58\n\nTable A11 :\nA11\nWikiText2 perplexity of Weight-only quantization results in OPT models.\nOPT / PPL\u2193125M1.3B2.7B6.7B13B30B66BFP16-27.6514.6312.47 10.86 10.12 9.569.34RTN7.2e31.3e45.7e4 7.8e3 7.6e4 1.3e43.6e5W2A16GPTQ597.66 115.16 61.59 20.18 21.36 12.7182.10g128AWQ251.84 47.9728.50 16.20 14.32 12.3114.54OmniQuant 75.4323.9518.13 14.43 12.94 11.3930.84RTN7.0e31.0e4 19.3e4 7.6e3 1.8e4 8.2e31.1e4W2A16GPTQ204.40 49.5829.37 16.81 16.65 11.87 356.01g64AWQ124.18 29.7820.64 14.63 13.28 11.5912.74OmniQuant 62.5621.4016.76 13.57 12.33 11.0010.59RTN1.2e31.3e41.6e4 6.5e3 4.6e3 1.5e36.1 e3W3A16GPTQ AWQ53.05 69.4321.17 28.01 263.10 15.13 20.09 35.74 16.83 15.09 11.73 10.3014.42 4.5e3OmniQuant 35.6616.6813.80 11.65 10.87 10.009.83RTN51.22 119.00 297.98 23.54 46.03 18.80 136.89wW3A16GPTQ39.2416.4713.69 11.65 10.35 9.7310.96g128AWQ36.7416.3213.58 11.41 10.68 9.859.60OmniQuant 32.2515.7213.18 11.27 10.47 9.799.53RTN37.2848.1716.92 12.10 11.32 10.97110W4A16GPTQ AWQ31.43 32.2815.56 15.4912.82 11.41 10.31 9.63 12.93 11.30 10.39 9.779.55 9.61OmniQuant 29.4515.0412.76 11.03 10.30 9.659.65RTN30.4715.2913.02 11.15 10.30 9.949.65W4A16GPTQ29.8114.8912.52 10.93 10.17 9.589.34g128AWQ29.1514.9412.74 10.93 10.21 9.599.40OmniQuant 28.8614.8812.65 10.96 10.20 9.629.37\n\nTable A12 :\nA12\nPTB perplexity of Weight-only quantization results in OPT models.\nOPT / PPL\u2193125M1.3B2.7B6.7B13B30B66BFP16-32.5416.9615.11 13.08 12.33 11.84 11.36RTN4.6e37.1e32.5e4 5.7e3 3.0e4 6.2e3 1.4e5W2A16GPTQ655.17 130.88 61.36 25.24 20.46 15.15 323.23g128AWQ263.88 71.8743.15 19.49 17.61 14.92 19.33OmniQuant 126.49 34.3325.28 18.92 16.74 14.51 139.17RTN5.1e39.4e37.7e4 6.1e3 8.2e3 4.1e3 6.2e3W2A16GPTQ245.28 55.6136.12 19.45 17.02 14.05 88.92g64AWQ143.18 41.1925.08 18.00 15.83 14.92 15.72OmniQuant 112.10 30.3622.63 17.58 15.70 13.98 13.51RTN1.2e31.1e41.0e4 5.2e3 3.6e3 1.4e3 3.6e3W3A16GPTQ AWQ34.05 80.7327.39 33.20 224.11 18.46 35.45 66.68 3.4e3 15.94 13.75 13.71 12.54 21.16OmniQuant 40.7619.0616.29 13.77 12.96 12.19 11.71RTN64.67 222.13 337.75 39.90 65.33 34.27 309.69W3A16GPTQ45.1719.9017.06 14.24 12.84 12.54 13.27g128AWQ44.0719.5916.52 13.98 12.87 66.68 3.4e3OmniQuant 45.2920.4217.08 14.23 13.49 12.54 12.06RTN44.9833.6322.23 16.05 15.40 14.17 274.23W4A16GPTQ AWQ37.75 38.7418.23 18.3513.75 12.58 11.98 11.58 15.70 13.59 12.72 12.06 11.58OmniQuant 34.9417.8015.52 13.41 12.62 11.95 11.86RTN36.5033.6322.23 16.05 15.40 14.17 11.79W4A16GPTQ35.4817.4115.42 13.21 12.42 11.89 11.51g128AWQ34.9517.4615.33 13.28 12.46 11.90 11.43OmniQuant 34.2817.4015.28 13.25 12.46 11.94 11.40\n\nTable A13 :\nA13\nC4 perplexity of Weight-only quantization results in OPT models.\nOPT / PPL\u2193125M1.3B2.7B6.7B13B30B66BFP16-24.6014.7213.16 11.74 11.19 10.69 10.28RTN5.0e37.7e33.8e4 5.2e3 2.8e4 6.5e3 2.6e5W2A16GPTQ597.66 60.8833.83 18.55 16.34 12.89 598.81g128AWQ168.35 38.3826.41 16.48 14.73 12.98 15.42OmniQuant 80.1027.3321.11 16.67 14.92 13.12 73.83RTN3.9e37.3e31.2e5 6.3e3 7.5e3 4.0e3 8.4e3W2A16GPTQ133.51 31.3123.23 16.24 14.48 12.24 58.60g64AWQ90.1927.3420.01 15.20 13.90 12.43 13.31OmniQuant 64.0123.7119.16 15.44 14.16 12.80 12.13RTN722.83 6.1e31.2e4 5.8e3 3.3e3 1.4e3 3.6e3W3A16GPTQ AWQ37.75 55.7319.45 24.56 154.49 15.84 23.71 55.01 3.8e3 13.75 15.67 12.28 11.34 13.68OmniQuant 32.1717.1014.93 12.78 12.13 11.37 10.82RTN40.13 126.47 372.23 32.56 44.12 25.70 286.87W3A16GPTQ30.0816.4714.54 12.48 11.58 10.91 11.35g128AWQ30.3916.2714.19 12.30 11.61 10.96 10.53OmniQuant 29.3416.1114.15 12.31 11.63 10.98 10.51RTN31.5824.6817.61 13.38 12.35 11.90 249.54W4A16GPTQ AWQ27.12 27.6415.57 15.6513.75 12.15 11.36 10.80 10.50 13.71 12.04 11.42 10.83 10.41OmniQuant 26.3615.2813.58 11.97 11.41 10.80 10.63RTN26.7915.7113.79 12.31 11.51 10.94 10.54W4A16GPTQ25.9615.0513.40 11.87 11.26 10.74 10.37g128AWQ25.9015.0413.39 11.87 11.28 10.75 10.34OmniQuant 25.6315.0313.38 11.85 11.29 10.75 10.33\nSigmoid(t) = 1/(1 + exp \u2212t )\nhttps://github.com/mlc-ai/mlc-llm\nACKNOWLEDGMENTSWe thank Wentao Liu from SenseTime for his valuable insights and discussions regarding LLM deployment.We also acknowledge Siyuan Feng from Apache TVM for assisting in the successful deployment of our OmniQuant in the MLC LLM project.TableA15: C4 perplexity of weight-activation quantization results in LLaMA-1 and LLaMA-2 models.Continue of Table2.\nPiqa: Reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033\n\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint\n\nQuip: 2-bit quantization of large language models with guarantees. Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa, arXiv:2307.133042023arXiv preprint\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, March 2023\n\nPact: Parameterized clipping activation for quantized neural networks. Jungwook Choi, Zhuo Wang, Swagath Venkataramani, I-Jen Pierce, Vijayalakshmi Chuang, Kailash Srinivasan, Gopalakrishnan, arXiv:1805.060852018arXiv preprint\n\nBoolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, arXiv:1905.100442019arXiv preprint\n\nThink you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint\n\nThe case for 4-bit precision: k-bit inference scaling laws. Tim Dettmers, Luke Zettlemoyer, International Conference on Machine Learning. PMLR2023\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.07339-bit matrix multiplication for transformers at scale. 20228arXiv preprintint8 (\n\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. 2023aarXiv preprint\n\nSpqr: A sparse-quantized representation for near-lossless llm weight compression. Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Alexander Saleh Ashkboos, Torsten Borzunov, Dan Hoefler, Alistarh, arXiv:2306.030782023barXiv preprint\n\nJeffrey L Steven K Esser, Deepika Mckinstry, Bablani, arXiv:1902.08153Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. 2019arXiv preprint\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.173232022arXiv preprint\n\nThe pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.000272020arXiv preprint\n\nA framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, Niklas Muennighoff, 2021Version v0. 0.1. Sept\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint\n\nChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park, Owq, arXiv:2306.02272Lessons learned from activation outliers for weight quantization in large language models. 2023arXiv preprint\n\nBrecq: Pushing the limit of post-training quantization by block reconstruction. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, Shi Gu, arXiv:2102.054262021arXiv preprint\n\nAwq: Activation-aware weight quantization for llm compression and acceleration. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han, arXiv:2306.009782023arXiv preprint\n\nNonuniform-touniform quantization: Towards accurate quantization via generalized straight-through estimation. Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, Zhiqiang Shen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022\n\nLlm-qat: Data-free quantization aware training for large language models. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas Chandra, arXiv:2305.178882023aarXiv preprint\n\nDeja vu: Contextual sparsity for efficient llms at inference time. Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, International Conference on Machine Learning. PMLR2023b\n\nThe penn treebank: Annotating predicate argument structure. Mitch Marcus, Grace Kim, Mary , Ann Marcinkiewicz, Robert Macintyre, Ann Bies, Mark Ferguson, Karen Katz, Britta Schasberger, Human Language Technology: Proceedings of a Workshop. Plainsboro, New JerseyMarch 8-11, 1994. 1994\n\nStephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843Pointer sentinel mixture models. 2016arXiv preprint\n\nEmbodiedgpt: Vision-language pre-training via embodied chain of thought. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo, arXiv:2305.150212023arXiv preprint\n\nUp or down? adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning. PMLR2020\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 2023arXiv preprint\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020\n\nThibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth\u00e9e Lachaux, Baptiste Lacroix, Naman Rozi\u00e8re, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023aarXiv preprint\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Open foundation and fine-tuned chat models. 2023b2arXiv preprint\n\nOutlier suppression: Pushing the limit of low-bit transformer language models. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu, Advances in Neural Information Processing Systems. 202235\n\nOutlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu, arXiv:2304.091452023arXiv preprint\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han, International Conference on Machine Learning. PMLR2023\n\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, Ping Luo, arXiv:2306.09265Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. 2023arXiv preprint\n\nRptq: Reorder-based post-training quantization for large language models. Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, Bingzhe Wu, arXiv:2304.010892023arXiv preprint\n\nHellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.078302019arXiv preprint\n\nOpt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint\n\nYiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue, arXiv:2307.10802Meta-transformer: A unified framework for multimodal learning. 2023arXiv preprint\n\nJudging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, arXiv:2306.056852023arXiv preprint\n", "annotations": {"author": "[{\"end\":155,\"start\":110},{\"end\":204,\"start\":156},{\"end\":258,\"start\":205},{\"end\":331,\"start\":259},{\"end\":343,\"start\":332},{\"end\":385,\"start\":344},{\"end\":434,\"start\":386},{\"end\":478,\"start\":435},{\"end\":521,\"start\":479},{\"end\":595,\"start\":522}]", "publisher": null, "author_last_name": "[{\"end\":120,\"start\":116},{\"end\":169,\"start\":165},{\"end\":219,\"start\":214},{\"end\":266,\"start\":264},{\"end\":342,\"start\":338},{\"end\":354,\"start\":352},{\"end\":399,\"start\":394},{\"end\":443,\"start\":440},{\"end\":486,\"start\":482},{\"end\":530,\"start\":527}]", "author_first_name": "[{\"end\":115,\"start\":110},{\"end\":164,\"start\":156},{\"end\":213,\"start\":205},{\"end\":263,\"start\":259},{\"end\":337,\"start\":332},{\"end\":351,\"start\":344},{\"end\":393,\"start\":386},{\"end\":439,\"start\":435},{\"end\":481,\"start\":479},{\"end\":526,\"start\":522}]", "author_affiliation": "[{\"end\":154,\"start\":122},{\"end\":203,\"start\":171},{\"end\":257,\"start\":221},{\"end\":300,\"start\":268},{\"end\":330,\"start\":302},{\"end\":384,\"start\":356},{\"end\":433,\"start\":401},{\"end\":477,\"start\":445},{\"end\":520,\"start\":488},{\"end\":564,\"start\":532},{\"end\":594,\"start\":566}]", "title": "[{\"end\":96,\"start\":1},{\"end\":691,\"start\":596}]", "venue": null, "abstract": "[{\"end\":2589,\"start\":761}]", "bib_ref": "[{\"end\":2669,\"start\":2642},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2703,\"start\":2680},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2813,\"start\":2789},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2831,\"start\":2813},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2852,\"start\":2831},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2995,\"start\":2978},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3011,\"start\":2995},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3030,\"start\":3011},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3131,\"start\":3110},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3282,\"start\":3262},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4129,\"start\":4107},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4146,\"start\":4129},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4169,\"start\":4146},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4186,\"start\":4169},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4399,\"start\":4380},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4416,\"start\":4399},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4434,\"start\":4416},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4451,\"start\":4434},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4810,\"start\":4792},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4833,\"start\":4810},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4850,\"start\":4833},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4917,\"start\":4899},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5208,\"start\":5186},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5225,\"start\":5208},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5242,\"start\":5225},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5343,\"start\":5324},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5385,\"start\":5367},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5488,\"start\":5469},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5744,\"start\":5725},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5786,\"start\":5764},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5931,\"start\":5912},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6485,\"start\":6466},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8025,\"start\":8006},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8082,\"start\":8063},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8103,\"start\":8082},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8275,\"start\":8255},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8303,\"start\":8286},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8475,\"start\":8456},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8496,\"start\":8475},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8519,\"start\":8496},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8536,\"start\":8519},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8553,\"start\":8536},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9070,\"start\":9048},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9156,\"start\":9132},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9180,\"start\":9162},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9208,\"start\":9190},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9491,\"start\":9467},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9522,\"start\":9503},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9864,\"start\":9845},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9900,\"start\":9877},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9944,\"start\":9926},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10150,\"start\":10132},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10410,\"start\":10391},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10442,\"start\":10423},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11005,\"start\":10986},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11049,\"start\":11031},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11358,\"start\":11334},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11385,\"start\":11367},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11466,\"start\":11449},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12669,\"start\":12649},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12694,\"start\":12677},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14154,\"start\":14134},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14170,\"start\":14154},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14255,\"start\":14232},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14569,\"start\":14549},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14586,\"start\":14569},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14604,\"start\":14586},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14786,\"start\":14767},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14815,\"start\":14795},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14920,\"start\":14901},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15088,\"start\":15068},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15106,\"start\":15088},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15889,\"start\":15870},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15912,\"start\":15891},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16388,\"start\":16366},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16405,\"start\":16388},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16810,\"start\":16791},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17204,\"start\":17185},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17248,\"start\":17230},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19200,\"start\":19181},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":20634,\"start\":20615},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20733,\"start\":20715},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21070,\"start\":21049},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21562,\"start\":21542},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21602,\"start\":21579},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21643,\"start\":21620},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21678,\"start\":21657},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21738,\"start\":21715},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21949,\"start\":21931},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21970,\"start\":21949},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22113,\"start\":22092},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22140,\"start\":22119},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22167,\"start\":22146},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22253,\"start\":22234},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22279,\"start\":22259},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22307,\"start\":22287},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22343,\"start\":22323},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22487,\"start\":22469},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22659,\"start\":22637},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22687,\"start\":22669},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22782,\"start\":22763},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22808,\"start\":22789},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22863,\"start\":22844},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23587,\"start\":23570},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24561,\"start\":24542},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24789,\"start\":24771},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24867,\"start\":24846},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25123,\"start\":25100},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25222,\"start\":25201},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25293,\"start\":25272},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25362,\"start\":25342},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25485,\"start\":25467},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30359,\"start\":30340},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31648,\"start\":31630},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31678,\"start\":31657},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32183,\"start\":32161},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36590,\"start\":36560}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39393,\"start\":39044},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39535,\"start\":39394},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39651,\"start\":39536},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39921,\"start\":39652},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39988,\"start\":39922},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40426,\"start\":39989},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42634,\"start\":40427},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44543,\"start\":42635},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45979,\"start\":44544},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":46578,\"start\":45980},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":47152,\"start\":46579},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":47559,\"start\":47153},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":47768,\"start\":47560},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":48172,\"start\":47769},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":48428,\"start\":48173},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":48741,\"start\":48429},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":49138,\"start\":48742},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":50347,\"start\":49139},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":51602,\"start\":50348},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":52892,\"start\":51603},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":54180,\"start\":52893}]", "paragraph": "[{\"end\":3587,\"start\":2605},{\"end\":4605,\"start\":3589},{\"end\":6109,\"start\":4607},{\"end\":7094,\"start\":6111},{\"end\":7789,\"start\":7096},{\"end\":7831,\"start\":7806},{\"end\":8754,\"start\":7833},{\"end\":8929,\"start\":8779},{\"end\":10742,\"start\":8931},{\"end\":11822,\"start\":10756},{\"end\":12113,\"start\":11824},{\"end\":12504,\"start\":12115},{\"end\":12536,\"start\":12506},{\"end\":13074,\"start\":12583},{\"end\":13083,\"start\":13076},{\"end\":13518,\"start\":13146},{\"end\":13616,\"start\":13520},{\"end\":13649,\"start\":13618},{\"end\":13726,\"start\":13651},{\"end\":14334,\"start\":13728},{\"end\":14989,\"start\":14364},{\"end\":15158,\"start\":14991},{\"end\":15780,\"start\":15257},{\"end\":16407,\"start\":15782},{\"end\":17008,\"start\":16447},{\"end\":17542,\"start\":17010},{\"end\":17794,\"start\":17544},{\"end\":18110,\"start\":17855},{\"end\":18476,\"start\":18112},{\"end\":18625,\"start\":18506},{\"end\":19280,\"start\":18627},{\"end\":19793,\"start\":19282},{\"end\":20000,\"start\":19857},{\"end\":20182,\"start\":20035},{\"end\":21508,\"start\":20184},{\"end\":21890,\"start\":21510},{\"end\":22529,\"start\":21892},{\"end\":22995,\"start\":22531},{\"end\":23855,\"start\":23032},{\"end\":24455,\"start\":23898},{\"end\":25714,\"start\":24457},{\"end\":26944,\"start\":25746},{\"end\":27625,\"start\":26959},{\"end\":27683,\"start\":27627},{\"end\":27748,\"start\":27685},{\"end\":27927,\"start\":27750},{\"end\":27996,\"start\":27929},{\"end\":28064,\"start\":27998},{\"end\":28152,\"start\":28066},{\"end\":28240,\"start\":28154},{\"end\":28829,\"start\":28265},{\"end\":28874,\"start\":28831},{\"end\":29045,\"start\":28876},{\"end\":29094,\"start\":29047},{\"end\":29127,\"start\":29125},{\"end\":29169,\"start\":29129},{\"end\":29214,\"start\":29193},{\"end\":29218,\"start\":29216},{\"end\":29363,\"start\":29360},{\"end\":29435,\"start\":29403},{\"end\":29531,\"start\":29453},{\"end\":30883,\"start\":29555},{\"end\":31286,\"start\":30885},{\"end\":32205,\"start\":31288},{\"end\":32757,\"start\":32226},{\"end\":33036,\"start\":32785},{\"end\":33881,\"start\":33038},{\"end\":34826,\"start\":33883},{\"end\":34936,\"start\":34828},{\"end\":36945,\"start\":34938},{\"end\":38241,\"start\":36991},{\"end\":38420,\"start\":38261},{\"end\":38456,\"start\":38422},{\"end\":38526,\"start\":38458},{\"end\":38608,\"start\":38528},{\"end\":38685,\"start\":38610},{\"end\":38761,\"start\":38687},{\"end\":38855,\"start\":38763},{\"end\":38942,\"start\":38857},{\"end\":39043,\"start\":38944},{\"end\":39392,\"start\":39058},{\"end\":39534,\"start\":39408},{\"end\":39650,\"start\":39552},{\"end\":39920,\"start\":39655},{\"end\":39987,\"start\":39938},{\"end\":40425,\"start\":40005},{\"end\":40623,\"start\":40430},{\"end\":42633,\"start\":41569},{\"end\":42807,\"start\":42648},{\"end\":44542,\"start\":43917},{\"end\":44726,\"start\":44557},{\"end\":46183,\"start\":45993},{\"end\":46748,\"start\":46594},{\"end\":47417,\"start\":47168},{\"end\":47607,\"start\":47575},{\"end\":47833,\"start\":47784},{\"end\":48342,\"start\":48188},{\"end\":48592,\"start\":48444},{\"end\":48797,\"start\":48757},{\"end\":49255,\"start\":49156},{\"end\":50436,\"start\":50365},{\"end\":51685,\"start\":51620},{\"end\":52974,\"start\":52910}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13145,\"start\":13084},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15256,\"start\":15159},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17854,\"start\":17795},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18505,\"start\":18477},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19855,\"start\":19794},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19856,\"start\":19855},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20034,\"start\":20001},{\"attributes\":{\"id\":\"formula_7\"},\"end\":29124,\"start\":29095},{\"attributes\":{\"id\":\"formula_8\"},\"end\":29192,\"start\":29170},{\"attributes\":{\"id\":\"formula_9\"},\"end\":29339,\"start\":29219},{\"attributes\":{\"id\":\"formula_10\"},\"end\":29359,\"start\":29339},{\"attributes\":{\"id\":\"formula_11\"},\"end\":29402,\"start\":29364},{\"attributes\":{\"id\":\"formula_12\"},\"end\":29452,\"start\":29436}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":14972,\"start\":14970},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16247,\"start\":16246},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":21436,\"start\":21434},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23087,\"start\":23086},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24179,\"start\":24178},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24233,\"start\":24230},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24253,\"start\":24252},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":29590,\"start\":29588},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30508,\"start\":30506},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":30925,\"start\":30923},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":31703,\"start\":31701},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":32024,\"start\":32022},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":32246,\"start\":32244},{\"end\":33293,\"start\":33291},{\"end\":34580,\"start\":34578},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":37703,\"start\":37701},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":38524,\"start\":38522},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":38606,\"start\":38603},{\"attributes\":{\"ref_id\":\"tab_16\"},\"end\":38683,\"start\":38680},{\"attributes\":{\"ref_id\":\"tab_17\"},\"end\":38759,\"start\":38756},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":38853,\"start\":38850},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":38940,\"start\":38937},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":39041,\"start\":39038}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2603,\"start\":2591},{\"attributes\":{\"n\":\"2\"},\"end\":7804,\"start\":7792},{\"attributes\":{\"n\":\"2.2\"},\"end\":8777,\"start\":8757},{\"attributes\":{\"n\":\"3\"},\"end\":10754,\"start\":10745},{\"attributes\":{\"n\":\"3.1\"},\"end\":12581,\"start\":12539},{\"attributes\":{\"n\":\"3.2\"},\"end\":14362,\"start\":14337},{\"attributes\":{\"n\":\"3.3\"},\"end\":16445,\"start\":16410},{\"attributes\":{\"n\":\"4.2\"},\"end\":23030,\"start\":22998},{\"attributes\":{\"n\":\"4.3\"},\"end\":23896,\"start\":23858},{\"attributes\":{\"n\":\"4.5\"},\"end\":25744,\"start\":25717},{\"attributes\":{\"n\":\"5\"},\"end\":26957,\"start\":26947},{\"end\":28263,\"start\":28243},{\"end\":29553,\"start\":29534},{\"end\":32224,\"start\":32208},{\"end\":32783,\"start\":32760},{\"end\":36989,\"start\":36948},{\"end\":38259,\"start\":38244},{\"end\":39055,\"start\":39045},{\"end\":39405,\"start\":39395},{\"end\":39548,\"start\":39537},{\"end\":39934,\"start\":39923},{\"end\":40001,\"start\":39990},{\"end\":42645,\"start\":42636},{\"end\":44554,\"start\":44545},{\"end\":45990,\"start\":45981},{\"end\":46590,\"start\":46580},{\"end\":47164,\"start\":47154},{\"end\":47571,\"start\":47561},{\"end\":47780,\"start\":47770},{\"end\":48184,\"start\":48174},{\"end\":48440,\"start\":48430},{\"end\":48753,\"start\":48743},{\"end\":49151,\"start\":49140},{\"end\":50360,\"start\":50349},{\"end\":51615,\"start\":51604},{\"end\":52905,\"start\":52894}]", "table": "[{\"end\":41568,\"start\":40624},{\"end\":43916,\"start\":42808},{\"end\":45979,\"start\":44727},{\"end\":46578,\"start\":46184},{\"end\":47152,\"start\":46749},{\"end\":47559,\"start\":47418},{\"end\":47768,\"start\":47608},{\"end\":48172,\"start\":47834},{\"end\":48428,\"start\":48343},{\"end\":48741,\"start\":48593},{\"end\":49138,\"start\":48798},{\"end\":50347,\"start\":49256},{\"end\":51602,\"start\":50437},{\"end\":52892,\"start\":51686},{\"end\":54180,\"start\":52975}]", "figure_caption": "[{\"end\":39393,\"start\":39057},{\"end\":39535,\"start\":39407},{\"end\":39651,\"start\":39551},{\"end\":39921,\"start\":39654},{\"end\":39988,\"start\":39937},{\"end\":40426,\"start\":40004},{\"end\":40624,\"start\":40429},{\"end\":42808,\"start\":42647},{\"end\":44727,\"start\":44556},{\"end\":46184,\"start\":45992},{\"end\":46749,\"start\":46593},{\"end\":47418,\"start\":47167},{\"end\":47608,\"start\":47574},{\"end\":47834,\"start\":47783},{\"end\":48343,\"start\":48187},{\"end\":48593,\"start\":48443},{\"end\":48798,\"start\":48756},{\"end\":49256,\"start\":49155},{\"end\":50437,\"start\":50364},{\"end\":51686,\"start\":51619},{\"end\":52975,\"start\":52909}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5084,\"start\":5076},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6424,\"start\":6423},{\"end\":6668,\"start\":6667},{\"end\":25448,\"start\":25447},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34006,\"start\":34004},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34182,\"start\":34180},{\"end\":34983,\"start\":34981},{\"end\":35141,\"start\":35139},{\"end\":35405,\"start\":35403},{\"end\":35770,\"start\":35766},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36772,\"start\":36770},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38455,\"start\":38453}]", "bib_author_first_name": "[{\"end\":54679,\"start\":54672},{\"end\":54691,\"start\":54686},{\"end\":54709,\"start\":54701},{\"end\":54720,\"start\":54715},{\"end\":54886,\"start\":54883},{\"end\":54902,\"start\":54894},{\"end\":54913,\"start\":54909},{\"end\":54928,\"start\":54921},{\"end\":54943,\"start\":54938},{\"end\":54945,\"start\":54944},{\"end\":54962,\"start\":54954},{\"end\":54979,\"start\":54973},{\"end\":54999,\"start\":54993},{\"end\":55013,\"start\":55007},{\"end\":55028,\"start\":55022},{\"end\":55105,\"start\":55096},{\"end\":55119,\"start\":55114},{\"end\":55141,\"start\":55136},{\"end\":55157,\"start\":55149},{\"end\":55170,\"start\":55166},{\"end\":55183,\"start\":55180},{\"end\":55196,\"start\":55191},{\"end\":55205,\"start\":55202},{\"end\":55222,\"start\":55215},{\"end\":55232,\"start\":55227},{\"end\":55424,\"start\":55419},{\"end\":55437,\"start\":55431},{\"end\":55452,\"start\":55443},{\"end\":55474,\"start\":55463},{\"end\":55477,\"start\":55475},{\"end\":55600,\"start\":55593},{\"end\":55616,\"start\":55609},{\"end\":55623,\"start\":55621},{\"end\":55633,\"start\":55629},{\"end\":55649,\"start\":55641},{\"end\":55657,\"start\":55654},{\"end\":55672,\"start\":55665},{\"end\":55686,\"start\":55680},{\"end\":55702,\"start\":55695},{\"end\":55717,\"start\":55711},{\"end\":55719,\"start\":55718},{\"end\":55733,\"start\":55730},{\"end\":55746,\"start\":55742},{\"end\":55748,\"start\":55747},{\"end\":55846,\"start\":55838},{\"end\":55857,\"start\":55853},{\"end\":55871,\"start\":55864},{\"end\":55892,\"start\":55887},{\"end\":55914,\"start\":55901},{\"end\":55930,\"start\":55923},{\"end\":56078,\"start\":56067},{\"end\":56092,\"start\":56086},{\"end\":56106,\"start\":56098},{\"end\":56117,\"start\":56114},{\"end\":56138,\"start\":56131},{\"end\":56156,\"start\":56148},{\"end\":56289,\"start\":56284},{\"end\":56302,\"start\":56297},{\"end\":56315,\"start\":56311},{\"end\":56331,\"start\":56325},{\"end\":56344,\"start\":56338},{\"end\":56363,\"start\":56356},{\"end\":56381,\"start\":56375},{\"end\":56490,\"start\":56487},{\"end\":56505,\"start\":56501},{\"end\":56578,\"start\":56575},{\"end\":56593,\"start\":56589},{\"end\":56607,\"start\":56601},{\"end\":56621,\"start\":56617},{\"end\":56735,\"start\":56732},{\"end\":56754,\"start\":56746},{\"end\":56767,\"start\":56764},{\"end\":56782,\"start\":56778},{\"end\":56965,\"start\":56962},{\"end\":56982,\"start\":56976},{\"end\":57001,\"start\":56997},{\"end\":57019,\"start\":57014},{\"end\":57037,\"start\":57032},{\"end\":57056,\"start\":57047},{\"end\":57080,\"start\":57073},{\"end\":57094,\"start\":57091},{\"end\":57158,\"start\":57151},{\"end\":57160,\"start\":57159},{\"end\":57184,\"start\":57177},{\"end\":57409,\"start\":57404},{\"end\":57426,\"start\":57419},{\"end\":57446,\"start\":57443},{\"end\":57571,\"start\":57568},{\"end\":57583,\"start\":57577},{\"end\":57597,\"start\":57594},{\"end\":57613,\"start\":57605},{\"end\":57629,\"start\":57623},{\"end\":57644,\"start\":57637},{\"end\":57658,\"start\":57653},{\"end\":57672,\"start\":57666},{\"end\":57682,\"start\":57677},{\"end\":57693,\"start\":57690},{\"end\":57796,\"start\":57793},{\"end\":57810,\"start\":57802},{\"end\":57822,\"start\":57816},{\"end\":57836,\"start\":57833},{\"end\":57851,\"start\":57844},{\"end\":57867,\"start\":57860},{\"end\":57884,\"start\":57876},{\"end\":57901,\"start\":57894},{\"end\":57911,\"start\":57907},{\"end\":57928,\"start\":57922},{\"end\":57972,\"start\":57969},{\"end\":57990,\"start\":57984},{\"end\":58004,\"start\":57998},{\"end\":58017,\"start\":58013},{\"end\":58029,\"start\":58023},{\"end\":58043,\"start\":58039},{\"end\":58055,\"start\":58050},{\"end\":58164,\"start\":58156},{\"end\":58176,\"start\":58170},{\"end\":58187,\"start\":58182},{\"end\":58201,\"start\":58193},{\"end\":58215,\"start\":58207},{\"end\":58440,\"start\":58434},{\"end\":58451,\"start\":58445},{\"end\":58460,\"start\":58458},{\"end\":58470,\"start\":58466},{\"end\":58481,\"start\":58477},{\"end\":58488,\"start\":58486},{\"end\":58503,\"start\":58496},{\"end\":58511,\"start\":58508},{\"end\":58521,\"start\":58518},{\"end\":58644,\"start\":58642},{\"end\":58657,\"start\":58650},{\"end\":58671,\"start\":58664},{\"end\":58683,\"start\":58678},{\"end\":58696,\"start\":58690},{\"end\":58707,\"start\":58703},{\"end\":58865,\"start\":58859},{\"end\":58881,\"start\":58871},{\"end\":58893,\"start\":58889},{\"end\":58905,\"start\":58901},{\"end\":58907,\"start\":58906},{\"end\":58922,\"start\":58914},{\"end\":59164,\"start\":59158},{\"end\":59176,\"start\":59170},{\"end\":59193,\"start\":59183},{\"end\":59205,\"start\":59200},{\"end\":59219,\"start\":59213},{\"end\":59233,\"start\":59227},{\"end\":59250,\"start\":59242},{\"end\":59266,\"start\":59256},{\"end\":59288,\"start\":59283},{\"end\":59409,\"start\":59402},{\"end\":59418,\"start\":59415},{\"end\":59428,\"start\":59425},{\"end\":59440,\"start\":59434},{\"end\":59454,\"start\":59447},{\"end\":59465,\"start\":59461},{\"end\":59481,\"start\":59472},{\"end\":59497,\"start\":59495},{\"end\":59513,\"start\":59505},{\"end\":59531,\"start\":59520},{\"end\":59658,\"start\":59653},{\"end\":59672,\"start\":59667},{\"end\":59682,\"start\":59678},{\"end\":59688,\"start\":59685},{\"end\":59710,\"start\":59704},{\"end\":59725,\"start\":59722},{\"end\":59736,\"start\":59732},{\"end\":59752,\"start\":59747},{\"end\":59765,\"start\":59759},{\"end\":59886,\"start\":59879},{\"end\":59902,\"start\":59895},{\"end\":59915,\"start\":59910},{\"end\":59933,\"start\":59926},{\"end\":60087,\"start\":60084},{\"end\":60100,\"start\":60092},{\"end\":60116,\"start\":60108},{\"end\":60127,\"start\":60121},{\"end\":60140,\"start\":60134},{\"end\":60150,\"start\":60147},{\"end\":60159,\"start\":60156},{\"end\":60172,\"start\":60166},{\"end\":60180,\"start\":60178},{\"end\":60191,\"start\":60187},{\"end\":60301,\"start\":60295},{\"end\":60313,\"start\":60309},{\"end\":60329,\"start\":60325},{\"end\":60350,\"start\":60342},{\"end\":60366,\"start\":60360},{\"end\":60445,\"start\":60436},{\"end\":60461,\"start\":60454},{\"end\":60478,\"start\":60472},{\"end\":60496,\"start\":60488},{\"end\":60517,\"start\":60507},{\"end\":60533,\"start\":60528},{\"end\":60553,\"start\":60545},{\"end\":60570,\"start\":60563},{\"end\":60589,\"start\":60583},{\"end\":60825,\"start\":60820},{\"end\":60838,\"start\":60834},{\"end\":60852,\"start\":60848},{\"end\":60871,\"start\":60862},{\"end\":60883,\"start\":60877},{\"end\":60899,\"start\":60892},{\"end\":60913,\"start\":60908},{\"end\":60923,\"start\":60920},{\"end\":60933,\"start\":60928},{\"end\":60935,\"start\":60934},{\"end\":60999,\"start\":60992},{\"end\":61021,\"start\":61014},{\"end\":61036,\"start\":61030},{\"end\":61056,\"start\":61046},{\"end\":61075,\"start\":61067},{\"end\":61093,\"start\":61085},{\"end\":61108,\"start\":61103},{\"end\":61122,\"start\":61118},{\"end\":61254,\"start\":61250},{\"end\":61269,\"start\":61264},{\"end\":61283,\"start\":61278},{\"end\":61296,\"start\":61291},{\"end\":61310,\"start\":61305},{\"end\":61329,\"start\":61322},{\"end\":61345,\"start\":61338},{\"end\":61363,\"start\":61357},{\"end\":61379,\"start\":61371},{\"end\":61396,\"start\":61390},{\"end\":61574,\"start\":61567},{\"end\":61587,\"start\":61580},{\"end\":61603,\"start\":61595},{\"end\":61617,\"start\":61611},{\"end\":61633,\"start\":61624},{\"end\":61643,\"start\":61641},{\"end\":61658,\"start\":61651},{\"end\":61672,\"start\":61663},{\"end\":61861,\"start\":61854},{\"end\":61874,\"start\":61867},{\"end\":61888,\"start\":61882},{\"end\":61901,\"start\":61893},{\"end\":61915,\"start\":61909},{\"end\":61929,\"start\":61922},{\"end\":61944,\"start\":61935},{\"end\":62085,\"start\":62076},{\"end\":62094,\"start\":62092},{\"end\":62107,\"start\":62100},{\"end\":62119,\"start\":62116},{\"end\":62130,\"start\":62124},{\"end\":62144,\"start\":62140},{\"end\":62210,\"start\":62206},{\"end\":62220,\"start\":62215},{\"end\":62234,\"start\":62227},{\"end\":62246,\"start\":62242},{\"end\":62256,\"start\":62252},{\"end\":62266,\"start\":62262},{\"end\":62279,\"start\":62272},{\"end\":62292,\"start\":62286},{\"end\":62302,\"start\":62300},{\"end\":62313,\"start\":62309},{\"end\":62518,\"start\":62511},{\"end\":62528,\"start\":62525},{\"end\":62540,\"start\":62534},{\"end\":62551,\"start\":62546},{\"end\":62565,\"start\":62557},{\"end\":62579,\"start\":62572},{\"end\":62594,\"start\":62587},{\"end\":62605,\"start\":62600},{\"end\":62618,\"start\":62610},{\"end\":62630,\"start\":62623},{\"end\":62731,\"start\":62726},{\"end\":62744,\"start\":62741},{\"end\":62762,\"start\":62755},{\"end\":62772,\"start\":62769},{\"end\":62787,\"start\":62782},{\"end\":62886,\"start\":62881},{\"end\":62901,\"start\":62894},{\"end\":62915,\"start\":62910},{\"end\":62928,\"start\":62923},{\"end\":62942,\"start\":62938},{\"end\":62956,\"start\":62949},{\"end\":62974,\"start\":62963},{\"end\":62986,\"start\":62982},{\"end\":62997,\"start\":62993},{\"end\":63004,\"start\":63002},{\"end\":63061,\"start\":63055},{\"end\":63077,\"start\":63069},{\"end\":63091,\"start\":63084},{\"end\":63108,\"start\":63099},{\"end\":63115,\"start\":63113},{\"end\":63127,\"start\":63122},{\"end\":63143,\"start\":63136},{\"end\":63311,\"start\":63304},{\"end\":63326,\"start\":63319},{\"end\":63339,\"start\":63335},{\"end\":63353,\"start\":63347},{\"end\":63370,\"start\":63362},{\"end\":63382,\"start\":63375},{\"end\":63393,\"start\":63391},{\"end\":63406,\"start\":63399},{\"end\":63418,\"start\":63411},{\"end\":63427,\"start\":63423}]", "bib_author_last_name": "[{\"end\":54684,\"start\":54680},{\"end\":54699,\"start\":54692},{\"end\":54713,\"start\":54710},{\"end\":54725,\"start\":54721},{\"end\":54892,\"start\":54887},{\"end\":54907,\"start\":54903},{\"end\":54919,\"start\":54914},{\"end\":54936,\"start\":54929},{\"end\":54952,\"start\":54946},{\"end\":54971,\"start\":54963},{\"end\":54991,\"start\":54980},{\"end\":55005,\"start\":55000},{\"end\":55020,\"start\":55014},{\"end\":55035,\"start\":55029},{\"end\":55112,\"start\":55106},{\"end\":55134,\"start\":55120},{\"end\":55147,\"start\":55142},{\"end\":55164,\"start\":55158},{\"end\":55178,\"start\":55171},{\"end\":55189,\"start\":55184},{\"end\":55200,\"start\":55197},{\"end\":55213,\"start\":55206},{\"end\":55225,\"start\":55223},{\"end\":55241,\"start\":55233},{\"end\":55429,\"start\":55425},{\"end\":55441,\"start\":55438},{\"end\":55461,\"start\":55453},{\"end\":55480,\"start\":55478},{\"end\":55607,\"start\":55601},{\"end\":55619,\"start\":55617},{\"end\":55627,\"start\":55624},{\"end\":55639,\"start\":55634},{\"end\":55652,\"start\":55650},{\"end\":55663,\"start\":55658},{\"end\":55678,\"start\":55673},{\"end\":55693,\"start\":55687},{\"end\":55709,\"start\":55703},{\"end\":55728,\"start\":55720},{\"end\":55740,\"start\":55734},{\"end\":55753,\"start\":55749},{\"end\":55851,\"start\":55847},{\"end\":55862,\"start\":55858},{\"end\":55885,\"start\":55872},{\"end\":55899,\"start\":55893},{\"end\":55921,\"start\":55915},{\"end\":55941,\"start\":55931},{\"end\":55957,\"start\":55943},{\"end\":56084,\"start\":56079},{\"end\":56096,\"start\":56093},{\"end\":56112,\"start\":56107},{\"end\":56129,\"start\":56118},{\"end\":56146,\"start\":56139},{\"end\":56166,\"start\":56157},{\"end\":56295,\"start\":56290},{\"end\":56309,\"start\":56303},{\"end\":56323,\"start\":56316},{\"end\":56336,\"start\":56332},{\"end\":56354,\"start\":56345},{\"end\":56373,\"start\":56364},{\"end\":56389,\"start\":56382},{\"end\":56499,\"start\":56491},{\"end\":56517,\"start\":56506},{\"end\":56587,\"start\":56579},{\"end\":56599,\"start\":56594},{\"end\":56615,\"start\":56608},{\"end\":56633,\"start\":56622},{\"end\":56744,\"start\":56736},{\"end\":56762,\"start\":56755},{\"end\":56776,\"start\":56768},{\"end\":56794,\"start\":56783},{\"end\":56801,\"start\":56796},{\"end\":56974,\"start\":56966},{\"end\":56995,\"start\":56983},{\"end\":57012,\"start\":57002},{\"end\":57030,\"start\":57020},{\"end\":57045,\"start\":57038},{\"end\":57071,\"start\":57057},{\"end\":57089,\"start\":57081},{\"end\":57102,\"start\":57095},{\"end\":57112,\"start\":57104},{\"end\":57175,\"start\":57161},{\"end\":57194,\"start\":57185},{\"end\":57203,\"start\":57196},{\"end\":57417,\"start\":57410},{\"end\":57441,\"start\":57427},{\"end\":57454,\"start\":57447},{\"end\":57464,\"start\":57456},{\"end\":57575,\"start\":57572},{\"end\":57592,\"start\":57584},{\"end\":57603,\"start\":57598},{\"end\":57621,\"start\":57614},{\"end\":57635,\"start\":57630},{\"end\":57651,\"start\":57645},{\"end\":57664,\"start\":57659},{\"end\":57675,\"start\":57673},{\"end\":57688,\"start\":57683},{\"end\":57703,\"start\":57694},{\"end\":57800,\"start\":57797},{\"end\":57814,\"start\":57811},{\"end\":57831,\"start\":57823},{\"end\":57842,\"start\":57837},{\"end\":57858,\"start\":57852},{\"end\":57874,\"start\":57868},{\"end\":57892,\"start\":57885},{\"end\":57905,\"start\":57902},{\"end\":57920,\"start\":57912},{\"end\":57940,\"start\":57929},{\"end\":57982,\"start\":57973},{\"end\":57996,\"start\":57991},{\"end\":58011,\"start\":58005},{\"end\":58021,\"start\":58018},{\"end\":58037,\"start\":58030},{\"end\":58048,\"start\":58044},{\"end\":58066,\"start\":58056},{\"end\":58168,\"start\":58165},{\"end\":58180,\"start\":58177},{\"end\":58191,\"start\":58188},{\"end\":58205,\"start\":58202},{\"end\":58220,\"start\":58216},{\"end\":58225,\"start\":58222},{\"end\":58443,\"start\":58441},{\"end\":58456,\"start\":58452},{\"end\":58464,\"start\":58461},{\"end\":58475,\"start\":58471},{\"end\":58484,\"start\":58482},{\"end\":58494,\"start\":58489},{\"end\":58506,\"start\":58504},{\"end\":58516,\"start\":58512},{\"end\":58524,\"start\":58522},{\"end\":58648,\"start\":58645},{\"end\":58662,\"start\":58658},{\"end\":58676,\"start\":58672},{\"end\":58688,\"start\":58684},{\"end\":58701,\"start\":58697},{\"end\":58711,\"start\":58708},{\"end\":58869,\"start\":58866},{\"end\":58887,\"start\":58882},{\"end\":58899,\"start\":58894},{\"end\":58912,\"start\":58908},{\"end\":58927,\"start\":58923},{\"end\":59168,\"start\":59165},{\"end\":59181,\"start\":59177},{\"end\":59198,\"start\":59194},{\"end\":59211,\"start\":59206},{\"end\":59225,\"start\":59220},{\"end\":59240,\"start\":59234},{\"end\":59254,\"start\":59251},{\"end\":59281,\"start\":59267},{\"end\":59296,\"start\":59289},{\"end\":59413,\"start\":59410},{\"end\":59423,\"start\":59419},{\"end\":59432,\"start\":59429},{\"end\":59445,\"start\":59441},{\"end\":59459,\"start\":59455},{\"end\":59470,\"start\":59466},{\"end\":59493,\"start\":59482},{\"end\":59503,\"start\":59498},{\"end\":59518,\"start\":59514},{\"end\":59534,\"start\":59532},{\"end\":59665,\"start\":59659},{\"end\":59676,\"start\":59673},{\"end\":59702,\"start\":59689},{\"end\":59720,\"start\":59711},{\"end\":59730,\"start\":59726},{\"end\":59745,\"start\":59737},{\"end\":59757,\"start\":59753},{\"end\":59777,\"start\":59766},{\"end\":59893,\"start\":59887},{\"end\":59908,\"start\":59903},{\"end\":59924,\"start\":59916},{\"end\":59940,\"start\":59934},{\"end\":60090,\"start\":60088},{\"end\":60106,\"start\":60101},{\"end\":60119,\"start\":60117},{\"end\":60132,\"start\":60128},{\"end\":60145,\"start\":60141},{\"end\":60154,\"start\":60151},{\"end\":60164,\"start\":60160},{\"end\":60176,\"start\":60173},{\"end\":60185,\"start\":60181},{\"end\":60195,\"start\":60192},{\"end\":60307,\"start\":60302},{\"end\":60323,\"start\":60314},{\"end\":60340,\"start\":60330},{\"end\":60358,\"start\":60351},{\"end\":60378,\"start\":60367},{\"end\":60452,\"start\":60446},{\"end\":60470,\"start\":60462},{\"end\":60486,\"start\":60479},{\"end\":60505,\"start\":60497},{\"end\":60526,\"start\":60518},{\"end\":60543,\"start\":60534},{\"end\":60561,\"start\":60554},{\"end\":60581,\"start\":60571},{\"end\":60596,\"start\":60590},{\"end\":60832,\"start\":60826},{\"end\":60846,\"start\":60839},{\"end\":60860,\"start\":60853},{\"end\":60875,\"start\":60872},{\"end\":60890,\"start\":60884},{\"end\":60906,\"start\":60900},{\"end\":60918,\"start\":60914},{\"end\":60926,\"start\":60924},{\"end\":60939,\"start\":60936},{\"end\":61012,\"start\":61000},{\"end\":61028,\"start\":61022},{\"end\":61044,\"start\":61037},{\"end\":61065,\"start\":61057},{\"end\":61083,\"start\":61076},{\"end\":61101,\"start\":61094},{\"end\":61116,\"start\":61109},{\"end\":61128,\"start\":61123},{\"end\":61136,\"start\":61130},{\"end\":61262,\"start\":61255},{\"end\":61276,\"start\":61270},{\"end\":61289,\"start\":61284},{\"end\":61303,\"start\":61297},{\"end\":61320,\"start\":61311},{\"end\":61336,\"start\":61330},{\"end\":61355,\"start\":61346},{\"end\":61369,\"start\":61364},{\"end\":61388,\"start\":61380},{\"end\":61404,\"start\":61397},{\"end\":61578,\"start\":61575},{\"end\":61593,\"start\":61588},{\"end\":61609,\"start\":61604},{\"end\":61622,\"start\":61618},{\"end\":61639,\"start\":61634},{\"end\":61649,\"start\":61644},{\"end\":61661,\"start\":61659},{\"end\":61676,\"start\":61673},{\"end\":61865,\"start\":61862},{\"end\":61880,\"start\":61875},{\"end\":61891,\"start\":61889},{\"end\":61907,\"start\":61902},{\"end\":61920,\"start\":61916},{\"end\":61933,\"start\":61930},{\"end\":61948,\"start\":61945},{\"end\":62090,\"start\":62086},{\"end\":62098,\"start\":62095},{\"end\":62114,\"start\":62108},{\"end\":62122,\"start\":62120},{\"end\":62138,\"start\":62131},{\"end\":62148,\"start\":62145},{\"end\":62213,\"start\":62211},{\"end\":62225,\"start\":62221},{\"end\":62240,\"start\":62235},{\"end\":62250,\"start\":62247},{\"end\":62260,\"start\":62257},{\"end\":62270,\"start\":62267},{\"end\":62284,\"start\":62280},{\"end\":62298,\"start\":62293},{\"end\":62307,\"start\":62303},{\"end\":62317,\"start\":62314},{\"end\":62523,\"start\":62519},{\"end\":62532,\"start\":62529},{\"end\":62544,\"start\":62541},{\"end\":62555,\"start\":62552},{\"end\":62570,\"start\":62566},{\"end\":62585,\"start\":62580},{\"end\":62598,\"start\":62595},{\"end\":62608,\"start\":62606},{\"end\":62621,\"start\":62619},{\"end\":62633,\"start\":62631},{\"end\":62739,\"start\":62732},{\"end\":62753,\"start\":62745},{\"end\":62767,\"start\":62763},{\"end\":62780,\"start\":62773},{\"end\":62792,\"start\":62788},{\"end\":62892,\"start\":62887},{\"end\":62908,\"start\":62902},{\"end\":62921,\"start\":62916},{\"end\":62936,\"start\":62929},{\"end\":62947,\"start\":62943},{\"end\":62961,\"start\":62957},{\"end\":62980,\"start\":62975},{\"end\":62991,\"start\":62987},{\"end\":63000,\"start\":62998},{\"end\":63017,\"start\":63005},{\"end\":63067,\"start\":63062},{\"end\":63082,\"start\":63078},{\"end\":63097,\"start\":63092},{\"end\":63111,\"start\":63109},{\"end\":63120,\"start\":63116},{\"end\":63134,\"start\":63128},{\"end\":63147,\"start\":63144},{\"end\":63317,\"start\":63312},{\"end\":63333,\"start\":63327},{\"end\":63345,\"start\":63340},{\"end\":63360,\"start\":63354},{\"end\":63373,\"start\":63371},{\"end\":63389,\"start\":63383},{\"end\":63397,\"start\":63394},{\"end\":63409,\"start\":63407},{\"end\":63421,\"start\":63419},{\"end\":63432,\"start\":63428}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":208290939},\"end\":54842,\"start\":54608},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218971783},\"end\":55094,\"start\":54844},{\"attributes\":{\"doi\":\"arXiv:2303.12712\",\"id\":\"b2\"},\"end\":55350,\"start\":55096},{\"attributes\":{\"doi\":\"arXiv:2307.13304\",\"id\":\"b3\"},\"end\":55516,\"start\":55352},{\"attributes\":{\"id\":\"b4\"},\"end\":55765,\"start\":55518},{\"attributes\":{\"doi\":\"arXiv:1805.06085\",\"id\":\"b5\"},\"end\":55993,\"start\":55767},{\"attributes\":{\"doi\":\"arXiv:1905.10044\",\"id\":\"b6\"},\"end\":56202,\"start\":55995},{\"attributes\":{\"doi\":\"arXiv:1803.05457\",\"id\":\"b7\"},\"end\":56425,\"start\":56204},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":254853733},\"end\":56573,\"start\":56427},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b9\"},\"end\":56730,\"start\":56575},{\"attributes\":{\"doi\":\"arXiv:2305.14314\",\"id\":\"b10\"},\"end\":56878,\"start\":56732},{\"attributes\":{\"doi\":\"arXiv:2306.03078\",\"id\":\"b11\"},\"end\":57149,\"start\":56880},{\"attributes\":{\"doi\":\"arXiv:1902.08153\",\"id\":\"b12\"},\"end\":57319,\"start\":57151},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b13\"},\"end\":57500,\"start\":57321},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b14\"},\"end\":57739,\"start\":57502},{\"attributes\":{\"id\":\"b15\"},\"end\":57967,\"start\":57741},{\"attributes\":{\"doi\":\"arXiv:2009.03300\",\"id\":\"b16\"},\"end\":58154,\"start\":57969},{\"attributes\":{\"doi\":\"arXiv:2306.02272\",\"id\":\"b17\"},\"end\":58352,\"start\":58156},{\"attributes\":{\"doi\":\"arXiv:2102.05426\",\"id\":\"b18\"},\"end\":58560,\"start\":58354},{\"attributes\":{\"doi\":\"arXiv:2306.00978\",\"id\":\"b19\"},\"end\":58747,\"start\":58562},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":244715141},\"end\":59082,\"start\":58749},{\"attributes\":{\"doi\":\"arXiv:2305.17888\",\"id\":\"b21\"},\"end\":59333,\"start\":59084},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":260815690},\"end\":59591,\"start\":59335},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5151364},\"end\":59877,\"start\":59593},{\"attributes\":{\"doi\":\"arXiv:1609.07843\",\"id\":\"b24\"},\"end\":60009,\"start\":59879},{\"attributes\":{\"doi\":\"arXiv:2305.15021\",\"id\":\"b25\"},\"end\":60231,\"start\":60011},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":216056295},\"end\":60434,\"start\":60233},{\"attributes\":{\"doi\":\"arXiv:2306.01116\",\"id\":\"b27\"},\"end\":60735,\"start\":60436},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":204838007},\"end\":60990,\"start\":60737},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b29\"},\"end\":61248,\"start\":60992},{\"attributes\":{\"doi\":\"arXiv:2307.09288\",\"id\":\"b30\"},\"end\":61486,\"start\":61250},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":252545187},\"end\":61735,\"start\":61488},{\"attributes\":{\"doi\":\"arXiv:2304.09145\",\"id\":\"b32\"},\"end\":61984,\"start\":61737},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":253708271},\"end\":62204,\"start\":61986},{\"attributes\":{\"doi\":\"arXiv:2306.09265\",\"id\":\"b34\"},\"end\":62435,\"start\":62206},{\"attributes\":{\"doi\":\"arXiv:2304.01089\",\"id\":\"b35\"},\"end\":62669,\"start\":62437},{\"attributes\":{\"doi\":\"arXiv:1905.07830\",\"id\":\"b36\"},\"end\":62828,\"start\":62671},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b37\"},\"end\":63053,\"start\":62830},{\"attributes\":{\"doi\":\"arXiv:2307.10802\",\"id\":\"b38\"},\"end\":63246,\"start\":63055},{\"attributes\":{\"doi\":\"arXiv:2306.05685\",\"id\":\"b39\"},\"end\":63468,\"start\":63248}]", "bib_title": "[{\"end\":54670,\"start\":54608},{\"end\":54881,\"start\":54844},{\"end\":56485,\"start\":56427},{\"end\":58857,\"start\":58749},{\"end\":59400,\"start\":59335},{\"end\":59651,\"start\":59593},{\"end\":60293,\"start\":60233},{\"end\":60818,\"start\":60737},{\"end\":61565,\"start\":61488},{\"end\":62074,\"start\":61986}]", "bib_author": "[{\"end\":54686,\"start\":54672},{\"end\":54701,\"start\":54686},{\"end\":54715,\"start\":54701},{\"end\":54727,\"start\":54715},{\"end\":54894,\"start\":54883},{\"end\":54909,\"start\":54894},{\"end\":54921,\"start\":54909},{\"end\":54938,\"start\":54921},{\"end\":54954,\"start\":54938},{\"end\":54973,\"start\":54954},{\"end\":54993,\"start\":54973},{\"end\":55007,\"start\":54993},{\"end\":55022,\"start\":55007},{\"end\":55037,\"start\":55022},{\"end\":55114,\"start\":55096},{\"end\":55136,\"start\":55114},{\"end\":55149,\"start\":55136},{\"end\":55166,\"start\":55149},{\"end\":55180,\"start\":55166},{\"end\":55191,\"start\":55180},{\"end\":55202,\"start\":55191},{\"end\":55215,\"start\":55202},{\"end\":55227,\"start\":55215},{\"end\":55243,\"start\":55227},{\"end\":55431,\"start\":55419},{\"end\":55443,\"start\":55431},{\"end\":55463,\"start\":55443},{\"end\":55482,\"start\":55463},{\"end\":55609,\"start\":55593},{\"end\":55621,\"start\":55609},{\"end\":55629,\"start\":55621},{\"end\":55641,\"start\":55629},{\"end\":55654,\"start\":55641},{\"end\":55665,\"start\":55654},{\"end\":55680,\"start\":55665},{\"end\":55695,\"start\":55680},{\"end\":55711,\"start\":55695},{\"end\":55730,\"start\":55711},{\"end\":55742,\"start\":55730},{\"end\":55755,\"start\":55742},{\"end\":55853,\"start\":55838},{\"end\":55864,\"start\":55853},{\"end\":55887,\"start\":55864},{\"end\":55901,\"start\":55887},{\"end\":55923,\"start\":55901},{\"end\":55943,\"start\":55923},{\"end\":55959,\"start\":55943},{\"end\":56086,\"start\":56067},{\"end\":56098,\"start\":56086},{\"end\":56114,\"start\":56098},{\"end\":56131,\"start\":56114},{\"end\":56148,\"start\":56131},{\"end\":56168,\"start\":56148},{\"end\":56297,\"start\":56284},{\"end\":56311,\"start\":56297},{\"end\":56325,\"start\":56311},{\"end\":56338,\"start\":56325},{\"end\":56356,\"start\":56338},{\"end\":56375,\"start\":56356},{\"end\":56391,\"start\":56375},{\"end\":56501,\"start\":56487},{\"end\":56519,\"start\":56501},{\"end\":56589,\"start\":56575},{\"end\":56601,\"start\":56589},{\"end\":56617,\"start\":56601},{\"end\":56635,\"start\":56617},{\"end\":56746,\"start\":56732},{\"end\":56764,\"start\":56746},{\"end\":56778,\"start\":56764},{\"end\":56796,\"start\":56778},{\"end\":56803,\"start\":56796},{\"end\":56976,\"start\":56962},{\"end\":56997,\"start\":56976},{\"end\":57014,\"start\":56997},{\"end\":57032,\"start\":57014},{\"end\":57047,\"start\":57032},{\"end\":57073,\"start\":57047},{\"end\":57091,\"start\":57073},{\"end\":57104,\"start\":57091},{\"end\":57114,\"start\":57104},{\"end\":57177,\"start\":57151},{\"end\":57196,\"start\":57177},{\"end\":57205,\"start\":57196},{\"end\":57419,\"start\":57404},{\"end\":57443,\"start\":57419},{\"end\":57456,\"start\":57443},{\"end\":57466,\"start\":57456},{\"end\":57577,\"start\":57568},{\"end\":57594,\"start\":57577},{\"end\":57605,\"start\":57594},{\"end\":57623,\"start\":57605},{\"end\":57637,\"start\":57623},{\"end\":57653,\"start\":57637},{\"end\":57666,\"start\":57653},{\"end\":57677,\"start\":57666},{\"end\":57690,\"start\":57677},{\"end\":57705,\"start\":57690},{\"end\":57802,\"start\":57793},{\"end\":57816,\"start\":57802},{\"end\":57833,\"start\":57816},{\"end\":57844,\"start\":57833},{\"end\":57860,\"start\":57844},{\"end\":57876,\"start\":57860},{\"end\":57894,\"start\":57876},{\"end\":57907,\"start\":57894},{\"end\":57922,\"start\":57907},{\"end\":57942,\"start\":57922},{\"end\":57984,\"start\":57969},{\"end\":57998,\"start\":57984},{\"end\":58013,\"start\":57998},{\"end\":58023,\"start\":58013},{\"end\":58039,\"start\":58023},{\"end\":58050,\"start\":58039},{\"end\":58068,\"start\":58050},{\"end\":58170,\"start\":58156},{\"end\":58182,\"start\":58170},{\"end\":58193,\"start\":58182},{\"end\":58207,\"start\":58193},{\"end\":58222,\"start\":58207},{\"end\":58227,\"start\":58222},{\"end\":58445,\"start\":58434},{\"end\":58458,\"start\":58445},{\"end\":58466,\"start\":58458},{\"end\":58477,\"start\":58466},{\"end\":58486,\"start\":58477},{\"end\":58496,\"start\":58486},{\"end\":58508,\"start\":58496},{\"end\":58518,\"start\":58508},{\"end\":58526,\"start\":58518},{\"end\":58650,\"start\":58642},{\"end\":58664,\"start\":58650},{\"end\":58678,\"start\":58664},{\"end\":58690,\"start\":58678},{\"end\":58703,\"start\":58690},{\"end\":58713,\"start\":58703},{\"end\":58871,\"start\":58859},{\"end\":58889,\"start\":58871},{\"end\":58901,\"start\":58889},{\"end\":58914,\"start\":58901},{\"end\":58929,\"start\":58914},{\"end\":59170,\"start\":59158},{\"end\":59183,\"start\":59170},{\"end\":59200,\"start\":59183},{\"end\":59213,\"start\":59200},{\"end\":59227,\"start\":59213},{\"end\":59242,\"start\":59227},{\"end\":59256,\"start\":59242},{\"end\":59283,\"start\":59256},{\"end\":59298,\"start\":59283},{\"end\":59415,\"start\":59402},{\"end\":59425,\"start\":59415},{\"end\":59434,\"start\":59425},{\"end\":59447,\"start\":59434},{\"end\":59461,\"start\":59447},{\"end\":59472,\"start\":59461},{\"end\":59495,\"start\":59472},{\"end\":59505,\"start\":59495},{\"end\":59520,\"start\":59505},{\"end\":59536,\"start\":59520},{\"end\":59667,\"start\":59653},{\"end\":59678,\"start\":59667},{\"end\":59685,\"start\":59678},{\"end\":59704,\"start\":59685},{\"end\":59722,\"start\":59704},{\"end\":59732,\"start\":59722},{\"end\":59747,\"start\":59732},{\"end\":59759,\"start\":59747},{\"end\":59779,\"start\":59759},{\"end\":59895,\"start\":59879},{\"end\":59910,\"start\":59895},{\"end\":59926,\"start\":59910},{\"end\":59942,\"start\":59926},{\"end\":60092,\"start\":60084},{\"end\":60108,\"start\":60092},{\"end\":60121,\"start\":60108},{\"end\":60134,\"start\":60121},{\"end\":60147,\"start\":60134},{\"end\":60156,\"start\":60147},{\"end\":60166,\"start\":60156},{\"end\":60178,\"start\":60166},{\"end\":60187,\"start\":60178},{\"end\":60197,\"start\":60187},{\"end\":60309,\"start\":60295},{\"end\":60325,\"start\":60309},{\"end\":60342,\"start\":60325},{\"end\":60360,\"start\":60342},{\"end\":60380,\"start\":60360},{\"end\":60454,\"start\":60436},{\"end\":60472,\"start\":60454},{\"end\":60488,\"start\":60472},{\"end\":60507,\"start\":60488},{\"end\":60528,\"start\":60507},{\"end\":60545,\"start\":60528},{\"end\":60563,\"start\":60545},{\"end\":60583,\"start\":60563},{\"end\":60598,\"start\":60583},{\"end\":60834,\"start\":60820},{\"end\":60848,\"start\":60834},{\"end\":60862,\"start\":60848},{\"end\":60877,\"start\":60862},{\"end\":60892,\"start\":60877},{\"end\":60908,\"start\":60892},{\"end\":60920,\"start\":60908},{\"end\":60928,\"start\":60920},{\"end\":60941,\"start\":60928},{\"end\":61014,\"start\":60992},{\"end\":61030,\"start\":61014},{\"end\":61046,\"start\":61030},{\"end\":61067,\"start\":61046},{\"end\":61085,\"start\":61067},{\"end\":61103,\"start\":61085},{\"end\":61118,\"start\":61103},{\"end\":61130,\"start\":61118},{\"end\":61138,\"start\":61130},{\"end\":61264,\"start\":61250},{\"end\":61278,\"start\":61264},{\"end\":61291,\"start\":61278},{\"end\":61305,\"start\":61291},{\"end\":61322,\"start\":61305},{\"end\":61338,\"start\":61322},{\"end\":61357,\"start\":61338},{\"end\":61371,\"start\":61357},{\"end\":61390,\"start\":61371},{\"end\":61406,\"start\":61390},{\"end\":61580,\"start\":61567},{\"end\":61595,\"start\":61580},{\"end\":61611,\"start\":61595},{\"end\":61624,\"start\":61611},{\"end\":61641,\"start\":61624},{\"end\":61651,\"start\":61641},{\"end\":61663,\"start\":61651},{\"end\":61678,\"start\":61663},{\"end\":61867,\"start\":61854},{\"end\":61882,\"start\":61867},{\"end\":61893,\"start\":61882},{\"end\":61909,\"start\":61893},{\"end\":61922,\"start\":61909},{\"end\":61935,\"start\":61922},{\"end\":61950,\"start\":61935},{\"end\":62092,\"start\":62076},{\"end\":62100,\"start\":62092},{\"end\":62116,\"start\":62100},{\"end\":62124,\"start\":62116},{\"end\":62140,\"start\":62124},{\"end\":62150,\"start\":62140},{\"end\":62215,\"start\":62206},{\"end\":62227,\"start\":62215},{\"end\":62242,\"start\":62227},{\"end\":62252,\"start\":62242},{\"end\":62262,\"start\":62252},{\"end\":62272,\"start\":62262},{\"end\":62286,\"start\":62272},{\"end\":62300,\"start\":62286},{\"end\":62309,\"start\":62300},{\"end\":62319,\"start\":62309},{\"end\":62525,\"start\":62511},{\"end\":62534,\"start\":62525},{\"end\":62546,\"start\":62534},{\"end\":62557,\"start\":62546},{\"end\":62572,\"start\":62557},{\"end\":62587,\"start\":62572},{\"end\":62600,\"start\":62587},{\"end\":62610,\"start\":62600},{\"end\":62623,\"start\":62610},{\"end\":62635,\"start\":62623},{\"end\":62741,\"start\":62726},{\"end\":62755,\"start\":62741},{\"end\":62769,\"start\":62755},{\"end\":62782,\"start\":62769},{\"end\":62794,\"start\":62782},{\"end\":62894,\"start\":62881},{\"end\":62910,\"start\":62894},{\"end\":62923,\"start\":62910},{\"end\":62938,\"start\":62923},{\"end\":62949,\"start\":62938},{\"end\":62963,\"start\":62949},{\"end\":62982,\"start\":62963},{\"end\":62993,\"start\":62982},{\"end\":63002,\"start\":62993},{\"end\":63019,\"start\":63002},{\"end\":63069,\"start\":63055},{\"end\":63084,\"start\":63069},{\"end\":63099,\"start\":63084},{\"end\":63113,\"start\":63099},{\"end\":63122,\"start\":63113},{\"end\":63136,\"start\":63122},{\"end\":63149,\"start\":63136},{\"end\":63319,\"start\":63304},{\"end\":63335,\"start\":63319},{\"end\":63347,\"start\":63335},{\"end\":63362,\"start\":63347},{\"end\":63375,\"start\":63362},{\"end\":63391,\"start\":63375},{\"end\":63399,\"start\":63391},{\"end\":63411,\"start\":63399},{\"end\":63423,\"start\":63411},{\"end\":63434,\"start\":63423}]", "bib_venue": "[{\"end\":54836,\"start\":54790},{\"end\":59078,\"start\":59012},{\"end\":59855,\"start\":59833},{\"end\":54788,\"start\":54727},{\"end\":55086,\"start\":55037},{\"end\":55330,\"start\":55259},{\"end\":55417,\"start\":55352},{\"end\":55591,\"start\":55518},{\"end\":55836,\"start\":55767},{\"end\":56065,\"start\":55995},{\"end\":56282,\"start\":56204},{\"end\":56563,\"start\":56519},{\"end\":56703,\"start\":56651},{\"end\":56857,\"start\":56819},{\"end\":56960,\"start\":56880},{\"end\":57299,\"start\":57221},{\"end\":57402,\"start\":57321},{\"end\":57566,\"start\":57502},{\"end\":57791,\"start\":57741},{\"end\":58134,\"start\":58084},{\"end\":58332,\"start\":58243},{\"end\":58432,\"start\":58354},{\"end\":58640,\"start\":58562},{\"end\":59010,\"start\":58929},{\"end\":59156,\"start\":59084},{\"end\":59580,\"start\":59536},{\"end\":59831,\"start\":59779},{\"end\":59989,\"start\":59958},{\"end\":60082,\"start\":60011},{\"end\":60424,\"start\":60380},{\"end\":60715,\"start\":60614},{\"end\":60981,\"start\":60941},{\"end\":61227,\"start\":61154},{\"end\":61464,\"start\":61422},{\"end\":61727,\"start\":61678},{\"end\":61852,\"start\":61737},{\"end\":62194,\"start\":62150},{\"end\":62415,\"start\":62335},{\"end\":62509,\"start\":62437},{\"end\":62724,\"start\":62671},{\"end\":62879,\"start\":62830},{\"end\":63226,\"start\":63165},{\"end\":63302,\"start\":63248}]"}}}, "year": 2023, "month": 12, "day": 17}