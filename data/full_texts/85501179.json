{"id": 85501179, "updated": "2023-09-30 11:47:43.488", "metadata": {"title": "Attended Temperature Scaling: A Practical Approach for Calibrating Deep Neural Networks", "authors": "[{\"first\":\"Azadeh\",\"last\":\"Mozafari\",\"middle\":[\"Sadat\"]},{\"first\":\"Hugo\",\"last\":\"Gomes\",\"middle\":[\"Siqueira\"]},{\"first\":\"Wilson\",\"last\":\"Leao\",\"middle\":[]},{\"first\":\"Steeven\",\"last\":\"Janny\",\"middle\":[]},{\"first\":\"Christian\",\"last\":\"Gagn'e\",\"middle\":[]}]", "venue": null, "journal": "arXiv: Learning", "publication_date": {"year": 2018, "month": 10, "day": 27}, "abstract": "Recently, Deep Neural Networks (DNNs) have been achieving impressive results on wide range of tasks. However, they suffer from being overconfident. In decision-making applications such as autonomous driving or medical diagnosing, the confidence of deep networks plays an important role to bring the trust and reliability to the system. To calibrate the deep networks many probabilistic and measure-based approaches are proposed. Temperature Scaling (TS) is a state-of-the-art measure-based calibration method which has low time and memory complexity as well as effectiveness. In this paper, we study TS functionality and show it does not work properly when the validation set that TS uses for calibration has small size or contains noisy-labeled samples. TS also cannot calibrate highly accurate networks as well as none highly accurate ones. Accordingly, we propose Attended Temperature Scaling (ATS) which preserves the advantages of TS while improves calibration in aforementioned challenging situations. We provide theoretical justifications for ATS and assess its effectiveness on wide range of deep models and datasets. We also compare the calibration results of TS and ATS on skin lesion detection application as a practical problem where well-calibrated system can play important role in making a decision.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1810.11586", "mag": "2923345718", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": null}}, "content": {"source": {"pdf_hash": "27f16de95799cdb727838609503b2270ef5b7c0d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1810.11586v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "91f8947b75ada69d07336b4be6817911a3de9b30", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/27f16de95799cdb727838609503b2270ef5b7c0d.txt", "contents": "\nAttended Temperature Scaling: A Practical Approach for Calibrating Deep Neural Networks\n\n\nAzadeh Sadat Mozafari azadeh-sadat.mozafari.1@ulaval.ca \nHugo Siqueira Gomes \nWilson Le\u00e3o wilson.leao@petrobras.com.br \nSteeven Janny steeven.janny@ens-paris-saclay.fr \nChristian Gagn\u00e9 christian.gagne@gel.ulaval.ca \nAttended Temperature Scaling: A Practical Approach for Calibrating Deep Neural Networks\n\nRecently, Deep Neural Networks (DNNs) have been achieving impressive results on wide range of tasks. However, they suffer from being overconfident. In decisionmaking applications such as autonomous driving or medical diagnosing, the confidence of deep networks plays an important role to bring the trust and reliability to the system. To calibrate the deep networks many probabilistic and measure-based approaches are proposed. Temperature Scaling (TS) is a state-of-the-art measure-based calibration method which has low time and memory complexity as well as effectiveness. In this paper, we study TS functionality and show it does not work properly when the validation set that TS uses for calibration has small size or contains noisy-labeled samples. TS also cannot calibrate highly accurate networks as well as none highly accurate ones. Accordingly, we propose Attended Temperature Scaling (ATS) which preserves the advantages of TS while improves calibration in aforementioned challenging situations. We provide theoretical justifications for ATS and assess its effectiveness on wide range of deep models and datasets. We also compare the calibration results of TS and ATS on skin lesion detection application as a practical problem where well-calibrated system can play important role in making a decision. Label = Dermatofibroma Pred. = Dermatofibroma Confidence = 0.99 Calib. Confidence = 0.97 Label = Vascular lesion Pred. = Vascular lesion Confidence = 1 Calib. Confidence = 0.98 Label = Melanocytic nevus Pred. = Melanocytic nevus Confidence = 1 Calib. Confidence = 0.91 Label = Melanoma Pred. = Benign keratosis Confidence = 0.96 Calib. Confidence = 0.45 Label = BCC Pred. = Melanocytic nevus Confidence = 0.92 Calib. Confidence = 0.30 Label = Bowen Pred. = Melanocytic nevus Confidence = 0.92 Calib. Confidence = 0.29\n\nIntroduction\n\nDeep Neural Networks (DNNs) show dramatically accurate results on challenging tasks such as computer vision [13,38] speech recognition [11] and medical diagnosis [2]. However, in real-world decision-making applications, accuracy is not the only element considered and the confidence of the network is also essential for having a secure and re- Figure 1: Output of a medical assistant system for skin anomaly detection. Before calibration, the confidence of the system is high for both correctly and misclassified samples which can be dangerous for missing the deadly cases with high confidence. After applying calibration, the network keeps the confidence of correctly classified samples high while decrease the confidence of misclassified samples.\n\nliable system. In DNNs, confidence usually corresponds to the output of a softmax layer, which is typically interpreted as the likeliness (probability) of different class occurrence. Most of the time, this value is far from the true probability of each class occurrence, with a tendency to get overconfident (i.e., output of one class close to 1 and other classes close to 0). In such case, we usually consider the DNN not to be well-calibrated. Calibration in DNNs is a recent challenge in machine learning community which was not an issue previously for shallow neural networks [34]. Gua et al. [12] studies the role of different parameters which makes a neural network uncalibrated. They show a deep network which finds the optimal weights by minimizing Negative Log Likelihood (NLL) loss function, can reach to the higher accuracy when it gets overfitted to NLL. However, the side effect of overfitting to NLL is to make the network overconfident.\n\nHaving calibrated network is important for real-world applications. In a self-driving car [5] deciding about transferring the control of the car to the human observer is taken regarding to the confidence of the detected objects. In medical care systems [16], the deadly diseases can be missed when they are wrongly detected as a non-problematic case with high confidence. Calibration adds more information to the system which consequences reliability. Figure 1 compares the output of an overconfident system and get calibrated one for misclassified and correctly classified samples in a skin lesion detection system. The calibrated networks decrease the confidence in the case of wrongly detected samples while preserves the confidence for most of correctly classified ones. Calibration methods for DNNs are widely investigated in recent literature and can be categorized into two main directions: 1-probabilistic approachs 2-measure-based approachs. Probabilistic approaches generally include approximated Bayesian formalism [28,31,26,4]. In practice, the quality of predictive uncertainty in Bayesian-based methods relies heavily on the accuracy of sampling approximation and correctly estimated prior distribution. Despite of significant achievements in distribution estimation, these approaches are complex and suffer from a significant computational burden, time-and memory complexity.\n\nComparatively, measure-based approachs are more practical. They are generally post-processing methods that do not need to retrain the network to make it calibrated. Temperature Scaling (TS) [12] is the state-of-the-art measurebased approach that comparing to the others, achieves better calibration with minimum computational complexity (optimizing only one parameter T to soften the softmax) which makes it the most appealing method in practice. It also preserves the accuracy rate of the network that can be degraded during the calibration phase. In TS the best T parameter is found by minimizing Negative Log Likelihood (NLL) loss respecting to T on validation set. One big challenge in real scenario is gathering enough number of samples for validation set. The other challenge is to label samples by an expert which is costly. Asking none professional experts to label the samples for decreasing the expenses, may bring labeling noise to the validation set, especially in medical applications. Therefore being robust to the noise and size of validation set is a rising need in calibration applications. Despite of TS interesting results, when the DNN is highly accurate, or the validation set is small or contains noisy labels, TS cannot calibrate the DNN successfully.\n\nContribution: In this paper, we propose a new TS family method which is called Attended Temperature Scaling (ATS) to make a better adjustment of confidence in DNNs. Comparing to TS algorithm, ATS preserves the time and memory complexity advantage of classic TS as well as intact accuracy while it brings better calibration. It specially works properly in the case of small-size validation set, highly accurate DNNs or validation set with labeling noise in which TS is not functioning well. We analyze theoretically ATS and demonstrate why it works better in these situations.\n\n\nRelated Works\n\nRecently, in the literature there is interest in adapting NNs to encompass uncertainty. The studies are summarized into two categories of probabilistic and measurebased approaches. The probabilistic approach is referred to Bayesian theory [3] for estimating the conditional distribution of data. In these methods a prior distribution is defined on the parameter of a NN and given the training set, the posterior of the parameters will be computed. This is the general definition of Bayesian Neural Network which brings back the uncertainty to DNNs framework. As the exact Bayesian inference is not practical, a variety of approximation are proposed such as Laplace approximation [28,37,36,18], Variational Bayesian methods [29,27,4,26] and Monte Carlo Markov Chains (MCMC) [31,1,7] to make Bayesian deep networks tractable. MC-dropout [10] is another probabilistic approach which removes the complicated training setup of Bayesian models and replace it with simple dropout in training and test phases. Gal et al. [10] show MC-dropout approximates Variational Bayesian inference. The testing and training time complexity of MCdropout is high. Although it is a simple approach to apply. Ensemble of DNNs [22] is another non-complicated probabilistic approach that can achieve better calibrated results than MC-dropout. In this method, an ensemble of different deep models are constructed by bagging. This approach is appropriate for the parallel computating with GPUs that can train multiple DNNs with lower time complexity. However, keeping the models in the memory during the test time brings high memory complexity to this method.\n\nMeasure-based approaches are much less complex in applying calibration comparing to probabilistic approaches. In measure-based approach, the main idea is to decrease the miscalibration of the network by minimizing a loss which is a calibration measure. The common calibration measures are: Negative Log Likelihood (NLL), Expected Calibration Error (ECE) [30] and Brier score [6]. Generally for training the neural network, NLL is used which simultaneously increases accuracy and decreases miscalibration. However, it easily gets overfitted and makes the network overconfident [12]. Kumar et al [21] propose a RKHS kernel based measure which they call it MMCE as a derivable surrogate of ECE (ECE is not smooth function). They use MMCE with NLL as the loss function which is minimized during the training to get the network calibrated. The other group of measure-based approaches like Temperature Scaling [12], Platt-Scaling [35] , Histogram Binning [40], Isotonic Regression [41] and Baysian Binning into Quantiles [30] finetune the softmax layer by keeping the DNNs' weights unchanged. They do not need to retrain the deep network from scratch and they only need to find the best parameter of softmax softening function by minimizing a calibration loss on a small validation set. Therefore they are appropriate for real scenarios in which the time and memory complexity of calibration is the concern such as autonomous driving [33] and weather forecasting [20]. In this paper, we have focused on Temperature Scaling family [12] and propose a new approach that can come into better calibration.\n\n\nProblem Setup\n\nIn this section, we set up the problem, notations and introduce two different calibration measures that we will use in the following of the paper. Assumptions: We assume to have access to a pre-trained deep model D(\u00b7) with the ability of detecting K different classes. D(\u00b7) is trained on samples generated from distribution function Q(x, y). We also have access to a small validation dataset\nV = {(x i , y i )} N i=1\nwith the same distribution as the training and test set. For each sample\nx i , there exist h i = [h 1 i , h 2 i , . . . , h K i ] which is the logit layer. D(x i ) = (\u0177 i , S y=\u0177i (x i )) defines that the network D(\u00b7) de- tects label\u0177 i for input data x i and confidence S y=\u0177i (x i ). S y (x) = exp(h y i )/ K j=1 exp(h j i )\nis the softmax output function of the model that here is interpreted as the confidence. Goal: The objective is to adjust h i in order to minimize calibration error of the model.\n\n\nMeasures for Calibration\n\nBased on the different definitions given for calibrated model, two common measures are utilized in the literature which are NLL and ECE.\n\n\nNegative Log Likelihood (NLL)\n\nWhen the network is calibrated, the softmax output layer should be exact approximation of the true conditional distribution Q(y|x). To measure the calibration, the amount of similarity between S y (x) and Q(y|x) functions can be computed. As Q(y|x) distribution function is not available and only some generated samples from it are available (validation set), the similarity can be computed based on Gibbs inequality given in Eq. (1):\n\u2212 E Q(x,y) [log (Q(y|x))] \u2264 \u2212E Q(x,y) [log (P (y|x))], (1)\nwhere E is the expected value function. The minimum of \u2212E Q(x,y) [log P (y|x)] happens when P (y|x) is equal to the true conditional distribution of Q(y|x). This inequality is valid for any arbitrary distribution function P (y|x). NLL is defined as the empirical estimation of \u2212E Q(x,y) [log P (y|x)] which in deep neural networks is rephrased as:\nNLL = \u2212 (xi,yi) log (S y=yi (x i )) , (x i , y i ) \u223c Q(x, y).\n(2) NLL can be used as a calibration measure that shows the similarity between a probability function S y (x) and the true conditional distribution Q(y|x) of data in which the smaller means more calibrated.\n\n\nExpected Calibration Error (ECE)\n\nAnother way to define calibration is based on the relation between the accuracy and confidence. Miscalibration can be interpreted as the difference between confidence and probability of correctly classifying a sample. For instance, in the case of a calibrated model, if we have the group of samples which has the confidence of S y (x) = 0.9, it is supposed to have 0.9 percentage of accuracy. Based on this definition of calibration, ECE is proposed as empirical expectation error between the accuracy and confidence [30]. It is calculated by partitioning the range of confidence between [0 , 1] into L equally-spaced confidence bins and then assign the samples to each bin B l where l = {1, . . . , L} by their confidence range. Later it calculates the weighted absolute difference between the accuracy and confidence for each subset B l . More specifically:\nECE = L l=1 |B l | N acc(B l ) \u2212 conf(B l ) ,(3)\nwhere N is the total number of samples. ECE is not derivable function, therefore we focus on NLL loss function as the measure for the proposed calibration method. However, we report the calibration error on NLL and ECE to show the model will get calibrated by both definitions.\n\n\nTemperature Scaling (TS)\n\nTS is a post-processing approach which rescales the logit layer of a deep model by parameter T that is called temperature. TS is used to soften the output of the softmax layer and makes it more calibrated. The best value of T will be obtained by minimizing NLL loss function respecting to T conditioned by T > 0 on validation set V as defined in Eq. (4):\nT * = arg min T \u2212 N i=1 log S y=yi (x i , T ) S.t : T > 0, (x i , y i ) \u2208 V,(4)where S y=yi (x i , T ) = exp( h y i i T )/ K j=1 exp( h j i T )\n, is the softed version of softmax by applying parameter T . TS has the minimum time and memory complexity among calibration approaches as it only optimizes one parameter T on small validation set. Having only one parameter helps TS not only to be efficient and practical but also not to get overfitted to NLL loss function when it is optimized on small validation set V.\n\n\nAnalyzing TS Approach\n\nTS previously is applied for calibration [12], distilling the knowledge [14] and enhancing the output of DNNs for better discrimination between the in and out distribution samples [25]. It only rescales the output of logit layer to calibrate the network which causes preserving the accuracy unchanged. For the post-processing calibration approaches, keeping the accuracy intact is an important property as the other calibration methods which can change the accuracy is in danger of overfitting to the validation set and accuracy drop. Referring to Eq. (4), by computing the derivative of NLL respecting to T and putting it equal to zero, we will have :\nN i=1 h yi i = N i=1 K k=1 h k i S y=k (x i , T * ).(5)\nIt shows regarding to the true label of the samples, TS selects the T value which maximizes the S y=k (x i , T * ) for k = y i and minimize S y=k (x i , T * ) for all the other k = y i . Therefore for correctly classified samples that y i = arg max(S y (x i )) the maximum confidence of the DNN is for the true class of the samples, T approaches 0 to increase the confidence twoard 1. For missclassified samples, T goes toward \u221e to decrease the confidence of predicted label which is misclassified and increase the confidence of the true label toward 1/K. The balance between the correctly classified and misclassified samples brings back the optimal point T . When the validation set does not contain enough correctly and misclassified samples, TS finds the suboptimal T value. This case happens in calibrating the highly accurate classifiers that the number of misclassified samples for them is few or when the size of validation set is small. TS is also sensitive to the noise of the labels as the optimal T value is dependent strongly on the true label of the samples in NLL loss function.\n\n\nAttended Temperature Scaling (ATS)\n\nTS cannot find the optimal T value when the number of samples in validation set is not enough. The idea of ATS is increasing the number of samples in the validation set with low computational cost. Previously, TS minimizes NLL to decrease the dissimilarity between the S y (x, T ) and Q(y|x). Instead, ATS attends to the conditional distribution of each class and decreases the dissimilarity between S y=k (x, T ) and Q(y = k|x) for each class k = 1, . . . , K. This setting brings a chance to increase the number of samples and robustness to the noise. As the first step, ATS should gather the samples from each class conditional distribution Q(y = k|x). It divides V into K sub validation sets M k which are supposed to contain the samples gener-  ated from the Q(y = k|x). Naturally, the samples whose true label is y = k belongs to this subset. ATS also adds more samples from y = k classes (samples with different class label than k) to M k which have high probability to be generated from distribution Q(x, y = k). Selecting the most probable samples from other classes is done based on:\nQ(x, y = k) = Q(y = k|x) Q(y = k|x) Q(x, y = k)(6)\nwhich is derived from Bayesian Theorem [17]. Eq. (6) says the probability that a sample belongs to distribution Q(x, y = k) is equal to the probability of belonging to the distribution Q(x, y = k) by applying weight W = Q(y = k|x)/Q(y = k|x). In this equation, Q(y = k|x) is equal to 1 \u2212 Q(y = k|x). Referring to Eq. (6), ATS selects the samples with true label y = k which are more probable to belong to Q(x, y = k), i.e. selecting the samples with bigger W . To calculate W , ATS considers S y=k (x) as an approximation to Q(y = k|x). It selects the samples whose S y=k (x) is bigger than a threshold \u03b8. Notice that those samples are generally located near to the boundary of classifier of class k. Figure 2 gives  (7) where \u03b8 is a hyperparameter. After preparing the samples generated from each distribution Q(y = k|x), the optimal T value will be found by minimizing the dissimilarity between S y=k (x, T ) and Q(y = k|x). As ATS changes the distribution of data by adding surrogate samples from other classes to make M k subsets, NLL loss function is not a calibration measure for it anymore. Therefore, we propose a new calibration measure for ATS which can be used as the loss function to find the optimal T :\nM k = {(x i , y i ) | y i = k or S y=k (x i ) \u2265 \u03b8}L AT S = K k=1 (xi,yi)\u2208M k \u2212 log S y=k (x i , T )(1 \u2212 S y=yi (x i , T )) S y =k (x i , T ) , T * = arg min T (L AT S ), S.t : T > 0,(8)\nEq. (8) describe the problem as the binary calibration setting where the labels of the samples are y i \u2208 {k, = k} for each subset M k . We show in supplementary materials that minimizing L AT S with respecting to T on M k leads S y=k (x, T * ) to approach Q(y = k|x) for all k = 1, . . . , K.\n\nConsequently, S y (x, T * ) approaches Q(y|x) which means the model get calibrated. In other words, L AT S is a calibration measure for ATS that by minimizing it the model will get calibrated.\n\n\nAnalyzing ATS Approach\n\nATS considers M k as the samples which are generated from the Q(y = k|x) distribution regardless of their true labels. Therefore it is more robust to the noise than TS. Even for the term (1 \u2212 S y=yi (x i , T )) in L AT S which needs the true label of the sample, because it changes the problem to two-class-label y i \u2208 {k, = k} instead y i \u2208 {1, . . . , K}, it decreases the influence of the noise. ATS also increases the number of samples for estimating Q(y = k|x) by reusing the other class samples in the validation set which leads finding more accurate T values for small validation set. As the added samples are mostly near to the decision boundary, ATS also can improve calibration for the almost accurate DNNs that have few number of misclassified samples to cover that part of distribution.\n\n\nExperiments\n\nWe conduct the experiments in two parts: 1-analyzing behavior of ATS 2-Impact of better calibration in making a decision maker application more reliable. In the first part, we compare ATS with different measure-based and probabilistic-based post-processing approaches in calibrating a wide range of model-dataset. We also compare the robustness of ATS versus TS to the noise of the labels and validation size to show empirically ATS is more reliable and robust than TS. In the second part, we demonstrate the impact of improvement in calibration for making a lesion diagnose system reliable. The list of models and datasets which are used in the experiments are as follows (more details are provided in supplementary material): Datasets: To investigate the validity of ATS, we test the methods on CIFAR-10 [19], CIFAR-100 [19], SVHN [32], MNIST [24], Calthec-UCSD Birds [39] and ImageNet2012 [9]. For the medical application we use ISIC [8] dataset that contains images of 7 different skin lesion types. Models: We try wide range of different state-of-the-art deep convolutional networks with variations in depth. The selected DNNs are Resnet [13], WideResnet [42], DenseNet [15], Lenet [23], and VGG [38]. We use the data preprocessing, training procedures and hyper-parameters as described in each paper. Experiment setup: In all experiments, we select 20% of the test set as validation and the rest as the test dataset to report the results on. For the experiments that have different validation size, the data is selected randomly from the validation set which is already separated from the test. For ATS method, hyper-parameter \u03b8 will be fine-tuned on validation to find the minimum NLL. According to the accuracy of the network, the search interval for \u03b8 is changing between [0, 1], [0, 0.1] or [0, 0.001] and the search step of it would change between 0.01, 0.001 or 0.0001 respectively. When the model is more accurate or the dataset contains more classes the search interval becomes smaller with smaller steps. Baselines to Compare: We compared ATS with different post-processing calibration methods which are:\n\n1. Temperature Scaling [12]: It is explained in Sec. (4) 2. Matrix and Vector Scaling [35]: Matrix Scaling applies a linear transformation on the logits:\nS y=\u0177i (x i , W , b) = max k \u03c3(W .h i + b) (k) y i = arg max k \u03c3(W .h i + b) (k)(9)\nWhere \u03c3 is the softmax function which takes logit layer as an input, and S y=\u0177 (x i , W, b) is the confidence of sample (x i , y i ). The parameters W K\u00d7K and b K are optimized with respect to NLL on the validation set. Vector Scaling is the relaxed version of Matrix Scaling in which W K\u00d7K is a diagonal matrix.\n\n3. MC-dropout [10]: A pre-trained model which is trained with dropout rate p = 0.5 is used with keeping the dropout active during the test. Each sample will be tested 100 times and the average of 100 confidences will be used as the final decision.\n\n\n4.\n\nEnsemble [22]: The ensemble of 3 same-architecture DNN models, which are trained with different random initial weights. The final confidence is the average of the confidence of the models.\n\nEvaluation metric We report the results based on two calibration metric ECE and NLL which are already explained in Sec. (3.1). For ECE we set the number of bins to 15 for all the experiments.\n\n\nResults\n\n\nCalibration\n\nIn Table 1, the calibration result of TS family methods which have only one parameter for fine-tuning the softmax output layer is compared with Matrix and Vector scaling which apply a linear function on logit layer. TS family methods achieves better calibration results as well as preserving the accuracy rate of the network. It seems, however, Matrix and Vector Scaling can define more complex functions to soften the softmax layer, they suffer from over-fitting to the validation set in both accuracy and confidence. ATS in most cases calibrates the network better than all the others. Especially in the case of highly  accurate networks such as VGG16-CIFAR10, ResNet110-SVHN, ResNet110-CIFAR10 and LeNet5-MNIST the calibration error improvement is more than the moderate accurate networks. This can be explained by the lack of misclassified samples in the validation set which prevents TS from converging to the local optima. Different T values which are found for TS and ATS methods with the selected threshold \u03b8 are provided in Table 2. It shows even a small change in the value of T can improve NLL significantly.\n\nIn Table 3 we provide the calibration result of two post-processing probabilistic approaches. To compare with other measure-based approaches, we also report the results of the same model-datasets in the last three lines of Table 1. For investigating the impact of dropout in making the model calibrated, we have trained LeNet5-MNIST, VGG16-CIFAR10 and VGG16-CIFAR100 model-datasets with and without dropout. The only model that uses dropout during the training is MC-Dropout. It shows MC-Dropout and ensemble can improve the accuracy and calibration of the model simultaneously and make the model more calibrated than other measure-based approaches. However, they increase time and memory complexity and they cannot be used to calibrate an already pretrained models.\n\n\nRobustness to Noise\n\nAs explained in Sec. 5.1, theoretically ATS is more robust to noise than TS. In this experiment, we empirically investigate the robustness of ATS to noise in comparing to TS method. We select four different combination of modeldatasets and apply 5 times a range of random noise from 10% to 50% to labels and report the mean and std of NLL (more results are provided in supplementary material). The behavior of ATS and TS are depicted in Figure 3. It shows TS is sensitive to the label noise and even with few percentage of labeling noise it cannot converge to optimal T . However, ATS is more robust to labeling noise and still can calibrate the model when the labels is not severely defected.\n\n\nRobustness to the Size of Validation Set\n\nATS is much less sensitive to the size of the validation set comparing to TS. Figure 4 depicts the results for ATS vs. TS in calibrating four different model-datasets (more results are provided in supplementary material). Each experiment is conducted 5 times and the mean and std are reported. When the validation size is small, TS is not stable. By increasing the number of samples, TS converges to the optimal T value. ATS comparing to TS can find the optimal T by smaller size of the validation set and it is more stable. \n\n\nApplication\n\nIn this section, we show the impact of calibration to improve the decision making in medical assistant system. One of medical applications is anomaly detection for skin spots. We use ISIC [8] dataset which contains 10000 images of 7 different skin lesion types. We divide the dataset into two randomly selected parts of training and test samples with 6000 and 4000 samples respectively. As the number of samples for training is not enough we applied data augmentation which increases both the amount and diversity of data by randomly augmenting to train the ResNet200. Training settings and details of application is reported in supplementary materials.\n\nIn a medical assistant system, the final confidence is used to refer the patient to specialist for further experiments. Therefore the ideal detection system should be certain when it correctly classifies a sample and uncertain when it missclasifies a sample. However, it is obvious that the system cannot be certain about the correctly classified samples which are located near to the decision boundary. The ResNet200 trained model on the dataset is overconfident. We divide the test set into 400 images validation set and 1600 test images. Later, we apply TS and ATS to calibrate the model. The results of calibration is reported in Table  4. In a referral system, the samples that have higher confidence than a specific threshold, they will be accepted as the correctly classified and for the samples which have the confidence less than the threshold it will be referred to the specialist for further experiments.\n\nATS improves calibration in order to have better referral system. To compare which method of TS or ATS can operate better as a referral system, we plot percentage of correctly classified samples that have confidence higher than the selected threshold versus percentage of misclassified samples that have confidence lower than that threshold. Figure 5 compares uncalibrated , calibrated with TS and ATS for different values of threshold which is changing in the interval of [0, 1] with step size 0.05. The Area Under Curve (AUC) is provided. It shows ATS by better calibration can improve the decision making system to refer less correctly classified cases to the specialist and do not miss the misclassified one by not referring them. Figure 6 gives some examples of canfidence output of the sytem before and after calibrating with TS and ATS methods. Comparatively, ATS can increase the gap between the confidence of correctly and misclassified samples more. Figure 5: Reliability diagram of the skin lesions system. In the same rate of correctly classified samples upper than specific threshold the system that have more misclassified samples lower than that threshold is more reliable.  \n\n\nConclusion\n\nDespite of dramatically improved accuracy of deep neural networks, they suffer from being overconfident. In this paper, we proposed ATS as a practical solution for calibrating DNNs. ATS is a Temperature Scaling family method which tries to find the optimal T value for rescaling the softmax layer and makes it more calibrated. ATS can find the optimal T in the situation that the deep network is highly accurate, validation set is small or when it contains noisy labels where classic TS fails to find the best T . ATS is a post-processing approach that does not need to retrain the network for calibrating it which makes it an appropriate solution for calibrating already pretrained models and can be used in many applications as a practical solution.\n\nFigure 2 :\n2A three class classification problem. The samples which are selected for each subset M k is shown inside the colored region. ATS considers two groups of samples to construct M k subset: the samples with true label y = k and the samples with true label y = k that located near to the decision boundary of that class (the black line).\n\nFigure 3 :\n3Calibration of different models-datasets with TS and ATS methods for 10% \u223c 50% of labeling noise.\n\nFigure 4 :\n4Calibration of different models-datasets with TS and ATS methods for different validation size.\n\nFigure 6 :\n6Comparison between TS and ATS calibration method for calibrating the correctly classified and missclassified samples. ATS increase the confidence gap for the correctly classified and misclassified samples (more examples are provided in supplementary materials).\n\nTable 1 :\n1The results of different measure-based calibration methods for variation of datasets and models. ATS achieves the best calibration results for almost all experiments.Uncalib. \nTS, ATS \nUncalibrated \nTS \nATS \nMatrix Scaling \nVector Scaling \n\nModel \nDataset \nACC \nNLL \nECE% \nNLL \nECE% \nNLL \nECE% \nACC \nNLL \nECE% \nACC \nNLL \nECE% \nVGG16 \nBirds \n75.975% \n0.929 \n6.033 \n0.929 \n6.021 \n0.919 \n3.572 \n74.870% \n0.961 \n5.683 \n76.99% \n1.153 \n11.992 \nResNet152 \nImageNet \n76.71% \n0.935 \n5.935 \n0.927 \n5.412 \n0.900 \n1.982 \n76.03% \n0.935 \n5.932 \n76.99% \n1.175 \n12.214 \nDenseNet40 \nCIFAR10 \n92.61% \n0.286 \n4.089 \n0.234 \n3.241 \n0.221 \n0.657 \n81.50% \n0.590 \n5.793 \n92.09% \n0.360 \n4.939 \nDenseNet40 \nCIFAR100 \n71.73% \n1.088 \n8.456 \n1.000 \n1.148 \n1.000 \n1.004 \n57.50% \n1.918 \n19.269 \n32.48% \n9.655 \n52.609 \nDenseNet100 \nCIFAR10 \n95.06% \n0.199 \n2.618 \n0.156 \n0.594 \n0.156 \n0.580 \n94.38% \n0.191 \n2.272 \n94.97% \n0.247 \n3.263 \nDenseNet100 \nCIFAR100 \n76.21% \n1.119 \n11.969 \n0.886 \n4.742 \n0.871 \n1.583 \n63.92% \n1.857 \n19.977 \n73.67% \n1.602 \n18.073 \nDenseNet100 \nSVHN \n95.72% \n0.181 \n1.630 \n0.162 \n0.548 \n0.161 \n0.514 \n95.73% \n0.170 \n1.126 \n95.99% \n0.162 \n0.548 \nResNet110 \nCIFAR10 \n93.71% \n0.312 \n4.343 \n0.228 \n4.298 \n0.206 \n0.972 \n92.13% \n0.285 \n3.597 \n93.17% \n0.375 \n5.033 \nResNet110 \nCIFAR100 \n70.31% \n1.248 \n12.752 \n1.051 \n1.804 \n1.050 \n1.529 \n58.03% \n2.074 \n20.749 \n68.15% \n1.705 \n19.751 \nResNet110 \nSVHN \n96.06% \n0.209 \n2.697 \n0.158 \n1.552 \n0.154 \n0.849 \n96.00% \n0.173 \n1.769 \n96.06% \n0.254 \n2.946 \nWideResNet32 \nCIFAR100 \n75.41% \n1.166 \n13.406 \n0.909 \n4.096 \n0.891 \n2.511 \n66.18% \n1.673 \n18.265 \n73.17% \n1.693 \n18.7 \nLeNet 5 \nMNIST \n99.03% \n0.105 \n0.727 \n0.061 \n0.674 \n0.0341 \n0.354 \n98.59% \n0.115 \n1.152 \n98.33 % \n0.048 \n0.668 \nVGG16 \nCIFAR10 \n92.09% \n0.427 \n5.99 \n0.301 \n6.015 \n0.271 \n2.978 \n90.66% \n0.364 \n5.776 \n91.90% \n0.432 \n5.968 \nVGG16 \nCIFAR100 \n69.00% \n1.984 \n21.493 \n1.283 \n8.072 \n1.273 \n8.283 \n51.30% \n2.290 \n19.022 \n68.92% \n2.441 \n23.055 \n\n\n\nTable 2 :\n2The selected threshold \u03b8, TS temperature and ATS temperature for the datasets and models reported inTable 1.Model \nDataset \nT T S \nT AT S \n\u03b8 \n\nVGG16 \nBirds \n1.000 \n1.101 \n0.01 \nResNet152 \nImageNet \n1.064 \n1.322 \n0.03 \nDenseNet40 \nCIFAR10 \n2.505 \n1.921 \n0.04 \nDenseNet40 \nCIFAR100 \n1.450 \n1.435 \n0.02 \nDenseNet100 \nCIFAR10 \n1.801 \n1.809 \n0.05 \nDenseNet100 \nCIFAR100 \n2.178 \n1.904 \n0.01 \nDenseNet100 \nSVHN \n1.407 \n1.355 \n0.05 \nResNet110 \nCIFAR10 \n2.960 \n2.321 \n0.02 \nResNet110 \nCIFAR100 \n1.801 \n1.630 \n0.02 \nResNet110 \nSVHN \n2.090 \n1.975 \n0.01 \nWideResNet32 \nCIFAR100 \n2.104 \n1.971 \n0.01 \nLeNet 5 \nMNIST \n1.645 \n2.832 \n0.0008 \nVGG16 \nCIFAR10 \n3.229 \n1.941 \n0.02 \nVGG16 \nCIFAR100 \n2.741 \n1.968 \n0.01 \n\n\n\nTable 3 :\n3The calibration results of probabilistic-based approaches MC-dropout and ensemble.an example of selected samples for M k in the case of three class classification problem. Formal saying M k is:MC-Dropout \nEnsemble \nModel \nDataset \nACC \nNLL \nECE% ACC \nNLL \nECE% \nLeNet 5 \nMNIST \n99.33% 0.044 \n0.453 \n99.12% 0.026 \n0.307 \nVGG16 \nCIFAR10 \n93.32% 0.262 \n4.239 \n92.1% \n0.342 \n5.99 \nVGG16 \nCIFAR100 70.45% 1.527 \n18.25 \n74.38% 1.218 \n6.756 \n\n\n\nTable 4 :\n4Comparing calibrating ResNet200 model trained on ISIC dataset with TS and ATS approaches.Model \nDataset \nACC \nUncalibrated \nTS \nATS \nT T S \nT AT S \n\u03b8 \nNLL \nECE% \nNLL \nECE% \nNLL \nECE% \nResNet200 \nISIC \n89.14% \n0.696 \n8.374 \n0.379 \n7.978 \n0.333 \n1.157 \n4.934 \n3.548 \n0.01 \n\n\n\n\nLabel = Benign keratosis Pred. = Benign keratosis Confidence = 0.99 ATS Confidence = 0.93 TS Confidence = 0.55 Label = Dermatofibroma Pred. = Dermatofibroma Confidence = 0.99 ATS Confidence = 0.93 TS Confidence = 0.60 Label = Vascular lesion Pred. = Vascular lesion Confidence = 0.99 ATS Confidence = 0.90 TS Confidence = 0.57 Label = Melanoma Pred. = Vascular lesion Confidence = 0.96 ATS Confidence = 0.69 TS Confidence = 0.42 Label = BCC Pred. = Melanocytic nevus Confidence = 0.98 ATS Confidence = 0.76 TS Confidence = 0.46 Label = Bowen Pred. = Benign keratosis Confidence = 0.86 ATS Confidence = 0.65 TS Confidence = 0.44\n\nBayesian dark knowledge. A K Balan, V Rathod, K P Murphy, M Welling, Advances in Neural Information Processing Systems. A. K. Balan, V. Rathod, K. P. Murphy, and M. Welling. Bayesian dark knowledge. In Advances in Neural Informa- tion Processing Systems, pages 3438-3446, 2015.\n\nChest pathology detection using deep learning with non-medical training. Y Bar, I Diamant, L Wolf, S Lieberman, E Konen, H Greenspan, ISBI. CiteseerY. Bar, I. Diamant, L. Wolf, S. Lieberman, E. Konen, and H. Greenspan. Chest pathology detection using deep learn- ing with non-medical training. In ISBI, pages 294-297. Cite- seer, 2015.\n\nBayesian theory. J M Bernardo, A F Smith, John Wiley & Sons405J. M. Bernardo and A. F. Smith. Bayesian theory, volume 405. John Wiley & Sons, 2009.\n\nC Blundell, J Cornebise, K Kavukcuoglu, D Wierstra, arXiv:1505.05424Weight uncertainty in neural networks. arXiv preprintC. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wier- stra. Weight uncertainty in neural networks. arXiv preprint arXiv:1505.05424, 2015.\n\nEnd to end learning for self-driving cars. M Bojarski, D Testa, D Dworakowski, B Firner, B Flepp, P Goyal, L D Jackel, M Monfort, U Muller, J Zhang, arXiv:1604.07316arXiv preprintM. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\n\nVerification of forecasts expressed in terms of probability. G W Brier, Monthey Weather Review. 781G. W. Brier. Verification of forecasts expressed in terms of probability. Monthey Weather Review, 78(1):1-3, 1950.\n\nStochastic gradient hamiltonian monte carlo. T Chen, E Fox, C Guestrin, International Conference on Machine Learning. T. Chen, E. Fox, and C. Guestrin. Stochastic gradient hamil- tonian monte carlo. In International Conference on Machine Learning, pages 1683-1691, 2014.\n\nSkin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). N C Codella, D Gutman, M E Celebi, B Helba, M A Marchetti, S W Dusza, A Kalloo, K Liopyris, N Mishra, H Kittler, 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018). IEEEN. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler, et al. Skin lesion analysis toward melanoma de- tection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic). In 2018 IEEE 15th Interna- tional Symposium on Biomedical Imaging (ISBI 2018), pages 168-172. IEEE, 2018.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.\n\nDropout as a bayesian approximation: Representing model uncertainty in deep learning. Y Gal, Z Ghahramani, international conference on machine learning. Y. Gal and Z. Ghahramani. Dropout as a bayesian approxi- mation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050- 1059, 2016.\n\nSpeech recognition with deep recurrent neural networks. A Graves, A Mohamed, G Hinton, Acoustics, speech and signal processing (icassp), 2013 ieee international conference on. IEEEA. Graves, A.-r. Mohamed, and G. Hinton. Speech recog- nition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee interna- tional conference on, pages 6645-6649. IEEE, 2013.\n\nC Guo, G Pleiss, Y Sun, K Q Weinberger, arXiv:1706.04599On calibration of modern neural networks. arXiv preprintC. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. arXiv preprint arXiv:1706.04599, 2017.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 770-778, 2016.\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, arXiv:1503.02531arXiv preprintG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nF Iandola, M Moskewicz, S Karayev, R Girshick, T Darrell, K Keutzer, arXiv:1404.1869Densenet: Implementing efficient convnet descriptor pyramids. arXiv preprintF. Iandola, M. Moskewicz, S. Karayev, R. Girshick, T. Dar- rell, and K. Keutzer. Densenet: Implementing efficient con- vnet descriptor pyramids. arXiv preprint arXiv:1404.1869, 2014.\n\nCalibrating predictive model estimates to support personalized medicine. X Jiang, M Osl, J Kim, L Ohno-Machado, Journal of the American Medical Informatics Association. 192X. Jiang, M. Osl, J. Kim, and L. Ohno-Machado. Calibrating predictive model estimates to support personalized medicine. Journal of the American Medical Informatics Association, 19(2):263-274, 2011.\n\nIntrospective classification with convolutional nets. L Jin, J Lazarow, Z Tu, Advances in Neural Information Processing Systems. L. Jin, J. Lazarow, and Z. Tu. Introspective classification with convolutional nets. In Advances in Neural Information Processing Systems, pages 823-833, 2017.\n\nOvercoming catastrophic forgetting in neural networks. J Kirkpatrick, R Pascanu, N Rabinowitz, J Veness, G Desjardins, A A Rusu, K Milan, J Quan, T Ramalho, A Grabska-Barwinska, Proceedings of the national academy of sciences. the national academy of sciences114J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Des- jardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic for- getting in neural networks. Proceedings of the national academy of sciences, 114(13):3521-3526, 2017.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, CiteseerTechnical reportA. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.\n\nAccurate uncertainties for deep learning using calibrated regression. V Kuleshov, N Fenner, S Ermon, arXiv:1807.00263arXiv preprintV. Kuleshov, N. Fenner, and S. Ermon. Accurate uncer- tainties for deep learning using calibrated regression. arXiv preprint arXiv:1807.00263, 2018.\n\nTrainable calibration measures for neural networks from kernel mean embeddings. A Kumar, S Sarawagi, U Jain, International Conference on Machine Learning. A. Kumar, S. Sarawagi, and U. Jain. Trainable calibration measures for neural networks from kernel mean embeddings. In International Conference on Machine Learning, pages 2810-2819, 2018.\n\nSimple and scalable predictive uncertainty estimation using deep ensembles. B Lakshminarayanan, A Pritzel, C Blundell, Advances in Neural Information Processing Systems. B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty estimation using deep en- sembles. In Advances in Neural Information Processing Sys- tems, pages 6402-6413, 2017.\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, et al. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998.\n\n. Y Lecun, C Cortes, C Burges, Mnist datasetY. LeCun, C. Cortes, and C. Burges. Mnist dataset, 1998.\n\nEnhancing the reliability of out-of-distribution image detection in neural networks. S Liang, Y Li, R Srikant, arXiv:1706.02690arXiv preprintS. Liang, Y. Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. arXiv preprint arXiv:1706.02690, 2017.\n\nStructured and efficient variational deep learning with matrix gaussian posteriors. C Louizos, M Welling, ternational Conference on Machine Learning. C. Louizos and M. Welling. Structured and efficient varia- tional deep learning with matrix gaussian posteriors. In In- ternational Conference on Machine Learning, pages 1708- 1716, 2016.\n\nMultiplicative normalizing flows for variational bayesian neural networks. C Louizos, M Welling, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70C. Louizos and M. Welling. Multiplicative normalizing flows for variational bayesian neural networks. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pages 2218-2227. JMLR. org, 2017.\n\nBayesian methods for adaptive models. D J Mackay, California Institute of TechnologyPhD thesisD. J. MacKay. Bayesian methods for adaptive models. PhD thesis, California Institute of Technology, 1992.\n\nVariational dropout sparsifies deep neural networks. D Molchanov, A Ashukha, D Vetrov, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70D. Molchanov, A. Ashukha, and D. Vetrov. Variational dropout sparsifies deep neural networks. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pages 2498-2507. JMLR. org, 2017.\n\nObtaining well calibrated probabilities using bayesian binning. M P Naeini, G Cooper, M Hauskrecht, Twenty-Ninth AAAI Conference on Artificial Intelligence. M. P. Naeini, G. Cooper, and M. Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Twenty- Ninth AAAI Conference on Artificial Intelligence, 2015.\n\nBayesian learning for neural networks. R M Neal, Springer Science & Business Media118R. M. Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media, 2012.\n\nReading digits in natural images with unsupervised feature learning. Y Netzer, T Wang, A Coates, A Bissacco, B Wu, A Ng, NIPS. 012011Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y Ng. Reading digits in natural images with unsupervised feature learning. NIPS, 01 2011.\n\nRelaxed softmax: Efficient confidence auto-calibration for safe pedestrian detection. L Neumann, A Zisserman, A Vedaldi, Machine Learning for Intelligent Transportation Systems Workshop, NIPS. L. Neumann, A. Zisserman, and A. Vedaldi. Relaxed soft- max: Efficient confidence auto-calibration for safe pedes- trian detection. In Machine Learning for Intelligent Trans- portation Systems Workshop, NIPS, 2018.\n\nPredicting good probabilities with supervised learning. A Niculescu-Mizil, R Caruana, Proceedings of the 22nd international conference on Machine learning. the 22nd international conference on Machine learningACMA. Niculescu-Mizil and R. Caruana. Predicting good proba- bilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning, pages 625- 632. ACM, 2005.\n\nProbabilistic outputs for support vector machines and comparisons to regularized likelihood methods. J Platt, Advances in large margin classifiers. 103J. Platt et al. Probabilistic outputs for support vector ma- chines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 10(3):61-74, 1999.\n\nOnline structured laplace approximations for overcoming catastrophic forgetting. H Ritter, A Botev, D Barber, Advances in Neural Information Processing Systems. H. Ritter, A. Botev, and D. Barber. Online structured laplace approximations for overcoming catastrophic forgetting. In Advances in Neural Information Processing Systems, pages 3742-3752, 2018.\n\nA scalable laplace approximation for neural networks. H Ritter, A Botev, D Barber, H. Ritter, A. Botev, and D. Barber. A scalable laplace ap- proximation for neural networks. 2018.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nThe caltech-ucsd birds. C Wah, S Branson, P Welinder, P Perona, S Belongie, C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n\nObtaining calibrated probability estimates from decision trees and naive bayesian classifiers. B Zadrozny, C Elkan, Icml. Citeseer1B. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers. In Icml, volume 1, pages 609-616. Citeseer, 2001.\n\nTransforming classifier scores into accurate multiclass probability estimates. B Zadrozny, C Elkan, Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. the eighth ACM SIGKDD international conference on Knowledge discovery and data miningACMB. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability estimates. In Proceed- ings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 694-699. ACM, 2002.\n\nS Zagoruyko, N Komodakis, arXiv:1605.07146Wide residual networks. arXiv preprintS. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\n", "annotations": {"author": "[{\"end\":147,\"start\":91},{\"end\":168,\"start\":148},{\"end\":210,\"start\":169},{\"end\":259,\"start\":211},{\"end\":306,\"start\":260}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":104},{\"end\":167,\"start\":162},{\"end\":180,\"start\":176},{\"end\":224,\"start\":219},{\"end\":275,\"start\":270}]", "author_first_name": "[{\"end\":97,\"start\":91},{\"end\":103,\"start\":98},{\"end\":152,\"start\":148},{\"end\":161,\"start\":153},{\"end\":175,\"start\":169},{\"end\":218,\"start\":211},{\"end\":269,\"start\":260}]", "author_affiliation": null, "title": "[{\"end\":88,\"start\":1},{\"end\":394,\"start\":307}]", "venue": null, "abstract": "[{\"end\":2227,\"start\":396}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2355,\"start\":2351},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2358,\"start\":2355},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2382,\"start\":2378},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2408,\"start\":2405},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3577,\"start\":3573},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3594,\"start\":3590},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4039,\"start\":4036},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4203,\"start\":4199},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4976,\"start\":4972},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4979,\"start\":4976},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4982,\"start\":4979},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4984,\"start\":4982},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5532,\"start\":5528},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7449,\"start\":7446},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7890,\"start\":7886},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7893,\"start\":7890},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7896,\"start\":7893},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7899,\"start\":7896},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7934,\"start\":7930},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7937,\"start\":7934},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7939,\"start\":7937},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7942,\"start\":7939},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7984,\"start\":7980},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7986,\"start\":7984},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7988,\"start\":7986},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8046,\"start\":8042},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8224,\"start\":8220},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8413,\"start\":8409},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9198,\"start\":9194},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9218,\"start\":9215},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9420,\"start\":9416},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9438,\"start\":9434},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9748,\"start\":9744},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9768,\"start\":9764},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9793,\"start\":9789},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9819,\"start\":9815},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9859,\"start\":9855},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10272,\"start\":10268},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10301,\"start\":10297},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10368,\"start\":10364},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13240,\"start\":13236},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14875,\"start\":14871},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14906,\"start\":14902},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15014,\"start\":15010},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17859,\"start\":17855},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21356,\"start\":21352},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21372,\"start\":21368},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21383,\"start\":21379},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21395,\"start\":21391},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21420,\"start\":21416},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21441,\"start\":21438},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21486,\"start\":21483},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21693,\"start\":21689},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21710,\"start\":21706},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21725,\"start\":21721},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21737,\"start\":21733},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":21751,\"start\":21747},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22694,\"start\":22690},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22757,\"start\":22753},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23237,\"start\":23233},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23486,\"start\":23482},{\"end\":23788,\"start\":23783},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27261,\"start\":27258}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30944,\"start\":30599},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31055,\"start\":30945},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31164,\"start\":31056},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31439,\"start\":31165},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33386,\"start\":31440},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34098,\"start\":33387},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34547,\"start\":34099},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34832,\"start\":34548},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35462,\"start\":34833}]", "paragraph": "[{\"end\":2991,\"start\":2243},{\"end\":3944,\"start\":2993},{\"end\":5336,\"start\":3946},{\"end\":6612,\"start\":5338},{\"end\":7189,\"start\":6614},{\"end\":8838,\"start\":7207},{\"end\":10434,\"start\":8840},{\"end\":10843,\"start\":10452},{\"end\":10941,\"start\":10869},{\"end\":11373,\"start\":11196},{\"end\":11538,\"start\":11402},{\"end\":12006,\"start\":11572},{\"end\":12413,\"start\":12066},{\"end\":12682,\"start\":12476},{\"end\":13578,\"start\":12719},{\"end\":13905,\"start\":13628},{\"end\":14288,\"start\":13934},{\"end\":14804,\"start\":14433},{\"end\":15482,\"start\":14830},{\"end\":16632,\"start\":15539},{\"end\":17764,\"start\":16671},{\"end\":19032,\"start\":17816},{\"end\":19511,\"start\":19219},{\"end\":19705,\"start\":19513},{\"end\":20530,\"start\":19732},{\"end\":22665,\"start\":20546},{\"end\":22820,\"start\":22667},{\"end\":23217,\"start\":22905},{\"end\":23466,\"start\":23219},{\"end\":23661,\"start\":23473},{\"end\":23854,\"start\":23663},{\"end\":24999,\"start\":23880},{\"end\":25767,\"start\":25001},{\"end\":26484,\"start\":25791},{\"end\":27054,\"start\":26529},{\"end\":27723,\"start\":27070},{\"end\":28640,\"start\":27725},{\"end\":29832,\"start\":28642},{\"end\":30598,\"start\":29847}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10868,\"start\":10844},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11195,\"start\":10942},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12065,\"start\":12007},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12475,\"start\":12414},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13627,\"start\":13579},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14368,\"start\":14289},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14432,\"start\":14368},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15538,\"start\":15483},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17815,\"start\":17765},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19083,\"start\":19033},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19218,\"start\":19083},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22904,\"start\":22821}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23890,\"start\":23883},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24920,\"start\":24913},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25011,\"start\":25004},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28367,\"start\":28359}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2241,\"start\":2229},{\"attributes\":{\"n\":\"2.\"},\"end\":7205,\"start\":7192},{\"attributes\":{\"n\":\"3.\"},\"end\":10450,\"start\":10437},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11400,\"start\":11376},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":11570,\"start\":11541},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":12717,\"start\":12685},{\"attributes\":{\"n\":\"4.\"},\"end\":13932,\"start\":13908},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14828,\"start\":14807},{\"attributes\":{\"n\":\"5.\"},\"end\":16669,\"start\":16635},{\"attributes\":{\"n\":\"5.1.\"},\"end\":19730,\"start\":19708},{\"attributes\":{\"n\":\"6.\"},\"end\":20544,\"start\":20533},{\"end\":23471,\"start\":23469},{\"attributes\":{\"n\":\"6.1.\"},\"end\":23864,\"start\":23857},{\"attributes\":{\"n\":\"6.1.1\"},\"end\":23878,\"start\":23867},{\"attributes\":{\"n\":\"6.1.2\"},\"end\":25789,\"start\":25770},{\"attributes\":{\"n\":\"6.1.3\"},\"end\":26527,\"start\":26487},{\"attributes\":{\"n\":\"6.2.\"},\"end\":27068,\"start\":27057},{\"attributes\":{\"n\":\"7.\"},\"end\":29845,\"start\":29835},{\"end\":30610,\"start\":30600},{\"end\":30956,\"start\":30946},{\"end\":31067,\"start\":31057},{\"end\":31176,\"start\":31166},{\"end\":31450,\"start\":31441},{\"end\":33397,\"start\":33388},{\"end\":34109,\"start\":34100},{\"end\":34558,\"start\":34549}]", "table": "[{\"end\":33386,\"start\":31618},{\"end\":34098,\"start\":33507},{\"end\":34547,\"start\":34304},{\"end\":34832,\"start\":34649}]", "figure_caption": "[{\"end\":30944,\"start\":30612},{\"end\":31055,\"start\":30958},{\"end\":31164,\"start\":31069},{\"end\":31439,\"start\":31178},{\"end\":31618,\"start\":31452},{\"end\":33507,\"start\":33399},{\"end\":34304,\"start\":34111},{\"end\":34649,\"start\":34560},{\"end\":35462,\"start\":34835}]", "figure_ref": "[{\"end\":2595,\"start\":2587},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18525,\"start\":18517},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26236,\"start\":26228},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26615,\"start\":26607},{\"end\":28992,\"start\":28984},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29385,\"start\":29377},{\"end\":29610,\"start\":29602}]", "bib_author_first_name": "[{\"end\":35490,\"start\":35489},{\"end\":35492,\"start\":35491},{\"end\":35501,\"start\":35500},{\"end\":35511,\"start\":35510},{\"end\":35513,\"start\":35512},{\"end\":35523,\"start\":35522},{\"end\":35817,\"start\":35816},{\"end\":35824,\"start\":35823},{\"end\":35835,\"start\":35834},{\"end\":35843,\"start\":35842},{\"end\":35856,\"start\":35855},{\"end\":35865,\"start\":35864},{\"end\":36098,\"start\":36097},{\"end\":36100,\"start\":36099},{\"end\":36112,\"start\":36111},{\"end\":36114,\"start\":36113},{\"end\":36230,\"start\":36229},{\"end\":36242,\"start\":36241},{\"end\":36255,\"start\":36254},{\"end\":36270,\"start\":36269},{\"end\":36535,\"start\":36534},{\"end\":36547,\"start\":36546},{\"end\":36556,\"start\":36555},{\"end\":36571,\"start\":36570},{\"end\":36581,\"start\":36580},{\"end\":36590,\"start\":36589},{\"end\":36599,\"start\":36598},{\"end\":36601,\"start\":36600},{\"end\":36611,\"start\":36610},{\"end\":36622,\"start\":36621},{\"end\":36632,\"start\":36631},{\"end\":36943,\"start\":36942},{\"end\":36945,\"start\":36944},{\"end\":37142,\"start\":37141},{\"end\":37150,\"start\":37149},{\"end\":37157,\"start\":37156},{\"end\":37558,\"start\":37557},{\"end\":37560,\"start\":37559},{\"end\":37571,\"start\":37570},{\"end\":37581,\"start\":37580},{\"end\":37583,\"start\":37582},{\"end\":37593,\"start\":37592},{\"end\":37602,\"start\":37601},{\"end\":37604,\"start\":37603},{\"end\":37617,\"start\":37616},{\"end\":37619,\"start\":37618},{\"end\":37628,\"start\":37627},{\"end\":37638,\"start\":37637},{\"end\":37650,\"start\":37649},{\"end\":37660,\"start\":37659},{\"end\":38234,\"start\":38233},{\"end\":38242,\"start\":38241},{\"end\":38250,\"start\":38249},{\"end\":38263,\"start\":38259},{\"end\":38269,\"start\":38268},{\"end\":38275,\"start\":38274},{\"end\":38653,\"start\":38652},{\"end\":38660,\"start\":38659},{\"end\":38964,\"start\":38963},{\"end\":38974,\"start\":38973},{\"end\":38985,\"start\":38984},{\"end\":39311,\"start\":39310},{\"end\":39318,\"start\":39317},{\"end\":39328,\"start\":39327},{\"end\":39335,\"start\":39334},{\"end\":39337,\"start\":39336},{\"end\":39600,\"start\":39599},{\"end\":39606,\"start\":39605},{\"end\":39615,\"start\":39614},{\"end\":39622,\"start\":39621},{\"end\":40007,\"start\":40006},{\"end\":40017,\"start\":40016},{\"end\":40028,\"start\":40027},{\"end\":40188,\"start\":40187},{\"end\":40199,\"start\":40198},{\"end\":40212,\"start\":40211},{\"end\":40223,\"start\":40222},{\"end\":40235,\"start\":40234},{\"end\":40246,\"start\":40245},{\"end\":40605,\"start\":40604},{\"end\":40614,\"start\":40613},{\"end\":40621,\"start\":40620},{\"end\":40628,\"start\":40627},{\"end\":40957,\"start\":40956},{\"end\":40964,\"start\":40963},{\"end\":40975,\"start\":40974},{\"end\":41248,\"start\":41247},{\"end\":41263,\"start\":41262},{\"end\":41274,\"start\":41273},{\"end\":41288,\"start\":41287},{\"end\":41298,\"start\":41297},{\"end\":41312,\"start\":41311},{\"end\":41314,\"start\":41313},{\"end\":41322,\"start\":41321},{\"end\":41331,\"start\":41330},{\"end\":41339,\"start\":41338},{\"end\":41350,\"start\":41349},{\"end\":41785,\"start\":41784},{\"end\":41799,\"start\":41798},{\"end\":42022,\"start\":42021},{\"end\":42034,\"start\":42033},{\"end\":42044,\"start\":42043},{\"end\":42313,\"start\":42312},{\"end\":42322,\"start\":42321},{\"end\":42334,\"start\":42333},{\"end\":42653,\"start\":42652},{\"end\":42673,\"start\":42672},{\"end\":42684,\"start\":42683},{\"end\":43036,\"start\":43035},{\"end\":43045,\"start\":43044},{\"end\":43055,\"start\":43054},{\"end\":43065,\"start\":43064},{\"end\":43241,\"start\":43240},{\"end\":43250,\"start\":43249},{\"end\":43260,\"start\":43259},{\"end\":43426,\"start\":43425},{\"end\":43435,\"start\":43434},{\"end\":43441,\"start\":43440},{\"end\":43724,\"start\":43723},{\"end\":43735,\"start\":43734},{\"end\":44054,\"start\":44053},{\"end\":44065,\"start\":44064},{\"end\":44460,\"start\":44459},{\"end\":44462,\"start\":44461},{\"end\":44676,\"start\":44675},{\"end\":44689,\"start\":44688},{\"end\":44700,\"start\":44699},{\"end\":45112,\"start\":45111},{\"end\":45114,\"start\":45113},{\"end\":45124,\"start\":45123},{\"end\":45134,\"start\":45133},{\"end\":45420,\"start\":45419},{\"end\":45422,\"start\":45421},{\"end\":45640,\"start\":45639},{\"end\":45650,\"start\":45649},{\"end\":45658,\"start\":45657},{\"end\":45668,\"start\":45667},{\"end\":45680,\"start\":45679},{\"end\":45686,\"start\":45685},{\"end\":45939,\"start\":45938},{\"end\":45950,\"start\":45949},{\"end\":45963,\"start\":45962},{\"end\":46318,\"start\":46317},{\"end\":46337,\"start\":46336},{\"end\":46769,\"start\":46768},{\"end\":47077,\"start\":47076},{\"end\":47087,\"start\":47086},{\"end\":47096,\"start\":47095},{\"end\":47406,\"start\":47405},{\"end\":47416,\"start\":47415},{\"end\":47425,\"start\":47424},{\"end\":47602,\"start\":47601},{\"end\":47614,\"start\":47613},{\"end\":47817,\"start\":47816},{\"end\":47824,\"start\":47823},{\"end\":47835,\"start\":47834},{\"end\":47847,\"start\":47846},{\"end\":47857,\"start\":47856},{\"end\":48073,\"start\":48072},{\"end\":48085,\"start\":48084},{\"end\":48360,\"start\":48359},{\"end\":48372,\"start\":48371},{\"end\":48810,\"start\":48809},{\"end\":48823,\"start\":48822}]", "bib_author_last_name": "[{\"end\":35498,\"start\":35493},{\"end\":35508,\"start\":35502},{\"end\":35520,\"start\":35514},{\"end\":35531,\"start\":35524},{\"end\":35821,\"start\":35818},{\"end\":35832,\"start\":35825},{\"end\":35840,\"start\":35836},{\"end\":35853,\"start\":35844},{\"end\":35862,\"start\":35857},{\"end\":35875,\"start\":35866},{\"end\":36109,\"start\":36101},{\"end\":36120,\"start\":36115},{\"end\":36239,\"start\":36231},{\"end\":36252,\"start\":36243},{\"end\":36267,\"start\":36256},{\"end\":36279,\"start\":36271},{\"end\":36544,\"start\":36536},{\"end\":36553,\"start\":36548},{\"end\":36568,\"start\":36557},{\"end\":36578,\"start\":36572},{\"end\":36587,\"start\":36582},{\"end\":36596,\"start\":36591},{\"end\":36608,\"start\":36602},{\"end\":36619,\"start\":36612},{\"end\":36629,\"start\":36623},{\"end\":36638,\"start\":36633},{\"end\":36951,\"start\":36946},{\"end\":37147,\"start\":37143},{\"end\":37154,\"start\":37151},{\"end\":37166,\"start\":37158},{\"end\":37568,\"start\":37561},{\"end\":37578,\"start\":37572},{\"end\":37590,\"start\":37584},{\"end\":37599,\"start\":37594},{\"end\":37614,\"start\":37605},{\"end\":37625,\"start\":37620},{\"end\":37635,\"start\":37629},{\"end\":37647,\"start\":37639},{\"end\":37657,\"start\":37651},{\"end\":37668,\"start\":37661},{\"end\":38239,\"start\":38235},{\"end\":38247,\"start\":38243},{\"end\":38257,\"start\":38251},{\"end\":38266,\"start\":38264},{\"end\":38272,\"start\":38270},{\"end\":38283,\"start\":38276},{\"end\":38657,\"start\":38654},{\"end\":38671,\"start\":38661},{\"end\":38971,\"start\":38965},{\"end\":38982,\"start\":38975},{\"end\":38992,\"start\":38986},{\"end\":39315,\"start\":39312},{\"end\":39325,\"start\":39319},{\"end\":39332,\"start\":39329},{\"end\":39348,\"start\":39338},{\"end\":39603,\"start\":39601},{\"end\":39612,\"start\":39607},{\"end\":39619,\"start\":39616},{\"end\":39626,\"start\":39623},{\"end\":40014,\"start\":40008},{\"end\":40025,\"start\":40018},{\"end\":40033,\"start\":40029},{\"end\":40196,\"start\":40189},{\"end\":40209,\"start\":40200},{\"end\":40220,\"start\":40213},{\"end\":40232,\"start\":40224},{\"end\":40243,\"start\":40236},{\"end\":40254,\"start\":40247},{\"end\":40611,\"start\":40606},{\"end\":40618,\"start\":40615},{\"end\":40625,\"start\":40622},{\"end\":40641,\"start\":40629},{\"end\":40961,\"start\":40958},{\"end\":40972,\"start\":40965},{\"end\":40978,\"start\":40976},{\"end\":41260,\"start\":41249},{\"end\":41271,\"start\":41264},{\"end\":41285,\"start\":41275},{\"end\":41295,\"start\":41289},{\"end\":41309,\"start\":41299},{\"end\":41319,\"start\":41315},{\"end\":41328,\"start\":41323},{\"end\":41336,\"start\":41332},{\"end\":41347,\"start\":41340},{\"end\":41368,\"start\":41351},{\"end\":41796,\"start\":41786},{\"end\":41806,\"start\":41800},{\"end\":42031,\"start\":42023},{\"end\":42041,\"start\":42035},{\"end\":42050,\"start\":42045},{\"end\":42319,\"start\":42314},{\"end\":42331,\"start\":42323},{\"end\":42339,\"start\":42335},{\"end\":42670,\"start\":42654},{\"end\":42681,\"start\":42674},{\"end\":42693,\"start\":42685},{\"end\":43042,\"start\":43037},{\"end\":43052,\"start\":43046},{\"end\":43062,\"start\":43056},{\"end\":43073,\"start\":43066},{\"end\":43247,\"start\":43242},{\"end\":43257,\"start\":43251},{\"end\":43267,\"start\":43261},{\"end\":43432,\"start\":43427},{\"end\":43438,\"start\":43436},{\"end\":43449,\"start\":43442},{\"end\":43732,\"start\":43725},{\"end\":43743,\"start\":43736},{\"end\":44062,\"start\":44055},{\"end\":44073,\"start\":44066},{\"end\":44469,\"start\":44463},{\"end\":44686,\"start\":44677},{\"end\":44697,\"start\":44690},{\"end\":44707,\"start\":44701},{\"end\":45121,\"start\":45115},{\"end\":45131,\"start\":45125},{\"end\":45145,\"start\":45135},{\"end\":45427,\"start\":45423},{\"end\":45647,\"start\":45641},{\"end\":45655,\"start\":45651},{\"end\":45665,\"start\":45659},{\"end\":45677,\"start\":45669},{\"end\":45683,\"start\":45681},{\"end\":45689,\"start\":45687},{\"end\":45947,\"start\":45940},{\"end\":45960,\"start\":45951},{\"end\":45971,\"start\":45964},{\"end\":46334,\"start\":46319},{\"end\":46345,\"start\":46338},{\"end\":46775,\"start\":46770},{\"end\":47084,\"start\":47078},{\"end\":47093,\"start\":47088},{\"end\":47103,\"start\":47097},{\"end\":47413,\"start\":47407},{\"end\":47422,\"start\":47417},{\"end\":47432,\"start\":47426},{\"end\":47611,\"start\":47603},{\"end\":47624,\"start\":47615},{\"end\":47821,\"start\":47818},{\"end\":47832,\"start\":47825},{\"end\":47844,\"start\":47836},{\"end\":47854,\"start\":47848},{\"end\":47866,\"start\":47858},{\"end\":48082,\"start\":48074},{\"end\":48091,\"start\":48086},{\"end\":48369,\"start\":48361},{\"end\":48378,\"start\":48373},{\"end\":48820,\"start\":48811},{\"end\":48833,\"start\":48824}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":84176918},\"end\":35741,\"start\":35464},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11362170},\"end\":36078,\"start\":35743},{\"attributes\":{\"id\":\"b2\"},\"end\":36227,\"start\":36080},{\"attributes\":{\"doi\":\"arXiv:1505.05424\",\"id\":\"b3\"},\"end\":36489,\"start\":36229},{\"attributes\":{\"doi\":\"arXiv:1604.07316\",\"id\":\"b4\"},\"end\":36879,\"start\":36491},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":122906757},\"end\":37094,\"start\":36881},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3228832},\"end\":37366,\"start\":37096},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":10768153},\"end\":38178,\"start\":37368},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":57246310},\"end\":38564,\"start\":38180},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":160705},\"end\":38905,\"start\":38566},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206741496},\"end\":39308,\"start\":38907},{\"attributes\":{\"doi\":\"arXiv:1706.04599\",\"id\":\"b11\"},\"end\":39551,\"start\":39310},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":39958,\"start\":39553},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b13\"},\"end\":40185,\"start\":39960},{\"attributes\":{\"doi\":\"arXiv:1404.1869\",\"id\":\"b14\"},\"end\":40529,\"start\":40187},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7716019},\"end\":40900,\"start\":40531},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":22378364},\"end\":41190,\"start\":40902},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4704285},\"end\":41727,\"start\":41192},{\"attributes\":{\"id\":\"b18\"},\"end\":41949,\"start\":41729},{\"attributes\":{\"doi\":\"arXiv:1807.00263\",\"id\":\"b19\"},\"end\":42230,\"start\":41951},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":49314079},\"end\":42574,\"start\":42232},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6294674},\"end\":42952,\"start\":42576},{\"attributes\":{\"id\":\"b22\"},\"end\":43236,\"start\":42954},{\"attributes\":{\"id\":\"b23\"},\"end\":43338,\"start\":43238},{\"attributes\":{\"doi\":\"arXiv:1706.02690\",\"id\":\"b24\"},\"end\":43637,\"start\":43340},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":216077482},\"end\":43976,\"start\":43639},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9280646},\"end\":44419,\"start\":43978},{\"attributes\":{\"id\":\"b27\"},\"end\":44620,\"start\":44421},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":18201582},\"end\":45045,\"start\":44622},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6292807},\"end\":45378,\"start\":45047},{\"attributes\":{\"id\":\"b30\"},\"end\":45568,\"start\":45380},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":16852518},\"end\":45850,\"start\":45570},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53530110},\"end\":46259,\"start\":45852},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":207158152},\"end\":46665,\"start\":46261},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":56563878},\"end\":46993,\"start\":46667},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":29169199},\"end\":47349,\"start\":46995},{\"attributes\":{\"id\":\"b36\"},\"end\":47531,\"start\":47351},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b37\"},\"end\":47790,\"start\":47533},{\"attributes\":{\"id\":\"b38\"},\"end\":47975,\"start\":47792},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":9594071},\"end\":48278,\"start\":47977},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3349576},\"end\":48807,\"start\":48280},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b41\"},\"end\":48982,\"start\":48809}]", "bib_title": "[{\"end\":35487,\"start\":35464},{\"end\":35814,\"start\":35743},{\"end\":36940,\"start\":36881},{\"end\":37139,\"start\":37096},{\"end\":37555,\"start\":37368},{\"end\":38231,\"start\":38180},{\"end\":38650,\"start\":38566},{\"end\":38961,\"start\":38907},{\"end\":39597,\"start\":39553},{\"end\":40602,\"start\":40531},{\"end\":40954,\"start\":40902},{\"end\":41245,\"start\":41192},{\"end\":42310,\"start\":42232},{\"end\":42650,\"start\":42576},{\"end\":43721,\"start\":43639},{\"end\":44051,\"start\":43978},{\"end\":44673,\"start\":44622},{\"end\":45109,\"start\":45047},{\"end\":45637,\"start\":45570},{\"end\":45936,\"start\":45852},{\"end\":46315,\"start\":46261},{\"end\":46766,\"start\":46667},{\"end\":47074,\"start\":46995},{\"end\":48070,\"start\":47977},{\"end\":48357,\"start\":48280}]", "bib_author": "[{\"end\":35500,\"start\":35489},{\"end\":35510,\"start\":35500},{\"end\":35522,\"start\":35510},{\"end\":35533,\"start\":35522},{\"end\":35823,\"start\":35816},{\"end\":35834,\"start\":35823},{\"end\":35842,\"start\":35834},{\"end\":35855,\"start\":35842},{\"end\":35864,\"start\":35855},{\"end\":35877,\"start\":35864},{\"end\":36111,\"start\":36097},{\"end\":36122,\"start\":36111},{\"end\":36241,\"start\":36229},{\"end\":36254,\"start\":36241},{\"end\":36269,\"start\":36254},{\"end\":36281,\"start\":36269},{\"end\":36546,\"start\":36534},{\"end\":36555,\"start\":36546},{\"end\":36570,\"start\":36555},{\"end\":36580,\"start\":36570},{\"end\":36589,\"start\":36580},{\"end\":36598,\"start\":36589},{\"end\":36610,\"start\":36598},{\"end\":36621,\"start\":36610},{\"end\":36631,\"start\":36621},{\"end\":36640,\"start\":36631},{\"end\":36953,\"start\":36942},{\"end\":37149,\"start\":37141},{\"end\":37156,\"start\":37149},{\"end\":37168,\"start\":37156},{\"end\":37570,\"start\":37557},{\"end\":37580,\"start\":37570},{\"end\":37592,\"start\":37580},{\"end\":37601,\"start\":37592},{\"end\":37616,\"start\":37601},{\"end\":37627,\"start\":37616},{\"end\":37637,\"start\":37627},{\"end\":37649,\"start\":37637},{\"end\":37659,\"start\":37649},{\"end\":37670,\"start\":37659},{\"end\":38241,\"start\":38233},{\"end\":38249,\"start\":38241},{\"end\":38259,\"start\":38249},{\"end\":38268,\"start\":38259},{\"end\":38274,\"start\":38268},{\"end\":38285,\"start\":38274},{\"end\":38659,\"start\":38652},{\"end\":38673,\"start\":38659},{\"end\":38973,\"start\":38963},{\"end\":38984,\"start\":38973},{\"end\":38994,\"start\":38984},{\"end\":39317,\"start\":39310},{\"end\":39327,\"start\":39317},{\"end\":39334,\"start\":39327},{\"end\":39350,\"start\":39334},{\"end\":39605,\"start\":39599},{\"end\":39614,\"start\":39605},{\"end\":39621,\"start\":39614},{\"end\":39628,\"start\":39621},{\"end\":40016,\"start\":40006},{\"end\":40027,\"start\":40016},{\"end\":40035,\"start\":40027},{\"end\":40198,\"start\":40187},{\"end\":40211,\"start\":40198},{\"end\":40222,\"start\":40211},{\"end\":40234,\"start\":40222},{\"end\":40245,\"start\":40234},{\"end\":40256,\"start\":40245},{\"end\":40613,\"start\":40604},{\"end\":40620,\"start\":40613},{\"end\":40627,\"start\":40620},{\"end\":40643,\"start\":40627},{\"end\":40963,\"start\":40956},{\"end\":40974,\"start\":40963},{\"end\":40980,\"start\":40974},{\"end\":41262,\"start\":41247},{\"end\":41273,\"start\":41262},{\"end\":41287,\"start\":41273},{\"end\":41297,\"start\":41287},{\"end\":41311,\"start\":41297},{\"end\":41321,\"start\":41311},{\"end\":41330,\"start\":41321},{\"end\":41338,\"start\":41330},{\"end\":41349,\"start\":41338},{\"end\":41370,\"start\":41349},{\"end\":41798,\"start\":41784},{\"end\":41808,\"start\":41798},{\"end\":42033,\"start\":42021},{\"end\":42043,\"start\":42033},{\"end\":42052,\"start\":42043},{\"end\":42321,\"start\":42312},{\"end\":42333,\"start\":42321},{\"end\":42341,\"start\":42333},{\"end\":42672,\"start\":42652},{\"end\":42683,\"start\":42672},{\"end\":42695,\"start\":42683},{\"end\":43044,\"start\":43035},{\"end\":43054,\"start\":43044},{\"end\":43064,\"start\":43054},{\"end\":43075,\"start\":43064},{\"end\":43249,\"start\":43240},{\"end\":43259,\"start\":43249},{\"end\":43269,\"start\":43259},{\"end\":43434,\"start\":43425},{\"end\":43440,\"start\":43434},{\"end\":43451,\"start\":43440},{\"end\":43734,\"start\":43723},{\"end\":43745,\"start\":43734},{\"end\":44064,\"start\":44053},{\"end\":44075,\"start\":44064},{\"end\":44471,\"start\":44459},{\"end\":44688,\"start\":44675},{\"end\":44699,\"start\":44688},{\"end\":44709,\"start\":44699},{\"end\":45123,\"start\":45111},{\"end\":45133,\"start\":45123},{\"end\":45147,\"start\":45133},{\"end\":45429,\"start\":45419},{\"end\":45649,\"start\":45639},{\"end\":45657,\"start\":45649},{\"end\":45667,\"start\":45657},{\"end\":45679,\"start\":45667},{\"end\":45685,\"start\":45679},{\"end\":45691,\"start\":45685},{\"end\":45949,\"start\":45938},{\"end\":45962,\"start\":45949},{\"end\":45973,\"start\":45962},{\"end\":46336,\"start\":46317},{\"end\":46347,\"start\":46336},{\"end\":46777,\"start\":46768},{\"end\":47086,\"start\":47076},{\"end\":47095,\"start\":47086},{\"end\":47105,\"start\":47095},{\"end\":47415,\"start\":47405},{\"end\":47424,\"start\":47415},{\"end\":47434,\"start\":47424},{\"end\":47613,\"start\":47601},{\"end\":47626,\"start\":47613},{\"end\":47823,\"start\":47816},{\"end\":47834,\"start\":47823},{\"end\":47846,\"start\":47834},{\"end\":47856,\"start\":47846},{\"end\":47868,\"start\":47856},{\"end\":48084,\"start\":48072},{\"end\":48093,\"start\":48084},{\"end\":48371,\"start\":48359},{\"end\":48380,\"start\":48371},{\"end\":48822,\"start\":48809},{\"end\":48835,\"start\":48822}]", "bib_venue": "[{\"end\":39769,\"start\":39707},{\"end\":41451,\"start\":41419},{\"end\":44198,\"start\":44145},{\"end\":44832,\"start\":44779},{\"end\":46470,\"start\":46417},{\"end\":48567,\"start\":48482},{\"end\":35582,\"start\":35533},{\"end\":35881,\"start\":35877},{\"end\":36095,\"start\":36080},{\"end\":36334,\"start\":36297},{\"end\":36532,\"start\":36491},{\"end\":36975,\"start\":36953},{\"end\":37212,\"start\":37168},{\"end\":37742,\"start\":37670},{\"end\":38348,\"start\":38285},{\"end\":38717,\"start\":38673},{\"end\":39081,\"start\":38994},{\"end\":39406,\"start\":39366},{\"end\":39705,\"start\":39628},{\"end\":40004,\"start\":39960},{\"end\":40331,\"start\":40271},{\"end\":40698,\"start\":40643},{\"end\":41029,\"start\":40980},{\"end\":41417,\"start\":41370},{\"end\":41782,\"start\":41729},{\"end\":42019,\"start\":41951},{\"end\":42385,\"start\":42341},{\"end\":42744,\"start\":42695},{\"end\":43033,\"start\":42954},{\"end\":43423,\"start\":43340},{\"end\":43787,\"start\":43745},{\"end\":44143,\"start\":44075},{\"end\":44457,\"start\":44421},{\"end\":44777,\"start\":44709},{\"end\":45202,\"start\":45147},{\"end\":45417,\"start\":45380},{\"end\":45695,\"start\":45691},{\"end\":46043,\"start\":45973},{\"end\":46415,\"start\":46347},{\"end\":46813,\"start\":46777},{\"end\":47154,\"start\":47105},{\"end\":47403,\"start\":47351},{\"end\":47599,\"start\":47533},{\"end\":47814,\"start\":47792},{\"end\":48097,\"start\":48093},{\"end\":48480,\"start\":48380},{\"end\":48873,\"start\":48851}]"}}}, "year": 2023, "month": 12, "day": 17}