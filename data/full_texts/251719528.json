{"id": 251719528, "updated": "2023-10-05 11:34:08.095", "metadata": {"title": "Artifact-Based Domain Generalization of Skin Lesion Models", "authors": "[{\"first\":\"Alceu\",\"last\":\"Bissoto\",\"middle\":[]},{\"first\":\"Catarina\",\"last\":\"Barata\",\"middle\":[]},{\"first\":\"Eduardo\",\"last\":\"Valle\",\"middle\":[]},{\"first\":\"Sandra\",\"last\":\"Avila\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Deep Learning failure cases are abundant, particularly in the medical area. Recent studies in out-of-distribution generalization have advanced considerably on well-controlled synthetic datasets, but they do not represent medical imaging contexts. We propose a pipeline that relies on artifacts annotation to enable generalization evaluation and debiasing for the challenging skin lesion analysis context. First, we partition the data into levels of increasingly higher biased training and test sets for better generalization assessment. Then, we create environments based on skin lesion artifacts to enable domain generalization methods. Finally, after robust training, we perform a test-time debiasing procedure, reducing spurious features in inference images. Our experiments show our pipeline improves performance metrics in biased cases, and avoids artifacts when using explanation methods. Still, when evaluating such models in out-of-distribution data, they did not prefer clinically-meaningful features. Instead, performance only improved in test sets that present similar artifacts from training, suggesting models learned to ignore the known set of artifacts. Our results raise a concern that debiasing models towards a single aspect may not be enough for fair skin lesion analysis.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2208.09756", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/BissotoBVA22", "doi": "10.48550/arxiv.2208.09756"}}, "content": {"source": {"pdf_hash": "81dc8abb788d85d39b3fd9180d31d246e621d5b9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2208.09756v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5751be5393aada2737ca3beefbcadfad2c4c71c4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/81dc8abb788d85d39b3fd9180d31d246e621d5b9.txt", "contents": "\nArtifact-based Domain Generalization of Skin Lesion Models\n\n\n\nInstitute of Computing\nUniversity of Campinas\nBrazil\n\n\nInstitute for Systems and Robotics\nInstituto Superior T\u00e9cnico\nPortugal\n\n\nSchool of Electrical and Computing Engineering\nUniversity of Campinas\nBrazil\n\n\nRecod.ai Lab\nUniversity of Campinas\nBrazil\n\nArtifact-based Domain Generalization of Skin Lesion Models\nskin lesionsartifactsdebiasingdomain generalization\nAlceu Bissoto [0000\u22120003\u22122293\u22126160]1,4 , Catarina Barata [0000\u22120002\u22122852\u22127723]2 , Eduardo Valle [0000\u22120001\u22125396\u22129868]3,4 , and Sandra Avila [0000\u22120001\u22129068\u2212938X]1,4Abstract. Deep Learning failure cases are abundant, particularly in the medical area. Recent studies in out-of-distribution generalization have advanced considerably on well-controlled synthetic datasets, but they do not represent medical imaging contexts. We propose a pipeline that relies on artifacts annotation to enable generalization evaluation and debiasing for the challenging skin lesion analysis context. First, we partition the data into levels of increasingly higher biased training and test sets for better generalization assessment. Then, we create environments based on skin lesion artifacts to enable domain generalization methods. Finally, after robust training, we perform a test-time debiasing procedure, reducing spurious features in inference images. Our experiments show our pipeline improves performance metrics in biased cases, and avoids artifacts when using explanation methods. Still, when evaluating such models in outof-distribution data, they did not prefer clinically-meaningful features. Instead, performance only improved in test sets that present similar artifacts from training, suggesting models learned to ignore the known set of artifacts. Our results raise a concern that debiasing models towards a single aspect may not be enough for fair skin lesion analysis.\n\nIntroduction\n\nDespite Deep Learning's superhuman performance on many tasks, models still struggle to generalize, stalling the adoption of AI for critical decisions such as medical diagnosis.\n\nSkin lesion analysis is no exception. Recent works exposed concerning model behaviors, such as achieving high performances with the lesions fully occluded on the image [6], or exploiting the presence of artifacts (e.g., rulers positioned by dermatologists to measure lesions) to shortcut learning [7]. Moreover, current arXiv:2208.09756v1 [cs.CV] 20 Aug 2022 models fail to cope with underrepresented populations such as Black, Hispanic, and Asian people. Those shortcomings prevent automated skin analysis solutions from wider adoption and from realizing their potential public health benefits.\n\nDomain Generalization (DG), which in computer vision studies how models fall prey to spurious correlations, is yet to be adequately adopted by the medical image analysis literature, partly because medical data often lack the labeled environments which are a critical input to most DG techniques. Within a corpus of data, Environments are groups or domains that share a common characteristic (e.g., predominant image color, image capturing device, demographic similarities). In DG research, datasets are often synthetic, creating environments on demand, or multi-sourced, with an environment for each source. Medical data, however, pose special challenges due to their complexity and multi-faceted nature, presenting multiple ways of grouping data, or latent environments whose full annotation is next to impossible. We are interested in adapting DG techniques to benefit those complex and rich tasks, considering those challenges.\n\nWe start by allowing the assessment of generalization performance, even when out-of-distribution data are unavailable, using a tunable version of \"trap sets\" [7]. Next, we infer existing, latent environments from available data, enabling the adoption of robust learning methods developed in the DG literature. Finally, after model training, we select robust features during test-time, censoring irrelevant information. Our extensive experiments show that it is possible to obtain models that are resilient to training with highly biased data. Code to reproduce our experiments is available at https://github.com/alceubissoto/ artifact-generalization-skin.\n\nOur main contributions are:\n\n-We propose a method to adapt existing annotations into environments, successfully increasing the robustness of skin lesion analysis models; -We propose a test-time procedure that consistently improves biased models' performance; -We show that model debiasing is insufficient to increase out-of-distribution performance. Better characterization of out-of-distribution spurious sources is necessary to train more robust models.\n\n\nBackground\n\nDomain Generalization (DG) and Domain Adaptation (DA) aim to study and mitigate distribution shifts between training and test data for a known (in the case of DA) or an unknown (in the case of DG) distribution test data distribution.\n\nHere we focus on DG techniques since the test distribution is almost always unknown for medical analysis. A complete review of the extensive literature on DG is outside the scope of this work. We point the reader to two recent surveys of the area [33,43]. In this section, we will briefly review the two techniques directly used in this work.\n\nDG techniques are contrasted with the classical Empirical Risk Minimization [37] (ERM) learning criterion, which assumes that the samples are independent and identically distributed (i.i.d.) and that train and test sets are sampled from the same distribution. For the sake of completeness, the ERM minimization goal is defined as R ERM (\u03b8) = 1 n n i=1 \u2113 (x i , y i ; \u03b8), where \u2113 is the classification loss, \u03b8 is the model's parameters, and n is the number of samples (x, y). DG techniques deal with train-test distribution shifts. We present two of them below.\n\nDistributional Robust Optimization (DRO) [19,30] methods minimize the maximum risk for all groups (while ERM minimizes the global average risk). That way, the model focuses on high-risk groups, which usually comprise those with correlations underrepresented in the dataset. The risk is calculated as:\nR DRO (\u03b8) := max e\u2208Etr\u00ca P e [\u2113(x, y; \u03b8)],(1)\nwhere we evaluate the expectation separately for each environment distribution P e , and the data is separated into environments e, sampled from the set of all environments available for training E tr . DRO can prevent models from exploiting spurious correlations, for example, if the risk is low for a biased group and high for an unbiased group. In that case, success depends on groups being separated by bias. DRO can also raise the importance of small groups (e.g., rare animal subspecies, rare pathological conditions), which would be obliterated by averaging. DRO techniques require explicitly labeled environments, and one of our main contributions is evaluating one of them (GroupDRO) on inferred environments.\n\nRepresentation Self-Challenging (RSC) [20] is a three-step robust deep learning training method. At each training iteration, RSC sets to zero the most predictive part of the model representation, according to the gradients. More specifically, the model representations with the highest gradients will be set to zero before the model update. Such feature selection causes less dominant features in the training set to be learned by the model, potentially discarding easy-to-learn spurious correlations and thus preventing the so-called shortcut learning [32]. We use RSC as a strong baseline for comparing with our proposed pipeline, since this technique does not require environment labels, being adaptable to any classification problem. In a recent benchmark [40], RSC appears as one of the few effective methods, including for the PatchCamelyon histopathology dataset [4,22].\n\n\nMethodology\n\nThe main objective is to learn more robust skin lesion representations using deep learning for skin lesion analysis, considering the binary problem of melanoma vs. benign. To achieve this, we present a pipeline ( Fig. 1) that proposes 1) partitioning data into train/test trap sets that simulate a highly biased scenario; 2) crafting and exploiting training data partitions (environments) to learn robust representations through GroupDRO [30]; 3) selecting task-relevant features for inference, avoiding spurious ones.  \n\n\nTrap Sets\n\nSince spurious correlations inflate metrics, DG methods require carefully crafted protocols to measure generalization. Often, datasets introduce correlations with the class labels (using an extraneous feature, such as color in ColorMNIST [2]), which purposefully differ between the training and the test split. Here, we follow the \"trap set\" procedure [7] to craft training and test with amplified correlations between artifacts and class labels (malignant vs. benign), which appear in opposite directions in the dataset splits. We adapt the trap set protocol, introducing a tunable level of bias, from 0 (randomly selected sets) to 1 (highly biased). This level controls, for each sample in the split, the probability of selecting it at random versus following the trap set procedure. Table 1 illustrates the correlations between artifact and class labels on the splits for the bias levels used in this work. We think our trap sets can expand generalization measurements to be used outside of specialized literature, reaching problems that urgently need out-of-distribution performance assessment.\n\n\nArtifact-based Environments\n\nIn DG, environments divide data according to spurious characteristics. For example, ColorMNIST [2] is divided into two environments: one that correlates colors with values (one color for each digit), and another with colors chosen randomly. Some environments correspond to data sources, as in PatchCamelyon [4,22], where each environment comprises data collected at the same hospital. Arjovsky et al. [2] mention that environments act to \"reduce degrees of freedom in the space of invariant solutions\". Thus, more environments help discard spurious features during training [29]. The plethora of concepts available enables multiple ways of dividing the dataset into environments, some of which will be more successful than others at achieving robust representations.\n\nMany annotated concepts could be used for environment generation for skin lesion datasets. Recently, Daneshjou et al. [15] released a clinical skin lesion dataset presenting per-image specialist annotated information on Fitzpatrick skin types. Other metadata such as anatomical location, patient sex, and age are available for some datasets, such as the ISIC2019 [12]. In this work, we use the presence of artifacts in the image capture process to create the environment. The presence of those artifacts gives models the opportunity to exploit spurious correlations in order to shortcut learning [7,12,39]. We aim to prevent that, creating more robust models.\n\nThe 7 artifact types (see Table 1) may co-occur in lesions, with 2 7 = 128 combinations. Adding the binary class label, that gives 256 potential environments (e.g., benign with no artifacts, benign with dark-corners, malignant with darkcorners and rulers, etc.), although some of those may contain very few (or zero) images. We use non-empty environments to train a robust learning algorithm.\n\nOur risk minimization of choice is Group Distributionally Robust Optimization (GroupDRO) [30], a variation of DRO that includes more aggressive regularization, in the form of a hyperparameter to encourage fitting smaller groups, higher \u2113 2 regularization, and early stopping. It is a good fit due to our setting with many few-samples environments.\n\n\nNoiseCrop: Test-time Feature Selection\n\nThe last step in the pipeline is selecting robust features for inference. Recent work [8] shows that test-time feature selection yields considerable gains in performance, even when spurious correlations are learned.\n\nIn this step, we censor the input images' information to prevent models from using spurious features. We employ segmentation masks to separate foreground lesions, which host robust features, from background skin areas, which concentrate spurious information (e.g., skin tones, patches, and image artifacts).\n\nWe employ the ground-truth segmentation masks when available, and infer the segmentation (with a Deep Learning model [10]) when they are not. Since we post-process all masks through a convex hull operation, masks do not need to be pixel-perfect, instead they must roughly cover the whole lesion. To minimize the effect of the background pixels on the models, we replace them with a noisy background sampled uniformly from 0 to 255 in each RGB channel. We also eliminate lesion size information since the lack of scale guidelines for image capture makes size an unreliable feature subjected to spurious correlations. The convex hull of the segmentation mask is used to crop and re-scale the image such that lesion occupies the largest possible area while keeping the aspect ratio. We call those censoring procedures NoiseCrop (Fig. 2). Again, we stress, this censoring is applied only to test images. \n\n\nResults\n\n\nData\n\nWe employ several high-quality datasets in this study ( Table 2). The class labels are selected and grouped such that the task is always a binary classification of melanoma vs. benign (other, except for carcinomas). We removed from all analysis samples labeled basal cell carcinoma or squamous cell carcinoma. In the out-of-distribution test sets, we kept only samples labeled melanoma, nevus, and benign/seborrheic keratosis. The artifact annotations [7] comprise 7 types: dark corners (vignetting), hair, gel borders, gel bubbles, rulers, ink markings/staining, and patches applied to the patient skin. Ground-truth labels for those are available for the ISIC2018 [36] and Derm7pt [21]. For the larger ISIC2019, we infer those labels using independent binary per-artifact classifiers fine-tuned on the ISIC2018 annotations 5 .\n\n\nModel Selection and Implementation Details\n\nHyperparameter selection is crucial for DG. Following GroupDRO [30] protocol, we first performed a grid-search over learning rate (values 0.00001, 0.0001, 0.001), and weight-decay (0.001, 0.01, 0.1, 1.0), for 2 runs, on a validation set randomly split from the training set. Although GroupDRO suggests an unbiased (equal presence of all artifacts) validation set, we found such constraint unrealistic, since a perfectly unbiased data distribution is impossible to predict at training time. We follow the same hyperparameter search procedures for all techniques, including the baselines. Given the best combination on the validation set, we searched for GroupDRO's generalization adjustment argument among the values [0..5]. Sagawa et al. [30] added that hyperparameter to encourage fitting smaller groups. We provide, to illustrate an upper-bound of GroupDRO's performance, an oracle version whose hyperparameters were selected with privileged information from test time.\n\nAll models employ a ResNet-50 [17] backbone, fine-tuned for up to 100 epochs with SGD with momentum and patience of 22 epochs. Conventional data augmentation (shifts, rotations, color) is used on training and testing, with 50 replicas for the latter. On all plots, lines refer to the average of 10 runs, with shaded areas showing the standard error. Each run has a different training/validation partition and random seed.\n\n\nDebiasing of Skin Lesion Models\n\nThe trap set protocol partitions train and test in an intentional challenging way that is catastrophic for naive models. Models that exploit spurious correlations in the train \"fall in the trap\" resulting in very low performance (Table 3).\n\nERM achieves a ROC AUC of only 0.58, showing that trap sets successfully creates challenging biased train and test sets [7]. Our pipeline consider Group-DRO enabled by our artifact-based environments, followed by the application of NoiseCrop in test images. Debiased methods should produce solutions that are more invariant to the training bias, varying less from low to high bias scenarios. Our solution reaches 0.74 AUC in the most biased scenario, while the ERM baseline performs not much better than chance -a difference of 16 percentage points. Other robust methods that do not make use of environments (RSC and Bissoto et al. [7]) failed to improve over ERM. To the best of our knowledge, this is the first time debiasing solutions succeed for skin lesion analysis. Summary: Our pipeline is an effective strategy for debiasing, surpassing baselines and previous works by 16 percentage points in high-bias scenarios.\n\n\nAblation Study\n\nNext, we provide an ablation of our pipeline, individually evaluating the effects of the robust training enabled by our artifact-based environments and Noise-Crop. We consider increasingly high training biases to check the differences of performances in low and high biased scenarios. We show our results in Fig. 3: performances on the right inferred over NoiseCrop images, and without it (original images) on the left.\n\nArtifact-based GroupDRO GroupDRO increases the robustness to artifacts, yielding an improvement of around 10 percentage points in the AUC metric for high-bias scenarios. In such biased contexts, trap sets punish the model for relying on the artifacts, causing both ERM and RSC to fall under 0.6 AUC. In  low-bias scenarios, GroupDRO prevents models from relying on artifacts, causing the performance to drop compared to the baseline. When using privileged information to select hyperparameters for GroupDRO, our oracle reached 0.77 AUC. In the DG literature, deciding hyperparameters is a crucial step, and it is not uncommon to see methods completely fail when hyperparameters are chosen without privileged information over unbiased sets [1,16]. We believe that our fine-grained environments considering each possible combination of artifacts allowed for more robustness to hyperparameter decision.\n\nTest-time debiasing To complete our proposed pipeline (as Fig. 1), we perform feature selection on inference-time. Unlike the direction usually pursued in the literature [2,30], our debiasing method does not require altering any procedure during training. The idea is to select the features present in the image during test evaluation, forcing the network to use the correct correlations learned to make the prediction. In Fig. 3 (right), the scenario drastically changes when the same networks from the left of the figure are tested with NoiseCrop images, especially for the most biased scenario. The ERM model, which was slightly better than chance when classifying skin lesions with unchanged images, surpassed 0.72 AUC when evaluated with NoiseCrop images. Composing this test procedure alongside robust training methods further improves performance, achieving our best result. The reported harm in the performance for less biased scenarios can be illusory since exploiting biases naturally translates to better in-distribution performance but less generalization power. The steep increase in performance when using NoiseCrop test samples with the baseline model suggests that the network learns correct correlations even when training is heavily contaminated with spurious correlations, contrary to previous belief [28]. Still, we achieve our highest performance by using the debiasing procedure (through GroupDRO) and the NoiseCrop test. As in our pipeline, training and test-time debiasing are necessary to create more robust models. Test-time debiasing appears as a quick effective method to increase robustness at the cost of using domain knowledge of the task. The main challenge is to make test-time debiasing more general, relying less on existing annotations, such as the segmentation masks we use for skin lesion images. Summary: Artifact-based GroupDRO is an effective strategy for debiasing, and masking artifacts (spurious correlations) during test enable correct features to be used for inference. Our ablation suggests that models still learn robust predictive features even when trained on highly-biased data, but are ignored when known spurious correlations appear during test-time.\n\n\nOut-of-distribution Evaluation\n\nWe have previously shown the increased robustness of skin lesion analysis models when training with our artifact-based environments and NoiseCrop test samples. Now, we investigate the effect of the acquired robustness on out-of-distribution sets, which present different artifacts and attributes. Does robustness to the annotated artifacts cause models to rely more on robust features in general? We show our results in Fig. 4. The performances on out-of-distribution test sets are more stable than on trap tests across training biases. This is because trap-test contains opposite correlations from training, punishing the model for learning the encouraged spu-rious correlations. Still, PH2 and PAD-UFES-20 lines show slight negative and positive trends, respectively, indicating the presence and exploitation of biases.\n\nOur results show very noisy out-of-distribution performance according to the technique used. Our full pipeline present consistent advantage for PAD-UFES-20, while presenting lower performances at all other cases. Interestingly, when we skip NoiseCrop, using only GroupDRO for debiasing, we achieve positive results for all training biases in PH2, and for high training biases in Derm7pt Clinical. For Derm7pt-Dermato, the robust training procedure yielded no gains.\n\nThe differences between artifacts present in training (which are increasingly reinforced as training bias increases) and test may explain such irregular behavior. Analyzing the artifacts of each out-of-distribution test-set, we verified that the datasets most affected by the debiasing procedures reliably display a subset of the artifacts present on training. Specifically, PH2 presents dark corners, while PAD-UFES-20 display ink-markings. Derm7pt present rare cases of dark corners, and different style of rulers. Hair is the only artifact in all 4 test sets, while patches, and gel borders are absent in all sets. In Fig. 5, we show a selection of the artifacts from each considered out-of-distribution test-set. In such scenario, the models appear to learn to avoid known artifacts from training environments instead of learning to rely on clinically-relevant features. Another possible explanation for such variation is hinted by the overall low performance of NoiseCrop (except for PAD-UFES-20). There is a chance that the low performances are due to the domain shift introduced by the background noise, but this is unlikely since such shift did not affect our ISIC2019 experiments, where NoiseCrop reliably achieved our best performances. A more concerning and plausible explanation is that when censored of background information, models can not exploit other available sources of spurious correlations. Such spurious correlations are present in training and may even have very low correlations to the label. In addition, the natural distribution shift of correct features that happen in out-of-distribution sets, cause performances to drop. It is possible then, that the performance achieved by ERM and GroupDRO are overoptimistic. This shows the challenges of debiasing skin lesion models, agreeing with previous works [7] that suspected models combine weak correlations from several sources that may be hard to detect. For further advancing debiasing, future datasets must explicitly describe possible sources of spurious correlations [14]. Summary: When considering biased training scenarios, our proposed debiasing solutions surpassed baselines in 3 out of 4 test sets. Still, improvements depend on the similarity between the confounders used to partition environments and the ones present in test. Models fail when background is censored.\n\n\nQualitative Analysis\n\nTo inspect the effects from another angle, we used ScoreCAM [38] to create saliency maps 6 . We contrast our robust trained model with the ERM solution on the most biased scenario (training bias 1.0). In Fig. 6, we show cherry-picked malignant cases from the trap-test set that were misclassified by the ERM or Group-DRO, and that focused on an artifact. There are numerous samples in which the saliency maps indicate that ERM models focus on rulers. When trained with GroupDRO, models often correctly shift their attention to the lesion, causing the prediction to be correct. There are also cases where the baseline's attention correctly focuses on a lesion (even though the prediction is erroneous) and the robust model focuses on the artifact, but these are considerably less frequent. \n\n\nRelated Work\n\nArtifacts on skin lesion datasets. Artifacts affect skin-lesion-analysis models, which achieve a performance considerably higher than chance in images with the lesion fully occluded [6]. Generative models can amplify such biases [25]. Further investigation [7] analyzed the correlations between artifacts and labels, showing that, even with modest correlations, artifacts harmed performances. An analysis of the ISIC 2019 challenge [12] quantified the error rates of the top-ranked models when artifacts were present, finding that ink markings were particularly harmful for melanoma classification. Another work [14] recommended that future skin-lesion datasets describe artifacts and other potential confounders as metadata.\n\nEvaluation of generalization performance. Out-of-distribution performance must be measured in challenging protocols, whose craft is laborious requiring attention to class proportions, correlations to other objects, or to background colors, textures, and scenes (e.g., ObjectNet [5], ImageNet-A [18]). Partially or fully synthetic datasets (e.g., natural images on artificial backgrounds) allow fine control of the spurious correlationsand are often employed in more theoretical works, or as a first round of evaluations elsewhere [1,2]. An alternative to synthetic or handcrafted datasets is to employ naturally occurring environments (such as data source) and split the data holding out some environments exclusively for testing [16,34], e.g., in PatchCamelyon17-WILDS [4], one of the five source hospitals is used for testing, while the others are used for training.\n\nOur assessment scheme forgoes either synthetic data or splitting the sets by hand. Our scheme requires (ground-truth or inferred) annotations for potential bias sources (such as the artifacts we use in this work), but once those are available, the trap sets automatically amplify their effect by creating train and test splits with inverse correlations. The tunable trap sets proposed in this work allow controlling a level of bias.\n\nDebiasing medical imaging. Environment-dependent debiasing techniques seldom appear in the literature on medical image analysis. That is partly due to the lack of environment annotations, e.g., potential biasing attributes or artifacts. One way to see environments is through the lenses of causality, where they could be thought of as interventions in data [2]. Direct interventions in real-world data can be unfeasible or at least uncommon. For example, collecting the same image under different acquisition devices is uncommon if not for domain generalization purpose studies. Other types of shifts, such as the ones characterized by physical attributes, are impossible to intervene upon. It is impossible, for example, to see how a lesion on the face would be if it were in the palms and soles. Still, ideally, we would have enough environments to explain every source of noise in data, with slight differences between them. When environments are not annotated a priori, works develop mechanisms to create them. A common strategy is to assign whole data sources as environments [3,4,22]. However, when this strategy is successful, the different data sources (and environments) characterize only changes in a few aspects, such as the acquisition device. When differences across data sources are considerable, environments differ in many aspects simultaneously, harming debiasing performance. Other methods to generate environments rely on using differences in classes distributions [41], data augmentation procedures [9,42], or generative modeling [23]. After environments or domains are artificially generated through one of the techniques above, robust training use environments for feature alignment.\n\nIn our work, we use annotations of artifacts to create environments. Each environment presents a unique combination of artifact and label, yielding over 90 in training environments. Models trained with our environments successfully learned to avoid using artifacts for inference, improving performance in high-bias setups.\n\n\nConclusion\n\nDebiasing skin lesion models is possible. In this paper, we introduced a pipeline that enables bias generalization assessment without access to out-of-distribution sets, followed by a strategy to create environments from available metadata, and finally, a test-set debiasing procedure. We evaluated our pipeline using a large challenging training dataset and noisy (inferred) artifact annotations.\n\nOur findings suggest that domain generalization techniques, such as Group-DRO, can be employed for debiasing, as long as the environments represent spurious fine-grained differences, such as the presence of artifacts. Also, we showed that models learn a diverse set of features (spurious and robust), even in biased scenarios, and that removing spurious ones during test yields surprisingly good results without any training procedure changes. When we use training and testtime debiasing, we achieve our best result -GroupDRO enabled learning more robust features, while NoiseCrop allows using them during inference. For outof-distribution sets, the debiasing success depends on the similarity between the artifacts they display and those in training, used to partition environments. Despite potentially learning more robust features with GroupDRO, the presence of different artifacts and spurious correlations in test-time can still bias predictions.\n\nIn future work, we envision methods that are less reliant on labels for both environment partition and test-time debiasing. The domain generalization literature is evolving, proposing methods that learn to separate environments solely from data [1,13], but are still to see the same success of supervised approaches. Alternatively to test-time debiasing, methods for model editing [26,31] could enable practitioners to guide models from a few annotated images by making explicit the presence of artifacts and other spurious features.\n\nFig. 1 :\n1Our proposed pipeline for debiasing. Images with artifacts are represented by the drawings of rulers, dark corners, and the combination of both. White circles represent samples where artifacts are absent. Our first step (A) partitions data into challenging train and test sets, called Trap sets. The training set is divided into environments (B). Each environment groups samples containing the same set of artifacts. These environments are used to train a robust learning algorithm, such as GroupDRO. In the last step (C), we select features of our trap-test set, censuring the background which may provide spurious correlations.\n\nFig. 2 :\n2Comparison between Original and NoiseCrop images. In NoiseCrop, we remove the background information, replace it with a uniform noise, and resize the lesion to occupy the whole image.\n\nFig. 4 :\n4The different lines compare the ERM baseline, our environment-enabled GroupDRO, and our full pipeline. We train the models with increasingly high biased sets (trap train). We evaluate the performance on 4 out-of-distribution test sets comprising clinical and dermoscopic samples. Unlike the plots using trap test for evaluation, trends here are subtler. The debiasing procedure improves performances on PH2, PAD-UFES-20, and for biased models on Derm7pt-Clinical. On Derm7pt-Dermoscopic, baselines still perform better, despite all the bias in train.\n\nFig. 5 :\n5Artifacts from the out-of-distribution test sets. While (a) PH2 and (b) PAD-UFES-20 present similar artifacts to ISIC2019 (our training set), Derm7pt ((c) Clinical and (d) Dermoscopic) present different ones. We hypothesize this caused debiasing solutions to be more effective in PH2 and PAD-UFES-20.\n\nFig. 6 :\n6Qualitative analysis of malignant samples from the trap-test. We show three sets, each showing the original image followed by ScoreCam saliency maps of ERM and GroupDRO models (ours), in this order. Red (dashed) and blue (solid) borders mark wrong and correct predictions, respectively. In most scenarios, GroupDRO can shift the focus of the model from the artifact to the lesion (first two cases). However, there are still failure cases where the opposite happens (last case).\n\nTable 1 :\n1Spearman correlations between diagnostic and each of the 7 considered artifacts to build the trap sets. As the factor increase, so does the correlations and differences between train and test. set dark corner hair gel border gel bubble ruler ink patchesfactor 0 \ntrain \n0.119 \n-0.104 \n0.003 \n0.055 \n0.142 0.023 -0.138 \ntest \n0.135 \n-0.112 \n0.023 \n0.047 \n0.162 0.030 -0.149 \n\n0.5 train \n0.233 \n-0.185 \n0.083 \n0.052 \n0.246 0.048 -0.110 \ntest \n-0.129 \n0.083 \n-0.156 \n0.038 \n-0.074 -0.025 -0.217 \n\n1.0 train \n0.36 \n-0.282 \n0.178 \n0.056 \n0.352 0.096 -0.062 \ntest \n-0.438 \n0.296 \n-0.35 \n0.049 \n-0.335 -0.113 -0.319 \n\n\n\nTable 2 :\n2Datasets used in our workDataset \n# Samples Classes \nSet \nType \n\nISIC2019 (train) [11] \n12,360 melanoma vs. nevus, actinic \nkeratosis, benign keratosis, \ndermatofibroma, vascular lesion \n\ntraining \ndermoscopic \n\nISIC2019 (val) [11] \n2,060 as above \nvalidation dermoscopic \n\nISIC2019 (test) [11] \n6,182 as above \ntest \ndermoscopic \n\nPH2 [24] \n200 melanoma vs. nevus, benign \nkeratosis \n\ntest \ndermoscopic \n\nDerm7pt-\nDermoscopic [21] \n\n872 as above \ntest \ndermoscopic \n\nDerm7pt-Clinical [21] \n839 as above \ntest \nclinical \n\nPAD-UFES-20 [27] \n531 as above \ntest \nclinical \n\n\n\nTable 3 :\n3Results for different pipelines on a strong trap test (training bias = 1). Our results considerably surpass the state of the art in that scenario. \u2020Reported from the original, using a ResNet-152 model on the ISIC2018 dataset.Method \nROC AUC \n\nERM [37] \n0.58 \nRSC [20] \n0.59 \nBissoto et al. [7]  \u2020 \n0.54 \nGroupDRO (Ours) \n0.68 \nFull Pipeline (Ours) \n0.74 \n\n\nEach model is an ImageNet-pretrained Inceptionv4[35] fine tuned with stochastic gradient descent, with momentum 0.9, weight decay 10 \u22123 , and learning rate 10 \u22123 , reduced to 10 \u22124 after epoch 25. Batch size is 32, with reshuffling before each epoch. Data augmented with random crops, rotations, flips, and color transformations.\nTo minimize stochastic effects in the saliency maps, we compare models trained with the same random seed.\nAcknowledgmentsA. Bissoto is funded by FAPESP 2019/19619-7. C. Barata is funded by the FCT projects LARSyS (UID/50009/2020) and CEECIND/00326/2017. E. Valle is partially funded by CNPq 315168/2020-0. S. Avila is partially funded by CNPq 315231/2020-3, FAPESP 2013/08293-7, 2020/09838-0, and Google LARA 2021. The Recod.ai lab is supported by projects from FAPESP, CNPq, and CAPES.\nSystematic generalisation with group invariant predictions. F Ahmed, Y Bengio, H Van Seijen, A Courville, International Conference on Learning Representations (ICLR). Ahmed, F., Bengio, Y., van Seijen, H., Courville, A.: Systematic generalisation with group invariant predictions. In: International Conference on Learning Repre- sentations (ICLR) (2021)\n\nInvariant risk minimization. M Arjovsky, L Bottou, I Gulrajani, D Lopez-Paz, arXiv:1907.02893Arjovsky, M., Bottou, L., Gulrajani, I., Lopez-Paz, D.: Invariant risk minimization. arXiv:1907.02893 (2019)\n\nM Aubreville, N Stathonikos, C A Bertram, R Klopleisch, N Ter Hoeve, F Ciompi, F Wilm, C Marzahl, T A Donovan, A Maier, arXiv:2204.03742Mitosis domain generalization in histopathology images-the midog challenge. arXiv preprintAubreville, M., Stathonikos, N., Bertram, C.A., Klopleisch, R., ter Hoeve, N., Ciompi, F., Wilm, F., Marzahl, C., Donovan, T.A., Maier, A., et al.: Mitosis do- main generalization in histopathology images-the midog challenge. arXiv preprint arXiv:2204.03742 (2022)\n\nFrom detection of individual metastases to classification of lymph node status at the patient level: the came-lyon17 challenge. P Bandi, O Geessink, Q Manson, M Van Dijk, M Balkenhol, M Hermsen, B E Bejnordi, B Lee, K Paeng, A Zhong, IEEE Transactions on Medical Imaging. 382Bandi, P., Geessink, O., Manson, Q., Van Dijk, M., Balkenhol, M., Hermsen, M., Bejnordi, B.E., Lee, B., Paeng, K., Zhong, A., et al.: From detection of individual metastases to classification of lymph node status at the patient level: the came- lyon17 challenge. IEEE Transactions on Medical Imaging 38(2), 550-560 (2018)\n\nObjectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. A Barbu, D Mayo, J Alverio, W Luo, C Wang, D Gutfreund, J Tenenbaum, B Katz, Advances in Neural Information Processing Systems (NeurIPS). Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., Katz, B.: Objectnet: A large-scale bias-controlled dataset for pushing the lim- its of object recognition models. In: Advances in Neural Information Processing Systems (NeurIPS) (2019)\n\nDe)Constructing bias on skin lesion datasets. A Bissoto, M Fornaciali, E Valle, S Avila, IEEE Conference on Computer Vision and Pattern Recognition Workshops. CVPRWBissoto, A., Fornaciali, M., Valle, E., Avila, S.: (De)Constructing bias on skin lesion datasets. In: IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2019)\n\nDebiasing skin lesion datasets and models? not so fast. A Bissoto, E Valle, S Avila, IEEE Conference on Computer Vision and Pattern Recognition Workshops. CVPRWBissoto, A., Valle, E., Avila, S.: Debiasing skin lesion datasets and models? not so fast. In: IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2020)\n\nContemplating real-world object classification. A Borji, International Conference on Learning Representations (ICLR. Borji, A.: Contemplating real-world object classification. In: International Confer- ence on Learning Representations (ICLR) (2021)\n\nStain mix-up: Unsupervised domain generalization for histopathology images. J R Chang, M S Wu, W H Yu, C C Chen, C K Yang, Y Y Lin, C Y Yeh, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). Chang, J.R., Wu, M.S., Yu, W.H., Chen, C.C., Yang, C.K., Lin, Y.Y., Yeh, C.Y.: Stain mix-up: Unsupervised domain generalization for histopathology images. In: International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) (2021)\n\nEncoder-decoder with atrous separable convolution for semantic image segmentation. L C Chen, Y Zhu, G Papandreou, F Schroff, H Adam, European Conference on Computer Vision (ECCV). Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H.: Encoder-decoder with atrous separable convolution for semantic image segmentation. In: European Conference on Computer Vision (ECCV) (2018)\n\nSkin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic). N Codella, V Rotemberg, P Tschandl, M E Celebi, S Dusza, D Gutman, B Helba, A Kalloo, K Liopyris, M Marchetti, arXiv:1902.03368arXiv preprintCodella, N., Rotemberg, V., Tschandl, P., Celebi, M.E., Dusza, S., Gutman, D., Helba, B., Kalloo, A., Liopyris, K., Marchetti, M., et al.: Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368 (2019)\n\nValidation of artificial intelligence prediction models for skin cancer diagnosis using dermoscopy images: the 2019 international skin imaging collaboration grand challenge. M Combalia, N Codella, V Rotemberg, C Carrera, S Dusza, D Gutman, B Helba, H Kittler, N R Kurtansky, K Liopyris, The Lancet Digital Health. 45Combalia, M., Codella, N., Rotemberg, V., Carrera, C., Dusza, S., Gutman, D., Helba, B., Kittler, H., Kurtansky, N.R., Liopyris, K., et al.: Validation of artificial intelligence prediction models for skin cancer diagnosis using dermoscopy images: the 2019 international skin imaging collaboration grand challenge. The Lancet Digital Health 4(5), e330-e339 (2022)\n\nEnvironment inference for invariant learning. E Creager, J H Jacobsen, R Zemel, International Conference on Machine Learning (ICML). Creager, E., Jacobsen, J.H., Zemel, R.: Environment inference for invariant learn- ing. In: International Conference on Machine Learning (ICML) (2021)\n\nChecklist for evaluation of image-based artificial intelligence reports in dermatology: Clear derm consensus guidelines from the international skin imaging collaboration artificial intelligence working group. R Daneshjou, C Barata, B Betz-Stablein, M E Celebi, N Codella, M Combalia, P Guitera, D Gutman, A Halpern, B Helba, JAMA dermatology. 1581Daneshjou, R., Barata, C., Betz-Stablein, B., Celebi, M.E., Codella, N., Combalia, M., Guitera, P., Gutman, D., Halpern, A., Helba, B., et al.: Checklist for evaluation of image-based artificial intelligence reports in dermatology: Clear derm consensus guidelines from the international skin imaging collaboration artificial intelligence working group. JAMA dermatology 158(1), 90-96 (2022)\n\nR Daneshjou, K Vodrahalli, W Liang, R A Novoa, M Jenkins, V Rotemberg, J Ko, S M Swetter, E E Bailey, O Gevaert, arXiv:2111.08006Disparities in dermatology ai: Assessments using diverse clinical images. arXiv preprintDaneshjou, R., Vodrahalli, K., Liang, W., Novoa, R.A., Jenkins, M., Rotemberg, V., Ko, J., Swetter, S.M., Bailey, E.E., Gevaert, O., et al.: Disparities in dermatol- ogy ai: Assessments using diverse clinical images. arXiv preprint arXiv:2111.08006 (2021)\n\nIn search of lost domain generalization. I Gulrajani, D Lopez-Paz, International Conference on Learning Representations (ICLR. Gulrajani, I., Lopez-Paz, D.: In search of lost domain generalization. In: Interna- tional Conference on Learning Representations (ICLR) (2021)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016)\n\nNatural adversarial examples. D Hendrycks, K Zhao, S Basart, J Steinhardt, D Song, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., Song, D.: Natural adversar- ial examples. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)\n\nDoes distributionally robust supervised learning give robust classifiers?. W Hu, G Niu, I Sato, M Sugiyama, International Conference on Machine Learning (ICML). Hu, W., Niu, G., Sato, I., Sugiyama, M.: Does distributionally robust supervised learning give robust classifiers? In: International Conference on Machine Learning (ICML) (2018)\n\nSelf-challenging improves crossdomain generalization. Z Huang, H Wang, E P Xing, D Huang, European Conference on Computer Vision (ECCV. Huang, Z., Wang, H., Xing, E.P., Huang, D.: Self-challenging improves cross- domain generalization. In: European Conference on Computer Vision (ECCV) (2020)\n\nSeven-point checklist and skin lesion classification using multitask multimodal neural nets. J Kawahara, S Daneshvar, G Argenziano, G Hamarneh, IEEE Journal of Biomedical and Health Informatics. 232Kawahara, J., Daneshvar, S., Argenziano, G., Hamarneh, G.: Seven-point checklist and skin lesion classification using multitask multimodal neural nets. IEEE Journal of Biomedical and Health Informatics 23(2), 538-546 (2019)\n\nP W Koh, S Sagawa, H Marklund, S M Xie, M Zhang, A Balsubramani, W Hu, M Yasunaga, R L Phillips, S Beery, arXiv:2012.07421Wilds: A benchmark of in-the-wild distribution shifts. Koh, P.W., Sagawa, S., Marklund, H., Xie, S.M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R.L., Beery, S., et al.: Wilds: A benchmark of in-the-wild distribution shifts. arXiv:2012.07421 (2020)\n\nDomain generalization in restoration of cataract fundus images via high-frequency components. H Liu, H Li, M Ou, Y Zhao, H Qi, Y Hu, J Liu, International Symposium on Biomedical Imaging (ISBI). Liu, H., Li, H., Ou, M., Zhao, Y., Qi, H., Hu, Y., Liu, J.: Domain generaliza- tion in restoration of cataract fundus images via high-frequency components. In: International Symposium on Biomedical Imaging (ISBI) (2022)\n\nPh2: A public database for the analysis of dermoscopic images. T Mendon\u00e7a, M Celebi, T Mendonca, J Marques, Dermoscopy image analysisMendon\u00e7a, T., Celebi, M., Mendonca, T., Marques, J.: Ph2: A public database for the analysis of dermoscopic images. Dermoscopy image analysis (2015)\n\nThe (de) biasing effect of ganbased augmentation methods on skin lesion images. A Miko\u0142ajczyk, S Majchrowska, S C Limeros, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). Miko\u0142ajczyk, A., Majchrowska, S., Limeros, S.C.: The (de) biasing effect of gan- based augmentation methods on skin lesion images. In: International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) (2022)\n\nFast model editing at scale. E Mitchell, C Lin, A Bosselut, C Finn, C D Manning, International Conference on Learning Representations (ICLR. Mitchell, E., Lin, C., Bosselut, A., Finn, C., Manning, C.D.: Fast model editing at scale. In: International Conference on Learning Representations (ICLR) (2022)\n\nPad-ufes-20: A skin lesion dataset composed of patient data and clinical images collected from smartphones. A G Pacheco, G R Lima, A S Salom\u00e3o, B Krohling, I P Biral, G G De Angelo, F C AlvesJr, J G Esgario, A C Simora, P B Castro, 32Pacheco, A.G., Lima, G.R., Salom\u00e3o, A.S., Krohling, B., Biral, I.P., de Angelo, G.G., Alves Jr, F.C., Esgario, J.G., Simora, A.C., Castro, P.B., et al.: Pad-ufes-20: A skin lesion dataset composed of patient data and clinical images collected from smartphones. Data in brief 32 (2020)\n\nGradient starvation: A learning proclivity in neural networks. M Pezeshki, O Kaba, Y Bengio, A C Courville, D Precup, G Lajoie, Advances in Neural Information Processing Systems (NeurIPS). Pezeshki, M., Kaba, O., Bengio, Y., Courville, A.C., Precup, D., Lajoie, G.: Gra- dient starvation: A learning proclivity in neural networks. Advances in Neural Information Processing Systems (NeurIPS) (2021)\n\nThe risks of invariant risk minimization. E Rosenfeld, P Ravikumar, A Risteski, International Conference on Learning Representations (ICLR. Rosenfeld, E., Ravikumar, P., Risteski, A.: The risks of invariant risk minimization. In: International Conference on Learning Representations (ICLR) (2021)\n\nDistributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. S Sagawa, P W Koh, T B Hashimoto, P Liang, International Conference on Learning Representations (ICLR. Sagawa, S., Koh, P.W., Hashimoto, T.B., Liang, P.: Distributionally robust neu- ral networks for group shifts: On the importance of regularization for worst-case generalization. In: International Conference on Learning Representations (ICLR) (2020)\n\nEditing a classifier by rewriting its prediction rules. S Santurkar, D Tsipras, M Elango, D Bau, A Torralba, A Madry, Advances in Neural Information Processing Systems (NeurIPS). Santurkar, S., Tsipras, D., Elango, M., Bau, D., Torralba, A., Madry, A.: Editing a classifier by rewriting its prediction rules. In: Advances in Neural Information Processing Systems (NeurIPS) (2021)\n\nH Shah, K Tamuly, A Raghunathan, P Jain, P Netrapalli, The pitfalls of simplicity bias in neural networks. In: Advances in Neural Information Processing Systems (NeurIPS). Shah, H., Tamuly, K., Raghunathan, A., Jain, P., Netrapalli, P.: The pitfalls of simplicity bias in neural networks. In: Advances in Neural Information Processing Systems (NeurIPS) (2020)\n\nZ Shen, J Liu, Y He, X Zhang, R Xu, H Yu, P Cui, arXiv:2108.13624Towards out-ofdistribution generalization: A survey. Shen, Z., Liu, J., He, Y., Zhang, X., Xu, R., Yu, H., Cui, P.: Towards out-of- distribution generalization: A survey. arXiv:2108.13624 (2021)\n\nAn investigation of critical issues in bias mitigation techniques. R Shrestha, K Kafle, C Kanan, IEEE Winter Conference on Applications of Computer Vision (WACV). Shrestha, R., Kafle, K., Kanan, C.: An investigation of critical issues in bias mitiga- tion techniques. In: IEEE Winter Conference on Applications of Computer Vision (WACV) (2022)\n\nInception-v4, inception-resnet and the impact of residual connections on learning. C Szegedy, S Ioffe, V Vanhoucke, A A Alemi, AAAI Conference on Artificial Intelligence (AAAI. Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A.: Inception-v4, inception-resnet and the impact of residual connections on learning. In: AAAI Conference on Arti- ficial Intelligence (AAAI) (2017)\n\nThe ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. P Tschandl, C Rosendahl, H Kittler, Scientific data. 51Tschandl, P., Rosendahl, C., Kittler, H.: The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data 5(1), 1-9 (2018)\n\nPrinciples of risk minimization for learning theory. V Vapnik, Advances in Neural Information Processing Systems (NeurIPS). Vapnik, V.: Principles of risk minimization for learning theory. In: Advances in Neural Information Processing Systems (NeurIPS) (1992)\n\nScore-cam: Score-weighted visual explanations for convolutional neural networks. H Wang, Z Wang, M Du, F Yang, Z Zhang, S Ding, P Mardziel, X Hu, IEEE Conference on Computer Vision and Pattern Recognition Workshops. CVPRWWang, H., Wang, Z., Du, M., Yang, F., Zhang, Z., Ding, S., Mardziel, P., Hu, X.: Score-cam: Score-weighted visual explanations for convolutional neural networks. In: IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (2020)\n\nAssociation between surgical skin markings in dermoscopic images and diagnostic performance of a deep learning convolutional neural network for melanoma recognition. J Winkler, C Fink, F Toberer, A Enk, T Deinlein, R Hofmann-Wellenhof, L Thomas, A Lallas, A Blum, W Stolz, JAMA Dermatology. 15510Winkler, J., Fink, C., Toberer, F., Enk, A., Deinlein, T., Hofmann-Wellenhof, R., Thomas, L., Lallas, A., Blum, A., Stolz, W., et al.: Association between surgical skin markings in dermoscopic images and diagnostic performance of a deep learn- ing convolutional neural network for melanoma recognition. JAMA Dermatology 155(10), 1135-1141 (2019)\n\nOod-bench: Quantifying and understanding two dimensions of out-of-distribution generalization. N Ye, K Li, H Bai, R Yu, L Hong, F Zhou, Z Li, J Zhu, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Ye, N., Li, K., Bai, H., Yu, R., Hong, L., Zhou, F., Li, Z., Zhu, J.: Ood-bench: Quantifying and understanding two dimensions of out-of-distribution generaliza- tion. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2022)\n\nGeneralizable feature learning in the presence of data bias and domain class imbalance with application to skin lesion classification. C Yoon, G Hamarneh, R Garbi, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). Yoon, C., Hamarneh, G., Garbi, R.: Generalizable feature learning in the presence of data bias and domain class imbalance with application to skin lesion classifica- tion. In: International Conference on Medical Image Computing and Computer- Assisted Intervention (MICCAI) (2019)\n\nSemi-supervised domain generalization for medical image analysis. R Zhang, Q Xu, C Huang, Y Zhang, Y Wang, International Symposium on Biomedical Imaging (ISBI). Zhang, R., Xu, Q., Huang, C., Zhang, Y., Wang, Y.: Semi-supervised domain gen- eralization for medical image analysis. In: International Symposium on Biomedical Imaging (ISBI) (2022)\n\nK Zhou, Z Liu, Y Qiao, T Xiang, C C Loy, arXiv:2103.02503Domain generalization: A survey. Zhou, K., Liu, Z., Qiao, Y., Xiang, T., Loy, C.C.: Domain generalization: A survey. arXiv:2103.02503 (2021)\n", "annotations": {"author": "[{\"end\":116,\"start\":62},{\"end\":189,\"start\":117},{\"end\":268,\"start\":190},{\"end\":313,\"start\":269}]", "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": "[{\"end\":115,\"start\":63},{\"end\":188,\"start\":118},{\"end\":267,\"start\":191},{\"end\":312,\"start\":270}]", "title": "[{\"end\":59,\"start\":1},{\"end\":372,\"start\":314}]", "venue": null, "abstract": "[{\"end\":1889,\"start\":425}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2254,\"start\":2251},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2383,\"start\":2380},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3773,\"start\":3770},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5225,\"start\":5221},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5228,\"start\":5225},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5398,\"start\":5394},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5925,\"start\":5921},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5928,\"start\":5925},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6988,\"start\":6984},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7503,\"start\":7499},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7710,\"start\":7706},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7819,\"start\":7816},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7822,\"start\":7819},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8281,\"start\":8277},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8614,\"start\":8611},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8728,\"start\":8725},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9601,\"start\":9598},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9813,\"start\":9810},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9816,\"start\":9813},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9907,\"start\":9904},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10081,\"start\":10077},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10393,\"start\":10389},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10638,\"start\":10634},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10870,\"start\":10867},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10873,\"start\":10870},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10876,\"start\":10873},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11419,\"start\":11415},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11805,\"start\":11802},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12363,\"start\":12359},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13616,\"start\":13613},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13831,\"start\":13827},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13848,\"start\":13844},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14103,\"start\":14099},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14778,\"start\":14774},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15043,\"start\":15039},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15830,\"start\":15827},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16342,\"start\":16339},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17810,\"start\":17807},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17813,\"start\":17810},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18142,\"start\":18139},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18145,\"start\":18142},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19293,\"start\":19289},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23330,\"start\":23327},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23548,\"start\":23544},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23940,\"start\":23936},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24867,\"start\":24864},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24915,\"start\":24911},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24942,\"start\":24939},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25118,\"start\":25114},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25298,\"start\":25294},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25690,\"start\":25687},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25707,\"start\":25703},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25942,\"start\":25939},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25944,\"start\":25942},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26143,\"start\":26139},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26146,\"start\":26143},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26182,\"start\":26179},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27073,\"start\":27070},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27797,\"start\":27794},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27799,\"start\":27797},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27802,\"start\":27799},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28201,\"start\":28197},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28235,\"start\":28232},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28238,\"start\":28235},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28267,\"start\":28263},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30357,\"start\":30354},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30360,\"start\":30357},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30494,\"start\":30490},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30497,\"start\":30494},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34471,\"start\":34467}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":31283,\"start\":30643},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31478,\"start\":31284},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32040,\"start\":31479},{\"attributes\":{\"id\":\"fig_5\"},\"end\":32352,\"start\":32041},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32841,\"start\":32353},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33465,\"start\":32842},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34049,\"start\":33466},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34418,\"start\":34050}]", "paragraph": "[{\"end\":2081,\"start\":1905},{\"end\":2678,\"start\":2083},{\"end\":3610,\"start\":2680},{\"end\":4267,\"start\":3612},{\"end\":4296,\"start\":4269},{\"end\":4724,\"start\":4298},{\"end\":4972,\"start\":4739},{\"end\":5316,\"start\":4974},{\"end\":5878,\"start\":5318},{\"end\":6180,\"start\":5880},{\"end\":6944,\"start\":6226},{\"end\":7823,\"start\":6946},{\"end\":8359,\"start\":7839},{\"end\":9471,\"start\":8373},{\"end\":10269,\"start\":9503},{\"end\":10930,\"start\":10271},{\"end\":11324,\"start\":10932},{\"end\":11673,\"start\":11326},{\"end\":11931,\"start\":11716},{\"end\":12240,\"start\":11933},{\"end\":13142,\"start\":12242},{\"end\":13989,\"start\":13161},{\"end\":15007,\"start\":14036},{\"end\":15430,\"start\":15009},{\"end\":15705,\"start\":15466},{\"end\":16628,\"start\":15707},{\"end\":17066,\"start\":16647},{\"end\":17967,\"start\":17068},{\"end\":20172,\"start\":17969},{\"end\":21028,\"start\":20207},{\"end\":21495,\"start\":21030},{\"end\":23851,\"start\":21497},{\"end\":24665,\"start\":23876},{\"end\":25407,\"start\":24682},{\"end\":26277,\"start\":25409},{\"end\":26711,\"start\":26279},{\"end\":28418,\"start\":26713},{\"end\":28742,\"start\":28420},{\"end\":29154,\"start\":28757},{\"end\":30107,\"start\":29156},{\"end\":30642,\"start\":30109}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6225,\"start\":6181}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9166,\"start\":9159},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10965,\"start\":10958},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13224,\"start\":13217},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15704,\"start\":15695}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1903,\"start\":1891},{\"attributes\":{\"n\":\"2\"},\"end\":4737,\"start\":4727},{\"attributes\":{\"n\":\"3\"},\"end\":7837,\"start\":7826},{\"attributes\":{\"n\":\"3.1\"},\"end\":8371,\"start\":8362},{\"attributes\":{\"n\":\"3.2\"},\"end\":9501,\"start\":9474},{\"attributes\":{\"n\":\"3.3\"},\"end\":11714,\"start\":11676},{\"attributes\":{\"n\":\"4\"},\"end\":13152,\"start\":13145},{\"attributes\":{\"n\":\"4.1\"},\"end\":13159,\"start\":13155},{\"attributes\":{\"n\":\"4.2\"},\"end\":14034,\"start\":13992},{\"attributes\":{\"n\":\"4.3\"},\"end\":15464,\"start\":15433},{\"attributes\":{\"n\":\"4.4\"},\"end\":16645,\"start\":16631},{\"attributes\":{\"n\":\"4.5\"},\"end\":20205,\"start\":20175},{\"attributes\":{\"n\":\"4.6\"},\"end\":23874,\"start\":23854},{\"attributes\":{\"n\":\"5\"},\"end\":24680,\"start\":24668},{\"attributes\":{\"n\":\"6\"},\"end\":28755,\"start\":28745},{\"end\":30652,\"start\":30644},{\"end\":31293,\"start\":31285},{\"end\":31488,\"start\":31480},{\"end\":32050,\"start\":32042},{\"end\":32362,\"start\":32354},{\"end\":32852,\"start\":32843},{\"end\":33476,\"start\":33467},{\"end\":34060,\"start\":34051}]", "table": "[{\"end\":33465,\"start\":33107},{\"end\":34049,\"start\":33503},{\"end\":34418,\"start\":34287}]", "figure_caption": "[{\"end\":31283,\"start\":30654},{\"end\":31478,\"start\":31295},{\"end\":32040,\"start\":31490},{\"end\":32352,\"start\":32052},{\"end\":32841,\"start\":32364},{\"end\":33107,\"start\":32854},{\"end\":33503,\"start\":33478},{\"end\":34287,\"start\":34062}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8058,\"start\":8052},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13075,\"start\":13067},{\"end\":16961,\"start\":16955},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18033,\"start\":18027},{\"end\":18406,\"start\":18392},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20633,\"start\":20627},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22124,\"start\":22118},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24086,\"start\":24080}]", "bib_author_first_name": "[{\"end\":35297,\"start\":35296},{\"end\":35306,\"start\":35305},{\"end\":35316,\"start\":35315},{\"end\":35330,\"start\":35329},{\"end\":35621,\"start\":35620},{\"end\":35633,\"start\":35632},{\"end\":35643,\"start\":35642},{\"end\":35656,\"start\":35655},{\"end\":35795,\"start\":35794},{\"end\":35809,\"start\":35808},{\"end\":35824,\"start\":35823},{\"end\":35826,\"start\":35825},{\"end\":35837,\"start\":35836},{\"end\":35851,\"start\":35850},{\"end\":35864,\"start\":35863},{\"end\":35874,\"start\":35873},{\"end\":35882,\"start\":35881},{\"end\":35893,\"start\":35892},{\"end\":35895,\"start\":35894},{\"end\":35906,\"start\":35905},{\"end\":36415,\"start\":36414},{\"end\":36424,\"start\":36423},{\"end\":36436,\"start\":36435},{\"end\":36446,\"start\":36445},{\"end\":36458,\"start\":36457},{\"end\":36471,\"start\":36470},{\"end\":36482,\"start\":36481},{\"end\":36484,\"start\":36483},{\"end\":36496,\"start\":36495},{\"end\":36503,\"start\":36502},{\"end\":36512,\"start\":36511},{\"end\":36987,\"start\":36986},{\"end\":36996,\"start\":36995},{\"end\":37004,\"start\":37003},{\"end\":37015,\"start\":37014},{\"end\":37022,\"start\":37021},{\"end\":37030,\"start\":37029},{\"end\":37043,\"start\":37042},{\"end\":37056,\"start\":37055},{\"end\":37440,\"start\":37439},{\"end\":37451,\"start\":37450},{\"end\":37465,\"start\":37464},{\"end\":37474,\"start\":37473},{\"end\":37801,\"start\":37800},{\"end\":37812,\"start\":37811},{\"end\":37821,\"start\":37820},{\"end\":38133,\"start\":38132},{\"end\":38411,\"start\":38410},{\"end\":38413,\"start\":38412},{\"end\":38422,\"start\":38421},{\"end\":38424,\"start\":38423},{\"end\":38430,\"start\":38429},{\"end\":38432,\"start\":38431},{\"end\":38438,\"start\":38437},{\"end\":38440,\"start\":38439},{\"end\":38448,\"start\":38447},{\"end\":38450,\"start\":38449},{\"end\":38458,\"start\":38457},{\"end\":38460,\"start\":38459},{\"end\":38467,\"start\":38466},{\"end\":38469,\"start\":38468},{\"end\":38919,\"start\":38918},{\"end\":38921,\"start\":38920},{\"end\":38929,\"start\":38928},{\"end\":38936,\"start\":38935},{\"end\":38950,\"start\":38949},{\"end\":38961,\"start\":38960},{\"end\":39345,\"start\":39344},{\"end\":39356,\"start\":39355},{\"end\":39369,\"start\":39368},{\"end\":39381,\"start\":39380},{\"end\":39383,\"start\":39382},{\"end\":39393,\"start\":39392},{\"end\":39402,\"start\":39401},{\"end\":39412,\"start\":39411},{\"end\":39421,\"start\":39420},{\"end\":39431,\"start\":39430},{\"end\":39443,\"start\":39442},{\"end\":39967,\"start\":39966},{\"end\":39979,\"start\":39978},{\"end\":39990,\"start\":39989},{\"end\":40003,\"start\":40002},{\"end\":40014,\"start\":40013},{\"end\":40023,\"start\":40022},{\"end\":40033,\"start\":40032},{\"end\":40042,\"start\":40041},{\"end\":40053,\"start\":40052},{\"end\":40055,\"start\":40054},{\"end\":40068,\"start\":40067},{\"end\":40520,\"start\":40519},{\"end\":40531,\"start\":40530},{\"end\":40533,\"start\":40532},{\"end\":40545,\"start\":40544},{\"end\":40968,\"start\":40967},{\"end\":40981,\"start\":40980},{\"end\":40991,\"start\":40990},{\"end\":41008,\"start\":41007},{\"end\":41010,\"start\":41009},{\"end\":41020,\"start\":41019},{\"end\":41031,\"start\":41030},{\"end\":41043,\"start\":41042},{\"end\":41054,\"start\":41053},{\"end\":41064,\"start\":41063},{\"end\":41075,\"start\":41074},{\"end\":41498,\"start\":41497},{\"end\":41511,\"start\":41510},{\"end\":41525,\"start\":41524},{\"end\":41534,\"start\":41533},{\"end\":41536,\"start\":41535},{\"end\":41545,\"start\":41544},{\"end\":41556,\"start\":41555},{\"end\":41569,\"start\":41568},{\"end\":41575,\"start\":41574},{\"end\":41577,\"start\":41576},{\"end\":41588,\"start\":41587},{\"end\":41590,\"start\":41589},{\"end\":41600,\"start\":41599},{\"end\":42013,\"start\":42012},{\"end\":42026,\"start\":42025},{\"end\":42290,\"start\":42289},{\"end\":42296,\"start\":42295},{\"end\":42305,\"start\":42304},{\"end\":42312,\"start\":42311},{\"end\":42577,\"start\":42576},{\"end\":42590,\"start\":42589},{\"end\":42598,\"start\":42597},{\"end\":42608,\"start\":42607},{\"end\":42622,\"start\":42621},{\"end\":42945,\"start\":42944},{\"end\":42951,\"start\":42950},{\"end\":42958,\"start\":42957},{\"end\":42966,\"start\":42965},{\"end\":43264,\"start\":43263},{\"end\":43273,\"start\":43272},{\"end\":43281,\"start\":43280},{\"end\":43283,\"start\":43282},{\"end\":43291,\"start\":43290},{\"end\":43597,\"start\":43596},{\"end\":43609,\"start\":43608},{\"end\":43622,\"start\":43621},{\"end\":43636,\"start\":43635},{\"end\":43927,\"start\":43926},{\"end\":43929,\"start\":43928},{\"end\":43936,\"start\":43935},{\"end\":43946,\"start\":43945},{\"end\":43958,\"start\":43957},{\"end\":43960,\"start\":43959},{\"end\":43967,\"start\":43966},{\"end\":43976,\"start\":43975},{\"end\":43992,\"start\":43991},{\"end\":43998,\"start\":43997},{\"end\":44010,\"start\":44009},{\"end\":44012,\"start\":44011},{\"end\":44024,\"start\":44023},{\"end\":44412,\"start\":44411},{\"end\":44419,\"start\":44418},{\"end\":44425,\"start\":44424},{\"end\":44431,\"start\":44430},{\"end\":44439,\"start\":44438},{\"end\":44445,\"start\":44444},{\"end\":44451,\"start\":44450},{\"end\":44796,\"start\":44795},{\"end\":44808,\"start\":44807},{\"end\":44818,\"start\":44817},{\"end\":44830,\"start\":44829},{\"end\":45096,\"start\":45095},{\"end\":45111,\"start\":45110},{\"end\":45126,\"start\":45125},{\"end\":45128,\"start\":45127},{\"end\":45504,\"start\":45503},{\"end\":45516,\"start\":45515},{\"end\":45523,\"start\":45522},{\"end\":45535,\"start\":45534},{\"end\":45543,\"start\":45542},{\"end\":45545,\"start\":45544},{\"end\":45887,\"start\":45886},{\"end\":45889,\"start\":45888},{\"end\":45900,\"start\":45899},{\"end\":45902,\"start\":45901},{\"end\":45910,\"start\":45909},{\"end\":45912,\"start\":45911},{\"end\":45923,\"start\":45922},{\"end\":45935,\"start\":45934},{\"end\":45937,\"start\":45936},{\"end\":45946,\"start\":45945},{\"end\":45948,\"start\":45947},{\"end\":45961,\"start\":45960},{\"end\":45963,\"start\":45962},{\"end\":45974,\"start\":45973},{\"end\":45976,\"start\":45975},{\"end\":45987,\"start\":45986},{\"end\":45989,\"start\":45988},{\"end\":45999,\"start\":45998},{\"end\":46001,\"start\":46000},{\"end\":46362,\"start\":46361},{\"end\":46374,\"start\":46373},{\"end\":46382,\"start\":46381},{\"end\":46392,\"start\":46391},{\"end\":46394,\"start\":46393},{\"end\":46407,\"start\":46406},{\"end\":46417,\"start\":46416},{\"end\":46740,\"start\":46739},{\"end\":46753,\"start\":46752},{\"end\":46766,\"start\":46765},{\"end\":47121,\"start\":47120},{\"end\":47131,\"start\":47130},{\"end\":47133,\"start\":47132},{\"end\":47140,\"start\":47139},{\"end\":47142,\"start\":47141},{\"end\":47155,\"start\":47154},{\"end\":47530,\"start\":47529},{\"end\":47543,\"start\":47542},{\"end\":47554,\"start\":47553},{\"end\":47564,\"start\":47563},{\"end\":47571,\"start\":47570},{\"end\":47583,\"start\":47582},{\"end\":47855,\"start\":47854},{\"end\":47863,\"start\":47862},{\"end\":47873,\"start\":47872},{\"end\":47888,\"start\":47887},{\"end\":47896,\"start\":47895},{\"end\":48216,\"start\":48215},{\"end\":48224,\"start\":48223},{\"end\":48231,\"start\":48230},{\"end\":48237,\"start\":48236},{\"end\":48246,\"start\":48245},{\"end\":48252,\"start\":48251},{\"end\":48258,\"start\":48257},{\"end\":48544,\"start\":48543},{\"end\":48556,\"start\":48555},{\"end\":48565,\"start\":48564},{\"end\":48905,\"start\":48904},{\"end\":48916,\"start\":48915},{\"end\":48925,\"start\":48924},{\"end\":48938,\"start\":48937},{\"end\":48940,\"start\":48939},{\"end\":49310,\"start\":49309},{\"end\":49322,\"start\":49321},{\"end\":49335,\"start\":49334},{\"end\":49606,\"start\":49605},{\"end\":49895,\"start\":49894},{\"end\":49903,\"start\":49902},{\"end\":49911,\"start\":49910},{\"end\":49917,\"start\":49916},{\"end\":49925,\"start\":49924},{\"end\":49934,\"start\":49933},{\"end\":49942,\"start\":49941},{\"end\":49954,\"start\":49953},{\"end\":50452,\"start\":50451},{\"end\":50463,\"start\":50462},{\"end\":50471,\"start\":50470},{\"end\":50482,\"start\":50481},{\"end\":50489,\"start\":50488},{\"end\":50501,\"start\":50500},{\"end\":50522,\"start\":50521},{\"end\":50532,\"start\":50531},{\"end\":50542,\"start\":50541},{\"end\":50550,\"start\":50549},{\"end\":51024,\"start\":51023},{\"end\":51030,\"start\":51029},{\"end\":51036,\"start\":51035},{\"end\":51043,\"start\":51042},{\"end\":51049,\"start\":51048},{\"end\":51057,\"start\":51056},{\"end\":51065,\"start\":51064},{\"end\":51071,\"start\":51070},{\"end\":51525,\"start\":51524},{\"end\":51533,\"start\":51532},{\"end\":51545,\"start\":51544},{\"end\":51998,\"start\":51997},{\"end\":52007,\"start\":52006},{\"end\":52013,\"start\":52012},{\"end\":52022,\"start\":52021},{\"end\":52031,\"start\":52030},{\"end\":52277,\"start\":52276},{\"end\":52285,\"start\":52284},{\"end\":52292,\"start\":52291},{\"end\":52300,\"start\":52299},{\"end\":52309,\"start\":52308},{\"end\":52311,\"start\":52310}]", "bib_author_last_name": "[{\"end\":35303,\"start\":35298},{\"end\":35313,\"start\":35307},{\"end\":35327,\"start\":35317},{\"end\":35340,\"start\":35331},{\"end\":35630,\"start\":35622},{\"end\":35640,\"start\":35634},{\"end\":35653,\"start\":35644},{\"end\":35666,\"start\":35657},{\"end\":35806,\"start\":35796},{\"end\":35821,\"start\":35810},{\"end\":35834,\"start\":35827},{\"end\":35848,\"start\":35838},{\"end\":35861,\"start\":35852},{\"end\":35871,\"start\":35865},{\"end\":35879,\"start\":35875},{\"end\":35890,\"start\":35883},{\"end\":35903,\"start\":35896},{\"end\":35912,\"start\":35907},{\"end\":36421,\"start\":36416},{\"end\":36433,\"start\":36425},{\"end\":36443,\"start\":36437},{\"end\":36455,\"start\":36447},{\"end\":36468,\"start\":36459},{\"end\":36479,\"start\":36472},{\"end\":36493,\"start\":36485},{\"end\":36500,\"start\":36497},{\"end\":36509,\"start\":36504},{\"end\":36518,\"start\":36513},{\"end\":36993,\"start\":36988},{\"end\":37001,\"start\":36997},{\"end\":37012,\"start\":37005},{\"end\":37019,\"start\":37016},{\"end\":37027,\"start\":37023},{\"end\":37040,\"start\":37031},{\"end\":37053,\"start\":37044},{\"end\":37061,\"start\":37057},{\"end\":37448,\"start\":37441},{\"end\":37462,\"start\":37452},{\"end\":37471,\"start\":37466},{\"end\":37480,\"start\":37475},{\"end\":37809,\"start\":37802},{\"end\":37818,\"start\":37813},{\"end\":37827,\"start\":37822},{\"end\":38139,\"start\":38134},{\"end\":38419,\"start\":38414},{\"end\":38427,\"start\":38425},{\"end\":38435,\"start\":38433},{\"end\":38445,\"start\":38441},{\"end\":38455,\"start\":38451},{\"end\":38464,\"start\":38461},{\"end\":38473,\"start\":38470},{\"end\":38926,\"start\":38922},{\"end\":38933,\"start\":38930},{\"end\":38947,\"start\":38937},{\"end\":38958,\"start\":38951},{\"end\":38966,\"start\":38962},{\"end\":39353,\"start\":39346},{\"end\":39366,\"start\":39357},{\"end\":39378,\"start\":39370},{\"end\":39390,\"start\":39384},{\"end\":39399,\"start\":39394},{\"end\":39409,\"start\":39403},{\"end\":39418,\"start\":39413},{\"end\":39428,\"start\":39422},{\"end\":39440,\"start\":39432},{\"end\":39453,\"start\":39444},{\"end\":39976,\"start\":39968},{\"end\":39987,\"start\":39980},{\"end\":40000,\"start\":39991},{\"end\":40011,\"start\":40004},{\"end\":40020,\"start\":40015},{\"end\":40030,\"start\":40024},{\"end\":40039,\"start\":40034},{\"end\":40050,\"start\":40043},{\"end\":40065,\"start\":40056},{\"end\":40077,\"start\":40069},{\"end\":40528,\"start\":40521},{\"end\":40542,\"start\":40534},{\"end\":40551,\"start\":40546},{\"end\":40978,\"start\":40969},{\"end\":40988,\"start\":40982},{\"end\":41005,\"start\":40992},{\"end\":41017,\"start\":41011},{\"end\":41028,\"start\":41021},{\"end\":41040,\"start\":41032},{\"end\":41051,\"start\":41044},{\"end\":41061,\"start\":41055},{\"end\":41072,\"start\":41065},{\"end\":41081,\"start\":41076},{\"end\":41508,\"start\":41499},{\"end\":41522,\"start\":41512},{\"end\":41531,\"start\":41526},{\"end\":41542,\"start\":41537},{\"end\":41553,\"start\":41546},{\"end\":41566,\"start\":41557},{\"end\":41572,\"start\":41570},{\"end\":41585,\"start\":41578},{\"end\":41597,\"start\":41591},{\"end\":41608,\"start\":41601},{\"end\":42023,\"start\":42014},{\"end\":42036,\"start\":42027},{\"end\":42293,\"start\":42291},{\"end\":42302,\"start\":42297},{\"end\":42309,\"start\":42306},{\"end\":42316,\"start\":42313},{\"end\":42587,\"start\":42578},{\"end\":42595,\"start\":42591},{\"end\":42605,\"start\":42599},{\"end\":42619,\"start\":42609},{\"end\":42627,\"start\":42623},{\"end\":42948,\"start\":42946},{\"end\":42955,\"start\":42952},{\"end\":42963,\"start\":42959},{\"end\":42975,\"start\":42967},{\"end\":43270,\"start\":43265},{\"end\":43278,\"start\":43274},{\"end\":43288,\"start\":43284},{\"end\":43297,\"start\":43292},{\"end\":43606,\"start\":43598},{\"end\":43619,\"start\":43610},{\"end\":43633,\"start\":43623},{\"end\":43645,\"start\":43637},{\"end\":43933,\"start\":43930},{\"end\":43943,\"start\":43937},{\"end\":43955,\"start\":43947},{\"end\":43964,\"start\":43961},{\"end\":43973,\"start\":43968},{\"end\":43989,\"start\":43977},{\"end\":43995,\"start\":43993},{\"end\":44007,\"start\":43999},{\"end\":44021,\"start\":44013},{\"end\":44030,\"start\":44025},{\"end\":44416,\"start\":44413},{\"end\":44422,\"start\":44420},{\"end\":44428,\"start\":44426},{\"end\":44436,\"start\":44432},{\"end\":44442,\"start\":44440},{\"end\":44448,\"start\":44446},{\"end\":44455,\"start\":44452},{\"end\":44805,\"start\":44797},{\"end\":44815,\"start\":44809},{\"end\":44827,\"start\":44819},{\"end\":44838,\"start\":44831},{\"end\":45108,\"start\":45097},{\"end\":45123,\"start\":45112},{\"end\":45136,\"start\":45129},{\"end\":45513,\"start\":45505},{\"end\":45520,\"start\":45517},{\"end\":45532,\"start\":45524},{\"end\":45540,\"start\":45536},{\"end\":45553,\"start\":45546},{\"end\":45897,\"start\":45890},{\"end\":45907,\"start\":45903},{\"end\":45920,\"start\":45913},{\"end\":45932,\"start\":45924},{\"end\":45943,\"start\":45938},{\"end\":45958,\"start\":45949},{\"end\":45969,\"start\":45964},{\"end\":45984,\"start\":45977},{\"end\":45996,\"start\":45990},{\"end\":46008,\"start\":46002},{\"end\":46371,\"start\":46363},{\"end\":46379,\"start\":46375},{\"end\":46389,\"start\":46383},{\"end\":46404,\"start\":46395},{\"end\":46414,\"start\":46408},{\"end\":46424,\"start\":46418},{\"end\":46750,\"start\":46741},{\"end\":46763,\"start\":46754},{\"end\":46775,\"start\":46767},{\"end\":47128,\"start\":47122},{\"end\":47137,\"start\":47134},{\"end\":47152,\"start\":47143},{\"end\":47161,\"start\":47156},{\"end\":47540,\"start\":47531},{\"end\":47551,\"start\":47544},{\"end\":47561,\"start\":47555},{\"end\":47568,\"start\":47565},{\"end\":47580,\"start\":47572},{\"end\":47589,\"start\":47584},{\"end\":47860,\"start\":47856},{\"end\":47870,\"start\":47864},{\"end\":47885,\"start\":47874},{\"end\":47893,\"start\":47889},{\"end\":47907,\"start\":47897},{\"end\":48221,\"start\":48217},{\"end\":48228,\"start\":48225},{\"end\":48234,\"start\":48232},{\"end\":48243,\"start\":48238},{\"end\":48249,\"start\":48247},{\"end\":48255,\"start\":48253},{\"end\":48262,\"start\":48259},{\"end\":48553,\"start\":48545},{\"end\":48562,\"start\":48557},{\"end\":48571,\"start\":48566},{\"end\":48913,\"start\":48906},{\"end\":48922,\"start\":48917},{\"end\":48935,\"start\":48926},{\"end\":48946,\"start\":48941},{\"end\":49319,\"start\":49311},{\"end\":49332,\"start\":49323},{\"end\":49343,\"start\":49336},{\"end\":49613,\"start\":49607},{\"end\":49900,\"start\":49896},{\"end\":49908,\"start\":49904},{\"end\":49914,\"start\":49912},{\"end\":49922,\"start\":49918},{\"end\":49931,\"start\":49926},{\"end\":49939,\"start\":49935},{\"end\":49951,\"start\":49943},{\"end\":49957,\"start\":49955},{\"end\":50460,\"start\":50453},{\"end\":50468,\"start\":50464},{\"end\":50479,\"start\":50472},{\"end\":50486,\"start\":50483},{\"end\":50498,\"start\":50490},{\"end\":50519,\"start\":50502},{\"end\":50529,\"start\":50523},{\"end\":50539,\"start\":50533},{\"end\":50547,\"start\":50543},{\"end\":50556,\"start\":50551},{\"end\":51027,\"start\":51025},{\"end\":51033,\"start\":51031},{\"end\":51040,\"start\":51037},{\"end\":51046,\"start\":51044},{\"end\":51054,\"start\":51050},{\"end\":51062,\"start\":51058},{\"end\":51068,\"start\":51066},{\"end\":51075,\"start\":51072},{\"end\":51530,\"start\":51526},{\"end\":51542,\"start\":51534},{\"end\":51551,\"start\":51546},{\"end\":52004,\"start\":51999},{\"end\":52010,\"start\":52008},{\"end\":52019,\"start\":52014},{\"end\":52028,\"start\":52023},{\"end\":52036,\"start\":52032},{\"end\":52282,\"start\":52278},{\"end\":52289,\"start\":52286},{\"end\":52297,\"start\":52293},{\"end\":52306,\"start\":52301},{\"end\":52315,\"start\":52312}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":232273825},\"end\":35589,\"start\":35236},{\"attributes\":{\"doi\":\"arXiv:1907.02893\",\"id\":\"b1\"},\"end\":35792,\"start\":35591},{\"attributes\":{\"doi\":\"arXiv:2204.03742\",\"id\":\"b2\"},\"end\":36284,\"start\":35794},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":59600431},\"end\":36882,\"start\":36286},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":202777185},\"end\":37391,\"start\":36884},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":121288771},\"end\":37742,\"start\":37393},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":216144410},\"end\":38082,\"start\":37744},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":232168371},\"end\":38332,\"start\":38084},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":238207479},\"end\":38833,\"start\":38334},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3638670},\"end\":39214,\"start\":38835},{\"attributes\":{\"doi\":\"arXiv:1902.03368\",\"id\":\"b10\"},\"end\":39790,\"start\":39216},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":248338266},\"end\":40471,\"start\":39792},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":232097557},\"end\":40756,\"start\":40473},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":244814521},\"end\":41495,\"start\":40758},{\"attributes\":{\"doi\":\"arXiv:2111.08006\",\"id\":\"b14\"},\"end\":41969,\"start\":41497},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":220347682},\"end\":42241,\"start\":41971},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":206594692},\"end\":42544,\"start\":42243},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":196831327},\"end\":42867,\"start\":42546},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":19819905},\"end\":43207,\"start\":42869},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220363892},\"end\":43501,\"start\":43209},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":51614248},\"end\":43924,\"start\":43503},{\"attributes\":{\"doi\":\"arXiv:2012.07421\",\"id\":\"b21\"},\"end\":44315,\"start\":43926},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":248407116},\"end\":44730,\"start\":44317},{\"attributes\":{\"id\":\"b23\"},\"end\":45013,\"start\":44732},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":250144840},\"end\":45472,\"start\":45015},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":239050360},\"end\":45776,\"start\":45474},{\"attributes\":{\"id\":\"b26\"},\"end\":46296,\"start\":45778},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":227013102},\"end\":46695,\"start\":46298},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":222290886},\"end\":46993,\"start\":46697},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":208176471},\"end\":47471,\"start\":46995},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":244799629},\"end\":47852,\"start\":47473},{\"attributes\":{\"id\":\"b31\"},\"end\":48213,\"start\":47854},{\"attributes\":{\"doi\":\"arXiv:2108.13624\",\"id\":\"b32\"},\"end\":48474,\"start\":48215},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":232478802},\"end\":48819,\"start\":48476},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1023605},\"end\":49195,\"start\":48821},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4533221},\"end\":49550,\"start\":49197},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":15348764},\"end\":49811,\"start\":49552},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":215745726},\"end\":50283,\"start\":49813},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":199574483},\"end\":50926,\"start\":50285},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":247839962},\"end\":51387,\"start\":50928},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":201625607},\"end\":51929,\"start\":51389},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":248407562},\"end\":52274,\"start\":51931},{\"attributes\":{\"doi\":\"arXiv:2103.02503\",\"id\":\"b42\"},\"end\":52473,\"start\":52276}]", "bib_title": "[{\"end\":35294,\"start\":35236},{\"end\":36412,\"start\":36286},{\"end\":36984,\"start\":36884},{\"end\":37437,\"start\":37393},{\"end\":37798,\"start\":37744},{\"end\":38130,\"start\":38084},{\"end\":38408,\"start\":38334},{\"end\":38916,\"start\":38835},{\"end\":39964,\"start\":39792},{\"end\":40517,\"start\":40473},{\"end\":40965,\"start\":40758},{\"end\":42010,\"start\":41971},{\"end\":42287,\"start\":42243},{\"end\":42574,\"start\":42546},{\"end\":42942,\"start\":42869},{\"end\":43261,\"start\":43209},{\"end\":43594,\"start\":43503},{\"end\":44409,\"start\":44317},{\"end\":45093,\"start\":45015},{\"end\":45501,\"start\":45474},{\"end\":46359,\"start\":46298},{\"end\":46737,\"start\":46697},{\"end\":47118,\"start\":46995},{\"end\":47527,\"start\":47473},{\"end\":48541,\"start\":48476},{\"end\":48902,\"start\":48821},{\"end\":49307,\"start\":49197},{\"end\":49603,\"start\":49552},{\"end\":49892,\"start\":49813},{\"end\":50449,\"start\":50285},{\"end\":51021,\"start\":50928},{\"end\":51522,\"start\":51389},{\"end\":51995,\"start\":51931}]", "bib_author": "[{\"end\":35305,\"start\":35296},{\"end\":35315,\"start\":35305},{\"end\":35329,\"start\":35315},{\"end\":35342,\"start\":35329},{\"end\":35632,\"start\":35620},{\"end\":35642,\"start\":35632},{\"end\":35655,\"start\":35642},{\"end\":35668,\"start\":35655},{\"end\":35808,\"start\":35794},{\"end\":35823,\"start\":35808},{\"end\":35836,\"start\":35823},{\"end\":35850,\"start\":35836},{\"end\":35863,\"start\":35850},{\"end\":35873,\"start\":35863},{\"end\":35881,\"start\":35873},{\"end\":35892,\"start\":35881},{\"end\":35905,\"start\":35892},{\"end\":35914,\"start\":35905},{\"end\":36423,\"start\":36414},{\"end\":36435,\"start\":36423},{\"end\":36445,\"start\":36435},{\"end\":36457,\"start\":36445},{\"end\":36470,\"start\":36457},{\"end\":36481,\"start\":36470},{\"end\":36495,\"start\":36481},{\"end\":36502,\"start\":36495},{\"end\":36511,\"start\":36502},{\"end\":36520,\"start\":36511},{\"end\":36995,\"start\":36986},{\"end\":37003,\"start\":36995},{\"end\":37014,\"start\":37003},{\"end\":37021,\"start\":37014},{\"end\":37029,\"start\":37021},{\"end\":37042,\"start\":37029},{\"end\":37055,\"start\":37042},{\"end\":37063,\"start\":37055},{\"end\":37450,\"start\":37439},{\"end\":37464,\"start\":37450},{\"end\":37473,\"start\":37464},{\"end\":37482,\"start\":37473},{\"end\":37811,\"start\":37800},{\"end\":37820,\"start\":37811},{\"end\":37829,\"start\":37820},{\"end\":38141,\"start\":38132},{\"end\":38421,\"start\":38410},{\"end\":38429,\"start\":38421},{\"end\":38437,\"start\":38429},{\"end\":38447,\"start\":38437},{\"end\":38457,\"start\":38447},{\"end\":38466,\"start\":38457},{\"end\":38475,\"start\":38466},{\"end\":38928,\"start\":38918},{\"end\":38935,\"start\":38928},{\"end\":38949,\"start\":38935},{\"end\":38960,\"start\":38949},{\"end\":38968,\"start\":38960},{\"end\":39355,\"start\":39344},{\"end\":39368,\"start\":39355},{\"end\":39380,\"start\":39368},{\"end\":39392,\"start\":39380},{\"end\":39401,\"start\":39392},{\"end\":39411,\"start\":39401},{\"end\":39420,\"start\":39411},{\"end\":39430,\"start\":39420},{\"end\":39442,\"start\":39430},{\"end\":39455,\"start\":39442},{\"end\":39978,\"start\":39966},{\"end\":39989,\"start\":39978},{\"end\":40002,\"start\":39989},{\"end\":40013,\"start\":40002},{\"end\":40022,\"start\":40013},{\"end\":40032,\"start\":40022},{\"end\":40041,\"start\":40032},{\"end\":40052,\"start\":40041},{\"end\":40067,\"start\":40052},{\"end\":40079,\"start\":40067},{\"end\":40530,\"start\":40519},{\"end\":40544,\"start\":40530},{\"end\":40553,\"start\":40544},{\"end\":40980,\"start\":40967},{\"end\":40990,\"start\":40980},{\"end\":41007,\"start\":40990},{\"end\":41019,\"start\":41007},{\"end\":41030,\"start\":41019},{\"end\":41042,\"start\":41030},{\"end\":41053,\"start\":41042},{\"end\":41063,\"start\":41053},{\"end\":41074,\"start\":41063},{\"end\":41083,\"start\":41074},{\"end\":41510,\"start\":41497},{\"end\":41524,\"start\":41510},{\"end\":41533,\"start\":41524},{\"end\":41544,\"start\":41533},{\"end\":41555,\"start\":41544},{\"end\":41568,\"start\":41555},{\"end\":41574,\"start\":41568},{\"end\":41587,\"start\":41574},{\"end\":41599,\"start\":41587},{\"end\":41610,\"start\":41599},{\"end\":42025,\"start\":42012},{\"end\":42038,\"start\":42025},{\"end\":42295,\"start\":42289},{\"end\":42304,\"start\":42295},{\"end\":42311,\"start\":42304},{\"end\":42318,\"start\":42311},{\"end\":42589,\"start\":42576},{\"end\":42597,\"start\":42589},{\"end\":42607,\"start\":42597},{\"end\":42621,\"start\":42607},{\"end\":42629,\"start\":42621},{\"end\":42950,\"start\":42944},{\"end\":42957,\"start\":42950},{\"end\":42965,\"start\":42957},{\"end\":42977,\"start\":42965},{\"end\":43272,\"start\":43263},{\"end\":43280,\"start\":43272},{\"end\":43290,\"start\":43280},{\"end\":43299,\"start\":43290},{\"end\":43608,\"start\":43596},{\"end\":43621,\"start\":43608},{\"end\":43635,\"start\":43621},{\"end\":43647,\"start\":43635},{\"end\":43935,\"start\":43926},{\"end\":43945,\"start\":43935},{\"end\":43957,\"start\":43945},{\"end\":43966,\"start\":43957},{\"end\":43975,\"start\":43966},{\"end\":43991,\"start\":43975},{\"end\":43997,\"start\":43991},{\"end\":44009,\"start\":43997},{\"end\":44023,\"start\":44009},{\"end\":44032,\"start\":44023},{\"end\":44418,\"start\":44411},{\"end\":44424,\"start\":44418},{\"end\":44430,\"start\":44424},{\"end\":44438,\"start\":44430},{\"end\":44444,\"start\":44438},{\"end\":44450,\"start\":44444},{\"end\":44457,\"start\":44450},{\"end\":44807,\"start\":44795},{\"end\":44817,\"start\":44807},{\"end\":44829,\"start\":44817},{\"end\":44840,\"start\":44829},{\"end\":45110,\"start\":45095},{\"end\":45125,\"start\":45110},{\"end\":45138,\"start\":45125},{\"end\":45515,\"start\":45503},{\"end\":45522,\"start\":45515},{\"end\":45534,\"start\":45522},{\"end\":45542,\"start\":45534},{\"end\":45555,\"start\":45542},{\"end\":45899,\"start\":45886},{\"end\":45909,\"start\":45899},{\"end\":45922,\"start\":45909},{\"end\":45934,\"start\":45922},{\"end\":45945,\"start\":45934},{\"end\":45960,\"start\":45945},{\"end\":45973,\"start\":45960},{\"end\":45986,\"start\":45973},{\"end\":45998,\"start\":45986},{\"end\":46010,\"start\":45998},{\"end\":46373,\"start\":46361},{\"end\":46381,\"start\":46373},{\"end\":46391,\"start\":46381},{\"end\":46406,\"start\":46391},{\"end\":46416,\"start\":46406},{\"end\":46426,\"start\":46416},{\"end\":46752,\"start\":46739},{\"end\":46765,\"start\":46752},{\"end\":46777,\"start\":46765},{\"end\":47130,\"start\":47120},{\"end\":47139,\"start\":47130},{\"end\":47154,\"start\":47139},{\"end\":47163,\"start\":47154},{\"end\":47542,\"start\":47529},{\"end\":47553,\"start\":47542},{\"end\":47563,\"start\":47553},{\"end\":47570,\"start\":47563},{\"end\":47582,\"start\":47570},{\"end\":47591,\"start\":47582},{\"end\":47862,\"start\":47854},{\"end\":47872,\"start\":47862},{\"end\":47887,\"start\":47872},{\"end\":47895,\"start\":47887},{\"end\":47909,\"start\":47895},{\"end\":48223,\"start\":48215},{\"end\":48230,\"start\":48223},{\"end\":48236,\"start\":48230},{\"end\":48245,\"start\":48236},{\"end\":48251,\"start\":48245},{\"end\":48257,\"start\":48251},{\"end\":48264,\"start\":48257},{\"end\":48555,\"start\":48543},{\"end\":48564,\"start\":48555},{\"end\":48573,\"start\":48564},{\"end\":48915,\"start\":48904},{\"end\":48924,\"start\":48915},{\"end\":48937,\"start\":48924},{\"end\":48948,\"start\":48937},{\"end\":49321,\"start\":49309},{\"end\":49334,\"start\":49321},{\"end\":49345,\"start\":49334},{\"end\":49615,\"start\":49605},{\"end\":49902,\"start\":49894},{\"end\":49910,\"start\":49902},{\"end\":49916,\"start\":49910},{\"end\":49924,\"start\":49916},{\"end\":49933,\"start\":49924},{\"end\":49941,\"start\":49933},{\"end\":49953,\"start\":49941},{\"end\":49959,\"start\":49953},{\"end\":50462,\"start\":50451},{\"end\":50470,\"start\":50462},{\"end\":50481,\"start\":50470},{\"end\":50488,\"start\":50481},{\"end\":50500,\"start\":50488},{\"end\":50521,\"start\":50500},{\"end\":50531,\"start\":50521},{\"end\":50541,\"start\":50531},{\"end\":50549,\"start\":50541},{\"end\":50558,\"start\":50549},{\"end\":51029,\"start\":51023},{\"end\":51035,\"start\":51029},{\"end\":51042,\"start\":51035},{\"end\":51048,\"start\":51042},{\"end\":51056,\"start\":51048},{\"end\":51064,\"start\":51056},{\"end\":51070,\"start\":51064},{\"end\":51077,\"start\":51070},{\"end\":51532,\"start\":51524},{\"end\":51544,\"start\":51532},{\"end\":51553,\"start\":51544},{\"end\":52006,\"start\":51997},{\"end\":52012,\"start\":52006},{\"end\":52021,\"start\":52012},{\"end\":52030,\"start\":52021},{\"end\":52038,\"start\":52030},{\"end\":52284,\"start\":52276},{\"end\":52291,\"start\":52284},{\"end\":52299,\"start\":52291},{\"end\":52308,\"start\":52299},{\"end\":52317,\"start\":52308}]", "bib_venue": "[{\"end\":35401,\"start\":35342},{\"end\":35618,\"start\":35591},{\"end\":36004,\"start\":35930},{\"end\":36556,\"start\":36520},{\"end\":37122,\"start\":37063},{\"end\":37550,\"start\":37482},{\"end\":37897,\"start\":37829},{\"end\":38199,\"start\":38141},{\"end\":38570,\"start\":38475},{\"end\":39013,\"start\":38968},{\"end\":39342,\"start\":39216},{\"end\":40104,\"start\":40079},{\"end\":40604,\"start\":40553},{\"end\":41099,\"start\":41083},{\"end\":41698,\"start\":41626},{\"end\":42096,\"start\":42038},{\"end\":42383,\"start\":42318},{\"end\":42694,\"start\":42629},{\"end\":43028,\"start\":42977},{\"end\":43343,\"start\":43299},{\"end\":43696,\"start\":43647},{\"end\":44101,\"start\":44048},{\"end\":44509,\"start\":44457},{\"end\":44793,\"start\":44732},{\"end\":45233,\"start\":45138},{\"end\":45613,\"start\":45555},{\"end\":45884,\"start\":45778},{\"end\":46485,\"start\":46426},{\"end\":46835,\"start\":46777},{\"end\":47221,\"start\":47163},{\"end\":47650,\"start\":47591},{\"end\":48024,\"start\":47909},{\"end\":48331,\"start\":48280},{\"end\":48637,\"start\":48573},{\"end\":48996,\"start\":48948},{\"end\":49360,\"start\":49345},{\"end\":49674,\"start\":49615},{\"end\":50027,\"start\":49959},{\"end\":50574,\"start\":50558},{\"end\":51142,\"start\":51077},{\"end\":51648,\"start\":51553},{\"end\":52090,\"start\":52038},{\"end\":52364,\"start\":52333}]"}}}, "year": 2023, "month": 12, "day": 17}