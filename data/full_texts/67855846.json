{"id": 67855846, "updated": "2023-10-02 09:21:39.053", "metadata": {"title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs", "authors": "[{\"first\":\"Dheeru\",\"last\":\"Dua\",\"middle\":[]},{\"first\":\"Yizhong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Pradeep\",\"last\":\"Dasigi\",\"middle\":[]},{\"first\":\"Gabriel\",\"last\":\"Stanovsky\",\"middle\":[]},{\"first\":\"Sameer\",\"last\":\"Singh\",\"middle\":[]},{\"first\":\"Matt\",\"last\":\"Gardner\",\"middle\":[]}]", "venue": "NAACL", "journal": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4% F1 on our generalized accuracy metric, while expert human performance is 96%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51% F1.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1903.00161", "mag": "2951365061", "acl": "N19-1246", "pubmed": null, "pubmedcentral": null, "dblp": "conf/naacl/DuaWDSS019", "doi": "10.18653/v1/n19-1246"}}, "content": {"source": {"pdf_hash": "9498f5b9ff0052c22e41979df49bc8efca0a9d17", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1903.00161v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ce2eabbb5ce75d34a214fec20d553456481d4fec", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9498f5b9ff0052c22e41979df49bc8efca0a9d17.txt", "contents": "\nDROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\n\n\nDheeru Dua ddua@uci.edu \nUniversity of California\nIrvineUSA\n\nYizhong Wang \nPeking University\nBeijingChina\n\n\u2666 \nPradeep Dasigi \nAllen Institute for Artificial Intelligence\nSeattleWashingtonUSA\n\nGabriel Stanovsky \nAllen Institute for Artificial Intelligence\nSeattleWashingtonUSA\n\nSameer Singh \nUniversity of California\nIrvineUSA\n\n\u2663 \nMatt Gardner \nAllen Institute for Artificial Intelligence\nIrvineCaliforniaUSA\n\nDROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\n\nReading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96kquestion benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 32.7% F 1 on our generalized accuracy metric, while expert human performance is 96.0%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F 1 .\n\nIntroduction\n\nThe task of reading comprehension, where systems must understand a single passage of text well enough to answer arbitrary questions about it, has seen significant progress in the last few years, so much that the most popular datasets available for this task have been solved (Chen et al., 2016;Devlin et al., 2018). We introduce a substantially more challenging English reading comprehension dataset aimed at pushing the field towards more comprehensive analysis of paragraphs of text. In * Work done while interning at the Allen Institute for Artificial Intelligence in Irvine, California. this new benchmark, which we call DROP, a system is given a paragraph and a question and must perform some kind of Discrete Reasoning Over the text in the Paragraph to obtain the correct answer.\n\nThese questions that require discrete reasoning (such as addition, sorting, or counting; see Table 1) are inspired by the complex, compositional questions commonly found in the semantic parsing literature. We focus on this type of questions because they force a structured analysis of the content of the paragraph that is detailed enough to permit reasoning. Our goal is to further paragraph understanding; complex questions allow us to test a system's understanding of the paragraph's semantics.\n\nDROP is also designed to further research on methods that combine distributed representations with symbolic, discrete reasoning. In order to do well on this dataset, a system must be able to find multiple occurrences of an event described in a question (presumably using some kind of soft matching), extract arguments from the events, then perform a numerical operation such as a sort, to answer a question like \"Who threw the longest touchdown pass?\".\n\nWe constructed this dataset through crowdsourcing, first collecting passages from Wikipedia that are easy to ask hard questions about, then encouraging crowd workers to produce challenging questions. This encouragement was partially through instructions given to workers, and partially through the use of an adversarial baseline: we ran a baseline reading comprehension method (BiDAF) (Seo et al., 2016) in the background as crowd workers were writing questions, requiring them to give questions that the baseline system could not correctly answer. This resulted in a dataset of 96,567 questions from a variety of categories in Wikipedia, with a particular emphasis on sports game summaries and history passages. The answers to the questions are required to be spans in the passage or question, numbers, or dates, which allows for easy and accurate evaluation metrics.\n\nWe present an analysis of the resulting dataset to show what phenomena are present. We find that many questions combine complex question semantics with SQuAD-style argument finding; e.g., in the first question in Table 1, BiDAF correctly finds the amount the painting sold for, but does not understand the question semantics and cannot perform the numerical reasoning required to answer the question. Other questions, such as the fifth question in Table 1, require finding all events in the passage that match a description in the question, then aggregating them somehow (in this instance, by counting them and then performing an argmax). Very often entity coreference is required. Table 1 gives a number of different phenomena, with their proportions in the dataset.\n\nWe used three types of systems to judge baseline performance on DROP: (1) heuristic baselines, to check for biases in the data; (2) SQuAD-style reading comprehension methods; and (3) semantic parsers operating on a pipelined analysis of the passage. The reading comprehension methods perform the best, with our best baseline achieving 32.7% F 1 on our generalized accuracy metric, while expert human performance is 96.7%.\n\nFinally, we contribute a new model for this task that combines limited numerical reasoning with standard reading comprehension methods, allowing the model to answer questions involving counting, addition and subtraction. This model reaches 47% F 1 , a 14.3% absolute increase over the best baseline system.\n\nThe dataset, easily-extendable code for the baseline systems, and a leaderboard with a hidden test set can be found at https://allennlp.org/ drop.\n\n\nRelated Work\n\nQuestion answering datasets: With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2018;Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017;Yang et al., 2018), mismatched passages and questions (Saha et al., 2018;Kocisk\u00fd et al., 2018;Rajpurkar et al., 2018), in-tegrating knowledge from external sources (Mihaylov et al., 2018;, or a particular kind of \"multi-step\" reasoning over multiple documents (Welbl et al., 2018;Khashabi et al., 2018). We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions. 1 One could argue that we are adding numerical reasoning as an \"additional complexity\", and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding.\n\nMany existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015;Ling et al., 2017). Our dataset is different in that it typically has much longer contexts, is more open domain, and requires deeper paragraph understanding.\n\nSemantic parsing: The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996;Zettlemoyer and Collins, 2005;Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the Wik-iTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tabular representation of the content of our paragraphs, DROP would be largely the same as WikiTableQuestions, with similar (possibly even simpler) question semantics. Our novelty is that we are the first to combine these complex questions with paragraph understanding, with the aim of encouraging systems that can produce comprehensive structural analyses of paragraphs, either explicitly or implicitly.\n\nAdversarial dataset construction: We continue a recent trend in creating datasets with adversarial baselines in the loop (Paperno et al., 2016;Minervini and Riedel, 2018;Zellers et al., 2018b;Zellers et al., 2018a). In our case, instead of using an adversarial baseline to  filter automatically generated examples, we use it in a crowdsourcing task, to raise the difficulty level of the questions provided by crowd workers.\n\nNeural symbolic reasoning: DROP is designed to encourage research on methods that combine neural methods with discrete, symbolic reasoning.\n\nWe present one such model in Section 6. Other related work along these lines has been done by Reed and de Freitas (2015), Neelakantan et al. (2015), and Liang et al. (2017).\n\n\nDROP Data Collection\n\nIn this section, we describe our annotation protocol, which consists of three phases. First, we automatically extract passages from Wikipedia which are expected to contain an interesting semantic structure. Second, we crowdsource question-answer pairs on these passages, eliciting questions which require discrete reasoning. Finally, we validate the development and test portions of DROP to ensure their quality and report inter-annotator agreement.\n\nPassage extraction We searched Wikipedia for passages that had a narrative sequence of events, particularly with a high proportion of numbers, as our initial pilots indicated that these passages were the easiest to ask complex questions about. We found that National Football League (NFL) game summaries and history articles were particularly promising, and we additionally sampled from any Wikipedia passage that contained at least twenty numbers. 2 This process yielded a collection of about 7,000 passages.\n\nQuestion collection We used Amazon Mechanical Turk 3 to crowdsource the collection of questionanswer pairs, where each question could be answered in the context of a single Wikipedia passage. In order to allow some flexibility during the annotation process, in each human intelligence task (HIT) workers were presented with a random sample of 5 of our Wikipedia passages, and were asked to produce a total of at least 12 question-answer pairs on any of these.\n\nWe presented workers with example questions from five main categories, inspired by questions from the semantic parsing literature (addition/subtraction, minimum/maximum, counting, selection and comparison; see examples in Table 1), to elicit questions that require complex linguistic understanding and discrete reasoning. In addition, to further increase the difficulty of the questions in DROP, we employed a novel adverserial annotation setting, where workers were only allowed to submit questions which a real-time QA model BiDAF could not solve. 4 Following, each worker answered their own question with one of three answer types: spans of text from either question or passage, a date (which was common in history and open-domain text) and numbers, allowed only for questions which explicitly stated a specific unit of measurement (e.g., \"How many yards did Brady run?\"), in an attempt to simplify the evaluation process.\n\nInitially, we opened our HITs to all United States workers and gradually reduced our worker pool to workers who understood the task and annotated it well. Each HIT paid $5 and could be completed within 30 minutes, compensating a trained worker  with an average pay of $10 / hour. Overall, we collected a total of 96,567 questionanswer pairs. The dataset was randomly partitioned into training (80%), development (10%) and test (10%) sets, with mutually exclusive passages across the splits.\n\nValidation In order to test inter-annotator agreement and to improve the quality of evaluation against DROP, we collected at least two additional answers for each question in the development and test sets.\n\nIn a separate HIT, workers were given context passages and a previously crowdsourced question, and were asked to either answer the question, or mark it as invalid (this has occurred for 0.7% of the data, which we subsequently filtered out). We found that the resulting inter-annotator agreement was good and at par with other QA tasks; overall Cohen's \u03ba was 0.74, with 0.81 for numbers, 0.62 for spans, and 0.65 for dates.\n\n\nDROP Data Analysis\n\nIn the following, we quantitatively analyze properties of passages, questions, and answers in DROP. Different statistics of the dataset are depicted in Table 2. Notably, questions have a diverse vocabulary of more than 17.5K different words in our training set.\n\nQuestion analysis To assess the question type distribution, we sampled 250 questions from the training and development sets and manually annotated the categories of discrete operations required to answer the question. Table 1 shows the distribution of these categories in the dataset. In addition, to get a better sense of the lexical diversity of questions in the dataset, we find the most frequently trigram patterns in the questions per answer type. We find that the dataset offers a huge variety of linguistic constructs, with the most frequent pattern (\"Which team scored\") appearing in only 4% of the span type questions. For number type questions,  Table 3: Distribution of answer types in DROP, according to an automatic named entity recognition.\n\nthe 5 most frequent question patterns all start with \"How many\", indicating the need to perform counting and other arithmetic operations. See Appendix for a more detailed question analysis.\n\nAnswer analysis In order to discern the level of passage understanding needed to answer the questions in DROP, we annotate the set of spans in the passage that are necessary for answering the 250 questions mentioned above. We find that on an average 1.99 spans need to be considered to answer a question and the average distance between these spans is 23 words, with 15% samples needing at least 4 spans (See appendix for examples). Finally, we assess the answer distribution in Table 3, by running Part-of-Speech tagger and Named Entity Recognizer from spaCy 5 to automatically partition all the answers into various categories. We find that a majority of the answers are numerical values and proper nouns.\n\n\nBaseline Systems\n\nIn this section we describe the initial baselines that we evaluated on the DROP dataset. We used three types of baselines: state-of-the-art semantic parsers ( \u00a75.1), state-of-the-art reading comprehension models ( \u00a75.2), and heuristics looking for annotation artifacts ( \u00a75.3).\n\n\nSemantic Parsing\n\nSemantic parsing has been used to translate natural language utterances into formal executable languages (e.g., SQL) that can perform discrete operations against a structured knowledge representation, such as knowledge graphs or tabular databases (Zettlemoyer and Collins, 2005;Berant et al., 2013b;Yin and Neubig, 2017;Chen and Mooney, 2011, inter alia). Since many of DROP's questions require similar discrete reasoning, it is appealing to port some of the successful work in semantic parsing to the DROP dataset. Specifically, we use the grammar-constrained semantic parsing model built by Krishnamurthy et al. (2017) (KDG) for the WIKITABLEQUESTIONS tabular dataset (Pasupat and Liang, 2015).\n\nSentence representation schemes We experimented with three paradigms to represent paragraphs as structured contexts: (1) Stanford dependencies (de Marneffe and Manning, 2008), which capture word-level syntactic relations, (2) Open Information Extraction (Banko et al., 2007, Open IE), a shallow semantic representation which directly links predicates and arguments, and (3) Semantic Role Labeling (Carreras and M\u00e0rquez, 2005, SRL), which disambiguates senses for polysemous predicates and assigns predicate-specific argument roles. 6 To adhere to KDG's structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles.\n\nLogical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, numbers, and dates. In addition, it defines functions that operate on these elements, such as counters and filters. 7 Following Krishnamurthy et al. (2017), we use the argument and return types of these functions to automatically induce a grammar to constrain the parser. We also add context-specific rules to produce strings occurring in both question and paragraph, and those paragraph strings that are neighbors of question tokens in the GloVe embedding space (Pennington et al., 2014), up to a cosine distance of d. 8 The complete set of functions used in our language and their induced grammar are included in the Appendix.\n\nTraining and inference During training, the KDG parser maximizes the marginal likelihood of a set of (possibly spurious) question logical forms that evaluate to the correct answer. We obtain such a set by performing an exhaustive search, driven by the grammar, up to a preset tree depth. At test time, we use beam search to produce the most likely logical form, which is then executed to predict an answer.\n\n\nSQuAD-style Reading Comprehension\n\nWe test four different SQuAD-style reading comprehension models on DROP: (1) BiDAF (Seo et al., 2016), which is the adversarial baseline we used in data construction (66.8% EM on SQuAD 1.1); (2) QANet (Yu et al., 2018), currently the best-performing published model on SQuAD 1.1 without data augmentation or pretraining (72.7% EM); (3) QANet + ELMo, which enhances the QANet model by concatenating pretrained ELMo representations (Peters et al., 2018) to the original embeddings (78.7% EM); (4) BERT (Devlin et al., 2018), which recently achieved improvements on many NLP tasks with a novel pretraining technique (84.7% EM). 9 These models require a few minor adaptations when training on DROP. While SQuAD provides answer indices in the passage, our dataset only provides the answer strings. To address this, we adopted the marginal likelihood objective function proposed by , which sums over the probabilities of all the matching spans. 10 We also omitted the training questions which can not be answered by a span in the passage (45%), and could therefore cannot be represented by these systems.\n\n\nHeuristic Baselines\n\nA recent line of work (Gururangan et al., 2018;Kaushik and Lipton, 2018) has identified that popular synthetic NLP datasets (such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015) are prone to have artifacts and annotation biases which can be exploited by supervised algorithms that learn to pick up these artifacts as signal instead of more meaningful semantic features.\n\nWe estimate artifacts by training the QANet model described in Section 5.2 on a version of DROP where either the question or the paragraph input representation vectors are zeroed out (question-only and paragraph-only, respectively). Consequently, the resulting models can then only predict answer spans from either the question or the paragraph.\n\nIn addition, we devise a baseline which estimates the answer variance in DROP. We start by counting the unigram and bigram answer frequency for each wh question-word in the train set (taken as the first word in the question). The majority baseline then predicts an answer as the set of 3 most common answer spans for the input question word (e.g., for \"when\", these were \"quarter\", \"end\" and \"October\").\n\n\nAugmented QANet\n\nDROP is designed to encourage models that combine neural reading comprehension with symbolic reasoning. None of the baselines we described in Section 5 can do this. As a preliminary attempt toward this goal, we propose an augmented QANet model, which allows the state-of-the-art reading comprehension system to produce three new answer types: (1) spans from the question; (2) counts;\n\n(3) addition or subtraction over numbers. To predict numbers, the model first predicts whether the answer is a count or an arithmetic expression. It then predicts the specific numbers involved in the expression. This can be viewed as the neural model producing a partially executed logical form, leaving the final arithmetic to a symbolic system. While this model can currently only handle a very limited set of operations, we believe this is a promising approach to combining neural methods and symbolic reasoning. The model is trained by marginalizing over all execution paths that lead to the correct answer.\n\n\nModel Description\n\nOur augmented QANet model follows the typical architecture of previous reading comprehension models, which is composed of embedding, encoding, passage-question attention, and output layers. We use the original QANet architecture for everything up to the output layer. This gives us a question representation Q \u2208 R m\u00d7d , and a projected question-aware passage representationP \u2208 R n\u00d7d . We have four different output layers, for the four different kinds of answers the model can produce:\n\nPassage span As in the original QANet model, to predict an answer in the passage we apply three repetitions of the QANet encoder to the passage representationP and get their outputs as M 0 , M 1 , M 2 respectively. Then the probabilities of the starting and ending positions from the passage can be computed as:\np p start = softmax(FFN([M 0 ; M 1 ]),(1)p p end = softmax(FFN([M 0 ; M 2 ])(2)\nwhere FFN is a two-layer feed-forward network with the RELU activation.\n\nQuestion span Some questions in DROP have their answer in the question instead of the passage.\n\nTo predict an answer from the question, the model first computes a vector h P that represents the information it finds in the passage:\n\u03b1 P = softmax(W PP ),(3)h P = \u03b1 PP(4)\nThen it computes the probabilities of the starting and ending positions from the question as:\np q start = softmax(FFN([Q; e |Q| \u2297 h P ]), (5) p q end = softmax(FFN([Q; e |Q| \u2297 h P ])(6)\nwhere the outer product with the identity (e |Q| \u2297 \u00b7) simply repeats h P for each question word.\n\nCount We model the capability of counting as a multi-class classification problem. Specifically, we consider ten numbers (0-9) in this preliminary model and the probabilities of choosing these numbers is computed based on the passage vector h P :\np count = softmax(FFN(h P ))(7)\nArithmetic expression Many questions in DROP require the model to locate multiple numbers in the passage and add or subtract them to get the final answer. To model this process, we first extract all the numbers from the passage and then learn to assign a plus, minus or zero for each number. In this way, we get an arithmetic expression composed of signed numbers, which can be evaluated to give the final answer.\n\nTo do this, we first apply another QANet encoder to M 2 and get a new passage representation M 3 . Then we select an index over the concatenation of M 0 and M 3 , to get a representation for each number in this passage. The i th number can be represented as h N i and the probabilities of this number being assigned a plus, minus or zero are:  Answer type prediction We use a categorical variable to decide between the above four answer types, with probabilities computed as:\np sign i = softmax(FFN(h N i ))(8)p type = softmax(FFN([h P , h Q ]))(9)\nwhere h Q is computed over Q, in a similar way as we did for h P . At test time, we first determine this answer type greedily and then get the best answer from the selected type.\n\n\nWeakly-Supervised Training\n\nFor supervision, DROP contains only the answer string, not which of the above answer types is used to arrive at the answer. To train our model, we adopt the weakly supervised training method widely used in the semantic parsing literature (Berant et al., 2013a). We find all executions that evaluate to the correct answer, including matching passage spans and question spans, correct count numbers, as well as sign assignments for numbers. Our training objective is then to maximize the marginal likelihood of these executions. 11  \n\n\nResults and Discussion\n\nThe performance of all tested models on the DROP dataset is presented in Table 4. We employed the standard implementation of Exact-Match accuracy as used by SQuAD, however, in case of F1, if the number in predicted answer does not match the number in gold (wherever applicable) we assign F1 as zero for that answer. Most notably, all models perform significantly worse than on other prominent reading comprehension datasets, while human performance remains at similar high levels. 12 For example, BERT, the current state-of-the-art on SQuAD, drops by more than 50 absolute F1 points. This is a positive indication that DROP is indeed a challenging reading comprehension dataset, which opens the door for tackling new and complex reasoning problems on a large scale.\n\nThe best performance is obtained by our augmented QANet model. Table 6 shows that our gains are obtained on the challenging and frequent number answer type, which requires various complex types of reasoning. Future work may also try combining our model with BERT. Furthermore, we find that all heuristic baselines do poorly on our data, hopefully attesting to relatively small biases in DROP.\n\nWhen examining the performance of the semantic parsing baselines, we find that they all perform quite poorly on DROP. This is partly since we had to limit the search space for logical forms up to depth 9 due to its exponential run time (finding 12 Human performance was estimated by the authors answering 420 questions from the test set.  a plausible logical form for less than 20% of the training questions), indicating that more research is needed to address the scalability of these models to large-scale reading comprehension datasets.\n\nError Analysis Finally, in order to better understand the outstanding challenges in DROP, we conducted an error analysis on a random sample of 100 erroneous augmented QANet predictions. The most common errors were on questions which required complex type of reasoning, such as arithmetic operations (evident in 51% of the errors), counting (30%), domain knowledge and common sense (23%), co-reference (6%), or a combination of different types of reasoning (40%). See Table 5 for examples of some of the common phenomena.\n\n\nConclusion\n\nWe have presented DROP, a dataset of complex reading comprehension questions that require Discrete Reasoning Over Paragraphs. This dataset is substantially more challenging than existing datasets, with the best baseline achieving only 32.7% F1, while humans achieve 96%. We hope this dataset will spur research into more compre-hensive analysis of paragraphs, and into methods that combine distributed representations with symbolic reasoning. We have additionally presented initial work in this direction, with a model that augments QANet with limited numerical reasoning capability, achieving 47% F1 on DROP.\n\nA Distribution of question patterns \n\n\nB Experimental Setting\n\nFor the BiDAF baseline, we use the implementation in AllenNLP but change it to marginal objective. For QANet model, our settings differ from the original paper only in the batch size (16 v.s. 32) and number of blocks in the modeling layer (6 v.s. 7) due to the GPU memory limit. We adopt the ELMo representations trained on 5.5B corpus for the QANet+ELMo baseline and the large uncased BERT model for the BERT baseline. The hyperparameters for our augmented QANet model are the same as the QANet baseline. \n\nFigure 1 :\n1Distribution of the most popular question prefixes for two different subsets of the training data.\n\nFigure 2 :Figure 3 :\n23Question Answering HIT sample with passage on the left and input fields for answer on the right Highlighted candidate spans\n\n\nJames Douglas was the second son of Sir George Douglas of Pittendreich, and Elizabeth Douglas, daughter David Douglas of Pittendreich. Before 1543 he married Elizabeth, daughter of James Douglas, 3rd Earl of Morton. In 1553 James Douglas succeeded to the title and estates of his father-in-law.This Annual Financial Report is our principal financial statement of accountability. The AFR gives a comprehensive view of the Department's financial activities ...Reasoning Passage (some parts shortened) \n\nQuestion \nAnswer \nBiDAF \n\nSubtraction \n(31.2%) \n\nThat year, his Untitled (1981), a painting of a haloed, \nblack-headed man with a bright red skeletal body, de-\npicted amid the artists signature scrawls, was sold by \nRobert Lehrman for $16.3 million, well above its $12 \nmillion high estimate. \n\nHow many more dol-\nlars was the Untitled \n(1981) painting sold \nfor than the 12 million \ndollar estimation? \n\n4300000 \n$16.3 \nmillion \n\nComparison \n(20.4%) \n\nIn 1517, the seventeen-year-old King sailed to Castile. \nThere, his Flemish court . . . . In May 1518, Charles \ntraveled to Barcelona in Aragon. \n\nWhere did Charles \ntravel to first, Castile \nor Barcelona? \n\nCastile \nAragon \n\nSelection \n(18.4%) \n\nIn 1970, to commemorate the 100th anniversary of the \nfounding of Baldwin City, Baker University professor \nand playwright Don Mueller and Phyllis E. Braun, \nBusiness Manager, produced a musical play entitled \nThe Ballad Of Black Jack to tell the story of the events \nthat led up to the battle. \n\nWho was the Uni-\nversity professor that \nhelped produce The \nBallad Of Black Jack, \nIvan Boyd or Don \nMueller? \n\nDon \nMueller \n\nBaker \n\nAddition \n(12%) \n\nBefore the UNPROFOR fully deployed, the HV clashed \nwith an armed force of the RSK in the village of Nos \nKalik, located in a pink zone near\u0160ibenik, and captured \nthe village at 4:45 p.m. on 2 March 1992. The JNA \nformed a battlegroup to counterattack the next day. \n\nWhat date did the JNA \nform a battlegroup to \ncounterattack after the \nvillage of Nos Kalik \nwas captured? \n\n3 March \n1992 \n\n2 March \n1992 \n\nCount \n(16%) and \nSort \n(8.8%) \n\nDenver would retake the lead with kicker Matt Prater \nnailing a 43-yard field goal, yet Carolina answered as \nkicker John Kasay ties the game with a 39-yard field \ngoal. . . . Carolina closed out the half with Kasay nail-\ning a 44-yard field goal. . . . In the fourth quarter, Car-\nolina sealed the win with Kasay's 42-yard field goal. \n\nWhich kicker kicked \nthe most field goals? \n\nJohn \nKasay \n\nMatt \nPrater \n\nCoreference \nResolution \n(4%) \n\nHow many years af-\nter he married Eliza-\nbeth did James Dou-\nglas succeed to the ti-\ntle and estates of his \nfather-in-law? \n\n10 \n1553 \n\nOther \nArithmetic \n(2.8%) \n\nAlthough the movement initially gathered some 60,000 \nadherents, the subsequent establishment of the Bulgar-\nian Exarchate reduced their number by some 75%. \n\nHow many adherents \nwere left after the es-\ntablishment of the Bul-\ngarian Exarchate? \n\n15000 \n60,000 \n\nSet of \nspans \n(2.4%) \n\nAccording to some sources 363 civilians were killed in \nKavadarci, 230 in Negotino and 40 in Vatasha. \n\nWhat were the 3 vil-\nlages that people were \nkilled in? \n\nKavadarci, \nNegotino, \nVatasha \n\nNegotino \nand 40 in \nVatasha \n\nOther \n(6.4%) \n\nWhat does AFR stand \nfor? \n\nAnnual \nFinancial \nReport \n\none of the \nBig Four \naudit firms \n\n\n\nTable 1 :\n1Example questions and answers from the DROP dataset, showing the relevant parts of the associated passage and the reasoning required to answer the question.\n\nTable 2 :\n2Dataset statistics across the different splits.\n\nTable 4 :\n4Performance of the different models on our de-\nvelopment and test set, in terms of Exact Match (EM), \nand token-level F1. Both metrics are calculated as the \nmaximum against a set of gold answers. \n\n\n\n\n. . . Twenty-five of his 150 men were sick, and his advance stalled . . .How many ofBartolom de Amsqueta's 150 men were not sick? 125 145 Count + Filter . . . Macedonians were the largest ethnic group in Skopje, with 338,358 inhabitants . . . Then came . . . Domain knowledge . . . Smith was sidelined by a torn pectoral muscle suffered during practice . . . Addition . . . culminating in the Battle of Vienna of 1683, which marked the start of the 15-year-long Great Turkish War . . .Phenomenon \n\nPassage Highlights \nQuestion \nAnswer \nOur \nmodel \n\nSubtraction \n+ Coreference \n\nSerbs (14,298 \ninhabitants), Turks (8,595), Bosniaks \n(7,585) and Vlachs (2,557) . . . \n\nHow many ethnicities had less than \n10000 people? \n\n3 \n2 \n\nHow many quarters did Smith play? \n0 \n2 \n\nWhat year did the Great Turkish \nWar end? \n\n1698 \n1668 \n\n\n\nTable 5 :\n5Representative examples from our model's error analysis. We list the identified semantic phenomenon, the relevant passage highlights, a gold question-answer pair, and the erroneous prediction by our model.\n\nTable 6 :\n6Dev set performance breakdown by different answer types; our model (Augmented QA, marked as QN+) vs. BERT, the best-performing baseline.\nSome questions in our dataset require limited sports domain knowledge to answer; we expect that there are enough such questions that systems can reasonably learn this knowledge from the data.\nWe used an October 2018 Wikipedia dump, as well as scraping of online Wikipedia. 3 www.mturk.com 4 While BiDAF is no longer state-of-the-art, performance is reasonable and the AllenNLP implementation made it the easiest to deploy as a server.\nhttps://spacy.io/\nWe used the AllenNLP implementations of state-of-theart models for all of these representationsDozat et al., 2017;Stanovsky et al., 2018;He et al., 2017).7  For example filter number greater takes a set of predicate-argument structures, the name of a relation, and a number, and returns all those structures where the numbers in the argument specified by the relation are greater than the given number. 8 d = 0.3 was manually tuned on the development set.\nThe first three scores are based on our own implementation, while the score for BERT is based on an open-source implementation from Hugging Face: https://github.com/huggingface/pytorch-pretrained-bert 10 For the black-box BERT model, we convert DROP to SQuAD format by using the first match as the gold span.\nDue to the exponential search space and the possible noise, we currently only searched the addition/subtraction of two numbers. Given this limited search space, the search and marginalization are exact. * Results are on subset of data. We are working on getting results on full data.\n\nOpen information extraction from the web. Michele Banko, Michael J Cafarella, Stephen Soderland, G Matthew, Oren Broadhead, Etzioni, IJCAI. Michele Banko, Michael J. Cafarella, Stephen Soder- land, Matthew G Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In IJCAI.\n\nSemantic parsing on freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, EMNLP. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013a. Semantic parsing on freebase from question-answer pairs. In EMNLP.\n\nSemantic parsing on freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013b. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing, pages 1533-1544.\n\nA large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, EMNLP. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP.\n\nIntroduction to the conll-2005 shared task: Semantic role labeling. Xavier Carreras, Llu\u00eds M\u00e0rquez, Proceedings of CONLL. CONLLXavier Carreras and Llu\u00eds M\u00e0rquez. 2005. Introduc- tion to the conll-2005 shared task: Semantic role la- beling. In Proceedings of CONLL, pages 152-164.\n\nA thorough examination of the cnn/daily mail reading comprehension task. Danqi Chen, Jason Bolton, Christopher D Manning, abs/1606.02858CoRRDanqi Chen, Jason Bolton, and Christopher D. Man- ning. 2016. A thorough examination of the cnn/daily mail reading comprehension task. CoRR, abs/1606.02858.\n\nLearning to interpret natural language navigation instructions from observations. L David, Raymond J Chen, Mooney, AAAI. 2David L Chen and Raymond J Mooney. 2011. Learn- ing to interpret natural language navigation instruc- tions from observations. In AAAI, volume 2, pages 1-2.\n\nQuac: Question answering in context. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Yejin Wen Tau Yih, Percy Choi, Luke S Liang, Zettlemoyer, EMNLP. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen tau Yih, Yejin Choi, Percy Liang, and Luke S. Zettle- moyer. 2018. Quac: Question answering in context. In EMNLP.\n\nSimple and effective multi-paragraph reading comprehension. Christopher Clark, Matt Gardner, ACL. Christopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehen- sion. In ACL.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, abs/1810.04805CoRRJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. CoRR, abs/1810.04805.\n\nStanford's graph-based neural dependency parser at the conll 2017 shared task. Timothy Dozat, Peng Qi, Christopher D Manning, CoNLL Shared Task. Timothy Dozat, Peng Qi, and Christopher D. Manning. 2017. Stanford's graph-based neural dependency parser at the conll 2017 shared task. In CoNLL Shared Task.\n\nAllennlp: A deep semantic natural language processing platform. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F Liu, Matthew Peters, Michael Schmitz, Luke S Zettlemoyer, Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke S. Zettlemoyer. 2017. Allennlp: A deep semantic natural language processing platform.\n\nAnnotation artifacts in natural language inference data. Swabha Suchin Gururangan, Omer Swayamdipta, Roy Levy, Samuel Schwartz, Noah A Bowman, Smith, Proc. of NAACL. of NAACLSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural lan- guage inference data. In Proc. of NAACL.\n\nDeep semantic role labeling: What works and what's next. Luheng He, Kenton Lee, Mike Lewis, Luke S Zettlemoyer, ACL. Luheng He, Kenton Lee, Mike Lewis, and Luke S. Zettlemoyer. 2017. Deep semantic role labeling: What works and what's next. In ACL.\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension. S Mandar, Eunsol Joshi, Daniel S Choi, Luke S Weld, Zettlemoyer, ACL. Mandar S. Joshi, Eunsol Choi, Daniel S. Weld, and Luke S. Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL.\n\nHow much reading does reading comprehension require? a critical investigation of popular benchmarks. Divyansh Kaushik, Zachary Chase Lipton, EMNLP. Divyansh Kaushik and Zachary Chase Lipton. 2018. How much reading does reading comprehension re- quire? a critical investigation of popular bench- marks. In EMNLP.\n\nLooking beyond the surface: A challenge set for reading comprehension over multiple sentences. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, Dan Roth, NAACL-HLT. Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In NAACL- HLT.\n\nThe narrativeqa reading comprehension challenge. Tom\u00e1s Kocisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, Edward Grefenstette, TACL. 6Tom\u00e1s Kocisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. TACL, 6:317-328.\n\nParsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas Ang, TACL. 3Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equa- tions. TACL, 3:585-597.\n\nNeural semantic parsing with type constraints for semi-structured tables. Jayant Krishnamurthy, Pradeep Dasigi, Matt Gardner, EMNLP. Jayant Krishnamurthy, Pradeep Dasigi, and Matt Gard- ner. 2017. Neural semantic parsing with type con- straints for semi-structured tables. In EMNLP.\n\nNeural symbolic machines: Learning semantic parsers on freebase with weak supervision. Chen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, Ni Lao, ACL. Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, and Ni Lao. 2017. Neural symbolic ma- chines: Learning semantic parsers on freebase with weak supervision. In ACL.\n\nProgram induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, ACL. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun- som. 2017. Program induction by rationale genera- tion: Learning to solve and explain algebraic word problems. In ACL.\n\nThe stanford typed dependencies representation. Marie-Catherine De Marneffe, Christopher D Manning, CFCFPE@COLING. Marie-Catherine de Marneffe and Christopher D. Man- ning. 2008. The stanford typed dependencies repre- sentation. In CFCFPE@COLING.\n\nCan a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, EMNLP. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct elec- tricity? a new dataset for open book question answer- ing. In EMNLP.\n\nAdversarially regularising neural nli models to integrate logical background knowledge. Pasquale Minervini, Sebastian Riedel, In CoNLLPasquale Minervini and Sebastian Riedel. 2018. Ad- versarially regularising neural nli models to integrate logical background knowledge. In CoNLL.\n\nNeural programmer: Inducing latent programs with gradient descent. Arvind Neelakantan, V Quoc, Ilya Le, Sutskever, abs/1511.04834CoRRArvind Neelakantan, Quoc V. Le, and Ilya Sutskever. 2015. Neural programmer: Inducing la- tent programs with gradient descent. CoRR, abs/1511.04834.\n\n. Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan, Raffaella Pham, Sandro Bernardi, Marco Pezzelle, Gemma Baroni, Boleda, and Raquel Fern\u00e1ndez. 2016. The lambada dataset: Word prediction requiring a broad discourse context. CoRR, abs/1606.06031Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazari- dou, Quan Ngoc Pham, Raffaella Bernardi, San- dro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. 2016. The lambada dataset: Word prediction requiring a broad discourse context. CoRR, abs/1606.06031.\n\nCompositional semantic parsing on semi-structured tables. Panupong Pasupat, Percy Liang, ACL. Panupong Pasupat and Percy Liang. 2015. Composi- tional semantic parsing on semi-structured tables. In ACL.\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, EMNLP. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word rep- resentation. In EMNLP.\n\nDeep contextualized word representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, NAACL-HLT. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In NAACL-HLT.\n\nKnow what you don't know: Unanswerable questions for squad. Pranav Rajpurkar, Robin Jia, Percy Liang, ACL. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable ques- tions for squad. In ACL.\n\nSquad: 100, 000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, EMNLP. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, 000+ questions for machine comprehension of text. In EMNLP.\n\nCoqa: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, abs/1808.07042CoRRSiva Reddy, Danqi Chen, and Christopher D. Manning. 2018. Coqa: A conversational question answering challenge. CoRR, abs/1808.07042.\n\nNeural programmer-interpreters. E Scott, abs/1511.06279CoRRReed and Nando de FreitasScott E. Reed and Nando de Freitas. 2015. Neural programmer-interpreters. CoRR, abs/1511.06279.\n\nDuorc: Towards complex language understanding with paraphrased reading comprehension. Amrita Saha, Rahul Aralikatte, M Mitesh, Karthik Khapra, Sankaranarayanan, ACL. Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and Karthik Sankaranarayanan. 2018. Duorc: Towards complex language understanding with paraphrased reading comprehension. In ACL.\n\nBidirectional attention flow for machine comprehension. Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi, abs/1611.01603CoRRMin Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional at- tention flow for machine comprehension. CoRR, abs/1611.01603.\n\nSupervised open information extraction. Gabriel Stanovsky, Julian Michael, Luke S Zettlemoyer, Ido Dagan, NAACL-HLT. Gabriel Stanovsky, Julian Michael, Luke S. Zettle- moyer, and Ido Dagan. 2018. Supervised open in- formation extraction. In NAACL-HLT.\n\nConstructing datasets for multi-hop reading comprehension across documents. Johannes Welbl, Pontus Stenetorp, Sebastian Riedel, TACL. 6Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. TACL, 6:287-302.\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, EMNLP. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answer- ing. In EMNLP.\n\nA syntactic neural model for general-purpose code generation. Pengcheng Yin, Graham Neubig, ACL'17. Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. In ACL'17.\n\n. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, V Quoc, Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V.\n\nQanet: Combining local convolution with global self-attention for reading comprehension. Le, abs/1804.09541CoRRLe. 2018. Qanet: Combining local convolution with global self-attention for reading comprehen- sion. CoRR, abs/1804.09541.\n\nLearning to parse database queries using inductive logic programming. M John, Raymond J Zelle, Mooney, AAAI/IAAI. 2John M. Zelle and Raymond J. Mooney. 1996. Learn- ing to parse database queries using inductive logic programming. In AAAI/IAAI, Vol. 2.\n\nFrom recognition to cognition: Visual commonsense reasoning. Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi, abs/1811.10830CoRRRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2018a. From recognition to cognition: Visual commonsense reasoning. CoRR, abs/1811.10830.\n\nSwag: A large-scale adversarial dataset for grounded commonsense inference. Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi, EMNLP. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018b. Swag: A large-scale adversarial dataset for grounded commonsense inference. In EMNLP.\n\nLearning to map sentences to logical form: Structured classification with probabilistic categorial grammars. Luke S Zettlemoyer, Michael Collins, UAI. Luke S. Zettlemoyer and Michael Collins. 2005. Learn- ing to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI.\n\nRecord: Bridging the gap between human and machine commonsense reading comprehension. Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme, abs/1810.12885CoRRSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and ma- chine commonsense reading comprehension. CoRR, abs/1810.12885.\n", "annotations": {"author": "[{\"end\":148,\"start\":88},{\"end\":194,\"start\":149},{\"end\":197,\"start\":195},{\"end\":279,\"start\":198},{\"end\":364,\"start\":280},{\"end\":414,\"start\":365},{\"end\":417,\"start\":415},{\"end\":496,\"start\":418}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":95},{\"end\":161,\"start\":157},{\"end\":212,\"start\":206},{\"end\":297,\"start\":288},{\"end\":377,\"start\":372},{\"end\":430,\"start\":423}]", "author_first_name": "[{\"end\":94,\"start\":88},{\"end\":156,\"start\":149},{\"end\":196,\"start\":195},{\"end\":205,\"start\":198},{\"end\":287,\"start\":280},{\"end\":371,\"start\":365},{\"end\":416,\"start\":415},{\"end\":422,\"start\":418}]", "author_affiliation": "[{\"end\":147,\"start\":113},{\"end\":193,\"start\":163},{\"end\":278,\"start\":214},{\"end\":363,\"start\":299},{\"end\":413,\"start\":379},{\"end\":495,\"start\":432}]", "title": "[{\"end\":85,\"start\":1},{\"end\":581,\"start\":497}]", "venue": null, "abstract": "[{\"end\":1731,\"start\":583}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2041,\"start\":2022},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2061,\"start\":2041},{\"end\":2337,\"start\":2318},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3889,\"start\":3871},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6163,\"start\":6139},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6368,\"start\":6348},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6386,\"start\":6368},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6436,\"start\":6416},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6454,\"start\":6436},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6509,\"start\":6490},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6530,\"start\":6509},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6553,\"start\":6530},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6623,\"start\":6600},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6716,\"start\":6696},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6738,\"start\":6716},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7537,\"start\":7505},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7555,\"start\":7537},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7917,\"start\":7893},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7947,\"start\":7917},{\"end\":7980,\"start\":7947},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8136,\"start\":8111},{\"end\":8711,\"start\":8689},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8738,\"start\":8711},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8760,\"start\":8738},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8782,\"start\":8760},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9281,\"start\":9256},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9306,\"start\":9287},{\"end\":11306,\"start\":11305},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15340,\"start\":15309},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15361,\"start\":15340},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15382,\"start\":15361},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15390,\"start\":15382},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15682,\"start\":15655},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15757,\"start\":15732},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16033,\"start\":16014},{\"end\":16190,\"start\":16157},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17182,\"start\":17157},{\"end\":17215,\"start\":17214},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17869,\"start\":17851},{\"end\":17986,\"start\":17969},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18219,\"start\":18198},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18289,\"start\":18268},{\"end\":18709,\"start\":18707},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18937,\"start\":18912},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18962,\"start\":18937},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19052,\"start\":19029},{\"end\":19082,\"start\":19052},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24282,\"start\":24260},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33788,\"start\":33769},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33811,\"start\":33788},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33827,\"start\":33811}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28109,\"start\":27998},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28257,\"start\":28110},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31584,\"start\":28258},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31753,\"start\":31585},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31813,\"start\":31754},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32025,\"start\":31814},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32853,\"start\":32026},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":33071,\"start\":32854},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":33220,\"start\":33072}]", "paragraph": "[{\"end\":2532,\"start\":1747},{\"end\":3030,\"start\":2534},{\"end\":3484,\"start\":3032},{\"end\":4354,\"start\":3486},{\"end\":5123,\"start\":4356},{\"end\":5546,\"start\":5125},{\"end\":5854,\"start\":5548},{\"end\":6002,\"start\":5856},{\"end\":7409,\"start\":6019},{\"end\":7694,\"start\":7411},{\"end\":8566,\"start\":7696},{\"end\":8991,\"start\":8568},{\"end\":9132,\"start\":8993},{\"end\":9307,\"start\":9134},{\"end\":9781,\"start\":9332},{\"end\":10292,\"start\":9783},{\"end\":10753,\"start\":10294},{\"end\":11680,\"start\":10755},{\"end\":12172,\"start\":11682},{\"end\":12379,\"start\":12174},{\"end\":12803,\"start\":12381},{\"end\":13087,\"start\":12826},{\"end\":13843,\"start\":13089},{\"end\":14034,\"start\":13845},{\"end\":14743,\"start\":14036},{\"end\":15041,\"start\":14764},{\"end\":15758,\"start\":15062},{\"end\":16497,\"start\":15760},{\"end\":17322,\"start\":16499},{\"end\":17730,\"start\":17324},{\"end\":18866,\"start\":17768},{\"end\":19275,\"start\":18890},{\"end\":19622,\"start\":19277},{\"end\":20027,\"start\":19624},{\"end\":20430,\"start\":20047},{\"end\":21043,\"start\":20432},{\"end\":21550,\"start\":21065},{\"end\":21863,\"start\":21552},{\"end\":22015,\"start\":21944},{\"end\":22111,\"start\":22017},{\"end\":22247,\"start\":22113},{\"end\":22379,\"start\":22286},{\"end\":22568,\"start\":22472},{\"end\":22816,\"start\":22570},{\"end\":23262,\"start\":22849},{\"end\":23739,\"start\":23264},{\"end\":23991,\"start\":23813},{\"end\":24553,\"start\":24022},{\"end\":25345,\"start\":24580},{\"end\":25739,\"start\":25347},{\"end\":26280,\"start\":25741},{\"end\":26802,\"start\":26282},{\"end\":27426,\"start\":26817},{\"end\":27464,\"start\":27428},{\"end\":27997,\"start\":27491}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":21905,\"start\":21864},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21943,\"start\":21905},{\"attributes\":{\"id\":\"formula_2\"},\"end\":22272,\"start\":22248},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22285,\"start\":22272},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22471,\"start\":22380},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22848,\"start\":22817},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23774,\"start\":23740},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23812,\"start\":23774}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":2634,\"start\":2627},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":4576,\"start\":4569},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":4811,\"start\":4804},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":5045,\"start\":5038},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":10984,\"start\":10977},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12985,\"start\":12978},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13314,\"start\":13307},{\"end\":13752,\"start\":13745},{\"end\":14522,\"start\":14515},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24660,\"start\":24653},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":25417,\"start\":25410},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":26756,\"start\":26749}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1745,\"start\":1733},{\"attributes\":{\"n\":\"2\"},\"end\":6017,\"start\":6005},{\"attributes\":{\"n\":\"3\"},\"end\":9330,\"start\":9310},{\"attributes\":{\"n\":\"4\"},\"end\":12824,\"start\":12806},{\"attributes\":{\"n\":\"5\"},\"end\":14762,\"start\":14746},{\"attributes\":{\"n\":\"5.1\"},\"end\":15060,\"start\":15044},{\"attributes\":{\"n\":\"5.2\"},\"end\":17766,\"start\":17733},{\"attributes\":{\"n\":\"5.3\"},\"end\":18888,\"start\":18869},{\"attributes\":{\"n\":\"6\"},\"end\":20045,\"start\":20030},{\"attributes\":{\"n\":\"6.1\"},\"end\":21063,\"start\":21046},{\"attributes\":{\"n\":\"6.2\"},\"end\":24020,\"start\":23994},{\"attributes\":{\"n\":\"7\"},\"end\":24578,\"start\":24556},{\"attributes\":{\"n\":\"8\"},\"end\":26815,\"start\":26805},{\"end\":27489,\"start\":27467},{\"end\":28009,\"start\":27999},{\"end\":28131,\"start\":28111},{\"end\":31595,\"start\":31586},{\"end\":31764,\"start\":31755},{\"end\":31824,\"start\":31815},{\"end\":32864,\"start\":32855},{\"end\":33082,\"start\":33073}]", "table": "[{\"end\":31584,\"start\":28718},{\"end\":32025,\"start\":31826},{\"end\":32853,\"start\":32513}]", "figure_caption": "[{\"end\":28109,\"start\":28011},{\"end\":28257,\"start\":28134},{\"end\":28718,\"start\":28260},{\"end\":31753,\"start\":31597},{\"end\":31813,\"start\":31766},{\"end\":32513,\"start\":32028},{\"end\":33071,\"start\":32866},{\"end\":33220,\"start\":33084}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":34773,\"start\":34766},{\"end\":34788,\"start\":34781},{\"end\":34790,\"start\":34789},{\"end\":34809,\"start\":34802},{\"end\":34822,\"start\":34821},{\"end\":34836,\"start\":34832},{\"end\":35085,\"start\":35077},{\"end\":35100,\"start\":35094},{\"end\":35110,\"start\":35107},{\"end\":35125,\"start\":35120},{\"end\":35340,\"start\":35332},{\"end\":35355,\"start\":35349},{\"end\":35365,\"start\":35362},{\"end\":35380,\"start\":35375},{\"end\":35849,\"start\":35848},{\"end\":35863,\"start\":35858},{\"end\":35883,\"start\":35872},{\"end\":35903,\"start\":35892},{\"end\":35905,\"start\":35904},{\"end\":36167,\"start\":36161},{\"end\":36183,\"start\":36178},{\"end\":36452,\"start\":36447},{\"end\":36464,\"start\":36459},{\"end\":36484,\"start\":36473},{\"end\":36486,\"start\":36485},{\"end\":36755,\"start\":36754},{\"end\":36772,\"start\":36763},{\"end\":36995,\"start\":36989},{\"end\":37004,\"start\":37002},{\"end\":37014,\"start\":37009},{\"end\":37026,\"start\":37022},{\"end\":37041,\"start\":37036},{\"end\":37060,\"start\":37055},{\"end\":37071,\"start\":37067},{\"end\":37073,\"start\":37072},{\"end\":37338,\"start\":37327},{\"end\":37350,\"start\":37346},{\"end\":37565,\"start\":37560},{\"end\":37582,\"start\":37574},{\"end\":37596,\"start\":37590},{\"end\":37610,\"start\":37602},{\"end\":37905,\"start\":37898},{\"end\":37917,\"start\":37913},{\"end\":37933,\"start\":37922},{\"end\":37935,\"start\":37934},{\"end\":38192,\"start\":38188},{\"end\":38206,\"start\":38202},{\"end\":38217,\"start\":38213},{\"end\":38233,\"start\":38227},{\"end\":38250,\"start\":38243},{\"end\":38265,\"start\":38259},{\"end\":38267,\"start\":38266},{\"end\":38280,\"start\":38273},{\"end\":38296,\"start\":38289},{\"end\":38310,\"start\":38306},{\"end\":38312,\"start\":38311},{\"end\":38604,\"start\":38598},{\"end\":38628,\"start\":38624},{\"end\":38645,\"start\":38642},{\"end\":38658,\"start\":38652},{\"end\":38673,\"start\":38669},{\"end\":38675,\"start\":38674},{\"end\":38961,\"start\":38955},{\"end\":38972,\"start\":38966},{\"end\":38982,\"start\":38978},{\"end\":38994,\"start\":38990},{\"end\":38996,\"start\":38995},{\"end\":39238,\"start\":39237},{\"end\":39253,\"start\":39247},{\"end\":39267,\"start\":39261},{\"end\":39269,\"start\":39268},{\"end\":39280,\"start\":39276},{\"end\":39282,\"start\":39281},{\"end\":39592,\"start\":39584},{\"end\":39609,\"start\":39602},{\"end\":39615,\"start\":39610},{\"end\":39897,\"start\":39891},{\"end\":39915,\"start\":39908},{\"end\":39935,\"start\":39928},{\"end\":39947,\"start\":39942},{\"end\":39961,\"start\":39958},{\"end\":40231,\"start\":40226},{\"end\":40249,\"start\":40241},{\"end\":40263,\"start\":40259},{\"end\":40278,\"start\":40273},{\"end\":40289,\"start\":40285},{\"end\":40296,\"start\":40290},{\"end\":40311,\"start\":40306},{\"end\":40325,\"start\":40319},{\"end\":40589,\"start\":40586},{\"end\":40617,\"start\":40609},{\"end\":40636,\"start\":40630},{\"end\":40652,\"start\":40648},{\"end\":40667,\"start\":40662},{\"end\":40937,\"start\":40931},{\"end\":40960,\"start\":40953},{\"end\":40973,\"start\":40969},{\"end\":41232,\"start\":41228},{\"end\":41248,\"start\":41240},{\"end\":41261,\"start\":41257},{\"end\":41273,\"start\":41266},{\"end\":41275,\"start\":41274},{\"end\":41286,\"start\":41284},{\"end\":41572,\"start\":41568},{\"end\":41583,\"start\":41579},{\"end\":41599,\"start\":41594},{\"end\":41610,\"start\":41606},{\"end\":41861,\"start\":41846},{\"end\":41886,\"start\":41875},{\"end\":41888,\"start\":41887},{\"end\":42140,\"start\":42135},{\"end\":42156,\"start\":42151},{\"end\":42170,\"start\":42164},{\"end\":42183,\"start\":42177},{\"end\":42472,\"start\":42464},{\"end\":42493,\"start\":42484},{\"end\":42731,\"start\":42725},{\"end\":42746,\"start\":42745},{\"end\":42757,\"start\":42753},{\"end\":42948,\"start\":42943},{\"end\":42964,\"start\":42958},{\"end\":42985,\"start\":42977},{\"end\":43001,\"start\":42997},{\"end\":43017,\"start\":43008},{\"end\":43030,\"start\":43024},{\"end\":43046,\"start\":43041},{\"end\":43062,\"start\":43057},{\"end\":43531,\"start\":43523},{\"end\":43546,\"start\":43541},{\"end\":43722,\"start\":43715},{\"end\":43742,\"start\":43735},{\"end\":43762,\"start\":43751},{\"end\":43764,\"start\":43763},{\"end\":43960,\"start\":43953},{\"end\":43962,\"start\":43961},{\"end\":43975,\"start\":43971},{\"end\":43990,\"start\":43985},{\"end\":44002,\"start\":43998},{\"end\":44023,\"start\":44012},{\"end\":44037,\"start\":44031},{\"end\":44047,\"start\":44043},{\"end\":44316,\"start\":44310},{\"end\":44333,\"start\":44328},{\"end\":44344,\"start\":44339},{\"end\":44548,\"start\":44542},{\"end\":44564,\"start\":44560},{\"end\":44582,\"start\":44572},{\"end\":44597,\"start\":44592},{\"end\":44815,\"start\":44811},{\"end\":44828,\"start\":44823},{\"end\":44846,\"start\":44835},{\"end\":44848,\"start\":44847},{\"end\":45043,\"start\":45042},{\"end\":45283,\"start\":45277},{\"end\":45295,\"start\":45290},{\"end\":45309,\"start\":45308},{\"end\":45325,\"start\":45318},{\"end\":45601,\"start\":45593},{\"end\":45616,\"start\":45607},{\"end\":45630,\"start\":45627},{\"end\":45648,\"start\":45640},{\"end\":45885,\"start\":45878},{\"end\":45903,\"start\":45897},{\"end\":45917,\"start\":45913},{\"end\":45919,\"start\":45918},{\"end\":45936,\"start\":45933},{\"end\":46175,\"start\":46167},{\"end\":46189,\"start\":46183},{\"end\":46210,\"start\":46201},{\"end\":46463,\"start\":46457},{\"end\":46474,\"start\":46470},{\"end\":46487,\"start\":46479},{\"end\":46501,\"start\":46495},{\"end\":46517,\"start\":46510},{\"end\":46519,\"start\":46518},{\"end\":46533,\"start\":46527},{\"end\":46560,\"start\":46549},{\"end\":46562,\"start\":46561},{\"end\":46867,\"start\":46858},{\"end\":46879,\"start\":46873},{\"end\":47016,\"start\":47011},{\"end\":47020,\"start\":47017},{\"end\":47030,\"start\":47025},{\"end\":47048,\"start\":47038},{\"end\":47059,\"start\":47056},{\"end\":47069,\"start\":47066},{\"end\":47084,\"start\":47076},{\"end\":47095,\"start\":47094},{\"end\":47504,\"start\":47503},{\"end\":47518,\"start\":47511},{\"end\":47520,\"start\":47519},{\"end\":47752,\"start\":47747},{\"end\":47769,\"start\":47762},{\"end\":47779,\"start\":47776},{\"end\":47794,\"start\":47789},{\"end\":48049,\"start\":48044},{\"end\":48066,\"start\":48059},{\"end\":48076,\"start\":48073},{\"end\":48092,\"start\":48087},{\"end\":48372,\"start\":48368},{\"end\":48374,\"start\":48373},{\"end\":48395,\"start\":48388},{\"end\":48668,\"start\":48663},{\"end\":48684,\"start\":48676},{\"end\":48698,\"start\":48690},{\"end\":48712,\"start\":48704},{\"end\":48723,\"start\":48718},{\"end\":48737,\"start\":48729}]", "bib_author_last_name": "[{\"end\":34779,\"start\":34774},{\"end\":34800,\"start\":34791},{\"end\":34819,\"start\":34810},{\"end\":34830,\"start\":34823},{\"end\":34846,\"start\":34837},{\"end\":34855,\"start\":34848},{\"end\":35092,\"start\":35086},{\"end\":35105,\"start\":35101},{\"end\":35118,\"start\":35111},{\"end\":35131,\"start\":35126},{\"end\":35347,\"start\":35341},{\"end\":35360,\"start\":35356},{\"end\":35373,\"start\":35366},{\"end\":35386,\"start\":35381},{\"end\":35856,\"start\":35850},{\"end\":35870,\"start\":35864},{\"end\":35890,\"start\":35884},{\"end\":35911,\"start\":35906},{\"end\":35920,\"start\":35913},{\"end\":36176,\"start\":36168},{\"end\":36191,\"start\":36184},{\"end\":36457,\"start\":36453},{\"end\":36471,\"start\":36465},{\"end\":36494,\"start\":36487},{\"end\":36761,\"start\":36756},{\"end\":36777,\"start\":36773},{\"end\":36785,\"start\":36779},{\"end\":37000,\"start\":36996},{\"end\":37007,\"start\":37005},{\"end\":37020,\"start\":37015},{\"end\":37034,\"start\":37027},{\"end\":37053,\"start\":37042},{\"end\":37065,\"start\":37061},{\"end\":37079,\"start\":37074},{\"end\":37092,\"start\":37081},{\"end\":37344,\"start\":37339},{\"end\":37358,\"start\":37351},{\"end\":37572,\"start\":37566},{\"end\":37588,\"start\":37583},{\"end\":37600,\"start\":37597},{\"end\":37620,\"start\":37611},{\"end\":37911,\"start\":37906},{\"end\":37920,\"start\":37918},{\"end\":37943,\"start\":37936},{\"end\":38200,\"start\":38193},{\"end\":38211,\"start\":38207},{\"end\":38225,\"start\":38218},{\"end\":38241,\"start\":38234},{\"end\":38257,\"start\":38251},{\"end\":38271,\"start\":38268},{\"end\":38287,\"start\":38281},{\"end\":38304,\"start\":38297},{\"end\":38324,\"start\":38313},{\"end\":38622,\"start\":38605},{\"end\":38640,\"start\":38629},{\"end\":38650,\"start\":38646},{\"end\":38667,\"start\":38659},{\"end\":38682,\"start\":38676},{\"end\":38689,\"start\":38684},{\"end\":38964,\"start\":38962},{\"end\":38976,\"start\":38973},{\"end\":38988,\"start\":38983},{\"end\":39008,\"start\":38997},{\"end\":39245,\"start\":39239},{\"end\":39259,\"start\":39254},{\"end\":39274,\"start\":39270},{\"end\":39287,\"start\":39283},{\"end\":39300,\"start\":39289},{\"end\":39600,\"start\":39593},{\"end\":39622,\"start\":39616},{\"end\":39906,\"start\":39898},{\"end\":39926,\"start\":39916},{\"end\":39940,\"start\":39936},{\"end\":39956,\"start\":39948},{\"end\":39966,\"start\":39962},{\"end\":40239,\"start\":40232},{\"end\":40257,\"start\":40250},{\"end\":40271,\"start\":40264},{\"end\":40283,\"start\":40279},{\"end\":40304,\"start\":40297},{\"end\":40317,\"start\":40312},{\"end\":40338,\"start\":40326},{\"end\":40607,\"start\":40590},{\"end\":40628,\"start\":40618},{\"end\":40646,\"start\":40637},{\"end\":40660,\"start\":40653},{\"end\":40677,\"start\":40668},{\"end\":40951,\"start\":40938},{\"end\":40967,\"start\":40961},{\"end\":40981,\"start\":40974},{\"end\":41238,\"start\":41233},{\"end\":41255,\"start\":41249},{\"end\":41264,\"start\":41262},{\"end\":41282,\"start\":41276},{\"end\":41290,\"start\":41287},{\"end\":41577,\"start\":41573},{\"end\":41592,\"start\":41584},{\"end\":41604,\"start\":41600},{\"end\":41618,\"start\":41611},{\"end\":41873,\"start\":41862},{\"end\":41896,\"start\":41889},{\"end\":42149,\"start\":42141},{\"end\":42162,\"start\":42157},{\"end\":42175,\"start\":42171},{\"end\":42193,\"start\":42184},{\"end\":42482,\"start\":42473},{\"end\":42500,\"start\":42494},{\"end\":42743,\"start\":42732},{\"end\":42751,\"start\":42747},{\"end\":42760,\"start\":42758},{\"end\":42771,\"start\":42762},{\"end\":42956,\"start\":42949},{\"end\":42975,\"start\":42965},{\"end\":42995,\"start\":42986},{\"end\":43006,\"start\":43002},{\"end\":43022,\"start\":43018},{\"end\":43039,\"start\":43031},{\"end\":43055,\"start\":43047},{\"end\":43069,\"start\":43063},{\"end\":43077,\"start\":43071},{\"end\":43539,\"start\":43532},{\"end\":43552,\"start\":43547},{\"end\":43733,\"start\":43723},{\"end\":43749,\"start\":43743},{\"end\":43772,\"start\":43765},{\"end\":43969,\"start\":43963},{\"end\":43983,\"start\":43976},{\"end\":43996,\"start\":43991},{\"end\":44010,\"start\":44003},{\"end\":44029,\"start\":44024},{\"end\":44041,\"start\":44038},{\"end\":44059,\"start\":44048},{\"end\":44326,\"start\":44317},{\"end\":44337,\"start\":44334},{\"end\":44350,\"start\":44345},{\"end\":44558,\"start\":44549},{\"end\":44570,\"start\":44565},{\"end\":44590,\"start\":44583},{\"end\":44603,\"start\":44598},{\"end\":44821,\"start\":44816},{\"end\":44833,\"start\":44829},{\"end\":44856,\"start\":44849},{\"end\":45049,\"start\":45044},{\"end\":45288,\"start\":45284},{\"end\":45306,\"start\":45296},{\"end\":45316,\"start\":45310},{\"end\":45332,\"start\":45326},{\"end\":45350,\"start\":45334},{\"end\":45605,\"start\":45602},{\"end\":45625,\"start\":45617},{\"end\":45638,\"start\":45631},{\"end\":45659,\"start\":45649},{\"end\":45895,\"start\":45886},{\"end\":45911,\"start\":45904},{\"end\":45931,\"start\":45920},{\"end\":45942,\"start\":45937},{\"end\":46181,\"start\":46176},{\"end\":46199,\"start\":46190},{\"end\":46217,\"start\":46211},{\"end\":46468,\"start\":46464},{\"end\":46477,\"start\":46475},{\"end\":46493,\"start\":46488},{\"end\":46508,\"start\":46502},{\"end\":46525,\"start\":46520},{\"end\":46547,\"start\":46534},{\"end\":46570,\"start\":46563},{\"end\":46871,\"start\":46868},{\"end\":46886,\"start\":46880},{\"end\":47023,\"start\":47021},{\"end\":47036,\"start\":47031},{\"end\":47054,\"start\":47049},{\"end\":47064,\"start\":47060},{\"end\":47074,\"start\":47070},{\"end\":47092,\"start\":47085},{\"end\":47100,\"start\":47096},{\"end\":47289,\"start\":47287},{\"end\":47509,\"start\":47505},{\"end\":47526,\"start\":47521},{\"end\":47534,\"start\":47528},{\"end\":47760,\"start\":47753},{\"end\":47774,\"start\":47770},{\"end\":47787,\"start\":47780},{\"end\":47799,\"start\":47795},{\"end\":48057,\"start\":48050},{\"end\":48071,\"start\":48067},{\"end\":48085,\"start\":48077},{\"end\":48097,\"start\":48093},{\"end\":48386,\"start\":48375},{\"end\":48403,\"start\":48396},{\"end\":48674,\"start\":48669},{\"end\":48688,\"start\":48685},{\"end\":48702,\"start\":48699},{\"end\":48716,\"start\":48713},{\"end\":48727,\"start\":48724},{\"end\":48747,\"start\":48738}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207169186},\"end\":35018,\"start\":34724},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6401679},\"end\":35273,\"start\":35020},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6401679},\"end\":35780,\"start\":35275},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14604520},\"end\":36091,\"start\":35782},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16509032},\"end\":36372,\"start\":36093},{\"attributes\":{\"doi\":\"abs/1606.02858\",\"id\":\"b5\"},\"end\":36670,\"start\":36374},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":215717032},\"end\":36950,\"start\":36672},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52057510},\"end\":37265,\"start\":36952},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":223637},\"end\":37476,\"start\":37267},{\"attributes\":{\"doi\":\"abs/1810.04805\",\"id\":\"b9\"},\"end\":37817,\"start\":37478},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":28616370},\"end\":38122,\"start\":37819},{\"attributes\":{\"id\":\"b11\"},\"end\":38539,\"start\":38124},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4537113},\"end\":38896,\"start\":38541},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":33626727},\"end\":39145,\"start\":38898},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":26501419},\"end\":39481,\"start\":39147},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52011616},\"end\":39794,\"start\":39483},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5112038},\"end\":40175,\"start\":39796},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2593903},\"end\":40536,\"start\":40177},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4894130},\"end\":40855,\"start\":40538},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1675452},\"end\":41139,\"start\":40857},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2742513},\"end\":41468,\"start\":41141},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12777818},\"end\":41796,\"start\":41470},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3542573},\"end\":42044,\"start\":41798},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52183757},\"end\":42374,\"start\":42046},{\"attributes\":{\"id\":\"b24\"},\"end\":42656,\"start\":42376},{\"attributes\":{\"doi\":\"abs/1511.04834\",\"id\":\"b25\"},\"end\":42939,\"start\":42658},{\"attributes\":{\"id\":\"b26\"},\"end\":43463,\"start\":42941},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9027681},\"end\":43666,\"start\":43465},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1957433},\"end\":43909,\"start\":43668},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3626819},\"end\":44248,\"start\":43911},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":47018994},\"end\":44478,\"start\":44250},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":11816014},\"end\":44756,\"start\":44480},{\"attributes\":{\"doi\":\"abs/1808.07042\",\"id\":\"b32\"},\"end\":45008,\"start\":44758},{\"attributes\":{\"doi\":\"abs/1511.06279\",\"id\":\"b33\"},\"end\":45189,\"start\":45010},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":5071138},\"end\":45535,\"start\":45191},{\"attributes\":{\"doi\":\"abs/1611.01603\",\"id\":\"b35\"},\"end\":45836,\"start\":45537},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":44145304},\"end\":46089,\"start\":45838},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":9192723},\"end\":46380,\"start\":46091},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":52822214},\"end\":46794,\"start\":46382},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":12718048},\"end\":47007,\"start\":46796},{\"attributes\":{\"id\":\"b40\"},\"end\":47196,\"start\":47009},{\"attributes\":{\"doi\":\"abs/1804.09541\",\"id\":\"b41\"},\"end\":47431,\"start\":47198},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":263135},\"end\":47684,\"start\":47433},{\"attributes\":{\"doi\":\"abs/1811.10830\",\"id\":\"b43\"},\"end\":47966,\"start\":47686},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":52019251},\"end\":48257,\"start\":47968},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":449252},\"end\":48575,\"start\":48259},{\"attributes\":{\"doi\":\"abs/1810.12885\",\"id\":\"b46\"},\"end\":48972,\"start\":48577}]", "bib_title": "[{\"end\":34764,\"start\":34724},{\"end\":35075,\"start\":35020},{\"end\":35330,\"start\":35275},{\"end\":35846,\"start\":35782},{\"end\":36159,\"start\":36093},{\"end\":36752,\"start\":36672},{\"end\":36987,\"start\":36952},{\"end\":37325,\"start\":37267},{\"end\":37896,\"start\":37819},{\"end\":38596,\"start\":38541},{\"end\":38953,\"start\":38898},{\"end\":39235,\"start\":39147},{\"end\":39582,\"start\":39483},{\"end\":39889,\"start\":39796},{\"end\":40224,\"start\":40177},{\"end\":40584,\"start\":40538},{\"end\":40929,\"start\":40857},{\"end\":41226,\"start\":41141},{\"end\":41566,\"start\":41470},{\"end\":41844,\"start\":41798},{\"end\":42133,\"start\":42046},{\"end\":43521,\"start\":43465},{\"end\":43713,\"start\":43668},{\"end\":43951,\"start\":43911},{\"end\":44308,\"start\":44250},{\"end\":44540,\"start\":44480},{\"end\":45275,\"start\":45191},{\"end\":45876,\"start\":45838},{\"end\":46165,\"start\":46091},{\"end\":46455,\"start\":46382},{\"end\":46856,\"start\":46796},{\"end\":47501,\"start\":47433},{\"end\":48042,\"start\":47968},{\"end\":48366,\"start\":48259}]", "bib_author": "[{\"end\":34781,\"start\":34766},{\"end\":34802,\"start\":34781},{\"end\":34821,\"start\":34802},{\"end\":34832,\"start\":34821},{\"end\":34848,\"start\":34832},{\"end\":34857,\"start\":34848},{\"end\":35094,\"start\":35077},{\"end\":35107,\"start\":35094},{\"end\":35120,\"start\":35107},{\"end\":35133,\"start\":35120},{\"end\":35349,\"start\":35332},{\"end\":35362,\"start\":35349},{\"end\":35375,\"start\":35362},{\"end\":35388,\"start\":35375},{\"end\":35858,\"start\":35848},{\"end\":35872,\"start\":35858},{\"end\":35892,\"start\":35872},{\"end\":35913,\"start\":35892},{\"end\":35922,\"start\":35913},{\"end\":36178,\"start\":36161},{\"end\":36193,\"start\":36178},{\"end\":36459,\"start\":36447},{\"end\":36473,\"start\":36459},{\"end\":36496,\"start\":36473},{\"end\":36763,\"start\":36754},{\"end\":36779,\"start\":36763},{\"end\":36787,\"start\":36779},{\"end\":37002,\"start\":36989},{\"end\":37009,\"start\":37002},{\"end\":37022,\"start\":37009},{\"end\":37036,\"start\":37022},{\"end\":37055,\"start\":37036},{\"end\":37067,\"start\":37055},{\"end\":37081,\"start\":37067},{\"end\":37094,\"start\":37081},{\"end\":37346,\"start\":37327},{\"end\":37360,\"start\":37346},{\"end\":37574,\"start\":37560},{\"end\":37590,\"start\":37574},{\"end\":37602,\"start\":37590},{\"end\":37622,\"start\":37602},{\"end\":37913,\"start\":37898},{\"end\":37922,\"start\":37913},{\"end\":37945,\"start\":37922},{\"end\":38202,\"start\":38188},{\"end\":38213,\"start\":38202},{\"end\":38227,\"start\":38213},{\"end\":38243,\"start\":38227},{\"end\":38259,\"start\":38243},{\"end\":38273,\"start\":38259},{\"end\":38289,\"start\":38273},{\"end\":38306,\"start\":38289},{\"end\":38326,\"start\":38306},{\"end\":38624,\"start\":38598},{\"end\":38642,\"start\":38624},{\"end\":38652,\"start\":38642},{\"end\":38669,\"start\":38652},{\"end\":38684,\"start\":38669},{\"end\":38691,\"start\":38684},{\"end\":38966,\"start\":38955},{\"end\":38978,\"start\":38966},{\"end\":38990,\"start\":38978},{\"end\":39010,\"start\":38990},{\"end\":39247,\"start\":39237},{\"end\":39261,\"start\":39247},{\"end\":39276,\"start\":39261},{\"end\":39289,\"start\":39276},{\"end\":39302,\"start\":39289},{\"end\":39602,\"start\":39584},{\"end\":39624,\"start\":39602},{\"end\":39908,\"start\":39891},{\"end\":39928,\"start\":39908},{\"end\":39942,\"start\":39928},{\"end\":39958,\"start\":39942},{\"end\":39968,\"start\":39958},{\"end\":40241,\"start\":40226},{\"end\":40259,\"start\":40241},{\"end\":40273,\"start\":40259},{\"end\":40285,\"start\":40273},{\"end\":40306,\"start\":40285},{\"end\":40319,\"start\":40306},{\"end\":40340,\"start\":40319},{\"end\":40609,\"start\":40586},{\"end\":40630,\"start\":40609},{\"end\":40648,\"start\":40630},{\"end\":40662,\"start\":40648},{\"end\":40679,\"start\":40662},{\"end\":40953,\"start\":40931},{\"end\":40969,\"start\":40953},{\"end\":40983,\"start\":40969},{\"end\":41240,\"start\":41228},{\"end\":41257,\"start\":41240},{\"end\":41266,\"start\":41257},{\"end\":41284,\"start\":41266},{\"end\":41292,\"start\":41284},{\"end\":41579,\"start\":41568},{\"end\":41594,\"start\":41579},{\"end\":41606,\"start\":41594},{\"end\":41620,\"start\":41606},{\"end\":41875,\"start\":41846},{\"end\":41898,\"start\":41875},{\"end\":42151,\"start\":42135},{\"end\":42164,\"start\":42151},{\"end\":42177,\"start\":42164},{\"end\":42195,\"start\":42177},{\"end\":42484,\"start\":42464},{\"end\":42502,\"start\":42484},{\"end\":42745,\"start\":42725},{\"end\":42753,\"start\":42745},{\"end\":42762,\"start\":42753},{\"end\":42773,\"start\":42762},{\"end\":42958,\"start\":42943},{\"end\":42977,\"start\":42958},{\"end\":42997,\"start\":42977},{\"end\":43008,\"start\":42997},{\"end\":43024,\"start\":43008},{\"end\":43041,\"start\":43024},{\"end\":43057,\"start\":43041},{\"end\":43071,\"start\":43057},{\"end\":43079,\"start\":43071},{\"end\":43541,\"start\":43523},{\"end\":43554,\"start\":43541},{\"end\":43735,\"start\":43715},{\"end\":43751,\"start\":43735},{\"end\":43774,\"start\":43751},{\"end\":43971,\"start\":43953},{\"end\":43985,\"start\":43971},{\"end\":43998,\"start\":43985},{\"end\":44012,\"start\":43998},{\"end\":44031,\"start\":44012},{\"end\":44043,\"start\":44031},{\"end\":44061,\"start\":44043},{\"end\":44328,\"start\":44310},{\"end\":44339,\"start\":44328},{\"end\":44352,\"start\":44339},{\"end\":44560,\"start\":44542},{\"end\":44572,\"start\":44560},{\"end\":44592,\"start\":44572},{\"end\":44605,\"start\":44592},{\"end\":44823,\"start\":44811},{\"end\":44835,\"start\":44823},{\"end\":44858,\"start\":44835},{\"end\":45051,\"start\":45042},{\"end\":45290,\"start\":45277},{\"end\":45308,\"start\":45290},{\"end\":45318,\"start\":45308},{\"end\":45334,\"start\":45318},{\"end\":45352,\"start\":45334},{\"end\":45607,\"start\":45593},{\"end\":45627,\"start\":45607},{\"end\":45640,\"start\":45627},{\"end\":45661,\"start\":45640},{\"end\":45897,\"start\":45878},{\"end\":45913,\"start\":45897},{\"end\":45933,\"start\":45913},{\"end\":45944,\"start\":45933},{\"end\":46183,\"start\":46167},{\"end\":46201,\"start\":46183},{\"end\":46219,\"start\":46201},{\"end\":46470,\"start\":46457},{\"end\":46479,\"start\":46470},{\"end\":46495,\"start\":46479},{\"end\":46510,\"start\":46495},{\"end\":46527,\"start\":46510},{\"end\":46549,\"start\":46527},{\"end\":46572,\"start\":46549},{\"end\":46873,\"start\":46858},{\"end\":46888,\"start\":46873},{\"end\":47025,\"start\":47011},{\"end\":47038,\"start\":47025},{\"end\":47056,\"start\":47038},{\"end\":47066,\"start\":47056},{\"end\":47076,\"start\":47066},{\"end\":47094,\"start\":47076},{\"end\":47102,\"start\":47094},{\"end\":47291,\"start\":47287},{\"end\":47511,\"start\":47503},{\"end\":47528,\"start\":47511},{\"end\":47536,\"start\":47528},{\"end\":47762,\"start\":47747},{\"end\":47776,\"start\":47762},{\"end\":47789,\"start\":47776},{\"end\":47801,\"start\":47789},{\"end\":48059,\"start\":48044},{\"end\":48073,\"start\":48059},{\"end\":48087,\"start\":48073},{\"end\":48099,\"start\":48087},{\"end\":48388,\"start\":48368},{\"end\":48405,\"start\":48388},{\"end\":48676,\"start\":48663},{\"end\":48690,\"start\":48676},{\"end\":48704,\"start\":48690},{\"end\":48718,\"start\":48704},{\"end\":48729,\"start\":48718},{\"end\":48749,\"start\":48729}]", "bib_venue": "[{\"end\":34862,\"start\":34857},{\"end\":35138,\"start\":35133},{\"end\":35474,\"start\":35388},{\"end\":35927,\"start\":35922},{\"end\":36213,\"start\":36193},{\"end\":36445,\"start\":36374},{\"end\":36791,\"start\":36787},{\"end\":37099,\"start\":37094},{\"end\":37363,\"start\":37360},{\"end\":37558,\"start\":37478},{\"end\":37962,\"start\":37945},{\"end\":38186,\"start\":38124},{\"end\":38705,\"start\":38691},{\"end\":39013,\"start\":39010},{\"end\":39305,\"start\":39302},{\"end\":39629,\"start\":39624},{\"end\":39977,\"start\":39968},{\"end\":40344,\"start\":40340},{\"end\":40683,\"start\":40679},{\"end\":40988,\"start\":40983},{\"end\":41295,\"start\":41292},{\"end\":41623,\"start\":41620},{\"end\":41911,\"start\":41898},{\"end\":42200,\"start\":42195},{\"end\":42462,\"start\":42376},{\"end\":42723,\"start\":42658},{\"end\":43557,\"start\":43554},{\"end\":43779,\"start\":43774},{\"end\":44070,\"start\":44061},{\"end\":44355,\"start\":44352},{\"end\":44610,\"start\":44605},{\"end\":44809,\"start\":44758},{\"end\":45040,\"start\":45010},{\"end\":45355,\"start\":45352},{\"end\":45591,\"start\":45537},{\"end\":45953,\"start\":45944},{\"end\":46223,\"start\":46219},{\"end\":46577,\"start\":46572},{\"end\":46894,\"start\":46888},{\"end\":47285,\"start\":47198},{\"end\":47545,\"start\":47536},{\"end\":47745,\"start\":47686},{\"end\":48104,\"start\":48099},{\"end\":48408,\"start\":48405},{\"end\":48661,\"start\":48577},{\"end\":35547,\"start\":35476},{\"end\":36220,\"start\":36215},{\"end\":38715,\"start\":38707}]"}}}, "year": 2023, "month": 12, "day": 17}