{"id": 237635292, "updated": "2023-10-05 22:27:32.356", "metadata": {"title": "Modeling Dynamic Attributes for Next Basket Recommendation", "authors": "[{\"first\":\"Yongjun\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jia\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Chenghao\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Chenxi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Markus\",\"last\":\"Anderle\",\"middle\":[]},{\"first\":\"Julian\",\"last\":\"McAuley\",\"middle\":[]},{\"first\":\"Caiming\",\"last\":\"Xiong\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 9, "day": 23}, "abstract": "Traditional approaches to next item and next basket recommendation typically extract users' interests based on their past interactions and associated static contextual information (e.g. a user id or item category). However, extracted interests can be inaccurate and become obsolete. Dynamic attributes, such as user income changes, item price changes (etc.), change over time. Such dynamics can intrinsically reflect the evolution of users' interests. We argue that modeling such dynamic attributes can boost recommendation performance. However, properly integrating them into user interest models is challenging since attribute dynamics can be diverse such as time-interval aware, periodic patterns (etc.), and they represent users' behaviors from different perspectives, which can happen asynchronously with interactions. Besides dynamic attributes, items in each basket contain complex interdependencies which might be beneficial but nontrivial to effectively capture. To address these challenges, we propose a novel Attentive network to model Dynamic attributes (named AnDa). AnDa separately encodes dynamic attributes and basket item sequences. We design a periodic aware encoder to allow the model to capture various temporal patterns from dynamic attributes. To effectively learn useful item relationships, intra-basket attention module is proposed. Experimental results on three real-world datasets demonstrate that our method consistently outperforms the state-of-the-art.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2109.11654", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2109-11654", "doi": null}}, "content": {"source": {"pdf_hash": "27c4102b00a04681e5adbde4493006fd41628b35", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2109.11654v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "14f57b0e0b38dd8a001c404ef937d26f80157b78", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/27c4102b00a04681e5adbde4493006fd41628b35.txt", "contents": "\nModeling Dynamic Attributes for Next Basket Recommendation\n\n\nSalesforce, USAYongjun Chen \nSalesforce, USAMarkus Anderle \nUCJulian Mcauley \nUSASan Diego \nYongjun Chen \nJia Li \nChenghao Liu \nChenxi Li \nMarkus Anderle \nJulian Mcauley \nCaiming Xiong \n\nJIA LI\nSalesforceUSA\n\n\nCHENGHAO LIU\nSalesforceSingapore\n\n\nCHENXI LI\nSalesforceUSA\n\n\nCAIMING XIONG\nSalesforceUSA\n\nModeling Dynamic Attributes for Next Basket Recommendation\nACM Reference Format: 2021. Modeling Dynamic Attributes for Next Basket Recommendation. In CARS: Workshop on Context-Aware Recommender Systems (RecSys '21), October 2nd, 2021, Online, Worldwide. ACM, New York, NY, USA, 10 pages. https://doi.org/nn.nnnn/nnnnnnn.nnnnnnnCCS Concepts:Information systems \u2192 Recommender systems;Computing methodologies \u2192 Neural networks Additional Key Words and Phrases: Dynamic Attributes, Context Interaction Learning, Next Basket Recommendation\nTraditional approaches to next-item and next basket recommendation typically extract users' interests based on their past interactions and associated static contextual information (e.g. a user id or item category). However, extracted interests can be inaccurate and become obsolete. Dynamic attributes, such as user income changes, item price changes (etc.), change over time. Such dynamics can intrinsically reflect the evolution of users' interests. We argue that modeling such dynamic attributes can boost recommendation performance. However, properly integrating them into user interest models is challenging since attribute dynamics can be diverse such as time-interval aware, periodic patterns (etc.), and they represent users' behaviors from different perspectives, which can happen asynchronously with interactions. Besides dynamic attributes, items in each basket contain complex interdependencies which might be beneficial but nontrivial to effectively capture. To address these challenges, we propose a novel Attentive network to model Dynamic attributes (named AnDa). AnDa separately encodes dynamic attributes and basket item sequences. We design a periodic aware encoder to allow the model to capture various temporal patterns from dynamic attributes. To effectively learn useful item relationships, intra-basket attention module is proposed. Experimental results on three real-world datasets demonstrate that our method consistently outperforms the state-of-the-art.\n\nNext basket recommendation [16,28] assumes that users interact with multiple items during each round (i.e., a basket).\n\nThe goal is to recommend a basket of items that a user is likely to interact with at the next time step. In the next basket recommendation task, in addition to the sequential patterns underlying historical interactions, items in each basket , and context of users or items often provides useful information. .\n\nExisting solutions for next-basket recommendation tasks train a sequential recommender based on the users' interaction history [8,11,18,28], or with additional static contextual attributes [2] (product category, brand, etc.) to extract users' interests. However, extracted interests can be inaccurate and obsolete. Dynamic attributes, which change over time, appear in many applications, and can provide more accurate descriptions of the shift of a user's interests or changes in item properties. For example, in a bank product recommendation scenario (see Figure 1), products from the bank are recommended to customers. Given only a sequence of monthly records of a customer's products, the next basket recommended to the user is deterministic. Instead, dynamic attributes such as the household income and the customer membership type , which are changing overtime, help a recommender to beter capture a user's changing interests. Case 1 and 2 in Figure 1 illustrate that two customers with the same historical purchase behaviors but different household income and membership type sequences can have different interests.  Although it is essential to model dynamic attributes, properly integrating dynamic attributes into sequential models is challenging. First, the temporal patterns underlying dynamic attributes can be diverse. There can be time-interval (i.e., two items purchased with different time-intervals has different impact on users' future purchase behaviors.) and\n\nperiodic patterns (i.e., seasons, weekday/weekend patterns, etc.  To allow the model to capture time-interval aware and periodic patterns,   we propose an input encoder containing a time-aware padding and periodic index embedding to encode dynamic attributes. To capture complex item relationships in each basket, an intra-basket attentive module is introduced. It is applied to each basket item to extract useful item relationships. We conduct experiments on three real-world datasets for next basket recommendation. Our experimental results demonstrate that our proposed method significantly outperforms baseline approaches.\n\n\nRELATED WORK\n\n\nNext Basket Recommendation\n\nTo capture sequential patterns at a basket level for next basket recommendation, DREAM [28] encodes items in each basket using max and average pooling and learns the sequence representation through an RNN-based network. ANAM [2] improves upon DREAM by considering static item attributes using vanilla attention. Sets2Sets [8] views the task as a multiple baskets prediction and proposes a RNN based encoder-decoder method to improve the performance. To capturing item relationships at each basket, Beacon [11] defines pre-computed static item correlation information based on the co-occurring items in the observed training baskets and then incorporates it into an RNN-based model. IIANN [3] learns the correlation between the most recent basket items and the target item to summarize users' short-term interests. In this work, we use multi-head self-attention within each basket so that complex item interrelationships (e.g., co-occurrences) can be captured. MITGNN [15] focus on capturing users' intention information in each basket and across different users'. In comparison, our work focus on leverage dynamic attributes for providing more accurate user interests.\n\n\nFeature Interaction Learning\n\nFactorization Machines (FMs) [4,9,17,27] capture second-order feature interactions and have proven to be effective for recommendation [19]. With the success of deep neural networks (DNN), many works start to explore high-order feature interactions using DNN. NFM [7] combines FM with a DNN to model high-order feature interactions. The\n\nWide&Deep [5] model uses a wide part to model second-order interactions and a deep part to model the higher-order interactions. Different from Wide&Deep, Deep&Cross [25] uses a cross-product transformation to integrate features, and xDeepFM [13] proposes a CIN module to take the outer product at a vector-wise level. These works, which use DNN to capture high-order feature interactions implicitly, lack good explanation ability in general. To this end, AutoInt [21] uses a self-attention mechanism to model high-order interactions with a more precise explanation of the interacted features. Inspired by this, we also apply multi-head self-attention to learn higher-order feature interactions and item interrelationships in each basket.\n\n\nAttention Mechanisms\n\nWith the success of Transformer networks in machine translation tasks [24], purely attention-based models SASRec [10] is the first work that uses a pure self-attention mechanism to model sequential recommendation and demonstrates better performance than RNN-based methods. TiSASRec [12] extends SASRec using self-attention to model the time interval between two adjacent interactions. BERT4Rec [23] uses bidirectional self-attention with the Cloze objective.\n\nFDSA [29] further improves sequential recommendation by incorporating the usage of static item attributes and using vanilla attention to capture users' interests. However, it does not incorporate dynamic user attributes and only models the sequential patterns at the item level instead of the basket level. Although this approach could be extended for if the trend of a usage metric is going up or down in the past three months, FDSA will not capture such a trend due to its averaging operation. Instead, our time-level attention module can capture these temporal patterns.\n\n\nPROPOSED APPROACH\n\nIn this section, we first define the notation and formalize the next basket recommendation task with dynamic user attribute information. And then we present the proposed framework AnDa in detail. The framework is illustrated in Figure 2 and the notation is summarized in Table 1.\n\n\nProblem Statement\n\nIn next basket recommender systems with dynamic attributes, historical basket interactions and dynamic attribute sequences are given, and the goal is to recommend the next basket's items. Formally, we denotes , and a sets of users, items and user attributes respectively. For a user \u2208 , a sequence of baskets = 1 , 2 , \u00b7 \u00b7 \u00b7 , represents his or her item interactions sorted by time. is the maximum time steps of each sequence, and \u2286 is a set of items that user interacted with at time step . A sequence = 1 , 2 , \u00b7 \u00b7 \u00b7 , represents the value of dynamic user attributes of user ordered by time. Specifically, \u2208 are all the attribute values of at time step . The goal is to predict basket items that user will interact with at time step + 1 given historical baskets and attributes .\n\n\nTime-Interval and Periodic Aware Input Encoder\n\nEmbedding Lookup: For each basket of dynamic attributes , we model categorical and numerical features differently.\n\nCategorical attributes ,cat \u2286 are represented by an | cat |-dimensional multi-hot vector denoted by cat \u2208 R | cat |\u00d71 .\n\nNumerical attributes are normalized into the range [\u22121, 1] using min-max normalization, denoted as num \u2208 R | num |\u00d71 .\n\nEach basket of items is represented by a | |-dimensional multi-hot representation, denoted by \u2208 R | |\u00d71 . After that, we apply a concatenation-based lookup function [2] to encode cat and :\ncat = CONCAT \u2212 LOOKUP ( , cat ), = CONCAT \u2212 LOOKUP ( , )(1)\nwhere \u2208 R | cat |\u00d7 and \u2208 R | |\u00d7 are learnable embedding matrices for categorical attributes and items. The \"CONCAT-LOOKUP\" function retrieves the corresponding embedding vectors and then concatenates them together to form matrices cat \u2208 R | cat |\u00d7 , and \u2208 R | |\u00d7 . is the embedding dimension of each item and categorical attribute.\n\n| | is the total number of items in . Since the number of items in each basket varies, we set the maximum number of items in the basket as the largest basket size in the training set | max |, and add padding vectors for baskets smaller than | max |. Time-aware Padding Operation: We set the maximum sequence length as to get up to the latest position steps' information. If the sequence length is shorter than , a zero-pad operation will be applied to empty positions.\nA Basket Item Sequence A Dynamic Attribute Sequence \u22ef Intra-Basket Module Input Encoder Time-Level Attention \u22ef \u22ee \u22ee \u22ee \u22ef \u22ee \u22ee \u22ee \u22ee \u22ee \u22ee \u22ee \u22ee \u22ee \u22ee \u22ee \u22ee FFNN FFNN FFNN \u22ef Candidate Item List \u22ee \u22ee \u2297 \u2297 \u2297 \u22ef Shared Embedding Table ! \" # \" $ \" ! \" # \" $ \" Intra-Attribute Module Time-Level Attention Input Encoder ! | &| # | &| $ | &| ! ! # # $ $ \u22ee \u22ee \u22ee \u22ef ! | '| # | '| $ | '| ! ! # # $ $ ! ( )#\nOtherwise, we truncate to the last positional steps. Unlike previous works [10,23,29] that pad zeros to the left until the sequence length is , we pad zeros to the missed time steps to keep the time interval information. We denote , cat , and num as the padded basket item, categorical attribute, and numerical attribute sequences respectively.\n\nPeriodic Index Embedding: We introduce a periodic index embedding \u2208 R \u2032 \u00d7 for attention modules to discover periodic patterns. The index repeats over every \u2032 time steps of a sequence. For example, \u2032 = 12 can be used for capturing seasonal patterns when the time interval between each two baskets is one month. Positional embeddings [20] that commonly used to identify item positions is also used in this paper. Formally, we concatenate the periodic and positional index embedding with as = [ , , ], where \u2208 {1, \u00b7 \u00b7 \u00b7 , }, \u2208 and \u2208 are dimensional periodic and positional embedding vectors. Then a basket sequence is represented as = 1 , 2 , \u00b7 \u00b7 \u00b7 , . We also add positional and periodic index embeddings to cat and num to form cat and num respectively.\n\n\nTime Level Attention Module\n\nTo capture temporal patterns from , cat and num , we separately encoder them via multi-head self-attention (MHSA) [24]. Formally, let 0 = = cat , and then fed into a MHSA block as shown below: Eq. 2\n(1) = MHSA( (0) , \u210e) = [head 1 , head 2 , . . . , head h ]W Concate , head i = ( (0) Query , (0) Key , (0) Value ),(2)\nwhere \u210e is the number of sub-spaces, Query \u2208 R \u00d7 , Key \u2208 R \u00d7 , Value \u2208 R \u00d7 and Concate \u2208 R \u2032 \u00d7 are learned parameters ( = (| cat + 2| \u00b7 ), and \u2032 = \u210e ). Similar, and num are also encoded via Eq. 2.\n\nFollowing [10], we add causality mask to avoid future information leek. To enhance the representation learning of the self-attention block, residual connections [6], dropout [22], layer normalization [1], and two fully connected layers with ReLU activation functions are added to form the entire multi-head self-attention block (MHSAB) as follows:\n(1) = MHSAB( (0) , \u210e)(3)\nWe stack multiple attention blocks to capture more complex feature interactions:\n( ) = MHSAB( ( \u22121) , \u210e ), > 0(4)\nwhere ( \n\n\nIntra-Basket and Intra-Attribute Self-Attention Modules\n\nItem correlations in each basket can reveal some useful information such as co-purchase relationships. The key problem is how to determine which items should be combined or are correlated. In this paper, we use multi-head self-attention to learn information such as items' correlation relationships. Specially, given representations of all items in a basket , a single-head self-attention module will first compute the similarity matrix which is seen as item correlation scores, and then it updates the item representation by combining all relevant items using the similarity coefficients (generated based on the similarity matrix). We use MHSAB to enhance the model's capability of capturing complex item correlations, which is formed as follows:\n( +1) = MHSAB( ( ) , \u210e +1 ), > 0, \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , }(5)\nwhere ( ) is the output of the time level attention module at time , and \u210e +1 is the ( + 1) th attention block.\n\nWe stack Eq. 5 times to capture more complex item relationships ( + ) . Similarly, we can stack Eq. 5 on dynamic attribute (named Intra-Attribute Attention) to get higher level categorical attribute interactions ( + ) cat .\n\n\nModel Training\n\nThe encoded user representations are projected as: all = FFNN ([\n( + ) , ( + ) cat , ( + ) num ]),\nwhere FFNN is a feed forward network, all \u2208 R 1\u00d7 is the final representation given dynamic attributes and basket items from time step 1 to . We then adopt the binary cross-entropy loss as the objective for training the model defined as:\n\u2212 \u2211\ufe01 \u2208 \u2211\ufe01 \u2208 {1,\u00b7\u00b7\u00b7 , }] [ \u2211\ufe01 \u2208 +1 ( ( all \u00b7 ))) + \u2211\ufe01 \u2209 +1 (1 \u2212 ( all \u00b7 ))](6)\nwhere is the sigmoid function ( ) = 1/(1 + \u2212 ). is the item embedding matrix which is shared for encoding basket items in input encoders. The target basket items for user are a shifted version of , denoted by 2 , 3 , \u00b7 \u00b7 \u00b7 , +1 .\n\n\nEXPERIMENTS\n\n\nExperimental Setting\n\n4.1.1 Datasets. Table 2 summarizes the statistics of the datasets. EPR is a private dataset sampled from a leading enterprise cloud platform. The task is to recommend products to businesses. Examples of the dynamic attributes are behavior metrics on the website, sales information, and marketing activities of the business. SPR 1 is a public dataset on product recommendations for the Santander bank. Ta-Feng 2 is a grocery shopping dataset. We followed Beacon [11] to create train, validation, and test sets by chronological order. The <train, validation, test> sets for EPR, SPR, and\n\nTa-Feng datasets are <1 st -20 th , 21 st , 22 nd -24 th >, <1 st -16 th , 17 \u210e , 18 th > , and <1 st -3 rd , successive 0.5, last 0.5> month(s) respectively. The interval between each two time steps is 1 month for EPR and SPR datasets, and 1 day for Ta-Feng dataset.  16, and 30 in EPR, SPR, and Ta-Feng respectively. We report the hyper-parameter sensitivity study results in Figure 3. Table 3 shows overall results compared with baseline approaches. We observe that, first, our proposed approach consistently outperforms all baselines significantly in terms of Hit Rate, NDCG, and MAP by 3.65% -21.87%, 9.09% -43.76%, and 2.32% -24.53%, which demonstrate the effectiveness of our proposed method. Beside, The next basket recommenders (DREAM, Beacon, CTA, and Sets2Sets) outperform those for next item recommendation (FMC, FPMC).\n\n\nOverall Performance Comparison\n\nThis indicates that learning the sequential patterns with the encoding of the intra-baseket information can better capture users' dynamic interests. The FDSA+ method performs the best among baselines in the EPR and SPR datasets, while Sets2Sets performs the best in the Ta-Feng dataset. The main reason is that FDSA+ leverages attribute information where EPR and SPR have more attributes. We also report the models' average inference time (milliseconds per sequence) on 400 sequence inputs in Table 3 (last row). The proposed method takes more time to generate recommendation lists than baseline methods, though is comparable with Set2Sets, DREAM, CTA, and FDSA+. \n\n\nAblation Study\n\nTo understand the impact of different components in AnDa, we conduct a detailed ablation study using the SPR dataset in Table 4. AnDa(P) is AnDa without periodic index embedding. The results show that the periodic index can help capture users' seasonal purchase patterns, and thus helps to improve performance. AnDa(B) is AnDa with basket information only. Without dynamic attributes, AnDa(B-) removes the intra-basket module from AnDa(B), AnDa(T)\n\nis AnDa without using intra-basket and intra-attribute modules on both items and attributes, and AnDa(I) is AnDa without applying the time level attention module. The performance degradation on the sub-models shows the benefits of each component. \n\n\nAttention Visualization\n\nWe visualize the attention weights of time-level, intra-attribute, and intra-basket attentions on sampled sequences from the SPR dataset in Figure 3 \n\n\nCONCLUSION\n\nIn this paper, we propose a novel attentive network AnDa, which models dynamic attributes to better capture users' dynamically changing interests and intentions. AnDa separately extracts temporal patterns from dynamic attributes and user historical interactions with a novel input encoder. AnDa also generates feature interactions and uncovers item interrelationships in each basket with proposed intra-attribute and intra-basket modules respectively. We evaluate AnDa on three real-world datasets and demonstrate the usefulness of modeling dynamic attributes for next basket recommendation.\n\nFig. 1 .\n1An illustration of a recommender system with or without dynamic attributes.).\n\nFig. 2 .\n2The Model Architecture of AnDa.\n\n0 )\n0= and \u210e is the number of heads at the th attention block. ( ) is the output after stacking multiple timelevel attention layers. The extracted representation vector at time step can be denoted ( ) and contains information extracted from time 1 to time of the input sequence.\n\n\nSettings. We tune the embedding dimension from {10, 15, 30, 50}, learning rate from {0.0001, 0.001, 0.01}, and dropout from {0.0, 0.1, 0.2, 0.5}. For DREAM, we tune with RNN, GRU, and LSTM modules. AdamOptimizer is used to update the network with moment estimates 1 = 0.9 and 2 = 0.999. For AnDa, we tune the self-attention layers from {1, 2, 4} and head number on each attention block from {1, 2, 4, 6}. Maximum sequence lengths are set as 12,\n\nFig. 3 .\n3(B) to gain more insights. (a) and (b) are attention weights from two different layers (layer 1 and 4) of time level basket attention, (c) and (d) are from two different heads of the first intra-attribute layer, and (e) and (f) are from two different head of the first intra-basket layer. From (a) and (b), we can see the attention varies over different layers. While the weights in layer 4 focus more on recent items, the weights in layer 1 attend more evenly to all previous histories. From (c) and (d), we observe that the attention weights vary over different heads, and the module captures meaningful feature interactions. For example, in (c), the position (11, 1) (marked by a red square) corresponding to interacted feature value <\"Foreigner index\": NO, \"Customer's Country residence\": ES> (the bank is based in Spain, so a customer who lives in Spain is not a foreigner). We can also observe that intra-basket attention can capture different item relationships under different heads comparing with (e) and (f). (A): hyperparamter sensitivity study results of AnDa. (B): visualization of attention weights on different MHSA modules.\n\n\n). Second, dynamic attributes represent users' behaviors from different perspectives, and can happen asynchronously with interactions. Directly concatenating basket items with dynamic attributes at each time step may not be stable to model diverse sequential patterns.Besides dynamic attributes, in users' historical interactions, items in each basket contain complex interrelationships (item correlations, co-purchases, etc.). Existing solutions[3,11] either pre-define static item correlations based on co-occurring items, or (vanilla) attention to extract item correlations based on the whole items the last basket. However, multiple item interrelationships can exist based on multiple subsets of items in a basket and together influence next basket items. For example, in a grocery shopping, a customer bought apple, banana, TV and Speaker. The apple and banana are high correlated while TV and Speaker are also high correlated. The user may purchase some fruits again with TV accessories. Using multi-head attention allows the model to capture different item relationships under different subset of items in a historical basket.To address the above challenges, we propose a novel Attentive network to model the Dynamic attributes as well as users' historical interacted items (AnDa for short). AnDa separately encodes and learns representations from dynamic attributes and interactions with basket items.\n\nTable 1 .\n1Summary of Main Notation.modeling dynamic features, it uses vanilla attention to average out different attributes at each time step. As a result, it is not able to learn high-order feature interactions. Also, it loses the temporal aspects of individual features. For example,Notation \nDescription \nNotation \nDescription \n\nset of users with total number | | \n\n,num \n\nnumerical attributes values of at time \nset of items with total number | | \n, cat \nmulti-hot vector of \nand ,cat \nset of attributes with total number | | \npositional embedding vector at time \na basket of items that interacted with at time \nperiodic index embedding vector at time \n\n,cat \n\ncategorical attribute values of at time \n\n, \n\npredicted relevance score for item at time \n\n\n\nTable 2 .\n2Dataset information.Datasets/Information # Users # Items # Attributes Sparsity Avg. Baskets Avg. Basket Size \n\nEPR \n229314 \n23 \n169 \n7.92% \n17.04 \n1.34 \nSPR \n956645 \n24 \n24 \n4.92% \n14.51 \n1.47 \nTa-Feng \n13541 7691 \n5 \n3.58% \n7.12 \n5.8 \n\n4.1.2 Baselines & Evaluation Metrics. We include three groups of baseline methods: PopRec considers no sequential \n\npatterns; FMC [18] and FPMC [18] are Markov Chain-based sequential methods; and Neural Network based methods \n\nwith (FDSA+) or without (DREAM [28], Beacon [11], Sets2sets [8], and CTA [26]) dynamic attributes. We evaluate all \n\nmethods on the whole item set without sampling. All the items are first ranked and then evaluated by Hit Rate (HR@ ), \n\nNormalized Discounted Cumulative Gain (NDCG@ ), and Mean Average Precision (MAP). In this work, we report HR \n\nand NDCG with K=5. \n\n\n\nTable 3 .\n3Performance Comparison of different methods on next basket recommendation. Bold/underlined scores are the best/second best in each row. The last column shows AnDa's relative improvement over the best baseline.Dataset \nMetric \nFMC FPMC PopRec CTA Sets2Sets DREAM Beacon FDSA+ AnDa Improv. \n\nEPR \n\nHR@5 0.1024 0.1297 0.4099 0.3108 0.4155 \n0.2285 0.3211 0.4613 0.5622 21.87% \nNDCG@5 0.0691 0.0889 0.2189 0.2065 0.2207 \n0.1082 0.1453 0.3211 0.4616 43.76% \nMAP \n0.0938 0.1103 0.1805 0.1789 0.1912 \n0.1324 0.1385 0.2556 0.3183 24.53% \n\nSPR \n\nHR@5 0.2197 0.3246 0.4527 0.4783 0.5632 \n0.1441 0.5027 0.6834 0.7481 9.47% \nNDCG@5 0.1026 0.1358 0.1192 0.2137 0.3021 \n0.0694 0.2259 0.3123 0.4049 29.65% \nMAP \n0.1196 0.1271 0.1465 0.1842 0.2365 \n0.0991 0.1699 0.2476 0.2638 6.54% \n\nTa-Feng \n\nHR@5 0.0064 0.0089 0.0414 0.0379 0.0498 \n0.0401 0.0442 0.0301 0.0573 15.06% \nNDCG@5 0.0035 0.0044 0.0155 0.0214 0.0271 \n0.0226 0.0256 0.0141 0.0308 13.65% \nMAP \n0.0027 0.0039 0.0229 0.0202 0.0259 \n0.0200 0.0255 0.0188 0.0265 2.32% \n\nInference Time msec./seq. 0.1697 0.1929 0.8428 1.8341 2.3212 \n2.2542 0.6911 1.9521 2.5172 \n-\n\n\n\nTable 4 .\n4Ablation Study on the SPR Dataset.Models HR@5 NDCG@5 MAP Average Models HR@5 NDCG@5 MAP Average \nFDSA(+) 0.6834 0.3123 0.2476 -13.91% AnDa(P) 0.7340 0.3961 0.4749 -1.42% \nAnDa(B) 0.7408 0.4037 0.2605 -0.90% AnDa(B-) 0.7280 0.3933 0.4753 -1.89% \nAnDa(T) 0.7401 0.3977 0.2610 -1.06% AnDa(I) 0.7182 0.3813 0.4666 -3.61% \nAnDa 0.7481 0.4049 0.2638 \n-\nAnDa 0.7481 0.4049 0.2638 \n-\n\n\nhttps://www.kaggle.com/c/santander-product-recommendation 2 https://www.kaggle.com/chiranjivdas09/ta-feng-grocery-dataset\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.06450Layer normalization. arXiv preprintJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016).\n\nAn attribute-aware neural attentive model for next basket recommendation. Ting Bai, Jian-Yun Nie, Wayne Xin Zhao, Yutao Zhu, Pan Du, Ji-Rong Wen, The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. Ting Bai, Jian-Yun Nie, Wayne Xin Zhao, Yutao Zhu, Pan Du, and Ji-Rong Wen. 2018. An attribute-aware neural attentive model for next basket recommendation. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. 1201-1204.\n\nInter-Basket and Intra-Basket Adaptive Attention Network for Next Basket Recommendation. Pengpeng Binbin Che, Junhua Zhao, Lei Fang, Zhao, S Victor, Zhiming Sheng, Cui, IEEE Access. 7Binbin Che, Pengpeng Zhao, Junhua Fang, Lei Zhao, Victor S Sheng, and Zhiming Cui. 2019. Inter-Basket and Intra-Basket Adaptive Attention Network for Next Basket Recommendation. IEEE Access 7 (2019), 80644-80650.\n\nGradient boosting factorization machines. Chen Cheng, Fen Xia, Tong Zhang, Irwin King, Michael R Lyu, Proceedings of the 8th ACM Conference on Recommender systems. the 8th ACM Conference on Recommender systemsChen Cheng, Fen Xia, Tong Zhang, Irwin King, and Michael R Lyu. 2014. Gradient boosting factorization machines. In Proceedings of the 8th ACM Conference on Recommender systems. 265-272.\n\nWide & deep learning for recommender systems. Heng-Tze, Levent Cheng, Jeremiah Koc, Tal Harmsen, Tushar Shaked, Hrishi Chandra, Glen Aradhye, Greg Anderson, Wei Corrado, Mustafa Chai, Ispir, Proceedings of the 1st workshop on deep learning for recommender systems. the 1st workshop on deep learning for recommender systemsHeng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st workshop on deep learning for recommender systems. 7-10.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770-778.\n\nNeural factorization machines for sparse predictive analytics. Xiangnan He, Tat-Seng Chua, Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. the 40th International ACM SIGIR conference on Research and Development in Information RetrievalXiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse predictive analytics. In Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. 355-364.\n\nSets2sets: Learning from sequential sets with neural networks. Haoji Hu, Xiangnan He, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningHaoji Hu and Xiangnan He. 2019. Sets2sets: Learning from sequential sets with neural networks. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1491-1499.\n\nField-aware factorization machines for CTR prediction. Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, Chih-Jen Lin, Proceedings of the 10th ACM Conference on Recommender Systems. the 10th ACM Conference on Recommender SystemsYuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Field-aware factorization machines for CTR prediction. In Proceedings of the 10th ACM Conference on Recommender Systems. 43-50.\n\nSelf-attentive sequential recommendation. Wang-Cheng Kang, Julian Mcauley, 2018 IEEE International Conference on Data Mining (ICDM). IEEEWang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 197-206.\n\nCorrelation-sensitive next-basket recommendation. Duc-Trong Le, W Hady, Yuan Lauw, Fang, Duc-Trong Le, Hady W Lauw, and Yuan Fang. 2019. Correlation-sensitive next-basket recommendation. (2019).\n\nTime Interval Aware Self-Attention for Sequential Recommendation. Jiacheng Li, Yujie Wang, Julian Mcauley, Proceedings of the 13th International Conference on Web Search and Data Mining. the 13th International Conference on Web Search and Data MiningJiacheng Li, Yujie Wang, and Julian McAuley. 2020. Time Interval Aware Self-Attention for Sequential Recommendation. In Proceedings of the 13th International Conference on Web Search and Data Mining. 322-330.\n\nxdeepfm: Combining explicit and implicit feature interactions for recommender systems. Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, Guangzhong Sun, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningJianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature interactions for recommender systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 1754-1763.\n\nZhiwei Liu, Yongjun Chen, Jia Li, S Philip, Julian Yu, Caiming Mcauley, Xiong, arXiv:2108.06479Contrastive Self-supervised Sequential Recommendation with Robust Augmentation. arXiv preprintZhiwei Liu, Yongjun Chen, Jia Li, Philip S Yu, Julian McAuley, and Caiming Xiong. 2021. Contrastive Self-supervised Sequential Recommendation with Robust Augmentation. arXiv preprint arXiv:2108.06479 (2021).\n\nBasket recommendation with multi-intent translation graph neural network. Zhiwei Liu, Xiaohan Li, Ziwei Fan, Stephen Guo, Kannan Achan, S Yu Philip, 2020 IEEE International Conference on Big Data (Big Data). IEEEZhiwei Liu, Xiaohan Li, Ziwei Fan, Stephen Guo, Kannan Achan, and S Yu Philip. 2020. Basket recommendation with multi-intent translation graph neural network. In 2020 IEEE International Conference on Big Data (Big Data). IEEE, 728-737.\n\nBasconv: Aggregating heterogeneous interactions for basket recommendation with graph convolutional neural network. Zhiwei Liu, Mengting Wan, Stephen Guo, Kannan Achan, Philip S Yu, Proceedings of the 2020 SIAM International Conference on Data Mining. SIAM. the 2020 SIAM International Conference on Data Mining. SIAMZhiwei Liu, Mengting Wan, Stephen Guo, Kannan Achan, and Philip S Yu. 2020. Basconv: Aggregating heterogeneous interactions for basket recommendation with graph convolutional neural network. In Proceedings of the 2020 SIAM International Conference on Data Mining. SIAM, 64-72.\n\nFactorization machines. Steffen Rendle, 2010 IEEE International Conference on Data Mining. IEEESteffen Rendle. 2010. Factorization machines. In 2010 IEEE International Conference on Data Mining. IEEE, 995-1000.\n\nFactorizing personalized markov chains for next-basket recommendation. Steffen Rendle, Christoph Freudenthaler, Lars Schmidt-Thieme, Proceedings of the 19th international conference on World wide web. the 19th international conference on World wide webSteffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized markov chains for next-basket recommendation. In Proceedings of the 19th international conference on World wide web. 811-820.\n\nFast context-aware recommendations with factorization machines. Zeno Steffen Rendle, Christoph Gantner, Lars Freudenthaler, Schmidt-Thieme, Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. the 34th international ACM SIGIR conference on Research and development in Information RetrievalSteffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2011. Fast context-aware recommendations with factorization machines. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. 635-644.\n\nPeter Shaw, Jakob Uszkoreit, Ashish Vaswani, arXiv:1803.02155Self-attention with relative position representations. arXiv preprintPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155 (2018).\n\nAutoint: Automatic feature interaction learning via self-attentive neural networks. Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, Jian Tang, Proceedings of the 28th ACM International Conference on Information and Knowledge Management. the 28th ACM International Conference on Information and Knowledge ManagementWeiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-attentive neural networks. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 1161-1170.\n\nDropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, 15Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research 15, 1 (2014), 1929-1958.\n\nBERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, Peng Jiang, Proceedings of the 28th ACM International Conference on Information and Knowledge Management. the 28th ACM International Conference on Information and Knowledge ManagementFei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management. 1441-1450.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998-6008.\n\nDeep & cross network for ad click predictions. Ruoxi Wang, Bin Fu, Gang Fu, Mingliang Wang, Proceedings of the ADKDD'17. the ADKDD'17Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD'17. 1-7.\n\nD\u00e9j\u00e0 vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation. Jibang Wu, Renqin Cai, Hongning Wang, Proceedings of The Web Conference 2020. The Web Conference 2020Jibang Wu, Renqin Cai, and Hongning Wang. 2020. D\u00e9j\u00e0 vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation. In Proceedings of The Web Conference 2020. 2199-2209.\n\nAttentional factorization machines: Learning the weight of feature interactions via attention networks. Jun Xiao, Xiangnan Hao Ye, Hanwang He, Fei Zhang, Tat-Seng Wu, Chua, arXiv:1708.04617arXiv preprintJun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua. 2017. Attentional factorization machines: Learning the weight of feature interactions via attention networks. arXiv preprint arXiv:1708.04617 (2017).\n\nA dynamic recurrent model for next basket recommendation. Feng Yu, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan, Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. the 39th International ACM SIGIR conference on Research and Development in Information RetrievalFeng Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. A dynamic recurrent model for next basket recommendation. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. 729-732.\n\nFeature-level deeper self-attention network for sequential recommendation. Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor Sheng, Jiajie Xu, Deqing Wang, Guanfeng Liu, Xiaofang Zhou, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial IntelligenceAAAI PressTingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor Sheng, Jiajie Xu, Deqing Wang, Guanfeng Liu, and Xiaofang Zhou. 2019. Feature-level deeper self-attention network for sequential recommendation. In Proceedings of the 28th International Joint Conference on Artificial Intelligence. AAAI Press, 4320-4326.\n", "annotations": {"author": "[{\"end\":90,\"start\":62},{\"end\":121,\"start\":91},{\"end\":139,\"start\":122},{\"end\":153,\"start\":140},{\"end\":167,\"start\":154},{\"end\":175,\"start\":168},{\"end\":189,\"start\":176},{\"end\":200,\"start\":190},{\"end\":216,\"start\":201},{\"end\":232,\"start\":217},{\"end\":247,\"start\":233},{\"end\":270,\"start\":248},{\"end\":305,\"start\":271},{\"end\":331,\"start\":306},{\"end\":361,\"start\":332}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":85},{\"end\":120,\"start\":113},{\"end\":138,\"start\":131},{\"end\":152,\"start\":147},{\"end\":166,\"start\":162},{\"end\":174,\"start\":172},{\"end\":188,\"start\":185},{\"end\":199,\"start\":197},{\"end\":215,\"start\":208},{\"end\":231,\"start\":224},{\"end\":246,\"start\":241}]", "author_first_name": "[{\"end\":84,\"start\":77},{\"end\":112,\"start\":106},{\"end\":130,\"start\":124},{\"end\":146,\"start\":143},{\"end\":161,\"start\":154},{\"end\":171,\"start\":168},{\"end\":184,\"start\":176},{\"end\":196,\"start\":190},{\"end\":207,\"start\":201},{\"end\":223,\"start\":217},{\"end\":240,\"start\":233}]", "author_affiliation": "[{\"end\":269,\"start\":249},{\"end\":304,\"start\":272},{\"end\":330,\"start\":307},{\"end\":360,\"start\":333}]", "title": "[{\"end\":59,\"start\":1},{\"end\":420,\"start\":362}]", "venue": null, "abstract": "[{\"end\":2378,\"start\":897}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2411,\"start\":2407},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2414,\"start\":2411},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2941,\"start\":2938},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2944,\"start\":2941},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2947,\"start\":2944},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2950,\"start\":2947},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3003,\"start\":3000},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5053,\"start\":5049},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5190,\"start\":5187},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5287,\"start\":5284},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5471,\"start\":5467},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5653,\"start\":5650},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5933,\"start\":5929},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6195,\"start\":6192},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6197,\"start\":6195},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6200,\"start\":6197},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6203,\"start\":6200},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6301,\"start\":6297},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6429,\"start\":6426},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6513,\"start\":6510},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6669,\"start\":6665},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6745,\"start\":6741},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6967,\"start\":6963},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7336,\"start\":7332},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7379,\"start\":7375},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7548,\"start\":7544},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7660,\"start\":7656},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7731,\"start\":7727},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9974,\"start\":9971},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11314,\"start\":11310},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11317,\"start\":11314},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11320,\"start\":11317},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11917,\"start\":11913},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12482,\"start\":12478},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12894,\"start\":12890},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13044,\"start\":13041},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13058,\"start\":13054},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13083,\"start\":13080},{\"end\":14421,\"start\":14416},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15741,\"start\":15737},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21353,\"start\":21350},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21356,\"start\":21353}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":18980,\"start\":18892},{\"attributes\":{\"id\":\"fig_1\"},\"end\":19023,\"start\":18981},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19303,\"start\":19024},{\"attributes\":{\"id\":\"fig_3\"},\"end\":19750,\"start\":19304},{\"attributes\":{\"id\":\"fig_4\"},\"end\":20901,\"start\":19751},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":22313,\"start\":20902},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":23072,\"start\":22314},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":23918,\"start\":23073},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":25036,\"start\":23919},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":25426,\"start\":25037}]", "paragraph": "[{\"end\":2498,\"start\":2380},{\"end\":2809,\"start\":2500},{\"end\":4288,\"start\":2811},{\"end\":4916,\"start\":4290},{\"end\":6130,\"start\":4962},{\"end\":6498,\"start\":6163},{\"end\":7237,\"start\":6500},{\"end\":7720,\"start\":7262},{\"end\":8295,\"start\":7722},{\"end\":8596,\"start\":8317},{\"end\":9398,\"start\":8618},{\"end\":9563,\"start\":9449},{\"end\":9684,\"start\":9565},{\"end\":9804,\"start\":9686},{\"end\":9994,\"start\":9806},{\"end\":10386,\"start\":10055},{\"end\":10856,\"start\":10388},{\"end\":11579,\"start\":11235},{\"end\":12332,\"start\":11581},{\"end\":12562,\"start\":12364},{\"end\":12878,\"start\":12682},{\"end\":13227,\"start\":12880},{\"end\":13333,\"start\":13253},{\"end\":13375,\"start\":13367},{\"end\":14182,\"start\":13435},{\"end\":14350,\"start\":14239},{\"end\":14575,\"start\":14352},{\"end\":14658,\"start\":14594},{\"end\":14929,\"start\":14693},{\"end\":15237,\"start\":15008},{\"end\":15861,\"start\":15276},{\"end\":16694,\"start\":15863},{\"end\":17393,\"start\":16729},{\"end\":17859,\"start\":17412},{\"end\":18108,\"start\":17861},{\"end\":18285,\"start\":18136},{\"end\":18891,\"start\":18300}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10054,\"start\":9995},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11234,\"start\":10857},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12681,\"start\":12563},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13252,\"start\":13228},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13366,\"start\":13334},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14238,\"start\":14183},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14692,\"start\":14659},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15007,\"start\":14930}]", "table_ref": "[{\"end\":4537,\"start\":4356},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":8595,\"start\":8588},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15299,\"start\":15292},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16258,\"start\":16251},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17229,\"start\":17222},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17539,\"start\":17532}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":4931,\"start\":4919},{\"attributes\":{\"n\":\"2.1\"},\"end\":4960,\"start\":4934},{\"attributes\":{\"n\":\"2.2\"},\"end\":6161,\"start\":6133},{\"attributes\":{\"n\":\"2.3\"},\"end\":7260,\"start\":7240},{\"attributes\":{\"n\":\"3\"},\"end\":8315,\"start\":8298},{\"attributes\":{\"n\":\"3.1\"},\"end\":8616,\"start\":8599},{\"attributes\":{\"n\":\"3.2\"},\"end\":9447,\"start\":9401},{\"attributes\":{\"n\":\"3.3\"},\"end\":12362,\"start\":12335},{\"attributes\":{\"n\":\"3.4\"},\"end\":13433,\"start\":13378},{\"attributes\":{\"n\":\"3.5\"},\"end\":14592,\"start\":14578},{\"attributes\":{\"n\":\"4\"},\"end\":15251,\"start\":15240},{\"attributes\":{\"n\":\"4.1\"},\"end\":15274,\"start\":15254},{\"attributes\":{\"n\":\"4.2\"},\"end\":16727,\"start\":16697},{\"attributes\":{\"n\":\"4.3\"},\"end\":17410,\"start\":17396},{\"attributes\":{\"n\":\"4.4\"},\"end\":18134,\"start\":18111},{\"attributes\":{\"n\":\"5\"},\"end\":18298,\"start\":18288},{\"end\":18901,\"start\":18893},{\"end\":18990,\"start\":18982},{\"end\":19028,\"start\":19025},{\"end\":19760,\"start\":19752},{\"end\":22324,\"start\":22315},{\"end\":23083,\"start\":23074},{\"end\":23929,\"start\":23920},{\"end\":25047,\"start\":25038}]", "table": "[{\"end\":23072,\"start\":22601},{\"end\":23918,\"start\":23105},{\"end\":25036,\"start\":24140},{\"end\":25426,\"start\":25083}]", "figure_caption": "[{\"end\":18980,\"start\":18903},{\"end\":19023,\"start\":18992},{\"end\":19303,\"start\":19030},{\"end\":19750,\"start\":19306},{\"end\":20901,\"start\":19762},{\"end\":22313,\"start\":20904},{\"end\":22601,\"start\":22326},{\"end\":23105,\"start\":23085},{\"end\":24140,\"start\":23931},{\"end\":25083,\"start\":25049}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3376,\"start\":3368},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3767,\"start\":3759},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8553,\"start\":8545},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16249,\"start\":16241},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18284,\"start\":18276}]", "bib_author_first_name": "[{\"end\":25557,\"start\":25552},{\"end\":25561,\"start\":25558},{\"end\":25571,\"start\":25566},{\"end\":25576,\"start\":25572},{\"end\":25592,\"start\":25584},{\"end\":25594,\"start\":25593},{\"end\":25855,\"start\":25851},{\"end\":25869,\"start\":25861},{\"end\":25880,\"start\":25875},{\"end\":25884,\"start\":25881},{\"end\":25896,\"start\":25891},{\"end\":25905,\"start\":25902},{\"end\":25917,\"start\":25910},{\"end\":26383,\"start\":26375},{\"end\":26402,\"start\":26396},{\"end\":26412,\"start\":26409},{\"end\":26426,\"start\":26425},{\"end\":26442,\"start\":26435},{\"end\":26729,\"start\":26725},{\"end\":26740,\"start\":26737},{\"end\":26750,\"start\":26746},{\"end\":26763,\"start\":26758},{\"end\":26777,\"start\":26770},{\"end\":26779,\"start\":26778},{\"end\":27141,\"start\":27135},{\"end\":27157,\"start\":27149},{\"end\":27166,\"start\":27163},{\"end\":27182,\"start\":27176},{\"end\":27197,\"start\":27191},{\"end\":27211,\"start\":27207},{\"end\":27225,\"start\":27221},{\"end\":27239,\"start\":27236},{\"end\":27256,\"start\":27249},{\"end\":27741,\"start\":27734},{\"end\":27753,\"start\":27746},{\"end\":27769,\"start\":27761},{\"end\":27779,\"start\":27775},{\"end\":28196,\"start\":28188},{\"end\":28209,\"start\":28201},{\"end\":28719,\"start\":28714},{\"end\":28732,\"start\":28724},{\"end\":29185,\"start\":29179},{\"end\":29196,\"start\":29192},{\"end\":29214,\"start\":29205},{\"end\":29229,\"start\":29221},{\"end\":29591,\"start\":29581},{\"end\":29604,\"start\":29598},{\"end\":29896,\"start\":29887},{\"end\":29902,\"start\":29901},{\"end\":29913,\"start\":29909},{\"end\":30107,\"start\":30099},{\"end\":30117,\"start\":30112},{\"end\":30130,\"start\":30124},{\"end\":30587,\"start\":30580},{\"end\":30602,\"start\":30594},{\"end\":30616,\"start\":30609},{\"end\":30632,\"start\":30624},{\"end\":30643,\"start\":30639},{\"end\":30659,\"start\":30649},{\"end\":31145,\"start\":31139},{\"end\":31158,\"start\":31151},{\"end\":31168,\"start\":31165},{\"end\":31174,\"start\":31173},{\"end\":31189,\"start\":31183},{\"end\":31201,\"start\":31194},{\"end\":31617,\"start\":31611},{\"end\":31630,\"start\":31623},{\"end\":31640,\"start\":31635},{\"end\":31653,\"start\":31646},{\"end\":31665,\"start\":31659},{\"end\":31677,\"start\":31673},{\"end\":32107,\"start\":32101},{\"end\":32121,\"start\":32113},{\"end\":32134,\"start\":32127},{\"end\":32146,\"start\":32140},{\"end\":32162,\"start\":32154},{\"end\":32870,\"start\":32863},{\"end\":32888,\"start\":32879},{\"end\":32908,\"start\":32904},{\"end\":33336,\"start\":33332},{\"end\":33362,\"start\":33353},{\"end\":33376,\"start\":33372},{\"end\":33898,\"start\":33893},{\"end\":33910,\"start\":33905},{\"end\":33928,\"start\":33922},{\"end\":34265,\"start\":34258},{\"end\":34278,\"start\":34272},{\"end\":34291,\"start\":34284},{\"end\":34305,\"start\":34298},{\"end\":34317,\"start\":34312},{\"end\":34326,\"start\":34322},{\"end\":34338,\"start\":34334},{\"end\":34921,\"start\":34915},{\"end\":34942,\"start\":34934},{\"end\":34955,\"start\":34951},{\"end\":34972,\"start\":34968},{\"end\":34990,\"start\":34984},{\"end\":35343,\"start\":35340},{\"end\":35352,\"start\":35349},{\"end\":35362,\"start\":35358},{\"end\":35375,\"start\":35367},{\"end\":35385,\"start\":35381},{\"end\":35396,\"start\":35391},{\"end\":35405,\"start\":35401},{\"end\":35906,\"start\":35900},{\"end\":35920,\"start\":35916},{\"end\":35934,\"start\":35930},{\"end\":35948,\"start\":35943},{\"end\":35965,\"start\":35960},{\"end\":35978,\"start\":35973},{\"end\":35980,\"start\":35979},{\"end\":35994,\"start\":35988},{\"end\":36008,\"start\":36003},{\"end\":36348,\"start\":36343},{\"end\":36358,\"start\":36355},{\"end\":36367,\"start\":36363},{\"end\":36381,\"start\":36372},{\"end\":36661,\"start\":36655},{\"end\":36672,\"start\":36666},{\"end\":36686,\"start\":36678},{\"end\":37052,\"start\":37049},{\"end\":37067,\"start\":37059},{\"end\":37083,\"start\":37076},{\"end\":37091,\"start\":37088},{\"end\":37107,\"start\":37099},{\"end\":37434,\"start\":37430},{\"end\":37444,\"start\":37439},{\"end\":37453,\"start\":37450},{\"end\":37463,\"start\":37458},{\"end\":37476,\"start\":37470},{\"end\":38020,\"start\":38012},{\"end\":38036,\"start\":38028},{\"end\":38049,\"start\":38043},{\"end\":38061,\"start\":38055},{\"end\":38075,\"start\":38069},{\"end\":38086,\"start\":38080},{\"end\":38101,\"start\":38093},{\"end\":38115,\"start\":38107}]", "bib_author_last_name": "[{\"end\":25564,\"start\":25562},{\"end\":25582,\"start\":25577},{\"end\":25601,\"start\":25595},{\"end\":25859,\"start\":25856},{\"end\":25873,\"start\":25870},{\"end\":25889,\"start\":25885},{\"end\":25900,\"start\":25897},{\"end\":25908,\"start\":25906},{\"end\":25921,\"start\":25918},{\"end\":26394,\"start\":26384},{\"end\":26407,\"start\":26403},{\"end\":26417,\"start\":26413},{\"end\":26423,\"start\":26419},{\"end\":26433,\"start\":26427},{\"end\":26448,\"start\":26443},{\"end\":26453,\"start\":26450},{\"end\":26735,\"start\":26730},{\"end\":26744,\"start\":26741},{\"end\":26756,\"start\":26751},{\"end\":26768,\"start\":26764},{\"end\":26783,\"start\":26780},{\"end\":27133,\"start\":27125},{\"end\":27147,\"start\":27142},{\"end\":27161,\"start\":27158},{\"end\":27174,\"start\":27167},{\"end\":27189,\"start\":27183},{\"end\":27205,\"start\":27198},{\"end\":27219,\"start\":27212},{\"end\":27234,\"start\":27226},{\"end\":27247,\"start\":27240},{\"end\":27261,\"start\":27257},{\"end\":27268,\"start\":27263},{\"end\":27744,\"start\":27742},{\"end\":27759,\"start\":27754},{\"end\":27773,\"start\":27770},{\"end\":27783,\"start\":27780},{\"end\":28199,\"start\":28197},{\"end\":28214,\"start\":28210},{\"end\":28722,\"start\":28720},{\"end\":28735,\"start\":28733},{\"end\":29190,\"start\":29186},{\"end\":29203,\"start\":29197},{\"end\":29219,\"start\":29215},{\"end\":29233,\"start\":29230},{\"end\":29596,\"start\":29592},{\"end\":29612,\"start\":29605},{\"end\":29899,\"start\":29897},{\"end\":29907,\"start\":29903},{\"end\":29918,\"start\":29914},{\"end\":29924,\"start\":29920},{\"end\":30110,\"start\":30108},{\"end\":30122,\"start\":30118},{\"end\":30138,\"start\":30131},{\"end\":30592,\"start\":30588},{\"end\":30607,\"start\":30603},{\"end\":30622,\"start\":30617},{\"end\":30637,\"start\":30633},{\"end\":30647,\"start\":30644},{\"end\":30663,\"start\":30660},{\"end\":31149,\"start\":31146},{\"end\":31163,\"start\":31159},{\"end\":31171,\"start\":31169},{\"end\":31181,\"start\":31175},{\"end\":31192,\"start\":31190},{\"end\":31209,\"start\":31202},{\"end\":31216,\"start\":31211},{\"end\":31621,\"start\":31618},{\"end\":31633,\"start\":31631},{\"end\":31644,\"start\":31641},{\"end\":31657,\"start\":31654},{\"end\":31671,\"start\":31666},{\"end\":31684,\"start\":31678},{\"end\":32111,\"start\":32108},{\"end\":32125,\"start\":32122},{\"end\":32138,\"start\":32135},{\"end\":32152,\"start\":32147},{\"end\":32165,\"start\":32163},{\"end\":32618,\"start\":32604},{\"end\":32877,\"start\":32871},{\"end\":32902,\"start\":32889},{\"end\":32923,\"start\":32909},{\"end\":33351,\"start\":33337},{\"end\":33370,\"start\":33363},{\"end\":33390,\"start\":33377},{\"end\":33406,\"start\":33392},{\"end\":33903,\"start\":33899},{\"end\":33920,\"start\":33911},{\"end\":33936,\"start\":33929},{\"end\":34270,\"start\":34266},{\"end\":34282,\"start\":34279},{\"end\":34296,\"start\":34292},{\"end\":34310,\"start\":34306},{\"end\":34320,\"start\":34318},{\"end\":34332,\"start\":34327},{\"end\":34343,\"start\":34339},{\"end\":34932,\"start\":34922},{\"end\":34949,\"start\":34943},{\"end\":34966,\"start\":34956},{\"end\":34982,\"start\":34973},{\"end\":35004,\"start\":34991},{\"end\":35347,\"start\":35344},{\"end\":35356,\"start\":35353},{\"end\":35365,\"start\":35363},{\"end\":35379,\"start\":35376},{\"end\":35389,\"start\":35386},{\"end\":35399,\"start\":35397},{\"end\":35411,\"start\":35406},{\"end\":35914,\"start\":35907},{\"end\":35928,\"start\":35921},{\"end\":35941,\"start\":35935},{\"end\":35958,\"start\":35949},{\"end\":35971,\"start\":35966},{\"end\":35986,\"start\":35981},{\"end\":36001,\"start\":35995},{\"end\":36019,\"start\":36009},{\"end\":36353,\"start\":36349},{\"end\":36361,\"start\":36359},{\"end\":36370,\"start\":36368},{\"end\":36386,\"start\":36382},{\"end\":36664,\"start\":36662},{\"end\":36676,\"start\":36673},{\"end\":36691,\"start\":36687},{\"end\":37057,\"start\":37053},{\"end\":37074,\"start\":37068},{\"end\":37086,\"start\":37084},{\"end\":37097,\"start\":37092},{\"end\":37110,\"start\":37108},{\"end\":37116,\"start\":37112},{\"end\":37437,\"start\":37435},{\"end\":37448,\"start\":37445},{\"end\":37456,\"start\":37454},{\"end\":37468,\"start\":37464},{\"end\":37480,\"start\":37477},{\"end\":38026,\"start\":38021},{\"end\":38041,\"start\":38037},{\"end\":38053,\"start\":38050},{\"end\":38067,\"start\":38062},{\"end\":38078,\"start\":38076},{\"end\":38091,\"start\":38087},{\"end\":38105,\"start\":38102},{\"end\":38120,\"start\":38116}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b0\"},\"end\":25775,\"start\":25550},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":49648997},\"end\":26284,\"start\":25777},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":195775425},\"end\":26681,\"start\":26286},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3352281},\"end\":27077,\"start\":26683},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3352400},\"end\":27686,\"start\":27079},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":206594692},\"end\":28123,\"start\":27688},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2021204},\"end\":28649,\"start\":28125},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":196177434},\"end\":29122,\"start\":28651},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1472236},\"end\":29537,\"start\":29124},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52127932},\"end\":29835,\"start\":29539},{\"attributes\":{\"id\":\"b10\"},\"end\":30031,\"start\":29837},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":208780552},\"end\":30491,\"start\":30033},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3930042},\"end\":31137,\"start\":30493},{\"attributes\":{\"doi\":\"arXiv:2108.06479\",\"id\":\"b13\"},\"end\":31535,\"start\":31139},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":225039815},\"end\":31984,\"start\":31537},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":210919923},\"end\":32578,\"start\":31986},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17265929},\"end\":32790,\"start\":32580},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207178809},\"end\":33266,\"start\":32792},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":207189080},\"end\":33891,\"start\":33268},{\"attributes\":{\"doi\":\"arXiv:1803.02155\",\"id\":\"b19\"},\"end\":34172,\"start\":33893},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":53100214},\"end\":34804,\"start\":34174},{\"attributes\":{\"id\":\"b21\"},\"end\":35241,\"start\":34806},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":119181611},\"end\":35871,\"start\":35243},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":13756489},\"end\":36294,\"start\":35873},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6011288},\"end\":36567,\"start\":36296},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":211011303},\"end\":36943,\"start\":36569},{\"attributes\":{\"doi\":\"arXiv:1708.04617\",\"id\":\"b26\"},\"end\":37370,\"start\":36945},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2023817},\"end\":37935,\"start\":37372},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":199465766},\"end\":38584,\"start\":37937}]", "bib_title": "[{\"end\":25849,\"start\":25777},{\"end\":26373,\"start\":26286},{\"end\":26723,\"start\":26683},{\"end\":27123,\"start\":27079},{\"end\":27732,\"start\":27688},{\"end\":28186,\"start\":28125},{\"end\":28712,\"start\":28651},{\"end\":29177,\"start\":29124},{\"end\":29579,\"start\":29539},{\"end\":30097,\"start\":30033},{\"end\":30578,\"start\":30493},{\"end\":31609,\"start\":31537},{\"end\":32099,\"start\":31986},{\"end\":32602,\"start\":32580},{\"end\":32861,\"start\":32792},{\"end\":33330,\"start\":33268},{\"end\":34256,\"start\":34174},{\"end\":35338,\"start\":35243},{\"end\":35898,\"start\":35873},{\"end\":36341,\"start\":36296},{\"end\":36653,\"start\":36569},{\"end\":37428,\"start\":37372},{\"end\":38010,\"start\":37937}]", "bib_author": "[{\"end\":25566,\"start\":25552},{\"end\":25584,\"start\":25566},{\"end\":25603,\"start\":25584},{\"end\":25861,\"start\":25851},{\"end\":25875,\"start\":25861},{\"end\":25891,\"start\":25875},{\"end\":25902,\"start\":25891},{\"end\":25910,\"start\":25902},{\"end\":25923,\"start\":25910},{\"end\":26396,\"start\":26375},{\"end\":26409,\"start\":26396},{\"end\":26419,\"start\":26409},{\"end\":26425,\"start\":26419},{\"end\":26435,\"start\":26425},{\"end\":26450,\"start\":26435},{\"end\":26455,\"start\":26450},{\"end\":26737,\"start\":26725},{\"end\":26746,\"start\":26737},{\"end\":26758,\"start\":26746},{\"end\":26770,\"start\":26758},{\"end\":26785,\"start\":26770},{\"end\":27135,\"start\":27125},{\"end\":27149,\"start\":27135},{\"end\":27163,\"start\":27149},{\"end\":27176,\"start\":27163},{\"end\":27191,\"start\":27176},{\"end\":27207,\"start\":27191},{\"end\":27221,\"start\":27207},{\"end\":27236,\"start\":27221},{\"end\":27249,\"start\":27236},{\"end\":27263,\"start\":27249},{\"end\":27270,\"start\":27263},{\"end\":27746,\"start\":27734},{\"end\":27761,\"start\":27746},{\"end\":27775,\"start\":27761},{\"end\":27785,\"start\":27775},{\"end\":28201,\"start\":28188},{\"end\":28216,\"start\":28201},{\"end\":28724,\"start\":28714},{\"end\":28737,\"start\":28724},{\"end\":29192,\"start\":29179},{\"end\":29205,\"start\":29192},{\"end\":29221,\"start\":29205},{\"end\":29235,\"start\":29221},{\"end\":29598,\"start\":29581},{\"end\":29614,\"start\":29598},{\"end\":29901,\"start\":29887},{\"end\":29909,\"start\":29901},{\"end\":29920,\"start\":29909},{\"end\":29926,\"start\":29920},{\"end\":30112,\"start\":30099},{\"end\":30124,\"start\":30112},{\"end\":30140,\"start\":30124},{\"end\":30594,\"start\":30580},{\"end\":30609,\"start\":30594},{\"end\":30624,\"start\":30609},{\"end\":30639,\"start\":30624},{\"end\":30649,\"start\":30639},{\"end\":30665,\"start\":30649},{\"end\":31151,\"start\":31139},{\"end\":31165,\"start\":31151},{\"end\":31173,\"start\":31165},{\"end\":31183,\"start\":31173},{\"end\":31194,\"start\":31183},{\"end\":31211,\"start\":31194},{\"end\":31218,\"start\":31211},{\"end\":31623,\"start\":31611},{\"end\":31635,\"start\":31623},{\"end\":31646,\"start\":31635},{\"end\":31659,\"start\":31646},{\"end\":31673,\"start\":31659},{\"end\":31686,\"start\":31673},{\"end\":32113,\"start\":32101},{\"end\":32127,\"start\":32113},{\"end\":32140,\"start\":32127},{\"end\":32154,\"start\":32140},{\"end\":32167,\"start\":32154},{\"end\":32620,\"start\":32604},{\"end\":32879,\"start\":32863},{\"end\":32904,\"start\":32879},{\"end\":32925,\"start\":32904},{\"end\":33353,\"start\":33332},{\"end\":33372,\"start\":33353},{\"end\":33392,\"start\":33372},{\"end\":33408,\"start\":33392},{\"end\":33905,\"start\":33893},{\"end\":33922,\"start\":33905},{\"end\":33938,\"start\":33922},{\"end\":34272,\"start\":34258},{\"end\":34284,\"start\":34272},{\"end\":34298,\"start\":34284},{\"end\":34312,\"start\":34298},{\"end\":34322,\"start\":34312},{\"end\":34334,\"start\":34322},{\"end\":34345,\"start\":34334},{\"end\":34934,\"start\":34915},{\"end\":34951,\"start\":34934},{\"end\":34968,\"start\":34951},{\"end\":34984,\"start\":34968},{\"end\":35006,\"start\":34984},{\"end\":35349,\"start\":35340},{\"end\":35358,\"start\":35349},{\"end\":35367,\"start\":35358},{\"end\":35381,\"start\":35367},{\"end\":35391,\"start\":35381},{\"end\":35401,\"start\":35391},{\"end\":35413,\"start\":35401},{\"end\":35916,\"start\":35900},{\"end\":35930,\"start\":35916},{\"end\":35943,\"start\":35930},{\"end\":35960,\"start\":35943},{\"end\":35973,\"start\":35960},{\"end\":35988,\"start\":35973},{\"end\":36003,\"start\":35988},{\"end\":36021,\"start\":36003},{\"end\":36355,\"start\":36343},{\"end\":36363,\"start\":36355},{\"end\":36372,\"start\":36363},{\"end\":36388,\"start\":36372},{\"end\":36666,\"start\":36655},{\"end\":36678,\"start\":36666},{\"end\":36693,\"start\":36678},{\"end\":37059,\"start\":37049},{\"end\":37076,\"start\":37059},{\"end\":37088,\"start\":37076},{\"end\":37099,\"start\":37088},{\"end\":37112,\"start\":37099},{\"end\":37118,\"start\":37112},{\"end\":37439,\"start\":37430},{\"end\":37450,\"start\":37439},{\"end\":37458,\"start\":37450},{\"end\":37470,\"start\":37458},{\"end\":37482,\"start\":37470},{\"end\":38028,\"start\":38012},{\"end\":38043,\"start\":38028},{\"end\":38055,\"start\":38043},{\"end\":38069,\"start\":38055},{\"end\":38080,\"start\":38069},{\"end\":38093,\"start\":38080},{\"end\":38107,\"start\":38093},{\"end\":38122,\"start\":38107}]", "bib_venue": "[{\"end\":26017,\"start\":25923},{\"end\":26466,\"start\":26455},{\"end\":26845,\"start\":26785},{\"end\":27342,\"start\":27270},{\"end\":27862,\"start\":27785},{\"end\":28327,\"start\":28216},{\"end\":28833,\"start\":28737},{\"end\":29296,\"start\":29235},{\"end\":29670,\"start\":29614},{\"end\":29885,\"start\":29837},{\"end\":30218,\"start\":30140},{\"end\":30761,\"start\":30665},{\"end\":31312,\"start\":31234},{\"end\":31743,\"start\":31686},{\"end\":32241,\"start\":32167},{\"end\":32669,\"start\":32620},{\"end\":32991,\"start\":32925},{\"end\":33519,\"start\":33408},{\"end\":34007,\"start\":33954},{\"end\":34437,\"start\":34345},{\"end\":34913,\"start\":34806},{\"end\":35505,\"start\":35413},{\"end\":36070,\"start\":36021},{\"end\":36415,\"start\":36388},{\"end\":36731,\"start\":36693},{\"end\":37047,\"start\":36945},{\"end\":37593,\"start\":37482},{\"end\":38203,\"start\":38122},{\"end\":26892,\"start\":26847},{\"end\":27401,\"start\":27344},{\"end\":27926,\"start\":27864},{\"end\":28425,\"start\":28329},{\"end\":28916,\"start\":28835},{\"end\":29344,\"start\":29298},{\"end\":30283,\"start\":30220},{\"end\":30844,\"start\":30763},{\"end\":32302,\"start\":32243},{\"end\":33044,\"start\":32993},{\"end\":33617,\"start\":33521},{\"end\":34516,\"start\":34439},{\"end\":35584,\"start\":35507},{\"end\":36429,\"start\":36417},{\"end\":36756,\"start\":36733},{\"end\":37691,\"start\":37595},{\"end\":38271,\"start\":38205}]"}}}, "year": 2023, "month": 12, "day": 17}