{"id": 254636529, "updated": "2023-10-05 06:48:50.143", "metadata": {"title": "NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior", "authors": "[{\"first\":\"Wenjing\",\"last\":\"Bian\",\"middle\":[]},{\"first\":\"Zirui\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Kejie\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jia-Wang\",\"last\":\"Bian\",\"middle\":[]},{\"first\":\"Victor\",\"last\":\"Prisacariu\",\"middle\":[\"Adrian\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Training a Neural Radiance Field (NeRF) without pre-computed camera poses is challenging. Recent advances in this direction demonstrate the possibility of jointly optimising a NeRF and camera poses in forward-facing scenes. However, these methods still face difficulties during dramatic camera movement. We tackle this challenging problem by incorporating undistorted monocular depth priors. These priors are generated by correcting scale and shift parameters during training, with which we are then able to constrain the relative poses between consecutive frames. This constraint is achieved using our proposed novel loss functions. Experiments on real-world indoor and outdoor scenes show that our method can handle challenging camera trajectories and outperforms existing methods in terms of novel view rendering quality and pose estimation accuracy. Our project page is https://nope-nerf.active.vision.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.07388", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/BianWLB23", "doi": "10.1109/cvpr52729.2023.00405"}}, "content": {"source": {"pdf_hash": "9a1646e96ae3bda9f528ca747a3c7f591735f2c0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.07388v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "45a82a2be6a3d2b595aa83985981359fe810e769", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9a1646e96ae3bda9f528ca747a3c7f591735f2c0.txt", "contents": "\nNoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior\n\n\nWenjing Bian wenjing@robots.ox.ac.uk \nActive Vision Lab\nUniversity of Oxford\n\n\nZirui Wang \nActive Vision Lab\nUniversity of Oxford\n\n\nKejie Li \nActive Vision Lab\nUniversity of Oxford\n\n\nJia-Wang Bian \nActive Vision Lab\nUniversity of Oxford\n\n\nVictor Adrian Prisacariu victor@robots.ox.ac.uk \nActive Vision Lab\nUniversity of Oxford\n\n\nNoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior\n\nOurs BARFNeRFmm SC-NeRFFigure 1. Novel view synthesis comparison. We propose NoPe-NeRF for joint pose estimation and novel view synthesis. Our method enables more robust pose estimation and renders better novel view synthesis than previous state-of-the-art methods.AbstractTraining a Neural Radiance Field (NeRF) without precomputed camera poses is challenging. Recent advances in this direction demonstrate the possibility of jointly optimising a NeRF and camera poses in forward-facing scenes. However, these methods still face difficulties during dramatic camera movement. We tackle this challenging problem by incorporating undistorted monocular depth priors. These priors are generated by correcting scale and shift parameters during training, with which we are then able to constrain the relative poses between consecutive frames. This constraint is achieved using our proposed novel loss functions. Experiments on real-world indoor and outdoor scenes show that our method can handle challenging camera trajectories and outperforms existing methods in terms of novel view rendering quality and pose estimation accuracy. Our project page is https://nope-nerf. active.vision.\n\nIntroduction\n\nThe photo-realistic reconstruction of a scene from a stream of RGB images requires both accurate 3D geometry reconstruction and view-dependent appearance modelling. Recently, Neural Radiance Fields (NeRF) [24] have demonstrated the ability to build high-quality results for generating photo-realistic images from novel viewpoints given a sparse set of images.\n\nAn important preparation step for NeRF training is the estimation of camera parameters for the input images. A current go-to option is the popular Structure-from-Motion (SfM) library COLMAP [35]. Whilst easy to use, this preprocessing step could be an obstacle to NeRF research and real-world deployments in the long term due to its long processing time and its lack of differentiability. Recent works such as NeRFmm [47], BARF [18] and SC-NeRF [12] propose to simultaneously optimise camera poses and the neural implicit representation to address these issues. Nevertheless, these methods can only handle forward-facing scenes when no initial parameters are supplied, and fail in dramatic camera motions, e.g.a casual handheld captured video. This limitation has two key causes. First, all these methods estimate a camera pose for each input image individually without considering relative poses between images. Looking back to the literature of Simultaneous localisation and mapping (SLAM) and visual odometry, pose estimation can significantly benefit from estimating relative poses be-tween adjacent input frames. Second, the radiance field is known to suffer from shape-radiance ambiguity [56]. Estimating camera parameters jointly with NeRF adds another degree of ambiguity, resulting in slow convergence and unstable optimisation.\n\nTo handle the limitation of large camera motion, we seek help from monocular depth estimation [22,28,29,52]. Our motivation is threefold: First, monocular depth provides strong geometry cues that are beneficial to constraint shaperadiance ambiguity. Second, relative poses between adjacent depth maps can be easily injected into the training pipeline via Chamfer Distance. Third, monocular depth is lightweight to run and does not require camera parameters as input, in contrast to multi-view stereo depth estimation. For simplicity, we use the term mono-depth from now on.\n\nUtilising mono-depth effectively is not straightforward with the presence of scale and shift distortions. In other words, mono-depth maps are not multi-view consistent. Previous works [9,17,48] simply take mono-depth into a depth-wise loss along with NeRF training. Instead, we propose a novel and effective way to thoroughly integrate mono-depth into our system. First, we explicitly optimise scale and shift parameters for each mono-depth map during NeRF training by penalising the difference between rendered depth and mono-depth. Since NeRF by itself is trained based on multiview consistency, this step transforms mono-depth maps to undistorted multiview consistent depth maps. We further leverage these multiview consistent depth maps in two loss terms: a) a Chamfer Distance loss between two depth maps of adjacent images, which injects relative pose to our system; and b) a depth-based surface rendering loss, which further improves relative pose estimation.\n\nIn summary, we propose a method to jointly optimise camera poses and a NeRF from a sequence of images with large camera motion. Our system is enabled by three contributions. First, we propose a novel way to integrate monodepth into unposed-NeRF training by explicitly modelling scale and shift distortions. Second, we supply relative poses to the camera-NeRF joint optimisation via an inter-frame loss using undistorted mono-depth maps. Third, we further regularise our relative pose estimation with a depth-based surface rendering loss.\n\nAs a result, our method is able to handle large camera motion, and outperforms state-of-the-art methods by a significant margin in terms of novel view synthesis quality and camera trajectory accuracy.\n\n\nRelated Work\n\nNovel View Synthesis. While early Novel View Synthesis (NVS) approaches applied interpolations between pixels [3], later works often rendered images from 3D reconstruction [1,6]. In recent years, different representations of the 3D scene are used, e.g.meshes [30,31], Multi-Plane Images [42,60], layered depth [43] etc. Among them, NeRF [24] has become a popular scene representation for its photorealistic rendering.\n\nA number of techniques are proposed to improve NeRF's performance with additional regularisation [13,26,55], depth priors [7,32,48,54], surface enhancements [27,44,50] or latent codes [41,45,53]. Other works [2,10,25,34] have also accelerated NeRF training and rendering. However, most of these approaches require pre-computed camera parameters obtained from SfM algorithms [11,35].\n\nNeRF With Pose Optimisation. Removing camera parameter preprocessing is an active line of research recently. One category of the methods [33,39,61] use a SLAM-style pipeline, that either requires RGB-D inputs or relies on accurate camera poses generated from the SLAM tracking system. Another category of works optimises camera poses with the NeRF model directly. We term this type of method as unposed-NeRF in this paper. iNeRF [51] shows that poses for novel view images can be estimated using a reconstructed NeRF model. GNeRF [21] combines Generative Adversarial Networks with NeRF to estimate camera poses but requires a known sampling distribution for poses. More relevant to our work, NeRFmm [47] jointly optimises both camera intrinsics and extrinsics alongside NeRF training. BARF [18] proposes a coarse-to-fine positional encoding strategy for camera poses and NeRF joint optimisation. SC-NeRF [12] further parameterises camera distortion and employs a geometric loss to regularise rays. GARF [4] shows that using Gaussian-MLPs makes joint pose and scene optimisation easier and more accurate. Recently, SiNeRF [49] uses SIREN [37] layers and a novel sampling strategy to alleviate the sub-optimality of joint optimisation in NeRFmm.\n\nAlthough showing promising results on the forward-facing dataset like LLFF [23], these approaches face difficulties when handling challenging camera trajectories with large camera motion. We address this issue by closely integrating mono-depth maps with the joint optimisation of camera parameters and NeRF.\n\n\nMethod\n\nWe tackle the challenge of handling large camera motions in unposed-NeRF training. Given a sequence of images, camera intrinsics, and their mono-depth estimations, our method recovers camera poses and optimises a NeRF simultaneously. We assume camera intrinsics are available in the image meta block, and we run an off-the-shelf monodepth network DPT [7] to acquire mono-depth estimations. Without repeating the benefit of mono-depth, we unroll this section around the effective integration of monocular depth into unposed-NeRF training.\n\nThe training is a joint optimisation of the NeRF, camera poses, and distortion parameters of each mono-depth  Our method takes a sequence of images as input to reconstruct NeRF and jointly estimates the camera poses of the frames. We first generate monocular depth maps from a mono-depth estimation network and reconstruct the point clouds. We then optimise NeRF, camera poses, and depth distortion parameters jointly with inter-frame and NeRF losses.\n\nmap. The distortion parameters are supervised by minimising the discrepancies between the mono-depth maps and depth maps rendered from the NeRF, which are multiview consistent. The undistorted depth maps in return effectively mediate the shape-radiance ambiguity, which eases the training of NeRF and camera poses.\n\nSpecifically, the undistorted depth maps enable two constraints. We constrain global pose estimation by supplying relative pose between adjacent images. This is achieved via a Chamfer-Distance-based correspondence between two point clouds, back-projected from undistorted depth maps. Further, we regularise relative pose estimation with a surface-based photometric consistency where we treat undistorted depth as surface.\n\nWe detail our method in the following sections, starting from NeRF in Sec. 3.1 and unposed-NeRF training in Sec. 3.2, looking into mono-depth distortions in Sec. 3.3, followed by our mono-depth enabled loss terms in Sec. 3.4, and finishing with an overall training pipeline Sec. 3.5.\n\n\nNeRF\n\nNeural Radiance Field (NeRF) [24] represents a scene as a mapping function F \u0398 : (x, d) \u2192 (c, \u03c3) that maps a 3D location x \u2208 R 3 and a viewing direction d \u2208 R 3 to a radiance colour c \u2208 R 3 and a volume density value \u03c3. This mapping is usually implemented with a neural network parameterised by \nL rgb = N i \u2225I i \u2212\u00ce i \u2225 2\n2 between synthesised images\u00ce and captured images I:\n\u0398 * = arg min \u0398 L rgb (\u00ce | I, \u03a0),(1)\nwhere\u00ce i is rendered by aggregating radiance colour on camera rays r(h) = o + hd between near and far bound h n and h f . More concretely, we synthesise\u00ce i with a volumetric rendering function\nI i (r) = h f hn T (h)\u03c3(r(h))c(r(h), d)dh,(2)\nwhere T (h) = exp(\u2212 h hn \u03c3(r(s))ds) is the accumulated transmittance along a ray. We refer to [24] for further details.\n\n\nJoint Optimisation of Poses and NeRF\n\nPrior works [12,18,47] show that it is possible to estimate both camera parameters and a NeRF at the same time by minimising the above photometric error L rgb under the same volumetric rendering process in Eq. (2).\n\nThe key lies in conditioning camera ray casting on variable camera parameters \u03a0, as the camera ray r is a function of camera pose. Mathematically, this joint optimisation can be formulated as:\n\u0398 * , \u03a0 * = arg min \u0398,\u03a0 L rgb (\u00ce,\u03a0 | I),(3)\nwhere\u03a0 denotes camera parameters that are updated during optimising. Note that the only difference between Eq. (1) and Eq. (3) is that Eq. (3) considers camera parameters as variables.\n\nIn general, the camera parameters \u03a0 include camera intrinsics, poses, and lens distortions. We only consider estimating camera poses in this work, e.g., camera pose for\nframe I i is a transformation T i = [R i | t i ] with a rotation R i \u2208 SO(3) and a translation t i \u2208 R 3 .\n\nUndistortion of Monocular Depth\n\nWith an off-the-shelf monocular depth network, e.g., DPT [28], we generate mono-depth sequence D = {D i | i = 0 . . . N \u2212 1} from input images. Without surprise, mono-depth maps are not multi-view consistent so we aim to recover a sequence of multi-view consistent depth maps, which are further leveraged in our relative pose loss terms.\n\nSpecifically, we consider two linear transformation parameters for each mono-depth map, resulting in a sequence of transformation parameters for all frames \u03a8 =\n{(\u03b1 i , \u03b2 i ) | i = 0 . . . N \u2212 1},\nwhere \u03b1 i and \u03b2 i denote a scale and a shift factor. With multi-view consistent constraint from NeRF, we aim to recover a multi-view consistent depth map D * i for D i :\nD * i = \u03b1 i D i + \u03b2 i ,(4)\nby joint optimising \u03b1 i and \u03b2 i along with a NeRF. This joint optimisation is mostly achieved by enforcing the consistency between an undistorted depth map D * i and a NeRF rendered depth mapD i via a depth loss:\nL depth = N i D * i \u2212D i ,(5)\nwhereD\ni (r) = h f hn T (h)\u03c3(r(h))dh(6)\ndenotes a volumetric rendered depth map from NeRF. It is important to note that both NeRF and mono-depth benefit from Eq. (5). On the one hand, mono-depth provides strong geometry prior for NeRF training, reducing shaperadiance ambiguity. On the other hand, NeRF provides multi-view consistency so we can recover a set of multiview consistent depth maps for relative pose estimations.\n\n\nRelative Pose Constraint\n\nAforementioned unposed-NeRF methods [12,18,47] optimise each camera pose independently, resulting in an overfit to target images with incorrect poses. Penalising incorrect relative poses between frames can help to regularise the joint optimisation towards smooth convergence, especially in a complex camera trajectory. Therefore, we propose two losses that constrain relative poses.\n\nPoint Cloud Loss. We back-project the undistorted depth maps D * using the known camera intrinsics, to point clouds\nP * = {P * i | i = 0 . . . N \u2212 1}\nand optimise the relative pose between consecutive point clouds by minimising a point cloud loss L pc :\nL pc = (i,j) l cd (P * j , T ji P * i ),(7)\nwhere T ji = T j T \u22121 i represents the related pose that transforms point cloud P * i to P * j , tuple (i, j) denotes indices of a consecutive pair of instances, and l cd denotes Chamfer Distance:\nl cd (P i , P j ) = pi\u2208Pi min pj \u2208Pj \u2225p i \u2212p j \u2225 2 + pj \u2208Pj min pi\u2208Pi \u2225p i \u2212p j \u2225 2 .(8)\nSurface-based Photometric Loss. While the point cloud loss L pc offers supervision in terms of 3D-3D matching, we observe that a surface-based photometric error can alleviate incorrect matching. With the photometric consistency assumption, this photometric error penalises the differences in appearance between associated pixels. The association is established by projecting the point cloud P * i onto images I i and I j .\n\nThe surface-based photometric loss can then be defined as:\nL rgb\u2212s = (i,j) \u2225I i \u27e8K i P * i \u27e9 \u2212 I j \u27e8K j T j T \u22121 i P * i \u27e9\u2225,(9)\nwhere \u27e8\u00b7\u27e9 represents the sampling operation on the image and K i denotes a projection matrix for i th camera.\n\n\nOverall Training Pipeline\n\nAssembling all loss terms, we get the overall loss function:\nL = L rgb + \u03bb 1 L depth + \u03bb 2 L pc + \u03bb 3 L rgb\u2212s ,(10)\nwhere \u03bb 1 , \u03bb 2 , \u03bb 3 are the weighting factors for respective loss terms. By minimising the combined of loss L:\n\u0398 * , \u03a0 * , \u03a8 * = arg min \u0398,\u03a0,\u03a8 L(\u00ce,D,\u03a0,\u03a8 | I, D),(11)\nour method returns the optimised NeRF parameters \u0398, camera poses \u03a0, and distortion parameters \u03a8.\n\n\nExperiments\n\nWe begin with a description of our experimental setup in Sec. 4.1. In Sec. 4.2, we compare our method with poseunknown methods. Next, we compare our method with the COLMAP-assisted NeRF baseline in Sec   \n\n\nExperimental Setup\n\nDatasets. We conduct experiments on two datasets Tanks and Temples [15] and ScanNet [5]. Tanks and Temples: we use 8 scenes to evaluate pose accuracy and novel view synthesis quality. We chose scenes captured at both indoor and outdoor locations, with different frame sampling rates and lengths. All images are down-sampled to a resolution of 960 \u00d7 540. For the family scene, we sample 200 images and take 100 frames with odd frame ids as training images and the remaining 100 frames for novel view synthesis, in order to analyse the performance under smooth motion. For the remaining scenes, following NeRF [24], 1/8 of the images in each sequence are held out for novel view synthesis, unless otherwise specified. ScanNet: we select 4 scenes for evaluating pose accuracy, depth accuracy, and novel view synthesis quality. For each scene, we take 80-100 consecutive images and use 1/8 of these images for novel view synthesis. For evaluation, we employ depth maps and poses provided by ScanNet as ground truth. ScanNet images are down-sampled to 648 \u00d7 484. We crop images with dark orders during preprocessing.\n\nMetrics. We evaluate our proposed method in three aspects. For novel view synthesis, we follow previous methods [12,18,47], and use standard evaluation metrics, including Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM) [46] and Learned Perceptual Image Patch Similarity (LPIPS) [57]. For pose evaluation, We use standard visual odometry metrics [16,38,58], including the Absolute Trajectory Error (ATE) and Relative Pose Error (RPE). ATE measures the difference between the estimated camera positions and the ground truth positions. RPE measures the relative pose errors between pairs of images, which consists of relative rotation error (RPE r ) and relative translation error (RPE t ). The estimated trajectory is aligned with the ground truth using Sim(3) with 7 degrees of freedom. We use standard depth metrics [8,19,20,40] (Abs Rel, Sq Rel, RMSE, RMSE log, \u03b4 1 , \u03b4 2 and \u03b4 3 ) for depth evaluation. For further detail, please refer to the supplementary material. To recover the metric scale, we follow Zhou et al. [59] and match the median value between rendered and ground truth depth maps.\n\nImplementation Details. Our model architecture is based on NeRF [24] with a few modifications: a) replacing ReLU activation function with Softplus and b) sampling 128 points along each ray uniformly with noise, between a predefined range (0.1, 10). We use 2 separate Adam optimisers [14] for NeRF and other parameters. The initial learning rate for NeRF is 0.001 and for the pose and distortion is 0.0005. Camera rotations are optimised in axis-angle representation \u03d5 i \u2208 so(3). We first train the model with all losses with constant learning rates until convergence. Then, we decay the learning rates with different schedulers and gradually reduce weights for the inter-frame losses and depth loss to further train for 10,000 epochs. We balance the loss terms with \u03bb 1 = 0.04, \u03bb 2 = 1.0 and \u03bb 3 = 1.0. For each training step, we randomly sample 1024 pixels (rays) from each input image and 128 samples per ray. More details are provided in the supplementary material.\n\n\nComparing With Pose-Unknown Methods\n\nWe compare our method with pose-unknown baselines, including BARF [18], NeRFmm [47] and SC-NeRF [12].\n\nView Synthesis Quality. To obtain the camera poses of test views for rendering, we minimise the photometric error  of the synthesised images while keeping the NeRF model fixed, as in NeRFmm [47]. Each test pose is initialised with the learned pose of the training frame that is closest to it. We use the same pre-processing for all baseline approaches, which results in higher accuracy than their original implementations. More details are provided in the supplementary material. Our method outperforms all the baselines by a large margin. The quantitative results are summarised in Tab. 1, and qualitative results are shown in Fig. 3. We recognised that because the test views, which are sampled from videos, are close to the training views, good results may be obtained due to overfitting to the training images. Therefore, we conduct an additional qualitative evaluation on more novel views. Specifically, we fit a bezier curve from the estimated training poses and sample interpolated poses for each method to render novel view videos. Sampled results are shown in Fig. 5, and the rendered videos are in the supplementary material. These results show that our method renders photo-realistic images consistently, while other methods generate visible artifacts.\n\nCamera Pose. Our method significantly outperforms other baselines in all metrics. The quantitative pose evaluation results are shown in Tab. 2. For ScanNet, we use the camera poses provided by the dataset as ground truth. For Tanks and Temples, not every video comes with ground truth poses, so we use COLMAP estimations for reference. Our estimated trajectory is better aligned with the ground truth than other methods, and our estimated rotation is two  Table 3. Depth map evaluation on ScanNet. Our depth estimation is more accurate than baseline models BARF [18], NeRFmm [47] and SC-NeRF [12]. Compared with DPT [59], we show our depth is more accurate after undistortion.\n\norders of magnitudes more accurate than others. We visualise the camera trajectories and rotations in Fig. 4. Depth. We evaluate the accuracy of the rendered depth maps on ScanNet, which provides the ground-truth depths for evaluation. Our rendered depth maps achieve superior accuracy over the previous alternatives. We also compare with the mono-depth maps estimated by DPT. Our rendered depth maps, after undistortion using multiview consistency in the NeRF optimisation, outperform DPT by a large margin. The results are summarised in Tab. 3, and sampled qualitative results are illustrated in Fig. 3. \n\n\nComparing With COLMAP Assisted NeRF\n\nWe make a comparison of pose estimation accuracy between our method and COLMAP against ground truth poses in ScanNet. We achieve on-par accuracy with COLMAP, as shown in Tab. 4. We further analyse the novel view synthesis quality of the NeRF model trained with our learned poses to COLMAP poses on ScanNet and Tanks and Temples. The original NeRF training contains two stages, finding poses using COLMAP and optimising the scene representation. In order to make our comparison fairer, in this section only, we mimic a similar two-stage training as the original NeRF [24]. In the first stage, we train our method with all losses for camera pose estimation, i.e., mimicking the COLMAP processing. Then, we fix the optimised poses and train a NeRF model from scratch, using the same set- tings and loss as the original NeRF. This evaluation enables us to compare our estimated poses to the COLMAP poses indirectly, i.e., in terms of contribution to view synthesis. Our two-stage method outperforms the COLMAPassisted NeRF baseline, which indicates a better pose estimation for novel view synthesis. The results are summarised in Tab. 5.\n\nAs is commonly known, COLMAP performs poorly in low-texture scenes and sometimes fails to find accurate camera poses. Fig. 6 shows an example of a low-texture scene where COLMAP provides inaccurate pose estimation that causes NeRF to render images with visible artifacts. In contrast, our method renders high-quality images, thanks to robust optimisation of camera pose.\n\nInterestingly, this experiment also reveals that the two-    Table 5. Comparison to NeRF with COLMAP poses. Our twostage method (Ours-r) outperforms both COLMAP+NeRF and our one-stage method (Ours). stage method shows higher accuracy than the one-stage method. We hypothesise that the joint optimisation (from randomly initialised poses) in the one-stage approach causes the NeRF optimisation to be trapped in a local minimum, potentially due to the bad pose initialisation. The two-stage approach circumvents this issue by re-initialising the NeRF and re-training with well-optimised poses, resulting in higher performance.\n\n\nAblation Study\n\nIn this section, we analyse the effectiveness of the parameters and components that have been added to our model. The results of ablation studies are shown in Tab. 6.\n\nEffect of Distortion Parameters. We find that ignoring depth distortions (i.e., setting scales to 1 and shifts to 0 as constants) leads to a degradation in pose accuracy, as inconsistent distortions of depth maps introduce errors to the estimation of relative poses and confuse NeRF for geometry reconstruction.\n\nEffect of Inter-frame Losses. We observe that the interframe losses are the major contributor to improving relative poses. When removing the pairwise point cloud loss L pc or the surface-based photometric loss L rgb\u2212s , there is less constraint between frames, and thus the pose accuracy becomes lower.\n\nEffect of NeRF Losses. When the depth loss L depth is removed, the distortions of input depth maps are only optimised locally through the inter-frame losses. We find that this can lead to drift and degradation in pose accuracy.\n\n\nLimitations\n\nOur proposed method optimises camera pose and the NeRF model jointly and works on challenging scenes where other baselines fail. However, the optimisation of the model is also affected by non-linear distortions and the accuracy of the mono-depth estimation, which we did not consider.\n\n\nConclusion\n\nIn this work, we present NoPe-NeRF, an end-to-end differentiable model for joint camera pose estimation and novel view synthesis from a sequence of images. We demonstrate that previous approaches have difficulty with complex trajectories. To tackle this challenge, we use mono-depth maps to constrain the relative poses between frames and regularise the geometry of NeRF, which leads to better pose estimation. We show the effectiveness and robustness of NoPe-NeRF on challenging scenes. The improved pose estimation leads to better novel view synthesis quality and geometry reconstruction compared with other approaches. We believe our method is an important step towards applying the unknown-pose NeRF models to largescale scenes in the future.\n\n\nA.1. Dataset\n\nWe select sequences containing dramatic camera motions from ScanNet [5] and Tanks and Tamples [15] for training and evaluation. Tab. 7 lists details about these sequences, where Max rotation denotes the maximum relative rotation angle between any two frames in a sequence. The sampled images are further split into training and test sets. Starting from the 5th image, we sample every 8th image in a sequence as a test image. However, this leads to a change in the sampling rate in the temporal domain among training images. We found that the rotation errors are often higher than average at these positions where the sampling rate changes. In order to study the effect of the sampling rate changes, for scene Family in Tanks and Temples [15], we sample every other image as test images, i.e. training on images with odd frame ids and testing on images with even frame ids.  Table 7. Details of selected sequences. We downsample several videos to a lower frame rate. FPS denote frame per second. Max rotation denotes the maximum relative rotation angle between any two frames in a sequence. We show our method can handle dramatic camera motion (large maximum rotation angle) whereas previous methods can only handle forward-facing scenes.\n\n\nA.2. Training Details\n\nDuring training, we sample 1024 pixels/rays for an image and we sample 128 points along each ray for our approaches and all baselines. For all approaches, we use the same pre-defined sampling range (i.e., near and far) and sample uniformly between this range. During scheduling, the learning rate of NeRF model decays every 10 epochs with 0.9954, and the learning rate for the camera poses decays every 100 epochs with 0.9. As the scene scales can be arbitrary, the optimised scale parameter of the depth map during training is also arbitrary. To avoid scale collapsing (all scales reduced to 0.0) during training, we manually set the scale of the depth map for the last frame to 1.0. We also use the normalised point clouds when computing the inter-frame point cloud loss.\n\n\nA.3. Test-time Optimisation\n\nDuring the evaluation for novel view synthesis, following our baselines NeRFmm [47], BARF [18] and SC-NeRF [12], we run a test-time optimisation to align the camera poses of the test set by minimising the photometric error on the synthesised images, while keeping the trained NeRF model froze. Although all these baseline methods have their own way to align camera poses (discussed below), all of them fail to align camera poses in complex camera trajectories in ScanNet and Tanks and Temples.\n\nTo fairly evaluate all methods in challenging camera trajectories, we propose to align test camera poses by first initialising from learned poses of adjacent training images, followed by a test-time optimisation. We shorthand this alignment as Neighbour + opt. In practice, we find this initialisation is robust and provides the best alignment for all approaches. All results in our main paper are evaluated in this way.\n\nThe following paragraphs outline previous alignment methods, and we show a comparison for all method with a ScanNet scene in Tab. 8.\n\nIdentity + opt. BARF [18] uses test-time optimisation to identify poses for the test frames, where all poses are initialised with identity matrices. This initialisation works well for simple forward-facing scenes, but not for complex trajectories. The optimisation is sensitive to the learning rate, and can easily fall into local minima when the target pose is far from the identity initialisation.\n\nSim(3) + opt. In NeRFmm [47], the poses are first initialised using Sim(3) alignment with an ATE toolbox [58]. Then, an additional test-time optimisation is used to further adjust the test poses. This initialisation works well when the learned poses can be aligned precisely to COLMAP poses (Ours in Tab. 8). However, incorrect pose estimations can affect the Sim(3) alignment.\n\nSim(3) + no opt. In SC-NeRF [12], the test poses are identified using a Sim(3) alignment between COLMAP poses and the learned poses. And no test-time optimisation is used. However, the results are biased toward COLMAP estimations, and misalignment can affect the view synthesis quality significantly.\n\n\nA.4. Evaluation Metrics\n\nNovel View Synthesis. We use Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM) [46] and Learned Perceptual Image Patch Similarity (LPIPS) [57] to measure the novel view synthesis quality. For LPIPS, we use a VGG architecture [36].  Table 9. Novel view synthesis results on LLFF-NeRF dataset.\n\nDepth. The error metrics we use for depth evaluation include Abs Rel, Sq Rel, RMSE, RMSE log, \u03b4 1 , \u03b4 2 and \u03b4 3 . The definitions are as follows:\n\n\u2022 where d is the estimated depth, d gt is the ground truth depth, and V is the collection of all valid pixels on a depth map.\n\n\nAppendix B. Additional Results\n\nLLFF-NeRF Dataset. We compare our approach against NeRFmm on the LLFF-NeRF dataset [23] in terms of novel view synthesis quality (Tab. 9) and pose accuracy (Tab. 10). We show better performances than NeRFmm in both pose accuracy and synthesis quality. We use the normalized device coordinate (NDC) for both approaches.\n\nDepth Estimation. We show detailed depth evaluation results for ScanNet scenes in Tabs. 11 to 14. Our depth estimation accuracy outperforms other baselines by a large margin.\n\nPose Estimation. We visualise additional results for pose estimation on Tanks and Temples (Fig. 9) and ScanNet (Fig. 10). More Visualisations. We present additional qualitative results for novel view synthesis and depth estimation on Tanks and Temples (Fig. 7) and ScanNet (Fig. 8).\n\nFigure 2 .\n2Method Overview.\n\n\nF \u0398 . Given N images I = {I i | i = 0 . . . N \u2212 1} with their camera poses \u03a0 = {\u03c0 i | i = 0 . . . N \u2212 1}, NeRF can be optimised by minimising photometric error\n\n\n. 4.3. Lastly, we conduct ablation studies in Sec. 4.4.\n\nFigure 3 .\n3Qualitative results of novel view synthesis and depth prediction on Tanks and Temples. We visualise the synthesised images and the rendered depth maps (top left of each image) for all methods. NoPe-NeRF is able to recover details for both colour and geometry.\n\nFigure 4 .\n4Pose Estimation Comparison. We visualise the trajectory (3D plot) and relative rotation errors RPEr (bottom colour bar) of each method on Ballroom and Museum. The colour bar on the right shows the relative scaling of colour. More results are in the supplementary.\n\nFigure 5 .\n5Sampled frames from rendered novel view videos. For each method, we fit the learned trajectory with a bezier curve and uniformly sample new viewpoints for rendering. Our method generates significantly better results than previous methods, which show visible artifacts. The full rendered videos and details about generating novel views are provided in the supplementary.\n\nFigure 6 .\n6COLMAP failure case. On a rotation-dominant sequence with low-texture areas, COLMAP fails to estimate correct poses, which results in artifacts in synthesised images.\n\nTable 1 .\n1Novel view synthesis results on ScanNet and Tanks and Temples. Each baseline method is trained with its public code under the original settings and evaluated with the same evaluation protocol.scenes \nOurs \nBARF \nNeRFmm \nSC-NeRF \nPSNR \u2191 \nSSIM \u2191 \nLPIPS \u2193 \nPSNR SSIM LPIPS \nPSNR SSIM LPIPS \nPSNR SSIM LPIPS \n\nScanNet \n\n0079 00 \n32.47 \n0.84 \n0.41 \n32.31 \n0.83 \n0.43 \n30.59 \n0.81 \n0.49 \n31.33 \n0.82 \n0.46 \n0418 00 \n31.33 \n0.79 \n0.34 \n31.24 \n0.79 \n0.35 \n30.00 \n0.77 \n0.40 \n29.05 \n0.75 \n0.43 \n0301 00 \n29.83 \n0.77 \n0.36 \n29.31 \n0.76 \n0.38 \n27.84 \n0.72 \n0.45 \n29.45 \n0.77 \n0.35 \n0431 00 \n33.83 \n0.91 \n0.39 \n32.77 \n0.90 \n0.41 \n31.44 \n0.88 \n0.45 \n32.57 \n0.90 \n0.40 \nmean \n31.86 \n0.83 \n0.38 \n31.41 \n0.82 \n0.39 \n29.97 \n0.80 \n0.45 \n30.60 \n0.81 \n0.41 \n\nTanks and Temples \n\nChurch \n25.17 \n0.73 \n0.39 \n23.17 \n0.62 \n0.52 \n21.64 \n0.58 \n0.54 \n21.96 \n0.60 \n0.53 \nBarn \n26.35 \n0.69 \n0.44 \n25.28 \n0.64 \n0.48 \n23.21 \n0.61 \n0.53 \n23.26 \n0.62 \n0.51 \nMuseum \n26.77 \n0.76 \n0.35 \n23.58 \n0.61 \n0.55 \n22.37 \n0.61 \n0.53 \n24.94 \n0.69 \n0.45 \nFamily \n26.01 \n0.74 \n0.41 \n23.04 \n0.61 \n0.56 \n23.04 \n0.58 \n0.56 \n22.60 \n0.63 \n0.51 \nHorse \n27.64 \n0.84 \n0.26 \n24.09 \n0.72 \n0.41 \n23.12 \n0.70 \n0.43 \n25.23 \n0.76 \n0.37 \nBallroom \n25.33 \n0.72 \n0.38 \n20.66 \n0.50 \n0.60 \n20.03 \n0.48 \n0.57 \n22.64 \n0.61 \n0.48 \nFrancis \n29.48 \n0.80 \n0.38 \n25.85 \n0.69 \n0.57 \n25.40 \n00.69 \n0.52 \n26.46 \n0.73 \n0.49 \nIgnatius \n23.96 \n0.61 \n0.47 \n21.78 \n0.47 \n0.60 \n21.16 \n0.45 \n0.60 \n23.00 \n0.55 \n0.53 \nmean \n26.34 \n0.74 \n0.39 \n23.42 \n0.61 \n0.54 \n22.50 \n0.59 \n0.54 \n23.76 \n0.65 \n0.48 \n\n\n\nTable 2 .\n2Pose accuracy on ScanNet and Tanks and Temples. Note that we use COLMAP poses in Tanks and Temples as the \"ground truth\". The unit of RPEr is in degrees, ATE is in the ground truth scale and RPEt is scaled by 100.scenes \n\nOurs \nBARF \nNeRFmm \nSC-NeRF \nRPEt \u2193 \nRPEr \u2193 \nATE\u2193 \nRPEt \nRPEr \nATE \nRPEt \nRPEr \nATE \nRPEt \nRPEr \nATE \n\nScanNet \n\n0079 00 \n0.752 \n0.204 \n0.023 \n1.110 \n0.480 \n0.062 \n1.706 \n0.636 \n0.100 \n2.064 \n0.664 \n0.115 \n0418 00 \n0.455 \n0.119 \n0.015 \n1.398 \n0.538 \n0.020 \n1.402 \n0.460 \n0.013 \n1.528 \n0.502 \n0.016 \n0301 00 \n0.399 \n0.123 \n0.013 \n1.316 \n0.777 \n0.219 \n3.097 \n0.894 \n0.288 \n1.133 \n0.422 \n0.056 \n0431 00 \n1.625 \n0.274 \n0.069 \n6.024 \n0.754 \n0.168 \n6.799 \n0.624 \n0.496 \n4.110 \n0.499 \n0.205 \nmean \n0.808 \n0.180 \n0.030 \n2.462 \n0.637 \n0.117 \n3.251 \n0.654 \n0.224 \n2.209 \n0.522 \n0.098 \n\nTanks and Temples \n\nChurch \n0.034 \n0.008 \n0.008 \n0.114 \n0.038 \n0.052 \n0.626 \n0.127 \n0.065 \n0.836 \n0.187 \n0.108 \nBarn \n0.046 \n0.032 \n0.004 \n0.314 \n0.265 \n0.050 \n1.629 \n0.494 \n0.159 \n1.317 \n0.429 \n0.157 \nMuseum \n0.207 \n0.202 \n0.020 \n3.442 \n1.128 \n0.263 \n4.134 \n1.051 \n0.346 \n8.339 \n1.491 \n0.316 \nFamily \n0.047 \n0.015 \n0.001 \n1.371 \n0.591 \n0.115 \n2.743 \n0.537 \n0.120 \n1.171 \n0.499 \n0.142 \nHorse \n0.179 \n0.017 \n0.003 \n1.333 \n0.394 \n0.014 \n1.349 \n0.434 \n0.018 \n1.366 \n0.438 \n0.019 \nBallroom \n0.041 \n0.018 \n0.002 \n0.531 \n0.228 \n0.018 \n0.449 \n0.177 \n0.031 \n0.328 \n0.146 \n0.012 \nFrancis \n0.057 \n0.009 \n0.005 \n1.321 \n0.558 \n0.082 \n1.647 \n0.618 \n0.207 \n1.233 \n0.483 \n0.192 \nIgnatius \n0.026 \n0.005 \n0.002 \n0.736 \n0.324 \n0.029 \n1.302 \n0.379 \n0.041 \n0.533 \n0.240 \n0.085 \nmean \n0.080 \n0.038 \n0.006 \n1.046 \n0.441 \n0.078 \n1.735 \n0.477 \n0.123 \n1.890 \n0.489 \n0.129 \n\nAbs Rel \u2193 Sq Rel \u2193 RMSE \u2193 RMSE log \u2193 \u03b4 1 \u2191 \u03b4 2 \u2191 \u03b4 3 \u2191 \nOurs \n0.141 \n0.137 \n0.568 \n0.176 \n0.828 0.970 0.987 \nBARF \n0.376 \n0.684 \n0.990 \n0.401 \n0.490 0.751 0.884 \nNeRFmm 0.590 \n1.721 \n1.672 \n0.587 \n0.316 0.560 0.743 \nSC-NeRF 0.417 \n0.642 \n1.079 \n0.476 \n0.362 0.658 0.832 \nDPT \n0.197 \n0.246 \n0.751 \n0.226 \n0.747 0.934 0.975 \n\n\n\n\nSSIM \u2191 LPIPS \u2193 PSNR SSIM LPIPS PSNR SSIM LPIPSscenes \nOurs \nOurs-r \nCOLMAP+NeRF \nPSNR \u2191 ScanNet \n\n0079 00 \n32.47 \n0.84 \n0.41 \n33.12 0.85 0.40 31.98 0.83 0.43 \n0418 00 \n31.33 \n0.79 \n0.34 \n30.49 0.77 0.40 30.60 0.78 0.40 \n0301 00 \n29.83 \n0.77 \n0.36 \n30.05 0.78 0.34 30.01 0.78 0.36 \n0431 00 \n33.83 \n0.91 \n0.39 \n33.86 0.91 0.39 33.54 0.91 0.39 \nmean \n31.86 \n0.83 \n0.38 \n31.88 0.83 0.38 31.53 0.82 0.40 \n\nTanks and Temples \n\nChurch \n25.17 \n0.73 \n0.39 \n26.74 0.78 0.32 25.72 0.75 0.37 \nBarn \n26.35 \n0.69 \n0.44 \n26.58 0.71 0.42 26.72 0.71 0.42 \nMuseum 26.77 \n0.76 \n0.35 \n26.98 0.77 0.36 27.21 0.78 0.34 \nFamily \n26.01 \n0.74 \n0.41 \n26.21 0.75 0.40 26.61 0.77 0.39 \nHorse \n27.64 \n0.84 \n0.26 \n28.06 0.84 0.26 27.02 0.82 0.29 \nBallroom 25.33 \n0.72 \n0.38 \n25.53 0.73 0.38 25.47 0.73 0.38 \nFrancis \n29.48 \n0.80 \n0.38 \n29.73 0.81 0.38 30.05 0.81 0.38 \nIgnatius \n23.96 \n0.61 \n0.47 \n23.98 0.62 0.46 24.08 0.61 0.47 \nmean \n26.34 \n0.74 \n0.39 \n26.73 0.75 0.37 26.61 0.75 0.38 \n\n\n\n\nNVS Pose PSNR\u2191 SSIM \u2191 LPIPS \u2193 RPE t \u2193 RPE r \u2193 ATE \u2193Ours \n31.86 0.83 \n0.38 \n0.801 0.181 0.031 \nOurs w/o \u03b1, \u03b2 \n31.46 0.82 \n0.39 \n1.929 0.321 0.066 \nOurs w/o L pc \n31.73 0.82 \n0.38 \n2.227 0.453 0.101 \nOurs w/o L rgb\u2212s 31.05 0.81 \n0.41 \n1.814 0.401 0.156 \nOurs w/o L depth 31.20 0.81 \n0.40 \n1.498 0.383 0.089 \n\nTable 6. Ablation study results on ScanNet. \n\n\n\nTable 8 .\n8Sim(3) + no opt. Comparison of various pose alignment methods during test-time optimisation (ScanNet 0079 00).Identity + opt. \nSim(3) + opt. \n(4) Neighbour + opt \nPSNR\u2191 \nSSIM \u2191 \nLPIPS \u2193 \nPSNR \nSSIM \nLPIPS \nPSNR \nSSIM \nLPIPS \nPSNR \nSSIM \nLPIPS \nOurs \n17.24 \n0.62 \n0.58 \n13.38 \n0.39 \n0.70 \n32.47 \n0.84 \n0.41 \n32.47 \n0.84 \n0.41 \nBARF \n14.68 \n0.55 \n0.66 \n19.56 \n0.65 \n0.57 \n17.82 \n0.60 \n0.61 \n32.31 \n0.83 \n0.43 \nNeRFmm \n11.28 \n0.40 \n0.80 \n30.59 \n0.81 \n0.49 \n12.46 \n0.43 \n0.80 \n30.59 \n0.81 \n0.49 \nSC-NeRF \n10.68 \n0.38 \n0.80 \n22.39 \n0.71 \n0.55 \n11.25 \n0.40 \n0.80 \n31.33 \n0.82 \n0.46 \n\nscenes \nOurs \nNeRFmm \nPSNR \u2191 SSIM \u2191 LPIPS \u2193 \nPSNR SSIM LPIPS \nFern \n23.01 \n0.71 \n0.38 \n20.58 \n0.59 \n0.50 \nFlower \n29.39 \n0.86 \n0.19 \n27.02 \n0.76 \n0.32 \nFortress \n29.38 \n0.80 \n0.28 \n24.94 \n0.57 \n0.57 \nHorns \n25.24 \n0.73 \n0.37 \n23.67 \n0.66 \n0.48 \nLeaves \n19.85 \n0.60 \n0.40 \n19.46 \n0.55 \n0.46 \nOrchids \n19.51 \n0.56 \n0.43 \n16.77 \n0.40 \n0.55 \nRoom \n28.54 \n0.89 \n0.28 \n26.14 \n0.84 \n0.39 \nTrex \n25.82 \n0.84 \n0.29 \n24.13 \n0.77 \n0.39 \nmean \n25.09 \n0.75 \n0.33 \n22.84 \n0.64 \n0.46 \n\n\n\n\nAbs Rel: 1 |V| d\u2208V \u2225d \u2212 d gt \u2225/d gt ; \u2022 Sq Rel: 1 |V| d\u2208V \u2225d \u2212 d gt \u2225 2 2 /d gt ;\u2022 RMSE:d\u2208V \u2225 log d \u2212 log d gt \u2225 2 2 ; \u2022 \u03b4 i : % of y s.t. max( d dgt ,1 \n|V| \n\nd\u2208V \u2225d \u2212 d gt \u2225 2 \n2 ; \n\n\u2022 RMSE log: \n\n1 \n|V| \n\ndgt \n\nd ) = \u03b4 < i; \n\n\n\n\nTable 10. Pose accuracy on LLFF-NeRF dataset.0079 00 Abs Rel \u2193 Sq Rel \u2193 RMSE \u2193 RMSE log \u2193 \u03b4 1 \u2191 \u03b4 2 \u2191 \u03b4 3 \u2191Table 11. Depth map evaluation on ScanNet 0079 00.0418 00 Abs Rel \u2193 Sq Rel \u2193 RMSE \u2193 RMSE log \u2193 \u03b4 1 \u2191 \u03b4 2 \u2191 \u03b4 3 \u2191Table 12. Depth map evaluation on ScanNet 0418 00. 00 Abs Rel \u2193 Sq Rel \u2193 RMSE \u2193 RMSE log \u2193 \u03b4 1 \u2191 \u03b4 2 \u2191 \u03b4 3 \u2191Table 13. Depth map evaluation on ScanNet 0301 00. 0431 00 Abs Rel \u2193 Sq Rel \u2193 RMSE \u2193 RMSE log \u2193 \u03b4 1 \u2191 \u03b4 2 \u2191 \u03b4 3 \u2191Table 14. Depth map evaluation on ScanNet 0431 00.scenes \nOurs \nNeRFmm \nRPE t \u2193 RPE r \u2193 ATE\u2193 \nRPE t RPE r ATE \nFern \n0.252 \n0.993 \n0.003 \n0.706 1.816 0.007 \nFlower \n0.035 \n0.096 \n0.001 \n0.086 0.418 0.001 \nFortress \n0.081 \n0.296 \n0.001 \n0.233 0.739 0.004 \nHorns \n0.217 \n0.452 \n0.004 \n0.321 0.850 0.008 \nLeaves \n0.218 \n0.143 \n0.002 \n0.138 0.051 0.001 \nOrchids \n0.203 \n0.383 \n0.003 \n0.686 2.030 0.010 \nRoom \n0.244 \n0.936 \n0.004 \n0.670 1.664 0.011 \nTrex \n0.219 \n0.319 \n0.004 \n0.542 0.775 0.009 \nmean \n0.184 \n0.452 \n0.003 \n0.423 1.043 0.006 \n\nOurs \n0.099 \n0.047 \n0.335 \n0.128 \n0.904 0.995 1.000 \nBARF \n0.208 \n0.165 \n0.588 \n0.263 \n0.639 0.896 0.983 \nNeRFmm 0.494 \n1.049 \n1.419 \n0.534 \n0.378 0.567 0.765 \nSC-NeRF 0.360 \n0.450 \n0.902 \n0.396 \n0.407 0.730 0.908 \nDPT \n0.149 \n0.095 \n0.456 \n0.173 \n0.818 0.978 0.999 \n\nOurs \n0.152 \n0.137 \n0.645 \n0.185 \n0.738 0.988 0.997 \nBARF \n0.718 \n1.715 \n1.563 \n0.630 \n0.205 0.569 0.769 \nNeRFmm 0.907 \n3.650 \n2.176 \n0.769 \n0.240 0.456 0.621 \nSC-NeRF 0.319 \n0.441 \n0.898 \n0.377 \n0.456 0.792 0.930 \nDPT \n0.190 \n0.187 \n0.745 \n0.211 \n0.719 0.965 0.997 \n\n0301 Ours \n0.185 \n0.252 \n0.711 \n0.233 \n0.792 0.918 0.958 \nBARF \n0.179 \n0.146 \n0.502 \n0.268 \n0.736 0.883 0.938 \nNeRFmm 0.444 \n0.830 \n1.239 \n0.481 \n0.397 0.680 0.845 \nSC-NeRF 0.383 \n0.378 \n0.810 \n0.452 \n0.360 0.663 0.846 \nDPT \n0.317 \n0.568 \n1.133 \n0.350 \n0.597 0.821 0.914 \n\nOurs \n0.127 \n0.111 \n0.579 \n0.160 \n0.877 0.978 0.994 \nBARF \n0.398 \n0.710 \n1.307 \n0.444 \n0.381 0.655 0.847 \nNeRFmm 0.514 \n1.354 \n1.855 \n0.562 \n0.250 0.539 0.742 \nSC-NeRF 0.608 \n1.300 \n1.706 \n0.677 \n0.225 0.446 0.645 \nDPT \n0.132 \n0.135 \n0.670 \n0.171 \n0.855 0.973 0.991 \n\n\nAcknowledgementsWe thank Theo Costain, Michael Hobley, Shuai Chen and Xinghui Li for their helpful proofreading and discussions.Wenjing Bian is supported by the China Scholarship Council -University of Oxford Scholarship.Appendix A. Implementation DetailsThe following sections include more details about the datasets we use, our training procedure and evaluation metrics.Figure 10. Pose Estimation Comparison on ScanNet. We visualise the trajectory (3D plot) and relative rotation errors RPEr (bottom colour bar) of each method on Ballroom and Museum. The colour bar on the right shows the relative scaling of colour.\nUnstructured lumigraph rendering. Chris Buehler, Michael Bosse, Leonard Mcmillan, Steven Gortler, Michael Cohen, SIGGRAPH. Chris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. Unstructured lumigraph ren- dering. In SIGGRAPH, 2001. 2\n\nTensorf: Tensorial radiance fields. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su, ECCV. 2022Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In ECCV, 2022. 2\n\nView interpolation for image synthesis. Eric Shenchang, Lance Chen, Williams, SIGGRAPH. Shenchang Eric Chen and Lance Williams. View interpola- tion for image synthesis. In SIGGRAPH, 1993. 2\n\nGarf: Gaussian activated radiance fields for high fidelity reconstruction and pose estimation. Shin-Fang, Sameera Chng, Jamie Ramasinghe, Simon Sherrah, Lucey, 2022arXiv eprintsShin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and Simon Lucey. Garf: Gaussian activated radiance fields for high fidelity reconstruction and pose estimation. arXiv e- prints, 2022. 2\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, X Angel, Manolis Chang, Maciej Savva, Thomas Halber, Matthias Funkhouser, Nie\u00dfner, CVPR. 511Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 5, 11\n\nModeling and rendering architecture from photographs: A hybrid geometry-and image-based approach. Paul E Debevec, J Camillo, Jitendra Taylor, Malik, SIGGRAPH. Paul E Debevec, Camillo J Taylor, and Jitendra Malik. Mod- eling and rendering architecture from photographs: A hybrid geometry-and image-based approach. In SIGGRAPH, 1996. 2\n\nDepth-supervised nerf: Fewer views and faster training for free. Kangle Deng, Andrew Liu, Jun-Yan Zhu, Deva Ramanan, CVPR. 2022Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra- manan. Depth-supervised nerf: Fewer views and faster train- ing for free. In CVPR, 2022. 2\n\nDeep ordinal regression network for monocular depth estimation. Huan Fu, Mingming Gong, Chaohui Wang, CVPR. Kayhan Batmanghelich, and Dacheng TaoHuan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat- manghelich, and Dacheng Tao. Deep ordinal regression net- work for monocular depth estimation. In CVPR, 2018. 5\n\nDynamic view synthesis from dynamic monocular video. Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang, ICCV. Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In ICCV, 2021. 2\n\nJ Stephan, Marek Garbin, Matthew Kowalski, Jamie Johnson, Julien Shotton, Valentin, Fastnerf: High-fidelity neural rendering at 200fps. In ICCV. Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In ICCV, 2021. 2\n\nMultiple view geometry in computer vision. Richard Hartley, Andrew Zisserman, 2Richard Hartley and Andrew Zisserman. Multiple view ge- ometry in computer vision. 2003. 2\n\nSelf-calibrating neural radiance fields. Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima Anandkumar, Minsu Cho, Jaesik Park, ICCV. 711Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima Anandkumar, Minsu Cho, and Jaesik Park. Self-calibrating neural radiance fields. In ICCV, 2021. 1, 2, 3, 4, 5, 7, 11\n\nInfonerf: Ray entropy minimization for few-shot neural volume rendering. Mijeong Kim, Seonguk Seo, Bohyung Han, CVPR. 2022Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf: Ray entropy minimization for few-shot neural volume ren- dering. In CVPR, 2022. 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, Yoshua Bengio and Yann LeCun. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, ICLR, 2015. 5\n\nTanks and temples: Benchmarking large-scale scene reconstruction. Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, Vladlen Koltun, ACM Transactions on Graphics. 511Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics, 2017. 5, 11\n\nRobust consistent video depth estimation. Johannes Kopf, Xuejian Rong, Jia-Bin Huang, CVPR. Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In CVPR, 2021. 5\n\nNeural scene flow fields for space-time view synthesis of dynamic scenes. Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang, CVPR. Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dy- namic scenes. In CVPR, 2021. 2\n\nChen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey, Bundle-adjusting neural radiance fields. Barf711ICCVChen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si- mon Lucey. Barf: Bundle-adjusting neural radiance fields. In ICCV, 2021. 1, 2, 3, 4, 5, 7, 11\n\nLearning depth from single monocular images using deep convolutional neural fields. Fayao Liu, Chunhua Shen, Guosheng Lin, Ian Reid, TPAMI. 5Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid. Learning depth from single monocular images using deep convolutional neural fields. TPAMI, 2015. 5\n\nConsistent video depth estimation. Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, Johannes Kopf, ToG. 5Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ToG. 5\n\nGnerf: Gan-based neural radiance field without posed camera. Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, Jingyi Yu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionQuan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and Jingyi Yu. Gnerf: Gan-based neural radiance field without posed camera. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, 2021. 2\n\nBoosting monocular depth estimation models to high-resolution via content-adaptive multiresolution merging. S Mahdi, H Miangoleh, Sebastian Dille, Long Mai, Sylvain Paris, Yagiz Aksoy, CVPR. S Mahdi H Miangoleh, Sebastian Dille, Long Mai, Sylvain Paris, and Yagiz Aksoy. Boosting monocular depth estima- tion models to high-resolution via content-adaptive multi- resolution merging. In CVPR, 2021. 2\n\nLocal light field fusion: Practical view synthesis with prescriptive sampling guidelines. Ben Mildenhall, P Pratul, Rodrigo Srinivasan, Nima Khademi Ortiz-Cayon, Ravi Kalantari, Ren Ramamoorthi, Abhishek Ng, Kar, ACM Transactions on Graphics. 212Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view syn- thesis with prescriptive sampling guidelines. ACM Transac- tions on Graphics (TOG), 2019. 2, 12\n\nNerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, Communications of the ACM. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view syn- thesis. Communications of the ACM, 2021. 1, 2, 3, 5, 7\n\nInstant neural graphics primitives with a multiresolution hash encoding. Thomas M\u00fcller, Alex Evans, Christoph Schied, Alexander Keller, ACM Trans. Graph. 2Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexan- der Keller. Instant neural graphics primitives with a multires- olution hash encoding. ACM Trans. Graph. 2\n\nRegnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, S M Mehdi, Andreas Sajjadi, Noha Geiger, Radwan, CVPR. 2022Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg- nerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In CVPR, 2022. 2\n\nUnisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. Michael Oechsle, Songyou Peng, Andreas Geiger, ICCV. Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In ICCV, 2021. 2\n\nVision transformers for dense prediction. Ren\u00e9 Ranftl, Alexey Bochkovskiy, Vladlen Koltun, ICCV, 2021. 24Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi- sion transformers for dense prediction. In ICCV, 2021. 2, 4\n\nTowards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun, TPAMI. 2Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. TPAMI, 2020. 2\n\nFree view synthesis. Gernot Riegler, Vladlen Koltun, ECCV. Gernot Riegler and Vladlen Koltun. Free view synthesis. In ECCV, 2020. 2\n\nStable view synthesis. Gernot Riegler, Vladlen Koltun, CVPR. Gernot Riegler and Vladlen Koltun. Stable view synthesis. In CVPR, 2021. 2\n\nDense depth priors for neural radiance fields from sparse input views. Barbara Roessle, Jonathan T Barron, Ben Mildenhall, P Pratul, Matthias Srinivasan, Nie\u00dfner, CVPR. 2022Barbara Roessle, Jonathan T Barron, Ben Mildenhall, Pratul P Srinivasan, and Matthias Nie\u00dfner. Dense depth pri- ors for neural radiance fields from sparse input views. In CVPR, 2022. 2\n\nNerfslam: Real-time dense monocular slam with neural radiance fields. Antoni Rosinol, J John, Luca Leonard, Carlone, arXiv:2210.136412022arXiv preprintAntoni Rosinol, John J Leonard, and Luca Carlone. Nerf- slam: Real-time dense monocular slam with neural radiance fields. arXiv preprint arXiv:2210.13641, 2022. 2\n\nPlenoxels: Radiance fields without neural networks. Sara Fridovich, - Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa, CVPR. 2022Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022. 2\n\nStructurefrom-motion revisited. L Johannes, Jan-Michael Schonberger, Frahm, CVPR. 1Johannes L Schonberger and Jan-Michael Frahm. Structure- from-motion revisited. In CVPR, 2016. 1, 2\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.155611arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 11\n\nImplicit neural representations with periodic activation functions. Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein, NeurIPS. 2Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representa- tions with periodic activation functions. NeurIPS, 2020. 2\n\nA benchmark for the evaluation of rgb-d slam systems. J\u00fcrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, Daniel Cremers, IROS. IEEE. J\u00fcrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. A benchmark for the eval- uation of rgb-d slam systems. In IROS. IEEE, 2012. 5\n\nand Andrew Davison. iMAP: Implicit mapping and positioning in real-time. Edgar Sucar, Shikun Liu, Joseph Ortiz, ICCV. Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew Davi- son. iMAP: Implicit mapping and positioning in real-time. In ICCV, 2021. 2\n\nSc-depthv3: Robust selfsupervised monocular depth estimation for dynamic scenes. Libo Sun, Jia-Wang Bian, Huangying Zhan, Wei Yin, Ian Reid, Chunhua Shen, arXiv:2211.03660arXiv preprintLibo Sun, Jia-Wang Bian, Huangying Zhan, Wei Yin, Ian Reid, and Chunhua Shen. Sc-depthv3: Robust self- supervised monocular depth estimation for dynamic scenes. arXiv preprint arXiv:2211.03660, 2022. 5\n\nGrf: Learning a general radiance field for 3d representation and rendering. Alex Trevithick, Bo Yang, ICCV. Alex Trevithick and Bo Yang. Grf: Learning a general ra- diance field for 3d representation and rendering. In ICCV, 2021. 2\n\nSingle-view view synthesis with multiplane images. Richard Tucker, Noah Snavely, CVPR. Richard Tucker and Noah Snavely. Single-view view syn- thesis with multiplane images. In CVPR, 2020. 2\n\nLayer-structured 3d scene inference via view synthesis. Shubham Tulsiani, Richard Tucker, Noah Snavely, ECCV. Shubham Tulsiani, Richard Tucker, and Noah Snavely. Layer-structured 3d scene inference via view synthesis. In ECCV, 2018. 2\n\nNeus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang, NeurIPS. 2Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, 2021. 2\n\nIbrnet: Learning multi-view image-based rendering. Qianqian Wang, Zhicheng Wang, Kyle Genova, P Pratul, Howard Srinivasan, Jonathan T Zhou, Ricardo Barron, Noah Martin-Brualla, Thomas Snavely, Funkhouser, CVPR. Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr- net: Learning multi-view image-based rendering. In CVPR, 2021. 2\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, IEEE transactions on image processing. 511Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 2004. 5, 11\n\nZirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, Victor Adrian Prisacariu, arXiv:2102.07064NeRF\u2212\u2212: Neural radiance fields without known camera parameters. 711arXiv preprintZirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF\u2212\u2212: Neural radiance fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021. 1, 2, 3, 4, 5, 6, 7, 11\n\nNerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, Jie Zhou, ICCV. Yi Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou. Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo. In ICCV, 2021. 2\n\nSinerf: Sinusoidal neural radiance fields for joint pose estimation and scene reconstruction. Yitong Xia, Hao Tang, Radu Timofte, Luc Van Gool, 2022Yitong Xia, Hao Tang, Radu Timofte, and Luc Van Gool. Sinerf: Sinusoidal neural radiance fields for joint pose esti- mation and scene reconstruction. 2022. 2\n\nVolume rendering of neural implicit surfaces. Lior Yariv, Jiatao Gu, Yoni Kasten, Yaron Lipman, NeurIPS. Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol- ume rendering of neural implicit surfaces. In NeurIPS, 2021. 2\n\nLin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto Rodriguez, Phillip Isola, Tsung-Yi Lin, inerf: Inverting neural radiance fields for pose estimation. IROSLin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting neural radiance fields for pose estimation. In IROS, 2021. 2\n\nTowards accurate reconstruction of 3d scene shape from a single monocular image. Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Simon Chen, Yifan Liu, Chunhua Shen, TPAMI. 2Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Si- mon Chen, Yifan Liu, and Chunhua Shen. Towards accurate reconstruction of 3d scene shape from a single monocular image. TPAMI, 2022. 2\n\nAlex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa, Neural radiance fields from one or few images. CVPRAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In CVPR, 2021. 2\n\nMonosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. 2022Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat- tler, and Andreas Geiger. Monosdf: Exploring monocu- lar geometric cues for neural implicit surface reconstruction. NeurIPS, 2022. 2\n\nRay priors through reprojection: Improving neural radiance fields for novel view extrapolation. Jian Zhang, Yuanqing Zhang, Huan Fu, Xiaowei Zhou, Bowen Cai, Jinchi Huang, Rongfei Jia, Binqiang Zhao, Xing Tang, CVPR. 2022Jian Zhang, Yuanqing Zhang, Huan Fu, Xiaowei Zhou, Bowen Cai, Jinchi Huang, Rongfei Jia, Binqiang Zhao, and Xing Tang. Ray priors through reprojection: Improving neu- ral radiance fields for novel view extrapolation. In CVPR, 2022. 2\n\nKai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun, arXiv:2010.07492Nerf++: Analyzing and improving neural radiance fields. arXiv preprintKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492, 2020. 2\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, CVPR. 511Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. 5, 11\n\nA tutorial on quantitative trajectory evaluation for visual (-inertial) odometry. Zichao Zhang, Davide Scaramuzza, IROS. IEEE. 511Zichao Zhang and Davide Scaramuzza. A tutorial on quanti- tative trajectory evaluation for visual (-inertial) odometry. In IROS. IEEE, 2018. 5, 11\n\nUnsupervised learning of depth and ego-motion from video. Tinghui Zhou, Matthew Brown, Noah Snavely, David G Lowe, CVPR. 57Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In CVPR, 2017. 5, 7\n\nStereo magnification: Learning view synthesis using multiplane images. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Noah Snavely, Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view syn- thesis using multiplane images. 2018. 2\n\nNice-slam: Neural implicit scalable encoding for slam. Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R Oswald, Marc Pollefeys, CVPR. 2022Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hu- jun Bao, Zhaopeng Cui, Martin R. Oswald, and Marc Polle- feys. Nice-slam: Neural implicit scalable encoding for slam. In CVPR, 2022. 2\n", "annotations": {"author": "[{\"end\":144,\"start\":66},{\"end\":197,\"start\":145},{\"end\":248,\"start\":198},{\"end\":304,\"start\":249},{\"end\":394,\"start\":305}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":74},{\"end\":155,\"start\":151},{\"end\":206,\"start\":204},{\"end\":262,\"start\":258},{\"end\":329,\"start\":319}]", "author_first_name": "[{\"end\":73,\"start\":66},{\"end\":150,\"start\":145},{\"end\":203,\"start\":198},{\"end\":257,\"start\":249},{\"end\":311,\"start\":305},{\"end\":318,\"start\":312}]", "author_affiliation": "[{\"end\":143,\"start\":104},{\"end\":196,\"start\":157},{\"end\":247,\"start\":208},{\"end\":303,\"start\":264},{\"end\":393,\"start\":354}]", "title": "[{\"end\":63,\"start\":1},{\"end\":457,\"start\":395}]", "venue": null, "abstract": "[{\"end\":1638,\"start\":459}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1863,\"start\":1859},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2209,\"start\":2205},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2436,\"start\":2432},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2447,\"start\":2443},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2464,\"start\":2460},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3213,\"start\":3209},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3452,\"start\":3448},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3455,\"start\":3452},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3458,\"start\":3455},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3461,\"start\":3458},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4116,\"start\":4113},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4119,\"start\":4116},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4122,\"start\":4119},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5766,\"start\":5763},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5828,\"start\":5825},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5830,\"start\":5828},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5916,\"start\":5912},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5919,\"start\":5916},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5944,\"start\":5940},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":5947,\"start\":5944},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5967,\"start\":5963},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5994,\"start\":5990},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6173,\"start\":6169},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6176,\"start\":6173},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6179,\"start\":6176},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6197,\"start\":6194},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6200,\"start\":6197},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6203,\"start\":6200},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6206,\"start\":6203},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6233,\"start\":6229},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6236,\"start\":6233},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6239,\"start\":6236},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6260,\"start\":6256},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6263,\"start\":6260},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6266,\"start\":6263},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6283,\"start\":6280},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6286,\"start\":6283},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6289,\"start\":6286},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6292,\"start\":6289},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6450,\"start\":6446},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6453,\"start\":6450},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6597,\"start\":6593},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6600,\"start\":6597},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":6603,\"start\":6600},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6889,\"start\":6885},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6990,\"start\":6986},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7159,\"start\":7155},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7250,\"start\":7246},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7364,\"start\":7360},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7462,\"start\":7459},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7581,\"start\":7577},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7597,\"start\":7593},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7780,\"start\":7776},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8373,\"start\":8370},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10075,\"start\":10071},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10791,\"start\":10787},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10869,\"start\":10865},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10872,\"start\":10869},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10875,\"start\":10872},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11066,\"start\":11063},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11863,\"start\":11859},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13270,\"start\":13266},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13273,\"start\":13270},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13276,\"start\":13273},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15583,\"start\":15579},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15599,\"start\":15596},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16124,\"start\":16120},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16741,\"start\":16737},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16744,\"start\":16741},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16747,\"start\":16744},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":16878,\"start\":16874},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":16937,\"start\":16933},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17004,\"start\":17000},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17007,\"start\":17004},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":17010,\"start\":17007},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17474,\"start\":17471},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17477,\"start\":17474},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17480,\"start\":17477},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17483,\"start\":17480},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17679,\"start\":17675},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17822,\"start\":17818},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18041,\"start\":18037},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18832,\"start\":18828},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":18845,\"start\":18841},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18862,\"start\":18858},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":19059,\"start\":19055},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20696,\"start\":20692},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20709,\"start\":20705},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20726,\"start\":20722},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":20750,\"start\":20746},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22024,\"start\":22020},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25765,\"start\":25762},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25792,\"start\":25788},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26435,\"start\":26431},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27845,\"start\":27841},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27856,\"start\":27852},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27873,\"start\":27869},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28838,\"start\":28834},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":29242,\"start\":29238},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":29323,\"start\":29319},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29625,\"start\":29621},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30032,\"start\":30028},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30091,\"start\":30087},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30178,\"start\":30174},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30636,\"start\":30632}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":31357,\"start\":31328},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31519,\"start\":31358},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31577,\"start\":31520},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31850,\"start\":31578},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32127,\"start\":31851},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32510,\"start\":32128},{\"attributes\":{\"id\":\"fig_8\"},\"end\":32690,\"start\":32511},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34220,\"start\":32691},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36204,\"start\":34221},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37167,\"start\":36205},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37523,\"start\":37168},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38585,\"start\":37524},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":38817,\"start\":38586},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":40875,\"start\":38818}]", "paragraph": "[{\"end\":2013,\"start\":1654},{\"end\":3352,\"start\":2015},{\"end\":3927,\"start\":3354},{\"end\":4895,\"start\":3929},{\"end\":5434,\"start\":4897},{\"end\":5636,\"start\":5436},{\"end\":6070,\"start\":5653},{\"end\":6454,\"start\":6072},{\"end\":7699,\"start\":6456},{\"end\":8008,\"start\":7701},{\"end\":8556,\"start\":8019},{\"end\":9009,\"start\":8558},{\"end\":9325,\"start\":9011},{\"end\":9748,\"start\":9327},{\"end\":10033,\"start\":9750},{\"end\":10337,\"start\":10042},{\"end\":10416,\"start\":10364},{\"end\":10646,\"start\":10454},{\"end\":10812,\"start\":10693},{\"end\":11067,\"start\":10853},{\"end\":11261,\"start\":11069},{\"end\":11490,\"start\":11306},{\"end\":11660,\"start\":11492},{\"end\":12139,\"start\":11802},{\"end\":12300,\"start\":12141},{\"end\":12506,\"start\":12337},{\"end\":12746,\"start\":12534},{\"end\":12783,\"start\":12777},{\"end\":13201,\"start\":12817},{\"end\":13612,\"start\":13230},{\"end\":13729,\"start\":13614},{\"end\":13867,\"start\":13764},{\"end\":14108,\"start\":13912},{\"end\":14620,\"start\":14198},{\"end\":14680,\"start\":14622},{\"end\":14859,\"start\":14750},{\"end\":14949,\"start\":14889},{\"end\":15117,\"start\":15005},{\"end\":15269,\"start\":15173},{\"end\":15489,\"start\":15285},{\"end\":16623,\"start\":15512},{\"end\":17752,\"start\":16625},{\"end\":18722,\"start\":17754},{\"end\":18863,\"start\":18762},{\"end\":20128,\"start\":18865},{\"end\":20806,\"start\":20130},{\"end\":21414,\"start\":20808},{\"end\":22587,\"start\":21454},{\"end\":22959,\"start\":22589},{\"end\":23585,\"start\":22961},{\"end\":23770,\"start\":23604},{\"end\":24083,\"start\":23772},{\"end\":24387,\"start\":24085},{\"end\":24616,\"start\":24389},{\"end\":24916,\"start\":24632},{\"end\":25677,\"start\":24931},{\"end\":26931,\"start\":25694},{\"end\":27730,\"start\":26957},{\"end\":28255,\"start\":27762},{\"end\":28677,\"start\":28257},{\"end\":28811,\"start\":28679},{\"end\":29212,\"start\":28813},{\"end\":29591,\"start\":29214},{\"end\":29893,\"start\":29593},{\"end\":30240,\"start\":29921},{\"end\":30387,\"start\":30242},{\"end\":30514,\"start\":30389},{\"end\":30867,\"start\":30549},{\"end\":31043,\"start\":30869},{\"end\":31327,\"start\":31045}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10363,\"start\":10338},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10453,\"start\":10417},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10692,\"start\":10647},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11305,\"start\":11262},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11767,\"start\":11661},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12336,\"start\":12301},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12533,\"start\":12507},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12776,\"start\":12747},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12816,\"start\":12784},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13763,\"start\":13730},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13911,\"start\":13868},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14197,\"start\":14109},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14749,\"start\":14681},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15004,\"start\":14950},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15172,\"start\":15118}]", "table_ref": "[{\"end\":20593,\"start\":20586},{\"end\":23029,\"start\":23022},{\"end\":26575,\"start\":26568},{\"end\":30188,\"start\":30181}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1652,\"start\":1640},{\"attributes\":{\"n\":\"2.\"},\"end\":5651,\"start\":5639},{\"attributes\":{\"n\":\"3.\"},\"end\":8017,\"start\":8011},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10040,\"start\":10036},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10851,\"start\":10815},{\"attributes\":{\"n\":\"3.3.\"},\"end\":11800,\"start\":11769},{\"attributes\":{\"n\":\"3.4.\"},\"end\":13228,\"start\":13204},{\"attributes\":{\"n\":\"3.5.\"},\"end\":14887,\"start\":14862},{\"attributes\":{\"n\":\"4.\"},\"end\":15283,\"start\":15272},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15510,\"start\":15492},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18760,\"start\":18725},{\"attributes\":{\"n\":\"4.3.\"},\"end\":21452,\"start\":21417},{\"attributes\":{\"n\":\"4.4.\"},\"end\":23602,\"start\":23588},{\"attributes\":{\"n\":\"4.5.\"},\"end\":24630,\"start\":24619},{\"attributes\":{\"n\":\"5.\"},\"end\":24929,\"start\":24919},{\"end\":25692,\"start\":25680},{\"end\":26955,\"start\":26934},{\"end\":27760,\"start\":27733},{\"end\":29919,\"start\":29896},{\"end\":30547,\"start\":30517},{\"end\":31339,\"start\":31329},{\"end\":31589,\"start\":31579},{\"end\":31862,\"start\":31852},{\"end\":32139,\"start\":32129},{\"end\":32522,\"start\":32512},{\"end\":32701,\"start\":32692},{\"end\":34231,\"start\":34222},{\"end\":37534,\"start\":37525}]", "table": "[{\"end\":34220,\"start\":32895},{\"end\":36204,\"start\":34446},{\"end\":37167,\"start\":36253},{\"end\":37523,\"start\":37221},{\"end\":38585,\"start\":37646},{\"end\":38817,\"start\":38739},{\"end\":40875,\"start\":39310}]", "figure_caption": "[{\"end\":31357,\"start\":31341},{\"end\":31519,\"start\":31360},{\"end\":31577,\"start\":31522},{\"end\":31850,\"start\":31591},{\"end\":32127,\"start\":31864},{\"end\":32510,\"start\":32141},{\"end\":32690,\"start\":32524},{\"end\":32895,\"start\":32703},{\"end\":34446,\"start\":34233},{\"end\":36253,\"start\":36207},{\"end\":37221,\"start\":37170},{\"end\":37646,\"start\":37536},{\"end\":38739,\"start\":38588},{\"end\":39310,\"start\":38820}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":19499,\"start\":19493},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":19940,\"start\":19934},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20916,\"start\":20910},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":21413,\"start\":21406},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":22713,\"start\":22707},{\"end\":31143,\"start\":31135},{\"end\":31164,\"start\":31156},{\"end\":31305,\"start\":31297},{\"end\":31326,\"start\":31318}]", "bib_author_first_name": "[{\"end\":41534,\"start\":41529},{\"end\":41551,\"start\":41544},{\"end\":41566,\"start\":41559},{\"end\":41583,\"start\":41577},{\"end\":41600,\"start\":41593},{\"end\":41800,\"start\":41795},{\"end\":41814,\"start\":41807},{\"end\":41826,\"start\":41819},{\"end\":41841,\"start\":41835},{\"end\":41849,\"start\":41846},{\"end\":42025,\"start\":42021},{\"end\":42042,\"start\":42037},{\"end\":42286,\"start\":42279},{\"end\":42298,\"start\":42293},{\"end\":42316,\"start\":42311},{\"end\":42608,\"start\":42602},{\"end\":42615,\"start\":42614},{\"end\":42630,\"start\":42623},{\"end\":42644,\"start\":42638},{\"end\":42658,\"start\":42652},{\"end\":42675,\"start\":42667},{\"end\":43006,\"start\":43005},{\"end\":43024,\"start\":43016},{\"end\":43297,\"start\":43291},{\"end\":43310,\"start\":43304},{\"end\":43323,\"start\":43316},{\"end\":43333,\"start\":43329},{\"end\":43564,\"start\":43560},{\"end\":43577,\"start\":43569},{\"end\":43591,\"start\":43584},{\"end\":43861,\"start\":43857},{\"end\":43872,\"start\":43867},{\"end\":43888,\"start\":43880},{\"end\":43902,\"start\":43895},{\"end\":44045,\"start\":44044},{\"end\":44060,\"start\":44055},{\"end\":44076,\"start\":44069},{\"end\":44092,\"start\":44087},{\"end\":44108,\"start\":44102},{\"end\":44396,\"start\":44389},{\"end\":44412,\"start\":44406},{\"end\":44565,\"start\":44558},{\"end\":44580,\"start\":44573},{\"end\":44597,\"start\":44586},{\"end\":44609,\"start\":44604},{\"end\":44627,\"start\":44622},{\"end\":44639,\"start\":44633},{\"end\":44905,\"start\":44898},{\"end\":44918,\"start\":44911},{\"end\":44931,\"start\":44924},{\"end\":45128,\"start\":45127},{\"end\":45144,\"start\":45139},{\"end\":45391,\"start\":45387},{\"end\":45409,\"start\":45403},{\"end\":45423,\"start\":45416},{\"end\":45437,\"start\":45430},{\"end\":45701,\"start\":45693},{\"end\":45715,\"start\":45708},{\"end\":45729,\"start\":45722},{\"end\":45932,\"start\":45925},{\"end\":45942,\"start\":45937},{\"end\":45956,\"start\":45952},{\"end\":45972,\"start\":45966},{\"end\":46147,\"start\":46137},{\"end\":46161,\"start\":46153},{\"end\":46173,\"start\":46166},{\"end\":46189,\"start\":46184},{\"end\":46488,\"start\":46483},{\"end\":46501,\"start\":46494},{\"end\":46516,\"start\":46508},{\"end\":46525,\"start\":46522},{\"end\":46732,\"start\":46728},{\"end\":46745,\"start\":46738},{\"end\":46760,\"start\":46753},{\"end\":46776,\"start\":46771},{\"end\":46793,\"start\":46785},{\"end\":46990,\"start\":46986},{\"end\":47002,\"start\":46997},{\"end\":47015,\"start\":47009},{\"end\":47026,\"start\":47021},{\"end\":47034,\"start\":47031},{\"end\":47042,\"start\":47039},{\"end\":47053,\"start\":47047},{\"end\":47064,\"start\":47058},{\"end\":47542,\"start\":47541},{\"end\":47551,\"start\":47550},{\"end\":47572,\"start\":47563},{\"end\":47584,\"start\":47580},{\"end\":47597,\"start\":47590},{\"end\":47610,\"start\":47605},{\"end\":47927,\"start\":47924},{\"end\":47941,\"start\":47940},{\"end\":47957,\"start\":47950},{\"end\":47974,\"start\":47970},{\"end\":47982,\"start\":47975},{\"end\":48000,\"start\":47996},{\"end\":48015,\"start\":48012},{\"end\":48037,\"start\":48029},{\"end\":48424,\"start\":48421},{\"end\":48438,\"start\":48437},{\"end\":48454,\"start\":48447},{\"end\":48475,\"start\":48467},{\"end\":48477,\"start\":48476},{\"end\":48490,\"start\":48486},{\"end\":48502,\"start\":48499},{\"end\":48850,\"start\":48844},{\"end\":48863,\"start\":48859},{\"end\":48880,\"start\":48871},{\"end\":48898,\"start\":48889},{\"end\":49182,\"start\":49175},{\"end\":49201,\"start\":49193},{\"end\":49203,\"start\":49202},{\"end\":49215,\"start\":49212},{\"end\":49229,\"start\":49228},{\"end\":49231,\"start\":49230},{\"end\":49246,\"start\":49239},{\"end\":49260,\"start\":49256},{\"end\":49596,\"start\":49589},{\"end\":49613,\"start\":49606},{\"end\":49627,\"start\":49620},{\"end\":49851,\"start\":49847},{\"end\":49866,\"start\":49860},{\"end\":49887,\"start\":49880},{\"end\":50129,\"start\":50125},{\"end\":50144,\"start\":50138},{\"end\":50160,\"start\":50155},{\"end\":50175,\"start\":50169},{\"end\":50194,\"start\":50187},{\"end\":50433,\"start\":50427},{\"end\":50450,\"start\":50443},{\"end\":50568,\"start\":50562},{\"end\":50585,\"start\":50578},{\"end\":50754,\"start\":50747},{\"end\":50772,\"start\":50764},{\"end\":50774,\"start\":50773},{\"end\":50786,\"start\":50783},{\"end\":50800,\"start\":50799},{\"end\":50817,\"start\":50809},{\"end\":51111,\"start\":51105},{\"end\":51122,\"start\":51121},{\"end\":51133,\"start\":51129},{\"end\":51406,\"start\":51402},{\"end\":51419,\"start\":51418},{\"end\":51430,\"start\":51426},{\"end\":51442,\"start\":51435},{\"end\":51458,\"start\":51451},{\"end\":51473,\"start\":51465},{\"end\":51487,\"start\":51481},{\"end\":51711,\"start\":51710},{\"end\":51733,\"start\":51722},{\"end\":51935,\"start\":51930},{\"end\":51952,\"start\":51946},{\"end\":52219,\"start\":52212},{\"end\":52236,\"start\":52230},{\"end\":52254,\"start\":52245},{\"end\":52269,\"start\":52264},{\"end\":52285,\"start\":52279},{\"end\":52544,\"start\":52538},{\"end\":52559,\"start\":52552},{\"end\":52576,\"start\":52571},{\"end\":52592,\"start\":52585},{\"end\":52608,\"start\":52602},{\"end\":52872,\"start\":52867},{\"end\":52886,\"start\":52880},{\"end\":52898,\"start\":52892},{\"end\":53129,\"start\":53125},{\"end\":53143,\"start\":53135},{\"end\":53159,\"start\":53150},{\"end\":53169,\"start\":53166},{\"end\":53178,\"start\":53175},{\"end\":53192,\"start\":53185},{\"end\":53512,\"start\":53508},{\"end\":53527,\"start\":53525},{\"end\":53723,\"start\":53716},{\"end\":53736,\"start\":53732},{\"end\":53919,\"start\":53912},{\"end\":53937,\"start\":53930},{\"end\":53950,\"start\":53946},{\"end\":54187,\"start\":54183},{\"end\":54201,\"start\":54194},{\"end\":54211,\"start\":54207},{\"end\":54226,\"start\":54217},{\"end\":54241,\"start\":54237},{\"end\":54257,\"start\":54250},{\"end\":54527,\"start\":54519},{\"end\":54542,\"start\":54534},{\"end\":54553,\"start\":54549},{\"end\":54563,\"start\":54562},{\"end\":54578,\"start\":54572},{\"end\":54599,\"start\":54591},{\"end\":54601,\"start\":54600},{\"end\":54615,\"start\":54608},{\"end\":54628,\"start\":54624},{\"end\":54651,\"start\":54645},{\"end\":54985,\"start\":54981},{\"end\":54996,\"start\":54992},{\"end\":54998,\"start\":54997},{\"end\":55007,\"start\":55006},{\"end\":55021,\"start\":55015},{\"end\":55281,\"start\":55276},{\"end\":55296,\"start\":55288},{\"end\":55306,\"start\":55301},{\"end\":55315,\"start\":55312},{\"end\":55328,\"start\":55322},{\"end\":55335,\"start\":55329},{\"end\":55739,\"start\":55737},{\"end\":55752,\"start\":55745},{\"end\":55766,\"start\":55758},{\"end\":55776,\"start\":55772},{\"end\":55788,\"start\":55783},{\"end\":55796,\"start\":55793},{\"end\":56085,\"start\":56079},{\"end\":56094,\"start\":56091},{\"end\":56105,\"start\":56101},{\"end\":56118,\"start\":56115},{\"end\":56342,\"start\":56338},{\"end\":56356,\"start\":56350},{\"end\":56365,\"start\":56361},{\"end\":56379,\"start\":56374},{\"end\":56523,\"start\":56520},{\"end\":56538,\"start\":56534},{\"end\":56557,\"start\":56549},{\"end\":56559,\"start\":56558},{\"end\":56575,\"start\":56568},{\"end\":56594,\"start\":56587},{\"end\":56610,\"start\":56602},{\"end\":56944,\"start\":56941},{\"end\":56958,\"start\":56950},{\"end\":56972,\"start\":56966},{\"end\":56984,\"start\":56979},{\"end\":56999,\"start\":56994},{\"end\":57011,\"start\":57006},{\"end\":57024,\"start\":57017},{\"end\":57236,\"start\":57232},{\"end\":57247,\"start\":57241},{\"end\":57259,\"start\":57252},{\"end\":57274,\"start\":57268},{\"end\":57562,\"start\":57557},{\"end\":57574,\"start\":57567},{\"end\":57588,\"start\":57581},{\"end\":57929,\"start\":57925},{\"end\":57945,\"start\":57937},{\"end\":57957,\"start\":57953},{\"end\":57969,\"start\":57962},{\"end\":57981,\"start\":57976},{\"end\":57993,\"start\":57987},{\"end\":58008,\"start\":58001},{\"end\":58022,\"start\":58014},{\"end\":58033,\"start\":58029},{\"end\":58288,\"start\":58285},{\"end\":58302,\"start\":58296},{\"end\":58316,\"start\":58312},{\"end\":58333,\"start\":58326},{\"end\":58666,\"start\":58659},{\"end\":58681,\"start\":58674},{\"end\":58695,\"start\":58689},{\"end\":58697,\"start\":58696},{\"end\":58708,\"start\":58705},{\"end\":58726,\"start\":58720},{\"end\":59002,\"start\":58996},{\"end\":59016,\"start\":59010},{\"end\":59257,\"start\":59250},{\"end\":59271,\"start\":59264},{\"end\":59283,\"start\":59279},{\"end\":59300,\"start\":59293},{\"end\":59533,\"start\":59526},{\"end\":59547,\"start\":59540},{\"end\":59560,\"start\":59556},{\"end\":59574,\"start\":59568},{\"end\":59586,\"start\":59582},{\"end\":59812,\"start\":59807},{\"end\":59825,\"start\":59818},{\"end\":59838,\"start\":59832},{\"end\":59854,\"start\":59848},{\"end\":59864,\"start\":59859},{\"end\":59878,\"start\":59870},{\"end\":59890,\"start\":59884},{\"end\":59892,\"start\":59891},{\"end\":59905,\"start\":59901}]", "bib_author_last_name": "[{\"end\":41542,\"start\":41535},{\"end\":41557,\"start\":41552},{\"end\":41575,\"start\":41567},{\"end\":41591,\"start\":41584},{\"end\":41606,\"start\":41601},{\"end\":41805,\"start\":41801},{\"end\":41817,\"start\":41815},{\"end\":41833,\"start\":41827},{\"end\":41844,\"start\":41842},{\"end\":41852,\"start\":41850},{\"end\":42035,\"start\":42026},{\"end\":42047,\"start\":42043},{\"end\":42057,\"start\":42049},{\"end\":42277,\"start\":42268},{\"end\":42291,\"start\":42287},{\"end\":42309,\"start\":42299},{\"end\":42324,\"start\":42317},{\"end\":42331,\"start\":42326},{\"end\":42612,\"start\":42609},{\"end\":42621,\"start\":42616},{\"end\":42636,\"start\":42631},{\"end\":42650,\"start\":42645},{\"end\":42665,\"start\":42659},{\"end\":42686,\"start\":42676},{\"end\":42695,\"start\":42688},{\"end\":43003,\"start\":42989},{\"end\":43014,\"start\":43007},{\"end\":43031,\"start\":43025},{\"end\":43038,\"start\":43033},{\"end\":43302,\"start\":43298},{\"end\":43314,\"start\":43311},{\"end\":43327,\"start\":43324},{\"end\":43341,\"start\":43334},{\"end\":43567,\"start\":43565},{\"end\":43582,\"start\":43578},{\"end\":43596,\"start\":43592},{\"end\":43865,\"start\":43862},{\"end\":43878,\"start\":43873},{\"end\":43893,\"start\":43889},{\"end\":43908,\"start\":43903},{\"end\":44053,\"start\":44046},{\"end\":44067,\"start\":44061},{\"end\":44085,\"start\":44077},{\"end\":44100,\"start\":44093},{\"end\":44116,\"start\":44109},{\"end\":44126,\"start\":44118},{\"end\":44404,\"start\":44397},{\"end\":44422,\"start\":44413},{\"end\":44571,\"start\":44566},{\"end\":44584,\"start\":44581},{\"end\":44602,\"start\":44598},{\"end\":44620,\"start\":44610},{\"end\":44631,\"start\":44628},{\"end\":44644,\"start\":44640},{\"end\":44909,\"start\":44906},{\"end\":44922,\"start\":44919},{\"end\":44935,\"start\":44932},{\"end\":45137,\"start\":45129},{\"end\":45151,\"start\":45145},{\"end\":45155,\"start\":45153},{\"end\":45401,\"start\":45392},{\"end\":45414,\"start\":45410},{\"end\":45428,\"start\":45424},{\"end\":45444,\"start\":45438},{\"end\":45706,\"start\":45702},{\"end\":45720,\"start\":45716},{\"end\":45735,\"start\":45730},{\"end\":45935,\"start\":45933},{\"end\":45950,\"start\":45943},{\"end\":45964,\"start\":45957},{\"end\":45977,\"start\":45973},{\"end\":46151,\"start\":46148},{\"end\":46164,\"start\":46162},{\"end\":46182,\"start\":46174},{\"end\":46195,\"start\":46190},{\"end\":46492,\"start\":46489},{\"end\":46506,\"start\":46502},{\"end\":46520,\"start\":46517},{\"end\":46530,\"start\":46526},{\"end\":46736,\"start\":46733},{\"end\":46751,\"start\":46746},{\"end\":46769,\"start\":46761},{\"end\":46783,\"start\":46777},{\"end\":46798,\"start\":46794},{\"end\":46995,\"start\":46991},{\"end\":47007,\"start\":47003},{\"end\":47019,\"start\":47016},{\"end\":47029,\"start\":47027},{\"end\":47037,\"start\":47035},{\"end\":47045,\"start\":47043},{\"end\":47056,\"start\":47054},{\"end\":47067,\"start\":47065},{\"end\":47548,\"start\":47543},{\"end\":47561,\"start\":47552},{\"end\":47578,\"start\":47573},{\"end\":47588,\"start\":47585},{\"end\":47603,\"start\":47598},{\"end\":47616,\"start\":47611},{\"end\":47938,\"start\":47928},{\"end\":47948,\"start\":47942},{\"end\":47968,\"start\":47958},{\"end\":47994,\"start\":47983},{\"end\":48010,\"start\":48001},{\"end\":48027,\"start\":48016},{\"end\":48040,\"start\":48038},{\"end\":48045,\"start\":48042},{\"end\":48435,\"start\":48425},{\"end\":48445,\"start\":48439},{\"end\":48465,\"start\":48455},{\"end\":48484,\"start\":48478},{\"end\":48497,\"start\":48491},{\"end\":48514,\"start\":48503},{\"end\":48518,\"start\":48516},{\"end\":48857,\"start\":48851},{\"end\":48869,\"start\":48864},{\"end\":48887,\"start\":48881},{\"end\":48905,\"start\":48899},{\"end\":49191,\"start\":49183},{\"end\":49210,\"start\":49204},{\"end\":49226,\"start\":49216},{\"end\":49237,\"start\":49232},{\"end\":49254,\"start\":49247},{\"end\":49267,\"start\":49261},{\"end\":49275,\"start\":49269},{\"end\":49604,\"start\":49597},{\"end\":49618,\"start\":49614},{\"end\":49634,\"start\":49628},{\"end\":49858,\"start\":49852},{\"end\":49878,\"start\":49867},{\"end\":49894,\"start\":49888},{\"end\":50136,\"start\":50130},{\"end\":50153,\"start\":50145},{\"end\":50167,\"start\":50161},{\"end\":50185,\"start\":50176},{\"end\":50201,\"start\":50195},{\"end\":50441,\"start\":50434},{\"end\":50457,\"start\":50451},{\"end\":50576,\"start\":50569},{\"end\":50592,\"start\":50586},{\"end\":50762,\"start\":50755},{\"end\":50781,\"start\":50775},{\"end\":50797,\"start\":50787},{\"end\":50807,\"start\":50801},{\"end\":50828,\"start\":50818},{\"end\":50837,\"start\":50830},{\"end\":51119,\"start\":51112},{\"end\":51127,\"start\":51123},{\"end\":51141,\"start\":51134},{\"end\":51150,\"start\":51143},{\"end\":51416,\"start\":51407},{\"end\":51424,\"start\":51420},{\"end\":51433,\"start\":51431},{\"end\":51449,\"start\":51443},{\"end\":51463,\"start\":51459},{\"end\":51479,\"start\":51474},{\"end\":51496,\"start\":51488},{\"end\":51720,\"start\":51712},{\"end\":51745,\"start\":51734},{\"end\":51752,\"start\":51747},{\"end\":51944,\"start\":51936},{\"end\":51962,\"start\":51953},{\"end\":52228,\"start\":52220},{\"end\":52243,\"start\":52237},{\"end\":52262,\"start\":52255},{\"end\":52277,\"start\":52270},{\"end\":52295,\"start\":52286},{\"end\":52550,\"start\":52545},{\"end\":52569,\"start\":52560},{\"end\":52583,\"start\":52577},{\"end\":52600,\"start\":52593},{\"end\":52616,\"start\":52609},{\"end\":52878,\"start\":52873},{\"end\":52890,\"start\":52887},{\"end\":52904,\"start\":52899},{\"end\":53133,\"start\":53130},{\"end\":53148,\"start\":53144},{\"end\":53164,\"start\":53160},{\"end\":53173,\"start\":53170},{\"end\":53183,\"start\":53179},{\"end\":53197,\"start\":53193},{\"end\":53523,\"start\":53513},{\"end\":53532,\"start\":53528},{\"end\":53730,\"start\":53724},{\"end\":53744,\"start\":53737},{\"end\":53928,\"start\":53920},{\"end\":53944,\"start\":53938},{\"end\":53958,\"start\":53951},{\"end\":54192,\"start\":54188},{\"end\":54205,\"start\":54202},{\"end\":54215,\"start\":54212},{\"end\":54235,\"start\":54227},{\"end\":54248,\"start\":54242},{\"end\":54262,\"start\":54258},{\"end\":54532,\"start\":54528},{\"end\":54547,\"start\":54543},{\"end\":54560,\"start\":54554},{\"end\":54570,\"start\":54564},{\"end\":54589,\"start\":54579},{\"end\":54606,\"start\":54602},{\"end\":54622,\"start\":54616},{\"end\":54643,\"start\":54629},{\"end\":54659,\"start\":54652},{\"end\":54671,\"start\":54661},{\"end\":54990,\"start\":54986},{\"end\":55004,\"start\":54999},{\"end\":55013,\"start\":55008},{\"end\":55028,\"start\":55022},{\"end\":55040,\"start\":55030},{\"end\":55286,\"start\":55282},{\"end\":55299,\"start\":55297},{\"end\":55310,\"start\":55307},{\"end\":55320,\"start\":55316},{\"end\":55346,\"start\":55336},{\"end\":55743,\"start\":55740},{\"end\":55756,\"start\":55753},{\"end\":55770,\"start\":55767},{\"end\":55781,\"start\":55777},{\"end\":55791,\"start\":55789},{\"end\":55801,\"start\":55797},{\"end\":56089,\"start\":56086},{\"end\":56099,\"start\":56095},{\"end\":56113,\"start\":56106},{\"end\":56127,\"start\":56119},{\"end\":56348,\"start\":56343},{\"end\":56359,\"start\":56357},{\"end\":56372,\"start\":56366},{\"end\":56386,\"start\":56380},{\"end\":56532,\"start\":56524},{\"end\":56547,\"start\":56539},{\"end\":56566,\"start\":56560},{\"end\":56585,\"start\":56576},{\"end\":56600,\"start\":56595},{\"end\":56614,\"start\":56611},{\"end\":56948,\"start\":56945},{\"end\":56964,\"start\":56959},{\"end\":56977,\"start\":56973},{\"end\":56992,\"start\":56985},{\"end\":57004,\"start\":57000},{\"end\":57015,\"start\":57012},{\"end\":57029,\"start\":57025},{\"end\":57239,\"start\":57237},{\"end\":57250,\"start\":57248},{\"end\":57266,\"start\":57260},{\"end\":57283,\"start\":57275},{\"end\":57565,\"start\":57563},{\"end\":57579,\"start\":57575},{\"end\":57597,\"start\":57589},{\"end\":57935,\"start\":57930},{\"end\":57951,\"start\":57946},{\"end\":57960,\"start\":57958},{\"end\":57974,\"start\":57970},{\"end\":57985,\"start\":57982},{\"end\":57999,\"start\":57994},{\"end\":58012,\"start\":58009},{\"end\":58027,\"start\":58023},{\"end\":58038,\"start\":58034},{\"end\":58294,\"start\":58289},{\"end\":58310,\"start\":58303},{\"end\":58324,\"start\":58317},{\"end\":58340,\"start\":58334},{\"end\":58672,\"start\":58667},{\"end\":58687,\"start\":58682},{\"end\":58703,\"start\":58698},{\"end\":58718,\"start\":58709},{\"end\":58731,\"start\":58727},{\"end\":59008,\"start\":59003},{\"end\":59027,\"start\":59017},{\"end\":59262,\"start\":59258},{\"end\":59277,\"start\":59272},{\"end\":59291,\"start\":59284},{\"end\":59305,\"start\":59301},{\"end\":59538,\"start\":59534},{\"end\":59554,\"start\":59548},{\"end\":59566,\"start\":59561},{\"end\":59580,\"start\":59575},{\"end\":59594,\"start\":59587},{\"end\":59816,\"start\":59813},{\"end\":59830,\"start\":59826},{\"end\":59846,\"start\":59839},{\"end\":59857,\"start\":59855},{\"end\":59868,\"start\":59865},{\"end\":59882,\"start\":59879},{\"end\":59899,\"start\":59893},{\"end\":59915,\"start\":59906}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":215780580},\"end\":41757,\"start\":41495},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":247519170},\"end\":41979,\"start\":41759},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7680709},\"end\":42171,\"start\":41981},{\"attributes\":{\"id\":\"b3\"},\"end\":42537,\"start\":42173},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7684883},\"end\":42889,\"start\":42539},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2609415},\"end\":43224,\"start\":42891},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":235743051},\"end\":43494,\"start\":43226},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":46968214},\"end\":43802,\"start\":43496},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":234482661},\"end\":44042,\"start\":43804},{\"attributes\":{\"id\":\"b9\"},\"end\":44344,\"start\":44044},{\"attributes\":{\"id\":\"b10\"},\"end\":44515,\"start\":44346},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":237363825},\"end\":44823,\"start\":44517},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":245634586},\"end\":45081,\"start\":44825},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6628106},\"end\":45319,\"start\":45083},{\"attributes\":{\"id\":\"b14\"},\"end\":45649,\"start\":45321},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":228083838},\"end\":45849,\"start\":45651},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":227208781},\"end\":46135,\"start\":45851},{\"attributes\":{\"id\":\"b17\"},\"end\":46397,\"start\":46137},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15774646},\"end\":46691,\"start\":46399},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":216915079},\"end\":46923,\"start\":46693},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":232404358},\"end\":47431,\"start\":46925},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":235248354},\"end\":47832,\"start\":47433},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":219947110},\"end\":48347,\"start\":47834},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":213175590},\"end\":48769,\"start\":48349},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":246016186},\"end\":49089,\"start\":48771},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":244773517},\"end\":49493,\"start\":49091},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":233307004},\"end\":49803,\"start\":49495},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":232352612},\"end\":50026,\"start\":49805},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":195776274},\"end\":50404,\"start\":50028},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":221112229},\"end\":50537,\"start\":50406},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":226964601},\"end\":50674,\"start\":50539},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":244921004},\"end\":51033,\"start\":50676},{\"attributes\":{\"doi\":\"arXiv:2210.13641\",\"id\":\"b32\"},\"end\":51348,\"start\":51035},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":245006364},\"end\":51676,\"start\":51350},{\"attributes\":{\"id\":\"b34\"},\"end\":51860,\"start\":51678},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b35\"},\"end\":52142,\"start\":51862},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":219720931},\"end\":52482,\"start\":52144},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206942855},\"end\":52792,\"start\":52484},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":232320654},\"end\":53042,\"start\":52794},{\"attributes\":{\"doi\":\"arXiv:2211.03660\",\"id\":\"b39\"},\"end\":53430,\"start\":53044},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":236975860},\"end\":53663,\"start\":53432},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":216080881},\"end\":53854,\"start\":53665},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":50783765},\"end\":54090,\"start\":53856},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":235490453},\"end\":54466,\"start\":54092},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":232045969},\"end\":54905,\"start\":54468},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":207761262},\"end\":55274,\"start\":54907},{\"attributes\":{\"doi\":\"arXiv:2102.07064\",\"id\":\"b46\"},\"end\":55647,\"start\":55276},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":237386110},\"end\":55983,\"start\":55649},{\"attributes\":{\"id\":\"b48\"},\"end\":56290,\"start\":55985},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":235605960},\"end\":56518,\"start\":56292},{\"attributes\":{\"id\":\"b50\"},\"end\":56858,\"start\":56520},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":251903426},\"end\":57230,\"start\":56860},{\"attributes\":{\"id\":\"b52\"},\"end\":57467,\"start\":57232},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":249240205},\"end\":57827,\"start\":57469},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":248721906},\"end\":58283,\"start\":57829},{\"attributes\":{\"doi\":\"arXiv:2010.07492\",\"id\":\"b55\"},\"end\":58585,\"start\":58285},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":4766599},\"end\":58912,\"start\":58587},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":53453235},\"end\":59190,\"start\":58914},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":11977588},\"end\":59453,\"start\":59192},{\"attributes\":{\"id\":\"b59\"},\"end\":59750,\"start\":59455},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":245385791},\"end\":60117,\"start\":59752}]", "bib_title": "[{\"end\":41527,\"start\":41495},{\"end\":41793,\"start\":41759},{\"end\":42019,\"start\":41981},{\"end\":42600,\"start\":42539},{\"end\":42987,\"start\":42891},{\"end\":43289,\"start\":43226},{\"end\":43558,\"start\":43496},{\"end\":43855,\"start\":43804},{\"end\":44556,\"start\":44517},{\"end\":44896,\"start\":44825},{\"end\":45125,\"start\":45083},{\"end\":45385,\"start\":45321},{\"end\":45691,\"start\":45651},{\"end\":45923,\"start\":45851},{\"end\":46481,\"start\":46399},{\"end\":46726,\"start\":46693},{\"end\":46984,\"start\":46925},{\"end\":47539,\"start\":47433},{\"end\":47922,\"start\":47834},{\"end\":48419,\"start\":48349},{\"end\":48842,\"start\":48771},{\"end\":49173,\"start\":49091},{\"end\":49587,\"start\":49495},{\"end\":49845,\"start\":49805},{\"end\":50123,\"start\":50028},{\"end\":50425,\"start\":50406},{\"end\":50560,\"start\":50539},{\"end\":50745,\"start\":50676},{\"end\":51400,\"start\":51350},{\"end\":51708,\"start\":51678},{\"end\":52210,\"start\":52144},{\"end\":52536,\"start\":52484},{\"end\":52865,\"start\":52794},{\"end\":53506,\"start\":53432},{\"end\":53714,\"start\":53665},{\"end\":53910,\"start\":53856},{\"end\":54181,\"start\":54092},{\"end\":54517,\"start\":54468},{\"end\":54979,\"start\":54907},{\"end\":55735,\"start\":55649},{\"end\":56336,\"start\":56292},{\"end\":56939,\"start\":56860},{\"end\":57555,\"start\":57469},{\"end\":57923,\"start\":57829},{\"end\":58657,\"start\":58587},{\"end\":58994,\"start\":58914},{\"end\":59248,\"start\":59192},{\"end\":59805,\"start\":59752}]", "bib_author": "[{\"end\":41544,\"start\":41529},{\"end\":41559,\"start\":41544},{\"end\":41577,\"start\":41559},{\"end\":41593,\"start\":41577},{\"end\":41608,\"start\":41593},{\"end\":41807,\"start\":41795},{\"end\":41819,\"start\":41807},{\"end\":41835,\"start\":41819},{\"end\":41846,\"start\":41835},{\"end\":41854,\"start\":41846},{\"end\":42037,\"start\":42021},{\"end\":42049,\"start\":42037},{\"end\":42059,\"start\":42049},{\"end\":42279,\"start\":42268},{\"end\":42293,\"start\":42279},{\"end\":42311,\"start\":42293},{\"end\":42326,\"start\":42311},{\"end\":42333,\"start\":42326},{\"end\":42614,\"start\":42602},{\"end\":42623,\"start\":42614},{\"end\":42638,\"start\":42623},{\"end\":42652,\"start\":42638},{\"end\":42667,\"start\":42652},{\"end\":42688,\"start\":42667},{\"end\":42697,\"start\":42688},{\"end\":43005,\"start\":42989},{\"end\":43016,\"start\":43005},{\"end\":43033,\"start\":43016},{\"end\":43040,\"start\":43033},{\"end\":43304,\"start\":43291},{\"end\":43316,\"start\":43304},{\"end\":43329,\"start\":43316},{\"end\":43343,\"start\":43329},{\"end\":43569,\"start\":43560},{\"end\":43584,\"start\":43569},{\"end\":43598,\"start\":43584},{\"end\":43867,\"start\":43857},{\"end\":43880,\"start\":43867},{\"end\":43895,\"start\":43880},{\"end\":43910,\"start\":43895},{\"end\":44055,\"start\":44044},{\"end\":44069,\"start\":44055},{\"end\":44087,\"start\":44069},{\"end\":44102,\"start\":44087},{\"end\":44118,\"start\":44102},{\"end\":44128,\"start\":44118},{\"end\":44406,\"start\":44389},{\"end\":44424,\"start\":44406},{\"end\":44573,\"start\":44558},{\"end\":44586,\"start\":44573},{\"end\":44604,\"start\":44586},{\"end\":44622,\"start\":44604},{\"end\":44633,\"start\":44622},{\"end\":44646,\"start\":44633},{\"end\":44911,\"start\":44898},{\"end\":44924,\"start\":44911},{\"end\":44937,\"start\":44924},{\"end\":45139,\"start\":45127},{\"end\":45153,\"start\":45139},{\"end\":45157,\"start\":45153},{\"end\":45403,\"start\":45387},{\"end\":45416,\"start\":45403},{\"end\":45430,\"start\":45416},{\"end\":45446,\"start\":45430},{\"end\":45708,\"start\":45693},{\"end\":45722,\"start\":45708},{\"end\":45737,\"start\":45722},{\"end\":45937,\"start\":45925},{\"end\":45952,\"start\":45937},{\"end\":45966,\"start\":45952},{\"end\":45979,\"start\":45966},{\"end\":46153,\"start\":46137},{\"end\":46166,\"start\":46153},{\"end\":46184,\"start\":46166},{\"end\":46197,\"start\":46184},{\"end\":46494,\"start\":46483},{\"end\":46508,\"start\":46494},{\"end\":46522,\"start\":46508},{\"end\":46532,\"start\":46522},{\"end\":46738,\"start\":46728},{\"end\":46753,\"start\":46738},{\"end\":46771,\"start\":46753},{\"end\":46785,\"start\":46771},{\"end\":46800,\"start\":46785},{\"end\":46997,\"start\":46986},{\"end\":47009,\"start\":46997},{\"end\":47021,\"start\":47009},{\"end\":47031,\"start\":47021},{\"end\":47039,\"start\":47031},{\"end\":47047,\"start\":47039},{\"end\":47058,\"start\":47047},{\"end\":47069,\"start\":47058},{\"end\":47550,\"start\":47541},{\"end\":47563,\"start\":47550},{\"end\":47580,\"start\":47563},{\"end\":47590,\"start\":47580},{\"end\":47605,\"start\":47590},{\"end\":47618,\"start\":47605},{\"end\":47940,\"start\":47924},{\"end\":47950,\"start\":47940},{\"end\":47970,\"start\":47950},{\"end\":47996,\"start\":47970},{\"end\":48012,\"start\":47996},{\"end\":48029,\"start\":48012},{\"end\":48042,\"start\":48029},{\"end\":48047,\"start\":48042},{\"end\":48437,\"start\":48421},{\"end\":48447,\"start\":48437},{\"end\":48467,\"start\":48447},{\"end\":48486,\"start\":48467},{\"end\":48499,\"start\":48486},{\"end\":48516,\"start\":48499},{\"end\":48520,\"start\":48516},{\"end\":48859,\"start\":48844},{\"end\":48871,\"start\":48859},{\"end\":48889,\"start\":48871},{\"end\":48907,\"start\":48889},{\"end\":49193,\"start\":49175},{\"end\":49212,\"start\":49193},{\"end\":49228,\"start\":49212},{\"end\":49239,\"start\":49228},{\"end\":49256,\"start\":49239},{\"end\":49269,\"start\":49256},{\"end\":49277,\"start\":49269},{\"end\":49606,\"start\":49589},{\"end\":49620,\"start\":49606},{\"end\":49636,\"start\":49620},{\"end\":49860,\"start\":49847},{\"end\":49880,\"start\":49860},{\"end\":49896,\"start\":49880},{\"end\":50138,\"start\":50125},{\"end\":50155,\"start\":50138},{\"end\":50169,\"start\":50155},{\"end\":50187,\"start\":50169},{\"end\":50203,\"start\":50187},{\"end\":50443,\"start\":50427},{\"end\":50459,\"start\":50443},{\"end\":50578,\"start\":50562},{\"end\":50594,\"start\":50578},{\"end\":50764,\"start\":50747},{\"end\":50783,\"start\":50764},{\"end\":50799,\"start\":50783},{\"end\":50809,\"start\":50799},{\"end\":50830,\"start\":50809},{\"end\":50839,\"start\":50830},{\"end\":51121,\"start\":51105},{\"end\":51129,\"start\":51121},{\"end\":51143,\"start\":51129},{\"end\":51152,\"start\":51143},{\"end\":51418,\"start\":51402},{\"end\":51426,\"start\":51418},{\"end\":51435,\"start\":51426},{\"end\":51451,\"start\":51435},{\"end\":51465,\"start\":51451},{\"end\":51481,\"start\":51465},{\"end\":51498,\"start\":51481},{\"end\":51722,\"start\":51710},{\"end\":51747,\"start\":51722},{\"end\":51754,\"start\":51747},{\"end\":51946,\"start\":51930},{\"end\":51964,\"start\":51946},{\"end\":52230,\"start\":52212},{\"end\":52245,\"start\":52230},{\"end\":52264,\"start\":52245},{\"end\":52279,\"start\":52264},{\"end\":52297,\"start\":52279},{\"end\":52552,\"start\":52538},{\"end\":52571,\"start\":52552},{\"end\":52585,\"start\":52571},{\"end\":52602,\"start\":52585},{\"end\":52618,\"start\":52602},{\"end\":52880,\"start\":52867},{\"end\":52892,\"start\":52880},{\"end\":52906,\"start\":52892},{\"end\":53135,\"start\":53125},{\"end\":53150,\"start\":53135},{\"end\":53166,\"start\":53150},{\"end\":53175,\"start\":53166},{\"end\":53185,\"start\":53175},{\"end\":53199,\"start\":53185},{\"end\":53525,\"start\":53508},{\"end\":53534,\"start\":53525},{\"end\":53732,\"start\":53716},{\"end\":53746,\"start\":53732},{\"end\":53930,\"start\":53912},{\"end\":53946,\"start\":53930},{\"end\":53960,\"start\":53946},{\"end\":54194,\"start\":54183},{\"end\":54207,\"start\":54194},{\"end\":54217,\"start\":54207},{\"end\":54237,\"start\":54217},{\"end\":54250,\"start\":54237},{\"end\":54264,\"start\":54250},{\"end\":54534,\"start\":54519},{\"end\":54549,\"start\":54534},{\"end\":54562,\"start\":54549},{\"end\":54572,\"start\":54562},{\"end\":54591,\"start\":54572},{\"end\":54608,\"start\":54591},{\"end\":54624,\"start\":54608},{\"end\":54645,\"start\":54624},{\"end\":54661,\"start\":54645},{\"end\":54673,\"start\":54661},{\"end\":54992,\"start\":54981},{\"end\":55006,\"start\":54992},{\"end\":55015,\"start\":55006},{\"end\":55030,\"start\":55015},{\"end\":55042,\"start\":55030},{\"end\":55288,\"start\":55276},{\"end\":55301,\"start\":55288},{\"end\":55312,\"start\":55301},{\"end\":55322,\"start\":55312},{\"end\":55348,\"start\":55322},{\"end\":55745,\"start\":55737},{\"end\":55758,\"start\":55745},{\"end\":55772,\"start\":55758},{\"end\":55783,\"start\":55772},{\"end\":55793,\"start\":55783},{\"end\":55803,\"start\":55793},{\"end\":56091,\"start\":56079},{\"end\":56101,\"start\":56091},{\"end\":56115,\"start\":56101},{\"end\":56129,\"start\":56115},{\"end\":56350,\"start\":56338},{\"end\":56361,\"start\":56350},{\"end\":56374,\"start\":56361},{\"end\":56388,\"start\":56374},{\"end\":56534,\"start\":56520},{\"end\":56549,\"start\":56534},{\"end\":56568,\"start\":56549},{\"end\":56587,\"start\":56568},{\"end\":56602,\"start\":56587},{\"end\":56616,\"start\":56602},{\"end\":56950,\"start\":56941},{\"end\":56966,\"start\":56950},{\"end\":56979,\"start\":56966},{\"end\":56994,\"start\":56979},{\"end\":57006,\"start\":56994},{\"end\":57017,\"start\":57006},{\"end\":57031,\"start\":57017},{\"end\":57241,\"start\":57232},{\"end\":57252,\"start\":57241},{\"end\":57268,\"start\":57252},{\"end\":57285,\"start\":57268},{\"end\":57567,\"start\":57557},{\"end\":57581,\"start\":57567},{\"end\":57599,\"start\":57581},{\"end\":57937,\"start\":57925},{\"end\":57953,\"start\":57937},{\"end\":57962,\"start\":57953},{\"end\":57976,\"start\":57962},{\"end\":57987,\"start\":57976},{\"end\":58001,\"start\":57987},{\"end\":58014,\"start\":58001},{\"end\":58029,\"start\":58014},{\"end\":58040,\"start\":58029},{\"end\":58296,\"start\":58285},{\"end\":58312,\"start\":58296},{\"end\":58326,\"start\":58312},{\"end\":58342,\"start\":58326},{\"end\":58674,\"start\":58659},{\"end\":58689,\"start\":58674},{\"end\":58705,\"start\":58689},{\"end\":58720,\"start\":58705},{\"end\":58733,\"start\":58720},{\"end\":59010,\"start\":58996},{\"end\":59029,\"start\":59010},{\"end\":59264,\"start\":59250},{\"end\":59279,\"start\":59264},{\"end\":59293,\"start\":59279},{\"end\":59307,\"start\":59293},{\"end\":59540,\"start\":59526},{\"end\":59556,\"start\":59540},{\"end\":59568,\"start\":59556},{\"end\":59582,\"start\":59568},{\"end\":59596,\"start\":59582},{\"end\":59818,\"start\":59807},{\"end\":59832,\"start\":59818},{\"end\":59848,\"start\":59832},{\"end\":59859,\"start\":59848},{\"end\":59870,\"start\":59859},{\"end\":59884,\"start\":59870},{\"end\":59901,\"start\":59884},{\"end\":59917,\"start\":59901}]", "bib_venue": "[{\"end\":41616,\"start\":41608},{\"end\":41858,\"start\":41854},{\"end\":42067,\"start\":42059},{\"end\":42266,\"start\":42173},{\"end\":42701,\"start\":42697},{\"end\":43048,\"start\":43040},{\"end\":43347,\"start\":43343},{\"end\":43602,\"start\":43598},{\"end\":43914,\"start\":43910},{\"end\":44187,\"start\":44128},{\"end\":44387,\"start\":44346},{\"end\":44650,\"start\":44646},{\"end\":44941,\"start\":44937},{\"end\":45185,\"start\":45157},{\"end\":45474,\"start\":45446},{\"end\":45741,\"start\":45737},{\"end\":45983,\"start\":45979},{\"end\":46236,\"start\":46197},{\"end\":46537,\"start\":46532},{\"end\":46803,\"start\":46800},{\"end\":47140,\"start\":47069},{\"end\":47622,\"start\":47618},{\"end\":48075,\"start\":48047},{\"end\":48545,\"start\":48520},{\"end\":48923,\"start\":48907},{\"end\":49281,\"start\":49277},{\"end\":49640,\"start\":49636},{\"end\":49906,\"start\":49896},{\"end\":50208,\"start\":50203},{\"end\":50463,\"start\":50459},{\"end\":50598,\"start\":50594},{\"end\":50843,\"start\":50839},{\"end\":51103,\"start\":51035},{\"end\":51502,\"start\":51498},{\"end\":51758,\"start\":51754},{\"end\":51928,\"start\":51862},{\"end\":52304,\"start\":52297},{\"end\":52628,\"start\":52618},{\"end\":52910,\"start\":52906},{\"end\":53123,\"start\":53044},{\"end\":53538,\"start\":53534},{\"end\":53750,\"start\":53746},{\"end\":53964,\"start\":53960},{\"end\":54271,\"start\":54264},{\"end\":54677,\"start\":54673},{\"end\":55079,\"start\":55042},{\"end\":55426,\"start\":55364},{\"end\":55807,\"start\":55803},{\"end\":56077,\"start\":55985},{\"end\":56395,\"start\":56388},{\"end\":56675,\"start\":56616},{\"end\":57036,\"start\":57031},{\"end\":57330,\"start\":57285},{\"end\":57634,\"start\":57599},{\"end\":58044,\"start\":58040},{\"end\":58412,\"start\":58358},{\"end\":58737,\"start\":58733},{\"end\":59039,\"start\":59029},{\"end\":59311,\"start\":59307},{\"end\":59524,\"start\":59455},{\"end\":59921,\"start\":59917},{\"end\":46242,\"start\":46238},{\"end\":47198,\"start\":47142}]"}}}, "year": 2023, "month": 12, "day": 17}