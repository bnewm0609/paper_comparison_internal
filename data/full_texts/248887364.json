{"id": 248887364, "updated": "2023-10-05 14:15:56.348", "metadata": {"title": "SNaC: Coherence Error Detection for Narrative Summarization", "authors": "[{\"first\":\"Tanya\",\"last\":\"Goyal\",\"middle\":[]},{\"first\":\"Junyi Jessy\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Greg\",\"last\":\"Durrett\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Progress in summarizing long texts is inhibited by the lack of appropriate evaluation frameworks. A long summary that appropriately covers the facets of that text must also present a coherent narrative, but current automatic and human evaluation methods fail to identify gaps in coherence. In this work, we introduce SNaC, a narrative coherence evaluation framework for fine-grained annotations of long summaries. We develop a taxonomy of coherence errors in generated narrative summaries and collect span-level annotations for 6.6k sentences across 150 book and movie summaries. Our work provides the first characterization of coherence errors generated by state-of-the-art summarization models and a protocol for eliciting coherence judgments from crowdworkers. Furthermore, we show that the collected annotations allow us to benchmark past work in coherence modeling and train a strong classifier for automatically localizing coherence errors in generated summaries. Finally, our SNaC framework can support future work in long document summarization and coherence evaluation, including improved summarization modeling and post-hoc summary correction.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2205.09641", "mag": null, "acl": "2022.emnlp-main.29", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/GoyalLD22", "doi": "10.18653/v1/2022.emnlp-main.29"}}, "content": {"source": {"pdf_hash": "f9ea8fe3598e957e1582210d24e78e056f7466ef", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.emnlp-main.29.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d1dad7b6fbdb6d2306dbf50dba4c3cd33a6a0773", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f9ea8fe3598e957e1582210d24e78e056f7466ef.txt", "contents": "\nSNAC: Coherence Error Detection for Narrative Summarization\nDecember 7-11, 2022\n\nTanya Goyal tanyagoyal@utexas.edu \nDepartment of Computer Science\n\n\nJunyi Jessy Li \nDepartment of Linguistics\nThe University of Texas at Austin\n\n\nGreg Durrett \nDepartment of Computer Science\n\n\nSNAC: Coherence Error Detection for Narrative Summarization\n\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\nthe 2022 Conference on Empirical Methods in Natural Language ProcessingDecember 7-11, 2022\nProgress in summarizing long texts is inhibited by the lack of appropriate evaluation frameworks. A long summary that appropriately covers the facets of that text must also present a coherent narrative, but current automatic and human evaluation methods fail to identify gaps in coherence. In this work, we introduce SNAC, a narrative coherence evaluation framework for fine-grained annotations of long summaries. We develop a taxonomy of coherence errors in generated narrative summaries and collect spanlevel annotations for 6.6k sentences across 150 book and movie summaries. Our work provides the first characterization of coherence errors generated by state-of-the-art summarization models and a protocol for eliciting coherence judgments from crowdworkers. Furthermore, we show that the collected annotations allow us to benchmark past work in coherence modeling and train a strong classifier for automatically localizing coherence errors in generated summaries. Finally, our SNAC framework can support future work in long document summarization and coherence evaluation, including improved summarization modeling and posthoc summary correction. 1Corresponding excerpt from the human-written summary properly contextualizes the new character.\n\nIntroduction\n\nAs pre-trained models for news summarization (Lewis et al., 2020;Zhang et al., 2020;Brown et al., 2020) have improved drastically, researchers have begun tackling increasingly challenging settings, particularly long document summarization and generation of longer summaries (Kry\u015bci\u0144ski et al., 2021;Huang et al., 2021;Zhang et al., 2022;Wu et al., 2021). Summaries in these settings differ considerably from the newswire summaries of past research efforts (Nallapati et al., 2016;Narayan et al., 2018): models now need to extract salient information from different parts of a significantly Figure 1: Excerpt from a generated book summary by OpenAI's 175B model (Wu et al., 2021). Individual segments do not follow a coherent structure and extra information is often needed to understand the narrative.\n\nlonger document, and na\u00efvely combining these in a much longer output is less likely to yield a summary with coherent discourse structure.\n\nThis shift in the scope of the summarization task calls for a reexamination of the summarization evaluation framework. Even for short newswire summaries, Fabbri et al. (2021) showed that automated metrics are inadequate, and consequently, reporting results from a human evaluation study has become the standard practice. However, human evaluation is rarely done for longer summaries possibly due to the associated labor costs of reading and evaluating long text. It is also unclear whether A/B testing or Likert-scale based annotation frameworks transfer to long summary settings. Establishing human evaluation protocols is critical for comparing different modeling approaches and measuring progress.\n\nRecently, Wu et al. (2021) proposed a strong book summarization model but showed that although generated summaries covered important information from the books, they read like a list of events stapled together without any coherent narrative structure (see Figure 1). We found similar \n\n\nCoherence Errors Current Summary Segment\n\nFenwick's wife becomes frightened when a tramp threatens to kill her and her child. Figure 2: SNAC's error schema. Given context, i.e., the generated summary until that point, annotators identify error spans in the current summary segment. We define two high-level error categories: (1) Coherence Errors that directly affect narrative understanding, and (2) Language Errors that measure other aspects, like grammar. characteristics in other recent narrative summarization models (Kry\u015bci\u0144ski et al., 2021;Zhang et al., 2022). Now that models are so good at generating fluent and on-topic sentences, the coherence of the whole summary becomes a first-order issue that must be evaluated in these new settings.\n\nIn this work, we introduce SNAC, a framework for collecting fine-grained annotations to evaluate Summary Narrative Coherence. We develop an error schema with 7 narrative error types grounded in actual errors made by current summarization models. Our fine-grained taxonomy allows annotators to explicitly state what kind of coherence error exists in a summary and pinpoint where it occurs. We show that such a fine-grained annotation framework is better suited for collecting crowd annotations than a Likert scale-based holistic evaluation of coherence.\n\nWe enlist crowdworkers to collect a large-scale dataset of 9.6k span-level error annotations in narrative summaries generated by current state-of-theart summarization models (Wu et al., 2021;Zhang et al., 2022) on two datasets: movie screenplays (Chen et al., 2022) and books (Kry\u015bci\u0144ski et al., 2021). Our work is the first to characterize specific errors made by these systems and gaps that exist with human-written coherent summaries. While recent efforts have studied errors in open-ended generation (Dou et al., 2022), these differ drastically from summarization errors and their taxonomies and findings are not transferable (see Appendix A).\n\nWe also evaluate the performance of automatic coherence models, comparing synthetic data generation techniques (Moon et al., 2019;Shen et al., 2021) against SNAC annotations as training sources. Not only do models fine-tuned on SNAC outperform those trained on synthetic datasets, we find that they also report higher recall than individual human annotators at identifying fine-grained coherence error categories.\n\nOur collected dataset and analysis provides a foundation for downstream applications such as better long summary evaluation, coherence-aware generation, and post-correction of summaries.\n\n\nLong Narrative Summarization\n\nWe study coherence errors in two domains, books and movie screenplays, although our taxonomy and annotation methodology are broadly applicable.\n\nBooks We evaluate the depth 1 book summaries generated by a GPT-3 based model (Wu et al., 2021). We evaluate both its 175B and 6B versions, denoted by BOOK-175B and BOOK-6B respectively. On average, these are~35 sentences long.\n\n\nMovie Screenplays\n\nWe generate summaries for the movie scripts dataset (Papalampidi et al., 2020) using the BART-based Summ\u02c6N model (Zhang et al., 2022). 2 These are~40 sentences in length. We refer to them as MOVIE-BART.\n\nThe majority of prior research in evaluation of evaluation metrics (Kry\u015bci\u0144ski et al., 2019;Bhandari et al., 2020;Fabbri et al., 2021) has focused on news summarization datasets (Nallapati et al., 2016;Narayan et al., 2018). However there exist substantial differences in the scale of news settings and narrative summaries: the former are considerably shorter at~3 sentences per summary. We first explore whether existing approaches to evaluation can work well despite this difference. \n\n\nLimitations of Current Human Evaluation\n\nSummary-level Likert scale annotations are the most commonly used setup for collecting coherence in single-document news summarization research (Fabbri et al., 2021). Here, we run an analogous study for our longer narrative summaries. We ask 3 Mechanical Turk workers with prior experience in annotation for NLP tasks, specifically discourse analysis and text simplification, to rate the overall coherence of 100 generated summaries on a 5-point scale. Table 1 reports the observed agreement, measured by Kripendorff's \u03b1. Compared to newswire summaries collected under a similar setup (Fabbri et al., 2021), annotations for longer narratives have a much lower agreement. This shows the difficulty in obtaining a consensus on coherence for a 500+ word summary through a single value on a 5-point scale.\n\nIn Appendix B, we further show that automatic metrics like ROUGE and BERTScore (Zhang et al., 2019) that are primarily used for evaluating long document summarization fail to penalize coherence errors in summaries. Better tools for both automatic and human evaluation are needed.\n\n\nSNAC Annotation Methodology\n\nWe design our methodology to: 1) simplify the summary-level annotation task into smaller subtasks, and 2) provide a structured framework that allows annotators to specify the type of coherence error, instead of evaluating coherence holistically.\n\n\nTask Workflow and Notation\n\nWe decompose the summary-level task into smaller segment-level tasks: at each step, annotators evaluate a subpart of the summary, which is usually 2-4 sentences long. Let S 0 , S 1 ...S N denote these summary segments. While evaluating segment S i , coherence judgments are made with respect to both the context S 0 , S 1 ...S i\u22121 and text within S i .\n\nTo annotate a single error in S i , annotators select the error span t j \u2208 S i and the coherence error type e j (error taxonomy outlined in Section 3.2) to construct the error triple a j = (S i , t j , e j ). This process is repeated until all errors in segment S i have been added, after which they proceed to the next segment S i+1 for annotation. At the end of the annotation, workers produce the full set of annotations A = {a j \u2200j} across all the text segments. The outcome of this is shown in Figure 2.\n\nFor book summaries, i.e. BOOK-175B and BOOK-6B, our segments come from boundaries present in the generated summaries. These are an average of 2.7 sentences. For MOVIE-BART, we segment summaries into chunks of 3 sentences.\n\n\nError Taxonomy\n\nReinhart (1980) states three conditions for coherence: connectedness (cohesion), consistency, and relevance. Our error taxonomy is guided by these conditions while covering the broad range of coherence errors produced by current models.\n\nWe divide errors into two categories: a) Coherence errors: these measure whether the summary is well-structured and events in the summary make narrative sense, and b) Language errors: these measure other aspects of the quality of generated text, such as grammar. While these do not come under the ambit of coherence errors, we found it useful to provide these additional error types for crowd workers to anchor other \"badness\" in text to.\n\n\nCoherence Errors\n\nNew character without introduction (CharE) These refer to scenarios where a new person is introduced in the narrative without providing any background about the person, or their relation with other characters in the story. This violates condition 1 of coherence, i.e. connectedness. Note that well-known people, e.g. Barack Obama, do not need an introduction. 3\n\nMissing reference to an event or object (RefE) These refer to scenarios where an event or object is mentioned for the first time, but the phrasing strongly implies that it must have been introduced previously or that some context is missing to fully understand it. E.g., in Figure 2, the phrasing of her husband's suicide gives the strong impression that the reader is already aware of this event.\n\nAbrupt scene transition (SceneE) These occur where there is a sudden shift in the narrative and are  Figure 3: An example of expert annotations for a BOOK-175B summary (we only show coherence errors). The number of annotators who identified each span is denoted by v; for simplicity, we omit v = 1 errors. We see that annotators often identify overlapping error spans; this fine-grained picture of coherence cannot be achieved by a summary-level score.\n\nrelated to both connectedness and relevance. For these, we ask annotators to select whole sentences.\n\nInconsistency (InconE) These errors violate the second condition of coherence, i.e. contradicting other information in the Context or within the Current Segment. For these errors, we also ask annotators to choose the previous span it is inconsistent with.\n\n\nLanguage Errors\n\nRepetition (RepE) These are used to detect repetition. Similar to InconE, annotators also select the antecedent span with the repeated content.\n\nUngrammatical or Nonsensical Text (GramE) These refer to text spans with grammar errors. Also included in this category are cases where there are obvious model degenerations.\n\nUnclear coreference (CorefE) These refer to cases where it is unclear who or what a pronoun is referring to. While sometimes requiring extra clarity, we found that there errors rarely affected the overall narrative understanding unless they cooccured with GramE. Therefore, we do not include them in the coherence error category. The version of definitions and task instructions given to the annotators is in Appendix D.\n\n\nData Collection\n\nWe collect annotations from two types of annotators: experts and crowdworkers.  \n\n\nExpert Annotations\n\nExpert annotations were collected from 3 authors who have previously published papers in text summarization and have experience engaging with model-generated text. Each annotator evaluated 10 book summaries, 5 each from BOOK-175B and BOOK-6B. This resulted in a dataset of~700 spanlevel error annotations. Furthermore, we project span-level annotations to obtain binary coherent (no coherence error) and incoherent labels (at least one coherence error) at the sentence-and segmentlevels. Table 2 provides statistics at these levels. We observed high inter-annotator agreement for expert annotators at both the sentence-and segment-levels (see Table 4). We used this dataset to train crowdworkers in the next stage.\n\n\nCrowd Annotations\n\nWe first launched a qualification task to recruit MTurk workers. The qualification was only made available to a subset of workers who had previously worked on other data annotation efforts for  Figure 4: The left graph shows the fraction of times a specific error type is detected for each individual dataset: CharE, RefE and SceneE errors constitute the majority of coherence errors. The right graph shows the average fraction of tokens belonging to each error-type. NLP tasks. It included detailed instructions explaining the task workflow, interface, and error schema. Each worker was asked to annotate 2 book summaries; these summaries were chosen from the set of expert annotations. Workers were paid $12 for attempting this qualification.\n\nWe evaluated each worker's annotations against experts and sent individual feedback. Among coherence errors, we observed that workers generally tended to disagree on RefE; each worker had a different calibration of which events or objects require more context to improve overall narrative understanding. Another common source of disagreement between workers and experts were SceneE errors. To help align their understanding with experts, we provided workers with a complete set of expert annotations for a whole summary for reference.\n\nWe recruited 11 workers after the qualification to annotate 150 generated summaries. Each summary was annotated by 3 different annotators. Workers were paid an average of $12/hr.\n\n\nSNAC Dataset\n\nOur resulting dataset consists of~9.6k span-level annotations for coherence judgments, across 160 summaries. Dataset statistics for the entire collected dataset, including both expert and crowd annotations, are shown in Table 2.\n\nA summary-wide expert annotation is SNAC is shown in Figure 3. Noticeably, CharE spans constitute the majority of errors; this observation is consistent throughout all datasets. Annotators tend to show higher recall and agreement over this category. SceneE and RefE are the next two major error categories. The annotations also illustrate the two reasons for SceneE: 1) there is a sudden change in setting and characters, e.g. Mr Lorry visits the... and 2) the previous scene is abruptly cut off, e.g. In court, Mr. Darnay..., where Ms. Mannette's story is unfinished.\n\nWe observed that worker annotations are high precision but low recall (CharE errors are an exception; workers have both high precision and recall for this category). This means that error spans identified by each worker tended to be actual errors, even when they were not detected by other annotators. Therefore, we combine annotations of all 3 annotators to construct the full SNAC dataset. Figure 4 shows the fraction of unique errors of each error type annotated across all datasets. As seen in Figure 3 annotations, the majority of the coherence errors are due to CharE, RefE or SceneE. The bottom graph of Figure 4 shows the number of error tokens annotated (instead of numbers of errors) for each error type. We see that annotators mark a larger fraction of tokens in the BOOK-6B dataset as erroneous compared to BOOK-175B. The main difference comes from the difference in SceneE (annotators are instructed to select entire sentences) and GramE. As expected, for smaller summarization models, i.e. GPT-3 6B and BART, a larger fraction of errors and error tokens are associated with language errors compared to GPT-3 175B. In fact, we noticed that workers were more likely to skip coherence error annotations, e.g. RefE, when these co-occur with GramE for these models, particularly on BOOK-6B.\n\n\nError Distributions\n\nHuman annotators focus on language errors while assessing coherence holistically. To understand which aspects of a summary contribute to the summary-level coherence rating provided by crowd workers, we compute the correlation between the number of errors of each type with the overall coherence score (Likert rating on a scale of 1-5, described previously in Section 2). 4\n\n\nError Type Coherence Language\n\nTotal r -0.26 * -0.34 * -0.33 * Table 3: Pearson correlation between no. of errors and summary-level coherence score for error categories. Annotators tend to focus on grammar instead of coherencespecific errors while assigning overall summary-score. * : p-value < 0.05, according to a two-tailed test. Table 3 outlines our results. First, it shows that the total number of errors is correlated with the overall coherence score, but annotators tend to weight language errors higher than coherencespecific errors. Surprisingly, we see negligible correlation with SceneE errors although these are a prominent distinguisher between generated and human-written summaries. Amongst other error types, both RefE errors and GramE errors show relatively higher correlation. Although not directly evaluating coherence, Clark et al. (2021) report similar observations where annotators tend to focus on grammar errors while judging text quality.\n\n\nCoherence Errors Language Errors\nCharE -0.22 * RepE -0.21 RefE -0.29 * CorefE -0.24 * SceneE -0.05 GramE -0.25 * InconE -0.09\nNarrative Summarization \u0338 = Open-Ended Generation In story completion, models are not required to cover all salient information from a document and only condition on past generated text; generated open-ended summaries rarely diverge off-topic. Examples of GPT-3 generated stories in Figure 8 (Appendix A) show that these generate almost no CharE, RefE or SceneE errors that form the majority in SNaC, and instead mainly exhibit repetition. Therefore, research efforts that introduce fine-grained taxonomies for this task, e.g. Scarecrow (Dou et al., 2022), are directly applicable to summarization which needs to be independently studied.\n\n\nInter-Annotator Agreement\n\nWe first compute inter-annotator agreements at the sentence-and segment-levels. This allows for an apples-to-apples comparison with Fabbri et al.\n\n(2021) as the average length of news summaries is roughly equal to our segment length. We convert their 5-point Likert ratings into binary labels using that each annotator's aggregated segment-level errors are correlated with their own summary-level judgment; here, agreement between annotators is not relevant.   the threshold that gives the best agreement score. We compare Krippendorff's \u03b1 for SNAC and news in Table 4: SNAC reports high inter-annotator agreement at both the sentence-and segment-level. Notably, this segment level agreement is better than that of crowdworkers in the news domain.\n\nSpan-level analysis Next, we evaluate categoryspecific agreement between annotators at the span level. We report two metrics: 1) Krippendorff's \u03b1 and 2) two-agree %; borrowed from Dou et al. (2022), this reports the percentage of tokens labeled as erroneous by at least one annotator that were also labelled by one or more additional annotators. For RefE and InconE, we noticed that small differences in span boundaries caused a significant drop in agreement, therefore, for these we also report metrics after normalizing span boundaries of overlapping spans to their union.  % is as low as 20 and 12 respectively for 10 annotators. Note that we only have 3 annotators, so we expect our two-agree numbers to be much lower. 5 Figure 5 shows an example of a summary with low crowd agreement over the RefE errors (we omit all other identified errors in this figure). For the first highlight, it is reasonable to seek more clarity on why Gabriel's reputation as a shepherd makes it difficult for him to find work as it presupposes negative connotations associated with his profession that the reader is not privy to. The second highlight asserts that Bathsheba owns or works at \"the farm\" as a known fact, which is information that has not been mentioned previously. Although only annotated by one annotator, these qualify as RefE errors according to our definition.\n\n\nDetecting Coherence Errors\n\nSetup We aim to see whether models can automatically detect our coherence errors. We formulate all models as sequence classifiers: given a context c and a sentence s, the goal is to classify whether s contains coherence errors. Similar to Section 4.3, we project span-level errors to a sentence-level gold coherence label y * \u2208 {0, 1}. Let E = {(e * j , t * j )} denote the set of error types and corresponding spans in s.\n\nWe split SNAC into train (4.2k), dev (230)  Metrics First, we consider a sentence-level binary classification version of this task: can models correctly predict if a sentence contains coherence errors? In this case, our models take the form P (y | c, s) where y \u2208 {0, 1}. We report precision, recall and F1. Note that the sentence-level y pred judgment can be due to any of the error types. 5 We omit comparison with Krippendorff's \u03b1 reported in Dou et al. (2022) as they report observed agreement without normalizing by expected agreement. We re-compute their interannotator agreement on their dataset with normalization for a randomly selected subset of 3 annotators (comparable to our setting). This gives an average of 0.14 Krippendorff's \u03b1 across all categories, with the bottom 5 categories reporting an average of 0.05 \u03b1.\n\nWe next evaluate fine-grained prediction: can models identify the specific coherence error type and pinpoint the error span? In this case, our models predict P (y | c, s), where y is a bundle consisting of y and a set of error tuples {(e pred j , t pred j )} if y = 0. We report the precision, recall and F1 performance at correctly identifying the error type, i.e. e pred j = e * j \u2200e j . We also report ov. computed as the fraction of times the predicted error span overlaps with the correct error span.\n\n\nModels for Comparison\n\nWe compare performances of three types of models: (1) unsupervised (UNSUP).\n\n(2) Models trained on synthetic data targeting coherent errors (SYN). We follow prior work (Joty et al., 2018;Shen et al., 2021) and generate synthetic training data by introducing artificial coherence errors in reference text, specifically on the BookSum dataset (Kry\u015bci\u0144ski et al., 2021). We ensure zero overlap between this synthetic train set and the evaluation test set. (3) Models fine-tuned on the SNAC data (FT).\n\n(UNSUP) LM Perplexity We use GPT-2 (Radford et al., 2019) to obtain the probability of the sentence s, given the context c, i.e. P (s | c). The dev set is used to select a threshold \u03c4 LM and obtain binary labels from these probabilities: predict an error if P (s | c) < \u03c4 LM .\n\n(UNSUP) Entity Grid We construct entity grids (Barzilay andLapata, 2005, 2008) for both predicted and gold summaries in order to compare their discourse structures. Using gold summaries in the BookSum dataset, we estimate the probabilities of syntactic role transitions of entities between sentences, e.g. p(S \u2192 O), p(S \u2192 X), p(O \u2192 S), etc. Then, we score the coherence of a predicted summary s as the log probability of the transition from c \u22121 , i.e. the last sentence of context c, to sentence s: w(c, s) = e\u2208E log p(r(s, e) | r(c \u22121 , e)). Here, E is the full set of entities in s and c \u22121 and r(x, e) denotes the role of entity e in sentence x.\n\nThe SNAC dev set is used to select a threshold \u03c4 EG and obtain binary labels from these scores: predict a coherence error if w(c, s) < \u03c4 EG . an entity. We derive non-coherent examples by setting s = s j and removing sentence s i from the context, i.e. c = s 1 s 2 ...s i\u22121 s i+1 ...s j\u22121 (shown in Figure 6). Conversely, for positive coherent training data, we retain the original context from the gold summaries, i.e. c = s 1 s 2 ...s i ...s j\u22121 . We fine-tune T5-Large (Raffel et al., 2020) for binary classification P (y | c, s) on these (y, c, s) triples; training data sizes and intrinsic performance are reported in Appendix C.\n\n(SYN) Next-Sentence This method is designed to target SceneE errors and closely resembles the sentence insertion method from prior work (Shen et al., 2021). Given context c = s 1 s 2 ...s i , we obtain negative coherence examples by replacing the next sentence with another randomly sampled sentence from the remainder of the same summary, i.e. s = s j , where j > i + 1. Positive examples are created by retaining the original summary completion, i.e. s = s i+1 . Figure 6 illustrates this. We fine-tune T5-Large to model P (y | c, s).\n\n(FT) Models trained on SNAC data We consider two versions: 1) w/o span: trained to generate yes/no reflecting the coherence of sentence s, and 2) w/ span: trained to additionally predict the error category (e.g. CharE) and the corresponding error spans. Note that s can have errors belonging to multiple error categories, the model is trained to generate these in sequence. Figure 6 illustrates this. For SceneE, we omit span prediction as these are designed to incorporate the whole sentence.   Similar to SYN, we fine-tune T5-Large on these datasets.\n\n\nResults\n\nSentence-level binary classification Figure 7 shows an ROC curve of different models; the dotted black line indicates random chance. It shows that the entity-grid approach performs poorly compared to all neural approaches. Next, all trained models outperform the LM perplexity model; language models aggregating token-level probabilities cannot detect coherence errors. Finally, models trained on SNAC outperform synthetic datasets which are the primary source of training data in prior coherence work. This show that human annotations are needed to train strong coherence classifiers.\n\nFine-grained prediction Only our FT w/ span model is trained to predict both the error category and the corresponding spans. Therefore, we compare its performance against human annotators. For an apples-to-apples comparison, we re-construct our test set by aggregating annotations of two randomly chosen annotators. This unfairly penalizes FT w/ span by introducing a mismatch between its train and test conditions, especially precision. Therefore, we also report precision scores on the original test set in brackets. Full set of results on the original test set are in Appendix C. Table 6 outlines the results. As observed during qualitative evaluation, the held-out human annotations are high precision and low recall. On the other hand, FT w/ span is trained on the aggregated annotations from three annotators and reports higher recall than humans. Consequently, its F1 scores are comparable to human performance except for InconE. We attribute this to the limited number of training examples of this category.\n\nSimilar to previous analysis, we observe that models and humans report the best performance at detecting CharE. Interestingly, the trained model can identify both SceneE and RefE with higher recall compared to human annotators. For these top three error types, trained models are successful at localizing error to specific spans, reporting high overlap scores.\n\n\nDiscussion\n\nOur analysis of current narrative summarization models reveals that these do not generate coherent narratives; in fact, each generated summary contains~30 coherence errors of varying degrees of severity. Moreover, both automatic and human approaches for coherence evaluation fail to reliably measure coherence. SNAC addresses this gap.\n\nHowever, we stop short of providing a prepackaged metric: which errors are more severe is application-dependent and subjective, and overall error counts cannot be compared. We encourage future work to focus on fine-grained error annotations, like those we present here, instead of sentence-or document-level annotations that do not provide actionable insights. We also recommend fine-grained error modeling for future coherence systems as well. While previous modeling has targeted document-or sentence-level coherence, our models trained on SNAC data can detect span-level coherence errors, particularly CharE errors with high accuracy. This automatic error localization opens up future avenues of post-hoc error correction systems built on top of coherence models.\n\n\nRelated Work\n\nCoherence frameworks Inspired by Centering Theory (Grosz et al., 1995), Barzilay andLapata (2005, 2008) proposed entity-grid models to measure coherence through transitions of entity roles. This was further extended to incorporate non-head entities (Elsner and Charniak, 2011), discourse roles (Lin et al., 2011), and other improvements (Feng and Hirst, 2012;Feng et al., 2014), including neural variations (Guinaudeau and Strube, 2013;Nguyen and Joty, 2017;Joty et al., 2018) to better model text coherence. However, these models have been evaluated primarily on document-level essay scoring tasks (Mesgar and Strube, 2018) or artificial sentence-ordering tasks (Shen et al., 2021), and not on model-generated coherence errors. \n\n\nSummarization Evaluation\n\n\nConclusion\n\nWe introduce SNAC, a narrative coherence evaluation framework for long summaries. We develop an error taxonomy grounded in coherence errors made by current models and annotate data to provide the first characterization of such errors in narrative summaries. We also make our annotation tool publicly available to support future research efforts.\n\n\nLimitations\n\nAlthough we view this work as an important step towards better understanding and evaluation of coherence in summaries, we acknowledge there is much more to do here. In this work, we only collect annotations and analyze coherence errors in summaries of English language books and movie screenplays. Our proposed taxonomy may not cover errors made by text summarization models for other languages and our trained models and analysis are Englishspecific.\n\nMoreover, some of these books summarized were written decades ago and may reflect the societal biases of those times, which could conceivably bias our trained error detection models. In this work, we use the text from the model generated summaries as is and do not perform any filtering.\n\nFinally, our work studies generated summaries for long narrative text. While we believe that our taxonomy is generalizable to other types of narrative text, we do not investigate whether it covers other domains involving summarization of long documents, such as government report summarization (Huang et al., 2021) or meeting summarization (Zhong et al., 2021).\n\n\nAcknowledgments\n\nThanks to Eunsol Choi and Michael Strube for providing feedback on this work, as well as our Mechanical Turk annotators for conducting the annotation. This project was partially supported by Good Systems, 6 a UT Austin Grand Challenge to develop responsible AI technologies, a grant from the UT Austin Office of the Vice President for Research through the \"Creating Connections for National Security Research Grants\" program, a grant from Open Philanthropy, NSF grants IIS-2107524, IIS-2145479, and gifts from Salesforce, Amazon, and Adobe. \n\n\nReferences\n\n\nA Narrative Summarization \u0338 = Open-Ended Generation\n\nIn Section 4.3, we noted that narrative summarization exhibits substantially different errors than open-ended text generation tasks like story generation or story completion, hence the need for our new taxonomy. We show examples of generated stories using the GPT-3 DaVinci in Figure 8. We prompt the GPT-3 text-davinci-002 model with the first few sentences of three generated summaries and ask for a 500-word completion. The coherence errors contained in these model outputs are very different from those in our narrative summarization setting. In particular, the stories hardly introduce any new characters (only Mr. Greene is introduced in the third example), and when they do, these are properly contextualized with the narrative. Furthermore, these models rarely generate RefE and generate no SceneE type of errors. In fact, repetition errors, shown in blue, dominate these narratives. Therefore, error taxonomies devised for these tasks, e.g. SCARECROW (Dou et al., 2022), are not useful for summarization settings which needs to be independently studied.\n\n\nB Limitations of Automatic Metrics\n\nLong document summarization research (Chen et al., 2022;Huang et al., 2021;Kry\u015bci\u0144ski et al., 2021;Mao et al., 2021;Pang et al., 2022) has primarily relied on ROUGE scores to evaluate summaries. But do these capture narrative coherence? We test this for long narrative summaries, using the BOOK-175B dataset as a case study. Specifically, we test whether ROUGE or BERTScore (Zhang et al., 2019) can differentiate between actual generated summaries and their corrupted versions with artificially injected coherence errors. We introduce 3 types of coherence errors to generated summaries:\n\n1. Random shuffling using a random permutation of all sentences in a BOOK-175B summary. This does not change the overall length of the generated summary.\n\n2. Repetition of a randomly selected subset of sentences. We randomly sample 50% of the sentences to repeat, all other sentences only occur once.\n\n3. Retaining only named entities in the summary and top generated bigrams. We first extract the top 200 bigrams from the generated summaries in BOOK-175B, which include frequent bigrams like of the, that he, then he, in the, etc. For each test set summary, we construct a corrupted summary by concatenating all named entities in the summary (appending each named entity as many times as it occurs in the original summary) and the top bigrams extracted from the testset-wide summaries.\n\nFor an upper bound, we also report metrics for a different human-written summary for the same input book sampled from the BookSum dataset.\n\nAutomatic metrics fail to penalize coherence errors. Table 7 shows that both shuffling and repetition do not hurt ROUGE or BERTScore, despite introducing critical coherence errors in generated summaries. The +NE & bigram setting does lead to a significant drop in BERTScore as these summaries are no longer fully-formed sentences. However, even this trivial baseline reports ROUGE scores on par with the original BOOK-175B summaries, showing that ROUGE is easy to \"game\" for this task. Finally, we see that human-written summaries, i.e., gold coherent summaries, only report 2 points of improvement in R2 and BERTScore over artificially incoherent baselines. This clearly shows that these metrics are inadequate to mea-\n\n\nPrompt (Human-Written):\n\nComplete this story in 500 words. John Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career. In London, he impresses several wealthy art collectors with his work.\n\n\nContinuation by GPT-3 DaVinci:\n\nOne of them, Mr. Greene, purchases one of his paintings for a large sum of money. With the money from the sale, Fenwick is able to pay back Mr. Morrison and also has enough left over to buy materials for more paintings. He continues to sell his paintings and gains more recognition in the art world. John Fenwick was grateful when Mr. Morrison agreed to loan him the money to move to London so he could pursue his art career. He was determined to make a success of himself and worked hard to impress the wealthy art collectors in the city. His efforts paid off when one of his paintings was purchased by Mr. Greene for a large sum of money. Fenwick was able to pay back the loan and also had enough money left over to buy materials for more paintings. His talent and hard work continued to be rewarded with more sales and recognition in the art world.\n\nHe was on his way to becoming one of the most successful artists in London. One day, however, he was visited by Mr. Morrison, who had come to collect on the loan. Fenwick was surprised and disappointed to learn that the loan had never been forgiven and that he now owed a considerable amount of money, with interest. He was forced to sell more paintings to pay off the debt and was left with little money to live on. His dream of being a successful artist was over. Fenwick was heartbroken but he didn't give up. He continued to paint and eventually regained his place in the art world. He never forgot what Mr. Morrison did to him and he was determined to make sure that no one else would ever be treated so unfairly. Fenwick became a vocal advocate for artists' rights and worked to ensure that they were fairly compensated for their work. Thanks to his efforts, artists today are able to earn a living from their art and pursue their dreams. John Fenwick's story is one of talent, hard work, and determination. Despite being dealt a difficult hand, he was able to overcome adversity and make a lasting impact on the art world.\n\nJohn Fenwick's story is one of talent, hard work and determination. Despite being dealt a difficult hand, he was able to overcome adversity and make a lasting impact on the art world. When Fenwick was first starting out, he accepted a loan from Mr. Morrison to help him move to London so he could pursue his art career. He quickly made a name for himself in the art world and started impressing wealthy art collectors with his work. One of these collectors, Mr. Greene, ended up purchasing one of Fenwick's paintings for a large sum of money.\n\n\nPrompt (Human-Written):\n\nComplete this story in 500 words. The narrator, a Yale graduate, moves to New York to learn the bond business. He visits his second cousin, Daisy, and her husband, Tom, for dinner. During the dinner, Daisy mentions she wants to go back to Chicago the next day.\n\n\nContinuation by GPT-3 DaVinci:\n\nThe narrator asks if he can go with her, and Daisy agrees. When they get back to Daisy and Tom's apartment, the narrator sees a man in the shadows who he later learns is Tom's mistress's husband. The next day, the narrator and Daisy take the train to Chicago. On the train, Daisy tells the narrator that she is pregnant and that Tom wants her to have an abortion. Daisy says she wants to keep the baby, but is afraid of what Tom will do if she does. The narrator offers to help Daisy, and they make a plan to meet in Chicago the following week.\n\nThe following week, the narrator meets Daisy in Chicago. They go to a hotel, and the narrator gets a room for Daisy. Daisy is hesitant to go into the room, but the narrator assures her that she will be safe. Daisy takes a bath, and the narrator reads to her from a book of poems. Later, Daisy falls asleep, and the narrator watches over her. In the morning, the narrator wakes Daisy and tells her it is time to go. They leave the hotel and take a taxi to the train station. Daisy thanks the narrator for his help, and they say goodbye. The narrator returns to New York and tells Tom what happened. Tom is angry and demands that the narrator leave. The narrator leaves, but not before telling Tom that Daisy is going to keep the baby. The narrator moves back to Chicago and meets Daisy again. Daisy is now married to a man named Gatsby, and they have a baby. The narrator is happy for Daisy and her new family.\n\nThe Yale graduate narrator moves to New York to learn the bond business. A few months after arriving, he visits his second cousin Daisy and her husband Tom for dinner. Daisy mentions wanting to go back to Chicago the next day, and the narrator asks if he can come along. The following day, the pair take the train to Chicago. On the ride, Daisy confides in the narrator that she is pregnant and Tom wants her to get an abortion. Daisy is torn because she wants to keep the baby, but is afraid of Tom's reaction. The narrator offers to help Daisy, and they make a plan to meet in Chicago the following week. The next week, the narrator meets Daisy in Chicago and takes her to a hotel. He gets her a room and Daisy takes a bath while the narrator reads to her from a book of poems. Daisy falls asleep and the narrator watches over her overnight. In the morning, the narrator wakes Daisy and tells her it is time to go. They leave the hotel and take a taxi to the train station. Daisy thanks the narrator for his help, and they say goodbye. The narrator returns to New York and tells Tom what happened. Tom is angry and demands that the narrator leave. The narrator leaves, but not before telling Tom that Daisy is going to keep the baby. The narrator moves back to Chicago and meets Daisy again. Daisy is now married to a man named Gatsby, and they have a baby. The narrator is happy for Daisy and her new family.\n\n\nPrompt (Human-Written):\n\nComplete this story in 500 words. Miss Manette receives a letter from the bank informing her that information about her father's small property has been discovered.\n\n\nContinuation by GPT-3 DaVinci:\n\nThe letter is from the bank. It reads: \"Dear Miss Manette, We regret to inform you that information about your father's small property has been discovered. The property was willed to you upon his death, but due to some legal complications, it has been tied up in the courts. We are currently working to resolve the matter, but in the meantime, we thought you should be aware of the situation. We will keep you updated on any developments. Sincerely, The Bank\"\n\nMiss Manette is surprised and disappointed to learn about the property. She had no idea her father even owned it. She wonders what legal complications are preventing her from accessing it.The bank assures her they are working to resolve the matter, and she decides to wait and see what happens. In the meantime, she is curious to learn more about the property and what her father's intentions were for her to have it. A few weeks later, the bank contacts Miss Manette again to let her know that the matter has been resolved and she is now the owner of the property. They give her the address and she is shocked to find that it is a small house in a rundown neighborhood. Despite the property's condition, Miss Manette is thrilled to finally have it and begins making plans to renovate it. She knows it will be a lot of work, but she is up for the challenge. With the help of her friends, she is able to transform the house into a cozy home that she can be proud of. Though it is not the property she originally envisioned, Miss Manette is happy to have a place to call her own.\n\nMiss Manette was surprised and disappointed to learn about the property her father owned. She had no idea what legal complications were preventing her from accessing it, but the bank assured her they were working to resolve the matter. A few weeks later, the bank contacted Miss Manette again to let her know that the matter had been resolved and she was now the owner of the property. They gave her the address and she was shocked to find that it was a small house in a rundown neighborhood. Despite the property's condition, Miss Manette was thrilled to finally have it and began making plans to renovate it. She knew it would be a lot of work, but she was up for the challenge. With the help of her friends, she was able to transform the house into a cozy home that she could be proud of. Though it was not the property she originally envisioned, Miss Manette was happy to have a place to call her own.        Table 10: Sentence-level recall of different errors types. Models (except FT w/ span) do not predict the error category; here, we treat these methods as binary classifiers and compute recalls as described in Appendix C.3.\n\n\nC.2 Implementation Details\n\n\nC.3 Additional Results\n\nSentence-level binary classification In Section 5, we reported sentence-level binary classification results for all models. However, the sentence-level y pred judgment in that setting can be due to any of the 4 error types or their combination and binary classification metrics do not tell us which of these error types are easier to detect.\n\nTo answer this, we compute the error-wise recall under the binary setting. We assume e pred j = 0 if y pred = 0 for all error types e j ; that is, a prediction of a binary error counts as detecting an error of any type in that sentence. This overestimates the recall performance and can be viewed as an upper bound; a model that can only detect CharE may report nonzero recall for other errors if these co-occur with CharE.\n\nFor fair comparison between different models, we report category-wise recall for all models at the same precision level P = 0.7. Table 10 outlines our results. Both synthetic models report higher recall for the error category they were designed for. E.g., the coref-based method can detect CharE errors better than other error types. However, our FT models significantly outperform both synthetic approaches across all error types at thresholds with high precision performance. In particular, we observe high recall scores for CharE and SceneE.\n\nFine-grained prediction In Table 6, we compared human and model (FT w/ spans) performance on a modified test set created by combining annotations from 2 crowdworkers. This unfairly penalized the trained models, which may have slightly higher recall due to being trained on annotations from 3 crowdworkers. In Table 11, we report results on the original test set that combines annotations from all 3 annotators. Table 11: Performance of the T5-Large model finetuned on the SNAC dataset at predicting the correct error type in each summary sentence. We also report the percentage of times the predicted span overlaps with the error span in the gold data.\n\n\nD SNAC Error Types and Task Interface\n\nThe definitions of error types and illustrative examples provided to the crowdworkers during training are outlined here.\n\n\nD.1 CharE\n\nWe call these New Person not Introduced in the task interface. We provide the illustrative example show in Figure 9 along with the following definition:\n\n\"These refer to coherence errors where a new person is introduced into the narrative WITHOUT providing any background about the person, or their relation with other characters in the story. Note, however, that famous or well-known people do not need to be explicitly introduced.\"\n\n\nContext:\n\nJohn Fenwick, an aspiring artist, accepts a loan to move to London to pursue his art career.  \n\n\nD.2 RefE\n\nWe call these Missing Information about an Event/Object in the task interface. We provide the illustrative example show in Figure 10 along with the following definition:\n\n\"These refer to coherence errors where an event or object is mentioned for the first time, but the phrasing strongly implies some context is missing to understand this event/object and that it must have been introduced previously.\"  \n\n\nD.3 SceneE\n\nThese are called Abrupt Transition from the Previous Scene in the task interface. We provide the illustrative example show in Figure 11 along with the following definition:\n\n\"These refer to coherence errors where there is a sudden shift in the setting or the narrative in the story. These often happen in two scenarios:\n\n1. There is an abrupt change in the people/characters being discussed and/or an abrupt change in the surroundings/event.\n\n2. Scenarios where the previous scene's phrasing strongly implies that more information/events are forthcoming, but the previous scene gets abruptly cut off and a completely new scene starts.\n\nPlease choose full sentences as spans for this error type.\"\n\n\nContext:\n\nJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career.\n\nHe becomes infatuated with Madame de Pastourelles, a beautiful and intelligent artist.\n\n\nReasoning:\n\nHere, the scene suddenly shifts from the previous one (talking about Fenwick's infatuation), to a different scene where a character is threatened by a tramp. In this case, this entire next segment span should be selected, and annotated as 'Abrupt Scene Transition' Error.\n\n\nCurrent Segment:\n\nFenwick's wife becomes frightened when a tramp threatens to kill her and her child.  Figure 12 shows an example of Inconsistent error shown to annotators.\n\n\nD.4 InconE\n\n\"These refer to text spans that contradict previous content (either in the context or the next segment box itself.)\n\nNote: You will also be asked to highlight the 'previous' span that is contradictory to the selected span. Highlighting this previous span (from either the context or the next segment box itself) will populate the relevant input box automatically.\"\n\n\nContext:\n\nJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career.\n\nStep 1: Highlight the span in the Next Segment box that is inconsistent with earlier text.\n\nStep 2: Highlight the earlier span that is being contradicted. This will automatically populate the relevant text box.\n\n\nCurrent Segment:\n\nHe moves to Paris to set up his workshop.  Figure 13 shows an example of Unclear Coreference provided to annotators.\n\n\nD.5 CorefE\n\n\"These refer to errors where it is unclear who/what a pronoun or refers to.\" Current Segment:\n\nKendall and Greenlee go to Aiden's house the next evening. She rings the doorbell.\n\n'She' could be referring to either Kendall or Greenlee. This coreference is unclear. Figure 13: Illustration of CorefE errors provided to crowdworkers during training. Figure 14 shows an example of Repetition errors.\n\n\nD.6 RepE\n\n\"These refer to spans where content is repeated. Note: For these, you will also be asked to highlight the 'previous' span that contains the same text/content as the selected span. Highlighting this previous span (from either the context or the next segment box itself) will populate the relevant input box automatically.\"\n\n\nD.7 GramE\n\nThese are called Ungrammatical/Nonsensical in the interface.\n\n\"These refer to text spans that have grammar errors. Also included in this category are cases\n\nStep 1: Highlight the span in the Next Segment box that is repeated\n\nStep 2: Highlight the earlier span that is being repeated. This will automatically populate the relevant text box.\n\n\nCurrent Segment:\n\nFenwick is an aspiring artist who searches for work in London.\n\n\nContext:\n\nJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career. Figure 14: Illustration of RepE errors provided to crowd workers during training.\n\n\nContext\n\nJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career.\n\nStep 2: Choose Error Type e j\n\n\nCharE\n\nStep 1: Highlight Span t j\n\nStep 3: (only for InconE and RepE) Select antecedent from context/current segment.\n\nStep 4: Add to final set of annotations .  where there are obvious commonsense errors or the text does not make any sense at all.\"\n\n\nD.8 Task Interface\n\nIn Section 3.1, we described the annotation work for the SNAC framework. Figure 15 visually illustrates this overall workflow for annotating errors in segment S i . A screenshot of the actual task interface is shown in Figure 16. We also include screenshots of our task instructions. Figure 17 explains the basic task to the annotators. Figure 18 shows the detailed task workflow and the steps to annotate errors in a text segment. Figure 19 shows an example annotation with multiple coherence errors for reference.\n\n\nE Datasheet for SNAC E.1 Motivation for Dataset Creation\n\nWhy was the dataset created? Despite recent interest in long document summarization research and generation of long narrative summaries (Kry\u015bci\u0144ski et al., 2021;Zhang et al., 2022;Mao et al., 2021;Wu et al., 2021), we lack evaluation frameworks to compare these approaches and measure progress. Current automatic and human evaluation Who is supporting and maintaining the dataset? This dataset is maintained by authors of this paper.\n\n\nE.5 Legal and Ethical Considerations\n\nWere workers told what the dataset would be used for and did they consent? Crowdworkers were aware that their responses were being collected as part of a research study on analyzing coherence errors in narrative text. The Amazon Mechanical Turk Participation Agreement permits the use of their annotated responses for this work. We do not release any personal information, e.g. worker IDs, of the crowdworkers.\n\nIf it relates to people, could this dataset expose people to harm or legal action? No.\n\nIf it relates to people, does it unfairly advantage or disadvantage a particular social group? No. \n\nFigure 5 :\n5RefE errors identified by only one annotator.\n\nFigure 7 :\n7Performance of the different models on the SNAC test set. Models trained on SNAC outperform those trained on synthetically generated datasets.\n\n\nAutomatic metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019), and others have been used to evaluate summarization, but Fabbri et al. (2021)showed that these correlate poorly with summary quality. Human evaluation is widely considered the gold standard for generation tasks, however, recent work(Karpinska et al., 2021; Clark  et al., 2021)  demonstrated that humans are not reliable for evaluating strong models like GPT-3.\n\nFigure 8 :\n8Examples of open-ended story completion by the GPT-3 text-davinci-002 model. The coherence errors observed under this setting (chiefly repetition errors, in blue) have little or no overlap with those from the summarization setting. Therefore, error taxonomies like SCARECROW that are devised for open-ended generated are not applicable to the summarization task.\n\nFigure 9 :\n9Illustration of CharE errors provided to crowdworkers during training.\n\nFigure 10 :\n10Illustration of RefE errors provided to crowdworkers during training.\n\nFigure 11 :\n11Illustration of SceneE errors provided to crowdworkers during training.\n\nFigure 12 :\n12Illustration of InconE errors provided to crowdworkers during training.\n\nFigure 15 :\n15Workflow for annotating coherence errors in segment S i with respect to the context, i.e. S 0 , S 1 , ..., S i\u22121 .\n\nFigure 16 :Figure 17 : 462 Figure 18 :Figure 19 :\n16174621819Screenshot of the task interface for SNAC annotations Screenshot of the first page of the tutorial provided to crowd annotators Screenshot of the second page of the tutorial provided to crowd annotators Screenshot of the last page of the tutorial provided to crowd annotators\n\n\nIn court, Mr. Darnay is accused of treason. However, he is acquitted after his patriot friend, Roger Cly, testifies against him.Why restore to life? \nIs her father sick? \n\nWho is Mr. Darnay? \n\nExcerpt from Generated Summary \n\nWho is Mr. Lorry? \n\nSudden shift in scene \n(what happened to \nMiss Mannette?) \n\nEVENT UNCLEAR \n\nCHARACTER UNCLEAR \n\nSCENE TRANSITION \n\nCHARACTER UNCLEAR \n\nMiss Manette receives a letter from the bank \ninforming her that information about her \nfather's small property has been discovered. \nShe wants to travel to France to identify him \nand restore him to life. Mr. Lorry explains \nthat her father has been found under another \nname and is being held in a house in Paris. \n\nFive years later, in 1780, a young Frenchman, named Charles \nDarnay, is accused of being a traitor and a spy. Lucie [Mannette] \nand her father are reluctant witnesses for the prosecution [\u2026] \n\n\n\nIn Paris, he impresses Lord Findon with his work.Inconsistent \n(InconE) \n\nMissing \nreference to \nEvent/Object \n(RefE) \n\nAbrupt scene \ntransition \n(SceneE) \n\nNew \nCharacter not \nintroduced \n(CharE) \n\nOne afternoon, he writes a letter to Mrs. Morrison \nexpressing sympathy for her husband's suicide. \n\nUnnecessary \nRepetition \n(RepE) \n\nUngrammatical \n/Nonsensical \n(GramE) \n\nUnclear \nCoreference \n(CorefE) \n\nLanguage & Fluency Errors \n\nOne afternoon, Fenwick goes to lunch with Madam de \nPastourelles, a beautiful artist, and Eugenie, a wealthy \nbenefactor. She promises to help him help customers. \n\n\n\n\nTable 1: Summary-level agreement, measured by Krippendorff's \u03b1. * * Expert agreement after one round of annotations; this aligns with the crowd setting.News (Expert) News (Crowd) Books (Crowd) \n\n0.41  *  *  \n0.48 \n0.19 \n\n\n\n\nMiss Manette receives a letter from the bank informing her that information about her father's small property has been discovered. She wants to travel to France to identify him and restore him to life.Mr. Lorry explains that her father has been found under another name and is being held in a house in Paris.In court, Mr. Darnay is accused of treason. However, he is acquitted after his patriot friend, Roger Cly, testifies against him.Mr. Lorry visits the Doctor's house on a Sunday afternoon as he often does. Miss Pross, the housekeeper, worries that many people will come to the house to look for Ladybird.Suddenly, the Doctor starts to feel ill and says they should go inside.Charles Darnay, the Marquis' nephew, returns to France to pursue the sacred object that took him away. He tells the Marquis that he renounces his French property as it is full of misery.Charles has been in love with Lucie Manette for a long time but has never told her about his feelings.Stryver tells Lorry that he intends to marry Lucie for pragmatic reasons.CharE, v = 3 \n\nCharE, v = 3 \n\nCharE, v = 3 \n\nSceneE, v = 2 \nCharE, v = 3 \nInconE, v = 2 \n\nSceneE, v = 3 \nCharE, v = 3 \n\nCharE, v = 3 \n\nCharE, v = 3 \n\nSceneE, v = 2 \nRefE, v = 2 \n\nRefE, v = 2 \n\nNew character \nwithout introduction \n(CharE) \n\nInconsistent \n(InconE) \n\nMissing reference \nto object/event \n(RefE) \n\nAbrupt scene \ntransition \n(SceneE) \n\n\n\nTable 2 :\n2Statistics for expert and crowd annotations \nper level of granularity: span-, sentence-and segment-\nlevels. Span-level annotations are multi-class, sentence-\nand segment-level have binary labels of coherence. \n\n\n\nTable 4 :\n4Segment and sentence-level agreement, mea-\nsured by Krippendorff's \u03b1 for SNAC. Our dataset re-\nports higher inter-annotator agreement compared to \nnewswire summaries adapted to a similar setting. \n\nError \nKrippendorff's \u03b1 \nTwo-agree % \nExpert \nCrowd \nExpert \nCrowd \n\nCharE \n.91 \n.69 \n86 \n67 \nSceneE \n.57 \n.30 \n62 \n35 \nRefE .25 (.39) .10 (22) 27 (39) 11 (23) \nInconE .18 (.29) .13 (21) 20 (37) 14 (23) \n\n\n\nTable 5 :\n5Token-level agreement for errors in the coherence sub-category. For RefE and InconE, we also report agreement (in parentheses) after normalizing span boundaries for overlapping errors.\n\nTable 5\n5outlines the agreement: for both expert and crowdworkers, we see high agreement for CharE and fair agreement for SceneE. On the other hand, lower agreement is observed for RefE; this aligns with our observation that individual annotators may have low recall. Different annotators fundamentally have different notions of what extra information is critical for understanding the text.Similar overall results at the token-level are reported byDou et al. (2022)  for their error taxonomy: their error categories Commonsense and Encyclopedic report the lowest metrics, the two-agree Gabriel Oak leases a sheep farm and becomes infatuated with Bathsheba, a beautiful young woman. He asks her aunt for her hand in marriage, but she turns him down because she doesn't love him. Gabriel's reputation as a shepherd makes it difficult for him to find work, so he plays his flute to earn money. Bathsheba dismisses the bailiff for stealing and decides to manage the farm on her own.\n\n\nand test (1.8k) examples and evaluate on the test set.\n\n\nSYN) Coref-based This technique is designed to specifically target CharE and RefE errors. We run a coreference model (Lee et al., 2018) to extract coreferent chains in gold summaries. Let s i , s j>i be sentences with the first and second mention of Mr. Bingley meets the Bennet family at Netherfield Park. Jane, the eldest Bennett girl is attracted to him. Darcy starts to notice Elizabeth and asks her to marry him. S1 [SEP] S2 S1, S2 [SEP] S3 T5No CharE Darcy Elizabeth[sep] SceneEFanny is also displeased by the closeness between Edward, her brother, and Elinor, the elder Dashwood daughter. S4The Dashwood family is introduced.Mr. Dashwood's wife is left with little when he dies and the estate goes to his son, John Dashwood.John and his wife Fanny have a lot of money. Yet they refuse to help.Figure 6: Training data generation, and T5 inputs and outputs for the SYN and FT w/ span models. The FT w/o span model only generates yes/no and not the specific category or span.S1 \n\nS2 \n\nS3 \n\nSceneE \nCharE \nCharE \n\nYes \n\nT5 \n\nS1 \n\nS2 \n\nS3 \n\nCoref-based \n\nS1, S2 [SEP] S3 \nS1 [SEP] S3 \n\nNext-Sentence-based \n\nS1, S2 [SEP] S3 \nS1, S2 [SEP] S4 \n\nT5 \n\nT5 \n\nT5 \n\nT5 \n\nYes \n\nNo John \n\nYes \n\nNo \n\ncoreferent \n\nSYNTHETIC DATA \n\nFT w/ span \n\nReference Summary \n\nSNaC Summary \n\n\n\nTable 6 :\n6Comparison between FT w/ span model and humans. Humans have higher precision while trained models report better recall across the top 3 error types.\n\n\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72. Vanessa Wei Feng and Graeme Hirst. 2012. Extending the entity-based coherence model with multiple ranks. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315-324.Regina Barzilay and Mirella Lapata. 2005. Modeling \nlocal coherence: an entity-based approach. In Pro-\nceedings of the 43rd Annual Meeting on Association \nfor Computational Linguistics, pages 141-148. \n\nRegina Barzilay and Mirella Lapata. 2008. Modeling \nlocal coherence: An entity-based approach. Compu-\ntational Linguistics, 34(1):1-34. \n\nManik Bhandari, Pranav Narayan Gour, Atabak Ash-\nfaq, Pengfei Liu, and Graham Neubig. 2020. Re-\nevaluating evaluation in text summarization. In \nProceedings of the 2020 Conference on Empirical \nMethods in Natural Language Processing (EMNLP), \npages 9347-9359. \n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie \nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind \nNeelakantan, Pranav Shyam, Girish Sastry, Amanda \nAskell, et al. 2020. Language models are few-shot \nlearners. Advances in Neural Information Processing \nSystems, 33:1877-1901. \n\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin \nGimpel. 2022. SummScreen: A dataset for abstrac-\ntive screenplay summarization. In Proceedings of the \n60th Annual Meeting of the Association for Compu-\ntational Linguistics. \nElizabeth Clark, Tal August, Sofia Serrano, Nikita \nHaduong, Suchin Gururangan, and Noah A Smith. \n2021. All that's 'human'is not gold: Evaluating hu-\nman evaluation of generated text. In Proceedings \nof the 59th Annual Meeting of the Association for \nComputational Linguistics and the 11th International \nJoint Conference on Natural Language Processing \n(Volume 1: Long Papers), pages 7282-7296. \n\nYao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, \nNoah A Smith, and Yejin Choi. 2022. Is GPT-3 \nText Indistinguishable from Human Text? Scarecrow: \nA Framework for Scrutinizing Machine Text. In \nProceedings of the 60th Annual Meeting of the Asso-\nciation for Computational Linguistics. \n\nMicha Elsner and Eugene Charniak. 2011. Extend-\ning the entity grid with entity-specific features. In \nProceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 125-129. \n\nAlexander Richard Fabbri, Wojciech Kry\u015bci\u0144ski, Bryan \nMcCann, Caiming Xiong, Richard Socher, and \nDragomir Radev. 2021. SummEval: Re-evaluating \nSummarization Evaluation. Transactions of the Asso-\nciation for Computational Linguistics, 9:391-409. \n\nVanessa Wei Feng, Ziheng Lin, and Graeme Hirst. 2014. \nThe impact of deep hierarchical discourse structures \nin the evaluation of text coherence. In Proceedings of \nCOLING 2014, the 25th International Conference on \nComputational Linguistics: Technical Papers, pages \n940-949. \n\nBarbara J Grosz, Scott Weinstein, and Aravind K Joshi. \n1995. Centering: a framework for modeling the local \ncoherence of discourse. Computational Linguistics, \n21(2):203-225. \n\nCamille Guinaudeau and Michael Strube. 2013. Graph-\nbased local coherence modeling. In Proceedings \nof the 51st Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long Papers), \npages 93-103. \n\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng \nJi, and Lu Wang. 2021. Efficient Attentions for Long \nDocument Summarization. In Proceedings of the \n2021 Conference of the North American Chapter of \nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1419-1436. \n\nShafiq Joty, Muhammad Tasnim Mohiuddin, and \nDat Tien Nguyen. 2018. Coherence modeling of \nasynchronous conversations: A neural entity grid \napproach. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics \n(Volume 1: Long Papers), pages 558-568. \nMarzena Karpinska, Nader Akoury, and Mohit Iyyer. \n2021. The perils of using mechanical turk to evaluate \nopen-ended text generation. In Proceedings of the \n2021 Conference on Empirical Methods in Natural \nLanguage Processing, pages 1265-1285. \n\nWojciech Kry\u015bci\u0144ski, Nitish Shirish Keskar, Bryan Mc-\nCann, Caiming Xiong, and Richard Socher. 2019. \nNeural text summarization: A critical evaluation. In \nProceedings of the 2019 Conference on Empirical \nMethods in Natural Language Processing and the 9th \nInternational Joint Conference on Natural Language \nProcessing (EMNLP-IJCNLP), pages 540-551. \n\nWojciech Kry\u015bci\u0144ski, Nazneen Rajani, Divyansh Agar-\nwal, Caiming Xiong, and Dragomir Radev. 2021. \nBookSum: A collection of datasets for long-\nform narrative summarization. \narXiv preprint \narXiv:2105.08209. \n\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018. \nHigher-order coreference resolution with coarse-to-\nfine inference. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 2 (Short Papers), pages \n687-692. \n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan \nGhazvininejad, Abdelrahman Mohamed, Omer \nLevy, Veselin Stoyanov, and Luke Zettlemoyer. \n2020. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. In Proceedings of the 58th \nAnnual Meeting of the Association for Computational \nLinguistics, pages 7871-7880. \n\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text summariza-\ntion branches out, pages 74-81. \n\nZiheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Au-\ntomatically evaluating text coherence using discourse \nrelations. In Proceedings of the 49th Annual Meet-\ning of the Association for Computational Linguistics: \nHuman Language Technologies, pages 997-1006. \n\nZiming Mao, Chen Henry Wu, Ansong Ni, Yusen Zhang, \nRui Zhang, Tao Yu, Budhaditya Deb, Chenguang \nZhu, Ahmed H Awadallah, and Dragomir Radev. \n2021. DYLE: Dynamic Latent Extraction for Ab-\nstractive Long-Input Summarization. arXiv preprint \narXiv:2110.08168. \n\nMohsen Mesgar and Michael Strube. 2018. A neural \nlocal coherence model for text quality assessment. \nIn Proceedings of the 2018 conference on empirical \nmethods in natural language processing, pages 4328-\n4339. \n\nHan Cheol Moon, Muhammad Tasnim Mohiuddin, \nShafiq Joty, and Chi Xu. 2019. A unified neural \ncoherence model. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference \n\non Natural Language Processing (EMNLP-IJCNLP), \npages 2262-2272. \n\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, \nCaglar Gulcehre, and Bing Xiang. 2016. Abstrac-\ntive Text Summarization using Sequence-to-sequence \nRNNs and Beyond. In Proceedings of The 20th \nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280-290. \n\nShashi Narayan, Shay B Cohen, and Mirella Lapata. \n2018. Don't Give Me the Details, Just the Summary! \nTopic-Aware Convolutional Neural Networks for Ex-\ntreme Summarization. In Proceedings of the 2018 \nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797-1807. \n\nDat Tien Nguyen and Shafiq Joty. 2017. A neural local \ncoherence model. In Proceedings of the 55th Annual \nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1320-1330. \n\nBo Pang, Erik Nijkamp, Wojciech Kry\u015bci\u0144ski, Sil-\nvio Savarese, Yingbo Zhou, and Caiming Xiong. \n2022. Long Document Summarization with Top-\ndown and Bottom-up Inference. arXiv preprint \narXiv:2203.07586. \nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter \nLiu. 2020. PEGASUS: Pre-training with extracted \ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages \n11328-11339. PMLR. \n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. BERTScore: Evalu-\nating Text Generation with BERT. In International \nConference on Learning Representations. \n\nYusen Zhang, Ansong Ni, Ziming Mao, Chen Henry \nWu, Chenguang Zhu, Budhaditya Deb, Ahmed H \nAwadallah, Dragomir Radev, and Rui Zhang. 2022. \nSumm\u02c6N: A Multi-Stage Summarization Framework \nfor Long Input Dialogues and Documents. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics. \n\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia \nMutuma, Rahul Jha, Ahmed Hassan, Asli Celikyil-\nmaz, Yang Liu, Xipeng Qiu, et al. 2021. QMSum: \nA New Benchmark for Query-based Multi-domain \nMeeting Summarization. In Proceedings of the 2021 \nConference of the North American Chapter of the \nAssociation for Computational Linguistics: Human \nLanguage Technologies, pages 5905-5921. \n\n\n\nTable 7 :\n7ROUGE and BERTScore for BOOK-175B and \nseveral artificially corrupted versions. Results show that \nautomatic metrics fail to penalize coherence errors. \n\nMethod \n#train #dev F1 Acc. \n\nCoref-based \n6.0k \n920 \n.78 \n.77 \nNext-Sent \n3.8k \n880 \n.71 \n.74 \n\n\n\nTable 8 :\n8Dataset sizes and intrinsic performance of T5-\nLarge models trained on synthetic datasets. \n\nsure coherence, or even overall quality, for long \nsummaries. \n\nC Detecting Coherence Errors: Details \nand Additional Results \n\nC.1 Models trained on synthetic data (SYN) \n\n\n\nTable 8\n8shows the training data and development \ndata size, as well as the intrinsic performance of \nthese synthetic dataset-based coherence models \non this development set. We construct both our \ndatasets with an equal number of positive and nega-\ntive coherence examples. The results show that T5 \nlearns to model the synthetic task with reasonable \naccuracy. We do not expect the models to perform \nperfectly, as the synthetic data may have false pos-\nitives (examples constructed to exhibit errors that \nare actually coherent). \n\n\n\nTable 9\n9shows the hyperparameters used for finetuning the T5-Large models on both synthetic training datasets and SNAC.Computing Infrastructure 32GB NVIDIA V100 GPU \nMax Input Seq Length \n1024 \nMax Output Seq Length \n80 (for FT w/ span) \nOptimizer \nAdam \nOptimizer Params \n\u03b2 = (0.9, 0.999), \u03f5 = 10 \u22128 \nLearning Rate Decay \nLinear \nLearning rate \n1e-4 \nBatch size \n8 \nEpochs \n5 \n\n\n\nTable 9 :\n9Hyperparameters used for fine-tuning T5-Large on synthetic and SNAC train sets.Model CharE SceneE RefE InconE All \n\nCoref-based \n.61 \n.47 \n.48 \n.15 \n.43 \nNext-Sent \n.31 \n.35 \n.32 \n.09 \n.27 \n\nFT w/o span \n.89 \n.84 \n.64 \n.51 \n.73 \nFT w/ span \n.90 \n.82 \n.58 \n.47 \n.70 \n\n\n\nCurrent Segment :\nSegmentIn London, he impresses Lord Findon with his work.Reasoning:Here, a new person Lord Findon is introduced without explicitly stating who he is, or his connection to the other previous characters.On the other hand, if the sentence read \"Lord Findon, a wealthy benefactor\", then this would not be a coherence error.Current Segment:A data firm that worked on Trump's campaign is shutting down amid allegations that it misused Facebook data Trump is a well-known person, does not need to be explicitly introduced to maintain coherence.\n\nContext :\nContextJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career. Reasoning: Here, the event \"husband's suicide\" is incoherent with the context, where he is alive and provides the loan. The phrasing of the text implies that the readers are aware of the husband's suicide. On the other hand, if the sentence read \"One day, he hears about Mr. Morrison's suicide and writes a letter\u2026\" would be coherent. John Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career. He burns Mr. Morrison's letter and goes to visit galleries in London.Reasoning:Here, the object 'Mr. Morrison's letter' is not previously introduced, but is referred to familiarly. Therefore, it is marked incoherent.He writes a letter to Mrs. Morrison expressing sympathy for her husband's suicide.Context: \n\nCurrent Segment: \n\nCurrent Segment: \n\n\n\n\na = {S i , t j , e j } ARepeat until all errors in are annotated. Then, go to .In London, he impresses Lord Findon with his work.S i \nS i+1 \n\nCurrent Segment \n\nRefE \nSceneE \nInconE \n\n(S 0 S 1 . . . S i\u22121 ) \n\n(S i ) \n\nRepE \nCorefE \nGramE \n\n\nAll collected annotations and models released at: https: //github.com/tagoyal/snac.\nSumm\u02c6N is trained on TV episode screenplays. However, TV episodes are not self-contained narratives and often refer to events from previous episodes, making this an update summarization task which is harder to evaluate for coherence out of context. Therefore, we summarize movie scripts instead.\nWe special-cased this class of error because it was so frequent in our data. Our narratives are about fictional people in real-world settings, so places, organizations, and other named entity types are less likely to require explicit introduction.\nWe previously showed that annotators do not agree on overall summary ratings. However, this experiment differs in\nhttps://goodsystems.utexas.edu/\nhttps://creativecommons.org/licenses/by-sa/4. 0/legalcode\nmethods fail to identify gaps in narrative coherence and are not suited for evaluating long summaries. Our SNAC dataset and annotation framework releases a large-scale dataset of fine-grained coherence annotations and establishes a protocol for eliciting such annotations from crowdworkers. This provides a foundation for future research efforts in this area.Has the dataset been used already? At the time of submission, the dataset has only been used in the current paper for analysis of generation errors made by current state-of-the-art summarization models and for training automatic coherence detection models.Who funded the dataset? We withhold this information to maintain anonymity but will include it upon publication.E.2 Dataset CompositionWhat are the instances? Each instance in this dataset is a model generated summary from either the book or the movie domain. All summaries are in the English language.How many instances are there? Our dataset contains annotations for 160 generated summaries (including both expert and crowd annotations).What data does each instance consist of? Each instance contains multiple span-level highlights corresponding to coherence errors, each of which is tagged with a specific error category.Does the data rely on external sources? Yes. For the book datasets, we annotate summaries from the publicly available model outputs released byWu et al. (2021). For movies, we generate summaries using the Summ\u02c6N model (Zhang et al., 2022) on the publicly available TRIPOD dataset(Papalampidi et al., 2020).Are there recommended data splits or evaluation measures? We will include the recommended training, development, and test splits for our annotations with the dataset release. The statistics for the data splits are outlined in Section 5.E.2.1 Data Collection ProcessWho was involved in the collection process and what were their roles? For expert annotations, 3 authors of the paper with experience in engaging with model-generated text annotated 10 book summaries. To recruit crowd annotators, we launched a qualification task on Mechanical Turk. After this qualification, 11 workers were asked to annotate 150 summaries.How was the dataset collected? Given a generated summary, annotators were asked to select span highlights that correspond with coherence errors and categorize the type of that error. We provided all annotators with detailed instructions describing the task interface, error type definitions as well as the overall workflow.Over what time frame was the data collected? The dataset was collected over the months of March and April 2022.Does the dataset contain all possible instances?No, we only annotate narrative summaries from two summarization models on two domains (movies and books). Moreover, our dataset only contains English language summaries.If the dataset is a sample, then what is the population? The dataset is a subset of generated summaries produced by state-of-the-art summarization models on narratives like books or movie screenplays.E.3 Data PreprocessingWhat preprocessing/cleaning was done? We fix sentence and word boundaries for highlighted spans from crowd annotations.Was the raw data saved in addition to the cleaned data? Yes.Does this dataset collection/preprocessing procedure achieve the initial motivation? Yes. This dataset serves as a large-scale collection of annotated coherence errors and provides the first characterization of such errors in long narrative summaries.E.4 Dataset DistributionHow is the dataset distributed? Our dataset is publicly released at this link: https://github. com/tagoyal/snac.When was it released? The dataset was released in October, 2022.What license (if any) is it distributed under?The dataset is released under the CC BY-SA 4.0 license. 7\n. Chare, 86 .74 .80 .99CharE .86 .74 .80 .99\n\n. Scenee, 58 .49 .53 1.0SceneE .58 .49 .53 1.0\n\n. Incone, 25 .01 .02 0.0InconE .25 .01 .02 0.0\n", "annotations": {"author": "[{\"end\":149,\"start\":82},{\"end\":227,\"start\":150},{\"end\":274,\"start\":228}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":88},{\"end\":164,\"start\":162},{\"end\":240,\"start\":233}]", "author_first_name": "[{\"end\":87,\"start\":82},{\"end\":155,\"start\":150},{\"end\":161,\"start\":156},{\"end\":232,\"start\":228}]", "author_affiliation": "[{\"end\":148,\"start\":117},{\"end\":226,\"start\":166},{\"end\":273,\"start\":242}]", "title": "[{\"end\":60,\"start\":1},{\"end\":334,\"start\":275}]", "venue": "[{\"end\":422,\"start\":336}]", "abstract": "[{\"end\":1762,\"start\":514}]", "bib_ref": "[{\"end\":1843,\"start\":1823},{\"end\":1862,\"start\":1843},{\"end\":1881,\"start\":1862},{\"end\":2077,\"start\":2052},{\"end\":2096,\"start\":2077},{\"end\":2115,\"start\":2096},{\"end\":2131,\"start\":2115},{\"end\":2258,\"start\":2234},{\"end\":2279,\"start\":2258},{\"end\":2456,\"start\":2439},{\"end\":2894,\"start\":2874},{\"end\":3448,\"start\":3432},{\"end\":4255,\"start\":4230},{\"end\":4274,\"start\":4255},{\"end\":5204,\"start\":5187},{\"end\":5223,\"start\":5204},{\"end\":5278,\"start\":5259},{\"end\":5314,\"start\":5289},{\"end\":5535,\"start\":5517},{\"end\":5792,\"start\":5773},{\"end\":5810,\"start\":5792},{\"end\":6536,\"start\":6519},{\"end\":6768,\"start\":6742},{\"end\":6823,\"start\":6803},{\"end\":6986,\"start\":6961},{\"end\":7008,\"start\":6986},{\"end\":7028,\"start\":7008},{\"end\":7096,\"start\":7072},{\"end\":7117,\"start\":7096},{\"end\":7589,\"start\":7568},{\"end\":8030,\"start\":8009},{\"end\":8326,\"start\":8306},{\"end\":19430,\"start\":19402},{\"end\":20489,\"start\":20472},{\"end\":22501,\"start\":22500},{\"end\":22572,\"start\":22555},{\"end\":23657,\"start\":23638},{\"end\":23675,\"start\":23657},{\"end\":23836,\"start\":23811},{\"end\":24306,\"start\":24293},{\"end\":24325,\"start\":24306},{\"end\":25689,\"start\":25670},{\"end\":29805,\"start\":29785},{\"end\":29819,\"start\":29807},{\"end\":29838,\"start\":29819},{\"end\":30047,\"start\":30029},{\"end\":30094,\"start\":30072},{\"end\":30112,\"start\":30094},{\"end\":30171,\"start\":30142},{\"end\":30193,\"start\":30171},{\"end\":30211,\"start\":30193},{\"end\":30417,\"start\":30398},{\"end\":31923,\"start\":31903},{\"end\":31969,\"start\":31949},{\"end\":33578,\"start\":33560},{\"end\":33757,\"start\":33738},{\"end\":33776,\"start\":33757},{\"end\":33800,\"start\":33776},{\"end\":33817,\"start\":33800},{\"end\":33835,\"start\":33817},{\"end\":34095,\"start\":34075},{\"end\":52770,\"start\":52745},{\"end\":52789,\"start\":52770},{\"end\":52806,\"start\":52789},{\"end\":52822,\"start\":52806}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53741,\"start\":53683},{\"attributes\":{\"id\":\"fig_1\"},\"end\":53897,\"start\":53742},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54403,\"start\":53898},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54779,\"start\":54404},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54863,\"start\":54780},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54948,\"start\":54864},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55035,\"start\":54949},{\"attributes\":{\"id\":\"fig_7\"},\"end\":55122,\"start\":55036},{\"attributes\":{\"id\":\"fig_8\"},\"end\":55252,\"start\":55123},{\"attributes\":{\"id\":\"fig_9\"},\"end\":55590,\"start\":55253},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":56484,\"start\":55591},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":57086,\"start\":56485},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":57310,\"start\":57087},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":58702,\"start\":57311},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":58926,\"start\":58703},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":59342,\"start\":58927},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":59539,\"start\":59343},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":60520,\"start\":59540},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":60577,\"start\":60521},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":61850,\"start\":60578},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":62011,\"start\":61851},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":70854,\"start\":62012},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":71118,\"start\":70855},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":71397,\"start\":71119},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":71934,\"start\":71398},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":72316,\"start\":71935},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":72596,\"start\":72317},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":73153,\"start\":72597},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":74076,\"start\":73154},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":74318,\"start\":74077}]", "paragraph": "[{\"end\":2579,\"start\":1778},{\"end\":2718,\"start\":2581},{\"end\":3420,\"start\":2720},{\"end\":3706,\"start\":3422},{\"end\":4457,\"start\":3751},{\"end\":5011,\"start\":4459},{\"end\":5660,\"start\":5013},{\"end\":6075,\"start\":5662},{\"end\":6263,\"start\":6077},{\"end\":6439,\"start\":6296},{\"end\":6668,\"start\":6441},{\"end\":6892,\"start\":6690},{\"end\":7380,\"start\":6894},{\"end\":8225,\"start\":7424},{\"end\":8506,\"start\":8227},{\"end\":8783,\"start\":8538},{\"end\":9166,\"start\":8814},{\"end\":9676,\"start\":9168},{\"end\":9899,\"start\":9678},{\"end\":10154,\"start\":9918},{\"end\":10594,\"start\":10156},{\"end\":10976,\"start\":10615},{\"end\":11375,\"start\":10978},{\"end\":11829,\"start\":11377},{\"end\":11931,\"start\":11831},{\"end\":12188,\"start\":11933},{\"end\":12351,\"start\":12208},{\"end\":12527,\"start\":12353},{\"end\":12949,\"start\":12529},{\"end\":13049,\"start\":12969},{\"end\":13786,\"start\":13072},{\"end\":14552,\"start\":13808},{\"end\":15088,\"start\":14554},{\"end\":15268,\"start\":15090},{\"end\":15513,\"start\":15285},{\"end\":16083,\"start\":15515},{\"end\":17384,\"start\":16085},{\"end\":17780,\"start\":17408},{\"end\":18746,\"start\":17814},{\"end\":19513,\"start\":18875},{\"end\":19688,\"start\":19543},{\"end\":20290,\"start\":19690},{\"end\":21654,\"start\":20292},{\"end\":22107,\"start\":21685},{\"end\":22937,\"start\":22109},{\"end\":23444,\"start\":22939},{\"end\":23545,\"start\":23470},{\"end\":23967,\"start\":23547},{\"end\":24245,\"start\":23969},{\"end\":24896,\"start\":24247},{\"end\":25532,\"start\":24898},{\"end\":26070,\"start\":25534},{\"end\":26624,\"start\":26072},{\"end\":27221,\"start\":26636},{\"end\":28238,\"start\":27223},{\"end\":28600,\"start\":28240},{\"end\":28950,\"start\":28615},{\"end\":29718,\"start\":28952},{\"end\":30464,\"start\":29735},{\"end\":30851,\"start\":30506},{\"end\":31318,\"start\":30867},{\"end\":31607,\"start\":31320},{\"end\":31970,\"start\":31609},{\"end\":32531,\"start\":31990},{\"end\":33662,\"start\":32600},{\"end\":34287,\"start\":33701},{\"end\":34442,\"start\":34289},{\"end\":34589,\"start\":34444},{\"end\":35075,\"start\":34591},{\"end\":35215,\"start\":35077},{\"end\":35936,\"start\":35217},{\"end\":36178,\"start\":35964},{\"end\":37064,\"start\":36213},{\"end\":38195,\"start\":37066},{\"end\":38739,\"start\":38197},{\"end\":39027,\"start\":38767},{\"end\":39606,\"start\":39062},{\"end\":40517,\"start\":39608},{\"end\":41930,\"start\":40519},{\"end\":42122,\"start\":41958},{\"end\":42616,\"start\":42157},{\"end\":43695,\"start\":42618},{\"end\":44831,\"start\":43697},{\"end\":45228,\"start\":44887},{\"end\":45653,\"start\":45230},{\"end\":46199,\"start\":45655},{\"end\":46853,\"start\":46201},{\"end\":47015,\"start\":46895},{\"end\":47181,\"start\":47029},{\"end\":47462,\"start\":47183},{\"end\":47569,\"start\":47475},{\"end\":47751,\"start\":47582},{\"end\":47986,\"start\":47753},{\"end\":48173,\"start\":48001},{\"end\":48320,\"start\":48175},{\"end\":48442,\"start\":48322},{\"end\":48635,\"start\":48444},{\"end\":48696,\"start\":48637},{\"end\":48819,\"start\":48709},{\"end\":48907,\"start\":48821},{\"end\":49193,\"start\":48922},{\"end\":49368,\"start\":49214},{\"end\":49498,\"start\":49383},{\"end\":49747,\"start\":49500},{\"end\":49870,\"start\":49760},{\"end\":49962,\"start\":49872},{\"end\":50082,\"start\":49964},{\"end\":50219,\"start\":50103},{\"end\":50327,\"start\":50234},{\"end\":50411,\"start\":50329},{\"end\":50629,\"start\":50413},{\"end\":50963,\"start\":50642},{\"end\":51037,\"start\":50977},{\"end\":51132,\"start\":51039},{\"end\":51201,\"start\":51134},{\"end\":51317,\"start\":51203},{\"end\":51400,\"start\":51338},{\"end\":51605,\"start\":51413},{\"end\":51727,\"start\":51617},{\"end\":51758,\"start\":51729},{\"end\":51794,\"start\":51768},{\"end\":51878,\"start\":51796},{\"end\":52010,\"start\":51880},{\"end\":52548,\"start\":52033},{\"end\":53042,\"start\":52609},{\"end\":53493,\"start\":53083},{\"end\":53581,\"start\":53495},{\"end\":53682,\"start\":53583}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18874,\"start\":18782}]", "table_ref": "[{\"end\":7884,\"start\":7877},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":13567,\"start\":13560},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":13722,\"start\":13715},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":15512,\"start\":15505},{\"end\":17853,\"start\":17846},{\"end\":18123,\"start\":18116},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":20111,\"start\":20104},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":27813,\"start\":27806},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":35277,\"start\":35270},{\"end\":44618,\"start\":44610},{\"end\":45792,\"start\":45784},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":46235,\"start\":46228},{\"end\":46518,\"start\":46510},{\"end\":46620,\"start\":46612}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1776,\"start\":1764},{\"end\":3749,\"start\":3709},{\"attributes\":{\"n\":\"2\"},\"end\":6294,\"start\":6266},{\"end\":6688,\"start\":6671},{\"end\":7422,\"start\":7383},{\"attributes\":{\"n\":\"3\"},\"end\":8536,\"start\":8509},{\"attributes\":{\"n\":\"3.1\"},\"end\":8812,\"start\":8786},{\"attributes\":{\"n\":\"3.2\"},\"end\":9916,\"start\":9902},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":10613,\"start\":10597},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":12206,\"start\":12191},{\"attributes\":{\"n\":\"4\"},\"end\":12967,\"start\":12952},{\"attributes\":{\"n\":\"4.1\"},\"end\":13070,\"start\":13052},{\"attributes\":{\"n\":\"4.2\"},\"end\":13806,\"start\":13789},{\"attributes\":{\"n\":\"4.3\"},\"end\":15283,\"start\":15271},{\"end\":17406,\"start\":17387},{\"end\":17812,\"start\":17783},{\"end\":18781,\"start\":18749},{\"attributes\":{\"n\":\"4.4\"},\"end\":19541,\"start\":19516},{\"attributes\":{\"n\":\"5\"},\"end\":21683,\"start\":21657},{\"attributes\":{\"n\":\"5.1\"},\"end\":23468,\"start\":23447},{\"attributes\":{\"n\":\"5.2\"},\"end\":26634,\"start\":26627},{\"attributes\":{\"n\":\"6\"},\"end\":28613,\"start\":28603},{\"attributes\":{\"n\":\"7\"},\"end\":29733,\"start\":29721},{\"end\":30491,\"start\":30467},{\"attributes\":{\"n\":\"8\"},\"end\":30504,\"start\":30494},{\"attributes\":{\"n\":\"9\"},\"end\":30865,\"start\":30854},{\"end\":31988,\"start\":31973},{\"end\":32544,\"start\":32534},{\"end\":32598,\"start\":32547},{\"end\":33699,\"start\":33665},{\"end\":35962,\"start\":35939},{\"end\":36211,\"start\":36181},{\"end\":38765,\"start\":38742},{\"end\":39060,\"start\":39030},{\"end\":41956,\"start\":41933},{\"end\":42155,\"start\":42125},{\"end\":44860,\"start\":44834},{\"end\":44885,\"start\":44863},{\"end\":46893,\"start\":46856},{\"end\":47027,\"start\":47018},{\"end\":47473,\"start\":47465},{\"end\":47580,\"start\":47572},{\"end\":47999,\"start\":47989},{\"end\":48707,\"start\":48699},{\"end\":48920,\"start\":48910},{\"end\":49212,\"start\":49196},{\"end\":49381,\"start\":49371},{\"end\":49758,\"start\":49750},{\"end\":50101,\"start\":50085},{\"end\":50232,\"start\":50222},{\"end\":50640,\"start\":50632},{\"end\":50975,\"start\":50966},{\"end\":51336,\"start\":51320},{\"end\":51411,\"start\":51403},{\"end\":51615,\"start\":51608},{\"end\":51766,\"start\":51761},{\"end\":52031,\"start\":52013},{\"end\":52607,\"start\":52551},{\"end\":53081,\"start\":53045},{\"end\":53694,\"start\":53684},{\"end\":53753,\"start\":53743},{\"end\":54415,\"start\":54405},{\"end\":54791,\"start\":54781},{\"end\":54876,\"start\":54865},{\"end\":54961,\"start\":54950},{\"end\":55048,\"start\":55037},{\"end\":55135,\"start\":55124},{\"end\":55303,\"start\":55254},{\"end\":58713,\"start\":58704},{\"end\":58937,\"start\":58928},{\"end\":59353,\"start\":59344},{\"end\":59548,\"start\":59541},{\"end\":61861,\"start\":61852},{\"end\":70865,\"start\":70856},{\"end\":71129,\"start\":71120},{\"end\":71406,\"start\":71399},{\"end\":71943,\"start\":71936},{\"end\":72327,\"start\":72318},{\"end\":72615,\"start\":72598},{\"end\":73164,\"start\":73155}]", "table": "[{\"end\":56484,\"start\":55721},{\"end\":57086,\"start\":56536},{\"end\":57310,\"start\":57241},{\"end\":58702,\"start\":58355},{\"end\":58926,\"start\":58715},{\"end\":59342,\"start\":58939},{\"end\":61850,\"start\":61559},{\"end\":70854,\"start\":62521},{\"end\":71118,\"start\":70867},{\"end\":71397,\"start\":71131},{\"end\":71934,\"start\":71408},{\"end\":72316,\"start\":72056},{\"end\":72596,\"start\":72408},{\"end\":74076,\"start\":74027},{\"end\":74318,\"start\":74208}]", "figure_caption": "[{\"end\":53741,\"start\":53696},{\"end\":53897,\"start\":53755},{\"end\":54403,\"start\":53900},{\"end\":54779,\"start\":54417},{\"end\":54863,\"start\":54793},{\"end\":54948,\"start\":54879},{\"end\":55035,\"start\":54964},{\"end\":55122,\"start\":55051},{\"end\":55252,\"start\":55138},{\"end\":55590,\"start\":55315},{\"end\":55721,\"start\":55593},{\"end\":56536,\"start\":56487},{\"end\":57241,\"start\":57089},{\"end\":58355,\"start\":57313},{\"end\":59539,\"start\":59355},{\"end\":60520,\"start\":59550},{\"end\":60577,\"start\":60523},{\"end\":61559,\"start\":60580},{\"end\":62011,\"start\":61863},{\"end\":62521,\"start\":62014},{\"end\":72056,\"start\":71945},{\"end\":72408,\"start\":72329},{\"end\":73153,\"start\":72623},{\"end\":74027,\"start\":73172},{\"end\":74208,\"start\":74079}]", "figure_ref": "[{\"end\":3686,\"start\":3678},{\"end\":9675,\"start\":9667},{\"end\":11260,\"start\":11252},{\"end\":11486,\"start\":11478},{\"end\":14010,\"start\":14002},{\"end\":15576,\"start\":15568},{\"end\":16485,\"start\":16477},{\"end\":16591,\"start\":16583},{\"end\":16704,\"start\":16696},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19166,\"start\":19158},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21025,\"start\":21017},{\"end\":25205,\"start\":25197},{\"end\":26007,\"start\":25999},{\"end\":26454,\"start\":26446},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26681,\"start\":26673},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32885,\"start\":32877},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":47144,\"start\":47136},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":47714,\"start\":47705},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":48136,\"start\":48127},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":49308,\"start\":49299},{\"end\":50155,\"start\":50146},{\"end\":50507,\"start\":50498},{\"end\":50590,\"start\":50581},{\"end\":51533,\"start\":51524},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52115,\"start\":52106},{\"end\":52261,\"start\":52252},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":52326,\"start\":52317},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":52379,\"start\":52370},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":52474,\"start\":52465}]", "bib_author_first_name": null, "bib_author_last_name": "[{\"end\":78931,\"start\":78926},{\"end\":78978,\"start\":78972},{\"end\":79026,\"start\":79020}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":78968,\"start\":78924},{\"attributes\":{\"id\":\"b1\"},\"end\":79016,\"start\":78970},{\"attributes\":{\"id\":\"b2\"},\"end\":79064,\"start\":79018}]", "bib_title": null, "bib_author": "[{\"end\":78933,\"start\":78926},{\"end\":78980,\"start\":78972},{\"end\":79028,\"start\":79020}]", "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}