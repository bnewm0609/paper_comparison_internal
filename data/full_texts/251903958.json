{"id": 251903958, "updated": "2023-10-05 12:59:46.094", "metadata": {"title": "CH-MARL: A Multimodal Benchmark for Cooperative, Heterogeneous Multi-Agent Reinforcement Learning", "authors": "[{\"first\":\"Vasu\",\"last\":\"Sharma\",\"middle\":[]},{\"first\":\"Prasoon\",\"last\":\"Goyal\",\"middle\":[]},{\"first\":\"Kaixiang\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Govind\",\"last\":\"Thattai\",\"middle\":[]},{\"first\":\"Qiaozi\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Gaurav\",\"last\":\"Sukhatme\",\"middle\":[\"S.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We propose a multimodal (vision-and-language) benchmark for cooperative and heterogeneous multi-agent learning. We introduce a benchmark multimodal dataset with tasks involving collaboration between multiple simulated heterogeneous robots in a rich multi-room home environment. We provide an integrated learning framework, multimodal implementations of state-of-the-art multi-agent reinforcement learning techniques, and a consistent evaluation protocol. Our experiments investigate the impact of different modalities on multi-agent learning performance. We also introduce a simple message passing method between agents. The results suggest that multimodality introduces unique challenges for cooperative multi-agent learning and there is significant room for advancing multi-agent reinforcement learning methods in such settings.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2208.13626", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2208-13626", "doi": "10.48550/arxiv.2208.13626"}}, "content": {"source": {"pdf_hash": "b99f1622420314f20b9b63a8f6dd337b31d8e76d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2208.13626v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1d224ffca3869b26c2f71ed0de00dd92fb61f7cf", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b99f1622420314f20b9b63a8f6dd337b31d8e76d.txt", "contents": "\nCH-MARL: A Multimodal Benchmark for Cooperative, Heterogeneous Multi-Agent Reinforcement Learning\n\n\nPrasoon Goyal \nAlexa AI\nAmazon Inc\n\n\nVasu Sharma \nAlexa AI\nAmazon Inc\n\n\nKaixiang Lin \nAlexa AI\nAmazon Inc\n\n\nGovind Thattai \nAlexa AI\nAmazon Inc\n\n\nQiaozi Gao \nAlexa AI\nAmazon Inc\n\n\nGaurav S Sukhatme \nAlexa AI\nAmazon Inc\n\n\nUniversity of Southern California\n\n\nCH-MARL: A Multimodal Benchmark for Cooperative, Heterogeneous Multi-Agent Reinforcement Learning\n\nWe propose a multimodal (vision-and-language) benchmark for cooperative and heterogeneous multi-agent learning. We introduce a benchmark multimodal dataset with tasks involving collaboration between multiple simulated heterogeneous robots in a rich multi-room home environment. We provide an integrated learning framework, multimodal implementations of state-of-the-art multi-agent reinforcement learning techniques, and a consistent evaluation protocol. Our experiments investigate the impact of different modalities on multi-agent learning performance. We also introduce a simple message passing method between agents. The results suggest that multimodality introduces unique challenges for cooperative multi-agent learning and there is significant room for advancing multi-agent reinforcement learning methods in such settings.\n\nIntroduction\n\nWe posit that progress in multi-agent learning and its application to multi-robot problems could be sped up with the introduction of standard, sophisticated environments for training and evaluation. Prior work on cooperative multi-agent learning has focused on simplified environments [16]. Visually rich environments that support multi-agent, cooperative tasks have not been explored until very recently [24,8,9,18,23]. We propose the first multimodal benchmark on Cooperative Heterogeneous Multi-Agent Reinforcement Learning (CH-MARL) wherein two simulated robots must collaboratively find an object and place it at a target location.\n\nCH-MARL is built using visually rich scenes from VirtualHome [18], and includes language. We implement a language generator that procedurally provides feedback to guide embodied agents to achieve tasks. In addition to providing a novel large-scale vision and language dataset for collaborative task completion in simulated household environments, we conduct a comprehensive evaluation of several state of the art MARL algorithms under various setting for our collaborative robot benchmark task. We investigate and analyze the impact of various aspects of the collaborative MARL algorithms, including heterogeneity and multi-modality. We also propose and implement a message passing interface between agents to enable effective information sharing, especially in decentralized model setups where they would otherwise not have the ability to collaborate with each other. The results reveal interesting insights: 2. Vision and language grounding helps the learning process 3. Even simple multi-agent communication protocols substantially improve task performance by allowing effective collaboration.\n\nTo our knowledge, this is the first dataset to support multiple heterogeneous agents in a virtual environment collaboratively completing a specified task. A comparative study of state of the art embodied AI datasets is in Table 1. We expect this work to contribute towards a standard multi-modal testbed for MARL and foster research in this area. \u00d7 \u00d7 \u00d7 House3D [25] \u00d7 \u00d7 \u00d7 iGibson [13] \u00d7 \u00d7 Watch and Help [19] \u00d7 \u00d7 CH-MARL (Ours)\n\n\nRelated Work\n\nCollaborative multi-agent RL is well-studied problem. For brevity we only mention the most relevant work here. Kurenkov et al [12] consider the object finding problem, where the target object is described using natural language. However, their focus is to exploit semantic priors about object placements (e.g. cheese is likely to be found in a fridge, which in turn is likely to be in the kitchen). In our setup, objects are not placed according to semantic priors, since our focus is multi-agent collaboration to search for and move objects efficiently. Jain et al ( [9], [8]) and Nachum et al [17] propose a setting where the agents must perform certain actions synchronously, e.g., for lifting a heavy object. This is different from our setting, which doesn't require synchronous actions; rather, the agents need to communicate with each other to explore the environment efficiently. These prior works assume homogeneous agents, whereas we consider heterogeneous agents. Zhu et al [26] study object finding an object in a multi-agent setup, but unlike us, their setting does not involve interactions with the environment, and the agents are homogeneous. Liu et al among others ([15], [14], [6], [5])) consider the problem of collaborative perception, where there are some degraded agents, and the goal is to learn an efficient communication strategy to improve the observation of the degraded agents. Unlike our setting, their agents are fixed and there is no interaction (navigation or manipulation) with the environment. Baker et al [2] train agents for multi-agent hide-and-seek, where the (homogeneous) agents can navigate and interact in the environment.\n\nWatch and Help [19] by Puig et al. is also a multi agent task completion setup built on top of the Virtual Home [18] environment where multiple agents try to complete a human specified task. However, this dataset doesn't have support for heterogeneous agents and doesn't include natural language feedback. IQUAD V1 [4] by Gordon et al is built over the AI2-THOR [11] environment and is a embodied QA dataset with support for multiple agents. The agents are tasked to navigate the environment to answer a question based on it. Again, there is no support for heterogeneous agents or natural language feedback in this dataset.\n\n\nProposed Benchmark\n\n\nProblem Setting\n\nOur setting consists of two simulated robots with different capabilities situated in a home environment. The robots need to cooperate to efficiently complete a task described in natural language. The first robot is a simulated humanoid, while the second is a drone. The humanoid (H) has an egocentric field of view, and can physically interact with objects in the environment, while the drone (D) has a top-down view of a part of the environment, but cannot physically interact with objects. Given a task described in natural language, such as \"Put a glass on the desk\", the goal is to complete it in as few steps as possible. In order to complete the task, the robots need to find the objects of interest (i.e. glass and desk), and the humanoid needs to perform pick-and-place operations to accomplish the desired configuration. Effective inter-robot cooperation is required for efficiency; the drone with its larger field of view can explore the environment more effectively, while only the humanoid can interact with objects. Thus, by combining their respective strengths, the two robots can complete the given tasks efficiently. Environment We use VirtualHome [18], a Unity-based environment designed for embodied multi-agent collaborative tasks. VirtualHome consists of 7 different scenes; each scene contains multiple rooms. The environment allows initializing objects at different locations in the scene, which can be used to generate various configurations of object placements. We simulate the drone by an overhead camera attached to an invisible agent in VirtualHome. Examples of observations of collaboration between ground agent and drone are shown in Table 3 Tasks Tasks in the environment involve placing a graspable object on a receptacle object. There are 45 graspable objects and 16 receptacle objects. For each task, a graspable object, initial receptacle object, and target receptacle object are randomly sampled. The graspable object is initialized at the initial receptacle object, and the goal is to move it to the target receptacle object. The task is described using natural language, such as \"Put the graspable object on the receptacle object .\" State space VirtualHome provides both scene graph and visual representations. Hence, our benchmark consists of both -the scene graph representation that circumvents the object recognition problem (allows focus on multi-agent cooperation), with lower compute requirements, while the visual representation requires object recognition in addition to developing the multi-agent cooperation algorithms, and is closer to the real world. Each robot receives a local observation at every time step (humanoid: egocentric view of the environment and drone: top-down view of a part of the environment, depending on its current location). In the visual setting, the observations consist of RGB frames for each robot. In the scene graph setting, the observation consists of a graph where the nodes are all the objects present in the visual observation of the robot, and edges describe the relationships between them. For example a coffee mug placed on the dining table will be represented in the scene graph by the nodes \"coffee mug\" and \"table\" and the edge between them for the relationship \"on\". Action space To make the setting amenable to reinforcement learning, the action space consists of high-level navigation and manipulation actions, as well as low-level navigation actions. \n\n\nReward\n\nThe reward for taking action at state is defined in terms of a potential function, ( , , ) = ( ) \u2212 ( ), where the potential function (\u00b7) is defined as follows:\n( ) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3\n\nImplementation Details\n\nTrajectory Generation using Planner To create a dataset for offline training, we implement a planner, that given a task, finds a trajectory to complete the task, using privileged information. For instance, if the object of interest is visible to the drone, the humanoid is directed to the location of the object. Using the planner, we generate 6,100 trajectories, which we divide into training, validation, and test splits (subsection 3.3). Language Data We generate 100 task descriptions using a single template, and 100 feedback language instructions using 2 templates. We use Amazon Mechanical Turk to obtain 1 paraphrase for each description and feedback item, from which we generate additional templates and extract synonyms for objects. The resulting natural language descriptions and feedback have 183 unique words, and a mean sentence length of 14.04 words. See Table 2 for example task descriptions/feedback. \n\n\nEvaluation Protocol\n\nSplits The VirtualHome environment has 7 scenes. The positions of objects can be modified to create different configurations. We create 6000 tasks across scenes 1-5, which are split into 5,500 training, 250 validation-seen and 250 test-seen tasks. 50 tasks are created for scenes 6 and 7 each, which are used as validation-unseen and test-unseen respectively. Evaluation Metrics We compare approaches based on two evaluation metrics -the success rate of completing tasks, and the episode length for successful completion. These two metrics can be used to compute the path-length-weighted (PLW) score [1] = * /max( , * ) , where is 1 if the task was successfully completed, and 0 otherwise, is the number of step taken by the approach to complete the task, and * is the optimal number of steps to complete the task, which is estimated as the number of steps in the trajectory generated by the planner (subsection 3.2). The final score of the algorithm is computed as the average path-length-weighted score across all tasks in the test-unseen split. Input settings We experiment with 2 variants -scene graph representation and visual representation (subsection 3.1), and present results for both the settings (section 4). Table 3: Trajectory snippets demonstrating inter-robot collaboration. The task description is \"Grab the toy and place it on the bed\". In step (a) The ground agent has located the toy while the drone goes and locates the bed. It conveys this information to the drone agent allowing it to quickly navigate to the bed as seen in step (b) and finally in step (c) the ground agent completes the mission by placing the toy on it.\n\nStep Drone view ground agent view (a) t=0\n(b) t=3\n(c) t=5 Table 4: Drone and ground robot view examples from the dataset. The message vector encodes which room the object of interest or the target receptacle is in, once either robot finds it. The state value encodes progress in the task (higher is better). In the first row, the language instruction is \"Pick up the whipped cream\", the corresponding message is [0, 1, 0, 0], and the state value ( ) = 2. In the second row the instruction is \"Place it on the bed\", the corresponding message is [0, 0, 0, 1], with state value ( ) = 6.\n\nDrone view Ground robot view\n\n\nExperiments\n\nWe benchmark several approaches on our proposed problem setting, including behavior cloning and several state-of-the-art multi-agent RL algorithms. Our model architectures are shown in Figure 1.\n\n\nFeature Extractors\n\nVisual Observation. The input image is passed through a pretrained ResNet-18 network ( [7]), and the feature vector from the pre-final layer is further projected to a 512dimensional vector using a linear layer, which is used as the visual representation of the scene. Scene Graph Observation. The object class (e.g. bed, table, etc.) and the state (e.g. open, closed, etc.) of each node in the input scene graph is first encoded into vectors using an object class embedding layer and a state embedding layer respectively. These vectors are concatenated to obtain a vector representation for each node. We then apply a Graph Convolution Layer ( [10]) to obtain contextualized embeddings for each node, which are aggregated using a mean-pooling operation to obtain the final vector representation of the input scene graph.\n\n\nMessage Passing\n\nA key component of a cooperative multi-agent setup is for agents to communicate effectively. We propose a message passing method which allows agents to share information with each other. A message is a shared state between the two agents. It is a binary vector of length equal to the number of rooms. Each bit of the vector corresponding to the respective room is set to 1 if either of the agents identify the object of interest to be in that room. We use two such messages, one each for the object of interest and the target receptacle. The messages are used exclusively in the decentralized setups to allow agents to share information. An example simulation of how the agents communicate is shown in Table 3   IQL: Independent Q-learning trains the Q-function of each agent on its history of local observations. VDN: Value Decomposition Network decomposes the joint Q-function into a sum of Q-functions of the individual agents, and trains each agent on its own Q-function using DQN loss. QMIX: extends VDN by relaxing the decomposition of the joint Q-function to be any monotonic function of the individual Q-functions, and trains as in VDN. QTRAN: extends both VDN and QMIX by transforming the joint Q-function into an alternate that is expected to be easier to factorize. We base our implementation on [20], [3] and extend it for explicit message-passing (subsection 4.1).\n\n\nResults\n\nOur results (Table 5 and Table 6) show that:\n\n(a) The message passing-based decentralized models (labelled (ours)) are significantly better than their non message-passing based versions. Decentralized behaviorcloning performs best in seen environments in PLW score, both in the visual and scene-graph representations. Both highlight the importance of effective communication (b) Each of the RL models perform better on unseen test and validation environments (best PLW 20.05) compared to behavior cloning methods (best PLW 13.72), demonstrating their ability to generalize to newer and unseen environments (c) The visual observation (best test-unseen PLW score: 6.45) is significantly harder than scene graph observation (best test-unseen PLW score: 20.05). A potential reason for this could be the need for better representing visual features using a feature extractor trained on in-domain data. (d) All the existing algorithms achieve a relatively low PLW score on the unseen splits, suggesting CH-MARL could spur creation of new algorithms/models.\n\n\nAnalysis of Natural Language Feedback\n\nTo understand whether the language feedback is helpful, we run an ablation experiment where the language feedback is turned off. We choose QMIX as the algorithm and study four different input settings including visual input, visual input with feedback, scene graph input, scene graph input with feedback. The results of this experiment and presented in Table 7. We can clearly observe that language feedback provides 10% relative improvement over the baselines without language feedback for both visual and scene graph inputs demonstrating the necessity of the natural language feedback.\n\n\nConclusions\n\nWe proposed CH-MARL, a new multimodal, multiagent, cooperative learning benchmark, with heterogeneous agents -a simulated humanoid with an egocentric field of view that can interact with objects in the environment, and a simulated drone with a larger field of view that cannot physically interact with the environment. We create tasks that require effective collaboration between agents. We introduce a simple message passing-based communication interface to allow efficient collaboration between agents, leading to significant performance gains over the non communicative baselines, highlighting the need for better communication between the agents to improve task success. We benchmark existing algorithms on the proposed problem; there is significant room for improvement in a multimodal setup to solve tasks effectively by developing new algorithms that leverage the strengths of each agent, and learn an efficient cooperative policy.\n\n\nFuture Work\n\nFor future work, we plan to extensively study better communication methods between the agents to allow for more effective collaboration between them. We plan to explore both dialog and symbolic communication techniques Our setup allows for addition of more robots with different capabilities allowing us to explore how the agent dynamics and communication patterns change as more agents with varying abilities are introduced to the environment to accomplish a task. Our testbed also enables the exploration of advanced multimodal models which could better leverage the rich multimodal information the environment provides.\n\n-\nHigh-level navigation actions: These are of the form Goto [ROOM], where ROOM is one of the rooms in the scene (i.e. kitchen, bedroom, bathroom, livingroom). These actions are available to both the agents. -High-level manipulation actions: Only available to the humanoid robot, and consist of Pick and Place operations. To keep the action space small, we do not require specifying the argument for these actions. Instead, if the robot executes the Pick action when the graspable object of interest is in its view, or the Place action when the receptacle object of interest is in its view (and the robot is holding the graspable object), the actions lead to picking up the target graspable object, and placing the object on the target receptacle object, respectively. Otherwise, the action fails, and results in no change to the environment. -Low-level navigation actions: For the humanoid robot, these actions are Move Forward, Turn Left, and Turn Right, while for the drone, these actions are Move Forward, Move Backward, Move Left, and Move Right. -The Stay action: Both robots also have a Stay action, which results in no movement of either robot or interaction with the environment.\n\nFig. 1 :\n1Model architecture for Centralized (top) and Decentralized (bottom) approaches. As can be seen, the centralized model uses both the ground and drone agent observations as input and generates action predictions for both the agents simultaneously. In the decentralized approach, the model relies only on a single agent's observation while receiving information from the other agent in the form of a message and predicts the action for each agent separately.\n\n1 .\n1The multimodal (vision and language) setting presents an extra challenge for existing techniquesarXiv:2208.13626v1 [cs.AI] 26 Aug 2022 \n\n\nTable 1 :\n1Comparison between state of the art embodied AI simulators and their support for multi agent systemsDataset \nMulti agent Heterogeneous agents Language feedback \nAlfred [22] \n\u00d7 \n\u00d7 \nHabitat [21] \n\n\nTable 2 :\n2Task descriptions (top) and feedback (bottom). Take the lotion and keep it on top of the towel rack 4. Pick the coffee pot and put it onto the kitchen counter 5. Pick up the mobile phone and place it on top of the sofa 1. You should have placed the notes on the kitchen table rather than going to the bedroom 2. You didn't have to go to the living room, you should have placed the sports ball on the kitchen counter instead 3. You had to place the board game on the bed instead of moving forward 4. Rather than turning left, you should have placed the plate on the game box 5. Instead of staying, you should have placed the cooking pot on the kitchen counter1. Grab the washing scrub and keep it on the kitchencounter \n2. Place the wine bottle on top of the work table \n3. \n\nTable 5 :\n5Comparisonof algorithms with visual observation. (Ours) here refers to the \nalgorithms with our message passing interface \n\nAlgorithm \n\nSuccess rate \nValidation \nTest \nSeen \nUnseen \nSeen \nUnseen \nBC; decentralized \n37.21 \u00b1 2.40 \n5.36 \u00b1 1.50 \n42.02 \u00b1 1.16 \n3.00 \u00b1 1.00 \nBC; decentralized(ours) \n40.89 \u00b1 2.64 \n6.52 \u00b1 1.81 \n48.54 \u00b1 1.66 \n5.02 \u00b1 0.82 \nBC; centralized \n30.80 \u00b1 2.00 \n4.35 \u00b1 0.61 \n34.92 \u00b1 2.96 \n4.00 \u00b1 0.73 \nIQL \n4.05 \u00b1 0.50 \n2.01 \u00b1 0.71 \n8.42 \u00b1 1.12 \n2.33 \u00b1 0.76 \nIQL(ours) \n20.89 \u00b1 2.01 \n7.56 \u00b1 0.48 \n22.08 \u00b1 4.02 \n10.92 \u00b1 2.11 \nVDN \n0.83 \u00b1 0.11 \n0.67 \u00b1 0.18 \n2.94 \u00b1 0.96 \n0.00 \u00b1 0.00 \nVDN(ours) \n14.18 \u00b1 1.72 \n2.55 \u00b1 0.40 \n16.56 \u00b1 2.08 \n5.21 \u00b1 0.88 \nQMIX \n17.73 \u00b1 2.12 \n3.68 \u00b1 0.56 \n17.96 \u00b1 4.58 \n2.33 \u00b1 0.61 \nQMIX(ours) \n22.26 \u00b1 2.12 \n8.01 \u00b1 0.56 \n23.85 \u00b1 4.58 \n12.03 \u00b1 2.31 \nQTRAN \n1.26 \u00b1 0.23 \n2.01 \u00b1 0.29 \n2.52 \u00b1 0.72 \n0.33 \u00b1 0.08 \nQTRAN(ours) \n13.69 \u00b1 1.22 \n7.07 \u00b1 0.60 \n19.58 \u00b1 1.66 \n8.00 \u00b1 0.72 \n\n(a) \n\nAlgorithm \n\nPLW score \nValidation \nTest \nSeen \nUnseen \nSeen \nUnseen \nBC; decentralized \n30.34 \u00b1 2.02 \n4.84 \u00b1 0.78 \n35.43 \u00b1 1.99 \n2.09 \u00b1 0.81 \nBC; decentralized(ours) \n37.42 \u00b1 2.51 \n5.51 \u00b1 0.82 \n40.11 \u00b1 2.14 \n3.50 \u00b1 1.31 \nBC; centralized \n23.07 \u00b1 2.55 \n4.19 \u00b1 0.73 \n27.37 \u00b1 3.06 \n2.29 \u00b1 0.74 \nIQL \n2.68 \u00b1 0.97 \n1.79 \u00b1 0.51 \n6.31 \u00b1 1.32 \n1.73 \u00b1 0.21 \nIQL(ours) \n13.80 \u00b1 2.55 \n5.54 \u00b1 1.12 \n19.05 \u00b1 2.55 \n6.10 \u00b1 1.81 \nVDN \n0.60 \u00b1 0.09 \n0.51 \u00b1 0.14 \n2.37 \u00b1 0.81 \n0.00 \u00b1 0.00 \nVDN(ours) \n10.32 \u00b1 1.14 \n3.76 \u00b1 0.65 \n12.22 \u00b1 1.02 \n3.00 \u00b1 0.28 \nQMIX \n14.09 \u00b1 0.71 \n3.39 \u00b1 1.06 \n15.21 \u00b1 2.91 \n1.88 \u00b1 0.79 \nQMIX(ours) \n15.55 \u00b1 2.71 \n6.34 \u00b1 1.06 \n19.36 \u00b1 2.91 \n6.45 \u00b1 2.29 \nQTRAN \n1.09 \u00b1 0.12 \n1.45 \u00b1 0.72 \n2.45 \u00b1 0.31 \n0.33 \u00b1 0.08 \nQTRAN(ours) \n9.73 \u00b1 0.89 \n5.94 \u00b1 0.61 \n15.30 \u00b1 1.25 \n4.82 \u00b1 0.55 \n\n(b) \n\n\nTable 6 :\n6Comparison of algorithms with scene graph observation. (Ours) here refers to the algorithms with our message passing interface\n\nTable 7 :\n7Ablation experiments on QMIX Scene graph, with feedback 39.25 \u00b1 2.90 27.85 \u00b1 1.67 38.13 \u00b1 2.23 28.95 \u00b1 3.51 Scene graph; w/o feedback 36.54 \u00b1 2.76 25.73 \u00b1 1.43 36.23 \u00b1 3.70 15.33 \u00b1 4.62 Scene graph, with feedback 25.36 \u00b1 3.56 21.44 \u00b1 0.58 28.68 \u00b1 3.50 20.05 \u00b1 5.98 Scene graph; w/o feedback 11.93 \u00b1 4.04 16.80 \u00b1 1.20 12.41 \u00b1 6.10 11.95 \u00b1 3.36Algorithm \n\nSuccess rate \nValidation \nTest \nSeen \nUnseen \nSeen \nUnseen \nVisual, with feedback \n22.26 \u00b1 2.12 \n8.01 \u00b1 0.56 23.85 \u00b1 4.58 12.03 \u00b1 2.31 \nVisual; w/o feedback \n20.47 \u00b1 2.86 \n7.55 \u00b1 1.15 21.45 \u00b1 2.07 10.33 \u00b1 0.58 \n(a) \n\nAlgorithm \n\nPLW score \nValidation \nTest \nSeen \nUnseen \nSeen \nUnseen \nVisual, with feedback \n15.55 \u00b1 2.71 \n6.34 \u00b1 1.06 19.36 \u00b1 2.91 \n6.45 \u00b1 2.29 \nVisual; w/o feedback \n14.90 \u00b1 2.00 \n6.13 \u00b1 1.02 17.31 \u00b1 0.98 \n6.18 \u00b1 0.75 \n(b) \n\n\n, if X placed on Y 6, if X grasped, and Y visible to G 5, if X grasped, and Y visible to D 4, if X grasped, and Y not visible to either G or D 2, if X not grasped, and X visible to G 1, if X not grasped, and X visible to D 0, if X not grasped, and X not visible to either G or D where X is the object of interest, and Y is the target location. Language Feedback In addition to the reward, the robots might receive natural language feedback from the environment when they perform a suboptimal action. For instance, if the target object is visible to the humanoid, and it does not pick it up, the feedback may be \"You should have picked up the glass instead of going to the livingroom.\"\nAlgorithmsBehavior Cloning We use the (state, action) pairs in the dataset (subsection 3.2) to train policy networks using supervised learning, for both centralized and decentralized scenarios. The decentralized scenario uses message passing between agents. For each scenario, we experiment with both visual and scene graph representations, where the states are encoded using the feature extractor architectures (subsection 4.1), and the networks are trained end-to-end using an Adam optimizer. Decentralized RL Algorithms Next, we benchmark several state-of-the-art decentralized RL algorithms, described below.\nP Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.06757On evaluation of embodied navigation agents. arXiv preprintAnderson, P., Chang, A., Chaplot, D.S., Dosovitskiy, A., Gupta, S., Koltun, V., Kosecka, J., Malik, J., Mottaghi, R., Savva, M., et al.: On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757 (2018)\n\nMordatch, I.: Emergent tool use from multi-agent autocurricula. B Baker, I Kanitscheider, T Markov, Y Wu, G Powell, B Mcgrew, arXiv:1909.07528arXiv preprintBaker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., Mor- datch, I.: Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528 (2019)\n\nLearning to communicate with deep multi-agent reinforcement learning. J N Foerster, Y M Assael, N De Freitas, S Whiteson, arXiv:1605.06676arXiv preprintFoerster, J.N., Assael, Y.M., De Freitas, N., Whiteson, S.: Learning to communicate with deep multi-agent reinforcement learning. arXiv preprint arXiv:1605.06676 (2016)\n\nIqa: Visual question answering in interactive environments. D Gordon, A Kembhavi, M Rastegari, J Redmon, D Fox, A Farhadi, Computer Vision and Pattern Recognition (CVPR). Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., Farhadi, A.: Iqa: Visual question answering in interactive environments. In: Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on. IEEE (2018)\n\nS Gurumurthy, A Agarwal, V Sharma, M Lewis, K Sycara, Community regularization of visually-grounded dialog. International Conference on Autonomous Agents and Multiagent Systems(AAMAS 2019. Gurumurthy, S., Agarwal, A., Sharma, V., Lewis, M., Sycara, K.: Community reg- ularization of visually-grounded dialog. International Conference on Autonomous Agents and Multiagent Systems(AAMAS 2019) (2019)\n\nMind your language: Learning visually grounded dialog in a multi-agent setting. S Gurumurthy, A Agarwal, V Sharma, K P Sycara, Adaptive Learning Agents (ALA. 2018Gurumurthy, S., Agarwal, A., Sharma, V., Sycara, K.P.: Mind your language: Learning visually grounded dialog in a multi-agent setting. Adaptive Learning Agents (ALA) 2018 (2018)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHe, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778 (2016)\n\nA cordial sync: Going beyond marginal policies for multi-agent embodied tasks. U Jain, L Weihs, E Kolve, A Farhadi, S Lazebnik, A Kembhavi, A Schwing, European Conference on Computer Vision. SpringerJain, U., Weihs, L., Kolve, E., Farhadi, A., Lazebnik, S., Kembhavi, A., Schwing, A.: A cordial sync: Going beyond marginal policies for multi-agent embodied tasks. In: European Conference on Computer Vision, pp. 471-490. Springer (2020)\n\nTwo body problem: Collaborative visual task completion. U Jain, L Weihs, E Kolve, M Rastegari, S Lazebnik, A Farhadi, A G Schwing, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJain, U., Weihs, L., Kolve, E., Rastegari, M., Lazebnik, S., Farhadi, A., Schwing, A.G., Kembhavi, A.: Two body problem: Collaborative visual task completion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6689-6699 (2019)\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, M Welling, arXiv:1609.02907arXiv preprintKipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016)\n\nE Kolve, R Mottaghi, W Han, E Vanderbilt, L Weihs, A Herrasti, D Gordon, Y Zhu, A Gupta, A Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv. Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Gordon, D., Zhu, Y., Gupta, A., Farhadi, A.: AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv (2017)\n\nSemantic and geometric modeling with neural message passing in 3d scene graphs for hierarchical mechanical search. A Kurenkov, R Mart\u00edn-Mart\u00edn, J Ichnowski, K Goldberg, S Savarese, arXiv:2012.04060arXiv preprintKurenkov, A., Mart\u00edn-Mart\u00edn, R., Ichnowski, J., Goldberg, K., Savarese, S.: Se- mantic and geometric modeling with neural message passing in 3d scene graphs for hierarchical mechanical search. arXiv preprint arXiv:2012.04060 (2020)\n\n. C Li, F Xia, R Mart\u00edn-Mart\u00edn, M Lingelbach, S Srivastava, B Shen, K Vainio, C Gokmen, G Dharan, T Jain, A Kurenkov, K Liu, H Gweon, J Wu, L Fei-Fei, S Savarese, igibson 2.0: Object-centric simulation for robot learning of everyday household tasksLi, C., Xia, F., Mart\u00edn-Mart\u00edn, R., Lingelbach, M., Srivastava, S., Shen, B., Vainio, K., Gokmen, C., Dharan, G., Jain, T., Kurenkov, A., Liu, K., Gweon, H., Wu, J., Fei-Fei, L., Savarese, S.: igibson 2.0: Object-centric simulation for robot learning of everyday household tasks (2021)\n\nWhen2com: multi-agent perception via communication graph grouping. Y C Liu, J Tian, N Glaser, Z Kira, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLiu, Y.C., Tian, J., Glaser, N., Kira, Z.: When2com: multi-agent perception via communication graph grouping. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4106-4115 (2020)\n\nWho2com: Collaborative perception via learnable handshake communication. Y C Liu, J Tian, C Y Ma, N Glaser, C W Kuo, Z Kira, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEELiu, Y.C., Tian, J., Ma, C.Y., Glaser, N., Kuo, C.W., Kira, Z.: Who2com: Col- laborative perception via learnable handshake communication. In: 2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 6876-6883. IEEE (2020)\n\nMulti-agent actor-critic for mixed cooperative-competitive environments. R Lowe, Y Wu, A Tamar, J Harb, P Abbeel, I Mordatch, arXiv:1706.02275arXiv preprintLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., Mordatch, I.: Multi-agent actor-critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275 (2017)\n\nMulti-agent manipulation via locomotion using hierarchical sim2real. O Nachum, M Ahn, H Ponte, S Gu, V Kumar, arXiv:1908.05224arXiv preprintNachum, O., Ahn, M., Ponte, H., Gu, S., Kumar, V.: Multi-agent manipulation via locomotion using hierarchical sim2real. arXiv preprint arXiv:1908.05224 (2019)\n\nVirtualhome: Simulating household activities via programs. X Puig, K Ra, M Boben, J Li, T Wang, S Fidler, A Torralba, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPuig, X., Ra, K., Boben, M., Li, J., Wang, T., Fidler, S., Torralba, A.: Virtual- home: Simulating household activities via programs. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8494-8502 (2018)\n\nWatch-and-help: A challenge for social perception and human-{ai} collaboration. X Puig, T Shu, S Li, Z Wang, Y H Liao, J B Tenenbaum, S Fidler, A Torralba, International Conference on Learning Representations. Puig, X., Shu, T., Li, S., Wang, Z., Liao, Y.H., Tenenbaum, J.B., Fidler, S., Torralba, A.: Watch-and-help: A challenge for social perception and human-{ai} collabo- ration. In: International Conference on Learning Representations (2021). URL https://openreview.net/forum?id=w_7JMpGZRh0\n\nThe StarCraft Multi-Agent Challenge. M Samvelyan, T Rashid, C S De Witt, G Farquhar, N Nardelli, T G J Rudner, C M Hung, P H S Torr, J Foerster, S Whiteson, CoRR abs/1902.04043Samvelyan, M., Rashid, T., de Witt, C.S., Farquhar, G., Nardelli, N., Rudner, T.G.J., Hung, C.M., Torr, P.H.S., Foerster, J., Whiteson, S.: The StarCraft Multi-Agent Challenge. CoRR abs/1902.04043 (2019)\n\nM Savva, A Kadian, O Maksymets, Y Zhao, E W\u0133mans, B Jain, J Straub, J Liu, V Koltun, J Malik, D Parikh, D Batra, Habitat: A platform for embodied ai research. Savva, M., Kadian, A., Maksymets, O., Zhao, Y., W\u0133mans, E., Jain, B., Straub, J., Liu, J., Koltun, V., Malik, J., Parikh, D., Batra, D.: Habitat: A platform for embodied ai research (2019)\n\nALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., Zettle- moyer, L., Fox, D.: ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020). URL https://arxiv.org/abs/1912.01734\n\nMulti-agent embodied question answering in interactive environments. S Tan, W Xiang, H Liu, D Guo, F Sun, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerProceedings, Part XIII 16Tan, S., Xiang, W., Liu, H., Guo, D., Sun, F.: Multi-agent embodied question answering in interactive environments. In: Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIII 16, pp. 663-678. Springer (2020)\n\nH Wang, W Wang, X Zhu, J Dai, L Wang, arXiv:2107.01151Collaborative visual navigation. arXiv preprintWang, H., Wang, W., Zhu, X., Dai, J., Wang, L.: Collaborative visual navigation. arXiv preprint arXiv:2107.01151 (2021)\n\nBuilding generalizable agents with a realistic and rich 3d environment. Y Wu, Y Wu, G Gkioxari, Y Tian, arXiv:1801.02209arXiv preprintWu, Y., Wu, Y., Gkioxari, G., Tian, Y.: Building generalizable agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209 (2018)\n\nMain: A multi-agent indoor navigation benchmark for cooperative learning. F Zhu, S Hu, Y Zhang, H Hong, Y Zhu, X Chang, X Liang, Zhu, F., Hu, S., Zhang, Y., Hong, H., Zhu, Y., Chang, X., Liang, X.: Main: A multi-agent indoor navigation benchmark for cooperative learning (2021)\n", "annotations": {"author": "[{\"end\":137,\"start\":101},{\"end\":172,\"start\":138},{\"end\":208,\"start\":173},{\"end\":246,\"start\":209},{\"end\":280,\"start\":247},{\"end\":357,\"start\":281}]", "publisher": null, "author_last_name": "[{\"end\":114,\"start\":109},{\"end\":149,\"start\":143},{\"end\":185,\"start\":182},{\"end\":223,\"start\":216},{\"end\":257,\"start\":254},{\"end\":298,\"start\":290}]", "author_first_name": "[{\"end\":108,\"start\":101},{\"end\":142,\"start\":138},{\"end\":181,\"start\":173},{\"end\":215,\"start\":209},{\"end\":253,\"start\":247},{\"end\":287,\"start\":281},{\"end\":289,\"start\":288}]", "author_affiliation": "[{\"end\":136,\"start\":116},{\"end\":171,\"start\":151},{\"end\":207,\"start\":187},{\"end\":245,\"start\":225},{\"end\":279,\"start\":259},{\"end\":320,\"start\":300},{\"end\":356,\"start\":322}]", "title": "[{\"end\":98,\"start\":1},{\"end\":455,\"start\":358}]", "venue": null, "abstract": "[{\"end\":1287,\"start\":457}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1592,\"start\":1588},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1712,\"start\":1708},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1714,\"start\":1712},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1716,\"start\":1714},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1719,\"start\":1716},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1722,\"start\":1719},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2006,\"start\":2002},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3404,\"start\":3400},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3423,\"start\":3419},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3447,\"start\":3443},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3613,\"start\":3609},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4054,\"start\":4051},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4059,\"start\":4056},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4082,\"start\":4078},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4471,\"start\":4467},{\"end\":4668,\"start\":4663},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4674,\"start\":4670},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4679,\"start\":4676},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4684,\"start\":4681},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5024,\"start\":5021},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5166,\"start\":5162},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5263,\"start\":5259},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5465,\"start\":5462},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5513,\"start\":5509},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6979,\"start\":6975},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13034,\"start\":13031},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13592,\"start\":13588},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15095,\"start\":15091},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15100,\"start\":15097}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":19633,\"start\":18445},{\"attributes\":{\"id\":\"fig_1\"},\"end\":20100,\"start\":19634},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":20243,\"start\":20101},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":20450,\"start\":20244},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":21236,\"start\":20451},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":22966,\"start\":21237},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":23105,\"start\":22967},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":23915,\"start\":23106}]", "paragraph": "[{\"end\":1939,\"start\":1303},{\"end\":3037,\"start\":1941},{\"end\":3466,\"start\":3039},{\"end\":5145,\"start\":3483},{\"end\":5770,\"start\":5147},{\"end\":9255,\"start\":5811},{\"end\":9425,\"start\":9266},{\"end\":10429,\"start\":9511},{\"end\":12096,\"start\":10453},{\"end\":12139,\"start\":12098},{\"end\":12681,\"start\":12148},{\"end\":12711,\"start\":12683},{\"end\":12921,\"start\":12727},{\"end\":13764,\"start\":12944},{\"end\":15161,\"start\":13784},{\"end\":15217,\"start\":15173},{\"end\":16223,\"start\":15219},{\"end\":16852,\"start\":16265},{\"end\":17806,\"start\":16868},{\"end\":18444,\"start\":17822}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9485,\"start\":9426},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12147,\"start\":12140}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":3268,\"start\":3261},{\"end\":7482,\"start\":7475},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":10388,\"start\":10381},{\"end\":11680,\"start\":11673},{\"end\":12163,\"start\":12156},{\"end\":14493,\"start\":14486},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15193,\"start\":15185},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":15206,\"start\":15198},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":16625,\"start\":16618}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1301,\"start\":1289},{\"attributes\":{\"n\":\"2\"},\"end\":3481,\"start\":3469},{\"attributes\":{\"n\":\"3\"},\"end\":5791,\"start\":5773},{\"attributes\":{\"n\":\"3.1\"},\"end\":5809,\"start\":5794},{\"end\":9264,\"start\":9258},{\"attributes\":{\"n\":\"3.2\"},\"end\":9509,\"start\":9487},{\"attributes\":{\"n\":\"3.3\"},\"end\":10451,\"start\":10432},{\"attributes\":{\"n\":\"4\"},\"end\":12725,\"start\":12714},{\"attributes\":{\"n\":\"4.1\"},\"end\":12942,\"start\":12924},{\"attributes\":{\"n\":\"4.2\"},\"end\":13782,\"start\":13767},{\"attributes\":{\"n\":\"4.4\"},\"end\":15171,\"start\":15164},{\"attributes\":{\"n\":\"4.5\"},\"end\":16263,\"start\":16226},{\"attributes\":{\"n\":\"5\"},\"end\":16866,\"start\":16855},{\"attributes\":{\"n\":\"6\"},\"end\":17820,\"start\":17809},{\"end\":18447,\"start\":18446},{\"end\":19643,\"start\":19635},{\"end\":20105,\"start\":20102},{\"end\":20254,\"start\":20245},{\"end\":20461,\"start\":20452},{\"end\":21247,\"start\":21238},{\"end\":22977,\"start\":22968},{\"end\":23116,\"start\":23107}]", "table": "[{\"end\":20243,\"start\":20203},{\"end\":20450,\"start\":20356},{\"end\":21236,\"start\":21121},{\"end\":22966,\"start\":21259},{\"end\":23915,\"start\":23460}]", "figure_caption": "[{\"end\":19633,\"start\":18448},{\"end\":20100,\"start\":19645},{\"end\":20203,\"start\":20107},{\"end\":20356,\"start\":20256},{\"end\":21121,\"start\":20463},{\"end\":21259,\"start\":21249},{\"end\":23105,\"start\":22979},{\"end\":23460,\"start\":23118}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12920,\"start\":12912}]", "bib_author_first_name": "[{\"end\":25215,\"start\":25214},{\"end\":25227,\"start\":25226},{\"end\":25236,\"start\":25235},{\"end\":25238,\"start\":25237},{\"end\":25249,\"start\":25248},{\"end\":25264,\"start\":25263},{\"end\":25273,\"start\":25272},{\"end\":25283,\"start\":25282},{\"end\":25294,\"start\":25293},{\"end\":25303,\"start\":25302},{\"end\":25315,\"start\":25314},{\"end\":25685,\"start\":25684},{\"end\":25694,\"start\":25693},{\"end\":25711,\"start\":25710},{\"end\":25721,\"start\":25720},{\"end\":25727,\"start\":25726},{\"end\":25737,\"start\":25736},{\"end\":26027,\"start\":26026},{\"end\":26029,\"start\":26028},{\"end\":26041,\"start\":26040},{\"end\":26043,\"start\":26042},{\"end\":26053,\"start\":26052},{\"end\":26067,\"start\":26066},{\"end\":26339,\"start\":26338},{\"end\":26349,\"start\":26348},{\"end\":26361,\"start\":26360},{\"end\":26374,\"start\":26373},{\"end\":26384,\"start\":26383},{\"end\":26391,\"start\":26390},{\"end\":26675,\"start\":26674},{\"end\":26689,\"start\":26688},{\"end\":26700,\"start\":26699},{\"end\":26710,\"start\":26709},{\"end\":26719,\"start\":26718},{\"end\":27153,\"start\":27152},{\"end\":27167,\"start\":27166},{\"end\":27178,\"start\":27177},{\"end\":27188,\"start\":27187},{\"end\":27190,\"start\":27189},{\"end\":27460,\"start\":27459},{\"end\":27466,\"start\":27465},{\"end\":27475,\"start\":27474},{\"end\":27482,\"start\":27481},{\"end\":27895,\"start\":27894},{\"end\":27903,\"start\":27902},{\"end\":27912,\"start\":27911},{\"end\":27921,\"start\":27920},{\"end\":27932,\"start\":27931},{\"end\":27944,\"start\":27943},{\"end\":27956,\"start\":27955},{\"end\":28310,\"start\":28309},{\"end\":28318,\"start\":28317},{\"end\":28327,\"start\":28326},{\"end\":28336,\"start\":28335},{\"end\":28349,\"start\":28348},{\"end\":28361,\"start\":28360},{\"end\":28372,\"start\":28371},{\"end\":28374,\"start\":28373},{\"end\":28385,\"start\":28384},{\"end\":28880,\"start\":28879},{\"end\":28882,\"start\":28881},{\"end\":28890,\"start\":28889},{\"end\":29062,\"start\":29061},{\"end\":29071,\"start\":29070},{\"end\":29083,\"start\":29082},{\"end\":29090,\"start\":29089},{\"end\":29104,\"start\":29103},{\"end\":29113,\"start\":29112},{\"end\":29125,\"start\":29124},{\"end\":29135,\"start\":29134},{\"end\":29142,\"start\":29141},{\"end\":29151,\"start\":29150},{\"end\":29528,\"start\":29527},{\"end\":29540,\"start\":29539},{\"end\":29557,\"start\":29556},{\"end\":29570,\"start\":29569},{\"end\":29582,\"start\":29581},{\"end\":29859,\"start\":29858},{\"end\":29865,\"start\":29864},{\"end\":29872,\"start\":29871},{\"end\":29889,\"start\":29888},{\"end\":29903,\"start\":29902},{\"end\":29917,\"start\":29916},{\"end\":29925,\"start\":29924},{\"end\":29935,\"start\":29934},{\"end\":29945,\"start\":29944},{\"end\":29955,\"start\":29954},{\"end\":29963,\"start\":29962},{\"end\":29975,\"start\":29974},{\"end\":29982,\"start\":29981},{\"end\":29991,\"start\":29990},{\"end\":29997,\"start\":29996},{\"end\":30008,\"start\":30007},{\"end\":30459,\"start\":30458},{\"end\":30461,\"start\":30460},{\"end\":30468,\"start\":30467},{\"end\":30476,\"start\":30475},{\"end\":30486,\"start\":30485},{\"end\":30935,\"start\":30934},{\"end\":30937,\"start\":30936},{\"end\":30944,\"start\":30943},{\"end\":30952,\"start\":30951},{\"end\":30954,\"start\":30953},{\"end\":30960,\"start\":30959},{\"end\":30970,\"start\":30969},{\"end\":30972,\"start\":30971},{\"end\":30979,\"start\":30978},{\"end\":31375,\"start\":31374},{\"end\":31383,\"start\":31382},{\"end\":31389,\"start\":31388},{\"end\":31398,\"start\":31397},{\"end\":31406,\"start\":31405},{\"end\":31416,\"start\":31415},{\"end\":31705,\"start\":31704},{\"end\":31715,\"start\":31714},{\"end\":31722,\"start\":31721},{\"end\":31731,\"start\":31730},{\"end\":31737,\"start\":31736},{\"end\":31995,\"start\":31994},{\"end\":32003,\"start\":32002},{\"end\":32009,\"start\":32008},{\"end\":32018,\"start\":32017},{\"end\":32024,\"start\":32023},{\"end\":32032,\"start\":32031},{\"end\":32042,\"start\":32041},{\"end\":32514,\"start\":32513},{\"end\":32522,\"start\":32521},{\"end\":32529,\"start\":32528},{\"end\":32535,\"start\":32534},{\"end\":32543,\"start\":32542},{\"end\":32545,\"start\":32544},{\"end\":32553,\"start\":32552},{\"end\":32555,\"start\":32554},{\"end\":32568,\"start\":32567},{\"end\":32578,\"start\":32577},{\"end\":32969,\"start\":32968},{\"end\":32982,\"start\":32981},{\"end\":32992,\"start\":32991},{\"end\":32994,\"start\":32993},{\"end\":33005,\"start\":33004},{\"end\":33017,\"start\":33016},{\"end\":33029,\"start\":33028},{\"end\":33033,\"start\":33030},{\"end\":33043,\"start\":33042},{\"end\":33045,\"start\":33044},{\"end\":33053,\"start\":33052},{\"end\":33057,\"start\":33054},{\"end\":33065,\"start\":33064},{\"end\":33077,\"start\":33076},{\"end\":33313,\"start\":33312},{\"end\":33322,\"start\":33321},{\"end\":33332,\"start\":33331},{\"end\":33345,\"start\":33344},{\"end\":33353,\"start\":33352},{\"end\":33363,\"start\":33362},{\"end\":33371,\"start\":33370},{\"end\":33381,\"start\":33380},{\"end\":33388,\"start\":33387},{\"end\":33398,\"start\":33397},{\"end\":33407,\"start\":33406},{\"end\":33417,\"start\":33416},{\"end\":33741,\"start\":33740},{\"end\":33753,\"start\":33752},{\"end\":33765,\"start\":33764},{\"end\":33775,\"start\":33774},{\"end\":33783,\"start\":33782},{\"end\":33790,\"start\":33789},{\"end\":33802,\"start\":33801},{\"end\":33817,\"start\":33816},{\"end\":34263,\"start\":34262},{\"end\":34270,\"start\":34269},{\"end\":34279,\"start\":34278},{\"end\":34286,\"start\":34285},{\"end\":34293,\"start\":34292},{\"end\":34660,\"start\":34659},{\"end\":34668,\"start\":34667},{\"end\":34676,\"start\":34675},{\"end\":34683,\"start\":34682},{\"end\":34690,\"start\":34689},{\"end\":34954,\"start\":34953},{\"end\":34960,\"start\":34959},{\"end\":34966,\"start\":34965},{\"end\":34978,\"start\":34977},{\"end\":35242,\"start\":35241},{\"end\":35249,\"start\":35248},{\"end\":35255,\"start\":35254},{\"end\":35264,\"start\":35263},{\"end\":35272,\"start\":35271},{\"end\":35279,\"start\":35278},{\"end\":35288,\"start\":35287}]", "bib_author_last_name": "[{\"end\":25224,\"start\":25216},{\"end\":25233,\"start\":25228},{\"end\":25246,\"start\":25239},{\"end\":25261,\"start\":25250},{\"end\":25270,\"start\":25265},{\"end\":25280,\"start\":25274},{\"end\":25291,\"start\":25284},{\"end\":25300,\"start\":25295},{\"end\":25312,\"start\":25304},{\"end\":25321,\"start\":25316},{\"end\":25691,\"start\":25686},{\"end\":25708,\"start\":25695},{\"end\":25718,\"start\":25712},{\"end\":25724,\"start\":25722},{\"end\":25734,\"start\":25728},{\"end\":25744,\"start\":25738},{\"end\":26038,\"start\":26030},{\"end\":26050,\"start\":26044},{\"end\":26064,\"start\":26054},{\"end\":26076,\"start\":26068},{\"end\":26346,\"start\":26340},{\"end\":26358,\"start\":26350},{\"end\":26371,\"start\":26362},{\"end\":26381,\"start\":26375},{\"end\":26388,\"start\":26385},{\"end\":26399,\"start\":26392},{\"end\":26686,\"start\":26676},{\"end\":26697,\"start\":26690},{\"end\":26707,\"start\":26701},{\"end\":26716,\"start\":26711},{\"end\":26726,\"start\":26720},{\"end\":27164,\"start\":27154},{\"end\":27175,\"start\":27168},{\"end\":27185,\"start\":27179},{\"end\":27197,\"start\":27191},{\"end\":27463,\"start\":27461},{\"end\":27472,\"start\":27467},{\"end\":27479,\"start\":27476},{\"end\":27486,\"start\":27483},{\"end\":27900,\"start\":27896},{\"end\":27909,\"start\":27904},{\"end\":27918,\"start\":27913},{\"end\":27929,\"start\":27922},{\"end\":27941,\"start\":27933},{\"end\":27953,\"start\":27945},{\"end\":27964,\"start\":27957},{\"end\":28315,\"start\":28311},{\"end\":28324,\"start\":28319},{\"end\":28333,\"start\":28328},{\"end\":28346,\"start\":28337},{\"end\":28358,\"start\":28350},{\"end\":28369,\"start\":28362},{\"end\":28382,\"start\":28375},{\"end\":28394,\"start\":28386},{\"end\":28887,\"start\":28883},{\"end\":28898,\"start\":28891},{\"end\":29068,\"start\":29063},{\"end\":29080,\"start\":29072},{\"end\":29087,\"start\":29084},{\"end\":29101,\"start\":29091},{\"end\":29110,\"start\":29105},{\"end\":29122,\"start\":29114},{\"end\":29132,\"start\":29126},{\"end\":29139,\"start\":29136},{\"end\":29148,\"start\":29143},{\"end\":29159,\"start\":29152},{\"end\":29537,\"start\":29529},{\"end\":29554,\"start\":29541},{\"end\":29567,\"start\":29558},{\"end\":29579,\"start\":29571},{\"end\":29591,\"start\":29583},{\"end\":29862,\"start\":29860},{\"end\":29869,\"start\":29866},{\"end\":29886,\"start\":29873},{\"end\":29900,\"start\":29890},{\"end\":29914,\"start\":29904},{\"end\":29922,\"start\":29918},{\"end\":29932,\"start\":29926},{\"end\":29942,\"start\":29936},{\"end\":29952,\"start\":29946},{\"end\":29960,\"start\":29956},{\"end\":29972,\"start\":29964},{\"end\":29979,\"start\":29976},{\"end\":29988,\"start\":29983},{\"end\":29994,\"start\":29992},{\"end\":30005,\"start\":29998},{\"end\":30017,\"start\":30009},{\"end\":30465,\"start\":30462},{\"end\":30473,\"start\":30469},{\"end\":30483,\"start\":30477},{\"end\":30491,\"start\":30487},{\"end\":30941,\"start\":30938},{\"end\":30949,\"start\":30945},{\"end\":30957,\"start\":30955},{\"end\":30967,\"start\":30961},{\"end\":30976,\"start\":30973},{\"end\":30984,\"start\":30980},{\"end\":31380,\"start\":31376},{\"end\":31386,\"start\":31384},{\"end\":31395,\"start\":31390},{\"end\":31403,\"start\":31399},{\"end\":31413,\"start\":31407},{\"end\":31425,\"start\":31417},{\"end\":31712,\"start\":31706},{\"end\":31719,\"start\":31716},{\"end\":31728,\"start\":31723},{\"end\":31734,\"start\":31732},{\"end\":31743,\"start\":31738},{\"end\":32000,\"start\":31996},{\"end\":32006,\"start\":32004},{\"end\":32015,\"start\":32010},{\"end\":32021,\"start\":32019},{\"end\":32029,\"start\":32025},{\"end\":32039,\"start\":32033},{\"end\":32051,\"start\":32043},{\"end\":32519,\"start\":32515},{\"end\":32526,\"start\":32523},{\"end\":32532,\"start\":32530},{\"end\":32540,\"start\":32536},{\"end\":32550,\"start\":32546},{\"end\":32565,\"start\":32556},{\"end\":32575,\"start\":32569},{\"end\":32587,\"start\":32579},{\"end\":32979,\"start\":32970},{\"end\":32989,\"start\":32983},{\"end\":33002,\"start\":32995},{\"end\":33014,\"start\":33006},{\"end\":33026,\"start\":33018},{\"end\":33040,\"start\":33034},{\"end\":33050,\"start\":33046},{\"end\":33062,\"start\":33058},{\"end\":33074,\"start\":33066},{\"end\":33086,\"start\":33078},{\"end\":33319,\"start\":33314},{\"end\":33329,\"start\":33323},{\"end\":33342,\"start\":33333},{\"end\":33350,\"start\":33346},{\"end\":33360,\"start\":33354},{\"end\":33368,\"start\":33364},{\"end\":33378,\"start\":33372},{\"end\":33385,\"start\":33382},{\"end\":33395,\"start\":33389},{\"end\":33404,\"start\":33399},{\"end\":33414,\"start\":33408},{\"end\":33423,\"start\":33418},{\"end\":33750,\"start\":33742},{\"end\":33762,\"start\":33754},{\"end\":33772,\"start\":33766},{\"end\":33780,\"start\":33776},{\"end\":33787,\"start\":33784},{\"end\":33799,\"start\":33791},{\"end\":33814,\"start\":33803},{\"end\":33821,\"start\":33818},{\"end\":34267,\"start\":34264},{\"end\":34276,\"start\":34271},{\"end\":34283,\"start\":34280},{\"end\":34290,\"start\":34287},{\"end\":34297,\"start\":34294},{\"end\":34665,\"start\":34661},{\"end\":34673,\"start\":34669},{\"end\":34680,\"start\":34677},{\"end\":34687,\"start\":34684},{\"end\":34695,\"start\":34691},{\"end\":34957,\"start\":34955},{\"end\":34963,\"start\":34961},{\"end\":34975,\"start\":34967},{\"end\":34983,\"start\":34979},{\"end\":35246,\"start\":35243},{\"end\":35252,\"start\":35250},{\"end\":35261,\"start\":35256},{\"end\":35269,\"start\":35265},{\"end\":35276,\"start\":35273},{\"end\":35285,\"start\":35280},{\"end\":35294,\"start\":35289}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1807.06757\",\"id\":\"b0\"},\"end\":25618,\"start\":25214},{\"attributes\":{\"doi\":\"arXiv:1909.07528\",\"id\":\"b1\"},\"end\":25954,\"start\":25620},{\"attributes\":{\"doi\":\"arXiv:1605.06676\",\"id\":\"b2\"},\"end\":26276,\"start\":25956},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4670339},\"end\":26672,\"start\":26278},{\"attributes\":{\"id\":\"b4\"},\"end\":27070,\"start\":26674},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":49746342},\"end\":27411,\"start\":27072},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206594692},\"end\":27813,\"start\":27413},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":220424783},\"end\":28251,\"start\":27815},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":109933186},\"end\":28811,\"start\":28253},{\"attributes\":{\"doi\":\"arXiv:1609.02907\",\"id\":\"b9\"},\"end\":29059,\"start\":28813},{\"attributes\":{\"id\":\"b10\"},\"end\":29410,\"start\":29061},{\"attributes\":{\"doi\":\"arXiv:2012.04060\",\"id\":\"b11\"},\"end\":29854,\"start\":29412},{\"attributes\":{\"id\":\"b12\"},\"end\":30389,\"start\":29856},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219177177},\"end\":30859,\"start\":30391},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":214612389},\"end\":31299,\"start\":30861},{\"attributes\":{\"doi\":\"arXiv:1706.02275\",\"id\":\"b15\"},\"end\":31633,\"start\":31301},{\"attributes\":{\"doi\":\"arXiv:1908.05224\",\"id\":\"b16\"},\"end\":31933,\"start\":31635},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":49317780},\"end\":32431,\"start\":31935},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":224803224},\"end\":32929,\"start\":32433},{\"attributes\":{\"doi\":\"CoRR abs/1902.04043\",\"id\":\"b19\"},\"end\":33310,\"start\":32931},{\"attributes\":{\"id\":\"b20\"},\"end\":33659,\"start\":33312},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":208617407},\"end\":34191,\"start\":33661},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":227232796},\"end\":34657,\"start\":34193},{\"attributes\":{\"doi\":\"arXiv:2107.01151\",\"id\":\"b23\"},\"end\":34879,\"start\":34659},{\"attributes\":{\"doi\":\"arXiv:1801.02209\",\"id\":\"b24\"},\"end\":35165,\"start\":34881},{\"attributes\":{\"id\":\"b25\"},\"end\":35444,\"start\":35167}]", "bib_title": "[{\"end\":26336,\"start\":26278},{\"end\":27150,\"start\":27072},{\"end\":27457,\"start\":27413},{\"end\":27892,\"start\":27815},{\"end\":28307,\"start\":28253},{\"end\":30456,\"start\":30391},{\"end\":30932,\"start\":30861},{\"end\":31992,\"start\":31935},{\"end\":32511,\"start\":32433},{\"end\":33738,\"start\":33661},{\"end\":34260,\"start\":34193}]", "bib_author": "[{\"end\":25226,\"start\":25214},{\"end\":25235,\"start\":25226},{\"end\":25248,\"start\":25235},{\"end\":25263,\"start\":25248},{\"end\":25272,\"start\":25263},{\"end\":25282,\"start\":25272},{\"end\":25293,\"start\":25282},{\"end\":25302,\"start\":25293},{\"end\":25314,\"start\":25302},{\"end\":25323,\"start\":25314},{\"end\":25693,\"start\":25684},{\"end\":25710,\"start\":25693},{\"end\":25720,\"start\":25710},{\"end\":25726,\"start\":25720},{\"end\":25736,\"start\":25726},{\"end\":25746,\"start\":25736},{\"end\":26040,\"start\":26026},{\"end\":26052,\"start\":26040},{\"end\":26066,\"start\":26052},{\"end\":26078,\"start\":26066},{\"end\":26348,\"start\":26338},{\"end\":26360,\"start\":26348},{\"end\":26373,\"start\":26360},{\"end\":26383,\"start\":26373},{\"end\":26390,\"start\":26383},{\"end\":26401,\"start\":26390},{\"end\":26688,\"start\":26674},{\"end\":26699,\"start\":26688},{\"end\":26709,\"start\":26699},{\"end\":26718,\"start\":26709},{\"end\":26728,\"start\":26718},{\"end\":27166,\"start\":27152},{\"end\":27177,\"start\":27166},{\"end\":27187,\"start\":27177},{\"end\":27199,\"start\":27187},{\"end\":27465,\"start\":27459},{\"end\":27474,\"start\":27465},{\"end\":27481,\"start\":27474},{\"end\":27488,\"start\":27481},{\"end\":27902,\"start\":27894},{\"end\":27911,\"start\":27902},{\"end\":27920,\"start\":27911},{\"end\":27931,\"start\":27920},{\"end\":27943,\"start\":27931},{\"end\":27955,\"start\":27943},{\"end\":27966,\"start\":27955},{\"end\":28317,\"start\":28309},{\"end\":28326,\"start\":28317},{\"end\":28335,\"start\":28326},{\"end\":28348,\"start\":28335},{\"end\":28360,\"start\":28348},{\"end\":28371,\"start\":28360},{\"end\":28384,\"start\":28371},{\"end\":28396,\"start\":28384},{\"end\":28889,\"start\":28879},{\"end\":28900,\"start\":28889},{\"end\":29070,\"start\":29061},{\"end\":29082,\"start\":29070},{\"end\":29089,\"start\":29082},{\"end\":29103,\"start\":29089},{\"end\":29112,\"start\":29103},{\"end\":29124,\"start\":29112},{\"end\":29134,\"start\":29124},{\"end\":29141,\"start\":29134},{\"end\":29150,\"start\":29141},{\"end\":29161,\"start\":29150},{\"end\":29539,\"start\":29527},{\"end\":29556,\"start\":29539},{\"end\":29569,\"start\":29556},{\"end\":29581,\"start\":29569},{\"end\":29593,\"start\":29581},{\"end\":29864,\"start\":29858},{\"end\":29871,\"start\":29864},{\"end\":29888,\"start\":29871},{\"end\":29902,\"start\":29888},{\"end\":29916,\"start\":29902},{\"end\":29924,\"start\":29916},{\"end\":29934,\"start\":29924},{\"end\":29944,\"start\":29934},{\"end\":29954,\"start\":29944},{\"end\":29962,\"start\":29954},{\"end\":29974,\"start\":29962},{\"end\":29981,\"start\":29974},{\"end\":29990,\"start\":29981},{\"end\":29996,\"start\":29990},{\"end\":30007,\"start\":29996},{\"end\":30019,\"start\":30007},{\"end\":30467,\"start\":30458},{\"end\":30475,\"start\":30467},{\"end\":30485,\"start\":30475},{\"end\":30493,\"start\":30485},{\"end\":30943,\"start\":30934},{\"end\":30951,\"start\":30943},{\"end\":30959,\"start\":30951},{\"end\":30969,\"start\":30959},{\"end\":30978,\"start\":30969},{\"end\":30986,\"start\":30978},{\"end\":31382,\"start\":31374},{\"end\":31388,\"start\":31382},{\"end\":31397,\"start\":31388},{\"end\":31405,\"start\":31397},{\"end\":31415,\"start\":31405},{\"end\":31427,\"start\":31415},{\"end\":31714,\"start\":31704},{\"end\":31721,\"start\":31714},{\"end\":31730,\"start\":31721},{\"end\":31736,\"start\":31730},{\"end\":31745,\"start\":31736},{\"end\":32002,\"start\":31994},{\"end\":32008,\"start\":32002},{\"end\":32017,\"start\":32008},{\"end\":32023,\"start\":32017},{\"end\":32031,\"start\":32023},{\"end\":32041,\"start\":32031},{\"end\":32053,\"start\":32041},{\"end\":32521,\"start\":32513},{\"end\":32528,\"start\":32521},{\"end\":32534,\"start\":32528},{\"end\":32542,\"start\":32534},{\"end\":32552,\"start\":32542},{\"end\":32567,\"start\":32552},{\"end\":32577,\"start\":32567},{\"end\":32589,\"start\":32577},{\"end\":32981,\"start\":32968},{\"end\":32991,\"start\":32981},{\"end\":33004,\"start\":32991},{\"end\":33016,\"start\":33004},{\"end\":33028,\"start\":33016},{\"end\":33042,\"start\":33028},{\"end\":33052,\"start\":33042},{\"end\":33064,\"start\":33052},{\"end\":33076,\"start\":33064},{\"end\":33088,\"start\":33076},{\"end\":33321,\"start\":33312},{\"end\":33331,\"start\":33321},{\"end\":33344,\"start\":33331},{\"end\":33352,\"start\":33344},{\"end\":33362,\"start\":33352},{\"end\":33370,\"start\":33362},{\"end\":33380,\"start\":33370},{\"end\":33387,\"start\":33380},{\"end\":33397,\"start\":33387},{\"end\":33406,\"start\":33397},{\"end\":33416,\"start\":33406},{\"end\":33425,\"start\":33416},{\"end\":33752,\"start\":33740},{\"end\":33764,\"start\":33752},{\"end\":33774,\"start\":33764},{\"end\":33782,\"start\":33774},{\"end\":33789,\"start\":33782},{\"end\":33801,\"start\":33789},{\"end\":33816,\"start\":33801},{\"end\":33823,\"start\":33816},{\"end\":34269,\"start\":34262},{\"end\":34278,\"start\":34269},{\"end\":34285,\"start\":34278},{\"end\":34292,\"start\":34285},{\"end\":34299,\"start\":34292},{\"end\":34667,\"start\":34659},{\"end\":34675,\"start\":34667},{\"end\":34682,\"start\":34675},{\"end\":34689,\"start\":34682},{\"end\":34697,\"start\":34689},{\"end\":34959,\"start\":34953},{\"end\":34965,\"start\":34959},{\"end\":34977,\"start\":34965},{\"end\":34985,\"start\":34977},{\"end\":35248,\"start\":35241},{\"end\":35254,\"start\":35248},{\"end\":35263,\"start\":35254},{\"end\":35271,\"start\":35263},{\"end\":35278,\"start\":35271},{\"end\":35287,\"start\":35278},{\"end\":35296,\"start\":35287}]", "bib_venue": "[{\"end\":25382,\"start\":25339},{\"end\":25682,\"start\":25620},{\"end\":26024,\"start\":25956},{\"end\":26447,\"start\":26401},{\"end\":26861,\"start\":26728},{\"end\":27228,\"start\":27199},{\"end\":27565,\"start\":27488},{\"end\":28004,\"start\":27966},{\"end\":28477,\"start\":28396},{\"end\":28877,\"start\":28813},{\"end\":29221,\"start\":29161},{\"end\":29525,\"start\":29412},{\"end\":30574,\"start\":30493},{\"end\":31054,\"start\":30986},{\"end\":31372,\"start\":31301},{\"end\":31702,\"start\":31635},{\"end\":32130,\"start\":32053},{\"end\":32641,\"start\":32589},{\"end\":32966,\"start\":32931},{\"end\":33469,\"start\":33425},{\"end\":33891,\"start\":33823},{\"end\":34350,\"start\":34299},{\"end\":34744,\"start\":34713},{\"end\":34951,\"start\":34881},{\"end\":35239,\"start\":35167},{\"end\":27629,\"start\":27567},{\"end\":28545,\"start\":28479},{\"end\":30642,\"start\":30576},{\"end\":32194,\"start\":32132},{\"end\":34363,\"start\":34352}]"}}}, "year": 2023, "month": 12, "day": 17}