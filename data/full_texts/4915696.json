{"id": 4915696, "updated": "2023-09-28 12:25:43.703", "metadata": {"title": "Crowded Scene Analysis: A Survey", "authors": "[{\"first\":\"Teng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Huan\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Meng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Bingbing\",\"last\":\"Ni\",\"middle\":[]},{\"first\":\"Richang\",\"last\":\"Hong\",\"middle\":[]},{\"first\":\"Shuicheng\",\"last\":\"Yan\",\"middle\":[]}]", "venue": "IEEE Transactions on Circuits and Systems for Video Technology", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "publication_date": {"year": 2015, "month": null, "day": null}, "abstract": "Automated scene analysis has been a topic of great interest in computer vision and cognitive science. Recently, with the growth of crowd phenomena in the real world, crowded scene analysis has attracted much attention. However, the visual occlusions and ambiguities in crowded scenes, as well as the complex behaviors and scene semantics, make the analysis a challenging task. In the past few years, an increasing number of works on crowded scene analysis have been reported, covering different aspects including crowd motion pattern learning, crowd behavior and activity analysis, and anomaly detection in crowds. This paper surveys the state-of-the-art techniques on this topic. We first provide the background knowledge and the available features related to crowded scenes. Then, existing models, popular algorithms, evaluation protocols, as well as system performance are provided corresponding to different aspects of crowded scene analysis. We also outline the available datasets for performance evaluation. Finally, some research problems and promising future directions are presented with discussions.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1502.01812", "mag": "2079023123", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/LiCWNHY15", "doi": "10.1109/tcsvt.2014.2358029"}}, "content": {"source": {"pdf_hash": "c33f02a04c7a7b683480e126230a0487ee0a1358", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1502.01812v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1502.01812", "status": "GREEN"}}, "grobid": {"id": "41074a92f51e54034094ffe47d3ea52d222c4dcd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c33f02a04c7a7b683480e126230a0487ee0a1358.txt", "contents": "\nIEEE TRANSACTIONS ON XXX, VOL. X, NO. XX, MONTH YEAR 1 Crowded Scene Analysis: A Survey\n\n\nTeng Li \nHuan Chang \nMeng Wang \nBingbing Ni \nRichang Hong \nSenior Member, IEEEShuicheng Yan \nIEEE TRANSACTIONS ON XXX, VOL. X, NO. XX, MONTH YEAR 1 Crowded Scene Analysis: A Survey\nIndex Terms-Crowded scene analysis, survey\nAutomated scene analysis has been a topic of great interest in computer vision and cognitive science. Recently, with the growth of crowd phenomena in the real world, crowded scene analysis has attracted much attention. However, the visual occlusions and ambiguities in crowded scenes, as well as the complex behaviors and scene semantics, make the analysis a challenging task. In the past few years, an increasing number of works on crowded scene analysis have been reported, covering different aspects including crowd motion pattern learning, crowd behavior and activity analysis, and anomaly detection in crowds. This paper surveys the state-of-the-art techniques on this topic. We first provide the background knowledge and the available features related to crowded scenes. Then, existing models, popular algorithms, evaluation protocols, as well as system performance are provided corresponding to different aspects of crowded scene analysis. We also outline the available datasets for performance evaluation. Finally, some research problems and promising future directions are presented with discussions.Index Terms-Crowded scene analysis, survey. DATA FEATURES INFERENCE images, videos, etc. optical flow, motion histogram, tracklet, texture descriptor, etc.KNOWLEDGEAnnotationsAlgorithms (unsupervised,  supervised, etc.)   Models (physical-based models,  statistical learning models, etc.)\n\nI. INTRODUCTION\n\nW ITH the increase of population and diversity of human activities, crowded scenes have been more frequent in the real world than ever. It brings enormous challenges to public management, security or safety. Some examples of crowded scenes are shown in Figure 1. Humans have the ability to extract useful information of behavior patterns in the surveillance area, monitor the scene for abnormal situations in real time, and provide the potential for immediate response [1]. However, psychophysical research indicates that there are severe limitations in their ability to monitor simultaneous signals [2]. Extremely crowded scenes require monitoring an excessive number of individuals and their activities, which is a significant challenge even for a human observer.\n\nIn the past decade, automated scene understanding or analysis has already attracted much research attention in the computer vision community [3]- [8]. One important application is intelligent surveillance in replace of the traditional passive T. Li   Copyright (c) 2014 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org. video surveillance. Although many algorithms have been developed to track, recognize and understand the behaviors of various objects in video [9], they were mainly designed for common scenes with a low density of population [5], [6], [10]. When it comes to crowded scenes, the problems can not be handled well, since the large number of individuals involved not only cause the detection and tracking fail, but also greatly increase computational complexity. Under such circumstance, crowded scene analysis as a unique topic, is specifically addressed. Driven by the practical demand, it is becoming an important research direction and has already attracted lots of efforts [9], [11]- [16]. The opportunity for such study has never been better. As noted in [3], [4], scene understanding may refer to scene layout (locating roads, buildings, sidewalks) [17], motion patterns (vehicles turning, pedestrian crossing) [3], [4], [18], [19] and scene status (crowd congestion, split, merge, etc.) [14], [20]. In this paper, combined with previous studies, we will elaborate the key aspects of crowded scene analysis in automated video surveillance.\n\n\nA. Real-World Applications\n\nResearch of crowded scene analysis could lead to a lot of critical applications.\n\n1) Visual Surveillance: Many places of security interests such as railway station and shopping mall are very crowded. Conventional surveillance system may fail for high density of objects, regarding both accuracy and computation. We can leverage the results of crowd behavior analysis to crowd flux statistics and congestion analysis [20], [21], anomaly detection and alarming [11]- [13], [22], etc.\n\n2) Crowd Management: In mass gatherings such as music festivals and sports events, crowded scene analysis can be used to develop crowd management strategies and assist the movement of the crowd or individuals, to avoid the crowd disasters and ensure the public safety [23], [24].\n\n3) Public Space Design: The analysis of crowd dynamics and its relevant findings [25], [26] can provide some guidelines for public space design, and therefore increase the efficiency arXiv:1502.01812v1 [cs.CV] 6 Feb 2015 IEEE TRANSACTIONS ON XXX, VOL. X, NO. XX, MONTH YEAR 2 and safety of train stations, airport terminals, theaters, public buildings, and mass events in the future. 4) Entertainment: With the in-depth understanding of crowd phenomena, the establishment of mathematical models can provide more accurate simulation, which can be used in computer games, film and television industries [27]. Some recent works have been proposed to synthesize crowd videos with realistic microscale behavior [28].\n\n\nB. Problems and Motivations\n\nVideo analysis and scene understanding usually involve object detection, tracking and behavior recognition [5], [6]. For crowded scenes, due to extreme clutters, severe occlusions and ambiguities, the conventional methods without special considerations are not appropriate. As Ali pointed out [29], the mechanics of human crowds are complex as a crowd exhibits both dynamics and psychological characteristics, which are often goal directed. This makes it very challenging to figure out an appropriate level of granularity to model the dynamics of a crowd. Another challenge in crowded scene analysis is that the specific crowd behaviors needed to be detected and classified may be both rare and subtle [30], and in most surveillance scenarios, these behaviors have few examples to learn.\n\nThese challenges have partially drawn attention at some recent conferences, and several relevant scientific papers have also been published in academic journals. In this paper, we try to explore those problems with a comprehensive review and general discussions. The state-of-the-art technical advances in crowded scene analysis will be covered. Feature extraction, segmentation and model learning are considered as core problems addressed in visual behavior analysis [31]. We will discuss the methods in crowded scene analysis regarding these basic issues.\n\nIt is noted that survey papers [9], [15], [32], [33] relevant to the topic of crowd analysis have been written in the past few years. Zhan et al. [15] presented a survey in 2008 on crowd analysis methods in computer vision. They covered the techniques of crowd density estimation, pedestrian/crowd recognition and crowd tracking. They paid much attention on the perspectives from other research disciplines, such as sociology, psychology and computer graphics, to the computer vision approach. The paper [9] by Junior et al. in 2010 also presented a survey on a wide range of computer vision techniques for crowd analysis, covering people tracking, crowd density estimation, event detection, validation and simulation. They devoted large sections to reporting how related the areas of computer vision and computer graphics should be to deal with challenges in crowd analysis. Later in 2011, Sjarif et al. [32] wrote a survey with more emphasis on abnormal behaviors detection in crowded scenes. Recently, Thida et al. [33] gave a general review on crowd video analysis, providing some valuable summarizations. But still many recent important works have been missed, and the descriptions of methods were rather brief.\n\nIn this study, we seek to create a more focused review of recent publications on high-level crowded scene understanding, related to motion and behavior analysis in crowd videos. Several important recent works from 2010 to now will be covered and compared, which have not yet been surveyed previously. In order to better elaborate this topic, we divide it into three subtopics according to the purposes of task: motion pattern segmentation, crowd behavior recognition and anomaly detection. It is indicated that these three aspects are closely related to each other in crowded scene analysis [34]. Differently from previous survey such as [15], crowd counting or density estimation, a closely related topic [35]- [37], is not covered in this survey. We intend to cover the area related to the analysis of behaviors and activities in crowd videos, which is broad enough. The reviewed methods are all about the behaviors or activities, and are usually based on the motion features, while in crowd counting, static visual features can be important.\n\nAs an indispensable basis for each crowded scene analysis method, feature representation is discussed and summarized separately before elaboration of the three subtopics. Feature representation can be shared by different methods. To clearly and properly situate the problem at hand for the readers, we still follow the task-orientated way to categorize the methods into three subtopics. Besides, some background knowledge and available physical crowd models are provided beforehand. They could be utilized in the analysis methods. Figure 2 illustrates the diagram of crowded scene analysis. It also reveals what this paper is going to elaborate and explain.\n\n\nC. Organization of The Paper\n\nThe remainder of this paper is organized as follows. Section II describes some background knowledge and physical models that could be utilized in crowded scene analysis. In Section III, we introduce the features commonly used in the literature. In Section IV, the concept of motion pattern segmentation and the relevant approaches are detailed. Section V provides a detailed review on the topic of crowd behavior recognition, and the approaches for anomaly detection are elaborated in Section VI. Section VII provides the detailed information of frequently used datasets for crowded scene analysis. We conclude the paper in Section VIII and end this review by providing some promising future directions.\n\n\nII. KNOWLEDGE OF THE CROWD\n\nCrowded scenes can be divided into two categories according to the motion of the crowd [38]: structured and unstructured. In the structured crowded scenes, the crowd moves coherently in a common direction, the motion direction does not vary frequently, and each spatial location of the scene contains only one main crowd behavior over the time. The unstructured crowded scenes represent the scenes with chaotic or random crowd motion, where participants move in different directions at different times, and each spatial location contains multiple crowd behaviors [16]. Figure 1(a) is structured crowded scenes, while Figure 1(b) is unstructured scenes. Obviously, they have different dynamic and visual characteristics.\n\nThe crowd has been defined as \"a large group of individuals in the same physical environment, sharing a common goal\" [39]. It can be viewed hierarchically: individuals are collected into groups, and the resulting groups are collected into a crowd, with a set of motivations and basic rules [40]. This representation permits a flexible analysis of a large spectrum of crowd densities and complicated behaviors.\n\nThe analysis of crowd can be conducted at macroscopic or microscopic levels. At the macroscopic level, we are interested in the global motions of a mass of people, without concerning the movements of any individual; at the microscopic level, we concern the movements of each individual pedestrian and do analyze based on the collective information of them.\n\nThe analysis of crowded scenes could involve the knowledge from both vision area and crowd dynamics. Except for the vision algorithms usually adopted in conventional scene analysis, the physical models from crowd dynamics could also be utilized. In the below we will introduce some available knowledge such as the models from crowd dynamics, as well as their application in crowded scene analysis.\n\n\nA. Models in Crowd Dynamics\n\nCrowd dynamics have been studied intensively for more than 40 years. It can be considered as the study of how and where crowds form and move above the critical density level [41], and how individuals in the crowd interact with each other to influence the crowd status. In the past, studies were mainly conducted in order to support planning of urban infrastructure, e.g., building entrances and corridors [42].\n\nThere are two major approaches for computational modeling of crowd behavior: continuum-based approach and agentbased approach. In crowd simulation, both of them have been frequently used in to reproduce crowd phenomena. Continuum-based approach works better at the macroscopic level for medium and high density crowds, while the agentbased approach is more suitable for low density crowds at the microscopic level, where the movement of each individual pedestrian is concerned.\n\nFor the continuum-based approach, the crowd is treated as a physical fluid with particles, thus a lot of analytical methods from statistical mechanics and thermodynamics are introduced [43], [44]. Hughes et al. [43] have developed a model representing pedestrians as a continuous density field, and have presented a pair of elegant partial differential equations describing the crowd dynamics. Moreover, a realtime crowd model based on continuum dynamics has been presented in [44]. It could yield a set of dynamic potentials and velocity fields to guide the individuals' motions.\n\nFor the agent-based approach, individuals in the crowd are considered as autonomous agents which actively sense the environment and make decisions according to some predefined rules [41], [45]. Following this style, the social force model (SFM), first proposed by Helbing et al. [45], has been proven to be capable of reproducing specific crowd phenomena. The assumption is that the interaction force between pedestrians is a significant feature for analyzing crowd behaviors. The SFM can be formulated as:\nm i dv i dt = m i ( v p i \u2212 v i \u03c4 i ) + F int(1)\nwhere m i denotes the mass of the individual, v i indicates its actual velocity which varies given the presence of obstacles in the scene, \u03c4 i is a relaxing parameter, F int indicates the interaction force encountered by the individual defined as the sum of attraction and repulsive forces, and v p i is the desired velocity of the individual. Fig. 3 visualizes of the forces and velocities in SFM.\n\nGeneralized SFM has been adopted as the basic model in many studies of crowd behavior analysis [22], [46], [47]. Furthermore, the calibrated agent-based framework proposed in [41] could also accurately model a number of observed crowd phenomena.\n\n\nB. Crowd Model in Video Analysis\n\nThere has been a series of attempts to incorporate the research findings of crowd simulation to automatic crowded scenes analysis [40]. Several physics-inspired crowd models have been utilized for the purposes of recognition and classification [11], [13], [22], [29], [48].\n\nTo analyze at the macroscopic level, usually holistic properties of the scene are modeled. Assuming that high density crowd behaves like a complex dynamic system, many dynamical crowd evolution models have been proposed [11]- [13], [49]. The concepts of motion field and dynamical potential borrowed from fluid dynamics community were utilized [50]. The motion field is a rich dynamical descriptor of the flow which can be related to the velocity of flow, while the potential accounts for several physical quantities such as the density or the pressure in the flow. Although people do not always follow the laws of physics, they have choices in their direction, have no conservation of momentum and can stop and start at willing [41], the coupling of crowd dynamics and real data has exhibited promising results in crowd video analysis and opened a rich area of research.\n\nAt the microscopic level, agent-based models have also been popular in video analysis. They analyze the stimuli, or driven factors of crowd behavior, based on the assumption that crowd behavior originates from the interaction of its elementary individuals. Mehran et al. [22] and Zhao et al. [47] applied the SFM to detect the abnormal events of the crowd. Zhou et al. [7], [8] used dynamic pedestrian-agent model to learn the collective behavior patterns of pedestrians in crowded scenes.\n\nIt is noted that cues from the two levels could be jointly used. For example, [38], [51] employ the global motion information to improve tracking individuals in a crowd scene. Also the microscopic information of individual movements can be used as basic units in the holistic scene models.\n\nBesides, visual feature extraction, object tracking, learning, and other related algorithms from vision area also play important roles in crowded scene analysis. They will be given brief introductions in method descriptions.\n\n\nIII. MOTION REPRESENTATION IN CROWDED SCENES\n\nAs a crucial basis, an appropriate feature representation can benefit the subsequent tasks. Motion information representation is the basis for crowded scene analysis. Although other types of visual features such as scene structure, geometrical information and viewing direction could also be helpful, motion features are dominant in our interested area.\n\nAccording to the representation level, previous crowd motion features can be categorized into three categories: flowbased features, local spatio-temporal features and trajectory/tracklet. Flow-based features are extracted densely on the pixel level. Local spatio-temporal features represent the scene based on local information from 2D patches or 3D cubes. On a higher level, trajectory/tracklet tries to compute the individual tracks, as the basic features for motion representation. These feature representations have been used in various tasks as motion pattern segmentation, crowd behavior recognition and crowd anomaly detection.\n\n\nA. Flow-Based Features\n\nIn the context of high density crowded scenes, tracking a person or an object is always a difficult task, and sometimes unfeasible. Fortunately, when we look at crowd, we care about what is happening, not who is doing it. The specific actions of individual pedestrians may appear relatively random, but the overall look of the crowd can still be convincing [52]. For that reason, several optical-flow alike features have been presented in recent years [11]- [13], [22], [53]- [55], these methods avoid tracking from the macroscopic level and have achieved some success in addressing complex crowd flows in the scenes.\n\n1) Optical Flow: Optical flow is to compute pixel-wise instantaneous motion between consecutive frames [56]. Optical flow is robust to multiple and simultaneous camera and object motions, and it is widely used in crowd motion detection and segmentation [4], [19]- [21], [53], [54]. However, optical flow does not capture long-range temporal dependencies, and can not represent spatial and temporal properties of a flow. These properties can be useful for many applications.\n\n2) Particle Flow: Recently, based on the Lagrangian framework of fluid dynamics [57], a notion of particle flow was introduced in computer vision [11], [12], [22]. Particle flow is computed by moving a grid of particles with the optical flow through numerical integration, providing trajectories that relate a particles initial position to its position at a later time. Impressive results employing particle flow have been demonstrated on crowd segmentation [12] and abnormal crowd behavior detection [11], [22]. However, in particle flow the spatial changes are ignored, and time delay is significant.\n\n3) Streak Flow: In order to achieve an accurate representation of the flow from crowd motion, Mehran et al. [13] introduced the notion of streakline to compute the motion field for crowd video scene analysis, referred to as streak flow. They also provided the comparison between optical flow, particle flow and streak flow with discussion. Streaklines are well known in flow visualization [58] and fluid mechanics [50] as a tool for measurement and analysis of the flow. It encapsulates motion information of the flow for a period of time. This resembles particle flow where the advection of a grid of particles provides information for segmenting the crowd motion. Streak flow exhibits changes in the flow faster than particle flow, and therefore captures crowd motions better in a dynamically changing flow. Fig. 4 gives an example of the optical flow feature. It also shows a comparison of optical flow, particle flow and streak flow using a locally uniform flow field changing over time.\n\n\nB. Local Spatio-Temporal Features\n\nSome extremely crowded scenes, though similar in density, are less structural due to the high variability of pedestrian movements. The motion within each local area may be nonuniform and generated by any number of moving objects. In such circumstances, even a fine-grain representation, such as optical flow, would not provide enough motion information.\n\nOne solution is to exploit the dense local motion patterns created by the subjects, and model their spatio-temporal relationships to represent the underlying intrinsic structure they form in the video [59].\n\nThe related methods generally consider the motion as a whole, and characterize its spatio-temporal distributions based on local 2D patches or 3D cubes, such as spatio-temporal gradients [59], [60], and histogram functions [19], [61].\n\n1) Spatio-Temporal Gradients: The distribution of spatiotemporal gradients has been utilized as the base representation [59], [60]. For each pixel i in patch I, the spatio-temporal gradient \u2207I i is calculated as\n\u2207I i = [I i,x I i,y I i,t ] T = [ \u2202I \u2202x \u2202I \u2202y \u2202I \u2202t ] T(2)\nwhere x, y and t are the video's horizontal, vertical, and temporal dimensions, respectively. The 3D gradients of each pixel collectively represent the characteristic motion pattern within the patch. By capturing the steady-state motion behavior with the spatio-temporal motion pattern models, Kratz et al. [59], [60] demonstrated that unusual activities can be detected as statistical deviations naturally.\n\n2) Motion Histogram: Motion histograms can be considered as a kind of motion information defined on local regions. Fig. 5 illustrates the motion histograms calculated from three sample pixels. In fact it originally is ill-suited for crowd motion analysis, since computing motion orientation on a motion histogram is not only time-consuming but also errorprone due to the aligning problem. Therefore, researchers have developed some more advanced features based on motion histogram [19], [61]. In the below brief descriptions are given.\n\nJodoin et al. [19] proposed a feature called orientation distribution function (ODF). It is the probability density function of a given motion orientation. Opposed to motion histograms, ODFs have no information on the magnitude of the flow. This makes the ODF representation simpler (1D instead of 2D), and is a key advantage in computation for the upcoming motion pattern learning.\n\nCong et al. [61] proposed a novel feature descriptor called multi-scale histogram of optical flow (MHOF). It preserves not only the motion information, but also the spatial contextual information. For event representation, features of all types of bases with various spatial structures are concatenated as MHOF. After estimating the motion field by optical flow, they partitioned the image into a few basic units, i.e., 2D image patches or 3D spatio-temporal cubes, then extracted MHOF from each unit.\n\nOverall, spatio-temporal features have shown particular promise in motion understanding due to their strong descriptive power, and therefore, they have been widely used as in various tasks such as crowd anomaly detection.\n\n\nC. Trajectory/Tracklet\n\nTypical crowd motion is usually regular and repetitive, so that we can analyze crowd activities based on motion features extracted from trajectories of objects, such as relative distance between objects, acceleration, or motion energy. long-range MPs as every moving object/person is tracked. However, it is broadly accepted that object-tracking is illsuited for videos with a large number of moving objects, especially in crowds as in Fig. 2 (a) and (d).\n\nAs a solution, non-tracking methods have been proposed. Similar to our method, particle advection methods have been shown successful to handle dense videos [ 21,24,2,15]. However, some of these methods were designed to detect local anomalies [24,15] and thus are not suited to recover long-range MPs. Alternatively, Ali and Shah [ 2] proposes a scene segmentation method based on particle advection which can recover long-range MPs. Unfortunately, that method was not meant to deal with overlapping MPs (as in Fig. 2(c) and (d)) since each pixel is assigned only one motion label. Solmaz et al. [21] use particles to recognize among five crowd behaviors based on the eigenvalues of a flow-based Jacobian matrix. It is not clear though how this method can recover individual MPs, especially if their shape differs from the five predefined behaviors.\n\nWork by Hu, Ali and Shah [2,3,11] is to our knowledge the closest contribution to ours. Given flow vectors, they find motion paths with a tracking method which they call sink seeking. Sink seeking leads to sink paths which are then clustered into super tracks. However, since their method relies on pixel-based flow vectors, the tracking method cannot recover overlapping MPs which are frequent in crossroads and in Y-shape roads (results are reported in section 4). From the same lab [12], a scene segmentation method based on a motion flow field but without particles has been proposed. Motion flow vectors are clustered together with a hierarchical clustering based on a geodesic converts motion histograms into ODFs, 3) performs metatracking and 4) clusters meta-tracks to recover MPs as well as the entry/exit points. \n\n\nMotion Histograms\n\nConsider I, a video with constant inter-frame interval, and (u t , v t ) the optical flow at time t computed by a motion estimation method. In this work, we use the Horn/Schunk -Lucas/Kanade method proposed by Bruhn et al. [ 6] which we found to be a good compromise between precision and speed. Although ODFs (to be defined later) could be obtained with tracklets as in [27], we empirically found that optical flow is much easier and faster to compute. Once optical flow has been computed for each frame, a motion histogram M p (u, v) is computed at each pixel p. This histogram contains the number of times a pixel p had its motion vector (u t,p , v t,p ) equal to (u, v) during the entire video. Since M p takes integer indices, motion vectors (u t,p , v t,p ) are rounded up to the nearest integer. Fig. 4 shows three pixel-based motion histograms. Comparing with the the other two types of feature representations, trajectory/tracklet is more semantic and it seems to be attractive. However, as mentioned previously, traditional pipeline of object detection and the subsequent tracking of those detections can hardly perform accurate object detection and tracking, as the density of the crowd increases and the scene clutter becomes severe [7].\n\nConsidering the difficulties in obtaining complete trajectories, a motion feature called tracklet has been proposed. A tracklet is a fragment of a trajectory obtained by the tracker within a short period. Tracklets terminate when ambiguities caused by occlusions or scene clutters arise. They are more conservative and less likely to drift than long trajectories [7]. In previous works [62]- [64], tracklets have been mainly used to be connected into complete trajectories for tracking or human action recognition. Recently, several tracklet based approaches for learning semantic regions and clustering trajectories [7], [8], [65], [66] were proposed. In the approaches, tracklets are often extracted from dense feature points and then a certain model is applied to enforce the spatial and temporal coherence between tracklets, to finally detect behavior patterns of pedestrians in crowded scenes.\n\n\nD. Summary\n\nConsidering the characteristics of crowded scenes, such as the congestion level, the degree of occlusion between objects, whether there is obvious motion structure, the size of field view of camera, and the resolution of a single target in the scene, we can briefly summary to which scenarios these features are suitable:\n\n\u2022 For outdoor scenes, often with wide field of view but low resolution for each target, we mainly aim to analyze the holistic crowd motion trends. The optical flow alike features are suitable. These features can be easily combined with flow-based models, which has priority in structured crowded scene analysis.\n\n\u2022 For indoor scenes, e.g., subway stations and shopping malls, the resolution of a single target is high enough and the crowd density may not be so high. The object trajectories or tracklet-based features may be good choices. These features are usually combined with agent-based models, which is applicable for analyzing activities or semantic regions in unstructured crowded scenes.\n\n\u2022 Besides, when the field of view is not wide, but crowd density is high with severe occlusion, neither optical flow IEEE TRANSACTIONS ON XXX, VOL. X, NO. XX, MONTH YEAR 6 alike features nor tracklets will meet the requirements. Various local spatio-temporal features could be considered.\n\n\nIV. CROWD MOTION PATTERN SEGMENTATION\n\nMotion patterns learning is important in automated visual surveillance [5], [6], [21]. In crowded scene analysis, it is highly desirable to analyze the motion patterns and obtain some high-level interpretation.\n\nThe term motion pattern here refers to a spatial region of the scene that has a high degree of local similarity of the speed, as well as flow direction within the region and otherwise outside [3]. Motion patterns not only describe the segmentation in the spatial space, but also reflect the motion tendency in a period. These patterns can be joint or disjoint in the image space. They usually have a semantic level interpretation and contain sources and sinks of the paths described by the patterns.\n\nTo analyze motion patterns in crowded scenes, various methods have been proposed. According to the principle to segment or cluster the motions, these methods can be divided into three categories: flow field model based segmentation, similarity based clustering, and probability model based clustering. The first category tries to simulate the image spatial segmentation based on flow field models, and therefore tends to produce spatially continuous segments. The later two categories utilize various well-developed clustering algorithms, usually based on local motion features, e.g., tacklets or motion video words. The resulting segments may be scattered, but they could be applicable to unstructured scenes with complex motions.\n\n\nA. Flow Field Model Based Segmentation\n\nAmong many physical-based models applied in crowd analysis, flow field models [12], [13], [19], [53]- [55], [67] are well studied in crowd motion pattern segmentation. By treating a moving crowd as a time dependent flow field consisting of regions with qualitatively different dynamics, the motion patterns emerging from spatio-temporal interactions of the participants can be reflected. Based on this kind of representations, methods such as edge-based segmentation, graph-based segmentation, watershed segmentation can be applied.\n\nAli et al. [12] proposed the Lagrangian particle dynamic to segment high density crowd flows. To uncover the spatial organization of the flow field, clouds of particles generated by the crowd motion are examined. Then Lagrangian coherent structures (LCS) [57] is utilized to map to the boundaries of different crowd segments. Similar to the edges for image, the LCSs for flow data can be used to segment flow regions of different dynamics. The proposed method of [12] could reveal the underlying flow structures of velocity field, and it is insensitive to the scene density. However, slow motions might not be segmented out, and low crowd density might cause over-segmented.\n\nTo detect typical motion patterns in crowded scenes, Hu et al. [53] constructed a directed neighborhood graph to measure the closeness of motion flow vectors, and then grouped them into motion patterns. Based on the same idea, Hu et al. [54] later invented a method to learn dominant motion patterns in videos. This is accomplished by first detecting the representative modes (sinks) of motion patterns, followed by construction of the super tracks, i.e., collective representations of the discovered motion patterns. The methods do not require complete trajectories, avoiding the problem of occlusion. But they are not applicable to unstructured scenes and the number of motion patterns needs to be predefined.\n\nAnother approach is local-translational domain segmentation (LTDS) model proposed in Wu et al. [67]. Local crowd motion is approximated as a translational motion field, and the evolution of domain boundaries is derived from the G\u00e2teaux derivative of an objective function. To represent crowd motion in an accurate and efficient way, optical flow is computed at salient locations instead of all the pixel locations. Then the problem of crowd motion partitioning is transformed to scattered motion field segmentation. This method can automatically determine the number of groups and can be applied to both medium and high density crowded scenes.\n\nAs introduced previously in section III, the streakline framework [13] can recognize spatio-temporal flow changes more quickly than other methods. However, this framework computes the optical flow field using the conventional method. It has poor anti-interference performance, and serious deviation would be brought to the computation of the optical flow field. In [55], Wang et al. improved the streakline framework with a highly accurate variational model [56]. Different motion patterns are separated in crowded scenes by computing the similarity of streaklines and streak flows using watershed segmentation. This method finds a balance between recognition of local spatial changes and filling spatial gaps in the flow. But it must be noted that, the streak flow computation is vulnerable to disturbance, which may result in incorrect segmentation.\n\nFlow field model based segmentation has shown success in handling high density scenes with complex crowd flows. However, the optical flow alike features are designed to detect local changes, not to recover long-range motion patterns. Alternatively, the particle flow methods used in [13], [19], [29] treat the crowd as a single entity, and ignore the spatial changes in coherent motion patterns. Another disadvantage of the methods in [29], [53], [54] is that they can not handle overlapping motion patterns, since each pixel is assigned to only one motion label. It is often not the case in unstructured scenes. In addition, with the decrease in crowd density, flow field model would no longer work, and cause the video scene to be over-segmented.\n\n\nB. Similarity Based Clustering\n\nIn this kind of methods, motion pattern segmentation is treated as a clustering problem: once motion features are detected and extracted, they are grouped into similar categories through some similarity measurements. Then, the semantic regions are estimated from the spatial extents of trajectory/tracklet clusters. The detailed descriptions are as follows.\n\nCheriyadat et al. [69] used a distance measure for feature trajectories based on longest common subsequence (LCSS)  [73]. The method begins with independently tracking low-level features using optical flow, and then clusters these tracks into smooth dominant motions. It has a great advantage of speed in measuring the similarity between all pairs of tracks. But the parameters for clustering need to be fine-tuned for different situations. In addition, feature points tracking may suffer from noise. Based on tracklets, Zhao et al. [34] used a manifold learning method to infer the local geometric structures in image space, and to infer the motion patterns in videos. They embedded tracklet points into (x, y, \u03b8) space, where (x, y) stand for the image space and \u03b8 represents motion direction. In this space, points automatically form intrinsic manifold structures each corresponding to a motion pattern.\n\nAlso based on tracklets, Zhou et al. [70] proposed a general technique of detecting coherent motion patterns in noisy timeseries data, named coherent filtering. When applying this technique to coherent motion detection in the crowd, tracklets are firstly extracted by Kanade-Lucas-Tomasi (KLT) [74] keypoint tracker. Then a similarity measure called coherent neighbor invariance is used to characterize these tracklets and cluster them into different motion patterns. An approach similar to [70] was proposed in Wang et al. [65], to analyze motion patterns from the tracklets in dynamical crowded scenes. Tracklets are collected by tracking dense feature points from the video of crowded scenes, and motion patterns are then learned by clustering the tracklets.\n\nIn Jodoin et al. [19], a method of meta-tracking has been proposed to extract dominant motion patterns and the main entry/exit areas from a surveillance video. This method relies on pixel-based orientation distributed functions (ODFs), which summarize the directions of the flows at each point of the scene. Once all pixels have been assigned to ODFs, particle trajectories are computed through an iterative algorithm. They are called \"meta-tracks\". Finally, based on a hierarchical clustering method, nearest meta-tracks are merged together to form the motion patterns.\n\nCompared with trajectories, local motion feature is insensitive to scene clutter and tracking errors. Through clustering or linking process, the tracklets or optical flow with common features can be properly grouped, resulting in different semantic regions. Another advantage is that local feature clustering can be used for both structured and unstructured crowded scenes, since even mutually overlapping local motion features can be well separated in the learning process.\n\n\nC. Probability Model Based Clustering\n\nBeing widely utilized in visual clustering, probability Bayesian models can also be adopted here for crowd motion pattern segmentation. Low-level motion features to be clustered are fitted with the designed models. Popular models in vision area, such as Gaussian mixture model (GMM), random field topic (RFT), and latent Dirichlet allocation (LDA) have been applied. In contrast to the simple averaging optical flow methods, the use of an probability model allows for longterm analysis of a scene. Moreover, it can capture both the overlapping behaviors at any given location in a scene and the spatial dependencies between behaviors. Finally, the statistical model can incorporate a priori knowledge on where, when and what types of activities occur.\n\nYang et al. [4] proposed a novel method to automatically discover key motion patterns in a scene by observing the scene for an extended period. Firstly low-level motion features are extracted through computing optical flow. These motion features are then quantized into video words based on their direction and location. Next, some video words are screened out based on the entropy over all clips for a given word.\n\nThe key motion patterns are discovered using diffusion maps embedding [75] and clustering.\n\nFor the same purpose, Saleemi et al. [3] introduced a statistical model for motion patterns representation based on raw optical flow. The method is based on hierarchical problemspecific learning. GMM is exploited as a co-occurrence free measure of spatio-temporal proximity and flow similarity between features. Finally, a pixel-level representation of motion patterns is proposed by deriving conditional expectation of optical flow.\n\nThe RFT model has been applied in semantic region analysis in crowded scenes in Zhou et al. [7], [8], based on the motions of objects. In the approach, a tracklet is treated as a document, and observations (points) on tracklets are quantized into words according to a codebook based on their locations and velocity directions. In addition, Markov Random Fields (MRF) is used as a prior to enforce the spatial and temporal coherence between tracklets during the learning process. The MRF model encourages tracklets spatially and temporally close to have similar distributions over semantic regions. Each semantic region has its preferred source and sink. Therefore, activities observed in the same semantic region have similar semantic interpretations.\n\nThe LDA model has also been adopted [71]. Assuming that motion patterns involved in a complex dynamic scene usually have a hierarchical nature, a two-level motion pattern mining approach has been proposed. At the first level, single-agent motion patterns are modeled as distributions over pixel-based features. At the second level, interaction patterns are modeled as distributions over single-agent motion patterns. Then, LDA is applied to discover both single-agent motion patterns and interaction patterns in the video.\n\nMoreover, Fu et al. [72] extracted optical flow features from each pair of consecutive frames, and quantized them into discrete visual words. The video is represented by a word-document hierarchical topic model through a generative process, and an improved sparse topical coding approach is used for model learning.\n\nAn advantage of probability models is that it can provide much more compact representations than directly clustering high dimensional motion feature vectors computed from video clips. Furthermore, it models the spatio-temporal interrelationships among different events at the global scene level, which can facilitate the crowd behavior understanding.\n\n\nD. Experiments\n\nTo give a pilot evaluation of crowd motion pattern segmentation methods, we test five representative methods on six videos standing for different challenges. These videos were taken from the UCF datasets and [19], and their length ranges from 100 frames to 5000 frames. Some have a small number of moving objects (Pedestrians, Crosswalk, Roundabout) while others are highly crowded (Marathon, Mecca, Pilgrims); some have simple layout (Marathon, Mecca, Crosswalk) while others are complex (Pedestrians, Pilgrims, Roundabout); some have well structured dynamics (Marathon, Mecca) while others present fairly unstructured scenes(Pedestrians) or semistructured scenes (Pilgrims, Crosswalk, Roundabout). Fig. 6 gives the motion segmentation results of the selected five methods on six videos. It can be found that these methods produce similar results on sequences of well structured (Marathon, Mecca) and semi-structured scenes (Pilgrims, Crosswalk, Roundabout), but different results on unstructured scenes (Pedestrian). For structured scenes, all the methods well segment the areas with different motion characteristics, and each resulted region looks continuous and unified; for unstructured scenes, though the segmented patterns by particle flow with FTLE field and streak flow with watershed are still unified, the truth is that individuals in the resulted region move differently; the optical flow with DBSCAN (densitybased spatial clustering of applications with noise) obtains poor results; the coherent-filtering produces quite scattered pieces, reflecting motion patterns at a certain point in time; the segmentation results of meta-tracking look quite clutter, but they mix several motion patterns together. The meta-tracking can well handle the unstructured scenes for its representative ability for multiple motion patterns.\n\nAn experiment on video clips of 100 frames length shows that the average execution times of DBSCAN [68], FTLE [12], watershed [13], meta-tracking [19] and coherent-filtering [70] are around 6.3 seconds, 22.4 seconds, 5.6 seconds, 9.3 seconds and 0.4 seconds, respectively (CPU: i7-3770, 3.4GHz; memory: 8G). Here we do not consider the computation of motion features extraction. The difference in computation times lies in the fact that, FTLE and meta-tracking require motion information across the whole video sequence to generate only the motion pattern map, while DBSCAN, watershed and coherent-filtering can generate a motion pattern map with just two adjacent video frames in each iteration, which could be more efficient.\n\nTo evaluate the motion pattern segmentation results quantitatively, we manually label the detected motion regions. The numbers of true and false detections from different methods for three representative videos (Mecca for highly crowded structured scene, Roundabout for semi-structured scene, and Pedestrian for unstructured scene) are given in Table II. Moreover, due to the noise in motion clusters, the motion detection number cannot fully reflect the performance. We use completeness as an additional measurement of the segmentation accuracy. Here, completeness is the ratio of the detected motion patterns area to the ground-truth area. The results are given in Table II, for comparison. Motion pattern segmentation can also be used for source/sink seeking and path planing. We also conduct an experiment on the New York grand central station video from CHUK dataset. The test video is 30 minutes long and contains an unstructured crowded scene. Since flow field based models and coherent-filtering can not handle the extremely stochastic crowded scenes, we only evaluate two methods in this experiment: meta-tracking [19] and RFT model [7]. From the experimental results shown in Fig. 7, we can clearly see that some regular paths are extracted from mixed crowd motion patterns. Overall, results produced by the two methods are similar. The results from RFT look better, since the model can incorporate a prior knowledge from annotation (e.g. the number and positions of sources/sinks) in the training ; the second row is produced by particle flow and FTLE field segmentation [12]; the third row is produced by streak flow and watershed segmentation [13]; the fourth row is produced by coherent-filtering [70]; and the fifth row is produced by meta-tracking [19]. For figures in the first to fourth rows, different colors represent different motion patterns. For figures in the fifth row, both color and line continuity distinguish different motion patterns. (Best viewed in color)\n(a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) (m) (n)\nMetatracking Random Field Topic Fig. 7. Part of source/sink seeking results from meta-tracking [19] and RFT model [7]. Different colors in the columns represent different motion patterns, each indicating a regular path of pedestrians. (Best viewed in color) process. The meta-tracking method finds the sources/sinks via clustering, performing the whole process automatically without human intervention.\n\nThe quantitative evaluation results are shown in Table III. The number of true and false sources/sinks are used. Here, a true detection means that the algorithm finds an entry/exit point accordance with the manually labeled ground-truth; while a false detection means that the algorithm finds a wrong entry/exit point. Besides, to evaluate the generated paths, we first cluster each detected path shown in Fig. 7 into a smooth dominant trajectory using the method proposed in [69], then we compute the similarity between the dominant trajectory and the labeled path. A distance measure of longest common subsequences (LCSS) [76] is used. The LCSS distance between path F i and F j is defined as\nD LCSS (F i , F j ) = LCSS(F i , F j ) min(T i , T j )(3)\nwhere T i , T j are the lengths of F i , F j , respectively. LCSS(F i , F j ) specifies the number of matching points between two trajectories. A good algorithm should result in high similarity values. In Table III, we only consider the true detected paths and compute the average LCSS distance over them.  Note different true detection results produced by meta-tracking [19] may correspond to one ground-truth label. RFT model is referred to [7].\n\n\nE. Summary\n\nIn general, flow field model based segmentation and similarity based clustering require little human intervention. Thus motion segmentation can be performed in an unsupervised way, and it is convenient in many video analysis applications. Table I summarizes the reviewed studies on crowd motion pattern segmentation, providing the information of test dataset, applicable scene and crowd density level in the experimental settings of each method.\n\nFlow field model based methods are the most studied in motion pattern segmentation. Flow field can well simulate the crowd motions, considering the individuals as particles. Similarity based clustering methods are becoming more and more popular, because in high density crowds, the local motion features such as tracklets can be obtained more easily than the complete trajectories, and they show to be more discriminative than local optical flows. Recently, probability models, especially the topic models borrowed from language processing, have been applied to capture spatial and temporal dependency. Motion patterns in crowded scenes can be interpreted hierarchically. Individual movements constitute small group motions and they further form large motion patterns. The probability topic model may be a good choice due to its capacity to discover semantic regions and explore more details within the motion patterns.\n\nThe learned motion patterns can be used in a range of applications including path or source/sink seeking in crowded scenes, as we have shown in experiments. Besides, several tracking algorithms [38], [51], [59], [77], [78] also have interests in how to learn scene-specific motion patterns to improve the effect of tracking.\n\n\nV. CROWD BEHAVIOR RECOGNITION\n\nCrowd behavior analysis has been an active research topic in simulation and graphics fields where the main goal is to create realistic crowd motions [22]. Relatively little effort has been spent on reliable classification and understanding of human activities in real-world crowded scenes.\n\nIn general, approaches for crowd behavior analysis can be divided to \"holistic\" and \"object-based\". The former treats the crowd as a single entity, which may be suitable to structured scenes of medium or high density [9], while the later treats the crowd as a collection of individuals. In holistic approaches, crowd dynamics models are usually adopted to judge the behaviors on the whole. But local behaviors in unstructured scenes can not be handled. Object-based approaches infer both the behaviors and their associated individuals.\n\n\nA. Holistic Approach\n\nIn highly crowded surveillance scenes, moving objects in the sensor range appear small or even unresolved. Very few features can be detected and extracted from an individual object. In such situations, understanding crowd behaviors without knowing the actions of individuals is often advantageous [14].\n\nAssuming the crowd fluid is incompressible and irrotational, Mehran et al. [13] proposed a concept of potential functions, which consists of two parts: stream function and velocity function. The former provides information regarding the steady and non-divergent part of the flow, whereas the later contains information regarding the local changes in the non-curling motions. With this perspective, the potential function field is capable of discriminating lanes and divergent/convergent regions in different scenes.\n\nTo detect major motion patterns and crowd events, Benabbas et al. [21] first clustered low-level motion features to learn the direction and magnitude models of crowds, and then used a region-based segmentation algorithm to generate different motion patterns. After that, crowd events such as merge, split, walk, run, local dispersion, and evacuation were detected by analyzing the instantaneous optical flow vectors and comparing with the learned models.\n\nLater in Solmaz et al. [14], crowd behaviors including bottlenecks, fountainheads, lanes, arches, and blockings can be recognized. In this framework, a scene is represented by a grid of particles initializing a dynamical system defined by the optical flow. Time integration of the dynamical system provides particle trajectories that represent the motion in the scene. These trajectories are used to locate regions of interest in the scene. Behavior classification is obtained according to the Jacobian matrix based on linear approximation of the dynamical system. The eigenvalues are used to determine the dynamic stability of points in the flow and each type of stability corresponds to one of the five crowd behaviors.\n\nRecently, a similar spatio-temporal viscous fluid (STVF) field was adopted in Su et al. [20], to model crowd motion patterns by exploring both appearance of crowd behaviors and interaction among pedestrians. In their approach, large-scale crowd behaviors are recognized based on the characteristics of the fluid field. First, a spatio-temporal variation matrix is proposed to measure the local fluctuation for specific pixels. Then, the force among pedestrians are modeled with shear force in the spatio-temporal variation fluid field. Finally, a codebook is constructed by clustering neighboring pixels with similar spatio-temporal features, and crowd behaviors are recognized using the LDA model.\n\n\nB. Object-Based Approach\n\nIn unstructured crowded scenes, considering the crowd as one entity would fail to identify abnormal events that arise due to inappropriate actions of an individual. For instance, a running person in a crowd can indicate an abnormal event if the rest of the crowd are walking. Object-based methods may overcome this problem. Without considering the high crowd density, conventional approaches for behavior analysis are usually performed based on detection and segmentation of each individual. They suffer from the complexity to isolate individuals in the dense crowd. Addressing this, some researchers have extended this kind of approaches to highly crowd scenes, by utilizing low-level features and probability models instead of tracking a single object [8], [79]. Wang et al. [79] used hierarchical Bayesian models to connect three elements in visual surveillance: low-level visual features, simple \"atomic\" activities, and interactions. Atomic local motions are classified into atomic activities if they are observed in certain semantic regions. The global behaviors of video clips are modeled based on the distributions of lowlevel visual features, and multi-agent interactions are modeled based on the distributions of atomic activities. Without labeled training data and tracking procedure, the framework fulfils many challenging visual surveillance tasks, such as segmenting motions into different activities and supporting high-level queries on activities and interactions.\n\nLater in [8], Zhou et al. proposed a mixture model of dynamics pedestrian-agents (MDA) to learn the collective behavior patterns of pedestrians in crowded scenes. In the agentbased modeling, each pedestrian in the crowd is driven by a dynamic pedestrian-agent and the whole crowd is modeled as a mixture of dynamic pedestrian-agents. Once the model is learned from the training data, MDA can well infer the past behaviors, predict the future behaviors of pedestrians given their partially observed trajectories, and classify different pedestrian behaviors in the scene. However, some limitations exit, e.g., MDA assumes affine transform, and it has difficulty in representing some complex shapes.\n\n\nC. Summary\n\nThe principle to classify crowd behavior recognition methods depends on the perspective from which we observe the crowd: a single entity or a bunch of independent individuals. Holistic approaches are to classify the whole streams of people to normal or abnormal, or recognize the predefined crowd behaviors. This kind of methods ignore individual difference, and consider all individuals in the crowd to have similar motion characteristics. Such a hypothesis allows us to analyze the crowd states of behavior from a systematic perspective. However, without information from object detection and tracking, a particular activity cannot be separated from other activities simultaneously occurring in the same stream.\n\nIn contrast, object-based approaches are able to locate typical activities and interactions in the scene, detect normal and abnormal activities, and support high-level semantic queries on activities and interactions. However, these methods can not handle dense crowded scenes, where individual objects detection does not work, and the crowd dynamics in this area appears chaotic. Under such circumstance, the spatial distribution of low-level visual features is also chaotic, and subsequent clustering procedure will not work well. Table IV lists the representative crowd behavior recognition techniques, as well as their reported performances. A missing entry means that the quantitative result is not reported in the available literature. Besides, the crowd behaviors defined in different works are not the same. So, it is impossible to directly compare the performances of these methods, and each of the relevant studies has been conducted under different experimental conditions, using different data and different evaluation criteria.\n\n\nVI. CROWD ANOMALY DETECTION\n\nAnomaly detection is a key aspect of the crowded scene analysis, which has attracted much attention [11]- [13], [22], [60], [61], [80]- [84]. However, the problem of anomaly detection is still greatly open, and research efforts are scattered not only in approaches, but also in the interpretation of the problem, assumptions and objectives [48].\n\nCrowd anomaly detection methods could be learned on different supervision levels: from data with labels of both normal and abnormal behaviors; or from a corpus of unlabeled data, assuming that most parts are normal.\n\nDepending on the scale of interest, previous studies on anomaly detection can be categorized into two classes: global anomaly detection and local anomaly detection [61], namely \"does the scene contain an anomaly or not?\" and \"where is the anomaly taking place?\". Detailed descriptions will be given as follows.\n\n\nA. Global Anomaly Detection\n\nUsually, the self-organization effects occurring in crowds result in regular motion patterns. However, when abnormal events affecting public safety happen, such as fires, explosions, transportation disasters, people would escape, and it causes the crowd dynamics into a completely different state. Global anomaly detection aims to distinguish the abnormal states of crowd from normal ones. Related methodologies usually tend to detect the changes or events based on the apparent motion estimated on the whole. It is also important for a global anomaly detection system to not only do well in detecting the presence of anomaly in the scene, but also accurately determine the starting and end of the events, as well as the transitions between them.\n\nIt should be noticed that holistic approaches for crowd behavior recognition mentioned in Section V, such as [13], [14], [20], [21], can be applied for global crowd anomalies detection. There also exist some works specifically for anomaly detection, in global style.\n\nIn Chen et al. [85], each isolated region is considered as a vertex and a human crowd is represented with a graph. To effectively model the topology variations, local characteristics (e.g. triangle deformations and eigenvalue-based subgraph analysis), and global features (e.g. moments) are used. They are finally combined as an indicator to detect if any anomaly of the crowd presents in the scene.\n\nRecently, a Bayesian framework for crowd escape behavior detection in videos was proposed [86], to directly model crowd motions as non-escape and escape. Crowd motions are characterized using optical flow fields, and the associated classconditional probability density functions are constructed based on the field attributes. Crowd escape behavior can be detected by a Bayesian formulation. Experiments demonstrated that the method is more accurate than state-of-the-art techniques in detecting crowd escape behavior. However, this method cannot be applied to high density crowded scenes for the moment, since the crowd escape behavior in this case is significantly different from that in low or medium density crowded scenes.\n\n\nB. Local Anomaly Detection\n\nIn addition to global anomaly detection, we often need to know where the anomaly event happens. To this aim, various local anomaly detection techniques have been proposed. Popular models from both crowd dynamics (e.g., flow field model, social force model, and crowd energy model) and vision area (hidden Markov model, dynamic texture, bag-of-words, sparse representation, and manifold learning) have been applied here.\n\nWe divide the methods into two classes: vision-based approaches that learn model and predict anomaly purely using techniques from computer vision area, based on visual features; and physics-inspired approaches that incorporate related physical models for crowd dynamics representation, and achieve anomaly detection with learning methods.\n\n1) Vision-Based Approach: Many machine learning techniques have achieved great success in vision tasks. They have also been applied in local anomaly detection. These methods usually extract visual features and construct a set of clusters to represent several possible event patterns. erage KL distance to the mean. Thus we capture the characteristic motion of a scene by identifying the motion variations within each prototypical pattern, compactly representing the rich information stored in the local spatio-temporal motion patterns. Now that we have a characteristic representation of the motion within a scene, we may potentially model an extremely crowded scene given a training video sequence of usual activity, and detect unusual activities in a query video by identifying local spatio-temporal motion patterns with low likelihoods. Given a local spatio-temporal motion pattern O n t , we can evaluate the probability of it belonging to a specific prototypical observation using the KL distance to remove the bias. Thus the probability of an observation O n t given prototype s is\np (O n t |s) = p d (O n t , P s ) \u03c3 s \u223c N (0, 1) ,(4)\nwhere d is the KL distance measure. Since motion patterns that occur regularly in one spatial location of the video may be unusual in another, observations are only evaluated for prototypical distributions that occurred in the same spatial location n, or tube, in the training video. We compute the confidence measure for each cuboid as the maximum likelihood given the possible prototypical distributions within a tube. We then identify unusual motion patterns by thresholding low confidence values. Since extremely crowded scenes may contain larger variations in one location than another, we normalize the measure by the minimum confidence value of the training set in each spatial location n.\n\n\nCapturing Temporal Statistics in Distribution Based Hidden Markov Models\n\nWhile the set of prototypes provides a picture of similar activities in the scene, it does not capture the relationship between their occurrences. As a result, we cannot assume that the approach in the previous section will lead to robust detection of unusual activities. We will now consider modeling and leveraging the temporal dynamics of the motion Ord {H, o, the pos probab and A HMM cation is a con plex ob this wo in each the pro the em is crea functio volume main c the tem mainta do not prototy estimat and \u03c0 n The HMM algorith individ tempor cuboid dictive Our tem Fig. 8. The temporal relationship between local motion patterns is encoded in a distribution-based HMM at each spatial location. Originally shown in [60]. a) Hidden Markov Model: The hidden Markov model (HMM) is able to take into account the inherently dynamic nature of the observed features [48]. It is applicable in video event detection as well as anomaly detection.\n\nBased on HMM, Kratz et al. [60] have presented a framework for modeling local spatio-temporal motion behaviors in extremely crowded scenes. Fig. 8 illustrates a single HMM for each spatial location of observation.\n\nIn the training phase, the temporal relationship between local motion patterns is captured via a distribution-based HMM, and the spatial relationship is modeled by a coupled HMM. In the testing phase, unusual events are identified as statistical deviations in video sequences of the same scene. The experimental results indicated that the proposed representation is suitable for analyzing extremely crowded scenes. However, the authors only set up one HMM for each local area, so that the method could work only for limited kinds of normal behaviors or specific crowded scenes. If we change the normal behavior type, the detection rate of the abnormal behaviors will decrease, unless the model is re-trained.\n\nA similar scheme has been proposed in Wang et al. [81]. In their approach, the high-frequency and spatio-temporal (HFST) information is computed by the wavelet transformation, to characterize the dynamic properties of the local region. Then, in order to detect various local abnormal crowd events, multiple HMMs are adopted, and each HMM accounts for a type of behavior. b) Dynamic Texture Model: The dynamic texture [90] is a spatio-temporal generative model for video. It represents video sequences as observations from a linear dynamical system, and exhibits spatio-temporal stationary properties [91]. Recent research works [80], [87] have shown that dynamic texture is more suitable for local unusual event detection in crowded scenes than optical flow.\n\nOriginally proposed for motion segmentation in Chan et al. [92], the mixture of dynamic texture (MDT) is a generative model, where a collection of video sequences are modeled as samples from a set of underlying dynamic textures. Fig.  9 illustrates the MDT from a video patch. Based on MDT, Li et al. [80], [87] proposed a joint detector of temporal and spatial anomalies in crowded scenes. The proposed detector is based on a video representation that accounts for both appearance and dynamics, using a set of MDT models. The normal patterns is learned through MDT model per scene  the spatial support of anomaly detection, for both spatial and temporal anomalies.\n\n\nNORMALCY AND ANOMALY MODELING\n\nIn this section we review the MDT model, discuss the intensities. Observations of low probability under these GMMs are declared foreground. For anomaly detection in crowds, the GMM is replaced by an MDT, and the pixel grid replaced by one of preset displacement. Grid locations define the center of video cells, from which video patches are extracted. The patches extracted from a subregion (group of cells) are used to learn an MDT, during a training phase, as illustrated in Fig. 1. After this phase, subregion patches of low probability under the associated MDT are considered anomalies. Given patch x 1:\u03c4 , the distribution of the hidden state sequence s \u03c4 1 under the i th DT component, p S|X (s 1:\u03c4 |x 1:\u03c4 , z = i), is estimated with a Kalman filter and smoother [29], [30], as discussed in [28]. The value of the temporal anomaly map at location l is the negative-log probability of the most-likely state sequence for the patch at l\nT (l) = \u2212 log K i=1 \u03c0 i p(s {i} 1:\u03c4 (l)|z = i) ,(3)\nwhere s {i} 1:\u03c4 (l) = argmax s 1:\u03c4 p(s 1:\u03c4 |x \u03c4 (l), z = i). We note that this generalizes the mixture of PCA models of optical flow [3]. The matrix C z of (2b) is a PCA basis for patches drawn from mixture component z, but the PCA decomposition reports to patch appearance, not subregion, in training phase. A multi-scale temporal anomaly map is produced by measuring the negative log probability of each video patch under the MDT of the corresponding region. In the testing phase, subregion patches of low probability under the associated MDT are considered as anomalies.\n\nc) Bag-of-Words Model: One representative approach in anomaly detection is to use local spatio-temporal video volumes based bag-of-words (BOW) models. This approach usually extracts local low-level visual features, such as motion and texture, either by constructing a pixel-level background model and behavior templates, or by employing spatio-temporal video volumes [82].\n\nIn [82], Roshtkhari et al. extended BOW model for detecting suspicious events in videos. The method codes a video as a compact set of spatio-temporal volumes. Uncertainty is considered in the codebook construction. The spatio-temporal compositions of video volumes are modeled using a probabilistic framework, and anomalous events are assumed to be video presentations with very low frequency of occurrence. As a result, an observation is considered to be abnormal if it cannot be reconstructed with previous observations. LDA has been adopted in Wang et al. [93] based on the spatio-temporal cuboid from the video sequence with an adaptive size. To compute the similarity between two spatiotemporal cuboids with different sizes, they designed a novel data structure of codebook constructed as a set of two-level trees. LDA model is used to learn an appropriate number of topics to represent these scenarios. A new sample will be classified as anomaly if it does not belong to these topics. d) Sparse Representation Models: Recently, Cong et al. [61] presented a novel algorithm for abnormal event detection based on the sparse reconstruction cost (SRC) for multi-level histogram of optical flows. Given an image sequence or a collection of local spatio-temporal patches, MHOF features are calculated. Then, SRC over the normal dictionary is used to measure the normalness of the testing sample. By introducing a prior weight of each basis during sparse reconstruction, the proposed SRC is more robust than other outlier detection criteria.\n\nCombined with dynamic texture, Xu et al. [91] proposed a novel approach for unusual event detection via sparse reconstruction on an over-complete basis set. The dynamic texture is described by local binary patterns from three orthogonal planes (LBPTOP). In the detection process, given the basis set learned from the training procedure and the input observation, the sparse coefficients are computed and the reconstruction error is defined. The unusual events are identified as those dynamic textures with high reconstruction error. e) Manifold Learning Model: In [83], the manifold learning-based framework has also been applied for the detection of anomalies in a crowded scene. The spatiotemporal Lagrangian eigenmap method is employed to study the local motion structure of the scene. Besides, a pairwise graph is constructed by considering the visual context of multiple local patches in both spatial and temporal domains. Such a process embeds local motion patterns into different spatial locations where similar patterns are usually close and different patterns are far apart. This allows to cluster embedding points and to discover different motion patterns in the scene. Finally, a local probability model is used to localize the abnormal regions in the crowded scene, where the clusters with small data points or outliers in the embedded space can be considered as abnormal.\n\n2) Physics-Inspired Approach: Several physics-inspired models have been proposed for crowd representation, and they have also been utilized and combined with machine learning techniques for anomaly detection. For example, continuumbased approach and agent-based approach from crowd simulation field both have been adopted for anomaly detection in crowded scenes. a) Flow Field Model: Usually, we need to understand how crowds evolve with time and try to find some regular patterns. So that we can know immediately where and how the motion pattern of the crowd changes. As noted in section IV, the work of Ali et al. [12] has shown success on motion pattern segmentation. Furthermore, they also extended their framework to anomaly detection. They constructed a finite time Lyapunov exponent (FTLE) field whose boundaries vary with the crowd changes in terms of the dynamic behavior of the flow. New Lagrangian coherent structures (LCS) [57] will appear in the FTLE field exactly at those locations where the changes happen. Any change in the number of flow segments over time is regarded as an instability, and it is detected by establishing correspondences between flow segments over time.\n\nWu et al. [11] proposed a method for crowd flow modeling and anomaly detection for both structured and unstructured scenes. The overall work-flow begins with particle advection based on optical flow, and particle trajectories are clustered to obtain representative trajectories for a crowd flow. Next, the chaotic dynamics of all representative trajectories are extracted and quantified using chaotic invariants. This is known as maximal Lyapunov exponent and correlation dimension in dynamic system. Probability model is learned from these chaotic feature sets. Finally, a maximum likelihood estimation criterion is adopted to identify a query video of a scene as normal or abnormal.\n\nCommencing with the optical flow field estimation adapted from [94], Loy et al. [95] presented a global motion saliency detection framework. The associated flow vector in the field is represented by its phase angle \u2212\u03c0 \u2264 \u03d5 x,y,t \u2264 \u03c0 and the velocity magnitude \u03b3 x,y,t \u2265 0, referred as motion signature for salient motion detection. Then the spectral residual approach [96] is applied on the motion signature for motion saliency detection. This method has shown its potential in unstable region detection in extremely crowded scenes, and gave fairly similar results to [12].\n\nb) Social Force Model: The social force model (SFM) has been successfully employed in research fields as simulation and analysis of crowds. Mehran et al. [22] introduced a novel method to detect and localize abnormal behaviors in crowd videos using SFM [45]. For this purpose, the framework proposed by Ali et al. [12] is utilized to compute particle flows, and their interaction forces are estimated using SFM. The interaction force is then mapped into the image plane to obtain force flow for every pixel in every frame. Randomly selected spatio-temporal volumes of force flows are used to model the normal behavior of the crowd. Finally the frames are classified as normal or abnormal by using BOW. The regions of anomalies in the abnormal frames are localized using interaction forces.\n\nInspired by [22], some methods based on SFM to detect abnormal crowd behaviors were later proposed. In [46], [97], Raghavendra et al. introduced the particle swarm optimization (PSO) method for optimizing the interaction force computed using SFM. The main objective of the proposed method is to drift the population of particles towards the areas of the main image motion. Such displacement is driven by the PSO fitness function, which aims at minimizing the interaction force, so as to model the most diffused and typical crowd behavior.\n\nA velocity-field based SFM has been proposed in Zhao et al. [47] to locate crowd behavior instability spatio-temporally. The traditional SFM defines the interaction force as a dependent variable of relative geometric position of the individuals. Differently, the proposed improved model can provide a better prediction of interactions using the collision probability in a dynamic crowd. With spatio-temporal instability analysis, we can extract video clips with potential abnormality and locate the regions of interest where the abnormalities are likely to happen.\n\nc) Crowd Energy Model: The crowd has its own characteristics. For example, local density and velocity are key parameters for measuring the crowd dynamics. Yang et al. [88] proposed an efficient method based on the histogram of oriented pressure (HOP) to detect crowd anomaly. SFM and local binary pattern (LBP) are adopted to calculate the pressure. Cross histogram is utilized to produce the feature vector instead of parallel merging the magnitude histogram and direction histogram. Afterwards, support vector machine and median filter are adopted to detect the anomaly.\n\nIn [98], Xiong et al. proposed a novel method to detect two typical abnormal activities: pedestrian gathering and running. The method is based on the potential energy [99] and kinetic energy. A term called crowd distribution index (CDI) is defined to represent the dispersion, which can later determine the kinetic energy. Finally the abnormal activities are detected through threshold analysis.\n\nAnother abnormal crowd behavior detection model using behavior entropy has been proposed in [89]. The key idea is to analyze the change of scene behavior entropy (SBE) over time, and localize abnormal behaviors according to pixels' behavior entropy distribution in image space. Experiments reveal that SBE of the frame will rise when running, dispersion, gathering or regressive walking occurs.\n\nThis energy-based model can well denote the dispersion on different directions and locate moving information and interacting information among individuals. The model works owing to the fact that obvious differences exist between normal states and abnormal states in crowd dynamic characteristics. Usually some threshold-based methods are employed here, and the threshold usually has to be determined empirically when applied to different crowd scenes.\n\n\nC. Summary\n\nIt is usually difficult to compare different methods objectively, since anomalies are often defined in a somewhat subjective form, sometimes according to what the algorithms can detect [87]. We make a brief comparison of recently developed anomaly detection techniques in Table V. These techniques are evaluated on different datasets with different criteria. It is hard to compare them directly, and this table intends to provide a quick way to understand the solutions on the whole.\n\nMethods based on knowledge from physical systems for crowd representation are convenient to apply but their capacity is limited to recognize certain patterns [100]. In order to overcome these limitations, data-driven learning-based models can be utilized or be combined to represent the events and structures of the scene. Among them, generative topic models seem to be promising in the interested area [82], [93]. The topic models share a fundamental idea that \"a crowded scene with its various events can be simulated as a document with mixture of topics\". Characterizing unusual events by low word-topic probabilities far from existing typical topics, the topic models have the ability to automatically discover meaningful events or activities from the co-occurrences of visual words.\n\nIn addition, recently, some techniques in video event detection, such as spatio-temporal path searching [101] and background substraction [102] have been extended to crowded scenes. They can be utilized for improving crowd event detection.\n\n\nVII. CROWD VIDEO DATASETS\n\nWith the development of crowded scene analysis, several crowd datasets are available now. In the following, we will list some existing benchmark crowd video datasets. In Table   VI, the following information is given: brief descriptions, size of each database, the labeling level and the accessibility. UCF Crowd Dataset [12] This crowd dataset is collected mainly from the BBC Motion Gallery and Getty Images website, and it is publicly available. The most distinguishing feature is its variations in lighting and field of view, which can facilitate the performance evaluation of algorithms developed for crowded scenes.\n\nUMN Crowd Dataset [103] It is also a publicly available dataset containing normal and abnormal crowd videos from University of Minnesota. Each video consists of an initial part of a normal behavior and ends with sequences of the abnormal behavior.\n\nUCSD Anomaly Detection Dataset [104] This dataset was acquired with a stationary camera mounted at an elevation, overlooking pedestrian walkways. The crowd density in the walkways ranges from sparse to very crowded. Abnormal events are caused by either: (i) the circulation of non pedestrian entities in the walkways or (ii) anomalous pedestrian motion patterns.\n\nViolent-Flows Dataset [107] It is a dataset of real-world video footages of crowd violence, along with standard benchmark protocols designed to test both violent/non-violent classification and violence outbreak detection. All the videos were downloaded from YouTube, and the average length of a video clip is 3.60 seconds.\n\nCUHK Dataset [79] This dataset is for research on activity or behavior analysis in crowded scenes. It includes 2 subsets: a traffic dataset (MIT traffic) and a pedestrian dataset. The traffic dataset includes a traffic video sequence of 90 minutes long. Ground truths about pedestrians of some sampled frames are manually labeled. The pedestrian dataset were recorded in New York's grand center station, contain a 30 minutes long video sequence, without giving any ground truth or labeled data.\n\nQMUL Dataset [105] This dataset has two subsets: the first contains three different dense traffic flow videos at crossroads, of nearly 60 minutes long; the second contains a video of shopping mall from a publicly accessible webcam. Over 60,000 pedestrians were labeled in 2000 frames, and the head positions of every pedestrians are labeled, making this dataset more convenient for crowd counting and profiling research. PETS2009 Dataset [106] This dataset contains multi-sensor sequences of different crowd activities. It is composed of five parts: (i) calibration data, (ii) training data, (iii) person count and density estimation data, (iv) people tracking data, and (v) flow analysis and event recognition data. Each subset contains several sequences and each sequence contains different views (4 up to 8).\n\nRodriguez's Web-Collected Dataset [78] The dataset reported in was collected by crawling and downloading videos from search engines and stock footage websites (e.g., Gettyimages and YouTube). In addition to the large collection of crowd videos, the dataset contains ground-truth trajectories for 100 individuals, which were selected randomly from the set of all moving people. This dataset is not open to the public yet.\n\nUCF Crowd Behavior Dataset [14] This dataset is col-  [14] image sequences from the web videos and PETS2009 61 sequences All Yes lected from the web (Getty-Images, BBC Motion Gallery, Youtube, Thought Equity) and PETS2009, representing crowd and traffic scenes. It is publicly available in the form of image sequences. Differently from [12], it is mainly designed for crowd behaviors recognition, with ground-truth labels.\n\n\nVIII. CONCLUSIONS AND FUTURE DEVELOPMENTS\n\nWe have presented a review of the state-of-the-art techniques for crowded scene analysis across three key aspects: motion pattern segmentation, crowd behavior recognition and anomaly detection. The interested problems have become active research areas in recent decades because of their promising real-world applications. It can be seen that anomaly detection in crowded scenes has attracted quite a lot efforts, which reflects its importance in applications. Similar to these subtopics, feature representation is put as another important section deserving detailed descriptions, since feature representation, as an indispensable basis, is highly correlated with each of the three subtopics. Furthermore, available knowledge of crowd from areas such as crowd dynamics is summarized beforehand. It could provide the fundamental crowd models for many scene analysis algorithms.\n\nAlthough a variety of representation approaches and models have been proposed, at the moment there is still no generally accepted solution to crowded scene analysis tasks. When retrospect the surveyed literatures, we can see two common models promising to visually characterize the crowded scenes: flow field model inspired by physics and generative topic model from machine learning. For flow field model, the crowd is treated as physical fluid and particles, and high density crowd behaves like a complex dynamic system. Many dynamical crowd evolution models have been proposed, along with the concepts of motion field and dynamical potential, borrowed from fluid dynamics community. By treating the moving crowd as a time dependent flow field which consists of regions with qualitatively different dynamics, the motion patterns emerging from the spatiotemporal interactions of the participants can be reflected.\n\nFor topic model, the basic idea is \"a crowded scenes with its various events is a document with mixture of topics\". It could be along with different statistical assumptions. The topic model has the ability to automatically discover meaningful events or activities from the visual word co-occurrences. Moreover, it is flexible to utilize various features and other learning algorithms.\n\nAlthough a large amount of works has been done, many issues in crowded scene analysis are still open, and they deserve further research. In the following we list some of the promising topics.\n\nMulti-Sensor Information Fusion Crowded scenes often contain severe clutter and object occlusions, which are quite challenging for current visual-based techniques. To fuse information from multi-sensors is always an effective way to reduce the confusion and to improve the accuracy [6]. Visual surveillance of crowded scenes could greatly benefit from the use of multiple sensors, such as audio, radar and laser. Multicamera contexts could also be explored, as revealed by a recent work on group activity analysis [108]. By combining the multisensor data, different forms of information can complement to each other, and facilitate the system to obtain an accurate and comprehensive understanding of the scene.\n\nTracking-Learning-Detection Framework Many current video analysis systems perform tracking, learning and detection by simple integration, without considering the interactions between the functional modules. To fully utilize the hierarchical contextual information, it is better for crowded scene analysis systems to simultaneously perform tracking, model learning, and behavior detection in a fully online and unified way. Some works on video surveillance have shown the advantages of the unified framework [109], [110], which should require attention.\n\nThe tracking module provides motion features, based on which crowd models are learned, and behaviors or events can be detected. On the other hand, crowded scene knowledge can facilitate accurate individuals tracking. For example, a person in particular crowd flow is always influenced by the global motions. Moreover, events or activities can be detected on the basis of tracking results and the learned models. They could also provide contextual knowledge for tracking and learning. A unified tracking-learning-detection framework can use all these contexts to improve these components simultaneously.\n\nDeep Learning for Crowded Scene Analysis Though various methods have been proposed on feature extraction and model learning in crowded scene analysis, still there is no public accepted crowded scene representation currently. In recent years, deep learning has achieved great success in several vision tasks related to visual surveillance and scene analysis [111], [112]. It has demonstrated its representation learning ability from multiple features. It also could be a prospective solution in crowded scene analysis, given enough training data. How to designate the framework and utilize the power of deep learning in tasks of crowded scene analysis deserve our future efforts. Real-time Processing and Generalization As such an area driven by practical applications, the real-time computation must be considered for the algorithms to work in real life. Current solutions usually target at accurate scene understanding, without considering the computation. Furthermore, many studies in the literature typically evaluate on video data of a specified condition. Research on the effective methods, which can well handle more generalized situations, could also be valuable.\n\n\nThis work is supported by the National Natural Science Foundation (NSF) of China (No. 61300056, 61272393, 61322201), the Anhui Provincial Natural Science Foundation of China (No. 1408085QF118) and the Open Project Program of the National Laboratory of Pattern Recognition (NLPR) (No. 201306282). This work is also supported in part by the National 973 Program of China (No. 2014CB347600), and a research grant for the Human Sixth Sense Programme at the Advanced Digital Sciences Center from Singapores Agency for Science, Technology and Research (A*STAR).\n\nFig. 1 .\n1Examples of crowded scenes: (a) a stage of the tour de france (structured scene); (b) people gathered in a square (unstructured scene).\n\nFig. 2 .\n2General structure of crowded scene analysis, also illustrating the main issues discussed in this survey.\n\nFig. 3 .\n3An exemplar visualization of the forces and velocities in the SFM.\n\nFig. 4 .\n4(a) An visualization of the optical flow feature: the rotating sphere generates the optical flow field shown in the middle. (b) (c) (d) compare the change of the feature vectors in time period t = 0 to t = 18 of optical flow, particle flow, and streak flow, respectively. (b) (c) (d) are originally shown in[13].\n\nFigure 4 .\n4Motion histograms showing (A) horizontal, (B) diagonal right, and (C) bimodal motion (upper left and upper right).\n\nFig. 5 .\n5Motion histograms of the three pixels with different moving directions: (A) horizontal, (B) diagonal right, and (C) bimodal motion (upper left and upper right).Figure originallyshown in[19].\n\nFig. 6 .\n6Motion segmentation results from the selected five methods. The first row is produced by optical flow with DBSCAN[68]\n\nFigure 2 .\n2The temporal relationship between local spatio-temporal motion patterns is encoded in a distribution-based HMM at each spatial location.\n\n\nFigure 3 tempora spatially\n\nFig. 1 .\n1Temporal anomaly detection. An MDT is learned per scene subregion, at training time. A temporal anomaly map is produced by measuring the negative log probability of each video patch under the MDT of the corresponding region.\n\nFig. 9 .\n9MDT for temporal abnormality detection. For each region of the scene, an MDT is learned. Originally shown in[87].\n\n\nand H. Chang are with Anhui University, Hefei 230601, P. R. China (email: tenglwy@gmail.com; changhuan@ahu.edu.cn); M. Wang and R. Hong are with Hefei University of Technology (email: eric.mengwang@gmail.com; hongrc.hfut@gmail.com); B. Ni is with Advanced Digital Sciences Center, 1 Fusionoplis Way, #08-10 Connexis North Tower, 138632, Singapore (email: bingbing.ni@adsc.com.sg); S. Yan is with National University of Singapore, Singapore 117576 (email: eleyans@nus.edu.sg).\n\nTABLE I A\nIBRIEF SUMMARY OF MOTION PATTERN ANALYSIS TECHNIQUESReferences \nDatasets \nFeatures \nApplicable Scenes \nDensity level \nFlow Field Model Based Segmentation \n\nAli et al. 2007 [12] \nUCF \nParticle Flow \nStructured \nHigh \n\nHu et al. 2008 [54] \nUCF \nOptical Flow \nStructured \nHigh \n\nMehren et al. 2010 [13] \nUCF \nStreak Flow \nStructured \nHigh \n\nWu et al. 2012 [67] \nUCF / UCSD \nOptical Flow \nStructured \nHigh / Medium / Low \n\nHe et al. 2012 [68] \nUCF \nOptical Flow \nStructured \nMedium / Low \n\nSimilarity Based Clustering \n\nCheriyadat et al. 2008 [69] \n[69] \nOptical Flow \nStructured \nLow \n\nZhou et al. 2012 [70] \nUCF / [70] \nTracklet \nStructured / Unstructured \nHigh / Medium \n\nWang et al. 2013 [65] \nUCF \nTracklet \nStructured / Unstructured \nHigh / Medium \n\nJodoin et al. 2013 [19] \nMIT Traffic / UCF / [19] \nMotion Histogram \nStructured / Unstructured \nHigh / Medium / Low \n\nProbability Model Based Clustering \n\nYang et al. 2009 [4] \nMIT Traffic \nOptical Flow \nStructured \nLow \n\nSaleemi et al. 2010 [3] \nMIT Traffic \nOptical Flow \nStructured \nMedium / Low \n\nSong et al. 2011 [71] \nMIT Traffic \nOptical Flow \nStructured / Unstructured \nLow \n\nZhou et al. 2011 [7] \nCUHK \nTracklet \nStructured / Unstructured \nMedium \n\nFu et al. 2012 [72] \nUCF / QMUL \nMotion Histogram \nStructured / Unstructured \nHigh / Medium / Low \n\n\n\nTABLE II MOTION\nIIPATTERN SEGMENTATION RESULTSScene \nMethod \nLabels \nTrue \nFalse \nComplete-\nNumber Detections Detections \nness \n\nMecca \n\n[68] \n2 \n0.67 \n1.0 \n0.65 \n\n[12] \n2 \n0.5 \n1.0 \n0.96 \n\n[13] \n2 \n0.4 \n1.0 \n0.97 \n\n[70] \n2 \n1.0 \n1.0 \n0.63 \n\n[19] \n2 \n0.67 \n1.0 \n0.92 \n\nRoundabout \n\n[68] \n7 \n1.0 \n0.57 \n0.74 \n\n[12] \n7 \n1.0 \n0.43 \n0.94 \n\n[13] \n7 \n1.0 \n0.43 \n0.74 \n\n[70] \n7 \n0.8 \n0.57 \n0.83 \n\n[19] \n7 \n1.0 \n0.72 \n0.96 \n\nPedestrian \n\n[68] \n8 \n0.44 \n0.5 \n0.68 \n\n[12] \n8 \n1.0 \n0.125 \n0.75 \n\n[13] \n8 \n1.0 \n0.125 \n0.72 \n\n[70] \n8 \n0.5 \n0.625 \n0.70 \n\n[19] \n8 \n0.6 \n0.75 \n0.83 \n\n\nTABLE III\nIIISOURCE/SINK SEEKING AND PATH PLANING RESULTSMethods \nLabels \nTrue \nFalse \nLCSS \nNumber \nDetections Detections Distance \n\nMeta-tracking \n16 \n22 \n7 \n0.73 \n\nRFT model \n16 \n16 \n1 \n0.91 \n\n\n\nTABLE IV COMPARISON\nIVOF CROWD BEHAVIOR RECOGNITION TECHNIQUESReference \nDataset \nMethod \nPerformance \nAUC \nACC \nDR \nHolistic Approach \n\nMehran 2010 [13] \nUMN / [13] \nStreak flow field + Potential field \n\u2248 0.848 \nN/A \n\u2248 97.0% \nBenabbas 2011 [21] \nPETS2009 / [21] \nOptical flow field + Block clustering \nN/A \u2248 77.0% \nN/A \n\nSolmaz 2012 [14] \nPETS2009 / [14] \nEigenvalue analysis + Stability analysis \n\u2248 0.825 \n\u2248 71.2% \n\u2248 85.3% \nSu 2013 [20] PETS2009 / UMN \nSTVF + LDA \n0.873 \n87.9% \nN/A \n\nObject-Based Approach \n\nWang 2009 [79] \n[79] \nHierarchical Bayesian Models \nN/A \n85.7% \nN/A \n\nZhou 2012 [8] \n[8] \nMDA \nN/A \nN/A \nN/A \n\n1 AUC: Area Under ROC Curve, ACC: Accuracy, DR: Detection Rate, N/A: Not Available \n2 Note that the ACC result of [79] is on the video segmentation task for different types of interactions; in [8], the average prediction error \nof individual behavior is shown to be \u2248 68 pixels/140 frames. \n\n\n\nTABLE V COMPARISON\nVOF ANOMALY DETECTION TECHNIQUESReference \nDataset \nMethod \nApplication \nPerformance \nGlobal Local \nAUC \nACC \nDR \nERR \nVision-Based Approach \n\nMahadevan 2010 [80] \nUCSD \nMDT \n\u221a \n\u2248 0.735 \n75% \nN/A \n25% \n\nBenabbas 2011 [21] PETS2009 / CUHK \nOptical flow + Block \nclustering \n\n\u221a \nN/A \n\u2248 77% \nN/A \nN/A \n\nWang 2012 [81] \nUMN \nHSFT + HMMs \n\u221a \n\u2248 0.900 \nN/A \n85% \nN/A \n\nCong 2012 [61] \nUMN / UCSD \nSRC + LSDS \n\u221a \n\u221a \nG: 0.980 \nG: N/A G: N/A \n20% \n\nL: 0.478 \nL: 46% L: 46% \n\nCong 2013 [84] \nUCSD \nSTMC + DPG \n\u221a \n0.868 \nN/A \nN/A 23.9% \n\nRoshtkhari 2013 [82] \nUCSD \nSTC + BOW \n\u221a \n\u2248 0.879 \nN/A \n95.8% 20.5% \n\nThida 2013 [83] \nUMN / UCSD \nSLE \n\u221a \n\u2248 0.977 \nN/A \nN/A 17.8% \n\nLi 2013 [87] \nUMN / [87] \nMDT \n\u221a \n0.995 \nN/A \nN/A \n3.7% \n\nChen 2013 [85] \nUMN \nEigenvalue-based \ngraph analysis \n\n\u221a \nN/A \nN/A \n91% \nN/A \n\nWu 2014 [86] \nUMN / PETS2009 \nOptical flow + Bayes \nClassification \n\n\u221a \nN/A \u2248 91.57% \nN/A \nN/A \n\nPhysics-Inspired Approach \n\nMehran 2009 [22] \nUMN / [22] \nSFM + LDA \n\u221a \n0.960 \nN/A \nN/A \nN/A \n\nWu 2010 [11] \nUMN \nChaotic invariance \n\u221a \n0.990 \nN/A \nN/A \nN/A \n\nRaghavendra 2011 [46] \nUCSD \nParticle flow + \nPSO-SFM \n\n\u221a \n0.875 \nN/A \n52% 17.0% \n\nZhao 2011 [47] \nUMN \nVelocity-field-based \nSFM \n\n\u221a \n\u2248 0.940 \nN/A \nN/A \nN/A \n\nYang 2012 [88] \nUMN \nLocal pressure model \n\u221a \n\u2248 0.975 \nN/A \nN/A \nN/A \n\nRen 2012 [89] \nUMN / UCSD \nBehavior entropy \nmodel \n\n\u221a \n\u221a \n0.893 \nN/A \nN/A \nN/A \n\n1 AUC: Area Under ROC Curve, ACC: Accuracy, DR: Detection Rate, ERR: Equal Error Rate, N/A: Not Available, G: Global Detection, L: \nLocalization, DPG: Dynamic Path Grouping, HSFT: High-Frequency and Spatio-Temporal Feature, SLE: Spatio-temporal Laplacian Eigenmap, \nSTMC: Spatio-Temporal Motion Context, STC: Spatio-Temporal Compositions, \n4 \nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE \n\n# \n\n\n+ \n+ + \n+ + \n+ \n+ + \n+ + \n\n+ \n+ + + \n+ \n\n+ + \n+ \n+ \n+ \n\n+ \n+ \n+ \n+ + \n\n+ \n\n+ \n\n+ \n\n+ \n\n+ \n\n+ \n\n+ \n+ \n+ \n+ + \n\n+ \n\n+ \n\n+ \n\n+ \n\n+ \n+ \n+ \n+ \n+ + \n+ + \n+ \n+ \n\n+ \n+ \n+ \n\n+ \n\n+ \n\n+ \n+ \n\n+ \n\n+ \n+ + + \n+ \n\n+ \n\n+ \n\n+ \n\n+ \n+ + \n+ \n+ \n+ + \n\n+ \n\n+ \n\n+ \n\n+ \n+ + \n+ \n+ \n+ \n+ \n+ + \n\n+ \n\n+ \n+ \n+ \n\n+ \n+ \n\n+ \n+ \n+ \n+ \n\n+ \n\n+ \n\n+ \n\n+ \n\n+ \n\n+ \n\n+ \n\n+ \n\n+ + \n\n+ \n\n+ \n+ \n\n! \n\n# \n# \n\n! \n\n\n# \n\n\n! \n\n\n# \n\n\u2212 log p \n\n\n\nTABLE VI DATASETS\nVIFOR CROWDED SCENE ANALYSIS UCF [12] Videos of crowds, vehicle flows and other high density moving objects 38 videos Partial Yes UMN [103] Scenarios of an escape event in 3 different indoor and outdoor scenes 11 videos All Yes UCSD [104] Subset1: 34 training video clips and 36 testing video clips 98 video clips All Yes Subset2: 16 training video clips and 12 testing video clipsReference \nDescription \nSize \nLabel Accessibility \n\nCUHK [79] 1 traffic video sequence and 1 crowd video sequence \n2 videos \nPartial \nYes \n\nQMUL [105] 3 traffic video sequence and 1 pedestrians video sequence \n4 videos \nPartial \nYes \n\nPETS2009 [106] 8 video sequences of different crowd activities with calibration data \n8 videos \nAll \nYes \n\nVIOLENT-FLOWS [107] Real-world video sequences of crowd violence \n246 videos \nPartial \nYes \n\nRodriguez's [78] Large collection of crowd videos with 100 labeled object trajectories \n520 videos \nPartial \nNo \n\nUCF Behavior \n\nVideo behaviour mining using a dynamic topic model. T Hospedales, S Gong, T Xiang, International Journal of Computer Vision. 983T. Hospedales, S. Gong, and T. Xiang, \"Video behaviour mining using a dynamic topic model,\" International Journal of Computer Vision, vol. 98, no. 3, pp. 303-323, 2012.\n\nHow effective is human video surveillance performance?. N Sulman, T Sanocki, D Goldgof, R Kasturi, IEEE Internation Conference on Pattern Recognition. N. Sulman, T. Sanocki, D. Goldgof, and R. Kasturi, \"How effective is human video surveillance performance?\" in IEEE Internation Confer- ence on Pattern Recognition, 2008, pp. 1-3.\n\nScene understanding by statistical modeling of motion patterns. I Saleemi, L Hartung, M Shah, IEEE Conference on Computer Vision and Pattern Recognition. I. Saleemi, L. Hartung, and M. Shah, \"Scene understanding by statis- tical modeling of motion patterns,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 2069-2076.\n\nVideo scene understanding using multiscale analysis. Y Yang, J Liu, M Shah, IEEE International Conference on Computer Vision. Y. Yang, J. Liu, and M. Shah, \"Video scene understanding using multi- scale analysis,\" in IEEE International Conference on Computer Vision, 2009, pp. 1669-1676.\n\nA system for learning statistical motion patterns. W Hu, X Xiao, Z Fu, D Xie, T Tan, S Maybank, IEEE Transactions onPattern Analysis and Machine Intelligence. 289W. Hu, X. Xiao, Z. Fu, D. Xie, T. Tan, and S. Maybank, \"A system for learning statistical motion patterns,\" IEEE Transactions onPattern Analysis and Machine Intelligence, vol. 28, no. 9, pp. 1450-1464, 2006.\n\nA survey on visual surveillance of object motion and behaviors. W Hu, T Tan, L Wang, S Maybank, IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews. 343W. Hu, T. Tan, L. Wang, and S. Maybank, \"A survey on visual surveillance of object motion and behaviors,\" IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. 34, no. 3, pp. 334-352, 2004.\n\nRandom field topic model for semantic region analysis in crowded scenes from tracklets. B Zhou, X Wang, X Tang, IEEE Conference on Computer Vision and Pattern Recognition. B. Zhou, X. Wang, and X. Tang, \"Random field topic model for semantic region analysis in crowded scenes from tracklets,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 3441-3448.\n\nUnderstanding collective crowd behaviors: Learning a mixture model of dynamic pedestrian-agents. B Zhou, X Wang, X Tang, IEEE Conference on Computer Vision and Pattern Recognition. B. Zhou, X. Wang, and X. Tang, \"Understanding collective crowd behaviors: Learning a mixture model of dynamic pedestrian-agents,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2871-2878.\n\nCrowd analysis using computer vision techniques. C S Jacques Junior, S R Musse, C R Jung, IEEE Signal Processing Magazine. 275C. S. Jacques Junior, S. R. Musse, and C. R. Jung, \"Crowd analysis using computer vision techniques,\" IEEE Signal Processing Magazine, vol. 27, no. 5, pp. 66-77, 2010.\n\nA survey of vision-based trajectory learning and analysis for surveillance. B T Morris, M M Trivedi, IEEE Transactions on Circuits and Systems for Video Technology. 18B. T. Morris and M. M. Trivedi, \"A survey of vision-based trajectory learning and analysis for surveillance,\" IEEE Transactions on Circuits and Systems for Video Technology, vol. 18, no. 8, pp. 1114-1127, 2008.\n\nChaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes. S Wu, B E Moore, M Shah, IEEE Conference on Computer Vision and Pattern Recognition. S. Wu, B. E. Moore, and M. Shah, \"Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 2054-2060.\n\nA lagrangian particle dynamics approach for crowd flow segmentation and stability analysis. S Ali, M Shah, IEEE Conference on Computer Vision and Pattern Recognition. S. Ali and M. Shah, \"A lagrangian particle dynamics approach for crowd flow segmentation and stability analysis,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2007, pp. 1-6.\n\nA streakline representation of flow in crowded scenes. R Mehran, B E Moore, M Shah, European Conference on Computer Vision. R. Mehran, B. E. Moore, and M. Shah, \"A streakline representation of flow in crowded scenes,\" in European Conference on Computer Vision, 2010, pp. 439-452.\n\nIdentifying behaviors in crowd scenes using stability analysis for dynamical systems. B Solmaz, B E Moore, M Shah, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3410B. Solmaz, B. E. Moore, and M. Shah, \"Identifying behaviors in crowd scenes using stability analysis for dynamical systems,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 10, pp. 2064-2070, 2012.\n\nCrowd analysis: a survey. B Zhan, D N Monekosso, P Remagnino, S A Velastin, L Xu, Machine Vision and Applications. 195-6B. Zhan, D. N. Monekosso, P. Remagnino, S. A. Velastin, and L. Xu, \"Crowd analysis: a survey,\" Machine Vision and Applications, vol. 19, no. 5-6, pp. 345-357, 2008.\n\nCrowd analysis and its applications. N Sjarif, S Shamsuddin, S Hashim, S Yuhaniz, Software Engineering and Computer Systems. 179N. Amir Sjarif, S. Shamsuddin, S. Mohd Hashim, and S. Yuhaniz, \"Crowd analysis and its applications,\" in Software Engineering and Computer Systems, 2011, vol. 179, pp. 687-697.\n\nRoad scene segmentation from a single image. J M Alvarez, T Gevers, Y Lecun, A Lopez, European Conference on Computer Vision. J. M. Alvarez, T. Gevers, Y. LeCun, and A. Lopez, \"Road scene seg- mentation from a single image,\" in European Conference on Computer Vision, 2012, pp. 376-389.\n\nUnsupervised activity perception in crowded and complicated scenes using hierarchical bayesian models. X Wang, X Ma, W E L Grimson, IEEE Transactions on Pattern Analysis and Machine Intelligence. 313X. Wang, X. Ma, and W. E. L. Grimson, \"Unsupervised activity perception in crowded and complicated scenes using hierarchical bayesian models,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 3, pp. 539-555, 2009.\n\nMeta-tracking for video scene understanding. P.-M Jodoin, Y Benezeth, Y Wang, IEEE International Conference onAdvanced Video and Signal Based Surveillance. P.-M. Jodoin, Y. Benezeth, and Y. Wang, \"Meta-tracking for video scene understanding,\" in IEEE International Conference onAdvanced Video and Signal Based Surveillance, 2013, pp. 1-6.\n\nThe large-scale crowd behavior perception based on spatio-temporal viscous fluid field. H Su, H Yang, S Zheng, Y Fan, S Wei, IEEE Transactions on Information Forensics and Security. 810H. Su, H. Yang, S. Zheng, Y. Fan, and S. Wei, \"The large-scale crowd behavior perception based on spatio-temporal viscous fluid field,\" IEEE Transactions on Information Forensics and Security, vol. 8, no. 10, pp. 1575-1589, 2013.\n\nMotion pattern extraction and event detection for automatic visual surveillance. Y Benabbas, N Ihaddadene, C Djeraba, Journal on Image and Video Processing. 7Y. Benabbas, N. Ihaddadene, and C. Djeraba, \"Motion pattern extrac- tion and event detection for automatic visual surveillance,\" Journal on Image and Video Processing, vol. 7, pp. 1-15, 2011.\n\nAbnormal crowd behavior detection using social force model. R Mehran, A Oyama, M Shah, IEEE Conference on Computer Vision and Pattern Recognition. R. Mehran, A. Oyama, and M. Shah, \"Abnormal crowd behavior detection using social force model,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 935-942.\n\nFrom crowd dynamics to crowd safety: A video-based analysis. A Johansson, D Helbing, H Z Al-Abideen, S Al-Bosta, Advances in Complex Systems. 114A. Johansson, D. Helbing, H. Z. Al-Abideen, and S. Al-Bosta, \"From crowd dynamics to crowd safety: A video-based analysis,\" Advances in Complex Systems, vol. 11, no. 4, pp. 497-527, 2008.\n\nLoveparade 2010: Automatic video analysis of a crowd disaster. B Krausz, C Bauckhage, Computer Vision and Image Understanding. 1163B. Krausz and C. Bauckhage, \"Loveparade 2010: Automatic video analysis of a crowd disaster,\" Computer Vision and Image Understand- ing, vol. 116, no. 3, pp. 307-319, 2012.\n\nCrowd psychology and engineering. J D Sime, Safety Science. 211J. D. Sime, \"Crowd psychology and engineering,\" Safety Science, vol. 21, no. 1, pp. 1-14, 1995.\n\nExperimental study of pedestrian flow through a bottleneck. T Kretz, A Gr\u00fcnebohm, M Schreckenberg, Journal of Statistical Mechanics: Theory and Experiment. 10T. Kretz, A. Gr\u00fcnebohm, and M. Schreckenberg, \"Experimental study of pedestrian flow through a bottleneck,\" Journal of Statistical Mechan- ics: Theory and Experiment, vol. 2006, no. 10, pp. 100-114, 2006.\n\nSimulating crowd behaviour in computer games. R Berggren, Lule\u00e5 University of TechnologyPh.D. dissertationR. Berggren, \"Simulating crowd behaviour in computer games,\" Ph.D. dissertation, Lule\u00e5 University of Technology, 2005.\n\nVideo-based crowd synthesis. M Flagg, J M Rehg, IEEE Transactions on Visualization and Computer Graphics. 1911M. Flagg and J. M. Rehg, \"Video-based crowd synthesis,\" IEEE Transactions on Visualization and Computer Graphics, vol. 19, no. 11, pp. 1935-1947, 2013.\n\nTaming crowded visual scenes. S Ali, University of Central FloridaPh.D. dissertationS. Ali, \"Taming crowded visual scenes,\" Ph.D. dissertation, University of Central Florida, 2008.\n\nIdentifying rare and subtle behaviors: A weakly supervised joint topic model. T M Hospedales, J Li, S Gong, T Xiang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3312T. M. Hospedales, J. Li, S. Gong, and T. Xiang, \"Identifying rare and subtle behaviors: A weakly supervised joint topic model,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 12, pp. 2451-2464, 2011.\n\nVisual analysis of behaviour. S Gong, T Xiang, SpringerS. Gong and T. Xiang, Visual analysis of behaviour. Springer, 2011.\n\nDetection of abnormal behaviors in crowd scenes: a review. N Sjarif, S Shamsuddin, S Hashim, International Journal of Advances in Soft Computing and Its Applications. 33N. Sjarif, S. Shamsuddin, and S. Hashim, \"Detection of abnormal be- haviors in crowd scenes: a review,\" International Journal of Advances in Soft Computing and Its Applications, vol. 3, no. 3, pp. 1-33, 2011.\n\nA literature review on video analytics of crowded scenes. M Thida, Y Yong, P Climent-Prez, H Eng, P Remagnino, Intelligent Multimedia Surveillance. Berlin HeidelbergSpringerM. Thida, Y. Yong, P. Climent-Prez, H.-l. Eng, and P. Remagnino, \"A literature review on video analytics of crowded scenes,\" in Intelligent Multimedia Surveillance. Springer Berlin Heidelberg, 2013, pp. 17- 36.\n\nRobust unsupervised motion pattern inference from video and applications. X Zhao, G Medioni, IEEE International Conference on Computer Vision. X. Zhao and G. Medioni, \"Robust unsupervised motion pattern infer- ence from video and applications,\" in IEEE International Conference on Computer Vision, 2011, pp. 715-722.\n\nFeature mining for localised crowd counting. K Chen, C C Loy, S Gong, T Xiang, British Machine Vision Conference. 13K. Chen, C. C. Loy, S. Gong, and T. Xiang, \"Feature mining for localised crowd counting,\" in British Machine Vision Conference, vol. 1, no. 2, 2012, p. 3.\n\nCrossing the line: Crowd counting by integer programming with local features. Z Ma, A B Chan, IEEE Conference on Computer Vision and Pattern Recognition. Z. Ma and A. B. Chan, \"Crossing the line: Crowd counting by integer programming with local features,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 2539-2546.\n\nCounting moving persons in crowded scenes. D Conte, P Foggia, G Percannella, M Vento, Machine vision and Applications. 245D. Conte, P. Foggia, G. Percannella, and M. Vento, \"Counting moving persons in crowded scenes,\" Machine vision and Applications, vol. 24, no. 5, pp. 1029-1042, 2013.\n\nTracking in unstructured crowded scenes. M Rodriguez, S Ali, T Kanade, IEEE International Conference on Computer Vision. M. Rodriguez, S. Ali, and T. Kanade, \"Tracking in unstructured crowded scenes,\" in IEEE International Conference on Computer Vision, 2009, pp. 1389-1396.\n\nA model of human crowd behavior: Group inter-relationship and collision detection analysis. S R Musse, D Thalmann, Computer Animation and Simulation. S. R. Musse and D. Thalmann, \"A model of human crowd behavior: Group inter-relationship and collision detection analysis,\" in Computer Animation and Simulation, 1997, pp. 39-51.\n\nSimulation of crowd problems for computer vision. E L Andrade, R B Fisher, First International Workshop on Crowd Simulation. 3E. L. Andrade and R. B. Fisher, \"Simulation of crowd problems for computer vision,\" in First International Workshop on Crowd Simula- tion, vol. 3, 2005, pp. 71-80.\n\n. Ieee, Vol X Xxx, No, Month Xx, Year, IEEE TRANSACTIONS ON XXX, VOL. X, NO. XX, MONTH YEAR 18\n\nCrowd dynamics. G K Still, University of WarwickPh.D. dissertationG. K. Still, \"Crowd dynamics,\" Ph.D. dissertation, University of Warwick, 2000.\n\nFor pedestrians only: Planning, design, and management of traffic-free zones. R Brambilla, G Longo, B Rudofsky, Whitney Library of Design New YorkR. Brambilla, G. Longo, and B. Rudofsky, For pedestrians only: Planning, design, and management of traffic-free zones. Whitney Library of Design New York, 1977.\n\nA continuum theory for the flow of pedestrians. R L Hughes, Transportation Research Part B: Methodological. 366R. L. Hughes, \"A continuum theory for the flow of pedestrians,\" Transportation Research Part B: Methodological, vol. 36, no. 6, pp. 507-535, 2002.\n\nContinuum crowds. A Treuille, S Cooper, Z Popovi\u0107, ACM Transactions on Graphics. 253A. Treuille, S. Cooper, and Z. Popovi\u0107, \"Continuum crowds,\" in ACM Transactions on Graphics, vol. 25, no. 3, 2006, pp. 1160-1168.\n\nSocial force model for pedestrian dynamics. D Helbing, P Moln\u00e1r, Physical Review E. 51D. Helbing and P. Moln\u00e1r, \"Social force model for pedestrian dynam- ics,\" Physical Review E, vol. 51, pp. 4282-4286, 1995.\n\nAbnormal crowd behavior detection by social force optimization. R Raghavendra, A Bue, M Cristani, V Murino, Human Behavior Understanding. R. Raghavendra, A. Del Bue, M. Cristani, and V. Murino, \"Abnormal crowd behavior detection by social force optimization,\" in Human Behavior Understanding, 2011, pp. 134-145.\n\nCrowd instability analysis using velocity-field based social force model. J Zhao, Y Xu, X Yang, Q Yan, IEEE Conference on Visual Communications and Image Processing. J. Zhao, Y. Xu, X. Yang, and Q. Yan, \"Crowd instability analysis using velocity-field based social force model,\" in IEEE Conference on Visual Communications and Image Processing, 2011, pp. 1-4.\n\nA review of anomaly detection in automated surveillance. A A Sodemann, M P Ross, B J Borghetti, IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews. 426A. A. Sodemann, M. P. Ross, and B. J. Borghetti, \"A review of anomaly detection in automated surveillance,\" IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. 42, no. 6, pp. 1257-1272, 2012.\n\nCrowd flow characterization with optimal control theory. P Allain, N Courty, T Corpetti, Asian Conference of Computer Vision. P. Allain, N. Courty, and T. Corpetti, \"Crowd flow characterization with optimal control theory,\" in Asian Conference of Computer Vision, 2009, pp. 279-290.\n\nComputational methods for fluid dynamics. J H Ferziger, M Peri\u0107, Springer3BerlinJ. H. Ferziger and M. Peri\u0107, Computational methods for fluid dynamics. Springer Berlin, 1996, vol. 3.\n\nFloor fields for tracking in high density crowd scenes. S Ali, M Shah, European Conference on Computer Vision. S. Ali and M. Shah, \"Floor fields for tracking in high density crowd scenes,\" in European Conference on Computer Vision, 2008, pp. 1-14.\n\nReal-time crowd simulation: A review. R Leggett, CiteseerR. Leggett, Real-time crowd simulation: A review. Citeseer, 2004.\n\nLearning motion patterns in crowded scenes using motion flow field. M Hu, S Ali, M Shah, IEEE International Conference on Pattern Recognition. M. Hu, S. Ali, and M. Shah, \"Learning motion patterns in crowded scenes using motion flow field,\" in IEEE International Conference on Pattern Recognition, 2008.\n\nDetecting global motion patterns in complex videos. M Hu, S Ali, M Shah, International Conference on Pattern Recognition. M. Hu, S. Ali, and M. Shah, \"Detecting global motion patterns in complex videos,\" in International Conference on Pattern Recognition, 2008.\n\nA high accuracy flow segmentation method in crowded scenes based on streakline. X Wang, X Yang, X He, Q Teng, M Gao, International Journal for Light and Electron Optics. 1253X. Wang, X. Yang, X. He, Q. Teng, and M. Gao, \"A high accuracy flow segmentation method in crowded scenes based on streakline,\" International Journal for Light and Electron Optics, vol. 125, no. 3, pp. 924-929, 2014.\n\nHigh accuracy optical flow estimation based on a theory for warping. T Brox, A Bruhn, N Papenberg, J Weickert, European Conference on Computer Vision. T. Brox, A. Bruhn, N. Papenberg, and J. Weickert, \"High accuracy optical flow estimation based on a theory for warping,\" in European Conference on Computer Vision, 2004, pp. 25-36.\n\nDefinition and properties of lagrangian coherent structures from finite-time lyapunov exponents in two-dimensional aperiodicflows. S C Shadden, F Lekien, J E Marsden, Physica D: Nonlinear Phenomena. 21234S. C. Shadden, F. Lekien, and J. E. Marsden, \"Definition and properties of lagrangian coherent structures from finite-time lyapunov exponents in two-dimensional aperiodicflows,\" Physica D: Nonlinear Phenomena, vol. 212, no. 34, pp. 271-304, 2005.\n\nImage based flow visualization. J J Van Wijk, ACM Transactions on Graphics. 213J. J. Van Wijk, \"Image based flow visualization,\" in ACM Transactions on Graphics, vol. 21, no. 3, 2002, pp. 745-754.\n\nTracking pedestrians using local spatiotemporal motion patterns in extremely crowded scenes. L Kratz, K Nishino, IEEE Transactions on Pattern Analysis and Machine Intelligence. 345L. Kratz and K. Nishino, \"Tracking pedestrians using local spatio- temporal motion patterns in extremely crowded scenes,\" IEEE Trans- actions on Pattern Analysis and Machine Intelligence, vol. 34, no. 5, pp. 987-1002, 2012.\n\nAnomaly detection in extremely crowded scenes using spatio-temporal motion pattern models. L Kratz, K Nishino, IEEE Conference on Computer Vision and Pattern Recognition. L. Kratz and K. Nishino, \"Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models,\" in IEEE Confer- ence on Computer Vision and Pattern Recognition, 2009, pp. 1446- 1453.\n\nAbnormal event detection in crowded scenes using sparse representation. Y Cong, J Yuan, J Liu, Pattern Recognition. 467Y. Cong, J. Yuan, and J. Liu, \"Abnormal event detection in crowded scenes using sparse representation,\" Pattern Recognition, vol. 46, no. 7, pp. 1851-1864, 2013.\n\nMulti-target tracking by discriminative analysis on riemannian manifold. S Bak, D.-P Chau, J Badie, E Corvee, F Br\u00e9mond, M Thonnat, IEEE International Conference on Image Processing. S. Bak, D.-P. Chau, J. Badie, E. Corvee, F. Br\u00e9mond, and M. Thonnat, \"Multi-target tracking by discriminative analysis on riemannian man- ifold,\" in IEEE International Conference on Image Processing, 2012, pp. 1605-1608.\n\nTracklet reidentification in crowded scenes using bag of spatiotemporal histograms of oriented gradients. M Lewandowski, D Simonnet, D Makris, S Velastin, J Orwell, Pattern Recognition. 7914M. Lewandowski, D. Simonnet, D. Makris, S. Velastin, and J. Orwell, \"Tracklet reidentification in crowded scenes using bag of spatio- temporal histograms of oriented gradients,\" in Pattern Recognition, 2013, vol. 7914, pp. 94-103.\n\nMulti-target tracking by online learned discriminative appearance models. C.-H Kuo, C Huang, R Nevatia, IEEE Conference on Computer Vision and Pattern Recognition. C.-H. Kuo, C. Huang, and R. Nevatia, \"Multi-target tracking by on- line learned discriminative appearance models,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 685-692.\n\nAnalyzing motion patterns in crowded scenes via automatic tracklets clustering. C Wang, X Zhao, Y Zou, Y Liu, China Communications. 104C. Wang, X. Zhao, Y. Zou, and Y. Liu, \"Analyzing motion patterns in crowded scenes via automatic tracklets clustering,\" China Communi- cations, vol. 10, no. 4, pp. 144-154, 2013.\n\nMotion pattern analysis in crowded scenes based on hybrid generative-discriminative feature maps. C Wang, X Zhao, Z Wu, Y Liu, IEEE International Conference on Image Processing. C. Wang, X. Zhao, Z. Wu, and Y. Liu, \"Motion pattern analysis in crowded scenes based on hybrid generative-discriminative feature maps,\" in IEEE International Conference on Image Processing, 2013, pp. 2837-2841.\n\nCrowd motion partitioning in a scattered motion field. S Wu, H , San Wong, IEEE Transactions on Systems, Man, and Cybernetics. 425Part B: CyberneticsS. Wu and H. San Wong, \"Crowd motion partitioning in a scattered motion field,\" IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 42, no. 5, pp. 1443-1454, 2012.\n\nMotion pattern analysis in crowded scenes by using density based clustering. W He, Z Liu, IEEE International Conference on Fuzzy Systems and Knowledge Discovery. W. He and Z. Liu, \"Motion pattern analysis in crowded scenes by using density based clustering,\" in IEEE International Conference on Fuzzy Systems and Knowledge Discovery, 2012, pp. 1855-1858.\n\nDetecting dominant motions in dense crowds. A M Cheriyadat, R J Radke, IEEE Journal of Selected Topics in Signal Processing. 24A. M. Cheriyadat and R. J. Radke, \"Detecting dominant motions in dense crowds,\" IEEE Journal of Selected Topics in Signal Processing, vol. 2, no. 4, pp. 568-581, 2008.\n\nCoherent filtering: Detecting coherent motions from crowd clutters. B Zhou, X Tang, X Wang, European Conference on Computer Vision. B. Zhou, X. Tang, and X. Wang, \"Coherent filtering: Detecting coherent motions from crowd clutters,\" in European Conference on Computer Vision, 2012, pp. 857-871.\n\nUnderstanding dynamic scenes by hierarchical motion pattern mining. L Song, F Jiang, Z Shi, A K Katsaggelos, IEEE International Conference on Multimedia and Expo. L. Song, F. Jiang, Z. Shi, and A. K. Katsaggelos, \"Understanding dynamic scenes by hierarchical motion pattern mining,\" in IEEE International Conference on Multimedia and Expo., 2011, pp. 1-6.\n\nLearning semantic motion patterns for dynamic scenes by improved sparse topical coding. W Fu, J Wang, Z Li, H Lu, S Ma, IEEE International Conference on Multimedia and Expo. W. Fu, J. Wang, Z. Li, H. Lu, and S. Ma, \"Learning semantic motion patterns for dynamic scenes by improved sparse topical coding,\" in IEEE International Conference on Multimedia and Expo., 2012, pp. 296-301.\n\nAlgorithms for the longest common subsequence problem. D S Hirschberg, Journal of the ACM. 244D. S. Hirschberg, \"Algorithms for the longest common subsequence problem,\" Journal of the ACM, vol. 24, no. 4, pp. 664-675, 1977.\n\nDetection and tracking of point features. C Tomasi, T Kanade, School of Computer Science, Carnegie Mellon Univ.C. Tomasi and T. Kanade, Detection and tracking of point features. School of Computer Science, Carnegie Mellon Univ., 1991.\n\nDiffusion maps and coarse-graining: A unified framework for dimensionality reduction, graph partitioning, and data set parameterization. S Lafon, A B Lee, IEEE Transactions on Pattern Analysis and Machine Intelligence. 289S. Lafon and A. B. Lee, \"Diffusion maps and coarse-graining: A unified framework for dimensionality reduction, graph partitioning, and data set parameterization,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 9, pp. 1393-1403, 2006.\n\nDiscovering similar multidimensional trajectories. M Vlachos, G Kollios, D Gunopulos, 18th International Conference on Data Engineering. M. Vlachos, G. Kollios, and D. Gunopulos, \"Discovering similar multidimensional trajectories,\" in 18th International Conference on Data Engineering, 2002, pp. 673-684.\n\nTracking using motion patterns for very crowded scenes. X Zhao, D Gong, G Medioni, European Conference on Computer Vision. X. Zhao, D. Gong, and G. Medioni, \"Tracking using motion patterns for very crowded scenes,\" in European Conference on Computer Vision, 2012, pp. 315-328.\n\nData-driven crowd analysis in videos. M Rodriguez, J Sivic, I Laptev, J.-Y Audibert, IEEE International Conference on Computer Vision. M. Rodriguez, J. Sivic, I. Laptev, and J.-Y. Audibert, \"Data-driven crowd analysis in videos,\" in IEEE International Conference on Com- puter Vision, 2011, pp. 1235-1242.\n\nUnsupervised activity perception in crowded and complicated scenes using hierarchical bayesian models. X Wang, X Ma, W E L Grimson, IEEE Transactions on Pattern Analysis and Machine Intelligence. 313X. Wang, X. Ma, and W. E. L. Grimson, \"Unsupervised activity perception in crowded and complicated scenes using hierarchical bayesian models,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 3, pp. 539-555, 2009.\n\nAnomaly detection in crowded scenes. V Mahadevan, W Li, V Bhalodia, N Vasconcelos, IEEE Conference on Computer Vision and Pattern Recognition. V. Mahadevan, W. Li, V. Bhalodia, and N. Vasconcelos, \"Anomaly detection in crowded scenes,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 1975-1981.\n\nAbnormal crowd behavior detection using high-frequency and spatio-temporal features. B Wang, M Ye, X Li, F Zhao, J Ding, Machine Vision and Applications. 23B. Wang, M. Ye, X. Li, F. Zhao, and J. Ding, \"Abnormal crowd behavior detection using high-frequency and spatio-temporal features,\" Machine Vision and Applications, vol. 23, no. 3, pp. 501-511, 2012.\n\nAn on-line, real-time learning method for detecting anomalies in videos using spatio-temporal compositions. M , Javan Roshtkhari, M D Levine, Computer Vision and Image Understanding. 11710M. Javan Roshtkhari and M. D. Levine, \"An on-line, real-time learn- ing method for detecting anomalies in videos using spatio-temporal compositions,\" Computer Vision and Image Understanding, vol. 117, no. 10, pp. 1436-1452, 2013.\n\nLaplacian eigenmap with temporal constraints for local abnormality detection in crowded scenes. M Thida, H.-L Eng, P Remagnino, IEEE Transactions on Cybernetics. 436M. Thida, H.-L. Eng, and P. Remagnino, \"Laplacian eigenmap with temporal constraints for local abnormality detection in crowded scenes,\" IEEE Transactions on Cybernetics, vol. 43, no. 6, pp. 2147- 2156, 2013.\n\nVideo anomaly search in crowded scenes via spatio-temporal motion context. Y Cong, J Yuan, Y Tang, IEEE Transactions on Information Forensics and Security. 810Y. Cong, J. Yuan, and Y. Tang, \"Video anomaly search in crowded scenes via spatio-temporal motion context,\" IEEE Transactions on Information Forensics and Security, vol. 8, no. 10, pp. 1590-1599, 2013.\n\nVisual-based human crowds behavior analysis based on graph modeling and matching. D Chen, P Huang, IEEE Sensors Journal. 136D. Chen and P. Huang, \"Visual-based human crowds behavior analysis based on graph modeling and matching,\" IEEE Sensors Journal, vol. 13, no. 6, pp. 2129-2138, 2013.\n\nA bayesian model for crowd escape behavior detection. S Wu, H.-S Wong, Z Yu, IEEE Transactions on Circuits and Systems for Video Technology. 241S. Wu, H.-S. Wong, and Z. Yu, \"A bayesian model for crowd escape behavior detection,\" IEEE Transactions on Circuits and Systems for Video Technology, vol. 24, no. 1, pp. 85-98, 2014.\n\nAnomaly detection and localization in crowded scenes. W Li, V Mahadevan, N Vasconcelos, IEEE Transactions on Pattern Analysis and Machine Intelligence. 3699W. Li, V. Mahadevan, and N. Vasconcelos, \"Anomaly detection and localization in crowded scenes,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 99, pp. 18-32, 2013.\n\nAbnormal crowd behavior detection based on local pressure model. H Yang, Y Cao, S Wu, W Lin, S Zheng, Z Yu, Signal Information Processing Association Annual Summit and Conference. H. Yang, Y. Cao, S. Wu, W. Lin, S. Zheng, and Z. Yu, \"Abnormal crowd behavior detection based on local pressure model,\" in Signal Information Processing Association Annual Summit and Conference, 2012, pp. 1-4.\n\nAbnormal crowd behavior detection using behavior entropy model. W Ren, G Li, J Chen, H Liang, IEEE International Conference on Wavelet Analysis and Pattern Recognition. W. Ren, G. Li, J. Chen, and H. Liang, \"Abnormal crowd behavior detec- tion using behavior entropy model,\" in IEEE International Conference on Wavelet Analysis and Pattern Recognition, 2012, pp. 212-221.\n\nA brief survey of dynamic texture description and recognition. D Chetverikov, R P\u00e9teri, Computer Recognition Systems. D. Chetverikov and R. P\u00e9teri, \"A brief survey of dynamic texture description and recognition,\" in Computer Recognition Systems, 2005, pp. 17-26.\n\nDynamic texture reconstruction from sparse codes for unusual event detection in crowded scenes. J Xu, S Denman, S Sridharan, C Fookes, R Rana, Proceedings of the 2011 Joint ACM Workshop on Modeling and Representing Events. the 2011 Joint ACM Workshop on Modeling and Representing EventsJ. Xu, S. Denman, S. Sridharan, C. Fookes, and R. Rana, \"Dynamic texture reconstruction from sparse codes for unusual event detection in crowded scenes,\" in Proceedings of the 2011 Joint ACM Workshop on Modeling and Representing Events, 2011, pp. 25-30.\n\nModeling, clustering, and segmenting video with mixtures of dynamic textures. A B Chan, N Vasconcelos, IEEE Transactions on. 305Pattern Analysis and Machine IntelligenceA. B. Chan and N. Vasconcelos, \"Modeling, clustering, and segmenting video with mixtures of dynamic textures,\" Pattern Analysis and Ma- chine Intelligence, IEEE Transactions on, vol. 30, no. 5, pp. 909-926, 2008.\n\nAbnormal crowd behavior detection using size-adapted spatio-temporal features. B Wang, M Ye, X Li, F Zhao, International Journal of Control, Automation and Systems. 95B. Wang, M. Ye, X. Li, and F. Zhao, \"Abnormal crowd behavior detection using size-adapted spatio-temporal features,\" International Journal of Control, Automation and Systems, vol. 9, no. 5, pp. 905-912, 2011.\n\nBeyond pixels: exploring new representations and applications for motion analysis. C Liu, Massachusetts Institute of TechnologyPh.D. dissertationC. Liu, \"Beyond pixels: exploring new representations and applications for motion analysis,\" Ph.D. dissertation, Massachusetts Institute of Technology, 2009.\n\nSalient motion detection in crowded scenes. C C Loy, T Xiang, S Gong, International Symposium on Communications Control and Signal Processing. C. C. Loy, T. Xiang, and S. Gong, \"Salient motion detection in crowded scenes,\" in International Symposium on Communications Control and Signal Processing, 2012, pp. 1-4.\n\nSaliency detection: A spectral residual approach. X Hou, L Zhang, IEEE Conference on Computer Vision and Pattern Recognition. X. Hou and L. Zhang, \"Saliency detection: A spectral residual ap- proach,\" in IEEE Conference on Computer Vision and Pattern Recog- nition, 2007, pp. 1-8.\n\nOptimizing interaction force for global anomaly detection in crowded scenes. R Raghavendra, A Bue, M Cristani, V Murino, IEEE International Conference on Computer Vision Workshops. R. Raghavendra, A. Del Bue, M. Cristani, and V. Murino, \"Optimizing interaction force for global anomaly detection in crowded scenes,\" in IEEE International Conference on Computer Vision Workshops, 2011, pp. 136-143.\n\nAbnormal crowd behavior detection based on the energy model. G Xiong, X Wu, Y.-L Chen, Y Ou, IEEE International Conference on Information and Automation. G. Xiong, X. Wu, Y.-L. Chen, and Y. Ou, \"Abnormal crowd behavior detection based on the energy model,\" in IEEE International Confer- ence on Information and Automation, 2011, pp. 495-500.\n\nCrowd density estimation based on image potential energy model. G Xiong, X Wu, J Cheng, Y.-L Chen, Y Ou, Y Liu, IEEE International Conference on Robotics and Biomimetics. G. Xiong, X. Wu, J. Cheng, Y.-L. Chen, Y. Ou, and Y. Liu, \"Crowd density estimation based on image potential energy model,\" in IEEE International Conference on Robotics and Biomimetics, 2011, pp. 538- 543.\n\nVideo-based abnormal human behavior recognition -a review. O P Popoola, K Wang, IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews. 426O. P. Popoola and K. Wang, \"Video-based abnormal human behavior recognition -a review,\" IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. 42, no. 6, pp. 865-878, 2012.\n\nVideo event detection: From subvolume localization to spatio-temporal path search. D Tran, J Yuan, D Forsyth, IEEE transactions on Pattern Analysis and Machine Intelligence. 362D. Tran, J. Yuan, and D. Forsyth, \"Video event detection: From subvol- ume localization to spatio-temporal path search,\" IEEE transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 2, pp. 404-416, 2013.\n\nSelective eigenbackground for background modeling and subtraction in crowded scenes. Y Tian, Y Wang, Z Hu, T Huang, IEEE transactions on Circuits and Systems for Video Technology. 23Y. Tian, Y. Wang, Z. Hu, and T. Huang, \"Selective eigenbackground for background modeling and subtraction in crowded scenes,\" IEEE transactions on Circuits and Systems for Video Technology, vol. 23, no. 11, pp. 1849-1864, 2013.\n\n. Dataset Umn Crowd, UMN Crowd Dataset, http://mha.cs.umn.edu/proj events.shtml#crowd.\n\n. UCSD Anomaly Detection Dataset. UCSD Anomaly Detection Dataset, http://www.svcl.ucsd.edu/projects/ anomaly/dataset.htm.\n\n. Dataset Qmul Crowd, QMUL Crowd Dataset, http://www.eecs.qmul.ac.uk/ \u223c sgg.\n\n. Dataset, PETS2009 Dataset, http://www.cvg.rdg.ac.uk/PETS2009.\n\n. Violence-Flows, Dataset, VIOLENCE-FLOWS Dataset, http://www.openu.ac.il/home/hassner/ data/violentflows/index.html.\n\nDetecting group activities with multi-camera context. Z Zha, H Zhang, M Wang, H Luan, T Chua, IEEE Trans. on Circuits and Systems for Video Technology. 235Z. Zha, H. Zhang, M. Wang, H. Luan, and T. Chua, \"Detecting group activities with multi-camera context,\" IEEE Trans. on Circuits and Systems for Video Technology, vol. 23, no. 5, pp. 856-869, 2013.\n\nA fully online and unsupervised system for large and high-density area surveillance: Tracking, semantic scene learning and abnormality detection. X Song, X Shao, Q Zhang, R Shibasaki, H Zhao, J Cui, H Zha, ACM Transactions on Intelligent Systems and Technology. 42X. Song, X. Shao, Q. Zhang, R. Shibasaki, H. Zhao, J. Cui, and H. Zha, \"A fully online and unsupervised system for large and high-density area surveillance: Tracking, semantic scene learning and abnormality detection,\" ACM Transactions on Intelligent Systems and Technology, vol. 4, no. 2, pp. 1-21, 2013.\n\nAn online approach: Learning-semantic-scene-by-tracking and trackingby-learning-semantic-scene. X Song, X Shao, H Zhao, J Cui, R Shibasaki, H Zha, IEEE Conference on Computer Vision and Pattern Recognition. X. Song, X. Shao, H. Zhao, J. Cui, R. Shibasaki, and H. Zha, \"An online approach: Learning-semantic-scene-by-tracking and tracking- by-learning-semantic-scene,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 739-746.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" in Advances in Neural Information Processing Systems, 2012, pp. 1097-1105.\n\nMulti-stage contextual deep learning for pedestrian detection. X Zeng, W Ouyang, X Wang, IEEE International Conference on Computer Vision. X. Zeng, W. Ouyang, and X. Wang, \"Multi-stage contextual deep learning for pedestrian detection,\" in IEEE International Conference on Computer Vision, 2013.\n\nChinese Academy of Sciences (CASIA) in 2004, and Ph.D. from Korea Advanced Institute of Science and Technolgy (KAIST) in 2010. He is currently a tenuretrack professor with Anhui University. He received the Best Paper Award from IEEE. Dr. Teng Li received B.S. from University of Science and Technology of China (USTC2001, M.S. from Institute of Automation. and the Best Paper Award of ICIMCS09Dr. Teng Li received B.S. from University of Science and Technology of China (USTC) in 2001, M.S. from Institute of Automation, Chinese Academy of Sciences (CASIA) in 2004, and Ph.D. from Korea Advanced Institute of Science and Tech- nolgy (KAIST) in 2010. He is currently a tenure- track professor with Anhui University. He received the Best Paper Award from IEEE T-CSVT in 2014, and the Best Paper Award of ICIMCS09.\n\nChina in 2012. He is currently a master candidate with College of Electrical Engineering and Automation, Anhui University. Huan Chang received B.S. from Anhui University (AHU)His research interests are in the areas of computer visionHuan Chang received B.S. from Anhui University (AHU), China in 2012. He is currently a master candidate with College of Electrical Engineering and Automation, Anhui University. His research interests are in the areas of computer vision.\n\nSingapore. His research interests are in the areas of computer vision, machine learning and multimedia. Dr. Ni worked in Microsoft Research Asia, Beijing as a research intern in 2009. He also worked as a software engineer intern in. Dr, Meng, 18th ACM International Conference on Multimedia, the best paper award from the 16th International Multimedia Modeling Conference, the best paper award from the 4th International Conference on Internet Multimedia Computing and Service, and the best demo award from the 20th ACM International Conference on Multimedia. Singapore (NUS; Mountain View, CAACMBingbing Ni received his B.Eng. degree in Electrical Engineering from Shanghai Jiao Tong University (SJTUHe received the Best Paper Award from PCM11 and the Best Student Paper Award from PREMIA08. He won the first prize in International Contest on Human Activity Recognition and Localization (HARL) in conjunction with International Conference on Pattern Recognition. He also won the second prize in ChaLearn 2014 human action recognition challenge. He is a member of the Association for Computing MachineryDr. Meng Wang is a professor in the Hefei Uni- versity of Technology, China. He received the B.E. degree and Ph.D. degree in the Special Class for the Gifted Young and the Department of Electronic Engineering and Information Science from the Uni- versity of Science and Technology of China (USTC), Hefei, China, respectively. His current research in- terests include multimedia content analysis, search, mining, recommendation, and large-scale comput- ing. He received the best paper awards successively from the 17th and 18th ACM International Confer- ence on Multimedia, the best paper award from the 16th International Multi- media Modeling Conference, the best paper award from the 4th International Conference on Internet Multimedia Computing and Service, and the best demo award from the 20th ACM International Conference on Multimedia. Dr. Bingbing Ni received his B.Eng. degree in Electrical Engineering from Shanghai Jiao Tong University (SJTU), China in 2005 and obtained his Ph.D. from National University of Singapore (NUS), Singapore in 2011. Dr. Ni is currently a research scientist in Advanced Digital Sciences Center, Sin- gapore. His research interests are in the areas of computer vision, machine learning and multimedia. Dr. Ni worked in Microsoft Research Asia, Beijing as a research intern in 2009. He also worked as a software engineer intern in Google Inc., Mountain View, CA in 2010. He received the Best Paper Award from PCM11 and the Best Student Paper Award from PREMIA08. He won the first prize in International Contest on Human Activity Recognition and Localization (HARL) in conjunction with International Conference on Pattern Recognition, 2012. He also won the second prize in ChaLearn 2014 human action recognition challenge. He is a member of the Association for Computing Machinery (ACM).\n", "annotations": {"author": "[{\"end\":99,\"start\":91},{\"end\":111,\"start\":100},{\"end\":122,\"start\":112},{\"end\":135,\"start\":123},{\"end\":149,\"start\":136},{\"end\":183,\"start\":150}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":96},{\"end\":110,\"start\":105},{\"end\":121,\"start\":117},{\"end\":134,\"start\":132},{\"end\":148,\"start\":144},{\"end\":182,\"start\":179}]", "author_first_name": "[{\"end\":95,\"start\":91},{\"end\":104,\"start\":100},{\"end\":116,\"start\":112},{\"end\":131,\"start\":123},{\"end\":143,\"start\":136},{\"end\":178,\"start\":169}]", "author_affiliation": null, "title": "[{\"end\":88,\"start\":1},{\"end\":271,\"start\":184}]", "venue": null, "abstract": "[{\"end\":1712,\"start\":315}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2203,\"start\":2200},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2334,\"start\":2331},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2642,\"start\":2639},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2647,\"start\":2644},{\"end\":2746,\"start\":2744},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3109,\"start\":3106},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3191,\"start\":3188},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3196,\"start\":3193},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3202,\"start\":3198},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3640,\"start\":3637},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3646,\"start\":3642},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3652,\"start\":3648},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3723,\"start\":3720},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3728,\"start\":3725},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3819,\"start\":3815},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3880,\"start\":3877},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3885,\"start\":3882},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3891,\"start\":3887},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3897,\"start\":3893},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3958,\"start\":3954},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3964,\"start\":3960},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4556,\"start\":4552},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4562,\"start\":4558},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4599,\"start\":4595},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4605,\"start\":4601},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4611,\"start\":4607},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4891,\"start\":4887},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4897,\"start\":4893},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4985,\"start\":4981},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4991,\"start\":4987},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5505,\"start\":5501},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5610,\"start\":5606},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5753,\"start\":5750},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5758,\"start\":5755},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5940,\"start\":5936},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6349,\"start\":6345},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6904,\"start\":6900},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7025,\"start\":7022},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7031,\"start\":7027},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7037,\"start\":7033},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7043,\"start\":7039},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7141,\"start\":7137},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7498,\"start\":7495},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7900,\"start\":7896},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8013,\"start\":8009},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8804,\"start\":8800},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8851,\"start\":8847},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8919,\"start\":8915},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8925,\"start\":8921},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10770,\"start\":10766},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11246,\"start\":11242},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11521,\"start\":11517},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11694,\"start\":11690},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12776,\"start\":12772},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13007,\"start\":13003},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13678,\"start\":13674},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13684,\"start\":13680},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13704,\"start\":13700},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13970,\"start\":13966},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14257,\"start\":14253},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14263,\"start\":14259},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14354,\"start\":14350},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15126,\"start\":15122},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15132,\"start\":15128},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15138,\"start\":15134},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15206,\"start\":15202},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15443,\"start\":15439},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15557,\"start\":15553},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15563,\"start\":15559},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15569,\"start\":15565},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15575,\"start\":15571},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":15581,\"start\":15577},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15808,\"start\":15804},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15814,\"start\":15810},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15820,\"start\":15816},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15932,\"start\":15928},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16317,\"start\":16313},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16732,\"start\":16728},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":16753,\"start\":16749},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16829,\"start\":16826},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16834,\"start\":16831},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17030,\"start\":17026},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":17036,\"start\":17032},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":18889,\"start\":18885},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18984,\"start\":18980},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18990,\"start\":18986},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18996,\"start\":18992},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":19002,\"start\":18998},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":19008,\"start\":19004},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":19254,\"start\":19250},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19403,\"start\":19400},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19409,\"start\":19405},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19415,\"start\":19411},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":19421,\"start\":19417},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":19427,\"start\":19423},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19706,\"start\":19702},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19772,\"start\":19768},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19778,\"start\":19774},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19784,\"start\":19780},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20084,\"start\":20080},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20127,\"start\":20123},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20133,\"start\":20129},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20338,\"start\":20334},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":20619,\"start\":20615},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20644,\"start\":20640},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":21815,\"start\":21811},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22008,\"start\":22004},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":22014,\"start\":22010},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22044,\"start\":22040},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":22050,\"start\":22046},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22177,\"start\":22173},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":22183,\"start\":22179},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22635,\"start\":22631},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":22641,\"start\":22637},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23218,\"start\":23214},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":23224,\"start\":23220},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23288,\"start\":23284},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":23670,\"start\":23666},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25023,\"start\":25018},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25026,\"start\":25023},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25028,\"start\":25026},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25031,\"start\":25028},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25108,\"start\":25104},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25111,\"start\":25108},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25195,\"start\":25191},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25461,\"start\":25457},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25740,\"start\":25737},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25742,\"start\":25740},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25745,\"start\":25742},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26201,\"start\":26197},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26784,\"start\":26780},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26932,\"start\":26928},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27805,\"start\":27802},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28174,\"start\":28171},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":28198,\"start\":28194},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":28204,\"start\":28200},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28428,\"start\":28425},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28433,\"start\":28430},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":28439,\"start\":28435},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":28445,\"start\":28441},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30146,\"start\":30143},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30151,\"start\":30148},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30157,\"start\":30153},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30479,\"start\":30476},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31641,\"start\":31637},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31647,\"start\":31643},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31653,\"start\":31649},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":31659,\"start\":31655},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":31665,\"start\":31661},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":31671,\"start\":31667},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32108,\"start\":32104},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":32352,\"start\":32348},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32560,\"start\":32556},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":32836,\"start\":32832},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":33010,\"start\":33006},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":33581,\"start\":33577},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34197,\"start\":34193},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":34496,\"start\":34492},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":34589,\"start\":34585},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35267,\"start\":35263},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":35273,\"start\":35269},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35279,\"start\":35275},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35419,\"start\":35415},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":35425,\"start\":35421},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":35431,\"start\":35427},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":36144,\"start\":36140},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":36242,\"start\":36238},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":36659,\"start\":36655},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":37071,\"start\":37067},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":37328,\"start\":37324},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":37525,\"start\":37521},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":37558,\"start\":37554},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":37814,\"start\":37810},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39649,\"start\":39646},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":40124,\"start\":40120},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":40182,\"start\":40179},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40672,\"start\":40669},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":40677,\"start\":40674},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":41370,\"start\":41366},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":41878,\"start\":41874},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":42752,\"start\":42748},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":44479,\"start\":44475},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":44490,\"start\":44486},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":44506,\"start\":44502},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":44526,\"start\":44522},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":44554,\"start\":44550},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46232,\"start\":46228},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":46250,\"start\":46247},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":46691,\"start\":46687},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46765,\"start\":46761},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":46820,\"start\":46816},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46873,\"start\":46869},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":47248,\"start\":47244},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":47266,\"start\":47263},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":48033,\"start\":48029},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":48181,\"start\":48177},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":48681,\"start\":48677},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":48752,\"start\":48749},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":50334,\"start\":50330},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":50340,\"start\":50336},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":50346,\"start\":50342},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":50352,\"start\":50348},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":50358,\"start\":50354},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":50647,\"start\":50643},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":51005,\"start\":51002},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":51646,\"start\":51642},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":51728,\"start\":51724},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":52236,\"start\":52232},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":52649,\"start\":52645},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":53437,\"start\":53433},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54829,\"start\":54826},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":54835,\"start\":54831},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":54853,\"start\":54849},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":55566,\"start\":55563},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":58155,\"start\":58151},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":58161,\"start\":58157},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":58167,\"start\":58163},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":58173,\"start\":58169},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":58179,\"start\":58175},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":58185,\"start\":58181},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":58191,\"start\":58187},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":58395,\"start\":58391},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":58783,\"start\":58779},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":59818,\"start\":59814},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":59824,\"start\":59820},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":59830,\"start\":59826},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":59836,\"start\":59832},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":59992,\"start\":59988},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":60468,\"start\":60464},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":64535,\"start\":64531},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":64679,\"start\":64675},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":64785,\"start\":64781},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":65733,\"start\":65729},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":66100,\"start\":66096},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":66283,\"start\":66279},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":66311,\"start\":66307},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":66317,\"start\":66313},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":66502,\"start\":66498},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":66744,\"start\":66740},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":66750,\"start\":66746},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":67911,\"start\":67907},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":67917,\"start\":67913},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":67939,\"start\":67935},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":68266,\"start\":68263},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":69076,\"start\":69072},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":69086,\"start\":69082},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":69642,\"start\":69638},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":70129,\"start\":70125},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":70666,\"start\":70662},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":71189,\"start\":71185},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":72627,\"start\":72623},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":72946,\"start\":72942},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":73212,\"start\":73208},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":73951,\"start\":73947},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":73968,\"start\":73964},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":74255,\"start\":74251},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":74455,\"start\":74451},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":74616,\"start\":74612},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":74715,\"start\":74711},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":74776,\"start\":74772},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":75265,\"start\":75261},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":75356,\"start\":75352},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":75362,\"start\":75358},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":75853,\"start\":75849},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":76526,\"start\":76522},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":76936,\"start\":76932},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":77100,\"start\":77096},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":77422,\"start\":77418},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":78377,\"start\":78373},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":78836,\"start\":78831},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":79080,\"start\":79076},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":79086,\"start\":79082},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":79571,\"start\":79566},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":79605,\"start\":79600},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":80056,\"start\":80052},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":80377,\"start\":80372},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":80639,\"start\":80634},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":80994,\"start\":80989},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":81308,\"start\":81304},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":81805,\"start\":81800},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":82230,\"start\":82225},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":82638,\"start\":82634},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":83053,\"start\":83049},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":83080,\"start\":83076},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":83362,\"start\":83358},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":86147,\"start\":86144},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":86381,\"start\":86376},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":87086,\"start\":87081},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":87093,\"start\":87088},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":88094,\"start\":88089},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":88101,\"start\":88096},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":90124,\"start\":90120},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":90454,\"start\":90450},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":90584,\"start\":90580},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":91123,\"start\":91119}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":89460,\"start\":88903},{\"attributes\":{\"id\":\"fig_1\"},\"end\":89607,\"start\":89461},{\"attributes\":{\"id\":\"fig_2\"},\"end\":89723,\"start\":89608},{\"attributes\":{\"id\":\"fig_3\"},\"end\":89801,\"start\":89724},{\"attributes\":{\"id\":\"fig_4\"},\"end\":90125,\"start\":89802},{\"attributes\":{\"id\":\"fig_5\"},\"end\":90253,\"start\":90126},{\"attributes\":{\"id\":\"fig_6\"},\"end\":90455,\"start\":90254},{\"attributes\":{\"id\":\"fig_7\"},\"end\":90584,\"start\":90456},{\"attributes\":{\"id\":\"fig_8\"},\"end\":90734,\"start\":90585},{\"attributes\":{\"id\":\"fig_9\"},\"end\":90763,\"start\":90735},{\"attributes\":{\"id\":\"fig_10\"},\"end\":90999,\"start\":90764},{\"attributes\":{\"id\":\"fig_11\"},\"end\":91124,\"start\":91000},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":91602,\"start\":91125},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":92924,\"start\":91603},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":93493,\"start\":92925},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":93691,\"start\":93494},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":94607,\"start\":93692},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":96799,\"start\":94608},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":97762,\"start\":96800}]", "paragraph": "[{\"end\":2496,\"start\":1731},{\"end\":4105,\"start\":2498},{\"end\":4216,\"start\":4136},{\"end\":4617,\"start\":4218},{\"end\":4898,\"start\":4619},{\"end\":5611,\"start\":4900},{\"end\":6430,\"start\":5643},{\"end\":6989,\"start\":6432},{\"end\":8207,\"start\":6991},{\"end\":9253,\"start\":8209},{\"end\":9912,\"start\":9255},{\"end\":10648,\"start\":9945},{\"end\":11398,\"start\":10679},{\"end\":11809,\"start\":11400},{\"end\":12167,\"start\":11811},{\"end\":12566,\"start\":12169},{\"end\":13008,\"start\":12598},{\"end\":13487,\"start\":13010},{\"end\":14069,\"start\":13489},{\"end\":14577,\"start\":14071},{\"end\":15025,\"start\":14627},{\"end\":15272,\"start\":15027},{\"end\":15582,\"start\":15309},{\"end\":16455,\"start\":15584},{\"end\":16946,\"start\":16457},{\"end\":17237,\"start\":16948},{\"end\":17463,\"start\":17239},{\"end\":17865,\"start\":17512},{\"end\":18501,\"start\":17867},{\"end\":19145,\"start\":18528},{\"end\":19620,\"start\":19147},{\"end\":20224,\"start\":19622},{\"end\":21217,\"start\":20226},{\"end\":21608,\"start\":21255},{\"end\":21816,\"start\":21610},{\"end\":22051,\"start\":21818},{\"end\":22264,\"start\":22053},{\"end\":22731,\"start\":22324},{\"end\":23268,\"start\":22733},{\"end\":23652,\"start\":23270},{\"end\":24155,\"start\":23654},{\"end\":24378,\"start\":24157},{\"end\":24860,\"start\":24405},{\"end\":25710,\"start\":24862},{\"end\":26535,\"start\":25712},{\"end\":27806,\"start\":26557},{\"end\":28706,\"start\":27808},{\"end\":29042,\"start\":28721},{\"end\":29355,\"start\":29044},{\"end\":29740,\"start\":29357},{\"end\":30030,\"start\":29742},{\"end\":30282,\"start\":30072},{\"end\":30783,\"start\":30284},{\"end\":31516,\"start\":30785},{\"end\":32091,\"start\":31559},{\"end\":32767,\"start\":32093},{\"end\":33480,\"start\":32769},{\"end\":34125,\"start\":33482},{\"end\":34978,\"start\":34127},{\"end\":35728,\"start\":34980},{\"end\":36120,\"start\":35763},{\"end\":37028,\"start\":36122},{\"end\":37791,\"start\":37030},{\"end\":38363,\"start\":37793},{\"end\":38839,\"start\":38365},{\"end\":39632,\"start\":38881},{\"end\":40048,\"start\":39634},{\"end\":40140,\"start\":40050},{\"end\":40575,\"start\":40142},{\"end\":41328,\"start\":40577},{\"end\":41852,\"start\":41330},{\"end\":42169,\"start\":41854},{\"end\":42521,\"start\":42171},{\"end\":44374,\"start\":42540},{\"end\":45103,\"start\":44376},{\"end\":47092,\"start\":45105},{\"end\":47551,\"start\":47149},{\"end\":48247,\"start\":47553},{\"end\":48753,\"start\":48306},{\"end\":49213,\"start\":48768},{\"end\":50134,\"start\":49215},{\"end\":50460,\"start\":50136},{\"end\":50783,\"start\":50494},{\"end\":51320,\"start\":50785},{\"end\":51647,\"start\":51345},{\"end\":52164,\"start\":51649},{\"end\":52620,\"start\":52166},{\"end\":53343,\"start\":52622},{\"end\":54043,\"start\":53345},{\"end\":55552,\"start\":54072},{\"end\":56250,\"start\":55554},{\"end\":56978,\"start\":56265},{\"end\":58019,\"start\":56980},{\"end\":58396,\"start\":58051},{\"end\":58613,\"start\":58398},{\"end\":58925,\"start\":58615},{\"end\":59703,\"start\":58957},{\"end\":59971,\"start\":59705},{\"end\":60372,\"start\":59973},{\"end\":61100,\"start\":60374},{\"end\":61550,\"start\":61131},{\"end\":61890,\"start\":61552},{\"end\":62979,\"start\":61892},{\"end\":63730,\"start\":63034},{\"end\":64752,\"start\":63807},{\"end\":64967,\"start\":64754},{\"end\":65677,\"start\":64969},{\"end\":66437,\"start\":65679},{\"end\":67104,\"start\":66439},{\"end\":68077,\"start\":67138},{\"end\":68703,\"start\":68130},{\"end\":69077,\"start\":68705},{\"end\":70619,\"start\":69079},{\"end\":72005,\"start\":70621},{\"end\":73196,\"start\":72007},{\"end\":73882,\"start\":73198},{\"end\":74456,\"start\":73884},{\"end\":75247,\"start\":74458},{\"end\":75787,\"start\":75249},{\"end\":76353,\"start\":75789},{\"end\":76927,\"start\":76355},{\"end\":77324,\"start\":76929},{\"end\":77720,\"start\":77326},{\"end\":78173,\"start\":77722},{\"end\":78671,\"start\":78188},{\"end\":79460,\"start\":78673},{\"end\":79701,\"start\":79462},{\"end\":80352,\"start\":79731},{\"end\":80601,\"start\":80354},{\"end\":80965,\"start\":80603},{\"end\":81289,\"start\":80967},{\"end\":81785,\"start\":81291},{\"end\":82598,\"start\":81787},{\"end\":83020,\"start\":82600},{\"end\":83444,\"start\":83022},{\"end\":84365,\"start\":83490},{\"end\":85281,\"start\":84367},{\"end\":85667,\"start\":85283},{\"end\":85860,\"start\":85669},{\"end\":86572,\"start\":85862},{\"end\":87126,\"start\":86574},{\"end\":87730,\"start\":87128},{\"end\":88902,\"start\":87732}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14626,\"start\":14578},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22323,\"start\":22265},{\"attributes\":{\"id\":\"formula_2\"},\"end\":47148,\"start\":47093},{\"attributes\":{\"id\":\"formula_3\"},\"end\":48305,\"start\":48248},{\"attributes\":{\"id\":\"formula_4\"},\"end\":63033,\"start\":62980},{\"attributes\":{\"id\":\"formula_5\"},\"end\":68129,\"start\":68078}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45458,\"start\":45450},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45797,\"start\":45772},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":47611,\"start\":47602},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":48520,\"start\":48511},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49014,\"start\":49007},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":57520,\"start\":57512},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":78467,\"start\":78460},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":79911,\"start\":79901}]", "section_header": "[{\"end\":1729,\"start\":1714},{\"end\":4134,\"start\":4108},{\"end\":5641,\"start\":5614},{\"end\":9943,\"start\":9915},{\"end\":10677,\"start\":10651},{\"end\":12596,\"start\":12569},{\"end\":15307,\"start\":15275},{\"end\":17510,\"start\":17466},{\"end\":18526,\"start\":18504},{\"end\":21253,\"start\":21220},{\"end\":24403,\"start\":24381},{\"attributes\":{\"n\":\"3.1.\"},\"end\":26555,\"start\":26538},{\"end\":28719,\"start\":28709},{\"end\":30070,\"start\":30033},{\"end\":31557,\"start\":31519},{\"end\":35761,\"start\":35731},{\"end\":38879,\"start\":38842},{\"end\":42538,\"start\":42524},{\"end\":48766,\"start\":48756},{\"end\":50492,\"start\":50463},{\"end\":51343,\"start\":51323},{\"end\":54070,\"start\":54046},{\"end\":56263,\"start\":56253},{\"end\":58049,\"start\":58022},{\"end\":58955,\"start\":58928},{\"end\":61129,\"start\":61103},{\"attributes\":{\"n\":\"4.\"},\"end\":63805,\"start\":63733},{\"attributes\":{\"n\":\"4\"},\"end\":67136,\"start\":67107},{\"end\":78186,\"start\":78176},{\"end\":79729,\"start\":79704},{\"end\":83488,\"start\":83447},{\"end\":89470,\"start\":89462},{\"end\":89617,\"start\":89609},{\"end\":89733,\"start\":89725},{\"end\":89811,\"start\":89803},{\"end\":90137,\"start\":90127},{\"end\":90263,\"start\":90255},{\"end\":90465,\"start\":90457},{\"end\":90596,\"start\":90586},{\"end\":90773,\"start\":90765},{\"end\":91009,\"start\":91001},{\"end\":91613,\"start\":91604},{\"end\":92941,\"start\":92926},{\"end\":93504,\"start\":93495},{\"end\":93712,\"start\":93693},{\"end\":94627,\"start\":94609},{\"end\":96818,\"start\":96801}]", "table": "[{\"end\":92924,\"start\":91666},{\"end\":93493,\"start\":92972},{\"end\":93691,\"start\":93552},{\"end\":94607,\"start\":93755},{\"end\":96799,\"start\":94660},{\"end\":97762,\"start\":97200}]", "figure_caption": "[{\"end\":89460,\"start\":88905},{\"end\":89607,\"start\":89472},{\"end\":89723,\"start\":89619},{\"end\":89801,\"start\":89735},{\"end\":90125,\"start\":89813},{\"end\":90253,\"start\":90139},{\"end\":90455,\"start\":90265},{\"end\":90584,\"start\":90467},{\"end\":90734,\"start\":90598},{\"end\":90763,\"start\":90737},{\"end\":90999,\"start\":90775},{\"end\":91124,\"start\":91011},{\"end\":91602,\"start\":91127},{\"end\":91666,\"start\":91615},{\"end\":92972,\"start\":92944},{\"end\":93552,\"start\":93508},{\"end\":93755,\"start\":93715},{\"end\":94660,\"start\":94629},{\"end\":97200,\"start\":96821}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":1992,\"start\":1984},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9794,\"start\":9786},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11259,\"start\":11248},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11304,\"start\":11296},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14977,\"start\":14971},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21042,\"start\":21036},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":22854,\"start\":22848},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24847,\"start\":24841},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25378,\"start\":25372},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27366,\"start\":27360},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":43246,\"start\":43240},{\"end\":46297,\"start\":46291},{\"end\":47187,\"start\":47181},{\"end\":47965,\"start\":47959},{\"end\":64388,\"start\":64382},{\"end\":64900,\"start\":64894},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":66675,\"start\":66668},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":67621,\"start\":67615}]", "bib_author_first_name": "[{\"end\":97817,\"start\":97816},{\"end\":97831,\"start\":97830},{\"end\":97839,\"start\":97838},{\"end\":98119,\"start\":98118},{\"end\":98129,\"start\":98128},{\"end\":98140,\"start\":98139},{\"end\":98151,\"start\":98150},{\"end\":98459,\"start\":98458},{\"end\":98470,\"start\":98469},{\"end\":98481,\"start\":98480},{\"end\":98792,\"start\":98791},{\"end\":98800,\"start\":98799},{\"end\":98807,\"start\":98806},{\"end\":99078,\"start\":99077},{\"end\":99084,\"start\":99083},{\"end\":99092,\"start\":99091},{\"end\":99098,\"start\":99097},{\"end\":99105,\"start\":99104},{\"end\":99112,\"start\":99111},{\"end\":99462,\"start\":99461},{\"end\":99468,\"start\":99467},{\"end\":99475,\"start\":99474},{\"end\":99483,\"start\":99482},{\"end\":99899,\"start\":99898},{\"end\":99907,\"start\":99906},{\"end\":99915,\"start\":99914},{\"end\":100286,\"start\":100285},{\"end\":100294,\"start\":100293},{\"end\":100302,\"start\":100301},{\"end\":100634,\"start\":100633},{\"end\":100636,\"start\":100635},{\"end\":100654,\"start\":100653},{\"end\":100656,\"start\":100655},{\"end\":100665,\"start\":100664},{\"end\":100667,\"start\":100666},{\"end\":100956,\"start\":100955},{\"end\":100958,\"start\":100957},{\"end\":100968,\"start\":100967},{\"end\":100970,\"start\":100969},{\"end\":101355,\"start\":101354},{\"end\":101361,\"start\":101360},{\"end\":101363,\"start\":101362},{\"end\":101372,\"start\":101371},{\"end\":101748,\"start\":101747},{\"end\":101755,\"start\":101754},{\"end\":102071,\"start\":102070},{\"end\":102081,\"start\":102080},{\"end\":102083,\"start\":102082},{\"end\":102092,\"start\":102091},{\"end\":102383,\"start\":102382},{\"end\":102393,\"start\":102392},{\"end\":102395,\"start\":102394},{\"end\":102404,\"start\":102403},{\"end\":102734,\"start\":102733},{\"end\":102742,\"start\":102741},{\"end\":102744,\"start\":102743},{\"end\":102757,\"start\":102756},{\"end\":102770,\"start\":102769},{\"end\":102772,\"start\":102771},{\"end\":102784,\"start\":102783},{\"end\":103031,\"start\":103030},{\"end\":103041,\"start\":103040},{\"end\":103055,\"start\":103054},{\"end\":103065,\"start\":103064},{\"end\":103345,\"start\":103344},{\"end\":103347,\"start\":103346},{\"end\":103358,\"start\":103357},{\"end\":103368,\"start\":103367},{\"end\":103377,\"start\":103376},{\"end\":103691,\"start\":103690},{\"end\":103699,\"start\":103698},{\"end\":103705,\"start\":103704},{\"end\":103709,\"start\":103706},{\"end\":104078,\"start\":104074},{\"end\":104088,\"start\":104087},{\"end\":104100,\"start\":104099},{\"end\":104458,\"start\":104457},{\"end\":104464,\"start\":104463},{\"end\":104472,\"start\":104471},{\"end\":104481,\"start\":104480},{\"end\":104488,\"start\":104487},{\"end\":104867,\"start\":104866},{\"end\":104879,\"start\":104878},{\"end\":104893,\"start\":104892},{\"end\":105197,\"start\":105196},{\"end\":105207,\"start\":105206},{\"end\":105216,\"start\":105215},{\"end\":105524,\"start\":105523},{\"end\":105537,\"start\":105536},{\"end\":105548,\"start\":105547},{\"end\":105550,\"start\":105549},{\"end\":105564,\"start\":105563},{\"end\":105860,\"start\":105859},{\"end\":105870,\"start\":105869},{\"end\":106135,\"start\":106134},{\"end\":106137,\"start\":106136},{\"end\":106321,\"start\":106320},{\"end\":106330,\"start\":106329},{\"end\":106343,\"start\":106342},{\"end\":106671,\"start\":106670},{\"end\":106880,\"start\":106879},{\"end\":106889,\"start\":106888},{\"end\":106891,\"start\":106890},{\"end\":107144,\"start\":107143},{\"end\":107374,\"start\":107373},{\"end\":107376,\"start\":107375},{\"end\":107390,\"start\":107389},{\"end\":107396,\"start\":107395},{\"end\":107404,\"start\":107403},{\"end\":107742,\"start\":107741},{\"end\":107750,\"start\":107749},{\"end\":107895,\"start\":107894},{\"end\":107905,\"start\":107904},{\"end\":107919,\"start\":107918},{\"end\":108273,\"start\":108272},{\"end\":108282,\"start\":108281},{\"end\":108290,\"start\":108289},{\"end\":108306,\"start\":108305},{\"end\":108313,\"start\":108312},{\"end\":108674,\"start\":108673},{\"end\":108682,\"start\":108681},{\"end\":108963,\"start\":108962},{\"end\":108971,\"start\":108970},{\"end\":108973,\"start\":108972},{\"end\":108980,\"start\":108979},{\"end\":108988,\"start\":108987},{\"end\":109268,\"start\":109267},{\"end\":109274,\"start\":109273},{\"end\":109276,\"start\":109275},{\"end\":109574,\"start\":109573},{\"end\":109583,\"start\":109582},{\"end\":109593,\"start\":109592},{\"end\":109608,\"start\":109607},{\"end\":109861,\"start\":109860},{\"end\":109874,\"start\":109873},{\"end\":109881,\"start\":109880},{\"end\":110188,\"start\":110187},{\"end\":110190,\"start\":110189},{\"end\":110199,\"start\":110198},{\"end\":110475,\"start\":110474},{\"end\":110477,\"start\":110476},{\"end\":110488,\"start\":110487},{\"end\":110490,\"start\":110489},{\"end\":110726,\"start\":110723},{\"end\":110728,\"start\":110727},{\"end\":110743,\"start\":110738},{\"end\":110828,\"start\":110827},{\"end\":110830,\"start\":110829},{\"end\":111037,\"start\":111036},{\"end\":111050,\"start\":111049},{\"end\":111059,\"start\":111058},{\"end\":111315,\"start\":111314},{\"end\":111317,\"start\":111316},{\"end\":111544,\"start\":111543},{\"end\":111556,\"start\":111555},{\"end\":111566,\"start\":111565},{\"end\":111785,\"start\":111784},{\"end\":111796,\"start\":111795},{\"end\":112015,\"start\":112014},{\"end\":112030,\"start\":112029},{\"end\":112037,\"start\":112036},{\"end\":112049,\"start\":112048},{\"end\":112338,\"start\":112337},{\"end\":112346,\"start\":112345},{\"end\":112352,\"start\":112351},{\"end\":112360,\"start\":112359},{\"end\":112682,\"start\":112681},{\"end\":112684,\"start\":112683},{\"end\":112696,\"start\":112695},{\"end\":112698,\"start\":112697},{\"end\":112706,\"start\":112705},{\"end\":112708,\"start\":112707},{\"end\":113099,\"start\":113098},{\"end\":113109,\"start\":113108},{\"end\":113119,\"start\":113118},{\"end\":113368,\"start\":113367},{\"end\":113370,\"start\":113369},{\"end\":113382,\"start\":113381},{\"end\":113565,\"start\":113564},{\"end\":113572,\"start\":113571},{\"end\":113796,\"start\":113795},{\"end\":113950,\"start\":113949},{\"end\":113956,\"start\":113955},{\"end\":113963,\"start\":113962},{\"end\":114239,\"start\":114238},{\"end\":114245,\"start\":114244},{\"end\":114252,\"start\":114251},{\"end\":114530,\"start\":114529},{\"end\":114538,\"start\":114537},{\"end\":114546,\"start\":114545},{\"end\":114552,\"start\":114551},{\"end\":114560,\"start\":114559},{\"end\":114911,\"start\":114910},{\"end\":114919,\"start\":114918},{\"end\":114928,\"start\":114927},{\"end\":114941,\"start\":114940},{\"end\":115306,\"start\":115305},{\"end\":115308,\"start\":115307},{\"end\":115319,\"start\":115318},{\"end\":115329,\"start\":115328},{\"end\":115331,\"start\":115330},{\"end\":115659,\"start\":115658},{\"end\":115661,\"start\":115660},{\"end\":115918,\"start\":115917},{\"end\":115927,\"start\":115926},{\"end\":116321,\"start\":116320},{\"end\":116330,\"start\":116329},{\"end\":116679,\"start\":116678},{\"end\":116687,\"start\":116686},{\"end\":116695,\"start\":116694},{\"end\":116962,\"start\":116961},{\"end\":116972,\"start\":116968},{\"end\":116980,\"start\":116979},{\"end\":116989,\"start\":116988},{\"end\":116999,\"start\":116998},{\"end\":117010,\"start\":117009},{\"end\":117400,\"start\":117399},{\"end\":117415,\"start\":117414},{\"end\":117427,\"start\":117426},{\"end\":117437,\"start\":117436},{\"end\":117449,\"start\":117448},{\"end\":117793,\"start\":117789},{\"end\":117800,\"start\":117799},{\"end\":117809,\"start\":117808},{\"end\":118158,\"start\":118157},{\"end\":118166,\"start\":118165},{\"end\":118174,\"start\":118173},{\"end\":118181,\"start\":118180},{\"end\":118491,\"start\":118490},{\"end\":118499,\"start\":118498},{\"end\":118507,\"start\":118506},{\"end\":118513,\"start\":118512},{\"end\":118839,\"start\":118838},{\"end\":118845,\"start\":118844},{\"end\":118851,\"start\":118848},{\"end\":119201,\"start\":119200},{\"end\":119207,\"start\":119206},{\"end\":119524,\"start\":119523},{\"end\":119526,\"start\":119525},{\"end\":119540,\"start\":119539},{\"end\":119542,\"start\":119541},{\"end\":119844,\"start\":119843},{\"end\":119852,\"start\":119851},{\"end\":119860,\"start\":119859},{\"end\":120140,\"start\":120139},{\"end\":120148,\"start\":120147},{\"end\":120157,\"start\":120156},{\"end\":120164,\"start\":120163},{\"end\":120166,\"start\":120165},{\"end\":120517,\"start\":120516},{\"end\":120523,\"start\":120522},{\"end\":120531,\"start\":120530},{\"end\":120537,\"start\":120536},{\"end\":120543,\"start\":120542},{\"end\":120867,\"start\":120866},{\"end\":120869,\"start\":120868},{\"end\":121079,\"start\":121078},{\"end\":121089,\"start\":121088},{\"end\":121410,\"start\":121409},{\"end\":121419,\"start\":121418},{\"end\":121421,\"start\":121420},{\"end\":121811,\"start\":121810},{\"end\":121822,\"start\":121821},{\"end\":121833,\"start\":121832},{\"end\":122122,\"start\":122121},{\"end\":122130,\"start\":122129},{\"end\":122138,\"start\":122137},{\"end\":122382,\"start\":122381},{\"end\":122395,\"start\":122394},{\"end\":122404,\"start\":122403},{\"end\":122417,\"start\":122413},{\"end\":122754,\"start\":122753},{\"end\":122762,\"start\":122761},{\"end\":122768,\"start\":122767},{\"end\":122772,\"start\":122769},{\"end\":123130,\"start\":123129},{\"end\":123143,\"start\":123142},{\"end\":123149,\"start\":123148},{\"end\":123161,\"start\":123160},{\"end\":123499,\"start\":123498},{\"end\":123507,\"start\":123506},{\"end\":123513,\"start\":123512},{\"end\":123519,\"start\":123518},{\"end\":123527,\"start\":123526},{\"end\":123879,\"start\":123878},{\"end\":123887,\"start\":123882},{\"end\":123901,\"start\":123900},{\"end\":123903,\"start\":123902},{\"end\":124286,\"start\":124285},{\"end\":124298,\"start\":124294},{\"end\":124305,\"start\":124304},{\"end\":124640,\"start\":124639},{\"end\":124648,\"start\":124647},{\"end\":124656,\"start\":124655},{\"end\":125009,\"start\":125008},{\"end\":125017,\"start\":125016},{\"end\":125271,\"start\":125270},{\"end\":125280,\"start\":125276},{\"end\":125288,\"start\":125287},{\"end\":125599,\"start\":125598},{\"end\":125605,\"start\":125604},{\"end\":125618,\"start\":125617},{\"end\":125962,\"start\":125961},{\"end\":125970,\"start\":125969},{\"end\":125977,\"start\":125976},{\"end\":125983,\"start\":125982},{\"end\":125990,\"start\":125989},{\"end\":125999,\"start\":125998},{\"end\":126352,\"start\":126351},{\"end\":126359,\"start\":126358},{\"end\":126365,\"start\":126364},{\"end\":126373,\"start\":126372},{\"end\":126724,\"start\":126723},{\"end\":126739,\"start\":126738},{\"end\":127021,\"start\":127020},{\"end\":127027,\"start\":127026},{\"end\":127037,\"start\":127036},{\"end\":127050,\"start\":127049},{\"end\":127060,\"start\":127059},{\"end\":127544,\"start\":127543},{\"end\":127546,\"start\":127545},{\"end\":127554,\"start\":127553},{\"end\":127928,\"start\":127927},{\"end\":127936,\"start\":127935},{\"end\":127942,\"start\":127941},{\"end\":127948,\"start\":127947},{\"end\":128309,\"start\":128308},{\"end\":128574,\"start\":128573},{\"end\":128576,\"start\":128575},{\"end\":128583,\"start\":128582},{\"end\":128592,\"start\":128591},{\"end\":128895,\"start\":128894},{\"end\":128902,\"start\":128901},{\"end\":129204,\"start\":129203},{\"end\":129219,\"start\":129218},{\"end\":129226,\"start\":129225},{\"end\":129238,\"start\":129237},{\"end\":129587,\"start\":129586},{\"end\":129596,\"start\":129595},{\"end\":129605,\"start\":129601},{\"end\":129613,\"start\":129612},{\"end\":129933,\"start\":129932},{\"end\":129942,\"start\":129941},{\"end\":129948,\"start\":129947},{\"end\":129960,\"start\":129956},{\"end\":129968,\"start\":129967},{\"end\":129974,\"start\":129973},{\"end\":130306,\"start\":130305},{\"end\":130308,\"start\":130307},{\"end\":130319,\"start\":130318},{\"end\":130709,\"start\":130708},{\"end\":130717,\"start\":130716},{\"end\":130725,\"start\":130724},{\"end\":131109,\"start\":131108},{\"end\":131117,\"start\":131116},{\"end\":131125,\"start\":131124},{\"end\":131131,\"start\":131130},{\"end\":131443,\"start\":131436},{\"end\":131654,\"start\":131647},{\"end\":131962,\"start\":131961},{\"end\":131969,\"start\":131968},{\"end\":131978,\"start\":131977},{\"end\":131986,\"start\":131985},{\"end\":131994,\"start\":131993},{\"end\":132408,\"start\":132407},{\"end\":132416,\"start\":132415},{\"end\":132424,\"start\":132423},{\"end\":132433,\"start\":132432},{\"end\":132446,\"start\":132445},{\"end\":132454,\"start\":132453},{\"end\":132461,\"start\":132460},{\"end\":132929,\"start\":132928},{\"end\":132937,\"start\":132936},{\"end\":132945,\"start\":132944},{\"end\":132953,\"start\":132952},{\"end\":132960,\"start\":132959},{\"end\":132973,\"start\":132972},{\"end\":133349,\"start\":133348},{\"end\":133363,\"start\":133362},{\"end\":133376,\"start\":133375},{\"end\":133378,\"start\":133377},{\"end\":133692,\"start\":133691},{\"end\":133700,\"start\":133699},{\"end\":133710,\"start\":133709}]", "bib_author_last_name": "[{\"end\":97828,\"start\":97818},{\"end\":97836,\"start\":97832},{\"end\":97845,\"start\":97840},{\"end\":98126,\"start\":98120},{\"end\":98137,\"start\":98130},{\"end\":98148,\"start\":98141},{\"end\":98159,\"start\":98152},{\"end\":98467,\"start\":98460},{\"end\":98478,\"start\":98471},{\"end\":98486,\"start\":98482},{\"end\":98797,\"start\":98793},{\"end\":98804,\"start\":98801},{\"end\":98812,\"start\":98808},{\"end\":99081,\"start\":99079},{\"end\":99089,\"start\":99085},{\"end\":99095,\"start\":99093},{\"end\":99102,\"start\":99099},{\"end\":99109,\"start\":99106},{\"end\":99120,\"start\":99113},{\"end\":99465,\"start\":99463},{\"end\":99472,\"start\":99469},{\"end\":99480,\"start\":99476},{\"end\":99491,\"start\":99484},{\"end\":99904,\"start\":99900},{\"end\":99912,\"start\":99908},{\"end\":99920,\"start\":99916},{\"end\":100291,\"start\":100287},{\"end\":100299,\"start\":100295},{\"end\":100307,\"start\":100303},{\"end\":100651,\"start\":100637},{\"end\":100662,\"start\":100657},{\"end\":100672,\"start\":100668},{\"end\":100965,\"start\":100959},{\"end\":100978,\"start\":100971},{\"end\":101358,\"start\":101356},{\"end\":101369,\"start\":101364},{\"end\":101377,\"start\":101373},{\"end\":101752,\"start\":101749},{\"end\":101760,\"start\":101756},{\"end\":102078,\"start\":102072},{\"end\":102089,\"start\":102084},{\"end\":102097,\"start\":102093},{\"end\":102390,\"start\":102384},{\"end\":102401,\"start\":102396},{\"end\":102409,\"start\":102405},{\"end\":102739,\"start\":102735},{\"end\":102754,\"start\":102745},{\"end\":102767,\"start\":102758},{\"end\":102781,\"start\":102773},{\"end\":102787,\"start\":102785},{\"end\":103038,\"start\":103032},{\"end\":103052,\"start\":103042},{\"end\":103062,\"start\":103056},{\"end\":103073,\"start\":103066},{\"end\":103355,\"start\":103348},{\"end\":103365,\"start\":103359},{\"end\":103374,\"start\":103369},{\"end\":103383,\"start\":103378},{\"end\":103696,\"start\":103692},{\"end\":103702,\"start\":103700},{\"end\":103717,\"start\":103710},{\"end\":104085,\"start\":104079},{\"end\":104097,\"start\":104089},{\"end\":104105,\"start\":104101},{\"end\":104461,\"start\":104459},{\"end\":104469,\"start\":104465},{\"end\":104478,\"start\":104473},{\"end\":104485,\"start\":104482},{\"end\":104492,\"start\":104489},{\"end\":104876,\"start\":104868},{\"end\":104890,\"start\":104880},{\"end\":104901,\"start\":104894},{\"end\":105204,\"start\":105198},{\"end\":105213,\"start\":105208},{\"end\":105221,\"start\":105217},{\"end\":105534,\"start\":105525},{\"end\":105545,\"start\":105538},{\"end\":105561,\"start\":105551},{\"end\":105573,\"start\":105565},{\"end\":105867,\"start\":105861},{\"end\":105880,\"start\":105871},{\"end\":106142,\"start\":106138},{\"end\":106327,\"start\":106322},{\"end\":106340,\"start\":106331},{\"end\":106357,\"start\":106344},{\"end\":106680,\"start\":106672},{\"end\":106886,\"start\":106881},{\"end\":106896,\"start\":106892},{\"end\":107148,\"start\":107145},{\"end\":107387,\"start\":107377},{\"end\":107393,\"start\":107391},{\"end\":107401,\"start\":107397},{\"end\":107410,\"start\":107405},{\"end\":107747,\"start\":107743},{\"end\":107756,\"start\":107751},{\"end\":107902,\"start\":107896},{\"end\":107916,\"start\":107906},{\"end\":107926,\"start\":107920},{\"end\":108279,\"start\":108274},{\"end\":108287,\"start\":108283},{\"end\":108303,\"start\":108291},{\"end\":108310,\"start\":108307},{\"end\":108323,\"start\":108314},{\"end\":108679,\"start\":108675},{\"end\":108690,\"start\":108683},{\"end\":108968,\"start\":108964},{\"end\":108977,\"start\":108974},{\"end\":108985,\"start\":108981},{\"end\":108994,\"start\":108989},{\"end\":109271,\"start\":109269},{\"end\":109281,\"start\":109277},{\"end\":109580,\"start\":109575},{\"end\":109590,\"start\":109584},{\"end\":109605,\"start\":109594},{\"end\":109614,\"start\":109609},{\"end\":109871,\"start\":109862},{\"end\":109878,\"start\":109875},{\"end\":109888,\"start\":109882},{\"end\":110196,\"start\":110191},{\"end\":110208,\"start\":110200},{\"end\":110485,\"start\":110478},{\"end\":110497,\"start\":110491},{\"end\":110721,\"start\":110717},{\"end\":110732,\"start\":110729},{\"end\":110736,\"start\":110734},{\"end\":110746,\"start\":110744},{\"end\":110752,\"start\":110748},{\"end\":110836,\"start\":110831},{\"end\":111047,\"start\":111038},{\"end\":111056,\"start\":111051},{\"end\":111068,\"start\":111060},{\"end\":111324,\"start\":111318},{\"end\":111553,\"start\":111545},{\"end\":111563,\"start\":111557},{\"end\":111574,\"start\":111567},{\"end\":111793,\"start\":111786},{\"end\":111803,\"start\":111797},{\"end\":112027,\"start\":112016},{\"end\":112034,\"start\":112031},{\"end\":112046,\"start\":112038},{\"end\":112056,\"start\":112050},{\"end\":112343,\"start\":112339},{\"end\":112349,\"start\":112347},{\"end\":112357,\"start\":112353},{\"end\":112364,\"start\":112361},{\"end\":112693,\"start\":112685},{\"end\":112703,\"start\":112699},{\"end\":112718,\"start\":112709},{\"end\":113106,\"start\":113100},{\"end\":113116,\"start\":113110},{\"end\":113128,\"start\":113120},{\"end\":113379,\"start\":113371},{\"end\":113388,\"start\":113383},{\"end\":113569,\"start\":113566},{\"end\":113577,\"start\":113573},{\"end\":113804,\"start\":113797},{\"end\":113953,\"start\":113951},{\"end\":113960,\"start\":113957},{\"end\":113968,\"start\":113964},{\"end\":114242,\"start\":114240},{\"end\":114249,\"start\":114246},{\"end\":114257,\"start\":114253},{\"end\":114535,\"start\":114531},{\"end\":114543,\"start\":114539},{\"end\":114549,\"start\":114547},{\"end\":114557,\"start\":114553},{\"end\":114564,\"start\":114561},{\"end\":114916,\"start\":114912},{\"end\":114925,\"start\":114920},{\"end\":114938,\"start\":114929},{\"end\":114950,\"start\":114942},{\"end\":115316,\"start\":115309},{\"end\":115326,\"start\":115320},{\"end\":115339,\"start\":115332},{\"end\":115670,\"start\":115662},{\"end\":115924,\"start\":115919},{\"end\":115935,\"start\":115928},{\"end\":116327,\"start\":116322},{\"end\":116338,\"start\":116331},{\"end\":116684,\"start\":116680},{\"end\":116692,\"start\":116688},{\"end\":116699,\"start\":116696},{\"end\":116966,\"start\":116963},{\"end\":116977,\"start\":116973},{\"end\":116986,\"start\":116981},{\"end\":116996,\"start\":116990},{\"end\":117007,\"start\":117000},{\"end\":117018,\"start\":117011},{\"end\":117412,\"start\":117401},{\"end\":117424,\"start\":117416},{\"end\":117434,\"start\":117428},{\"end\":117446,\"start\":117438},{\"end\":117456,\"start\":117450},{\"end\":117797,\"start\":117794},{\"end\":117806,\"start\":117801},{\"end\":117817,\"start\":117810},{\"end\":118163,\"start\":118159},{\"end\":118171,\"start\":118167},{\"end\":118178,\"start\":118175},{\"end\":118185,\"start\":118182},{\"end\":118496,\"start\":118492},{\"end\":118504,\"start\":118500},{\"end\":118510,\"start\":118508},{\"end\":118517,\"start\":118514},{\"end\":118842,\"start\":118840},{\"end\":118856,\"start\":118852},{\"end\":119204,\"start\":119202},{\"end\":119211,\"start\":119208},{\"end\":119537,\"start\":119527},{\"end\":119548,\"start\":119543},{\"end\":119849,\"start\":119845},{\"end\":119857,\"start\":119853},{\"end\":119865,\"start\":119861},{\"end\":120145,\"start\":120141},{\"end\":120154,\"start\":120149},{\"end\":120161,\"start\":120158},{\"end\":120178,\"start\":120167},{\"end\":120520,\"start\":120518},{\"end\":120528,\"start\":120524},{\"end\":120534,\"start\":120532},{\"end\":120540,\"start\":120538},{\"end\":120546,\"start\":120544},{\"end\":120880,\"start\":120870},{\"end\":121086,\"start\":121080},{\"end\":121096,\"start\":121090},{\"end\":121416,\"start\":121411},{\"end\":121425,\"start\":121422},{\"end\":121819,\"start\":121812},{\"end\":121830,\"start\":121823},{\"end\":121843,\"start\":121834},{\"end\":122127,\"start\":122123},{\"end\":122135,\"start\":122131},{\"end\":122146,\"start\":122139},{\"end\":122392,\"start\":122383},{\"end\":122401,\"start\":122396},{\"end\":122411,\"start\":122405},{\"end\":122426,\"start\":122418},{\"end\":122759,\"start\":122755},{\"end\":122765,\"start\":122763},{\"end\":122780,\"start\":122773},{\"end\":123140,\"start\":123131},{\"end\":123146,\"start\":123144},{\"end\":123158,\"start\":123150},{\"end\":123173,\"start\":123162},{\"end\":123504,\"start\":123500},{\"end\":123510,\"start\":123508},{\"end\":123516,\"start\":123514},{\"end\":123524,\"start\":123520},{\"end\":123532,\"start\":123528},{\"end\":123898,\"start\":123888},{\"end\":123910,\"start\":123904},{\"end\":124292,\"start\":124287},{\"end\":124302,\"start\":124299},{\"end\":124315,\"start\":124306},{\"end\":124645,\"start\":124641},{\"end\":124653,\"start\":124649},{\"end\":124661,\"start\":124657},{\"end\":125014,\"start\":125010},{\"end\":125023,\"start\":125018},{\"end\":125274,\"start\":125272},{\"end\":125285,\"start\":125281},{\"end\":125291,\"start\":125289},{\"end\":125602,\"start\":125600},{\"end\":125615,\"start\":125606},{\"end\":125630,\"start\":125619},{\"end\":125967,\"start\":125963},{\"end\":125974,\"start\":125971},{\"end\":125980,\"start\":125978},{\"end\":125987,\"start\":125984},{\"end\":125996,\"start\":125991},{\"end\":126002,\"start\":126000},{\"end\":126356,\"start\":126353},{\"end\":126362,\"start\":126360},{\"end\":126370,\"start\":126366},{\"end\":126379,\"start\":126374},{\"end\":126736,\"start\":126725},{\"end\":126746,\"start\":126740},{\"end\":127024,\"start\":127022},{\"end\":127034,\"start\":127028},{\"end\":127047,\"start\":127038},{\"end\":127057,\"start\":127051},{\"end\":127065,\"start\":127061},{\"end\":127551,\"start\":127547},{\"end\":127566,\"start\":127555},{\"end\":127933,\"start\":127929},{\"end\":127939,\"start\":127937},{\"end\":127945,\"start\":127943},{\"end\":127953,\"start\":127949},{\"end\":128313,\"start\":128310},{\"end\":128580,\"start\":128577},{\"end\":128589,\"start\":128584},{\"end\":128597,\"start\":128593},{\"end\":128899,\"start\":128896},{\"end\":128908,\"start\":128903},{\"end\":129216,\"start\":129205},{\"end\":129223,\"start\":129220},{\"end\":129235,\"start\":129227},{\"end\":129245,\"start\":129239},{\"end\":129593,\"start\":129588},{\"end\":129599,\"start\":129597},{\"end\":129610,\"start\":129606},{\"end\":129616,\"start\":129614},{\"end\":129939,\"start\":129934},{\"end\":129945,\"start\":129943},{\"end\":129954,\"start\":129949},{\"end\":129965,\"start\":129961},{\"end\":129971,\"start\":129969},{\"end\":129978,\"start\":129975},{\"end\":130316,\"start\":130309},{\"end\":130324,\"start\":130320},{\"end\":130714,\"start\":130710},{\"end\":130722,\"start\":130718},{\"end\":130733,\"start\":130726},{\"end\":131114,\"start\":131110},{\"end\":131122,\"start\":131118},{\"end\":131128,\"start\":131126},{\"end\":131137,\"start\":131132},{\"end\":131453,\"start\":131444},{\"end\":131665,\"start\":131655},{\"end\":131732,\"start\":131725},{\"end\":131804,\"start\":131790},{\"end\":131813,\"start\":131806},{\"end\":131966,\"start\":131963},{\"end\":131975,\"start\":131970},{\"end\":131983,\"start\":131979},{\"end\":131991,\"start\":131987},{\"end\":131999,\"start\":131995},{\"end\":132413,\"start\":132409},{\"end\":132421,\"start\":132417},{\"end\":132430,\"start\":132425},{\"end\":132443,\"start\":132434},{\"end\":132451,\"start\":132447},{\"end\":132458,\"start\":132455},{\"end\":132465,\"start\":132462},{\"end\":132934,\"start\":132930},{\"end\":132942,\"start\":132938},{\"end\":132950,\"start\":132946},{\"end\":132957,\"start\":132954},{\"end\":132970,\"start\":132961},{\"end\":132977,\"start\":132974},{\"end\":133360,\"start\":133350},{\"end\":133373,\"start\":133364},{\"end\":133385,\"start\":133379},{\"end\":133697,\"start\":133693},{\"end\":133707,\"start\":133701},{\"end\":133715,\"start\":133711},{\"end\":135444,\"start\":135442},{\"end\":135450,\"start\":135446}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9676536},\"end\":98060,\"start\":97764},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6073848},\"end\":98392,\"start\":98062},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":12667570},\"end\":98736,\"start\":98394},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":7637306},\"end\":99024,\"start\":98738},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7839639},\"end\":99395,\"start\":99026},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":17516538},\"end\":99808,\"start\":99397},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4231589},\"end\":100186,\"start\":99810},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3085328},\"end\":100582,\"start\":100188},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206485682},\"end\":100877,\"start\":100584},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5755403},\"end\":101256,\"start\":100879},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14411054},\"end\":101653,\"start\":101258},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8190391},\"end\":102013,\"start\":101655},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1243044},\"end\":102294,\"start\":102015},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":123956},\"end\":102705,\"start\":102296},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1417739},\"end\":102991,\"start\":102707},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":12909589},\"end\":103297,\"start\":102993},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17781204},\"end\":103585,\"start\":103299},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4242592},\"end\":104027,\"start\":103587},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":920128},\"end\":104367,\"start\":104029},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":648645},\"end\":104783,\"start\":104369},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":207565051},\"end\":105134,\"start\":104785},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2430392},\"end\":105460,\"start\":105136},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8058328},\"end\":105794,\"start\":105462},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":20249873},\"end\":106098,\"start\":105796},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":145760136},\"end\":106258,\"start\":106100},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":17271988},\"end\":106622,\"start\":106260},{\"attributes\":{\"id\":\"b26\"},\"end\":106848,\"start\":106624},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15738519},\"end\":107111,\"start\":106850},{\"attributes\":{\"id\":\"b28\"},\"end\":107293,\"start\":107113},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":220766223},\"end\":107709,\"start\":107295},{\"attributes\":{\"id\":\"b30\"},\"end\":107833,\"start\":107711},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":61507129},\"end\":108212,\"start\":107835},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":54706173},\"end\":108597,\"start\":108214},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14307353},\"end\":108915,\"start\":108599},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1910869},\"end\":109187,\"start\":108917},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":5937097},\"end\":109528,\"start\":109189},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2226219},\"end\":109817,\"start\":109530},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2350582},\"end\":110093,\"start\":109819},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":18965681},\"end\":110422,\"start\":110095},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":12410940},\"end\":110713,\"start\":110424},{\"attributes\":{\"id\":\"b40\"},\"end\":110809,\"start\":110715},{\"attributes\":{\"id\":\"b41\"},\"end\":110956,\"start\":110811},{\"attributes\":{\"id\":\"b42\"},\"end\":111264,\"start\":110958},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":154706693},\"end\":111523,\"start\":111266},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":37342840},\"end\":111738,\"start\":111525},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":29333691},\"end\":111948,\"start\":111740},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":14119368},\"end\":112261,\"start\":111950},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":10335587},\"end\":112622,\"start\":112263},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":15466712},\"end\":113039,\"start\":112624},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":3785212},\"end\":113323,\"start\":113041},{\"attributes\":{\"id\":\"b50\"},\"end\":113506,\"start\":113325},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":9560504},\"end\":113755,\"start\":113508},{\"attributes\":{\"id\":\"b52\"},\"end\":113879,\"start\":113757},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":9945811},\"end\":114184,\"start\":113881},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":6301699},\"end\":114447,\"start\":114186},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":121934172},\"end\":114839,\"start\":114449},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":76390},\"end\":115172,\"start\":114841},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":14546090},\"end\":115624,\"start\":115174},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":14771144},\"end\":115822,\"start\":115626},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":2296945},\"end\":116227,\"start\":115824},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":11607634},\"end\":116604,\"start\":116229},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":16933149},\"end\":116886,\"start\":116606},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":14520710},\"end\":117291,\"start\":116888},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":44411909},\"end\":117713,\"start\":117293},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":11266272},\"end\":118075,\"start\":117715},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":8172104},\"end\":118390,\"start\":118077},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":16800608},\"end\":118781,\"start\":118392},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":8030090},\"end\":119121,\"start\":118783},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":8203339},\"end\":119477,\"start\":119123},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":7458299},\"end\":119773,\"start\":119479},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":8202993},\"end\":120069,\"start\":119775},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":1506680},\"end\":120426,\"start\":120071},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":6844671},\"end\":120809,\"start\":120428},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":11431150},\"end\":121034,\"start\":120811},{\"attributes\":{\"id\":\"b74\"},\"end\":121270,\"start\":121036},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":6485120},\"end\":121757,\"start\":121272},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":5389712},\"end\":122063,\"start\":121759},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":18343904},\"end\":122341,\"start\":122065},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":215754778},\"end\":122648,\"start\":122343},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":4242592},\"end\":123090,\"start\":122650},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":206591190},\"end\":123411,\"start\":123092},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":14842257},\"end\":123768,\"start\":123413},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":2139329},\"end\":124187,\"start\":123770},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":11401549},\"end\":124562,\"start\":124189},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":6397828},\"end\":124924,\"start\":124564},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":1079180},\"end\":125214,\"start\":124926},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":206661721},\"end\":125542,\"start\":125216},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":3143815},\"end\":125894,\"start\":125544},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":1463664},\"end\":126285,\"start\":125896},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":36529090},\"end\":126658,\"start\":126287},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":209587},\"end\":126922,\"start\":126660},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":14460915},\"end\":127463,\"start\":126924},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":3033434},\"end\":127846,\"start\":127465},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":123000990},\"end\":128223,\"start\":127848},{\"attributes\":{\"id\":\"b94\"},\"end\":128527,\"start\":128225},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":604536},\"end\":128842,\"start\":128529},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":15611611},\"end\":129124,\"start\":128844},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":16102573},\"end\":129523,\"start\":129126},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":16787055},\"end\":129866,\"start\":129525},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":24927},\"end\":130244,\"start\":129868},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":40100916},\"end\":130623,\"start\":130246},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":18970097},\"end\":131021,\"start\":130625},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":3873213},\"end\":131432,\"start\":131023},{\"attributes\":{\"id\":\"b103\"},\"end\":131520,\"start\":131434},{\"attributes\":{\"id\":\"b104\"},\"end\":131643,\"start\":131522},{\"attributes\":{\"id\":\"b105\"},\"end\":131721,\"start\":131645},{\"attributes\":{\"id\":\"b106\"},\"end\":131786,\"start\":131723},{\"attributes\":{\"id\":\"b107\"},\"end\":131905,\"start\":131788},{\"attributes\":{\"id\":\"b108\",\"matched_paper_id\":19627385},\"end\":132259,\"start\":131907},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":14736010},\"end\":132830,\"start\":132261},{\"attributes\":{\"id\":\"b110\",\"matched_paper_id\":848674},\"end\":133281,\"start\":132832},{\"attributes\":{\"id\":\"b111\",\"matched_paper_id\":195908774},\"end\":133626,\"start\":133283},{\"attributes\":{\"id\":\"b112\",\"matched_paper_id\":1243243},\"end\":133923,\"start\":133628},{\"attributes\":{\"id\":\"b113\"},\"end\":134736,\"start\":133925},{\"attributes\":{\"id\":\"b114\"},\"end\":135207,\"start\":134738},{\"attributes\":{\"id\":\"b115\"},\"end\":138135,\"start\":135209}]", "bib_title": "[{\"end\":97814,\"start\":97764},{\"end\":98116,\"start\":98062},{\"end\":98456,\"start\":98394},{\"end\":98789,\"start\":98738},{\"end\":99075,\"start\":99026},{\"end\":99459,\"start\":99397},{\"end\":99896,\"start\":99810},{\"end\":100283,\"start\":100188},{\"end\":100631,\"start\":100584},{\"end\":100953,\"start\":100879},{\"end\":101352,\"start\":101258},{\"end\":101745,\"start\":101655},{\"end\":102068,\"start\":102015},{\"end\":102380,\"start\":102296},{\"end\":102731,\"start\":102707},{\"end\":103028,\"start\":102993},{\"end\":103342,\"start\":103299},{\"end\":103688,\"start\":103587},{\"end\":104072,\"start\":104029},{\"end\":104455,\"start\":104369},{\"end\":104864,\"start\":104785},{\"end\":105194,\"start\":105136},{\"end\":105521,\"start\":105462},{\"end\":105857,\"start\":105796},{\"end\":106132,\"start\":106100},{\"end\":106318,\"start\":106260},{\"end\":106877,\"start\":106850},{\"end\":107371,\"start\":107295},{\"end\":107892,\"start\":107835},{\"end\":108270,\"start\":108214},{\"end\":108671,\"start\":108599},{\"end\":108960,\"start\":108917},{\"end\":109265,\"start\":109189},{\"end\":109571,\"start\":109530},{\"end\":109858,\"start\":109819},{\"end\":110185,\"start\":110095},{\"end\":110472,\"start\":110424},{\"end\":111312,\"start\":111266},{\"end\":111541,\"start\":111525},{\"end\":111782,\"start\":111740},{\"end\":112012,\"start\":111950},{\"end\":112335,\"start\":112263},{\"end\":112679,\"start\":112624},{\"end\":113096,\"start\":113041},{\"end\":113562,\"start\":113508},{\"end\":113947,\"start\":113881},{\"end\":114236,\"start\":114186},{\"end\":114527,\"start\":114449},{\"end\":114908,\"start\":114841},{\"end\":115303,\"start\":115174},{\"end\":115656,\"start\":115626},{\"end\":115915,\"start\":115824},{\"end\":116318,\"start\":116229},{\"end\":116676,\"start\":116606},{\"end\":116959,\"start\":116888},{\"end\":117397,\"start\":117293},{\"end\":117787,\"start\":117715},{\"end\":118155,\"start\":118077},{\"end\":118488,\"start\":118392},{\"end\":118836,\"start\":118783},{\"end\":119198,\"start\":119123},{\"end\":119521,\"start\":119479},{\"end\":119841,\"start\":119775},{\"end\":120137,\"start\":120071},{\"end\":120514,\"start\":120428},{\"end\":120864,\"start\":120811},{\"end\":121407,\"start\":121272},{\"end\":121808,\"start\":121759},{\"end\":122119,\"start\":122065},{\"end\":122379,\"start\":122343},{\"end\":122751,\"start\":122650},{\"end\":123127,\"start\":123092},{\"end\":123496,\"start\":123413},{\"end\":123876,\"start\":123770},{\"end\":124283,\"start\":124189},{\"end\":124637,\"start\":124564},{\"end\":125006,\"start\":124926},{\"end\":125268,\"start\":125216},{\"end\":125596,\"start\":125544},{\"end\":125959,\"start\":125896},{\"end\":126349,\"start\":126287},{\"end\":126721,\"start\":126660},{\"end\":127018,\"start\":126924},{\"end\":127541,\"start\":127465},{\"end\":127925,\"start\":127848},{\"end\":128571,\"start\":128529},{\"end\":128892,\"start\":128844},{\"end\":129201,\"start\":129126},{\"end\":129584,\"start\":129525},{\"end\":129930,\"start\":129868},{\"end\":130303,\"start\":130246},{\"end\":130706,\"start\":130625},{\"end\":131106,\"start\":131023},{\"end\":131959,\"start\":131907},{\"end\":132405,\"start\":132261},{\"end\":132926,\"start\":132832},{\"end\":133346,\"start\":133283},{\"end\":133689,\"start\":133628},{\"end\":135440,\"start\":135209}]", "bib_author": "[{\"end\":97830,\"start\":97816},{\"end\":97838,\"start\":97830},{\"end\":97847,\"start\":97838},{\"end\":98128,\"start\":98118},{\"end\":98139,\"start\":98128},{\"end\":98150,\"start\":98139},{\"end\":98161,\"start\":98150},{\"end\":98469,\"start\":98458},{\"end\":98480,\"start\":98469},{\"end\":98488,\"start\":98480},{\"end\":98799,\"start\":98791},{\"end\":98806,\"start\":98799},{\"end\":98814,\"start\":98806},{\"end\":99083,\"start\":99077},{\"end\":99091,\"start\":99083},{\"end\":99097,\"start\":99091},{\"end\":99104,\"start\":99097},{\"end\":99111,\"start\":99104},{\"end\":99122,\"start\":99111},{\"end\":99467,\"start\":99461},{\"end\":99474,\"start\":99467},{\"end\":99482,\"start\":99474},{\"end\":99493,\"start\":99482},{\"end\":99906,\"start\":99898},{\"end\":99914,\"start\":99906},{\"end\":99922,\"start\":99914},{\"end\":100293,\"start\":100285},{\"end\":100301,\"start\":100293},{\"end\":100309,\"start\":100301},{\"end\":100653,\"start\":100633},{\"end\":100664,\"start\":100653},{\"end\":100674,\"start\":100664},{\"end\":100967,\"start\":100955},{\"end\":100980,\"start\":100967},{\"end\":101360,\"start\":101354},{\"end\":101371,\"start\":101360},{\"end\":101379,\"start\":101371},{\"end\":101754,\"start\":101747},{\"end\":101762,\"start\":101754},{\"end\":102080,\"start\":102070},{\"end\":102091,\"start\":102080},{\"end\":102099,\"start\":102091},{\"end\":102392,\"start\":102382},{\"end\":102403,\"start\":102392},{\"end\":102411,\"start\":102403},{\"end\":102741,\"start\":102733},{\"end\":102756,\"start\":102741},{\"end\":102769,\"start\":102756},{\"end\":102783,\"start\":102769},{\"end\":102789,\"start\":102783},{\"end\":103040,\"start\":103030},{\"end\":103054,\"start\":103040},{\"end\":103064,\"start\":103054},{\"end\":103075,\"start\":103064},{\"end\":103357,\"start\":103344},{\"end\":103367,\"start\":103357},{\"end\":103376,\"start\":103367},{\"end\":103385,\"start\":103376},{\"end\":103698,\"start\":103690},{\"end\":103704,\"start\":103698},{\"end\":103719,\"start\":103704},{\"end\":104087,\"start\":104074},{\"end\":104099,\"start\":104087},{\"end\":104107,\"start\":104099},{\"end\":104463,\"start\":104457},{\"end\":104471,\"start\":104463},{\"end\":104480,\"start\":104471},{\"end\":104487,\"start\":104480},{\"end\":104494,\"start\":104487},{\"end\":104878,\"start\":104866},{\"end\":104892,\"start\":104878},{\"end\":104903,\"start\":104892},{\"end\":105206,\"start\":105196},{\"end\":105215,\"start\":105206},{\"end\":105223,\"start\":105215},{\"end\":105536,\"start\":105523},{\"end\":105547,\"start\":105536},{\"end\":105563,\"start\":105547},{\"end\":105575,\"start\":105563},{\"end\":105869,\"start\":105859},{\"end\":105882,\"start\":105869},{\"end\":106144,\"start\":106134},{\"end\":106329,\"start\":106320},{\"end\":106342,\"start\":106329},{\"end\":106359,\"start\":106342},{\"end\":106682,\"start\":106670},{\"end\":106888,\"start\":106879},{\"end\":106898,\"start\":106888},{\"end\":107150,\"start\":107143},{\"end\":107389,\"start\":107373},{\"end\":107395,\"start\":107389},{\"end\":107403,\"start\":107395},{\"end\":107412,\"start\":107403},{\"end\":107749,\"start\":107741},{\"end\":107758,\"start\":107749},{\"end\":107904,\"start\":107894},{\"end\":107918,\"start\":107904},{\"end\":107928,\"start\":107918},{\"end\":108281,\"start\":108272},{\"end\":108289,\"start\":108281},{\"end\":108305,\"start\":108289},{\"end\":108312,\"start\":108305},{\"end\":108325,\"start\":108312},{\"end\":108681,\"start\":108673},{\"end\":108692,\"start\":108681},{\"end\":108970,\"start\":108962},{\"end\":108979,\"start\":108970},{\"end\":108987,\"start\":108979},{\"end\":108996,\"start\":108987},{\"end\":109273,\"start\":109267},{\"end\":109283,\"start\":109273},{\"end\":109582,\"start\":109573},{\"end\":109592,\"start\":109582},{\"end\":109607,\"start\":109592},{\"end\":109616,\"start\":109607},{\"end\":109873,\"start\":109860},{\"end\":109880,\"start\":109873},{\"end\":109890,\"start\":109880},{\"end\":110198,\"start\":110187},{\"end\":110210,\"start\":110198},{\"end\":110487,\"start\":110474},{\"end\":110499,\"start\":110487},{\"end\":110723,\"start\":110717},{\"end\":110734,\"start\":110723},{\"end\":110738,\"start\":110734},{\"end\":110748,\"start\":110738},{\"end\":110754,\"start\":110748},{\"end\":110838,\"start\":110827},{\"end\":111049,\"start\":111036},{\"end\":111058,\"start\":111049},{\"end\":111070,\"start\":111058},{\"end\":111326,\"start\":111314},{\"end\":111555,\"start\":111543},{\"end\":111565,\"start\":111555},{\"end\":111576,\"start\":111565},{\"end\":111795,\"start\":111784},{\"end\":111805,\"start\":111795},{\"end\":112029,\"start\":112014},{\"end\":112036,\"start\":112029},{\"end\":112048,\"start\":112036},{\"end\":112058,\"start\":112048},{\"end\":112345,\"start\":112337},{\"end\":112351,\"start\":112345},{\"end\":112359,\"start\":112351},{\"end\":112366,\"start\":112359},{\"end\":112695,\"start\":112681},{\"end\":112705,\"start\":112695},{\"end\":112720,\"start\":112705},{\"end\":113108,\"start\":113098},{\"end\":113118,\"start\":113108},{\"end\":113130,\"start\":113118},{\"end\":113381,\"start\":113367},{\"end\":113390,\"start\":113381},{\"end\":113571,\"start\":113564},{\"end\":113579,\"start\":113571},{\"end\":113806,\"start\":113795},{\"end\":113955,\"start\":113949},{\"end\":113962,\"start\":113955},{\"end\":113970,\"start\":113962},{\"end\":114244,\"start\":114238},{\"end\":114251,\"start\":114244},{\"end\":114259,\"start\":114251},{\"end\":114537,\"start\":114529},{\"end\":114545,\"start\":114537},{\"end\":114551,\"start\":114545},{\"end\":114559,\"start\":114551},{\"end\":114566,\"start\":114559},{\"end\":114918,\"start\":114910},{\"end\":114927,\"start\":114918},{\"end\":114940,\"start\":114927},{\"end\":114952,\"start\":114940},{\"end\":115318,\"start\":115305},{\"end\":115328,\"start\":115318},{\"end\":115341,\"start\":115328},{\"end\":115672,\"start\":115658},{\"end\":115926,\"start\":115917},{\"end\":115937,\"start\":115926},{\"end\":116329,\"start\":116320},{\"end\":116340,\"start\":116329},{\"end\":116686,\"start\":116678},{\"end\":116694,\"start\":116686},{\"end\":116701,\"start\":116694},{\"end\":116968,\"start\":116961},{\"end\":116979,\"start\":116968},{\"end\":116988,\"start\":116979},{\"end\":116998,\"start\":116988},{\"end\":117009,\"start\":116998},{\"end\":117020,\"start\":117009},{\"end\":117414,\"start\":117399},{\"end\":117426,\"start\":117414},{\"end\":117436,\"start\":117426},{\"end\":117448,\"start\":117436},{\"end\":117458,\"start\":117448},{\"end\":117799,\"start\":117789},{\"end\":117808,\"start\":117799},{\"end\":117819,\"start\":117808},{\"end\":118165,\"start\":118157},{\"end\":118173,\"start\":118165},{\"end\":118180,\"start\":118173},{\"end\":118187,\"start\":118180},{\"end\":118498,\"start\":118490},{\"end\":118506,\"start\":118498},{\"end\":118512,\"start\":118506},{\"end\":118519,\"start\":118512},{\"end\":118844,\"start\":118838},{\"end\":118848,\"start\":118844},{\"end\":118858,\"start\":118848},{\"end\":119206,\"start\":119200},{\"end\":119213,\"start\":119206},{\"end\":119539,\"start\":119523},{\"end\":119550,\"start\":119539},{\"end\":119851,\"start\":119843},{\"end\":119859,\"start\":119851},{\"end\":119867,\"start\":119859},{\"end\":120147,\"start\":120139},{\"end\":120156,\"start\":120147},{\"end\":120163,\"start\":120156},{\"end\":120180,\"start\":120163},{\"end\":120522,\"start\":120516},{\"end\":120530,\"start\":120522},{\"end\":120536,\"start\":120530},{\"end\":120542,\"start\":120536},{\"end\":120548,\"start\":120542},{\"end\":120882,\"start\":120866},{\"end\":121088,\"start\":121078},{\"end\":121098,\"start\":121088},{\"end\":121418,\"start\":121409},{\"end\":121427,\"start\":121418},{\"end\":121821,\"start\":121810},{\"end\":121832,\"start\":121821},{\"end\":121845,\"start\":121832},{\"end\":122129,\"start\":122121},{\"end\":122137,\"start\":122129},{\"end\":122148,\"start\":122137},{\"end\":122394,\"start\":122381},{\"end\":122403,\"start\":122394},{\"end\":122413,\"start\":122403},{\"end\":122428,\"start\":122413},{\"end\":122761,\"start\":122753},{\"end\":122767,\"start\":122761},{\"end\":122782,\"start\":122767},{\"end\":123142,\"start\":123129},{\"end\":123148,\"start\":123142},{\"end\":123160,\"start\":123148},{\"end\":123175,\"start\":123160},{\"end\":123506,\"start\":123498},{\"end\":123512,\"start\":123506},{\"end\":123518,\"start\":123512},{\"end\":123526,\"start\":123518},{\"end\":123534,\"start\":123526},{\"end\":123882,\"start\":123878},{\"end\":123900,\"start\":123882},{\"end\":123912,\"start\":123900},{\"end\":124294,\"start\":124285},{\"end\":124304,\"start\":124294},{\"end\":124317,\"start\":124304},{\"end\":124647,\"start\":124639},{\"end\":124655,\"start\":124647},{\"end\":124663,\"start\":124655},{\"end\":125016,\"start\":125008},{\"end\":125025,\"start\":125016},{\"end\":125276,\"start\":125270},{\"end\":125287,\"start\":125276},{\"end\":125293,\"start\":125287},{\"end\":125604,\"start\":125598},{\"end\":125617,\"start\":125604},{\"end\":125632,\"start\":125617},{\"end\":125969,\"start\":125961},{\"end\":125976,\"start\":125969},{\"end\":125982,\"start\":125976},{\"end\":125989,\"start\":125982},{\"end\":125998,\"start\":125989},{\"end\":126004,\"start\":125998},{\"end\":126358,\"start\":126351},{\"end\":126364,\"start\":126358},{\"end\":126372,\"start\":126364},{\"end\":126381,\"start\":126372},{\"end\":126738,\"start\":126723},{\"end\":126748,\"start\":126738},{\"end\":127026,\"start\":127020},{\"end\":127036,\"start\":127026},{\"end\":127049,\"start\":127036},{\"end\":127059,\"start\":127049},{\"end\":127067,\"start\":127059},{\"end\":127553,\"start\":127543},{\"end\":127568,\"start\":127553},{\"end\":127935,\"start\":127927},{\"end\":127941,\"start\":127935},{\"end\":127947,\"start\":127941},{\"end\":127955,\"start\":127947},{\"end\":128315,\"start\":128308},{\"end\":128582,\"start\":128573},{\"end\":128591,\"start\":128582},{\"end\":128599,\"start\":128591},{\"end\":128901,\"start\":128894},{\"end\":128910,\"start\":128901},{\"end\":129218,\"start\":129203},{\"end\":129225,\"start\":129218},{\"end\":129237,\"start\":129225},{\"end\":129247,\"start\":129237},{\"end\":129595,\"start\":129586},{\"end\":129601,\"start\":129595},{\"end\":129612,\"start\":129601},{\"end\":129618,\"start\":129612},{\"end\":129941,\"start\":129932},{\"end\":129947,\"start\":129941},{\"end\":129956,\"start\":129947},{\"end\":129967,\"start\":129956},{\"end\":129973,\"start\":129967},{\"end\":129980,\"start\":129973},{\"end\":130318,\"start\":130305},{\"end\":130326,\"start\":130318},{\"end\":130716,\"start\":130708},{\"end\":130724,\"start\":130716},{\"end\":130735,\"start\":130724},{\"end\":131116,\"start\":131108},{\"end\":131124,\"start\":131116},{\"end\":131130,\"start\":131124},{\"end\":131139,\"start\":131130},{\"end\":131455,\"start\":131436},{\"end\":131667,\"start\":131647},{\"end\":131734,\"start\":131725},{\"end\":131806,\"start\":131790},{\"end\":131815,\"start\":131806},{\"end\":131968,\"start\":131961},{\"end\":131977,\"start\":131968},{\"end\":131985,\"start\":131977},{\"end\":131993,\"start\":131985},{\"end\":132001,\"start\":131993},{\"end\":132415,\"start\":132407},{\"end\":132423,\"start\":132415},{\"end\":132432,\"start\":132423},{\"end\":132445,\"start\":132432},{\"end\":132453,\"start\":132445},{\"end\":132460,\"start\":132453},{\"end\":132467,\"start\":132460},{\"end\":132936,\"start\":132928},{\"end\":132944,\"start\":132936},{\"end\":132952,\"start\":132944},{\"end\":132959,\"start\":132952},{\"end\":132972,\"start\":132959},{\"end\":132979,\"start\":132972},{\"end\":133362,\"start\":133348},{\"end\":133375,\"start\":133362},{\"end\":133387,\"start\":133375},{\"end\":133699,\"start\":133691},{\"end\":133709,\"start\":133699},{\"end\":133717,\"start\":133709},{\"end\":135446,\"start\":135442},{\"end\":135452,\"start\":135446}]", "bib_venue": "[{\"end\":108379,\"start\":108362},{\"end\":127210,\"start\":127147},{\"end\":135802,\"start\":135769},{\"end\":97887,\"start\":97847},{\"end\":98211,\"start\":98161},{\"end\":98546,\"start\":98488},{\"end\":98862,\"start\":98814},{\"end\":99183,\"start\":99122},{\"end\":99577,\"start\":99493},{\"end\":99980,\"start\":99922},{\"end\":100367,\"start\":100309},{\"end\":100705,\"start\":100674},{\"end\":101042,\"start\":100980},{\"end\":101437,\"start\":101379},{\"end\":101820,\"start\":101762},{\"end\":102137,\"start\":102099},{\"end\":102473,\"start\":102411},{\"end\":102820,\"start\":102789},{\"end\":103116,\"start\":103075},{\"end\":103423,\"start\":103385},{\"end\":103781,\"start\":103719},{\"end\":104183,\"start\":104107},{\"end\":104549,\"start\":104494},{\"end\":104940,\"start\":104903},{\"end\":105281,\"start\":105223},{\"end\":105602,\"start\":105575},{\"end\":105921,\"start\":105882},{\"end\":106158,\"start\":106144},{\"end\":106414,\"start\":106359},{\"end\":106668,\"start\":106624},{\"end\":106954,\"start\":106898},{\"end\":107141,\"start\":107113},{\"end\":107474,\"start\":107412},{\"end\":107739,\"start\":107711},{\"end\":108000,\"start\":107928},{\"end\":108360,\"start\":108325},{\"end\":108740,\"start\":108692},{\"end\":109029,\"start\":108996},{\"end\":109341,\"start\":109283},{\"end\":109647,\"start\":109616},{\"end\":109938,\"start\":109890},{\"end\":110243,\"start\":110210},{\"end\":110547,\"start\":110499},{\"end\":110825,\"start\":110811},{\"end\":111034,\"start\":110958},{\"end\":111372,\"start\":111326},{\"end\":111604,\"start\":111576},{\"end\":111822,\"start\":111805},{\"end\":112086,\"start\":112058},{\"end\":112427,\"start\":112366},{\"end\":112804,\"start\":112720},{\"end\":113165,\"start\":113130},{\"end\":113365,\"start\":113325},{\"end\":113617,\"start\":113579},{\"end\":113793,\"start\":113757},{\"end\":114022,\"start\":113970},{\"end\":114306,\"start\":114259},{\"end\":114617,\"start\":114566},{\"end\":114990,\"start\":114952},{\"end\":115371,\"start\":115341},{\"end\":115700,\"start\":115672},{\"end\":115999,\"start\":115937},{\"end\":116398,\"start\":116340},{\"end\":116720,\"start\":116701},{\"end\":117069,\"start\":117020},{\"end\":117477,\"start\":117458},{\"end\":117877,\"start\":117819},{\"end\":118207,\"start\":118187},{\"end\":118568,\"start\":118519},{\"end\":118908,\"start\":118858},{\"end\":119283,\"start\":119213},{\"end\":119602,\"start\":119550},{\"end\":119905,\"start\":119867},{\"end\":120232,\"start\":120180},{\"end\":120600,\"start\":120548},{\"end\":120900,\"start\":120882},{\"end\":121076,\"start\":121036},{\"end\":121489,\"start\":121427},{\"end\":121894,\"start\":121845},{\"end\":122186,\"start\":122148},{\"end\":122476,\"start\":122428},{\"end\":122844,\"start\":122782},{\"end\":123233,\"start\":123175},{\"end\":123565,\"start\":123534},{\"end\":123951,\"start\":123912},{\"end\":124349,\"start\":124317},{\"end\":124718,\"start\":124663},{\"end\":125045,\"start\":125025},{\"end\":125355,\"start\":125293},{\"end\":125694,\"start\":125632},{\"end\":126074,\"start\":126004},{\"end\":126454,\"start\":126381},{\"end\":126776,\"start\":126748},{\"end\":127145,\"start\":127067},{\"end\":127588,\"start\":127568},{\"end\":128011,\"start\":127955},{\"end\":128306,\"start\":128225},{\"end\":128670,\"start\":128599},{\"end\":128968,\"start\":128910},{\"end\":129305,\"start\":129247},{\"end\":129677,\"start\":129618},{\"end\":130037,\"start\":129980},{\"end\":130410,\"start\":130326},{\"end\":130797,\"start\":130735},{\"end\":131201,\"start\":131139},{\"end\":131554,\"start\":131524},{\"end\":132057,\"start\":132001},{\"end\":132521,\"start\":132467},{\"end\":133037,\"start\":132979},{\"end\":133436,\"start\":133387},{\"end\":133765,\"start\":133717},{\"end\":134157,\"start\":133925},{\"end\":134859,\"start\":134738},{\"end\":135767,\"start\":135452}]"}}}, "year": 2023, "month": 12, "day": 17}