{"id": 246485799, "updated": "2023-04-21 04:40:50.071", "metadata": {"title": "Byzantine-Robust Decentralized Learning via Self-Centered Clipping", "authors": "[{\"first\":\"Lie\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Sai\",\"last\":\"Karimireddy\",\"middle\":[\"Praneeth\"]},{\"first\":\"Martin\",\"last\":\"Jaggi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In this paper, we study the challenging task of Byzantine-robust decentralized training on arbitrary communication graphs. Unlike federated learning where workers communicate through a server, workers in the decentralized environment can only talk to their neighbors, making it harder to reach consensus. We identify a novel dissensus attack in which few malicious nodes can take advantage of information bottlenecks in the topology to poison the collaboration. To address these issues, we propose a Self-Centered Clipping (SCC LIP ) algorithm for Byzantine-robust consensus and optimization, which is the \ufb01rst to provably converge to a O ( \u03b4 max \u03b6 2 /\u03b3 2 ) neighborhood of the stationary point for non-convex objectives under standard assumptions. Finally, we demonstrate the encouraging empirical performance of SCC LIP under a large number of attacks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2202-01545", "doi": null}}, "content": {"source": {"pdf_hash": "5273a3706a1cb40426a110b09eef7ad3ad684321", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2202.01545v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "73439870eb82ba62cc4e6d60d5f13af3151e7bd4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5273a3706a1cb40426a110b09eef7ad3ad684321.txt", "contents": "\nByzantine-Robust Decentralized Learning via Self-Centered Clipping\n\n\nLie He \nSai Praneeth Karimireddy \nMartin Jaggi \nByzantine-Robust Decentralized Learning via Self-Centered Clipping\n\nIn this paper, we study the challenging task of Byzantine-robust decentralized training on arbitrary communication graphs. Unlike federated learning where workers communicate through a server, workers in the decentralized environment can only talk to their neighbors, making it harder to reach consensus. We identify a novel dissensus attack in which few malicious nodes can take advantage of information bottlenecks in the topology to poison the collaboration. To address these issues, we propose a Self-Centered Clipping (SCCLIP) algorithm for Byzantine-robust consensus and optimization, which is the first to provably converge to a O(\u03b4 max \u03b6 2 /\u03b3 2 ) neighborhood of the stationary point for non-convex objectives under standard assumptions. Finally, we demonstrate the encouraging empirical performance of SC-CLIP under a large number of attacks.\n\nIntroduction\n\n\"Divide et impera\".\n\nMany machine learning tasks involve training models on decentralized data due to privacy constraints, distributed training arise as an important topic (McMahan et al., 2017;Kairouz et al., 2019). As the server-worker paradigm may suffer from single point of failure, there are growing amount of works on training in the absence of servers (Lian et al., 2017;Nedic, 2020;Koloskova et al., 2020b). There are two regimes of decentralized training: one allows direct communication between any two workers, such as in a data center; on the other hand, the available communication links are often significantly constrained by physical factors, such as in a sensor network where each device can only communicate with devices nearby, or when one has to respect a given general network topology (e.g. the internet).\n\nFailures-from malfunctioning or even malicious participants-are ubiquitous in all kinds of distributed computing. A Byzantine adversarial worker can deviate from the prescribed algorithm and send arbitrary messages, and is assumed to have knowledge of the whole system. The second assumption indicates that Byzantine workers can collude and know the information sent by all regular workers. However, they cannot compromise the messages sent between two connected regular workers. When the communication topology is constrained and there is no trusted setup, as in the decentralized case, the known secure broadcast algorithms (Pease et al., 1980;Dolev & Strong, 1983;Hirt & Raykov, 2014) are not applicable. A Byzantine worker could even send different values to several of its regular neighbors, compromising the system (Pasquini et al., 2021).\n\nIn this work, we address the Byzantine-robustness of decentralized training in a constrained topology. The main contributions of our paper are summarized as follows:\n\n\u2022 We identify a new information bottleneck for consensus, characterized in terms of the spectral gap of the communication graph (\u03b3) and number of Byzantine workers (\u03b4) and empirically demonstrate that common Byzantine-robust aggregators fail to reach consensus in training. \u2022 We propose a novel DISSENSUS attack against consensus and decentralized optimization which hones in on such information bottlenecks in the topology and amplifies exisiting disagreements among the workers. \u2022 We address the above issues by proposing a novel Byzantine-robust aggregator for decentralized learning, termed Self-Centered Clipping (SCCLIP), and empirically verify its superior performance. \u2022 We provide, for the first time, the proof of convergence to a O(\u03b4 max \u03b6 2 /\u03b3 2 ) neighborhood of a stationary point for non-convex objectives under standard assumptions and give the feasibility conditions in terms of \u03b3 and \u03b4. \u2022 Along the way, we also obtain the regular (non-Byzantine) fastest convergence rates for decentralized stochastic nonconvex optimization, improving upon (Koloskova et al., 2020b) by using local worker momentum.\n\n\nRelated work\n\nRecently there have been extensive works on Byzantineresilient distributed learning with a trustworthy server. The statistics-based robust aggregation methods cover a wide arXiv:2202.01545v1 [cs.LG] 3 Feb 2022 spectrum of works including median (Chen et al., 2017;Blanchard et al., 2017;Yin et al., 2018;Mhamdi et al., 2018;Xie et al., 2018;Yin et al., 2019), geometric median (Pillutla et al., 2019), signSGD (Bernstein et al., 2019;Li et al., 2019;yong Sohn et al., 2020), clipping (Karimireddy et al., 2021a;, and concentration filtering (Alistarh et al., 2018;Allen-Zhu et al., 2020;Data & Diggavi, 2021). If the server in addition has training dataset, then one can: 1) leverage it to score each input gradient and filter the abnormal ones (Xie et al., 2020a;Regatti et al., 2020); 2) create redundancy to achieve exact Byzantine resilience (Su & Vaidya, 2016b;Chen et al., 2018;Rajput et al., 2019;Gupta et al., 2021). The state-of-the-art attacks take advantage of the variance of good gradients and exert bias over time (Baruch et al., 2019;Xie et al., 2019). In order to provably defend against such attacks in the server case, Karimireddy et al. (2021a); El Mhamdi et al. (2021) propose to use momentum to reduce variance and Allen-Zhu et al. (2021) propose to use concentration filtering.\n\nByzantine-robustness is challenging when the training is combined with other constraints. For example, asynchronous distributed training allows Byzantine workers to send more gradients to the server and potentially compromise the whole system. This issue is addressed in a few works such as KARDAM (Damaskinos et al., 2018), ZENO++ (Xie et al., 2020b), BASGD (Yang & Li, 2021). The data heterogeneity makes it much harder to distinguish Byzantine gradients from good (regular) gradients. Popular solutions including bucketing-based methods (Karimireddy et al., 2021b;Peng & Ling, 2020), RSA (Li et al., 2019) and concentration filtering (Data & Diggavi, 2021). He et al. (2020); Burkhalter et al. (2021) address the issue of combining Byzantine-resilience with privacy. However, these works all assume there exists a central server which can communicate with all regular workers.\n\nDecentralized machine learning is widely studied (Lian et al., 2017;Koloskova et al., 2020b;Li et al., 2021;Ying et al., 2021b;Lin et al., 2021;Kong et al., 2021;Yuan et al., 2021;Kovalev et al., 2021). Many works focus on compression-techniques (Koloskova et al., 2019;2020a;Vogels et al., 2020), data heterogeneity (Tang et al., 2018;Vogels et al., 2021;Koloskova et al., 2021), and communication topology (Assran et al., 2019;Ying et al., 2021a). However, combining Byzantine robustness with decentralized machine learning is less studied. If all pairs of workers have a direct communication link, then there exists secure broadcast protocols (Pease et al., 1980;Dolev & Strong, 1983;Hirt & Raykov, 2014) which forces Byzantine workers to send same values to all workers (Gorbunov et al., 2021;El-Mhamdi et al., 2021). On the other hand, if not all workers have direct communication link (the setting we are interested in), then a Byzantine worker is more powerful since it can distort messages being passed and setup a man-in-the-middle type attack. One line of work constructs a Public-Key Infrastructure (PKI) so that the message from each worker can be authenticated using digital signatures. However, this is very inefficient requiring quadratic communication (Abraham et al., 2020). Further, it also requires every worker to have a globally unique identifier which is known to every other worker. This assumption is rendered impossible on general communication graphs, motivating our work to explicitly address the graph topology in decentralized training.\n\nMore related to the approaches we study, Su & Vaidya (2016a); Yang & Bajwa (2019b;a) use trimmed mean at each worker to aggregate models of its neighbors. This approach only works when each good worker has a honest majority among its neighbors. Guo et al. (2021) evaluate the incoming models of a good worker with its local samples and only keep those well-perform models for its local update step. However, the method only works for IID data. Peng & Ling (2020) reformulate the original problem by adding TV-regularization and propose a GossipSGD type algorithm which works for strongly convex and non-IID objectives. However, its convergence guarantees are inferior to nonparallel SGD. In this work, we address all of the above issues and are able to provably relate the communication graph (spectral gap) with the fraction of Byzantine workers.\n\n\nSetup\n\n\nThreat model\n\nConsider an undirected graph G = (V, E) where V = {1, . . . , n} denotes the set of workers and E denotes the set of edges. Let N i \u2282 V be the neighbors of node i and N i := N i \u222a {i}. In addition we assume there are no selfloops. Let V B \u2282 V be the set of Byzantine workers and the set of regular (good) workers is V R := V\\V B .\n\nLet G R be the subgraph of G induced by the regular nodes V R . If the reduced graph G R is disconnected, then there exist two regular workers who cannot reliably exchange information. In such a setting, the goal of training on the combined data of all the good workers is impossible. Hence, we make the following necessary assumption.\n\n(A1) Connectivity. G R is connected.\n\nRemark 1. The reduced graph assumption in (Su & Vaidya, 2016a) not only assumes the regular workers are connected but also assume each regular worker receives models with honest majority. (Yang & Bajwa, 2019b;a) follow the same assumption as (Su & Vaidya, 2016a) and additionally assumes that the reduced graph has size of at least b + 1. Our work and (Peng & Ling, 2020) use (A1) which is weaker.\n\nNote that Sybil attacks are an important orthogonal issue where a single Byzantine node can create innumerable \"fake nodes\" overwhelming the network (cf. recent overview by Ford (2021)). While truly decentralized solutions to this are challenging and sometimes rely on heavy machinery such as blockchains (Poupko et al., 2021) or Proof-of-Personhood (Borge et al., 2017), in practice authentication protocols such as OAuth (Hammer-Lahav et al., 2010;Hardt et al., 2012) and Terms of Service (TOS) agreements suffice.\n\n\nDecentralized optimization\n\nWe study the general distributed optimization problem\nmin x\u2208R d f (x) := 1 |V R | i\u2208VR f i (x) := E \u03bei\u223cDi F i (x; \u03be i ) (1)\non heterogeneous (non-iid) data, where f i is the local objective on worker i with data distribution D i and independent noise \u03be i . We assume that the gradients computed over these data distributions satisfy the following properties.\n\n(A2) Bounded noise and heterogeneity. Assume that for all i \u2208 V R and x \u2208 R d , we have\nE \u03be\u223cDi \u2207F i (x; \u03be i ) \u2212 \u2207f i (x) 2 \u2264 \u03c3 2(2)\nand\nE j\u223cVR \u2207f j (x) \u2212 \u2207f (x) 2 \u2264 \u03b6 2 . (3) (A3) L-smooth. For i \u2208 V R , f i (x) : R d \u2192 R is differen- tiable and there exists a constant L \u2265 0 such that for each x, y \u2208 R d : \u2207f i (x) \u2212 \u2207f i (y) \u2264 L x \u2212 y .(4)\nLet x t i \u2208 R d be the state of worker i at time t. The interworker communication can be modeled by schemes x t+1 i = n j=1 W ij x t j where W \u2208 R n\u00d7n is a non-negative mixing matrix whose positive entry W ij > 0 for j \u2208 N i . In addition, we assume (A4) . W is symmetric and doubly stochastic: \u2200i, j \u2208 [n]\nW ij = W ji , n i=1 W ij = 1, n j=1 W ij = 1.\nNote that W is determined before training and is fixed throughout the training. However, only weights between regular workers are reliable -Byzantine workers could violate W in our threat model. For each regular node i \u2208 V R , define \u03b4 i := j\u2208VB W ij to be the total weight of Byzantine edges around i. As regular nodes can trust and utilize their own updates, we have W ii > 0 and \u03b4 i \u2264 1 \u2212 W ii . Therefore, we can assume that\n(A5) . There exists \u03b4 max \u2208 [0, 1) s.t. \u03b4 i \u2264 \u03b4 max , \u2200i \u2208 V R .\nRemark 2. In the decentralized setting, the total fraction of Byzantine nodes is irrelevant. Instead, what matters is the fraction of the edge weights they control which are adjacent to regular nodes (as defined by \u03b4 i and \u03b4 max ). This is because a Byzantine worker can send different messages along each edge. Thus, a single Byzantine worker connected to all other workers with large edge weights can have a large influence on all the other workers. Similarly, a potentially very large number of Byzantine workers may overall have very little effect-if the edges they control towards good nodes have little weight. When we have a uniform fully connected graph (such as in the centralized setting), the two notions of bad nodes & edges become equivalent.\n\nWe also define W \u2208 R (n\u2212b)\u00d7(n\u2212b) for G R to simulate the adversary-free communication. Its entry i, j \u2208 V R is defined by\nW ij = W ij if i = j W ii + \u03b4 i if i = j.(5)\nRemark 3. By the construction of W , (A4) implies that W is also doubly stochastic. Further, by (A1), we can guarantee that W has a positive spectral gap which we will use next.\n(A6) Graph parameter. Assume there exists p \u2208 [0, 1] such that for all x \u2208 R n\u2212b andx = 1 x n\u2212b 1 \u2208 R n\u2212b W x \u2212x 2 2 \u2264 (1 \u2212 p) x \u2212x 2 2 .(6)\nThe parameter p relates to the spectral gap \u03b3( W ) of the graph as \u03b3 := 1 \u2212 \u221a 1 \u2212 p. In the rest of paper we thus use \u03b3 or p interchangeably.\n\n\nRobust Decentralized Consensus\n\nOne of the fundamental question in distributed computing is to achieve consensus among regular workers. First consider \u03b4 = 0 and each worker holds a vector x i , by recursively applying the following gossip averaging step on i \u2208 [n]\nx t+1 i := n j=1 W ij x t j , t = 0, 1, . . .(7)\neventually we have for all\nx \u221e i =x = 1 n n j=1 x 0 j for all i \u2208 [n]\n. Such consensus is also called average consensus. Reaching consensus in the presence of Byzantine workers is much more challenging, with a long history of study (Lamport et al., 2019;Su & Vaidya, 2016a).\n\n\nLimits due to information bottlenecks\n\nThe communication between workers can be constrained for many reasons, such as physical distances. If a subset of workers are clustered while loosely connected to the rest, then the link between two clusters become crucial for information diffusion. The existence of such bottleneck make it easier for the attackers to compromise the communication, especially when clusters are heterogeneous. From the perspective of cluster A, the two worlds are identical-it seems to be connected to one cluster with value 0 and another with value 1. Thus, it must make \u2126(1) error at least in one of the worlds. This proves that consensus is impossible in this setting. While arguments above are similar to classical lowerbounds in decentralized consensus (Fischer et al., 1986), in our case there is only 1 Byzantine node (out of 2n + 1 regular nodes) which controls only 2 edges. This impossibility result drives home the added challenge restrictive communication topology introduces.\n\n\nDissensus attack\n\nIn this section, inspired by our impossibility construction above, we introduce a novel DISSENSUS attack whose goal is to prevent worker models from reaching consensus. It does this by amplifying already present variance among the workers. In a gossip averaging step, regular worker i moves its value to the weighted average of its closed neighborhood. If there is no attacker, then consensus distance drop by (A6). Thus the goal of the DISSENSUS attackers around worker i is to send updates in the opposite direction of the regular neighbors of i. Then in the aggregation step, consensus distance drops slower or even grows which motivates the name \"dissensus\".\n\nWe can parameterize the attack through hyperparameter \u03b5 i and summarize the attack as follows\n\nDefinition A (DISSENSUS attack.). Let \u03b5 i > 0 for all i \u2208 V R . The Byzantine node j \u2208 N i \u2229 V B sends  Figure 2. Example of the DISSENSUS attack. The gray (resp. red) denotes regular (resp. Byzantine) nodes. The blue dots represents the parameters of regular nodes after gossip steps.\nx j := x i \u2212 \u03b5 i k\u2208N i \u2229V R W ik (x k \u2212xi) j\u2208N i \u2229V B Wij .(8)\nThe \u03b5 i determines the behavior of the attack. By taking smaller \u03b5, updates from Byzantine workers can be closer to the target updates i and difficult to be detected. On the other hand, a larger \u03b5 pulls the model away from the consensus. Note that this attack requires omniscience since it exploits model information from across the network. If the attackers in addition can choose which node to attack, then they can choose either to spread about the attack across the network or focus on the targeting information bottleneck, that is min-cut of the graph.\n\nProposition I. (i) For all i \u2208 V R , under the dissensus attack with \u03b5 i = 1, the gossip averaging step (7) is equivalent to no communication on i, x t+1 i = x t i . Secondly, (ii) If the graph is fully connected, gossip averaging recovers the correct consensus even in the presence of dissensus attack.\n\nProof. For the first part, by definition (7) we know that\nx t+1 i = n j=1 W ij x t j = x t i + j\u2208Ni W ij (x t j \u2212 x t i )\nBy setting \u03b5 i = 1 in the attack (8), the second term 0 and therefore x t+1 i = x t i . For part (ii), note that in a fully connected graph the gossip average is the same as standard average. Averaging all the pertubations introduced by the dissensus attack gives\n\u2212\u03b5 i,j\u2208VR W i,j (x t j \u2212 x t i ) = 0 .\nAll terms cancel and sum to 0 by symmetry. Thus, in a fully connected graph the dissensus perturbations cancel out and the gossip average returns the correct consensus.\n\nThe above proposition illustrates two interesting aspects of the attack. Firstly, dissensus works by negating the progress that would be made by gossip. The attack in (Peng & Ling, 2020) also satisfies this property (see Appendix for additional discussion). Secondly, it is a uniquely decentralized attack and has no effect in the centralized setting. Hence, its effect can be used to measure the additional difficulty posed due to the restricted communication topology.\n\n\nDecentralized defenses\n\nRobust aggregators in the federated learning setup can be applied directly on each regular node and replace gossip averaging (7). Let's take geometric median and coordinatewise trimmed mean for example.\n\n\u2022 Geometric median (GM). Pillutla et al. (2019) implements the geometric median\nGM(x 1 , . . . , x n ) := arg min v n i=1 v \u2212 x i 2 .\n\u2022 Coordinate-wise trimmed mean (TM). Yin et al.\n\n(2018); Yang & Bajwa (2019a) computes the k-th coordinate of TM as\n[TM(x 1 , . . . , x n )] k := 1 (1\u22122\u03b2)n i\u2208U k [x i ] k\nwhere U k is a subset of [n] obtained by removing the largest and smallest \u03b2-fraction of its elements.\n\nThese aggregators don't take advantage of the trusted local information and treat all vectors equally. Note that MOZI algorithm proposed in (Guo et al., 2021) does not fit in this framework and thus we discuss it in the Appendix A.2. 1 A subtle additional challenge faced by previous methods is the difficulty in incorporating weights. For ease of comparison, all our graphs are symmetric with uniform edge weights.\n\nOur method. Motivated by (Karimireddy et al., 2021a) for federated learning, we introduce a novel decentralized aggregator, termed self centered clipping (SCCLIP), which uses the local update as center and clips all received neighbor vectors. Formally, for CLIP(z, \u03c4 ) := min(1, \u03c4 / z )\u00b7z, we define\nSCCLIP i (x 1 ,. . .,x n ) := n j=1 W ij (x i +CLIP(x j \u2212x i , \u03c4 i )) .\nTheorem II. Letx := 1 |VR | i\u2208VR x i be the average iterate over unknown set of regular nodes and choose \u03c4 i to\n\u03c4 i = 1 \u03b4i j\u2208VR W ij E x i \u2212 x j 2 2 .(9)\nIf the initial consensus distance is bounded as\n1 |VR | i\u2208VR E x i \u2212x 2 \u2264 \u03c1 2 , then for all i \u2208 V R , SCCLIP outputsx i such that 1 |VR | i\u2208VR E x i \u2212x 2 \u2264 (1 \u2212 \u03b3 + c \u221a \u03b4 max ) 2 \u03c1 2\nwhere the expectation is over the random variable {x i } i\u2208VR and c > 0 is a constant.\n\nWe inspect Theorem II on corner cases for sanity checks. If regular workers already reach consensus before aggregation (\u03c1 = 0), then \u03c4 i = 0 chosen by (9) for all i \u2208 V R ensures consensus after aggregation. If there is no Byzantine worker 1 MOZI is renamed to UBAR in the latest version. (\u03b4 max = 0), then the robust aggregator must reduce the consensus distance by a factor of (1 \u2212 \u03b3) 2 = 1 \u2212 p which recovers (A6). For the complete graph (\u03b3 = 1) SCCLIP satisfies the centralized notion of (\u03b4 max , c 2 )-robust aggregator in (Karimireddy et al., 2021a, Definition C).\n\nMore importantly, if the topology is poorly connected such that the spectral gap \u03b3 < c \u221a \u03b4 max , then there is no guarantee that the consensus distance will reduce after aggregation. This is in line with our impossibility result in Section 4.1-if the connectivity is poor then the effect of Byzantine workers is significantly amplified.\n\n\nSimulations\n\nIn Figure 3 show the final consensus error by three defenses to the dissensus attack with different \u03b4 max and \u03b3. Details regarding the topology, data distribution, attack, and timeto-error curves are deferred to Appendix A.1.\n\nIn this setup, TM and MEDIAN have a large error even for small \u03b4 max and large \u03b3. On the other hand, the right subfigure shows that SCCLIP reaches consensus for \u03b4 max /\u03b3 2 smaller than a threshold, and has error increasing almost linearly with \u03b4 max /\u03b3 2 in log scale until \u03b4 max /\u03b3 2 is too large such that no meaningful progress could be made. This observation matches with our expectation in Theorem II. This phenomenon is however not observed by looking at \u03b3 \u22122 or \u03b4 max alone, validating our theoretical analysis.\n\n\nRobust Decentralized Optimization\n\nThe general decentralized training can be formulated as Algorithm 1 Self-Centered Clipping (SCCLIP)\nRequire: x 0 \u2208 R d , \u03b1, \u03b7, {\u03c4 t i }, m 0 i = g i (x 0 ) 1: for t = 0, 1, . . . do 2:\nfor all i = 1, . . . , n do 3:\nm t+1 i = (1 \u2212 \u03b1)m t i + \u03b1g i (x t i ) 4: x t+ 1 /2 i = x t i \u2212 \u03b7m t+1 i if i \u2208 V R else * 5: Exchange x t+ 1 /2 i with N i 6: x t+1 i = SCCLIP i (x t+ 1 /2 1 , . . . , x t+ 1 /2 n ; \u03c4 t+1 i ) 7:\nend for 8: end for\nx t+ 1 /2 i := x t i \u2212 \u03b7g i (x t i ) i \u2208 V R * i \u2208 V B x t+1 i := AGG i ({x t+ 1 /2 k : k \u2208 N i })\nwhere \u03b7 is the learning rate, g i (x) := \u2207F (x, \u03be i ) is the stochastic gradient, and \u03be t \u223c D i is the random batch at time t on worker i. The message x t+ 1 /2 i can be arbitrary for Byzantine nodes i \u2208 V B . Replacing AGG with (7) recovers the standard gossip SGD (Koloskova et al., 2019). In order to improve robustness of training, we can replace AGG with SCCLIP and use local worker momentum to reduce the variance \u03c3 2 (Karimireddy et al., 2021a). The full procedure is described in Algorithm 1.\n\n\nAnalysis\n\nTheorem III. Suppose Assumptions 1-6 hold and \u03b4 max \u2264 \u03b3 2 /10 \u00b7 2 10 . Define clipping radius to be\n\u03c4 t+1 i = 1 \u03b4i j\u2208VR W ij E x t+ 1 /2 i \u2212 x t+ 1 /2 j 2 2 . (10)\nBy taking \u03b7 \u2264 \u03b3 40L =: 1 d0 and \u03b1 := 3\u03b7L, we have\n1 T +1 T t=0 \u2207f (x t ) 2 2 \u2264 200 2 \u03b3 2 \u03b4 max \u03b6 2 + 2( 3 2 |VR | + 320 2 \u03b3 2 \u03b4 max ) 1 /2 3L\u03c3 2 r0 T +1 1 /2 + 2 200 \u03b3 2 \u03b6 2 1 /3 r0L T +1 2 /3 + 2 600 \u03b3 2 \u03c3 2 1 /4 r0L T +1 3 /4 + d0r0 T +1 . where r 0 := f (x 0 ) \u2212 f .\nNote that the first term O(\u03b4 max \u03b6 2 ) is also a lower bound (Karimireddy et al., 2021b, Theorem III). We summarize the comparison between our analysis and existing works for non-convex objectives in Table 1.\n\n\nRegular decentralized training.\n\nIf there is no Byzantine worker (\u03b4 max = 0), our robust convergence rate is slightly faster than that of standard gossip SGD (Koloskova et al., 2020b). The difference being out 3rd term O( \u03c3 2/3 \u03b3 2/3 \u03b5 4/3 ) is faster than their O( \u03c3 \u221a \u03b3\u03b5 3/2 ) for large \u03c3 and small \u03b5. This is because we use local momentum which reduces the effect of variance \u03c3. Thus momentum has is doubly useful.\n\nByzantine-robust federated learning. We compare our analysis on fully connected graph (\u03b3 = 1) with state of the art Byzantine-robust aggregator in federated learning (Karimireddy et al., 2021b). Both algorithms converge to \u0398(\u03b4\u03b6 2 )-neighborhood of stationary point and share same leading term. We incur additional higher order terms O( \u03b6 \u03b3\u03b5 3/2 + \u03c3 2/3 \u03b3 2/3 \u03b5 4/3 ) as a penalty of the generality of our analysis. This shows that the trusted server of federated learning can be removed without significant slowdowns.\n\nByzantine-robust decentralized SGD with fully connected topology. We limit our analysis to a special case of fully connected graph (\u03b3 = 1) and IID (\u03b6 = 0). Our rate has same leading term as ( (Peng & Ling, 2020) on strongly convex objectives is inferior to parallel SGD. In contrast, our convergence rate matches standard stochastic analysis.\n\nFinally, we point some avenues for further improvement: our results depend on the worst-case \u03b4 max . We believe it is possible to replace it with a (weighted) average of the {\u03b4 i } instead. Also, extending our protocols to time-varying topologies would greatly increase their practicality. Another limitation is the choice of clipping radius: Remark 4 (Choice of clipping radius \u03c4 ). The ideal \u03c4 t+1 i defined in (10) cannot be computed directly due to the subset of Byzantine workers is unknown. In practice, one can choose top \u03b4 max percentile of { x i \u2212 x j } j\u2208Ni to avoid additional hyperparameter tuning.\n\n\nExperiments\n\nIn this section, we empirically demonstrate successes and failures of decentralized training in the presence of Byzantine workers, and compare the performance of SCCLIP with existing robust aggregators: 1) geometric median GM (Pillutla et al., 2019); 2) coordinate-wise trimmed mean TM (Yang & Bajwa, 2019a); 3) MOZI (Guo et al., 2020). Table 1. Comparison of convergence rates for non-convex objectives. For Byzantine-robust decentralized SGD, (Peng & Ling, 2020) is excluded from comparison as they didn't prove a rate for non-convex objectives. For our clipping-based Byzantine-robust Federated Learning, the convergence rate is provided to the O(\u03b4\u03b6 2 )-neighborhood of stationary point.\n\n\nReference\n\nSetting Convergence to \u03b5-accuracy Topology and data distribution. The communication topology among regular workers is a \"dumbbell\", c.f. Figure 1, which consists of two fully connected cliques (i.e., A and B) of equal size with an edge (i.e., \"bridge\") connecting them. The Byzantine workers are added to the endpoints of the bottleneck edge to influence communication. As non-IID data distribution, we split the training dataset by labels such that workers in clique A are training on digits 0 to 4 while workers in clique B are training on digits 5 to 9. This entanglement of topology and data distribution is motivated by realistic geographic constraints such as continents with dense intra-connectivity but sparse inter-connection links e.g. through an undersea cable.\nRegular Decentralized SGD Koloskova et al. (2020b) - O( \u03c3 2 n\u03b5 2 + \u03b6 \u03b3\u03b5 3/2 + \u03c3 \u221a \u03b3\u03b5 3/2 + 1 \u03b3\u03b5 ) This work \u03b4 = 0 O( \u03c3 2 n\u03b5 2 + \u03b6 \u03b3\u03b5 3/2 + \u03c3 2/3 \u03b3 2/3 \u03b5 4/3 + 1 \u03b3\u03b5 ) Byzantine-robust Decentralized SGD Guo et al. (2021) - Gorbunov et al. (2021) b known O( \u03c3 2 n\u03b5 2 + n\u03b4\u03c3 2 m\u03b5 + 1 \u03b5 ) Gorbunov et al. (2021) b unknown O( \u03c3 2 n\u03b5 2 + n 2 \u03b4\u03c3 2 m\u03b5 + 1 \u03b5 ) This work \u03b3 = 1, \u03b6 = 0 O( \u03c3 2 n\u03b5 2 + \u03b4\u03c3 2 \u03b5 2 + 1 \u03b5 ) Byzantine-robust Federated Learning Karimireddy et al. (2021b) - O( \u03c3 2 \u03b5 2 (\u03b4 + 1 n ) + 1 \u03b5 ) This work \u03b3 = 1 O( \u03c3 2 \u03b5 2 (\u03b4 + 1 n ) + \u03b6 \u03b5 3/2 + \u03c3 2/3 \u03b5 4/3 + 1 \u03b5 )\n\nDecentralized defenses without attackers\n\nThe first experiment investigates the impact of topology and data distribution over aggregators. In Figure 4 we show the accuracy of the averaged model in clique A. Unlike the IID data setup where all aggregators perform similarly well (i.e. 1st subplot), the non-IID setup alone (i.e. 2nd subplot) leads to either slower convergence (gossip averaging, SCCLIP) or stuck at 50% accuracy (GM, TM, MOZI), even without Byzantine nodes. 2 The code is available at this url.\n\nThe reason for the stark contrast between aggregator performances is that when both cliques have the same data distribution, workers in clique A do not rely on the \"bridge\" to access the full spectrum of data and thus the \"bridge\" is not the information bottleneck; when clique A and B have distinct data distributions, their sparse connection becomes the information bottleneck, slowing down gossip averaging. SCCLIP attains similar performance as gossip averaging, matching this information upper bound.\n\nOn the other hand, the low 50% accuracy attained by GM, TM, and MOZI suggest that these aggregators completely ignored the updates from clique B. This is due to receiving a many similar updates from clique A and few updates from clique B via the sparse connection, making these aggregators falsely treat clique B updates as outliers (suspected Byzantine), thus failing to incorporate any information from B.\n\nTo address these issues, we locally employ the bucketing technique of (Karimireddy et al., 2021b) for heterogeneous data aggregation in the 3rd subplot. Plots 4 and 5 demonstrate the impact of one additional edge between the cliques to alleviate the communication bottleneck.\n\n\u2022 The bucketing technique randomly inputs received vectors into buckets of equal size, averages the vectors in each bucket, and finally feeds the averaged vectors to the aggregator. While bucketing helps TM to overcome 50% accuracy, TM is still behind SCCLIP. GM only improves by 1% while MOZI remains at almost the same accuracy. \u2022 Adding one more random edge between two cliques improves the spectral gap \u03b3 from 0.0286 to 0.0154. The SCCLIP and gossip averaging converge faster as the theory predicts. However, TM, GM, and MOZI still stuck at 50% for same heterogeneity reason. \u2022 Combining both bucketing and an additional random edge helps all aggregators achieve higher than 50% accuracy.  \n\n\nEffect of the number of Byzantine workers\n\nWe investigate the effect of Byzantine workers on a dumbbell topology with fixed graph parameter \u03b3 and heterogeneity \u03b6 2 , but different \u03b4 max fraction of Byzantine edges under the dissensus attack. The resulting Figure 5 shows that with increasing \u03b4 max the model quality drops significantly. This is in line with our proven robust convergence rate in terms of dependency in \u03b4 max . It is interesting to see that for large \u03b4 max , the model averaged over all workers performs even worse than those averaged within cliques. It means the models in two cliques are essentially disconnected and are converging to different local minima or stationary points of a non-convex landscape. We defer details to Appendix A.2.2.\n\n\nDefense without honest majority\n\nIn the decentralized environment, the common honest majority assumption in the federated learning setup can be strengthen to honest majority everywhere, meaning all regular workers have an honest majority of neighbors (Su & Vaidya, 2016b; Yang & Bajwa, 2019b;a). Considering a ring of 5 regular workers with IID data, and adding 2 Byzantine workers to each node will still satisfy the honest majority assumption everywhere. Now adding one more Byzantine worker to a node will break the assumption.  Figure 6 shows that while TM and GM can sometimes counter the attack under the honest majority assumption, adding one more Byzantine worker always corrupts the entire training. MOZI and SCCLIP defend attacks successfully even beyond the assumption, because they leverage the fact that local updates are trustworthy. This suggest that existing statistics-based aggregators which take no advantage of local information are vulnerable under this realistic decentralized threat model.\n\n\nDiscussion\n\nIn this paper, we demonstrated that data heterogeneity and information bottlenecks from the graph topology make many existing aggregators fail to converge even in absence of Byzantine workers. We then proposed decentralized Self-Centered Clipping (SCCLIP) to address these issues and proved its convergence under standard assumptions. In addition, we found that using bucketing aggregation for heterogeneous data and adding few random edges can empirically help existing aggregators to improve robust convergence.\n\nA main takeaway from our work is that ill-connected communication topologies can vastly magnify the effect of bad actors. Given that decentralized consensus has been proposed as a backbone for digital democracy (Bulteau et al., 2021), and that decentralized learning is touted to be an alternative to current centralized training paradigms, our findings are significant. A simple strategy we recommend (along with using SCCLIP) is adding random edges in order to improve the connectivity and robustify the network.\n\nAcknowledgements. This project was supported by SNSF grant 200020 200342. We are indebted to the master thesis of Cappelletti (2021) for initiating this project and analyzing the Byzantine-free setting. We also thank Anastasiia Koloskova and L\u00ea Nguy\u00ean Hoang for fruitful discussions on optimization and authentication. Allen-Zhu, Z., Ebrahimian, F., Li, J., and Alistarh, D. Byzantineresilient non-convex stochastic gradient descent. arXiv preprint arXiv:2012.14368, 2020.\n\nAllen-Zhu, Z., Ebrahimianghazani, F., Li, J., and Alistarh, D.   \n\n\nAppendices A. Experiments\n\nWe list the setups and resdults of experiments for consensus in Appendix A.1 and optimization in Appendix A.2.\n\n\nA.1. Attacks against consensus\n\nIn this section, we provide detailed setups for Figure 3. The Figure 7 demonstrates the topology for the experiment. The 4 regular workers are connected with two of them holding value 0 and the others holding 200. Then the average consensus is 100 with initial mean square error equals 10000. Two Byzantine workers are connected to two regular workers in the middle. We can tune the weights of each edge to change the mixing matrix and \u03b3. Then we can decide the weight \u03b4 on the Byzantine edge. The set of \u03b3 and \u03b4 used In Figure 8, we show the iteration-to-error curves for all possible combinations of \u03b3 and \u03b4. In addition, we provide a version of TM and MEDIAN which takes the mixing weight into account. As we can see, the naive TM, MEDIAN, and MEDIAN* cannot bring workers closer because of the data distribution we constructed. The TM* is performing better than the other baselines but worse than SCCLIP especially on the challenging cases where \u03b3 is small and \u03b4 is large. For SCCLIP, it matches with our intuition that for a fixed \u03b3 the convergences is worse with increasing \u03b4 while for a fixed \u03b4 the convergence is worse with decreasing \u03b3.\n\u2022 p = 1 \u2212 (1 \u2212 \u03b3) 2 \u2208 [\n\nA.2. Challenges of decentralized optimization\n\nThe default experiment setup is listed in Table 2 and per-experiment setup in Table 3. The default hyperparameters of the aggregators are summarized as follows Aggregators Hyperparameters\nGM T = 8 TM \u03b2 = \u03b4 max SCCLIP \u03c4 = 1 MOZI \u03b1 = 0.5\nwhere \u03b1 is the model averaging hyperparameter\n\nWe summarize the running environment of this paper as in Table 4. Mixing matrix. Let d max be the maximum degree of nodes in a graph. We use the following naive construction\nW ij = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 dmax+1 j \u2208 N i , 1 \u2212 |Ni| dmax+1 j = i, 0\nOtherwise.\n\nNote that this construction of mixing matrix usually have worse spectral gap than the common Metropolis-Hastings weight (Hastings, 1970). However, the Metropolis-Hastings give different weight to each edge based on the degree of each node which is not justified in the Byzantine-resilient sense. We leave the study of better mixing matrix to future work.\n\n\nMozi. Guo et al. (2021) applies two screening steps on worker\ni \u2208 V R N s i := arg min N * \u2282Ni |N * |=\u03b4i|Ni| j\u2208N * x i \u2212 x j , N r i :=N s i \u2229 {j \u2208 [n] : (x j , \u03be i ) \u2264 (x i , \u03be i )}\nwhere \u03be i \u223c D i is a random sample. If N r i = \u2205, then redefine N r i := {arg min j (x j , \u03be i )}. Then they update the model with\nx t+1 i := \u03b1x t i + 1\u2212\u03b1 |N r i | j\u2208N r i x t j \u2212 \u03b7\u2207F i (x t i ; \u03be t i )\nwhere \u03b1 \u2208 [0, 1] is an hyperparameter.  Table 3. Setups for each optimization experiment. The learning rate \u03b7 = 0.01 throughout all experiments and momentum \u03b1 = 0.9.\n\n\nTopology n b #Iters Results\n\nExp 1   In this experiment we choose n \u2212 b = 11 and b = 0, 1, 2, 3. Therefore their W and p remain the same for all these b. Then we can easily investigate the relation between \u03b4 max \u2208 [0, b b+3 ] and p by varying b. \n\n\nB. Attacks in the decentralized environment\n\nIn this section, we describe attacks in the centralized environment and further explain how to transform them into attacks in the decentralized environment. \nz = max z \u03c6(z) < n \u2212 b \u2212 s n \u2212 b(12)\nwhere s = n 2 + 1 \u2212 b and \u03c6 is the cumulative standard normal function.\n\nInner product manipulation attack. The inner product manipulation (IPM) attack is proposed in (Xie et al., 2019) which lets all attackers send same corrupted gradient u based on the good gradients\nu j = \u2212\u03b5AVG({v i : i \u2208 V R }) \u2200 j \u2208 V B .\nIf \u03b5 is small enough, then u j can be detected as good by the defense, thus circumventing the defense. In contrast, there are 3 main differences when we conduct attacks in the decentralized environment:\n\n1. Byzantine workers may not connected to the same good worker.\n\n\nThe model vectors are transmitted instead of gradients.\n\n3. The AVG should be replaced by its equivalent gossip form.\n\nTherefore, each Byzantine worker should compute their own corrupted model updates. More specifically, for j \u2208 V B and j \u2208 V R be the only neighbor of j, then\nu j = v j \u2212 \u03b5 i\u2208VR \u2229N i W j i (v i \u2212 v j ).\nThen the gossip average at node j gives\ni\u2208VR \u2229N i W j i v i + j\u2208VB \u2229N i W j j u j = i\u2208VR \u2229N i W j i v i + \uf8eb \uf8ed v j \u2212 \u03b5 i\u2208VR \u2229N i W j i (v i \u2212 v j ) \uf8f6 \uf8f8 j\u2208VB \u2229N i W j j =v j + \uf8eb \uf8ed i\u2208VR \u2229N i W j i (v i \u2212 v j ) \uf8f6 \uf8f8 \uf8eb \uf8ed 1 \u2212 \u03b5 j\u2208VB \u2229N i W j j \uf8f6 \uf8f8 Note that v j + GOSSIPAVG j ({v i \u2212 v j : i \u2208 V R \u2229 N i }).\nRelation with zero-sum attack and dissensus. Peng & Ling (2020) propose the \"zero-sum\" attack which achieves similar effects as Proposition I part (i). This attack is defined for j \u2208 V B\nx j := \u2212 k\u2208N i \u2229V R x k |Ni\u2229VB | .\nThe key difference between zero-sum attack and our proposed attack is three-folded. First, zero-sum attack ensures j\u2208Ni x j = 0 which means the Byzantine models have to be far away from x t i and therefore easy to detect. This attack pull the aggregated model to 0. On the other hand, our attack ensures 1 j\u2208Ni W ij j\u2208Ni W ij x t j = x t i and the Byzantine updates can be very close to x t i and it is more difficult to be detected. Second, our proposed attack considers the gossip averaging which is prevalent in decentralized training (Koloskova et al., 2020b) while the zero-sum attack only targets simple average. Third, our attack has an additional parameter \u03b5 controlling the strength of the attack with \u03b5 > 1 further compromise the model quality while zero-sum attack is fixed to training alone.\n\n\nC. Analysis\n\nLet us restate the core equations in Algorithm 1 at time t on worker i\nm t+1 i = (1 \u2212 \u03b1)m t i + \u03b1g i (x t i ) (13) x t+ 1 /2 i = x t i \u2212 \u03b7m t+1 i (14) z t+1 j\u2192i = x t+ 1 /2 i + CLIP(x t+ 1 /2 j \u2212 x t+ 1 /2 i , \u03c4 t i ) (15) x t+1 i = n j=1 W ij z t+1 j\u2192i(16)\nIn addition, we define the following virtual iterates on the set of good nodes V R\n\u2022x t = 1\n|VR | i\u2208VR x t i the average of good iterates.\n\n\u2022m t = 1 |VR | i\u2208VR m t i the average of momentum iterates.\n\nIn this section, we show that the convergence behavior of the virtual iteratesx t . The structure of this section is as follows:\n\n\u2022 In Appendix C.1, we give common quantities, simplified notations and list common equalities/inequalities used in the proof.\n\n\u2022 In Appendix C.2, we provide all auxiliary lemmas necessary for the proof. Among these lemmas, Lemma 7 is the key sufficient descent lemma.\n\n\u2022 In Appendix C.3, we provide the proof of the main theorem.\n\n\nC.1. Definitions, and inequalities\n\nNotations for the proof. We use the following variables to simplify the notation \u2022 Sub-optimality:\nr t := f (x t ) \u2212 f \u2022 Consensus distance: \u039e t := 1 |V R | i\u2208VR x t i \u2212x t 2 2\n\u2022 The distance between the idea gradient and actual averaged momentum\ne t+1 1 := E \u2207f (x t ) \u2212m t+1 2 2\n\u2022 Similar distance between the idea gradient and individual momentum e t+1\n1 := 1 |V R | i\u2208VR E \u2207f (x t ) \u2212 m t+1 i 2 2\n\u2022 Similar distance between the idea gradient and individual momentum e t+1\n1 := 1 |V R | i\u2208VR E j\u2208VR W ij (\u2207f j (x t ) \u2212 m t+1 j ) 2 2\n\u2022 Similar we have e t+1\nI = 1 |V R | i\u2208VR E m t+1 i \u2212 \u2207f i (x t ) 2 2 ,\n\u2022 Let e t+1 2 be the averaged squared error introduced by clipping and Byzantine workers\ne t+1 2 := 1 |V R | i\u2208VR E j\u2208VR W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 j ) + j\u2208VB W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 i ) 2 2 .\nLemma 5 (Common equalities and inequalities). We use the following equalities and inequalities\n\u2022 The cosine theorem: \u2200 x, y \u2208 R d x, y = \u2212 1 2 x \u2212 y 2 2 + 1 2 x 2 2 + 1 2 y 2 2(17)\n\u2022 Young's inequality: For \u03b5 > 0 and x, y \u2208 R d\nx + y 2 2 \u2264 (1 + \u03b5) x 2 2 + (1 + \u03b5 \u22121 ) y 2 2 (18) \u2022 If f is convex, then for \u03b1 \u2208 [0, 1] and x, y \u2208 R d f (\u03b1x + (1 \u2212 \u03b1)y) \u2264 \u03b1f (x) + (1 \u2212 \u03b1)f (y)(19)\n\u2022 Cauchy-Schwarz inequality x, y \u2264 x 2 y 2 (20)\n\u2022 Let {x i : i \u2208 [m]} be independent random variables and E x i = 0 and E x i 2 = \u03c3 2 then E 1 m m i=1 x i 2 2 = \u03c3 2 m(21)\n\nC.2. Lemmas\n\nThe following lemma establish the update rule forx t .\n\nLemma 6. Assume (A4). Let \u2206 t+1 be the error incurred by clipping and V B\n\u2206 t+1 := 1 |V R | i\u2208VR \uf8eb \uf8ed j\u2208VR W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 j ) + j\u2208VB W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 i ) \uf8f6 \uf8f8 .(22)\nThen the virtual iterate updatesx\nt+1 =x t \u2212 \u03b7m t+1 + \u2206 t+1 .(23)\nProof. Expandx t+1 with the definition of x t+1 i in (16) yields\nx t+1 = 1 |V R | i\u2208VR x t+1 i = 1 |V R | i\u2208VR \uf8eb \uf8ed j\u2208VR W ij z t+1 j\u2192i + j\u2208VB W ij z t+1 j\u2192i \uf8f6 \uf8f8 = 1 |V R | i\u2208VR \uf8eb \uf8ed j\u2208VR W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 j ) + j\u2208VR W ij x t+ 1 /2 j \uf8f6 \uf8f8 + 1 |V R | i\u2208VR \uf8eb \uf8ed j\u2208VB W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 i ) + j\u2208VB W ij x t+ 1 /2 i \uf8f6 \uf8f8 .\nReorganize the terms to form \u2206 t+1\nx t+1 = 1 |V R | i\u2208VR \uf8eb \uf8ed j\u2208VR W ij x t+ 1 /2 j + j\u2208VB W ij x t+ 1 /2 i \uf8f6 \uf8f8 + \u2206 t+1 = 1 |V R | j\u2208VR (1 \u2212 \u03b4 j )x t+ 1 /2 j + 1 |V R | i\u2208VR \u03b4 i x t+ 1 /2 i + \u2206 t+1 = 1 |V R | i\u2208VR x t+ 1 /2 i + \u2206 t+1 = 1 |V R | i\u2208VR (x t i \u2212 \u03b7m t+1 i ) + \u2206 t+1 =x t i \u2212 \u03b7m t+1 + \u2206 t+1 .\nNote that the \u2206 t+1 can be written as the follows\n\u2206 t+1 = 1 |V R | i\u2208VR \uf8eb \uf8ed x t+1 i \u2212 j\u2208VRW ij x t+ 1 /2 j \uf8f6 \uf8f8 =x t+1 \u2212 1 |V R | i\u2208VR x t+ 1 /2 i .\nwhere measures the error introduced tox t+1 considering the impact of Byzantine workers and clipping. Therefore when V B = \u2205 and \u03c4 is sufficiently large, \u2206 t+1 = 0 andx t+1 converge at the same rate as the centralized SGD with momentum.\n\nRecall that e t+1\n1 := E \u2207f (x t ) \u2212m t+1 2 2 .\nThe key descent lemma is stated as follow Lemma 7 (Sufficient decrease). Assume (A3) and \u03b7 \u2264 1 2L , then\nE f (x t+1 ) \u2264f (x t ) \u2212 \u03b7 2 \u2207f (x t ) 2 2 \u2212 \u03b7 4 E m t+1 \u2212 1 \u03b7 \u2206 t+1 2 2 + \u03b7e t+1 1 + 1 \u03b7 e t+1 2 .\nProof. Use smoothness (A3) and expand it with (23)\nf (x t+1 ) \u2264f (x t ) \u2212 \u2207f (x t ), \u03b7m t+1 \u2212 \u2206 t+1 + L 2 \u03b7m t+1 \u2212 \u2206 t+1 2 2\nApply cosine theorem (17) to the inner product \u03b7 \u2207f (x t ),m t+1 \u2212 1 \u03b7 \u2206 t+1 yields\nE f (x t+1 ) \u2264f (x t ) \u2212 \u03b7 2 \u2207f (x t ) 2 2 \u2212 \u03b7 \u2212 L\u03b7 2 2 E m t+1 \u2212 1 \u03b7 \u2206 t+1 2 2 + \u03b7 2 E \u2207f (x t ) \u2212m t+1 + 1 \u03b7 \u2206 t+1 2 2 .\nIf step size \u03b7 \u2264 1 2L , then \u2212 \u03b7\u2212L\u03b7 2 2 \u2264 \u2212 \u03b7 4 . Applying inequality (20) to the last term\n\u03b7 2 E \u2207f (x t ) \u2212m t+1 + 1 \u03b7 \u2206 t+1 2 2 \u2264 \u03b7 E \u2207f (x t ) \u2212m t+1 2 2 + 1 \u03b7 E \u2206 t+1 2 2 .\nSince e t+1 1 := E \u2207f (x t ) \u2212m t+1 2 2 and E \u2206 t+1 2 2 \u2264 e t+1 2 , then we have\nE f (x t+1 ) \u2264f (x t ) \u2212 \u03b7 2 \u2207f (x t ) 2 2 \u2212 \u03b7 4 E m t+1 \u2212 1 \u03b7 \u2206 t+1 2 2 + \u03b7e t+1 1 + 1 \u03b7 e t+1 2 .\nIn the next lemma, we establish the recursion for the distance between momentums and gradients Lemma 8. Assume (A2) to (A4), For any doubly stochastic mixing matrix A \u2208 R n\u00d7n\ne t+1 A = 1 |V R | i\u2208VR E j\u2208VR A ij (m t+1 j \u2212 \u2207f j (x t )) 2 2 ,\nthen we have the following recursion\ne t+1 A \u2264 (1 \u2212 \u03b1)e t A + \u03b1 2 \u03c3 2 |V R | A 2 F,VR + 2\u03b1L 2 \u039e t + 2L 2 \u03b7 2 \u03b1 m t \u2212 1 \u03b7 \u2206 t 2 2 .(24)\nwhere we define A 2 F,VR := i\u2208VR j\u2208VR A 2 ij Therefore,\n\n\u2022 If A ij = 1 |VR | for all i, j \u2208 V R , then e t+1 A = e t+1 1 and A 2 F,VR = 1.\n\n\u2022 If A = W , then e t+1 A =\u0113 t+1 1 and A 2 F,VR = i\u2208VR j\u2208VR W 2 ij \u2264 |V R |.\n\n\u2022 If A = I, then A 2 F,VR = |V R |. In addition,\u1ebd \n= 1 |V R | i\u2208VR E j\u2208VR A ij ((1 \u2212 \u03b1)m t j + \u03b1g j (x t j ) \u2212 \u2207f j (x t )) 2 2 = 1 |V R | i\u2208VR E j\u2208VR A ij ((1\u2212\u03b1)m t j +\u03b1(g j (x t j ) \u00b1 \u2207f j (x t j ))\u2212\u2207f j (x t )) 2 2\nExtract the stochastic term g j (x t j ) \u2212 \u2207f j (x t j ) inside the norm and use that E g j (x t j ) = \u2207f j (x t j ),\ne t+1 A = 1 |V R | i\u2208VR j\u2208VR A ij ((1\u2212\u03b1)m t j +\u03b1\u2207f j (x t j )\u2212\u2207f j (x t )) 2 2 + 1 |V R | i\u2208VR E j\u2208VR A ij \u03b1(g j (x t j ) \u2212 \u2207f j (x t j )) 2 2 \u2264 1 |V R | i\u2208VR j\u2208VR A ij ((1\u2212\u03b1)m t j +\u03b1\u2207f j (x t j )\u2212\u2207f j (x t )) 2 2 + \u03b1 2 |V R | i\u2208VR j\u2208VR A 2 ij E g j (x t j ) \u2212 \u2207f j (x t j ) 2 2 .\nThen we can use (A2) for the last term to get\ne t+1 A = 1 |V R | i\u2208VR j\u2208VR A ij ((1\u2212\u03b1)m t j +\u03b1\u2207f j (x t j )\u2212\u2207f j (x t )) 2 2 + \u03b1 2 \u03c3 2 |V R | A 2 F,VR .\nThen we insert \u00b1(1 \u2212 \u03b1)\u2207f j (x t\u22121 ) inside the first norm and expand using (19)\ne t+1 A \u2264 1 \u2212 \u03b1 |V R | i\u2208VR j\u2208VR A ij (m t j \u2212 \u2207f j (x t\u22121 )) 2 2 + \u03b1 2 \u03c3 2 |V R | A 2 F,VR + \u03b1 |V R | i\u2208VR j\u2208VR A ij (\u2207f j (x t j ) \u2212 \u2207f j (x t ) + 1 \u2212 \u03b1 \u03b1 (\u2207f j (x t\u22121 ) \u2212 \u2207f j (x t )) 2 2 .\nNote that the first term is e t A and by the convexity of \u00b7 for the last term we have\ne t+1 A \u2264(1 \u2212 \u03b1)e t A + \u03b1 2 \u03c3 2 |V R | A 2 F,VR + \u03b1 |V R | j\u2208VR \u2207f j (x t j ) \u2212 \u2207f j (x t ) + 1 \u2212 \u03b1 \u03b1 (\u2207f j (x t\u22121 ) \u2212 \u2207f j (x t )) 2 2 .\nThen we can further expand the last term\ne t+1 A \u2264(1 \u2212 \u03b1)e t A + \u03b1 2 \u03c3 2 |V R | A 2 F,VR + 2\u03b1 |V R | j\u2208VR \u2207f j (x t j ) \u2212 \u2207f j (x t ) 2 2 + 2(1 \u2212 \u03b1) 2 \u03b1|V R | j\u2208VR \u2207f j (x t\u22121 ) \u2212 \u2207f j (x t ) 2 2 .\nThen we can apply smoothness (A3) and use (1 \u2212 \u03b1) 2 \u2264 1\ne t+1 A \u2264(1 \u2212 \u03b1)e t A + \u03b1 2 \u03c3 2 |V R | A 2 F,VR + 2\u03b1L 2 \u039e t + 2L 2 \u03b7 2 \u03b1 m t \u2212 1 \u03b7 \u2206 t 2 2 .\nBesides, consider\u1ebd t+1\n1 e t+1 1 = 1 |V R | i\u2208VR E m t+1 i \u2212 \u2207f (x t ) 2 2 = 1 |V R | i\u2208VR E m t+1 i \u00b1 \u2207f i (x t ) \u2212 \u2207f (x t ) 2 2 \u22642 1 |V R | i\u2208VR E m t+1 i \u2212 \u2207f i (x t ) 2 2 + 2 1 |V R | i\u2208VR \u2207f i (x t ) \u2212 \u2207f (x t ) 2 2 =2e t+1 I + 2\u03b6 2 .\nAs we know that \u2206 t+1 2 2 \u2264 e t+1 2 , then we need to finally bound e t+1 2 Lemma 9 (Bound on e t+1 2 ). For \u03b4 max := max i\u2208VR \u03b4 i , if\n\u03c4 t+1 i = 1 \u03b4 i j\u2208VR W ij E x t+ 1 /2 i \u2212 x t+ 1 /2 j 2 2 , then we have e t+1 2 \u2264 32\u03b4 max (2\u03b7 2 (e t+1 I + \u03b6 2 ) + \u039e t ).\nProof. Use Young's inequality (18) to bound e t+1 2 by two parts\ne t+1 2 = 1 |V R | i\u2208VR E j\u2208VR W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 j ) + j\u2208VB W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 i ) 2 2 \u2264 2 |V R | i\u2208VR E j\u2208VR W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 j ) 2 2 =:A1 + 2 |V R | i\u2208VR E j\u2208VB W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 i ) 2 2 =:A2 .\nLook at the first term use triangular inequality of \u00b7 and the definition of \u03c4 t+1\ni A 1 \u2264 2 |V R | i\u2208VR \uf8eb \uf8ed j\u2208VR W ij E z t+1 j\u2192i \u2212 x t+ 1 /2 j 2 \uf8f6 \uf8f8 2 \u2264 2 |V R | i\u2208VR \uf8eb \uf8ed 1 \u03c4 t+1 i j\u2208VR W ij E x t+ 1 /2 i \u2212 x t+ 1 /2 j 2 2 \uf8f6 \uf8f8 2 .\nOn the other hand,\nA 2 \u2264 2 |V R | i\u2208VR \uf8eb \uf8ed j\u2208VB W ij E z t+1 j\u2192i \u2212 x t+ 1 /2 i 2 \uf8f6 \uf8f8 2 \u2264 2 |V R | i\u2208VR \uf8eb \uf8ed j\u2208VB W ij (\u03c4 t+1 i ) \uf8f6 \uf8f8 2 = 2 |V R | i\u2208VR \u03b4 2 i (\u03c4 t+1 i ) 2 .\nThen minimizing the RHS of e t+1 2 by tuning radius for clipping\n\u03c4 t+1 i = 1 \u03b4 i j\u2208VR W ij E x t+ 1 /2 i \u2212 x t+ 1 /2 j 2 2\nThen we come to the following bound\ne t+1 2 \u2264 4 |V R | i\u2208VR \u03b4 i j\u2208VR W ij E x t+ 1 /2 i \u2212 x t+ 1 /2 j 2 2 .\nThen we expand the norm as follows E x\nt+ 1 /2 i \u2212 x t+ 1 /2 j 2 2 = E x t i \u2212 \u03b7m t+1 i \u2212 x t j + \u03b7m t+1 j 2 2 = E x t i \u00b1x t \u2212 x t j + \u03b7m t+1 j \u00b1 \u03b7\u2207f (x t ) \u2212 \u03b7m t+1 i 2 2 \u22644\u03b7 2 E m t+1 i \u2212 \u2207f (x t ) 2 2 + 4\u03b7 2 E m t+1 j \u2212 \u2207f (x t ) 2 2 + 4 x t i \u2212x t 2 2 + 4 x t j \u2212x t 2 2 Use the fact that j\u2208VR W ij = 1 \u2212 \u03b4 i we have e t+1 2 \u2264 16\u03b7 2 |V R | i\u2208VR \u03b4 i (1 \u2212 \u03b4 i ) E m t+1 i \u2212 \u2207f (x t ) 2 2 + 16\u03b7 2 |V R | j\u2208VR i\u2208VR \u03b4 i W ij E m t+1 j \u2212 \u2207f (x t ) 2 2 + 16 |V R | i\u2208VR \u03b4 i (1 \u2212 \u03b4 i ) x t i \u2212x t 2 2 + 16 |V R | j\u2208VR i\u2208VR \u03b4 i W ij x t j \u2212x t 2 2\nUse the fact that \u03b4 i \u2264 \u03b4 max and 1 \u2212 \u03b4 i \u2264 1 for all i \u2208 V R , e t+1 2 \u2264 32\u03b4 max (2\u03b7 2 (e t+1 I + \u03b6 2 ) + \u039e t ).\n\nTheorem II. Letx := 1 |VR | i\u2208VR x i be the average iterate over unknown set of regular nodes and choose \u03c4 i to\n\u03c4 i = 1 \u03b4i j\u2208VR W ij E x i \u2212 x j 2 2 .(9)\nIf the initial consensus distance is bounded as\n1 |VR | i\u2208VR E x i \u2212x 2 \u2264 \u03c1 2 ,\nthen for all i \u2208 V R , SCCLIP outputsx i such that\n1 |VR | i\u2208VR E x i \u2212x 2 \u2264 (1 \u2212 \u03b3 + c \u221a \u03b4 max ) 2 \u03c1 2\nwhere the expectation is over the random variable {x i } i\u2208VR and c > 0 is a constant.\n\nProof. We can consider the 1-step consensus problem as 1-step of optimization problem with \u03c1 2 = \u039e t and \u03b7 = 0. Then we look for the upper bound of 1 |VR | i\u2208VR E x t+1 i \u2212x t 2 2 in terms of \u03c1 2 , p, and \u03b4 max .\n1 |V R | i\u2208VR E x t+1 i \u2212x t 2 2 = 1 |V R | i\u2208VR E n j=1 W ij z t+1 j\u2192i \u2212x t 2 2 = 1 |V R | i\u2208VR E ( j\u2208VR W ij x t j \u2212x t ) + ( n j=1 W ij z t+1 j\u2192i \u2212 j\u2208VR W ij x t j ) 2 2 .\nApply (18) with \u03b5 > 0 and use the expected improvement (A6)\n1 |V R | i\u2208VR E x t+1 i \u2212x t 2 2 \u2264 1 + \u03b5 |V R | i\u2208VR j\u2208VR W ij x t j \u2212x t 2 2 + 1 + 1 \u03b5 |V R | i\u2208VR E n j=1 W ij z t+1 j\u2192i \u2212 j\u2208VR W ij x t j 2 2 \u2264 (1 + \u03b5)(1 \u2212 p) |V R | i\u2208VR x t i \u2212x t 2 2 + 1 + 1 \u03b5 |V R | i\u2208VR E n j=1 W ij z t+1 j\u2192i \u2212 j\u2208VR W ij x t j 2 2 \u2264(1 + \u03b5)(1 \u2212 p)\u039e t + 1 + 1 \u03b5 |V R | i\u2208VR E n j=1 W ij z t+1 j\u2192i \u2212 j\u2208VR W ij x t j 2 2\nReplace x t j = x t+ 1 /2 j + \u03b7m t+1 j using (14), then apply (20) and \u03b7 = 0\n1 |V R | i\u2208VR E x t+1 i \u2212x t 2 2 \u2264 (1 + \u03b5)(1 \u2212 p)\u039e t + 1 + 1 \u03b5 |V R | i\u2208VR E n j=1 W ij z t+1 j\u2192i \u2212 j\u2208VR W ij x t+ 1 /2 j 2 2 .\nRecall the definition of e t+1 2 e t+1\n2 := 1 |V R | i\u2208VR E n j=1 W ij z t+1 j\u2192i \u2212 j\u2208VR W ij x t+ 1 /2 j 2 2 .\nThen use Lemma 8 with the case A = W and apply Lemma 9 with \u03b7 = 0\n1 |V R | i\u2208VR E x t+1 i \u2212x t 2 2 \u2264 (1 + \u03b5)(1 \u2212 p)\u039e t + (1 + 1 \u03b5 )e t+1 2 \u2264 (1 + \u03b5)(1 \u2212 p)\u039e t + (1 + 1 \u03b5 )32\u03b4 max \u039e t .\nLet's minimize the right hand side of the above inequality by taking \u03b5 such that \u03b5(1 \u2212 p) = 32\u03b4max \u03b5 which leads to \u03b5 = 32\u03b4max 1\u2212p , then the above inequality becomes 1 |V R | i\u2208VR E x t+1 i \u2212x t 2 2 \u2264 (1 \u2212 p + 32\u03b4 max + 2 32\u03b4 max (1 \u2212 p))\u039e t = ( 1 \u2212 p + 32\u03b4 max ) 2 \u039e t .\n\nThe consensus distance to the average consensus is only guaranteed to reduce if \u221a 1 \u2212 p + \u221a 32\u03b4 max < 1 which is\n\u03b4 max < 1 32 (1 \u2212 1 \u2212 p) 2 .\nFinally, we complete the proof by simplifying the notation to spectral gap \u03b3 := 1 \u2212 \u221a 1 \u2212 p.\n\nRecall that\ne t+1 2 := 1 |V R | i\u2208VR j\u2208VR W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 j ) + j\u2208VB W ij (z t+1 j\u2192i \u2212 x t+ 1 /2 i ) 2 2 .(25)\nNext we consider the bound on consensus distance \u039e t .\n\nLemma 10 (Bound consensus distance \u039e t ). Assume (A6), then \u039e t has the following iteration \u039e t+1 \u2264 (1 + \u03b5)(1 \u2212 p)\u039e t + 5(1 + 1 \u03b5 ) e t+1 2 + \u03b7 2\u0113t+1 1 + \u03b7 2 \u03b6 2 + \u03b7 2 \u2207f (x t ) 2 2 + \u03b7 2 E m t+1 \u2212 1 \u03b7 \u2206 t+1 2 2 .\n\nwhere \u03b5 > 0 is determined later.\n\nProof. Expand the consensus distance at time t + 1\n\u039e t+1 = 1 |V R | i\u2208VR E x t+1 i \u2212x t+1 2 2 = 1 |V R | i\u2208VR E n j=1 W ij z t+1 j\u2192i \u2212x t+1 2 1 1 + T T t=0 e t+1\nI , E 2 := 1 1 + T T t=0 e t+1 2 First we apply average to Lemma 9 E 2 \u2264 32\u03b4 max (2\u03b7 2 (E I + \u03b6 2 ) + D 1 ).\n\nThen we rewrite key Lemma 7 as \u2207f (x t ) 2 2 + 1 2 E m t+1 \u2212 1 \u03b7 \u2206 t+1 2 2 \u2264 2 \u03b7 (r t \u2212 r t+1 ) + 2e t+1 1 + 2 \u03b7 2 e t+1 2 , and further average over time t C 1 + 1 2 C 2 \u2264 2r 0 \u03b7(T + 1) + 2E 1 + 2 \u03b7 2 E 2\n\nwhere we use \u2212f (x T +1 ) \u2264 \u2212f . Combined with (26) gives\nC 1 + 1 2 C 2 \u2264 2r 0 \u03b7(T + 1) + 2E 1 + 128\u03b4 max E I + 128\u03b4 max \u03b6 2 + 64\u03b4 max \u03b7 2 D 1(27)\nNow we also average Lemma 8 for e t+1 1 over t gives\n1 1 + T T t=0 e t+1 1 \u2264 1 \u2212 \u03b1 1 + T T t=0 e t 1 + 2\u03b1L 2 D 1 + \u03b1 2 \u03c3 2 |V R | + 2L 2 \u03b7 2 \u03b1 1 1 + T T t=0 m t \u2212 1 \u03b7 \u2206 t 2 2 \u2264 1 \u2212 \u03b1 1 + T T t=0 e t+1 1 + 2\u03b1L 2 D 1 + \u03b1 2 \u03c3 2 |V R | + 2L 2 \u03b7 2 \u03b1 C 2\nwhere we use \u039e 0 = e 0 1 = 0 andm 0 = \u2206 0 = 0. Then let \u03b2 1 := 2L 2 \u03b7 2 \u03b1 2 E 1 \u2264 2L 2 D 1 + \u03b1\u03c3 2 |V R | + \u03b2 1 C 2 .\n\nThis leads to 64\u03b4 max \u2264 \u03b3 2 25 \u2264 1 and \u03b2 2 \u2264 1, then we know D 1 \u2264 20\u03b7 2 \u03b3 2 (2\u03b1\u03c3 2 + 2\u03b6 2 + C 1 + (1 + 2\u03b2 1 )C 2 )\n\nFinally, we combine (27), (28), (30) C 1 + 1 2 C 2 \u2264 2r 0 \u03b7(T + 1) + 2E 1 + 128\u03b4 max E I + 128\u03b4 max \u03b6 2 + 64\u03b4 max \u03b7 2 D 1 \u2264 2r 0 \u03b7(T + 1) +(4L 2 D 1 + 2\u03b1\u03c3 2 |VR | + 2\u03b2 1 C 2 )+64\u03b4 max (4L 2 D 1 + 2\u03b2 2 \u03b1\u03c3 2 + 2\u03b2 1 C 2 ) + 128\u03b4 max \u03b6 2 + 64\u03b4 max \u03b7 2 D 1 \u2264 2r 0 \u03b7(T + 1) + (4L 2 + 256\u03b4 max L 2 + 64\u03b4 max \u03b7 2 )D 1 + ( 1 |VR | + 64\u03b4 max )2\u03b1\u03c3 2 +4\u03b2 1 C 2 + 128\u03b4 max \u03b6 2\n\nThen we replace D 1 with (31) C 1 + 1 2 C 2 \u2264 2r0 \u03b7(T +1) + ( 1 |VR | + 64\u03b4 max )2\u03b1\u03c3 2 +4\u03b2 1 C 2 + 128\u03b4 max \u03b6 2 + (4L 2 \u03b7 2 + 256\u03b4 max L 2 \u03b7 2 + 64\u03b4 max ) 20 \u03b3 2 (2\u03b1\u03c3 2 + 2\u03b6 2 + C 1 + (1 + 2\u03b2 1 )C 2 )\n\nTo have a valid bound on C 1 , there are two constraints on the coefficient of the RHS C 1 and C 2 .\n\n(4L 2 \u03b7 2 + 256\u03b4 max L 2 \u03b7 2 + 64\u03b4 max ) 20 \u03b3 2 <1 (4L 2 \u03b7 2 + 256\u03b4 max L 2 \u03b7 2 + 64\u03b4 max ) 20 \u03b3 2 (1 + 2\u03b2 1 ) + 4\u03b2 1 \u2264 1 2 .\n\nWe can strength the first requirement to (4L 2 \u03b7 2 + 256\u03b4 max L 2 \u03b7 2 + 64\u03b4 max ) 20 \u03b3 2 \u2264 1 4 .\n\nThen, apply this inequality to the second inequality gives 1 4 + 1 2 \u03b2 1 + 4\u03b2 1 \u2264 1 2 which requires \u03b7 \u2264 \u03b1 3L . Next (33) can be achieved by requiring \u03b4 max \u2264 \u03b3 2\n\n\n2\u00b75120\n\n(4 + 256\u03b4 max )L 2 \u03b7 2 + 64\u03b4 max \u2264 8L 2 \u03b7 2 + 64\u03b4 max \u2264 \u03b3 2 80 which requires 8\u03b7 2 L 2 \u2264 \u03b3 2 160 , and we can simplify it to \u03b7 \u2264 \u03b3 40L . Now we can simplify (32) with (33) 3 4 C 1 \u2264 2r0 \u03b7(T +1) + ( 1 |VR | + 64\u03b4 max )2\u03b1\u03c3 2 + 128\u03b4 max \u03b6 2 + (4L 2 \u03b7 2 + 256\u03b4 max L 2 \u03b7 2 + 64\u03b4 max ) 20 \u03b3 2 (2\u03b1\u03c3 2 + 2\u03b6 2 )\n\nMultiply both sides with 4 3 and relax constant 4 3 \u00b7 2 \u2264 3. Then by taking \u03b7 \u2264 1 2L we have that C 1 \u2264 3r0 \u03b7(T +1) + ( 1 |VR | + 151 \u03b3 2 64\u03b4 max )3\u03b1\u03c3 2 + 200 2 \u03b3 2 \u03b4 max \u03b6 2 + 200 \u03b3 2 (\u03b1\u03c3 2 + \u03b6 2 )L 2 \u03b7 2 By taking \u03b1 := 3\u03b7L and relax the constants we have C 1 \u2264 3r0 \u03b7(T +1) + ( 3 2 |VR | + 320 2 \u03b3 2 \u03b4 max )L\u03c3 2 \u03b7 + 200 \u03b3 2 (\u03b1\u03c3 2 + \u03b6 2 )L 2 \u03b7 2 + 200 2 \u03b3 2 \u03b4 max \u03b6 2 .\n\nFigure 3 .\n3Comparison of SCCLIP and TM and MEDIAN against dissensus attack with varying \u03b3 and \u03b4max. The points in the figure refer to the last iterates of all experiments. The x-axis is in log scale and the y-axis measures the mean square error of the last iterate to the average consensus. The TM and MEDIAN generate same outputs in this setup. The red dashed lines characterize the output of SCCLIP depending on \u03b4max/\u03b3 2 where in the rightmost region SCCLIP perform similar to TM and MEDIAN .\n\n\u03b5\nGorbunov et al., 2021)  which enjoys the scaling of total number of regular nodes. The second term O( ) for small \u03b5 because they additionally validates m random updates after each training step. However, it relies on secure protocols which do not easily generalize to constrained communication.Byzantine-robust decentralized SGD with constrained communication. Guo et al. (2021) do not provide theoretical analysis on convergence and Yang & Bajwa (2019a) only prove for full gradient, not SGD. Peng & Ling (2020) don't prove a rate for non-convex objective; but (Gorbunov et al., 2021) which show\n\nFigure 4 .Figure 5 .\n45Accuracy of the averaged model in clique A for the dumbbell topology. In the plot title \"B.\" stands for the bucketing technique for heterogeneous data from (Karimireddy et al., 2021b) and \"R.\" stands for adding 1 additional random edge between two cliques. Accuracies of SCCLIP under dissensus attack with a fixed \u03b3, \u03b6 2 and varying \u03b4max. The solid (resp. dashed) lines denote models averaged over all (resp. clique A or B) regular workers.The right sub-figure shows the performance of last iterates of curves in the left sub-figure.\n\nFigure 6 .\n6Accuracy of aggregators with or without the honest majority everywhere (H.M.E.) assumption. Regular workers are connected through a ring and have IID data.\n\nBibliography\nAbraham, I., Chan, T.-H. H., Dolev, D., Nayak, K., Pass, R., Ren, L., and Shi, E. Communication complexity of byzantine agreement, revisited, 2020. Alistarh, D., Allen-Zhu, Z., and Li, J. Byzantine stochastic gradient descent. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,  Montr\u00e9al, Canada, pp. 4618-4628, 2018. URL https: //proceedings.neurips.cc/paper/2018/hash/ a07c2f3b3b907aaf8436a26c6d77f0a2-Abstract. html.\n\n\nGorbunov, E., Borzunov, A., Diskin, M., and Ryabinin, M. Secure distributed training at scale, 2021.Guo, S., Zhang, T., Xie, X., Ma, L., Xiang, T., and Liu, Y. Towards byzantine-resilient learning in decentralized systems. arXiv 2002.08569, 2020.Guo, S., Zhang, T., Yu, H., Xie, X., Ma, L., Xiang, T., and Liu, Y. Byzantine-resilient decentralized stochastic gradient descent, 2021.Gupta, N., Doan, T. T., and Vaidya, N. Byzantine fault-tolerance in federated local sgd under 2f-redundancy. arXiv preprint arXiv:2108.11769, 2021.Hammer-Lahav, E., Recordon, D., and Hardt, D. The oauth 1.0 protocol. Technical report, RFC 5849, April, 2010. Hardt, D. et al. The oauth 2.0 authorization framework, 2012. Hastings, W. K. Monte carlo sampling methods using markov chains and their applications. 1970. He, L., Karimireddy, S. P., and Jaggi, M. Secure byzantine-robust machine learning. arXiv 2006.04747, 2020. Hirt, M. and Raykov, P. Multi-valued byzantine broadcast: The t \u00a1 n case. In ASIACRYPT, 2014. Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., D'Oliveira, R. G. L., Rouayheb, S. E., Evans, D., Gardner, J., Garrett, Z., Gasc\u00f3n, A., Ghazi, B., Gibbons, P. B., Gruteser, M., Harchaoui, Z., He, C., He, L., Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Khodak, M., Konecn\u00fd, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T., Liu, Y., Mittal, P., Mohri, M., Nock, R.,\u00d6zg\u00fcr, A., Pagh, R., Raykova, M., Qi, H., Ramage, D., Raskar, R., Song, D., Song, W., Stich, S. U., Sun, Z., Suresh, A. T., Tram\u00e8r, F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang, Q., Yu, F. X., Yu, H., and Zhao, S. Advances and open problems in federated learning. arXiv 1912.04977, 2019. Karimireddy, S. P., He, L., and Jaggi, M. Learning from history for byzantine robust optimization. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 5311-5319. PMLR, 2021a. URL http://proceedings. mlr.press/v139/karimireddy21a.html.Karimireddy, S. P., He, L., and Jaggi, M. Byzantine-robust learning on heterogeneous datasets via bucketing, 2021b.Koloskova, A., Stich, S. U., and Jaggi, M. Decentralized stochastic optimization and gossip algorithms with compressed communication, 2019.\n\nFigure 7 .\n7-compatible combination of \u03b3 and \u03b4 are ignored in theFigure 3. The dissensus attack is applied with \u03b5 = 0.05. The hyperparameter \u03b2 of trimmed mean (TM) is set to the actual number of Byzantine workers around the regular worker. The clipping radius of SCCLIP is chosen according to (10). The topology for the attacks on consensus. Black workers are regular while red ones are Byzantine.\n\nFigure 8 .\n8The iteration-to-error curves for defenses under dissensus attack. The TM* and MEDIAN* refer to the version of TM and MEDIAN which considers mixing weight.\n\nFigure 9 . 4 ?Figure 10 .\n9410Example of variant of dumbbell topology which consists of two cliques A and B (black) connected by an edge. TM \u03b2 = \u03b4 max SCCLIP \u03c4 = 0.1 MOZI \u03b1 = 0.5 where \u03b1 is the model averaging hyperparameter. \u03c1 i = 0.Ring topology used in experiment 3. Note that if the edge with question mark exists, then we don't have honest majority.\n\n\nA little is enough (ALIE): The attackers estimate the mean \u00b5 Ni and standard deviation \u03c3 Ni of the regular models, and send \u00b5 Ni \u2212 z\u03c3 Ni to regular worker i where z is a small constant controlling the strength of the attack(Baruch et al., 2019). The hyperparameter z for ALIE is computed according to(Baruch et al., 2019)    \n\n\nCoordinate-wise median(Yin et al., 2018)  and Krum(Blanchard et al., 2017)  usually perform worse than GM so we exclude them in the experiments. For attacks, we implement our dissensus along with some state of the art federated attacks Inner product manipulation(IPM) (Xie et al., 2019) and A little is enough (ALIE) (Baruch et al., 2019). More details on the adaptation of FL attacks to the decentralized setup is provided in Appendix B. 2 All implementations are based on PyTorch (Paszke et al., 2019) and evaluated on different graph topologies, with a distributed MNIST dataset (LeCun & Cortes, 2010).\n\n\nChen, L., Wang, H., Charles, Z. B., and Papailiopoulos, D. S. DRACO: byzantine-resilient distributed training via redundant gradients. In Dy, J. G. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 902-911. PMLR, 2018. URL http://proceedings.mlr.press/v80/chen18l.html.Byzantine-resilient non-convex stochastic gradient descent. In \n9th International Conference on Learning Representations, \nICLR 2021, Virtual Event, Austria, May 3-7, 2021. Open-\nReview.net, 2021. URL https://openreview.net/ \nforum?id=PbEHqvFtcS. \n\nAssran, M., Loizou, N., Ballas, N., and Rabbat, M. Stochastic \ngradient push for distributed deep learning, 2019. \n\nBaruch, G., Baruch, M., and Goldberg, Y. A little is enough: Cir-\ncumventing defenses for distributed learning. In Wallach, H. M., \nLarochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E. B., \nand Garnett, R. (eds.), Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural Information \nProcessing Systems 2019, NeurIPS 2019, December 8-14, 2019, \nVancouver, BC, Canada, pp. 8632-8642, 2019. URL https: \n//proceedings.neurips.cc/paper/2019/hash/ \nec1c59141046cd1866bbbcdfb6ae31d4-Abstract. \nhtml. \n\nBernstein, J., Zhao, J., Azizzadenesheli, K., and Anandkumar, A. \nsignsgd with majority vote is communication efficient and fault \ntolerant, 2019. \n\nBlanchard, P., Mhamdi, E. M. E., Guerraoui, R., and Stainer, \nJ. Machine learning with adversaries: Byzantine tolerant \ngradient descent. In Guyon, I., von Luxburg, U., Bengio, \nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., \nand Garnett, R. (eds.), Advances in Neural Information \nProcessing Systems 30: Annual Conference on Neural \n\nInformation Processing Systems 2017, December 4-9, 2017, \nLong Beach, CA, USA, pp. 119-129, 2017. URL https: \n//proceedings.neurips.cc/paper/2017/hash/ \nf4b9ec30ad9f68f89b29639786cb62ef-Abstract. \nhtml. \n\nBorge, M., Kokoris-Kogias, E., Jovanovic, P., Gasser, L., Gailly, \nN., and Ford, B. Proof-of-personhood: Redemocratizing permis-\nsionless cryptocurrencies. In 2017 IEEE European Symposium \non Security and Privacy Workshops (EuroS&PW), pp. 23-26. \nIEEE, 2017. \n\nBulteau, L., Shahaf, G., Shapiro, E., and Talmon, N. Aggregation \nover metric spaces: Proposing and voting in elections, budget-\ning, and legislation. Journal of Artificial Intelligence Research, \n70:1413-1439, 2021. \n\nBurkhalter, L., Lycklama, H., Viand, A., K\u00fcchler, N., and Hith-\nnawi, A. Rofl: Attestable robustness for secure federated learn-\ning, 2021. \n\nCappelletti, W. Byzantine-robust decentralized optimization \nfor Machine Learning. Master Thesis, EPFL, https: \n//williamcappelletti.github.io/assets/ \npdf/Cappelletti-ByzantineRobustDeSGD.pdf, \n2021. \n\nChen, Y., Su, L., and Xu, J. Distributed statistical machine learning \nin adversarial settings: Byzantine gradient descent. Proceed-\nings of the ACM on Measurement and Analysis of Computing \nSystems, 1(2):1-25, 2017. \n\nDamaskinos, G., Mhamdi, E. M. E., Guerraoui, R., Patra, R., and \nTaziki, M. Asynchronous byzantine machine learning (the case \nof SGD). In Dy, J. G. and Krause, A. (eds.), Proceedings of \nthe 35th International Conference on Machine Learning, ICML \n2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, \nvolume 80 of Proceedings of Machine Learning Research, pp. \n1153-1162. PMLR, 2018. URL http://proceedings. \nmlr.press/v80/damaskinos18a.html. \n\nData, D. and Diggavi, S. Byzantine-resilient sgd in high dimen-\nsions on heterogeneous data. In 2021 IEEE International Sym-\nposium on Information Theory (ISIT), pp. 2310-2315. IEEE, \n2021. \n\nDolev, D. and Strong, H. R. Authenticated algorithms for byzantine \nagreement. SIAM J. Comput., 12:656-666, 1983. \n\nEl-Mhamdi, E. M., Farhadkhani, S., Guerraoui, R., Guirguis, A., \nHoang, L.-N., and Rouault, S. Collaborative learning in the \njungle (decentralized, byzantine, heterogeneous, asynchronous \nand nonconvex learning). Advances in Neural Information \nProcessing Systems, 34, 2021. \n\nEl Mhamdi, E. M., Guerraoui, R., and Rouault, S. L. A. Distributed \nmomentum for byzantine-resilient stochastic gradient descent. \nIn 9th International Conference on Learning Representations \n(ICLR), number CONF, 2021. \nFischer, M. J., Lynch, N. A., and Merritt, M. Easy impossibility \nproofs for distributed consensus problems. Distributed Comput-\ning, 1(1):26-39, 1986. \n\nFord, B. 10. technologizing democracy or democratizing tech-\nnology? a layered-architecture perspective on potentials and \nchallenges. In Digital Technology and Democratic Theory, pp. \n274-321. University of Chicago Press, 2021. \n\n\n\n\nLamport, L., Shostak, R., and Pease, M. The byzantine generals problem.In Concurrency: the Works of Leslie Lamport, pp. 203-226. 2019. LeCun, Y. and Cortes, C. MNIST handwritten digit database. 2010. URL http://yann.lecun.com/exdb/mnist/. Xie, C., Koyejo, S., and Gupta, I. Zeno++: Robust fully asynchronous SGD. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 10495-10503. PMLR, 2020a. URL http: //proceedings.mlr.press/v119/xie20c.html.Koloskova, A., Lin, T., Stich, S. U., and Jaggi, M. Decentral-\nized deep learning with arbitrary communication compression, \n2020a. \n\nKoloskova, A., Loizou, N., Boreiri, S., Jaggi, M., and \nStich, S. U. A unified theory of decentralized SGD with \nchanging topology and local updates. In Proceedings of \nthe 37th International Conference on Machine Learning, \nICML 2020, 13-18 July 2020, Virtual Event, volume 119 \nof Proceedings of Machine Learning Research, pp. 5381-\n5393. PMLR, 2020b. URL http://proceedings.mlr. \npress/v119/koloskova20a.html. \n\nKoloskova, A., Lin, T., and Stich, S. U. An improved analysis of \ngradient tracking for decentralized machine learning. Advances \nin Neural Information Processing Systems, 34, 2021. \n\nKong, L., Lin, T., Koloskova, A., Jaggi, M., and Stich, S. U. Con-\nsensus control for decentralized deep learning. arXiv preprint \narXiv:2102.04828, 2021. \n\nKovalev, D., Koloskova, A., Jaggi, M., Richtarik, P., and Stich, S. \nA linearly convergent algorithm for decentralized optimization: \nSending less bits for free! In International Conference on \nArtificial Intelligence and Statistics, pp. 4087-4095. PMLR, \n2021. \n\nLi, L., Xu, W., Chen, T., Giannakis, G. B., and Ling, Q. RSA: \nbyzantine-robust stochastic aggregation methods for distributed \nlearning from heterogeneous datasets. In The Thirty-Third \nAAAI Conference on Artificial Intelligence, AAAI 2019, The \nThirty-First Innovative Applications of Artificial Intelligence \nConference, IAAI 2019, The Ninth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI 2019, Hon-\nolulu, Hawaii, USA, January 27 -February 1, 2019, pp. \n1544-1551. AAAI Press, 2019. doi: 10.1609/aaai.v33i01. \n33011544. URL https://doi.org/10.1609/aaai. \nv33i01.33011544. \n\nLi, X., Yang, W., Wang, S., and Zhang, Z. Communication-\nefficient local decentralized sgd methods, 2021. \n\nLian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and \nLiu, J. Can decentralized algorithms outperform centralized \nalgorithms? a case study for decentralized parallel stochastic \ngradient descent, 2017. \n\nLin, T., Karimireddy, S. P., Stich, S. U., and Jaggi, M. Quasi-\nglobal momentum: Accelerating decentralized deep learning on \nheterogeneous data. arXiv preprint arXiv:2102.04761, 2021. \n\nMcMahan, B., Moore, E., Ramage, D., Hampson, S., and y Ar-\ncas, B. A. Communication-efficient learning of deep net-\nworks from decentralized data. In Singh, A. and Zhu, X. J. \n(eds.), Proceedings of the 20th International Conference on \nArtificial Intelligence and Statistics, AISTATS 2017, 20-22 \nApril 2017, Fort Lauderdale, FL, USA, volume 54 of Proceed-\nings of Machine Learning Research, pp. 1273-1282. PMLR, \n2017. URL http://proceedings.mlr.press/v54/ \nmcmahan17a.html. \nSu, L. and Vaidya, N. Multi-agent optimization in the presence of \nbyzantine adversaries: Fundamental limits. In 2016 American \nControl Conference (ACC), pp. 7183-7188. IEEE, 2016a. \n\nSu, L. and Vaidya, N. H. Robust multi-agent optimization: coping \nwith byzantine agents with input redundancy. In International \nSymposium on Stabilization, Safety, and Security of Distributed \nSystems, pp. 368-382. Springer, 2016b. \n\nTang, H., Lian, X., Yan, M., Zhang, C., and Liu, J. D 2 : Decentral-\nized training over decentralized data, 2018. \n\nVogels, T., Karimireddy, S. P., and Jaggi, M. Practical low-rank \ncommunication compression in decentralized deep learning. In \nNeurIPS, 2020. \n\nVogels, T., He, L., Koloskova, A., Lin, T., Karimireddy, S. P., Stich, \nS. U., and Jaggi, M. Relaysum for decentralized deep learning \non heterogeneous data, 2021. \n\nXie, C., Koyejo, O., and Gupta, I. Generalized byzantine-tolerant \nsgd. arXiv 1802.10116, 2018. \n\nXie, C., Koyejo, O., and Gupta, I. Fall of empires: Breaking \nbyzantine-tolerant SGD by inner product manipulation. In \nGloberson, A. and Silva, R. (eds.), Proceedings of the Thirty-\nFifth Conference on Uncertainty in Artificial Intelligence, UAI \n2019, Tel Aviv, Israel, July 22-25, 2019, volume 115 of Proceed-\nings of Machine Learning Research, pp. 261-270. AUAI Press, \n2019. URL http://proceedings.mlr.press/v115/ \nxie20a.html. \n\nXie, C., Koyejo, S., and Gupta, I. Zeno++: Robust fully asyn-\nchronous sgd. In International Conference on Machine Learn-\ning, pp. 10495-10503. PMLR, 2020b. \n\nYang, Y. and Li, W. BASGD: buffered asynchronous SGD for \nbyzantine learning. In Meila, M. and Zhang, T. (eds.), Proceed-\nings of the 38th International Conference on Machine Learning, \nICML 2021, 18-24 July 2021, Virtual Event, volume 139 of \nProceedings of Machine Learning Research, pp. 11751-11761. \nPMLR, 2021. URL http://proceedings.mlr.press/ \nv139/yang21e.html. \n\nYang, Z. and Bajwa, W. U. Bridge: Byzantine-resilient decentral-\nized gradient descent. arXiv 1908.08098, 2019a. \n\nYang, Z. and Bajwa, W. U. Byrdie: Byzantine-resilient distributed \ncoordinate descent for decentralized learning. IEEE Trans-\nactions on Signal and Information Processing over Networks, \n2019b. \n\nYin, D., Chen, Y., Ramchandran, K., and Bartlett, P. L. Byzantine-\nrobust distributed learning: Towards optimal statistical rates. \nIn Dy, J. G. and Krause, A. (eds.), Proceedings of the 35th \nInternational Conference on Machine Learning, ICML 2018, \nStockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, vol-\nume 80 of Proceedings of Machine Learning Research, pp. \n5636-5645. PMLR, 2018. URL http://proceedings. \nmlr.press/v80/yin18a.html. \n\nYin, D., Chen, Y., Kannan, R., and Bartlett, P. Defending against \nsaddle point attack in byzantine-robust distributed learning. In \nInternational Conference on Machine Learning, pp. 7074-7084. \nPMLR, 2019. \nYing, B., Yuan, K., Chen, Y., Hu, H., Pan, P., and Yin, W. Exponen-\ntial graph is provably efficient for decentralized deep training. \nAdvances in Neural Information Processing Systems, 34, 2021a. \n\nYing, B., Yuan, K., Hu, H., Chen, Y., and Yin, W. Bluefog: Make \ndecentralized algorithms practical for optimization and deep \nlearning. arXiv preprint arXiv:2111.04287, 2021b. \n\nyong Sohn, J., Han, D.-J., Choi, B., and Moon, J. Election coding \nfor distributed learning: Protecting signsgd against byzantine \nattacks, 2020. \n\nYuan, K., Chen, Y., Huang, X., Zhang, Y., Pan, P., Xu, Y., and Yin, \nW. Decentlam: Decentralized momentum sgd for large-batch \ndeep training. arXiv preprint arXiv:2104.11981, 2021. \n\n\nTable 2 .\n2Default experimental settings for MNISTDataset \nMNIST \nArchitecture \nCONV-CONV-DROPOUT-FC-DROPOUT-FC \nTraining objective \nNegative log likelihood loss \nEvaluation objective \nTop-1 accuracy \n\nBatch size per worker 32 \nMomentum \n0.9 \nLearning rate \n0.01 \nLR decay \nNo \nLR warmup \nNo \nWeight decay \nNo \n\nRepetitions \n1 \nReported metric \nMean test accuracy over the last 150 iterations \n\n\n\nTable 4 .\n4Runtime hardwares and softwares.CPU \nModel name \nIntel (R) Xeon (R) Gold 6132 CPU @ 2.60 GHz \n# CPU(s) \n56 \nNUMA node(s) \n2 \n\nGPU \nProduct Name \nTesla V100-SXM2-32GB \nCUDA Version \n11.0 \n\nPyTorch \nVersion \n1.7.1 \n\nA.2.1. EXP 1: DECENTRALIZED DEFENSES WITHOUT ATTACKERS \n\nAggregators Hyperparameters \n\nGM \nT = 8 \nTM \n\u03b2 = \u03b4 max \nSCCLIP \n\u03c4 = 1 \nMOZI \n\u03b1 = 0.5 where \u03b1 is the model averaging hyperparameter. \u03c1 i = 0.99 \n\nBucketing \nEach bucket holds at most s = 2 vectors \n\nA.2.2. EXP 2: EFFECTS OF THE NUMBER OF BYZANTINE WORKERS \n\n\n\n\nwhere A = I.t+1 \n1 \n\n\u2264 2e t+1 \n\nI \n\n+ 2\u03b6 2 \n\nProof. Expand e t+1 \nA by expanding m t+1 \n\nj \n\ne t+1 \n\nA \n\n(13) \n\n\nWe can extend(Koloskova et al., 2020b, Lemma 15)) to the following lemma Lemma 11 (Tuning stepsize.).\u03a8 T := r 0 \u03b7(T + 1) + b\u03b7 + e\u03b7 2 + f \u03b7 3 \u2264 2( br 0 T + 1 )Then, take the uniform upper bound of the upper bound gives the result. I the only difference is that we don't have 1 n for \u03c3 2Similarly, let's call \u03b2 2 := 1The consensus distance Lemma 10 hasReplace E 2 using(26)gives(30),(29), then D 1 \u2264((1 + \u03b5)(1 \u2212 p) + 5(1 + 1 \u03b5 )(32\u03b4 max (1 + 4L 2 \u03b7 2 ) + 2L 2 \u03b7 2 )) + 5(1 + 1 \u03b5 )\u03b7 2 ((64\u03b4 max + \u03b2 2 )\u03b1\u03c3 2 + (64\u03b4 max + 1)\u03b6 2 + ((64\u03b4 max + 1)\u03b2 1 + 1)C 2 + C 1 ).By enforcing \u03b7 \u2264 \u03b3 9L and \u03b4 max \u2264 \u03b3 2 1600 we havewe can achieve 160\u03b4 max (1 + 4L 2 \u03b7 2 ) + 10L 2 \u03b7 2 \u2264 \u03b3 2 .Then D 1 \u2264 ((1 + \u03b5)(1 \u2212 p) + (1 + 1 \u03b5 ) \u03b3 2 4 ) =:T2 D 1 + 5(1 + 1 \u03b5 )\u03b7 2 ((64\u03b4 max + \u03b2 2 )\u03b1\u03c3 2 + (64\u03b4 max + 1)\u03b6 2 + ((64\u03b4 max + 1)\u03b2 1 + 1)C 2 + C 1 ).Let us minimize the the coefficients of D 1 on the right hand side of inequality by having. Then the coefficient becomesThen we use 1 \u03b5 = 4(1\u2212p) \u03b3 2 \u2264 2 \u03b3 and 1 + 1 \u03b5 \u2264 3 \u03b3 D 1 \u2264 20\u03b7 2 \u03b3 2 ((64\u03b4 max + \u03b2 2 )\u03b1\u03c3 2 + (64\u03b4 max + 1)\u03b6 2 + ((64\u03b4 max + 1)\u03b2 1 + 1)C 2 + C 1 ).\u2207f (x t ) 2 2 \u2264 200 2 \u03b3 2 \u03b4 max \u03b6 2 + 2 \uf8eb \uf8ed ( 3 2 |VR | + 320 2 \u03b3 2 \u03b4 max )3L\u03c3 2 r 0 T + 1\nThe hidden vulnerability of distributed learning in byzantium. E M E Mhamdi, R Guerraoui, S Rouault, PMLRProceedings of the 35th International Conference on Machine Learning, ICML 2018. Dy, J. G. and Krause, A.the 35th International Conference on Machine Learning, ICML 2018Stockholmsm\u00e4ssan, Stockholm, Sweden80Mhamdi, E. M. E., Guerraoui, R., and Rouault, S. The hidden vul- nerability of distributed learning in byzantium. In Dy, J. G. and Krause, A. (eds.), Proceedings of the 35th International Con- ference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceed- ings of Machine Learning Research, pp. 3518-3527. PMLR, 2018. URL http://proceedings.mlr.press/v80/ mhamdi18a.html.\n\nDistributed gradient methods for convex machine learning problems in networks: Distributed optimization. A Nedic, IEEE Signal Processing Magazine. 373Nedic, A. Distributed gradient methods for convex machine learn- ing problems in networks: Distributed optimization. IEEE Signal Processing Magazine, 37(3):92-101, 2020.\n\nEluding secure aggregation in federated learning via model inconsistency. D Pasquini, D Francati, Ateniese , G , Pasquini, D., Francati, D., and Ateniese, G. Eluding secure aggre- gation in federated learning via model inconsistency, 2021.\n\nAn imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A K\u00f6pf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Pytorch, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E. B., and Garnett, R.NeurIPS; BC, CanadaVancouverPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K\u00f6pf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Van- couver, BC, Canada, pp. 8024-8035, 2019. URL https: //proceedings.neurips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract. html.\n\nReaching agreement in the presence of faults. M C Pease, R E Shostak, L Lamport, J. ACM. 27Pease, M. C., Shostak, R. E., and Lamport, L. Reaching agreement in the presence of faults. J. ACM, 27:228-234, 1980.\n\nByzantine-robust decentralized stochastic optimization. J Peng, Q Ling, 10.1109/ICASSP40776.2020.90543772020 IEEE International Conference on Acoustics, Speech and Signal Processing. Barcelona, SpainIEEE20202020Peng, J. and Ling, Q. Byzantine-robust decentralized stochastic optimization. In 2020 IEEE International Conference on Acous- tics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pp. 5935-5939. IEEE, 2020. doi: 10.1109/ICASSP40776.2020.9054377. URL https://doi. org/10.1109/ICASSP40776.2020.9054377.\n\nRobust aggregation for federated learning. K Pillutla, S M Kakade, Z Harchaoui, arXiv:1912.13445arXiv preprintPillutla, K., Kakade, S. M., and Harchaoui, Z. Robust aggregation for federated learning. arXiv preprint arXiv:1912.13445, 2019.\n\nBuilding a sybil-resilient digital community utilizing trust-graph connectivity. O Poupko, G Shahaf, E Shapiro, N Talmon, IEEE/ACM Transactions on Networking. Poupko, O., Shahaf, G., Shapiro, E., and Talmon, N. Building a sybil-resilient digital community utilizing trust-graph connec- tivity. IEEE/ACM Transactions on Networking, 2021.\n\nDETOX: A redundancy-based framework for faster and more robust gradient aggregation. S Rajput, H Wang, Z B Charles, D S ; M Papailiopoulos, H Larochelle, A Beygelzimer, F Buc, E B Fox, Garnett , Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. R.NeurIPS; BC, CanadaVancouverWallach, HRajput, S., Wang, H., Charles, Z. B., and Papailiopoulos, D. S. DETOX: A redundancy-based framework for faster and more robust gradient aggregation. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Van- couver, BC, Canada, pp. 10320-10330, 2019. URL https: //proceedings.neurips.cc/paper/2019/hash/ 415185ea244ea2b2bedeb0449b926802-Abstract. html.\n\nByzantine sgd with arbitrary number of attackers. J Regatti, H Chen, A Gupta, Bygars, Regatti, J., Chen, H., and Gupta, A. Bygars: Byzantine sgd with arbitrary number of attackers, 2020.\n", "annotations": {"author": "[{\"end\":77,\"start\":70},{\"end\":103,\"start\":78},{\"end\":117,\"start\":104}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":74},{\"end\":102,\"start\":91},{\"end\":116,\"start\":111}]", "author_first_name": "[{\"end\":73,\"start\":70},{\"end\":81,\"start\":78},{\"end\":90,\"start\":82},{\"end\":110,\"start\":104}]", "author_affiliation": null, "title": "[{\"end\":67,\"start\":1},{\"end\":184,\"start\":118}]", "venue": null, "abstract": "[{\"end\":1037,\"start\":186}]", "bib_ref": "[{\"end\":1247,\"start\":1225},{\"end\":1268,\"start\":1247},{\"end\":1432,\"start\":1413},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1444,\"start\":1432},{\"end\":1468,\"start\":1444},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2528,\"start\":2508},{\"end\":2549,\"start\":2528},{\"end\":2569,\"start\":2549},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2726,\"start\":2703},{\"end\":4224,\"start\":4220},{\"end\":4293,\"start\":4274},{\"end\":4316,\"start\":4293},{\"end\":4333,\"start\":4316},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4353,\"start\":4333},{\"end\":4370,\"start\":4353},{\"end\":4387,\"start\":4370},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4429,\"start\":4406},{\"end\":4463,\"start\":4439},{\"end\":4479,\"start\":4463},{\"end\":4502,\"start\":4479},{\"end\":4540,\"start\":4513},{\"end\":4593,\"start\":4570},{\"end\":4616,\"start\":4593},{\"end\":4637,\"start\":4616},{\"end\":4793,\"start\":4774},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4814,\"start\":4793},{\"end\":4895,\"start\":4875},{\"end\":4913,\"start\":4895},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4933,\"start\":4913},{\"end\":4952,\"start\":4933},{\"end\":5078,\"start\":5057},{\"end\":5095,\"start\":5078},{\"end\":5192,\"start\":5166},{\"end\":5217,\"start\":5197},{\"end\":5288,\"start\":5265},{\"end\":5653,\"start\":5621},{\"end\":5681,\"start\":5655},{\"end\":5706,\"start\":5683},{\"end\":5897,\"start\":5870},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5915,\"start\":5897},{\"end\":5938,\"start\":5917},{\"end\":5989,\"start\":5967},{\"end\":6007,\"start\":5991},{\"end\":6033,\"start\":6009},{\"end\":6279,\"start\":6260},{\"end\":6303,\"start\":6279},{\"end\":6319,\"start\":6303},{\"end\":6338,\"start\":6319},{\"end\":6355,\"start\":6338},{\"end\":6373,\"start\":6355},{\"end\":6391,\"start\":6373},{\"end\":6412,\"start\":6391},{\"end\":6481,\"start\":6457},{\"end\":6487,\"start\":6481},{\"end\":6507,\"start\":6487},{\"end\":6547,\"start\":6528},{\"end\":6567,\"start\":6547},{\"end\":6590,\"start\":6567},{\"end\":6640,\"start\":6619},{\"end\":6659,\"start\":6640},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6877,\"start\":6857},{\"end\":6898,\"start\":6877},{\"end\":6918,\"start\":6898},{\"end\":7008,\"start\":6985},{\"end\":7031,\"start\":7008},{\"end\":7501,\"start\":7479},{\"end\":8040,\"start\":8023},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8240,\"start\":8222},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9728,\"start\":9709},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10082,\"start\":10061},{\"end\":10126,\"start\":10106},{\"end\":10206,\"start\":10179},{\"end\":10225,\"start\":10206},{\"end\":13808,\"start\":13786},{\"end\":13827,\"start\":13808},{\"end\":14633,\"start\":14611},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17615,\"start\":17596},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18177,\"start\":18155},{\"end\":18697,\"start\":18679},{\"end\":19008,\"start\":18981},{\"end\":22281,\"start\":22257},{\"end\":23332,\"start\":23307},{\"end\":23761,\"start\":23734},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24298,\"start\":24279},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25306,\"start\":25283},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25521,\"start\":25502},{\"end\":27579,\"start\":27578},{\"end\":32043,\"start\":32021},{\"end\":34924,\"start\":34908},{\"end\":36373,\"start\":36355},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37455,\"start\":37437},{\"end\":38177,\"start\":38152}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":53340,\"start\":52844},{\"attributes\":{\"id\":\"fig_2\"},\"end\":53940,\"start\":53341},{\"attributes\":{\"id\":\"fig_4\"},\"end\":54498,\"start\":53941},{\"attributes\":{\"id\":\"fig_5\"},\"end\":54667,\"start\":54499},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55305,\"start\":54668},{\"attributes\":{\"id\":\"fig_7\"},\"end\":57730,\"start\":55306},{\"attributes\":{\"id\":\"fig_8\"},\"end\":58129,\"start\":57731},{\"attributes\":{\"id\":\"fig_9\"},\"end\":58298,\"start\":58130},{\"attributes\":{\"id\":\"fig_11\"},\"end\":58654,\"start\":58299},{\"attributes\":{\"id\":\"fig_12\"},\"end\":58982,\"start\":58655},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":59590,\"start\":58983},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":64311,\"start\":59591},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":71207,\"start\":64312},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":71604,\"start\":71208},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":72145,\"start\":71605},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":72260,\"start\":72146}]", "paragraph": "[{\"end\":1072,\"start\":1053},{\"end\":1880,\"start\":1074},{\"end\":2727,\"start\":1882},{\"end\":2894,\"start\":2729},{\"end\":4012,\"start\":2896},{\"end\":5328,\"start\":4029},{\"end\":6209,\"start\":5330},{\"end\":7776,\"start\":6211},{\"end\":8625,\"start\":7778},{\"end\":8980,\"start\":8650},{\"end\":9317,\"start\":8982},{\"end\":9355,\"start\":9319},{\"end\":9754,\"start\":9357},{\"end\":10272,\"start\":9756},{\"end\":10356,\"start\":10303},{\"end\":10661,\"start\":10427},{\"end\":10750,\"start\":10663},{\"end\":10798,\"start\":10795},{\"end\":11312,\"start\":11006},{\"end\":11787,\"start\":11359},{\"end\":12608,\"start\":11853},{\"end\":12731,\"start\":12610},{\"end\":12954,\"start\":12777},{\"end\":13237,\"start\":13096},{\"end\":13504,\"start\":13272},{\"end\":13580,\"start\":13554},{\"end\":13828,\"start\":13624},{\"end\":14841,\"start\":13870},{\"end\":15524,\"start\":14862},{\"end\":15619,\"start\":15526},{\"end\":15906,\"start\":15621},{\"end\":16527,\"start\":15970},{\"end\":16832,\"start\":16529},{\"end\":16891,\"start\":16834},{\"end\":17219,\"start\":16956},{\"end\":17427,\"start\":17259},{\"end\":17899,\"start\":17429},{\"end\":18128,\"start\":17926},{\"end\":18209,\"start\":18130},{\"end\":18311,\"start\":18264},{\"end\":18379,\"start\":18313},{\"end\":18537,\"start\":18435},{\"end\":18954,\"start\":18539},{\"end\":19255,\"start\":18956},{\"end\":19439,\"start\":19328},{\"end\":19529,\"start\":19482},{\"end\":19752,\"start\":19666},{\"end\":20324,\"start\":19754},{\"end\":20662,\"start\":20326},{\"end\":20903,\"start\":20678},{\"end\":21423,\"start\":20905},{\"end\":21560,\"start\":21461},{\"end\":21676,\"start\":21646},{\"end\":21891,\"start\":21873},{\"end\":22491,\"start\":21991},{\"end\":22603,\"start\":22504},{\"end\":22717,\"start\":22668},{\"end\":23146,\"start\":22938},{\"end\":23566,\"start\":23182},{\"end\":24085,\"start\":23568},{\"end\":24429,\"start\":24087},{\"end\":25041,\"start\":24431},{\"end\":25747,\"start\":25057},{\"end\":26533,\"start\":25761},{\"end\":27614,\"start\":27146},{\"end\":28121,\"start\":27616},{\"end\":28530,\"start\":28123},{\"end\":28807,\"start\":28532},{\"end\":29503,\"start\":28809},{\"end\":30265,\"start\":29549},{\"end\":31280,\"start\":30301},{\"end\":31808,\"start\":31295},{\"end\":32324,\"start\":31810},{\"end\":32798,\"start\":32326},{\"end\":32865,\"start\":32800},{\"end\":33005,\"start\":32895},{\"end\":34185,\"start\":33040},{\"end\":34445,\"start\":34258},{\"end\":34539,\"start\":34494},{\"end\":34714,\"start\":34541},{\"end\":34786,\"start\":34776},{\"end\":35142,\"start\":34788},{\"end\":35458,\"start\":35328},{\"end\":35696,\"start\":35531},{\"end\":35945,\"start\":35728},{\"end\":36150,\"start\":35993},{\"end\":36259,\"start\":36188},{\"end\":36457,\"start\":36261},{\"end\":36702,\"start\":36500},{\"end\":36767,\"start\":36704},{\"end\":36887,\"start\":36827},{\"end\":37046,\"start\":36889},{\"end\":37130,\"start\":37091},{\"end\":37578,\"start\":37392},{\"end\":38417,\"start\":37614},{\"end\":38503,\"start\":38433},{\"end\":38773,\"start\":38691},{\"end\":38829,\"start\":38783},{\"end\":38890,\"start\":38831},{\"end\":39020,\"start\":38892},{\"end\":39147,\"start\":39022},{\"end\":39289,\"start\":39149},{\"end\":39351,\"start\":39291},{\"end\":39488,\"start\":39390},{\"end\":39636,\"start\":39567},{\"end\":39745,\"start\":39671},{\"end\":39865,\"start\":39791},{\"end\":39949,\"start\":39926},{\"end\":40086,\"start\":39998},{\"end\":40290,\"start\":40196},{\"end\":40423,\"start\":40377},{\"end\":40621,\"start\":40574},{\"end\":40813,\"start\":40759},{\"end\":40888,\"start\":40815},{\"end\":41035,\"start\":41002},{\"end\":41132,\"start\":41068},{\"end\":41435,\"start\":41401},{\"end\":41753,\"start\":41704},{\"end\":42088,\"start\":41852},{\"end\":42107,\"start\":42090},{\"end\":42242,\"start\":42138},{\"end\":42393,\"start\":42343},{\"end\":42551,\"start\":42468},{\"end\":42766,\"start\":42675},{\"end\":42933,\"start\":42853},{\"end\":43208,\"start\":43034},{\"end\":43311,\"start\":43275},{\"end\":43465,\"start\":43410},{\"end\":43548,\"start\":43467},{\"end\":43626,\"start\":43550},{\"end\":43678,\"start\":43628},{\"end\":43963,\"start\":43846},{\"end\":44290,\"start\":44245},{\"end\":44478,\"start\":44398},{\"end\":44757,\"start\":44672},{\"end\":44936,\"start\":44896},{\"end\":45149,\"start\":45094},{\"end\":45265,\"start\":45243},{\"end\":45619,\"start\":45484},{\"end\":45807,\"start\":45743},{\"end\":46125,\"start\":46044},{\"end\":46294,\"start\":46276},{\"end\":46511,\"start\":46447},{\"end\":46605,\"start\":46570},{\"end\":46716,\"start\":46678},{\"end\":47335,\"start\":47222},{\"end\":47448,\"start\":47337},{\"end\":47538,\"start\":47491},{\"end\":47621,\"start\":47571},{\"end\":47761,\"start\":47675},{\"end\":47975,\"start\":47763},{\"end\":48210,\"start\":48151},{\"end\":48629,\"start\":48553},{\"end\":48796,\"start\":48758},{\"end\":48934,\"start\":48869},{\"end\":49326,\"start\":49054},{\"end\":49440,\"start\":49328},{\"end\":49562,\"start\":49470},{\"end\":49575,\"start\":49564},{\"end\":49741,\"start\":49687},{\"end\":49956,\"start\":49743},{\"end\":49990,\"start\":49958},{\"end\":50042,\"start\":49992},{\"end\":50262,\"start\":50154},{\"end\":50469,\"start\":50264},{\"end\":50528,\"start\":50471},{\"end\":50670,\"start\":50618},{\"end\":50983,\"start\":50867},{\"end\":51100,\"start\":50985},{\"end\":51465,\"start\":51102},{\"end\":51667,\"start\":51467},{\"end\":51769,\"start\":51669},{\"end\":51896,\"start\":51771},{\"end\":51994,\"start\":51898},{\"end\":52158,\"start\":51996},{\"end\":52472,\"start\":52169},{\"end\":52843,\"start\":52474}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10426,\"start\":10357},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10794,\"start\":10751},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11005,\"start\":10799},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11358,\"start\":11313},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11852,\"start\":11788},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12776,\"start\":12732},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13095,\"start\":12955},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13553,\"start\":13505},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13623,\"start\":13581},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15969,\"start\":15907},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16955,\"start\":16892},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17258,\"start\":17220},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18263,\"start\":18210},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18434,\"start\":18380},{\"attributes\":{\"id\":\"formula_14\"},\"end\":19327,\"start\":19256},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19481,\"start\":19440},{\"attributes\":{\"id\":\"formula_16\"},\"end\":19665,\"start\":19530},{\"attributes\":{\"id\":\"formula_17\"},\"end\":21645,\"start\":21561},{\"attributes\":{\"id\":\"formula_18\"},\"end\":21872,\"start\":21677},{\"attributes\":{\"id\":\"formula_19\"},\"end\":21990,\"start\":21892},{\"attributes\":{\"id\":\"formula_20\"},\"end\":22667,\"start\":22604},{\"attributes\":{\"id\":\"formula_21\"},\"end\":22937,\"start\":22718},{\"attributes\":{\"id\":\"formula_22\"},\"end\":27102,\"start\":26534},{\"attributes\":{\"id\":\"formula_23\"},\"end\":34209,\"start\":34186},{\"attributes\":{\"id\":\"formula_24\"},\"end\":34493,\"start\":34446},{\"attributes\":{\"id\":\"formula_25\"},\"end\":34775,\"start\":34715},{\"attributes\":{\"id\":\"formula_27\"},\"end\":35327,\"start\":35207},{\"attributes\":{\"id\":\"formula_28\"},\"end\":35530,\"start\":35459},{\"attributes\":{\"id\":\"formula_29\"},\"end\":36187,\"start\":36151},{\"attributes\":{\"id\":\"formula_30\"},\"end\":36499,\"start\":36458},{\"attributes\":{\"id\":\"formula_31\"},\"end\":37090,\"start\":37047},{\"attributes\":{\"id\":\"formula_32\"},\"end\":37391,\"start\":37131},{\"attributes\":{\"id\":\"formula_33\"},\"end\":37613,\"start\":37579},{\"attributes\":{\"id\":\"formula_34\"},\"end\":38690,\"start\":38504},{\"attributes\":{\"id\":\"formula_35\"},\"end\":38782,\"start\":38774},{\"attributes\":{\"id\":\"formula_36\"},\"end\":39566,\"start\":39489},{\"attributes\":{\"id\":\"formula_37\"},\"end\":39670,\"start\":39637},{\"attributes\":{\"id\":\"formula_38\"},\"end\":39790,\"start\":39746},{\"attributes\":{\"id\":\"formula_39\"},\"end\":39925,\"start\":39866},{\"attributes\":{\"id\":\"formula_40\"},\"end\":39997,\"start\":39950},{\"attributes\":{\"id\":\"formula_41\"},\"end\":40195,\"start\":40087},{\"attributes\":{\"id\":\"formula_42\"},\"end\":40376,\"start\":40291},{\"attributes\":{\"id\":\"formula_43\"},\"end\":40573,\"start\":40424},{\"attributes\":{\"id\":\"formula_44\"},\"end\":40744,\"start\":40622},{\"attributes\":{\"id\":\"formula_45\"},\"end\":41001,\"start\":40889},{\"attributes\":{\"id\":\"formula_46\"},\"end\":41067,\"start\":41036},{\"attributes\":{\"id\":\"formula_47\"},\"end\":41400,\"start\":41133},{\"attributes\":{\"id\":\"formula_48\"},\"end\":41703,\"start\":41436},{\"attributes\":{\"id\":\"formula_49\"},\"end\":41851,\"start\":41754},{\"attributes\":{\"id\":\"formula_50\"},\"end\":42137,\"start\":42108},{\"attributes\":{\"id\":\"formula_51\"},\"end\":42342,\"start\":42243},{\"attributes\":{\"id\":\"formula_52\"},\"end\":42467,\"start\":42394},{\"attributes\":{\"id\":\"formula_53\"},\"end\":42674,\"start\":42552},{\"attributes\":{\"id\":\"formula_54\"},\"end\":42852,\"start\":42767},{\"attributes\":{\"id\":\"formula_55\"},\"end\":43033,\"start\":42934},{\"attributes\":{\"id\":\"formula_56\"},\"end\":43274,\"start\":43209},{\"attributes\":{\"id\":\"formula_57\"},\"end\":43409,\"start\":43312},{\"attributes\":{\"id\":\"formula_58\"},\"end\":43845,\"start\":43679},{\"attributes\":{\"id\":\"formula_59\"},\"end\":44244,\"start\":43964},{\"attributes\":{\"id\":\"formula_60\"},\"end\":44397,\"start\":44291},{\"attributes\":{\"id\":\"formula_61\"},\"end\":44671,\"start\":44479},{\"attributes\":{\"id\":\"formula_62\"},\"end\":44895,\"start\":44758},{\"attributes\":{\"id\":\"formula_63\"},\"end\":45093,\"start\":44937},{\"attributes\":{\"id\":\"formula_64\"},\"end\":45242,\"start\":45150},{\"attributes\":{\"id\":\"formula_65\"},\"end\":45483,\"start\":45266},{\"attributes\":{\"id\":\"formula_66\"},\"end\":45742,\"start\":45620},{\"attributes\":{\"id\":\"formula_67\"},\"end\":46043,\"start\":45808},{\"attributes\":{\"id\":\"formula_68\"},\"end\":46275,\"start\":46126},{\"attributes\":{\"id\":\"formula_69\"},\"end\":46446,\"start\":46295},{\"attributes\":{\"id\":\"formula_70\"},\"end\":46569,\"start\":46512},{\"attributes\":{\"id\":\"formula_71\"},\"end\":46677,\"start\":46606},{\"attributes\":{\"id\":\"formula_72\"},\"end\":47221,\"start\":46717},{\"attributes\":{\"id\":\"formula_73\"},\"end\":47490,\"start\":47449},{\"attributes\":{\"id\":\"formula_74\"},\"end\":47570,\"start\":47539},{\"attributes\":{\"id\":\"formula_75\"},\"end\":47674,\"start\":47622},{\"attributes\":{\"id\":\"formula_76\"},\"end\":48150,\"start\":47976},{\"attributes\":{\"id\":\"formula_77\"},\"end\":48552,\"start\":48211},{\"attributes\":{\"id\":\"formula_78\"},\"end\":48757,\"start\":48630},{\"attributes\":{\"id\":\"formula_79\"},\"end\":48868,\"start\":48797},{\"attributes\":{\"id\":\"formula_80\"},\"end\":49053,\"start\":48935},{\"attributes\":{\"id\":\"formula_81\"},\"end\":49469,\"start\":49441},{\"attributes\":{\"id\":\"formula_82\"},\"end\":49686,\"start\":49576},{\"attributes\":{\"id\":\"formula_83\"},\"end\":50153,\"start\":50043},{\"attributes\":{\"id\":\"formula_85\"},\"end\":50617,\"start\":50529},{\"attributes\":{\"id\":\"formula_86\"},\"end\":50866,\"start\":50671}]", "table_ref": "[{\"end\":23145,\"start\":23138},{\"end\":25401,\"start\":25394},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34307,\"start\":34300},{\"end\":34343,\"start\":34336},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34605,\"start\":34598},{\"end\":35578,\"start\":35571}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1051,\"start\":1039},{\"attributes\":{\"n\":\"2.\"},\"end\":4027,\"start\":4015},{\"attributes\":{\"n\":\"3.\"},\"end\":8633,\"start\":8628},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8648,\"start\":8636},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10301,\"start\":10275},{\"attributes\":{\"n\":\"4.\"},\"end\":13270,\"start\":13240},{\"attributes\":{\"n\":\"4.1.\"},\"end\":13868,\"start\":13831},{\"attributes\":{\"n\":\"4.2.\"},\"end\":14860,\"start\":14844},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17924,\"start\":17902},{\"attributes\":{\"n\":\"4.4.\"},\"end\":20676,\"start\":20665},{\"attributes\":{\"n\":\"5.\"},\"end\":21459,\"start\":21426},{\"attributes\":{\"n\":\"5.1.\"},\"end\":22502,\"start\":22494},{\"end\":23180,\"start\":23149},{\"attributes\":{\"n\":\"6.\"},\"end\":25055,\"start\":25044},{\"end\":25759,\"start\":25750},{\"attributes\":{\"n\":\"6.1.\"},\"end\":27144,\"start\":27104},{\"attributes\":{\"n\":\"6.2.\"},\"end\":29547,\"start\":29506},{\"attributes\":{\"n\":\"6.3.\"},\"end\":30299,\"start\":30268},{\"attributes\":{\"n\":\"7.\"},\"end\":31293,\"start\":31283},{\"end\":32893,\"start\":32868},{\"end\":33038,\"start\":33008},{\"end\":34256,\"start\":34211},{\"end\":35206,\"start\":35145},{\"end\":35726,\"start\":35699},{\"end\":35991,\"start\":35948},{\"attributes\":{\"n\":\"2.\"},\"end\":36825,\"start\":36770},{\"end\":38431,\"start\":38420},{\"end\":39388,\"start\":39354},{\"end\":40757,\"start\":40746},{\"end\":52167,\"start\":52161},{\"end\":52855,\"start\":52845},{\"end\":53343,\"start\":53342},{\"end\":53962,\"start\":53942},{\"end\":54510,\"start\":54500},{\"end\":54681,\"start\":54669},{\"end\":57742,\"start\":57732},{\"end\":58141,\"start\":58131},{\"end\":58325,\"start\":58300},{\"end\":71218,\"start\":71209},{\"end\":71615,\"start\":71606}]", "table": "[{\"end\":64311,\"start\":60030},{\"end\":71207,\"start\":64880},{\"end\":71604,\"start\":71259},{\"end\":72145,\"start\":71649},{\"end\":72260,\"start\":72160}]", "figure_caption": "[{\"end\":53340,\"start\":52857},{\"end\":53940,\"start\":53344},{\"end\":54498,\"start\":53965},{\"end\":54667,\"start\":54512},{\"end\":55305,\"start\":54682},{\"end\":57730,\"start\":55308},{\"end\":58129,\"start\":57744},{\"end\":58298,\"start\":58143},{\"end\":58654,\"start\":58330},{\"end\":58982,\"start\":58657},{\"end\":59590,\"start\":58985},{\"end\":60030,\"start\":59593},{\"end\":64880,\"start\":64314},{\"end\":71259,\"start\":71220},{\"end\":71649,\"start\":71617},{\"end\":72160,\"start\":72148}]", "figure_ref": "[{\"end\":15733,\"start\":15725},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20689,\"start\":20681},{\"end\":24278,\"start\":24277},{\"end\":25906,\"start\":25898},{\"end\":27254,\"start\":27246},{\"end\":29770,\"start\":29762},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30808,\"start\":30800},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33096,\"start\":33088},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33110,\"start\":33102},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":33569,\"start\":33561}]", "bib_author_first_name": "[{\"end\":73501,\"start\":73500},{\"end\":73505,\"start\":73502},{\"end\":73515,\"start\":73514},{\"end\":73528,\"start\":73527},{\"end\":74281,\"start\":74280},{\"end\":74571,\"start\":74570},{\"end\":74583,\"start\":74582},{\"end\":74602,\"start\":74594},{\"end\":74606,\"start\":74605},{\"end\":74799,\"start\":74798},{\"end\":74809,\"start\":74808},{\"end\":74818,\"start\":74817},{\"end\":74827,\"start\":74826},{\"end\":74836,\"start\":74835},{\"end\":74848,\"start\":74847},{\"end\":74858,\"start\":74857},{\"end\":74869,\"start\":74868},{\"end\":74876,\"start\":74875},{\"end\":74890,\"start\":74889},{\"end\":74900,\"start\":74899},{\"end\":74913,\"start\":74912},{\"end\":74921,\"start\":74920},{\"end\":74929,\"start\":74928},{\"end\":74939,\"start\":74938},{\"end\":74949,\"start\":74948},{\"end\":74959,\"start\":74958},{\"end\":74975,\"start\":74974},{\"end\":74986,\"start\":74985},{\"end\":74994,\"start\":74993},{\"end\":75001,\"start\":75000},{\"end\":76040,\"start\":76039},{\"end\":76042,\"start\":76041},{\"end\":76051,\"start\":76050},{\"end\":76053,\"start\":76052},{\"end\":76064,\"start\":76063},{\"end\":76260,\"start\":76259},{\"end\":76268,\"start\":76267},{\"end\":76788,\"start\":76787},{\"end\":76800,\"start\":76799},{\"end\":76802,\"start\":76801},{\"end\":76812,\"start\":76811},{\"end\":77066,\"start\":77065},{\"end\":77076,\"start\":77075},{\"end\":77086,\"start\":77085},{\"end\":77097,\"start\":77096},{\"end\":77408,\"start\":77407},{\"end\":77418,\"start\":77417},{\"end\":77426,\"start\":77425},{\"end\":77428,\"start\":77427},{\"end\":77439,\"start\":77438},{\"end\":77445,\"start\":77440},{\"end\":77463,\"start\":77462},{\"end\":77477,\"start\":77476},{\"end\":77492,\"start\":77491},{\"end\":77499,\"start\":77498},{\"end\":77501,\"start\":77500},{\"end\":77514,\"start\":77507},{\"end\":78281,\"start\":78280},{\"end\":78292,\"start\":78291},{\"end\":78300,\"start\":78299}]", "bib_author_last_name": "[{\"end\":73512,\"start\":73506},{\"end\":73525,\"start\":73516},{\"end\":73536,\"start\":73529},{\"end\":74287,\"start\":74282},{\"end\":74580,\"start\":74572},{\"end\":74592,\"start\":74584},{\"end\":74806,\"start\":74800},{\"end\":74815,\"start\":74810},{\"end\":74824,\"start\":74819},{\"end\":74833,\"start\":74828},{\"end\":74845,\"start\":74837},{\"end\":74855,\"start\":74849},{\"end\":74866,\"start\":74859},{\"end\":74873,\"start\":74870},{\"end\":74887,\"start\":74877},{\"end\":74897,\"start\":74891},{\"end\":74910,\"start\":74901},{\"end\":74918,\"start\":74914},{\"end\":74926,\"start\":74922},{\"end\":74936,\"start\":74930},{\"end\":74946,\"start\":74940},{\"end\":74956,\"start\":74950},{\"end\":74972,\"start\":74960},{\"end\":74983,\"start\":74976},{\"end\":74991,\"start\":74987},{\"end\":74998,\"start\":74995},{\"end\":75010,\"start\":75002},{\"end\":75019,\"start\":75012},{\"end\":76048,\"start\":76043},{\"end\":76061,\"start\":76054},{\"end\":76072,\"start\":76065},{\"end\":76265,\"start\":76261},{\"end\":76273,\"start\":76269},{\"end\":76797,\"start\":76789},{\"end\":76809,\"start\":76803},{\"end\":76822,\"start\":76813},{\"end\":77073,\"start\":77067},{\"end\":77083,\"start\":77077},{\"end\":77094,\"start\":77087},{\"end\":77104,\"start\":77098},{\"end\":77415,\"start\":77409},{\"end\":77423,\"start\":77419},{\"end\":77436,\"start\":77429},{\"end\":77460,\"start\":77446},{\"end\":77474,\"start\":77464},{\"end\":77489,\"start\":77478},{\"end\":77496,\"start\":77493},{\"end\":77505,\"start\":77502},{\"end\":78289,\"start\":78282},{\"end\":78297,\"start\":78293},{\"end\":78306,\"start\":78301},{\"end\":78314,\"start\":78308}]", "bib_entry": "[{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b0\",\"matched_paper_id\":3473997},\"end\":74173,\"start\":73437},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":218494668},\"end\":74494,\"start\":74175},{\"attributes\":{\"id\":\"b2\"},\"end\":74735,\"start\":74496},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":202786778},\"end\":75991,\"start\":74737},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6429068},\"end\":76201,\"start\":75993},{\"attributes\":{\"doi\":\"10.1109/ICASSP40776.2020.9054377\",\"id\":\"b5\",\"matched_paper_id\":216273855},\"end\":76742,\"start\":76203},{\"attributes\":{\"doi\":\"arXiv:1912.13445\",\"id\":\"b6\"},\"end\":76982,\"start\":76744},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":218763627},\"end\":77320,\"start\":76984},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":198968245},\"end\":78228,\"start\":77322},{\"attributes\":{\"id\":\"b9\"},\"end\":78416,\"start\":78230}]", "bib_title": "[{\"end\":73498,\"start\":73437},{\"end\":74278,\"start\":74175},{\"end\":74796,\"start\":74737},{\"end\":76037,\"start\":75993},{\"end\":76257,\"start\":76203},{\"end\":77063,\"start\":76984},{\"end\":77405,\"start\":77322}]", "bib_author": "[{\"end\":73514,\"start\":73500},{\"end\":73527,\"start\":73514},{\"end\":73538,\"start\":73527},{\"end\":74289,\"start\":74280},{\"end\":74582,\"start\":74570},{\"end\":74594,\"start\":74582},{\"end\":74605,\"start\":74594},{\"end\":74609,\"start\":74605},{\"end\":74808,\"start\":74798},{\"end\":74817,\"start\":74808},{\"end\":74826,\"start\":74817},{\"end\":74835,\"start\":74826},{\"end\":74847,\"start\":74835},{\"end\":74857,\"start\":74847},{\"end\":74868,\"start\":74857},{\"end\":74875,\"start\":74868},{\"end\":74889,\"start\":74875},{\"end\":74899,\"start\":74889},{\"end\":74912,\"start\":74899},{\"end\":74920,\"start\":74912},{\"end\":74928,\"start\":74920},{\"end\":74938,\"start\":74928},{\"end\":74948,\"start\":74938},{\"end\":74958,\"start\":74948},{\"end\":74974,\"start\":74958},{\"end\":74985,\"start\":74974},{\"end\":74993,\"start\":74985},{\"end\":75000,\"start\":74993},{\"end\":75012,\"start\":75000},{\"end\":75021,\"start\":75012},{\"end\":76050,\"start\":76039},{\"end\":76063,\"start\":76050},{\"end\":76074,\"start\":76063},{\"end\":76267,\"start\":76259},{\"end\":76275,\"start\":76267},{\"end\":76799,\"start\":76787},{\"end\":76811,\"start\":76799},{\"end\":76824,\"start\":76811},{\"end\":77075,\"start\":77065},{\"end\":77085,\"start\":77075},{\"end\":77096,\"start\":77085},{\"end\":77106,\"start\":77096},{\"end\":77417,\"start\":77407},{\"end\":77425,\"start\":77417},{\"end\":77438,\"start\":77425},{\"end\":77462,\"start\":77438},{\"end\":77476,\"start\":77462},{\"end\":77491,\"start\":77476},{\"end\":77498,\"start\":77491},{\"end\":77507,\"start\":77498},{\"end\":77517,\"start\":77507},{\"end\":78291,\"start\":78280},{\"end\":78299,\"start\":78291},{\"end\":78308,\"start\":78299},{\"end\":78316,\"start\":78308}]", "bib_venue": "[{\"end\":73746,\"start\":73647},{\"end\":75247,\"start\":75228},{\"end\":76402,\"start\":76386},{\"end\":77652,\"start\":77633},{\"end\":73621,\"start\":73542},{\"end\":74320,\"start\":74289},{\"end\":74568,\"start\":74496},{\"end\":75133,\"start\":75021},{\"end\":76080,\"start\":76074},{\"end\":76384,\"start\":76307},{\"end\":76785,\"start\":76744},{\"end\":77141,\"start\":77106},{\"end\":77629,\"start\":77517},{\"end\":78278,\"start\":78230}]"}}}, "year": 2023, "month": 12, "day": 17}