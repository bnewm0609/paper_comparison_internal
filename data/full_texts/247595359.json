{"id": 247595359, "updated": "2023-10-31 00:22:23.511", "metadata": {"title": "PediCXR: An open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children", "authors": "[{\"first\":\"Hieu\",\"last\":\"Pham\",\"middle\":[\"H.\"]},{\"first\":\"Ngoc\",\"last\":\"Nguyen\",\"middle\":[\"H.\"]},{\"first\":\"Thanh\",\"last\":\"Tran\",\"middle\":[\"T.\"]},{\"first\":\"Tuan\",\"last\":\"Nguyen\",\"middle\":[\"N.\",\"M.\"]},{\"first\":\"Ha\",\"last\":\"Nguyen\",\"middle\":[\"Q.\"]}]", "venue": "Scientific Data", "journal": "Scientific Data", "publication_date": {"year": 2023, "month": 4, "day": 27}, "abstract": "Computer-aided diagnosis systems in adult chest radiography (CXR) have recently achieved great success thanks to the availability of large-scale, annotated datasets and the advent of high-performance supervised learning algorithms. However, the development of diagnostic models for detecting and diagnosing pediatric diseases in CXR scans is undertaken due to the lack of high-quality physician-annotated datasets. To overcome this challenge, we introduce and release PediCXR, a new pediatric CXR dataset of 9,125 studies retrospectively collected from a major pediatric hospital in Vietnam between 2020 and 2021. Each scan was manually annotated by a pediatric radiologist with more than ten years of experience. The dataset was labeled for the presence of 36 critical findings and 15 diseases. In particular, each abnormal finding was identified via a rectangle bounding box on the image. To the best of our knowledge, this is the first and largest pediatric CXR dataset containing lesion-level annotations and image-level labels for the detection of multiple findings and diseases. For algorithm development, the dataset was divided into a training set of 7,728 and a test set of 1,397. To encourage new advances in pediatric CXR interpretation using data-driven approaches, we provide a detailed description of the PediCXR data sample and make the dataset publicly available on https://physionet.org/content/vindr-pcxr/1.0.0/.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2203.10612", "mag": null, "acl": null, "pubmed": "37100784", "pubmedcentral": "10133237", "dblp": null, "doi": "10.1038/s41597-023-02102-5"}}, "content": {"source": {"pdf_hash": "7b04109a96c48c6d7c8007bcbbdfaf82942e95b1", "pdf_src": "PubMedCentral", "pdf_uri": "[\"https://export.arxiv.org/pdf/2203.10612v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": null, "status": null}}, "grobid": {"id": "f0f014c4ce146ea830b8c3890bdc0a6e79c7305f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7b04109a96c48c6d7c8007bcbbdfaf82942e95b1.txt", "contents": "\nPediCXR: an open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children\n\n\nHieu H Pham \nNgoc H Nguyen \nThanh T Tran \nTuan N M Nguyen \nHa Q Nguyen \nPediCXR: an open, large-scale chest radiograph dataset for interpretation of common thoracic diseases in children\n10.1038/s41597-023-02102-51 Scientific Data |\nComputer-aided diagnosis systems in adult chest radiography (CXR) have recently achieved great success thanks to the availability of large-scale, annotated datasets and the advent of highperformance supervised learning algorithms. However, the development of diagnostic models for detecting and diagnosing pediatric diseases in CXR scans is undertaken due to the lack of high-quality physician-annotated datasets. to overcome this challenge, we introduce and release PediCXR, a new pediatric CXR dataset of 9,125 studies retrospectively collected from a major pediatric hospital in Vietnam between 2020 and 2021. Each scan was manually annotated by a pediatric radiologist with more than ten years of experience. The dataset was labeled for the presence of 36 critical findings and 15 diseases. In particular, each abnormal finding was identified via a rectangle bounding box on the image. To the best of our knowledge, this is the first and largest pediatric CXR dataset containing lesion-level annotations and image-level labels for the detection of multiple findings and diseases. For algorithm development, the dataset was divided into a training set of 7,728 and a test set of 1,397. to encourage new advances in pediatric CXR interpretation using data-driven approaches, we provide a detailed description of the PediCXR data sample and make the dataset publicly available on https://physionet.org/content/vindr-pcxr/1.0.0/.\n\nwww.nature.com/scientificdata www.nature.com/scientificdata/ In an effort to provide a large-scale pediatric CXR dataset with high-quality annotations for the research community, we have built the PediCXR dataset in DICOM format. The dataset consists of 9,125 posteroanterior (PA) view CXR scans in patients younger than 10 years that were retrospectively collected from three major hospitals in Vietnam from 2020 to 2021. In particular, all CXR scans come with both the localization of critical findings and the classification of common thoracic diseases. These images were annotated by a group of three radiologists with at least 10 years of experience for the presence of 36 critical findings (local labels) and 15 diagnoses (global labels). Here, the local labels should be annotated with rectangle bounding boxes that localize the findings, while the global labels reflect the diagnostic impression of the radiologist at the image-level. For algorithm development, we randomly divided the dataset into two parts: the training set of 7,728 scans (84.7%) and the test set of 1,397 scans (15.3%). To the best of our knowledge, the released PediCXR is currently the largest public pediatric CXR dataset with radiologist-generated annotations in both training and test sets. Table 1 below shows an overview of existing public datasets for CXR interpretation in pediatric patients, compared with the PediCXR. Compared to the previous works, the PediCXR dataset shows two main advantages. First, the dataset is labeled for multiple findings and diseases. Meanwhile, most pediatric CXR datasets have focused on a single disease such as pneumonia 19 or pneumothorax 20 . Second, the dataset provides bounding box annotations at lesion level, which is useful for developing explainable artificial intelligent models 21 for the CXR interpretation in children. We believe the introduction of the PediCXR provides a suitable imaging source for investigating the ability of supervised machine learning models in identifying common lung diseases in pediatric patients.\n\n\nMethods\n\nData collection. Data collection was conducted at the Phu Tho Obstetric & Pediatric Hospital (PTOPH) between 2020-2021. The ethical clearance of this study was approved by the Institutional Review Boards (IRBs) of the PTOPH. The need for obtaining informed patient consent was waived because this retrospective study did not impact clinical care or workflow at these two hospitals, and all patient-identifiable information in the data has been removed. We retrospectively collected more than 10,000 CXRs in DICOM format from a local picture archiving and communication system (PACS) at PTOPH. The imaging dataset was then transferred and analyzed at Smart Health Center, VinBigData JSC.\n\n\nDataset\n\nRelease year # findings # samples Image-level labels Local labels www.nature.com/scientificdata www.nature.com/scientificdata/ overview of approach. The building of the PediCXR dataset is illustrated in Fig. 1. In particular, the collection and normalization of the dataset were divided into four main steps: (1) data collection, (2) data de-identification, (3) data filtering, and (4) data labeling. We describe each step in detail as below.\n\nData de-identification. In this study, we follow the HIPAA Privacy Rule 22 to protect individually identifiable health information from the DICOM images. To this end, we removed or replaced with random values all personally identifiable information associated with the images via a two-stage de-identification process. At the first stage, a Python script was used to remove all DICOM tags of protected health information (PHI) 23 such as patient's name, patient's date of birth, patient ID, or acquisition time and date, etc. For the purpose of loading and processing DICOM files, we only retained a limited number of DICOM attributes that are necessary, as indicated in Table 2 (Supplementary materials). In the second stage, we manually removed all textual information appearing on the image data, i.e., pixel annotations that could include patient's identifiable information.\n\nData filtering. The collected raw data included a significant amount of outliers including CXRs of adult patients, body parts other than chest (abdominal, spine, and others), low-quality images, or lateral CXRs. To filter a large number of CXR scans, we trained a lightweight convolutional neural network (CNN) 24 to remove all outliers automatically. Next, a manual verification was performed to ensure all outliers had been fully removed. Data labeling. The PediCXR dataset was labeled for a total of 36 findings and 15 diagnoses. These labels were divided into two categories: local labels (#1-#36) and global labels (#37-#52). The local labels should be marked with bounding boxes that localize the findings, while the global labels should reflect the diagnostic impression of the radiologist. This list of labels was suggested by a committee of the most experienced pediatric radiologists. To select these labels, the committee took into account two key factors. First, findings and diseases are prevalent. Second, they can be differentiated on pediatric chest X-ray scans. Figure 2 illustrates several samples with both local and global labels annotated by our radiologists.  www.nature.com/scientificdata www.nature.com/scientificdata/ To facilitate the labeling process, we designed and built a web-based framework called VinDr Lab 25 that allows a team of experienced radiologists remotely annotate the data. Specifically, this is a web-based labeling tool that was developed to store, manage, and remotely annotate DICOM data. The radiologists were oriented to locate the abnormal findings from the DICOM viewer and draw the bounding boxes. All the annotators have been well-trained to ensure that the annotations are consistently annotated. In addition, all the radiologists participating in the labeling process were certified in diagnostic radiology and received healthcare professional certificates. In total, three pediatric radiologists with at least 15 years of experience were involved in the annotation process. Each sample in the training set was assigned to one radiologist for annotation. Additionally, all of the participating radiologists were blinded to relevant clinical information. A set of 9,125 pediatric CXRs were randomly annotated from the filtered data, of which 7,728 scans serve as the training set, and the remaining 1,397 studies form the test set. Note the 9,125 studies correspond to 9,125 patients, and each study has a single CXR scan.\n\nOnce the labeling was completed, the annotations of all pediatric CXRs were exported in JavaScript Object Notation (JSON) format. We developed a Python script to parse JSON files and organized the annotations in the form of a single comma-separated values (CSV) file. Each CSV file contains labels, bounding box coordinates, and their corresponding image identifiers (IDs). The data characteristics, including patient demographic and the prevalence of each finding or disease, are summarized in Table 3. The distributions of abnormal findings and pathologies in the training set are drawn in Figs. 3, 4, respectively.\n\n\nData Records\n\nThe PediCXR dataset will be made available for public download on PhysioNet 26 . We offer complete imaging data as well as ground truth labels for both the training and test datasets. The pediatric scans were split into two folders: one for training and one for testing, named as \"train\" and \"test\", respectively. Since each study has only one instance and each patient has maximum one study, therefore, the value of the SOP Instance UID provided by the DICOM tag (0008,0018) was encoded into a unique, anonymous identifier for each image. To this end, we used the Python hashlib module (see Code Availability) to encode the SOP Instance UIDs into image IDs. The radiologists' local annotations of the training set were provided in a CSV file called anno-tations_train.csv. Each row of the CSV file represents a bounding box annotation with the following attributes: image ID (image_id), radiologist ID (rad_id), label's name (class_name), bounding box coordinates (x_min, y_min, x_max, y_max), and label class ID (class_id). The coordinates of the box's upper-left corner are (x_min, y_min), and the coordinates of the box's lower right corner are (x_ max, y_max). Meanwhile, the image-level labels of the training set were stored in a different CSV file called image_labels_train.csv, with the following fields: Image ID (image_id), radiologist ID (rad_ID), and labels (labels) for both the findings and diagnoses. Each image ID is associated with a vector of multiple labels corresponding to different pathologies, with positive pathologies encoded as \"1\" and negative pathologies encoded as \"0\". Similarly, the test set's bounding-box annotations and image-level labels were saved in the files annotations_test.csv and image_labels_test.csv, respectively.     www.nature.com/scientificdata www.nature.com/scientificdata/\n\n\ntechnical Validation\n\nThe data de-identification process was controlled. Specifically, all DICOM meta-data was parsed and manually reviewed to ensure that all individually identifiable health information (PHI) 23 of the children patients has been removed to meet the U.S. HIPAA 22 regulations. In addition, pixel values of all pediatric CXR scans were also carefully examined by human readers. During this review process, all scans were manually reviewed case-by-case by a team of 10 human readers. A small number of images containing private textual information that had not been removed by our algorithm was excluded from the dataset. The manual review process also helped identify and discard out-of-distribution samples such as CXRs of adult patients, body parts other than the chest, low-quality images, or lateral CXRs that our machine learning classifier was not able to detect. A set of rules underlying our web-based annotation tool were developed to control the quality of the labeling process. These rules prevent human annotators from mechanical mistakes like forgetting to choose global labels or marking lesions on the image while choosing \"No finding\" as the global label.\n\n\nUsage Notes\n\nThe PediCXR dataset was established for the purpose of developing and evaluating machine learning algorithms for detecting and localizing anomalies in pediatric CXR images. The dataset has been previously used in a study on the diagnosis of multiple diseases in pediatric patients 27 and showed promising results. Specifically, the authors 27 introduced a deep learning network to detect common pulmonary pathologies on CXR of pediatric patients. On the test set of 777 studies of the PediCXR dataset, the network yielded an area under the receiver operating characteristic (AUC) of 0.709 (95% CI, 0.690-0.729). The sensitivity, specificity, and F1-score at the cutoff value are 0.722 (0.694-0.750), 0.579 (0.563-0.595), and 0.389 (0.373-0.405), respectively. However, they recognized that its performance remains low compared to medical experts. This work revealed the major challenge in learning disease features on pediatric CXR images using representation learning techniques, opening huge aspects for future research.\n\nThe primary uses for which the PediCXR dataset was conceptualized include:\n\n\u2022 Developing and validating a predictive model for the classification of common thoracic diseases in pediatric patients. \u2022 Developing and validating a predictive model for the localization of multiple abnormal findings on the pediatric chest X-ray scans.  www.nature.com/scientificdata www.nature.com/scientificdata/ Finally, the released dataset remains with limitations that still need to be addressed in the future, including:\n\n\u2022 The dataset did not contain clinical information associated with DICOM images, which is essential for the interpretation of CXR in children patients. \u2022 The number of examples for rare diseases (e.g., Congenital pulmonary airway malformation (CPAM), Congenital emphysema, Diagphramatic hernia, Mediastinal tumor, Pleuro-pneumonia, Situs inversus, Lung tumor) or findings (Emphysema, Edema, Calcification, Chest wall mass, Bronchectasis, Pleural thickening, Clavicle fracture, Pleuropulmonary mass, Paraveterbral mass, etc.) are limited. Hence, training supervised learning algorithms, which requires a large-scale annotated dataset, on the PediCXR dataset to diagnose the rare diseases and findings is not reliable.\n\nTo download and use the PediCXR, users are required to accept the https://physionet.org/content/ mimic-cxr/view-license/2.0.0/PhysioNet Credentialed Health Data License 1.5.0. By accepting this license, users agree that they will not share access to the dataset with anyone else. For any publication that explores this resource, the authors must cite this original paper and release their code and models.\n\nFig. 2\n2Several examples of pediatric CXR images with radiologist's annotations. Local labels marked by radiologists are plotted on the original images for visualization purposes. These annotations show abnormal findings from the scans. The global labels, that classify images into diseases, are in bold and listed at the bottom of each example. (2023) 10:240 | https://doi.org/10.1038/s41597-023-02102-5\n\nFig. 3\n3Distribution of abnormal findings on the training set of PediCXR. Rare findings (less than 10 examples) are not included.\n\nFig. 4\n4Distribution of pathologies on the training set of PediCXR. Rare diseases (less than 10 examples) are not included.\n\nTable 1 .\n1An overview of existing public datasets for CXR interpretation in pediatric patients.Fig. 1Construction of the PediCXR dataset. First, raw pediatric scans in DICOM format were collected retrospectively from the hospital's PACS at PTOPH. These images were de-identified to protect patient's privacy. Then, invalid files (including adult CXR images, images of other modalities or other body parts, images with lowKermany et al. 19 \n2018 \n2 \n5,856 \nAvailable \nNot available \n\nChen et al. 20 \n2020 \n5 \n2,668 \nAvailable \nNot available \n\nPediCXR (ours) \n2021 \n52 \n9,125 \nAvailable \nAvailable \n\n\n\n\nRatio of the vertical size and horizontal size of the pixels in the image specified by a pair of integer values where the first value is the vertical pixel size, and the second value is the horizontal pixel size. The value b in relationship between stored values (SV) and the output units specified in Rescale Type (0028,1054). Each output unit is equal to m*SV + b. Specifies whether an image has undergone lossy compression (at a point in its lifetime).DICOM Tag Attribute Name \nDescription \n\n(0010, 0040) \nPatient's Sex \nSex of the named patient. \n\n(0010, 1010) \nPatient's Age \nAge of the patient. \n\n(0010, 1020) \nPatient's Size \nLength or size of the patient, in meters. \n\n(0010, 1030) \nPatient's Weight \nWeight of the patient, in kilograms. \n\n(0028, 0010) \nRows \nNumber of rows in the image. \n\n(0028, 0011) \nColumns \nNumber of columns in the image. \n\n(0028, 0030) \nPixel Spacing \nPhysical distance in the patient between the center of each pixel, specified by a numeric \npair -adjacent row spacing (delimiter) adjacent column spacing in mm. \n\n(0028, 0034) \nPixel Aspect Ratio \n\n(0028, 0100) \nBits Allocated \nNumber of bits allocated for each pixel sample. Each sample shall have the same number of \nbits allocated. \n\n(0028, 0101) \nBits Stored \nNumber of bits stored for each pixel sample. Each sample shall have the same number of \nbits stored. \n\n(0028, 0102) \nHigh Bit \nMost significant bit for pixel sample data. Each sample shall have the same high bit. \n\n(0028, 0103) \nPixel Representation \nData representation of the pixel samples. Each sample shall have the same pixel \nrepresentation. \n\n(0028, 0106) \nSmallest Image Pixel Value \nThe minimum actual pixel value encountered in this image. \n\n(0028, 0107) \nLargest Image Pixel Value \nThe maximum actual pixel value encountered in this image. \n\n(0028, 1050) \nWindow Center \nWindow center for display. \n\n(0028, 1051) \nWindow Width \nWindow width for display. \n\n(0028, 1052) \nRescale Intercept \n(0028, 1053) \nRescale Slope \nValue of m in the equation specified by Rescale Intercept (0028,1052). \n\n(7FE0, 0010) Pixel Data \nA data stream of the pixel samples that comprise the image. \n\n(0028, 0004) \nPhotometric Interpretation \nSpecifies the intended interpretation of the pixel data. \n\n(0028, 2110) \nLossy Image Compression \n(0028, 2114) \nLossy Image Compression \nMethod \nA label for the lossy compression method(s) that have been applied to this image. \n\n(0028, 2112) \nImage Compression Ratio \nDescribes the approximate lossy compression ratio(s) that have been applied to this image. \n\n(0028, 0002) \nSamples per Pixel \nNumber of samples (planes) in this image. \n\n(0028, 0008) \nNumber of Frames \nNumber of frames in a multi-frame image. \n\n\n\nTable 2 .\n2The list of DICOM tags that were retained for loading and processing raw images. All other tags were removed for protecting patient privacy. Details about all these tags can be found from DICOM Standard Browser at https://dicom.innolitics.com/ciods.\n\nTable 3 .\n3Dataset characteristics of PediCXR.\nScientific Data | (2023) 10:240 | https://doi.org/10.1038/s41597-023-02102-5\nwww.nature.com/scientificdata www.nature.com/scientificdata/\n\u00a9 The Author(s) 2023\nCode availabilityThis study used the following open-source repositories to load and process DICOM scans: Python 3.7.0 (https:// www.python.org/); Pydicom 1.2.0 (https://pydicom.github.io/); OpenCV-Python 4.2.0.34 (https://pypi.org/project/ opencv-python/); and Python hashlib (https://docs.python.org/3/library/hashlib.html). The code for data deidentification was made publicly available at https://github.com/vinbigdata-medical/vindr-cxr. The code to train CNN classifier for the out-of-distribution task was made publicly available at https://github.com/vinbigdata-medical/ DICOM-Imaging-Router. The VinDr Lab is an open source software and can be found at https://vindr.ai/vindr-lab.acknowledgementsThe collection of this dataset was funded by the Smart Health Center, VinBigData JSC. The authors would like to acknowledge the Phu Tho Obstetric & Pediatric Hospital for agreeing to make the PediCXR dataset publicly available. We are especially thankful to Anh T. Nguyen, Huong T.T. Nguyen, Ngan T.T. Nguyen for their helps in the data collection and labeling process.Competing interestsThis work was funded by the Vingroup JSC. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.additional informationCorrespondence and requests for materials should be addressed to H.H.P.Reprints and permissions information is available at www.nature.com/reprints.Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Open Access This article is licensed under a Creative Commons Attribution 4.0 InternationalLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\nEstimates of the global, regional, and national morbidity, mortality, and aetiologies of lower respiratory tract infections in 195 countries: a systematic analysis for the global burden of disease study. G L Collaborators, The Lancet Infect. Dis. 17Collaborators, G. L. Estimates of the global, regional, and national morbidity, mortality, and aetiologies of lower respiratory tract infections in 195 countries: a systematic analysis for the global burden of disease study 2015. The Lancet Infect. Dis. 17, 1133-1161 (2017).\n\nT M Wardlaw, E W Johansson, M Hodge, W H Organization, U N C F Unicef), Pneumonia, The forgotten killer of children. Wardlaw, T. M., Johansson, E. W., Hodge, M., Organization, W. H. & (UNICEF), U. N. C. F. Pneumonia: The forgotten killer of children (2006).\n\nPediatric Chest Disorders: Practical Imaging Approach to Diagnosis. A Hart, E Y Lee, Dis. Chest, Breast, Hear. Vessel. Hart, A. & Lee, E. Y. Pediatric Chest Disorders: Practical Imaging Approach to Diagnosis. Dis. Chest, Breast, Hear. Vessel. 2019-2022 107-125 (2019).\n\n. Chest. Chest radiograph (pediatric). https://radiopaedia.org/articles/chest-radiograph-paediatric. Accessed: 2021-09-24.\n\nObserver variation in detecting lymphadenopathy on chest radiography. G Du Toit, G Swingler, K Iloni, Int. J. Tuberc. Lung Dis. 6Du Toit, G., Swingler, G. & Iloni, K. Observer variation in detecting lymphadenopathy on chest radiography. Int. J. Tuberc. Lung Dis. 6, 814-817 (2002).\n\nChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. X Wang, 10.1109/CVPR.2017.369Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Wang, X. et al. ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2097-2106, https://doi.org/10.1109/CVPR.2017.369 (2017).\n\nA Bustos, A Pertusa, J.-M Salinas, M De La Iglesia-Vay\u00e1, Padchest, arXiv:1901.07441A large chest X-ray image dataset with multi-label annotated reports. arXiv preprintBustos, A., Pertusa, A., Salinas, J.-M. & de la Iglesia-Vay\u00e1, M. Padchest: A large chest X-ray image dataset with multi-label annotated reports. arXiv preprint arXiv:1901.07441 (2019).\n\nCheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. J Irvin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Irvin, J. et al. CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI Conference on Artificial Intelligence 33, 590-597 (2019).\n\nMIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. A E Johnson, 10.1038/s41597-019-0322-0Sci. Data. 6Johnson, A. E. et al. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. Sci. Data 6, 317, https://doi.org/10.1038/s41597-019-0322-0 (2019).\n\nVinDr-CXR: An open dataset of chest X-rays with radiologist's annotations. H Q Nguyen, Sci. Data. 9429Nguyen, H. Q. et al. VinDr-CXR: An open dataset of chest X-rays with radiologist's annotations. Sci. Data 9, 429 (2022).\n\nTwo public chest X-ray datasets for computer-aided screening of pulmonary diseases. S Jaeger, 10.3978/j.issn.2223-4292.2014.11.20Quant. Imaging Medicine Surg. 4Jaeger, S. et al. Two public chest X-ray datasets for computer-aided screening of pulmonary diseases. Quant. Imaging Medicine Surg. 4, 475-477, https://doi.org/10.3978/j.issn.2223-4292.2014.11.20 (2014).\n\nCOVIDGR dataset and COVID-SDNet methodology for predicting COVID-19 based on chest X-ray images. S Tabik, IEEE journal biomedical health informatics. 24Tabik, S. et al. COVIDGR dataset and COVID-SDNet methodology for predicting COVID-19 based on chest X-ray images. IEEE journal biomedical health informatics 24, 3595-3605 (2020).\n\nP Rajpurkar, arXiv:1711.05225Radiologist-level pneumonia detection on chest X-rays with deep learning. arXiv preprintRajpurkar, P. et al. CheXNet: Radiologist-level pneumonia detection on chest X-rays with deep learning. arXiv preprint arXiv:1711.05225 (2017).\n\nDeep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists. P Rajpurkar, 10.1371/journal.pmed.1002686PLoS Medicine. 15Rajpurkar, P. et al. Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists. PLoS Medicine 15, e1002686, https://doi.org/10.1371/journal.pmed.1002686 (2018).\n\nChest radiograph interpretation with deep learning models: Assessment with radiologist adjudicated reference standards and population-adjusted evaluation. A Majkowska, 10.1148/radiol.2019191293Radiology. 294Majkowska, A. et al. Chest radiograph interpretation with deep learning models: Assessment with radiologist adjudicated reference standards and population-adjusted evaluation. Radiology 294, 421-431, https://doi.org/10.1148/radiol.2019191293 (2020).\n\nCheXpedition: Investigating generalization challenges for translation of chest X-ray algorithms to the clinical setting. P Rajpurkar, arXiv:2002.11379arXiv preprintRajpurkar, P. et al. CheXpedition: Investigating generalization challenges for translation of chest X-ray algorithms to the clinical setting. arXiv preprint arXiv:2002.11379 (2020).\n\nAutomated abnormality classification of chest radiographs using deep convolutional neural networks. npj Digit. Y.-X Tang, 10.1038/s41746-020-0273-zMedicine. 3Tang, Y.-X. et al. Automated abnormality classification of chest radiographs using deep convolutional neural networks. npj Digit. Medicine 3, 1-8, https://doi.org/10.1038/s41746-020-0273-z (2020).\n\nInterpreting chest X-rays via CNNs that exploit hierarchical disease dependencies and uncertainty labels. H H Pham, T T Le, D Q Tran, D T Ngo, H Q Nguyen, Neurocomputing. 437Pham, H. H., Le, T. T., Tran, D. Q., Ngo, D. T. & Nguyen, H. Q. Interpreting chest X-rays via CNNs that exploit hierarchical disease dependencies and uncertainty labels. Neurocomputing 437, 186-194 (2021).\n\nIdentifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. D S Kermany, 10.1016/j.cell.2018.02.010Cell. 172Kermany, D. S. et al. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell 172, 1122-1131.e9, https://doi.org/10.1016/j.cell.2018.02.010 (2018).\n\nDiagnosis of common pulmonary diseases in children by X-ray images and deep learning. K.-C Chen, Sci. Reports. 10Chen, K.-C. et al. Diagnosis of common pulmonary diseases in children by X-ray images and deep learning. Sci. Reports 10, 1-9 (2020).\n\nExplainable artificial intelligence for safe intraoperative decision support. L Gordon, T Grantcharov, F Rudzicz, JAMA surgery. 154Gordon, L., Grantcharov, T. & Rudzicz, F. Explainable artificial intelligence for safe intraoperative decision support. JAMA surgery 154, 1064-1065 (2019).\n\nSummary of the HIPAA privacy rule. US Department of Health and Human ServicesUS Department of Health and Human Services. Summary of the HIPAA privacy rule. https://www.hhs.gov/hipaa/for-professionals/ privacy/laws-regulations/index.html (2003).\n\nProtected Health Information (PHI). S Isola, Y Al Khalili, Isola, S. & Al Khalili, Y. Protected Health Information (PHI). https://www.ncbi.nlm.nih.gov/books/NBK553131/ (2019).\n\nH H Pham, D V Do, H Q Nguyen, Dicom Imaging, Router, arXiv:2108.06490An Open Deep Learning Framework for Classification of Body Parts from DICOM X-ray Scans. arXiv preprintPham, H. H., Do, D. V. & Nguyen, H. Q. DICOM Imaging Router: An Open Deep Learning Framework for Classification of Body Parts from DICOM X-ray Scans. arXiv preprint arXiv:2108.06490 (2021).\n\nVinDr Lab: A Data Platform for Medical AI. N T Nguyen, Nguyen, N. T. et al. VinDr Lab: A Data Platform for Medical AI. URL: https://github.com/vinbigdata-medical/vindr-lab (2021).\n\nPediCXR: An open, large-scale pediatric chest X-ray dataset for interpretation of common thoracic diseases (version 1.0.0). H H Pham, T T Tran, H Q Nguyen, 10.13026/k8qc-na36PhysioNet. Pham, H. H., Tran, T. T. & Nguyen, H. Q. PediCXR: An open, large-scale pediatric chest X-ray dataset for interpretation of common thoracic diseases (version 1.0.0). PhysioNet https://doi.org/10.13026/k8qc-na36 (2022).\n\nLearning to automatically diagnose multiple diseases in pediatric chest radiographs using deep convolutional neural networks. T T Tran, IEEE Conference on Computer Vision and Pattern Recognition Workshop (ICCV 2021. Tran, T. T. et al. Learning to automatically diagnose multiple diseases in pediatric chest radiographs using deep convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recognition Workshop (ICCV 2021) (2021).\n", "annotations": {"author": "[{\"end\":129,\"start\":117},{\"end\":144,\"start\":130},{\"end\":158,\"start\":145},{\"end\":175,\"start\":159},{\"end\":188,\"start\":176}]", "publisher": null, "author_last_name": "[{\"end\":128,\"start\":124},{\"end\":143,\"start\":137},{\"end\":157,\"start\":153},{\"end\":174,\"start\":168},{\"end\":187,\"start\":181}]", "author_first_name": "[{\"end\":121,\"start\":117},{\"end\":123,\"start\":122},{\"end\":134,\"start\":130},{\"end\":136,\"start\":135},{\"end\":150,\"start\":145},{\"end\":152,\"start\":151},{\"end\":163,\"start\":159},{\"end\":167,\"start\":164},{\"end\":178,\"start\":176},{\"end\":180,\"start\":179}]", "author_affiliation": null, "title": "[{\"end\":114,\"start\":1},{\"end\":302,\"start\":189}]", "venue": null, "abstract": "[{\"end\":1778,\"start\":349}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3425,\"start\":3423},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3444,\"start\":3442},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3593,\"start\":3591},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5421,\"start\":5419},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6185,\"start\":6183},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9063,\"start\":9061},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11024,\"start\":11022},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11092,\"start\":11090},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12298,\"start\":12296}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":15075,\"start\":14670},{\"attributes\":{\"id\":\"fig_2\"},\"end\":15206,\"start\":15076},{\"attributes\":{\"id\":\"fig_3\"},\"end\":15331,\"start\":15207},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":15932,\"start\":15332},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":18628,\"start\":15933},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":18890,\"start\":18629},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":18938,\"start\":18891}]", "paragraph": "[{\"end\":3838,\"start\":1780},{\"end\":4536,\"start\":3850},{\"end\":4990,\"start\":4548},{\"end\":5870,\"start\":4992},{\"end\":8349,\"start\":5872},{\"end\":8968,\"start\":8351},{\"end\":10809,\"start\":8985},{\"end\":11999,\"start\":10834},{\"end\":13037,\"start\":12015},{\"end\":13113,\"start\":13039},{\"end\":13544,\"start\":13115},{\"end\":14262,\"start\":13546},{\"end\":14669,\"start\":14264}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":3062,\"start\":3055},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":5670,\"start\":5663},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":8853,\"start\":8846}]", "section_header": "[{\"end\":3848,\"start\":3841},{\"end\":4546,\"start\":4539},{\"end\":8983,\"start\":8971},{\"end\":10832,\"start\":10812},{\"end\":12013,\"start\":12002},{\"end\":14677,\"start\":14671},{\"end\":15083,\"start\":15077},{\"end\":15214,\"start\":15208},{\"end\":15342,\"start\":15333},{\"end\":18639,\"start\":18630},{\"end\":18901,\"start\":18892}]", "table": "[{\"end\":15932,\"start\":15755},{\"end\":18628,\"start\":16390}]", "figure_caption": "[{\"end\":15075,\"start\":14679},{\"end\":15206,\"start\":15085},{\"end\":15331,\"start\":15216},{\"end\":15755,\"start\":15344},{\"end\":16390,\"start\":15935},{\"end\":18890,\"start\":18641},{\"end\":18938,\"start\":18903}]", "figure_ref": "[{\"end\":4757,\"start\":4751},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6959,\"start\":6951}]", "bib_author_first_name": "[{\"end\":21715,\"start\":21714},{\"end\":21717,\"start\":21716},{\"end\":22037,\"start\":22036},{\"end\":22039,\"start\":22038},{\"end\":22050,\"start\":22049},{\"end\":22052,\"start\":22051},{\"end\":22065,\"start\":22064},{\"end\":22074,\"start\":22073},{\"end\":22076,\"start\":22075},{\"end\":22092,\"start\":22091},{\"end\":22098,\"start\":22093},{\"end\":22364,\"start\":22363},{\"end\":22372,\"start\":22371},{\"end\":22374,\"start\":22373},{\"end\":22760,\"start\":22759},{\"end\":22771,\"start\":22770},{\"end\":22783,\"start\":22782},{\"end\":23117,\"start\":23116},{\"end\":23608,\"start\":23607},{\"end\":23618,\"start\":23617},{\"end\":23632,\"start\":23628},{\"end\":23643,\"start\":23642},{\"end\":24051,\"start\":24050},{\"end\":24464,\"start\":24463},{\"end\":24466,\"start\":24465},{\"end\":24780,\"start\":24779},{\"end\":24782,\"start\":24781},{\"end\":25013,\"start\":25012},{\"end\":25391,\"start\":25390},{\"end\":25626,\"start\":25625},{\"end\":26015,\"start\":26014},{\"end\":26458,\"start\":26457},{\"end\":26882,\"start\":26881},{\"end\":27222,\"start\":27218},{\"end\":27570,\"start\":27569},{\"end\":27572,\"start\":27571},{\"end\":27580,\"start\":27579},{\"end\":27582,\"start\":27581},{\"end\":27588,\"start\":27587},{\"end\":27590,\"start\":27589},{\"end\":27598,\"start\":27597},{\"end\":27600,\"start\":27599},{\"end\":27607,\"start\":27606},{\"end\":27609,\"start\":27608},{\"end\":27928,\"start\":27927},{\"end\":27930,\"start\":27929},{\"end\":28246,\"start\":28242},{\"end\":28483,\"start\":28482},{\"end\":28493,\"start\":28492},{\"end\":28508,\"start\":28507},{\"end\":28975,\"start\":28974},{\"end\":28984,\"start\":28983},{\"end\":29116,\"start\":29115},{\"end\":29118,\"start\":29117},{\"end\":29126,\"start\":29125},{\"end\":29128,\"start\":29127},{\"end\":29134,\"start\":29133},{\"end\":29136,\"start\":29135},{\"end\":29522,\"start\":29521},{\"end\":29524,\"start\":29523},{\"end\":29784,\"start\":29783},{\"end\":29786,\"start\":29785},{\"end\":29794,\"start\":29793},{\"end\":29796,\"start\":29795},{\"end\":29804,\"start\":29803},{\"end\":29806,\"start\":29805},{\"end\":30190,\"start\":30189},{\"end\":30192,\"start\":30191}]", "bib_author_last_name": "[{\"end\":21731,\"start\":21718},{\"end\":22047,\"start\":22040},{\"end\":22062,\"start\":22053},{\"end\":22071,\"start\":22066},{\"end\":22089,\"start\":22077},{\"end\":22106,\"start\":22099},{\"end\":22117,\"start\":22108},{\"end\":22369,\"start\":22365},{\"end\":22378,\"start\":22375},{\"end\":22768,\"start\":22761},{\"end\":22780,\"start\":22772},{\"end\":22789,\"start\":22784},{\"end\":23122,\"start\":23118},{\"end\":23615,\"start\":23609},{\"end\":23626,\"start\":23619},{\"end\":23640,\"start\":23633},{\"end\":23662,\"start\":23644},{\"end\":23672,\"start\":23664},{\"end\":24057,\"start\":24052},{\"end\":24474,\"start\":24467},{\"end\":24789,\"start\":24783},{\"end\":25020,\"start\":25014},{\"end\":25397,\"start\":25392},{\"end\":25636,\"start\":25627},{\"end\":26025,\"start\":26016},{\"end\":26468,\"start\":26459},{\"end\":26892,\"start\":26883},{\"end\":27227,\"start\":27223},{\"end\":27577,\"start\":27573},{\"end\":27585,\"start\":27583},{\"end\":27595,\"start\":27591},{\"end\":27604,\"start\":27601},{\"end\":27616,\"start\":27610},{\"end\":27938,\"start\":27931},{\"end\":28251,\"start\":28247},{\"end\":28490,\"start\":28484},{\"end\":28505,\"start\":28494},{\"end\":28516,\"start\":28509},{\"end\":28981,\"start\":28976},{\"end\":28995,\"start\":28985},{\"end\":29123,\"start\":29119},{\"end\":29131,\"start\":29129},{\"end\":29143,\"start\":29137},{\"end\":29158,\"start\":29145},{\"end\":29166,\"start\":29160},{\"end\":29531,\"start\":29525},{\"end\":29791,\"start\":29787},{\"end\":29801,\"start\":29797},{\"end\":29813,\"start\":29807},{\"end\":30197,\"start\":30193}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2494545},\"end\":22034,\"start\":21510},{\"attributes\":{\"id\":\"b1\"},\"end\":22293,\"start\":22036},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":86655847},\"end\":22563,\"start\":22295},{\"attributes\":{\"id\":\"b3\"},\"end\":22687,\"start\":22565},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":30741122},\"end\":22970,\"start\":22689},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.369\",\"id\":\"b5\",\"matched_paper_id\":8945673},\"end\":23605,\"start\":22972},{\"attributes\":{\"doi\":\"arXiv:1901.07441\",\"id\":\"b6\"},\"end\":23958,\"start\":23607},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":58981871},\"end\":24361,\"start\":23960},{\"attributes\":{\"doi\":\"10.1038/s41597-019-0322-0\",\"id\":\"b8\",\"matched_paper_id\":209342303},\"end\":24702,\"start\":24363},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":229923729},\"end\":24926,\"start\":24704},{\"attributes\":{\"doi\":\"10.3978/j.issn.2223-4292.2014.11.20\",\"id\":\"b10\",\"matched_paper_id\":24215976},\"end\":25291,\"start\":24928},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":219179286},\"end\":25623,\"start\":25293},{\"attributes\":{\"doi\":\"arXiv:1711.05225\",\"id\":\"b12\"},\"end\":25885,\"start\":25625},{\"attributes\":{\"doi\":\"10.1371/journal.pmed.1002686\",\"id\":\"b13\",\"matched_paper_id\":53944752},\"end\":26300,\"start\":25887},{\"attributes\":{\"doi\":\"10.1148/radiol.2019191293\",\"id\":\"b14\",\"matched_paper_id\":208611383},\"end\":26758,\"start\":26302},{\"attributes\":{\"doi\":\"arXiv:2002.11379\",\"id\":\"b15\"},\"end\":27105,\"start\":26760},{\"attributes\":{\"doi\":\"10.1038/s41746-020-0273-z\",\"id\":\"b16\",\"matched_paper_id\":218624512},\"end\":27461,\"start\":27107},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":211483063},\"end\":27842,\"start\":27463},{\"attributes\":{\"doi\":\"10.1016/j.cell.2018.02.010\",\"id\":\"b18\",\"matched_paper_id\":3516426},\"end\":28154,\"start\":27844},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":222826231},\"end\":28402,\"start\":28156},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":202556502},\"end\":28690,\"start\":28404},{\"attributes\":{\"id\":\"b21\"},\"end\":28936,\"start\":28692},{\"attributes\":{\"id\":\"b22\"},\"end\":29113,\"start\":28938},{\"attributes\":{\"doi\":\"arXiv:2108.06490\",\"id\":\"b23\"},\"end\":29476,\"start\":29115},{\"attributes\":{\"id\":\"b24\"},\"end\":29657,\"start\":29478},{\"attributes\":{\"doi\":\"10.13026/k8qc-na36\",\"id\":\"b25\"},\"end\":30061,\"start\":29659},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":236994784},\"end\":30514,\"start\":30063}]", "bib_title": "[{\"end\":21712,\"start\":21510},{\"end\":22361,\"start\":22295},{\"end\":22757,\"start\":22689},{\"end\":23114,\"start\":22972},{\"end\":24048,\"start\":23960},{\"end\":24461,\"start\":24363},{\"end\":24777,\"start\":24704},{\"end\":25010,\"start\":24928},{\"end\":25388,\"start\":25293},{\"end\":26012,\"start\":25887},{\"end\":26455,\"start\":26302},{\"end\":27216,\"start\":27107},{\"end\":27567,\"start\":27463},{\"end\":27925,\"start\":27844},{\"end\":28240,\"start\":28156},{\"end\":28480,\"start\":28404},{\"end\":29781,\"start\":29659},{\"end\":30187,\"start\":30063}]", "bib_author": "[{\"end\":21733,\"start\":21714},{\"end\":22049,\"start\":22036},{\"end\":22064,\"start\":22049},{\"end\":22073,\"start\":22064},{\"end\":22091,\"start\":22073},{\"end\":22108,\"start\":22091},{\"end\":22119,\"start\":22108},{\"end\":22371,\"start\":22363},{\"end\":22380,\"start\":22371},{\"end\":22770,\"start\":22759},{\"end\":22782,\"start\":22770},{\"end\":22791,\"start\":22782},{\"end\":23124,\"start\":23116},{\"end\":23617,\"start\":23607},{\"end\":23628,\"start\":23617},{\"end\":23642,\"start\":23628},{\"end\":23664,\"start\":23642},{\"end\":23674,\"start\":23664},{\"end\":24059,\"start\":24050},{\"end\":24476,\"start\":24463},{\"end\":24791,\"start\":24779},{\"end\":25022,\"start\":25012},{\"end\":25399,\"start\":25390},{\"end\":25638,\"start\":25625},{\"end\":26027,\"start\":26014},{\"end\":26470,\"start\":26457},{\"end\":26894,\"start\":26881},{\"end\":27229,\"start\":27218},{\"end\":27579,\"start\":27569},{\"end\":27587,\"start\":27579},{\"end\":27597,\"start\":27587},{\"end\":27606,\"start\":27597},{\"end\":27618,\"start\":27606},{\"end\":27940,\"start\":27927},{\"end\":28253,\"start\":28242},{\"end\":28492,\"start\":28482},{\"end\":28507,\"start\":28492},{\"end\":28518,\"start\":28507},{\"end\":28983,\"start\":28974},{\"end\":28997,\"start\":28983},{\"end\":29125,\"start\":29115},{\"end\":29133,\"start\":29125},{\"end\":29145,\"start\":29133},{\"end\":29160,\"start\":29145},{\"end\":29168,\"start\":29160},{\"end\":29533,\"start\":29521},{\"end\":29793,\"start\":29783},{\"end\":29803,\"start\":29793},{\"end\":29815,\"start\":29803},{\"end\":30199,\"start\":30189}]", "bib_venue": "[{\"end\":21755,\"start\":21733},{\"end\":22151,\"start\":22119},{\"end\":22412,\"start\":22380},{\"end\":22572,\"start\":22567},{\"end\":22815,\"start\":22791},{\"end\":23229,\"start\":23145},{\"end\":23758,\"start\":23690},{\"end\":24120,\"start\":24059},{\"end\":24510,\"start\":24501},{\"end\":24800,\"start\":24791},{\"end\":25085,\"start\":25057},{\"end\":25441,\"start\":25399},{\"end\":25726,\"start\":25654},{\"end\":26068,\"start\":26055},{\"end\":26504,\"start\":26495},{\"end\":26879,\"start\":26760},{\"end\":27262,\"start\":27254},{\"end\":27632,\"start\":27618},{\"end\":27970,\"start\":27966},{\"end\":28265,\"start\":28253},{\"end\":28530,\"start\":28518},{\"end\":28725,\"start\":28692},{\"end\":28972,\"start\":28938},{\"end\":29271,\"start\":29184},{\"end\":29519,\"start\":29478},{\"end\":29842,\"start\":29833},{\"end\":30277,\"start\":30199},{\"end\":23300,\"start\":23231},{\"end\":24168,\"start\":24122}]"}}}, "year": 2023, "month": 12, "day": 17}