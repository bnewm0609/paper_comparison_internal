{"id": 11337237, "updated": "2023-04-05 07:57:46.714", "metadata": {"title": "Planning as heuristi sear h", "authors": "[{\"first\":\"Blai\",\"last\":\"Bonet\",\"middle\":[]},{\"first\":\"Hector\",\"last\":\"Geffner\",\"middle\":[]}]", "venue": "Artif. Intell.", "journal": "Artif. Intell.", "publication_date": {"year": 2001, "month": null, "day": null}, "abstract": "In the AIPS98 Planning Contest, the HSP planner showed that heuristic search planners can be competitive with state-of-the-art Graphplan and SAT planners. Heuristic search planners like HSP transform planning problems into problems of heuristic search by automatically extracting heuristics from Strips encodings. They differ from specialized problem solvers such as those developed for the 24-Puzzle and Rubik\u2019s Cube in that they use a general declarative language for stating problems and a general mechanism for extracting heuristics from these representations. In this paper, we study a family of heuristic search planners that are based on a simple and general heuristic that assumes that action preconditions are independent. The heuristic is then used in the context of best-first and hill-climbing search algorithms, and is tested over a large collection of domains. We then consider variations and extensions such as reversing the direction of the search for speeding node evaluation, and extracting information about propositional invariants for avoiding dead-ends. We analyze the resulting planners, evaluate their performance, and explain when they do best. We also compare the performance of these planners with two state-of-the-art planners, and show that the simplest planner based on a pure best-first search yields the most solid performance over a large set of problems. We also discuss the strengths and limitations of this approach, establish a correspondence between heuristic search planning and Graphplan, and briefly survey recent ideas that can reduce the current gap in performance between general heuristic search planners and specialized solvers. \uf6d9 2001 Elsevier Science B.V. All rights reserved.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2122054842", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ai/BonetG01", "doi": "10.1016/s0004-3702(01)00108-4"}}, "content": {"source": {"pdf_hash": "7e6f0ee77e1e489c8eeac943df3a5443663ccbd4", "pdf_src": "CiteSeerX", "pdf_uri": "[\"http://www.ldc.usb.ve/~hector/reports/hsp-aij.ps\"]", "oa_url_match": false, "oa_info": {"license": "elsevier-specific: oa user license", "open_access_url": "https://doi.org/10.1016/s0004-3702(01)00108-4", "status": "BRONZE"}}, "grobid": {"id": "5290a2d26c7db6673c32ce04ed11fef86a6158e3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7e6f0ee77e1e489c8eeac943df3a5443663ccbd4.txt", "contents": "\nPlanning as Heuristic Search\n\n\nBlai Bonet fbonet@usb.ve \nDepto. de Computaci on Universidad Sim on Bol var\nAptdo. 890001080-ACaracasVenezuela\n\nH Ector Ge \nDepto. de Computaci on Universidad Sim on Bol var\nAptdo. 890001080-ACaracasVenezuela\n\nPlanning as Heuristic Search\n\nIn the AIPS98 Planning Contest, the hsp planner showed that heuristic search planners can be competitive with state of the art Graphplan and sat planners. Heuristic search planners like hsp transform planning problems into problems of heuristic search by automatically extracting heuristics from Strips encodings. They di er from specialized problem solvers such as those developed for the 24-Puzzle and Rubik's cube in that they use a general declarative language for stating problems and a general mechanism for extracting heuristics from these representations.In this paper, we study a family of heuristic search planners that are based on a simple and general heuristic that assumes that action preconditions are independent. The heuristic is then used in the context of best-rst and hill-climbing search algorithms, and is tested over a large collection of domains. We then consider variations and extensions such as reversing the direction of the search for speeding node evaluation, and extracting information about propositional invariants for avoiding dead-ends. We analyze the resulting planners, evaluate their performance, and explain when they do best. We also compare the performance of these planners with two state of the art planners, and show that the simplest planner based on a pure best-rst search yields the most solid performance over a large set of problems. We also discuss the strengths and limitations of this approach, establish a correspondence between heuristic search planning and Graphplan, and brie y survey recent ideas that can reduce the current gap in performance between general heuristic-search planners and specialized solvers.\n\nIntroduction\n\nThe last few years have seen a number of promising new approaches in Planning. Most prominent among these are Graphplan BF97] and Satplan KS96]. Both work in stages by building suitable structures and then searching them for solutions. In Graphplan, the structure is a graph, while in Satplan, it is a set of clauses. Both planners have shown impressive performance and have attracted a good deal of attention. Recent implementations and signi cant extensions have been reported in KNHD97,LF99,KS99,ASW98].\n\nIn the recent AIPS98 Planning Competition McD98a], three out of the four planners in the Strips track, ipp KNHD97], stan LF99], and blackbox KS99], were based on these ideas. The fourth planner, hsp BG99], was based on the ideas of heuristic search Nil80,Pea83]. In hsp, the search is assumed to be similar to the search in problems like the 8-Puzzle, the main di erence being in the heuristic: while in problems like the 8-Puzzle the heuristic is normally given (e.g., as the sum of Manhattan distances), in planning it has to be extracted automatically from the declarative representation of the problem. hsp thus appeals to a simple scheme for computing the heuristic from Strips encodings and uses the heuristic to guide the search for the goal.\n\nThe idea of extracting heuristics from declarative problem representations for guiding the search in planning has been advanced recently by Drew McDermott McD96,McD99], and by Bonet, Loerincs and Ge ner BLG97]. In this paper, we extend these ideas, test them over a large number of problems and domains, and introduce a family of planners that are competitive with some of the best current planners.\n\nPlanners based on the ideas of heuristic search are related to specialized solvers such as those developed for domains like the 24-puzzle KT96], Rubik's cube Kor98], and Sokoban JS99] but di er from them mainly in the use of a general language for stating problems and a general mechanism for extracting heuristics. Heuristic search planners, as all planners, are general problem solvers in which the same code must be able to process problems from di erent domains NS63]. This generality comes normally at a price: as noted in JS99], the performance of the best current planners is still well behind the performance of specialized solvers. Closing this gap, however, is the main challenge in planning research where the ultimate goal is to have systems that combine exibility and e ciency: exibility for modeling a wide range of problems, and e ciency for obtaining good solutions fast. 1 In heuristic search planning, this challenge can only be met by the formulation, analysis, and evaluation of suitable domain-independent heuristics and optimizations. In this paper we aim to present the basic ideas and results we have obtained, and discuss more recent ideas that we nd promising. 2 More precisely, we will present a family of heuristic search planners based on a simple and general heuristic that assumes that action preconditions are independent. This heuristic is then used in the context of best-rst and hill-climbing search algorithms, and is tested over a large class of domains. We also consider variations and extensions, such as reversing the direction of the search for speeding node evaluation, and extracting information about propositional invariants for avoiding dead-ends. We compare the resulting planners with some of the best current planners and show that the simplest planner based on a pure best-rst search, yields the most solid performance over a large set of problems. We also discuss the strengths and limitations of this approach, establish a correspondence between heuristic search planning and Graphplan (a recent and popular planning framework BF97]) and brie y survey recent ideas that can help reduce the performance gap between general heuristic-search planners and specialized solvers.\n\nOur focus is on non-optimal sequential planning. This is contrast with the recent emphasis on optimal parallel planning following Graphplan and sat-based planners KS96]. Algorithms are evaluated in terms of the problems that they solve (given limited time and memory resources), and the quality of the solutions found (measured by the solution time and length). The use of heuristics for optimal sequential and parallel planning is considered in HG00] and is brie y discussed in Sect. 8.\n\nIn this paper we review and extend the ideas and results reported in BG99]. However, rather than focusing on the two speci c planners hsp and hspr, we consider and analyze a broader space of alternatives and perform a more exhaustive empirical evaluation. This more systematic study led us to revise some of the conjectures in BG99] and to understand better the strengths and limitations involved in the choice of the heuristics, the search algorithms, and the direction of the search.\n\nThe rest of the paper is organized as follows. We cover rst general state models (Sect. 2) and the state models underlying problems expressed in Strips (Sect. 3). We then present a domain independent heuristic that can be obtained from Strips encodings (Sect. 4), and use this heuristic in the context of forward and backward state planners (Sect. 5 and 6). We then consider related work (Sect. 7), summarize the main ideas and results, and discuss current open problems (Sect. 8).\n\n\nState Models\n\nState spaces provide the basic action model for problems involving deterministic actions and complete information. A state space consists of a nite set of states S, a nite set of actions A, a state transition function f that describes how actions map one state into another, and a cost function c(a; s) > 0 that measures the cost of doing action a in state s NS72,Nil80]. A state space extended with a given initial state s 0 and a set S G of goal states will be called a state model. State models are thus the models underlying most of the problems considered in heuristic search Pea83] as well as the problems that fall into the scope of classical planning Nil80]. Formally, a state model is a tuple S = hS; s 0 ; S G ; A; f; ci where S is a nite and non-empty set of states s s 0 2 S is the initial state S G S is a non-empty set of goal states A(s) A denotes the actions applicable in each state s 2 S f(a; s) denotes a state transition function for all s 2 S and a 2 A(s), and c(a; s) stands for the cost of doing action a in state s. A solution of a state model is a sequence of actions a 0 , a 1 , . .. , a n that generates a state trajectory s 0 , s 1 = f(s 0 ), . .. , s n+1 = f(s n ; a n ) such that each action a i is applicable in s i and s n+1 is a goal state, i.e., a i 2 A(s i ) and s n+1 2 G. The solution is optimal when the total cost P n i=0 c(s i ; a i ) is minimized. In problem solving, it is common to build state models adapted to the target domain by explicitly de ning the state space and explicitly coding the state transition function f(a; s) and the action applicability conditions A(s) in a suitable programming language. In planning, state models are de ned implicitly in a general declarative language that can easily accommodate representations of di erent problems. We consider next the state models underlying problems expressed in the Strips language FN71]. 3\n\n\nThe Strips State Model\n\nA planning problem in Strips is represented by a tuple P = hA; O; I; Gi where A is a set of atoms, O is a set of operators, and I A and G A encode the initial and goal situations. The operators op 2 O are all assumed grounded (i.e., with the variables replaced by constants). Each operator has a precondition, add, and delete lists denoted as Prec(op), Add(op), and Del(op) respectively. They are all given by sets of atoms from A. A Strips problem P = hA; O; I; Gi de nes a state-space S P = hS; s 0 ; S G ; A( ); f; ci where S1. the states s 2 S are collections of atoms from A S2. the initial state s 0 is I S3. the goal states s 2 S G are such that G s S4. the actions a 2 A(s) are the operators op 2 O such that Prec(op) s S5. the transition function f maps states s into states s 0 = s?Del(a)+Add(a) for a 2 A(s) S6. all action costs c(a; s) are 1\n\nThe (optimal) solutions of the problem P are the (optimal) solutions of the state model S P . A possible way to nd such solutions is by performing a search in such space. This approach, however, has not been popular in planning where approaches based on divide-and-conquer ideas and search in the space of plans have been more common Nil80,Wel94]. This situation however has changed in the last few years, after Graphplan BF97] and sat approaches KS96] achieved orders of magnitude speed ups over previous approaches. More recently, the idea of planning as state-space search has been advanced in McD96] and BLG97]. In both cases, the key ingredient is the heuristic used to guide the search that is extracted automatically from the problem representation. Here we follow the formulation in BLG97].\n\n\nHeuristics\n\nThe heuristic function h for solving a problem P in BLG97] is obtained by considering a`relaxed' problem P 0 in which all delete lists are ignored. From any state s, the optimal cost h 0 (s) for solving the relaxed problem P 0 can be shown to be a lower bound on the optimal cost h (s) for solving the original problem P. As a result, the function h 0 (s) could be used as an admissible heuristic for solving the original problem P. However, solving the`relaxed' problem P 0 and obtaining the function h 0 are NP-hard problems. 4 We thus use an approximation and set h(s) to an estimate of the optimal value function h 0 (s) of the relaxed problem. In this approximation, we estimate the cost of achieving the goal atoms from s and then set h 0 (s) to a suitable combination of those estimates. The cost of individual atoms is computed by a procedure which is similar to the ones used for computing shortest paths in graphs AMO93]. Indeed, the initial state and the actions can be understood as de ning a graph in atom space in which for every action op there is a directed link from the preconditions of op to its positive e ects. The cost of achieving an atom p is then re ected in the length of the paths that lead to p from the initial state. This intuition is formalized below.\n\nWe will denote the cost of achieving an atom p from the state s as g s (p). These estimates can be de ned recursively as 5 g s (p) = 0 if p 2 s min op2O(p) 1 + g s (Prec(op))] otherwise\n\n(1)\n\nwhere O(p) stands for the actions op that add p, i.e., with p 2 Add(op), and g s (Prec(op)), to be de ned below, stands for the estimated cost of achieving the preconditions of action op from s. While there are many algorithms for obtaining the function g s de ned by (1), we use a simple forward chaining procedure in which the measures g s (p) are initialized to 0 if p 2 s and to 1 otherwise. Then, every time an operator op is applicable in s, each atom p 2 Add(op) is added to s and g s (p) is updated to g s (p) := min g s (p) ; 1 + g s (Prec(op)) ]\n\n(2) These updates continue until the measures g s (p) do not change. It's simple to show that the resulting measures satisfy Equation 1. The procedure is polynomial in the number of atoms and actions, and corresponds essentially to a version of the Bellman-Ford algorithm for nding shortest paths in graphs AMO93].\n\nThe expression g s (Prec(op)) in both (1) and (2) stands for the estimated cost of the set of atoms given by the preconditions of action op. In planners such as hsp, the cost g s (C) of a set of atoms C is de ned in terms of the cost of the atoms in the set. As we will see below, this can be done in di erent ways. In any case, the resulting heuristic h(s) that estimates the cost of achieving the goal G from a state s is de ned as h(s) def = g s (G)\n\n(3) The cost g s (C) of sets of atoms can be de ned as the weighted sum of costs of individual atoms, the minimum of the costs, the maximum of the costs, etc. We consider two ways. The rst is as the sum of the costs of the individual atoms in C:\ng + s (C) = X r2C g s (r) (additive costs)(4)\nWe call the heuristic that results from setting the costs g s (C) to g + s (C), the additive heuristic and denote it by h add . The heuristic h add assumes that subgoals are independent. This is not true in general as the achievement of some subgoals can make the achievement of the other subgoals more or less di cult. For this reason, the additive heuristic is not admissible (i.e., it may overestimate the true costs). Still, we will see that it is quite useful in planning.\n\nSecond, an admissible heuristic can be de ned by combining the cost of atoms by the max operation as: g max s (C) = max r2C g s (r) (max costs)\n\nWe call the heuristic that results from setting the costs g s (C) to g max s (C), the max heuristic and denote it by h max . The max heuristic unlike the additive heuristic is admissible as the cost of achieving a set of atoms cannot be lower than the cost of achieving each of the atoms in the set. On the other hand, the max heuristic is often less informative. In fact, while the additive heuristic combines the costs of all subgoals, the max heuristic focuses only on the most di cult subgoals ignoring all others. In Sect. 7, however, we will see that a re ned version of the max heuristic is used in Graphplan.\n\n\nForward State Planning\n\n\nHSP: A Hill Climbing Planner\n\nThe planner hsp BG99] that was entered into the AIPS98 Planning Contest, uses the additive heuristic h add to guide a hill-climbing search from the initial state to the goal. The hill-climbing search is very simple: at every step, one of the best children is selected for expansion and the same process is repeated until the goal is reached. Ties are broken randomly. The best children of a node are the ones that minimize the heuristic h add . Thus, in every step, the estimated atom costs g s (p) and the heuristic h(s) are computed for the states s that are generated. In hsp, the hill climbing search is extended in several ways; in particular, the number of consecutive plateau moves in which the value of the heuristic h add is not decremented is counted and the search is terminated and restarted when this number exceeds a given threshold. In addition, all states that have been generated are stored in a fast memory (a hash table) so that states that have been already visited are avoided in the search and their heuristic values do not have to be recomputed. Also, a simple scheme for restarting the search from di erent states is used for avoiding getting trapped into the same plateaus.\n\nMany of the design choices in hsp are ad-hoc. They were motivated by the goal of getting a better performance for the Planning Contest and by our earlier work on a real-time planner based on the  The table shows for each planner in each round, the number of problems solved, the average time taken over the problems that were solved, and the number of problems in which each planner was fastest or produced shortest solutions. ipp, stan and blackbox, are optimal parallel planners that minimize the number of time steps (in which several actions can be performed concurrently) but not the number of actions.\n\nAs it can be seen from the table, hsp solved more problems than the other planners but it often took more time or produced longer plans. More details about the setting and results of the competition can be found in McD98a] and in an article to appear in the AI Magazine.\n\n\nHSP2: A Best-First Search Planner\n\nThe results above and a number of additional experiments suggest that hsp is competitive with the best current planners over many domains. However, hsp is not an optimal planner, and what's worse { considering that optimality has not been a traditional concern in planning { the search algorithm in hsp is not complete. In this section, we show that this last problem can be overcome by switching from hill-climbing to a best-rst search (bfs) Pea83]. Moreover, the resulting bfs planner is superior in performance to hsp, and it appears to be superior to some of the best planners over a large class of problems. By performance we mean: the number of problems solved, the time the get those solutions, and the length of those solutions measured by the number of actions.\n\nWe will refer to the planner that results from the use of the additive heuristic h add in a best-rst search from the initial state to the goal, as hsp2. This best-rst search keeps an Open and a Closed list of nodes as a Nil80, Pea83] but weights nodes by an evaluation function f(n) = g(n) + W h(n), where g(n) is the accumulated cost, h(n) is the estimated cost to the goal, and W 1 is a constant.\n\nFor W = 1, the algorithm is a and for W 6 = 1 it corresponds to the so-called wa algorithm Pea83].\n\nHigher values of W usually lead to the goal faster but with solutions of lower quality Kor93]. Indeed, if the heuristic is admissible, the solutions found by wa are guaranteed not to exceed the optimal costs in more than W times. hsp2 uses the wa algorithm with the non-admissible heuristic h add which is evaluated from scratch in every new state generated. The value of the parameter W is xed at 5, even though values in the range 2; 10] do not make a signi cant di erence.\n\n\nExperiments\n\nIn the experiments below, we assess the performance of the two heuristic search planners, hsp and hsp2, in comparison with two state of the art planners, stan 3.0 LF99], and blackbox 3.6 KS99]. hsp and hsp2 both perform a forward state-space search guided by the additive heuristic h add . The rst performs an extended hill-climbing search and the later performs a best-rst search with an evaluation function in which the heuristic is multiplied by the constant W = 5. stan and blackbox are both based on Graphplan BF97], but the latter maps the plan graph into a set of clauses that are checked for satis ability. The version of hsp used in the experiments, hsp 1.2, improves the version used in the AIPS98 Contest and the one used in BG99].\n\nFor each planner and each planning problem we evaluate whether the planner solves the problem, and if so, the time taken by the planner and the number of actions in the solution. stan and blackbox are optimal parallel planners that minimize the number of time steps but not necessarily the number of actions. Both planners were run with their default options.\n\nThe experiments were performed on an Ultra-5 with 128Mb RAM and 2Mb of Cache running Solaris 7 at 333Mhz. The exception are the results for blackbox on the Logistics problems that were taken from the blackbox distribution as the default options produced much poorer results. 6 The domains considered are: Blocks, Logistics, Gripper, 8-Puzzle, Hanoi, and Tire-world. They constitute a representative sample of di cult but solvable instances for current planners. Five of the 10 block-world instances are of our own BG99], the rest of the problems are taken from others as noted. A planner is said not to solve a problem when it either runs out of memory or runs out of time (10 mins). Failure to nd solutions due either to memory or time constraints are displayed as plans with length ?1.\n\nAll the planners are implemented in C and accept problems in the pddl language (the standard language used in the AIPS98 Contest; McD98b]). Moreover, the planners in the hsp family convert every problem instance in pddl into a program in C. Generating, compiling, linking, and loading such program takes in the order of 2 seconds. This time is roughly constant for all instances and domains, and is not included in the gures below.\n\n\nBlocks-World\n\nThe rst experiments deal with the blocks-world. The blocks-world is challenging for domain-independent planners due to the interactions among subgoals and the size of the state space. The ten instances considered involve from 7 to 19 blocks. Five of these instances are taken from the blackbox distribution and ve are from BG99]. The results for this domain are shown in Fig. 1 that displays for each planner the length of the solutions on the left, and the time to get those solutions on the right.\n\nThe lengths produced by stan and blackbox are not necessarily optimal in this domain as there is some parallelism (e.g., moving blocks among disjoint pairs of towers). This is the reason the lengths they report do not always coincide. In any case, the solutions reported by the four planners are roughly equivalent over instances 1{5, with stan and hsp producing slightly longer solutions for instances 4 and 5. Over the more di cult instances 6{10, the situation changes and only hsp and hsp2 report solutions, with the plans found by hsp2 being shorter. Regarding solution times, the times for hsp and hsp2 are roughly even, and slightly shorter than those for stan and blackbox over the rst ve instances. Over the last ve instances stan and blackbox run out of memory.\n\n\nLogistics\n\nThe second set of experiments deals with the logistics domain, a domain that involves the transportation of packages by either trucks or airplanes. Trucks can move among locations in the same city, and airplanes can move between airports in one city to airports in another city. Packages can be loaded and unloaded in trucks and airplanes, and the task is to transport them from their original locations to some target locations. This is a highly parallel domain, where many operations can be done in parallel. As a result, plans involve many actions but the number of time steps is usually much smaller. The domain is from Kautz and Selman from an earlier version due to Manuela Veloso. The 30 instances we consider are from the blackbox distribution.\n\nThe results for this domain are shown in Fig. 2; instances 1{15 at the top and instances 16{30 at the bottom. As before, the number of actions in the plans are reported on the left, times are reported on the right, and failures to nd a plan are reported with length ?1. Both hsp2 and blackbox solve all 30 instances, hsp2 being roughly two orders of magnitude faster. stan and hsp, on the other, fail on 13 and 10 instances respectively. Interestingly, the times reported by stan on the instances it solves tend to be close to those reported by hsp2. On the other instances stan runs out of memory (instances 3,16,17,20,22,28) or time (instances 2,6,7,8,9,15,21).\n\n\nGripper\n\nThe third set of experiments deals with the Gripper domain used in the AIPS98 Planning Contest and due to J. Koehler. This is a domain that concerns a robot with two grippers that must transport a set of balls from one room to another. It is very simple for humans to solve but in the Planning Contest proved di cult to most of the planners. Indeed, the domain is not challenging for specialized solvers, but is challenging for certain types of domain-independent planners.\n\nThe results over 10 Gripper instances from the AIPS98 Contest are shown in Fig. 3. The planners hsp and hsp2 have no di culties and compute plans with similar lengths. On the other hand, blackbox solves the rst two instances only, and stan the rst four instances. As shown on the left, the time required by both planners grows exponentially and they run out of time over the larger instances. On the other hand, hsp and hsp2 scale up smoothly with hsp2 being slightly faster than hsp.\n\nOne of the reasons for the failure of both stan and blackbox in Gripper is that the heuristic implicitly represented by the plan graph is a very poor estimator in this domain. As a result, Graphplanbased planners, such as stan and blackbox that perform a form of ida search must do many iterations before nding a solution. Actually, the same exponential growth in Gripper occurs also in hsp planners when the heuristic h max is used in place of the additive heuristic. As before, the problem is that the h max heuristic is almost useless in this domain where subgoals are mostly independent. The heuristic  Instance i has i + 2 disks, thus problems range from 3 disks up to 8 disks. Over all these problems hsp2 and stan generate plans of the same quality, hsp2 being slightly faster than stan. hsp also solves all instances but the solutions are longer. blackbox solves the rst two instances.\n\n\nTire-World\n\nThe tire-world domain is due to S. Russell and involves operations for xing at tires: opening and closing the trunk of a car, fetching and putting away tools, loosening and tightening nuts, etc. Fig. 6 shows the results. Here both stan and blackbox solve all three instances producing optimal plans. hsp and hsp2 also solve these instances but in some cases they produce inferior solutions. On the time scale, hsp2 is slightly faster than stan, and both are faster than blackbox in one case by two orders of magnitude. As before, hsp is slower than hsp2 and produces longer solutions. The experiments above, based on a representative sample of problems, show that the two forward heuristic search planners hsp and hsp2 are capable of solving the problems solved by two state of the art planners. In addition, in some domains, hsp and in particular hsp2 solve problems that the other planners with their default settings do not currently solve. The planner hsp2, based on a standard best rst search, tends to be faster and more robust than the hill-climbing hsp planner. Thus, the arguments in BG99] in support of a hill-climbing strategy based on the slow node generation rate that results from the computation of the heuristic in every state do not appear to hold in general. Indeed, the combination of the additive heuristic h add and the multiplying constant W > 1 often drive the best-rst planner to the goal with as few node evaluations as the hill-climbing planner, already providing the necessary`greedy' bias. An a search with an admissible and consistent heuristic, on the other hand, is bound to expand all nodes n whose cost f(n) is below the optimal cost. This however does no apply to the wa strategy used in hsp2.\n\nIn the experiments the W parameter in hsp2 was set to the constant value 5. Yet hsp2 is not particularly sensitive to the exact value of this constant. Indeed, in most of the domains, values in the interval 2; 10] produce similar results. This is likely due to the fact that the heuristic h add is not admissible and by itself tends to overestimate the true costs without the need of a multiplying factor. On the other hand, in some domains like Logistics and Gripper, the value W = 1 does not lead to solutions. This is precisely because in these domains that involve subgoals that are mostly independent, the additive heuristic is not`su ciently' overestimating. Finally, in problems like the sliding tile puzzles, values of W closer to 1 produce better solutions in more time, in correspondence with the normal pattern observed in cases in which the heuristic is admissible Kor93]. Fig. 7 shows the e ects of three di erent values of W on the quality and times of the solutions, and the number of nodes generated. The values considered are W = 1, W = 2, and W = 5. The top three curves that correspond to Hanoi, are typical for most of the other domains and show little e ect. The second set of curves corresponds to Gripper where hsp2 fails to solve the last six instances for W = 1. Indeed, the two right most curves show an exponential growth in time and the number of generated nodes. In Logistics, hsp2 with W = 1 also fails to solve most of the instances. As noted above, these are two domains where subgoals are mostly independent and where the additive heuristic is not su ciently overestimating and hence fails to provide the`greedy bias' necessary to nd the solutions. Indeed, the state space in Logistics is very large, while in Gripper it's the branching factor that is large due to the (undetected) symmetries in the problem.\n\nThe bottom set of curves in Fig. 7 correspond to the Puzzle domain. In this domain, hsp2 with W = 1 and W = 2 produce better solutions and in some cases take more time. This probably happens in Puzzle because, as in Gripper and Logistics, there is a degree of decomposability in the domain (that's why the sum of the Manhattan distance works), that makes the additive heuristic behave as an admissible heuristic in wa . Unlike Gripper and Logistic, however the branching factor of the problem and the size of the state space allow the resulting bfs algorithm to solve the instances even with W = 1. Actually, with W = 1 and W = 2, hsp2 solves the sixth instance of Puzzle which is not solved with W = 5.\n\n6 HSPr: Heuristic Regression Planning A main bottleneck in both hsp and hsp2 is the computation of the heuristic from scratch in every new state. 7 This takes more than 80% of the total time in both planners and makes the node generation rate very low. Indeed, in a problem like the 15-Puzzle, both planners generate less than a thousand nodes per 7 The same applies also to McDermott's unpop.  second, while a specialized solver such as KT96] generates several hundred thousand nodes per second for the more complex 24-puzzle. The reason for the low node generation rate is the computation of the heuristic in which the estimated costs g s (p) for all atoms p are computed from scratch in every new state s.\n\nIn BG99], we noted that this problem could by solved by performing the search backward from the goal rather than forward from the initial state. In that case, the estimated costs g s0 (p) derived for all atoms from the initial state could be used without recomputation for de ning the heuristic of any state s arising in the backward search. Indeed, the estimated distance from s to s 0 is equal to the distance from s 0 to s, and this distance can be estimated simply as the sum (or max) of the costs g s0 (p) for the atoms p in s. This trick for simplifying the computation of the heuristic and speeding up node generation results from computing the estimated atom costs from a state s 0 which then becomes the target of the search. An alternative is to estimate the atom costs from the goal and then perform a forward search toward the goal. This is actually the idea in RV99]. The problem with this latter scheme is that the goal in planning is not a state but a set of states; namely, the states where the goal atoms hold. And computing the heuristic from a set of states in a principled manner is bound to be more di cult than computing the heuristic from a given state (thus the need to`complete' the goal description in RV99]).\n\nWe thus present below a scheme for performing planning as heuristic search that avoids the recomputation of the atom costs in every new state by computing these costs once from the initial state s 0 . These costs are then used without recomputation to de ne an heuristic that is used to guide a regression search from the goal. The bene t of the search scheme is that node generation will be 6-7 times faster. This will show in the solution of some of the problems considered above such as Logistics and Gripper. However, as we will also see, in many problems the new search scheme does not help, and in several cases, it actually hurts. We discuss such issues below.\n\n\nRegression State Space\n\nWe refer to the planner that searches backward from the goal rather than forward from the initial state as hspr. Backward search is an old idea in planning that is known as regression search Nil80,Wel94]. In regression search, the states can be thought as sets of subgoals; i.e., the`application' of an action in a goal yields a situation in which the execution of the action achieves the goal. Moreover, while a set of atoms fp; q; rg in the forward search represents the unique state in which the atoms p, q, and r are true and all other atoms are false, the same set of atoms in the regression search represents the collection of states in which the atoms p, q, and r are true. In particular, the set of goals atoms G, which determines the root node of the regression search, stands for the collection of goal states, that is, the states s such that G s.\n\nFor making precise the nature of the backward search, we will thus de ne explicitly the state space being searched. We will call it the regression space and de ne it in analogy to the progression space S P de ned by S1]{ S5] above. The regression space R P associated with a Strips problem P = hA; O; I; Gi is given by the tuple R P = hS; s 0 ; S G ; A( ); f; ci where R1. the states s are sets of atoms from A R2. the initial state s 0 is the goal G The solution of this state space is, like the solution of any state model hS; s 0 ; S G ; A( ); f; ci, a nite sequence of actions a 0 , a 1 , . .. , a n such that for a sequence of states s 0 , s 1 , .. ., s n+1 , s i+1 = f(a i ; s i ), for i = 0; : : :; n, a i 2 A(s i ), and s n+1 2 S G . The solution of the progression and regression spaces are related in the obvious way; one is the inverse of the other.\n\nWe use di erent fonts for referring to states s in the progression space S P and states s in the regression space R P . While they are both represented by sets of atoms, they have a di erent meaning. As we said above, the state s = fp; q; rg in the regression space stands for the set of states s, fp; q; rg s in the progression space. For this reason, forward and backward search in planning are not symmetric, unlike forward and backward search in problems like the 15-Puzzle or Rubik's Cube.\n\n\nHeuristic\n\nThe planner hspr searches the regression space R1]{ R5] using an heuristic based on the additive cost estimates g s (p) described in Sect 4. These estimates are computed only once from the initial state s 0 2 S. The heuristic h add (s) associated with any state s is then de ned as h add (s) = X p2s g s0 (p)\n\nWhile in hsp, the heuristic h add (s) combines the cost estimates g s (p) of a xed set of goal atoms computed from each state s, in hspr, the heuristic h add (s) combines the cost estimates of the set of subgoals p in s from a xed state s 0 . The heuristic h max (s) can be de ned in an analogous way by replacing sums by maximizations.\n\n\nMutexes\n\nThe regression search often leads to states s that are not reachable from the initial state s 0 . develop a simple mechanism for detecting some pairs of atoms fp; qg such that any state containing those pairs can be proven to be unreachable from the initial state, and thus can be given an in nite heuristic value and pruned. The idea is adapted from a similar idea used in Graphplan BF97] and thus we call such pairs of unreachable atoms mutually exclusive pairs or mutex pairs. As in Graphplan, the de nition below is not guaranteed to identify all mutex pairs, and furthermore, it says nothing about larger sets of atoms that are not achievable from s 0 but whose proper subsets are.\n\nA tentative de nition is to identify a pair of atoms R as a mutex when R is not true in the initial state s 0 and every action that asserts an atom in R deletes the other. This de nition is sound (it only recognizes pairs of atoms that are not achievable jointly) but is too weak. In particular, it does not recognize a set of atoms like fon(a; b); on(a; c)g as a mutex, since actions like move(a; d; b) add the rst atom but do not delete the second.\n\nWe thus use a di erent de nition in which a pair of atoms R is recognized as mutex when the actions that add one of the atoms in R and do not delete the other atom, can guarantee through their preconditions that such atom will not be true after the action. To formalize this, we consider sets of mutexes rather that individual pairs.\n\nDe nition 1 A set M of atom pairs is a mutex set given a set of operators O and an initial state s 0 i for all atoms pairs R = fp; qg in M 1. R is not true in s 0 , 2. for every op 2 O that adds p, either op deletes q, or op does no add q and for some precondition r of op, R 0 = fr; qg is a pair in M.\n\nIt is simple to verify that if a pair of atoms R belongs to a mutex set, then the atoms in R are really mutually exclusive, i.e., not achievable from the initial state given the available operators. Also if M 1 and M 2 are two mutex sets, M 1 M 2 will be a mutex set as well, and hence according to this de nition, there is a single largest mutex set. Rather than computing this set, however, that is di cult, we compute an approximation as follows.\n\nWe say that a pair R 0 is a`bad pair' in M when R 0 does not comply with one of the conditions 1{2 above. The procedure for constructing a mutex set starts with a set of pairs M := M 0 and iteratively removes all bad pairs from M until no bad pair remains. The initial set M 0 of`potential' mutexes can be chosen in a number of ways. In all cases, the result of this procedure is a mutex set M such that M M 0 . One possibility is to set M 0 to the set of all pairs of atoms. In BG99], to avoid the overhead involved in dealing with the N 2 =2 pairs of atoms and many useless mutexes, we chose a smaller set M 0 of potential mutexes that turns out to be adequate for many domains. Such set M 0 was de ned as the union of the sets M A and M B where M A is the set of pairs P = fp; qg such that some action adds p and deletes q, M B is the set of pairs P = fr; qg such that for some pair P 0 = fp; qg in M A and some action a, r 2 Prec(a) and p 2 Add(a)\n\nThe structure of this de nition mirrors the structure of the de nition of mutex sets.\n\nA mutex in hspr refers to a pair in the set M obtained from the set M 0 = M A +M B by sequentially removing all`bad' pairs. Like the analogous de nition in Graphplan, the set M does not capture all actual mutexes, yet it can be computed fast, and in many of the domains we have considered appears to prune the obvious unreachable states. A di erence with Graphplan is that this de nition identi es structural mutexes while Graphplan identi es time-dependent mutexes. On the other hand, because of the xed point construction, this de nition can identify mutexes that Graphplan cannot. For example, in the complete TSP domain LF99], pairs like hat(city 1 ); at(city 2 )i would be recognized as a mutex by this de nition but not by Graphplan, as the actions of going to di erent cities are not mutually exclusive for Graphplan.\n\n\nAlgorithm\n\nThe planner hspr uses the additive heuristic h add and the mutex set M to guide a regression search from the goal. The additive heuristic is obtained from the estimated costs g s0 (p) computed once for all atoms p from the initial state s 0 . The mutex set M is used to`patch' the heuristic: states s arising in the search that contain a pair in M get an in nite cost and are pruned. The algorithm used for searching the regression space is the same as the ones used in hsp2: a wa algorithm with the constant W set to 5. Here we depart from the description of hspr in BG99] where the wa algorithm was given a`greedy' bias. As above, we stick to a pure bfs algorithm. The set of experiments below cover more domains than those in BG99] and will help us to assess better the strengths and limitations of regression heuristic planning in relation to forward heuristic planning.\n\n\nExperiments\n\nIn the experiments, we compare the regression planner hspr with the forward planner hsp2. Both are based on a wa search and both use the same additive heuristic (in the case of hspr, patched with the mutex information). hspr avoids the recomputation of the atom costs in every state, and thus computes the heuristic faster and can explore more nodes in the same time. As we will see, this helps in some domains. However, in other domains, hspr is not more powerful than hsp2, and in some domains hspr is actually weaker. This is due to two reasons: rst, the additional information obtained by the recomputation of the atom costs in every state sometimes pays o , and second, the regression search often generates spurious states that are not recognized as such by the mutex mechanism and cause of lot of useless search. These problems are not signi cant in the two domains considered in BG99] but are signi cant in other domains. Fig. 8 shows the results of the two planners hspr and hsp2 over the Logistics instances 16{30. The curves show the length of the solutions (left), the time required to nd the solutions (center), and the number of generated nodes (right). It is interesting to see that hspr generates more nodes than hsp2 and yet it takes roughly four times less time than hsp2 to solve the problems. This follows from the faster evaluation of the heuristic. On the other hand, the plans found by hspr are often longer than those found by hsp2. Similar results obtain for the logistics instances that are not shown in the gure.\n\n\nLogistics\n\n\nGripper\n\nAs shown in Fig. 9, a similar pattern arises in Gripper. Here hspr generates slightly less nodes than hsp2, but since it generates nodes faster, the time gap between the two planner gets larger as the size of the problems grows. In this case, the solutions found by hspr are uniformly better than the solutions found by hsp2, and this di erence grows with the size of the problems. hspr is also stronger than hsp2 in Puzzle where, unlike hsp2 (with W = 5) solves the last instance in the set (a 15-Puzzle instance). However, for the other three domains hspr does not improve on hsp2, and indeed, in two of these domains (Hanoi and Tire-World) it does signi catively worse.\n\n\nHanoi and Tire-World\n\nThe results for Hanoi are shown in Fig. 10. hspr solves the rst three instances (up to 5 disks), but it does not solve the other three. Indeed, as it can be seen, in the rst three instances the time to nd the solutions and the number of nodes generated grow much faster in hspr than in hsp2. The same situation arises in the Tire-World where hsp2 solves all three instances and hspr solves only the rst one. The problems, as we mentioned above, are two: spurious states generated in the regression search that are not detected by the mutex mechanisms, and the lack of the`feedback' provided by the recomputation of the atom costs in every state. Indeed, errors in the estimated costs of atoms in hsp2 can be corrected when they are recomputed; in hspr, on the other hand, they are never recomputed. So the recomputation of these costs has two e ects, one that is bad (time overhead) and one that is good (additional information). In domains where subgoals interact in complex ways, the idea of a forward search in which atom costs are recomputed in every state as implemented in hsp2 will probably make sense; on the other hand, in domains where the additive heuristic is adequate, the backward search with no recomputations as implemented in hspr can be more e cient.\n\nThe results for the hspr and hsp2 planners in the Tire-World show the same pattern as Hanoi. Indeed, hspr solves just the rst instance, while hsp2 solves the three instances. As we show below, however, part of the problem in this domain has to do with the spurious states generated in the regression search.\n\n\nImproved Mutex Computation\n\nThe regression search often leads to states s that are not reachable from the initial situation and which often violate implicit domain constraints. For example, the regression of a state s = fon(c; d); on(a; b)g through the action move(a; d; b) leads to the state s' = fon(c; d); on(a; d); clear(b); clear(a)g in which two blocks c and a are on top of the same block d. hspr uses a procedure inspired in Graphplan to detect such mutex pairs and prune spurious states like s' that contain them. Neither our mechanism, nor Graphplan's, however, can detect all spurious (unreachable) states. Indeed, there are unreachable states that do not contain any unreachable pair of atoms such as the state where a is on b, b is on c, and c is on a BF97]. So a potential serious problem in regression planners such as Graphplan and hspr is the presence of such states. hspr has an additional problem and that is that it fails to detect some structural mutexes that Graphplan detects. The procedure used in hspr to identify mutexes starts with a set M 0 of potential mutexes and then removes the`bad' pairs from M 0 until no`bad' pair remains. A problem we have detected with the de nition in BG99] which we have used here is that the set of potential mutexes M 0 sometimes is not large enough and hence useful mutexes are lost. Indeed, we performed experiments in which M 0 is set to the collection of all atom pairs, and the same procedure is applied to this set until no`bad' pairs remains. In most of the domains, this change didn't yield a di erent behavior. However, there were two exceptions. While hspr solved only the rst instance of the Tire-World, hspr using the extended set of potential mutexes solved the three instances. This shows that in this case hspr was a ected by the problem of spurious states. On the other hand, in problems like logistics, the new set M 0 leads to a much larger set of mutexes M that are not as useful and yet have to be checked in all the states generated. This slows down node generation with no compensating gain thus making hspr several times slower. The corresponding curves are shown in Fig. 11, where`mutex-1' and`mutex-2' correspond to the original and extended de nition of the set M 0 of potential mutexes. Since, the bene ts appear to be more important than the loses, the extended de nition seems worthwhile and we will make it the default option in the next version of hspr. However, since for the reasons above, the new mechanism is not complete either, the problem caused by the presence of spurious states in regression planning remains open. 8\n\n\nAdditional Issues in Heuristic Regression Planning\n\nAdditive vs. Max Heuristic. While we de ned two heuristics, the additive heuristic h add and the max heuristic h max , we have used only the additive heuristic. The intuition underlying this choice is that the additive heuristic is more informed as it takes into account all subgoals, while the max heuristic only focuses on the subgoals that are perceived as most costly. In order to test this intuition we ran hspr over all the domains with the additive heuristic and the max heuristic. In problems that involve many independent subgoals such as Gripper and Logistics, the max heuristic is almost useless and very few instances are solved. On the other hand, in problem that involve more complex interactions among goals such as Hanoi and Tire-World, the heuristic h max does slightly better than h add , and indeed, in Tire-World it solves the second instance that h add does not solve (within hspr). Finally in Blocks-World and Puzzle where there is a certain degree of decomposability, the h max heuristic is worse than the h add heuristic but still manages to solve roughly the same set of instances taking more time.\n\nIn summary, the additive heuristic yields a better behavior in hspr than the max heuristic, but this does not mean that the max heuristic is useless. The heuristic used implicitly in Graphplan is as a re nement of the h max heuristic, as is the family of higher-order heuristic formulated in HG00]. We will say more about those heuristics below.\n\nGreedy Best-First Search. The algorithm used in the version of the hspr planner presented in BG99] uses the same wa algorithm but with the following variation: when some of the children of the last expanded node improve the heuristic value of the parent, the best such child is selected for expansion even if such node is not a least cost node in the Open list. The idea is to provide an additional greedy bias in the search. This modi cation helps in some instances and in general does not appear to hurt, yet the boost in performance across a large set of domains is small. For this reason, we have dropped this feature from hspr which is now a pure bfs regression planner.\n\nBranching Factor. A common argument for performing regression search rather than forward search in planning has been based on considerations related to the branching factors of the two spaces Nil80,Wel94]. We have measured the forward and backward branching factor in all the domains and found that they vary a lot from instance to instance. For example, the forward branching factor in the Blocks-World instances ranges from 16:83 to 84:62, while the backward branching factor ranges from 4:73 to 12:15. In Logistics, the forward branching factor ranges from 7:89 to 37:91, while the backward factor ranges from 9:68 to 25:80. In problems like Puzzle and Hanoi, the average branching factors are roughly constant over the di erent instances, and are similar in both directions.\n\nWe have found, however, that the performance of the two planners, hsp2 and hspr, on the same problem is not in direct correspondence with the size of the forward and backward branching factors. For example, while for each blocks-world instance the average branching factor in hspr is less than half the one in hsp2, and moreover the rst planner generates nodes 6-7 times faster on average than the second, hspr is not better than hsp2 in blocks-world. On the other hand, in logistics, where the average branching factor in hsp2 is often smaller than the one in hspr, hspr does better. Thus while considerations related to the branching factor of the forward and backward spaces are relevant to the performance of planners, they are not the only or most important consideration. As we mentioned, two considerations that are relevant for explaining the performance of hspr in relation to hsp2 are the quality of the heuristic (which in hsp2 is recomputed in every state), and the presence of spurious states in the regression search (that do not arise in the forward search). This last problem, however, could be solved by the formulation of better planning heuristics in which the cost of a set of atoms is not de ned in terms of the costs of the individual atoms in the set as in the h add and h max heuristics. Such heuristics are considered in HG00] and are brie y discussed below. The idea of performing a regression search from the goal for avoiding the recomputation of the atom costs was presented in BG99] where the hspr planner was introduced. The version of hspr considered here, unlike the version reported in BG99], is based on a pure bfs algorithm. Likewise, the pure bfs forward planner that we have called hsp2, hasn't been discussed elsewhere. hsp2 is the simplest, and as the experiments have illustrated, it is also the most solid planner in the hsp family.\n\nTwo advantages of forward planners over regression planners is that the former do not generate spurious states and they often bene t from the additional information obtained by the recomputation of the atom costs in every state. Mutex mechanisms such as those used by hspr and Graphplan can prune some of spurious states in some problems, but they cannot be complete.\n\nThe idea of combining a forward propagation from s 0 to compute all atom costs and a backward search from the goal for avoding the recomputation of these costs appears in reverse form in RV99]. Refanidis and Vlahavas compute cost estimates by a backward propagation from the goal and then use those estimates to perform a forward state-space search from the initial state. In addition, they compute the heuristic in a di erent way so they get more accurate estimates.\n\n\nDerivation of Heuristics\n\nThe non-admissible heuristic h add used in hsp is derived as an approximation of the optimal cost function of a relaxed problem where deletes lists are ignored. This formulation has two obvious problems. First, the approximation is not very good as it ignores the positive interactions among subgoals that can make one goal simpler after a second one has been achieved (this results in the heuristic being non-admissible). Second, the relaxation is not good as it ignores the negative interactions among subgoals that are lost when delete lists are discarded. These two problems are addressed in the heuristic proposed by Refanidis and Vlahavas RV99] but their heuristic is still non-admissible and does not have clear justi cation.\n\nA di erent approach for addressing these limitations has been reported recently in HG00]. While the idea of the h max heuristic presented in Sect. 4 is to approximate the cost of a set of atoms by the cost of the most costly atom in the set, the idea in HG00] is to approximate the cost of a set of atoms by the cost of the the most costly atom pair in the set. The resulting heuristic, called h 2 is admissible and more informative than the h max heuristic, and can be computed reasonably fast. Indeed, in HG00] the h 2 heuristic is used in the context of an ida search to compute optimal plans. Higher order heuristics h m in which the cost of sets of atoms is approximated by the most costly subset of size m are also discused. Such higher-order heuristics may prove useful in problems in which subgoals interact in complex ways.\n\nThe derivation of admissible heuristics by the consideration of relaxed models has a long history in AI. Indeed, the Manhattan distance heuristic in sliding tile puzzles is normally explained in terms of the solution of a relaxed problem in which tiles can move to any neighboring position Pea83]. A similar relaxation is used to explain the Minimum Spanning Tree heuristic used for solving the Traveling Salesman Problem. Moreover, in Pea83], these relaxations are shown to follow from simpli cations in suitable Strips encodings, and in particular the Manhattan heuristic is derived by ignoring some action preconditions.\n\nThe idea of deriving heuristics from suitable relaxations is a powerful idea. However, it is often too general to provide practical guidance in the formulation of concrete heuristics for speci c problems. Indeed, the idea of dropping action preconditions from Strips encodings is guaranteed to lead to admissible heuristics but computing such heuristics can be as hard as solving the original problem. Indeed, unless we remove all preconditions the class of`relaxed' planning problems is still intractable. In this paper, we have used a di erent relaxation in which delete lists are removed. While, the resulting problem is still intractable, its optimal cost can be approximated by the methods discussed in Sect. 4. It'd be interesting to see if useful heuristics for planning could be obtained by polynomial approximations that simplify the preconditions rather than the action delete lists. The scheme for deriving admissible heuristics from HG00] can actually be seen from this perspective.\n\nThe automatic derivations of useful admissible heuristics has also been tackled by Prieditis Pri93]. Prieditis' scheme is based on a set of transformations that generate a large space of relaxations given problems expressed in a version of Strips. This space is then searched for relaxations that produce heuristics that speed up the search in the original problem. He shows that a number of interesting heuristics can be identi ed in this way. Our work departs from this in that we stick to one particular type of relaxation for all problems. However, an scheme like Prieditis' could be used as an o -line learning component of heuristic search planners that could tune the type of heuristic for the given domain.\n\nA more recent scheme for deriving heuristics is based on the notion of pattern databases developed by Culberson and Schae er CS98] and used by Korf for nding optimal solutions to Rubik's Cube Kor98]. In a problem like the 15-Puzzle, a pattern database can be understood as a table that contains the optimal costs associated with a relaxed (abstracted) state model in which the location of a certain set of tiles are ignored. Since the relaxed state model can have a much smaller size than the original state model, it can be solved optimally by blind search (e.g., breadth-rst search). Then the heuristic h(s) of a state s can be obtained by taking the distance from the projection of s to the projection of the goal G in the relaxed state model. If there are several pattern databases, the maximum of these distances is taken instead. The idea of pattern databases is powerful but is not completely general. Indeed, the size of the relaxed state model that arises in planning problems when the values of certain state-variables are ignored, is not necessarily smaller than the size of the original problem ( HH99] mentions the case of the blocks-world). However, the idea applies very well to permutation problems such as sliding tile puzzles and Rubik's cube, and may have application in many planning domains.\n\nKorf and Taylor KT96] also sketch a theory of heuristics that may have application in domainindependent planning. In the sliding tile puzzles, their idea is to solve a number of`relaxed' problems in which we only care about disjoint subsets of tiles and in each case we only count the moves of the tiles selected. Then the addition of such counts provides an admissible heuristic for the original problem. As in the case of pattern databases, each of the relaxed problems involves a smaller state model that can be solved by brute force methods. Also as for pattern databases, the approach seems applicable to permutation problems but not to arbitrary planning problems. In particular, it's not clear how to apply these ideas to a problem like blocks-world.\n\n\nHeuristic Regression Planning and Graphplan\n\nThe operation of the regression planner hspr consists of two phases. In the rst, a forward propagation is used to estimate the costs of all atoms from the initial state s 0 , and in the second, a regression search is performed using those measures. These two phases are in correspondence with the two operation phases in Graphplan BF97] where a plan graph is built forward in a rst phase, and is searched backward for plans in the second. The two planners are also related in the use of mutexes, and idea that hspr borrows from Graphplan. For the rest, hspr and Graphplan look quite di erent. However, Graphplan can also be understood as an heuristic search planner with a precise heuristic function and search algorithm. From this point of view, the main innovation in Graphplan is the implementation of the search that takes advantage of the plan graph and is quite e cient, and the derivation of the heuristic that makes use of the mutex information. More precisely, from the perspective of heuristic search planning, the main features of Graphplan can be understood as follows:\n\n1. Plan Graph: The plan graph encodes an admissible heuristic h G where h G (s) = j i j is the index of the rst level in the graph that includes s without a mutex and in which s is not memoized (memoizations are updates on the heuristic function; see 4). The heuristic h G is a re ned version of the h max heuristic discussed in Sect. 4, and is closely related to the family of admissible heuristics formulated in HG00].\n\nIn HG00] Graphplan is compared with a pure ida planner based on an admissible heuristic equivalent to Graphplan's h G . In sequential domains the planners have a similar performance, but on parallel domains, Graphplan is more than an order-of-magnitude faster due to the more e cient ida search a orded by the plan graph. The plan graph, however, restricts Graphplan to ida searches, and it cannot be easily adapted to best-rst searches or wida searches Kor93]. That's why it is not simple to modify a Graphplan-based planner to produce suboptimal solutions fast, while this is trivial in heuristic search algorithms where a multiplying constant W > 1 in the heuristic term of the evaluation function often achieves the desired e ect Pea83]. In addition, in serial problems in which subgoals are mostly independent, like Gripper, the heuristic h G computed by Graphplan yields poor estimates, and Graphplan-based planners are not likely to do well.\n\n\nConclusions\n\nWe have presented a formulation of planning as heuristic search and have shown that simple state-space search algorithms guided by a general domain-independent heuristic produce a family of planners that are competitive with some of the best current planners. We have also explored a number of variations, such as reversing the direction of the search for accelerating node evaluation, and extracting information about propositional invariants for avoiding dead-ends. The planner that showed the most solid performance, however, was the simplest planner, hsp2, based on a best-rst forward search, in which atom costs are recomputed from scratch in every state.\n\nHeuristic search planners are related to specialized problem solvers but di er from them in the use of a general declarative language for stating problems and a general mechanism for extracting heuristics. Planners must o er good modeling language for expressing problems in a convenient way, and general solvers for operating on those representations and producing e cient solutions.\n\nA concrete challenge for the future is to reduce the gap in performance between heuristic search planners and specialized problem solvers in domains like the 24-puzzle KT96], Rubik's cube Kor98], and Sokoban JS99]. In addition, for planners to be more applicable to real problems, it is necessary that they handle aspects such as non-boolean variables, action durations, and parallel actions. Planners should be able to accommodate a rich class of scheduling problems, yet very few planners currently have such capabilities, and even fewer if any can compete with specialized solvers. Three issues that we believe must be addressed in order to make heuristic search planners more general and more powerful are the ones discussed below.\n\nHeuristics: the heuristics h add and h max considered in this paper are poor estimators, and cannot compete with specialized heuristics. The heuristic h G used in Graphplan is better than h max but it is not good enough for problems like Rubik's Cube or the 24-puzzle where subgoals interact in complex ways. In HG00], a class of admissible heuristics h m are formulated in which the cost of a set of atoms C is approximated by the cost of the mostly costly subset of size m. For m = 1, h m reduces to the h max heuristic, and for m = 2, h m reduces to the Graphplan heuristic. Higher order heuristics for m > 2, may prove e ective in complex problems such as the 24-puzzle and Rubik's cube, and they may actually be competitive with the specialized heuristics used for those problems KT96,Kor98]. As mentioned in HG00], the challenge is to compute such heuristics reasonably fast, and to use them with little overhead at run time. Such higher-order heuristics are related to pattern databases but they are applicable to all planning problems and not only to permutation problems.\n\nBranching rules: in highly parallel domains like rockets and logistics, sat approaches appear to perform best among optimal parallel planners. This may be due to the branching scheme used. In sat formulations, the space is explored by setting the value of any variable at any time point, and then considering each of the resulting state partitions separately. In Graphplan and in heuristic search approaches, the splitting is done by applying all possible actions. Yet alternative branching schemes, are common in heuristic branch-and-bound search procedures LRK85], in particular, in scheduling applications CP89]. Work on parallel planning, in particular involving actions of di erent durations, would most likely require such alternative branching schemes.\n\nModeling Languages: all the planners discussed in this paper are Strips planners. Yet few real problems can actually be encoded e ciently in Strips. This has motivated the development of extensions such as adl Ped89] and Functional Strips Gef99]. From the point of view of heuristic search planning, the issue becomes the derivation of good heuristics from such richer languages. The ideas considered in this paper do not carry directly to such languages but it seems that it should be possible to exploit the richer representations for extracting better heuristics.\n\nFigure 1 :\n1Solution length (left) and time (right) over 10 blocks-world instances.\n\nFigure 2 :\n2Solution length (left) and time (right) over 30 Logistics instances from Kautz and Selman\n\nFigure 3 :Figure 4 :\n34Solution length (left) and time (right) over 10 gripper instances from Solution length (left) and time (right) over four instances of the 8-Puzzle (1{4) and two instances of the 15-Puzzle (5{6) implicit in the plan graph is a re nement of the h max heuristic; the relation between Graphplan and heuristic search planning will be analyzed further in Sect. 7. Puzzle The next problems are four instances of the familiar 8-Puzzle and two instances from the larger 15-Puzzle. Three of the four 8-Puzzle instances are hard as their optimal solutions involves 31 steps, the maximum plan length in such domain. The 15-Puzzle instances are of medium di culty. As shown in Fig. 4, hsp and stan solve the rst four instances, and hsp2 solves the rst ve. The solutions computed by stan are optimal in this domain which is purely serial. The solutions computed by hsp and hsp2, on the other hand, are poorer, and are often twice as long. On the other hand, as shown on the left part of the gure, hsp2 is two orders of magnitude faster than stan over the di cult 8-Puzzle instances (2{4) and can also solve instance 5. The times for hsp are worse and does not solve instance 5. blackbox does not solve any of the instances. Hanoi Fig. 5 shows the results for Hanoi.\n\nFigure 5 :Figure 6 :\n56Solution length (left) and time (right) over six Hanoi instances. Instance i has i Solution length (left) and time (right) over three instances of Tire\n\nFigure 7 :\n7In uence of value of W in the hsp2 planner on the length of the solutions (left), the time required to nd solutions (center), and number of nodes generated (right). The domains from top to bottom are Hanoi, Gripper, and Puzzle.\n\n\nR3. the goal states s 2 S G are the states for which s I R4. the set of actions A(s) applicable in s are the operators op 2 O that are relevant and consistent; namely, for which Add(op) \\ s 6 = ; and Del(op) \\ s = ; R5. the state s 0 = f(a; s) that follows the application of a 2 A(s) is such that s' = s?Add(a)+ Prec(a) R6. the action costs c(a; s) are all 1\n\n\nFor example, in the blocks world, the regression of the state s = fon(c; d); on(a; b)g through the action move(a; d; b) leads to the state s' = fon(c; d); on(a; d); clear(b); clear(a)gThis state represents a situation in which two blocks, c and a are on the same block d. It is simple to show that such situations are unreachable in the block-worlds given a`normal' initial state. Such unreachable situations are common in regression planning, and if undetected, cause a lot of useless search. A good heuristic would assign an in nite cost to such situations but our heuristics are not as good. Indeed, the basic assumption underlying both the additive and the max heuristics | that the estimated cost of a set of atoms is a function of the estimated cost of the atoms in the set | is violated in such situations. Indeed, while the cost of each of the atoms on(c; d) and on(a; d) is nite, the cost of the pair of atoms fon(c; d); on(a; d)g is in nite. Better heuristics that do not make this assumption and correctly re ect the cost of such pairs of atoms have been recently described in HG00]. Here we follow BG99] and\n\nFigure 8 :\n8Comparison between hspr vs hsp2 over Logistics instances 16{30. Curves show solution length (left), time (center), and number of nodes generated (right).\n\nFigure 9 :\n9Comparison between hspr vs hsp2 over Gripper instances. Curves show solution length (left), solution (center), and number of nodes generated (right)\n\nFigure 10 :\n10Comparison between hspr vs hsp2 over Hanoi. Curves show solution length (left), solution (center), and number of nodes generated (right). Instance i has i + 2 disks.\n\nFigure 11 :\n11Impact of original vs. extended de nition of the set M 0 of potential mutexes in Tire-World and Logistics\n\n\nof extracting heuristics from declarative problem representations in planning has been proposed recently byMcDermott McD96]  and by Bonet, Loerincs, and Ge ner BLG97]. In BLG97], the heuristic is used to guide a real-time planner based on the lrta algorithm Kor90], while inMcD96], the heuristic is used to guide a limited discrepancy search HG95]. The heuristics in both cases are similar, even though the formulation and the algorithms used for computing them are di erent. The performance of McDermott's unpop, however, does not appear to be competitive with the type of planners discussed in this paper. This may be due to the fact that it is written in Lisp and deals with variables and matching operations at run-time. Most current planners, including those reported in this paper, are written in C and deal with grounded operators only. On the other hand, while most of these planners are restricted to small variations of the Strips language, unpop deals with the more expressive adl Ped89].\nInterestingly, the area of constraint programminghas similar goals although it is focused on a di erent class ofproblems HSD92]. Yet, see BC99] for a recent attempt to apply the ideas of constraint programming in planning.2 Another way to reduce the gap between planners and specialized solvers is by making room in planning languages for expressing domain-dependent control knowledge (e.g., BK98]). In this paper, however, we don't consider this option which is perfectly compatible and complementary with the ideas that we discuss.\nAs it is common, we use the current version of the Strips language as de ned by the Strips subset ofpddl  McD98b] rather than original version in FN71].\nThis can be shown by reducing set-covering to Strips with no deletes.5  Where the min of an empty set is de ned to be in nite.\nThose results were obtained on a SPARC Ultra 2 with a 296MHz clock and 256M ofRAM BK98].\nAs noted in BF97], the problem of detecting all mutexes, and even only all mutex pairs, is as hard the plan existence problem.\n. Mutex: Mutexes are used to prune states in the regression search (as inhspr)  and to re ne the heuristic h max . In particular, the cost of a set of atoms C is no longer given by the cost of the most costly atom in the set when in the rst layer that contains C, C occurs with a mutex.3. Algorithm: The search algorithm is a version of Iterative Deepening a (ida )Kor85], where the sum of the accumulated cost g(n) and the estimated cost h G (n) is used to prune nodes n whose cost exceed the current threshold. Actually Graphplan never generates such nodes. The algorithm takes advantage of the information stored in the plan graph and converts the search in a`solution extraction' procedure.4. Memoization: Memoizations are updates on the heuristic function h G (see 1). The resultingalgorithm is a memory-extended version of ida that closely corresponds to the mrec algorithm SB89]. In mrec, the heuristic of a node n is updated and stored in a hash-table after the search below the children of n completes without a solution (given the current threshold). 5. Parallelism: Graphplan, unlike hspr, searches a parallel regression space. While the branching factor in this search can be very high, Graphplan makes smart use of the information in the graph to generate only the children that are`relevant' and whose cost does not exceed the current threshold. The branching rule used in Graphplan is made explicit in HG00].\nAcknowledgmentsWe thank Daniel Le Berre, Rina Dechter, Patrik Haslum, Joerg Ho man, Rao Kambhampati, and Richard Korf for discussions related to this work. This work has been partially supported by grant S1-96001365 from Conicit, Venezuela and by the Wallenberg Foundation, Sweden. Blai Bonet is currently at UCLA with a USB-Conicit fellowship.\nR Ahuja, T Magnanti, J Orlin, Network Flows. Prentice-HallR. Ahuja, T. Magnanti, and J. Orlin. Network Flows. Prentice-Hall, 1993.\n\nConditional e ects in graphplan. C Anderson, D Smith, D Weld, Proceedings AIPS-98. AIPS-98AAAI PressC. Anderson, D. Smith, and D. Weld. Conditional e ects in graphplan. In Proceedings AIPS-98. AAAI Press, 1998.\n\nCPlan: a constraint programming approach to planning. P Van Beek, X Chen, Proceedings AAAI-99. AAAI-99P. Van Beek and X. Chen. CPlan: a constraint programming approach to planning. In Proceedings AAAI-99, 1999.\n\nFast planning through planning graph analysis. A Blum, M Furst, Arti cial Intelligence. A. Blum and M. Furst. Fast planning through planning graph analysis. Arti cial Intelligence, pages 281{300, 1997.\n\nPlanning as heuristic search: New results. B Bonet, H Ge, Proceedings of ECP-99. ECP-99SpringerB. Bonet and H. Ge ner. Planning as heuristic search: New results. In Proceedings of ECP-99. Springer, 1999.\n\nUsing temporal logics to express search control knowledge for planning. F Bacchus, F Kabanza, SubmittedF. Bacchus and F. Kabanza. Using temporal logics to express search control knowledge for planning, 1998. Submitted.\n\nA robust and fast action selection mechanism for planning. B Bonet, G Loerincs, H Ge, Proceedings of AAAI-97. AAAI-97MIT PressB. Bonet, G. Loerincs, and H. Ge ner. A robust and fast action selection mechanism for planning. In Proceedings of AAAI-97, pages 714{719. MIT Press, 1997.\n\nAn algorithm for solving the job shop problem. J Carlier, E Pinson, Management Science. 352J. Carlier and E. Pinson. An algorithm for solving the job shop problem. Management Science, 35(2), 1989.\n\nPattern databases. J Culberson, J Schae Er, Computational Intelligence. 143J. Culberson and J. Schae er. Pattern databases. Computational Intelligence, 14(3):319{333, 1998.\n\nSTRIPS: A new approach to the application of theorem proving to problem solving. R Fikes, N Nilsson, Arti cial Intelligence. 1R. Fikes and N. Nilsson. STRIPS: A new approach to the application of theorem proving to problem solving. Arti cial Intelligence, 1:27{120, 1971.\n\nFunctional strips: a more general language for planning and problem solving. Logic-based AI Workshop. H Ge, Washington D.C.H. Ge ner. Functional strips: a more general language for planning and problem solving. Logic-based AI Workshop, Washington D.C., 1999.\n\nLimited discrepancy search. W Harvey, M Ginsberg, Proceedings IJCAI-95. IJCAI-95Morgan KaufmannW. Harvey and M. Ginsberg. Limited discrepancy search. In Proceedings IJCAI-95, pages 607{613. Morgan Kaufmann, 1995.\n\nAdmissible heuristics for optimal planning. P Haslum, H Ge, Proceedings AIPS-2000. AIPS-2000AAAI PressTo AppearP. Haslum and H. Ge ner. Admissible heuristics for optimal planning. In Proceedings AIPS- 2000. AAAI Press, 2000. To Appear.\n\nA space-time tradeo for memory-based heuristics. R Holte, I Hernadvolgyi, Proceedings AAAI-99, pages 704{709. AAAI-99, pages 704{709Mit PressR. Holte and I. Hernadvolgyi. A space-time tradeo for memory-based heuristics. In Pro- ceedings AAAI-99, pages 704{709. Mit Press, 1999.\n\nConstraint satisfaction using constraint logic programming. P Van Hentenryck, H Simonis, M Dincbas, Arti cial Intelligence. 581{3P. Van Hentenryck, H. Simonis, and M. Dincbas. Constraint satisfaction using constraint logic programming. Arti cial Intelligence, 58(1{3):113{159, 1992.\n\nDomain-dependent single-agent search enhancements. A Junghanns, J Schae Er, Proceedings IJCAI-99. IJCAI-99Morgan KaufmannA. Junghanns and J. Schae er. Domain-dependent single-agent search enhancements. In Proceedings IJCAI-99. Morgan Kaufmann, 1999.\n\nExtending planning graphs to an ADL subset. J Koehler, B Nebel, J Ho Man, Y Dimopoulos, Proc. 4th European Conf. on Planning. 4th European Conf. on PlanningSpringer1248J. Koehler, B. Nebel, J. Ho man, and Y. Dimopoulos. Extending planning graphs to an ADL subset. In Proc. 4th European Conf. on Planning, volume LNAI 1248. Springer, 1997.\n\nDepth-rst iterative-deepening: an optimal admissible tree search. R Korf, Arti cial Intelligence. 271R. Korf. Depth-rst iterative-deepening: an optimal admissible tree search. Arti cial Intel- ligence, 27(1):97{109, 1985.\n\nReal-time heuristic search. R Korf, Arti cial Intelligence. 42R. Korf. Real-time heuristic search. Arti cial Intelligence, 42:189{211, 1990.\n\nLinear-space best-rst search. R Korf, Arti cial Intelligence. 62R. Korf. Linear-space best-rst search. Arti cial Intelligence, 62:41{78, 1993.\n\nFinding optimal solutions to Rubik's cube using pattern databases. R Korf, Proceedings of AAAI-98. AAAI-98R. Korf. Finding optimal solutions to Rubik's cube using pattern databases. In Proceedings of AAAI-98, pages 1202{1207, 1998.\n\nPushing the envelope: Planning, propositional logic, and stochastic search. H Kautz, B Selman, Proceedings of AAAI-96. AAAI-96H. Kautz and B. Selman. Pushing the envelope: Planning, propositional logic, and stochastic search. In Proceedings of AAAI-96, pages 1194{1201, 1996.\n\nUnifying SAT-based and Graph-based planning. H Kautz, B Selman, Proceedings IJCAI-99. IJCAI-99Morgan KaufmannH. Kautz and B. Selman. Unifying SAT-based and Graph-based planning. In Proceedings IJCAI-99. Morgan Kaufmann, 1999.\n\nFinding optimal solutions to the twenty-four puzzle. R Korf, L Taylor, Proceedings of AAAI-96. AAAI-96MIT PressR. Korf and L. Taylor. Finding optimal solutions to the twenty-four puzzle. In Proceedings of AAAI-96, pages 1202{1207. MIT Press, 1996.\n\nThe e cient implementation of the plan-graph. D Long, M Fox, JAIR. 10D. Long and M. Fox. The e cient implementation of the plan-graph. JAIR, 10:85{115, 1999.\n\nThe Traveling Salesman Problem : A Guided Tour of Combinatorial Optimization. E Lawler, A Rinnooy-Kan, WileyE. Lawler and A. Rinnooy-Kan, editors. The Traveling Salesman Problem : A Guided Tour of Combinatorial Optimization. Wiley, 1985.\n\nA heuristic estimator for means-ends analysis in planning. D Mcdermott, Proceedings AIPS-96. AIPS-96D. McDermott. A heuristic estimator for means-ends analysis in planning. In Proceedings AIPS-96, 1996.\n\nD Mcdermott, AIPS-98 Planning Competition Results. D. McDermott. AIPS-98 Planning Competition Results. http://ftp.cs.yale.edu/pub- /mcdermott/aipscomp-results.html, 1998.\n\nPDDL { the planning domain de nition language. D Mcdermott, D. McDermott. PDDL { the planning domain de nition language. Available at http://ftp.cs.yale.edu/pub/mcdermott, 1998.\n\nUsing regression-match graphs to control search in planning. D Mcdermott, Arti cial Intelligence. 1091-2D. McDermott. Using regression-match graphs to control search in planning. Arti cial Intelligence, 109(1-2):111{159, 1999.\n\nPrinciples of Arti cial Intelligence. N Nilsson, TiogaN. Nilsson. Principles of Arti cial Intelligence. Tioga, 1980.\n\nGPS: a program that simulates human thought. A Newell, H Simon, Computers and Thought. E. Feigenbaum and J. FeldmanMcGraw HillA. Newell and H. Simon. GPS: a program that simulates human thought. In E. Feigenbaum and J. Feldman, editors, Computers and Thought, pages 279{293. McGraw Hill, 1963.\n\nHuman Problem Solving. A Newell, H Simon, Prentice{HallEnglewood Cli s, NJA. Newell and H. Simon. Human Problem Solving. Prentice{Hall, Englewood Cli s, NJ, 1972.\n\nADL: Exploring the middle ground between Strips and the situation calculus. E Pednault, Proceedings KR-89. KR-89E. Pednault. ADL: Exploring the middle ground between Strips and the situation calculus. In Proceedings KR-89, pages 324{332, 1989.\n\nMachine discovery of e ective admissible heuristics. A Prieditis, Machine Learning. 12A. Prieditis. Machine discovery of e ective admissible heuristics. Machine Learning, 12:117{ 141, 1993.\n\nGRT: A domain independent heuristic for Strips worlds based on greedy regression tables. I Refanidis, I Vlahavas, Proceedings of ECP-99. ECP-99SpringerI. Refanidis and I. Vlahavas. GRT: A domain independent heuristic for Strips worlds based on greedy regression tables. In Proceedings of ECP-99. Springer, 1999.\n\nFast recursive formulations for BFS that allow controlled used of memory. A Sen, A Bagchi, Proceedings IJCAI-89. IJCAI-89A. Sen and A. Bagchi. Fast recursive formulations for BFS that allow controlled used of memory. In Proceedings IJCAI-89, pages 297{302, 1989.\n\nAn introduction to least commitment planning. AI Magazine. D Weld, D. Weld. An introduction to least commitment planning. AI Magazine, 1994.\n", "annotations": {"author": "[{\"end\":143,\"start\":32},{\"end\":241,\"start\":144}]", "publisher": null, "author_last_name": "[{\"end\":42,\"start\":37},{\"end\":154,\"start\":146}]", "author_first_name": "[{\"end\":36,\"start\":32},{\"end\":145,\"start\":144}]", "author_affiliation": "[{\"end\":142,\"start\":58},{\"end\":240,\"start\":156}]", "title": "[{\"end\":29,\"start\":1},{\"end\":270,\"start\":242}]", "venue": null, "abstract": "[{\"end\":1939,\"start\":272}]", "bib_ref": "[{\"end\":2080,\"start\":2065},{\"end\":2098,\"start\":2085},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2444,\"start\":2437},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2449,\"start\":2444},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2454,\"start\":2449},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2460,\"start\":2454},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2512,\"start\":2505},{\"end\":2569,\"start\":2566},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2577,\"start\":2570},{\"end\":2583,\"start\":2579},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2589,\"start\":2584},{\"end\":2603,\"start\":2595},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2609,\"start\":2604},{\"end\":2661,\"start\":2658},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2667,\"start\":2662},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2718,\"start\":2712},{\"end\":2724,\"start\":2718},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3375,\"start\":3369},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3381,\"start\":3375},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3779,\"start\":3773},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4086,\"start\":4081},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4148,\"start\":4143},{\"end\":4504,\"start\":4503},{\"end\":4803,\"start\":4802},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6009,\"start\":6004},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6404,\"start\":6399},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7679,\"start\":7674},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7685,\"start\":7679},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10432,\"start\":10426},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10438,\"start\":10432},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10695,\"start\":10689},{\"end\":11434,\"start\":11433},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17329,\"start\":17322},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18780,\"start\":18774},{\"end\":20559,\"start\":20558},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21209,\"start\":21202},{\"end\":30730,\"start\":30729},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30824,\"start\":30819},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":33219,\"start\":33213},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":33225,\"start\":33219},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":48988,\"start\":48983},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":49765,\"start\":49759},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":49771,\"start\":49765},{\"end\":56375,\"start\":56360},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":57191,\"start\":57185},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":58107,\"start\":58100},{\"end\":59448,\"start\":59433},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":62823,\"start\":62817},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":64156,\"start\":64151},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":64162,\"start\":64156},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":70054,\"start\":70048},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":71421,\"start\":71414},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":72175,\"start\":72169}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":65860,\"start\":65776},{\"attributes\":{\"id\":\"fig_1\"},\"end\":65963,\"start\":65861},{\"attributes\":{\"id\":\"fig_2\"},\"end\":67239,\"start\":65964},{\"attributes\":{\"id\":\"fig_3\"},\"end\":67415,\"start\":67240},{\"attributes\":{\"id\":\"fig_5\"},\"end\":67656,\"start\":67416},{\"attributes\":{\"id\":\"fig_6\"},\"end\":68018,\"start\":67657},{\"attributes\":{\"id\":\"fig_7\"},\"end\":69140,\"start\":68019},{\"attributes\":{\"id\":\"fig_8\"},\"end\":69307,\"start\":69141},{\"attributes\":{\"id\":\"fig_9\"},\"end\":69469,\"start\":69308},{\"attributes\":{\"id\":\"fig_10\"},\"end\":69650,\"start\":69470},{\"attributes\":{\"id\":\"fig_11\"},\"end\":69771,\"start\":69651},{\"attributes\":{\"id\":\"fig_12\"},\"end\":70773,\"start\":69772}]", "paragraph": "[{\"end\":2461,\"start\":1955},{\"end\":3212,\"start\":2463},{\"end\":3613,\"start\":3214},{\"end\":5839,\"start\":3615},{\"end\":6328,\"start\":5841},{\"end\":6815,\"start\":6330},{\"end\":7298,\"start\":6817},{\"end\":9210,\"start\":7315},{\"end\":10090,\"start\":9237},{\"end\":10890,\"start\":10092},{\"end\":12187,\"start\":10905},{\"end\":12374,\"start\":12189},{\"end\":12379,\"start\":12376},{\"end\":12936,\"start\":12381},{\"end\":13252,\"start\":12938},{\"end\":13706,\"start\":13254},{\"end\":13953,\"start\":13708},{\"end\":14477,\"start\":14000},{\"end\":14622,\"start\":14479},{\"end\":15240,\"start\":14624},{\"end\":16496,\"start\":15298},{\"end\":17105,\"start\":16498},{\"end\":17377,\"start\":17107},{\"end\":18185,\"start\":17415},{\"end\":18585,\"start\":18187},{\"end\":18685,\"start\":18587},{\"end\":19162,\"start\":18687},{\"end\":19920,\"start\":19178},{\"end\":20281,\"start\":19922},{\"end\":21070,\"start\":20283},{\"end\":21503,\"start\":21072},{\"end\":22019,\"start\":21520},{\"end\":22792,\"start\":22021},{\"end\":23558,\"start\":22806},{\"end\":24223,\"start\":23560},{\"end\":24708,\"start\":24235},{\"end\":25194,\"start\":24710},{\"end\":26089,\"start\":25196},{\"end\":27831,\"start\":26104},{\"end\":29674,\"start\":27833},{\"end\":30379,\"start\":29676},{\"end\":31089,\"start\":30381},{\"end\":32326,\"start\":31091},{\"end\":32995,\"start\":32328},{\"end\":33879,\"start\":33022},{\"end\":34741,\"start\":33881},{\"end\":35237,\"start\":34743},{\"end\":35559,\"start\":35251},{\"end\":35897,\"start\":35561},{\"end\":36595,\"start\":35909},{\"end\":37047,\"start\":36597},{\"end\":37382,\"start\":37049},{\"end\":37686,\"start\":37384},{\"end\":38137,\"start\":37688},{\"end\":39090,\"start\":38139},{\"end\":39177,\"start\":39092},{\"end\":40003,\"start\":39179},{\"end\":40891,\"start\":40017},{\"end\":42446,\"start\":40907},{\"end\":43142,\"start\":42470},{\"end\":44435,\"start\":43167},{\"end\":44744,\"start\":44437},{\"end\":47363,\"start\":44775},{\"end\":48541,\"start\":47418},{\"end\":48888,\"start\":48543},{\"end\":49565,\"start\":48890},{\"end\":50345,\"start\":49567},{\"end\":52221,\"start\":50347},{\"end\":52590,\"start\":52223},{\"end\":53059,\"start\":52592},{\"end\":53820,\"start\":53088},{\"end\":54654,\"start\":53822},{\"end\":55279,\"start\":54656},{\"end\":56275,\"start\":55281},{\"end\":56991,\"start\":56277},{\"end\":58305,\"start\":56993},{\"end\":59064,\"start\":58307},{\"end\":60193,\"start\":59112},{\"end\":60615,\"start\":60195},{\"end\":61565,\"start\":60617},{\"end\":62241,\"start\":61581},{\"end\":62627,\"start\":62243},{\"end\":63364,\"start\":62629},{\"end\":64446,\"start\":63366},{\"end\":65207,\"start\":64448},{\"end\":65775,\"start\":65209}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13999,\"start\":13954}]", "table_ref": "[{\"end\":16709,\"start\":16694}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1953,\"start\":1941},{\"attributes\":{\"n\":\"2\"},\"end\":7313,\"start\":7301},{\"attributes\":{\"n\":\"3\"},\"end\":9235,\"start\":9213},{\"attributes\":{\"n\":\"4\"},\"end\":10903,\"start\":10893},{\"attributes\":{\"n\":\"5\"},\"end\":15265,\"start\":15243},{\"attributes\":{\"n\":\"5.1\"},\"end\":15296,\"start\":15268},{\"attributes\":{\"n\":\"5.2\"},\"end\":17413,\"start\":17380},{\"attributes\":{\"n\":\"5.3\"},\"end\":19176,\"start\":19165},{\"end\":21518,\"start\":21506},{\"end\":22804,\"start\":22795},{\"end\":24233,\"start\":24226},{\"end\":26102,\"start\":26092},{\"attributes\":{\"n\":\"6.1\"},\"end\":33020,\"start\":32998},{\"attributes\":{\"n\":\"6.2\"},\"end\":35249,\"start\":35240},{\"attributes\":{\"n\":\"6.3\"},\"end\":35907,\"start\":35900},{\"attributes\":{\"n\":\"6.4\"},\"end\":40015,\"start\":40006},{\"attributes\":{\"n\":\"6.5\"},\"end\":40905,\"start\":40894},{\"end\":42458,\"start\":42449},{\"end\":42468,\"start\":42461},{\"end\":43165,\"start\":43145},{\"attributes\":{\"n\":\"6.6\"},\"end\":44773,\"start\":44747},{\"attributes\":{\"n\":\"6.7\"},\"end\":47416,\"start\":47366},{\"attributes\":{\"n\":\"7.2\"},\"end\":53086,\"start\":53062},{\"attributes\":{\"n\":\"7.3\"},\"end\":59110,\"start\":59067},{\"attributes\":{\"n\":\"8\"},\"end\":61579,\"start\":61568},{\"end\":65787,\"start\":65777},{\"end\":65872,\"start\":65862},{\"end\":65985,\"start\":65965},{\"end\":67261,\"start\":67241},{\"end\":67427,\"start\":67417},{\"end\":69152,\"start\":69142},{\"end\":69319,\"start\":69309},{\"end\":69482,\"start\":69471},{\"end\":69663,\"start\":69652}]", "table": null, "figure_caption": "[{\"end\":65860,\"start\":65789},{\"end\":65963,\"start\":65874},{\"end\":67239,\"start\":65988},{\"end\":67415,\"start\":67264},{\"end\":67656,\"start\":67429},{\"end\":68018,\"start\":67659},{\"end\":69140,\"start\":68021},{\"end\":69307,\"start\":69154},{\"end\":69469,\"start\":69321},{\"end\":69650,\"start\":69485},{\"end\":69771,\"start\":69666},{\"end\":70773,\"start\":69774}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21897,\"start\":21891},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23607,\"start\":23601},{\"end\":24791,\"start\":24785},{\"end\":26305,\"start\":26299},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28724,\"start\":28718},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29710,\"start\":29704},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":41843,\"start\":41837},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":42488,\"start\":42482},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43209,\"start\":43202},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46903,\"start\":46896}]", "bib_author_first_name": "[{\"end\":73574,\"start\":73573},{\"end\":73583,\"start\":73582},{\"end\":73595,\"start\":73594},{\"end\":73739,\"start\":73738},{\"end\":73751,\"start\":73750},{\"end\":73760,\"start\":73759},{\"end\":73972,\"start\":73971},{\"end\":73984,\"start\":73983},{\"end\":74177,\"start\":74176},{\"end\":74185,\"start\":74184},{\"end\":74376,\"start\":74375},{\"end\":74385,\"start\":74384},{\"end\":74610,\"start\":74609},{\"end\":74621,\"start\":74620},{\"end\":74817,\"start\":74816},{\"end\":74826,\"start\":74825},{\"end\":74838,\"start\":74837},{\"end\":75088,\"start\":75087},{\"end\":75099,\"start\":75098},{\"end\":75258,\"start\":75257},{\"end\":75271,\"start\":75270},{\"end\":75494,\"start\":75493},{\"end\":75503,\"start\":75502},{\"end\":75788,\"start\":75787},{\"end\":75974,\"start\":75973},{\"end\":75984,\"start\":75983},{\"end\":76204,\"start\":76203},{\"end\":76214,\"start\":76213},{\"end\":76446,\"start\":76445},{\"end\":76455,\"start\":76454},{\"end\":76736,\"start\":76735},{\"end\":76754,\"start\":76753},{\"end\":76765,\"start\":76764},{\"end\":77011,\"start\":77010},{\"end\":77024,\"start\":77023},{\"end\":77255,\"start\":77254},{\"end\":77266,\"start\":77265},{\"end\":77275,\"start\":77274},{\"end\":77285,\"start\":77284},{\"end\":77617,\"start\":77616},{\"end\":77802,\"start\":77801},{\"end\":77946,\"start\":77945},{\"end\":78127,\"start\":78126},{\"end\":78369,\"start\":78368},{\"end\":78378,\"start\":78377},{\"end\":78615,\"start\":78614},{\"end\":78624,\"start\":78623},{\"end\":78850,\"start\":78849},{\"end\":78858,\"start\":78857},{\"end\":79092,\"start\":79091},{\"end\":79100,\"start\":79099},{\"end\":79283,\"start\":79282},{\"end\":79293,\"start\":79292},{\"end\":79503,\"start\":79502},{\"end\":79648,\"start\":79647},{\"end\":79867,\"start\":79866},{\"end\":80060,\"start\":80059},{\"end\":80265,\"start\":80264},{\"end\":80390,\"start\":80389},{\"end\":80400,\"start\":80399},{\"end\":80663,\"start\":80662},{\"end\":80673,\"start\":80672},{\"end\":80880,\"start\":80879},{\"end\":81102,\"start\":81101},{\"end\":81329,\"start\":81328},{\"end\":81342,\"start\":81341},{\"end\":81627,\"start\":81626},{\"end\":81634,\"start\":81633},{\"end\":81876,\"start\":81875}]", "bib_author_last_name": "[{\"end\":73580,\"start\":73575},{\"end\":73592,\"start\":73584},{\"end\":73601,\"start\":73596},{\"end\":73748,\"start\":73740},{\"end\":73757,\"start\":73752},{\"end\":73765,\"start\":73761},{\"end\":73981,\"start\":73973},{\"end\":73989,\"start\":73985},{\"end\":74182,\"start\":74178},{\"end\":74191,\"start\":74186},{\"end\":74382,\"start\":74377},{\"end\":74388,\"start\":74386},{\"end\":74618,\"start\":74611},{\"end\":74629,\"start\":74622},{\"end\":74823,\"start\":74818},{\"end\":74835,\"start\":74827},{\"end\":74841,\"start\":74839},{\"end\":75096,\"start\":75089},{\"end\":75106,\"start\":75100},{\"end\":75268,\"start\":75259},{\"end\":75280,\"start\":75272},{\"end\":75500,\"start\":75495},{\"end\":75511,\"start\":75504},{\"end\":75791,\"start\":75789},{\"end\":75981,\"start\":75975},{\"end\":75993,\"start\":75985},{\"end\":76211,\"start\":76205},{\"end\":76217,\"start\":76215},{\"end\":76452,\"start\":76447},{\"end\":76468,\"start\":76456},{\"end\":76751,\"start\":76737},{\"end\":76762,\"start\":76755},{\"end\":76773,\"start\":76766},{\"end\":77021,\"start\":77012},{\"end\":77033,\"start\":77025},{\"end\":77263,\"start\":77256},{\"end\":77272,\"start\":77267},{\"end\":77282,\"start\":77276},{\"end\":77296,\"start\":77286},{\"end\":77622,\"start\":77618},{\"end\":77807,\"start\":77803},{\"end\":77951,\"start\":77947},{\"end\":78132,\"start\":78128},{\"end\":78375,\"start\":78370},{\"end\":78385,\"start\":78379},{\"end\":78621,\"start\":78616},{\"end\":78631,\"start\":78625},{\"end\":78855,\"start\":78851},{\"end\":78865,\"start\":78859},{\"end\":79097,\"start\":79093},{\"end\":79104,\"start\":79101},{\"end\":79290,\"start\":79284},{\"end\":79305,\"start\":79294},{\"end\":79513,\"start\":79504},{\"end\":79658,\"start\":79649},{\"end\":79877,\"start\":79868},{\"end\":80070,\"start\":80061},{\"end\":80273,\"start\":80266},{\"end\":80397,\"start\":80391},{\"end\":80406,\"start\":80401},{\"end\":80670,\"start\":80664},{\"end\":80679,\"start\":80674},{\"end\":80889,\"start\":80881},{\"end\":81112,\"start\":81103},{\"end\":81339,\"start\":81330},{\"end\":81351,\"start\":81343},{\"end\":81631,\"start\":81628},{\"end\":81641,\"start\":81635},{\"end\":81881,\"start\":81877}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":73703,\"start\":73573},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1936953},\"end\":73915,\"start\":73705},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2346572},\"end\":74127,\"start\":73917},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":11518222},\"end\":74330,\"start\":74129},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5527200},\"end\":74535,\"start\":74332},{\"attributes\":{\"id\":\"b5\"},\"end\":74755,\"start\":74537},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1104560},\"end\":75038,\"start\":74757},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":123510822},\"end\":75236,\"start\":75040},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2074622},\"end\":75410,\"start\":75238},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8623866},\"end\":75683,\"start\":75412},{\"attributes\":{\"id\":\"b10\"},\"end\":75943,\"start\":75685},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":623054},\"end\":76157,\"start\":75945},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":196186},\"end\":76394,\"start\":76159},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12981356},\"end\":76673,\"start\":76396},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6119968},\"end\":76957,\"start\":76675},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":5963378},\"end\":77208,\"start\":76959},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9396158},\"end\":77548,\"start\":77210},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":228404709},\"end\":77771,\"start\":77550},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11749385},\"end\":77913,\"start\":77773},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":207508052},\"end\":78057,\"start\":77915},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1620991},\"end\":78290,\"start\":78059},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5740182},\"end\":78567,\"start\":78292},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8555488},\"end\":78794,\"start\":78569},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8201647},\"end\":79043,\"start\":78796},{\"attributes\":{\"id\":\"b24\"},\"end\":79202,\"start\":79045},{\"attributes\":{\"id\":\"b25\"},\"end\":79441,\"start\":79204},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":614762},\"end\":79645,\"start\":79443},{\"attributes\":{\"id\":\"b27\"},\"end\":79817,\"start\":79647},{\"attributes\":{\"id\":\"b28\"},\"end\":79996,\"start\":79819},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10996197},\"end\":80224,\"start\":79998},{\"attributes\":{\"id\":\"b30\"},\"end\":80342,\"start\":80226},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14328775},\"end\":80637,\"start\":80344},{\"attributes\":{\"id\":\"b32\"},\"end\":80801,\"start\":80639},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8515044},\"end\":81046,\"start\":80803},{\"attributes\":{\"id\":\"b34\"},\"end\":81237,\"start\":81048},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":17793266},\"end\":81550,\"start\":81239},{\"attributes\":{\"id\":\"b36\"},\"end\":81814,\"start\":81552},{\"attributes\":{\"id\":\"b37\"},\"end\":81956,\"start\":81816}]", "bib_title": "[{\"end\":73736,\"start\":73705},{\"end\":73969,\"start\":73917},{\"end\":74174,\"start\":74129},{\"end\":74373,\"start\":74332},{\"end\":74814,\"start\":74757},{\"end\":75085,\"start\":75040},{\"end\":75255,\"start\":75238},{\"end\":75491,\"start\":75412},{\"end\":75971,\"start\":75945},{\"end\":76201,\"start\":76159},{\"end\":76443,\"start\":76396},{\"end\":76733,\"start\":76675},{\"end\":77008,\"start\":76959},{\"end\":77252,\"start\":77210},{\"end\":77614,\"start\":77550},{\"end\":77799,\"start\":77773},{\"end\":77943,\"start\":77915},{\"end\":78124,\"start\":78059},{\"end\":78366,\"start\":78292},{\"end\":78612,\"start\":78569},{\"end\":78847,\"start\":78796},{\"end\":79089,\"start\":79045},{\"end\":79500,\"start\":79443},{\"end\":80057,\"start\":79998},{\"end\":80387,\"start\":80344},{\"end\":80877,\"start\":80803},{\"end\":81099,\"start\":81048},{\"end\":81326,\"start\":81239},{\"end\":81624,\"start\":81552}]", "bib_author": "[{\"end\":73582,\"start\":73573},{\"end\":73594,\"start\":73582},{\"end\":73603,\"start\":73594},{\"end\":73750,\"start\":73738},{\"end\":73759,\"start\":73750},{\"end\":73767,\"start\":73759},{\"end\":73983,\"start\":73971},{\"end\":73991,\"start\":73983},{\"end\":74184,\"start\":74176},{\"end\":74193,\"start\":74184},{\"end\":74384,\"start\":74375},{\"end\":74390,\"start\":74384},{\"end\":74620,\"start\":74609},{\"end\":74631,\"start\":74620},{\"end\":74825,\"start\":74816},{\"end\":74837,\"start\":74825},{\"end\":74843,\"start\":74837},{\"end\":75098,\"start\":75087},{\"end\":75108,\"start\":75098},{\"end\":75270,\"start\":75257},{\"end\":75282,\"start\":75270},{\"end\":75502,\"start\":75493},{\"end\":75513,\"start\":75502},{\"end\":75793,\"start\":75787},{\"end\":75983,\"start\":75973},{\"end\":75995,\"start\":75983},{\"end\":76213,\"start\":76203},{\"end\":76219,\"start\":76213},{\"end\":76454,\"start\":76445},{\"end\":76470,\"start\":76454},{\"end\":76753,\"start\":76735},{\"end\":76764,\"start\":76753},{\"end\":76775,\"start\":76764},{\"end\":77023,\"start\":77010},{\"end\":77035,\"start\":77023},{\"end\":77265,\"start\":77254},{\"end\":77274,\"start\":77265},{\"end\":77284,\"start\":77274},{\"end\":77298,\"start\":77284},{\"end\":77624,\"start\":77616},{\"end\":77809,\"start\":77801},{\"end\":77953,\"start\":77945},{\"end\":78134,\"start\":78126},{\"end\":78377,\"start\":78368},{\"end\":78387,\"start\":78377},{\"end\":78623,\"start\":78614},{\"end\":78633,\"start\":78623},{\"end\":78857,\"start\":78849},{\"end\":78867,\"start\":78857},{\"end\":79099,\"start\":79091},{\"end\":79106,\"start\":79099},{\"end\":79292,\"start\":79282},{\"end\":79307,\"start\":79292},{\"end\":79515,\"start\":79502},{\"end\":79660,\"start\":79647},{\"end\":79879,\"start\":79866},{\"end\":80072,\"start\":80059},{\"end\":80275,\"start\":80264},{\"end\":80399,\"start\":80389},{\"end\":80408,\"start\":80399},{\"end\":80672,\"start\":80662},{\"end\":80681,\"start\":80672},{\"end\":80891,\"start\":80879},{\"end\":81114,\"start\":81101},{\"end\":81341,\"start\":81328},{\"end\":81353,\"start\":81341},{\"end\":81633,\"start\":81626},{\"end\":81643,\"start\":81633},{\"end\":81883,\"start\":81875}]", "bib_venue": "[{\"end\":73795,\"start\":73788},{\"end\":74019,\"start\":74012},{\"end\":74419,\"start\":74413},{\"end\":74874,\"start\":74867},{\"end\":76025,\"start\":76017},{\"end\":76251,\"start\":76242},{\"end\":76528,\"start\":76506},{\"end\":77065,\"start\":77057},{\"end\":77366,\"start\":77336},{\"end\":78165,\"start\":78158},{\"end\":78418,\"start\":78411},{\"end\":78663,\"start\":78655},{\"end\":78898,\"start\":78891},{\"end\":79543,\"start\":79536},{\"end\":80915,\"start\":80910},{\"end\":81382,\"start\":81376},{\"end\":81673,\"start\":81665},{\"end\":73616,\"start\":73603},{\"end\":73786,\"start\":73767},{\"end\":74010,\"start\":73991},{\"end\":74215,\"start\":74193},{\"end\":74411,\"start\":74390},{\"end\":74607,\"start\":74537},{\"end\":74865,\"start\":74843},{\"end\":75126,\"start\":75108},{\"end\":75308,\"start\":75282},{\"end\":75535,\"start\":75513},{\"end\":75785,\"start\":75685},{\"end\":76015,\"start\":75995},{\"end\":76240,\"start\":76219},{\"end\":76504,\"start\":76470},{\"end\":76797,\"start\":76775},{\"end\":77055,\"start\":77035},{\"end\":77334,\"start\":77298},{\"end\":77646,\"start\":77624},{\"end\":77831,\"start\":77809},{\"end\":77975,\"start\":77953},{\"end\":78156,\"start\":78134},{\"end\":78409,\"start\":78387},{\"end\":78653,\"start\":78633},{\"end\":78889,\"start\":78867},{\"end\":79110,\"start\":79106},{\"end\":79280,\"start\":79204},{\"end\":79534,\"start\":79515},{\"end\":79696,\"start\":79660},{\"end\":79864,\"start\":79819},{\"end\":80094,\"start\":80072},{\"end\":80262,\"start\":80226},{\"end\":80429,\"start\":80408},{\"end\":80660,\"start\":80639},{\"end\":80908,\"start\":80891},{\"end\":81130,\"start\":81114},{\"end\":81374,\"start\":81353},{\"end\":81663,\"start\":81643},{\"end\":81873,\"start\":81816}]"}}}, "year": 2023, "month": 12, "day": 17}