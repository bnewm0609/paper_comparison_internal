{"id": 221586462, "updated": "2023-10-06 11:17:47.04", "metadata": {"title": "Self-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism", "authors": "[{\"first\":\"Levi\",\"last\":\"McClenny\",\"middle\":[]},{\"first\":\"Ulisses\",\"last\":\"Braga-Neto\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Physics-Informed Neural Networks (PINNs) have emerged recently as a promising application of deep neural networks to the numerical solution of nonlinear partial differential equations (PDEs). However, it has been recognized that adaptive procedures are needed to force the neural network to fit accurately the stubborn spots in the solution of\"stiff\"PDEs. In this paper, we propose a fundamentally new way to train PINNs adaptively, where the adaptation weights are fully trainable and applied to each training point individually, so the neural network learns autonomously which regions of the solution are difficult and is forced to focus on them. The self-adaptation weights specify a soft multiplicative soft attention mask, which is reminiscent of similar mechanisms used in computer vision. The basic idea behind these SA-PINNs is to make the weights increase as the corresponding losses increase, which is accomplished by training the network to simultaneously minimize the losses and maximize the weights. We show how to build a continuous map of self-adaptive weights using Gaussian Process regression, which allows the use of stochastic gradient descent in problems where conventional gradient descent is not enough to produce accurate solutions. Finally, we derive the Neural Tangent Kernel matrix for SA-PINNs and use it to obtain a heuristic understanding of the effect of the self-adaptive weights on the dynamics of training in the limiting case of infinitely-wide PINNs, which suggests that SA-PINNs work by producing a smooth equalization of the eigenvalues of the NTK matrix corresponding to the different loss terms. In numerical experiments with several linear and nonlinear benchmark problems, the SA-PINN outperformed other state-of-the-art PINN algorithm in L2 error, while using a smaller number of training epochs.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2009.04544", "mag": "3084353952", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaaiss/McClennyB21", "doi": null}}, "content": {"source": {"pdf_hash": "f084bcbe162f16b1e2c83e1dfa9ff02c4fc7f7c0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2009.04544v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e8cf8f7e55e6691bcc131973b8786fe2919de6c7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f084bcbe162f16b1e2c83e1dfa9ff02c4fc7f7c0.txt", "contents": "\nSelf-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism\n6 Apr 2022\n\nLevi D Mcclenny levimcclenny@tamu.edu \nDepartment of Electrical and Computer Engineering\nTexas A&M University College Station\nTX USA\n\nUlisses Braga-Neto ulisses@tamu.edu \nDepartment of Electrical and Computer Engineering\nTexas A&M University College Station\nTX USA\n\nSelf-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism\n6 Apr 2022Preprint. Under review.\nPhysics-Informed Neural Networks (PINNs) have emerged recently as a promising application of deep neural networks to the numerical solution of nonlinear partial differential equations (PDEs). However, it has been recognized that adaptive procedures are needed to force the neural network to fit accurately the stubborn spots in the solution of \"stiff\" PDEs. In this paper, we propose a fundamentally new way to train PINNs adaptively, where the adaptation weights are fully trainable and applied to each training point individually, so the neural network learns autonomously which regions of the solution are difficult and is forced to focus on them. The self-adaptation weights specify a soft multiplicative soft attention mask, which is reminiscent of similar mechanisms used in computer vision. The basic idea behind these SA-PINNs is to make the weights increase as the corresponding losses increase, which is accomplished by training the network to simultaneously minimize the losses and maximize the weights. We show how to build a continuous map of self-adaptive weights using Gaussian Process regression, which allows the use of stochastic gradient descent in problems where conventional gradient descent is not enough to produce accurate solutions. Finally, we derive the Neural Tangent Kernel matrix for SA-PINNs and use it to obtain a heuristic understanding of the effect of the self-adaptive weights on the dynamics of training in the limiting case of infinitely-wide PINNs, which suggests that SA-PINNs work by producing a smooth equalization of the eigenvalues of the NTK matrix corresponding to the different loss terms. In numerical experiments with several linear and nonlinear benchmark problems, the SA-PINN outperformed other state-of-the-art PINN algorithm in L2 error, while using a smaller number of training epochs.As part of the burgeoning field of scientific machine learning [1], physics-informed neural networks (PINNs) have emerged recently as an alternative to traditional numerical methods for partial different equations (PDE)[2,3,4,5]. Typical data-driven deep learning methodologies do not take into account physical understanding of the problem domain. The PINN approach is based on a strong physics prior that constrains the output of a deep neural network by means of a system of PDEs. The potential of using neural networks as universal function approximators to solve PDEs had been recognized since the 1990's[6]. However, PINNs promise to take this approach to a different level by using deep neural networks, which is made possible by the vast advances in computational capabilities and training algorithms since that time[7,8], as well as the availability of automatic differentiation methods [9, 10].A great advantage of PINNs over traditional time-stepping PDE solvers is that it is possible to obtain the solution over the entire spatial-temporal domain at once, using training points distributed *\n\nirregularly across the domain, obviating the need of constructing computationally-expensive grids. In addition, the PINN solution defines a function over the continuous domain, rather that a discrete solution on a grid as in traditional methods. Finally, PINNs allow sample data assimilation in a natural and efficient way.\n\nThe continuous PINN algorithm proposed in [2], henceforth referred to as the \"baseline PINN\" algorithm, is effective at estimating solutions that are reasonably smooth with simple boundary conditions, such as the viscous Burgers, Poisson and Schr\u00f6dinger PDEs. On the other hand, it has been observed that the baseline PINN has convergence and accuracy problems when solving \"stiff\" PDEs [11], with solutions that contain sharp space transitions or fast time evolution [12]. This is known to be the case, for example, when attempting to solve the nonlinear Allen-Cahn equation with the baseline PINN [4]. As we will see in this paper, this may occur even in the case of the linear wave and advection PDEs.\n\nThis paper introduces Self-Adaptive PINNs (SA-PINNs), a fundamentally new method to train PINNs adaptively, which addresses the issues mentioned previously. SA-PINNs applies trainable weights on each training point, in a way that is reminiscent of soft multiplicative attention masks used in computer vision [13,14]. The adaptation weights are trained concurrently with the network weights. As a result, initial, boundary or residue points in difficult regions of the solution are automatically weighted more in the loss function, forcing the approximation to improve on those points. The basic principle in SA-PINNs is to make the weights increase as the corresponding losses do, which is accomplished by training the network to simultaneously minimize the losses and maximize the weights, i.e., to find a saddle point in the cost surface. We show that this is formally equivalent to a penalty-based solution of PDE-constrained optimization methods. Experimental results show that self-adaptive PINNs can solve a \"stiff\" Allen-Cahn PDE with significantly better accuracy than other state-of-the-art PINN algorithms, while using a smaller number of training epochs. Results obtained with the viscous Burgers and Helmholtz PDEs confirm the trends observed in the Allen-Cahn experiments.\n\nWe also propose a methodology to build a continuous map of self-adaptive weights based on Gaussian Process regression, in order to allow the use of stochastic gradient descent in training self-adaptive PINNs. This is illustrated by application to a 1-D wave PDE that is challenging to non-SGD training.\n\nFinally, we derive the Neural Tangent Kernel matrix for self-adaptive PINNs and use it to obtain a heuristic understanding of the effect of the self-adaptive weights on the dynamics of training in the limiting case of infinitely-wide PINNs. We examine the effect of the self-adaptive weights on the eigenvalues of the NTK matrix in the solution of a linear advection PDE, and observe that it not only equalizes the magnitudes between the different loss components, but also smooths the shape of the distribution of eigenvalues.\n\n\nBackground\n\n\nPhysics-Informed Neural Networks\n\nConsider the initial-boundary value problem:\nN x,t [u(x, t)] = f (x, t) , x \u2208 \u2126 , t \u2208 (0, T ] ,(1)B x,t [u(x, t)] = g(x, t) , x \u2208 \u2202\u2126, t \u2208 (0, T ] ,(2)u(x, 0) = h(x) , x \u2208 \u2126 .(3)\nHere, the domain \u2126 \u2282 R d in a open set, \u2126 is its closure, u : \u2126 \u00d7 [0, T ] \u2192 R is the desired solution,\n\nx \u2208 \u2126 is a spatial vector variable, t is time, and N x,t and B x,t are spatial-temporal differential operators. The problem data is provided by the forcing function f : \u2126 \u2192 R, the boundary condition function g : \u2202\u2126 \u00d7 (0, T ], and the initial condition function h : \u2126 \u2192 R. Additionally, sensor data in the interior of the domain may be available. In any case, we assume that the data are sufficient and appropriate for a well-posed problem. Time-independent problems and other types of data can be handled similarly, so we will use the equations (1)-(3) as a model.\n\nFollowing [2], let u(x, t) be approximated by the output u(x, t; w) of a deep neural network with inputs x and t (in the case of a PDE system, this would be a neural network with multiple outputs). The value of N x,t [u(x, t; w)] and B x,t [u(x, t; w)] can be computed quickly and accurately using reverse-mode automatic differentiation [9,10].\n\nThe network weights w are trained by minimizing a loss function that penalizes the output for not satisfying (1)- (3):\nL(w) = L s (w) + L r (w) + L b (w) + L 0 (w) ,(4)\nwhere L s is the loss term corresponding to sample data (if any), while L r , L b , and L 0 are loss terms corresponding to not satisfying the PDE (1), the boundary condition (2), and the initial condition (51), respectively:\nL s (w) = 1 2 Ns i=1 |u(x i s , t i s ; w) \u2212 y i s | 2 ,(5)L r (w) = 1 2 Nr i=1 |N x,t [u(x i r , t i r ; w)] \u2212 f (x i r , t i r )| 2 ,(6)L b (w) = 1 2 N b i=1 |B x,t [u(x i b , t i b ; w)] \u2212 g(x i b , t i b )| 2 ,(7)L 0 (w) = 1 2 N0 i=1 |u(x i 0 , 0; w) \u2212 h(x i 0 )| 2 . (8) where {x i s , t i s , y i s } Ns i=1 are sensor data (if any), {x i 0 } N0 i=1 are initial condition points, {x i b , t i b } N b i=1 are boundary condition points, {x i r , t i r } Nr i=1\nare residue (\"collocation\") points randomly distributed in the domain \u2126, and N s , N 0 , N b and N r denote the total number of sensor, initial, boundary, and residue points, respectively. The network weights w can be tuned by minimizing the total training loss L(w) via standard gradient descent procedures used in deep learning.\n\n\nRelated Work\n\nThe baseline PINN algorithm described in the previous section, though remarkably successful in the solution of many linear and nonlinear PDEs, can produce inaccurate approximations, or fail to converge entirely, in the solution of certain \"stiff\" PDEs. A large amount of evidence has accumulated indicating that this happens due to the shortcomings of gradient descent applied to the multi-part or multi-objective loss function (4); e.g., see [4,15,12,5]. This occurs because gradient descent is a greedy procedure that may latch on some of the components at the expense of others, which creates imbalance in the rate of descent among the different loss components and prevents convergence to the correct solution. The standard approach in the literature of PINNs to try to correct the imbalance is the introduction of weights in (4):\nL(w) = \u03bb s L s (w) + \u03bb r L r (w) + \u03bb b L b (w) + \u03bb 0 L 0 (w) ,(9)\nSeveral methods, from very simple to complex, have been advanced to set the values of these weights; we mention a few below.\n\nNonadaptive Weighting In [4], it was pointed out that a premium should be put on forcing the neural network to satisfy the initial conditions closely, especially for PDEs describing time-irreversible processes, where the solution has to be approximated well early. Accordingly, a loss function of the form\nL(\u03b8) = L r (\u03b8) + L b (\u03b8) + C L 0 (\u03b8) was suggested, where C 1 is a hyperparameter.\nLearning Rate Annealing In [12], it is argued that the optimal value of the weight C in the previous scheme may vary wildly among different PDEs so that choosing its value would be difficult. Instead they propose to use weights that are tuned during training using statistics of the backpropagated gradients of the loss function. It is noteworthy that the weights themselves are not adjusted by backpropagation. Instead, they behave as learning rate coefficients, which are updated after each epoch of training.\n\nNeural Tangent Kernel (NTK) Weighting Recently, [5] derived the NTK kernel matrix for PINNs, and used a heuristic argument to set the weights adaptively based on the evolution of the eigenvalues of the NTK matrix during training.\n\nMimimax Weighting In [16], a methodology was proposed to update the weights during training using gradient descent for the network weights, and gradient ascent for the loss weights, seeking to find a saddle point in weight space. Loss components that do not decrease are assigned larger weights.\n\nMore generally, the need to use weighting to correct imbalance in multi-part loss functions has been recognized in the general deep learning literature [17,18,19]. Note that the multi-part loss (9) corresponds to a linear scalarization of this multiple-objective problem [20].\n\nAll the previous methods employ a linearly-scalarized function such as (9), the only difference among them being the way the weights are updated. The self-adaptive weighting method proposed in this paper is fundamentally different in that the weights apply to individual training points in the different loss components, rather than the entire loss component. The previous methods can be seen as a special case of this, when all self-adaptive weights for a particular loss component are updated in tandem. Among the previous methods, the independently-developed Minimax weighting scheme [16] is the closest to SA-PINNs, as it also updates its weights via gradient ascent; however, these weights still apply to the whole loss components. This paper presents empirical and theoretical evidence that having the flexibility of weighting each training point in the various loss terms brings additional flexibility that can lead to better performance.\n\n\nSelf-Adaptive Physics-Informed Neural Networks\n\nWhile previously proposed weighting methods produce improvements in stability and accuracy over the baseline PINN, they are either nonadaptive or introduce inflexible adaptation. Here we propose a simple procedure that applies fully-trainable weights to produce a multiplicative soft attention mask, in a manner that is reminiscent of attention mechanisms used in computer vision [13,14]. Instead of hard-coding weights at particular regions of the solution, the proposed method is in agreement with the neural network philosophy of self-adaptation, where the weights in the loss function are updated by gradient descent side-by-side with the network weights.\n\nUsing the PDE in (1)-(3) as reference, the proposed self-adaptive PINN utilizes the following loss function\nL(w, \u03bb r , \u03bb b , \u03bb 0 ) = L s (w) + L r (w, \u03bb r ) + L b (w, \u03bb b ) + L 0 (w, \u03bb 0 ) ,(10)\nwhere \u03bb r = (\u03bb 1 r , . . . , \u03bb Nr r ), \u03bb b = (\u03bb 1 b , . . . , \u03bb N b b ), and \u03bb 0 = (\u03bb 1 0 , . . . , \u03bb N0 0 ) are trainable, nonnegative self-adaptation weights for the initial, boundary, and residue points, respectively, and\nL r (w, \u03bb r ) = 1 2 Nr i=1 m(\u03bb i r ) |N x,t [u(x i r , t i r ; w)] \u2212 f (x i r , t i r )| 2 (11) L b (w, \u03bb b ) = 1 2 N b i=1 m(\u03bb i b ) |B x,t [u(x i r , t i r ; w)] \u2212 g(x i b , t i b )| 2 (12) L 0 (w, \u03bb 0 ) = 1 2 N0 i=1 m(\u03bb i 0 ) |u(x i 0 , 0; w) \u2212 h(x i 0 )| 2 .(13)\nwhere the self-adaptation mask function m(\u03bb) defined on [0, \u221e) is a nonnegative, differentiable, strictly increasing function of \u03bb. A key feature of self-adaptive PINNs is that the loss L(w, \u03bb r , \u03bb b , \u03bb 0 ) is minimized with respect to the network weights w, as usual, but is maximized with respect to the self-adaptation weights \u03bb r , \u03bb b , \u03bb 0 , i.e., the objective is:\nmin w max \u03bbr,\u03bb b ,\u03bb0 L(w, \u03bb r , \u03bb b , \u03bb 0 ) .(14)\nConsider the updates of a gradient descent/ascent approach to this problem:\nw k+1 = w k \u2212 \u03b7 k \u2207 w L(w k , \u03bb k r , \u03bb k b , \u03bb k 0 )(15)\u03bb k+1 r = \u03bb k r + \u03c1 k r \u2207 \u03bbr L(w k , \u03bb k r , \u03bb k b , \u03bb k 0 )(16)\u03bb k+1 b = \u03bb k b + \u03c1 k b \u2207 \u03bb b L(w k , \u03bb k r , \u03bb k b , \u03bb k 0 )(17)\u03bb k+1 0 = \u03bb k 0 + \u03c1 k 0 \u2207 \u03bb0 L(w k , \u03bb k r , \u03bb k b , \u03bb k 0 ) .(18)\nwhere \u03b7 k > 0 is the learning rate for the neural network weights at step k, \u03c1 k p > 0 is a separate learning rate for the self-adaption weights, for p = r, b, 0, and\n\u2207 \u03bbr L(w k , \u03bb k r , \u03bb k b , \u03bb k 0 ) = 1 2 \uf8ee \uf8f0 m (\u03bb k,1 r ) N x,t [u(x i r , t i r ; w k )] \u2212 f (x 1 r , t 1 r ) 2 \u00b7 \u00b7 \u00b7 m (\u03bb k,Nr r ) N x,t [u(x i r , t i r ; w k )] \u2212 f (x Nr r , t Nr r ) 2 \uf8f9 \uf8fb ,(19)\u2207 \u03bb b L(w k , \u03bb k r , \u03bb k b , \u03bb k 0 ) = 1 2 \uf8ee \uf8ef \uf8f0 m (\u03bb k,1 b ) B x,t [u(x i b , t i b ; w k )] \u2212 g(x 1 b , t 1 b ) 2 \u00b7 \u00b7 \u00b7 m (\u03bb k,N b b ) B x,t [u(x i b , t i b ; w k )] \u2212 g(x N b b , t N b b ) 2 \uf8f9 \uf8fa \uf8fb ,(20)\u2207 \u03bb0 L(w k , \u03bb k r , \u03bb k b , \u03bb k 0 ) = 1 2 \uf8ee \uf8ef \uf8f0 m (\u03bb k,1 0 ) u(x 1 0 , 0; w k ) \u2212 h(x 1 0 , t 1 0 ) 2 \u00b7 \u00b7 \u00b7 m (\u03bb k,N0 0 ) u(x i 0 , 0; w k )] \u2212 h(x N0 0 ) 2 \uf8f9 \uf8fa \uf8fb .(21)\nHence, since m (\u03bb) > 0 (the mask function is strictly increasing, by assumption), then \u2207 \u03bbr L, \u2207 \u03bb b L, \u2207 \u03bb0 L \u2265 0, and any gradient component is zero if and only if the corresponding unmasked loss is zero. This shows that the sequences of weights {\u03bb k r ; k = 1, 2, . . .}, {\u03bb k b ; k = 1, 2, . . .}, {\u03bb k 0 ; k = 1, 2, . . .} (and the associated mask values) are monotonically increasing, provided that the corresponding unmasked losses are nonzero. Furthermore, the magnitude of the gradients \u2207 \u03bbr L, \u2207 \u03bb b L, \u2207 \u03bb0 L, and therefore of the updates, are larger if the corresponding unmasked losses are larger. In addition, the magnitude of updates can be controlled by specifying a schedule for the learning rates \u03c1 k p , for p = r, b, 0, adding extra flexibility. This progressively penalizes the network more for not fitting the residual, boundary, and initial points closely.\n\nWe remark that any of the weights can be set to fixed, non-trainable values, if desired. For example, by setting \u03bb k b \u2261 1, only the weights of the initial and residue points would be trained. The sensor data loss is not masked in this formulation, since if these data consist of noisy observations, weighting them requires extra care to avoid overfitting (though this remains an open research problem).\n\nNotice that the self-adaptive weights need to be initialized at the beginning of training. They could be initialized to 1 (no weighting) or to a different value, depending on the problem. They could also be initialized randomly over an interval, similarly as to how neural network weights are often initialized. Here, prior knowledge plays an important role; e.g., if it is known that the initial conditions in a problem are hard to fit, then the initial condition weights could be initialized to a larger value than the other weights (alternatively, it could be initialized at the same value as the other weights, but employ a larger learning rate).\n\nThe shape of the function g affects mask sharpness and training of the PINN. Examples include polynomial masks m(\u03bb) = c\u03bb q , for c, q > 0, and sigmoidal masks. See Figure 1 for a few examples. In practice, the polynomial mask functions have to be kept below a suitable (large) value, to avoid numerical overflow. The sigmoidal masks do not have this issue, and can be used to produce sharp masks. For example, in the bottom right example in Figure 1, the mask is essentially binary; it starts small for small starting values of the self-adaptive weight \u03bb, and after these exceed a certain threshold, the mask value will quickly take on the upper saturation value. Similarly to neural network nonlinearities, sigmoid mask functions can suffer from vanishing gradients during training. This is particularly a problem at the lower starting value. Therefore, excessively sharp sigmoidal mask functions should be avoided. For another perspective, consider the following PDE-constrained optimization problem\nmin 1 2 Ns i=1 |u(x i s , t i s ; w) \u2212 y i s | 2 (22) subject to N x,t [u(x i r , t i r ; w)] = f (x i r , t i r ), , i = 1, . . . , N r (23) B x,t [u(x i b , t i b ; w)] = g(x b r , t i b ), , i = 1, . . . , N b (24) u(x i 0 , 0; w) = h(x i r ), , i = 1, . . . , N 0 (25)(26)\nFormally, SA-PINN training corresponds to a penalty method to solve the previous optimization problem [21]. In a typical penalty optimization method, a constrained problem\nmin L(x) (27) subject to r(x) = 0(28)\nis solved as a sequence of unconstrained problems\nmin L(x) + c k P (x), k = 1, 2, . . . ,(29)\nwhere c 1 < c 2 < . . . is a fixed sequence of increasing penalty costs, and P (x) is a suitable penalty function, which is small in the feasible region R = {x | r(x) = 0}, and large outside of it. A typical choice is the polynomial penalty P (x) = |r(x)| p , for p > 1. It can be shown that a limit point of a sequence of solutions of the unconstrained problem, where the solution at step k is used as the initialization for step k + 1, is a solution of the original constrained problem [21].\n\nOne can see SA-PINN training as a penalty method, with costs c k = m(\u03bb k ). The self-adaptation weights, and mask values, produce increasing penalty costs that are not selected a-priori, but are adaptively updated by the neural network training procedure. Although the use of neural networks in regular penalty-based constrained optimization has been suggested [22,23], neural networks have not been used in PDE-constrained problems, as far as we know.\n\nThe gradient ascent/descent step can be implemented easily using off-the-self neural network software, by simply flipping the sign of \u2207 \u03bbr L, \u2207 \u03bb b L, and \u2207 \u03bb0 L. In our implementation of SA-PINNs, we use Tensorflow 2.3 with a fixed number of iterations of Adam [24]. In some case, these are followed by another fixed number of iterations of the L-BFGS quasi-newton method [25]. This is consistent with the baseline PINN formulation in [2], as well as follow-up literature [4]. However, the adaptive weights are only updated in the Adam training steps, and are held constant during L-BFGS training, if any. A full implementation of the methodology described here has been made publicly available by the authors 2 and it is included in the open-source software TensorDiffEq [26].\n\n\nNumerical Examples\n\nIn this section we present numerical experiments demonstrating the SA-PINN performance on various benchmarks. The main figure of merit used is the L2-error:\nL 2 error = N U i=1 |u(x i , t i ) \u2212 U (x i , t i )| 2 N U i=1 |U (x i , t i )| 2 .(30)\nwhere u(x, t) is the trained approximation, and U (x, t) is a high-fidelity solution over a fine mesh {x i , t i } containing N U points. In all cases below, we repeat the training process over 10 random restarts and report the average L2 error and its standard deviation.\n\n\nViscous Burgers Equation\n\nThe viscous Burgers PDE considered here is\nu t + uu x \u2212 (0.01/\u03c0)u xx = 0 , x \u2208 [\u22121, 1], t \u2208 [0, 1] ,(31)u(0, x) = \u2212 sin(\u03c0x) ,(32)u(t, \u22121) = u(t, 1) = 0 .(33)\nAll results for the viscous Burgers PDE were generated from a fully-connected network with input layer size 2 corresponding to the x and t inputs, 8 hidden layers of 20 neurons each, and an output layer of size 1 corresponding to the output of the approximation u(x, t). This directly mimics the setup of the viscous Burgers PDE result presented in [2]. All training is done for 10k iterations of Adam, followed by 10k iterations of L-BFGS to fine tune the network weights, consistent with related work. Additionally, the number of points selected for the trials shown are N 0 = 100, N b = 200, and N r = 10000. Training with this architecture took 96ms/iteration on a single Nvidia V100 GPU. We initialize the self adaptive weights on the IC and the residual points to be U (0, 1) and the learning rates for all self-adaptive weights were set to 5e-3.\n\nWe achieved an L2 error of 4.803e-04 \u00b1 1.01e-4, which is a smaller error than the errors reported in [2] in 1/5 of the number of training iterations for an identical fully-connected architecture. The high-fidelity and predicted solutions are displayed in figure 2. Figure 3 demonstrate the accuracy of the proposed approach, using a significantly shorter training horizon than the baseline PINN.   Figure 4 shows that the sharp discontinuity at x = 0 in the solution has correspondingly large weights, indicating that the model must pay extra attention to those particular points in its solution, resulting in an increase in approximation accuracy and training efficiency.\n\n\nHelmholtz Equation\n\nThe Helmholtz PDE is typically used to describe the behavior of wave and diffusion processes, and can be employed to model evolution in a spatial domain or combined spatial-temporal domain. Here we study a particular Helmholtz PDE existing only in the spatial (x, y) domain, described as:\nu xx + u yy + k 2 u \u2212 q(x, y) = 0 (34) u(\u22121, y) = u(1, y) = u(x, \u22121) = u(x, 1) = 0(35)\nwhere x \u2208 [\u22121, 1], y \u2208 [\u22121, 1] and q(x, y) = \u2212 (a 1 \u03c0) 2 sin(a 1 \u03c0x) sin(a 2 \u03c0y)\n\n\u2212 (a 2 \u03c0) 2 sin(a 1 \u03c0x) sin(a 2 \u03c0y)\n+ k 2 sin(a 1 \u03c0x) sin(a 2 \u03c0y)w(36)\nis a forcing term that results in a closed-form analytical solution u(x, y) = sin(a 1 \u03c0x) sin(a 2 \u03c0y) .\n\nTo allow a direct comparison to the Helmholtz PDE result reported in [12], we take a 1 = 1 and a 2 = 4 and use the same neural network architecture with layer sizes [2, 50, 50, 50, 50, 1]. Our architecture is trained for 10k Adam and 10k L-BFGS iterations, again keeping the self-adaptive mask weights constant through the L-BFGS training iterations and only allowing those to train via Adam. We sample N b = 400 (100 points per boundary). Given the steady-state initialization and constant forcing term, there is no applicable initial condition and consequently no N 0 . We create a mesh of size (1001,1001) corresponding to the x \u2208 [\u22121, 1], y \u2208 [\u22121, 1] range, yielding 1,002,001 total mesh points, from which we select N r =100k residue points. We initialize the self adaptive weights on the BC and the residual points to be U (0, 1) and the learning rates for all self-adaptive weights were set to 5e-3.\n\nWe can see in figure 5 that the SA-PINN prediction is very accurate and indistinguishable from the exact solution. We achieve a relative L2 error of 3.2e-3 \u00b1 2.2e-4, which improves upon the learning-rate annealing weighted scheme proposed in [12], and begins to encroach on the accuracy of their improved fully-connected scheme with no additional modifications to the network structure itself. It is also worth noting that the SA-PINN is trained for 1/2 of the training iterations listed in [12] as well (at 10k Adam and 10 L-BFGS vs. 40k Adam), and achieves better L2 accuracy than a comparable architecture listed in table 2 of [12].   Figure 7 shows that the Self-Adaptive PINN largely ignores the flat areas in the solution, while focusing its attention on the nonflat areas.\n\n\nAllen-Cahn Reaction-Diffusion Equation\n\nIn this section, we report experimental results obtained with the Allen-Cahn PDE, which contrast the performance of the proposed SA-PINN algorithm against the baseline PINN and two of the PINN algorithms mentioned in Section 1.2, namely, the nonadaptive weighting and time-adaptive schemes (for the latter, Approach 1 in [4] was used).\n\nThe Allen-Cahn reaction-diffusion PDE is typically encountered in phase-field models, which can be used, for instance, to simulate the phase separation process in the microstructure evolution of metallic alloys [27,28,29]. The Allen-Cahn PDE considered here is specified as follows:\nu t \u2212 0.0001u xx + 5u 3 \u2212 5u = 0 , x \u2208 [\u22121, 1], t \u2208 [0, 1] ,(38)\nu(x, 0) = x 2 cos(\u03c0x) , (39) u(t, \u22121) = u(t, 1) ,\n(40) u x (t, \u22121) = u x (t, 1) .(41)\nThe Allen-Cahn PDE is an interesting benchmark for PINNs for multiple reasons. It is a \"stiff\" PDE that challenges PINNs to approximate solutions with sharp space and time transitions, and is also introduces periodic boundary conditions (40, 41). In order to deal with the latter, the boundary loss function L b (w, \u03bb b ) in (12) is replaced by\nL b (w, \u03bb b ) = 1 N b N b i=1 g(\u03bb i b )(|u(1, t i b ) \u2212 u(\u22121, t i b )| 2 + |u x (1, t i b ) \u2212 u x (\u22121, t i b )| 2 )(42)\nThe neural network architecture is fully connected with layer sizes [2,128,128,128,128,1]. This architecture is identical to the one used in the Allen-Cahn PDE result reported in [4], in order to allow a direct comparison of performance. We set the number of residue, initial, and boundary points to N r = 20, 000, N 0 = 100 and N b = 100, respectively (due to the periodic boundary condition, there are in fact 200 boundary points). Here we hold the boundary weights w i b at 1, while the initial weights w i 0 and residue weights w i r are trained. The initial and residue weights are initialized from a uniform distribution in the intervals [0, 100] and [0, 1], respectively. Training took 65ms/iteration on a single Nvidia V100 GPU.\n\nNumerical results obtained with the SA-PINN are displayed in figure 8. The average L2 error across 10 runs with random restarts was 2.1% \u00b1 1.21%, while the L2 error on 10 runs obtained by the time-adaptive approach in [4] was 8.0% \u00b1 0.56%. Neither the baseline PINN nor the nonadaptive weighted scheme, with initial condition weight C = 100, were able to solve this PDE satisfactorily, with L2 errors 96.15% \u00b1 6.45% and 49.61% \u00b1 2.50%, respectively (these numbers matched almost exactly those reported in [4]). Figure 9 is unique to the proposed SA-PINN algorithm. It displays the trained self-adaptive weights for the residue points across the spatio-temporal domain. These are the weights of the multiplicative soft attention mask self-imposed by the PINN. This plot stays remarkably constant across different runs with random restarts, which is an indication that it is a property of the particular PDE being solved. We can observe that in this case, more attention is needed early in the solution, but not uniformly across the space variable. In [4], this observation was justified by the fact that the Allen-Cahn PDEs describes a time-irreversible diffusion-reaction processes, where the solution has to be approximated well early. However, here this fact is \"discovered\" by the SA-PINN itself.\n\nIn order to study the behavior of the SA-PINN more closely, we plot in Figure 10 the average value of the residue weights from various partitions of the solution domain. While all the weights are increasing, as must be the case since the mask function is required to be monotone, the rate of increase is of importance. Notice that the initial condition weights grow much faster than the residue weights, as expected, since the initial condition tends to be neglected by the PINN, otherwise. As for the residue weights, we see that, for small values of t, they increase faster than for large values of t. This shows that the SA-PINN has learned that the early part of the evolution is the most critical part of the solution. (This agrees with what was seen in the map of Figure 9.) In contrast with traditional time-marching approaches, where earlier time steps are solved prior to later ones, the SA-PINN solves the PDE over the entire space-time domain at once; however, the self-adaptive weights allow it to concentrate on the early part of the evolution.\n\nFinally, Figure 11 displays the training loss for the baseline PINN and SA-PINN as a function of training iteration. For the SA-PINN, the weights were removed from the loss value to provide a direct comparison to the baseline. These plots are generated from 10 random restarts of the SA and baseline PINN training cycles over 10k Adam training iterations with consistent learning rates. We can see that the SA-PINN achieves significantly lower initial condition loss than the baseline PINN for the initial. Indeed, this is the major issue faced by the baseline PINN in the AC problem. As for the residual loss, we see that the baseline PINN decreases it fast (at the expense of the initial condition loss), but that eventually the SA PINN is able to achieve a loss two orders of magnitude smaller.   Note that earlier times require heavier weighting, with the highest average weights being the initial condition weights. This is consistent with the rationale that earlier solutions must be correctly learned for time-diffusive processes.\n\nThe oscillatory behavior of the residue loss in the SA-PINN reveals the dynamics of the competing self-adaptive weighted initial condition and residue loss terms.\n\n\nSelf-Adaptive Weight Training Hyperparameters\n\nWe comment here on the choice of hyperparameter settings made in the experiments reported in the previous sections. In all experiments, a constant learning rate of 5e-3 for the gradient ascent of the self-adaptive learning rates, and Adam optimization was used. Better results could potentially be obtained by using learning rate scheduling.\n\nEmpirically, we observed that effective training strategies for self-adaptive PINNs tend to lean on the lower side of learning rate for the neural network weights (i.e., 1e-5), and the higher side (i.e. 1e-3 to 1e-1) for the self-adaptive weights.\n\nAs specified in Section 3.3, the results shown in Figure 8 employ random initialization of the initial condition and residue self-adaptive weights in the intervals [0, 100] and [0, 1], respectively, and the learning rates are held constant and equal. This choice is dictated by the prior knowledge that heavier weighting of the initial condition is needed in the AC problem [4]. On the other hand, in the results displayed in figure 10, all weights were initialized randomly in the interval [0, 1]. In this case, it is observed that the initial conditions increase faster on their own, even though all learning rates are held constant and equal.\n\n\nSelf-Adaptive PINNs with Stochastic Gradient Descent\n\nStochastic gradient descent (SGD) [30] uses randomly sampled subsets of the training data to compute approximations to the gradient for training neural networks by gradient descent [31]. It has been claimed that the empirical superior performance of stochastic gradient descent over large-batch training is due to a tendency of the latter to converge to \"sharp\" minima in the loss surface, which have poor performance, while SGD with small batches converge to better \"flat\" minima [32]. The issue has not been well studied in the context of PINNs at the time of writing, though there is some empirical evidence that SGD can indeed improve the L 2 performance of PINNs with some PDEs. It should be pointed out that PINNs are well-suited to SGD since a new set of residue, initial and boundary points can be sampled each time rather than subsampling a given set of training data points as in conventional machine learning.\n\nThe baseline SA-PINN algorithm described previously cannot take advantage of small-batch SGD since the self-adaptive weights are attached to specific training points. In this section, we examine an extension of SA-PINN that allows the use of SGD. The basic idea is to use a spatial-temporal predictor of the value of self-adaptive weights for the newly sampled points. Here we use standard Gaussian processes regression due to its flexibility and power.\n\nA problem where SGD has been found empirically to have a strong impact is the 1D wave equation:\nu tt (x, t) \u2212 4u xx (x, t) = 0 , x \u2208 [0, 1] , t \u2208 [0, 1] , (43) u(0, t) = 0 , u(1, t) = 0, t \u2208 [0, 1] , (44) u t (x, 0) = 0 , x \u2208 [0, 1] ,(45)\nu(x, 0) = sin(\u03c0x)\n+ 1 2 sin(4\u03c0x) , x \u2208 [0, 1] .(46)\nThis problem was considered in [5] to study their NTK weighting scheme. The problem has an analytical solution:\n\nu(x, t) = sin(\u03c0x) cos(2\u03c0t)\n+ 1 2 sin(4\u03c0x) cos(8\u03c0t) , x \u2208 [0, 1] , t \u2208 [0, 1] .(47)\nThe baseline PINN struggles in this problem due to its stiffness. Here, we investigate the improvement provided by SGD, fixed weights, and self-adaptive weights. The architecture of the neural network consists of 5 layers of 500 neurons each with the tanh nonlinearity, and the number of residue, initial, and boundary points were set to 300, 100, and 100, respectively (these are the same hyperparameters used in [5]). The small sample sizes are appropriate to study the impact of SGD.\n\nIn all experiments, the learning rate for the neural network weights is kept fixed at 10 \u22125 for a total of 80,000 iterations. The self-adaptive weights are all initialized to 1.0, with learning rate 0.01 for the residue points, 0.05 for the initial condition on u t , and 0.25 for all other initial and boundary conditions on u. In the fixed-weight experiment, the weights were kept constant at 1.0 for the residue points, 5.0 for the initial condition on u t , and 50.0 for all other initial and boundary conditions on u.\n\nThese values make the final average values taken by the self-adaptive weights at the end of training approximately match the fixed weights. SGD is applied by resampling all training points every 100 iterations. The GPs were trained using fixed hyperparameters (no automatic tuning is performed).\n\nResults based on 10 independent random initializations of the neural network weights are displayed in Table 1. We can observe that all methods fail in the absence of SGD. On the other hand, while SGD is not able to improve the performance of the baseline PINN, it produces a significant improvement to the fixed-weight PINN, and a large improvement to the SA-PINN. In fact, the SA-PINN achieves an average L2-error of 2.95%, which is an order of magnitude better than the fixed-weight result. This L2-error is however larger than the one reported in [5]. Optimizations to the SGD process, including adaptive tuning of the GP hyperparameters, will be part of future work, and are expected to improve performance. PINN Table 1: Wave PDE results. The L2 error mean and standard deviation are based on 10 independent runs. The training time is an average over the 10 runs.\n\nThese results can perhaps be better appreciated in the plots in Figure 12. As an extra feature, the Gaussian Process predictor produces a continuous self-adaptive weight maps. Figure 13 displays the self-adaptive weight GP maps for the initial condition and residue points. We can observe in the 1D map that the self-adaptive weights become larger at the (high and low) peaks of the initial condition u(x, 0), which is where the curvature is maximum in magnitude; these are the most difficult regions to approximate with the neural network. In the 2D map, we can see that the self-adaptive weights are larger in the initial time, once again indicating the importance of approximating the solution early in time-evolution problems.\n\n\nNeural Tangent Kernel Analysis of Self-Adaptive PINNs\n\nIn this section, we investigate the dynamics of SA-PINN training by studying its neural tangent kernel (NTK). We derive the expression for the NTK matrix for self-adpative PINNs and then use it to obtain First, note that (15) can be written as\nw k+1 \u2212 w k \u03b7 k = \u2212\u2207 w L(w k , \u03bb k r , \u03bb k b , \u03bb k 0 ) .(48)\nIn the limit as the learning rate \u03b7 k tends to zero, the previous expression yields the gradient flow differential equation [33]: where \u03c4 \u2265 0 denotes the (continuous) training time. Notice that the usual gradient descent step corresponds to a forward Euler discretization of (49). It follows that the properties of gradient descent optimization can be investigated by studying this differential equation.\ndw(\u03c4 ) d\u03c4 = \u2212\u2207 w L(w(\u03c4 ), \u03bb r (\u03c4 ), \u03bb b (\u03c4 ), \u03bb 0 (\u03c4 )) ,(49)\nUnder this vanishing learning-rate limit, the neural tangent kernel (NTK) [34] characterizes the training dynamics of the neural network, i.e., the evolution of the output u(x, t; w(\u03c4 )) as a function of training time \u03c4 . In [5], the NTK for PINNs was derived and its properties were studied. Here we show how that a simple modification to their derivation produces the NTK for SA-PINNs.\n\nFor definiteness, consider the PDE problem:\nN x [u(x)] = f (x, t) , x \u2208 \u2126 ,(50)u(x) = g(x) , x \u2208 \u0393 \u2286 \u2202\u2126 .(51)\nFor a time-evolution problem, t becomes one of the components of x, and the set \u0393 typically includes an initial condition at t = 0. More complex boundary conditions and sample data can be added to the analysis below in a straightforward way.\n\nGiven residue points {x i r } Nr i=1 and boundary condition points\n{x i b } N b i=1 , let the response vectors be u r (\u03c4 ) = [N x [u(x 1 r ; w(\u03c4 ))], . . . , N x [u(x Nr r ; w(\u03c4 ))]] T ,(52)u b (\u03c4 ) = [u(x 1 b ; w(\u03c4 )), . . . , u(x N b r ; w(\u03c4 ))] T .(53)\nLikewise, the data vectors are denoted by\nv r = [f (x 1 r ), . . . , f (x Nr r )] T ,(54)v b = [g(x 1 b ), . . . , g(x N b b )] T .(55)\nWe write u p (\u03c4 ) = (u 1 p (\u03c4 ), . . . , u Np p (\u03c4 )) and v p = (v 1 p , . . . , v Np p ) to identify the individual responses u i p (\u03c4 ) and data point v i p , for p = r, b. The loss function at training time \u03c4 can be written similarly to (11)-(13):\nL(w(\u03c4 ), \u03bb r (\u03c4 ), \u03bb b (\u03c4 )) = 1 2 q=r,b Nq j=1 m(\u03bb j q (\u03c4 )) |u j q (\u03c4 ) \u2212 v j q | 2(56)\nHence, the gradient flow in (49) becomes\ndw d\u03c4 = \u2212 q=r,b Nq j=1 \u2207 w u j q (\u03c4 )m(\u03bb j q (\u03c4 )) (u j q (\u03c4 ) \u2212 v i q ) (57) = \u2212 q=r,b J T q (\u03c4 )\u0393 q (\u03c4 )(u q (\u03c4 ) \u2212 v q )(58)\nwhere J q (\u03c4 ) is the Jacobian of u q (\u03c4 ) with respect to w, for q = r, b, and \u0393 q (\u03c4 ) is a diagonal matrix of dimension N q \u00d7 N q containing the self-adaptive mask values m(\u03bb 1 q (\u03c4 )), . . . , m(\u03bb Nq q (\u03c4 )) in the diagonal, for q = r, b.\n\n\nIt follows that\ndu p (\u03c4 ) d\u03c4 = J p (\u03c4 ) \u00b7 dw(\u03c4 ) d\u03c4 = \u2212 q=r,b J p (\u03c4 )J T q (\u03c4 )\u0393 q (\u03c4 )(u q (\u03c4 ) \u2212 v q ) ,(59)\nfor p = r, b.\n\nNow define K pq (\u03c4 ) = J p (\u03c4 )J T q (\u03c4 ), for p, q = r, b. Notice that these are matrices of dimensions N p \u00d7 N q , with i, j elements\n(K pq ) ij (\u03c4 ) = \u2207 w u i p (\u03c4 ) T \u00b7 \u2207 w u j q (\u03c4 ) = w\u2208w du i p (\u03c4 ) dw \u00b7 du j q (\u03c4 ) dw .(60)\nIt is clear from the definition that the matrices K pp (\u03c4 ) are symmetric and positive semi-definite, and that K pq (\u03c4 ) = K qp (\u03c4 ) T , for p, q = r, b.\n\nThis allows us to collect the previous results in the following differential equation describing the evolution of the output of the SA-PINN in the vanishing learning-rate limit:\ndu(\u03c4 ) d\u03c4 = \u2212K(\u03c4 ) \u00b7 (u(\u03c4 ) \u2212 v) ,(61)\nwhere\nu(\u03c4 ) = u r (\u03c4 ) u b (\u03c4 ) , v = v r v b ,(62)\nand\nK(\u03c4 ) = K rr (\u03c4 )\u0393 r (\u03c4 ) K rb (\u03c4 )\u0393 b (\u03c4 ) K br (\u03c4 )\u0393 r (\u03c4 ) K bb (\u03c4 )\u0393 b (\u03c4 )(63)\nis the empirical neural tangent kernel matrix for the SA-PINN. (When all the mask values are 1, this reduces to the expression in Lemma 3.1 of [5].)\n\nNext, we employ the previous result to perform a heuristic analysis of the self-adaptive weights through their effects in the gradient flow ODE system in (61). The analysis is based on the infinitewidth limit of neural networks, when it can be shown that, under the vanishing learning rate regime, the NTK matrix converges to a constant deterministic value throughout training [34]. Under certain regularity conditions, it is shown in [5] that this result still holds in the case of PINNs, i.e., the matrices K rr (\u03c4 ), K rb (\u03c4 ) = K T br (\u03c4 ) and K bb (\u03c4 ) are constant and equal to their respective values at initialization (\u03c4 = 0) throughout training. In [5], this was proved for PINNs with one hidden layer and linear PDEs, though the authors conjectured that this result also holds for multiple-layer PINNs and nonlinear PDEs.\n\nWe thus make the assumption that for a wide PINN under a small learning rate, the NTK matrix and self-adaptive weights change little during training, i.e., K pq (\u03c4 ) \u2248 K pq and \u0393 p (\u03c4 ) \u2248 \u0393 p , for p, q = r, b, \u03c4 \u2265 0. In addition, we make the the simplifying approximation that the ODE system (61) can be decoupled, so that\ndu p (\u03c4 ) d\u03c4 \u2248 \u2212K pp \u0393 p \u00b7 (u p (\u03c4 ) \u2212 v) ,(64)\nfor p = r, b. (This approximation is also made, implicitly, in Section 7.3 of [5].) Some justification for the decoupling approximation comes from empirical evidence (not shown) that the matrix norms of the cross-terms K rb \u0393 b and K br \u0393 r are smaller than those of K rr \u0393 r and K bb \u0393 b and, in some cases, much smaller. This approximation allows us to gain a qualitative understanding of the importance of the residual and boundary loss components separately.\n\nFor p = r, b, matrix K pp is real symmetric and positive semi-definite, and thus diagonalizable with nonnegative eigenvalues. However, matrix K pp \u0393 p is not symmetric and not diagonalizable, in general. Fortunately, with the extra minor assumption that K pp is positive definite, and thus invertible, K pp \u0393 p is diagonalizable. To see this, note that little can be said about it other than the transformed eigenvalues are in the interval determined by m(\u03bb 1 )\u03b3 1 p and m(\u03bb n )\u03b3 n p , as stated in (66)-(67). The simple linear scaling introduced by traditional weighting can certainly help reduce the imbalance among the various terms, but it is less flexible than the transformation introduced by the pointwise self-adaptive weights, which can also change the shape of the eigenvalue distribution.\n\nNext, we illustrate this analysis with the classical univariate advection PDE: [35]:\nq t (x, t) +\u016bq x (x, t) = 0 , x \u2208 [0, L] , t \u2208 [0, T ] ,(71)u(0, t) = u(L, t) = 0, t \u2208 [0, T ] ,(72)u(x, 0) = g(x) , x \u2208 [0, L] .(73)\nwhere q(x, t) is for example the concentration of a tracer being transported in a fluid in a tube of length L, where\u016b > 0 is the fluid constant velocity. For simplicity, it is assumed that g(x) = 0 outside an interval in [0, L], and that T is short enough that the Dirichlet boundary condition is satisfied. (This could be changed at the expense of more complex boundary conditions.) In this scenario, the problem has a simple solution:\nq(x, t) = g(x \u2212\u016bt) , x \u2208 [0, L] , t \u2208 [0, T ] ,(74)\ni.e., the initial concentration profile is simply translated to the right at constant speed\u016b. Here, we adopt a fairly complex initial condition, containing several discontinuities, which makes the problem rather difficult to solve with the baseline PINN -or indeed numerical methods in general [35].\n\nThe results presented in figures 14, 15, and 16 are generated with a neural network architecture of [2, 400, 400, 400, 400, 1], trained for 10k Adam iterations with a neural network weight learning rate of 0.001 (hence, a wide PINN with a smal learning rate, as required by the theory). The learning rate for all self-adaptive weight was set at 0.1. Glorot Normal initialization was utilized, and all training was completed in Tensorflow on a single V100 GPU with an average training time of 7 seconds for 10k iterations. At the end of 10k training iterations, the baseline PINN failed to grasp even the rough structure of the solution, while the SA-PINN was able to approximate the solution within 5% L2 error. (More accurate results could have obtained by using more training epochs and a decreasing learning rate schedule.) An analysis of NTK eigenvalues similar to that performed in [5] is demonstrated in figure 16. (In this example, the boundary condition weights were fixed and equal to 1.0, and we disregarded this component in the analysis.) We can see that the eigenvalues become closely matched in scale between K uu and K rr , removing the imbalance between these two loss components and enabling convergence to the solution (here we are only looking at the initial and . Importantly, the shape of the eigenvalue distribution is also nicely equalized, as opposed to simply being scaled up as would be the case with traditional weighting of the entire loss component.\n\n\nConclusion\n\nIn this paper, we introduced Self-Adaptive Physics-Informed Neural Networks, a novel class of physics-constrained neural networks. This approach uses a similar conceptual framework as soft self-attention mechanisms in computer vision, in that the network identifies which inputs are most important to its own training. It was shown that training of the SA-PINN is formally equivalent to solving a PDE-constrained optimization problem using penalty-based method, though in a way where the monotonically-nondecreasing penalty coefficients are trainable. Experimental results with several linear and nonlinear PDE benchmarks indicate that SA-PINNs produce more accurate solutions than other state-of-the-art PINN algorithms. It was seen that SA-PINNs can employ stochastic gradient training, through continuous Gaussian-process interpolated self-adaptive maps, which allows the solution of a difficult wave PDE. These experimental results were complemented by a theoretical analysis based on the Neural Tangent Kernel for SA-PINNs.\n\nWe believe that SA-PINNs open up new possibilities for the use of deep neural networks in forward and inverse modeling in engineering and science. However, there is much that is not known yet about this class of algorithms, and indeed PINNs in general. For example, the use of standard off-the-shelf optimization algorithms for training deep neural networks, such as Adam, may not be appropriate, since those algorithms were mostly developed for image classification problems. How to obtain optimization algorithms specifically tailored to PINN problems in an open problem. In addition, the relationship between PINNs and constrained-optimization problems, hinted at here, is likely a fruitful topic of future study. Figure 16: NTK eigenvalues of the baseline PINN (solid) vs. the SA-PINN (dashed) for \u03c4 =1000, 5000, and 10000 training iterations. It can be observed that the SA-PINN accurately matches the magnitudes of the NTK eigenvalues between terms of the loss function, in this case the initial condition K uu and the residual loss K rr\n\nFigure 1 :\n1Mask function examples. From the upper left to the bottom right: polynomial mask, q = 2; polynomial mask, q = 4; smooth logistic mask; sharp logistic mask.\n\nFigure 2 :Figure 3 :\n23High-fidelity (left) vs. predicted (right) solutions for the viscous Burgers PDE. 2 https://github.com/levimcclenny/SA-PINNs Top: predicted solution of the viscous Burgers PDE. Middle: Cross-sections of the approximated vs. actual solutions for various x-domain snapshots. Bottom left: Residual r(x, t) across the spatial-temporal domain. Bottom right: Absolute error between prediction and high-fidelity solution across the spatial-temporal domain.\n\nFigure 4 :\n4Trained weights for residue points across the domain \u2126. Larger/brighter colored points correspond to larger weights.\n\nFigure 5 :\n5Exact (left) vs. predicted (right) solutions for the Helmholtz PDE.\n\nFigure 6\n6shows individual cross-sections of the Helmholtz solution, demonstrating the SA-PINN's ability to accurately approximate the sinusoidal solution on the whole domain.\n\nFigure 6 :Figure 7 :\n67Top predicted solution of Helmholtz equation. Bottom Cross-sections of the approximated vs. actual solutions for various x-domain snapshots Self-learned weights after training via Adam for the Helmholtz system. Brighter/larger points correspond to larger weights.\n\nFigure 8 :\n8Top: Plot of the approximation u(x, t) via the SA-PINN. Middle: Snapshots of the approximation u(x, t) vs. the high-fidelity solution U (x, t) at various time points through the temporal evolution. Bottom left: Residual r(x, t) across the spatial-temporal domain. As expected, it is close to 0 for the whole domain \u2126. Bottom right: Absolute error between approximation and high-fidelity solution across the spatial-temporal domain.\n\nFigure 9 :\n9Learned self-adaptive weights across the spatio-temporal domain. Brighter colors and larger points indicate larger weights.\n\nFigure 10 :\n10Average learned residue weights across various partitions of the solution domain.\n\nFigure 11 :\n11Average values for the initial condition loss, residue loss, and total loss, over 10k Adam training iterations. For the SA-PINN, the weights were removed from the loss value to provide a direct comparison to the baseline.\n\nFigure 12 :\n12Top: Exact solution of the wave problem. Left: Approximations obtained without SGD. Right: Approximations with SGD. From top to bottom: baseline, fixed weights, and self-adaptive weights.a heuristic understanding of the effect of the self-adaptive weights on the dynamics of training in the limiting case of infinitely-wide PINNs. We examine the effect of the self-adaptive weights on the eigenvalues of the NTK matrix in the solution of a linear advection PDE.\n\nFigure 13 :\n13Gaussian-Process maps of self-adaptive weights. Top: 1D map for initial condition. Bottom: 2D map for PDE residue.\n\nFigure 14 :\n14Top: Plot of the approximation u(x, t) via the baseline PINN, showing the exact solution vs predicted solution vs absolute error. Bottom: The SA-PINN results, L2 error decreases by an order of magnitude and the SA-PINN closely captures the exact solution.\n\nFigure 15 :\n15Top: Plot of the approximation u(x, t) via the baseline PINN, showing cross sections of the spatial domain at t = 0.02, 0.10, 0.18 Bottom: The SA-PINN results at the same time steps, with the same number of epochs (10k Adam) and all other parameters held constant.\nAcknowledgmentsThe authors would like to acknowledge the support of the D 3 EM program funded through NSF Award DGE-1545403. The authors would further like to thank the US Army CCDC Army Research Lab for their generous support and affiliation.pp is a product of symmetric matrices, and thus symmetric itself. Hence, K pp \u0393 p is similar to a real symmetric matrix, and thus diagonalizable. Furthermore, it is fairly simple fact of matrix theory that if \u03b3 1 p \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b3 Np p and \u00b5 1 p \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u00b5 Np p are the ordered eigenvalues of K pp and K pp \u0393 p , respectively, and \u03bb 1 p , \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bb Np are the self-adaptive weights sorted by magnitude, thenIn particular, (67) implies that all eigenvalues of K pp \u0393 p are nonnegative (and positive, if K pp is positive definite).It follows that, under the assumption that u p (0) \u2248 0 (this can be achieved with proper initialization of the neural network weights), the solution of the ODE (64) is given bywhich can be rewritten aswhere Q is the matrix of eigenvectors and M is the diagonal matrix of eigenvalues \u00b5 1 p , . . . , \u00b5 Np p of K pp \u0393 p , for p = r, b. This implies that the training error u i p (\u03c4 ) \u2212 v i p decreases at a rate e \u2212mu i p rate. Large variation among the eigenvalues \u00b5 1 p , . . . , \u00b5 Np p , both across the different loss terms p = r, b and the different data points in each loss term, will potentially lead to training imbalances and loss of convergence.The standard weighted loss function in(9)corresponds to the case when all the self-adaptive weights for each loss component are equal, with \u0393 p = \u03bb p I, in which case the eigenvalues of the NTK matrix are simply scaled by \u03bb p : \u00b5 i p = \u03bb i p \u03b3 i p , for i = 1, . . . , N p . On the other hand, the transformation effected on the eigenvalues of the NTK matrix by the self-adaptive weights is nonlinear. In general,\nNathan Baker, Frank Alexander, Timo Bremer, Aric Hagberg, Yannis Kevrekidis, Habib Najm, Manish Parashar, Abani Patra, James Sethian, Stefan Wild, Karen Willcox, Steven Lee, Workshop report on basic research needs for scientific machine learning: Core technologies for artificial intelligence. 22019Nathan Baker, Frank Alexander, Timo Bremer, Aric Hagberg, Yannis Kevrekidis, Habib Najm, Manish Parashar, Abani Patra, James Sethian, Stefan Wild, Karen Willcox, and Steven Lee. Workshop report on basic research needs for scientific machine learning: Core technologies for artificial intelligence, 2 2019.\n\nPhysics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Maziar Raissi, Paris Perdikaris, George E Karniadakis, Journal of Computational Physics. 378Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686-707, 2019.\n\nMaziar Raissi, arXiv:1804.07010Forward-backward stochastic neural networks: Deep learning of highdimensional partial differential equations. arXiv preprintMaziar Raissi. Forward-backward stochastic neural networks: Deep learning of high- dimensional partial differential equations. arXiv preprint arXiv:1804.07010, 2018.\n\nSolving allen-cahn and cahn-hilliard equations using the adaptive physics informed neural networks. L Colby, Jia Wight, Zhao, arXiv:2007.04542arXiv preprintColby L Wight and Jia Zhao. Solving allen-cahn and cahn-hilliard equations using the adaptive physics informed neural networks. arXiv preprint arXiv:2007.04542, 2020.\n\nWhen and why pinns fail to train: A neural tangent kernel perspective. Sifan Wang, Xinling Yu, Paris Perdikaris, arXiv:2007.14527arXiv preprintSifan Wang, Xinling Yu, and Paris Perdikaris. When and why pinns fail to train: A neural tangent kernel perspective. arXiv preprint arXiv:2007.14527, 2020.\n\nNeural-network-based approximations for solving partial differential equations. N Mwmg Dissanayake, Phan-Thien, communications in Numerical Methods in Engineering. 103MWMG Dissanayake and N Phan-Thien. Neural-network-based approximations for solving partial differential equations. communications in Numerical Methods in Engineering, 10(3):195- 201, 1994.\n\nTensorflow: A system for large-scale machine learning. Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, 12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16). Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In 12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16), pages 265-283, 2016.\n\nForward-mode automatic differentiation in julia. Jarrett Revels, Miles Lubin, Theodore Papamarkou, arXiv:1607.07892arXiv preprintJarrett Revels, Miles Lubin, and Theodore Papamarkou. Forward-mode automatic differentiation in julia. arXiv preprint arXiv:1607.07892, 2016.\n\nAutomatic differentiation in machine learning: a survey. At\u0131l\u0131m G\u00fcnes Baydin, A Barak, Alexey Pearlmutter, Jeffrey Mark Andreyevich Radul, Siskind, The Journal of Machine Learning Research. 181At\u0131l\u0131m G\u00fcnes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(1):5595-5637, 2017.\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\n. L Richard, Douglas J Burden, Faires, Richard L Burden and Douglas J Faires. Numerical analysis. 1985.\n\nUnderstanding and mitigating gradient pathologies in physics-informed neural networks. Sifan Wang, Yujun Teng, Paris Perdikaris, arXiv:2001.04536arXiv preprintSifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient patholo- gies in physics-informed neural networks. arXiv preprint arXiv:2001.04536, 2020.\n\nResidual attention network for image classification. Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionFei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156-3164, 2017.\n\nMask-guided attention network for occluded pedestrian detection. Yanwei Pang, Jin Xie, Muhammad Haris Khan, Rao Muhammad Anwer, Fahad Shahbaz Khan, Ling Shao, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionYanwei Pang, Jin Xie, Muhammad Haris Khan, Rao Muhammad Anwer, Fahad Shahbaz Khan, and Ling Shao. Mask-guided attention network for occluded pedestrian detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 4967-4975, 2019.\n\nYeonjong Shin, Jerome Darbon, George Em Karniadakis, arXiv:2004.01806On the convergence and generalization of physics informed neural networks. arXiv preprintYeonjong Shin, Jerome Darbon, and George Em Karniadakis. On the convergence and general- ization of physics informed neural networks. arXiv preprint arXiv:2004.01806, 2020.\n\nA dual-dimer method for training physics-constrained neural networks with minimax architecture. Dehao Liu, Yan Wang, Neural Networks. 136Dehao Liu and Yan Wang. A dual-dimer method for training physics-constrained neural networks with minimax architecture. Neural Networks, 136:112-125, 2021.\n\narXiv:1912.12355Softadapt: Techniques for adaptive loss weighting of neural networks with multi-part loss functions. arXiv preprintSoftadapt: Techniques for adaptive loss weighting of neural networks with multi-part loss functions. arXiv preprint arXiv:1912.12355, 2019.\n\nMulti-objective optimization for selfadjusting weighted gradient in machine learning tasks. Silva Conrado, Fernando Jos\u00e9 Von Miranda, Zuben, arXiv:1506.01113arXiv preprintConrado Silva Miranda and Fernando Jos\u00e9 Von Zuben. Multi-objective optimization for self- adjusting weighted gradient in machine learning tasks. arXiv preprint arXiv:1506.01113, 2015.\n\nAutoloss: Learning discrete schedules for alternate optimization. Haowen Xu, Hao Zhang, Zhiting Hu, Xiaodan Liang, Ruslan Salakhutdinov, Eric Xing, arXiv:1810.02442arXiv preprintHaowen Xu, Hao Zhang, Zhiting Hu, Xiaodan Liang, Ruslan Salakhutdinov, and Eric Xing. Au- toloss: Learning discrete schedules for alternate optimization. arXiv preprint arXiv:1810.02442, 2018.\n\nA tutorial on multiobjective optimization: fundamentals and evolutionary methods. Michael Emmerich, H Andr\u00e9, Deutz, Natural computing. 173Michael Emmerich and Andr\u00e9 H Deutz. A tutorial on multiobjective optimization: fundamentals and evolutionary methods. Natural computing, 17(3):585-609, 2018.\n\nLinear and nonlinear programming. G David, Yinyu Luenberger, Ye, Springer3rd editionDavid G Luenberger and Yinyu Ye. Linear and nonlinear programming. Springer, 3rd edition, 2008.\n\nOn solving constrained optimization problems with neural networks: A penalty method approach. E Walter, Mei Heng Lillo, Stefen Loh, Stanislaw H Hui, Zak, IEEE Transactions on neural networks. 46Walter E Lillo, Mei Heng Loh, Stefen Hui, and Stanislaw H Zak. On solving constrained optimization problems with neural networks: A penalty method approach. IEEE Transactions on neural networks, 4(6):931-940, 1993.\n\nNeural networks for solving constrained optimization problems. M Valeri, N Mladenov, Maratos, Proc. of CSCC'00. of CSCC'00Athens, Greece,(N. MastorakisValeri M Mladenov and N Maratos. Neural networks for solving constrained optimization problems. Proc. of CSCC'00, Athens, Greece,(N. Mastorakis, 2000.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nOn the limited memory bfgs method for large scale optimization. C Dong, Jorge Liu, Nocedal, Mathematical programming. 45Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1-3):503-528, 1989.\n\nLevi D Mcclenny, A Mulugeta, Ulisses Haile, Braga-Neto, arXiv:2103.16034Tensordiffeq: Scalable multi-gpu forward and inverse solvers for physics informed neural networks. arXiv preprintLevi D McClenny, Mulugeta A Haile, and Ulisses M Braga-Neto. Tensordiffeq: Scalable multi-gpu forward and inverse solvers for physics informed neural networks. arXiv preprint arXiv:2103.16034, 2021.\n\nAn introduction to phase-field modeling of microstructure evolution. Nele Moelans, Bart Blanpain, Patrick Wollants, Calphad. 322Nele Moelans, Bart Blanpain, and Patrick Wollants. An introduction to phase-field modeling of microstructure evolution. Calphad, 32(2):268-294, 2008.\n\nNumerical approximations of allen-cahn and cahn-hilliard equations. Jie Shen, Xiaofeng Yang, Discrete & Continuous Dynamical Systems-A. 2841669Jie Shen and Xiaofeng Yang. Numerical approximations of allen-cahn and cahn-hilliard equations. Discrete & Continuous Dynamical Systems-A, 28(4):1669, 2010.\n\nUlisses Braga-Neto, and Raymundo Arroyave. Semi-supervised learning approaches to class assignment in ambiguous microstructures. Courtney Kunselman, Vahid Attari, Levi Mcclenny, Acta Materialia. 188Courtney Kunselman, Vahid Attari, Levi McClenny, Ulisses Braga-Neto, and Raymundo Arroy- ave. Semi-supervised learning approaches to class assignment in ambiguous microstructures. Acta Materialia, 188:49-62, 2020.\n\nA stochastic approximation method. The annals of mathematical statistics. Herbert Robbins, Sutton Monro, Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400-407, 1951.\n\nAn overview of gradient descent optimization algorithms. Sebastian Ruder, arXiv:1609.04747arXiv preprintSebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.\n\nOn large-batch training for deep learning: Generalization gap and sharp minima. Dheevatsa Nitish Shirish Keskar, Jorge Mudigere, Mikhail Nocedal, Ping Tak Peter Smelyanskiy, Tang, arXiv:1609.04836arXiv preprintNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.\n\nExact solutions to the nonlinear dynamics of learning in deep linear neural networks. M Andrew, James L Saxe, Surya Mcclelland, Ganguli, arXiv:1312.6120arXiv preprintAndrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.\n\nNeural tangent kernel: Convergence and generalization in neural networks. Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler, Advances in neural information processing systems. 31Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018.\n\nFinite volume methods for hyperbolic problems. J Randall, Leveque, Cambridge university press31Randall J LeVeque et al. Finite volume methods for hyperbolic problems, volume 31. Cambridge university press, 2002.\n", "annotations": {"author": "[{\"end\":226,\"start\":93},{\"end\":358,\"start\":227}]", "publisher": null, "author_last_name": "[{\"end\":108,\"start\":100},{\"end\":245,\"start\":235}]", "author_first_name": "[{\"end\":97,\"start\":93},{\"end\":99,\"start\":98},{\"end\":234,\"start\":227}]", "author_affiliation": "[{\"end\":225,\"start\":132},{\"end\":357,\"start\":264}]", "title": "[{\"end\":80,\"start\":1},{\"end\":438,\"start\":359}]", "venue": null, "abstract": "[{\"end\":3417,\"start\":473}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3789,\"start\":3786},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4135,\"start\":4131},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4216,\"start\":4212},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4346,\"start\":4343},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4762,\"start\":4758},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4765,\"start\":4762},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7479,\"start\":7476},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7806,\"start\":7803},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7809,\"start\":7806},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8159,\"start\":8156},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9466,\"start\":9463},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9469,\"start\":9466},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9472,\"start\":9469},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9474,\"start\":9472},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10075,\"start\":10072},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10467,\"start\":10463},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11000,\"start\":10997},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11205,\"start\":11201},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11633,\"start\":11629},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11636,\"start\":11633},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11639,\"start\":11636},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11674,\"start\":11671},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11752,\"start\":11748},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12346,\"start\":12342},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13135,\"start\":13131},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13138,\"start\":13135},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18920,\"start\":18916},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19610,\"start\":19606},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19978,\"start\":19974},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19981,\"start\":19978},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20333,\"start\":20329},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20444,\"start\":20440},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20506,\"start\":20503},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20543,\"start\":20540},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20844,\"start\":20840},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21924,\"start\":21921},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22530,\"start\":22527},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23828,\"start\":23824},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24909,\"start\":24905},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25158,\"start\":25154},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25297,\"start\":25293},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25809,\"start\":25806},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26037,\"start\":26033},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26040,\"start\":26037},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26043,\"start\":26040},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26792,\"start\":26789},{\"end\":26796,\"start\":26792},{\"end\":26800,\"start\":26796},{\"end\":26804,\"start\":26800},{\"end\":26808,\"start\":26804},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26810,\"start\":26808},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26903,\"start\":26900},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27680,\"start\":27677},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27967,\"start\":27964},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28512,\"start\":28509},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32039,\"start\":32036},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32402,\"start\":32398},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32549,\"start\":32545},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":32849,\"start\":32845},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34066,\"start\":34063},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34645,\"start\":34642},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36090,\"start\":36087},{\"end\":36253,\"start\":36249},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37420,\"start\":37416},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37628,\"start\":37624},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":38045,\"start\":38041},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38195,\"start\":38192},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":40873,\"start\":40870},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":41258,\"start\":41254},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":41315,\"start\":41312},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":41538,\"start\":41535},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":42163,\"start\":42160},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":43430,\"start\":43426},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":44353,\"start\":44349},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":45246,\"start\":45243}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":48091,\"start\":47923},{\"attributes\":{\"id\":\"fig_1\"},\"end\":48565,\"start\":48092},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48695,\"start\":48566},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48776,\"start\":48696},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48953,\"start\":48777},{\"attributes\":{\"id\":\"fig_5\"},\"end\":49241,\"start\":48954},{\"attributes\":{\"id\":\"fig_6\"},\"end\":49686,\"start\":49242},{\"attributes\":{\"id\":\"fig_7\"},\"end\":49823,\"start\":49687},{\"attributes\":{\"id\":\"fig_8\"},\"end\":49920,\"start\":49824},{\"attributes\":{\"id\":\"fig_9\"},\"end\":50157,\"start\":49921},{\"attributes\":{\"id\":\"fig_10\"},\"end\":50634,\"start\":50158},{\"attributes\":{\"id\":\"fig_11\"},\"end\":50764,\"start\":50635},{\"attributes\":{\"id\":\"fig_12\"},\"end\":51035,\"start\":50765},{\"attributes\":{\"id\":\"fig_13\"},\"end\":51315,\"start\":51036}]", "paragraph": "[{\"end\":3742,\"start\":3419},{\"end\":4448,\"start\":3744},{\"end\":5735,\"start\":4450},{\"end\":6039,\"start\":5737},{\"end\":6568,\"start\":6041},{\"end\":6662,\"start\":6618},{\"end\":6898,\"start\":6796},{\"end\":7464,\"start\":6900},{\"end\":7810,\"start\":7466},{\"end\":7930,\"start\":7812},{\"end\":8206,\"start\":7981},{\"end\":9003,\"start\":8673},{\"end\":9854,\"start\":9020},{\"end\":10045,\"start\":9921},{\"end\":10352,\"start\":10047},{\"end\":10947,\"start\":10436},{\"end\":11178,\"start\":10949},{\"end\":11475,\"start\":11180},{\"end\":11753,\"start\":11477},{\"end\":12700,\"start\":11755},{\"end\":13410,\"start\":12751},{\"end\":13519,\"start\":13412},{\"end\":13831,\"start\":13607},{\"end\":14472,\"start\":14099},{\"end\":14598,\"start\":14523},{\"end\":15018,\"start\":14852},{\"end\":16476,\"start\":15597},{\"end\":16881,\"start\":16478},{\"end\":17533,\"start\":16883},{\"end\":18536,\"start\":17535},{\"end\":18985,\"start\":18814},{\"end\":19073,\"start\":19024},{\"end\":19611,\"start\":19118},{\"end\":20065,\"start\":19613},{\"end\":20845,\"start\":20067},{\"end\":21024,\"start\":20868},{\"end\":21385,\"start\":21113},{\"end\":21456,\"start\":21414},{\"end\":22424,\"start\":21572},{\"end\":23098,\"start\":22426},{\"end\":23409,\"start\":23121},{\"end\":23577,\"start\":23497},{\"end\":23614,\"start\":23579},{\"end\":23753,\"start\":23650},{\"end\":24661,\"start\":23755},{\"end\":25442,\"start\":24663},{\"end\":25820,\"start\":25485},{\"end\":26104,\"start\":25822},{\"end\":26219,\"start\":26170},{\"end\":26600,\"start\":26256},{\"end\":27457,\"start\":26721},{\"end\":28758,\"start\":27459},{\"end\":29817,\"start\":28760},{\"end\":30856,\"start\":29819},{\"end\":31020,\"start\":30858},{\"end\":31411,\"start\":31070},{\"end\":31660,\"start\":31413},{\"end\":32307,\"start\":31662},{\"end\":33284,\"start\":32364},{\"end\":33739,\"start\":33286},{\"end\":33836,\"start\":33741},{\"end\":33997,\"start\":33980},{\"end\":34143,\"start\":34032},{\"end\":34171,\"start\":34145},{\"end\":34714,\"start\":34228},{\"end\":35238,\"start\":34716},{\"end\":35535,\"start\":35240},{\"end\":36405,\"start\":35537},{\"end\":37137,\"start\":36407},{\"end\":37438,\"start\":37195},{\"end\":37904,\"start\":37500},{\"end\":38354,\"start\":37967},{\"end\":38399,\"start\":38356},{\"end\":38707,\"start\":38466},{\"end\":38775,\"start\":38709},{\"end\":39006,\"start\":38965},{\"end\":39351,\"start\":39101},{\"end\":39482,\"start\":39442},{\"end\":39853,\"start\":39611},{\"end\":39981,\"start\":39968},{\"end\":40118,\"start\":39983},{\"end\":40368,\"start\":40215},{\"end\":40547,\"start\":40370},{\"end\":40592,\"start\":40587},{\"end\":40642,\"start\":40639},{\"end\":40875,\"start\":40727},{\"end\":41708,\"start\":40877},{\"end\":42033,\"start\":41710},{\"end\":42544,\"start\":42082},{\"end\":43345,\"start\":42546},{\"end\":43431,\"start\":43347},{\"end\":44002,\"start\":43566},{\"end\":44354,\"start\":44055},{\"end\":45834,\"start\":44356},{\"end\":46877,\"start\":45849},{\"end\":47922,\"start\":46879}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6716,\"start\":6663},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6768,\"start\":6716},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6795,\"start\":6768},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7980,\"start\":7931},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8266,\"start\":8207},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8345,\"start\":8266},{\"attributes\":{\"id\":\"formula_6\"},\"end\":8424,\"start\":8345},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8672,\"start\":8424},{\"attributes\":{\"id\":\"formula_8\"},\"end\":9920,\"start\":9855},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10435,\"start\":10353},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13606,\"start\":13520},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14098,\"start\":13832},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14522,\"start\":14473},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14656,\"start\":14599},{\"attributes\":{\"id\":\"formula_14\"},\"end\":14720,\"start\":14656},{\"attributes\":{\"id\":\"formula_15\"},\"end\":14785,\"start\":14720},{\"attributes\":{\"id\":\"formula_16\"},\"end\":14851,\"start\":14785},{\"attributes\":{\"id\":\"formula_17\"},\"end\":15220,\"start\":15019},{\"attributes\":{\"id\":\"formula_18\"},\"end\":15427,\"start\":15220},{\"attributes\":{\"id\":\"formula_19\"},\"end\":15596,\"start\":15427},{\"attributes\":{\"id\":\"formula_20\"},\"end\":18813,\"start\":18537},{\"attributes\":{\"id\":\"formula_21\"},\"end\":19023,\"start\":18986},{\"attributes\":{\"id\":\"formula_22\"},\"end\":19117,\"start\":19074},{\"attributes\":{\"id\":\"formula_23\"},\"end\":21112,\"start\":21025},{\"attributes\":{\"id\":\"formula_24\"},\"end\":21518,\"start\":21457},{\"attributes\":{\"id\":\"formula_25\"},\"end\":21543,\"start\":21518},{\"attributes\":{\"id\":\"formula_26\"},\"end\":21571,\"start\":21543},{\"attributes\":{\"id\":\"formula_27\"},\"end\":23496,\"start\":23410},{\"attributes\":{\"id\":\"formula_28\"},\"end\":23649,\"start\":23615},{\"attributes\":{\"id\":\"formula_30\"},\"end\":26169,\"start\":26105},{\"attributes\":{\"id\":\"formula_31\"},\"end\":26255,\"start\":26220},{\"attributes\":{\"id\":\"formula_32\"},\"end\":26720,\"start\":26601},{\"attributes\":{\"id\":\"formula_33\"},\"end\":33979,\"start\":33837},{\"attributes\":{\"id\":\"formula_34\"},\"end\":34031,\"start\":33998},{\"attributes\":{\"id\":\"formula_35\"},\"end\":34227,\"start\":34172},{\"attributes\":{\"id\":\"formula_36\"},\"end\":37499,\"start\":37439},{\"attributes\":{\"id\":\"formula_37\"},\"end\":37966,\"start\":37905},{\"attributes\":{\"id\":\"formula_38\"},\"end\":38435,\"start\":38400},{\"attributes\":{\"id\":\"formula_39\"},\"end\":38465,\"start\":38435},{\"attributes\":{\"id\":\"formula_40\"},\"end\":38899,\"start\":38776},{\"attributes\":{\"id\":\"formula_41\"},\"end\":38964,\"start\":38899},{\"attributes\":{\"id\":\"formula_42\"},\"end\":39054,\"start\":39007},{\"attributes\":{\"id\":\"formula_43\"},\"end\":39100,\"start\":39054},{\"attributes\":{\"id\":\"formula_44\"},\"end\":39441,\"start\":39352},{\"attributes\":{\"id\":\"formula_45\"},\"end\":39610,\"start\":39483},{\"attributes\":{\"id\":\"formula_46\"},\"end\":39967,\"start\":39872},{\"attributes\":{\"id\":\"formula_47\"},\"end\":40214,\"start\":40119},{\"attributes\":{\"id\":\"formula_48\"},\"end\":40586,\"start\":40548},{\"attributes\":{\"id\":\"formula_49\"},\"end\":40638,\"start\":40593},{\"attributes\":{\"id\":\"formula_50\"},\"end\":40726,\"start\":40643},{\"attributes\":{\"id\":\"formula_51\"},\"end\":42081,\"start\":42034},{\"attributes\":{\"id\":\"formula_52\"},\"end\":43492,\"start\":43432},{\"attributes\":{\"id\":\"formula_53\"},\"end\":43532,\"start\":43492},{\"attributes\":{\"id\":\"formula_54\"},\"end\":43565,\"start\":43532},{\"attributes\":{\"id\":\"formula_55\"},\"end\":44054,\"start\":44003}]", "table_ref": "[{\"end\":35646,\"start\":35639},{\"end\":36261,\"start\":36254}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":6581,\"start\":6571},{\"attributes\":{\"n\":\"1.1\"},\"end\":6616,\"start\":6584},{\"attributes\":{\"n\":\"1.2\"},\"end\":9018,\"start\":9006},{\"attributes\":{\"n\":\"2\"},\"end\":12749,\"start\":12703},{\"attributes\":{\"n\":\"3\"},\"end\":20866,\"start\":20848},{\"attributes\":{\"n\":\"3.1\"},\"end\":21412,\"start\":21388},{\"attributes\":{\"n\":\"3.2\"},\"end\":23119,\"start\":23101},{\"attributes\":{\"n\":\"3.3\"},\"end\":25483,\"start\":25445},{\"attributes\":{\"n\":\"3.4\"},\"end\":31068,\"start\":31023},{\"attributes\":{\"n\":\"4\"},\"end\":32362,\"start\":32310},{\"attributes\":{\"n\":\"5\"},\"end\":37193,\"start\":37140},{\"end\":39871,\"start\":39856},{\"attributes\":{\"n\":\"6\"},\"end\":45847,\"start\":45837},{\"end\":47934,\"start\":47924},{\"end\":48113,\"start\":48093},{\"end\":48577,\"start\":48567},{\"end\":48707,\"start\":48697},{\"end\":48786,\"start\":48778},{\"end\":48975,\"start\":48955},{\"end\":49253,\"start\":49243},{\"end\":49698,\"start\":49688},{\"end\":49836,\"start\":49825},{\"end\":49933,\"start\":49922},{\"end\":50170,\"start\":50159},{\"end\":50647,\"start\":50636},{\"end\":50777,\"start\":50766},{\"end\":51048,\"start\":51037}]", "table": null, "figure_caption": "[{\"end\":48091,\"start\":47936},{\"end\":48565,\"start\":48116},{\"end\":48695,\"start\":48579},{\"end\":48776,\"start\":48709},{\"end\":48953,\"start\":48788},{\"end\":49241,\"start\":48978},{\"end\":49686,\"start\":49255},{\"end\":49823,\"start\":49700},{\"end\":49920,\"start\":49839},{\"end\":50157,\"start\":49936},{\"end\":50634,\"start\":50173},{\"end\":50764,\"start\":50650},{\"end\":51035,\"start\":50780},{\"end\":51315,\"start\":51051}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17707,\"start\":17699},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17984,\"start\":17976},{\"end\":22699,\"start\":22691},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22832,\"start\":22824},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24685,\"start\":24677},{\"end\":25309,\"start\":25301},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27528,\"start\":27520},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27978,\"start\":27970},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28840,\"start\":28831},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29538,\"start\":29530},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29837,\"start\":29828},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31720,\"start\":31712},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32097,\"start\":32088},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36480,\"start\":36471},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36592,\"start\":36583},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45275,\"start\":45266},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47605,\"start\":47596}]", "bib_author_first_name": "[{\"end\":53160,\"start\":53154},{\"end\":53173,\"start\":53168},{\"end\":53189,\"start\":53185},{\"end\":53202,\"start\":53198},{\"end\":53218,\"start\":53212},{\"end\":53236,\"start\":53231},{\"end\":53249,\"start\":53243},{\"end\":53265,\"start\":53260},{\"end\":53278,\"start\":53273},{\"end\":53294,\"start\":53288},{\"end\":53306,\"start\":53301},{\"end\":53322,\"start\":53316},{\"end\":53919,\"start\":53913},{\"end\":53933,\"start\":53928},{\"end\":53952,\"start\":53946},{\"end\":53954,\"start\":53953},{\"end\":54277,\"start\":54271},{\"end\":54694,\"start\":54693},{\"end\":54705,\"start\":54702},{\"end\":54993,\"start\":54988},{\"end\":55007,\"start\":55000},{\"end\":55017,\"start\":55012},{\"end\":55298,\"start\":55297},{\"end\":55635,\"start\":55629},{\"end\":55647,\"start\":55643},{\"end\":55663,\"start\":55656},{\"end\":55677,\"start\":55670},{\"end\":55688,\"start\":55684},{\"end\":55703,\"start\":55696},{\"end\":55718,\"start\":55710},{\"end\":55732,\"start\":55726},{\"end\":55751,\"start\":55743},{\"end\":55767,\"start\":55760},{\"end\":56232,\"start\":56225},{\"end\":56246,\"start\":56241},{\"end\":56262,\"start\":56254},{\"end\":56527,\"start\":56526},{\"end\":56541,\"start\":56535},{\"end\":56562,\"start\":56555},{\"end\":56567,\"start\":56563},{\"end\":56900,\"start\":56896},{\"end\":56912,\"start\":56909},{\"end\":56927,\"start\":56920},{\"end\":56945,\"start\":56938},{\"end\":56960,\"start\":56954},{\"end\":56974,\"start\":56967},{\"end\":56989,\"start\":56983},{\"end\":57000,\"start\":56995},{\"end\":57016,\"start\":57012},{\"end\":57029,\"start\":57025},{\"end\":57230,\"start\":57229},{\"end\":57247,\"start\":57240},{\"end\":57249,\"start\":57248},{\"end\":57424,\"start\":57419},{\"end\":57436,\"start\":57431},{\"end\":57448,\"start\":57443},{\"end\":57722,\"start\":57719},{\"end\":57737,\"start\":57729},{\"end\":57749,\"start\":57745},{\"end\":57760,\"start\":57756},{\"end\":57772,\"start\":57767},{\"end\":57785,\"start\":57777},{\"end\":57801,\"start\":57793},{\"end\":57814,\"start\":57808},{\"end\":58298,\"start\":58292},{\"end\":58308,\"start\":58305},{\"end\":58322,\"start\":58314},{\"end\":58328,\"start\":58323},{\"end\":58338,\"start\":58335},{\"end\":58360,\"start\":58355},{\"end\":58379,\"start\":58375},{\"end\":58774,\"start\":58766},{\"end\":58787,\"start\":58781},{\"end\":58802,\"start\":58796},{\"end\":58805,\"start\":58803},{\"end\":59199,\"start\":59194},{\"end\":59208,\"start\":59205},{\"end\":59761,\"start\":59756},{\"end\":59788,\"start\":59771},{\"end\":60092,\"start\":60086},{\"end\":60100,\"start\":60097},{\"end\":60115,\"start\":60108},{\"end\":60127,\"start\":60120},{\"end\":60141,\"start\":60135},{\"end\":60161,\"start\":60157},{\"end\":60481,\"start\":60474},{\"end\":60493,\"start\":60492},{\"end\":60724,\"start\":60723},{\"end\":60737,\"start\":60732},{\"end\":60965,\"start\":60964},{\"end\":60977,\"start\":60974},{\"end\":60982,\"start\":60978},{\"end\":60996,\"start\":60990},{\"end\":61011,\"start\":61002},{\"end\":61013,\"start\":61012},{\"end\":61344,\"start\":61343},{\"end\":61354,\"start\":61353},{\"end\":61628,\"start\":61627},{\"end\":61644,\"start\":61639},{\"end\":61866,\"start\":61865},{\"end\":61878,\"start\":61873},{\"end\":62083,\"start\":62082},{\"end\":62101,\"start\":62094},{\"end\":62523,\"start\":62519},{\"end\":62537,\"start\":62533},{\"end\":62555,\"start\":62548},{\"end\":62800,\"start\":62797},{\"end\":62815,\"start\":62807},{\"end\":63167,\"start\":63159},{\"end\":63184,\"start\":63179},{\"end\":63197,\"start\":63193},{\"end\":63524,\"start\":63517},{\"end\":63540,\"start\":63534},{\"end\":63744,\"start\":63735},{\"end\":63985,\"start\":63976},{\"end\":64014,\"start\":64009},{\"end\":64032,\"start\":64025},{\"end\":64056,\"start\":64042},{\"end\":64417,\"start\":64416},{\"end\":64431,\"start\":64426},{\"end\":64433,\"start\":64432},{\"end\":64445,\"start\":64440},{\"end\":64755,\"start\":64749},{\"end\":64769,\"start\":64763},{\"end\":64786,\"start\":64779},{\"end\":65084,\"start\":65083}]", "bib_author_last_name": "[{\"end\":53166,\"start\":53161},{\"end\":53183,\"start\":53174},{\"end\":53196,\"start\":53190},{\"end\":53210,\"start\":53203},{\"end\":53229,\"start\":53219},{\"end\":53241,\"start\":53237},{\"end\":53258,\"start\":53250},{\"end\":53271,\"start\":53266},{\"end\":53286,\"start\":53279},{\"end\":53299,\"start\":53295},{\"end\":53314,\"start\":53307},{\"end\":53326,\"start\":53323},{\"end\":53926,\"start\":53920},{\"end\":53944,\"start\":53934},{\"end\":53966,\"start\":53955},{\"end\":54284,\"start\":54278},{\"end\":54700,\"start\":54695},{\"end\":54711,\"start\":54706},{\"end\":54717,\"start\":54713},{\"end\":54998,\"start\":54994},{\"end\":55010,\"start\":55008},{\"end\":55028,\"start\":55018},{\"end\":55315,\"start\":55299},{\"end\":55327,\"start\":55317},{\"end\":55641,\"start\":55636},{\"end\":55654,\"start\":55648},{\"end\":55668,\"start\":55664},{\"end\":55682,\"start\":55678},{\"end\":55694,\"start\":55689},{\"end\":55708,\"start\":55704},{\"end\":55724,\"start\":55719},{\"end\":55741,\"start\":55733},{\"end\":55758,\"start\":55752},{\"end\":55773,\"start\":55768},{\"end\":56239,\"start\":56233},{\"end\":56252,\"start\":56247},{\"end\":56273,\"start\":56263},{\"end\":56524,\"start\":56505},{\"end\":56533,\"start\":56528},{\"end\":56553,\"start\":56542},{\"end\":56585,\"start\":56568},{\"end\":56594,\"start\":56587},{\"end\":56907,\"start\":56901},{\"end\":56918,\"start\":56913},{\"end\":56936,\"start\":56928},{\"end\":56952,\"start\":56946},{\"end\":56965,\"start\":56961},{\"end\":56981,\"start\":56975},{\"end\":56993,\"start\":56990},{\"end\":57010,\"start\":57001},{\"end\":57023,\"start\":57017},{\"end\":57035,\"start\":57030},{\"end\":57238,\"start\":57231},{\"end\":57256,\"start\":57250},{\"end\":57264,\"start\":57258},{\"end\":57429,\"start\":57425},{\"end\":57441,\"start\":57437},{\"end\":57459,\"start\":57449},{\"end\":57727,\"start\":57723},{\"end\":57743,\"start\":57738},{\"end\":57754,\"start\":57750},{\"end\":57765,\"start\":57761},{\"end\":57775,\"start\":57773},{\"end\":57791,\"start\":57786},{\"end\":57806,\"start\":57802},{\"end\":57819,\"start\":57815},{\"end\":58303,\"start\":58299},{\"end\":58312,\"start\":58309},{\"end\":58333,\"start\":58329},{\"end\":58353,\"start\":58339},{\"end\":58373,\"start\":58361},{\"end\":58384,\"start\":58380},{\"end\":58779,\"start\":58775},{\"end\":58794,\"start\":58788},{\"end\":58817,\"start\":58806},{\"end\":59203,\"start\":59200},{\"end\":59213,\"start\":59209},{\"end\":59769,\"start\":59762},{\"end\":59796,\"start\":59789},{\"end\":59803,\"start\":59798},{\"end\":60095,\"start\":60093},{\"end\":60106,\"start\":60101},{\"end\":60118,\"start\":60116},{\"end\":60133,\"start\":60128},{\"end\":60155,\"start\":60142},{\"end\":60166,\"start\":60162},{\"end\":60490,\"start\":60482},{\"end\":60499,\"start\":60494},{\"end\":60506,\"start\":60501},{\"end\":60730,\"start\":60725},{\"end\":60748,\"start\":60738},{\"end\":60752,\"start\":60750},{\"end\":60972,\"start\":60966},{\"end\":60988,\"start\":60983},{\"end\":61000,\"start\":60997},{\"end\":61017,\"start\":61014},{\"end\":61022,\"start\":61019},{\"end\":61351,\"start\":61345},{\"end\":61363,\"start\":61355},{\"end\":61372,\"start\":61365},{\"end\":61637,\"start\":61629},{\"end\":61651,\"start\":61645},{\"end\":61655,\"start\":61653},{\"end\":61871,\"start\":61867},{\"end\":61882,\"start\":61879},{\"end\":61891,\"start\":61884},{\"end\":62080,\"start\":62065},{\"end\":62092,\"start\":62084},{\"end\":62107,\"start\":62102},{\"end\":62119,\"start\":62109},{\"end\":62531,\"start\":62524},{\"end\":62546,\"start\":62538},{\"end\":62564,\"start\":62556},{\"end\":62805,\"start\":62801},{\"end\":62820,\"start\":62816},{\"end\":63177,\"start\":63168},{\"end\":63191,\"start\":63185},{\"end\":63206,\"start\":63198},{\"end\":63532,\"start\":63525},{\"end\":63546,\"start\":63541},{\"end\":63750,\"start\":63745},{\"end\":64007,\"start\":63986},{\"end\":64023,\"start\":64015},{\"end\":64040,\"start\":64033},{\"end\":64068,\"start\":64057},{\"end\":64074,\"start\":64070},{\"end\":64424,\"start\":64418},{\"end\":64438,\"start\":64434},{\"end\":64456,\"start\":64446},{\"end\":64465,\"start\":64458},{\"end\":64761,\"start\":64756},{\"end\":64777,\"start\":64770},{\"end\":64794,\"start\":64787},{\"end\":65092,\"start\":65085},{\"end\":65101,\"start\":65094}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":53758,\"start\":53154},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":57379996},\"end\":54269,\"start\":53760},{\"attributes\":{\"doi\":\"arXiv:1804.07010\",\"id\":\"b2\"},\"end\":54591,\"start\":54271},{\"attributes\":{\"doi\":\"arXiv:2007.04542\",\"id\":\"b3\"},\"end\":54915,\"start\":54593},{\"attributes\":{\"doi\":\"arXiv:2007.14527\",\"id\":\"b4\"},\"end\":55215,\"start\":54917},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":120328171},\"end\":55572,\"start\":55217},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6287870},\"end\":56174,\"start\":55574},{\"attributes\":{\"doi\":\"arXiv:1607.07892\",\"id\":\"b7\"},\"end\":56446,\"start\":56176},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3766791},\"end\":56856,\"start\":56448},{\"attributes\":{\"id\":\"b9\"},\"end\":57225,\"start\":56858},{\"attributes\":{\"id\":\"b10\"},\"end\":57330,\"start\":57227},{\"attributes\":{\"doi\":\"arXiv:2001.04536\",\"id\":\"b11\"},\"end\":57664,\"start\":57332},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1806714},\"end\":58225,\"start\":57666},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":204509364},\"end\":58764,\"start\":58227},{\"attributes\":{\"doi\":\"arXiv:2004.01806\",\"id\":\"b14\"},\"end\":59096,\"start\":58766},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":218486841},\"end\":59390,\"start\":59098},{\"attributes\":{\"doi\":\"arXiv:1912.12355\",\"id\":\"b16\"},\"end\":59662,\"start\":59392},{\"attributes\":{\"doi\":\"arXiv:1506.01113\",\"id\":\"b17\"},\"end\":60018,\"start\":59664},{\"attributes\":{\"doi\":\"arXiv:1810.02442\",\"id\":\"b18\"},\"end\":60390,\"start\":60020},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":46972841},\"end\":60687,\"start\":60392},{\"attributes\":{\"id\":\"b20\"},\"end\":60868,\"start\":60689},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":24922451},\"end\":61278,\"start\":60870},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":12452554},\"end\":61581,\"start\":61280},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b23\"},\"end\":61799,\"start\":61583},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":5681609},\"end\":62063,\"start\":61801},{\"attributes\":{\"doi\":\"arXiv:2103.16034\",\"id\":\"b25\"},\"end\":62448,\"start\":62065},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4411233},\"end\":62727,\"start\":62450},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1263354},\"end\":63028,\"start\":62729},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":214006853},\"end\":63441,\"start\":63030},{\"attributes\":{\"id\":\"b29\"},\"end\":63676,\"start\":63443},{\"attributes\":{\"doi\":\"arXiv:1609.04747\",\"id\":\"b30\"},\"end\":63894,\"start\":63678},{\"attributes\":{\"doi\":\"arXiv:1609.04836\",\"id\":\"b31\"},\"end\":64328,\"start\":63896},{\"attributes\":{\"doi\":\"arXiv:1312.6120\",\"id\":\"b32\"},\"end\":64673,\"start\":64330},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":49321232},\"end\":65034,\"start\":64675},{\"attributes\":{\"id\":\"b34\"},\"end\":65247,\"start\":65036}]", "bib_title": "[{\"end\":53911,\"start\":53760},{\"end\":55295,\"start\":55217},{\"end\":55627,\"start\":55574},{\"end\":56503,\"start\":56448},{\"end\":57717,\"start\":57666},{\"end\":58290,\"start\":58227},{\"end\":59192,\"start\":59098},{\"end\":60472,\"start\":60392},{\"end\":60962,\"start\":60870},{\"end\":61341,\"start\":61280},{\"end\":61863,\"start\":61801},{\"end\":62517,\"start\":62450},{\"end\":62795,\"start\":62729},{\"end\":63157,\"start\":63030},{\"end\":64747,\"start\":64675}]", "bib_author": "[{\"end\":53168,\"start\":53154},{\"end\":53185,\"start\":53168},{\"end\":53198,\"start\":53185},{\"end\":53212,\"start\":53198},{\"end\":53231,\"start\":53212},{\"end\":53243,\"start\":53231},{\"end\":53260,\"start\":53243},{\"end\":53273,\"start\":53260},{\"end\":53288,\"start\":53273},{\"end\":53301,\"start\":53288},{\"end\":53316,\"start\":53301},{\"end\":53328,\"start\":53316},{\"end\":53928,\"start\":53913},{\"end\":53946,\"start\":53928},{\"end\":53968,\"start\":53946},{\"end\":54286,\"start\":54271},{\"end\":54702,\"start\":54693},{\"end\":54713,\"start\":54702},{\"end\":54719,\"start\":54713},{\"end\":55000,\"start\":54988},{\"end\":55012,\"start\":55000},{\"end\":55030,\"start\":55012},{\"end\":55317,\"start\":55297},{\"end\":55329,\"start\":55317},{\"end\":55643,\"start\":55629},{\"end\":55656,\"start\":55643},{\"end\":55670,\"start\":55656},{\"end\":55684,\"start\":55670},{\"end\":55696,\"start\":55684},{\"end\":55710,\"start\":55696},{\"end\":55726,\"start\":55710},{\"end\":55743,\"start\":55726},{\"end\":55760,\"start\":55743},{\"end\":55775,\"start\":55760},{\"end\":56241,\"start\":56225},{\"end\":56254,\"start\":56241},{\"end\":56275,\"start\":56254},{\"end\":56526,\"start\":56505},{\"end\":56535,\"start\":56526},{\"end\":56555,\"start\":56535},{\"end\":56587,\"start\":56555},{\"end\":56596,\"start\":56587},{\"end\":56909,\"start\":56896},{\"end\":56920,\"start\":56909},{\"end\":56938,\"start\":56920},{\"end\":56954,\"start\":56938},{\"end\":56967,\"start\":56954},{\"end\":56983,\"start\":56967},{\"end\":56995,\"start\":56983},{\"end\":57012,\"start\":56995},{\"end\":57025,\"start\":57012},{\"end\":57037,\"start\":57025},{\"end\":57240,\"start\":57229},{\"end\":57258,\"start\":57240},{\"end\":57266,\"start\":57258},{\"end\":57431,\"start\":57419},{\"end\":57443,\"start\":57431},{\"end\":57461,\"start\":57443},{\"end\":57729,\"start\":57719},{\"end\":57745,\"start\":57729},{\"end\":57756,\"start\":57745},{\"end\":57767,\"start\":57756},{\"end\":57777,\"start\":57767},{\"end\":57793,\"start\":57777},{\"end\":57808,\"start\":57793},{\"end\":57821,\"start\":57808},{\"end\":58305,\"start\":58292},{\"end\":58314,\"start\":58305},{\"end\":58335,\"start\":58314},{\"end\":58355,\"start\":58335},{\"end\":58375,\"start\":58355},{\"end\":58386,\"start\":58375},{\"end\":58781,\"start\":58766},{\"end\":58796,\"start\":58781},{\"end\":58819,\"start\":58796},{\"end\":59205,\"start\":59194},{\"end\":59215,\"start\":59205},{\"end\":59771,\"start\":59756},{\"end\":59798,\"start\":59771},{\"end\":59805,\"start\":59798},{\"end\":60097,\"start\":60086},{\"end\":60108,\"start\":60097},{\"end\":60120,\"start\":60108},{\"end\":60135,\"start\":60120},{\"end\":60157,\"start\":60135},{\"end\":60168,\"start\":60157},{\"end\":60492,\"start\":60474},{\"end\":60501,\"start\":60492},{\"end\":60508,\"start\":60501},{\"end\":60732,\"start\":60723},{\"end\":60750,\"start\":60732},{\"end\":60754,\"start\":60750},{\"end\":60974,\"start\":60964},{\"end\":60990,\"start\":60974},{\"end\":61002,\"start\":60990},{\"end\":61019,\"start\":61002},{\"end\":61024,\"start\":61019},{\"end\":61353,\"start\":61343},{\"end\":61365,\"start\":61353},{\"end\":61374,\"start\":61365},{\"end\":61639,\"start\":61627},{\"end\":61653,\"start\":61639},{\"end\":61657,\"start\":61653},{\"end\":61873,\"start\":61865},{\"end\":61884,\"start\":61873},{\"end\":61893,\"start\":61884},{\"end\":62082,\"start\":62065},{\"end\":62094,\"start\":62082},{\"end\":62109,\"start\":62094},{\"end\":62121,\"start\":62109},{\"end\":62533,\"start\":62519},{\"end\":62548,\"start\":62533},{\"end\":62566,\"start\":62548},{\"end\":62807,\"start\":62797},{\"end\":62822,\"start\":62807},{\"end\":63179,\"start\":63159},{\"end\":63193,\"start\":63179},{\"end\":63208,\"start\":63193},{\"end\":63534,\"start\":63517},{\"end\":63548,\"start\":63534},{\"end\":63752,\"start\":63735},{\"end\":64009,\"start\":63976},{\"end\":64025,\"start\":64009},{\"end\":64042,\"start\":64025},{\"end\":64070,\"start\":64042},{\"end\":64076,\"start\":64070},{\"end\":64426,\"start\":64416},{\"end\":64440,\"start\":64426},{\"end\":64458,\"start\":64440},{\"end\":64467,\"start\":64458},{\"end\":64763,\"start\":64749},{\"end\":64779,\"start\":64763},{\"end\":64796,\"start\":64779},{\"end\":65094,\"start\":65083},{\"end\":65103,\"start\":65094}]", "bib_venue": "[{\"end\":57962,\"start\":57900},{\"end\":58507,\"start\":58455},{\"end\":61431,\"start\":61392},{\"end\":53446,\"start\":53328},{\"end\":54000,\"start\":53968},{\"end\":54410,\"start\":54302},{\"end\":54691,\"start\":54593},{\"end\":54986,\"start\":54917},{\"end\":55379,\"start\":55329},{\"end\":55857,\"start\":55775},{\"end\":56223,\"start\":56176},{\"end\":56636,\"start\":56596},{\"end\":56894,\"start\":56858},{\"end\":57417,\"start\":57332},{\"end\":57898,\"start\":57821},{\"end\":58453,\"start\":58386},{\"end\":58908,\"start\":58835},{\"end\":59230,\"start\":59215},{\"end\":59507,\"start\":59408},{\"end\":59754,\"start\":59664},{\"end\":60084,\"start\":60020},{\"end\":60525,\"start\":60508},{\"end\":60721,\"start\":60689},{\"end\":61060,\"start\":61024},{\"end\":61390,\"start\":61374},{\"end\":61625,\"start\":61583},{\"end\":61917,\"start\":61893},{\"end\":62234,\"start\":62137},{\"end\":62573,\"start\":62566},{\"end\":62863,\"start\":62822},{\"end\":63223,\"start\":63208},{\"end\":63515,\"start\":63443},{\"end\":63733,\"start\":63678},{\"end\":63974,\"start\":63896},{\"end\":64414,\"start\":64330},{\"end\":64845,\"start\":64796},{\"end\":65081,\"start\":65036}]"}}}, "year": 2023, "month": 12, "day": 17}