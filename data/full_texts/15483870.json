{"id": 15483870, "updated": "2023-09-29 06:04:58.072", "metadata": {"title": "Diffusion-Convolutional Neural Networks", "authors": "[{\"first\":\"James\",\"last\":\"Atwood\",\"middle\":[]},{\"first\":\"Don\",\"last\":\"Towsley\",\"middle\":[]}]", "venue": "NIPS", "journal": "arXiv: Learning", "publication_date": {"year": 2015, "month": 11, "day": 6}, "abstract": "We present diffusion-convolutional neural networks (DCNNs), a new model for graph-structured data. Through the introduction of a diffusion-convolution operation, we show how diffusion-based representations can be learned from graph-structured data and used as an effective basis for node classification. DCNNs have several attractive qualities, including a latent representation for graphical data that is invariant under isomorphism, as well as polynomial-time prediction and learning that can be represented as tensor operations and efficiently implemented on the GPU. Through several experiments with real structured datasets, we demonstrate that DCNNs are able to outperform probabilistic relational models and kernel-on-graph methods at relational node classification tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1511.02136", "mag": "2963984147", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/AtwoodT16", "doi": null}}, "content": {"source": {"pdf_hash": "ae7c646e95bba8e342a6e4f55cda46a87d8f3230", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1511.02136v6.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8c80732c957c1698eb4cb4c2ebe0a6d774f2f973", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ae7c646e95bba8e342a6e4f55cda46a87d8f3230.txt", "contents": "\nDiffusion-Convolutional Neural Networks\n\n\nJames Atwood \nCollege of Information and Computer Science\nUniversity of Massachusetts Amherst\n01003MA\n\nDon Towsley \nCollege of Information and Computer Science\nUniversity of Massachusetts Amherst\n01003MA\n\nDiffusion-Convolutional Neural Networks\n\nWe present diffusion-convolutional neural networks (DCNNs), a new model for graph-structured data. Through the introduction of a diffusion-convolution operation, we show how diffusion-based representations can be learned from graphstructured data and used as an effective basis for node classification. DCNNs have several attractive qualities, including a latent representation for graphical data that is invariant under isomorphism, as well as polynomial-time prediction and learning that can be represented as tensor operations and efficiently implemented on the GPU. Through several experiments with real structured datasets, we demonstrate that DCNNs are able to outperform probabilistic relational models and kernel-on-graph methods at relational node classification tasks.\n\nIntroduction\n\nWorking with structured data is challenging. On one hand, finding the right way to express and exploit structure in data can lead to improvements in predictive performance; on the other, finding such a representation may be difficult, and adding structure to a model can dramatically increase the complexity of prediction and learning.\n\nThe goal of this work is to design a flexible model for a general class of structured data that offers improvements in predictive performance while avoiding an increase in complexity. To accomplish this, we extend convolutional neural networks (CNNs) to general graph-structured data by introducing a 'diffusion-convolution' operation. Briefly, rather than scanning a 'square' of parameters across a grid-structured input like the standard convolution operation, the diffusion-convolution operation builds a latent representation by scanning a diffusion process across each node in a graph-structured input.\n\nThis model is motivated by the idea that a representation that encapsulates graph diffusion can provide a better basis for prediction than a graph itself. Graph diffusion can be represented as a matrix power series, providing a straightforward mechanism for including contextual information about entities that can be computed in polynomial time and efficiently implemented on the GPU.\n\nIn this paper, we present diffusion-convolutional neural networks (DCNNs) and explore their performance at various classification tasks on graphical data. Many techniques include structural information in classification tasks, such as probabilistic relational models and kernel methods; DCNNs offer a complementary approach that provides a significant improvement in predictive performance at node classification tasks.\n\nAs a model class, DCNNs offer several advantages:\n\n\u2022 Accuracy: In our experiments, DCNNs significantly outperform alternative methods for node classification tasks and offer comparable performance to baseline methods for graph classification tasks. \nW d (H x F) N t Y t N t N t (a) Node classification P t X t N t N t F Z t H Y t W c (H x F) W d (H x F) N t (b) Graph classification P t X t E t N t F Z t H W c (H x F) W d (H x F) E t Y t N t E t B t N t\n(c) Edge classification Figure 1: DCNN model definition for node, graph, and edge classification tasks.\n\n\u2022 Flexibility: DCNNs provide a flexible representation of graphical data that encodes node features, edge features, and purely structural information with little preprocessing. DC-NNs can be used for a variety of classification tasks with graphical data, including node classification, edge classification, and whole-graph classification. \u2022 Speed: Prediction from an DCNN can be expressed as a series of polynomial-time tensor operations, allowing the model to be implemented efficiently on a GPU using existing libraries.\n\nThe remainder of this paper is organized as follows. In Section 2, we present a formal definition of the model, including descriptions of prediction and learning procedures. This is followed by several experiments in Section 3 that explore the performance of DCNNs at node and graph classification tasks. We briefly describe the limitations of the model in Section 4, then, in Section 5, we present related work and discuss the relationship between DCNNs and other methods. Finally, conclusions and future work are presented in Section 6.\n\n\nModel\n\nConsider a situation where we have a set of T graphs G = {G t |t \u2208 1...T }. Each graph G t = (V t , E t ) is composed of vertices V t and edges E t . The vertices are collectively described by an N t \u00d7 F design matrix X t of features 1 , where N t is the number of nodes in G t , and the edges E t are encoded by an N t \u00d7 N t adjacency matrix A t , from which we can compute a degree-normalized transition matrix P t that gives the probability of jumping from node i to node j in one step. No constraints are placed on the form G t ; the graph can be weighted or unweighted, directed or undirected. Either the nodes, edges, or graphs have labels Y associated with them, with the dimensionality of Y differing in each case.\n\nWe are interested in learning to predict Y ; that is, to predict a label for each of the nodes in each graph, or a label for each of the edges in each graph, or a label for each graph itself. In each case, we have access to some labeled entities (be they nodes, graphs, or edges), and our task is predict the values of the remaining unlabeled entities.\n\nThis setting is capable of representing several well-studied machine learning tasks. If T = 1 (i.e. there is only one input graph) and the labels Y are associated with the nodes or edges, this reduces to the problem of semisupervised classification; if there are no edges present in the input graph, this reduces further to standard supervised classification. If T > 1 and the labels Y are associated with each graph, then this represents the problem of supervised graph classification.\n\nDCNNs were designed to perform any task that can be represented within this formulation. An DCNN takes G as input and returns either a hard prediction for Y or a conditional distribution P(Y |X). Each entity of interest (be it a node, a graph, or an edge) is transformed to a diffusionconvolutional representation, which is a H \u00d7 F real matrix defined by H hops of graph diffusion over F features, and it is defined by an H \u00d7 F real-valued weight tensor W c and a nonlinear differentiable function f that computes the activations. So, for node classification tasks, the diffusion-convolutional representation of graph t, Z t , will be a N t \u00d7 H \u00d7 F tensor, as illustrated in Figure 1a; For graph or edge classification tasks, Z t will be a H \u00d7 F matrix or a M t \u00d7 H \u00d7 F tensor respectively, as illustrated in Figures 1b and 1c.\n\nThe term 'diffusion-convolution' is meant to evoke the ideas of feature learning, parameter tying, and invariance that are characteristic of convolutional neural networks. The core operation of a DCNN is a mapping from nodes and their features to the results of a diffusion process that begins at that node. In contrast with standard CNNs, DCNN parameters are tied according to search depth rather than their position in a grid. The diffusion-convolutional representation is invariant with respect to node index rather than position; in other words, the diffusion-convolututional activations of two isomorphic input graphs will be the same 2 . Unlike standard CNNs, DCNNs have no pooling operation.\n\nNode Classification Consider a node classification task where a label Y is predicted for each input node in a graph. If we let P * t be an N t \u00d7 H \u00d7 N t tensor containing the power series of P t , the diffusion-convolutional activation Z tijk for node i, hop j, and feature k of graph t is given by\nZ tijk = f W c jk \u00b7 Nt l=1 P * tijl X tlk(1)\nThe activations can be expressed more concisely using tensor notation as The model is completed by a dense layer that connects Z to Y . A hard prediction for Y , denoted\u0176 , can be obtained by taking the maximum activation and a conditional probability distribution P(Y |X) can be found by applying the softmax function:\nZ t = f (W c P * t X t )(2)Y = arg max f W d Z (3) P(Y |X) = softmax f W d Z (4)\nThis keeps the same form in the following extensions.\n\nGraph Classification DCNNs can be extended to graph classification by simply taking the mean activation over the nodes Edge Classification and Edge Features Edge features and labels can be included by converting each edge to a node that is connected to the nodes at the tail and head of the edge. This graph graph can be constructed efficiently by augmenting the adjacency matrix with the incidence matrix:\nZ t = f W c 1 T Nt P * t X t /N t (5) where 1 Nt is an N t \u00d7 1A t = A t B T t B t 0(6)\nA t can then be used to compute P t and used in place of P t to classify nodes and edges.\n\nPurely Structural DCNNs DCNNs can be applied to input graphs with no features by associating a 'bias feature' with value 1.0 with each node. Richer structure can be encoded by adding additional structural node features such as Pagerank or clustering coefficient, although this does introduce some hand-engineering and pre-processing.\n\nLearning DCNNs are learned via stochastic minibatch gradient descent on backpropagated error. At each epoch, node indices are randomly grouped into several batches. The error of each batch is computed by taking slices of the graph definition power series and propagating the input forward to predict the output, then setting the weights by gradient ascent on the back-propagated error. We also make use of windowed early stopping; training is ceased if the validation error of a given epoch is greater than the average of the last few epochs. \n\n\nExperiments\n\nIn this section we present several experiments to investigate how well DCNNs perform at node and graph classification tasks. In each case we compare DCNNs to other well-known and effective approaches to the task.\n\nIn each of the following experiments, we use the AdaGrad algorithm [1] for gradient ascent with a learning rate of 0.05. All weights are initialized by sampling from a normal distribution with mean zero and variance 0.01. We choose the hyperbolic tangent for the nonlinear differentiable function f and use the multiclass hinge loss between the model predictions and ground truth as the training objective. The model was implemented in Python using Lasagne and Theano [2].\n\n\nNode classification\n\nWe ran several experiments to investigate how well DCNNs can classify nodes within a single graph. The graphs were constructed from the Cora and Pubmed datasets, which each consist of scientific papers (nodes), citations between papers (edges), and subjects (labels).\n\nProtocol In each experiment, the set G consists of a single graph G. During each trial, the input graph's nodes are randomly partitioned into training, validation, and test sets, with each set having the same number of nodes. During training, all node features X, all edges E, and the labels Y of the training and validation sets are visible to the model. We report classification accuracy as well as micro-and macro-averaged F1; each measure is reported as a mean and confidence interval computed from several trials.\n\nWe also provide learning curves for the CORA and Pubmed datasets. In this experiment, the validation and test set each contain 10% of the nodes, and the amount of training data is varied between 10% and 100% of the remaining nodes.  Table 1: A comparison of the performance between baseline 1 and 2-regularized logistic regression models, exponential diffusion and Laplacian exponential diffusion kernel models, loopy belief propagation (LBP) on a partially-observed conditional random field (CRF), and a two-hop DCNN on the Cora and Pubmed datasets. The DCNN offers the best performance according to each measure, and the gain is statistically significant in each case. The CRF-LBP result is quoted from [3], which follows the same experimental protocol. Baseline Methods 'l1logistic' and 'l2logistic' indicate 1 and 2-regularized logistic regression, respectively. The inputs to the logistic regression models are the node features alone (e.g. the graph structure is not used) and the regularization parameter is tuned using the validation set. 'KED' and 'KLED' denote the exponential diffusion and Laplacian exponential diffusion kernels-on-graphs, respectively, which have previously been shown to perform well on the Cora dataset [4]. These kernel models take the graph structure as input (e.g. node features are not used) and the validation set is used to determine the kernel hyperparameters. 'CRF-LBP' indicates a partially-observed conditional random field that uses loopy belief propagation for inference. Results for this model are quoted from prior work [3] that uses the same dataset and experimental protocol.\n\nNode Classification Data The Cora corpus [5] consists of 2,708 machine learning papers and the 5,429 citation edges that they share. Each paper is assigned a label drawn from seven possible machine learning subjects, and each paper is represented by a bit vector where each feature corresponds to the presence or absence of a term drawn from a dictionary with 1,433 unique entries. We treat the citation network as an undirected graph.\n\nThe Pubmed corpus [5] consists of 19,717 scientific papers from the Pubmed database on the subject of diabetes. Each paper is assigned to one of three classes. The citation network that joins the papers consists of 44,338 links, and each paper is represented by a Term Frequency Inverse Document Frequency (TFIDF) vector drawn from a dictionary with 500 terms. As with the CORA corpus, we construct an adjacency-based DCNN that treats the citation network as an undirected graph. Table 1 compares the performance of a two-hop DCNN with several baselines. The DCNN offers the best performance according to different measures including classification accuracy and micro-and macro-averaged F1, and the gain is statistically significant in each case with negligible p-values. For all models except the CRF, we assessed this via a one-tailed two-sample Welch's t-test. The CRF result is quoted from prior work, so we used a one-tailed one-sample test. Figure 2c show the learning curves for the Cora and Pubmed datasets. The DCNN generally outperforms the baseline methods on the Cora dataset regardless of the amount of training data available, although the Laplacian exponential diffusion kernel does offer comparable performance when the entire training set is available. Note that the kernel methods were prohibitively slow to run on the Pubmed dataset, so we do not include them in the learning curve.\n\n\nResults Discussion\n\n\nFigures 2b and\n\nFinally, the impact of diffusion breadth on performance is shown in Figure 2. Most of the performance is gained as the diffusion breadth grows from zero to three hops, then levels out as the diffusion process converges.\n\n\nGraph Classification\n\nWe also ran experiments to investigate how well DCNNs can learn to label whole graphs.\n\nProtocol  graphs and features are made visible and the graph labels are predicted and compared with ground truth. Table 2 reports the mean accuracy, micro-averaged F1, and macro-averaged F1 over several trials.\n\nWe also provide learning curves for the MUTAG (Figure 3b) and ENZYMES (Figure 3c) datasets.\n\nIn these experiments, validation and test sets each containing 10% of the graphs, and we report the performance of each model as a function of the proportion of the remaining graphs that are made available for training.\n\nBaseline Methods As a simple baseline, we apply linear classifiers to the average feature vector of each graph; 'l1logistic' and 'l2logistic' indicate 1 and 2-regularized logistic regression applied as described. 'deepwl' indicates the Weisfeiler-Lehman (WL) subtree deep graph kernel. Deep graph kernels decompose a graph into substructures, treat those substructures as words in a sentence, and fit a word-embedding model to obtain a vectorization [6].\n\nGraph Classification Data We apply DCNNs to a standard set of graph classification datasets that consists of NCI1, NCI109, MUTAG, PCI, and ENZYMES. The NCI1 and NCI109 [7] datasets consist of 4100 and 4127 graphs that represent chemical compounds. Each graph is labeled with whether it is has the ability to suppress or inhibit the growth of a panel of human tumor cell lines, and each node is assigned one of 37 (for NCI1) or 38 (for NCI109) possible labels. MUTAG [8] contains 188 nitro compounds that are labeled as either aromatic or heteroaromatic with seven node features. PTC [9] contains 344 compounds labeled with whether they are carcinogenic in rats with 19 node features. Finally, ENZYMES [10] is a balanced dataset containing 600 proteins with three node features.\n\n\nResults Discussion\n\nIn contrast with the node classification experiments, there is no clear best model choice across the datasets or evaluation measures. In fact, according to Table 2, the only clear choice is the 'deepwl' graph kernel model on the ENZYMES dataset, which significantly outperforms the other methods in terms of accuracy and micro-and macro-averaged F measure. Furthermore, as shown in Figure 3, there is no clear benefit to broadening the search breadth H. These results suggest that, while diffusion processes are an effective representation for nodes, they do a poor job of summarizing entire graphs. It may be possible to improve these results by finding a more effective way to aggregate the node operations than a simple mean, but we leave this as future work.\n\n\nLimitations\n\nScalability DCNNs are realized as a series of operations on dense tensors. Storing the largest tensor (P * , the transition matrix power series) requires O(N 2 t H) memory, which can lead to out-of-memory errors on the GPU for very large graphs in practice. As such, DCNNs can be readily applied to graphs of tens to hundreds of thousands of nodes, but not to graphs with millions to billions of nodes.\n\nLocality The model is designed to capture local behavior in graph-structured data. As a consequence of constructing the latent representation from diffusion processes that begin at each node, we may fail to encode useful long-range spatial dependencies between individual nodes or other non-local graph behavior.\n\n\nRelated Work\n\nIn this section we describe existing approaches to the problems of semi-supervised learning, graph classification, and edge classification, and discuss their relationship to DCNNs.\n\nOther Graph-Based Neural Network Models Other researchers have investigated how CNNs can be extended from grid-structured to more general graph-structured data. [11] propose a spatial method with ties to hierarchical clustering, where the layers of the network are defined via a hierarchical partitioning of the node set. In the same paper, the authors propose a spectral method that extends the notion of convolution to graph spectra. Later, [12] applied these techniques to data where a graph is not immediately present but must be inferred. DCNNs, which fall within the spatial category, are distinct from this work because their parameterization makes them transferable; a DCNN learned on one graph can be applied to another. A related branch of work that has focused on extending convolutional neural networks to domains where the structure of the graph itself is of direct interest [13,14,15]. For example, [15] construct a deep convolutional model that learns real-valued fingerprint representation of chemical compounds.\n\nProbabilistic Relational Models DCNNs also share strong ties to probabilistic relational models (PRMs), a family of graphical models that are capable of representing distributions over relational data [16]. In contrast to PRMs, DCNNs are deterministic, which allows them to avoid the exponential blowup in learning and inference that hampers PRMs.\n\nOur results suggest that DCNNs outperform partially-observed conditional random fields, the stateof-the-art model probabilistic relational model for semi-supervised learning. Furthermore, DCNNs offer this performance at considerably lower computational cost. Learning the parameters of both DCNNs and partially-observed CRFs involves numerically minimizing a nonconvex objective -the backpropagated error in the case of DCNNs and the negative marginal log-likelihood for CRFs. In practice, the marginal log-likelihood of a partially-observed CRF is computed using a contrastof-partition-functions approach that requires running loopy belief propagation twice; once on the entire graph and once with the observed labels fixed [17]. This algorithm, and thus each step in the numerical optimization, has exponential time complexity O(E t N Ct t ) where C t is the size of the maximal clique in G t [18]. In contrast, the learning subroutine for an DCNN requires only one forward and backward pass for each instance in the training data. The complexity is dominated by the matrix multiplication between the graph definition matrix A and the design matrix V , giving an overall polynomial complexity of O(N 2 t F ).\n\nKernel Methods Kernel methods define similarity measures either between nodes (so-called kernels on graphs) [4] or between graphs (graph kernels) and these similarities can serve as a basis for prediction via the kernel trick. Note that 'kernels on graphs', which are concerned with nodes, should not be confused with 'graph kernels', which are concerned with whole graphs. The performance of graph kernels can be improved by decomposing a graph into substructures, treating those substructures as a words in a sentence, and fitting a word-embedding model to obtain a vectorization [6].\n\nDCNNs share ties with the exponential diffusion family of kernels on graphs. The exponential diffusion graph kernel K ED is a sum of a matrix power series:\nK ED = \u221e j=0 \u03b1 j A j j! = exp(\u03b1A)(7)\nThe diffusion-convolution activation given in (2) is also constructed from a power series. However, the representations have several important differences. First, the weights in (2) are learned via backpropagation, whereas the kernel representation is not learned from data. Second, the diffusionconvolutional representation is built from both node features and the graph structure, whereas the exponential diffusion kernel is built from the graph structure alone. Finally, the representations have different dimensions: K ED is an N t \u00d7 N t kernel matrix, whereas Z t is a N t \u00d7 H \u00d7 F tensor that does not conform to the definition of a kernel.\n\n\nConclusion and Future Work\n\nBy learning a representation that encapsulates the results of graph diffusion, diffusion-convolutional neural networks offer performance improvements over probabilistic relational models and kernel methods at node classification tasks. We intend to investigate methods for a) improving DCNN performance at graph classification tasks and b) making the model scalable in future work.\n\n\nAppendix: Representation Invariance for Isomorphic Graphs\n\nIf two graphs G 1 and G 2 are isomorphic, then their diffusion-convolutional activations are the same. Proof by contradiction; assume that G 1 and G 2 are isomorphic and that their diffusion-convolutional activations are different. The diffusion-convolutional activations can be written as\nZ 1jk = f W c jk v\u2208V1 v \u2208V1 P * 1vjv X 1v k /N 1 Z 2jk = f W c jk v\u2208V2 v \u2208V2 P * 2vjv X 2v k /N 2 Note that V 1 = V 2 = V X 1vk = X 2vk = X vk \u2200 v \u2208 V, k \u2208 [1, F ] P * 1vjv = P * 2vjv = P * vjv \u2200 v, v \u2208 V, j \u2208 [0, H] N 1 = N 2 = N\nby isomorphism, allowing us to rewrite the activations as\nZ 1jk = f W c jk v\u2208V v \u2208V P * vjv X v k /N Z 2jk = f W c jk v\u2208V v \u2208V P * vjv X v k /N\nThis implies that Z 1 = Z 2 which presents a contradiction and completes the proof.\n\n\nwhere the operator represents element-wise multiplication; see Figure 1a. The model only entails O(H \u00d7 F ) parameters, making the size of the latent diffusion-convolutional representation independent of the size of the input.\n\n\nvector of ones, as illustrated inFigure 1b.\n\nFigure 2 :\n2The effect of search breadth (2a) and learning curves (2b -2c) for the Cora and Pubmed datasets.\n\nFigure 3 :\n3The effect of search breadth (3a), as well as learning curves for the MUTAG (3b) and ENZYMES (3c) datasets.\n\n\nAt the beginning of each trial, input graphs are randomly assigned to training, validation, or test, with each set having the same number of graphs. During the learning phase, the training and validation graphs, their node features, and their labels are made visible; the training set is used to determine the parameters and the validation set to determine hyperparameters. At test time, the testNCI1 \n\nNCI109 \nModel \nAccuracy F (micro) F (macro) Accuracy F (micro) F (macro) \nl1logistic \n0.5728 \n0.5728 \n0.5711 \n0.5555 \n0.5555 \n0.5411 \nl2logistic \n0.5688 \n0.5688 \n0.5641 \n0.5586 \n0.5568 \n0.5402 \ndeepwl \n0.6215 \n0.6215 \n0.5821 \n0.5801 \n0.5801 \n0.5178 \n2-hop DCNN \n0.6250 \n0.5807 \n0.5807 \n0.6275 \n0.5884 \n0.5884 \n5-hop DCNN \n0.6261 \n0.5898 \n0.5898 \n0.6286 \n0.5950 \n0.5899 \nMUTAG \nPTC \nModel \nAccuracy F (micro) F (macro) Accuracy F (micro) F (macro) \nl1logistic \n0.7190 \n0.7190 \n0.6405 \n0.5470 \n0.5470 \n0.4272 \nl2logistic \n0.7016 \n0.7016 \n0.5795 \n0.5565 \n0.5565 \n0.4460 \ndeepwl \n0.6563 \n0.6563 \n0.5942 \n0.5113 \n0.5113 \n0.4444 \n2-hop DCNN \n0.6635 \n0.7975 \n0.79747 \n0.5660 \n0.0500 \n0.0531 \n5-hop DCNN \n0.6698 \n0.8013 \n0.8013 \n0.5530 \n0.0 \n0.0526 \nENZYMES \nModel \nAccuracy F (micro) F (macro) \nl1logistic \n0.1640 \n0.1640 \n0.0904 \nl2logistic \n0.2030 \n0.2030 \n0.1110 \ndeepwl \n0.2155 \n0.2155 \n0.1431 \n2-hop DCNN \n0.1590 \n0.1590 \n0.0809 \n5-hop DCNN \n0.1810 \n0.1810 \n0.0991 \n\n\n\nTable 2 :\n2A comparison of the performance between baseline methods and two and five-hop DCNNs \non several graph classification datasets. \n\n\nWithout loss of generality, we assume that the features are real-valued.\nA proof is given in the appendix.\nAcknowledgmentsWe would like to thank Bruno Ribeiro, Pinar Yanardag, and David Belanger for their feedback on drafts of this paper.\nAdaptive subgradient methods for online learning and stochastic optimization. John Duchi, Elad Hazan, Yoram Singer, The Journal of Machine Learning Research. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 2011.\n\nTheano: a CPU and GPU math expression compiler. James Bergstra, Olivier Breuleux, Fr\u00e9d\u00e9ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, Yoshua Bengio, Proceedings of the Python for Scientific Computing Conference (SciPy). the Python for Scientific Computing Conference (SciPy)James Bergstra, Olivier Breuleux, Fr\u00e9d\u00e9ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), 2010.\n\nLink-based classification. P Sen, Getoor, Technical ReportP Sen and L Getoor. Link-based classification. Technical Report, 2007.\n\nAn experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification. Fran\u00e7ois Fouss, Kevin Francoisse, Luh Yen, Alain Pirotte, Marco Saerens, Neural Networks. 31Fran\u00e7ois Fouss, Kevin Francoisse, Luh Yen, Alain Pirotte, and Marco Saerens. An experimen- tal investigation of kernels on graphs for collaborative recommendation and semisupervised classification. Neural Networks, 31:53-72, July 2012.\n\nCollective Classification in Network Data. Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, Tina Eliassi-Rad, AI MagazinePrithviraj Sen, Galileo Mark Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad. Collective Classification in Network Data. AI Magazine, 2008.\n\nDeep Graph Kernels. Pinar Yanardag, S V N Vishwanathan, the 21th ACM SIGKDD International Conference. New York, New York, USAACM PressPinar Yanardag and S V N Vishwanathan. Deep Graph Kernels. In the 21th ACM SIGKDD International Conference, pages 1365-1374, New York, New York, USA, 2015. ACM Press.\n\nComparison of descriptor spaces for chemical compound retrieval and classification. Nikil Wale, A Ian, George Watson, Karypis, Knowledge and Information Systems. 143Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. Knowledge and Information Systems, 14(3):347-375, August 2007.\n\nStructure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Asim Kumar Debnath, Rosa L Lopez De Compadre, Gargi Debnath, Alan J Shusterman, Corwin Hansch, Journal of medicinal chemistry. 342Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2):786-797, 1991.\n\nStatistical evaluation of the predictive toxicology challenge. Ashwin Hannu Toivonen, Srinivasan, D Ross, Stefan King, Christoph Kramer, Helma, Bioinformatics. 1910Hannu Toivonen, Ashwin Srinivasan, Ross D King, Stefan Kramer, and Christoph Helma. Statis- tical evaluation of the predictive toxicology challenge 2000-2001. Bioinformatics, 19(10):1183- 1193, 2003.\n\nProtein function prediction via graph kernels. M Karsten, Borgwardt, Cheng Soon, Stefan Ong, Sch\u00f6nauer, Alex J Svn Vishwanathan, Hans-Peter Smola, Kriegel, Bioinformatics. 211supplKarsten M Borgwardt, Cheng Soon Ong, Stefan Sch\u00f6nauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl 1):i47-i56, 2005.\n\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, Yann Lecun, Spectral networks and locally connected networks on graphs. arXiv.org. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv.org, 2014.\n\nM Henaff, Y Bruna, Lecun, Deep Convolutional Networks on Graph-Structured Data. arXiv.org. M Henaff, J Bruna, and Y LeCun. Deep Convolutional Networks on Graph-Structured Data. arXiv.org, 2015.\n\nThe Graph Neural Network Model. F Scarselli, Ah Gori, M Chung Tsoi, G Hagenbuchner, Monfardini, IEEE Transactions on Neural Networks. F Scarselli, M Gori, Ah Chung Tsoi, M Hagenbuchner, and G Monfardini. The Graph Neural Network Model. IEEE Transactions on Neural Networks, 2009.\n\nNeural Network for Graphs: A Contextual Constructive Approach. A Micheli, IEEE Transactions on Neural Networks. A Micheli. Neural Network for Graphs: A Contextual Constructive Approach. IEEE Transac- tions on Neural Networks, 2009.\n\n. Dougal David K Duvenaud, Jorge Maclaurin, Rafael Aguilera-Iparraguirre, Timothy G\u00f3mez-Bombarelli, Al\u00e1n Hirzel, Ryan P Aspuru-Guzik, Adams, Convolutional Networks on Graphs for Learning Molecular Fingerprints. NIPS. David K Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael G\u00f3mez-Bombarelli, Timothy Hirzel, Al\u00e1n Aspuru-Guzik, and Ryan P Adams. Convolutional Networks on Graphs for Learning Molecular Fingerprints. NIPS, 2015.\n\nProbabilistic Graphical Models: Principles and Techniques. Daphne Koller, Nir Friedman, The MIT PressDaphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. The MIT Press, 2009.\n\nScene segmentation with crfs learned from partially labeled images. NIPS. Jakob Verbeek, William Triggs, Jakob Verbeek and William Triggs. Scene segmentation with crfs learned from partially labeled images. NIPS, 2007.\n\nTrevor Cohn, Efficient Inference in Large Conditional Random Fields. ECML. Trevor Cohn. Efficient Inference in Large Conditional Random Fields. ECML, 2006.\n", "annotations": {"author": "[{\"end\":145,\"start\":43},{\"end\":247,\"start\":146}]", "publisher": null, "author_last_name": "[{\"end\":55,\"start\":49},{\"end\":157,\"start\":150}]", "author_first_name": "[{\"end\":48,\"start\":43},{\"end\":149,\"start\":146}]", "author_affiliation": "[{\"end\":144,\"start\":57},{\"end\":246,\"start\":159}]", "title": "[{\"end\":40,\"start\":1},{\"end\":287,\"start\":248}]", "venue": null, "abstract": "[{\"end\":1067,\"start\":289}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10127,\"start\":10124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10528,\"start\":10525},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12050,\"start\":12047},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12580,\"start\":12577},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12911,\"start\":12908},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13011,\"start\":13008},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13425,\"start\":13422},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16156,\"start\":16153},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16330,\"start\":16327},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16628,\"start\":16625},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16745,\"start\":16742},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16864,\"start\":16860},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18817,\"start\":18813},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19099,\"start\":19095},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19544,\"start\":19540},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19547,\"start\":19544},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19550,\"start\":19547},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19569,\"start\":19565},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19887,\"start\":19883},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20760,\"start\":20756},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20930,\"start\":20926},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21354,\"start\":21351},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21828,\"start\":21825}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24119,\"start\":23892},{\"attributes\":{\"id\":\"fig_1\"},\"end\":24165,\"start\":24120},{\"attributes\":{\"id\":\"fig_2\"},\"end\":24275,\"start\":24166},{\"attributes\":{\"id\":\"fig_3\"},\"end\":24396,\"start\":24276},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":25767,\"start\":24397},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":25909,\"start\":25768}]", "paragraph": "[{\"end\":1418,\"start\":1083},{\"end\":2027,\"start\":1420},{\"end\":2414,\"start\":2029},{\"end\":2835,\"start\":2416},{\"end\":2886,\"start\":2837},{\"end\":3086,\"start\":2888},{\"end\":3395,\"start\":3292},{\"end\":3919,\"start\":3397},{\"end\":4459,\"start\":3921},{\"end\":5191,\"start\":4469},{\"end\":5545,\"start\":5193},{\"end\":6033,\"start\":5547},{\"end\":6862,\"start\":6035},{\"end\":7562,\"start\":6864},{\"end\":7862,\"start\":7564},{\"end\":8227,\"start\":7908},{\"end\":8362,\"start\":8309},{\"end\":8770,\"start\":8364},{\"end\":8947,\"start\":8858},{\"end\":9282,\"start\":8949},{\"end\":9827,\"start\":9284},{\"end\":10055,\"start\":9843},{\"end\":10529,\"start\":10057},{\"end\":10820,\"start\":10553},{\"end\":11340,\"start\":10822},{\"end\":12965,\"start\":11342},{\"end\":13402,\"start\":12967},{\"end\":14805,\"start\":13404},{\"end\":15064,\"start\":14845},{\"end\":15175,\"start\":15089},{\"end\":15387,\"start\":15177},{\"end\":15480,\"start\":15389},{\"end\":15701,\"start\":15482},{\"end\":16157,\"start\":15703},{\"end\":16936,\"start\":16159},{\"end\":17721,\"start\":16959},{\"end\":18139,\"start\":17737},{\"end\":18453,\"start\":18141},{\"end\":18650,\"start\":18470},{\"end\":19680,\"start\":18652},{\"end\":20029,\"start\":19682},{\"end\":21241,\"start\":20031},{\"end\":21829,\"start\":21243},{\"end\":21986,\"start\":21831},{\"end\":22669,\"start\":22024},{\"end\":23081,\"start\":22700},{\"end\":23432,\"start\":23143},{\"end\":23721,\"start\":23664},{\"end\":23891,\"start\":23808}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3291,\"start\":3087},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7907,\"start\":7863},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8255,\"start\":8228},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8308,\"start\":8255},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8833,\"start\":8771},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8857,\"start\":8833},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22023,\"start\":21987},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23663,\"start\":23433},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23807,\"start\":23722}]", "table_ref": "[{\"end\":11582,\"start\":11575},{\"end\":13891,\"start\":13884},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15298,\"start\":15291},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":17122,\"start\":17115}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1081,\"start\":1069},{\"attributes\":{\"n\":\"2\"},\"end\":4467,\"start\":4462},{\"attributes\":{\"n\":\"3\"},\"end\":9841,\"start\":9830},{\"attributes\":{\"n\":\"3.1\"},\"end\":10551,\"start\":10532},{\"end\":14826,\"start\":14808},{\"end\":14843,\"start\":14829},{\"attributes\":{\"n\":\"3.2\"},\"end\":15087,\"start\":15067},{\"end\":16957,\"start\":16939},{\"attributes\":{\"n\":\"4\"},\"end\":17735,\"start\":17724},{\"attributes\":{\"n\":\"5\"},\"end\":18468,\"start\":18456},{\"attributes\":{\"n\":\"6\"},\"end\":22698,\"start\":22672},{\"attributes\":{\"n\":\"7\"},\"end\":23141,\"start\":23084},{\"end\":24177,\"start\":24167},{\"end\":24287,\"start\":24277},{\"end\":25778,\"start\":25769}]", "table": "[{\"end\":25767,\"start\":24795},{\"end\":25909,\"start\":25780}]", "figure_caption": "[{\"end\":24119,\"start\":23894},{\"end\":24165,\"start\":24122},{\"end\":24275,\"start\":24179},{\"end\":24396,\"start\":24289},{\"end\":24795,\"start\":24399}]", "figure_ref": "[{\"end\":3324,\"start\":3316},{\"end\":6719,\"start\":6710},{\"end\":6861,\"start\":6844},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14360,\"start\":14351},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14921,\"start\":14913},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15445,\"start\":15435},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15469,\"start\":15459},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17349,\"start\":17341}]", "bib_author_first_name": "[{\"end\":26231,\"start\":26227},{\"end\":26243,\"start\":26239},{\"end\":26256,\"start\":26251},{\"end\":26529,\"start\":26524},{\"end\":26547,\"start\":26540},{\"end\":26566,\"start\":26558},{\"end\":26582,\"start\":26576},{\"end\":26598,\"start\":26592},{\"end\":26617,\"start\":26608},{\"end\":26636,\"start\":26630},{\"end\":26650,\"start\":26645},{\"end\":26671,\"start\":26665},{\"end\":27122,\"start\":27121},{\"end\":27351,\"start\":27343},{\"end\":27364,\"start\":27359},{\"end\":27380,\"start\":27377},{\"end\":27391,\"start\":27386},{\"end\":27406,\"start\":27401},{\"end\":27725,\"start\":27715},{\"end\":27738,\"start\":27731},{\"end\":27743,\"start\":27739},{\"end\":27759,\"start\":27752},{\"end\":27772,\"start\":27768},{\"end\":27786,\"start\":27781},{\"end\":27802,\"start\":27798},{\"end\":28020,\"start\":28015},{\"end\":28036,\"start\":28031},{\"end\":28386,\"start\":28381},{\"end\":28394,\"start\":28393},{\"end\":28406,\"start\":28400},{\"end\":28814,\"start\":28810},{\"end\":28834,\"start\":28830},{\"end\":28836,\"start\":28835},{\"end\":28861,\"start\":28856},{\"end\":28875,\"start\":28871},{\"end\":28877,\"start\":28876},{\"end\":28896,\"start\":28890},{\"end\":29316,\"start\":29310},{\"end\":29346,\"start\":29345},{\"end\":29359,\"start\":29353},{\"end\":29375,\"start\":29366},{\"end\":29660,\"start\":29659},{\"end\":29699,\"start\":29693},{\"end\":29720,\"start\":29716},{\"end\":29722,\"start\":29721},{\"end\":29751,\"start\":29741},{\"end\":29998,\"start\":29994},{\"end\":30014,\"start\":30006},{\"end\":30030,\"start\":30024},{\"end\":30042,\"start\":30038},{\"end\":30260,\"start\":30259},{\"end\":30270,\"start\":30269},{\"end\":30487,\"start\":30486},{\"end\":30501,\"start\":30499},{\"end\":30509,\"start\":30508},{\"end\":30523,\"start\":30522},{\"end\":30799,\"start\":30798},{\"end\":30976,\"start\":30970},{\"end\":31000,\"start\":30995},{\"end\":31018,\"start\":31012},{\"end\":31049,\"start\":31042},{\"end\":31072,\"start\":31068},{\"end\":31087,\"start\":31081},{\"end\":31476,\"start\":31470},{\"end\":31488,\"start\":31485},{\"end\":31704,\"start\":31699},{\"end\":31721,\"start\":31714},{\"end\":31851,\"start\":31845}]", "bib_author_last_name": "[{\"end\":26237,\"start\":26232},{\"end\":26249,\"start\":26244},{\"end\":26263,\"start\":26257},{\"end\":26538,\"start\":26530},{\"end\":26556,\"start\":26548},{\"end\":26574,\"start\":26567},{\"end\":26590,\"start\":26583},{\"end\":26606,\"start\":26599},{\"end\":26628,\"start\":26618},{\"end\":26643,\"start\":26637},{\"end\":26663,\"start\":26651},{\"end\":26678,\"start\":26672},{\"end\":27126,\"start\":27123},{\"end\":27134,\"start\":27128},{\"end\":27357,\"start\":27352},{\"end\":27375,\"start\":27365},{\"end\":27384,\"start\":27381},{\"end\":27399,\"start\":27392},{\"end\":27414,\"start\":27407},{\"end\":27729,\"start\":27726},{\"end\":27750,\"start\":27744},{\"end\":27766,\"start\":27760},{\"end\":27779,\"start\":27773},{\"end\":27796,\"start\":27787},{\"end\":27814,\"start\":27803},{\"end\":28029,\"start\":28021},{\"end\":28049,\"start\":28037},{\"end\":28391,\"start\":28387},{\"end\":28398,\"start\":28395},{\"end\":28413,\"start\":28407},{\"end\":28422,\"start\":28415},{\"end\":28828,\"start\":28815},{\"end\":28854,\"start\":28837},{\"end\":28869,\"start\":28862},{\"end\":28888,\"start\":28878},{\"end\":28903,\"start\":28897},{\"end\":29331,\"start\":29317},{\"end\":29343,\"start\":29333},{\"end\":29351,\"start\":29347},{\"end\":29364,\"start\":29360},{\"end\":29382,\"start\":29376},{\"end\":29389,\"start\":29384},{\"end\":29668,\"start\":29661},{\"end\":29679,\"start\":29670},{\"end\":29691,\"start\":29681},{\"end\":29703,\"start\":29700},{\"end\":29714,\"start\":29705},{\"end\":29739,\"start\":29723},{\"end\":29757,\"start\":29752},{\"end\":29766,\"start\":29759},{\"end\":30004,\"start\":29999},{\"end\":30022,\"start\":30015},{\"end\":30036,\"start\":30031},{\"end\":30048,\"start\":30043},{\"end\":30267,\"start\":30261},{\"end\":30276,\"start\":30271},{\"end\":30283,\"start\":30278},{\"end\":30497,\"start\":30488},{\"end\":30506,\"start\":30502},{\"end\":30520,\"start\":30510},{\"end\":30536,\"start\":30524},{\"end\":30548,\"start\":30538},{\"end\":30807,\"start\":30800},{\"end\":30993,\"start\":30977},{\"end\":31010,\"start\":31001},{\"end\":31040,\"start\":31019},{\"end\":31066,\"start\":31050},{\"end\":31079,\"start\":31073},{\"end\":31100,\"start\":31088},{\"end\":31107,\"start\":31102},{\"end\":31483,\"start\":31477},{\"end\":31497,\"start\":31489},{\"end\":31712,\"start\":31705},{\"end\":31728,\"start\":31722},{\"end\":31856,\"start\":31852}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":538820},\"end\":26474,\"start\":26149},{\"attributes\":{\"id\":\"b1\"},\"end\":27092,\"start\":26476},{\"attributes\":{\"id\":\"b2\"},\"end\":27222,\"start\":27094},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2838031},\"end\":27670,\"start\":27224},{\"attributes\":{\"id\":\"b4\"},\"end\":27993,\"start\":27672},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":207227372},\"end\":28295,\"start\":27995},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2596211},\"end\":28654,\"start\":28297},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":19990980},\"end\":29245,\"start\":28656},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":60027311},\"end\":29610,\"start\":29247},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8174592},\"end\":29992,\"start\":29612},{\"attributes\":{\"id\":\"b10\"},\"end\":30257,\"start\":29994},{\"attributes\":{\"id\":\"b11\"},\"end\":30452,\"start\":30259},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206756462},\"end\":30733,\"start\":30454},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":17486263},\"end\":30966,\"start\":30735},{\"attributes\":{\"id\":\"b14\"},\"end\":31409,\"start\":30968},{\"attributes\":{\"id\":\"b15\"},\"end\":31623,\"start\":31411},{\"attributes\":{\"id\":\"b16\"},\"end\":31843,\"start\":31625},{\"attributes\":{\"id\":\"b17\"},\"end\":32000,\"start\":31845}]", "bib_title": "[{\"end\":26225,\"start\":26149},{\"end\":26522,\"start\":26476},{\"end\":27341,\"start\":27224},{\"end\":28013,\"start\":27995},{\"end\":28379,\"start\":28297},{\"end\":28808,\"start\":28656},{\"end\":29308,\"start\":29247},{\"end\":29657,\"start\":29612},{\"end\":30484,\"start\":30454},{\"end\":30796,\"start\":30735}]", "bib_author": "[{\"end\":26239,\"start\":26227},{\"end\":26251,\"start\":26239},{\"end\":26265,\"start\":26251},{\"end\":26540,\"start\":26524},{\"end\":26558,\"start\":26540},{\"end\":26576,\"start\":26558},{\"end\":26592,\"start\":26576},{\"end\":26608,\"start\":26592},{\"end\":26630,\"start\":26608},{\"end\":26645,\"start\":26630},{\"end\":26665,\"start\":26645},{\"end\":26680,\"start\":26665},{\"end\":27128,\"start\":27121},{\"end\":27136,\"start\":27128},{\"end\":27359,\"start\":27343},{\"end\":27377,\"start\":27359},{\"end\":27386,\"start\":27377},{\"end\":27401,\"start\":27386},{\"end\":27416,\"start\":27401},{\"end\":27731,\"start\":27715},{\"end\":27752,\"start\":27731},{\"end\":27768,\"start\":27752},{\"end\":27781,\"start\":27768},{\"end\":27798,\"start\":27781},{\"end\":27816,\"start\":27798},{\"end\":28031,\"start\":28015},{\"end\":28051,\"start\":28031},{\"end\":28393,\"start\":28381},{\"end\":28400,\"start\":28393},{\"end\":28415,\"start\":28400},{\"end\":28424,\"start\":28415},{\"end\":28830,\"start\":28810},{\"end\":28856,\"start\":28830},{\"end\":28871,\"start\":28856},{\"end\":28890,\"start\":28871},{\"end\":28905,\"start\":28890},{\"end\":29333,\"start\":29310},{\"end\":29345,\"start\":29333},{\"end\":29353,\"start\":29345},{\"end\":29366,\"start\":29353},{\"end\":29384,\"start\":29366},{\"end\":29391,\"start\":29384},{\"end\":29670,\"start\":29659},{\"end\":29681,\"start\":29670},{\"end\":29693,\"start\":29681},{\"end\":29705,\"start\":29693},{\"end\":29716,\"start\":29705},{\"end\":29741,\"start\":29716},{\"end\":29759,\"start\":29741},{\"end\":29768,\"start\":29759},{\"end\":30006,\"start\":29994},{\"end\":30024,\"start\":30006},{\"end\":30038,\"start\":30024},{\"end\":30050,\"start\":30038},{\"end\":30269,\"start\":30259},{\"end\":30278,\"start\":30269},{\"end\":30285,\"start\":30278},{\"end\":30499,\"start\":30486},{\"end\":30508,\"start\":30499},{\"end\":30522,\"start\":30508},{\"end\":30538,\"start\":30522},{\"end\":30550,\"start\":30538},{\"end\":30809,\"start\":30798},{\"end\":30995,\"start\":30970},{\"end\":31012,\"start\":30995},{\"end\":31042,\"start\":31012},{\"end\":31068,\"start\":31042},{\"end\":31081,\"start\":31068},{\"end\":31102,\"start\":31081},{\"end\":31109,\"start\":31102},{\"end\":31485,\"start\":31470},{\"end\":31499,\"start\":31485},{\"end\":31714,\"start\":31699},{\"end\":31730,\"start\":31714},{\"end\":31858,\"start\":31845}]", "bib_venue": "[{\"end\":26305,\"start\":26265},{\"end\":26749,\"start\":26680},{\"end\":27119,\"start\":27094},{\"end\":27431,\"start\":27416},{\"end\":27713,\"start\":27672},{\"end\":28095,\"start\":28051},{\"end\":28457,\"start\":28424},{\"end\":28935,\"start\":28905},{\"end\":29405,\"start\":29391},{\"end\":29782,\"start\":29768},{\"end\":30119,\"start\":30050},{\"end\":30348,\"start\":30285},{\"end\":30586,\"start\":30550},{\"end\":30845,\"start\":30809},{\"end\":31183,\"start\":31109},{\"end\":31468,\"start\":31411},{\"end\":31697,\"start\":31625},{\"end\":31918,\"start\":31858},{\"end\":26805,\"start\":26751},{\"end\":28120,\"start\":28097}]"}}}, "year": 2023, "month": 12, "day": 17}