{"id": 211572839, "updated": "2023-10-06 18:25:55.352", "metadata": {"title": "LEEP: A New Measure to Evaluate Transferability of Learned Representations", "authors": "[{\"first\":\"Cuong\",\"last\":\"Nguyen\",\"middle\":[\"V.\"]},{\"first\":\"Tal\",\"last\":\"Hassner\",\"middle\":[]},{\"first\":\"Cedric\",\"last\":\"Archambeau\",\"middle\":[]},{\"first\":\"Matthias\",\"last\":\"Seeger\",\"middle\":[]}]", "venue": "ICML", "journal": "7294-7305", "publication_date": {"year": 2020, "month": 2, "day": 27}, "abstract": "We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2002.12462", "mag": "3035412324", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/NguyenHSA20", "doi": null}}, "content": {"source": {"pdf_hash": "f7dd4fa80f9b2853ce3873c2b914175d758f57e6", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2002.12462v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "b7b51dfcdc00677caabc7c91f5daa2fbe7846e1d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f7dd4fa80f9b2853ce3873c2b914175d758f57e6.txt", "contents": "\nLEEP: A New Measure to Evaluate Transferability of Learned Representations\n\n\nCuong V Nguyen \nTal Hassner \nC\u00e9dric Archambeau \nMatthias Seeger \nLEEP: A New Measure to Evaluate Transferability of Learned Representations\n\nWe introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.\n\nIntroduction\n\nTransferability estimation (Eaton et al., 2008;Ammar et al., 2014;Sinapov et al., 2015) is the problem of quantitatively estimating how easy it is to transfer knowledge learned from one classification task to another. Specifically, given a source task, represented by a labeled data set or a pre-trained model, and a target task, represented by a labeled data set, transferability estimation aims to develop a measure (or a score) that can tell us, ideally without training on the target task, how effectively transfer learning algorithms can transfer knowledge from the source task to the target task.\n\nAnswering this question is important, since good estimations of transferability can help understand the relationships between tasks (Tran et al., 2019), select groups of highly transferable tasks for joint training (Zamir et al., 2018), or choose good source models for a given target task Bao et al., 2019;Bhattacharjee et al., 2019). Previous approaches to transferability estimation often require running a transfer learning algorithm that involves expensive parameter optimization (Zamir et al., 2018;Achille et al., 2019), do not have a simple interpretation (Bao et al., 2019), or make strong assumptions about the data sets that limit their applicability (Zamir et al., 2018;Tran et al., 2019).\n\nWe propose a novel measure called the Log Expected Empirical Prediction (LEEP) for transferability estimation of deep networks that overcomes all the shortcomings above. In contrast to previous approaches, LEEP scores are obtained without training on the target task, thus avoiding the expensive parameter optimization step. Additionally, they have a simple interpretation and can be applied in general settings to a wide range of modern deep networks.\n\nIn particular, LEEP scores are obtained from a source model and a target data set by making a single forward pass of the model through the target data. This is a simpler process than previous methods, such as Taskonomy (Zamir et al., 2018) and Task2Vec , where one has to re-train at least part of the source model on the target data set. Furthermore, LEEP has a simple interpretation: it is the average log-likelihood of the expected empirical predictor, a simple classifier that makes prediction based on the expected empirical conditional distribution between source and target labels. Finally, LEEP does not make any assumption on the source and target input samples, except that they have the same size. This is more general and applicable than previous work (Zamir et al., 2018;Tran et al., 2019) where source and target data sets were assumed to share the same input samples.\n\nContributions. We formally define LEEP and rigorously analyze it, both theoretically and empirically. We show two theoretical properties of the measure: (1) LEEP is upper bounded by the average log-likelihood of the optimal model, obtained by re-training the head classifier while freezing the feature extractor; (2) LEEP is related to the negative conditional entropy measure proposed by Tran et al. (2019).\n\nWe conduct extensive experiments to evaluate our LEEP measure in several scenarios. We show that the measure is useful for predicting the performance of two commonly used transfer learning algorithms -head classifier re-training Razavian et al., 2014) and model fine-tuning (Agrawal et al., 2014;Girshick et al., arXiv:2002.12462v1 [cs.LG] 27 Feb 2020 2014) -not only for large target data sets, but also for small or imbalanced target data sets that are difficult to use for re-training. We also show that LEEP can predict the convergence speed of the fine-tuning method for transfer learning.\n\nWe further demonstrate that LEEP can predict the performance of a recently developed meta-transfer learning method, the Conditional Neural Adaptive Processes (Requeima et al., 2019). Meta-transfer learning (Wei et al., 2018;Sun et al., 2019;Requeima et al., 2019) is a framework for learning to transfer using several meta-training tasks. Importantly, to our knowledge, our work is the first to develop a transferability measure for meta-transfer learning.\n\nWe empirically compare our method with the very recent negative conditional entropy measure (Tran et al., 2019) and H scores (Bao et al., 2019). Our comparisons show that LEEP better correlates with the actual transfer accuracy than these methods. Finally, we demonstrate the effectiveness of LEEP for the source model selection problem in comparison with the negative conditional entropy and H scores.\n\n\nLog Expected Empirical Prediction\n\nConsider transfer learning between two classification tasks: a source task, represented by a pre-trained model, and a target task, represented by a labeled data set. Formally, assume the source task requires learning a model that maps input instances from a domain X = R N to labels in a label set Z. Further, assume that we already trained such a model, which we call the source model, denoted by \u03b8. We note that X can contain text or images since they can be flatten to vectors in R N . Transfer learning seeks to learn a model for a target task mapping inputs from X to some target label set Y, given a target data set D = {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x n , y n )}, where x i \u2208 X and y i \u2208 Y, for this purpose. We emphasize that, unlike recent previous work (Zamir et al., 2018;Tran et al., 2019), the domain X in our setting is general: the input instances of the source and target tasks are only assumed to have the same dimension, such as ImageNet (Russakovsky et al., 2015) and CIFAR (Krizhevsky, 2009) images scaled to the same size.\n\nThe transfer learning setting above is very common in computer vision. For instance, a popular transfer learning method takes a model, pre-trained on ImageNet, and re-trains its classifier (the head) on the target data set while freezing the feature extractor. The result is a new model for the target task that uses the representation learned on the source with a head classifier learned on the target Razavian et al., 2014;Zeiler & Fergus, 2014;Oquab et al., 2014;Whatmough et al., 2019). We call this the head re-training method. Another popular transfer learning method, called the fine-tuning method, fine-tunes the feature extractor while re-training the new head classifier on the target data set to get a model for the target task (Agrawal et al., 2014;Girshick et al., 2014;Chatfield et al., 2014;Dhillon et al., 2020).\n\nIn this work, we study the transferability estimation problem, which aims to develop a measure (or a score) that can tell us, without training on the target data set, how effectively these transfer learning algorithms can transfer knowledge learned in the source model \u03b8 to the target task, using the target data set D.\n\nWe now describe our proposed measure, LEEP, that requires no expensive training on the target task and offers an a priori estimate of how well a model will transfer to the target task. The measure can be efficiently computed from the source model \u03b8 and the target data set D. The computation involves the following three steps.\n\nStep 1: Compute dummy label distributions of the inputs in the target data set D. We apply \u03b8 to each input x i in D to get the predicted distribution over Z, the label set of the source task. We denote this predicted distribution by \u03b8(x i ), which is a categorical distribution over Z. Note that \u03b8(x i ) is a dummy distribution over labels of the source task, since these labels may not be meaningful for the example x i . For instance, if \u03b8 is a model pre-trained on ImageNet and D is the CIFAR data set, then \u03b8(x i ) is a distribution over ImageNet labels, which may not be semantically related to the true label of x i in the CIFAR data set.\n\nStep 2: Compute the empirical conditional distribution P (y|z) of the target label y given the source label z. We next compute the empirical conditional distributionP (y|z) for all (y, z) \u2208 Y \u00d7 Z. To this end, we first compute the empirical joint distributionP (y, z) for all (y, z) \u2208 Y \u00d7 Z:\nP (y, z) = 1 n i:yi=y \u03b8(x i ) z ,(1)\nwhere the summation is over all indices i \u2208 {1, 2, . . . , n} such that y i = y, and \u03b8(x i ) z is the probability of the label z according to the categorical distribution \u03b8(x i ).\n\nFrom this empirical joint distribution, we can compute the empirical marginal distributionP (z) and then the empirical conditional distributionP (y|z):\nP (z) = y\u2208YP (y, z) = 1 n n i=1 \u03b8(x i ) z ,P (y|z) =P (y, z) P (z) .\nStep 3: Compute LEEP using \u03b8(x) andP (y|z). For any input x \u2208 X , consider a classifier that predicts a label y of x by first randomly drawing a dummy label z from \u03b8(x) and then randomly drawing y fromP (y|z). Equivalently, this classifier can predict y by directly drawing a label from the distribution p(y|x; \u03b8, D) = z\u2208ZP (y|z) \u03b8(x) z . We call this classifier the Expected Empirical Predictor (EEP).\n\nFor the target data set D, we define LEEP as the average loglikelihood of the EEP classifier given the data D. Formally, our LEEP measure of transferability is defined as:\nT (\u03b8, D) = 1 n n i=1 log z\u2208ZP (y i |z) \u03b8(x i ) z . (2)\nIntuitively, this measure tells us how well the EEP performs on D. We argue that since the EEP is constructed mainly from the source model \u03b8 with a minimal use of D (i.e., D is only used to compute the simple empirical conditional distribution), it can serve as an indicator of how \"close\" \u03b8 and D are, and hence a measure of transferability. This measure is also efficient to compute since its computational bottleneck (step 1 above) requires only a single forward pass through the target data set D.\n\n\nTheoretical Properties of LEEP\n\nAssume \u03b8 = (w, h) where w is a feature extractor that maps an input x \u2208 X to a representation (or embedding), r = w(x), and h is a classifier (or head) that takes the representation r as input and returns a probability distribution over Z. Next, assume we fix w and re-train the classifier by maximum likelihood, using the target data set D, to obtain the new classifier k * . That is,\nk * = arg max k\u2208K l(w, k),(3)\nwhere l(w, k) = 1 n n i=1 log p(y i |x i ; w, k) is the average log-likelihood of (w, k) on the target data set D, and K is the space of classifiers from which we would like to select k. We note that K contains classifiers that map w(x) to labels in Y. If we assume K contains the EEP, we can easily show the following property of the LEEP measure (see proof in Appendix A):\n\nProperty 1. LEEP is a lower bound of the optimal average log-likelihood. Formally, T (\u03b8, D) \u2264 l(w, k * ).\n\nThe assumption that K contains the EEP can be easily satisfied if we select K =K \u222a {k EEP }, where the classifier k EEP is the EEP andK is a space of classifiers that we can easily optimize over (e.g., the parameter space of a linear classifier). We can then solve Eq. (3) by the following twostage process. First, we solvek = arg max k\u2208K l(w, k) using, for instance, stochastic gradient descent (SGD) (Robbins & Monro, 1951;Bottou, 1991). Then, we solve k * = arg max k\u2208{k,kEEP} l(w, k) by simple comparison. In our experiments, we skip this second step and choose k * =k, as usually done in practice. The results reported in Sec. 5.1 show that LEEP correlates well with the test accuracy ofk, indicating that the measure can be used to estimate the performance of the transferred model in practice.\n\nAs a second property, we show a relationship between LEEP and the negative conditional entropy (NCE) measure recently proposed by Tran et al. (2019). To measure the transferability between \u03b8 and D, we can compute the NCE measure as follows. First, we use \u03b8 to label all x i 's in D. For example, we can label x i by a dummy label z i = arg max z\u2208Z \u03b8(x i ) z . Then, using the dummy label set Z = (z 1 , z 2 , . . . , z n ) and the true label set Y = (y 1 , y 2 , . . . , y n ), we can compute NCE(Y |Z). It can be shown that the following property relating LEEP and NCE holds (see proof in Appendix A): Property 2. LEEP is an upper bound of the NCE measure plus the average log-likelihood of the dummy labels.\nFormally, T (\u03b8, D) \u2265 NCE(Y |Z) + 1 n n i=1 log \u03b8(x i ) zi .\nFrom Properties 1 and 2, we know that LEEP is bounded between NCE(Y |Z) + 1 n i log \u03b8(x i ) zi and the average log-likelihood of the re-trained model. When the re-trained model does not overfit, its average log-likelihood is a reasonable indicator of the model's performance (Tran et al., 2019). When LEEP is close to this average log-likelihood, it can be considered, in some sense, a reasonable indicator of transferability. Our experiments in Sec. 5.6 show the effectiveness of LEEP over NCE.\n\nWe note that in the setting of Tran et al. (2019), the source data set is given, instead of the source model \u03b8. Hence, NCE is a more natural measure of transferability in their case. In contrast, LEEP is more natural for our setting, since we are only given the source model \u03b8. In return, by assuming a source model, LEEP is not restricted to their setting, where the two tasks are defined over the exact same input instances.\n\n\nRelated Work\n\nOur work is related to several research areas in machine learning and computer vision, including transfer learning (Weiss et al., 2016;Yang et al., 2020), meta-transfer learning (Wei et al., 2018;Sun et al., 2019;Requeima et al., 2019), task space modeling (Zamir et al., 2018;Achille et al., 2019), and domain adaptation Azizzadenesheli et al., 2019). We discuss below previous work that is closely related to ours.\n\nTransfer learning. Our paper addresses the problem of predicting the performance of transfer learning algorithms between two classification tasks, without actually executing these algorithms. This problem is also called transferability estimation between classification tasks (Bao et al., 2019;Tran et al., 2019). Early theoretical work in transfer and multi-task learning studied the relatedness between tasks and proposed several types of distances between tasks. These distances include the F-relatedness (Ben- David & Schuller, 2003), A-distance (Kifer et al., 2004;Ben-David et al., 2007), and discrepancy distance (Mansour et al., 2009). Although useful for theoretical analysis, these approaches are unsuited for measuring transferability in practice because they cannot be computed easily and are symmetric. Transferability measures should be non-symmetric since transferring from one task to another (e.g., from a hard task to an easy one) is different from transferring in the reverse direction (e.g., from the easy task to the hard one).\n\nMore recently, Azizzadenesheli et al. (2019) studied domain adaptation under label shift. The authors considered the case where only the marginal label distribution changes between domains. This assumption is more restrictive than the setting in our paper as we allow both the input distribution and the label distribution to change arbitrarily.\n\nThe most related work to ours are transferability measures recently proposed by Tran et al. (2019) and Bao et al. (2019). Tran et al. (2019) developed the negative conditional entropy measure between the source and target label sets, under the assumption that the source and target data sets share the same input examples. Our paper removes this requirement and allows the input data to come from arbitrarily different distributions. Bao et al. (2019) developed a transferability measure based on H scores, which are derived from information-theoretic principles. Although, like us, their transferability measure can be applied to general settings, their measure is hard to interpret since it involves solving a Hirschfeld-Gebelein-R\u00e9nyi maximum correlation problem (Hirschfeld, 1935;Gebelein, 1941;R\u00e9nyi, 1959). LEEP scores, on the other hand, have a simple interpretation related to the EEP and are easy to implement and compute.\n\nMeta-transfer learning. Meta-transfer learning is a framework for learning to transfer from a source task to a target task (Wei et al., 2018;Sun et al., 2019;Requeima et al., 2019). Similar to our transfer learning setting, Sun et al. (2019) and Requeima et al. (2019) also adapted a pre-trained model on the source task to the target task. These metalearning methods learn the adaptation from several metatraining tasks, which consist of additional target data sets and target test sets from different domains that are intended to mimic the transfer learning scenario, where one wants to transfer knowledge to unseen tasks. Because of the additional data sets, the transfer learning mechanism in metalearning departs from that of regular transfer learning. Nevertheless, we show that LEEP scores can also predict the performance of meta-transfer learning algorithms, such as the conditional neural adaptive processes (CNAPs) recently proposed by Requeima et al. (2019). To our knowledge, we are the first to develop a transferability measure that can be applied to meta-transfer learning.\n\nTask space representation. Our paper is related to task space representation (Edwards & Storkey, 2017;Zamir et al., 2018;Achille et al., 2019;Jomaa et al., 2019) in the sense that transferability may be estimated from a distance between the representations (or embeddings) of tasks. For instance, Task2Vec  tried to map tasks (or data sets) to vectors in a vector space. Transferability between tasks was then estimated using a non-symmetric distance between the corresponding vectors. This method requires training a large reference network (called the probe network), adapting it to the target data set, and computing the Fisher information matrix to obtain a task embedding. Our method, on the other hand, is much simpler and computationally more efficient. Additionally, we require only the source model and a small target data set, which are both usually available in practice.\n\nEdwards & Storkey (2017) extended the variational autoencoder (Kingma & Welling, 2014) to compute statistics of data sets that are useful in a range of applications, including clustering data sets or classifying unseen classes. These statistics, however, were not shown to be useful for transferability estimation. Another related work, Taskonomy (Zamir et al., 2018), created a taxonomy of tasks that revealed task structures useful for reducing the number of labeled training data. Taskonomy involves several steps, one of which requires computing the task affinity matrix by re-training networks on target data sets. LEEP scores can be regarded as an efficient approximation of the task affinity, as we avoid network re-training.\n\n\nExperiments\n\nWe evaluate the ability of LEEP to predict the performance of transfer and meta-transfer learning algorithms, prior to applying these algorithms in practice. We further show that LEEP is useful even in the small or imbalanced data settings, where training on the target task could be hard. We compare LEEP with the state of the art NCE transferability measure of Tran et al. (2019) and H score of Bao et al. (2019). Finally, we demonstrate the use of LEEP for source model selection. Our experiments are implemented in Gluon/MXNet (Chen et al., 2015;Guo et al., 2019).\n\n\nLEEP vs. Transfer Accuracy\n\nWe show that LEEP scores effectively predict the accuracies of models transferred from source to target tasks. We consider two different source models, each one representing a different source task: ResNet18 (He et al., 2016), which is pre-trained on ImageNet (Russakovsky et al., 2015), and ResNet20 (He et al., 2016), which is pre-trained on CIFAR10 (Krizhevsky, 2009). For each model, we construct 200 different target tasks from the CIFAR100 data set (Krizhevsky, 2009) as follows. The label set of each target task is constructed by randomly drawing a subset of the 100 classes, with the subset's size ranging from 2 to 100. The target data set then consists of all training examples of the selected classes in the CIFAR100 data set. We use the test examples of these selected classes as the target test set to compute the accuracy of the transferred model.\n\nWe experiment with two commonly used transfer learning methods to train a transferred model to a target task:\n\nRe-train head. This method keeps the feature extractor layers of the source model fixed and then trains a new head classifier using the target data set from scratch. We re-train the new classifier by running SGD on the cross entropy loss.\n\nFine-tune. This method replaces the head classifier of the source model with a new head and then fine-tunes the entire model -the feature extractor and the new head -using the target data set. Fine-tuning is performed again by running SGD on the cross entropy loss.\n\nTo clarify, we let the feature extractor be the portion of the source model's network up to and including the penultimate layer. The head classifier is the network' last fully connected layer. For each target task, models transferred using these two methods were evaluated on the target test set to obtain the test accuracies. We then compare these accuracies with our LEEP scores evaluated on these target tasks.\n\nIn all tests, we ran SGD for 100 epochs with learning rate 0.01 and batch size 10 since they were sufficient to obtain good transferred models. We found that varying the number of epochs does not significantly change the results of our experiments, although in principle, fine-tuning the whole network until convergence could decouple its dependence on the source task.  Table 1 (see the first two rows of each transfer method). From Fig. 1 and Table 1, LEEP scores clearly correlate with test accuracies, with correlation coefficients higher than 0.94 and p < 0.001 in all cases. Evidently, LEEP scores are a reliable indicator of the performance of transferred models.\n\n\nLEEP vs. Transfer Accuracy in Small Data Regime\n\nTransfer learning is often performed in cases where target data sets are small. We next evaluate LEEP in such small data settings. We repeat the experiments of Sec. 5.1 with the additional restriction that target data sets contain exactly five random classes and 50 random examples per class. Thus, the target data sets contain only 10% of the training examples per class, compared to the full CIFAR100 data set.\n\nWe further add experiments where target data sets are constructed from the FashionMNIST data set (Xiao et al., 2017). To factor out the noise when evaluating LEEP scores on small target data sets, we also consider partitioning the scores' range into five equal bins and averaging the test accuracies of tasks in each bin. This allows us to compare the average test accuracies of tasks belonging in five transferability levels. Fig. 2(a) shows a typical result for this case, where higher transferability levels generally imply better accuracies for both transfer methods. Full results are given in Fig. 6 in Appendix B. These results testify that LEEP scores can predict accuracies of transferred models, even in small data regimes.\n\n\nLEEP vs. F1 Score on Imbalanced Data\n\nImbalanced target data sets are commonly encountered in practice (Al-Stouhi & Reddy, 2016; Wang et al., 2017). We now evaluate LEEP in this setting. Specifically, we also repeat the experiments of Sec. 5.1 and 5.2 where, this time, we restrict target data sets to imbalanced binary classification sets. The two classes of each target data set are drawn . Performance of transferred models on small target data sets in five transferability levels predicted from LEEP scores. The higher the level, the easier the transfer. The transfer is from a ResNet18 pre-trained on ImageNet to target tasks constructed from CIFAR100. We considered both (a) balanced and (b) imbalanced target data sets. See Sec. 5.2 and 5.3 for details.\n\nrandomly. The size of the smaller class is chosen uniformly from 30 to 60, while the other class is five times larger. Because the target data sets are small with only two classes, we only need to re-train the head or fine-tune for 20 epochs.\n\nTable 1 (see the last four rows of the first two algorithms) shows the correlation coefficients between LEEP scores and test F1 scores in these settings. The results also indicate a reasonably positive correlations with coefficients greater than 0.5 in all cases. Fig. 2(b) further shows the average test F1 scores in five transferability levels predicted from LEEP for a typical case, where higher transferability levels imply better F1 scores for both transfer methods. Full results are given in Fig. 7 in Appendix B. These results again confirm that LEEP scores can predict the performance of transferred models, even for imbalanced target data sets.\n\n\nLEEP vs. Accuracy of Meta-Transferred Models\n\nMeta-transfer learning is a framework for learning to adapt from a source task to a target task (Wei et al., 2018;Sun et al., 2019;Requeima et al., 2019). Next, we show that LEEP can also predict the test accuracies of CNAPs (Requeima et al., 2019), a recently proposed meta-transfer learning method. CNAPs adapt a pre-trained model on the source task to a target task by adding scale and shift parameters to each channel of its convolutional layers. The additional parameters are outputs of adaptation networks, which are trained by meta-learning from several training tasks. When given a target data set, the adaptation networks return the appropriate scale and shift parameters that can augment the source model to make predictions on the target test set.\n\nIn our experiment, we follow the original training procedure proposed for CNAPs by Requeima et al. (2019), where the source model is a ResNet18 pre-trained on ImageNet, and the adaptation networks are trained using the Meta-data set (Triantafillou et al., 2020). We also test CNAPs on 200 random target tasks drawn from CIFAR100 as follows. Each target data set contains five random labels and 50 random examples per class, drawn from the test set of CIFAR100. The remaining 50 test examples of the selected classes are used as the target test set. This testing procedure is consistent with the one used by Requeima et al. (2019), except that in our experiments, we fix the number of classes.\n\nThe last row of Table 1 shows the correlation coefficient between LEEP scores and test accuracies of CNAPs. The coefficient is 0.591 with p < 0.001, indicating that LEEP scores are a good measure of meta-transferability. Similar to Sec. 5.2 and 5.3, Fig. 3 provides the average test accuracies of tasks in five LEEP score transferability levels. Fig. 3 clearly shows that higher transferability levels correspond to better test accuracies for CNAPs.\n\n\nLEEP vs. Convergence of Fine-tuned Models\n\nFor each target task, let us consider a reference model: one that is trained from scratch using only the target data set. When it is easier to transfer from a source task to the target task, we often expect the fine-tuned models to converge more quickly and even exceed the performance of this reference model. The experiments in this section show that LEEP scores indeed predict this behavior.\n\nWe use the same small data settings defined in Sec. 5.2 and train a reference model for each target task. Reference models are trained using SGD with 100 epochs for target tasks from CIFAR100 and with 40 epochs for those from FashionMNIST. When fine-tuning a model, we track the difference between the fine-tuned model's test accuracy and the final test accuracy of the corresponding reference model (i.e., we always compare against the fully trained reference model). Similar to Sec. 5.2, we also consider different transferability levels according to LEEP scores, and average the accuracy differences of all tasks within the same transferability level.  Figure 4. Convergence of accuracy of fine-tuned models to the accuracy of reference models trained from scratch using only the target data set. The convergence is represented by the accuracy difference between the fine-tuned and the reference models. Each curve is the average over tasks within the same transferability level. The zero lines indicate where the fine-tuned models match the accuracy of the reference models. See Sec. 5.5 for details.\n\ntasks are similar and given in Fig. 8 in Appendix B. From Fig. 4, on average, fine-tuned models on tasks in higher transferability levels have better convergence speeds (i.e., their curves reach zero faster). Furthermore, these models can outperform the reference models by larger margins (i.e., their curves reach higher values). In all cases, the fine-tuned models match the performance of the reference models using far fewer training epochs. These results confirm the advantage of transfer learning in small data settings, especially between highly transferable tasks, which in turn, can be efficiently predicted using our LEEP scores.\n\n\nComparison of LEEP, NCE, and H scores\n\nWe compare the LEEP measure with the NCE measure proposed by Tran et al. (2019) and the H score proposed by Bao et al. (2019). Particularly, for all the experimental settings in Sec. 5.1, 5.2, 5.3, and 5.4 above, we compute the NCE score for each transfer from a source model to a target task using the method described in Sec. 3. The computation is efficient since it also requires a single forward pass through the target data set to get the dummy labels. After obtaining the NCE scores, we evaluate their Pearson correlation coefficients and p values with test accuracies (or F1 for imbalanced data) of the three transfer (or meta-transfer) learning algorithms. Similarly, we also compute the correlation coefficients and p values of the H scores between the features returned by the source model and the target labels. The features are obtained by applying the source model to the target inputs. Table 1 shows the correlation coefficients of NCE and H scores in comparison with those of LEEP scores. When compared with NCE, LEEP scores have equal or better correlations with transfer performance in all except for two cases of the fine-tuning method (see the second and third rows of this method in Table 1). Even in these two cases, LEEP scores are only slightly worse than NCE scores. These compar-isons confirm that LEEP scores are better than NCE scores for our transfer settings, with up to 30% improvement in terms of correlation coefficients.\n\nWhen compared with H scores, LEEP scores give better correlations in 14/21 cases. We note that the performance of H scores is not consistent in all experiments. Particularly, it completely fails to capture the transferability in 9 cases (those marked with asterisks). Even when it successfully captures transferability, it performs worse than LEEP on large target data sets. By comparison, our LEEP scores capture the transferability in all cases.\n\n\nLEEP for Source Model Selection\n\nWe evaluate LEEP for the source model selection problem, where we need to select the best source model from 9 candidate models and transfer it to CIFAR100. The candidate models are pre-trained on ImageNet and include ResNet18, ResNet34, ResNet50 (He et al., 2016), MobileNet1.0, MobileNet0.75, MobileNet0.5, MobileNet0.25 (Howard et al., 2017), DarkNet53 (Redmon & Farhadi, 2018), and SENet154 (Hu et al., 2018). Our target data set is the full CIFAR100 training set, while the target test set is the full CIFAR100 test set. We compare our LEEP measure to the NCE (Tran et al., 2019) and H score (Bao et al., 2019) baselines. We also consider ImageNet top-1 accuracy as an additional baseline since previous work (Kornblith et al., 2019) has shown that ImageNet accuracy can predict the performance of transferred models. Fig. 5 shows the results of this experiment for the head retraining method. From the figure, LEEP scores can predict well the test accuracy of models whose head classifiers are re-trained. In comparison, the other baselines all perform worse than LEEP. For example, NCE fails to predict the performance of MobileNet1.0, while H score and ImageNet accuracy fail on the SENet154.\n\nWe also give results for the fine-tuning method in Fig. 9 in Appendix B. Generally, all the considered measures do not predict well the test accuracy of fine-tuned models, especially for ResNet18 and ResNet34 source models. One possible explanation is that the performance of fine-tuned models is sensitive to the architecture and the size of the source networks. However, the transferability measures considered in this section do not take these factors into account.\n\n\nDiscussions\n\nWe proposed LEEP, a novel transferability measure that can efficiently estimate the performance of transfer and metatransfer learning algorithms before actually executing them. We show both theoretically and empirically that LEEP is an effective transferability measure that is useful in several scenarios. Below are more discussions about our work.  (Tran et al., 2019), and H scores (Bao et al., 2019). Correlations are computed with respect to test accuracies (or F1) of three (meta)-transfer learning algorithms in various experimental settings. Correlations marked with asterisks (*) are not statistically significant (p > 0.05), while the rest are statistically significant with p < 0.001. Source model assumption. Our work assumes a source model pre-trained on the source task. If, instead, we are given the source data set, we can first train a source model from the data and then use this model to compute the LEEP scores. The LEEP scores would depend on the architectural choice and thus the performance of the source model. This is expected and has been pointed out in previous work (Kornblith et al., 2019;Tran et al., 2019).\n\n\nAlgorithm\n\n\nApplications of LEEP.\n\nOur experiments, reported in Sec. 5, showed that LEEP is applicable in diverse scenarios. In general, LEEP scores can be used to efficiently select highly transferable pairs of source and target tasks, yielding high transfer accuracy and good convergence speeds. This ability can support source model selection in transfer/meta-transfer learning scenarios, where we need to select a source model among several others in order to optimize for best performance on a target task.\n\nAside from transfer and meta-transfer learning, our LEEP scores are potentially useful for continual learning (Zenke et al., 2017;Nguyen et al., 2019), multi-task learning (Misra et al., 2016;Standley et al., 2019), and feature selection (Yosinski et al., 2014). For instance, LEEP scores can be used to estimate the hardness of task sequences, thereby helping to analyze properties of continual learning algorithms . For multi-task learning, LEEP scores can be used for selecting groups of tasks for joint training (Standley et al., 2019). Finally, LEEP scores could be useful for hyperparameter transfer learning and Bayesian optimization (Perrone et al., 2018). These are promising research directions that we leave to future work.   . Average test F1 score of transferred models on small, imbalanced target data sets in five transferability levels obtained from LEEP scores. The higher the level, the easier the transfer. A \u2192 B in the subcaptions indicate that the source model is trained on A and the target datasets are constructed from B. The source models are ResNet18 for ImageNet (a,c) and ResNet20 for CIFAR10 (b,d).\n\n\nA. Proofs\n\n\nA.1. Proof of Property 1\n\nThis proof is straight-forward because l(w, k * ) is the maximal average log-likelihood over k \u2208 K, T (\u03b8, D) is the average log-likelihood of the EEP, and the EEP is in K. Thus, T (\u03b8, D) \u2264 l(w, k * ).\n\n\nA.2. Proof of Property 2\n\nLet Z = (z 1 , z 2 , . . . , z n ) be the dummy labels of (x 1 , x 2 , . . . , x n ) obtained when computing the NCE, and let Y = (y 1 , y 2 , . . . , y n ) be the true label set. We have: \n\n\nB. Full Experimental Results\n\nFig . 6 shows the results for all experimental settings with small balanced target data sets. Fig. 7 shows the results for all experimental settings with small imbalanced target data sets. Fig. 8 shows the results for all experimental settings with the convergence speed of fine-tuned models. For a clearer comparison, we only consider two LEEP transferability levels for target tasks constructed from FashionMNIST.  Figure 8. Convergence of accuracy for fine-tuned models to the accuracy of a reference model trained from scratch using only the target dataset. The convergence is represented by the accuracy difference between the fine-tune model and the reference model. Each curve is the average of the accuracy difference curves over tasks within the same transferability level. The zero lines indicate where the fine-tuned models match the accuracy of the reference model. . Test accuracy vs. transferability according to LEEP score, NCE score (Tran et al., 2019), H score (Bao et al., 2019), and ImageNet accuracy (Kornblith et al., 2019) for 9 candidate source models (see the legend) pre-trained on ImageNet. The transferred models are obtained by (a) re-training the head classifier, and (b) fine-tuning the source model.\n\nFig. 1\n1shows the LEEP scores and the corresponding test accuracies for transferred models on 200 target tasks. Following the correlation analysis by Nguyen et al. (2019) and Tran et al. (2019), we compute the Pearson correlation coefficients and the p values between LEEP scores and test accuracies, which are shown in\n\nFigure 2\n2Figure 2. Performance of transferred models on small target data sets in five transferability levels predicted from LEEP scores. The higher the level, the easier the transfer. The transfer is from a ResNet18 pre-trained on ImageNet to target tasks constructed from CIFAR100. We considered both (a) balanced and (b) imbalanced target data sets. See Sec. 5.2 and 5.3 for details.\n\nFigure 3 .\n3Average test accuracy of CNAPs on tasks in five transferability levels predicted from LEEP scores. The higher the level, the easier the transfer. See Sec. 5.4 for details.\n\nFig. 4\n4plots the average accuracy difference curves of five different transferability levels when transferring to CI-FAR100 target tasks. Results for FashionMNIST target\n\nFigure 7\n7Figure 7. Average test F1 score of transferred models on small, imbalanced target data sets in five transferability levels obtained from LEEP scores. The higher the level, the easier the transfer. A \u2192 B in the subcaptions indicate that the source model is trained on A and the target datasets are constructed from B. The source models are ResNet18 for ImageNet (a,c) and ResNet20 for CIFAR10 (b,d).\n\n\nP (y i |z i ) \u03b8(x i ) zi (monotonicity of log) (y i |z i ) + 1 n n i=1 log \u03b8(x i ) zi .According to the proof of Theorem 1 of Tran et al. \u03b8(x i ) zi .\n\nFig. 9\n9shows the results for all experimental settings in the source model selection problem.\n\nFigure 9\n9Figure 9. Test accuracy vs. transferability according to LEEP score, NCE score (Tran et al., 2019), H score (Bao et al., 2019), and ImageNet accuracy (Kornblith et al., 2019) for 9 candidate source models (see the legend) pre-trained on ImageNet. The transferred models are obtained by (a) re-training the head classifier, and (b) fine-tuning the source model.\n\nTable 1 .\n1Comparison of Pearson correlation coefficients of LEEP, NCE\n\n\nFigure 5. Test accuracy vs. transferability predicted from LEEP, NCE(Tran et al., 2019), H score(Bao et al., 2019), and ImageNet top-1 accuracy(Kornblith et al., 2019) for 9 candidate source models (see the legend) pre-trained on ImageNet. The transferred models are obtained by re-training the head classifier. See Sec. 5.7 for details.Experiment Setting \nLEEP score NCE score H score \n\nSource \nTarget \nSource model Properties of target data set Details in \n\nCIFAR10 \nCIFAR100 \nResNet20 \nlarge, balanced \nSec. 5.1 \n0.982 \n0.982 \n0.831 \nImageNet \nCIFAR100 \nResNet18 \nlarge, balanced \nSec. 5.1 \n0.974 \n0.973 \n0.924 \nCIFAR10 \nCIFAR100 \nResNet20 \nsmall, balanced \nSec. 5.2 \n0.744 \n0.743 \n0.877 \nImageNet \nCIFAR100 \nResNet18 \nsmall, balanced \nSec. 5.2 \n0.798 \n0.715 \n0.026  *  \nRe-train \nCIFAR10 FashionMNIST \nResNet20 \nsmall, balanced \nSec. 5.2 \n0.518 \n0.429 \n0.787 \nhead \nImageNet FashionMNIST \nResNet18 \nsmall, balanced \nSec. 5.2 \n0.631 \n0.622 \n0.005  *  \nCIFAR10 \nCIFAR100 \nResNet20 \nsmall, imbalanced \nSec. 5.3 \n0.862 \n0.847 \n0.787 \nImageNet \nCIFAR100 \nResNet18 \nsmall, imbalanced \nSec. 5.3 \n0.522 \n0.484 \n-0.058  *  \nCIFAR10 FashionMNIST \nResNet20 \nsmall, imbalanced \nSec. 5.3 \n0.704 \n0.688 \n0.822 \nImageNet FashionMNIST \nResNet18 \nsmall, imbalanced \nSec. 5.3 \n0.645 \n0.624 \n0.059  *  \n\nCIFAR10 \nCIFAR100 \nResNet20 \nlarge, balanced \nSec. 5.1 \n0.967 \n0.967 \n0.787 \nImageNet \nCIFAR100 \nResNet18 \nlarge, balanced \nSec. 5.1 \n0.944 \n0.945 \n0.875 \nCIFAR10 \nCIFAR100 \nResNet20 \nsmall, balanced \nSec. 5.2 \n0.396 \n0.401 \n0.737 \nImageNet \nCIFAR100 \nResNet18 \nsmall, balanced \nSec. 5.2 \n0.762 \n0.584 \n-0.029  *  \nFine-tune \nCIFAR10 FashionMNIST \nResNet20 \nsmall, balanced \nSec. 5.2 \n0.339 \n0.258 \n0.826 \nImageNet FashionMNIST \nResNet18 \nsmall, balanced \nSec. 5.2 \n0.609 \n0.578 \n0.018  *  \nCIFAR10 \nCIFAR100 \nResNet20 \nsmall, imbalanced \nSec. 5.3 \n0.597 \n0.582 \n0.758 \nImageNet \nCIFAR100 \nResNet18 \nsmall, imbalanced \nSec. 5.3 \n0.565 \n0.503 \n-0.069  *  \nCIFAR10 FashionMNIST \nResNet20 \nsmall, imbalanced \nSec. 5.3 \n0.603 \n0.589 \n0.904 \nImageNet FashionMNIST \nResNet18 \nsmall, imbalanced \nSec. 5.3 \n0.507 \n0.425 \n0.056  *  \n\nCNAPS \nImageNet \nCIFAR100 \nResNet18 \nsmall, balanced \nSec. 5.4 \n0.591 \n0.310 \n0.025  *  \n\n4.5 \n4.0 \n3.5 \nLEEP score \n\n0.0 \n\n0.1 \n\n0.2 \n\n0.3 \n\nTest accuracy \n\nResNet18 \nResNet34 \nResNet50 \nMobileNet1.0 \nMobileNet0.75 \nMobileNet0.5 \nMobileNet0.25 \nDarkNet53 \nSENet154 \n\n4.0 \n3.7 \n3.4 \nNCE score \n4 \n11 \n18 \nH score \n0.5 0.6 0.7 0.8 \nImageNet accuracy \n\n\n\n\nFigure 6. Average test accuracy of transferred models on small, balanced target data sets in five transferability levels obtained from LEEP scores. The higher the level, the easier the transfer. A \u2192 B in the subcaptions indicate that the source model is trained on A and the target datasets are constructed from B. The source models are ResNet18 for ImageNet (a,c) and ResNet20 for CIFAR10 (b,d).1 \n2 \n3 \n4 \n5 \nTransferability level \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\nAverage test accuracy \nfine-tune \nretrain head \n\n(a) ImageNet \u2192 CIFAR100 \n\n1 \n2 \n3 \n4 \n5 \nTransferability level \n\n0.4 \n\n0.6 \n\n0.8 \n\n1.0 \n\nAverage test accuracy \nfine-tune \nretrain head \n\n(b) CIFAR10 \u2192 CIFAR100 \n\n1 \n2 \n3 \n4 \n5 \nTransferability level \n\n0.4 \n\n0.6 \n\n0.8 \n\n1.0 \n\nAverage test accuracy \nfine-tune \nretrain head \n\n(c) ImageNet \u2192 FashionMNIST \n\n1 \n2 \n3 \n4 \n5 \nTransferability level \n\n0.5 \n\n0.7 \n\n0.9 \n\nAverage test accuracy \nfine-tune \nretrain head \n\n(d) CIFAR10 \u2192 FashionMNIST \n\n\nAmazon Web Services 2 Facebook AI (work done at Amazon). Correspondence to: Cuong V. Nguyen <nguycuo@amazon.com>.\n\nTask2vec: Task embedding for meta-learning. A Achille, M Lam, R Tewari, A Ravichandran, S Maji, C C Fowlkes, S Soatto, P Perona, International Conference on Computer Vision. Achille, A., Lam, M., Tewari, R., Ravichandran, A., Maji, S., Fowlkes, C. C., Soatto, S., and Perona, P. Task2vec: Task embedding for meta-learning. In International Con- ference on Computer Vision, pp. 6430-6439, 2019.\n\nAnalyzing the performance of multilayer neural networks for object recognition. P Agrawal, R Girshick, Malik , J , European Conference on Computer Vision. Agrawal, P., Girshick, R., and Malik, J. Analyzing the performance of multilayer neural networks for object recognition. In European Conference on Computer Vision, pp. 329-344, 2014.\n\nTransfer learning for class imbalance problems with inadequate data. Knowledge and information systems. S Al-Stouhi, C K Reddy, 48Al-Stouhi, S. and Reddy, C. K. Transfer learning for class imbalance problems with inadequate data. Knowledge and information systems, 48(1):201-228, 2016.\n\nAn automated measure of MDP similarity for transfer in reinforcement learning. H B Ammar, E Eaton, M E Taylor, D C Mocanu, K Driessens, G Weiss, K Tuyls, AAAI Conference on Artificial Intelligence Workshops. Ammar, H. B., Eaton, E., Taylor, M. E., Mocanu, D. C., Driessens, K., Weiss, G., and Tuyls, K. An automated measure of MDP similarity for transfer in reinforcement learning. In AAAI Conference on Artificial Intelligence Workshops, 2014.\n\nRegularized learning for domain adaptation under label shifts. K Azizzadenesheli, A Liu, F Yang, Anandkumar , A , International Conference on Learning Representations. Azizzadenesheli, K., Liu, A., Yang, F., and Anandkumar, A. Regularized learning for domain adaptation under label shifts. In International Conference on Learning Representations, 2019.\n\nAn information-theoretic approach to transferability in task transfer learning. Y Bao, Y Li, S.-L Huang, L Zhang, L Zheng, A Zamir, L Guibas, IEEE International Conference on Image Processing. Bao, Y., Li, Y., Huang, S.-L., Zhang, L., Zheng, L., Zamir, A., and Guibas, L. An information-theoretic approach to transferability in task transfer learning. In IEEE Interna- tional Conference on Image Processing, pp. 2309-2313, 2019.\n\nExploiting task relatedness for multiple task learning. S Ben-David, R Schuller, Learning Theory and Kernel Machines. SpringerBen-David, S. and Schuller, R. Exploiting task relatedness for multiple task learning. In Learning Theory and Kernel Machines, pp. 567-580. Springer, 2003.\n\nAnalysis of representations for domain adaptation. S Ben-David, J Blitzer, K Crammer, F Pereira, Advances in Neural Information Processing Systems. Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F. Analysis of representations for domain adaptation. In Advances in Neural Information Processing Systems, pp. 137-144, 2007.\n\nP2L: Predicting transfer learning for images and semantic relations. B Bhattacharjee, N Codella, J R Kender, S Huo, P Watson, M R Glass, P Dube, M Hill, B Belgodere, arXiv:1908.07630Bhattacharjee, B., Codella, N., Kender, J. R., Huo, S., Wat- son, P., Glass, M. R., Dube, P., Hill, M., and Belgodere, B. P2L: Predicting transfer learning for images and semantic relations. arXiv:1908.07630, 2019.\n\nStochastic gradient learning in neural networks. L Bottou, Proceedings of Neuro-N\u0131mes. Neuro-N\u0131mes9112Bottou, L. Stochastic gradient learning in neural networks. Proceedings of Neuro-N\u0131mes, 91(8):12, 1991.\n\nReturn of the devil in the details: Delving deep into convolutional nets. K Chatfield, K Simonyan, A Vedaldi, A Zisserman, British Machine Vision Conference. Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. In British Machine Vision Conference, 2014.\n\nMXNet: A flexible and efficient machine learning library for heterogeneous distributed systems. T Chen, M Li, Y Li, M Lin, N Wang, M Wang, T Xiao, B Xu, C Zhang, Z Zhang, arXiv:1512.01274Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang, C., and Zhang, Z. MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems. arXiv:1512.01274, 2015.\n\nA baseline for few-shot image classification. G S Dhillon, P Chaudhari, A Ravichandran, S Soatto, International Conference on Learning Representations. Dhillon, G. S., Chaudhari, P., Ravichandran, A., and Soatto, S. A baseline for few-shot image classification. In Inter- national Conference on Learning Representations, 2020.\n\nDeCAF: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, Darrell , T , International Conference on Machine Learning. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. DeCAF: A deep convolutional activation feature for generic visual recognition. In Inter- national Conference on Machine Learning, pp. 647-655, 2014.\n\nModeling transfer relationships between learning tasks for improved inductive transfer. E Eaton, T Lane, European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases. Eaton, E., Lane, T., et al. Modeling transfer relationships between learning tasks for improved inductive transfer. In European Conference on Machine Learning and Princi- ples and Practice of Knowledge Discovery in Databases, pp. 317-332, 2008.\n\nTowards a neural statistician. H Edwards, A Storkey, International Conference on Learning Representations. Edwards, H. and Storkey, A. Towards a neural statistician. In International Conference on Learning Representations, 2017.\n\nDas statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung. H Gebelein, ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift f\u00fcr Angewandte Mathematik und Mechanik. 216Gebelein, H. Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammen- hang mit der ausgleichsrechnung. ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift f\u00fcr Ange- wandte Mathematik und Mechanik, 21(6):364-379, 1941.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, Malik , J , IEEE Conference on Computer Vision and Pattern Recognition. Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and se- mantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587, 2014.\n\nJ Guo, H He, T He, L Lausen, M Li, H Lin, X Shi, C Wang, J Xie, S Zha, arXiv:1907.04433Deep learning in computer vision and natural language processing. arXiv preprintGuo, J., He, H., He, T., Lausen, L., Li, M., Lin, H., Shi, X., Wang, C., Xie, J., Zha, S., et al. GluonCV and GluonNLP: Deep learning in computer vision and natural language processing. arXiv preprint arXiv:1907.04433, 2019.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.\n\nA connection between correlation and contingency. H O Hirschfeld, Mathematical Proceedings of the Cambridge Philosophical Society. 31Hirschfeld, H. O. A connection between correlation and contingency. In Mathematical Proceedings of the Cam- bridge Philosophical Society, volume 31, pp. 520-524, 1935.\n\nA G Howard, M Zhu, B Chen, D Kalenichenko, W Wang, T Weyand, M Andreetto, Adam , H Mobilenets, arXiv:1704.04861Efficient convolutional neural networks for mobile vision applications. Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv:1704.04861, 2017.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, IEEE Conference on Computer Vision and Pattern Recognition. Hu, J., Shen, L., and Sun, G. Squeeze-and-excitation net- works. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 7132-7141, 2018.\n\nH S Jomaa, J Grabocka, L Schmidt-Thieme, Dataset2vec, arXiv:1905.11063Learning dataset meta-features. Jomaa, H. S., Grabocka, J., and Schmidt-Thieme, L. Dataset2vec: Learning dataset meta-features. arXiv:1905.11063, 2019.\n\nDetecting change in data streams. D Kifer, S Ben-David, J Gehrke, International Conference on Very Large Data Bases. 4Kifer, D., Ben-David, S., and Gehrke, J. Detecting change in data streams. In International Conference on Very Large Data Bases, volume 4, pp. 180-191, 2004.\n\nStochastic gradient vb and the variational auto-encoder. D P Kingma, M Welling, International Conference on Learning Representations. 19Kingma, D. P. and Welling, M. Stochastic gradient vb and the variational auto-encoder. In International Conference on Learning Representations, volume 19, 2014.\n\nDo better Imagenet models transfer better. S Kornblith, J Shlens, Q V Le, IEEE Conference on Computer Vision and Pattern Recognition. Kornblith, S., Shlens, J., and Le, Q. V. Do better Imagenet models transfer better? In IEEE Conference on Computer Vision and Pattern Recognition, pp. 2661-2671, 2019.\n\nLearning Multiple Layers of Features from Tiny Images. A Krizhevsky, University of TorontoMaster's thesisKrizhevsky, A. Learning Multiple Layers of Features from Tiny Images. Master's thesis, University of Toronto, 2009.\n\nDomain adaptation: Learning bounds and algorithms. Y Mansour, M Mohri, A Rostamizadeh, Annual Conference on Learning Theory. Mansour, Y., Mohri, M., and Rostamizadeh, A. Domain adaptation: Learning bounds and algorithms. In Annual Conference on Learning Theory, 2009.\n\nCrossstitch networks for multi-task learning. I Misra, A Shrivastava, A Gupta, M Hebert, IEEE Conference on Computer Vision and Pattern Recognition. Misra, I., Shrivastava, A., Gupta, A., and Hebert, M. Cross- stitch networks for multi-task learning. In IEEE Confer- ence on Computer Vision and Pattern Recognition, pp. 3994-4003, 2016.\n\nMeta-analysis of continual learning. C V Nguyen, A Achille, M Lam, T Hassner, V Mahadevan, S Soatto, Workshop on Meta-Learning @ NeurIPS. Nguyen, C. V., Achille, A., Lam, M., Hassner, T., Ma- hadevan, V., and Soatto, S. Meta-analysis of continual learning. In Workshop on Meta-Learning @ NeurIPS, 2019.\n\nLearning and transferring mid-level image representations using convolutional neural networks. M Oquab, L Bottou, I Laptev, J Sivic, IEEE Conference on Computer Vision and Pattern Recognition. Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and transferring mid-level image representations using convo- lutional neural networks. In IEEE Conference on Com- puter Vision and Pattern Recognition, pp. 1717-1724, 2014.\n\nScalable hyperparameter transfer learning. V Perrone, R Jenatton, M W Seeger, C Archambeau, Advances in Neural Information Processing Systems. Perrone, V., Jenatton, R., Seeger, M. W., and Archambeau, C. Scalable hyperparameter transfer learning. In Ad- vances in Neural Information Processing Systems, pp. 6845-6855, 2018.\n\nCNN features off-the-shelf: an astounding baseline for recognition. A S Razavian, H Azizpour, J Sullivan, Carlsson , S , IEEE Conference on Computer Vision and Pattern Recognition Workshops. Razavian, A. S., Azizpour, H., Sullivan, J., and Carlsson, S. CNN features off-the-shelf: an astounding baseline for recognition. In IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 806-813, 2014.\n\nJ Redmon, A Farhadi, Yolov3, arXiv:1804.02767An incremental improvement. Redmon, J. and Farhadi, A. Yolov3: An incremental im- provement. arXiv:1804.02767, 2018.\n\nOn measures of dependence. A R\u00e9nyi, Acta Mathematica Hungarica. 103-4R\u00e9nyi, A. On measures of dependence. Acta Mathematica Hungarica, 10(3-4):441-451, 1959.\n\nFast and flexible multi-task classification using conditional neural adaptive processes. J Requeima, J Gordon, J Bronskill, S Nowozin, R E Turner, Advances in Neural Information Processing Systems. Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R. E. Fast and flexible multi-task classification us- ing conditional neural adaptive processes. In Advances in Neural Information Processing Systems, pp. 7957-7968, 2019.\n\nA stochastic approximation method. The Annals of Mathematical Statistics. H Robbins, S Monro, Robbins, H. and Monro, S. A stochastic approximation method. The Annals of Mathematical Statistics, pp. 400- 407, 1951.\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, F.-F Li, International Journal of Computer Vision. 1153Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Li, F.-F. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.\n\nLearning inter-task transferability in the absence of target task samples. J Sinapov, S Narvekar, M Leonetti, P Stone, International Conference on Autonomous Agents and Multiagent Systems. Sinapov, J., Narvekar, S., Leonetti, M., and Stone, P. Learn- ing inter-task transferability in the absence of target task samples. In International Conference on Autonomous Agents and Multiagent Systems, pp. 725-733, 2015.\n\nWhich tasks should be learned together in multi-task learning?. T Standley, A R Zamir, D Chen, L Guibas, J Malik, S Savarese, arXiv:1905.07553Standley, T., Zamir, A. R., Chen, D., Guibas, L., Malik, J., and Savarese, S. Which tasks should be learned together in multi-task learning? arXiv:1905.07553, 2019.\n\nReturn of frustratingly easy domain adaptation. B Sun, J Feng, K Saenko, AAAI Conference on Artificial Intelligence. Sun, B., Feng, J., and Saenko, K. Return of frustratingly easy domain adaptation. In AAAI Conference on Artificial Intelligence, 2016.\n\nMeta-transfer learning for few-shot learning. Q Sun, Y Liu, T.-S Chua, B Schiele, IEEE Conference on Computer Vision and Pattern Recognition. Sun, Q., Liu, Y., Chua, T.-S., and Schiele, B. Meta-transfer learning for few-shot learning. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 403-412, 2019.\n\nTransferability and hardness of supervised classification tasks. A T Tran, C V Nguyen, T Hassner, International Conference on Computer Vision. Tran, A. T., Nguyen, C. V., and Hassner, T. Transferability and hardness of supervised classification tasks. In Inter- national Conference on Computer Vision, pp. 1395-1405, 2019.\n\nMeta-dataset: A dataset of datasets for learning to learn from few examples. E Triantafillou, T Zhu, V Dumoulin, P Lamblin, K Xu, R Goroshin, C Gelada, K Swersky, P.-A Manzagol, H Larochelle, International Conference on Learning Representations. Triantafillou, E., Zhu, T., Dumoulin, V., Lamblin, P., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.-A., and Larochelle, H. Meta-dataset: A dataset of datasets for learning to learn from few examples. In International Conference on Learning Representations, 2020.\n\nBalanced distribution adaptation for transfer learning. J Wang, Y Chen, S Hao, W Feng, Z Shen, IEEE International Conference on Data Mining. Wang, J., Chen, Y., Hao, S., Feng, W., and Shen, Z. Bal- anced distribution adaptation for transfer learning. In IEEE International Conference on Data Mining, pp. 1129-1134, 2017.\n\nTransfer learning via learning to transfer. Y Wei, Y Zhang, J Huang, Yang , Q , International Conference on Machine Learning. Wei, Y., Zhang, Y., Huang, J., and Yang, Q. Transfer learn- ing via learning to transfer. In International Conference on Machine Learning, pp. 5085-5094, 2018.\n\nA survey of transfer learning. K Weiss, T M Khoshgoftaar, Wang , D , Journal of Big Data. 319Weiss, K., Khoshgoftaar, T. M., and Wang, D. A survey of transfer learning. Journal of Big Data, 3(1):9, 2016.\n\nEfficient hardware for mobile computer vision via transfer learning. P N Whatmough, C Zhou, P Hansen, S K Venkataramanaiah, J.-S Seo, M Mattina, Fixynn, Conference on Systems and Machine Learning. Whatmough, P. N., Zhou, C., Hansen, P., Venkataramanaiah, S. K., Seo, J.-s., and Mattina, M. FixyNN: Efficient hardware for mobile computer vision via transfer learning. In Conference on Systems and Machine Learning, 2019.\n\nH Xiao, K Rasul, R Vollgraf, arXiv:1708.07747Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms. Xiao, H., Rasul, K., and Vollgraf, R. Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms. arXiv:1708.07747, 2017.\n\nTransfer learning. Q Yang, Y Zhang, W Dai, S J Pan, Cambridge University PressYang, Q., Zhang, Y., Dai, W., and Pan, S. J. Transfer learning. Cambridge University Press, 2020.\n\nHow transferable are features in deep neural networks?. J Yosinski, J Clune, Y Bengio, H Lipson, Advances in Neural Information Processing Systems. Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, pp. 3320-3328, 2014.\n\nDisentangling task transfer learning. A R Zamir, A Sax, W Shen, L J Guibas, J Malik, S Savarese, Taskonomy, IEEE Conference on Computer Vision and Pattern Recognition. Zamir, A. R., Sax, A., Shen, W., Guibas, L. J., Malik, J., and Savarese, S. Taskonomy: Disentangling task transfer learning. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 3712-3722, 2018.\n\nVisualizing and understanding convolutional networks. M D Zeiler, R Fergus, European Conference on Computer Vision. Zeiler, M. D. and Fergus, R. Visualizing and understand- ing convolutional networks. In European Conference on Computer Vision, pp. 818-833, 2014.\n\nContinual learning through synaptic intelligence. F Zenke, B Poole, S Ganguli, International Conference on Machine Learning. Zenke, F., Poole, B., and Ganguli, S. Continual learning through synaptic intelligence. In International Confer- ence on Machine Learning, pp. 3987-3995, 2017.\n", "annotations": {"author": "[{\"end\":93,\"start\":78},{\"end\":106,\"start\":94},{\"end\":125,\"start\":107},{\"end\":142,\"start\":126}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":86},{\"end\":105,\"start\":98},{\"end\":124,\"start\":114},{\"end\":141,\"start\":135}]", "author_first_name": "[{\"end\":83,\"start\":78},{\"end\":85,\"start\":84},{\"end\":97,\"start\":94},{\"end\":113,\"start\":107},{\"end\":134,\"start\":126}]", "author_affiliation": null, "title": "[{\"end\":75,\"start\":1},{\"end\":217,\"start\":143}]", "venue": null, "abstract": "[{\"end\":1113,\"start\":219}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1176,\"start\":1156},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1195,\"start\":1176},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1216,\"start\":1195},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":1884,\"start\":1865},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":1968,\"start\":1948},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2040,\"start\":2023},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2067,\"start\":2040},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2238,\"start\":2218},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2259,\"start\":2238},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2315,\"start\":2297},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2415,\"start\":2395},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2433,\"start\":2415},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3129,\"start\":3109},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3674,\"start\":3654},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3692,\"start\":3674},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4181,\"start\":4163},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4435,\"start\":4413},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4480,\"start\":4458},{\"end\":4520,\"start\":4480},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4961,\"start\":4938},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5004,\"start\":4986},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5021,\"start\":5004},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5042,\"start\":5021},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5349,\"start\":5330},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5381,\"start\":5363},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6468,\"start\":6448},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6486,\"start\":6468},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6667,\"start\":6641},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6696,\"start\":6678},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7155,\"start\":7133},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7177,\"start\":7155},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7196,\"start\":7177},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7219,\"start\":7196},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7491,\"start\":7469},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7513,\"start\":7491},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7536,\"start\":7513},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7557,\"start\":7536},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12078,\"start\":12055},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12091,\"start\":12078},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12603,\"start\":12585},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13519,\"start\":13500},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13771,\"start\":13753},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14300,\"start\":14280},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14318,\"start\":14300},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14361,\"start\":14343},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14378,\"start\":14361},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14400,\"start\":14378},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":14442,\"start\":14422},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14463,\"start\":14442},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14516,\"start\":14487},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14877,\"start\":14859},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14895,\"start\":14877},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15120,\"start\":15097},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15153,\"start\":15133},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15176,\"start\":15153},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15225,\"start\":15203},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15677,\"start\":15648},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":16078,\"start\":16060},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16100,\"start\":16083},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":16120,\"start\":16102},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16431,\"start\":16414},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16764,\"start\":16746},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16779,\"start\":16764},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16791,\"start\":16779},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17054,\"start\":17036},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17071,\"start\":17054},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17093,\"start\":17071},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17154,\"start\":17137},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17181,\"start\":17159},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17882,\"start\":17860},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18106,\"start\":18081},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":18125,\"start\":18106},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18146,\"start\":18125},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18165,\"start\":18146},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18974,\"start\":18950},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":19255,\"start\":19235},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20017,\"start\":19999},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20050,\"start\":20033},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20186,\"start\":20167},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20203,\"start\":20186},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20460,\"start\":20443},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20521,\"start\":20495},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20553,\"start\":20536},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20604,\"start\":20587},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20707,\"start\":20690},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23384,\"start\":23365},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24150,\"start\":24132},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":25825,\"start\":25807},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25842,\"start\":25825},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25864,\"start\":25842},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25959,\"start\":25936},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26576,\"start\":26554},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":26732,\"start\":26704},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27100,\"start\":27078},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29922,\"start\":29904},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29968,\"start\":29951},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32044,\"start\":32027},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32124,\"start\":32103},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32160,\"start\":32136},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32192,\"start\":32175},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32364,\"start\":32345},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32395,\"start\":32377},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32518,\"start\":32494},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":33836,\"start\":33817},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33869,\"start\":33851},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34584,\"start\":34560},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":34602,\"start\":34584},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":35249,\"start\":35229},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":35269,\"start\":35249},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":35311,\"start\":35291},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":35333,\"start\":35311},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":35380,\"start\":35357},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":35658,\"start\":35635},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":35782,\"start\":35760},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37706,\"start\":37687},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37734,\"start\":37716},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":37782,\"start\":37758},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":40228,\"start\":40209},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":40255,\"start\":40237},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":40308,\"start\":40284}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38289,\"start\":37969},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38678,\"start\":38290},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38863,\"start\":38679},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39035,\"start\":38864},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39445,\"start\":39036},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39598,\"start\":39446},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39694,\"start\":39599},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40066,\"start\":39695},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40138,\"start\":40067},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42606,\"start\":40139},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43557,\"start\":42607}]", "paragraph": "[{\"end\":1731,\"start\":1129},{\"end\":2434,\"start\":1733},{\"end\":2888,\"start\":2436},{\"end\":3772,\"start\":2890},{\"end\":4182,\"start\":3774},{\"end\":4778,\"start\":4184},{\"end\":5236,\"start\":4780},{\"end\":5640,\"start\":5238},{\"end\":6728,\"start\":5678},{\"end\":7558,\"start\":6730},{\"end\":7879,\"start\":7560},{\"end\":8208,\"start\":7881},{\"end\":8854,\"start\":8210},{\"end\":9147,\"start\":8856},{\"end\":9364,\"start\":9185},{\"end\":9517,\"start\":9366},{\"end\":9989,\"start\":9587},{\"end\":10162,\"start\":9991},{\"end\":10719,\"start\":10218},{\"end\":11139,\"start\":10754},{\"end\":11544,\"start\":11170},{\"end\":11651,\"start\":11546},{\"end\":12453,\"start\":11653},{\"end\":13164,\"start\":12455},{\"end\":13720,\"start\":13225},{\"end\":14148,\"start\":13722},{\"end\":14581,\"start\":14165},{\"end\":15631,\"start\":14583},{\"end\":15978,\"start\":15633},{\"end\":16911,\"start\":15980},{\"end\":18002,\"start\":16913},{\"end\":18886,\"start\":18004},{\"end\":19620,\"start\":18888},{\"end\":20204,\"start\":19636},{\"end\":21097,\"start\":20235},{\"end\":21208,\"start\":21099},{\"end\":21448,\"start\":21210},{\"end\":21715,\"start\":21450},{\"end\":22130,\"start\":21717},{\"end\":22802,\"start\":22132},{\"end\":23266,\"start\":22854},{\"end\":24000,\"start\":23268},{\"end\":24763,\"start\":24041},{\"end\":25007,\"start\":24765},{\"end\":25662,\"start\":25009},{\"end\":26469,\"start\":25711},{\"end\":27163,\"start\":26471},{\"end\":27614,\"start\":27165},{\"end\":28054,\"start\":27660},{\"end\":29160,\"start\":28056},{\"end\":29801,\"start\":29162},{\"end\":31296,\"start\":29843},{\"end\":31745,\"start\":31298},{\"end\":32980,\"start\":31781},{\"end\":33450,\"start\":32982},{\"end\":34603,\"start\":33466},{\"end\":35117,\"start\":34641},{\"end\":36246,\"start\":35119},{\"end\":36487,\"start\":36287},{\"end\":36705,\"start\":36516},{\"end\":37968,\"start\":36738}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9184,\"start\":9148},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9586,\"start\":9518},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10217,\"start\":10163},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11169,\"start\":11140},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13224,\"start\":13165}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22510,\"start\":22503},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22584,\"start\":22577},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27188,\"start\":27181},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30750,\"start\":30743},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31053,\"start\":31046}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1127,\"start\":1115},{\"attributes\":{\"n\":\"2.\"},\"end\":5676,\"start\":5643},{\"attributes\":{\"n\":\"3.\"},\"end\":10752,\"start\":10722},{\"attributes\":{\"n\":\"4.\"},\"end\":14163,\"start\":14151},{\"attributes\":{\"n\":\"5.\"},\"end\":19634,\"start\":19623},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20233,\"start\":20207},{\"attributes\":{\"n\":\"5.2.\"},\"end\":22852,\"start\":22805},{\"attributes\":{\"n\":\"5.3.\"},\"end\":24039,\"start\":24003},{\"attributes\":{\"n\":\"5.4.\"},\"end\":25709,\"start\":25665},{\"attributes\":{\"n\":\"5.5.\"},\"end\":27658,\"start\":27617},{\"attributes\":{\"n\":\"5.6.\"},\"end\":29841,\"start\":29804},{\"attributes\":{\"n\":\"5.7.\"},\"end\":31779,\"start\":31748},{\"attributes\":{\"n\":\"6.\"},\"end\":33464,\"start\":33453},{\"end\":34615,\"start\":34606},{\"end\":34639,\"start\":34618},{\"end\":36258,\"start\":36249},{\"end\":36285,\"start\":36261},{\"end\":36514,\"start\":36490},{\"end\":36736,\"start\":36708},{\"end\":37976,\"start\":37970},{\"end\":38299,\"start\":38291},{\"end\":38690,\"start\":38680},{\"end\":38871,\"start\":38865},{\"end\":39045,\"start\":39037},{\"end\":39606,\"start\":39600},{\"end\":39704,\"start\":39696},{\"end\":40077,\"start\":40068}]", "table": "[{\"end\":42606,\"start\":40478},{\"end\":43557,\"start\":43005}]", "figure_caption": "[{\"end\":38289,\"start\":37978},{\"end\":38678,\"start\":38301},{\"end\":38863,\"start\":38692},{\"end\":39035,\"start\":38873},{\"end\":39445,\"start\":39047},{\"end\":39598,\"start\":39448},{\"end\":39694,\"start\":39608},{\"end\":40066,\"start\":39706},{\"end\":40138,\"start\":40079},{\"end\":40478,\"start\":40141},{\"end\":43005,\"start\":42609}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22572,\"start\":22566},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23704,\"start\":23695},{\"end\":23872,\"start\":23866},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":25282,\"start\":25273},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25513,\"start\":25507},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27421,\"start\":27415},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27517,\"start\":27511},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28720,\"start\":28712},{\"end\":29199,\"start\":29193},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29226,\"start\":29220},{\"end\":32609,\"start\":32603},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33039,\"start\":33033},{\"end\":36745,\"start\":36742},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36838,\"start\":36832},{\"end\":36933,\"start\":36927},{\"end\":37163,\"start\":37155}]", "bib_author_first_name": "[{\"end\":43718,\"start\":43717},{\"end\":43729,\"start\":43728},{\"end\":43736,\"start\":43735},{\"end\":43746,\"start\":43745},{\"end\":43762,\"start\":43761},{\"end\":43770,\"start\":43769},{\"end\":43772,\"start\":43771},{\"end\":43783,\"start\":43782},{\"end\":43793,\"start\":43792},{\"end\":44149,\"start\":44148},{\"end\":44160,\"start\":44159},{\"end\":44176,\"start\":44171},{\"end\":44180,\"start\":44179},{\"end\":44512,\"start\":44511},{\"end\":44525,\"start\":44524},{\"end\":44527,\"start\":44526},{\"end\":44774,\"start\":44773},{\"end\":44776,\"start\":44775},{\"end\":44785,\"start\":44784},{\"end\":44794,\"start\":44793},{\"end\":44796,\"start\":44795},{\"end\":44806,\"start\":44805},{\"end\":44808,\"start\":44807},{\"end\":44818,\"start\":44817},{\"end\":44831,\"start\":44830},{\"end\":44840,\"start\":44839},{\"end\":45204,\"start\":45203},{\"end\":45223,\"start\":45222},{\"end\":45230,\"start\":45229},{\"end\":45247,\"start\":45237},{\"end\":45251,\"start\":45250},{\"end\":45575,\"start\":45574},{\"end\":45582,\"start\":45581},{\"end\":45591,\"start\":45587},{\"end\":45600,\"start\":45599},{\"end\":45609,\"start\":45608},{\"end\":45618,\"start\":45617},{\"end\":45627,\"start\":45626},{\"end\":45981,\"start\":45980},{\"end\":45994,\"start\":45993},{\"end\":46259,\"start\":46258},{\"end\":46272,\"start\":46271},{\"end\":46283,\"start\":46282},{\"end\":46294,\"start\":46293},{\"end\":46607,\"start\":46606},{\"end\":46624,\"start\":46623},{\"end\":46635,\"start\":46634},{\"end\":46637,\"start\":46636},{\"end\":46647,\"start\":46646},{\"end\":46654,\"start\":46653},{\"end\":46664,\"start\":46663},{\"end\":46666,\"start\":46665},{\"end\":46675,\"start\":46674},{\"end\":46683,\"start\":46682},{\"end\":46691,\"start\":46690},{\"end\":46985,\"start\":46984},{\"end\":47217,\"start\":47216},{\"end\":47230,\"start\":47229},{\"end\":47242,\"start\":47241},{\"end\":47253,\"start\":47252},{\"end\":47576,\"start\":47575},{\"end\":47584,\"start\":47583},{\"end\":47590,\"start\":47589},{\"end\":47596,\"start\":47595},{\"end\":47603,\"start\":47602},{\"end\":47611,\"start\":47610},{\"end\":47619,\"start\":47618},{\"end\":47627,\"start\":47626},{\"end\":47633,\"start\":47632},{\"end\":47642,\"start\":47641},{\"end\":47932,\"start\":47931},{\"end\":47934,\"start\":47933},{\"end\":47945,\"start\":47944},{\"end\":47958,\"start\":47957},{\"end\":47974,\"start\":47973},{\"end\":48293,\"start\":48292},{\"end\":48304,\"start\":48303},{\"end\":48311,\"start\":48310},{\"end\":48322,\"start\":48321},{\"end\":48333,\"start\":48332},{\"end\":48342,\"start\":48341},{\"end\":48357,\"start\":48350},{\"end\":48361,\"start\":48360},{\"end\":48735,\"start\":48734},{\"end\":48744,\"start\":48743},{\"end\":49134,\"start\":49133},{\"end\":49145,\"start\":49144},{\"end\":49460,\"start\":49459},{\"end\":49927,\"start\":49926},{\"end\":49939,\"start\":49938},{\"end\":49950,\"start\":49949},{\"end\":49965,\"start\":49960},{\"end\":49969,\"start\":49968},{\"end\":50254,\"start\":50253},{\"end\":50261,\"start\":50260},{\"end\":50267,\"start\":50266},{\"end\":50273,\"start\":50272},{\"end\":50283,\"start\":50282},{\"end\":50289,\"start\":50288},{\"end\":50296,\"start\":50295},{\"end\":50303,\"start\":50302},{\"end\":50311,\"start\":50310},{\"end\":50318,\"start\":50317},{\"end\":50693,\"start\":50692},{\"end\":50699,\"start\":50698},{\"end\":50708,\"start\":50707},{\"end\":50715,\"start\":50714},{\"end\":51001,\"start\":51000},{\"end\":51003,\"start\":51002},{\"end\":51253,\"start\":51252},{\"end\":51255,\"start\":51254},{\"end\":51265,\"start\":51264},{\"end\":51272,\"start\":51271},{\"end\":51280,\"start\":51279},{\"end\":51296,\"start\":51295},{\"end\":51304,\"start\":51303},{\"end\":51314,\"start\":51313},{\"end\":51330,\"start\":51326},{\"end\":51334,\"start\":51333},{\"end\":51680,\"start\":51679},{\"end\":51686,\"start\":51685},{\"end\":51694,\"start\":51693},{\"end\":51911,\"start\":51910},{\"end\":51913,\"start\":51912},{\"end\":51922,\"start\":51921},{\"end\":51934,\"start\":51933},{\"end\":52168,\"start\":52167},{\"end\":52177,\"start\":52176},{\"end\":52190,\"start\":52189},{\"end\":52468,\"start\":52467},{\"end\":52470,\"start\":52469},{\"end\":52480,\"start\":52479},{\"end\":52752,\"start\":52751},{\"end\":52765,\"start\":52764},{\"end\":52775,\"start\":52774},{\"end\":52777,\"start\":52776},{\"end\":53067,\"start\":53066},{\"end\":53285,\"start\":53284},{\"end\":53296,\"start\":53295},{\"end\":53305,\"start\":53304},{\"end\":53549,\"start\":53548},{\"end\":53558,\"start\":53557},{\"end\":53573,\"start\":53572},{\"end\":53582,\"start\":53581},{\"end\":53878,\"start\":53877},{\"end\":53880,\"start\":53879},{\"end\":53890,\"start\":53889},{\"end\":53901,\"start\":53900},{\"end\":53908,\"start\":53907},{\"end\":53919,\"start\":53918},{\"end\":53932,\"start\":53931},{\"end\":54240,\"start\":54239},{\"end\":54249,\"start\":54248},{\"end\":54259,\"start\":54258},{\"end\":54269,\"start\":54268},{\"end\":54614,\"start\":54613},{\"end\":54625,\"start\":54624},{\"end\":54637,\"start\":54636},{\"end\":54639,\"start\":54638},{\"end\":54649,\"start\":54648},{\"end\":54964,\"start\":54963},{\"end\":54966,\"start\":54965},{\"end\":54978,\"start\":54977},{\"end\":54990,\"start\":54989},{\"end\":55009,\"start\":55001},{\"end\":55013,\"start\":55012},{\"end\":55310,\"start\":55309},{\"end\":55320,\"start\":55319},{\"end\":55500,\"start\":55499},{\"end\":55720,\"start\":55719},{\"end\":55732,\"start\":55731},{\"end\":55742,\"start\":55741},{\"end\":55755,\"start\":55754},{\"end\":55766,\"start\":55765},{\"end\":55768,\"start\":55767},{\"end\":56142,\"start\":56141},{\"end\":56153,\"start\":56152},{\"end\":56334,\"start\":56333},{\"end\":56349,\"start\":56348},{\"end\":56357,\"start\":56356},{\"end\":56363,\"start\":56362},{\"end\":56373,\"start\":56372},{\"end\":56385,\"start\":56384},{\"end\":56391,\"start\":56390},{\"end\":56400,\"start\":56399},{\"end\":56412,\"start\":56411},{\"end\":56422,\"start\":56421},{\"end\":56435,\"start\":56434},{\"end\":56437,\"start\":56436},{\"end\":56448,\"start\":56444},{\"end\":56839,\"start\":56838},{\"end\":56850,\"start\":56849},{\"end\":56862,\"start\":56861},{\"end\":56874,\"start\":56873},{\"end\":57242,\"start\":57241},{\"end\":57254,\"start\":57253},{\"end\":57256,\"start\":57255},{\"end\":57265,\"start\":57264},{\"end\":57273,\"start\":57272},{\"end\":57283,\"start\":57282},{\"end\":57292,\"start\":57291},{\"end\":57534,\"start\":57533},{\"end\":57541,\"start\":57540},{\"end\":57549,\"start\":57548},{\"end\":57785,\"start\":57784},{\"end\":57792,\"start\":57791},{\"end\":57802,\"start\":57798},{\"end\":57810,\"start\":57809},{\"end\":58122,\"start\":58121},{\"end\":58124,\"start\":58123},{\"end\":58132,\"start\":58131},{\"end\":58134,\"start\":58133},{\"end\":58144,\"start\":58143},{\"end\":58458,\"start\":58457},{\"end\":58475,\"start\":58474},{\"end\":58482,\"start\":58481},{\"end\":58494,\"start\":58493},{\"end\":58505,\"start\":58504},{\"end\":58511,\"start\":58510},{\"end\":58523,\"start\":58522},{\"end\":58533,\"start\":58532},{\"end\":58547,\"start\":58543},{\"end\":58559,\"start\":58558},{\"end\":58962,\"start\":58961},{\"end\":58970,\"start\":58969},{\"end\":58978,\"start\":58977},{\"end\":58985,\"start\":58984},{\"end\":58993,\"start\":58992},{\"end\":59272,\"start\":59271},{\"end\":59279,\"start\":59278},{\"end\":59288,\"start\":59287},{\"end\":59300,\"start\":59296},{\"end\":59304,\"start\":59303},{\"end\":59546,\"start\":59545},{\"end\":59555,\"start\":59554},{\"end\":59557,\"start\":59556},{\"end\":59576,\"start\":59572},{\"end\":59580,\"start\":59579},{\"end\":59789,\"start\":59788},{\"end\":59791,\"start\":59790},{\"end\":59804,\"start\":59803},{\"end\":59812,\"start\":59811},{\"end\":59822,\"start\":59821},{\"end\":59824,\"start\":59823},{\"end\":59847,\"start\":59843},{\"end\":59854,\"start\":59853},{\"end\":60141,\"start\":60140},{\"end\":60149,\"start\":60148},{\"end\":60158,\"start\":60157},{\"end\":60434,\"start\":60433},{\"end\":60442,\"start\":60441},{\"end\":60451,\"start\":60450},{\"end\":60458,\"start\":60457},{\"end\":60460,\"start\":60459},{\"end\":60648,\"start\":60647},{\"end\":60660,\"start\":60659},{\"end\":60669,\"start\":60668},{\"end\":60679,\"start\":60678},{\"end\":60961,\"start\":60960},{\"end\":60963,\"start\":60962},{\"end\":60972,\"start\":60971},{\"end\":60979,\"start\":60978},{\"end\":60987,\"start\":60986},{\"end\":60989,\"start\":60988},{\"end\":60999,\"start\":60998},{\"end\":61008,\"start\":61007},{\"end\":61355,\"start\":61354},{\"end\":61357,\"start\":61356},{\"end\":61367,\"start\":61366},{\"end\":61615,\"start\":61614},{\"end\":61624,\"start\":61623},{\"end\":61633,\"start\":61632}]", "bib_author_last_name": "[{\"end\":43726,\"start\":43719},{\"end\":43733,\"start\":43730},{\"end\":43743,\"start\":43737},{\"end\":43759,\"start\":43747},{\"end\":43767,\"start\":43763},{\"end\":43780,\"start\":43773},{\"end\":43790,\"start\":43784},{\"end\":43800,\"start\":43794},{\"end\":44157,\"start\":44150},{\"end\":44169,\"start\":44161},{\"end\":44522,\"start\":44513},{\"end\":44533,\"start\":44528},{\"end\":44782,\"start\":44777},{\"end\":44791,\"start\":44786},{\"end\":44803,\"start\":44797},{\"end\":44815,\"start\":44809},{\"end\":44828,\"start\":44819},{\"end\":44837,\"start\":44832},{\"end\":44846,\"start\":44841},{\"end\":45220,\"start\":45205},{\"end\":45227,\"start\":45224},{\"end\":45235,\"start\":45231},{\"end\":45579,\"start\":45576},{\"end\":45585,\"start\":45583},{\"end\":45597,\"start\":45592},{\"end\":45606,\"start\":45601},{\"end\":45615,\"start\":45610},{\"end\":45624,\"start\":45619},{\"end\":45634,\"start\":45628},{\"end\":45991,\"start\":45982},{\"end\":46003,\"start\":45995},{\"end\":46269,\"start\":46260},{\"end\":46280,\"start\":46273},{\"end\":46291,\"start\":46284},{\"end\":46302,\"start\":46295},{\"end\":46621,\"start\":46608},{\"end\":46632,\"start\":46625},{\"end\":46644,\"start\":46638},{\"end\":46651,\"start\":46648},{\"end\":46661,\"start\":46655},{\"end\":46672,\"start\":46667},{\"end\":46680,\"start\":46676},{\"end\":46688,\"start\":46684},{\"end\":46701,\"start\":46692},{\"end\":46992,\"start\":46986},{\"end\":47227,\"start\":47218},{\"end\":47239,\"start\":47231},{\"end\":47250,\"start\":47243},{\"end\":47263,\"start\":47254},{\"end\":47581,\"start\":47577},{\"end\":47587,\"start\":47585},{\"end\":47593,\"start\":47591},{\"end\":47600,\"start\":47597},{\"end\":47608,\"start\":47604},{\"end\":47616,\"start\":47612},{\"end\":47624,\"start\":47620},{\"end\":47630,\"start\":47628},{\"end\":47639,\"start\":47634},{\"end\":47648,\"start\":47643},{\"end\":47942,\"start\":47935},{\"end\":47955,\"start\":47946},{\"end\":47971,\"start\":47959},{\"end\":47981,\"start\":47975},{\"end\":48301,\"start\":48294},{\"end\":48308,\"start\":48305},{\"end\":48319,\"start\":48312},{\"end\":48330,\"start\":48323},{\"end\":48339,\"start\":48334},{\"end\":48348,\"start\":48343},{\"end\":48741,\"start\":48736},{\"end\":48749,\"start\":48745},{\"end\":49142,\"start\":49135},{\"end\":49153,\"start\":49146},{\"end\":49469,\"start\":49461},{\"end\":49936,\"start\":49928},{\"end\":49947,\"start\":49940},{\"end\":49958,\"start\":49951},{\"end\":50258,\"start\":50255},{\"end\":50264,\"start\":50262},{\"end\":50270,\"start\":50268},{\"end\":50280,\"start\":50274},{\"end\":50286,\"start\":50284},{\"end\":50293,\"start\":50290},{\"end\":50300,\"start\":50297},{\"end\":50308,\"start\":50304},{\"end\":50315,\"start\":50312},{\"end\":50322,\"start\":50319},{\"end\":50696,\"start\":50694},{\"end\":50705,\"start\":50700},{\"end\":50712,\"start\":50709},{\"end\":50719,\"start\":50716},{\"end\":51014,\"start\":51004},{\"end\":51262,\"start\":51256},{\"end\":51269,\"start\":51266},{\"end\":51277,\"start\":51273},{\"end\":51293,\"start\":51281},{\"end\":51301,\"start\":51297},{\"end\":51311,\"start\":51305},{\"end\":51324,\"start\":51315},{\"end\":51345,\"start\":51335},{\"end\":51683,\"start\":51681},{\"end\":51691,\"start\":51687},{\"end\":51698,\"start\":51695},{\"end\":51919,\"start\":51914},{\"end\":51931,\"start\":51923},{\"end\":51949,\"start\":51935},{\"end\":51962,\"start\":51951},{\"end\":52174,\"start\":52169},{\"end\":52187,\"start\":52178},{\"end\":52197,\"start\":52191},{\"end\":52477,\"start\":52471},{\"end\":52488,\"start\":52481},{\"end\":52762,\"start\":52753},{\"end\":52772,\"start\":52766},{\"end\":52780,\"start\":52778},{\"end\":53078,\"start\":53068},{\"end\":53293,\"start\":53286},{\"end\":53302,\"start\":53297},{\"end\":53318,\"start\":53306},{\"end\":53555,\"start\":53550},{\"end\":53570,\"start\":53559},{\"end\":53579,\"start\":53574},{\"end\":53589,\"start\":53583},{\"end\":53887,\"start\":53881},{\"end\":53898,\"start\":53891},{\"end\":53905,\"start\":53902},{\"end\":53916,\"start\":53909},{\"end\":53929,\"start\":53920},{\"end\":53939,\"start\":53933},{\"end\":54246,\"start\":54241},{\"end\":54256,\"start\":54250},{\"end\":54266,\"start\":54260},{\"end\":54275,\"start\":54270},{\"end\":54622,\"start\":54615},{\"end\":54634,\"start\":54626},{\"end\":54646,\"start\":54640},{\"end\":54660,\"start\":54650},{\"end\":54975,\"start\":54967},{\"end\":54987,\"start\":54979},{\"end\":54999,\"start\":54991},{\"end\":55317,\"start\":55311},{\"end\":55328,\"start\":55321},{\"end\":55336,\"start\":55330},{\"end\":55506,\"start\":55501},{\"end\":55729,\"start\":55721},{\"end\":55739,\"start\":55733},{\"end\":55752,\"start\":55743},{\"end\":55763,\"start\":55756},{\"end\":55775,\"start\":55769},{\"end\":56150,\"start\":56143},{\"end\":56159,\"start\":56154},{\"end\":56346,\"start\":56335},{\"end\":56354,\"start\":56350},{\"end\":56360,\"start\":56358},{\"end\":56370,\"start\":56364},{\"end\":56382,\"start\":56374},{\"end\":56388,\"start\":56386},{\"end\":56397,\"start\":56392},{\"end\":56409,\"start\":56401},{\"end\":56419,\"start\":56413},{\"end\":56432,\"start\":56423},{\"end\":56442,\"start\":56438},{\"end\":56451,\"start\":56449},{\"end\":56847,\"start\":56840},{\"end\":56859,\"start\":56851},{\"end\":56871,\"start\":56863},{\"end\":56880,\"start\":56875},{\"end\":57251,\"start\":57243},{\"end\":57262,\"start\":57257},{\"end\":57270,\"start\":57266},{\"end\":57280,\"start\":57274},{\"end\":57289,\"start\":57284},{\"end\":57301,\"start\":57293},{\"end\":57538,\"start\":57535},{\"end\":57546,\"start\":57542},{\"end\":57556,\"start\":57550},{\"end\":57789,\"start\":57786},{\"end\":57796,\"start\":57793},{\"end\":57807,\"start\":57803},{\"end\":57818,\"start\":57811},{\"end\":58129,\"start\":58125},{\"end\":58141,\"start\":58135},{\"end\":58152,\"start\":58145},{\"end\":58472,\"start\":58459},{\"end\":58479,\"start\":58476},{\"end\":58491,\"start\":58483},{\"end\":58502,\"start\":58495},{\"end\":58508,\"start\":58506},{\"end\":58520,\"start\":58512},{\"end\":58530,\"start\":58524},{\"end\":58541,\"start\":58534},{\"end\":58556,\"start\":58548},{\"end\":58570,\"start\":58560},{\"end\":58967,\"start\":58963},{\"end\":58975,\"start\":58971},{\"end\":58982,\"start\":58979},{\"end\":58990,\"start\":58986},{\"end\":58998,\"start\":58994},{\"end\":59276,\"start\":59273},{\"end\":59285,\"start\":59280},{\"end\":59294,\"start\":59289},{\"end\":59552,\"start\":59547},{\"end\":59570,\"start\":59558},{\"end\":59801,\"start\":59792},{\"end\":59809,\"start\":59805},{\"end\":59819,\"start\":59813},{\"end\":59841,\"start\":59825},{\"end\":59851,\"start\":59848},{\"end\":59862,\"start\":59855},{\"end\":59870,\"start\":59864},{\"end\":60146,\"start\":60142},{\"end\":60155,\"start\":60150},{\"end\":60167,\"start\":60159},{\"end\":60439,\"start\":60435},{\"end\":60448,\"start\":60443},{\"end\":60455,\"start\":60452},{\"end\":60464,\"start\":60461},{\"end\":60657,\"start\":60649},{\"end\":60666,\"start\":60661},{\"end\":60676,\"start\":60670},{\"end\":60686,\"start\":60680},{\"end\":60969,\"start\":60964},{\"end\":60976,\"start\":60973},{\"end\":60984,\"start\":60980},{\"end\":60996,\"start\":60990},{\"end\":61005,\"start\":61000},{\"end\":61017,\"start\":61009},{\"end\":61028,\"start\":61019},{\"end\":61364,\"start\":61358},{\"end\":61374,\"start\":61368},{\"end\":61621,\"start\":61616},{\"end\":61630,\"start\":61625},{\"end\":61641,\"start\":61634}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":60440365},\"end\":44066,\"start\":43673},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1811598},\"end\":44405,\"start\":44068},{\"attributes\":{\"id\":\"b2\"},\"end\":44692,\"start\":44407},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":11596016},\"end\":45138,\"start\":44694},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":68220930},\"end\":45492,\"start\":45140},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":202782600},\"end\":45922,\"start\":45494},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":13967968},\"end\":46205,\"start\":45924},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":10908021},\"end\":46535,\"start\":46207},{\"attributes\":{\"doi\":\"arXiv:1908.07630\",\"id\":\"b8\"},\"end\":46933,\"start\":46537},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":12410481},\"end\":47140,\"start\":46935},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7204540},\"end\":47477,\"start\":47142},{\"attributes\":{\"doi\":\"arXiv:1512.01274\",\"id\":\"b11\"},\"end\":47883,\"start\":47479},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":202230734},\"end\":48211,\"start\":47885},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6161478},\"end\":48644,\"start\":48213},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14414833},\"end\":49100,\"start\":48646},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4994434},\"end\":49330,\"start\":49102},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":119970955},\"end\":49842,\"start\":49332},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":215827080},\"end\":50251,\"start\":49844},{\"attributes\":{\"doi\":\"arXiv:1907.04433\",\"id\":\"b18\"},\"end\":50644,\"start\":50253},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206594692},\"end\":50948,\"start\":50646},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":123648642},\"end\":51250,\"start\":50950},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b21\"},\"end\":51644,\"start\":51252},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":140309863},\"end\":51908,\"start\":51646},{\"attributes\":{\"doi\":\"arXiv:1905.11063\",\"id\":\"b23\"},\"end\":52131,\"start\":51910},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":52800392},\"end\":52408,\"start\":52133},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":51841161},\"end\":52706,\"start\":52410},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":43928547},\"end\":53009,\"start\":52708},{\"attributes\":{\"id\":\"b27\"},\"end\":53231,\"start\":53011},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6178817},\"end\":53500,\"start\":53233},{\"attributes\":{\"id\":\"b29\"},\"end\":53838,\"start\":53502},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":209517421},\"end\":54142,\"start\":53840},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206592191},\"end\":54568,\"start\":54144},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":54035096},\"end\":54893,\"start\":54570},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6383532},\"end\":55307,\"start\":54895},{\"attributes\":{\"doi\":\"arXiv:1804.02767\",\"id\":\"b34\"},\"end\":55470,\"start\":55309},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":28185082},\"end\":55628,\"start\":55472},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":190000591},\"end\":56065,\"start\":55630},{\"attributes\":{\"id\":\"b37\"},\"end\":56280,\"start\":56067},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2930547},\"end\":56761,\"start\":56282},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":16510135},\"end\":57175,\"start\":56763},{\"attributes\":{\"doi\":\"arXiv:1905.07553\",\"id\":\"b40\"},\"end\":57483,\"start\":57177},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":16439870},\"end\":57736,\"start\":57485},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":54448258},\"end\":58054,\"start\":57738},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":201303557},\"end\":58378,\"start\":58056},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":71145737},\"end\":58903,\"start\":58380},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":6480652},\"end\":59225,\"start\":58905},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":51869212},\"end\":59512,\"start\":59227},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":740063},\"end\":59717,\"start\":59514},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":67855411},\"end\":60138,\"start\":59719},{\"attributes\":{\"doi\":\"arXiv:1708.07747\",\"id\":\"b49\"},\"end\":60412,\"start\":60140},{\"attributes\":{\"id\":\"b50\"},\"end\":60589,\"start\":60414},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":362467},\"end\":60920,\"start\":60591},{\"attributes\":{\"id\":\"b52\"},\"end\":61298,\"start\":60922},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":3960646},\"end\":61562,\"start\":61300},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":10409742},\"end\":61848,\"start\":61564}]", "bib_title": "[{\"end\":43715,\"start\":43673},{\"end\":44146,\"start\":44068},{\"end\":44771,\"start\":44694},{\"end\":45201,\"start\":45140},{\"end\":45572,\"start\":45494},{\"end\":45978,\"start\":45924},{\"end\":46256,\"start\":46207},{\"end\":46982,\"start\":46935},{\"end\":47214,\"start\":47142},{\"end\":47929,\"start\":47885},{\"end\":48290,\"start\":48213},{\"end\":48732,\"start\":48646},{\"end\":49131,\"start\":49102},{\"end\":49457,\"start\":49332},{\"end\":49924,\"start\":49844},{\"end\":50690,\"start\":50646},{\"end\":50998,\"start\":50950},{\"end\":51677,\"start\":51646},{\"end\":52165,\"start\":52133},{\"end\":52465,\"start\":52410},{\"end\":52749,\"start\":52708},{\"end\":53282,\"start\":53233},{\"end\":53546,\"start\":53502},{\"end\":53875,\"start\":53840},{\"end\":54237,\"start\":54144},{\"end\":54611,\"start\":54570},{\"end\":54961,\"start\":54895},{\"end\":55497,\"start\":55472},{\"end\":55717,\"start\":55630},{\"end\":56331,\"start\":56282},{\"end\":56836,\"start\":56763},{\"end\":57531,\"start\":57485},{\"end\":57782,\"start\":57738},{\"end\":58119,\"start\":58056},{\"end\":58455,\"start\":58380},{\"end\":58959,\"start\":58905},{\"end\":59269,\"start\":59227},{\"end\":59543,\"start\":59514},{\"end\":59786,\"start\":59719},{\"end\":60645,\"start\":60591},{\"end\":60958,\"start\":60922},{\"end\":61352,\"start\":61300},{\"end\":61612,\"start\":61564}]", "bib_author": "[{\"end\":43728,\"start\":43717},{\"end\":43735,\"start\":43728},{\"end\":43745,\"start\":43735},{\"end\":43761,\"start\":43745},{\"end\":43769,\"start\":43761},{\"end\":43782,\"start\":43769},{\"end\":43792,\"start\":43782},{\"end\":43802,\"start\":43792},{\"end\":44159,\"start\":44148},{\"end\":44171,\"start\":44159},{\"end\":44179,\"start\":44171},{\"end\":44183,\"start\":44179},{\"end\":44524,\"start\":44511},{\"end\":44535,\"start\":44524},{\"end\":44784,\"start\":44773},{\"end\":44793,\"start\":44784},{\"end\":44805,\"start\":44793},{\"end\":44817,\"start\":44805},{\"end\":44830,\"start\":44817},{\"end\":44839,\"start\":44830},{\"end\":44848,\"start\":44839},{\"end\":45222,\"start\":45203},{\"end\":45229,\"start\":45222},{\"end\":45237,\"start\":45229},{\"end\":45250,\"start\":45237},{\"end\":45254,\"start\":45250},{\"end\":45581,\"start\":45574},{\"end\":45587,\"start\":45581},{\"end\":45599,\"start\":45587},{\"end\":45608,\"start\":45599},{\"end\":45617,\"start\":45608},{\"end\":45626,\"start\":45617},{\"end\":45636,\"start\":45626},{\"end\":45993,\"start\":45980},{\"end\":46005,\"start\":45993},{\"end\":46271,\"start\":46258},{\"end\":46282,\"start\":46271},{\"end\":46293,\"start\":46282},{\"end\":46304,\"start\":46293},{\"end\":46623,\"start\":46606},{\"end\":46634,\"start\":46623},{\"end\":46646,\"start\":46634},{\"end\":46653,\"start\":46646},{\"end\":46663,\"start\":46653},{\"end\":46674,\"start\":46663},{\"end\":46682,\"start\":46674},{\"end\":46690,\"start\":46682},{\"end\":46703,\"start\":46690},{\"end\":46994,\"start\":46984},{\"end\":47229,\"start\":47216},{\"end\":47241,\"start\":47229},{\"end\":47252,\"start\":47241},{\"end\":47265,\"start\":47252},{\"end\":47583,\"start\":47575},{\"end\":47589,\"start\":47583},{\"end\":47595,\"start\":47589},{\"end\":47602,\"start\":47595},{\"end\":47610,\"start\":47602},{\"end\":47618,\"start\":47610},{\"end\":47626,\"start\":47618},{\"end\":47632,\"start\":47626},{\"end\":47641,\"start\":47632},{\"end\":47650,\"start\":47641},{\"end\":47944,\"start\":47931},{\"end\":47957,\"start\":47944},{\"end\":47973,\"start\":47957},{\"end\":47983,\"start\":47973},{\"end\":48303,\"start\":48292},{\"end\":48310,\"start\":48303},{\"end\":48321,\"start\":48310},{\"end\":48332,\"start\":48321},{\"end\":48341,\"start\":48332},{\"end\":48350,\"start\":48341},{\"end\":48360,\"start\":48350},{\"end\":48364,\"start\":48360},{\"end\":48743,\"start\":48734},{\"end\":48751,\"start\":48743},{\"end\":49144,\"start\":49133},{\"end\":49155,\"start\":49144},{\"end\":49471,\"start\":49459},{\"end\":49938,\"start\":49926},{\"end\":49949,\"start\":49938},{\"end\":49960,\"start\":49949},{\"end\":49968,\"start\":49960},{\"end\":49972,\"start\":49968},{\"end\":50260,\"start\":50253},{\"end\":50266,\"start\":50260},{\"end\":50272,\"start\":50266},{\"end\":50282,\"start\":50272},{\"end\":50288,\"start\":50282},{\"end\":50295,\"start\":50288},{\"end\":50302,\"start\":50295},{\"end\":50310,\"start\":50302},{\"end\":50317,\"start\":50310},{\"end\":50324,\"start\":50317},{\"end\":50698,\"start\":50692},{\"end\":50707,\"start\":50698},{\"end\":50714,\"start\":50707},{\"end\":50721,\"start\":50714},{\"end\":51016,\"start\":51000},{\"end\":51264,\"start\":51252},{\"end\":51271,\"start\":51264},{\"end\":51279,\"start\":51271},{\"end\":51295,\"start\":51279},{\"end\":51303,\"start\":51295},{\"end\":51313,\"start\":51303},{\"end\":51326,\"start\":51313},{\"end\":51333,\"start\":51326},{\"end\":51347,\"start\":51333},{\"end\":51685,\"start\":51679},{\"end\":51693,\"start\":51685},{\"end\":51700,\"start\":51693},{\"end\":51921,\"start\":51910},{\"end\":51933,\"start\":51921},{\"end\":51951,\"start\":51933},{\"end\":51964,\"start\":51951},{\"end\":52176,\"start\":52167},{\"end\":52189,\"start\":52176},{\"end\":52199,\"start\":52189},{\"end\":52479,\"start\":52467},{\"end\":52490,\"start\":52479},{\"end\":52764,\"start\":52751},{\"end\":52774,\"start\":52764},{\"end\":52782,\"start\":52774},{\"end\":53080,\"start\":53066},{\"end\":53295,\"start\":53284},{\"end\":53304,\"start\":53295},{\"end\":53320,\"start\":53304},{\"end\":53557,\"start\":53548},{\"end\":53572,\"start\":53557},{\"end\":53581,\"start\":53572},{\"end\":53591,\"start\":53581},{\"end\":53889,\"start\":53877},{\"end\":53900,\"start\":53889},{\"end\":53907,\"start\":53900},{\"end\":53918,\"start\":53907},{\"end\":53931,\"start\":53918},{\"end\":53941,\"start\":53931},{\"end\":54248,\"start\":54239},{\"end\":54258,\"start\":54248},{\"end\":54268,\"start\":54258},{\"end\":54277,\"start\":54268},{\"end\":54624,\"start\":54613},{\"end\":54636,\"start\":54624},{\"end\":54648,\"start\":54636},{\"end\":54662,\"start\":54648},{\"end\":54977,\"start\":54963},{\"end\":54989,\"start\":54977},{\"end\":55001,\"start\":54989},{\"end\":55012,\"start\":55001},{\"end\":55016,\"start\":55012},{\"end\":55319,\"start\":55309},{\"end\":55330,\"start\":55319},{\"end\":55338,\"start\":55330},{\"end\":55508,\"start\":55499},{\"end\":55731,\"start\":55719},{\"end\":55741,\"start\":55731},{\"end\":55754,\"start\":55741},{\"end\":55765,\"start\":55754},{\"end\":55777,\"start\":55765},{\"end\":56152,\"start\":56141},{\"end\":56161,\"start\":56152},{\"end\":56348,\"start\":56333},{\"end\":56356,\"start\":56348},{\"end\":56362,\"start\":56356},{\"end\":56372,\"start\":56362},{\"end\":56384,\"start\":56372},{\"end\":56390,\"start\":56384},{\"end\":56399,\"start\":56390},{\"end\":56411,\"start\":56399},{\"end\":56421,\"start\":56411},{\"end\":56434,\"start\":56421},{\"end\":56444,\"start\":56434},{\"end\":56453,\"start\":56444},{\"end\":56849,\"start\":56838},{\"end\":56861,\"start\":56849},{\"end\":56873,\"start\":56861},{\"end\":56882,\"start\":56873},{\"end\":57253,\"start\":57241},{\"end\":57264,\"start\":57253},{\"end\":57272,\"start\":57264},{\"end\":57282,\"start\":57272},{\"end\":57291,\"start\":57282},{\"end\":57303,\"start\":57291},{\"end\":57540,\"start\":57533},{\"end\":57548,\"start\":57540},{\"end\":57558,\"start\":57548},{\"end\":57791,\"start\":57784},{\"end\":57798,\"start\":57791},{\"end\":57809,\"start\":57798},{\"end\":57820,\"start\":57809},{\"end\":58131,\"start\":58121},{\"end\":58143,\"start\":58131},{\"end\":58154,\"start\":58143},{\"end\":58474,\"start\":58457},{\"end\":58481,\"start\":58474},{\"end\":58493,\"start\":58481},{\"end\":58504,\"start\":58493},{\"end\":58510,\"start\":58504},{\"end\":58522,\"start\":58510},{\"end\":58532,\"start\":58522},{\"end\":58543,\"start\":58532},{\"end\":58558,\"start\":58543},{\"end\":58572,\"start\":58558},{\"end\":58969,\"start\":58961},{\"end\":58977,\"start\":58969},{\"end\":58984,\"start\":58977},{\"end\":58992,\"start\":58984},{\"end\":59000,\"start\":58992},{\"end\":59278,\"start\":59271},{\"end\":59287,\"start\":59278},{\"end\":59296,\"start\":59287},{\"end\":59303,\"start\":59296},{\"end\":59307,\"start\":59303},{\"end\":59554,\"start\":59545},{\"end\":59572,\"start\":59554},{\"end\":59579,\"start\":59572},{\"end\":59583,\"start\":59579},{\"end\":59803,\"start\":59788},{\"end\":59811,\"start\":59803},{\"end\":59821,\"start\":59811},{\"end\":59843,\"start\":59821},{\"end\":59853,\"start\":59843},{\"end\":59864,\"start\":59853},{\"end\":59872,\"start\":59864},{\"end\":60148,\"start\":60140},{\"end\":60157,\"start\":60148},{\"end\":60169,\"start\":60157},{\"end\":60441,\"start\":60433},{\"end\":60450,\"start\":60441},{\"end\":60457,\"start\":60450},{\"end\":60466,\"start\":60457},{\"end\":60659,\"start\":60647},{\"end\":60668,\"start\":60659},{\"end\":60678,\"start\":60668},{\"end\":60688,\"start\":60678},{\"end\":60971,\"start\":60960},{\"end\":60978,\"start\":60971},{\"end\":60986,\"start\":60978},{\"end\":60998,\"start\":60986},{\"end\":61007,\"start\":60998},{\"end\":61019,\"start\":61007},{\"end\":61030,\"start\":61019},{\"end\":61366,\"start\":61354},{\"end\":61376,\"start\":61366},{\"end\":61623,\"start\":61614},{\"end\":61632,\"start\":61623},{\"end\":61643,\"start\":61632}]", "bib_venue": "[{\"end\":43845,\"start\":43802},{\"end\":44221,\"start\":44183},{\"end\":44509,\"start\":44407},{\"end\":44900,\"start\":44848},{\"end\":45306,\"start\":45254},{\"end\":45685,\"start\":45636},{\"end\":46040,\"start\":46005},{\"end\":46353,\"start\":46304},{\"end\":46604,\"start\":46537},{\"end\":47020,\"start\":46994},{\"end\":47298,\"start\":47265},{\"end\":47573,\"start\":47479},{\"end\":48035,\"start\":47983},{\"end\":48408,\"start\":48364},{\"end\":48854,\"start\":48751},{\"end\":49207,\"start\":49155},{\"end\":49571,\"start\":49471},{\"end\":50030,\"start\":49972},{\"end\":50404,\"start\":50340},{\"end\":50779,\"start\":50721},{\"end\":51079,\"start\":51016},{\"end\":51433,\"start\":51363},{\"end\":51758,\"start\":51700},{\"end\":52010,\"start\":51980},{\"end\":52248,\"start\":52199},{\"end\":52542,\"start\":52490},{\"end\":52840,\"start\":52782},{\"end\":53064,\"start\":53011},{\"end\":53356,\"start\":53320},{\"end\":53649,\"start\":53591},{\"end\":53976,\"start\":53941},{\"end\":54335,\"start\":54277},{\"end\":54711,\"start\":54662},{\"end\":55084,\"start\":55016},{\"end\":55380,\"start\":55354},{\"end\":55534,\"start\":55508},{\"end\":55826,\"start\":55777},{\"end\":56139,\"start\":56067},{\"end\":56493,\"start\":56453},{\"end\":56950,\"start\":56882},{\"end\":57239,\"start\":57177},{\"end\":57600,\"start\":57558},{\"end\":57878,\"start\":57820},{\"end\":58197,\"start\":58154},{\"end\":58624,\"start\":58572},{\"end\":59044,\"start\":59000},{\"end\":59351,\"start\":59307},{\"end\":59602,\"start\":59583},{\"end\":59914,\"start\":59872},{\"end\":60266,\"start\":60185},{\"end\":60431,\"start\":60414},{\"end\":60737,\"start\":60688},{\"end\":61088,\"start\":61030},{\"end\":61414,\"start\":61376},{\"end\":61687,\"start\":61643},{\"end\":47033,\"start\":47022}]"}}}, "year": 2023, "month": 12, "day": 17}